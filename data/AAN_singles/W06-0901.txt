Proceedings of the Workshop on Annotating and Reasoning about Time and Events, pages 1?8,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe stages of event extractionDavid AhnIntelligent Systems Lab AmsterdamUniversity of Amsterdamahn@science.uva.nlAbstractEvent detection and recognition is a com-plex task consisting of multiple sub-tasksof varying difficulty.
In this paper, wepresent a simple, modular approach toevent extraction that allows us to exper-iment with a variety of machine learningmethods for these sub-tasks, as well as toevaluate the impact on performance thesesub-tasks have on the overall task.1 IntroductionEvents are undeniably temporal entities, but theyalso possess a rich non-temporal structure that isimportant for intelligent information access sys-tems (information retrieval, question answering,summarization, etc.).
Without information aboutwhat happened, where, and to whom, temporal in-formation about an event may not be very useful.In the available annotated corpora geared to-ward information extraction, we see two mod-els of events, emphasizing these different aspects.On the one hand, there is the TimeML model, inwhich an event is a word that points to a nodein a network of temporal relations.
On the otherhand, there is the ACE model, in which an eventis a complex structure, relating arguments that arethemselves complex structures, but with only an-cillary temporal information (in the form of tem-poral arguments, which are only noted when ex-plicitly given).
In the TimeML model, every eventis annotated, because every event takes part in thetemporal network.
In the ACE model, only ?in-teresting?
events (events that fall into one of 34predefined categories) are annotated.The task of automatically extracting ACEevents is more complex than extracting TimeMLevents (in line with the increased complexity ofACE events), involving detection of event anchors,assignment of an array of attributes, identificationof arguments and assignment of roles, and deter-mination of event coreference.
In this paper, wepresent a modular system for ACE event detectionand recognition.
Our focus is on the difficulty andimportance of each sub-task of the extraction task.To this end, we isolate and perform experimentson each stage, as well as evaluating the contribu-tion of each stage to the overall task.In the next section, we describe events in theACE program in more detail.
In section 3, we pro-vide an overview of our approach and some infor-mation about our corpus.
In sections 4 through 7,we describe our experiments for each of the sub-tasks of event extraction.
In section 8, we comparethe contribution of each stage to the overall task,and in section 9, we conclude.2 Events in the ACE programThe ACE program1 provides annotated data, eval-uation tools, and periodic evaluation exercises fora variety of information extraction tasks.
There arefive basic kinds of extraction targets supported byACE: entities, times, values, relations, and events.The ACE tasks for 2005 are more fully describedin (ACE, 2005).
In this paper, we focus on events,but since ACE events are complex structures in-volving entities, times, and values, we briefly de-scribe these, as well.ACE entities fall into seven types (person, or-ganization, location, geo-political entity, facility,vehicle, weapon), each with a number of subtypes.Within the ACE program, a distinction is made be-tween entities and entity mentions (similarly be-1http://www.nist.gov/speech/tests/ace/1tween event and event mentions, and so on).
Anentity mention is a referring expression in text (aname, pronoun, or other noun phrase) that refersto something of an appropriate type.
An entity,then, is either the actual referent, in the world,of an entity mention or the cluster of entity men-tions in a text that refer to the same actual entity.The ACE Entity Detection and Recognition taskrequires both the identification of expressions intext that refer to entities (i.e., entity mentions) andcoreference resolution to determine which entitymentions refer to the same entities.There are also ACE tasks to detect and recog-nize times and a limited set of values (contact in-formation, numeric values, job titles, crime types,and sentence types).
Times are annotated accord-ing to the TIMEX2 standard, which requires nor-malization of temporal expressions (timexes) to anISO-8601-like value.ACE events, like ACE entities, are restrictedto a range of types.
Thus, not all events in atext are annotated?only those of an appropriatetype.
The eight event types (with subtypes inparentheses) are Life (Be-Born, Marry, Divorce,Injure, Die), Movement (Transport), Transaction(Transfer-Ownership, Transfer-Money), Business(Start-Org, Merge-Org, Declare-Bankruptcy, End-Org), Conflict (Attack, Demonstrate), Contact(Meet, Phone-Write), Personnel (Start-Position,End-Position, Nominate, Elect), Justice (Arrest-Jail, Release-Parole, Trial-Hearing, Charge-Indict,Sue, Convict, Sentence, Fine, Execute, Extradite,Acquit, Appeal, Pardon).
Since there is nothinginherent in the task that requires the two levels oftype and subtype, for the remainder of the paper,we will refer to the combination of event type andsubtype (e.g., Life:Die) as the event type.In addition to their type, events have four otherattributes (possible values in parentheses): modal-ity (Asserted, Other), polarity (Positive, Nega-tive), genericity (Specific, Generic), tense (Past,Present, Future, Unspecified).The most distinctive characteristic of events(unlike entities, times, and values, but like rela-tions) is that they have arguments.
Each event typehas a set of possible argument roles, which may befilled by entities, values, or times.
In all, there are35 role types, although no single event can have all35 roles.
A complete description of which roles gowith which event types can be found in the anno-tation guidelines for ACE events (LDC, 2005).Events, like entities, are distinguished fromtheir mentions in text.
An event mention is a spanof text (an extent, usually a sentence) with a dis-tinguished anchor (the word that ?most clearly ex-presses [an event?s] occurrence?
(LDC, 2005)) andzero or more arguments, which are entity men-tions, timexes, or values in the extent.
An event iseither an actual event, in the world, or a cluster ofevent mentions that refer to the same actual event.Note that the arguments of an event are the enti-ties, times, and values corresponding to the entitymentions, timexes, and values that are argumentsof the event mentions that make up the event.The official evaluation metric of the ACE pro-gram is ACE value, a cost-based metric whichassociates a normalized, weighted cost to systemerrors and subtracts that cost from a maximumscore of 100%.
For events, the associated costsare largely determined by the costs of the argu-ments, so that errors in entity, timex, and valuerecognition are multiplied in event ACE value.Since it is useful to evaluate the performance ofevent detection and recognition independently ofthe recognition of entities, times, and values, theACE program includes diagnostic tasks, in whichpartial ground truth information is provided.
Ofparticular interest here is the diagnostic task forevent detection and recognition, in which groundtruth entities, values, and times are provided.
Forthe remainder of this paper, we use this diagnos-tic methodology, and we extend it to sub-taskswithin the task, evaluating components of ourevent recognition system using ground truth out-put of upstream components.
Furthermore, in ourevaluating our system components, we use themore transparent metrics of precision, recall, F-measure, and accuracy.3 Our approach to event extraction3.1 A pipeline for detecting and recognizingeventsExtracting ACE events is a complex task.
Our goalwith the approach we describe in this paper is toestablish baseline performance in this task using arelatively simple, modular system.
We break downthe task of extracting events into a series of clas-sification sub-tasks, each of which is handled by amachine-learned classifier.1.
Anchor identification: finding event anchors(the basis for event mentions) in text and as-signing them an event type;22.
Argument identification: determining whichentity mentions, timexes, and values are ar-guments of each event mention;3.
Attribute assignment: determining the valuesof the modality, polarity, genericity, and tenseattributes for each event mention;4.
Event coreference: determining which eventmentions refer to the same event.In principle, these four sub-tasks are highly inter-dependent, but for the approach described here,we do not model all these dependencies.
Anchoridentification is treated as an independent task.
Ar-gument finding and attribute assignment are eachdependent only on the results of anchor identifica-tion, while event coreference depends on the re-sults of all of the other three sub-tasks.To learn classifiers for the first three tasks, weexperiment with TiMBL2, a memory-based (near-est neighbor) learner (Daelemans et al, 2004),and MegaM3, a maximum entropy learner (Daume?III, 2004).
For event coreference, we use onlyMegaM, since our approach requires probabilities.In addition to comparing the performance of thesetwo learners on the various sub-tasks, we also ex-periment with the structure of the learning prob-lems for the first two tasks.In the remainder of this paper, we present exper-iments for each of these sub-tasks (sections 4?
7),focusing on each task in isolation, and then look athow the sub-tasks affect performance in the over-all task (section 8).
First, we discuss the prepro-cessing of the corpus required for our experiments.3.2 Preprocessing the corpusBecause of restrictions imposed by the organiz-ers on the 2005 ACE program data, we use onlythe ACE 2005 training corpus, which contains 599documents, for our experiments.
We split this cor-pus into training and test sets at the document-level, with 539 training documents and 60 testdocuments.
From the training set, another 60 doc-uments are reserved as a development set, whichis used for parameter tuning by MegaM.
For theremainder of the paper, we will refer to the 539training documents as the training corpus and the60 test documents as the test corpus.For our machine learning experiments, we needa range of information in order to build feature2http://ilk.uvt.nl/timbl/3http://www.isi.edu/?hdaume/megam/vectors.
Since we are interested only in perfor-mance on event extraction, we follow the method-ology of the ACE diagnostic tasks and use theground truth entity, timex2, and value annotationsboth for training and testing.
Additionally, eachdocument is tokenized and split into sentences us-ing a simple algorithm adapted from (Grefenstette,1994, p. 149).
These sentences are parsed usingthe August 2005 release of the Charniak parser(Charniak, 2000)4.
The parses are converted intodependency relations using a method similar to(Collins, 1999; Jijkoun and de Rijke, 2004).
Thesyntactic annotations thus provide access both toconstituency and dependency information.
Notethat with respect to these two sources of syntacticinformation, we use the word head ambiguously torefer both to the head of a constituent (i.e., the dis-tinguished word within the constituent from whichthe constituent inherits its category features) andto the head of a dependency relation (i.e., the wordon which the dependent in the relation depends).Since parses and entity/timex/value annotationsare produced independently, we need a strategy formatching (entity/timex/value) mentions to parses.Given a mention, we first try to find a single con-stituent whose offsets exactly match the extent ofthe mention.
In the training and development data,there is an exact-match constituent for 89.2% ofthe entity mentions.
If there is no such constituent,we look for a sequence of constituents that matchthe mention extent.
If there is no such sequence,we back off to a single word, looking first for aword whose start offset matches the start of themention, then for a word whose end offset matchesthe end of the mention, and finally for a word thatcontains the entire mention.
If all these strategiesfail, then no parse information is provided for themention.
Note that when a mention matches a se-quence of constituents, the head of the constituentin the sequence that is shallowest in the parse treeis taken to be the (constituent) head of the entiresequence.
Given a parse constituent, we take theentity type of that constituent to be the type of thesmallest entity mention overlapping with it.4 Identifying event anchors4.1 Task structureWe model anchor identification as a word classifi-cation task.
Although an event anchor may in prin-ciple be more than one word, more than 95% of4ftp://ftp.cs.brown.edu/pub/nlparser/3the anchors in the training data consist of a singleword.
Furthermore, in the training data, anchorsare restricted in part of speech (to nouns: NN,NNS, NNP; verbs: VB, VBZ, VBP, VBG, VBN,VBD, AUX, AUXG, MD; adjectives: JJ; adverbs:RB, WRB; pronouns: PRP, WP; determiners: DT,WDT, CD; and prepositions: IN).
Thus, anchoridentification for a document is reduced to the taskof classifying each word in the document with anappropriate POS tag into one of 34 classes (the 33event types plus a None class for words that arenot an event anchor).The class distribution for these 34 classes isheavily skewed.
In the 202,135 instances inthe training data, the None class has 197,261instances, while the next largest class (Con-flict:Attack) has only 1410 instances.
Thus, in ad-dition to modeling anchor identification as a sin-gle multi-class classification task, we also try tobreak down the problem into two stages: first, abinary classifier that determines whether or not aword is an anchor, and then, a multi-class classi-fier that determines the event type for the positiveinstances from the first task.
For this staged task,we train the second classifier on the ground truthpositive instances.4.2 Features for event anchorsWe use the following set of features for all config-urations of our anchor identification experiments.?
Lexical features: full word, lowercase word,lemmatized word, POS tag, depth of word inparse tree?
WordNet features: for each WordNet POScategory c (from N, V, ADJ, ADV):?
If the word is in catgory c and there is acorresponding WordNet entry, the ID ofthe synset of first sense is a feature value?
Otherwise, if the word has an entry inWordNet that is morphologically relatedto a synset of category c, the ID of therelated synset is a feature value?
Left context (3 words): lowercase, POS tag?
Right context (3 words): lowercase, POS tag?
Dependency features: if the candidate wordis the dependent in a dependency relation, thelabel of the relation is a feature value, as arethe dependency head word, its POS tag, andits entity type?
Related entity features: for each en-tity/timex/value type t:?
Number of dependents of candidateword of type t?
Label(s) of dependency relation(s) todependent(s) of type t?
Constituent head word(s) of depen-dent(s) of type t?
Number of entity mentions of type treachable by some dependency path(i.e., in same sentence)?
Length of path to closest entity mentionof type t4.3 ResultsIn table 1, we present the results of our anchorclassification experiments (precision, recall and F-measure).
The all-at-once conditions refer to ex-periments with a single multi-class classifier (us-ing either MegaM or TiMBL), while the split con-ditions refer to experiments with two staged clas-sifiers, where we experiment with using MegaMand TiMBL for both classifiers, as well as withusing MegaM for the binary classification andTiMBL for the multi-class classification.
In ta-ble 2, we present the results of the two first-stagebinary classifiers, and in table 3, we present theresults of the two second-stage multi-class classi-fiers on ground truth positive instances.
Note thatwe always use the default parameter settings forMegaM, while for TiMBL, we set k (number ofneighbors to consider) to 5, we use inverse dis-tance weighting for the neighbors and weightedoverlap, with information gain weighting, for allnon-numeric features.Both for the all-at-once condition and for multi-class classification of positive instances, the near-est neighbor classifier performs substantially bet-ter than the maximum entropy classifier.
For bi-nary classification, though, the two methods per-form similarly, and staging either binary classi-fier with the nearest neighbor classifier for posi-tive instances yields the best results.
In practicalterms, using the maximum entropy classifier forbinary classification and then the TiMBL classifierto classify only the positive instances is the bestsolution, since classification with TiMBL tends tobe slow.4Precision Recall FAll-at-once/megam 0.691 0.239 0.355All-at-once/timbl 0.666 0.540 0.596Split/megam 0.589 0.417 0.489Split/timbl 0.657 0.551 0.599Split/megam+timbl 0.725 0.513 0.601Table 1: Results for anchor detection and classifi-cationPrecision Recall FBinary/megam 0.756 0.535 0.626Binary/timbl 0.685 0.574 0.625Table 2: Results for anchor detection (i.e., binaryclassification of anchor instances)5 Argument identification5.1 Task structureIdentifying event arguments is a pair classificationtask.
Each event mention is paired with each of theentity/timex/value mentions occurring in the samesentence to form a single classification instance.There are 36 classes in total: 35 role types and aNone class.
Again, the distribution of classes isskewed, though not as heavily as for the anchortask, with 20,556 None instances out of 29,450training instances.
One additional considerationis that no single event type allows arguments ofall 36 possible roles; each event type has its ownset of allowable roles.
With this in mind, we ex-periment with treating argument identification asa single multi-class classification task and withtraining a separate multi-class classifier for eachevent type.
Note that all classifiers are trained us-ing ground truth event mentions.5.2 Features for argument identificationWe use the following set of features for all our ar-gument classifiers.?
Anchor word of event mention: full, lower-case, POS tag, and depth in parse treeAccuracyMulti/megam 0.649Multi/timbl 0.824Table 3: Accuracy for anchor classification (i.e.,multi-class classification of positive anchor in-stances)Precision Recall FAll-at-once/megam 0.708 0.430 0.535All-at-once/timbl 0.509 0.453 0.480CPET/megam 0.689 0.490 0.573CPET/timbl 0.504 0.535 0.519Table 4: Results for arguments?
Event type of event mention?
Constituent head word of entity mention:full, lowercase, POS tag, and depth in parsetree?
Determiner of entity mention, if any?
Entity type and mention type (name, pro-noun, other NP) of entity mention?
Dependency path between anchor word andconstituent head word of entity mention, ex-pressed as a sequence of labels, of words, andof POS tags5.3 ResultsIn table 4, we present the results for argumentidentification.
The all-at-once conditions referto experiments with a single classifier for all in-stances.
The CPET conditions refer to experi-ments with a separate classifier for each eventtype.
Note that we use the same parameter settingsfor MegaM and TiMBL as for anchor classifica-tion, except that for TiMBL, we use the modifiedvalue difference metric for the three dependencypath features.Note that splitting the task into separate tasksfor each event type yields a substantial improve-ment over using a single classifier.
Unlike in theanchor classification task, maximum entropy clas-sification handily outperforms nearest-neighborclassification.
This may be related to the binariza-tion of the dependency-path features for maximumentropy training: the word and POS tag sequences(but not the label sequences) are broken down intotheir component steps, so that there is a separatebinary feature corresponding to the presence of agiven word or POS tag in the dependency path.Table 5 presents results of each of the classi-fiers restricted to Time-* arguments (Time-Within,Time-Holds, etc.).
These arguments are of partic-ular interest not only because they provide the linkbetween events and times in this model of events,but also because Time-* roles, unlike other role5Precision Recall FAll-at-once/megam 0.688 0.477 0.564All-at-once/timbl 0.500 0.482 0.491CPET/megam 0.725 0.451 0.556CPET/timbl 0.357 0.404 0.379Table 5: Results for Time-* argumentsAccuracymegam 0.795timbl 0.793baseline 0.802majority (in training) 0.773Table 6: Genericitytypes, are available to all event types.
We see that,in fact, the all-at-once classifiers perform betterfor these role types, which suggests that it may beworthwhile to factor out these role types and builda classifier specifically for temporal arguments.6 Assigning attributes6.1 Task structureIn addition to the event type and subtype attributes,(the event associated with) each event mentionmust also be assigned values for genericity, modal-ity, polarity, and tense.
We train a separate classi-fier for each attribute.
Genericity, modality, andpolarity are each binary classification tasks, whiletense is a multi-class task.
We use the same fea-tures as for the anchor identification task, with theexception of the lemmatized anchor word and theWordNet features.6.2 ResultsThe results of our classification experiments aregiven in tables 6, 7, 8, and 9.
Note that modal-ity, polarity, and genericity are skewed tasks whereit is difficult to improve on the baseline majorityclassification (Asserted, Positive, and Specific, re-spectively) and where maximum entropy and near-est neighbor classification perform very similarly.For tense, however, both learned classifiers per-form substantially better than the majority base-line (Past), with the maximum entropy classifierproviding the best performance.Accuracymegam 0.750timbl 0.759baseline 0.738majority (in training) 0.749Table 7: ModalityAccuracymegam 0.955timbl 0.955baseline 0.950majority (in training) 0.967Table 8: Polarity7 Event coreference7.1 Task structureFor event coreference, we follow the approachto entity coreference detailed in (Florian et al,2004).
This approach uses a mention-pair coref-erence model with probabilistic decoding.
Eachevent mention in a document is paired with ev-ery other event mention, and a classifier assignsto each pair of mentions the probability that thepaired mentions corefer.
These probabilities areused in a left-to-right entity linking algorithm inwhich each mention is compared with all already-established events (i.e., event mention clusters) todetermine whether it should be added to an exist-ing event or start a new one.
Since the classifierneeds to output probabilities for this approach, wedo not use TiMBL, but only train a maximum en-tropy classifier with MegaM.7.2 Features for coreference classificationWe use the following set of features for ourmention-pair classifier.
The candidate is the ear-lier event mention in the text, and the anaphor isthe later mention.?
CandidateAnchor+AnaphorAnchor, alsoPOS tag and lowercaseAccuracymegam 0.633timbl 0.613baseline 0.535majority (in training) 0.512Table 9: Tense6Precision Recall Fmegam 0.761 0.580 0.658baseline 0.167 1.0 0.286Table 10: Coreference?
CandidateEventType+AnaphorEventType?
Depth of candidate anchor word in parse tree?
Depth of anaphor anchor word in parse tree?
Distance between candidate and anchor, mea-sured in sentences?
Number, heads, and roles of shared argu-ments (same entity/timex/value w/same role)?
Number, heads, and roles of candidate argu-ments that are not anaphor arguments?
Number, heads, and roles of anaphor argu-ments that are not candidate arguments?
Heads and roles of arguments shared by can-didate and anaphor in different roles?
CandidateModalityVal+AnaphorModalityVal,also for polarity, genericity, and tense7.3 ResultsIn table 10, we present the performance of ourevent coreference pair classifier.
Note that thedistribution for this task is also skewed: only3092 positive instances of 42,736 total training in-stances.
Simple baseline of taking event mentionsof identical type to be coreferent does quite poorly.8 Evaluation with ACE valueTable 11 presents results of performing the fullevent detection and recognition task, swapping inground truth (gold) or learned classifiers (learned)for the various sub-tasks (we also swap in major-ity classifiers for the attribute sub-task).
For theanchor sub-task, we use the split/megam+timblclassifier; for the argument sub-task, we use theCPET/megam classifier; for the attribute sub-tasks, we use the megam classifiers; for the coref-erence sub-task, we use the approach outlined insection 7.
Since in our approach, the argument andattribute sub-tasks are dependent on the anchorsub-task and the coreference sub-task is depen-dent on all of the other sub-tasks, we cannot freelyswap in ground truth?e.g., if we use a learnedclassifier for the anchor sub-task, then there is noground truth for the corresponding argument andattribute sub-tasks.The learned coreference classifier provides asmall boost to performance over doing no coref-erence at all (7.5% points for the condition inwhich all the other sub-tasks use ground truth (1vs.
8), 0.6% points when all the other sub-tasksuse learned classifiers (7 vs. 12)).
From perfectcoreference, using ground truth for the other sub-tasks, the loss in value is 11.4% points (recall thatmaximum ACE value is 100%).
Note that the dif-ference between perfect coreference and no coref-erence is only 18.9% points.Looking at the attribute sub-tasks, the effects onACE value are even smaller.
Using the learnedattribute classifiers (with ground truth anchors andarguments) results in 4.8% point loss in value fromground truth attributes (1 vs. 5) and only a 0.5%point gain in value from majority class attributes(4 vs. 5).
With learned anchors and arguments, thelearned attribute classifiers result in a 0.4% loss invalue from even majority class attributes (3 vs. 7).Arguments clearly have the greatest impact onACE value (which is unsurprising, given that ar-guments are weighted heavily in event value).
Us-ing ground truth anchors and attributes, learned ar-guments result in a loss of value of 35.6% pointsfrom ground truth arguments (1 vs. 2).
When thelearned coreference classifier is used, the loss invalue from ground truth arguments to learned ar-guments is even greater (42.5%, 8 vs. 10).Anchor identification also has a large impact onACE value.
Without coreference but with learnedarguments and attributes, the difference betweenusing ground truth anchors and learned anchors is22.2% points (6 vs. 7).
With coreference, the dif-ference is still 21.0% points (11 vs. 12).Overall, using the best learned classifiers forthe various subtasks, we achieve an ACE valuescore of 22.3%, which falls within the range ofscores for the 2005 diagnostic event extractiontask (19.7%?32.7%).5 Note, though, that thesescores are not really comparable, since they in-volve training on the full training set and testingon a separate set of documents (as noted above,the 2005 ACE testing data is not available for fur-ther experimentation, so we are using 90% of theoriginal training data for training/development and5For the diagnostic task, ground truth entities, values, andtimes, are provided, as they are in our experiments.7anchors args attrs coref ACE value1 gold gold gold none 81.1%2 gold learned gold none 45.5%3 learned learned maj none 22.1%4 gold gold maj none 75.8%5 gold gold learned none 76.3%6 gold learned learned none 43.9%7 learned learned learned none 21.7%8 gold gold gold learned 88.6%9 gold gold learned learned 79.4%10 gold learned gold learned 46.1%11 gold learned learned learned 43.3%12 learned learned learned learned 22.3%Table 11: ACE value10% for the results presented here).9 Conclusion and future workIn this paper, we have presented a system for ACEevent extraction.
Even with the simple breakdownof the task embodied by the system and the limitedfeature engineering for the machine learned classi-fiers, the performance is not too far from the levelof the best systems at the 2005 ACE evaluation.Our approach is modular, and it has allowed us topresent several sets of experiments exploring theeffect of different machine learning algorithms onthe sub-tasks and exploring the effect of the differ-ent sub-tasks on the overall performance (as mea-sured by ACE value).There is clearly a great deal of room for im-provement.
As we have seen, improving anchorand argument identification will have the great-est impact on overall performance, and the exper-iments we have done suggest directions for suchimprovement.
For anchor identification, takingone more step toward binary classification andtraining a binary classifier for each event type (ei-ther for all candidate anchor instances or only forpositive instances) may be helpful.
For argumentidentification, we have already discussed the ideaof modeling temporal arguments separately; per-haps introducing a separate classifier for each roletype might also be helpful.For all the sub-tasks, there is more feature en-gineering that could be done (a simple example:for coreference, boolean features corresponding toidentical anchors and event types).
Furthermore,the dependencies between sub-tasks could be bet-ter modeled.References2005.
The ACE 2005 (ACE05) evaluation plan.http://www.nist.gov/speech/tests/ace/ace05/doc/ace05-evalplan.v3.pdf.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st Meetingof NAACL, pages 132?139.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Walter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2004.
TiMBL: Tilburg Mem-ory Based Learner, version 5.1, Reference Guide.University of Tilburg, ILK Technical Report ILK-0402.
http://ilk.uvt.nl/.Hal Daume?
III.
2004.
Notes on CG and LM-BFGSoptimization of logistic regression.
Paper availableat http://www.isi.edu/?hdaume/docs/daume04cg-bfgs.ps, August.Radu Florian, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicolas Nicolov, and Salim Roukos.
2004.
A sta-tistical model for multilingual entity detection andtracking.
In Proceedings of HLT/NAACL-04.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer.Valentin Jijkoun and Maarten de Rijke.
2004.
Enrich-ing the output of a parser using memory-based learn-ing.
In Proceedings of the 42nd Meeting of the ACL.Linguistic Data Consortium, 2005.
ACE (AutomaticContent Extraction) English Annotation Guidelinesfor Events, version 5.4.3 2005.07.01 edition.8
