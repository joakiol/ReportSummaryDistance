A Statistical Model for Word Discoveryin Transcribed SpeechAnand Venkataraman*A statistical model/or segmentation a d word discovery in continuous speech is presented.
Anincremental unsupervised learning algorithm to infer word boundaries based on this model isdescribed.
Results are also presented of empirical tests howing that the algorithm is competitivewith other models that have been used/or similar tasks.1.
IntroductionEnglish speech lacks the acoustic analog of blank spaces that people are accustomedto seeing between words in written text.
Discovering words in continuous pokenspeech is thus an interesting problem and one that has been treated at length in theliterature.
The problem of identifying word boundaries is particularly significant inthe parsing of written text in languages that do not explicitly include spaces betweenwords.
In addition, if we assume that children start out with little or no knowledgeof the inventory of words the language possesses identification of word boundaries ia significant problem in the domain of child language acquisition.
1 Although speechlacks explicit demarcation of word boundaries, it is undoubtedly the case that it never-theless possesses significant other cues for word discovery.
However, it is still a matterof interest o see exactly how much can be achieved without the incorporation of theseother cues; that is, we are interested in the performance of a bare-bones language model.For example, there is much evidence that stress patterns (Jusczyk, Cutler, and Redanz1993; Cutler and Carter 1987) and phonotactics of speech (Mattys and Jusczyk 1999)are of considerable aid in word discovery.
But a bare-bones tatistical model is stilluseful in that it allows us to quantify precise improvements in performance upon theintegration of each specific cue into the model.
We present and evaluate one suchstatistical model in this paper.
2The main contributions of this study are as follows: First, it demonstrates the ap-plicability and competitiveness of a conservative, traditional approach for a task forwhich nontraditional pproaches have been proposed even recently (Brent 1999; Brentand Cartwright 1996; de Marcken 1995; Elman 1990; Christiansen, Allen, and Seiden-berg 1998).
Second, although the model leads to the development of an algorithm thatlearns the lexicon in an unsupervised fashion, results of partial supervision are pre-sented, showing that its performance is consistent with results from learning theory.Third, the study extends previous work to higher-order n-grams, specifically up to* STAR Lab, SRI International, 333 Ravenswood Ave., Menlo Park, CA 94025.
E-mail:anand@speech.sri.com1 See, however, work in Jusczyk and Hohne (1997) and Jusczyk (1997) that presents strong evidence infavor of a hypothesis that children already have a reasonably powerful and accurate lexicon at theirdisposal as early as 9 months of age.2 Implementations of all the programs discussed in this paper and the input corpus are readily availableupon request from the author.
The programs (totaling about 900 lines) have been written in C++ tocompile under Unix/Linux.
The author will assist in porting it to other architectures or to versions ofUnix other than Linux or SunOS/Solaris if required.
(~) 2001 Association for Computational LinguisticsComputational Linguistics Volume 27, Number 3trigrams, and discusses the results in their light.
Finally, results of experiments sug-gested in Brent (1999) regarding different ways of estimating phoneme probabilitiesare also reported.
Wherever possible, results are averaged over 1000 repetitions of theexperiments, thus removing any potential advantages the algorithm may have haddue to ordering idiosyncrasies within the input corpus.Section 2 briefly discusses related literature in the field and recent work on thesame topic.
The model is described in Section 3.
Section 4 describes an unsupervisedlearning algorithm based directly on the model developed in Section 3.
This sectionalso describes the data corpus used to test the algorithms and the methods used.Results are presented and discussed in Section 5.
Finally, the findings in this work aresummarized in Section 6.2.
Related WorkWhile there exists a reasonable body of literature regarding text segmentation, espe-cially with respect o languages uch as Chinese and Japanese that do not explicitlyinclude spaces between words, most of the statistically based models and algorithmstend to fall into the supervised learning category.
These require the model to be trainedfirst on a large corpus of text before it can segment i s input.
3 It is only recently that in-terest in unsupervised algorithms for text segmentation seems to have gained ground.A notable exception in this regard is the work by Ando and Lee (1999) which triesto infer word boundaries from character n-gram statistics of Japanese Kanji strings.For example, a decision to insert a word boundary between two characters i  madesolely based on whether character n-grams adjacent o the proposed boundary arerelatively more frequent han character n-grams that straddle it.
This algorithm, how-ever, is not based on a formal statistical model and is closer in spirit to approachesbased on transitional probability between phonemes or syllables in speech.
One suchapproach derives from experiments by Saffran, Newport, and Aslin (1996) suggestingthat young children might place word boundaries between two syllables where thesecond syllable is surprising given the first.
This technique is described and evaluatedin Brent (1999).
Other approaches not based on explicit probability models includethose based on information theoretic criteria such as minimum description length(Brent and Cartwright 1996; de Marcken 1995) and simple recurrent networks (Elman1990; Christiansen, Allen, and Seidenberg 1998).
The maximum likelihood approachdue to Olivier (1968) is probabilistic in the sense that it is geared toward explicitlycalculating the most probable segmentation of each block of input utterances ( ee alsoBatchelder 1997).
However, the algorithm involves heuristic steps in periodic purg-ing of the lexicon and in the creation in the lexicon of new words.
Furthermore, thisapproach is again not based on a formal statistical model.Model Based Dynamic Programming, hereafter referred to as MBDP-1 (Brent 1999),is probably the most recent work that addresses exactly the same issue as that con-sidered in this paper.
Both the approach presented in this paper and Brent's MBDP-1are unsupervised approaches based on explicit probability models.
Here, we describeonly Brent's MBDP-1 and direct the interested reader to Brent (1999) for an excellentreview and evaluation of many of the algorithms mentioned above.2.1 Brent's model-based ynamic programming methodBrent (1999) describes a model-based approach to inferring word boundaries in child-directed speech.
As the name implies, this technique uses dynamic programming to3 See, for example, Zimin and Tseng (1993).Venkataraman Word Discovery in Transcribed Speechinfer the best segmentation.
It is assumed that the entire input corpus, consisting of aconcatenation of all utterances in sequence, is a single event in probability space andthat the best segmentation of each utterance is implied by the best segmentation of thecorpus itself.
The model thus focuses on explicitly calculating probabilities for everypossible segmentation f the entire corpus, and subsequently picking the segmentationwith the maximum probability.
More precisely, the model attempts to calculateP(wm) = ~ ~ ~ ~P(Wm i n, L , i ,s )P(n,L ,d,s)n L f sfor each possible segmentation f the input corpus where the left-hand side is the exactprobability of that particular segmentation of the corpus into words Wm = WlW2 " ' "  Win;and the sums are over all possible numbers of words n, in the lexicon, all possiblelexicons L, all possible frequencies f,  of the individual words in this lexicon and allpossible orders of words s, in the segmentation.
In practice, the implementation usesan incremental approach that computes the best segmentation of the entire corpusup to step i, where the ith step is the corpus up to and including the ith utterance.Incremental performance is thus obtained by computing this quantity anew after eachsegmentation i - 1, assuming however, that segmentations of utterances up to but notincluding i are fixed.There are two problems with this approach.
First, the assumption that the entirecorpus of observed speech should be treated as a single event in probability space ap-pears rather adical.
This fact is appreciated even in Brent (1999), which states "From acognitive perspective, we know that humans segment each utterance they hear with-out waiting until the corpus of all utterances they will ever hear becomes available"(p. 89).
Thus, although the incremental algorithm in Brent (1999) is consistent with adevelopmental model, the formal statistical model of segmentation is not.Second, making the assumption that the corpus is a single event in probabilityspace significantly increases the computational complexity of the incremental algo-rithm.
The approach presented in this paper circumvents these problems through theuse of a conservative statistical model that is directly implementable asan incrementalalgorithm.
In the following section, we describe the model and how its 2-gram and3-gram extensions are adapted for implementation.3.
Model DescriptionThe language model described here is a fairly standard one.
The interested reader isreferred to Jelinek (1997, 57-78), where a detailed exposition can be found.
Basically,we seek--- argmax P(W) (1)Wn= argmaxr - \ [P (w i lw l  .
.
.
.
.
Wi_l) (2)W i=1n= argmin~-~- logP(wi lwl  .
.
.
.
.
wi -1)  (3)W i=1where W = wl .
.
.
.
.
wn with w i C L denotes a particular string of n words belonging toa lexicon L.The usual n-gram approximation is made by grouping histories wl .
.
.
.
.
wi-1 intoequivalence classes, allowing us to collapse contexts into histories at most n - 1 wordsComputational Linguistics Volume 27, Number 3backwards (for n-grams).
Estimations of the required n-gram probabilities are thendone with relative frequencies using back-off to lower-order n-grams when a higher-order estimate is not reliable enough (Katz 1987).
Back-off is done using the Witten andBell (1991) technique, which allocates a probability of Ni/(Ni q- Si) to unseen/-gramsat each stage, with the final back-off from unigrams being to an open vocabularywhere word probabilities are calculated as a normalized product of phoneme or letterprobabilities.
Here, Ni is the number of distinct /-grams and Si is the sum of theirfrequencies.
The model can be summarized as follows:( s~ C(w~_2,wi_l,wi) f C(wi_2, Wi_l, wi) ~> 0 p(wilwi_2, wi_l ) = INB+S3 C(w,_,,w,) (4)\[ N3~3P(wi \[ Wi-1) otherwise( s2 C(wi_l,wi)P(Wi \] Wi--1) ~-  ~ N2+S2 ~ if C(wi-1, wi) > 0N2 P(wi) (5) / ~ otherwise(c (~)P(wi) = ~Nkts ~ if C(wi) > 0 (6)\[ ~P~(wi )  otherwisekir(#) I-I r(wi~'\])PE (wi) = j= l  1 - r(#) (7)where C 0 denotes the count or frequency function, ki denotes the length of word wi,excluding the sentinel character #, wi\[j\] denotes its jth phoneme, and r 0 denotes therelative frequency function.
The normalization by dividing using 1 - r(#) in Equa-tion (7) is necessary because otherwiseO0~P(w)  = ~(1-  P(#))iP(#) (8)w i=1= 1 - P(#) (9)Since we estimate P(w~\]) by r(w~\]), dividing by 1 - r (#)  will ensure that ~w P(w) = 1.4.
MethodAs in Brent (1999), the model described in Section 3 is presented as an incrementallearner.
The only knowledge built into the system at start-up is the phoneme table,with a uniform distribution over all phonemes, including the sentinel phoneme.
Thelearning algorithm considers each utterance in turn and computes the most proba-ble segmentation of the utterance using a Viterbi search (Viterbi 1967) implementedas a dynamic programming algorithm, as described in Section 4.2.
The most likelyplacement of word boundaries thus computed is committed to before the algorithmconsiders the next utterance presented.
Committing to a segmentation i volves learn-ing unigram, bigram, and trigram frequencies, as well as phoneme frequencies, fromthe inferred words.
These are used to update the respective tables.To account for effects that any specific ordering of input utterances may have onthe segmentations that are output, the performance of the algorithm is averaged over1000 runs, with each run receiving as input a random permutation of the input corpus.4.1 The input corpusThe corpus, which is identical to the one used by Brent (1999), consists of orthographictranscripts made by Bernstein-Ratner (1987) from the CHILDES collection (MacWhin-354Venkataraman Word Discovery in Transcribed SpeechTable 1Twenty randomly chosen utterances from the input corpus with their orthographic transcripts.See the appendix for a list of the ASCII representations of the phonemes.Phonemic Transcription Orthographic English texthQ sIli 6v mi1Uk D*z D6 b7 wiT hIz h&t9 TINk 9 si 6nADR bUktuDis wAnr9t WEn De wOkhuz an D6 tE16fon &llssit dQnk&n yu rid It tu D6 dOgiD*du yu si him h(1Ukyu want It InW* did It go&nd WAt # Dozh9 m6rioke Its 6 cIky& 1Uk WAt yu didoketek It QtHow silly of meLook, there's the boy with his hatI think I see another bookTwoThis oneRight when they walkWho's on the telephone, Alice?Sit downCan you feed it to the doggie?ThereDo you see him here?LookYou want it inWhere did it go?And what are those?Hi MaryOkay it's a chickYeah, look what you didOkayTake it outney and Snow 1985).
The speakers in this study were nine mothers peaking freely totheir children, whose ages averaged 18 months (range 13-21).
Brent and his colleaguestranscribed the corpus phonemically (using the ASCII phonemic representation in theappendix to this paper) ensuring that the number of subjective judgments in the pro-nunciation of words was minimized by transcribing every occurrence of the sameword identically.
For example, "look", "drink", and "doggie" were always transcribed"lUk", "driNk", and "dOgi" regardless of where in the utterance they occurred andwhich mother uttered them in what way.
Thus transcribed, the corpus consists of atotal of 9790 such utterances and 33,399 words, and includes one space after eachword and one newline after each utterance.
For purposes of illustration, Table 1 liststhe first 20 such utterances from a random permutation of this corpus.It should be noted that the choice of this particular corpus for experimentation ismotivated purely by its use in Brent (1999).
As has been pointed out by reviewers of anearlier version of this paper, the algorithm is equally applicable to plain text in Englishor other languages.
The main advantage of the CHILDES corpus is that it allowsfor ready comparison with results hitherto obtained and reported in the literature.Indeed, the relative performance of all the algorithms discussed is mostly unchangedwhen tested on the 1997 Switchboard telephone speech corpus with disfluency eventsremoved.4.2 AlgorithmThe dynamic programming algorithm finds the most probable word sequence for eachinput utterance by assigning to each segmentation a score equal to its probability, andcommitting to the segmentation with the highest score.
In practice, the implementationcomputes the negative logarithm of this score and thus commits to the segmentationwith the least negative logarithm of its probability.
The algorithm for the unigram355Computational Linguistics Volume 27, Number 3BEGINInput (by ref) utterance u\[O..n\] where u\[i\] are the characters in it.bestSegpoint := n;bestScore := evalWord(u\[O..n\]);for i from 0 to n-l; dosubUtterance := copy(u\[O..i\]) ;word := copy(u\[i+l..n\]);score := evalUtterance(subUtterance) + evalWord(word);if (score < bestScore); thenbestScore = score;bestSegpoint := i;fidoneinsertWordBoundary (u, bestSegpoint)return bestScore ;ENDFigure 1.
Algorithm: evalUtteranceRecursive optimization algorithm to find the best segmentation f an input utterance using theunigram language model described in this paper.BEGINInput (by reference) word w\[O..k\] where w\[i\] are the phonemes in it.score = O;if L.frequency(word) =-- O; then {escape = L. size () / (L. size () +L.
sumFrequencies ())P_O = phonemes, relat iveFrequency ( ' # ' ) ;score = -log(escape) -log(P O/(I-P_O));for each w\[i\]; doscore -= log (phonemes.
relat iveFrequency (w \[i\] ) ) ;done} else {P w = L.frequency(w)/(L.size()+L.sumFrequencies()) ;score = -log(P_w);}return score;ENDFigure 2.
Function: evalWordThe function to compute - logP(w) of an input word w. L stands for the lexicon object.
If theword is novel, then it backs off to using a distribution over the phonemes in the word.language model is presented in recursive form in Figure 1 for readability.
The actualimplementation, however, used an iterative version.
The algorithm to evaluate theback-off probability of a word is given in Figure 2.
Algorithms for bigram and trigramlanguage models are straightforward extensions of that given for the unigram model.Essentially, the algorithm description can be summed up semiformally as follows: Foreach input utterance u, we evaluate very possible way of segmenting it as u = u' + wwhere u' is a subutterance from the beginning of the original utterance up to somepoint within it and w--the lexical difference between u and u'-- is treated as a word.The subutterance u' is itself evaluated recursively using the same algorithm.
The basecase for recursion when the algorithm rewinds is obtained when a subutterance cannotbe split further into a smaller component subutterance and word, that is, when itslength is zero.
Suppose for example, that a given utterance is abcde, where the lettersrepresent phonemes.
If seg(x) represents the best segmentation of the utterance x and356Venkataraman Word Discovery in Transcribed Speechword(x) denotes that x is treated as a word, then~ word(abcde)seg(a) + word(bcde)seg(abcde) = best of seg(ab) + word(cde)seg(abc) + word(de)seg(abcd) + word(e)The evalUtterance algorithm in Figure 1 does precisely this.
It initially assumes theentire input utterance to be a word on its own by assuming a single segmentation pointat its right end.
It then compares the log probability of this segmentation successivelyto the log probabilities of segmenting it into all possible subutterance-word pairs.The implementation maintains four separate tables internally, one each for uni-grams, bigrams, and trigrams, and one for phonemes.
When the procedure is initiallystarted, all the internal n-gram tables are empty.
Only the phoneme table is popu-lated with equipossible phonemes.
As the program considers each utterance in turnand commits to its best segmentation according to the evalUtterance algorithm, thevarious internal n-gram tables are updated correspondingly.
For example, after someutterance abcde is segmented into a bc de, the unigram table is updated to increment thefrequencies of the three entries a, bc, and de, each by 1, the bigram table to incrementthe frequencies of the adjacent bigrams a bc and bc de, and the trigram table to incre-ment the frequency of the trigram a bc de.
4 Furthermore, the phoneme table is updatedto increment the frequencies of each of the phonemes in the utterance, including onesentinel for each word inferred.
5 Of course, incrementing the frequency of a currentlyunknown n-gram is equivalent to creating a new entry for it with frequency 1.
Notethat the very first utterance is necessarily segmented as a single word.
Since all then-gram tables are empty when the algorithm attempts to segment i , all probabilitiesare necessarily computed from the level of phonemes up.
Thus, the more words inthe segmentation f the first utterance, the more sentinel characters will be includedin the probability calculation, and so the lesser the corresponding segmentation prob-ability will be.
As the program works its way through the corpus, n-grams inferredcorrectly by virtue of their relatively greater preponderance ompared to noise tend todominate their respective n-gram distributions and thus dictate how future utterancesare segmented.One can easily see that the running time of the program is O(mn 2) in the totalnumber of utterances (m) and the length of each utterance (n), assuming an efficientimplementation f a hash table allowing nearly constant lookup time is available.
Sinceindividual utterances typically tend to be small, especially in child-directed speech asevidenced in Table 1, the algorithm practically approximates to a linear time procedure.A single run over the entire corpus typically completes in under 10 seconds on a 300MHz i686-based PC running Linux 2.2.5-15.Although the algorithm is presented as an unsupervised learner, a further exper-iment to test the responsiveness of each algorithm to training data is also describedhere: The procedure involves reserving for training increasing amounts of the inputcorpus, from 0% in steps of approximately 1% (100 utterances).
During the trainingperiod, the algorithm is presented with the correct segmentation f the input utter-ance, which it uses to update trigram, bigram, unigram, and phoneme frequencies as4 Amending the algorithm toinclude special markers for the start and end of each utterance was notfound to make a significant difference in its performance.5 In this context, see also Section 5.2 regarding experiments conducted to investigate different ways ofestimating phoneme probabilities.357Computational Linguistics Volume 27, Number 3required.
After the initial training segment of the input corpus has been considered,subsequent u terances are then processed in the normal way.4.3 ScoringIn line with the results reported in Brent (1999), three scores are reported - -  precision,recall, and lexicon precision.
Precision is defined as the proportion of predicted wordsthat are actually correct.
Recall is defined as the proportion of correct words that werepredicted.
Lexicon precision is defined as the proportion of words in the predictedlexicon that are correct.
In addition to these, the number of correct and incorrectwords in the predicted lexicon were computed, but they are not graphed here becauselexicon precision is a good indicator of both.Precision and recall scores were computed incrementally and cumulatively withinscoring blocks, each of which consisted of 100 utterances.
These scores were computedand averaged only for the utterances within each block scored, and thus represent theperformance of the algorithm only on the block scored, occurring in the exact contextamong the other scoring blocks.
Lexicon scores carried over blocks cumulatively.
Incases where the algorithm used varying amounts of training data, precision, recall, andlexical precision scores are computed over the entire corpus.
All scores are reportedas percentages.5.
ResultsFigures 3-5 plot the precision, recall, and lexicon precision of the proposed algorithmfor each of the unigram, bigram, and trigram models against he MBDP-1 algorithm.Although the graphs compare the performance of the algorithm with only one pub-lished result in the field, comparison with other related approaches i  implicitly avail-able.
Brent (1999) reports results of running the algorithms due to Elman (1990) andOlivier (1968), as well as algorithms based on mutual information and transitionalprobability between pairs of phonemes, over exactly the same corpus.
These are allshown to perform significantly worse than Brent's MBDP-1.
The random baseline al-gorithm in Brent (1999), which consistently performs with under 20% precision andrecall, is not graphed for the same reason.
This baseline algorithm offers an importantadvantage: It knows the exact number of word boundaries, even though it does notknow their locations.
Brent argued that if MBDP-1 performs as well as this randombaseline, then at the very least, it suggests that the algorithm is able to infer informa-tion equivalent to knowing the right number of word boundaries.
A second importantreason for not graphing the algorithms with worse performance was that the scaleon the vertical axis could be expanded significantly by their omission, thus allowingdistinctions between the plotted graphs to be seen more clearly.The plots originally given in Brent (1999) are over blocks of 500 utterances.
How-ever, because they are a result of running the algorithm on a single corpus, there is noway of telling whether the performance of the algorithm was influenced by any partic-ular ordering of the utterances in the corpus.
A further undesirable effect of reportingresults of a run on exactly one ordering of the input is that there tends to be too muchvariation between the values reported for consecutive scoring blocks.
To mitigate bothof these problems, we report averaged results from running the algorithms on 1000random permutations of the input data.
This has the beneficial side effect of allowingus to plot with higher granularity, since there is much less variation in the precisionand recall scores.
They are now clustered much closer to their mean values in eachblock, allowing a block size of 100 to be used to score the output.
These plots are thusmuch more readable than those obtained without such averaging of the results.358Venkataraman Word Discovery in Transcribed Speech7"570.~_ 65o)O_>~ 605550Figure 3i i i i i i i i r' l -g ram'  - -'2 -gram'  - - -  -'3 -gram'  - .
.
.
.
'MBDP '  ?
........, .
?
.
- .
, , ' " " ,  / ,  .
.
.
.
?',/-..
- .
.
/ " ,  , " "  ,,, , ' ,  , ,, ,".. _...., A,?
,/',, ,., , / " " .
.
.
/ ' , , ' ,  / / '  -I I I I I I I I10 20  30  40  50  60  70  80  90Scoring blocks100Averaged precision.
This is a plot of the segmentation precision over 100 utterance blocksaveraged over 1000 runs, each using a random permutation of the input corpus.
Precision isdefined as the percentage of identified words that are correct, as measured against he targetdata.
The horizontal axis represents he number of blocks of data scored, where each blockrepresents 100 utterances.
The plots show the performance of the 1-gram, 2-gram, 3-gram, andMBDP-1 algorithms.
The plot for MBDP-1 is not visible because it coincides almost exactlywith the plot for the 1-gram model.
Discussion of this level of similarity is provided inSection 5.5.
The performance of related algorithms due to Elman (1990), Olivier (1968) andothers is implicitly available in this and the following graphs since Brent (1999) demonstratesthat these all perform significantly worse than MBDP-1.One may object that the original transcripts carefully preserve the order of ut-terances directed at children by their mothers, and hence randomly permuting thecorpus would destroy the fidelity of the simulation.
However, as we argued, the per-mutation and averaging does have significant beneficial side effects, and if anything,it only eliminates from the point of view of the algorithms the important advan-tage that real children may be given by their mothers through a specific ordering ofthe utterances.
In any case, we have found no significant difference in performancebetween the permuted and unpermuted cases as far as the various algorithms areconcerned.In this context, it would be interesting to see how the algorithms would fare ifthe utterances were in fact favorably ordered, that is, in order of increasing length.Clearly, this is an important advantage for all the algorithms concerned.
Section 5.3presents the results of an experiment based on a generalization of this situation, whereinstead of ordering the utterances favorably, we treat an initial portion of the corpusas a training component, effectively giving the algorithms free word boundaries aftereach word.359Computational Linguistics Volume 27, Number 3757065I Ig~ 6o <5550Figure 4// ,'b i' l -g ram'  - -'2-gram' - .
.
.
.
'3-gram' ~ ....'MBDP ...........-,  .
, / " , / " ' - " "I I I I I I I I10 20 30 40 50 60 70 80 90 100Scoring blocksAveraged recall over 1000 runs, each using a random permutation of the input corpus.o =e,0.
.. i /0~75706560555045403530i i' l -g ram'  - -'2-gram' - .
.
.
.
'3-gram''MBDP'25 I I I i i I I I i10 20 30 40 50 60 70 80 90 100Scoring blocksFigure 5Averaged lexicon precision over 1000 runs, each using a random permutation of the inputcorpus.360Venkataraman Word Discovery in Transcribed Speech5.1 DiscussionClearly, the performance of the present model is competitive with MBDP-1 and, as aconsequence, with other algorithms evaluated in Brent (1999).
However, note that themodel proposed in this paper has been developed entirely along conventional linesand has not made the somewhat radical assumption that the entire observed corpusis a single event in probability space.
Assuming that the corpus consists of a singleevent, as Brent does, requires the explicit calculation of the probability of the lexiconin order to calculate the probability of any single segmentation.
This calculation isa nontrivial task since one has to sum over all possible orders of words in L. Thisfact is recognized in Brent (1999, Appendix), where the expression for P(L) is derivedas an approximation.
One can imagine then that it would be correspondingly moredifficult to extend the language model in Brent (1999) beyond the case of unigrams.
Inpractical terms, recalculating lexicon probabilities before each segmentation i creasesthe running time of an implementation of the algorithm.
Although all the algorithmsdiscussed tend to complete within one minute on the reported corpus, MBDP-I'srunning time is quadratic in the number of utterances, while the language modelspresented here enable computation in almost linear time.
The typical running timeof MBDP-1 on the 9790-utterance orpus averages around 40 seconds per run on a300 MHz i686 PC while the 1-gram, 2-gram, and 3-gram models average around 7, 10,and 14 seconds, respectively.Furthermore, the language models presented in this paper estimate probabilitiesas relative frequencies, using commonly used back-off procedures, and so they donot assume any priors over integers.
However, MBDP-1 requires the assumption oftwo distributions over integers, one to pick a number for the size of the lexicon andanother to pick a frequency for each word in the lexicon.
Each is assumed such thatthe probability of a given integer P(i) is given by 6 ~-~i2.
We have since found someevidence suggesting that the choice of a particular prior does not offer any significantadvantage over the choice of any other prior.
For example, we have tried runningMBDP-1 using P(i) = 2 -i and still obtained comparable results.
It should be noted,however, that no such subjective prior needs to be chosen in the model presented inthis paper.The other important difference between MBDP-1 and the present model is thatMBDP-1 assumes a uniform distribution over all possible word orders.
That is, in acorpus that contains nk distinct words such that the frequency in the corpus of the ithdistinct word is given byfk(i), the probability of any one ordering of the words in thecorpus isI-ITk ilk(i)!k~because the number of unique orderings is precisely the reciprocal of the above quan-tity.
Brent (1999) mentions that there may well be efficient ways of using n-gramdistributions in the MBDP-1 model.
The framework presented in this paper is a for-mal statement of a model that lends itself to such easy n-gram extendibility using theback-off scheme proposed here.
In fact, the results we present are direct extensions ofthe unigram model into bigrams and trigrams.In this context, an intriguing feature of the results is worth discussing here.
Notethat while, with respect to precision, the 3-gram model is better than the 2-gram model,which in turn is better than the 1-gram model, with respect to recall, their performanceis exactly the opposite.
A possible xplanation of this behavior is as follows: Since the3-gram model places greatest emphasis on word triples, which are relatively less fre-quent than words and word pairs, it has, of all the models, the least evidence available361Computational Linguistics Volume 27, Number 3to infer word boundaries from the observed ata.
Even though back-off is performedfor bigrams when a trigram is not found, there is a cost associated with such backingoff--this is the extra fractional factor N3/(N3 + $3) in the calculation of the segmen-tation's probability.
Consequently, the 3-gram model is the most conservative in itspredictions.
When it does infer a word boundary, it is likely to be correct.
This con-tributes to its relatively higher precision since precision is a measure of the proportionof inferred boundaries that were correct.
More often than not, however, when the3-gram model does not have enough evidence to infer words, it simply outputs thedefault segmentation, which is a single word (the entire utterance) instead of morethan one incorrectly inferred one.
This contributes to its poorer recall since recall isan indicator of the number of words the model fails to infer.
Poorer lexicon precisionis likewise explained.
Because the 3-gram model is more conservative, it infers newwords only when there is strong evidence for them.
As a result many utterances areinserted as whole words into its lexicon, thereby contributing to decreased lexiconprecision.
The framework presented here thus provides a systematic way of tradingoff precision against recall or vice-versa.
Models utilizing higher-order n-grams givebetter ecall at the expense of precision.5.2 Estimation of phoneme probabilitiesBrent (1999, 101) suggests that it might be worthwhile to study whether learningphoneme probabilities from distinct lexical entries yields better esults than learningthese probabilities from the input corpus.
That is, rather than inflating the probabilityof the phoneme "th" in the by the preponderance of the and the-like words in actualspeech, it is better to control it by the number of such distinct words.
Presented beloware an initial analysis and experimental results in this regard.Assume the existence of some function ~x: N ~ N that maps the size, n, of acorpus C, onto the size of some subset X of C we may define.
If this subset X = C,then ~c is the identity function, and if X = L is the set of distinct words in C, wehave q~L(n) = ILl.Let lx be the average number of phonemes per word in X and let E~x be theaverage number of occurrences of phoneme a per word in X.
Then we may estimatethe probability of an arbitrary phoneme a from X as follows.P(a J X)C(a IX)~,  C(ai I X)E~x~x(N)IxqJx(N)where, as before, C(a I X) is the count function that gives the frequency of phoneme ain X.
If ~x is deterministic, we can then writeP(a I X) = Eax (10)IxOur experiments suggest that EaL ~ Eac and that 1L ~ lC.
We are thus led to suspectthat estimates hould be roughly the same regardless of whether probabilities areestimated from L or C. This is indeed borne out by the results we present below.
Ofcourse, this is true only if there exists, as we assumed, some deterministic function ~Land this may not necessarily be the case.
There is, however, some evidence that thenumber of distinct words in a corpus can be related to the total number of words in362Venkataraman Word Discovery in Transcribed Speech1800 , , , , , , , , ,'1 -g ram''2 -gram'  - .
.
.
.
.
.'
3 -g ram'  - .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1600  " 'MBDP '  - ............ .
.
.
.
.
.
.
- .
.
.
.
.
'Ac tua l '  - .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.k*sqr t (N)  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.1400  .
............... ~1000 .
.
J  .
.
.
.
.
- " .
.
.
.800 " /  '600400 ," .
""2000 \[ I I I I I I I I I0 10  20  30  40  50  60  70  80  90  100Number  o f  words  in corpus  as  percentage  o f  to ta lFigure 6Plot shows the rate of growth of the lexicon with increasing corpus size as percentage of totalsize.
Actual is the actual number of distinct words in the input corpus.
1-gram, 2-gram, 3-gramand MBDP plot the size of the lexicon as inferred by each of the algorithms.
It is interestingthat the rates of lexicon growth are roughly similar to each other regardless of the algorithmused to infer words and that they may all potentially be modeled by a function such as kv~where N is the corpus size.the corpus in this way.
In Figure 6 the rate of lexicon growth is plotted against theproportion of the corpus size considered.
The values for lexicon size were collectedusing the Unix filtercat $* I t r  ' ' \ \O l2 lawk '{print (LE$O\]++)?
v : ++v;}'and smoothed by averaging over 100 runs, each on a separate permutation of theinput corpus.
The plot strongly suggests that the lexicon size can be approximated bya deterministic function of the corpus size.
It is interesting that the shape of the plotis roughly the same regardless of the algorithm used to infer words, suggesting thatall the algorithms egment word-like units that share at least some statistical propertieswith actual words.Table 2 summarizes our empirical findings in this regard.
For each model--namely,1-gram, 2-gram, 3-gram and MBDP- I - -we test all three of the following possibilities:1.
Always use a uniform distribution over phonemes.2.
Learn the phoneme distribution from the lexicon.3.
Learn the phoneme distribution from the corpus, that is, from all words,whether distinct or not.363Computational Linguistics Volume 27, Number 3Table 2Summary of results from each of the algorithms for each of the following cases:Lexicon-Phoneme probabilities estimated from the lexicon, Corpus-Phoneme probabilitiesestimated from input corpus and Uniform-Phoneme probabilities assumed uniform andconstant.LexiconCorpusUniformLexiconCorpusUniformPrecision1-gram 2-gram 3-gram MBDP67.7 68.08 68.02 6766.25 66.68 68.2 66.4658.08 64.38 65.64 57.15Recall1-gram 2-gram 3-gram MBDP70.18 68.56 65.07 69.3969.33 68.02 66.06 69.565.6 69.17 67.23 65.07Lexicon Precision1-gram 2-gram 3-gram MBDPLexicon 52.85 54.45 47.32 53.56Corpus 52.1 54.96 49.64 52.36Uniform 41.46 52.82 50.8 40.89The row labeled Lexicon lists scores on the entire corpus from a program thatlearned phoneme probabilities from the lexicon.
The row labeled Corpus lists scoresfrom a program that learned these probabilities from the input corpus, and the rowlabeled Uniform lists scores from a program that just assumed uniform phoneme prob-abilities throughout.
While the performance is clearly seen to suffer when a uniformdistribution over phonemes i assumed, whether the distribution is estimated fromthe lexicon or the corpus does not seem to make any significant difference.
Theseresults lead us to believe that, from an empirical point of view, it really does notmatter whether phoneme probabilities are estimated from the corpus or the lexicon.Intuitively, however, it seems that the right approach ought to be one that estimatesphoneme frequencies from the corpus data since frequent words ought to have agreater influence on the phoneme distribution than infrequent ones.5.3 Respons iveness  to trainingIt is interesting to compare the responsiveness of the various algorithms to the effectof training data.
Figures 7-8 plot the results (precision and recall) over the wholeinput corpus, that is, blocksize = cxD, as a function of the initial proportion of thecorpus reserved for training.
This is done by dividing the corpus into two segments,with an initial training segment being used by the algorithm to learn word, bigram,trigram, and phoneme probabilities, and the second segment actually being used asthe test data.
A consequence of this is that the amount of data available for testingbecomes progressively smaller as the percentage r served for training grows.
So thesignificance of the test diminishes correspondingly.
We can assume that the plots ceaseto be meaningful and interpretable when more than about 75% (about 7500 utterances)of the corpus is used for training.
At 0%, there is no training information for any364Venkataraman Word Discovery in Transcribed Speech100 i i I I I I I I I' l-gram' - -'2-gram''3-gram''MBDP'9590o~o.858075 I I I I I i i I I0 10 20 30 40 50 60 70 80 90 100Percentage of input used for trainingFigure 7Responsiveness of the algorithm to training information.
The horizontal axis represents theinitial percentage of the data corpus that was used for training the algorithm.
This graphshows the improvement in precision with training size.100989694~ 02 rr9088I i F I l I l i I't-gram''2-gram' - .
.
.
.
____.~--- --~-.~'3-gram' - .....I I I I I50 60 70 80 90Percentage of input used for training86I I I I0 I 0 20 30 40Figure 8Improvement in recall with training size.100365Computational Linguistics Volume 27, Number 3Table 3Errors in the output of a fully trained 3-gram language model.
Erroneous egmentations areshown in boldface.# 3-gram output Target3482 ... in the doghouse ... in the dog house5572 aclock a clock5836 that's alright that's all right7602 that's right it's a hairbrush that's right it's a hair brushalgorithm and the scores are identical to those reported earlier.
We increase the amountof training data in steps of approximately 1% (100 utterances).
For each training setsize, the results reported are averaged over 25 runs of the experiment, each over aseparate random permutation of the corpus.
As before, this was done both to correctfor ordering idiosyncrasies, and to smooth the graphs to make them easier to interpret.We interpret Figures 7 and 8 as suggesting that the performance of all algorithmsdiscussed here can be boosted significantly with even a small amount of training.
Itis noteworthy and reassuring to see that, as one would expect from results in compu-tational learning theory (Haussler 1988), the number of training examples required toobtain a desired value of precision p, appears to grow with 1/(1 - p).
The intriguingreversal in the performance of the various n-gram models with respect o precisionand recall is again seen here and the explanation for this too is the same as discussedearlier.
We further note, however, that the difference in performance between the dif-ferent models tends to narrow with increasing training size; that is, as the amountof evidence available to infer word boundaries increases, the 3-gram model rapidlycatches up with the others in recall and lexicon precision.
It is likely, therefore, thatwith adequate training data, the 3-gram model might be the most suitable one to use.The following experiment lends support o this conjecture.5.4 Fully trained algorithmsThe preceding discussion raises the question of what would happen if the percentageof input used for training was extended to the limit, that is, to 100% of the corpus.
Thisprecise situation was tested in the following way: The entire corpus was concatenatedonto itself; the models were then trained on the first half and tested on the secondhalf of the corpus thus augmented.
Although the unorthodox nature of this proceduredoes not allow us to attach all that much significance to the outcome, we neverthelessfind the results interesting enough to warrant some mention, and we thus discuss herethe performance ofeach of the four algorithms on the test segment of the input corpus(the second half).
As one would expect from the results of the preceding experiments,the trigram language model outperforms all others.
It has a precision and recall of100% on the test input, except for exactly four utterances.
These four utterances areshown in Table 3, retranscribed into plain English.Intrigued as to why these rrors occurred, we examined the corpus, only to find er-roneous transcriptions in the input, dog house is transcribed as a single word "dOghQs"in utterance 614, and as two words elsewhere.
Likewise, o'clock is transcribed "6klAk"in utterance 5917, alright is transcribed "Olr9t" in utterance 3937, and hair brush istranscribed "h*brAS" in utterances 4838 and 7037.
Elsewhere in the corpus, these aretranscribed as two words.The erroneous egmentations in the output of the 2-gram language model areshown in Table 4.
As expected, the effect of reduced history is apparent from an in-366Venkataraman Word Discovery in Transcribed SpeechTable 4Errors in the output of a fully trained 2-gram language model.
Erroneous egmentations areshown in boldface.# 2-gram output Target614 you want the dog house you want the doghouse3937 thats all right that's alright5572 a clock a clock7327 look a hairbrush look a hair brush7602 that's right its a hairbrush that's right its a hair brush7681 hairbrush hair brush7849 it's called a hairbrush it's called a hair brush7853 hairbrush hair brushcrease in the total number of errors.
However, it is interesting to note that while the3-gram model incorrectly segmented an incorrect ranscription (utterance 5836) that'sall right to produce that's alright, the 2-gram model incorrectly segmented a correcttranscription (utterance 3937) that's alright to produce that's all right.
The reason forthis is that the bigram that's all is encountered relatively frequently in the corpus andthis biases the algorithm toward segmenting the all out of alright when it followsthat's.
However, the 3-gram model is not likewise biased because, having encoun-tered the exact 3-gram that's all right earlier, there is no back-off to try bigrams at thisstage.Similarly, it is interesting that while the 3-gram model incorrectly segments theincorrectly transcribed og house into doghouse in utterance 3482, the 2-gram modelincorrectly segments the correctly transcribed oghouse into dog house in utterance 614.In the trigrarn model, - log P(houselthe, dog) = 4.8 and - log P(doglin, the) = 5.4,giving a score of 10.2 to the segmentation dog house.
However, due to the error intranscription, the trigram in the doghouse is never encountered in the training data,although the bigram the doghouse is.
Backing off to bigrams, - logP(doghouse l the  )is calculated as 8.1.
Hence the probability that doghouse is segmented as dog houseis less than the probability that it is a single word.
In the 2-gram model, however,- logP(dog\]the)P(houseldog ) = 3.7 + 3.2 = 6.9 while - logP(doghouse\[the) = 7.5,whence dog house is the preferred segmentation even though the training data con-tained instances of all three bigrams.
For errors in the output of a 1-gram model, seeTable 5.The errors in the output of Brent's fully trained MBDP-1 algorithm are not shownhere because they are identical to those produced by the 1-gram model except for oneutterance.
This single difference is the segmentation of utterance 8999, "lItL QtlEts"(little outlets), which the 1-gram model segmented correctly as "lItL QtlEts", but MBDP-1 segmented as "lItL Qt lets".
In both MBDP-1 and the 1-gram model, all four words,little, out, lets and outlets, are familiar at the time of segmenting this utterance.
MBDP-1assigns a score of 5.3+5.95 = 11.25 to the segmentation out + lets versus a score of 11.76to outlets.
As a consequence, out + lets is the preferred segmentation.
In the 1-gramlanguage model, the segmentation out + lets scores 5.31 + 5.97 = 11.28, whereas outletsscores 11.09.
Consequently, it selects outlets as the preferred segmentation.
The onlything we could surmise from this was either that this difference must have come aboutdue to chance (meaning that this may not have occurred if certain parts of the corpushad been different in any way) or else the interplay between the different elements inthe two models is too subtle to be addressed within the scope of this paper.367Computational Linguistics Volume 27, Number 3Table 5Errors in the output of a fully trained 1-gram language model.# 1-gram output Target244 brush Al ice's hair brush Alice's hair503 you're in to distraction ..- you're into distraction ...1066 you my trip it you might rip it1231 this is little doghouse this is little dog house1792 stick it on to there stick it onto there3056 ... so he doesn't run in to ... so he doesn't run into3094 ... to be in the highchair --.
to be in the high chair3098 ... for this highchair ... for this high chair3125 ... already .
.
.
.
.
.
all ready.
.
-3212 -.. could talk in to it ... could talk into it3230 can heel I down on them can he lie down on them3476 that's a doghouse that's a dog house3482 ... in the doghouse ... in the dog house3923 ... when it's nose ... when it snows3937 that's all right that's alright4484 its about mealtime s its about meal times5328 tell him to way cup tell him to wake up5572 o'clock a clock5671 where's my little hairbrush where's my little hair brush6315 that's a nye that's an i6968 okay mommy take seat okay mommy takes it7327 look a hairbrush look a hair brush7602 that's right its a hairbrush that's right its a hair brush7607 go along way to find it today go a long way to find it today7676 morn put sit mom puts it7681 hairbrush hair brush7849 its called a hairbrush its called a hair brush7853 hairbrush hair brush8990 ... in the highchair ...  in the high chair8994 for baby's a nice highchair for baby's a nice high chair8995 that's like a highchair that's right that's like a high chair that's right9168 he has along tongue he has a long tongue9567 you wanna go in the highchair you wanna go in the high chair9594 along red tongue a long red tongue9674 doghouse dog house9688 highchair again high chair again9689 ... the highchari --.
the high chair9708 I have along tongue I have a long tongue5.5 Similarit ies between MBDP-1 and the 1-gram Mode lThe similarit ies between the outputs  of MBDP-1 and the 1-gram mode l  are so greatthat we suspect hey may be captur ing essential ly the same nuances  of the domain .A l though Brent (1999) explicit ly states that probabi l i t ies are not  est imated for words,it turns  out that imp lementat ions  of MBDP-1 do end up  hav ing  the same effect asest imat ing probabi l i t ies from relative frequencies as the 1-gram mode l  does.
The relativeprobability of a famil iar word  is g iven in Equat ion 22 of Brent (1999) as2368Venkataraman Word Discovery in Transcribed Speechwhere k is the total number of words and fk(k) is the frequency at that point in seg-mentation of the kth word.
It effectively approximates to the relative frequencyfk(k)kas fk(k) grows.
The 1-gram language model of this paper explicitly claims to use thisspecific estimator for the unigram probabilities.
From this perspective, both MBDP-1 and the 1-gram model tend to favor the segmenting out of familiar words thatdo not overlap.
It is interesting, however, to see exactly how much evidence achneeds before such segmentation is carried out.
In this context, the author recalls ananecdote recounted by a British colleague who, while visiting the USA, noted that thepopulace in the vicinity of his institution had grown up thinking that Damn Britishwas a single word, by virtue of the fact that they had never heard the latter word inisolation.
We test this particular scenario here with both algorithms.
The programs arefirst presented with the utterance damnbritish.
Having no evidence to infer otherwise,both programs assume that damnbritish is a single word and update their lexiconsaccordingly.
The interesting question ow is exactly how many instances of the wordbritish in isolation each program would have to see before being able to successfullysegment a subsequent presentation of damnbritish correctly.Obviously, if the word damn is also unfamiliar, there will never be enough evidenceto segment i out in favor of the familiar word damnbritish.
Hence each program ispresented next with two identical utterances, damn.
Unless two such utterances arepresented, the estimated probabilities of the familiar words damn and damnbritish willbe equal; and consequently, the probability of any segmentation f damnbritish thatcontains the word damn will be less than the probability of damnbritish considered asa single word.At this stage, we present each program with increasing numbers of utterancesconsisting solely of the word british followed by a repetition of the very first utterance--damnbritish.
We find that MBDP-1 needs to see the word british on its own threetimes before having enough evidence to disabuse itself of the notion that damnbritishis a single word.
In comparison, the 1-gram model is more skeptical.
It needs to seethe word british on its own seven times before committing to the right segmentation.To illustrate the inherent simplicity of the model presented here, we can show that it iseasy to predict his number analytically from the 1-gram model.
Let x be the numberof instances of british required.
Then using the discounting scheme described, we haveP(damnbritish) = 1/(x + 6)P(damn) = 2/(x+6)P(british) = x/(x + 6)andWe seek an x for which P(damn)P(british) > P(damnbritish).
Thus, we get2x/(x + 6) 2 > 1/(x q- 6) ~ x > 6The actual scores for MBDP-1 when presented with damnbritish for a second time are- logP(damnbritish) = 2.8 and - logP(D&m) -logP(brItIS) = 1.8 + 0.9 = 2.7.
Forthe 1-gram model, - log P(damnbritish) = 2.6 and - log P(D&m) - log P(brItIS) =1.9 + 0.6 ~ 2.5.
Note, however, that skepticism in this regard is not always a badattribute.
It is desirable for the model to be skeptical in inferring new words becausea badly inferred word will adversely influence future segmentation accuracy.369Computational Linguistics Volume 27, Number 36.
SummaryIn summary, we have presented a formal model of word discovery in continuousspeech.
The main advantages of this model over that of Brent (1999) are: First, thepresent model has been developed entirely by direct application of standard tech-niques and procedures in speech processing.
Second, it makes few assumptions aboutthe nature of the domain and remains conservative as far as possible in its develop-ment.
Finally, the model can be easily extended to incorporate more historical detail.This is clearly evidenced by the extension of the unigram model to handle bigramsand trigrams.
Empirical results from experiments suggest hat the algorithm performscompetitively with alternative unsupervised algorithms proposed for inferring wordsfrom continous peech.
We have also carried out and reported results from experimentsto determine whether particular ways of estimating phoneme (or letter) probabilitiesmay be more suitable than others.Although the algorithm is originally presented as an unsupervised learner, wehave shown the effect that training data has on its performance.
It appears that the3-gram model is the most responsive to training information with regard to segmen-tation precision, obviously by virtue of the fact that it keeps more knowledge from theutterances presented.
Indeed, we see that a fully trained 3-gram model performs with100% accuracy on the test set.
Admittedly, the test set in this case was identical to thetraining set, but we should keep in mind that we were keeping only limited history--namely 3-grams--and a significant number of utterances in the input corpus (4023utterances) were four words or more in length.
Thus, it is not completely insignificantthat the algorithm was able to perform this well.7.
Future workWe are presently working on the incorporation into the model of more complexphoneme distributions, such as the biphone and triphone distributions.
Some pre-liminary results we have obtained in this regard appear to be encouraging.With regard to estimation of word probabilities, a fruitful avenue we are exploringinvolves modification of the model to address the sparse data problem using interpo-lation such thatP(wi \[ Wi--a, Wi--1) = /~3f(wi I Wi--2'Wi--1) ?
,~2f(wi l Wi--1) ?
41f(wi)where the positive coefficients atisfy 41 + 42 ?
43 = 1 and can be derived so as tomaximize P(W).Taking the lead from Brent (1999), attempts to model more complex distributionsfor unigrams uch as those based on template grammars, as well as the systematic n-corporation of prosodic, stress, and phonotactic constraint information into the model,are both subjects of current interest.
Unpublished results already obtained suggestthat biasing the segmentation such that every word must have at least one vowel init dramatically increases egmentation precision from 67.7% to 81.8%, and imposinga constraint hat words can begin or end only with permitted clusters of consonantsincreases precision to 80.65%.
We are planning experiments to investigate models inwhich these properties can be learned in the same way as n-grams.Appendix: Inventory of PhonemesThe following tables list the ASCII representations of the phonemes used to transcribethe corpus into a form suitable for processing by the algorithms.370Venkataraman Word Discovery in Transcribed SpeechConsonants VowelsASCII Example ASCII Examplep pan I bitb ban E betm man & batt tan A butd dam a hotn nap O lawk can U putg go 6 herN sing i beetf fan e baitv van u bootT thin o boatD than 9 buys sand Q boutz zap 7 boyS shipZ pleasureh hatc chipG gel1 lapr rapy yetw wallW whenL bottleM rhythmbuttonVowel + rASCII Example3 birdR butter# arm% horn?
air( ear) lureAcknowledgmentsThe author thanks Michael Brent forstimulating his interest in the area.
Thanksare also due to Koryn Grant for verifyingthe results presented here.
Eleanor OldsBatchelder and Andreas Stolcke gave manyconstructive comments and useful pointersduring the preparation of a revised versionof this paper.
The "Damn British" anecdoteis due to Robert Linggard.
Judith Lee at SRIedited the manuscript to remove manytypographical nd stylistic errors.
Inaddition, the author is very grateful toanonymous reviewers of an initial version,particularly "B" and "D", whoseencouragement and constructive criticismhelped significantly.ReferencesAndo, R. K. and Lillian Lee.
1999.Unsupervised statistical segmentation ofJapanese Kanji strings.
Technical ReportTR99-1756, Cornell University, Ithaca, NY.Batchelder, Eleanor Olds.
1997.Computational evidence~or the use offrequency information i  discovery of theinfant's first lexicon.
Unpublished Ph.D.dissertation, City University of New York,New York, NY.Bernstein-Ratner, N. 1987.
The phonology ofparent child speech.
In K. Nelson and A.van Kleeck, editors, Children's Language,volume 6.
Erlbaum, Hillsdale, NJ.Brent, Michael R. 1999.
An efficientprobabilistically sound algorithm forsegmentation and word discovery.Machine Learning, 34:71-105.Brent, Michael R. and Timothy A.Cartwright.
1996.
Distributional regularityand phonotactics are useful forsegmentation.
Cognition, 61:93-125.Christiansen, M. H., J. Allen, and M.Seidenberg.
1998.
Learning to segmentspeech using multiple cues: Aconnectionist model.
Language andCognitive Processes, 13:221-268.Cutler, Anne and D. M. Carter.
1987.Predominance of strong initial syllables inthe English vocabulary.
Computer Speechand Language, 2:133-142.de Marcken, C. 1995.
Unsupervisedacquisition of a lexicon from continuousspeech.
Technical Report AI Memo No.1558, Massachusetts Institute ofTechnology, Cambridge, MA.371Computational Linguistics Volume 27, Number 3Elman, J. L. 1990.
Finding structure in time.Cognitive Science, 14:179-211.Haussler, David.
1988.
Quantifyinginductive bias: AI learning algorithmsand Valiant's learning framework.Artificial Intelligence, 36:177-221.Jelinek, F. 1997.
Statistical Methods for SpeechRecognition.
MIT Press, Cambridge, MA.Jusczyk, Peter W. 1997.
The Discovery ofSpoken Language.
MIT Press, Cambridge,MA.Jusczyk, Peter W., Anne Cutler, and N.Redanz.
1993.
Preference for predominantstress patterns of English words.
ChildDevelopment, 64:675-687.Jusczyk, Peter W. and E. A. Hohne.
1997.Infants' memory for spoken words.Science, 27:1984-1986.Katz, Slava M. 1987.
Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer.
IEEE Transactions on Acoustics,Speech and Signal Processing,ASSP-35 (3) :400-401.MacWhinney, Brian and C. Snow.
1985.
Thechild language data exchange system.Journal of Child Language, 12:271-296.Mattys, Sven L. and Peter W. Jusczyk.
1999.Phonotactic and prosodic effects on wordsegmentation in infants.
CognitivePsychology, 38:465-494.Olivier, D. C. 1968.
Stochastic grammars andlanguage acquisition mechanisms.Unpublished Ph.D. dissertation, HarvardUniversity, Cambridge, MA.Saffran, Jennifer R., E. L. Newport, andR.
N. Aslin.
1996.
Word segmentation:The role of distributional cues.
Journal ofMemory and Language, 35:606-621.Viterbi, Andrew J.
1967.
Error bounds forconvolutional codes and anasymptotically optimal decodingalgorithm.
IEEE Transactions on InformationTheory, IT-13:260-269.Witten, Ian H. and Timothy C. Bell.
1991.The zero-frequency problem: Estimatingthe probabilities of novel events inadaptive text compression.
IEEETransactions on Information Theory,37(4):1085-1091.Zimin, W. and G. Tseng.
1993.
Chinese textsegmentation for text retrieval problemsand achievements.
JASIS, 44(9):532-542.372
