Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1292?1302,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsImage Description using Visual Dependency RepresentationsDesmond ElliottSchool of InformaticsUniversity of Edinburghd.elliott@ed.ac.ukFrank KellerSchool of InformaticsUniversity of Edinburghkeller@inf.ed.ac.ukAbstractDescribing the main event of an image in-volves identifying the objects depicted andpredicting the relationships between them.Previous approaches have represented imagesas unstructured bags of regions, which makesit difficult to accurately predict meaningfulrelationships between regions.
In this pa-per, we introduce visual dependency represen-tations to capture the relationships betweenthe objects in an image, and hypothesize thatthis representation can improve image de-scription.
We test this hypothesis using anew data set of region-annotated images, as-sociated with visual dependency representa-tions and gold-standard descriptions.
We de-scribe two template-based description gener-ation models that operate over visual depen-dency representations.
In an image descrip-tion task, we find that these models outper-form approaches that rely on object proxim-ity or corpus information to generate descrip-tions on both automatic measures and on hu-man judgements.1 IntroductionHumans are readily able to produce a description ofan image that correctly identifies the objects and ac-tions depicted.
Automating this process is useful forapplications such as image retrieval, where users cango beyond keyword-search to describe their infor-mation needs, caption generation for improving theaccessibility of existing image collections, story il-lustration, and in assistive technology for blind andpartially sighted people.
Automatic image descrip-tion presents challenges on a number of levels: rec-ognizing the objects in an image and their attributesare difficult computer vision problems; while deter-mining how the objects interact, which relationshipshold between them, and which events are depictedrequires considerable background knowledge.Previous approaches to automatic descriptiongeneration have typically tackled the problem us-ing an object recognition system in conjunction witha natural language generation component based onlanguage models or templates (Kulkarni et al 2011;Li et al 2011).
Some approaches have utilised thevisual attributes of objects (Farhadi et al 2010),generated descriptions by retrieving the descriptionsof similar images (Ordonez et al 2011; Kuznetsovaet al 2012), relied on an external corpus to pre-dict the relationships between objects (Yang et al2011), or combined sentence fragments using a tree-substitution grammar (Mitchell et al 2012).A common aspect of existing work is that an im-age is represented as a bag of image regions.
Bagsof regions encode which objects co-occur in an im-age, but they are unable to express how the regionsrelate to each other, which makes it hard to describewhat is happening.
As an example, consider Fig-ure 1a, which depicts a man riding a bike.
If theman was instead repairing the bike, then the bag-of-regions representation would be the same, eventhough the image would depict a different action andwould have to be described differently.
This typeof co-occurrence of regions indicates the need for amore structured image representation; an image de-scription system that has access to structured repre-1292(a)A man is riding a bike down the road.A car and trees are in the background.
(b)ROOT bike car man road trees- --onaboveA man is riding a bike down the road.detnsubjauxrootdetdobjadvmoddetpobj(c)Figure 1: (a) Image with regions marked up: BIKE, CAR,MAN, ROAD, TREES; (b) human-generated image de-scription; (c) visual dependency representation express-ing the relationships between MAN, BIKE, and ROADaligned to the syntactic dependency parse of the first sen-tence in the human-generated description (b).sentations would be able to correctly infer the actionthat is taking place, such as the distinction betweenrepairing or riding a bike, which would greatly im-prove the descriptions it is able to generate.In this paper, we introduce visual dependency rep-resentations (VDRs) to represent the structure of im-ages.
This representation encodes the geometric re-lations between the regions of an image.
An ex-ample can be found in Figure 1c, which depicts theVDR for Figure 1a.
It encodes that the MAN is abovethe BIKE, and that the BIKE is on the ROAD.
Theserelationships make it possible to infer that the manis riding a bike down the road, which correspondsto the first sentence of the human-generated imagedescription in Figure 1b.In order to test the hypothesis that structured im-age representations are useful for description gener-ation, we present a series of template-based imagedescription models.
Two of these models are basedon approaches in the literature that represent imagesas bags of regions.
The other two models use vi-sual dependency representations, either on their ownor in conjunction with gold-standard image descrip-tions at training time.We find that descriptions generated using theVDR-based models are significantly better thanthose generated using bag-of-region models in au-tomatic evaluations using smoothed BLEU scoresand in human judgements.
The BLEU score im-provements are found at bi-, tri-, and four-gram lev-els, and humans rate VDR-based image descriptions1.2 points above the next-best model on a 1?5 scale.Finally, we also show that the benefit of the vi-sual dependency representation is maintained whenimage descriptions are generated from automaticallyparsed VDRs.
We use a modified version of theedge-factored parser of McDonald et al(2005) topredict VDRs over a set of annotated object regions.This result reaffirms the potential utility of this rep-resentation as a means to describe events in images.Note that throughout the paper, we work with gold-standard region annotations; this makes it possibleto explore the effect of structured image representa-tions independently of automatic object detection.2 Visual Dependency RepresentationIn analogy to dependency grammar for natural lan-guage syntax, we define Visual Dependency Gram-mar to describe the spatial relations between pairsof image regions.
A directed arc between two re-gions is labelled with the spatial relationship be-tween those regions, defined in terms of three ge-ometric properties: pixel overlap, the angle betweenregions, and the distance between regions.
Table 1presents a detailed explanation of the spatial rela-tionships defined in the grammar.A visual dependency representation of an imageis constructed by creating a directed acyclic graph1293X ?
?on YMore than 50% of the pixels of re-gion X overlap with region Y.1X??????
?surrounds YThe entirety of region X overlapswith region Y.X???
?beside YThe angle between the centroid ofX and the centroid of Y lies be-tween 315?
and 45?
or 135?
and225?.X?????
?opposite YSimilar to beside, but used whenthere X and Y are at opposite sidesof the image.X???
?above YThe angle between X and Y lies be-tween 225?
and 315?.X???
?below YThe angle between X and Y lies be-tween 45?
and 135?.X????
?infront YThe Z-plane relationship betweenthe regions is dominant.X????
?behind YIdentical to infront except X is be-hind Y in the Z-plane.Table 1: Visual Dependency Grammar defines eight re-lations between pairs of annotated regions.
To simplifyexplanation, all regions are circles, where X is the greyregion and Y is the white region.
All relations are consid-ered with respect to the centroid of a region and the anglebetween those centroids.
We follow the definition of theunit circle, in which 0?
lies to the right and a turn aroundthe circle is counter-clockwise.over the set of regions in an image using the spa-tial relationships in the Visual Dependency Gram-mar.
It is created from a region-annotated image anda corresponding image description by first identify-ing the central actor of the image.
The central actoris the person or object carrying out the depicted ac-tion; this typically corresponds to the subject of thesentence describing the image.
The region corre-sponding to the central actor is attached to the ROOTnode of the graph.
The remaining regions are thenattached based on their relationship with either theactor or the other regions in the image as they are1As per the PASCAL VOC definition of overlap in the objectdetection task (Everingham et al 2011).mentioned in the description.
Each arc introducedis labelled with one of the spatial relations definedin the grammar, or with no label if the region is notdescribed in relation to anything else in the image.As an example of the output of this annotationprocess, consider Figure 1a, its description in 1b,and its VDR in 1c.
Here, the MAN is the centralactor in the image, as he is carrying out the depictedaction (riding a bike).
The region corresponding toMAN is therefore attached to ROOT without a spa-tial relation.
The BIKE region is then attached to theMAN region using the???
?above relation and BIKE is at-tached to the ROAD with the ?
?on relation.
In the sec-ond sentence of the description, CAR and TREES arementioned without a relationship to anything else inthe image, so they are attached to the ROOT node.
Ifthese regions were attached to other regions, such asCAR???
?above ROAD then this would imply structure inthe image that is not conveyed in the description.2.1 DataOur data set uses the images from the PASCALVisual Object Classification Challenge 2011 actionrecognition taster competition (Everingham et al2011).
This is a closed-domain data set containingimages of people performing ten types of actions,such as making a phone call, riding a bike, and tak-ing a photo.
We annotated the data set in a three-stepprocess: (1) collect a description for each image;(2) annotate the regions in the image; and (3) create avisual dependency representation of the image.
Notethat Steps (2) and (3) are dependent on the image de-scription, as both the region labels and the relationsbetween them are derived from the description.2.2 Image DescriptionsWe collected three descriptions of each image in ourdata set from Amazon Mechanical Turk.
Workerswere asked to describe an image in two sentences.The first sentence describes the action in the image,the person performing the action and the region in-volved in the action; the second sentence describesany other regions in the image not directly involvedin the action.
An example description is given inFigure 1b.A total of 2,424 images were described by threeworkers each, resulting in a total of 7,272 image de-1294manwomanpersonpeople treeshorse girl wallboycomputerchildbookphonechairwindow grasscamerabicycle bikelaptopFrequency0100200300400500Figure 2: Top 20 annotated regions.scriptions.
The workers, drawn from those regis-tered in the US with a minimum HIT acceptance rateof 95%, described an average of 145 ?
93 images;they were encouraged to describe fewer than 300 im-ages each to ensure a linguistically diverse data set.They were paid $0.04 per image and it took on av-erage 67 ?
123 seconds to describe a single image.The average length of a description was 19.9 ?
6.5words in a range of 8?50 words.
Dependency parsesof the descriptions were produced using the MST-Parser (McDonald et al 2005) trained on sections2-21 of the WSJ portion of the Penn Treebank.2.3 Region AnnotationsWe trained two annotators to draw polygons aroundthe outlines of the regions in an image using the La-belMe annotation tool (Russell et al 2008).
Theregions annotated for a given image were limited tothose mentioned in the description paired with theimage.
Region annotation was performed on a sub-set of 341 images and resulted in a total of 5,034annotated regions with a mean of 4.19 ?
1.94 an-notations per image.
A total of 496 distinct labelswere used to label regions.
Figure 2 shows thedistribution of the top 20 region annotations in thedata; people-type regions are the most commonlyannotated regions.
Given the prevalence of labelsreferring to the same types of regions, we defined26 sets of equivalent labels to reduce label sparsity(e.g., BIKE was considered equivalent to BICYCLE).This normalization process reduced the size of theregion label vocabulary from 496 labels to 362 la-noneinfrontbesideabove onsurroundsbehindbelowoppositeFrequency05001000150020002500Figure 3: Distribution of the spatial relations.bels.
Inter-annotator agreement was 74.3% for re-gion annotations, this was measured by computingpolygon overlap over the annotated regions.2.4 Visual Dependency RepresentationsThe same two annotators were trained to constructgold-standard visual dependency representations forannotated image?description pairs.
The process forcreating a visual dependency representation of animage is described earlier in this section of the pa-per.
The 341 region-annotated images resulted in aset of 1,023 visual dependency representations.
Theannotated data set comprised a total of 5,748 spatialrelations, corresponding to a mean of 4.79 ?
3.51relations per image.
Figure 3 shows the distributionof spatial relation labels in the data set.
It can beseen that the majority of regions are attached to theROOT node, i.e., they have the relation label none.Inter-annotator agreement on a subset of the datawas measured at 84% agreement for labelled de-pendency accuracy and 95.1% for unlabelled depen-dency accuracy.
This suggests the task of generatingvisual dependency representations can be performedreliably by human annotators.
We induced an align-ment between the annotated region labels and wordsin the image description using simple lexical match-ing augmented with WordNet hyponym lookup.
SeeFigure 1c for an example of the alignments.3 Image Description ModelsWe present four template-based models for gener-ating image descriptions in this section.
Table 21295Regions VDRExternalCorpusParalleltextPROXIMITY XCORPUS X XSTRUCTURE X XPARALLEL X X XTable 2: The data available to each model at training time.presents an overview of the amount of informationavailable to each model at training time, rangingfrom only the annotated regions of an image to us-ing visual dependency representation of an imagealigned with the syntactic dependency representa-tion of its description.
At test time, all models haveaccess to image regions and their labels, and usethese to generate image descriptions.
Two of themodels also have access to VDRs at test time, al-lowing us to test the hypothesis that image structureis useful for generating good image descriptions.The aim of each model is to determine what ishappening in the image, which regions are impor-tant for describing it, and how these regions relate toeach other.
Recall that all our images depict actions,and that the gold-standard annotation was performedwith this in mind.
A good description therefore isone that relates the main actors depicted in the im-age to each other, typically through a verb; a mereenumeration of the regions in the image is not suffi-cient.
All models attempt to generate a two-sentencedescription, as per the gold standard descriptions.In the remainder of this section, we will use Fig-ure 1 as a running example to demonstrate the typeof language each model is capable of generating.
Allmodels share the set of templates in Table 3.3.1 PROXIMITYPROXIMITY is based on the assumption that peopledescribe the relationships between regions that arenear each other.
It has access to only the annotatedimage regions and their labels.Region?region relationships that are potentiallyrelevant for the description are extracted by calculat-ing the proximity of the annotated regions.
Here, oiis the subject region, o j is the object region, and si jis the spatial relationship between the regions.
LetT1 DT Oi AUX REL DT O j. T5?T2 There AUX also {DT Oi}|unrelated|i=1 in the image.T3 DT Oi AUX REL DT O j REL DT Ok. T5?T4 REL DT O j.T5 PRP AUX {REL DT Oi}|dependents|i=1 .Table 3: The language generation templates.R = {(oi, si j, o j), .
.
.}
be the set of possible region?region relationships found by calculating the near-est neighbour of each region in Euclidean space be-tween the centroids of the polygons that mark the re-gion boundaries.
The tuple with the subject closestto the centre of the image is used to describe what ishappening in the image, and the remaining regionsare used to describe the background.The first sentence of the description is realisedwith template T1 from Table 3. oi is the label ofthe subject region and o j is the label of the objectregion.
DT is a simple determiner chosen from {the,a}, depending on whether the region label is a pluralnoun; AUX is either {is, are}, depending on the num-ber of the region label; and REL is a word to describethe relationship between the regions.
For this model,REL is the spatial relationship between the centroidschosen from {above, below, beside}, depending onthe angle formed between the region centroids, us-ing the definitions in Table 1.
The second sentenceof the description is realised with template T2 overthe subjects oi in R that were not used in the firstsentence.
An example of the language generated is:(1) The man is beside the bike.
There is also aroad, a car, and trees in the image.With the exception of visual attributes to describesize, colour, or texture, this model is based on theapproach described by Kulkarni et al(2011).3.2 CORPUSThe biggest limitation of PROXIMITY is that regionsthat are near each other are not always in a rele-vant relationship for a description.
For example, inFigure 1, the BIKE and the CAR regions are near-est neighbours but they are unlikely to be describedas being in an relationship by a human annotator.The model CORPUS addresses this issue by using an1296external text corpus to determine which pairs of re-gions are likely to be in a describable relationship.Furthermore, CORPUS can generate verbs instead ofspatial relations between regions, leading to morehuman-like descriptions.
CORPUS is based on Yanget al(2011), except we do not use scene type (in-door, outdoor, etc.)
as part of the model.
At trainingtime, the model has access to the annotated imageregions and labels, and to the dependency-parsedversion of the English Gigaword Corpus (Napoleset al 2012).
The corpus is used to extract subject?verb?object subtrees, which are then used to predictthe best pairs of regions, as well as the verb that re-lates the regions.The set of region?region relationshipsR = {(oi, vi j, o j), .
.
.}
is determined by search-ing for the most likely o?j ,v?
given an oi over a setof verbs V extracted from the corpus and the otherregions in the image.
This is shown in Equation 1.o?j ,v?|oi = argmaxo j ,vp(oi) ?
p(v|oi) ?
p(o j|v,oi) (1)We can easily estimate p(oi), p(v|oi), and p(o j|v,oi)directly from the corpus.
If we cannot find an o?j ,v?for a region, we back-off to the spatial relationshipcalculation as defined in PROXIMITY.
When wehave found the best pairs of regions, we select themost probable pair and generate the first sentence ofthe description using that pair an template T1.
Thesecond sentence is realised with template T2 over thesubjects in R not used in generating the first sen-tence.
An example of the language generated is:(2) The man is riding the bike.
There is also acar, a road, and trees in the image.In comparison to PROXIMITY, this model will onlydescribe pairs of regions that have observed rela-tions in the external corpus.
The corpus also pro-vides a verb that relates the regions, which pro-duces descriptions that are more in line with human-generated text.
However, since noun co-occurrencein the corpus controls which regions can be men-tioned in the description, this model will be proneto relating regions simply because their labels occurtogether frequently in the corpus.3.3 STRUCTUREThe model STRUCTURE exploits the visual depen-dency representation of an image to generate lan-guage for only the relationships that hold betweenpairs of regions.
It has access to the image regions,the region labels, and the visual dependency repre-sentation of an image.Region?region relationships are generated duringa depth-first traversal of the VDR using templatesT1, T3, T4, and T5.
The VDR of an image is traversedand language fragments are generated and then com-bined depending on the number of children of a nodein the tree.
If a node has only one child then weuse T1 to generate text for the head-child relation-ship.
If a node has more than one child, we need todecide how to order the language generated by themodel.
We generate sentence fragments using T4 foreach child independently and combine them later.
InSTRUCTURE, the sentence fragments are sorted bythe Euclidean distance of the children from the par-ent.
In order to avoid problematic descriptions suchas ?The woman is above the horse is above the fieldis beside the house?, we include a special case forwhen a node has more than one child.
In these cases,the nearest region is realized in direct relation to thehead using either T3 (two children) or T1 (more thantwo children), and the remaining regions form a sep-arate sentence using T5.
This sorting and combingprocess would result in ?The woman is above thehorse.
She is above field and beside the house?
forthe case mentioned above.An example of the type of description that can begenerated during a traversal is:(3) The man is above the bike above the road.There is also a car and trees in the image.In comparison to PROXIMITY, this model can exploita representation of an image that encodes the rela-tionships between regions in an image (the VDR).However, it is limited to generating spatial relations,because it cannot predict verbs to relate regions.3.4 PARALLELThe model PARALLEL is an extension of STRUC-TURE that uses the image descriptions available to1297predict verbs that relate regions in parent-child re-lationships in a VDR.
At training time it has ac-cess to the annotated regions and labels, the visualdependency representations, and the gold-standardimage descriptions.
Recall from Section 2.1 thatthe descriptions were dependency-parsed using theparser of McDonald et al(2005) and alignmentswere calculated between the nodes in the VDRs andthe words in the parsed image descriptions.We estimate two distributions fromthe image descriptions using the align-ments: p(verb|ohead ,ochild ,relhead?child) andp(verb|ohead ,ochild).
The second distribution is usedas a backoff when we do not observe the arc labelbetween the regions in the training data.
The gener-ation process is similar to that used in STRUCTURE,with two exceptions: (1) it can generate verbsduring the generation steps, and (2) when a nodehas multiple dependents, the sentence fragmentsare sorted by the probability of the verb associatedwith them.
This sorting step governs which child isin a relationship with its parent.
When the modelgenerates text, it only generates a verb for themost probable sentence fragment.
The remainingfragments revert back to spatial relationships toavoid generating language that places the subjectregion in multiple relationships with other regions.An example of the language generated is:(4) The man is riding the bike on the road.
Thereis also a car and trees in the image.In comparison to CORPUS, this model generates de-scriptions in which the relations between the regionsdetermined by the image itself and not by an externalcorpus.
In comparison to PROXIMITY and STRUC-TURE, this model generates descriptions that expressmeaningful relations between the regions and notsimple spatial relationships.4 Image ParsingThe STRUCTURE and PARALLEL models rely on vi-sual dependency representations, but it is unreal-istic to assume gold-standard representations willalways be available because they are expensive toconstruct.
In this section we describe an imageparser that can induce VDRs automatically fromregion-annotated images, providing the input forthe STRUCTURE-PARSED and PARALLEL-PARSEDmodels at test time.The parser is based on the arc-factored depen-dency parsing model of McDonald et al(2005).This model generates a dependency representationby maximizing the score s computed over all edgesof the representation.
In our notation, xvis is the setof annotated regions and yvis is a visual dependencyrepresentation of the image; (i, j) is a directed arcfrom node i to node j in xvis, f(i, j) is a feature rep-resentation of the arc (i, j), and w is a vector of fea-ture weights to be learned by the model.
The overallscore of a visual dependency representation is:s(xvis,yvis) = ?
(i, j)?yvisw ?
f(i, j) (2)The features in the model are defined over re-gion labels in the visual dependency representationas well as the relationship labels.
As our depen-dency representations are unordered, none of thefeatures encode the linear order of region labels,unlike the feature set of the original model.
Uni-gram features describe how likely individual regionlabels are to appear as either heads or arguments andbigram feature captures which region labels are inhead-argument relationships.
All features are con-joined with the relationship label.We evaluate our parser on the 1,023 visual depen-dency representations from the data set.
The evalu-ation is run over 10 random splits into 80% train-ing, 10% development, and 10% test data.2 Per-formance is measured with labelled and unlabelleddirected dependency accuracy.
The parser achieves58.2%?
3.1 labelled accuracy and 65.5%?
3.3 un-labelled accuracy, significantly better than the base-line of 51.6% ?
2.5 for both labelled and unlabelledaccuracy (the baseline was calculated by attachingall image regions to the root node; this is the mostfrequent form of attachment in our data).5 Language Generation ExperimentsWe evaluate the image description models in an au-tomatic setting and with human judgements.
In2Different visual dependency representations of the sameimage are never split between the training and test data.1298Automatic Evaluation Human JudgementsBLEU-1 BLEU-2 BLEU-3 BLEU-4 Grammar Action ScenePARALLEL-PARSED 45.4 ?
2.0 16.1 ?
0.9 6.4 ?
0.7 2.7 ?
0.5 4.2 ?
1.3 3.3 ?
1.7 3.5 ?
1.3PROXIMITY 45.1 ?
0.8 10.2 ?
1.0?
2.1 ?
0.6?
0.4 ?
0.2?
3.7 ?
1.5?
2.1 ?
0.3?
3.0 ?
1.4?CORPUS 46.1 ?
1.1 12.4 ?
1.3?
3.1 ?
0.8?
0.7 ?
0.3?
4.4 ?
1.1 2.2 ?
1.3?
3.4 ?
1.3STRUCTURE 40.2 ?
3.0?
11.5 ?
1.2?
3.5 ?
0.5?
0.3 ?
0.1?
4.1 ?
1.4 2.1 ?
1.4?
3.0 ?
1.4?STRUCTURE-PARSED 41.1 ?
2.1?
12.2 ?
0.9?
3.6 ?
0.4?
0.4 ?
0.2?
4.0 ?
1.4 1.6 ?
1.3?
3.2 ?
1.3PARALLEL 44.6 ?
3.1 16.0 ?
1.5 6.8 ?
1.0 2.9 ?
0.7 4.5 ?
1.0?
3.4 ?
1.6 3.7 ?
1.3GOLD - - - - 4.8 ?
0.4?
4.8 ?
0.6?
4.6 ?
0.7?Table 4: Automatic evaluation results averaged over 10 random test splits of the data, and human judgements on themedian scoring BLEU-4 test split for PARALLEL.
We find significant differences (?p < 0.05) in the descriptions gener-ated by PARALLEL-PARSED compared to models that operate over an unstructured bag of image regions representation.Bold means PARALLEL-PARSED is significantly better than PROXIMITY, CORPUS, and STRUCTURE.the automatic setting, we follow previous work andmeasure how close the model-generated descrip-tions are to the gold-standard descriptions using theBLEU metric.
Human judgements were collectedfrom Amazon Mechanical Turk.5.1 MethodologyThe task is to produce a description of an image.The PROXIMITY and CORPUS models have accessto gold-standard region labels and region bound-aries at test time.
The STRUCTURE and PARALLELmodels have additional access to the visual depen-dency representation of the image.
These represen-tations are either the gold-standard, or in the case ofSTRUCTURE-PARSED and PARALLEL-PARSED, pro-duced by the image parser described in Section 4.Table 2 provides a reminder of the information thedifferent models have access to at training time.Our data set of 1,023 image?description?VDRtuples was randomly split into 10 folds of 80%training data, 10% development data, and 10% testdata.
The results we report are means computedover the 10 splits.
The image parser used for mod-els STRUCTURE-PARSED and PARALLEL-PARSEDis trained on the gold-standard VDRs of the train-ing splits, and then predicts VDRs on the develop-ment and test splits.
Significant differences weremeasured using a one-way ANOVA with PARALLEL-PARSED as the reference3, with differences betweenpairs of mean checked with a Tukey HSD test.5.2 Automatic EvaluationThe model-generated descriptions are comparedagainst the human-written gold-standard descrip-tions using the smoothed BLEU measure (Lin andOch, 2004).
BLEU is commonly used in ma-chine translation experiments to measure the effec-tive overlap between a reference sentence and a pro-posed translation sentence.
Table 4 shows the re-sults on the test data and Figure 4 shows sample out-puts for two images.
PARALLEL, the model withaccess to both image structure and aligned imagedescriptions at training time outperforms all othermodels on higher-order BLEU measures.
One rea-son for this improvement is that PARALLEL can for-mulate sentence fragments that relate the subject, averb, and an object without trying to predict the bestobject, unlike CORPUS.
The probability associatedwith each fragment generated for nodes with mul-tiple children also tends to lead to a more accurateorder of mentioning image regions.
It can also beseen that PARALLEL-PARSED remains significantlybetter than the other models when the VDRs of im-ages are predicted by an image parser, rather thanbeing gold-standard.3Recall that PARALLEL uses gold-standard VDRs andPARALLEL-PARSED uses the output of the image parser de-scribed in Section 4.1299The weakest results are obtained from a modelthat relies on the proximity of regions to generate de-scriptions.
PROXIMITY achieves competitive BLEU-1 scores but this is mostly due to it correctly gener-ating region names and determiners.
CORPUS is bet-ter than PROXIMITY at correctly producing higher-order n-grams than because it has a better model ofthe region?region relationships in an image.
How-ever, it has difficulties guessing the correct verb fora description, as it relies on corpus co-occurrencesfor this (see the second example in Table 4).
STRUC-TURE uses the VDR of an image to generate the de-scription, which this leads to an improvement overPROXIMITY on some of the BLEU metrics; however,it is not sufficient to outperform CORPUS.5.3 Human JudgementsWe conducted a human judgement study on Me-chanical Turk to complement the automatic evalu-ation.
Workers were paid $0.05 to rate the quality ofan image?description pair generated by one of themodels using three criteria on a scale from 1 to 5:1.
Grammaticality: give high scores if the de-scription is correct English and doesn?t containany grammatical mistakes.2.
Action: give high scores if the description cor-rectly describes what people are doing in theimage.3.
Scene: give high scores if the description cor-rectly describes the rest of the image (back-ground, other objects, etc).A total of 101 images were used for this evalua-tion and we obtained five judgments for each image-description pair, resulting in a total of 3,535 judg-ments.
To ensure a fair evaluation, we chose theimages from the split of the data that gave medianBLEU-4 accuracy for PARALLEL, the best perform-ing model in the automatic evaluations.The right side of Table 4 shows the mean judge-ments for each model for across the three evalua-tion criteria.
The gold-standard descriptions elicitedjudgements around five, and were significantly bet-ter than the model outputs on all aspects.
Further-more, all models produce highly grammatical out-put, with mean ratings of between 3.7 and 4.5.
Thiscan be explained by the fact that the models all reliedon templates to ensure grammatical output.The ratings of the action descriptions reveal theusefulness of structural information.
PROXIMITY,CORPUS, and STRUCTURE all perform badly withmean judgements around two, PARALLEL, whichuses both image structure and aligned descriptions,significantly outperforms all other models with theexception of PARALLEL-PARSED, which has verysimilar performance.
The fact that PARALLEL andPARALLEL-PARSED perform similarly on all threehuman measures confirms that automatically parsedVDRs are as useful for image description as gold-standard VDRs.When we compare the quality of the scene de-scriptions, we notice that all models perform simi-larly, around the middle of the scale.
This is proba-bly due to the fact that they all have access to gold-standard region labels, which enables them to cor-rectly refer to regions in the scene most of the time.The additional information about the relationshipsbetween regions that STRUCTURE and PARALLELhave access to does not improve the quality of thebackground scene description.6 Related WorkPrevious work on image description can be groupedinto three approaches: description-by-retrieval, de-scription using language models, and template-based description.
Ordonez et al(2011), Farhadiet al(2010), and Kuznetsova et al(2012) gener-ate descriptions by retrieving the most similar imagefrom a large data set of images paired with descrip-tions.
These approaches are restricted to generatingdescriptions that are only present in the training set;also, they typically require large amounts of trainingdata and assume images that share similar properties(scene type, objects present) should be described ina similar manner.Kulkarni et al(2011) and Li et al(2011) generatedescriptions using n-gram language models trainedon a subset of Wikipedia.
Both approaches firstdetermine the attributes and relationships betweenregions in an image as region?preposition?regiontriples.
The disadvantage of relying on region?preposition?region triples is that they cannot distin-guish between the main event of the image and the1300PROXIMITY A man is beside a phone.
There is also a wall and a sign in the image.CORPUS A man is holding a sign.
There is also a wall and a phone in the image.STRUCTURE A wall is above a wall.
A man is beside a sign.PARALLEL A man is holding a phone.
A wall is beside a sign.GOLD A foreign man with sunglasses talking on a cell phone.A large building and a mountain in the background.PROXIMITY A beach is above a beach.There are also horses, a woman, and a man in the image.CORPUS A woman is outnumbering a man.There are also horses and beaches in the image.STRUCTURE A man is beside a woman above a horse.A horse is beside a woman beside a beach.PARALLEL A man is riding a horse above a beach.A horse is beside a beach beside a woman.GOLD There is a man and women both on horses.They are on a beach during the day.Figure 4: Some example descriptions produced by PROXIMITY, CORPUS, STRUCTURE and PARALLEL.background regions.
Kulkarni et al(2011) is closelyrelated to our PROXIMITY baseline.Yang et al(2011) fill in a sentence templateby selecting the likely objects, verbs, prepositions,and scene types based on a Hidden Markov Model.Verbs are generated by finding the most likely pair-ing of object labels in an external corpus.
Thismodel is closely related to our CORPUS baseline.Mitchell et al(2012) over-generates syntacticallywell-formed sentence fragments and then recom-bines these using a tree-substitution grammar.Previous research has relied extensively on auto-matically detecting object regions in an image usingstate-of-the art object detectors (Felzenszwalb et al2010).
We use gold-standard region annotations toremove this noisy component from the descriptiongeneration pipeline, allowing us to focus on the util-ity of image structure for description generation.7 ConclusionIn this paper we introduced a novel representationof an image as a set of dependencies over its an-notated regions.
This visual dependency represen-tation encodes which regions are related to eachother in an image, and can be used to infer the ac-tion or event that is depicted.
We found that im-age description models based on visual dependencyrepresentations significantly outperform competingmodels in both automatic and human evaluations.We showed that visual dependency representationscan be induced automatically using a standard de-pendency parser and that the descriptions generatedfrom the induced representations are as good as theones generated from gold-standard representations.Future work will focus on improvements to the im-age parser, on exploring this representation in open-domain data sets, and on using the output of an ob-ject detector to obtain a fully automated model.AcknowledgmentsThe authors would like to thank M. Lapata and S.Frank for feedback on an earlier draft of the pa-per and the anonymous reviewers for their feed-back.
A. M. Enoch, N. Ghahremani-Azghandi, L. S.McAlpine, and K. Tsagkaridis helped annotate thedata.
The research presented here was supported bythe European Research Council under award 203427Synchronous Linguistic and Visual Processing.1301ReferencesMark Everingham, Luc Van Gool, Christopher K. I.Williams, John Winn, and Andrew Zisserman.
2011.The PASCAL Visual Object Classes Challenge 2011(VOC2011) Results.Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi,Peter Young, Cyrus Rashtchian, Julia Hockenmaier,and David Forsyth.
2010.
Every picture tells a story:generating sentences from images.
In ECCV ?10,pages 15?29, Heraklion, Crete, Greece.P F Felzenszwalb, R B Girshick, D McAllester, andD Ramanan.
2010.
Object Detection with Discrimi-natively Trained Part-Based Models.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,32(9):1627?1645.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, SimingLi, Yejin Choi, Alexander C. Berg, and Tamara L.Berg.
2011.
Baby talk: Understanding and generat-ing simple image descriptions.
In CVPR ?11, pages1601?1608, Colorado Springs, Colorado, U.S.A.Polina Kuznetsova, Vicente Ordonez, Alexander C. Berg,Tamara L. Berg, and Yejin Choi.
2012.
CollectiveGeneration of Natural Image Descriptions.
In ACL?12, pages 359?368, Jeju Island, South Korea.Siming Li, Girish Kulkarni, Tamara L. Berg, Alexan-der C. Berg, and Yejin Choi.
2011.
Composing sim-ple image descriptions using web-scale n-grams.
InCoNLL ?11, pages 220?228, Portland, Oregon, U.S.A.Chin-Yew Lin and Franz Josef Och.
2004.
Automaticevaluation of machine translation quality using longestcommon subsequence and skip-bigram statistics.
InACL ?04, pages 605?612, Barcelona, Spain.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online large-margin training of dependencyparsers.
In ACL ?05, pages 91?98, University ofMichigan, U.S.A.Margaret Mitchell, Jesse Dodge, Amit Goyal, Kota Ya-maguchi, Karl Stratos, Alyssa Mensch, Alex Berg,Tamara Berg, and Hal Daum.
2012.
Midge : Generat-ing Image Descriptions From Computer Vision Detec-tions.
In EACL ?12, pages 747?756, Avignon, France.Courtney Napoles, Matthew Gormley, and Benjamin VanDurme.
2012.
Annotated Gigaword.
In AKBC-WEKEX Workshop at NAACL-HLT ?12, Montreal,Canada.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2Text: Describing Images Using 1 MillionCaptioned Photographs.
In NIPS 24, Granada, Spain.Bryan C. Russell, Antonio Torralba, Kevin P. Murphy,and William T. Freeman.
2008.
LabelMe: A Databaseand Web-Based Tool for Image Annotation.
IJCV,77(1-3):157?173.Yezhou Yang, Ching Lik Teo, Hal Daume?
III, and YiannisAloimonos.
2011.
Corpus-Guided Sentence Genera-tion of Natural Images.
In EMNLP ?11, pages 444?454, Edinburgh, Scotland, UK.1302
