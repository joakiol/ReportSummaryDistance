Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 169?172,Sydney, July 2006. c?2006 Association for Computational LinguisticsCharacter Language Models for Chinese Word Segmentation and NamedEntity RecognitionBob CarpenterAlias-i, Inc.carp@alias-i.comAbstractWe describe the application of the Ling-Pipe toolkit to Chinese word segmentationand named entity recognition for the 3rdSIGHAN bakeoff.1 Word SegmentationChinese is written without spaces between words.For the word segmentation task, four training cor-pora were provided with one sentence per line anda single space character between words.
Test dataconsisted of Chinese text, one sentence per line,without spaces between words.
The task is to in-sert single space characters between the words.For this task and named entity recognition, weused the UTF8-encoded Unicode versions of thecorpora converted from their native formats by thebakeoff organizers.2 Named Entity RecognitionNamed entities consist of proper noun mentionsof persons (PER), locations (LOC), and organiza-tions (ORG).
Two training corpora were provided.Each line consists of a single character, a singlespace character, and then a tag.
The tags were inthe standard BIO (begin/in/out) encoding.
B-PERtags the first character in a person entity, I-PERtags subsequent characters in a person, and 0 char-acters not part of entities.
We segmented thedata into sentences by taking Unicode character0x3002, which is rendered as a baseline-alignedsmall circle, as marking end of sentence (EOS).
Asjudged by our own sentence numbers (see Figures1 and 2), this missed around 20% of the sentenceboundaries in the City U NE corpus and 5% ofthe boundaries in the Microsoft NE corpus.
Testdata is in the same format as the word segmenta-tion task.3 LingPipeLingPipe is a Java-based natural language process-ing toolkit distributed with source code by Alias-i(2006).
For this bakeoff, we used two LingPipepackages, com.aliasi.spell for Chineseword segmentation and com.aliasi.chunkfor named-entity extraction.
Both of these de-pend on the character language modeling pack-age com.aliasi.lm, and the chunker alsodepends on the hidden Markov model packagecom.alias.hmm.
The experiments reported inthis paper were carried out in May 2006 using (aprerelease version of) LingPipe 2.3.0.3.1 LingPipe?s Character Language ModelsLingPipe provides n-gram based character lan-guage models with a generalized form of Witten-Bell smoothing, which performed better than otherapproaches to smoothing in extensive English tri-als (Carpenter 2005).
Language models providea probability distribution P (?)
defined for strings?
?
??
over a fixed alphabet of characters ?.
Webegin with Markovian language models normal-ized as random processes.
This means the sum ofthe probabilities for strings of a fixed length is 1.0.The chain rule factors P (?c) = P (?)
?
P (c|?
)for a character c and string ?.
The n-gram Marko-vian assumption restricts the context to the previ-ous n?1 characters, taking P (cn|?c1 ?
?
?
cn?1) =P (cn|c1 ?
?
?
cn?1).The maximum likelihood estimator for n-gramsis P?ML(c|?)
= count(?c)/extCount(?
), wherecount(?)
is the number of times the sequence ?was observed in the training data and extCount(?
)169is the number of single-character extensions of ?observed: extCount(?)
=?c count(?c).Witten-Bell smoothing uses linear interpolationto form a mixture model of all orders of maximumlikelihood estimates down to the uniform estimatePU (c) = 1/|?|.
The interpolation ratio ?(d?
)ranges between 0 and 1 depending on the context:P?
(c|d?)
= ?(d?)PML(c|d?
)+ (1 ?
?(d?))P?
(c|?)P?
(c) = ?
()PML(c)+ (1 ?
?
())(1/|?|)Generalized Witten-Bell smoothing defines theinterpolation ratio with a hyperparameter ?:?(?)
=extCount(?)extCount(?)
+ ?
?
numExts(?
)We take numExts(?)
= |{c|count(?c) > 0}| to bethe number of different symbols observed follow-ing ?
in the training data.
The original Witten-Bellestimator set the hyperparameter ?
= 1.
Ling-Pipe?s default sets ?
equal to the n-gram order.3.2 Noisy Channel Spelling CorrectionLingPipe performs spelling correction with anoisy-channel model.
A noisy-channel modelconsists of a source model Ps(?)
defining theprobability of message ?, coupled with a chan-nel model Pc(?|?)
defining the likelihood of a sig-nal ?
given a message ?.
In LingPipe, the sourcemodel Ps is a character language model.
Thechannel model Pc is a (probabilistically normal-ized) weighted edit distance (with transposition).LingPipe?s decoder finds the most likely message?
to have produced a signal ?
: argmax?P (?|?)
=argmax?P (?)
?
P (?|?
).For spelling correction, the channel Pc(?|?)
isa model of what is likely to be typed given an in-tended message.
Uniform models work fairly welland ones tuned to brainos and typos work even bet-ter.
The source model is typically estimated froma corpus of ordinary text.For Chinese word segmentation, the sourcemodel is trained over the corpus with spaces in-serted.
The noisy channel deterministically elim-inates spaces so that Pc(?|?)
= 1.0 if ?
isidentical to ?
with all of the spaces removed,and 0.0 otherwise.
This channel is easily imple-mented as a weighted edit distance where dele-tion of a single space is 100% likely (log proba-bility edit ?cost?
is zero) and matching a charac-ter is 100% likely, with any other operation be-ing 0% likely (infinite cost).
This makes any seg-mentation equally likely according to the channelmodel, reducing decoding to finding the highestlikelihood hypothesis consisting of the test stringwith spaces inserted.
This approach reduces tothe cross-entropy/compression-based approach of(Teahan et al 2000).
Experiments showed thatskewing these space-insertion/matching probabil-ities reduces decoding accuracy.3.3 LingPipe?s Named Entity RecognitionLingPipe 2.1 introduced a hidden Markovmodel interface with several decoders: first-best(Viterbi), n-best (Viterbi forward, A* backwardwith exact Viterbi estimates), and confidence-based (forward-backward).LingPipe 2.2 introduced a chunking implemen-tation that codes a chunking problem as an HMMtagging problem using a refinement of the stan-dard BIO coding.
The refinement both introducescontext and greatly simplifies confidence estima-tion over the approach using standard BIO cod-ing in (Culotta and McCallum 2004).
The tagsare B-T for the first character in a multi-characterentity of type T, M-T for a middle character in amulti-character entity, E-T for the end character ina multi-character entity, and W-T for a single char-acter entity.
The out tags are similarly contextual-ized, with additional information on the start/endtags to model their context.
Specifically, the tagsused are B-O-T for a character not in an entityfollowing an entity of type T, I-O for any mid-dle character not in an entity, and E-O-T for acharacter not in an entity but preceding a charac-ter in an entity of type T, and finally, W-O-T fora character that is a single character between twoentities, the following entity being of type T. Fi-nally, the first tag is conditioned on the begin-of-sentence tag (BOS) and after the last tag, the end-of-sentence tag (EOS) is generated.
Thus the prob-abilities normalize to model string/tag joint prob-abilities.In the HMM implementation considered here,transitions between states (tags) in the HMM aremodeled by a maximum likelihood estimate overthe training data.
Tag emissions are generated bybounded character language models.
Rather thanthe process estimate P (X), we use P (X#|#),where # is a distinguished boundary character170Corpus Encod Sents Chars Uniq Words Uniq Test S Test Ch UnseenCity U HK HKSCS (trad) 57K 4.3M 5113 1.6M 76K 7.5K 364K 0.046%Microsoft gb18030 (simp) 46K 3.4M 4768 1.3M 63K 4.4K 173K 0.046%Ac Sinica Big5 (trad) 709K 13.2M 6123 5.5M 146K 11.0K 146K 0.560%Penn/Colo CP936 (simp) 19K 1.3M 4294 0.5M 37K 5.1K 256K 0.160%Figure 1: Word Segmentation CorporaCorpus Sents Chars Uniq LOC PER ORG Test S Test Ch UnseenCity U HK 48K 2.7M 5113 48.2K 36.4K 27.8K 7.5K 364K 0.046%Microsoft 44K 2.2M 4791 36.9K 17.6K 20.6K 4.4K 173K 0.046%Figure 2: Named Entity Recognition Corporanot in the training or test character sets.
We alsotrain with boundaries.
For Chinese at the charac-ter level, this bounding is irrelevant as all tokensare length 1, so probabilities are already normal-ized and there is no contextual position to take ac-count of within a token.
In the more usual word-tokenized case, it normalizes probabilities over allstrings and accounts for the special status of pre-fixes and suffixes (e.g.
capitalization, inflection).Consider the chunking consisting of the stringJohn J. Smith lives in Seattle.
with John J. Smith aperson mention and Seattle a location mention.
Inthe coded HMM model, the joint estimate is:P?ML(B-PER|BOS) ?
P?B-PER(John#|#)?
P?ML(I-PER|B-PER) ?
P?I-PER(J#|#)?
P?ML(I-PER|I-PER) ?
P?I-PER(.#|#)?
P?ML(E-PER|I-PER) ?
P?E-PER(Smith#|#)?
P?ML(B-O-PER|E-PER) ?
P?B-O-PER(lives#|#)?
P?ML(E-O-LOC|B-O-PER) ?
P?E-O-LOC(in#|#)?
P?ML(W-LOC|E-O-LOC) ?
P?W-LOC(Seattle#|#)?
P?ML(W-O-EOS|W-LOC) ?
P?W-O-EOS(.#|#)?
P?ML(EOS|W-O-EOS)LingPipe 2.3 introduced an n-best chunking im-plementation that adapts an underlying n-bestchunker via rescoring.
In rescoring, each of theseoutputs is scored on its own and the new bestoutput is returned.
The rescoring model is alonger-distance generative model that producesalternating out/entity tags for all characters.
Thejoint probability of the specified chunking is:P?OUT(cPER|cBOS)?
P?PER(John J. SmithcOUT|cOUT)?
P?OUT( lives in cLOC|cPER)?
P?LOC(SeattlecOUT|cOUT)?
P?OUT(.cEOS|cLOC)where each estimator is a character languagemodel, and where the cT are distinct charactersnot in the training/test sets that encode begin-of-sentence (BOS), end-of-sentence (EOS), and type(e.g.
PER, LOC, ORG).
In words, we generate analternating sequence of OUT and type estimates,starting and ending with an OUT estimate.
Webegin by conditioning on the begin-of-sentencetag.
Because the first character is in an entity, wedo not generate any text, but rather generate acharacter indicating that we are done generatingthe OUT characters and ready to switch to gen-erating person characters.
We then generate thephrase John J. Smith in the person model; notethat type estimates always begin and end with thecOUT character, essentially making them boundedmodels.
After generating the name and thecharacter to end the entity, we revert to generatingmore out characters, starting from a person andending with a location.
Note that we are generat-ing the phrase lives in including the preceding andfollowing space.
All such spaces are generated inthe OUT models for English; there are no spaces inthe Chinese input.
Next, we generate the locationphrase the same way as the person phrase.
Next,we generate the final period in the OUT modeland then the end-of-sentence symbol.
Note thatthe OUT category?s language model shouldersthe brunt of the burden of estimating contextualeffects.
It conditions on the preceding type, sothat the likelihood of lives in is conditioned onfollowing a person entity.
Furthermore, the choiceto begin an entity of type location is based onthe fact that it follows lives in.
This includesbegin-of-sentence and end-of-sentence effects,so the model is sensitive to initial capitalizationin the out model as a distribution of charactersequences likely to follow BOS.
Similarly, the171Corpus R P F1 Best F1 OOV ROOVCity Uni Hong Kong .966 .957 .961 .972 4.0% .555Microsoft Research .959 .955 .957 .963 3.4% .494Academia Sinica .951 .935 .943 .958 4.2% .389U Penn and U Colorado .919 .895 .907 .933 8.8% .459Figure 3: Word Segmentation Results (Closed Category)Corpus R P F1 Best F1 PLOC RLOC PPER RPER PORG RORGCity Uni HK .8417 .8690 .8551 .8903 .8961 .8762 .8749 .8943 .6997 .8176MS Research .8097 .8188 .8142 .8651 .8351 .8716 .7968 .8438 .7739 .6899Figure 4: Named Entity Recognition Results (Closed Category)end-of-sentence is conditioned on the precedingtext, in this case a single period.
The resultingmodel defines a (properly normalized) jointprobability distribution over chunkings.4 Held-out Parameter TuningWe ran preliminary tests on MUC 6 English andCity University of Hong Kong data for Chineseand found baseline performance around 72% andrescored performance around 82%.
The underly-ing model was designed to have good recall in gen-erating hypotheses.
Over 99% of the MUC testsentences had their correct analysis in a 1024-bestlist generated by the underlying model.
Neverthe-less, setting the number of hypotheses beyond 64did not improve results in either English or Chi-nese, so we reported runs with n-best set to 64.We believe this is because the two language-modelbased approaches make highly correlated rankingdecisions based on character n-grams.Held-out scores peaked with 5-grams for Chi-nese; 3-grams and 4-grams were not much worseand longer n-grams performed nearly identically.We used 7500 as the number of distinct charac-ters, though this parameter is not at all sensitiveto within an order of magnitude.
We used Ling-Pipe?s default of setting the interpolation parame-ter equal to the n-gram length; for the final eval-uation ?
= 5.0.
Higher interpolation ratios favorprecision over recall, lower ratios favor recall.
Val-ues within an order of magnitude performed with1% F-measure and 2% precision/recall.5 Bakeoff Time and EffortThe total time spent on this SIGHAN bakeoff wasabout 2 hours for the word segmentation task and10 hours for the named-entity task (not includingwriting this paper).
We started from a workingword segmentation system for the last SIGHAN.Most of the time was spent munging entity data,with the rest devoted to held out analysis.
The finalcode was roughly one page per task, with only adozen or so LingPipe-specific lines.
The final run,including unpacking, training and testing, took 45minutes on a 512MB home PC; most of the timewas named-entity decoding.6 ResultsOfficial bakeoff results for the four word segmen-tation corpora are shown in Figure 3, and for thetwo named entity corpora in Figure 4.
Columnlabels are R for recall, P for precision, F1 forbalanced F -measure, Best F1 for the best closedsystem?s F1 score, OOV for the out-of-vocabularyrate in the test corpus, and ROOV for recall on theout-of-vocabulary items.
For the named-entity re-sults, precision and recall are also broken down bycategory.7 DistributionLingPipe may be downloaded from its homepage,http://www.alias-i.com/lingpipe.
The codefor the bakeoff is available via anonymous CVSfrom the sandbox.
An Apache Ant makefile is pro-vided to generate our bakeoff submission from theofficial data distribution format.ReferencesCarpenter, B.
2005.
Scaling high-order character languagemodels to gigabytes.
ACL Software Workshop.
Ann Arbor.Culotta, A. and A. McCallum.
2004.
Confidence estimationfor information extraction.
HLT/NAACL 2004.
Boston.Teahan, W. J., Y. Wen, R. McNab, and I. H. Witten.
2000.
Acompression-based algorithm for Chinese word segmenta-tion.
Computational Linguistics, 26(3):375?393.172
