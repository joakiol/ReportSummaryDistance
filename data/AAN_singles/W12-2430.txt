Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 240?243,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsBoosting the protein name recognition performanceby bootstrapping on selected textYue Wang and Jin-Dong KimDatabase Center for Life Science,Research Organization of Information and Systems2-11-16 Yayoi, Bunkyo-ku, Tokyo, Japan 113-0032{wang,jdkim}@dbcls.rois.ac.jpAbstractWhen only a small amount of manually anno-tated data is available, application of a boot-strapping method is often considered to com-pensate for the lack of sufcient training ma-terial for a machine-learning method.
Thepaper reports a series of experimental resultsof bootstrapping for protein name recogni-tion.
The results show that the performancechanges signicantly according to the choiceof text collection where the training samplesto bootstrap, and that an improvement can beobtained only with a well chosen text collec-tion.1 IntroductionWhile machine learning-based approaches are be-coming more and more popular for the developmentof natural language processing (NLP) systems, cor-pora with annotation are regarded as a critical re-source for the training process.
Nonetheless, the cre-ation of corpus annotation is an expensive and time-consuming work (Cohen et al, 2005), and it is of-ten the case that lack of sufcient annotation hindersthe development of NLP systems.
Bootstrappingmethod (Becker et al, 2005; Vlachos and Gasperin,2006) can be considered as a way to automaticallyinate the amount of corpus annotation to comple-ment the lack of sufcient annotation.In this study, we report the experimental results onthe effect of bootstrapping for the training of proteinname recognizers, particularly in the situation whenwe have only a small amount of corpus annotations.In summary, we begin with a small corpus withmanual annotation for protein names.
A named en-tity tagger trained on the small corpus is applied toa big collection of text, to obtain more annotation.We hope the newly created annotation to be preciseenough so that the training of a protein tagger canbenet from the increased training material.We assume that the accuracy of a bootstrappingmethod (Ng, 2004) depends on two factors: the ac-curacy of the bootstrap tagger itself and the similar-ity of the text to the original corpus.
While accuracyof the bootstrap tagger may be maximized by nd-ing the optimal parameters of the applied machinelearning method, the choice of text where the origi-nal annotations will bootstrap may also be a criticalfactor for the success of the bootstrapping method.Experimental results presented in this paper con-rm that we can get a improvement by using a boot-strapping method with a well chosen collection oftexts.The paper is organized as follows.
Section 2 intro-duces the two datasets used in this paper.
Followingthat, in Section 3, we briey introduce the experi-ments performed in our research.
The experimentalresults are demonstrated in Section 4.
The researchis concluded in Section 5 and in the meanwhile, fu-ture work is discussed.2 Datasets2.1 The cyanobacteria genome databaseCyanobacteria are prokaryotic organisms that haveserved as important model organisms for studyingoxygenic photosynthesis and have played a signi-240cant role in the Earthfs history as primary producersof atmospheric oxygen (Nakao et al, 2010).The cyanobacteria genome database (abbreviatedto CyanoBase1) includes the annotations to thePubMed text.
In total, 39 species of the cyanobacte-ria are covered in the CyanoBase.In our cyanobacteria data (henceforth, the Kazusadata for short), 270 abstracts were annotated by twoindependent annotators.
We take the entities, aboutwhich both of the annotators agreed with each other.In total, there are 1,101 entities in 2,630 sentences.The Kazusa data was split equally into three sub-sets and the subsets were used in turn as the training,development and testing sets in the experiments.2.2 The BioCreative dataThe BioCreative data, which was used for theBioCreative II gene mention task2, is described asthe tagged gene/protein names in the PubMed text.The training set is used in the research, and totallythere are 15,000 sentences in the dataset.Unlike other datasets, the BioCreative data wasdesigned to contain sentences both with and withoutprotein names, in a variety of contexts.
Since thecollection is made to explicitly compile positive andnegative examples for protein recognition, there is achance that the sample of text is not comprehensive,and gray-zone expressions may be missed.The reason that we chose the BioCreative datafor the bootstrapping is that, the BioCreative data(henceforth, the BC2 data for short) is the collectionfor the purpose of training and evaluation of proteinname taggers.3 Experiment summaryIn the following experiments, the NERSuite3, anamed entity tagger based on Conditional RandomFields (CRFs) (Lafferty et al, 2001; Sutton and Mc-Callum, 2007), is used.
The NERSuite is executableopen-source and serves as a machine learning sys-tem for named entity recognition (NER).
The sigmavalue for the L2-regularization is optimizable and inour experiments, we tune the sigma value between10?1 to 104.1http://genome.kazusa.or.jp/cyanobase2http://www.biocreative.org/3http://nersuite.nlplab.org/As mentioned in Section 2.1, the three subsets ofKazusa data are used for training, tuning and testingpurposes, in turn.
We experimented with all the sixcombinations.Experiments were performed to compare threedifferent strategies.
First, with the baseline strat-egy, the protein tagger is trained only on the Kazusatraining set.
The sigma value is optimized on thetuning set, and the performance is evaluated on thetest set.
It is the most typical strategy particularlywhen it is believed there is a sufcient training ma-terial.Second, with the bootstrapping strategy, theKazusa training set is used as the seed data.
A tag-ger for bootstrapping (bootstrap tagger, hereafter) istrained on the seed data, and applied to the BC2 datato bootstrap the training examples.
Another pro-tein tagger (application tagger) is then trained on thebootstrapped BC2 data together with the seed data.The Kazusa tuning set is used to optimize the twosigma values for the two protein taggers, and theperformance is evaluated on the test set.
With thisstrategy, we wish the bootstrapped examples com-plement the lack of sufcient training examples.Experiment Seed BT BT+SSE1 368 647 647 (1,103)E2 368 647 647 (1,103)E3 366 759 759 (1,200)E4 366 769 590 (1,056)E5 367 882 558 (1,068)E6 367 558 558 (1,068)Table 1: The number of positive examples used in eachexperiment.
The ?BT?
column shows the number of posi-tive examples obtained by the bootstrapping in the 15,000BC2 sentences.
In the last column, the gures in paren-theses are the number of the selected sentences.Third, the bootstrapping with sentence selectionstrategy is almost the same with the bootstrappingstrategy, except that the second tagger is trained afterthe non-relevant sentences are ltered out from theBC2 data.
Here, non-relevant sentences mean thosethat are not tagged by the the bootstrap tagger.
Withthis strategy, we wish an improvement with the boot-strapping by removing noisy data.
Table 1 shows thenumber of the seed and bootstrapped examples usedfor the three strategies.
It is observed that the seed241Training Tuning Testing Baseline BT BT+SSE1 A B C 63.7/29.2/40.0 [102] 61.3/25.9/36.4 [104-101] 61.7/38.2/47.1 [104-104]E2 A C B 65.2/36.9/47.1 [103] 67.7/35.0/46.1 [104-101] 61.7/46.7/53.2 [104-104]E3 B C A 75.3/36.4/49.1 [102] 75.2/31.3/44.2 [102-101] 67.1/40.0/50.1 [102-101]E4 B A C 68.5/33.8/45.3 [102] 70.2/28.9/40.9 [104-101] 66.7/36.5/47.2 [101-102]E5 C B A 77.7/35.1/48.3 [101] 71.8/27.7/40.0 [104-102] 70.9/38.3/49.7 [100-101]E6 C A B 73.0/39.1/50.9 [101] 76.1/32.2/45.3 [100-102] 67.7/41.8/51.7 [100-102]Table 2: Experimental results of using the Kazusa and BC2 data (Precision/Recall/F-score).
?BT?
and ?SS?
representthe bootstrapping and sentence selection strategies, respectively.
The gures in square brackets are the sigma valuesoptimized in the experiments.annotation bootstrap only on a small portion of theBC2 data set, e.g., 1,103 vs. 15,000 sentences in thecase of E1 (less than 10%), suggesting that a largeportion of the data set may be irrelevant to the origi-nal data set.4 Experimental resultsThe experimental results of all the six combinationsare shown in Table 2.
The use of the three subsets,denoted by A, B, C, of the Kazusa data set for train-ing, tuning and testing in each experiment is spec-ied in ?training?, ?tuning?
and ?testing?
columns.The results of the baseline strategy that uses onlythe Kazusa data are shown in the ?baseline?
column,whereas the results with the bootstrapping methodswith and without sentence selection are shown in thelast two columns.
As explained in Section 3, thesigma values are optimized using the tuning set foreach experiment.
Note that for bootstrapping, weneed two sigma values for the bootstrapping taggerand the application tagger.
See section 3.The performance of named entity recognition ismeasured in terms of precision, recall and F-score.For matching criterion, in order to avoid underesti-mation, instead of the exact matching, system per-formance is evaluated under a soft matching, theoverlapping matching criterion.
That is, if any partof the annotated protein/gene names is recognizedby the NER tagger, we will regard that as a correctanswer.4.1 Results with the bootstrapping strategyComparing the two columns, ?baseline?
and ?BT?,we observe that the use of bootstrapping may leadto a degradation of the performance.
Note that thesigma values are optimized on the development setfor each experiment, and the text for bootstrappingis BC2 corpus which is expected to be similar to theKazusa corpus, but still it is observed that the boot-strapping does not work, suggesting that the text col-lection may not yet similar enough.4.2 Results with bootstrapping with sentenceselectionComparing the last column (the ?BT+SS?
column)to the ?baseline?
column, we observe that the appli-cation of the bootstrapping method with sentence se-lection consistently improves the performance.
Theimprovement is sometimes signicant, e.g., 7.1% ofdifference in F-score in the case of E1, but some-times not, e.g., only 0.8% in the case of E6, but theperformance is improved in the every experiments.The results conrm our assumption that the choiceof text for bootstrapping is important, and that thesentence selection is a stable method for the choiceof text.5 Conclusion and future workIn order to compensate for the lack of sufcienttraining data for a CRF-based protein name recog-nizer, the potential of a bootstrapping method hasbeen explored through a series of experiments.
TheBC2 data was chosen for the bootstrapping as thedata set was one collected for protein name recogni-tion.Our initial experiment showed that the seed anno-tations bootstrapped only on a very small portion ofthe BC2 data set, suggesting that a big portion of thedata set might be less relevant to the seed corpus.From a series of experiments, it was observed thatthe performance of protein name recognition was al-ways improved with bootstrapping by selecting only242the sentences where the seed annotations bootstrap,and by using them as an additional training data.The goal was to be able to predict more possibleprotein mentions (recall) at a relatively satisfactorylevel of the quality (precision).
The experimentalresults suggest us, in order to achieve the goal, thechoice of text collection is important for the successof the use of a bootstrapping method.For the future work, we would like to take use ofthe original annotations in the BC2 data.
A lteringstrategy (Wang, 2010) will be performed.
Instead ofcompletely using the output of the Kazusa-trainedtagger, we compare the output of the Kazusa-trainedtagger with the BioCreative annotations.
If the en-tity is recognized by the tagger and also annotatedin the BioCreative data, then the annotation to thisentity will be kept.
The entity will be regarded asa true positive according to the BioCreative annota-tions.
Otherwise, we will remove the annotation tothe entity from the BioCreative annotations.Further, we also would like to combine the boot-strapping with the ltering.
Besides keeping the truepositives, we also want to include some false pos-itives from the bootstrapping.
Because these falsepositives helps in improving the recall, when the tag-ger is applied to the Kazusa testing subset.
To dis-criminate this strategy from the bootstrapping andltering strategies, different sigma value should beused.AcknowledgementWe thank Shinobu Okamoto for providing theKazusa data and for many useful discussion.
Thiswork was supported by the ?Integrated DatabaseProject?
funded by the Ministry of Education, Cul-ture, Sports, Science and Technology (MEXT) ofJapan.ReferencesK.
Bretonnel Cohen, Lynne Fox, Philip Ogren andLawrence Hunter.
2005.
Empirical data on corpusdesign and usage in biomedical natural language pro-cessing.
Proceedings of the AMIA Annual Symposium,38?45.Markus Becker, Ben Hachey, Beatrice Alex, ClaireGrover.
2005.
Optimising Selective Sampling forBootstrapping Named Entity Recognition.
Proceed-ings of the Workshop on Learning with Multiple Views,5?11.Andreas Vlachos and Caroline Gasperin.
2006.
Boot-strapping and Evaluating Named Entity Recognitionin the Biomedical domain.
Proceedings of the BioNLPWorkshop, 138?145.Andrew Ng.
2004.
Feature selection, L1 vs. L2 regu-larization, and rotational invariance.
Proceedings ofthe 21st International Conference on Machine Learn-ing (ICML).Mitsuteru Nakao, Shinobu Okamoto, Mitsuyo Kohara,Tsunakazu Fujishiro, Takatomo Fujisawa, ShuseiSato, Satoshi Tabata, Takakazu Kaneko and YasukazuNakamura.
2010.
CyanoBase: the cyanobacteriagenome database update 2010.
Nucleic Acids Re-search, 38:D379?D381.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional Random Fields: Probabilistic Mod-els for Segmenting and Labeling Sequence Data.
Pro-ceedings of the 18th International Conference on Ma-chine Learning, 282?289.Charles Sutton and Andrew McCallum.
2007.
An Intro-duction to Conditional Random Fields for RelationalLearning.
Introduction to Statistical Relational Learn-ing, MIT Press.Yue Wang.
2010.
Developing Robust Protein NameRecognizers Based on a Comparative Analysis of Pro-tein Annotations in Different Corpora.
University ofTokyo, Japan, PhD Thesis.243
