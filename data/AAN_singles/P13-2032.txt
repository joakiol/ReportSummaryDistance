Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 177?182,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsImproving Chinese Word Segmentation on Micro-blog Using RichPunctuationsLongkai Zhang Li Li Zhengyan He Houfeng Wang?
Ni SunKey Laboratory of Computational Linguistics (Peking University) Ministry of Education, Chinazhlongk@qq.com, li.l@pku.edu.cn, hezhengyan.hit@gmail.com,wanghf@pku.edu.cn,sunny.forwork@gmail.comAbstractMicro-blog is a new kind of mediumwhich is short and informal.
While nosegmented corpus of micro-blogs is avail-able to train Chinese word segmentationmodel, existing Chinese word segmenta-tion tools cannot perform equally wellas in ordinary news texts.
In this pa-per we present an effective yet simple ap-proach to Chinese word segmentation ofmicro-blog.
In our approach, we incor-porate punctuation information of unla-beled micro-blog data by introducing char-acters behind or ahead of punctuations,for they indicate the beginning or end ofwords.
Meanwhile a self-training frame-work to incorporate confident instances isalso used, which prove to be helpful.
Ex-periments on micro-blog data show thatour approach improves performance, espe-cially in OOV-recall.1 INTRODUCTIONMicro-blog (also known as tweets in English) isa new kind of broadcast medium in the form ofblogging.
A micro-blog differs from a traditionalblog in that it is typically smaller in size.
Further-more, texts in micro-blogs tend to be informal andnew words occur more frequently.
These new fea-tures of micro-blogs make the Chinese Word Seg-mentation (CWS) models trained on the source do-main, such as news corpus, fail to perform equallywell when transferred to texts from micro-blogs.For example, the most widely used Chinese seg-menter ?ICTCLAS?
yields 0.95 f-score in newscorpus, only gets 0.82 f-score on micro-blog data.The poor segmentation results will hurt subse-quent analysis on micro-blog text.
?Corresponding authorManually labeling the texts of micro-blog istime consuming.
Luckily, punctuations provideuseful information because they are used as indi-cators of the end of previous sentence and the be-ginning of the next one, which also indicate thestart and the end of a word.
These ?natural bound-aries?
appear so frequently in micro-blog texts thatwe can easily make good use of them.
TABLE 1shows some statistics of the news corpus vs. themicro-blogs.
Besides, English letters and digitsare also more than those in news corpus.
Theyall are natural delimiters of Chinese characters andwe treat them just the same as punctuations.We propose a method to enlarge the trainingcorpus by using punctuation information.
Webuild a semi-supervised learning (SSL) frameworkwhich can iteratively incorporate newly labeled in-stances from unlabeled micro-blog data during thetraining process.
We test our method on micro-blog texts and experiments show good results.This paper is organized as follows.
In section 1we introduce the problem.
Section 2 gives detaileddescription of our approach.
We show the experi-ment and analyze the results in section 3.
Section4 gives the related works and in section 5 we con-clude the whole work.2 Our method2.1 PunctuationsChinese word segmentation problem might betreated as a character labeling problem whichgives each character a label indicating its positionin one word.
To be simple, one can use label ?B?to indicate a character is the beginning of a word,and use ?N?
to indicate a character is not the be-ginning of a word.
We also use the 2-tag in ourwork.
Other tag sets like the ?BIES?
tag set are notsuiteable because the puctuation information can-not decide whether a character after punctuationshould be labeled as ?B?
or ?S?
(word with Single177Chinese English Number PunctuationNews 85.7% 0.6% 0.7% 13.0%micro-blog 66.3% 11.8% 2.6% 19.3%Table 1: Percentage of Chinese, English, number, punctuation in the news corpus vs. the micro-blogs.character).Punctuations can serve as implicit labels for thecharacters before and after them.
The characterright after punctuations must be the first characterof a word, meanwhile the character right beforepunctuations must be the last character of a word.An example is given in TABLE 2.2.2 AlgorithmOur algorithm ?ADD-N?
is shown in TABLE 3.The initially selected character instances are thoseright after punctuations.
By definition they are alllabeled with ?B?.
In this case, the number of train-ing instances with label ?B?
is increased while thenumber with label ?N?
remains unchanged.
Be-cause of this, the model trained on this unbal-anced corpus tends to be biased.
This problem canbecome even worse when there is inexhaustiblesupply of texts from the target domain.
We as-sume that labeled corpus of the source domain canbe treated as a balanced reflection of different la-bels.
Therefore we choose to estimate the bal-anced point by counting characters labeling ?B?and ?N?
and calculate the ratio which we denoteas ?.
We assume the enlarged corpus is also bal-anced if and only if the ratio of ?B?
to ?N?
is justthe same to?
of the source domain.Our algorithm uses data from source domain tomake the labels balanced.
When enlarging corpususing characters behind punctuations from textsin target domain, only characters labeling ?B?
areadded.
We randomly reuse some characters label-ing ?N?
from labeled data until ratio?
is reached.We do not use characters ahead of punctuations,because the single-character words ahead of punc-tuations take the label of ?B?
instead of ?N?.
Insummary our algorithm tackles the problem by du-plicating labeled data in source domain.
We de-note our algorithm as ?ADD-N?.We also use baseline feature templates includethe features described in previous works (Sun andXu, 2011; Sun et al, 2012).
Our algorithm is notnecessarily limited to a specific tagger.
For sim-plicity and reliability, we use a simple Maximum-Entropy tagger.3 Experiment3.1 Data setWe evaluate our method using the data fromweibo.com, which is the biggest micro-blog ser-vice in China.
We use the API provided byweibo.com1 to crawl 500,000 micro-blog texts ofweibo.com, which contains 24,243,772 charac-ters.
To keep the experiment tractable, we first ran-domly choose 50,000 of all the texts as unlabeleddata, which contain 2,420,037 characters.
Wemanually segment 2038 randomly selected micro-blogs.We follow the segmentation standard as thePKU corpus.In micro-blog texts, the user names and URLshave fixed format.
User names start with ?
@?, fol-lowed by Chinese characters, English letters, num-bers and ?
?, and terminated when meeting punc-tuations or blanks.
URLs also match fixed pat-terns, which are shortened using ?http://t.cn/?
plus six random English letters or numbers.Thus user names and URLs can be pre-processedseparately.
We follow this principle in followingexperiments.We use the benchmark datasets provided by thesecond International Chinese Word SegmentationBakeoff2 as the labeled data.
We choose the PKUdata in our experiment because our baseline meth-ods use the same segmentation standard.We compare our method with three baselinemethods.
The first two are both famous Chineseword segmentation tools: ICTCLAS3 and Stan-ford Chinese word segmenter4, which are widelyused in NLP related to word segmentation.
Stan-ford Chinese word segmenter is a CRF-based seg-mentation tool and its segmentation standard ischosen as the PKU standard, which is the sameto ours.
ICTCLAS, on the other hand, is a HMM-based Chinese word segmenter.
Another baselineis Li and Sun (2009), which also uses punctua-tion in their semi-supervised framework.
F-score1http://open.weibo.com/wiki2http://www.sighan.org/bakeoff2005/3http://ictclas.org/4http://nlp.stanford.edu/projects/chinese-nlp.shtml\#cws178?
?
?
?
?
?
?
?
?
?
?
?B - - - - - B - - - - -B N B B N B B N B B N BTable 2: The first line represents the original text.
The second line indicates whether each character isthe Beginning of sentence.
The third line is the tag sequence using ?BN?
tag set.ADD-N algorithmInput: labeled data {(xi, yi)li?1}, unlabeled data {xj}l+uj=l+1.1.
Initially, let L = {(xi, yi)li?1} and U = {xj}l+uj=l+1.2.
Label instances behind punctuations in U as ?B?
and add them intoL.3.
Calculate ?B?, ?N?
ratio ?
in labeled data.4.
Randomly duplicate characters whose labels are ?N?
in L to make?B?/?N?= ?5.
Repeat:5.1 Train a classifier f from L using supervised learning.5.2 Apply f to tag the unlabeled instances in U .5.3 Add confident instances from U to L.Table 3: ADD-N algorithm.is used as the accuracy measure.
The recall ofout-of-vocabulary is also taken into consideration,which measures the ability of the model to cor-rectly segment out of vocabulary words.3.2 Main resultsMethod P R F OOV-RStanford 0.861 0.853 0.857 0.639ICTCLAS 0.812 0.861 0.836 0.602Li-Sun 0.707 0.820 0.760 0.734Maxent 0.868 0.844 0.856 0.760No-punc 0.865 0.829 0.846 0.760No-balance 0.869 0.877 0.873 0.757Our method 0.875 0.875 0.875 0.773Table 4: Segmentation performance with differentmethods on the development data.TABLE 4 summarizes the segmentation results.In TABLE 4, Li-Sun is the method in Li andSun (2009).
Maxent only uses the PKU data fortraining, with neither punctuation information norself-training framework incorporated.
The next 4methods all require a 100 iteration of self-training.No-punc is the method that only uses self-trainingwhile no punctuation information is added.
No-balance is similar to ADD N. The only differencebetween No-balance and ADD-N is that the for-mer does not balance label ?B?
and label ?N?.The comparison of Maxent and No-punctuationshows that naively adding confident unlabeled in-stances does not guarantee to improve perfor-mance.
The writing style and word formation ofthe source domain is different from target domain.When segmenting texts of the target domain usingmodels trained on source domain, the performancewill be hurt with more false segmented instancesadded into the training set.The comparison of Maxent, No-balance andADD-N shows that considering punctuation aswell as self-training does improve performance.Both the f-score and OOV-recall increase.
Bycomparing No-balance and ADD-N alone we canfind that we achieve relatively high f-score if weignore tag balance issue, while slightly hurt theOOV-Recall.
However, considering it will im-prove OOV-Recall by about +1.6% and the f-score +0.2%.We also experimented on different size of un-labeled data to evaluate the performance whenadding unlabeled target domain data.
TABLE 5shows different f-scores and OOV-Recalls on dif-ferent unlabeled data set.We note that when the number of texts changesfrom 0 to 50,000, the f-score and OOV both areimproved.
However, when unlabeled data changesto 200,000, the performance is a bit decreased,while still better than not using unlabeled data.This result comes from the fact that the method?ADD-N?
only uses characters behind punctua-179Size P R F OOV-R0 0.864 0.846 0.855 0.75410000 0.872 0.869 0.871 0.76550000 0.875 0.875 0.875 0.773100000 0.874 0.879 0.876 0.772200000 0.865 0.865 0.865 0.759Table 5: Segmentation performance with differentsize of unlabeled datations from target domain.
Taking more texts intoconsideration means selecting more characters la-beling ?N?
from source domain to simulate thosein target domain.
If too many ?N?s are introduced,the training data will be biased against the true dis-tribution of target domain.3.3 Characters ahead of punctuationsIn the ?BN?
tagging method mentioned above,we incorporate characters after punctuations fromtexts in micro-blog to enlarge training set.We alsotry an opposite approach, ?EN?
tag, which uses?E?
to represent ?End of word?, and ?N?
to rep-resent ?Not the end of word?.
In this contrastingmethod, we only use characters just ahead of punc-tuations.
We find that the two methods show sim-ilar results.
Experiment results with ADD-N areshown in TABLE 6 .Unlabeled ?BN?
tag ?EN?
tagData size F OOV-R F OOV-R50000 0.875 0.773 0.870 0.763Table 6: Comparison of BN and EN.4 Related WorkRecent studies show that character sequence la-beling is an effective formulation of Chineseword segmentation (Low et al, 2005; Zhao et al,2006a,b; Chen et al, 2006; Xue, 2003).
Thesesupervised methods show good results, however,are unable to incorporate information from newdomain, where OOV problem is a big challengefor the research community.
On the other handunsupervised word segmentation Peng and Schu-urmans (2001); Goldwater et al (2006); Jin andTanaka-Ishii (2006); Feng et al (2004); Maosonget al (1998) takes advantage of the huge amountof raw text to solve Chinese word segmentationproblems.
However, they usually are less accurateand more complicated than supervised ones.Meanwhile semi-supervised methods have beenapplied into NLP applications.
Bickel et al (2007)learns a scaling factor from data of source domainand use the distribution to resemble target do-main distribution.
Wu et al (2009) uses a Domainadaptive bootstrapping (DAB) framework, whichshows good results on Named Entity Recognition.Similar semi-supervised applications include Shenet al (2004); Daume?
III and Marcu (2006); Jiangand Zhai (2007); Weinberger et al (2006).
Be-sides, Sun and Xu (2011) uses a sequence labelingframework, while unsupervised statistics are usedas discrete features in their model, which prove tobe effective in Chinese word segmentation.There are previous works using punctuations asimplicit annotations.
Riley (1989) uses it in sen-tence boundary detection.
Li and Sun (2009) pro-posed a compromising solution to by using a clas-sifier to select the most confident characters.
Wedo not follow this approach because the initial er-rors will dramatically harm the performance.
In-stead, we only add the characters after punctua-tions which are sure to be the beginning of words(which means labeling ?B?)
into our training set.Sun and Xu (2011) uses punctuation informationas discrete feature in a sequence labeling frame-work, which shows improvement compared to thepure sequence labeling approach.
Our methodis different from theirs.
We use characters afterpunctuations directly.5 ConclusionIn this paper we have presented an effective yetsimple approach to Chinese word segmentation onmicro-blog texts.
In our approach, punctuation in-formation of unlabeled micro-blog data is used,as well as a self-training framework to incorpo-rate confident instances.
Experiments show thatour approach improves performance, especially inOOV-recall.
Both the punctuation information andthe self-training phase contribute to this improve-ment.AcknowledgmentsThis research was partly supported by Na-tional High Technology Research and Devel-opment Program of China (863 Program) (No.2012AA011101), National Natural Science Foun-dation of China (No.91024009) and MajorNational Social Science Fund of China(No.12&ZD227).180ReferencesBickel, S., Bru?ckner, M., and Scheffer, T. (2007).Discriminative learning for differing trainingand test distributions.
In Proceedings of the 24thinternational conference on Machine learning,pages 81?88.
ACM.Chen, W., Zhang, Y., and Isahara, H. (2006).
Chi-nese named entity recognition with conditionalrandom fields.
In 5th SIGHAN Workshop onChinese Language Processing, Australia.Daume?
III, H. and Marcu, D. (2006).
Domainadaptation for statistical classifiers.
Journal ofArtificial Intelligence Research, 26(1):101?126.Feng, H., Chen, K., Deng, X., and Zheng, W.(2004).
Accessor variety criteria for chineseword extraction.
Computational Linguistics,30(1):75?93.Goldwater, S., Griffiths, T., and Johnson, M.(2006).
Contextual dependencies in unsuper-vised word segmentation.
In Proceedings ofthe 21st International Conference on Computa-tional Linguistics and the 44th annual meetingof the Association for Computational Linguis-tics, pages 673?680.
Association for Computa-tional Linguistics.Jiang, J. and Zhai, C. (2007).
Instance weight-ing for domain adaptation in nlp.
In AnnualMeeting-Association For Computational Lin-guistics, volume 45, page 264.Jin, Z. and Tanaka-Ishii, K. (2006).
Unsuper-vised segmentation of chinese text by use ofbranching entropy.
In Proceedings of the COL-ING/ACL on Main conference poster sessions,pages 428?435.
Association for ComputationalLinguistics.Li, Z. and Sun, M. (2009).
Punctuation as im-plicit annotations for chinese word segmenta-tion.
Computational Linguistics, 35(4):505?512.Low, J., Ng, H., and Guo, W. (2005).
A maximumentropy approach to chinese word segmenta-tion.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing,volume 164.
Jeju Island, Korea.Maosong, S., Dayang, S., and Tsou, B.
(1998).Chinese word segmentation without using lex-icon and hand-crafted training data.
In Pro-ceedings of the 17th international confer-ence on Computational linguistics-Volume 2,pages 1265?1271.
Association for Computa-tional Linguistics.Pan, S. and Yang, Q.
(2010).
A survey on trans-fer learning.
Knowledge and Data Engineering,IEEE Transactions on, 22(10):1345?1359.Peng, F. and Schuurmans, D. (2001).
Self-supervised chinese word segmentation.
Ad-vances in Intelligent Data Analysis, pages 238?247.Riley, M. (1989).
Some applications of tree-basedmodelling to speech and language.
In Pro-ceedings of the workshop on Speech and Nat-ural Language, pages 339?352.
Association forComputational Linguistics.Shen, D., Zhang, J., Su, J., Zhou, G., and Tan,C.
(2004).
Multi-criteria-based active learningfor named entity recognition.
In Proceedingsof the 42nd Annual Meeting on Association forComputational Linguistics, page 589.
Associa-tion for Computational Linguistics.Sun, W. and Xu, J.
(2011).
Enhancing chi-nese word segmentation using unlabeled data.In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing,pages 970?979.
Association for ComputationalLinguistics.Sun, X., Wang, H., and Li, W. (2012).
Fast on-line training with frequency-adaptive learningrates for chinese word segmentation and newword detection.
In Proceedings of the 50thAnnual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers),pages 253?262, Jeju Island, Korea.
Associationfor Computational Linguistics.Weinberger, K., Blitzer, J., and Saul, L. (2006).Distance metric learning for large margin near-est neighbor classification.
In In NIPS.
Citeseer.Wu, D., Lee, W., Ye, N., and Chieu, H. (2009).Domain adaptive bootstrapping for named en-tity recognition.
In Proceedings of the 2009Conference on Empirical Methods in Natu-ral Language Processing: Volume 3-Volume3, pages 1523?1532.
Association for Computa-tional Linguistics.Xue, N. (2003).
Chinese word segmentation ascharacter tagging.
Computational Linguisticsand Chinese Language Processing, 8(1):29?48.Zhao, H., Huang, C., and Li, M. (2006a).
An im-proved chinese word segmentation system with181conditional random field.
In Proceedings of theFifth SIGHAN Workshop on Chinese LanguageProcessing, volume 117.
Sydney: July.Zhao, H., Huang, C., Li, M., and Lu, B.
(2006b).Effective tag set selection in chinese word seg-mentation via conditional random field model-ing.
In Proceedings of PACLIC, volume 20,pages 87?94.182
