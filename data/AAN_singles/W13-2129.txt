Proceedings of the 14th European Workshop on Natural Language Generation, pages 200?201,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsThoughtland: Natural Language Descriptions forMachine Learning n-dimensional Error FunctionsPablo Ariel DuboueLes Laboratoires Foulab999 du CollegeMontreal, Quebe?cpablo.duboue@gmail.comAbstractThis demo showcases Thoughtland, anend-to-end system that takes training dataand a selected machine learning model,produces a cloud of points via cross-validation to approximate its error func-tion, then uses model-based clustering toidentify interesting components of the er-ror function and natural language genera-tion to produce an English text summariz-ing the error function.1 IntroductionFor Machine Learning practitioners of supervisedclassification, the task of debugging and improv-ing their classifiers involves repeated iterations oftraining with different parameters.
Usually, ateach stage the trained model is kept as an opaqueconstruct of which only aggregate statistics (pre-cision, recall, etc.)
are investigated.
Thoughtland(Duboue, 2013) improves this scenario by gener-ating Natural Language descriptions for the errorfunction of trained machine learning models.
It isa pipeline with four components:(1) A cross-validation step that uses a machinealgorithm from a given learning library run overa given dataset with a given set of parameters.This component produces a cloud of points in n-dimensions, where n = F + 1, where F is thenumber of features in the training data (the ex-tra dimension is the error value).
(2) A clusteringstep that identifies components within the cloud ofpoints.
(3) An analysis step that compares eachof the components among themselves and to thewhole cloud of points.
(4) A verbalization stepthat describes the error function by means of thedifferent relations identified in the analysis step.2 Structure of the DemoThis demo encompasses a number of trainingdatasets obtained from the UCI Machine Learn-ing repository (attendees can select different train-ing parameters and see together the changes in thetext description).
It might be possible to work withsome datasets provided by the attendee at demotime, if they do not take too long to train and theyhave it available in the regular Weka ARFF format.A Web demo where people can submit ARFFfiles (of up to a certain size) and get the differ-ent text descriptions is will also be available athttp://thoughtland.duboue.net (Fig.
1).
Moreover, theproject is Free Software1 and people can install itand share their experiences on the Website and atthe demo booth.3 An ExampleI took a small data set from the UCI MachineLearning repository, the Auto-Mpg Data2 andtrain on it using Weka (Witten and Frank, 2000).Applying a multi-layer perceptron with two hid-den layers with three and two units, respectively,we achieve an accuracy of 65% and the followingdescription:There are four components and eight di-mensions.
Components One, Two andThree are small.
Components One, Twoand Three are very dense.
ComponentsFour, Three and One are all far fromeach other.
The rest are all at a gooddistance from each other.When using a single hidden layer with eight unitswe obtain an accuracy 65.7%:1https://github.com/DrDub/Thoughtland.2http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/200Figure 1: Web Interface to Thoughtland (composite).There are four components and eight di-mensions.
Components One, Two andThree are small.
Components One, Twoand Three are very dense.
ComponentsFour and Three are far from each other.The rest are all at a good distance fromeach other.As both descriptions are very similar (we haveemphasized the difference, which in the first caseis also an example of our clique-based aggregationsystem), we can conclude that the two systems areperforming quite similarly.
However, if we use asingle layer with only two units, the accuracy low-ers to 58% and the description becomes:There are five components and eight di-mensions.
Components One, Two andThree are small and Component Four isgiant.
Components One, Two and Threeare very dense.
Components One andFour are at a good distance from eachother.
Components Two and Three arealso at a good distance from each other.Components Two and Five are also at agood distance from each other.
The restare all far from each other.4 Final RemarksThoughtland follows the example of Mathematics,where understanding high dimensional objects isan everyday activity, thanks to a mixture of formu-lae and highly technical language.
It?s long termgoal is to mimic these types of descriptions auto-matically for the error function of trained machinelearning models.The problem of describing n-dimensional ob-jects is a fascinating topic which Throughtland juststarts to address.
It follows naturally the long terminterest in NLG for describing 3D scenes (Blocheret al 1992).Thoughtland is Free Software, distributed un-der the terms of the GPLv3+ and it is written inScala, which allow for easy extension in both Javaand Scala and direct access to the many machinelearning libraries programmed in Java.
It containsa straightforward, easy to understand and modifyclassic NLG pipeline based on well understoodtechnology like McKeown?s (1985) schemata andGatt and Reiter?s (2009) SimpleNLG project.
Thispipeline presents a non-trivial NLG applicationthat is easy to improve upon and can be used di-rectly in classroom presentations.ReferencesA.
Blocher, E. Stopp, and T. Weis.
1992.
ANTLIMA-1: Ein System zur Generierung von Bildvorstellun-gen ausgehend von Propositionen.
Technical Re-port 50, University of Saarbru?cken, Informatik.P.A.
Duboue.
2013.
On the feasibility of automaticallydescribing n-dimensional objects.
In ENLG?13.A.
Gatt and E. Reiter.
2009.
SimpleNLG: A realisationengine for practical applications.
In Proc.
ENLG?09.K.R.
McKeown.
1985.
Text Generation: Using Dis-course Strategies and Focus Constraints to Gener-ate Natural Language Text.
Cambridge UniversityPress.Ian H. Witten and Eibe Frank.
2000.
Data Min-ing: Practical Machine Learning Tools and Tech-niques with Java Implementations.
Morgan Kauf-mann Publishers.201
