Proceedings of NAACL-HLT 2013, pages 947?957,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsMulti-Metric Optimization Using Ensemble TuningBaskaran Sankaran, Anoop SarkarSimon Fraser UniversityBurnaby BC.
CANADA{baskaran,anoop}@cs.sfu.caKevin DuhNara Institute of Science & TechnologyIkoma, Nara.
JAPANkevinduh@is.naist.jpAbstractThis paper examines tuning for statistical ma-chine translation (SMT) with respect to mul-tiple evaluation metrics.
We propose severalnovel methods for tuning towards multiple ob-jectives, including some based on ensembledecoding methods.
Pareto-optimality is a nat-ural way to think about multi-metric optimiza-tion (MMO) and our methods can effectivelycombine several Pareto-optimal solutions, ob-viating the need to choose one.
Our bestperforming ensemble tuning method is a newalgorithm for multi-metric optimization thatsearches for Pareto-optimal ensemble models.We study the effectiveness of our methodsthrough experiments on multiple as well assingle reference(s) datasets.
Our experimentsshow simultaneous gains across several met-rics (BLEU, RIBES), without any significantreduction in other metrics.
This contrasts thetraditional tuning where gains are usually lim-ited to a single metric.
Our human evaluationresults confirm that in order to produce betterMT output, optimizing multiple metrics is bet-ter than optimizing only one.1 IntroductionTuning algorithms are used to find the weights for astatistical machine translation (MT) model by min-imizing error with respect to a single MT evalua-tion metric.
The tuning process improves the per-formance of an SMT system as measured by thismetric; with BLEU (Papineni et al 2002) beingthe most popular choice.
Minimum error-rate train-ing (MERT) (Och, 2003) was the first approach inMT to directly optimize an evaluation metric.
Sev-eral alternatives now exist: MIRA (Watanabe et al2007; Chiang et al 2008), PRO (Hopkins and May,2011), linear regression (Bazrafshan et al 2012)and ORO (Watanabe, 2012) among others.However these approaches optimize towards thebest score as reported by a single evaluation met-ric.
MT system developers typically use BLEU andignore all the other metrics.
This is done despitethe fact that other metrics model wide-ranging as-pects of translation: from measuring the translationedit rate (TER) in matching a translation output to ahuman reference (Snover et al 2006), to capturinglexical choices in translation as in METEOR (Lavieand Denkowski, 2009) to modelling semantic simi-larity through textual entailment (Pado?
et al 2009)to RIBES, an evaluation metric that pays attentionto long-distance reordering (Isozaki et al 2010).While some of these metrics such as TER, ME-TEOR are gaining prominence, BLEU enjoys thestatus of being the de facto standard tuning metricas it is often claimed and sometimes observed thatoptimizing with BLEU produces better translationsthan other metrics (Callison-Burch et al 2011).The gains obtained by the MT system tuned ona particular metric do not improve performance asmeasured under other metrics (Cer et al 2010), sug-gesting that over-fitting to a specific metric mighthappen without improvements in translation quality.In this paper we propose a new tuning frameworkfor jointly optimizing multiple evaluation metrics.Pareto-optimality is a natural way to think aboutmulti-metric optimization and multi-metric opti-mization (MMO) was recently explored using thenotion of Pareto optimality in the Pareto-basedMulti-objective Optimization (PMO) approach (Duhet al 2012).
PMO provides several equivalent solu-tions (parameter weights) having different trade-offsbetween the different MT metrics.
In (Duh et al2012) the choice of which option to use rests withthe MT system developer and in that sense their ap-proach is an a posteriori method to specify the pref-erence (Marler and Arora, 2004).In contrast to this, our tuning framework pro-vides a principled way of using the Pareto optimaloptions using ensemble decoding (Razmara et al2012).
We also introduce a novel method of ensem-ble tuning for jointly tuning multiple MT evaluationmetrics and further combine this with the PMO ap-947proach (Duh et al 2012).
We also introduce threeother approaches for multi-metric tuning and com-pare their performance to the ensemble tuning.
Ourexperiments yield the highest metric scores acrossmany different metrics (that are being optimized),something that has not been possible until now.Our ensemble tuning method over multiple met-rics produced superior translations than single met-ric tuning as measured by a post-editing task.HTER (Snover et al 2006) scores in our humanevaluation confirm that multi-metric optimizationcan lead to better MT output.2 Related WorkIn grammar induction and parsing (Spitkovsky et al2011; Hall et al 2011; Auli and Lopez, 2011) haveproposed multi-objective methods based on round-robin iteration of single objective optimizations.Research in SMT parameter tuning has seen asurge of interest recently, including online/batchlearning (Watanabe, 2012; Cherry and Foster, 2012),large-scale training (Simianer et al 2012; Heand Deng, 2012), and new discriminative objec-tives (Gimpel and Smith, 2012; Zheng et al 2012;Bazrafshan et al 2012).
However, few workshave investigated the multi-metric tuning problem indepth.
Linear combination of BLEU and TER is re-ported in (Zaidan, 2009; Dyer et al 2009; Servanand Schwenk, 2011); an alternative is to optimize onBLEU with MERT while enforcing that TER doesnot degrade per iteration (He and Way, 2009).
Stud-ies on metric tunability (Liu et al 2011; Callison-Burch et al 2011; Chen et al 2012) have foundthat the metric used for evaluation may not be thebest metric used for tuning.
For instance, (Mauser etal., 2008; Cer et al 2010) report that tuning on lin-ear combinations of BLEU-TER is more robust thana single metric like WER.The approach in (Devlin and Matsoukas, 2012)modifies the optimization function to include traitssuch as output length so that the hypotheses pro-duced by the decoder have maximal score accordingto one metric (BLEU) but are subject to an outputlength constraint, e.g.
that the output is 5% shorter.This is done by rescoring an N-best list (forest) forthe metric combined with each trait condition andthen the different trait hypothesis are combined us-ing a system combination step.
The traits are in-dependent of the reference (while tuning).
In con-trast, our method is able to combine multiple metrics(each of which compares to the reference) during thetuning step and we do not depend on N-best list (orforest) rescoring or system combination.Duh et.
al.
(2012) proposed a Pareto-based ap-proach to SMT multi-metric tuning, where the lin-ear combination weights do not need to be known inadvance.
This is advantageous because the optimalweighting may not be known in advance.
However,the notion of Pareto optimality implies that multiple?best?
solutions may exist, so the MT system devel-oper may be forced to make a choice after tuning.These approaches require the MT system devel-oper to make a choice either before tuning (e.g.
interms of linear combination weights) or afterwards(e.g.
the Pareto approach).
Our method here is dif-ferent in that we do not require any choice.
Weuse ensemble decoding (Razmara et al 2012) (seesec 3) to combine the different solutions resultingfrom the multi-metric optimization, providing an el-egant solution for deployment.
We extend this ideafurther and introduce ensemble tuning, where themetrics have separate set of weights.
The tuningprocess alternates between ensemble decoding andthe update step where the weights for each metricare optimized separately followed by joint update ofmetric (meta) weights.3 Ensemble DecodingWe now briefly review ensemble decoding (Razmaraet al 2012) which is used as a component in the al-gorithms we present.
The prevalent model of statis-tical MT is a log-linear framework using a vector offeature functions ?
:p(e|f) ?
exp(w ?
?
)(1)The idea of ensemble decoding is to combine sev-eral models dynamically at decode time.
Given mul-tiple models, the scores are combined for each par-tial hypothesis across the different models duringdecoding using a user-defined mixture operation ?.p(e|f) ?
exp(w1 ?
?1 ?
w2 ?
?2 ?
.
.
.
)(2)(Razmara et al 2012) propose several mixtureoperations, such as log-wsum (simple linear mix-ture), wsum (log-linear mixture) and max (choose lo-948cally best model) among others.
The different mix-ture operations allows the user to encode the be-liefs about the relative strengths of the models.
Ithas been applied successfully for domain adaptationsetting and shown to perform better approaches thatpre-compute linear mixtures of different models.4 Multi-Metric OptimizationIn statistical MT, the multi-metric optimizationproblem can be expressed as:w?
= argmaxwg([M1(H), .
.
.
,Mk(H)])(3)where H = N (f ;w)where N (f ;w) is the decoding function generatinga set of candidate hypotheses H based on the modelparameters w, for the source sentences f .
For eachsource sentence fi ?
f there is a set of candidatehypotheses {hi} ?
H .
The goal of the optimiza-tion is to find the weights that maximize the func-tion g(.)
parameterized by different evaluation met-rics M1, .
.
.
,Mk.For the Pareto-optimal based approach such asPMO (Duh et al 2012), we can replace g(?)
abovewith gPMO(?)
which returns the points in the Paretofrontier.
Alternately a weighted averaging functiongwavg(?)
would result in a linear combination of themetrics being considered, where the tuning methodwould maximize the joint metric.
This is similar tothe (TER-BLEU)/2 optimization (Cer et al 2010;Servan and Schwenk, 2011).We introduce four methods based on the aboveformulation and each method uses a different typeof g(?)
function for combining different metrics andwe compare experimentally with existing methods.4.1 PMO EnsemblePMO (Duh et al 2012) seeks to maximize the num-ber of points in the Pareto frontier of the metrics con-sidered.
The inner routine of the PMO-PRO tuningis described in Algorithm 1.
This routine is con-tained within an outer loop that iterates for a fixednumber iterations of decoding the tuning set and op-timizing the weights.The tuning process with PMO-PRO is inde-pendently repeated with different set of weightsfor metrics1 yielding a set of equivalent solutions1For example Duh et al(2012) use five different weightAlgorithm 1 PMO-PRO (Inner routine for tuning)1: Input: Hypotheses H = N (f ;w); Weights w2: Initialize T = {}3: for each f in tuning set f do4: {h} = H(f)5: {M({h})} = ComputeMetricScore({h}, e?
)6: {F} = FindParetoFrontier({M({h})})7: for each h in {h} do8: if h ?
F then add (1, h) to T9: else add (`, h) to T (see footnote 1)10: wp ?
PRO(T ) (optimize using PRO)11: Output: Pareto-optimal weights wp{ps1 , .
.
.
, psn} which are points on the Pareto fron-tier.
The user then chooses one solution by making atrade-off between the performance gains across dif-ferent metrics.
However, as noted earlier this a pos-teriori choice ignores other solutions that are indis-tinguishable from the chosen one.We alleviate this by complementing PMO withensemble decoding, which we call PMO ensemble,in which each point in the Pareto solution is a dis-tinct component in the ensemble decoder.
This ideacan also be used in other MMO approaches such aslinear combination of metrics (gwavg(.))
mentionedabove.
In this view, PMO ensemble is a special caseof ensemble combination, where the decoding is per-formed by an ensemble of optimal solutions.The ensemble combination model introduces newhyperparameters ?
that are the weights of the en-semble components (meta weights).
These ensem-ble weights could set to be uniform in a na??veimplementation.
Or the user can encode her be-liefs or expectations about the individual solutions{ps1 , .
.
.
, psn} to set the ensemble weights (basedon the relative importance of the components).
Fi-nally, one could also include a meta-level tuning stepto set the weights ?.The PMO ensemble approach is graphically il-lustrated in Figure 1; we will also refer to this fig-ure while discussing other methods.2 The orig-settings for metrics (M1,M2), viz.
(0.0, 1.0), (0.3, 0.7),(0.5, 0.5), (0.7, 0.3) and (1.0, 0.0).
They combine the met-ric weights qi with the sentence-level metric scores Mi as` =(?k qkMk)/k where ` is the target value for negativeexamples (the else line in Alg 1) in the optimization step.2The illustration is based on two metrics, metric-1 andmetric-2, but could be applied to any number of metrics.
With-out loss of generality we assume accuracy metrics, i.e.
higher949Metric?2Metric?1Figure 1: Illustration of different MMO approaches involvingtwo metrics.
Solid (red) arrows indicate optimizing two met-rics independently and the dashed (green) arrow optimize themjointly.
The Pareto frontier is indicated by the curve.inal PMO-PRO seeks to maximize the points onthe Pareto frontier (blue curve in the figure) lead-ing to Pareto-optimal solutions.
On the other hand,the PMO ensemble combines the different Pareto-optimal solutions and potentially moving in the di-rection of dashed (green) arrows to some point thathas higher score in either or both dimensions.4.2 Lateen MMOLateen EM has been proposed as a way of jointlyoptimizing multiple objectives in the context of de-pendency parsing (Spitkovsky et al 2011).
It usesa secondary hard EM objective to move away, whenthe primary soft EM objective gets stuck in a localoptima.
The course correction could be performedunder different conditions leading to variations thatare based on when and how often to shift from oneobjective function to another during optimization.The lateen technique can be applied to the multi-metric optimization in SMT by treating the differ-ent metrics as different objective functions.
Whilethe several lateen variants are also applicable for ourtask, our objective here is to improve performanceacross the different metrics (being optimized).
Thus,we restrict ourselves to the style where the searchalternates between the metrics (in round-robin fash-ion) at each iteration.
Since the notion of conver-gence is unclear in lateen setting, we stop after afixed number of iterations optimizing the tuning set.In terms of Figure 1, lateen MMO corresponds to al-ternately maximizing the metrics along two dimen-sions as depicted by the solid arrows.By the very nature of lateen-alternation, themetric score is better.weights obtained at each iteration are likely to bebest for the metric that was optimized in that itera-tion.
Thus, one could use weights from the last kiterations (for lateen-tuning with as many metrics)and then decode the test set with an ensemble ofthese weights as in PMO ensemble.
However inpractice we find the weights to converge and we sim-ply use the weights from the final iteration to decodethe test set in our lateen experiments.4.3 Union of MetricsAt each iteration lateen MMO excludes all but onemetric for optimization.
An alternative would be toconsider all the metrics at each iteration so that theoptimizer could try to optimize them jointly.
Thishas been the general motivation for considering thelinear combination of metrics (Cer et al 2010; Ser-van and Schwenk, 2011) resulting in a joint metric,which is then optimized.However due to the scaling differences betweenthe scores of different metrics, the linear combi-nation might completely suppress the metric hav-ing scores in the lower-range.
As an example, theRIBES scores that are typically in the high 0.7-0.8range, dominate the BLEU scores that is typicallyaround 0.3.
While the weighted linear combinationtries to address this imbalance, they introduce ad-ditional parameters that are manually fixed and notseparately tuned.We avoid this linear combination pitfall by takingthe union of the metrics under which we considerthe union of training examples from all metrics andoptimize them jointly.
Mathematically,w?
= argmaxwg(M1(H)) ?
.
.
.
?
g(Mk(H)) (4)Most of the optimization approaches involve twophases: i) select positive and negative examples andii) optimize parameters to favour positive exampleswhile penalizing negative ones.
In the union ap-proach, we independently generate positive and neg-ative sets of examples for all the metrics and taketheir union.
The optimizer now seeks to move to-wards positive examples from all metrics, while pe-nalizing others.This is similar to the PMO-PRO approach exceptthat here the optimizer tries to simultaneously max-imize the number of high scoring points across all950metrics.
Thus, instead of the entire Pareto frontiercurve in Figure 1, the union approach optimizes thetwo dimensions simultaneously in each iteration.5 Ensemble TuningThese methods, even though novel, under utilize thepower of ensembles as they combine the solutiononly at the end of the tuning process.
We wouldprefer to tightly integrate the idea of ensembles intothe tuning.
We thus extend the ensemble decodingto ensemble tuning.
The feature weights are repli-cated separately for each evaluation metric, whichare treated as components in the ensemble decodingand tuned independently in the optimization step.Initially the ensemble decoder decodes a devset us-ing a weighted ensemble to produce a single N-bestlist.
For the optimization, we employ a two-step ap-proach of optimizing the feature weights (of eachensemble component) followed by a step for tun-ing the meta (component) weights.
The optimizedweights are then used for decoding the devset in thenext iteration and the process is repeated for a fixednumber of iterations.Modifying the MMO representation in Equa-tion 3, we formulate ensemble tuning as:Hens = Nens(f ; {wM};?;?)(5)w?
={argmaxwMiHens | 1?i?k}(6)?
= argmax?g ({Mi(Hens)|1?i?k} ;w?)
(7)Here the ensemble decoder function Nens(.
)is parameterized by an ensemble of weightswM1 , .
.
.
, wMk (denoted as {wM} in Eq 5) for eachmetric and a mixture operation (?).
?
represents theweights of the ensemble components.Pseudo-code for ensemble tuning is shown in Al-gorithm 2.
In the beginning of each iteration (line 2),the tuning process ensemble decodes (line 4) thetuning set using the weights obtained from the pre-vious iteration.
Equation 5 gives the detailed expres-sion for the ensemble decoding, whereHens denotesthe N-best list generated by the ensemble decoder.The method now uses a dual tuning strategy in-volving two phases to optimize the weights.
In thefirst step it optimizes each of the k metrics indepen-dently (lines 6-7) along its respective dimension inAlgorithm 2 Ensemble Tuning Algorithm1: Input: Tuning set f ,Metrics M1, .
.
.
,Mk (ensemble components)Initial weights {wM} ?
wM1 , .
.
.
wMk andComponent (meta) weights ?2: for j = 1, .
.
.
do3: {w(j)M } ?
{wM}4: Ensemble decode the tuning setHens = Nens(f ; {w(j)M };?;?
)5: {wM} = {}6: for each metric Mi ?
{M} do7: w?Mi ?
PRO(Hens, wMi) (use PRO)8: Add w?Mi to {wM}9: ??
PMO-PRO(Hens, {wM}) (Alg 1)10: Output: Optimal weights {wM} and ?the multi-metric space (as shown by the solid arrowsalong the two axes in Figure 1).
This yields a newset of weights w?
for the features in each metric.The second tuning step (line 9) then optimizesthe meta weights (?)
so as to maximize the multi-metric objective along the joint k-dimensional spaceas shown in Equation 7.
This is illustrated by thedashed arrows in the Figure 1.
While g(.)
could beany function that combines multiple metrics, we usethe PMO-PRO algorithm (Alg.
1) for this step.The main difference between ensemble tuning andPMO ensemble is that the former is an ensemblemodel over metrics and the latter is an ensemblemodel over Pareto solutions.
Additionally, PMO en-semble uses the notion of ensembles only for the fi-nal decoding after tuning has completed.5.1 Implementation NotesAll the proposed methods fit naturally within theusual SMT tuning framework.
However, somechanges are required in the decoder to support en-semble decoding and in the tuning scripts for op-timizing with multiple metrics.
For ensemble de-coding, the decoder should be able to use multipleweight vectors and dynamically combine them ac-cording to some desired mixture operation.
Notethat, unlike Razmara et al(2012), our approach usesjust one model but has different weight vectors foreach metric and the required decoder modificationsare simpler than full ensemble decoding.While any of the mixture operations proposedby Razmara et al(2012) could be used, in this pa-951per we use log-wsum ?
the linear combination of theensemble components and log-wmax ?
the combina-tion that prefers the locally best component.
Theseare simpler to implement and also performed com-petitively in their domain adaptation experiments.Unless explicitly noted otherwise, the results pre-sented in Section 6 are based on linear mixture oper-ation log-wsum, which empirically performed betterthan the log-wmax for ensemble tuning.6 ExperimentsWe evaluate the different methods on Arabic-English translation in single as well as multiple ref-erences scenario.
Corpus statistics are shown inTable 1.
For all the experiments in this paper,we use Kriya, our in-house Hierarchical phrase-based (Chiang, 2007) (Hiero) system, and inte-grated the required changes for ensemble decoding.Kriya performs comparably to the state of the art inphrase-based and hierarchical phrase-based transla-tion over a wide variety of language pairs and datasets (Sankaran et al 2012).We use PRO (Hopkins and May, 2011) for op-timizing the feature weights and PMO-PRO (Duhet al 2012) for optimizing meta weights, wher-ever applicable.
In both cases, we use SVM-Rank (Joachims, 2006) as the optimizer.We used the default parameter settings for dif-ferent MT tuning metrics.
For METEOR, we triedboth METEOR-tune and METEOR-hter settingsand found the latter to perform better in BLEU andTER scores, even though the former was marginallybetter in METEOR3 and RIBES scores.
We ob-served the margin of loss in BLEU and TER to out-weigh the gains in METEOR and RIBES and wechose METEOR-hter setting for both optimizationand evaluation of all our experiments.6.1 Evaluation on Tuning SetUnlike conventional tuning methods, PMO (Duh etal., 2012) was originally evaluated on the tuning setto avoid potential mismatch with the test set.
Inorder to ensure robustness of evaluation, they re-decode the devset using the optimal weights fromthe last tuning iteration and report the scores on 1-3This behaviour was also noted by Denkowski and Lavie(2011) in their analysis of Urdu-English system for tunable met-rics task in WMT11.best candidates.Corpus Training size Tuning/ test setISI corpus 1.1 M1664/ 1313 (MTA)1982/ 987 (ISI)Table 1: Corpus Statistics (# of sentences) for Arabic-English.MTA (4-refs) and ISI (1-ref).We follow the same strategy and compare ourPMO-ensemble approach with PMO-PRO (denotedP) and a linear combination4 (denoted L) base-line.
Similar to Duh et al(2012), we usefive different BLEU:RIBES weight settings, viz.
(0.0, 1.0), (0.3, 0.7), (0.5, 0.5), (0.7, 0.3) and(1.0, 0.0), marked L1 through L5 or P1 through P5.The Pareto frontier is then computed from 80 points(5 runs and 15 iterations per run) on the devset.Figure 2(a) shows the Pareto frontier of L and Pbaselines using BLEU and RIBES as two metrics.The frontier of the P dominates that of L for mostpart showing that the PMO approach benefits frompicking Pareto points during the optimization.We use the PMO-ensemble approach to combinethe optimized weights from the 5 tuning runs andre-decode the devset employing ensemble decoding.This yields the points LEns and PEns in the plot,which obtain better scores than most of the indi-vidual runs of L and P. This ensemble approach ofcombining the final weights also generalizes to theunseen test set as we show later.Figure 2(b) plots the change in BLEU during tun-ing in the multiple references and the single refer-ence scenarios.
We show for each baseline method Land P, plots for two different weight settings that ob-tain high BLEU and RIBES scores.
In both datasets,our ensemble tuning approach dominates the curvesof the (L and P) baselines.
In summary, these resultsconfirm that the ensemble approach achieves resultsthat are competitive with previous MMO methodson the devset Pareto curve.
We now provide a morecomprehensive evaluation on the test set.6.2 Evaluation on Test SetThis section contains multi-metric optimization re-sults on the unseen test sets, one test set has multi-ple references and the other has a single-reference.4Linear combination is a generalized version of the com-bined (TER-BLEU)/2 metric and its variants.9520.7650.7660.7670.7680.7690.770.7710.7720.7730.33  0.335  0.34  0.345  0.35RIBESBLEUL1L2L3  L4L5P1   P2 P3P4 P5LEnsPEnsPMO-PROLin-Comb(a) Pareto frontier and BLEU-RIBES scores: MTA 4-refs devset0.260.280.30.320.340.360.382  4  6  8  10  12  14BLEUIterationsL1-mtaL5-mtaP1-mtaP5-mtaEns-Tune-mtaL1-isiL3-isiP1-isiP3-isiEns-Tune-isi(b) Tuning BLEU scores: MTA 4-refs and ISI 1-ref devsetsFigure 2: Devset (redecode): Comparison of Lin-comb (L) and PMO-PRO (P) with Ensemble decoding (Lens and PEns) andEnsemble tuning (Ens-Tune)We plot BLEU scores against other metrics (RIBES,METEOR and TER) and this allows us to comparethe performance of each metric relative to the de-facto standard BLEU metric.Baseline points are identified by single letters Bfor BLEU, T for TER, etc.
and the baseline (single-metric optimized) score for each metric is indicatedby a dashed line on the corresponding axis.
MMOpoints use a series of single letters referring to themetrics used, e.g.
BT for BLEU-TER.
The union ofmetrics method is identified with the suffix ?J?
andlateen method with suffix ?L?
(thus BT-L refers to thelateen tuning with BLEU-TER).
MMO points with-out any suffix use the ensemble tuning approach.Figures 3 and 4(a) plot the scores for the MTA testset with 4-references.
We see noticeable and somestatistically significant improvements in BLEU andRIBES (see Table 2 for BLEU improvements).All our MMO approaches, except for the unionmethod, show gains on both BLEU and RIBES axes.Figures 3(b) and 4(a) show that none of the proposedmethods managed to improve the baseline scores forMETEOR and TER.
However, several of our en-semble tuning combinations work well for both ME-TEOR (BR, BMRTB3, etc.)
and TER (BMRT andBRT) in that they improved or were close to thebaseline scores in either dimension.
We again see inthese figures that the MMO approaches can improvethe BLEU-only tuning by 0.3 BLEU points, withoutmuch drop in other metrics.
This is in tune with thefinding that BLEU could be tuned easily (Callison-Burch et al 2011) and also explains why it remainsApproach and Tuning Metric(s)BLEUMTA ISISingle Objective BaselinesBLEU 36.06 37.20METEOR 35.05 36.91RIBES 33.35 36.60TER 33.92 35.85Ensemble Tuning: 2 MetricsB-M 36.02 37.26B-R 36.15 37.37B-T 35.72 36.31Ensemble Tuning: 3 MetricsB-M-R 36.36 37.37B-M-T 36.22 36.89B-R-T 35.97 36.72Ensemble Tuning: > 3 MetricsB-M-R-T 35.94 36.84B-M-R-T-B3 36.16 37.12B-M-R-T-B3-B2-B1 36.08 37.24Table 2: BLEU Scores on MTA (4 refs) and ISI (1 ref) test setsusing the standard mteval script.
Boldface scores indicate scoresthat are comparable to or better than the baseline BLEU-onlytuning.
Italicized scores indicate statistically significant differ-ences at p-value 0.05 computed with bootstrap significance test.a popular choice for optimizing SMT systems.Among the different MMO methods the ensem-ble tuning performs better than lateen or union ap-proaches.
In terms of the number of metrics beingoptimized jointly, we see substantial gains when us-ing a small number (typically 2 or 3) of metrics.
Re-sults seem to suffer beyond this number; probablybecause there might not be a space that contain so-lution(s) optimal for all the metrics that are jointlyoptimized.We hypothesize that each metric correlates well9530.8080.810.8120.8140.8160.8180.820.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37RIBESBLEUBMRT  PEnsLEnsBMBRBTMRBMRBMTBRTBMRTBMRTB3BMRTB3B2B1BM-JBR-JBT-JBMT-JBM-LBR-LBMR-L(a) BLEU-RIBES scores0.4950.50.5050.510.5150.520.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37METEORBLEUBMRTPEns(BR)LEns(BR)  BM  BRBTMRBMRBMTBRTBMRTB3BMRTB3B2B1BM-JBR-JBT-JBMT-JBM-LBR-LBMR-L(b) BLEU-METEOR scoresFigure 3: MTA 4-refs testset: Comparison of different MMO approaches.
The dashed lines correspond to baseline scores tuned onthe respective metrics in the axes.
The union of metrics method is identified with the suffix J and lateen with suffix L.0.40.4050.410.4150.420.4250.430.4350.33  0.335  0.34  0.345  0.35  0.355  0.36  0.365  0.37nTERBLEUBMRTPEns(BR)LEns(BR)BM   BRBTMR   BMRBMTBRTBMRTBMRTB3BM-JBR-JBT-J BMT-JBM-LBR-LBMR-L(a) MTA (4-refs)0.440.4450.450.4550.460.4650.355  0.36  0.365  0.37  0.375nTERBLEUBMRTPEns(BR)   LEns(BR)BMBRBMRB3MTBRTBMRTBMRTB3BMRTB3B2B1BR-JBR-L(b) ISI (1-ref)Figure 4: BLEU-TER scores: Comparison of different MMO approaches.
We plot nTER (1-TER) scores for easy reading of theplots.
The dashed lines correspond to baseline scores tuned on the respective metrics in the axes.
(in a looser sense) with few others, but not all.
Forexample, union optimizations BR-J and BMT-J per-form close to or better than RIBES and TER base-lines, but get very poor score in METEOR.
On theother hand BM-J is close to the METEOR baseline,while doing poorly on the RIBES and TER.
This be-haviour is also evident from the single-metric base-lines, where R and T-only settings are clearly distin-guished from the M-only system.
It is not clear ifsuch distinct classes of metrics could be bridged bysome optimal solution and the metric dichotomy re-quires further study as this is key to practical multi-metric tuning in SMT.The lateen and union approaches appear to bevery sensitive to the number of metrics and theygenerally perform well for two metrics case andshow degradation for more metrics.
Unlike otherapproaches, the union approach failed to improveover the baseline BLEU and this could be attributedto the conflict of interest among the metrics, whilechoosing example points for the optimization step.The positive example preferred by a particular met-ric could be a negative example for the other metric.This would only confuse the optimizer resulting inpoor solutions.
Our future line of work would be tostudy the effect of avoiding such of conflicting ex-amples in the union approach.For the single-reference (ISI) dataset, we onlyplot the BLEU-TER case in Figure 4(b) due to lackof space.
The results are similar to the multiplereferences set indicating that MMO approaches areequally effective for single references5.
Table 25One could argue that MMO methods require multiple ref-erences since each metric might be picking out a different ref-954MetricSingle-metric Tuning Ensemble TuningB-only M-only B-M-RBLEU 37.89 37.18 39.01HBLEU 51.93 53.59 53.14METEOR 61.31 61.56 61.68HMETEOR 72.35 72.39 72.74TER 0.520 0.532 0.516HTER 0.361 0.370 0.346Table 3: Post-editing Human Evaluation: Regular (untargeted)and human-targeted scores.
Human targeted scores are com-puted against the post-edited reference and regular scores arecomputed with the original references.
Best scores are in bold-face and statistically significant ones (at p = 0.05) are italicized.shows the BLEU scores for our ensemble tuningmethod (for various combinations) and we again seeimprovements over the baseline BLEU-only tuning.6.3 Human EvaluationSo far we have shown that multi-metric optimiza-tion can improve over single-metric tuning on a sin-gle metric like BLEU and we have shown that ourmethods find a tuned model that performs well withrespect to multiple metrics.
Is the output that scoreshigher on multiple metrics actually a better trans-lation?
To verify this, we conducted a post-editinghuman evaluation experiment.
We compared our en-semble tuning approach involving BLEU, METEORand RIBES (B-M-R) with systems optimized forBLEU (B-only) and METEOR (M-only).We selected 100 random sentences (that are atleast 15 words long) from the Arabic-English MTA(4 references) test set and translated them using thethree systems (two single metric systems and BMRensemble tuning).
We shuffled the resulting trans-lations and split them into 3 sets such that each sethas equal number of the translations from three sys-tems.
The translations were edited by three humanannotators in a post-editing setup, where the goalwas to edit the translations to make them as closeto the references as possible, using the Post-EditingTool: PET (Aziz et al 2012).
The annotators werenot Arabic-literate and relied only on the referencetranslations during post-editing.
The identifiers thatlink each translation to the system that generated itare removed to avoid annotator bias.In the end we collated post-edited translations foreach system and then computed the system-levelerence sentence.
Our experiment shows that even with a singlereference MMO methods can work.human-targeted (HBLEU, HMETEOR, HTER)scores, by using respective post-edited translationsas the reference.
First comparing the HTER (Snoveret al 2006) scores shown in Table 3, we seethat the single-metric system optimized for ME-TEOR performs slightly worse than the one op-timized for BLEU, despite using METEOR-hterversion (Denkowski and Lavie, 2011).
Ensembletuning-based system optimized for three metrics (B-M-R) improves HTER by 4% and 6.3% over BLEUand METEOR optimized systems respectively.The single-metric system tuned with M-only set-ting scores high on HBLEU, closely followed by theensemble system.
We believe this to be caused bychance rather than any systematic gains by the M-only tuning; the ensemble system scores high onHMETEOR compared to the M-only system.
WhileHTER captures the edit distance to the targeted ref-erence, HMETEOR and HBLEU metrics capturemissing content words or synonyms by exploitingn-grams and paraphrase matching.We also computed the regular variants (BLEU,METEOR and TER), which are scored against orig-inal references.
The ensemble system outperformedthe single-metric systems in all the three metrics.The improvements were also statistically significantat p-value of 0.05 for BLEU and TER.7 ConclusionWe propose and present a comprehensive study ofseveral multi-metric optimization (MMO) methodsin SMT.
First, by exploiting the idea of ensemble de-coding (Razmara et al 2012), we propose an effec-tive way to combine multiple Pareto-optimal modelweights from previous MMO methods (e.g.
Duh etal.
(2012)), obviating the need for manually tradingoff among metrics.
We also proposed two new vari-ants: lateen-style MMO and union of metrics.We also extended ensemble decoding to a newtuning algorithm called ensemble tuning.
Thismethod demonstrates statistically significant gainsfor BLEU and RIBES with modest reduction in ME-TEOR and TER.
Further, in our human evaluation,ensemble tuning obtains the best HTER among com-peting baselines, confirming that optimizing on mul-tiple metrics produces human-preferred translationscompared to the conventional optimization approachinvolving a single metric.955ReferencesMichael Auli and Adam Lopez.
2011.
Training a log-linear parser with loss functions via softmax-margin.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing, pages 333?343, Edinburgh, Scotland, UK., July.
Association forComputational Linguistics.Wilker Aziz, Sheila Castilho Monteiro de Sousa, andLucia Specia.
2012.
PET: a tool for post-editingand assessing machine translation.
In Proceed-ings of the Eight International Conference on Lan-guage Resources and Evaluation (LREC?12), Istanbul,Turkey, may.
European Language Resources Associa-tion (ELRA).Marzieh Bazrafshan, Tagyoung Chung, and DanielGildea.
2012.
Tuning as linear regression.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the ACL: Human Language Technolo-gies, pages 543?547, Montre?al, Canada.
ACL.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion, pages 22?64, Edinburgh, Scotland, July.
ACL.Daniel Cer, Christopher D. Manning, and Daniel Juraf-sky.
2010.
The best lexical metric for phrase-basedstatistical mt system optimization.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the ACL, pages 555?563.
ACL.Boxing Chen, Roland Kuhn, and Samuel Larkin.
2012.Port: a precision-order-recall mt evaluation metric fortuning.
In Proceedings of the 50th Annual Meetingof the ACL (Volume 1: Long Papers), pages 930?939,Jeju Island, Korea.
ACL.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the ACL: Human Language Technolo-gies, pages 427?436, Montre?al, Canada.
ACL.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 224?233.
ACL.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimization andEvaluation of Machine Translation Systems.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation, Edinburgh, Scotland, July.
ACL.Jacob Devlin and Spyros Matsoukas.
2012.
Trait-basedhypothesis selection for machine translation.
In Pro-ceedings of the 2012 Conference of the North Ameri-can Chapter of the ACL: Human Language Technolo-gies, pages 528?532.
ACL.Kevin Duh, Katsuhito Sudoh, Xianchao Wu, HajimeTsukada, and Masaaki Nagata.
2012.
Learning totranslate with multiple objectives.
In Proceedings ofthe 50th Annual Meeting of the ACL, Jeju Island, Ko-rea.
ACL.Chris Dyer, Hendra Setiawan, Yuval Marton, and PhilipResnik.
2009.
The university of maryland statisticalmachine translation system for the fourth workshop onmachine translation.
In Proc.
of the Fourth Workshopon Machine Translation.Kevin Gimpel and Noah A. Smith.
2012.
Struc-tured ramp loss minimization for machine transla-tion.
In Proceedings of the 2012 Conference ofthe North American Chapter of the ACL: HumanLanguage Technologies, pages 221?231, Montre?al,Canada.
ACL.Keith Hall, Ryan T. McDonald, Jason Katz-Brown, andMichael Ringgaard.
2011.
Training dependencyparsers by jointly optimizing multiple objectives.
InProceedings of the Empirical Methods in Natural Lan-guage Processing, pages 1489?1499.Xiaodong He and Li Deng.
2012.
Maximum expectedbleu training of phrase and lexicon translation mod-els.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 292?301, Jeju Island, Ko-rea.
ACL.Yifan He and Andy Way.
2009.
Improving the objec-tive function in minimum error rate training.
In MTSummit.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of the 2011 Conference on Empir-ical Methods in Natural Language Processing, Edin-burgh, Scotland.
ACL.Hideki Isozaki, Tsutomu Hirao, Kevin Duh, KatsuhitoSudoh, and Hajime Tsukada.
2010.
Automatic evalu-ation of translation quality for distant language pairs.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages 944?952, Cambridge, MA.
ACL.Thorsten Joachims.
2006.
Training linear svms in lineartime.
In Proceedings of the 12th ACM SIGKDD inter-national conference on Knowledge discovery and datamining, pages 217?226.Alon Lavie and Michael J. Denkowski.
2009.
The me-teor metric for automatic evaluation of machine trans-lation.
Machine Translation, 23(2-3):105?115.956Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.
2011.Better evaluation metrics lead to better machine trans-lation.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing.R.
T. Marler and J. S. Arora.
2004.
Survey ofmulti-objective optimization methods for engineer-ing.
Structural and Multidisciplinary Optimization,26(6):369?395, April.Arne Mauser, Sas?a Hasan, and Hermann Ney.
2008.Automatic evaluation measures for statistical machinetranslation system optimization.
In International Con-ference on Language Resources and Evaluation, Mar-rakech, Morocco.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the ACL, pages 160?167.
ACL.Sebastian Pado?, Daniel Cer, Michel Galley, Dan Jurafsky,and Christopher D. Manning.
2009.
Measuring ma-chine translation quality as semantic equivalence: Ametric based on entailment features.
Machine Trans-lation, 23(2-3):181?193.Kishore Papineni, Salim Roukos, Todd Ward, and Wie-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of Asso-ciation of Computational Linguistics, pages 311?318.ACL.Majid Razmara, George Foster, Baskaran Sankaran, andAnoop Sarkar.
2012.
Mixing multiple translationmodels in statistical machine translation.
In Proceed-ings of the 50th Annual Meeting of the ACL, Jeju, Re-public of Korea.
ACL.Baskaran Sankaran, Majid Razmara, and Anoop Sarkar.2012.
Kriya an end-to-end hierarchical phrase-basedmt system.
The Prague Bulletin of Mathematical Lin-guistics (PBML), 97(97):83?98.Christophe Servan and Holger Schwenk.
2011.
Optimis-ing multiple metrics with mert.
Prague Bull.
Math.Linguistics, 96:109?118.Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012.Joint feature selection in distributed stochastic learn-ing for large-scale discriminative training in smt.
InProceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 11?21, Jeju Island, Korea.
ACL.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas, pages 223?231.Valentin I. Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2011.
Lateen EM: Unsupervised training withmultiple objectives, applied to dependency grammarinduction.
In Proceedings of the Empirical Methods inNatural Language Processing, pages 1269?1280.
As-sociation of Computational Linguistics.Taro Watanabe, Jun Suzuki, Hajime Tsukada, and HidekiIsozaki.
2007.
Online large-margin training for sta-tistical machine translation.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 764?773.
ACL.Taro Watanabe.
2012.
Optimized online rank learn-ing for machine translation.
In Proceedings of the2012 Conference of the North American Chapter of theACL: Human Language Technologies, pages 253?262,Montre?al, Canada, June.
ACL.Omar F. Zaidan.
2009.
Z-MERT: A fully configurableopen source tool for minimum error rate training ofmachine translation systems.
The Prague Bulletin ofMathematical Linguistics, 91:79?88.Daqi Zheng, Yifan He, Yang Liu, and Qun Liu.
2012.Maximum rank correlation training for statistical ma-chine translation.
In MT Summit XIII.957
