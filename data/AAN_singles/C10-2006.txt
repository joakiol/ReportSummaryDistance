Coling 2010: Poster Volume, pages 45?53,Beijing, August 2010Benchmarking for syntax-based sentential inferencePaul BedarideINRIA/LORIAUniversite?
Henri Poincare?paul.bedaride@loria.frClaire GardentCNRS/LORIAclaire.gardent@loria.frAbstractWe propose a methodology for investigat-ing how well NLP systems handle mean-ing preserving syntactic variations.
Westart by presenting a method for the semiautomated creation of a benchmark whereentailment is mediated solely by meaningpreserving syntactic variations.
We thenuse this benchmark to compare a seman-tic role labeller and two grammar basedRTE systems.
We argue that the proposedmethodology (i) supports a modular eval-uation of the ability of NLP systems tohandle the syntax/semantic interface and(ii) permits focused error mining and er-ror analysis.1 IntroductionFirst launched in 2005, the Recognising TextualInference Challenge (RTE)1 aims to assess in howfar computer systems can emulate a human beingin determining whether a short text fragment Hreferred to as the hypothesis, follows from or iscontradicted by a text fragment T .
In the RTEbenchmarks, the hypothesis is a short constructedsentence whilst the text fragments are short pas-sages of naturally occurring texts.
As a result, theRTE challenge permits evaluating the capacity ofNLP systems to handle local textual inference onreal data, an enabling technology for any applica-tions involving document interpretation.In this paper, we focus on entailments based onmeaning entailing, syntactic transformations suchas:(1) The man gives the woman the flowers thatsmell nice ?
The flowers which are givento the woman smell nice1http://www.pascal-network.org/Challenges/RTEWe start (Section 2) by motivating the ap-proach.
We argue that the proposed evaluationmethodology (i) interestingly complements theRTE challenge in that it permits a modular, ana-lytic evaluation of the ability of NLP systems tohandle syntax-based, sentential inference and (ii)permits focused error mining and analysis .In Section 3, we go on to describe the bench-mark construction process.
Each item of the con-structed benchmark associates two sentences witha truth value (true or false) indicating whetheror not the second sentence can be understood tofollow from the first.
The construction of thesebenchmark items relies on the use of a gram-mar based surface realiser and we show how thispermits automatically associating with each infer-ence item, an entailment value (true or false) anda detailed syntactic annotation reflecting the syn-tactic constructs present in the two sentences con-stituting each benchmark item.In section 4, we use the benchmark to evaluateand compare three systems designed to recognisemeaning preserving syntactic variations namely,a semantic role labeller, Johan Bos?
NutcrackerRTE system (where the syntax/semantic interfaceis handled by a semantic construction moduleworking on the output of combinatory categorialgrammar parser) and the Afazio system, a hybridsystem combining statistical parsing, symbolic se-mantic role labelling and sentential entailment de-tection using first order logic.
We give the eval-uation figures for each system.
Additionally, weshow how the detailed syntactic annotations au-tomatically associated with each benchmark itemby the surface realiser can be used to identify themost likely source of errors that is, the syntacticconstructs that most frequently co-occur with enentailment recognition error.452 MotivationsArguably focusing on meaning entailing syntac-tic transformations is very weak.
Indeed, one ofthe key conclusions at the second RTE ChallengeWorkshop was that entailment modeling requiresvast knowledge resources that correspond to dif-ferent types of entailment reasoning e.g., ontolog-ical and lexical relationships, paraphrases and en-tailment rules, meaning entailing syntactic trans-formations and last but not least, world knowl-edge.
Further, Manning (2006) has strongly ar-gued against circumscribing the RTE data to cer-tain forms of inference such as for instance, infer-ences based solely on linguistic knowledge.
Fi-nally, it is also often insisted that naturally occur-ring data should be favored over constructed data.While we agree that challenges such as the RTEchallenge are useful in testing systems abilities tocope with real data, we believe there is also roomfor more focused evaluation setups.Focusing on syntax based entailments.
Asmentioned above, syntax based entailment is onlyone of the many inference types involved in deter-mining textual entailment.
Nevertheless, a manualanalysis of the RTE1 data by (Vanderwende et al,2005) indicates that 37% of the examples couldbe handled by considering syntax alone.
Sim-ilarly, (Garoufi, 2007) shows that 37.5% of theRTE2 data does not involve deep reasoning andmore specifically, that 33.8% of the RTE2 data in-volves syntactic or lexical knowledge only.
Hencealthough the holistic, blackbox type of evaluationpracticed in the RTE challenge is undeniably use-ful in assessing the ability of existing systems tohandle local textual inference, a more analytic,modular kind of evaluation targeting syntax-basedentailment reasoning is arguably also of interest.Another interesting feature of the SSI (syntax-based sentential entailment) task we propose isthat it provides an alternative way of evaluatingsemantic role labelling (SRL) systems.
Typically,the evaluation of SRL systems relies on a hand an-notated corpus such as PropBank or the FrameNetcorpus.
The systems precision and recall are thencomputed w.r.t.
this reference corpus.
As has beenrepeatedly argued (Moll and Hutchinson, 2003;Galliers and Jones, 1993), intrinsic evaluationsmay be of very limited value.
For semanticallyoriented tools such as SRL systems, it is importantto also assess their results w.r.t.
the task whichthey are meant support namely reasoning : Dothe semantic representations built by SRL help inmaking the correct inferences ?
Can they be used,for instance, to determine whether a given sen-tence answers a given question ?
or whether thecontent of one sentence follow from that another ?As explained in (Giampiccolo et al, 2007), entail-ment recognition is a first, major step towards an-swering these questions.
Accordingly, instead ofcomparing the representations produced by SRLsystems against a gold standard, the evaluationscheme presented here, permits evaluating themw.r.t.
their ability to capture syntax based senten-tial inference.It is worth adding that, although the present pa-per focuses on entailments strictly based on syn-tax, the proposed methodology should straight-forwardly extend to further types of entailmentsuch as in particular, entailments involving lexi-cal relations (synonymy, antonymy, etc.)
or entail-ments involving more complex semantic phenom-ena such as the interplay between different classesof complement taking verbs, polarity and authorcommitment discussed in (Nairn et al, 2006).This is because as we shall see in section 3, ourapproach is based on an extensive, hand writtengrammar of English integrating syntax and se-mantics.
By modifying the grammar, the lexiconand/or the semantics, data of varying linguistictype and complexity can be produced and used forevaluation.Hand constructed vs. naturally occurring data.Although in the 90s, hand tailored testsuites suchas (Lehmann et al, 1996; Cooper et al, 1995)were deemed useful for evaluating NLP systems,it is today generally assumed that, for evaluationpurposes, naturally occurring data is best.
We ar-gue that constructed data can interestingly com-plement naturally occurring data.To start with, we agree with (Crouch et al,2006; Cohen et al, 2008) that science generallybenefits from combining laboratory and field stud-ies and more specifically, that computational lin-guistics can benefit from evaluating systems on46a combination of naturally occurring and con-structed data.Moreover, constructed data need not be handconstructed.
Interestingly, automating the produc-tion of this data can help provide better data anno-tation as well as better and better balanced datacoverage than both hand constructed data and nat-urally occurring data.
Indeed, as we shall showin section 4, the benchmark creation process pre-sented here supports a detailed and fully auto-mated annotation of the syntactic properties as-sociated with each benchmark item.
As shownin section 5, this in turn allows for detailed er-ror mining making it possible to identify the mostlikely causes of system errors.
Additionally, theproposed methodology permits controlling oversuch benchmark parameters as the size of the dataset, the balance between true and false entail-ments, the correlation between word overlap andentailment value and/or the specific syntactic phe-nomena involved.
This is in contrast with the RTEdata collection process where ?the distribution ofexamples is arbitrary to a large extent, being de-termined by manual selection2?
(Giampiccolo etal., 2007).
As has been repeatedly pointed out(Burchardt et al, 2007; Garoufi, 2007), the RTEdatasets are poorly balanced w.r.t., both the fre-quency and the coverage of the various phenom-ena interacting with textual inference.3 BenchmarkWe now present the content of an SSI benchmarkand the method for constructing it.An SSI benchmark item (cf.
e.g., Figure 1) con-sists of two sentences and a truth value (true orfalse) indicating whether or not the second sen-tence can be understood to follow from the first.In addition, each sentence is associated with a de-tailed syntactic annotation describing the syntac-tic constructs present in the sentence.The benchmark construction process consistsof two main steps.
First, a generation bank isbuilt.
Second, this generation bank is drawn upon2The short texts of the RTE benchmarks are automaticallyextracted from real texts using different applications (e.g.,Q/A, summarisation, information extraction, information re-trieval systems) but the query used to retrieve these texts iseither constructed manually or post-edited.T: The man gives the woman the flowers that smellnicesmell:{n0Va1,active,relSubj,canAdj}give:{n0Vn2n1,active,canSubj,canObj,canIObj}H: The flowers are given to the womangive:{n0Vn1Pn2,shortPassive,canSubj,canIObj}Entailment: TRUEFigure 1: An SSI Benchmark itemto construct a balanced data set for SSI evaluation.We now describe each of these processes in turn.Constructing a generation bank We use theterm ?generation bank?
to refer to a dataset whoseitems are produced by a surface realiser i.e., asentence generator.
A surface realiser in turnis a program which associates with a given se-mantic representation, the set of sentences ver-balising the meaning encoded by that representa-tion.
To construct our generation bank, we use theGenI surface realiser (Gardent and Kow, 2007).This realiser uses a Feature based Tree AdjoiningGrammar (FTAG) augmented with a unificationsematics as proposed in (Gardent and Kallmeyer,2003) to produce all the sentences associated bythe grammar with a given semantic representa-tion.
Interestingly, the FTAG used has been com-piled out of a factorised representation and as aresult, each elementary grammar unit (i.e., ele-mentary FTAG tree) and further each parse tree, isassociated with a list of items indicating the syn-tactic construct(s) captured by that unit/tree3.
Inshort, GenI permits associating with a given se-mantics, a set of sentences and further for each ofthese sentences, a set of items indicating the syn-tactic construct(s) present in the syntactic tree ofthat sentence.
For instance, the sentences and thesyntactic constructs associated by GenI with thesemantics given in (2) are those given in (3).
(2) A:give(B C D E) G:the(C) F:man(C)H:the(D) I:woman(D) J:the(E) K:flower(E)L:passive(B) L:smell(M E N) O:nice(N)(3) a.
The flower which smells nice is given tothe woman by the man3Space is lacking to give a detailed explanation of thisprocess here.
We refer the reader to (Gardent and Kow, 2007)for more details on how GenI associates with a given seman-tics, a set of sentences and for each sentence a set of itemsindicating the syntactic construct(s) present in the syntactictree of that sentence.47give:n0Vn1Pn2-Passive-CanSubj-ToObj-ByAgt,smell:n0V-active-OvertSubjectRelativeb.
The flower which smells nice is giventhe woman by the mangive:n0Vn2n1-Passive,smell:n0V-active-OvertSubjectRelativec.
The flower which is given the woman bythe man smells nicegive:n0Vn2n1-Passive-CovertSubjectRelative,smell:n0V-actived.
The flower which is given to the womanby the man smells nicegive:n0Vn1Pn2-Passive-OvertSubjectRelative,smell:n0V-activee.
The flower that smells nice is given tothe woman by the mangive:n0Vn1Pn2-Passive,smell:n0V-CovertSubjectRelativef.
The flower that smells nice is given thewoman by the mangive:n0Vn2n1-Passive,smell:n0V-CovertSubjectRelativeg.
The flower that is given the woman bythe man smells nicegive:n0Vn2n1-Passive-CovertSubjectRelative,smell:n0V-activeh.
The flower that is given to the woman bythe man smells nicegive:n0Vn1Pn2-Passive-CovertSubjectRelative,smell:n0V-activeThe tagset of syntactic annotation covers the sub-categorisation type of the verb, a specification ofthe verb mood and a description of how argumentsare realised.The semantic representation language used isa simplified version of the flat semantics usedin e.g., (Copestake et al, 2005) which is suf-ficient for the cases handled in the present pa-per.
The grammar and therefore the generator,can however easily be modified to integrate themore sophisticated version proposed in (Gardentand Kallmeyer, 2003) and thereby provide an ad-equate treatment of scope.Constructing an SSI benchmark.
Given ageneration bank, false and true sentential entail-ment pairs can be automatically produced by tak-ing pairs of sentences ?S1, S2?
and comparingtheir semantics: if the semantics of S2 is entailedby the semantics of S1, the pair is marked as TRUEelse as FALSE.
The syntactic annotations asso-ciated in the generation bank with each sentenceare carried over to the SSI benchmark thereby en-suring that the overall information contained ineach SSI benchmark is as illustrated in Figure 1namely, two pairs of syntactically annotated sen-tences and a truth value indicating (non) entail-ment.To determine whether a sentence textually en-tails another we translate their flat semantic rep-resentation into first order logic and check forlogical entailment.
Differences in semantic rep-resentations which are linked to functional sur-face differences such as active/passive or thepresence/absence of a complementizer (John seesMary leaving/John sees that Mary leaves) aredealt with by (automatically) removing the corre-sponding semantic literals from the semantic rep-resentation before translating it to first order logic.In other words, active/passive variants of the samesentence are deemed semantically equivalent.Note that contrary to what is assumed in theRTE challenge, entailment is here logical ratherthan textual (i.e., determined by a human) entail-ment.
By using logical, rather than textual (i.e.,human based) entailment, it is possible that somecases of syntax mediated textual entailments arenot taken into account.
However, intuitively, itseems reasonable to assume that for most of theentailments mediated by syntax alone, logical andtextual entailments coincide.3.1 The SSI benchmarkUsing the methodology just described, we firstproduced a generation bank of 226 items using 81input formula distributed over 4 verb types.
Fromthis generation bank, a total of 6 396 SSI-pairswere built with a ratio of 42.6% true and 57.4%false entailments.For our experiment, we extracted from this SSI-suite, 1000 pairs with an equal proportion of trueand false entailments and a 7/23/30/40 distribu-tion of four subcategorisation types namely, ad-jectival predicative (n0Va1 e.g., The cake tastesgood), intransitive (n0V), transitive (n0Vn1) andditransitive (n0Vn2n1)4.
We furthermore con-4The subcategorisation type of an SSI item is determinedmanually and refers either to the main verb if the sentence is48strained the suite to respect a neutral correlationbetween word overlap and entailment.
Following(Garoufi, 2007), we define this correlation as fol-lows.
The word overlap wo(T,H) between twosentences T and H is the ratio of common lem-mas between T and H on the number of lemmasin H (non content words are ignored).
If entail-ment holds, the word overlap/entailment correla-tion value of the sentence pair is wo(T,H).
Oth-erwise it is 1 ?
wo(T,H).
The 1000 items of theSSI suite used in our experiment were chosen insuch a way that the word overlap/entailment cor-relation value of the SSI suite is 0.49.In sum, the SSI suite used for testing exhibitsthe following features.
First, it is balanced w.r.t.entailment.
Second, it displays good syntacticvariability based both on the constrained distribu-tion of the four subcategorisation types and on theuse of the XTAG grammar to construct sentencesfrom abstract representations (cf.
the paraphrasesin (3) generated by GenI from the representationgiven in (2)).
Third, it contains 1000 items andcould easily be extended to cover more and morevaried data.
Fourth, it is specifically tailored tocheck systems on their ability to deal with syntaxbased sentential entailment: word overlap is high,syntactic variability is provided and the correla-tion between word overlap and entailment is notbiased.4 System evaluation and comparisonSRL and grammar based systems equipped witha compositional semantics are primary targets foran SSI evaluation.
Indeed these systems aim toabstract away from syntactic differences by pro-ducing semantic representations of a text whichcapture predicate/argument relations independentof their syntactic realisation.We evaluated three such systems on the SSIbenchmark namely, NutCracker, (Johansson andNugues, 2008)?s Semantic Role Labeller and theAfazio RTE system.4.1 SystemsNutcracker Nutcracker is a system for recog-nising textual entailment which uses deep seman-a clause or to the embedded verb if the sentence is a complexsentence.tic processing and automated reasoning.
Deep se-mantic processing associates each sentence with aDiscourse Representation Structure (DRS (Kampand Reyle, 1993)) by first, using a statisticalparser to build the syntactic parse of the sentenceand second, using a symbolic semantic construc-tion module to associate a DRS with the syn-tactic parse.
Entailment between two DRSs isthen checked by translating this DRS into a first-order logical (FOL) formula and first trying tofind a proof.
If a proof is found then the en-tailment is set to true.
Otherwise, Nutcrackerbacks off with a word overlap module computedover an abstract representation of the input sen-tences and taking into account WordNet relatedinformation.
Nutcracker was entered in the firstRTE challenge and scored an accuraccy (percent-age of correct judgments) of 0.562 when used asis and 0.612 when combined with machine learn-ing techniques.
For our experiment, we use theonline version of Nutcracker and the given defaultparameters.Afazio Like Nutcracker, the Afazio systemcombines a statistical parser (the Stanford parser)with a symbolic semantic component.
This com-ponent pipelines several rewrite modules whichtranslate the parser output into a first order logicformula intended to abstract away from sur-face differences and assign syntactic paraphrasesthe same representation (Bedaride and Gardent,2009).
Special emphasis is placed on captur-ing syntax based equivalences such as syntac-tic (e.g., active/passive) variations, redistributionsand noun/verb variants.
Once the parser out-put has been normalised into predicate/argumentrepresentations capturing these equivalences, theresulting structures are rewritten into first orderlogic formulae.
Like Nutcracker, Afazio checksentailment using first order automated reasonersnamely, Equinox and Paradox 5.SRL (Johansson and Nugues, 2008)?s seman-tic role labeller achieved the top score in theclosed CoNLL 2008 challenge reaching a labeledsemantic F1 of 81.65.
To allow for compari-son with Nutcracker and Afazio, we adapted the5http://www.cs.chalmers.se/?koen/folkung/49rewrite module used in Afazio to rewrite Pred-icate/Argument structures into FOL formula insuch a way as to fit (Johansson and Nugues,2008)?s SRL output.
We then use FOL automatedreasoner to check entailment.4.2 Evaluation scheme and resultsThe results obtained by the three systems aresummarised in Table 1.
TP (true positives) isthe number of entailments recognised as such bythe system and TN (true negatives) of non entail-ments.
Conversely, FN and FP indicate how oftenthe systems get it wrong: FP is the number of nonentailments labelled as entailments by the systemand FN, the number of entailments labelled as nonentailments.
?ERROR?
refers to cases where theCCG parser used by Nutcracker fails to find aparse.
The last three columns indicate the over-all ability of the systems to recognise false entail-ments (TN/N with N the number of false entail-ment in the benchmark), true entailments (TP/P)and all true and false entailment (Precision).Overall, Afazio outperforms both Nutcrackerand the SRL system.
This is unsurprising sincecontrary to these other two systems, Afazio wasspecifically designed to handle syntax based sen-tential entailment.
Its strength is that it combinesa full SRL system with a semantic constructionmodule designed for entailment detection.
Moresurprisingly, the CCG parser used by Nutcrackeroften fails to find a parse.The SRL system has a high rate of false nega-tives.
Using the error mining technique presentedin the next section, we found that the most sus-picious syntactic constructs all included a rela-tivised argument.
A closer look at the analysesshowed that this was due to the fact that SRL sys-tems fail to identify the antecedent of a relativepronoun, an identification that is necessary for en-tailment checking.
Another important differencewith Afazio is that the SRL system produces asingle output.
In contrast, Afazio checks entail-ment for any of the pairs of semantic representa-tions derived from the first 9 parses of the Stan-ford parser.
The number 9 was determined em-pirically and proved to yield the best results over-all although as we shall see in the error miningsection, taking such a high number of parses intoaccount often leads to incorrect results when thehypothesis (H) is short.Nutcracker, on the other hand, produces manyfalse positives.
This is in part due to cases wherethe time bound is reached and the word overlapbackoff triggered.
Since the overall word overlapof the SSI suite is high, the backoff often predictsan entailment where in fact there is none (for in-stance, the pair ?John gave flowers to Mary/Marygave flowers to John has a perfect word overlapbut entailment does not hold).
When removingthe backoff results i.e., when assigning all backoffcases a negative entailment value, overall preci-sion approximates 60%.
In other words, on casessuch as those present in the SSI benchmark whereword overlap is generally high but the correla-tion between word overlap and entailment value isneutral, Nutcracker should be used without back-off.5 Finding the source of errorsThe annotations contained in the automaticallyconstructed testsuite can help identify the mostlikely sources of failures.
We use (Sagot and deLa Clergerie, 2006)?s suspicion rate to computethe probability that a given pair of sets of syntac-tic tags is responsible for an RTE detection failure.The tag set pairs with highest suspicion rate in-dicate which syntactic phenomena often cooccurswith failure.More specifically, we store for each testsuiteitem (T,H), all tag pairs (tj , hk) such that the syn-tactic tags tj and hk are associated with the samepredicate Pi but tj occurs in T and hk in H. That is,we collect the tag pairs formed by taking the tagsthat label the occurrence of the same predicate onboth sides of the implication.
If a predicate occursonly in H then for each syntactic tag hk labellingthis predicate, the pair (nil, hk) is created.
Con-versely, if a predicate occurs only in T, the pair(tj , nil) is added.
Furthermore, the tags describ-ing the subcategorisation type and the form of theverb are grouped into a single tag so as to reducethe tagset and limit data sparseness.
For instance,given the pair of sentences in Figure (1), the fol-lowing tag pairs are produced:(n0Va1:active:relSubj, nil)(n0Va1:active:canAdj, nil)50system ERROR TN FN TP FP TN/N TP/P Precafazio 0 360 147 353 140 0.7200 0.7060 71.3%nutcracker 155 22 62 312 449 0.0467 0.8342 39.5% (60% w/o B.O.
)srl 0 487 437 63 13 0.9740 0.1260 55.0%Table 1: Results of the three systems on the SSI-testsuite ( TN = true negatives, FN = false negatives,TP = true positives, FP = false positives, N = TN + FP, P = TP + FN, Prec = Precision, ERROR: noparse tree found)(n0Vn2n1:active:canSubj,n0Vn1Pn2:shortPassive:canSubj)(n0Vn2n1:active:canSubj,n0Vn1Pn2:shortPassive:canIObj)(n0Vn2n1:active:canObj,n0Vn1Pn2:shortPassive:canSubj)(n0Vn2n1:active:canObj,n0Vn1Pn2:shortPassive:canIObj)(n0Vn2n1:active:canIObj,n0Vn1Pn2:shortPassive:canSubj)(n0Vn2n1:active:canIObj,n0Vn1Pn2:shortPassive:canIObj)For each tag pair, we then compute the suspi-cion rate of that pair using (Sagot and de La Clerg-erie, 2006)?s fix point algorithm.
To also take intoaccount pairs of sets of tags (rather than just pairsof single tags), we furthermore preprocess the dataaccording to (de Kok et al, 2009)?s proposal forhandling n-grams.Computing the suspicion rate of a tag pair.The error mining?s suspicion rate algorithm of(Sagot and de La Clergerie, 2006) is a fix point al-gorithm used to detect the possible cause of pars-ing failures.
We apply this algorithm to the pairof annotated sentences resulting from running thethree systems on the automatically created test-suite as follows.
Each such pair consists of a pairof sentences, a set of tag pairs, an entailment value(true or false) and a result value namely FP (falsepositive), FN (false negative), TP (true positive) orTN (true negative).
To search for the most likelycauses of failure, we consider separately entail-ments from non entailments.
If entailment holds,the suspicion rate of a sentence pair is 0 for truepositive and 1 for false positives.
Conversely, ifentailment does not hold, the suspicion rate of thesentence pair is 0 for true negatives and 1 for falsenegatives.The aim is to detect the tag pair most likely tomake entailment detection fail6.
The algorithm it-erates between tag pair occurrences and tag pairforms, redistributing probabilities with each itera-tion as follows.
Initially, all tag pair occurrences6We make the simplifying hypothesis that for each entail-ment not recognised, a single tag pair or tag pair n-gram isthe cause of the failure.in a given sentence have the same suspicion ratenamely, the suspicion rate of the sentence (1 if theentailment could not be recognised, 0 otherwise)divided by the number of tag pair occurrences inthat sentence.
Next, the suspicion rate of a tagpair form is defined as the average suspicion rateof all occurrences of that tag pair.
The suspicionrate of a tag pair occurrence within each particularsentence is then recalculated as the suspicion rateof that tag pair form normalised by the suspicionrates of the other tag pair forms occurring withinthe same sentence.
The iteration stops when theprocess reaches a fixed point where the suspicionrates have stabilised.Extending the approach to pairs of tag sets.To account for entailment recognition due to morethan one tag pair, we follow (de Kok et al, 2009)and introduce a preprocessing step which first, ex-pands tag pair unigrams to tag pair n-grams whenthere is evidence that it is useful that is, whenan n-gram has a higher suspicion rate than eachof its sub n-grams.
For this preprocessing, thesuspicion of a tag pair t is defined as the ratioof t occurrences in unrecognised entailments andthe total number of t occurrences in the corpus.To compensate for data sparseness, an additionalexpansion factor is used which depends on thefrequency of an n-gram and approaches one forhigher frequency.
In this way, long n-grams thathave low frequency are not favoured.
The longerthe n-gram is, the more frequent or the more sus-picious it needs to be in order to be selected by thepreprocessing step.We apply this extension to the SSI setting.
Wefirst extend the set of available tag pairs with tagset pairs such that the suspicion rate of these pairsis higher that the suspicion rate of each of thesmaller tagset pairs that can be constructed fromthese sets.
We then apply (Sagot and de La Clerg-51n0Vs1:act:CanSubj nil 0.85n0Vn1:act:CanObj nil 0.46n0V:betaVn nil 0.28Table 2: The first 3 suspects for false positivesn0V:act n0V:act:RelCSubj 0.73n0Vs1:act:CanSubj n0Vs1:act:CanSubj 0.69n0V:act:RelOSubj n0V:betaVnn0Vs1:act:CanSub n0Vs1:act:CanSubj 0.69n0V:act:CanSubj n0V:betaVnTable 3: The first 3 suspects for false negativeserie, 2006)?s fix point algorithm to compute thesuspicion rate of the resulting tag pairs and tag setspairs.Results and discussion.
We now show how er-ror mining can help shed some light on the mostprobable sources of error when using Afazio.For false positives (non entailment labelledas entailment by Afazio), the 3 most suspecttag pairs are given in Table 2.
The first pair(n0Vs1:act:CanSubj,nil) points out to cases suchas Bill sees the woman give the flower to the man/ The man gives the flower to the woman.
whereT contains a verb with a sentential argument notpresent in H. In such cases, we found that the sen-tential argument in T is usually incorrectly anal-ysed, the analyses produced are fragmented andentailment goes through.
Similarly, the secondsuspect (n0Vn1:act:CanObj,nil) points to casessuch as a man sees Lisa dancing / a man dances,where the transitive verb in T has no counterpart inH.
Here the high number of analyses relied on byAfazio together with the small size of H leads toentailment detection: because we consider manypossible analyses for T and H and because H isvery short, one pair of analyses is found to match.Finally, the third suspect (n0V:betaVn,nil) pointsto cases such as Bill insists for the singing man todance / Bill dances where the gerund is wronglyanalysed and a relation is incorrectly establishedby the parser between Bill and dance (in H).For false negatives, the first suspect indicatesincorrect analyses for cases where an intransitivewith canonical subject in H is matched by an in-transitive with covert relative subject (e.g., Billsees the woman give the flower to the man / theman gives the flower to the woman.).
The sec-ond suspect points to cases such as Bill insists forthe man who sings to dance / Bill insists that thesinging man dances.
where an embedded verbwith relative overt subject in T (sings) is matchedin H by an embedded gerund.
Similarly, the thirdsuspect points to embedded verbs with canonicalsubject matched by gerund verbs as in the manwho Bill insists that dances sings / Bill insists thatthe singing man dances.6 ConclusionThe development of a linguistically principledtreatment of the RTE task requires a clear under-standing of the strength and weaknesses of RTEsystems w.r.t.
to the various types of reasoning in-volved.
The main contribution of this paper is thespecification of an evaluation methodology whichpermits a focused evaluation of syntax based rea-soning on arbitrarily many inputs.
As the resultsshow, there is room for improvment even on thatmost basic level.
In future work, we plan to extendthe approach to other types of inferences requiredfor textual entailment recognition.
A more so-phisticated compositional semantics in the gram-mar used by the sentence generator would allowfor entailments involving more complex semanticphenomena such as the interplay between implica-tive verbs, polarity and downward/upward mono-tonicity discussed in (Nairn et al, 2006).
For in-stance, it would allow for sentence pairs such asEd did not forget to force Dave to leave / Daveleft to be assigned the correct entailment value.ReferencesBedaride, P. and C. Gardent.
2009.
Noun/verb entail-ment.
In 4th Language and Technology Conference,Poznan, Poland.Burchardt, A., N. Reiter, S. Thater, and A. Frank.2007.
A semantic approach to textual entailment:System evaluation and task analysis.
In Proceed-ings of the ACL-PASCAL Workshop on Textual En-tailment and Paraphrasing, pages 10?16.Cohen, K., W. Baumgartner, and L. Hunter.
2008.Software testing and the naturally occurring data as-sumption in natural language processing.
In Proc.of ?Software engineering, testing, and quality as-surance for natural language processing ACL Work-shop?.52Cooper, R., R. Crouch, J. van Eijck, C. Fox, J. van Gen-abith, J. Jaspars, H. Kamp, M. Pinkal, D. Milward,M.
Poesio, and S. Pulman.
1995.
A framework forcomputational semantics, FraCaS.
Technical report.MS.
Stanford University.Copestake, A., D. Flickinger, C. Pollard, and I. A.Sag.
2005.
Minimal recursion semantics: an intro-duction.
Research on Language and Computation,3.4:281?332.Crouch, R., L. Karttunen, and A. Zaenen.
2006.
Cir-cumscribing is not excluding: A reply to manning.MS.
Palo Alto Research Center.de Kok, D., J. Ma, and G. van Noord.
2009.
A gen-eralized method for iterative error mining in parsingresults.
In Proceedings of the 2009 Workshop onGrammar Engineering Across Frameworks (GEAF2009), pages 71?79, Suntec, Singapore, August.
As-sociation for Computational Linguistics.Galliers, J. R. and K. Sparck Jones.
1993.
Evaluat-ing natural language processing systems.
Technicalreport, Computer Laboratory, University of Cam-bridge.
Technical Report 291.Gardent, C. and L. Kallmeyer.
2003.
Semantic con-struction in ftag.
In Proceedings of the 10th meet-ing of the European Chapter of the Association forComputational Linguistics, Budapest, Hungary.Gardent, C. and E. Kow.
2007.
A symbolic approachto near-deterministic surface realisation using treeadjoining grammar.
In ACL07.Garoufi, K. 2007.
Towards a better understanding ofapplied textual entailment: Annotation and evalua-tion of the rte-2 dataset.
Master?s thesis, SaarlandUniversity, Saarbrcken.Giampiccolo, D., B. Magnini, I. Dagan, and B. Dolan.2007.
The third pascal recognizing textual en-tailment challenge.
In Proceedings of the ACL-PASCAL Workshop on Textual Entailment and Para-phrasing, pages 1?9.Johansson, R. and P. Nugues.
2008.
Dependency-based syntactic-semantic analysis with propbankand nombank.
In CoNLL ?08: Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, pages 183?187, Morristown, NJ,USA.
Association for Computational Linguistics.Kamp, H. and U. Reyle.
1993.
From Discourseto Logic.
Introduction to Modeltheoretic Semanticsof Natural Language, Formal Logic and DiscourseRepresentation Theory.
Kluwer.Lehmann, S., S. Oepen, H. Baur, O. Lbdkan, andD.
Arnold.
1996. tsnlp ?
test suites for naturallanguage processing.
In In J. Nerbonne (Ed.
), Lin-guistic Databases.
CSLI Publications.Manning, C. D. 2006.
Local textual inference: It?shard to circumscribe, but you know it when you seeit - and nlp needs it.
MS. Stanford University.Moll, D. and B. Hutchinson.
2003.
Intrinsic versusextrinsic evaluations of parsing systems.
In Pro-ceedings European Association for ComputationalLinguistics (EACL), workshop on Evaluation Initia-tives in Natural Language Processing, Budapest.Nairn, R., C. Condoravdi, and L. Kartunen.
2006.Computing relative polarity for textual inference.
InProceedings of ICoS-5 (Inference in ComputationalSemantics), Buxton, UK.Sagot, B. and E. de La Clergerie.
2006.
Error miningin parsing results.
In Proceedings of ACL-CoLing06, pages 329?336, Sydney, Australie.Vanderwende, L., D. Coughlin, and B. Dolan.
2005.What syntax can contribute in entailment task.
InProceedings of the First PASCAL RTE Workshop,pages 13?17.53
