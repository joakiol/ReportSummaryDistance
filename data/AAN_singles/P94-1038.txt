Similar ity-Based Est imat ion of Word CooccurrenceProbabi l i t iesIdo Dagan Fernando PereiraAT&T Bell Laboratories600 Mountain Ave.Murray Hill, NJ 07974, USAdagan?research ,  a t t .
compereira?research, att.
comAbstractIn many applications of natural language processing itis necessary to determine the likelihood of a given wordcombination.
For example, a speech recognizer mayneed to determine which of the two word combinations"eat a peach" and "eat a beach" is more likely.
Statis-tical NLP methods determine the likelihood of a wordcombination according to its frequency in a training cor-pus.
However, the nature of language is such that manyword combinations are infrequent and do not occur in agiven corpus.
In this work we propose a method for es-t imating the probability of such previously unseen wordcombinations using available information on "most sim-ilar" words.We describe a probabilistic word association modelbased on distributional word similarity, and apply itto improving probability estimates for unseen word bi-grams in a variant of Katz's back-off model.
Thesimilarity-based method yields a 20% perplexity im-provement in the prediction of unseen bigrams and sta-tistically significant reductions in speech-recognition er-ror.IntroductionData sparseness is an inherent problem in statisticalmethods for natural language processing.
Such meth-ods use statistics on the relative frequencies of config-urations of elements in a training corpus to evaluatealternative analyses or interpretations of new samplesof text or speech.
The most likely analysis will be takento be the one that contains the most frequent config-urations.
The problem of data sparseness arises whenanalyses contain configurations that never occurred inthe training corpus.
Then it is not possible to estimateprobabilities from observed frequencies, andsome otherestimation scheme has to be used.We focus here on a particular kind of configuration,word cooccurrence.
Examples of such cooccurrencesinclude relationships between head words in syntacticconstructions (verb-object or adjective-noun, for exam-ple) and word sequences (n-grams).
In commonly usedmodels, the probability estimate for a previously un-seen cooccurrence is a function of the probability esti-Lillian LeeDivision of Applied SciencesHarvard University33 Oxford St. Cambridge MA 02138, USAl l ee?das ,  harvard ,  edumates for the words in the cooccurrence.
For example,in the bigram models that we study here, the probabil-ity P(w21wl) of a conditioned word w2 that has neveroccurred in training following the conditioning word wlis calculated from the probability of w~, as estimatedby w2's frequency in the corpus (Jelinek, Mercer, andRoukos, 1992; Katz, 1987).
This method depends onan independence assumption on the cooccurrence of Wland w2: the more frequent w2 is, the higher will be theestimate of P(w2\[wl), regardless of Wl.Class-based and similarity-based models provide analternative to the independence assumption.
In thosemodels, the relationship between given words is mod-eled by analogy with other words that are in some sensesimilar to the given ones.Brown et a\].
(1992) suggest a class-based n-grammodel in which words with similar cooccurrence distri-butions are clustered in word classes.
The cooccurrenceprobability of a given pair of words then is estimated ac-cording to an averaged cooccurrence probability of thetwo corresponding classes.
Pereira, Tishby, and Lee(1993) propose a "soft" clustering scheme for certaingrammatical cooccurrences in which membership of aword in a class is probabilistic.
Cooccurrence probabil-ities of words are then modeled by averaged cooccur-rence probabilities of word clusters.Dagan, Markus, and Markovitch (1993) argue thatreduction to a relatively small number of predeterminedword classes or clusters may cause a substantial loss ofinformation.
Their similarity-based model avoids clus-tering altogether.
Instead, each word is modeled by itsown specific class, a set of words which are most simi-lar to it (as in k-nearest neighbor approaches in patternrecognition).
Using this scheme, they predict whichunobserved cooccurrences are more likely than others.Their model, however, is not probabilistic, that is, itdoes not provide a probability estimate for unobservedcooccurrences.
It cannot therefore be used in a com-plete probabilistic framework, such as n-gram languagemodels or probabilistic lexicalized grammars (Schabes,1992; Lafferty, Sleator, and Temperley, 1992).We now give a similarity-based method for estimatingthe probabilities of cooccurrences unseen in training.272Similarity-based estimation was first used for languagemodeling in the cooccurrence smoothing method of Es-sen and Steinbiss (1992), derived from work on acous-tic model smoothing by Sugawara et al (1985).
Wepresent a different method that takes as starting pointthe back-off scheme of Katz (1987).
We first allocate anappropriate probability mass for unseen cooccurrencesfollowing the back-off method.
Then we redistributethat mass to unseen cooccurrences according to an av-eraged cooccurrence distribution of a set of most similarconditioning words, using relative entropy as our sim-ilarity measure.
This second step replaces the use ofthe independence assumption in the original back-offmodel.We applied our method to estimate unseen bigramprobabilities for Wall Street Journal text and comparedit to the standard back-off model.
Testing on a held-outsample, the similarity model achieved a 20% reductionin perplexity for unseen bigrams.
These constitutedjust 10.6% of the test sample, leading to an overall re-duction in test-set perplexity of 2.4%.
We also exper-imented with an application to language modeling forspeech recognition, which yielded a statistically signifi-cant reduction in recognition error.The remainder of the discussion is presented in termsof bigrams, but it is valid for other types of word cooc-currence as well.D iscount ing  and  Red is t r ibut ionMany low-probability bigrams will be missing from anyfinite sample.
Yet, the aggregate probability of all theseunseen bigrams is fairly high; any new sample is verylikely to contain some.Because of data sparseness, we cannot reliably use amaximum likelihood estimator (MLE) for bigram prob-abilities.
The MLE for the probability of a bigram(wi, we) is simply:PML(Wi, we) -- c(w , we) N , (1)where c(wi, we) is the frequency of (wi, we) in the train-ing corpus and N is the total number of bigrams.
How-ever, this estimates the probability of any unseen hi-gram to be zero, which is clearly undesirable.Previous proposals to circumvent the above problem(Good, 1953; Jelinek, Mercer, and Roukos, 1992; Katz,1987; Church and Gale, 1991) take the MLE as an ini-tial estimate and adjust it so that the total probabilityof seen bigrams is less than one, leaving some probabil-ity mass for unseen bigrams.
Typically, the adjustmentinvolves either interpolation, in which the new estimatoris a weighted combination of the MLE and an estimatorthat is guaranteed to be nonzero for unseen bigrams, ordiscounting, in which the MLE is decreased according toa model of the unreliability of small frequency counts,leaving some probability mass for unseen bigrams.The back-off model of Katz (1987) provides a clearseparation between frequent events, for which observedfrequencies are reliable probability estimators, and low-frequency events, whose prediction must involve addi-tional information sources.
In addition, the back-offmodel does not require complex estimations for inter-polation parameters.A hack-off model requires methods for (a) discountingthe estimates of previously observed events to leave outsome positive probability mass for unseen events, and(b) redistributing among the unseen events the probabil-ity mass freed by discounting.
For bigrams the resultingestimator has the general formfPd(w21wl) if c(wi,w2) > 0D(w21wt) = ~.a(Wl)Pr(w2\]wt) o herwise , (2)where Pd represents the discounted estimate for seenbigrams, P~ the model for probability redistributionamong the unseen bigrams, and a(w) is a normalizationfactor.
Since the overall mass left for unseen bigramsstarting with wi is given by~,  P,~(welwi) ,w~:c(wi ,w~)>0~(wi) = 1 -the normalizationEw2 P(w2\[ wl) : 1 is=factor required to ensure(wl)1 - ~:c(~i,w2)>0 Pr(we\[wi)The second formulation of the normalization is compu-tationally preferable because the total number of pos-sible bigram types far exceeds the number of observedtypes.
Equation (2) modifies slightly Katz's presenta-tion to include the placeholder Pr for alternative modelsof the distribution of unseen bigrams.Katz uses the Good-Turing formula to replace theactual frequency c(wi, w2) of a bigram (or an event, ingeneral) with a discounted frequency, c*(wi,w2), de-fined byc*(wi, w2) = (C(Wl, w2) + 1)nc(wl'~)+i , (3)nc(wl,w2)where nc is the number of different bigrams in the cor-pus that have frequency c. He then uses the discountedfrequency in the conditional probability calculation fora bigram:c* (wi, w2) (4)Pa(w21wt) - C(Wl)In the original Good-Turing method (Good, 1953)the free probability mass is redistributed uniformlyamong all unseen events.
Instead, Katz's back-offscheme redistributes the free probability mass non-uniformly in proportion to the frequency of w2, by set-tingPr(weJwi) = P(w~) (5 )273Katz thus assumes that for a given conditioning wordwl the probability of an unseen following word w2 isproportional to its unconditional probability.
However,the overall form of the model (2) does not depend onthis assumption, and we will next investigate an esti-mate for P~(w21wl) derived by averaging estimates forthe conditional probabilities that w2 follows words thatare distributionally similar to wl.The S imi la r i ty  Mode lOur scheme is based on the assumption that words thatare "similar" to wl can provide good predictions forthe distribution of wl in unseen bigrams.
Let S(Wl)denote a set of words which are most similar to wl,as determined by some similarity metric.
We definePsiM(W21Wl), the similarity-based model for the condi-tional distribution of wl, as a weighted average of theconditional distributions of the words in S(Wl):PsiM(W21wl) =- ,  ? '
-  ' ~ w(~i,~') (6) ZWleS(Wl) 2\[--~'(~\]~l'\['/fll)~"~ W/w,  ~j ) 'where W(W~l, wl) is the (unnormalized) weight given tow~, determined by its degree of similarity to wl.
Ac-cording to this scheme, w2 is more likely to follow wl ifit tends to follow words that are most similar to wl.
Tocomplete the scheme, it is necessary to define the simi-larity metric and, accordingly, S(wl) and W(w~, Wl).Following Pereira, Tishby, and Lee (1993), wemeasure word similarity by the relative ntropy, orKullback-Leibler (KL) distance, between the corre-sponding conditional distributionsD(w~ II w~) = Z P(w2\]wl) og P(w2Iwl) (7)~ P(w2lw~) "The KL distance is 0 when wl = w~, and it increasesas the two distribution are less similar.To compute (6) and (7) we must have nonzero esti-mates of P(w21wl) whenever necessary for (7) to be de-fined.
We use the estimates given by the standard back-off model, which satisfy that requirement.
Thus ourapplication of the similarity model averages togetherstandard back-off estimates for a set of similar condi-tioning words.We define S(wl) as the set of at most k nearestwords to wl (excluding wl itself), that also satisfyD(Wl II w~) < t. k and t are parameters that controlthe contents of $(wl)  and are tuned experimentally, aswe will see below.W(w~, wl) is defined asW(w~, Wl) --- exp -/3D(Wl II ~i)The weight is larger for words that are more similar(closer) to wl.
The parameter fl controls the relativecontribution of words in different distances from wl: asthe value of fl increases, the nearest words to Wl get rel-atively more weight.
As fl decreases, remote words geta larger effect.
Like k and t,/3 is tuned experimentally.Having a definition for PSIM(W2\[Wl), we could use itdirectly as Pr(w2\[wl) in the back-off scheme (2).
Wefound that it is better to smooth PsiM(W~\[Wl) by inter-polating it with the unigram probability P(w2) (recallthat Katz used P(w2) as Pr(w2\[wl)).
Using linear in-terpolation we getP,(w2\[wl) =7P(w2) + (1 - 7)PsiM(W2lWl) , (8)where "f is an experimentally-determined i terpolationparameter.
This smoothing appears to compensatefor inaccuracies in Pslu(w2\]wl), mainly for infrequentconditioning words.
However, as the evaluation be-low shows, good values for 7 are small, that is, thesimilarity-based model plays a stronger role than theindependence assumption.To summarize, we construct a similarity-based modelfor P(w2\[wl) and then interpolate it with P(w2).
Theinterpolated model (8) is used in the back-off schemeas Pr(w2\[wl), to obtain better estimates for unseen bi-grams.
Four parameters, to be tuned experimentally,are relevant for this process: k and t, which determinethe set of similar words to be considered,/3, which deter-mines the relative effect of these words, and 7, which de-termines the overall importance of the similarity-basedmodel.Eva luat ionWe evaluated our method by comparing its perplexity 1and effect on speech-recognition accuracy with the base-line bigram back-off model developed by MIT LincolnLaboratories for the Wall Streel Journal (WSJ) textand dictation corpora provided by ARPA's HLT pro-grain (Paul, 1991).
2 The baseline back-off model followsclosely the Katz design, except that for compactness allfrequency one bigrams are ignored.
The counts used illthis model and in ours were obtained from 40.5 millionwords of WSJ text from the years 1987-89.For perplexity evaluation, we tuned the similaritymodel parameters by minimizing perplexity on an ad-ditional sample of 57.5 thousand words of WSJ text,drawn from the ARPA HLT development test set.
Thebest parameter values found were k = 60, t = 2.5,/3 = 4and 7 = 0.15.
For these values, the improvement inperplexity for unseen bigrams in a held-out 18 thou-sand word sample, in which 10.6% of the bigrams areunseen, is just over 20%.
This improvement on unseen1The perplexity of a conditional bigram probabilitymodel /5 with respect to the true bigram distribution isan information-theoretic measure of model quality (Jelinek,Mercer, and Roukos, 1992) that can be empirically esti-mated by exp - -~ ~-~i log P(w, tu, i_l ) for a test set of lengthN.
Intuitively, the lower the perplexity of a model the morelikely the model is to assign high probability to bigrams thatactually occur.
In our task, lower perplexity will indicatebetter prediction of unseen bigrams.2The ARPA WSJ development corpora come in two ver-sions, one with verbalized punctuation and the other with-out.
We used the latter in all our experiments.274k t ~ 7 training reduction (%) test reduction (%)60 2.5 4 0.15 18.4 20.5150 2.5 4 0.15 18.38 20.4540 2.5 4 0.2 18.34 20.0330 2.5 4 0.25 18.33 19.7670 2.5 4 0.1 18.3 20.5380 2.5 4.5 0.1 18.25 20.55100 2.5 4.5 0.1 18.23 20.5490 2.5 4.5 0.1 18.23 20.5920 1.5 4 0.3 18.04 18.710 1.5 3.5 0.3 16.64 16.94Table 1: Perplexity Reduction on Unseen Bigrams for Different Model Parametersbigrams corresponds to an overall test set perplexityimprovement of 2.4% (from 237.4 to 231.7).
Table 1shows reductions in training and test perplexity, sortedby training reduction, for different choices in the num-ber k of closest neighbors used.
The values of f~, 7 andt are the best ones found for each k. 3From equation (6), it is clear that the computationalcost of applying the similarity model to an unseen bi-gram is O(k).
Therefore, lower values for k (and alsofor t) are computationally preferable.
From the table,we can see that reducing k to 30 incurs a penalty of lessthan 1% in the perplexity improvement, so relativelylow values of k appear to be sufficient to achieve mostof the benefit of the similarity model.
As the table alsoshows, the best value of 7 increases as k decreases, thatis, for lower k a greater weight is given to the condi-tioned word's frequency.
This suggests that the predic-tive power of neighbors beyond the closest 30 or so canbe modeled fairly well by the overall frequency of theconditioned word.The bigram similarity model was also tested as a lan-guage model in speech recognition.
The test data forthis experiment were pruned word lattices for 403 WSJclosed-vocabulary test sentences.
Arc scores in thoselattices are sums of an acoustic score (negative log like-lihood) and a language-model score, in this case thenegative log probability provided by the baseline bi-gram model.From the given lattices, we constructed new latticesin which the arc scores were modified to use the similar-ity model instead of the baseline model.
We comparedthe best sentence hypothesis in each original attice andin the modified one, and counted the word disagree-ments in which one of the hypotheses i correct.
Therewere a total of 96 such disagreements.
The similaritymodel was correct in 64 cases, and the back-off model in32.
This advantage for the similarity model is statisti-cally significant at the 0.01 level.
The overall reductionin error rate is small, from 21.4% to 20.9%, becausethe number of disagreements is small compared with3Values of fl and t refer to base 10 logarithms and expo-nentials in all calculations.the overall number of errors in our current recognitionsetup.Table 2 shows some examples of speech recognitiondisagreements between the two models.
The hypothesesare labeled 'B'  for back-off and 'S' for similarity, and thebold-face words are errors.
The similarity model seemsto be able to model better regularities uch as semanticparallelism in lists and avoiding a past tense form after"to."
On the other hand, the similarity model makesseveral mistakes in which a function word is inserted ina place where punctuation would be found in writtentext.Re la ted  WorkThe cooccurrence smooihing technique (Essen andSteinbiss, 1992), based on earlier stochastic speechmodeling work by Sugawara et al (1985), is the mainprevious attempt o use similarity to estimate the prob-ability of unseen events in language modeling.
In addi-tion to its original use in language modeling for speechrecognition, Grishman and Sterling (1993) applied thecooccurrence smoothing technique to estimate the like-lihood of selectional patterns.
We will outline herethe main parallels and differences between our methodand cooccurrence smoothing.
A more detailed analy-sis would require an empirical comparison of the twomethods on the same corpus and task.In cooccurrence smoothing, as in our method, a base-line model is combined with a similarity-based modelthat refines some of its probability estimates.
The sim-ilarity model in cooccurrence smoothing is based onthe intuition that the similarity between two words wand w' can be measured by the confusion probabilityPc(w'lw ) that w' can be substituted for w in an arbi-trary context in the training corpus.
Given a baselineprobability model P, which is taken to be the MLE, theconfusion probability Pc(w~lwl) between conditioningwords w~ and wl is defined asl Pc(wllwl) - -1 (9)  P( l) p(wllw2)p(wl 1 2)P( 2) 'the probability that wl is followed by the same contextwords as w~.
Then the bigram estimate derived by275B commitments .. .
from leaders felt  the  three point six billion dollarsS \] commitments .. .
from leaders fell to three point six billion dollarsB I followed bv France the US agreed  in ltalv ,y I yS \[ followed by France the US Greece .
.
.
I ta lyB \[ he whispers to made aS \[ he whispers to an aideB the necessity for change existS \[ the necessity for change existsB \] without .
.
.addit ional reserves Centrust would have reportedS \[ without .
.
.addit ional reserves of  Centrust would have reportedB \] in the darkness past the churchS in the darkness passed  the churchTable 2: Speech Recognition Disagreements between Modelscooccurrence smoothing is given byPs(w21wl) = ~ P(w~lw'l)Pc(w'llwONotice that this formula has the same form as our sim-ilarity model (6), except that it uses confusion proba-bilities where we use normalized weights.
4 In addition,we restrict the summation to sufficiently similar words,whereas the cooccurrence smoothing method sums overall words in the lexicon.The similarity measure (9) is symmetric in the sensethat Pc(w'lw) and Pc(w\[w') are identical up to fre-Pc(w'l w) _ P(w) quency normalization, that is Pc(wlw') - P(w,)" Incontrast, D(w H w') (7) is asymmetric in that it weighseach context in proportion to its probability of occur-rence with w, but not with wq In this way, if w andw' have comparable frequencies but w' has a sharpercontext distribution than w, then D(w' I\[ w) is greaterthan D(w \[\[ w').
Therefore, in our similarity modelw' will play a stronger role in estimating w than viceversa.
These properties motivated our choice of relativeentropy for similarity measure, because of the intuitionthat words with sharper distributions are more infor-mative about other words than words with flat distri-butions.4This presentation corresponds to model 2-B in Essenand Steinbiss (1992).
Their presentation follows the equiv-alent model l-A, which averages over similar conditionedwords, with the similarity defined with the preceding wordas context.
In fact, these equivalent models are symmetricin their treatment of conditioning and conditioned word, asthey can both be rewritten asPs(w2lwl) ,~, , , , , P(w2\[Wl)P(Wl = Iw~)P(w21wl )They also consider other definitions of confusion probabil-ity and smoothed probability estimate, but the one aboveyielded the best experimental results.Finally, while we have used our similarity model onlyfor missing bigrams in a back-off scheme, Essen andSteinbiss (1992) used linear interpolation for all bi-grams to combine the cooccurrence smoothing modelwith MLE models of bigrams and unigrams.
Notice,however, that the choice of back-off or interpolation isindependent from the similarity model used.Fur ther  ResearchOur model provides a basic scheme for probabilisticsimilarity-based estimation that can be developed inseveral directions.
First, variations of (6) may be tried,such as different similarity metrics and different weight-ing schemes.
Also, some simplification of the currentmodel parameters may be possible, especially with re-spect to the parameters t and k used to select the near-est neighbors of a word.
A more substantial variationwould be to base the model on similarity between con-ditioned words rather than on similarity between con-ditioning words.Other evidence may be combined with the similarity-based estimate.
For instance, it may be advantageousto weigh those estimates by some measure of the re-liability of the similarity metric and of the neighbordistributions.
A second possibility is to take into ac-count negative vidence: if Wl is frequent, but w2 neverfollowed it, there may be enough statistical evidenceto put an upper bound on the estimate of P(w21wl).This may require an adjustment of the similarity basedestimate, possibly along the lines of (Rosenfeld andHuang, 1992).
Third, the similarity-based estimate canbe used to smooth the naaximum likelihood estimatefor small nonzero frequencies.
If the similarity-basedestimate is relatively high, a bigram would receive ahigher estimate than predicted by the uniform discount-ing method.Finally, the similarity-based model may be appliedto configurations other than bigrams.
For trigrams,it is necessary to measure similarity between differ-ent conditioning bigrams.
This can be done directly,276by measuring the distance between distributions of theform P(w31wl, w2), corresponding to different bigrams(wl, w~).
Alternatively, and more practically, it wouldbe possible to define a similarity measure between bi-grams as a function of similarities between correspond-ing words in them.
Other types of conditional cooccur-rence probabilities have been used in probabilistic pars-ing (Black et al, 1993).
If the configuration i questionincludes only two words, such as P(objectlverb), then itis possible to use the model we have used for bigrams.If the configuration i cludes more elements, it is nec-essary to adjust the method, along the lines discussedabove for trigrams.Conc lus ionsSimilarity-based models uggest an appealing approachfor dealing with data sparseness.
Based on corpusstatistics, they provide analogies between words that of-ten agree with our linguistic and domain intuitions.
Inthis paper we presented a new model that implementsthe similarity-based approach to provide estimates forthe conditional probabilities of unseen word cooccur-fences.Our method combines similarity-based estimateswith Katz's back-off scheme, which is widely used forlanguage modeling in speech recognition.
Although thescheme was originally proposed as a preferred way ofimplementing the independence assumption, we suggestthat it is also appropriate for implementing similarity-based models, as well as class-based models.
It enablesus to rely on direct maximum likelihood estimates whenreliable statistics are available, and only otherwise re-sort to the estimates of an "indirect" model.The improvement we achieved for a bigram model isstatistically significant, hough modest in its overall ef-fect because of the small proportion of unseen events.While we have used bigrams as an easily-accessible plat-form to develop and test the model, more substantialimprovements might be obtainable for more informa-tive configurations.
An obvious case is that of tri-grams, for which the sparse data problem is much moresevere.
~ Our longer-term goal, however, is to applysimilarity techniques to linguistically motivated wordcooccurrence configurations, as suggested by lexical-ized approaches to parsing (Schabes, 1992; Lafferty,Sleator, and Temperley, 1992).
In configurations likeverb-object and adjective-noun, there is some evidence(Pereira, Tishby, and Lee, 1993) that sharper wordcooccurrence distributions are obtainable, leading toimproved predictions by similarity techniques.AcknowledgmentsWe thank Slava Katz for discussions on the topic of thispaper, Doug McIlroy for detailed comments, Doug Paul5For WSJ trigrams, only 58.6% of test set trigramsoccur in 40M of words of training (Doug Paul, personalcommunication).for help with his baseline back-off model, and AndreLjolje and Michael Riley for providing the word latticesfor our experiments.Re ferencesBlack, Ezra, Fred Jelinek, John Lafferty, David M.Magerman, David Mercer, and Salim Roukos.
1993.Towards history-based grammars: Using richer mod-els for probabilistic parsing.
In 30th Annual Meet-ing of the Association for Computational Linguistics,pages 31-37, Columbus, Ohio.
Ohio State University,Association for Computational Linguistics, Morris-town, New Jersey.Brown, Peter F., Vincent J. Della Pietra, Peter V.deSouza, Jenifer C. Lai, and Robert L. Mercer.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):467-479.Church, Kenneth W. and William A. Gale.
1991.
Acomparison ofthe enhanced Good-Turing and deletedestimation methods for estimating probabilities ofEnglish bigrams.
Computer Speech and Language,5:19-54.Dagan, Ido, Shaul Markus, and Shaul Markovitch.1993.
Contextual word similarity and estimationfrom sparse data.
In 30th Annual Meeting of the As-sociation for Computational Linguistics, pages 164-171, Columbus, Ohio.
Ohio State University, Asso-ciation for Computational Linguistics, Morristown,New Jersey.Essen, Ute and Volker Steinbiss.
1992.
Coocurrencesmoothing for stochastic language modeling.
In Pro-ceedings of ICASSP, volume I, pages 161-164.
IEEE.Good, I .
J .
1953.
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40(3):237-264.Grishman, Ralph and John Sterling.
1993.
Smoothingof automatically generated selectional constraints.
InHuman Language Technology, pages 254-259, SanFrancisco, California.
Advanced Research ProjectsAgency, Software and Intelligent Systems TechnologyOffice, Morgan Kaufmann.Jelinek, Frederick, Robert L. Mercer, and SalimRoukos.
1992.
Principles of lexical language mod-eling for speech recognition.
In Sadaoki Furui andM.
Mohan Sondhi, editors, Advances in Speech Sig-nal Processing.
Mercer Dekker, Inc., pages 651-699.Katz, Slava M. 1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speeech and Signal Processing, 35(3):400-401.Lafferty, John, Daniel Sleator, and Davey Temperley.1992.
Grammatical trigrams: aa probabilistic modelof link grammar.
In Robert Goldman, editor, AAAI277Fall Symposium on Probabilistic Approaches to Natu-ral Language Processing, Cambridge, Massachusetts.American Association for Artificial Intelligence.Paul, Douglas B.
1991.
Experience with a stackdecoder-based HMM CSR and back-off n-gram lan-guage models.
In Proceedings of the Speech and Nat-ural Language Workshop, pages 284-288, Palo Alto,California, February.
Defense Advanced ResearchProjects Agency, Information Science and Technol-ogy Office, Morgan Kaufmann.Pereira, Fernando C. N., Naftali Z. Tishby, and Lil-lian Lee.
1993.
Distributional c ustering of Englishwords.
In $Oth Annual Meeting of the Association forComputational Linguistics, pages 183-190, Co\]urn-bus, Ohio.
Ohio State University, Association forComputational Linguistics, Morristown, New Jersey.Rosenfeld, Ronald and Xuedong Huang.
1992.
Im-provements in stochastic language modeling.
InDARPA Speech and Natural Language Workshop,pages 107-111, Harriman, New York, February.
Mor-gan Kaufmann, San Mateo, California.Sehabes, Yves.
1992.
Stochastic lexiealized tree-adjoining grammars.
In Proceeedings of the 14thInternational Conference on Computational Linguis-tics, Nantes, France.Sugawara, K., M. Nishimura, K. Toshioka, M. Okoehi,and T. Kaneko.
1985.
Isolated word recognitionusing hidden Markov models.
In Proceedings ofICASSP, pages 1-4, Tampa, Florida.
IEEE.278
