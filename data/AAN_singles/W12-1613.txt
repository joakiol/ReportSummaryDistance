Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 99?107,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsThe Structure and Generality of Spoken Route InstructionsAasish Pappu and Alexander RudnickyLanguage Technologies Institute, Carnegie Mellon University{aasish, air}@cs.cmu.eduAbstractA robust system that understands route instructionsshould be able to process instructions generated nat-urally by humans.
Also desirable would be the abil-ity to handle repairs and other modifications to exist-ing instructions.
To this end, we collected a corpusof spoken instructions (and modified instructions)produced by subjects provided with an origin anda destination.
We found that instructions could beclassified into four categories, depending on theirintent such as imperative, feedback, or meta com-ment.
We asked a different set of subjects to fol-low these instructions to determine the usefulnessand comprehensibility of individual instructions.
Fi-nally, we constructed a semantic grammar and evalu-ated its coverage.
To determine whether instruction-giving forms a predictable sub-language, we testedthe grammar on three corpora collected by othersand determined that this was largely the case.
Ourwork suggests that predictable sub-languages mayexist for well-defined tasks.Index Terms: Robot Navigation, Spoken Instructions1 IntroductionGenerating and interpreting instructions is a topic of en-during interest.
Cognitive psychologists have examinedhow people perceive spatial entities and structure routeinstructions (Daniel and Denis, 1998; Allen, 1997).
Lin-guists and others have investigated how people articulateroute instructions in conversation with people or agents(Eberhard et al, 2010; Gargett et al, 2010; Stoia et al,2008; Marge and Rudnicky, 2010).
Artificial intelligenceresearchers have shown that under supervised conditionsautonomous agents can learn to interpret route instruc-tions (Kollar et al, 2010; MacMahon et al, 2006; Ma-tuszek et al, 2010; Bugmann et al, 2004; Chen andMooney, 2010).While the subject has been approached from differentperspectives, it has been generally held that the languageof directions is mostly limited and only parts of the vo-cabulary (such as location names) will vary from case tocase.
We are interested in being able to interpret naturaldirections, as might be given to a robot, and generatingcorresponding trajectory.
But natural directions containdifferent types of information, some (more-or-less) eas-ily interpreted (e.g., "go to the end of the hall") whileothers seem daunting (e.g., "walk past the abstract muralwith birds").
So the question might actually be "is thereenough interpretable data in human directions to supportplanning a usable trajectory?
".The language of instructions contains a variety of rel-evant propositions: a preface to a route, an imperativestatement, or a description of a landmark.
Previous workhas proposed both coarse and fine-grained instructiontaxonomies.
(Bugmann et al, 2004) proposed a taxon-omy of 15 primitive categories in a concrete ?action?framework.
In contrast, (Daniel and Denis, 1998) sug-gested a five-way categorization based on cognitive prop-erties of instructions.Instructions vary greatly and can include superfluousdetail.
(Denis et al, 1999) found that when people wereasked to read and assess a set of instructions some of theinstructions were deemed unnecessary and could be dis-carded.
There is some evidence (Lovelace et al, 1999;Caduff and Timpf, 2008) that only the mention of sig-nificant landmarks along the route leads to better-qualityinstructions.
Computational (rather than descriptive) ap-proaches to this problem include: using sequence label-ing approach to capture spatial relations, landmarks, andaction verbs (Kollar et al, 2010), generating a framestructure for an instruction (MacMahon et al, 2006), orusing statistical machine translation techniques to trans-late instructions into actions (Matuszek et al, 2010).We describe a new instructions corpus, its analysis interms of a taxonomy suitable for automated understand-ing and a verification that the instructions are in fact us-able by humans.
With a view to automating understand-ing, we also constructed a grammar capable of processingthis language, and show that it provides good coverage99for both our corpus and three other corpora (Kollar et al,2010; Marge and Rudnicky, 2010; Bugmann et al, 2004)This paper is organized as following: Section 2 de-scribes the corpus collection study.
Then in Section 3,we discuss the taxonomy of route instructions.
Section 4focuses on which categories are important for navigation.In Section 5, we report our results and error analysis onparsing instructions from our corpus and three other cor-pora containing route instructions, followed by lessonslearned and future work.2 The Navagati1 CorpusWe collected a corpus of spoken instructions describinghow to get from one part of a large building complexto another.
To ensure consistency we recruited individ-uals who were familiar with the environment and conse-quently could formulate such instructions without refer-ence to maps or other materials.
Since we are ultimatelyinterested in how such instructions are edited, we also in-cluded conditions in which subjects were asked to modifytheir instructions in several ways.
The corpus is publiclyavailable2.2.1 Participants and ProcedureWe recruited subjects who were both fluent Englishspeakers and were also familiar with the environment (auniversity building complex).
Subjects were told to imag-ine that they had encountered a visitor, not familiar withthe campus, at a specific location (in front of elevators ona particular floor) who needed instructions to a specificlocation, a caf?
two buildings away.For each set of instructions, subjects were asked tothink about the route and their instructions, then recordthem as a single monologue.
Subjects sat in front ofa computer and wore a close-talking microphone.
Ini-tially no map was provided and they were expected torely on their memory.
In subsequent tasks they wereshown a floor-plan indicating a specific location of thevisitor and asked to modify their instructions.
Speechwas transcribed using Amazon Mechanical Turk, shownto be a reliable resource for spoken language transcription(Marge et al, 2010).
Transcriptions were normalized tostandardize spellings (e.g., building names).2.2 DesignPrevious works have focused on eliciting route instruc-tions between multiple pairs of locations.
There is a gen-eral agreement that the structure of instructions did notvary with the increase in number of start-end locationpairs.
However previous works have not looked at howinstructions would be modified under different situations.1Sanskrit root for Navigation meaning "to travel by boat"2http://tts.speech.cs.cmu.edu/apappu/navagati/We were interested in two general cases: normal in-structions (Simple scenario) and repairing existing in-structions (Repair scenario).
Each scenario includedthree tasks, as described below.We selected two locations that could be walked be-tween without necessarily going outside.
However thesubjects were free to to give instructions for a route oftheir choice between a location pair.
The first location (A)was in front of an elevator on the seventh floor of GatesHillman Center, the second location (B) was a cafe on thefifth floor of Wean Hall.
The expected pathway includedchanges in floor, direction and passing through a differentbuilding.
It required reasonably detailed instructions.In the Simple scenario, subjects were asked to generatethree variants, as follows: (1) instructions for A?
B; (2)for B ?
A; and (3) a simplified version of (2).The motivation behind (2) is to learn whether peoplewould make references about the parts of the route thatwere previously traversed in the opposite direction.
Inthe case of (3), we were interested in the degree of in-struction reuse and the condensation strategy.
We explic-itly told the subject ?Imagine that the visitor found yourinstructions confusing.
They asked you to simplify theinstructions.
How would you do that?
?The Repair scenario was designed to probe how a sub-ject would alter their instructions in response to compli-cations.
Subjects were asked to modify their intial Simpleinstructions (A ?
B) to cope with: (1) visitor missing alandmark and takes a wrong turn; (2) an obstruction (con-struction) blocking the original path; and (3) the visitorgetting lost and ends up in an unknown part of the (mid-dle) building.
For each case, the subject was given a map(as in figure 1) that marked the visitor?s location and hadto get the visitor back on track.Figure 1: Map of the construction area (marked as star)The tasks in this scenario were designed to see whetherpeople modify directions differently when three differentsituations are presented.
Precisely, we want to know if100there is any difference in the discourse structure and ver-bosity of the directions.2.3 AnalysisNine subjects performed 6 tasks each, producing 54 setsof instructions, for a total of 65 minutes of speech.
Pleasenote that other corpora in the route instructions domainhave similiar scale (see Figure 5(a)).
The transcriptionswere segmented semi-automatically into atomic unitscorresponding to instruction steps.
For example, the in-struction ?Go left, then turn right?
was segmented into:?go left?, and ?then turn right?
based on bigram heuris-tics.
We compiled a list of most frequent bigrams andtrigrams in the corpus e.g., ?and then?, ?after that?
etc.The transcriptions were segmented at the bigram/trigramboundaries and were manually verified for the correctnessof a segment.
The Simple scenario generated 552 instruc-tions, the Repair part contained 382 instructions, a totalof 934.
The vocabulary has 508 types and 7937 tokens.Table 1 summarizes the factors measured in both the sce-narios.
Only two (marked by *) differed between scenar-ios (t-test at p < 0.05).
We examined acoustic properties(for example mean pitch) but did not find any significantdifferences across scenario type.Table 1: Simple vs Repair ScenarioFactors Simple Repair# Tokens 4461 3476# Types 351 375# Instructions 552 382# Words-per-Instruction* 7.5 8.0# Landmarks 450 314# Motion Verbs* 775 506# Spatial Prepositions 61 60# Filler Phrases 414 380We can compare language similarity across scenar-ios by comparing the perplexity of text in the two sce-narios.
If the instructions and repairs are similar, wewould expect that a model built from one scenario shouldbe able to capture data from the other scenario.
Werandomly divided data from each scenario into training(70%) and testing data (30%).
We built a trigram lan-guage model (LM) smoothed with absolute discountingusing the CMU-SLM toolkit (Rosenfield, 1995).
Then,we computed the perplexity on testing data from eachscenario against each model.
From Table 2, Simple-LM has lower perplexity compared to Repair-LM on thetest sets.
The perplexity of Simple-LM on Repair-Testis slightly higher when compared to Simple-Test.
Thiscould be due to the lexical diversity of the Repair scenarioor simply to the smaller sample size.
Table 1 (row 1) indi-cates that the data in Repair scenario is smaller than datain Simple scenario.
To explore the lexical diversity ofthese two scenarios we conducted a qualitative analysisof the instructions from both the scenarios.In Task 1 of the Simple scenario, we only observeda sequence of instructions.
However in Task 2 of SimpleScenario, we noticed references to instructions from Task1 via words like ?remember?, ?same route?, etc.
Thissuggests that instructions may be considered in context ofprevious exchanges and that this history should normallybe available for interpretation purposes.
In Task 3 of theSimple scenario, 7 out of 9 subjects simply repeated theinstructions from Task 2 while the rest provided a differ-ent version of the same instructions.
We did not observeany other qualitative differences across three tasks in theSimple scenario.In Task 1 of the Repair scenario, all but one subjectgave instructions that returned the visitor to the missedlandmark, instead of bypassing the landmark.
In Task 2,the obstruction on the path could be negotiated througha shorter or longer detour.
But only 4 out of 9 partici-pants suggested the shorter detour.
In Task 3, we did notobserve anything different from Task 2.
Despite the dif-ference in the situations, the language of repair was foundto be quite similar.
The structure of the delivery was orga-nized as follows: (1) Subjects introduced the situation ofthe visitor; (2) then modified the instructions according tothe situation.
Introduction of the situation was differentin each task, (e.g., ?you are facing the workers?
vs ?lookslike you are near office spaces?
vs ?if you have missedthe atrium you took a wrong turn?).
But the modificationor repair of the instructions was similar across the situa-tions.
The repaired instructions are sequences of instruc-tions with a few cautionary statements inserted betweeninstructions.
We believe that subjects added cautionarystatements in order to warn the visitor from going off-the-route.
We observed that 6.3% of the repaired instructionswere cautionary statements; we did not observe caution-ary statements in the original Simple scenario.
In orderto see the effect of these cautionary statements we re-moved them from both training and testing sets of theRepair scenario, then built a trigram LM using this con-densed training data (Repair?w/o-cautionLM).
Table 2shows that perplexity drops when cautionary statementsare excluded from the repair scenario, indicating thatSimple and Repair scenarios are similar except for thesecautionary statements.3 Taxonomy of Route InstructionsTaxonomies have been proposed in the past.
Danieland Denis (1998) proposed a taxonomy that reflected at-tributes of spatial cognition and included 5 classes: (1)Imperatives; (2) Imperatives referring a landmark; (3)Introduction of a landmark without an action; (4) Non-spatial description of landmarks and (5) Meta comments.101Table 2: Perplexity of Simple/Repair Language ModelsLM/Test Simple-Test Repair-Test Repair-w/o-cautionSimple-LM 29.6 36.5 30.3Repair-LM 37.4 37.3 35.6Repair-w/o-cautionLM31.9 37.6 26.8Bugmann et al (2004) suggested 15 primitive (robot-executable) actions.
We present a hierarchical instructiontaxonomy that takes into account both cognitive proper-ties and the needs of robot navigation.
This taxonomy isbased on 934 route instruction monologues.
It should benoted that this taxonomy is not based on dialog acts butrather takes the intent of the instruction into the account.3.1 CategoriesWe segmented the spoken instructions using a criterionthat split individual actions and observations.
Our taxon-omy is roughly comparable to that of (Daniel and Denis,1998) but differs in the treatment of landmarks becausethe mention of the landmarks in an instruction can be oftwo types: contextual mention and positional mention.Contextual Mention means when a landmark in the sur-roundings but it is not on the path.
On the other hand, po-sitional mention requires the landmark to be on the path.In our taxonomy, contextual mention becomes Advisoryinstruction and positional mention is called Groundinginstruction.
The taxonomy has four major categories thatsubsume 18 sub-categories; these are given in Table 3.For instance, ?You want to take a right?
belongs to theImperative category.
?You will see a black door?
is anAdvisory instruction about the surroundings.
?You are onthe first floor?
denotes Grounding.
?Your destination islocated in another building and you will walk across threebuildings in this route?
gives an overview of the route, aMeta Comment.
From Figure 2, we see that majority ofthe route instructions are Imperative.0 20 40 60GroundingMeta CommentsAdvisoryImperative 56.2%18.6%17.6%7.6%% distributionFigure 2: First Tier Instruction Categories3.1.1 Imperative InstructionsImperative instructions are executable and can resultin physical displacement.
We identified seven subcate-gories of Imperatives that distinguish different contexts(e.g., going along a corridor, changing floors via elevatoror stairs, or going to a specific location).Imperative instructions can also include preconditionsor postconditions.
The order of their execution variesbased on the directionality of the condition between twoinstructions.
Continue is interesting because it canhave travel-distance and travel-direction arguments, oreven no arguments.
In the latter case the follower contin-ues an action (e.g., ?keep walking?
), until some unspeci-fied condition ends it.3.1.2 Advisory InstructionsWhile giving route instructions people mention land-marks along the route as feedback to the direction-follower.
Some of these landmarks are not part of the pathbut do serve as waypoints for the follower (e.g., ?you willsee a hallway right there?).
We observe that landmarksare distinct either functionally and/or physically.
For ex-ample, a hallway is both functionally and physically dif-ferent from an elevator but only physically different froma door because both function as an instrument (or path) toget from one place to another.
Based on this distinction,we divided advisory instructions into five sub-categoriesdepending on the type of landmark mentioned in the in-struction (see Table 3).Compound locations (see Table 3) are closely locatedbut physically distinct.
They may constitute part-wholerelationships e.g., ?TV screen with a motion sensor?.We observed that compound locations are used to disam-biguate when multiple instances of a landmark type arepresent e.g., ?chair near the elevator vs ?chair near thehallway?.3.1.3 Grounding InstructionsGrounding instructions report absolute position.
Theseinstructions indicate current view or location as opposedto future view or location (indicated through advisoryinstructions).
These instructions constitute a landmarkname similar to advisory instructions and also follow thedistinction between the type of landmark mentioned inthe instruction (see Table 3).3.1.4 Meta CommentsMeta comments are non-executable instructions addedto route instructions.
People often make these commentsat the beginning of instructions and sometimes in be-tween two imperative statements e.g., a precautionarystatement.
In our corpus we found meta-comments intwo situations: (1) Preface or introduction of the route;(2) Caution against a (metaphorical) pitfall in the route.102Category SubCategory Distribution ExampleImperativeLeave-Location 2.3% Exit the building; Come out of the roomFollow-Path 7.0% Walk along the corridor; go across the bridgeFloor-Transition 11.2% Take the elevator to fourth floor; Take the stairs to the fifthTurn 24.2% Turn leftGo-To 27.2% Walk to the elevatorsContinue 28.0% Keep going straight for few stepsAdvisoryFloor-Level 5.4% You will see fourth floor of other buildingFloor-Transition 12.2% You will see elevatorsCompound-Location 13.4% You will see a hallway to the right of elevatorsEnd-of-Pathway 21.5% You will see end of the hallwayLandmark 47.5% You will see a TV screenGroundingCompound-Location 5.9% You are on a hallway right next to the elevatorsEnd-of-Pathway 8.2% You are on the bridge leading to other buildingFloor-Level 42.4% You are on fourth floor of the buildingLandmark 43.5% You are on standing near TV screenMeta CommentsCaution 14.7% You can find it immediately; Don?t go that sideMiscellaneous 36.0% Let me guide you through it; I guess a simpler way would bePreface 49.3% I will guide you to the cafe in that buildingTable 3: Taxonomy of Categories with ExamplesBoth the example instructions and the distribution of thesubcategories are given in Table 3.The language of meta comments is more diverse thanthat of the other three categories.
If we build trigramlanguage models for each category and measure the per-plexity on a held-out set from same category the perplex-ity is relatively high for Meta (49.6) compared to othercategories (Advisory: 19.5; Imperative: 18.5; Ground-ing: 11.4).
This suggests that automatic understandingof meta comments might be problematic, consequently itwould be useful to determine the ralative utility of differ-ent instruction categories.
The next section describes atattempt to do this.4 Which Instructions are Relevant?Given a variety of information present in a set of routeinstructions, we wanted to investigate whether all that in-formation is relevant for navigation.
In order to find thatout we devised a user study asking people to follow in-structions collected in our previous study.
(Daniel andDenis, 1998) conducted a similar study where they askedsubjects to read a set of instructions and strike-off in-structions with too much or too little information.
How-ever, people may or may not feel the same when they fol-low (physically navigate) these instructions.
Therefore,in our study the experimenter read instructions (of vary-ing amount of detail) to the subjects while they physicallynavigated through the environment.4.1 Participants and ProcedureWe chose 5 out of the 9 instruction sets, spoken by differ-ent subjects (of average length 26.8 instructions per set)from Task 1 of the Simple scenario discussed above.
Wedid not use the others because they contained few instruc-tions (average of 13.5) and provided fewer instances ofinstructions in different categories.
Also, we did not useinstructions from Repair Scenario because those instruc-tions dependent on a scenario and a set of instructionsthat were already provided to the direction follower.Our set of instructions included the full set, a set withonly imperatives and additional sets adding only one ofthe remaining categories to the imperative set (see Ta-ble 4), producing 25 distinct sets of instructions.
Addi-tionally, building names and the destination name (tran-scribed in the instructions) were anonymized to avoid re-vealing the destination or the ?heading?
at the early stageof the route.We recruited 25 subjects, each doing one variant of theinstructions.
In the session, the experimenter read one in-struction at a time to the subject and walked behind thesubject as they proceeded.
Subjects were asked to say?done?
when ready for the next instruction; they wereallowed to ask the experimenter to repeat instructions butotherwise were on their own.
The experimenter kept trackof how and where a subject got lost on their way to des-tination.
(No systematic effects were observed, but seebelow.)
At the end subjects were handed the entire set ofinstructions and were asked to mark which instructionswere difficult to follow and which were redundant.
Re-maining instructions were deemed to be useful and inter-pretable.Table 4: Variants of an Instruction SetVariant Imperative Advisory Grounding MetaImp XImp+Adv X XImp+Grnd X XImp+Meta X XEntire Set X X X X103Category/Variant Imp Imp+Grnd Imp+Meta Imp+Adv Entire Set Category/Variant Imp Imp+Grnd Imp+Meta Imp+Adv Entire SetDiff-Imp 11 10 12 9 12 Redun-Imp 5 8 12 11 8Diff-Adv 0 10 5 10 10 Redun-Adv 5 10 19 10 29Diff-Grnd 0 0 13 0 0 Redun-Grnd 20 13 47%47 53%53 27Diff-Meta 4 15 12 4 4 Redun-Meta 19 31 65%65 23 50%50Diff-All 6 9 11 7 9 Redun-All 9 13 26 17 21Figure 3: What percent of instructions are Difficult (Diff) or Redundant (Redun)?
On the left: Darker is Difficult right:Darker is More Redundant Instructions4.2 AnalysisExcept for one subject, everybody reached the destina-tion.
Subjects found Imperative and Advisory instruc-tions more useful compared to Grounding instructionsand Meta comments, irrespective of the instruction-setthey followed (see Figure 3).
Figure 3(a) shows percent-age of category-wise difficult instructions in each vari-ant of an instruction set and 3(b) shows percentage ofcategory-wise redundant instructions in each variant of aninstruction set.
For e.g., Diff-Imp/Imp+Meta means that12% of imperative-instructions are difficult in the Imper-ative+Meta variant.16 out 25 Subjects got lost at least once i.e., they misin-terpreted an instruction, followed along wrong path, thenthey realized inconsistencies with spatial information andthe following instruction, and finally recovered from themisinterpreted instruction.
A subject lost thrice in the en-tire experiment who misunderstood one instruction twiceand another instruction once.
The subject was lost at anintersection of three hallways and only one of them leadstowards the destination.
This instruction did not havesufficient information about the next heading.
All sub-jects who recovered from misinterpretation informed thatlandmark?s attributes such as number of floors in a build-ing (if building is the landmark) and the spatial orienta-tion of the landmark helped them in recovery.Instructions that lacked spatial orientation were foundto be particularly difficult to follow.
Subjects found a fewof the imperative and advisory instructions difficult to fol-low.
While following these difficult instructions, peoplerealized that they got lost and asked the experimenter torepeat the instructions.
Examples of difficult instructionsand the people?s complaint on that instruction are as fol-lows:?
So you kind of cross the atrium Complaint: partic-ipants reported that they were not sure how far theyhad to walk across the atrium.?
Go beside the handrails till the other end of thisbuilding Complaint: no absolute destination, mul-tiple hallways at the end of handrails?
Just walk down the hallway exit the building Com-plaint: multiple exits to the building?
After you get off the elevator, take a left and then leftagain Complaint: more than one left confused thesubjects?
You can see the building just in front of you Com-plaint: there were three buildings standing in frontand the target building was slightly to the left.?
You will see the corridor that you want to take Com-plaint: there were two corridors and the orientationwas unspecified in the instruction5 Understanding ExperimentsThe Navagati (NAV) corpus instructions were dividedinto training set (henceforth abbreviated as NAV-train)and testing set (abbreviated as NAV-test) of size 654 (of 6subjects) and 280 (of 3 subjects).
The training set wasused to create a grammar based on the taxonomy de-scribed in Section 3.5.1 GrammarA domain-specific grammar was written to cover mostfrequent phrases from the training set using the Phoenix(Ward, 1991) format.
Phoenix grammars specify a hier-archy of target concepts and is suited to parsing spon-taneous speech.
The resulting grammar produced cor-rect and complete parses on 78% of the training data(NAV-train).
The remaining training instances were notincluded due to unusual phrasing and disfluencies.
Theconcepts in the grammar are listed in the Table 5.5.1.1 Managing Variable VocabularyConcepts such as Locations, Pathways and Adjectives-of-Location use vocabulary that is specific to an environ-ment, and the vocabulary of these concepts will change104Corpus #Instr Words/Instr Environmnt Modality H/R-H/R LiftingDevice PathWays Landmarks AdjectivesNAV 934 9 UnivCampus Speech Human-Human 0.029 0.046 0.169 0.13MIT 684 15 UnivCampus Written Human-Human 0.045 0.016 0.163 0.062IBL 769 8 ModelCity Speech Human-Robot n.a.
0.039 0.076 0.13TTALK 1619 7 OpenSpace Speech Human-Robot n.a.
0.027 0.01 0.039Figure 4: (a) Nature of the Corpora (b) Type-Token Ratio of Concepts across CorporaTable 5: Higher level and Leaf node Concepts in GrammarCategory Concepts ExamplesImperative GoToPlace, Turn, etcConditional Imperative Move_Until_X where X is a conditionAdvisory Instructions You_Will_See_LocationGrounding Instructions You_are_at_LocationAuxillary Concepts ExamplesLocations buildings, other landmarks on the routeAdjectives-of-Locations large, open, black, small etc.Pathways hallway, corridor, bridge, doors, etc.LiftingDevice elevator, staircase, stairwell, etc.Spatial Relations behind, above, on right, on left, etc.Numbers turn-angles, distance, etc.Ordinals first, second as in floor numbersFiller phrases you may want to; you are gonna; etc.with surroundings.
We used an off-the-shelf part-of-speech tagger (Toutanova et al, 2003) on NAV-train toidentify ?location-based?
nouns and adjectives.
Thesewere added to the grammar as instances of their respec-tive concepts.5.2 Parsing NAV InstructionsA parse can fall into one of the following categories: 1)Complete: clean and correct parse with all concepts andactions mentioned in the instruction.
2) Incomplete: Ifsome arguments for an action are missing.
3) Misparse:no usable parse produced for an instruction.Table 6 shows that 87% of the instructions from theNAV corpus (excluding meta comments) are parsed cor-rectly.
Correct parses were produced for 89% of Imper-atives, 87% of Advisory and 73% of Grounding instruc-tions.
Meta comments were excluded because they donot constitute any valid actions and can be ignored.
Nev-ertheless 20% of the meta comments produced a validparse (i.e.
unintended action).5.3 Grammar GeneralityThe results for the NAV corpus seem encouraging but itwould be useful to know whether the NAV grammar gen-eralizes to other directions scenarios.
We selected threecorpora to examine this question: MIT (Kollar et al,2010), IBL3 (Bugmann et al, 2004) and TTALK4 (Margeand Rudnicky, 2010).
All were navigation scenarios butwere collected in a variety of settings (see Figure 4(a)).Corpus vocabularies were normalized using the processdescribed in 5.1.1 and location specific nouns and adjec-tives added to the grammar.
Punctuation was removed.Figure 4(b) shows the type-token ratios for ?variable?concepts.
There are more landmarks and adjectives (thattag along landmarks) in NAV and MIT compared to IBLand fewest in TTALK corpus (a closed space with tworobots).
Since, IBL and TTALK do not involve exten-sive navigation inside the buildings there are no instancesof the elevator concept.
However, IBL corpus has ?ex-its, roads, streets?
in the city environment which wereincluded in the PathWay concept.5.4 Performance across CorporaWe randomly sampled 300 instructions from each of thethree corpora (MIT, IBL and TTALK) and evaluated theirparses against manually-created parses.
Table 6) showsresults for each type of parse (Complete, Incomplete, orMisparse).
Meta comments were excluded, as discussedearlier.
The NAV grammar appears portable to three othercorpora.
As shown in Category-Accuracy of Table 6 Im-peratives and Advisory instructions are well-parsed bythe grammar.
In TTALK corpus, there are very few land-mark names but there are certain unusual sentences e.g.,?she to the rear left hand wall of the room?
causing loweraccuracy in Advisory instructions.
We noticed that MITcorpus had longer description of the landmarks, leadingto lower accuracy for Grounding.
From Table 6 11% to16% of Imperative instructions fail to get parsed acrossthe corpora.
We consider these failures/errors below.5.5 Error AnalysisWe found six situations that produced incomplete andmisparsed instructions: (1) Underspecified arguments;(2) Unusual or unobserved phrases; (2) False-starts andungrammatical language; (3) Uncovered words; (4) Pro-longed description of landmarks within an instruction;3http://www.tech.plym.ac.uk/soc/staff/guidbugm/ibl/readme1.html4http://www.cs.cmu.edu/?robotnavcps/105Table 6: Parse ResultsParse Results NAV MIT IBL TTALK# Instructions 280 300 300 300% Complete 87% 78.8% 83.8% 83.4%% Incomplete 3.1% 17% 6.6% 3.7%% Misparse 9.8% 4.1% 9.5% 13%Category AccuracyImperative 89% 89.4% 86.5% 84.7%Advisory 87% 93.4% 87.4% 60%Grounding 73% 62% 100% 100%(5) Coreferences; 6) Non-specific instructions (eg.
eithertake the right hallway or the left hallway).5.5.1 Incomplete and Misparsed InstructionsOut-of-Vocabulary (OOV) words were responsible forthe majority of incomplete parses across all the corpora;many were singletons.
Unusual phrases such as ?as if youare doubling back on yourself?
caused incomplete parses.We also observed lengthy descriptions in instructions inthe MIT corpus, leading to incomplete parses.
This cor-pus was unusual in that it is composed of written, as op-posed to spoken, instructions.Misparsed instructions were caused due to both un-grammatical phrases and OOV words.
Ungrammaticalinstructions contained either missed key content wordslike verbs or false starts.
These instructions did containmeaningful fragments but they did not form a coherentutterance e.g., ?onto a roundabout?.We note that incomplete or otherwise non-understandable utterancess can in principle be recoveredthrough clarification dialog (see e.g., (Bohus and Rud-nicky, 2005).
Direction giving should perhaps not belimited to monologue delivery.Table 7: Error Analysis for Incomplete and Misparsed instruc-tionsIncomplete NAV MIT IBL TTALK# Incomplete Instructions 8 49 19 10MissingArgs 50% 8% 0% 0%UnusualPhrases 0% 28% 35% 60%Lengthy Descriptions 0% 20.4% 0% 0%Coreferences 0% 0% 20.2% 0%Non-concrete phrases 3% 2% 5% 0%OOVs 47% 41.6% 39.8% 40%Misparse# Misparse Instructions 25 12 27 39Ungrammatical phrases 24% 44% 16% 10%OOVs 76% 66% 84% 90%6 ConclusionTo better understand the structure of instructions and toinvestigate how these might be automatically processed,we collected a corpus of spoken instructions.
We foundthat instructions can be organized in terms of a straighfor-ward two-level taxonomy.
We examined the informationcontents of different components and found that that theImperative and Advisory categories appear to be the mostrelevant, though our subjects had little difficulty dealingwith instructions composed of only Imperatives; physicalcontext would seem to matter.We found that it was possible to design a grammar thatreasonably covered the information-carrying instructionsin a set of instructions.
And that a grammar built from ourcorpus generalized quite well to corpora collected underdifferent circumstances.Our study suggests that robust instruction-understanding systems can be implemented and,other than the challenge of dealing with location-specificdata, can be deployed in different environments.
Webelieve that this study also highlights the importanceof dialog-based clarification and the need for strate-gies that can recognize and capture out-of-vocabularywords.
These capabilities are being incorporated into arobot navigation system that can take instructions fromhumans.ReferencesG.
Allen.
1997.
From knowledge to words to wayfinding:Issues in the production and comprehension of route direc-tions.
Spatial Information Theory A Theoretical Basis forGIS, pages 363?372.D.
Bohus and A.I.
Rudnicky.
2005.
Sorry, i didn?t catch that!-an investigation of non-understanding errors and recoverystrategies.
In 6th SIGdial Workshop on Discourse and Di-alogue.G.
Bugmann, E. Klein, S. Lauria, and T. Kyriacou.
2004.Corpus-based robotics: A route instruction example.
Intelli-gent Autonomous Systems 8.D.
Caduff and S. Timpf.
2008.
On the assessment of land-mark salience for human navigation.
Cognitive processing,9(4):249?267.D.L.
Chen and R.J. Mooney.
2010.
Learning to interpretnatural language navigation instructions from observations.Journal of Artificial Intelligence Research, 37:397?435.M.P.
Daniel and M. Denis.
1998.
Spatial descriptions as navi-gational aids: A cognitive analysis of route directions.
Kog-nitionswissenschaft, 7(1):45?52.M.
Denis, F. Pazzaglia, C. Cornoldi, and L. Bertolo.
1999.Spatial discourse and navigation: An analysis of route di-rections in the city of venice.
Applied Cognitive Psychology,13(2):145?174.K.
Eberhard, H. Nicholson, S. Kubler, S. Gundersen, andM.
Scheutz.
2010.
The indiana .cooperative remote searchtask.
(crest) corpus.
In Proc.
of LREC, volume 10.A.
Gargett, K. Garoufi, A. Koller, and K. Striegnitz.
2010.
Thegive-2 corpus of giving instructions in virtual environments.In Proc.
of LREC.106T.
Kollar, S. Tellex, D. Roy, and N. Roy.
2010.
Toward under-standing natural language directions.
In Proceeding of the5th ACM/IEEE HRI.
ACM.K.
Lovelace, M. Hegarty, and D. Montello.
1999.
Elementsof good route directions in familiar and unfamiliar environ-ments.
Spatial information theory.
Cognitive and computa-tional foundations of geographic information science, pages751?751.M.
MacMahon, B. Stankiewicz, and B. Kuipers.
2006.
Walkthe talk: Connecting language, knowledge, and action inroute instructions.
Def, 2(6):4.M.
Marge and A.I.
Rudnicky.
2010.
Comparing spoken lan-guage route instructions for robots across environment rep-resentations.
In SIGDIAL.M.
Marge, S. Banerjee, and A.I.
Rudnicky.
2010.
Us-ing the amazon mechanical turk for transcription of spo-ken language.
In Acoustics Speech and Signal Processing(ICASSP), 2010 IEEE International Conference on, pages5270?5273.
IEEE.C.
Matuszek, D. Fox, and K. Koscher.
2010.
Following direc-tions using statistical machine translation.
In Proceeding ofthe 5th ACM/IEEE international conference on Human-robotinteraction, pages 251?258.
ACM.R.
Rosenfield.
1995.
The cmu statistical language modelingtoolkit and its use in the 1994 arpa csr evaluation.L.
Stoia, D.M.
Shockley, D.K.
Byron, and E. Fosler-Lussier.2008.
Scare: A situated corpus with annotated referring ex-pressions.
In LREC 2008.K.
Toutanova, D. Klein, C.D.
Manning, and Y.
Singer.
2003.Feature-rich part-of-speech tagging with a cyclic dependencynetwork.
In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computational Lin-guistics on Human Language Technology-Volume 1, pages173?180.
Association for Computational Linguistics.W.
Ward.
1991.
Understanding spontaneous speech: thephoenix system.
In ICASSP.
IEEE.107
