Proceedings of the Fourth International Natural Language Generation Conference, pages 133?135,Sydney, July 2006. c?2006 Association for Computational LinguisticsShared-task Evaluations in HLT: Lessons for NLGAnja BelzUniversity of Brighton, UKA.S.Belz@brighton.ac.ukAdam KilgarriffLexical Computing Ltd., UKadam@lexmasterclass.com1 IntroductionWhile natural language generation (NLG) has astrong evaluation tradition, in particular in user-based and task-oriented evaluation, it has neverevaluated different approaches and techniques bycomparing their performance on the same tasks(shared-task evaluation, STE).
NLG is charac-terised by a lack of consolidation of results, andby isolation from the rest of NLP where STE isnow standard.
It is, moreover, a shrinking field(state-of-the-art MT and summarisation no longerperform generation as a subtask) which lacks thekind of funding and participation that natural lan-guage understanding (NLU) has attracted.Evidence from other NLP fields shows that STEcampaigns (STECs) can lead to rapid technolog-ical progress and substantially increased partici-pation.
The past year has seen a groundswell ofinterest in comparative evaluation among NLG re-searchers, the first comparative results are beingreported (Belz and Reiter, 2006), and the move to-wards some form of comparative evaluation seemsinevitable.
In this paper we look at how twodecades of NLP STECs might help us decide howbest to make this move.2 Shared-task evaluation in HLTOver the past twenty years, virtually every fieldof research in human language technology (HLT)has introduced STECs.
A small selection is pre-sented in the table below1.
NLG researchers havetended to be somewhat unconvinced of the benefitsof comparative evaluation in general, and the kindof competitive, numbers-driven STECs that havebeen typical of NLU in particular.
Yet STECs donot have to be hugely competitive events fixatedon one task with associated input/output data andsingle evaluation metric, static over time.Tasks: There is a distinction between (i) evalu-ations designed to help potential users to decide1Apologies for omissions, and for bias towards English.whether the technology will be valuable to them,and (ii) evaluations designed to help system devel-opers improve the core technology (Spa?rck Jonesand Galliers, 1996).
In the former, the applica-tion context is a critical variable in the task defi-nition; in the latter it is fixed.
Developer-orientedevaluation promotes focus on the task in isolation,but if the context is fixed badly, or if the outsideworld changes but the evaluation does not, thenit becomes irrelevant.
NLP STECs have so far fo-cused on developer-oriented evaluation, but thereare increasing calls for more ?embedded?, moretask-based types of evaluations2 .Existing NLP STECs show that tasks need to bebroadly based and continuously evolving.
To be-gin with, the task needs to be simple, easy to un-derstand and easy for people to recognise as theirtask.
Over time, as the limitations of the sim-ple task are noted and a more substantial com-munity is ?on board?, tasks can multiply, diversifyand become more sophisticated.
This is somethingthat TREC has been good at (still going strong 14years on), and the parsing community has failed toachieve (see notes in table).Evaluation: NLP STECs have tended to use au-tomatic evaluations because of their speed and re-producibility, but some have used human evalua-tors, in particular in fields where language is gen-erated (MT, summarisation, speech synthesis).Evaluation scores are not independent of thetask and context for which they are calculated.This is clearly true of human-based evaluation, buteven scores by a simple metric like word errorrate in speech recognition are not comparable un-less certain parameters are the same: background-noise, language, whether or not speech is con-trolled.
Development of evaluation methods andbenchmark tasks therefore must go hand in hand.Evaluation methods have to be accepted by theresearch community as providing a true approxi-2A prominent theme at the 2005 ELRA/ELDA Workshopon HLT Evaluation.133Name Start Domain Sponsors NotesMUC 1987 Information extraction US Govt Rapidly came to define IE; ended 1998.PARSEVAL 1991 Parsing ?
Only ever defined a metric, no STEC1.TREC 1992 Information retrieval US Govt Large and long-running, multiple tracks.SEMEVAL 1994 Semantic interpretation US Govt No STEC emerged2.NIST-MT 1994 Machine translation US Govt Revitalised since 2001 by BLEU3.Morpholympics 1994 Morphological analysis GLDV German morphological analysis; one-off.SENSEVAL 1998 Word sense disambiguation ACL-SIGLEX Validity of WSD task problematic.SUMMAC 1998 Text summarization US Govt One-off.CoNLL 1999 Various ACL-SIGNLL Focus on learning algorithms.CLEF 2000 IR across languages EU ProjectDUC 2001 Document summarization US Govt Defines field.Morfolimpiadas 2003 Morphological analysis Portuguese Govt Portuguese language only.SIGHAN 2003 Chinese tokenization ACL-SIGHAN Key benchmark.Blizzard 2003 Speech synthesis Festvox project Building synthetic voice from given data.HAREM 2005 Named-entity recognition Portuguese Govt Portuguese language only.RTE 2005 Textual entailment EU ProjectTC-STAR 2005 Speech-to-speech translation EU integrated project Black-box and glass-box evaluation4.Notes1.
PARSEVAL is an evaluation measure, not a full STEC.
This has proved problematic: the parsing community no longeraccepts the PARSEVAL measure, but there has been no organisational framework for establishing an alternative.2.
SEMEVAL did not proceed largely because it was too ambitious and agreement between people with different interestsand theoretical positions was not achieved.
It was eventually reduced in scope and aspects became incorporated in MUC,SUMMAC and SENSEVAL.3.
MT has been transformed by corpus methods, which have shifted MT from a backwater to perhaps the most vibrant areaof NLP in the last five years.4.
In TC-STAR, the SST task is broken down into numerous subtasks.
The modules and systems that meet the given criteriaare exchanged among the participants, lowering the barrier to entry.mation of quality.
E.g.
BLEU is strongly dislikedin the non-statistical part of the MT community be-cause it is biased in favour of statistical MT sys-tems.
PARSEVAL stopped being used when theparsing community moved towards dependencyparsing and related approaches.Sharing: As PARSEVAL shows, measures andresources alone are not enough.
Also required are(i) an event (or better, cycle of events) so peoplecan attend and feel part of a community; (ii) a fo-rum for reviewing task definitions and evaluationmethods; (iii) a committee which ?owns?
the STEC,and organises the next campaign.Funding is usually needed for gold-standardcorpus creation but rarely for anything else (Kil-garriff, 2003).
Participants can be expected tocover the cost of system development and work-shop attendance.
A funded project is best seen assupporting and enabling the STEC (especially dur-ing the early stages) rather than being it.In sum, STECs are good for community build-ing.
They produce energy (as we saw when thepossibility was raised for NLG at UCNLG?05 andENLG?05) which can lead to rapid scientific andtechnological progress.
They make the field looklike a game and draw people in.3 Towards an NLG STECIn 1981, Spa?rck Jones wrote that IR lacked con-solidation and the ability to build new work onold, and that this was substantially because therewas no commonly agreed framework for describ-ing and evaluating systems (Spa?rck Jones, 1981,p.
245).
Since 1981, various NLP sub-disciplineshave consolidated results and progressed collec-tively through STECs, and have seen successfulcommercial deployment of NLP technology (e.g.speech recognition software, document retrievaland dialogue systems).However, Spa?rck Jones?s 1981 analysis couldbe said to still hold of NLG today.
There hasbeen little consolidation of results or collectiveprogress, and there still is virtually no commercialdeployment of NLG systems or components.We believe that comparative evaluation is keyif NLG is to consolidate and progress collectively.Conforming to the evaluation paradigm now com-mon to the rest of NLP will also help re-integration,and open up the field to new researchers.Tasks: In defining sharable tasks with associ-ated data resources for NLG, the core problem isdeciding what inputs should look like.
There isa real risk that agreement cannot be achieved on134this, so not many groups participate, or the plannever reaches fruition (as happened in SEMEVAL).There are, however, ways in which this problemcan be circumvented.
One is to use a more abstracttask specification describing system functionality,so that participants can use their own inputs, andsystems are compared in task-based evaluationssimilar to the traditions and standards of softwareevaluation (as in Morpholympics).
An alternativeis to approach the issue through tasks with inputsand outputs that ?occur naturally?, so that partic-ipants can use their own NLG-specific represen-tations.
Examples include data-to-text mappingswhere e.g.
time-series data or a data repository aremapped to fault reports, forecasts, etc.Both data-independent task definitions andtasks with naturally occurring data have promise,but we propose the second as the simpler, easierto organise solution, at least initially.
A specificproposal of a set of tasks can be found elsewherein this volume (Reiter and Belz, 2006).
An inter-esting idea (recommended by ELRA/ELDA) is tobreak down the input-output mapping into stages(as in the TC-STAR workshops, see table) and then,in a second round of evaluations, to make availableintermediate representations from the most suc-cessful systems from the first round.
In this way,standardised representations might develop almostas a side-effect of STECs.Evaluation: As in MT there are at least two cri-teria of quality for NLG systems: language quality(fluency in MT) and correctness of content (ade-quacy in MT).
In NLG, these have mostly beenevaluated directly using human scores or prefer-ence judgments, although recently automatic met-rics such as BLEU have been used.
They have alsobeen evaluated indirectly, e.g.
by measuring read-ing speeds and manual post-processing3 .
A moreuser-oriented type of evaluation has been to assessreal-world usefulness, in other words, whether thegenerated texts achieve their purpose (e.g.
whetherusers learn more with NLG techniques than withcheaper alternatives4).The majority of NLP STECs have used automaticevaluation methods, and the ability to produce re-sults ?at the push of a button?, quickly and repro-ducibly, is ideal in the context of STECs.
However,existing metrics are unlikely to be suitable for NLG3E.g.
in the SkillSum and SumTime projects at Aberdeen.4E.g.
evaluation of the NL interface of the DIAG intelligenttutoring system, di Eugenio et al(Belz and Reiter, 2006), and there is a lot of scepti-cism among NLG researchers regarding automaticevaluation.
We believe that NLG should developits own automatic metrics (development of suchmetrics is part of the proposal by Reiter and Belz,this volume), but for the time being an NLG STECneeds to involve human-based evaluations of theintrinsic as well as extrinsic type.Sharing: A recent survey conducted on the mainNLG and corpus-based NLP mailing lists5 revealedthat there are virtually no data resources that couldbe directly used in shared tasks.
Considerable in-vestment has to go into developing such resources,and direct funding is necessary.
This points to afunded project, but we recommend direct involve-ment of the NLG community and SIGGEN.
Otheraspects of organisation are not NLG-specific, sothe general recommendations in the preceding sec-tion apply.4 ConclusionSTECs have been remarkable stimulants toprogress in other areas of HLT, through theircommunity-building role, and through ?hot-housing?
solutions to specific problems.
There arealso lessons to be learnt about STECs not beingoverly ambitious, remaining responsive to devel-opments in the broader field and wider world, andhaving appropriate institutional standing.
We be-lieve that NLG can benefit greatly from the intro-duction of shared tasks, provided that an inclusiveand flexible approach is taken which is informedby the specific requirements of NLG.ReferencesA.
Belz and E. Reiter.
2006.
Comparing automaticand human evaluation of NLG systems.
In Proc.EACL?06, pages 313?320.A.
Kilgarriff.
2003.
No-bureaucracy evaluation.
InProc.
Workshop on Evaluation Initiatives in NLP,EACL?03.E.
Reiter and A. Belz.
2006.
GENEVAL: A pro-posal for shared-task evaluation in NLG.
In Proc.INLG?06.K.
Spa?rck Jones and J. R. Galliers.
1996.
EvaluatingNatural Language Processing Systems: An Analysisand Review.
Springer Verlag.K.
Spa?rck Jones, 1981.
Information Retrieval Experi-ment, chapter 12.
Butterworth & Co.5Belz, March 2006.135
