Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 77?84,Sydney, July 2006. c?2006 Association for Computational LinguisticsRe-evaluating Machine Translation Results with Paraphrase SupportLiang Zhou, Chin-Yew Lin, and Eduard HovyUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695{liangz, cyl, hovy} @isi.eduAbstractIn this paper, we present ParaEval, anautomatic evaluation framework that usesparaphrases to improve the quality ofmachine translation evaluations.
Previouswork has focused on fixed n-gramevaluation metrics coupled with lexicalidentity matching.
ParaEval addressesthree important issues: support for para-phrase/synonym matching, recall meas-urement, and correlation with humanjudgments.
We show that ParaEval corre-lates significantly better than BLEU withhuman assessment in measurements forboth fluency and adequacy.1 IntroductionThe introduction of automated evaluation proce-dures, such as BLEU (Papineni et al, 2001) formachine translation (MT) and ROUGE (Lin andHovy, 2003) for summarization, have promptedmuch progress and development in both of theseareas of research in Natural Language Processing(NLP).
Both evaluation tasks employ a compari-son strategy for comparing textual units frommachine-generated and gold-standard texts.
Ide-ally, this comparison process would be per-formed manually, because of humans?
abilities toinfer, paraphrase, and use world knowledge torelate differently worded pieces of equivalentinformation.
However, manual evaluations aretime consuming and expensive, thus makingthem a bottleneck in system development cycles.BLEU measures how close machine-generatedtranslations are to professional human transla-tions, and ROUGE does the same with respect tosummaries.
Both methods incorporate the com-parison of a system-produced text to one or morecorresponding reference texts.
The closeness be-tween texts is measured by the computation of anumeric score based on n-gram co-occurrencestatistics.
Although both methods have gainedmainstream acceptance and have shown goodcorrelations with human judgments, their defi-ciencies have become more evident and seriousas research in MT and summarization progresses(Callison-Burch et al, 2006).Text comparisons in MT and summarizationevaluations are performed at different text granu-larity levels.
Since most of the phrase-based,syntax-based, and rule-based MT systems trans-late one sentence at a time, the text comparisonin the evaluation process is also performed at thesingle-sentence level.
In summarization evalua-tions, there is no sentence-to-sentence corre-spondence between summary pairs?essentiallya multi-sentence-to-multi-sentence comparison,making it more difficult and requiring a com-pletely different implementation for matchingstrategies.
In this paper, we focus on the intrica-cies involved in evaluating MT results and ad-dress two prominent problems associated withthe BLEU-esque metrics, namely their lack ofsupport for paraphrase matching and the absenceof recall scoring.
Our solution, ParaEval, utilizesa large collection of paraphrases acquiredthrough an unsupervised process?identifyingphrase sets that have the same translation in an-other language?using state-of-the-art statisticalMT word alignment and phrase extraction meth-ods.
This collection facilitates paraphrase match-ing, additionally coupled with lexical identitymatching which is designed for comparingtext/sentence fragments that are not consumed byparaphrase matching.
We adopt a unigram count-ing strategy for contents matched between sen-tences from peer and reference translations.
Thisunweighted scoring scheme, for both precisionand recall computations, allows us to directlyexamine both the power and limitations of77ParaEval.
We show that ParaEval is a more sta-ble and reliable comparison mechanism thanBLEU, in both fluency and adequacy rankings.This paper is organized in the following way:Section 2 shows an overview on BLEU and lexi-cal identity n-gram statistics; we describe ParaE-val?s implementation in detail in Section 3; theevaluation of ParaEval is shown in Section 4;recall computation is discussed in Section 5; inSection 6, we discuss the differences betweenBLEU and ParaEval when the numbers of refer-ence translations change; and we conclude anddiscuss future work in Section 7.2  N-gram Co-occurrence StatisticsBeing an $8 billion industry (Browner, 2006),MT calls for rapid development and the ability todifferentiate good systems from less adequateones.
The evaluation process consists of compar-ing system-generated peer translations  to humanwritten reference translations  and assigning anumeric score to each system.
While human as-sessments are still the most reliable evaluationmeasurements, it is not practical to solicit manualevaluations repeatedly while making incrementalsystem design changes that would only result inmarginal performance gains.
To overcome themonetary and time constraints associated withmanual evaluations, automated procedures havebeen successful in delivering benchmarks forperformance hill-climbing with little or no cost.While a variety of automatic evaluation meth-ods have been introduced, the underlining com-parison strategy is similar?matching based onlexical identity.
The most prominent implemen-tation of this type of matching is demonstrated inBLEU (Papineni et al 2002).
The remaining partof this section is devoted to an overview ofBLEU, or the BLEU-esque philosophy.2 .1  The BL E U-esque Matching PhilosophyThe primary task that a BLEU-esque procedureperforms is to compare n-grams from the peertranslation with the n-grams from one or morereference translations and count the number ofmatches.
The more matches a peer translationgets, the better it is.BLEU is a precision-based metric, which isthe ratio of the number of n-grams from the peertranslation that occurred in reference translationsto the total number of n-grams in the peer trans-lation.
The notion of Modified n-gram Precisi onwas introduced to detect and avoid rewardingfalse positives generated by translation systems.To gain high precision, systems could potentiallyover-generate ?good?
n-grams, which occur mul-tiple times in multiple references.
The solution tothis problem was to adopt the policy that an n-gram, from both reference and peer translations,is considered exhausted after participating in amatch.
As a result, the maximum number ofmatches an n-gram from a peer translation canreceive, when comparing to a set of referencetranslations, is the maximum number of timesthis n-gram occurred in any single referencetranslation.
Papineni et al (2002) called this cap-ping technique clipp ing.
Figure 1, taken from theoriginal BLEU paper, demonstrates the computa-tion of the modified unigram precision for a peertranslation sentence.To compute the modified n-gram precision,P n, for a whole test set, including all translationsegments (usually in sentences), the formula is:2 .2  Lack of Paraphrasing SupportHumans are very good at finding creative waysto convey the same information.
There is no onedefinitive reference translation in one languagefor a text written in another.
Having acknowl-edged this phenomenon, however natural it is,human evaluations on system-generated transla-tions are much more preferred and trusted.
How-ever, what humans can do with ease puts ma-chines at a loss.
BLEU-esque procedures recog-nize equivalence only when two n-grams exhibitthe same surface-level representations, i.e.
thesame lexical identities.
The BLEU implementa-tion addresses its deficiency in measuring seman-tic closeness by incorporating the comparisonwith multiple reference translations.
The ration-ale is that multiple references give a higherchance that the n-grams, assuming correct trans-lations, appearing in the peer translation wouldbe rewarded by one of the reference?s n-grams.The more reference translations used, the betterFigure 1.
Modified n-gram precision fromBLEU.78the matching and overall evaluation quality.
Ide-ally (and to an extreme), we would need to col-lect a large set of human-written translations tocapture all possible combinations of verbalizingvariations before the translation comparison pro-cedure reaches its optimal matching ability.One can argue that an infinite number of ref-erences are not needed in practice because anymatching procedure would stabilize at a certainnumber of references.
This is true if precisionmeasure is the only metric computed.
However,using precision scores alone unfairly rewardssystems that ?under-generate?
?producing un-reasonably short translations.
Recall measure-ments would provide more balanced evaluations.When using multiple reference translations, if ann-gram match is made for the peer, this n-gramcould appear in any of the references.
The com-putation of recall becomes difficult, if not impos-sible.
This problem can be reversed if there iscrosschecking for phrases occurring across refer-ences?paraphrase recognition.
BLEU uses thecalculation of a brevity penalty to compensatethe lack of recall computation problem.
Thebrevity penalty is computed as follows:Then, the BLEU score for a peer translation iscomputed as:BLEU?s adoption of the brevity penalty to off-set the effect of not having a recall computationhas drawn criticism on its crudeness in measur-ing translation quality.
Callison-Burch et al(2006) point out three prominent factors:?
``Synonyms and paraphrases are onlyhandled if they are in the set of multiplereference translations [available].?
The scores for words are equallyweighted so missing out on content-bearing material brings no additional pen-alty.?
The brevity penalty is a stop-gap meas-ure to compensate for the fairly seriousproblem of not being able to calculate re-call.
?With the introduction of ParaEval, we will ad-dress two of these three issues, namely the para-phrasing problem and providing a recall meas-ure.3  ParaEval for MT Evaluation3.1  OverviewReference translations are created from the samesource text (written in the foreign language) tothe target language.
Ideally, they are supposed tobe semantically equivalent, i.e.
overlap com-pletely.
However, as shown in Figure 2, whenmatching based on lexical identity is used (indi-cated by links), only half (6 from the left and 5from the right) of the 12 words from these twosentences are matched.
Also, ?to?
was a mis-match.
In applying paraphrase matching for MTevaluation from ParaEval, we aim to match allshaded words from both sentences.3 .2  Paraphrase AcquisitionThe process of acquiring a large enough collec-tion of paraphrases is not an easy task.
Manualcorpus analyses produce domain-specific collec-tions that are used for text generation and areapplication-specific.
But operating in multipledomains and for multiple tasks translates intomultiple manual collection efforts, which couldbe very time-consuming and costly.
In order tofacilitate smooth paraphrase utilization across avariety of NLP applications, we need an unsu-pervised paraphrase collection mechanism thatcan be easily conducted, and produces para-phrases that are of adequate quality and can bereadily used with minimal amount of adaptationeffort.Our method (Anonymous, 2006), also illus-trated in (Bannard and Callison-Burch, 2005), toautomatically construct a large domain-independent paraphrase collection is based on theassumption that two different phrases of thesame meaning may have the same translation in aFigure 2.
Two reference translations.
Greyareas are matched by using BLEU.79foreign language.
Phrase-based Statistical Ma-chine Translation (SMT) systems analyze largequantities of bilingual parallel texts in order tolearn translational alignments between pairs ofwords and phrases in two languages (Och andNey, 2004).
The sentence-based translationmodel makes word/phrase alignment decisionsprobabilistically by computing the optimal modelparameters with application of the statistical es-timation theory.
This alignment process results ina corpus of word/phrase-aligned parallel sen-tences from which we can extract phrase pairsthat are translations of each other.
We ran thealignment algorithm from (Och and Ney, 2003)on a Chinese-English parallel corpus of 218 mil-lion English words, available from the LinguisticData Consortium (LDC).
Phrase pairs are ex-tracted by following the method described in(Och and Ney, 2004) where all contiguousphrase pairs having consistent alignments areextraction candidates.
Using these pairs we buildparaphrase sets by joining together all Englishphrases that have the same Chinese translation.Figure 3 shows an example word/phrase align-ment for two parallel sentence pairs from ourcorpus where the phrases ?blowing up?
and?bombing?
have the same Chinese translation.On the right side of the figure we show the para-phrase set which contains these two phrases,which is typical in our collection of extractedparaphrases.Although our paraphrase extraction method issimilar to that of (Bannard and Callison-Burch,2005), the paraphrases we extracted are for com-pletely different applications, and have a broaderdefinition for what constitutes a paraphrase.
In(Bannard and Callison-Burch, 2005), a languagemodel is used to make sure that the paraphrasesextracted are direct substitutes, from the samesyntactic categories, etc.
So, using the examplein Figure 3, the paraphrase table would containonly ?bombing?
and ?bombing attack?.
Para-phrases that are direct substitutes of one anotherare useful when translating unknown phrases.For instance, if a MT system does not have theChinese translation for the word ?bombing?, buthas seen it in another set of parallel data (not in-volving Chinese) and has determined it to be adirect substitute of the phrase ?bombing attack?,then the Chinese translation of ?bombing attack?would be used in place of the translation for?bombing?.
This substitution technique hasshown some improvement in translation quality(Callison-Burch et al, 2006).3 .3  The ParaEval Evaluation ProcedureWe adopt a two-tier matching strategy for MTevaluation in ParaEval.
At the top tier, a para-phrase match is performed on system-translatedsentences and corresponding reference sentences.Then, unigram matching is performed on thewords not matched by paraphrases.
Precision ismeasured as the ratio of the total number ofwords matched to the total number of words inthe peer translation.Running our system on the example in Figure2, the paraphrase-matching phase consumes thewords marked in grey and aligns ?have been?and ?to be?, ?completed?
and ?fully?, ?to date?and ?up till now?, and ?sequence?
and ?se-quenced?.
The subsequent unigram-matchingaligns words based on lexical identity.We maintain the computation of modified uni-gram precisi on , defined by the BLEU-esque Phi-losophy, in principle.
In addition to clipping in-dividual candidate words  with their correspond-ing maximum reference counts (only for wordsnot matched by paraphrases), we clip candidateparaphrases  by their maximum reference para-phrase counts.
So two completely differentphrases in a reference sentence can be counted astwo occurrences of one phrase.
For example inFigure 4, candidate phrases ?blown up?
and?bombing?
matched with three phrases from thereferences, namely ?bombing?
and two instancesof ?explosion?.
Treating these two candidatephrases as one (paraphrase match), we can see itsclip is 2 (from Ref 1, where ?bomb ing?
and ?ex-plosion?
are counted as two occurrences of a sin-gle phrase).
The only word that was matched byits lexical identity is ?was?.
The modified uni-gram precision calculated by our method is 4/5,where as BLEU gives 2/5.Figure 3.
An example of the paraphrase extractionprocess.804  Evaluating ParaEvalTo be effective in MT evaluations, an automatedprocedure should be capable of distinguishinggood translation systems from bad ones, humantranslations from systems?, and human transla-tions of differing quality.
For a particular evalua-tion exercise, an evaluation system produces aranking for system and human translations, andcompares this ranking with one created by hu-man judges (Turian et al, 2003).
The closer asystem?s ranking is to the human?s, the better theevaluation system is.4 .1  Validating ParaEvalTo test ParaEval?s ability, NIST 2003 ChineseMT evaluation results were used (NIST 2003).This collection consists of 100 source documentsin Chinese, translations from eight individualtranslation systems, reference translations fromfour humans, and human assessments (on flu-ency and adequacy).
The Spearman rank-ordercoefficient is computed as an indicator of howclose a system ranking is to gold-standard humanranking.
It should be noted that the 2003 MTdata is separate from the corpus that we extractedparaphrases from.For comparison purposes, BLEU 1  was alsorun.
Table 1 shows the correlation figures for thetwo automatic systems with the NIST rankingson fluency and adequacy.
The lower and higher95% confidence intervals are labeled as ?L-CI?and ?H-CI?.
To estimate the significance of therank-order correlation figures, we applied boot-strap resampling to calculate the confidence in-tervals.
In each of 1000 runs, systems wereranked based on their translations of 100 ran-domly selected documents.
Each ranking wascompared with the NIST ranking, producing acorrelation score for each run.
A t-test was then1 Results shown are from BLEU v.11 (NIST).performed on the 1000 correlation scores.
In bothfluency and adequacy measurements, ParaEvalcorrelates significantly better than BLEU.
TheParaEval scores used were precision scores.
Inaddition to distinguishing the quality of MT sys-tems, a reliable evaluation procedure must beable to distinguish system translations from hu-mans?
(Lin and Och, 2004).
Figure 5 shows theoverall system and human ranking.
In the upperleft corner, human translators are grouped to-gether, significantly separated from the auto-matic MT systems clustered into the lower rightcorner.4 .2  Implications to Word-alignmentWe experimented with restricting the para-phrases being matched to various lengths.
Whenallowing only paraphrases of three or morewords to match, the correlation figures becomestabilized and ParaEval achieves even highercorrelation with fluency measurement to 0.7619on the Spearman ranking coefficient.This phenomenon indicates to us that the bi-gram and unigram paraphrases extracted usingSMT word-alignment and phrase extraction pro-grams are not reliable enough to be applied toevaluation tasks.
We speculate that word pairsextracted from (Liang et al, 2006), where a bidi-rectional discriminative training method wasused to achieve consensus for word-alignmentFigure 4.
ParaEval?s matching process.BLEU ParaEvalFluency 0.6978 0.757595% L-CI 0.6967 0.755395% H-CI 0.6989 0.7596Adequacy 0.6108 0.691895% L-CI 0.6083 0.689595% H-CI 0.6133 0.694Table 1.
Ranking correlations with humanassessments.Figure 5.
Overall system and human ranking.81(mostly lower n-grams), would help to elevatethe level of correlation by ParaEval.4 .3  Implications to Evaluating ParaphraseQualityUtilizing paraphrases in MT evaluations is also arealistic way to measure the quality of para-phrases acquired through unsupervised channels.If a comparison strategy, coupled with para-phrase matching, distinguishes good and bad MTand summarization systems in close accordancewith what human judges do, then this strategyand the paraphrases used are of sufficient quality.Since our underlining comparison strategy is thatof BLEU-1 for MT evaluation, and BLEU hasbeen proven to be a good metric for their respec-tive evaluation tasks, the performance of theoverall comparison is directly and mainly af-fected by the paraphrase collection.5  ParaEval?s Support for Recall Com-putationDue to the use of multiple references and allow-ing an n-gram from the peer translation to bematched with its corresponding n-gram from anyof the reference translations, BLEU cannot beused to compute recall scores, which are conven-tionally paired with precision to detect length-related problems from systems under evaluation.5 .1  Using Single References for RecallThe primary goal in using multiple references isto overcome the limitation in matching on lexicalidentity.
More translation choices give morevariations in verbalization, which could lead tomore matches between peer and reference trans-lations.
Since MT results are generated andevaluated at a sentence-to-sentence level (or asegment level, where each segment may containa small number of sentences) and no text con-densation is employed, the number of differentand correct ways to state the same sentence issmall.
This is in comparison to writing genericmulti-document summaries, each of which con-tains multiple sentences and requires significantamount of ?rewriting?.
When using a large col-lection of paraphrases while evaluating, we areprovided with the alternative verbalizationsneeded.
This property allows us to use singlereferences to evaluate MT results and computerecall measurements.5 .2  Recall and Adequacy CorrelationsWhen validating the computed recall scores forMT systems, we correlate with human assess-ments on adequacy only.
The reason is that ac-cording to the definition of recall, the contentcoverage in references, and not the fluency re-flected from the peers, is being measured.
Table2 shows ParaEval?s recall correlation with NIST2003 Chinese MT evaluation results on systemsranking.
We see that ParaEval?s correlation withadequacy has improved significantly when usingrecall scores to rank than using precision scores.5 .3  Not All Single References are CreatedEqualHuman-written translations differ not only inword choice, but also in other idiosyncrasies thatcannot be captured with paraphrase recognition.So it would be presumptuous to declare that us-ing paraphrases from ParaEval is enough to al-low using just one reference translation to evalu-ate.
Using multiple references allow more para-phrase sets to be explored in matching.In Table 3, we show ParaEval?s correlationfigures when using single reference translations.E01?E04 indicate the sets of human translationsused correspondingly.Notice that the correlation figures vary a greatdeal depending on the set of single referencesused.
How do we differentiate human transla-tions and know which set of references to use?
Itis difficult to quantify the quality that a humanwritten translation reflects.
We can only define?good?
human translations as translations thatare written not very differently from what otherhumans would write, and ?bad?
translations asthe ones that are written in an unconventionalfashion.
Table 4 shows the differences betweenthe four sets of reference translations when com-BLEU ParaEvalAdequacy 0.6108 0.737395% L-CI 0.6083 0.736895% H-CI 0.6133 0.7377Table 2.
ParaEval?s recall ranking correlation.Table 3.
ParaEval?s correlation (precision)while using only single references.E01 E02 E03 E04Fluency 0.683 0.6501 0.7284 0.619295% L-CI 0.6795 0.6482 0.7267 0.617295% H-CI 0.6864 0.6519 0.73 0.6208Adequacy 0.6308 0.5741 0.6688 0.585895% L-CI 0.6266 0.5705 0.665 0.582195% H-CI 0.635 0.5777 0.6727 0.589582paring one set of references to the other three.The scores here are the raw ParaEval precisionscores.
E01 and E03 are better, which explainsthe higher correlations ParaEval has using thesetwo sets of references individually, shown in Ta-ble 3.6  Observation of Change in Number ofReferencesWhen matching on lexical identity, it is the gen-eral consensus that using more reference transla-tions would increase the reliability of the MTevaluation (Turian et al, 2003).
It is expectedthat we see an improvement in ranking correla-tions when moving from using one referencetranslation to more.
However, when runningBLEU for the NIST 2003 Chinese MT evalua-tion, this trend is inverted, and using single refer-ence translation gave higher correlation than us-ing all four references, as illustrated in Table 5.Turian et al (2003) reports the same peculiarbehavior from BLEU on Arabic MT evaluationsin Figure 5b of their paper.
When using threereference translations, as the number of segments(sentences usually) increases, BLEU correlatesworse than using single references.Since the matching and underlining countingmechanisms of ParaEval are built upon thefundamentals of BLEU, we were keen to find outthe differences, other than paraphrase matching,between the two methods when the number ofreference translation changes.
By following thedescription from the original BLEU paper, threeincremental steps were set up for duplicating itsimplementation, namely modified unigram preci-sion (MUP), geometric mean of MUP (GM), andmultiplying brevity penalty with GM to get thefinal score (BP-BLEU).
At each step, correla-tions were computed for both using single- andmulti- references, shown in Table 6a, b, and c.Given that many small changes have beenmade to the original BLEU design, our replica-tion would not produce the same scores from thecurrent version of BLEU.
Nevertheless, the in-verted behavior was observed in fluency correla-tions at the BP-BLEU step, not at MUP and GM.This indicates to us that the multiplication of thebrevity penalty to balance precision scores isproblematic.
According to (Turian et al, 2003),correlation scores computed from using fewerreferences are inflated because the comparisonsexclude the longer n-gram matches that makeautomatic evaluation procedures diverge fromthe human judgments.
Using a large collection ofparaphrases in comparisons allows those longern-gram matches to happen even if single refer-ences are used.
This collection also allowsParaEval to directly compute recall scores,avoiding an approximation of recall that isproblematic.ParaEval 95% L-CI 95% H- C IE01 0.8086 0.8 0 .8172E02 0.7383 0.7268 0.7497E03 0.7839 0.7754 0.7923E04 0.7742 0.7617 0.7866Table 4.
Differences among referencetranslations (raw ParaEval precisionscores).6(a).
System-ranking correlation when using modifiedunigram precision (MUP) scores.6(b).
System-ranking correlation when using geometric mean(GM) of MUPs.6(c).
System-ranking correlation when multiplying thebrevity penalty with GM.Table 6.
Incremental implementation ofBLEU and the correlation behavior at thethree steps: MUP, GM, and BP-BLEU.MUP E01 E02 E03 E04 4 refsFluency 0.6597 0.6216 0.6923 0.4912 0.69295% L-CI 0.6568 0.6189 0.6917 0.4863 0.691595% H-CI 0.6626 0.6243 0.6929 0.496 0.6925Adequacy 0.5818 0.5459 0.6141 0.4602 0.616595% L-CI 0.5788 0.5432 0.6132 0.4566 0.615695% H-CI 0.5847 0.5486 0.6151 0.4638 0.6174GM E01 E02 E03 E04 4 refsFluency 0.6633 0.6228 0.6925 0.4911 0.692295% L-CI 0.6604 0.6201 0.692 0.4862 0.691895% H-CI 0.6662 0.6255 0.6931 0.4961 0.6929Adequacy 0.5817 0.548 0.615 0.4641 0.615995% L-CI 0.5813 0.5453 0.614 0.4606 0.61595% H-CI 0.5871 0.5508 0.616 0.4676 0.6169BP-BLEU E01 E02 E03 E04 4 refsFluency 0.6637 0.6227 0.6921 0.4947 0.574395% L-CI 0.6608 0.62 0.6916 0.4899 0.569995% H-CI 0.6666 0.6254 0.6927 0.4996 0.5786Adequacy 0.5812 0.5486 0.5486 0.5486 0.667195% L-CI 0.5782 0.5481 0.5458 0.5458 0.664595% H-CI 0.5842 0.5514 0.5514 0.5514 0.6697Table 5.
BLEU?s correlating behavior withmulti- and single-reference.BLEU E01 E02 E03 E04 4 refsFluency 0.7114 0.701 0.7084 0.7192 0.697895% L-CI 0.7099 0.6993 0.7065 0.7177 0.696795% H-CI 0.7129 0.7026 0.7102 0.7208 0.6989Adequacy 0.644 0.6238 0.6535 0.675 0.610895% L-CI 0.6404 0.6202 0.6496 0.6714 0.608395% H-CI 0.6476 0.6274 0.6574 0.6786 0.6133837  Conclusion and Future WorkIn this paper, we have described ParaEval, anautomatic evaluation framework for measuringmachine translation results.
A large collection ofparaphrases, extracted through an unsupervisedfashion using SMT methods, is used to improvethe quality of the evaluations.
We addressedthree important issues, the paraphrasing support,the computation of recall measurement, and pro-viding high correlations with human judgments.Having seen that using paraphrases helps agreat deal in evaluation tasks, naturally the nexttask is to explore the possibility in paraphraseinduction.
The question becomes how to use con-textual information to calculate semantic close-ness between two phrases.
Can we expand theidentification of paraphrases to longer ones, ide-ally sentences?The problem in which content bearing wordscarry the same weights as the non-content bear-ing ones is not addressed.
From examining theparaphrase extraction process, it is unclear howto relate translation probabilities and confidenceswith semantic closeness.
We plan to explore theparallels between the two to enable a weightedimplementation of ParaEval.ReferenceAnonymous.
2006.
Complete citation omitted due tothe blind review process.Bannard, C. and C. Callison-Burch.
2005.
Paraphras-ing with bilingual parallel corpora.
Proceedings ofACL-20 0 5 .Browner, J.
2006.
The translator?s blues.http://www.slate.com/id/2133922/.Callison-Burch, P. Koehn, and M. Osborne.
2006.Improved statistical machine translation usingparaphrases.
In Proceedings of H L T / NA A CL-20 0 6 .Callison-Burch, C., M. Osborne, and P. Koehn.
2006.Re-evaluating the role of bleu in machine transla-tion research.
In Proceedings of EA CL-20 0 6 .Inkpen, D. Z. and G. Hirst.
2003.
Near-synonymchoice in natural language generation.
Proceedingsof R A NL P-20 0 3 .Leusch, G., N. Ueffing, and H. Ney.
2003.
A novelstring-to-string distance measure with applicationsto machine translation evaluation.
In Proceedingsof MT  Summit I X .Liang, P., B. Taskar, and D. Klein.
Consensus of sim-ple unsupervised models for word alignment.
InProceedings in H LT / N AA CL-2 0 0 6 .Lin, C.Y.
and E. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.Proceedings of the H L T-20 0 3 .Lin, C.Y.
and F. J. Och.
2004.
Automatic evaluationof machine translation quality using longest com-mon subsequence and skip-bigram statistics.
Pro-ceedings of A CL- 20 0 4 .Och, F. J. and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguist ics , 29(1): 19?51, 2003.Och, F. J. and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Compu-tational L inguis tics , 30(4), 2004.Papineni, K., S. Roukos, T. Ward, and W. J. Zhu.2002.
IBM research report Bleu: a method forautomatic evaluation of machine translation I B MResearch D iv is ion Technical Report , RC22176,2001.Turian, J. P., L. Shen, and I. D. Melamed.
2003.Evaluation of machine translation and its evalua-tion.
Proceedings of MT  Summit I X .84
