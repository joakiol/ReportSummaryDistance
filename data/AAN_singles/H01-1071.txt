Towards Automatic Sign TranslationJie Yang, Jiang Gao, Ying Zhang, Alex WaibelInteractive Systems LaboratoryCarnegie Mellon UniversityPittsburgh, PA 15213 USA{yang+,jgao,joy,waibel}@cs.cmu.eduABSTRACTSigns are everywhere in our lives.
They make our lives easierwhen we are familiar with them.
But sometimes they also poseproblems.
For example, a tourist might not be able to understandsigns in a foreign country.
In this paper, we present our effortstowards automatic sign translation.
We discuss methods forautomatic sign detection.
We describe sign translation usingexample based machine translation technology.
We use a user-centered approach in developing an automatic sign translationsystem.
The approach takes advantage of human intelligence inselecting an area of interest and domain for translation if needed.A user can determine which sign is to be translated if multiplesigns have been detected within the image.
The selected part ofthe image is then processed, recognized, and translated.
We havedeveloped a prototype system that can recognize Chinese signsinput from a video camera which is a common gadget for a tourist,and translate them into English text or voice stream.KeywordsSign, sign detection, sign recognition, sign translation.1.
INTRODUCTIONLanguages play an important role in human communication.We communicate with people and information systemsthrough diverse media in increasingly varied environments.One of those media is a sign.
A sign is something thatsuggests the presence of a fact, condition, or quality.
Signsare everywhere in our lives.
They make our lives easierwhen we are familiar with them.
But sometimes they alsopose problems.
For example, a tourist might not be able tounderstand signs in a foreign country.
Unfamiliar languageand environment make it difficult for international touriststo read signs, take a taxi, order food, and understand thecomments of passersby.At the Interactive Systems Lab of Carnegie MellonUniversity, we are developing technologies for touristapplications [12].
The systems are equipped with a uniquecombination of sensors and software.
The hardwareincludes computers, GPS receivers, lapel microphones andearphones, video cameras and head-mounted displays.
Thiscombination enables a multimodal interface to takeadvantage of speech and gesture inputs to provideassistance for tourists.
The software supports naturallanguage processing, speech recognition, machinetranslation, handwriting recognition and multimodal fusion.A vision module is trained to locate and read writtenlanguage, is able to adapt to new environments, and is ableto interpret intentions offered by the user, such as a spokenclarification or pointing gesture.In this paper, we present our efforts towards automatic signtranslation.
A system capable of sign detection andtranslation would benefit three types of individuals: tourists,the visually handicapped and military intelligence.
Signtranslation, in conjunction with spoken languagetranslation, can help international tourists to overcome thesebarriers.
Automatic sign recognition can help us to increaseenvironmental awareness by effectively increasing our fieldof vision.
It can also help blind people to extractinformation.
A successful sign translation system relies onthree key technologies: sign extraction, optical characterrecognition (OCR), and language translation.
Althoughmuch research has been directed to automatic speechrecognition, handwriting recognition, OCR, speech and texttranslation, little attention has been paid to automatic signrecognition and translation in the past.
Our current researchis focused on automatic sign detection and translation whiletaking advantage of OCR technology available.
We havedeveloped robust automatic sign detection algorithms.
Wehave applied Example Based Machine Translation (EBMT)technology [1] in sign translation.Fully automatic extraction of signs from the environment isa challenging problem because signs are usually embeddedin the environment.
Sign translation has some specialproblems compared to a traditional language translationtask.
They can be location dependent.
The same text ondifferent signs can be treated differently.
For example, it isnot necessary to translate the meanings for names, such asstreet names or company names, in most cases.
In thesystem development, we use a user-centered approach.
Theapproach takes advantage of human intelligence in selectingan area of interest and domain for translation if needed.
Forexample, a user can determine which sign is to be translatedif multiple signs have been detected within the image.
Theselected part of the image is then processed, recognized,and translated, with the translation displayed on a hand-heldwearable display, or a head mounted display, or synthesizedas a voice output message over the earphones.
By focusingonly on the information of interest and providing domainknowledge, the approach provides a flexible method forsign translation.
It can enhance the robustness of signrecognition and translation, and speed up the recognitionand translation process.
We have developed a prototypesystem that can recognize Chinese sign input from a videocamera which is a common gadget for a tourist, andtranslate the signs into English text or voice stream.The organization of this paper is as follows: Section 2describes challenges in sign recognition and translation.Section 3 discusses methods for sign detection.
Section 4addresses the application of EBMT technology into signtranslation.
Section 5 introduces a prototype system forChinese sign translation.
Section 6 gives experimentalresults.
Section 7concludes the paper.2.
PROBLEM DESCRIPTIONA sign can be a displayed structure bearing letters orsymbols, used to identify or advertise a place of business.
Itcan also be a posted notice bearing a designation, direction,or command.
Figure 1 and Figure 2 illustrate two examplesof signs.
Figure 1 shows a Russian sign completelyembedded in the background.
Figure 2 is a sign thatcontains German text with no verb and article.
In thisresearch, we are interested in translating signs that havedirect influence upon a tourist from a different country orculture.
These signs, at least, include the followingcategories:?
Names: street, building, company, etc.?
Information: designation, direction, safetyadvisory, warning, notice, etc.?
Commercial: announcement, advertisement, etc.?
Traffic: warning, limitation, etc.?
Conventional symbol: especially those areconfusable to a foreign tourist, e.g., some symbolsare not international.Fully automatic extraction of signs from the environment isa challenging problem because signs are usually embeddedin the environment.
The related work includes video OCRand automatic text detection.
Video OCR is used to capturetext in the video images and recognize the text.
Manyvideo images contain text contents.
Such text can comefrom computer-generated text that is overlaid on theimagery (e.g., captions in broadcast news programs) or textthat appears as a part of the video scene itself (e.g., a signoutside a place of business, or a post).
Location andrecognition of text in video imagery is challenging due tolow resolution of characters and complexity of background.Research in video OCR has mainly focused on locating thetext in the image and preprocessing the text area for OCR[4][6][7][9][10].
Applications of the research includeautomatically identifying the contents of video imagery forvideo index [7][9], and capturing documents from papersource during reading and writing [10].
Compared to othervideo OCR tasks, sign extraction takes place in a moredynamic environment.
The user?s movement can causeunstable input images.
Non-professional equipment canmake the video input poorer than that of other video OCRtasks, such as detecting captions in broadcast newsprograms.
In addition, sign extraction has to beimplemented in real time using limited resources.Figure 1 A sign embedded in the backgroundFigure 2 A German signSign translation requires sign recognition.
A straightforwardidea is to use advanced OCR technology.
Although OCRtechnology works well in many applications, it requiressome improvements before it can be applied to signrecognition.
At current stage of the research, we will focusour research on sign detection and translation while takingadvantage of state-of-the-art OCR technologies.Sign translation has some special problems compared to atraditional language translation task.
The function of signslead to the characteristic of the text used in the sign: it hasto be short and concise.
The lexical mismatch and structuralmismatch problems become more severe in sign translationbecause shorter words/phrases are more likely to beambiguous and insufficient information from the text toresolve the ambiguities which are related to theenvironment of the sign.We assume that a tourist has a video camera to capturesigns into a wearable or portable computer.
The procedureof sign translation is as follows: capturing the image withsigns, detecting signs in the image, recognizing signs, andtranslating results of sign recognition into target language.3.
AUTOMATIC SIGN DETECTIONFully automatic extraction of signs from the environment isvery difficult, because signs are usually embedded in theenvironment.
There are many challenges in sign detection,such as variation, motion and occlusion.
We have nocontrol in font, size, orientation, and position of sign texts.Originating in 3-D space, text on signs in scene images canbe distorted by slant, tilt, and shape of objects on whichthey are found [8].
In addition to the horizontal left-to-rightorientation, other orientations include vertical, circularlywrapped around another object, slanted, sometimes with thecharacters tapering (as in a sign angled away from thecamera), and even mixed orientations within the same textarea (as would be found on text on a T-shirt or wrinkledsign).
Unlike other text detection and video OCR tasks, signextraction is in a more dynamic environment.
The user?smovement can cause unstable input images.
Furthermore,the quality of the video input is poorer than that of othervideo OCR tasks, such as detecting captions in broadcastnews programs, because of low quality of equipment.Moreover, sign detection has to be real-time using a limitedresource.
Though automatic sign detection is a difficulttask, it is crucial for a sign translation system.We use a hierarchical approach to address these challenges.We detect signs at three different levels.
At the first level,the system performs coarse detection by extracting featuresfrom edges, textures, colors/intensities.
The systememphasizes robust detection at this level and tries toeffectively deal with the different conditions such aslighting, noise, and low resolution.
A multi-resolutiondetection algorithm is used to compensate different lightingand low contrasts.
The algorithm provides hypotheses ofsign regions for a variety of scenes with large variations inboth lighting condition and contrast.
At the second level,the system refines the initial detection by employing variousadaptive algorithms.
The system focuses on each detectedarea and makes elaborate analysis to guarantee reliable andcomplete detection.
In most cases, the adaptive algorithmscan lead to finding the regions without missing any signregion.
At the third level, the system performs layoutanalysis based on the outcome from the previous levels.The design and layout of signs are language and culturedependent.
For example, many Asia languages, such asChinese and Japanese, have two types of layout: thehorizontal and the vertical.
The system providesconsiderable flexibility to allow the detection of slantedsigns and signs with non-uniform character sizes.4.
SIGN TRANSLATIONSign translation has some special problems compared to atraditional language translation task.
Sign translationdepends not only on domain but also on functionality of thesign.
The same text on different signs can be treateddifferently.
In general, the text used in the sign is short andconcise.
For example, the average length of each sign in ourChinese sign database is 6.02 Chinese characters.
Thelexical mismatch and structural mismatch problems becomemore severe for sign translation because shorterwords/phrases are more likely to be ambiguous and thereisn?t sufficient information from the text to resolve theambiguities which are related to the environment of thesign.
For example, in order to make signs short,abbreviations are widely used in signs, e.g.,  (/jiyan suo/) is the abbreviation for ,(/jisheng chong yan jiu suo/ institute of parasites), suchabbreviations are difficult, if not impossible, even for ahuman to understand without knowledge of the context ofthe sign.
Since designers of signs always assume thatreaders can use the information from other sources tounderstand the meaning of the sign, they tend to use shortwords.
e.g.
in sign (/man xing/, drive slowly), theword (/xing/, walk, drive) is ambiguous, it can mean(/xing zou/ ?move of human,?
walk) or?move of a car,?
drive).
The human reader can understandthe meaning if he knows it is a traffic sign for cars, butwithout this information, MT system cannot select thecorrect translation for this word.
Another problem in sign isstructural mismatch.
Although this is one of the basicproblems for all MT systems, it is more serious in signtranslation: some grammatical functions are omitted tomake signs concise.
Examples include: (1) the subject ?we?is omitted in (/li mao dai ke/, treat customerspolitely); (2) the sentence is reordered to emphasize thetopic: rather than saying(/qing jiang bao zhuang zhitou ru la ji xiang/, please throw wrapping paper into thegarbage can), using (/baozhuang zhi qing tou ru la ji xiang/, wrapping paper, pleasethrow it into the garbage can) to highlight the ?wrappingpaper.?
With these special features, sign translation is not atrivial problem of just using existing MT technologies totranslate the text recognized by OCR module.Although a knowledge-based MT system works well withgrammatical sentences, it requires a great amount of humaneffort to construct its knowledge base, and it is difficult forsuch a system to handle ungrammatical text that appearsfrequently in signs.We can use a database search method to deal with names,phrases, and symbols related to tourists.
Names are usuallylocation dependent, but they can be easily obtained frommany information sources such as maps and phone books.Phrases and symbols related to tourists are relative fixed fora certain country.
The database of phrases and symbols isrelatively stable once it is builtWe propose to apply Generalized Example Based MachineTranslation (GEBMT) [1][2] enhanced with domaindetection to a sign translation task.
This is a data-drivenapproach.
What EBMT needs are a set of bilingual corporaeach for one domain and a bilingual dictionary where thelatter can be constructed statistically from the corpora.Matched from the corpus, EBMT can give the same style oftranslations as the corpus.
The domain detection can beachieved from other sources.
For example, shape/color ofthe sign and semantics of the text can be used to choose thedomain of the sign.We will start with the EBMT software [1].
The system willbe used as a shallow system that can function using nothingmore than sentence-aligned plain text and a bilingualdictionary; and given sufficient parallel text, the dictionarycan be extracted statistically from the corpus.
In atranslation process, the system looks up all matchingphrases in the source-language half of the parallel corpusand performs a word-level alignment on the entriescontaining matches to determine a (usually partial)translation.
Portions of the input for which there are nomatches in the corpus do not generate a translation.Because the EBMT system does not generate translationsfor 100% of its input text, a bilingual dictionary and phrasalglossary are used to fill any gaps.
Selection of the ?best?translation is guided by a trigram model of the targetlanguage and a chart table [3].5.
A PROTOTYPE SYSTEMWe have developed a prototype system for Chinese signrecognition and translation.
Figure 3 shows the architectureof the prototype system.
A user can interactively involvesign recognition and translation process when needed.
Forexample, a user can select the area of interest, or indicatethat the sign is a street name.
The system works as follows.The system captures the sign in a natural background usinga video camera.
The system then automatically detects orinteractively selects the sign region.
The system performssign recognition and translation within the detected/selectedregion.
It first preprocesses the selected region, binarizesthe image to get text or symbol, and feeds the binary imageinto the sign recognizer.
OCR software from a third party isused for text recognition.
The recognized text is thentranslated into English.
The output of the translation is fedto the user by display on screen or synthesized speech.Festival, a general purpose multi-lingual text-to-speech(TTS) system is used for speech synthesis.Figure 3 Architecture of the prototype systemFigure 4  The interface of the prototype systemAn efficient user interface is important to a user-centeredsystem.
Use of interaction is not only necessary for aninteractive system, but also useful for an automatic system.A user can select a sign from multiple detected signs fortranslation, and get involved when automatic sign detectionis wrong.
Figure 4 is the interface of the system.
Thewindow of the interface displays the image from a videocamera.
The translation result is overlaid on the location ofthe sign.
A user can select the sign text using pen or mouseanywhere in the window.6.
EXPERIMENTAL RESULTSWe have evaluated the prototype system for automatic signdetection and translation.
We have built a Chinese signdatabase with about 800 images taken from China andSingapore.
We have tested the automatic detection moduleusing 50 images randomly selected from the database.Table 1 shows the test result of automatic sign detection.Figure 5 and Figure 6 show examples of automatic signdetection with white rectangles indicating the sign regions.Figure 5 shows correct detection after layout analysis.Figure 6 illustrates a result with a false detection (Note thesmall detection box below and to the left of the largerdetection).Table 1 Test Results of Automatic Detection on 50Chinese SignsDetectionwithout missingcharactersDetectionwith falsealarmDetection withmissing characters43 12 5Figure 5 An example of automatic sign detectionFigure 6 An example of false detectionFigure 7 illustrates two difficult examples of sign detection.The text in Figure 7(a) is easily confused with the reflectivebackground.
The sign in Figure 7(b) is embedded in thebackground(a)                                                (b)Figure 7 Difficult examples of sign detectionWe have also tested the EBMT based method.
We assumeperfect sign recognition in our test.
We randomly selected50 signs from our database.
We first tested the systemincludes a Chinese-English dictionary from the LinguisticData Consortium, and a statistical dictionary built from theHKLC (Hong Kong Legal Code) corpus.
As a result, weonly got about 30% reasonable translations.
We thentrained with a small corpus of 670 pairs of bilingualsentences [7], The accuracy is improved from 30% to 52%on 50 test signs.
Some examples of errors are illustratedbelow:Mis-segmentaion:Chinese with wrong segmentation:/ge zhong che liang qing rao xing/Translation from MT:All vehicles are please wind professionCorrect segmentation:Translation if segmentation is correct:All vehicles please use detourLack-domain information:Chinese with segmentation:/qing wu dong shou/Please don?t touch itTranslation from MT:Please do not get to workDomain knowledge needed to translate :?start to work?
in domain such as work plan and?don?t touch?
in domains like tourism, exhibitionetc.Proper Name:Chinese with segmentation:/bei jing tong ren yi yuan/Beijing Tongren HospitalTranslation from MT:Beijing similar humane hospitalis translated to the meaning of eachcharacter because it is not identified as a propername which then should only be represented by itspronunciation.Figure 8 illustrates error analysis of the translation module.It is interesting to note that 40% of errors come from mis-segmentation of words.
There is a big room forimprovement in proper word segmentation.
In addition, wecan take advantage of the contextual information providedby the OCR module to further improve the translationquality.Error source41.11%31.11%10.00%14.44%3.33%Mis-segmentationLack domain knowledgeMis-detection of propernameCorpus/dict not large enoughOtherFigure 8 Error analysis of the translation module7.
CONCLUSIONWe have reported progress on automatic sign translation inthis paper.
Sign translation, in conjunction with spokenlanguage translation, can help international tourists toovercome language barriers.
A successful sign translationsystem relies on three key technologies: sign extraction,OCR, and language translation.
We have developedalgorithms for robust sign detection.
We have appliedEBMT technology for sign translation.
We have employeda user-centered approach in developing an automatic signtranslation system.
The approach takes advantage of humanintelligence in selecting an area of interest and domain fortranslation if needed.
We have developed a prototypesystem that can recognize Chinese signs input from a videocamera which is a common gadget for a tourist, andtranslate them into English text or voice stream.ACKNOWLEDGMENTSWe would like to thank Dr. Ralf Brown and Dr. RobertFrederking for providing initial EBMT software andWilliam Kunz for developing the interface for the prototypesystem.
We would also like to thank other members in theInteractive Systems Labs for their inspiring discussions andsupport.
This research is partially supported by DARPAunder TIDES project.REFERENCES[1] R.D.
Brown.
Example-based machine translation in thepangloss system.
Proceedings of the 16th InternationalConference on Computational Linguistics, pp.
169-174, 1996.
[2] R.D.
Brown.
Automated generalization of translationexamples".
In Proceedings of the EighteenthInternational Conference on Computational Linguistics(COLING-2000), p. 125-131.
Saarbr?cken, Germany,August 2000.
[3] C. Hogan and R.E.
Frederking.
An evaluation of themulti-engine MT architecture.
Machine Translationand the Information Soup: Proceedings of the ThirdConference of the Association for Machine Translationin the Americas (AMTA ?98), vol.
1529 of LectureNotes in Artificial Intelligence, pp.
113-123.
Springer-Verlag, Berlin, October.
[4] A.K.
Jain and B. Yu.
Automatic text location in imagesand video frames.
Pattern Recognition, vol.
31, no.
12,pp.
2055--2076, 1998.
[5] C. C. Kubler.
"Read Chinese Signs".
Published byChheng & Tsui Company, 1993.
[6] H. Li and D. Doermann, Automatic Identification ofText in Digital Video Key Frames, Proceedings ofIEEE International Conference of Pattern Recognition,pp.
129-132, 1998.
[7] R. Lienhart, Automatic Text Recognition for VideoIndexing, Proceedings of ACM Multimedia 96, pp.
11-20, 1996.
[8] J. Ohya, A. Shio, and S. Akamatsu.
Recognitioncharacters in scene images.
IEEE Transactions onPattern Analysis and Machine Intelligence, vol.
16, no.2, pp.
214--220, 1994.
[9] T. Sato, T. Kanade, E.K.
Hughes, and M.A.
Smith.Video ocr for digital news archives.
IEEE Int.Workshop on Content-Based Access of Image andVideo Database, 1998.
[10] M.J. Taylor, A. Zappala, W.M.
Newman, and C.R.Dance, Documents through cameras, Image and VisionComputing, vol.
17, no.
11, pp.
831-844, 1999.
[11] V. Wu, R. Manmatha, and E.M. Riseman, Textfinder:an automatic system to detect and recognize text inimages.
IEEE Transactions on Pattern Analysis andMachine Intelligence, vol.
21, no.
11, pp.
1224-1229,1999.
[12] J. Yang, W. Yang, M. Denecke, and A. Waibel.
Smartsight: a tourist assistant system.
Proceedings of ThirdInternational Symposium on Wearable Computers, pp.73--78.
1999.
