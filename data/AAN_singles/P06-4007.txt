Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 25?28,Sydney, July 2006. c?2006 Association for Computational LinguisticsFERRET: Interactive Question-Answering for Real-World EnvironmentsAndrew Hickl, Patrick Wang, John Lehmann, and Sanda HarabagiuLanguage Computer Corporation1701 North Collins BoulevardRichardson, Texas 75080 USAferret@languagecomputer.comAbstractThis paper describes FERRET, an interac-tive question-answering (Q/A) system de-signed to address the challenges of inte-grating automatic Q/A applications intoreal-world environments.
FERRET utilizesa novel approach to Q/A ?
known as pre-dictive questioning ?
which attempts toidentify the questions (and answers) thatusers need by analyzing how a user inter-acts with a system while gathering infor-mation related to a particular scenario.1 IntroductionAs the accuracy of today?s best factoid question-answering (Q/A) systems (Harabagiu et al, 2005;Sun et al, 2005) approaches 70%, research has be-gun to address the challenges of integrating auto-matic Q/A systems into real-world environments.A new class of applications ?
known as interactiveQ/A systems ?
are now being developed which al-low users to ask questions in the context of ex-tended dialogues in order to gather informationrelated to any number of complex scenarios.
Inthis paper, we describe our interactive Q/A system?
known as FERRET ?
which uses an approachbased on predictive questioning in order to meetthe changing information needs of users over thecourse of a Q/A dialogue.Answering questions in an interactive settingposes three new types of challenges for traditionalQ/A systems.
First, since current Q/A systems aredesigned to answer single questions in isolation,interactive Q/A systems must look for ways to fos-ter interaction with a user throughout all phases ofthe research process.
Unlike traditional Q/A ap-plications, interactive Q/A systems must do morethan cooperatively answer a user?s single question.Instead, in order to keep a user collaborating withthe system, interactive Q/A systems need to pro-vide access to new types of information that aresomehow relevant to the user?s stated ?
and un-stated ?
information needs.Second, we have found that users of Q/A sys-tems in real-world settings often ask questions thatare much more complex than the types of fac-toid questions that have been evaluated in the an-nual Text Retrieval Conference (TREC) evalua-tions.
When faced with a limited period of timeto gather information, even experienced users ofQ/A may find it difficult to translate their infor-mation needs into the simpler types of questionsthat Q/A systems can answer.
In order to pro-vide effective answers to these questions, interac-tive question-answering systems need to includequestion decomposition techniques that can breakdown complex questions into the types of simplerfactoid-like questions that traditional Q/A systemswere designed to answer.Finally, interactive Q/A systems must be sen-sitive not only to the content of a user?s question?
but also to the context that it is asked in.
Likeother types of task-oriented dialogue systems, in-teractive Q/A systems need to model both what auser knows ?
and what a user wants to know ?over the course of a Q/A dialogue: systems thatfail to represent a user?s knowledge base run therisk of returning redundant information, while sys-tems that do not model a user?s intentions can endup returning irrelevant information.In the rest of this paper, we discuss how theFERRET interactive Q/A system currently ad-dresses the first two of these three challenges.25Figure 1: The FERRET Interactive Q/A System2 The FERRET InteractiveQuestion-Answering SystemThis section provides a basic overview of the func-tionality provided by the FERRET interactive Q/Asystem.
1FERRET returns three types of information inresponse to a user?s query.
First, FERRET uti-lizes an automatic Q/A system to find answers tousers?
questions in a document collection.
In or-der to provide users with the timely results thatthey expect from information gathering applica-tions (such as Internet search engines), every ef-fort was made to reduce the time FERRET takes toextract answers from text.
(In the current versionof the system, answers are returned on average in12.78 seconds.
2)In addition to answers, FERRET also providesinformation in the form of two different typesof predictive question-answer pairs (or QUABs).With FERRET, users can select from QUABs that1For more details on FERRET?s question-answering ca-pabilities, the reader is invited to consult (Harabagiu et al,2005a); for more information on FERRET?s predictive ques-tion generation component, please see (Harabagiu et al,2005b).2This test was run on a machine with a Pentium 4 3.0 GHzprocessor with 2 GB of RAM.were either generated automatically from the setof documents returned by the Q/A system or thatwere selected from a large database of more than10,000 question-answer pairs created offline byhuman annotators.
In the current version of FER-RET, the top 10 automatically-generated and hand-crafted QUABs that are most judged relevant tothe user?s original question are returned to the useras potential continuations of the dialogue.
Eachset of QUABs is presented in a separate panefound to the right of the answers returned by theQ/A system; QUABs are ranked in order of rele-vance to the user?s original query.Figure 1 provides a screen shot of FERRET?sinterface.
Q/A answers are presented in the cen-ter pane of the FERRET browser, while QUABquestion-answer pairs are presented in two sep-arate tabs found in the rightmost pane of thebrowser.
FERRET?s leftmost pane includes a?drag-and-drop?
clipboard which facilitates note-taking and annotation over the course of an inter-active Q/A dialogue.3 Predictive Question-AnsweringFirst introduced in (Harabagiu et al, 2005b),a predictive questioning approach to automatic26question-answering assumes that Q/A systems canuse the set of documents relevant to a user?s queryin order to generate sets of questions ?
known aspredictive questions ?
that anticipate a user?s in-formation needs.
Under this approach, topic repre-sentations like those introduced in (Lin and Hovy,2000) and (Harabagiu, 2004) are used to identify aset of text passages that are relevant to a user?s do-main of interest.
Topic-relevant passages are thensemantically parsed (using a PropBank-style se-mantic parser) and submitted to a question gener-ation module, which uses a set of syntactic rewriterules in order to create natural language questionsfrom the original passage.Generated questions are then assembled intoquestion-answer pairs ?
known as QUABs ?
withthe original passage serving as the question?s ?an-swer?, and are then returned to the user.
For ex-ample, two of the predictive question-answer pairsgenerated from the documents returned for ques-tion Q0, ?What has been the impact of job out-sourcing programs on India?s relationship with theU.S.?
?, are presented in Table 1.Q0 What has been the impact of job outsourcing programs on India?srelationship with the U.S.?PQ1 How could India respond to U.S. efforts to limit job outsourcing?A1 U.S. officials have countered that the best way for India tocounter U.S. efforts to limit job outsourcing is to further liber-alize its markets.PQ2 What benefits does outsourcing provide to India?A2 India?s prowess in outsourcing is no longer the only reason whyoutsourcing to India is an attractive option.
The difference liesin the scalability of major Indian vendors, their strong focus onquality and their experience delivering a wide range of services?,says John Blanco, senior vice president at Cablevision SystemsCorp.
in Bethpage, N.Y.PQ2 Besides India, what other countries are popular destinations foroutsourcing?A2 A number of countries are now beginning to position themselvesas outsourcing centers including China, Russia, Malaysia, thePhilippines, South Africa and several countries in Eastern Eu-rope.Table 1: Predictive Question-Answer PairsWhile neither PQ1 nor PQ2 provide users withan exact answer to the original question Q0, bothQUABs can be seen as providing users informa-tion which is complementary to acquiring infor-mation on the topic of job outsourcing: PQ1 pro-vides details on how India could respond to anti-outsourcing legislation, while PQ2 talks aboutother countries that are likely targets for outsourc-ing.We believe that QUABs can play an impor-tant role in fostering extended dialogue-like in-teractions with users.
We have observed that theincorporation of predictive-question answer pairsinto an interactive question-answering system likeFERRET can promote dialogue-like interactionsbetween users and the system.
When presentedwith a set of QUAB questions, users typically se-lected a coherent set of follow-on questions whichserved to elaborate or clarify their initial question.The dialogue fragment in Table 2 provides an ex-ample of the kinds of dialogues that users can gen-erate by interacting with the predictive questionsthat FERRET generates.UserQ1: What has been the impact of job outsourcing programson India?s relationship with the U.S.?QUAB1: How could India respond to U.S. efforts to limit job out-sourcing?QUAB2: Besides India, what other countries are popular destinationsfor outsourcing?UserQ2: What industries are outsourcing jobs to India?QUAB3: Which U.S. technology companies have opened customerservice departments in India?QUAB4: Will Dell follow through on outsourcing technical supportjobs to India?QUAB5: Why do U.S. companies find India an attractive destinationfor outsourcing?UserQ3: What anti-outsourcing legislation has been considered inthe U.S.?QUAB6: Which Indiana legislator introduced a bill that would makeit illegal to outsource Indiana jobs?QUAB7: What U.S.
Senators have come out against anti-outsourcinglegislation?Table 2: Dialogue FragmentIn experiments with human users of FERRET,we have found that QUAB pairs enhanced thequality of information retrieved that users wereable to retrieve during a dialogue with the sys-tem.
3 In 100 user dialogues with FERRET, usersclicked hyperlinks associated with QUAB pairs56.7% of the time, despite the fact the system re-turned (on average) approximately 20 times moreanswers than QUAB pairs.
Users also derivedvalue from information contained in QUAB pairs:reports written by users who had access to QUABswhile gathering information were judged to be sig-nificantly (p < 0.05) better than those reports writ-ten by users who only had access to FERRET?sQ/A system alone.4 Answering Complex QuestionsAs was mentioned in Section 2, FERRET usesa special dialogue-optimized version of an auto-matic question-answering system in order to findhigh-precision answers to users?
questions in adocument collection.During a Q/A dialogue, users of interactive Q/Asystems frequently ask complex questions thatmust be decomposed syntactically and semanti-cally before they can be answered using traditionalQ/A techniques.
Complex questions submitted to3For details of user experiments with FERRET, pleasesee (Harabagiu et al, 2005b).27FERRET are first subject to a set of syntactic de-composition heuristics which seek to extract eachovertly-mentioned subquestion from the originalquestion.
Under this approach, questions featuringcoordinated question stems, entities, verb phrases,or clauses are split into their separate conjuncts;answers to each syntactically decomposed ques-tion are presented separately to the user.
Table 3provides an example of syntactic decompositionperformed in FERRET.CQ1 What industries have been outsourcing or offshoring jobsto India or Malaysia?QD1 What industries have been outsourcing jobs to India?QD2 What industries have been offshoring jobs to India?QD3 What industries have been outsourcing jobs to Malaysia?QD4 What industries have been offshoring jobs to Malaysia?Table 3: Syntactic DecompositionFERRET also performs semantic decompositionof complex questions using techniques first out-lined in (Harabagiu et al, 2006).
Under this ap-proach, three types of semantic and pragmatic in-formation are identified in complex questions: (1)information associated with a complex question?sexpected answer type, (2) semantic dependenciesderived from predicate-argument structures dis-covered in the question, and (3) and topic informa-tion derived from documents retrieved using thekeywords contained the question.
Examples of thetypes of automatic semantic decomposition that isperformed in FERRET is presented in Table 4.CQ2 What has been the impact of job outsourcing programson India?s relationship with the U.S.?QD5 What is meant by India?s relationship with the U.S.?QD6 What outsourcing programs involve India and the U.S.?QD7 Who has started outsourcing programs for India and theU.S.
?QD8 What statements were made regarding outsourcing on In-dia?s relationship with the U.S.?Table 4: Semantic Question DecompositionComplex questions are decomposed by a pro-cedure that operates on a Markov chain, by fol-lowing a random walk on a bipartite graph ofquestion decompositions and relations relevant tothe topic of the question.
Unlike with syntacticdecomposition, FERRET combines answers fromsemantically decomposed question automaticallyand presents users with a single set of answersthat represents the contributions of each question.Users are notified that semantic decomposition hasoccurred, however; decomposed questions are dis-played to the user upon request.In addition to techniques for answering com-plex questions, FERRET?s Q/A system improvesperformance for a variety of question types by em-ploying separate question processing strategies inorder to provide answers to four different types ofquestions, including factoid questions, list ques-tions, relationship questions, and definition ques-tions.5 ConclusionsWe created FERRET as part of a larger effort de-signed to address the challenges of integratingautomatic question-answering systems into real-world research environments.
We have focusedon two components that have been implementedinto the latest version of FERRET: (1) predic-tive questioning, which enables systems to provideusers with question-answer pairs that may antici-pate their information needs, and (2) question de-composition, which serves to break down complexquestions into sets of conceptually-simpler ques-tions that Q/A systems can answer successfully.6 AcknowledgmentsThis material is based upon work funded in wholeor in part by the U.S. Government and any opin-ions, findings, conclusions, or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the views of the U.S.Government.ReferencesS.
Harabagiu, D. Moldovan, C. Clark, M. Bowden, A. Hickl,and P. Wang.
2005a.
Employing Two Question Answer-ing Systems in TREC 2005.
In Proceedings of the Four-teenth Text REtrieval Conference.Sanda Harabagiu, Andrew Hickl, John Lehmann, andDan Moldovan.
2005b.
Experiments with InteractiveQuestion-Answering.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguistics(ACL?05).Sanda Harabagiu, Finley Lacatusu, and Andrew Hickl.
2006.Answering complex questions with random walk models.In Proceedings of the 29th Annual International ACM SI-GIR Conference on Research and Development in Infor-mation Retrieval, Seattle, WA.Sanda Harabagiu.
2004.
Incremental Topic Representations.In Proceedings of the 20th COLING Conference, Geneva,Switzerland.Chin-Yew Lin and Eduard Hovy.
2000.
The auto-mated acquisition of topic signatures for text summariza-tion.
In Proceedings of the 18th COLING Conference,Saarbru?cken, Germany.R.
Sun, J. Jiang, Y. F. Tan, H. Cui, T.-S. Chua, and M.-Y.
Kan.2005.
Using Syntactic and Semantic Relation Analysis inQuestion Answering.
In Proceedings of The FourteenthText REtrieval Conference (TREC 2005).28
