Speech Understanding and Dialogue over thetelephone: an overview of the ESPRIT SUNDIALproject.Je remy PeckhamLo caBetjeman House104 Hills RoadCambridge CB2 1LQ1.
In t roduct ionOne of the most obvious and natural applications for speech technology is in providinga gateway to information services over the telephone network.
Already a significantgrowth is occurring the provision of information from a centralised computing systemusing stored messages or synthetic speech derived from text files.
Most of thesesystems however currently rely on the use of touch tone input for selection of theinformation.
In the UK only 15% of homes and businesses have tone dialling,although the network is currently able to support around 50% tone dialling.
The abilityto recognise a small number of words or even the digits without he user requiring totrain the system therefore has widespread application.
As the scope of the informationservice expands o also does the need for more intelligent dialogues with much largervocabularies for speech understanding.The SUNDIAL project, led by Logica Cambridge, is currently one of Europe's largestcollaborative projects in speech technology involving partners from the UK, France,Germany, Italy and Sweden in 170 man years of effort over five years.
The project whichis part funded by the Commission of the European Communities under the ESPRITprogramme, commenced in September 1988 and will end in July 1993.
The partnersinvolved in the project are shown below.-UK?
France?
Italy?
Germany?
SwedenLogica (Project Leader)University of SurreyCNETCAP SESA InnovationIRISA - University of RennesCSELTSaritelPolitechnico di Torino (associate partner)Dairnler BenzSiemensUniversity of ErlangenInfovox (subcontractor)The computer systems which the SUNDIAL research aims to produce will enable users tomaintain telephone conversations about specific topics such as flight arrivals, schedules and14reservations.
The systems are planned to support a speaker independent vocabulary of up to1000 - 2000 words in four languages (English, French, German and Italian).The project builds on experience gained by partners in previous ESPRIT projects uch asP26 - "Advanced Algorithms and Architectures for Speech and Image Processing", P316 -ESTEAM and P1015 - "Palabre", as well as other projects funded by national programmessuch as VODIS (UK) and SPICOS (FRG) (Fissore et al88, Niemann et al88, Peckham 89,Brenner 89) A number of these projects have already demonstrated speaker dependentspeech understanding with vocabularies of around 1000 words.2.
ArchitectureA common architecture has been agreed amongst the partners in order to fulfil two mainpurposes;to give the detailed specifications of the interfaces betweenthe major modules (front end, linguistic processing,dialogue management and message generation) thusallowing exchanges and comparative assessment.to allow the implementation of different processingstrategies by modifying the control parameters and theflow of data between the modules.The architecture is of a 'Distributed Database' type which uses bi-directional interfaces,currently only between each major module.
The Distributed Database Architecture (DDA)is most fully exploited at the present time within the Dialogue Manager module and this isdescribed more fully in a section 6.The major modules and architecture of the system are shown in figure 1.
These modulesmake use of knowledge-based techniques for dialogue management and rule-based as wellas statistical natural language processing.
Recognition is based on an acoustic pattern-matching technique which uses Hidden Markov Models (HMM's) of phoneme sized speechunits, thus providing a means of more efficiently handling a large vocabulary and thecoarticulation effects associated with fluent speech.
It is planned to carry out experimentsduring 1991 on the use of predictions at various levels within the architecture.
Theseinclude passing down predictions generated by the dialogue module to the Linguisticprocessor.
This module may in turn provide lexical constraints at the acoustic-worddecoding stage.Four separate language demonstrators are being built with applications coveringIntercity Train times (Germany and Italy) and Flight Enqtfiries/Reservations (UK&France).
The acoustic-phonetic decoding is aimed at speaker independence with theaim of operation over the telephone network.
In all cases the goal is to recognisenaturally spoken sentences, relevant to the chosen application domains.
The interactivedialogue comlxment is expected be a crucial part of any robust speech based computerinformation system, providing graceful error ecovery and dialogue repair as well aseliciting further information, handling clarification and confirmation.15Computer \[InformationSystemDialogueManagementMessageGenerationLanguageProcessingText ToSpeechPSTNAcousticPhoneticDecoding?SignalProcessingfigure 1: - SUNDIAL ArchitectureExamples of the type of dialogue which SUNDIAL aims to support are shown below for aflight enquiries application.AGENT: Flight information.USER: er:m I'd like some information on flight number AJ296 er it departsfrom Chicago and arrives in London today.er:m I would like the exact arrival time please.AGENT: Flight AJ296 from Chicago to London Heathrow terminal four is runningahead of schedule it will be arriving at ten fifteen.USER: Is that one fifteen er AM or PMAGENT: ten fifteen AM163.
Application studies and simulationOne of the key motivations of the project is to base the technology research anddevelopment ona clear understanding of user equirements and the spoken languagephenomena for each of the selected application domains.
In order to provide insightinto user requirements a number of 'Wizard of Oz' simulations have been performed forsome of the selected applications.
In addition analysis has been carried out on a largecorpus of human-human dialogues relating to flight enquiries and reservations, in orderto inform the simulations and also to provide a baseline for unconstrained dialogues.The methodology for Wizard of Oz simulations i  that a human accomplice takes therole of the computer speech understanding component and the output o the user isprovided by either synthetic speech or vocoded natural speech (Fraser and Gilbert, 90).Subjects are presented with various cenarios to enable them to make enquiries aboutflights or hotels.
The human accomplice isgiven certain constraints oattempt toemulate the performance ofa speech understanding system.
These constraints, in someof the simulations, were imposed through amenu-driven tool which restricted theflexibility of the systems output at any stage in the dialogue.
(Ponamale t al, 90).
Asmall database was also used to access the requested information.A~N~lr la!
Iio17SUBJECTFigure 2: - Wizard of Oz Simulation ScenarioThe simulations therefore provide a useful means of assessing the potential impact onusers of the expected limitations or constraints ofa future computer speechunderstanding and co-operative dialogue system.
The corpora derived from thesesimulations have been analysed to define the spoken language phenomena which occur,as well as the required lexicon, grammar, semantics and dialogue rules for the chosenapplications.17It is already clear from analysing the corpora from the first round of simulations thatmany of the requirements in terms of dialogue strategies are common to the fourlanguages studied and to the different applications.
It is expected that hese requirementswill also prove to be generic to other information service domains.4.
Speech pattern processingThe Front End Processing (FEP) module carries out the acoustic-phonetic decoding ofthe incoming speech signal and produces alattice or graph of word hypotheses.Techniques for handling the variable quality of telephone speech, mostly due to the useof different handsets and varying handset positions between calls, are also included inthis module.
The FEP for all four languages i based on Hidden Markov Modelling(HMM) of sub-word units, usually context sensitive phone models.
Some special casessuch as function words and digits are modelled separately toimprove performance.Particular objectives in the speech pattern processing module include:refinement of the acoustic/phonetic units used, in line withrequirements for easy extension of the vocabulary, rapidspeaker adaptation and handling fluent speech (the co-articulation problem).?
improvement of speaker-independent phonetic rules andclassification techniques.?
improvements in speech recognition over the telephone.?
fast lexical access.improvements in speech pattern processing techniquesbased on Hidden Markov Modelling including comparisonand trade offs between Continuous Density HMM(CDHMM) and Discrete Density HMM (DDHMM)techniques and Vector Quantisation.
* real-time performance on DSP and/or Transputers.A number of experiments have been divided amongst partners to address theseobjectives where the issue is expected to be language independent (Table 1).DDHMM vs CDHMM vs Semi CDHMMVector Quantisation (Soft VQ, VQ of distributions)Model Topolo~No.
of mixtures for CDHMMVariable Frame Rate Anal~,sisRecognition algorithms (e~ N best word chains)Linguistic onstraints (including statistical grammars)Training Algorithms and Methodlo~,Table 1: - HMM Experiments18Language dependent issues, such as the precise inventory of speech units to be used,are being investigated independently for each lan .wage.
Some experimentation is beingdone with 'context independent' phone models using mixture distributions for theCDHMM and training the phones in many different contexts.
A typical trainingprocedure is shown in figure 3.Initialise ModelsForced RecognitionCI rainin  llCorpus TrainingTest ModelsNII Context ) Sensitive (CS) Phone ModelsIIn Context "~ dependent (CI) Phone Models._.L_X.ModelClusteringFigure 3: - A typical HMM Training ProcedureTo support he development of speaker independent phone models a number of largemultispeaker corpora have already been recorded for each language, mostly over thetelephone.
These contain a set of phonetically balanced sentences for each language(for English these are similar to the TIM1T and SCRIBE sentences) and a set ofapplication specific sentences.Preliminary results for the acoustic-phonetic decoding module show that continuousdensity HMM's (CDHMM) achieve 75.83% word accuracy on sentences, compared to67.13% for discrete density HMM's, using 305 phonetic units for the Italian languageand around a 1000 word vocabulary (Fissorc et al 90).
Word accuracy includesinsertions, deletions and substitutions.
These results are for speaker independentrecognition of telephone quality sentences; however they do not take into account heeffect of the linguistic processing module on sentence understanding performance.19Some benchmarking experiments have been done and is also ongoing using theDARPA TIMIT and Resource Management Databases.
Results for the Englishlanguage using CDHHM's show that phoneme recognition accuracy on the DARPATIMIT database iscomparable tothat achieved by Kai-Fu Lee in the Carnegie MellonSPHINX system.The lexical access tage and the use of phonological knowledge in this process to allowpronunciation differences and co-articulation effects is shown in figure 4.
A wordgraph resulting from this process is illustrated in figure 5.
A common formalism hasbeen agreed amongst all partners for the representation of this data of the form;where:ts, teB andEstringscore\[(B,E) (string) (ts, te) (score)lare the start frame and end frame respectivelyare equal to ts and te in the case of a linear word chainor are equal to the start and end nodes for a word graphis a pointer to to a single word or sequence of wordsis the acoustic likelihood or score~!
i i~!
!
: .
.
.
:~ \ [~ .~.~ ..1..C~.
:~:~%~ :#~~:-.
:(i:i!~ ~ii~iii::ii~i iiiiiM::iilii:::iiiii.
:iiii :~i!iii~~iiii#Mi i  ....SignalLexiconAcousticAnalysisI f@r fO) N~RecognitionCWor  Orap )PhoneModelsfigure 4: - Lexical access20orthography: wantphonetic variant: /wan/score: -768start flame: 4end frame: 8orthography: tophonetic variant:score: -405start frame: 9end frame: 11f l ightsflight\[27\]1271figure 5: - An example of a partial word graphIn co-operation with the ESPRIT Adverse Environment Recognition of Speech (ARS)project, an extensive evaluation of a number of acoustic analysis techniques has beencarried out.
This has used the Recogniser Sensitivity Analysis technique developed byLogica in the Alvey Speech Technology Assessment project (Peckham,90).
Theseexperiments were performed with a standard CDHMM modeler and recogniser using 6mixtures and 8 states per word for a speaker independent isolated word recognitiontask.
Tests on the recogniser using RSA have shown a best performance of95.6%correct recognition ( _+ 0.7% at the 95% confidence l vel) on the RSA 31 wordvocabulary.
All tests were carried out with telephone quality speech.Following this work a small vocabulary over-the-telephone sp aker-independentrecogniser has been developed with a vocabulary of around 50 words, suitable for atelephone banking application.
A full continuous speech telephone banking applicationfor English, with a vocabulary of around 300 words will be demonstrated at the nextproject review in October.
This prototype will combine work on linguistic processing,dialogue management and message generation.5.
Linguistic processingConsiderable work has been carried out over the last 5 years within the ESPRITprogramme as well as n/ttional programmes in the area of natural written languageprocessing.
This has resulted in a number of significant formalisms and grammar rules21for various European languages.
Many of the SUNDIAL partners brought significantbackground into the project both in parsing technology and grammars and it wastherefore uneconomic toconsider asingle formalism for the whole project.
Theprincipal formalisms currently in use in SUNDIAL are Unification Categofial Grammar- UCG (English and French), Augmented Phrase Structure Grammar - APSG(German) and Dependency Grammar - DG (Italian) (Poesio 87, Tropf 89, Zeevat 87).In the case of UCG the semantics are compiled into the lexicon whereas in both APSGand DG the semantics are represented separately as either a semantic net or case frames.The major objectives of the linguistic processing module are:the modification of linguistic processing algorithms tomeet the characteristics of spoken language such ashesitations and ill-formed or incomplete utterances.
Thismay require, for example, the partial analysis of completeutterances;the development of analysis algorithms to handleindeterminacy within the linguistic analysis tage.the study and modelling of the spoken language sub-setused in the selected applications;exchange and evaluation of different parsing algorithms;development and evaluation of stochastic grammars inrelation to rule based systems, with particular reference tocomputational efficiency, coverage, performance andextensibility;investigation and comparison of ways of integratinglinguistic knowledge with pattern processing and withdialogue management;use of predictions from the linguistic processing moduleto improve hypotheses atthe front end;Two main classes of parser have been implemented: a left to fight bottom up chartparser and an island driven parser which makes use of the best acoustic scoringhypotheses todetermine starting points for parsing (Giachin 88).
A fast 'C' version ofthe island driven parser has been implemented which can be ported to a transputerarray.
Typical parse times on a SUN 4 are on the order of a few seconds for amoderately complex sentence with a 1000 word lexicon.
This version will be evaluatedfor both Italian and English.
Experiments are also underway to explore the benefits ofbigram statistics as an initial filter on word hypothesis generation.6.
Dialogue managementHuman-human dialogue is capable of robust handling of adverse conditions andrecovery from communication failure.
In human-machkne communication which isspeech based, it becomes necessary to repair ecognition failure (Young 89).Knowledge from a variety of sources: syntactic, semantic and pragmatic, and22knowledge of the application domain, may be brought o bear both in understandingand recovery.A major goal of the project is the development of an intelligent co-operative dialoguesystem.
Most of the work done to date in the area of dialogue management has been intext based systems.
Whilst speech-based systems can learn much from this work,particularly in the areas of architectures and the use of dialogue histories, oral dialoguecontains pecific phenomena not seen in text, such as hesitations and false starts.Knowledge is also used differently, for instance pragmatic knowledge is not only usedto solve ambiguities but also to strengthen or weaken hypotheses generated frompotentiaUy error-prone acoustic-phonetic decoding.To progress in the modelling of oral dialogue, extensive use is being made ofsimulations and real applications in the selected information service domains uch asflight enquiries, intercity train times and hotel information.
A substantial mount ofanalysis is already complete on a large corpus of spoken dialogues for the fourlanguages.
This analysis is providing information on:?
user requirements;oral dialogue strategies; trade-offs between user andsystem initiative in te~i~as of user acceptability andconstraining the domain (necessary to avoid overambitious questions), confirmation and repair strategies;?
oral language requirements (grammar, lexicon, specificphenomena of spoken dialogue);?
the complexity of the semantic space of the domainschosen;generic rules for dialogue for information servicesdomains where these can be abstracted from particularapplications, including cross-language differences indialogue strategies.It is a major goal of SUNDIAL to handle a number of the observed attributes of normalhuman-human dialogue such as: turn taking, anaphora nd ellipsis (utterances whichdepend on their context for their meaning), hesitations, coughs, changing the subject,strategies for dealing with communication failure, implicit and explicit confirmation,and resolution of ambiguity via questioning.
After some initial prototyping and thedevelopment of a common (to all partners) functional specification, aprototypeDialogue Manager which is based on a distributed atabase architecture has beenimplemented.
In this approach, various modules or agents (shown in figure 6)communicate with each other, maintaining their own histories or knowledge bases (theso called distributed atabase).23Figure 6: - Dialogue Manager Distributed Database ArchitectureThe system operates by interleaving control between agents; these communicate bymessage passing.
In the current phase of implementation  serial hardware, only oneagent is active at any one time.Typically, the cycle is initiated by the task module, which makes a request for taskinformation to the dialogue module.
This module keeps a structured history of thedialogue so far, and is able to reason about possible next moves.
It may propose anextmove for the system and inform the message planner, and send a set of possible nextcaller moves to the linguistic interface module, as predictions.
These predictions areelaborated into linguistic patterns that can serve to constrain the parser.After the caller's input has been processed, it is assumed that a best predictioninstantiation has been found, and this is translated into dialogic terms by the linguisticinterface and passed back to the dialogue module.
The semantic component oftherecognised utterance ispassed to the belief module, for updating of the belief state andresolution of anaphora and ellipses.
Finally, the contextually elaborated message ispassed on to the task module, which may need to go through several cycles ofparameter acquisition and negotiation, before a database access can be made whichsatisfies the user's requirements.7.
Oral message generationWhilst research on natural language generation is more recent han natural languageparsing and understanding, computer based language generators are rapidly expandingm response to growing needs for intelligent human-machine i teraction.
The emphasishitherto has again been on written language.The particular requirements of oral output in the course of a dialogue (intelligibility,reduced length, enumeration a d requests for repetition etc) is being taken into account24in the message generation component.
Prosody (stress on words and melodic contourof an utterance) is affected by both linguistic and pragmatic constraints.
A suitablesymbolic description of the effects of these constraints on a particular utterance ispassed, along with the text, to the Text to Speech Synthesis ('ITS) system.
(House andYoud 90).The message generation module contains a message planning and message generationcomponent.
The message planner is responsible for formulating the output according tocontext and overall prosodic contour planning.
In the future it will also handle issuessuch as summarising.
The linguistic generator uses the 'pivot' generation algorithmfor generation and also makes use of the same grammar formalism as the linguisticprocessing module.
It handles ellipsis and refering expressions and also provides thefinal prosodically annotated output to the Text To Speech ('ITS) synthesiser.The text to speech synthesisers (for each of the four languages) are based on existingsystems (diphone synthesis from CNET and formant synthesis from Infovox).
Workis under way to improve the segmental quality of both the diphone and formantsynthesis approaches and a preliminary comparative evaluation of quality usingpreference tests has already been carried out for the four languages.
Much of the workfor British English is being carried out by Infovox of Sweden who is a subcontractor toLogica.8 .
Integration and real time constraintsA number of laboratory prototypes, in four languages are planned for July 1991 whichwill be designed to run in real-time or near-real-time principally for the purposes ofevaluation.
Progress on the co-operative dialogue element and its interaction with thelower modules can only realistically be evaluated with potential users.
It is not intendedhowever to develop special purpose hardware since this has already be done in anumber of partners' laboratories.
Specific hardware to be used includes transputerarrays, Digital Signal Processors (DSPs) and Sun workstations.Careful consideration is being given to the requirements of interaction betweensymbolic and numeric processing (typically between the higher and lower levels, e.g.semantics, syntax and the acoustic pattern processing), the latter being performed ondigital signal processing boards and transputers.9 .
ConclusionThe SUNDIAL project is one of the most ambitious speech and language researchprojects in Europe, Japan or the USA.
The US DARPA projects on spoken languageunderstanding for example, have some similar goals to those of SUNDIAL (Zue 90,Price 90), however the major differences are that SUNDIAL places greater emphasis onthe dialogue component and also attempts speech understanding over the telephone.Already encouraging results are being obtained within modules of the SUNDIALsystem and excellent co-operation has been established between partners includingexchange of software and techniques.The first major milestone at which progress of complete systems will be demonstratedis in July 1991 At this point both flight enquiry and reservations as well as traintimetable enquiry applications will be shown.
These systems will form the basis forfurther refmement and development in the final phase of the project leading up to July1993.The pay-offs for success in achieving the goals of SUNDIAL could be considerable astelecommunications services expand in the 1990's.
Despite the increasing take up oftone dialing in Europe and the popularity of terminal based services uch as Minitel inFrance, public and professional ccess to information over standard telephones byspeech is likely to find wide acceptance.
The challenge is in matching the technologycapability to the user equirements and market demand, to this end much work remainsto be done.ReferencesBrenner, M. et alWord Recognition in Continuous speech using a phonological basedTwo-Network Matching Parser and a synthesis based Prediction, Proc.
ICASSP,p457-460, 1989.Fissore,L., Giachin,E., Laface,P., Micca,G., Pieraccini,R and Rullent,CExperimental results on large vocabulary continuous speech recognition andunderstanding, Proc.
of the ICASSP '88, pp.
414-417, New York, NY April.
1988.Fissore, L.,et alPerformance of a Speaker Independent Continuous SpeechRecogniser, Proc NATO ASI Speech Recognition and Understanding RecentAdvances, Trends and Applications", Cetraro,Italy 1990.Fraser, N, and Gilbert, G.N., Simulating Speech Systems, to appear in ComputerSpeech and Language.Giachin,E.P., Rullent C., Robust Parsing of Severely Corrupted Spoken Utterances, inCOLING, pp 196-201, Budapest, 1988.House, J. and Youd, N. Contextually appropriate intonation in speech synthesis,Proc.
ESCA: Tutorial and Workshop on Speech Synthesis, Sept 1990.Niemann, H, Brietzmann, A, Ehrlich,U, Posch,S, Regal,P, Sagerer,G, Salzbrunn,R and Schukat-Talamazzini, G. A knowledge based speech understanding system,Int.
J.. Pattern Recognition and Artificial Intelligence, 2(2): 321-350, 1988.Peckham, J. VODIS - a Voice Operated Database Enquiry System, in RecentDevelopments and Applications of Natural Language Processing, ed.
JeremyPeckham.
Kogan Page 1989.Peckham, Let al, Recogniser Sensitivity Analysis: A method for assessing theperformance ofspeech recognisers, in publication - Speech Communication 1990.Poesio,M., Rullent C., Modified caseframe parsing for speech understanding systems,Proc.
IJCAI 87, Milano, 1987.Ponarnale M., Bilange,E., Choukri, K and Soudoplatoff, S A., Computer-aidedApproach to the Design of an Oral Dialogue System, In Proceedings of Eastern Multi-conference 90: AI and Simulation, Nashville, Tennessee, pp229-232, 1990.Price, P., SRI's Spoken Language System for Air Travel Planning,..ProceedingsSpeech Tech 90, 371-374, 1990.26Tropf, H.S., Syntax in the Spoken Dialogue System SPICOS-II, in Eurospeech 89,pp.
30-33, Paris, 1989.Young, S.J., Proctor, C.E., The Design and Implementation of Dialogue Control inVoice Operated Database Inquiry Systems, Computer Speech and Language, 3,pp.329-353, 1989.Zeevat, H., Klein, E., and Calder J., An Introduction toUnification CategorialGrammar, in Haddock N., Klein E. and Morill G. (eds) Edinburgh Working Papersin Cognitive Science, V.I: Categorial Grammar, Unification Grammar and Parsing,1987.Zue, V,et al The Voyager Speech Understanding System: Preliminary Developmentand Evaluation, Proceeding ICASSP 90, pp73-76, 1990.27
