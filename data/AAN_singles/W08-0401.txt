Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 1?9,ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsImposing Constraints from the Source Tree on ITG Constraints for SMTHirofumi Yamamoto?,??,???
Hideo Okuma?,??
?National Institute of Information and Communications Technology/ 2-2-2 Hikaridai Seika-cho Soraku-gun Kyoto Japan?
?ATR Spoken Language Communication Research Labs.??
?Kinki University School of Sience and Engineering Department of Information{hirofumi.yamamoto,hideo.okuma,eichiro.sumita}@nict.go.jpEiichiro Sumita?,?
?AbstractIn current statistical machine translation(SMT), erroneous word reordering is one ofthe most serious problems.
To resolve thisproblem, many word-reordering constrainttechniques have been proposed.
The inver-sion transduction grammar (ITG) is one ofthese constraints.
In ITG constraints, target-side word order is obtained by rotating nodesof the source-side binary tree.
In these noderotations, the source binary tree instance isnot considered.
Therefore, stronger con-straints for word reordering can be obtainedby imposing further constraints derived fromthe source tree on the ITG constraints.
Forexample, for the source word sequence { ab c d }, ITG constraints allow a total oftwenty-two target word orderings.
How-ever, when the source binary tree instance ((ab) (c d)) is given, our proposed ?imposingsource tree on ITG?
(IST-ITG) constraintsallow only eight word orderings.
The re-duction in the number of word-order permu-tations by our proposed stronger constraintsefficiently suppresses erroneous word order-ings.
In our experiments with IST-ITG usingthe NIST MT08 English-to-Chinese transla-tion track?s data, the proposed method re-sulted in a 1.8-points improvement in char-acter BLEU-4 (35.2 to 37.0) and a 6.2%lower CER (74.1 to 67.9%) compared withour baseline condition.1 IntroductionStatistical methods are widely used for machinetranslation.
One of the popular statistical machinetranslation paradigms is the phrase-based model(PBSMT) (Marcu et al, 2002; Koehn et al, 2003;Och et al, 2004).
In PBSMT, errors in word re-ordering, especially in global reordering, are one ofthe most serious problems.
Approaches used to re-solve this problem are categorized into two types.The first type is linguistically syntax-based.
In thisapproach, source (Quirk et al, 2005; Liu et al, 2006;Huang et al, 2006), target (Yamada et al, 2000; Gal-ley et al, 2006; Marcu et al, 2006), or both side(Melamed 2004; Ding et al, 2005) tree structuresare used for model training.
The second type is for-mal constraints on word permutations.
IBM con-straints (Berger et al, 1996), lexical word reorderingmodel (Tillmann, 2004), and inversion transductiongrammar (ITG) constraints (Wu, 1995; Wu, 1997)belong to this type of approach.
Our approach is anextension of ITG constraints and is a hybrid of thefirst and second type of approach.We propose ?imposing source tree on ITG?
(IST-ITG) constraints for directly introducing source sen-tence structure into our set of constraints.
In IST-ITG, ITG constraints under the given source sen-tence tree structure are used as stronger constraintsthan the original ITG.
For example, IST-ITG allowsonly eight word orderings for a four-word sentence,even though twenty-two word orderings are possiblewith respect of in the original ITG constraints.In Section 2, we present the proposed IST-ITGfor word-based translation.
In Section 3, the pro-posed method is extended to phrase-based transla-tion.
In Section 4, we present a real-time decodingalgorithm for IST-ITG constraints.
In Section 5, wegive details of the experiments and present the re-sults.
Finally, in Section 6, we offer a summary andsome concluding remarks.12 Imposing the Source Tree on ITGConstraintsFirst, we introduce three previous studies on wordreordering constraints: IBM constraints; lexical re-ordering model; and ITG constraints.
Here, we con-sider one-to-one word-aligned source and target lan-guage sentence pairs as the simplest cases.2.1 IBM constraintsIn this constraint, a distortion penalty is given in ac-cordance with the gap between the previously andthe currently translated words, which is representedas the following equation.pD = exp(?
?idi) (1)where di for each i is defined as:di = abs(position(ei?1) + 1 ?
position(ei)) (2)where ei represents the translated word from the ithsource word fi, position(w) represents the positionof the word w. Sometimes, a limit is set for difor similar language pairs such as French and En-glish.
However, for dissimilar language pairs, suchas Japanese and English or Chinese and English,limiting di is not beneficial.2.2 Lexical Reordering ModelIn the lexical reordering model, reordering proba-bilities are assigned to each word pair {fi, ei}.
Re-ordering positions are categorized into three types,monotone, swap, and discontinuous.
The probabil-ity is assigned to left and right sides as ps(t|fi, ei),where, s is left (l) or right (r), t is monotone (m),swap (s), or discontinuous (d).
Therefore, a total ofsix probabilities are assigned to each word pair.
Forthe source word sub-sequence fi?1, fi, probabilitiesof target sub-sequences are calculated as follows:?
p(ei?1, ei) = pr(m|fi?1, ei?1)pl(m|fi, ei)?
p(ei, ei?1) = pr(s|fi?1, ei?1)pl(s|fi, ei)?
p(otherwise) = pr(d|fi?1, ei?1)pl(d|fi, ei)2.3 ITG ConstraintsIn one-to-one word-alignment, the source word fi istranslated into the target word ei.
The source sen-tence [f1, f2, ..., fN ] is translated into a reorderedsequence of word [e1, e2, ..., eN ].
The number of re-orderings is N !.
When ITG is introduced, this com-bination N !
can be reduced in accordance with thefollowing constraints.?
All possible binary tree structures are generatedfrom the source word sequence.?
The target sentence is obtained by rotating anynode of the binary trees.When N = 4, the ITG constraints can re-duce the number of combinations from 4!
=24 to 22 by rejecting combinations [e3, e1, e4, e2]and [e2, e4, e1, e3].
For a 4-word sentence,the search space is reduced to 92%(22/24), butfor 10-word sentence, the search space is only6%(206,098/3,628,800) of the original full space.2.4 Imposing Source Tree ConstraintsIn ITG constraints, the source-side binary treeinstance is not considered.
Therefore, if thesource sentence binary tree is utilized, strongerconstraints than the original ITG can be created.By parsing the source sentence, a parse tree isobtained.
After parsing, a bracketed sentence isobtained by removing the node labels, and thisbracketed sentence can be converted to a binarytree.
For example, the parse tree, (S1 (S (NP (DTThis)) (VP (AUX is) (NP (DT a) (NN pen))))),is obtained from the source sentence ?This is apen?.
By removing the node labels, a bracketedsentence ((This) ((is) ((a) (pen)))) is obtained.
Sucha bracketed sentence (equivalent to a binary tree)can be used to produce constraints.
If IST-ITG isapplied, the number of word orderings in N = 4 isreduced to 8, down from 22 with ITG.
For example,for the source-side bracketed tree ((f1 f2)(f3 f4)),eight target sequences [e1, e2, e3, e4], [e2, e1, e3, e4],[e1, e2, e4, e3], [e2, e1, e4, e3], [e3, e4, e1, e2],[e3, e4, e2, e1], [e4, e3, e1, e2], and [e4, e3, e2, e1]are accepted.
For the source-side bracketed tree(((f1 f2)f3)f4), eight sequences [e1, e2, e3, e4],[e2, e1, e3, e4], [e3, e1, e2, e4], [e3, e1, e2, e4],[e4, e1, e2, e3], [e4, e2, e1, e3], [e4, e3, e1, e2], and2[e4, e3, e2, e1] are accepted.
Generally, the numberof word orderings is reduced to 2N?1.
Table 1shows the number of word orderings in a targetword sequence for each N with ITG, IST-ITG, andno constraints.Table 1: Number of word orderings in each type ofconstraintN IST-ITG ITG No Constraint1 1 1 12 2 2 23 4 6 64 8 22 245 16 90 1206 32 394 7207 64 1806 50408 128 8558 403209 256 41586 36288010 512 206098 362880015 16384 745387038 13076743680002.5 Extension to Non-binary TreeIn the above subsection, a source binary tree was as-sumed in order to perform IST-ITG.
However, pars-ing results sometimes are not binary trees.
In thiscase, some tree nodes have more than two branches.For a non-binary node, any reordering of branches isallowed.
In a non-binary tree (f1(f2 f3 f4)), twelvetarget-side sequences [e1, e2, e3, e4], [e1, e2, e4, e3],[e1, e3, e2, e4], [e1, e3, e4, e2], [e1, e4, e2, e3],[e1, e4, e3, e2], [e2, e3, e4, e1], [e2, e4, e3, e1],[e3, e2, e4, e1], [e3, e4, e2, e1], [e4, e2, e3, e1], and[e4, e3, e2, e1] are allowed.
For nodes that have morethan three branches, the original ITG constraintsare locally applied.
Therefore, for a non-binary tree(f1(f2 f3 f4 f5)), 22 ?
2 = 44 word orderings areallowed in the target-side and represented by thefollowing formula.n?i=1(SBi) (3)where Sk represents the number of combinationsfrom the original ITG constraints for N = k and Birepresents the number of branches at the ith node.3 IST-ITG in Phrase-based SMTIn the above section, we described each constraintin the case of a one-to-one word-alignment.
In thissection, we consider phrase-based models.
Whena phrase-based model is used, each constraint mustbe extended.
For IBM constraints, equation (2) isrewritten using phrase Pen instead of word en asfollows:di = abs(last position(Pei?1) + 1?first position(Pei)) (4)where last position(Pen) represents the posi-tion of the last word in nth phrase, andfirst position(Pen) represents the position of thefirst word in nth phrase.
The lexical reorderingmodel and ITG constraints can be extended bychanging the model (or constraint) unit from ?word?to ?phrase?.
However, in IST-ITG, ?word?
must beused for the constraint unit since the parse (brack-eted tree) unit is in ?words?.
To absorb differentunits between translation models and IST-ITG con-straints, we investigated a new limitation for wordordering as follows.?
Word ordering that destroys a phrase is not al-lowed.When this limitation is applied, the translated wordordering is obtained from the bracketed source sen-tence tree by reordering the nodes in the tree, thesame as for one-to-one word-alignment.
Accordingto this limitation, the following nodes cannot be re-ordered.
If a sub-tree with root node X includes partof a phrase ph, node X cannot be reordered.
Con-sider the source bracketed source tree ( ( ea eb ec )( ( ed ee ) ( ef eg ) ) ), in which eb ec, and ed forma phrase eph as in Figure 1.
Node 1 cannot be re-ordered since part of the phrase eb ec is included innode 1?s sub-tree.
For the same reason, node 2 and 4cannot be reordered.
Node 3 can be reordered sincethe sub-tree does not include the phrase (target se-quence [fafphfefgff ] is obtained by rotating node3).
Node 5 also can be reordered since it includesthe whole phrase (target sequence [fgfffefphfa] isobtained by rotating node 5).
If node 2 is reordered,phrase ph is split into two parts, and translated intwo parts in the target sentence.
It is inconsistent3with the condition that phrase-to-phrase alignmentis one-to-one.
As a result, only the target sequences[fafphfefffg], [fafphfefgff ], [fgfffefphfa], and[fffgfefphfa] are allowed.
Here, fph represents anequivalent phrase in the translation for eph.e e ee1 2 35ee e4ea ph e f gb c dFigure 1: Example sentence tree with a phrase4 Decoding with IST-ITG ConstraintsIn this section, we describe a one-pass decodingalgorithm that uses IST-ITG constraints in the de-coder.
The translation target sentence is sequen-tially generated from left (sentence head) to right(sentence tail).
To introduce the IST-ITG constraintsinto a decoder, the target candidate must be checkedwhether it satisfies the IST-ITG constraints or notwhenever a new phrase is selected to extend a targetcandidate.To explain this checking algorithm, we catego-rized source sub-trees into four types UNTRANS-LATED, TRANSLATED, TRANSLATING, andNG (no good) as follows:?
If a sub-tree consists of only leaf word nodes,and all leaf words are not yet translated, thissub-tree is defined as UNTRANSLATED.?
If a sub-tree consists of only UNTRANS-LATED sub-trees, this sub-tree is also UN-TRANSLATED.?
If a sub-tree consists of only leaf word nodes,and all leaf words are already translated, thissub-tree is defined as TRANSLATED.?
If a sub-tree consists of only TRANSLATEDsub-trees, this sub-tree is also TRANSLATED.?
If a sub-tree consists of only leaf word nodeswith both translated and untranslated words,this sub-tree is defined as TRANSLATING.?
If a sub-tree consists of both TRANSLATEDand UNTRANSLATED sub-trees, this sub-tree is TRANSLATING.?
If a sub-tree includes only one TRANSLAT-ING sub-tree and any number (including zero)of TRANSLATED and UNTRANSLATEDsub-trees, this sub-tree is TRANSLATING.?
If a sub-tree includes more than one TRANS-LATING sub-tree, this sub-tree is NG.?
If a sub-tree includes NG sub-tree, this sub-treeis also NG.If a translation candidate includes TRANSLAT-ING sub-tree t, t must become TRANSLATEDbefore anything else can happen.
Given sub-tree((ab)c), a is translated, b and c are not yet trans-lated.
In this case, b must be translated before c.If c is translated before b, the target word order be-comes ACB.
This word order does not satisfy theIST-ITG constraints.
For the same reason, a can-didate that includes an NG sub-tree does not satisfythe IST-ITG constraints.
The checking algorithm forIST-ITG constraints is as follows.1.
For old translation candidates, the smallestTRANSLATING sub-tree t and its untrans-lated part u are calculated.2.
When a new target phrase fph is generated, thesource phrase eph and untranslated part u cal-culated in above step are compared.
If eph doesnot include and is not included in u, the newcandidate is rejected.
For example, in Figure1, only source word ea is already translated.The smallest TRANSLATING sub-tree is 1and its untranslated part u is [ebec].
In this case,phrases containing [eb], [ec], or [ebec] are ac-cepted since these are included in u.
Phrases[ebeced] or [ebecedee] are also accepted sincethese include u.3.
If a new candidate includes NG sub-trees, thiscandidate is rejected.45 Experiments5.1 Evaluation MeasuresWe evaluated the proposed method using four eval-uation measures, BLEU (Papineni et al, 2002),NIST (Doddington 2002), WER(word error rate),and PER(position independent word error rate).
Be-fore discussing the evaluation, the characteristics ofeach one are analyzed.?
BLEU: This evaluation measure takesinto account middle range word order,but does not take into account globalword order.
When the translation result is[w1, w2, ..., wj?1, X,wj+1, ..., wn] for refer-ence translation [w1, w2, ..., wn], both WERand BLEU scores will be high.
For a transla-tion result [wj+1, ..., wn, X,w1, w2, ..., wj?1],the BLEU score will be the same as theprevious result since BLEU only takes intoaccount 4grams.
However, the WER score willbe zero since global word positions are takeninto account.
Therefore, the effectiveness ofthe proposed method using BLEU is less thanthat of using WER.?
NIST: This evaluation measure only takes intoaccount n-grams like BLEU.
However, impor-tance of higher order n-grams are less thanBLEU.
Therefore, the effectiveness of the pro-posed method using NIST will be less than thatof using BLEU.?
WER: This evaluation measure takes into ac-count not only local but also global word or-der, and is the most suitable for evaluating ourmethod.?
PER: With this evaluation measure, we arealmost incapable of considering word order.Therefore, our proposed method would seem tooffer no improvement in this evaluation mea-sure.5.2 English and Japanese Patent CorpusExperimentsFirst, we conducted experiments on English andJapanese patent translations.
Details of the experi-mental corpus are shown in Table 2.
This corpus iscreated by automatic sentence alignment (Uchiyama2003).
The first nine hundred sentence pairs with thebest alignment scores were used as the evaluationdata (single reference) and the next thousand sen-tence pairs were used as the development data.
Thiscorpus is a subset of the training corpus that will beused in the NTCIR-7 Workshop patent translationtrack.Table 2: E-J patent corpus# of sent.
Total words # of entriesE/J Train 1.8M 60M/64M 188K/118KE/J Dev 916 30K/32K 4,072/3,646E/J Eval 899 29K/32K 3,967/3,6825.2.1 English-to-Japanese TranslationThe translation direction of the first experimentwas English-to-Japanese (E-J).
For phrase-basedtranslation model training, we used the GIZA++toolkit (Och et al, 2003).
For language model train-ing, the SRI language model tool kit (Stolcke 2002)was used.
The language model type was word 5-gram smoothed by Kneser-Ney discounting (Kneser1995).
For tuning of decoder parameters, we con-ducted minimum error training (Och 2003) with re-spect to the BLEU score using 916 developmentsentence pairs.
For extraction of source sentencetree structure, we used the Charniak parser (Char-niak 2000).
We used Chasen for segmentation of theJapanese.
The numbers of entries in the languagemodels were 0.1 M, 2.1 M, 4.3 M, 6.2 M, and 6.9 Mfor 1, 2, 3, 4, and 5grams respectively.
The numberof entries in the phrase-table was 76 M. For decod-ing, we used an in-house decoder that is a close rel-ative to the Moses decoder.
The performance of thisdecoder was configured to be the same as Moses.Another conditions are the same as the default con-ditions of Moses decoder.In the previous work (Zens et al, 2003, 2004),an IBM constraints and an ITG constraints are com-pared.
In these experiments, a lexical reorderingmodel, the proposed IST-ITC, and combinations ofthese are added as comparison targets.
The combi-nation of constraints in these experiments is as fol-lows.51.
Monotone: Monotone translation (no reorder-ing).2.
No constraints: There were no constraints forword reordering.
Any word order was allowedwithout penalty.3.
IBM: IBM constraints without distortion limit.4.
ITG: ITG constraints.5.
IBM+ITG: Both IBM and ITG constraints wereused at the same time.6.
IBM+LR: Both IBM constraints and lexical re-ordering model.7.
IST: Only the proposed IST-ITC constraints.8.
IBM+IST: Both IBM and IST-ITC constraints.9.
IBM+LR+IST: IBM constraints, Lexical re-ordering model, and IST-ITG constraints wereused at the same time.Table 3 shows the following experimental results.In comparing the original ITG constraints (ITG)with the proposed IST-ITG (IST) method, the im-provement in BLEU was 2.67 points, and in WERwas 5.39%.
WER had the largest improvement, nextwas BLEU.
This particular improvement order wasthe same as in the previous subsection.
The large im-provement of WER helped us confirm the effective-ness of the proposed method for global word order-ing.
When IBM constraints were used at the sametime (IBM+ITG and IBM+IST), the BLEU scoreimproved by 1.57 points and WER improved by4.63%.
When the lexical reordering model was usedat the same time (IBM+LR and IBM+LR+IST),BLEU improved by 1.03 points and WER improvedby 5.12%.
The lexical reordering model fixed phraseposition for the monotone and swap categories, butdid not fix phrase position for the discontinuous cat-egory.
IST-ITG fixed phrase position for the dis-continuous category, even though it did not assigna probability.
Combinations of the lexical reorder-ing model and IST-ITG resulted in a better WERthan with both IBM+LR and IBM+IST since bothposition and probability could be assigned for thediscontinuous category.Table 3: Evaluation results in E-J patent translationBLEU NIST WER PERMonotone 24.91 6.95 79.97 42.02No constraint 26.83 7.19 81.10 39.52IBM 28.35 7.29 78.35 39.25ITG 27.59 7.26 80.29 39.15IBM+ITG 28.50 7.30 78.01 39.29IBM+LR 31.17 7.50 76.30 38.61IST 30.26 7.41 74.90 38.93IBM+IST 30.07 7.41 73.38 39.05IBM+LR+IST 32.20 7.61 71.18 38.155.2.2 Japanese-to-English TranslationNext, we conducted J-E translation experimentsusing the same corpus.
The numbers of entries inthe language models were 0.2 M, 3.1 M, 4.1 M, 5.7M, and 5.9 M for 1, 2, 3, 4, and 5grams The im-provement.
The number of entries in the phrase-table was 76 M. For parsing of Japanese, we usedthe dependency structure analyzer CaboCha.
Fromthe dependency structure, Japanese bracketed treeswere generated.
The combination of constraints inthese experiments was the same as those of the E-Jtranslation experiments.Table 4 shows the translation results of sentenceevaluation with the top five alignment scores.
Incomparing the original ITG constraints (ITG) withthe proposed IST-ITG (IST), BLEU was improvedby 1.21 points, and by in 3.81% in WER.
Thelargest improvement was in WER, and BLEU hadthe next largest.
This particular improvement orderof these evaluation measures was the same as thatof the E-J translation experiments.
When IBM con-straints were used at the same time (IBM+ITG andIBM+IST), there was no improvement in BLEU, butWER improved by 3.89%.
When the lexical reorder-ing model was used at the same time (IBM+LR andIBM+LR+IST), there was also no improvement inBLEU, but WER improved by 4.47%.
One pos-sible reason for the small (or no) improvement inBLEU is the lower parsing accuracy of Japanesecompared with that of the English.
However, betterthe WER figure indicates that using IST-ITC con-straints leads to better word order.
In the Appendix,differences in the translation results for the first five6evaluation sentences between IBM+LR (Baseline:)and IBM+LR+IST (Proposed:) are shown.Table 4: Evaluation results in J-E patent translationBLEU NIST WER PERMonotone 26.29 7.25 76.42 40.85No constraint 26.20 7.18 81.41 40.76IBM 27.87 7.34 78.16 39.94ITG 27.01 7.24 80.43 40.50IBM+ITG 28.16 7.35 78.04 40.07IBM+LR 29.93 7.54 77.27 39.12IST 28.32 7.31 76.62 40.67IBM+IST 28.14 7.32 74.13 40.40IBM+LR+IST 29.77 7.50 72.80 39.735.3 NIST MT08 English-to-ChineseTranslation ExperimentsNext, we conducted English-to-Chinese (E-C) news-paper translation experiments for different languagepairs.
The training and evaluation corpora were usedin the NIST MT08 evaluation campaign English-to-Chinese translation track.
For the translation modeltraining, we used 6.2M bilingual sentences.
For thelanguage model training, we used 20.1M sentences.A development set with 1,664 sentences was usedas evaluation data in the Chinese-to-English transla-tion track in the NIST MT07 evaluation campaign.A single reference was used in the developmentset.
The evaluation set with 1,859 sentences is thesame as MT08?s evaluation data, with 4 references.Model training and decoding conditions were thesame as those in the E-J experiments.
In both base-line and proposed condition, IBM constraints andlexical reordering model were used at the same time.Therefore, the baseline conditions correspond to theIBM+LR condition in the J-E experiments, the pro-posed conditions correspond to the IBM+LR+IST inthe J-E experiments.The evaluation unit was both the Chinese char-acter and word as defined by the PKU corpus.
Asin the E-J experiments, the improvements in WERand CER (character error rate) were large.
The im-provements in WER, CER, word BLEU, and charac-ter BLEU were 5.3% (from 75.0% to 69.7%), 6.2%(from 74.1% to 67.9%), 2.2-points (from 21.0 to23.2), and 1.8-points (from 35.2 to 37.0) respec-tively.
We again demonstrated that the proposedmethod is effective (especially in WER) for multi-ple language pairs.6 ConclusionWe proposed new word reordering constraints forPBSMT using source tree structure.
The proposedIST-ITG constraints are extensions of the ITG con-straints.
In ITG constraints, the instance of thesource-side tree is not taken into account.
On theother hand, in IST-ITG constraints, the tree thatis obtained by source sentence parsing is imposedon the decoding process.
Therefore, IST-ITG con-straints are stronger than those of the original ITG.For example, for four-word source sentences, IST-ITG constraints allow eight word orderings in a tar-get sentence compared with twenty-two orderingsunder the original ITG constraints.
IST-ITG con-straints can be applied to a common decoder to de-termine a target sentence from one-pass without re-scoring.
In our E-J patent translation experiments,the proposed method resulted in a 2.7-point im-provement in BLEU and a 5.7% improvement inWER compared with those of the original ITG con-straints.
In this paper we have argued the WER isthe most appropriate measure to gauge the effective-ness of our approach since it gives importance to theglobal word order.
Our approach gave rise to con-siderable gains in term of WER in all of our experi-ments, indicating that a respectable improvement inglobal word order was achieved.
The improvementcould clearly be seen from visual inspection of theoutput, a few examples of which are presented inthe following Appendix.A Samples from the Translation ofJapanese Patent into EnglishA.1 Sentence 1Source: 	fiffffifl !#"$&%'#()*+,-.
/ffi01*+,-2	&ff43.57698.:;)=<>,?
;@A&BCEDFReference: and, the kinetic energy of the liquid filledbetween the rotor 16 and stator 15 is converted intothermal energy to thereby produce a brake torque.Baseline: then, the rotor 16 and the kinetic energy isconverted to thermal energy braking torque is gener-7ated between the liquid filled in the stator 15.Proposed: then, the rotor 16 and between the stator15, the liquid filling the kinetic energy is convertedto thermal energy braking torque is generated.A.2 Sentence 2Source:  ,>;, fiff)fl,ffi "!
@$#%"& '&DFReference: a sealant 7, which serves as a seal forcutting gas 9, also serves as a guide for the movingholder 3.Baseline: the seal and movement of the holder 3 alsoserves as a guide for the seal member 7 is a work gas9.Proposed: 7 denotes a seal material, which alsoserves as a guide for the working gas 9 describedlater seal and movement of the holder 3.A.3 Sentence 3Source: (ff *)+,-E/.
*01E3245fi/76899(*:<;2475 / 6	.D=@?>*@ A3BC7DE;2 F.
?/G,H9IKJML 	NffOQP95SRSDEffi2 TD$UV1@$W2 XY2CD9FReference: suppose that the red signal light of a traf-fic signal installed at a crossing situated ahead is on,the driver has recognized the red signal light, andthe driver ?s foot is about to shift from the accelera-tor pedal to the brake pedal to stop the vehicle.Baseline: next , the tread brake by the driver, theaccelerator to be stopped from the traffic of recog-nizing traffic signals is ?
red ?
and the intersectionahead of the vehicle is red, it is described as an ex-ample.Proposed: next, a case will be exemplified below soas to tread brakes from the accelerator to be stopped,and of recognizing traffic signals of red, the driver is?
red ?
and is traffic light ahead of the vehicle.A.4 Sentence 4Source:6I ffZ[\]1/^_ @K`ba'cd! [*eE/gfh ji CTSDEffK,*k*l Kmn*ofip<q@$rs   <t<uSv	w<T2FReference: in addition, this method is not econom-ical because it requires special steps such as pre-washing of the substrate surface, pre-treatments forproviding the substrate with adherability to a coat-ing, a drying step and the like.Baseline: further, the coating film is apt to be de-posited on the surface of the object to be coated bywashing and drying process is required, and the pre-liminary process advance not economical.Proposed: further, to clean the surface of the objectto be coated beforehand so as to facilitate the ad-hesion of the coating film preprocessing and dryingprocess is required, and not economical.A.5 Sentence 5Source: xyz{@}|  ~2%'?3?g?@&K?g??
4D9FReference: an oil passage 4 is formed as a hollowportion in the main body 1.Baseline: 4 is a hollow portion of the body 1 with anoil supply passage is shown.Proposed: 4 is an oil supply passage, with a hollowportion of the main body 1.8ReferencesAdam L. Berger, Peter F. Brown, Stephen A. DellaPietra, Vincent J. Della Pietra, Andrew S. Kehler, andRobert L. Mercer, ?Language translation apparatusand method of using context-based translation models,?
United States patent, patent number 5510981, April,1996.Cabochahttp://chasen.org/ taku/software/cabocha/Eugene Charniak, ?A Maximum-Entropy-InspiredParser,?
Proc.
NAACL-2000, Seattle, Washington,pp.132-139, 2000.Chasenhttp://chasen-legacy.sourceforge.jp/Yuan Ding, Martha Palmer, ?Machine translation us-ing probabilistic synchronous dependency insert gram-mars,?
Proc.
ACL, Ann Arbor, pp.
541-548, 2005.George Doddington, ?Automatic evaluation of machinetranslation quality using n-gram co-occurrence statis-tics,?
Proc.
ARPA Workshop on Human LanguageTechnology, San Diego, CA, 2002.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, Ignacio Thayer,?Scalable Inference and Training of Context-RichSyntactic Models,?
Proc.
ACL-COLING, Sydney Aus-tralia, pp.
961-968, 2006.Liang Huang, Kevin Knight, Aravind Joshi, ?StatisticalSyntax-Directed Translation with Extended Domain ofLocality,?
Proc.
AMTA, Massachusetts, 2006Reinhard Kneser, Hermann Ney, ?Improved backing-off for m-gram language model,?
Proceedings of theIEEE International Conference of Acoustic, Speech,and Signal processing.
Vol.
1, pp.
181-184, 1995.Yang Liu, Qun Liu, Shouxun Lin, ?Tree-to-String Align-ment Template for Statistical Machine Translation,?Proc.
ACL-COLING, Sydney Australia, pp.
609-616,2006.Daniel Marcu, William Wong, ?A phrase-based, jointprobability model for statistical machine translation,?Proc.
EMNLP-2002, Philadelphia, pp.133-139, 2002.p.
127-133, 2003.Daniel Marcu, Wei Wang, Abdessamad Echihabi, KevinKnight, ?SPMT: Statistical Machine Translation withSyntactified Target Language Phrases,?
Proc.
EMNLP-2006, Sydney Australia, pp.
44-52, 2006.Dan Melamed, ?Statistical machine translation by pars-ing, ?Proc.
ACL, Barcelona, pp.
653-660, 2004.Moseshttp://www.statmt.org/moses/NIST MT08http://www.nist.gov/speech/tests/mt/2008/NTCIR-7http://ntcir.nii.ac.jp/Franz Josef Och, Hermann Ney, ?A Systematic Compar-ison of Various Statistical Alignment Models,?
Com-putational Linguistics, No.
1, Vol.
29, pp.
19-51, 2003.Franz Josef Och, ?Minimum error rate training for statis-tical machine trainslation,?
Proc.
ACL, Sapporo Japan,pp.
160-167, 2003.Franz Josef Och, Hermann Ney, ?The alignment templateapproach to statistical machine translation, Computa-tional Linguistics, 30(4), pp417-449, 2004.Kishore Papineni, Salim Roukos, Todd Ward, Wei-JingZhu, ?Bleu: a method for automatic evaluation of ma-chine translation,?
Proc.
ACL, Philadelphia PA, pp.311-318, 2002.Chris Quirk, Arul Menezes, Colin Cherry, ?Dependencytreelet translation: Syntactically informed phrasalSMT,?
Proc.
ACL, Ann Arbor, pp.
271-279, 2005.Andreas Stolcke, ?SRILM - An Extensible LanguageModel Toolkit,?
Proc.
ICSLP?02, Denver, pp.
901-904,2002. http://www.speech.sri.com/projects/srilm/Christopher Tillmann, ?A unigram orientation model forstatistical machine translation,?
HLT-NAACL, Boston,pp.
101-104, 2004.Masao Uchiyama and Hitoshi Isahara, ?Reliable Mea-sures for Aligning Japanese-English News Articlesand Sentences?, Proc.
ACL, Sapporo Japan, pp.
72-79,2003.Dekai Wu, ?Stochastic inversion transduction grammars,with application to segmentation, bracketing, andalignment of parallel corpora,?
In Proc.
IJCAI, pp.1328-1334, Montreal, 1995.Dekai Wu, ?Stochastic inversion transuduction grammarsand bilingual parsing of parallel corpora,?
Computa-tional Linguiatics, 23(3), pp.377-403, 1997.Kenji Yamada, Kevin Knight, ?A syntax-based statisticaltranslation model,?
Proc.
ACL, Hong Kong, pp.
523-530, 2000.Richard Zens, Hermann Ney, ?A Comparative Study onReordering Constraints in Statistical Machine Transla-tion?
Proc.
ACL, Sapporo Japan, pp.
144-151, 2003.Richard Zens, Hermann Ney, Taro Watanabe, Ei-ichiro Sumita, ?Reordering Constraints for Phrase-Based Statistical Machine Translation?
Proc.
Coling,Geneva, pp.
205-211, 2004.9
