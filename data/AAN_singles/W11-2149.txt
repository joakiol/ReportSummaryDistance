Proceedings of the 6th Workshop on Statistical Machine Translation, pages 405?412,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsThe RWTH Aachen Machine Translation System for WMT 2011Matthias Huck, Joern Wuebker, Christoph Schmidt, Markus Freitag, Stephan Peitz,Daniel Stein, Arnaud Dagnelies, Saab Mansour, Gregor Leusch and Hermann NeyRWTH Aachen UniversityAachen, Germanysurname@cs.rwth-aachen.deAbstractThis paper describes the statistical machinetranslation (SMT) systems developed byRWTH Aachen University for the translationtask of the EMNLP 2011 Sixth Workshop onStatistical Machine Translation.
Both phrase-based and hierarchical SMT systems weretrained for the constrained German-Englishand French-English tasks in all directions.
Ex-periments were conducted to compare differ-ent training data sets, training methods and op-timization criteria, as well as additional mod-els on dependency structure and phrase re-ordering.
Further, we applied a system com-bination technique to create a consensus hy-pothesis from several different systems.1 OverviewWe sketch the baseline architecture of RWTH?s se-tups for the WMT 2011 shared translation task byproviding an overview of our translation systems inSection 2.
In addition to the baseline features, weadopted several novel methods, which will be pre-sented in Section 3.
Details on the respective se-tups and translation results for the French-Englishand German-English language pairs (in both trans-lation directions) are given in Sections 4 and 5.
Wefinally conclude the paper in Section 6.2 Translation SystemsFor the WMT 2011 evaluation we utilized RWTH?sstate-of-the-art phrase-based and hierarchical trans-lation systems as well as our in-house system com-bination framework.
GIZA++ (Och and Ney, 2003)was employed to train word alignments, languagemodels have been created with the SRILM toolkit(Stolcke, 2002).2.1 Phrase-Based SystemWe applied a phrase-based translation (PBT) systemsimilar to the one described in (Zens and Ney, 2008).Phrase pairs are extracted from a word-aligned bilin-gual corpus and their translation probability in bothdirections is estimated by relative frequencies.
Thestandard feature set moreover includes an n-gramlanguage model, phrase-level single-word lexiconsand word-, phrase- and distortion-penalties.
To lexi-calize reordering, a discriminative reordering model(Zens and Ney, 2006a) is used.
Parameters are opti-mized with the Downhill-Simplex algorithm (Nelderand Mead, 1965) on the word graph.2.2 Hierarchical SystemFor the hierarchical setups described in this paper,the open source Jane toolkit (Vilar et al, 2010) wasemployed.
Jane has been developed at RWTH andimplements the hierarchical approach as introducedby Chiang (2007) with some state-of-the-art exten-sions.
In hierarchical phrase-based translation, aweighted synchronous context-free grammar is in-duced from parallel text.
In addition to contiguouslexical phrases, hierarchical phrases with up to twogaps are extracted.
The search is typically carriedout using the cube pruning algorithm (Huang andChiang, 2007).
The standard models integrated intoour Jane systems are: phrase translation probabil-ities and lexical translation probabilities on phraselevel, each for both translation directions, length405penalties on word and phrase level, three binary fea-tures marking hierarchical phrases, glue rule, andrules with non-terminals at the boundaries, source-to-target and target-to-source phrase length ratios,four binary count features and an n-gram languagemodel.
The model weights are optimized with stan-dard MERT (Och, 2003) on 100-best lists.2.3 System CombinationSystem combination is used to produce consensustranslations from multiple hypotheses produced withdifferent translation engines that are better in termsof translation quality than any of the individual hy-potheses.
The basic concept of RWTH?s approachto machine translation system combination has beendescribed by Matusov et al (Matusov et al, 2006;Matusov et al, 2008).
This approach includes anenhanced alignment and reordering framework.
Alattice is built from the input hypotheses.
The trans-lation with the best score within the lattice accordingto a couple of statistical models is selected as con-sensus translation.3 Translation ModelingWe incorporated several novel methods into our sys-tems for the WMT 2011 evaluation.
This sectionprovides a short survey of three of the methodswhich we suppose to be of particular interest.3.1 Language Model Data SelectionFor the English and German language models,we applied the data selection method proposed in(Moore and Lewis, 2010).
Each sentence is scoredby the difference in cross-entropy between a lan-guage model trained from in-domain data and a lan-guage model trained from a similar-sized sample ofthe out-of-domain data.
As in-domain data we usedthe news-commentary corpus.
The out-of-domaindata from which the data was selected are the newscrawl corpus for both languages and for English the109 corpus and the LDC Gigaword data.
We used a3-gram trained with the SRI toolkit to compute thecross-entropy.
For the news crawl corpus, only 1/8of the sentences were discarded.
Of the 109 corpuswe retained 1/2 and of the LDC Gigaword data weretained 1/4 of the sentences to train the languagemodels.3.2 Phrase Model TrainingFor the German?English and French?Englishtranslation tasks we applied a forced alignment pro-cedure to train the phrase translation model with theEM algorithm, similar to the one described in (DeN-ero et al, 2006).
Here, the phrase translation prob-abilities are estimated from their relative frequen-cies in the phrase-aligned training data.
The phrasealignment is produced by a modified version of thetranslation decoder.
In addition to providing a statis-tically well-founded phrase model, this has the ben-efit of producing smaller phrase tables and thus al-lowing more rapid experiments.
A detailed descrip-tion of the training procedure is given in (Wuebkeret al, 2010).3.3 Soft String-to-DependencyGiven a dependency tree of the target language,we are able to introduce language models that spanover longer distances than the usual n-grams, as in(Shen et al, 2008).
To obtain dependency structures,we apply the Stanford parser (Klein and Manning,2003) on the target side of the training material.RWTH?s open source hierarchical translation toolkitJane has been extended to include dependency infor-mation in the phrase table and to build dependencytrees on the output hypotheses at decoding time fromthis information.Shen et al (2008) use only phrases that meet cer-tain restrictions.
The first possibility is what the au-thors call a fixed dependency structure.
With theexception of one word within this phrase, calledthe head, no outside word may have a dependencywithin this phrase.
Also, all inner words may onlydepend on each other or on the head.
For a secondstructure, called a floating dependency structure, thehead dependency word may also exist outside thephrase.
If the dependency structure of a phrase con-forms to these restrictions, it is denoted as valid.In our phrase table, we mark those phrases thatpossess a valid dependency structure with a binaryfeature, but all phrases are retained as translation op-tions.
In addition to storing the dependency informa-tion, we also memorize for all hierarchical phrasesif the content of gaps has been dependent on the leftor on the right side.
We utilize the dependency in-formation during the search process by adding three406French EnglishSentences 3 710 985Running Words 98 352 916 87 689 253Vocabulary 179 548 216 765Table 1: Corpus statistics of the preprocessed high-quality training data (Europarl, news-commentary, andselected parts of the 109 and UN corpora) for theRWTH systems for the WMT 2011 French?English andEnglish?French translation tasks.
Numerical quantitiesare replaced by a single category symbol.features to the log-linear model: merging errors tothe left, merging errors to the right, and the ratio ofvalid vs. non-valid dependency structures.
The de-coder computes the corresponding costs when it triesto construct a dependency tree of a (partial) hypothe-sis on-the-fly by merging the dependency structuresof the used phrase pairs.In an n-best reranking step, we compute depen-dency language model scores on the dependencieswhich were assembled on the hypotheses by thesearch procedure.
We apply one language modelfor left-side dependencies and one for right-side de-pendencies.
For head structures, we also computetheir scores by exploiting a simple unigram languagemodel.
We furthermore include a language countfeature that is incremented each time we computea dependency language model score.
As trees withfew dependencies have less individual costs to becomputed, they tend to obtain lower overall coststhan trees with more complex structures in othersentences.
The intention behind this feature is thuscomparable to the word penalty in combination witha normal n-gram language model.4 French-English SetupsWe set up both hierarchical and standard phrase-based systems for the constrained condition of theWMT 2011 French?English and English?Frenchtranslation tasks.
The English?French RWTH pri-mary submission was produced with a single hierar-chical system, while a system combination of threesystems was used to generate a final hypothesis forthe French?English primary submission.Besides the Europarl and news-commentary cor-pora, the provided parallel data also comprehendsFrench EnglishSentences 29 996 228Running Words 916 347 538 778 544 843Vocabulary 1 568 089 1 585 093Table 2: Corpus statistics of the preprocessed full trainingdata for the RWTH primary system for the WMT 2011English?French translation task.
Numerical quantitiesare replaced by a single category symbol.the large French-English 109 corpus and the French-English UN corpus.
Since model training withsuch a huge amount of data requires a consider-able computational effort, RWTH decided to selecta high-quality part of altogether about 2 Mio.
sen-tence pairs from the latter two corpora.
The selec-tion of parallel sentences was carried out accordingto three criteria: (1) Only sentences of minimumlength of 4 tokens are considered, (2) at least 92%of the vocabulary of each sentence occurs in new-stest2008, and (3) the ratio of the vocabulary sizeof a sentence and the number of its tokens is mini-mum 80%.
Word alignments in both directions weretrained with GIZA++ and symmetrized according tothe refined method that was proposed in (Och andNey, 2003).
The phrase tables of the translationsystems are extracted from the Europarl and news-commentary parallel training data as well as the se-lected high-quality parts the 109 and UN corporaonly.
The only exception is the hierarchical systemused for the English?French RWTH primary sub-mission which comprehends a second phrase tablewith lexical (i.e.
non-hierarchical) phrases extractedfrom the full parallel data (approximately 30 Mio.sentence pairs).Detailed statistics of the high-quality paralleltraining data (Europarl, news-commentary, and theselected parts of the 109 and UN corpora) are givenin Table 1, the corpus statistics of the full paralleldata from which the second phrase table with lexi-cal phrases for the English?French RWTH primarysystem was created are presented in Table 2.The translation systems use large 4-gram lan-guage models with modified Kneser-Ney smooth-ing.
The French language model was trained onmost of the provided French data including themonolingual LDC Gigaword corpora, the English407newstest2009 newstest2010French?English BLEU TER BLEU TERSystem combination of ?
systems (primary) 26.7 56.0 27.4 54.9PBT with triplet lexicon, no forced alignment (contrastive) ?
26.2 56.7 27.2 55.3Jane as below + improved LM (contrastive) 26.3 57.4 26.7 56.2Jane with parse match + syntactic labels + dependency ?
26.2 57.5 26.5 56.4PBT with forced alignment phrase training ?
26.0 57.1 26.3 56.0Table 3: RWTH systems for the WMT 2011 French?English translation task (truecase).
BLEU and TER results arein percentage.newstest2009 newstest2010English?French BLEU TER BLEU TERJane shallow + in-domain TM + lexical phrases from full data 25.3 60.1 27.1 57.2Jane shallow + in-domain TM + triplets + DWL + parse match 24.8 60.5 26.6 57.5PBT with triplets, DWL, sentence-level word lexicon, discrim.
reord.
24.8 60.1 26.5 57.3Table 4: RWTH systems for the WMT 2011 English?French translation task (truecase).
BLEU and TER results arein percentage.language model was trained on automatically se-lected English data (cf.
Section 3.1) from the pro-vided resources including the 109 corpus and LDCGigaword.The scaling factors of the log-linear model com-bination are optimized towards BLEU on new-stest2009, newstest2010 is used as an unseen test set.4.1 Experimental Results French?EnglishThe results for the French?English task are given inTable 3.
RWTH?s three submissions ?
one primaryand two contrastive ?
are labeled accordingly in thetable.
The first contrastive submission is a phrase-based system with a standard feature set plus an ad-ditional triplet lexicon model (Mauser et al, 2009).The triplet lexicon model was trained on in-domainnews commentary data only.
The second contrastivesubmission is a hierarchical Jane system with threesyntax-based extensions: A parse match model (Vi-lar et al, 2008), soft syntactic labels (Stein et al,2010), and the soft string-to-dependency extensionas described in Section 3.3.
The primary submis-sion combines the phrase-based contrastive system,a hierarchical system that is very similar to the Janecontrastive submission but with a slightly worse lan-guage model, and an additional PBT system that hasbeen trained with forced alignment (Wuebker et al,2010) on WMT 2010 data only.4.2 Experimental Results English?FrenchThe results for the English?French task are givenin Table 4.
We likewise submitted two contrastivesystems for this translation direction.
The first con-trastive submission is a phrase-based system, en-hanced with a triplet lexicon model and a discrim-inative word lexicon model (Mauser et al, 2009) ?both trained on in-domain news commentary dataonly ?
as well as a sentence-level single-word lex-icon model and a discriminative reordering model(Zens and Ney, 2006a).
The second contrastive sub-mission is a hierarchical Jane system with shallowrules (Iglesias et al, 2009), a triplet lexicon model, adiscriminative word lexicon, the parse match model,and a second phrase table extracted from in-domaindata only.
Our primary submission is very similarto the latter Jane setup.
It does not comprise the ex-tended lexicon models and the parse match exten-sion, but instead includes lexical phrases from thefull 30 Mio.
sentence corpus as described above.5 German-English SetupsWe trained phrase-based and hierarchical transla-tion systems for both translation directions of theGerman-English language pair.
The corpus statis-408German EnglishSentences 1 857 745Running Words 48 449 977 50 559 217Vocabulary 387 593 123 470Table 5: Corpus statistics of the preprocessed train-ing data for the WMT 2011 German?English andEnglish?German translation tasks.
Numerical quantitiesare replaced by a single category symbol.tics can be found in Table 5.
Word alignments weregenerated with GIZA++ and symmetrized as for theFrench-English setups.The language models are 4-grams trained on thebilingual data as well as the provided News crawlcorpus.
For the English language model the 109French-English and LDC Gigaword corpora wereused additionally.
For the 109 French-English andLDC Gigaword corpora RWTH applied the data se-lection technique described in Section 3.1.
We ex-amined two different language models, one withLDC data and one without.Systems were optimized on the newstest2009 dataset, newstest2008 was used as test set.
The scoresfor newstest2010 are included for completeness.5.1 Morpho-Syntactic AnalysisIn order to reduce the source vocabulary size forthe German?English translation, the source sidewas preprocessed by splitting German compoundwords with the frequency-based method describedin (Koehn and Knight, 2003).
To further reducetranslation complexity, we performed the long-rangepart-of-speech based reordering rules proposed by(Popovic?
et al, 2006).
For additional experimentswe used the TreeTagger (Schmid, 1995) to producea lemmatized version of the German source.5.2 Optimization CriterionWe studied the impact of different optimization cri-teria on tranlsation performance.
The usual prac-tice is to optimize the scaling factors to maximizeBLEU.
We also experimented with two differentcombinations of BLEU and Translation Edit Rate(TER): TER?BLEU and TER?4BLEU.
The firstdenotes the equally weighted combination, while forthe latter BLEU is weighted 4 times as strong asTER.5.3 Experimental Results German?EnglishFor the German?English task we conducted ex-periments comparing the standard phrase extractionwith the phrase training technique described in Sec-tion 3.2.
For the latter we applied log-linear phrase-table interpolation as proposed in (Wuebker et al,2010).
Further experiments included the use of addi-tional language model training data, reranking of n-best lists generated by the phrase-based system, anddifferent optimization criteria.
We also carried outa system combination of several systems, includingphrase-based systems on lemmatized German andon source data without compound splitting and twohierarchical systems optimized for different criteria.The results are given in Table 6.A considerable increase in translation quality canbe achieved by application of German compoundsplitting.
The system that operates on Germansurface forms without compound splitting (SUR)clearly underperforms the baseline system with mor-phological preprocessing.
The system on lemma-tized German (LEM) is at about the same level asthe system on surface forms.In comparison to the standard heuristic phrase ex-traction technique, performing phrase training (FA)gives an improvement in BLEU on newstest2008and newstest2009, but a degradation in TER.
Theaddition of LDC Gigaword corpora (+GW) to thelanguage model training data shows improvementsin both BLEU and TER.
Reranking was done on1000-best lists generated by the the best availablesystem (PBT (FA)+GW).
Following models wereapplied: n-gram posteriors (Zens and Ney, 2006b),sentence length model, a 6-gram LM and single-word lexicon models in both normal and inverse di-rection.
These models are combined in a log-linearfashion and the scaling factors are tuned in the samemanner as the baseline system (using TER?4BLEUon newstest2009).The table includes three identical Jane systemswhich are optimized for different criteria.
The oneoptimized for TER?4BLEU offers the best balancebetween BLEU and TER, but was not finished intime for submission.
As primary submission wechose the reranked PBT system, as secondary thesystem combination.409newstest2008 newstest2009 newstest2010German?English opt criterion BLEU TER BLEU TER BLEU TERSyscombi of ?
(secondary) TER?BLEU 21.1 62.1 20.8 61.2 23.7 59.2Jane +GW ?
BLEU 21.5 63.9 21.0 63.3 22.9 61.7Jane +GW TER?4BLEU 21.4 62.6 21.1 62.0 23.5 60.3PBT (FA) rerank +GW (primary) ?
TER?4BLEU 21.4 62.8 21.1 61.9 23.4 60.1PBT (FA) +GW ?
TER?4BLEU 21.1 63.0 21.1 62.2 23.3 60.3Jane +GW ?
TER?BLEU 20.9 61.1 20.4 60.5 23.4 58.3PBT (FA) TER?4BLEU 21.1 63.2 20.6 62.4 23.2 60.4PBT TER?4BLEU 20.6 62.7 20.3 61.9 23.3 59.7PBT (SUR) ?
TER?4BLEU 19.5 66.5 18.9 65.8 21.0 64.9PBT (LEM) ?
TER?4BLEU 19.2 66.1 18.9 65.4 21.0 63.5Table 6: RWTH systems for the WMT 2011 German?English translation task (truecase).
BLEU and TER resultsare in percentage.
FA denotes systems with phrase training, +GW the use of LDC data for the language model.SUR and LEM denote the systems without compound splitting and on the lemmatized source, respectively.
The threehierarchical Jane systems are identical, but used different parameter optimization criterea.newstest2008 newstest2009 newstest2010English?German opt criterion BLEU TER BLEU TER BLEU TERPBT + discrim.
reord.
(primary) TER?4BLEU 15.3 70.2 15.1 69.8 16.2 65.6PBT + discrim.
reord.
BLEU 15.2 70.6 15.2 70.1 16.2 66.0PBT TER?4BLEU 15.2 70.7 15.2 70.2 16.2 66.1Jane BLEU 15.1 72.1 15.4 71.2 16.4 67.4Jane TER?4BLEU 15.1 68.4 14.6 69.5 14.6 65.9Table 7: RWTH systems for the WMT 2011 English?German translation task (truecase).
BLEU and TER results arein percentage.5.4 Experimental Results English?GermanWe likewise studied the effect of using BLEU onlyversus using TER?4BLEU as optimization crite-rion in the English?German translation direction.Moreover, we tested the impact of the discriminativereordering model (Zens and Ney, 2006a).
The re-sults can be found in Table 7.
For the phrase-basedsystem, optimizing towards TER?4BLEU leads toslightly better results both in BLEU and TER thanoptimizing towards BLEU.
Using the discriminativereordering model yields some improvements both onnewstest2008 and newstest2010.
In the case of thehierarchical system, the effect of the optimizationcriterion is more pronounced than for the phrase-based system.
However, in this case it clearly leadsto a tradeoff between BLEU and TER, as the choiceof TER?4BLEU harms the translation results oftest2010 with respect to BLEU.6 ConclusionFor the participation in the WMT 2011 shared trans-lation task, RWTH experimented with both phrase-based and hierarchical translation systems.
We usedall bilingual and monolingual data provided for theconstrained track.
To limit the size of the lan-guage model, a data selection technique was applied.Several techniques yielded improvements over thebaseline, including three syntactic models, extendedlexicon models, a discriminative reordering model,forced alignment training, reranking methods anddifferent optimization criteria.AcknowledgmentsThis work was achieved as part of the Quaero Pro-gramme, funded by OSEO, French State agency forinnovation.410ReferencesD.
Chiang.
2007.
Hierarchical Phrase-Based Transla-tion.
Computational Linguistics, 33(2):201?228.J.
DeNero, D. Gillick, J. Zhang, and D. Klein.
2006.Why Generative Phrase Models Underperform SurfaceHeuristics.
In Proceedings of the Workshop on Statis-tical Machine Translation, pages 31?38.L.
Huang and D. Chiang.
2007.
Forest Rescoring: FasterDecoding with Integrated Language Models.
In Proc.Annual Meeting of the Association for ComputationalLinguistics, pages 144?151, Prague, Czech Republic,June.G.
Iglesias, A. de Gispert, E.R.
Banga, and W. Byrne.2009.
Rule Filtering by Pattern for Efficient Hierar-chical Translation.
In Proceedings of the 12th Con-ference of the European Chapter of the ACL (EACL2009), pages 380?388.D.
Klein and C.D.
Manning.
2003.
Accurate Unlexi-calized Parsing.
In Proceedings of the 41st AnnualMeeting on Association for Computational Linguistics- Volume 1, ACL ?03, pages 423?430.P.
Koehn and K. Knight.
2003.
Empirical Methodsfor Compound Splitting.
In Proceedings of EuropeanChapter of the ACL (EACL 2009), pages 187?194.E.
Matusov, N. Ueffing, and H. Ney.
2006.
ComputingConsensus Translation from Multiple Machine Trans-lation Systems Using Enhanced Hypotheses Align-ment.
In Conference of the European Chapter of theAssociation for Computational Linguistics (EACL),pages 33?40.E.
Matusov, G. Leusch, R.E.
Banchs, N. Bertoldi,D.
Dechelotte, M. Federico, M. Kolss, Y.-S. Lee,J.B.
Marino, M. Paulik, S. Roukos, H. Schwenk, andH.
Ney.
2008.
System Combination for MachineTranslation of Spoken and Written Language.
IEEETransactions on Audio, Speech and Language Pro-cessing, 16(7):1222?1237.A.
Mauser, S. Hasan, and H. Ney.
2009.
Extending Sta-tistical Machine Translation with Discriminative andTrigger-Based Lexicon Models.
In Conference onEmpirical Methods in Natural Language Processing,pages 210?217.R.C.
Moore and W. Lewis.
2010.
Intelligent Selectionof Language Model Training Data.
In ACL (Short Pa-pers), pages 220?224, Uppsala, Sweden, July.J.A.
Nelder and R. Mead.
1965.
The Downhill SimplexMethod.
Computer Journal, 7:308.F.J.
Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29(1):19?51.F.J.
Och.
2003.
Minimum Error Rate Training for Statis-tical Machine Translation.
In Proc.
Annual Meeting ofthe Association for Computational Linguistics, pages160?167, Sapporo, Japan, July.M.
Popovic?, D. Stein, and H. Ney.
2006.
StatisticalMachine Translation of German Compound Words.In FinTAL - 5th International Conference on NaturalLanguage Processing, Springer Verlag, LNCS, pages616?624.H.
Schmid.
1995.
Improvements in Part-of-Speech Tag-ging with an Application to German.
In Proceedingsof the ACL SIGDAT-Workshop, pages 47?50, Dublin,Ireland, March.L.
Shen, J. Xu, and R. Weischedel.
2008.
A New String-to-Dependency Machine Translation Algorithm with aTarget Dependency Language Model.
In Proceedingsof ACL-08: HLT.
Association for Computational Lin-guistics, pages 577?585, June.D.
Stein, S. Peitz, D. Vilar, and H. Ney.
2010.
A Cocktailof Deep Syntactic Features for Hierarchical MachineTranslation.
In Conference of the Association for Ma-chine Translation in the Americas 2010, page 9, Den-ver, USA, October.A.
Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proc.
Int.
Conf.
on Spoken LanguageProcessing, volume 2, pages 901 ?
904, Denver, Col-orado, USA, September.D.
Vilar, D. Stein, and H. Ney.
2008.
Analysing SoftSyntax Features and Heuristics for Hierarchical PhraseBased Machine Translation.
In Proc.
of the Int.
Work-shop on Spoken Language Translation (IWSLT), pages190?197, Waikiki, Hawaii, October.D.
Vilar, S. Stein, M. Huck, and H. Ney.
2010.
Jane:Open Source Hierarchical Translation, Extended withReordering and Lexicon Models.
In ACL 2010 JointFifth Workshop on Statistical Machine Translation andMetrics MATR, pages 262?270, Uppsala, Sweden,July.J.
Wuebker, A. Mauser, and H. Ney.
2010.
TrainingPhrase Translation Models with Leaving-One-Out.
InProceedings of the 48th Annual Meeting of the Assoc.for Computational Linguistics, pages 475?484, Upp-sala, Sweden, July.R.
Zens and H. Ney.
2006a.
Discriminative ReorderingModels for Statistical Machine Translation.
In HumanLanguage Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics AnnualMeeting (HLT-NAACL), Workshop on Statistical Ma-chine Translation, pages 55?63, New York City, June.R.
Zens and H. Ney.
2006b.
N-gram Posterior Proba-bilities for Statistical Machine Translation.
In HumanLanguage Technology Conf.
/ North American Chap-ter of the Assoc.
for Computational Linguistics AnnualMeeting (HLT-NAACL), Workshop on Statistical Ma-chine Translation, pages 72?77, New York City, June.411R.
Zens and H. Ney.
2008.
Improvements in DynamicProgramming Beam Search for Phrase-based Statisti-cal Machine Translation.
In Proc.
of the Int.
Workshopon Spoken Language Translation (IWSLT), Honolulu,Hawaii, October.412
