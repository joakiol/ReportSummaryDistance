DISCOVERY PROCEDURES FOR SUBLANGUAGE SELECTIONAL PATTERNS:INITIAL EXPERIMENTSRa lph  Gr i shmanComputer  Science DepartmentCourant Institute of Mathematical  SciencesNew York UniversityNew York, NY 10012Lynet te  H i rschmanResearch and Development DivisionSystem Development Corporat ion - A Burroughs CompanyPaoli, PA 19301Ngo Thanh NhanComputer  Science DepartmentCourant Institute of Mathematical  SciencesNew York UniversitySelectional constraints specify, for a particular domain, the combinations of semantic lasses accepta-ble in subject-verb-object relationships and other syntactic structures.
These constraints are importantin blocking incorrect analyses in natural anguage processing systems.
However, these constraints aredomain-specific and hence must be developed anew when a system is ported to a new domain.
Adiscovery procedure for selectional constraints is therefore essential in enhancing the portability ofsuch systems.This paper describes a semi-automated procedure for collecting the co-occurrence patterns from asample of texts in a domain, and then using these patterns as the basis for selectional constraints inanalyzing further texts.
We discuss some of the difficulties in automating the collection process, anddescribe two experiments that measure the completeness of these patterns and their effectivenesscompared with manually-prepared patterns.
We then describe and evaluate a procedure for selectionalconstraint relaxation, intended to compensate for gaps in the set of patterns.
Finally, we suggest howthese procedures could be combined with a system that queries a domain expert, in order to produce amore efficient discovery procedure.1 INTRODUCTION:THE NEED FOR DISCOVERY PROCEDURESIn order to analyze natural language texts reliably, acomputer system requires a great deal of informationabout the syntax of the language, about the structure ofthe discourse, and about the subject matter with whichthe text deals.
Because of the need for detailed know-ledge of the subject matter, natural language systems atpresent are limited to handling texts within very limiteddomains of discourse.
Once such a system has beendeveloped, the question of portability naturally arises:can the system be readily moved to a new domain?Portability involves two separate issues.
The firstissue is whether a large portion of the natural languagesystem is domain-independent, so that this "core" can beused in the new application without modification.
Thesecond issue is whether the domain-dependent i forma-tion required by the system can be gathered in a method-ical and efficient fashion.
Our paper addresses the latterCopyright1986 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is granted provided thatthe copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
To copyotherwise, or to republish, requires a fee and/or specific permission.0362-613X/86/030205-215503.00Computational Linguistics, Volume 12, Number 3, July-September 1986 205Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patternsissue.
Specifically, we report on some experiments aimedat developing a semi-automated procedure for discover-ing selectional patterns (the local semantic onstraints oflanguage in a particular domain) from the analysis of asample of text in that domain.2 SUBLANGUAGE AND SELECTION2.1 SUBLANGUAGEA sublanguage is a specialized form of natural languageused to describe a limited subject matter, generallyemployed by a group of specialists dealing with thissubject.
Examples of sublanguages that have beenstudied are weather reports (Chevalier et al 1978),aircraft maintenance manuals (Lehrberger 1983),medical reports (Hirschman and Sager 1983), and equip-ment failure reports (Marsh, Hamburger, and Grishman1984).
A sublanguage will generally be much moreconstrained than the "standard language", but it mayalso include extensions to the standard language, such assentence fragments found in telegraphic-style messagetext.Zellig Harris, one of the first linguists to studylanguage use in restricted omains, defined sublanguagesin terms of one particular constraint: the constraint onwhat words can co-occur within a particular syntacticpattern, such as a subject-verb-object structure (Harris1968).
Just as speakers of the standard language distin-guish between grammatical and ungrammaticalsentences, speakers of the sublanguage will distinguishbetween acceptable and unacceptable (meaningless)sentences, even though the unacceptable sentences maybe grammatical sentences of the standard language.
Forexample, in the sublanguage of medical records, a speak-er would accept he sentence The X-ray revealed a tumor.but not The tumor revealed an X-ray.Harris hypothesized that, for any particular sublan-guage, we can define sublanguage word classes - sets ofwords that are acceptable in the same contexts (Harris1968).
For example, in the context m revealed a tumor,we might find words such as X-ray, fi lm, and scan.
Suchclasses, even though defined on purely distributionalgrounds, correspond closely to the natural semantic lass-es that might be identified by an expert in the domain.Thus, we might label the group of words X-ray, fi lm, andscan as a TEST class, and similarly (for medical reports)identify classes such as FINDING and MEDICATION.See section 3.2 below for discussion of an experimentverifying Harris's hypothesis.
The sublanguage co-occur-rence constraints, when stated between word classes, arecommonly called selectional constraints.2.2 IMPLEMENTING SELECTIONAL CONSTRAINTSIt is generally recognized that these selectionalconstraints play an important role in distinguishingbetween correct and incorrect sentence analyses.
Conse-quently, most natural anguage systems incorporate someform of selectional constraints.
We describe here, brief-ly, how these constraints are implemented in the Linguis-tic String Parser; more detailed descriptions are given inGrishman, Hirschman, and Friedman (1982, 1983).The Linguistic String Parser English grammar (Sager1981) is an augmented context-free grammar.
Its princi-pal components are a context-free grammar (stated interms of the grammatical categories of English), a set ofprocedural restrictions (written in Restriction Language:Sager and Grishman 1975), and a lexicon.
Adding selec-tional constraints to this grammar involved specifying thesublanguage classifications of words, specifying theallowed co-occurrence patterns in positive terms, andproviding restrictions that check the parse tree for thesepatterns.Each word in the domain vocabulary is assigned toone or more sublanguage word classes; these classassignments are recorded as part of the lexical entry foreach word.
Thus a lexical entry consists of each majorsyntactic lass for a word followed by a list of attributes,including its domain subclass.
Words that have two ormore meanings and, as a result, occur in differentcontexts will be assigned to more than one word class;such words are referred to as homographs.
For example,in the medical domain, discharge refers both to a patient'sdischarge from a hospital and to the excretion of some-thing from the patient's body.
It is classified as a nounwith attributes H-VMD (medical verb for 'discharge fromhospital') and H-BODYPART (for 'bloody discharge').As a verb, however, discharge is classified only asH-VMD.
(Only words specific to the domain receivesublanguage classes and participate in selection.
)The allowed co-occurrence patterns are specified by aset of lists in the grammar.
There is a separate list foreach major syntactic relation: clauses (subject-verb-ob-ject structures), prepositional phrases, prenominal adjec-tives, compound nouns.
Each list element may haveassociated sub-lists; this recursive structure provides areasonably compact specification of the allowable combi-nation of sublanguage classes.
For example, the V-S-Olist gives the positive co-occurrence patterns for theVERB-SUBJECT-OBJECT relation, where the list associ-ated with each verb class (e.g., H-SHOW below) has asub-list of associated subject classes (e.g., H-BODYPARTand H-TEST), each of which have associated lists ofobject types (H-INDIC, H-RESULT, H-DIAG).LIST V-S-O --H-SHOW: (H-BODYPART: (H-INDIC, H-RESULT,H-DIAG),H-TEST: (H-INDIC, H-RESULT,H-DIAG),...),This is interpreted as follows:?
H-SHOW verbs with H-BODYPART ("body part" class)as subject take objects of classes H-INDIC("indicators"), H-RESULT or H-DIAG ("diagnosis"),as in liver showed no abnormalities;206 Computational Linguistics, Volume 12, Number 3, July-September 1986Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns?
with H-TEST ("test") as subject, H-SHOW verbs alsotake the objects H-INDIC, H-DIAG, and H-RESULT, asin test showed metastasis.The list imposes selection only for listed verbs, and notall verbs appear on the list.
In particular, be and relatedverbs do not, since they obey a different kind of selection(between subject and object).
Similarly, not all prep-ositions participate in selection for prepositional phrases;specifically, of has too broad a distribution for the state-ment of selectional patterns.
In this way, selection isapplied only to sublanguage-specific constructs, where itis possible to describe the allowed patterns with reason-able conciseness.The selectional constraints are enforced by a set ofrestrictions that use the lists of co-occurrence patterns.Whenever a structure involved in selection (e.g., clause,noun phrase, prepositional phrase) is completed uring aparse, a restriction is executed that compares the classesassigned to the words in the parse tree with the allowedselectional patterns for this structure.
If the word partic-ipates in selection, but its associated arguments do notmatch the patterns on the list, then the analysis isrejected and the parser backs up to seek an alternativeanalysis.
Because it operates on surface structure, therestriction that tests for subject-verb-0bject selectionmust take into account all the transforms of this basicstructure.
For clauses, the restriction checks selection forboth active and passive sentences, sentences with inter-vening aspectuals (as in patient continued to receive medi-cation), and relative and reduced relative clauses.
It doesthis by identifying the "transformed" subject, verb, andobject; it can then use a single canonical set of subject-verb-object patterns for selection.2.3 THE VALUE OF SELECTIONAlthough it is generally agreed that selectionalconstraints are important in separating correct and incor-rect analyses, we are not aware of any measurements ofthe impact of selectional constraints, particularly in textanalysis (as contrasted with the analysis of naturallanguage database queries, for example).
In order toobtain a more objective measure of the importance ofthese constraints, we conducted an experiment compar-ing the effectiveness of grammars with and without selec-tional constraints.The test corpus was a set of hospital dischargesummaries containing 407 sentences and sentence frag-ments.
We used the NYU Linguistic String Projectmedical grammar, which is a modification of the Linguis-tic String Project English grammar including the sentencefragments and other constructs (such as descriptions ofmedication dosages) that appear in medical reports butnot in standard English (Marsh 1983).
Each sentencewas analyzed twice, once without any selectionalconstraints and once with selectional constraints (theselectional patterns were developed manually at NYU bylinguists from a study of this test corpus and other similarmedical reports).
The results of each analysis were clas-sifted into one of three categories: no parses obtained;one or more parses obtained, good first parse; 1 one ormore parses obtained, bad first parse.
These results aresummarized in Table 1.good parsesbad parsesno parseswith selection without selection308 (76%) 306 (75%)30 (7%) 68 (17%)69 (17%) 33 (8%)Table 1.
Parsing results for 407 sentences, run with andwithout selectional constraints.We found, in brief, that adding selectional constraintshad only a marginal effect on the number of good parses.However, it greatly reduced the number of bad parses.Sentences that previously got bad parses now got noparse at all.
This somewhat surprising result can beexplained by noting that a certain number of sentencesthat had previously parsed correctly were blocked byover-constraining due to selection.
For example, thephrase herpes type lesion was parsed successfully withoutselection, but failed to parse with selection, because therewere no selection patterns for allowing a compound nounof the form herpes type (H-DIAG + H-TYPE) + lesion(H-INDIC).
On the other hand, some sentences thatreceived an incorrect first parse without selectionreceived a correct parse with selection because the incor-rect parse is blocked by selection.
For example, thepatient had no JVD and no increase in thyroid size parsedincorrectly without selection due to incorrect distributionof the adjunct in thyroid size, but correctly with selection.Overall, 21 sentences (out of a corpus of 400) changedfrom good to no parse and 23 from bad parse to good oracceptable parse.Despite the fact that the number of correct parses didnot show any significant increase, the use of selectionproduced a very substantial improvement in reliability ofparses.
We consider this an important benefit, for tworeasons.
First, in critical applications, an undetectederror (bad parse) may lead to erroneous data in the database; this is much worse than an error detected by thesystem (no parse).
Second, if the analysis failure can bedetected, it is possible to try various recovery techniques,such as employing a different analysis technique orasking the user for additional information.2.3 THE ROLE OF SELECTIONWe recognize that selectional constraints may only be thetip of the iceberg in terms of domain-specific informa-tion.
Much more detailed knowledge of the domain andthe structure of discourse in the sublanguage will doubt-less be needed for a high-quality text analysis system.Nonetheless, we believe selection has an important roleto play.
As shown by the experiment just described,more than half of the analysis errors resulting fromsyntactic analysis can be detected using selectionalComputational Linguistics, Volume 12, Number 3, July-September 1986 207Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patternsconstraints alone.
In addition, selectional constraints aresimple in structure and have been more intensivelystudied than most other domain knowledge; in particular,their relationship to distributional information in thesublanguage is better understood.
It therefore seemedappropriate to focus on selectional constraints in ourstudies of discovery procedures for domain-specificknowledge.3 DISCOVERY PROCEDURES3.1 EXPERT VS. TEXT-BASED PROCEDURESTwo basic approaches have been proposed for mechaniz-ing (or partially mechanizing) the acquisition of domain-specific information for natural language systems.
Oneof these is based on the systematic interviewing of adomain expert, who provides information on the basicsemantic classes and relations of the domain and theirlinguistic forms and properties.
Such an approach hasbeen incorporated into some natural language interfacesfor database retrieval, such as TEAM (Grosz 1983) andLDC (Ballard, Lusth, and Tinkham 1984).
Thisapproach assumes that the domain expert has somemodel of the relations in the domain, and a knowledge ofthe different ways in which these relations can be refer-enced.
This is not unreasonable in the database context,since the database schema can serve as a domain model(divining all the ways in which a relationship can bereferenced may still be difficult, however).
Thisapproach is more difficult, however, in text analysisapplications, particularly because the user may not havesuch a clear model of the domain semantics from whichto work.An alternative approach is to acquire some of thedomain-specific nformation from the text itself.
To theextent that this information is reflected in distributionalrelationships in the text, we can hope to extract thisinformation by automatically analyzing a sample of textin a new domain.
We have been pursuing this approachfor a number of years, and describe some of our earlierefforts in the next subsection.Although we present the expert and text-basedapproaches as alternatives, we do not believe them to bemutually exclusive.
It may turn out that the most effec-tive approach is a combination of these two, in whichinformation gleaned from the text "fills in" the skeletalinformation provided by an expert, and the expertprovides generalizations that could not easily be deriveddirectly from the text.
We shall return to this point in ourconcluding section.3.2 OUR PRIOR WORKOur previous work on discovery procedures has aimed atautomating the characterization of syntactic usage andthe identification of the principal semantic classes insublanguages.
Both of these procedures, as well as theprocedure to be described below, start from a set ofparse trees (prepared automatically or manually) for asample text in the domain.
The procedures for determin-ing syntactic usage process the file of parse trees toextract frequency data on the use of various productionsfrom the context-free grammar.
Recent tests of thisprocedure on both medical records and equipment failurerecords indicate that accurate characterizations can beobtained from a sample of a few hundred sentences, andthat (for both sublanguages) the size of the grammarused was roughly one-third the size of the full LinguisticString Parser English grammar (Grishman, Nhan, andMarsh 1984; Grishman, Nhan, Marsh, and Hirschman1984).The procedure for discovering sublanguage classes isbased on identifying words that occur in the text in simi-lar syntactic ontexts, e.g., as subject of a given verb oras object of a given verb or as adjective modifying agiven head, etc.
We defined a similarity coefficient forpairs of words, based on the number of contexts thewords shared.
Then, using a statistical clustering proce-dure, we grouped together words of high mutual similari-ty.
This procedure was successful in identifying classescontaining the high frequency words of the domain(Hirschman, Grishman, and Sager 1975); the procedurewas not effective for words that occurred only a fewtimes in the sample corpus.
Also, a number of false clus-ters were generated, due to linguistic phenomena wewere able to identify, such as the omission of the head ina noun phrase.
This produced anomalies in the classifica-tion, since the text contained occurrences uch as chestnormal (understood as 'chest X-ray normal') as well asX-ray normal.
The result was a high similarity betweenchest and X-ray and a resulting false cluster containingchest with various test words such as X-ray, film, andmammogram.4 AUTOMATIC GENERATION OF SELECT1ONALPATI'ERNSDetermining the selectional constraints for a new sublan-guage involves both determining the sublanguage wordclasses and determining the allowed co-occurrencepatterns among those classes.
In principle, both can bedetermined by a distributional analysis of a samplecorpus.
In practice, this is a labor-intensive procedureinvolving iteration between setting up sublanguage class-es and identifying sublanguage patterns.
However, inorder to simplify the work during our initial experiments,we chose to separate these two tasks.
We mentioned justabove the experiments we had conducted earlier ondiscovering sublanguage classes.
We describe here acomplementary set of experiments to demonstrate ourability to generate co-occurrence patterns from a textsample.
These experiments assume a (manual) assign-ment of words to sublanguage classes and aim at collect-ing the co-occurrence patterns and evaluating theircompleteness.
These complementary experiments areneeded to validate our techniques before we address themore difficult problem of building the selectional patterns208 Computational Linguistics, Volume 12, Number 3, July-September 1986Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patternsfor a new domain (see section 7 for a discussion of thisissue).Given our goal of evaluating the completeness of auto-matically generated patterns, our initial experiments drewon a domain where sublanguage vocabulary had alreadybeen classified.
Our test corpus consisted of elevenmedical reports.
Six were patient documents thatincluded patient history, examination, and plan of treat-ment; five were hospital "discharge summaries" whichincluded patient history, examination, and summaries ofthe course of treatment in the hospital.
The corpuscontained about 750 sentences and sentence fragments.In analyzing these sentences, we used the LinguisticString Project medical grammar, a modification of theLSP English grammar that had been used for processing anumber of medical documents (Hirschman et al 1981),including the discharge summaries in our corpus.
Thesublanguage word classes, which are recorded in our lexi-con, had been developed based on the discharge summa-ries and other similar medical records.
However, neitherthe grammar nor the word classes had been revised toreflect any new syntactic forms or semantic patterns thatappeared in the other six patient documents; these docu-ments were being analyzed for the first time.The discovery procedure for selectional patterns hasfive principal steps:1. generating a set of correct parses;2. resolving homographs;3. generating instances of selectional patterns from theparses;4. collecting the patterns into lists sorted by syntacticconstruct (e.g., a list for subject-verb-object patterns,a list for head-prepositional modifier patterns, etc.);5.
final review of patterns for correctness.We began by parsing the entire text with the Linguis-tic String Parser and the Linguistic String Project medicalgrammar, and collecting the resulting parse trees.
Gener-ation of correct patterns depends critically on havingcorrect parses; therefore, the automatically generatedparse trees had to be manually screened to select onlycorrect parses.
One possibility would have been togenerate (without relying on selection) all parses for eachsentence and then to choose the correct parse by hand.This would have required a great deal of work, sincewithout selection there may be many parses for eachsentence.
Since we were focusing on evaluating thecompleteness of the set of generated patterns, rather thanon the feasibility of acquiring the selectional patterns in anew domain, we chose to use the existing selection mech-anism as a short-cut to getting the correct parse.
Thisreduced drastically the number of parses that had to bescreened; it did not affect correctness of the chosenparse, since the parse is or is not correct, regardless ofhow it is generated.
However, for sentences blocked byselection, we did parse these sentences without selectionand did go through the manual procedure to select thecorrect parse.
For a number of sentences, we failed toobtain an automatically generated correct parse by eitherparsing method.
These sentences were not included inthe corpus.
This procedure furnished us with good parsesfor about 520 sentences and sentence fragments (about2/3  of the initial corpus).The next step was to resolve homographs.
As wenoted above, some words may have more than one mean-ing or be used in more than one way, and thus beassigned to more than one sublanguage class.
Within anyparticular sentence, the word was used in one of thesesenses and should therefore have been identified with thecorresponding sublanguage class in order to produce thecorrect sublanguage co-occurrence patterns.
Much of thehomograph resolution was done automatically, by theselection mechanism.
However, in certain cases, a wordemerged from the processing with multiple sublanguageclasses.
In some cases, this was due to insufficientcontext resulting from omission of implicit (zeroed)information, e.g., discharge on 1/12 would probably referto discharge of patient from the hospital, but selectioncould not rule out the SYMPTOM reading of dischargefrom this limited context.
A second source of unresolvedhomographs came from parses generated withoutselection, in which case there was no automatic mech-anism for homograph resolution.
Whatever the source,words listed as having multiple subclasses were screenedand disambiguated manually: we scanned the parse treesfor sentences containing multiply-classified words, and,in each case, selected manually the sublanguage classrelevant o its use in that sentence.We then proceeded to the task of extracting thesublanguage class co-occurrence patterns from the file ofcorrect surface parse trees.
Since co-occurrence patternsreflect a regularized or canonical structure (e.g., verb-subject-object relations), it was necessary to map surfacestructure into the normalized set of relations required forco-occurrence patterns.
This involved locating the"logical" subject and object in passive sentences, relativeclauses, reduced relatives, and clauses with aspectualverbs.
For example, in patient continued to receive medi-cation, the verb/subject/object co-occurrence pattern ofinterest is "receive/patient/medication".The computation of co-occurrence patterns was doneby a set of restrictions that borrowed code from thoseused for the selection mechanism itself.
(This was possi-ble because the selection mechanism also has to find thelogical elements involved in co-occurrence, including thecases where these differ from the surface structure).
Anappropriate restriction (e.g., a subject-verb-object or anadjective-noun or a noun-preposition-noun restriction)identified the structures that participate in selection(subject-verb-object, adjective-noun, or noun-preposi-tion-noun).
For each such structure, the restrictionlocated the words participating in the co-occurrencerelation and retrieved the sublanguage classes associatedwith these words.
The restriction then wrote the sublan-guage class pattern onto a file.
The pattern consisted ofComputational Linguistics, Volume 12, Number 3, July-September 1986 209Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Seleetional Patternsa tag identifying the pattern type (e.g., PRED-ARG1-ARG2 for verb-subject-object or NVAR-APOS fornoun-adjective) and the actual words in that instance ofthe pattern, followed by a line for each member of thatpattern, containing the major class (e.g., noun = N orpast participle = VEN), followed by the word, followedby the subclass.
*81A 1C.
1.11PRED-ARG 1-ARG2 EXAMINED ()VEN EXAMINED (H-VMD)N 0 (NIL)N JOINTS (H-AREA)*81A 1C.
I .
l lNVAR-APOS JOINTS OTHERN JOINTS (H-AREA)ADJ OTHER OTHERJO INTSThe final stage involved collecting, counting and refor-matting the set of co-occurrence patterns into the selec-tional lists required by the grammar.
This permitted us touse the automatically generated sets of co-occurrencepatterns as input to the selectional constraints of thegrammar.Prior to running any parsing experiments, wecompared the automatically generated selection lists tothe lists produced manually by a linguist.
Our expecta-tion was that the automatically generated lists would be asubset of the manually prepared set.
It turned out thatthis was not the case, primarily because a number ofhuman errors had allowed erroneous patterns to enter thefile: errors in assigning sublanguage classes to words,errors in resolving homographs, oversights in weeding outbad parses.
We therefore found it necessary in practiceto make a final manual pass over the file of patterns,discarding bad patterns that had crept in in one way oranother.
Only then were the patterns uitable for use asdata to the selectional restrictions.Although most of the data manipulation (generation ofparse trees, generation and collection of selectionalpatterns) was automated, considerable manual inter-vention was still needed to verify the processing at eachstage.
We shall return to this issue below.5 EVALUATIONWe have evaluated the selectional patterns obtained bythe procedure just described in two ways.
First, we havetried to estimate how complete the set of patterns is.Second, we have compared the effectiveness of thesepatterns in parsing new material with that of selectionalpatterns generated by hand.5.1 GROWTH CURVESA crucial question we wanted to answer with our exper-iment was whether the size of our text sample wasadequate to obtain a reasonably complete set of selec-tional patterns.
To answer this question, we plotted thegrowth in our sets of selectional patterns as a function ofthe size of the sample we have processed (i.e., thenumber of different patterns encountered in the first Xsentences).
Figures 1, 2, and 3 show the growth curvesfor the subject-verb-object, prepositional phrase, andadjective-noun selectional patterns.
2SUBJECT-VERB-13BJECT PATTERNS140130'I 20I tOI0090807060503020 7100 a...t..a...t..a_O 50//JI00 t50 200 250 300 350 400 450 500 550S IZE  OF TEXT SAMPLE \ [SENTENCESIFigure 1.
The growth in the number of subject-verb-object selectional patterns as a function of the size of the textsample (in sentences).
The solid curve is the actual data, the dashed line the least-squares fit to a functionof the form A*(1-exp( -B*x) ) .210 Computational Linguistics, Volume 12, Number 3, July-September 1986Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional PatternsFigure 2.160lqO120~n iooqO2000PREPOSIT IONAL PHRASE PATTERNSi i , i I i , i , I v i i , I i , i i I i v ~ i I i , , , I i i , i I i , i , I i i , i I i v i , I i ,,t,.~r7,Y //, , , I * , , i I i i i i I , , , , I i , i I I i i i t I i i i i I , , , , I i , i i I i i i t I i t ,50 t O0 t50  200 250 300 350 400 qSO 500 550S IZE  BF TEXT SAMPLE (SENTENCES)The growth in the number of prepositional phrase selectional patterns as a function of the size of the textsample (in sentences).
The solid curve is the actual data, the dashed line the least-squares fit to a function ofthe form A*(1 -exp( -B*x) ) .t2t-#120l lO100ADJECT I VE-NOUN PATTERNS70 /j/-~6OSOqO3O2Oto ~/0 \[ I I I J I ' I I \[ i i i i \[ i i i A \ [ l l l l J  i l i a  i l l l l l l l l i i t l t t \ [ l l l l  \ [ l t  I0 50 t O0 t 50 200 250 900 350 400 450 500 550SIZE BF TEXT SAMPLE (SENTENCES)Figure 3.
The growth in the number of adjective-noun selectional patterns as a function of the size of the text'sample(in sentences).
The solid curve is the actual data, the dashed line the least-squares fit to a function of theform A* ( 1 -exp( -B*x) ) .Computational Linguistics, Volume 12, Number 3, July-September 1986 211Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional PatternsIf our corpus had yielded a reasonably complete set ofpatterns, we would expect the growth curves to flattenout by the end (indicating that very few new patternswere being encountered in the text).
In our earlier studyof syntactic patterns in sublanguages, we had found justsuch an effect after 200-250 sentences.
Unfortunately -as is evident in the figures - this is not the case ihere evenafter 500 sentences; the slope of the curve has clearlydecreased, but it is by no means flat.A pessimistic reader might suggest at this point thatthe set of selectional patterns is not closed, and that thecurve will continue rising at a substantial rate until nearlyall possible patterns are present.
Our experience withsublanguage selection - and that of other linguists -suggests, however, that, to a first approximation, the setof patterns is closed and that, with a text sample severaltimes larger than the present one, the curve will flattenout.
In order to get a more quantitative stimate of thesize of corpus that will be needed, we can use the follow-ing crude model.
The successive patterns encountered inprocessing the sentences represent a random selection(with replacement) from a finite population (thecomplete set of patterns for the sublanguage).
We there-fore expect the growth curve to have the formY=A*(1 -exp( -Bs ) ) ,  where s is the number ofsentences processed, A is the size of the complete set ofpatterns, and B is a parameter elated to the rate ofgrowth of the set of patterns.
A least-squares fit of thisfunction to the growth curve yields the following values:for subject-verb-object patterns, A=180,  B=0.00264;for prepositional phrase patterns, A=276,  B=0.00162;for adjective-noun patterns, A=126,  B=0.00416.
Thefitted exponential curves are shown as dashed lines inFigures 1-3.
These results can be more meaningfullyviewed in terms of the size of the corpus we would needto get 90% complete patterns: for subject-verb-objectpatterns, about 900 sentences; for prepositional phrasepatterns, about 1400 sentences; for adjective-nounpatterns, about 550 sentences.5.2 PARSING TESTSThe primary objective of our discovery procedure is toproduce a set of selectional patterns that can be used inparsing further texts in the sublanguage; the ultimate testof the patterns we generate, therefore, is to use them inparsing new text and see how they affect the parsingrates.
The prospects for such a' test are clouded by theresults of the previous subsection, which showed that theset of patterns we had collected was still quite incom-plete.
Nonetheless we thought it worthwhile to proceedwith this second stage of evaluation.In order to avoid the substantial labor associated withprocessing a new text (entering the text, entering defi-nitions for the new words, etc.
), we proceeded as follows.We took two medical records from our corpus of 11records (about 20% of the corpus) and treated them as"new text".
We reran the programs for collecting selec-tional patterns, using only the remaining nine records(the "old text").
We then parsed the new text using theselectional patterns thus generated, and classified theresults for each sentence: good parse (first parse correct),bad parse (first parse incorrect), or no parse for thesentence.We compared these results with the results of parsingthe text using manually prepared selectional patterns.These patterns had been prepared by a computationallinguist, based on a study of various medical records(including five of the records in our current corpus),generalizing from observed patterns where it seemedreasonable.The results of the comparison are shown in Table 2.The rate of successful analyses was substantially lowerwith the automatically generated selectional patterns.This is not surprising iven our observations in the earliersection about the incompleteness of these patterns.Where a selectional pattern is missing, an analysis will beblocked, thus generally producing no parse for thesentence.good parsesbad parsesno parsesSelectional patterns generated:manually542527automatically432241Table2.
Parsing results for 106 "new" sentences,comparing manually and automatically gener-ated selectional patterns.The parsing rates shown here are relatively low (about50% good parses) when compared with the data ofTable 1 (about 75% success).
This reflects the fact thatthe six patient summaries in our corpus, including the twotreated here as new text, were analyzed "cold": wordswere added to the lexicon as needed, but otherwise noadjustments were made to the grammar or lexicon.
Thedocuments are unedited medical reports with manysentence fragments; they contain a substantial number ofsublanguage-specific constructs not previously encoun-tered in processing other types of reports, (for example,prepositions were sometimes omitted before body parts:Synovial thickening both wrists bilaterally.).
In addition,the experiments revealed a substantial number of errorsin the lexicon.
Of the 63 failures (bad or no parse) usingthe automatically generated patterns, 20 were due tosyntactic constructs not present in the grammar, 3 toother grammar or parser bugs, 11 to errors in the diction-ary, and 23 to missing selectional patterns; 4 sentencesgot a bad parse on the first parse and a good parse on thesecond parse; 2 more were considered unanalyzablesentence fragments.
The syntactic gaps and lexical errorsuniformly depress the success rates for these exper-iments, but we feel that the data is still valid for compar-ing different sets of selectional constraints.212 Computational Linguistics, Volume 12, Number 3, July-September 1986Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patterns5.3 RESTRICTION RELAXATIONThe incompleteness of semantic information is a seriousand general problem that transcends our particular workon discovery procedures.
As the domains with whichnatural language systems deal become more complex, itbecomes more difficult to acquire a complete set of selec-tional patterns.
Furthermore, in many sublanguage textsthere are passages that fall outside the sublanguage; forexample, in one medical record domain, there is amention of a vacation a patient ook, during which he gotsick.
These passages will not satisfy the selectionalconstraints of the sublanguage.In the manual preparation of selectional patterns,some small measures were taken to compensate for thisincompleteness.
In preparing the patterns, the linguistgeneralized from the patterns observed in the text,adding new patterns that seemed equally reasonablebased on a knowledge of the domain.
For certain verycommon prepositions (e.g., of) for which it would bedifficult to collect all the selectional patterns, selectionwas disabled.
Similarly the linguist chose to omit someverbs from the selectional patterns; in these cases,subject-verb-object selection was not applied.In the automatically generated patterns, no similarmeasures were taken.
This, combined with the limitedcorpus used to gather the patterns, accentuated the effectof the incompleteness of the patterns.
Because absenceof a co-occurrence pattern can be interpreted as eithernegative information (a particular pattern is not allowedin the sublanguage) or as incomplete information (thispattern has not yet been seen), any automatically gener-ated set of patterns will over-constrain the parsing.
Wetherefore sought some way of automatically compensat-ing for this incompleteness.The approach we chose to try was restriction relaxa-tion.
If no parse can be obtained satisfying all selectionalconstraints, the parser tries for an analysis that will satis-fy all but one of the selectional constraints.
3 In effect, theparser is willing to relax any one of the selectionalconstraints in order to get an analysis.
Such an approachhas been suggested before by several computationallinguists (for example, Weischedel and Sondheimer1983), although primarily to account for ungrammaticalinput rather than for incompleteness of semantic know-ledge.We originally applied this technique in connectionwith the manually generated selectional patterns.
Theseresults were not very encouraging: about 5% of thesentences in the sample that had previously gotten noparse now got a correct parse, but another 5% got a badparse.
This was not too surprising in retrospect; thevarious measures mentioned above to compensate for theincompleteness of the patterns resulted in a set of rela-tively "loose" constraints, and any further loosening(such as restriction relaxation) would let quite a few badparses through.Our results using this technique in connection with theautomatically generated patterns, which are tighter andless complete, have been more positive (although basedto date on an extremely small sample).
Within our two-record sample, there were 14 sentences that had previ-ously not gotten a parse and now got one with restrictionrelaxation.
Of these, 10 were correct and 4 were incor-rect.
The automatically generated patterns, whencoupled with the mechanism for restriction relaxation,did about as well as the manually generated patterns(Table 3).
Given the small text sample, and theacknowledged incompleteness of the set of patterns, wefound this somewhat encouraging.
Of course, theseexperiments are still too small to reach any definiteconclusions.good parsesbad parsesno parsesSelectional patterns generated:manually542527automatically532627Table3.
Parsing results for 106 "new" sentences,comparing manually and automatically gener-ated selectional patterns, and using restrictionrelaxation when parsing with automaticallygenerated patterns.6 WHY Is IT So HARD.
'?When it is first described, the discovery procedure -parse the text, extract certain syntactic structures, collectthe sublanguage class patterns - may seem quite simpleand straightforward.
It has, however, taken us severaliterations to achieve even the small success describedhere.
It is worthwhile to reflect briefly on why this is so.First, there are several sources of human error, each ofwhich contributes ome errors to the final set of patterns.There are errors of word classification, where the wrongsublanguage class is recorded in the lexicon.
There areerrors in weeding out bad parses: a small defect (e.g.,incorrect conjunction scope) is easily overlooked.
Final-ly, there are errors due to selecting the wrong subclassfor a homograph.
We have tried to cope with theseerrors by repeatedly reviewing the generated set ofsublanguage patterns, going back each time to find thesource of any unexpected patterns.
However, as our textsamples grow from thousands of words to tens of thou-sands (as they must to get a better set of patterns), moresystematic control will be needed to minimize sucherrors.Second, there are a number of linguistic phenomenathat complicate the extraction of the selectional patterns.Specifically, there are cases in which the sublanguageclass of a noun phrase cannot be determined from theclass of the head alone.
In some constructs of the formN1 preposition N2, the head N1 is "transparent", and theComputational Linguistics, Volume 12, Number 3, July-September 1986 213Ralph Grishman, Lynette Hirschman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional Patternsphrase has the class of (has the distribution of) N2.Examples are his tory  o f  .
.
.
.
increase  in .
.
.
.
In othercases, the class of the phrase depends on both the headand the modifier; thus throat  has the class BODYPARTbut sore  throat  the class SYMPTOM.
We have incorpo-rated the patterns and procedures for computing suchphrasal attributes for the medical domain into our selec-tional restrictions.
In moving to a new domain, we wouldhave to acquire new sets of phrasal attribute patterns aswell as selectional patterns.
To limit our current exper-iment, however, our procedure for generating selectionalpatterns used the phrasal attribute patterns that had beenpreviously developed manually.None of these difficulties pose insurmountable road-blocks to our goal.
Rather they point out that, as in anyexperiment where a large body of reliable data must becollected, the procedures may be complex and specialmeasures must be taken to assure accuracy.7 CONCLUSIONSOverall, the experiments we have conducted using ourdiscovery procedure are encouraging but not conclusive.The selectional patterns gathered from a limited textsample - when coupled with a procedure for restrictionrelaxation - do about as well as manually prepared selec-tional patterns.
Furthermore, the growth curves for theselectional patterns suggest hat a corpus several timeslarger would yield a more complete set of patterns andthus better performance in parsing.We have learned that such a procedure requiressubstantial human interaction and we intend, beforeadvancing to a larger corpus, to restructure the system tofacilitate this interaction.
The present system is basicallyorganized for batch processing; interaction takes placeby editing intermediate files.
Our next step will be tomove to an interactive environment that supports thefollowing capabilities:?
isolating parse ambiguities and homographs andprompting the user to choose the appropriatereading/meaning;?
displaying new selectional patterns the first time theyare encountered;?
supporting simultaneous inspection and manipulationof text, parse tree, and selectional patterns.In all of this interaction, however, the user is stillacting only as a monitor of the patterns generated.
Weare still faced with the difficult issue of how to bootstrapthe system into a new domain.
In the absence of selec-tional patterns, choosing the correct parse can become atedious and time-consuming procedure, requiring exten-sive interaction with both a domain expert and a linguist.It is clearly not a realistic method of building up a set ofpatterns ufficient for semi-automated processing ~3f thetype described above.Combining the text-based approach with elicitationprocedures offers a more practical method of acquiringan initial set of domain knowledge.
An expert couldprovide some initial word classes and a partial set ofrelationships, from which to generate selectional patterns.A sample of text would then provide additional examples,with the expert available to elaborate on further patterns.For example, a system being developed at BBN 4 uses ahierarchy of sublanguage classes; given a selectionalpattern, it asks the user to generalize it by replacingclasses with superclasses where possible.
This initial peri-od of intensive interaction with an expert would providea sufficient pattern base so that the text-driven toolswould become effective in filling in the knowledge base.Such an approach would offer the assurance of goodcoverage provided by a text-based system while requiringa smaller text sample than a purely text-based procedure.ACKNOWLEDGEMENTThis material is based upon work supported by theNational Science Foundation under grants MCS-80-02453,DCR-82-02373, and DCR-82-02397.REFERENCESBallard, B.; Lusth, J.; and Tinkham, N. 1984 LDC-I: A TransportableKnowledge-Based Natural Language Processor for Office Environ-ments.
ACM Transactions on Office Information Systems 2: 1-25.Chevalier, M. et al 1978 TAUM-METEO: Description du Syst~me.Groupe de recherche n traduction automatique, Universit6 deMontr6al.Grishman, R. and Hirschman, L. 1982 Natural Language InterfacesUsing Limited Semantic Information.
NSF Final Report, New YorkUniversity, New York, New York.Grishman, R.; Hirschman, L.; and Friedman, C. 1982 NaturalLanguage Interfaces using Limited Semantic Information.Proceedings of the 9th International Conference on ComputationalLinguistics.
Prague, Czechoslovakia: 89-94.Grishman, R.; Hirsehman, L.; and Friedman, C. 1983 IsolatingDomain Dependencies in Natural Language Interfaces.
Proceedingsof the Conference on Applied Natural Language Processing.
SantaMonica, California: 46-53.Grishman, R.; Nhan, N. T.; and Marsh, E. 1984 Tuning NaturalLanguage Grammars for New Domains.
Proceedings of the Confer-ence on Intelligent Systems and Machines.
Rochester, Minnesota:342-346.Grishman, R.; Nhan, N. T.; Marsh, E.; and Hirschman, L. 1984 Auto-mated Determination of Sublanguage Syntactic Usage.
Proceedingsof COLING84 (Tenth International Conference on ComputationalLinguistics).
Stanford, California: 96-100.Grosz, B.
1983 TEAM: A Transportable Natural-Language InterfaceSystem.
Proceedings of Conference on Applied Natural Language Proc-essing.
Santa Monica, California: 39-45.Harris, Z.
1968 Mathematical Structures of Language.
Wiley Intersci-ence, New York, New York.Hirschman, L.; Grishman, R.; and Sager, N. 1975 Grammatically-based Automatic Word Class Formation.
Information Processing andManagement 11 : 39-57.Hirschman, L.; Story, G.; Marsh, E.; Lyman, M.; and N. Sager.
1981An Experiment in Automated Health Care Evaluation of NarrativeMedical Records.
Computers and Biomedical Research 14: 447-463.Hirschman, L. and Sager, N. 1983 Automatic Information Formattingof a Medical Sublanguage.
In Kittredge and Lehrberger: 27-80.Kittredge, R. and Lehrberger, J., Eds.
1983 Sublanguage: Studies ofLanguage in Restricted Semantic Domains.
Series of Foundations ofCommunications, Walter de Gruyter, Berlin: 27-80.Lehrberger, J.
1983 Automatic Translation and the Concept ofSublanguage.
In Kittredge and Lehrberger.214 Computational Linguistics, Volume 12, Number 3, July-September 1986Ralph Grishman, Lynette Hirsehman, Ngo Thanh Nhan Discovery Procedures for Sublanguage Selectional PatternsMarsh, E. 1983 Utilizing Domain-Specific Information for ProcessingCompact Text,~ Proceedings of Conference on Applied NaturalLanguage Processing.
Santa Monica, California: 99-103.Marsh, E.; Hamburger, H.; and Grishman, R. 1984 A Production RuleSystem for Message Summarization.
Proceedings of the 1984National Conference on Artificial Intelligence.
Austin, Texas.Sager, N. 1981 Natural Language Information Processing: A ComputerGrammar of English and its Applications.
Addison-Wesley, Reading,Massachusetts.Sager, N. and Grishman, R. 1975 The Restriction Language forComputer Grammars of Natural Language.
Communications ACM18: 390-400.Weischedel, R. M. and Sondheimer, N.K .
1983 Meta-rules as a Basisfor Processing Ill-Formed Input.
Journal of Computational Linguis-tics 9: 161-177.NOTES1.
"Good parses" included some parses that were not entirely correctbut that were good enough so they did not cause errors in theprocess that converted the parsed trees into information formats(a structured ata base).2.
The curves are rather jagged because the reports are divided intosections containing different ypes of information; when we beginprocessing a new section, new patterns are encountered, and thereis therefore a sharp rise in the growth curves.3.
If no analysis can be obtained by relaxing one restriction, theparser is able to try for an analysis that relaxes two, three, or morerestrictions.
Our experiments have indicated, however, that relax-ing more than one restriction produced bad parses more often thangood ones.4.
Private communication with M. Bates.Computational Linguistics, Volume 12, Number 3, July-September 1986 2 | 5
