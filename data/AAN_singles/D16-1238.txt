Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204?2214,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsBi-directional Attention with Agreement for Dependency ParsingHao Cheng Hao FangUniversity of Washington{chenghao,hfang}@uw.eduXiaodong He Jianfeng Gao Li DengMicrosoft Research{xiaohe,jfgao,deng}@microsoft.comAbstractWe develop a novel bi-directional attentionmodel for dependency parsing, which learnsto agree on headword predictions from the for-ward and backward parsing directions.
Theparsing procedure for each direction is for-mulated as sequentially querying the memorycomponent that stores continuous headwordembeddings.
The proposed parser makes useof soft headword embeddings, allowing themodel to implicitly capture high-order pars-ing history without dramatically increasingthe computational complexity.
We conductexperiments on English, Chinese, and 12 otherlanguages from the CoNLL 2006 shared task,showing that the proposed model achievesstate-of-the-art unlabeled attachment scoreson 6 languages.11 IntroductionRecently, several neural network models have beendeveloped for efficiently accessing long-term mem-ory and discovering dependencies in sequential data.The memory network framework has been studiedin the context of question answering and languagemodeling (Weston et al, 2015; Sukhbaatar et al,2015), whereas the neural attention model underthe encoder-decoder framework has been appliedto machine translation (Bahdanau et al, 2015) andconstituency parsing (Vinyals et al, 2015b).
Bothframeworks learn the latent alignment between thesource and target sequences, and the mechanism of1Our software and models are available at https://github.com/hao-cheng/biattdp.attention over the encoder can be viewed as a softoperation on the memory.
Although already usedin the encoder for capturing global context informa-tion (Bahdanau et al, 2015), the bi-directional recur-rent neural network (RNN) has yet to be employedin the decoder.
Bi-directional decoding is expectedto be advantageous over the previously developeduni-directional counterpart, because the former ex-ploits richer contextual information.
Intuitively, wecan use two separate uni-directional RNNs whereeach one constructs its respective attended encodercontext vectors for computing RNN hidden states.However, the drawback of this approach is that thedecoder would often produce different alignmentsresulting in discrepancies for the forward and back-ward directions.
In this paper, we design a trainingobjective function to enforce attention agreementbetween both directions, inspired by the alignment-by-agreement idea from Liang et al (2006).
Specif-ically, we develop a dependency parser (BiAtt-DP)using a bi-directional attention model based on thememory network.
Given that the golden alignmentis observed for dependency parsing in the trainingstage, we further derive a simple and interpretableapproximation for the agreement objective, whichmakes a natural connection between the latent andobserved alignment cases.The proposed BiAtt-DP parses a sentence in alinear order via sequentially querying the memorycomponent that stores continuous embeddings forall headwords.
In other words, we consider all pos-sible arcs during the parsing.
This formulation isadopted by graph-based parsers such as the MST-Parser (McDonald et al, 2005).
The consideration2204of all possible arcs makes the proposed BiAtt-DPdifferent from many recently developed neural de-pendency parsers (Chen and Manning, 2014; Weisset al, 2015; Alberti et al, 2015; Dyer et al, 2015;Ballesteros et al, 2015), which use a transition-based algorithm by modeling the parsing procedureas a sequence of actions on buffers.
Moreover,unlike most graph-based parsers which may sufferfrom high computational complexity when utilizinghigh-order parsing history (McDonald and Pereira,2006), the proposed BiAtt-DP can implicitly injectsuch information into the model while keeping thecomputational complexity in the order of O(n2) fora sentence with n words.
This is achieved by feed-ing the RNN in the query component with a softheadword embedding, which is computed as theprobability-weighted sum of all headword embed-dings in the memory component.To the best of our knowledge, this is the first at-tempt to apply memory network models to graph-based dependency parsing.
Moreover, it is thefirst extension of neural attention models from uni-direction to multi-direction by enforcing agreementon alignments.
Experiments on English, Chinese,and 12 languages from the CoNLL 2006 shared taskshow the BiAtt-DP can achieve competitive parsingaccuracy with several state-of-the-art parsers.
Fur-thermore, our model achieves the highest unlabeledattachment score (UAS) on Chinese, Czech, Dutch,German, Spanish and Turkish.2 A MemNet-based Dependency ParserThe proposed parser first encodes each word ina sentence by continuous embeddings using a bi-directional RNN, and then performs two types ofoperations, i.e.
1) headword predictions based on bi-directional parsing history and 2) the relation pre-diction conditioned on the current modifier and itspredicted headword both in the embedding space.In the following, we first present how the token em-beddings are constructed.
Then, the key componentsof the proposed parser, i.e.
the memory componentand the query component, are discussed in detail.Lastly, we describe the parsing algorithm using a bi-directional attention model with agreement.2.1 Token EmbeddingsIn the proposed BiAtt-DP, the memory and querycomponents share the same token embeddings.
Weuse the notion of additive token embedding as in(Botha and Blunsom, 2014) to utilize the availableinformation about the token, e.g., its word form,lemma, part-of-speech (POS) tag, and morpholog-ical features.
Specifically, the token embedding iscomputed asEformeformi + Eposeposi + Elemmaelemmai + ?
?
?
,where ei?s are one-hot encoding vectors for the i-th word, and E?s are parameters to be learned thatstore the continuous embeddings for correspondingfeature.
Note those one-hot encoding vectors havedifferent dimensions, depending on individual vo-cabulary sizes, and all E?s have the same first di-mension but different second dimension.
The addi-tive token embeddings allow us to easily integrate avariety of information.
Moreover, we only need tomake a single decision on the dimensionality of thetoken embedding, rather than a combination of deci-sions on word embeddings and POS tag embeddingsas in concatenated token embeddings used by Chenand Manning (2014), Dyer et al (2015) and Weisset al (2015).
It reduces the number of model param-eters to be tuned, especially when lots of differentfeatures are used.
In our experiments, the word formand fine-grained POS tag are always used, whereasother features are used depending on their availabil-ity in the dataset.
All singleton words, lemmas, andPOS tags are replaced by special tokens.The additive token embeddings are transformedinto another space before they are used by the mem-ory and query components, i.e.xi = LReL[P(Eformeformi + ?
?
?
)],where P is the projection matrix and is shared bythe memory and query components as well.
The ac-tivation function of this projection layer is the leakyrectified linear (LReL) function (Mass et al, 2013)with 0.1 as the slope of the negative part.
In the re-maining part of the paper, we refer to xi ?
Rp as thetoken embedding for word at position i.
Note thesubscript i is substituted by j and t for the memoryand query components, respectively.22052.2 ComponentsAs shown in Figure 1, the proposed BiAtt-DP hasthree components, i.e.
a memory component, a left-to-right query component, and a right-to-left querycomponent.
Given a sentence of length n, the parserfirst uses a bi-directional RNN to construct n + 1headword embeddings, m0,m1, .
.
.
,mn ?
Re,with m0 reserved for the ROOT symbol.
Each querycomponent is an uni-directional attention model.
Ina query component, a sequence of n modifier em-beddings q1, .
.
.
,qn ?
Rd are constructed recur-sively by conditioning on all headword embeddings.To address the vanishing gradient issue in RNNs, weuse the gated recurrent unit (GRU) proposed by Choet al (2014), where an update gate and a reset gateare employed to control the information flow.
We re-place the hyperbolic tangent function in GRU withthe LReL function, which is faster to compute andachieves better parsing accuracy in our preliminarystudies.
In the following, we refer to headword andmodifier embeddings as memory and query vectors,respectively.Memory Component: The proposed BiAtt-DPuses a bi-directional RNN to obtain the memory vec-tors.
At time step j, the current hidden state vec-tor hlj ?
Re/2 (or hrj ?
Re/2) is computed as anon-linear transformation based on the current in-put vector xj and the previous hidden state vec-tor hlj?1 (or hrj+1), i.e.
hlj = GRU(hlj?1,xj) (orhrj = GRU(hrj+1,xj)).
Ideally, the recursive natureof the RNN allows it to capture all context infor-mation from one-side, and a bi-directional RNN canthus capture context information from both sides.We concatenate the hidden layers of the left-to-rightRNN and the right-to-left RNN for the word at posi-tion j as the memory vector mj =[hlj ;hrj].
Thesememory vectors are expected to encode the wordsand their context information in the headword space.Query Component: For each query component,we use a single-directional RNN with GRU to obtainthe query vectors qj?s, which are the hidden statevectors of the RNN.
Each qt is used to query thememory component, returning association scoresst,j?s between the word at position t and the head-Figure 1: The structure of the BiAtt-DP.
The figure only illus-trates the parsing process at the time step for has.
Blue andyellow circles are memory and query vectors, respectively.
Redand purple circles represent headword probabilities predictedfrom corresponding query components.
Green circles representsoft headword embeddings.
Black arrowed lines are connec-tions carrying weight matrices.
?
and ?
indicate element-wisemultiplication and addition, respectively.
For simplicity, we ig-nore the token embedding xt connected to the RNN hidden lay-ers mj , qlt and qrt .word at position j for j ?
{0, ?
?
?
, n}, i.e.st,j = vT?
(Cmj + Dqt) , (1)where ?(?)
is the element-wise hyperbolic tangentfunction, and C ?
Rh?e, D ?
Rh?d and v ?
Rhare model parameters.
Then, we can obtain proba-bilities (aka attention weights), at,0, ?
?
?
, at,n, overall headwords in the sentence by normalizing st,j?s,using a softmax functionat = softmax(st).
(2)The soft headword embedding is then defined asm?t =?nj=1 at,jmj .
At each time step t, the2206RNN takes the soft headword embedding m?lt?1 orm?rt+1 as the input, in addition to the token embed-ding xt.
Formally, for the forward case, the qt canbe computed as qt = GRU (qt?1, [m?t;xt]).
Al-though the RNN is able to capture long-span con-text information to some extent, the local contextmay very easily dominate the hidden state.
There-fore, this additional soft headword embedding al-lows the model to access long-span context infor-mation in a different channel.
On the other hand,by recursively feeding both the query vector and thesoft headword embedding into the RNN, the modelimplicitly captures high-order parsing history infor-mation, which can potentially improve the parsingaccuracy (Yamada and Matsumoto, 2003; McDon-ald and Pereira, 2006).
However, for a graph-baseddependency parser, utilizing parsing history featuresis computationally expensive.
For example, an k-thorder MSTParser (McDonald and Pereira, 2006) hasO(nk+1) complexity for a sentence of n words.
Incontrast, the BiAtt-DP implicitly captures high-orderparsing history while keeping the complexity in theorder of O(n2), i.e.
for each direction.
we computen(n+1) pair-wise probabilities at,j for t = 1, ?
?
?
, nand j = 0, ?
?
?
, n.In this paper, we choose to use soft headword em-beddings rather than making hard decisions on head-words.
In the latter case, beam search may poten-tially improve the parsing accuracy at the cost ofhigher computational complexity, i.e.
O(Bn2) witha beam width of B.
When using soft headword em-beddings, there is no need to perform beam search.Moreover, it is straightforward to incorporate pars-ing history from both directions by using two querycomponents at the cost of O(2n2), which cannot beeasily achieved when using beam search.
The pars-ing decision can be made directly based on atten-tion weights from the two query components or fur-ther rescored by the maximum spanning tree (MST)search algorithm.2.3 Parsing by Attention with AgreementFor the bi-directional attention model, the underly-ing probability distributions alt and art may not agreewith each other.
In order to encourage the agree-ment, we use the mathematically convenient metric,i.e.
the squared Hellinger distance H2 (alt||art), forquantifying the distance between these two distri-butions.
For dependency parsing, when the goldenalignment is known during training, we can derivean upper bound on the latent agreement objective asH2(alt,art ) ?
2?D(gt||alt) +D(gt||art ),where D(?||?)
is the KL-divergence.
The completederivation is provided in the Appendix A. Duringoptimization, we can safely drop the constant scalerand the square root operation in the upper bound,leading to the following loss functionD(gt||alt) +D(gt||art ) = 2D(gt||alt  art ), (3)where  indicates element-wise multiplication.
Theresulting loss function is equivalent to the cross-entropy loss, which is widely adopted for trainingneural networks.As we can see, the loss function (3) tries to min-imize the distance between the golden alignmentand the intersection of the two directional attentionalignments at every time step.
Therefore, duringinference, the headword prediction for the word attime step t can be obtained asargmaxjlog alt,j + log art,j ,seeking for agreement between both query compo-nents.
This parsing procedure is also similar tothe exhaustive left-to-right modifier-first search al-gorithm described in (Covington, 2001), but it is en-hanced by an additional right-to-left search with theagreement enforcement.
Alternatively, we can treat(log alt,j + log art,j) as a score of the correspondingarc and then search for the MST to form a depen-dency parse tree, as proposed in (McDonald et al,2005).
The MST search is achieved via the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Ed-monds, 1967), which can be implemented in O(n2)for dense graphs according to Tarjan (1977).
In prac-tice, the MST search slows down the parsing speedby 6?10%.
However, it forces the parser to producea valid tree, and we observe a slight improvement onparsing accuracy in most cases.After obtaining each modifier and its soft headerembeddings, we use a single-layer perceptron to pre-dict the head-modifier relation, i.e.yt = softmax(U[m?lt; m?rt]+ W[qlt; qrt]), (4)2207where yt,1, ?
?
?
, yt,m are the probabilities of m pos-sible relations, and U ?
Rm?2e and W ?
Rm?2dare model parameters.3 Model LearningFor the t-th word (modifier) wt in a sentence oflength n, let H lt and Hrt denote random variablesrepresenting the predicted headword from forward(left-to-right) and backward (right-to-left) parsingdirections, respectively.
Also let Rt denote the ran-dom variable representing the dependency relationfor wt.
The joint probability of headword and rela-tion predictions can be written asP (R1:n, H l1:n, Hr1:n|w1:n)=n?t=1P (Rt|w1:n)P (H lt |w1:n)P (Hrt |w1:n)=n?t=1ylt,Rt ?
alt,Hlt ?
art,Hrt (5)where at each time step we assume head-modifierrelations and headwords from both directions areindependent with each other when conditioned onthe global knowledge of the whole sentence.
Notethat the long-span context and high-order parsinghistory information are injected when we modelP (H lt |w1:n), P (Hrt |w1:n) and P (Rt|w1:n), as dis-cussed in Section 2.2.As discussed in Section 2.3, the model can betrained by encouraging attention agreement betweentwo query components.
From (5), we observe that itis equivalent to maximizing the log-likelihood of thegolden dependency tree (or minimizing the cross-entropy) for each training sentence, i.e.n?t=1(log yt,relationt + log alt,headt + log art,headt),where at,j and yt,r are defined in (2) and (4), re-spectively, and relationt and headt are goldenrelation and headword labels, respectively.
The gra-dients are computed via the back-propagation algo-rithm (Rumelhart et al, 1986).
Errors of yt comefrom the arc labels, whereas there are two sourceof errors for at, one from the headword labels andthe other back-propagated from errors of yt.
Weuse stochastic gradient descent with the Adam al-gorithm proposed in (Kingma and Ba, 2015).
Thelearning rate is halved at each iteration once the log-likelihood of the dev set decreases.
The whole train-ing procedure terminates when the log-likelihooddecreases for the second time.
All learning param-eters except bias terms are initialized randomly ac-cording to the Gaussian distribution N (0, 10?2).
Inour experiments, we tune the initial learning ratewith a step size of 0.0002, and choose the best onebased on the log-likelihood on the dev set at the firstepoch.
Empirically, the selected initial learning ratesfall in the range of [0.0004, 0.0010] for hidden layersize [128, 320], and tend to be larger when using asmaller hidden layer size, i.e.
[0.0016, 0.0034] forhidden layer size around 80.
The training data arerandomly shuffled at every epoch.4 ExperimentsIn this section, we present the parsing accuracy ofthe proposed BiAtt-DP on 14 languages.
We re-port both UAS and labeled attachment score (LAS),obtained by the CoNLL-X eval.pl script2 which ig-nores punctuation symbols.
The headword pre-dictions are made through the MST search, whichslightly improves both UAS and LAS (less than0.3% absolutely).
Overall, the proposed BiAtt-DPachieves competitive parsing accuracy on all lan-guages as state-of-the-art parsers, and obtains betterUAS in 6 languages.
We also show the impact ofusing POS tags and pre-trained word embeddings.Moreover, different variants of the full model arecompared in this section.4.1 DataWe work on the English Treebank-3 (PTB) dataset(Marcus et al, 1999), the Chinese Treebank-5.1(CTB) dataset (Palmer et al, 2005), and 12 otherlanguages from the CoNLL 2006 shared task (Buch-holz and Marsi, 2006).
For PTB and CTB datasets,we use exactly the same setup as in (Chen and Man-ning, 2014; Dyer et al, 2015).
Specifically, we con-vert the English and Chinese data using the Stan-ford parser v3.3.0 (de Marneffe et al, 2006) and thePenn2Malt tool (Zhang and Clark, 2008), respec-tively.For English, POS tags are obtained using theStanford POS tagger v3.3.0 (Toutanova et al, 2003),2http://ilk.uvt.nl/conll/software.html2208whereas for Chinese, we use gold segmentation andPOS tags.
When constructing the token embeddingsfor English and Chinese, both the word form and thePOS tag are used.
We also initialize Eform by pre-trained word embeddings3.For the 12 other languages, we randomly hold out5% of the training data as the dev set.
In additionto the word form and find-grained POS tags, we useextra features such as lemmas, coarse-grained POStags, and morphemes when they are available in thedataset.
No pre-trained word embeddings are usedfor these 12 languages.4.2 Model ConfigurationsThe hidden layer size is kept the same across allRNNs in the proposed BiAtt-DP.
We also require thedimension of the token embeddings to be the sameas the hidden layer size.
Note that we concatenatethe hidden layers of two RNNs for constructing mj ,and thus we have e = 2d.
The weight matrices Cand D respectively project vectors mj and qt to thesame dimension h, which is equivalent to d. ForEnglish and Chinese, since the dimension of pre-trained word embeddings are 300, we use 300 ?
has the dimension of embedding parameters E?s.
Forthe 12 other languages, we use square matrices forthe embedding parameters E?s.
For all languages,We tune the hidden layer size and choose one ac-cording to UAS on the dev set.
The selected hiddenlayer sizes for these languages are: 368 (English),114 (Chinese), 128 (Arabic), 160 (Bulgarian), 224(Czech), 176 (Danish), 220 (Dutch), 200 (German),128 (Japanese), 168 (Portuguese), 128 (Slovene),144 (Spanish), 176 (Swedish), and 128 (Turkish).4.3 ResultsWe first compare our parser with state-of-the-artneural transition-based dependency parsers on PTBand CTB.
For English, we also compare with state-of-the-art graph-based dependency parsers.
The re-sults are shown in Table 1 and Table 2, respectively.It can be seen that the BiAtt-DP outperforms allother graph-based parsers on PTB.
Compared with3For English, we use the dependency-based word embed-dings at https://goo.gl/tWke3I (Levy and Goldberg,2014).
For Chinese, we pre-train 192-dimension skip-gram em-beddings (Mikolov et al, 2013) on Chinese Gigawords (Graffet al, 2005).Type Method UAS LASTrans.C&M (2014) 91.8 89.6Dyer et al (2015) 93.2 90.9B&N (2012)?
93.33 91.22Alberti et al (2015)?
94.23 92.41Weiss et al (2015)?
94.26 92.41Andor et al (2016)?
94.41 92.55GraphBohnet (2010)?
92.88 90.71Martins et al (2013)?
92.89 90.55Z&M (2014)?
93.22 91.02BiAtt-DP 94.10 91.49Table 1: Parsing accuracy on PTB test set.
Our parser usesthe same POS tagger as C&M (2014) and Dyer et al (2015),whereas other parsers use a different POS tagger.
Results with?
and ?
are provided in (Alberti et al, 2015) and (Andor et al,2016), respectively.Dev TestUAS LAS UAS LASC&M (2014) 84.0 82.4 83.9 82.4Dyer et al (2015) 87.2 85.9 87.2 85.7BiAtt-DP 87.7 85.3 88.1 85.7Table 2: Parsing accuracy on CTB dev and test sets.the transition-based parsers, it achieves better accu-racy than Chen and Manning (2014), which uses afeed-forward neural network, and Dyer et al (2015),which uses three stack LSTM networks.
Comparedwith the integrated parsing and tagging models, theBiAtt-DP outperforms Bohnet and Nivre (2012) buthas a small gap to Alberti et al (2015).
On CTB,it achieves best UAS and similar LAS.
This maybe caused by that the relation vocabulary size isrelatively smaller than the average sentence length,which biases the joint objective to be more sensitiveto UAS.
The parsing speed is around 50?60 sents/secmeasured on a desktop with Intel Core i7 CPU @3.33GHz using single thread.Next, in Table 3 we show the parsing accuracyof the proposed BiAtt-DP on 12 languages in theCoNLL 2006 shared task, including comparisonwith state-of-the-art parsers.
Specifically, we showUAS of the 3rd-order RBGParser as reported in(Lei et al, 2014) since it also uses low-dimensionalcontinuous embeddings.
However, there are sev-eral major differences between the RBGParser andthe BiAtt-DP.
First, in (Lei et al, 2014), the low-dimensional continuous embeddings are derived2209Language BiAtt-DP RBGParser Best Published Crossed Uncrossed %CrossedArabic 80.34 [68.58] 79.95 81.12 (Ma11) 17.24 80.71 0.58Bulgarian 93.96 [89.55] 93.50 94.02 (Zh14) 79.59 94.10 0.98Czech 91.16 [85.14] 90.50 90.32 (Ma13) 81.62 91.63 4.68Danish 91.56 [85.53] 91.39 92.00 (Zh13) 73.33 91.89 1.80Dutch 87.15 [82.41] 86.41 86.19 (Ma13) 82.82 87.66 10.48German 92.71 [89.80] 91.97 92.41 (Ma13) 85.93 92.90 2.70Japanese 93.44 [90.67] 93.71 93.72 (Ma11) 48.67 94.48 2.26Portuguese 92.77 [88.44] 91.92 93.03 (Ko10) 73.02 93.28 2.52Slovene 86.01 [75.90] 86.24 86.95 (Ma11) 60.11 86.99 3.66Spanish 88.74 [84.03] 88.00 87.98 (Zh14) 50.00 88.77 0.08Swedish 90.50 [84.05] 91.00 91.85 (Zh14) 45.16 90.78 0.62Turkish 78.43 [66.16] 76.84 77.55 (Ko10) 38.85 79.71 3.13Table 3: UAS on 12 languages in the CoNLL 2006 shared task (Buchholz and Marsi, 2006).
We also report corresponding LASin squared brackets.
The results of the 3rd-order RBGParser are reported in (Lei et al, 2014).
Best published results on thesame dataset in terms of UAS among (Pitler and McDonald, 2015), (Zhang and McDonald, 2014), (Zhang et al, 2013), (Zhangand McDonald, 2012), (Rush and Petrov, 2012), (Martins et al, 2013), (Martins et al, 2010), and (Koo et al, 2010).
To study theeffectiveness of the parser in dealing with non-projectivity, we follow (Pitler and McDonald, 2015), to compute the recall of crossedand uncrossed arcs in the gold tree, as well as the percentage of crossed arcs.from low-rank tensors.
Second, the RBGParseruses combined scoring of arcs by including tradi-tional features from the MSTParser (McDonald andPereira, 2006) / TurboParser (Martins et al, 2013).Third, the RBGParser employs a third-order parsingalgorithm based on (Zhang et al, 2014), althoughit also implements a first-order parsing algorithm,which achieves lower UAS in general.
In Table 3,we show that the proposed BiAtt-DP outperformsthe RBGParser in most languages except Japanese,Slovene, and Swedish.It can be observed from Table 3 that the BiAtt-DP has highly competitive parsing accuracy as state-of-the-art parsers.
Moreover, it achieves best UASfor 5 out of 12 languages.
For the remaining sevenlanguages, the UAS gaps between the BiAtt-DPand state-of-the-art parsers are within 1.0%, exceptSwedish.
An arguably fair comparison for the BiAtt-DP is the MSTParser (McDonald and Pereira, 2006),since the BiAtt-DP replaces the scoring function forarcs but uses exactly the same search algorithm.
Dueto the space limit, we refer readers to (Lei et al,2014) for results of the MSTParsers (also shown inAppendix B).
The BiAtt-DP consistently outper-forms both parser by up to 5% absolute UAS score.Finally, following (Pitler and McDonald, 2015),we also analyze the performance of the BiAtt-DP onboth crossed and uncrossed arcs.
Since the BiAtt-DP uses a graph-based non-projective parsing algo-rithm, it is interesting to evaluate the performanceon crossed arcs, which result in the non-projectivityof the dependency tree.
The last three columns ofTable 3 show the recall of crossed arcs, that of un-crossed arcs, and the percentage of crossed arcs inthe test set.
Pitler and McDonald (2015) reportednumbers on the same data for Dutch, German, Por-tuguese, and Slovene as in this paper.
For these fourlanguages, the BiAtt-DP achieves better UAS thanthat reported in (Pitler and McDonald, 2015).
Moreimportantly, we observe that the improvement on re-call of crossed arcs (around 10?18% absolutely) ismuch more significant than that of uncrossed arcs(around 1?3% absolutely), which indicates the ef-fectiveness of the BiAtt-DP in parsing languageswith non-projective trees.4.4 Ablative StudyHere we try to study the impact of using pre-trainedword embeddings, POS tags, as well as the bi-directional query components on our model.
Firstof all, we start from our best model (Model 1 inTable 4) on English, which uses 300 as the tokenembedding dimension and 368 as the hidden layersize.
We keep those model parameter dimensionsunchanged and analyze different factors by compar-ing the parsing accuracy on PTB dev set.2210No.
INIT POS L2R R2L UAS LAS1 X X X X 93.99 91.322 X X X 93.36 90.423 X X 91.87 87.854 X X 92.64 89.665 X X 92.47 89.476 X X?
X?
93.03 90.06Table 4: Parsing accuracy on PTB dev set for different variantsof the full model.
INIT refers to using pre-trained word em-bddings to initialize Eform.
POS refers to using POS tags intoken embeddings.
L2R and R2L respectively indicate whetherto use the left-to-right and right-to-left query components.
?means the query component drops soft headword embeddingswhen constructing RNN hidden states.The results are summarized in Table 4.
Compar-ing Models 1?3, it can be observed that without us-ing pre-trained word embeddings, both UAS andLAS drop by 0.6%, and without using POS tagsin token embeddings, the numbers further drop by1.6% in UAS and around 2.6% in LAS.
In termsof query components, using single query compo-nent (Models 4?5) degrades UAS by 0.7?0.9% andLAS by around 1.0%, compared with Model 2.
ForModel 6, the soft headword embedding is only usedfor arc label predictions but not fed into the next hid-den state, which is around 0.3% worse than Model 2.This supports the hypothesis about the usefulness ofthe parsing history information.
We also implementa variant of Model 6 which produces one at insteadtwo by using both qlt and qrt in (1).
It gets 92.44%UAS and 89.26% LAS, indicating that naively ap-plying a bi-directional RNN may not be enough.5 Related WorkNeural Dependency Parsing: Recently de-veloped neural dependency parsers are mostlytransition-based models, which read words sequen-tially from a buffer into a stack and incrementallybuild a parse tree by predicting a sequence oftransitions (Yamada and Matsumoto, 2003; Nivre,2003; Nivre, 2004).
A feed-forward neural networkis used in (Chen and Manning, 2014), wherethey represent the current state with 18 selectedelements such as the top words on the stack andbuffer.
Each element is encoded by concatenatedembeddings of words, POS tags, and arc labels.Their dependency parser achieves improvementon both accuracy and parsing speed.
Weiss et al(2015) improve the parser using semi-supervisedstructured learning and unlabeled data.
The modelis extended to integrate parsing and tagging in(Alberti et al, 2015).
On the other hand, Dyer etal.
(2015) develop the stack LSTM architecture,which uses three LSTMs to respectively modelthe sequences of buffer states, stack states, andactions.
Unlike the transition-based formulation, theproposed BiAtt-DP directly predicts the headwordand the dependency relation at each time step.Specifically, there is no explicit representation ofactions or headwords in our model.
The modellearns to retrieve the most relevant information fromthe input memory to make decisions on headwordsand head-modifier relations.Graph-based Dependency Parsing: In additionto the transition-based parsers, another line of re-search in dependency parsing uses graph-basedmodels.
Graph-based parser usually build a de-pendency tree from a directed graph and learns toscoring the possible arcs.
Due to this nature, non-projective parsing can be done straightforwardly bymost graph-based dependency parsers.
The MST-Parser (McDonald et al, 2005) and the TurboParser(Martins et al, 2010) are two examples of graph-based parsers.
The MSTParser formulates the pars-ing as searching for the MST, whereas the Tur-boParser performs approximate variational infer-ence over a factor graph.
The RBGParser pro-posed in (Lei et al, 2014) can also be viewedas a graph-based parser, which scores arcs usinglow-dimensional continuous features derived fromlow-rank tensors as well as features used by MST-Parser/TurboParser.
It also employs a sampler-basedalgorithm for parsing (Zhang et al, 2014).Neural Attention Model: The proposed BiAtt-DP is closely related to the memory network(Sukhbaatar et al, 2015) for question answering,as well as the neural attention models for machinetranslation (Bahdanau et al, 2015) and constituencyparsing (Vinyals et al, 2015b).
The way we querythe memory component and obtain the soft head-word embeddings is essentially the attention mech-anism.
However, different from the above studieswhere the alignment information is latent, in de-pendency parsing, the arc between the modifier and2211headword is known during training.
Thus, we canutilize these labels for attention weights.
The similaridea is employed by the pointer network in (Vinyalset al, 2015a), which is used to solve three differentcombinatorial optimization problems.6 ConclusionIn this paper, we develop a bi-directional attentionmodel by encouraging agreement between the la-tent attention alignments.
Through a simple and in-terpretable approximation, we make the connectionbetween latent and observed alignments for train-ing the model.
We apply the bi-directional attentionmodel incorporating the agreement objective duringtraining to the proposed memory-network-based de-pendency parser.
The resulting parser is able to im-plicitly capture the high-order parsing history with-out suffering from issue of high computational com-plexity for graph-based dependency parsing.We have carried out empirical studies over 14languages.
The parsing accuracy of the proposedmodel is highly competitive with state-of-the-art de-pendency parsers.
For English, the proposed BiAtt-DP outperforms all graph-based parsers.
It alsoachieves state-of-the-art performance in 6 languagesin terms of UAS, demonstrating the effectiveness ofthe proposed mechanism of bi-directional attentionwith agreement and its use in dependency parsing.A Upper Bound on H2(p,q)Here, we use the following definition of squaredHellinger distance for countable spaceH2(p,q) = 12?i(?pi ?
?qi)2where p,q ?
?k are two k-simplexes.
Introducingg ?
?k, the squared Hellinger distance can be upperbounded asH2(p,q) ?
?2H(p,q) (6)?
?2 [H(p,g) +H(q,g)] (7)?
2?H2(p,g) +H2(q,g) (8)where (6), (7) and (8) follow the inequalities be-tween the `1-norm and the `2-norm, the triangleinequality defined for a metric, and the Cauchy-Schwarz?s inequality, respectively.
Using the rela-tionship between the KL-divergence and the squaredHellinger distance, (8) can be further bounded by2?D(g||p) +D(g||q).B UAS Scores of MSTParsersLanguage 1st-order 2nd-orderArabic 78.30 (2.02) 78.75 (1.57)Bulgarian 90.98 (3.00) 91.56 (2.42)Czech 86.18 (4.88) 87.30 (3.76)Danish 89.84 (1.80) 90.50 (1.14)Dutch 82.89 (4.54) 84.11 (3.32)German 89.54 (3.17) 90.14 (2.57)Japanese 93.38 (0.14) 92.92 (0.60)Portuguese 89.92 (3.17) 91.08 (2.01)Slovene 82.09 (4.54) 83.25 (3.38)Spanish 83.79 (4.59) 84.33 (4.05)Swedish 88.27 (1.95) 89.05 (1.17)Turkish 74.81 (3.74) 74.39 (4.16)Average 85.83 (2.85) 86.45 (2.23)Table 5: UAS scores of 1st-order and 2-nd order MSTParserson 12 languages in the CoNLL 2006 shared task (Buchholz andMarsi, 2006).
We use the numbers reported in (Lei et al, 2014).Numbers in brackets indicate the absolute improvement of theproposed BiAtt-DP over the MSTParsers.ReferencesChris Alberti, David Weiss, Slav Petrov, and Slav Petrov.2015.
Improved transition-based parsing and tag-ging with neural networks.
In Proc.
Conf.
EmpiricalMethods Natural Language Process.
(EMNLP), pages1354?1359.Daniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev, SlavPetrov, and Michael Collins.
2016.
Globally normal-ized transition-based neural networks.
In Proc.
Annu.Meeting Assoc.
for Computational Linguistics (ACL).Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proc.
Int.
Conf.Learning Representations (ICLR).Miguel Ballesteros, Chris Dyer, and Noah A. Smith.2015.
Improved transition-based parsing by model-ing characters instead of words with lstms.
In Proc.Conf.
Empirical Methods Natural Language Process.
(EMNLP), pages 349?359.2212Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Proc.Conf.
Empirical Methods Natural Language Process.
(EMNLP), pages 1455?1465.Bernd Bohnet.
2010.
Very high accurarcy and fast de-pendency parsing is not a contradiction.
In Proc.
Int.Conf.
Computational Linguistics (COLING), pages89?97.Jan A. Botha and Phil Blunsom.
2014.
Composi-tional morphology for word representations and lan-guage modelling.
In Proc.
Int.
Conf.
Machine Learn-ing (ICML).Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProc.
Conf.
Computational Natural Language Learn-ing (CoNLL), pages 149?164.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In Proc.
Conf.
Empirical Methods NaturalLanguage Process.
(EMNLP), pages 740?750.Kyunghyun Cho, Bart van Merrie?nboer, Caglar Gul-cehre, Dzmitry Bahadanau, Fethhi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learning phraserepresentations using RNN encoder-decoder for statis-tical machine translation.
In Proc.
Conf.
EmpiricalMethods Natural Language Process.
(EMNLP), pages1724?1734.Yoeng-Jin Chu and Tseng-Hong Liu.
1965.
On the short-est arborescene of a directed graph.
Science Sinica,14:1396?1400.Michael A. Covington.
2001.
A fundamental algorithmfor dependency parsing.
In Proc.
Annu.
ACM South-east Conf., pages 95?102.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProc.
Int.
Conf.
Language Resources and Evaluation(LREC).Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-termmemory.
In Proc.
Annu.
Meeting Assoc.
for Computa-tional Linguistics (ACL), pages 334?343.Jack Edmonds.
1967.
Optimum branchings.
Jour-nal of Research of the National Bureau of Standards,718(4):233?240.David Graff, Ke Chen, Junbo Kong, and KazuakiMaeda.
2005.
Chinese Gigaword Second EditionLDC2005T14.
Web Download.Diederik Kingma and Jimmy Ba.
2015.
Adam: Amethod for stochastic optimization.
In Proc.
Int.
Conf.Learning Representations (ICLR).Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decompo-sition for parsing with non-projective head automata.In Proc.
Conf.
Empirical Methods Natural LanguageProcess.
(EMNLP), pages 1288?1298.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proc.
Annu.
MeetingAssoc.
for Computational Linguistics (ACL), pages1381?1391.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proc.
Annu.
Meeting As-soc.
for Computational Linguistics (ACL), pages 302?308.Percy Liang, Ben Tasker, and Dan Klein.
2006.
Align-ment by agreement.
In Proc.
Human LanguageTechnology Conf.
and Conf.
North American ChapterAssoc.
for Computational Linguistics (HLT-NAACL),pages 104?111.Mitchell Marcus, Beatrice Santorini, Mary AnnMarcinkiewicz, and Ann Taylor.
1999.
Treebank-3LDC99T42.
Web Download.Andre` F. T. Martins, Noah A. Smith, and Eric P. Xing.2010.
Turbo parsers: Dependency parsing by approx-imate variational inference.
In Proc.
Conf.
EmpiricalMethods Natural Language Process.
(EMNLP), pages34?44.Andre` F. T. Martins, Miguel B. Almeida, and Noah A.Smith.
2013.
Turing on the turbo: Fast third-ordernon-projective turbo parsers.
In Proc.
Annu.
Meet-ing Assoc.
for Computational Linguistics (ACL), pages617?622.Andrew L. Mass, Awni Y. Hannun, and Andrew Y. Ng.2013.
Rectifier nonlinearities improve neural networkacoustic models.
In Proc.
Int.
Conf.
Machine Learning(ICML).Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Proc.
European Chapter Assoc.
for Com-putational Linguistics (EACL), pages 81?88.Ryan McDonald, Fernando Pererira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependency parsingusing spanning tree algorithms.
In Proc.
Human Lan-guage Technology Conf.
and Conf.
Empirical Meth-ods Natural Language Process.
(HLT/EMNLP), pages523?530.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
In Proc.
Workshop at Int.
Conf.Learning Representations.Joakim Nivre.
2003.
An efficient algorithm for projec-tive dependency parsing.
In Proc.
Int.
Conf.
ParsingTechnologies (IWPT), pages 149?160.2213Joakim Nivre.
2004.
Incrementality in deterministic de-pendency parsing: Bringing engineering and cognitiontogether.
In Proc.
Workshop at ACL.Martha Palmer, Fu-Dong Chiou, Nianwen Xue, andTsan-Kuang Lee.
2005.
Chinese Treebank 5.0LDC2005T01.
Web Download.Emily Pitler and Ryan McDonald.
2015.
A linear-timetranslation system for crossing interval trees.
In Proc.Conf.
North American Chapter Assoc.
for Computa-tional Linguistics (NAACL), pages 662?671.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning representations by back-propogating errors.
Nature, 323(6088):533?536, Oc-tober.Alexander M. Rush and Slav Petrov.
2012.
Vine prun-ing for efficient multi-pass dependency parsing.
InProc.
Conf.
North American Chapter Assoc.
for Com-putational Linguistics: Human Language Technolo-gies (NAACL-HLT), pages 498?507.Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, andRob Fergus.
2015.
End-to-end memory networks.In Proc.
Annu.
Conf.
Neural Inform.
Process.
Syst.
(NIPS), pages 2431?2439.Robert E. Tarjan.
1977.
Finding optimum branchings.Networks, 7(1):25?35.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proc.Human Language Technology Conf.
and Conf.
NorthAmerican Chapter Assoc.
for Computational Linguis-tics (HLT-NAACL), pages 173?180.Oriol Vinyals, Meire Fortunato, and Navdeep Jaitly.2015a.
Pointer networks.
In Proc.
Annu.
Conf.
NeuralInform.
Process.
Syst.
(NIPS), pages 2692?2700.Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015b.
Gram-mar as a foreign language.
In Proc.
Annu.
Conf.
Neu-ral Inform.
Process.
Syst.
(NIPS), pages 2755?2763.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proc.
Annu.
Meeting As-soc.
for Computational Linguistics (ACL), pages 323?333.Jason Weston, Sumit Chopra, and Antoine Bordes.
2015.Memory networks.
In Proc.
Int.
Conf.
Learning Rep-resentations (ICLR).Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machine.
InProc.
Int.
Conf.
Parsing Technologies (IWPT), pages195?206.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: investigating and combining graph-based andtransition-based depdency parsing using beam-search.In Proc.
Conf.
Empirical Methods Natural LanguageProcess.
(EMNLP), pages 562?571.Hao Zhang and Ryan McDonald.
2012.
Generalizedhigher-order dependency parsing with cube pruning.In Proc.
Conf.
Empirical Methods Natural LanguageProcess.
and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 320?331.Hao Zhang and Ryan McDonald.
2014.
Enforcing struc-tural diversity in cube-pruned dependency parsing.
InProc.
Annu.
Meeting Assoc.
for Computational Lin-guistics (ACL), pages 656?661.Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDon-ald.
2013.
Online learning for inexact hypergraphsearch.
In Proc.
Conf.
Empirical Methods NaturalLanguage Process.
(EMNLP), pages 908?913.Yuan Zhang, Tao Lei, Regina Barzilay, Tommi Jaakkola,and Amir Golberson.
2014.
Steps to excellence: Sim-ple inference with the refined scoring of dependencytrees.
In Proc.
Annu.
Meeting Assoc.
for Computa-tional Linguistics (ACL), pages 197?207.2214
