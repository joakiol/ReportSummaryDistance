Proceedings of the 6th Workshop on Statistical Machine Translation, pages 71?77,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsAMBER: A Modified BLEU, Enhanced Ranking MetricBoxing Chen and Roland KuhnNational Research Council of Canada, Gatineau, Qu?bec, CanadaFirst.Last@nrc.gc.caAbstractThis paper proposes a new automatic ma-chine translation evaluation metric:AMBER, which is based on the metricBLEU but incorporates recall, extra penal-ties, and some text processing variants.There is very little linguistic information inAMBER.
We evaluate its system-level cor-relation and sentence-level consistencyscores with human rankings from theWMT shared evaluation task; AMBERachieves state-of-the-art performance.1 IntroductionAutomatic evaluation metrics for machine transla-tion (MT) quality play a critical role in the devel-opment of statistical MT systems.
Several metricshave been proposed in recent years.
Metrics suchas BLEU (Papineni et al, 2002), NIST (Dodding-ton, 2002), WER, PER, and TER (Snover et al,2006) do not use any linguistic information - theyonly apply surface matching.
METEOR (Banerjeeand Lavie, 2005), METEOR-NEXT (Denkowskiand Lavie 2010), TER-Plus (Snover et al, 2009),MaxSim (Chan and Ng, 2008), and TESLA (Liu etal., 2010) exploit some limited linguistic resources,such as synonym dictionaries, part-of-speech tag-ging or paraphrasing tables.
More sophisticatedmetrics such as RTE (Pado et al, 2009) and DCU-LFG (He et al, 2010) use higher level syntactic orsemantic analysis to score translations.Though several of these metrics have shown bet-ter correlation with human judgment than BLEU,BLEU is still the de facto standard evaluation me-tric.
This is probably due to the following facts:1.
BLEU is language independent (except forword segmentation decisions).2.
BLEU can be computed quickly.
This is im-portant when choosing a metric to tune anMT system.3.
BLEU seems to be the best tuning metricfrom a quality point of view - i.e., modelstrained using BLEU obtain the highestscores from humans and even from othermetrics (Cer et al, 2010).When we developed our own metric, we decidedto make it a modified version of BLEU whoserankings of translations would (ideally) correlateeven more highly with human rankings.
Thus, ourmetric is called AMBER: ?A Modified Bleu, En-hanced Ranking?
metric.
Some of the AMBERvariants use an information source with a mild lin-guistic flavour ?
morphological knowledge aboutsuffixes, roots and prefixes ?
but otherwise, themetric is based entirely on surface comparisons.2 AMBERLike BLEU, AMBER is composed of two parts: ascore and a penalty.penaltyscoreAMBER ?=                 (1)To address weaknesses of BLEU described inthe literature (Callison-Burch et al, 2006; Lavieand Denkowski, 2009), we use more sophisticatedformulae to compute the score and penalty.2.1 Enhancing the scoreFirst, we enrich the score part with geometric av-erage of n-gram precisions (AvgP), F-measure de-rived from the arithmetic averages of precision andrecall (Fmean), and arithmetic average of F-measure of precision and recall for each n-gram(AvgF).
Let us define n-gram precision and recallas follows:71)(#)(#)(TngramsRTngramsnp ?=               (2))(#)(#)(RngramsRTngramsnr?=               (3)where T = translation, R = reference.Then the geometric average of n-gram preci-sions AvgP, which is also the score part of theBLEU metric, is defined as:NNnnpNAvgP11)()( ???????
?= ?=(4)The arithmetic averages for n-gram precisionand recall are:?==NnnpNNP1)(1)(                       (5)?==MnnrMMR1)(1)(                      (6)The F-measure that is derived from P(N) andR(M), (Fmean), is given by:)()1()()()(),,(MRNPMRNPMNFmean???
?+=     (7)The arithmetic average of F-measure of preci-sion and recall for each n-gram (AvgF) is given by:?=?+=Nn nrnpnrnpNNAvgF1 )()1()()()(1),(???
(8)The score is the weighted average of the threevalues: AvgP, Fmean, and AvgF.),()1(),,()()(2121??????NAvgFMNFmeanNAvgPNscore??
?+?+?=(9)The free parameters N, M,?
, 1?
and 2?
weremanually tuned on a dev set.2.2 Various penaltiesInstead of the original brevity penalty, we experi-mented with a product of various penalties:?==Piwiipenpenalty1(10)where wi is the weight of each penalty peni.Strict brevity penalty (SBP): (Chiang et al,2008) proposed this penalty.
Let ti be the transla-tion of input sentence i, and let ri be its reference(or if there is more than one, the reference whoselength in words || ir  is closest to length || it ).
Set????????
?= ?
?i iii irtrSBP |}||,min{|||1exp     (11)Strict redundancy penalty (SRP): long sen-tences are preferred by recall.
Since we rely onboth recall and precision to compute the score, it isnecessary to punish the sentences that are too long.????????
?= ?
?i ii iirrtSRP |||}||,max{|1exp      (12)Character-based strict brevity penalty(CSBP) and Character-based strict redundancypenalty (CSRP) are defined similarly.
The onlydifference with the above two penalties is thathere, length is measured in characters.Chunk penalty (CKP): the same penalty as inMETEOR:??
?????????
?= )(##1wordmatcheschunksCKP       (13)?
and ?
are free parameters.
We do not computethe word alignment between the translation andreference; therefore, the number of chunks is com-puted as )(#)(## wordmatchesbigrammatcheschunks ?= .For example, in the following two-sentence trans-lation (references not shown), let ?mi?
stand for amatched word, ?x?
stand for zero, one or moreunmatched words:S1: m1 m2 x m3 m4 m5 x m6S2: m7 x m8 m9 x m10 m11 m12 x m13If we consider only unigrams and bigrams, thereare 13 matched words and 6 matched bigrams (m1m2, m3 m4, m4 m5, m8 m9, m10 m11, m11 m12), so thereare 13-6=7 chunks (m1 m2, m3 m4 m5, m6, m7, m8 m9,m10 m11 m12, m13).Continuity penalty (CTP): if all matchedwords are continuous, thensegmentRTgramsnRTngrams#)()1(#)(#???
?equals 1.Example:S3: m1 m2 m3 m4 m5m6S4: m7 m8 m9m10 m11 m12 m13There are 13 matched unigrams, and 11 matchedbi-grams; we get 11/(13-2)=1.
Therefore, a conti-nuity penalty is computed as:72?????????????
?= ?=Nn segmentRTgramsnRTngramsNCTP2 #)()1(#)(#11exp (14)Short word difference penalty (SWDP): agood translation should have roughly the samenumber of stop words as the reference.
To makeAMBER more portable across all Indo-Europeanlanguages, we use short words (those with fewerthan 4 characters) to approximate the stop words.
))(#||exp(runigrambaSWDP ?
?=           (15)where a and b are the number of short words in thetranslation and reference respectively.Long word difference penalty (LWDP): is de-fined similarly to SWDP.
))(#||exp(runigramdcLWDP ?
?=           (15)where c and d  are the number of long words (thoselonger than 3 characters) in the translation and ref-erence respectively.Normalized Spearman?s correlation penalty(NSCP): we adopt this from (Isozaki et al, 2010).This penalty evaluates similarity in word order be-tween the translation and reference.
We first de-termine word correspondences between thetranslation and reference; then, we rank words bytheir position in the sentences.
Finally, we computeSpearman?s correlation between the ranks of the nwords common to the translation and reference.
)1()1(12?+?=?nnndi i?
(16)where di indicates the distance between the ranksof the i-th element.
For example:T: Bob reading book likesR: Bob likes reading bookThe rank vector of the reference is [1, 2, 3, 4],while the translation rank vector is [1, 3, 4, 2].
TheSpearman?s correlation score between these twovectors is)14(4)14()42()34()23(01222??
?+?+?+?+?=0.90.In order to avoid negative values, we normalizedthe correlation score, obtaining the penalty NSCP:2)1( /?NSCP +=                     (17)Normalized Kendall?s correlation penalty(NKCP):  this is adopted from (Birch and Os-borne, 2010) and (Isozaki et al, 2010).
In the pre-vious example, where the rank vector of thetranslation is [1, 3, 4, 2], there are 624 =C  pairs ofintegers.
There are 4 increasing pairs: (1,3), (1,4),(1,2) and (3,4).
Kendall?s correlation is defined by:1##2 ??=pairsallpairsasingincre?
(18)Therefore, Kendall?s correlation for the transla-tion ?Bob reading book likes?
is 16/42 ??
=0.33.Again, to avoid negative values, we normalizedthe coefficient score, obtaining the penalty NKCP:2)1( /NKCP ?+=                     (19)2.3 Term weightingThe original BLEU metric weights all n-gramsequally; however, different n-grams have differentamounts of information.
We experimented withapplying tf-idf to weight each n-gram according toits information value.2.4 Four matching strategiesIn the original BLEU metric, there is only onematching strategy: n-gram matching.
In AMBER,we provide four matching strategies (the bestAMBER variant used three of these):1.
N-gram matching: involved in computingprecision and recall.2.
Fixed-gap n-gram: the size of the gap be-tween words ?word1 [] word2?
is fixed;involved in computing precision only.3.
Flexible-gap n-gram:  the size of the gapbetween words ?word1 * word2?
is flexi-ble; involved in computing precision only.4.
Skip n-gram: as used ROUGE (Lin, 2004);involved in computing precision only.2.5 Input preprocessingThe AMBER score can be computed with differenttypes of preprocessing.
When using more than onetype, we computed the final score as an averageover runs, one run per type (our default AMBERvariant used three of the preprocessing types):?==TttAMBERTAMBERFinal1)(1_We provide 8 types of possible text input:0.
Original - true-cased and untokenized.731.
Normalized - tokenized and lower-cased.
(All variants 2-7 below also tokenized andlower-cased.)2.
?Stemmed?
- each word only keeps its first4 letters.3.
?Suffixed?
- each word only keeps its last4 letters.4.
Split type 1 - each longer-than-4-letterword is segmented into two sub-words,with one being the first 4 letters and theother the last 2 letters.
If the word has 5letters, the 4th letter appears twice: e.g.,?gangs?
becomes ?gang?
+ ?gs?.
If theword has more than 6 letters, the middlepart is thrown away5.
Split type 2 - each word is segmented intofixed-length (4-letter) sub-word sequences,starting from the left.6.
Split type 3 - each word is segmented intoprefix, root, and suffix.
The list of Englishprefixes, roots, and suffixes used to splitthe word is from the Internet1; it is used tosplit words from all languages.
Linguisticknowledge is applied here (but not in anyother aspect of AMBER).7.
Long words only - small words (those withfewer than 4 letters) are removed.3 Experiments3.1 Experimental dataWe evaluated AMBER on WMT data, using WMT2008 all-to-English submissions as the dev set.Test sets include WMT 2009 all-to-English, WMT2010 all-to-English and 2010 English-to-all sub-missions.
Table 1 summarizes the dev and test setstatistics.Set Dev Test1 Test2 Test3Year 2008 2009 2010 2010Lang.
xx-en xx-en xx-en en-xx#system 43 39 53 32#sent-pair 7,861 13,912 14,212 13,165Table 1: statistics of the dev and test sets.1http://en.wikipedia.org/wiki/List_of_Greek_and_Latin_roots_in_English3.2 Default settingsBefore evaluation, we manually tuned all free pa-rameters on the dev set to maximize the system-level correlation with human judgments and de-cided on the following default settings forAMBER:1.
The parameters in the formula),()1(),,()()(2121??????NAvgFMNFmeanNAvgPNscore??
?+?+?=are set as  N=4, M=1, ?
=0.9, 1?
= 0.3and 2?
= 0.5.2.
All penalties are applied; the manually setpenalty weights are shown in Table 2.3.
We took the average of runs over input texttypes 1, 4, and 6 (i.e.
normalized text,split type 1 and split type 3).4.
In Chunk penalty (CKP), 3=?
, and?
=0.1.5.
By default, tf-idf is not applied.6.
We used three matching strategies: n-gram,fixed-gap n-gram, and flexible-gap n-gram; they are equally weighted.Name of penalty Weight valueSBP 0.30SRP 0.10CSBP 0.15CSRP 0.05SWDP 0.10LWDP 0.20CKP 1.00CTP 0.80NSCP 0.50NKCP 2.00Table 2: Weight of each penalty3.3 Evaluation metricsWe used Spearman?s rank correlation coefficient tomeasure the correlation of AMBER with the hu-man judgments of translation at the system level.The human judgment score we used is based on the?Rank?
only, i.e., how often the translations of thesystem were rated as better than the translationsfrom other systems (Callison-Burch et al, 2008).Thus, AMBER and the other metrics were eva-luated on how well their rankings correlated with74the human ones.
For the sentence level, we useconsistency rate, i.e., how consistent the ranking ofsentence pairs is with the human judgments.3.4 ResultsAll test results shown in this section are averagedover all three tests described in 3.1.
First, we com-pare AMBER with two of the most widely usedmetrics: original IBM BLEU and METEOR v1.0.Table 3 gives the results; it shows both the versionof AMBER with basic preprocessing, AMBER(1)(with tokenization and lowercasing) and the defaultversion used as baseline for most of our experi-ments (AMBER(1,4,6)).
Both versions of AMBERperform better than BLEU and METEOR on bothsystem and sentence levels.MetricDev     3 tests average   ?
testsBLEU_ibm(baseline)syssent0.68            0.72               N/A0.37            0.40               N/AMETEORv1.0syssent0.80            0.80              +0.080.58            0.56              +0.17AMBER(1)(basic preproc.
)syssent0.83            0.83              +0.110.61            0.58              +0.19AMBER(1,4,6)(default)syssent0.84            0.86              +0.140.62            0.60              +0.20Table 3: Results of AMBER vs BLEU and METEORSecond, as shown in Table 4, we evaluated theimpact of different types of preprocessing, andsome combinations of preprocessing (we do onerun of evaluation for each type and average theresults).
From this table, we can see that splittingwords into sub-words improves both system- andsentence-level correlation.
Recall that input 6 pre-processing splits words according to a list of Eng-lish prefixes, roots, and suffixes: AMBER(4,6) isthe best variant.
Although test 3 results, for targetlanguages other than English, are not broken outseparately in this table,  they are as follows: input 1yielded 0.8345  system-level correlation and0.5848 sentence-level consistency, but input 6yielded 0.8766 (+0.04 gain) and 0.5990 (+0.01)respectively.
Thus, surprisingly, splitting non-English words up according to English morpholo-gy helps performance, perhaps because French,Spanish, German, and even Czech share someword roots with English.
However, as indicated bythe underlined results, if one wishes to avoid theuse of any linguistic information, AMBER(4) per-forms almost as well as AMBER(4,6).
The defaultsetting, AMBER(1,4,6), doesn?t perform quite aswell as AMBER(4,6) or AMBER(4), but is quitereasonable.Varying the preprocessing seems to have moreimpact than varying the other parameters we expe-rimented with.
In Table 5, ?none+tf-idf?
meanswe do one run without tf-idf and one run for ?tf-idfonly?, and then average the scores.
Here, applyingtf-idf seems to benefit performance slightly.InputDev     3 tests average     ?
tests0(baseline)syssent0.84            0.79                 N/A0.59            0.58                 N/A1 syssent0.83            0.83               +0.040.61            0.58               +0.002 syssent0.83            0.84               +0.050.61            0.59               +0.013 syssent0.83            0.84               +0.050.61            0.58               +0.004 syssent0.84            0.87               +0.080.62            0.60               +0.015 syssent0.82            0.86               +0.070.61            0.56               +0.016 syssent0.83            0.88               +0.090.62            0.60               +0.027 syssent0.34            0.56               -0.230.58            0.53               -0.051,4 syssent0.84            0.85               +0.070.62            0.60               +0.014,6 syssent0.83            0.88               +0.090.62            0.60               +0.021,4,6 syssent0.84            0.86               +0.070.62            0.60               +0.02Table 4: Varying AMBER preprocessing (bestlinguistic = bold, best non-ling.
= underline)tf-idfDev     3 tests average    ?
testsnone(baseline)syssent0.84             0.86                N/A0.62             0.60                N/Atf-idfonlysyssent0.81             0.88              +0.020.62             0.61              +0.01none+tf-idfsyssent0.82             0.87              +0.010.62             0.61              +0.01Table 5: Effect of tf-idf on AMBER(1,4,6)Table 6 shows what happens if you disable onepenalty at a time (leaving the weights of the otherpenalties at their original values).
The biggest sys-tem-level performance degradation occurs whenLWDP is dropped, so this seems to be the most75useful penalty.
On the other hand, dropping CKP,CSRP, and SRP may actually improve perfor-mance.
Firm conclusions would require retuning ofweights each time a penalty is dropped; this is fu-ture work.PenaltiesDev     3 tests average    ?
testsAll(baseline)syssent0.84            0.86               N/A0.62            0.60               N/A-SBP syssent0.82            0.84               -0.020.62            0.60               -0.00-SRP syssent0.83            0.88              +0.010.62            0.60              +0.00-CSBP syssent0.84            0.85               -0.010.62            0.60              +0.00-CSRP syssent0.83            0.87              +0.010.62            0.60               -0.00-SWDP syssent0.84            0.86               -0.000.62            0.60              +0.00-LWDP syssent0.83            0.83               -0.030.62            0.60               -0.00-CTP syssent0.82            0.84               -0.020.62            0.60               -0.00-CKP syssent0.83            0.87              +0.010.62            0.60               -0.00-NSCP syssent0.83            0.86               -0.000.62            0.60              +0.00-NKCP syssent0.82            0.85               -0.010.62            0.60              +0.00Table 6: Dropping penalties from AMBER(1,4,6) ?biggest drops on test in boldMatchingDev     3 tests avg     ?
testsn-gram + fxd-gap+ flx-gap(default)syssent0.84             0.86         N/A0.62             0.60         N/An-gram syssent0.84             0.86         -0.000.62             0.60         -0.00fxd-gap+n-gramsyssent0.84             0.86         -0.000.62             0.60         -0.00flx-gap+n-gramsyssent0.83             0.86         -0.000.62             0.60         -0.00skip+n-gramsyssent0.83             0.85         -0.010.62             0.60         -0.00All fourmatchingssyssent0.83             0.86         -0.010.62             0.60          0.00Table 7: Varying matching strategy for AMBER(1,4,6)Finally, we evaluated the effect of the matchingstrategy.
According to the results shown in Table7, our default strategy, which uses three of the fourtypes of matching (n-grams, fixed-gap n-grams,and flexible-gap n-grams) is close to optimal;  theuse of skip n-grams (either by itself or in combina-tion) may hurt performance at both system andsentence levels.4 ConclusionThis paper describes AMBER, a new machinetranslation metric that is a modification of thewidely used BLEU metric.
We used more sophisti-cated formulae to compute the score, we developedseveral new penalties to match the human judg-ment, we tried different preprocessing types, wetried tf-idf, and we tried four n-gram matchingstrategies.
The choice of preprocessing typeseemed to have the biggest impact on performance.AMBER(4,6) had the best performance of any va-riant we tried.
However, it has the disadvantage ofusing some light linguistic knowledge about Eng-lish morphology (which, oddly, seems to be help-ful for other languages too).
A purist may preferAMBER(1,4) or AMBER(4), which use no linguis-tic information and still match human judgmentmuch more closely than either BLEU orMETEOR.
These variants of AMBER shareBLEU?s virtues: they are language-independentand can be computed quickly.Of course, AMBER could incorporate more lin-guistic information: e.g., we could use linguistical-ly defined stop word lists in the SWDP and LWDPpenalties, or use synonyms or paraphrasing in then-gram matching.AMBER can be thought of as a weighted com-bination of dozens of computationally cheap fea-tures based on word surface forms for evaluatingMT quality.
This paper has shown that combiningsuch features can be a very effective strategy forattaining better correlation with human judgment.Here, the weights on the features were manuallytuned; in future work, we plan to learn weights onfeatures automatically.
We also plan to redesignAMBER so that it becomes a metric that is highlysuitable for tuning SMT systems.ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An auto-matic metric for MT evaluation with improved corre-lation with human judgments.
In Proceedings of ACLWorkshop on Intrinsic & Extrinsic Evaluation Meas-ures for Machine Translation and/or Summarization.76A.
Birch and M. Osborne.
2010.
LRscore for evaluatinglexical and reordering quality in MT.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 302?307.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz andJ.
Schroeder.
2008.
Further Meta-Evaluation of Ma-chine Translation.
In Proceedings of WMT.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.Re-evaluating the role of BLEU in machine transla-tion research.
In Proceedings of EACL.D.
Cer, D. Jurafsky and C. Manning.
2010.
The BestLexical Metric for Phrase-Based Statistical MT Sys-tem Optimization.
In Proceedings of NAACL.Y.
S. Chan and H. T. Ng.
2008.
MAXSIM: A maximumsimilarity metric for machine translation evaluation.In Proceedings of ACL.D.
Chiang, S. DeNeefe, Y. S. Chan, and H. T. Ng.
2008.Decomposability of translation metrics for improvedevaluation and efficient algorithms.
In Proceedingsof EMNLP, pages 610?619.M.
Denkowski and A. Lavie.
2010.
Meteor-next and themeteor paraphrase tables: Improved evaluation sup-port for five target languages.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Transla-tion and MetricsMATR, pages 314?317.George Doddington.
2002.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proceedings of HLT.Y.
He, J.
Du, A.
Way, and J. van Genabith.
2010.
TheDCU dependency-based metric in WMT-MetricsMATR 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 324?328.H.
Isozaki, T. Hirao, K. Duh, K. Sudoh, H. Tsukada.2010.
Automatic Evaluation of Translation Qualityfor Distant Language Pairs.
In Proceedings ofEMNLP.A.
Lavie and M. J. Denkowski.
2009.
The METEORmetric for automatic evaluation of machine transla-tion.
Machine Translation, 23.C.-Y.
Lin.
2004.
ROUGE: a Package for AutomaticEvaluation of Summaries.
In Proceedings of theWorkshop on Text Summarization Branches Out(WAS 2004), Barcelona, Spain.C.
Liu, D. Dahlmeier, and H. T. Ng.
2010.
Tesla: Trans-lation evaluation of sentences with linear-programming-based analysis.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Transla-tion and MetricsMATR, pages 329?334.S.
Pado, M. Galley, D. Jurafsky, and C.D.
Manning.2009.
Robust machine translation evaluation with en-tailment features.
In Proceedings of ACL-IJCNLP.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceedings of ACL.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.Makhoul.
2006.
A Study of Translation Edit Ratewith Targeted Human Annotation.
In Proceedings ofAssociation for Machine Translation in the Americas.M.
Snover, N. Madnani, B. Dorr, and R. Schwartz.2009.
Fluency, Adequacy, or HTER?
Exploring Dif-ferent Human Judgments with a Tunable MT Metric.In Proceedings of the Fourth Workshop on StatisticalMachine Translation, Athens, Greece.77
