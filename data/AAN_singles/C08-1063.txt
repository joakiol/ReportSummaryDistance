Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 497?504Manchester, August 2008Understanding and Summarizing Answers in Community-BasedQuestion Answering ServicesYuanjie Liu1, Shasha Li2, Yunbo Cao1,3, Chin-Yew Lin3, Dingyi Han1, Yong Yu11Shanghai Jiao Tong University,Shanghai, China, 200240{lyjgeorge,handy,yyu}@apex.sjtu.edu.cn2National University ofDefense Technology,Changsha, China, 410074Shashali@nudt.edu.cn3Microsoft Research Asia,Beijing, China, 100080{yunbo.cao,cyl}@microsoft.comAbstractCommunity-based question answering(cQA) services have accumulated millionsof questions and their answers over time.In the process of accumulation, cQA ser-vices assume that questions always haveunique best answers.
However, with an in-depth analysis of questions and answerson cQA services, we find that the assump-tion cannot be true.
According to the anal-ysis, at least 78% of the cQA best answersare reusable when similar questions areasked again, but no more than 48% ofthem are indeed the unique best answers.We conduct the analysis by proposingtaxonomies for cQA questions and an-swers.
To better reuse the cQA content,we also propose applying automatic sum-marization techniques to summarize an-swers.
Our results show that question-typeoriented summarization techniques canimprove cQA answer quality significantly.1 IntroductionCommunity-based question and answering (cQA)service is becoming a popular type of search re-lated activity.
Major search engines around theworld have rolled out their own versions of cQAservice.
Yahoo!
Answers, Baidu Zhidao, andNaver Ji-Sik-In1 are some examples.In general, a cQA service has the followingworkflow.
First, a question is posted by the askerin a cQA service and then people in the commu-nity can answer the question.
After enough num-ber of answers are collected, a best answer can?
2008.
Licensed under the Creative Commons Attri-bution- Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.be chosen by the asker or voted by the communi-ty.
The resulting question and answer archivesare large knowledge repositories and can be usedto complement online search.
For example, Nav-er?s Ji-Sik-In (Knowledge iN) has accumulatedabout 70 million entries2.In an ideal scenario, a search engine can servesimilar questions or use best answers as searchresult snippets when similar queries are submit-ted.
To support such applications, we have toassume the best answers from cQA services aregood and relevant answers for their pairing ques-tions.
However, the assumption might not be trueas exemplified by the following examples.Question TitleWhich actress has the most seductivevoice?..could range from a giggly goldiehawn..to a sultry anne bancroft?QuestionDescriptionor any other type of voice that you find allur-ing.
..Best Answer(Polls & Surveys) Fenella Fielding, wow!!!
!Best Answer(Movies) i think joanna lumlley has a really sexy voiceTable 1.
Same Question / Different Best AnswersQuestion Title Does anyone know of any birthdays coming up soon?QuestionDescriptionCelerities, people you know, you?
Anyway Ineed the name and the date.
If you want toknow it is for mysite,  http://www.jessicaparke2.piczo.com...and that is not site advertising.AnswerNovembers Are:Paul Dickov nov 1stNelly (not furtado) nov 2nd ?Best Answer Check imdb.com, they have this celebrity birthdays listed.Table 2.
Question with Alternative AnswersTable 1 presents a question asking communi-ty opinions about ?who is the actress has themost seductive voice?.
The asker posted the samequestion twice at different Yahoo!
Answers cate-gories: one in Polls & Surveys and one in Movies.1 Yahoo!
Answers: answers.yahoo.com; Baidu Zhidao: zhi-dao.baidu.com; Naver Ji-Sik-In: kin.naver.com2www.iht.com/articles/2007/07/04/technology/naver.php497Two different best answers were chosen by thesame asker due to non-overlapping of answers.Table 2 shows another example, it asks about?the coming birthdays of stars?.
The best answerchosen by the asker is very good because it pro-vides useful URL information where the askercan find her answers.
However, other answerslisted a variety of birthdays of stars that alsoanswered the question.
These two examples indi-cate that the conventional cQA policy of allow-ing askers or voters to choose best answers mightbe working fine with the purpose of cQA but itmight not be a good one if we want to reuse thesebest answers without any post-processing.To find out what might be the alternatives tothe best answers, we first carried out an in-depthanalysis of cQA data by developing taxonomiesfor questions and answers.
Then we proposesummarizing answers in a consideration of ques-tion type, as the alternative to the best answers.For example, for the ?actress voice?
question, asummary of different people?s opinions rankedby popularity might be a better way for express-ing the question?s answers.
Similar to the ?ac-tress voice?
question, the ?celebrity birthday?question does not have a fix set of answers but isdifferent from the ?actress voice?
question that itsanswers are facts not opinions.
For fact-basedopen ended questions, combining different an-swers will be useful for reuse of those answers.The rest of this paper is arranged as follows.We review related work in Section 2.
We devel-op a framework for answer type taxonomy inSection 3 and a cQA question taxonomy in Sec-tion 4.
Section 5 presents methods to summarizecQA answers.
Finally, we conclude this paperand discuss future work in Section 6.2 Related WorkPrevious research on cQA (community-basedQuestion and Answering) domain focused onthree major areas: (1) how to find similar ques-tions given a new question (Jeon et al 2005a;Jeon et al, 2005b), (2) how to find experts givena community network(Liu et al, 2005; Jurczyk &Agichtein, 2007), and (3) how to measure answerquality and its effect on question retrieval.
Thethird area of focus is the most relevant to our re-search.
Jeon et al (2006)?s work on assessingcQA answer quality is one typical example.
Theyfound that about 1/3 of the answers among the1,700 Q&A pairs from Naver.com cQA datahave quality problems and approximately 1/10 ofthem have bad answers 3 .
They used 13 non-textual features and trained a maximum entropymodel to predict answer quality.
They showedthat retrieval relevance was significantly im-proved when answer quality measure was inte-grated in a log likelihood retrieval model.As mentioned in Section 1, cQA servicesprovide an alternative way for users to find in-formation online.
Questions posted on cQA sitesshould reflect users?
needs as queries submittedto search engines do.
Broder (2002) proposedthat search queries can be classified into threecategories, i.e.
navigational, informational, andtransactional.
Ross and Levinson (2004) sug-gested a more elaborated taxonomy with fivemore subcategories for informational queries andfour more subcategories for resource (transac-tional) queries.
In open-domain question answer-ing research that automatic systems are requiredto extract exact answers from a text databasegiven a set of factoid questions (Voorhees and M.Ellen, 2003), all top performing systems had in-corporated question taxonomies (Hovy et al,2001; Moldovan et al, 2000; Lytinen et al, 2002;Jijkoun et al, 2005).
Based on the past expe-riences from the annual NIST TREC Questionand Answering Track 4  (TREC QA Track), aninternational forum dedicating to evaluate andcompare different open-domain question answer-ing systems, we conjecture that a cQA questiontaxonomy would help us determine what type ofbest answer is expected given a question type.Automatic summarization of cQA answers isone of the main focuses of this paper.
We pro-pose that summarization techniques (Hovy andLin, 1999; Lin and Hovy, 2002) can be used tocreate cQA answer summaries for different ques-tion types.
Creating an answer summary given aquestion and its answers can be seen as a multi-document summarization task.
We simply re-place documents with answers and apply thesetechniques to generate the answer summary.
Thetask has been one of the main tasks the Docu-ment Understanding Conference5 since 2004.3 A Framework  for Answer TypeTo study how to exploit the best answers of cQA,we need to first analyze cQA answers.
We wouldlike to know whether the existing best answer ofa specific question is good for reuse.
If not, we3 Answers in Jeon el al.
?s work were rated in three levels:good, medium, and bad.4 http://trec.nist.gov/data/qamain.html5 http://duc.nist.gov498want toare.
Weby cQAferentiattomaticaWe mswers fofor answal categexaminithe 4 mories (100tainmen(S&C), Hwe devbased onterminescan be rilar to thOne oercise anThe taxcussionsnotatorscategoryannotatoon a sinmade ttaxonomdiscussanswer tFigurFiguronomy.Reusablmeans thsimilarwhile areused.Factualthat canjective Bas the beFUniqueunderstand wwill refer toaskers or voe it with belly generateade use ofr developiner type.
Theories in Yahng 400 randost popularquestions ft & Musicealth, andeloped a cQthe principa BA?s anseused or note BA?s quesf the authord developedonomy wasamong theto do the alabel thatrs.
If none ogle categorhe final dy is describthe questionaxonomy ine 1. cQA Sere 1 shows thIt first divide and Notat it can bequestion toNot ReusabThe Reusabland Subjectbe used as tA is one ofst answer.ReusableactualNot?UniqueDirect IndireSuhy and whathe ?best aters as BA hst answersd in our expequestions frg and testingre are overoo!
Answemly selectetop Yahoo!rom each ca(E&M), SoComputers &A answerle of BA reuwer type baswhen a quetion is askeds carried outhe initial athen modifauthors.
Wennotation.
Wwas agreedf the three ay label, oneecision.
Thed in thistype and tnext sectionvices BA Tye resultinges BA intoReusable.
Areused as theits questionle BA meae BA is furive.
A Facthe best answthe opinionsBest?Answerctbjective Ret the alternanswers?
seleenceforth toannotated orriments.om Yahoo!our framew1,000 hierarrs.
By manud questionsAnswers cattegory) ?
Eciety & CuInternet (Ctype taxonsability thaed on ?if thstion that isagain?.t this manuanswer taxonied throughasked threee assignedby at leastnnotators agof the aute answersection andhe relation.pe Taxonomanswer typetwo categoReusablebest answeis asked ans it cannother dividedual BA is aer; while athat can beNot?
?Reusablelevant Irretivescteddif-au-An-orkchic-allyfromego-nter-lture&I),omyt de-e BAsim-l ex-omy.dis-an-thetworeedhorstypewewithy.tax-ries:BAr if again;t beintofactSub-usedTUniqa unanswUniqThetypeits qswerple,IndiwhilbirthAfor oquesEachTvantusedlevaaskeNick?I'mly Soanswer?squesbestits qtiongiveanswemaTo bAnswUniquDirecIndireFactuSubjeReusaRelevIrrelevNot RTtypemorriestwoAmos(50%the oor vanswlevanthe Factualue and Notique best aner add mue BA hasNot Uniques: Direct anuestion dires its questiothe questionrect BA whe there is alday lists.Subjectivepinions or rtion asked ?answerer whe Not Reusand Irrelevas a best annt to its qud ?Why wasLachey sonot sure whuth Jersey,er is relevalocation whtion; an Irreanswer to iuestion.
Thperiod hasn that meetser?.?
of thil without shox?
is in thier Typeetctal Totalctiveble Totalantanteusable TotalTable 3.
Dable 3 shows on four cae than 48%.tend to havcategories.mong the fotly not uniq) of subjecne BA per coters is not ger.
HoweveBA typeUnique.
Aswer to its qore informaother alterBA type is dd Indirect.
Actly; whilen through imentionedich gives aso a Direct aBA answersecommendaWhich is thould have hable BA hasant.
A Relevswer to itsestion, for"I Can't Hashortlived?
?ere you livethat song want but withoich does nlevant BA cts question ae BA ?It apexpired.
Ifyour needse question ?owing the ems case.C&I47%28%9%84%4%88%3%9%12%istribution os the disttegories.
UnThe C&I ane more facur categorieue and havtive answersQA questioood enoughr, we mighthas two sUnique BAuestion andtion; whilenative bestivided intoDirect BAan Indirectnference.
Foin section 1website renswer just gquestions ttions.
For exe best sci-fiis own idea.two subtypant BA couquestion buexample, ate You AnymA Relevant, but in NJ,s played ouut knowingot really anould not be und it is irrepears that tan answer h, please pichow to forail addressE&M Heal28% 487% 303% 538% 83%40% 778% 90%1% 121% 922% 10%f Answer Tyribution ofique answerd the Healthtual BAs ths, S&C anse a high pe.
This indicn chosen byfor reuse asbe able to aubtypes:has onlyno othera Notanswers.two sub-answersBA an-r exam-has theference,ives thehat lookample, amovie?
?es: Rele-ld not bet it is re-questionore" byBA saidespecial-t?
?, thisthe ask-swer thesed as alevant tohe ques-as beenk a ?bestward anes in theth S&C% 13%% 18%% 2%33%% 50%83%% 0%% 17%17%peAnswers are nocatego-an otherwers arercentageates thatits askerthe bestpply au-499tomaticsummar(but notible soluETTableover whon a sinthe quesstable (o4 A CAs we wmy, wethemselvwell.
Asquestionbest answRose anengine qtheir taxengine qwe folloonomy amodate tFiFigurmy.
Weand propInformailar as inry consian answwith peoNavigseekingwould liknow theTranstend to gcomputeNavigatsummarizaized answersunique) answtions in SectCategoryComputer & Inntertainment &HealthSociety & Culable 4.
Disag4 shows tich none ofgle categorytion taxonomver at least 7QA Questere developoften couldes and hadwe discusswould helper types.d Levinson?ueries hasonomy wasueries.
Instewed the band made sohe particulagure 2.
Quee 2 shows thretain Brodose a new Stional and TrBroder?s tasts of questier but just wple in cQAational caURLs of spke to visit,fan sites ofactional caet resourcesr program thional InformConstantOpiniontion technifor at leasters.
We proion 5.ternetMusicturereement onhe percentathe three alabel.
The ry develope9% questionion Taxoning our answnot solelyto consider ted in Sectious determs (2004) taxsimilar goaldeveloped tad of startisic hierarchyme modificr of cQA serstion Type Te resultinger?s taxonoocial categoansactionalxonomy whions that doere used toservices.tegory conecific websitfor exampleHannah Motegory con.
A typical oat lets you ccQAQuestionationalDynamicContext?DependentTransques to chalf of reusvide some pPercenta18%17%21%20%Answer Typge of quesnnotators agesults showd above is ps).omyer type taxrely on ansheir questionn 2, the typine the expeonomy of seto ours tho classify seng from scrof R&L?sations to accvices.axonomyquestion taxmy at top lery.
Navigatiare definedle Social catnot intend toelicit interatains queses that the a, ?Does anyntana?
?tains quesne is ?Is thereate a planOpenactionalreateableoss-geetionsreedthatrettyono-werss ase ofctedarchougharchatch,tax-om-ono-velsonal,sim-ego-getctiontionsskerbodytionsre aet?
?Fto tConanswdichportbetwtaxoR&LqueslatiowouFtegoOpinQuepeopthinkjectsple.tiondiffe?WhdiffeOpesomhaveselvtioncomfollocludcontTservto gjoketiallyor olazytogebecogoogwilltheya newhoTquescateonlyquesoccusincsearSocialor Informatiwo subcategstant questioers while dotomy of inour intentioeen the quenomy.
Cons?s closed qtion is ?Whin??
but ?Wld be a dynaor Dynamicries: Opinioion questiostions in thle in cQAof some p. ?Is MicrosContext-deps having difrent contexat is the poprent answern categorye facts or ma variety oes may have?Can you liing week?
?ws R&L?ses what is next-dependehe new Socices.
Questioet an answers and expre, askers treanline forumspeople comther with thme a hackerle search?continue tocan give upgative sentimasked how table 5 showtion typesgories.
Weoccupy 1tions are evr in the same people vech engineson category,ories: Conns have a fynamic queformationaln to establistion taxontant questiouery type.
Ach country hhat is the pomic questioncategory, wn, Context-Dns are thoseis categorycommunitieople, someoft Vista woendent quesferent answt.
For exaulation of Cs accordingcontains qethods.
Thf answers ounconstrainst some birtis an examopen queryot coverednt categoriesial categoryns in this ca.
These quessing askerst cQA servi.
The questie on here sie question?
It really ishopefully soask, will clifaster?
?
aent towardo become as the distron 4 differeobserve tha1% ~ 20%en fewer suple questionry likely wto discoverwe first divstant and Dixed or stabstions do ncategory issh intuitiveomy and then type is sn exampleas the largepulation of.e define threependent anasking for oseek opinioes about wevents, or srth it??
is ations are thoers accordinmple, thehina??
shoto the differuestions aske questionsr their answed depth.
Thdays of staple.
This escategory.
Itby the opin.is specifictegory do nstions includ?
own ideasce as chattinon ?Why domply just todescriptionn't that harme of the peck the link bctually is exs a number ohacker.ibution ofnt Yahoo!t constant qwhile navch that theys.
This is reould be ablanswers ofide it in-ynamic.le set ofot.
Thisto sup-mappinganswerimilar toconstantst popu-China?
?e subca-d Open.pinions.ns fromhat theyome ob-n exam-se ques-g to thequestionuld haveent date.ing forusuallyer them-he ques-rs in thesentiallyalso in-ion andto cQAot intende telling.
Essen-g roomsso manyask...??
?how tod to do aople thatelow sopressingf peopledifferentAnswersuestionsigationaldo notasonablee to usenaviga-500tional and constant questions.
They do not haveto ask these types of question on community-based question answering services.
On the con-trary, open and opinion questions are frequentlyasked, it ranges from 56%~83%.Question Type C&I E&M Health S&CNavigational Total 0% 0% 0% 0%Constant 15% 20% 15% 11%Opinion 8% 37% 16% 60%Context     Dependent 0% 1% 1% 0%Open 59% 19% 67% 18%Dynamic Total 67% 57% 84% 78%Informational Total 82% 77% 99% 89%Transactional Total 14% 8% 0% 1%Social Total 4% 15% 1% 10%Table 5 Distribution of Question TypeIntersection Number UNI DIR IND SUB REL IRRNavigational 0 0 0 0 0 0Constant 48 9 3 0 1 0Open 51 62 13 15 5 17Context-dep 0 0 1 0 0 1Opinion 15 13 1 84 0 8Transactional 10 7 4 1 0 1Social 0 0 0 1 0 29Table 6.
Question Answer CorrelationTable 6 (UNI: unique, DIR: direct, IND: indi-rect, SUB: subjective, REL: relevant, IRR: irre-levant) gives the correlation statistics of questiontype vs. answer type.
There exists a strong corre-lation between question type and answer type.Every question type tends to be associated withonly one or two answer types (bold numbers inTable 6).5 Question-Type Oriented AnswerSummarizationSince the BAs for at least half of questions donot cover all useful information of other answers,it is better to adopt post-processing techniquessuch as answer summarization for better reuse ofthe BAs.
As observed in the previous sections,answer types can be basically predicted by ques-tion type.
Thus, in this section, we propose to usemulti-document summarization (MDS) tech-niques for summarizing answers according toquestion type.
Here we assume that question typecan be determined automatically.
In the follow-ing sub-sections, we will focus on the summari-zation of answers to open or opinion questions asthey occupy more than half of the cQA questions.5.1 Open QuestionsAlgorithm: For open questions, we follow typi-cal MDS procedure: topic identification, inter-pretation & fusion, and then summary generation(Hovy and Lin, 1999; Lin and Hovy, 2002).
Ta-ble 7 describes the algorithm.1.
Employ the clustering algorithm on answers2.
Extract the noun phrases in each cluster, using a shallow parser.63.
For each cluster and each label (or noun phrase), calculate thescore by using the Relevance Scoring Function:?p?w|?
?PMI?w, l|C?
?
D??|C?
?Where ?
is the cluster, w is the word, l is the label or noun phrase, Cis the background context which is composed of 5,000 questionsin the same category, p(?)
is conditional probability, PMI(?)
ispointwise mutual information, and D(?)
is KL-divergence4.
Extract the key answer which contains the noun phrase that hasthe highest score in each cluster5.
Rank these key answers by cluster size and present the results.Table 7.
Summarization Algorithm(Open-Type)In the first step, we use a bottom-up approachfor clustering answers to do topic identification.Initially, each answer forms a cluster.
Then wecombine the most similar two clusters as a newcluster if their similarity is higher than a thre-shold.
This process is repeated until no new clus-ters can be formed.
For computing similarities,we regard the highest cosine similarity of twosentences from two different clusters as the simi-larity of the two clusters.
Then we extract salientnoun phrases, i.e.
cluster labels, from each clus-ter using the first-order relevance scoring func-tion proposed by Mei et al (2007), (step 2,3 inTable 7).
In the fusion phase (step 4), thesephrases are then used to rank answers withintheir cluster.
Finally in the generation phase (step5), we present the summarized answer by ex-tracting the most important answer in every clus-ter and sort them according to the cluster sizewhere they come from.Case Example: Table 8 presents an exampleof summarization results of open-type questions.The question asks how to change Windows XPdesktop to Mac style.
There are many softwaresproviding such functionalities.
The BA only listsone choice ?
the StarDock products, while otheranswers suggest Flyakite and LiteStep.
The au-tomatic summarized answer (ASA) contains avariety of for turning Windows XP desktop intoMac style with their names highlighted as clusterlabels.
Compared with manually-summarizedanswer (MSA), ASA contains most informationof MSA while retains similar length with BA andMSA.5.2 Opinion QuestionsAlgorithm: For opinion questions, a comprehen-sive investigation of this topic would be beyondthe scope of this paper since this is still a field6 http://opennlp.sourceforge.net501under active development (Wiebe et al, 2003;Kim and Hovy, 2004).
We build a simple yetnovel opinion-focused answer summarizer whichprovides a global view of all answers.
We divideopinion questions into two subcategories.
One issentiment-oriented question that asks the senti-ment about something, for example, ?what doyou think of ??.
The other is list-oriented ques-tion that intends to get a list of answers and seewhat item is the most popular.For sentiment-oriented questions, askers careabout how many people support or against some-thing.
We use an opinion word dictionary7, a cuephrase list, a simple voting strategy, and someheuristic rules to classify the sentences into Sup-port, Neutral, or Against category and use theoverall attitude with key sentences to build sum-marization.
For list-oriented questions, a simplecounting algorithm that tallies different answersof questions together with their supporting voteswould be good answer summaries.
Details of thealgorithm are shown in Table 9, 10.Case Example: Table 11 presents the summa-rization result of an sentiment-oriented question,it asks ?whether it is strange for a 16-year childto talk to a teddy bear?
?, the BA is a negativeresponse.
However, if we consider all answers,7 Inquirer dictionary  http://www.wjh.harvard.edu/~inquirer.we find that half of the answers agree but anotherhalf of them disagree.
The distribution of differ-ent sentiments is similar as MSA.
Table 12shows the summarization result of a list-orientedquestion, the question asks ?what is the best sci-fimovie??
The BA just gives one choice ?Indepen-dence day?
while the summarized answer gives alist of best sci-fi movies with the number of sup-porting vote.
Though it is not complete comparedwith MSA, it contains most of the options whichhas highest votes among all answers.1.
Employ the same cluster procedure of Open-Type question.2.
If an answer begins with negative cue phrase (e.g.
?No, it isn?t?etc.
), it is annotated as Against.
If a response begins with positivecue phrase (e.g.
?Yes, it is?
etc.
), it is annotated as Support.3.
For a clause, if number of positive sentiment word is larger thannegative sentiment word, the sentiment of the clause is Positive.Otherwise, the sentiment of the clause is Negative.4.
If there are negative indicators such as ?don?t/never/??
in frontof the clause, the sentiment should be reversed.5.
If number of negative clauses is larger than number of positiveclauses, the sentiment of the answer is Negative.
Otherwise, thesentiment of the answer is Positive.6.
Denote the sentiment value of question as s(q), the sentimentvalue of an answer as s(a), and then the final sentiment of the an-swer is logical AND of s(q) and s(a)7.
Present key sentiments with attitude labelTable 9.
Summarization Algorithm (Senti-ment-Opinion)1.
Segment the answers into sentences2.
Cluster sentences  by using similar process in open-type3.
For each cluster, choose the key sentence based on mutual infor-mation between itself and other sentences within the cluster4.
Rank the key sentences by the cluster size and present themogether with votesTable 10.
Summarization Algorithm (List-Opinion)Question (http://answers.yahoo.com/question/?qid=1006050125145)I am 16 and i stil talk to my erm..teddy bear..am i wierd??
?Best Answer Chosennot at all i'm 14 and i too do thatAuto-summarized AnswerSupportA: It's might be a little uncommon for a 16 year old to talk to ateddy bear but there would be a serious problem if you told me thatyour teddy bear answered back as you talked to him!!
:)A: I slept with my teddy bear until I graduated.
Can't say that Iever had a conversation with him, but if I had I'm sure he would'vebeen a very good listener.AgainstA: i talk to a  seed im growing .. its not weird .... :)A: No, you're not weird.....you're Pratheek!
:DA: no, i like to hold on to my old memories too.
i do it sometimestoo.A: It will get weird when he starts to answer back!A: not really.
it depends how you talk i mean not if you talk to itlike its a little kid like my brother does.Overall Attitude: Support 5 / Neutral 1 / Against 5Manually-summarized Answersupport (vote 4)neutral (vote 2)against (vote 5) reasons: i like to hold on to my old memories too.
(vote 1) I slept with my teddy bear until I graduated.
(vote 1) i'm 14and i too do that (vote 1)Table 11.
Summary of Sentiment-OpinionQuestionQuestion(http://answers.yahoo.com/question/?qid=1005120801427)What is the best way to make XP look like Mac osX?Best Answer ChosenI found the best way to do this is to use WindowsBlinds.
A pro-gram that, if you use the total StarDock, package will allow you toadd the ObjectBar in addition to changed the toolbars to be OS Xstylized.
If you want added functionality you can download pro-grams off the internet that will mimic the Expose feature which willshow you a tiled set of all open windows.
Programs that will do thisinclude: WinPlosion, Windows Exposer, and Top DeskAuto-summarized AnswerLiteStep:An additional option is LiteStep - a "Shell Replacement"for Windows that has a variety of themes you can install.
Undoub-tedly there are various Mac OSX themes avaialable for LiteStep.
Ihave included a source to a max osx theme for Litestep at custom-ize.org.Flyakite:Flyakite is a transformation pack and the most compre-hensive in terms of converting an XP system's look to that of an OSX system, google it up and you should find it, v3 seems to be indevelopment and should be out soon.Window Blinds:http://www.stardock.com/products/windowb...Manually-summarized AnswerOne way is to use WindowsBlinds.
The package will allow you toadd the ObjectBar for changing to the OSX theme.
You can alsomake added functionality of Expose feature by downloading theprograms like WinPlosion, Windows Exposer and Top Desk.
TheURL of it is http://www.stardock.com/products/windowblinds/.Another option is to use Flyakite which is a transformation pack.The third Option is the LiteStep, it is a "Shell Replacement" forwindows that has a variety of Mac OSX tehmes you can install.The url is http://litestep.net and I have included a source of Mac OStheme for Litestep at http://www.customize.org/details/33409.Table 8.
Summary of Open-Question502Question (http://answers.yahoo.com/question/?qid=20060718083151AACYQJn)What is the best sci-fi movie u ever saw?Best Answer ChosenIndependance DayAuto-summarized Answerstar wars (5)Blade Runner (3)fi movie has to be Night of the Lepus (2)But the best "B" sci (2)I liked Stargate it didn't scare me and I thought they did a great jobrecreating Egypt (3)Independance Day (3)Manually-summarized AnswerStar Wars (vote 6); The Matrix (vote 3); Independence Day (vote2); Blade Runner (vote 2); Starship Troopers (vote 2); Alien (vote2); Alien v.s Predator (vote 1); MST3K (vote 1);Table 12.
Summary of List-Opinion Question5.3 ExperimentsInformation Content: To evaluate the effec-tiveness of automatic summarization, we use theinformation content criterion for comparing ASAwith BA.
It focuses on whether ASA or BA con-tains more useful information to the question.Information point is used in the evaluation.Usually, one kind of solution for open questionsor one kind of reason for opinion questions cancontribute one information point.
By summingall information points in both ASA and BA, wethen can compare which one contains more in-formation.
Intuitively, longer texts would containmore information.
Thus, when comparing theinformation content, we limit the length of ASAwith several levels to do the evaluation.
Takequestion in Table 8 as an example, the BA justgives one software, which contributes one infor-mation point while the ASA lists three kinds ofsoftware which contributes three informationpoints.
Thus, ASA is considered better than BA.For each question, we generate 100%, 150%,and 200% BA word-length ASAs.
Three annota-tors are asked to determine whether an ASA isbetter than, equal to, or worse than its corres-ponding BA in terms of information content.Voting strategy is used to determine the finallabel.
If three labels are all different, it is labeledas Unknown.
We extract 163 open questions and121 opinion questions from all four categories byusing final question category labels mentioned inSection 4.
To make meaningful comparison,questions having unique answers or having onlyone answer are excluded.
After the removal,there are 104 open questions and 99 opinionquestions left for comparison.
The results areshown in Table 13.We are encouraged by the evaluation resultsthat our automatic summarization methods gen-erate better coverage of contents in most of thecases at every answer summary length.
We ob-serve a big difference between 100% and 150%answer summaries.
It should not be a surprisesince a 150% answer summary contains 50%more content than its corresponding BA.
Whileat the 100% length, we still have about 30%ASAs better than BA.
Questions which have bet-ter ASA than BA usually have a long BA butwith little information.
Table 14 provides theexample.
By using summarization, answers thatare compact and direct to the question can beincluded.
The results indicate that summarycompression technique might be helpful to packmore information in short answers.Open ASA Better BA Better Equal Unknown100% 30% 12% 45% 13%150% 55% 7% 28% 10%200% 63% 4% 24% 9%Opinion ASA Better BA Better Equal Unknown100% 37% 20% 32% 11%150% 44% 16% 30% 10%200% 54% 16% 23% 7%Table 13.
Evaluation by Information ContentQ Why wont japanese characters burn onto the DVD?BA man, the answers here are too stupid for hteir own.You arecreating a DVD on Western Platform.
I take it, you areusing an OS that is in English?In order to "view" japaneseas part of your filenames, you need your operating systemto accept Japanese coding (characters).If you are usingWindows, then you will need ot isntall the Japanese cha-racter Set for your operating systemIf you are using MacOS .
i have no idea.100%ASAThe dvd writerProbably because your burner, the DVD writer, doesn'tsupport double bytes code, such as Japanese, Korean, andChinese.
Check the supporting language of your software.Or change all the file name in single byte code, like alpha-bets.
man, the answers here are too stupid for hteir own.You are creating a DVD on Western Platform.
I take it,you are using an OS that is in English?Table 14.
Examples of 100% ASAReadability: Besides the information content,we would also like to study the readability ofautomatic summarized answers.
10 questions(each from open and opinion category) are ex-tracted and we make both manual summarizedanswer (MSA) and automatic summarized an-swer (ASA) for comparison with BA.
We usedthe information content (INFO) and readability(READ) criteria for evaluation.
The readability isjudged basically by the time for understanding.We make two kinds of comparison: ASA vs. BAand MSA vs. BA.
The first one is used to judgewhether the current summarization method isbetter than current cQA scenario.
The second oneis used as an expectation for how much thesummarization methods can be better than BA.503For ASA vs. BA, the results in Table 15 showthat all the annotators agree ASAs providingmore information content but not being with sa-tisfying readability.
For MSA vs. BA, better re-sults in readability can be achieved as Table 16.This suggests that the proposed approach cansucceed as more sophisticated summarizationtechniques are developed.Open Annotator 1 Annotator 2 Annotator 3ASA INFO READ INFO READ INFO READBetter 40% 10% 90% 10% 80% 0%Equal 60% 60% 10% 80% 20% 60%Worse 0% 30% 0% 10% 0% 40%Opinion Annotator 1 Annotator 2 Annotator 3ASA INFO READ INFO READ INFO READBetter 90% 10% 90% 10% 70% 40%Equal 10% 60% 10% 60% 10% 20%Worse 0% 30% 0% 30% 20% 40%Table 15.
ASA vs. BA EvaluationOpen Annotator 1 Annotator 2 Annotator 3MSA INFO READ INFO READ INFO READBetter 100% 30% 100% 90% 100% 90%Equal 0% 50% 0% 0% 0% 0%Worse 0% 20% 0% 10% 0% 10%Opinion Annotator 1 Annotator 2 Annotator 3MSA INFO READ INFO READ INFO READBetter 90% 20% 60% 70% 100% 100%Equal 10% 80% 40% 30% 0% 0%Worse 0% 0% 0% 0% 0% 0%Table 16.
MSA vs. BA Evaluation6 Conclusion and Future WorkIn this paper, we have carried out a comprehen-sive analysis of the question types in community-based question answering (cQA) services andhave developed taxonomies for questions andanswers.
We find that questions do not alwayshave unique best answers.
Open and opinionquestions usually have multiple good answers.They occupied about 56%~83% and most oftheir best answers can be improved.
By usingquestion type as a guide, we propose applyingautomatic summarization techniques to summa-rization answers or improving cQA best answersthrough answer editing.
Our results show thatcustomized question-type focused summarizationtechniques can improve cQA answer quality sig-nificantly.Looking into the future, we are to develop au-tomatic question type identification methods tofully automate answer summarization.
Further-more, we would also like to utilize more sophis-ticated summarization techniques to improvecontent compaction and readability.AcknowledgementsWe thank the anonymous reviewers for their val-uable suggestions and comments to this paper.ReferencesBroder  A.
A taxonomy of web search.
2002.
SIGIRForum Vol.36, No.
2, 3-10.Hovy Edward, Laurie Gerber,  Ulf Hermjakob, Chin-Yew Lin,   Deepak Ravichandran.
2001.
TowardSemantics-Based Answer Pinpointing.
In Proc.
ofHLT?01.Hovy E., C. Lin.
1999.
Automated Text Summariza-tion and the SUMMARIST System.
In Advancesin Automated Text SummarizationJeon J., W. B. Croft, and J. Lee.
2005a.
Finding se-mantically similar questions based on their an-swers.
In Proc.
of SIGIR?05.Jeon J., W. B. Croft, and J. Lee.
2005b.
Finding simi-lar questions in large question and answer arc-hives.
In Proc.
of CIKM?05.Jurczyk P., E. Agichtein.
2007.
Hits on question an-swer portals: exploration of link analysis for au-thor ranking.
In Proc.
of SIGIR '07.Jeon J. , W.B.
Croft, J. Lee, S. Park.
2006.
A Frame-work to predict the quality of answers with non-textual features.
In Proc.
of SIGIR ?06.Jijkoun V., M. R. 2005.
Retrieving Answers fromFrequently Asked Questions Pages on the Web.
InProc.
of CIKM?05.Kleinberg J.
1999.
Authoritative sources in a hyper-linked environment.
Journal of the ACM, vol.
46,Kim S., E.  Hovy.
2004.
Determining the Sentiment ofOpinions.
In Proc.
of COLING?04.Liu X., W.B.
Croft, M. Koll.
2005.
Finding experts incommunity-based question-answering services.In Proc.
of CIKM '05.Lin C.Y., E. Hovy.
2002.
From single to multi-document summarization: a prototype system andits evaluation.
In Proc.
of ACL'02.Lytinen S., N. Tomuro.
2002.
The Use of QuestionTypes to Match Questions in FAQFinder.
In Proc.of AAAI?02.Moldovan D., S. Harabagiu, et al 2000.
The Structureand an Open-Domain Question Answering Sys-tem.
In Proc.
of ACL?00.Mei Q., X. Shen, C. Zhai.
2007.
Automatic labeling ofmultinomial topic models.
In Proc.
of   KDD'07.Rose  D. E., D. Levinson.
2004.
Understanding usergoals in web search.
In Proc.
of WWW '04.Voorhees, M. Ellen.
2003.
Overview of the TREC2003 Question Answering Track.
In Proc.
ofTREC?03.Wiebe J., E. Breck, et al 2003.
Recognizing and Or-ganizing Opinions Expressed in the World Press504
