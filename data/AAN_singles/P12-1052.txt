Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 497?505,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsDependency Hashing for n-best CCG ParsingDominick Ng and James R. Currane-lab, School of Information TechnologiesUniversity of SydneyNSW, 2006, Australia{dominick.ng,james.r.curran}@sydney.edu.auAbstractOptimising for one grammatical representa-tion, but evaluating over a different one isa particular challenge for parsers and n-bestCCG parsing.
We find that this mismatchcauses many n-best CCG parses to be semanti-cally equivalent, and describe a hashing tech-nique that eliminates this problem, improvingoracle n-best F-score by 0.7% and rerankingaccuracy by 0.4%.
We also present a compre-hensive analysis of errors made by the C&CCCG parser, providing the first breakdown ofthe impact of implementation decisions, suchas supertagging, on parsing accuracy.1 IntroductionReranking techniques are commonly used for im-proving the accuracy of parsing (Charniak and John-son, 2005).
Efficient decoding of a parse forest isinfeasible without dynamic programming, but thisrestricts features to local tree contexts.
Rerankingoperates over a list of n-best parses according to theoriginal model, allowing poor local parse decisionsto be identified using arbitrary rich parse features.The performance of reranking depends on thequality of the underlying n-best parses.
Huang andChiang (2005)?s n-best algorithms are used in a widevariety of parsers, including an n-best version of theC&C CCG parser (Clark and Curran, 2007; Brennan,2008).
The oracle F-score of this parser (calculatedby selecting the most optimal parse in the n-best list)is 92.60% with n = 50 over a baseline 1-best F-score of 86.84%.
In contrast, the Charniak parserrecords an oracle F-score of 96.80% in 50-best modeover a baseline of 91.00% (Charniak and Johnson,2005).
The 4.2% oracle score difference suggeststhat further optimisations may be possible for CCG.We describe how n-best parsing algorithms thatoperate over derivations do not account for absorp-tion ambiguities in parsing, causing semanticallyidentical parses to exist in the CCG n-best list.
Thisis caused by the mismatch between the optimisa-tion target (different derivations) and the evaluationtarget (CCG dependencies).
We develop a hash-ing technique over dependencies that removes du-plicates and improves the oracle F-score by 0.7%to 93.32% and reranking accuracy by 0.4%.
Huanget al (2006) proposed a similar idea where stringsgenerated by a syntax-based MT rescoring systemwere hashed to prevent duplicate translations.Despite this improvement, there is still a substan-tial gap between the C&C and Charniak oracle F-scores.
We perform a comprehensive subtractiveanalysis of the C&C parsing pipeline, identifying therelative contribution of each error class and why thegap exists.
The parser scores 99.49% F-score withgold-standard categories on section 00 of CCGbank,and 94.32% F-score when returning the best parsein the chart using the supertagger on standard set-tings.
Thus the supertagger contributes roughly 5%of parser error, and the parser model the remaining7.5%.
Various other speed optimisations also detri-mentally affect accuracy to a smaller degree.Several subtle trade-offs are made in parsers be-tween speed and accuracy, but their actual impactis often unclear.
Our work investigates these and thegeneral issue of how different optimisation and eval-uation targets can affect parsing performance.497Jack swims across the riverNP S\NP ((S\NP)\(S\NP))/NP NP/N N>NP>(S\NP)\(S\NP)<S\NP<SFigure 1: A CCG derivation with a PP adjunct, demon-strating forward and backward combinator application.Adapted from Villavicencio (2002).2 BackgroundCombinatory Categorial Grammar (CCG, Steedman,2000) is a lexicalised grammar formalism based onformal logic.
The grammar is directly encoded inthe lexicon in the form of categories that govern thesyntactic behaviour of each word.Atomic categories such as N (noun), NP (nounphrase), and PP (prepositional phrase) representcomplete units.
Complex categories encode subcat-egorisation information and are functors of the formX /Y or X \Y .
They represent structures whichcombine with an argument category Y to produce aresult category X .
In Figure 1, the complex categoryS\NP for swims represents an intransitive verb re-quiring a subject NP to the left.Combinatory rules are used to combine categoriestogether to form an analysis.
The simplest rulesare forward and backward application, where com-plex categories combine with their outermost argu-ments.
Forward and backward composition allowcategories to be combined in a non-canonical order,and type-raising turns a category into a higher-orderfunctor.
A ternary coordination rule combines twoidentical categories separated by a conj into one.As complex categories are combined with their ar-guments, they create a logical form representing thesyntactic and semantic properties of the sentence.This logical form can be expressed in many ways;we will focus on the dependency representation usedin CCGbank (Hockenmaier and Steedman, 2007).
InFigure 1, swims generates one dependency:?swims, S [dcl]\NP1 , 1, Jack , ?
?where the dependency contains the head word,head category, argument slot, argument word, andwhether the dependency is long-range.Jack swims across the riverNP (S\NP)/PP PP/NP NP/N N>NP>PP>S\NP<SFigure 2: A CCG derivation with a PP argument (note thecategories of swims and across).
The bracketing is identi-cal to Figure 1, but nearly all dependencies have changed.2.1 Corpora and evaluationCCGbank (Hockenmaier, 2003) is a transformationof the Penn Treebank (PTB) data into CCG deriva-tions, and it is the standard corpus for English CCGparsing.
Other CCG corpora have been induced in asimilar way for German (Hockenmaier, 2006) andChinese (Tse and Curran, 2010).
CCGbank con-tains 99.44% of the sentences from the PTB, andseveral non-standard rules were necessary to achievethis coverage.
These include punctuation absorptionrules and unary type-changing rules for clausal ad-juncts that are otherwise difficult to represent.The standard CCG parsing evaluation calculateslabeled precision, recall, and F-score over the de-pendencies recovered by a parser as compared toCCGbank (Clark et al, 2002).
All components ofa dependency must match the gold standard for it tobe scored as correct, and this makes the proceduremuch harsher than the PARSEVAL labeled bracketsmetric.
In Figure 2, the PP across the river has beeninterpreted as an argument rather than an adjunct asin Figure 1.
Both parses would score identicallyunder PARSEVAL as their bracketing is unchanged.However, the adjunct to argument change results indifferent categories for swims and across; nearly ev-ery CCG dependency in the sentence is headed byone of these two words and thus each one changesas a result.
An incorrect argument/adjunct distinc-tion in this sentence produces a score close to 0.All experiments in this paper use the normal-formC&C parser model over CCGbank 00 (Clark andCurran, 2007).
Scores are reported for sentenceswhich the parser could analyse; we observed simi-lar conclusions when repeating our experiments overthe subset of sentences that were parsable under allconfigurations described in this paper.4982.2 The C&C parserThe C&C parser (Clark and Curran, 2007) is a fastand accurate CCG parser trained on CCGbank 02-21,with an accuracy of 86.84% on CCGbank 00 withthe normal-form model.
It is a two-phase system,where a supertagger assigns possible categories towords in a sentence and the parser combines themusing the CKY algorithm.
An n-best version incor-porating the Huang and Chiang (2005) algorithmshas been developed (Brennan, 2008).
Recent workon a softmax-margin loss function and integrated su-pertagging via belief propagation has improved thisto 88.58% (Auli and Lopez, 2011).A parameter ?
is passed to the supertagger as amulti-tagging probability beam.
?
is initially set at avery restrictive value, and if the parser cannot forman analysis the supertagger is rerun with a lower ?,returning more categories and giving the parser moreoptions in constructing a parse.
This adaptive su-pertagging prunes the search space whilst maintain-ing coverage of over 99%.The supertagger also uses a tag dictionary, as de-scribed by Ratnaparkhi (1996), and accepts a cut-off k. Words seen more than k times in CCGbank02-21 may only be assigned categories seen withthat word more than 5 times in CCGbank 02-21;the frequency must also be no less than 1/500th ofthe most frequent tag for that word.
Words seenfewer than k times may only be assigned categoriesseen with the POS of the word in CCGbank 02-21,subject to the cutoff and ratio constraint (Clark andCurran, 2004b).
The tag dictionary eliminates infre-quent categories and improves the performance ofthe supertagger, but at the cost of removing unseenor infrequently seen categories from consideration.The parser accepts POS-tagged text as input; un-like many PTB parsers, these tags are fixed andremain unchanged throughout during the parsingpipeline.
The POS tags are important features for thesupertagger; parsing accuracy using gold-standardPOS tags is typically 2% higher than using automat-ically assigned POS tags (Clark and Curran, 2004b).2.3 n-best parsing and rerankingMost parsers use dynamic programming, discard-ing infeasible states in order to maintain tractability.However, constructing an n-best list requires keep-ing the top n states throughout.
Huang and Chiang(2005) define several n-best algorithms that allowdynamic programming to be retained whilst generat-ing precisely the top n parses ?
using the observationthat once the 1-best parse is generated, the 2nd bestparse must differ in exactly one location from it, andso forth.
These algorithms are defined on a hyper-graph framework equivalent to a chart, so the parsesare distinguished based on their derivations.
Huanget al (2006) develop a translation reranking modelusing these n-best algorithms, but faced the issue ofdifferent derivations yielding the same string.
Thiswas overcome by storing a hashtable of strings ateach node in the tree, and rejecting any derivationsthat yielded a previously seen string.Collins (2000)?s parser reranker uses n-bestparses of PTB 02-21 as training data.
Reranker fea-tures include lexical heads and the distances be-tween them, context-free rules in the tree, n-gramsand their ancestors, and parent-grandparent relation-ships.
The system improves the accuracy of theCollins parser from 88.20% to 89.75%.Charniak and Johnson (2005)?s reranker uses asimilar setup to the Collins reranker, but utilisesmuch higher quality n-best parses.
Additional fea-tures on top of those from the Collins reranker suchas subject-verb agreement, n-gram local trees, andright-branching factors are also used.
In 50-bestmode the parser has an oracle F-score of 96.80%,and the reranker produces a final F-score of 91.00%(compared to an 89.70% baseline).3 Ambiguity in n-best CCG parsingThe type-raising and composition combinators al-low the same logical form to be created from dif-ferent category combination orders in a derivation.This is termed spurious ambiguity, where differentderivational structures are semantically equivalentand will evaluate identically despite having a differ-ent phrase structure.
The C&C parser employs thenormal-form constraints of Eisner (1996) to addressspurious ambiguity in 1-best parsing.Absorption ambiguity occurs when a constituentmay be legally placed at more than one location ina derivation, and all of the resulting derivations aresemantically equivalent.
Punctuation such as com-mas, brackets, and periods are particularly prone to499Avg P/sent Distinct P/sent % Distinct10-best 9.8 5.1 5250-best 47.6 16.0 3410-best# 9.0 9.0 10050-best# 37.9 37.9 100Table 1: Average and distinct parses per sentence overCCGbank 00 with respect to CCG dependencies.
# indi-cates the inclusion of dependency hashingabsorption ambiguity in CCG; Figure 3 depicts foursemantically equivalent sequences of absorption andcombinator application in a sentence fragment.The Brennan (2008) CCG n-best parser differen-tiates CCG parses by derivation rather than logicalform.
To illustrate how this is insufficient, we ranthe parser using Algorithm 3 of Huang and Chiang(2005) with n = 10 and n = 50, and calculated howmany parses were semantically distinct (i.e.
yielddifferent dependencies).
The results (summarised inTable 1) are striking: just 52% of 10-best parses and34% of 50-best parses are distinct.
We can also seethat fewer than n parses are found on average foreach sentence; this is mostly due to shorter sentencesthat may only receive one or two parses.We perform the same diversity experiment us-ing the DepBank-style grammatical relations (GRs,King et al, 2003; Briscoe and Carroll, 2006) out-put of the parser.
GRs are generated via a depen-dency to GR mapping in the parser as well as apost-processing script to clean up common errors(Clark and Curran, 2007).
GRs provide a moreformalism-neutral comparison and abstract awayfrom the raw CCG dependencies; for example, inFigures 1 and 2, the dependency from swims to Jackwould be abstracted into (subj swims Jack)and thus would be identical in both parses.
Hence,there are even fewer distinct parses in the GR resultssummarised in Table 2: 45% and 27% of 10-best and50-best parses respectively yield unique GRs.3.1 Dependency hashingTo address this problem of semantically equivalentn-best parses, we define a uniqueness constraintover all the n-best candidates:Constraint.
At any point in the derivation, any n-best candidate must not have the same dependenciesas any candidate already in the list.Avg P/sent Distinct P/sent % Distinct10-best 9.8 4.4 4550-best 47.6 13.0 2710-best# 8.9 8.1 9150-best# 37.1 31.5 85Table 2: Average and distinct parses per sentence overCCGbank 00 with respect to GRs.
# indicates the inclu-sion of dependency hashingEnforcing this constraint is non-trivial as it is in-feasible to directly compare every dependency in apartial tree with another.
Due to the flexible no-tion of constituency in CCG, dependencies can begenerated at a variety of locations in a derivationand in a variety of orders.
This means that compar-ing all of the dependencies in a particular state mayrequire traversing the entire sub-derivation at thatpoint.
Parsing is already a computationally expen-sive process, so we require as little overhead fromthis check as possible.Instead, we represent all of the CCG dependenciesin a sub-derivation using a hash value.
This allowsus to compare the dependencies in two derivationswith a single numeric equality check rather than afull iteration.
The underlying idea is similar to thatof Huang et al (2006), who maintain a hashtableof unique strings produced by a translation reranker,and reject new strings that have previously been gen-erated.
Our technique does not use a hashtable, andinstead only stores the hash value for each set of de-pendencies, which is much more efficient but runsthe risk of filtering unique parses due to collisions.As we combine partial trees to build the deriva-tion, we need to convolve the hash values in a con-sistent manner.
The convolution operator must beorder-independent as dependencies may be gener-ated in an arbitrary order at different locations ineach tree.
We use the bitwise exclusive OR (?)
op-eration as our convolution operator: when two par-tial derivations are combined, their hash values areXOR?ed together.
XOR is commonly employed inhashing applications for randomly permuting num-bers, and it is also order independent: a?
b ?
b?
a.Using XOR, we enforce a unique hash value con-straint in the n-best list of candidates, discarding po-tential candidates with an identical hash value to anyalready in the list.500big red ball )N /N N /N N RRB>N>N>Nbig red ball )N /N N /N N RRB>N>N>Nbig red ball )N /N N /N N RRB>N>N>Nbig red ball )N /N N /N N RRB>BN /N>N>NFigure 3: All four derivations have a different syntactic structure, but generate identical dependencies.Collisions Comparisons %10-best 300 54861 0.5550-best 2109 225970 0.93Table 3: Dependency hash collisions and comparisonsover 00 of CCGbank.3.2 Hashing performanceWe evaluate our hashing technique with several ex-periments.
A simple test is to measure the number ofcollisions that occur, i.e.
where two partial trees withdifferent dependencies have the same hash value.We parsed CCGbank 00 with n = 10 and n = 50using a 32 bit hash, and exhaustively checked thedependencies of colliding states.
We found that lessthan 1% of comparisons resulted in collisions inboth 10-best and 50-best mode, and decided that thiswas acceptably low for distinguishing duplicates.We reran the diversity experiments, and verifiedthat every n-best parse for every sentence in CCG-bank 00 was unique (see Table 1), corroborating ourdecision to use hashing alone.
On average, thereare fewer parses per sentence, showing that hashingis eliminating many equivalent parses for more am-biguous sentences.
However, hashing also leads to anear doubling of unique parses in 10-best mode anda 2.3x increase in 50-best mode.
Similar results arerecorded for the GR diversity (see Table 2), thoughnot every set of GRs is unique due to the many-to-many mapping from CCG dependencies.
Theseresults show that hashing prunes away equivalentparses, creating more diversity in the n-best list.We also evaluate the oracle F-score of the parserusing dependency hashing.
Our results in Table 4include a 1.1% increase in 10-best mode and 0.72%in 50-best mode using the new constraints, showinghow the diversified parse list contains better candi-dates for reranking.
Our highest oracle F-score was93.32% in 50-best mode.Experiment LP LR LF AFbaseline 87.27 86.41 86.84 84.91oracle 10-best 91.50 90.49 90.99 89.01oracle 50-best 93.17 92.04 92.60 90.68oracle 10-best# 92.67 91.51 92.09 90.15oracle 50-best# 94.00 92.66 93.32 91.47Table 4: Oracle precision, recall, and F-score on gold andauto POS tags for the C&C n-best parser.
# denotes theinclusion of dependency hashing.Test dataTraining data no hashing hashingno hashing 86.83 86.35hashing 87.21 87.15Table 5: Reranked parser accuracy; labeled F-score usinggold POS tags, with and without dependency hashing3.3 CCG reranking performanceFinally, we implement a discriminative maximumentropy reranker for the n-best C&C parser andevaluate it when using dependency hashing.
Wereimplement the features described in Charniak andJohnson (2005) and add additional features based onthose used in the C&C parser and on features of CCGdependencies.
The training data is cross-fold n-bestparsed sentences of CCGbank 02-21, and we use theMEGAM optimiser1 in regression mode to predict thelabeled F-score of each n-best candidate parse.Our experiments rerank the top 10-best parsesand use four configurations: with and without de-pendency hashing for generating the training andtest data for the reranker.
Table 5 shows that la-beled F-score improves substantially when depen-dency hashing is used to create reranker trainingdata.
There is a 0.4% improvement using no hash-ing at test, and a 0.8% improvement using hashing1http://hal3.name/megam501at test, showing that more diverse training data cre-ates a better reranker.
The results of 87.21% with-out hashing at test and 87.15% using hashing at testare statistically indistinguishable from one other;though we would expect the latter to perform better.Our results also show that the reranker performsextremely poorly using diversified test parses andundiversified training parses.
There is a 0.5% per-formance loss in this configuration, from 86.83%to 86.35% F-score.
This may be caused by thereranker becoming attuned to selecting between se-mantically indistinguishable derivations, which arepruned away in the diversified test set.4 Analysing parser errorsA substantial gap exists between the oracle F-scoreof our improved n-best parser and other PTB n-bestparsers (Charniak and Johnson, 2005).
Due to thedifferent evaluation schemes, it is difficult to directlycompare these numbers, but whether there is furtherroom for improvement in CCG n-best parsing is anopen question.
We analyse three main classes of er-rors in the C&C parser in order to answer this ques-tion: grammar error, supertagger error, and modelerror.
Furthermore, insights from this analysis willprove useful in evaluating tradeoffs made in parsers.Grammar error: the parser implements a subsetof the grammar and unary type-changing rules inCCGbank for efficiency, with some rules, such assubstitution, omitted for efficiency (Clark and Cur-ran, 2007).
This means that, given the correct cat-egories for words in a sentence, the parser may beunable to combine them into a derivation yieldingthe correct dependencies, or it may not recognise thegold standard category at all.There is an additional constraint in the parser thatonly allows two categories to combine if they havebeen seen to combine in the training data.
This seenrules constraint is used to reduce the size of the chartand improve parsing speed, at the cost of only per-mitting category combinations seen in CCGbank 02-21 (Clark and Curran, 2007).Supertagger error: The supertagger uses a re-stricted set of 425 categories determined by a fre-quency cutoff of 10 over the training data (Clark andCurran, 2004b).
Words with gold categories that arenot in this set cannot be tagged correctly.The ?
parameter restricts the categories to withina probability beam, and the tag dictionary restrictsthe set of categories that can be considered for eachword.
Supertagger model error occurs when the su-pertagger can assign a word its correct category, butthe statistical model does not assign the correct tagenough probability for it to fall within the ?.Model error: The parser model features maybe rich enough to capture certain characteristics ofparses, causing it to select a suboptimal parse.4.1 Subtractive experimentsWe develop an oracle methodology to distinguishbetween grammar, supertagger, and model errors.This is the most comprehensive error analysis of aparsing pipeline in the literature.First, we supplied gold-standard categories foreach word in the sentence.
In this experimentthe parser only needs to combine the categoriescorrectly to form the gold parse.
In our testingover CCGbank 00, the parser scores 99.49% F-score given perfect categories, with 95.61% cover-age.
Thus, grammar error accounts for about 0.5%of overall parser errors as well as a 4.4% drop in cov-erage2.
All results in this section will be comparedagainst this 99.49% result as it removes the grammarerror from consideration.4.2 Supertagger and model errorTo determine supertagger and model error, we runthe parser on standard settings over CCGbank 00and examined the chart.
If it contains the gold parse,then a model error results if the parser returns anyother parser.
Otherwise, it is a supertagger or gram-mar error, where the parser cannot construct the bestparse.
For each sentence, we found the best parse inthe chart by decoding against the gold dependencies.Each partial tree was scored using the formula:score = ncorrect?
nbadwhere ncorrect is the number of dependencieswhich appear in the gold standard, and nbad is thenumber of dependencies which do not appear in thegold standard.
The top scoring derivation in the treeunder this scheme is then returned.2Clark and Curran (2004a) performed a similar experimentwith lower accuracy and coverage; our improved numbers aredue to changes in the parser.502Experiment LP LR LF AF cover ?LF ?AForacle cats 99.72 99.27 99.49 99.49 95.61 0.00 0.00best in chart -tagdict -seen rules 96.88 94.81 95.84 94.17 99.01 -3.65 -5.32best in chart -tagdict 96.13 94.72 95.42 93.56 99.37 -4.07 -5.93best in chart -seen rules 96.10 93.66 94.86 93.35 98.85 -4.63 -6.14best in chart 95.15 93.50 94.32 92.60 99.16 -5.17 -6.89baseline 87.27 86.41 86.84 84.91 99.16 -12.65 -14.58Table 6: Oracle labeled precision, recall, F-score, F-score with auto POS, and coverage over CCGbank 00.
-tagdictindicates disabling the tag dictionary, -seen rules indicates disabling the seen rules constraint?
k cats/word sent/sec LP LR LF AF cover ?LF ?AFgold cats - - 99.72 99.27 99.49 - 95.61 0.00 0.000.075 20 1.27 40.5 95.46 93.90 94.68 93.07 94.30 -4.81 -6.420.03 20 1.43 33.0 96.23 94.87 95.54 94.01 96.03 -3.95 -5.480.01 20 1.72 19.1 97.02 95.82 96.42 95.02 96.86 -3.07 -4.470.005 20 1.98 10.7 97.26 96.09 96.68 95.32 97.23 -2.81 -4.170.001 150 3.57 1.18 98.33 97.37 97.85 96.76 96.13 -1.64 -2.73Table 7: Category ambiguity, speed, labeled P, R, F-score on gold and auto POS, and coverage over CCGbank 00 forthe standard supertagger parameters selecting the best scoring parse against the gold parse in the chart.We obtain an overall maximum possible F-scorefor the parser using this scoring formula.
The dif-ference between this maximum F-score and the or-acle result of 99.49% represents supertagger error(where the supertagger has not provided the correctcategories), and the difference to the baseline per-formance indicates model error (where the parsermodel has not selected the optimal parse given thecurrent categories).
We also try disabling the seenrules constraint to determine its impact on accuracy.The impact of tag dictionary errors must be neu-tralised in order to distinguish between the types ofsupertagger error.
To do this, we added the goldcategory for a word to the set of possible tags con-sidered for that word by the supertagger.
This wasdone for categories that the supertagger could use;categories that were not in the permissible set of425 categories were not considered.
This is an opti-mistic experiment; removing the tag dictionary en-tirely would greatly increase the number of cate-gories considered by the supertagger and may dra-matically change the tagging results.Table 6 shows the results of our experiments.
Thedelta columns indicate the difference in labeled F-score to the oracle result, which discounts the gram-mar error in the parser.
We ran the experiment infour configurations: disabling the tag dictionary, dis-abling the seen rules constraint, and disabling both.There are coverage differences of less than 0.5% thatwill have a small impact on these results.The ?best in chart?
experiment produces a resultof 94.32% with gold POS tags and 92.60% with autoPOS tags.
These numbers are the upper bound of theparser with the supertagger on standard settings.
Ourresult with gold POS tags is statistically identical tothe oracle experiment conducted by Auli and Lopez(2011), which exchanged brackets for dependenciesin the forest oracle algorithm of Huang (2008).
Thisillustrates the validity of our technique.A perfect tag dictionary that always contains thegold standard category if it is available results inan upper bound accuracy of 95.42%.
This showsthat overall supertagger error in the parser is around5.2%, with roughly 1% attributable to the use of thetag dictionary and the remainder to the supertaggermodel.
The baseline parser is 12.5% worse than theoracle categories result due to model error and su-pertagger error, so model error accounts for roughly7.3% of the loss.Eliminating the seen rules constraint contributesto a 0.5% accuracy improvement over both the stan-dard parser configuration and the -tagdict configura-tion, at the cost of roughly 0.3% coverage to both.This is of similar magnitude to grammar error; but503Experiment LF cover ?LFbaseline 86.84 99.16 0.00auto POS parser 86.57 99.16 -0.27auto POS super 85.33 99.06 -1.51auto POS both 84.91 99.06 -1.93Table 8: Labeled F-score, coverage, and deltas overCCGbank 00 for combinations of gold and auto POS tags.here accuracy is traded off against coverage.The results also show that model and supertaggererror largely accounts for the remaining oracle accu-racy difference between the C&C n-best parser andthe Charniak/Collins n-best parsers.
The absoluteupper bound of the C&C parser is only 1% higherthan the oracle 50-best score in Table 4, placing then-best parser close to its theoretical limit.4.3 Varying supertagger parametersWe conduct a further experiment to determine theimpact of the standard ?
and k values used in theparser.
We reran the ?best in chart?
configuration,but used each standard ?
and k value individuallyrather than backing off to a lower ?
value to find themaximum score at each individual value.Table 7 shows that the oracle accuracy improvesfrom 94.68% F-score and 94.30% coverage with?
= 0.075, k = 20 to 97.85% F-score and 96.13%coverage with ?
= 0.001, k = 150.
At higher?
values, accuracy is lost because the correct cat-egory is not returned to the parser, while lower ?values are more likely to return the correct category.The coverage peaks at the second-lowest value be-cause at lower ?
values, the number of categoriesreturned means all of the possible derivations cannotbe stored in the chart.
The back-off approach sub-stantially increases coverage by ensuring that parsesthat fail at higher ?
values are retried at lower ones,at the cost of reducing the upper accuracy bound tobelow that of any individual ?.The speed of the parser varies substantially in thisexperiment, from 40.5 sents/sec at the first ?
levelto just 1.18 sents/sec at the last.
This illustratesthe trade-off in using supertagging: the maximumachievable accuracy drops by nearly 5% for parsingspeeds that are an order of magnitude faster.4.4 Gold and automatic POS tagsThere is a substantial difference in accuracy betweenexperiments that use gold POS and auto POS tags.Table 6 shows a corresponding drop in upper boundaccuracy from 94.32% with gold POS tags to 92.60%with auto POS tags.
Both the supertagger and parseruse POS tags independently as features, but this re-sult suggests that the bulk of the performance differ-ence comes from the supertagger.
To fully identifythe error contributions, we ran an experiment wherewe provide gold POS tags to one of the parser andsupertagger, and auto POS tags to the other, and thenrun the standard evaluation (the oracle experimentwill be identical to the ?best in chart?
).Table 8 shows that supplying the parser with autoPOS tags reduces accuracy by 0.27% compared tothe baseline parser, while supplying the supertaggerwith auto POS tags results in a 1.51% decrease.
Theparser uses more features in a wider context than thesupertagger, so it is less affected by POS tag errors.5 ConclusionWe have described how a mismatch between the wayCCG parses are modeled and evaluated caused equiv-alent parses to be produced in n-best parsing.
Weeliminate duplicates by hashing dependencies, sig-nificantly improving the oracle F-score of CCG n-best parsing by 0.7% to 93.32%, and improving theperformance of CCG reranking by up to 0.4%.We have comprehensively investigated thesources of error in the C&C parser to explain the gapin oracle performance compared with other n-bestparsers.
We show the impact of techniques thatsubtly trade off accuracy for speed and coverage.This will allow a better choice of parameters forfuture applications of parsing in CCG and otherlexicalised formalisms.AcknowledgmentsWe would like to thank the reviewers for their com-ments.
This work was supported by AustralianResearch Council Discovery grant DP1097291, theCapital Markets CRC, an Australian Postgradu-ate Award, and a University of Sydney Vice-Chancellor?s Research Scholarship.504ReferencesMichael Auli and Adam Lopez.
2011.
Training aLog-Linear Parser with Loss Functions via Softmax-Margin.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing(EMNLP-11), pages 333?343.
Edinburgh, Scotland,UK.Forrest Brennan.
2008. k-best Parsing Algorithms for aNatural Language Parser.
Master?s thesis, Universityof Oxford.Ted Briscoe and John Carroll.
2006.
Evaluating the Ac-curacy of an Unlexicalized Statistical Parser on thePARC DepBank.
In Proceedings of the COLING/ACL2006 Main Conference Poster Sessions, pages 41?48.Sydney, Australia.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-Fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguis-tics (ACL-05), pages 173?180.
Ann Arbor, Michigan,USA.Stephen Clark and James R. Curran.
2004a.
Parsing theWSJ Using CCG and Log-Linear Models.
In Proceed-ings of the 42nd Annual Meeting of the Association forComputational Linguistics (ACL-04), pages 103?110.Barcelona, Spain.Stephen Clark and James R. Curran.
2004b.
The Impor-tance of Supertagging for Wide-Coverage CCG Pars-ing.
In Proceedings of the 20th International Con-ference on Computational Linguistics (COLING-04),pages 282?288.
Geneva, Switzerland.Stephen Clark and James R. Curran.
2007.
Wide-Coverage Efficient Statistical Parsing with CCG andLog-Linear Models.
Computational Linguistics,33(4):493?552.Stephen Clark, Julia Hockenmaier, and Mark Steedman.2002.
Building Deep Dependency Structures using aWide-Coverage CCG Parser.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics (ACL-02), pages 327?334.
Philadel-phia, Pennsylvania, USA.Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
In Proceedings of the17th International Conference on Machine Learning(ICML-00), pages 175?182.
Palo Alto, California,USA.Jason Eisner.
1996.
Efficient Normal-Form Parsing forCombinatory Categorial Grammar.
In Proceedings ofthe 34th Annual Meeting of the Association for Com-putational Linguistics (ACL-96), pages 79?86.
SantaCruz, California, USA.Julia Hockenmaier.
2003.
Parsing with Generative Mod-els of Predicate-Argument Structure.
In Proceedingsof the 41st Annual Meeting of the Association for Com-putational Linguistics (ACL-03), pages 359?366.
Sap-poro, Japan.Julia Hockenmaier.
2006.
Creating a CCGbank anda Wide-Coverage CCG Lexicon for German.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguis-tics (COLING/ACL-06), pages 505?512.
Sydney, Aus-tralia.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank.
Compu-tational Linguistics, 33(3):355?396.Liang Huang.
2008.
Forest Reranking: DiscriminativeParsing with Non-Local Features.
In Proceedings ofthe Human Language Technology Conference at the45th Annual Meeting of the Association for Compu-tational Linguistics (HLT/ACL-08), pages 586?594.Columbus, Ohio.Liang Huang and David Chiang.
2005.
Better k-best Pars-ing.
In Proceedings of the Ninth International Work-shop on Parsing Technology (IWPT-05), pages 53?64.Vancouver, British Columbia, Canada.Liang Huang, Kevin Knight, and Aravind K. Joshi.
2006.Statistical Syntax-Directed Translation with ExtendedDomain of Locality.
In Proceedings of the 7th BiennialConference of the Association for Machine Transla-tion in the Americas (AMTA-06), pages 66?73.
Boston,Massachusetts, USA.Tracy Holloway King, Richard Crouch, Stefan Riezler,Mary Dalrymple, and Ronald M. Kaplan.
2003.
ThePARC 700 Dependency Bank.
In Proceedings of the4th International Workshop on Linguistically Inter-preted Corpora, pages 1?8.
Budapest, Hungary.Adwait Ratnaparkhi.
1996.
A Maximum Entropy Modelfor Part-of-Speech Tagging.
In Proceedings of the1996 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-96), pages 133?142.Philadelphia, Pennsylvania, USA.Mark Steedman.
2000.
The Syntactic Process.
MIT Press,Cambridge, Massachusetts, USA.Daniel Tse and James R. Curran.
2010.
Chinese CCG-bank: extracting CCG derivations from the PennChinese Treebank.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(COLING-2010), pages 1083?1091.
Beijing, China.Aline Villavicencio.
2002.
Learning to Distinguish PPArguments from Adjuncts.
In Proceedings of the 6thConference on Natural Language Learning (CoNLL-2002), pages 84?90.
Taipei, Taiwan.505
