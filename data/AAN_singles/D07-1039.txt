Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
369?379, Prague, June 2007. c?2007 Association for Computational LinguisticsDetecting Compositionality of Verb-Object Combinations using SelectionalPreferencesDiana McCarthyUniversity of SussexFalmer, East SussexBN1 9QH, UKdianam@sussex.ac.ukSriram VenkatapathyInternational Instituteof Information TechnologyHyderabad, Indiasriram@research.iiit.ac.inAravind K. JoshiUniversity of Pennsylvania,PhiladelphiaPA, USA.joshi@linc.cis.upenn.eduAbstractIn this paper we explore the use of se-lectional preferences for detecting non-compositional verb-object combinations.
Tocharacterise the arguments in a given gram-matical relationship we experiment withthree models of selectional preference.
Twouse WordNet and one uses the entries froma distributional thesaurus as classes for rep-resentation.
In previous work on selectionalpreference acquisition, the classes used forrepresentation are selected according to thecoverage of argument tokens rather than be-ing selected according to the coverage ofargument types.
In our distributional the-saurus models and one of the methods us-ing WordNet we select classes for represent-ing the preferences by virtue of the numberof argument types that they cover, and thenonly tokens under these classes which arerepresentative of the argument head data areused to estimate the probability distributionfor the selectional preference model.
Wedemonstrate a highly significant correlationbetween measures which use these ?type-based?
selectional preferences and composi-tionality judgements from a data set used inprevious research.
The type-based modelsperform better than the models which use to-kens for selecting the classes.
Furthermore,the models which use the automatically ac-quired thesaurus entries produced the bestresults.
The correlation for the thesaurusmodels is stronger than any of the individ-ual features used in previous research on thesame dataset.1 IntroductionCharacterising the semantic behaviour of phrases interms of compositionality has particularly attractedattention in recent years (Lin, 1999; Schone and Ju-rafsky, 2001; Bannard, 2002; Bannard et al, 2003;Baldwin et al, 2003; McCarthy et al, 2003; Ban-nard, 2005; Venkatapathy and Joshi, 2005).
Typi-cally the phrases are putative multiwords and non-compositionality is viewed as an important featureof many such ?words with spaces?
(Sag et al, 2002).For applications such as paraphrasing, informationextraction and translation, it is essential to take thewords of non-compositional phrases together as aunit because the meaning of a phrase cannot be ob-tained straightforwardly from the constituent words.In this work we are investigate methods of deter-mining semantic compositionality of verb-object 1combinations on a continuum following previousresearch in this direction (McCarthy et al, 2003;Venkatapathy and Joshi, 2005).Much previous research has used a combinationof statistics and distributional approaches wherebydistributional similarity is used to compare the con-stituents of the multiword with the multiword itself.In this paper, we will investigate the use of selec-tional preferences of verbs.
We will use the pref-erences to find atypical verb-object combinations aswe anticipate that such combinations are more likelyto be non-compositional.1We use object to refer to direct objects.369Selectional preferences of predicates have beenmodelled using the man-made thesaurus Word-Net (Fellbaum, 1998), see for example (Resnik,1993; Li and Abe, 1998; Abney and Light, 1999;Clark and Weir, 2002).
There are also distribu-tional approaches which use co-occurrence data tocluster distributionally similar words together.
Thecluster output can then be used as classes for se-lectional preferences (Pereira et al, 1993), or onecan directly use frequency information from distri-butionally similar words for smoothing (Grishmanand Sterling, 1994).We used three different types of probabilisticmodels, which vary in the classes selected for rep-resentation over which the probability distribution ofthe argument heads 2 is estimated.
Two use WordNetand the other uses the entries in a thesaurus of distri-butionally similar words acquired automatically fol-lowing (Lin, 1998).
The first method is due to Li andAbe (1998).
The classes over which the probabil-ity distribution is calculated are selected accordingto the minimum description length principle (MDL)which uses the argument head tokens for finding thebest classes for representation.
This method has pre-viously been tried for modelling compositionality ofverb-particle constructions (Bannard, 2002).The other two methods (we refer to them as ?type-based?)
also calculate a probability distribution us-ing argument head tokens but they select the classesover which the distribution is calculated using thenumber of argument head types (of a verb in a cor-pus) in a given class, rather than the number of ar-gument head tokens in contrast to previous WordNetmodels (Resnik, 1993; Li and Abe, 1998; Clark andWeir, 2002).
For example, if the object slot of theverb park contains the argument heads { car, car,car, car, van, jeep } then the type-based models usethe word type ?car?
only once when determining theclasses over which the probability distribution is tobe estimated.
Classes are selected which maximisethe number of types that they cover, rather than thenumber of tokens.
This is done to avoid the selec-tional preferences being heavily influenced by noisefrom highly frequent arguments which may be poly-semous and some or all of their meanings may not be2Argument heads are the nouns occurring in the object slotof the target verb.semantically related to the ?prototypical?
argumentsof the verb.
For example car has a gondola sense inWordNet.The third method uses entries in a distributionalthesaurus rather than classes from WordNet.
The en-tries used as classes for representation are selectedby virtue of the number of argument types they en-compass.
As with the WordNet models, the tokensare used to estimate a probability distribution overthese entries.In the next section, we discuss related work onidentifying compositionality.
In section 3, we de-scribe the methods we are using for acquiring ourmodels of selectional preference.
In section 4, wetest our models on a dataset used in previous re-search.
We compare the three types of models in-dividually and also investigate the best performingmodel when used in combination with other featuresused in previous research.
We conclude in section 5.2 Related WorkMost previous work using distributional approachesto compositionality either contrasts distributionalinformation of candidate phrases with constituentwords (Schone and Jurafsky, 2001; Bannard et al,2003; Baldwin et al, 2003; McCarthy et al, 2003)or uses distributionally similar words to detect non-productive phrases (Lin, 1999).Lin (1999) used his method (Lin, 1998) for au-tomatic thesaurus construction.
He identified can-didate phrases involving several open-class wordsoutput from his parser and filtered these by the log-likelihood statistic.
Lin proposed that if there is aphrase obtained by substitution of either the heador modifier in the phrase with a ?nearest neighbour?from the thesaurus then the mutual information ofthis and the original phrase must be significantly dif-ferent for the original phrase to be considered non-compositional.
He evaluated the output manually.As well as distributional similarity, researchershave used a variety of statistics as indicators ofnon-compositionality (Blaheta and Johnson, 2001;Krenn and Evert, 2001).
Fazly and Stevenson (2006)use statistical measures of syntactic behaviour togauge whether a verb and noun combination is likelyto be a idiom.
Although they are not specificallydetecting compositionality, there is a strong corre-370lation between syntactic rigidity and semantic id-iosyncrasy.Venkatapathy and Joshi (2005) combine differ-ent statistical and distributional methods using sup-port vector machines (SVMs) for identifying non-compositional verb-object combinations.
They ex-plored seven features as measures of compositional-ity:1. frequency2.
pointwise mutual information (Church andHanks, 1990),3. least mutual information difference with simi-lar collocations, based on (Lin, 1999) and us-ing Lin?s thesaurus (Lin, 1998) for obtainingthe similar collocations.4.
The distributed frequency of an object, whichtakes an average of the frequency of occurrencewith an object over all verbs occurring with theobject above a threshold.5.
distributed frequency of an object, using theverb, which considers the similarity betweenthe target verb and the verbs occurring with thetarget object above the specified threshold.6.
a latent semantic approach (LSA) basedon (Schu?tze, 1998; Baldwin et al, 2003) andconsidering the dissimilarity of the verb-objectpair with its constituent verb7.
the same LSA approach, but considering thesimilarity of the verb-object pair with the ver-bal form of the object (to capture support verbconstructions e.g.
give a smileVenkatapathy and Joshi (2005) produced a datasetof verb-object pairs with human judgements of com-positionality.
We say more about this dataset andVenkatapathy and Joshi?s results in section 4 sincewe use the dataset for our experiments.In this paper, we investigate the use of selec-tional preferences to detect compositionality.
Ban-nard (2002) did some pioneering work to try andestablish a link between the compositionality ofverb particle constructions and the selectional pref-erences of the multiword and its constituent verb.His results were hampered by models based on (Liand Abe, 1998) which involved rather uninforma-tive models at the roots of WordNet.
There areseveral reasons for this.
The classes for the modelare selected using MDL by compromising between asimple model with few classes and one which ex-plains the data well.
The models are particularlyaffected by the quantity of data available (Wagner,2002).
Also noise from frequent but idiosyncratic orpolysemous arguments weakens the signal.
Thereis scope for experimenting with other approachessuch as (Clark and Weir, 2002), however, we feela type-based approach is worthwhile to avoid thenoise introduced from frequent but polysemous ar-guments and bias from highly frequent argumentswhich might be part of a multiword rather than a pro-totypical argument of the predicate in question, forexample eat hat.
In contrast to Bannard, our experi-ments are with verb-object combinations rather thanverb particle constructions.
We compare Li and Abemodels with WordNet models which use the num-ber of argument types to obtain the classes for rep-resentation of the selectional preferences.
In addi-tion to experiments with these WordNet models, wepropose models using entries in distributional the-sauruses for representing preferences.3 Three Methods for Acquiring SelectionalPreferencesAll models were acquired from verb-object data ex-tracted using the RASP parser (Briscoe and Carroll,2002) from the 90 million words of written Englishfrom the BNC (Leech, 1992).
We extracted verb andcommon noun tuples where the noun is the argu-ment head of the object relation.
The parser was alsoused to extract the grammatical relation data usedfor acquisition of the thesaurus described below insection 3.3.3.1 TCMsThis approach is a reimplementation of Li and Abe(1998).
Each selectional preference model (referredto as a tree cut model, or TCM) comprises a set ofdisjunctive noun classes selected from all the pos-sibilities in the WordNet hyponym hierarchy 3 us-ing MDL (Rissanen, 1978).
The TCM covers all the3We use WordNet version 2.1 for the work in this paper.371noun senses in the WordNet hierarchy and is associ-ated with a probability distribution over these nounsenses in the hierarchy reflecting the argument headdata occurring in the given grammatical relationshipwith the specified verb.
MDL finds the classes in theTCM by considering the cost measured in bits of de-scribing both the model and the argument head dataencoded in the model.
A compromise is made byhaving as simple a model as possible using classesfurther up the hierarchy whilst also providing a goodmodel for the set of argument head tokens (TK).The classes are selected by recursing from the topof the WordNet hierarchy comparing the cost (or de-scription length) of using the mother class to the costof using the hyponym daughter classes.
In any path,the mother is preferred unless using the daughterswould reduce the cost.
If using the daughters for themodel is less costly than the mother then the recur-sion continues to compare the cost of the hyponymsbeneath.The cost (or description length) for a set of classesis calculated as the model description length (mdl)and the data description length (ddl) 4 :-mdl + ddlk2 ?
log |TK| + ?
?tk?TK log p(tk) (1)k, is the number of WordNet classes being cur-rently considered for the TCM minus one.
The MDLmethod uses the size of TK on the assumption thata larger dataset warrants a more detailed model.
Thecost of describing the argument head data is calcu-lated using the log of the probability estimate fromthe classes currently being considered for the model.The probability estimate for a class being consideredfor the model is calculated using the cumulative fre-quency of all the hyponym nouns under that classthat occur in TK , divided by the number of nounsenses that these nouns have, to account for theirpolysemy.
This cumulative frequency is also dividedby the total number of noun hyponyms under thatclass in WordNet to obtain a smoothed estimate forall nouns under the class.
The probability of theclass is obtained by dividing this frequency estimateby the total frequency of the argument heads.
Thealgorithm is described fully by Li and Abe (1998).4See (Li and Abe, 1998) for a full explanation.0.17distancestreetvancarmilestreet car distancelane corner0.18 0.10 0.17 0.03entityphysical_entityentityabstract_way gondolaExample nounshyponym classeslocationvehicletcmself?propelledFigure 1: portion of the TCM for the objects of park.A small portion of the TCM for the object slot ofpark is shown in figure 1.
WordNet classes are dis-played in boxes with a label which best reflects themeaning of the class.
The probability estimates areshown for the classes on the TCM.
Examples of theargument head data are displayed below the Word-Net classes with dotted lines indicating membershipat a hyponym class beneath these classes.
We can-not show the full TCM due to lack of space, but weshow some of the higher probability classes whichcover some typical nouns that occur as objects ofpark.
Note that probability under the classes ab-stract entity, way and location arise because of asystematic parsing error where adverbials such asdistance in park illegally some distance from therailway station are identified by the parser as ob-jects.
Systematic noise from the parser has an im-pact on all the selectional preference models de-scribed in this paper.3.2 WNPROTOsWe propose a method of acquiring selectional pref-erences which instead of covering all the nounsenses in WordNet, just gives a probability distribu-tion over a portion of prototypical classes, we referto these models as WNPROTOs.
A WNPROTO con-sists of classes within the noun hierarchy which havethe highest proportion of word types occurring inthe argument head data, rather than using the num-ber of tokens, or frequency, as is used for the TCMs.This allows less frequent, but potentially informa-tive arguments to have some bearing on the modelsacquired to reduce the impact of highly frequent butpolysemous arguments.
We then used the frequencydata to populate these selected classes.372The classes (C) in the WNPROTO are selectedfrom those which include at least a threshold of 2argument head types 5 occurring in the training data.Each argument head in the training data is disam-biguated according to whichever of the WordNetclasses it occurs at or under which has the highest?type ratio?.
Let TY be the set of argument headtypes in the object slot of the verb for which we areacquiring the preference model.
The type ratio for aclass (c) is the ratio of noun types (ty ?
TY ) occur-ring in the training data also listed at or beneath thatclass in WordNet to the total number of noun typeslisted at or beneath that particular class in WordNet(wnty ?
c).
The argument types attested in thetraining data are divided by the number of Word-Net classes that the noun (classes(ty)) belongs to,to account for polysemy in the training data.type ratio(c) =?ty?TY ?c1|classes(ty)||wnty ?
c|(2)If more than one class has the same type ratio thenthe argument is not used for calculating the probabil-ity of the preference model.
In this way, only argu-ments that can be disambiguated are used for calcu-lating the probability distribution.
The advantage ofusing the type ratio to determine the classes used torepresent the model and to disambiguate the argu-ments is that it prevents high frequency verb nouncombinations from masking the information fromprototypical but low frequency arguments.
We wishto use classes which are as representative of the ar-gument head types as possible to help detect whenan argument head is not related to these classes andis therefore more likely to be non-compositional.For example, the class motor vehicle is selectedfor the WNPROTO model of the object slot of parkeven though there are 5 meanings of car in WordNetincluding elevator car and gondola.
There are 174occurrences of car which overwhelms the frequencyof the other objects (e.g.
van 11, vehicle 8) but bylooking for classes with a high proportion of types(rather than word tokens) car is disambiguated ap-propriately and the class motor vehicle is selectedfor representation.5We have experimented with a threshold of 3 and obtainedsimilar results.0.030.04tankerboatpram.61car0.05vancaravanentityphysical_entityExample nounshyponym classesvehicleself?propelledtransportvehiclewheeledmodelmotorvehicle caravanclasses inFigure 2: Part of WNPROTO for the object slot ofparkThe relative frequency of each class is obtainedfrom the set of disambiguated argument head tokensand used to provide the probability distribution overthis set of classes.
Note that in WNPROTO, classescan be subsumed by others in the hyponym hierar-chy.
The probability assigned to a class is appli-cable to any descendants in the hyponym hierarchy,except those within any hyponym classes within theWNPROTO.
The algorithm for selecting C and cal-culating the probability distribution is shown as Al-gorithm 1.
Note that we use brackets for comments.In figure 2 we show a small portion of the WN-PROTO for park.
Again, WordNet classes are dis-played in boxes with a label which best reflects themeaning of the class.
The probability estimates areshown in the boxes for all the classes included inthe WNPROTO.
The classes in the WNPROTO modelare shown with dashed lines.
Examples of the ar-gument head data are displayed below the WordNetclasses with dotted lines indicating membership ata hyponym class beneath these classes.
We cannotshow the full WNPROTO due to lack of space, butwe show some of the classes with higher probabilitywhich cover some typical nouns that occur as objectsof park.373Algorithm 1 WNPROTO algorithmC = (){classes in WNPROTO}D = () {disambiguated ty ?
TY }fD = 0 {frequency of disambiguated items}TY = argument head types {nouns occurring as objects of verb, with associated frequencies}C1 ?
WordNetwhere |ty ?
TY occurring in c ?
C1| > 1for all ty ?
TY dofind c ?
classes(ty) ?
C1 where c = argmaxc typeratio(c)if c & c /?
C thenadd c to Cadd ty ?
c to D {Disambiguated ty with c}end ifend forfor all c ?
C doif |ty ?
c ?
D| > 1 thenfD = fD + frequency(ty){sum frequencies of types under classes to be used in model}elseremove c from C {classes with less than two disambiguated nouns are removed}end ifend forfor all c ?
C dop(c) = frequency-of-all-tys-disambiguated-to-class(c,D)fD {calculating class probabilities}end forAlgorithm 2 DSPROTO algorithmC = (){classes in DSPROTO}D = () {disambiguated ty ?
TY }fD = 0 {frequency of disambiguated items}TY = argument head types {nouns occurring as objects of verb, with associated frequencies}C1 = cty ?
TY where num-types-in-thesaurus(cty, TY ) > 1order C1 by num-types-in-thesaurus(cty, TY ) {classes ordered by coverage of argument head types}for all cty ?
ordered C1 doDcty = () {disambiguated for this class}for all ty ?
TY where in-thesaurus-entry(cty, ty) doif ty /?
D thenadd ty to Dcty {types disambiguated to this class only if not disambiguated by a class used already}end ifend forif |Dcty| > 1 thenadd cty to Cfor all ty ?
Dcty doadd ty ?
cty to D {Disambiguated ty with cty}fD = fD + frequency(ty)end forend ifend forfor all cty ?
C dop(cty) = frequency-of-all-tys-disambiguated-to-class(cty,D)fD {calculating class probabilities}end for3743.3 DSPROTOsWe use a thesaurus acquired using the methodproposed by Lin (1998).
For input we used thegrammatical relation data from automatic parses ofthe BNC.
For each noun we considered the co-occurring verbs in the object and subject relation,the modifying nouns in noun-noun relations andthe modifying adjectives in adjective-noun relations.Each thesaurus entry consists of the target noun andthe 50 most similar nouns, according to Lin?s mea-sure of distributional similarity, to the target.The argument head noun types (TY ) are usedto find the entries in the thesaurus as the ?classes?
(C) of the selectional preference for a given verb.As with WNPROTOs, we only cover argument typeswhich form coherent groups with other argumenttypes since we wish i) to remove noise and ii) tobe able to identify argument types which are not re-lated with the other types and therefore may be non-compositional.
As our starting point we only con-sider an argument type as a class for C if its entry inthe thesaurus covers at least a threshold of 2 types.
6To select C we use a best first search.
This methodprocesses each argument type in TY in order of thenumber of the other argument types from TY that ithas in its thesaurus entry of 50 similar nouns.
An ar-gument head is selected as a class for C (cty ?
C) 7if it covers at least 2 of the argument heads that arenot in the thesaurus entries of any of the other classesalready selected for C .
Each argument head is dis-ambiguated by whichever class in C under which itis listed in the thesaurus and which has the largestnumber of the TY in its thesaurus entry.
When thealgorithm finishes processing the ordered argumentheads to select C , all argument head types are dis-ambiguated by C apart from those which after dis-ambiguation occur in isolation in a class withoutother argument types.
Finally a probability distri-bution over C is estimated using the frequency (to-kens) of argument types that occur in the thesaurusentries for any cty ?
C .
If an argument type oc-curs in the entry of more than one cty then it is as-signed to whichever of these has the largest number6As with the WNPROTOs, we experimented with a value of3 for this threshold and obtained similar results.7We use cty for the classes of the DSPROTO.
These classesare simply groups of nouns which occur under the entry of anoun (ty) in the thesaurus.class (p(c)) disambiguated objects (freq)van (0.86) car (174) van (11) vehicle (8) .
.
.mile (0.05) street (5) distance (4) mile (1) .
.
.yard (0.03) corner (4) lane (3) door (1)backside (0.02) backside (2) bum (1) butt (1) .
.
.Figure 3: First four classes of DSPROTO model forparkof disambiguated argument head types and its tokenfrequency is attributed to that class.
We show thealgorithm as Algorithm 2.The algorithms for WNPROTO algorithm 1 andDSPROTO (algorithm 2) differ because of the na-ture of the inventories of candidate classes (Word-Net and the distributional thesaurus).
There are agreat many candidate classes in WordNet.
The WN-PROTO algorithm selects the classes from all thosethat the argument heads belong to directly and indi-rectly by looping over all argument types to find theclass that disambiguates each by having the largesttype ratio calculated using the undisambiguated ar-gument heads.
The DSPROTO only selects classesfrom the fixed set of argument types.
The algorithmloops over the argument types with at least two ar-gument heads in the thesaurus entry and ordered bythe number of undisambiguated argument heads inthe thesaurus entry.
This is a best first search to min-imise the number of argument heads used in C butmaximise the coverage of argument types.In figure 3, we show part of a DSPROTO model forthe object of park.
8 Note again that the class milearises because of a systematic parsing error whereadverbials such as distance in park illegally somedistance from the railway station are identified bythe parser as objects.4 ExperimentsVenkatapathy and Joshi (2005) produced a dataset ofverb-object pairs with human judgements of com-positionality.
They obtained values of rs between0.111 and 0.300 by individually applying the 7 fea-tures described above in section 2.
The best corre-lation was given by feature 7 and the second bestwas feature 3.
They combined all 7 features usingSVMs and splitting their data into test and trainingdata and achieve a rs of 0.448, which demonstrates8We cannot show the full model due to lack of space.375significantly better correlation with the human gold-standard than any of the features in isolationWe evaluated our selectional preference modelsusing the verb-object pairs produced by Venkatapa-thy and Joshi (2005).
9 This dataset has 765 verb-object collocations which have been given a rat-ing between 1 and 6, by two annotators (both flu-ent speakers of English).
Kendall?s Tau (Siegel andCastellan, 1988) was used to measure agreement,and a score of 0.61 was obtained which was highlysignificant.
The ranks of the two annotators gave aSpearman?s rank-correlation coefficient (rs) of 0.71.The Verb-Object pairs included some adjectives(e.g.
happy, difficult, popular), pronouns and com-plements e.g.
become director.
We used the sub-set of 638 verb-object pairs that involved commonnouns in the object relationship since our preferencemodels focused on the object relation for commonnouns.
For each verb-object pair we used the pref-erence models acquired from the RASP parses of theBNC to obtain the probability of the class that thisobject occurs under.
Where the object noun is amember of several classes (classes(noun) ?
C)in the model, the class with the largest probabilityis used.
Note though that for WNPROTOs we havethe added constraint that a hyponym class from C isselected in preference to a hypernym in C .
Compo-sitionality of an object noun and verb is computedas:-comp(noun, verb) = maxc?classes(noun)?C p(c|verb) (3)We use the probability of the class, rather than anestimate of the probability of the object, because wewant to determine how likely any word belongingto this class might occur with the given verb, ratherthan the probability of the specific noun which maybe infrequent, yet typical, of the objects that occurwith this verb.
For example, convertible may bean infrequent object of park, but it is quite likelygiven its membership of the class motor vehicle.We do not want to assume anything about the fre-quency of non-compositional verb-object combina-tions, just that they are unlikely to be members ofclasses which represent prototypical objects.
We9This verb-object dataset is available fromhttp://www.cis.upenn.edu/?sriramv/mywork.html.method rs p < (one tailed)selectional preferencesTCM 0.090 0.0119WNPROTO 0.223 0.00003DSPROTO 0.398 0.00003features from V&Jfrequency (f1) 0.141 0.00023MI (f2) 0.274 0.00003Lin99 (f3) 0.139 0.00023LSA2 (f7) 0.209 0.00003combination with SVMf2,3,7 0.413 0.00003f1,2,3,7 0.419 0.00003DSPROTO f1,2,3,7 0.454 0.00003Table 1: Correlation scores for 638 verb object pairswill contrast these models with a baseline frequencyfeature used by Venkatapathy and Joshi.We use our selectional preference models to pro-vide the probability that a candidate is represen-tative of the typical objects of the verb.
That is,if the object might typically occur in such a rela-tionship then this should lessen the chance that thisverb-object combination is non-compositional.
Weused the probability of the classes from our 3 selec-tional preference models to rank the pairs and thenused Spearman?s rank-correlation coefficient (rs) tocompare these ranks with the ranks from the gold-standard.Our results for the three types of preference mod-els are shown in the first section of table 1.
10 All thecorrelation values are significant, but we note thatusing the type based selectional preference mod-els achieves a far greater correlation than using theTCMs.
The DSPROTO models achieve the best re-sults which is very encouraging given that they onlyrequire raw data and an automatic parser to obtainthe grammatical relations.We applied 4 of the features used by Venkatapa-thy and Joshi (2005) 11 and described in section 2to our subset of 638 items.
These features were ob-10We show absolute values of correlation following (Venkat-apathy and Joshi, 2005).11The other 3 features performed less well on this dataset sowe do not report the details here.
This seems to be because theyworked particularly well with the adjective and pronoun data inthe full dataset.376tained using the same BNC dataset used by Venkat-apathy and Joshi which was obtained using Bikel?sparser (Bikel, 2004).
We obtained correlation val-ues for these features as shown in table 1 underV&J.
These features are feature 1 frequency, feature2 pointwise mutual information, feature 3 based on(Lin, 1999) and feature 7 LSA feature which consid-ers the similarity of the verb-object pair with the ver-bal form of the object.
Pointwise mutual informa-tion did surprisingly well on this 84% subset of thedata, however the DSPROTO preferences still out-performed this feature.
We combined the DSPROTOand V&J features with an SVM ranking function andused 10 fold cross validation as Venkatapathy andJoshi did.
We contrast the result with the V&J fea-tures without the preference models.
The results inthe bottom section of table 1 demonstrate that thepreference models can be combined with other fea-tures to produce optimal results.5 Conclusions and Directions for FutureWorkWe have demonstrated that the selectional prefer-ences of a verbal predicate can be used to indi-cate if a specific combination with an object is non-compositional.
We have shown that selectional pref-erence models which represent prototypical argu-ments and focus on argument types (rather than to-kens) do well at the task.
Models produced fromdistributional thesauruses are the most promisingwhich is encouraging as the technique could be ap-plied to a language without a man-made thesaurus.We find that the probability estimates from ourmodels show a highly significant correlation, andare very promising for detecting non-compositionalverb-object pairs, in comparison to individual fea-tures used previously.Further comparison of WNPROTOs andDSPROTOs to other WordNet models are war-ranted to contrast the effect of our proposal fordisambiguation using word types with iterativeapproaches, particularly those of Clark and Weir(2002).
A benefit of the DSPROTOs is that theydo not require a hand-crafted inventory.
It wouldalso be worthwhile comparing the use of raw datadirectly, both from the BNC and from google?sWeb 1T corpus (Brants and Franz, 2006) sinceweb counts have been shown to outperform theClark and Weir models on a pseudo-disambiguationtask (Keller and Lapata, 2003).We believe that preferences should NOT be usedin isolation.
Whilst a low preference for a nounmay be indicative of peculiar semantics, this maynot always be the case, for example chew the fat.Certainly it would be worth combining the prefer-ences with other measures, such as syntactic fixed-ness (Fazly and Stevenson, 2006).
We also believe itis worth targeting features to specific types of con-structions, for example light verb constructions un-doubtedly warrant special treatment (Stevenson etal., 2003)The selectional preference models we have pro-posed here might also be applied to other tasks.
Wehope to use these models in tasks such as diathesisalternation detection (McCarthy, 2000; Tsang andStevenson, 2004) and contrast with WordNet mod-els previously used for this purpose.6 AcknowledgementsWe acknowledge support from the Royal SocietyUK for a Dorothy Hodgkin Fellowship to the firstauthor.
We thank the anonymous reviewers for theirconstructive comments on this work.ReferencesSteven Abney and Marc Light.
1999.
Hiding a semanticclass hierarchy in a Markov model.
In Proceedings ofthe ACL Workshop on Unsupervised Learning in Nat-ural Language Processing, pages 1?8.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
In Proceed-ings of the ACL Workshop on multiword expressions:analysis, acquisition and treatment, pages 89?96.Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003.
A statistical approach to the semantics ofverb-particles.
In Proceedings of the ACL Workshopon multiword expressions: analysis, acquisition andtreatment, pages 65?72.Colin.
Bannard.
2002.
Statistical techniquesfor automatically inferring the semantics of verb-particle constructions.
Technical Report WP-2002-06, University of Edinburgh, School of Informatics.http://lingo.stanford.edu/pubs/WP-2002-06.pdf.377Colin Bannard.
2005.
Learning about the meaning ofverb-particle constructions from corpora.
ComputerSpeech and Language, 19(4):467?478.Daniel M. Bikel.
2004.
A distributional analysis of a lex-icalized statistical parsing model.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2004), Barcelona, Spain,July.
Association for Computational Linguistics.Don Blaheta and Mark Johnson.
2001.
Unsuper-vised learning of multi-word verbs.
In Proceedingsof the ACL Workshop on Collocations, pages 54?60,Toulouse, France.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gramcorpus version 1.1.
Technical Report.Edward Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof the Third International Conference on LanguageResources and Evaluation (LREC), pages 1499?1504,Las Palmas, Canary Islands, Spain.Kenneth Church and Patrick Hanks.
1990.
Word asso-ciation norms, mutual information and lexicography.Computational Linguistics, 19(2):263?312.Stephen Clark and David Weir.
2002.
Class-based prob-ability estimation using a semantic hierarchy.
Compu-tational Linguistics, 28(2):187?206.Afsaneh Fazly and Suzanne Stevenson.
2006.
Automat-ically constructing a lexicon of verb phrase idiomaticcombinations.
In Proceedings of the 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics (EACL-2006), pages 337?344,Trento, Italy, April.Christiane Fellbaum, editor.
1998.
WordNet, An Elec-tronic Lexical Database.
The MIT Press, Cambridge,MA.Ralph Grishman and John Sterling.
1994.
Generalizingautomatically generated selectional patterns.
In Pro-ceedings of the 15th International Conference of Com-putational Linguistics.
COLING-94, volume I, pages742?747.Frank Keller and Mirella Lapata.
2003.
Using the web toobtain frequencies for unseen bigrams.
ComputationalLinguistics, 29(3):459?484.Brigitte Krenn and Stefan Evert.
2001.
Can we do betterthan frequency?
A case study on extracting PP-verbcollocations.
In Proceedings of the ACL Workshop onCollocations, pages 39?46, Toulouse, France.Geoffrey Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.Hang Li and Naoki Abe.
1998.
Generalizing case framesusing a thesaurus and the MDL principle.
Computa-tional Linguistics, 24(2):217?244.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of COLING-ACL 98,Montreal, Canada.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL-99,pages 317?324, Univeristy of Maryland, College Park,Maryland.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a continuum of compositionality in phrasalverbs.
In Proceedings of the ACL 03 Workshop: Multi-word expressions: analysis, acquisition and treatment,pages 73?80.Diana McCarthy.
2000.
Using semantic preferences toidentify verbal participation in role switching alter-nations.
In Proceedings of the First Conference ofthe North American Chapter of the Association forComputational Linguistics.
(NAACL), pages 256?263,Seattle,WA.Fernando Pereira, Nattali Tishby, and Lillian Lee.
1993.Distributional clustering of English words.
In Pro-ceedings of the 31st Annual Meeting of the Associationfor Computational Linguistics, pages 183?190.Philip Resnik.
1993.
Selection and Information:A Class-Based Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania.Jorma Rissanen.
1978.
Modelling by shortest data de-scription.
Automatica, 14:465?471.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword expres-sions: A pain in the neck for NLP.
In Proceedings ofthe Third International Conference on Intelligent TextProcessing and Computational Linguistics (CICLing2002), pages 1?15, Mexico City, Mexico.Patrick Schone and Daniel Jurafsky.
2001.
Isknowledge-free induction of multiword unit dictionaryheadwords a solved problem?
In Proceedings of the2001 Conference on Empirical Methods in NaturalLanguage Processing, pages 100?108, Hong Kong.Hinrich Schu?tze.
1998.
Automatic word sense discrimi-nation.
Computational Linguistics, 24(1):97?123.Sidney Siegel and N. John Castellan.
1988.
Non-Parametric Statistics for the Behavioral Sciences.McGraw-Hill, New York.Suzanne Stevenson, Afsaneh Fazly, and Ryan North.2003.
Statistical measures of the semi-productivity oflight verb constructions.
In Proceedings of the ACL2004 Workshop on Multiword Expressions: Integrat-ing Processing, Barcelona, Spain.378Vivian Tsang and Suzanne Stevenson.
2004.
Using se-lectional profile distance to detect verb alternations.
InProceedings of NAACL Workshop on ComputationalLexical Semantics (CLS-04), pages 30?37, Boston,MA.Sriram Venkatapathy and Aravind K. Joshi.
2005.
Mea-suring the relative compositionality of verb-noun (v-n)collocations by integrating features.
In Proceedings ofthe joint conference on Human Language Technologyand Empirical methods in Natural Language Process-ing, pages 899?906, Vancouver, B.C., Canada.Andreas Wagner.
2002.
Learning thematic role relationsfor wordnets.
In Proceedings of ESSLLI-2002 Work-shop on Machine Learning Approaches in Computa-tional Linguistics, Trento.379
