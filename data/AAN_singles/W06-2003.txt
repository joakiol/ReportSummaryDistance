Induction of Cross-Language Affix andLetter Sequence CorrespondenceAri RappoportInstitute of Computer ScienceThe Hebrew Universitywww.cs.huji.ac.il/~arirTsahi Levent-LeviInstitute of Computer ScienceThe Hebrew UniversityAbstractWe introduce the problem of explicitmodeling of form relationships betweenwords in different languages, focusinghere on languages having an alphabeticwriting system and affixal morphology.We present an algorithm that learns thecross-language correspondence betweenaffixes and letter sequences.
The algo-rithm does not assume prior knowledgeof affixes in any of the languages, usingonly a simple single letter correspon-dence as seed.
Results are given for theEnglish-Spanish language pair.1 IntroductionStudying various relationships between lan-guages is a central task in computational linguis-tics, with many application areas.
In this paperwe introduce the problem of induction of formrelationships between words in different lan-guages.
More specifically, we focus on lan-guages having an alphabetic writing system andaffixal morphology, and we construct a modelfor the cross-language correspondence betweenletter sequences and between affixes.
Since thewriting system is alphabetic, letter sequences arehighly informative regarding sound sequences aswell.Concretely, the model is designed to answerthe following question: what are the affixes andletter sequences in one language that correspondfrequently to similar entities in another lan-guage?
Such a model has obvious applications tothe construction of learning materials in languageeducation and to statistical machine translation.The input to our algorithm consists of wordpairs from two languages, a sizeable fraction ofwhich is assumed to be related graphemicallyand affixally.
The algorithm has three mainstages.
First, an alignment between the wordpairs is computed by an EM algorithm that usesan edit distance metric based on an increasinglyrefined individual letter correspondence costfunction.
Second, affix pair candidates are dis-covered and ranked, based on a language inde-pendent abstract model of affixal morphology.Third, letter sequences that correspond produc-tively in the two languages are discovered andranked by EM iterations that use a cost functionbased on the discovered affixes and on compati-bility of alignments.The affix learning part of the algorithm is to-tally unsupervised, in that we do not assumeknowledge of affixes in any of the single lan-guages involved.
The letter sequence learningpart utilizes a simple initial correspondence be-tween individual letters, and the rest of its opera-tion is unsupervised.We believe that this is the first paper that ex-plicitly addresses cross-language morphology,and the first that presents a comprehensive inter-language word form correspondence model thatcombines morphology and letter sequences.Section 2 motivates the problem and defines itin detail.
In Section 3 we discuss relevant previ-ous work.
The algorithm is presented in Section4, and results for English-Spanish in Section 5.2 Problem Motivation and DefinitionWe would like to discover characteristics ofword form correspondence between languages.In this section we discuss what exactly thismeans and why it is useful.17Word form.
Word forms have at least three dif-ferent aspects: sound, writing system, and inter-nal structure, corresponding to the linguisticsfields of phonology, orthography and morphol-ogy.
When the writing system is phoneticallybased, the written form of a word is highly in-formative of how the word sounds.
Individualwriting units are referred to as graphemes.Morphology studies the internal structure ofwords when viewed as comprised of semanticscarrying components.
Morphological units canbe classified into two general classes, stems (orroots) and bound morphemes, which combine tocreate words using various kinds of operators.The linear affixing operator combines stems andbound morphemes (affixes) using linear orderingwith possible fusion effects, usually at the seams.Word form correspondence.
In this paper westudy cross-language word form correspondence.We should first ask why there should be any re-lationship at all between word forms in differentlanguages.
There are at least two factors that cre-ate such relationships.
First, languages may sharea common ancestor.
Second, languages may bor-row words, writing systems and even morpho-logical operators from each other.
Note that us-age of proper names can be viewed as a kind ofborrowing.
In both cases form relationships areaccompanied by semantic relatedness.
Wordsthat possess a degree of similarity of form andmeaning are usually termed cognates.Our goal in examining word forms in differentlanguages is to identify correspondence phenom-ena that could be useful for certain applications.These would usually be correspondence similari-ties that are common to many word pairs.Problem statement for the present paper.
Forreasons of paper length, we focus here on lan-guages having the following two characteristics.First, we assume an alphabetic writing system.This implies that grapheme correspondences willbe highly informative of sound correspondencesas well.
From now on we will use the term ?let-ter?
instead of ?grapheme?.
Second, we assumelinear affixal morphology (prefixing and suffix-ing), which is an extremely frequent morpho-logical operator in many languages.We address the two fundamental word formentities in languages that obey those assump-tions: affixes and letter sequences.
Our goal is todiscover frequent cross-language pairs of thoseentities and quantify the correspondence.
Pairingof letter sequences is expected to be mostly dueto regular sound transformations and spellingconventions.
Pairing of affixes could be due tomorphological principles ?
predictable relation-ships between the affixing operators (their formand meaning) ?
or, again, due to sound transfor-mations and spelling.The input to the algorithm consists of a set ofordered pairs of words, one from each language.We do not assume that all input word pairs ex-hibit the correspondence relationships of interest,but obviously the quality of results will dependon the fraction of the pair set that does exhibitthem.
A particular word may participate in morethan a single pair.
As explained above, the rela-tionships of interest to us in this paper usuallyimply semantic affinity between the words;hence, a suitable pair set can be generated byselecting word pairs that are possible translationsof each other.
Practical ways to obtain such pairsare using a bilingual dictionary or a word alignedparallel corpus.
We had used the former, whichimplies that we addressed only derivational, notinflectional, morphology.
Using a dictionary pro-vides a kind of semantic supervision that allowsus to focus on the desired form relationships.We also assume that the algorithm is providedwith a prototypical individual letter mapping asseed.
Such a mapping is trivial to obtain in virtu-ally all practical situations, either because bothlanguages utilize the same alphabet or by using amanually prepared, coarse alphabet mapping(e.g., anybody even shallowly familiar with Cy-rillic or Semitic scripts can prepare such a map-ping in just a few minutes.
)We do not assume knowledge of affixes in anyof the languages.
Our algorithm is thus fully un-supervised in terms of morphology and veryweakly seeded in term of orthography.Motivating applications.
There are two mainapplications that motivate our research.
In sec-ond language education, a major challenge foradult learners is the high memory load due to thehuge number of lexical items in a language.
Itemmemorization is known to be greatly assisted bytying items with existing knowledge (Matlin02).When learning a second language lexicon, it isbeneficial to consciously note similarities be-tween new and known words.
Discovering andexplaining such similarities automatically wouldhelp teachers in preparing reliable study materi-als, and learners in remembering words.Recognition of familiar components also helpslearners when encountering previously unseenwords.
For example, suppose an English speakerwho learns Spanish and sees the word ?parcial-18mente?.
A word form correspondence modelwould tell her that ?mente?
is an affix stronglycorresponding to the English ?ly?, and that theletter pair ?ci?
often corresponds to the English?ti?.
The model thus enables guessing or recallingthe English word ?partially?.Our model could also warn the learner of cog-nates that are possibly false, by recognizing simi-lar words that are not paired in the dictionary.A second application area is machine transla-tion.
Both cognate identification (Kondrak et al03) and morphological information in one of thelanguages (Niessen00) have been proven usefulin statistical machine translation.3 Previous WorkCross-language models for phonology and or-thography have been developed for back-transliteration in cross-lingual information re-trieval (CLIR), mostly from Japanese and Chi-nese to English.
(Knight98) uses a series ofweighted finite state transducers, each focusingon a particular mapping.
(Lin02) uses minimaledit distance with a ?confusion matrix?
that mod-els phonetic similarity.
(Li04, Bilac04) general-ize using the sequence alignment algorithm pre-sented in (Brill00) for spelling correction.
(Bi-lac04) explicitly separates the phonemic and gra-phemic models.
None of that work addressesmorphology and in all of it grapheme and pho-neme correspondence is only a transient toolwhich is not studied on its own.
(Mueller05)explicitly models phonological similarities be-tween related languages, but does not addressmorphology and orthography.Cognate identification has been studied incomputational historical linguistics.
(Coving-ton96, Kondrak03a) use a fixed, manually de-termined single entity mapping.
(Kondrak03b)generalizes to letter sequences based on the algo-rithm in (Melamed97).
The results are good forthe historical linguistics application.
However,morphology is not addressed, and the sequencecorrespondence model is less powerful than thatemployed in the back-transliteration and spellingcorrection literature.
In addition, all effects thatoccur at word endings, including suffixes, arecompletely ignored.
(Mackay05) presents goodresults for cognate identification using a wordsimilarity measure based on pair hidden Markovmodels.
Again, morphology was not modeledexplicitly.A nice application for cross-language mor-phology is (Schulz04), which acquires a Spanishmedical lexicon from a Portuguese seed lexiconusing a manually prepared table of 842 Spanishaffixes.Unsupervised learning of affixal morphologyin a single language is a heavily researched prob-lem.
(Medina00) studies several methods, includ-ing the squares method we use in Section 4.
(Goldsmith01) presents an impressive systemthat searches for ?signatures?, which can beviewed as generalized squares.
(Creutz04) pre-sents a very general method that excels at dealingwith highly inflected languages.
(Wicen-towsky04) deals with inflectional and irregularmorphology by using semantic similarity be-tween stem and stem+affix, also addressingstem-affix fusion effects.
None of these papersdeals with cross-language morphology.4 The AlgorithmOverview.
Letter sequences and affixes are dif-ferent entities exhibiting different correspon-dence phenomena, hence are addressed at sepa-rate stages.
The result of addressing one will as-sist us in addressing the other.The fundamental tool that we use to discovercorrespondence effects is alignment of the twowords in a pair.
Stage 1 of the algorithm createsan alignment using the given coarse individualletter mapping, which is simultaneously im-proved to a much more accurate one.Stage 2 discovers affix pairs using a generallanguage independent affixal morphology model.In stage 3 we utilize the improved individualletter relation from stage 1 and the affix pairsdiscovered in stage 2 to create a general lettersequence mapping, again using word alignments.In the following we describe in detail each ofthese stages.Initial alignment.
The main goal of stage 1 is toalign the letters of each word pair.
This is doneby a standard minimal edit distance algorithm,efficiently implemented using dynamic pro-gramming (Gusfield97, Ristad98).
We use thestandard edit distance operations of replace, in-sert and delete.
The letter mapping given as inputdefines a cost matrix where replacement of cor-responding letters has a low (0) cost and of allothers a high (1) cost.
The cost of insert and de-lete is arbitrarily set to be the same as that of re-placing non-identical letters.
We use a hash tablerather than a matrix, to prepare for later stages ofthe algorithm.When the correspondence between the lan-guages is very high, this initial alignment can19already provide acceptable results for the nextstage.
However, in order to increase the accuracyof the alignment we now refine the letter costmatrix by employing an EM algorithm that itera-tively updates the cost matrix using the currentalignment and computes an improved alignmentbased on the updated cost matrix (Brill00, Lin02,Li04, Bilac04).
The cost of mapping a letter K toa letter L is updated to be proportional to thecount of this mapping in all of the current align-ments divided by the total number of mappingsof the letter K.Affix pairs.
The computed letter alignment as-sists us in addressing affixes.
Recall that we pos-sess no knowledge of affixes; hence, we need todiscover not only pairing of affixes, but the par-ticipating affixes as well.
Our algorithm discov-ers affixes and their pairing simultaneously.
It isinspired by the squares algorithm for affix learn-ing in a single language (Medina00)1.The squares method assumes that affixes gen-erally combine with very many stems, and thatstems are generally utilized more than once.These assumptions are due to a functional viewof affixal morphology as a process whose goal isto create a large number of word forms usingfewer parameters.
A stem that combines with anaffix is quite likely to also appear alone, so theempty affix is allowed.We first review the method as it is used in asingle language.
Given a word W=AB (where Aand B are non-empty letter sequences), our taskis to measure how likely it is for B to be a suffix(prefix learning is similar.)
We refer to AB as asegmentation of W, using a hyphen to showsegmentations of concrete words.
Define asquare to be four words (including W) of theforms W=AB, U=AD, V=CB, and Y=CD (one ofthe letter sequences C, D is allowed to beempty.
)Such a square might attest that B, D are suf-fixes and that A, C are stems.
However, we mustbe careful: it might also attest that B, D are stemsand A, C are prefixes.
A square attests for a seg-mentation, not for a particular labeling of itscomponents.As an example, if W is ?talking?, a possiblesquare is {talk-ing, hold-ing, talk-s, hold-s}where A=talk, B=ing, C=hold, and D=s.
Anotherpossible square is {talk-ing, danc-ing, talk-ed,danc-ed}, where A=talk, B=ing, C=danc, andD=ed.
This demonstrates a drawback of the1 (Medina00) attributes the algorithm to Joseph Greenberg.method, namely its sensitivity to spelling; C withthe empty suffix is written ?dance?, not ?danc?.The four words {talking, dancing, talk, dance}do not form a square.We now count the number of squares in whichB appears.
If this number is relatively large(which needs to be precisely defined), we have astrong evidence that B is a suffix or a stem.
Wecan distinguish between these two cases usingthe number of witnesses ?
actual words in whichB appears.We generalize the squares method to the dis-covery of cross-language affix pairs, as follows.We now use W to denote not a single word but aword pair W1:W2.
B does not denote a suffixcandidate but a suffix pair candidate, B1:B2, andsimilarly for D. A and C denote stem pair candi-dates A1:A2 and C1:C2, respectively.We now define a key concept.
Given a wordpair W=W1:W2 aligned under an alignment T,two segmentations W1=A1B1 and W2=A2B2are said to be compatible if no alignment line ofT connects a subset of A1 to a subset of B2 or asubset of A2 to a subset of B1.
This definition isalso applicable to alignments between letter se-quences.We now impose our key requirement: for allof the words involved in the cross-lingual square,their segmentations into two parts must be com-patible under the alignment computed at stage 1.For example, consider the English-Spanishword pair affirmation : afirmacion.
The segmen-tation affirma-tion : afirma-cion is attested by thesquare A1B1 : A2B2 = affirma-tion : afirma-cion A1D1 : A2D2 = affirma-tively :afirma-tivamente C1B1 : C2B2 = coopera-tion : coopera-cion C1D1 : C2D2 = coopera-tively :coopera-tivamenteassuming that the appropriate parts are aligned.Note that ?tively?
is comprised of two smalleraffixes, but the squares method legitimately con-siders it an affix by itself.
Note also that since allof A1, A2, C1 and C2 end with the same letter,that letter can be moved to the beginning of B1,B2, D1, D2 to produce a different square (affirm-ation : afirm-acion, etc.)
from the same fourword pairs.Since we have no initial reason to favor a par-ticular affix candidate over another, and since thetotal computational cost is not prohibitive, we20now simply count the number of attestingsquares for all possible compatible segmenta-tions of all word pairs, and sort the list accordingto the number of witnesses.
To reduce noise, weremove affix candidates for which the absolutenumber of witnesses or squares is small (e.g.,ten.
)Letter sequences.
The third and last stage of thealgorithm discovers letter sequences that corre-spond frequently.
This is again done by an editdistance algorithm, generalizing that of stage 1so that sequences longer than a single letter canbe replaced, inserted or deleted.
In order to re-duce noise, prior to that we remove word pairswhose stems are very different.
Those are identi-fied by comparing their edit distance costs,which should hence be normalized according tolength (of the longer stem in a pair.)
Note thataccuracy is increased by considering only stems:affix pairs might be very different, thus mightincrease edit distance cost even when the stemsdo exhibit good sequence pairing effects.When generalizing the edit distance algorithm,we need to specify which letter sequences will beconsidered, because it does not make sense toconsider all possible mappings of all subsets toall possible subsets ?
the number of differentsuch pairs will be too large to show any mean-ingful statistics.The letter sequences considered were obtainedby ?fattening?
the lines in alignments yieldingminimal edit distances, using an EM algorithm asdone in (Brill00, Bilac04, Li04).
The details ofthe algorithm can be found in these papers.
Themost important step, line fattening, is done asfollows.
We examine all alignment lines, eachconnecting two letter sequences (initially, oflength 1.)
We unite those sequences with adja-cent sequences in all ways that are compatiblewith the alignment, and add the new sequencesto the cost function to be used in the next EMiteration.If we kept letter sequence pairs that are notfrequent in the cost function, they would distortthe counts of more frequent letter sequences withwhich they partially overlap.
We thus need toretain only some of the sequence pairs discov-ered.
We have experimented with several waysto do that, all yielding quite similar results.
Forthe results presented in this paper, we used theidea that sequences that clearly map to specificsequences are more important to our model thansequences that ?fuzzily?
map to many sequences.To quantify this approach, for each language-1sequence we sorted the corresponding language-2 sequences according to count, and removedpairs in which the language-2 item was responsi-ble for only a small percentage of the total (weused a threshold of 0.05).
We further removedsequence pairs whose absolute counts are low.Discussion.
We deal with affixes before se-quences because, as we have seen, identificationof affixes helps us in identifying sequences,while the opposite order actually hurts us: se-quences sometimes contain letters from bothstem and affix, which invalidates squares that areotherwise valid.It may be asked why the squares stage isneeded at all ?
perhaps affixes would be discov-ered anyway as sequences in stage 3.
Our as-sumption was that affixes are best discoveredusing properties resulting from their very nature.We have experimented with the option of remov-ing stage 2 and discovering affixes as letter se-quences in stage 3, and verified that it givesmarkedly lower quality results.
Even the veryfrequent pair -ly:-mente was not signaled out,because its count was lowered by those of thepairs -ly:-ente, -ly:nte, -y:-te, etc.5 ResultsWe have run the algorithm on several languagepairs using affixal morphology and the Latin al-phabet: English vs. Spanish, Portuguese and Ital-ian, and Spanish vs. Portuguese.
All of them arerelated both historically and through borrowing(obviously at varying degrees), so we expectrelatively many correspondence phenomena.Testing results for one of these pairs, English ?Spanish, are presented in this section.The input word pair set was created from a bi-lingual dictionary (Freelang04) by taking alltranslations of single English words to singleSpanish words, generating about 13,000 wordpairs.Individual letter mapping.
The cost matrix af-ter EM convergence (25 iterations) exhibits thefollowing phenomena (e:s (c) denotes that thefinal cost of replacing the English letter e by theSpanish letter s is c): (1) English letters mostlymap to identical Spanish letters, apart from let-ters that Spanish does not make use of like k andw; (2) some English vowels map frequently tosome Spanish vowels: y maps almost exclusivelyto i (0.01), e:a (0.47) is highly productive, e:o(0.98), i:e (0.97), e:o (0.98); (3) some Englishconsonants map to different Spanish ones: t:c21(0.89) (due to an affix, -tion:-cion); m:n (0.44) ishighly frequent; b:v(0.80); x:j (0.78), x:s(0.94);w always maps to v; j:y (0.11); (4) h usually dis-appears, h:NULL (0.13); and (5) inserted Span-ish letters include the vowels o, e, a and i, at thatorder, where o overwhelms the others.
The Eng-lish o maps exclusively to the Spanish o and notto other vowels.Affixes.
Table 1 shows some of the conspicuousaffix pairs discovered by the algorithm.
We showboth the number of witnesses and of squares.The table shows many interesting correspon-dence phenomena.
However, discussing those atdepth from a linguistic point of view is out of thescope of this paper.
Some notes: (1) some of themost frequent affix pairs are not that close ortho-graphically: -ity:-idad, -ness:- (nouns), -ate:-ar(verbs), -ly-:-mente (adverbs), -al:-o (adjectives),so will not necessarily be found using ordinaryedit distance methods; (2) some affixes areranked high both with and without a letter thatthey favor when attaching to a stem: -ation:-acion, -ate:-ar; (3) some English suffixes mapstrongly to several Spanish ones: -er:-o, -er:-ador.Recall that the table cannot include inflec-tional affixes, since our input was taken from abilingual dictionary, not from a text corpus.Letter sequences.
Table 2 shows some nice pair-ings, stemming from all three expected phenom-ena: st-:est- (due to phonology), ph:f, th:t, ll:l(due to orthography), and tion:cion, tia:cia (dueto morphology: affixes located in the middle ofwords.
)Such affix and letter sequence pairing resultscan clearly be useful for English speakers learn-ing Spanish (and vice versa), for rememberingwords by associating them to known ones, foravoidance of spelling mistakes, and for analyzingpreviously unseen words.Evaluation.
An unsupervised learning model canbe evaluated on the strength of the phenomenathat it discovers, on its predictive power for un-seen data, or by comparing its data analysis re-sults with results obtained using other means.
Wehave performed all three evaluations.For evaluating the discovered phenomena, arepository of known phenomena is needed.
Theonly such repository of which we are aware arelanguage learning texts.
Unfortunately, the phe-nomena these present are limited to the few mostconspicuous pairs (e.g., -ly:-mente, -ity:-idad,ph:f), all of which are easily discovered by ourmodel.
The next best thing are studies that pre-sent data of a single language.
We took the affixinformation given in a recent, highly detailed,corpus based English grammar (Biber99), andcompared it manually to ours.
Of the 35 mostproductive affixes, our model finds 27.
Carefulstudy of the word pair list showed that the re-maining 8 (-ment, -ship, -age, -ful, -less, -en, dis-, mis-) indeed do not map to Spanish ones fre-quently.
Note that some of those are indeed ex-tremely frequent inside English yet do not corre-spond significantly with any Spanish affix.As a second test, we took a comprehensiveEnglish-Spanish dictionary (Collins), selected 10pages at random (out of 680), studied them, andlisted the prominent word form phenomena (85).All but one (the verbal suffix in seduce:seducir)were found by our model.The numbers reported above for the two testsare recall numbers.
To evaluate affix precision,we have manually graded the top 100 affix pairs(as sorted at the end of stage 2 of the algorithm.
)8 of those were clearly not affixes; however, 3 ofthe 8 (-t:-te, ?t:-to, -ve:-vo) were important pho-nological phenomena that should indeed appearin our final model.
Of the remaining 92, 15 werevalid but ?duplicates?
in the sense of being sub-strings of other affixes (e.g., -ly:-mente, -ly:-emente.)
In the next 50 pairs, only 6 were clearlynot affixes.
Note that by their very definition, weshould not expect the number of frequent deriva-tional affixes to be very large, so there is notmuch point in looking further down the list.Nonetheless, inspection of the rest of the list re-veals that it is not dominated by noise but by du-plicates, with many specialized, less frequentaffixes (e.g., -graphy:-grafia) being discovered.Regarding letter sequences, precision was veryhigh: of the 38 different pairs discovered, onlyone (hr:r) was not regular, and there were 11 du-plicates.
Recall was impressive, but harder toverify due to the lack of standards.
We foundonly one (not very frequent) pair that was notdiscovered (-sp:-esp).To evaluate the model on its data analysis ca-pability, we took out 100 word pairs at random,trained the model without them, analyzed themusing the final cost function, and compared withprominent phenomena noted manually (again, wehad to grade manually due to the lack of a goldstandard.)
The model identified those prominentphenomena (including a total lack thereof) in 91of the pairs.
Notable failures included the pairssuperscribe : sobrescribir and coded : codificado,where none of the prefixes and suffixes were22identified.
Some successful examples are listedbelow (affixes are denoted by [], sequences by<>, and insert by _: or :_):installation : instalacion.
<ll:l>, [ation:acion]volution : circonvolucion.
_:c, _:i, _:r, _:c, _:o, _:n,[tion:cion]intelligibility : inteligibilidad.
[in:in], <ll:l>,[ity:idad]sapper : zapador.
<s:z>, <pp:p>, [er:ador]harpist : arpista.
<h:_>, [ist:ista]pathologist : patologo.
<th:t>, [ist:o]elongate : prolongar.
[te:r]industrialize: industrializar.
[in:in], <ial>, [e:ar]demographic : demografico.
<ph:f>, [ic:ico]gynecological :ginecologico.
<yn:in>, [ical:ico]peeled : pelado.
[ed:ado]The third and final evaluation method is tocompare the model?s results with results obtainedusing other means.
We are not aware of any databank in which cross-language affix or letter se-quence correspondences are explicitly tagged, sowe had used a relatively simple algorithm as abaseline: We invoked the squares method foreach language independently, ending up withaffix candidates.
For every word pair E:S, if Econtains an affix candidate C and S contains anaffix candidate D, we increment the count of thecandidate affix pair C:D. Finally, we sort thecandidates according to their count.Baseline recall is obviously as good as in ouralgorithm (it produces a superset), but precisionis so bad so as to render the baseline method use-less: out of the first 100, only 19 were affixes,the rest being made up of noise and badly seg-mented ?duplicates?.In summary, the results are good, but goldstandards are needed for a more consistentevaluation of different cross-language word formalgorithms.
Results for the other language pairswere overall good as well.6 DiscussionWe have introduced the problem of cross-language modeling of word forms, presented analgorithm for addressing affixal morphology andletter sequences, and described good results onEnglish-Spanish dictionary word pairs.Natural directions for future work on themodel include: (1) test the algorithm on morelanguage pairs, including languages utilizingnon-Latin alphabets; (2) modify the input modelto assume that single language affixes areknown; (3) address additional morphologicaloperators, such as templatic morphology; (4) ad-dress phonology directly instead of indirectly; (5)use pairs acquired from a parallel corpus ratherthan a dictionary, to address inflectional mor-phology and to see how the algorithm performswith more noisy data; (6) extend the algorithm toother types of writing systems; (7) examine moresophisticated affix discovery algorithms, such as(Goldsmith01); and (8) improve the evaluationmethodology.There are many possible applications of themodel: (1) for statistical machine translation; (2)for computational historical linguistics; (3) forCLIR back-transliteration; (4) for constructinglearning materials and word memorization meth-ods in second language education; and (5) forimproving word form learning algorithms insidea single language.The length and diversity of the lists above pro-vide an indication of the benefit and importanceof cross-language word form modeling in com-putational linguistics and its application areas.ReferencesBiber Douglas, 1999.
Longman Grammar of Spokenand Written English.
(Pages 320, 399, 530, 539.
)Bilac Slaven, Tanaka Hozumi, 2004.
A Hybrid Back-Transliteration System for Japanese.
COLING2004.Brill Eric, Moore Robert, 2000.
An Improved ErrorModel for Noisy Channel Spelling Correction.ACL 2000.Covington Michael A, 1996.
An Algorithm to AlignWords for Historical Comparison.
Comput.
Ling.,22(4):481?496.Creutz Mathias, Lugas Krista, 2004.
Induction of aSimple Morphology for Highly-Inflecting Lan-guages.
ACL 2004 Workshop on Comput.
Phonol-ogy and Morphology.Freelang, 2004.  http: // www.freelang.net / dictionary/ spanish.html.Goldsmith John.
2001.
Unsupervised Learning of theMorphology of a Natural Language.
Comput.
Ling.153-189 (also see an unpublished 2004 documentathttp://humanities.uchicago.edu/faculty/goldsmith.
)Gusfield, Dan, 1997.
Algorithms on Strings, Trees,and Sequences.
Cambridge University Press.Knight Kevin, Graehl Jonathan, 1998.
MachineTransliteration.
Comput.
Ling.
24(4):599?612.Kondrak Grzegorz, 2003a.
Phonetic Alignment andSimilarity.
Comput.
& the Humanities 37:273?291.Kondrak Grzegorz, 2003b.
Identifying ComplexSound Correspondences in Bilingual Wordlists..Comput.
Ling.
& Intelligent Text Processing (CI-CLing 2003).Kondrak Grzegorz, Marcu Daniel, Knight Kevin,2003.
Cognates Can Improve Statistical Transla-23tion Models.
Human Language Technology (HLT)2003.Li Haizhou et al 2004.
A Joint Source-ChannelModel for Machine Transliteration.
ACL 2004.Lin Wei-Hao, Chen Hsin-Hsi, 2002.
Backward Ma-chine Transliteration by Learning Phonetic Similar-ity.
CoNLL 2002.Mackay Wesley, Kondrak Grzegorz, 2005.
Comput-ing Word Similarity and Identifying Cognates withPair Hidden Markov Models.
CoNLL 2005.Matlin Margaret W., 2002.
Cognition, 6th ed.
JohnWiley & Sons.Medina Urrea Alfonso, 2000.
Automatic Discovery ofAffixes by Means of a Corpus: A Catalog of Span-ish Affixes.
J. of Quantitative Linguistics 7(3):97 ?114.Melamed Dan, 1997.
Automatic Discovery of Non-Compositional Compounds in Parallel Data.EMNLP 1997.Mueller Karin, 2005.
Revealing Phonological Simi-larities between Related Languages from Auto-matically Generated Parallel Corpora.
ACL ?05Workshop on Building and Using Parallel Texts.Niessen Sonja, Ney Hermann, 2000.
Improving SMTQuality with Morph-syntactic analysis.
COLING2000.Ristad Eric Sven, Yianilos Peter, 1998.
LearningString Edit Distance.
IEEE PAMI, 20(5):522?532.Schulz Stefan, et al2004.
Cognate Mapping.
COL-ING 2004.Wicentowsky Richard, 2004.
Multilingual Noise-Robust Supervised Morphological Analysis usingthe WordFrame Model.
ACL 2004 Workshop onComput.
Phonology and Morphology.Eng.
Span.
Wit.
Squ.
Example-tion - 623 309 reformation:reforma-e -ar 461 1182 convene:convocar-tion -cion 434 3770 vibration:vibracionco- co- 363 95 coexistence:coexistencia-ness - 352 128 persuasiveness:persuasiva-ation -acion 333 4854 formulation:formulacionin- In- 332 1294 inapt:ineptore- Re- 312 194 recreative:recreativo-ed -ado 289 102 abridged:abreviado-ic -ico 274 3192 strategic:estrategico-ly -mente 269 207 aggressively:agresivamente-y -ia 251 2086 agronomy:agronomia-ble -ble 238 153 incredible:increible-al -al 233 440 genital:genital-ity -idad 208 687 stability:estabilidad-te -r 206 3603 tabulate:tabular-er -o 203 166 biographer:biografo-al -o 186 2728 practical:practicode- de- 174 68 deformation:deformacion-ate -ar 170 3593 manipulate:manipular-ous -o 154 59 analogous:analogocon- con- 153 53 conceivable:concebible-ism -ismo 147 2173 tourism:turismoun- In- 134 164 undistinguishable:indistinto-er -ador 134 95 progammer:programador-nt -nte 120 514 tolerant:tolerante-ical -ico 111 3185 lyrical:lirico-ist -ista 111 1691 tourist:turista-ize -izar 90 974 privatize:privatizar-ce -cia 87 445 belligerence:beligerancia-tive -tivo 70 249 superlative:superlativoTable 1: Some affix pairs discovered.Eng.
Span.
Exampleph f aphoristic:aforisticoth t lithography:litografiall l collaboration:colaboraciontion cion unconditional:incodicionalst- est- stylist:estilistatia cia unnegotiable:innegociableTable 2: Some letter sequence pairs discovered.24
