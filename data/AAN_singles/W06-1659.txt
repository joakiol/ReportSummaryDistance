Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 501?508,Sydney, July 2006. c?2006 Association for Computational LinguisticsUnsupervised Information Extraction Approach Using Graph MutualReinforcementHany Hassan Ahmed Hassan Ossama EmamIBM Cairo Technology Development CenterGiza, EgyptP.O.
Box 166 Al-Ahramhanyh@eg.ibm.com hasanah@eg.ibm.com emam@eg.ibm.comAbstractInformation Extraction (IE) is the task ofextracting knowledge from unstructuredtext.
We present a novel unsupervisedapproach for information extractionbased on graph mutual reinforcement.The proposed approach does not requireany seed patterns or examples.
Instead, itdepends on redundancy in large data setsand graph based mutual reinforcement toinduce generalized ?extraction patterns?.The proposed approach has been used toacquire extraction patterns for the ACE(Automatic Content Extraction) RelationDetection and Characterization (RDC)task.
ACE RDC is considered a hard taskin information extraction due to the ab-sence of large amounts of training dataand inconsistencies in the available data.The proposed approach achieves superiorperformance which could be compared tosupervised techniques with reasonabletraining data.1 IntroductionIn this paper we propose a novel, and completelyunsupervised approach for information extrac-tion.
We present a general technique; howeverwe focus on relation extraction as an importanttask of Information Extraction.
The approachdepends on constructing generalized extractionpatterns, which could match many instances, anddeploys graph based mutual reinforcement toweight the importance of these patterns.
The mu-tual reinforcement is used to automatically iden-tify the most informative patterns, where patternsthat match many instances tend to be correct.Similarly, instances matched by many patternstend to be correct.
The intuition is that large un-supervised data is redundant, i.e.
different in-stances of information could be found manytimes in different contexts and by different repre-sentation.
The problem can therefore be seen ashubs (instances) and authorities (patterns) prob-lem which can be solved using the HypertextInduced Topic Selection (HITS) algorithm(Kleinberg, 1998).HITS is an algorithmic formulation of the no-tion of authority in web pages link analysis,based on a relationship between a set of relevant?authoritative pages?
and a set of ?hub pages?.The HITS algorithm benefits from the followingobservation:  when a page (hub) links to anotherpage (authority), the former confers authorityover the latter.By analogy to the authoritative web pagesproblem, we could represent the patterns as au-thorities and instances as hubs, and use mutualreinforcement between patterns and instances toweight the most authoritative patterns.
Highlyweighted patterns are then used in extracting in-formation.The proposed approach does not need anyseeds or examples.
Human involvement is onlyneeded in determining the entities of interest; theentities among which we are seeking relations.The paper proceeds as follows: in Section 2we discuss previous work followed by a briefdefinition of our general notation in Section 3.
Adetailed description of the proposed approachthen follows in Section 4.
Section 5 discusses theapplication of the proposed approach to the prob-501lem of detecting semantic relations from text.Section 6 discusses experimental results whilethe conclusion is presented in Section 7.2 Previous WorkMost of the previous work on Information Ex-traction (IE) focused on supervised learning.
Re-lation Detection and Characterization (RDC) wasintroduced in the Automatic Content ExtractionProgram (ACE) (ACE, 2004).
The approachesproposed to the ACE RDC task such as kernelmethods (Zelenko et al, 2002) and MaximumEntropy methods (Kambhatla, 2004) required theavailability of large set of human annotated cor-pora which are tagged with relation instances.However human annotated instances are limited,expensive, and time consuming to obtain, due tothe lack of experienced human annotators and thelow inter-annotator agreements.Some previous work adopted weakly super-vised or unsupervised learning approaches.These approaches have the advantage of notneeding large tagged corpora but need seed ex-amples or seed extraction patterns.
The majordrawback of these approaches is their depend-ency on seed examples or seed patterns whichmay lead to limited generalization due to de-pendency on handcrafted examples.
Some ofthese approaches are briefed here:(Brin,98) presented an approach for extractingthe authorship information as found in books de-scription on the World Wide Web.
This tech-nique is based on dual iterative pattern relationextraction wherein a relation and pattern set isiteratively constructed.
This approach has twomajor drawbacks: the use of handcrafted seedexamples to extract more examples similar tothese handcrafted seed examples and the use of alexicon as the main source for extracting infor-mation.
(Blum and Mitchell, 1998) proposed an ap-proach based on co-training that uses unlabeleddata in a particular setting.
They exploit the factthat, for some problems, each example can bedescribed by multiple representations.
(Riloff & Jones, 1999) presented the Meta-Bootstrapping algorithm that uses an un-annotated training data set and a set of seeds tolearn a dictionary of extraction patterns and adomain specific semantic lexicon.
Other workstried to exploit the duality of patterns and theirextractions for the purpose of inferring the se-mantic class of words like (Thelen & Riloff,2002) and (Lin et al 2003).
(Muslea et al, 1999) introduced an inductivealgorithm to generate extraction rules based onuser labeled training examples.
This approachsuffers from the labeled data bottleneck.
(Agichtein et.
al, 2000) presented an approachusing seed examples to generate initial patternsand to iteratively obtain further patterns.
Thenad-hoc measures were deployed to estimate therelevancy of the patterns that have been newlyobtained.
The major drawbacks of this approachare:  its dependency on seed examples leads tolimited capability of generalization, and the esti-mation of patterns relevancy requires the de-ployment of ad-hoc measures.
(Hasegawa et.
al.
2004) introduced unsuper-vised approach for relation extraction dependingon clustering context words between named enti-ties; this approach depends on ad-hoc contextsimilarity between phrases in the context andfocused on certain types of relations.
(Etzioni et al 2005) proposed a system forbuilding lists of named entities found on the web.Their system uses a set of eight domain-independent extraction patterns to generate can-didate facts.All approaches, proposed so far, suffer fromeither requiring large amount of labeled data orthe dependency on seed patterns (or examples)that result in limited generalization.3 General NotationIn graph theory, a graph is a set of objects calledvertices joined by links called edges.
A bipartitegraph, also called a bigraph, is a special graphwhere the set of vertices can be divided into twodisjoint sets with no two vertices of the same setsharing an edge.The Hypertext Induced Topic Selection(HITS) algorithm is an algorithm for rating, andtherefore ranking, web pages.
The HITS algo-rithm makes use of the following observation:when a page (hub) links to another page (author-ity), the former confers authority over the latter.HITS uses two values for each page, the "author-ity value" and the "hub value".
"Authority value"and "hub value" are defined in terms of one an-other in a mutual recursion.
An authority value iscomputed as the sum of the scaled hub valuesthat point to that authority.
A hub value is thesum of the scaled authority values of the authori-ties it points to.A template, as we define for this work, is a se-quence of generic forms that could generalize502over the given instances.
An example templateis:GPE POS  (PERSON)+GPE: Geographical Political En-tityPOS: possessive endingPERSON: PERSON EntityThis template could match the sentence:?France?s President Jacque Chirac...?.
This tem-plate is derived from the representation of theNamed Entity tags, Part-of-Speech (POS) tagsand semantic tags.
The choice of the templaterepresentation here is for illustration purposeonly; any combination of tags, representationsand tagging styles might be used.A pattern is more specific than a template.
Apattern specifies the role played by the tags (firstentity, second entity, or relation).
An example ofa pattern is:GPE(E2)  POS   (PERSON)+(E1)This pattern indicates that the word(s) with thetag GPE in the sentence represents the seconden-tity (Entity 2) in the relation, while theword(s) tagged PERSON represents the first en-tity (Entity 1) in this relation, the ?+?
symbolmeans that the (PERSON) entity is repetitive (i.e.may consist of several tokens).A tuple, in our notation during this paper, isthe result of the application of a pattern to un-structured text.
In the above example, one resultof applying the pattern to some raw text is thefollowing tuple:Entity 1: Jacque ChiracEntity 2: FranceRelation: EMP-Executive4 The ApproachThe unsupervised graph-based mutual rein-forcement approach, we propose, depends on theconstruction of generalized ?extraction patterns?that could match many instances.
The patternsare then weighted according to their importanceby deploying graph based mutual reinforcementtechniques.
This duality in patterns and extractedinformation (tuples) could be stated that patternscould match different tuples, and tuples in turncould be matched by different patterns.
The pro-posed approach is composed of two main stepsnamely, initial patterns construction and patternweighting or induction.
Both steps are detailed inthe next sub-sections.4.1 Initial Patterns ConstructionAs shown in Figure 1, several syntactic, lexical,and semantic analyzers could be applied to theunstructured text.
The resulting analyses could beemployed in the construction of extraction pat-terns.
It is worth mentioning that the proposedapproach is general enough to accommodate anypattern design; the introduced pattern design isfor illustration purposes only.Initially, we need to start with some templatesand patterns to proceed with the induction proc-ess.
Relatively large amount of text data istagged with different taggers to produce the pre-viously mentioned patterns styles.
An n-gramlanguage model is built on this data and used toconstruct weighted finite state machines.Paths with low cost (high language modelprobabilities) are chosen to construct the initialset of templates; the intuition is that paths withlow cost (high probability) are frequent andcould represent potential candidate patterns.The resulting initial set of templates is appliedto a very large text data to produce all possiblepatterns.
The number of candidate initial patternscould be reduced significantly by specifying thecandidate types of entities; for example we mightspecify that the first entity could be PEROSN orPEOPLE while the second entity could be OR-GANIZATION, LOCATION, COUNTRY andetc...The candidate patterns are then applied to thetagged stream and the unstructured text to collecta set of patterns and matched tuples pairs.The following procedure briefs the Initial Pat-tern Construction Step:?
Select a random set of text data.American vice President   Al Gore said today...PEOPLE    O         O       PERSON   O    O...ADJ     NOUN_PHRASE   NNP  VBD CD...PEOPLE NOUN_PHRASE  PERSON  VBD CD...EntitiesPOSTaggedStreamFigure 1:  An example of the output of analys-ers applied to the unstructured text503?
Apply various taggers on text data and con-struct templates style.?
Build n-gram language model on templatestyle data.?
Construct weighted finite state machinesfrom the n-gram language model.?
Choose n-best paths in the finite state ma-chines.?
Use best paths as initial templates.?
Apply initial templates on large text data.?
Construct initial patterns and associated tu-ples sets.4.2 Pattern InductionThe inherent duality in the patterns and tuplesrelation suggests that the problem could be inter-preted as a hub authority problem.
This problemcould be solved by applying the HITS algorithmto iteratively assign authority and hub scores topatterns and tuples respectively.Patterns and tuples are represented by a bipar-tite graph as illustrated in figure 2.
Each patternor tuple is represented by a node in the graph.Edges represent matching between patterns andtuples.
The pattern induction problem can beformulated as follows: Given a very large set ofdata D containing a large set of patterns P whichmatch a large set of tuples T, the problem is toidentify P~, the set of patterns that match the setof the most correct tuples  T~.
The intuition isthat the tuples matched by many different pat-terns tend to be correct and the patterns matchingmany different tuples tend to be good patterns.
Inother words; we want to choose, among the largespace of patterns in the data, the most informa-tive, highest confidence patterns that could iden-tify correct tuples; i.e.
choosing the most ?au-thoritative?
patterns in analogy with the hub au-thority problem.
However, both P~and T~are un-known.
The induction process proceeds as fol-lows:  each pattern p in P is associated with anumerical authority weight av which expresseshow many tuples match that pattern.
Similarly,each tuple t in T has a numerical hub weight htwhich expresses how many patterns werematched by this tuple.
The weights are calculatediteratively as follows:( ) ( )( )=+=pTu iiiHuhpa1 )()()1((1)( ) ( )( )=+=tPu iiiAuath1 )()()1((2)where T(p) is the set of tuples matched by p, P(t)is the set of patterns matching t, ( )pa i )1( +  is theauthoritative weight of pattern p  at iteration)1( +i , and ( )th i )1( +  is the hub weight of tuple tat iteration  )1( +i  .
H(i) and A(i) are normaliza-tion factors defined as:( )( ) = ==||1 1)()( PppTuii uhH  (3)( )( ) = ==||1 1)()( TvtPuii uaA(4)Highly weighted patterns are identified and usedfor extracting relations.4.3 Tuple ClusteringThe tuple space should be reduced to allow morematching between pattern-tuple pairs.
This spacereduction could be accomplished by seeking atuple similarity measure, and constructing aweighted undirected graph of tuples.
Two tuplesare linked with an edge if their similarity meas-ure exceeds a certain threshold.
Graph clusteringalgorithms could be deployed to partition thegraph into a set of homogeneous communities orclusters.
To reduce the space of tuples, we seek amatching criterion that group similar tuples to-gether.
Using WordNet, we can measure the se-mantic similarity or relatedness between a pair ofconcepts (or word senses), and by extension, be-tween a pair of sentences.
We use the similarityPPPPPTTTTTPPTTPatterns TuplesFigure 2: A bipartite graph represent-ing patterns and tuples504measure described in (Wu and Palmer, 1994)which finds the path length to the root  nodefrom the least common subsumer (LCS) of thetwo word senses which is the most specific wordsense they share as an ancestor.
The similarityscore of two tuples, ST, is calculated as follows:2221 EET SSS +=    (5)where SE1, and SE2 are the similarity scores of thefirst entities in the two tuples, and their secondentitles respectively.The tuple matching procedure assigns a simi-larity measure to each pair of tuples in the data-set.
Using this measure we can construct an undi-rected graph G. The vertices of G are the tuples.Two vertices are connected with an edge if thesimilarity measure between their underlying tu-ples exceeds a certain threshold.
It was noticedthat the constructed graph consists of a set ofsemi isolated groups as shown in figure 3.
Thosegroups have a very large number of inter-groupedges and meanwhile a rather small number ofintra-group edges.
This implies that using agraph clustering algorithm would eliminate thoseweak intra-group edges and produce separategroups or clusters representing similar tuples.
Weused Markov Cluster Algorithm (MCL) for graphclustering (Dongen, 2000).
MCL is a fast andscalable unsupervised clustering algorithm forgraphs based on simulation of stochastic flow.Figure 3: Applying Clustering Algorithms to Tu-ple graphAn example of a couple of tuples that could bematched by this technique is:United Stated(E2) presi-dent(E1)US(E2) leader(E1)A bipartite graph of patterns and tuple clustersis constructed.
Weights are assigned to patternsand tuple clusters by iteratively applying theHITS algorithm and the highly ranked patternsare then used for relation extraction.5 Experimental Setup5.1 ACE Relation Detection and Charac-terizationIn this section, we describe Automatic ContentExtraction (ACE).
ACE is an evaluation con-ducted by NIST to measure Entity Detection andTracking (EDT) and Relation Detection andCharacterization (RDC).
The EDT task is con-cerned with the detection of mentions of entities,and grouping them together by identifying theircoreference.
The RDC task detects relations be-tween entities identified by the EDT task.
Wechoose the RDC task to show the performance ofthe graph based unsupervised approach we pro-pose.
To this end we need to introduce the notionof mentions and entities.
Mentions are any in-stances of textual references to objects like peo-ple, organizations, geopolitical entities (countries,cities ?etc), locations, or facilities.
On the otherhand, entities are objects containing all mentionsto the same object.
Here, we present some exam-ples of ACE entities and relations:Spain?s Interior Ministerannounced this evening thearrest of separatist organi-zation Eta?s presumed leaderIgnacio Garcia Arregui.
Ar-regui, who is considered tobe the Eta organization?stop man, was arrested at17h45 Greenwich.
The Spanishjudiciary suspects Arreguiof ordering a failed attackon King Juan Carlos in 1995.In this fragment, all the underlined phrases arementions to ?Eta?
organization, or to ?GarciaArregui?.
There is a management relation be-tween ?leader?
which references to ?Gar-cia Arregui?
and ?Eta?.5.2 Patterns Construction and InductionWe used the LDC English Gigaword Corpus,AFE source from January to August 1996 as asource for unstructured text.
This provides a totalof 99475 documents containing 36 M words.
Inthe performed experiments, we focus on twotypes of relations EMP-ORG relations and GPE-AFF relations which represent almost 50% of allrelations in RDC ?
ACE task.TT TTTTTTTTTTT TTT TTTTTTTTTTT TBefore Clustering After Clustering505POS (part of speech) tagger and mention taggerwere applied to the data, the used pattern designconsists of a mix between the part of speech(POS) tags and the mention tags for the words inthe unsupervised data.
We use the mention tag, ifit exists; otherwise we use the part of speech tag.An example of the analyzed text and the pre-sumed associated pattern is shown:Text: Eta?s presumed leaderArregui ?Pos: NNP POS JJ NN NNPMention: ORG 0 0 0 PERSONPattern: ORG(E2) POS JJNN(R) PERSON(E1)An n-gram language model, 5-gram model andback off to lower order n-grams, was built on thedata tagged with the described patterns?
style.Weighted finite states machines were constructedwith the language model probabilities.
The n-bestpaths, 20 k paths, were identified and deployedas the initial template set.
Sequences that do notcontain the entities of interest, and hence cannotrepresent relations, were automatically filteredout.
This resulted in an initial templates set ofaround 3000 element.
This initial templates setwas applied on the text data to establish initialpatterns and tuples pairs.
Graph based mutualreinforcement technique was deployed with 10iterations on the patterns and tuples pairs toweight the patterns.We conducted two groups of experiments, thefirst with simple syntactic tuple matching, andthe second with semantic tuple clustering as de-scribed in section 4.36 Results and DiscussionWe compare our results to a state-of-the-art su-pervised system similar to the system describedin (Kambhatla, 2004).
Although it is unfair tomake a comparison between a supervised systemand a completely unsupervised system, we choseto make this comparison to test the performanceof the proposed unsupervised approach on a realtask with defined test set and state-of-the-art per-formance.
The supervised system was trained on145 K words which contain 2368 instances of thetwo relation types we are considering.The system performance is measured usingprecision, recall and F-Measure with variousamounts of induced patterns.
Table 1 presents theprecision, recall and F-measure for the two rela-tions using the presented approach with the utili-zation of different amount of highly weightedpatterns.
Table 2 presents the same results usingsemantic tuple matching and clustering, as de-scribed in section 4.3.No.
ofPatterns Precision Recall F-Measure1500 35.9 66.3 46.581000 41.2 59.7 48.75700 43.1 58.1 49.49500 46 56.5 50.71400 46.9 52.9 49.72200 50.1 44.9 47.36Table 1:  The effect of varying the number ofinduced patterns on the system performance(syntactic tuple matching)No.
ofPatterns Precision Recall F-Measure1500 36.1 67.2 46.971000 43.7 59.6 50.43700 44.1 59.3 50.58500 46.3 57.2 51.18400 47.3 57.6 51.94200 48.1 45.9 46.97Table 2:  The effect of varying the number ofinduced patterns on the system performance (se-mantic tuple matching)01020304050607080Sup 67.1 54.2 59.96Unsup-Syn 46 56.5 50.71Unsup-Sem 47.3 57.6 51.94Precision Recall F MeasureFigure 4:  A comparison between the supervisedsystem (Sup), the unsupervised system with syn-tactic tuple matching (Unsup-Syn), and with se-mantic tuple matching (Unsup-Sem)Best F-Measure is achieved using relativelysmall number of induced patterns (400 and  500patterns) while using more patterns increases therecall but degrades the precision.Table 2 indicates that the semantic clusteringof tuples did not provide significant improve-506ment; although better performance was achievedwith less number of patterns (400 patterns).
Wethink that the deployed similarity measure and itneeds further investigation to figure out the rea-son for that.Figure 4 presents the comparison between theproposed unsupervised systems and the referencesupervised system.
The unsupervised systemsachieves good results even in comparison to  astate-of-the-art supervised system.Sample patterns and corresponding matchingtext are introduced in Table 3 and Table 4.
Table3 shows some highly ranked patterns while Table4 shows examples of low ranked patterns.Pattern MatchesGPE (PERSON)+ Peruvian President Alberto Fu-jimoriGPE (PERSON)+ Zimbabwean President Robert MugabeGPE (PERSON)+ PLO leader Yasser ArafatGPE POS (PERSON)+ Zimbabwe 's President Robert MugabeGPE JJ PERSON    American clinical neuropsy-chologistGPE JJ PERSON    American diplomatic personnelPERSON IN JJ GPE candidates for local governmentORGANIZATION PER-SON Airways spokesmanORGANIZATION PER-SON      Ajax playersPERSON IN DT (OR-GANIZATION)+chairman of the opposition par-ties(ORGANIZATION)+PERSON    opposition parties chairmansTable3: Examples of patterns with high weightsPattern MatchesGPE CC (PERSON)+ Barcelona and JohanCruyffGPE , CC PERSON Paris , but RiccardiGPE VBZ VBN PERSON Pyongyang has acceptedGallucciGPE VBZ VBN PERSON Russia has abandoned usGPE VBZ VBN P PER-SONRwanda 's defeated HutuGPE VBZ VBN PERSON state has pressed ArafatGPE VBZ VBN TO VBPERSONTaiwan has tried to keepLee(PERSON)+ VBD GPEORGANIZATIONAlfred Streim told Ger-man radio(PERSON)+ VBD GPEORGANIZATIONDennis Ross met Syrianarmy(PERSON)+ VBD GPEORGANIZATIONVan Miert told EU indus-tryTable4: Examples of patterns with low weights7 Conclusion and Future WorkIn this work, a general framework for unsuper-vised information extraction based on mutualreinforcement in graphs has been introduced.
Weconstruct generalized extraction patterns and de-ploy graph based mutual reinforcement to auto-matically identify the most informative patterns.We provide motivation for our approach from agraph theory and graph link analysis perspective.Experimental results have been presented sup-porting the applicability of the proposed ap-proach to ACE Relation Detection and Charac-terization (RDC) task, demonstrating its applica-bility to hard information extraction problems.The proposed approach achieves remarkable re-sults comparable to a state-of-the-art supervisedsystem, achieving 51.94 F-measure compared to59.96 F-measure of the state-of-the-art super-vised system which requires huge amount of hu-man annotated data.
The proposed approachrepresents a powerful unsupervised technique forinformation extraction in general and particularlyfor relations extraction that requires no seed pat-terns or examples and achieves significant per-formance.In our future work, we plan to focus on general-izing the approach for targeting more NLP prob-lems.8 AcknowledgementsWe would like to thank Salim Roukos for hisinvaluable suggestions and support.
We wouldalso like to thank Hala Mostafa for helping withthe early investigation of this work.
Finally wewould like to thank the anonymous reviewers fortheir constructive criticism and helpful com-ments.ReferencesACE.
2004.
The NIST ACE evaluation website.http://www.nist.gov/speech/tests/ace/Eugene Agichtein and Luis Gravano.
2000.
Snow-ball: Extracting Relations from Large Plain-TextCollections.
Proceedings of the 5th ACM Confer-ence on Digital Libraries (DL 2000).Sergy Brin.
1998.
Extracting Patterns and Relationsfrom the World Wide Web.
Proceedings of the 1998International Workshop on the Web and Data-bases?Stijn van Dongen.
2000.
A Cluster Algorithm forGraphs.
Technical Report INS-R0010, NationalResearch Institute for Mathematics and ComputerScience in the Netherlands.507Stijn van Dongen.
2000.
Graph Clustering by FlowSimulation.
PhD thesis, University of UtrechtOren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2004.
Web-scale information extraction in KnowItAll (prelimi-nary results).
In Proceedings of the 13th WorldWide Web Conference, pages 100-109.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised Named-Entity Extraction from the Web:An Experimental Study.
Artificial Intelligence,2005.Radu Florian, Hany Hassan, Hongyan Jing, NandaKambhatla, Xiaqiang Luo, Nicolas Nicolov, andSalim Roukos.
2004.
A Statistical Model for multi-lingual entity detection and tracking.
Proceedingsof the Human Language Technologies Conference(HLT-NAACL 2004).Dayne Freitag, and Nicholas Kushmerick.
2000.Boosted wrapper induction.
The 14th EuropeanConference on Artificial Intelligence Workshop onMachine Learning for Information ExtractionRayid Ghani and Rosie Jones.
2002.
A Comparison ofEfficacy and Assumptions of Bootstrapping Algo-rithms for Training Information Extraction Sys-tems.
Workshop on Linguistic Knowledge Acquisi-tion and Representation: Bootstrapping AnnotatedData at the Linguistic Resources and EvaluationConference (LREC 2002).Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman.2004.
Discovering Relations among Named Enti-ties from Large Corpora.
Proceedings of The 42ndAnnual Meeting of the Association for Computa-tional Linguistics (ACL 2004).Taher Haveliwala.
2002.
Topic-sensitive PageRank.Proceedings of the 11th International World WideWeb ConferenceThorsten Joachims.
2003.
Transductive Learning viaSpectral Graph Partitioning.
Proceedings of the In-ternational Conference on Machine Learning(ICML 2003).Nanda Kambhatla.
2004.
Combining Lexical, Syntac-tic, and Semantic Features with Maximum EntropyModels for Information Extraction.
Proceedings ofThe 42nd Annual Meeting of the Association forComputational Linguistics (ACL 2004).John Kleinberg.
1998.
Authoritative Sources in a Hy-perlinked Environment.
Proceedings of the 9thACM-SIAM Symposium on Discrete Algorithms.N.
Kushmerick, D.S.
Weld, R.B.
Doorenbos.
1997.Wrapper Induction for Information Extraction.Proceedings of the International Joint Conferenceon Artificial Intelligence.Winston Lin, Roman Yangarber, Ralph Grishman.2003.
Bootstrapped Learning of Semantic Classesfrom Positive and Negative Examples.
Proceedingsof the 20th International Conference on MachineLearning (ICML 2003) Workshop on The Contin-uum from Labeled to Unlabeled Data in MachineLearning and Data Mining.Ion Muslea, Steven Minton, and CraigKnoblock.1999.
A hierarchical approach to wrap-per induction.
Proceedings of the Third Interna-tional Conference on Autonomous Agents.Ted Pedersen, Siddharth Patwardhan, and JasonMichelizzi.
2004, WordNet::Similarity - Measuringthe Relatedness of Concepts.
Proceedings of FifthAnnual Meeting of the North American Chapter ofthe Association for Computational Linguistics(NAACL 2004)Ellen Riloff and Rosie Jones.
2003.
Learning diction-aries for information extraction by multilevel boot-strapping.
Proceedings of the Sixteenth nationalConference on Artificial Intelligence (AAAI 1999).Michael Thelen and Ellen Riloff.
2002.
A Bootstrap-ping Method for Learning Semantic Lexicons usingExtraction Pattern Contexts.
Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2002).Scott White, and Padhraic Smyth.
2003.
Algorithmsfor Discoveing Relative Importance in Graphs.Proceedings of Ninth ACM SIGKDD InternationalConference on Knowledge Discovery and DataMining.Zhibiao Wu, and Martha Palmer.
1994.
Verb seman-tics and lexical selection.
Proceedings of the 32ndAnnual Meeting of the Association for Computa-tional Linguistics (ACL 1994).Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-supervised Learning using GaussianFields and Harmonic Functions.
Proceedings ofthe 20th International Conference on MachineLearning (ICML 2003).508
