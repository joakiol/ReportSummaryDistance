Proceedings of the Workshop on Language in Social Media (LASM 2013), pages 59?68,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsA Case Study of Sockpuppet Detection in WikipediaThamar Solorio and Ragib Hasan and Mainul MizanThe University of Alabama at Birmingham1300 University Blvd.Birmingham, AL 35294, USA{solorio,ragib,mainul}@cis.uab.eduAbstractThis paper presents preliminary results of usingauthorship attribution methods for the detec-tion of sockpuppeteering in Wikipedia.
Sock-puppets are fake accounts created by malicioususers to bypass Wikipedia?s regulations.
Ourdataset is composed of the comments madeby the editors on the talk pages.
To overcomethe limitations of the short lengths of thesecomments, we use an voting scheme to com-bine predictions made on individual user en-tries.
We show that this approach is promisingand that it can be a viable alternative to thecurrent human process that Wikipedia uses toresolve suspected sockpuppet cases.1 IntroductionCollaborative projects in social media have becomevery popular in recent years.
A very successful ex-ample of this is Wikipedia, which has emerged as theworld?s largest crowd-sourced encyclopaedia.
Thistype of decentralized collaborative processes are ex-tremely vulnerable to vandalism and malicious be-havior.
Anyone can edit articles in Wikipedia and/ormake comments in article discussion pages.
Reg-istration is not mandatory, but anyone can registeran account in Wikipedia by providing only little in-formation about themselves.
This ease of creatingan identity has led malicious users to create mul-tiple identities and use them for various purposes,ranging from block evasion, false majority opinionclaims, and vote stacking.
This is an example ofthe multi aliasing problem known as ?The Sybil At-tack?
(Douceur, 2002).
Unfortunately, Wikipediadoes not provide any facility to detect such multi-ple identities.
The current process is carried out byhumans, is very time consuming, and final resolu-tion to cases of multiple identities is based on humanintuition.
A smart sockpuppet can therefore evadedetection by using multiple IP addresses, modifyingwriting style, and changing behavior.
Also, a mali-cious user can create sleeper accounts that performbenign edits from time to time, but are used for sock-puppetry when needed.
Identifying such accountsas sockpuppets is not obvious as these accounts mayhave a long and diverse edit history.Sockpuppets are a prevalent problem in Wikipedia,there were close to 2,700 unique suspected casesreported in 2012.
In this paper, we present a smallscale study of automated detection of sockpuppetsbased on machine learning.
We approach thisproblem from the point of view of authorship attri-bution (AA), where the task consists of analyzing awritten document to predict the true author.
If wecan successfully model the editors?
unique writingstyle from their comments, then we can use thisinformation to link the sockpuppet accounts to theircorresponding puppeteer.
We focus on the contentfrom the talk pages since the articles edited onWikipedia have a fixed and very uniform style.
Incontrast, we have observed that editors write in amore free-form style during discussions carried outon the talk pages.
Our results show that a two-stageprocess for the task can achieve promising results.The contributions of this study are as follows:?
We present encouraging preliminary results onusing authorship attribution approaches for un-59covering real sockpuppet cases in Wikipedia.
Tothe best of our knowledge, we are the first totackle this problem.?
We identify novel features that have high dis-criminative power and are suitable for this task,where the input text is very short.
These featurescan be helpful in other social media settings, asthere are many shared characteristics across thisgenre.The rest of the paper is organized as follows:in Section 2, we provide a detailed discussion onWikipedia?s editing environment and culture.
In Sec-tion 3, we talk about authorship attribution and re-lated work.
Then in Section 4, we present our de-tailed approach.
In Sections 5, 6, and 7, we discussthe data set, experimental setup, and results, respec-tively.
Finally, we present an overall discussion andfuture directions in Sections 8 and 9.2 BackgroundIn Wikipedia, whenever a user acts in bad faith, van-dalizes existing articles, or creates spurious articles,that user is banned from editing new content.
Theban can last for some hours, to days, and in somecases it can be permanent.
Sometimes, a banned usercreates a new account to circumvent the ban, or editsWikipedia without signing in.These extra accounts or IP addresses, from whichlogged out edits are made, are called sockpuppets.The primary (oldest) account is called the sockpup-peteer.
Whenever an editor is suspected to be a sock-puppet of another editor, a sockpuppet investigationcase is filed against those accounts.
Any editor canfile a case, but the editor must provide supporting evi-dence as well.
Typical evidence includes informationabout the editing actions related to those accounts,such as the articles, the topics, vandalism patterns,timing of account creation, timing of edits, and votingpattern in disagreements.Sometime after the case is filed, an administratorwill investigate the case.
An administrator is an editorwith privileges to make account management deci-sions, such as banning an editor.
If the administratoris convinced that the suspect is a sockpuppet, he de-clares the verdict as confirmed.
He also issues bansto the corresponding accounts and closes the case.If an administrator cannot reach a verdict on a case,he asks for a check user to intervene.
Check usersare higher privileged editors, who have access to pri-vate information regarding editors and edits, such asthe IP address from which an editor has logged in.Other interested editors in the case, or the originaleditor who filed the case can also ask for a checkuser to intervene.
The check user will review the ev-idence, as well as private information regarding thecase, and will try to establish the connection betweenthe sockpuppet and puppeteer.
Then the check userwill rule on the case.
Finally, another administratorwill look at the check user report and issue a finalverdict.
During the process, the accused editors, boththe puppeteer and the sockpuppet, can submit evi-dence in their favor.
But this additional evidence isnot mandatory.The current process to resolve suspected cases ofsockpuppets has several disadvantages.
We have al-ready mentioned the first one.
Because it is a manualprocess, it is time consuming and expensive.
Perhapsa more serious weakness is the fact that relaying onIP addresses is not robust, as simple counter mea-sures can fool the check users.
An alternative to thisprocess could be an automated framework that re-lies on the analysis of the comments to link editoraccounts, as we propose in this paper.3 Related WorkModern approaches to AA typically follow a textclassification framework where the classes are theset of candidate authors.
Different machine learningalgorithms have been used, including memory-basedlearners (Luyckx and Daelemans, 2008a; Luyckxand Daelemans, 2010), Support Vector Machines(Escalante et al 2011), and Probabilistic ContextFree Grammars (Raghavan et al 2010).Similarity-based approaches have also been suc-cessfully used for AA.
In this setting, the trainingdocuments from the same author are concatenatedinto a single file to generate profiles from author-specific features.
Then authorship predictions arebased on similarity scores.
(Keselj et al 2003; Sta-matatos, 2007; Koppel et al 2011) are examples ofsuccessful examples of this approach.Previous research has shown that low-level fea-tures, such as character n-grams are very powerful60discriminators of writing styles.
Although, enrichingthe models with other types of features can boostaccuracy.
In particular, stylistic features (punctuationmarks, use of emoticons, capitalization information),syntactic information (at the part-of-speech level andfeatures derived from shallow parsing), and even se-mantic features (bag-of-words) have shown to beuseful.Because of the difficulties in finding data fromreal cases, most of the published work in AA eval-uates the different methods on data collections thatwere gathered originally for other purposes.
Exam-ples of this include the Reuters Corpus (Lewis et al2004) that has been used for benchmarking differentapproaches to AA (Stamatatos, 2008; Plakias andStamatatos, 2008; Escalante et al 2011) and thedatasets used in the 2011 and 2012 authorship identi-fication competitions from the PAN Workshop series(Argamon and Juola, 2011; Juola, 2012).
Other re-searchers have invested efforts in creating their ownAA corpus by eliciting written samples from subjectsparticipating in their studies (Luyckx and Daelemans,2008b; Goldstein-Stewart et al 2008), or crawlingthough online websites (Narayanan et al 2012).In contrast, in this paper we focus on data fromWikipedia, where there is a real need to identify ifthe comments submitted by what appear to be dif-ferent users, belong to a sockpuppeteer.
Data fromreal world scenarios like this make solving the AAproblem an even more urgent and practical matter,but also impose additional challenges to what is al-ready a difficult problem.
First, the texts analyzed inthe Wikipedia setting were generated by people withthe actual intention of deceiving the administratorsinto believing they are indeed coming from differ-ent people.
With few exceptions (Afroz et al 2012;Juola and Vescovi, 2010), most of the approaches toAA have been evaluated with data where the authorswere not making a conscious effort to deceive or dis-guise their own identities or writeprint.
Since therehas been very little research done on deception detec-tion, it is not well understood how AA approachesneed to be adapted for these situations, or what kindsof features must be included to cope with deceptivewriting.
However, we do assume this adds a com-plicating factor to the task, and previous researchhas shown considerable decreases in AA accuracywhen deception is present (Brennan and Greenstadt,2009).
Second, the length of the documents is usu-ally shorter for the Wikipedia comments than that ofother collections used.
Document length will clearlyaffect the prediction performance of AA approaches,as the shorter documents will contain less informa-tion to develop author writeprint models and to makean inference on attribution.
As we will describe later,this prompted us to reframe our solution in order tocircumvent this short document length issue.
Lastly,the data available is limited, there is an average of 80entries per user in the training set from the collectionwe gathered, and an average of 8 messages in the testset, and this as well limits the amount of evidenceavailable to train author models.
Moreover, the testcases have an average of 8 messages.
This is a verysmall amount of texts to make the final prediction.4 ApproachIn our framework, each comment made by a user isconsidered a ?document?
and therefore, each com-ment represents an instance of the classification task.There are two steps in our method.
In the first step,we gather predictions from the classifier on each com-ment.
Then in the second step we take the predictionsfor each comment and combine them in a majorityvoting schema to assign final decisions to each ac-count.The two step process we just described helps usdeal with the challenging length of the individualcomments.
It is also an intuitive approach, since whatwe need to determine is if the account belongs to thesockpuppeteer.
The ruling is at the account-level,which is also consistent with the human process.
Inthe case of a positive prediction by our system, wetake as a confidence measure on the predictions thepercentage of comments that were individually pre-dicted as sockpuppet cases.4.1 Feature EngineeringIn this study, we have selected typical features ofauthorship attribution, as well as new features wecollected from inspecting the data by hand.
In total,we have 239 features that capture stylistic, grammati-cal, and formatting preferences of the authors.
Thefeatures are described below.Total number of characters: The goal of thisfeature is to model the author?s behavior of writing61long wordy texts, or short comments.Total number of sentences: We count the totalnumber of sentences in the comments.
While this fea-ture is also trying to capture some preferences regard-ing the productivity of the author?s comments, it cantell us more about the author?s preference to organizethe text in sentences.
Some online users tend to writein long sentences and thus end up with a smaller num-ber of sentences.
To fragment the comments into sen-tences, we use the Lingua-EN-Sentence-0.25 fromwww.cpan.org (The Comprehensive Perl ArchiveNetwork).
This off-the-shelf tool prevents abbrevia-tions to be considered as sentence delimiters.Total number of tokens: We define a token asany sequence of consecutive characters with no whitespaces in between.
Tokens can be words, numbers,numbers with letters, or with punctuation, such asapple, 2345, 15th, and wow!!!.
For this feature wejust count how many tokens are in the comment.Words without vowels: Most English words haveone or more vowels.
The rate of words without vow-els can also be a giveaway marker for some authors.Some words without vowels are try, cry, fly, myth,gym, and hymn.Total alphabet count: This feature consists ofthe count of all the alphabetic characters used by theauthor in the text.Total punctuation count: Some users use punctu-ation marks in very unique ways.
For instance, semi-colons and hyphens show noticeable differences intheir use, some people avoid them completely, whileothers might use them in excess.
Moreover, the useof commas is different in different parts of the world,and that too can help identify the author.Two/three continuous punctuation count: Se-quences of the same punctuation mark are often usedto emphasize or to add emotion to the text, such aswow!!
!, and really??.
Signaling emotion in writtentext varies greatly for different authors.
Not every-one displays emotions explicitly or feels comfortableexpressing them in text.
We believe this could alsohelp link users to sockpuppet cases.Total contraction count: Contractions are usedfor presenting combined words such as don?t, it?s,I?m, and he?s.
The contractions, or the spelled-out-forms are both correct grammatically.
Hence, the useof contraction is somewhat a personal writing styleattribute.
Although the use of contractions variesacross different genres, in social media they are com-monly used.Parenthesis count: This is a typical authorship at-tribution feature that depicts the rate at which authorsuse parenthesis in their comments.All caps letter word count: This is a featurewhere we counted the number of tokens having allupper case letters.
They are either abbreviations, orwords presented with emphasis.
Some examples areUSA, or ?this is NOT correct?.Emoticons count: Emoticons are pictorial rep-resentations of feelings, especially facial expres-sions with parenthesis, punctuation marks, and letters.They typically express the author?s mood.
Some com-monly used emoticons are :) or :-) for happy face, :(for sad face, ;) for winking, :D for grinning, <3 forlove/heart, :O for being surprised, and :P for beingcheeky/tongue sticking out.Happy emoticons count: As one of the mostwidely used emoticons, happy face was counted as aspecific feature.
Both :) and :-) were counted towardsthis feature.Sentence count without capital letter at the be-ginning: Some authors start sentences with numbersor small letters.
This feature captures that writingstyle.
An example can be ?1953 was the year, ...?
or,?big, bald, and brass - all applies to our man?.Quotation count: This is an authorship attribu-tion feature where usage of quotation is counted asa feature.
When quoting, not everyone uses the quo-tation punctuation and hence quotation marks countmay help discriminate some writers from others.Parts of speech (POS) tags frequency: We tooka total of 36 parts of speech tags from the Penn Tree-bank POS (Marcus et al 1993) tag set into considera-tion.
We ignored all tags related to punctuation marksas we have other features capturing these characters.Frequency of letters: We compute the frequencyof each of the 26 English letters in the alphabet.
Thecount is normalized by the total number of non-whitecharacters in the comment.
This contributed 26 fea-tures to the feature set.Function words frequency: It has been widelyacknowledged that the rate of function words is agood marker of authorship.
We use a list of functionwords taken from the function words in (Zheng etal., 2006).
This list contributed 150 features to thefeature set.62All the features described above have been usedin previous work on AA.
Following are the featuresthat we found by manually inspecting the Wikipediadata set.
All the features involving frequency countsare normalized by the length of the comment.Small ?i?
frequency: We found the use of small?i?
in place of capital ?I?
to be common for someauthors.
Interestingly, authors who made this mistakerepeated it quite often.Full stop without white space frequency: Notusing white space after full stop was found quitefrequently, and authors repeated it regularly.Question frequency: We found that some authorsuse question marks more frequently than others.
Thisis an idiosyncratic feature as we found some authorsabuse the use of question marks for sentences that donot require question marks, or use multiple questionmarks where one question mark would suffice.Sentence with small letter frequency: Some au-thors do not start a sentence with the first letter cap-italized.
This behavior seemed to be homogeneous,meaning an author with this habit will do it almostalways, and across all of its sockpuppet accounts.Alpha, digit, uppercase, white space, and tabfrequency: We found that the distribution of thesespecial groups of characters varies from author toauthor.
It captures formatting preferences of textsuch as the use of ?one?
and ?zero?
in place of ?1?and ?0?, and uppercase letters for every word.
?A?, and an error frequency: Error with usageof ?a?, and ?an?
was quite common.
Many authorstend to use ?a?
in place of ?an?, and vice versa.
Weused a simple rate of all ?a?
in front of words startingwith vowel, or ?an?
in front of words starting withconsonant.
?he?, and ?she?
frequency: Use of ?he?, or ?she?is preferential to each author.
We found that the use of?he?, or ?she?
by any specific author for an indefinitesubject is consistent across different comments.5 DataWe collected our data from cases filed by real userssuspecting sockpupeteering in the English Wikipedia.Our collection consists of comments made by theaccused sockpuppet and the suspected puppeteer invarious talk pages.
All the information about sock-puppet cases is freely available, together with infor-Class Total Avg.
Msg.TrainAvg.
Mesg.TestSockpuppet 41 88.75 8.5Non-sockpuppet 36 77.3 7.9Table 1: Distribution of True/False sockpuppet cases inthe experimental data set.
We show the average numberof messages in train and test partitions for both classes.mation about the verdict from the administrators.
Forthe negative examples, we also collected commentsmade by other editors in the comment threads of thesame talk pages.
For each comment, we also col-lected the time when the comment was posted asan extra feature.
We used this time data to investi-gate if non-authorship features can contribute to theperformance of our model, and to compare the perfor-mance of stylistic features and external user accountinformation.Our dataset has two types of cases: confirmedsockpuppet, and rejected sockpuppet.
The confirmedcases are those where the administrators have made fi-nal decisions, and their verdict confirmed the case asa true sockpuppet case.
Alternatively, for the rejectedsockpuppet cases, the administrator?s verdict exoner-ates the suspect of all accusations.
The distributionof different cases is given in Table 1.Of the cases we have collected, one of the notablepuppeteers is ?-Inanna-?.
This editor was active inWikipedia for a considerable amount of time, fromDecember 2005 to April 2006.
He also has a numberof sockpuppet investigation cases against him.
Ta-ble 2 shows excerpts from comments made by thiseditor on the accounts confirmed as sockpuppet.
Wehighlight in boldface the features that are more no-ticeable as similar patterns between the different useraccounts.An important aspect of our current evaluationframework is the preprocessing of the data.
We?cleansed?
the data by removing content that wasnot written by the editor.
The challenge we face isthat Wikipedia does not have a defined structure forcomments.
We can get the difference of each modifi-cation in the history of a comment thread.
However,not all modifications are comments.
Some can bereverts (changing content back to an old version), orupdates.
Additionally, if an editor replies to morethan one part of a thread in response to multiple com-63Comment from the sockpuppeteer: -Inanna-Mine was original and i have worked on it more than 4 hours.I have changedit many times by opinions.Last one was accepted by all the users(except forkhokhoi).I have never used sockpuppets.Please dont care Khokhoi,Tombseyeand Latinus.They are changing all the articles about Turks.The most importantand famous people are on my picture.Comment from the sockpuppet: AltauHello.I am trying to correct uncited numbers in Battle of Sarikamis and CrimeanWar by resources but khoikhoi and tombseye always try to revert them.Couldyou explain them there is no place for hatred and propagandas, please?Comment from the others: KhoikhoiActually, my version WAS the original image.
Ask any other user.
Inanna?simage was uploaded later, and was snuck into the page by Inanna?s sockpuppetbefore the page got protected.
The image has been talked about, and peoplehave rejected Inanna?s image (see above).Table 2: Sample excerpt from a single sockpuppet case.
We show in boldface some of the stylistic features sharedbetween the sockpuppeter and the sockpuppet.System P R F A (%)B-1 0.53 1 0.69 53.24B-2 0.53 0.51 0.52 50.64Our System 0.68 0.75 0.72 68.83Table 3: Prediction performance for sockpuppet detec-tion.
Measures reported are Precision (P), Recall (R),F-measure (F), and Accuracy (A).
B-1 is a simple baselineof the majority class and B-2 is a random baseline.ments, or edits someone else?s comments for anyreason, there is no fixed structure to distinguish eachaction.
Hence, though our initial data collector toolgathered a large volume of data, we could not use allof it as the preprocessing step was highly involvedand required some manual intervention.6 Experimental SettingWe used Weka (Witten and Frank, 2005) ?
a widelyrecognized free and open source data-mining tool, toperform the classification.
For the purpose of thisstudy, we chose Weka?s implementation of SupportVector Machine (SVM) with default parameters.To evaluate in a scenario similar to the real settingin Wikipedia, we process each sockpuppet case sepa-rately, we measure prediction performance, and thenaggregate the results of each case.
For example, wetake data from a confirmed sockpuppet case and gen-erate the training and test instances.
The training datacomes from the comments made by the suspectedsockpuppeteer, while the test data comes from thecomments contributed by the sockpuppet account(s).We include negative samples for these cases by col-lecting comments made on the same talk pages byeditors not reported or suspected of sockpuppeteer-ing.
Similarly, to measure the false positive ratio ofour approach, we performed experiments with con-firmed non-sockpuppet editors that were also filed aspotential sockpuppets in Wikipedia.7 ResultsThe results of our experiments are shown in Table 3.For comparison purposes we show results of twosimple baseline systems.
B-1 is the trivial classifierthat predicts every case as sockpuppet (majority).
B-2 is the random baseline (coin toss).
However as seenin the table, both baseline systems are outperformedby our system that reached an accuracy of 68%.
B-1reached an accuracy of 53% and B-2 of 50%.For the miss-classified instances of confirmedsockpuppet cases, we went back to the original com-ment thread and the investigation pages to find outthe sources of erroneous predictions for our system.We found investigation remarks for 4 cases.
Of these4 cases, 2 cases were tied on the predictions for theindividual comments.
We flip a coin in our systemto break ties.
From the other 2 cases, one has theneutral comment from administrators: ?Possible?,which indicates some level of uncertainty.
The lastone has comments that indicate a meat puppet.
Ameat puppet case involves two different real people64where one is acting under the influence of the other.A reasonable way of taking advantage of the currentsystem is to use the confidence measure to make pre-dictions of the cases where our system has the highestconfidence, or higher than some threshold, and letthe administrators handle those cases that are moredifficult for an automated approach.We have also conducted an experiment to rank ourfeature set with the goal of identifying informativefeatures.
We used information gain as the rankingmetric.
A snapshot of the top 30 contributing fea-tures according to information gain is given in Ta-ble 4.
We can see from the ranking that some of thetop-contributing features are idiosyncratic features.Such features are white space frequency, beginningof the sentence without capital letter, and no whitespace between sentences.
We can also infer fromTable 4 that function word features (My, me, its, that,the, I, some, be, have, and since), and part of speechtags (VBG-Verb:gerund or present participle, CD-Cardinal number, VBP-Verb:non-3rd person singularpresent, NNP-Singular proper noun, MD-Modal, andRB-Adverb) are among the most highly ranked fea-tures.
Function words have been identified as highlydiscriminative features since the earliest work on au-thorship attribution.Finally, we conducted experiments with two edittiming features for 49 cases.
These two features areedit time of the day in a 24 hour clock, and editday of the week.
We were interested in exploring ifadding these non-stylistic features could contributeto classification performance.
To compare perfor-mance of these non-authorship attribution features,we conducted the same experiments without thesefeatures.
The results are shown in Table 5.
We cansee that average confidence of the classification, aswell as F-measure goes up with the timing features.These timing features are easy to extract automati-cally, therefore they should be included in an auto-mated approach like the one we propose here.8 DiscussionThe experiments presented in the previous section areencouraging.
They show that with a relatively smallset of automatically generated features, a machinelearning algorithm can identify, with a reasonable per-formance, the true cases of sockpuppets in Wikipedia.FeaturesWhitespace frequencyPunctuation countAlphabet countContraction countUppercase letter frequencyTotal charactersNumber of tokensmemyitsthatBeginning of the sentence without capital letter ?VBG-Verb:gerund or present participleNo white space between sentences ?theFrequency of LICD-Cardinal numberFrequency of FVBP-Verb:non-3rd person singular presentSentence start with small letter ?someNNP-Singular proper nounbeTotal SentencesMD-Modal?
mark frequencyhavesinceRB-AdverbTable 4: Ranking of the top 30 contributing features for theexperimental data using information gain.
Novel featuresfrom our experiment are denoted by ?.Features used Confidence F-measureAll + timing features 84.04% 0.72All - timing features 78.78% 0.69Table 5: Experimental result showing performance of themethod with and without timing features for the problemof detecting sockpuppet cases.
These results are on asubset of 49 cases.6572 74 76 78 80 82 84 86Confidence in %0.650.660.670.680.690.700.710.720.73F-measureabcdefgFigure 1: A plot of confidence in % for successful cases vs. F-measure for the system where we remove one featuregroup at a time.
Here marker a) represents performance of the system with all the features.
Markers b) timing features, c)part of speech tags, d) idiosyncratic features, e) function words, f) character frequencies, and g) AA features, representperformance of the system when the specified feature group is removed.Since falsely accusing someone of using a sockpup-pet could lead to serious credibility loss for users,we believe a system like ours could be used as a firstpass in resolving the suspected sockpuppet cases, andbring into the loop the administrators for those caseswhere the certainty is not high.To further investigate the contribution of differentgroups of features in our feature set, we conductedadditional experiments where we remove one featuregroup at a time.
Our goal is to see which featuregroup causes larger decreases in prediction perfor-mance when it is not used in the classification.
Wesplit our feature set into six groups, namely timingfeatures, parts of speech tags, idiosyncratic features,function words, character frequencies, and author-ship attribution features.
In Figure 1, we show theresult of the experiments.
From the figure, we ob-serve that function words are the most influentialfeatures as both confidence, and F-measure showedthe largest drop when this group was excluded.
Theidiosyncratic features that we have included in thefeature set showed the second largest decrease in pre-diction performance.
Timing features, and part ofspeech tags have similar drops in F-measure but theyshowed a different degradation pattern on the con-fidence: part of speech tags caused the confidenceto decrease by a larger margin than the timing fea-tures.
Finally, character frequencies, and authorshipattribution features did not affect F-measure much,but the confidence from the predictions did decreaseconsiderably with AA features showing the secondlargest drop in confidence overall.9 Conclusion and Future DirectionsIn this paper, we present a first attempt to develop anautomated detection method of sockpuppets basedsolely on the publicly available comments from thesuspected users.
Sockpuppets have been a bane forWikipedia as they are widely used by malicious usersto subvert Wikipedia?s editorial process and consen-sus.
Our tool was inspired by recent work on thepopular field of authorship attribution.
It requires noadditional administrative rights (e.g., the ability toview user IP addresses) and therefore can be usedby regular users or administrators without check userrights.
Our experimental evaluation with real sock-66puppet cases from the English Wikipedia shows thatour tool is a promising solution to the problem.We are currently working on extending this studyand improving our results.
Specific aspects we wouldlike to improve include a more robust confidencemeasure and a completely automated implementation.We are aiming to test our system on all the cases filedin the history of the English Wikipedia.
Later on, itwould be ideal to have a system like this running inthe background and pro-actively scanning all activeeditors in Wikipedia, instead of running in a usertriggered mode.
Another useful extension wouldbe to include other languages, as English is onlyone of the many languages currently represented inWikipedia.AcknowledgementsThis research was supported in part by ONR grantN00014-12-1-0217.
The authors would like to thankthe anonymous reviewers for their comments on aprevious version of this paper.ReferencesS.
Afroz, M. Brennan, and R. Greenstadt.
2012.
Detectinghoaxes, frauds, and deception in writing style online.
InProceedings of the 2012 IEEE Symposium on Securityand Privacy (S&P), pages 461?475.
IEEE, May.Shlomo Argamon and Patrick Juola.
2011.
Overview ofthe international authorship identification competitionat PAN-2011.
In Proceedings of the PAN 2011 Lab Un-covering Plagiarism, Authorship, and Social SoftwareMisuse, held in conjunction with the CLEF 2011 Con-ference on Multilingual and Multimodal InformationAccess Evaluation, Amsterdam.M.
Brennan and R. Greenstadt.
2009.
Practical attacksagainst authorship recognition techniques.
In Proceed-ings of the Twenty-First Innovative Applications of Ar-tificial Intelligence Conference.John Douceur.
2002.
The Sybil attack.
In Peter Dr-uschel, Frans Kaashoek, and Antony Rowstron, editors,Peer-to-Peer Systems, volume 2429 of Lecture Notesin Computer Science, pages 251?260.
Springer Berlin /Heidelberg.H.
J. Escalante, T. Solorio, and M. Montes-y Go?mez.2011.
Local histograms of character n-grams for au-thorship attribution.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 288?298,Portland, Oregon, USA, June.
Association for Compu-tational Linguistics.Jade Goldstein-Stewart, Kerri A. Goodwin, Roberta EvansSabin, and Ransom K. Winder.
2008.
Creating andusing a correlated corpus to glean communicative com-monalities.
In Proceedings of LREC-2008, the SixthInternational Language Resources and Evaluation Con-ference.P.
Juola and D. Vescovi.
2010.
Empirical evaluation ofauthorship obfuscation using JGAAP.
In Proceedingsof the 3rd ACM workshop on Artificial Intelligence andSecurity, pages 14?18.
ACM.Patrick Juola.
2012.
An overview of the traditional author-ship attribution subtask.
In PAN 2012 Lab, UncoveringPlagiarism, Authorship and Social Software Misuse,held in conjunction with CLEF 2012.V.
Keselj, F. Peng, N. Cercone, and C. Thomas.
2003.N-gram based author profiles for authorship attribution.In Proceedings of the Pacific Association for Computa-tional Linguistics, pages 255?264.Moshe Koppel, Jonathan Schler, and Shlomo Argamon.2011.
Authorship attribution in the wild.
LanguageResources and Evaluation, 45:83?94.David D. Lewis, Yiming Yang, Tony G. Rose, and FanLi.
2004.
Rcv1: A new benchmark collection for textcategorization research.
J. Mach.
Learn.
Res., 5:361?397, December.Kim Luyckx and Walter Daelemans.
2008a.
Authorshipattribution and verification with many authors and lim-ited data.
In Proceedings of the 22nd International Con-ference on Computational Linguistics (Coling 2008),pages 513?520, Manchester, UK, August.Kim Luyckx and Walter Daelemans.
2008b.
Personae: acorpus for author and personality prediction from text.In Proceedings of LREC-2008, the Sixth InternationalLanguage Resources and Evaluation Conference.Kim Luyckx and Walter Daelemans.
2010.
The effectof author set size and data size in authorship attribu-tion.
Literary and Linguistic Computing, pages 1?21,August.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of English: the Penn treebank.
Comput.
Linguist.,19(2):313?330, June.A.
Narayanan, H. Paskov, N.Z.
Gong, J. Bethencourt,E.
Stefanov, E.C.R.
Shin, and D. Song.
2012.
On thefeasibility of internet-scale author identification.
InProceedings of the 33rd conference on IEEE Sympo-sium on Security and Privacy, pages 300?314.
IEEE.S.
Plakias and E. Stamatatos.
2008.
Tensor space modelsfor authorship attribution.
In Proceedings of the 5thHellenic Conference on Artificial Intelligence: Theo-ries, Models and Applications, volume 5138 of LNCS,pages 239?249, Syros, Greece.67Sindhu Raghavan, Adriana Kovashka, and RaymondMooney.
2010.
Authorship attribution using proba-bilistic context-free grammars.
In Proceedings of the48th Annual Meeting of the ACL 2010, pages 38?42,Uppsala, Sweden, July.
Association for ComputationalLinguistics.E.
Stamatatos.
2007.
Author identification using imbal-anced and limited training texts.
In Proceedings of the18th International Workshop on Database and ExpertSystems Applications, DEXA ?07, pages 237?241, Sept.E.
Stamatatos.
2008.
Author identification: Using textsampling to handle the class imbalance problem.
Infor-mation Processing and Managemement, 44:790?799.I.
H. Witten and E. Frank.
2005.
Data Mining: Practi-cal Machine Learning Tools and Techniques.
MorganKauffmann, 2nd edition.Rong Zheng, Jiexun Li, Hsinchun Chen, and Zan Huang.2006.
A framework for authorship identification ofonline messages: Writing-style features and classifica-tion techniques.
Journal of the American Society forInformation Science and Technology, 57(3):378?393.68
