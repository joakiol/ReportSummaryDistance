Constraints on Non-Projective Dependency ParsingJoakim NivreVa?xjo?
University, School of Mathematics and Systems EngineeringUppsala University, Department of Linguistics and Philologyjoakim.nivre@msi.vxu.seAbstractWe investigate a series of graph-theoreticconstraints on non-projective dependencyparsing and their effect on expressivity,i.e.
whether they allow naturally occurringsyntactic constructions to be adequatelyrepresented, and efficiency, i.e.
whetherthey reduce the search space for the parser.In particular, we define a new measurefor the degree of non-projectivity in anacyclic dependency graph obeying thesingle-head constraint.
The constraints areevaluated experimentally using data fromthe Prague Dependency Treebank and theDanish Dependency Treebank.
The resultsindicate that, whereas complete linguisticcoverage in principle requires unrestrictednon-projective dependency graphs, limit-ing the degree of non-projectivity to atmost 2 can reduce average running timefrom quadratic to linear, while excludingless than 0.5% of the dependency graphsfound in the two treebanks.
This is a sub-stantial improvement over the commonlyused projective approximation (degree 0),which excludes 15?25% of the graphs.1 IntroductionData-driven approaches to syntactic parsing hasuntil quite recently been limited to representationsthat do not capture non-local dependencies.
Thisis true regardless of whether representations arebased on constituency, where such dependenciesare traditionally represented by empty categoriesand coindexation to avoid explicitly discontinuousconstituents, or on dependency, where it is morecommon to use a direct encoding of so-called non-projective dependencies.While this ?surface dependency approximation?
(Levy and Manning, 2004) may be acceptablefor certain applications of syntactic parsing, it isclearly not adequate as a basis for deep semanticinterpretation, which explains the growing body ofresearch devoted to different methods for correct-ing this approximation.
Most of this work has sofar focused either on post-processing to recovernon-local dependencies from context-free parsetrees (Johnson, 2002; Jijkoun and De Rijke, 2004;Levy and Manning, 2004; Campbell, 2004), or onincorporating nonlocal dependency information innonterminal categories in constituency represen-tations (Dienes and Dubey, 2003; Hockenmaier,2003; Cahill et al, 2004) or in the categories usedto label arcs in dependency representations (Nivreand Nilsson, 2005).By contrast, there is very little work on parsingmethods that allow discontinuous constructions tobe represented directly in the syntactic structure,whether by discontinuous constituent structuresor by non-projective dependency structures.
No-table exceptions are Plaehn (2000), where discon-tinuous phrase structure grammar parsing is ex-plored, and McDonald et al (2005b), where non-projective dependency structures are derived usingspanning tree algorithms from graph theory.One question that arises if we want to pursue thestructure-based approach is how to constrain theclass of permissible structures.
On the one hand,we want to capture all the constructions that arefound in natural languages, or at least to providea much better approximation than before.
On theother hand, it must still be possible for the parsernot only to search the space of permissible struc-tures in an efficient way but also to learn to selectthe most appropriate structure for a given sentencewith sufficient accuracy.
This is the usual tradeoff73between expressivity and complexity, where a lessrestricted class of permissible structures can cap-ture more complex constructions, but where theenlarged search space makes parsing harder withrespect to both accuracy and efficiency.Whereas extensions to context-free grammarhave been studied quite extensively, there are veryfew corresponding results for dependency-basedsystems.
Since Gaifman (1965) proved that hisprojective dependency grammar is weakly equiva-lent to context-free grammar, Neuhaus and Bro?ker(1997) have shown that the recognition problemfor a dependency grammar that can define arbi-trary non-projective structures is NP complete,but there are no results for systems of intermedi-ate complexity.
The pseudo-projective grammarproposed by Kahane et al (1998) can be parsedin polynomial time and captures non-local depen-dencies through a form of gap-threading, but thestructures generated by the grammar are strictlyprojective.
Moreover, the study of formal gram-mars is only partially relevant for research on data-driven dependency parsing, where most systemsare not grammar-based but rely on inductive infer-ence from treebank data (Yamada and Matsumoto,2003; Nivre et al, 2004; McDonald et al, 2005a).For example, despite the results of Neuhaus andBro?ker (1997), McDonald et al (2005b) performparsing with arbitrary non-projective dependencystructures in O(n2) time.In this paper, we will therefore approach theproblem from a slightly different angle.
Insteadof investigating formal dependency grammars andtheir complexity, we will impose a series of graph-theoretic constraints on dependency structures andsee how these constraints affect expressivity andparsing efficiency.
The approach is mainly ex-perimental and we evaluate constraints using datafrom two dependency-based treebanks, the PragueDependency Treebank (Hajic?
et al, 2001) and theDanish Dependency Treebank (Kromann, 2003).Expressivity is investigated by examining howlarge a proportion of the structures found in thetreebanks are parsable under different constraints,and efficiency is addressed by considering thenumber of potential dependency arcs that need tobe processed when parsing these structures.
Thisis a relevant metric for data-driven approaches,where parsing time is often dominated by the com-putation of model predictions or scores for sucharcs.
The parsing experiments are performed witha variant of Covington?s algorithm for dependencyparsing (Covington, 2001), using the treebank asan oracle in order to establish an upper boundon accuracy.
However, the results are relevantfor a larger class of algorithms that derive non-projective dependency graphs by treating everypossible word pair as a potential dependency arc.The paper is structured as follows.
In section 2we define dependency graphs, and in section 3we formulate a number of constraints that canbe used to define different classes of dependencygraphs, ranging from unrestricted non-projectiveto strictly projective.
In section 4 we introduce theparsing algorithm used in the experiments, and insection 5 we describe the experimental setup.
Insection 6 we present the results of the experimentsand discuss their implications for non-projectivedependency parsing.
We conclude in section 7.2 Dependency GraphsA dependency graph is a labeled directed graph,the nodes of which are indices corresponding tothe tokens of a sentence.
Formally:Definition 1 Given a set R of dependency types(arc labels), a dependency graph for a sentencex = (w1, .
.
.
, wn) is a labeled directed graphG = (V,E,L), where:1.
V = Zn+12.
E ?
V ?
V3.
L : E ?
RDefinition 2 A dependency graph G is well-formed if and only if:1.
The node 0 is a root (ROOT).2.
G is connected (CONNECTEDNESS).1The set of V of nodes (or vertices) is the setZn+1 = {0, 1, 2, .
.
.
, n} (n ?
Z+), i.e., the set ofnon-negative integers up to and including n. Thismeans that every token index i of the sentence is anode (1 ?
i ?
n) and that there is a special node0, which does not correspond to any token of thesentence and which will always be a root of thedependency graph (normally the only root).The set E of arcs (or edges) is a set of orderedpairs (i, j), where i and j are nodes.
Since arcs areused to represent dependency relations, we will1To be more exact, we require G to be weakly connected,which entails that the corresponding undirected graph is con-nected, whereas a strongly connected graph has a directedpath between any pair of nodes.74(?Only one of them concerns quality.?
)0 1RZ(Out-of ?AuxP2Pnichthem ?Atr3VBjeis ?Pred4Tjenonly ?AuxZ5Cjednaone-FEM-SG ?Sb6Rnato ?AuxP7N4kvalituquality? Adv8Z:..) ?AuxKFigure 1: Dependency graph for Czech sentence from the Prague Dependency Treebanksay that i is the head and j is the dependent ofthe arc (i, j).
As usual, we will use the notationi ?
j to mean that there is an arc connecting iand j (i.e., (i, j) ?
E) and we will use the nota-tion i??
j for the reflexive and transitive closureof the arc relation E (i.e., i ??
j if and only ifi = j or there is a path of arcs connecting i to j).The function L assigns a dependency type (arclabel) r ?
R to every arc e ?
E. Figure 1 showsa Czech sentence from the Prague DependencyTreebank with a well-formed dependency graphaccording to Definition 1?2.3 ConstraintsThe only conditions so far imposed on dependencygraphs is that the special node 0 be a root and thatthe graph be connected.
Here are three furtherconstraints that are common in the literature:3.
Every node has at most one head, i.e., if i?jthen there is no node k such that k 6= i andk ?
j (SINGLE-HEAD).4.
The graph G is acyclic, i.e., if i?
j then notj ??
i (ACYCLICITY).5.
The graph G is projective, i.e., if i ?
j theni??
k, for every node k such that i < k < jor j < k < i (PROJECTIVITY).Note that these conditions are independent in thatnone of them is entailed by any (combination)of the others.
However, the conditions SINGLE-HEAD and ACYCLICITY together with the basicwell-formedness conditions entail that the graphis a tree rooted at the node 0.
These constraintsare assumed in almost all versions of dependencygrammar, especially in computational systems.By contrast, the PROJECTIVITY constraint ismuch more controversial.
Broadly speaking, wecan say that whereas most practical systems fordependency parsing do assume projectivity, mostdependency-based linguistic theories do not.
Moreprecisely, most theoretical formulations of depen-dency grammar regard projectivity as the normbut also recognize the need for non-projectiverepresentations to capture non-local dependencies(Mel?c?uk, 1988; Hudson, 1990).In order to distinguish classes of dependencygraphs that fall in between arbitrary non-projectiveand projective, we define a notion of degree ofnon-projectivity, such that projective graphs havedegree 0 while arbitrary non-projective graphshave unbounded degree.Definition 3 Let G = (V,E,L) be a well-formeddependency graph, satisfying SINGLE-HEAD andACYCLICITY, and let Ge be the subgraph of Gthat only contains nodes between i and j for thearc e = (i, j) (i.e., Ve = {i+1, .
.
.
, j?1} if i < jand Ve = {j+1, .
.
.
, i?1} if i > j).1.
The degree of an arc e ?
E is the number ofconnected components c in Ge such that theroot of c is not dominated by the head of e.2.
The degree of G is the maximum degree ofany arc e ?
E.To exemplify the notion of degree, we note thatthe dependency graph in Figure 1 (which satisfiesSINGLE-HEAD and ACYCLICITY) has degree 1.The only non-projective arc in the graph is (5, 1)and G(5,1) contains three connected components,each of which consists of a single root node (2, 3and 4).
Since only one of these, 3, is not domi-nated by 5, the arc (5, 1) has degree 1.4 Parsing AlgorithmCovington (2001) describes a parsing strategy fordependency representations that has been known75since the 1960s but not presented in the literature.The left-to-right (or incremental) version of thisstrategy can be formulated in the following way:PARSE(x = (w1, .
.
.
, wn))1 for i = 1 up to n2 for j = i?
1 down to 13 LINK(i, j)The operation LINK(i, j) nondeterministicallychooses between (i) adding the arc i ?
j (withsome label), (ii) adding the arc j ?
i (with somelabel), and (iii) adding no arc at all.
In this way, thealgorithm builds a graph by systematically tryingto link every pair of nodes (i, j) (i > j).
Thisgraph will be a well-formed dependency graph,provided that we also add arcs from the root node0 to every root node in {1, .
.
.
, n}.
Assuming thatthe LINK(i, j) operation can be performed in someconstant time c, the running time of the algorithmis?ni=1 c(n ?
1) = c(n22 ?
n2 ), which in terms ofasymptotic complexity is O(n2).In the experiments reported in the followingsections, we modify this algorithm by making theperformance of LINK(i, j) conditional on the arcs(i, j) and (j, i) being permissible under the givengraph constraints:PARSE(x = (w1, .
.
.
, wn))1 for i = 1 up to n2 for j = i?
1 down to 13 if PERMISSIBLE(i, j, C)4 LINK(i, j)The function PERMISSIBLE(i, j, C) returns trueiff i ?
j and j ?
i are permissible arcs relativeto the constraint C and the partially built graphG.
For example, with the constraint SINGLE-HEAD, LINK(i, j) will not be performed if bothi and j already have a head in the dependencygraph.
We call the pairs (i, j) (i > j) for whichLINK(i, j) is performed (for a given sentence andset of constraints) the active pairs, and we usethe number of active pairs, as a function of sen-tence length, as an abstract measure of runningtime.
This is well motivated if the time requiredto compute PERMISSIBLE(i, j, C) is insignificantcompared to the time needed for LINK(i, j), as istypically the case in data-driven systems, whereLINK(i, j) requires a call to a trained classifier,while PERMISSIBLE(i, j, C) only needs access tothe partially built graph G.The results obtained in this way will be partiallydependent on the particular algorithm used, butthey can in principle be generalized to any algo-rithm that tries to link all possible word pairs andthat satisfies the following condition:For any graph G = (V,E,L) derived bythe algorithm, if e, e?
?
E and e coverse?, then the algorithm adds e?
before e.This condition is satisfied not only by Covington?sincremental algorithm but also by algorithms thatadd arcs strictly in order of increasing length, suchas the algorithm of Eisner (2000) and other algo-rithms based on dynamic programming.5 Experimental SetupThe experiments are based on data from two tree-banks.
The Prague Dependency Treebank (PDT)contains 1.5M words of newspaper text, annotatedin three layers (Hajic?, 1998; Hajic?
et al, 2001)according to the theoretical framework of Func-tional Generative Description (Sgall et al, 1986).Our experiments concern only the analytical layerand are based on the dedicated training section ofthe treebank.
The Danish Dependency Treebank(DDT) comprises 100K words of text selectedfrom the Danish PAROLE corpus, with annotationof primary and secondary dependencies based onDiscontinuous Grammar (Kromann, 2003).
Onlyprimary dependencies are considered in the exper-iments, which are based on 80% of the data (againthe standard training section).The experiments are performed by parsing eachsentence of the treebanks while using the goldstandard dependency graph for that sentence as anoracle to resolve the nondeterministic choice in theLINK(i, j) operation as follows:LINK(i, j)1 if (i, j) ?
Eg2 E ?
E ?
{(i, j)}3 if (j, i) ?
Eg4 E ?
E ?
{(j, i)}where Eg is the arc relation of the gold standarddependency graph Gg and E is the arc relation ofthe graph G built by the parsing algorithm.Conditions are varied by cumulatively addingconstraints in the following order:1.
SINGLE-HEAD2.
ACYCLICITY3.
Degree d ?
k (k ?
1)4.
PROJECTIVITY76Table 1: Proportion of dependency arcs and complete graphs correctly parsed under different constraintsin the Prague Dependency Treebank (PDT) and the Danish Dependency Treebank (DDT)PDT DDTConstraint Arcs Graphs Arcs Graphsn = 1255590 n = 73088 n = 80193 n = 4410PROJECTIVITY 96.1569 76.8498 97.7754 84.6259d ?
1 99.7854 97.7507 99.8940 98.0272d ?
2 99.9773 99.5731 99.9751 99.5238d ?
3 99.9956 99.9179 99.9975 99.9546d ?
4 99.9983 99.9863 100.0000 100.0000d ?
5 99.9987 99.9945 100.0000 100.0000d ?
10 99.9998 99.9986 100.0000 100.0000ACYCLICITY 100.0000 100.0000 100.0000 100.0000SINGLE-HEAD 100.0000 100.0000 100.0000 100.0000None 100.0000 100.0000 100.0000 100.0000The purpose of the experiments is to study howdifferent constraints influence expressivity andrunning time.
The first dimension is investigatedby comparing the dependency graphs producedby the parser with the gold standard dependencygraphs in the treebank.
This gives an indication ofthe extent to which naturally occurring structurescan be parsed correctly under different constraints.The results are reported both as the proportion ofindividual dependency arcs (per token) and as theproportion of complete dependency graphs (persentence) recovered correctly by the parser.In order to study the effects on running time,we examine how the number of active pairs variesas a function of sentence length.
Whereas theasymptotic worst-case complexity remains O(n2)under all conditions, the average running time willdecrease with the number of active pairs if theLINK(i, j) operation is more expensive than thecall to PERMISSIBLE(i, j, C).
For data-drivendependency parsing, this is relevant not only forparsing efficiency, but also because it may improvetraining efficiency by reducing the number of pairsthat need to be included in the training data.6 Results and DiscussionTable 1 displays the proportion of dependencies(single arcs) and sentences (complete graphs) inthe two treebanks that can be parsed exactly withCovington?s algorithm under different constraints.Starting at the bottom of the table, we see thatthe unrestricted algorithm (None) of course repro-duces all the graphs exactly, but we also see thatthe constraints SINGLE-HEAD and ACYCLICITYdo not put any real restrictions on expressivitywith regard to the data at hand.
However, this isprimarily a reflection of the design of the treebankannotation schemes, which in themselves requiredependency graphs to obey these constraints.2If we go to the other end of the table, we seethat PROJECTIVITY, on the other hand, has a verynoticeable effect on the parser?s ability to capturethe structures found in the treebanks.
Almost 25%of the sentences in PDT, and more than 15% inDDT, are beyond its reach.
At the level of indi-vidual dependencies, the effect is less conspicu-ous, but it is still the case in PDT that one depen-dency in twenty-five cannot be found by the parsereven with a perfect oracle (one in fifty in DDT).
Itshould be noted that the proportion of lost depen-dencies is about twice as high as the proportionof dependencies that are non-projective in them-selves (Nivre and Nilsson, 2005).
This is due toerror propagation, since some projective arcs areblocked from the parser?s view because of missingnon-projective arcs.Considering different bounds on the degree ofnon-projectivity, finally, we see that even the tight-est possible bound (d ?
1) gives a much betterapproximation than PROJECTIVITY, reducing the2It should be remembered that we are only concerned withone layer of each annotation scheme, the analytical layer inPDT and the primary dependencies in DDT.
Taking severallayers into account simultaneously would have resulted inmore complex structures.77Table 2: Quadratic curve estimation for y = ax+ bx2 (y = number of active pairs, x = number of words)PDT DDTConstraint a b r2 a b r2PROJECTIVITY 1.9181 0.0093 0.979 1.7591 0.0108 0.985d ?
1 3.2381 0.0534 0.967 2.2049 0.0391 0.969d ?
2 3.1467 0.1192 0.967 2.0273 0.0680 0.964ACYCLICITY 0.3845 0.2587 0.971 1.4285 0.1106 0.967SINGLE-HEAD 0.7187 0.2628 0.976 1.9003 0.1149 0.967None ?0.5000 0.5000 1.000 ?0.5000 0.5000 1.000proportion of non-parsable sentences with about90% in both treebanks.
At the level of individualarcs, the reduction is even greater, about 95% forboth data sets.
And if we allow a maximum degreeof 2, we can capture more than 99.9% of all depen-dencies, and more than 99.5% of all sentences, inboth PDT and DDT.
At the same time, there seemsto be no principled upper bound on the degree ofnon-projectivity, since in PDT not even an upperbound of 10 is sufficient to correctly capture alldependency graphs in the treebank.3Let us now see how different constraints affectrunning time, as measured by the number of ac-tive pairs in relation to sentence length.
A plot ofthis relationship for a subset of the conditions canbe found in Figure 2.
For reasons of space, weonly display the data from DDT, but the PDT dataexhibit very similar patterns.
Both treebanks arerepresented in Table 2, where we show the resultof fitting the quadratic equation y = ax + bx2 tothe data from each condition (where y is the num-ber of active words and x is the number of words inthe sentence).
The amount of variance explained isgiven by the r2 value, which shows a very good fitunder all conditions, with statistical significancebeyond the 0.001 level.4Both Figure 2 and Table 2 show very clearlythat, with no constraints, the relationship betweenwords and active pairs is exactly the one predictedby the worst case complexity (cf.
section 4) andthat, with each added constraint, this relationshipbecomes more and more linear in shape.
When weget to PROJECTIVITY, the quadratic coefficient bis so small that the average running time is prac-tically linear for the great majority of sentences.3The single sentence that is not parsed correctly at d ?
10has a dependency arc of degree 12.4The curve estimation has been performed using SPSS.However, the complexity is not much worse forthe bounded degrees of non-projectivity (d ?
1,d ?
2).
More precisely, for both data sets, thelinear term ax dominates the quadratic term bx2for sentences up to 50 words at d ?
1 and up to30 words at d ?
2.
Given that sentences of 50words or less represent 98.9% of all sentences inPDT and 98.3% in DDT (the corresponding per-centages for 30 words being 88.9% and 86.0%), itseems that the average case running time can beregarded as linear also for these models.7 ConclusionWe have investigated a series of graph-theoreticconstraints on dependency structures, aiming tofind a better approximation than PROJECTIVITYfor the structures found in naturally occurringdata, while maintaining good parsing efficiency.In particular, we have defined the degree of non-projectivity in terms of the maximum number ofconnected components that occur under a depen-dency arc without being dominated by the headof that arc.
Empirical experiments based on datafrom two treebanks, from different languages andwith different annotation schemes, have shownthat limiting the degree d of non-projectivity to1 or 2 gives an average case running time that islinear in practice and allows us to capture about98% of the dependency graphs actually found inthe treebanks with d ?
1, and about 99.5% withd ?
2.
This is a substantial improvement overthe projective approximation, which only allows75?85% of the dependency graphs to be capturedexactly.
This suggests that the integration of suchconstraints into non-projective parsing algorithmswill improve both accuracy and efficiency, but wehave to leave the corroboration of this hypothesisas a topic for future research.780.0 20.0 40.0 60.0 80.0 100.0Words0.001000.002000.003000.004000.00PairsNone0.0 20.0 40.0 60.0 80.0 100.0Words0.00200.00400.00600.00800.001000.001200.00PairsSingle-Head0.0 20.0 40.0 60.0 80.0 100.0Words0.00200.00400.00600.00800.001000.001200.00PairsAcyclic0.0 20.0 40.0 60.0 80.0 100.0Words0.00200.00400.00600.00800.00Pairsd <= 20.0 20.0 40.0 60.0 80.0 100.0Words0.00100.00200.00300.00400.00500.00600.00Pairsd <= 10.0 20.0 40.0 60.0 80.0 100.0Words0.0050.00100.00150.00200.00250.00PairsProjectivityFigure 2: Number of active pairs as a function of sentence length under different constraints (DDT)79AcknowledgmentsThe research reported in this paper was partiallyfunded by the Swedish Research Council (621-2002-4207).
The insightful comments of threeanonymous reviewers helped improve the finalversion of the paper.ReferencesAoife Cahill, Michael Burke, Ruth O?Donovan, JosefVan Genabith, and Andy Way.
2004.
Long-distance dependency resolution in automatically ac-quiredwide-coverage PCFG-based LFG approxima-tions.
Proceedings of ACL, pp.
320?327.Richard Campbell.
2004.
Using linguistic principlesto recover empty categories.
Proceedings of ACL,pp.
646?653.Michael Collins, Jan Hajic?, Eric Brill, Lance Ramshaw,and Christoph Tillmann.
1999.
A statistical parserfor Czech.
Proceedings of ACL, pp.
505?512.Michael A. Covington.
2001.
A fundamental algo-rithm for dependency parsing.
Proceedings of the39th Annual ACM Southeast Conference, pp.
95?102.Pe?ter Dienes and Amit Dubey.
2003.
Deep syntac-tic processing by combining shallow methods.
Pro-ceedings of ACL, pp.
431?438.Jason M. Eisner.
2000.
Bilexical grammars and theircubic-time parsing algorithms.
In Harry Bunt andAnton Nijholt, editors, Advances in Probabilisticand Other Parsing Technologies, pp.
29?62.
Kluwer.Haim Gaifman.
1965.
Dependency systems andphrase-structure systems.
Information and Control,8:304?337.Jan Hajic?, Barbora Vidova Hladka, Jarmila Panevova?,Eva Hajic?ova?, Petr Sgall, and Petr Pajas.
2001.Prague Dependency Treebank 1.0.
LDC, 2001T10.Jan Hajic?.
1998.
Building a syntactically annotatedcorpus: The Prague Dependency Treebank.
Issuesof Valency and Meaning, pp.
106?132.
Karolinum.Julia Hockenmaier.
2003.
Data and Models for Sta-tistical Parsing with Combinatory Categorial Gram-mar.
Ph.D. thesis, University of Edinburgh.Richard A. Hudson.
1990.
English Word Grammar.Blackwell.Valentin Jijkoun and Maarten De Rijke.
2004.
En-riching the output of a parser using memory-basedlearning.
Proceedings of ACL, pp.
312?319.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
Proceedings of ACL, pp.
136?143.Sylvain Kahane, Alexis Nasr and Owen Rambow.Pseudo-Projectivity: A Polynomially Parsable Non-Projective Dependency Grammar.
Proceedings ofACL-COLING, pp.
646?652.Matthias Trautner Kromann.
2003.
The Danish De-pendency Treebank and the DTAG treebank tool.Proceedings of TLT, pp.
217?220.Roger Levy and Christopher Manning.
2004.
Deepdependencies from context-free statistical parsers:Correcting the surface dependency approximation.Proceedings of ACL, pp.
328?335.Hiroshi Maruyama.
1990.
Structural disambiguationwith constraint propagation.
Proceedings of ACL,pp.
31?38.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005a.
Online large-margin training of de-pendency parsers.
Proceedings of ACL, pp.
91?98.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
Proceedings ofHLT/EMNLP, pp.
523?530.Igor Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press.Peter Neuhaus and Norbert Bro?ker.
1997.
The com-plexity of recognition of linguistically adequate de-pendency grammars.
Proceedings of ACL-EACL,pages 337?343.Joakim Nivre and Jens Nilsson.
2005.
Pseudo-projective dependency parsing.
Proceedings ACL,pp.
99?106.Joakim Nivre, Johan Hall, and Jens Nilsson.
2004.Memory-based dependency parsing.
Proceedings ofCoNLL, pp.
49?56.Oliver Plaehn.
2000.
Computing the most probablyparse for a discontinuous phrase structure grammar.Proceedings of IWPT.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The Meaning of the Sentence in Its Pragmatic As-pects.
Reidel.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
Proceedings of IWPT, pp.
195?206.80
