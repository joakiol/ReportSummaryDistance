Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 507?516,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsFeature-Based Selection of Dependency Pathsin Ad Hoc Information RetrievalK.
Tamsin MaxwellSchool of InformaticsUniversity of EdinburghEdinburgh EH8 9AB, UKt.maxwell@ed.ac.ukJon OberlanderSchool of InformaticsUniversity of EdinburghEdinburgh EH8 9AB, UKj.oberlander@ed.ac.ukW.
Bruce CroftDept.
of Computer ScienceUniversity of MassachusettsAmherst, MA 01003, USAcroft@cs.umass.eduAbstractTechniques that compare short text seg-ments using dependency paths (or simply,paths) appear in a wide range of automatedlanguage processing applications includingquestion answering (QA).
However, fewmodels in ad hoc information retrieval (IR)use paths for document ranking due tothe prohibitive cost of parsing a retrievalcollection.
In this paper, we introduce aflexible notion of paths that describe chainsof words on a dependency path.
Thesechains, or catenae, are readily applied instandard IR models.
Informative catenaeare selected using supervised machinelearning with linguistically informed fea-tures and compared to both non-linguisticterms and catenae selected heuristicallywith filters derived from work on paths.Automatically selected catenae of 1-2words deliver significant performancegains on three TREC collections.1 IntroductionIn the past decade, an increasing number oftechniques have used complex and effectivesyntactic and semantic features to determine thesimilarity, entailment or alignment between shorttexts.
These approaches are motivated by the ideathat sentence meaning can be flexibly captured bythe syntactic and semantic relations between words,and encoded in dependency parse tree fragments.Dependency paths (or simply, paths) are comparedusing techniques such as tree edit distance (Pun-yakanok et al, 2004; Heilman and Smith, 2010),relation probability (Gao et al, 2004) and parse treealignment (Wang et al, 2007; Park et al, 2011).Much work on sentence similarity usingdependency paths focuses on question answering(QA) where textual inference requires attentionto linguistic detail.
Dependency-based techniquescan also be highly effective for ad hoc informationretrieval (IR) (Park et al, 2011).
However, fewpath-based methods have been explored for adhoc IR, largely because parsing large documentcollections is computationally prohibitive.In this paper, we explore a flexible applicationof dependency paths that overcomes this difficulty.We reduce paths to chains of words called catenae(Osborne and Gro?, 2012) that capture salientsemantic content in an underspecified manner.Catenae can be used as lexical units in a reformu-lated query to explicitly indicate important wordrelationships while retaining efficient and flexibleproximity matching.
Crucially, this does notrequire parsing documents.
Moreover, catenae arecompatible with a variety of existing IR models.We hypothesize that catenae identify most unitsof salient knowledge in text.
This is becausethey are a condition for ellipsis, in which salientknowledge can be successfully omitted from text(Osborne and Gro?, 2012).
To our knowledge, thispaper is the first time that catenae are proposedas a means for term selection in IR, and whereellipsis is considered as a means for identificationof semantic units.We also extend previous work with developmentof a linguistically informed, supervised machinelearning technique for selection of informativecatenae.
Previous heuristic filters for dependencypaths (Lin and Pantel, 2001; Shen et al, 2005;Cui et al, 2005) can exclude informative relations.Alternatively, treating all paths as equally infor-mative (Punyakanok et al, 2004; Park et al, 2011;Moschitti, 2008) can generate noisy word relationsand is computationally intensive.The challenge of path selection is that noexplicit information in text indicates which pathsare relevant.
Consider the catenae captured byheuristic filters for the TREC1 query, ?Whatrole does blood-alcohol level play in automobileaccident fatalities?
(#358, Table 1).
It may appearobvious that the component words of ?role play?1Text REtrieval Conference, see http://trec.nist.gov/507blood alcohollevel playauto accidentaccident fatalrole playplay fatalblood alcohol playplay accident fatalauto accident fatallevel play fatalrole play fatalrole level playblood alcohollevel playauto accidentaccident fatalrole bloodalcohol levelplay autoblood alcohollevel playauto accidentaccident fatalrole playplay fatalCatenae Sequential dependenceGovernor?ependentQuery: What role does blood-alcohol level play in automobile* accident fatalities*?
(*abbreviated to `auto', `fatal')auto accidentaccident fatalplay fatalplay accident fatalauto accident fatalPredicate?rgumentauto accidentaccident fatalauto accident fatallevel play fatalrole play fatalNominal end slotsTable 1: Catenae derived from dependency paths, as selected by heuristic methods.
Selections arecompared to sequential bigrams that use no linguistic knowledge.and ?level play?
do not have an important semanticrelationship relative to the query, yet these catenaeare described by parent-child relations that arecommonly used to filter paths in text processingapplications.
Alternative filters that avoid suchtrivial word combinations also omit descriptions ofkey entities such as ?blood alcohol?, and identifylonger catenae that may be overly restrictive.These shortcomings suggest that an optimizedselection process may improve performance oftechniques that use dependency paths in ad hoc IR.We identify three previously proposed selectionmethods, and compare them on the task of catenaeselection for ad hoc IR.
Selections are testedusing three TREC collections: Robust04, WT10G,and GOV2.
This provides a diverse platform forexperiments.
We also develop a linguisticallyinformed machine learning technique for catenaeselection that captures both key aspects of heuristicfilters, and novel characteristics of catenae andpaths.
The basic idea is that selection, or weighting,of catenae can be improved by features that arespecific to paths, rather than generic for all terms.Results show that our selection method is moreeffective in identifying key catenae comparedto previously proposed filters.
Integration of theidentified catenae in queries also improves IR ef-fectiveness compared to a highly effective baselinethat uses sequential bigrams with no linguisticknowledge.
This model represents the obviousalternative to catenae for term selection in IR.The rest of this paper is organised as follows.
?2 reviews related work, ?3 describes catenaeand their linguistic motivation and ?4 describesour selection method.
?5 evaluates classificationexperiments using the supervised filter.
?6 presentsthe results of experiments in ad hoc IR.
Finally, ?7concludes the paper.2 Related workTechniques that compare short text segmentsusing dependency paths are applied to a wide rangeof automated language processing tasks, includingparaphrasing, summarization, entailment detection,QA, machine translation and the evaluation ofword, phrase and sentence similarity.
A genericapproach uses a matching function to compare adependency path between any two stemmed termsx and y in a sentence A with any dependency pathbetween x and y in sentence B.
The match scorefor A and B is computed over all dependencypaths in A.In QA this approach improves question repre-sentation, answer selection and answer rankingcompared to methods that use bag-of-wordsand ngram features (Surdeanu et al, 2011).
Forexample, Lin and Pantel (2001) present a methodto derive paraphrasing rules for QA using analysisof paths that connect two nouns; Echihabi andMarcu (2003) align all paths in questions withtrees for heuristically pruned answers; Cui etal.
(2005) score answers using a variation of theIBM translation model 1; Wang et al (2007)use quasi-synchronous translation to map allparent-child paths in a question to any path in ananswer; and Moschitti (2008) explores syntacticand semantic kernels for QA classification.In ad hoc IR, most models of term dependenceuse word co-occurrence and proximity (Song andCroft, 1999; Metzler and Croft, 2005; Srikanth andSrihari, 2002; van Rijsbergen, 1993).
Syntacticlanguage models for IR are a significant departurefrom this trend (Gao et al, 2004; Lee et al, 2006;Cai et al, 2007; Maisonnasse et al, 2007) thatuse dependency paths to address long-distancedependencies and normalize spurious differencesin surface text.
Paths are constrained in both508prd loc pmod loc pmodIs    polio under control  in   China ?X1 X2 X3 X4 X5 X6poliopolio controlcontrolcontrol ChinaChinapolio control Chinapolio       under        controlcontrol       in       Chinapolio       under        control       in        Chinaloc pmodloc pmodloc pmod loc pmodCatenae ?toplisted?
Dependency pathsFigure 1: Catenae are an economical and intuitiverepresentation of dependency paths.queries and documents to parent-child relations.In contrast, (Park et al, 2011) present a quasi-synchronous translation model for IR that does notlimit paths.
This is based on the observation thatsemantically related words have a variety of directand indirect relations.
All of these models requireparsing of an entire document collection.Techniques using dependency paths in both QAand ad hoc IR show promising results, but thereis no clear understanding of which path constraintsresult in the greatest IR effectiveness.
We directlycompare selections of catenae as a simplifiedrepresentation of paths.In addition, a vast number of methods havebeen presented for term weighting and selectionin ad hoc IR.
Our supervised selection extends thesuccessful method presented by Bendersky andCroft (2008) for selection and weighting of querynoun phrases (NPs).
It also extends work for deter-mining the variability of governor-dependent pairs(Song et al, 2008).
In contrast to this work, weapply linguistic features that are specific to catenaeand dependency paths, and select among unitscontaining more than two content-bearing words.3 Catenae as semantic unitsCatenae (Latin for ?chain?, singular catena) aredependency-based syntactic units.
This sectionoutlines their unique semantic properties.A catena is defined on a dependency graph thathas lexical nodes (or words) linked by binary asym-metrical relations called dependencies.
Depen-dencies hold between a governor and a dependentand may be syntactic or semantic in nature (Nivre,2005).
A dependency graph is usually acyclic suchthat each node has only one governor, and one rootnode of the tree does not depend on any other node.A catena is a word, or sequence of words that arecontinuous with respect to a walk on a dependencyIs polio under control in China, and is polio under control in India?AntecedentFirst conjunct:Antecedent clause Second conjunct:Elliptical/target clauseElided text RemnantFigure 2: Ellipsis in a coordinated construct.graph.
For example, Fig.
1 shows a dependencyparse that generates 21 catenae in total: (usingi for Xi) 1, 2, 3, 4, 5, 6, 12, 23, 34, 45, 56, 123,234, 345, 456, 1234, 2345, 3456, 12345, 23456,123456.
We process catenae to remove stop wordson the INQUERY stoplist (Allan et al, 2000) andlexical units containing 18 TREC description stopwords such as ?describe?.
This results in a reducedset of catenae as shown in Fig.
1.A dependency path is ordered and includes bothword tokens and the relations between them.
Incontrast, a catena is a set of word types that maybe ordered or partially ordered.
A catena is aneconomical, intuitive lexical unit that correspondsto a dependency path and is argued to play animportant role in syntax (Osborne et al, 2012).In this paper, we explore catenae instead of pathsfor ad hoc IR due to their suitability for efficient IRmodels and flexible representation of language se-mantics.
Specifically, we note that catenae identifywords that can be omitted in elliptical constructions(Osborne et al, 2012).
They thus represent salientsemantic information in text.
To clarify this insight,we briefly review catenae in ellipsis.3.1 Semantic units in ellipsisFig.
2 shows terminology for the phenomenonof ellipsis.
The omitted words are called elidedtext, and words that could be omitted, but are not,we call elliptical candidates.Ellipsis relies on the logical structure of acoordinated construction in which two or moreelements, such as sentences, are joined by aconjunctive word or phrase such as ?and?
or?more than?.
A coordinated structure is requiredbecause the omitted words are ?filled in?
byassuming a parallel relation p between the firstand second conjunct.
In ellipsis, p is omitted andits arguments are retained in text.
In order forellipsis to be successful and grammatically correct,p must be salient shared knowledge at the time ofcommunication (Prince, 1986; Steedman, 1990).
Ifp is salient then the omitted text can be inferred.
Ifp is not salient then the omission of words merelyresults in ungrammatical, or incoherent, sentences.This framework is practically illustrated in Fig.509Is polio under control in China, and ?is polio under control?
in India ?
Is polio under control in China, and is cancer under observation ?in China?
?
* Is polio under control in China, and ?is?
cancer ?nder?
observation ?in China?
?
* Is polio under control in China, and ?is polio?
under ?ontrol in?
India ?Caatentn ?optpslin ds?ip to tlsat?c lyih s?i ?liosia?
in India    ?b?
is cancer under observation        ?c?
*    cancer            observation        ?d?
*                under                 India   ?Is polio under control in China, and...Caatpip niolio?inFigure 3: For ellipsis to be successful, elided words must be catenae.
Ellipsis candidates are catenae2.Is    polio under control  in   China ?X1 X2X3 X4X5 X6Figure 4: A parse in which ?polio China?
is acatena.3 for the query, ?Is polio under control in China?
?.Sentences marked by * are incoherent, and it isevident that the omitted words do not form a salientsemantic unit.
They also do not form catenae.
Incontrast, the omitted words in successful ellipsisdo form catenae, and they represent informativeword combinations with respect to the query.
Thisobservation leads us to an ellipsis hypothesis:Ellipsis hypothesis: For queries formulatedinto coordinated structures, the subset ofcatenae that are elliptical candidates identifythe salient semantic units in the query.3.2 Limitations of paths and catenaeThe prediction of salient semantic units by cate-nae is quite robust.
However, there are two prob-lems that can limit the effectiveness of any tech-nique that uses catenae or dependency paths in IR.1) Syntactic ambiguity: We make the simpli-fying assumption that the most probable parse ofa query is accurate and sufficient for the extractionof relevant catenae.
However, this is not alwaystrue.
For example, the sentence ?Is polio undercontrol in China, and under observation ?
?constitutes successful ellipsis.
The elided words?polio in china?
are relevant to a base query, ?Ispolio under control in China??.
Unfortunately,in Fig.
1 the elided text does not qualify as acatena.
A parse with alternative prepositionalphrase attachment is shown in Fig.
4.
Here, thesuccessfully elided text does qualify as a catena.This highlights the fact that a single dependencyparse may only partially represent the ambiguoussemantics of a query.
More accurate parsing doesnot address this problem.2) Rising: Automatic extraction of catenae islimited by the phenomenon of rising.
Let theIs poolooiundeoer cdeltoolsooolooC lhuaX4X3X2X1X5X6 X7Standard structure?ooiundeoer cdeltoIs poolsooolooC lhuaX3X2X1 X4g X5X6 X7Rising structureFigure 5: A parse with and without rising.
Thedashed dependency edge marks where a head isnot also the governor and the g-script marks thegovernor of the risen catena.governor of a catena be the word that licensesit (in Fig.
5 ?used?
licenses ?a toxic chemical?e.g.
?used what??).
Let the head of a catena beits parent in a dependency tree.
Rising occurswhen the head is not the same as the governor.This is frequently seen with wh-fronting questionsthat start who, what etc., as well as with manyother syntactic discontinuities (Osborne and Gro?,2012).
More specifically, rising occurs when acatena is separated from its governor by wordsthat its governor does not dominate, or the catenadominates the governor, as in Fig.
5.
Note thatin the risen structure, the words for the catena?chemical as a weapon?
are discontinuous on thesurface, interrupted by the word ?used?.4 Selection method for catenaeCatenae describe relatively few of the possibleword combinations in a sentence, but still includemany combinations that do not result in successfulellipsis and are not informative for IR.This section describes our supervised methodfor selection of informative catenae.
Candidatecatenae are identified using two constraints thatenable more efficient extraction: stopwords areremoved, and stopped catenae must contain fewerthan four words (single words are permitted).
Weuse a pseudo-projective joint dependency parseand semantic role labelling system (Johansson and510Nugues, 2008) to generate the dependency parse.This enables us to explore semantic classificationfeatures and is highly accurate.
However, anydependency parser may be applied instead.
Forcomparison, catenae extracted from 500 queriesusing the Stanford dependency parser (de Marneffeet al, 2006) overlap with 77% of catenae extractedfrom the same queries using the applied parser.4.1 Feature ClassesFour feature classes are presented in Table 2:Ellipsis candidates: The ellipsis hypothesissuggests that informative catenae are ellipticalcandidates.
However, queries are not in thecoordinated structures required for ellipsis.
Toenable extraction of characteristic features we (a)construct a coordinated query by adding the queryto itself; and (b) elide catenae from the secondconjunct.
For example, for the query, Is poliounder control in China?
we have:(a) Is polio under control in China, and ispolio under control in China?
(b) Is polio under control in China, and ispolio in China?We refer to the words in (b) as the query remainderand use this to identify features detailed in Table 2.Dependency path features: Part-of-speechtags and semantic roles have been used to filterdependency paths.
We identify several features thatuse these characteristics from prior work (Table 2).In addition, variability in the separation distancein documents observed for words that havegovernor-dependent relations in queries has beenproposed for identification of promising paths(Song et al, 2008).
We also observe that due to thephenomenon of rising, words that form catenae canbe discontinuous in text, and the ability of catenaeto match similar word combinations is limited byvariability of how they appear in documents.
Thus,we propose features for separation distance, but useefficient collection statistics rather than summingstatistics for every document in a collection.Co-occurrence features: A governor w1 tendsto subcategorize for its dependents wn.
Thismeans that w1 often determines the choice of wn.We conclude that co-occurrence is an importantfeature of dependency relations (Mel?c?uk, 2003).In addition, term frequencies and inverse documentfrequencies calculated using word co-occurrencemeasures are commonly used in IR.
We usefeatures previously proposed for filtering terms inIR (Bendersky and Croft, 2008) with two methodsto normalize co-occurrence counts for catenae ofdifferent lengths: a factor |c||c|, where |c| is thenumber of words in catena c (Hagen et al, 2011),and the average score for a feature type over allpairwise word combinations in c.IR performance predictors: Catenae take thesame form as typical IR search terms.
For thisreason, we also use predictors of IR effectivenesspreviously applied to IR terms.In general, path and co-occurrence features aresimilar to those applied by Surdeanu et al (2011)but we do not parse documents.
Path featuresare also similar to Song et al (2008), but moreefficient and suited to units of variable length.Ellipsis features have not been used before.5 Experimental setup5.1 ClassificationCatenae selection is framed as a supervisedclassification problem trained on binary humanjudgments of informativeness: how well catenaerepresent a query and discriminate betweenrelevant and non-relevant documents in a col-lection.
Kappa for two annotators on catenaein 100 sample queries was 0.63, and test-retestreliability for individual judges was similar (0.62)3.Although this is low, human annotations producedconsistently better classification accuracy thanother labelling methods explored.We use the Weka (Hall et al, 2009) Ad-aBoost.M1 meta-classifier (Freund and Schapire,1996) with unpruned C4.5 decision trees as baselearners to classify catenae as informative ornot.
Adaboost.M1 boosts decisions over T weaklearners for T features using weighted majorityvoting.
At each round, predictions of a new learnerare focused on incorrectly classified examplesfrom the previous round.
Adaboost.M1 wasselected in preference to other algorithms becauseit performed better in preliminary experiments,leverages many weak features to advantage, andusually does not overfit (Schapire et al, 1997).Predictions are made using 10-fold cross-validation.
There are roughly three times thenumber of uninformative catenae compared toinformative catenae.
In addition, the number oftraining examples is small (1295 to 5163 per collec-tion).
To improve classifier accuracy, the trainingdata for each collection is supplemented andbalanced by generating examples from queries for3Catenae, judgments and annotation details available atciir.cs.umass.edu/?tmaxwell511isSeqMinimum perplexity of ngrams with length 2, 3, and 4 in a window of up to a 3 words around the site of catenae omission.
This is the area where ungrammaticality may be introduced.
For the remainder R=`ABCDE&ABE' we compute ppl1 for ?ABE, &AB, ABE, &A, AB, BE?R_ppl1R_strictCompliance with strict hand?oded rules for grammaticality of a remainder.
Rules include unlikely orderings of punctuation and part?f?speech ?OS?
tags ?.g.
,, ?, poor placement of determiners and punctuation, and orphaned words, such as adjectives without the nouns they modify.R_relaxA relaxed version of hand?oded rules for R_strict.
Some rules were observed to be overly aggressive in detection of ungrammatical remainders.Ellipsis candidate features (E)Co-occurrence features (C)IR performance prediction features (I)c_ppl1Dependency path features (D) (continued)Dependency paths traverse nodes including stopwords and may be filtered based on POS tags.
We use perplexity for the sequence of POS tags in catenae before removing stopwords.
This is computed using a POS language model built on ukWaC parsed wikipedia data ?aroni et al, 2009?.phClassPhrasal class for a catena, with options NP, VP and Other.
A catena has a NP or VP class if it is, or is entirely contained by, an NP or VP ?ong et al, 2008?.NP_splitUnsuccessful ellipsis often results if elided words only partly describe a base NP.
Boolean feature for presence of a partial NP in the remainder.
NPs ?nd PPs?
are identified using the MontyLingua toolkit.PP_split As for NP_split, defined for prepositional phrases (PP).F_split As for NP_split, defined for finite clauses.semRoleBoolean feature indicating whether a catena describes all, or part of, a predicate?rgument structure ?AS?.
Previous work approximated PAS by using paths between head nouns and verbs, and all paths excluding those within base chunks.c_len Length of a stopped catenae.
Longer terms tend to reduce IR recall.Boolean indicating if catena words are sequential in stoplisted surface text.cf_owFrequency of a catena in the retrieval collection, words appearing ordered in a window the length of the catena.cf_uw As for cf_ow, but words may appear unordered.cf_uw8 As for cf_uw, but the window has a length of 8 words.idf_owInverse document frequency ?idf?
where documentfrequency ?df?
of a catena is calculated using cf_owwindows.
Let N  be the number of documents in the retrieval collection, then:idf(Ci) = log2Ndf(Ci)and idf(Ci) = N  if df(Ci) = 0.idf_uw As for idf_ow, but words may appear unordered.idf_uw8 As for idf_uw, but the window has a length of 8 words.gfGoogle ngrams frequency ?rants and Franz, 2006?
from a web crawl of approximately one trillion English word tokens.
Counts from a large collection are expected to be more reliable than those fromsmaller test collections.WIGNormalized Weighted Information Gain ?WIG?
is the change in information over top ranked documents between a random ranked list and an actual ranked list retrieved with a catena c ?hou and Croft, 2007?.wig(c) =1k?d?Dk(c) log p(c|d) ?
log p(c|C)?log p(c|C)where Dk are the top k=50 documents retrievedwith catena c from collection C, and p(c|?)
are maximum likelihood estimates.
A second feature uses the average WIG score for all pairwise word combinations in c.qf_inFrequency of appearance in queries from the Live Search 2006 search query log ?pproximately 15 million queries?.
Query log frequencies are a measure of the likelihood that a catena will appear in any query.wf_in As for qf_in, but using frequency counts in Wikipedia titles instead of queries.sepModeMost frequent separation distance of words in catena c in the retrieval collection, with possiblevalues S = ?1, 2, 3, long?.
1 means that all words areadjacent, 2 means separation by 0-1 words, and longmeans containment in a window of size 4 ?
|c|.H_cEntropy for separation distance s of words in catenac in the retrieval collection.fs is the frequency of cin window size s, and fS is the frequency of c in awindow of size 4 ?
|c| .
All f are normalized forcatena length using |c||c| ?agen et al, 2011?.Hc =?s?Sfs + 0.5fS + 0.5 log2fs + 0.5fS + 0.5sepRatioWhere fs and fS are defined as for H_c:sepRatioc =fs>2 + 0.5fS + 0.5wRatioFor words w in catena c; fS is defined as for H_c.wRatioc =0.5 + 1|c|?w?c fwfS + 0.5nomEndBoolean indicating whether the words at each end of the catena are nouns ?r the catena is a single noun?.Dependency path features (D)Table 2: Classifier features.512Feature ClassesPrROB04WT10GGOV2D-CIE-CIE-DE-D-CIR86.2 72.879.3 67.177.0 68.0Pr R83.5 67.576.9 59.770.9 61.8Pr R86.2 71.777.2 65.672.8 63.9Pr R86.2 72.079.6 66.175.5 67.2Table 3: Average classifier precision (Pr) and recall(R) over 10 folds.
Pr is % positive predictionsthat are correct.
R is % positive labeled instancespredicted as positive.
A combination of all classesmarginally performs best.other collections used in this paper, plus TREC8-QA.
For example, training data for Robust04includes data from WT10G, GOV2 and TREC8-QA.
Any examples that replicate catenae in the testcollection are excluded.
For Robust04, WT10Gand GOV2 respectively, 30%, 82% and 69% of thetraining data is derived from other collections.5.2 Classification resultsAverage classification precision and recall isshown in Table 3.
Co-occurrence and IR effective-ness prediction features (CI) was the most influen-tial class, and accounted for 70% of all features inthe model.
Performance is marginally better usingall features (E-D-CI) with a moderate improvementover human agreement on the annotation task.
TheE-D-CI filter is used in subsequent experiments.Catenae were predicted for all queries.
Predic-tions were more accurate for Robust04 than theother two collections.
One potential explanationis that Robust04 queries are longer on average(up to 32 content words per query, compared toup to 16 words) so they generate a more diverseset of catenae that are more easily distinguishedwith respect to informativeness.
The proportionof training data specific to the retrieval collectionmay also be a factor.
Longer queries produce agreater number of catenae, so less training datafrom other collections is required.6 Evaluation framework6.1 Baseline IR modelsBaselines are a unigram query likelihood (QL)model (bag of words) and a highly effectivesequential dependence (SD) variant of the Markovrandom field (MRF) model (Metzler and Croft,2005).
SD uses a linear combination of threecliques of terms, where each clique is prioritizedby a weight ?c.
The first clique contains individualwords (query likelihood QL), ?1 = 0.85.
Thesecond clique contains query bigrams that matchdocument bigrams in 2-word ordered windows(?#1?
), ?2 = 0.1.
The third clique uses the samebigrams as clique 2 with an 8-word unorderedwindow (?#uw8?
), ?3 = 0.05.
For example, thequery new york city in Indri4 query language is:#weight(?1 #combine(new york city)?2 #combine(#1(new york) #1(york city))?3 #combine(#uw8(new york) #uw8(york city)))SD is a competitive baseline in IR (Benderskyand Croft, 2008; Park et al, 2011; Xue et al,2010).
Our reformulated model uses the samequery format as SD, but the second and thirdcliques contain filtered catenae instead of querybigrams.
In addition, because catenae may bemulti-word units, we adjust the unordered windowsize to 4 ?
|c|.
So, if two catenae ?york?
and ?newyork city?
are selected, the last clique has the form:?3 #combine( york #uw12(new york city))This query representation enables word relationsto be explicitly indicated while maintainingefficient and flexible matching of catenae indocuments.
Moreover, it does not use dependencyrelations between words during retrieval, so thereis no need to parse a collection.6.2 Baseline catenae selectionWe explore four filters for catenae.
Three arebased on previous work and describe heuristicfeatures of promising catenae.
The fourth is ournovel supervised classifier.NomEnd: Catenae starting and ending withnouns, or containing only one word that is a noun.Paths between nouns are used by Lin and Pantel(2001).SemRol: Catenae in which all componentwords are either predicates or argument heads.This is based on work that uses paths between headnouns and verbs (Shen et al, 2005), semantic roles(Moschitti, 2008), and all dependency paths exceptthose that occur between words in the same basechunk (e.g.
noun / verb phrase) (Cui et al, 2005).GovDep: Cantenae containing words with agovernor-dependent relation.
Many IR modelsuse this form of path filtering e.g.
(Gao et al,2004; Wang et al, 2007).
Relations are ?collapsed?by removing stopwords to reduce the distancebetween content nodes in a dependency graph.4http://www.lemurproject.org/513ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrQL 25.25 28.69 19.55 22.77 25.77 31.26SD 26.57?
30.02?
20.63 24.31?
28.00?
33.30?NomEnd 25.91?
29.35?
20.81?
24.27?
27.41?
32.94?GovDep 26.26?
29.63?
21.06 24.23?
27.87?
33.51?SemRol 25.70?
29.06 19.78 22.93 26.76 32.49?SFeat 27.04?
30.11?
20.84?
24.31?
28.43?
33.84?SF-12 27.03?
30.20?
21.62?
24.81?
28.57?
34.01?Table 4: IR results using filtered catenae consistently improve over non-linguistic methods.Significance(p < .05) shown compared to QL (?)
and SD (?
).ROBUST04 WT10G GOV2MAP R-Pr MAP R-Pr MAP R-PrSF-12 27.03 30.20 21.62 24.81 28.57 34.01SF-123 26.83 30.34 21.34 24.64 28.77 34.24SF-NE 26.51 29.86 21.42 24.55 27.96 33.26SF-GD 26.22 29.48 20.33 23.72 28.30 33.83Gold 27.92 31.15 22.56 25.69 29.65 35.08Table 5: Results with supervised selection of catenae with specified length (SF-12, SF-123) are moreeffective than combinations of SFeat with heuristic NomEnd (SF-NE) or GovDep (SF-GD).6.3 ExperimentsExperiments compare queries reformulatedusing catenae selected by baseline filters and oursupervised selection method (SFeat) to SD anda bag-of-words model (QL).
We also compare IReffectiveness of all catenae filtered using SFeatwith approaches that combine SFeat with baselinefilters.
All models are implemented using the Indriretrieval engine version 4.12.6.4 ResultsResults in Table 4 show significant improvementin mean average precision (MAP) of queries usingcatenae compared to QL.
Consistent improvementsover SD are also demonstrated for supervisedselection applied to all catenae (SFeat) and catenaewith only 1-2 words (SF-12) across all collections(Table 5).
Overall, changes are small and fairlyrobust, with one half to two thirds of all queriesshowing less than 10% change in MAP.Unlike sFeat, other filters tend to decrease per-formance compared to SD.
Governor-dependentrelations for WT10G are an exception and we spec-ulate that this is due to a negative influence of 3-word catenae for this collection.
Manual inspectionsuggests that WT10G queries are short and haverelatively simple syntactic structure (e.g.
few PPattachment ambiguities).
This means that 3-wordcatenae (in all models except GovDep) tend to in-clude uninformative words, such as ?reasons?
in?fasting religious reasons?.
In contrast, 3-word cate-nae in other collections tend to identify query sub-concepts or phrases, such as ?science plants water?.Classification results for catenae separated bylength, such that the classifier for catenae with aspecific length are trained on examples of catenaewith the same length, confirm this intuition.
Therejection rate for 3-word catenae is twice as highfor WT10G as for other collections.
It is alsomore difficult to distinguish informative 3-wordcatenae compared to catenae with 1-2 words.
Toassess the impact of classification accuracy on IReffectiveness, Table 5 shows results with oracleknowledge of annotator judgments.The SF-12 model combines catenae predicted forlengths 1 and 2.
Its strong performance across allcollections suggests that most of the benefit derivedfrom catenae in IR is found in governor-dependentand single word units, where single words areimportant (GovDep uses only 2-word catenae).Another major observation (Table 5) is that mixingbaseline heuristic filters with a supervised ap-proach is not as successful as supervised selectionalone.
In particular, performance decreases forfiltered governor-dependent pairs.
This suggeststhat some important word relations in GovDep andNomEnd are captured by triangulation.Finally, we review selected catenae for queriesthat perform significantly better or worse than SD(> 75% change in MAP).
The best IR effectivenessoccurs when selected catenae clearly focus on themost important aspect of a query.
Poor perfor-514mance is caused by a lack of focus in a catenae set,even though selected catenae are reasonable, or anemphasis on words that are not central to the query.The latter can occur when words that are not es-sential to query semantics appear in many catenaedue to their position in the dependency graph.7 ConclusionWe presented a flexible implementation ofdependency paths for long queries in ad hoc IR thatdoes not require dependency parsing a collection.Our supervised selection technique for catenaeaddresses the need to balance a representation oflanguage expressiveness with effective, efficientstatistical methods.
This is a core challenge incomputational linguistics.It is not possible to directly compare perfor-mance of our approach with ad hoc techniques inIR that parse a retrieval collection.
However, wenote that a recent result using query translationbased on dependency paths (Park et al, 2011)reports 14% improvement over query likelihood(QL).
Our approach achieves 7% improvementover QL on the same collection.
We conclude thatcatenae do not replace path-based techniques, butmay offer some insight into their application, andhave particular value when it is not practical toparse target documents to determine text similarity.AcknowledgmentsThis work was supported in part by the Centerfor Intelligent Information Retrieval.
Any opinions,findings and conclusions or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect those of the sponsor.ReferencesJames Allan, Margaret E. Connell, W. Bruce Croft,Fang-Fang Feng, David Fisher, and Xiaoyan Li.2000.
INQUERY and TREC-9.
In Proceedings ofTREC-9, pages 551?562.Michael Bendersky and W. Bruce Croft.
2008.Discovering key concepts in verbose queries.
InProceedings of the 31st annual international ACMSIGIR conference on Research and development ininformation retrieval, SIGIR ?08, pages 491?498,New York, NY, USA.
ACM.Keke Cai, Jiajun Bu, Chun Chen, and Guang Qiu.2007.
A novel dependency language model for in-formation retrieval.
Journal of Zhejiang UniversitySCIENCE A, 8(6):871?882.Hang Cui, Renxu Sun, Keya Li, Min-Yen Kan,and Tat-Seng Chua.
2005.
Question answeringpassage retrieval using dependency relations.
InProceedings of the 28th annual international ACMSIGIR conference on Research and development ininformation retrieval, SIGIR ?05, pages 400?407,New York, NY, USA.
ACM.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of LREC-2006.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.In Proceedings of the 41st Annual Meeting onAssociation for Computational Linguistics - Volume1, ACL ?03, pages 16?23, Stroudsburg, PA, USA.Association for Computational Linguistics.Yoav Freund and Robert E. Schapire.
1996.
Experi-ments with a new boosting algorithm.
In ICML?96,pages 148?156.Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, andGuihong Cao.
2004.
Dependence language modelfor information retrieval.
In Proceedings of the 27thannual international ACM SIGIR conference onResearch and development in information retrieval,SIGIR ?04, pages 170?177, New York, NY, USA.ACM.Matthias Hagen, Martin Potthast, Benno Stein, andChristof Bra?utigam.
2011.
Query segmentationrevisited.
In Proceedings of the 20th internationalconference on World wide web, WWW ?11, pages97?106, New York, NY, USA.
ACM.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an up-date.
SIGKDD Explorations Newsletter, 11:10?18,November.Michael Heilman and Noah A. Smith.
2010.
Treeedit models for recognizing textual entailments,paraphrases, and answers to questions.
In Hu-man Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, HLT?10, pages 1011?1019, Stroudsburg, PA, USA.Association for Computational Linguistics.Richard Johansson and Pierre Nugues.
2008.Dependency-based syntactic?semantic analysiswith PropBank and NomBank.
In Proceedings ofCoNNL 2008, pages 183?187.Changki Lee, Gary Geunbae Lee, and Myung-GilJang.
2006.
Dependency structure language modelfor information retrieval.
In In ETRI journal,volume 28, pages 337?346.Dekang Lin and Patrick Pantel.
2001.
DIRT - discov-ery of inference rules from text.
In Proceedingsof ACM Conference on Knowledge Discoveryand Data Mining (KDD-01), pages 323?328, SanFrancisco, CA.515Lo?
?c Maisonnasse, Eric Gaussier, and Jean-PierreChevallet.
2007.
Revisiting the dependencelanguage model for information retrieval.
InProceedings of the 30th annual international ACMSIGIR conference on Research and development ininformation retrieval, SIGIR ?07, pages 695?696,New York, NY, USA.
ACM.Igor A. Mel?c?uk.
2003.
Levels of dependency inlinguistic description: Concepts and problems.
InV.
Agel, L. Eichinger, H.-W. Eroms, P. Hellwig,H.
J. Herringer, and H. Lobin, editors, Dependencyand Valency.
An International Handbook of Contem-porary Research, volume 1, pages 188?229.
WalterDe Gruyter, Berlin?New York.Donald Metzler and W. Bruce Croft.
2005.
A Markovrandom field model for term dependencies.
InProceedings of the 28th annual international ACMSIGIR conference on Research and development ininformation retrieval, SIGIR ?05, pages 472?479,New York, NY, USA.
ACM.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.In Proceeding of the 17th ACM conference onInformation and knowledge management, CIKM?08, pages 253?262, New York, NY, USA.
ACM.Joakim Nivre.
2005.
Dependency grammar and depen-dency parsing.
Technical report, Va?xjo?
University:School of Mathematics and Systems Engineering.Timothy Osborne and Thomas Gro?.
2012.
Con-structions are catenae: Construction grammarmeets dependency grammar.
Cognitive Linguistics,23(1):165?216.Timothy Osborne, Michael Putnam, and Gro?.
2012.Catenae: Introducing a novel unit of syntacticanalysis.
Syntax, 15(4):354?396, December.Jae Hyun Park, W. Bruce Croft, and David A. Smith.2011.
A quasi-synchronous dependence model forinformation retrieval.
In Proceedings of the 20thACM international conference on Information andknowledge management, CIKM ?11, pages 17?26,New York, NY, USA.
ACM.Ellen F. Prince.
1986.
On the syntactic marking ofpresupposed open propositions.
In Proceedings ofthe 22nd Annual Meeting of the Chicago LinguisticSociety, pages 208?222.V.
Punyakanok, D. Roth, and W. Yih.
2004.
Mappingdependencies trees: An application to questionanswering.
In Proceedings of AI and MATHSymposium 2004 (Special session: Intelligent TextProcessing).Robert E. Schapire, Yoav Freund, Peter Bartlett, andWee Sun Lee.
1997.
Boosting the margin: A newexplanation for the effectiveness of voting methods.In Proceedings of ICML, pages 322?330.Dan Shen, Geert-Jan M. Kruijff, and Dietrich Klakow.2005.
Exploring syntactic relation patterns forquestion answering.
In Proceedings of the Secondinternational joint conference on Natural LanguageProcessing, IJCNLP?05, pages 507?518, Berlin,Heidelberg.
Springer-Verlag.Fei Song and W. Bruce Croft.
1999.
A generallanguage model for information retrieval.
In Pro-ceedings of the 8th ACM international conferenceon Information and knowledge management, CIKM?99, pages 316?321, New York, NY, USA.
ACM.Young-In Song, Kyoung-Soo Han, Sang-Bum Kim,So-Young Park, and Hae-Chang Rim.
2008.
Anovel retrieval approach reflecting variability of syn-tactic phrase representation.
Journal of IntelligentInformation Systems, 31(3):265?286, December.Munirathnam Srikanth and Rohini Srihari.
2002.Biterm language models for document retrieval.
InProceedings of the 25th annual international ACMSIGIR conference on Research and development ininformation retrieval, SIGIR ?02, pages 425?426,New York, NY, USA.
ACM.Mark J. Steedman.
1990.
Gapping as Constituent Co-ordination.
Linguistics and Philosophy, 13(2):207?263, April.Mihai Surdeanu, Massimiliano Ciaramita, and HugoZaragoza.
2011.
Learning to rank answersto non-factoid questions from web collections.Computational Linguistics, 37(2):351?383, June.C.
J. van Rijsbergen.
1993.
A theoretical basis for theuse of co-occurrence data in information retrieval.Journal of Documentation, 33(2):106?119.Mengqiu Wang, Noah A. Smith, and Teruko Mitamura.2007.
What is the Jeopardy model?
a quasi-synchronous grammar for QA.
In Proceedings ofthe 2007 Joint Conference on Empirical Methods inNatural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL),pages 22?32, Prague, Czech Republic, June.Association for Computational Linguistics.Xiaobing Xue, Samuel Huston, and W. Bruce Croft.2010.
Improving verbose queries using subsetdistribution.
In Proceedings of the 19th ACM inter-national conference on Information and knowledgemanagement, CIKM ?10, pages 1059?1068, NewYork, NY, USA.
ACM.516
