Pattern Abstraction and Term Similarity for Word Sense Disambiguation:IRST at Senseval-3Carlo Strapparava and Alfio Gliozzo and Claudio GiulianoITC-irst, Istituto per la Ricerca Scientica e Tecnologica, I-38050 Trento, ITALY{strappa, gliozzo, giuliano}@itc.itAbstractThis paper summarizes IRST?s participation inSenseval-3.
We participated both in the English all-words task and in some lexical sample tasks (En-glish, Basque, Catalan, Italian, Spanish).
We fol-lowed two perspectives.
On one hand, for the all-words task, we tried to refine the Domain DrivenDisambiguation that we presented at Senseval-2.The refinements consist of both exploiting a newtechnique (Domain Relevance Estimation) for do-main detection in texts, and experimenting with theuse of Latent Semantic Analysis to avoid reliance onmanually annotated domain resources (e.g.
WORD-NET DOMAINS).
On the other hand, for the lexicalsample tasks, we explored the direction of patternabstraction and we demonstrated the feasibility ofleveraging external knowledge using kernel meth-ods.1 IntroductionThe starting point for our research in the WordSense Disambiguation (WSD) area was to explorethe use of semantic domains in order to solve lex-ical ambiguity.
At the Senseval-2 competition weproposed a new approach to WSD, namely DomainDriven Disambiguation (DDD).
This approach con-sists of comparing the estimated domain of the con-text of the word to be disambiguated with the do-mains of its senses, exploiting the property of do-mains to be features of both texts and words.
Thedomains of the word senses can be either inferredfrom the learning data or derived from the informa-tion in WORDNET DOMAINS.For Senseval-3, we refined the DDD methodol-ogy with a fully unsupervised technique - DomainRelevance Estimation (DRE) - for domain detectionin texts.
DRE is performed by an expectation maxi-mization algorithm for the gaussian mixture model,which is exploited to differentiate relevant domaininformation in texts from noise.
This refined DDDsystem was presented in the English all-words task.Originally DDD was developed to assess the use-fulness of domain information for WSD.
Thus itdid not exploit other knowledge sources commonlyused for disambiguation (e.g.
syntactic patterns orcollocations).
As a consequence the performance ofthe DDD system is quite good for precision (it dis-ambiguates well the ?domain?
words), but as far asrecall is concerned it is not competitive comparedwith other state of the art techniques.
On the otherhand DDD outperforms the state of the art for unsu-pervised systems, demonstrating the usefulness ofdomain information for WSD.In addition, the DDD approach requires domainannotations for word senses (for the experiments weused WORDNET DOMAINS, a lexical resource de-veloped at IRST).
Like all manual annotations, suchan operation is costly (more than two man yearshave been spent for labeling the whole WORDNETDOMAINS structure) and affected by subjectivity.Thus, one drawback of the DDD methodology wasa lack of portability among languages and amongdifferent sense repositories (unless we have synset-aligned WordNets).Besides the improved DDD, our other proposalsfor Senseval-3 constitute an attempt to overcomethese previous issues.To deal with the problem of having a domain-annotated WORDNET, we experimented with anovel methodology to automatically acquire domaininformation from corpora.
For this aim we esti-mated term similarity from a large scale corpus, ex-ploiting the assumption that semantic domains aresets of very closely related terms.
In particular weimplemented a variation of Latent Semantic Analy-sis (LSA) in order to obtain a vector representationfor words, texts and synsets.
LSA performs a di-mensionality reduction in the feature space describ-ing both texts and words, capturing implicitly thenotion of semantic domains required by DDD.
Inorder to perform disambiguation, LSA vectors havebeen estimated for the synsets in WORDNET.
Weparticipated in the English all-words task also witha first prototype (DDD-LSA) that exploits LSA in-stead of WORDNET DOMAINS.Association for Computational Linguisticsfor the Semantic Analysis of Text, Barcelona, Spain, July 2004SENSEVAL-3: Third International Workshop on the Evaluation of SystemsTask SystemsEnglish All-Words DDD DDD-LSAEnglish Lex-sample Kernels-WSD TiesItalian Lex-Sample Kernels-WSD TiesBasque Lex-Sample Kernels-WSDCatalan Lex-Sample Kernels-WSDSpanish Lex-Sample Kernels-WSDTable 1: IRST participation at Senseval-3As far as lexical sample tasks are concerned, weparticipated in the English, Italian, Spanish, Cata-lan, and Basque tasks.
For these tasks, we ex-plored the direction of pattern abstraction for WSD.Pattern abstraction is an effective methodology forWSD (Mihalcea, 2002).
Our preliminary experi-ments have been performed using TIES, a general-ized Information Extraction environment developedat IRST that implements the boosted wrapper induc-tion algorithm (Freitag and Kushmerick, 2000).
Themain limitation of such an approach is, once more,the integration of different knowledge sources.
Inparticular, paradigmatic information seems hard tobe represented in the TIES framework, motivatingour decision to exploit kernel methods for WSD.Kernel methods is an area of recent interest inMachine Learning.
Kernels are similarity functionsbetween instances that allows to integrate differentknowledge sources and to model explicitly linguis-tic insights inside the powerful framework of sup-port vector machine classification.
For Senseval-3we implemented the Kernels-WSD system, whichexploits kernel methods to perform the followingoperations: (i) pattern abstraction; (ii) combinationof different knowledge sources, in particular domaininformation and syntagmatic information; (iii) inte-gration of unsupervised term proximity estimationin the supervised framework.The paper is structured as follows.
Section 2 in-troduces LSA and its relations with semantic do-mains.
Section 3 presents the systems for the En-glish all-words task (i.e.
DDD and DDD-LSA).
Insection 4 our supervised approaches are reported.In particular the TIES system is described in section4.1, while the approach based on kernel methods isdiscussed in section 4.2.2 Semantic Domains and LSADomains are common areas of human discussion,such as economics, politics, law, science etc., whichare at the basis of lexical coherence.
A substantialpart of the lexicon is composed by ?domain words?,that refer to concepts belonging to specific domains.In (Magnini et al, 2002) it has been claimed thatdomain information provides generalized features atthe paradigmatic level that are useful to discriminateamong word senses.The WORDNET DOMAINS1 lexical resourceis an extension of WORDNET which providessuch domain labels for all synsets (Magnini andCavaglia`, 2000).
About 200 domain labels were se-lected from a number of dictionaries and then struc-tured in a taxonomy according to the Dewey Deci-mal Classification (DDC).
The annotation method-ology was mainly manual and took about 2 personyears.WORDNET DOMAINS has been proven a usefulresource for WSD.
However some aspects inducedus to explore further developments.
These issuesare: (i) it is difficult to find an objective a-priorimodel for domains; (ii) the annotation procedurefollowed to develop WORDNET DOMAINS is veryexpensive, making hard the replicability of the lexi-cal resource for other languages or domain specificsub-languages; (iii) the domain distinctions are rigidin WORDNET DOMAINS, while a more ?fuzzy?
as-sociation between domains and concepts is oftenmore appropriate to describe term similarity.In order to generalize the domain approach and toovercome these issues, we explored the direction ofunsupervised learning on a large-scale corpus (weused the BNC corpus for all the experiments de-scribed in this paper).In particular, we followed the LSA approach(Deerwester et al, 1990).
In LSA, term co-occurrences in the documents of the corpus are cap-tured by means of a dimensionality reduction oper-ated on the term-by-document matrix.
The result-ing LSA vectors can be exploited to estimate bothterm and document similarity.
Regarding documentsimilarity, Latent Semantic Indexing (LSI) is a tech-nique that allows one to represent a document bya LSA vector.
In particular, we used a variationof the pseudo-document methodology described in(Berry, 1992).
Each document can be represented inthe LSA space by summing up the normalized LSAvectors of all the terms contained in it.By exploiting LSA vectors for terms, it is pos-sible to estimate domain vectors for the synsets ofWORDNET, in order to obtain similarity values be-tween concepts that can be used for synset cluster-ing and WSD.
Thus, term and document vectors canbe used instead of WORDNET DOMAINS for WSDand other applications in which term similarity anddomain relevance estimation is required.1WORDNET DOMAINS is freely available for research pur-poses at wndomains.itc.it3 All-Words systems: DDD and DDD-LSADDD with DRE.
DDD assignes the right sense ofa word in its context comparing the domain of thecontext to the domain of each sense of the word.This methodology exploits WORDNET DOMAINSinformation to estimate both the domain of the tex-tual context and the domain of the senses of theword to disambiguate.The basic idea to estimate domain relevance fortexts is to exploit lexical coherence inside texts.
Asimple heuristic approach to this problem, used inSenseval-2, is counting the occurrences of domainwords for every domain inside the text: the higherthe percentage of domain words for a certain do-main, the more relevant the domain will be for thetext.Unfortunately, the simple local frequency countis not a good domain relevance measure for sev-eral reasons.
Indeed irrelevant senses of ambigu-ous words contribute to augment the final score ofirrelevant domains, introducing noise.
Moreover,the level of noise is different for different domainsbecause of their different sizes and possible dif-ferences in the ambiguity level of their vocabular-ies.
We refined the original Senseval-2 DDD systemwith the Domain Relevance Estimation (DRE) tech-nique.
Given a certain domain, DRE distinguishesbetween relevant and non-relevant texts by meansof a Gaussian Mixture model that describes the fre-quency distribution of domain words inside a large-scale corpus (in particular we used the BNC corpusalso in this case).
Then, an Expectation Maximiza-tion algorithm computes the parameters that maxi-mize the likelihood of the model on the empiricaldata (Gliozzo et al, 2004).In order to represent domain information we in-troduced the notion of Domain Vectors (DV), whichare data structures that collect domain information.These vectors are defined in a multidimensionalspace, in which each domain represents a dimen-sion of the space.
We distinguish between twokinds of DVs: (i) synset vectors, which representthe relevance of a synset with respect to each con-sidered domain and (ii) text vectors, which repre-sent the relevance of a portion of text with respectto each domain in the considered set.
The core ofthe DDD algorithm is based on scoring the compar-ison of these kinds of vectors.
The synset vectorsare built considering WORDNET DOMAINS, whilein the calculation of scoring the system takes intoaccount synset probabilities on SemCor.
The sys-tem makes use of a threshold th-cut, ranging in theinterval [0,1], that allows us to tune the tradeoff be-tween precision and recall.th-cut Prec Recall Attempted0.0 0.583 0.583 99.760.9 0.729 0.441 60.51Table 2: DDD on the English all-words task.Latent Semantic Domains for DDD.
As seen inSection 2, it is possible to implement a DDD ver-sion that does not use WORDNET DOMAINS andinstead it exploits LSA term and document vectorsfor estimating synset vectors and text vectors, leav-ing the core of DDD algorithm unchanged.
As fortext vectors, we used the psedo-document techniquealso for building synset vectors: in this case we con-sider the synonymous terms contained in the synsetitself.The system presented at Senseval-3 does notmake use of any statistics on SemCor, and conse-quently it can be considered fully unsupervised.
Re-sults are reported in table 3 and do not differ muchfrom the results obtained by DDD in the same task.th-cut Prec Recall Attempted0.5 0.661 0.496 75.01Table 3: DDD-LSA on the English all-words task.4 Lexical Sample Systems: Patternabstraction and Kernel MethodsOne of the most discriminative features for lexi-cal disambiguation is the lexical/syntactic pattern inwhich the word appears.
A well known issue in theWSD area is the one sense per collocation claim(Yarowsky, 1993) stating that the word meaningsare strongly associated with the particular colloca-tion in which the word is located.
Collocations aresequences of words in the context of the word todisambiguate, and can be associated to word sensesperforming supervised learning.Another important knowledge source for WSD isthe shallow-syntactic pattern in which a word ap-pears.
Syntactic patterns, like lexical patterns, canbe obtained by exploiting pattern abstraction tech-niques on POS sequences.
In the WSD literatureboth lexical and syntactic patterns have been usedas features in a supervised learning schema by rep-resenting each instance using bigrams and trigramsin the surrounding context of the word to be ana-lyzed2.2More recently deep-syntactic features have been also con-sidered by several systems, as for example modifiers of nounsand verbs, object and subject of the sentence, etc.
In order toRepresenting each instance by a ?bag of features?presents several disadvantages from the point ofview of both machine learning and computationallinguistics: (1) Sparseness in the learning data: mostof the collocations found in the learning data occurjust once, reducing the generalization power of thelearning algorithm.
In addition most of the collo-cations found in the test data are often unseen inthe training data.
(2) Low flexibility for pattern ab-straction purposes: bigram and trigram extractionschemata are fixed in advance.
(3) Knowledge ac-quisition bottleneck: the size of the training data isnot large enough to cover each possible collocationin the language.To overcome problems 1 and 2 we investigatedsome pattern abstraction techniques from the areaof Information Extraction (IE) and we adapted themto WSD.
To overcome problem 3 we developed La-tent Semantic Kernels, which allow us to integrateexternal knowledge provided by unsupervised termsimilarity estimation.4.1 TIESOur first experiments have been performed exploit-ing TIES, an environment developed at IRST for IEthat induces patterns from the marked entities in thetraining phase, and then applies those patterns in thetest phase in order to assign a category if the pat-tern is satisfied.
For our experiments, we used theBoosted Wrapper Induction (BWI) algorithm (Fre-itag and Kushmerick, 2000) that is implemented inTIES.For Senseval-3 we used very few features (lemmaand POS).
We proposed the system in this configu-ration as a ?baseline?
system for pattern abstraction.Task Prec Recall AttemptedEnglish LS 0.706 0.505 71.50English LS (coarse) 0.767 0.548 71.50Italian LS 0.552 0.309 55.92Table 4: Performance of the TIES systemOur preliminary experiments with BWI haveshown that pattern abstraction is very attractive forWSD, allowing us to achieve a very high precisionfor a restricted number of words, in which the syn-tagmatic information is sufficient for disambigua-tion.
However, we still had some restrictions.
Inparticular, the integration with different knowledgesources for classification is not trivial.obtain such features parsing of the data is required.
However,we decided to do not use such information, while we plan tointroduce it in the next future.4.2 Kernel-WSDOur choice of exploiting kernel methods for WSDhas been motivated by the observation that pattern-based approaches for disambiguation are comple-mentary to the domain based ones: they require dif-ferent knowledge sources and different techniquesfor classification and feature description.
Both ap-proaches have to be simultaneously taken into ac-count in order to perform accurate disambiguation.Our aim was to combine them into a commonframework.Kernel methods, e.g.
Support Vector Machines(SVMs), are state-of-the-art learning algorithms,and they are successfully adopted in many NLPtasks.The idea of SVM (Cristianini and Shawe-Taylor,2000) is to map the set of training data into a higher-dimensional feature space F via a mapping func-tion ?
: ?
?
F , and construct a separating hy-perplane with maximum margin (distance betweenplanes and closest points) in the new space.
Gen-erally, this yields a nonlinear decision boundary inthe input space.
Since the feature space is high di-mensional, performing the transformation has of-ten a high computational cost.
Rather than use theexplicit mapping ?, we can use a kernel functionK : ???
?
< , that corresponds to the inner prod-uct in a feature space which is, in general, differentfrom the input space.Therefore, a kernel function provides a wayto compute (efficiently) the separating hyperplanewithout explicitly carrying out the map ?
into thefeature space - this is called the kernel trick.
In thisway the kernel acts as an interface between the dataand the learning algorithm by defining an implicitmapping into the feature space.
Intuitively, we cansee the kernel as a function that measures the sim-ilarity between pairs of objects.
The learning algo-rithm, which compares all pairs of data items, ex-ploits the information encoded in the kernel.
Animportant characteristic of kernels is that they arenot limited to vector objects but are applicable tovirtually any kind of object representation.In this work we use kernel methods to combineheterogeneous sources of information that we foundrelevant for WSD.
For each of these aspects it ispossible to define kernels independently.
Then theyare combined by exploiting the property that thesum of two kernels is still a kernel (i.e.
k(x, y) =k1(x, y) + k2(x, y)), taking advantage of each sin-gle contribution in an intuitive way3.3In order to keep the kernel values comparable for dif-ferent values and to be independent from the length of theexamples, we considered the normalized version K?
(x, y) =lsa Task Prec Recall Attempted MF-Baseline?
English LS 0.726 0.726 100 0.552?
English LS (coarse) 0.795 0.795 100 0.645- English LS (no-lsa) 0.704 0.704 100 0.552- Basque LS 0.655 0.655 100 0.558- Italian LS 0.531 0.531 100 0.183- Catalan LS 0.858 0.846 98.62 0.663- Spanish LS 0.842 0.842 100 0.677Table 5: Performance of the Kernels-WSD systemThe Word Sense Disambiguation Kernel is de-fined in this way:KWSD(x, y) = KS(x, y) + KP (x, y) (1)where KS is the Syntagmatic Kernel and KP isthe Paradigmatic Kernel.The Syntagmatic Kernel.
The syntagmatic ker-nel generalizes the word-sequence kernels definedby (Cancedda et al, 2003) to sequences of lem-mata and POSs.
Word sequence kernels are basedon the following idea: two sequences are similarif they have in common many sequences of wordsin a given order.
The similarity between two ex-amples is assessed by the number (possibly non-contiguous) of the word sequences matching.
Non-contiguous occurrences are penalized according tothe number of gaps they contain.
For example thesequence of words ?I go very quickly to school?
isless similar to ?I go to school?
than ?I go quickly toschool?.
Different than the bag-of-word approach,word sequence kernels capture the word order andallow gaps between words.
The word sequence ker-nels are parametric with respect to the length of the(sparse) sequences they want to capture.We have defined the syntagmatic kernel as thesum of n distinct word-sequence kernels for lem-mata (i.e.
Collocation Kernel - KC ) and sequencesof POSs (i.e.
POS Kernel - KPOS), according to theformula (for our experiments we set n to 2):KS(x, y) =nXi=1KCi(x, y) +nXi=1KPOSi(x, y) (2)In the above definition of syntagmatic kernel,only exact lemma/POS matches contribute to thesimilarity.
One shortcoming of this approach isthat (near-)synonyms will never be considered sim-ilar.
We address this problem by considering soft-matching of words employing a term similarityK(x, y)/sqrt(K(x, x)K(y, y))measure based on LSA4.
In particular we consid-ered equivalent two words having the same POS anda similarity value higher than an empirical thresh-old.
For example, if we consider as equivalentthe terms Ronaldo and football player the sequenceThe football player scored the first goal can be con-sidered equivalent to the sentence Ronaldo scoredthe first goal.
The properties of the kernel methodsoffer a flexible way to plug additional information,in this case unsupervised (we could also take this in-formation from a semantic network such as WORD-NET).The Paradigmatic Kernel.
The paradigmatickernel takes into account the paradigmatic aspect ofsense distinction (i.e.
domain aspects) (Gliozzo etal., 2004).
For example the word virus can be dis-ambiguated by recognizing the domain of the con-text in which it is placed (e.g.
computer sciencevs.
biology).
Usually such an aspect is capturedby ?bag-of-words?, in analogy to the Vector SpaceModel, widely used in Text Categorization and In-formation Retrieval.
The main limitation of thismodel for WSD is the knowledge acquisition bot-tleneck (i.e.
the lack of sense tagged data).
Bag ofwords are very sparse data that require a large scalecorpus to be learned.
To overcome such a limita-tion, Latent Semantic Indexing (LSI) can provide asolution.Thus we defined a paradigmatic kernel composedby the sum of a ?traditional?
bag of words kerneland an LSI kernel (Cristianini et al, 2002) as de-fined by formula 3:KP (x, y) = KBoW (x, y) + KLSI(x, y) (3)where KBoW computes the inner product be-tween the vector space model representations andKLSI computes the cosine between the LSI vectorsrepresenting the texts.4For languages other than English, we did not exploit thissoft-matching and the KLSI kernel described below.
See thefirst column in the table 5.Table 5 displays the performance of Kernel-WSD.
As a comparison, we also report the figureson the English task without using LSA.
The last col-umn reports the recall of the most-frequent baseline.AcknowledgmentsClaudio Giuliano is supported by the IST-Dot.Komproject sponsored by the European Commission(Framework V grant IST-2001-34038).
TIES andthe kernel package have been developed in the con-text of the Dot.Kom project.ReferencesM.
Berry.
1992.
Large-scale sparse singular valuecomputations.
International Journal of Super-computer Applications, 6(1):13?49.N.
Cancedda, E. Gaussier, C. Goutte, and J.M.
Ren-ders.
2003.
Word-sequence kernels.
Journal ofMachine Learning Research, 3(6):1059?1082.N.
Cristianini and J. Shawe-Taylor.
2000.
SupportVector Machines.
Cambridge University Press.N.
Cristianini, J. Shawe-Taylor, and H. Lodhi.2002.
Latent semantic kernels.
Journal of Intel-ligent Information Systems, 18(2):127?152.S.
Deerwester, S. T. Dumais, G. W. Furnas, T.K.Landauer, and R. Harshman.
1990.
Indexing bylatent semantic analysis.
Journal of the AmericanSociety for Information Science, 41(6):391?407.D.
Freitag and N. Kushmerick.
2000.
Boostedwrapper induction.
In Proc.
of AAAI-00, pages577?583, Austin, Texas.A.
Gliozzo, C. Strapparava, and I. Dagan.
2004.Unsupervised and supervised exploitation of se-mantic domains in lexical disambiguation.
Com-puter Speech and Language, Forthcoming.B.
Magnini and G. Cavaglia`.
2000.
Integrating sub-ject field codes into WordNet.
In Proceedings ofLREC-2000, Athens, Greece, June.B.
Magnini, C. Strapparava, G. Pezzulo, andA.
Gliozzo.
2002.
The role of domain informa-tion in word sense disambiguation.
Natural Lan-guage Engineering, 8(4):359?373.R.
F. Mihalcea.
2002.
Word sense disambiguationwith pattern learning and automatic feature selec-tion.
Natural Language Engineering, 8(4):343?358.D.
Yarowsky.
1993.
One sense per collocation.
InProceedings of ARPA Human Language Technol-ogy Workshop, pages 266?271, Princeton.
