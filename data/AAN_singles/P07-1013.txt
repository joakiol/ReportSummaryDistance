Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 96?103,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsPhonological Constraints and Morphological Preprocessing forGrapheme-to-Phoneme ConversionVera DembergSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, GBv.demberg@sms.ed.ac.ukHelmut SchmidIMSUniversity of StuttgartD-70174 Stuttgartschmid@ims.uni-stuttgart.deGregor Mo?hlerSpeech TechnologiesIBM Deutschland EntwicklungD-71072 Bo?blingenmoehler@de.ibm.comAbstractGrapheme-to-phoneme conversion (g2p) is acore component of any text-to-speech sys-tem.
We show that adding simple syllab-ification and stress assignment constraints,namely ?one nucleus per syllable?
and ?onemain stress per word?, to a joint n-grammodel for g2p conversion leads to a dramaticimprovement in conversion accuracy.Secondly, we assessed morphological pre-processing for g2p conversion.
While mor-phological information has been incorpo-rated in some past systems, its contributionhas never been quantitatively assessed forGerman.
We compare the relevance of mor-phological preprocessing with respect to themorphological segmentation method, train-ing set size, the g2p conversion algorithm,and two languages, English and German.1 IntroductionGrapheme-to-Phoneme conversion (g2p) is the taskof converting a word from its spelling (e.g.
?Stern-aniso?l?, Engl: star-anise oil) to its pronunciation(/"StERnPani:sP?:l/).
Speech synthesis modules witha g2p component are used in text-to-speech (TTS)systems and can be be applied in spoken dialoguesystems or speech-to-speech translation systems.1.1 Syllabification and Stress in g2p conversionIn order to correctly synthesize a word, it is not onlynecessary to convert the letters into phonemes, butalso to syllabify the word and to assign word stress.The problems of word phonemization, syllabifica-tion and word stress assignment are inter-dependent.Information about the position of a syllable bound-ary helps grapheme-to-phoneme conversion.
(Marc-hand and Damper, 2005) report a word error rate(WER) reduction of approx.
5 percentage points forEnglish when the letter string is augmented with syl-labification information.
The same holds vice-versa:we found that WER was reduced by 50% when run-ning our syllabifier on phonemes instead of letters(see Table 4).
Finally, word stress is usually definedon syllables; in languages where word stress is as-sumed1 to partly depend on syllable weight (such asGerman or Dutch), it is important to know where ex-actly the syllable boundaries are in order to correctlycalculate syllable weight.
For German, (Mu?ller,2001) show that information about stress assignmentand the position of a syllable within a word improveg2p conversion.1.2 Morphological PreprocessingIt has been argued that using morphological in-formation is important for languages where mor-phology has an important influence on pronuncia-tion, syllabification and word stress such as Ger-man, Dutch, Swedish or, to a smaller extent, alsoEnglish (Sproat, 1996; Mo?bius, 2001; Pounder andKommenda, 1986; Black et al, 1998; Taylor, 2005).Unfortunately, these papers do not quantify the con-tribution of morphological preprocessing in the task.Important questions when considering the inte-gration of a morphological component into a speech1This issue is controversial among linguists; for an overviewsee (Jessen, 1998).96synthesis system are 1) How large are the im-provements to be gained from morphological pre-processing?
2) Must the morphological system beperfect or can performance improvements also bereached with relatively simple morphological com-ponents?
and 3) How much does the benefit tobe expected from explicit morphological informa-tion depend on the g2p algorithm?
To determinethese factors, we compared morphological segmen-tations based on manual morphological annotationfrom CELEX to two rule-based systems and severalunsupervised data-based approaches.
We also anal-ysed the role of explicit morphological preprocess-ing on data sets of different sizes and compared itsrelevance with respect to a decision tree and a jointn-gram model for g2p conversion.The paper is structured as follows: We introducethe g2p conversion model we used in section 2 andexplain how we implemented the phonological con-straints in section 3.
Section 4 is concerned withthe relation between morphology, word pronuncia-tion, syllabification and word stress in German, andpresents different sources for morphological seg-mentation.
In section 5, we evaluate the contributionof each of the components and compare our meth-ods to state-of-the-art systems.
Section 6 summa-rizes our results.2 MethodsWe used a joint n-gram model for the grapheme-to-phoneme conversion task.
Models of this typehave previously been shown to yield very good g2pconversion results (Bisani and Ney, 2002; Galescuand Allen, 2001; Chen, 2003).
Models that do notuse joint letter-phoneme states, and therefore are notconditional on the preceding letters, but only on theactual letter and the preceding phonemes, achievedinferior results.
Examples of such approaches usingHidden Markov Models are (Rentzepopoulos andKokkinakis, 1991) (who applied the HMM to therelated task of phoneme-to-grapheme conversion),(Taylor, 2005) and (Minker, 1996).The g2p task is formulated as searching for themost probable sequence of phonemes given the or-thographic form of a word.
One can think of it as atagging problem where each letter is tagged with a(possibly empty) phoneme-sequence p. In our par-ticular implementation, the model is defined as ahigher-order Hidden Markov Model, where the hid-den states are a letter?phoneme-sequence pair ?l; p?,and the observed symbols are the letters l. The out-put probability of a hidden state is then equal to one,since all hidden states that do not contain the ob-served letter are pruned directly.The model for grapheme-to-phoneme conver-sion uses the Viterbi algorithm to efficiently com-pute the most probable sequence p?n1 of phonemesp?1, p?2, ..., p?n for a given letter sequence ln1 .
Theprobability of a letter?phon-seq pair depends on thek preceding letter?phon-seq pairs.
Dummy states ?#?are appended at both ends of each word to indicatethe word boundary and to ensure that all conditionalprobabilities are well-defined.p?n1 = argmaxpn1n+1?i=1P (?l; p?i | ?l; p?i?1i?k)In an integrated model where g2p conversion, syl-labification and word stress assignment are all per-formed at the same time, a state additionally con-tains a syllable boundary flag b and a stress flag a,yielding ?l; p; b; a?i.As an alternative architecture, we also designed amodular system that comprises one component forsyllabification and one for word stress assignment.The model for syllabification computes the mostprobable sequence b?n1 of syllable boundary-tags b?1,b?2, ..., b?n for a given letter sequence ln1 .b?n1 = argmaxbn1n+1?i=1P (?l; b?i | ?l; b?i?1i?k)The stress assignment model works on syllables.It computes the most probable sequence a?n1 of wordaccent-tags a?1, a?2, ..., a?n for a given syllable se-quence syln1 .a?n1 = argmaxan1n+1?i=1P (?syl; a?i | ?syl; a?i?1i?k)2.1 SmoothingBecause of major data sparseness problems, smooth-ing is an important issue, in particular for the stressmodel which is based on syllable?stress-tag pairs.Performance varied by up to 20% in function of thesmoothing algorithm chosen.
Best results were ob-tained when using a variant of Modified Kneser-NeySmoothing2 (Chen and Goodman, 1996).2For a formal definition, see(Demberg, 2006).972.2 PruningIn the g2p-model, each letter can on average maponto one of 12 alternative phoneme-sequences.When working with 5-grams3, there are about 125 =250,000 state sequences.
To improve time and spaceefficiency, we implemented a simple pruning strat-egy that only considers the t best states at any mo-ment in time.
With a threshold of t = 15, about 120words are processed per minute on a 1.5GHz ma-chine.
Conversion quality is only marginally worsethan when the whole search space is calculated.Running time for English is faster, because the av-erage number of candidate phonemes for each let-ter is lower.
We measured running time (includingtraining and the actual g2p conversion in 10-foldcross validation) for a Perl implementation of ouralgorithm on the English NetTalk corpus (20,008words) on an Intel Pentium 4, 3.0 GHz machine.Running time was less than 1h for each of the fol-lowing three test conditions: c1) g2p conversiononly, c2) syllabification first, then g2p conversion,c3) simultaneous g2p conversion and syllabification,given perfect syllable boundary input, c4) simulta-neous g2p conversion and syllabification when cor-rect syllabification is not available beforehand.
Thisis much faster than the times for Pronunciation byAnalogy (PbA) (Marchand and Damper, 2005) onthe same corpus.
Marchand and Damper reported aprocessing time of several hours for c4), two daysfor c2) and several days for c3).2.3 AlignmentOur current implementation of the joint n-grammodel is not integrated with an automatic alignmentprocedure.
We therefore first aligned letters andphonemes in a separate, semi-automatic step.
Eachletter was aligned with zero to two phonemes and,in the integrated model, zero or one syllable bound-aries and stress markers.3 Integration of Phonological ConstraintsWhen analysing the results from the model that doesg2p conversion, syllabification and stress assign-3There is a trade-off between long context windows whichcapture the context accurately and data sparseness issues.
Theoptimal value k for the context window size depends on thesource language (existence of multiletter graphemes, complex-ity of syllables etc.
).ment in a single step, we found that a large propor-tion of the errors was due to the violation of basicphonological constraints.Some syllables had no syllable nucleus, whileothers contained several vowels.
The reason for theerrors is that German syllables can be very long andtherefore sparse, often causing the model to back-off to smaller contexts.
If the context is too small tocover the syllable, the model cannot decide whetherthe current syllable contains a nucleus.In stress assignment, this problem is even worse:the context window rarely covers the whole word.The algorithm does not know whether it already as-signed a word stress outside the context window.This leads to a high error rate with 15-20% of in-correctly stressed words.
Thereof, 37% have morethan one main stress, about 27% are not assigned anystress and 36% are stressed in the wrong position.This means that we can hope to reduce the errors byalmost 2/3 by using phonological constraints.Word stress assignment is a difficult problem inGerman because the underlying processes involvesome deeper morphological knowledge which is notavailable to the simple model.
In complex words,stress mainly depends on morphological structure(i.e.
on the compositionality of compounds andon the stressing status of affixes).
Word stress insimplex words is assumed to depend on the sylla-ble position within the word stem and on syllableweight.
The current language-independent approachdoes not model these processes, but only capturessome of its statistics.Simple constraints can help to overcome the prob-lem of lacking context by explicitly requiring thatevery syllable must have exactly one syllable nu-cleus and that every word must have exactly one syl-lable receiving primary stress.3.1 ImplementationOur goal is to find the most probable syllabifiedand stressed phonemization of a word that does notviolate the constraints.
We tried two different ap-proaches to enforce the constraints.In the first variant (v1), we modified the proba-bility model to enforce the constraints.
Each statenow corresponds to a sequence of 4-tuples consist-ing of a letter l, a phoneme sequence p, a syllableboundary tag b, an accent tag a (as before) plus two98new flags A and N which indicate whether an ac-cent/nucleus precedes or not.
The A and N flags ofthe new state are a function of its accent and syllableboundary tag and the A and N flag of the precedingstate.
They split each state into four new states.
Thenew transition probabilities are defined as:P (?l; p; b; a?i | ?l; p; b; a?i?1i?k , A,N)The probability is 0 if the transition violates a con-straint, e.g., when the A flag is set and ai indicatesanother accent.A positive side effect of the syllable flag is that itstores separate phonemization probabilities for con-sonants in the syllable onset vs. consonants in thecoda.
The flag in the onset is 0 since the nucleus hasnot yet been encountered, whereas it is set to 1 in thecoda.
In German, this can e.g.
help in for syllable-final devoicing of voiced stops and fricatives.The increase in the number of states aggravatessparse-data problems.
Therefore, we implementedanother variant (v2) which uses the same set of states(with A and N flags), but with the transition proba-bilities of the original model, which did not enforcethe constraints.
Instead, we modified the Viterbi al-gorithm to eliminate the invalid transitions: For ex-ample, a transition from a state with the A flag setto a state where ai introduces a second stress, is al-ways ignored.
On small data sets, better results wereachieved with v2 (see Table 5).4 Morphological PreprocessingIn German, information about morphologicalboundaries is needed to correctly insert glottal stops[P] in complex words, to determine irregular pro-nunciation of affixes (v is pronounced [v] in ver-tikal but [f] in ver+ticker+n, and the suffix syllableheit is not stressed although superheavy and wordfinal) and to disambiguate letters (e.g.
e is alwayspronounced /@/ when occurring in inflectional suf-fixes).
Vowel length and quality has been arguedto also depend on morphological structure (Pounderand Kommenda, 1986).
Furthermore, morphologi-cal boundaries overrun default syllabification rules,such as the maximum onset principle.Applying default syllabification to the word?Sternaniso?l?
would result in a syllabification intoSter-na-ni-so?l (and subsequent phonemiza-tion to something like /StE?"na:niz?
:l/) instead ofStern-a-nis-o?l (/"StE?nPani:sP?:l/).
Syllabifi-cation in turn affects phonemization since voicedfricatives and stops are devoiced in syllable-final po-sition.
Morphological information also helps forgraphemic parsing of words such as ?Ro?schen?
(Engl: little rose) where the morphological bound-ary between Ro?s and chen causes the string sch tobe transcribed to /s?/ instead of /S/.
Similar ambigui-ties can arise for all other sounds that are representedby several letters in orthography (e.g.
doubled con-sonants, diphtongs, ie, ph, th), and is also valid forEnglish.
Finally, morphological information is alsocrucial to determine word stress in morphologicallycomplex words.4.1 Methods for Morphological SegmentationGood segmentation performance on arbitrary wordsis hard to achieve.
We compared several approacheswith different amounts of built-in knowledge.
Themorphological information is encoded in the let-ter string, where different digits represent differentkinds of morphological boundaries (prefixes, stems,derivational and inflectional suffixes).Manual Annotation from CELEXTo determine the upper bound of what can beachieved when exploiting perfect morphological in-formation, we extracted morphological boundariesand boundary types from the CELEX database.The manual annotation is not perfect as it con-tains some errors and many cases where words arenot decomposed entirely.
The words tagged [F] for?lexicalized inflection?, e.g.
gedra?ngt (past partici-ple of dra?ngen, Engl: push) were decomposed semi-automatically for the purpose of this evaluation.
Asexpected, annotating words with CELEX morpho-logical segmentation yielded the best g2p conver-sion results.
Manual annotation is only available fora small number of words.
Therefore, only automati-cally annotated morphological information can scaleup to real applications.Rule-based SystemsThe traditional approach is to use large morphemelexica and a set of rules that segment words into af-fixes and stems.
Drawbacks of using such a systemare the high development costs, limited coverage99and problems with ambiguity resolution between al-ternative analyses of a word.The two rule-based systems we evaluated, theETI4 morphological system and SMOR5 (Schmid etal., 2004), are both high-quality systems with largelexica that have been developed over several years.Their performance results can help to estimate whatcan realistically be expected from an automatic seg-mentation system.
Both of the rule-based systemsachieved an F-score of approx.
80% morphologicalboundaries correct with respect to CELEX manualannotation.Unsupervised Morphological SystemsMost attractive among automatic systems aremethods that use unsupervised learning, becausethese require neither an expert linguist to build largerule-sets and lexica nor large manually annotatedword lists, but only large amounts of tokenizedtext, which can be acquired e.g.
from the internet.Unsupervised methods are in principle6 language-independent, and can therefore easily be applied toother languages.We compared four different state-of-the-art unsu-pervised systems for morphological decomposition(cf.
(Demberg, 2006; Demberg, 2007)).
The algo-rithms were trained on a German newspaper cor-pus (taz), containing about 240 million words.
Thesame algorithms have previously been shown to helpa speech recognition task (Kurimo et al, 2006).5 Experimental Evaluations5.1 Training Set and Test Set DesignThe German corpus used in these experiments isCELEX (German Linguistic User Guide, 1995).CELEX contains a phonemic representation of each4Eloquent Technology, Inc. (ETI) TTS system.http://www.mindspring.com/?ssshp/ssshp_cd/ss_eloq.htm5The lexicon used by SMOR, IMSLEX, contains morpho-logically complex entries, which leads to high precision and lowrecall.
The results reported here refer to a version of SMOR,where the lexicon entries were decomposed using a rather na?
?vehigh-recall segmentation method.
SMOR itself does not disam-biguate morphological analyses of a word.
Our version usedtransition weights learnt from CELEX morphological annota-tion.
For more details refer to (Demberg, 2006).6Most systems make some assumptions about the underly-ing morphological system, for instance that morphology is aconcatenative process, that stems have a certain minimal lengthor that prefixing and suffixing are the most relevant phenomena.word, syllable boundaries and word stress infor-mation.
Furthermore, it contains manually verifiedmorphological boundaries.Our training set contains approx.
240,000 wordsand the test set consists of 12,326 words.
The testset is designed such that word stems in training andtest sets are disjoint, i.e.
the inflections of a certainstem are either all in the training set or all in the testset.
Stem overlap between training and test set onlyoccurs in compounds and derivations.
If a simplerandom splitting (90% for training set, 10% for testset) is used on inflected corpora, results are muchbetter: Word error rates (WER) are about 60% lowerwhen the set of stems in training and test set are notdisjoint.
The same effect can also be observed forthe syllabification task (see Table 4).5.2 Results for the Joint n-gram ModelThe joint n-gram model is language-independent.An aligned corpus with words and their pronuncia-tions is needed, but no further adaptation is required.Table 1 shows the performance of our model incomparison to alternative approaches on the Germanand English versions of the CELEX corpus, the En-glish NetTalk corpus, the English Teacher?s WordBook (TWB) corpus, the English beep corpus andthe French Brulex corpus.
The joint n-gram modelperforms significantly better than the decision tree(essentially based on (Lucassen and Mercer, 1984)),and achieves scores comparable to the Pronuncia-tion by Analogy (PbA) algorithm (Marchand andDamper, 2005).
For the Nettalk data, we also com-pared the influence of syllable boundary annotationfrom a) automatically learnt and b) manually anno-tated syllabification information on phoneme accu-racy.
Automatic syllabification for our model in-tegrated phonological constraints (as described insection 3.1), and therefore led to an improvementin phoneme accuracy, while the word error rate in-creased for the PbA approach, which does not incor-porate such constraints.
(Chen, 2003) also used a joint n-gram model.The two approaches differ in that Chen uses smallchunks (?
(l : |0..1|) : (p : |0..1|)?
pairs only) and it-eratively optimizes letter-phoneme alignment duringtraining.
Chen smoothes higher-order Markov Mod-els with Gaussian Priors and implements additionallanguage modelling such as consonant doubling.100corpus size jnt n-gr PbA Chen dec.treeG - CELEX 230k 7.5% 15.0%E - Nettalk 20k 35.4% 34.65% 34.6%a) auto.syll 35.3% 35.2%b) man.syll 29.4% 28.3%E - TWB 18k 28.5% 28.2%E - beep 200k 14.3% 13.3%E - CELEX 100k 23.7% 31.7%F - Brulex 27k 10.9%Table 1: Word error rates for different g2p conver-sion algorithms.
Constraints were only used in theE-Nettalk auto.
syll condition.5.3 Benefit of Integrating ConstraintsThe accuracy improvements achieved by integrat-ing the constraints (see Table 2) are highly statis-tically significant.
The numbers for conditions ?G-syllab.+stress+g2p?
and ?E-syllab.+g2p?
in Table 2differ from the numbers for ?G-CELEX?
and ?E-Nettalk?
in Table 1 because phoneme conversionerrors, syllabification errors and stress assignmenterrors are all counted towards word error rates re-ported in Table 2.Word error rate in the combined g2p-syllable-stress model was reduced from 21.5% to 13.7%.
Forthe separate tasks, we observed similar effects: Theword error rate for inserting syllable boundaries wasreduced from 3.48% to 3.1% on letters and from1.84% to 1.53% on phonemes.
Most significantly,word error rate was decreased from 30.9% to 9.9%for word stress assignment on graphemes.We also found similarly important improvementswhen applying the syllabification constraint to En-glish grapheme-to-phoneme conversion and syllabi-fication.
This suggests that our findings are not spe-cific to German but that this kind of general con-straints can be beneficial for a range of languages.no constr.
constraint(s)G - syllab.+stress+g2p 21.5% 13.7%G - syllab.
on letters 3.5% 3.1%G - syllab.
on phonemes 1.84% 1.53%G - stress assignm.
on letters 30.9% 9.9%E - syllab.+g2p 40.5% 37.5%E - syllab.
on phonemes 12.7% 8.8%Table 2: Improving performance on g2p conver-sion, syllabification and stress assignment throughthe introduction of constraints.
The table showsword error rates for German CELEX (G) and En-glish NetTalk (E).5.4 ModularityModularity is an advantage if the individual compo-nents are more specialized to their task (e.g.
by ap-plying a particular level of description of the prob-lem, or by incorporating some additional source ofknowledge).In a modular system, one componentcan easily be substituted by another ?
for example,if a better way of doing stress assignment in Germanwas found.
On the other hand, keeping everything inone module for strongly inter-dependent tasks (suchas determining word stress and phonemization) al-lows us to simultaneously optimize for the best com-bination of phonemes and stress.Best results were obtained from the joint n-grammodel that does syllabification, stress assignmentand g2p conversion all in a single step and inte-grates phonological constraints for syllabificationand word stress (WER = 14.4% using method v1,WER = 13.7% using method v2).
If the modular ar-chitecture is chosen, best results are obtained wheng2p conversion is done before syllabification andstress assignment (15.2% WER), whereas doing syl-labification and stress assignment first and then g2pconversion leads to a WER of 16.6%.
We can con-clude from this finding that an integrated approach issuperior to a pipeline architecture for strongly inter-dependent tasks such as these.5.5 The Contribution of MorphologicalPreprocessingA statistically significant (according to a two-tailedt-test) improvement in g2p conversion accuracy(from 13.7% WER to 13.2% WER) was obtainedwith the manually annotated morphological bound-aries from CELEX.
The segmentation from both ofthe rule-based systems (ETI and SMOR) also re-sulted in an accuracy increase with respect to thebaseline (13.6% WER), which is not annotated withmorphological boundaries.Among the unsupervised systems, best results7 onthe g2p task with morphological annotation were ob-tained with the RePortS system (Keshava and Pitler,2006).
But none of the segmentations led to an er-ror reduction when compared to a baseline that usedno morphological information (see Table 3).
Worderror rate even increased when the quality of the7For all results refer to (Demberg, 2006).101Precis.
Recall F-Meas.
WERRePortS (unsuperv.)
71.1% 50.7% 59.2% 15.1%no morphology 13.7%SMOR (rule-based) 87.1% 80.4% 83.6%ETI (rule-based) 75.4% 84.1% 79.5% 13.6%CELEX (manual) 100% 100% 100% 13.2%Table 3: Systems evaluation on German CELEXmanual annotation and on the g2p task using a jointn-gram model.
WERs refer to implementation v2.morphological segmentation was too low (the unsu-pervised algorithms achieved 52%-62% F-measurewith respect to CELEX manual annotation).Table 4 shows that high-quality morphologicalinformation can also significantly improve perfor-mance on a syllabification task for German.
We usedthe syllabifier described in (Schmid et al, 2005),which works similar to the joint n-gram model usedfor g2p conversion.
Just as for g2p conversion, wefound a significant accuracy improvement when us-ing the manually annotated data, a smaller improve-ment for using data from the rule-based morpholog-ical system, and no improvement when using seg-mentations from an unsupervised algorithm.
Syllab-ification works best when performed on phonemes,because syllables are phonological units and there-fore can be determined most easily in terms ofphonological entities such as phonemes.Whether morphological segmentation is worth theeffort depends on many factors such as training setsize, the g2p algorithm and the language considered.disj.
stems randomRePortS (unsupervised morph.)
4.95%no morphology 3.10% 0.72%ETI (rule-based morph.)
2.63%CELEX (manual annot.)
1.91% 0.53%on phonemes 1.53% 0.18%Table 4: Word error rates (WER) for syllabificationwith a joint n-gram model for two different trainingand test set designs (see Section 5.1).Morphology for Data Sparseness ReductionProbably the most important aspect of morpho-logical segmentation information is that it can helpto resolve data sparseness issues.
Because of the ad-ditional knowledge given to the system through themorphological information, similarly-behaving let-ter sequences can be grouped more effectively.Therefore, we hypothesized that morphologicalinformation is most beneficial in situations wherethe training corpus is rather small.
Our findings con-firm this expectation, as the relative error reductionthrough morphological annotation for a training cor-pus of 9,600 words is 6.67%, while it is only 3.65%for a 240,000-word training corpus.In our implementation, the stress flags and sylla-ble flags we use to enforce the phonological con-straints increase data sparseness.
We found v2 (theimplementation that uses the states without stressand syllable flags and enforces the constraints byeliminating invalid transitions, cf.
section 3.1) tooutperform the integrated version, v1, and more sig-nificantly in the case of more severe data sparseness.The only condition when we found v1 to performbetter than v2 was with a large data set and addi-tional data sparseness reduction through morpholog-ical annotation, as in section 4 (see Table 5).WER: designs v1 v2data set size 240k 9.6k 240k 9.6kno morph.
14.4% 32.3% 13.7% 25.5%CELEX 12.5% 29% 13.2% 23.8%Table 5: The interactions of constraints in trainingand different levels of data sparseness.g2p Conversion AlgorithmsThe benefit of using morphological preprocessingis also affected by the algorithm that is used for g2pconversion.
Therefore, we also evaluated the relativeimprovement of morphological annotation when us-ing a decision tree for g2p conversion.Decision trees were one of the first data-based ap-proaches to g2p and are still widely used (Kienappeland Kneser, 2001; Black et al, 1998).
The tree?sefficiency and ability for generalization largely de-pends on pruning and the choice of possible ques-tions.
In our implementation, the decision tree canask about letters within a context window of fiveback and five ahead, about five phonemes back andgroups of letters (e.g.
consonants vs. vowels).Both the decision tree and the joint n-gram modelconvert graphemes to phonemes, insert syllableboundaries and assign word stress in a single step(marked as ?WER-ss?
in Table 6.
The imple-mentation of the joint n-gram model incorporatesthe phonological constraints described in section 3(?WER-ss+?).
Our main finding is that the jointn-gram model profits less from morphological an-notation.
Without the constraints, the performance102difference is smaller: the joint n-gram model thenachieves a word error rate of 21.5% on the no-morphology-condition.In very recent work, (Demberg, 2007) developedan unsupervised algorithm (f-meas: 68%; an exten-sion of RePortS) whose segmentations improve g2pwhen using a the decision tree (PER: 3.45%).decision tree joint n-gramPER WER-ss PER WER-ss+RePortS 3.83% 28.3% 15.1%no morph.
3.63% 26.59% 2.52% 13.7%ETI 2.8% 21.13% 2.53% 13.6%CELEX 2.64% 21.64% 2.36% 13.2%Table 6: The effect of morphological preprocessingon phoneme error rates (PER) and word error rates(WER) in grapheme-to-phoneme conversion.Morphology for other LanguagesWe also investigated the effect of morphologicalinformation on g2p conversion and syllabificationin English, using manually annotated morphologicalboundaries from CELEX and the automatic unsuper-vised RePortS system which achieves an F-score ofabout 77% for English.
The cases where morpho-logical information affects word pronunciation arerelatively few in comparison to German, thereforethe overall effect is rather weak and we did not evenfind improvements with perfect boundaries.6 ConclusionsOur results confirm that the integration of phonolog-ical constraints ?one nucleus per syllable?
and ?onemain stress per word?
can significantly boost ac-curacy for g2p conversion in German and English.We implemented the constraints using a joint n-gram model for g2p conversion, which is language-independent and well-suited to the g2p task.We systematically evaluated the benefit to begained from morphological preprocessing on g2pconversion and syllabification.
We found that mor-phological segmentations from rule-based systemsled to some improvement.
But the magnitude ofthe accuracy improvement strongly depends on theg2p algorithm and on training set size.
State-of-the-art unsupervised morphological systems do notyet yield sufficiently good segmentations to help thetask, if a good conversion algorithm is used: Lowquality segmentation even led to higher error rates.AcknowledgmentsWe would like to thank Hinrich Schu?tze, Frank Keller and theACL reviewers for valuable comments and discussion.The first author was supported by Evangelisches Studienwerke.V.
Villigst.ReferencesM.
Bisani and H. Ney.
2002.
Investigations on joint multigrammodels for grapheme-to-phoneme conversion.
In ICSLP.A.
Black, K. Lenzo, and V. Pagel.
1998.
Issues in building gen-eral letter to sound rules.
In 3.
ESCA on Speech Synthesis.SF Chen and J Goodman.
1996.
An empirical study of smooth-ing techniques for language modeling.
In Proc.
of ACL.S.
F. Chen.
2003.
Conditional and joint models for grapheme-to-phoneme conversion.
In Eurospeech.V.
Demberg.
2006.
Letter-to-phoneme conversion for a Ger-man TTS-System.
Master?s thesis.
IMS, Univ.
of Stuttgart.V.
Demberg.
2007.
A language-independent unsupervisedmodel for morphological segmentation.
In Proc.
of ACL-07.L.
Galescu and J. Allen.
2001.
Bi-directional conversion be-tween graphemes and phonemes using a joint n-gram model.In Proc.
of the 4th ISCA Workshop on Speech Synthesis.CELEX German Linguistic User Guide, 1995.
Center for Lex-ical Information.
Max-Planck-Institut for Psycholinguistics,Nijmegen.M.
Jessen, 1998.
Word Prosodic Systems in the Languages ofEurope.
Mouton de Gruyter: Berlin.S.
Keshava and E. Pitler.
2006.
A simpler, intuitive approachto morpheme induction.
In Proceedings of 2nd Pascal Chal-lenges Workshop, pages 31?35, Venice, Italy.A.
K. Kienappel and R. Kneser.
2001.
Designing very com-pact decision trees for grapheme-to-phoneme transcription.In Eurospeech, Scandinavia.M.
Kurimo, M. Creutz, M. Varjokallio, E. Arisoy, and M. Sar-aclar.
2006.
Unsupervsied segmentation of words into mor-phemes ?
Challenge 2005: An introduction and evaluationreport.
In Proc.
of 2nd Pascal Challenges Workshop, Italy.J.
Lucassen and R. Mercer.
1984.
An information theoreticapproach to the automatic determination of phonemic base-forms.
In ICASSP 9.Y.
Marchand and R. I. Damper.
2005.
Can syllabification im-prove pronunciation by analogy of English?
Natural Lan-guage Engineering.W.
Minker.
1996.
Grapheme-to-phoneme conversion - an ap-proach based on hidden markov models.B.
Mo?bius.
2001.
German and Multilingual Speech Synthesis.phonetic AIMS, Arbeitspapiere des Instituts fu?r MaschinelleSpachverarbeitung.K.
Mu?ller.
2001.
Automatic detection of syllable boundariescombining the advantages of treebank and bracketed corporatraining.
In Proceedings of ACL, pages 402?409.A.
Pounder and M. Kommenda.
1986.
Morphological analysisfor a German text-to-speech system.
In COLING 1986.P.A.
Rentzepopoulos and G.K. Kokkinakis.
1991.
Phoneme tographeme conversion using HMM.
In Eurospeech.H.
Schmid, A. Fitschen, and U. Heid.
2004.
SMOR: A Germancomputational morphology covering derivation, compositionand inflection.
In Proc.
of LREC.H.
Schmid, B. Mo?bius, and J. Weidenkaff.
2005.
Tagging syl-lable boundaries with hidden Markov models.
IMS, unpub.R.
Sproat.
1996.
Multilingual text analysis for text-to-speechsynthesis.
In Proc.
ICSLP ?96, Philadelphia, PA.P.
Taylor.
2005.
Hidden Markov models for grapheme tophoneme conversion.
In INTERSPEECH.103
