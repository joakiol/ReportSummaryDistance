Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 293?303,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsMax-Margin Tensor Neural Network for Chinese Word SegmentationWenzhe Pei Tao Ge Baobao Chang?Key Laboratory of Computational Linguistics, Ministry of EducationSchool of Electronics Engineering and Computer Science, Peking UniversityBeijing, P.R.China, 100871{peiwenzhe,getao,chbb}@pku.edu.cnAbstractRecently, neural network models for nat-ural language processing tasks have beenincreasingly focused on for their abilityto alleviate the burden of manual featureengineering.
In this paper, we propose anovel neural network model for Chineseword segmentation called Max-MarginTensor Neural Network (MMTNN).
Byexploiting tag embeddings and tensor-based transformation, MMTNN has theability to model complicated interactionsbetween tags and context characters.
Fur-thermore, a new tensor factorization ap-proach is proposed to speed up the modeland avoid overfitting.
Experiments on thebenchmark dataset show that our modelachieves better performances than previ-ous neural network models and that ourmodel can achieve a competitive perfor-mance with minimal feature engineering.Despite Chinese word segmentation beinga specific case, MMTNN can be easilygeneralized and applied to other sequencelabeling tasks.1 IntroductionUnlike English and other western languages, Chi-nese do not delimit words by white-space.
There-fore, word segmentation is a preliminary and im-portant pre-process for Chinese language process-ing.
Most previous systems address this problemby treating this task as a sequence labeling prob-lem where each character is assigned a tag indi-cating its position in the word.
These systemsare effective because researchers can incorporate alarge body of handcrafted features into the models.However, the ability of these models is restricted?Corresponding authorby the design of features and the number of fea-tures could be so large that the result models aretoo large for practical use and prone to overfit ontraining corpus.Recently, neural network models have been in-creasingly focused on for their ability to mini-mize the effort in feature engineering.
Collobert etal.
(2011) developed the SENNA system that ap-proaches or surpasses the state-of-the-art systemson a variety of sequence labeling tasks for English.Zheng et al (2013) applied the architecture ofCollobert et al (2011) to Chinese word segmenta-tion and POS tagging and proposed a perceptron-style algorithm to speed up the training processwith negligible loss in performance.Workable as previous neural network modelsseem, a limitation of them to be pointed out isthat the tag-tag interaction, tag-character inter-action and character-character interaction are notwell modeled.
In conventional feature-based lin-ear (log-linear) models, these interactions are ex-plicitly modeled as features.
Take phrase ????
(play basketball)?
as an example, assuming weare labeling character C0=??
?, possible featurescould be:f1={1 C?1=???
and C1=???
and y0=?B?0 elsef2={1 C0=???
and y0=?B?
and y?1=?S?0 elseTo capture more interactions, researchers have de-signed a large number of features based on linguis-tic intuition and statistical information.
In previ-ous neural network models, however, hardly cansuch interactional effects be fully captured rely-ing only on the simple transition score and the sin-gle non-linear transformation (See section 2).
Inorder to address this problem, we propose a newmodel called Max-Margin Tensor Neural Network(MMTNN) that explicitly models the interactions293between tags and context characters by exploitingtag embeddings and tensor-based transformation.Moreover, we propose a tensor factorization ap-proach that effectively improves the model effi-ciency and prevents from overfitting.
We evalu-ate the performance of Chinese word segmentationon the PKU and MSRA benchmark datasets in thesecond International Chinese Word SegmentationBakeoff (Emerson, 2005) which are commonlyused for evaluation of Chinese word segmentation.Experiment results show that our model outper-forms other neural network models.Although we focus on the question that howfar we can go without using feature engineeringin this paper, the study of deep learning for NLPtasks is still a new area in which it is currentlychallenging to surpass the state-of-the-art with-out additional features.
Following Mansur et al(2013), we wonder how well our model can per-form with minimal feature engineering.
There-fore, we integrate additional simple character bi-gram features into our model and the result showsthat our model can achieve a competitive perfor-mance that other systems hardly achieve unlessthey use more complex task-specific features.The main contributions of our work are as fol-lows:?
We propose a Max-Margin Tensor Neu-ral Network for Chinese word segmentationwithout feature engineering.
The test re-sults on the benchmark dataset show that ourmodel outperforms previous neural networkmodels.?
We propose a new tensor factorization ap-proach that models each tensor slice as theproduct of two low-rank matrices.
Not onlydoes this approach improve the efficiency ofour model but also it avoids the risk of over-fitting.?
Compared with previous works that use alarge number of handcrafted features, ourmodel can achieve a competitive perfor-mance with minimal feature engineering.?
Despite Chinese word segmentation being aspecific case, our approach can be easily gen-eralized to other sequence labeling tasks.The remaining part of this paper is organized asfollows.
Section 2 describes the details of con-ventional neural network architecture.
Section 3Figure 1: Conventional Neural Networkdescribes the details of our model.
Experiment re-sults are reported in Section 4.
Section 5 reviewsthe related work.
The conclusions are given inSection 6.2 Conventional Neural Network2.1 Lookup TableThe idea of distributed representation for symbolicdata is one of the most important reasons why theneural network works.
It was proposed by Hin-ton (1986) and has been a research hot spot formore than twenty years (Bengio et al, 2003; Col-lobert et al, 2011; Schwenk et al, 2012; Mikolovet al, 2013a).
Formally, in the Chinese word seg-mentation task, we have a character dictionary Dof size |D|.
Unless otherwise specified, the char-acter dictionary is extracted from the training setand unknown characters are mapped to a specialsymbol that is not used elsewhere.
Each characterc ?
D is represented as a real-valued vector (char-acter embedding) Embed(c) ?
Rdwhere d is thedimensionality of the vector space.
The charac-ter embeddings are then stacked into a embeddingmatrix M ?
Rd?|D|.
For a character c ?
D thathas an associated index k, the corresponding char-acter embedding Embed(c) ?
Rdis retrieved bythe Lookup Table layer as shown in Figure 1:Embed(c) = Mek(1)Here ek?
R|D|is a binary vector which is zero inall positions except at k-th index.
The Lookup Ta-ble layer can be seen as a simple projection layerwhere the character embedding for each context294character is achieved by table lookup operation ac-cording to their indices.
The embedding matrixM is initialized with small random numbers andtrained by back-propagation.
We will analyze inmore detail about the effect of character embed-dings in Section 4.2.2 Tag ScoringThe most common tagging approach is the win-dow approach.
The window approach assumesthat the tag of a character largely depends on itsneighboring characters.
Given an input sentencec[1:n], a window of size w slides over the sentencefrom character c1to cn.
We set w = 5 in allexperiments.
As shown in Figure 1, at positionci, 1 ?
i ?
n, the context characters are fed intothe Lookup Table layer.
The characters exceedingthe sentence boundaries are mapped to one of twospecial symbols, namely ?start?
and ?end?
sym-bols.
The character embeddings extracted by theLookup Table layer are then concatenated into asingle vector a ?
RH1, where H1= w ?
d isthe size of Layer 1.
Then a is fed into the nextlayer which performs linear transformation fol-lowed by an element-wise activation function gsuch as tanh, which is used in our experiments:h = g(W1a+ b1) (2)where W1?
RH2?H1, b1?
RH2?1, h ?
RH2.
H2is a hyper-parameter which is the number of hid-den units in Layer 2.
Given a set of tags T of size|T |, a similar linear transformation is performedexcept that no non-linear function is followed:f(t|c[i?2:i+2]) = W2h+ b2(3)where W2?
R|T |?H2, b2?
R|T |?1.f(t|c[i?2:i+2]) ?
R|T |is the score vector for eachpossible tag.
In Chinese word segmentation, themost prevalent tag set T is BMES tag set, whichuses 4 tags to carry word boundary information.
Ituses B, M, E and S to denote the Beginning, theMiddle, the End of a word and a Single characterforming a word respectively.
We use this tag set inour method.2.3 Model Training and InferenceDespite sharing commonalities mentioned above,previous work models the segmentation task dif-ferently and therefore uses different training andinference procedure.
Mansur et al (2013) mod-eled Chinese word segmentation as a series ofclassification task at each position of the sentencein which the tag score is transformed into proba-bility using softmax function:p(ti|c[i?2:i+2]) =exp(f(ti|c[i?2:i+2]))?t?exp(f(t?|c[i?2:i+2]))The model is then trained in MLE-style whichmaximizes the log-likelihood of the tagged data.Obviously, it is a local model which cannot cap-ture the dependency between tags and does notsupport to infer the tag sequence globally.To model the tag dependency, previous neuralnetwork models (Collobert et al, 2011; Zhenget al, 2013) introduce a transition score Aijforjumping from tag i ?
T to tag j ?
T .
For ainput sentence c[1:n]with a tag sequence t[1:n], asentence-level score is then given by the sum oftransition and network scores:s(c[1:n], t[1:n], ?)
=n?i=1(Ati?1ti+f?
(ti|c[i?2:i+2]))(4)where f?
(ti|c[i?2:i+2]) indicates the score outputfor tag tiat the i-th character by the network withparameters ?
= (M,A,W1, b1,W2, b2).
Giventhe sentence-level score, Zheng et al (2013)proposed a perceptron-style training algorithm in-spired by the work of Collins (2002).
Comparedwith Mansur et al (2013), their model is a globalone where the training and inference is performedat sentence-level.Workable as these methods seem, one of thelimitations of them is that the tag-tag interactionand the neural network are modeled seperately.The simple tag-tag transition neglects the impactof context characters and thus limits the abilityto capture flexible interactions between tags andcontext characters.
Moreover, the simple non-linear transformation in equation (2) is also poorto model the complex interactional effects in Chi-nese word segmentation.3 Max-Margin Tensor Neural Network3.1 Tag EmbeddingTo better model the tag-tag interaction given thecontext characters, distributed representation fortags instead of traditional discrete symbolic repre-sentation is used in our model.
Similar to characterembeddings, given a fixed-sized tag set T , the tagembeddings for tags are stored in a tag embeddingmatrix L ?
Rd?|T |, where d is the dimensionality295Figure 2: Max-Margin Tensor Neural Networkof the vector space (same with character embed-dings).
Then the tag embedding Embed(t) ?
Rdfor tag t ?
T with index k can be retrieved by thelookup operation:Embed(t) = Lek(5)where ek?
R|T |?1is a binary vector which iszero in all positions except at k-th index.
The tagembeddings start from a random initialization andcan be automatically trained by back-propagation.Figure 2 shows the new Lookup Table layer withtag embeddings.
Assuming we are at the i-th char-acter of a sentence, besides the character embed-dings, the tag embeddings of the previous tags arealso considered1.
For a fast tag inference, onlythe previous tag ti?1is used in our model eventhough a longer history of tags can be considered.The concatenation operation in Layer 1 then con-catenates the character embeddings and tag em-bedding together into a long vector a.
In this way,the tag representation can be directly incorporatedin the neural network so that the tag-tag interac-tion and tag-character interaction can be explicitlymodeled in deeper layers (See Section 3.2).
More-over, the transition score in equation (4) is notnecessary in our model, because, by incorporatingtag embedding into the neural network, the effectof tag-tag interaction and tag-character interactionare covered uniformly in one same model.
Now1We also tried the architecture in which the tag embeddingof current tag is also considered, but this did not bring muchimprovement and runs slowerFigure 3: The tensor-based transformation inLayer 2. a is the input from Layer 1.
V is thetensor parameter.
Each dashed box represents oneof the H2-many tensor slices, which defines thebilinear form on vector a.equation (4) can be rewritten as follows:s(c[1:n], t[1:n], ?)
=n?i=1f?
(ti|c[i?2:i+2], ti?1)(6)where f?
(ti|c[i?2:i+2], ti?1) is the score output fortag tiat the i-th character by the network with pa-rameters ?.
Like Collobert et al (2011) and Zhenget al (2013), our model is also trained at sentence-level and carries out inference globally.3.2 Tensor Neural NetworkA tensor is a geometric object that describes rela-tions between vectors, scalars, and other tensors.It can be represented as a multi-dimensional arrayof numerical values.
An advantage of the tensoris that it can explicitly model multiple interactionsin data.
As a result, tensor-based model have beenwidely used in a variety of tasks (Salakhutdinov etal., 2007; Krizhevsky et al, 2010; Socher et al,2013b).In Chinese word segmentation, a proper model-ing of the tag-tag interaction, tag-character inter-action and character-character interaction is veryimportant.
In linear models, these kinds of inter-actions are usually modeled as features.
In con-ventional neural network models, however, the in-put embeddings only implicitly interact throughthe non-linear function which can hardly modelthe complexity of the interactions.
Given the ad-vantage of tensors, we apply a tensor-based trans-formation to the input vector.
Formally, we use a3-way tensor V[1:H2]?
RH2?H1?H1to directlymodel the interactions, where H2is the size of296Layer 2 and H1= (w + 1) ?
d is the size of con-catenated vector a in Layer 1 as shown in Figure2.
Figure 3 gives an example of the tensor-basedtransformation2.
The output of a tensor product isa vector z ?
RH2where each dimension ziis theresult of the bilinear form defined by each tensorslice V[i]?
RH1?H1:z = aTV[1:H2]a; zi= aTV[i]a =?j,kV[i]jkajak(7)Since vector a is the concatenation of characterembeddings and the tag embedding, equation (7)can be written in the following form:zi=?p,q?j,kV[i](p,q,j,k)E[p]jE[q]kwhere E[p]jis the j-th element of the p-th embed-ding in Lookup Table layer and V[i](p,q,j,k)is the cor-responding coefficient for E[p]jand E[q]kin V[i].As we can see, in each tensor slice i, the em-beddings are explicitly related in a bilinear formwhich captures the interactions between charac-ters and tags.
The multiplicative operations be-tween tag embeddings and character embeddingscan somehow be seen as ?feature combination?,which are hand-designed in feature-based models.Our model learns the information automaticallyand encodes them in tensor parameters and em-beddings.
Intuitively, we can interpret each sliceof the tensor as capturing a specific type of tag-character interaction and character-character inter-action.Combining the tensor product with linear trans-formation, the tensor-based transformation inLayer 2 is defined as:h = g(aTV[1:H2]a+W1a+ b1) (8)where W1?
RH2?H1, b1?
RH2?1, h ?
RH2.In fact, equation (2) used in previous work is aspecial case of equation (8) when V is set to 0.3.3 Tensor FactorizationDespite tensor-based transformation being effec-tive for capturing the interactions, introducingtensor-based transformation into neural networkmodels to solve sequence labeling task is time pro-hibitive since the tensor product operation drasti-cally slows down the model.
Without consider-ing matrix optimization algorithms, the complex-ity of the non-linear transformation in equation (2)2The bias term is omitted in Figure 3 for simplicityFigure 4: Tensor product with tensor factorizationis O(H1H2) while the tensor operation complex-ity in equation (8) is O(H21H2).
The tensor-basedtransformation is H1times slower.
Moreover, theadditional tensor could bring millions of param-eters to the model which makes the model suf-fer from the risk of overfitting.
To remedy this,we propose a tensor factorization approach thatfactorizes each tensor slice as the product of twolow-rank matrices.
Formally, each tensor sliceV[i]?
RH1?H1is factorized into two low rankmatrix P[i]?
RH1?rand Q[i]?
Rr?H1:V[i]= P[i]Q[i], 1 ?
i ?
H2(9)where r  H1is the number of factors.
Substi-tuting equation (9) into equation (8), we get thefactorized tensor function:h = g(aTP[1:H2]Q[1:H2]a+W1a+ b1) (10)Figure 4 illustrates the operation in each slice ofthe factorized tensor.
First, vector a is projectedinto two r-dimension vectors f1and f2.
Then theoutput zifor each tensor slice i is the dot-productof f1and f2.
The complexity of the tensor op-eration is now O(rH1H2).
As long as r is smallenough, the factorized tensor operation would bemuch faster than the un-factorized one and thenumber of free parameters would also be muchsmaller, which prevent the model from overfitting.3.4 Max-Margin TrainingWe use the Max-Margin criterion to train ourmodel.
Intuitively, the Max-Margin criterion pro-vides an alternative to probabilistic, likelihood-based estimation methods by concentrating di-rectly on the robustness of the decision boundaryof a model (Taskar et al, 2005).
We use Y (xi)to denote the set of all possible tag sequences for297a given sentence xiand the correct tag sequencefor xiis yi.
The parameters of our model are?
= {W1, b1,W2, b2,M,L, P[1:H2], Q[1:H2]}.
Wefirst define a structured margin loss 4(yi, y?)
forpredicting a tag sequence y?
for a given correct tagsequence yi:4(yi, y?)
=n?j?1{yi,j6= y?j} (11)where n is the length of sentence xiand ?
is a dis-count parameter.
The loss is proportional to thenumber of characters with an incorrect tag in theproposed tag sequence, which increases the moreincorrect the proposed tag sequence is.
For a giventraining instance (xi, yi), we search for the tag se-quence with the highest score:y?= argmaxy?
?Y (x)s(xi, y?, ?)
(12)where the tag sequence is found and scored by theTensor Neural Network via the function s in equa-tion (6).
The object of Max-Margin training is thatthe highest scoring tag sequence is the correct one:y?= yiand its score will be larger up to a marginto other possible tag sequences y?
?
Y (xi):s(x, yi, ?)
?
s(x, y?, ?)
+4(yi, y?
)This leads to the regularized objective function form training examples:J(?)
=1mm?i=1li(?)
+?2||?||2li(?)
= maxy?
?Y (xi)(s(xi, y?, ?)
+4(yi, y?
))?s(xi, yi, ?))
(13)By minimizing this object, the score of the correcttag sequence yiis increased and score of the high-est scoring incorrect tag sequence y?
is decreased.The objective function is not differentiable dueto the hinge loss.
We use a generalization of gra-dient descent called subgradient method (Ratliff etal., 2007) which computes a gradient-like direc-tion.
The subgradient of equation (13) is:?J?
?=1m?i(?s(xi, y?max, ?)???
?s(xi, yi, ?)??)+?
?where y?maxis the tag sequence with the highestscore in equation (13).
Following Socher et al(2013a), we use the diagonal variant of AdaGradPKU MSRAIdentical words 5.5?
1048.8?
104Total words 1.1?
1062.4?
106Identical characters 5?
1035?
103Total characters 1.8?
1064.1?
106Table 1: Details of the PKU and MSRA datasetsWindow size w = 5Character(tag) embedding size d = 25Hidden unit number H2= 50Number of factors r = 10Initial learning rate ?
= 0.2Margin loss discount ?
= 0.2Regularization ?
= 10?4Table 2: Hyperparameters of our model(Duchi et al, 2011) with minibatchs to minimizethe objective.
The parameter update for the i-thparameter ?t,iat time step t is as follows:?t,i= ?t?1,i???
?t?=1g2?,igt,i(14)where ?
is the initial learning rate and g??
R|?i|is the subgradient at time step ?
for parameter ?i.4 Experiment4.1 Data and Model SelectionWe use the PKU and MSRA data provided by thesecond International Chinese Word SegmentationBakeoff (Emerson, 2005) to test our model.
Theyare commonly used by previous state-of-the-artmodels and neural network models.
Details of thedata are listed in Table 1.
For evaluation, we usethe standard bake-off scoring program to calculateprecision, recall, F1-score and out-of-vocabulary(OOV) word recall.For model selection, we use the first 90% sen-tences in the training data for training and the rest10% sentences as development data.
The mini-batch size is set to 20.
Generally, the number ofhidden units has a limited impact on the perfor-mance as long as it is large enough.
We foundthat 50 is a good trade-off between speed andmodel performance.
The dimensionality of char-acter (tag) embedding is set to 25 which achievedthe best performance and faster than 50- or 100-dimensional ones.
We also validated on the num-ber of factors for tensor factorization.
The per-formance is not boosted and the training time in-298P R F OOVCRF 87.8 85.7 86.7 57.1NN 92.4 92.2 92.3 60.0NN+Tag Embed 93.0 92.7 92.9 61.0MMTNN 93.7 93.4 93.5 64.2Table 3: Test results with different configurations.NN stands for the conventional neural network.NN+Tag Embed stands for the neural networkwith tag embeddings.creases drastically when the number of factors islarger than 10.
We hypothesize that larger factorsize results in too many parameters to train andhence perform worse.
The final hyperparametersof our model are set as in Table 2.4.2 Experiment ResultsWe first perform a close test3on the PKU datasetto show the effect of different model configura-tions.
We also compare our model with the CRFmodel (Lafferty et al, 2001), which is a widelyused log-linear model for Chinese word segmen-tation.
The input feature to the CRF model is sim-ply the context characters (unigram feature) with-out any additional feature engineering.
We usean open source toolkit CRF++4to train the CRFmodel.
All the neural networks are trained us-ing the Max-Margin approach described in Sec-tion 3.4.
Table 3 summarizes the test results.As we can see, by using Tag embedding, the F-score is improved by +0.6% and OOV recall isimproved by +1.0%, which shows that tag embed-dings succeed in modeling the tag-tag interactionand tag-character interaction.
Model performanceis further boosted after using tensor-based trans-formation.
The F-score is improved by +0.6%while OOV recall is improved by +3.2%, whichdenotes that tensor-based transformation capturesmore interactional information than simple non-linear transformation.Another important result in Table 3 is that ourneural network models perform much better thanCRF-based model when only unigram features areused.
Compared with CRF, there are two differ-ences in neural network models.
First, the discretefeature vector is replaced with dense character em-beddings.
Second, the non-linear transformation3No other material or knowledge except the training datais allowed4http://crfpp.googlecode.com/svn/trunk/doc/index.html?source=navbar?
(one) ?
(Li) ?(period)?
(two) ?
(Zhao) ?(comma)?
(three) ?
(Jiang) ?(colon)?
(four) ?
(Kong) ?
(question mark)?
(five) ?
(Feng) ?
(quotation mark)?
(six) ?
(Wu) ?
(Chinese comma)Table 4: Examples of character embeddingsis used to discover higher level representation.
Infact, CRF can be regarded as a special neural net-work without non-linear function (Wang and Man-ning, 2013).
Wang and Manning (2013) conductan empirical study on the effect of non-linearityand the results suggest that non-linear models arehighly effective only when distributed representa-tion is used.
To explain why distributed represen-tation captures more information than discrete fea-tures, we show in Table 4 the effect of characterembeddings which are obtained from the lookuptable of MMTNN after training.
The first row liststhree characters we are interested in.
In each col-umn, we list the top 5 characters that are near-est (measured by Euclidean distance) to the cor-responding character in the first row according totheir embeddings.
As we can see, characters inthe first column are all Chinese number charactersand characters in the second column and the thirdcolumn are all Chinese family names and Chinesepunctuations respectively.
Therefore, comparedwith discrete feature representations, distributedrepresentation can capture the syntactic and se-mantic similarity between characters.
As a re-sult, the model can still perform well even if somewords do not appear in the training cases.We further compare our model with previousneural network models on both PKU and MSRAdatasets.
Since Zheng et al (2013) did notreport the results on the these datasets, we re-implemented their model and tested it on the testdata.
The results are listed in the first three rowsof Table 5, which shows that our model achievedhigher F-score than the previous neural networkmodels.4.3 Unsupervised Pre-trainingPrevious work found that the performance canbe improved by pre-training the character em-beddings on large unlabeled data and using theobtained embeddings to initialize the charac-ter lookup table instead of random initialization299Models PKU MSRAP R F OOV P R F OOV(Mansur et al, 2013) 87.1 87.9 87.5 48.9 92.3 92.2 92.2 53.7(Zheng et al, 2013) 92.8 92.0 92.4 63.3 92.9 93.6 93.3 55.7MMTNN 93.7 93.4 93.5 64.2 94.6 94.2 94.4 61.4(Mansur et al, 2013) + Pre-training 91.2 92.7 92.0 68.8 93.1 93.1 93.1 59.7(Zheng et al, 2013) + Pre-training 93.5 92.2 92.8 69.0 94.2 93.7 93.9 64.1MMTNN + Pre-training 94.4 93.6 94.0 69.0 95.2 94.6 94.9 64.8Table 5: Comparison with previous neural network models(Mansur et al, 2013; Zheng et al, 2013).
Mikolovet al (2013b) show that pre-trained embeddingscan capture interesting semantic and syntactic in-formation such as king?man+woman ?
queenon English data.
There are several ways to learnthe embeddings on unlabeled data.
Mansur et al(2013) used the model proposed by Bengio et al(2003) which learns the embeddings based on neu-ral language model.
Zheng et al (2013) followedthe model proposed by Collobert et al (2008).They constructed a neural network that outputshigh scores for windows that occur in the cor-pus and low scores for windows where one char-acter is replaced by a random one.
Mikolov etal.
(2013a) proposed a faster skip-gram modelword2vec5which tries to maximize classificationof a word based on another word in the same sen-tence.
In this paper, we use word2vec because pre-liminary experiments did not show differences be-tween performances of these models but word2vecis much faster to train.
We pre-train the embed-dings on the Chinese Giga-word corpus (Graff andChen, 2005).
As shown in Table 5 (last threerows), both the F-score and OOV recall of ourmodel boost by using pre-training.
Our model stilloutperforms other models after pre-training.4.4 Minimal Feature EngineeringAlthough we focus on the question that how far wecan go without using feature engineering in thispaper, the study of deep learning for NLP tasksis still a new area in which it is currently chal-lenging to surpass the state-of-the-art without ad-ditional features.
To incorporate features into theneural network, Mansur et al (2013) proposedthe feature-based neural network where each con-text feature is represented as feature embeddings.The idea of feature embeddings is similar to thatof character embeddings described in section 2.1.5https://code.google.com/p/word2vec/Model PKU MSRABest05(Chen et al, 2005) 95.0 96.0Best05(Tseng et al, 2005) 95.0 96.4(Zhang et al, 2006) 95.1 97.1(Zhang and Clark, 2007) 94.5 97.2(Sun et al, 2009) 95.2 97.3(Sun et al, 2012) 95.4 97.4(Zhang et al, 2013) 96.1 97.4MMTNN 94.0 94.9MMTNN + bigram 95.2 97.2Table 6: Comparison with state-of-the-art systemsFormally, we assume the extracted features form afeature dictionary Df.
Then each feature f ?
Dfis represented by a d-dimensional vector which iscalled feature embedding.
Following their idea,we try to find out how well our model can performwith minimal feature engineering.A very common feature in Chinese word seg-mentation is the character bigram feature.
For-mally, at the i-th character of a sentence c[1:n], thebigram features are ckck+1(i ?
3 < k < i + 2).In our model, the bigram features are extracted inthe window context and then the correspondingbigram embeddings are concatenated with char-acter embeddings in Layer 1 and fed into Layer2.
In Mansur et al (2013), the bigram embed-dings are pre-trained on unlabeled data with char-acter embeddings, which significantly improvesthe model performance.
Given the long time forpre-training bigram embeddings, we only pre-trainthe character embeddings and the bigram embed-dings are initialized as the average of characterembeddings of ckand ck+1.
Further improve-ment could be obtained if the bigram embeddingsare also pre-trained.
Table 6 lists the segmenta-tion performances of our model as well as pre-vious state-of-the-art systems.
When bigram fea-tures are added, the F-score of our model improves300from 94.0% to 95.2% on PKU dataset and from94.9% to 97.2% on MSRA dataset.
It is a com-petitive result given that our model only use sim-ple bigram features while other models use morecomplex features.
For example, Sun et al (2012)uses additional word-based features.
Zhang et al(2013) uses eight types of features such as Mu-tual Information and Accessor Variety and theyextract dynamic statistical features from both anin-domain corpus and an out-of-domain corpus us-ing co-training.
Since feature engineering is notthe main focus of this paper, we did not experi-ment with more features.5 Related WorkChinese word segmentation has been studied withconsiderable efforts in the NLP community.
Themost popular approach treats word segmentationas a sequence labeling problem which was firstproposed in Xue (2003).
Most previous systemsaddress this task by using linear statistical mod-els with carefully designed features such as bi-gram features, punctuation information (Li andSun, 2009) and statistical information (Sun andXu, 2011).
Recently, researchers have tended toexplore new approaches for word segmentationwhich circumvent the feature engineering by au-tomatically learning features with neural networkmodels (Mansur et al, 2013; Zheng et al, 2013).Our study is consistent with this line of research,however, our model explicitly models the interac-tions between tags and context characters and ac-cordingly captures more semantic information.Tensor-based transformation was also used inother neural network models for its ability to cap-ture multiple interactions in data.
For example,Socher et al (2013b) exploited tensor-based func-tion in the task of Sentiment Analysis to cap-ture more semantic information from constituents.However, given the small size of their tensor ma-trix, they do not have the problem of high timecost and overfitting problem as we faced in mod-eling a sequence labeling task like Chinese wordsegmentation.
That?s why we propose to decreasecomputational cost and avoid overfitting with ten-sor factorization.Various tensor factorization (decomposition)methods have been proposed recently for tensor-based dimension reduction (Cohen et al, 2013;Van de Cruys et al, 2013; Chang et al, 2013).For example, Chang et al (2013) proposed theMulti-Relational Latent Semantic Analysis.
Sim-ilar to LSA, a low rank approximation of the ten-sor is derived using a tensor decomposition ap-proch.
Similar ideas were also used for collab-orative filtering (Salakhutdinov et al, 2007) andobject recognition (Ranzato et al, 2010).
Our ten-sor factorization is related to these work but usesa different tensor factorization approach.
By in-troducing tensor factorization into the neural net-work model for sequence labeling tasks, the modeltraining and inference are speeded up and overfit-ting is prevented.6 ConclusionIn this paper, we propose a new model called Max-Margin Tensor Neural Network that explicitlymodels the interactions between tags and contextcharacters.
Moreover, we propose a tensor fac-torization approach that effectively improves themodel efficiency and avoids the risk of overfitting.Experiments on the benchmark datasets show thatour model achieve better results than previous neu-ral network models and that our model can achievea competitive result with minimal feature engi-neering.
In the future, we plan to further extendour model and apply it to other structure predic-tion problems.AcknowledgmentsThis work is supported by National NaturalScience Foundation of China under Grant No.61273318 and National Key Basic Research Pro-gram of China 2014CB340504.ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.2013.
Multi-relational latent semantic analysis.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pages1602?1612, Seattle, Washington, USA, October.Association for Computational Linguistics.Aitao Chen, Yiping Zhou, Anne Zhang, and GordonSun.
2005.
Unigram language model for chi-nese word segmentation.
In Proceedings of the 4thSIGHAN Workshop on Chinese Language Process-ing, pages 138?141.
Association for ComputationalLinguistics Jeju Island, Korea.301Shay B Cohen, Giorgio Satta, and Michael Collins.2013.
Approximate pcfg parsing using tensor de-composition.
In Proceedings of NAACL-HLT, pages487?496.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Association for Computational Linguistics.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Pro-ceedings of the 25th international conference onMachine learning, pages 160?167.
ACM.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 999999:2121?2159.Thomas Emerson.
2005.
The second international chi-nese word segmentation bakeoff.
In Proceedings ofthe Fourth SIGHAN Workshop on Chinese LanguageProcessing, volume 133.David Graff and Ke Chen.
2005.
Chinese gigaword.LDC Catalog No.
: LDC2003T09, ISBN, 1:58563?230.Geoffrey E Hinton.
1986.
Learning distributed repre-sentations of concepts.
In Proceedings of the eighthannual conference of the cognitive science society,pages 1?12.
Amherst, MA.Alex Krizhevsky, Geoffrey E Hinton, et al 2010.
Fac-tored 3-way restricted boltzmann machines for mod-eling natural images.
In International Conferenceon Artificial Intelligence and Statistics, pages 621?628.John Lafferty, Andrew McCallum, and Fernando CNPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.Zhongguo Li and Maosong Sun.
2009.
Punctuation asimplicit annotations for chinese word segmentation.Computational Linguistics, 35(4):505?512.Mairgup Mansur, Wenzhe Pei, and Baobao Chang.2013.
Feature-based neural language model andchinese word segmentation.
In Proceedings ofthe Sixth International Joint Conference on NaturalLanguage Processing.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013a.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Proceedings of NAACL-HLT, pages 746?751.Marc?Aurelio Ranzato, Alex Krizhevsky, and Geof-frey E Hinton.
2010.
Factored 3-way restrictedboltzmann machines for modeling natural images.Nathan D Ratliff, J Andrew Bagnell, and Martin AZinkevich.
2007.
(online) subgradient methods forstructured prediction.Ruslan Salakhutdinov, Andriy Mnih, and GeoffreyHinton.
2007.
Restricted boltzmann machines forcollaborative filtering.
In Proceedings of the 24th in-ternational conference on Machine learning, pages791?798.
ACM.Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, pruned or continuous spacelanguage models on a gpu for statistical machinetranslation.
In Proceedings of the NAACL-HLT 2012Workshop: Will We Ever Really Replace the N-gramModel?
On the Future of Language Modeling forHLT, pages 11?19.
Association for ComputationalLinguistics.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013a.
Parsing with composi-tional vector grammars.
In Annual Meeting of theAssociation for Computational Linguistics (ACL).Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013b.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
EMNLP.Weiwei Sun and Jia Xu.
2011.
Enhancing chineseword segmentation using unlabeled data.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing, pages 970?979.
As-sociation for Computational Linguistics.Xu Sun, Yaozhong Zhang, Takuya Matsuzaki, Yoshi-masa Tsuruoka, and Jun?ichi Tsujii.
2009.
A dis-criminative latent variable chinese segmenter withhybrid word/character information.
In Proceedingsof Human Language Technologies: The 2009 An-nual Conference of the North American Chapterof the Association for Computational Linguistics,pages 56?64.
Association for Computational Lin-guistics.Xu Sun, Houfeng Wang, and Wenjie Li.
2012.
Fast on-line training with frequency-adaptive learning ratesfor chinese word segmentation and new word detec-tion.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 253?262, Jeju Island,302Korea, July.
Association for Computational Linguis-tics.Ben Taskar, Vassil Chatalbashev, Daphne Koller, andCarlos Guestrin.
2005.
Learning structured predic-tion models: A large margin approach.
In Proceed-ings of the 22nd international conference on Ma-chine learning, pages 896?903.
ACM.Huihsin Tseng, Pichuan Chang, Galen Andrew, DanielJurafsky, and Christopher Manning.
2005.
A condi-tional random field word segmenter for sighan bake-off 2005.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, volume171.Tim Van de Cruys, Thierry Poibeau, and Anna Ko-rhonen.
2013.
A tensor-based factorization modelof semantic compositionality.
In Proceedings ofNAACL-HLT, pages 1142?1151.Mengqiu Wang and Christopher D Manning.
2013.Effect of non-linear deep architecture in sequencelabeling.
In Proceedings of the Sixth InternationalJoint Conference on Natural Language Processing.Nianwen Xue.
2003.
Chinese word segmentation ascharacter tagging.
Computational Linguistics andChinese Language Processing, 8(1):29?48.Yue Zhang and Stephen Clark.
2007.
Chinese seg-mentation with a word-based perceptron algorithm.In ANNUAL MEETING-ASSOCIATION FOR COM-PUTATIONAL LINGUISTICS, volume 45, page 840.Ruiqiang Zhang, Genichiro Kikui, and EiichiroSumita.
2006.
Subword-based tagging by condi-tional random fields for chinese word segmentation.In Proceedings of the Human Language Technol-ogy Conference of the NAACL, Companion Volume:Short Papers, pages 193?196.
Association for Com-putational Linguistics.Longkai Zhang, Houfeng Wang, Xu Sun, and MairgupMansur.
2013.
Exploring representations from un-labeled data with co-training for Chinese word seg-mentation.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 311?321, Seattle, Washington, USA,October.
Association for Computational Linguistics.Xiaoqing Zheng, Hanyang Chen, and Tianyu Xu.2013.
Deep learning for Chinese word segmenta-tion and POS tagging.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 647?657, Seattle, Wash-ington, USA, October.
Association for Computa-tional Linguistics.303
