Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 438?442, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSAIL: A hybrid approach to sentiment analysisNikolaos Malandrakis1, Abe Kazemzadeh2, Alexandros Potamianos3, Shrikanth Narayanan11 Signal Analysis and Interpretation Laboratory (SAIL), USC, Los Angeles, CA 90089, USA2 Annenberg Innovation Laboratory (AIL), USC, Los Angeles, CA 90089, USA3Department of ECE, Technical University of Crete, 73100 Chania, Greecemalandra@usc.edu, kazemzad@usc.edu, potam@telecom.tuc.gr, shri@sipi.usc.eduAbstractThis paper describes our submission for Se-mEval2013 Task 2: Sentiment Analysis inTwitter.
For the limited data condition we usea lexicon-based model.
The model uses an af-fective lexicon automatically generated from avery large corpus of raw web data.
Statisticsare calculated over the word and bigram af-fective ratings and used as features of a NaiveBayes tree model.
For the unconstrained datascenario we combine the lexicon-based modelwith a classifier built on maximum entropylanguage models and trained on a large exter-nal dataset.
The two models are fused at theposterior level to produce a final output.
Theapproach proved successful, reaching rank-ings of 9th and 4th in the twitter sentimentanalysis constrained and unconstrained sce-nario respectively, despite using only lexicalfeatures.1 IntroductionThe analysis of the emotional content of text, isrelevant to numerous natural language processing(NLP), web and multi-modal dialogue applications.To that end there has been a significant scientificeffort towards tasks like product review analysis(Wiebe and Mihalcea, 2006; Hu and Liu, 2004),speech emotion extraction (Lee and Narayanan,2005; Lee et al 2002; Ang et al 2002) and puretext word (Esuli and Sebastiani, 2006; Strappar-ava and Valitutti, 2004) and sentence (Turney andLittman, 2002; Turney and Littman, 2003) levelemotion extraction.The rise of social media in recent years has seena shift in research focus towards them, particularlytwitter.
The large volume of text data available isparticularly useful, since it allows the use of com-plex machine learning methods.
Also important isthe interest on the part of companies that are activelylooking for ways to mine social media for opinionsand attitudes towards them and their products.
Sim-ilarly, in journalism there is interest in sentimentanalysis for a way to process and report on the publicopinion about current events (Petulla, 2013).Analyzing emotion expressed in twitter borrowsfrom other tasks related to affective analysis, butalso presents unique challenges.
One common is-sue is the breadth of content available in twitter: amore limited domain would make the task easier,however there are no such bounds.
There is also asignificant difference in the form of language usedin tweets.
The tone is informal and typographicaland grammatical errors are very common, makingeven simple tasks, like Part-of-Speech tagging muchharder.
Features like hashtags and emoticons canalso be helpful (Davidov et al 2010).This paper describes our submissions for Se-mEval 2013 task 2, subtask B, which deals pri-marily with sentiment analysis in twitter.
For theconstrained condition (using only the organizer-provided twitter sentences) we implemented a sys-tem based on the use of an affective lexicon and part-of-speech tag information, which has been shownrelevant to the task (Pak and Paroubek, 2010).For the unconstrained condition (including externalsources of twitter sentences) we combine the con-strained model with a maximum entropy language438model trained on external data.2 Experimental procedureWe use two separate models, one for the constrainedcondition and a combination for the unconstrainedcondition.
Following are short descriptions.2.1 Lexicon-based modelThe method used for the constrained condition isbased on an affective lexicon containing out-of-context affective ratings for all terms contained ineach sentence.
We use an automated algorithm ofaffective lexicon expansion based on the one pre-sented in (Malandrakis et al 2011), which in turnis an expansion of (Turney and Littman, 2002).We assume that the continuous (in [?1, 1]) va-lence and arousal ratings of any term can be repre-sented as a linear combination of its semantic simi-larities to a set of seed words and the affective rat-ings of these words, as follows:v?
(wj) = a0 +N?i=1ai v(wi) dij , (1)where wj is the term we mean to characterize,w1...wN are the seed words, v(wi) is the valence rat-ing for seed word wi, ai is the weight correspondingto seed word wi (that is estimated as described next),dij is a measure of semantic similarity between wiand wj .
For the purposes of this work, the seman-tic similarity metric is the cosine similarity betweencontext vectors computed over a corpus of 116 mil-lion web snippets collected by posing one query forevery word in the Aspell spellchecker?s vocabularyto the Yahoo!
search engine and collecting up to 500of the top results.Given a starting, manually annotated, lexicon wecan select part of it to serve as seed words and thenuse 1 to create a system of linear equations wherethe only unknowns are the weights ai.
The systemis solved using Least Squares Estimation.
That pro-vides us with an equation that can generate affectiveratings for every term (not limited to words), as longas we can estimate the semantic similarity betweenit and the seed words.Seed word selection is performed by a simpleheuristic (though validated through experiments):we want seed words to have extreme affective rat-ings (maximum absolute value) and we want the setto be as closed to balanced as possible (sum of seedratings equal to zero).Given these term ratings, the next step is combin-ing them through statistics.
To do that we use sim-ple statistics (mean, min, max) and group by partof speech tags.
The results are statistics like ?max-imum valence among adjectives?, ?mean arousalamong proper nouns?
and ?number of verbs andnouns?.
The dimensions used are: valence, absolutevalence and arousal.
The grouping factors are the 39Penn treebank pos tags plus higher order tags (adjec-tives, verbs, nouns, adverbs and combinations of 2,3and 4 of them).
The statistics extracted are: mean,min, max, most extreme, sum, number, percentageof sentence coverage.
In the case of bigram terms nopart-of-speech filtering/grouping is applied.
Thesestatistics form the feature vectors.Finally we perform feature selection on the mas-sive set of candidates and use them to train a model.The model selected is a Naive Bayes tree, a tree withNaive Bayes classifiers on each leaf.
The motivationcomes by considering this a two stage problem: sub-jectivity detection and polarity classification, mak-ing a hierarchical model a natural choice.
NB treesproved superior to other types of trees during ourtesting, presumably due to the smoothing of obser-vation distributions.2.2 N-gram language modelThe method used for the unconstrained conditionis based on a combination of the automatically ex-panded affective lexicon described in the previ-ous section together with a bigram language modelbased on the work of (Wang et al 2012), whichuses a large set of twitter data from the U.S. 2012Presidential election.
As a part of the unconstrainedsystem, we were able to leverage external annotateddata apart from those provided by the SEMEVAL2013 sentiment task dataset.
Of the 315 milliontweets we collected about the election, we anno-tated a subset of 40 thousand tweets using Ama-zon Mechanical Turk.
The annotation labels thatwe used were ?positive?, ?negative?, ?neutral?, and?unsure?, and additionally raters could mark tweetsfor sarcasm and humor.
We excluded tweets markedas ?unsure?
as well as tweets that had disagree-439ment in labels if they were annotated by more thanone annotator.
To extract the bigram features, weused a twitter-specific tokenizer (Potts, 2011), whichmarked uniform resource locators (URLs), emoti-cons, and repeated characters, and which lowercasedwords that began with capital letters followed bylowercase letters (but left words in all capitals).
Thebigram features were computed as presence or ab-sense in the tweet rather than counts due to the smallnumber of words in tweets.
The machine learningmodel used to classify the tweets was the Megammaximum entropy classifier (Daume?
III, 2004) inthe Natural Language Toolkit (NLTK) (Bird et al2009).2.3 FusionThe submitted system for the unconstrained condi-tion leverages both the lexicon-based and bigramlanguage models.
Due to the very different natureof the models we opt to not fuse them at the featurelevel, using a late fusion scheme instead.
Both par-tial models are probabilistic, therefore we can usetheir per-class posterior probabilities as features ofa fusion model.
The fusion model is a linear kernelSVM using six features, the three posteriors fromeach partial model, and trained on held out data.3 ResultsFollowing are results from our method, evaluatedon the testing sets (of sms and twitter posts) ofSemEval2013 task 2.
We evaluate in terms of 3-class classification, polarity classification (positivevs.
negative) and subjectivity detection (neutral vs.other).
Results shown in terms of per category f-measure.3.1 ConstrainedThe preprocessing required for the lexicon-basedmodel is just part-of-speech tagging using Treetag-ger (Schmid, 1994).
The lexicon expansion methodis used to generate valence and arousal ratings forall words and ngrams in all datasets and the part ofspeech tags are used as grouping criteria to gener-ate statistics.
Finally, feature selection is performedusing a correlation criterion (Hall, 1999) and the re-sulting feature set is used to train a Naive Bayestree model.
The feature selection and model train-Table 1: F-measure results for the lexicon-based model,using different machine learning methods, evaluated onthe 3-class twitter testing data.modelper-class F-measureneg neu posNbayes 0.494 0.652 0.614SVM 0.369 0.677 0.583CART 0.430 0.676 0.593NBTree 0.561 0.662 0.643Table 2: F-measure results for the constrained condition,evaluated on the testing data.set classesper-class F-measureneg neu pos/othertwitter3-class 0.561 0.662 0.643pos vs neg 0.679 0.858neu vs other 0.685 0.699sms3-class 0.506 0.709 0.531pos vs neg 0.688 0.755neu vs other 0.730 0.628ing/classification was conducted using Weka (Wit-ten and Frank, 2000).The final model uses a total of 72 features, whichcan not be listed here due to space constraints.
Thevast majority of these features are necessary to de-tect the neutral category: positive-negative separa-tion can be achieved with under 30 features.One aspect of the model we felt worth investigat-ing, was the type of model to be used.
Using a multi-stage model, performing subjectivity detection be-fore positive-negative classification, has been shownto provide an improvement, however single modelshave also been used extensively.
We compared somepopular models: Naive Bayes, linear kernel SVM,CART-trained tree and Naive Bayes tree, all usingthe same features, on the twitter part of the SemEvaltesting data.
The results are shown in Table 1.
Thetwo Naive Bayes-based models proved significantlybetter, with NBTree being clearly the best model forthese features.Results from the submitted constrained model areshown in Table 2.
Looking at the twitter data re-sults and comparing the positive-negative vs the4403-class results, it appears the main weakness ofthis model is subjectivity detection, mostly on theneutral-negative side.
It is not entirely clear to uswhether that is an artifact of the model (the nega-tive class has the lowest prior probability, thus maysuffer compared to neutral) or of the more complexforms of negativity (sarcasm, irony) which we do notdirectly address.
There is a definite drop in perfor-mance when using the same twitter-trained model onsms data, which we would not expect, given that thefeatures used are not twitter-specific.
We believe thisgap is caused by lower part-of-speech tagger perfor-mance: visual inspection reveals the output on twit-ter data is fairly bad.Overall this model ranked 9th out of 35 in thetwitter set and 11th out of 28 in the sms set, amongall constrained submissions.3.2 UnconstrainedTable 3: F-measure results for the maximum entropymodel with bigram features, evaluated on the testing data.set classesper-class F-measureneg neu pos/othertwitter3-class 0.403 0.661 0.623pos vs neg 0.586 0.804neu vs other 0.661 0.704sms3-class 0.390 0.587 0.542pos vs neg 0.710 0.648neu vs other 0.587 0.641Table 4: F-measure results for the unconstrained condi-tion, evaluated on the testing data.set classesper-class F-measureneg neu pos/othertwitter3-class 0.565 0.679 0.655pos vs neg 0.672 0.881neu vs other 0.667 0.732sms3-class 0.502 0.723 0.538pos vs neg 0.625 0.772neu vs other 0.710 0.637In order to create the submitted unconstrainedmodel we train an SVM model using the lexicon-based and bigram language model posterior proba-bilities as features.
This fusion model is trained onheld-out data (the development set of the SemEvaldata).
The results of classification using the bigramlanguage model alone are shown in Table 3 and theresults from the final fused model are shown in Ta-ble 4.
Looking at relative per-class performance, theresults follow a form most similar to the constrainedmodel, though there are gains in all cases.
Thesegains are less significant when evaluated on the smsdata, resulting in a fair drop in ranks: the bigram lan-guage model (expectedly) suffers more when mov-ing to a different domain, since it uses words asfeatures rather than the more abstract affective rat-ings used by the lexicon-based model.
Also, becausethe external data used to train the bigram languagemodel was from discussions of politics on Twitter,the subject matter also varied in terms of prior senti-ment distribution in that the negative class was pre-dominant in politics, which resulted in high recallbut low precision for the negative class.This model ranked 4th out of 16 in the twitter setand 7th out of 17 in the sms set, among all uncon-strained submissions.4 ConclusionsWe presented a system of twitter sentiment analy-sis combining two approaches: a hierarchical modelbased on an affective lexicon and a language model-ing approach, fused at the posterior level.
The hier-archical lexicon-based model proved very successfuldespite using only n-gram affective ratings and part-of-speech information.
The language model wasnot as good individually, but provided a noticeableimprovement to the lexicon-based model.
Overallthe models achieved good performance, ranking 9thof 35 and 4th of 16 in the constrained and uncon-strained twitter experiments respectively, despite us-ing only lexical information.Future work will focus on incorporating im-proved tokenization (including part-of-speech tag-ging), making better use of twitter-specific featureslike emoticons and hashtags, and performing affec-tive lexicon generation on twitter data.441ReferencesJ.
Ang, R. Dhillon, A. Krupski, E. Shriberg, and A. Stol-cke.
2002.
Prosody-based automatic detection of an-noyance and frustration in human-computer dialog.
InProc.
ICSLP, pages 2037?2040.S.
Bird, E. Klein, and E. Loper.
2009.
Natural LanguageProcessing with Python.
O?Reilly Media.H.
Daume?
III.
2004.
Notes on cg and lm-bfgs op-timization of logistic regression.
Paper available athttp://pub.
hal3.
name# daume04cg-bfgs, implementa-tion available at http://hal3.
name/megam.D.
Davidov, O. Tsur, and A. Rappoport.
2010.
Enhancedsentiment learning using twitter hashtags and smileys.In Proc.
COLING, pages 241?249.A.
Esuli and F. Sebastiani.
2006.
Sentiwordnet: A pub-licly available lexical resource for opinion mining.
InProc.
LREC, pages 417?422.M.
A.
Hall.
1999.
Correlation-based feature selectionfor machine learning.
Ph.D. thesis, The University ofWaikato.M.
Hu and B. Liu.
2004.
Mining and summarizing cus-tomer reviews.
In Proc.
SIGKDD, KDD ?04, pages168?177.
ACM.C.
M. Lee and S. Narayanan.
2005.
Toward detectingemotions in spoken dialogs.
IEEE Transactions onSpeech and Audio Processing, 13(2):293?303.C.
M. Lee, S. Narayanan, and R. Pieraccini.
2002.
Com-bining acoustic and language information for emotionrecognition.
In Proc.
ICSLP, pages 873?876.N.
Malandrakis, A. Potamianos, E. Iosif, andS.
Narayanan.
2011.
Kernel models for affec-tive lexicon creation.
In Proc.
Interspeech, pages2977?2980.A.
Pak and P. Paroubek.
2010.
Twitter as a corpusfor sentiment analysis and opinion mining.
In Proc.LREC, pages 1320?1326.S.
Petulla.
2013.
Feelings, nothing more than feelings:The measured rise of sentiment analysis in journalism.Neiman Journalism Lab, January.C.
Potts.
2011.
Sentiment symposium tutorial: Tokeniz-ing.
Technical report, Stanford Linguistics.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In Proc.
International Confer-ence on New Methods in Language Processing, vol-ume 12, pages 44?49.C.
Strapparava and A. Valitutti.
2004.
WordNet-Affect:an affective extension of WordNet.
In Proc.
LREC,volume 4, pages 1083?1086.P.
Turney and M. L. Littman.
2002.
UnsupervisedLearning of Semantic Orientation from a Hundred-Billion-Word Corpus.
Technical report ERC-1094(NRC 44929).
National Research Council of Canada.P.
Turney and M. L. Littman.
2003.
Measuring praiseand criticism: Inference of semantic orientation fromassociation.
ACM Transactions on Information Sys-tems, 21:315?346.H.
Wang, D. Can, A. Kazemzadeh, F. Bar, andS.
Narayanan.
2012.
A system for real-time twittersentiment analysis of 2012 u.s. presidential electioncycle.
In Proc.
ACL, pages 115?120.J.
Wiebe and R. Mihalcea.
2006.
Word sense and subjec-tivity.
In Proc.
COLING/ACL, pages 1065?1072.Ian H.Witten and Eibe Frank.
2000.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann.442
