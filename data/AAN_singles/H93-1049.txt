HYPOTHESIZ ING UNTAGGED TEXT WORD ASSOCIAT ION FROMTomoyoshi MatsukawaBBN Systems and Technologies70 Fawcett St.Cambridge, MA 02138ABSTRACTThis paper reports a new method for suggesting wordassociations, based on a greedy algorithm that employs Chi-square statistics on joint frequencies of pairs of word groupscompared against chance co-occurrence.
The benefits of thisnew approach are: 1) we can consider even low frequencywords and word pairs, and 2) word groups and wordassociations can be automatically generated.
The methodprovided 87% accuracy in hypothesizing word associations forunobserved combinations of words in Japanese text.1.
INTRODUCTIONUsing mutual information for measuring word associationhas become popular since \[Church and Hanks, 1990\]defined word association ratio as mutual informationbetween two words.
Word association ratios are apromising tool for lexicography, but there seem to be atleast two limitations to the method: 1) much data with lowfrequency words or word pairs cannot be used and 2)generalization of word usage still depends totally onlexicographers.In this paper, we propose an alternative (or extended)method for suggesting word associations using Chi-squarestatistics, which can be viewed as an approximation tomutual information.
Rather than considering significanceof joint frequencies of word pairs as \[Church and Hanks,1990\] did, our algorithm uses joint frequencies of pairs ofword groups  instead.
The algorithm employs a hill-climbing search for a pair of word groups that occursignificantly frequently.The benefits of this new approach are:1) that we can consider even low frequencywords and word pairs, and2) that word groups or word associations can beautomatically generated, .namely automatichypothesis of word associations, which canlater be reviewed by a lexicographer.3) word associations can be used in parsing andunderstanding atural language, as well as innatural language generation \[Smadja ndMcKeown, 1990\].Our method proved to be 87% accurate in hypothesizingword associations for unobserved combinations of words inJapanese text, where accuracy was tested by humanverification of a random sample of hypothesized word pairs.We extracted 14,407 observations of word co-occurrences,involving 3,195 nouns and 4,365 verb/argument pairs.
Outof this we hypothesized 7,050 word associations.
Thecorpus size was 280,000 words.
We would like to applythe same approach to English.2.
RELATED WORKSome previous work (e.g., \[Weischedel, et al, 1990\])found verb-argument associations from bracketed text, suchas that in TREEBANK; however, this paper, and relatedwork has hypothesized word associations from untaggedtext.\[Hindle 1990\] confirmed that word association ratios canbe used for measuring similarity between nouns.
Forexample, "ship", "plane", "bus", etc., were automaticallyranked as similar to "boat".
\[Resnik 1992\] reported a wordassociation ratio for identifying noun classes from a pre-existing hierarchy as selectional constraints on the objectof a verb.\[Brown et.al.
1992\] proves that, under the assumption of abi-gram class model, the perplexity of a corpus isminimized when the average mutual information betweenword classes is maximized.
Based on that fact, they clusterwords via a greedy search algorithm which finds a localmaximum in average mutual information.Our algorithm considers joint frequencies of pairs of wordgroups (as \[Brown et.
al.
1992\] does) in contrast o jointfrequencies of word pairs as in \[Church and Hanks, 1990\]and \[Hindle 1990\].
Here a word group means any subset ofthe whole set of words.
For example, "ship," "plane,""boat" and "car" may be a word group.
The algorithm willfind pairs of such word groups.
Another similarity to\[Brown et.
al.
1992\]'s clustering algorithm is the use ofgreedy search for a pair of word groups that occursignificantly frequently, using an evaluation function basedon mutual information between classes.On the other hand, unlike \[Brown et.
al.
1992\], we assumesome automatic syntactic analysis of the corpus, namelypart-of-speech analysis and at least finite-stateapproximations to syntactic dependencies.
Moreover, theclustering is done depth first, not breadth first as \[Brown et.248al.
1992\], i.e., clusters are hypothesized one by one, not inparallel.3.
OVERVIEW OF THE METHODThe method consists of three phases:1) Automatic part of speech taggingof text.
First, texts are labeled by ourprobabilistic part of speech tagger (POST)which has been extended for Japanesemorphological processing \[Matsukawa et.
al.1993\].
This is fully automatic; humanreview is not necessary under the assumptionthat the tagger has previously been trainedon appropriate xt \[Meteer et.
al.
1991\] 12) Finite state pattern matching.Second, a finite-state pattern matcher withpatterns representing possible grammaticalrelations, such as verb/argument pairs,nominal compounds, etc.
is run over thesample text to suggest word pairs which willbe considered candidates for wordassociations.
As a result, we get a word co-occurrence matrix.
Again, no human reviewof the pattern matching is assumed.3) Filtering/Generalization of wordassociations via Chi-square.
Third,given the word co-occurrence matrix, theprogram starts from an initial pair of wordgroups (or a submatrix in the matrix),incrementally adding into the submatrix aword which locally gives the highest Chi-square score to the submatrix.
Finally,words are removed which give a higher Chi-square score by their removal.
By adding andremoving words until reaching anappropriate significance level, we get asubmatr ix  as a hypothesis of wordassociations between the cluster of wordsrepresented asrows in the submatrix and thecluster of words represented as columns inthe submatrix.4 WORD SEGMENTATION AND PARTOF SPEECH LABELING1 In our experience thus far in three domains and in bothJapanese and English, while retraining POST on domain-specific data would reduce the error rate, the effect on overallperformance of the system in data extraction from text hasbeen small enough to make retraining unnecessary.
The effectof domain-specific lexical entries (e.g., DRAM is a noun inmicroelectronics) often mitigates the need to retrain.Since in Japanese word separators such as spaces are notpresent, words must be segmented before we assign part ofspeech to words.
To do this, we use JUMAN from KyotoUniversity to segment Japanese text into words, AMED,an example-based segmentation corrector, and a HiddenMarkov Model (POST) \[Matsukawa, et.
al.
1993\].
Forexample, POST processes an input text such as thefollowing:and produces tagged text such as: 2- -~j /CONJ ,  /TT ~/CN ~/CN ~,~/rM~_:~)~:~.,/PN O/NCM ~ ~/SN ::\[~,~/CN "~/CML./ADV.
/ IT  ~/CN I JCN ~/NCM ~'~#2/ADJ~..jCN Q)/NCM .Jzff/./CN ~\[Iii~/CN I/~'T/CN ~/NCM~_ ~/FN "~/PT, \[FT ~ 2\[s;/PN ~TPT ~/NCM~/CM ~\]~t-~ ~/VB ~,~$/VSUF ?
/KT5.
FINITE STATE PATTERNMATCHINGWe use the following finite state patterns for extractingpossible Japanese verb/argument word co-occurrences fromautomatically segmented and tagged Japanese text.Completely different patterns would be used for English.PN PT "'" SNSNwhere CN = common ounPN = proper nameSN = Sa-inflection oun (nominal verb)CM = case marker (-nom/-acc argument)PT = particle (other arguments)VB = verbHere, the first part (CN, PN or SN) represents a noun.Since in Japanese the head noun of a noun phrase is alwaysat the right end of the phrase, this part should alwaysmatch a head noun.
The second part (CM or PT) representsa postposition which identifies an argument of a verb.
Thefinal pattern element (VB or SN) represents a verb.
Sa-inflection nouns (SN) are nominalized verbs which form averb phrase with the morpheme "suru.
"2 CONJ = conjunction; Tr = Japanese comma;CN = common oun; TM = Top ic  marker;PN - proper noun; etc.249Distance0124Matched Textg~| /CN \ ] j - -  t~/CN ~:'~_/CN k/PT ~J~/'SN xJ-&/VB ~ ~/FN "~TPT .
.
.
.7 9 MX/PN 0)/NCM ~_~/CN ~/PT ~a~/CN ~i~}~t~/SN ~/CM L~NB ...~I\]Z~/ADJ ~:~t/CN ~/'PT "~'/~--b/CN \]J?/CM ~i~l~/SN L~NB ...~- /CN -~/CN ~/CN ~/PT ~/CN ~/NNSU~- -F /CN ~/NCM~j #/ON~ig~/SN ~/NCMFigure 1: Examples of Pattern Matches with Skipping over Words.Since argument structure in Japanese is marked bypostpositions, i.e., case markers (i.e., "o," "ga") andpartic?,es (e.g., "ni," "kara," .
.
.
), word combinationsmatched with the patterns will represent associationsbetween a noun filling a particular argument type (e.g.,"o") and a verb.
Note that topic markers (TM; i.e., "wa")and toritate markers (TTM; e.g.
"mo", "sae", ...) are notincluded in the pattern since these do not uniquely identifythe case of the argument.Just as in English, the arguments of a verb in Japanesemay be quite distant from the verb; adverbial phrases andscrambling are two cases that may separate a verb from itsargument(s).
We approximate his in a finite state machineby allowing words to be skipped.
In our experiment, up tofour words could be skipped.
As shown in Figure 1,matching an argument structure varies from distance 0 to 4.By limiting the algorithm to a maximum of four wordgaps, and by not considering the ambiguous cases of topicmarkers and taritate markers, we have chosen to limit thecases considered in favor of high accuracy in automaticallyhypothesizing word associations.
\[Brent, 1991\] similarlylimited what his algorithm could learn in favor of highaccuracy.6.
F ILTERING ANDGENERAL IZAT ION V IA  CHI -SQUAREWord combinations found via the finite state patternsinclude a noun, postposition, and a verb.
A twodimensional matrix (a word co-occurrence matrix) isformed, where the columns are nouns, and the rows arepairs of a verb plus postposifion.
The cells of the matrixare the frequency of the noun (column element) co-occurring in the given case with that verb (row element).Starting from a submatrix, the algorithm successively addsto the submatfix the word with the largest Chi-square scoreamong all words outside the submatrix.
Words are addeduntil a local maximum is reached.
Finally, theappropriateness of the submatrix as a hypothesis of wordassociations i  checked with heuristic riteria based on thesizes of the row and the column of the submatrix.Currently, we use the following criteria for appropriatenessof a submatrix:LET  1 : size of row of submatfixm : size of column of submatrixC1, C2, C3 : parametersIF 1 > C1, andm > C1, and1 > C2 or m/l < C3, andm > C2 or l/m < C3THEN the submatrix is appropriate.For any submatrix found, the co-occurrence observationsfor the clustered words are removed from the word co-occurrence matrix and treated as a single column ofclustered nouns and a single row of clustered verb plus casepairs.
Currently, we use the following values for theparameters: C1=2, C2=10, and C3=10.Table 1. shows an example of clustering starting from theinitial submatrix shown in Figure 2.
The words in Figure2 were manually selected as words meaning "organization.
"In Table 1, the first (leftmost) column indicates the wordwhich was added to the submatfix at each step.
The secondcolumn gives an English gloss of the word.
The thirdcolumn reports fix,Y), the frequency of the co-occurrencesbetween the word and the words that co-occur with it.
Forexample, the first line of the table shows that the word"~/~L"  (establish/-acc) o-occurred with the"organization" words 26 times.
The rightmost columnspecifies I(X,Y), the scaled mutual information between therows and columns of the submatrix.
As the clusteringproceeds, I(X,Y) gets larger.~_~(company), ;~k:~l\](head quarter), mS(organization),~(coorporat ion) ,  iitij:~.J:(both companies), ~(schoo l ) ,~:~t\](the company), zj~:~i(child company), ~l~(bank),~/~(depar tment  store), ~t~t~\]~(agency), ~n0(coop.
),~j:~IXbusiness company), ~.~(ci ty bank), ~)~(stand),~-~ \[~l~(trust bank), 3~/~(branch), ~-~ (credit association),:~k)~(head store), ~--~--(university), :~-:~(each ompany),--~\] ~-- ~ (department store), JAR(agriculture cooperative),- -~  --(maker), :~:)~(book store), if" L," I~')-~j(TV station),7?~ :Y" ~ ~ ~ M(agency), X --)'?~(superrnarket),?~\[~tXjo int-stock corporation), ~(doctoFs  office),)~(all stores)Figure 2: The initial word group (submatrix) for theclustering shown in Table 1.250Word added~/~~/~~/~~/~~/~I~< ~~/~~/~~/~~?/~~/~A~/~f i?
/~~z/ ~ z~~z /~~/~~/~~/~~z /~~/~\ ]~~AZ~ZATTG lossestablish/-acctie-up/withtie-up/-nomunite/withcooperate/-nompossess/-nomunite/-nomadvance/-nomin successionproceed/-nompurchase/-accentrust/-accproduce/-nomdevelop/-nominvest/-nomexpand/withdevelop/-nompublish/-nomagree/-nomdemand/frominvest/insell/-nompurchase/-nomopen/-accintroduce/fromcreate/-nomutilize/atlimit/totreat/-nomconnect/-nomdo/-nomexclude/-accoppose/tosign/-copulasell/toparticipate/incorporationmajorJapanNisho-Iwaithree partiesDrug CompanySonydealerInstitutionHondaMitsubishiAT&TAir LinerespectivelyHondaFreq2625181178765456676334335734333333533344955434555433433I0.110.190.250.290.320.350.380.400.430.440.460.470.490.510.520.540.550.560.580.590.600.610.630.640.650.660.670.680.690.690.700.710.710.720.720.720.740.750.770.780.790.800.810.810.820.830.830.840.840.850.85~t~ Bank 7 0.85~j~:~ Air Line 6 0.85~-~.~ Trust Company 4 0.85~I~ Steel Company 4 0.85Table 1: Example of Clustering7.
EVALUATIONUsing 280,000 words of Japanese source text from theT IPSTER joint ventures domain, we tried severalvariations of the initial submatrices (word groups) fromwhich the search in step three of the method starts:a) complete bipartite subgraphs,b) pre-classified noun groups andc) significantly frequent word pairs.Based on the results of the experiments, we concluded thatalternative (b) gives both the most accurate wordassociations and the highest coverage of word associations.This technique is practical because classification of nounsis generally much simpler than that of verbs.
We don'tpropose any automatic algorithm to accomplish nounclassification, but instead note that we were able tomanually classify nouns in less than ten categories at about500 words/hour.
That productivity was achieved using ournew tool for manual word classification, which is partiallyinspired by EDR's way of classifying their semantic lexicaldata \[Matsukawa and Yokota, 1991 \].Based on a corpus of 280,000 words in the TIPSTER jointventures domain, the most frequently occurring Japanesenouns, proper nouns, and verbs were automaticallyidentified.
Then, a student classified the frequentlyoccurring nouns into one of the twelve categories in (1)below, and each frequently occurring proper noun into oneof the four categories in (2) below, using a menu-basedtool, we were able to categorize 3,195 lexical entries in 12person-hours.
3 These categories were then used as input tothe word co-occurrence algorithm.1.
Common noun categories1 a. OrganizationCORPORATIONGOVERNMENTUNDETERMINED-CORPORATIONOTHER-ORGANIZATION1 b. LocationCITYCOUNTRYPROVINCE3 We divided the process of classifying common nouns intotwo phases; classification into the four categories la, lb, lcand ld, and further classification i to the twelve categories.
Asa result, each word was checked twice.
We found that using twophases generally improves both overall productivity andconsistency.251OTHER-LOCATION1 c. PersonENTITY-OFFICERTrlLEOTHER-PERSON1 d. Other2.
Proper noun categoriesORGANIZATIONLOCATIONPERSONOTHERUsing the 280,000 word joint venture corpus, we collected14,407 word co-occurrences, involving 3,195 nouns and4,365 verb/argument pairs, by the finite state pattern givenin Section 5.
16 submatrices were clustered, grouping 810observed word co-occurrences and 6,240 unobserved (orhypothesized) word co-occurrences.
We evaluated theaccuracy of the system by manual review of a randomsample of 500 hypothesized word co-occurrences.
Of these,435, or 87% were judged reasonable.
This ratio is finecompared with a random sample of 500 arbitrary word co-occurrences between the 3,195 nouns and the 4,365verb/argument pairs, of which only 153 (44%) were judgedreasonable.
Table 2 below shows some examples judgedreasonable; questionable xamples are marked by "?
";unreasonable hypotheses are marked with an asterisk.With a small corpus (280,000 words) such as ours,considering small frequency co-occurrences is critical.Looking at Table 3 below, if we had to ignore co-occurrences with frequency less than five (as \[Church andHanks 1990\] did), there would be very little data.
With ourmethod, as long as the frequency of co-occurrence of theword being considered with the set is greater than two, thestatistic is stable.Frequency Number ofWord Pairs0 62401 6312 1133 364 185 46 27 39 110 116 1Table 3: Pair Frequencies8.
CONCLUSIONOur method achieved fully automatic hypothesis of wordassociations, tarting from untagged text and generalizingto unobserved word associations.
As a result of humanreview 87% of the hypotheses were judged to bereasonable.
Because the technique considers low frequencycases, most of the data was used in making generalizations.It remains to be determined how well this method willwork for English, but with appropriate finite state patterns,similar esults may be achieved.
(owner) (take office/as)A T T ~' 6 /~X(AT&T) (introduceA~rom)~$\ [ \ ]  ~: /~(melropolitan) (build/at)(personnel) (dispatch/-acc)(Commitee) (unite/with)(library) (sell/-nom)(Company) (organize/-acc)~r~ ~/~(agency) (publish/-nom)(post office) (tie-up/with)~t ~: /~(State) (developAo)(Cannon) (enter/-acc)~ ~: /~(doctor's office) (limit/to)(nations) (haveAn)~ ~/~(Nomura) (prroduce/-nom)~ ~/~(station employee) (take office/-nom)D R A M ~ / ~(DRAM) (unite/-nom)(Switzerland) (see/-nom)~ ~: /~(director) (announce/to)Table 2: Examples of reasonable hypothesized co-occurrencesACKNOWLEDGMENTSThe author wishes to thank Madeleine Bates, RalphWeischedel and Sean Boisen for significant contributions tothis paper.2521.2.3.4.5.6.7.8.9.10.REFERENCESBrent, M.R., (1991) "Automatic Acquisition ofSubcategorization Frames from Untagged Text,"Proceedings of the 29th annual Meeting of the ACL,pp.
209-214.Brown, P.F., et.
al., (1992) "Class-based N-gramModels of Natural Language," ComputationalLinguistics Vol.
18 (4), pp.
467-479.Church, K. and Hanks, P., (1990) "Word AssociationNorms, Mutual Information, and Lexicography,"Computational Linguistics Vol.
16 (1), pp.22-29.Hindle, D., (1990) "Noun Classification fromPredicate-Argument S ructures," Proceedings of the28th Annual Meeting of the ACL, pp.
268-275.Hoel P. G., (1971): Introduction to MathematicalStatistics, Chapter 9.
2.Resnik, P., (1992) "A Class-based Approach to LexicalDiscovery," Proceedings of the 30th Annual Meetingof the ACL, pp.
327-329.Smadja F.A.
and McKeown, K.R., (1990)"Automatically Extracting and RepresentingCollocations for Language Generation," Proceedingsof the 28th Annual Meeting of the ACL, pp.
252-259.Matsukawa T., Miller S. and Weischedel R. (1993)"Example-based Correction of Word Segmentation andPart of Speech Labelling," Proceedings of DARPAHuman Language Technologies Workshop.Matsukawa, T. and Yokota, E. (1991) "Developmentof the Concept Dictionary - Implementation f LexicalKnowledge," Proc.
of pre-conference workshopsponsored by the special Interest Group on the Lexicon(SIGLEX) of the Association for ComputationalLinguistics, 1991.Weischedel, R. et al (1991) "Partial Parsing: AReport on Work in Progress," Proceedings of theWorkshop on Speech and Natural Language, pp.
204-210.APPENDIX:  JUST IF ICAT ION OF CHISQUAREChi-square score is given by the following formula :I(X, Y)= ~ I(X, Y)p(X, Y) E p(X, Y" Io = / gp-~-p~)  (0)where .,~ Y= columns and rows of a word co-occurrencematrixX, Y = subsets of X~ Y, respectively(i.e.
word classes at the columns and the rows)This can be justified as follows.According to \[Hoel 1971\], the likelihood ratio LAMBDAfor a test of the hypothesis: p(i) = po(i) (i = 1, 2 .
.
.
.
.
k),where p(i) is the probabil ity of case i and po(i) is ahypothesized probability of it, when observations areindependent of each other, is given as:k , n(i) -2 log LAMBDA 2 ~ n(i) (1) = log~i=lwhere n(i) is the number of observations of case i, and e(i)is its expectation, i.e., e(i) = n p(i), where n is the totalnumber of observations.The distribution is chi-square when n is large.
I f  weassume two word classes, ci and cj, occur independently,then the expected value of the probability of their co-occurrence will be,e(ci, c j )=  n p(ci) p(cj) (2)where p(ci) and p(cj) are estimations of the probability ofoccurrence of ci and cj.
The maximum likelihood estimateof p(ci) and p(cj) is f(ci)/n and f(cj)/n, where f(cj) and f(cj)are the number of observations of words classified in ci andcj.
The maximum likelihood estimate of p(ci, cj), theprobability of the co-occurrences of words in ci and cj, isf(ci, cj)/n, where f(ci, cj) is the number of observations ofthe co-occurrences.
Then the number of the co-occurrencesn(ci, cj) (which is the same as f(ci, cj) ) can be representedas,n(ci, cj)= n p(ci, cj) (3)Therefore, given k classes, cl ,  c2 ..... ck, substituting (2)and (3) into (1).k i p(ci, c j) 2 ~ ~ np(ci, cj)logi=0 j=0 p(~i) }~(-~j ) (4)If  n is large, this will have a chi-square distribution;therefore, we can estimate how unlikely our assumption ofindependence among word classes is.
Since formula (4)gives a scaled average mutual information among the wordclasses, searching for a partition of words that providesmaximum average mutual information among word classesis equivalent to seeking classes where independence amongword classes is minimally likely.
The algorithm reportedin this paper searches for pairs of word classes whichprovide a local max imum I(X, Y), a term in thesummation of formula (0).253
