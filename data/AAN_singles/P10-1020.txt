Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 186?195,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsEntity-based local coherence modelling using topological fieldsJackie Chi Kit Cheung and Gerald PennDepartment of Computer ScienceUniversity of TorontoToronto, ON, M5S 3G4, Canada{jcheung,gpenn}@cs.toronto.eduAbstractOne goal of natural language generation isto produce coherent text that presents in-formation in a logical order.
In this pa-per, we show that topological fields, whichmodel high-level clausal structure, are animportant component of local coherencein German.
First, we show in a sen-tence ordering experiment that topologi-cal field information improves the entitygrid model of Barzilay and Lapata (2008)more than grammatical role and simpleclausal order information do, particularlywhen manual annotations of this informa-tion are not available.
Then, we incor-porate the model enhanced with topolog-ical fields into a natural language gen-eration system that generates constituentorders for German text, and show thatthe added coherence component improvesperformance slightly, though not statisti-cally significantly.1 IntroductionOne type of coherence modelling that has capturedrecent research interest is local coherence mod-elling, which measures the coherence of a docu-ment by examining the similarity between neigh-bouring text spans.
The entity-based approach,in particular, considers the occurrences of nounphrase entities in a document (Barzilay and Lap-ata, 2008).
Local coherence modelling has beenshown to be useful for tasks like natural languagegeneration and summarization, (Barzilay and Lee,2004) and genre classification (Barzilay and Lap-ata, 2008).Previous work on English, a language with rel-atively fixed word order, has identified factors thatcontribute to local coherence, such as the gram-matical roles associated with the entities.
There isgood reason to believe that the importance of thesefactors vary across languages.
For instance, freer-word-order languages exhibit word order patternswhich are dependent on discourse factors relatingto information structure, in addition to the gram-matical roles of nominal arguments of the mainverb.
We thus expect word order information to beparticularly important in these languages in dis-course analysis, which includes coherence mod-elling.For example, Strube and Hahn (1999) introduceFunctional Centering, a variant of Centering The-ory which utilizes information status distinctionsbetween hearer-old and hearer-new entities.
Theyapply their model to pronominal anaphora reso-lution, identifying potential antecedents of sub-sequent anaphora by considering syntactic andword order information, classifying constituentsby their familiarity to the reader.
They find thattheir approach correctly resolves more pronomi-nal anaphora than a grammatical role-based ap-proach which ignores word order, and the differ-ence between the two approaches is larger in Ger-man corpora than in English ones.
Unfortunately,their criteria for ranking potential antecedents re-quire complex syntactic information in order toclassify whether proper names are known to thehearer, which makes their algorithm hard to auto-mate.
Indeed, all evaluation is done manually.We instead use topological fields, a model ofclausal structure which is indicative of informationstructure in German, but shallow enough to be au-tomatically parsed at high accuracy.
We test thehypothesis that they would provide a good com-plement or alternative to grammatical roles in lo-cal coherence modelling.
We show that they aresuperior to grammatical roles in a sentence or-dering experiment, and in fact outperforms sim-ple word-order information as well.
We furthershow that these differences are particularly largewhen manual syntactic and grammatical role an-186Millionen von Mark   verschwendet   der Senat jeden Monat,   weil   er   sparen will.LK MF VCVF LK MFSNFS?The senate wastes millions of marks each month, because it wants to save.
?Figure 1: The clausal and topological field structure of a German sentence.
Notice that the subordinateclause receives its own topology.notations are not available.We then embed these topological field annota-tions into a natural language generation system toshow the utility of local coherence information inan applied setting.
We add contextual featuresusing topological field transitions to the modelof Filippova and Strube (2007b) and achieve aslight improvement over their model in a con-stituent ordering task, though not statistically sig-nificantly.
We conclude by discussing possiblereasons for the utility of topological fields in lo-cal coherence modelling.2 Background and Related Work2.1 German Topological Field ParsingTopological fields are sequences of one or morecontiguous phrases found in an enclosing syntac-tic region, which is the clause in the case of theGerman topological field model (Ho?hle, 1983).These fields may have constraints on the numberof words or phrases they contain, and do not nec-essarily form a semantically coherent constituent.In German, the topology serves to identify all ofthe components of the verbal head of a clause, aswell as clause-level structure such as complemen-tizers and subordinating conjunctions.
Topologi-cal fields are a useful abstraction of word order,because while Germanic word order is relativelyfree with respect to grammatical functions, the or-der of the topological fields is strict and unvarying.A German clause can be considered to be an-chored by two ?brackets?
which contain modals,verbs and complementizers.
The left bracket (linkeKlammer, LK) may contain a complementizer,subordinating conjunction, or a finite verb, de-pending on the clause type, and the right bracketcontains the verbal complex (VC).
The other topo-logical fields are defined in relation to these twobrackets, and contain all other parts of the clausesuch as verbal arguments, adjuncts, and discoursecues.The VF (Vorfeld or ?pre-field?)
is so-named be-cause it occurs before the left bracket.
As the firstconstituent of most matrix clauses in declarativesentences, it has special significance for the coher-ence of a passage, which we will further discussbelow.
The MF (Mittelfeld or ?middle field?)
isthe field bounded by the two brackets.
Most verbarguments, adverbs, and prepositional phrases arefound here, unless they have been fronted and putin the VF, or are prosodically heavy and postposedto the NF field.
The NF (Nachfeld or ?post-field?
)contains prosodically heavy elements such as post-posed prepositional phrases or relative clauses,and occasionally postposed noun phrases.2.2 The Role of the VorfeldOne of the reasons that we use topological fieldsfor local coherence modelling is the role that theVF plays in signalling the information structure ofGerman clauses, as it often contains the topic ofthe sentence.In fact, its role is much more complex than be-ing simply the topic position.
Dipper and Zins-meister (2009) distinguish multiple uses of the VFdepending on whether it contains an element re-lated to the surrounding discourse.
They find that45.1% of VFs are clearly related to the previouscontext by a reference or discourse relation, and afurther 21.9% are deictic and refer to the situationdescribed in the passage in a corpus study.
Theyalso run a sentence insertion experiment wheresubjects are asked to place an extracted sentencein its original location in a passage.
The authorsremark that extracted sentences with VFs that arereferentially related to previous context (e.g., theycontain a coreferential noun phrase or a discourserelation like ?therefore?)
are reinserted at higheraccuracies.187a)# Original Sentence and Translation1Einen Zufluchtsort fu?r Frauen, die von ihren Ma?nnern mi?handelt werden, gibt es nunmehr auchin Treptow.
?There is now a sanctuary for women who are mistreated by their husbands in Treptow as well.
?2Das Bezirksamt bietet Frauen (auch mit Kindern) in derartigen Notsituationen voru?bergehendeine Unterkunft.
?The district office offers women (even with children) in this type of emergency temporaryaccommodation.
?3Zugleich werden die Betroffenen der Regelung des Unterhalts, bei Beho?rdenga?ngen und auchbei der Wohnungssuche unterstu?tzt.
?At the same time, the affected are supported with provisions of necessities, in dealing withauthorities, and also in the search for new accommodations.
?b)DE Zufluchtsort Frauen Ma?nnern Treptow KindernEN sanctuary women husbands Treptow children1 acc oth oth oth ?2 ?
oth ?
?
oth3 ?
nom ?
?
?c)?
?
?
nom ?
acc ?
oth nom ?
nom nom nom acc nom oth0.3 0.0 0.0 0.1 0.0 0.0 0.0 0.0acc ?
acc nom acc acc acc oth oth ?
oth nom oth acc oth oth0.1 0.0 0.0 0.0 0.3 0.1 0.0 0.1Table 1: a) An example of a document from Tu?Ba-D/Z, b) an abbreviated entity grid representation ofit, and c) the feature vector representation of the abbreviated entity grid for transitions of length two.Mentions of the entity Frauen are underlined.
nom: nominative, acc: accusative, oth: dative, oblique,and other argumentsFilippova and Strube (2007c) also examine therole of the VF in local coherence and natural lan-guage generation, focusing on the correlation be-tween VFs and sentential topics.
They follow Ja-cobs (2001) in distinguishing the topic of addres-sation, which is the constituent for which theproposition holds, and frame-setting topics, whichis the domain in which the proposition holds, suchas a temporal expression.
They show in a userstudy that frame-setting topics are preferred to top-ics of addressation in the VF, except when a con-stituent needs to be established as the topic of ad-dressation.2.3 Using Entity Grids to Model LocalCoherenceBarzilay and Lapata (2008) introduce the entitygrid as a method of representing the coherence of adocument.
Entity grids indicate the location of theoccurrences of an entity in a document, which isimportant for coherence modelling because men-tions of an entity tend to appear in clusters ofneighbouring or nearby sentences in coherent doc-uments.
This last assumption is adapted from Cen-tering Theory approaches to discourse modelling.In Barzilay and Lapata (2008), an entity grid isconstructed for each document, and is representedas a matrix in which each row represents a sen-tence, and each column represents an entity.
Thus,a cell in the matrix contains information about anentity in a sentence.
The cell is marked by thepresence or absence of the entity, and can also beaugmented with other information about the en-tity in this sentence, such as the grammatical roleof the noun phrase representing that entity in thatsentence, or the topological field in which the nounphrase appears.Consider the document in Table 1.
An entitygrid representation which incorporates the syntac-tic role of the noun phrase in which the entity ap-188pears is also shown (not all entities are listed forbrevity).
We tabulate the transitions of entities be-tween different syntactic positions (or their non-occurrence) in sentences, and convert the frequen-cies of transitions into a feature vector representa-tion of transition probabilities in the document.To calculate transition probabilities, we dividethe frequency of a particular transition by the totalnumber of transitions of that length.This model of local coherence was investigatedfor German by Filippova and Strube (2007a).
Themain focus of that work, however, was to adaptthe model for use in a low-resource situation whenperfect coreference information is not available.This is particularly useful in natural language un-derstanding tasks.
They employ a semantic clus-tering model to relate entities.
In contrast, ourwork focuses on improving performance by anno-tating entities with additional linguistic informa-tion, such as topological fields, and is geared to-wards natural language generation systems whereperfect information is available.Similar models of local coherence include vari-ous Centering Theory accounts of local coherence((Kibble and Power, 2004; Poesio et al, 2004)inter alia).
The model of Elsner and Charniak(2007) uses syntactic cues to model the discourse-newness of noun phrases.
There are also moreglobal content models of topic shifts between sen-tences like Barzilay and Lee (2004).3 Sentence Ordering Experiments3.1 MethodWe test a version of the entity grid representa-tion augmented with topological fields in a sen-tence ordering experiment corresponding to Ex-periment 1 of Barzilay and Lapata (2008).
Thetask is a binary classification task to identify theoriginal version of a document from another ver-sion which contains the sentences in a randomlypermuted order, which is taken to be incoherent.We solve this problem in a supervised machinelearning setting, where the input is the feature vec-tor representations of the two versions of the doc-ument, and the output is a binary value indicatingthe document with the original sentence ordering.We use SVMlight?s ranking module for classifi-cation (Joachims, 2002).The corpus in our experiments consists of thelast 480 documents of Tu?Ba-D/Z version 4 (Telljo-hann et al, 2004), which contains manual corefer-ence, grammatical role and topological field infor-mation.
This set is larger than the set that was usedin Experiment 1 of Barzilay and Lapata (2008),which consists of 400 documents in two Englishsubcorpora on earthquakes and accidents respec-tively.
The average document length in the Tu?Ba-D/Z subcorpus is also greater, at 19.2 sentencescompared to about 11 for the two subcorpora.
Upto 20 random permutations of sentences were gen-erated from each document, with duplicates re-moved.There are 216 documents and 4126 original-permutation pairs in the training set, and 24 docu-ments and 465 pairs in the development set.
Theremaining 240 documents are in the final test set(4243 pairs).
The entity-based model is parame-terized as follows.Transition length ?
the maximum length of thetransitions used in the feature vector representa-tion of a document.Representation ?
when marking the presence ofan entity in a sentence, what information aboutthe entity is marked (topological field, grammat-ical role, or none).
We will describe the represen-tations that we try in section 3.2.Salience ?
whether to set a threshold for the fre-quency of occurrence of entities.
If this is set, allentities below a certain frequency are treated sep-arately from those reaching this frequency thresh-old when calculating transition probabilities.
Inthe example in Table 1, with a salience thresh-old of 2, Frauen would be treated separately fromMa?nnern or Kindern.Transition length, salience, and a regularizationparameter are tuned on the development set.
Weonly report results using the setting of transitionlength ?
4, and no salience threshold, becausethey give the best performance on the developmentset.
This is in contrast to the findings of Barzi-lay and Lapata (2008), who report that transitionlength ?
3 and a salience threshold of 2 performbest on their data.3.2 Entity RepresentationsThe main goal of this study is to compare wordorder, grammatical role and topological field in-formation, which is encoded into the entity grid ateach occurrence of an entity.
Here, we describethe variants of the entity representations that wecompare.189Baseline Representations We implement sev-eral baseline representations against which we testour topological field-enhanced model.
The sim-plest baseline representation marks the mere ap-pearance of an entity without any additional infor-mation, which we refer to as default.Another class of baseline representations markthe order in which entities appear in the clause.The correlation between word order and informa-tion structure is well known, and has formed thebasis of some theories of syntax such as the PragueSchool?s (Sgall et al, 1986).
The two versionsof clausal order we tried are order 1/2/3+,which marks a noun phrase as the first, the sec-ond, or the third or later to appear in a clause, andorder 1/2+, which marks a noun phrase as thefirst, or the second or later to appear in a clause.Since noun phrases can be embedded in othernoun phrases, overlaps can occur.
In this case, thedominating noun phrase takes the smallest ordernumber among its dominated noun phrases.The third class of baseline representations weemploy mark an entity by its grammatical rolein the clause.
Barzilay and Lapata (2008) foundthat grammatical role improves performance inthis task for an English corpus.
Because Ger-man distinguishes more grammatical roles mor-phologically than English, we experiment withvarious granularities of role labelling.
In particu-lar, subj/obj distinguishes the subject position,the object position, and another category for allother positions.
cases distinguishes five types ofentities corresponding to the four morphologicalcases of German in addition to another categoryfor noun phrases which are not complements ofthe main verb.Topological Field-Based These representationsmark the topological field in which an entity ap-pears.
Some versions mark entities which areprepositional objects separately.
We try versionswhich distinguish VF from non-VF, as well asmore general versions that make use of a greaterset of topological fields.
vfmarks the noun phraseas belonging to a VF (and not in a PP) or not.vfpp is the same as above, but allows preposi-tional objects inside the VF to be marked as VF.topf/pp distinguishes entities in the topologicalfields VF, MF, and NF, contains a separate cat-egory for PP, and a category for all other nounphrases.
topf distinguishes between VF, MF, andNF, on the one hand, and everything else on theother.
Prepositional objects are treated the sameas other noun phrases here.Combined We tried a representation whichcombines grammatical role and topological fieldinto a single representation, subj/obj?vf,which takes the Cartesian product of subj/objand vf above.Topological fields do not map directly to topic-focus distinctions.
For example, besides the topicof the sentence, the Vorfeld may contain discoursecues, expletive pronouns, or the informational orcontrastive focus.
Furthermore, there are addi-tional constraints on constituent order related topronominalization.
Thus, we devised additionalentity representations to account for these aspectsof German.topic attempts to identify the sentential topicof a clause.
A noun phrase is marked as TOPICif it is in VF as in vfpp, or if it is the firstnoun phrase in MF and also the first NP in theclause.
Other noun phrases in MF are markedas NONTOPIC.
Categories for NF and miscella-neous noun phrases also exist.
While this repre-sentation may appear to be very similar to sim-ply distinguishing the first entity in a clause as fororder 1/2+ in that TOPIC would correspondto the first entity in the clause, they are in fact dis-tinct.
Due to issues related to coordination, appos-itive constructions, and fragments which do notreceive a topology of fields, the first entity in aclause is labelled the TOPIC only 80.8% of thetime in the corpus.
This representation also distin-guishes NFs, which clausal order does not.topic+pron refines the above by taking intoaccount a word order restriction in German thatpronouns appear before full noun phrases in theMF field.
The following set of decisions repre-sents how a noun phrase is marked: If the first NPin the clause is a pronoun in an MF field and is thesubject, we mark it as TOPIC.
If it is not the sub-ject, we mark it as NONTOPIC.
For other NPs, wefollow the topic representation.3.3 Automatic annotationsWhile it is reasonable to assume perfect annota-tions of topological fields and grammatical roles inmany NLG contexts, this assumption may be lessappropriate in other applications involving text-to-text generation where the input to the system istext such as paraphrasing or machine translation.Thus, we test the robustness of the entity repre-190Representation Manual Automatictopf/pp 94.44 94.89topic 94.13 94.53topic+pron 94.08 94.51topf 93.87 93.11subj/obj 93.831 91.7++cases 93.312 90.93++order 1/2+ 92.51++ 92.1+subj/obj?vf 92.32++ 90.74++default 91.42++ 91.42++vfpp 91.37++ 91.68++vf 91.21++ 91.16++order 1/2/3+ 91.16++ 90.71++Table 2: Accuracy (%) of the permutation de-tection experiment with various entity represen-tations using manual and automatic annotationsof topological fields and grammatical roles.
Thebaseline without any additional annotation is un-derlined.
Two-tailed sign tests were calculated foreach result against the best performing model ineach column (1: p = 0.101; 2: p = 0.053; +: statis-tically significant, p < 0.05; ++: very statisticallysignificant, p < 0.01 ).sentations to automatic extraction in the absenceof manual annotations.
We employ the followingtwo systems for extracting topological fields andgrammatical roles.To parse topological fields, we use the Berke-ley parser of Petrov and Klein (2007), which hasbeen shown to perform well at this task (Cheungand Penn, 2009).
The parser is trained on sectionsof Tu?Ba-D/Z which do not overlap with the sec-tion from which the documents for this experimentwere drawn, and obtains an overall parsing per-formance of 93.35% F1 on topological fields andclausal nodes without gold POS tags on the sectionof Tu?Ba-D/Z it was tested on.We tried two methods to obtain grammaticalroles.
First, we tried extracting grammatical rolesfrom the parse trees which we obtained from theBerkeley parser, as this information is present inthe edge labels that can be recovered from theparse.
However, we found that we achieved bet-ter accuracy by using RFTagger (Schmid andLaws, 2008), which tags nouns with their morpho-logical case.
Morphological case is distinct fromgrammatical role, as noun phrases can function asadjuncts in possessive constructions and preposi-Annotation Accuracy (%)Grammatical role 83.6Topological field (+PP) 93.8Topological field (?PP) 95.7Clausal order 90.8Table 3: Accuracy of automatic annotations ofnoun phrases with coreferents.
+PP means thatprepositional objects are treated as a separate cate-gory from topological fields.
?PP means they aretreated as other noun phrases.tional phrases.
However, we can approximate thegrammatical role of an entity using the morpho-logical case.
We follow the annotation conven-tions of Tu?Ba-D/Z in not assigning a grammati-cal role when the noun phrase is a prepositionalobject.
We also do not assign a grammatical rolewhen the noun phrase is in the genitive case, asgenitive objects are very rare in German and arefar outnumbered by the possessive genitive con-struction.3.4 ResultsTable 2 shows the results of the sentence orderingpermutation detection experiment.
The top fourperforming entity representations are all topologi-cal field-based, and they outperform grammaticalrole-based and simple clausal order-based mod-els.
These results indicate that the informationthat topological fields provide about clause struc-ture, appositives, right dislocation, etc.
which isnot captured by simple clausal order is importantfor coherence modelling.
The representations in-corporating linguistics-based heuristics do not out-perform purely topological field-based models.Surprisingly, the VF-based models fare quitepoorly, performing worse than not adding any an-notations, despite the fact that topological field-based models in general perform well.
This resultmay be a result of the heterogeneous uses of theVF.The automatic topological field annotations aremore accurate than the automatic grammatical roleannotations (Table 3), which may partly explainwhy grammatical role-based models suffer morewhen using automatic annotations.
Note, how-ever, that the models based on automatic topolog-ical field annotations outperform even the gram-matical role-based models using manual annota-tion (at marginal significance, p < 0.1).
The topo-191logical field annotations are accurate enough thatautomatic annotations produce no decrease in per-formance.These results show the upper bound of entity-based local coherence modelling with perfectcoreference information.
The results we obtainare higher than the results for the English cor-pora of Barzilay and Lapata (2008) (87.2% on theEarthquakes corpus and 90.4% on the Accidentscorpus), but this is probably due to corpus differ-ences as well as the availability of perfect corefer-ence information in our experiments1.Due to the high performance we obtained, wecalculated Kendall tau coefficients (Lapata, 2006)over the sentence orderings of the cases in whichour best performing model is incorrect, to deter-mine whether the remaining errors are instanceswhere the permuted ordering is nearly identical tothe original ordering.
We obtained a ?
of 0.0456in these cases, compared to a ?
of ?0.0084 for allthe pairs, indicating that this is not the case.To facilitate comparison to the results of Filip-pova and Strube (2007a), we rerun this experimenton the same subsections of the corpus as in thatwork for training and testing.
The first 100 arti-cles of Tu?Ba-D/Z are used for testing, while thenext 200 are used for training and development.Unlike the previous experiments, we do not doparameter tuning on this set of data.
Instead, wefollow Filippova and Strube (2007a) in using tran-sition lengths of up to three.
We do not put ina salience threshold.
We see that our results aremuch better than the ones reported in that work,even for the default representation.
The mainreason for this discrepancy is probably the waythat entities are created from the corpus.
In ourexperiments, we create an entity for every singlenoun phrase node that we encounter, then mergethe entities that are linked by coreference.
Filip-pova and Strube (2007a) convert the annotationsof Tu?Ba-D/Z into a dependency format, then ex-tract entities from the noun phrases found there.They may thus annotate fewer entities, as there1Barzilay and Lapata (2008) use the coreference sys-tem of Ng and Cardie (2002) to obtain coreference anno-tations.
We are not aware of similarly well-tested, pub-licly available coreference resolution systems that handle alltypes of anaphora for German.
We considered adapting theBART coreference resolution toolkit (Versley et al, 2008) toGerman, but a number of language-dependent decisions re-garding preprocessing, feature engineering, and the learningparadigm would need to be made in order to achieve rea-sonable performance comparable to state-of-the-art Englishcoreference resolution systems.Representation Accuracy (%)topf/pp 93.83topic 93.31topic+pron 93.31topf 92.49subj/obj 88.99order 1/2+ 88.89order 1/2/3+ 88.84cases 88.63vf 87.60vfpp 88.17default 87.55subj/obj?vf 87.71(Filippova and Strube, 2007) 75Table 4: Accuracy (%) of permutation detectionexperiment with various entity representations us-ing manual and automatic annotations of topolog-ical fields and grammatical roles on subset of cor-pus used by Filippova and Strube (2007a).may be nested NP nodes in the original corpus.There may also be noise in the dependency con-version process.The relative rankings of different entity repre-sentations in this experiment are similar to therankings of the previous experiment, with topolog-ical field-based models outperforming grammati-cal role and clausal order models.4 Local Coherence for Natural LanguageGenerationOne of the motivations of the entity grid-basedmodel is to improve surface realization decisionsin NLG systems.
A typical experimental designwould pass the contents of the test section of acorpus as input to the NLG system with the order-ing information stripped away.
The task is then toregenerate the ordering of the information foundin the original corpus.
Various coherence modelshave been tested in corpus-based NLG settings.For example, Karamanis et al (2009) compareseveral versions of Centering Theory-based met-rics of coherence on corpora by examining howhighly the original ordering found in the corpusis ranked compared to other possible orderings ofpropositions.
A metric performs well if it ranksthe original ordering better than the alternative or-derings.In our next experiment, we incorporate local co-192herence information into the system of Filippovaand Strube (2007b).
We embed entity topologi-cal field transitions into their probabilistic model,and show that the added coherence componentslightly improves the performance of the baselineNLG system in generating constituent orderings ina German corpus, though not to a statistically sig-nificant degree.4.1 MethodWe use the WikiBiography corpus2 for our exper-iments.
The corpus consists of more than 1100 bi-ographies taken from the German Wikipedia, andcontains automatic annotations of morphological,syntactic, and semantic information.
Each articlealso contains the coreference chain of the subjectof the biography (the biographee).
The first 100articles are used for testing, the next 200 for de-velopment, and the rest for training.The baseline generation system already incor-porates topological field information into the con-stituent ordering process.
The system operates intwo steps.
First, in main clauses, one constituentis selected as the Vorfeld (VF).
This is done us-ing a maximum entropy model (call it MAXENT).Then, the remaining constituents are ordered usinga second maximum entropy model (MAXENT2).Significantly, Filippova and Strube (2007b) foundthat selecting the VF first, and then ordering theremaining constituents results in a 9% absoluteimprovement over the corresponding model wherethe selection is performed in one step by the sort-ing algorithm alone.The maximum entropy model for both steps relyon the following features:?
features on the voice, valency, and identity ofthe main verb of the clause?
features on the morphological and syntacticstatus of the constituent to be ordered?
whether the constituent occurs in the preced-ing sentence?
features for whether the constituent containsa determiner, an anaphoric pronoun, or a rel-ative clause?
the size of the constituent in number of mod-ifiers, in depth, and in number of words2http://www.eml-research.de/english/research/nlp/download/wikibiography.php?
the semantic class of the constituent (per-son, temporal, location, etc.)
The biographee,in particular, is marked by its own semanticclass.In the first VF selection step, MAXENT simplyproduces a probability of each constituent being aVF, and the constituent with the highest probabil-ity is selected.
In the second step, MAXENT2 takesthe featural representation of two constituents, andproduces an output probability of the first con-stituent preceding the second constituent.
The fi-nal ordering is achieved by first randomizing theorder of the constituents in a clause (besides thefirst one, which is selected to be the VF), thensorting them according to the precedence proba-bilities.
Specifically, a constituent A is put beforea constituent B if MAXENT2(A,B) > 0.5.
Becausethis precedence relation is not antisymmetric (i.e.,MAXENT2(A,B) > 0.5 and MAXENT2(B,A) >0.5 may be simultaneously true or simultaneouslyfalse), different initializations of the order pro-duce different sorted results.
In our experiments,we correct this by defining the precedence rela-tion to be A precedes B iff MAXENT2(A,B) >MAXENT2(B,A).
This change does not greatly im-pact the performance, and removes the random-ized element of the algorithm.The baseline system does not directly model thecontext when ordering constituents.
All of thefeatures but one in the original maximum entropymodels rely on local properties of the clause.
Weincorporate local coherence information into themodel by adding entity transition features whichwe found to be useful in the sentence ordering ex-periment in Section 3 above.Specifically, we add features indicating thetopological fields in which entities occur in theprevious sentences.
We found that looking backup to two sentences produces the best results (bytuning on the development set).
Because this cor-pus does not come with general coreference in-formation except for the coreference chain of thebiographee, we use the semantic classes instead.So, all constituents in the same semantic class aretreated as one coreference chain.
An example of afeature may be biog-last2, which takes on a valuesuch as ?v?
?, meaning that this constituent refersto the biographee, and the biographee occurs inthe VF two clauses ago (v), but does not appear inthe previous clause (?).
For a constituent which isnot the biographee, this feature would be marked193Method VF Acc (%) Acc (%) TauBaseline 68.7 60.9 0.72+Coherence 69.2 61.5 0.72Table 5: Results of adding coherence features intoa natural language generation system.
VF Acc%is the accuracy of selecting the first constituent inmain clauses.
Acc % is the percentage of per-fectly ordered clauses, tau is Kendall?s ?
on theconstituent ordering.
The test set contains 2246clauses, of which 1662 are main clauses.?na?
(not applicable).4.2 ResultsTable 5 shows the results of adding these contex-tual features into the maximum entropy models.We see that we obtain a small improvement in theaccuracy of VF selection, and in the accuracy ofcorrectly ordering the entire clause.
These im-provements are not statistically significant by Mc-Nemar?s test.
We suggest that the lack of coref-erence information for all entities in the articlemay have reduced the benefit of the coherencecomponent.
Also, the topline of performance issubstantially lower than 100%, as multiple order-ings are possible and equally valid.
Human judge-ments on information structuring for both inter-and intra-sentential units are known to have lowagreement (Barzilay et al, 2002; Filippova andStrube, 2007c; Lapata, 2003; Chen et al, 2007).Thus, the relative error reduction is higher than theabsolute reduction might suggest.5 ConclusionsWe have shown that topological fields are a use-ful source of information for local coherence mod-elling.
In a sentence-order permutation detectiontask, models which use topological field infor-mation outperform both grammatical role-basedmodels and models based on simple clausal or-der, with the best performing model achieving arelative error reduction of 40.4% over the originalbaseline without any additional annotation.
Ap-plying our local coherence model in another set-ting, we have embedded topological field transi-tions of entities into an NLG system which ordersconstituents in German clauses.
We find that thecoherence-enhanced model slightly outperformsthe baseline system, but this was not statisticallysignificant.We suggest that the utility of topological fieldsin local coherence modelling comes from the in-teraction between word order and informationstructure in freer-word-order languages.
Crucially,topological fields take into account issues suchas coordination, appositives, sentential fragmentsand differences in clause types, which word or-der alone does not.
They are also shallow enoughto be accurately parsed automatically for use inresource-poor applications.Further refinement of the topological field an-notations to take advantage of the fact that theydo not correspond neatly to any single informationstatus such as topic or focus could provide addi-tional performance gains.
The model also showspromise for other discourse-related tasks such ascoreference resolution and discourse parsing.AcknowledgementsWe are grateful to Katja Filippova for providing uswith source code for the experiments in Section 4and for answering related questions, and to Tim-othy Fowler for useful discussions and commentson a draft of the paper.
This work is supported inpart by the Natural Sciences and Engineering Re-search Council of Canada.ReferencesR.
Barzilay and M. Lapata.
2008.
Modeling local co-herence: An entity-based approach.
ComputationalLinguistics, 34(1):1?34.R.
Barzilay and L. Lee.
2004.
Catching the drift: Prob-abilistic content models, with applications to gen-eration and summarization.
In Proc.
HLT-NAACL2004, pages 113?120.R.
Barzilay, N. Elhadad, and K. McKeown.
2002.
In-ferring strategies for sentence ordering in multidoc-ument news summarization.
Journal of Artificial In-telligence Research, 17:35?55.E.
Chen, B. Snyder, and R. Barzilay.
2007.
Incremen-tal text structuring with online hierarchical ranking.In Proceedings of EMNLP, pages 83?91.J.C.K.
Cheung and G. Penn.
2009.
Topological FieldParsing of German.
In Proc.
47th ACL and 4th IJC-NLP, pages 64?72.
Association for ComputationalLinguistics.S.
Dipper and H. Zinsmeister.
2009.
The Role ofthe German Vorfeld for Local Coherence: A Pi-lot Study.
In Proceedings of the Conference of theGerman Society for Computational Linguistics andLanguage Technology (GSCL), pages 69?79.
GunterNarr.194M.
Elsner and E. Charniak.
2007.
A generativediscourse-new model for text coherence.
Technicalreport, Technical Report CS-07-04, Brown Univer-sity.K.
Filippova and M. Strube.
2007a.
Extending theentity-grid coherence model to semantically relatedentities.
In Proceedings of the Eleventh EuropeanWorkshop on Natural Language Generation, pages139?142.
Association for Computational Linguis-tics.K.
Filippova and M. Strube.
2007b.
Generating con-stituent order in German clauses.
In Proc.
45th ACL,pages 320?327.K.
Filippova and M. Strube.
2007c.
The German Vor-feld and Local Coherence.
Journal of Logic, Lan-guage and Information, 16(4):465?485.T.N.
Ho?hle.
1983.
Topologische Felder.
Ph.D. thesis,Ko?ln.J.
Jacobs.
2001.
The dimensions of topiccomment.Linguistics, 39(4):641?681.T.
Joachims.
2002.
Learning to Classify Text UsingSupport Vector Machines.
Kluwer.N.
Karamanis, C. Mellish, M. Poesio, and J. Oberlan-der.
2009.
Evaluating centering for information or-dering using corpora.
Computational Linguistics,35(1):29?46.R.
Kibble and R. Power.
2004.
Optimizing referentialcoherence in text generation.
Computational Lin-guistics, 30(4):401?416.M.
Lapata.
2003.
Probabilistic text structuring: Exper-iments with sentence ordering.
In Proc.
41st ACL,pages 545?552.M.
Lapata.
2006.
Automatic evaluation of informationordering: Kendall?s tau.
Computational Linguistics,32(4):471?484.V.
Ng and C. Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In Proc.40th ACL, pages 104?111.S.
Petrov and D. Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, pages 404?411.M.
Poesio, R. Stevenson, B.D.
Eugenio, and J. Hitze-man.
2004.
Centering: A parametric theoryand its instantiations.
Computational Linguistics,30(3):309?363.H.
Schmid and F. Laws.
2008.
Estimation of condi-tional probabilities with decision trees and an appli-cation to fine-grained POS tagging.
In Proc.
22ndCOLING, pages 777?784.
Association for Compu-tational Linguistics.P.
Sgall, E.
Hajic?ova?, J.
Panevova?, and J. Mey.
1986.The meaning of the sentence in its semantic andpragmatic aspects.
Springer.M.
Strube and U. Hahn.
1999.
Functional center-ing: Grounding referential coherence in informationstructure.
Computational Linguistics, 25(3):309?344.H.
Telljohann, E. Hinrichs, and S. Kubler.
2004.The Tu?Ba-D/Z treebank: Annotating German witha context-free backbone.
In Proc.
Fourth Interna-tional Conference on Language Resources and Eval-uation (LREC 2004), pages 2229?2235.Y.
Versley, S.P.
Ponzetto, M. Poesio, V. Eidelman,A.
Jern, J. Smith, X. Yang, and A. Moschitti.
2008.BART: A modular toolkit for coreference resolution.In Proc.
46th ACL-HLT Demo Session, pages 9?12.Association for Computational Linguistics.195
