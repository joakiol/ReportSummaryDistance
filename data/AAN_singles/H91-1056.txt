LEXICAL ACCESS WITH ASTAT IST ICALLY-DERIVED PHONETIC  NETWORKMichael D. Riley and Andrej LjoljeAT&T Bell LaboratoriesMurray Hill, NJ 07974ABSTRACTA probabilistic approach to lexieal access from a recognized phonesequence is presented.
Lexical access is seen as finding the word se-quence that maximizes the lexical likelihood of a sequence of phonesand durations as recognized by a phone recognizer.
This is theoreticallycorrect for minimum error rate recognition within the model presentedand is intuitively pleasing since it means that the "confusion matrix" ofthe phone recognizer will be learned and its regularities exploited.
Thelexical likelihoods are estimated from training data provided by thephone recognizer using statistical decision trees.
Classification treesare used to estimate the phone realiziation distributions and regressiontrees are used to estimate the phone duration distributions, We findthey can capture effectively allophonic variation, alternative pronun-ciation, word co-articulation and segmental durations.
We describe asimpified, but efficient implementation f these models to lexical accessin the DARPA resource management recognitiion task.1.
INTRODUCTIONWe describe a new approach to lexical access in a phone-based speech recognition system.
By "lexical access" we meantaking a sequence (or, more generally, a lattice) of phones anddurations that is output by a phone recognizer and mapping itonto a word sequence (or, more generally, a lattice).In conventional word-based speech recognizers, segmentaldurations, word co-articulation and alternative pronunciationsare usually poorly modelled if at all since the architecture is notconvenient or efficient for exploiting these constraints.Phone-based recognition offers an attractive alternative fromthis point of view.
Our approach will be to create a probabilisticmodel that provides the likelihood that a particular word se-quence gives rise to a particular phone sequence.
This model willtake into account allophonic variation, alternative pronunciation,word co-articulation and segmental durations.We then combine these lexical ikelihoods with the acousticlikelihoods generated by the phone recognizer and priors fromour language model to get an overall recognition model whoseerror rate we seek to minimize.We have taken this stochastic approach for two reasons.First, it provides a principled way to combine seemingly disparateinformation: (a) acoustic likelihoods, (b) segmental durations,(c) alternative pronunciations, and (d) the language model.
Sec-ond, the availability of large speech corpora now allow the sta-tistical estimation of these probabilities.2.
PROBABIL IST IC  MODELWe form the probabilistic model as follows.
Let w be asequence of words, let y be a sequence of phones, let d be asequence of durations, and let s be a (fixed) speech signal.
ThenP(wis) ~x ZP(s ly ,d )  P(y ,d \ [w)  P(w).
(2.1)y,dThe lefthand side of this relation is the probability that agiven speech signal corresponds to a particular word sequence.The word sequence that maximizes this term gives the minimumsentence rror rate.
The first factor on the righthand side givesthe acoustic likelihoods provided by the phone recognizer.
Thesecond factor gives the lexical likelihoods to be provided by thelexlcal access stage describe here.
The third factor representswhatever language model we use.In this paper, we have used the output of the current BellLabs phone recognizer as input to the lexical access component\[1\].
At present, this recognizer outputs a single sequence ofphones and durations per utterance, which represents its bestestimate of the true sequence.
As such, y and d are fixed inEq.
2.1 for a given speech signal.
A more general approach,which would consider alternative sequences - phone lattices - iscurrently under investigation, but not reported here.Also in this paper, in which we present results on the DARPAresource management task, we consider only the the simple word-pair language model.
Thus, for a given utterance, the best scoringword sequence, w, will be the one that maximizes the lexical ike-lihood, P(y,  d\[w) for a given phone recognizer output y and d,and which is a legal sequence in the word-pair grammar.
In thismodel, finding the word sequence that maximizes this likelihoodis the goal of lexical access and estimating this likelihood is thegoal of this paper.A crucial factor for this estimation is that y and d are notthe true sequence of phones and durations, but the output of aphone recognizer.
As such, we must train our estimator on theoutput of the phone recognizer.
This is theoretically correct forminimum error rate recognition in tnis model and is intuitivelypleasing since it means that the model will learn the "confusion289matrix" of the phone recognizer and thus exploit its regularities.This combined with our probabilistic model differeniates us fromother approaches to lexical access \[2,3\].We can further decompose this problem by breaking the lex-ical likelihoods into two factors:P(y,  d\[w) = P(y lw)  P(d\ ]w,y)  (2.2).The first factor is the pronunciation model, which gives theprobability of a phone sequence given a word sequence and thesecond factor is the segmental duration model which gives theprobability of a duration sequence given the phone and wordsequences.Given a word sequence we can use a dictionary to look upthe corresponding phoneme sequence \[4\].
We can then replacethe word sequence w in Eq.
2.2 with the phoneme sequence aug-mented with word boundaries and lexical stress with little loss ofinformation.It is important not to confuse phonemes and phones at thispoint.
A phoneme is a coarse description of the pronunciation ofa word as usually found in a dictionary.
A phone gives a finerdescription indicating how the speaker uttered a word in context.For example, the / '~/ in  'but te r '  may be pronounced as a flap,\[dx\], or as a released t, \ [ t c l  t \ ]  .
In this paper, we use theT IMITBET symbols, a superset of the ARPABET symbols, forspecifying phones \[5\].
Which phone will be the realization ofthe phoneme/ t / in  this word depends, in part, on the speaker'sdialect and speaking rate.
Nor is the phonetic realization of aphoneme always determinstic; only about 75% of the / t / ' s  ina similar context to 'but to r '  are flapped, estimated from theTIMIT database.
It is precisely the phoneme-to-phone mappingthat comprises the pronunciation model that we are trying togenerate.Let us make this idea precise.
Let y = zlz~...zm be the stringof phonemes of some sentence.
So that we can mark both wordboundaries and stress we augment he phoneme set to include/$ /as  a word boundary marker and split each syllabic phonemeinto an unstressed, a primary stressed, and a secondary stressedversion.
Further, let y = YlYz...Yn be the string of correspondingphones.
We include the phone symbol \[-J to indicate that aphoneme may delete.The most general form of our predictor is J3(y\[x), whereP estimates the probability that the phone sequence y is therealization of the phoneme sequence x.This specifies the probalitity of an entire phone sequencey.
For convenience, we want to decompose this into one phoneprediction at a time.
SinceP(y lx)  = Pn(Y,~Ixyl ...Yn-1 )Pn-1 (Yn-1 IXyl .
'.Yn-2)...Pl(Yl Ix),(2.3)we can restate the problem as finding a suitable predictor,ISk(yklxyl...yk_l), that estimates the probability that yk is thekth phone in the realization, given the phoneme sequence x andthe previous k-1 phones Yl...Yk-1.Eq.
2.3 is more general than necessary since realistically thekth phone wiLl depend only on a few neighboring phonemes andphones.
Suppose that we can place the phoneme and phonestrings into alignment.
In fact, forming a good alignment be-twen phonemes and phones is easy if deletions and insertions arepermitted, using a phonetic feature distance measure and stan-dard string alignment echniques \[6\].
Since we have augmentedthe phone set to include a deletion symbol, the only stumblingblock to such an alignment would be if phones insert.
For themoment, assume that they don't; we wiLl come back to insertionslater.
Thus, under this assumption we can talk about the kthphoneme and its corresponding phone.
We assumepk(yklx Ya...Yk-O = p(yklxk .
.
.
.
.
Zk-lZkXk+l--.gk+rYl.--Yk-1)-(2.4)In other words, ptc is stationary and depends only on the ::t::rneighboring phonemes.If we assume the kth phone does not depend any of the pre-vious phones, we haveP(yklxk .
.
.
.
.
Xk-lXk~k+l...;TkTrYl'..~tk-1)= p(vklz~ .
.
.
.
.
zk-lxkzk+l...z~+,) (2.5)This is the assumption that phones are conditionally independentgiven the phonemic ontext.To handle phone insertions, we add a second model thatpredicts the phone insertions.
Consider a phone sequencezoylzly2z2...y,~zn that is the reaLization of phoneme sequencexlx2...xn.
We view phone Yi as the realization of phoneme xland view phone zi as an insertion between phoneme yi and yi+l.3.
DECIS ION TREESWe now discuss the question of how, in general, we can es-timate the likelihoods in Eq.
2.2.
We stated in the introductionthat we intend to estimate them directly from training data bystatistical means.
In the DARPA resources management task,we use the output of the phone recognizer run on the trainingset.
Since the phone recognizer is also trained on this same dataset, the phone recognition rate would be much better than onindependent test sets if we did this directly.
Instead, we trainthe phone recognizer on 9/10 of the training set and then run iton the remaining 1/10.
By doing this ten times on the differentportions of the training set, we are able to obtain a more realisticphone training set for lexical access.Given this data, how can we obtain estimates the the pro-nunciation and duration likelihoods in Eq.
2.2?The simplest procedure would be to collect n-gram statisticson the training data.
A bi-phonemic or possibly tri-phonemiccontext would be the largest possible with available training dataif we want statistically reliable estimates.We believe that a straight-forward n-gram statistics on thephonemes are probably not ideal for this problem since the con-textual effects that we are trying to model often depend on awhole class of phonemes in a given position, e.g., whether thepreceding phoneme is a vowel or not.
A procedure that hadall vowels in that position clustered into one class for that casewould produce a more compact description, would be more eas-ily estimated, and would allow a wider effective context to be290examined.Thus intuitively we would like a procedure that pools to-gether contexts that behave similarly, but splits apart onesthat differ.
An attractive choice from this point of view is astatistically-generated decision tree with each branch labelledwith some subset of phonemes for a particular position.
Thetree is generated by spliting nodes that statistical tests, basedon available data, indicate improve prediction, but terminatingnodes otherwise.An excellent description of the theory and implementation ftree-based statistical models can be found in Classification andRegression Trees \[7\].
The interesting questions for generatinga decision tree from data - how to decide which splits to takeand when to label a node terminal and not expand it further -are discussed in these references along with the widely-adoptedsolutions.Suffice it to say here the result is a binary decision tree whosebranches are labelled with binary cuts on the continuous featuresand with binary partitions on the categorical features and whoseterminal nodes are labelled with continuous predictions (regres-sion tree) or categoricM predictions (classil\]cation tree).
By acontinuous feature or prediction we mean a real-valued, linearly-ordered variable (e.g., the duration of a phone, or the number ofphonemes in a word); by a categorical feature or prediction wemean an element of an unordered, finite set.
(e.g., the phonemeset).When categorical predictions are made, the relative proba-bility of each outcome at a node can be directly estimated, andwhen continuous predictions are made, the distribution at a nodecan be para~meterically estimated.
In this way, the trees can serveas estimators of distributions like in Eq.
2.2 and not just as clas-sifters and predictors.We have chosen to use decision trees to form our estima-tors since they (1) relatively efficiently use the available data,(2) are able to handle both categorical and continuous inputsand outputs, (3) are trainable to new corpuses quickly (whichis necessary since we train on the output of a changing phonerecognizer), and (4) generalize well to new test data due to thecross-validation procedure for selecting tree size \[7\].
The use ofdecision trees for these kinds of purposes has already met withsome success \[8-11\].4.
PRONUNCIAT ION MODELIn the exposition in Section 2, we combined word boundaryand stress information into the phoneme set itself.
When weactually input the features into the tree classification procedurewe have found it more convenient to keep them separate.We include ?r  phonemes around the phoneme that is to berealized (r = 2).
This is irrespective of word boundaries.
We padwith blank symbols at sentence start and end.Since there are about 40 different phonemes, if we directlyinput each phoneme into the tree classification routine, 240 pos-sible splits would have to be considered per phoneme posi-tion at each node, since, by default, all possible binary par-titions are considered.
This is clearly intractable, so insteadwe encode each phoneme as a feature vector.
A manage-able choice is to encode each phoneme as a four element vec-tor: (consonant-manner, consonant-place, vowel-manner,vowel -p lace) .
Each component can take one of about a dozenvalues and includes 'n/a' for 'not applicable'.
For exam-p le , /a / i s  encoded as (vo ice less - f r i ca t ive ,  pa la ta l ,  n /a ,n/a) and/ iy / i s  encoded as (n /a ,  n /a ,  y-diphthong,  h igh-front)If the phoneme to be realized is syllabic, then we also inputwhether it has primary or secondary stress or is unstressed.
Weuse stress as predicted by the Bell Labs text-to-speech system;this is essentially lexical stress with function words de-accented.If the phoneme is not syllabic, we input both the stress of thefirst syllabic segment to the left and to the right if present withinthe same word (and use 'n /a 'e '  if not).To encode word boundaries, we input the number ofphonemes from the beginning and end of the current word tothe phoneme that is being realized.Our output set is simply a direct encoding of the 47 elementphone set used in Ljolje\[1\] plus the symbol \[-\] if the phonemedeletes.
Computation time grows only linearly with the numberof output classes so this direct encoding presents no problemsimilar to the exponential growth found with size of the inputfeature dasses.We now describe the results of this model applied to theDARPA resource management database.
The phonetic transcrip-tion for 3838 sentences of the training set produced by our phonerecognizer as described above were aligned with their phone-mic transcription as predicted by the Bell Labs text4o-speechsystem from their orthographic transcription.
For each of theresulting 140168 phonemes, the phonemic context was encodedas described.
A classification tree was grown on this data andthe tree size was chosen to minimize prediction error in a 5-foldcross-validation.
The resulting tree had approximately 300 ter-minal nodes.
The resulting model predicts the phone output bythe recognizer 79.5% of the time (cross-validated), contains the"correct" phone in the top 5 guesses 97% of the time, and has aconditional entropy of 1.1 bits.The corresponding insertion tree predicts whether or not thephone recognizer inserts a phone between phonemes 94.5% of thetime.
This seemingly good prediction is, in fact, quite poor, sincethe mere constant decision "doesn't insert" is correct by almostthe same percentage.
The best cross-validated insertion tree hasonly six terminal nodes, which essentially represents a fixed in-sertion distribution depending little on context.
This reflects thefact that our choice of phone set does not produce many regularinsertions (as it would if stop closure and release were separatephones), and the fact that the phone recognizer apparently doesnot insert spurious phones in a predictable manner.5.
DURATION MODELOur duration model, corresponding to the second factor inEq.
2.2, has a very similar form to the pronunciation model.
Ourprediction, of course, is a continuous quantity, segmental dura-tion, so we use a regression tree.
We include all the input features291described above for the pronuncation tree, but we now add thecorresponding phone too.
We encode the phone with a schemesimilar to that for phonemes, but add a few extra categories tofully specify all the phones.
Perhaps a useful additional inputfeature would try to capture speech rate; we have not tried thisyet.The standard eviation in the residual in the prediction ofthe durations of phones output by the phone recognizer is 29msec.
This compares with an overall 45 msec.
standard eviationin the phones themselves.
The best cross-validated tree-length isabout 300 terminal nodes.For the lexical access, we need to represent the probabilitydistribution of the durations.
To do so, we can fit a gammadistribution to the data at each terminal node in the tree.6.
IMPLEMENTATION OF LEX ICAL  ACCESSWith these trees it is straight-forward to take a word se-quence and phone sequence and estimate the likelihood that theword sequence gives rise to the phone sequence.
We use the pro-nunciation trees to predict the first factor in Eq.
2.2 and theduration trees to predict he second factor.
This simple-mindedgenerate-and-test algorithm, of course, is not acceptable duringrecognition since the number of legal sentences i enormous.
In-stead, we have to find a more efficient way compute the exactsame thing or a close approximation.The simplest approach to an efficient implementation is touse the decision trees to form pronunciation a d duration net-works for each word in the vocabulary ahead of time.
Then, forevery possible starting phone and every possible stopping phonein the recognized phone sequence we match to the pronunciationnetwork for each word in the vocabulary.
To allow for inser-tions and deletions, this essentially becomes astring match withcosts in terms of log likelihoods in the probabilistic model \[cf.
3\].Dynamic programming permits an efficient match here \[6\].This approach presents one disadvantage; word co~articulation i formation is mostly lost, since the individual wordpronuncation model would need to be created without knowingthe lexical context.
To get around this, we can create multipleword models per word keyed to different lexical contexts.7.
RESULTSAt this time we have a simple version of the model de-scribed here running.
We have not yet implemented the word-coartlculation component and the lexical likelihood model hasthe form:P(y, dlw ) ~ P(ylw)P(dIw) (7.1).In other words, the duration model does not include the phonesequence only the phoneme sequence (cf.
Eq.
2.2).Testing the model on the February '89 DARPA resourcemanagement test set and using the word-pair grammar, weachieved 85.7% word correct and 83.2% word accurary.
Word in-sertion were 2.4% and deletions were 3.5%.
This is with a phonerecognizer that is achieves an estimated 81.5% phone correct and76.0% phone accuracy on the same test set, using automaticallyderived phonetic transcriptions \[ ee 1\].We are encouraged by this since it is a considerable improve-ment over this system's progenitor and approaching the best re-sults reported for phone-based recognition.
This improvementis due both to much better phone recognition and to improvedlexical access with this approach.We believe considerable further improvement will come whenwe include better duration information, word co-articulation,and, most importantly, when we input a phone lattice with rec-ognizer alternatives rather than just the best guess.
We have,in fact, implemented a crude version of a lattice in which thesegmentation produced by the best guess is used, but alternativephones and their likelihoods are included.
This performed 88.5%phone correct and 87.2% phone accuracy on the the Feb '89 testset.
We are now implementing a structure that allows a truelattice that will allow alternative segmentations.9.
REFERENCES\[1\] Ljolje, A.
1990.
Phone classification using high order phonat-actic constraints: preliminary results.
NATO ASI Speech Recog-nition and Understanding.
July 1990, Cetraro, Italy.\[2\] Z.ue,V., Glass, J., Phillips, M. and Seneff, S. 1989.
The MITSummit Speech Recognition System: a progress report.
Proc.DARPA Speech and Natural Language Workshop.
Feb 1989, pp.179-189.\[3\] Levinson, S.E., Ljolje, A. and Miller, L. 1990.
Continuousspeech recognition from a phonetic transcription.
Proe ICASSP'90.
pp 93-96.\[4\] Coker, C.. 1985.
A dictionary-intensive letter-to-sound pro-gram.
J. Acoust.
Soc.
Am.
78, Suppl.
1, $7.\[5\] Fisher, W., Zue, V., Bernstein, D. and Pallet, D. 1987.
Anacoustic-phonetic data base.
J. Acoust.
Soc.
Am.
81.
Suppl.
1.\[6\] Kruskal, J.
1983.
An overview of sequence comparison.
InTime Warps, String Edits, and Macromoleeules: the Theory andPractice of Sequence Comparison.
D. Sankoffand J. Kruskal, eds.Reading, MA: Addison Wesley.
pp.
1-44.\[7\] Brieman, L., et.
al.
1984.
Classification and Regression ~IYees.Monterey, CA: Wadsworth & Brooks.\[8\] Chen, F. 1990.
Identification ofcontextual factors for pronun-clation networks.
Proe.
ICASSP '90.
S14.9.\[9\] Riley, M. 1989.
Statistical tree-based modeling of phoneticsegment durations.
J Acoust.
Soc.
Am.
85.
$44.\[10\] Randolph, M. 1990.
A data-driven method for discoveringand predicting aUophonic variation.
Proc.
ICASSP '90.
S14.10.\[11\] Riley, M. 1989.
Some applications of tree-based modellingto speech and language.
Proc.
DARPA Speech and Natural Lan-guage Workshop.
Oct 1989, Cape Cod, MA, pp.
339-352.292
