Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 590?598,Beijing, August 2010Automatic generation of inter-passage links based on semantic similarityPetr Knoth, Jakub Novotny, Zdenek ZdrahalKnowledge Media InstituteThe Open Universityp.knoth@open.ac.ukAbstractThis paper investigates the use and theprediction potential of semantic similar-ity measures for automatic generation oflinks across different documents and pas-sages.
First, the correlation between theway people link content and the resultsproduced by standard semantic similaritymeasures is investigated.
The relation be-tween semantic similarity and the lengthof the documents is then also analysed.Based on these findings a new method forlink generation is formulated and tested.1 IntroductionText retrieval methods are typically designed tofind documents relevant to a query based onsome criterion, such as BM25 or cosine similar-ity (Manning et al, 2008).
Similar criteria havealso been used to identify documents relevant tothe given reference document, thus in principlelinking the reference document to the related doc-uments (Wilkinson and Smeaton, 1999).
This pa-per studies the correspondence between the resultsof this approach and the way linking is performedby people.
The study confirms that the length ofdocuments is an important factor usually causingthe quality of current link generation approachesto deteriorate.
As a result, methods working ata finer granularity than documents should be in-vestigated.
This will also improve the speed ofaccess to information.
For example, when usersread through a long document, they should be ableto quickly access a passage in another possiblyThis work has been partially supported by Eurogene -Contract no.
ECP-2006-EDU-410018)long document related to the discussed topic.
Theautomatic detection of document pairs containinghighly related passages is the task addressed inthis paper.A number of approaches for automatic linkgeneration have used measures of semantic sim-ilarity.
While these measures were widely usedfor the discovery of related documents in prac-tise, their correspondence to the way people linkcontent has not been sufficiently investigated (seeSection 2).
As our contribution to this topic, wepresent in this paper an approach which tries tofirst investigate this correspondence on a large textcorpus.
The resulting method is then motivated bythe outcomes of this analysis.It has been recognised in information retrievalthat when a collection contains long documents,better performance is often achieved by breakingeach document into subparts or passages and com-paring these rather than the whole documents to aquery (Manning et al, 2008).
A suitable granular-ity of the breakdown is dependent on a number ofcircumstances, such as the type of the documentcollection or the information need.
In this work,we have decided to work at the level of documentsand paragraphs.
Our task can be formalized as atwo-step process:1.
Given a collection of documents, our goal isto identify candidate pairs of documents be-tween which a link may be induced.2.
Given each candidate pair of documents, ourtask is to identify pairs of passages, such thatthe topics in the passages are related in bothdocuments.The method presented in this paper has many590potential applications.
First, it may be used for theinterlinking of resources that were not originallycreated as hypertext documents and for the main-tenance or the discovery of new links as the collec-tion grows.
Second, the method can be applied toimprove navigation in collections with long texts,such as books or newspaper articles.
A link maybe identified by the system automatically and theuser can be pointed immediately to the part of thetext which is relevant to the block of text currentlybeing read.
Similar application has been devel-oped by (Kolak and Schilit, 2008) who provided amethod for mining repeated word sequences (quo-tations) from very large text collections and inte-grated it with the Google Books archive.
Otherapplication areas may involve text summarizationand information retrieval.The paper makes the following contributions:?
It provides a new interpretation and insightin the use of semantic similarity measures forthe automatic generation of links.?
It develops a novel two-step approach forthe discovery of passage-passage links acrosspotentially long documents and it identifiesand discusses the selection of the parameters.The rest of the paper is organized as follows.Section 2 presents the related work in the field.Section 3 discusses the data selected for our exper-iment and Section 4 describes how the data wereprocessed in order to perform our investigation.
InSection 5, the analysis in which we compared theresults produced by semantic similarity measureswith respect to the way people link content is pre-sented.
Section 6 then draws on this analysis andintroduces the method for automatic generation oflinks which is finally evaluated in Section 7.2 Related WorkIn the 1990s, the main application area for linkgeneration methods were hypertext constructionsystems.
A survey of these methods is pro-vided by (Wilkinson and Smeaton, 1999).
Inthe last decade, methods for finding related docu-ments became the de-facto standard in large digi-tal repositories, such as PubMed or the ACM Dig-ital Library.
Search engines including Google alsogenerate links to related pages or research articles.Generating links pointing to units of a smallergranularity than a document, which can be con-sidered as a task of passage or focused retrieval,has also been addressed recently.
In this task, thesystem locates the relevant information inside thedocument instead of only providing a link to thedocument.
The Initiative for the Evaluation ofXML retrieval (INEX) started to play an essentialrole in link generation by providing tracks for theevaluation of link generation systems (Huang etal., 2008; Huang et al, 2009) using the Wikipediacollection at both the document and the passagelevel.Current approaches can be divided into threegroups: (1) link-based approaches discover newlinks by exploiting an existing link graph (Itakuraand Clarke, 2008; Jenkinson et al, 2008; Lu etal., 2008).
(2) semi-structured approaches try todiscover new links using semi-structured informa-tion, such as the anchor texts or document titles(Geva, 2007; Dopichaj et al, 2008; Granitzer etal., 2008).
(3) purely content-based approachesuse as an input plain text only.
They typicallydiscover related resources by calculating seman-tic similarity based on document vectors (Allan,1997; Green, 1998; Zeng and Bloniarz, 2004;Zhang and Kamps, 2008; He, 2008).
Some of thementioned approaches, such as (Lu et al, 2008),combine multiple approaches.Although link generation methods are widelyused in practise, more work is needed to under-stand which features contribute to the quality ofthe generated links.
Work in this area includes thestudy of (Green, 1999) who investigated how lex-ical chaining based on ontologies can contributeto the quality of the generated links, or the exper-iments of (Zeng and Bloniarz, 2004) who com-pared the impact of the manually and automati-cally extracted keywords.
There has also been ef-fort in developing methods that can in addition tolink generation assign a certain semantic type tothe extracted links and thus describe the relation-ship between documents (Allan, 1997).The method presented in this paper is purelycontent-based and therefore is applicable in anytext collection.
Its use in combination with link-based or semi-structured approaches is also pos-sible.
The rationale for the method comes from591the analysis of the prediction potential of semanticsimilarity for automatic link generation presentedin Section 5.
Related analysis is presented in (He,2008) which claims that linked articles are morelikely to be semantically similar1, however, thestudy does not provide sufficient evidence to con-firm and describe this relationship.
In link genera-tion, we are more interested in asking the oppositequestion, i.e.
whether articles with higher seman-tic similarity are more likely to be linked.
Ourstudy provides a new insight into this relationshipand indicates that the relationship is in fact morecomplex than originally foreseen by He.3 Data selectionThis section introduces the document collectionused for the analysis and the experiments.
Thefollowing properties were required for the docu-ment collection to be selected for the experiments.First, in order to be able to measure the correla-tion between the way people link content and theresults produced by semantic similarity measures,it was necessary to select a document collectionwhich can be considered as relatively well inter-linked.
Second, it was important for us to workwith a collection containing a diverse set of top-ics.
Third, we required the collection to containarticles of varied length.
We were mostly inter-ested in long documents, which create conditionsfor the testing of passage retrieval methods.
Wedecided to use the Wikipedia collection, becauseit satisfies all our requirements and has also beenused in the INEX Link-The-Wiki-Track.Wikipedia consists of more than four millionpages spread across five hundred thousands cat-egories.
As it would be for our calculation un-necessarily expensive to work with the whole en-cyclopedia, a smaller, but still a sufficiently largesubset of Wikipedia, which satisfies our require-ments of topic diversity and document length, wasselected.
Our document collection was generatedfrom articles in categories containing the wordsUnited Kingdom.
This includes categories, suchas United Kingdom, Geography of United King-dom or History of the United Kingdom.
Thereare about 3,000 such categories and 57,000 dis-tinct articles associated to them.
As longer arti-1With respect to the cosine similarity measure.cles provide better test conditions for passage re-trieval methods, we selected the 5,000 longest ar-ticles out of these 57,000.
This corresponds to aset where each article has the length of at least1,280 words.4 Data preprocessingBefore discussing the analysis performed on thedocument collection, let us briefly describe howthe documents were processed and the semanticsimilarity calculated.First, the N articles/documents D ={d1, d2, .
.
.
, dN} in our collection were prepro-cessed to extract plain text by removing the Wikimarkup.
The documents were then tokenized anda dictionary of terms T = {t1, t2, .
.
.
, tM} wascreated.
Assuming that the order of words canbe neglected (the bag-of-words assumption) thedocument collection can be represented usinga N ?
M term-document matrix.
In this way,each document is modelled as a vector corre-sponding to a particular row of the matrix.
As itis inefficient to represent such a sparse vector inmemory (most of the values are zeros), only thenon-zero values were stored.
Term frequency -inverse document frequency (tfidf) weighting wasused to calculate the values of the matrix.
Termfrequency tfti,dj is a normalized frequency ofterm ti in document dj :tfti,dj =f(ti, dj)?k f(tk, dj)Inverse document frequency idfti measures thegeneral importance of term ti in the collection ofdocuments D by counting the number of docu-ments which contain term ti:idfti = log|D||dj : ti ?
dj |tfidfti,dj = tfti,dj .idftiSimilarity is then defined as the functionsim(?
?x ,?
?y ) of the document vectors ?
?x and ?
?y .There exists a number of similarity measures usedfor the calculation of similarity between two vec-tors (Manning and Schuetze, 1999), such as co-sine, overlap, dice or Jaccard measures.
Somestudies employ algorithms for the reduction of di-mensions of the vectors prior to the calculation592of similarity to improve the results.
These ap-proaches may involve techniques, such as lexicalchaining (Green, 1999), Latent Semantic Indexing(Deerwester et al, 1990), random indexing (Wid-dows and Ferraro, 2008) and Latent Dirichlet Al-location (Blei et al, 2003).
In this work we inten-tionally adopted perhaps the most standard sim-ilarity measure - cosine similarity calculated onthe tfidf vectors and no dimensionality reductiontechnique was used.
The formula is provided forcompleteness:simcosine(?
?x ,?
?y ) =?
?x .?
?y|x|.|y|Cosine similarity with tfidf vectors has beenpreviously used in automatic link generation sys-tems producing state-of-the-art results when com-pared to other similarity measures (Chen et al,2004).
This allows us to report on the effective-ness of the most widely used measure with respectto the way the task is completed by people.
Whilemore advanced techniques might be in some casesbetter predictors for link generation, we did notexperiment with them as we preferred to focuson the investigation of the correlation between themost widely used measure and manually createdlinks.
Such study has to our knowledge never beendone before, but it is necessary for the justificationof automatic link generation methods.5 Semantic similarity as a predictor forlink generationThe document collection described in Section 3has been analysed as follows.
First, pair-wisesimilarities using the formulas described in Sec-tion 4 were calculated.
Cosine similarity is asymmetric function and, therefore, the calculationof all inter-document similarities in the datasetof 5, 000 documents requires the evaluation of5,00022 ?5, 000 = 12, 495, 000 combinations.
Fig-ure 1 shows the distribution of the document pairs(on a log10 scale) with respect to their similarityvalue.
The frequency follows a power law distri-bution.
In our case, 99% of the pairs have similar-ity lower than 0.1.To compare the semantic similarity measureswith the links created by Wikipedia authors, allinter-document intra-collection links, i.e.
linksFigure 1: The histogram shows the number ofdocument pairs on a log10 scale (y-axis) with re-spect to their cosine similarity (x-axis).created by users of Wikipedia commencing fromand pointing to a document within our collection,were extracted.
These links represent the connec-tions as seen by the users regardless of their direc-tion.
Each of these links can be associated witha similarity value calculated in the previous step.Documents with similarity lower than 0.1 were ig-nored.
Out of the 120, 602 document pairs withinter-document similarity higher than 0.1, 17, 657pairs were also connected by a user-created link.For the evaluation, interval with cosine simi-larity [0.1, 1] was divided evenly into 100 buck-ets and all 120,602 document pairs were assignedto the buckets according their similarity values.From the distribution shown in Figure 1, bucketscorresponding to higher similarity values containfewer document pairs than buckets correspondingto smaller similarity values.
Therefore, for eachbucket, the number of user created links withinthe bucket was normalized by the number of doc-ument pairs in the bucket.
This number is the like-lihood of the document pair being linked and willbe called linked-pair likelihood.
The relation be-tween semantic similarity and linked-pair likeli-hood is shown in Figure 2.As reported in Section 2, semantic similarityhas been previously used as a predictor for theautomatic generation of links.
The typical sce-nario was that the similarity between pairs of doc-uments was calculated and the links between the593Figure 2: The linked-pair likelihood (y-axis) withrespect to the cosine similarity (x-axis).most similar documents were generated (Wilkin-son and Smeaton, 1999).
If this approach was cor-rect, we would expect the curve shown in Figure 2to be monotonically increasing.
However, the re-lation shown in Figure 2 is in accordance with ourexpectations only up to the point 0.55.
For highervalues of inter-document similarity the linked-pairlikelihood does not rise or it even decreases.Spearman?s rank correlation and Pearson corre-lation were applied to estimate the correlation co-efficients and to test the statistical significance ofour observation.
This was performed in two inter-vals: [0, 0.55] and [0.55, 1].
A very strong positivecorrelation 0.986 and 0.987 have been receivedin the first interval for the Spearman?s and Pear-son coefficients respectively.
A negative correla-tion ?0.640 and ?0.509 have been acquired forthe second interval again for the Spearman?s andPearson coefficients respectively.
All the mea-sured correlations are significant for p-value wellbeyond p < 0.001.
Very similar results have beenachieved using different collections of documents.The results indicate that high similarity valueis not necessarily a good predictor for automaticlink generation.
A possible explanation for thisphenomenon is that people create links betweenrelated documents that provide new informationand therefore do not link nearly identical content.However, as content can be in general linked forvarious purposes, more research is needed to in-vestigate if document pairs at different similaritylevels also exhibit different qualitative properties.Figure 3: The average cosine similarity (y-axis) ofdocument pairs of various length (x-axis) betweenwhich there exists a link.
The x-axis is calculatedas a log10(l1.l2)More specifically, can the value of semantic sim-ilarity be used as a predictor for relationship typ-ing?An important property of semantic similarityas a measure for automatic generation of links isthe robustness with respect to the length of doc-uments.
As mentioned in Section 4, cosine sim-ilarity is by definition normalized by the productof the documents length.
Ideally the cosine sim-ilarity should be independent of the documentslength.
To verify this in our dataset, we have takenpairs of documents between which Wikipediausers assigned links and divided them into bucketswith respect to the function log10(l1.l2), where l1and l2 are the lengths of the two documents in thedocument pair and the logarithm is used for scal-ing.
The value of each bucket was calculated as anaverage similarity of the bucket members.
The re-sults are shown in Figure 3.
The graph shows thatthe average similarity value is slightly decreasingwith respect to the length of the articles.
Val-ues ?0.484 and ?0.231 were obtained for Spear-man?s and Pearson correlation coefficients respec-tively.
Both correlations are statistically signif-icant for p < 0.001.
A much stronger correla-tion was measured for Spearman?s than for Pear-son which can be explained by the fact that Spear-man?s correlation is calculated based on ranksrather than real values and is thus less sensitiveto outliers.594Our experience from repeating the same experi-ment on another Wikipedia subset generated fromcategories containing the word Geography tells usthat the decrease is even more noticeable whenshort and long articles are combined.
The de-crease in average similarity suggests that if co-sine similarity is used for the automatic gener-ation of links then document pairs with highervalue of l1.l2 have a higher linked-pair likelihoodthan pairs with a smaller value of this quantity.In other words, links created between documentswith small l1.l2 typically exhibit a larger valueof semantic similarity than links created betweendocuments with high value of l1.l2.
Although thedecrease may seem relatively small, we believethat this knowledge may be used for improvingautomatic link generation methods by adaptivelymodifying the thresholds with respect to the l1.l2length.6 Link generation methodIn this section we introduce the method for the au-tomatic generation of links.
The method can bedivided into two parts (1) Identification of candi-date link pairs (i.e.
the generation of document-to-document links) (2) Recognition of passages shar-ing a topic between the two documents (i.e.
thegeneration of passage-to-passage links).6.1 Document-to-document linksThe algorithm for link generation at the granular-ity of a document is motivated by the findings re-ported in Section 5.Algorithm 1: Generate document linksInput: A set of document vectors D,min.
sim.
?,max.
sim.
?
?
[0, 1], C = ?Output: A set C of candidate linksof form ?di, dj , sim?
?
C where di and dj aredocuments and sim ?
[0, 1] is their similarity1.for each {?di, dj?|i, j ?
?0 ?
i < j < |D|} do2.
simdi,dj := similarity(di, dj)3. if simdi,dj > ?
?
simdi,dj < ?
then4.
C := C ?
?di, dj , simdi,dj ?The algorithm takes as the input a set of doc-ument vectors and two constants - the minimumand maximum similarity thresholds - and iteratesover all pairs of document vectors.
It outputs alldocument vector pairs, such that their similarity ishigher than ?
and smaller than ?.
For well chosen?, the algorithm does not generate links betweennearly duplicate pairs.
If we liked to rank the dis-covered links according to the confidence of thesystem, we would suggest to assign each pair avalue using the following function.rankdi,dj = |simdi,dj ?
(?+?
?
?2 )|The ranking function makes use of the fact thatthe system is most confident in the middle of thesimilarity region defined by constants?
and ?, un-der the assumption that suitable values for theseconstants are used.
The higher the rank of a docu-ment pair, the better the system?s confidence.6.2 Passage-to-passage linksDue to a high number of combinations, it is typ-ically infeasible even for relatively small collec-tions to generate passage-to-passage links acrossdocuments directly.
However, the complexity ofthis task is substantially reduced when passage-to-passage links are discovered in a two-step process.Algorithm 2: Generate passage linksInput: Sets Pi, Pj of paragraph documentvectors for each pair in Cmin.
sim.
?,max.
sim.
?
?
[0, 1] such that?
< ?
?
?
< ?, , L = ?Output: A set L of passage linksof form ?pki , plj , sim?
?
L where pki andplj are paragraphs in documents di, djand sim ?
[0, 1] is their similarity1.for each {?pki , plj ?|pki ?
Pi, plj ?
Pj} do2.
simpki ,plj := similarity(pki , plj )3. if simpki ,plj > ?
?
simpki ,plj < ?
then4.
L := L ?
?pki , plj , simpki ,plj ?As Section 5 suggests, the results of Algorithm1 may be improved by adaptive changing of thethresholds ?
and ?
based on the length of the doc-ument vectors.
More precisely, in the case of co-sine similarity, this is the quantity lr = l1.l2.
The595value ?
should be higher (?
lower) for pairs withlow lr than for pairs with high lr and vice versa.Although the relative quantification of this ratio isleft for future work, we believe that we can ex-ploit these findings for the generation of passage-to-passage links.More specifically, we know that the length ofpassages (paragraphs in our case) is lower than thelength of the whole documents.
Hence, the sim-ilarity of a linked passage-to-passage pair shouldbe on average higher than the similarity of a linkeddocument-to-document pair, as revealed by theresults of our analysis.
This knowledge is usedwithin Algorithm 2 to set the parameters ?
and?.
The algorithm shows, how passage-to-passagelinks are calculated for a single document pairpreviously identified by Algorithm 1.
Applyingthe two-step process allows the discovery of doc-ument pairs, which are likely to contain stronglylinked passages, at lower similarity levels and torecognize the related passages at higher similaritylevels while still avoiding duplicate content.7 ResultsThe experimental evaluation of the methods pre-sented in Section 6 is divided into two parts:(1) the evaluation of document-to-document links(Algorithm 1) and (2) the evaluation of passage-to-passage links (Algorithm 2).7.1 Evaluation of document-to-documentlinksAs identified in Section 5 (and shown in Figure 2),the highest linked-pair likelihood does not occurat high similarity values, but rather somewhere be-tween similarity 0.5 and 0.7.
According to Figure2, the linked-pair likelihood in this similarity re-gion ranges from 60% to 70%.
This value is in ourview relatively high and we think that it can be ex-plained by the fact that Wikipedia articles are un-der constant scrutiny by users who eventually dis-cover most of the useful connections.
However,how many document pairs that could be linkedin this similarity region have been missed by theusers?
That is, how much can our system help inthe discovery of possible connections?Suppose that our task would be to find docu-ment pairs about linking of which the system ismost certain.
In that case we would set the thresh-olds ?
and ?
somewhere around these values de-pending on how many links we would like to ob-tain.
In our evaluation, we have extracted pairsof documents from the region between ?
= 0.65and ?
= 0.70 regardless of whether there origi-nally was a link assigned by Wikipedia users.
Anevaluation tool which allowed a subject to displaythe pair of Wiki documents next to each other andto decide whether there should or should not be alink between the documents was then developed.We did not inform the subject about the existenceor non-existence of links between the pages.
Morespecifically, the subject was asked to decide yes(link generated correctly) if and only if they foundit beneficial for a reader of the first or the sec-ond article to link them together regardless of thelink direction.
The subject was asked to decide no(link generated incorrectly) if and only if they feltthat navigating the user from or to the other doc-ument does not provide additional value.
For ex-ample, in cases where the relatedness of the doc-uments is based on their lexical rather than theirsemantic similarity.The study revealed that 91% of the generatedlinks were judged by the subject as correct and9% as incorrect.
Table 1 shows the results of theexperiment with respect to the links originally as-signed by the users of Wikipedia.
It is interest-ing to notice that in 3% of the cases the subjectdecided not to link the articles even though theywere in fact linked on Wikipedia.
Overall, the al-gorithm discovered in 30% of the cases a usefulconnection which was missing in Wikipedia.
Thisis in line with the findings of (Huang et al, 2008)who claims that the validity of existing links inWikipedia is sometimes questionable and usefullinks may be missing.An interesting situation in the evaluation oc-curred when the subject discovered a pair of ar-ticles with titles Battle of Jutland and Night Ac-tion at the Battle of Jutland.
The Wikipedia pageindicated that it is an orphan and asked users ofWikipedia to link it to other Wikipedia articles.Our method would suggest the first article as agood choice.596Wikipedia linkyes noSubject?s yes 0.61 0.30decision no 0.03 0.06Table 1: Document-to-document links from the[0.65, 0.7] similarity region.
The subject?s deci-sion in comparison to the Wikipedia links.Wikipedia linkyes noSubject?s decision yes 0.16 0.10at page level no 0.18 0.56Table 2: Document-to-document candidate linksgeneration from the [0.2, 0.21] similarity regionand document pairs with high lr (lr ?
[7.8?
8]).7.2 Evaluation of passage-to-passage linkingThe previous section provided evidence that thedocument-to-document linking algorithm is capa-ble of achieving high performance when param-eters ?, ?
are well selected.
However, Section5 indicated that it is more difficult to discoverlinks across long document pairs.
Thereby, wehave evaluated the passage-to-passage linking ondocument pairs with quite low value of similarity[0.2, 0.21].
According to Figure 2, this region hasonly 15% linked-pair likelihood.Clearly, our goal was not to evaluate the ap-proach in the best possible environment, but ratherto check whether the method is able to discovervaluable passage-to-passage links from very longarticles with low similarity.
Articles with thisvalue of similarity would be typically ranked verypoorly by link generation methods working at thedocument level.Table 2 shows the results after the first step ofthe approach, described in Section 6, with respectSystem?s decisionyes noSubject?s yes (correct) 0.14 0.46decision no (incorrect) 0.24 0.16Table 3: Passage-to-passage links generation forvery long documents.
Passages extracted from the[0.4, 0.8] similarity region.to the links assigned by Wikipedia users.
As in theprevious experiment, the subject was given pairsof documents and decided whether they should orshould not be linked.
Parameters ?
and ?
wereset to 0.2, 0.21 respectively.
Table 2 indicatesthat that the accuracy (16% + 10% = 26%) isat this similarity region much lower than the onereported in Table 1, which is exactly in line withour expectations.
It should be noticed that 34%of the document pairs were linked by Wikipediausers, even though only 15% would be predictedby linked-pair likelihood shown in Figure 2.
Thisconfirms that long document pairs exhibit a higherprobability of being linked in the same similarityregion than shorter document pairs.If our approach for passage-to-passage linkgeneration (Algorithm 2) is correct, we should beable to process the documents paragraphs and de-tect possible passage-to-passage links.
The selec-tion of the parameters ?
and ?
influences the will-ingness of the system to generate links.
For thisexperiment, we set the parameters ?, ?
to 0.4, 0.8respectively.
The subject was asked to decide: (1)if the connection discovered by the link generationmethod at the granularity of passages was useful(when the system generated a link) (2) whetherthe decision not to generate link is correct (whenthe system did not generate a link).
The results ofthis evaluation are reported in Table 3.
It can beseen that the system made in 60% (14% + 46%)of the cases the correct decision.
Most mistakeswere made by generating links that were not suffi-ciently related (24%).
This might be improved byusing a higher value of ?
(lower value of ?
).8 ConclusionsThis paper provided a new insight into the use ofsemantic similarity as a predictor for automaticlink generation by performing an investigation inthe way people link content.
This motivated usin the development of a novel purely content-based approach for automatic generation of linksat the granularity of both documents and para-graphs which does not expect semantic similarityand linked-pair likelihood to be directly propor-tional.597ReferencesAllan, James.
1997.
Building hypertext using infor-mation retrieval.
Inf.
Process.
Manage., 33:145?159, March.Blei, David M., Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
JOURNAL OFMACHINE LEARNING RESEARCH, 3:993?1022.Chen, Francine, Ayman Farahat, and Thorsten Brants.2004.
Multiple similarity measures and source-pairinformation in story link detection.
In In HLT-NAACL 2004, pages 2?7.Deerwester, Scott, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41:391?407.Dopichaj, Philipp, Andre Skusa, and Andreas He?.2008.
Stealing anchors to link the wiki.
In Gevaet al (Geva et al, 2009), pages 343?353.Geva, Shlomo, Jaap Kamps, and Andrew Trotman, ed-itors.
2009.
Advances in Focused Retrieval, 7th In-ternational Workshop of the Initiative for the Evalu-ation of XML Retrieval, INEX 2008, Dagstuhl Cas-tle, Germany, December 15-18, 2008.
Revised andSelected Papers, volume 5631 of Lecture Notes inComputer Science.
Springer.Geva, Shlomo.
2007.
Gpx: Ad-hoc queries and au-tomated link discovery in the wikipedia.
In Fuhr,Norbert, Jaap Kamps, Mounia Lalmas, and An-drew Trotman, editors, INEX, volume 4862 of Lec-ture Notes in Computer Science, pages 404?416.Springer.Granitzer, Michael, Christin Seifert, and Mario Zech-ner.
2008.
Context based wikipedia linking.
InGeva et al (Geva et al, 2009), pages 354?365.Green, Stephen J.
1998.
Automated link generation:can we do better than term repetition?
Comput.Netw.
ISDN Syst., 30(1-7):75?84.Green, Stephen J.
1999.
Building hypertext linksby computing semantic similarity.
IEEE Trans.
onKnowl.
and Data Eng., 11(5):713?730.He, Jiyin.
2008.
Link detection with wikipedia.
InGeva et al (Geva et al, 2009), pages 366?373.Huang, Wei Che, Andrew Trotman, and Shlomo Geva.2008.
Experiments and evaluation of link discoveryin the wikipedia.Huang, Wei Che, Shlomo Geva, and Andrew Trotman.2009.
Overview of the inex 2009 link the wiki track.Itakura, Kelly Y. and Charles L. A. Clarke.
2008.
Uni-versity of waterloo at inex 2008: Adhoc, book, andlink-the-wiki tracks.
In Geva et al (Geva et al,2009), pages 132?139.Jenkinson, Dylan, Kai-Cheung Leung, and AndrewTrotman.
2008.
Wikisearching and wikilinking.
InGeva et al (Geva et al, 2009), pages 374?388.Kolak, Okan and Bill N. Schilit.
2008.
Generatinglinks by mining quotations.
In HT ?08: Proceedingsof the nineteenth ACM conference on Hypertext andhypermedia, pages 117?126, New York, NY, USA.ACM.Lu, Wei, Dan Liu, and Zhenzhen Fu.
2008.
Csir atinex 2008 link-the-wiki track.
In Geva et al (Gevaet al, 2009), pages 389?394.Manning, Christopher D. and Hinrich Schuetze.
1999.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press, 1 edition, June.Manning, Ch.
D., P. Raghavan, and H. Schu?tze.
2008.Introduction to Information Retrieval.
Cambridge,July.Widdows, Dominic and Kathleen Ferraro.
2008.Semantic vectors: a scalable open source pack-age and online technology management applica-tion.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard Joseph Mari-ani Jan Odjik Stelios Piperidis Daniel Tapias, ed-itor, Proceedings of the Sixth International Lan-guage Resources and Evaluation (LREC?08), Mar-rakech, Morocco, may.
European Language Re-sources Association (ELRA).
http://www.lrec-conf.org/proceedings/lrec2008/.Wilkinson, Ross and Alan F. Smeaton.
1999.
Auto-matic link generation.
ACM Computing Surveys,31.Zeng, Jihong and Peter A. Bloniarz.
2004.
From key-words to links: an automatic approach.
InformationTechnology: Coding and Computing, InternationalConference on, 1:283.Zhang, Junte and Jaap Kamps.
2008.
A content-based link detection approach using the vector spacemodel.
In Geva et al (Geva et al, 2009), pages 395?400.598
