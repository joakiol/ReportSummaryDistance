l////I////////Natural Language Learning by Recurrent Neural Networks:A Comparison with probabilistic approachesMichael Towsey*, Joachim Diederich*, lngo Schellhammer*#, Stephan Chalup*,Claudia Brugman*** Neurocomputing Research Centre, Queensland University of Technology, QLD, 4001, Australia# Dept of Information Systems, University of Muenster, D-48149 Muenster, Germany**School of Languages, University ofOtago, Dunedin, New Zealandj oachim@fit, qut.
edu.
auAbstractWe present preliminary results of experiments with twotypes of recurrent neural networks for a natural languagelearning task.
The neural networks, Elman networks andRecurrent Cascade Correlation (RCC), were trained onthe text of a first-year primary school reader.
Thenetworks performed a one-step-look-ahead task, i.e.
theyhad to predict he lexical category of the next followingword.
Elman networks with 9 hidden units gave the besttraining results (72% correct) but scored only 63% whentested for generalisation using a "leave-one-sentence-out"cross-validation technique.
An RCC network could learn99.6% of the training set by adding up to 42 hidden unitsbut achieved best generalisation (63%) with only fourhidden units.
Results are presented showing networklearning in relation to bi-, t'i-, 4- and 5-gramperformance.
Greatest prediction uncertainty (measuredas the entropy of the output units) occurred, not at thesentence boundaries but when the first verb was the input.1.
IntroductionElman (1990) emphasised that natural anguage inputunfolds in time and therefore, recurrent networks whichcan accept a sequence of input patterns are the preferredchoice for many connectionist natural languageprocessing tasks.
In recurrent networks, knowledge isrepresented in activation patterns over hidden units andrevealed (i.e.
made explicit) by hierarchical clusteranalysis or other statistical methods.
Furthermore,recent evidence from cognitive neuroscience (Singer,1995) points to the importance of recurrent connectionsfor the formation of coherent cell assemblies.Recent work on recurrent neural networks hasfocussed on formal languages (Wiles & Elman, 1995).In this paper, we present preliminary results ofexperiments with recun'ent eural networks for a naturallanguage learning task.
Our strategy is to start withsimple children's texts and to step-wise increase thecomplexity of these texts to explore the learningcharacteristics of recurrent neural networks.
In the firstexperiments reported here, we are starting with a first-year primary school reader from which sentences withembedded structures have been diminatecL In futureexperiments, we will use unmodified first-year texts andwill continue with second-year textbooks and so on.1.1 Elman and RCC networksSimple recurrent networks (SRN's) of the Elman typeare similar to three-layer perceptrons but with recurrentconnections from the hidden layer to a context layer(also called state layer) which becomes part of theinput.
The activation patterns of the hidden units at timestep t are copied onto the context units and presentedwith the input at the next time step.
The hidden unitshave the task of mapping both an external input, andalso the previous internal state in order to produce somedesired output.
Thus, the internal representations thatdevelop are sensitive to temporal context; the effect oftime is implicit in these internal states (Elman, 1990).Finding the optimum hidden layer size for an Elmannetwork is a matter of trial and error and can be timeconsuming.
One approach to finding the optimumhidden layer size is to l~aln an RCC net whichimplements an inoremental learning algorithm.
The sizeof the generated RCC network gives an indication of areasonable size for a SRN.
It is important however todistinguish between the ability of an RCC network tolearn the training set and the generalisation of theresulting network.Fahlman (1991) introduced the RCC architecture.Instead of only adjusting the weights of a fixed networktopology, the idea is to start with a minimal network andto add hidden units as necessary.
The initial networkstarts with no hidden units, and only the weights to theoutputs are trained, If the resulting performance is notsatisfactory, a new hidden unit has to be added.The learning and network construction process worksas follows: The algorithm starts with a set of new units,called the candidate pool.
These units have randomlyinitialized, weighted connections from the input nodesand all the hidden units already present in the network.At that time their outputs are unconnected.
Then, theirincoming links are trained on the training set andadjusted in order to maximize the correlation betweenthe candidates' outputs and the remaining error(Fahlman, 1991).
When there is no improvement, thisprocess stops and the candidate with the best'correlation score' is added permanently.
These weightsare then frozen, which means that this node becomes anew feature detector in the network.
In order to integrateTowsey, Diederich, Schellhammer, Chalup, Brugman 3 Natural Language Learning by Recurrent Neural NetsMichael Towsey, Joachim Diederich, Ingo Schellhammer, Stephan Chalup and Claudia Brugman (1998) Natural LanguageLearning by Recurrent Neural Networks: a ?omparaison with probabilisti?
approaches.
In D.M.W.
Powers (ed.
)NeMLaP3/CoNLL98: New Methods in Language Processing and Computational N tural Language L arning ACL, pp 3-10.r"this new hidden unit, the weights of the output layer aretrained for multiple epochs (Fahlman, 1991).
Thisscheme of adding hidden units and retraining the outputlayer weights is repeated until success (when the errorfalls below a predefined threshold or no bit errors arefound) or a maximum number of hidden units is reached(which means failure).
Fahlman points out that thislearning algorithm is a 'greedy' algorithm because thenetwork is extended stepwise until a solution is found.Note that every new hidden unit adds a new hiddenlayer to the network, because it is connected to allprevious hidden units.
Recurrence is achieved by addingself-recurrent links to all hidden units.
So at time stept+l the activation of time step t is fed back to the biddenunit.
Such a constructive l arning algorithm is expectedto build a near minimal network for a given problem,because very hidden unit serves as a unique featuredetector, and no a priori and maybe inappropriateguesses about he size of hidden layers have to be made.1.2.
OverviewIn the remainder of this paper, we present results forElman networks which were evaluated by use of cross-validation.
We elucidate the Elman network's learningphases and compare its performance with RCCnetworks.
N-gram scores are used as benchmarks.
Weexamine the Elman network's uncertainty regarding theprediction of the next lexieal category and also thesequences it has correctly learned.2.
Methods2.1.
The dataThe natural anguage corpus used in these experimentswas obtained from a first-year primary school readerpublished circa 1950's (Hume).
This text was chosenbecause of its limited vocabulary and sentence structure.For this initial study, sentences with embeddedstructures (relative clauses) and a length of more thaneight words were eliminated.
The resulting corpusconsisted of 106 sentences ranging from three to eightwords in length, average length 5.06 words.
The wordswere converted to 10 lexieal categories, including asentence boundary marker.
The categories and theirabbreviations a used in the subsequent text and figuresare listed in Table 1.The resulting data consisted of a string of 643categories in 106 sentences.
There were 62 distinctsentence sequences of which 43 occurred only once, therest being replicated.
The maximum replication of anysequence was eight-fold.
Category frequencies are givenin Table I.For some experiments, the total data were used, fortraining and in other experiments he data were dividedinto training and test sets.
The test set consisted of everyfourth sentence taken from the total data yielding astring of 158 categories in 26 sentences.
The training setconsisted of the remaining data, a string of 486categories in 80 sentences.
Due to replication of somesentences, the test set contained sentence sequences thatalso occurred in the training set.TABLE 1: Percent frequencies of the ten lexicalcategories inthe text.Lexical Category %frequencyArticle AR 8%Conjunction CC 1%Preposition IN 7%Adjective JJ 4%Noun NN 30%Pronoun PR 10%Possessive ('s) PS 2%Adverb RB 1%Verb VB 20%Sentence /S 17%boundary2.2.
The networksThe two networks used in this study were an Elmannetwork and an RCC network.
For both nets there wereten input and ten output units representing the sparselycoded lexical categories.
The task in all cases was topredict the next lexical category given the currentcategory.
State unit activations were not reset o zero onpresentation of a sentence boundary as is sometimesdone.
The Elman network was trained by standardbackpropagation using momentum = 0.9.
Step-size andnumber of training epochs varied depending on therequirement for slow or fast training.
The slow trainingregime used stepsize = 0.0001 for 100,000 epochs and atypical fast training regime was 200 epochs at 0.01followed by another 200 at 0.001.
One training epochconsisted of one complete presentation of the trainingdata.
The RCC network was trained by Quickprop.Error of the outputs was measured as the root-mean-square (rms) of the difference between the output andsome target or reference value averaged over theoutputs.
The entropy of the outputs was calculated asI0I4o = -~ o i log 2 o; .i=12.3.
N-gramsIn order to assess the learning of the neural networks,prediction performance was compared with that of n-grams obtained by a statistical analysis of the data.Using the complete data sequence of 643 wordcategories, 48%, 62%, 72% and 76% correct predictionsTowsey, Diederich, Schellhammer, Chalup, Brugman 4 Natural Language Learning by Recurrent Neural Nets1IIIIIIIIIIlI/////l////////could be obtained using bigram, trigram, 4-gram and 5-gram probabilities respectively.3.
Results3.1.
Learning by Eiman networksAn Elman network having 9 hidden units and trained for100,000 epochs was able to learn 72% of the total data.However using a "leave-one-sentence-out" 106-foldcross-validation technique, the best generalisation resultfollowing fast training was 63%.
Figure 1 shows thefraction of the training data learned from I0 to 100,000epochs of slow training.
Early learning appears toproceed in discrete phases.
In the first phase (up to 1000epochs), the network predicts only NN, the categoryhaving highest frequency (30%).
In phase 2 (1000 to3000 epochs) the network predicts only NN or/S andscores 45% (the combined frequency of NN and/S is47%).
In phase 3 (3000 to 4000 epochs) the networkpredicts either NN,/S or VB, the three most commoncategories and at 5000 epochs it is predicting either NN,/S, VB orAILThe network's rms error with respect o the targets(labeled as "target error" in Figure 1) declinedeontinuonsly during learning down to 0.160 at 80,000epochs and increased slightly subsequently.
It is alsouseful to measure the network's rms error with respectto n-gram probabilities on the assumption that thenetwork should be learning n-gram probabilities with nincreasing during training.
These errors are referred toas bigram, trigram and 4-gram errors in Figure 1.Bigram error is initially less than trigram and 4-gramerrors and declines most rapidly from 800 to 3000epochs.
It begins to increase again after 4000 epochswhile trigram and 4-gram errors continue to decline.Al~er about 8,000 epochs, trigram error reaches aminimum value of 0.067 and then starts to increase.
4-gram error continue~ to decline to a value of 0.068 at80,000 epochs after which it also starts to increase.
5-gram error (not shown in Figure 1 to preserve clarity)declines to a value of 0.076 at 100,000 iterations but isbeginning to level out.To confirm that the Elman network is makingpredictions based on conditional probabilities and alsoto justify the calculation of output entropy as defined inthe Methods ection, we require that the sum of outputsshould be close to 1.0.
In Figure 2 it can be observedthat from about 100 epochs, the average sum of outputsis indeed close to 1.0, although the standard eviation ofthe average sum increases from 0.02 at 100 epochs to0.19 at 100,000 epochs.
The entropy of the outputs (ameasure of the network's 'uncertainty' about he nextpredicted category) declines as learning proceeds(Figure 2), but showing two 'fiat' periods correspondingto 'fiat' periods in target error.3.2.
Comparison of Eiman and RCCnetworksWhen trained on the set of 485 training patterns, theRCC network continued to add hidden units and wasable to learn 99.6% of patterns after adding 42 hiddenunits (Figure 3).
However amaximum generalisation of63% on the test set was achieved after only 4 hiddenunits and generalisation declined with further additionof hidden units (Figure 3).
By contrast, when Elmannetworks with 1-50 hidden units were trained on thesame data, there was no simple recognisablerelationship between generalisation and hidden layersize.
An Ehnan network with 4 hidden units scored 60%on the test set, 3% lower than an RCC net of the samesize.
An Elman network with 9 hidden units scored64%.
However the best generalisation score of 68% wasachieved with 42 hidden units.3.3 Predietio n~U ncertaintyFigure 4 shows a graph of prediction uncertainty(measured as the entropy of the output units) over a partof the sequence of category targets.
Each point islabeled with the target category.
Highest entropy alwaysoccurs when the input is the first VB in the sentence.
Anincrease in entropy is also associated with the firstcategory in the sentence.
By contrast here is a lowentropy associated with the prediction of sentencetermination, 89% of sentence ndings being correctlypredicted.3.4.
Correctly predicted sequencesIt is possible to reconstruct the sequences correctlylearned by the Elman network that had learned 72% ofthe training set.
They are shown in Figure 5.
Thetransitions marked with an asterix (>*) are those notpredicted by trigram probabilities.
Sequences 1 and 5include complete and grammatical sentence structures.4.
Discussion4.1.
Training the Elman networkAfter 10 iterations, the network was predicting the NNcategory for every pattern.
Since NN was the highestfrequency category, this was the quickest way for thenetwork to reduce its initial prediction error.
It could besaid that the network was performing equivalently to aunigram predictor.Towsey, Diederich, Schellhammer, Chalup, Brugman 5 Natural Language Learning by Recurrent Neural NetsI rt-.._o00 x_s .
_0.80.60.40.2010 1 O0 1000 10000 100000epochI--*--fraction learnt error --n--bigram error ,o trigram error = 4-gram -4--target errorFigure 1: Learning by a simple recurrent neural network of the Elman type having 9 hidden units.
Y-axis showsprogression of r.m.s, error with respect to the targets, bigram and trigram probabilities and the fraction of training setlearned as a function of training epochs.mooE:3 t~Q.0l -Q)654321010 100 1000 10000 100000epochst , ,  output sum --,,-output entropy tFigure 2: The progression of sum of outputs and entropy of the outputs of an SRN trained over 100,000 epochs.Towsey, Diederich, Schellhammer, Chalup, Brugman 6 Natural Language Learning by Recurrent Neural NetsKIIIIIIII!1IIIIIIIIIiIIIIII|||IIIIIIIIIII00m40 .., ,~: .
.
.
.
.0 10 20 30 40 50number of hidden units-- Train % - -*-  Test % !
iFigure 3: Progression of the score on training set and test set during the training of an RCCN.3VB VB PR PR IN2.5D.oE1.50.5IS PRNNS/SNNNNAVt,NNA VVtarget  sequenceFigure 4: Graph of entropy of the output units over six sentences of the input sequence.Towsey, Diederich, Schellhammer, Chalup, Brugman 7 Natural Language Learning by Recurrent Neural Nets(1)(2)(3)(4)(5)(6)(7)<8)CC >* NN CCV ~t V/S > NN > VB > VB > IN > AR > NN > /SA ARB VBIN > NN > /SAR > J J  > NN > /SAR>* NN > IN > PR> NN > /SINV~r/S >* PR >* VB > NN > PS > NN > /SNNV "R .PR > VB > NN > /SAR >* NN >* VB >* JJ >* ISIN >* PR >* /SFigure 5: Category sequences correctly learned by a simple recurrent eural network.
Those sequences marked with>* are not predicted by trigram probabilities.Although VB has the second highest frequency(higher than/S), during the second learning phase thenetwork outputs were confined to NN or/S (not V'B).This is because the network was beginning to learnbigram probabilities and an inspection of the bigrarnfrequency table revealed that there were 100 instnneesof/S prediction using bigram probabilities but only 35instances of predicting a VB.
It is during this secondlearning phase that the bigram error decreases mostrapidly.In phase 3, the VB category is added to the network'sprediction capability and in phase 4, AR is added, therebeing only 19 instances where AR would be predictedusing bigram probabilities.
In fact, using bigramprobabilities only these four categories (NN,/S, VB andAR) can be predicted.
At~er about 5000 epochs thenetwork was also correctly predicting other categories,which indicates that it was making predictions based onthe current and previous inputs.
And indeed we observe/.hat he trigram error rate falls below the bigram errorrate around 5000 epochs (Figure 1).In Figure 5 it is apparent hat the network hascorrectly learned category transitions that are notpredicted by trigram probabilities.
This is indicative thatthe network was using at least the current and twoprevious inputs as context for its decisions.
In fact 4-gram error continues to decline up to 80,000 epochs.Since the average sentence l ngth is 5.05 words, it is notsurprising that the 5-gram error remains above 4-grarnerror throughout learning.Of course it is not being suggested here, that arecurrent etwork is first learning all the probabilities ofa bigram model and then moves on to learn the trigrammodel and so on.
Network learning is driven by therequirement to minimise predictive rror.
Thus longersequences having high frequency will bias learningmore than infrequently occurring short sequences.Nevertheless an interesting feature of learning apparentin Figure 1 was that minimum bigram error wasachieved at 4000 epochs when the network had learned48% of the training set, equivalent tothe performance ofa bigram predictor.
Similarly minimum trigrarn and 4-gram error was achieved when the network had learnedthe equivalent of a lrigram and 4-gram predictorrespectively.Mention should be made of the decision not to resetstate unit activations to zero when the Elman networkencountered d sentence boundary.
When resets wereTowsey, Diederich, Schellharamer, Chalup, Brugman 8 Natural Language Learning by Recurrent Neural NetsIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIilIIIIIIIIIIIIIIIII!1IIused, network predictive performance dropped from70% to 69% with otherwise similar training regimes.
Inother words, there was minimal information transferover sentence boundaries and it is more interesting toobserve this aspect of network learning than to impose'forgetting' artifieiaUy.
The slight increase inperformance without resets was probably due to therepetitive nature of the sentences in this text meant forearly readers.
An additional reason for not using resetswas that it made comparisons of network performancewith n-gram statistics easier.4.2.
Comparison of Eiman network andRCC netsAlthough the RCC net was capable of learningalmost the entire training set, the hidden unitrepresentations that the network acquired did notgeneralise well.
On the other hand, the best generalisingRCC net with four hidden units did better than an Elmannetwork with the same number of bidden units.
Due todifferent learning algorithms, the two networkspresumably acquired different hidden unitrepresentations of the underlying task.
It is clear that,for this task at least, training an RCC net to find theoptimum number of hidden units for an Elman networkis not a satisfactory technique.The maximum RCC score on the test set of 63%was, in fact, an unexpectedly high score.
A bigrammodel acquired from the training set of 80 sentences,predicted 48% and 45% of the training and test setwords respectively.
The equivalent scores for thetrigram model were 63% and 17% respectively.
Thepoor generalisation f the n-gram models for n > 2 arosebecause the test sequences did not have the samestatistical slructure as the training sequences for n > 2.This is the consequence of using natural languagesentences and converting the words to lexicalcategories.
A similar difficulty was noted by Lawrenceet al(1996) for their NL task which required recurrentnetworks to classify sentences as either grammatical orungrammatical.The experimental paradigm used in our experimentdemands alternative measures of generalisation.
Thesemight include (1) testing on an artificially generatedsequence that has the same n-gram (statistical) structureas the NL training sequence (2) testing on the trainingsequence corrupted with output noise (3) testing on thetraining sequence but with the sentences in randomorder.
This last is appropriate where resets are not usedduring training.
Such alternative tests of generalisationwill be considered in future work.4.3.
Prediction UncertaintyElman (1990) found that when a recurrent net wastrained on letter sequences consisting of concatenatedwords, its prediction error tended to decrease frombeginning to end of each word.
Thus a sharp increase inprediction error could be used to segment he lettersequence into words.In our study, there was low entropy associated withend-of-sentence prediction, 89% of/S being correctlypredicted.
Furthermore, when the input was/S, outputentropy increased in 84% of cases.
However by far themost obvious increase in prediction uncertaintyoccurred when the input was the first VB of thesentence (Figure 4).We should not expect hat prediction uncertainty willdecrease from beginning to end of a sentence in thesame way that it does for words, because the ruleswhich govern word structure are different from thosewhich govern sentence structure.
For example, theinventory of units that makes up words is so muchsmaller and the articulation of phoneme sequences imore highly constrained.
It is not surprising therefore, tofind that in our task, a sharp increase in the network'sprediction uncertainty occurs other than when itencounters a sentence boundary.The first VB in our tagging system was either anauxiliary, or modal or the verb itself, if there was noauxiliary.
In other words, the first VB has the largestnumber of highly probable successors.
A linguisticinterpretation of the network behaviour is complicatedby the small number of lexical categories used in thestudy.
Ira more fine-grained system of tagging had beenused, the progression of prediction uncertainty throughthe sentences would have been different.
All thesentences in the text consisted of single clauses and thenetwork behaviour is consistent with the verb being themost important determinant of sentence or clausestructure.5.
ConclusionsWe have described results for the training of Elman andRCC networkson a natural language task.
The task is topredict he part-of-speech ategory of the next word in asentence given the category of the current word as input.The Elman network appears to be a more useful modelfor this one-step-look-ahead taskthan the RCC network.Elman networks are statistical learners and we haveshown that network learning can be interpreted in t=rrtsof learning n-gram statistics.
However because networklearning is driven by minimisation of predictive rror,longer sequences having high frequency bias learningmore than infrequently occurring short sequences.The sequences correctly learned by the Elmannetwork included some that were not predicted bytrigram probabilities, evidence that the network wasusing the previous three or more inputs as context forprediction.Prediction uncertainty was highest when the input wasthe first verb category in the sentence, possiblyTowsey, Diederich, Schellharnmer, Chalup, Brugman 9 Natural Language Learning by Recurrent Neural Netsconsistent with the important role that the verb plays inthe syntactic structure of a sentence.6.
ReferencesElman, J.L.
(1990).
Finding Structure in Time.Cognitive Science 14, 179-211.Fahlman, S.E.
(1991).
The Recurrent CascadeCorrelation Architecture.
(Tech.
Rep. CMU-CS-91-110).
Pittsburgh, PA.: Carnegie MellonUniversity.Lawrence, S., Fong, S. and Giles, C.L.
(1996), Naturallanguage grammatical inference: a comparison ofrecurrent neural networks and machine learningmethods.
In Connectionist, Statistical andSymbolic Approaches to Learning for NaturalLanguage Processing.
Eds S. Wermter, E. Riloffand G. Seheler.
pub Springer-Verlag.Hume, M.A.
(circa 1950).
The Happy Way to Reading.Blackie and Son Ltd, London and Glasgow.Singer, W. (1995).
Development and Plasticity ofCortical Processing Architectures.
Science, 270,758-764.Wiles, J.
& Elman, J.L.
(1995).
Learning to Countwithout a Counter: A case study of dynamics andactivation landscapes in recurrent networks.Proceedings of the Seventeenth Annual Conference.of the Cognitive Science Society., Cambridge, MA:MIT Press.Towsey, Diederich, Schellhammer, Chalup, Brugman 10 Natural Language Learning by Recurrent Neural Nets11IIII1IIII1IIIIIIII1IIII
