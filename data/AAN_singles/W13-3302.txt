Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 10?18,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsAnalysing Lexical Consistency in TranslationLiane GuillouSchool of InformaticsUniversity of EdinburghScotland, United KingdomL.K.Guillou@sms.ed.ac.ukAbstractA number of approaches have been takento improve lexical consistency in Statis-tical Machine Translation.
However, lit-tle has been written on the subject ofwhere and when to encourage consistency.I present an analysis of human authoredtranslations, focussing on words belong-ing to different parts-of-speech across anumber of different genres.1 IntroductionWriters are often given mixed messages with re-spect to word choice.
On one hand they are en-couraged to vary their use of words (in essay writ-ing): ?It is also important that the words you useare varied, so that you aren?t using the same wordsagain and again.?1.
On the other hand they areencouraged to use the same words (only chang-ing the determiner) when referring to the same en-tity a second time (in technical writing): ?The firsttime a single countable noun is introduced, use a.Thereafter, when referring to that same item, usethe.?
2.Halliday and Hassan (1976) showed that well-written documents exhibit lexical cohesion interms of what they call reiteration and colloca-tion.
Reiteration is achieved via repetition as wellas the use of synonyms and hypernyms.
A collo-cation is a sequence of words / terms that co-occurregularly in text.
Examples of collocated pairs ofwords include ?fast food?, ?bright idea?
and ?nu-clear family?.
Any source language document will1Purdue University, Online Writing Lab: http://owl.english.purdue.edu/engagement/index.php?category_id=2&sub_category_id=2&article_id=66.
Accessed 21/04/20132Monash University, Language and Learning On-Line: http://monash.edu.au/lls/llonline/grammar/engineering/articles/6.xml.
Ac-cessed 21/04/2013therefore contain repeated instances of the samewords or lemmas (morphological variants of thesame words).
This repeated use of words and lem-mas is known as lexical consistency and the in-stances can be grouped together to form lexicalchains (Morris and Hirst, 1991).
Lexical chainswere proposed by Lotfipour-Saedi (1997) as onefeature of a text via which translational equiva-lence between source and target could be mea-sured.While Statistical Machine Translation (SMT)has gone from ignoring these properties of dis-course by translating sentences independently, totrying to impose lexical consistency at a universallevel, both approaches have given little consider-ation to what might be standard practice amonghuman translators.In order to discover what the standard practicemight be, and thus what an SMT system might bet-ter aim to achieve, I have carried out a detailedanalysis of lexical consistency in human transla-tion.
For comparison, I also present an analysis oftranslations produced by an SMT system.
I haveconsidered a variety of genres, as genre correlateswith the function of a text, which in turn predictsits important elements.
A preliminary conclusionof this analysis is that human translators use lex-ical consistency to support what is important in atext.2 Related Work2.1 Unique Terms and Lexical ConsistencyIntuitively, it seems obvious that specialised,?semantically heavy?
words like ?genome?
and?hypochondria?
will only have a single exacttranslation into any given target language, and assuch will tend to be translated with greater consis-tency than semantically ?light?
words.
Melamed(1997) showed that this intuition could be quan-tified using the concept of entropy, which the10author uses over a large corpus to show whatwords and what parts-of-speech are more likely tobe translated consistently than others.
However,Melamed?s analysis ignores any segmentation ofthe corpus by document, topic, speaker/writer ortranslator, considering only overall translationaldistributions.
It is therefore similar to that whichcan be gleaned from the phrase table in a modernSMT system.2.2 Enforcing and Encouraging ConsistencyA number of approaches have been taken to bothencourage and enforce lexical consistency in SMT.These range from the cache-based model ap-proaches of Tiedemann (2010a; 2010b) and Gonget al(2011), to the post-editing approach of Xiaoet al(2011) and discriminative learning approachof Ma et al(2011) and He et al(2011).Carpuat (2009) and Ture et al(2012) suggestedthat the one sense per discourse constraint (Galeet al 1992) might apply as well to one sense pertranslation.
Both demonstrated that exploiting thisconstraint in SMT led to better quality transla-tions.
Ture et al(2012) encourage consistencythemselves using soft constraints implemented asadditional features in a hierarchical phrase-basedtranslation model.What has not been adequately addressed in theavailable MT literature is where and when lexicalconsistency is desirable in translation.2.3 Measuring ConsistencyIn contrast with entropy following from lexicalproperties of words (i.e.
how many senses a wordhas, and how many different possible ways thereare of translating each sense in a given target lan-guage), as explored in (Melamed, 1997), Itagaki etal (2007) developed a way to measure the termi-nological consistency of a single document.
Theydefine consistency as a measure of the number oftranslation variations for a term and the frequencyfor each variation.
They adapted the Herfindahl-Hirschman Index (HHI) measure, typically usedto measure market concentration, to measure theconsistency of a single term in a single document.HHI is defined as:HHI =n?i=1s2iWhere i ranges over the n different ways that thegiven term has been translated in the document,and si is the ratio of the number of times the termhas been translated as i to the number of times ithas been translated.
The lower the index, the morevariation there is in translation of the term, i.e.
theless consistent the translation.
The maximum in-dex is 10,000 (or 1 using the normalised scale) fora completely consistent translation.HHI is best illustrated with examples of dis-tributions over a single document.
An Englishword with two French translations that are ob-served with equal frequency will receive a scoreof: 0.502 +0.502 = 0.5.
A different English wordwith two French translations observed 80% and20% of the time will receive a score of: 0.902 +0.102 = 0.82 representing a more consistent trans-lation of the English word.
When the numberof possible French translations increases, the HHIscore will likely decrease unless one translation ismuch more frequent - see previous example.
AnEnglish word with three translations observed withequal frequency (33.3% each) will have a score of:0.332 +0.332 +0.332 = 0.33 representing a wordthat is translated with lower consistency.Itagaki et alincorporate these HHI scores (onescore per term, per document) in a wider calcula-tion that measures inter-document consistency ofa set of documents that all use the same term.
Asthe analyses presented in this paper are concernedwith single documents and their translations, theper term, per document HHI scores are sufficient.3 MethodologyThis section describes analyses of manual (hu-man) translation and automated translation (bya phrase-based SMT system).
The data usedis described in Section 3.1 and the methods foranalysing consistency in human and automatedtranslation are described in Sections 3.2 and 3.3.3.1 DataAs the focus of the analysis is lexical consistency,it was important to select texts that were writ-ten/translated by the same author.
The typicalcorpora used in training SMT systems were dis-missed; Europarl as speakers change frequentlyand news-crawl as the articles are typically tooshort to exhibit much lexical repetition.
InsteadI selected the INTERSECT corpus (Salkie, 2010)which contains a collection of sentence-alignedparallel texts from different genres.
From this cor-pus I extracted a number of texts from the English-11Title Genre Sentences Words En POS Count Fr POS CountEn Fr N A V N A VEnglish SourceXerox ScanWorx Manual Instructions 2,573 38,698 44,841 14,060 2,308 6,555 15,206 2,528 8,822On the Origin of Species Natural Science 1,702 62,454 68,016 13,774 6,868 9,857 17,452 6,291 12,895Dracula Ch.
1-2 Novel 584 11,209 10,840 2,147 817 2,110 2,659 745 2,336The Invisible Man Ch.
1-4 Novel 504 7,578 7,924 1,845 442 1,471 2,118 472 1,720French SourceNuclear Testing Public Info 613 13,127 13,563 3,918 1,412 1,808 4,261 1,344 2,253French Revolution to 1945 Public Info 1530 34,038 33,187 11,217 3,119 4,279 11,008 3,025 4,632The Immoralist Novel 1,377 29,323 24,942 5,299 2,049 5,888 5,813 1,513 6,138News article 1 News 126 1,757 1,751 549 122 284 558 115 324News article 2 News 126 2,306 2,254 590 150 430 673 125 459News article 3 News 85 1,891 1,756 501 183 332 534 122 332News article 4 News 97 2,236 1,974 641 157 367 609 120 356Table 1: Documents taken from the English (En) - French (Fr) section of the INTERSECT corpus.French collection (Table 1).
The frequencies fornouns (N), adjectives (A) and verbs (V) in this ta-ble were extracted automatically using the Tree-Tagger tool (Schmid, 1994).Word alignments for the parallel documentswere computed using Giza++ (Och and Ney,2003) run in both directions.
In order to improvethe robustness of the word alignments the doc-uments were concatenated into a single file, to-gether with English-French parallel data from theEuroparl corpus (Koehn, 2005).
The word align-ments for the relevant documents were then ex-tracted from the symmetrised alignment file.3.2 Consistency in Human TranslationThe motivation for this analysis was to assess theextent to which a human translator maintained lex-ical consistency when translating a document.
Inother words, in those places where the author of asource document makes consistent lexical choices,do human translators do so as well?
And if theydo, should we aim for the same in SMT?For each document, the English and Frenchparallel texts were processed using TreeTagger(Schmid, 1994).
Using the language in whichthe document was originally written (its born lan-guage) as the source language, word alignmentswere used to identify what each source wordaligned to in the (human) translation.Since I wanted to establish not just the degreeof consistency, but where consistency was beingmaintained, and because I felt that the Part-of-Speech (POS) tags output by TreeTagger weretoo fine-grained for this purpose, these tags weremapped to a set of coarse-grained tags.
TheUniversal POS tagset mapping file (Petrov et al2011) was used for English and a comparable filewas constructed for French.
In addition to this, Ialso sub-divided the coarse-grained verb class intothree classes: light verbs (e.g.
do, have, make),mid-range verbs (e.g.
build, read, speak) and rare-verbs (e.g.
revolutionise, obfuscate, perambulate).This was to test the hypothesis that light verbswill exhibit lower levels of consistency than otherverbs.
A light verb is defined a verb with little se-mantic content of its own that forms a predicatewith its argument (usually a noun).
For examplethe verb ?do?
in ?do lunch?
or ?make?
in ?makea request?.
As no predefined lists of light, mid-range and rare verbs are available, these groupswere approximated.
An English verb?s categoryis determined by its frequency in the British Na-tional Corpus (BNC) (Clear, 1993).
A verb witha frequency count in the bottom 5% is deemed arare verb, in the top 5% is deemed a light verband anything in between, is deemed a mid-rangeverb.
A manual inspection of the resulting cat-egory boundaries shows that these thresholds arereasonable.
For French, verb frequencies were ex-tracted from the French Treebank (Abeille?
et al2000).Herfindahl-Hirschman Index (HHI) (Itagaki etal., 2007) scores were calculated for each surfaceword (one score per surface word) in the bornlanguage document.
The documents were treatedseparately, and no inter-document scores are cal-culated.
These scores tell us how consistent thetranslation is into the target language.
For wordsin the English documents I considered what wordsand lemmas were present in the French transla-tion.
Lemmas are included as French verb inflec-tions may otherwise skew the results.
For com-12pleteness, lemmas in the English translation of theFrench documents are also considered.For each POS category, an average HHI scoreis calculated by taking the sum of the HHI scoresper word and dividing it by the number of words(for that POS category).
Only those words that arerepeated (i.e.
appear more than once in the sourcedocument with the same coarse POS category) areconsidered.
(That is, a word that appeared onceas a mid-range verb, once as a noun and once assomething else, would not be included).
A similaraverage is calculated for lemmas.HHI scores are normally presented in the rangeof 0 to 10,000.
However, for simplicity, the scorespresented in this paper are normalised to between0 to 1.3.3 Consistency in Automated TranslationThe aim of this analysis was to assess how the con-sistency in translations produced by an SMT sys-tem would compare to those by a human translator.The SMT system was an English-French phrase-based system trained and tuned using (Moses and)Europarl data.
Its language model was constructedfrom the French side of the parallel training cor-pus.
The system was used to translate the bornEnglish source documents (Xerox Manual, Onthe Origin of Species, Dracula and The InvisibleMan).
Word alignments and a file containing a listof Out of Vocabulary (OOV) words were also re-quested from the decoder.
Note that all of the doc-uments are considered to be ?out of domain?
withrespect to the training data used to build the SMTsystem.Using a similar process as described in Section3.2, but omitting those words that are reported bythe decoder as OOV, average HHI scores are cal-culated for each POS category.
OOV words areomitted as these will be ?carried through?
by thedecoder, appearing untranslated in the translationoutput.
They therefore do not say anything aboutthe consistency of the translation.The other major difference is that HHI scoresare calculated only at the word level, not at thelemma level as it is expected that the TreeTaggerwould perform poorly on SMT output and theseerrors could lead to misleading results.
In all otherrespects, the process for analysing text is the sameas described in Section 3.2.4 Results4.1 Consistency in Human TranslationThe results are presented in Table 2.
Higher (aver-age) HHI scores represent greater consistency.For both English and French source documents,nouns score highly, suggesting that in general hu-man translators translate nouns rather consistently.However, nouns don?t always receive the highestaverage score.
For verbs, the trend is that consis-tency is irrelevant in translating light verbs, rareverbs tend to be translated with the highest consis-tency, and mid-range verbs are somewhere in be-tween.
This suggests that consistency in the trans-lation of light verbs would be undesirable.Looking at some of the texts in more detail itmay be possible to infer certain qualities of textacross different genres.Novels: In all three texts, (Dracula, TheInvisible Man and The Immoralist), nouns receivethe highest average HHI score of all the POScategories.
An analysis of some of the mostfrequent (and aligned) nouns in Dracula (Table3) suggests that it is desirable to keep importantnouns constant - those that identify characters andother entities central to the story.
For example,the Count is an important character and is neverreferred to by any other name/title in the originaltext.
(N.B.
?count?
is also a mid-range verb, but itis used only as a noun in Dracula).
The translationto (le) comte in French is highly consistent.
Asimilar observation is made for horses which areimportant in the story.
Interestingly, the (same)coach driver is referred to as (le) chauffer, (le)conducteur and (le) cocher in French:English: ...and the driver said in excellent GermanFrench: Le conducteur me dit alors, en excellent allemandEnglish: Then the driver cracked his whipFrench: Puis le chauffeur fit claquer son fouetEnglish: When the caleche stopped, the driver jumped downFrench: La cale`che arre?te?e, le cocher sauta de son sie`geThis perhaps reflects a stylistic choice made by thetranslator to vary the terms used to refer to a char-acter of lesser importance.
It is worth noting thatthe English text also contains several instances of?coachman?
to refer to the ?driver?
but the varia-tion is much less compared with the French trans-lation.Verbs, on the other hand, receive lower (aver-age) HHI scores indicating that this may be an area13Title Noun Adj VerbAll Light Mid-Range RareEnglish SourceXerox ScanWorx Manual 0.6995 0.5900 0.5568 0.3256 0.5766 0.6485Xerox ScanWorx Manual (Lemmas) 0.7126 0.7112 0.6612 0.4172 0.6902 0.7086On the Origin of Species 0.6109 0.4390 0.4001 0.2339 0.4140 0.4592On the Origin of Species (Lemmas) 0.6417 0.5722 0.5056 0.3355 0.5273 0.5098Dracula 0.6182 0.4191 0.3631 0.2477 0.4175 0.5000Dracula (Lemmas) 0.6294 0.4979 0.4113 0.2902 0.4711 0.5000The Invisible Man 0.6290 0.5110 0.4159 0.3139 0.4797 0.4219The Invisible Man (Lemmas) 0.6275 0.5743 0.4573 0.3723 0.5121 0.4219French SourceNuclear Testing 0.7388 0.8079 0.5616 0.3312 0.5279 0.6228Nuclear Testing (Lemmas) 0.7521 0.8209 0.5972 0.4198 0.5599 0.6584French Revolution to 1945 0.6346 0.6587 0.5054 0.3041 0.4404 0.5521French Revolution to 1945 (Lemmas) 0.6509 0.6632 0.5266 0.3950 0.4710 0.5655The Immoralist 0.6807 0.5732 0.4868 0.3106 0.4524 0.5046The Immoralist (Lemmas) 0.7007 0.5856 0.5142 0.3821 0.4977 0.5236News article 1 0.7278 0.6400 0.5424 0.4336 0.5608 0.5734News article 1 (Lemmas) 0.7542 0.6400 0.5616 0.4943 0.5608 0.5911News article 2 0.6745 0.7140 0.5345 0.3660 0.5395 0.6751News article 2 (Lemmas) 0.6836 0.7140 0.5717 0.4083 0.5395 0.7778News article 3 0.6991 0.7986 0.5024 0.3016 0.5794 0.5988News article 3 (Lemmas) 0.7121 0.7986 0.5869 0.4801 0.6508 0.6204News article 4 0.6734 0.6556 0.5073 0.2408 0.6667 0.6295News article 4 (Lemmas) 0.6984 0.6333 0.6118 0.3790 0.6667 0.7545Table 2: Human Translation: Average HHI scores for words in the source and their aligned words (andlemmas) in the translations.
Scores are provided in the range of 0 to 1 and the highest score for eachdocument is highlighted in bold text.
The scores for rare verbs in Dracula and The Invisible Man are thesame for words and lemmas.
These documents contain very few repeated rare verbs (far fewer than theother English documents) and those that are repeated are very specific and diverse such that no differenceis seen between the two distributions.Noun (word) HHI score CountCount 0.9412 33driver 0.2985 28horses 0.9050 20room 0.4000 20time 0.1150 20door 0.5986 17place 0.6797 16night 0.4667 15Table 3: Dracula - most frequent noun wordsin which some artistic license may be used.These findings suggest that when aiming to en-courage consistency in the translation of novels,the focus should be on nouns.
As for adjectives,less frequent in novels than verbs and nouns (Table1), further analysis may show whether consistencyvaries depending on function (e.g.
modifier, pred-icate adjective) or frequency as well.
The transla-tion of pronouns also requires investigation.Natural Science: The natural science text Onthe Origin of Species exhibits a similar pattern oftranslational consistency to novels.
This is perhapsnot surprising as 19th century British natural sci-ence texts would have had the same middle-classaudience as the novels of the same era.
The trans-lation of modern scientific texts may or may notfollow this pattern.Instruction Manuals: In the Xerox Manualnouns receive the highest average HHI score atthe word level.
When considering what lemmasthe source words align to in the translation, nounsagain score the highest, closely followed by adjec-tives and rare verbs.
This overall pattern makessense as in an instruction manual it is important toidentify both the actions and entities involved ateach step.
Adjectives will help the user correctlyidentify the intended entities.
The word-level HHIscores for the most frequently used (and aligned)rare verbs are given in Table 4.The verb process has several translations inFrench: traitement (?treatment?/?processing?
),traiter (?process?)
and exe?cuter (?execute?).
(Note that traitement is in fact a noun, reflectinga change in the structure of the sentence.)
The14Rare Verb (word) HHI score Countprocess 0.5729 109previewing 0.5868 33previewed 0.6399 19verifying 0.5556 18formatted 0.3244 15scans 1.0000 13formatting 0.4380 11dithering 0.4380 11Table 4: Xerox Manual - most frequent rare verbwordsresulting translations into French are all clear, sothis may simply be a reflection of a difference interminology between English and French, at leastas used by Xerox.
For example:English: Process the page and save the output as an image.French: Traitement de la page et sauvegarde de la sortiecomme image.English: Page Settings enable you to describe the pages thatthe system is about to process.French: Les Instructions de page vous permettent de de?crireles pages que le syste`me va traiter.English: Load Verification Data, Loads a named verificationdata file to process a job.French: Charger donne?es de ve?rification, Charge un fichiernomme?
de donne?es de ve?rification pour exe?cuter une ta?che.What is also interesting is that in the English text,the word process is used as both a noun and a rareverb.
However, it is translated more consistentlywhen used as a verb (HHI: 0.5729) compared withits use as a noun (HHI: 0.2576).In this genre, accuracy and readability are im-portant and it is acceptable to produce a ?repeti-tive?
or ?boring?
text.
It may, therefore, be ap-propriate to encourage translational consistency ofnouns, rare verbs and adjectives in instructions.Unlike with novels, it would make sense that allentities in an instruction manual are of importance.Public Information: In the French Revolutionto 1945 and Nuclear Testing documents, adjectivesscore highest, followed by nouns.
Word-level HHIscores for the most frequent (and aligned) adjec-tives in the French Revolution to 1945 documentare presented in Table 5.Using a manual inspection of those nouns thatappear next to (i.e.
directly after) the adjective inFrench, the possibility that these nouns were se-mantically light was explored.
Focussing on theEnglish translation, WordNet (Miller, 1995) wasused to ascertain the distance of the noun from theroot of the relevant hierarchy.
The assumption isAdjective (word) HHI score Countnationale (national) 0.8233 75europe?enne (European) 0.8232 64e?conomique (economic) 0.8575 40constitutionnel (constitutional) 0.9474 37franc?aise (French) 0.4288 37constitutionnelle (constitutional) 1.0000 31franc?ais (French) 0.7899 26autres (other) 0.8496 25Table 5: French Revolution to 1945 - most fre-quent adjective wordsthe semantically light nouns appear closer to theroot than other nouns.
For all 82,115 noun synsetsin WordNet, the average minimum and maximumdepths to the root are 7.25 and 7.70 respectively.Taking the adjective economic (e?conomique inFrench) in the French Revolution to 1945 docu-ment as an example, the nouns it is paired with(e.g.
expansion, cooperation, development, ac-tion, council, etc.)
typically have depths belowthe average and therefore could be considered se-mantically light.
The adjectives used in the textinclude constitutionnel / constitutionnelle (?con-stitutional?
), e?conomique (?economic?)
and na-tionale (?national?).
These words are rather spe-cific (or ?semantically heavy?
), so there may befew alternative valid translations to choose from.This is supported by Melamed?s (1997) notion ofsemantic entropy, in which more specific words re-ceive lower entropy scores, reflecting greater con-sistency in translation.
For texts of this genre, itmay be appropriate to encourage the consistenttranslation of adjectives and nouns, allowing formore freedom in the translation of verbs.News Articles: The pattern for news articles isa little less predictable, although a similar pattern(to other document types) can be seen for light,mid-range and rare verbs.
This may be due to theshort length of the texts (circa 2,000 words) whichmay not be sufficient to establish a stable pattern.Or it may be that there are different writing styleswithin the news genre dependent on the type orsubject of the ?story?.4.2 Consistency in Automated TranslationThe results of a similar analysis of translationalconsistency in phrase-based SMT are presented inTable 6.
Overall, consistency is much higher thanin translations produced by human translators.
Butwhat does this mean?
Is the problem of consis-tency in SMT non-existent?
In short, no; there are15POS Category Xerox Manual Origin of Species Dracula The Invisible ManAutomated Human Automated Human Automated Human Automated HumanNoun 0.8502 0.6995 0.8481 0.6109 0.8318 0.6182 0.8308 0.6290Adj 0.6871 0.5900 0.6333 0.4390 0.6543 0.4191 0.6966 0.5110Verb (all) 0.7131 0.5568 0.6023 0.4001 0.5764 0.3631 0.5829 0.4159Light Verb 0.4919 0.3256 0.4538 0.2339 0.4310 0.2477 0.4873 0.3139Mid-Range Verb 0.7160 0.5766 0.5927 0.4140 0.6301 0.4175 0.6271 0.4797Rare Verb 0.8955 0.6485 0.8195 0.4592 0.8571 0.5000 0.8750 0.4218Table 6: Automated Translation: Average HHI scores taken for words in automated translations as com-pared with the scores from human translations.
Scores are provided in the range of 0 to 1still areas in which consistency is a real problem,but one needs to look more closely at the data tofind the problems.Any consistency in the output of an SMT sys-tem will be accidental, and not by design.
It is areflection of the data that the system was trainedwith and represents the ?best?
choice for translat-ing a word or phrase, as determined by scores fromthe phrase table and language model.
Carpuatand Simard (2012) suggest that consistency in thesource side local context may be sufficient to con-strain the phrase table and language model to pro-duce consist translations.
It is also important tonote that the outcome is very much dependenton the system used to perform the translation.Carpuat and Simard (2012) suggest that weakerSMT systems (i.e.
those that report lower BLEUscores) may be more consistent than their strongercounterparts due to fewer translation options.There are several possibilities.
A word in thesource language may be translated:?
Completely consistently (HHI = 1);?
Very inconsistently (HHI ?
0);?
or anywhere in betweenAdditionally, a translation that is deemed to becompletely consistent may be either correct or in-correct.
With humans, we assume the translationoutput to be of a high standard but we cannot as-sume the same of an SMT system.Examples of completely consistent translationsare horses as ?chevaux?, man as ?homme?
andnails as ?clous?.
All are taken from Dracula.While horses and man are translated correctly,?clous?
is an incorrect translation of nails whichthe context of the novel refer to Dracula?s finger-nails.
?ongles?
would have been the correct trans-lation.
The word ?clous?
is typically used in thesense of nails used in construction.
This is an ex-ample of a translation that could result either fromlack of sufficient local context (for disambigua-tion) or because ?ongles?
is not present in the datathe SMT system was trained on.Examples of inconsistent translations are for thebody parts arm and hand in the text of Drac-ula.
arm is translated either correctly as ?bras?
(arm, body part) or incorrectly as ?armer?
(theverb ?to arm?).
hand is translated correctly as?main?
(?hand?)
and incorrectly as co?te?
(?side?
)and ?part?
(?portion?).
In both cases, the correcttranslation was available to the system and a moreaccurate translation could have been obtained hadthe correct translation been identified and its con-sistency encouraged.Ambiguous words in particular can cause trou-ble for SMT systems.
There are many words thatcan function as both a verb and a noun, e.g.
pro-cess and count.
Local context might not alwaysbe sufficient to provide the correct disambigua-tion, resulting in opportunities for incorrect trans-lations.An example of where an ambiguous wordresults in problems is in the translation ofcount (i.e.
Count Dracula) as: omitted (4),?compter?
(21), ?compatage?
(2), ?comte?
(1) and?de?pouillement?
(5).
The only acceptable transla-tion from this set is ?comte?.
As for the reamingoptions: ?compte?
and ?comptage?
are both verbsmeaning ?to count?
and ?de?pouillement?
is a nounmeaning ?starkness?, ?austerity?
or ?analysis?
(ofdata).5 ConclusionThe analysis of human translation presented in thispaper is a first attempt to understand where andwhen it might be appropriate to encourage con-sistency in an SMT system.
I consider genre asthe where and parts-of-speech as the when, butother interpretations are also possible.
On thewhole, it seems reasonable to encourage the con-16sistent translation of nouns, across all genres.
Inaddition, encouraging consistency in the transla-tion of rare verbs and adjectives for technical doc-uments and of adjectives for public informationdocuments may also prove beneficial.With respect to verbs, variation in verb consis-tency has been shown to correlate with frequency(as a proxy to identify light and rare verbs).
Giventhe low consistency with which humans translatelight verbs, encouraging their consistency in auto-mated translation would be undesirable.Automated translation may look very consis-tent on the surface, but it is necessary to look be-yond this to see the errors.
While humans maymake inconsistent translations, we trust that theseinconsistencies will not confuse or mislead thereader.
SMT systems on the other hand gener-ate their translations based on statistics that saywhat the ?best choice?
might be, both at theword/phrase level (through the phrase table) andoverall (through the language model).
Further-more, they do nothing to guarantee consistency -this occurs by chance, whether desirable or not.As a result, inconsistencies may arise that makethe translations difficult to read.
These inconsis-tencies are not predictable and could occur in anySMT system.6 Future WorkThe findings presented in this paper are sugges-tive but only a small number of texts have beenincluded for each genre.
The analysis could beextended to include a larger set of documents anddifferent language pairs (the only requirement isfor a POS tagger for the source language).
Multi-ple translations of the same document could alsobe considered to identify whether similar patternscan be observed for different translators.There are a number of possible ways in whichto use this information to inform the design of aSMT system.
I have shown that SMT systemsare capable of highly consistent translations butthis consistency cannot be guaranteed and thereis the possibility that the translations will be con-sistent and incorrect.
Also, Carpuat and Simard(2012) have shown that inconsistent translationsin SMT often indicate translation errors.
A sys-tem which encourages translations which are bothconsistent and correct (or at least acceptable) forwords that belong to a predefined set (e.g.
by POStag) is desirable.
This ?encouragement?
could beachieved using rewards delivered via feature func-tions or within n-best list re-ranking ?
hypothe-ses which make re-use of the same translation(s)for repetitions of the same source word would beranked higher than those that introduced incon-sistencies.
Revisiting the cache-based models of(2010a; 2010b) and Gong et al(2011) could pro-vide a possible starting point.The initial focus could be on nouns, which aretranslated by human translators with high consis-tency for all genres.
Many nouns are used either tospecify entities that are only mentioned once in atext (essentially setting the scene for more promi-nent entities), or as ?predicate nominals?
on thosemore prominent entities (e.g.
in ?...is a horrificstory?).
However, other nouns occur within theNoun Phrases (NPs) that make up part of a corefer-ence chain, of subsequent reference to prominententities.As an extension to this work I will aim to inves-tigate the consistency of translation of those nounsthat belong to coreference chains and ultimately,to build a system that makes use of the resulting in-formation.
Work has already started to construct aparallel corpus in which coreference chains are an-notated so that the translation of coreference (bothNPs and pronouns) may be studied in more depth.Another question worth considering is whetherit would be desirable to replicate aspects of lowconsistency in human translation by encouraginginconsistent (but still acceptable) translations ofcertain words or word categories.
My instinct isthat this could lead to translations that better ap-proximate those produced by humans.7 AcknowledgementsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement 287658 (EU BRIDGE).
Thanks to Pro-fessor Bonnie Webber for her guidance and nu-merous helpful suggestions and to the three anony-mous reviewers for their feedback.ReferencesAnne Abeille?, Lionel Cle?ment, and Alexandra Kinyon.2000.
Building a treebank for french.
In In Pro-ceedings of the LREC 2000.Marine Carpuat and Michel Simard.
2012.
The troublewith smt consistency.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, WMT17?12, pages 442?449, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Marine Carpuat.
2009.
One translation per discourse.In Proceedings of the Workshop on Semantic Evalu-ations: Recent Achievements and Future Directions,DEW ?09, pages 19?27, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Jeremy H. Clear.
1993.
The digital word.
chapter TheBritish national corpus, pages 163?187.
MIT Press,Cambridge, MA, USA.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.
In Pro-ceedings of the workshop on Speech and NaturalLanguage, HLT ?91, pages 233?237, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Zhengxian Gong, Min Zhang, and Guodong Zhou.2011.
Cache-based document-level statistical ma-chine translation.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing, EMNLP ?11, pages 909?919, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Michael A.K.
Halliday and Ruqaiya Hasan.
1976.
Co-hesion in English.
Longman, London.Yifan He, Yanjun Ma, Andy Way, and Josef van Gen-abith.
2011.
Rich linguistic features for transla-tion memory-inspired consistent translatio.
In Pro-ceedings of Machine Translation Summit XIII, pages456?463.Masaki Itagaki, Takako Aikawa, and Xiaodon He.2007.
Automatic validation of terminology consis-tency with statistical method.
In Proceedings of Ma-chine Translation Summit XI, pages 269?274.
Euro-pean Associaton for Machine Translation.Philipp Koehn.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Sum-mit, pages 79?86.
AAMT, AAMT.Kazem Lotfipour-Saedi.
1997.
Lexical cohesion andtranslation equivalence.
Meta: Journal des Traduc-teurs / Meta: Translators?
Journal, 42(1):185?192.Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-abith.
2011.
Consistent translation using discrim-inative learning: a translation memory-inspired ap-proach.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1,HLT ?11, pages 1239?1248, Stroudsburg, PA, USA.Association for Computational Linguistics.I.
Dan Melamed.
1997.
Measuring semantic entropy.In Proceedings of the SIGLEX Workshop on TaggingText with Lexical Semantics, pages 41?46.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
Commun.
ACM, 38(11):39?41, Novem-ber.Jane Morris and Graeme Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indicator ofthe structure of text.
Comput.
Linguist., 17(1):21?48, March.Franz Josef Och and Hermann Ney.
2003.
Asystematic comparison of various statistical align-ment models.
Computational Linguistics, 29:19?51,March.Slav Petrov, Dipanjan Das, and Ryan McDonald.2011.
A universal part-of-speech tagset.
In INARXIV:1104.2086.Raphael Salkie.
2010.
The intersect trans-lation corpus.
Available on the web:http://arts.brighton.ac.uk/staff/raf-salkie/portfolio-of-major-works/intersect.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings ofInternational Conference on New Methods in Lan-guage Processing.Jo?rg Tiedemann.
2010a.
Context adaptation in sta-tistical machine translation using models with ex-ponentially decaying cache.
In Proceedings of the2010 Workshop on Domain Adaptation for NaturalLanguage Processing, DANLP 2010, pages 8?15,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Jo?rg Tiedemann.
2010b.
To cache or not to cache?experiments with adaptive models in statistical ma-chine translation.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation andMetrics MATR, pages 189?194, Uppsala, Sweden,July.
Association for Computational Linguistics.Ferhan Ture, Douglas W. Oard, and Philip Resnik.2012.
Encouraging consistent translation choices.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 417?426, Stroudsburg, PA,USA.
Association for Computational Linguistics.Tong Xiao, Jingbo Zhu, and Shujie Yao.
2011.Document-level consistency verification in machinetranslation.
In Proceedings of MT summit XIII,pages 131?138.18
