Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 208?215,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Training and Evaluating Error Minimization Rules for Statistical MachineTranslationAshish VenugopalSchool of Computer ScienceCarnegie Mellon Universityarv@andrew.cmu.eduAndreas ZollmannSchool of Computer ScienceCarnegie Mellon Universityzollmann@cs.cmu.eduAlex WaibelSchool of Computer ScienceCarnegie Mellon Universitywaibel@cs.cmu.eduAbstractDecision rules that explicitly account fornon-probabilistic evaluation metrics inmachine translation typically require spe-cial training, often to estimate parame-ters in exponential models that govern thesearch space and the selection of candi-date translations.
While the traditionalMaximum A Posteriori (MAP) decisionrule can be optimized as a piecewise lin-ear function in a greedy search of the pa-rameter space, the Minimum Bayes Risk(MBR) decision rule is not well suited tothis technique, a condition that makes pastresults difficult to compare.
We present anovel training approach for non-tractabledecision rules, allowing us to compare andevaluate these and other decision rules ona large scale translation task, taking ad-vantage of the high dimensional parame-ter space available to the phrase basedPharaoh decoder.
This comparison istimely, and important, as decoders evolveto represent more complex search spacedecisions and are evaluated against in-novative evaluation metrics of translationquality.1 IntroductionState of the art statistical machine translation takesadvantage of exponential models to incorporate alarge set of potentially overlapping features to se-lect translations from a set of potential candidates.As discussed in (Och, 2003), the direct translationmodel represents the probability of target sentence?English?
e = e1 .
.
.
eI being the translation for asource sentence ?French?
f = f1 .
.
.
fJ through anexponential, or log-linear modelp?
(e|f) =exp(?mk=1 ?k ?
hk(e, f))?e?
?E exp(?mk=1 ?k ?
hk(e?, f))(1)where e is a single candidate translation for ffrom the set of all English translations E, ?
is theparameter vector for the model, and each hk is afeature function of e and f .
In practice, we restrictE to the set Gen(f) which is a set of highly likelytranslations discovered by a decoder (Vogel et al,2003).
Selecting a translation from this model underthe Maximum A Posteriori (MAP) criteria yieldstransl?
(f) = argmaxep?
(e|f) .
(2)This decision rule is optimal under the zero-one loss function, minimizing the Sentence ErrorRate (Mangu et al, 2000).
Using the log-linearform to model p?
(e|f) gives us the flexibility tointroduce overlapping features that can representglobal context while decoding (searching the spaceof candidate translations) and rescoring (ranking aset of candidate translations before performing theargmax operation), albeit at the cost of the tradi-tional source-channel generative model of transla-tion proposed in (Brown et al, 1993).A significant impact of this paradigm shift, how-ever, has been the movement to leverage the flex-ibility of the exponential model to maximize per-formance with respect to automatic evaluation met-208rics.
Each evaluation metric considers different as-pects of translation quality, both at the sentence andcorpus level, often achieving high correlation to hu-man evaluation (Doddington, 2002).
It is clear thatthe decision rule stated in (1) does not reflect thechoice of evaluation metric, and substantial workhas been done to correct this mismatch in crite-ria.
Approaches include integrating the metric intothe decision rule, and learning ?
to optimize theperformance of the decision rule.
In this paperwe will compare and evaluate several aspects ofthese techniques, focusing on Minimum Error Rate(MER) training (Och, 2003) and Minimum BayesRisk (MBR) decision rules, within a novel trainingenvironment that isolates the impact of each compo-nent of these methods.2 Addressing Evaluation MetricsWe now describe competing strategies to address theproblem of modeling the evaluation metric withinthe decoding and rescoring process, and introduceour contribution towards training non-tractable errorsurfaces.
The methods discussed below make useof Gen(f), the approximation to the complete can-didate translation space E, referred to as an n-bestlist.
Details regarding n-best list generation fromdecoder output can be found in (Ueffing et al, 2002).2.1 Minimum Error Rate TrainingThe predominant approach to reconciling the mis-match between the MAP decision rule and the eval-uation metric has been to train the parameters ?
ofthe exponential model to correlate the MAP choicewith the maximum score as indicated by the evalu-ation metric on a development set with known ref-erences (Och, 2003).
We differentiate between thedecision ruletransl?
(f) = argmaxe?Gen(f)p?
(e|f) (3a)and the training criterion??
= argmin?Loss(transl?
(~f),~r) (3b)where the Loss function returns an evaluation re-sult quantifying the difference between the Englishcandidate translation transl?
(f) and its correspond-ing reference r for a source sentence f .
We indicatethat this loss function is operating on a sequence ofsentences with the vector notation.
To avoid overfit-ting, and since MT researchers are generally blessedwith an abundance of data, these sentences are froma separate development set.The optimization problem (3b) is hard since theargmax of (3a) causes the error surface to changein steps in Rm, precluding the use of gradient basedoptimization methods.
Smoothed error counts canbe used to approximate the argmax operator, but theresulting function still contains local minima.
Grid-based line search approaches like Powell?s algorithmcould be applied but we can expect difficultly whenchoosing the appropriate grid size and starting pa-rameters.
In the following, we summarize the opti-mization algorithm for the unsmoothed error countspresented in (Och, 2003) and the implementation de-tailed in (Venugopal and Vogel, 2005).?
Regard Loss(transl?
(~f),~r) as defined in (3b)as a function of the parameter vector ?
tooptimize and take the argmax to computetransl?
(~f) over the translations Gen(f) accord-ing to the n-best list generated with an initialestimate ?0.?
The error surface defined by Loss (as a func-tion of ?)
is piecewise linear with respect to asingle model parameter ?k, hence we can deter-mine exactly where it would be useful (valuesthat change the result of the argmax) to evalu-ate ?k for a given sentence using a simple lineintersection method.?
Merge the list of useful evaluation pointsfor ?k and evaluate the corpus levelLoss(transl?
(~f),~r) at each one.?
Select the model parameter that represents thelowest Loss as k varies, set ?k and consider theparameter ?j for another dimension j.This training algorithm, referred to as minimum er-ror rate (MER) training, is a greedy search in eachdimension of ?, made efficient by realizing thatwithin each dimension, we can compute the pointsat which changes in ?
actually have an impact onLoss.
The appropriate considerations for termina-tion and initial starting points relevant to any greedysearch procedure must be accounted for.
From the209nature of the training procedure and the MAP de-cision rule, we can expect that the parameters se-lected by MER training will strongly favor a fewtranslations in the n-best list, namely for each sourcesentence the one resulting in the best score, movingmost of the probability mass towards the translationthat it believes should be selected.
This is due to thedecision rule, rather than the training procedure, aswe will see when we consider alternative decisionrules.2.2 The Minimum Bayes Risk Decision RuleThe Minimum Bayes Risk Decision Rule as pro-posed by (Mangu et al, 2000) for the Word ErrorRate Metric in speech recognition, and (Kumar andByrne, 2004) when applied to translation, changesthe decision rule in (2) to select the translation thathas the lowest expected loss E[Loss(e, r)], whichcan be estimated by considering a weighted Lossbetween e and the elements of the n-best list, theapproximation to E, as described in (Mangu et al,2000).
The resulting decision rule is:transl?
(f) = argmine?Gen(f)?e?
?Gen(f)Loss(e, e?)p?
(e?|f) .
(4)(Kumar and Byrne, 2004) explicitly consider select-ing both e and a, an alignment between the Eng-lish and French sentences.
Under a phrase basedtranslation model (Koehn et al, 2003; Marcu andWong, 2002), this distinction is important and willbe discussed in more detail.
The representation ofthe evaluation metric or the Loss function is in thedecision rule, rather than in the training criterion forthe exponential model.
This criterion is hard to op-timize for the same reason as the criterion in (3b):the objective function is not continuous in ?.
Tomake things worse, it is more expensive to evalu-ate the function at a given ?, since the decision ruleinvolves a sum over all translations.2.3 MBR and the Exponential ModelPrevious work has reported the success of the MBRdecision rule with fixed parameters relating indepen-dent underlying models, typically including only thelanguage model and the translation model as fea-tures in the exponential model.We extend the MBR approach by developing atraining method to optimize the parameters ?
in theexponential model as an explicit form for the condi-tional distribution in equation (1).
The training taskunder the MBR criterion is??
= argmin?Loss(transl?
(~f),~r) (5a)wheretransl?
(f) = argmine?Gen(f)?e?
?Gen(f)Loss(e, e?)p?
(e?|f) .
(5b)We begin with several observations about this opti-mization criterion.?
The MAP optimal ??
are not the optimal para-meters for this training criterion.?
We can expect the error surface of the MBRtraining criterion to contain larger sections ofsimilar altitude, since the decision rule empha-sizes consensus.?
The piecewise linearity observation made in(Papineni et al, 2002) is no longer applicablesince we cannot move the log operation into theexpected value.3 Score SamplingMotivated by the challenges that the MBR trainingcriterion presents, we present a training method thatis based on the assumption that the error surface islocally non-smooth but consists of local regions ofsimilar Loss values.
We would like to focus thesearch within regions of the parameter space that re-sult in low Loss values, simulating the effect thatthe MER training process achieves when it deter-mines the merged error boundaries across a set ofsentences.Let Score(?)
be some function ofLoss(transl?
(~f),~r) that is greater or equalzero, decreases monotonically with Loss, and forwhich?(Score(?)
?
min??
Score(??))d?
is finite;e.g., 1 ?
Loss(transl?
(~f),~r) for the word-errorrate (WER) loss and a bounded parameter space.While sampling parameter vectors ?
and estimatingLoss in these points, we will constantly refineour estimate of the error surface and thereby ofthe Score function.
The main idea in our score210sampling algorithm is to make use of this Scoreestimate by constructing a probability distributionover the parameter space that depends on the Scoreestimate in the current iteration step i and samplethe parameter vector ?i+1 for the next iteration fromthat distribution.
More precisely, let S?c(i)be theestimate of Score in iteration i (we will explain howto obtain this estimate below).
Then the probabilitydistribution from which we sample the parametervector to test in the next iteration is given by:p(?)
=S?c(i)(?)
?
min??
S?c(i)(??)?(S?c(i)(?)
?
min??
S?c(i)(??))
d?.
(6)This distribution produces a sequence ?1, .
.
.
, ?n ofparameter vectors that are more concentrated in ar-eas that result in a high Score.
We can select thevalue from this sequence that generates the highestScore, just as in the MER training process.The exact method of obtaining the Score estimateS?c is crucial: If we are not careful enough and guesstoo low values of S?c(?)
for parameter regions thatare still unknown to us, the resulting sampling dis-tribution p might be zero in those regions and thuspotentially optimal parameters might never be sam-pled.
Rather than aiming for a consistent estimatorof Score (i.e., an estimator that converges to Scorewhen the sample size goes to infinity), we design S?cwith regard to yielding a suitable sampling distribu-tion p.Assume that the parameter space is bounded suchthat mink ?
?k ?
maxk for each dimension k,We then define a set of pivots P , forming a grid ofpoints in Rm that are evenly spaced between minkand maxk for each dimension k. Each pivot repre-sents a region of the parameter space where we ex-pect generally consistent values of Score.
We do notrestrict the values of ?m to be at these pivot pointsas a grid search would do, rather we treat the pivotsas landmarks within the search space.We approximate the distribution p(?)
with thediscrete distribution p(?
?
P), leaving the problemof estimating |P| parameters.
Initially, we set p tobe uniform, i.e., p(0)(?)
= 1/|P|.
For subsequentiterations, we now need an estimate of Score(?)
foreach pivot ?
?
P in the discrete version of equation(6) to obtain the new sampling distribution p. Eachiteration i proceeds as follows.?
Sample ?
?i from the discrete distributionp(i?1)(?
?
P) obtained by the previous it-eration.?
Sample the new parameter vector ?i by choos-ing for each k ?
{1, .
.
.
,m}, ?ik := ?
?ik + ?k,where ?k is sampled uniformly from the inter-val (?dk/2, dk/2) and dk is the distance be-tween neighboring pivot points along dimen-sion k. Thus, ?i is sampled from a regionaround the sampled pivot.?
Evaluate Score(?i) and distribute this score toobtain new estimates S?c(i)(?)
for all pivots ?
?P as described below.?
Use the updated estimates S?c(i)to generate thesampling distribution p(i) for the next iterationaccording top(i)(?)
=S?c(i)(?)
?
min??
S?c(i)(??)???P(S?c(i)(?)
?
min??
S?c(i)(??
)).The score Score(?i) of the currently evaluated pa-rameter vector does not only influence the score es-timate at the pivot point of the respective region, butthe estimates at all pivot points.
The closest piv-ots are influenced most strongly.
More precisely, foreach pivot ?
?
P , S?c(i)(?)
is a weighted averageof Score(?1), .
.
.
, Score(?i), where the weightsw(i)(?)
are chosen according tow(i)(?)
= infl(i)(?)
?
corr(i)(?)
withinfl(i)(?)
= mvnpdf(?, ?i,?)
andcorr(i)(?)
= 1/p(i?1)(?)
.Here, mvnpdf(x, ?,?)
denotes the m-dimensionalmultivariate-normal probability density functionwith mean ?
and covariance matrix ?, evaluatedat point x.
We chose the covariance matrix ?
=diag(d21, .
.
.
, d2m), where again dk is the distance be-tween neighboring grid points along dimension k.The term infl(i)(?)
quantifies the influence of theevaluated point ?i on the pivot ?, while corr(i)(?
)is a correction term for the bias introduced by hav-ing sampled ?i from p(i?1).211Smoothing uncertain regions In the beginning ofthe optimization process, there will be pivot regionsthat have not yet been sampled from and for whichnot even close-by regions have been sampled yet.This will be reflected in the low sum of influenceterms infl(1)(?)
+ ?
?
?
+ infl(i)(?)
of the respectivepivot points ?.
It is therefore advisable to discountsome probability mass from p(i) and distribute itover pivots with low influence sums (reflecting lowconfidence in the respective score estimates) accord-ing to some smoothing procedure.4 N-Best lists in Phrase Based DecodingThe methods described above make extensive use ofn-best lists to approximate the search space of can-didate translations.
In phrase based decoding we of-ten interpret the MAP decision rule to select the topscoring path in the translation lattice.
Selecting aparticular path means in fact selecting the pair ?e, s?,where s is a segmentation of the the source sentencef into phrases and alignments onto their translationsin e. Kumar and Byrne (2004) represent this deci-sion explicitly, since the Loss metrics considered intheir work evaluate alignment information as well aslexical (word) level output.
When considering lexi-cal scores as we do here, the decision rule minimiz-ing 0/1 loss actually needs to take the sum over allpotential segmentations that can generate the sameword sequence.
In practice, we only consider thehigh probability segmentation decisions, namely theones that were found in the n-best list.
This givesthe 0/1 loss criterion shown below.transl?
(f) = argmaxe?sp?
(e, s|f) (7)The 0/1 loss criterion favors translations that aresupported by several segmentation decisions.
In thecontext of phrase-based translations, this is a usefulcriterion, since a given lexical target word sequencecan be correctly segmented in several different ways,all of which would be scored equally by an evalua-tion metric that only considers the word sequence.5 Experimental FrameworkOur goal is to evaluate the impact of the three de-cision rules discussed above on a large scale trans-lation task that takes advantage of multidimensionalfeatures in the exponential model.
In this sectionwe describe the experimental framework used in thisevaluation.5.1 Data Sets and ResourcesWe perform our analysis on the data provided by the2005 ACL Workshop in Exploiting Parallel Texts forStatistical Machine Translation, working with theFrench-English Europarl corpus.
This corpus con-sists of 688031 sentence pairs, with approximately156 million words on the French side, and 138 mil-lion words on the English side.
We use the data asprovided by the workshop and run lower casing asour only preprocessing step.
We use the 15.5 mil-lion entry phrase translation table as provided for theshared workshop task for the French-English dataset.
Each translation pair has a set of 5 associatedphrase translation scores that represent the maxi-mum likelihood estimate of the phrase as well as in-ternal alignment probabilities.
We also use the Eng-lish language model as provided for the shared task.Since each of these decision rules has its respectivetraining process, we split the workshop test set of2000 sentences into a development and test set usingrandom splitting.
We tried two decoders for trans-lating these sets.
The first system is the Pharaoh de-coder provided by (Koehn et al, 2003) for the shareddata task.
The Pharaoh decoder has support for mul-tiple translation and language model scores as wellas simple phrase distortion and word length models.The pruning and distortion limit parameters remainthe same as in the provided initialization scripts,i.e., DistortionLimit = 4, BeamThreshold =0.1, Stack = 100.
For further information onthese parameter settings, confer (Koehn et al, 2003).Pharaoh is interesting for our optimization task be-cause its eight different models lead to a searchspace with seven free parameters.
Here, a princi-pled optimization procedure is crucial.
The seconddecoder we tried is the CMU Statistical TranslationSystem (Vogel et al, 2003) augmented with the fourtranslation models provided by the Pharaoh system,in the following called CMU-Pharaoh.
This systemalso leads to a search space with seven free parame-ters.2125.2 N-Best listsAs mentioned earlier, the model parameters ?
playa large role in the search space explored by a prun-ing beam search decoder.
These parameters affectthe histogram and beam pruning as well as the fu-ture cost estimation used in the Pharaoh and CMUdecoders.
The initial parameter file for Pharaoh pro-vided by the workshop provided a very poor esti-mate of ?, resulting in an n-best list of limited po-tential.
To account for this condition, we ran Min-imum Error Rate training on the development datato determine scaling factors that can generate a n-best list with high quality translations.
We realizethat this step biases the n-best list towards the MAPcriteria, since its parameters will likely cause moreaggressive pruning.
However, since we have cho-sen a large N=1000, and retrain the MBR, MAP, and0/1 loss parameters separately, we do not feel thatthe bias has a strong impact on the evaluation.5.3 Evaluation MetricThis paper focuses on the BLEU metric as presentedin (Papineni et al, 2002).
The BLEU metric is de-fined on a corpus level as follows.Score(~e,~r) = BP (~e,~r) ?
exp(1NN?1(log pn))where pn represent the precision of n-grams sug-gested in ~e and BP is a brevity penalty measur-ing the relative shortness of ~e over the whole cor-pus.
To use the BLEU metric in the candidate pair-wise loss calculation in (4), we need to make a de-cision regarding cases where higher order n-gramsmatches are not found between two candidates.
Ku-mar and Byrne (2004) suggest that if any n-gramsare not matched then the pairwise BLEU score is setto zero.
As an alternative we first estimate corpus-wide n-gram counts on the development set.
Whenthe pairwise counts are collected between sentencespairs, they are added onto the baseline corpus countsto and scored by BLEU.
This scoring simulates theprocess of scoring additional sentences after seeinga whole corpus.5.4 Training EnvironmentIt is important to separate the impact of the decisionrule from the success of the training procedure.
Toappropriately compare the MAP, 0/1 loss and MBRdecisions rules, they must all be trained with thesame training method, here we use the Score Sam-pling training method described above.
We also re-port MAP scores using the MER training describedabove to determine the impact of the training algo-rithm for MAP.
Note that the MER training approachcannot be performed on the MBR decision rule, asexplained in Section 2.3.
MER training is initializedat random values of ?
and run (successive greedysearch over the parameters) until there is no changein the error for three complete cycles through the pa-rameter set.
This process is repeated with new start-ing parameters as well as permutations of the para-meter search order to ensure that there is no bias inthe search towards a particular parameter.
To im-prove efficiency, pairwise scores are cached acrossrequests for the score at different values of ?, andfor MBR only the E[Loss(e, r)] for the top twentyhypotheses as ranked by the model are computed.6 ResultsThe results in Table 1 compare the BLEU scoreachieved by each training method on the develop-ment and test data for both Pharaoh and CMU-Pharaoh.
Score-sampling training was run for 150iterations to find ?
for each decision rule.
The MAP-MER training was performed to evaluate the effectof the greedy search method on the generalizationof the development set results.
Each row representsan alternative training method described in this pa-per, while the test set columns indicate the criteriaused to select the final translation output ~e.
Thebold face scores are the scores for matching train-ing and testing methods.
The underlined score isthe highest test set score, achieved by MBR decod-ing using the CMU-Pharaoh system trained for theMBR decision rule with the score-sampling algo-rithm.
When comparing MER training for MAP-decoding with score-sampling training for MAP-decoding, score-sampling surprisingly outperformsMER training for both Pharaoh and CMU-Pharaoh,although MER training is specifically tailored tothe MAP metric.
Note, however, that our score-sampling algorithm has a considerably longer run-ning time (several hours) than the MER algorithm(several minutes).
Interestingly, within MER train-213training method Dev.
set sc.
test set sc.
MAP test set sc.
0/1 loss test set sc.
MBRMAP MER (Pharaoh) 29.08 29.30 29.42 29.36MAP score-sampl.
(Pharaoh) 29.08 29.41 29.24 29.300/1 loss sc.-s. (Pharaoh) 29.08 29.16 29.28 29.30MBR sc.-s. (Pharaoh) 29.00 29.11 29.08 29.17MAP MER (CMU-Pharaoh) 28.80 29.02 29.41 29.60MAP sc.-s.
(CMU-Ph.)
29.10 29.85 29.75 29.550/1 loss sc.-s.
(CMU-Ph.)
28.36 29.97 29.91 29.72MBR sc.-s.
(CMU-Ph.)
28.36 30.18 30.16 30.28Table 1.
Comparing BLEU scores generated by alternative training methods and decision rulesing for Pharaoh, the 0/1 loss metric is the top per-former; we believe the reason for this disparity be-tween training and test methods is the impact ofphrasal consistency as a valuable measure within then-best list.The relative performance of MBR score-samplingw.r.t.
MAP and 0/1-loss score sampling is quite dif-ferent between Pharaoh and CMU-Pharaoh: WhileMBR score-sampling performs worse than MAPand 0/1-loss score sampling for Pharaoh, it yields thebest test scores across the board for CMU-Pharaoh.A possible reason is that the n-best lists generated byPharaoh have a large percentage of lexically iden-tical translations, differing only in their segmenta-tions.
As a result, the 1000-best lists generated byPharaoh contain only a small percentage of uniquetranslations, a condition that reduces the potentialof the Minimum Bayes Risk methods.
The CMUdecoder, contrariwise, prunes away alternatives be-low a certain score-threshold during decoding anddoes not recover them when generating the n-bestlist.
The n-best lists of this system are therefore typi-cally more diverse and in particular contain far moreunique translations.7 Conclusions and Further WorkThis work describes a general algorithm for the ef-ficient optimization of error counts for an arbitraryLoss function, allowing us to compare and evalu-ate the impact of alternative decision rules for sta-tistical machine translation.
Our results suggestthe value and sensitivity of the translation processto the Loss function at the decoding and reorder-ing stages of the process.
As phrase-based trans-lation and reordering models begin to dominatethe state of the art in machine translation, it willbecome increasingly important to understand thenature and consistency of n-best list training ap-proaches.
Our results are reported on a completepackage of translation tools and resources, allow-ing the reader to easily recreate and build upon ourframework.
Further research might lie in findingefficient representations of Bayes Risk loss func-tions within the decoding process (rather than justusing MBR to rescore n-best lists), as well asanalyses on different language pairs from the avail-able Europarl data.
We have shown score sam-pling to be an effective training method to con-duct these experiments and we hope to establish itsuse in the changing landscape of automatic trans-lation evaluation.
The source code is available at:www.cs.cmu.edu/?zollmann/scoresampling/8 AcknowledgmentsWe thank Stephan Vogel, Ying Zhang, and theanonymous reviewers for their valuable commentsand suggestions.ReferencesPeter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2):263?311.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In In Proc.
ARPA Workshop on Human Lan-guage Technology.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the Human Language Technology and North214American Association for Computational LinguisticsConference (HLT/NAACL), Edomonton, Canada, May27-June 1.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine translation.In Proceedings of the Human Language Technologyand North American Association for ComputationalLinguistics Conference (HLT/NAACL), Boston,MA,May 27-June 1.Lidia Mangu, Eric Brill, and Andreas Stolcke.
2000.Finding consensus in speech recognition: word errorminimization and other applications of confusion net-works.
CoRR, cs.CL/0010012.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proc.
of the Conference on Empirical Meth-ods in Natural Language Processing, Philadephia, PA,July 6-7.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proc.
of the Associ-ation for Computational Linguistics, Sapporo, Japan,July 6-7.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of theAssociation of Computational Linguistics, pages 311?318.Nicola Ueffing, Franz Josef Och, and Hermann Ney.2002.
Generation of word graphs in statistical ma-chine translation.
In Proc.
of the Conference onEmpirical Methods in Natural Language Processing,Philadephia, PA, July 6-7.Ashish Venugopal and Stephan Vogel.
2005.
Consider-ations in mce and mmi training for statistical machinetranslation.
In Proceedings of the Tenth Conferenceof the European Association for Machine Translation(EAMT-05), Budapest, Hungary, May.
The EuropeanAssociation for Machine Translation.Stephan Vogel, Ying Zhang, Fei Huang, Alicia Trib-ble, Ashish Venogupal, Bing Zhao, and Alex Waibel.2003.
The CMU statistical translation system.
In Pro-ceedings of MT Summit IX, New Orleans, LA, Septem-ber.215
