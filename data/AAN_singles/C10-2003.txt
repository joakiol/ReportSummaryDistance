Coling 2010: Poster Volume, pages 18?26,Beijing, August 2010Cross-Market Model Adaptation with Pairwise Preference Data forWeb Search RankingJing BaiMicrosoft Bing1065 La AvenidaMountain View, CA 94043jbai@microsoft.comFernando Diaz, Yi Chang, Zhaohui ZhengYahoo!
Labs701 First AvenueSunnyvale, CA 94089{diazf,yichang,zhaohui}@yahoo-inc.comKeke ChenComputer ScienceWright StateDayton, Ohio 45435keke.chen@wright.eduAbstractMachine-learned ranking techniques au-tomatically learn a complex documentranking function given training data.These techniques have demonstrated theeffectiveness and flexibility required of acommercial web search.
However, man-ually labeled training data (with multipleabsolute grades) has become the bottle-neck for training a quality ranking func-tion, particularly for a new domain.
Inthis paper, we explore the adaptation ofmachine-learned ranking models acrossa set of geographically diverse marketswith the market-specific pairwise prefer-ence data, which can be easily obtainedfrom clickthrough logs.
We proposea novel adaptation algorithm, Pairwise-Trada, which is able to adapt rankingmodels that are trained with multi-gradelabeled training data to the target mar-ket using the target-market-specific pair-wise preference data.
We present resultsdemonstrating the efficacy of our tech-nique on a set of commercial search en-gine data.1 IntroductionWeb search algorithms provide methods forranking web scale collection of documents givena short query.
The success of these algorithmsoften relies on the rich set of document prop-erties or features and the complex relationshipsbetween them.
Increasingly, machine learn-ing techniques are being used to learn theserelationships for an effective ranking function(Liu, 2009).
These techniques use a set of la-beled training data labeled with multiple rele-vance grades to automatically estimate parame-ters of a model which directly optimizes a per-formance metric.
Although training data oftenis derived from editorial labels of document rel-evance, it can also be inferred from a carefulanalysis of users?
interactions with a workingsystem (Joachims, 2002).
For example, in websearch, given a query, document preference in-formation can be derived from user clicks.
Thisdata can then be used with an algorithm whichlearns from pairwise preference data (Joachims,2002; Zheng et al, 2007).
However, automati-cally extracted pairwise preference data is sub-ject to noise due to the specific sampling meth-ods used (Joachims et al, 2005; Radlinski andJoachim, 2006; Radlinski and Joachim, 2007).One of the fundamental problems for a websearch engine with global reach is the develop-ment of ranking models for different regionalmarkets.
While the approach of training a singlemodel for all markets is attractive, it fails to fullyexploit of specific properties of the markets.
Onthe other hand, the approach of training market-specific models requires the huge overhead ofacquiring a large training set for each market.As a result, techniques have been developed tocreate a model for a small market, say a South-east Asian country, by combining a strong modelin another market, say the United States, with a18small amount of manually labeled training datain the small market (Chen et al, 2008b).
How-ever, the existing Trada method takes only multi-grade labeled training data for adaptation, mak-ing it impossible to take advantage of the easilyharvested pairwise preference data.
In fact, toour knowledge, there is no adaptation algorithmthat is specifically developed for pairwise data.In this paper, we address the developmentmarket-specific ranking models by leveragingpairwise preference data.
The pairwise prefer-ence data contains most market-specific train-ing examples, while a model from a large mar-ket may capture the common characteristics ofa ranking function.
By combining them algo-rithmically, our approach has two unique advan-tages.
(1) The biases and noises of the pairwisepreference data can be depressed by using thebase model from the large market.
(2) The basemodel can be tailored to the characteristics of thenew market by incorporating the market specificpairwise training data.
As the pairwise data hasthe particular form, the challenge is how to ef-fectively use pairwise data in adaptation.
Thisappeals to the following objective of many websearch engines: design algorithms which mini-mize manually labeled data requirements whilemaintaining strong performance.2 Related WorkIn recent years, the ranking problem is fre-quently formulated as a supervised machinelearning problem, which combines differentkinds of features to train a ranking function.The ranking problem can be formulated as learn-ing a function with pair-wise preference data,which is to minimize the number of contra-dicting pairs in training data.
For example,RankSVM (Joachims, 2002) uses support vectormachines to learn a ranking function from pref-erence data; RankNet (Burges et al, 2005a) ap-plies neural network and gradient descent to ob-tain a ranking function; RankBoost (Freund etal., 1998) applies the idea of boosting to con-struct an efficient ranking function from a set ofweak ranking functions; GBRank (Zheng et al,2007; Xia et al, 2008) using gradient descent infunction spaces, which is able to learn relativeranking information in the context of web search.In addition, Several studies have been focusedon learning ranking functions in semi-supervisedlearning framework (Amini et al, 2008; Duh andKirchhoff, 2008), where unlabeled data are ex-ploited to enhance ranking function.
Another ap-proach to learning a ranking function addressesthe problem of optimizing the list-wise perfor-mance measures of information retrieval, suchas mean average precision or Discount Cumu-lative Gain (Cao et al, 2007; Xu et al, 2008;Wu et al, 2009; Chen et al, 2008c).
The ideaof these methods is to obtain a ranking functionthat is optimal with respect to some informationretrieval performance measure.Model adaptation has previously been appliedin the area of natural language processing andspeech recognition.
This approach has been suc-cessfully applied to parsing (Hwa, 1999), tag-ging (Blitzer et al, 2006), and language model-ing for speech recognition (Bacchiani and Roark,2003).
Until very recently, several works havebeen presented on the topic of model adaptationfor ranking (Gao et al, 2009; Chen et al, 2008b;Chen et al, 2009), however, none of them targetthe model adaptation with the pair-wise learn-ing framework.
Finally, multitask learning forranking has also been proposed as a means ofaddressing problems similar to those we haveencountered in model adaptation (Chen et al,2008a; Bai et al, 2009; Geng et al, 2009).3 Background3.1 Gradient Boosted Decision Trees forRankingAssume we have a training data set, D ={?
(q, d), y?1, .
.
.
, ?
(q, d), y?n}, where ?
(q, d), t?iencodes the labeled relevance, y, of a docu-ment, d, given query, q.
Each query-documentpair, (q, d), is represented by a set of features,(q, d) = {xi1, xi2, xi3, ..., xim}.
These featuresinclude, for example, query-document matchfeatures, query-specific features, and document-specific features.
Each relevance judgment, y,is a relevance grade mapped (e.g.
?relevant?,?somewhat relevant?, ?non-relevant?)
to a real19x1 > a1?x2 > a2?
x3 > a3?YES NOFigure 1: An example of base tree, where x1, x2and x3 are features and a1, a2 and a3 are theirsplitting values.number.
Given this representation, we can learna gradient boosted decision tree (GBDT) whichmodels the relationship between document fea-tures, (q, d), and the relevance score, y, as a de-cision tree (Friedman, 2001).
Figure 1 shows aportion of such a tree.
Given a new query docu-ment pair, the GBDT can be used to predict therelevance grade of the document.
A ranking isthen inferred from these predictions.
We refer tothis method as GBDTreg.In the training phase, GBDTreg iterativelyconstructs regression trees.
The initial regres-sion tree minimizes the L2 loss with respect tothe targets, y,L2(f, y) =??(q,d),y?
(f(q, d)?
y)2 (1)As with other boosting algorithms, the subse-quent trees minimize the L2 loss with respect tothe residuals of the predicted values and the tar-gets.
The final prediction, then, is the sum of thepredictions of the trees estimated at each step,f(x) = f1(x) + .
.
.+ fk(x) (2)where f i(x) is the prediction of the ith tree.3.2 Pairwise TrainingAs alternative to the absolute grades in D,we can also imagine assembling a data setof relative judgments.
In this case, as-sume we have a training data set D ={?
(q, d), (q, d?
), ?
?1, .
.
.
, ?
(q, d), (q, d?
), ?
?n},where ?
(q, d), (q, d?
), ?
?i encodes the prefer-ence, of a document, d, to a second document,d?, given query, q.
Again, each query-documentpair is represented by a set of features.
Eachpreference judgment, ?
?
{,?
}, indicateswhether document d is preferred to document d?
(d  d?)
or not (d ?
d?
).Preference data is attractive for several rea-sons.
First, editors can often more easily deter-mine preference between documents than the ab-solute grade of single documents.
Second, rel-evance grades can often vary between editors.Some editors may tend to overestimate relevancecompared to another editor.
As a result, judg-ments need to be rescaled for editor biases.
Al-though preference data is not immune to inter-editor inconsistency, absolute judgments intro-duce two potential sources of noise: determin-ing a relevance ordering and determining a rele-vance grade.
Third, even if grades can be accu-rately labeled, mapping those grades to real val-ues is often done in a heuristic or ad hoc manner.Fourth, GBDTreg potentially wastes modelingeffort on predicting the grade of a document asopposed to focusing on optimizing the rank orderof documents, the real goal a search engine.
Fi-nally, preference data can often be mined from aproduction system using assumptions about userclicks.In order to support preference-basedtraining data, (Zheng et al, 2007) pro-posed GBRANK based on GBDTreg.
TheGBRANK training algorithm begins by con-structing an initial tree which predicts a constantscore, c, for all instances.
A pair is contra-dicting if the ?
(q, d), (q, d?
),?
and predictionf(q, d) < f(q, d?).
At each boosting stage,the algorithm constructs a set of contradictingpairs, Dcontra.
The GBRANK algorithm thenadjusts the response variables, f(q, d) andf(q, d?
), so that f(q, d) > f(q, d?).
Assumethat (q, d)  (q, d?)
and f(q, d) < f(q, d?).
Tocorrect the order, we modify the target values,f?
(q, d) = f(q, d) + ?
(3)f?
(q, d?)
= f(q, d?)?
?
(4)where ?
> 0 is a margin parameter that we20need to assign.
In our experiments, we set ?
to1.
Note that if preferences are inferred from ab-solute grades, D, minimizing the L2 to 0 alsominimizes the contradictions.3.3 Tree AdaptationRecall that we are also interested in using theinformation learned from one market, which wewill call the source market, on a second market,which we will call the target market.
To this end,the Trada algorithm adapts a GBDTreg modelfrom the source market for the target market byusing a small amount of target market absoluterelevance judgments (Chen et al, 2008b).
Letthe Ds be the data in the source domain andDt be the data in target domain.
Assume wehave trained a model using GBDTreg.
Our ap-proach will be to use the decision tree structurelearned from Ds but to adapt the thresholds ineach node?s feature.
We will use Figure 1 to il-lustrate Trada.
The splitting thresholds are a1, a2and a3 for rank features x1, x2 and x3.
Assumethat the data set Dt is being evaluated at the rootnode v in Figure 1.
We will split the using thefeature vx = x1 but will compute a new thresh-old v?a using Dt and the GBDTreg algorithm.Because we are discussing the root node, whenwe select a threshold b, Dt will be partitionedinto two sets, D>bt and D<bt representing thoseinstances whose feature x1 has a value greaterand lower than b.
The response value for eachpartition will be the uniform average of instancesin that partition,f =??
?1|D>bt |?di?D>bt yi if di ?
D>bt1|D<bt |?di?D<bt yi if di ?
D<bt(5)We would like to select a value for b which min-imizes the L2 loss between y and f in Equation5; equivalently, b can be selected to minimize thevariance of y in each partition.
In our imple-mentation, we compute the L2 loss for all pos-sible values of the feature v?x and select the valuewhich minimizes the loss.Once b is determined, the adaptation consistsof performing a linear interpolation between theoriginal splitting threshold va and the new split-ting threshold b as follows:v?a = pva + (1?
p)b (6)where p is an adaptation parameter which deter-mines the scale of how we want to adapt the treeto the new task.
If there is no additional informa-tion, we can select p according to the size of thedata set,p = |D<as ||D<as |+ |D<bt |(7)In practice, we often want to enhance the adapta-tion scale since the training data of the extendedtask is small.
Therefore, we add a parameter ?to boost the extended task as follows:p = |D<as ||D<as |+ ?|D<bt |(8)The value of ?
can be determined by cross-validation.
In our experiments, we set ?
to 1.The above process can also be applied to ad-just the response value of nodes as follows:v?f = pvf + (1?
p)f (9)where v?f is the adapted response at a node, vf isits original response value of source model, andf is the response value (Equation 5).The complete Trada algorithm used in our ex-periments is presented in Algorithm 1.Algorithm 1 Tree Adaptation AlgorithmTRADA(v,Dt, p)1 b?
COMPUTE-THRESHOLD(vx,Dt)2 v?a ?
pva + (1?
p)b3 v?f ?
pvf + (1?
p)MEAN-RESPONSE(Dt)4 D?t ?
{x ?
Dt : xi < v?a}5 v?< ?
TRADA(v<,D?t, p)6 D?
?t ?
{x ?
Dt : xi > v?a}7 v?> ?
TRADA(v>,D?
?t , p)8 return v?21The Trada algorithm can be augmented with asecond phase which directly incorporates the tar-get training data.
Assume that our source model,Ms, was trained using source data, Ds.
Re-call that Ms can be decomposed as a sum ofregression tree output, fMs(x) = f1Ms(x) +.
.
.
+ fkMs(x).
Additive tree adaptation refersaugmenting this summation with a set of regres-sion trees trained on the residuals between themodel, Ms, and the target training data, Dt.That is, fMt(x) = f1Ms(x) + .
.
.
+ fkMs(x) +fMt(x)k+1+.
.
.+fMt(x)k+k?
.
In order for us toperform additive tree adaptation, the source andtarget data must use the same absolute relevancegrades.4 Pairwise AdaptationBoth GBRANK and Trada can be usedto reduce the requirement on editorial data.GBRANK achieves the goal by leveraging pref-erence data, while Trada does so by leveragingdata from a different search market.
A naturalextension to these methods is to leverage bothsources of data simultaneously.
However, no al-gorithm has been proposed to do this so far inthe literature.
We propose an adaptation methodusing pairwise preference data.Our approach shares the same intuition asTrada: maintain the tree structure but adjustdecision threshold values against some targetvalue.
However, an important difference isthat our adjustment of threshold values does notregress against some target grade values; ratherits objective is to improve the ordering of doc-uments.
To make use of preference data inthe tree adaptation, we follow the method usedin GBRANK to adjust the target values when-ever necessary to preserve correct document or-der.
Given a base model, Ms, and preferencedata, Dt , we can use Equations 3 and 4 to in-fer target values.
Specifically, we construct a setDcontra from Dt and Ms. For each item (q, d)in Dcontra, we use the value of f?
(q, d) as the tar-get.
These tuples, ?
(q, d), f?
(q, d)?
along withMs are then are provided as input to Trada.
Ourapproach is described in Algorithm 2.Compared to Trada, Pairwise-Trada has twoAlgorithm 2 Pairwise Tree Adaptation Algo-rithmPAIRWISE-TRADA(Ms,Dt , p)1 Dcontra ?
FIND-CONTRADICTIONS(Ms,Dt )2 D?t ?
{?
(q, d), f?
(q, d)?
: (q, d) ?
Dcontra}3 return TRADA(ROOT(Ms), D?t, p)important differences.
First, Pairwise-Trada canuse a source GBDT model trained either againstabsolute or pairwise judgments.
When an orga-nization maintains a set of ranking models fordifferent markets, although the underlying mod-eling method may be shared (e.g.
GBDT), thelearning algorithm used may be market-specific(e.g.
GBRANK or GBDTreg).
Unfortunately,classic Trada relies on the source model beingtrained using GBDTreg.
Second, Pairwise-Tradacan be adapted using pairwise judgments.
Thismeans that we can expand our adaptation data toinclude click feedback, which is easily obtain-able in practice.5 Methods and MaterialsThe proposed algorithm is a straightforwardmodification of previous ones.
The question wewant to examine in this section is whether thissimple modification is effective in practice.
Inparticular, we want to examine whether pairwiseadaptation is better than the original adaptationTrada using grade data, and whether the pairwisedata from a market can help improve the rankingfunction on a different market.Our experiments evaluate the performance ofPairwise-Trada for web ranking in ten targetmarkets.
These markets, listed in Table 1, covera variety of languages and cultures.
Further-more, resources, in terms of documents, judg-ments, and click-through data, also varies acrossmarkets.
In particular, editorial query-documentjudgments range from hundreds of thousands(e.g.
SEA1) to tens of thousands (e.g.
SEA5).Editors graded query-document pairs on a five-point relevance scale, resulting in our data setD.Preference labels, D, are inferred from thesejudgments.22We also include a second set of experimentswhich incorporate click data.1 In these experi-ments, we infer a preference from click data byassuming the following model.
The user is pre-sented with ten results.
An item i  j if i the fol-lowing conditions hold: i is positioned below j,i receives a click, and j does not receive a click.In our experiments, we tested the followingruns,?
GBDTreg trained using only Ds or Dt?
GBRANK trained using only Ds or Dt?
GBRANK trained using only Ds , Dt , andCt?
Trada with both GBDTs and GBRANKs,adapted with Dt.?
Pairwise-Trada with both GBDTs andGBRANKs, adapted with Dt and Ct at dif-ferent ratios.In the all experiments, we use 400 additive treeswhen additive adaptation is used.All models are evaluated using discounted cu-mulative gain (DCG) at rank cutoff 5 (Ja?rvelinand Keka?la?inen, 2002).6 Results6.1 Adaptation with Manually LabeledDataIn Table 1, we show the results for all of our ex-perimental conditions.We can make a few observations about thenon-adaptation baselines.
First, models trainedon the (limited) target editorial data, GBDTtand GBRANKt, tend to outperform those trainedonly on the source editorial data, GBDTs andGBRANKs.
The critical exception is SEA5, themarket with the fewest judgments.
We believethat this behavior is a result of similarity betweenthe United States source data and the SEA5 tar-get market; both the source and target query pop-ulations share the same language, a property not1For technical reasons, this data set is slightly differ-ent from the results we show with the purely editorial data.Therefore the size of the training and testing sets are differ-ent, but not to a significant degree.exhibited in other markets.
Notice that othersmall markets such as LA2 and LA3 see modestimprovements when using target-only runs com-pared to source-only runs.
Second, GBRANKtends to outperform GBDT when only trained onthe source data.
This implies that we should pre-fer a base model which is based on GBRANK,something that is difficult to combine with clas-sic Trada.
Third, by comparing GBRANK andGBDT when only trained on the target data, wenotice that the effectiveness of GBRANK de-pends on the amount of training data.
For mar-kets where there training data is plentiful (e.g.SEA1), GBRANK outperforms GBDT.
On theother hand, for smaller markets (e.g.
LA3),GBDT outperforms GBRANK.In general, the results confirm the hypothe-sis that adaptation runs outperform all of non-adaptation baselines.
This is the case for bothTrada and Pairwise-Trada.
As with the baselineruns, the Australian market sees different perfor-mance as a result of the combination of a smalltarget editorial set and a representative sourcedomain.
This effect has been observed in pre-vious results (Chen et al, 2009).We can also make a few observations by com-paring the adaptation runs.
Trada works betterwith a GBDT base model than with a GBRANKbase model.
We We believe this is the case be-cause the absolute regression targets are diffi-cult to compare with the unbounded output ofGBRANK.
Pairwise-Trada on the other handtends to perform better with a GBRANK basemodel than with a GBDT base model.
Thereare a few exceptions, SEA3 and LA2, wherePairwise-Trada works better with a GBDT basemodel.
Comparing Trada to Pairwise-Trada, wefind that using preference targets tends to im-prove performance for some markets but not all.The underperformance of Pairwise-Trada tendsto occur in smaller markets such as LA1, LA2,and LA3.
This is similar to the behavior we ob-served in the non-adaptation runs and suggeststhat, in operation, a modeler may have to decideon the training algorithm based on the amount ofdata available.23SEA1 SEA2 EU1 SEA3 EU2 SEA4 LA1 LA2 LA3 SEA5training size 243,790 174,435 137,540 135,066 101,076 100,846 91,638 75,989 66,151 37,445testing size 18,652 26,752 11,431 13,839 12,118 12,214 11,038 16,339 10,379 21,034GBDTs 9.4483 8.1271 9.0018 10.0630 8.5339 5.9176 6.1699 11.4167 8.1416 10.5356GBDTt 9.6011 8.6225 9.3310 10.7591 9.0323 6.4185 6.8441 11.8553 8.5702 10.4561GBRANKs 9.6059 8.1784 9.0775 10.2486 8.6248 6.1298 6.2614 11.5186 8.2851 10.5915GBRANKt 9.6952 8.6225 9.3575 10.8595 9.0384 6.4620 6.8543 11.7086 8.4825 10.3469TradaGBDTs,Dt 9.6718 8.6120 9.3086 10.8001 9.1024 6.3440 6.9444 11.9513 8.6519 10.6279GBRANKs,Dt 9.6116 8.5681 9.2125 10.7597 8.9675 6.4110 6.8286 11.7326 8.5498 10.6508Pairwise-TradaGBDTs,Dt 9.7364 8.6261 9.3824 10.8549 9.0842 6.4705 6.9438 11.8255 8.5323 10.4655GBRANKs,Dt 9.7539 8.6538 9.4269 10.8362 9.1044 6.4716 6.9438 11.8034 8.6187 10.6564Table 1: Adaptation using manually labeled training data Southeast Asia (SEA), Europe (EU), andLatin America (LA) markets.
Markets are sorted by target training set size.
Significance tests usea t-test.
Bolded numbers indicate statistically significant improvements over the respective sourcemodel.SEA1 SEA2 EU1 SEA3 EU2 SEA4 LA1 LA2 LA3 SEA5training size 194,114 166,396 136,829 161,663 94,875 96,642 73,977 108,350 64,481 71,549testing size 15,655 11,844 11,028 11,839 11,118 5,092 10,038 12,246 10,201 7,477GBRANKs 9.0159 8.5763 8.7119 11.4512 9.7641 6.5941 6.894 7.9366 8.058 10.7935Pairwise-TradaGBRANKs,Dt, Cteditorial 9.3577 8.9205 8.901 12.2247 9.9531 6.7421 7.1455 8.2811 8.2503 10.7973click 9.1149 8.7622 8.8187 11.9361 9.8818 6.7703 7.1812 8.264 8.2485 10.9042editorial+click 9.4898 9.0177 8.945 12.3172 10.1156 6.8459 7.2414 8.4111 8.292 11.1407Table 2: Adaptation incorporating click data.
Bolded numbers indicate statistically significant im-provements over the baseline.
Markets ordered as in Table 1.6.2 Incorporating Click DataOne of the advantages of Pairwise-Trada is theability to incorporate multiple sources of pair-wise preference data.
In this paper, we use theheuristic rule approach which is introduced by(Dong et al, 2009) to extract pairwise preferencedata from the click log of the search engine.
Thisapproach yields both skip-next and skip-abovepairs (Joachims et al, 2005), which are sortedby confidence descending order respectively.
Inthese experiments, we combine manually gener-ated preferences with those gathered from clickdata.
We present these results in Table 2.We notice that no matter the source of prefer-ence data, Pairwise-Trada outperforms the base-line GBRANK model.
The magnitude of theimprovement depends on the source data used.Comparing the editorial-only to the click-onlymodels, we notice that click-only models outper-form editorial-only models for smaller markets(SEA4, LA1, and SEA5).
This is likely the casebecause the relative quantity of click data withrespect to editorial data is higher in these mar-kets.
This is despite the fact that the click datamay be noisier than the editorial data.
The bestperformance, though, comes when we combineboth editorial and click data.6.3 Additive tree adaptationRecall that Pairwise-Trada consists of two parts:parameter adaptation and additive tree adapta-tion.
In this section, we examine the contri-bution to performance each part is responsiblefor.
Figure 2 illustrates the adaptation results forthe LA1 market.
In this experiment, we use aUnited States base model and 100K LA1 edito-rial judgments for adaptation.
Pairwise-Trada isperformed on top of differently sized base mod-els with 600, 900 and 1200 trees.
The originalbase model has 1200 trees; we selected the first600, 900 or full 1200 trees for experiments.
Thenumber of trees used in the additive tree adap-tation step ranges up to 600 trees.
From Fig-ure 2 we can see that the additive adaptation can240 500 1000 1500 20006.06.26.46.66.87.0number of treesDCG5adaptationadditive (600)additive (900)additive (1200)source modelFigure 2: Illustration of additive tree adaptationfor LA1.
The curves are average performanceover a range of parameter settings.significantly increase DCG over simple parame-ter adaptation and is therefore a critical step ofPairwise-Trada.
When the number of trees inthe additive tree adaptation step reaches roughly400, the DCG plateaus.7 ConclusionWe have proposed a model for adapting retrievalmodels using preference data instead of abso-lute relevance grades.
Our experiments demon-strate that, when much editorial data is present,our method, Pairwise-Trada, may be preferableto competing methods based on absolute rele-vance grades.
However, in real world systems,we often have access to sources of preferencedata beyond those resulting from editorial judg-ments.
We demonstrated that Pairwise-Trada canexploit such data and boost performance signif-icantly.
In fact, if we omit editorial data alto-gether we see performance improvements overthe baseline model.
This suggests that, in prin-ciple, we can train a single, strong source modeland improve it using target click data alone.
De-spite the fact that the modification we made isquite simple, we showed that modification is ef-fective in practice.
This tends to validate thegeneral principle of using pairwise data from adifferent market.
This principle can be easilyused in other frameworks such as neural net-works (Burges et al, 2005b).
Therefore, the pro-posed method also points to a new direction forfuture improvements of search engines.There are several areas of future work.
First,we believe that detecting other sources of pref-erence data from user behavior can further im-prove the performance of our model.
Second,we only used a single source model in our ex-periments.
We would also like to explore theeffect of learning from an ensemble of sourcemodels.
The importance of each may depend onthe similarity to the target domain.
Finally, wewould also like to more accurately understandthe queries where click data improves adaptationand those where editorial judgments is required.This sort of knowledge will allow us to train sys-tems which maximally exploit our editorial re-sources.ReferencesAmini, M.-R., T.-V. Truong, and C. Goutte.
2008.A boosting algorithm for learning bipartite rank-ing functions with partially labeled data.
In SIGIR?08: Proceedings of the 31st annual internationalACM SIGIR conference on Research and develop-ment in information retrieval.Bacchiani, M. and B. Roark.
2003.
Unsuper-vised language model adaptation.
In ICASSP ?03:Proceedings of the International Conference onAcoustics, Speech and Signal Processing.Bai, J., K. Zhou, H. Zha, B. Tseng, Z. Zheng, andY.
Chang.
2009.
Multi-task learning for learningto rank in web search.
In CIKM ?09: Proceedingof the 18th ACM conference on Information andknowledge management.Blitzer, J., R. McDonald, and F. Pereira.
2006.Domain adaptation with structural correspondencelearning.
In EMNLP ?06: Proceedings of the2006 Conference on Empirical Methods on Nat-ural Language Processing.Burges, C., T. Shaked, E. Renshaw, A. Lazier,M.
Deeds, N. Hamilton, and G. Hullender.
2005a.Learning to rank using gradient descent.
In ICML?05: Proceedings of the 22nd International Con-ference on Machine learning.Burges, Chris, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hul-lender.
2005b.
Learning to rank using gradi-ent descent.
In ICML ?05: Proceedings of the2522nd international conference on Machine learn-ing, pages 89?96.
ACM.Cao, Z., T. Qin, T.-Y.
Liu, M.-F. Tsai, and H. Li.2007.
from pairwise approach to listwise ap-proach.
In ICML ?07: Proceedings of the 24thinternational conference on Machine learning.Chen, D., J. Yan, G. Wang, Y. Xiong, W. Fan, andZ.
Chen.
2008a.
Transrank: A novel algorithm fortransfer of rank learning.
In ICDM workshop ?08:Proceeding of IEEE Conference on Data Mining.Chen, K., R. Lu, C. K. Wong, G. Sun, L. Heck, andB.
Tseng.
2008b.
Trada: tree based ranking func-tion adaptation.
In CIKM ?08: Proceeding of the17th ACM conference on Information and knowl-edge management, pages 1143?1152, New York,NY, USA.
ACM.Chen, W., T.-Y.
Liu, Y. Lan, Z. Ma, and H. Li.
2008c.Measures and loss functions in learning to rank.
InNIPS ?08: Proceedings of the Twenty-Second An-nual Conference on Neural Information Process-ing Systems.Chen, K., J. Bai, S. Reddy, and B. Tseng.
2009.
Ondomain similarity and effectiveness of adapting-to-rank.
In CIKM ?09: Proceeding of the 18thACM conference on Information and knowledgemanagement, pages 1601?1604, New York, NY,USA.
ACM.Dong, A., Y. Chang, S. Ji, C. Liao, X. Li, andZ.
Zheng.
2009.
Empirical exploitation of clickdata for query-type-based ranking.
In EMNLP?09: Proceedings of the 2009 Conference on Em-pirical Methods on Natural Language Processing.Duh, K. and K. Kirchhoff.
2008.
Learning to rankwith partially-labeled data.
In SIGIR ?08: Pro-ceedings of the 31st annual international ACM SI-GIR conference on Research and development ininformation retrieval.Freund, Y., R. D. Iyer, R. E. Schapire, and Y. Singer.1998.
An efficient boosting algorithm for com-bining preferences.
In ICML ?98: Proceedings ofthe Fifteenth International Conference onMachineLearning.Friedman, J. H. 2001.
Greedy function approxima-tion: A gradient boosting machine.
The Annals ofStatistics, 29(5):1189?1232.Gao, J., Q. Wu, C. Burges, K. Svore, Y. Su, N. Khan,Shah S., and H. Zhou.
2009.
Model adapta-tion via model interpolation and boosting for websearch ranking.
In EMNLP ?09: Proceedings ofthe 2009 Conference on Empirical Methods onNatural Language Processing.Geng, B., L. Yang, C. Xu, and X.-S. Hua.
2009.Ranking model adaptation for domain-specificsearch.
In CIKM ?09: Proceeding of the 18th ACMconference on Information and knowledge man-agement, pages 197?206, New York, NY, USA.ACM.Hwa, R. 1999.
Supervised grammar induction usingtraining data with limited constituent information.In ACL ?99: Proceedings of the Conference of theAssociation for Computational Linguistics.Ja?rvelin, Kalervo and Jaana Keka?la?inen.
2002.
Cu-mulated gain-based evaluation of ir techniques.TOIS, 20(4):422?446.Joachims, T., L. Granka, B. Pan, and G. Gay.
2005.Accurately interpreting clickthrough data as im-plicit feedback.Joachims, T. 2002.
Optimizing search engines usingclickthrough data.
In KDD ?02: Proceedings ofthe eighth ACM SIGKDD international conferenceon Knowledge discovery and data mining, pages133?142.
ACM Press.Liu, T.-Y.
2009.
Learning to Rank for InformationRetrieval.
Now Publishers.Radlinski, F. and T. Joachim.
2006.
Minimally inva-sive randomization for collecting unbiased prefer-ences from clickthrough logs.Radlinski, F. and T. Joachim.
2007.
Active ex-ploration for learning rankings from clickthroughdata.Wu, M., Y. Chang, Z. Zheng, and H. Zha.
2009.Smoothing dcg for learning to rank: A novel ap-proach using smoothed hinge functions.
In CIKM?09: Proceeding of the 18th ACM conference onInformation and knowledge management.Xia, F., T.-Y.
Liu, J. Wang, W. Zhang, and H. Li.2008.
Listwise approach to learning to rank: The-orem and algorithm.
In ICML ?08: Proceedingsof the 25th international conference on Machinelearning.Xu, J., T.Y.
Liu, M. Lu, H. Li, and W.Y.
Ma.
2008.Directly optimizing evaluation measures in learn-ing to rank.
In SIGIR ?08: Proceedings of the31st annual international ACM SIGIR conferenceon Research and development in information re-trieval.Zheng, Z., K. Chen, G. Sun, and H. Zha.
2007.
A re-gression framework for learning ranking functionsusing relative relevance judgments.
In SIGIR ?07:Proceedings of the 30th annual international ACMSIGIR conference on Research and development ininformation retrieval, pages 287?294.
ACM.26
