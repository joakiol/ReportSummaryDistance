NAACL-HLT Workshop on the Induction of Linguistic Structure, pages 1?7,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Induction of Frame-Semantic RepresentationsAshutosh Modi Ivan Titov Alexandre KlementievSaarland UniversitySaarbru?cken, Germany{amodi|titov|aklement}@mmci.uni-saarland.deAbstractThe frame-semantic parsing task is challeng-ing for supervised techniques, even for thosefew languages where relatively large amountsof labeled data are available.
In this prelim-inary work, we consider unsupervised induc-tion of frame-semantic representations.
Anexisting state-of-the-art Bayesian model forPropBank-style unsupervised semantic roleinduction (Titov and Klementiev, 2012) is ex-tended to jointly induce semantic frames andtheir roles.
We evaluate the model perfor-mance both quantitatively and qualitatively bycomparing the induced representation againstFrameNet annotations.1 IntroductionShallow representations of meaning, and semanticrole labels in particular, have a long history in lin-guistics (Fillmore, 1968).
In this paper we focus onframe-semantic representations: a semantic frame isa conceptual structure describing a situation (or anentity) and its participants (or its properties).
Par-ticipants and properties are associated with seman-tic roles (also called frame elements).
For example,following the FrameNet annotation guidelines (Rup-penhofer et al, 2006), in the following sentences:(a) [COOK Mary] cooks [FOOD the broccoli][CONTAINER in a small pan].
(b) Sautee [FOOD the onions] [MANNER gently ][TEMP SETTING on low heat].the same semantic frame Apply Heat is evokedby verbs cook and sautee, and roles COOK andFOOD in the sentence (a) are filled by Mary andthe broccoli, respectively.
Note that roles are spe-cific to the frame, not to the individual lexical units(verbs cook and sautee, in the example).1Most approaches to predicting these representa-tions, called semantic role labeling (SRL), have re-lied on large annotated datasets (Gildea and Juraf-sky, 2002; Carreras and Ma`rquez, 2005; Surdeanuet al, 2008; Hajic?
et al, 2009).
By far, most ofthis work has focused on PropBank-style represen-tations (Palmer et al, 2005) where roles are definedfor each individual verb, or even individual senses ofa verb.
The only exceptions are modifiers and rolesA0 and A1 which correspond to proto-agent (a doer,or initiator of the action) and proto-patient (an af-fected entity), respectively.
However, the SRL taskis known to be especially hard for the FrameNet-style representations for a number of reasons, in-cluding, the lack of cross-frame correspondence formost roles, fine-grain definitions of roles and framesin FrameNet, and relatively small amounts of statis-tically representative data (Erk and Pado, 2006; Daset al, 2010; Palmer and Sporleder, 2010; Das andSmith, 2011).
Another reason for reduced interest inpredicting FrameNet representations is the lack ofannotated resources for most languages, with anno-tated corpora available or being developed only forEnglish (Ruppenhofer et al, 2006), German (Bur-chardt et al, 2006), Spanish (Subirats, 2009) andJapanese (Ohara et al, 2004).Due to scarcity of labeled data, purely unsuper-vised set-ups recently started to receive considerableattention (Swier and Stevenson, 2004; Grenager andManning, 2006; Lang and Lapata, 2010; Lang and1More accurately, FrameNet distinguishes core and non-core roles with non-core roles mostly corresponding to mod-ifiers, e.g., MANNER in sentence (b).
Non-core roles areexpected to generalize across frames.1cooksMarythe broccoli in a small  panCONTAINERCOOK FOODApply_HeatFigure 1: An example of a semantic dependency graph.Lapata, 2011a; Lang and Lapata, 2011b; Titov andKlementiev, 2012).
However, all these approacheshave focused on PropBank-style representations.This may seem somewhat unnatural as FrameNetrepresentations, though arguably more powerful, areharder to learn in the supervised setting, harder toannotate, and annotated data is available for a con-siderably fewer languages.
This is the gap which weaddress in this preliminary study.More specifically, we extend an existing state-of-the-art Bayesian model for unsupervised seman-tic role labeling and apply it to support FrameNet-style semantics.
In other words, our method jointlyinduces both frames and frame-specific semanticroles.
We experiment only with verbal predicatesand evaluate the performance of the model with re-spect to some natural baselines.
Though the scoresfor frame induction are not high, we argue that this isprimarily due to very high granularity of FrameNetframes which is hard to reproduce for unsupervisedsystems, as the implicit supervision signal is not ca-pable of providing these distinctions.2 Task DefinitionIn this work, we use dependency representationsof frame semantics.
Dependency representationsfor SRL (Johansson and Nugues, 2008) were madepopular by CoNLL-2008 and CoNLL-2009 sharedtasks (Surdeanu et al, 2008; Hajic?
et al, 2009), butfor English were limited to PropBank.
Recently,English FrameNet was also released in the depen-dency format (Bauer et al, 2012).
Instead of pre-dicting argument spans, in dependency representa-tion the goal is, roughly, to predict the syntactic headof the argument.
The semantic dependency repre-sentation for sentence (a) is shown in Figure 1, la-bels on edges denote roles and labels on words de-note frames.
Note that in practice the structures canbe more complex, as, for example, arguments canevoke their own frames or the same arguments canbe shared by multiple predicates, as in right noderaising constructions.The SRL task, or more specifically frame-semantic parsing task consists, at least conceptually,of four stages: (1) identification of frame-evokingelements(FEE), (2) identification of arguments, (3)frame labeling and (4) role labeling.
In this work,we focus only on the frame labeling and role label-ing stages, relying on gold standard (i.e.
the oracle)for FEEs and role identification.
In other words, ourgoal is to label (or cluster) edges and nodes in thedependency graph, Figure 1.
Since we focus in thisstudy on verbal predicates only, the first stage wouldbe trivial and the second stage could be handled withheuristics as in much of previous work on unsuper-vised SRL (Lang and Lapata, 2011a; Titov and Kle-mentiev, 2012).Additionally to considering only verbal predi-cates, we also assume that every verb belongs toa single frame.
This assumption, though restric-tive, may be reasonable in practice as (a) the dis-tributions across frames (i.e.
senses) are gener-ally highly skewed, (b) current state-of-the-art tech-niques for word-sense induction hardly beat most-frequent-sense baselines in accuracy metrics (Man-andhar et al, 2010).
This assumption, or its minorrelaxations, is relatively standard in work on unsu-pervised semantic parsing tasks (Poon and Domin-gos, 2009; Poon and Domingos, 2010; Titov andKlementiev, 2011).
From the modeling prospective,there are no major obstacles to relaxing this assump-tion, but it would lead to a major explosion of thesearch space and, as a result, slow inference.3 Model and InferenceWe follow previous work on unsupervised seman-tic role labeling (Lang and Lapata, 2011a; Titovand Klementiev, 2012) and associate arguments withtheir frame specific syntactic signatures which werefer to as argument keys:?
Active or passive verb voice (ACT/PASS).?
Argument position relative to predicate(LEFT/RIGHT).?
Syntactic relation to its governor.?
Preposition used for argument realization.Semantic roles are then represented as clusters ofargument keys instead of individual argument occur-rences.
This representation aids our models in in-ducing high purity clusters (of argument keys) while2reducing their granularity.
Thus, if an argument keyk is assigned to a role r (k ?
r), all of its occurrencesare labeled r.3.1 A model for frame-semantic parsingOur approach is similar to the models of Titov andKlementiev (2012; 2011).
Please, see Section 5 fora discussion of the differences.Our model encodes three assumptions aboutframes and semantic roles.
First, we assume thatthe distribution of lexical units (verbal predicates)is sparse for each semantic frame.
Second, we en-force the selectional restriction assumption: we as-sume that the distribution over potential argumentfillers is sparse for every role, implying that ?peaky?distributions of arguments for each role r are pre-ferred to flat distributions.
Third, each role normallyappears at most once per predicate occurrence.
Ourinference will search for a frame and role clusteringwhich meets the above requirements to the maximalextent.Our model associates three distributions with eachframe.
The first one (?)
models the selection of lex-ical units, the second (?)
governs the selection of ar-gument fillers for each semantic role, and the third(?)
models (and penalizes) duplicate occurrence ofroles.
Each frame occurrence is generated indepen-dently given these distributions.
Let us describe themodel by first defining how the set of model param-eters and an argument key clustering are drawn, andthen explaining the generation of individual frameinstances.
The generative story is formally presentedin Figure 2.For each frame, we begin by drawing a dis-tribution of its lexical units from a DP priorDP (?, H(P )) with a small concentration parame-ter ?, and a base distribution H(P ), pre-computed asnormalized counts of all verbs in our dataset.
Next,we generate a partition of argument keys Bf fromCRP(?)
with each subset r ?
Bf representing a sin-gle frame specific semantic role.
The crucial partof the model is the set of selectional preference pa-rameters ?f,r, the distributions of arguments x foreach role r of frame f .
We represent arguments bylemmas of their syntactic heads.2 In order to encode2For prepositional phrases, we take as head the head noun ofthe object noun phrase as it encodes crucial lexical information.However, the preposition is not ignored but rather encoded inthe assumption about sparseness of the distributions?f,r, we draw them from the DP prior DP (?, H(A))with a small concentration parameter ?, the baseprobability distribution H(A) is just the normalizedfrequencies of arguments in the corpus.
Finally,the geometric distribution ?f,r is used to model thenumber of times a role r appears with a given frameoccurrence.
The decision whether to generate atleast one role r is drawn from the uniform Bernoullidistribution.
If 0 is drawn then the semantic role isnot realized for the given occurrence, otherwise thenumber of additional roles r is drawn from the ge-ometric distribution Geom(?f,r).
The Beta priorsover ?
indicate the preference towards generating atmost one argument for each role.Now, when parameters and argument key cluster-ings are chosen, we can summarize the remainder ofthe generative story as follows.
We begin by inde-pendently drawing occurrences for each frame.
Foreach frame occurrence, we first draw its lexical unit.Then for each role we independently decide on thenumber of role occurrences.
Then we generate eachof the arguments (seeGenArgument in Figure 2) bygenerating an argument key kf,r uniformly from theset of argument keys assigned to the cluster r, and fi-nally choosing its filler xf,r, where the filler is eithera lemma or the syntactic head of the argument.3.2 InferenceWe use a simple approximate inference algo-rithm based on greedy search for the maximum a-posteriori clustering of lexical units and argumentkeys.
We begin by assigning each verbal predi-cate to its own frame, and then iteratively choosea pair of frames and merge them.
Note that eachmerge involves inducing a new set of roles, i.e.
are-clustering of argument keys, for the new mergedframe.
We use the search procedure proposed in(Titov and Klementiev, 2012), in order to cluster ar-gument keys for each frame.Our search procedure chooses a pair of frames tomerge based on the largest incremental change to theobjective due to the merge.
Computing the changeinvolves re-clustering of argument keys, so consider-ing all pairs of initial frames containing single verbalpredicates is computationally expensive.
Instead, wethe corresponding argument key.3Parameters:for each frame f = 1, 2, .
.
.
:?f ?
DP (?, H(P )) [distrib of lexical units]Bf ?
CRP (?)
[partition of arg keys]for each role r ?
Bf :?f,r ?
DP (?, H(A)) [distrib of arg fillers]?f,r ?
Beta(?0, ?1) [geom distr for dup roles]Data Generation:for each frame f = 1, 2, .
.
.
:for each occurrence of frame f :p ?
?f [draw a lexical unit]for every role r ?
Bf :if [n ?
Unif(0, 1)] = 1: [role appears at least once]GenArgument(f, r) [draw one arg]while [n ?
?f,r] = 1: [continue generation]GenArgument(f, r) [draw more args]GenArgument(f, r):kf,r ?
Unif(1, .
.
.
, |r|) [draw arg key]xf,r ?
?f,r [draw arg filler]Figure 2: Generative story for the frame-semantic parsingmodel.prune the space of possible pairs of verbs using asimple but effective pre-processing step.
Each verbis associated with a vector of normalized aggregatecorpus counts of syntactic dependents of the verb(ignoring the type of dependency relation).
Cosinesimilarity of these vectors are then used to prune thepairs of verbs so that only verbs which are distribu-tionally similar enough are considered for a merge.Finally, the search terminates when no additionalmerges result in a positive change to the objective.4 Experimental Evaluation4.1 DataWe used the dependency representation of theFrameNet corpus (Bauer et al, 2012).
The corpus isautomatically annotated with syntactic dependencytrees produced by the Stanford parser.
The data con-sists of 158,048 sentences with 3,474 unique verbalpredicates and 722 gold frames.4.2 Evaluation MetricsWe cannot use supervised metrics to evaluate ourmodels, since we do not have an alignment betweengold labels and clusters induced in the unsupervisedsetup.
Instead, we use the standard purity (PU) andcollocation (CO) metrics as well as their harmonicmean (F1) to measure the quality of the resultingclusters.
Purity measures the degree to which eachcluster contains arguments (verbs) sharing the samegold role (gold frame) and collocation evaluates thedegree to which arguments (verbs) with the samegold roles (gold frame) are assigned to a single clus-ter, see (Lang and Lapata, 2010).
As in previouswork, for role induction, the scores are first com-puted for individual predicates and then averagedwith the weights proportional to the total number oc-currences of roles for each predicate.4.3 Model ParametersThe model parameters were tuned coarsely by visualinspection: ?
= 1.e-5, ?
= 1.e-4, ?
= 1, ?0 = 100,?1 = 1.e-10.
Only a single model was evaluatedquantitatively to avoid overfitting to the evaluationset.4.4 Qualitative EvaluationOur model induced 128 multi-verb frames from thedataset.
Out of 78,039 predicate occurrences in thedata, these correspond to 18,963 verb occurrences(or, approximately, 25%).
Some examples of theinduced multi-verb frames are shown in Table 1.As we can observe from the table, our model clus-ters semantically related verbs into a single frame,even though they may not correspond to the samegold frame in FrameNet.
Consider, for example, theframe (ratify::sign::accede): the verbs are semanti-cally related and hence they should go into a singleframe, as they all denote a similar action.Another result worth noting is that the model of-ten clusters antonyms together as they are often usedin similar context.
For example, consider the frame(cool::heat::warm), the verbs cool, heat and warm,all denote a change in temperature.
This agrees wellwith annotation in FrameNet.
Similarly, we clus-ter sell and purchase together.
This contrasts withFrameNet annotation as FrameNet treats them notas antonyms but as different views on same situationand according to their guidelines, different framesare assigned to different views.Often frames in FrameNet correspond to morefine-grained meanings of the verbs, as we can seein the example for (plait::braid::dye).
The three de-scribe a similar activity involving hair but FrameNet4Induced frames FrameNet frames corresponding to the verbs(rush::dash::tiptoe) rush : [Self motion](150) [Fluidic motion](19)dash : [Self motion](100)tiptoe : [Self motion](114)(ratify::sign::accede) ratify : [Ratification](41)sign : [Sign agreement](81) [Hiring](18) [Text Creation](1)accede : [Sign Agreement](31)(crane::lean::bustle) crane : [Body movement](26)lean: [Change posture](70) [Placing](22) [Posture](12)bustle : [Self motion](55)(cool::heat::warm) cool : [Cause temperature change](27)heat: [Cause temperature change](52)warm: [Cause temperature change](41) [Inchoative change of temperature](16)(want::fib::dare) want : [Desiring](105) [Possession](44)fib : [Prevarication](9)dare : [Daring](21)(encourage::intimidate::confuse) encourage : [Stimulus focus](49)intimidate : [Stimulus focus](26)confuse: [Stimulus focus](45)(happen::transpire::teach) happen : [Event](38) [Coincidence](21) [Eventive affecting](1)transpire : [Event](15)teach : [Education teaching](7)(do::understand::hope) do : [Intentionally affect](6) [Intentionally act](56)understand : [Grasp](74) [Awareness](57) [Categorization](15)hope : [Desiring](77)(frighten::vary::reassure) frighten : [Emotion directed](44)vary : [Diversity](24)reassure : [Stimulus focus](35)(plait::braid::dye) plait : [Hair configuration](11) [Grooming](12)braid : [Hair configuration](7) [Clothing parts](6) [Rope manipulation](4)dye : [Processing materials](18)(sell::purchase) sell : [Commerce sell](107)purchase : [Commerce buy](93)(glisten::sparkle::gleam) glisten : [Location of light](52) [Light movement](1)sparkle : [Location of light](23) [Light movement](3)gleam : [Location of light](77) [Light movement](4)(forestall::shush) forestall : [Thwarting](12)shush : [Silencing](6)Table 1: Examples of the induced multi-verb frames.
The left column shows the induced verb clusters and the rightcolumn lists the gold frames corresponding to each verb and the number in the parentheses are their occurrence counts.gives them a finer distinction.
Arguably, implicit su-pervision signal present in the unlabeled data is notsufficient to provide such fine-grained distinctions.The model does not distinguish verb senses, i.e.
italways assigns a single frame to each verb, so thereis an upper bound on our clustering performance.4.5 Quantitative EvaluationNowwe turn to quantitative evaluation of both frameand role induction.Frame Labeling.
In this section, we evaluate howwell the induced frames correspond to the gold stan-dard annotation.
Because of the lack of relevantprevious work, we use only a trivial baseline whichplaces each verb in a separate cluster (NoCluster-ing).
The results are summarized in Table 3.As we can see from the results, our modelachieves a small, but probably significant, improve-ment in the F1-score.
Though the scores arefairly low, note that, as discussed in Section 4.4,the model is severely penalized even for induc-ing semantically plausible frames such as the frame(plait::braid::dye).Role Labeling.
In this section, we evaluate howwell the induced roles correspond to the gold stan-dard annotation.
We use two baselines: one isthe syntactic baseline SyntF, which simply clus-ters arguments according to the dependency rela-5PU CO F1Our approach 78.9 71.0 74.8NoFrameInduction 79.2 70.7 74.7SyntF 69.9 73.3 71.6Table 2: Role labeling performance.tion to their head, as described in (Lang and La-pata, 2010), and the other one is a version of ourmodel which does not attempt to cluster verbs andonly induces roles (NoFrameInduction).
Note thatthe NoFrameInduction baseline is equivalent to thefactoredmodel of Titov and Klementiev (2012).
Theresults are summarized in Table 2.First, observe that both our full model and its sim-plified version NoFrameInduction significantly out-perform the syntactic baseline.
It is important tonote that the syntactic baseline is not trivial to beatin the unsupervised setting (Lang and Lapata, 2010).Though there is a minor improvement from inducingframes, it is small and may not be significant.3Another observation is that the absolute scoresof all the systems, including the baselines, are sig-nificantly below the results reported in Titov andKlementiev (Titov and Klementiev, 2012) on theCoNLL-08 version of PropBank in a comparablesetting (auto parses, gold argument identification):73.9 % and 77.9 % F1 for SyntF and NoFrameIn-duction, respectively.
We believe that the main rea-son for this discrepancy is the difference in the syn-tactic representations.
The CoNLL-08 dependenciesinclude function tags (e.g., TMP, LOC), and, there-fore, modifiers do not need to be predicted, whereasthe Stanford syntactic dependencies do not providethis information and the model needs to induce it.It is clear from these results, and also from theprevious observation that only 25% of verb occur-rences belong to multi-verb clusters, that the modeldoes not induce sufficiently rich clustering of verbs.Arguably, this is largely due to the relatively smallsize of FrameNet, as it may not provide enough evi-dence for clustering.
Given that our method is quiteefficient, a single experiment was taking around 8hours on a single CPU, and the procedure is highlyparallelizable, the next step would be to use a muchlarger and statistically representative corpus to in-duce the representations.3There is no well-established methodology for testing statis-tical significance when comparing two clustering methods.PU CO F1Our approach 77.9 31.4 44.7NoClustering 80.8 29.0 42.7Table 3: Frame labeling performance.Additional visual inspection suggest that the datais quite noisy primarily due to mistakes in parsing.The large proportion of mistakes can probably be ex-plained by the domain shift: the parser is trained onthe WSJ newswire data and tested on more generalBNC texts.5 Related WorkThe space constraints do not permit us to pro-vide a comprehensive overview of related work.Aside from the original model of Titov and Klemen-tiev (2012), the most related previous method is theBayesian method of Titov and Klementiev (2011).In that work, along with predicate-argument struc-ture, they also induce clusterings of dependencytree fragments (not necessarily verbs).
However,their approach uses a different model for argumentgeneration, a different inference procedure, and ithas only been applied and evaluated on biomedi-cal data.
The same shallow semantic parsing taskhas also been considered in the work of Poon andDomingos (2009; 2010), but using a MLN modeland, again, only on the biomedical domain.
An-other closely related vein of research is on semi-supervised frame-semantic parsing (Fu?rstenau andLapata, 2009; Das and Smith, 2011).6 ConclusionsThis work is the first to consider the task of unsuper-vised frame-semantic parsing.
Though the quantita-tive results are mixed, we showed that meaningfulsemantic frames are induced.
In the future work, weintend to consider much larger corpora and to focuson a more general set-up by relaxing the assumptionthat frames are evoked only by verbal predicates.AcknowledgementsThe authors acknowledge the support of the MMCI Clus-ter of Excellence, and thank Caroline Sporleder, AlexisPalmer and the anonymous reviewers for their sugges-tions.6ReferencesDaniel Bauer, Hagen Fu?rstenau, and Owen Rambow.2012.
The dependency-parsed framenet corpus.
InInternational conference on Language Resources andEvaluation (LREC), Istanbul, Turkey.A.
Burchardt, K. Erk, A. Frank, A. Kowalski, S. Pado,and M. Pinkal.
2006.
The SALSA corpus: a germancorpus resource for lexical semantics.
In LREC.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 Shared Task: Semantic Role La-beling.
In CoNLL.D.
Das and N.A.
Smith.
2011.
Semi-supervised frame-semantic parsing for unknown predicates.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 1435?1444.
Associa-tion for Computational Linguistics.D.
Das, N. Schneider, D. Chen, and N.A.
Smith.
2010.Probabilistic frame-semantic parsing.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 948?956.
Associa-tion for Computational Linguistics.K.
Erk and S. Pado.
2006.
Shalmaneser?a toolchain forshallow semantic parsing.
In Proceedings of LREC,volume 6.
Citeseer.Charles J. Fillmore.
1968.
The case for case.
In BachE.
and Harms R.T., editors, Universals in LinguisticTheory, pages 1?88.
Holt, Rinehart, andWinston, NewYork.Hagen Fu?rstenau and Mirella Lapata.
2009.
Graph align-ment for semi-supervised semantic role labeling.
InEMNLP.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-belling of semantic roles.
Computational Linguistics,28(3):245?288.Trond Grenager and Christoph Manning.
2006.
Un-supervised discovery of a statistical verb lexicon.
InEMNLP.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In Proceedings of the13th Conference on Computational Natural LanguageLearning (CoNLL-2009), June 4-5.Richard Johansson and Pierre Nugues.
2008.Dependency-based semantic role labeling of Prop-Bank.
In EMNLP.Joel Lang and Mirella Lapata.
2010.
Unsupervised in-duction of semantic roles.
In ACL.Joel Lang and Mirella Lapata.
2011a.
Unsupervised se-mantic role induction via split-merge clustering.
InACL.Joel Lang and Mirella Lapata.
2011b.
Unsupervisedsemantic role induction with graph partitioning.
InEMNLP.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
Semeval-2010task 14: Word sense induction and disambiguation.
InProceedings of the 5th International Workshop on Se-mantic Evaluation.K.H.
Ohara, S. Fujii, T. Ohori, R. Suzuki, H. Saito, andS.
Ishizaki.
2004.
The japanese framenet project:An introduction.
In Proceedings of LREC-04 SatelliteWorkshop Building Lexical Resources from Semanti-cally Annotated Corpora(LREC 2004), pages 9?11.Alexis Palmer and Caroline Sporleder.
2010.
EvaluatingFrameNet-style semantic parsing: the role of coveragegaps in FrameNet.
In COLING.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In EMNLP.H.
Poon and P. Domingos.
2010.
Unsupervised ontol-ogy induction from text.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 296?305.
Association for Computa-tional Linguistics.Josef Ruppenhofer, Michael Ellsworth, MiriamR.
L. Petruck, Christopher R. Johnson, andJan Scheffczyk.
2006.
Framenet ii: Ex-tended theory and practice.
available at http://framenet.icsi.berkeley.edu/index.php?option=com_wrapper&Itemid=126.C.
Subirats.
2009.
Spanish framenet: A frame-semanticanalysis of the spanish lexicon.
Berlin/New York:Mouton de Gruyter, pages 135?162.Mihai Surdeanu, Adam Meyers Richard Johansson, Llu?
?sMa`rquez, and Joakim Nivre.
2008.
The CoNLL-2008shared task on joint parsing of syntactic and semanticdependencies.
In CoNLL 2008: Shared Task.Richard Swier and Suzanne Stevenson.
2004.
Unsuper-vised semantic role labelling.
In EMNLP.Ivan Titov and Alexandre Klementiev.
2011.
A Bayesianmodel for unsupervised semantic parsing.
In ACL.Ivan Titov and Alexandre Klementiev.
2012.
A bayesianapproach to semantic role induction.
In Proc.
EACL,Avignon, France.7
