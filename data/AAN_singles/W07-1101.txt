Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 1?8,Prague, June 2007. c?2007 Association for Computational LinguisticsA Measure of Syntactic Flexibility for Automatically Identifying MultiwordExpressions in CorporaColin BannardDepartment of Developmental and Comparative PsychologyMax Planck Institute for Evolutionary AnthropologyDeutscher Platz 6D-04103 Leipzigcolin.bannard@eva.mpg.deAbstractNatural languages contain many multi-wordsequences that do not display the variety ofsyntactic processes we would expect giventheir phrase type, and consequently must beincluded in the lexicon as multiword units.This paper describes a method for identify-ing such items in corpora, focussing on En-glish verb-noun combinations.
In an eval-uation using a set of dictionary-publishedMWEs we show that our method achievesgreater accuracy than existing MWE extrac-tion methods based on lexical association.1 IntroductionA multi-word expression (henceforth MWE) is usu-ally taken to be any word combination (adjacent orotherwise) that has some feature (syntactic, semanticor purely statistical) that cannot be predicted on thebasis of its component words and/or the combinato-rial processes of the language.
Such units need to beincluded in any language description that hopes toaccount for actual usage.
Lexicographers (for bothprinted dictionaries and NLP systems) therefore re-quire well-motivated ways of automatically identi-fying units of interest.
The work described in thispaper is a contribution to this task.Many linguists have offered classificationschemes for MWEs.
While these accounts vary intheir terminology, they mostly focus on three differ-ent phenomena: collocation, non-compositionalityand syntactic fixedness.
In computational linguis-tics, a great deal of work has been done on theextraction of collocations in the last decade anda half (see Pecina (2005) for a survey).
Therehave also been a number of papers focusing on thedetection of semantic non-compositional items inrecent years beginning with the work of Schoneand Jurafsky (2001).
The task of identifyingsyntactically-fixed phrases, however, has been muchless explored.
This third variety is the focus ofthe present paper.
Languages contain many wordcombinations that do not allow the variation wewould expect based solely on their grammaticalform.
In the most extreme case there are manyphrases which seem to allow no syntactic variationwhatsoever.
These include phrases such as byand large and in short, which do not allow anymorphological variation (*in shortest) or internalmodification (*by and pretty large).
We focus hereon phrases that allow some syntactic variation, butdo not allow other kinds.The small amount of previous work on the iden-tification of syntactic fixedness (Wermter and Hahn(2004), Fazly and Stevenson (2006)) has either fo-cused on a single variation variety, or has only beenevaluated for combinations of a small preselectedlist of words, presumably due to noise.
In this pa-per we employ a syntactic parser, thus allowing usto include a wider range of syntactic features in ourmodel.
Furthermore we describe a statistical mea-sure of variation that is robust enough to be freelyevaluated over the full set of possible word combi-nations found in the corpus.The remainder of our paper will be structured asfollows.
Section 2 will discuss the kinds of fixednessthat we observe in our target phrase variety.
Sec-1tion 3 will describe our model.
Section 4 will eval-uate the performance of the method and compare itto some other methods that have been described inthe literature.
Section 5 will describe some previouswork on the problem, and section 6 will review ourfindings.2 Syntactic Fixedness in English VerbPhrasesThe experiments described here deal with one par-ticular variety of phrase: English verb phrases of theform verb plus noun (e.g.
walk the dog, pull teeth,take a leaflet).
In a survey of the idiomatic phraseslisted in the Collins Cobuild Dictionary of Idioms,Villavicencio and Copestake (2002) found this kindof idiom to account for more of the entries than anyother.
Riehemann (2001) performed a manual cor-pus analysis of verb and noun phrase idioms foundin the Collins Cobuild Dictionary of Idioms.
Shefound considerable fixedness with some phrases al-lowing no variation at all.Based on this literature we identified three im-portant kinds of non-morphological variation thatsuch phrases can undergo, and which crucially havebeen observed to be restricted for particular combi-nations.
These are as follows:?
Variation, addition or dropping of a determinerso that, for example, run the show becomes runtheir show, make waves becomes make morewaves, or strike a chord becomes strike chordrespectively.?
Modification of the noun phrase so that, for ex-ample, break the ice becomes break the diplo-matic ice.
We refer to this as internal modifica-tion.?
The verb phrase passivises so that, for example,call the shots is realised as the shots were calledby.3 Our ModelWe use the written component of the BNC to makeobservations about the extent to which these varia-tions are permitted by particular verb-noun combi-nations.
In order to do this we need some way toa) identify such combinations, and b) identify whenthey are displaying a syntactic variation.
In order todo both of these we utilise a syntactic parser.We parse our corpus using the RASP system(Briscoe and Carroll, 2002).
The system containsa LR probabilistic parser, based on a tag-sequencegrammar.
It is particularly suited to this task be-cause unlike many contemporary parsers, it makesuse of no significant information about the probabil-ity of seeing relationships between particular lexicalitems.
Since we are looking here for cases wherethe syntactic behaviour of particular word combina-tions deviates from general grammatical patterns, itis desirable that the analysis we use has not alreadyfactored in lexical information.
Example output canbe seen in figure 1.
We extract all verb and nounspairs connected by an object relation in the parsedcorpus.
We are interested here in the object relation-ship between buy and apartment, and we can use theoutput to identify the variations that this phrase dis-plays.The first thing to note is that the phrase is pas-sivised.
Apartment is described as an object of buyby the ?obj?
relation that appears at the end of theline.
Because of the passivisation, apartment is alsodescribed as a non-clausal subject of buy by the ?nc-mod?
relation that appears at the beginning of theline.
This presence of a semantic object that appearsas a surface subject tells us that we are a dealingwith a passive.
The ?ncmod?
relation tells us thatthe adjective largest is a modifier of apartment.
Andfinally, the ?detmod?
relation tells us that the is a de-terminer attached to apartment.
We make a countover the whole corpus of the number of times eachverb-object pair occurs, and the number of times itoccurs with each relation of interest.For passivisation and internal modification, a vari-ation is simply the presence of a particular grammat-ical relation.
The addition, dropping or variation ofa determiner is not so straightforward.
We are inter-ested in the frequency with which each phrase variesfrom its dominant determiner status.
We need there-fore to determine what this dominant status is foreach item.
A verb and noun object pair where thenoun has no determiner relation is recorded as hav-ing no determiner.
This is one potential determinerstatus.
The other varieties of status are defined bythe kind of determiner that is appended.
The RASPparser uses the very rich CLAWS-2 tagset.
We con-2(|ncsubj| |buy+ed:6_VVN| |apartment:3_NN1| |obj|)(|arg_mod| |by:7_II| |buy+ed:6_VVN| |couple:10_NN1| |subj|)(|ncmod| _ |apartment:3_NN1| |largest:2_JJT|)(|detmod| _ |apartment:3_NN1| |The:1_AT|)(|ncmod| _ |couple:10_NN1| |Swedish:9_JJ|)(|detmod| _ |couple:10_NN1| |a:8_AT1|)(|mod| _ |buy+ed:6_VVN| |immediately:5_RR|)(|aux| _ |buy+ed:6_VVN| |be+ed:4_VBDZ|)Figure 1: RASP parse of sentence The largest apartment was immediately bought by a Swedish couple.sider each of these tags as a different determiner sta-tus.
Once the determiner status of all occurrenceshas been recorded, the dominant status for each itemis taken to be the status that occurs most frequently.The number of variations is taken to be the numberof times that the phrase occurs with any other status.3.1 Quantifying variationWe are interested here in measuring the degree ofsyntactic variation allowed by each verb-object pairfound in our corpus.
Firstly we use the counts thatwe extracted above to estimate the probability ofeach variation for each combination, employing aLaplace estimator to deal with zero counts.A straightforward product of these probabilitieswould give us the probability of free variation for agiven verb-object pair.
We need, however, to con-sider the fact that each phrase has a prior probabilityof variation derived from the probability of variationof the component words.
Take passivisation for ex-ample.
Some verbs are more prone to passivisationthan others.
The degree of passivisation of a phrasewill therefore depend to a large extent upon the pas-sivisation habits of the component verb.What we want is an estimate of the extent towhich the probability of variation for that combi-nation deviates from the variation we would expectbased on the variation we observe for its componentwords.
For this we use conditional pointwise mu-tual information.
Each kind of variation is associ-ated with a single component word.
Passivisation isassociated with the verb.
Internal modification anddeterminer variation are associated with the object.We calculate the mutual information of the syntacticvariation x and the word y given the word z, as seenin equation 1.
In the case of passivisation z will bethe verb and y will be the object.
In the case of inter-nal modification and determiner variation z will bethe object.I(x; y|z) = H(x|z) ?
H(x|y, z) (1)= ?
log2 p(x|z) ?
[?
log2 p(x|y, z)]= ?
log2 p(x|z) + log2 p(x|y, z)= log2p(x|y, z)p(x|z)Conditional pointwise mutual information tells usthe amount of information in bits that y providesabout x (and vice versa) given z (see e.g.
MacKay(2003)).
If a variation occurs for a given word pairwith greater likelihood than we would expect basedon the frequency of seeing that same variation withthe relevant component word, then the mutual infor-mation will be high.
We want to find the informa-tion that is gained about all the syntactic variationsby a particular verb and object combination.
Wetherefore calculate the information gained about allthe verb-relevant syntactic variations (passivisation)by the addition of the object, and the informationgained about all the object relevant variations (inter-nal modification and determiner dropping, variationor addition) by the addition of the verb.
Summingthese, as in equation 2 then gives us the total infor-mation gained about syntactic variation for the wordpair W, and we take this as our measure of the degreeof syntactic flexibility for this pair.SynV ar(W )=n?iI(V erbV ari;Obj|V erb) (2)+n?jI(ObjV arj ;V erb|Obj)34 EvaluationThis paper aims to provide a method for highlight-ing those verb plus noun phrases that are syntacti-cally fixed and consequently need to be included inthe lexicon.
This is intended as a tool for lexicog-raphers.
We hypothesize that in a list that has beeninversely ranked with the variability measure validMWEs will occur at the top.The evaluation procedure used here (first sug-gested by Evert and Krenn (2001) for evaluatingmeasures of lexical association) involves producingand evaluating just such a ranking.
The RASP parseridentifies 979,156 unique verb-noun pairs in theBNC.
The measure of syntactic flexibility was usedto inverse rank these items (the most fixed first).1This ranking was then evaluated using a list of id-ioms taken from published dictionaries, by observ-ing how many of the gold standard items were foundin each top n, and calculating the accuracy score.2 By reason of the diverse nature of MWEs, theselists can be expected to contain manyMWEs that arenot syntactically fixed, giving us a very low upperbound.
However this seems to us the evaluation thatbest reflects the application for which the measure isdesigned.
The list of gold standard idioms we usedwere taken from the Longman Dictionary of Englishidioms (Long and Summers, 1979) and the SAIDSyntactically Annotated Idiom Dataset (Kuiper etal., 2003).
Combining the two dictionaries gave usa list of 1109 unique verb-noun pairs, 914 of whichwere identified in the BNC.In order to evaluate the performance of our tech-nique it will be useful to compare its results with theranks of scores that can be obtained by other means.A simple method of sorting items available to thecorpus lexicographer that might expected to givereasonable performance is item frequency.
We takethis as our baseline.
In the introduction we referredto multiple varieties of MWE.
One such variety isthe collocation.
Although the collocation is a dif-ferent variety of MWE, any dictionary will containcollocations as well as syntactically fixed phrases.1Any ties were dealt with by generating a random numberfor each item and ranking the drawn items using this.2Note that because the number of candidate items in eachsample is fixed, the relative performance of any two methodswill be the same for recall as it is for precision.
In such circum-stances the term accuracy is preferred.The collocation has received more attention thanany other variety of MWE and it will therefore beuseful to compare our measure with these methodsas state-of-the-art extraction techniques.
We reportthe performance obtained when we rank our candi-date items using all four collocation extraction tech-niques described in Manning and Schutze (1999) :t-score, mutual information, log likelihood and ?2.4.1 ResultsFigure 2 provides a plot of the accuracy score eachsample obtains when evaluated using the superset ofthe two dictionaries for all samples from n = 1 to n= 5,000.Included in figure 2 are the scores obtained whenwe inverse ranked using the variation score for eachindividual feature, calculated with equation 1.
Thereis notable divergence in the performance of the dif-ferent features.
The best performing feature is pas-sivisation, followed by internal modification.
Deter-miner variation performs notably worse for all val-ues of n.We next wanted to look at combinations of thesefeatures using equation 2.
We saw that the varioussyntactic variations achieved very different scoreswhen used in isolation, and it was by no means cer-tain that combining all features would the best ap-proach.
Nonetheless we found that the best scoreswere achieved by combining all three - an accuracyof 18%, 14.2 and 5.86% for n of 100, 1000 and 5000respectively.
This can be see in figure 2.
The resultsachieved with frequency ranking can also be seen inthe plot.The accuracy achieved by the four collocationmeasures can be seen plotted in figure 3.
The bestperformers are the t-score and the log-likelihood ra-tio, with MI and ?-squared performing much worse.The best score for low values of n is t-score, withlog-likelihood overtaking for larger values.
The bestperforming collocation measures often give a perfor-mance that is only equal to and often worse than rawfrequency.
This is consistent with results reportedby Evert and Krenn (2001).
Our best syntactic vari-ation method outperforms all the collocation extrac-tion techniques.We can see, then, that our method is outperform-ing frequency ranking and the various collocationmeasures in terms of accuracy.
A major claim we40510152025010002000300040005000ACCURACYSAMPLESIZE"PASSIVE,INTERNAL &DETERMINER""FREQUENCY""PASSIVE""INTERNAL MODIFIER""DETERMINERVARIATION"Figure 2: Accuracy by sample size for syntactic variation measuresare making for the method however is that it ex-tracts a different kind of phrase.
A close examina-tion tells us that this is the case.
Table 1 lists thetop 25 verb-noun combinations extracted using ourbest performing combination of features, and thoseextracted using frequency ranking.
As can be seethere is no overlap between these lists.
In the top 50items there is an overlap of 3 between the two lists.Over the top 100 items of the two lists there is onlyan overlap of 6 items and over the top 1000 there isan overlap of only 98.This small overlap compares favourably with thatfound for the collocation scores.
While they pro-duce ranks that are different from pure frequency,the collocation measures are still based on relativefrequencies.
The two high-performing collocationmeasures, t-score and log-likelihood have overlapwith frequency of 795 and 624 out of 1000 respec-tively.
This tells us that the collocation measuresare significantly duplicating the information avail-able from frequency ranking.
The item overlap be-tween t-score items and those extracted using thethe best-performing syntactic variation measure is116.
The overlap between syntactic variation andlog-likelihood items is 108.
This small overlap tellsus that our measure is extracting very different itemsfrom the collocation measures.Given that our measure appears to be pinpoint-ing a different selection of items from those high-lighted by frequency ranking or lexical association,we next want looked at combining the two sourcesof information.
We test this by ranking our candi-date list using frequency and using the most consis-tently well-performing syntactic variation measurein two separate runs, and then adding together thetwo ranks achieved using the two methods for eachitem.
The items are then reranked using the result-ing sums.
When this ranking is evaluated against thedictionaries it gives the scores plotted in figure 3 - aclearly better performance than syntactic fixednessor frequency alone for samples of 1000 and above.Having reported all scores we now want to mea-sure whether any of them are beating frequencyranking at a level that is statistically significant.505101520250200040006000800010000ACCURACYSAMPLESIZE"PASSIVE,INTERNAL,DETERMINER &FREQUENCY""PASSIVE,INTERNAL &DETERMINER""FREQUENCY""TSCORE""LOG LIKELIHOOD""CHI SQUARED" "MI"Figure 3: Accuracy by sample size for lexical association measuresIn order to do this we pick three values of n(100,1000 and 5000) and examine whether the ac-curacy achieved by our method are greater thanthose achieved with frequency ranking at a levelthat is significantly greater than chance.
Conven-tional significance testing is problematic for thistask.
Rather than using a significance test that reliesupon an assumed distribution, then, we will use acomputationally-intensive randomization test of sig-nificance called stratified shuffling.
This techniqueworks by estimating the difference that might occurbetween scores by chance through a simulation (see(Cohen, 1995) for details).
As is standard we per-form 10,000 shuffle iterations.The results for our three chosen values of n canbe seen in table 2.
We accept any result of p < 0.05as significant, and scores that achieve this level ofsignificance are shown in bold.
As an additionalcheck on performance we also extend our evalua-tion.
In any evaluation against a gold standard re-source, there is a risk that the performance of a tech-nique is particular to the lexical resource used andwill not generalise.
For this reason we will here re-port results achieved using not only the combined setbut also each dictionary in isolation.
If the techniqueis effective then we would expect it to perform wellfor both resources.We can see that our syntactic variation measuresperform equal to or better than frequency over bothdictionaries in isolation for samples of 1000 and5000.
The good performance against two data setstells us that the performance does generalise beyonda single resource.
For the Longman dictionary, theaccuracy achieved by the syntactic variation mea-sure employing the three best performing features(?P, I and D?)
is significantly higher (at a level of p< 0.05) than that achieved when ranking with fre-quency for sample sizes of 1000 and 5000.
Theranking achieved using the combination of syntac-tic fixedness and frequency information produces aresult that is significant over all items for samples of1000 and 5000.
By contrast, none of the collocationscores perform significantly better than frequency.
33As very low frequency items have been observed to cause6Syntactic Variation CollocationDICTIONARY Freq P,I &D P,I,D &Freq t MI LLR ?2Top 100 itemsLONGMANS 14 21 15 16 0 13 0SAID 21 17 17 23 0 17 0BOTH 28 18 25 32 0 25 0Top 1000 itemsLONGMANS 6.6 10.4 10.2 6.3 0 6.5 0.3SAID 9.1 9 9.9 9 0 8.1 0.2BOTH 12.2 14.2 15.2 12 0 11.4 0.4Top 5000 itemsLONGMANS 3.24 4.28 4.84 3.12 0.06 3.44 0.58SAID 3.86 3.56 4.54 3.68 0.04 3.86 0.54BOTH 5.56 5.86 7.68 5.34 0.04 5.66 0.88Table 2: Accuracy for top 100, 1000 and 5000 items (scores beating frequency at p < 0.05 are in bold)An important issue for future research is howmuch the performance of our measure is affectedby the technology used.
In an evalutaion of RASP,Preiss (2003) reports an precision of 85.83 and recallof 78.48 for the direct object relation, 69.45/57.72for the ?ncmod?
relation, and 91.15/98.77 for the?detmod?
relation.
There is clearly some variancehere, but it is not easy to see any straightforward re-lationship with our results.
The highest performancerelation (?detmod?)
was our least informative fea-ture.
Meanwhile our other two features both rely onthe ?ncmod?
relation.
One way to address this issuein future research will be to replicate using multipleparsers.5 Previous workWermter and Hahn (2004) explore one kind ofsyntactic fixedness: the (non-)modifiability ofpreposition-noun-verb combinations in German.They extract all preposition-noun-verb combina-tions from a corpus of German news text, and iden-tify all the supplementary lexical information thatoccurs between the preposition and the verb.
Foreach phrase they calculate the probability of seeingeach piece of supplementary material, and take thisas its degree of fixedness.
A final score is then cal-culated by taking the product of this score and theproblems for collocation measures, we experimented with vari-ous cutoffs up to an occurence rate of 5.
We found that this didnot lead to any significant difference from frequency.probability of occurrence of the phrase.
They thenmanually evaluated how many true MWEs occurredin the top n items at various values of n. Like usthey report that their measure outperformed t-score,log likelihood ratio and frequency.Fazly and Stevenson (2006) propose a measurefor detecting the syntactic fixedness of English verbphrases of the same variety as us.
They use a set ofregular patterns to identify, for particular word com-binations (including one of a chosen set of 28 fre-quent ?basic?
verbs), the probability of occurrencein passive voice, with particular determiners and inplural form.
They then calculate the relative en-tropy of this probability distribution for the particu-lar word pair and the probabilities observed over allthe word combinations.
As we pointed out in section3.1 a comparison with all verbs is problematic aseach verb will have its own probability of variation,and this perhaps explains their focus on a small setof verbs.
They use a development set to establish athreshold on what constitutes relative fixedness andcalculate the accuracy.
This threshhold gives overthe set of 200 items, half of which were found ina dictionary and hence considered MWEs and halfweren?t.
They report an accuracy of 70%, against a50% baseline.
While this is promising, their use of asmall selection of items of a particular kind in theirevaluation makes it somewhat difficult to assess.7FREQUENCY P,I & D1 take place follow suit2 have effect draw level3 shake head give rise4 have time part company5 take part see chapter6 do thing give moment7 make decision open fire8 have idea run counter9 play role take refuge10 play part clear throat11 open door speak volume12 do job please contact13 do work leave net14 make sense give way15 have chance see page16 make use catch sight17 ask question cite argument18 spend time see table19 take care check watch20 have problem list engagement21 take step go bust22 take time change subject23 take action change hand24 find way keep pace25 have power see paragraphTable 1: Top 25 phrases6 DiscussionAny lexicon must contain multiword units as wellas individual words.
The linguistic literature con-tains claims for the inclusion of multiword itemsin the lexicon on the basis of a number of linguis-tic dimensions.
One of these is syntactic fixedness.This paper has shown that by quantifying the syntac-tic fixedness of verb-noun phrases we can identify agold standard set of dictionary MWEs with a greateraccuracy than the lexical association measures thathave hitherto dominated the literature, and that, per-haps more crucially, we can identify a different set ofexpressions, not available using existing techniques.AcknowledgementsThanks to Tim Baldwin, Francis Bond, Ted Briscoe,Chris Callison-Burch, Mirella Lapata, Alex Las-carides, Andrew Smith, Takaaki Tanaka and twoanonymous reviewers for helpful ideas and com-ments.ReferencesTed Briscoe and John Carroll.
2002.
Robust accuratestatistical annotation of general text.
In Proceedingsof LREC-2003.P.
Cohen.
1995.
Empirical Methods for Artificial Intelli-gence.
MIT Press.Stefan Evert and Brigitte Krenn.
2001.
Methods for thequalitative evaluation of lexical association measures.In Proceedings of ACL-2001.Afsaneh Fazly and Suzanne Stevenson.
2006.
Automat-ically constructing a lexicon of verb phrase idiomaticcombinations.
In Proceedings of EACL-2006.Koenraad Kuiper, Heather McCann, and Heidi Quinn.2003.
A syntactically annotated idiom database (said),v,1.Thomas H. Long and Della Summers.
1979.
LongmanDictionary of English Idioms.
Longman Dictionaries.David J.C. MacKay.
2003.
Information Theory, Infer-ence and Learning Algorithms.
Cambridge UniversityPress.C.
Manning and H. Schutze.
1999.
Foundations ofStatistical Natural Language Processing.
MIT Press,Cambridge, USA.Pavel Pecina.
2005.
An extensive empirical study ofcollocation extraction methods.
In Proceedings of theACL-2005 Student Research Workshop.Judita Preiss.
2003.
Using grammatical relations to com-pare parsers.
In Proceedings of EACL-03.Suzanne Riehemann.
2001.
A Constructional Approachto Idioms and Word Formation.
Ph.D. thesis, StanfordUniversity.Patrick Schone and Dan Jurafsky.
2001.
Is knowledge-free induction of multiword unit dictionary headwordsa solved problem?
In Proceedings of EMNLP-2001.Aline Villavicencio and Ann Copestake.
2002.
On thenature of idioms.
LinGO Working Paper No.
2002-04.Joachim Wermter and Udo Hahn.
2004.
Collocation ex-traction based on modifiability statistics.
In Proceed-ings of COLING-2004.8
