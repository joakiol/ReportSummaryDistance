Introduction to the Shared Taskon Comparing SemanticRepresentationsJohan BosUniversity of Rome ?La Sapienza?
(Italy)email: bos@di.uniroma1.itAbstractSeven groups participated in the STEP 2008 shared task on comparingsemantic representations as output by practical wide-coverage NLP sys-tems.
Each of this groups developed their own system for producing se-mantic representations for texts, each in their own semantic formalism.Each group was requested to provide a short sample text, producing ashared task set of seven texts, allowing participants to challenge eachother.
Following this, each group was asked to provide the raw systemoutput for all texts, which are made available on http://www.sigsem.org.
Two groups were extremely inspired by the shared task and alsoprovided gold-standard semantic representations for the seven texts, to-gether with evaluation measures.
The STEP 2008 workshop itself willcontinue the discussion, focusing on the feasibility of a theory-neutralgold standard for deep semantic representations.257258 Bos1 IntroductionFollowing advances made in computational syntax in the last years, we have recentlywitnessed progress in computational semantics too.
Thanks to the availability ofwide-coverage parsers, most of them implementing statistical approaches with modelstrained on the Penn Treebank, we now have at our disposal tools that are able to pro-duce formal, semantic representations on the basis of the output of the aforementionedparsers, achieving high coverage.
Computational semantics isn?t anymore limited totedious paper and pencil exercise, nor to implementations of tiny fragments of naturallanguage, and has genuinely matured to a level useful for real applications.As a direct consequence, the question as to how to measure the quality of semanticrepresentations output by these systems pops up.
This is an important issue for thesake of the field, but difficult to answer.
On the one hand one might think that thequality of semantic representations, because they are more abstract than surface andsyntactic representations, should be easy to evaluate.
On the other hand, however,because there are several ?competing?
semantic formalisms, and the depth of analysisis arbitrary, it is hard to define a universal theory-neutral gold standard for semanticrepresentations (see, e.g.
Bos, 2008a).Partly in response to this situation in the field, a ?shared task?
was organised as aspecial event on the STEP 2008 conference.
The aim of this shared task was primarilyto compare semantic representations for texts as output by state-of-the-art NLP sys-tems.
This was seen as a first step for designing evaluation methodologies in computa-tional semantics, with a practical bottom-up strategy: rather than defining theoreticalgold standard representations, we look what current systems can actually produce andstart working from that.2 ParticipantsIn response to the call for participation seven groups were accepted to take part inthe shared task.
Table 1 gives an overview of the participants, the systems they havedeveloped, and the semantic formalism they adopted.
This volume contains full de-scriptions of these systems (please follow the page numbers in Table 1).Table 1: Overview of shared task participants at STEP 2008System Type of Formalism Authors Pages1 BLUE Logical Form Clark and Harrison 263?2762 Boxer Discourse Representation Theory Bos 277?2863 GETARUNS Situation Semantics Delmonte 287?2984 LXGram Minimal Recursion Semantics Branco and Costa 299?3145 OntoSem Ontological Semantics Nirenburg et al 315?3266 TextCap Semantic Triples Callaway 327?3427 Trips Logical Form Allen et al 343?354All but one group have NLP systems developed to deal with the English language.One group has an NLP system for Portuguese (LXGram).
This made it more difficultto organise the task (the English text had to be translated, Branco and Costa (2008)),Introduction to the Shared Task on Comparing Semantic Representations 259but also more interesting.
After all, it is a reasonable assumption that semantic repre-sentations ought to be independent of the source language.Also note that basically all participants adopt different semantic formalisms (Ta-ble 1), even though they all claim to do more or less the same thing: computingsemantic representations for text.
These differences in (formal) background make theshared task only more interesting.3 The Shared Task TextsAll participants were asked to submit an authentic small text, not exceeding five sen-tences and 120 tokens.
The pool of test data for the shared task is composed out of allthe texts submitted by the seven participants.
This procedure allowed the participantsto ?challenge?
each other.
Below are the original texts as submitted by the participants?
the numbering follows the numbering of the participants in Table 1.Text 1An object is thrown with a horizontal speed of 20 m/s from a cliff that is125 m high.
The object falls for the height of the cliff.
If air resistance isnegligible, how long does it take the object to fall to the ground?
What isthe duration of the fall?Text 2Cervical cancer is caused by a virus.
That has been known for some timeand it has led to a vaccine that seems to prevent it.
Researchers have beenlooking for other cancers that may be caused by viruses.Text 3John went into a restaurant.
There was a table in the corner.
The waitertook the order.
The atmosphere was warm and friendly.
He began to readhis book.Text 4The first school for the training of leader dogs in the country is going to becreated in Mortagua and will train 22 leader dogs per year.
In Mortagua,Joao Pedro Fonseca and Marta Gomes coordinate the project that sevenpeople develop in this school.
They visited several similar places in Eng-land and in France, and two future trainers are already doing internship inone of the French Schools.
The communitarian funding ensures the oper-ation of the school until 1999.
We would like our school to work similarlyto the French ones, which live from donations, from the merchandisingand even from the raffles that children sell in school.Text 5As the 3 guns of Turret 2 were being loaded, a crewman who was oper-ating the center gun yelled into the phone, ?I have a problem here.
I amnot ready yet.?
Then the propellant exploded.
When the gun crew waskilled they were crouching unnaturally, which suggested that they knewthat an explosion would happen.
The propellant that was used was made260 Bosfrom nitrocellulose chunks that were produced during World War II andwere repackaged in 1987 in bags that were made in 1945.
Initially it wassuspected that this storage might have reduced the powder?s stability.Text 6Amid the tightly packed row houses of North Philadelphia, a pioneer-ing urban farm is providing fresh local food for a community that oftenlacks it, and making money in the process.
Greensgrow, a one-acre plotof raised beds and greenhouses on the site of a former steel-galvanizingfactory, is turning a profit by selling its own vegetables and herbs as wellas a range of produce from local growers, and by running a nursery sell-ing plants and seedlings.
The farm earned about $10,000 on revenue of$450,000 in 2007, and hopes to make a profit of 5 percent on $650,000 inrevenue in this, its 10th year, so it can open another operation elsewherein Philadelphia.Text 7Modern development of wind-energy technology and applications waswell underway by the 1930s, when an estimated 600,000 windmills sup-plied rural areas with electricity and water-pumping services.
Once broad-scale electricity distribution spread to farms and country towns, use ofwind energy in the United States started to subside, but it picked up againafter the U.S. oil shortage in the early 1970s.
Over the past 30 years, re-search and development has fluctuated with federal government interestand tax incentives.
In the mid-?80s, wind turbines had a typical maximumpower rating of 150 kW.
In 2006, commercial, utility-scale turbines arecommonly rated at over 1 MW and are available in up to 4 MW capacity.The first text is taken from an AP Physics exam (the fourth sentence is a simplifiedreformulation of the third sentence) and constitutes a multi-sentence science question(Clark and Harrison, 2008).
Text 2 is taken from the Economist, with the third sen-tence slightly simplified (Bos, 2008b).
Text 4 was taken from a Portuguese newspaperand translated into English (Branco and Costa, 2008).
Text 6 is also a fragment of anewspaper article, namely the New York Times (Callaway, 2008).
Text 7 is an excerptfrom http://science.howstuffworks.com.
The origin of Text 3 is unknown.4 Preliminary ResultsAll groups produced semantic representations for the texts using their NLP systems.The results are, for obvious reasons of space, not all listed here, but available at theSIGSEM website http://www.sigsem.org.
The papers that follow the current arti-cle describe the individual results in detail.
It should be noted that two groups createdgold standard representations for all seven texts, and already performed a self evalua-tion (Nirenburg et al, 2008; Allen et al, 2008).The workshop itself (to be held in Venice, September 2008) will feature furthercomparison and manual evaluation of the systems?
output?
the system with the mostcomplete and accurate semantic representation will receive a special STEP award.This event should naturally lead to a discussion on the feasibility of a gold standardIntroduction to the Shared Task on Comparing Semantic Representations 261for deep semantic representations, and furthermore identify a set of problematic andrelevant issues for semantic evaluation.ReferencesAllen, J. F., M. Swift, and W. de Beaumont (2008).
Deep Semantic Analysis of Text.In J. Bos and R. Delmonte (Eds.
), Semantics in Text Processing.
STEP 2008 Con-ference Proceedings, Volume 1 of Research in Computational Semantics, pp.
343?354.
College Publications.Bos, J.
(2008a).
Let?s not argue about semantics.
In Proceedings of the 6th LanguageResources and Evaluation Conference (LREC 2008), Marrakech, Morocco.Bos, J.
(2008b).
Wide-Coverage Semantic Analysis with Boxer.
In J. Bos and R. Del-monte (Eds.
), Semantics in Text Processing.
STEP 2008 Conference Proceedings,Volume 1 of Research in Computational Semantics, pp.
277?286.
College Publica-tions.Branco, A. and F. Costa (2008).
LXGram in the Shared Task ?Comparing SemanticRepresentations?
of STEP 2008.
In J. Bos and R. Delmonte (Eds.
), Semantics inText Processing.
STEP 2008 Conference Proceedings, Volume 1 of Research inComputational Semantics, pp.
299?314.
College Publications.Callaway, C. B.
(2008).
The TextCap Semantic Interpreter.
In J. Bos and R. Del-monte (Eds.
), Semantics in Text Processing.
STEP 2008 Conference Proceedings,Volume 1 of Research in Computational Semantics, pp.
327?342.
College Publica-tions.Clark, P. and P. Harrison (2008).
Boeing?s NLP System and the Challenges of Seman-tic Representation.
In J. Bos and R. Delmonte (Eds.
), Semantics in Text Processing.STEP 2008 Conference Proceedings, Volume 1 of Research in Computational Se-mantics, pp.
263?276.
College Publications.Delmonte, R. (2008).
Semantic and Pragmatic Computing with GETARUNS.
InJ.
Bos and R. Delmonte (Eds.
), Semantics in Text Processing.
STEP 2008 Confer-ence Proceedings, Volume 1 of Research in Computational Semantics, pp.
287?298.
College Publications.Nirenburg, S., S. Beale, and M. McShane (2008).
Baseline Evaluation of WSD andSemantic Dependency in OntoSem.
In J. Bos and R. Delmonte (Eds.
), Semanticsin Text Processing.
STEP 2008 Conference Proceedings, Volume 1 of Research inComputational Semantics, pp.
315?326.
College Publications.
