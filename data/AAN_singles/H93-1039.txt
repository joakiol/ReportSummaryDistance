But  Dict ionar ies Are Data  TooPeter F. Brown, Stephen A. Della Pietra, Vincent J. Della Pietra,Meredith J. Goldsmith, Jan Hajic, Robert L. Mercer, and Surya MohantyIBM Thomas J. Watson Research CenterYorktown Heights, NY 10598ABSTRACTAlthough empiricist approaches tomachine translationdepend vitally on data in the form of large bilingual cor-pora, bilingual dictionaries are also a source of information.We show how to model at least a part of the informationcontained in a bilingual dictionary so that we can treat abilingual dictionary and a bilingual corpus as two facets ofa unified collection of data from which to extract valuesfor the parameters of a probabilistic machine translationsystem.
We give an algorithm for obtaining maximum iike-fihood estimates of the parameters ofa probabilistic modelfrom this combined ata and we show how these param-eters are affected by inclusion of the dictionary for somesample words.There is a sharp dichotomy today between ratio-nalist and empiricist approaches to machine transla-tion: rationalist systems are based on information ca-joled fact by reluctant fact from the minds of humanexperts; empiricist systems are based on informationgathered wholesale from data.
The data most readilydigested by our translation system is from bilingualcorpora, but bilingual dictiona.ries are data too, andin this paper we show how to weave information fromthem into the fabric of our statistical model of thetranslation process.When a lexicographer creates an entry in a bilin-gum dictionary, he describes in one language themeaning and use of a word from another language.Often, he includes a list.
of simple translations.
For ex-ample, tile entry for disingenuousness in the Harper-Collins Robert French Dictionary \[1\] lists the trans-lations d(loyautd, manque de sincdritd, and fourberie.In constructing such a list., the lexicographer gath-ers, either through introspection or extrospection, in-202stances in which disingenuousness has been used invarious ways and records those of the different rans-lations that he deems of sufficient importance.
Al-though a dictionary is more than just a collection oflists, we will concentrate here on that portion of itthat is made up of lists.We formalize an intuitive account of lexicographicbehavior as follows.
We imagine that a lexicographer,when constructing an entry for the English word orphrase e, first chooses a random size s, and then se-lects at random a sample of s instances of the use of e,each with its French translation.
We imagine, further,that he includes in his entry for e a list consisting ofall of tile translations that occur at least once in hisrandom sample.
The probability that he will, in thisway, obtain tile list f i ,  .
.
.
,  f,,, isPr(fl,..., f,,, le) =sl>O 0 S I ' ' "(1)s,~/Pr(s le)  1-I~=1 Pr(file) s',where Pr(f, le) is the probability from our statisticalmodel that the phrase f, occurs as a translation of e,and Pr(sle) is the probability that the lexicographerchooses to sample s instances of e. The multinomialcoefficient is defined bys ) s!sl .
.
.sk - s1!
.
.
.
sk ! '
(2)and satisfies the recursion(3)$ where (,t) is the usual binomial coefficient.In genera.I, the sum in Equation (1) cannot be eval-uated in closed form, but we can organize an efficientcalculation of it as follows.
Leta H p(f~le)".
(4) E-.
Es~>0 s~>0 i=1Clearly,P( f l , ""  ,fml e) = ~ P(sle)ot(s, m).
(5)$Using Equation (3), it is easy to show that= p(f, le) - - 1 ) ,  (6 )and therefore, we can compute P( f l , ' " , fmle )  in timeproportional to s2m.
By judicious use of thresholds,even this can be substantiMly reduced.In the special case that Pr(s\[e) is a Poisson distri-bution with mean l(e), i.e., thatPr(sle ) -- A(e) 'e-~(e) s! '
(7)we can carry out the sum in Equation (1) explicitly,Tr~Pr(f l , - .
- , fmle) = e -x(e) H(e  x(e)p(f'le) - 1).
(8)i=1This is the form that we will assume throughout heremainder of the paper because of its simplicity.
No-tice that in this case, the probability of an entry isa product of factors, one for each of the translationsthat it contains.The series fi, .-., fm represents the translationsof e that are included in the dictionary.
We call thisset of translations De.
Because we ignore everythingabout the dictionary except for these lists, a completedictionary is just.
a collection of De's, one for eachof the English phrases that has an entry.
We treateach of these entries as independent and write theprobability of the entire dictionary asPr(D) ~ H Pr(Del e)' (9)eEDthe product here running over all entries.Equation (9) gives the probability of the dictio-nary in terms of the probabilities of the entries that203make it up.
The probabilities of these entries in turnare given by Equation (8) in terms of the probabilities,p(fle), of individual French phrases given individualEnglish phrases.
Combining these two equations, wecan writeP r (D)= H (ex(e)p(rle) -1 )  H e-X(e)" (10)(e,f)ED e~DWe take p(fle) to be given by the statistical modeldescribed in detail by Brown et al \[2\].
Their modelhas a set of translation probabilities, t(f le), giving foreach French word f and each English word e the prob-ability that f will appear as (part of) a translation ofe; a set of fertility probabilities, n(?le), giving for eachinteger ?
and each English word e the probability thate will be translated as a phrase containing ?
Frenchwords; and a set of distortion probabilities governingthe placement of French words in the translation ofan English phrase.
They show how to estimate theseparameters so as to maximize the probability,er(//)= H p(rl ), (1\])(e,f)EHof a collection of pairs of aligned translations, (e, f) E//.Let O represent the complete set of parameters ofthe model of Brown et al \[2\], and let 0 representany one of the parameters.
We extend the methodof Brown et al to develop a scheme for estimatingO so as to maximize the joint probability of the cor-pus and the dictionary, Pro( / / ,D) .
We assume thatPro(/ / ,  D) = Pro( / / )P ro(D) .
In general, it is pos-sible only to find local maxima of P ro( / / ,D)  as afunction of O, which we can do-by applying the EMalgorithm \[3, 4\].
The EM algorithm adjusts an initialestimate of O in a series of iterations.
Each itera-tion consists of an estimation step in which a count isdetermined for each parameter, followed by a maxi-mization step in which each parameter is replaced bya value proportional to its count.
The count ce for aparameter 0 is defined byco = a~0 log Pro(/ / ,  D).
(12)Because we assume that II and D are independent,we can write ce as the sum of a count for H and acount for D:ce = co(It) + co(D).
03)The corpus count is a sum of counts, one for eachtranslation in the corpus.
The dictionary count is alsoa sum of counts, but with each count weighted by afactor #(e,f)  which we call the effective multiplicityof the translation.
Thus,ce(H)= cde, f) 04)(e,f)?~and(e,f)ce(e,f) 05)(e,f)EDwith0cs(e,f) = e~0 logpo(fle ).
(16)The effective multiplicity is just the expected num-ber of times that our lexicographer observed thetranslation (e,f)  given the dictionary and the cor-pus.
In terms of the a priori multiplicity, p0(e,f)  =A(e)p(fle), it is given by#0(e,f)/s(e,f) - 1 - e-~0(e,f)" (17)Figure I shows the effective multiplicity as a func-tion of the a priori multiplicity.
For small valuesof lL0(e,f), /s(e,f) is approximately equal to 1 +#o(e, f)/2.
For very large values, #0(e, f) and p(e, f)are approximately equal.
Thus, if we expect a priori.that the lexicographer will see the translation (e, f )very many times, lhen the effective multiplicity willbe nearly equal to this number, but even if we expecta priori that he will scarcely ever see a translation, theeffective multiplicity for it cannot fall below 1.
Thisis reasonable because in our model for the dictionaryconstruction process, we assume that nothing can getinto the dictionary unless it is seen at least once bythe lexicographer.RESULTSWe have used the algorithm described above to es-timate translation probabilities and fertilities for ourstatistical model in two different ways.
First, we es-timated them from the corpus alone, then we esti-mated them from the corpus and the dictionary to-gether.
The corpus that we used is the proceedingsof the Canadian Parliament described elsewhere \[2\].The dictionary is a machine readable version of theHarperCollins Robert French Dictionary \[1\].We do not expect hat including informa.tion homthe dictionary will have much effect on words that20450- f~"  - : / -!_ ?
-/ /~/ / / /  .
.
./ / .// I I I I0 1 2 3 4 5/1,0Figure I: Effective multiplicity vs P0occur frequently in the corpus, and this is borne outby the data.
But.
for words that are rare, we expectthat there will be an effect._f tCfle) \[ ?
n(?le)toundra .233duns .097antre .048poser .048ceux .0483 .6449 .1601 .1442 .0210 .029Table 1: Parameters for tundra, corpus onlyf t(fle) ~ -(4'le)toundra .665duns .040autre .020poser .020ceux .020I .8553 .0890 .0299 .022Table 2: Parameters for tundra, corpus and dictionaryTables 1 and 2 show the two results for the Englishword tundra.
The entry for tundra in the Harper?Collins Robert French Dictionary \[1\] is simply theword toundra.
We interpret this as a list with onlyone entry.
We don't know how many times the lexi-cography ran across tundra, translated as toundra., butwe know that it was at least once, and we know thathe never ran across it translated as anything else.Even without the dictionary, toundra appears as themost probable translation, but with the dictionary, itsprobability is greatly improved.
A more significantfact is the change in the fertility probabilities.
Rarewords have a tendency to act as garbage collectorsin our system.
This is why tundra, in the absenceof guidance from the dictionary has, 3 as its mostprobable fertility and has a significant probability offertility 9.
With the dictionary, fertility 1 becomesthe overwhelming favorite, and fertifity 9 dwindles toinsignificance.Tables 3 and 4 show the trained parameters forjungle.
The entry for jungle in the HarperCollinsRobert French Dictionary is simply the word jun-gle.
As with tundra using the dictionary enhancesthe probability of the dictionary translation of jungleand also improves the fertility substantially,f *(fl e ) ?
n(?l e )jungle .277darts .072fouillis .045domaine .017devenir .017imbroglio .0172 .4011 .3545 .1203 .0804 .0206 .019Table 3: Parameters for jungle, corpus onlyf t(fle) , ?
n(?le)jungle .442dans .057fouillis .036domaine .014devenir .014imbroglio .0141 .5985 .0743 .0492 .0244 .0126 .012Table 4: Parameters for jungle, corpus and dictionaryprobabilistic functions of a Markov process," In-equalities, vol.
3, pp.
1-8, 1972.\[4\] A. Dempster, N. Laird, and D. Rubin, "Maximumlikelihood from incomplete data via the EM algo-rithm," Journal of the Royal Statistical Society,vol.
39, no.
B, pp.
1-38, 1977.REFERENCES\[1\] B.T.
Atkins, A. Dnv~, R. C. Milne, P.-H. Cousin,H.
M. A. Lewis, L. A. Sinclair, R. O. Birks, andM.-N. Lamy, HarperCollins Robert French Dictio-nary.
New York: Harper & Row, 1990.\[2\] P. F. Brown, S. A. DellaPietra, V. J. DellaPietra,and R. L. Mercer, "The mathematics of machinetranslation: Parameter estimation."
Submitted toComputational Linguistics, 1992.\[3\] L. Baum, "An inequality and associated max-imization technique in statistical estimation ?~'05z
