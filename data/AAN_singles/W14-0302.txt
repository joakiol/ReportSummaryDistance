Workshop on Humans and Computer-assisted Translation, pages 10?15,Gothenburg, Sweden, 26 April 2014. c?2014 Association for Computational LinguisticsProofreading Human Translations with an E-penVicent Alabau and Luis A. LeivaPRHLT Research CenterUniversitat Polite`cnica de Vale`ncia{valabau,luileito}@prhlt.upv.esAbstractProofreading translated text is a taskaimed at checking for correctness, con-sistency, and appropriate writing style.While this has been typically done witha keyboard and a mouse, pen-baseddevices set an opportunity for makingsuch corrections in a comfortable way,as if proofreading on physical paper.Arguably, this way of interacting witha computer is very appropriate whena small number of modifications arerequired to achieve high-quality stan-dards.
In this paper, we propose a tax-onomy of pen gestures that is tailoredto machine translation review tasks, af-ter human translator intervention.
Inaddition, we evaluate the recognitionaccuracy of these gestures using a cou-ple of popular gesture recognizers.
Fi-nally, we comment on open challengesand limitations, and discuss possibleavenues for future work.1 IntroductionCurrently, the workflow of many translationagencies include a final reviewing or proof-reading process1where the translators?
workis checked for correctness, consistency andappropriate writing style.
If the translationquality is good enough, only a small amountof changes would be necessary to reach ahigh-quality result.
However, the requiredcorrections are often spread sparingly andunequally among the screen, which renders1The reviewing process can be seen as a detailedproofreading process where the target sentence is alsocompared against the source sentence for errors such asmistranslations, etc.
However, for the purpose of thispaper, we can use the terms reviewing and proofread-ing indistinguishably.mouse/keyboard interaction both inefficientand unappealing.As a result of the popularization of touch-screen and pen-based devices, text-editing ap-plications can be operated today in a simi-lar way people interact with pen and paper.This way of reviewing is arguably more natu-ral and efficient than a keyboard or a mouse,since the e-pen can be used both to locate andcorrect an erroneous word, all at once.
Ad-ditionally, the expressiveness of e-pen interac-tion provides an opportunity to integrate use-ful gestures that are able correct other com-mon mistakes, such as word reordering or cap-italization.2 Related WorkThe first attempt that we are aware of to post-edit text with an e-pen interface dates back tothe early seventies of the past century (Cole-man, 1969).
In that work, Coleman proposeda set of unistroke gestures for post-editing.Later on, the same corpus was used by (Ru-bine, 1991) in his seminal work about gesturerecognition with excellent recognition results.However, the gesture set is too simplistic to beused in a real translation task today.Most of the modern applications to generateand edit textual content using ?digital ink?
arebased on ad-hoc interaction protocols2and of-ten do not ship handwriting recognition soft-ware.
To our knowledge, MyScript Notes Mo-bile3is the closest system to provide a naturalonscreen paper-like interaction style, includ-ing some text-editing gestures and a powerfulhandwriting recognition software.
However,this application relies on spatial relations ofthe ink strokes to perform handwriting recog-2http://appadvice.com/appguides/show/handwriting-apps-for-ipad3http://www.visionobjects.com10nition.
For instance, to insert a new wordin the middle of a sentence the user needs tomake room for space explicitly (i.e., if the wordhas N characters, the user needs to perform anInsert Space gesture N times).
Moreover, theproduced text does not flow on the UI, i.e., it isfixed to the position of the ink, which makesit difficult to modify.
As a result, this sys-tem does not seem suitable for reviewing trans-lations.
Other comparable work is MinGes-tures (Leiva et al., 2013), which proposes asimplified set of gestures for interactive textpost-editing.
Although MinGestures is veryefficient and accurate, it is also very limited inexpressiveness.
Only basic edition capabilitiesare allowed (insertion, deletion, and substitu-tion).
Thus, advanced e-pen gestures cannotbe used to improve the efficiency of the re-viewer.On the other hand, there are applicationsfor post-editing text where user interactionsare leveraged to propagate text corrections tothe rest of the sentence.
CueTIP (Shilman etal., 2006), CATTI (Romero et al., 2009) andIMT (Alabau et al., 2014) are the most ad-vanced representatives of this kind of applica-tions.
These systems allow the user to cor-rect text either in the form of unconstrainedcursive handwriting or (limited) pen gestures.Then, the corrections are leveraged by the sys-tem to provide smart auto-completion capa-bilities.
This way, user interaction is not onlytaken into account to amend the proposed cor-rection but other mistakes in the surroundingtext are automatically amended as well.
How-ever, user interaction is limited in these cases.In CueTIP, only one handwritten charactercan be submitted at a time and only 4 ges-tures can be performed (join, split, delete, andsubstitution).
In CATTI, the user can hand-write text freely but is still limited to perform4 gestures as well (substitute, insert, delete,and reject).
Finally, IMT does not supportgestures other than substitution.
Althoughthe auto-completion capability is a very inter-esting and promising topic, it should not beconsidered for reviewing: given the locality ofthe small amount of changes that are probablyneeded, auto-completion can make more harmthan good.Thus, in light of the current limitations ofstate-of-the-art approaches, in this work wepresent an exploratory research of how paper-like interaction should be approached to allowproofreading translated texts.3 A Taxonomy of ProofreadingGesturesIndicating text modifications on a sheet ofpaper can be made in many different ways.However, the lack of a consensus may leadto misinterpretations.
Fortunately, a seriesof authoritative proofreading and copy-editingsymbols have been proposed (AMA, 2007;CMO, 2010), even leading to an eventual stan-dardization (BS, 2005; ISO, 1983).We have studied the aforementioned author-itative sources and have found that there is ahuge overlap in the proposed symbols, withonly minor variations.
Moreover, such sym-bols are meant to ease human-human com-munication and therefore we need to adaptthem to ease human-computer communica-tion.
This way, we will focus on those sym-bols that could be used to review using stroke-based gestures.
As such, we will study gesturesthat allow to change the content and not theformatting of the text.
We can define the fol-lowing high-level operations; see Figure 1:Word change: change text?s written form.Letter case: change word/character casing.Punctuation: insert punctuation symbols.Word combination: separate or join words.Selection: select words or characters.Text displacement: move text around.It is worth noting that punctuation sym-bols are represented explicitly in the litera-ture, probably because of their importance incopy-editing tasks.
In addition, dot and hy-phen symbols are represented differently fromother insertion symbols.
The purpose of thisconvention is to reduce visual ambiguity in hu-man recognition.
Finally, the selection opera-tion is often devoted to spell out numbers orabbreviations.4 Preliminary EvaluationThe initial taxonomy (Figure 1) aims to be acomplete set of symbols for proofreading andcopy-editing onscreen.
Nonetheless, the suc-cess of these gestures will depend on the accu-11APOS QUOT DOTCOMMA SEMI COLON COLONPunctuationDELETE INSERT TEXTWord changeLOWERCASE UPPERCASE CAMELCASELetter caseENCIRCLESelectionREMOVE SPACE INSERT SPACE HYPHENWord combinationMOVE SELECTION FORWARD SWAP BLOCKS TRANSPOSE TEXT BLOCKSMOVE SELECTION BACKWARDText displacementFigure 1: Initial taxonomy, based on de facto proofreading symbols.racy of gesture recognizers, to correctly trans-late gestures into commands.As a first approach, we wanted to evalu-ate these symbols with state-of-the-art gesturerecognizers.
The initial taxonomy differs sig-nificantly from other gesture sets in the liter-ature (Anthony and Wobbrock, 2012; Vatavuet al., 2012), in the sense that the symbols weare researching are not expected to be drawnin isolation.
Instead, reviewers will issue a ges-ture in a very specific context, and so a proof-reading symbol may change its meaning.
Thisis specially true for symbols involving multiplespans of text or block displacements: depend-ing of the size of the span or the length of thedisplacement, the aspect ratio and proportionsamong the different parts of the gesture strokesmay vary.
Thus, the final shape of the gesturecan be significantly different.
An example isgiven in Figure 2.Lorem ipsum dolor sit amet(a) Move forward with 1 selected word and 2 worddisplacement.Lorem ipsum dolor sit amet(b) Move forward with 4 selected words and 1 worddisplacement.Figure 2: Examples of the same gesture ex-ecuted with different proportions.
As a re-sult, the shapes of both gestures significantlydiverge from each other.4.1 Gesture Samples AcquisitionWe carried out a controlled study in a real-world setup.
We developed an applicationthat requested a set of random challenges tothe users (Figure 3).
Then, we asked theusers if they would prefer to do the acquisi-tion on a digitizer tablet or on a tablet com-puter.
On a 1 to 5 point scale, with 1 mean-ing ?I prefer writing with a digitizer pen?
and5 ?I prefer writing with a pen-capable tablet?,users indicated that they would prefer a tabletcomputer (M=4.6, SD=0.8).
Consequently,we deployed the application into a LenovoThinkPad tablet, which had to be operatedwith an e-pen.
To make the paper-like ex-perience more realistic, the touchscreen func-tionality was disabled, so that users could resttheir hands on the screen.
Eventually, 12 usersaged 24?36 submitted 5 times each gesture fol-lowing the aforementioned random challenges.Figure 3: Acquisition application.4.2 The Family of $ RecognizersIn HCI, there is a popular ?dollar series?of template-matching gesture recognizers, us-ing a nearest-neighbor classifier with scoringfunctions based on Euclidean distance.
The$ recognizers present several advantages overother classifiers based on more complex pat-tern recognition algorithms.
First, $ recogniz-ers are easily understandable and fast to in-tegrate or re-implement in different program-ming languages.
Second, they do not dependon large amounts of training data to achieve12high accuracy, just on a small number of pre-defined templates.In particular, $N (Anthony and Wobbrock,2012) and $P (Vatavu et al., 2012) can beused to recognize multi-stroke gestures, sothey were the only suitable candidates to rec-ognize our initial gesture taxonomy.
On theone hand, $N deals with multiple strokes byrecombining in every possible way the strokesof the templates in order to generate new in-stances of unistroke templates, and then ap-ply either the $1 recognizer (Wobbrock et al.,2007) or Protractor (Li, 2010).
On the otherhand, $P considers gesture strokes as a cloudof points, removing thus information aboutstroke sequentiality.
Then, the best matchis found using an approximation of the Hun-garian algorithm, which pairs points from thetemplate with points of the query gesture.4.3 ResultsWe evaluated three fundamental aspects ofthe recognition process: accuracy, recognitiontime and memory requirements to store thewhole set of templates.
Aiming for a portablerecognizer that could work on most everydaydevices, we decided to use a JavaScript (ro-tation invariant) version of the $ family rec-ognizers.
Experiments were executed as anodejs program on a Ubuntu Linux computerwith a 2.83 GHz Intel QuadCoreTMand 4 GBof RAM.
We followed a leaving-one-out (LOO)setup, i.e., each user?s set of gestures was usedas templates and tested against the rest of theuser?s gestures.
All the values show the aver-age of the different LOO runs.Table 1 summarizes the experimental re-sults.
For the $N recognizer we found that,by resampling to 32 points and 5 templates,we can achieve very good recognition times(0.7ms in average) but high recognition errorrate (23.6%).
On the other hand, the $P rec-ognizer behaves even worse, with 27.1% errorrate.
Memory requirements are marginal butrecognition times increase more than one orderof magnitude.It must be noted that the space needed by$N to store just one template of n strokes isn!
?
2ntimes the space for the original tem-plate (Vatavu et al., 2012).
This is actuallya huge waste of resources.
For instance, onetemplate of the insert space gesture requiresRecognizer Error Time Mem.
usage$N 23.6% 0.7 ms 102 MB$P 27.1% 45 ms 1.8 MBTable 1: Results for $N and $P recognizers,with gestures resampled to 32 points and using5 templates per gesture.3840 times the original size, assuming that theuser has introduced the minimum strokes re-quired.
With a resampling 8 points, $N needsalmost 33MB of RAM to store 5 templates pergesture.4.4 Error analysisSurprised by the high error rates we decidedto delve into the results of the most accuratesetup so we could find the source of errors.We observed that the most difficult gestureto recognize was remove space, which rep-resented 12% of the total number of errors;being confused with comma and semi colonmore than 50% of the time, probably becausethey are formed by two arcs.
It was also con-fused, though less frequently, with move se-lection forward/backward.
These ges-tures, excepting the circle part, are also com-posed by two arcs.On the other hand, punctuation symbols ac-counted for 37% of the errors, being mostlyconfused with each other, as they have verysimilar shapes.
Finally, some errors are harderto dissect.
For instance, uppercase wasconfused mainly with both move selection(4.4% of the errors), and punctuation and dis-placement operations were also confused witheach other at some time, despite their very dif-ferent visual shapes and sizes.
We suspect itis because of the internal normalization proce-dures of the $ recognizers.5 DiscussionOur results suggest that the $ family of gesturerecognizers, although popular, are not appro-priate for proofreading translated texts.
Ourassumption is that the normalization proce-dures of these recognizers?mainly scaling andresampling?
are not appropriate to gesturesfor which the proportions of its constituentparts may vary according to the context.
For13Figure 4: One proposal for gesture set sim-plification.
A Pop-up menu could assist theuser to disambiguate among perceptually sim-ilar gestures.example, after resizing a move selectionforward that selects a small word and hasa long arrow, the final shape would be primar-ily that of the arrow (Figure 2).In the light of this analysis, several actionscan be taken for future work.
Firstly, othergesture recognizers should be explored thatcan deal with stroke sequences without resam-pling (Myers and Rabiner, 1981; Sezgin andDavis, 2005;?Alvaro et al., 2013).
However, itmust be remarked that response time is crucialto ensure an adequate user experience.
There-fore, the underlying algorithms should be im-plementable on thin clients, such as mobile de-vices, with reasonable recognition times.Secondly, it would be also necessary to re-duce the set of gestures, but not at the expenseof reducing also expressiveness as Leiva et al.
(2013) did.
For instance, taking advantage ofthe interaction that computers can provide, wecan group punctuation operations, space, andinsert hyphen all into insert above andbelow gestures.
Both gestures would pop-up a menu where the user could select deter-ministically the symbol to insert; see Figure 4.In the same manner, letter casing operationscould be grouped into a single selection cat-egory, which would also provide a contextualmenu to trigger the right command.
The re-sulting set of gestures should be, in principle,much easier to recognize.Additionally, the current set of proofread-ing gestures present further challenges.
Forinstance, we would need to identify the seman-tics of the gestures, i.e., which elements in thetext are affected by the gesture and how thesystem should proceed to accomplish the task.6 ConclusionsIn this work we have defined a set of gesturesthat is suitable for the reviewing process ofhuman-translated text.
We have performedan evaluation on gestures generated by realusers that show that popular recognizers arenot able to achieve a satisfactory accuracy.
Inconsequence, we have identified a series of ar-eas for improvement that could make e-pendevices realizable in the near future.7 AcknowledgmentsThis work is supported by the 7th Frame-work Program of European Commission un-der grant agreements 287576 (CasMaCat) and600707 (tranScriptorium).ReferencesV.
Alabau, A. Sanchis, and F. Casacuberta.
2014.Improving on-line handwritten recognition in in-teractive machine translation.
Pattern Recogni-tion, 47(3):1217?1228.2007.
AMA manual of style: A guide for authorsand editors.
10th ed.
Oxford University Press.L.
Anthony and J. O. Wobbrock.
2012.
$N-protractor: a fast and accurate multistroke rec-ognizer.
In Proc.
GI, pages 117?120.2005.
BS 5261-2:2005.
Copy preparation and proofcorrection.2010.
The Chicago manual of style.
16th ed.
Uni-versity Of Chicago Press.M.
L. Coleman.
1969.
Text editing on a graphicdisplay device using hand-drawn proofreader?ssymbols.
In Pertinent Concepts in ComputerGraphics, Proc.
2nd Univ.
Illinois Conf.
onComputer Graphics, pages 283?290.1983.
ISO 5776:1983.
Symbols for text correction.L.
A. Leiva, V. Alabau, and E. Vidal.
2013.
Error-proof, high-performance, and context-aware ges-tures for interactive text edition.
In Proc.
CHIEA, pages 1227?1232.Y.
Li.
2010.
Protractor: a fast and accurate ges-ture recognizer.
In Proc.
CHI, pages 2169?2172.C.
S. Myers and L. R. Rabiner.
1981.
A compar-ative study of several dynamic time-warping al-gorithms for connected-word.
Bell System Tech-nical Journal.14V.
Romero, L. A. Leiva, A. H. Toselli, and E. Vidal.2009.
Interactive multimodal transcription oftext images using a web-based demo system.
InProc.
IUI, pages 477?478.D.
Rubine.
1991.
Specifying gestures by example.In Proc.
SIGGRAPH, pages 329?337.T.
M. Sezgin and R. Davis.
2005.
HMM-basedefficient sketch recognition.
In Proc.
IUI, pages281?283.M.
Shilman, D. S. Tan, and P. Simard.
2006.CueTIP: a mixed-initiative interface for correct-ing handwriting errors.
In Proc.
UIST, pages323?332.R.
D. Vatavu, L. Anthony, and J. O. Wobbrock.2012.
Gestures as point clouds: A $P recognizerfor user interface prototypes.
In Proc.
ICMI,pages 273?280.J.
O. Wobbrock, A. D. Wilson, and Y. Li.
2007.Gestures without libraries, toolkits or training:A $1 recognizer for user interface prototypes.
InProc.
UIST, pages 159?168.F.
?Alvaro, J.-A.
Sa?nchez, and J.-M.
Bened??.
2013.Classification of on-line mathematical symbolswith hybrid features and recurrent neural net-works.
In Proc.
ICDAR, pages 1012?1016.15
