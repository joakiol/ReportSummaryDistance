Proceedings of the 12th European Workshop on Natural Language Generation, pages 58?65,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsReferring Expression Generation through Attribute-Based HeuristicsRobert Dale and Jette ViethenCentre for Language TechnologyMacquarie UniversitySydney, Australiardale@ics.mq.edu.au|jviethen@ics.mq.edu.auAbstractIn this paper, we explore a corpus ofhuman-produced referring expressions tosee to what extent we can learn the referen-tial behaviour the corpus represents.
De-spite a wide variation in the way subjectsrefer across a set of ten stimuli, we demon-strate that component elements of the re-ferring expression generation process ap-pear to generalise across participants to asignificant degree.
This leads us to pro-pose an alternative way of thinking of re-ferring expression generation, where eachattribute in a description is provided by aseparate heuristic.1 IntroductionThe last few years have witnessed a considerablemove towards empiricism in referring expressiongeneration; this is evidenced both by the growingbody of work that analyses and tries to replicatethe content of corpora of human-produced refer-ring expressions, and particularly by the signifi-cant participation in the TUNA and GREC chal-lenge tasks built around such activities (see, forexample, (Belz and Gatt, 2007; Belz et al, 2008;Gatt et al, 2008)).
One increasingly widespreadobservation?obvious in hindsight, but surpris-ingly absent from much earlier work on referringexpression generation?is that one person?s refer-ential behaviour differs from that of another: giventhe same referential task, different subjects willchoose different referring expressions to identifya target referent.
Faced with this apparent lack ofcross-speaker consistency in how to refer to enti-ties, we might question the validity of any exercisethat tries to develop an algorithm on the basis ofdata from multiple speakers.In this paper we revisit the corpus of datathat was introduced and discussed in (Viethenand Dale, 2008a; Viethen and Dale, 2008b) withthe objective of determining what referential be-haviour, if any, might be learned automaticallyfrom the data.
We find that, despite the apparentdiversity of the data when we consider the pro-duction of referring expressions across subjects,a closer examination reveals that individual at-tributes within referring expressions do appear tobe selected on the basis of contextual factors witha high degree of consistency.
This suggests that re-ferring behaviour might be best thought of as con-sisting of a combination of lower-level heuristics,with each individual?s overall referring behaviourbeing constructed from a potentially distinct com-bination of these common heuristics.In Section 2 we describe the corpus we use forthe experiments in this paper.
In Section 3 we ex-plore to what extent we can use this corpus to learnan algorithm for referring expression generation;in Section 4 we look more closely at the nature ofindividual variation within the corpus.
Section 5briefly discusses related work on the use of ma-chine learning in referring expression generation,and Section 6 draws some conclusions and pointsto future work.2 The Corpus2.1 General OverviewThe corpus we use was collected via a data gath-ering experiment described in (Viethen and Dale,2008a; Viethen and Dale, 2008b).
The purpose ofthe data gathering was to gain some insight intohow human subjects use relational referring ex-pressions, a relatively unexplored aspect of refer-ring expression generation.
Participants visited awebsite, where they first saw an introductory pagewith a set of simple instructions and a sample stim-ulus scene consisting of three objects.
Each par-ticipant was then assigned one of two trial sets often scenes each; the two trial sets are superficially58Figure 1: The stimulus scenes.
The letters indi-cate which schema from Figure 2 each column ofscenes is based on.different, but the elements of the sets are pairwiseidentical in terms of the factors explored in the re-search.
The complete set of 20 scenes is shown inFigure 1, where Trial Set 1 consists of Scenes 1through 10, and Trial Set 2 consists of Scenes 11through 20.1The scenes were presented successively in apreset order, which was the same for each partic-ipant.
Below each scene, the participant had tocomplete the sentence Please pick up the .
.
.
in atext box before clicking on a button to see the nextscene.
The task was to describe the target referentin the scene (marked by a grey arrow) in a way thatwould enable a friend looking at the same scene topick it out from the other objects.The experiment was completed by 74 partici-pants from a variety of different backgrounds andages; most were university-educated and in theirearly or mid twenties.
For reasons discussed in(Viethen and Dale, 2008b), the data of 11 partici-pants was discarded.
Of the remaining 63 partici-pants, 29 were female, while 34 were male.2.2 Stimulus DesignThe design of the stimuli used in the experiment isdescribed in detail in (Viethen and Dale, 2008a).1Scene 1 is paired with Scene 11, Scene 2 with Scene12, and so on; in each pair, the only differences are thecolour scheme used and the left?right orientation, with thesevariations being introduced to make the experiment lessmonotonous for subjects; (Viethen and Dale, 2008a) reportthat these characteristics of the scenes appear to have no sig-nificant effect on the forms of reference used.Figure 2: The schemata which form the basis forthe stimulus scenes.We provide a summary of the key points here.In order to explore even the most basic hypothe-ses with respect to the use of relational expres-sions, which was the aim of the original study,scenes containing at least three objects were re-quired.
One of these objects is the intended ref-erent, which is referred to here as the target.
Thesubject has to describe the target in such a way asto distinguish it from the other two objects in thescene.
Although the scenes presented to the sub-jects are such that spatial relations are never nec-essary to distinguish the target, they are set up sothat one of the two non-target objects was clearlycloser to the target.
This object is referred to as the(potential) landmark; and we call the third objectin the scene the distractor.To minimise the number of variables in the ex-periments, scenes are restricted to only two kindsof objects, cubes and balls.
The objects also varyin two dimensions: colour (either green, blue,yellow, or red); and size (either large or small).To further reduce the number of factors in thescene design, the landmark and distractor are al-ways placed clearly side by side, and the target islocated on top of or directly in front of the land-mark.Finally, to reduce the set of possible stimuli to amanageable number, five schemata (see Figure 2)were created as a basis for the final stimulus set.The design of these schemata was informed by anumber of research questions with regard to theuse of relations; see (Viethen and Dale, 2008b).
Aschema determines the type and size of each objectin the scenes that are based on it, and determineswhich objects share colour.
So, for example, inscenes based on Schema C, the target is a smallball; the landmark is a large cube with differentcolour from the target; and the distractor is a largeball sharing its colour with the target.59Label Pattern ExampleA ?tg col, tg type?
the blue cubeB ?tg col, tg type, rel, lm col, lm type?
the blue cube in front of the red ballC ?tg col, tg type, rel, lm size, lm col, lm type?
the blue cube in front of the large red ballD ?tg col, tg type, rel, lm size, lm type?
the blue cube in front of the large ballE ?tg col, tg type, rel, lm type?
the blue cube in front of the ballF ?tg size, tg col, tg type?
the large blue cubeG ?tg size, tg col, tg type, rel, lm col, lm type?
the large blue cube in front of the red ballH ?tg size, tg col, tg type, rel, lm size, lm col, lm type?
the large blue cube in front of the large red ballI ?tg size, tg col, tg type, rel, lm size, lm type?
the large blue cube in front of the large ballJ ?tg size, tg col, tg type, rel, lm type?
the large blue cube in front of the ballK ?tg size, tg type?
the large cubeL ?tg size, tg type, rel, lm size, lm type?
the large cube in front of the large ballM ?tg size, tg type, rel, lm type?
the large cube in front of the ballN ?tg type?
the cubeO ?tg type, rel, lm col, lm type?
the cube in front of the red ballP ?tg type, rel, lm size, lm col, lm type?
the cube in front of the large red ballQ ?tg type, rel, lm size, lm type?
the cube in front of the large ballR ?tg type, rel, lm type?
the cube in front of the ballTable 1: The 18 different patterns corresponding to the different forms of description that occur in theGRE3D3 corpus.From each schema, four distinct scenes weregenerated, resulting in the 20 stimulus scenesshown in Figure 1.
As noted above, there are reallyonly 10 distinct ?underlying?
scene types here, soin the remainder of this paper we will talk in termsof Scenes 1 through 10, where the data from thepairwise matched scenes are conflated.2.3 The GRE3D3 Corpus2Before conducting any quantitative data analysis,some syntactic and lexical normalisation was car-ried out on the data provided by the participants.In particular, spelling mistakes were corrected;normalised names were used for colour values andhead nouns (for example, box was replaced bycube); and complex syntactic structures such asrelative clauses were replaced with semanticallyequivalent simpler ones such as adjectives.
Thesenormalisation steps should be of no consequenceto the analysis presented here, since we are solelyinterested in exploring the semantic content of re-ferring expressions, not their lexical and syntacticsurface structure.For the purposes of the machine learning exper-iments described in this paper, we made a few fur-ther changes to the data set in order to keep thenumber of properties and their possible values low.We removed locative expressions that made refer-2The data set resulting from the experiment describedabove is known as the GRE3D3 Corpus; the name stands for?Generation of Referring Expressions in 3D scenes with 3Objects?.ence to a part of the scene (58 instances) and ref-erences to size as the same (4 instances); so, forexample, the blue cube on top of the green cubein the right and the blue cube on top of the greencube of the same size both became the blue cubeon top of the green cube.
We also removed themention of a third object from ten descriptions inorder to keep the number of possible objects perdescription to a maximum of two.
These changesresulted in seven descriptions no longer satisfyingthe criterion of being fully distinguishing, so weremoved these descriptions from the corpus.3 Learning Description PatternsThe resulting corpus consists of 623 descriptions.Every one of these is an instance of one of the 18patterns shown in Table 1; for ease of reference,we label these patterns A through R. Each patternindicates the sequence of attributes used in the de-scription, where each attribute is identified by theobject it describes (tg for target, lm for landmark)and the attribute used (col, size and type for colour,size and type respectively).Most work on referring expression generationattempts to determine what attributes should beused in a description by taking account of aspectsof the context of reference.
An obvious questionis then whether we can learn the description pat-terns in this data from the contexts in which theywere produced.
To explore this, we chose to cap-ture the relevant aspects of context by means ofthe notion of characteristics of scenes.
The char-60Label Attribute Valuestg type = lm type Target and Landmark share Type TRUE, FALSEtg type = dr type Target and Distractor share Type TRUE, FALSElm type = dr type Landmark and Distractor share Type TRUE, FALSEtg col = lm col Target and Landmark share Colour TRUE, FALSEtg col = dr col Target and Distractor share Colour TRUE, FALSElm col = dr col Landmark and Distractor share Colour TRUE, FALSEtg size = lm size Target and Landmark share Size TRUE, FALSEtg size = dr size Target and Distractor share Size TRUE, FALSElm size = dr size Landmark and Distractor share Size TRUE, FALSErel Relation between Target and Landmark on top of, in front ofTable 2: The 10 characteristics of scenesacteristics of scenes which we hypothesize mighthave an impact on the choice of referential formare those summarised in Table 2; these are pre-cisely the characteristics that were manipulated inthe design of the schemata in Figure 2.Of course, there is no one correct answer forhow to refer to the target in any given scene.Figure 3 shows the distribution of different pat-terns across the different scenes; so, for exam-ple, some scenes (Scenes 4, 5, 9 and 10) resultin only five semantically distinct referring expres-sion forms, whereas Scene 7 results in 12 distinctreferring expression forms.
All of these are distin-guishing descriptions, so all are acceptable formsof reference, although some contain more redun-dancy than others.
Most obvious from the chartis that, for many scenes, there is a predominantform of reference used; so, for example, pattern F(?tg size, tg col, tg type?)
accounts for 43 (68%)of the descriptions used in Scene 4, and patternA (?tg col, tg type?)
is very frequently used in anumber of scenes.3We used Weka (Witten and Eibe, 2005) with theJ48 decision tree classifer to see what correspon-dences might be learned between the character-isics of the scenes listed in Table 2 and the formsof referring expression used for the target refer-ents, as shown in Table 1.
The pruned decisiontree learned by this method predicted the actualform of reference used in only 48% of cases under10-fold cross-validation, but given that there aremany ?gold standard?
descriptions for each scene,3The chart as presented here is obviously too small to en-able detailed examination, and our use of colour coding willbe of no value in a monchrome rendering of the paper; how-ever, the overall shape of the data is sufficient to demonstratethe points we make here.this low score is hardly surprising; a mechanismwhich learns only one answer will inevitably be?wrong?
in many cases.
More revealing, however,is the rule learned from the data:if tg type = dr typethen use F (?tg size, tg col, tg type?
)else use A (?tg col, tg type?
)endifPatterns A and F are the two most prevalent pat-terns in the data, and indeed one or other appearsat least once in the human data for each scene;consequently, the learned rule is able to producea ?correct?
answer for every scene.44 Individual VariationOne of the most striking things about the data inthis corpus is the extent to which different subjectsappear to do different things when they constructreferring expressions, as demonstrated by the dis-tribution of patterns in Figure 3.
Another way oflooking at this variation is to characterise the be-haviour of each subject in terms of the sequence ofdescriptions they provide in response to the set of10 stimuli.Across the 63 subjects, there are 47 different se-quences; of these, only four occur more than once(in other words, 43 subjects did not produce thesame sequence of descriptions for the ten scenes asanyone else).
The recurrent sequences, i.e.
thoseused by at least two people, are shown in Table 3.Note that the most frequently recurring sequence,4The fact that the rule is conditioned on a property of thedistractor object may be an artefact of the stimulus set con-struction; this would require a more diverse set of scenes todetermine.61Figure 3: The profile of different description patterns (A through R) for each of the 10 scenes.
The lengthof the bar indicates how often each of the 18 patterns is used.which matches the behaviour of nine separate sub-jects, consists only of uses of patterns A and F.It remains to be seen to what extent a larger dataset would demonstrate more convergence; how-ever, the point to be made at present is that anyattempt to predict the behaviour of a given speakerby means of a model of referring behaviour is go-ing to have to take account of a great deal of indi-viual variation.Nonetheless, we re-ran the J48 classifier de-scribed in the previous section, this time usingthe participant ID as well as the scene character-istics in Table 2 as features.
This improved patternprediction to 57.62%.
This suggests that individ-ual differences may indeed be capturable from thedata, although we would need more data than themere 10 examples we have from each subject tolearn a good predictive model.In the face of this lack of data, another approachis to look for commonalities in the data in termsof the constituent elements of the different ref-erence patterns used for each scene.
This wayof thinking about the data was foreshadowed by(Viethen and Dale, 2008b), who observed that thesubjects could be separated into those who alwaysused relations, those who never used relations, andthose who sometimes used relations.
This leadsus to consider whether there are characteristics ofscenes or speakers which are highly likely to resultin specific attributes being used in descriptions.
Ifthis is the case, a decision tree learner should beable to learn for each individual attribute whetherit should be included in a given situation.An appropriate baseline for any experimentshere is the success rate of simply including or notincluding each attribute (basically a 0-R majorityclass classifier), irrespective of the characteristicsof the scene.
Table 4 compares the results forthis ?context-free?
approach with one model thatis trained on the characteristics of scenes, and an-other that takes both the characteristics of scenesand the participant ID into account.5Interestingly, the ?context-free?
strategies worksuprisingly well for predicting the inclusion ofsome attributes in the human data.
As has beennoted in other work (see for example (Viethen etal., 2008)), colour is often included in referring ex-pressions irrespective of its discriminatory power,and this is borne out by the data here.
Perhapsmore suprising is the large degree to which the in-clusion of landmark size is captured by a context-free strategy.5As before, the results reported are for the accuracy of apruned J48 decision tree, under 10-fold cross-validation.62Improvement on all attribues other than tar-get colour improves when we take into accountthe characteristics of the scenes, consistent withour assumptions that context does matter.
Whenwe add participant ID to the features used in thelearner, performance improves further still, indi-cating that there are speaker-specific consistenciesin the data.It is instructive to look at the rules learned onthe basis of the scene characteristics.
Not surpris-ingly, the rule derived for target colour inclusion issimply to always include the colour (i.e., the samecontext-free colour inclusion rule that proves mosteffective in modelling the data without referenceto scene characteristics).
The rules for includingthe other attributes on the basis of scene charac-teristics (but not participant ID) are shown in Fig-ure 4.The rules learned when we include participantID are more conplex, but can be summarised in away that demonstrates how this approach can re-veal something about the variety of ways in whichspeakers appear to approach the task of referringexpression generation.
Focussing, as an example,just on the question of whether or not to use thetarget object?s colour in a referring expression, wefind the following:?
48 participants always used colour, irrespec-tive of the context (this corresponds to thebaseline rule learned above).?
The other participants always use colour ifthe target and the landmark are of the sametype (which again is intuitively quite appro-priate).?
When the landmark and the target are notof the same type, we see more variation inbehaviour; 19 participants simply don?t usecolour, and the behaviour of seven can becaptured via a more complex analysis: fouruse colour if the target and the distractor arethe same size, two use colour if the target anddistractor are of the same size and the targetis on top of the landmark, and one uses colourif the target and distractor share colour.Again, the specific details of the rules learned hereare probably not particularly significant, based asthey are on a limited data set and a set of stimulithat may give elevated status to incidental proper-ties.
However, the general point remains that weTarget Size:if tg type = dr type then include tg sizeRelation:if rel = on top of and lm size = dr sizethen include relLandmark Colour:if we have used a relation then include lm colLandmark Size:if we have used a relation and tg col = lm colthen include lm sizeFigure 4: Rules learned on the basis of scene char-acteristicscan use this kind of analysis to identify possiblerules for the inclusion of individual attributes inreferring expressions.What this suggests is that we might be able tocapture the behaviour of individual speakers notin terms of an overall strategy, but as a compos-ite of heuristics, where each heuristic accounts forthe inclusion of a specific attribute.
The rules, orheuristics, shown in Figure 4 are just those whichare most successful in predicting the data; butthere can be many other rules that might be usedfor the inclusion of particular attributes.
So, forexample, I might be the kind of speaker who justautomatically includes the colour of an intendedreferent without any analysis of the scene; and Imight be the kind of speaker who always uses arelation to a nearby landmark in describing the in-tended referent.
Or I might be the kind of speakerwho surveys the scene and takes note of whetherthe landmark?s colour is distinctive; and so on.Thought of in this way, each speaker?s approachto reference is like a set of ?parallel gestalts?
thatcontribute information to the description beingconstructed.
The particular rules for inclusion thatany speaker uses might vary depending on theirpersonal past history, and perhaps even on the ba-sis of situation-specific factors that on a given oc-casion might lean the speaker towards either being?risky?
or ?cautious?
(Carletta, 1992).As alluded to earlier, the specific content of therules shown in Figure 4 may appear idiosyncratic;they are just what the limited data in the corpus63Pattern Sequence (?Scene#,DescriptionPattern?)
Number of subjects?1,A?, ?2,A?, ?3,G?, ?4,F?, ?5,A?, ?6,A?, ?7,A?, ?8,G?, ?9,F?, ?10,A?
2?1,B?, ?2,B?, ?3,G?, ?4,H?, ?5,B?, ?6,B?, ?7,B?, ?8,G?, ?9,H?, ?10,B?
2?1,N?, ?2,N?, ?3,K?, ?4,F?, ?5,A?, ?6,N?, ?7,N?, ?8,K?, ?9,F?, ?10,A?
6?1,A?, ?2,A?, ?3,F?, ?4,F?, ?5,A?, ?6,A?, ?7,A?, ?8,F?, ?9,F?, ?10,A?
9Table 3: Sequences of description patterns found more than onceAttribute to Include Baseline (0-R) Using Scene Using SceneCharacteristics Characteristicsand ParticipantTarget Colour 78.33% 78.33% 89.57%Target Size 57.46% 90.85% 90.85%Relation 64.04% 65.00% 81.22%Landmark Colour 74.80% 87.31% 93.74%Landmark Size 88.92% 95.02% 95.02%Table 4: Accuracy of Learning Attribute Inclusion; statistically significant increases (p<.01) in bold.supports, and some elements of the rules may bedue to artefacts of the specific stimuli used in thedata gathering.
We would require a more diverseset of stimuli to determine whether this is the case,but the basic point stands: we can find correlationsbetween characteristics of the scenes and the pres-ence or absence of particular attributes in referringexpressions, even if we cannot predict so well theparticular combinations of these correlations thata given speaker will use in a given situation.5 Related WorkThere is a significant body of work on the useof machine learning in referring expression gen-eration, although typically focussed on aspects ofthe problem that are distinct from those consideredhere.In the context of museum item descriptions,Poesio et al (1999) explore the decision of whattype of referring expression NP to use to refer toa given discourse entity, using a statistical modelto choose between using a proper name, a definitedescription, or a pronoun.
More recently, Stoia etal.
(2006) attempt a similar task, but this time inan interactive navigational domain; as well as de-termining what type of referring expression to use,they also try to learn whether a modifier should beincluded.
Cheng et al (2001) try to learn rules forthe incorporation of non-referring modifiers intonoun phrases.A number of the contributions to the 2008 GRECand TUNA evaluation tasks (Gatt et al, 2008) havemade use of machine learning techniques.
TheGREC task is primarily concerned with the choiceof form of reference (i.e.
whether a proper name, adescriptive NP or a pronoun should be used), andso is less relevant to the focus of the present pa-per.
Much of the work on the TUNA Task is rel-evant, however, since this also is concerned withdetermining the content of referring expressionsin terms of the attributes used to build a distin-guishing description.
In particular, Fabbrizio et al(2008) explore the impact of individual style andpriming on attribute selection for referring expres-sion generation, and Bohnet (2008) uses a nearest-neighbour learning technique to acquire an indi-vidual referring expression generation model foreach person.Other related approaches to attribute selectionin the context of the TUNA task are explored in(Gerva?s et al, 2008; de Lucena and Paraboni,2008; Kelleher and Mac Namee, 2008; King,2008).6 ConclusionsWe know that people?s referential behaviour variessignificantly.
Despite this apparent variation, wehave demonstrated above that there does appear tobe a reasonable correlation between characteristicsof the scene and the incorporation of particular at-tributes in a referring expression.
One way to con-ceptualise this is that the decision as to whether or64not to incorporate a given feature such as colouror size may vary from speaker to speaker; this isevidenced by the data.
We might think of these asindividual reference strategies; a good example ofsuch a strategy, widely attested across many exper-iments, is the decision to include colour in a refer-ring expression independent of its discriminatorypower, perhaps because it is an easily perceivableand often-useful attribute.
The overall approach toreference that is demonstrated by a given speakerthen consists of the gathering together of a numberof strategies; the particular combinations may varyfrom speaker to speaker, but as is demonstrated bythe analysis in this paper, some of the strategiesare widely used.In current work, we are gathering a much largerdata set using more complex stimuli.
This will al-low the further development and testing of the ba-sic ideas proposed in this paper as well as theirintegration into a full referring expression genera-tion algorithm.ReferencesAnja Belz and Albert Gatt.
2007.
The attribute selec-tion for GRE challenge: Overview and evaluationresults.
In Proceedings of UCNLG+MT: LanguageGeneration and Machine Translation, pages 75?83,Copenhagen, Denmark.Anja Belz, Eric Kow, Jette Viethen, and Albert Gatt.2008.
The GREC challenge 2008: Overview andevaluation results.
In Proceedings of the Fifth Inter-national Natural Language Generation Conference,pages 183?191, Salt Fork OH, USA.Bernd Bohnet.
2008.
The fingerprint of human refer-ring expressions and their surface realization withgraph transducers.
In Proceedings of the 5th Inter-national Conference on Natural Language Genera-tion, pages 207?210, Salt Fork OH, USA.Jean C. Carletta.
1992.
Risk-taking and Recovery inTask-Oriented Dialogue.
Ph.D. thesis, University ofEdinburgh.Hua Cheng, Massimo Poesio, Renate Henschel, andChris Mellish.
2001.
Corpus-based NP modifiergeneration.
In Proceedings of the Second Meetingof the North American Chapter of the Associationfor Computational Linguistics, Pittsburgh PA, USA.Diego Jesus de Lucena and Ivandre?
Paraboni.
2008.USP-EACH: Frequency-based greedy attribute se-lection for referring expressions generation.
In Pro-ceedings of the Fifth International Natural Lan-guage Generation Conference, pages 219?220, SaltFork OH, USA.Giuseppe Di Fabbrizio, Amanda J. Stent, and SrinivasBangalore.
2008.
Referring expression generationusing speaker-based attribute selection and trainablerealization (ATTR).
In Proceedings of the Fifth In-ternational Natural Language Generation Confer-ence, Salt Fork OH, USA.Albert Gatt, Anja Belz, and Eric Kow.
2008.
TheTUNA challenge 2008: Overview and evaluation re-sults.
In Proceedings of the Fifth International Nat-ural Language Generation Conference, pages 198?206, Salt Fork OH, USA.Pablo Gerva?s, Raquel Herva?s, and Carlos Leo?n.
2008.NIL-UCM: Most-frequent-value-first attribute se-lection and best-scoring-choice realization.
In Pro-ceedings of the Fifth International Natural Lan-guage Generation Conference, pages 215?218, SaltFork OH, USA.John D. Kelleher and Brian Mac Namee.
2008.
Refer-ring expression generation challenge 2008: DIT sys-tem descriptions.
In Proceedings of the Fifth Inter-national Natural Language Generation Conference,pages 221?223, Salt Fork OH, USA.Josh King.
2008.
OSU-GP: Attribute selection usinggenetic programming.
In Proceedings of the FifthInternational Natural Language Generation Confer-ence, pages 225?226, Salt Fork OH, USA.Massimo Poesio, Renate Henschel, Janet Hitzeman,and Rodger Kibble.
1999.
Statistical NP genera-tion: A first report.
In Proceedings of the ESSLLIWorkshop on NP Generation, Utrecht, The Nether-lands.Laura Stoia, Darla Magdalene Shockley, Donna K. By-ron, and Eric Fosler-Lussier.
2006.
Noun phrasegeneration for situated dialogs.
In Proceedings ofthe 4th International Conference on Natural Lan-guage Generation, pages 81?88, Sydney, Australia.Jette Viethen and Robert Dale.
2008a.
Generatingreferring expressions: What makes a difference?In Australasian Language Technology AssociationWorkshop 2008, pages 160?168, Hobart, Australia.Jette Viethen and Robert Dale.
2008b.
The use ofspatial relations in referring expression generation.In Proceedings of the 5th International Conferenceon Natural Language Generation, pages 59?67, SaltFork OH, USA.Jette Viethen, Robert Dale, Emiel Krahmer, Marie?tTheune, and Pascal Touset.
2008.
Controlling re-dundancy in referring expressions.
In Proceedingsof the 6th Language Resources and Evaluation Con-ference, Marrakech, Morocco.Ian H. Witten and Frank Eibe.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann, San Francisco, 2nd edition.65
