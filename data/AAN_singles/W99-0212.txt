Using Coreference for Question AnsweringThomas  S. Mor tonDepar tment  of Computer  and Informat ion ScienceUniversity of Pennslyvaniat sm6rton@cis, upenn, eduAbst ractWe present a system which retrieves answers toqueries based on coreference relationships be-tween entities and events in the query and doc-uments.
An evaluation of this system is givenwhich demonstrates that the the amount of in-formation that the user must process on aver-age, tQ find an answer to their query, is reducedby an order of magnitude.1 In t roduct ionSearch engines have become ubiquitous as ameans for accessing information.
When a rank-ing of documents is returned by a search en-gine the information retrieval task is usually notcomplete.
The document, as a unit of informa-tion, is often too large for many users informa-tion needs and finding information within theset of returned documents poses a burden of itsown.
Here we examine a technique for extract-ing sentences from documents which attemptsto satisfy the users information eeds by provid-ing an answer to the query presented.
The sys-tem does this by modeling coreference relation-ships between entities and events in the queryand documents.
An evaluation of this system isgiven which demonstrates that it performs bet-ter than using a standard tf .
idf weighting andthat the amount of information that the usermust process on average, to find an answer totheir query, is reduced by an order of magnitudeover document ranking alone.2 P rob lem StatementA query indicates an informational need by theuser to the search engine.
The information re-quired may take the form of a sentence or evena noun phrase.
Here the task is to retrieve thepassage of text which contains the answer tothe query from a small collection of documents.Sentences are then raaked and presented to theuser.
We only examine queries to which an-swers are likely to be stated in a sentence ornoun phrase since answers which are typicallylonger are can be difficult to annotate reliably.This technology differs from the standard ocu-ment ranking task in that, if successful the userwill likely not need to examine any of the re-trieved documents in their entirety.
This alsodiffers from the document summarization, pro-vided by many Search engines today, in that thesentences elected are influenced by the queryand are selected across multiple documents.We view a system such as ours as providinga secondary level of processing after a small setof documents, which the user believes containthe information desired, have been found.
Thisfirst step would likely be provided by a tradi-tional search engine, thus this technology servesas an enhancement to an existing document re-trieval systems rather than a replacement.
Ad-vancements in document retrieval would onlyhelp the performance of a system such as oursas these improvements would increase the like-lihood that the answer to the user's query is inone of the top ranked documents returned.3 ApproachA query is viewed as identifying a relation towhich a user desires a solution.
This relationwill most likely involve events and entities, andan answer to this relation will involve the sameevents and entities.
Our approach attempts tofind coreference relationships between the enti-ties and events evoked by the query and thoseevoked in the document.
Based on these rela-tionships, sentences are ranked, and the highestranked sentences are displayed to the user.The coreference relationships that are mod-eled by this system include identity, part-whole,85and synonymy relations.
Consider the followingquery and answer pairs.Query: What did Mark McGwire sayabout child abuse?Sentence: "What kills me is that youknow there are kids over there whoare being abused or neglected, youjust don't know which ones" McGwiresays.In the above query answer pair the system at-tempts to capture the identity relationship be-tween Mark McGwire and McGwire by deter-mining that the term McGwire in this sentenceis coreferent with a mention of Mark McGwireearlier in the document..
This allows the sys-tem to rank this sentence quivalently to a sen-tence mentioning the full name.
The systemalso treats the term child abuse as a nominaliza-tion which allows it to speculate that the termabused in the sentence is a related event.
Finallythe verb neglect occurs frequently within doc-uments which contain the verb abuse, which isnominalized in the query, so this term is treatedas a related event.
The system does not cur-rently have a mechanism which tries to capturethe relationship between kids and children.Query: Why did the U.S. bomb Su-dan?Sentence: Last month, the UnitedStates launched a cruise missile at-tack against the Shifa Pharmaceuti-cal Industries plant in Khartoum, al-leging that U.S. intelligence agencieshave turned up evidence - includingsoil samples - showing that the plantwas producing chemicals which couldbe used to make VX, a deadly nervegas.In this example one of the entity-based relation-ships of interest is the identity relationship be-tween U.S. and United States.
Also of interest isthe part-whole relationship between Sudan andKhartoum, it's capital.
Finally the bomb eventis related to the launch/attack event.
The sys-tem does not currently have a mechanism whichtries to capture the relationship between Whyand alleging or evidence.4 Implementat ionThe relationships above are captured by a num-ber of different echniques which can be placedin essentially two categories.
The first groupfinds identity relationships between different in-vocations of the same entity in a document.
Thesecond identifies more loosely defined relation-ships such as part-whole and synonymy.
Each ofthe relationships identified is given a weight andbased on the weights and relationships them-selves sentences are ranked and presented to theuser.4.1 Identity RelationshipsIdentity relationships are first determined be-tween the string instantiations of entities in sin-gle documents.
This is done so that the dis-course context in which these strings appearcan be taken into account.
The motivation forthis comes in part from example texts where thesame last name will be used to refer to differ-ent individuals in the same family.
This is of-ten unambiguous because full names are used inprevious sentences, however this requires somemodeling of which entities are most salient inthe discourse.
These relations are determinedusing techniques described in (Baldwin et al,1998).Another source of identity relationshipsis morphological and word order variations.Within noun phrases in the query the sys-tem constructs other possible word combina-tions which contain the head word of the nounphrase.
For example a noun phrase such as "thephotographed little trouper" would be extendedto include "the photographed trouper", "the lit-tle tropper", and "the trouper" as well as vari-ations excluding the determiner.
Each of thevariations is given a weight based on the ratio ofthe score that the new shorter term would havereceived if it had appeared in the query and theactual noun phrase that occured.
The morpho-logical roots of single word variations are alsoadded to the list a possible terms which referto the entity or event with no additional deduc-tion in weighting.
Finally query entities whichare found in an acronym database are added tothe list of corefering terms as well with a weightof 1.864.2 Par t -Who le  and  SynonymyRelationshipsThe system captures part-wt~ole and synonymyrelationships by examining co-occurrence statis-tics between certain classes of words.
Specif-ically co-occurrence statistics are gathered onverbs and nominalization which co-occur muchmore often then one would expect based onchance alone.
This is also done for propernouns.
For each verbal pair or proper noun pairthe mutual information between the two is com-puted as follows:I(wl, w2) " " p(Wl' w2) = ,ogtf l)p -C2 ))where Wl and w2 are words and an event is de-fined as a word occuring in a document.
Allwords w2 for which I(wl, w2) exceeds a thresh-old where Wl is a query term are added to thelist of terms with which the query term can bereferred to .
This relationship is given with aweight of I(wl, w2)/N where N is a normaliza-tion constant.
The counts for the mutual infor-mation statistics were gathered from a corpus ofover 62,000 Wall Street Journal articles whichhave been automatically tagged and parsed.4.3 Sentence  Rank ingBefore sentence ranking begins each entity orevent in the query is assigned a weight.
Thisweight is the sum of inverse document frequencymeasure of the entity or events term based onits occurrence in the Wall Street Journal corpusdescribed in the previous ection.
This measureis computed as:idf (wl ) --lOg(df~wl))where N is the total number of documents in thecorpus and dr(w1) is the number of documentswhich contain word Wl.
Once weighted, the sys-tem compares the entities and events evoked bythe query with the entities and events evoked bythe document.
The comparison is done via sim-ple string matching against all the terms withwhich the system has determined an entity orevent can be referred to.
Since these term ex-pansions are weighted the score for for a partic-ular term w2 and a query term Wl is:S(Wl, w2) = idf(wl) x weightwl (W2)where weightwl is the weight assigned duringone of the previous term expansion phases andidf is defined above.
The weightwl function isdefined to be 0 for any term w2 for which noexpansion took place.
The score for the a par-ticular entity or event in the document with re-spect to an entity or event in the query is themaximum value of S(Wl,W2) over all values ofWl and w2 for that entity or event.
A particularsentence's score is computed as the sum of thescores of the set of entities and events it evokes.For the purpose of evaluation a baseline sys-tem was also constructed.
This system fol-lowed a more standard information retrieval ap-proach to text ranking described in (Salton,1989).
Each token in the the query is assignedan idf score also based on the same corpus ofWall Street Journal articles as used with theother system.
Query expansion simply con-sisted of stemming the tokens using a version Ofthe Porter stemmer and sentences were scoredas a sum of all matching terms, giving the fa-miliar t f .
idf measure.5 Eva luat ionFor the evaluation of the system ten querieswere selected from a collection of actual queriespresented to an online search engine.
Querieswere selected based on their expressing the usersinformation need clearly, their being likely an-swered in a single sentence, and non-dubious in-tent.
The queries used in this evaluation are asfollows:?
Why has the dollar weakened against theyen??
What was the first manned Apollo missionto circle the moon??
What virus was spread in the U.S. in 1968??
Where were the 1968 Summer Olympicsheld??
Who wrote "The Once and Future King"??
What did Mark McGwire say about childabuse??
What are the symptoms of Chronic FatigueSyndrome??
What kind of tanks does Israel have??
What is the life span of a white tailed deer?87?
Who was the first president of Turkey?The information requested by the query wasthen searched for from a data source which wasconsidered likely to contain the answer.
Sourcesfor these experiments include Britannica On-line, CNN, and the Web at large.
Once apromising set of documents were retrieved, thetop ten were annotated for instances of the an-swer to the query.
The system was then asked toprocess the ten documents and present a rankedlisting of sentences.System performance is presented below as thetop ranked sentence which contained an answerto the question.
A question mark is used toindicate that an answer did not appear in thetop ten ranked sentences.Query910First answer's rankFull System Baseline2 42 38 62 47 81 34 ?
?1 11 16 D iscuss ionSentence xtraction and ranking while similarin its information retrieval goals with documentranking appears have very different properties.While a document can often stand alone in itsinterpretation the interpretation f a sentence isvery dependent on the context in which it ap-pears.
The modeling of the discourse gives theentity based system an advantage over a tokenbased models in situations where referring ex-pressions which provide little information out-side of their discourse context can be related tothe query.
The most extreme xample case ofthis being the use of pronouns.The query expansion techniques presentedhere are simplistic compared to many used infor information retrieval however they are try-ing to capture different phenomenon.
Here thegoal is to capture different lexicalizations of thesame entities and events.
Since short news ar-ticles are likely to focus on a small number ofentities and perhaps a single event or a group ofrelated events it is hoped that the co-occurrencestatistics gathered will reveal good candidatesfor alternate ways in which the query entitiesand events can be lexicalized.This work employs many of the techniquesused by (Baldwin and Morton, 1998) for per-forming query based summarization.
Here how-ever the retrieved information attempts to meetthe users information eeds rather then help-ing the user determine whether the entire doc-ument being summarized possibly meets thatneed.
This system also differs in that it canpresent he user with information from multi-ple documents.
While query sensitive multi-document systems exist (Mani and Bloedorn,1998), evaluating such systems for the purposeof comparison is difficult.Our evaluation shows that the system per-forms better than the baseline although thebaseline performs urprisingly well.
We believethat this is, in part, due to the lack of anynotion of recall in the evaluation.
While allqueries were answered by multiple sentences,for some queries such as 4,5 and 10 it is notclear what benefit the retrieval of additionalsentences would have.
The baseline benefitedfrom the fact that at least one of the answerstypically contained most of the query terms.Classifying queries as single answer or multi-ple answer, and evaluating them separately mayprovide a sharper distinction in performance.Comparing the users task with and with-out the system reveals a stark contrast in theamount of information eeded to be processed.On average the system required 290 bytes oftext to display the answer to the query to theuser.
In contrast, had the user reviewed thedocuments in the order presented by the searchengine, the answer on average, would appearafter more than 3000 bytes of text had beendisplayed.7 Future  WorkAs a preliminary investigation into this taskmany areas of future work were discovered.7.1 Term Mode l ingThe treatment of entities and events needs tobe extended to model the nouns which indicateevents more robustly and to exclude relational88verbs from consideration as events.
A proba-bilistic model of pronouns where referents aretreated as the basis for term expansion shouldalso be considered.
Another area which requiresattention is wh-words.
Even a simple modelwould likely reduce the space of entities con-sidered relevant in a sentence.7.2 ToolsIn order to be more effective the models used forbasic linguistic annotation, specifically the partof speech tagger, would need trained on a widerclass of questions than is available in the PennTreebank.
The incorporation of a Name EntityRecognizer would provide additional categorieson which co-occurrence statistics could be basedand would likely prove helpful in the modelingof wh-words.7.3 User  In teract ionFinally since many of the system's componentsare derived from unsupervised corpus analysis,the system's language models could be updatedas the user searches.
This may better charac-terize the distribution of words in the areas theuser is interested which could improve perfor-mance for that user.8 Conc lus ionWe have presented a system which ranks sen-tences such that the answer to a users querywill be presented on average in under 300 bytes.This system does this by finding entities andevents hared by the query and the documentsand by modeling coreference r lationships be-tween them.
While this is a preliminary inves-tigation and many areas of interest have yet tobe explored, the reduction in the amount of textthe user must process, to obtain the answersthey want, is already dramatic.ReferencesBreck Baldwin and Thomas Morton.
1998.
Dy-namic coreference-based summarization.
InProceedings of the Third Conference on Em-pirical Methods in Natural Language Process-ing, Granada, Spain, June.B.
Baldwin, T. Morton, Amit Bagga,J.
Baldridge, R. Chandraseker, A. Dim-itriadis, K. Snyder, and M. Wolska.
1998.Description of the UPENN CAMP systemas used for coreference.
In Proceedings of theSeventh Message Understanding Conference(MUC-7), Baltimore, Maryland.Inderjeet Mani and Eric Bloedorn.
1998.
Ma-chine learning of generic and user-focusedsummarization.
In Proceeding of the FifteenthNational Conference on Artificial intelligence(AAAI-98).Gerald Salton.
1989.
Automatic text process-ing: the transformation, analysis, and re-trieval of information by computer.
Addison-Wesley Publishing Company, Inc.89
