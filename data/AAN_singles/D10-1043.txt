Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 440?450,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsNon-isomorphic Forest Pair TranslationHui Zhang1, 2, 3   Min Zhang1   Haizhou Li1   Eng Siong Chng21Institute for Infocomm Research2Nanyang Technological University3USC Information Science Institutehuizhang.fuan@gmail.com   {mzhang, hli}@i2r.a-star.edu.sg   aseschng@ntu.edu.sgAbstractThis paper studies two issues, non-isomorphicstructure translation and target syntactic structureusage, for statistical machine translation in thecontext of forest-based tree to tree sequence trans-lation.
For the first issue, we propose a novelnon-isomorphic translation framework to capturemore non-isomorphic structure mappings than tra-ditional tree-based and tree-sequence-based trans-lation methods.
For the second issue, we propose aparallel space searching method to generate hypo-thesis using tree-to-string model and evaluate itssyntactic goodness using tree-to-tree/tree sequencemodel.
This not only reduces the search complexityby merging spurious-ambiguity translation pathsand solves the data sparseness issue in training, butalso serves as a syntax-based target language mod-el for better grammatical generation.
Experimentresults on the benchmark data show our proposedtwo solutions are very effective, achieving signifi-cant performance improvement over baselineswhen applying to different translation models.1 IntroductionRecently syntax-based methods have achieved verypromising results and attracted increasing interests instatistical machine translation (SMT) research com-munity due to their ability to provide informativecontext structure information and convenience incarrying out word transformation and sub-span reor-dering.
Fundamentally, syntax-based SMT viewstranslation as a structural transformation process.Generally speaking, from modeling viewpoint, asyntax-based model tries to convert the source struc-tures into target structures iteratively and recursivelywhile from decoding viewpoint a syntax-based sys-tem segments an input tree/forest into manysub-fragments, translates each of them separately,combines the translated sub-fragments and then findsout the best combinations.
Therefore, from bilingualviewpoint, we face two fundamental problems: themapping between bilingual structures and the way ofcarrying out the target structures combination.For the first issue, a number of models have beenproposed to model the structure mapping betweentree and string (Galley et al, 2004; Liu et al, 2006;Yamada and Knight, 2001; DeNeefe and Knight,2009) and between tree and tree (Eisner, 2003;Zhang et al, 2007 & 2008; Liu et al, 2009).
How-ever, one of the major challenges is that all the cur-rent models only allow one-to-one mapping from onesource frontier non-terminal node (Galley et al, 2004)to one target frontier non-terminal node in a bilingualtranslation rule.
Therefore, all those translation equi-valents with one-to-many frontier non-terminal nodemapping cannot be covered by the currentstate-of-the-art models.
This may largely compro-mise the modeling ability of translation rules.For the second problem, currently, the combina-tion is driven by only the source side (bothtree-to-string model and tree-to-tree model onlycheck the source span compatibility when combiningdifferent target structures in decoding) or only the440target side (string to tree model).
There is no wellstudy in considering both the source side informationand the compatibility between different target syn-tactic structures during combination.
In addition, it iswell known that the traditional tree-to-tree modelssuffer heavily from the data sparseness issue intraining and the spurious-ambiguity translation pathissue (the same translation with different syntacticstructures) in decoding.In addition, because of the performance limitationof automatic syntactic parser, researchers proposeusing packed forest (Tomita, 1987; Klein and Man-ning, 2001; Huang, 2008)1 instead of 1-best parsetree to carry out training (Mi and Huang, 2008) anddecoding (Mi et al, 2008) in order to reduce the sideeffect caused by parsing errors of the one-best tree.However, when we apply the tree-to-tree model tothe bilingual forest structures, both training and de-coding become very complicated.In this paper, to address the first issue, we proposea framework to model the non-isomorphic translationprocess from source tree fragment to target tree se-quence, allowing any one source frontiernon-terminal node to be translated into any numberof target frontier non-terminal nodes.
For the secondissue, we propose a technology to model the combi-nation task by considering both sides?
syntacticstructure information.
We evaluate and integrate thetwo technologies into forest-based tree to tree se-quence translation.
Experimental results on theNIST-2003 and NIST-2005 Chinese-English transla-tion tasks show that our methods significantly out-perform the forest-based tree to string and previoustree to tree models as well as the phrase-based model.The remaining of the paper is organized as fol-lowing.
Section 2 reviews the related work.
In sec-tion 3 and section 4, we discuss the proposed for-est-based rule extraction (non-isomorphic mapping)and decoding algorithms (target syntax informationusage).
Finally we report the experimental results insection 5 and conclude the paper in section 6.2 Related WorkMuch effort has been done in the syntax-based trans-lation modeling.
Yamada and Knight (2001) propose1 A packed forest is a compact representation of a set of treeswith sharing substructures; formally, it is defined as a triple atriple?
?, ?, ?
?, where ?
is non-terminal node set, ?
is hy-per-edge set and ?
is leaf node set (i.e.
all sentence words).Every node in ?
covers a consecutive sequence of leaf, everyhyper-edge in ?
connect the father node to its children nodes asin a tree.
Figure 8 is a packed forest contains two trees.a string to tree model.
Galley et al (2004) proposethe GHKM scheme to model the string-to-tree map-ping.
Liu et al (2006) propose a tree-to-string trans-lation model.
Liu et al (2007) propose the tree se-quence to string model to capture rules covered bycontinuous sequence of trees.
Shieber (2007), De-Neefe and Knight (2009) and Carreras and Collins(2009) propose synchronous tree adjoin grammar tocapture more tree-string mapping beyond the GHKMscheme.
Zhang et al (2009a) propose the concept ofvirtual node to reform a tree sequence as a tree, anddesign efficient algorithms for tree sequence modelin forest context.
All these works only consider eitherthe source side or the target side syntax information.To capture both side syntax contexts, Eisner (2003)studies the bilingual dependency tree-to-tree map-ping in conceptual level.
Zhang et al (2008) proposetree sequence-based tree-to-tree modeling.
Liu et al(2009) propose efficient algorithms for tree-to-treemodel in the forest-based training and decodingscheme.
One common limitation of the above worksis they only allow the one-to-one mapping betweeneach non-terminal frontier node, and thus they sufferfrom the issue of rule coverage.
On the other hand,due to the data sparseness issue and model coverageissue, previous tree-to-tree (Zhang et al, 2008; Liu etal., 2009) decoder has to rely solely on the span in-formation or source side information to combine thetarget syntactic structures, without checking thecompatibility of the merging nodes, in order not tofail many translation paths.
Thus, this solution failsto effectively utilize the target structure information.To address this issue, tree sequence (Liu et al,2007; Zhang et al, 2008) and virtual node (Zhang etal., 2009a) are two concepts with promising resultsreported.
In this paper, with the help of these twoconcepts, we propose a novel framework to solve theone-to-many non-isomorphic mapping issue.
In addi-tion, our proposed solution of using target syntaxinformation enables our forest-based tree-to-tree se-quence translation decoding algorithm to not onlycapture bilingual forest information but also havealmost the same complexity as forest-basedtree-to-string translation.
This reduces the time/spacecomplexity exponentially.3 Tree to Tree Sequence RulesThe motivation of introducing tree to tree sequencerules is to add target syntax information totree-to-string rules.
Following, we first briefly reviewthe definition of tree-to-string rules, and then de-scribe the tree-to-tree sequence rules.4413.1 Tree to String RulesVPADVPADVPVV??
(try hard to)??
(study)try to studyhardFig.
1.
A word-aligned sentence pair with source treeFig.
2 Examples of tree to string rulesFig.
2 illustrates the examples of tree to string rulesextracted from Fig.
1.
The tree-to-string rule is verysimple.
Its source side is a sub-tree of source parsetree and its target side is a string with only one varia-ble/non-terminal X.
The source side and the targetside is translation of each other with the constraint ofword alignments.
Please note that there is no anytarget syntactic or linguistic information used in thetree-to-string model.3.2 Tree to Tree Sequence RulesIt is more challenging when extracting rules withtarget tree structure as constraint.
Fig.
3 extends Fig.1 with target tree structure.
The problem is that, giv-en a source tree node, we are able to find its targetstring translation, but these target string may notform a linguistic sub-tree.
For example, in Fig.
3, thesource tree node ?ADVP?
in solid eclipse is trans-lated to ?try hard to?
in the target sentence, but thereis no corresponding sub-tree covering and only cov-ering it in the target side.Given the example rules in Fig.
2, what are theircorresponding rules with target syntax information?The answer is that the previous tree or tree se-quence-based models fail to model the Rule 1 andRule 2 at Fig.
2, since at frontier node level they onlyallow one-to-one node mapping but the solution isone-to-many non-terminal frontier node mapping.The concept of ?virtual node?
(Zhang et al 2009a) isa solution to this issue.
To facilitate discussion, wefirst introduce three concepts.Fig.
3.
A word-aligned bi-parsed treeFig.
4.
A restructured tree with a virtual span root?
Def.
1.
The ?node sequence?
is a sequence ofnodes (either leaf or internal nodes) covering aconsecutive span.
For example, in Fig 3, ?VBPRB TO?
and ?VBP ADVP TO?
are two ?nodesequence?
covering the same span ?try hard to?.442?
Def.
2.
The ?root node sequence?
of a span issuch a node sequence that any node in this se-quence could not be a child of a node in othernode sequence of the span.
Intuitively, the ?rootnode sequence?
of a span is the node sequencewith the highest topology level.
For example,?VBP ADVP TO?
is the ?root node sequence?of the span of ?try hard to?.
It is easy to provethat given any span, there exist one and only one?root node sequence?.?
Def.
3.
The ?span root?
of a span is such a nodethat if the ?root node sequence?
contains onlyone tree node, then the ?span root?
is this treenode; otherwise, the ?span root?
is the virtualfather node (Zhang et al, 2009a) of the ?rootnode sequence?.
Fig.
4 illustrates the reformedFig.
3 by introducing the virtual node?VBP+ADVP+TO?
as the ?span root?
of thespan of ?try hard to?.The ?span root?
facilitates us to extract rules withtarget side structure information.
Given a sub-tree ofthe source tree, we have a set of non-terminal frontiernodes.
For each such frontier node, we can find itscorresponding target ?span root?.
If the ?span root?is a virtual node, then we add it into the target tree asa virtual segmentation joint point.
After adding the?span root?
as joint point, we are able to ensure thateach frontier source node has only one correspondingtarget node, then we can use any traditional rule ex-traction algorithm to extract rules, including thoserules with one-to-many non-terminal frontier map-pings.Fig.
5.
Tree-to-tree sequence rulesFig.
5 lists the corresponding rules with targetstructure information of the tree-to-string rules in Fig2.
All the three rules cannot be extracted by previoustree-to-tree mapping methods (Liu et al, 2009).
Theprevious tree-sequence-based methods (Zhang et al,2008; Zhang et al, 2009a) can extracted rule 3 sincethey allow one-to-many mapping in root node level.But they cannot extract rule 1 and rule 2.
Therefore,for any tree-to-string rule, our method can alwaysfind the corresponding tree-to-tree sequence rule.
Asa result, our rule coverage is the same astree-to-string framework while our rules containmore informative target syntax information.
Later wewill show that using our decoding algorithm thetree-to-tree sequence search space is exponentiallyreduced to the same as tree-to-string search space.That is to say, we do not need to worry about the ex-ponential search space issue of tree-to-tree sequencemodel existing in previous work.3.3 Rule Extraction in Tree ContextGiven a word aligned tree pair, we first extract theset of minimum tree to string rules (Galley et al2004), then for each tree-to-string rule, we can easilyextract its corresponding tree-to-tree sequence ruleby introducing the virtual span root node.
After that,we generate the composite rules by iteratively com-bining small rules.Fig.
6.
Rule combination and virtual node removing443Please note that in generating composite rules, ifthe joint node is a virtual node, we have to recoverthe original link and remove this virtual node toavoid unnecessary ambiguity.
Fig.
6 illustrates thecombination process of rule 2 and rule 3 in Fig.
5.
Asa result, all of our extract rules do not contain anyinternal virtual nodes.3.4 Rule Extraction in Forest ContextIn forest pair context, we also first generate theminimum tree-to-string rule set as Mi et al (2008),and for each tree-to-string rule, we find its corres-ponding tree-to-tree sequence rules, and then do rulecomposition.In tree pair context, given a tree-to-string rule,there is one and only one corresponding tree-to-treesequence rule.
But in forest pair context, given onesuch tree-to-string rule, there are many correspond-ing tree-to-tree sequence rules.
All these sub-treesform one or more sub-forests2 of the entire big targetforest.
If we can identify the sub-forests, i.e., all ofthe hyper-edges of the sub-forests, we can retrieve allthe sub-trees from the sub-forests as the target sidesof the corresponding tree-to-tree sequence rules.Given a source sub-tree, we can obtain the targetroot span where the target sub-forests start and thefrontier spans where the target sub-forests stop.
Toindentify all the hyper-edges in the sub-forests, westart from every node covering the root span, traversefrom top to down, mark all the hyper-edges visitedand stop at the node if its span is a sub-span of one ofthe forest frontier spans or if it is a word node.
Thereason we stop at the node once it fell into a frontierspan (i.e.
the span of the node is a sub-span of thefrontier span) is to guarantee that given any frontierspan, we could stop at the ?root node sequence?
ofthis span by Def.
2.For example, Fig.
7 is a source sub-tree of rule 2in Fig.
5 and the circled part in Fig.
8 is one of itscorresponding target sub-forests.
Its correspondingtarget root span is [1,4] (corresponding to source root?VP? )
and its corresponding target frontier span is{[1,3], study[4,4]}.
Now given the target forest, westart from node VP[1,4] and traverse from top todown, finally stop at following nodes: VBP[1,1],ADVP[2,2], TO[3,3], study .2 All the sub-forests cover the same span.
But their roots havedifferent grammar tags as the roots?
names.
The root may be avirtual span root node in the case of the one-to-many frontiernon-terminal node mappings.Please note that the starting root node must be asingle node, being either a normal forest node or avirtual ?span root?
node.
The virtual ?span root?node serves as the frontier node of upper rules androot node of the currently being extracted rules.
Be-cause we extract rules in a top-to-down manner, thenecessary virtual ?span root?
node for currentsub-forest has already been added into the globalforest when extracting upper level rules.Figure 7.
A source sub-tree in rule 2Fig.
8.
The corresponding target sub-forest for the tree ofFigure 7.3.5 Fractional Count of RuleFollowing Mi and Huang (2008) and Liu et al(2009), we assign a fractional count to a rule tomeasure how likely it appears given the context ofthe forest pair.
In following equation, ?S?
meanssource sub-tree, ?T?
means target sub-tree, ?SF?
issource forest and ?TF?
is the target forest.??
?, ?
|?
?, ???
?
???|?
?, ???
?
???|?
?, ????
???|???
?
???|??
?444The above equation means the fractional count ofa source-target tree pair is just the product of each oftheir fractional count in corresponding forest contextin following equation.???????
?| ????????
?
???????????????????
???????
??????
????????
?
?
?????????????
?
?
?????????????????????????????
?????
?where ?
and ?
are the outside and inside probabil-ities.
In addition, if a sub-tree root is a virtual node(formed by a root node sequence), then we use fol-lowing equation to approximate the outside probabil-ity of the virtual node.?????
????????
????
?
??
?????
?
?
?# ??
?????
??
?
?4 Decoding4.1 Traditional Forest-based DecodingA typical translation process of a forest-based systemis to first convert the source packed forest into a tar-get translation forest, and then apply search algo-rithm to find the best translation result from this tar-get translation forest (Mi et al, 2008).For the tree-to-string model, the forest conversionprocess is as following: given an input packed forest,we do pattern matching (Zhang et al, 2009b) withthe source side structures in the rule set.
For eachmatched rule, we establish its target side as a hy-per-edge in the target forest.Fig.
9.
A forest conversion step in a tree to string modelFig.
9 exemplifies a conversion step in the tree tostring model.
A sub-tree structure with two hy-per-edge ?VP[2,4] => ADVP[2,2] VP[3,4]?
and?VP[3,4] => ADVP[3,4] VP[4,4]?
is converted intoa target hyper-edge ?X-VP[2,4] => X-ADVP[3,3]X-ADVP[2,2]  X-VP[4,4] ?.
The node ?X-VP[4,4]?in the target forest means that its syntactic label intarget forest is ?X?
and it is translated from thesource node ?VP[4,4]?
in the source forest.
In thistarget hyper-edge, ?X-ADVP[3,3] X-ADVP[2,2]?means the translation from source node ?ADVP[3,3]?is put before the translation from ?ADVP[2,2]?,representing a structure reordering.4.2 Toward Bilingual Syntax-aware Trans-lation GenerationAs we could see in section 4.1, there is only one kindof non-terminal symbol ?X?
in the target side.
It is abig challenge to rely on such a coarse label to gener-ate a translation with fine syntactic quality.
For ex-ample, a source node may be translated into a ?NP?
(noun phrase) in target side.
However, in this rule setwith the only symbol ?X?, it may be merged withupper structure as a ?VP?
(verb phrase) instead, be-cause there is no way to favor one over another.
Inthis case, the target tree does not well model thetranslation syntactically.
In addition, all of the inter-nal structure information in the target side is ignoredby the tree-to-string rules.One natural solution to the above issue is to usethe tree to tree/tree sequence model, which havericher target syntax structures for more discrimina-tive probability and finer labels to guide the combi-nation process.
However, the tree to tree/tree se-quence model may face very severe computationalproblem and so-called ?spurious ambiguities?
issue.Theoretically, if in the tree-to-tree sequence mod-el-based decoding, we just give a penalty to the in-compatible-node combinations instead of pruning outthe translation paths, then the set of sentences gener-ated by the tree-to-tree sequence model is identical tothat of the tree-to-string model since everytree-to-tree sequence rule can be projected into atree-to-string rule.
Motivated by this, we propose asolution call parallel hypothesis spaces searching tosolve the computational and ?spurious ambiguities?issues mentioned above.
In the meanwhile, we canfully utilize the target structure information to guidetranslation.We restructure the tree-to-tree sequence rule set bygrouping all the rules according to their correspond-ing tree-to-string rules.
This behaves like a?tree-to-forest?
rule.
The ?forest?
encodes all the treesequences with same corresponding string.
With there-constructed rule set, during decoding, we generatetwo target translation hypothesis spaces (in the formof packed forests) synchronously by the tree-to-string445rules and tree-to-tree sequence rules, and maintainthe projection between them.
In other words, wegenerate hypothesis (searching) from thetree-to-string forest and calculate the probability(evaluating syntax goodness) for each hypothesis bythe hyper-edges in the tree-to-tree sequence forest.4.3 Parallel Hypothesis SpacesFig.
10.
Mapping from tree-to-tree sequence intotree-to-string ruleIn this subsection, we describe what the parallelsearch spaces are and how to construct them.
Asshown at Fig.
10, given a tree-to-tree sequence rule,it is easy to find its corresponding tree-to-string ruleby simply ignoring the target inside structure andrenaming the root and leaves non-terminal labels into?X?.
We iterate through the tree-to-tree sequence ruleset, find its corresponding tree-to-string rule and thengroup those rules with the same tree-to-string projec-tion.
After that, the original tree-to-tree sequence ruleset becomes a set of smaller rule sets.
Each of them isindexed by a unique tree-to-string rule.We apply the tree-to-string rules to generate anexplicit target translation forest to represent the targetsentences space.
At the same time, whenever atree-to-string rule is applied, we also retrieve its cor-responding tree-to-tree sequence rule set and gener-ate a set of latent hyper-edges with fine-grained syn-tax information.
In this case, we have two parallelforests, one with coarse explicit hyper-edges and theother fine and latent.
Given a hyper-edge (or a node)in the coarse forest, there are a group of correspond-ing latent hyper-edges (or nodes) with finer syntaxlabels in the fine forest.
Accordingly, given a tree inthe coarse forest, there is a corresponding sub-forestin the latent fine forest.
We can view the latent fineforest as imbedded inside the explicit coarse forest.
Ifan explicit hyper-edge is viewed as a big cable, thenthe group of its corresponding latent hyper-edges isthe small wires inside it.We rely on the explicit hyper-edges to enumeratepossible hypothesis while using the latent hy-per-edges to measure its translation probability andsyntax goodness.
Thus, the complexity of the searchspace is reduced into the tree-to-string model level,while keeping the target language generation syntac-tic aware.
More importantly, we thoroughly avoidthose spurious ambiguities introduced by thetree-to-tree sequence rules.4.4 Decoding with Parallel HypothesisSpacesFig.
11.
Derivation path and derivation forestIn this subsection, we show exactly how our decoderfinds the best result from the parallel spaces.
Wegenerate hypothesis by traversing the coarse forest inthe parallel spaces with cube-pruning (Huang andChiang, 2007).
Given a newly generated hypothesis,it is affiliated with a derivation path (tree) in thecoarse forest and a group of derivation paths(sub-forest) in the finer forest.
As shown in Fig.
11,the left part is the derivation path formed by a coarsehyper-edge, consisting the newly-generated sub-tree?X => X X X?
connecting with three previous-ly-generated sub paths while the right part is the de-rivation forest formed by newly-generated finer hy-per-edges rooted at ?VP?
and ?S?, and previous-ly-generated sub-forests.In this paper, we use the sum of probabilities of allthe derivation paths in the finer forest to measure thequality of the candidate translation suggested by thehypothesis.
From Fig.
11, we can see there may bemore than one corresponding finer forests, it is easyto understand that the sum of all the trees?
probabili-ties in these finer forests is equal to the sum of theinside probability of all these root nodes of these fin-er forests.
We adopt the dynamic programming tocompute the probability of the finer forest: wheneverwe generate a new hypothesis by concatenating a446coarse hyper-edge and its sub-path, we find its cor-responding finer hyper-edges and sub-forests, do thecombination and accumulate probabilities from bot-tom to up.
For the coarse hyper-edge, because thereis only one label ?X?, any sub-path could be easilyconcatenated with upper structure covering the samesub-span without the need of checking label compa-tibility.
While for the finer hyper-edges, we only linkthe root nodes of sub-forests to upper hyper-edgeswith the same linking node label.
This is to guaranteesyntactic goodness.
In case there are some leaf nodesof the upper hyper-edges fail to find correspondingsub-forest roots with the same label (e.g.
the ?NP?
inred color in the rightmost of Fig 11), we simply linkit into the nodes with the least inside probability(among these sub-forests), and at the same time givea penalty score to this combination.
If some rootnodes of some sub-forest still cannot find upper leafnodes to concatenate (e.g.
the ?CP?
in red color inFig.
11), we simply ignore them.
After the combina-tion process, it is straightforward to accumulate theinside probability dynamically from bottom up.5 Experiment5.1 Experimental SettingsWe evaluate our method on the Chinese-Englishtranslation task.
We first carry out a series empiricalstudy on a set of parallel data with 30K sentencepairs, and then do experiment on a larger data set toensure that the effectiveness of our method is consis-tent across data set of different size.
We use theNIST 2002 test set as our dev set, and NIST 2003and NIST 2005 test sets as our test set.
A 3-gramlanguage model is trained on the target side of thetraining data by the SRILM Toolkits (Stolcke, 2002)with modified Kneser-Ney smoothing (Kneser andNey, 1995).
We train Charniak?s parser (Charniak,2000) on CTB5.0 for Chinese and ETB3.0 for Eng-lish and modify it to output packed forest.
GIZA++(Och and Ney, 2003) and the heuristics?grow-diag-final-and?
are used to generate m-to-nword alignments.
For the MER training (Och, 2003),Koehn?s MER trainer (Koehn, 2007) is modified forour system.
For significance test, we use Zhang etal.
?s implementation (Zhang et al 2004).
Our evalu-ation metrics is case-sensitive closest BLEU-4 (Pa-pineni et al, 2002).
We use following features in oursystems: 1) bidirectional tree-to-tree sequence proba-bility, 2) bidirectional tree-to-string probability, 3)bidirectional lexical translation probability, 4) targetlanguage model, 5) source tree probability 6) the av-erage number of unmatched nodes in the target forest.7) the length of the target translation, 8) the numberof glue rules used.5.2 Empirical Study on Small DataWe set forest pruning threshold (Mi et al, 2008) to 8on both source and target forests for rule extraction.For each source sub-tree, we set its height up to 3,width up to 7 and extract up to 10-best target struc-tures.
In decoding, we set the pruning threshold to 10for the input source forest.
Table 1 compares theperformance in NIST 2003 data set of our methodand several state-of-the-art systems as our baseline.1) MOSES: phrase-based system (Koehn et al,2007)2) FT2S: forest-based tree-to-string system (Miand Huang, 2008; Mi et al, 2008)3) FT2T: forest-based tree-to-tree system (Liu etal., 2009).4) FT2TS (1to1): our forest-based tree-to-treesequence system, where 1to1 means onlyone-to-one frontier non-terminal node map-ping is allowed, thus the system does not fol-low our non-isomorphic mapping framework.5) FT2TS (1toN): our forest-based tree-to-treesequence system that allows one-to-manyfrontier non-terminal node mapping by fol-lowing our non-isomorphic mapping frame-workIn addition, our proposed parallel searching space(PSS) technology can be applied to both tree to treeand tree to sequence systems.
Thus in table 1, for thetree-to-tree/tree sequence systems, we report twoBLEU scores, one uses this technology (withPSS)and one does not (noPSS).Model BLEU-4MOSES 23.39FT2S 26.10FT2T noPSS 23.40 withPSS 24.46FT2TS (1to1) noPSS 25.39 withPSS 26.58FT2TS (1toN) noPSS 26.30 withPSS 27.70Table 1.
Performance comparison of different methodsFrom Table 1, we can see that:4471) All the syntax-based systems (except FT2T(noPSS) (23.40)) consistently outperform thephrase-based system MOSES significantly(?
?
0.01 ), indicating that syntactic know-ledge is very useful to SMT.2) The PSS technology shows significant perfor-mance improvement ??
?
0.01?
in all mod-els, which clearly shows effectiveness of thePSS technology in utilizing target structuresfor target language generation.3) FT2TS (1toN) significantly outperforms(?
?
0.01) FT2TS (1to1) in both cases (noPSSand withPSS).
This convincingly shows theeffectiveness of our non-isomorphic mappingframework in capturing the non-isomorphicstructure translation equivalences.4) Both FT2TS systems significantly outperformFT2T( ?
?
0.01).
This verifies the effective-ness of tree sequence rules.5) FT2TS shows different level of performanceimprovements over FT2S with the best casehaving 1.6 (27.70-26.10) BLEU score im-provement over FT2S.
This suggests that thetarget structure information is very useful, butwe need to find a correct way to effectivelyutilize it.1to1 1toN ratio1735871 2363771 1:1.36Table 2.
Statistics on node mapping in forest, where?1to1?
means the number of nodes in source forestthat can be translated into one node in target forestand ?1toN?
means the number of nodes in sourceforest that have to be translated into more than onenode in target forest, where the node refers tonon-terminal nodes onlyModel # of rules T2S coveredFT2T 295732 26.8%FT2TS(1to1) 631487 57.1%FT2TS (1toN) 1945168 100%Table 3.
Statistics of rule coverage, where ?T2Scovered?
means the percentage of tree-to-stringrules that can be covered by the modelTable 2 studies the node isomorphism between bi-lingual forest pair.
We can see that thenon-isomorphic node translation mapping (1toN)accounts for 57.6% (=1.36/(1+1.36)) of all the forestnon-terminal nodes with target translation.
Thismeans that the one-to-many node mapping is a majorissue in structure transformation.
It also empiricallyjustifies the importance of our non-isomorphic map-ping framework.Table 3 shows the rule coverage of different bi-lingual structure mapping model.
FT2T only covers26.8% tree-to-string rules, so it performs worse thanFT2S as shown in Table 1.
FT2TS (1to1) does notallow one-to-many frontier node mapping, so it couldonly recover the non-isomorphic node mapping inthe root level, while FT2TS (1toN) could make it atboth root and leaf levels.
Therefore, it is not surpris-ing that in Table 3, FT2TS (1toN) cover many morerules than FT2TS (1to1) because given a source tree,there are many leaves, if any one of them isnon-isomorphic, then it could not be covered by theFT2TS (1to1).Decoding Method BLEU-4 Speed (sec/sent)Traditional:FT2TS (1toN) (noPPS) 26.30 152.6Ours:FT2TS (1toN) (withPPS) 27.70 5.22Table 4.
Performance and speed comparisonTable 4 clearly shows the advantage of our decod-er over the traditional one.
Ours could not only gen-erate better translation result, but also be152.6/5.22>30 times faster.
This mainly attributes totwo reasons: 1) one-to-many frontier node mappingequipments the model with more ability to capturemore non-isomorphic structure mappings than tradi-tional models, and 2) ?parallel search space?
enablesthe decoder to fully utilize target syntactic informa-tion, but keeping the size of search space the same asthat a ?tree to string?
model explores.5.3 Results on Larger Data SetWe also carry out experiment on a larger datasetconsisting of the small dataset used in last sectionand the FBIS corpus.
In total, there are 280K parallelsentence pairs with 9.3M Chinese words and 11.8MEnglish words.
A 3-gram language model is trainedon the target side of the parallel corpus and the GI-GA3 Xinhua portion.
We compare our system(FT2TS with 1toN and withPPS) with twostate-of-the-art baselines: the phrase-based systemMOSES and the forest-based tree-to-string system448implemented by us.
Table 5 clearly shows the effec-tiveness of our method is consistent across small andlarger corpora, outperforming FT2S by 1.6-1.8BLEU and the MOSES by 3.3-4.0 BLEU statisticallysignificantly (p<0.01).Model BLEUNIST2003 NIST2005MOSES 29.51 27.53FT2S 31.21 29.72FT2TS 32.88 31.50Table 5.
Performance on larger data set6 ConclusionsIn this paper, we propose a framework to address theissue of bilingual non-isomorphic structure mappingand a novel parallel searching space scheme to effec-tively utilize target syntactic structure information inthe context of forest-based tree to tree sequence ma-chine translation.
Based on this framework, we de-sign an efficient algorithm to extract tree-to-tree se-quence translation rules from word aligned bilingualforest pairs.
We also elaborate the parallel searchingspace-based decoding algorithm and the node labelchecking scheme, which leads to very efficient de-coding speed as fast as the forest-based tree-to-stringmodel does, at the same time is able to utilize infor-mative target structure knowledge.
We evaluate ourmethods on both small and large training data setsand two NIST test sets.
Experimental results showour methods statistically significantly outperform thestate-of-the-art models across different size of cor-pora and different test sets.
In the future, we are in-terested in testing our algorithm at forest-based treesequence to tree sequence translation.ReferencesEugene Charniak.
2000.
A maximum-entropy inspiredparser.
NAACL-00.Eugene Charniak, Kevin Knight, and Kenji Yamada.
2003.Syntax-based language models for statistical machinetranslation.
MT Summit IX.
40?46.David Chiang.
2007.
Hierarchical phrase-based transla-tion.Computational Linguistics, 33(2).Steve DeNeefe, Kevin Knight.
2009.
Synchronous TreeAdjoining Machine Translation.
EMNLP-2009.727-736.Jason Eisner.
2003.
Learning non-isomorphic tree map-pings for MT.
ACL-03 (companion volume).Michel Galley, Mark Hopkins, Kevin Knight and DanielMarcu.
2004.
What?s in a translation rule?HLT-NAACL-04.
273-280.Liang Huang.
2008.
Forest Reranking: DiscriminativeParsing with Non-Local Features.
ACL-HLT-08.586-594Liang Huang and David Chiang.
2005.
Better k-best Pars-ing.
IWPT-05.
53-64Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.ACL-07.
144?151Dan Klein and Christopher D. Manning.
2001.
Parsingand Hypergraphs.
IWPT-2001.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for M-gram language modeling.ICASSP-95, 181-184Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Cal-lison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantinand Evan Herbst.
2007.
Moses: Open Source Toolkit forStatistical Machine Translation.
ACL-07.
177-180.
(poster)Yang Liu, Qun Liu and Shouxun Lin.
2006.
Tree-to-StringAlignment Template for Statistical Machine Transla-tion.
COLING-ACL-06.
609-616.Yang Liu, Yun Huang, Qun Liu and Shouxun Lin.
2007.Forest-to-String Statistical Translation Rules.
ACL-07.704-711.Yang Liu, Yajuan L?, Qun Liu.
2009.
ImprovingTree-to-Tree Translation with Packed Forests.
ACL-09.558-566Haitao Mi, Liang Huang, and Qun Liu.
2008.
For-est-based translation.
ACL-HLT-08.
192-199.Haitao Mi and Liang Huang.
2008.
Forest-based Transla-tion Rule Extraction.
EMNLP-08.
206-214.Franz J. Och.
2003.
Minimum error rate training in statis-tical machine translation.
ACL-03.
160-167.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics.
29(1) 19-51.Kishore Papineni, Salim Roukos, Todd Ward andWei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
ACL-02.
311-318.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
ICSLP-02.
901-904.Masaru Tomita.
1987.
An Efficient Aug-mented-Context-Free Parsing Algorithm.
Computation-al Linguistics 13(1-2): 31-46449Xavier Carreras and Michael Collins.
2009.Non-projective Parsing for Statistical Machine Trans-lation.
EMNLP-2009.
200-209.K.
Yamada and K. Knight.
2001.
A Syntax-Based Statis-tical Translation Model.
ACL-01.
523-530.Hui Zhang, Min Zhang, Haizhou Li, Aiti Aw and ChewLim Tan.
2009a.
Forest-based Tree Sequence to StringTranslation Model.
ACL-IJCNLP-09.
172-180.Hui Zhang, Min Zhang, Haizhou Li, and Chew Lim Tan.2009b.
Fast Translation Rule Matching for Syn-tax-based Statistical Machine Translation.
EMNLP-09.1037-1045.Min Zhang, Hongfei Jiang, Ai Ti Aw, Jun Sun, Chew LimTan and Sheng Li.
2007.
A Tree-to-Tree Align-ment-based model for statistical Machine translation.MT-Summit-07.
535-542Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li, ChewLim Tan, Sheng Li.
2008.
A Tree Sequence Align-ment-based Tree-to-Tree Translation Model.ACL-HLT-08.
559-567.Ying Zhang, Stephan Vogel, Alex Waibel.
2004.
Inter-preting BLEU/NIST scores: How much improvement dowe need to have a better system?
LREC-04.
2051-2054.450
