Book ReviewsDiscourse ProcessingManfred StedeUniversity of PotsdamMorgan & Claypool (Synthesis Lectures on Human Language Technologies, editedby Graeme Hirst, volume 15), 2011, ix+155 pp; paperbound, ISBN 978-1-60845-734-2,$40.00; ebook, ISBN 978-1-60845-735-9, $30.00 or by subscriptionReviewed byBonnie WebberUniversity of EdinburghDiscourse is coming in from the cold.
After years of being ignored by researchers inother areas of computational linguistics and language technology, many of these sameresearchers are beginning to think that their own work could benefit from treating textas more than just a bag of sentences.
That is, they are beginning to think that discourseoffers some low-hanging fruit?achievable improvements in system performance thatexploit either aspects of text structure or the context that text establishes and uses forefficient referring and/or predicational expressions.?
2012 Association for Computational LinguisticsThis new monograph on Discourse Processing by Manfred Stede both reflects thisnew zeitgeist and provides an introduction to discourse for researchers in computationallinguistics or language technology with little or no background in the area.
This clearand timely monograph consists of a brief introduction to discourse, a meaty chapter oneach of the three aspects of discourse processing that hold most promise for languagetechnology, and a brief conclusion on where discourse research might go in the future.I will go through the three major chapters, and then make some general remarks.Chapter 2Chapter 2 addresses two distinct types of large-scale discourse structure: structure thatfollows from a text belonging to a particular genre, and structure that follows fromthe topic (or topic mix) of a text.
The genre of a text affects features such as styleand register.
What is relevant here is structure that genre may confer on a text.
Stedesuggests that some, but not all, texts inherit large-scale structure from their genre,calling some unstructured, some structured, and some semi-structured.
As a reader, Idid not find this distinction useful, because all text that belongs to a genre seems toget some large-scale structure from it.
On the other hand, all or part of this structuremight simply not be manifest in the kind of lexico-syntactic features that automatedsystems regularly rely on for text segmentation.
As a case in point, although Stede offersthe text Suffering (used as a running example throughout the book) as an example ofunstructured text, like other instances of Comments in the Talk of the Town section ofthe New Yorker magazine, its large-scale structure comprises a ?hook?
aimed at gettingthe reader?s attention, followed by a short essay that concludes with a serious point.Although ways of attracting a reader?s attention may not have specific lexico-syntacticfeatures, it might still be possible to recognize the transition between ?hook?
and essay,and essay structure itself is what ETS?s eRater system (Burstein and Chodorow 2010)aims to recognize and evaluate.Computational Linguistics Volume 38, Number 4This first half of Chapter 2 focuses on the genre-based structure of scientific textsand of film reviews.
Here researchers have already shown that language technologiessuch as information extraction and sentiment analysis benefit from taking such structureinto account, so this is entirely appropriate for the book?s target audience.
More ongenre-based functional structure and its use in producing structured biomedicalabstracts can be found in the recent survey of research on discourse structure andlanguage technology by Webber, Egg, and Kordoni (2012).The second half of Chapter 2 discusses large-scale discourse structure associatedwith patterns of topics.
Such structure is often found in expository writing such asencyclopedia articles and travel pieces.
Here, changing patterns of content words corre-late well with changes in topic, rendering them useful for the many approaches to textsegmentation that are well-described in this half of the chapter.
Because the discussionhere of probabilistic models for topic segmentation is rather short, the reader whowantsto know more should consult the excellent survey of topic segmentation methods byPurver (2011).Chapter 3Chapter 3, entitled Coreference Resolution, addresses more than this, dealing with theresolution of other expressions whose reduction is licensed by the discourse context,such as bridging reference and ?other?
reference, which Halliday and Hasan (1976)call comparative reference because it occurs with comparative forms such as ?largerfish?
and ?a more impressive poodle,?
as well as with ?other,?
?another,?
and ?such.
?Stede justifies inclusion of this chapter for two reasons?the close connection betweencoreference resolution and topic segmentation and the benefits to text analysis providedby having its pronouns resolved.
But another reason must be the link mentioned earlierbetween text and context: Discourse creates the context in which context-reducedexpressions make sense, so it falls naturally within the tasks of discourse processingto resolve them, either through modeling context explicitly or through the use ofproxies.The chapter starts with an overview of coreference and anaphora that covers boththeir forms and their functions.
This is followed by an important section on corpusannotation (Section 3.2), included because (as Stede notes) what has been annotated andwhy it has been annotated strongly determines what expressions are resolved and how.This section identifies many of the problems in coreference annotation that have beenraised in the literature, but recognizes that research has to make use of the resourcesthat exist and not just the resources it wants.
Several of these are indicated at the endof the section, reminding one that it would have been useful to have some pointers inChapter 2 to corpora available for genre-based segmentation (such as Liakata?s ARTcorpus)1 or for topic-based segmentation.Stede then links the current chapter to the previous one through a discussion ofentity-based coherence (Section 3.3) and then discusses how to identify when a pronounor definite noun phrase should be treated as anaphoric (Section 3.4) as groundworkfor discussion of anaphora resolution (Sections 3.5?3.7).
Missing from the discussion ofdetecting non-anaphoric (pleonastic) pronouns is mention of Bergsma?s recent systemNADA for doing this (Bergsma and Yarowsky 2011).21 Downloadable from http://www.aber.ac.uk/en/ns/research/cb/projects/art/art-corpus/2 Downloadable from http://code.google.com/p/nada-nonref-pronoun-detector/918Book ReviewsThe discussion of anaphora resolution covers rule-based approaches to resolvingnominal anaphora (Section 3.5) and then supervised machine learning methods foranaphora resolution (Section 3.6).
The latter follows the structure (albeit not the con-tent) of Ng?s survey (2010), in discussing mention-pair models, and then entity-mentionmodels.
Whereas Ng then discusses ranking models, including his cluster ranker (Rahmanand Ng 2009), which is conceptually similar to the Lappin and Leass (1994) approachdescribed in Section 3.5, Stede discusses a range of more recent models, most of whichare subsequent to Ng?s survey.Section 3.8 surveys methods evaluating coreference resolution and some of theknown problems in doing so.
A good complement to this is Byron?s too-little-knowndiscussion of problems in the consistent reporting of such results (Byron 2001).
Chap-ter 3 concludes with a section on Recent Trends, which would also have been useful inChapter 2.Chapter 4The fourth and longest chapter deals with semantic or pragmatically oriented coherencerelations that hold between adjacent text spans or discourse units.
Whereas the previoustwo chapters were essentially theory-neutral, the presentation in Chapter 4 largelyreflects the perspective of Rhetorical Structure Theory (Mann and Thompson 1988).
RSTtakes a text to be a sequence of elementary discourse units that comprise the leaves ofa tree structure of coherence relations between recursively defined discourse units.
RSTalso assumes that one of the arguments to a coherence relation may be more importantto the speaker?s purpose than the other, calling the former the nucleus and the latter,the satellite.This RST framework dictates the structure of the chapter: Following an introductorysection that explains andmotivates coherence relations, each subsequent section consid-ers the next task in an RST analysis?segmenting a text into elementary discourse units(Section 4.2), recognizing which (adjacent) units stand in a coherence relation and what(single) relation holds between them (Section 4.3), and finally, inducing the overall treestructure of coherence relations that hold between recursively defined discourse units(Section 4.4).
All these tasks are well described, both from a theoretical perspective andin terms of automated procedures for carrying them out.
Coverage of relevant work isvery high.Where the reader may get confused, however, is that a good proportion of themore recent work on identifying coherence relations does not fall within the frameworkof RST, and thus doesn?t adhere to several of its assumptions?in particular, that atext is divisible into a covering sequence of elementary discourse units, that only onerelation can hold between discourse units, that the arguments to a coherence relationmust be adjacent, that one argument to a coherence relation may intrinsically conveyinformation that is more important to the speaker?s purpose than the other, and thatcoherence relations impose an overall tree structure on a text in terms of recursivelydefined discourse units.Although Chapter 4 discusses the Penn Discourse TreeBank (Prasad et al 2008) andits ?somewhat modest annotations?
(page 126), the discussion is framed in terms ofRST tasks, whereas the assumptions underlying the Penn Discourse TreeBank reflect itsconcerns with a quite different set of tasks involved in recognizing coherence relations.The first task requires finding evidence for a coherence relation (in the form of adiscourse connective such as a coordinating or subordinating conjunction or a discourseadverbial, or in the form of sentence adjacency) and then determining (1) if the evidence919Computational Linguistics Volume 38, Number 4does indeed signal a coherence relation, given that evidence is often ambiguous; (2) ifit does, what constitutes its arguments; and (3) what is its sense.
Although Chapter 4covers some of this work (Dinesh et al 2005; Wellner and Pustejovsky 2007; Elwelland Baldridge 2008; Pitler and Nenkova 2009; Prasad, Joshi, and Webber 2010), itsappearance within the context of a discussion of RST-tasks may lead to some confusion.Chapter 4 concludes with a brief discussion of some important open issues regard-ing coherence relations, including problems with associating a large text span witha single recursive structure of coherence relations and problems with inter-annotatoragreement.SummaryFor its intended audience, this monograph will serve as a compact, readable intro-duction to the subject of discourse processing.
The relevant phenomena are presentedclearly, as are many of the computational methods for dealing with them.
What readerswon?t get is criteria for choosing among the methods or an understanding of what eachmethod is good for.
This problem may reflect the absence of comparable performanceresults and useful error analyses in the original publications, however.Also missing from the monograph is discussion of applications of discourse pro-cessing, and pointers to more of the resources available to researchers interested indiscourse structure.
This is where the additional resources I have mentioned may provecomplementary.Finally, a plea to the series editor: Monographs such as this one really need an index.Some monographs in the series have one, whereas others (like this one) don?t.
Becausethe series appears in both electronic and physical format, one could excuse the formernot having an explicit index, since in most cases, one can get away with the basic searchfacility in the Adobe Reader.
Nothing similar is available for the nicely sized physicalmonographs.
Their authors should be strongly encouraged to provide them.ReferencesBergsma, Shane and David Yarowsky.
2011.Nada: A robust system for non-referentialpronoun detection.
In Proceedings ofDAARC, 12 pages, Faro.Burstein, Jill and Martin Chodorow.
2010.Progress and new directions in technologyfor automated essay evaluation.In R. Kaplan, editor, The Oxford Handbookof Applied Linguistics.
Oxford UniversityPress, 2nd edition, pages 487?497.Byron, Donna.
2001.
The uncommondenominator: A proposal for consistentreporting of pronoun resolution results.Computational Linguistics, 27(4):569?577.Dinesh, Nikhil, Alan Lee, Eleni Miltsakaki,Rashmi Prasad, Aravind Joshi, andBonnie Webber.
2005.
Attributionand the (non-)alignment of syntacticand discourse arguments of connectives.In ACL Workshop on Frontiers in CorpusAnnotation, pages 29?36, Ann Arbor, MI.Elwell, Robert and Jason Baldridge.2008.
Discourse connective argumentidentification with connective specificrankers.
In Proceedings of the IEEEConference on Semantic Computing,8 pages, Santa Clara, CA.Halliday, Michael and Ruqaiya Hasan.1976.
Cohesion in English.
Longman.Lappin, Shalom and Herbert Leass.
1994.An algorithm for pronominal anaphoraresolution.
Computational Linguistics,20(4):535?561.Mann, William and Sandra Thompson.1988.
Rhetorical structure theory: Towarda functional theory of text organization.Text, 8(3):243?281.Ng, Vincent.
2010.
Supervised nounphrase coreference research: Thefirst 15 years.
In Proceedings of the48th Annual Meeting of the Associationfor Computational Linguistics,pages 1396?1411, Uppsala.Pitler, Emily and Ani Nenkova.
2009.
Usingsyntax to disambiguate explicit discourseconnectives in text.
In ACL-IJCNLP ?09:Proceedings of the 47th Meeting of the920Book ReviewsAssociation for Computational Linguisticsand the 4th International Joint Conference onNatural Language Processing, pages 13?16,Singapore.Prasad, Rashmi, Nikhil Dinesh, Alan Lee,Eleni Miltsakaki, Livio Robaldo, AravindJoshi, and Bonnie Webber.
2008.
The PennDiscourse TreeBank 2.0.
In Proceedings,6th International Conference on LanguageResources and Evaluation, pages 2961?2968,Marrakech.Prasad, Rashmi, Aravind Joshi, and BonnieWebber.
2010.
Exploiting scope for shallowdiscourse parsing.
In Proceedings of the7th International Conference on LanguageResources and Evaluation (LREC 2010),pages 2076?2083, Malta.Purver, Matthew.
2011.
Topic segmentation.In Gokhan Tur and Renato de Mori,editors, Spoken Language Understanding:Systems for Extracting SemanticInformation from Speech, Chapter 11.Wiley, Hoboken, NJ.Rahman, Altaf and Vincent Ng.
2009.Supervised models for coreferenceresolution.
In Proceedings of the 2009Conference on Empirical Methods inNatural Language Processing,pages 968?977, Singapore.Webber, Bonnie, Markus Egg, andValia Kordoni.
2012.
Discoursestructure and language technology.Natural Language Engineering,doi:10.1017/S1351324911000337.Wellner, Ben and James Pustejovsky.
2007.Automatically identifying the argumentsto discourse connectives.
In Proceedings ofthe 2007 Conference on Empirical Methods inNatural Language Processing (EMNLP),pages 92?101, Prague.This book review was edited by Pierre Isabelle.Bonnie Webber is a Professor of Informatics at Edinburgh University.
She received both her MScand PhD from Harvard University.
She is a Fellow of the Royal Society of Edinburgh and ofthe American Association for Artificial Intelligence.
Both her early and her recent research havefocused on computational approaches to discourse and question answering.
In between, shehas carried out research on animation from instructions, medical decision support systems, andbiomedical text processing.
Webber?s e-mail address is bonnie.webber@ed.ac.uk.921
