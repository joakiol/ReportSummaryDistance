Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 83?92,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsJigs and Lures: Associating Web Queries with Structured EntitiesPatrick PantelMicrosoft ResearchRedmond, WA, USAppantel@microsoft.comAriel FuxmanMicrosoft ResearchMountain View, CA, USAarielf@microsoft.comAbstractWe propose methods for estimating the prob-ability that an entity from an entity databaseis associated with a web search query.
Asso-ciation is modeled using a query entity clickgraph, blending general query click logs withvertical query click logs.
Smoothing tech-niques are proposed to address the inherentdata sparsity in such graphs, including inter-polation using a query synonymy model.
Alarge-scale empirical analysis of the smooth-ing techniques, over a 2-year click graphcollected from a commercial search engine,shows significant reductions in modeling er-ror.
The association models are then appliedto the task of recommending products to webqueries, by annotating queries with productsfrom a large catalog and then mining query-product associations through web search ses-sion analysis.
Experimental analysis showsthat our smoothing techniques improve cover-age while keeping precision stable, and over-all, that our top-performing model affects 9%of general web queries with 94% precision.1 IntroductionCommercial search engines use query associationsin a variety of ways, including the recommendationof related queries in Bing, ?something different?
inGoogle, and ?also try?
and related concepts in Ya-hoo.
Mining techniques to extract such query asso-ciations generally fall into four categories: (a) clus-tering queries by their co-clicked url patterns (Wenet al, 2001; Baeza-Yates et al, 2004); (b) leveragingco-occurrences of sequential queries in web searchquery sessions (Zhang and Nasraoui, 2006; Boldi etal., 2009); (c) pattern-based extraction over lexico-syntactic structures of individual queries (Pas?ca andDurme, 2008; Jain and Pantel, 2009); and (d) distri-butional similarity techniques over news or web cor-pora (Agirre et al, 2009; Pantel et al, 2009).
Thesetechniques operate at the surface level, associatingone surface context (e.g., queries) to another.In this paper, we focus instead on associating sur-face contexts with entities that refer to a particu-lar entry in a knowledge base such as Freebase,IMDB, Amazon?s product catalog, or The Libraryof Congress.
Whereas the former models might as-sociate the string ?Ronaldinho?
with the strings ?ACMilan?
or ?Lionel Messi?, our goal is to associate?Ronaldinho?
with, for example, the Wikipedia en-tity page ?wiki/AC Milan?
or the Freebase entity?en/lionel mess?.
Or for the query string ?ice fish-ing?, we aim to recommend products in a commer-cial catalog, such as jigs or lures.The benefits and potential applications are large.By knowing the entity identifiers associated with aquery (instead of strings), one can greatly improveboth the presentation of search results as well asthe click-through experience.
For example, considerwhen the associated entity is a product.
Not onlycan we present the product name to the web user,but we can also display the image, price, and re-views associated with the entity identifier.
Once theentity is clicked, instead of issuing a simple websearch query, we can now directly show a productpage for the exact product; or we can even performactions directly on the entity, such as buying the en-tity on Amazon.com, retrieving the product?s oper-83ating manual, or even polling your social networkfor friends that own the product.
This is a big steptowards a richer semantic search experience.In this paper, we define the association between aquery string q and an entity id e as the probabilitythat e is relevant given the query q, P (e|q).
Fol-lowing Baeza-Yates et al (2004), we model rele-vance as the likelihood that a user would click one given q, events which can be observed in largequery-click graphs.
Due to the extreme sparsityof query click graphs (Baeza-Yates, 2004), we pro-pose several smoothing models that extend the clickgraph with query synonyms and then use the syn-onym click probabilities as a background model.We demonstrate the effectiveness of our smoothingmodels, via a large-scale empirical study over real-world data, which significantly reduce model errors.We further apply our models to the task of query-product recommendation.
Queries in session logsare annotated using our association probabilities andrecommendations are obtained by modeling session-level query-product co-occurrences in the annotatedsessions.
Finally, we demonstrate that our modelsaffect 9% of general web queries with 94% recom-mendation precision.2 Related WorkWe introduce a novel application of significant com-mercial value: entity recommendations for generalWeb queries.
This is different from the vast bodyof work on query suggestions (Baeza-Yates et al,2004; Fuxman et al, 2008; Mei et al, 2008b; Zhangand Nasraoui, 2006; Craswell and Szummer, 2007;Jagabathula et al, 2011), because our suggestionsare actual entities (as opposed to queries or docu-ments).
There is also a rich literature on recom-mendation systems (Sarwar et al, 2001), includingsuccessful commercial systems such as the Ama-zon product recommendation system (Linden et al,2003) and the Netflix movie recommendation sys-tem (Bell et al, 2007).
However, these are entity-to-entity recommendations systems.
For example,Netflix recommends movies based on previouslyseen movies (i.e., entities).
Furthermore, these sys-tems have access to previous transactions (i.e., ac-tual movie rentals or product purchases), whereasour recommendation system leverages a different re-source, namely query sessions.In principle, one could consider vertical searchengines (Nie et al, 2007) as a mechanism for as-sociating queries to entities.
For example, if we typethe query ?canon eos digital camera?
on a commercesearch engine such as Bing Shopping or GoogleProducts, we get a listing of digital camera entitiesthat satisfy our query.
However, vertical search en-gines are essentially rankers that given a query, re-turn a sorted list of (pointers to) entities that are re-lated to the query.
That is, they do not expose actualassociation scores, which is a key contribution of ourwork, nor do they operate on general search queries.Our smoothing methods for estimating associ-ation probabilities are related to techniques de-veloped by the NLP and speech communities tosmooth n-gram probabilities in language model-ing.
The simplest are discounting methods, suchas additive smoothing (Lidstone, 1920) and Good-Turing (Good, 1953).
Other methods leveragelower-order background models for low-frequencyevents, such as Katz?
backoff smoothing (Katz,1987), Witten-Bell discounting (Witten and Bell,1991), Jelinek-Mercer interpolation (Jelinek andMercer, 1980), and Kneser-Ney (Kneser and Ney,1995).In the information retrieval community, Ponte andCroft (1998) are credited for accelerating the useof language models.
Initial proposals were basedon learning global smoothing models, where thesmoothing of a word would be independent of thedocument that the word belongs to (Zhai and Laf-ferty, 2001).
More recently, a number of localsmoothing models have been proposed (Liu andCroft, 2004; Kurland and Lee, 2004; Tao et al,2006).
Unlike global models, local models leveragerelationships between documents in a corpus.
In par-ticular, they rely on a graph structure that representsdocument similarity.
Intuitively, the smoothing of aword in a document is influenced by the smoothingof the word in similar documents.
For a completesurvey of these methods and a general optimizationframework that encompasses all previous proposals,please see the work of Mei, Zhang et al (2008a).All the work on local smoothing models has beenapplied to the prediction of priors for words in docu-ments.
To the best of our knowledge, we are the firstto establish that query-click graphs can be used to84create accurate models of query-entity associations.3 Association ModelTask Definition: Consider a collection of entitiesE.
Given a search query q, our task is to computeP (e|q), the probability that an entity e is relevant toq, for all e ?
E.We limit our model to sets of entities that canbe accessed through urls on the web, such as Ama-zon.com products, IMDB movies, Wikipedia enti-ties, and Yelp points of interest.Following Baeza-Yates et al (2004), we modelrelevance as the click probability of an entity givena query, which we can observe from click logs ofvertical search engines, i.e., domain-specific searchengines such as the product search engine at Ama-zon, the local search engine at Yelp, or the travelsearch engine at Bing Travel.
Clicked results in avertical search engine are edges between queries andentities e in the vertical?s knowledge base.
Generalsearch query click logs, which capture direct userintent signals, have shown significant improvementswhen used for web search ranking (Agichtein et al,2006).
Unlike for general search engines, verticalsearch engines have typically much less traffic re-sulting in extremely sparse click logs.In this section, we define a graph structure forrecording click information and we propose severalmodels for estimating P (e|q) using the graph.3.1 Query Entity Click GraphWe define a query entity click graph, QEC(Q?U ?E,Cu ?
Ce), as a tripartite graph consisting of a setof query nodes Q, url nodes U , entity nodes E, andweighted edges Cu exclusively between nodes of Qand nodes of U , as well as weighted edges Ce ex-clusively between nodes of Q and nodes of E. Eachedge in Cu and Ce represents the number of clicksobserved between query-url pairs and query-entitypairs, respectively.
Let wu(q, u) be the click weightof the edges in Cu, and we(q, e) be the click weightof the edges in Ce.IfCe is very large, then we can model the associa-tion probability, P (e|q), as the maximum likelihoodestimation (MLE) of observing clicks on e given thequery q:P?mle(e|q) =we(q,e)?e?
?E we(q,e?)
(3.1)Figure 1 illustrates an example query entitygraph linking general web queries to entities in alarge commercial product catalog.
Figure 1a illus-trates eight queries in Q with their observed clicks(solid lines) with products in E1.
Some probabil-ity estimates, assigned by Equation 3.1, include:P?mle(panfish jigs, e1) = 0, P?mle(ice jigs, e1) = 1,and P?mle(ice auger, e4) =ce(ice auger,e4)ce(ice auger,e3)+ce(ice auger,e4).Even for the largest search engines, query clicklogs are extremely sparse, and smoothing techniquesare necessary (Craswell and Szummer, 2007; Gao etal., 2009).
By considering only Ce, those clickedurls that map to our entity collection E, the sparsitysituation is even more dire.
The sparsity of the graphcomes in two forms: a) there are many queries forwhich an entity is relevant that will never be seenin the click logs (e.g., ?panfish jig?
in Figure 1a);and b) the query-click distribution is Zipfian andmost observed edges will have very low click countsyielding unreliable statistics.
In the following sub-sections, we present a method to expand QEC withunseen queries that are associated with entities in E.Then we propose smoothing methods for leveraginga background model over the expanded click graph.Throughout our models, we make the simplifyingassumption that the knowledge base E is complete.3.2 Graph ExpansionFollowing Gao et al (2009), we address the spar-sity of edges in Ce by inferring new edges throughtraversing the query-url click subgraph, UC(Q ?U,Cu), which contains many more edges than Ce.If two queries qi and qj are synonyms or near syn-onyms2, then we expect their click patterns to besimilar.We define the synonymy similarity, s(qi, qj) asthe cosine of the angle between qi and qj, the clickpattern vectors of qi and qj , respectively:cosine(qi,qj) =qi?qj?qi?qi?
?qj?qjwhere q is an nu dimensional vector consisting ofthe pointwise mutual information between q andeach url u in U , pmi(q, u):1Clicks are collected from a commerce vertical search en-gine described in Section 5.1.2A query qi is a near synonym of a query qj if most relevantresults of qi are also relevant to qj .
Section 5.2.1 describes ouradopted metric for near synonymy.85ice fishingice augerEskimoMakoAugerLuretechHot HooksHi-TechFish ?N?Bucketicefishingworld.comiceteam.comcabelas.comstrikemaster.comice fishing tacklefishusa.compower augerice jigsfishing bucketcustomjigs.comkeeperlures.companfish jigsd rockStrike-Lite IIAugerLuretechHot Hooksice fishing tackleice jigspanfish jigs?
?eqwe ,?
?eqwe ,?EQU?
?ji qqs ,ice augercabelas.comstrikemaster.compower augerd rock?
?uqwu ,a) b)c)d)fishingice fishingice fishing minnesotad rockice fishing tackleice fishingt 0t 1t 3t 4t2(e1)(e1)(e2)(e3)(e4)Figure 1: Example QEC graph: (a) Sample queries in Q, clicks connecting queries with urls in U , and clicks toentities in E; (b) Zoom on edges in Cu illustrating clicks observed on urls with weight wu(q, u) as well as synonymyedges between queries with similarity score s(qi, qj) (Section 3.2); (c) Zoom on edges in Ce where solid lines indicateobserved clicks with weight we(q, e) and dotted lines indicate inferred clicks with smoothed weight w?e(q, e) (Sec-tion 3.3); and (d) A temporal sequence of queries in a search session illustrating entity associations propagating fromthe QEC graph to the queries in the session (Section 4).pmi(q, u) = log(wu(q,u)??q??Q,u?
?U wu(q?,u?)?u?
?U wu(q,u?)?q?
?Q wu(q?,u))(3.2)PMI is known to be biased towards infrequentevents.
We apply the discounting factor, ?
(q, u),proposed in (Pantel and Lin, 2002):?
(q,u)= wu(q,u)wu(q,u)+1 ?min(?q?
?Q wu(q?,u),?u?
?U wu(q,u?))min(?q?
?Q wu(q?,u),?u?
?U wu(q,u?
))+1Enrichment: We enrich the original QEC graphby creating a new edge {q?,e}, where q?
?
Q and e ?E, if there exists a query q where s(q, q?)
> ?
andwe(q, e) > 0. ?
is set experimentally, as describedin Section 5.2.Figure 1b illustrates similarity edges created be-tween query ?ice auger?
and both ?power auger?and ?d rock?.
Since ?ice auger?
was connected toentities e3 and e4 in the original QEC, our expan-sion model creates new edges in Ce between {powerauger, e3}, {power auger, e4}, and {d rock, e3}.For each newly added edge {q,e}, P?mle = 0 ac-cording to our model from Equation 3.1 since wehave never observed any clicks between q and e. In-stead, we define a new model that uses P?mle whenclicks are observed and otherwise assigns uniformprobability mass, as:P?hybr(e|q) =??
?P?mle(e|q) if ?e?|we(q,e?)>01?e?
?E ?(q,e?
)otherwise(3.3)where ?
(q, e) is an indicator variable which is 1 ifthere is an edge between {q, e} in Ce.This model does not leverage the local synonymygraph in order to transfer edge weight to unseenedges.
In the next section, we investigate smooth-ing techniques for achieving this.3.3 SmoothingSmoothing techniques can be useful to alleviate datasparsity problems common in statistical models.
Inpractice, methods that leverage a background model(e.g., a lower-order n-gram model) have shown mostpromise (Katz, 1987; Witten and Bell, 1991; Je-linek and Mercer, 1980; Kneser and Ney, 1995).
Inthis section, we present two smoothing methods, de-rived from Jelinek-Mercer interpolation (Jelinek andMercer, 1980), for estimating the target associationprobability P (e|q).Figure 1c highlights two edges, illustrated withdashed lines, inserted into Ce during the graph ex-pansion phase of Section 3.2. w?e(q, e) representsthe weight of our background model, which can beviewed as smoothed click counts, and are obtained86Label Model ReferenceUNIF P?unif (e|q) Eq.
3.8MLE P?mle(e|q) Eq.
3.1HYBR P?hybr(e|q) Eq.
3.3INTU P?intu(e|q) Eq.
3.6INTP P?intp(e|q) Eq.
3.7Table 1: Models for estimating the association probabil-ity P (e|q).by propagating clicks to unseen edges using the syn-onymy model as follows:w?e(q, e) =?q??Qs(q,q?)Nsq?
P?mle(e|q?)
(3.4)where Nsq =?q?
?Q s(q, q?).
By normalizingthe smoothed weights, we obtain our backgroundmodel, P?bsim:P?bsim(e|q) =w?e(q,e)?e?
?E w?e(q,e?)
(3.5)Below we propose two models for interpolating ourforeground model from Equation 3.1 with the back-ground model from Equation 3.5.Basic Interpolation: This smoothing model,P?intu(e|q), linearly combines our foreground andbackground models using a model parameter ?:P?intu(e|q)=?P?mle(e|q)+(1??
)P?bsim(e|q) (3.6)Bucket Interpolation: Intuitively, edges {q, e} ?Ce with higher observed clicks, we(q, e), should betrusted more than those with low or no clicks.
Alimitation of P?intu(e|q) is that it weighs the fore-ground and background models in the same way ir-respective of the observed foreground clicks.
Ourfinal model, P?intp(e|q) parameterizes the interpola-tion by the number of observed clicks:P?intp(e|q)=?
[we(q, e)]P?mle(e|q)+ (1?
?
[we(q, e)])P?bsim(e|q)(3.7)In practice, we bucket the observed click parame-ter, we(q, e), into eleven buckets: {1-click, 2-clicks,..., 10-clicks, more than 10 clicks}.Section 5.2 outlines our procedure for learn-ing the model parameters for both P?intu(e|q) andP?intp(e|q).3.4 SummaryTable 1 summarizes the association models pre-sented in this section as well as a strawman that as-signs uniform probability to all edges in QEC:P?unif (e|q) =1?e?
?E ?
(q, e?
)(3.8)In the following section, we apply these modelsto the task of extracting product recommendationsfor general web search queries.
A large-scale exper-imental study is presented in Section 5 supportingthe effectiveness of our models.4 Entity RecommendationQuery recommendations are pervasive in commer-cial search engines.
Many systems extract recom-mendations by mining temporal query chains fromsearch sessions and clickthrough patterns (Zhangand Nasraoui, 2006).
We adopt a similar strategy,except instead of mining query-query associations,we propose to mine query-entity associations, whereentities come from an entity database as described inSection 1.
Our technical challenge lies in annotatingsessions with entities that are relevant to the session.4.1 Product Entity DomainAlthough our model generalizes to any entity do-main, we focus now on a product domain.
Specifi-cally, our universe of entities,E, consists of the enti-ties in a large commercial product catalog, for whichwe observe query-click-product clicks, Ce, from thevertical search logs.
Our QEC graph is completedby extracting query-click-urls from a search engine?sgeneral search logs, Cu.
These datasets are de-scribed in Section 5.1.4.2 Recommendation AlgorithmWe hypothesize that if an entity is relevant to aquery, then it is relevant to all other queries co-occurring in the same session.
Key to our methodare the models from Section 3.Step 1 ?
Query Annotation: For each query q in asession s, we annotate it with a set Eq, consisting ofevery pair {e, P?
(e|q)}, where e ?
E such that thereexists an edge {q, e} ?
Ce with probability P?
(e|q).Note that Eq will be empty for many queries.Step 2 ?
Session Analysis: We build a query-entity frequency co-occurrence matrix, A, consist-ing of n|Q| rows and n|E| columns, where each rowcorresponds to a query and each column to an entity.87The value of the cell Aqe is the sum over each ses-sion s, of the maximum edge weight between anyquery q?
?
s and e3:Aqe =?s?S ?
(s, e)where S consists of all observed search sessions and:?
(s, e) = argmaxP?
(e|q?
)({e, P?
(e|q?)}
?
Eq?),?q?
?
sStep 3 ?
Ranking: We compute ranking scoresbetween each query q and entity e using pointwisemutual information over the frequencies in A, simi-larly to Eq.
3.2.The final recommendations for a query q are ob-tained by returning the top-k entities e according toStep 3.
Filters may be applied on: f the frequencyAqe; and p the pointwise mutual information rank-ing score between q and e.5 Experimental Results5.1 DatasetsWe instantiate our models from Sections 3 and 4 us-ing search query logs and a large catalog of prod-ucts from a commercial search engine.
We formour QEC graphs by first collecting in Ce aggregatequery-click-entity counts observed over two yearsin a commerce vertical search engine.
Similarly,Cu is formed by collecting aggregate query-click-urlcounts observed over six months in a web search en-gine, where each query must have frequency at least10.
Three final QEC graphs are sampled by takingvarious snapshots of the above graph as follows: a)TRAIN consists of 50% of the graph; b) TEST con-sists of 25% of the graph; c) DEV consists of 25%of the graph.5.2 Association Models5.2.1 Model ParametersWe tune the ?
parameters for P?intu and P?intp againstthe DEV QEC graph.
There are twelve parametersto be tuned: ?
for P?intu and ?
(1), ?
(2), ..., ?(10),?
(> 10) for P?intp, where ?
(x) is the observedclick bucket as described in Section 3.3.
For each,we choose the parameter value that minimizes themean-squared error (MSE) of the DEV set, where3Note that this co-occurrence occurs because q?
was anno-tated with entity e in the same session as q occurred.00.050.10.150.20.250.30.350.40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9MSEAlphaAlpha vs. MSE: Heldout Training For Alpha Parameters BasicBucket_1Bucket_2Bucket_3Bucket_4Bucket_5Bucket_6Bucket_7Bucket_8Bucket_9Bucket_10Bucket_11Figure 2: Alpha tuning on held out data.Model MSE Var Err/MLE MSEW Var Err/MLEP?unif 0.0328?
0.0112 -25.7% 0.0663?
0.0211 -71.8%P?mle 0.0261 0.0111 ?
0.0386 0.0141 ?P?hybr 0.0232?
0.0071 11.1% 0.0385 0.0132 0.03%P?intu 0.0226?
0.0075 13.4% 0.0369?
0.0133 4.4%P?intp 0.0213?
0.0068 18.4% 0.0375?
0.0131 2.8%Table 2: Model analysis: MSE and MSEW with vari-ance and error reduction relative to P?mle.
?
indicates sta-tistical significance over P?mle with 95% confidence.model probabilities are computed using the TRAINQEC graph.
Figure 2 illustrates the MSE rangingover [0, 0.05, 0.1, ..., 1].We trained the query synonym model of Sec-tion 3.2 on the DEV set and hand-annotated 100 ran-dom synonymy pairs according to whether or not thepairs were synonyms 2.
Setting ?
= 0.4 results in aprecision > 0.9.5.2.2 AnalysisWe evaluate the quality of our models in Table 1 byevaluating their mean-squared error (MSE) againstthe target P (e|q) computed on the TEST set:MSE(P?
)=?
{q,e}?CTe(PT (e|q)?P?
(e|q))2MSEW (P?
)=?
{q,e}?CTewTe (q,e)?
(PT (e|q)?P?
(e|q))2where CTe are the edges in the TEST QEC graphwith weight wTe (q, e), PT (e|q) is the target proba-bility computed over the TEST QEC graph, and P?is one of our models trained on the TRAIN QECgraph.
MSE measures against each edge type,which makes it sensitive to the long tail of theclick graph.
Conversely, MSEW measures againsteach edge instance, which makes it a good mea-sure against the head of the click graph.
We expectour smoothing models to have much more impacton MSE (i.e., the tail) than on MSEW since headqueries do not suffer from data sparsity.Table 2 lists the MSE and MSEW results foreach model.
We consider P?unif as a strawman andP?mle as a strong baseline (i.e., without any graphexpansion nor any smoothing against a background8800.010.020.030.040.050.06MSEClick Bucket (scaled by query - instance coverage)Mean Squared Error vs. Click BucketUNI FML EH YBRI NTUI NTP1 2 43 5 6 7 8 9 10Figure 3: MSE of each model against the number ofclicks in the TEST corpus.
Buckets scaled by query in-stance coverage of all queries with 10 or fewer clicks.model).
P?unif performs generally very poorly, how-ever P?mle is much better, with an expected estima-tion error of 0.16 accounting for an MSE of 0.0261.As expected, our smoothing models have little im-provement on the head-sensitive metric (MSEW )relative to P?mle.
In particular, P?hybr performs nearlyidentically to P?mle on the head.
On the tail, all threesmoothing models significantly outperform P?mlewith P?intp reducing the error by 18.4%.
Table 3 listsquery-product associations for five randomly sam-pled products along with their model scores fromP?mle with P?intp.Figure 3 provides an intrinsic view into MSE asa function of the number of observed clicks in theTEST set.
As expected, for larger observed clickcounts (>4), all models perform roughly the same,indicating that smoothing is not necessary.
However,for low click counts, which in our dataset accountsfor over 20% of the overall click instances, we seea large reduction in MSE with P?intp outperformingP?intu, which in turn outperforms P?hybr.
P?unif per-forms very poorly.
The reason it does worse as theobserved click count rises is that head queries tend toresult in more distinct urls with high-variance clicks,which in turn makes a uniform model susceptible tomore error.Figure 3 illustrates that the benefit of the smooth-ing models is in the tail of the click graph, whichsupports the larger error reductions seen in MSE inTable 2.
For associations only observed once, P?intpreduces the error by 29% relative to P?mle.We also performed an editorial evaluation of thequery-entity associations obtained with bucket inter-polation.
We created two samples from the TESTdataset: one randomly sampled by taking clickweights into account, and the other sampled uni-formly at random.
Each set contains results forQuery P?mleP?intp Query P?mleP?intpGarmin GTM 20 GPS Canon PowerShot SX110 ISgarmin gtm 20 0.44 0.45 canon sx110 0.57 0.57garmin traffic receiver 0.30 0.27 powershot sx110 0.48 0.48garmin nuvi 885t 0.02 0.02 powershot sx110 is 0.38 0.36gtm 20 0 0.33 powershot sx130 is 0 0.33garmin gtm20 0 0.33 canon power shot sx110 0 0.20nuvi 885t 0 0.01 canon dig camera review 0 0.10Samsung PN50A450 50?
TV Devil May Cry: 5th Anniversary Col.samsung 50 plasma hdtv 0.75 0.83 devil may cry 0.76 0.78samsung 50 0.33 0.32 devilmaycry 0 1.0050?
hdtv 0.17 0.12 High Island Hammock/Stand Combosamsung plasma tv review 0 0.42 high island hammocks 1.00 1.0050?
samsung plasma hdtv 0 0.35 hammocks and stands 0 0.10Table 3: Example query-product association scores for arandom sample of five products.
Bold queries resultedfrom the expansion algorithm in Section 3.2.100 queries.
The former consists of 203 query-product associations, and the latter of 159 associa-tions.
The evaluation was done using Amazon Me-chanical Turk4.
We created a Mechanical Turk HIT5where we show to the Mechanical Turk workers thequery and the actual Web page in a Product searchengine.
For each query-entity association, we gath-ered seven labels and considered an association to becorrect if five Mechanical Turk workers gave a pos-itive label.
An association was considered to be in-correct if at least five workers gave a negative label.Borderline cases where no label got five votes werediscarded (14% of items were borderline for the uni-form sample; 11% for the weighted sample).
To en-sure the quality of the results, we introduced 30%of incorrect associations as honeypots.
We blockedworkers who responded incorrectly on the honey-pots so that the precision on honeypots is 1.
Theresult of the evaluation is that the precision of the as-sociations is 0.88 on the weighted sample and 0.90on the uniform sample.5.3 Related Product RecommendationWe now present an experimental evaluation of ourproduct recommendation system using the baselinemodel P?mle and our best-performing model P?intp.The goals of this evaluation are to (1) determinethe quality of our product recommendations; and (2)assess the impact of our association models on theproduct recommendations.5.3.1 Experimental SetupWe instantiate our recommendation algorithm fromSection 4.2 using session co-occurrence frequencies4https://www.mturk.com5HIT stands for Human Intelligence Task89Query Set Sample Query Bag Samplef 10 25 50 100 10 25 50 100p 10 10 10 10 10 10 10 10P?mle precision 0.89 0.93 0.96 0.96 0.94 0.94 0.93 0.92P?intp precision 0.86 0.92 0.96 0.96 0.94 0.94 0.93 0.94P?mle coverage 0.007 0.004 0.002 0.001 0.085 0.067 0.052 0.039P?intp coverage 0.008 0.005 0.003 0.002 0.094 0.076 0.059 0.045Rintp,mle 1.16 1.14 1.13 1.14 1.11 1.13 1.15 1.19Table 4: Experimental results for product recommenda-tions.
All configurations are for k = 10.from a one-month snapshot of user query sessions ata Web search engine, where session boundaries oc-cur when 60 seconds elapse in between user queries.We experiment with the recommendation parame-ters defined at the end of Section 4.2 as follows: k =10, f ranging from 10 to 100, and p ranging from 3to 10.For each configuration, we report coverage as thetotal number of queries in the output (i.e., the queriesfor which there is some recommendation) divided bythe total number of queries in the log.
For our per-formance metrics, we sampled two sets of queries:(a) Query Set Sample: uniform random sam-ple of 100 queries from the unique queries in theone-month log; and (b) Query Bag Sample:weighted random sample, by query frequency, of100 queries from the query instances in the one-month log.
For each sample query, we pooled to-gether and randomly shuffled all recommendationsby our algorithm using both P?mle and P?intp on eachparameter configuration.
We then manually anno-tated each {query, product} pair as relevant, mildlyrelevant or non-relevant.
In total, 1127 pairs wereannotated.
Interannotator agreement between twojudges on this task yielded a Cohen?s Kappa (Cohen,1960) of 0.56.
We therefore collapsed the mildlyrelevant and non-relevant classes yielding two finalclasses: relevant and non-relevant.
Cohen?s Kappaon this binary classification is 0.71.Let CM be the number of relevant (i.e., correct)suggestions recommended by a configurationM andlet |M | be the number of recommendations returnedby M .
Then we define the (micro-) precision of Mas: PM =CMC .
We define relative recall (Pantel etal., 2004) between two configurations M1 and M2as RM1,M2 =PM1?|M1|PM2?|M2|.5.3.2 ResultsTable 4 summarizes our results for some configura-tions (others omitted for lack of space).
Most re-Query Product Recommendationwedding gowns 27 Dresses (Movie Soundtrack)wedding gowns Bridal Gowns: The Basics of Designing, [...] (Book)wedding gowns Wedding Dress Hankiewedding gowns The Perfect Wedding Dress (Magazine)wedding gowns Imagine Wedding Designer (Video Game)low blood pressure Omron Blood Pressure Monitorlow blood pressure Healthcare Automatic Blood Pressure Monitorlow blood pressure Ridgecrest Blood Pressure Formula - 60 Capsuleslow blood pressure Omron Portable Wrist Blood Pressure Monitor?hello cupcake?
cookbook Giant Cupcake Cast Pan?hello cupcake?
cookbook Ultimate 3-In-1 Storage Caddy?hello cupcake?
cookbook 13 Cup Cupcakes and More Dessert Stand?hello cupcake?
cookbook Cupcake Stand Set (Toys)1 800 flowers Todd Oldham Party Perfect Bouquet1 800 flowers Hugs and Kisses Flower Bouquet with VaseTable 5: Sample product recommendations.markable is the {f = 10, p = 10} configurationwhere the P?intp model affected 9.4% of all queryinstances posed by the millions of users of a majorsearch engine, with a precision of 94%.
Althoughthis model covers 0.8% of the unique queries, thefact that it covers many head queries such as wal-mart and iphone accounts for the large query in-stance coverage.
Also since there may be many gen-eral web queries for which there is no appropriateproduct in the database, a coverage of 100% is notattainable (nor desirable); in fact the upper boundfor the coverage is likely to be much lower.Turning to the impact of the association modelson product recommendations, we note that precisionis stable in our P?intp model relative to our baselineP?mle model.
However, a large lift in relative recallis observed, up to a 19% increase for the {f = 100,p = 10} configuration.
These results are consistentwith those of Section 5.2, which compared the asso-ciation models independently of the application andshowed that P?intp outperforms P?mle.Table 5 shows sample product recommendationsdiscovered by our P?intp model.
Manual inspectionrevealed two main sources of errors.
First, ambiguityis introduced both by the click model and the graphexpansion algorithm of Section 3.2.
In many cases,the ambiguity is resolved by user click patterns (i.e.,users disambiguate queries through their browsingbehavior), but one such error was seen for the query?shark attack videos?
where several Shark-brandedvacuum cleaners are recommended.
This is becauseof the ambiguous query ?shark?
that is found in theclick logs and in query sessions co-occurring withthe query ?shark attack videos?.
The second sourceof errors is caused by systematic user errors com-monly found in session logs such as a user acciden-tally submitting a query while typing.
An example90session is: {?speedo?, ?speedometer?
}where the in-tended session was just the second query and the un-intended first query is associated with products suchas Speedo swimsuits.
This ultimately causes our sys-tem to recommend various swimsuits for the query?speedometer?.6 ConclusionLearning associations between web queries andentities has many possible applications, includingquery-entity recommendation, personalization byassociating entity vectors to users, and direct adver-tising.
Although many techniques have been devel-oped for associating queries to queries or queriesto documents, to the best of our knowledge this isthe first that aims to associate queries to entitiesby leveraging click graphs from both general searchlogs and vertical search logs.We developed several models for estimating theprobability that an entity is relevant given a userquery.
The sparsity of query entity graphs is ad-dressed by first expanding the graph with querysynonyms, and then smoothing query-entity clickcounts over these unseen queries.
Our best per-forming model, which interpolates between a fore-ground click model and a smoothed backgroundmodel, significantly reduces testing error when com-pared against a strong baseline, by 18%.
On associ-ations observed only once in our test collection, themodeling error is reduced by 29% over the baseline.We applied our best performing model to thetask of query-entity recommendation, by analyz-ing session co-occurrences between queries and an-notated entities.
Experimental analysis shows thatour smoothing techniques improve coverage whilekeeping precision stable, and overall, that our top-performing model affects 9% of general web querieswith 94% precision.References[Agichtein et al2006] Eugene Agichtein, Eric Brill, andSusan T. Dumais.
2006.
Improving web search rank-ing by incorporating user behavior information.
In SI-GIR, pages 19?26.
[Agirre et al2009] Eneko Agirre, Enrique Alfonseca,Keith Hall, Jana Kravalova, Marius Pas?ca, and AitorSoroa.
2009.
A study on similarity and relatednessusing distributional and wordnet-based approaches.
InNAACL, pages 19?27.
[Baeza-Yates et al2004] Ricardo Baeza-Yates, CarlosHurtado, and Marcelo Mendoza.
2004.
Query rec-ommendation using query logs in search engines.
InWolfgang Lindner, Marco Mesiti, Can Tu?rker, YannisTzitzikas, and Athena Vakali, editors, EDBT Work-shops, volume 3268 of Lecture Notes in ComputerScience, pages 588?596.
Springer.
[Baeza-Yates2004] Ricardo Baeza-Yates.
2004.
Web us-age mining in search engines.
In In Web Mining: Ap-plications and Techniques, Anthony Scime, editor.
IdeaGroup, pages 307?321.
[Bell et al2007] R. Bell, Y. Koren, and C. Volinsky.2007.
Modeling relationships at multiple scales toimprove accuracy of large recommender systems.
InKDD, pages 95?104.
[Boldi et al2009] Paolo Boldi, Francesco Bonchi, CarlosCastillo, Debora Donato, and Sebastiano Vigna.
2009.Query suggestions using query-flow graphs.
In WSCD?09: Proceedings of the 2009 workshop on Web SearchClick Data, pages 56?63.
ACM.
[Cohen1960] Jacob Cohen.
1960.
A coefficient of agree-ment for nominal scales.
Educational and Psycholog-ical Measurement, 20(1):37?46, April.
[Craswell and Szummer2007] Nick Craswell and MartinSzummer.
2007.
Random walks on the click graph.In SIGIR, pages 239?246.
[Fuxman et al2008] A. Fuxman, P. Tsaparas, K. Achan,and R. Agrawal.
2008.
Using the wisdom of thecrowds for keyword generation.
In WWW, pages 61?70.
[Gao et al2009] Jianfeng Gao, Wei Yuan, Xiao Li, Ke-feng Deng, and Jian-Yun Nie.
2009.
Smoothing click-through data for web search ranking.
In SIGIR, pages355?362.
[Good1953] Irving John Good.
1953.
The population fre-quencies of species and the estimation of populationparameters.
Biometrika, 40(3 and 4):237?264.
[Jagabathula et al2011] S. Jagabathula, N. Mishra, andS.
Gollapudi.
2011.
Shopping for products you don?tknow you need.
In To appear at WSDM.
[Jain and Pantel2009] Alpa Jain and Patrick Pantel.
2009.Identifying comparable entities on the web.
In CIKM,pages 1661?1664.
[Jelinek and Mercer1980] Frederick Jelinek andRobert L. Mercer.
1980.
Interpolated estimationof markov source parameters from sparse data.
In InProceedings of the Workshop on Pattern Recognitionin Practice, pages 381?397.
[Katz1987] Slava M. Katz.
1987.
Estimation of probabil-ities from sparse data for the language model compo-nent of a speech recognizer.
In IEEE Transactions on91Acoustics, Speech and Signal Processing, pages 400?401.
[Kneser and Ney1995] Reinhard Kneser and HermannNey.
1995.
Improved backing-off for m-gram lan-guage modeling.
In In Proceedings of the IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing, pages 181?184.
[Kurland and Lee2004] O. Kurland and L. Lee.
2004.Corpus structure, language models, and ad-hoc infor-mation retrieval.
In SIGIR, pages 194?201.
[Lidstone1920] George James Lidstone.
1920.
Note onthe general case of the bayes-laplace formula for in-ductive or a posteriori probabilities.
Transactions ofthe Faculty of Actuaries, 8:182?192.
[Linden et al2003] G. Linden, B. Smith, and J. York.2003.
Amazon.com recommendations: Item-to-itemcollaborative filtering.
IEEE Internet Computing,7(1):76?80.
[Liu and Croft2004] X. Liu and W. Croft.
2004.
Cluster-based retrieval using language models.
In SIGIR,pages 186?193.
[Mei et al2008a] Q. Mei, D. Zhang, and C. Zhai.
2008a.A general optimization framework for smoothing lan-guage models on graph structures.
In SIGIR, pages611?618.
[Mei et al2008b] Q. Mei, D. Zhou, and Church K. 2008b.Query suggestion using hitting time.
In CIKM, pages469?478.
[Nie et al2007] Z. Nie, J. Wen, and W. Ma.
2007.Object-level vertical search.
In Conference on Innova-tive Data Systems Research (CIDR), pages 235?246.
[Pantel and Lin2002] Patrick Pantel and Dekang Lin.2002.
Discovering word senses from text.
InSIGKDD, pages 613?619, Edmonton, Canada.
[Pantel et al2004] Patrick Pantel, Deepak Ravichandran,and Eduard Hovy.
2004.
Towards terascale knowl-edge acquisition.
In COLING, pages 771?777.
[Pantel et al2009] Patrick Pantel, Eric Crestan, ArkadyBorkovsky, Ana-Maria Popescu, and Vishnu Vyas.2009.
Web-scale distributional similarity and entityset expansion.
In EMNLP, pages 938?947.
[Pas?ca and Durme2008] Marius Pas?ca and Benjamin VanDurme.
2008.
Weakly-supervised acquisition ofopen-domain classes and class attributes from webdocuments and query logs.
In ACL, pages 19?27.
[Ponte and Croft1998] J. Ponte and B. Croft.
1998.
Alanguage modeling approach to information retrieval.In SIGIR, pages 275?281.
[Sarwar et al2001] B. Sarwar, G. Karypis, J. Konstan,and J. Reidl.
2001.
Item-based collaborative filteringrecommendation system.
In WWW, pages 285?295.
[Tao et al2006] T. Tao, X. Wang, Q. Mei, and C. Zhai.2006.
Language model information retrieval with doc-ument expansion.
In HLT/NAACL, pages 407?414.
[Wen et al2001] Ji-Rong Wen, Jian-Yun Nie, andHongJiang Zhang.
2001.
Clustering user queries of asearch engine.
In WWW, pages 162?168.
[Witten and Bell1991] I.H.
Witten and T.C.
Bell.
1991.The zero-frequency problem: Estimating the proba-bilities of novel events in adaptive text compression.IEEE Transactions on Information Theory, 37(4).
[Zhai and Lafferty2001] C. Zhai and J. Lafferty.
2001.
Astudy of smoothing methods for language models ap-plied to ad hoc information retrieval.
In SIGIR, pages334?342.
[Zhang and Nasraoui2006] Z. Zhang and O. Nasraoui.2006.
Mining search engine query logs for query rec-ommendation.
In WWW, pages 1039?1040.92
