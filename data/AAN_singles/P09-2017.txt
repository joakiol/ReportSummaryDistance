Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 65?68,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPDetecting Compositionality in Multi-Word ExpressionsIoannis KorkontzelosDepartment of Computer ScienceThe University of YorkHeslington, York, YO10 5NG, UKjohnkork@cs.york.ac.ukSuresh ManandharDepartment of Computer ScienceThe University of YorkHeslington, York, YO10 5NG, UKsuresh@cs.york.ac.ukAbstractIdentifying whether a multi-word expres-sion (MWE) is compositional or not is im-portant for numerous NLP applications.Sense induction can partition the contextof MWEs into semantic uses and there-fore aid in deciding compositionality.
Wepropose an unsupervised system to ex-plore this hypothesis on compound nom-inals, proper names and adjective-nounconstructions, and evaluate the contribu-tion of sense induction.
The evaluationset is derived from WordNet in a semi-supervised way.
Graph connectivity mea-sures are employed for unsupervised pa-rameter tuning.1 Introduction and related workMulti-word expressions (MWEs) are sequences ofwords that tend to cooccur more frequently thanchance and are either idiosyncratic or decompos-able into multiple simple words (Baldwin, 2006).Deciding idiomaticity of MWEs is highly impor-tant for machine translation, information retrieval,question answering, lexical acquisition, parsingand language generation.Compositionality refers to the degree to whichthe meaning of a MWE can be predicted by com-bining the meanings of its components.
Unlikesyntactic compositionality (e.g.
by and large), se-mantic compositionality is continuous (Baldwin,2006).In this paper, we propose a novel unsupervisedapproach that compares the major senses of aMWE and its semantic head using distributionalsimilarity measures to test the compositionality ofthe MWE.
These senses are induced by a graphbased sense induction system, whose parametersare estimated in an unsupervised manner exploit-ing a number of graph connectivity measures (Ko-rkontzelos et al, 2009).
Our method partitions thecontext space and only uses the major senses, fil-tering out minor senses.
In our approach the onlylanguage dependent components are a PoS taggerand a parser.There are several studies relevant to detectingcompositionality of noun-noun MWEs (Baldwin etal., 2003) verb-particle constructions (Bannard etal., 2003; McCarthy et al, 2003) and verb-nounpairs (Katz and Giesbrecht, 2006).
Datasets withhuman compositionality judgements are availablefor these MWE categories (Cook et al, 2008).Here, we focus on compound nominals, propernames and adjective-noun constructions.Our contributions are three-fold: firstly, we ex-perimentally show that sense induction can as-sist in identifying compositional MWEs.
Sec-ondly, we show that unsupervised parameter tun-ing (Korkontzelos et al, 2009) results in accuracythat is comparable to the best manually selectedcombination of parameters.
Thirdly, we proposea semi-supervised approach for extracting non-compositional MWEs from WordNet, to decreaseannotation cost.2 Proposed approachLet us consider the non-compositional MWE ?redcarpet?.
It mainly refers to a strip of red carpetinglaid down for dignitaries to walk on.
However, itis possible to encounter instances of ?red carpet?referring to any carpet of red colour.
Our methodfirst applies sense induction to identify the majorsemantic uses (senses) of a MWE (?red carpet?
)and its semantic head (?carpet?).
Then, it com-pares these uses to decide MWE compositionality.The more diverse these uses are, the more possi-bly the MWE is non-compositional.
Our algorithmconsists of 4 steps:A. Corpora collection and preprocessing.
Ourapproach receives as input a MWE (e.g.
?red car-pet?).
The dependency output of Stanford Parser(Klein and Manning, 2003) is used to locate the65Figure 1: ?red carpet?, sense induction exampleMWE semantic head.
Two different corpora arecollected (for the MWE and its semantic head).Each consists of webtext snippets of length 15 to200 tokens in which the MWE/semantic head ap-pears.
Given a MWE, a set of queries is created:All synonyms of the MWE extracted from Word-Net are collected1.
The MWE is paired with eachsynonym to create a set of queries.
For each query,snippets are collected by parsing the web-pages re-turned by Yahoo!.
The union of all snippets pro-duces the MWE corpus.
The corpus for a semantichead is created equivalently.To keep the computational time reasonable,only the longest 3, 000 snippets are kept from eachcorpus.
Both corpora are PoS tagged (GENIA tag-ger).
In common with Agirre et al (2006), onlynouns are kept and lemmatized, since they aremore discriminative than other PoS.B.
Sense Induction methods can be broadly di-vided into vector-space models and graph basedmodels.
Sense induction methods are evaluatedunder the SemEval-2007 framework (Agirre andSoroa, 2007).
We employ the collocational graph-based sense induction of Klapaftis and Manand-har (2008) in this work (henceforth referred to asKM).
The method consists of 3 stages:Corpus preprocessing aims to capture nounsthat are contextually related to the targetMWE/head.
Log-likelihood ratio (G2) (Dunning,1993) with respect to a large reference corpus, Web1T 5-gram Corpus (Brants and Franz, 2006), isused to capture the contextually relevant nouns.P1is the G2threshold below which nouns are re-moved from corpora.Graph creation.
A collocation is defined as apair of nouns cooccuring within a snippet.
Each1Thus, for ?red carpet?, corpora will be collected for ?redcarpet?
and ?carpet?.
The synonyms of ?red carpet?
are?rug?, ?carpet?
and ?carpeting?noun within a snippet is combined with everyother, generating(n2)collocations.
Each collo-cation is represented as a weighted vertex.
P2thresholds collocation frequencies and P3colloca-tion weights.
Weighted edges are drawn based oncooccurrence of the corresponding vertices in oneor more snippets (e.g.
w8and w7,9, fig.
1).
In con-trast to KM, frequencies for weighting vertices andedges are obtained from Yahoo!
web-page countsto deal with data sparsity.Graph clustering uses Chinese Whispers2(Bie-mann, 2006) to cluster the graph.
Each cluster nowrepresents a sense of the target word.KM produces larger number of clusters (uses)than expected.
To reduce it we exploit the onesense per collocation property (Yarowsky, 1995).Given a cluster li, we compute the set Siof snip-pets that contain at least one collocation of li.
Anyclusters laand lbare merged if Sa?
Sb.C.
Comparing the induced senses.
We usedtwo techniques to measure the distributional simi-larity of major uses of the MWE and its semantichead, both based on Jaccard coefficient (J).
?Ma-jor use?
denotes the cluster of collocations whichtags the most snippets.
Lee (1999) shows thatJ performs better than other symmetric similaritymeasures such as cosine, Jensen-Shannon diver-gence, etc.
The first is Jc= J(A,B) =|A?B||A?B|,where A, B are sets of collocations.
The second,Jsn, is based on the snippets that are tagged bythe induced uses.
Let Kibe the set of snippets inwhich at least one collocation of the use i occurs.Jsn= J(Kj,Kk), where j, k are the major usesof the MWE and its semantic head, respectively.D.
Determining compositionality.
Given themajor uses of a MWE and its semantic head,the MWE is considered as compositional, whenthe corresponding distributional similarity mea-sure (Jcor Jsn) value is above a parameter thresh-old, sim.
Otherwise, it is considered as non-compositional.3 Test set of MWEsTo the best of our knowledge there are no nouncompound datasets accompanied with composi-tionality judgements available.
Thus, we devel-oped an algorithm to aid human annotation.
Foreach of the 52, 217 MWEs of WordNet 3.0 (Miller,1995) we collected:2Chinese Whispers is not guaranteed to converge, thus200 was adopted as the maximum number of iterations.66Non-compositional MWEsagony aunt, black maria, dead end, dutch oven,fish finger, fool?s paradise, goat?s rue, green light,high jump, joint chiefs, lip service, living rock,monkey puzzle, motor pool, prince Albert,stocking stuffer, sweet bay, teddy boy, think tankCompositional MWEsbox white oak, cartridge brass, common iguana,closed chain, eastern pipistrel, field mushroom,hard candy, king snake, labor camp, lemon tree,life form, parenthesis-free notation, parking brake,petit juror, relational adjective, taxonomic category,telephone service, tea table, upland cottonTable 1: Test set with compositionality annotation.MWEs whose compositionality was successfullydetected by: (a) 1c1word baseline are in bold font,(b) manual parameter selection are underlined and(c) average cluster coefficient are in italics.1.
all synonyms of the MWE2.
all hypernyms of the MWE3.
sister-synsets of the MWE, within distance334.
synsets that are in holonymy or meronymy re-lation to the MWE, within distance 3If the semantic head of the MWE is also in theabove collection then the MWE is likely to be com-positional, otherwise it is likely that the MWE isnon-compositional.6, 287 MWEs were judged as potentially non-compositional.
We randomly chose 19 andchecked them manually.
Those that were compo-sitional were replaced by other randomly chosenones.
The process was repeated until we ended upwith 19 non-compositional examples.
Similarly,19 negative examples that were judged as compo-sitional were collected (Table 1).4 Evaluation setting and resultsThe sense induction component of our algorithmdepends upon 3 parameters: P1is the G2thresholdbelow which noun are removed from corpora.
P2thresholds collocation frequencies and P3colloca-tion weights.
We chose P1?
{5, 10, 15}, P2?
{102, 103, 104, 105} and P3?
{0.2, 0.3, 0.4}.
Forreference, P1values of 3.84, 6.63, 10.83 and15.13 correspond to G2values for confidence lev-els of 95%, 99%, 99.9% and 99.99%, respectively.To assess the performance of the proposed al-gorithm we compute accuracy, the percentage ofMWEs whose compositionality was correctly de-termined against the gold standard.3Locating sister synsets at distance D implies ascendingD steps and then descending D steps.Figure 2: Proposed system and 1c1word accuracy.Figure 3: Unweighted graph con/vity measures.We compared the system?s performance againsta baseline, 1c1word, that assigns the whole graphto a single cluster and no graph clustering isperformed.
1c1word corresponds to a relevantSemEval-2007 baseline (Agirre and Soroa, 2007)and helps in showing whether sense induction canassist determining compositionality.Our method was evaluated for each ?P1, P2, P3?combination and similarity measures Jcand Jsn,separately.
We used our development set to deter-mine if there are parameter values that verify ourhypothesis.
Given a sim value (see section 2, lastparagraph), we chose the best performing parame-ter combination manually.The best results for manual parameter selectionwere obtained for sim = 95% giving an accu-racy of 68.42% for detecting non-compositionalMWEs.
In all experiments, Jsnoutperforms Jc.With manually selected parameters, our system?saccuracy is higher than 1c1word for all sim values(5% points) (fig.
2, table 1).
The initial hypothesisholds; sense induction improves MWE composi-tionality detection.5 Unsupervised parameter tuningWe followed Korkontzelos et al (2009) to selectthe ?best?
parameters ?P1, P2, P3?
for the collo-cational graph of each MWE or head word.
Weapplied 8 graph connectivity measures (weightedand unweighted versions of average degree, clus-ter coefficient, graph entropy and edge density)separately on each of the clusters (resulting fromthe application of the chinese whispers algorithm).Each graph connectivity measure assigns ascore to each cluster.
We averaged the scores over67Figure 4: Weighted graph connectivity measures.the clusters from the same graph.
For each con-nectivity measure, we chose the parameter combi-nation ?P1, P2, P3?
that gave the highest score.While manual parameter tuning chooses a sin-gle globally best set of parameters (see section 4),the graph connectivity measures generate differentvalues of ?P1, P2, P3?
for each graph.5.1 Evaluation resultsThe best performing distributional similarity mea-sure is Jsn.
Unweighted versions of graph con-nectivity measures perform better than weightedones.
Figures 3 and 4 present a comparison be-tween the unweighted and weighted versions ofall graph connectivity measures, respectively, forall sim values.
Average cluster coefficient per-forms better or equally well to the other graphconnectivity measures for all sim values (exceptfor sim ?
[90%, 100%]).
The accuracy of aver-age cluster coefficient is equal (68.42%) to thatof manual parameter selection (section 4, table1).
The second best performing unweighted graphconnectivity measures is average graph entropy.For weighted graph connectivity measures, aver-age graph entropy performs best, followed by av-erage weighted clustering coefficient.6 Conclusion and Future WorkWe hypothesized that sense induction can assist inidentifying compositional MWEs.
We introducedan unsupervised system to experimentally explorethe hypothesis, and showed that it holds.
Weproposed a semi-supervised way to extract non-compositional MWEs from WordNet.
We showedthat graph connectivity measures can be success-fully employed to perform unsupervised parame-ter tuning of our system.
It would be interestingto explore ways to substitute querying Yahoo!
soas to make the system quicker.
Experimentationwith more sophisticated graph connectivity mea-sures could possibly improve accuracy.ReferencesE.
Agirre and A. Soroa.
2007.
Semeval-2007, task02: Evaluating WSI and discrimination systems.
Inproceedings of SemEval-2007.
ACL.E.
Agirre, D.
Mart?
?nez, O. de Lacalle, and A. Soroa.2006.
Two graph-based algorithms for state-of-the-art WSD.
In proceedings of EMNLP-2006.
ACL.T.
Baldwin, C. Bannard, T. Tanaka, and D. Widdows.2003.
An empirical model of MWE decomposabil-ity.
In proceedings of the MWE workshop.
ACL.T.
Baldwin.
2006.
Compositionality and MWEs: Sixof one, half a dozen of the other?
In proceedings ofthe MWE workshop.
ACL.C.
Bannard, T. Baldwin, and A. Lascarides.
2003.A statistical approach to the semantics of verb-particles.
In proceedings of the MWE workshop.ACL.C.
Biemann.
2006.
Chinese whispers - an efficientgraph clustering algorithm and its application toNLP problems.
In proceedings of TextGraphs.
ACL.T.
Brants and A. Franz.
2006.
Web 1t 5-gram corpus,version 1.
Technical report, Google Research.P.
Cook, A. Fazly, and S. Stevenson.
2008.
The VNC-Tokens Dataset.
In proceedings of the MWE work-shop.
ACL.T.
Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational Lin-guistics, 19(1):61?74.G.
Katz and E. Giesbrecht.
2006.
Automatic identifi-cation of non-compositional MWEs using latent se-mantic analysis.
In proceedings of the MWE work-shop.
ACL.I.
P. Klapaftis and S. Manandhar.
2008.
WSI usinggraphs of collocations.
In proceedings of ECAI-2008.D.
Klein and C. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.In proceedings of NIPS 15.
MIT Press.I.
Korkontzelos, I. Klapaftis, and S. Manandhar.
2009.Graph connectivity measures for unsupervised pa-rameter tuning of graph-based sense induction sys-tems.
In proceedings of the UMSLLS Workshop,NAACL HLT 2009.L.
Lee.
1999.
Measures of distributional similarity.
Inproceedings of ACL.D.
McCarthy, B. Keller, and J. Carroll.
2003.
De-tecting a continuum of compositionality in phrasalverbs.
In proceedings of the MWE workshop.
ACL.G.
A. Miller.
1995.
WordNet: a lexical database forEnglish.
ACM, 38(11):39?41.D.
Yarowsky.
1995.
Unsupervised WSD rivaling su-pervised methods.
In proceedings of ACL.68
