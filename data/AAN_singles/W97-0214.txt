Writing Annotation InstructionsJanyce WiebeDepar tment  of Computer  Science and the ComPut ing Research LaboratoryDept.
CS/Box  30001New Mexico State UniversityLas Cruces, NM 88003wiebe@cs, nmsu.
edu1 St ra teg ies  for  Wr i t ing  Annotat ion  Ins t ruct ionsIn two corpus annotation projects, we followed similar strategies for developing annotation instructionsand obtained good inter-coder reliability results for both (the instructions are similar in style to Allen &Core 1996).
Our goal in developing the annotation i structions was that they can be used reliably, after areasonable amount of training, by taggers who are non-experts but who have good language skills and theability to pay close attention to detail.
The instructions were developed iteratively, applying the currentscheme and then revising it in light of dlt~culties that arose.
We did not attempt to specify a formal set ofrules for the taggers to follow.
Rather, we give representative examples and appeal to the taggers' intuitions,asking them to generali,~ from the examples to new situations encountered in the text or dialog.An important strategy is to acknowledge, in the instructions, the weM~nesses of the task definition and thedit~iculties the tagger is likely to face.
If, for example, the taggers are being asked to categorize objects intoone of a set of mutually exclusive, exhaustive classes, for most NLP problems, the taggers will be faced withborderline, ambiguous, and vague instances.
We give the taggers trategies for dealing with such problems,such as asking themselves what is the most focal meaning component ofthe word in that particular context.The taggers hould also be assisted in targeting exactly which distinctions they are to make.
We have?
observed taggers' desires to take into account all aspects of the general problem surrounding the task.
Ifthere are closely related istinctions that are not to be tagged for, such as, for example, distinctions relatedto syntactic function, what we do is outline a related tagging task, to contrast it with the one the taggersare performing and to help them zero in on the particular distinctions they are to make.2 Work ing  Sess ion  FormatMany different strategies and types of instructions are possible.
The working session will be an opportunityfor participants to share their experiences and beliefs about questions uch as: how the annotation taskshould be defined (e.g., should borderline and ambiguous classifications be permitted, or should the taggerbe forced to choose a unique class for each object?
); what properties of annotation i structions are desirable;by what criteria should the annotators be selected; and what strategies for developing annotation i structionswork well.
The answers to such questions likely depend on the particular application.
Participants will alsoconsider how the following factors interact with the above questions: the purpose for which the corpus isbeing annotated; the purpose of the instructions themselves; how inter-coder reliability is to be evaluated;and how automatic systems for which the manual annotations are the standard are to be evaluated.For the workshop, participants will be supplied with a small data set to word-sense tag in advance, toestablish some common experience.
During the workshop, participants will address how this tagging mightbe made reliable with appropriate annotation i structions, taking into account he above issues.
It wouldalso be beneficial for participants o bring their own annotation i structions, characterizing them accordingto the above issues or related ones.
I will contact participants in advance to coordinate efforts.ReferencesAllen, James & Core, Mark.
1996.
Draft of DAMSL: Dialog Annotation Markup in Several Layers.Unpublished manuscript, available over the World Wide Web ath t tp : / /~ ,  cs .
rochester ,  edu/research/trains/annotat i n87
