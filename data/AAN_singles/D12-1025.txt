Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 266?275, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLarge Scale Decipherment for Out-of-Domain Machine TranslationQing Dou and Kevin KnightInformation Sciences InstituteDepartment of Computer ScienceUniversity of Southern California{qdou,knight}@isi.eduAbstractWe apply slice sampling to Bayesian de-cipherment and use our new deciphermentframework to improve out-of-domain machinetranslation.
Compared with the state of theart algorithm, our approach is highly scalableand produces better results, which allows usto decipher ciphertext with billions of tokensand hundreds of thousands of word types withhigh accuracy.
We decipher a large amountof monolingual data to improve out-of-domaintranslation and achieve significant gains of upto 3.8 BLEU points.1 IntroductionNowadays, state of the art statistical machine trans-lation (SMT) systems are built using large amountsof bilingual parallel corpora.
Those corpora areused to estimate probabilities of word-to-word trans-lation, word sequences rearrangement, and evensyntactic transformation.
Unfortunately, as paral-lel corpora are expensive and not available for ev-ery domain, performance of SMT systems dropssignificantly when translating out-of-domain texts(Callison-Burch et al2008).In general, it is easier to obtain in-domain mono-lingual corpora.
Is it possible to use domain specificmonolingual data to improve an MT system trainedon parallel texts from a different domain?
Some re-searchers have attempted to do this by adding a do-main specific dictionary (Wu et al2008), or miningunseen words (Daume?
and Jagarlamudi, 2011) us-ing one of several translation lexicon induction tech-niques (Haghighi et al2008; Koehn and Knight,2002; Rapp, 1995).
However, a dictionary is not al-ways available, and it is difficult to assign probabil-ities to a translation lexicon.
(Ravi and Knight, 2011b) have shown that onecan use decipherment to learn a full translationmodel from non-parallel data.
Their approach is ableto find translations, and assign probabilities to them.But their work also has certain limitations.
First ofall, the corpus they use to build the translation sys-tem has a very small vocabulary.
Secondly, althoughtheir algorithm is able to handle word substitutionciphers with limited vocabulary, its deciphering ac-curacy is low.The contributions of this work are:?
We improve previous decipherment work by in-troducing a more efficient sampling algorithm.In experiments, our new method improves de-ciphering accuracy from 82.5% to 88.1% on(Ravi and Knight, 2011b)?s domain specificdata set.
Furthermore, we also solve a verylarge word substitution cipher built from theEnglish Gigaword corpus and achieve 92.2%deciphering accuracy on news text.?
With the ability to handle a much larger vocab-ulary, we learn a domain specific translation ta-ble from a large amount of monolingual dataand use the translation table to improve out-of-domain machine translation.
In experiments,we observe significant gains of up to 3.8 BLEUpoints.
Unlike previous works, the translationtable we build from monolingual data do notonly contain unseen words but also words seenin parallel data.2662 Word Substitution CiphersBefore we present our new decipherment frame-work, we quickly review word substitution decipher-ment.Recently, there has been an increasing interest indecipherment work (Ravi and Knight, 2011a; Raviand Knight, 2008).
While letter substitution cipherscan be solved easily, nobody has been able to solvea word substitution cipher with high accuracy.As shown in Figure 1, a word substitution cipheris generated by replacing each word in a natural lan-guage (plaintext) sequence with a cipher token ac-cording to a substitution table.
The mapping in thetable is deterministic ?
each plaintext word type isonly encoded with one unique cipher token.
Solv-ing a word substitution cipher means recovering theoriginal plaintext from the ciphertext without know-ing the substitution table.
The only thing we rely onis knowledge about the underlying language.Figure 1: Encoding and Decipherment of a Word Substi-tution CipherHow can we solve a word substitution cipher?The approach is similar to those taken by cryptana-lysts who try to recover keys that convert encryptedtexts to readable texts.
Suppose we observe a largecipher string f and want to decipher it into English e.We can follow the work in (Ravi and Knight, 2011b)and assume that the cipher string f is generated inthe following way:?
Generate English plaintext sequence e =e1, e2...en with probability P(e).?
Replace each English plaintext token ei with acipher token fi with probability P (fi|ei).Based on the above generative story, we write theprobability of the cipher string f as:P (f)?
=?eP (e) ?n?iP?
(fi|ei) (1)We use this equation as an objective function formaximum likelihood training.
In the equation, P (e)is given by an ngram language model, which istrained using a large amount of monolingual texts.The rest of the task is to manipulate channel prob-abilities P?
(fi|ei) so that the probability of the ob-served texts P (f)?
is maximized.Theoretically, we can directly apply EM, as pro-posed in (Knight et al2006), or Bayesian decipher-ment (Ravi and Knight, 2011a) to solve the prob-lem.
However, unlike letter substitution ciphers,word substitution ciphers pose much greater chal-lenges to algorithm scalability.
To solve a word sub-stitution cipher, the EM algorithm has a computa-tional complexity of O(N ?
V 2 ?
R) and the com-plexity of Bayesian method is O(N ?
V ?
R), whereV is the size of plaintext vocabulary, N is the lengthof ciphertext, and R is the number of iterations.
Inthe world of word substitution ciphers, both V andN are very large, making these approaches impracti-cal.
(Ravi and Knight, 2011b) propose several mod-ifications to the existing algorithms.
However, themodified algorithms are only an approximation ofthe original algorithms and produce poor decipher-ing accuracy, and they are still unable to handle verylarge scale ciphers.To address the above problems, we propose thefollowing two new improvements to previous deci-pherment methods.?
We apply slice sampling (Neal, 2000) to scaleup to ciphers with a very large vocabulary.?
Instead of deciphering using the original ci-phertext, we break the ciphertext into bigrams,collect their counts, and use the bigrams withtheir counts for decipherment.267The new improvements allow us to solve a wordsubstitution cipher with billions of tokens and hun-dreds of thousands of word types.
Through betterapproximation, we achieve a significant increase indeciphering accuracy.
In the following section, wepresent details of our new approach.3 Slice Sampling for BayesianDeciphermentIn this section, we first give an introduction toBayesian decipherment and then describe how to useslice sampling for it.3.1 Bayesian DeciphermentBayesian inference has been widely used in naturallanguage processing (Goldwater and Griffiths, 2007;Blunsom et al2009; Ravi and Knight, 2011b).
It isvery attractive for problems like word substitutionciphers for the following reasons.
First, there areno memory bottlenecks as compared to EM, whichhas an O(N ?
V 2) space complexity.
Second, priorsencourage the model to learn a sparse distribution.The inference is usually performed using Gibbssampling.
For decipherment, a Gibbs sampler keepsdrawing samples from plaintext sequences accord-ing to derivation probability P (d):P (d) = P (e) ?n?iP (ci|ei) (2)In Bayesian inference, P (e) is still given by anngram language model, while the channel probabil-ity is modeled by the Chinese Restaurant Process(CRP):P (ci|ei) =?
?
prior + count(ci, ei)?+ count(ei)(3)Where prior is the base distribution (we set priorto 1C in all our experiments, where C is the numberof word types in ciphertext), and count, also called?cache?, records events that occurred in the history.Each sampling operation involves changing a plain-text token ei, which has V possible choices, whereV is the plaintext vocabulary size, and the final sam-ple is chosen with probability P (d)?Vn=1 P (d).3.2 Slice SamplingWith Gibbs sampling, one has to evaluate all possi-ble plaintext word types (10k?1M) for each sam-ple decision.
This become intractable when the vo-cabulary is large and the ciphertext is long.
Slicesampling (Neal, 2000) can solve this problem by au-tomatically adjusting the number of samples to beconsidered for each sampling operation.Suppose the derivation probability for currentsample is P (current s).
Then slice sampling drawsa sample in two steps:?
Select a threshold T uniformly from the range{0, P (current s)}.?
Draw a new sample new s uniformly from apool of candidates: {new s|P (new s) > T}.From the above two steps, we can see that given athreshold T , we only need to consider those sampleswhose probability is higher than the threshold.
Thiswill lead to a significant reduction on the numberof samples to be considered, if probabilities of themost samples are below T .
In practice, the first stepis easy to implement, while it is difficult to make thesecond step efficient.
An obvious way to collect can-didate samples is to go over all possible samples andrecord those with probabilities higher than T .
How-ever, doing this will not save any time.
Fortunately,for Bayesian decipherment, we are able to completethe second step efficiently.According to Equation 1, the probability of thecurrent sample is given by a language model P (e)and a channel model P (c|e).
The language modelis usually an ngram language model.
Suppose ourcurrent sample current s contains English tokensX , Y , and Z at position i ?
1, i, and i + 1 respec-tively.
Let ci be the cipher token at position i. Toobtain a new sample, we just need to change tokenY to Y ?.
Since the rest of the sample stays the same,we only need to calculate the probability of any tri-gram 1: P (XY ?Z) and the channel model probabil-ity: P (ci|Y ?
), and multiply them together as shownin Equation 4.P (XY ?Z) ?
P (ci|Y ?)
(4)1The probability is given by a bigram language model.268In slice sampling, each sampling operation hastwo steps.
For the first step, we choose a thresh-old T uniformly between 0 and P (XY Z) ?P (ci|Y ).For the second step, there are two cases.First, we notice that two types of Y ?
are morelikely to pass the threshold T : (1) Those that havea very high trigram probability , and (2) those thathave high channel model probability.
To find can-didates that have high trigram probability, we buildsorted lists ranked by P (XY ?Z), which can be pre-computed off-line.
We only keep the top K En-glish words for each of the sorted list.
When thelast item YK in the list satisfies P (XYkZ) ?
prior <T , We draw a sample in the following way: setA = {Y ?|P (XY ?Z) ?
prior > T} and set B ={Y ?|count(ci, Y ?)
> 0}, then we only need to sam-ple Y ?
uniformly from A ?
B until Equation 4 isgreater than T .
2Second, what happens when the last item YK inthe list does not satisfy P (XYkZ) ?
prior < T ?Then we always choose a word Y ?
randomly and ac-cept it as a new sample if Equation 4 is greater thanT .Our algorithm alternates between the two cases.The actual number of choices the algorithm looks atdepends on the K and the total number of possiblechoices.
In different experiments, we find that whenK = 500, the algorithm only looks at 0.06% of allpossible choices.
When K = 2000, this further re-duces to 0.007%.3.3 Deciphering with BigramsSince we can decipher with a bigram languagemodel, we posit that a frequency list of ciphertextbigrams may contain enough information for deci-pherment.
In our letter substitution experiments, wefind that breaking ciphertext into bigrams doesn?thurt decipherment accuracy.
Table 1 shows how fullEnglish sentences in the original data are broken intobigrams and their counts.Instead of doing sampling on full sentences, wetreat each bigram as a full ?sentence?.
There are2It is easy to prove that all other candidates that are not inthe sorted list and with count(ci, Y ?)
= 0 have a upper boundprobability: P (XYkZ) ?
prior.
Therefore, they are ignoredwhen P (XYkZ) ?
prior < T .man they took our land .they took our arable land .took our 2they took 2land .
2man they 1arable land 1Table 1: Converting full sentences to bigramstwo advantages to use bigrams and their counts fordecipherment.First of all, the bigrams and counts are a muchmore compact representation of the original cipher-text with full sentences.
For instance, after breakinga billion tokens from the English Gigaword corpus,we find only 29m bigrams and 58m tokens, whichis only 1/17 of the original text.
In practice, we fur-ther discard all bigrams with low frequency, whichmakes the ciphertext even shorter.Secondly, using bigrams significantly reduces thenumber of sorted lists (from |V |2 to 2|V |) mentionedin the previous section.
The number of lists reducesfrom |V |2 to 2|V | because words in a bigram onlyhave one neighbor.
Therefore, for any word W in abigram, we need only 2|V | lists (?words to the rightof W?
and ?words to the left of W?)
instead of |V |2lists (?pairs of words that surround W?
).3.4 Iterative SamplingAlthough we can directly apply slice sampling ona large number of bigrams, we find that graduallyincluding less frequent bigrams into a sampling pro-cess saves deciphering time ?
we call this iterativesampling:?
Break the ciphertext into bigrams and collecttheir counts?
Keep bigrams whose counts are greater than athreshold ?.
Then initialize the first samplerandomly and use slice sampling to performmaximum likelihood training.
In the end, ex-tract a translation table T according to the finalsample.?
Lower the threshold ?
to include more bi-grams into the sampling process.
Initialize thefirst sample using the translation table obtainedfrom the previous sampling run (for each ci-269pher token f, choose a plaintext token e whoseP (e|f) is the largest).
Perform sampling again.?
Repeat until ?
= 1.3.5 Parallel SamplingInspired by (Newman et al2009), our parallel sam-pling procedure is described below:?
Collect bigrams and their counts from cipher-text and split the bigrams into N parts.?
Run slice sampling on each part for 5 iterationsindependently.?
Combine counts from each part to form a newcount table and run sampling again on each partusing the new table.34 Decipherment ExperimentsIn this section, we evaluate our new sampling algo-rithm in two different experiments.
In the first ex-periment, we compare our method with (Ravi andKnight, 2011b) on their data set to prove correct-ness of our approach.
In the second experiment, wescale up to the whole English Gigaword corpus andachieve a much higher deciphering accuracy.4.1 Deciphering Transtac Corpus4.1.1 DataWe split the Transtac corpus the same way it wassplit in (Ravi and Knight, 2011b).
The data used tocreate ciphertext consists of 1 million tokens, and3397 word types.
The data for language modeltraining contains 2.7 million tokens and 25761 wordtypes.4 The ciphertext is created by replacing eachEnglish word with a cipher word.We use a bigram language model for decipher-ment training.
When the training terminates, a trans-lation table with probability P (c|e) is built based onthe counts collected from the final sample.
For de-coding, we employ a trigram language model usingfull sentences.
We use Moses (Koehn et al2007)3Except for combining the counts to form a new count table,other parameters remain the same.
For instance, each part i hasits own prior set to 1Ci , where Ci is the number of word typesin that part of ciphertext.4In practice, we replaced singletons with a ?UNK?
symbol,leaving around 16904 word types.Method Deciphering AccuracyRavi and Knight 80.0 (with bigram LM)82.5 (with trigram LM)Slice Sampling 88.1 (with bigram LM)Table 2: Decipherment Accuracy on Transtac Corpusfrom (Ravi and Knight, 2011b)Gold Decodedman i?ve come to filea complaint againstsome people .man i?ve come to handa telephone lines somepeople .man they took our land.man they took ourfarm .they took our arableland .they took our slidedoor .okay man .
okay man .eighty donums .
miflih donums .Table 3: Sample Decoding Results on Transtac Corpusfrom (Ravi and Knight, 2011b)to perform the decoding.
We set the distortion limitto 0 and cube the translation probabilities.
Essen-tially, Moses tries to find an English sequence e thatmaximizes P (e) ?
P (c|e)34.1.2 ResultsWe evaluate the performance of our algorithmby decipherment accuracy, which measures the per-centage of correctly deciphered cipher tokens.
Table2 compares the deciphering accuracy with the stateof the art algorithm.Results show that our algorithm improves the de-ciphering accuracy to 88.1%, which amounts to 33%reduction in error rate.
This justifies our claim: do-ing better approximation using slice sampling im-proves decipherment accuracy.Table 3 shows the first 5 decoding results andcompares them with the gold plaintext.
From the ta-ble we can see that the algorithm recovered the ma-jority of the plaintext correctly.4.2 Deciphering Gigaword CorpusTo prove the scalability of our new approach, we ap-ply it to solve a much larger word substitution cipherbuilt from English Gigaword corpus.
The corpuscontains news articles from different news agencies270and has a much larger vocabulary compared with theTranstac corpus.4.2.1 DataWe split the corpus into two parts chronologically.Each part contains approximately 1.2 billion tokens.We uses the first part to build a word substitutioncipher, which is 10k times longer than the one in theprevious experiment, and the second part to build abigram language model.
54.2.2 ResultsWe first use a single machine and apply iterativesampling to solve a 68 million token cipher.
Thenwe use the result from the first step to initialize ourparallel sampling process, which uses as many as100 machines.
For evaluation, we calculate deci-phering accuracy over the first 1000 sentences (33ktokens).After 2000 iterations of the parallel sampling pro-cess, the deciphering accuracy reaches 92.2%.
Fig-ure 2 shows the learning curve of the algorithm.
Itcan be seen from the graph that both token and typeaccuracy increase as more and more data becomesavailable.Figure 2: Learning curve for a very large word substitu-tion cipher: Both token and type accuracy rise as moreand more ciphertext becomes available.5Before building the language model, we replace low fre-quency word types with an ?UNK?
symbol, leaving 129kunique word types.5 Improving Out-of-Domain MachineTranslationDomain specific machine translation (MT) is a chal-lenge for statistical machine translation (SMT) sys-tems trained on parallel corpora.
It is common to seea significant drop in translation quality when trans-lating out-of-domain texts.
Although it is hard tofind parallel corpora for any specific domain, it isrelatively easy to find domain specific monolingualcorpora.
In this section, we show how to use our newdecipherment framework to learn a domain specifictranslation table and use it to improve out-of-domaintranslations.5.1 Baseline SMT SystemWe build a state of the art phrase-based SMT systemusing Moses (Koehn et al2007).
The baseline sys-tem has 3 models: a translation model, a reorderingmodel, and a language model.
The language modelcan be trained on monolingual data, and the rest aretrained on parallel data.
By default, Moses uses thefollowing 8 features to score a candidate translation:?
direct and inverse translation probabilities?
direct and inverse lexical weighting?
phrase penalty?
a language model?
a re-ordering model?
word penaltyEach of the 8 features has its own weight, whichcan be tuned on a held-out set using minimum errorrate training.
(Och, 2003).
In the following sections,we describe how to use decipherment to learn do-main specific translation probabilities, and use thenew features to improve the baseline.5.2 Learning a New Translation Table withDeciphermentFrom a decipherment perspective, machine transla-tion is a much more complex task than solving aword substitution cipher and poses three major chal-lenges:?
Mappings between languages are nondetermin-istic, as words can have multiple translations271?
Re-ordering of words?
Insertion and deletion of wordsFortunately, our decipherment model does not as-sume deterministic mapping and is able to discovermultiple translations.
For the reordering problem,we treat Spanish as a simple word substitution forFrench.
Despite the simplification in the assump-tion, we still expect to learn a useful word-to-wordlexicon via decipherment and use the lexicon to im-prove our baseline.Problem formulation: By ignoring word re-orderings, we can formulate MT decipherment prob-lem as word substitution decipherment.
We viewsource language f as ciphertext and target languagee as plaintext.
Our goal is to decipher f into e andestimate translation probabilities based on the deci-pherment.Probabilistic decipherment: Similar to solvinga word substitution cipher, all we have to do here isto estimate the translation model parameters P?
(f |e)using a large amount of monolingual data in f ande respectively.
According to Equation 5, our objec-tive is to estimate the model parameters so that theprobability of source text P(f) is maximized.argmax?
?eP (e) ?n?iP?
(fi|ei) (5)Building a translation table: Once the samplingprocess completes, we estimate translation probabil-ity P (f |e) from the final sample using maximumlikelihood estimation.
We also decipher from the re-verse direction to estimate P (e|f).
Finally, we builda phrase table by taking translation pairs seen in bothdecipherments.5.3 Combining Phrase TablesWe now have two phrase tables: one learnt from par-allel corpus and one learnt from non-parallel mono-lingual corpus through decipherment.
The phrase ta-ble learnt through decipherment only contains wordto word translations, and each translation optiononly has two scores.
Moses has a function to decodewith multiple phrase tables, so we just need to addthe newly learnt phrase table and specify two moreweights for the scores in it.
During decoding, if asource word only appears in the decipherment table,Train French: 28.5 million tokensSpanish: 26.6 million tokensTune French: 28k tokensSpanish: 26k tokensTest French: 30k tokensSpanish: 28k tokensTable 4: Europarl Training, Tuning, and Testing Datathat table?s translation will be used.
If a source wordexists in both tables, Moses will create two separatedecoding paths and choose the best one after takingother features into account.
If a word is not seen ineither of the tables, it is copied literally to the output.6 MT Experiments and Results6.1 DataIn our MT experiments, we translate French intoSpanish and use the following corpora to learn ourtranslation systems:?
Europarl Corpus (Koehn, 2005): The Europarlparallel corpus is extracted from the proceed-ings of the European Parliament and includesversions in 11 European languages.
The cor-pus contains articles from the political domainand is used to train our baseline system.
Weuse the 6th version of the corpus.
After clean-ing, there are 1.3 million lines left for training.We use the last 2k lines for tuning and testing(1k for each), and the rest for training.
Detailsof training, tuning, and testing data are listed inTable 4.?
EMEA Corpus (Tiedemann, 2009): EMEA isa parallel corpus made out of PDF documentsfrom the European Medicines Agency.
It con-tains articles from the medical domain, whichis a good test bed for out-of-domain tasks.
Weuse the first 2k pairs of sentences for tuningand testing (1k for each), and use the rest (1.1million lines) for decipherment training.
Wesplit the training corpus in ways that no parallelsentences are included in the training set.
Thesplitting methods are listed in Table 5.For decipherment training, we use lexical transla-tion tables learned from the Europarl corpus to ini-272Comparable EMEA :French: Every odd line, 8.7 million tokensSpanish: Every even line, 8.1 million tokensNon-parallel EMEA:French: First 550k sentences, 9.1 million tokensSpanish: Last 550k sentences, 7.7 million to-kensTable 5: EMEA Decipherment Training Datatialize our sampling process.6.2 ResultsBLEU (Papineni et al2002) is used as a standardevaluation metric.
We compare the following 3 sys-tems in our experiments, and present the results inTable 6.?
Baseline: Trained on Europarl?
Decipher-CP: Trained on Europarl + Compa-rable EMEA?
Decipher-NP: Trained on Europarl + Non-Parallel EMEAOur baseline system achieves 38.2 BLEU scoreon Europarl test set.
In the second row of Table6, the test set changes to EMEA, and the baselineBLEU score drops to 24.9.
In the third row, the base-line score rises to 30.5 with a language model builtfrom EMEA corpus.
Although it is much higherthan the previous baseline, we further improve itby including a new phrase table learnt from domainspecific monolingual data.
In a real out-of-domaintask, we are unlikely to have any parallel data totune weights for the new phrase table.
Therefore,we can only set it manually.
In experiments, eachscore in the new phrase table has a weight of 5, andthe BLEU score rises up to 33.2.
In the fourth rowof the table, we assume that there is a small amountof domain specific parallel data for tuning.
Withbetter weights, our baseline BLEU score increasesto 37.3, and our combined systems increase to 41.1and 39.7 respectively.
In the last row of the table, wecompare the combined systems with an even betterbaseline.
This time, the baseline is given half of theEMEA tuning set for training and uses the other halfFrench Spanish P (fr|es) P (es|fr)< < 0.32 1.00he?patique hepa?tico 0.88 0.08hepa?tica 0.76 0.85injectable inyectable 0.91 0.92dl dl 1.00 0.70> > 0.32 1.00ribavirine ribavirina 0.40 1.00olanzapine olanzapina 0.57 1.00clairance aclaramiento 0.99 0.64pellicule?ss recubiertos 1.00 1.00pharmaco-cine?tiquefarmaco-cine?tico 1.00 1.00Table 7: 10 most frequent OOV words in the table learntfrom non-parallel EMEA corpusfor weight tuning.
Results show that our combinedsystems still outperform the baseline.The phrase table learnt from monolingual dataconsists of both observed and unknown words.
Ta-ble 7 shows the top 10 most frequent OOV wordsin the table learnt from non-parallel EMEA corpus.Among the 10 words, 9 have correct translations.
Itis interesting to see that our algorithm finds mul-tiple correct translations for the word ?he?patique?.The only mistake in the table is sensible as Frenchword ?pellicule?s?
is translated into ?recubiertos conpel??cula?
in Spanish.7 Conclusion and Future WorkWe apply slice sampling to Bayesian Deciphermentand show significant improvement in decipheringaccuracy compared with the state of the art algo-rithm.
Our method is not only accurate but alsohighly scalable.
In experiments, we decipher at thescale of the English Gigaword corpus, which con-tains over billions of tokens and hundreds of thou-sands word types.
We further show the value ofour new decipherment algorithm by using it to im-prove out-of-domain translation.
In the future, wewill work with more language pairs, especially thosewith significant word re-orderings.
Moreover, themonolingual corpora used in the experiments are farsmaller than what our algorithm can handle.
We willcontinue to work in scenarios where large amount ofmonolingual data is readily available.273Train Data Tune Data Tune LM Test Data Test LM Baseline Decipher-CPDecipher-NPEuroparl Europarl Europarl Europarl Europarl 38.2Europarl Europarl Europarl EMEA Europarl 24.9Europarl Europarl Europarl EMEA EMEA 30.5 33.2(+2.7)32.4(+1.9)Europarl EMEA EMEA EMEA EMEA 37.3 41.1(+3.8)39.7(+2.4)Europarl +EMEA EMEA EMEA EMEA EMEA 67.468.7(+1.3)68.7(+1.3)Table 6: MT experiment results: The table shows how much the combined systems outperform the baseline system indifferent experiments.
Each row has a different set of training, tuning, and testing data.
Baseline is trained on paralleldata only.
Tune LM and Test LM specify language models used for tuning and testing respectively.
Decipher-CP andDecipher-NP use a phrase table learnt from comparable and non-parallel EMEA corpus respectively.8 AcknowledgmentsThis work was supported by NSF Grant 0904684.The authors would like to thank Philip Koehen,David Chiang, Jason Riesa, Ashish Vaswani, andHui Zhang for their comments and suggestions.ReferencesPhil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP.
Associa-tion for Computational Linguistics.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
Furthermeta-evaluation of machine translation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation.
Association for Computational Linguis-tics.Hal Daume?, III and Jagadeesh Jagarlamudi.
2011.
Do-main adaptation for machine translation by mining un-seen words.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies.
Association for Com-putational Linguistics.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics.
Associationfor Computational Linguistics.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick,and Dan Klein.
2008.
Learning bilingual lexiconsfrom monolingual corpora.
In Proceedings of ACL-08: HLT.
Association for Computational Linguistics.Kevin Knight, Anish Nair, Nishit Rathod, and Kenji Ya-mada.
2006.
Unsupervised analysis for decipher-ment problems.
In Proceedings of the COLING/ACL2006 Main Conference Poster Sessions.
Associationfor Computational Linguistics.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
In Pro-ceedings of the ACL-02 Workshop on UnsupervisedLexical Acquisition.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Interac-tive Poster and Demonstration Sessions.
Associationfor Computational Linguistics.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In In Proceedings of theTenth Machine Translation Summit, Phuket, Thailand.Asia-Pacific Association for Machine Translation.Radford Neal.
2000.
Slice sampling.
Annals of Statis-tics, 31.David Newman, Arthur Asuncion, Padhrai Smyth, andMax Welling.
2009.
Distributed algorithms for topicmodels.
Journal of Machine Learning Research, 10.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Computa-tional Linguistics.
Association for Computational Lin-guistics.274Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics.
Association for Computational Lin-guistics.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd annualmeeting on Association for Computational Linguistics.Association for Computational Linguistics.Sujith Ravi and Kevin Knight.
2008.
Attacking deci-pherment problems optimally with low-order n-grammodels.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing.
Associ-ation for Computational Linguistics.Sujith Ravi and Kevin Knight.
2011a.
Bayesian infer-ence for Zodiac and other homophonic ciphers.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies.
Association for ComputationalLinguistics.Sujith Ravi and Kevin Knight.
2011b.
Deciphering for-eign language.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies.
Association forComputational Linguistics.Jo?rg Tiedemann.
2009.
News from OPUS ?
a collectionof multilingual parallel corpora with tools and inter-faces.
In Recent Advances in Natural Language Pro-cessing V, volume 309 of Current Issues in LinguisticTheory.
John Benjamins.Hua Wu, Haifeng Wang, and Chengqing Zong.
2008.Domain adaptation for statistical machine translationwith domain dictionary and monolingual corpora.
InProceedings of the 22nd International Conference onComputational Linguistics.
Association for Computa-tional Linguistics.275
