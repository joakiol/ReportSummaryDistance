Proceedings of NAACL-HLT 2013, pages 709?714,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsDistributional semantic models for the evaluation of disordered languageMasoud Rouhizadeh?, Emily Prud?hommeaux?, Brian Roark?, Jan van Santen?
?Center for Spoken Language Understanding, Oregon Health & Science University?Center for Language Sciences, University of Rochester{rouhizad,vansantj}@ohsu.edu, {emilypx,roarkbr}@gmail.comAbstractAtypical semantic and pragmatic expression isfrequently reported in the language of childrenwith autism.
Although this atypicality oftenmanifests itself in the use of unusual or un-expected words and phrases, the rate of useof such unexpected words is rarely directlymeasured or quantified.
In this paper, weuse distributional semantic models to automat-ically identify unexpected words in narrativeretellings by children with autism.
The classi-fication of unexpected words is sufficiently ac-curate to distinguish the retellings of childrenwith autism from those with typical develop-ment.
These techniques demonstrate the po-tential of applying automated language anal-ysis techniques to clinically elicited languagedata for diagnostic purposes.1 IntroductionAutism spectrum disorder (ASD) is a neurodevelop-mental disorder characterized by impaired commu-nication and social behavior.
Although the symp-toms of ASD are numerous and varied, atypicaland idiosyncratic language has been one of thecore symptoms observed in verbal individuals withautism since Kanner first assigned a name to thedisorder (Kanner, 1943).
Atypical language cur-rently serves as a diagnostic criterion in many of themost widely used diagnostic instruments for ASD(Lord et al 2002; Rutter et al 2003), and the phe-nomenon is especially marked in the areas of seman-tics and pragmatics (Tager-Flusberg, 2001; Voldenand Lord, 1991).Because structured language assessment tools arenot always sensitive to the particular atypical seman-tic and pragmatic expression associated with ASD,measures of atypical language are often drawn fromspontaneous language samples.
Expert manual an-notation and analysis of spontaneous language inyoung people with ASD has revealed that childrenand young adults with autism include significantlymore bizarre and irrelevant content (Loveland et al1990; Losh and Capps, 2003) in their narratives andmore abrupt topic changes (Lam et al 2012) intheir conversations than their language-matched typ-ically developing peers.
Most normed clinical in-struments for analyzing children?s spontaneous lan-guage, however, focus on syntactic measures anddevelopmental milestones related to the acquisitionof vocabulary and syntactic structures.
Measures ofsemantic and pragmatic atypicality in spontaneouslanguage are rarely directly measured.
Instead, thedegree of language atypicality is often determinedvia subjective parental reports (e.g., asking a par-ent whether their child has ever used odd phrases(Rutter et al 2003)) or general impressions dur-ing clinical examination (e.g., rating the child?s de-gree of ?stereotyped or idiosyncratic use of words orphrases?
on a four-point scale (Lord et al 2002)).This has led to a lack of reliable and objective infor-mation about the frequency of atypical language useand its precise nature in ASD.In this study, we attempt to automatically detectinstances of contextually atypical language in spon-taneous speech at the lexical level in order to quan-tify its prevalence in the ASD population.
We firstdetermine manually the off-topic, surprising, or in-709appropriate words in a set of narrative retellingselicited in a clinical setting from children with ASDand typical development.
We then apply word rank-ing methods and distributional semantic modeling tothese narrative retellings in order to automaticallyidentify these unexpected words.
The results indi-cate not only that children with ASD do in fact pro-duce more semantically unexpected and inappropri-ate words in their narratives than typically develop-ing children but also that our automated methodsfor identifying these words are accurate enough toserve as an adequate substitute for manual annota-tion.
Although unexpected off-topic word use is justone example of the atypical language observed inASD, the work presented here highlights the poten-tial of computational language evaluation and analy-sis methods for improving our understanding of thelinguistic deficits associated with ASD.2 DataParticipants in this study included 37 children withtypical development (TD) and 21 children withautism spectrum disorder (ASD).
ASD was diag-nosed via clinical consensus according to the DSM-IV-TR criteria (American Psychiatric Association,2000) and the established threshold scores on twodiagnostic instruments: the Autism Diagnostic Ob-servation Schedule (ADOS) (Lord et al 2002), asemi-structured series of activities designed to allowan examiner to observe behaviors associated withautism; and the Social Communication Question-naire (SCQ) (Rutter et al 2003), a parental ques-tionnaire.
None of the children in this study metthe criteria for a language impairment, and therewere no significant between-group differences inage (mean=6.4) or full-scale IQ (mean=114).The narrative retelling task analyzed here is theNarrative Memory subtest of the NEPSY (Korkmanet al 1998), a large and comprehensive battery oftasks that test neurocognitive functioning in chil-dren.
The NEPSY Narrative Memory (NNM) sub-test is a narrative retelling test in which the subjectlistens to a brief narrative, excerpts of which areshown in Figure 1, and then must retell the narra-tive to the examiner.
The NNM was administeredto each participant in the study, and each partici-pant?s retelling was recorded and transcribed.
Us-ing Amazon?s Mechanical Turk, we also collecteda large corpus of retellings from neurotypical adults,who listened to a recording of the story and providedwritten retellings.
We describe how this corpus wasused in Section 3, below.Two annotators, blind to the diagnosis of the ex-perimental subjects, identified every word in eachretelling transcript that was unexpected or inappro-priate given the larger context of the story.
For in-stance, in the sentence T-rex could smell things, bothT-rex and smell were marked as unexpected, sincethere is no mention of either concept in the story.
Ina seemingly more appropriate sentence, the boy satup off the bridge, the word bridge is considered un-expected since the boy is trapped up in a tree ratherthan on a bridge.3 MethodsWe start with the expectation that different retellingsof the same source narrative will share a commonvocabulary and semantic space.
The presence ofwords outside of this vocabulary or semantic spacein a retelling may indicate that the speaker hasstrayed from the topic of the story.
Our approach forautomatically identifying these unexpected wordsrelies on the ranking of words according to thestrength of their association with the target topic ofthe corpus.
The word association scores used in theFigure 1: Excerpts from the NNM narrative.Jim was a boy whose best friend was Pepper.
Pepper was a big black dog.
[...] Near Jim?s house was avery tall oak tree with branches so high that he couldn?t reach them.
Jim always wanted to climb that tree,so one day he took a ladder from home and carried it to the oak tree.
He climbed up [...] When he startedto get down, his foot slipped, his shoe fell off, and the ladder fell to the ground.
[...] Pepper sat below thetree and barked.
Suddenly Pepper took Jim?s shoe in his mouth and ran away.
[...] Pepper took the shoe toAnna, Jim?s sister.
He barked and barked.
Finally, Anna understood that Jim was in trouble.
She followedPepper to the tree where Jim was stuck.
Anna put the ladder up and rescued Jim.710ranking are informed by the frequency of a wordin the child?s retelling relative to the frequency ofthat word in other retellings in the larger corpus ofretellings.
These association measures are similarto those developed for the information retrieval taskof topic modeling, in which the goal is to identifytopic-specific words ?
i.e., words that appear fre-quently in only a subset of documents ?
in orderto cluster together documents about a similar topic.Details about how these scores are calculated and in-terpreted are provided in the following sections.The pipeline for determining the set of unusualwords in each retelling begins by calculating wordassociation scores, described below, for each wordin each retelling and ranking the words according tothese scores.
A threshold over these scores is de-termined for each child using leave-one-out crossvalidation in order to select a set of potentially un-expected words.
This set of potential unexpectedwords is then filtered using two external resourcesthat allow us to eliminate words that were not usedin other retellings but are likely to be semanticallyrelated to topic of the narrative.
This final set ofwords is evaluated against the set of manually iden-tified words in order determine the accuracy of ourunexpected word classification.3.1 Word association measuresBefore calculating the word association measures,we tokenize, downcase, and stem (Porter, 1980) thetranscripts and remove all punctuation.
We then usetwo association measures to score each word in eachchild?s retelling: tf-idf, the term frequency-inversedocument frequency measure (Salton and Buckley,1988), and the log odds ratio (van Rijsbergen et al1981).
We use the following formulation to calcu-late tf-idf for each child?s retelling i and each wordin that retelling j, where cij is the count of word jin retelling i; fj is the number of retellings from thefull corpus of child and adult retellings containingthat word j; and D is the total number of retellingsin the full corpus (Manning et al 2008):tf-idfij ={(1 + log cij) log Dfj if cij ?
10 otherwiseThe log odds ratio, another association measureused in information retrieval and extraction tasks, isthe ratio between the odds of a particular word, j,appearing in a child?s retelling, i, as estimated us-ing its relative frequency in that retelling, and theodds of that word appearing in all other retellings,again estimated using its relative frequency in allother retellings.
Letting the probability of a wordappearing in a retelling be p1 and the probability ofthat word appearing in all other retellings be p2, wecan express the odds ratio as follows:odds ratio =odds(p1)odds(p2)=p1/(1?
p1)p2/(1?
p2)A large tf-idf or log odds score indicates that theword j is very specific to the retelling i, which inturn suggests that the word might be unexpected orinappropriate in the larger context of the NNM nar-rative.
Thus we expect that the words with higher as-sociation measure scores are likely to be the wordsthat were manually identified as unexpected in thecontext of the NNM narrative.3.2 Application of word association measuresAs previously mentioned, both of these word associ-ation measures are used in information retrieval (IR)to cluster together documents about a similar targettopic.
In IR, words that appear only in a subset ofdocuments from a large and varied corpus of docu-ments will have high word association scores, andthe documents containing those words will likely befocused on the same topic.
In our task, however,we have a single cluster of documents focused ona single topic: the NNM narrative.
Topic-specificwords ought to occur much more frequently thanother words.
As a result, words with high tf-idf andlog odds scores are likely to be those unrelated tothe topic of the NNM story.
If a child veers awayfrom the topic of the NNM story and uses words thatdo not occur frequently in the retellings producedby neurotypical speakers, his retellings will containmore words with high word association scores.
Wepredict that this set of high-scoring words is likely tooverlap significantly with the set of words identifiedby the manual annotators as unexpected or off-topic.Applying these word association scoring ap-proaches to each word in each child?s retelling yieldsa list of words from each retelling ranked in order ofdecreasing tf-idf or log odds score.
We use cross-validation to determine, for each measure, the op-711erating point that maximizes the unexpected wordidentification accuracy in terms of F-measure.
Foreach child, the threshold is found using the data fromall of the other children.
This threshold is then ap-plied to the ranked word list of the held-out child.All words above this threshold are potential unex-pected words, while all words below this thresholdare considered to be expected and appropriate in thecontext of the NNM narrative.
Table 1 shows therecall, precision, and F-measure using the two wordassociation measures discussed here.
We see thatthese two techniques result in high recall at the ex-pense of precision.
The next stage in the pipeline istherefore to use external resources to eliminate anysemantically appropriate words from the set of po-tentially unexpected or inappropriate words gener-ated via thresholding on the tf-idf or log odds score.3.3 Filtering with external resourcesRecall that the corpus of retellings used to gener-ate the word association measures described above,is very small.
It is therefore quite possible that achild may have used an entirely appropriate wordthat by chance was never used by another child orone of the neurotypical adults.
One way of increas-ing the lexical coverage of the corpus of retellingsis through semantic expansion using an external re-source.
For each word in the set of potential un-expected words, we located the WordNet synset forthat word (Fellbaum, 1998).
If any of the WordNetsynonyms of the potentially unexpected word waspresent in the source narrative or in one of the adultretellings, that word was removed from the set ofunexpected words.In the final step, we used the CHILDES corpusof transcripts of children?s conversational speech(MacWhinney, 2000) to generate topic estimates foreach remaining potentially unexpected word.
Foreach of these words, we located every utterance inthe CHILDES corpus containing that potentially un-expected word.
We then measured the associationof that word with every other open-class word thatappeared in an utterance with that word using thelog likelihood ratio (Dunning, 1993).
The 20 wordsfrom the CHILDES corpus with the highest log like-lihood ratio (i.e., the words most strongly associ-ated with the potentially unexpected word), were as-sumed to collectively represent a particular topic.
Ifmore than two of the words in the vector of wordsrepresenting this topic were also present in the NNMsource narrative or the adult retellings, the word thatgenerated that topic was eliminated from the set ofunexpected words.We note that the optimized threshold describedin Section 3.2, above, is determined after filtering.There is therefore potentially a different thresholdfor each condition tested, and hence we do not nec-essarily expect precision to increase and recall todecrease after filtering.
Rather, since the thresholdis selected in order to optimize F-measure, we ex-pect that if the filtering is effective, F-measure willincrease with each additional filtering condition ap-plied.4 ResultsWe evaluated the performance of our two word rank-ing techniques, both individually and combined bytaking either the maximum of the two measures orthe sum, against the set of manually annotations de-scribed in Section 2.
In addition, we report the re-sults of applying these word ranking techniques incombination with the two filtering techniques.
Wecompare these results with a simple baseline methodin which every word used in a retelling that is neverused in another retelling is considered to be unex-pected.
Table 1 shows the precision, accuracy, andF-measure of these approaches.
We see that all ofthe more sophisticated unexpected word identifica-tion approaches outperform the baseline by a widemargin, and that tf-idf and log odds perform compa-rably under the condition without filtering and bothfiltering conditions.
Filtering improves F-measureunder both word ranking schemes, and combiningthe two measures results in further improvementsunder both filtering conditions.
Although apply-ing topic-estimate filtering yields the highest preci-sion, the simple WordNet-based approach results inthe highest F-measure and a reasonable balance be-tween precision and recall.Recall that the purpose of identifying these un-expected words was to determine whether childrenwith ASD produce unexpected and inappropriatewords at a higher rate than children with typical de-velopment.
This appears to be true in our manu-ally annotated data.
On average, 7.5% of the words712Unexpected word identification method P R F1Baseline 46.3 74.0 57.0TF-IDF 72.1 79.5 75.6Log-odds 70.5 79.5 74.7Sum(TF-IDF, Log-odds) 72.2 83.3 77.4Max(TF-IDF, Log-odds) 69.9 83.3 76.0TF-IDF+WordNet 83.8 80.5 82.1Log-odds+WordNet 82.1 83.1 82.6Sum(TF-IDF, Log-odds)+WordNet 84.2 83.1 83.7Max(TF-IDF, Log-odds)+WordNet 83.3 84.4 83.9TF-IDF+WordNet+topic 85.7 77.9 81.7Log-odds+WordNet+topic 83.8 80.5 82.1Sum(TF-IDF, Log-odds)+WordNet+topic 86.1 80.5 83.2Max(TF-IDF, Log-odds)+WordNet+topic 85.1 81.8 83.4Table 1: Accuracy of unexpected word identification.types produced by children with ASD were markedas unexpected, while only 2.5% of words producedby children with TD were marked as unexpected, asignificant difference (p < 0.01, using a one-tailedt-test).
This significant between-group differencein rate of unexpected word use holds even whenusing the automated methods of unexpected wordidentification, with the best performing unexpectedword identification method estimating a mean of6.6% in the ASD group and 2.5% in the TD group(p < 0.01).5 Conclusions and future workThe automated methods presented here for rank-ing and filtering words according to their distribu-tions in different corpora, which are adapted fromtechniques originally developed for topic modelingin the context of information retrieval and extrac-tion tasks, demonstrate the utility of automated ap-proaches for the analysis of semantics and pragmat-ics.
We were able to use these methods to iden-tify unexpected or inappropriate words with highenough accuracy to replicate the patterns of unex-pected word use manually observed in our two di-agnostic groups.
This work underscores the poten-tial of automated techniques for improving our un-derstanding of the prevalence and diagnostic utilityof linguistic features associated with ASD and othercommunication and language disorders.In future work, we plan to use a development setto determine the optimal number of topical wordsto select during the topic estimate filtering stage ofthe pipeline in order to maintain improvements inprecision without a loss in recall.
We would alsolike to investigate using part-of-speech, word sense,and parse information to improve our approachesfor both semantic expansion and topic estimation.Although the rate of unexpected word use alone isunlikely to provide sufficient power to classify thetwo diagnostic groups investigated here, we expectthat it can serve as one feature in an array of fea-tures that capture the broad range of semantic andpragmatic atypicalities observed in the spoken lan-guage of children with autism.
Finally, we plan toapply these same methods to identify the confabula-tions and topic shifts often observed in the narrativeretellings of the elderly with neurodegenerative con-ditions.AcknowledgmentsThis work was supported in part by NSFGrant #BCS-0826654, and NIH NIDCD grant#1R01DC012033-01.
Any opinions, findings, con-clusions or recommendations expressed in this pub-lication are those of the authors and do not necessar-ily reflect the views of the NSF or the NIH.ReferencesAmerican Psychiatric Association.
2000.
DSM-IV-TR:Diagnostic and Statistical Manual of Mental Disor-ders.
American Psychiatric Publishing, Washington,DC.Ted Dunning.
1993.
Accurate methods for the statisticsof surprise and coincidence.
Computational linguis-tics, 19(1):61?74.713Christian Fellbaum.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.Leo Kanner.
1943.
Autistic disturbances of affectivecontent.
Nervous Child, 2:217?250.Marit Korkman, Ursula Kirk, and Sally Kemp.
1998.NEPSY: A developmental neuropsychological assess-ment.
The Psychological Corporation, San Antonio.Yan Grace Lam, Siu Sze, and Susanna Yeung.
2012.Towards a convergent account of pragmatic languagedeficits in children with high-functioning autism: De-picting the phenotype using the pragmatic rating scale.Research in Autism Spectrum Disorders, 6:792797.Catherine Lord, Michael Rutter, Pamela DiLavore, andSusan Risi.
2002.
Autism Diagnostic ObservationSchedule (ADOS).
Western Psychological Services,Los Angeles.Molly Losh and Lisa Capps.
2003.
Narrative abilityin high-functioning children with autism or asperger?ssyndrome.
Journal of Autism and Developmental Dis-orders, 33(3):239?251.Katherine Loveland, Robin McEvoy, and Belgin Tunali.1990.
Narrative story telling in autism and down?ssyndrome.
British Journal of Developmental Psychol-ogy, 8(1):9?23.Brian MacWhinney.
2000.
The CHILDES project: Toolsfor analyzing talk.
Lawrence Erlbaum Associates,Mahwah, NJ.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to information re-trieval.
Cambridge University Press.M.F.
Porter.
1980.
An algorithm for suffix stripping.Program, 14(3):130?137.Michael Rutter, Anthony Bailey, and Catherine Lord.2003.
Social Communication Questionnaire (SCQ).Western Psychological Services, Los Angeles.Gerard Salton and Christopher Buckley.
1988.
Term-weighting approaches in automatic text retrieval.
In-formation Processing and Management, 24(5):513?523.Helen Tager-Flusberg.
2001.
Understanding the lan-guage and communicative impairments in autism.
In-ternational Review of Research in Mental Retardation,23:185?205.C.J.
van Rijsbergen, D.J.
Harper, and M.F.
Porter.
1981.The selection of good search terms.
Information Pro-cessing and Management, 17(2):77?91.Joanne Volden and Catherine Lord.
1991.
Neologismsand idiosyncratic language in autistic speakers.
Jour-nal of Autism and Developmental Disorders, 21:109?130.714
