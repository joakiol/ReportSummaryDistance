Response Generation in Collaborative Negotiation*J enn i fe r  Chu-Car ro l l  and Sandra  Carber ryDepartment of  Computer  and Information SciencesUniversity of  DelawareNewark, DE 19716, USAE-marl: {jchu,carberry} @cis.udel.eduAbst rac tIn collaborative planning activities, since theagents are autonomous and heterogeneous, itis inevitable that conflicts arise in their beliefsduring the planning process.
In cases wheresuch conflicts are relevant to the t~t~k at hand,the agents hould engage in collaborative ne-gotiation as an attempt to square away the dis-crepancies in their beliefs.
This paper presentsa computational strategy for detecting conflictsregarding proposed beliefs and for engagingin collaborative n gotiation to resolve the con-flicts that warrant resolution.
Our model iscapable of selecting the most effective aspectto address in its pursuit of conflict resolution icases where multiple conflicts arise, and of se-lecting appropriate evidence to justify the needfor such modification.
Furthermore, by cap-turing the negotiation process in a recursivePropose-Evaluate.Modify cycle of actions, ourmodel can successfully handle embedded ne-gotiation subdialogues.1 In t roduct ionIn collaborative consultation dialogues, the consultantand the executing agent collaborate on developing a planto achieve the executing agent's domain goal.
Sinceagents are autonomous and heterogeneous, it isinevitablethat conflicts in their beliefs arise during the planning pro-cess.
In such cases, collaborative agents hould attemptto square away (Joshi, 1982) the conflicts by engaging incollaborative negotiation to determine what should con-stitute their shared plan of actions and shared beliefs.Collaborative negotiation differs from non-collaborativenegotiation and argum_entation mainly in the attitude ofthe participants, since collaborative agents are not self-centered, but act in a way as to benefit he agents asThis material isbased upon work supported by the NationalScience Foundation under Grant No.
IRI-9122026.a group.
Thus, when facing a conflict, a collaborativeagent should not automatically reject a belief with whichshe does not agree; instead, she should evaluate the beliefand the evidence provided to her and adopt the belief if theevidence isconvincing.
On the other hand, if the evalua-tion indicates that the agent should maintain her originalbelief, she should attempt to provide sufficient justifica-tion to convince the other agent o adopt his belief if thebelief is relevant to the task at hand.This paper presents a model for engaging in collabo-rative negoa~ion to resolve conflicts in agents' beliefsabout domain knowledge.
Our model 1) detects con-flicts in beliefs and initiates a negotiation subdialogueonly when the conflict is relevant to the current a.~k, 2)selects the most effective aspect to address in its pursuitof conflict resolution when multiple conflicts exist, 3)selects appropriate evidence to justify the system's pro-posed modification of the user's beliefs, and 4) capturesthe negotiation process in a recursive Propose-Evaluate-Mod/fy cycle of actions, thus enabling the system to han-dle embedded negotiation sulxlialognes.2 Related WorkResearchers have studied the analysis and generation ofarguments (Birnbaum et al, 1980; Reichman, 1981; Co-hen, 1987; Sycara, 1989; Quilici, 1992; Maybury, 1993);however, agents engaging in argumentative dialogues aresolely interested in winning an argument and thus ex-hibit different behavior f om collaborative agents.
Sidner(1992; 1994) formulated an artificial anguage for mod-eling collaborative discourse using propo~acceptanceand proposal/rejection sequences; however, her workis descriptive and does not specify response generationstrategies for agents involved in collaborative interac-tions.Webber and Joshi (1982) have noted the importance ofa cooperative system providing support for its responses.They identified strategies that a system can adopt in justi-fying its beliefs; however, they did not specify the criteriaunder which each of these strategies should be selected.136Walker (1994) described amethod of determining whento include optional warrants to justify a claim based onfactors uch as communication cost, inference cost, andcost of memory retrieval.
However, her model focuses ondetermining when to include informationally redundantutterances, whereas our model determines whether or notjustification is needed for a claim to be convincing and, ffso, selects appropriate evidence from the system's privatebeliefs to support he claim.Caswey et al (Cawsey et al, 1993; Logan et al,1994) introduced the idea of utilizing a belief revisionmechanism (Galliers, 1992) to predict whether a set ofevidence is sufficient o change a user's existing beliefand to generate responses for information retrieval di-alogues in a library domain.
They argued that in thelibrary dialogues they analyzed, "in no cases does ne-gotiation extend beyond the initial belief conflict and itsimmediate r solution:' (Logan et al, 1994, page 141).However, our analysis of naturally-occurring consultationdialogues (Columbia University Transcripts, 1985; SRITranscripts, 1992) shows that in other domains conflictresolution does extend beyond asingle exchange of con-flicting befiefs; therefore we employ a re, cursive modelfor collaboration that captures extended negotiation andrepresents he structure of the discourse.
Furthermore,their system deals with a single conflict, while our modelselects a focus in its pursuit of conflict resolution whenmultiple conflicts arise.
In addition, we provide aprocessfor selecting among multiple possible pieces of evidence.3 Features  of  Col laborat ive Negot iat ionCollaborative negoti~ion occurs when conflicts ariseamong agents developing a shared plan 1 during collab-orative planning.
A collaborative agent is driven by thegoal of developing a plan that best satisfies the interests ofall the agents as a group, instead of one that maximizes hisown interest.
This results in several distinctive f atures ofcollaborative negotiation: 1) A collaborative agent doesnot insist on winning an argument, and may change hisbeliefs ff another agent presents convincing justificationfor an opposing belief.
This differentiates collaborativenegotiation from argumentation (Birnbaum et al, 1980;Reichman, 1981; Cohen, 1987; Quilici, 1992).
2) Agentsinvolved in collaborative negotiation are open and hon-est with one another; they will not deliberately presentfalse information toother agents, present information isuch a way as to mislead the other agents, or strategi-cally hold back information from other agents for lateruse.
This distinguishes collaborative negotiation fromnon-collaborative n gotiation such as labor negotiation(Sycara, 1989).
3) Collaborative agents are interested in1The notion of shared plan has been used in (Grosz andSidner, 1990; Allen, 1991).others' beliefs in order to decide whether to revise theirown beliefs so as to come to agreement (Chu-Carroll andCarberry, 1995).
Although agents involvedin argumenta-tion and non-collaborative negotiation take other agents'beliefs into consideration, they do so mainly to find weakpoints in their opponents' beliefs and attack them to winthe argument.In our earlier work, we built on Sidner's pro-posal/acceptance and proposal/rejection sequences (Sit-net, 1994) and developed a model tha?
captures collabo-rative planning processes in a Propose-Evaluate-Modifycycle of actions (Chu-Carroll and Carberry, 1994).
Thismodel views co l l~t ive  planning as agent A propos-ing a set of actions and beliefs to be i~ted  into theplan being developed, agent B evaluating the pro-posal to determine whether or not he accepts the proposaland, ff not, agent B proposing aset of modifications toA'soriginal proposal.
The proposed modifications will againbe evaluated by A, and if conflicts arise, she may proposemodifications to B's previously proposed modifications,resulting in a recursive process.
However, our researchdid not specify, in cases where multiple conflicts arise,how an agent should identify which pm of an unaccept~proposal to address or how to select evidence to supportthe proposed modification.
This paper extends that workby i~t ing  into the modification process aslrategyto determine the aspect of the proposal that he agent willaddress in her pursuit of conflict resolution, as well asa means of selecting appropriate evidence to justify theneed for such modification.4 Response Generation in CollaborativeNegotiationIn order to capture the agents' intentions conveyed bytheir utterances, our model of collaborative negotiationutilizes an enhanced version of the dialogue model de-scribed in (Lambert and Carberry, 1991) to representthe current status of the interaction.
The enhanced di-alogue model has four levels: the domain level whichconsists of the domain plan being constructed for theuser's later execution, the problem-solving level whichcontains the actions being performed toconstruct the do-n~n plan, the belief level which consists of the mutualbeliefs pursued uring the planning process in order tofurther the problem-solving intentions, and the discourselevel which contains the communicative actions initiatedto achieve the mutual beliefs (Chu-Carroll and Carberry,1994).
This paper focuses on the evaluation and mod-ification of proposed beliefs, and details a strategy forengaging incollaborative negotiations.1374.1 Evaluating Proposed BeliefsOur system maintains a set of beliefs about he domainand about the user's beliefs.
Associated with each be-lief is a strength that represents he agent's confidencein holding that belief.
We model the strength of a beliefusing endorsements, which are explicit records of factorsthat affect one's certainty in a hypothesis (Cohen, 1985),following (Galliers, 1992; Logan et al, 1994).
Our en-dorsements are based on the semantics of the utteranceused to convey abefief, the level of expertise of the agentconveying the belief, stereotypical knowledge, tc.The belief level of the dialogue model consists of mu-tual beliefs proposed by the agents' discourse actions.When an agent proposes a new belief and gives (optional)supporting evidence for it, this set of proposed beliefs isrepresented asa belief tree, where the belief representedby a child node is intended to support that represented byits parent.
The root nodes of these belief trees (rap-levelbeliefs) contribute to problem-solving actions and thusaffect he domain plan being developed.
Given a set ofnewly proposed beliefs, the system must decide whetherto accept the proposal or m initiate anegotiation dialogueto resolve conflicts.
The evaluation of proposed beliefsstarts at the leaf nodes of the proposed belief trees sinceacceptance of a piece of proposed evidence may affect ac-ceptance of the parent belief it is intended to support.
Theprocess continues until the top-level proposed beliefs areevaluated.
Conflict resolution strategies are invoked onlyif the top-level proposed beliefs are not accepted becauseif collaborative agents agree on a belief relevant to thedomain plan being constructed, it is irrelevant whetherthey agree on the evidence for that belief (Young et al,1994).In determining whether to accept a proposed befiefor evidential relationship, the evaluator first constructsan evidence set containing the system's evidence thinsupports or attacks _bcl and the evidence accepted bythe system that was proposed by the user as support for-bel.
Each piece of evidence contains abelief _beli, andan evidential relationship supports(.beli,-bel).
Follow-ing Walker's weakest link assumption (Walker, 1992) thestrength of the evidence is the weaker of the strength ofthe belief and the strength of the evidential relationship.The evaluator then employs a simplified version of Gal-liers' belief revision mechanism 2 (Galliers, 1992; Loganet al, 1994) to compare the strengths of the evidence thatsupports and attacks _bel.
If the strength of one set of evi-dence strongly outweighs that of the other, the decision toaccept or reject.bel iseasily made.
However, if the differ-ence in their strengths does not exceed apre-determined2For details on how our model determines the acceptance ofa belief using the ranking of endorsements proposed by GaUiers,see (Chu-Carroll, 1995)...v.~ ..e~......n.~q.h..x~ ...........................
.,~."
-~ MB~3tSt-Teaches(Smith~I)) \]a ; 1 ~q.
, i\[MB~J,S,O.-S~,~KS,~th,n~,a ) ) ~, - .
.
-.Dlsc~rse Level ", i ......... : ............................................................... ".
"d"" "\[ lnf~J,S,~Teache~(Smi~ I i ,',\[Tell('O,S,-Teaches(Smith,AI))\] \[Address-Acceplance ~i ~'\[ I~?
'm(U,S,O"-S~ic~(Smith,~= Ye'O) k~"\[ TeU(U,S,On-S~,t,~(Smith,~xt y~0) I,.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
JDr.
Smith is not teaching ALDr.
Smith is going on sablmutical next year.Figure 1: Belief and Discourse Levels for (2) and (3)threshold, the evaluator has insufficient information todetermine whether to adopt _bel and therefore will ini-tiate an information-sharing subdialogue (Cho-Carmlland Carberry, 1995) to share information with the userso that each of them can knowiedgably re-evaluate heuser's original proposal.
If, during infommtion-sharing,the user provides convincing support for a belief whosenegation is held by the system, the system may adopt hebelief after the re-evaluation process, thus resolving theconflict without negotiation.4.1.1 ExampleTo illustrate the evaluation of proposed beliefs, con-sider the following uttermmes:(1) S: 1 think Dr. Smith is teaching AI  nextsemester.
(2) U: Dr. Smith is not teaching AL(3) He is going on sabbatical next year.Figure 1 shows the belief and discourse levels ofthe dialogue model that captures utterances (2) and(3).
The belief evaluation process will start withthe belief at the leaf node of the proposed belieftxee, On.Sabbatical(Smith, next year)).
The systemwill first gather its evidence pe~aining to the belief,which includes I) a warranted belief ~ that Dr. Smithhas postponed his sabbatical until 1997 (Postponed-Sabbatical(Smith, J997)), 2) a warranted belief thatDr.
Smith postponing his sabbatical until 1997 sup-ports the belief that he is not going on sabbaticalnext year (supports(Postponed-Sabbatical(Smith,1997),-~On-SabbaticaI(Smith, next year)), 3) a strong beliefthat Dr. Smith will not be a visitor at IBM next year(-~visitor(Smith, IBM, next year)), and 4) a warrantedbelief that Dr. Smith not being a visitor at IBM nextaThe strength of a belief is classified as: warranted, strong,or weak, based on the endorsement of the belief.138year supports the belief that he is not going on sab-batical next year (supports(-~visitor(Smith, IBM, nextyear), -,On-Sabbatical(Smith, next year)), perhaps be-cause Dr. Smith has expressed his desire to spend his sab-batical only at IBM).
The belief revision mechanism willthen be invoked to determine the system's belief aboutOn-Sabbatical(Smith, next year) based on the system'sown evidence and the user's statement.
Since beliefs (1)and (2) above constitute a warranted piece of evidenceagainst the proposed belief and beliefs (3) and (4) consti-tute a strong piece of evidence against it, the system willnot accept On-Sabbatical(Smith, next year).The system believes that being on sabbatical implies afaculty member is not teaching any courses; thus the pro-posed evidential relationship will be accepted.
However,the system will not accept he top-level proposed belief,-,Teaches(Smith, A/), since the system has a prior beliefto the contrary (as expressed inutterance ( 1)) and the onlyevidence provided by the user was an implication whoseantecedent was not accepted.4.2 Modifying Unaccepted ProposalsThe collaborative planning principle in (Whittak~ andStenton, 1988; Walker, 1992) suggests that "conversantsmust provide vidence of a detected discrepancy in beliefas soon as possible."
Thus, once an agent detects a rele-vant conflict, she must notify the other agent of the con-flict and initiate a negotiation subdialogne toresolve i t - -to do otherwise is to fail in her responsibility as a collab-orative agent.
We capture the attempt to resolve a con-flict with the problem-solving action Modify-Proposal,whose goal is to modify the proposal to a form that willpotentially be accepted by both agents.
When applied tobelief modification, Modify-Proposal has two specializa-tions: Correct-Node, for when a proposed belief is notaccepted, and Correct-Relation, for when a proposed ev-idential relationship is not accepted.
Figure 2 shows theproblem-solving recipes 4 for Correct-Node and its subac-tion, Modify-Node, that is responsible for the actual mod-ification of the proposal.
The applicability conditions 5 ofCorrect-Node specify that the action can only be invokedwhen _sl believes that _node is not acceptable while _s2believes that it is (when _sl and _s2 disagree about heproposed belief represented by ..node).
However, sincethis is a collaborative interaction, the actual modificationcan only be performed when both ..sl and _s2 believe that_node is not acceptable w that is, the conflict between_sl and .s2 must have been resolved.
This is captured by4A recipe (Pollack, 1986) is a template for performing ac-tions.
It contains the applicabifity conditions for performing anaction, the subactions comprising the body of an action, etc.SApplicabflity conditions are conditions that must alreadybe satisfied in order for an action to be reasonable to pursue,whereas an agent can try to achieve unsatisfied preconditions.Action:~y~:Appl Cond:Const:Body:Goal:Action:~ype:Appi Cond:Precond:Body:Goal:Figure 2:Correct-Node(_s I, .s2, .propow, d)Decompositionbelieve(_s 1,--acceptable(..node))believe(_s2, acceptable(_node))error-in-plan(_node,..proposed)Modify-Node(..s l,_s2,_proposed,..node)Insert-Correction(.s 1, ..s2, _proposed)accoptable(_proposed)Modify-Node(..s I ,..s2,.4noposed,.suxle)Specializationbelieve( .s1, .-,acceptable( ...node ) )believe(.s2,-,acceptable(_node))Remove-Node(_sl,_s2,_proposed,..node)Alter-Node(.s l,_s2,.proposed,.node)mod~ed(.proposed)The Correct-Node and Modify-Node Recipesthe applicability condition and precondition of Mod/fy-Node.
~ attempt to satisfy the precondition causes thesystem to post as a mutual belief to be achieved the beliefthat ..node is not acceptable, leading the system to adoptdiscourse actions to change _s2's beliefs, thus initiating acollaborative negotiation subdialogne, e4.2,1 Selecting the Focus of ModificationWhen multiple conflicts arise between the system andthe user regarding the user's proposal, the system mustidentify the aspect of the proposal on which it should fo-cus in its pursuit of conflict resolution.
For example, inthe case where Correct-Node isselected as the specializa-tion of Modify-Proposal, the system must determine howthe parameter node in Correct-Node should be instanti-ated.
The goal of the modification process is to resolvethe agents' conflicts regarding the unaccepted top-levelproposed beliefs.
For each such belief, the system couldprovide vidence against the befief itself, address the un-accepted evidence proposed by the user to eliminate theuser's justification for the belief, or both.
Since collab-orative agents are expected to engage in effective andefficient dialogues, the system should address the unac-cepted belief that it predicts will most quickly resolvethe top-level conflict.
Therefore, for each unacceptedtop-level belief, our process for selecting the focus ofmodificatkm involves two steps: identifying acandidatefoci tree from the proposed belief tree, and selecting aeThis subdialogue is considered an interrupt by Whittaker,Stenton, and Walker (Whittaker and Stenton, 1988; Walker andWhittaker, 1990), initiated to negotiate he truth of a piece of in-formation.
However, the utterances they classify as interruptsinclude not only our negotiation subdialogues, generated forthe purpose of modifying a proposal, but also clarification sub-dialogues, and information-sharing subdialogues (Chu-Carrolland Carberry, 1995), which we contend should be part of theevaluation process.139focus from the candidate foci tree using the heuristic "at-tack the belief(s) that will most likely resolve the conflictabout he top-level belief."
A candidate loci tree containsthe pieces of evidence in a proposed belief tree which, ifdisbelieved by the user, might change the user's view ofthe unaccepted top-level proposed belief (the root nodeof that belief tree).
It is identified by performing a depth-first search on the proposed belief tree.
When a nodeis visited, both the belief and the evidential relationshipbetween it and its parent are examined.
If  both the be-lief and relationship were accepted by the evaluator, thesearch on the current branch will terminate, since once thesystem accepts abelief, it is irrelevant whether it acceptsthe user's support for that belief (Young et al, 1994).Otherwise, this piece of evidence will be included in thecandidate loci tree and the system will continue to searchthrough the evidence in the belief tree proposed as supportfor the unaccepted belief and/or evidential relationship.Once a candidate foci tree is identified, the systemshould select he focus of modification based on the like-lihood of each choice changing the user's belief aboutthe top-level belief.
Figure 3 shows our algorithm forthis selection process.
Given an unaccept~ belief (.bel)and the beliefs proposed to support it, Select-Focus.Modification will annotate_bel with 1) its focus of mod-ification (.bel.focus), which contains aset of beliefs (.beland/or its descendents) which, if disbelieved by the user,are predicted to cause him to disbelieve _bel, and 2) thesystem's evidence against_bel itself (_hel.s-attack).Select-Focus-Modification determines whether to at-tack _bel's supporting evidence separately, thereby elim-inating the user's reasons for holding ..b?l, to at ta~ ..belitself, or both.
However, in evainating the effectiveness ofattacking the proposed evidence for.bel, the system mustdetermine whether or not it is possible to successfully re-fute a piece of evidence (i.e., whether or not the systembelieves that sufficient evidence is available to convincethe user that a piece of proposed evidence is invalid), andif so, whether it is mote effective to attack the evidence it-self or its support.
Thus the algorithm recursively appliesitself to the evidence proposed as support for _bel whichwas not accepted by the system (step 3).
In this recursiveprocess, the algorithm annotates each unaccepted beliefor evidential relationship roposed to support _bel withits focus of modification (-beli.focus) and the system'sevidence against it (_beli.s-attack).
_bell.focus containsthe beliefs selected to be addressed in order to change theuser's belief about ..beli, and its value will be nil if thesystem predicts that insufficient evidence is available tochange the user's belief about -bell.Based on the information obtained in step 3, Select.Focus-Modification decides whether to attack the evi-dence proposed to support _bel, or _bel itself (step 4).Its preference is to address the unaccepted evidence, be-Select  .Focus-Modlf lcat lon(_bel) :1.
_bel.u-evid +-- system's beliefs about he user's evidencepertaining to _bel_bel.s-attack 4- system's own evidence against _bel2.
If _bel is a leaf node in the candidate foci tree,2.1 If Predict(_bel, _bel.u-evid + _bel.s-attack) = -~_belthen _bel.focus ,-- .bel; return2.2 Else .bel.focus t -  nil; return3.
Select focus for each of .bel's children in the candidatefoci tree, .belx ..... ..bel,~:3.1 If supports(_beli,_bel) is accepted but .beli is not,Select-Focus-Modlficatioa(.bel~ ).3.2 Else if .beli is accepted but supports(_beli,.bel) isnot, Sdect-Focus-Modlficatlon(.beli,.bel).3.3 Else Select-Focu-Modificatioa(.bel~) and Select-Focus-Modification( supports(_beli ,.bel))4.
Choose between attacking the Woposed evidence for .beland attacking ..bel itself:4.1 eand-set ~-- {..beli I.beli E unaccepted user evidencefor _bel A ..beli.focus ~ nil}4.2 //Check if addressing _bol's unaccepted evidence issuffu:ientIf Predkt(.bel, _bel.u-evid - cand-set) = --,.~l (i.e.,the user's disbelief in all unaecepted vidence which.
the system can refute will cause him to reject _bel),min-set ~- Select-Mtu-Set(_bel,cand-set)..bel.focus ~- U_bel~ ?_min-set ..beli.focus4.3 //Check if addressing .bel itself is s~fcientElse if Predlct(.bel, ..bel.u-evid + .bel.s-attack) =-,.bel (i.e., the system's evidence against .bel willcause the user to reject _bel),.bel.focus ~-- .bel4.4 //Check if addressing both .l~el and its unacceptedevidence is s~OfcientElse if Predkt(..bel, _bel.s-attaek + .bel.u-evid -canal-set) =-,_bet,rain-set +-- Select-Mln-Set(.beL cand-set +_bel).bel.focus +-- U.beli~dnin-set ..beli.focus U .bel4.5 Else _bel.focus +-- nilFigure 3: Selecting the Focus of Modificationcause McKeown's focusing rules suggest that continuinga newly introduced topic (about which there is more to besaid) is preferable to returning to a previous topic OVIcK-cown, 1985).
Thus the algorithm first considers whetheror not attacking the user's upport for ..bel is sufficient toconvince him of--,-bel (step 4.2).
It does so by gathering(in cand-set) evidence proposed by the user as direct sup-port for _bel but which was not accepted by the systemand which the system predicts it can successfully refute(i.e., =beli.focus i not nil).
The algorithm then hypothe-sizes that the user has changed his mind about each beliefin cand-set and predicts how this will affect the user'sbelief about .bel (step 4.2).
I f  the user is predicted to ac-cept --,..bel under this hypothesis, the algorithm invokesSelect-Min-Set to select a minimum subset of cand-set asthe unaccepted beliefs that it would actually pursue, andthe focus of modification (..bel.focus) will be the union of140the focus for each of the beliefs in this minimum subset.If attacking the evidence for _bel does not appear tobe sufficient to convince the user of -~_bel, the algorithmchecks whether directly attacking _bel will accomplishthis goal.
If providing evidence directly against _bel ispredicted to be successful, then the focus of modifica-tion is _bcl itself (step 4.3).
If directly attacking _belis also predicted to fail, the algorithm considers the ef-fect of attacking both ..bel and its unaccepted proposedevidence by combining the previous two prediction pro-cesses (step 4.4).
If the combined evidence is still pre-dicted to fail, the system does not have sufficient evidenceto change the user's view of_bel; thus, the focus of mod-ification for .bel is nil (step 4.5).
7 Notice that steps 2 and4 of the algorithm invoke a function, Predict, that makesuse of the belief revision mechanism (Galliers, 1992) dis-cussed in Section 4.1 to predict he user's acceptance orunacceptance of..bel based on the system's knowledge ofthe user's beliefs and the evidence that could be presentedto him (Logan et al, 1994).
The result of Select-Focus-Modification is a set of user beliefs (in _bel.focus) thatneed to be modified in order to change the user's beliefabout he unaccepted top-level belief.
Thus, the negationsof these beliefs will be posted by the system as mutualbeliefs to be achieved in order to perform the Mod/fyactions.4.2.2 Selecting Justification for a ClaimStudies in communication a d social psychology haveshown that evidence improves the persuasiveness of amessage (Luchok and McCroskey, 1978; Reynolds andBurgoon, 1983; Petty and Cacioppo, 1984; Hampie,1985).
Research on the quantity of evidence indicatesthat there is no optimal amount of evidence, but that theuse of high-quality evidence is consistent with persua-sive effects (Reinard, 1988).
On the other hand, Cn'ice'smaxim of quantity (Grice, 1975) specifies that one shouldnot contribute more information than is required, s Thus,it is important that a collaborative agent selects uffmientand effective, but not excessive, evidence to justify anintended mutual belief.To convince the user ofa belief,_bel, our system selectsappropriate justification by identifying beliefs that could7In collaborative dialogues, an agent should reject a pro-posal only ff she has strong evidence against i .
When an agentdoes not have sufficient information to determine the accep-tance of a proposal, she should initiate an information-sharingsubdialogue to share information with the other agent and re-evaluate the proposal (Chu-Carroll and Carberry, 1995).
Thus,further esearch isneeded to determine whether or not the focusof modification for a rejected belief will ever be nil in collabo-rative dialogues.sWalker (1994) has shown the importance of IRU's Odor-mationally Redundant Utterances) in efficient discourse.
Weleave including appropriate IRU's for future work.be used to support_bel and applying filtering heuristics tothem.
The system must first determine wbether justifica-tion for_bel is needed by predicting whether or not merelyinforming the user of _bel will be sufficient to convincehim of _bel.
If so, no justification will be presented.
Ifjustification is predicted to be necessary, the system willfirst construct the justification chains that could be usedto support _bel.
For each piece of evidence t~t  couldbe used to directly support ..bel, the system first predictswhether the user will accept he evidence without justi-fication.
If the user is predicted not to accept apiece ofevidence (evidi), the system will augment the evidence tobe presented tothe user by posting evidi as a mutual be-lief to be achieved, and selecting propositions that couldserve as justification for it.
This results in a recursiveprocess that returns a chain of belief justifications thatcould be used to support.bel.Once a set of beliefs forming justification chains isidentified, the system must then select from this set thosebelief chains which, when presented to the user, are pre-dicted to convince the user of .bel.
Our system will firstconstruct a singleton set for each such justification chainand select he sets containing justification which, whenpresented, is predicted to convince the user of _bel.
Ifno single justification chain is predicted to be sufficientto change the nser's beliefs, new sets will be constructedby combining the single justification chains, and the se-lection ~ is repeated.
This will produce a set ofpossible candidate justification chains, and three heuris-tics will then be applied to select from among them.
Thefirst heuristic prefers evidence inwhich the system is mostconfident since high-quality evidence produces more at-titude change than any other evidence form (Luchok andMcCroskey, 1978).
Furthermore, the system can betterjustify a belief in which it has high confidence should theuser not accept it.
The second heuristic prefers evidencethat is novel to the user, since studies have shown that ev-idence is most persuasive ff it is previously unknown tothe hearer (Wyer, 1970; Morley, 1987).
The third heuris-tic is based on C.nice's maxim of quantity and prefersjustification chains that contain the fewest beliefs.4.2.3 ExampleAfter the evaluation of the di~ogue model in Figure 1,Modify-Proposal is invoked because the top-level pro-posed belief is not accepted.
In selecting the focus ofmodification, the system will first identify the candidatefoci tree and then invoke the Select-Focus-Modificationalgorithm on the belief at the root node of the candidatefoci tree.
The candidate foci tree will be identical to theproposed belief tree in Figure 1 since both the top-levelproposed belief and its proposed evidence were rejectedduring the evaluation process.
This indicates that the fo-cus of modification could be either -~Teaches(Smith,AI)141or On-Sabbatical(Smith, next year) (since the evidentialrelationship between them was accepted).
When Select-Focus-Modification is applied to --,Teaches(Smith,Al),the algorithm will first be recursively invoked on On-Sabbatical(Smith, next year) to determine the focus formodifying the child belief (step 3.1 in Figure 3).
Sincethe system has two pieces of evidence against On-Sabbatical(Smith, next year), 1) a warranted piece ofevidence containing Postponed-Sabbatical(Smittg1997)and supports( Postponed-Sabbatical(Smith,1997),-,On-Sabbatical(Smith, next year)), and 2) a strongpiece of evidence containing --,visitor(Smith, IBM, nextyear) and supports(-,visitor(Smith, IBM next year),-,On-Sabbatical(Smith, next year)), the evidence is pre-dicted to be sufficient to change the user's be-lief in On-Sabbatical(Smith, next year), and hence-,Teaches(Smith, A1); thus, the focus of modification willbe On-Sabbatical(Smith, next year).
The Correct-Nodespecialization of Modify-Proposal will be invoked sincethe focus of modification is a belief, and in order to sat-isfy the precondition f Modify.Node (Figure 2), MB( S, U,-~ On-Sabbatical(Smith, next year)) will be posted as a mu-tual belief to be achieved.Since the user has a warranted belief in On-Sabbatical(Smith, next year) ('indicated by the seman-tic form of utterance (3)), the system will predict h~merely informing the user of the intended mutual beliefis not sufficient to change his belief; therefore R willselect justificatkm from the two available pieces of evi-dence supporting -,On.Sabbatical(Smith, next year) pre-sented earlier.
The system will predict hat either pieceof evidence combined with the proposed mutual beliefis sufficient to change the user's belief; thus, the filter-ing heuristics are applied.
The first heuristic will causethe system to select Postponed.Sabbatical(Smith, 1997)and supports(Postponed-Sabbatical(Smith, 1997),-,On-Sabbatical(Smith, next year)) as support, since it is theevidence in which the system is more confident.The system will try to establish the mutual beliefs 9asan attempt to satisfy the precondition of Modify-Node.This will cause the system to invoke Inform cKscourseactions to generate he following utterances:(4) S: Dr. Smith is not going on sabbatical nextyear.
(5) He postponed his sabbatical until 199ZIf the user accepts the system's utterances, thus satisfy-ing the precondition that he conflict be resolved, Modify-Node can be performed and changes made to the originalproposed beliefs.
Otherwise, the user may propose mod-9Only MB( S, U, Postponed-Sabbatical( Smith, 1997)) will beproposed as justification because the system believes that heevidential relationship needed to complete he inference is heldby a stereotypical user.ifications to the system's proposed modifications, result-ing in an embedded negotiation sub4iaJogue.5 Conc lus ionThis paper has presented a computational strategy for en-gaging in collaborative negotiation tosquare away con-flicts in agents' beliefs.
The model captures featuresspecific to collaborative negotiation.
It also suppom ef-fective and efficient dialogues by identifying the focus ofmodification based on its predicted success in resolvingthe conflict about he top-level belief and by using heuris-tics motivated by research in social psychology toselecta set of evidence to justify the proposed modification ofbeliefs.
Furthermore, by capturing collaborative negoti-ation in a cycle of Propose-Evaluate-Modify actions, theevaluation and modification processes can be applied re,cursively to capture mbedded negotiation subdialogues.AcknowledgmentsDiscussions with Candy Sidner, Stephanie Elzer, andKathy McCoy have been very helpful in the developmentof this work.
Comments from the anonymous reviewershave also been very useful in preparing the final versionof this paper.ReferencesJames Allen.
1991.
Discourse structure in the TRAINSproject.
In Darpa Speech and Natural Language Work-shop.Lawrence Birnb~nm; Margot Flowexs, and Rod McGuire.1980.
Towards an AI model of argumentation.
InProceedings of the National Conference on ArtificialIntelligence, pages 313-315.Alison Cawsey, Julia Galliers, Brian Logan, StevenReec.e, and Karen Sparck Jones.
1993.
Revising be-fiefs and intentions: A unified framework for agentinteraction.
In The Ninth Biennial Conference of theSociety for the Study of Artificial Intelligence and Sim-ulation of Behaviour, pages 130-139.Jennifer Chu-Carroll and Sandra Carberry.
1994.
A plan-based model for response generation i collaborativetask-oriented dialogues.
In Proceedings of the TwelfthNational Conference on Artificial Intelligence, pages799-805.Jennifer Chu-Carroll and Sandra Carberry.
1995.
Gener-ating information-sharing subdialognes in expert-userconsultation.
In Proceedings of the 14th InternationalJoint Conference on Artificial Intelligence.
To appear.Jennifer Chu-Carroll.
1995.
A Plan-BasedModelforRe-sponse Generation in Collaborative Consultation Di.alogues.
Ph.D. thesis, University of Delaware.
Forth-coming.Paul R. Cohen.
1985.
Heuristic Reasoning about Un-certainty: An Artificial Intelligence Approach.
PitmanPublishing Company.142Robin Cohen.
1987.
Analyzing the structure of argu-mentative discourse.
ComputationalLinguistcis, 13(1-2): 11-24, January-June.Columbia University Transcripts.
1985.
Transcripts de-rived from audiotape conversations made at ColumbiaUniversity, New York, NY.
Provided by KathleenMcKeown.Julia R. Galliers.
1992.
Autonomous belief revision andcommunication.
I  Gardenfors, editor, BeliefRevision.Cambridge University Press.H.
Paul Grice.
1975.
Logic and conversation.
In PeterCole and Jerry L. Morgan, editors, Syntax and Seman-tics 3: Speech Acts, pages 41-58.
Academic Press,Inc., New York.Barbara J. Grosz and Caadace L. Sidner.
1990.
Plansfor discourse.
In Cohen, Morgan, and Pollack, editors,Intentions in Communication, chapter 20, pages 417-444.
MIT Press.Dale Hample.
1985.
Refinements on the cognitive modelof argument: Concreteness, involvement and groupscores.
The Western Journal of Speech Communica-tion, 49:267-285.Aravind K. Joshi.
1982.
Mutual beliefs in question-answer systems.
In N.V. Smith, editor, Mutual Knowl-edge, chapter 4, pages 181-197.
Academic Press.Lynn Lambert and Sandra Carberry.
1991.
A tripartiteplan-based model of dialogue.
In Proceedings of the29th Annual Meeting of the Association for Computa-tional Linguistics, pages 47-54.Brian Logan, Steven Reece, Alison Cawsey, Julia Gal-tiers, and Karen Sparck Jones.
1994.
Belief revisionand dialogue management in information retrieval.Technical Report 339, University of Cambridge, Com-puter Lalx)ratory.Joseph A. Luchok and James C. McCroskey.
1978.
Theeffect of quality of evidence on attitude change andsource credibility.
The Southern Speech Communica-tion Journal, 43:371-383.Mark T. Maybury.
1993.
Communicative acts for gen-erating natural anguage arguments.
In Proceedingsof the National Conference on Artificial Intelligence,pages 357-364.Kathleen R. McKeown.
1985.
Text Generation : UsingDiscourse Strategies and Focus Constraints to Gen-erate Natural Language Text.
Cambridge UniversityPress.DonaldD.
Morley.
1987.
Subjective message constructs:A theory of persuasion.
Conmmnication Monographs,54:183-203.Richard E. Petty and John T. Cacioppo.
1984.
The ef-fects of involvement on responses toargument quantityand quality: Central and peripheral routes to persua-sion.
Journal of Personality and Social Psychology,46(1):69-81.Martha E. Pollack.
1986.
A model of plan inferencethat distinguishes between the beliefs of actors and ob-servers.
In Proceedings of the 24th Annual Meeting ofthe Association for Computational Linguistics, pages207-214.Alex Quilici.
1992.
Arguing about planning alternatives.In Proceedings of the 14th International Conferenceon Computational Linguistics, pages 906-910.Rachel Reichman.
1981.
Modeling informal debates.
InProceedings of the 7th International Joint Conferenceon Artificial Intelligence, pages 19-24.John C. Reinard.
1988.
The empirical study of the per-suasive ffects of evidence, the status after fifty years ofresearch.
Human Communication Research, 15(1):3-59.Rodaey A. Reynolds and Michael Burgoon.
1983.
Be-lief processing, reasoning, and evidence.
In Bostrom,editor, Communication Yearbook 7, chapter 4, pages83-104.
Sage Publications.Candace L. Sidner.
1992.
Using discourse to negotiatein collaborative activity: An artificial language.
InAAA-92 Workshop: Cooperation Among Heteroge-neous Intelligent Systems, pages 121-128.Candace L. Sidner.
1994.
An artificial discourse lan-guage for collaborative n gotiation.
In Proceedings ofthe Twelfth National Conference on Artificial Intelli-gence, pages 814-819.SKI Transcripts.
1992.
Transcripts derived from audio-tape conversations made at SRI International, MenloPark, CA.
Prepared by Jacqueline Kowtko under thedirection of Patti Price.Katia Sycara.
1989.
Argumentation: Planning otheragents' plans.
In Proceedings of the l l th InternationalJoint Conference on Artificial Intelligence, pages 517-523.Marflyn Walker and Steve Whinak~.
1990.
Mixed ini-tiative in dialogue: An investigation i to discourse seg-mentation.
In Proceedings of the 28th Annual Meet-ing of the Association for Computational Linguistics,pages 70-78.Marilyn A. Walker.
1992.
Redundancy in collaborativedialogue.
In Proceedings of the 15th InternationalConference on Computational Linguistics, pages 345-351.Marilyn A. Walker.
1994.
Discourse and deliberation:Testing a collaborative strategy.
In Proceedings ofthe 15th International Conference on ComputationalLinguistics.Bonnie Webber and Atavind Joshi.
1982.
Taking theinitiative in natural anguage data base interactions:Justifying why.
In Proceedings of COLING-82, pages413-418.Steve Whittaker and Phil Stenton.
1988.
Cues and con-trol in expert-client dialogues.
In Proceedings of the26th Annual Meeting of the Association for Computa-tional Linguistics, pages 123-130.Robert S. Wyer, Jr. 1970.
Information redundancy, in-consistency, and novelty and their role in impressionformation.
Journal of Experimental Social Psychol-ogy, 6:111-127.R.
Michael Young, Johanna D. Moore, and Martha E.Pollack.
1994.
Towards a principled representationof discourse plans.
In Proceedings of the SixteenthAnnual Meeting of the Cognitive Science Society, pages946-951.143
