Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
448?457, Prague, June 2007. c?2007 Association for Computational LinguisticsEnhancing Single-document Summarization by Combining RankNet andThird-party SourcesKrysta M. SvoreMicrosoft Research1 Microsoft WayRedmond, WA 98052ksvore@microsoft.comLucy VanderwendeMicrosoft Research1 Microsoft WayRedmond, WA 98052Christopher J.C. BurgesMicrosoft Research1 Microsoft WayRedmond, WA 98052AbstractWe present a new approach to automaticsummarization based on neural nets, calledNetSum.
We extract a set of features fromeach sentence that helps identify its impor-tance in the document.
We apply novelfeatures based on news search query logsand Wikipedia entities.
Using the RankNetlearning algorithm, we train a pair-basedsentence ranker to score every sentence inthe document and identify the most impor-tant sentences.
We apply our system todocuments gathered from CNN.com, whereeach document includes highlights and anarticle.
Our system significantly outper-forms the standard baseline in the ROUGE-1measure on over 70% of our document set.1 IntroductionAutomatic summarization was first studied almost50 years ago by Luhn (Luhn, 1958) and has contin-ued to be a steady subject of research.
Automaticsummarization refers to the creation of a shortenedversion of a document or cluster of documents bya machine, see (Mani, 2001) for details.
The sum-mary can be an abstraction or extraction.
In an ab-stract summary, content from the original documentmay be paraphrased or generated, whereas in an ex-tract summary, the content is preserved in its originalform, i.e., sentences.
Both summary types can in-volve sentence compression, but abstracts tend to bemore condensed.
In this paper, we focus on produc-ing fully automated single-document extract sum-maries of newswire articles.To create an extract, most automatic systems uselinguistic and/or statistical methods to identify keywords, phrases, and concepts in a sentence or acrosssingle or multiple documents.
Each sentence is thenassigned a score indicating the strength of presenceof key words, phrases, and so on.
Sentence scoringmethods utilize both purely statistical and purely se-mantic features, for example as in (Vanderwende etal., 2006; Nenkova et al, 2006; Yih et al, 2007).Recently, machine learning techniques have beensuccessfully applied to summarization.
The meth-ods include binary classifiers (Kupiec et al, 1995),Markov models (Conroy et al, 2004), Bayesianmethods (Daume?
III and Marcu, 2005; Aone et al,1998), and heuristic methods to determine featureweights (Schiffman, 2002; Lin and Hovy, 2002).Graph-based methods have also been employed(Erkan and Radev, 2004a; Erkan and Radev, 2004b;Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihal-cea and Radev, 2006).In 2001?02, the Document Understanding Con-ference (DUC, 2001), issued the task of creat-ing a 100-word summary of a single news article.The best performing systems (Hirao et al, 2002;Lal and Ruger, 2002) used various learning andsemantic-based methods, although no system couldoutperform the baseline with statistical significance(Nenkova, 2005).
After 2002, the single-documentsummarization task was dropped.In recent years, there has been a decline in stud-ies on automatic single-document summarization,in part because the DUC task was dropped, and inpart because the task of single-document extractsmay be counterintuitively more difficult than multi-448document summarization (Nenkova, 2005).
How-ever, with the ever-growing internet and increasedinformation access, we believe single-documentsummarization is essential to improve quick ac-cess to large quantities of information.
Recently,CNN.com (CNN.com, 2007a) added ?Story High-lights?
to many news articles on its site to allowreaders to quickly gather information on stories.These highlights give a brief overview of the arti-cle and appear as 3?4 related sentences in the formof bullet points rather than a summary paragraph,making them even easier to quickly scan.Our work is motivated by both the addition ofhighlights to an extremely visible and reputable on-line news source, as well as the inability of pastsingle-document summarization systems to outper-form the extremely strong baseline of choosing thefirst n sentences of a newswire article as the sum-mary (Nenkova, 2005).
Although some recent sys-tems indicate an improvement over the baseline (Mi-halcea, 2005; Mihalcea and Tarau, 2005), statisticalsignificance has not been shown.
We show that byusing a neural network ranking algorithm and third-party datasets to enhance sentence features, our sys-tem, NetSum, can outperform the baseline with sta-tistical significance.Our paper is organized as follows.
Section 2 de-scribes our two studies: summarization and high-light extraction.
We describe our dataset in detail inSection 3.
Our ranking system and feature vectorsare outlined in Section 4.
We present our evaluationmeasure in Section 5.
Sections 6 and 7 report on ourresults on summarization and highlight extraction,respectively.
We conclude in Section 8 and discussfuture work in Section 9.2 Our TaskIn this paper, we focus on single-document summa-rization of newswire documents.
Each documentconsists of three highlight sentences and the articletext.
Each highlight sentence is human-generated,but is based on the article.
In Section 4 we discussthe process of matching a highlight to an article sen-tence.
The output of our system consists of purelyextracted sentences, where we do not perform anysentence compression or sentence generation.
Weleave such extensions for future work.We develop two separate problems based on ourdocument set.
First, can we extract three sentencesthat best ?match?
the highlights as a whole?
Inthis task, we concatenate the three sentences pro-duced by our system into a single summary or block,and similarly concatenate the three highlight sen-tences into a single summary or block.
We thencompare our system?s block against the highlightblock.
Second, can we extract three sentences thatbest ?match?
the three highlights, such that order-ing is preserved?
In this task, we produce three sen-tences, where the first sentence is compared againstthe first highlight, the second sentence is comparedagainst the second highlight, and the third sentenceis compared against the third highlight.
Credit isnot given for producing three sentences that matchthe highlights, but are out of order.
The second taskconsiders ordering and compares sentences on an in-dividual level, whereas the first task considers thethree chosen sentences as a summary or block anddisregards sentence order.
In both tasks, we assumethe title has been seen by the reader and will be listedabove the highlights.3 Evaluation CorpusOur data consists of 1365 news documents gatheredfrom CNN.com (CNN.com, 2007a).
Each documentwas extracted by hand, where a maximum of 50documents per day were collected.
The documentswere hand-collected on consecutive days during themonth of February.Each document includes the title, timestamp,story highlights, and article text.
The timestampon articles ranges from December 2006 to Febru-ary 2007, since articles remain posted on CNN.comfor up to several months.
The story highlights arehuman-generated from the article text.
The numberof story highlights is between 3?4.
Since all articlesinclude at least 3 story highlights, we consider onlythe task of extracting three highlights from each ar-ticle.4 Description of Our SystemOur goal is to extract three sentences from a singlenews document that best match various characteris-tics of the three document highlights.
One way toidentify the best sentences is to rank the sentences449TIMESTAMP: 1:59 p.m. EST, January 31, 2007TITLE: Nigeria reports first human death from bird fluHIGHLIGHT 1: Government boosts surveillance after woman diesHIGHLIGHT 2: Egypt, Djibouti also have reported bird flu in humansHIGHLIGHT 3: H5N1 bird flu virus has killed 164 worldwide since 2003ARTICLE: 1.
Health officials reported Nigeria?s first cases of bird flu in humans on Wednesday,saying one woman had died and a family member had been infected but was responding totreatment.
2.
The victim, a 22-year old woman in Lagos, died January 17, Information MinisterFrank Nweke said in a statement.
3.
He added that the government was boosting surveillanceacross Africa?s most-populous nation after the infections in Lagos, Nigeria?s biggest city.
4.The World Health Organization had no immediate confirmation.
5.
Nigerian health officialsearlier said 14 human samples were being tested.
6.
Nweke made no mention of those cases onWednesday.
7.
An outbreak of H5N1 bird flu hit Nigeria last year, but no human infections hadbeen reported until Wednesday.
8.
Until the Nigerian report, Egypt and Djibouti were the onlyAfrican countries that had confirmed infections among people.
9.
Eleven people have died inEgypt.
10.
The bird flu virus remains hard for humans to catch, but health experts fear H5N1may mutate into a form that could spread easily among humans and possibly kill millions ina flu pandemic.
11.
Amid a new H5N1 outbreak reported in recent weeks in Nigeria?s north,hundreds of miles from Lagos, health workers have begun a cull of poultry.
12.
Bird flu isgenerally not harmful to humans, but the H5N1 virus has claimed at least 164 lives worldwidesince it began ravaging Asian poultry in late 2003, according to the WHO.
13.
The H5N1 strainhad been confirmed in 15 of Nigeria?s 36 states.
14.
By September, when the last known caseof the virus was found in poultry in a farm near Nigeria?s biggest city of Lagos, 915,650 birdshad been slaughtered nationwide by government veterinary teams under a plan in which theowners were promised compensation.
15.
However, many Nigerian farmers have yet to receivecompensation in the north of the country, and health officials fear that chicken deaths may becovered up by owners reluctant to slaughter their animals.
16.
Since bird flu cases were firstdiscovered in Nigeria last year, Cameroon, Djibouti, Niger, Ivory Coast, Sudan and BurkinaFaso have also reported the H5N1 strain of bird flu in birds.
17.
There are fears that it hasspread even further than is known in Africa because monitoring is difficult on a poor continentwith weak infrastructure.
18.
With sub-Saharan Africa bearing the brunt of the AIDS epidemic,there is concern that millions of people with suppressed immune systems will be particularlyvulnerable, especially in rural areas with little access to health facilities.
19.
Many people keepchickens for food, even in densely populated urban areas.Figure 1: Example document containing highlightsand article text.
Sentences are numbered by theirposition.
Article is from (CNN.com, 2007b).using a machine learning approach, for example asin (Hirao et al, 2002).
A train set is labeled suchthat the labels identify the best sentences.
Then aset of features is extracted from each sentence in thetrain and test sets, and the train set is used to trainthe system.
The system is then evaluated on the testset.
The system learns from the train set the distri-bution of features for the best sentences and outputsa ranked list of sentences for each document.
In thispaper, we rank sentences using a neural network al-gorithm called RankNet (Burges et al, 2005).4.1 RankNetFrom the labels and features for each sentence, wetrain a model that, when run on a test set of sen-tences, can infer the proper ranking of sentencesin a document based on information gathered dur-ing training about sentence characteristics.
To ac-complish the ranking, we use RankNet (Burges etal., 2005), a ranking algorithm based on neural net-works.RankNet is a pair-based neural network algorithmused to rank a set of inputs, in this case, the setof sentences in a given document.
The system istrained on pairs of sentences (Si, Sj), such that Sishould be ranked higher or equal to Sj .
Pairs aregenerated between sentences in a single document,not across documents.
Each pair is determined fromthe input labels.
Since our sentences are labeled us-ing ROUGE (see Section 4.3), if the ROUGE scoreof Si is greater than the ROUGE score of Sj , then(Si, Sj) is one input pair.
The cost function forRankNet is the probabilistic cross-entropy cost func-tion.
Training is performed using a modified versionof the back propagation algorithm for two layer nets(Le Cun et al, 1998), which is based on optimiz-ing the cost function by gradient descent.
A simi-lar method of training on sentence pairs in the con-text of multi-document summarization was recentlyshown in (Toutanova et al, 2007).Our system, NetSum, is a two-layer neural nettrained using RankNet.
To speed up the performanceof RankNet, we implement RankNet in the frame-work of LambdaRank (Burges et al, 2006).
For de-tails, see (Burges et al, 2006; Burges et al, 2005).We experiment with between 5 and 15 hidden nodesand with an error rate between 10?2 and 10?7.We implement 4 versions of NetSum.
The first450version, NetSum(b), is trained for our first sum-marization problem (b indicates block).
The pairsare generated using the maximum ROUGE scoresl1 (see Section 4.3).
The other three rankers aretrained to identify the sentence in the documentthat best matches highlight n. We train one ranker,NetSum(n), for each highlight n, for n = 1, 2, 3,resulting in three rankers.
NetSum(n) is trained us-ing pairs generated from the l1,n ROUGE scores be-tween sentence Si and highlight Hn (see Section4.3).4.2 Matching Extracted to GeneratedSentencesIn this section, we describe how to determine whichsentence in the document best matches a given high-light.
Choosing three sentences most similar to thethree highlights is very challenging since the high-lights include content that has been gathered acrosssentences and even paragraphs, and furthermore in-clude vocabulary that may not be present in thetext.
Jing showed, for 300 news articles, that 19%of human-generated summary sentences contain nomatching article sentence (Jing, 2002).
In addition,only 42% of the summary sentences match the con-tent of a single article sentence, where there are stillsemantic and syntactic transformations between thesummary sentence and article sentence..
Since eachhighlight is human generated and does not exactlymatch any one sentence in the document, we mustdevelop a method to identify how closely related ahighlight is to a sentence.
We use the ROUGE (Lin,2004b) measure to score the similarity between anarticle sentence and a highlight sentence.
We antic-ipate low ROUGE scores for both the baseline andNetSum due to the difficulty of finding a single sen-tence to match a highlight.4.3 ROUGERecall-Oriented Understudy for Gisting Evaluation(Lin, 2004b), known as ROUGE, measures the qual-ity of a model-generated summary or sentence bycomparing it to a ?gold-standard?, typically human-generated, summary or sentence.
It has been shownthat ROUGE is very effective for measuring bothsingle-document summaries and single-documentheadlines (Lin, 2004a).ROUGE-N is a N -gram recall between a model-generated summary and a reference summary.
Weuse ROUGE-N , for N = 1, for labeling and evalua-tion of our model-generated highlights.1 ROUGE-1 and ROUGE-2 have been shown to be statisti-cally similar to human evaluations and can be usedwith a single reference summary (Lin, 2004a).
Wehave only one reference summary, the set of human-generated highlights, per document.
In our work,the reference summary can be a single highlight sen-tence or the highlights as a block.
We calculateROUGE-N as?gramj?R?Si Count(gramj)?gramj?R Count(gramj), (1)where R is the reference summary, Si is the model-generated summary, and N is the length of the N -gram gramj .2 The numerator cannot excede thenumber of N -grams (non-unique) in R.We label each sentence Si by its ROUGE-1 score.For the first problem of matching the highlightsas a block, we label each Si by l1, the maximumROUGE-1 score between Si and each highlight Hn,for n = 1, 2, 3, given by l1 = maxn(R(Si,Hn)).For the second problem of matching three sen-tences to the three highlights individually, we labeleach sentence Si by l1,n, the ROUGE-1 score be-tween Si and Hn, given by l1,n = R(Si,Hn).
Theranker for highlight n, NetSum(n), is passed sam-ples labeled using l1,n.4.4 FeaturesRankNet takes as input a set of samples, where eachsample contains a label and feature vector.
The la-bels were previously described in Section 4.3.
In thissection, we describe each feature in detail and moti-vate in part why each feature is chosen.
We generate10 features for each sentence Si in each document,listed in Table 1.
Each feature is chosen to identifycharacteristics of an article sentence that may matchthose of a highlight sentence.
Some of the featuressuch as position and N -gram frequencies are com-monly used for scoring.
Sentence scoring based on1We use an implementation of ROUGE that does not per-form stemming or stopword removal.2ROUGE is typically used when the length of the referencesummary is equal to length of the model-generated summary.Our reference summary and model-generated summary are dif-ferent lengths, so there is a slight bias toward longer sentences.451Symbol Feature NameF (Si) Is First SentencePos(Si) Sentence PositionSB(Si) SumBasic ScoreSBb(Si) SumBasic Bigram ScoreSim(Si) Title Similarity ScoreNT (Si) Average News Query Term ScoreNT+(Si) News Query Term Sum ScoreNTr(Si) Relative News Query Term ScoreWE(Si) Average Wikipedia Entity ScoreWE+(Si) Wikipedia Entity Sum ScoreTable 1: Features used in our model.sentence position, terms common with the title, ap-pearance of keyword terms, and other cue phrasesis known as the Edmundsonian Paradigm (Edmund-son, 1969; Alfonesca and Rodriguez, 2003; Mani,2001).
We use variations on these features as wellas a novel set of features based on third-party data.Typically, news articles are written such that thefirst sentence summarizes the article.
Thus, we in-clude a binary feature F (Si) that equals 1 if Si isthe first sentence of the document: F (Si) = ?i,1,where ?
is the Kronecker delta function.
This fea-ture is used only for NetSum(b) and NetSum(1).We include sentence position since we found inempirical studies that the sentence to best matchhighlight H1 is on average 10% down the article, thesentence to best match H2 is on average 20% downthe article, and the sentence to best match H3 is 31%down the article.3 We calculate the position of Si indocument D asPos(Si) =i?
, (2)where i = {1, .
.
.
, ?}
is the sentence number and ?is the number of sentences in D.We include the SumBasic score (Nenkova et al,2006) of a sentence to estimate the importance of asentence based on word frequency.
We calculate theSumBasic score of Si in document D asSB(Si) =?w?Si p(w)|Si|, (3)3Though this is not always the case, as the sentence to matchH2 precedes that to match H1 in 22.03% of documents, and thesentence to match H3 precedes that to match H2 in 29.32% ofand precedes that to match H1 in 28.81% of documents.where p(w) is the probability of word w and |Si| isthe number of words in sentence Si.
We calculatep(w) as p(w) = Count(w)|D| , where Count(w) is thenumber of times word w appears in document D and|D| is the number of words in document D. Notethat the score of a sentence is the average probabilityof a word in the sentence.We also include the SumBasic score over bi-grams, where w in Eq 3 is replaced by bigrams andwe normalize by the number of bigrams in Si.We compute the similarity of a sentence Si in doc-ument D with the title T of D as the relative proba-bility of title terms t ?
T in Si asSim(Si) =?t?Si p(t)|Si|, (4)where p(t) = Count(t)|T | is the number of times term tappears in T over the number of terms in T .The remaining features we use are based on third-party data sources.
Previously, third-party sourcessuch as WordNet (Fellbaum, 1998), the web (Ja-galamudi et al, 2006), or click-through data (Sunet al, 2005) have been used as features.
We pro-pose using news query logs and Wikipedia entitiesto enhance features.
We base several features onquery terms frequently issued to Microsoft?s newssearch engine http://search.live.com/news, and enti-ties4 found in the online open-source encyclopediaWikipedia (Wikipedia.org, 2007).
If a query term orWikipedia entity appears frequently in a CNN docu-ment, then we assume highlights should include thatterm or entity since it is important on both the doc-ument and global level.
Sentences containing queryterms or Wikipedia entities therefore contain impor-tant content.
We confirm the importance of thesethird-party features in Section 7.We collected several hundred of the most fre-quently queried terms in February 2007 from thenews query logs.
We took the daily top 200 termsfor 10 days.
Our hypothesis is that a sentence witha higher number of news query terms should be abetter candidate highlight.
We calculate the averageprobability of news query terms q in Si asNT (Si) =?q?Si p(q)|q ?
Si|, (5)4We define an entity as a title of a Wikipedia page.452where p(q) is the probability of a news term q and|q ?
Si| is the number of news terms in Si.
p(q) =Count(q)|q?D| , where Count(q) is the number of timesterm q appears in D and |q ?
D| is the number ofnews query terms in D.We also include the sum of news query terms inSi, given by NT+(Si) =?q?Si p(q), and the rela-tive probability of news query terms in Si, given byNTr(Si) =?q?Sip(q)|Si| .We perform term disambiguation on each doc-ument using an entity extractor (Cucerzan, 2007).Terms are disambiguated to a Wikipedia entityonly if they match a surface form in Wikipedia.Wikipedia surface forms are terms that disambiguateto a Wikipedia entity and link to a Wikipedia pagewith the entity as its title.
For example, ?WHO?
and?World Health Org.?
both refer to the World HealthOrganization, and should disambiguate to the entity?World Health Organization?.
Sentences in CNNdocument D that contain Wikipedia entities that fre-quently appear in CNN document D are consideredimportant.
We calculate the average Wikipedia en-tity score for Si asWE(Si) =?e?Si p(e)|e ?
Si|, (6)where p(e) is the probability of entity e, given byp(e) = Count(e)|e?D| , where Count(e) is the number oftimes entity e appears in CNN document D and |e ?D| is the total number of entities in CNN documentD.We also include the sum of Wikipedia entities,given by WE+(Si) =?e?Si p(e).Note that all features except position features area variant of SumBasic over different term sets.
Allfeatures are computed over sentences where everyword has been lowercased and punctuation has beenremoved after sentence breaking.
We examined us-ing stemming, but found stemming to be ineffective.5 EvaluationWe evaluate the performance of NetSum usingROUGE and by comparing against a baseline sys-tem.
For the first summarization task, we compareagainst the baseline of choosing the first three sen-tences as the block summary.
For the second high-lights task, we compare NetSum(n) against the base-line of choosing sentence n (to match highlight n).Both tasks are novel in attempting to match high-lights rather than a human-generated summary.We consider ROUGE-1 to be the measure of im-portance and thus train our model on ROUGE-1 (tooptimize ROUGE-1 scores) and likewise evaluateour system on ROUGE-1.
We list ROUGE-2 scoresfor completeness, but do not expect them to be sub-stantially better than the baseline since we did notdirectly optimize for ROUGE-2.5For every document in our corpus, we compareNetSum?s output with the baseline output by com-puting ROUGE-1 and ROUGE-2 between the high-light block and NetSum and between the highlightblock and the block of sentences.
Similarly, for eachhighlight, we compute ROUGE-1 and ROUGE-2between highlight n and NetSum(n) and betweenhighlight n and sentence n, for n = 1, 2, 3.
Foreach task, we calculate the average ROUGE-1 andROUGE-2 scores of NetSum and of the baseline.We also report the percent of documents where theROUGE-1 score of NetSum is equal to or better thanthe ROUGE-1 score of the baseline.We perform all experiments using five-fold cross-validation on our dataset of 1365 documents.
Wedivide our corpus into five random sets and train onthree combined sets, validate on one set, and test onthe remaining set.
We repeat this procedure for ev-ery combination of train, validation, and test sets.Our results are the micro-averaged results on the fivetest sets.
For all experiments, Table 3 lists the statis-tical tests performed and the significance of perfor-mance differences between NetSum and the baselineat 95% confidence.6 Results: SummarizationWe first find three sentences that, as a block, bestmatch the three highlights as a block.
NetSum(b)produces a ranked list of sentences for each docu-ment.
We create a block from the top 3 ranked sen-tences.
The baseline is the block of the first 3 sen-tences of the document.
A similar baseline outper-5NetSum can directly optimize for any measure by trainingon it, such as training on ROUGE-2 or on a weighted sum ofROUGE-1 and ROUGE-2 to optimize both.
Thus, ROUGE-2scores could be further improved.
We leave such studies forfuture work.453System Av.
ROUGE-1 Av.
ROUGE-2Baseline 0.4642 ?
0.0084 0.1726 ?
0.0064NetSum(b) 0.4956 ?
0.0075 0.1775 ?
0.0066Table 2: Results on summarization task with stan-dard error at 95% confidence.
Bold indicates signif-icance under paired tests.ROUGE-1 ROUGE-2System 1 2 3 1 2 3NetSum(b) x x x x o oNetSum(1) x x x o o oNetSum(2) x x x x o xNetSum(3) x x x x x xTable 3: Paired tests for statistical significanceat 95% confidence between baseline and NetSumperformance; 1: McNemar, 2: Paired t-test, 3:Wilcoxon signed-rank.
?x?
indicates pass, ?o?
in-dicates fail.
Since our studies are pair-wise, testslisted here are more accurate than error bars reportedin Tables 2?5.forms all previous systems for news article summa-rization (Nenkova, 2005) and has been used in theDUC workshops (DUC, 2001).For each block produced by NetSum(b) and thebaseline, we compute the ROUGE-1 and ROUGE-2scores of the block against the set of highlights as ablock.
For 73.26% of documents, NetSum(b) pro-duces a block with a ROUGE-1 score that is equalto or better than the baseline score.
The two systemsproduce blocks of equal ROUGE-1 score for 24.69%of documents.
Under ROUGE-2, NetSum(b) per-forms equal to or better than the baseline on 73.19%of documents and equal to the baseline on 40.51%of documents.Table 2 shows the average ROUGE-1 andROUGE-2 scores obtained with NetSum(b) and thebaseline.
NetSum(b) produces a higher qualityblock on average for ROUGE-1.Table 4 lists the sentences in the block producedby NetSum(b) and the baseline block, for the arti-cles shown in Figure 1.
The NetSum(b) summaryachieves a ROUGE-1 score of 0.52, while the base-line summary scores only 0.36.System Sent.
# ROUGE-1Baseline S1, S2, S3 0.36NetSum(b) S1, S7, S15 0.52Table 4: Block results for the block produced byNetSum(b) and the baseline block for the exam-ple article.
ROUGE-1 scores computed against thehighlights as a block are listed.7 Results: HighlightsOur second task is to extract three sentences froma document that best match the three highlights inorder.
To accomplish this, we train NetSum(n) foreach highlight n = 1, 2, 3.
We compare NetSum(n)with the baseline of picking the nth sentence of thedocument.
We perform five-fold cross-validationacross our 1365 documents.
Our results are reportedfor the micro-average of the test results.
For eachhighlight n produced by both NetSum(n) and thebaseline, we compute the ROUGE-1 and ROUGE-2 scores against the nth highlight.We expect that beating the baseline for n = 1 is amore difficult task than for n = 2 or 3 since the firstsentence of a news article typically acts as a sum-mary of the article and since we expect the first high-light to summarize the article.
NetSum(1), however,produces a sentence with a ROUGE-1 score that isequal to or better than the baseline score for 93.26%of documents.
The two systems produce sentencesof equal ROUGE-1 scores for 82.84% of documents.Under ROUGE-2, NetSum(1) performs equal to orbetter than the baseline on 94.21% of documents.Table 5 shows the average ROUGE-1 andROUGE-2 scores obtained with NetSum(1) and thebaseline.
NetSum(1) produces a higher quality sen-tence on average under ROUGE-1.The content of highlights 2 and 3 is typically fromlater in the document, so we expect the baseline tonot perform as well in these tasks.
NetSum(2) out-performs the baseline since it is able to identify sen-tences from further down the document as impor-tant.
For 77.73% of documents, NetSum(2) pro-duces a sentence with a ROUGE-1 score that is equalto or better than the score for the baseline.
The twosystems produce sentences of equal ROUGE-1 scorefor 33.92% of documents.
Under ROUGE-2, Net-Sum(2) performs equal to or better than the baseline454System Av.
ROUGE-1 Av.
ROUGE-2Baseline(1) 0.4343 ?
0.0138 0.1833 ?
0.0095NetSum(1) 0.4478 ?
0.0133 0.1857 ?
0.0085Baseline(2) 0.2451 ?
0.0128 0.0814 ?
0.0106NetSum(2) 0.3036 ?
0.0117 0.0877 ?
0.0107Baseline(3) 0.1707 ?
0.0103 0.0412 ?
0.0069NetSum(3) 0.2603 ?
0.0133 0.0615 ?
0.0075Table 5: Results on ordered highlights task withstandard error at 95% confidence.
Bold indicatessignificance under paired tests.System Sent.
# ROUGE-1Baseline S1 0.167NetSum(1) S1 0.167Baseline S2 0.111NetSum(2) S1 0.556Baseline S3 0.000NetSum(3) S15 0.400Table 6: Highlight results for highlight n producedby NetSum(n) and highlight n produced by the base-line for the example article.
ROUGE-1 scores com-puted against highlight n are listed.84.84% of the time.
For 81.09% of documents, Net-Sum(3) produces a sentence with a ROUGE-1 scorethat is equal to or better than the score for the base-line.
The two systems produce sentences of equalROUGE-1 score for 28.45% of documents.
UnderROUGE-2, NetSum(3) performs equal to or betterthan the baseline 89.91% of the time.Table 5 shows the average ROUGE-1 andROUGE-2 scores obtained for NetSum(2), Net-Sum(3), and the baseline.
Both NetSum(2) and Net-Sum(3) produce a higher quality sentence on aver-age under both measures.Table 6 gives highlights produced by NetSum(n)and the highlights produced by the baseline, for thearticle shown in Figure 1.
The NetSum(n) highlightsproduce ROUGE-1 scores equal to or higher than thebaseline ROUGE-1 scores.In feature ablation studies, we confirmed that theinclusion of news-based and Wikipedia-based fea-tures improves NetSum?s peformance.
For example,we removed all news-based and Wikipedia-basedfeatures in NetSum(3).
The resulting performancemoderately declined.
Under ROUGE-1, the base-line produced a better highlight on 22.34% of docu-ments, versus only 18.91% when using third-partyfeatures.
Similarly, NetSum(3) produced a sum-mary of equal or better ROUGE-1 score on only77.66% of documents, compared to 81.09% of doc-uments when using third-party features.
In addi-tion, the average ROUGE-1 score dropped to 0.2182and the average ROUGE-2 score dropped to 0.0448.The performance of NetSum with third-party fea-tures over NetSum without third-party features isstatistically significant at 95% confidence.
However,NetSum still outperforms the baseline without third-party features, leading us to conclude that RankNetand simple position and term frequency featurescontribute the maximum performance gains, but in-creased ROUGE-1 and ROUGE-2 scores are a clearbenefit of third-party features.8 ConclusionsWe have presented a novel approach to automaticsingle-document summarization based on neuralnetworks, called NetSum.
Our work is the firstto use both neural networks for summarization andthird-party datasets for features, using Wikipediaand news query logs.
We have evaluated our sys-tem on two novel tasks: 1) producing a block ofhighlights and 2) producing three ordered highlightsentences.
Our experiments were run on previouslyunstudied data gathered from CNN.com.
Our sys-tem shows remarkable performance over the base-line of choosing the first n sentences of the docu-ment, where the performance difference is statisti-cally significant under ROUGE-1.9 Future WorkAn immediate future direction is to further explorefeature selection.
We found third-party featuresbeneficial to the performance of NetSum and suchsources can be mined further.
In addition, feature se-lection for each NetSum system could be performedseparately since, for example, highlight 1 has differ-ent characteristics than highlight 2.In our experiments, ROUGE scores are fairly lowbecause a highlight rarely matches the content of asingle sentence.
To improve NetSum?s performance,we must consider extracting content across sentence455boundaries.
Such work requires a system to produceabstract summaries.
We hope to incorporate sen-tence simplification and sentence splicing and merg-ing in a future version of NetSum.Another future direction is the identification of?hard?
and ?easy?
inputs.
Although we report av-erage ROUGE scores, such measures can be mis-leading since some highlights are simple to matchand some are much more difficult.
A better systemevaluation measure would incorporate the difficultyof the input and weight reported results accordingly.ReferencesE.
Alfonesca and P. Rodriguez.
2003.
Description ofthe uam system for generating very short summariesat DUC?2003.
In DUC 2003: Document Under-standing Conference, May 31?June 1, 2003, Edmon-ton, Canada.C.
Aone, M. Okurowski, and J. Gorlinsky.
1998.
Train-able scalable summarization using robust nlp and ma-chine learning.
In Proceedings of the 17th COLINGand 36th ACL.C.J.C.
Burges, T. Shaked, E. Renshaw, A. Lazier,M.
Deeds, N. Hamilton, and G. Hullender.
2005.Learning to Rank using Gradient Descent.
In Luc DeRaedt and Stefan Wrobel, editors, ICML, pages 89?96.ACM.C.J.C.
Burges, R. Ragno, and Q.
Le.
2006.
Learning torank with nonsmooth cost functions.
In NIPS 2006:Neural Information Processing Systems, December 4-7, 2006, Vancouver, CA.CNN.com.
2007a.
Cable news network.http://www.cnn.com/.CNN.com.
2007b.
Nigeria reportsfirst human death from bird flu.http://edition.cnn.com/2007/WORLD/africa/01/31/nigeria.bird.flu.ap/index.html?eref=edition world.J.
Conroy, J. Schlesinger, J. Goldstein, and D. O?Leary.2004.
Left-brain/right-brain multi-document summa-rization.
In DUC 2004: Document UnderstandingWorkshop, May 6?7, 2004, Boston, MA, USA.S.
Cucerzan.
2007.
Large scale named entity disam-biguation based on wikipedia data.
In EMNLP 2007:Empirical Methods in Natural Language Processing,June 28-30, 2007, Prague, Czech Republic.H.
Daume?
III and D. Marcu.
2005.
Bayesian multi-document summarization at mse.
In Proceedings ofMSE.DUC.
2001.
Document understanding conferences.http://www-nlpir.nist.gov/projects/duc/index.html.H.P.
Edmundson.
1969.
New methods in automatic ex-tracting.
Journal for the Association of ComputingMachinery, 16:159?165.G.
Erkan and D. R. Radev.
2004a.
Lexpagerank:Prestige in multi-document text summarization.
InEMNLP 2004: Empirical Methods in Natural Lan-guage Processing, 2004, Barcelona, Spain.G.
Erkan and D. R. Radev.
2004b.
Lexrank: Graph-based centrality as salience in text summarization.Journal of Artificial Intelligence Research (JAIR), 22.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.T.
Hirao, Y. Sasaki, H. Isozaki, and E. Maeda.
2002.Ntt?s text summarization system for DUC?2002.
InDUC 2002: Workshop on Text Summarization, July11?12, 2002, Philadelphia, PA, USA.J.
Jagalamudi, P. Pingali, and V. Varma.
2006.
Queryindependent sentence scoring approach to DUC 2006.In DUC 2006: Document Understanding Conference,June 8?9, 2006, Brooklyn, NY, USA.H.
Jing.
2002.
Using hidden markov modeling to de-compose human-written summaries.
ComputationalLinguistics, 4(28):527?543.J.
Kupiec, J. Pererson, and F. Chen.
1995.
A trainabledocument summarizer.
Research and Development inInformation Retrieval, pages 68?73.P.
Lal and S. Ruger.
2002.
Extract-based summarizationwith simplification.
In DUC 2002: Workshop on TextSummarization, July 11?12, 2002, Philadelphia, PA,USA.Y.
Le Cun, L. Bottou, G.B.
Orr, and K.R.
Mu?ller.1998.
Efficient backprop.
In Neural Networks, Tricksof the Trade, Lecture Notes in Computer ScienceLNCS 1524.
Springer Verlag.C.Y.
Lin and E. Hovy.
2002.
Automated multi-documentsummarization in neats.
In Proceedings of the HumanLanguage Technology Conference (HLT2002).C.Y.
Lin.
2004a.
Looking for a few good metrics: Auto-matic summarization evaluation ?
how many samplesare enough?
In Proceedings of the NTCIR Workshop4, June 2?4, 2004, Tokyo, Japan.C.Y.
Lin.
2004b.
Rouge: A package for automatic evalu-ation of summaries.
In WAS 2004: Proceedings of theWorkshop on Text Summarization Branches Out, July25?26, 2004, Barcelona, Spain.456H.
Luhn.
1958.
The automatic creation of literature ab-stracts.
IBM Journal of Research and Development,2(2):159?165.I.
Mani.
2001.
Automatic Summarization.
John Ben-jamins Pub.
Co.R.
Mihalcea and D. R. Radev, editors.
2006.
Textgraphs:Graph-based methods for NLP.
New York City, NY.R.
Mihalcea and P. Tarau.
2005.
An algorithm for lan-guage independent single and multiple document sum-marization.
In Proceedings of the International JointConference on Natural Language Processing (IJC-NLP), October, 2005, Korea.R.
Mihalcea.
2005.
Language independent extractivesummarization.
In ACL 2005: Proceedings of the 43rdAnnual Meeting of the Association for ComputationalLinguistics, June, 2005, Ann Arbor, MI, USA.A.
Nenkova, L. Vanderwende, and K. McKeown.
2006.A compositional context sensitive multi-documentsummarizer: exploring the factors that influence sum-marization.
In E. N. Efthimiadis, S. T. Dumais,D.
Hawking, and K. Ja?rvelin, editors, SIGIR, pages573?580.
ACM.A.
Nenkova.
2005.
Automatic text summarization ofnewswire: Lessons learned from the document un-derstanding conference.
In Proceedings of the 20thNational Conference on Artificial Intelligence (AAAI2005), Pittsburgh, PA.B.
Schiffman.
2002.
Building a resource for evaluat-ing the importance of sentences.
In Proceedings ofthe Third International Conference on Language Re-sources and Evaluation (LREC).J.T.
Sun, D. Shen, H.J.
Zeng, Q. Yang, Y. Lu, andZ.
Chen.
2005.
Web-page summarization usingclick-through data.
In R. A. Baeza-Yates, N. Ziviani,G.
Marchionini, A. Moffat, and J. Tait, editors, SIGIR.ACM.K.
Toutanova, C. Brockett, M. Gamon, J. Jagarla-mudi, H. Suzuki, and L. Vanderwende.
2007.
Thepythy summarization system: Microsoft research atDUC2007.
In DUC 2007: Document UnderstandingConference, April 26?27, 2007, Rochester, NY, USA.L.
Vanderwende, H. Suzuki, and C. Brockett.
2006.
Mi-crosoft research at DUC2006: Task-focused summa-rization with sentence simplification.
In DUC 2006:Document Understanding Workshop, June 8?9, 2006,Brooklyn, NY, USA.Wikipedia.org.
2007.
Wikipedia org.http://www.wikipedia.org.W.T.
Yih, J. Goodman, L. Vanderwende, and H. Suzuki.2007.
Multi-document summarization by maximizinginformative content words.
In IJCAI 2007: 20th In-ternational Joint Conference on Artificial Intelligence,January, 2007.457
