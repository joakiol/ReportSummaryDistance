Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 226?235,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsCompressing Neural Language Models by Sparse Word RepresentationsYunchuan Chen,1,2Lili Mou,1,3Yan Xu,1,3Ge Li,1,3Zhi Jin1,3,?1Key Laboratory of High Confidence Software Technologies (Peking University), MoE, China2University of Chinese Academy of Sciences, chenyunchuan11@mails.ucas.ac.cn3Institute of Software, Peking University, doublepower.mou@gmail.com,{xuyan14,lige,zhijin}@pku.edu.cn?Corresponding authorAbstractNeural networks are among the state-of-the-art techniques for language modeling.Existing neural language models typicallymap discrete words to distributed, densevector representations.
After informationprocessing of the preceding context wordsby hidden layers, an output layer estimatesthe probability of the next word.
Such ap-proaches are time- and memory-intensivebecause of the large numbers of parame-ters for word embeddings and the outputlayer.
In this paper, we propose to com-press neural language models by sparseword representations.
In the experiments,the number of parameters in our model in-creases very slowly with the growth of thevocabulary size, which is almost imper-ceptible.
Moreover, our approach not onlyreduces the parameter space to a large ex-tent, but also improves the performance interms of the perplexity measure.11 IntroductionLanguage models (LMs) play an important rolein a variety of applications in natural languageprocessing (NLP), including speech recognitionand document recognition.
In recent years, neu-ral network-based LMs have achieved signifi-cant breakthroughs: they can model languagemore precisely than traditional n-gram statistics(Mikolov et al, 2011); it is even possible to gen-erate new sentences from a neural LM, benefit-ing various downstream tasks like machine trans-lation, summarization, and dialogue systems (De-vlin et al, 2014; Rush et al, 2015; Sordoni et al,2015; Mou et al, 2015b).1Code released on https://github.com/chenych11/lmExisting neural LMs typically map a discreteword to a distributed, real-valued vector repre-sentation (called embedding) and use a neuralmodel to predict the probability of each wordin a sentence.
Such approaches necessitate alarge number of parameters to represent the em-beddings and the output layer?s weights, whichis unfavorable in many scenarios.
First, with awider application of neural networks in resource-restricted systems (Hinton et al, 2015), such ap-proach is too memory-consuming and may fail tobe deployed in mobile phones or embedded sys-tems.
Second, as each word is assigned with adense vector?which is tuned by gradient-basedmethods?neural LMs are unlikely to learn mean-ingful representations for infrequent words.
Thereason is that infrequent words?
gradient is onlyoccasionally computed during training; thus theirvector representations can hardly been tuned ade-quately.In this paper, we propose a compressed neurallanguage model where we can reduce the numberof parameters to a large extent.
To accomplish this,we first represent infrequent words?
embeddingswith frequent words?
by sparse linear combina-tions.
This is inspired by the observation that, in adictionary, an unfamiliar word is typically definedby common words.
We therefore propose an op-timization objective to compute the sparse codesof infrequent words.
The property of sparseness(only 4?8 values for each word) ensures the effi-ciency of our model.Based on the pre-computed sparse codes, wedesign our compressed language model as follows.A dense embedding is assigned to each commonword; an infrequent word, on the other hand, com-putes its vector representation by a sparse combi-nation of common words?
embeddings.
We usethe long short term memory (LSTM)-based recur-rent neural network (RNN) as the hidden layer of226our model.
The weights of the output layer arealso compressed in a same way as embeddings.Consequently, the number of trainable neural pa-rameters is a constant regardless of the vocabularysize if we ignore the biases of words.
Even con-sidering sparse codes (which are very small), wefind the memory consumption grows impercepti-bly with respect to the vocabulary.We evaluate our LM on the Wikipedia corpuscontaining up to 1.6 billion words.
During train-ing, we adopt noise-contrastive estimation (NCE)(Gutmann and Hyv?arinen, 2012) to estimate theparameters of our neural LMs.
However, dif-ferent from Mnih and Teh (2012), we tailor theNCE method by adding a regression layer (calledZRegressoion) to predict the normalizationfactor, which stabilizes the training process.
Ex-perimental results show that, our compressed LMnot only reduces the memory consumption, butalso improves the performance in terms of the per-plexity measure.To sum up, the main contributions of this paperare three-fold.
(1) We propose an approach to rep-resent uncommon words?
embeddings by a sparselinear combination of common ones?.
(2) We pro-pose a compressed neural language model basedon the pre-computed sparse codes.
The memoryincreases very slowly with the vocabulary size (4?8 values for each word).
(3) We further introduce aZRegression mechanism to stabilize the NCEalgorithm, which is potentially applicable to otherLMs in general.2 Background2.1 Standard Neural LMsLanguage modeling aims to minimize the jointprobability of a corpus (Jurafsky and Martin,2014).
Traditional n-gram models impose aMarkov assumption that a word is only depen-dent on previous n ?
1 words and independent ofits position.
When estimating the parameters, re-searchers have proposed various smoothing tech-niques including back-off models to alleviate theproblem of data sparsity.Bengio et al (2003) propose to use a feed-forward neural network (FFNN) to replace themultinomial parameter estimation in n-gram mod-els.
Recurrent neural networks (RNNs) can also beused for language modeling; they are especiallycapable of capturing long range dependencies insentences (Mikolov et al, 2010; Sundermeyer etFigure 1: The architecture of a neural network-based language model.al., 2015).In the above models, we can view that a neuralLM is composed of three main parts, namely theEmbedding, Encoding, and Predictionsubnets, as shown in Figure 1.The Embedding subnet maps a word to adense vector, representing some abstract featuresof the word (Mikolov et al, 2013).
Note that thissubnet usually accepts a list of words (known ashistory or context words) and outputs a sequenceof word embeddings.The Encoding subnet encodes the history of atarget word into a dense vector (known as contextor history representation).
We may either leverageFFNNs (Bengio et al, 2003) or RNNs (Mikolovet al, 2010) as the Encoding subnet, but RNNstypically yield a better performance (Sundermeyeret al, 2015).The Prediction subnet outputs a distribu-tion of target words asp(w = wi|h) =exp(s(h,wi))?jexp(s(h,wj)), (1)s(h,wi) =W>ih+ bi, (2)where h is the vector representation of con-text/history h, obtained by the Encoding subnet.W = (W1,W2, .
.
.
,WV) ?
RC?Vis the outputweights of Prediction; b = (b1, b2, .
.
.
, bV) ?RCis the bias (the prior).
s(h,wi) is a scoringfunction indicating the degree to which the contexth matches a target word wi.
(V is the size of vo-cabulary V; C is the dimension of context/history,given by the Encoding subnet.
)2.2 Complexity Concerns of Neural LMsNeural network-based LMs can capture more pre-cise semantics of natural language than n-grammodels because the regularity of the Embeddingsubnet extracts meaningful semantics of a word227and the high capacity of Encoding subnet en-ables complicated information processing.Despite these, neural LMs also suffer from sev-eral disadvantages mainly out of complexity con-cerns.Time complexity.
Training neural LMs is typi-cally time-consuming especially when the vocab-ulary size is large.
The normalization factor inEquation (1) contributes most to time complex-ity.
Morin and Bengio (2005) propose hierar-chical softmax by using a Bayesian network sothat the probability is self-normalized.
Samplingtechniques?for example, importance sampling(Bengio and Sen?ecal, 2003), noise-contrastive es-timation (Gutmann and Hyv?arinen, 2012), and tar-get sampling (Jean et al, 2014)?are applied toavoid computation over the entire vocabulary.
In-frequent normalization maximizes the unnormal-ized likelihood with a penalty term that favors nor-malized predictions (Andreas and Klein, 2014).Memory complexity and model complexity.
Thenumber of parameters in the Embedding andPrediction subnets in neural LMs increaseslinearly with respect to the vocabulary size, whichis large (Table 1).
As said in Section 1, this issometimes unfavorable in memory-restricted sys-tems.
Even with sufficient hardware resources, itis problematic because we are unlikely to fullytune these parameters.
Chen et al (2015) pro-pose the differentiated softmax model by assign-ing fewer parameters to rare words than to fre-quent words.
However, their approach only han-dles the output weights, i.e., W in Equation (2);the input embeddings remain uncompressed intheir approach.In this work, we mainly focus on memory andmodel complexity, i.e., we propose a novel methodto compress the Embedding and Predictionsubnets in neural language models.2.3 Related WorkExisting work on model compression for neuralnetworks.
Bucilu?a et al (2006) and Hinton et al(2015) use a well-trained large network to guidethe training of a small network for model compres-sion.
Jaderberg et al (2014) compress neural mod-els by matrix factorization, Gong et al (2014) byquantization.
In NLP, Mou et al (2015a) learn anembedding subspace by supervised training.
Ourwork resembles little, if any, to the above methodsas we compress embeddings and output weightsusing sparse word representations.
Existing modelSub-nets RNN-LSTM FFNNEmbedding V E V EEncoding 4(CE + C2+ C) nCE + CPrediction V (C + 1) V (C + 1)TOTAL?O((C + E)V ) O((E + C)V )Table 1: Number of parameters in different neuralnetwork-based LMs.
E: embedding dimension;C: context dimension; V : vocabulary size.
?Notethat V  C (or E).compression typically works with a compromiseof performance.
On the contrary, our model im-proves the perplexity measure after compression.Sparse word representations.
We leveragesparse codes of words to compress neural LMs.Faruqui et al (2015) propose a sparse codingmethod to represent each word with a sparse vec-tor.
They solve an optimization problem to ob-tain the sparse vectors of words as well as a dic-tionary matrix simultaneously.
By contrast, we donot estimate any dictionary matrix when learningsparse codes, which results in a simple and easy-to-optimize model.3 Our Proposed ModelIn this section, we describe our compressed lan-guage model in detail.
Subsection 3.1 formal-izes the sparse representation of words, servingas the premise of our model.
On such a basis,we compress the Embedding and Predictionsubnets in Subsections 3.2 and 3.3, respectively.Finally, Subsection 3.4 introduces NCE for pa-rameter estimation where we further proposethe ZRegression mechanism to stabilize ourmodel.3.1 Sparse Representations of WordsWe split the vocabulary V into two disjoint subsets(B and C).
The first subset B is a base set, con-taining a fixed number of common words (8k inour experiments).
C = V\B is a set of uncommonwords.
We would like to use B?s word embeddingsto encode C?s.Our intuition is that oftentimes a word can bedefined by a few other words, and that rare wordsshould be defined by common ones.
Therefore,it is reasonable to use a few common words?
em-beddings to represent that of a rare word.
Follow-ing most work in the literature (Lee et al, 2006;Yang et al, 2011), we represent each uncommonword with a sparse, linear combination of com-228mon ones?
embeddings.
The sparse coefficientsare called a sparse code for a given word.We first train a word representation model likeSkipGram (Mikolov et al, 2013) to obtain a set ofembeddings for each word in the vocabulary, in-cluding both common words and rare words.
Sup-pose U = (U1,U2, .
.
.
,UB) ?
RE?Bis the(learned) embedding matrix of common words,i.e., Uiis the embedding of i-th word in B.
(Here,B = |B|.
)Each word in B has a natural sparse code (de-noted as x): it is a one-hot vector withB elements,the i-th dimension being on for the i-th word in B.For a wordw ?
C, we shall learn a sparse vectorx = (x1, x2, .
.
.
, xB) as the sparse code of theword.
Provided that x has been learned (whichwill be introduced shortly), the embedding of w is?w =B?j=1xjUj= Ux, (3)To learn the sparse representation of a certainword w, we propose the following optimizationobjectiveminx?Ux?w?22+ ?
?x?1+ ?|1>x?
1|+ ?1>max{0,?x}, (4)where max denotes the component-wise maxi-mum; w is the embedding for a rare word w ?
C.The first term (called fitting loss afterwards)evaluates the closeness between a word?s codedvector representation and its ?true?
representationw, which is the general goal of sparse coding.The second term is an `1regularizer, which en-courages a sparse solution.
The last two regular-ization terms favor a solution that sums to 1 andthat is nonnegative, respectively.
The nonnegativeregularizer is applied as in He et al (2012) due topsychological interpretation concerns.It is difficult to determine the hyperparameters?, ?, and ?.
Therefore we perform several tricks.First, we drop the last term in the problem (4), butclip each element in x so that all the sparse codesare nonnegative during each update of training.Second, we re-parametrize ?
and ?
by balanc-ing the fitting loss and regularization terms dy-namically during training.
Concretely, we solvethe following optimization problem, which isslightly different but closely related to the concep-tual objective (4):minxL(x) + ?tR1(x) + ?tR2(x), (5)where L(x) = ?Ux ?w?22, R1(x) = ?x?1, andR2(x) = |1>x?1|.
?tand ?tare adaptive param-eters that are resolved during training time.
Sup-pose xtis the value we obtain after the update ofthe t-th step, we expect the importance of fitnessand regularization remain unchanged during train-ing.
This is equivalent to?tR1(xt)L(xt)= w??
const, (6)?tR2(xt)L(xt)= w??
const.
(7)or?t=L(xt)R1(xt)w?and ?t=L(xt)R2(xt)w?,where w?and w?are the ratios between the regu-larization loss and the fitting loss.
They are mucheasier to specify than ?
or ?
in the problem (4).We have two remarks as follows.?
To learn the sparse codes, we first train the?true?
embeddings by word2vec2for bothcommon words and rare words.
However,these true embeddings are slacked during ourlanguage modeling.?
As the codes are pre-computed and remainunchanged during language modeling, theyare not tunable parameters of our neuralmodel.
Considering the learned sparse codes,we need only 4?8 values for each word on av-erage, as the codes contain 0.05?0.1% non-zero values, which are almost negligible.3.2 Parameter Compression for theEmbedding SubnetOne main source of LM parameters is theEmbedding subnet, which takes a list of words(history/context) as input, and outputs dense, low-dimensional vector representations of the words.We leverage the sparse representation of wordsmentioned above to construct a compressedEmbedding subnet, where the number of param-eters is independent of the vocabulary size.By solving the optimization problem (5) foreach word, we obtain a non-negative sparse codex ?
RBfor each word, indicating the degree towhich the word is related to common words inB.
Then the embedding of a word is given by?w = Ux.2https://code.google.com/archive/p/word2vec229We would like to point out that the embeddingof a word?w is not sparse becauseU is a dense ma-trix, which serves as a shared parameter of learn-ing all words?
vector representations.3.3 Parameter Compression for thePrediction SubnetAnother main source of parameters is thePrediction subnet.
As Table 1 shows, the out-put layer contains V target-word weight vectorsand biases; the number increases with the vocabu-lary size.
To compress this part of a neural LM, wepropose a weight-sharing method that uses words?sparse representations again.
Similar to the com-pression of word embeddings, we define a base setof weight vectors, and use them to represent therest weights by sparse linear combinations.Without loss of generality, we let D = W:,1:Bbe the output weights of B base target words, andc = b1:Bbe bias of the B target words.3The goalis to use D and c to represent W and b. How-ever, as the values ofW and b are unknown beforethe training of LM, we cannot obtain their sparsecodes in advance.We claim that it is reasonable to share thesame set of sparse codes to represent word vec-tors in Embedding and the output weights inthe Prediction subnet.
In a given corpus, anoccurrence of a word is always companied byits context.
The co-occurrence statistics about aword or corresponding context are the same.
Asboth word embedding and context vectors cap-ture these co-occurrence statistics (Levy and Gold-berg, 2014), we can expect that context vec-tors share the same internal structure as embed-dings.
Moreover, for a fine-trained network, givenany word w and its context h, the output layer?sweight vector corresponding to w should spec-ify a large inner-product score for the context h;thus these context vectors should approximate theweight vector of w. Therefore, word embed-dings and the output weight vectors should sharethe same internal structures and it is plausible touse a same set of sparse representations for bothwords and target-word weight vectors.
As we shallshow in Section 4, our treatment of compressingthe Prediction subnet does make sense andachieves high performance.Formally, the i-th output weight vector is esti-mated by?Wi=Dxi, (8)3W:,1:Bis the first B columns ofW .Figure 2: Compressing the output of neural LM.We apply NCE to estimate the parameters of thePrediction sub-network (dashed round rectan-gle).
The SpUnnrmProb layer outputs a sparse,unnormalized probability of the next word.
By?sparsity,?
we mean that, in NCE, the probabilityis computed for only the ?true?
next word (red)and a few generated negative samples.The biases can also be compressed as?bi= cxi.
(9)where xiis the sparse representation of the i-thword.
(It is shared in the compression of weightsand biases.
)In the above model, we have managed to com-pressed a language model whose number of pa-rameters is irrelevant to the vocabulary size.To better estimate a ?prior?
distribution ofwords, we may alternatively assign an indepen-dent bias to each word, i.e., b is not compressed.In this variant, the number of model parametersgrows very slowly and is also negligible becauseeach word needs only one extra parameter.
Exper-imental results show that by not compressing thebias vector, we can even improve the performancewhile compressing LMs.3.4 Noise-Contrastive Estimation withZRegressionWe adopt the noise-contrastive estimation (NCE)method to train our model.
Compared with themaximum likelihood estimation of softmax, NCEreduces computational complexity to a large de-gree.
We further propose the ZRegressionmechanism to stablize training.NCE generates a few negative samples for eachpositive data sample.
During training, we only230need to compute the unnormalized probability ofthese positive and negative samples.
Interestedreaders are referred to (Gutmann and Hyv?arinen,2012) for more information.Formally, the estimated probability of the wordwiwith history/context h isP (w|h;?)
=1ZhP0(wi|h;?
)=1Zhexp(s(wi, h;?
)), (10)where ?
is the parameters and Zhis a context-dependent normalization factor.
P0(wi|h;?)
isthe unnormalized probability of the w (given bythe SpUnnrmProb layer in Figure 2).The NCE algorithm suggests to take Zhas pa-rameters to optimize along with ?, but it is in-tractable for context with variable lengths or largesizes in language modeling.
Following Mnih andTeh (2012), we set Zh= 1 for all h in the basemodel (without ZRegression).The objective for each occurrence of con-text/history h isJ(?|h) = logP (wi|h;?
)P (wi|h;?)
+ kPn(wi)+k?j=1logkPn(wj)P (wj|h;?)
+ kPn(wj),where Pn(w) is the probability of drawing a nega-tive samplew; k is the number of negative samplesthat we draw for each positive sample.The overall objective of NCE isJ(?)
= Eh[J(?|h)] ?1MM?i=1J(?|hi),where hiis an occurrence of the context and M isthe total number of context occurrences.Although setting Zhto 1 generally works wellin our experiment, we find that in certain sce-narios, the model is unstable.
Experiments showthat when the true normalization factor is far awayfrom 1, the cost function may vibrate.
To com-ply with NCE in general, we therefore propose aZRegression layer to predict the normalizationconstant Zhdependent on h, instead of treating itas a constant.The regression layer is computed byZ?1h= exp(W>Zh+ bZ),Partitions Running wordsTrain (n-gram) 1.6 BTrain (neural LMs) 100 MDev 100 KTest 5 MTable 2: Statistics of our corpus.whereWZ?
RCand bZ?
R are weights and biasfor ZRegression.
Hence, the estimated proba-bility by NCE with ZRegression is given byP (w|h) = exp(s(h,w)) ?
exp(W>Zh+ bZ).Note that the ZRegression layer does notguarantee normalized probabilities.
During val-idation and testing, we explicitly normalize theprobabilities by Equation (1).4 EvaluationIn this part, we first describe our dataset in Subsec-tion 4.1.
We evaluate our learned sparse codes ofrare words in Subsection 4.2 and the compressedlanguage model in Subsection 4.3.
Subsection 4.4provides in-depth analysis of the ZRegressionmechanism.4.1 DatasetWe used the freely available Wikipedia4dump(2014) as our dataset.
We extracted plain sen-tences from the dump and removed all markups.We further performed several steps of preprocess-ing such as text normalization, sentence splitting,and tokenization.
Sentences were randomly shuf-fled, so that no information across sentences couldbe used, i.e., we did not consider cached languagemodels.
The resulting corpus contains about 1.6billion running words.The corpus was split into three parts for train-ing, validation, and testing.
As it is typically time-consuming to train neural networks, we sampled asubset of 100 million running words to train neu-ral LMs, but the full training set was used to trainthe backoff n-gram models.
We chose hyperpa-rameters by the validation set and reported modelperformance on the test set.
Table 2 presents somestatistics of our dataset.4.2 Qualitative Analysis of Sparse CodesTo obtain words?
sparse codes, we chose 8k com-mon words as the ?dictionary,?
i.e., B = 8000.4http://en.wikipedia.org231Figure 3: The sparse representations of selectedwords.
The x-axis is the dictionary of 8k commonwords; the y-axis is the coefficient of sparse cod-ing.
Note that algorithm, secret, and debate arecommon words, each being coded by itself with acoefficient of 1.We had 2k?42k uncommon words in different set-tings.
We first pretrained word embeddings ofboth rare and common words, and obtained 200dvectors U and w in Equation (5).
The dimensionwas specified in advance and not tuned.
As thereis no analytic solution to the objective, we opti-mized it by Adam (Kingma and Ba, 2014), whichis a gradient-based method.
To filter out small co-efficients around zero, we simply set a value to 0if it is less than 0.015 ?max{v ?
x}.
w?in Equa-tion (6) was set to 1 because we deemed fitting lossand sparsity penalty are equally important.
We setw?in Equation (7) to 0.1, and this hyperparameteris insensitive.Figure 3 plots the sparse codes of a few selectedwords.
As we see, algorithm, secret, and debateare common words, and each is (sparsely) codedby itself with a coefficient of 1.
We further noticethat a rare word like algorithms has a sparse rep-resentation with only a few non-zero coefficient.Moreover, the coefficient in the code of al-gorithms?corresponding to the base word algo-rithm?is large (?
0.6), showing that the wordsalgorithm and algorithms are similar.
Such phe-nomena are also observed with secret and debate.The qualitative analysis demonstrates that ourapproach can indeed learn a sparse code of a word,and that the codes are meaningful.4.3 Quantitative Analysis of CompressedLanguage ModelsWe then used the pre-computed sparse codes tocompress neural LMs, which provides quantita-tive analysis of the learned sparse representationsof words.
We take perplexity as the performancemeasurement of a language model, which is de-fined byPPL = 2?1N?Ni=1log2p(wi|hi)where N is the number of running words in thetest corpus.4.3.1 SettingsWe leveraged LSTM-RNN as the Encoding sub-net, which is a prevailing class of neural networksfor language modeling (Sundermeyer et al, 2015;Karpathy et al, 2015).
The hidden layer was 200d.We used the Adam algorithm to train our neuralmodels.
The learning rate was chosen by valida-tion from {0.001, 0.002, 0.004, 0.006, 0.008}.
Pa-rameters were updated with a mini-batch size of256 words.
We trained neural LMs by NCE, wherewe generated 50 negative samples for each pos-itive data sample in the corpus.
All our modelvariants and baselines were trained with the samepre-defined hyperparameters or tuned over a samecandidate set; thus our comparison is fair.We list our compressed LMs and competingmethods as follows.?
KN3.
We adopted the modified Kneser-Neysmoothing technique to train a 3-gram LM;we used the SRILM toolkit (Stolcke and oth-ers, 2002) in out experiment.?
LBL5.
A Log-BiLinear model introduced inMnih and Hinton (2007).
We used 5 preced-ing words as context.?
LSTM-s. A standard LSTM-RNN languagemodel which is applied in Sundermeyer et al(2015) and Karpathy et al (2015).
We im-plemented the LM ourselves based on Theano(Theano Development Team, 2016) and alsoused NCE for training.?
LSTM-z.
An LSTM-RNN enhanced withthe ZRegression mechanism described inSection 3.4.?
LSTM-z,wb.
Based on LSTM-z, we com-pressed word embeddings in Embeddingand the output weights and biases inPrediction.?
LSTM-z,w.
In this variant, we did not com-press the bias term in the output layer.
Foreach word in C, we assigned an independentbias parameter.4.3.2 PerformanceTables 3 shows the perplexity of our compressedmodel and baselines.
As we see, LSTM-basedLMs significantly outperform the log-bilinear232Vocabulary 10k 22k 36k 50kKN3?90.
4 125.3 146.4 159.9LBL5 116.
6 167.0 199.5 220.3LSTM-s 107.
3 159.5 189.4 222.1LSTM-z 75.
1 104.4 119.6 130.6LSTM-z,wb 73.
7 103.4 122.9 138.2LSTM-z,w 72.
9 101.9 119.3 129.2Table 3: Perplexity of our compressed languagemodels and baselines.
?Trained with the full cor-pus of 1.6 billion running words.Vocabulary 10k 22k 36k 50kLSTM-z,w 17.76 59.28 73.42 79.75LSTM-z,wb 17.80 59.44 73.61 79.95Table 4: Memory reduction (%) by our proposedmethods in comparison with the uncompressedmodel LSTM-z.
The memory of sparse codes areincluded.Figure 4: Fine-grained plot of performance(perplexity) and memory consumption (includingsparse codes) versus the vocabulary size.model as well as the backoff 3-gram LM, even ifthe 3-gram LM is trained on a much larger cor-pus with 1.6 billion words.
The ZRegressionmechanism improves the performance of LSTMto a large extent, which is unexpected.
Subsec-tion 4.4 will provide more in-depth analysis.Regarding the compression method proposedin this paper, we notice that LSTM-z,wb andLSTM-z,w yield similar performance to LSTM-z.In particular, LSTM-z,w outperforms LSTM-z inall scenarios of different vocabulary sizes.
More-over, both LSTM-z,wb and LSTM-z,w can reducethe memory consumption by up to 80% (Table 4).We further plot in Figure 4 the model perfor-mance (lines) and memory consumption (bars) ina fine-grained granularity of vocabulary sizes.
Wesee such a tendency that compressed LMs (LSTM-z,wb and LSTM-z,w, yellow and red lines) aregenerally better than LSTM-z (black line) whenwe have a small vocabulary.
However, LSTM-z,wb is slightly worse than LSTM-z if the vocabu-lary size is greater than, say, 20k.
The LSTM-z,wremains comparable to LSTM-z as the vocabularygrows.To explain this phenomenon, we may imaginethat the compression using sparse codes has twoeffects: it loses information, but it also enablesmore accurate estimation of parameters especiallyfor rare words.
When the second factor dominates,we can reasonably expect a high performance ofthe compressed LM.From the bars in Figure 4, we observe that tra-ditional LMs have a parameter space growing lin-early with the vocabulary size.
But the numberof parameters in our compressed models does notincrease?or strictly speaking, increases at an ex-tremely small rate?with vocabulary.These experiments show that our method canlargely reduce the parameter space with even per-formance improvement.
The results also verifythat the sparse codes induced by our model indeedcapture meaningful semantics and are potentiallyuseful for other downstream tasks.4.4 Effect of ZRegressionWe next analyze the effect of ZRegression forNCE training.
As shown in Figure 5a, the trainingprocess becomes unstable after processing 70% ofthe dataset: the training loss vibrates significantly,whereas the test loss increases.We find a strong correlation between unsta-bleness and the Zhfactor in Equation (10), i.e.,the sum of unnormalized probability (Figure 5b).Theoretical analysis shows that theZhfactor tendsto be self-normalized even though it is not forcedto (Gutmann and Hyv?arinen, 2012).
However,problems would occur, should it fail.In traditional methods, NCE jointly estimatesnormalization factor Z and model parameters(Gutmann and Hyv?arinen, 2012).
For languagemodeling, Zhdependents on context h. Mnihand Teh (2012) propose to estimate a separate Zhbased on two history words (analogous to 3-gram),but their approach hardly scales to RNNs becauseof the exponential number of different combina-tions of history words.We propose the ZRegression mechanism inSection 3.4, which can estimate the Zhfactor well(Figure 5d) based on the history vector h. Inthis way, we manage to stabilize the training pro-cess (Figure 5c) and improve the performance by233(a) Training/test loss vs. training time w/oZRegression.
(b) The validation perplexity and normalization factor Zhw/oZRegression.
(c) Training loss vs. training time w/ZRegression of different runs.
(d) The validation perplexity and normalization factor Zhw/ZRegression.Figure 5: Analysis of ZRegression.a large margin, as has shown in Table 3.It should be mentioned that ZRegression isnot specific to model compression and is generallyapplicable to other neural LMs trained by NCE.5 ConclusionIn this paper, we proposed an approach to repre-sent rare words by sparse linear combinations ofcommon ones.
Based on such combinations, wemanaged to compress an LSTM language model(LM), where memory does not increase with thevocabulary size except a bias and a sparse codefor each word.
Our experimental results also showthat the compressed LM has yielded a better per-formance than the uncompressed base LM.AcknowledgmentsThis research is supported by the National Ba-sic Research Program of China (the 973 Pro-gram) under Grant No.
2015CB352201, the Na-tional Natural Science Foundation of China underGrant Nos.
61232015, 91318301, 61421091 and61502014, and the China Post-Doctoral Founda-tion under Grant No.
2015M580927.ReferencesJacob Andreas and Dan Klein.
2014.
When and whyare log-linear models self-normalizing.
In Proceed-ings of the Annual Meeting of the North AmericanChapter of the Association for Computational Lin-guistics, pages 244?249.Yoshua Bengio and Jean-S?ebastien Sen?ecal.
2003.Quick training of probabilistic neural nets by im-portance sampling.
In Proceedings of the Ninth In-ternational Workshop on Artificial Intelligence andStatistics.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Cristian Bucilu?a, Rich Caruana, and AlexandruNiculescu-Mizil.
2006.
Model compression.
InProceedings of the 12th ACM SIGKDD Interna-tional Conference on Knowledge Discovery andData Mining, pages 535?541.Welin Chen, David Grangier, and Michael Auli.
2015.Strategies for training large vocabulary neural lan-guage models.
arXiv preprint arXiv:1512.04906.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard M Schwartz, and John Makhoul.2014.
Fast and robust neural network joint modelsfor statistical machine translation.
In Proceedingsof the 52rd Annual Meeting of the Association forComputational Linguistics, pages 1370?1380.234Manaal Faruqui, Yulia Tsvetkov, Dani Yogatama, ChrisDyer, and Noah A. Smith.
2015.
Sparse overcom-plete word vector representations.
In Proceedingsof the 53rd Annual Meeting of the Association forComputational Linguistics, pages 1491?1500.Yunchao Gong, Liu Liu, Ming Yang, and LubomirBourdev.
2014.
Compressing deep convolutionalnetworks using vector quantization.
arXiv preprintarXiv:1412.6115.Michael Gutmann and Aapo Hyv?arinen.
2012.
Noise-contrastive estimation of unnormalized statisticalmodels, with applications to natural image statis-tics.
The Journal of Machine Learning Research,13(1):307?361.Zhanying He, Chun Chen, Jiajun Bu, Can Wang, LijunZhang, Deng Cai, and Xiaofei He.
2012.
Documentsummarization based on data reconstruction.
In Pro-ceedings of the 26th AAAI Conference on ArtificialIntelligence, pages 620?626.Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
2015.Distilling the knowledge in a neural network.
arXivpreprint arXiv:1503.02531.Max Jaderberg, Andrea Vedaldi, and Andrew Zisser-man.
2014.
Speeding up convolutional neural net-works with low rank expansions.
In Proceedings ofthe British Machine Vision Conference.S?ebastien Jean, Kyunghyun Cho, Roland Memisevic,and Yoshua Bengio.
2014.
On using very large tar-get vocabulary for neural machine translation.
arXivpreprint arXiv:1412.2007.Dan Jurafsky and James H. Martin.
2014.
Speech andLanguage Processing.
Pearson.Andrej Karpathy, Justin Johnson, and Fei-Fei Li.
2015.Visualizing and understanding recurrent networks.arXiv preprint arXiv:1506.02078.Diederik P Kingma and Jimmy Ba.
2014.
Adam: Amethod for stochastic optimization.
arXiv preprintarXiv:1412.6980.Honglak Lee, Alexis Battle, Rajat Raina, and An-drew Y Ng.
2006.
Efficient sparse coding algo-rithms.
In Advances in Neural Information Process-ing Systems, pages 801?808.Omer Levy and Yoav Goldberg.
2014.
Linguistic reg-ularities in sparse and explicit word representations.In Proceedings of the Eighteenth Conference on Nat-ural Language Learning, pages 171?180.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH, pages 1045?1048.Tomas Mikolov, Anoop Deoras, Daniel Povey, LukasBurget, and Jan Cernock?y.
2011.
Strategies fortraining large scale neural network language models.In Proceedings of the IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 196?201.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In Proceedings of the 24th International Conferenceon Machine learning, pages 641?648.Andriy Mnih and Yee-Whye Teh.
2012.
A fast andsimple algorithm for training neural probabilisticlanguage models.
arXiv preprint arXiv:1206.6426.Fr?ederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the International Workshop on Arti-ficial Intelligence and Statistics, pages 246?252.Lili Mou, Ge Li, Yan Xu, Lu Zhang, and Zhi Jin.2015a.
Distilling word embeddings: An encodingapproach.
arXiv preprint arXiv:1506.04488.Lili Mou, Rui Yan, Ge Li, Lu Zhang, and Zhi Jin.2015b.
Backward and forward language modelingfor constrained natural language generation.
arXivpreprint arXiv:1512.06612.Alexander M Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing, pages 379?389.Alessandro Sordoni, Michel Galley, Michael Auli,Chris Brockett, Yangfeng Ji, Margaret Mitchell,Jian-Yun Nie, Jianfeng Gao, and Bill Dolan.
2015.A neural network approach to context-sensitive gen-eration of conversational responses.
In Proceed-ings of the 2015 Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages196?205.Andreas Stolcke et al 2002.
SRILM?An extensi-ble language modeling toolkit.
In INTERSPEECH,pages 901?904.Martin Sundermeyer, Hermann Ney, and Ralf Schl?uter.2015.
From feedforward to recurrent LSTM neuralnetworks for language modeling.
IEEE/ACM Trans-actions on Audio, Speech and Language Processing,23(3):517?529.Theano Development Team.
2016.
Theano: A Pythonframework for fast computation of mathematical ex-pressions.
arXiv preprint arXiv:1605.02688.Meng Yang, Lei Zhang, Jian Yang, and David Zhang.2011.
Robust sparse coding for face recognition.
InProceedings of the 2011 IEEE Conference on Com-puter Vision and Pattern Recognition, pages 625?632.235
