Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1942?1952,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSearch-Aware Tuning for Machine TranslationLemao LiuQueens CollegeCity University of New Yorklemaoliu@gmail.comLiang HuangQueens College and Graduate CenterCity University of New Yorkliang.huang.sh@gmail.comAbstractParameter tuning is an important problem instatistical machine translation, but surpris-ingly, most existing methods such as MERT,MIRA and PRO are agnostic about search,while search errors could severely degradetranslation quality.
We propose a search-aware framework to promote promising par-tial translations, preventing them from be-ing pruned.
To do so we develop two met-rics to evaluate partial derivations.
Our tech-nique can be applied to all of the threeabove-mentioned tuning methods, and ex-tensive experiments on Chinese-to-Englishand English-to-Chinese translation show upto +2.6 BLEU gains over search-agnosticbaselines.1 IntroductionParameter tuning has been a key problem for ma-chine translation since the statistical revolution.However, most existing tuning algorithms treat thedecoder as a black box (Och, 2003; Hopkins andMay, 2011; Chiang, 2012), ignoring the fact thatmany potentially promising partial translations arepruned by the decoder due to the prohibitivelylarge search space.
For example, the popularbeam-search decoding algorithm for phrase-basedMT (Koehn, 2004) only explores O(nb) items fora sentence of n words (with a beam width of b),while the full search space is O(2nn2) or worse(Knight, 1999).As one of the very few exceptions to the?search-agnostic?
majority, Yu et al.
(2013) andZhao et al.
(2014) propose a variant of the per-ceptron algorithm that learns to keep the refer-ence translations in the beam or chart.
How-ever, there are several obstacles that prevent theirmethod from becoming popular: First of all, theyrely on ?forced decoding?
to track gold derivationsthat lead to the reference translation, but in practiceonly a small portion of (mostly very short) sen-(a)0 1 2 3 4(b)Figure 1: (a) Some potentially promising partial trans-lations (in red) fall out of the beam (bin 2); (b) Weidentify such partial translations and assign them highermodel scores so that they are more likely to survive thesearch.tence pairs have at least one such derivation.
Sec-ondly, they learn the model on the training set, andwhile this does enable a sparse feature set, it is or-ders of magnitude slower compared to MERT andPRO.We instead propose a very simple framework,search-aware tuning, which does not depend onforced decoding, and thus can be trained on all sen-tence pairs of any dataset.
The key idea is that,besides caring about the rankings of the completetranslations, we also promote potentially promis-ing partial translations so that they are more likelyto survive throughout the search, see Figure 1 forillustration.
We make the following contributions:?
Our idea of search-aware tuning can be ap-plied (as a patch) to all of the three mostpopular tuning methods (MERT, PRO, andMIRA) by defining a modified objective func-tion (Section 4).?
To measure the ?promise?
or ?potential?
of apartial translation, we define a new concept?potential BLEU?
inspired by future cost inMT decoding (Koehn, 2004) and heuristics inA* search (Hart et al., 1968) (Section 3.2).This work is the first study of evaluating met-rics for partial translations.?
Our method obtains substantial and consistent1942improvements on both the large-scale NISTChinese-to-English and English-to-Chinesetranslation tasks on top of MERT, MIRA, andPRO baselines.
This is the first time that con-sistent improvements can be achieved with anew learning algorithm under dense featuresettings (Section 5).For simplicity reasons, in this paper we usephrase-based translation, but our work has the po-tential to be applied to other translation paradigms.2 Review: Beam Search for PBMTDecodingWe review beam search for phrase-based decodingin our notations which will facilitate the discussionof search-aware tuning in Section 4.
Following Yuet al.
(2013), let ?x, y?
be a Chinese-English sen-tence pair in the tuning set D, andd = r1?
r2?
.
.
.
?
r|d|be a (partial) derivation, where each ri=?c(ri), e(ri)?
is a rule, i.e., a phrase-pair.
Let |c(r)|be the number of Chinese words in rule r, ande(d)?= e(r1) ?
e(r2) .
.
.
?
e(r|d|) be the Englishprefix (i.e., partial translation) generated so far.In beam search, each binBi(x) contains the bestk derivations covering exactly i Chinese words,based on items in previous bins (see Figures 1and 2):B0(x) = {}Bi(x) = topkw0(?j=1..i{d ?
r | d?Bi?j(x), |c(r)|=j})where r is a rule covering j Chinese words, andtopkw0(?)
returns the top k derivations accordingto the current model w0.
As a special case, notethat top1w0(S) = argmaxd?Sw0?
?
(d), sotop1w0(B|x|(x)) is the final 1-best result.1See Fig-ure 2 for an illustration.3 Challenge: Evaluating PartialDerivationsAs mentioned in Section 1, the current mainstreamtuning methods such as MERT, MIRA, and PRO are1Actually B|x|(x) is an approximation to the k-best listsince some derivations are merged by dynamic programming;to recover those we can use Alg.
3 of Huang and Chiang(2005).0 1 2 3 4B0(x) B1(x) B2(x) B3(x) B4(x)Figure 2: Beam search for phrase-based decoding.
Theitem in red is top1w0(B4(x)), i.e., the 1-best result.Traditional tuning only uses the final bin B4(x) whilesearch-aware tuning considers all binsBi(x) (i = 1..4).all search-agnostic: they only care about the com-plete translations from the last bin, B|x|(x), ignor-ing all partial ones, i.e., Bi(x) for all i < |x|.
Asa result, many potentially promising partial deriva-tions never reach the final bin (See Figure 1).To address this problem, our new ?search-awaretuning?
aims to promote not only the accuratetranslations in the final bin, but more importantlythose potentially promising partial derivations innon-final bins.
The key challenge, however, ishow to evaluate the ?promise?
or ?potential?
ofa partial derivation.
In this Section, we developtwo such measures, a simple ?partial BLEU?
(Sec-tion 3.1) and a more principled ?potential BLEU?
(Section 3.2).
In Section 4, we will then adapt tra-ditional tuning methods to their search-aware ver-sions using these partial evaluation metrics.3.1 Solution 1: Simple and Naive Partial BLEUInspired by a trick in (Li and Khudanpur, 2009)and (Chiang, 2012) for oracle or hope extraction,we use a very simple metric to evaluate partialtranslations for tuning.
For a given derivation d,the basic idea is to evaluate the (short) partial trans-lation e(d) against the (full) reference y, but usinga ?prorated?
reference length proportional to c(d)which is the number of Chinese words covered sofar in d:|y| ?
|c(d)|/|x|For example, if d has covered 2 words on a 8-word Chinese sentence with a 12-word Englishreference, then the ?effective reference length?
is12?2/8 = 3.
We call this method ?partial BLEU?since it does not complete the translation, and de-note it by?
?|x|y(d) = ??
(y, e(d); reflen = |y| ?
|c(d)|/|x|).(1)1943?
(y, y?)
= ?Bleu+1(y, y?)
string distance metric?y(d) = ?
(y, e(d)) full derivations eval?xy(d) ={?
?|x|y(d) partial bleu (Sec.
3.1)?
(y, e?x(d)) potential bleu (Sec.
3.2)Table 1: Notations for evaluating full and partial deriva-tions.
Functions??|x|y(?)
and e?x(?)
are defined by Equa-tions 1 and 3, respectively.where reflen is the effective length of referencetranslations, see (Papineni et al., 2002) for details.3.1.1 Problem with Partial BLEUSimple as it is, this method does not work well inpractice because comparison of partial derivationsmight be unfair for different derivations coveringdifferent set of Chinese words, as it will naturallyfavor those covering ?easier?
portions of the in-put sentence (which we do observe empirically).For instance, consider the following Chinese-to-English example which involves a reordering ofthe Chinese PP:(2) w?oIc?ongfromSh`angh?aiShanghaif?eiflyd`aotoB?eij?
?ngBeijing?I flew from Shanghai to Beijing?Partial BLEU will prefer subtranslation ?I from?
to?I fly?
in bin 2 (covering 2 Chinese words) becausethe former has 2 unigram mathces while the latteronly 1, even though the latter is almost identicalto the reference and will eventually lead to a com-plete translation with substantially higher Bleu+1score (matching a 4-gram ?from Shanghai to Bei-jing?).
Similarly, it will prefer ?I from Shanghai?to ?I fly from?
in bin 3, without knowing that theformer will eventually pay the price of word-orderdifference.
This example suggests that we need amore ?global?
or less greedy metric (see below).3.2 Solution 2: Potential BLEU via ExtensionInspired by future cost computation in MT decod-ing (Koehn, 2004), we define a very simple fu-ture string by simply concatenating the best model-score translation (with no reorderings) in each un-covered span.
Let bestw(x[i:j]) denote the bestmonotonic derivation for span [i : j], thenfuture(d, x) = ?
[i:j]?uncov(d,x)e(bestw(x[i:j]))where ?
is the concatenation operator anduncov(d, x) returns an ordered list of uncoverede(d) future(d, x)x =e?x(d) =monotonicreordering Figure 3: Example of the extension function e?x(?)
(andfuture string) on an incomplete derivation d.spans of x.
See Figure 3 for an example.
This fu-ture string resembles (inadmissible) heuristic func-tion (Hart et al., 1968).
Now the ?extended trans-lation?
is simply a concatenation of the exist-ing partial translation e(d) and the future stringfuture(d, x):e?x(d) = e(d) ?
future(d, x).
(3)Instead of calculating bestw(x[i:j]) on-the-flyfor each derivation d, we can precompute it foreach span [i : j] during future-cost computa-tion, since the score of bestw(x[i:j]) is context-free (Koehn, 2004).
Algorithm 1 shows thepseudo-code of computing bestw(x[i:j]).
In prac-tice, since future-cost precomputation alreadysolves the best (monotonic) model-score for eachspan, is the only extra work for potential BLEUis to record (for each span) the subtranslation thatachieves that best score.
Therefore, the extra timefor potential BLEU is negligible (the time com-plexity is O(n2), but just as in future cost, the con-stant is much smaller than real decoding).
The im-plementation should require minimal hacking on aphrase-based decoder (such as Moses).To summarize the notation, we use ?xy(d) todenote a generic evaluation function for par-tial derivation d, which could be instantiated intwo ways, partial bleu (?
?|x|y(d)) or potential bleu(?
(y, e?x(d))).
See Table 1 for details.
The nextSection will only use the generic notation ?xy(d).Finally, it is important to note that althoughboth partial and potential metrics are not BLEU-specific, the latter is much easier to adapt to othermetrics such as TER since it does not change theoriginal Bleu+1definition.
By contrast, it is notclear to us at all how to generalize partial BLEU topartial TER.4 Search-Aware MERT, MIRA, and PROParameter tuning aims to optimize the weight vec-tor w so that the rankings based on model score de-fined by w is positively correlated with those based1944Algorithm 1 Computation of best Translations for Potential BLEU.Input: Source sentence x, a rule set < for x, and w.Output: Best translations e(bestw(x[i : j])) for all spans [i : j].1: for l in (0..|x|) do2: for i in (0..|x| ?
l) do3: j = i+ l + 14: best score = ?
?5: if <[i : j] 6= ?
then .
<[i : j] is a subset of rules < for span [i : j].6: bestw(x[i : j]) = argmaxr?<[i:j]w ??
({r}) .
{r} is a derivation consisting of one rule r.7: best score = w ??
(bestw(x[i : j]))8: for k in (i+ 1 .. i+ p) do .
p is the phrase length limit9: if best score < w ??
(bestw(x[i : k]) ?
bestw(x[k : j]))then10: bestw(x[i : j]) = bestw(x[i : k]) ?
bestw(x[k : j])11: best score = w ??
(bestw(x[i : j]))on some translation metric (such as BLEU (Pap-ineni et al., 2002)).
In other words, for a train-ing sentence pair ?x, y?, if a pair of its trans-lations y1= e(d1) and y2= e(d2) satisfiesBLEU(y, y1) > BLEU(y, y2), then we expect w ??
(d1) > w ??
(d2) to hold after tuning.4.1 From MERT to Search-Aware MERTSuppose D is a tuning set of ?x, y?
pairs.
Tra-ditional MERT learns the weight by iterativelyreranking the complete translations towards thosewith higher BLEU in the final bin B|x|(x) foreach x in D. Formally, it tries to minimize thedocument-level error of 1-best translations:`MERT(D,w) =??x,y?
?D?y(top1w(B|x|(x))),(4)where top1w(S) is the best derivation in S undermodel w, and ??(?)
is the full derivation metric asdefined in Table 1; in this paper we use ?y(y?)
=?BLEU(y, y?).
Here we follow Och (2003) andLopez (2008) to simplify the notations, where the?
operator (similar to?)
is an over-simplificationfor BLEU which, as a document-level metric, is ac-tually not factorizable across sentences.Besides reranking the complete translations astraditional MERT, our search-aware MERT (SA-MERT) also reranks the partial translations suchthat potential translations may survive in the mid-dle bins during search.
Formally, its objectivefunction is defined as follows:`SA-MERT(D,w)=??x,y?
?D?i=1..|x|?xy(top1w(Bi(x)))(5)where top1w(?)
is defined in Eq.
(4), and ?xy(d),defined in Table 1, is the generic metric for eval-uating a partial derivation d which has two imple-mentations (partial bleu or potential bleu).
In or-der words we can obtain two implementations ofsearch-aware MERT methods, SA-MERTparandSA-MERTpot.Notice that the traditional MERT is a specialcase of SA-MERT where i is fixed to |x|.4.2 From MIRA to Search-Aware MIRAMIRA is another popular tuning method for SMT.It firstly introduced in (Watanabe et al., 2007), andthen was improved in (Chiang et al., 2008; Chiang,2012; Cherry and Foster, 2012).
Its main idea is tooptimize a weight such that the model score dif-ference of a pair of derivations is greater than theirloss difference.In this paper, we follow the objective functionin (Chiang, 2012; Cherry and Foster, 2012), whereonly the violation between hope and fear deriva-tions is concerned.
Formally, we define d+(x, y)and d?
(x, y) as the hope and fear derivations inthe final bin (i.e., complete derivations):d+(x, y) = argmaxd?B|x|(x)w0??(d)?
?y(d) (10)d?
(x, y) = argmaxd?B|x|(x)w0??
(d) + ?y(d) (11)where w0is the current model.
The loss functionof MIRA is in Figure 4.
The update will be be-tween d+(x, y) and d?
(x, y).To adapt MIRA to search-aware MIRA (SA-MIRA), we need to extend the definitions of hope1945`MIRA(D,w) =12C?w?w0?2+??x,y??D[?
?y(d+(x, y), d?
(x, y))?w???
(d+(x, y), d?
(x, y))]+(6)`SA-MIRA(D,w)=12C?w?w0?2+??x,y??D|x|?i=1[?
?xy(d+i(x, y), d?i(x, y))?w???
(d+i(x, y), d?i(x, y))]+(7)`PRO(D,w) =??x,y?
?D?d1,d2?B|x|(x), ?
?y(d1,d2)>0log(1 + exp(?w???
(d1, d2)))(8)`SA-PRO(D,w) =??x,y?
?D|x|?i=1?d1,d2?Bi(x), ?
?xy(d1,d2)>0log(1 + exp(?w???
(d1, d2)))(9)Figure 4: Loss functions of MIRA, SA-MIRA, PRO, and SA-PRO.
The differences between traditional and search-aware versions are highlighted in gray.
The hope and fear derivations are defined in Equations 10?13, and wedefine ?
?y(d1, d2) = ?y(d1)?
?y(d2), and ?
?xy(d1, d2) = ?xy(d1)?
?xy(d2).
In addition, [?
]+= max{?, 0}.and fear derivations from the final bin to all bins:d+i(x, y) = argmaxd?Bi(x)w0??(d)?
?y(d) (12)d?i(x, y) = argmaxd?Bi(x)w0??
(d) + ?y(d) (13)The new loss function for SA-MIRA is Eq.
7 inFigure 4.
Now instead of one update per sentence,we will perform |x| updates, each based on a paird+i(x, y) and d?i(x, y).4.3 From PRO to Search-Aware PROFinally, the PRO algorithm (Hopkins and May,2011; Green et al., 2013) aims to correlate theranking under model score and the ranking un-der BLEU score, among all complete derivationsin the final bin.
For each preference-pair d1, d2?B|x|(x) such that d1has a higher BLEU score thand2(i.e., ?y(d1) < ?y(d2)), we add one positive ex-ample ?
(d1) ?
?
(d2) and one negative example?(d2)??
(d1).Now to adapt it to search-aware PRO (SA-PRO), we will have many more examples to con-sider: besides the final bin, we will include allpreference-pairs in the non-final bins as well.
Foreach bin Bi(x), for each preference-pairs d1, d2?Bi(x) such that d1has a higher partial or potentialBLEU score than d2(i.e., ?xy(d1) < ?xy(d2)), weadd one positive example ?(d1)??
(d2) and onenegative example ?(d2)??(d1).
In sum, search-aware PRO has |x| times more examples than tradi-tional PRO.
The loss functions of PRO and search-aware PRO are defined in Figure 4.5 ExperimentsWe evaluate our new tuning methods on two largescale NIST translation tasks: Chinese-to-English(CH-EN) and English-to-Chinese (EN-CH) tasks.5.1 System Preparation and DataWe base our experiments on Cubit2(Huang andChiang, 2007), a state-of-art phrase-based systemin Python.
We set phrase-limit to 7, beam size to30 and distortion limit 6.
We use the 11 densefeatures from Moses (Koehn et al., 2007), whichcan lead to good performance and are widely usedin almost all SMT systems.
The baseline tuningmethods MERT (Och, 2003), MIRA (Cherry andFoster, 2012), and PRO (Hopkins and May, 2011)are from the Moses toolkit, which are batch tuningmethods based on k-best translations.
The search-aware tuning methods are called SA-MERT, SA-MIRA, and SA-PRO, respectively.
Their partialBLEU versions are marked with superscript1andtheir potential BLEU versions are marked with su-perscript2, as explained in Section 3.
All thesesearch-aware tuning methods are implemented onthe basis of Moses toolkit.
They employ the de-2http://www.cis.upenn.edu/?lhuang3/cubit/1946Methods nist03 nist04 nist05 nist06 nist08 avgMERT 33.6 35.1 33.4 31.6 27.9 ?SA-MERTpar-0.2 +0.0 +0.1 -0.1 -0.1 ?SA-MERTpot+0.8 +1.1 +0.9 +1.7 +1.5 +1.2MIRA 33.5 35.2 33.5 31.6 27.6 ?SA-MIRApar+0.3 +0.3 +0.4 +0.4 +0.6 ?SA-MIRApot+1.3 +1.6 +1.4 +2.2 +2.6 +1.8PRO 33.3 35.1 33.3 31.1 27.5 ?
?SA-PROpar-2.0 -2.7 -2.2 -1.0 -1.7 ?
?SA-PROpot+0.8 +0.5 +1.0 +1.6 +1.6 +1.1Table 2: CH-EN task: BLEU scores on test sets (nist03, nist04, nist05, nist06, and nist08).par: partial BLEU;pot:potential BLEU.?
: SA-PRO tunes on only 109 short sentences (with less than 10 words) from nist02.Final bin All binsMERT 35.5 28.2SA-MERT -0.1 +3.1Table 3: Evaluation on nist02 tuning set using twomethods: BLEU is used to evaluate 1-best completetranslations in the final bin; while potential BLEU isused to evaluate 1-best partial translations in all bins.The search-aware objective cares about (the potentialof) all bins, not just the final bin, which can explain thisresult.fault settings following Moses toolkit: for MERTand SA-MERT, the stop condition is defined by theweight difference threshold; for MIRA, SA-MIRA,PRO and SA-PRO, their stop condition is definedby max iteration set to 25; for all tuning methods,we use the final weight for testing.The training data for both CH-EN and EN-CHtasks is the same, and it is collected from theNIST2008 Open Machine Translation Campaign.It consists of about 1.8M sentence pairs, includingabout 40M/48M words in Chinese/English sides.For CH-EN task, the tuning set is nist02 (878sents), and test sets are nist03 (919 sents), nist04(1788 sents), nist05 (1082 sents), nist06 (616 sentsfrom news portion) and nist08 (691 from news por-tion).
For EN-CH task, the tuning set is ssmt07(995 sents)3, and the test set is nist08 (1859 sents).For both tasks, all the tuning and test sets contain4 references.We use GIZA++ (Och and Ney, 2003) for wordalignment, and SRILM (Stolcke, 2002) for 4-gramlanguage models with the Kneser-Ney smoothing3On EN-CH task, there is only one test set available for us,and thus we use ssmt07 as the tuning set, which is providedat the Third Symposium on Statistical Machine Translation(http://mitlab.hit.edu.cn/ssmt2007.html).option.
The LM for EN-CH is trained on its targetside; and that for CH-EN is trained on the Xin-hua portion of Gigaword.
We use BLEU-4 (Pap-ineni et al., 2002) with ?average ref-len?
to evalu-ate the translation performance for all experiments.In particular, the character-based BLEU-4 is em-ployed for EN-CH task.
Since all tuning meth-ods involve randomness, all scores reported are av-erage of three runs, as suggested by Clark et al.
(2011) for fairer comparisons.5.2 Main Results on CH-EN TaskTable 2 depicts the main results of our methods onCH-EN translation task.
On all five test sets, ourmethods consistently achieve substantial improve-ments with two pruning options: SA-MERTpotgains +1.2 BLEU points over MERT on average;and SA-MIRApotgains +1.8 BLEU points overMIRA on average as well.
SA-PROpot, however,does not work out of the box when we use the en-tire nist02 as the tuning set, which might be at-tributed to the ?Monster?
behavior (Nakov et al.,2013).
To alleviate this problem, we only use the109 short sentences with less than 10 words fromnist02 as our new tuning data.
To our supurise,this trick works really well (despite using muchless data), and also made SA-PROpotan order ofmagnitude faster.
This further confirms that oursearch-aware tuning is consistent across all tuningmethods and datasets.As discussed in Section 3, evaluation metricsof partial derivations are crucial for search-awaretuning.
Besides the principled ?potential BLEU?version of search-aware tuning (i.e.
SA-MERTpot,SA-MIRApot, and SA-PROpot), we also run thesimple ?partial BLEU?
version of search-awaretuning (i.e.
SA-MERTpar, SA-MIRApar, and SA-19473031323334351  2  4  8  16  32  64BLEUBeam SizeTraditional MERT TuningSearch-aware MERT TuningFigure 5: BLEU scores against beam size on nist05.Our search-aware tuning can achieve (almost) the sameBLEU scores with much smaller beam size (beam of 4vs.
16).methods nist02 nist051-bestMERT 35.5 33.4SA-MERT -0.1 +0.9OracleMERT 44.3 41.1SA-MERT +0.5 +1.6Table 4: The k-best oracle BLEU comparison betweenMERT and SA-MERT.PROpar).
In Table 2, we can see that they mayachieve slight improvements over tradition tuningon some datasets, but SA-MERTpot, SA-MIRApot,and SA-PROpotusing potential BLEU consistentlyoutperform them on all the datasets.Even though our search-aware tuning gains sub-stantially on all test sets, it does not gain signif-icantly on nist02 tuning set.
The main reason isthat, search-aware tuning optimizes an objective(i.e.
BLEU for all bins) which is different fromthe objective for evaluation (i.e.
BLEU for the finalbin), and thus it is not quite fair to evaluate thecomplete translations for search-aware tuning asthe same done for traditional tuning on the tuningset.
Actally, if we evaluate the potential BLEU forall partial translations, we find that search-awaretuning gains about 3.0 BLEU on nist02 tuning set,as shown in Table 3.5.3 Analysis on CH-EN TaskDifferent beam size.
Since our search-aware tun-ing considers the rankings of partial derivationsin the middle bins besides complete ones in thelast bin, ideally, if the weight learned by search-aware tuning can exactly evaluate partial deriva-Diversity nist02 nist05MERT 0.216 0.204SA-MERT 0.227 0.213Table 5: The diversity comparison based on the k-bestlist in the final bin on both tuning and nist05 test setsby tuning methods.
The higher the metric is, the morediverse the k-best list is.tions, then accurate partial derivations will rankhigher according to model score.
In this way, evenwith small beam size, these accurate partial deriva-tions may still survive in the bins.
Therefore, itis expected that search-aware tuning can achievegood performance with smaller beam size.
Tojustify our conjecture, we run SA-MERTpotwithdifferent beam size (2,4,8,16,30,100), its testingresults on nist05 are depicted in Figure 5. ourmehtods achieve better trade-off between perfor-mance and efficiey.
Figure 5 shows that search-aware tuning is consistent with all beam sizes, andas a by-product, search-aware MERT with a beamof 4 can achieve almost identical BLEU scores toMERT with beam of 16.Oracle BLEU.
In addition, we examine the BLEUponits of oracle for MERT and SA-MERT.
Weuse the weight tuned by MERT and SA-MERT fork-best decoding on nist05 test set, and calculatethe oracle over these two k-best lists.
The oracleBLEU comparison is shown in Table 4.
On nist05test set, for MERT the oracle BLEU is 41.1; whilefor SA-MERT its oracle BLEU is 42.7, i.e.
with 1.6BLEU improvements.
Although search-aware tun-ing employs the objective different from the objec-tive of evaluation on nist02 tuning set, it still gains0.5 BLEU improvements.Diversity.
A k-best list with higher diversity canbetter represent the entire decoding space, and thustuning on such a k-best list may lead to bettertesing performance (Gimpel et al., 2013).
Intu-itively, tuning with all bins will encourage the di-versity in prefix, infix and suffix of complete trans-lations in the final bin.
To testify this, we need adiversity metric.Indeed, Gimpel et al.
(2013) define a diversitymetric based on the n-gram matches between twosentences y and y?as follows:d(y, y?)
= ?|y|?q?i=1|y?|?q?j=1[[yi:i+q= y?j:j+q]]1948Methodstuning set test sets (4-refs)set # refs # sents # words nist03 nist04 nist05 nist06 nist08MERT nist02 4 878 23181 33.6 35.1 33.4 31.6 27.9SA-MERTpotnist02 4 878 23181 34.4 36.2 34.3 33.3 29.4MAXFORCE nist02-px 1 434 6227 29.0 30.3 28.7 26.8 24.1MAXFORCE train-r-part 1 1225 22684 31.7 33.5 31.5 30.3 26.7MERT nist02-r 1 92 1173 31.6 32.7 31.3 29.3 25.9SA-MERTpotnist02-r 1 92 1173 33.5 35.0 33.4 31.5 28.0Table 6: Comparisons with MAXFORCE in terms of BLEU.
nist02-px is the non-trivial reachable prefix-data fromnist02 via forced decoding; nist02-r is a subset of nist02-px consisting of the fully reachable data; train-r is asubset of fully reachable data from training data that is comparable in size to nist02.
All experiments use onlydense features.where q = n?
1, and [[x]] equals to 1 if x is true, 0otherwise.
This metric, however, has the followingcritical problems:?
it is not length-normalized: longer strings willlook as if they are more different.?
it suffers from duplicates in n-grams.
Af-ter normalization, d(y, y) will exceed -1 forany y.
In the extreme case, consider y1=?the the the the?
and y2= ?the ... the?
with10 the?s then will be considered identical af-ter normalization by length.So we define a balanced metric based on their met-ricd?
(y, y?)
= 1?2?
d(y, y?
)d(y, y) + d(y?, y?
)which satisfies the following nice properties:?
d?
(y, y) = 0 for all y;?
0 ?
d?
(y, y?)
?
1 for all y, y?;?
d?
(y, y?)
= 1 if y and y?is completely dis-joint.?
it does not suffer from duplicates, and can dif-ferentiate y1and y2defined above.With this new metric, we evaluate the diversityof k-best lists for both MERT and SA-MERT.
Asshown in Table 5, on both tuning and test sets thek-best list generated by SA-MERT is more diverse.5.4 Comparison with Max-ViolationPerceptronOur method considers the rankings of partialderivations, which is simlar to MAXFORCEB`ush??
y?u Sh?al?ong j?ux?
?ng hu`?t?anBush and Sharon held a meetingBush held talks with Sharonqi?angsh?ou b`ei j?
?ngf?ang j?
?b`?police killed the gunmanthe gunman was shot dead?B`ush??
y?u Sh?al?ong j?ux?
?ng hu`?t?an Bush and Sharon held a meetingB`ush??
y?u Sh?al?ong j?ux?
?ng hu`?t?an Bush held talks with Sharonqi?angsh?ou b`ei j?
?ngf?ang j??b`?
police killed the gunmanqi?angsh?ou b`ei j?
?ngf?ang j??b`?
the gunman was shot deadFigure 6: Transformation of a tuning set in forced de-coding for MAXFORCE: the original tuning set (on thetop) contains 2 source sentences with 2 references foreach; while the transformed set (on the bottom) con-tains 4 source sentences with one reference for each.method (Yu et al., 2013), and thus we re-implement MAXFORCE method.
Since the nist02tuning set contains 4 references and forced decod-ing is performed for only one reference, we enlargethe nist02 set to a variant set following the trans-formation in Figure 6, and obtain a variant tun-ing set denoted as nist02-px, which consists of 4-times sentence-pairs.
On nist02-px, the non-trivialreachable prefix-data only accounts for 12% sen-tences and 7% words.
Both these sentence-leveland the word-level percentages are much lowerthan those on the training data as shown in Ta-ble 3 from (Yu et al., 2013).
This is because thereare many OOV words on a tuning set.
We run theMAXFORCE with dense feature setting on nist02-px and its testing results are shown in Table 6.
Wecan see that on all the test sets, its testing perfor-mance is lower than that of SA-MERTpottuning onnist02 with about 5 BLEU points.For more direct comparisons, we run MERT andSA-MERTpoton a data set similar to nist02-px.
Wepick up the fully reachable sentences from nist02-px, remove the sentence pairs with the same sourceside, and get a new tuning set denoted as nist02-r.When tuning on nist02-r, we find that MERT is bet-1949Methods tuning-set nist08MERT ssmt07 31.3MAXFORCE train-r-part 29.9SA-MERTparssmt07 31.3SA-MERTpotssmt07 31.7Table 7: EN-CH task: BLEU scores on nist08 test set forMERT, SA-MERT, and MAXFORCE on different tun-ing sets.
train-r-part is a part of fully reachable datafrom training data via forced decoding.
All the tuningmethods run with dense feature set.ter than MAXFORCE,4and SA-MERTpotare muchbetter than MERT on all the test sets.
In addition,we select about 1.2k fully reachable sentence pairsfrom training data, and run the forced decodingon this new tuning data (denoted as train-r-part),which is with similar size to nist02.5With moretuning data, the performance of max-violation isimproved largely, but it is still underperformed bySA-MERTpot.5.5 Results on EN-CH Translation TaskWe also run our search-aware tuning method onEN-CH task.
We use SA-MERT as the representa-tive of search-aware tuning methods, and compareits two versions with other tuning methods MERT,MAXFORCE.
For MAXFORCE, we first run forceddecoding on the training data and then select about1.2k fully reachable sentence pairs as its tuningset (denoted as train-r-part).
For MERT, SA-MERTpot, and SA-MERTpar, their tuning set isssmt07.
Table 7 shows that SA-MERTpotis muchbetter than MAXFORCE, i.e.
it achieves 0.4 BLEUimprovements over MERT.
Finally, comparisonbetween SA-MERTpotand SA-MERTparshowsthat the potential BLEU is better for evaluation ofpartial derivations.5.6 Discussions on Tuning EfficiencyAs shown in Figure 2, search-aware tuning consid-ers all partial translations in the middle bins besideall complete translations in the last bin, and thus itstotal number of training examples is much greaterthan that of the traditional tuning.
In details, sup-4Under the dense feature setting, MAXFORCE is worsethan standard MERT.
This result is consistent with that inFigure 12 of (Yu et al., 2013).5We run MAXFORCE on train-r-part, i.e.
a part of reach-able data instead of the entire reachable data, as we foundthat more tuning data does not necessarily lead to better test-ing performance under dense feature setting in our internalexperiments.Optimization time MERT MIRA PRObasline 3 2 2search-aware 50 7 6Table 8: Search-aware tuning slows down MERT sig-nificantly, and MIRA and PRO moderately.
The time (inminutes) is for optimization only (excluding decoding)and measured at the last iteration during the entire tun-ing (search aware tuning does not increase the numberof iterations in our experiments).
The decoding time is20 min.
on a single CPU but can be parallelized.pose the tuning data consists of two sentences withlength 10 and 30, respectively.
Then, for tradi-tional tuning its number of training examples is 2;but for search-aware tuning, the total number is 40.More training examples makes our search-awaretuning slower than the traditional tuning.Table 8 shows the training time comparisonsbetween search-aware tuning and the traditionaltuning.
From this Table, one can see that bothSA-MIRA and SA-PRO are with the same orderof magtitude as MIRA and PRO; but SA-MERTis much slower than MERT.
The main reason isthat, as the training examples increase dramati-cally, the envelope calculation for exact line search(see (Och, 2003)) in MERT is less efficient than theupdate based on (sub-)gradient with inexact linesearch in MIRA and PRO.One possible solution to speed up SA-MERT isthe parallelization but we leave it for future work.6 Related WorkMany tuning methods have been proposed forSMT so far.
These methods differ by the ob-jective function or training mode: their objectivefunctions are based on either evaluation-directedloss (Och, 2003; Galley and Quirk, 2011; Gal-ley et al., 2013) or surrogate loss (Hopkins andMay, 2011; Gimpel and Smith, 2012; Eidelmanet al., 2013); they are either batch (Och, 2003;Hopkins and May, 2011; Cherry and Foster, 2012)or online mode (Watanabe, 2012; Simianer et al.,2012; Flanigan et al., 2013; Green et al., 2013).These methods share a common characteristic:they learn a weight by iteratively reranking a set ofcomplete translations represented by k-best (Och,2003; Watanabe et al., 2007; Chiang et al., 2008)or lattice (hypergraph) (Tromble et al., 2008; Ku-mar et al., 2009), and they do not care about searcherrors that potential partial translations may bepruned during decoding, even if they agree with1950that their decoders are built on the beam pruningbased search.On the other hand, it is well-known that searcherrors can undermine the standard training formany beam search based NLP systems (Huang etal., 2012).
As a result, Collins and Roark (2004)and Huang et al.
(2012) propose the early-updateand max-violation update to deal with the searcherrors.
Their idea is to update on prefix or par-tial hypotheses when the correct solution falls outof the beam.
This idea has been successfullyused in many NLP tasks and improves the perfor-mance over the state-of-art NLP systems (Huangand Sagae, 2010; Huang et al., 2012; Zhang et al.,2013).Goldberg and Nivre (2012) propose the conceptof ?dynamic oracle?
which is the absolute best po-tential of a partial derivation, and is more akin toa strictly admissible heuristic.
This idea inspiredand is closely related to our potential BLEU, exceptthat in our case, computing an admissible heuristicis too costly, so our potential BLEU is more like anaverage potential.Gesmundo and Henderson (2014) also considerthe rankings between partial translation pairs aswell.
However, they evaluate a partial translationthrough extending it to a complete translation byre-decoding, and thus they need many passes ofdecoding for many partial translations, while oursonly need one pass of decoding for all partial trans-lations and thus is much more efficient.
In sum-mary, our tuning framework is more general andhas potential to be employed over all the state-of-art tuning methods mentioned above, even thoughours is only tested on three popular methods.7 Conclusions and Future WorkWe have presented a simple yet powerful approachof ?search-aware tuning?
by promoting promisingpartial derivations, and this idea can be applied toall three popular tuning methods.
To solve the keychallenge of evaluating partial derivations, we de-velop a concept of ?potential BLEU?
inspired byfuture cost in MT decoding.
Extensive experi-ments confirmed substantial BLEU gains with onlydense features.
We believe our framework can beapplied to sparse feature settings and other transla-tion paradigms, and potentially to other structuredprediction problems (such as incremental parsing)as well.AcknowledgementsWe thank the three anonymous reviewers for sug-gestions, and Kai Zhao and Feifei Zhai for dis-cussions.
In particular, we thank reviewer #3 andChin-Yew Lin for pushing us to think about di-versity.
This project was supported by DARPAFA8750-13-2-0041 (DEFT), NSF IIS-1449278, aGoogle Faculty Research Award, and a PSC-CUNY Award.ReferencesColin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of NAACL-HLT, pages 427?436, Montr?eal,Canada, June.David Chiang, Yuval Marton, and Philip Resnik.
2008.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of EMNLP2008.David Chiang.
2012.
Hope and fear for discriminativetraining of statistical translation models.
J. MachineLearning Research (JMLR), 13:1159?1187.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testing forstatistical machine translation: Controlling for opti-mizer instability.
In Proc.
of ACL 2011.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proceed-ings of ACL.Vladimir Eidelman, Yuval Marton, and Philip Resnik.2013.
Online relative margin maximization for sta-tistical machine translation.
In Proceedings of ACL,pages 1116?1126, Sofia, Bulgaria, August.Jeffrey Flanigan, Chris Dyer, and Jaime Carbonell.2013.
Large-scale discriminative training for statis-tical machine translation using held-out line search.In Proceedings of NAACL-HLT, pages 248?258, At-lanta, Georgia, June.Michel Galley and Chris Quirk.
2011.
Optimal searchfor minimum error rate training.
In Proceedings ofEMNLP, pages 38?49, Edinburgh, Scotland, UK.,July.Michel Galley, Chris Quirk, Colin Cherry, and KristinaToutanova.
2013.
Regularized minimum error ratetraining.
In Proceedings of EMNLP, pages 1948?1959, Seattle, Washington, USA, October.Andrea Gesmundo and James Henderson.
2014.
Undi-rected machine translation with discriminative rein-forcement learning.
In Proceedings of the 14th Con-ference of the European Chapter of the Associationfor Computational Linguistics, April.1951Kevin Gimpel and Noah A. Smith.
2012.
Struc-tured ramp loss minimization for machine transla-tion.
In Proceedings of NAACL-HLT, pages 221?231, Montr?eal, Canada, June.Kevin Gimpel, Dhruv Batra, Chris Dyer, and GregoryShakhnarovich.
2013.
A systematic exploration ofdiversity in machine translation.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, October.Yoav Goldberg and Joakim Nivre.
2012.
Trainingdeterministic parsers with non-deterministic oracles.In Proceedings of COLING 2012.Spence Green, Sida Wang, Daniel Cer, and ChristopherManning.
2013.
Fast and adaptive online trainingof feature-rich translation models.
In Proc.
of ACL2013.P.
E. Hart, N. J. Nilsson, and B. Raphael.
1968.
A for-mal basis for the heuristic determination of minimumcost paths.
IEEE Transactions on Systems Scienceand Cybernetics, 4(2):100?107.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of EMNLP.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technologies (IWPT-2005).Liang Huang and David Chiang.
2007.
Forest rescor-ing: Fast decoding with integrated language models.In Proceedings of ACL, Prague, Czech Rep., June.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InProceedings of ACL 2010.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Pro-ceedings of NAACL.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25(4):607?615.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: open source toolkitfor statistical machine translation.
In Proceedings ofACL: Demonstrations.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of AMTA, pages 115?124.Shankar Kumar, Wolfgang Macherey, Chris Dyer, andFranz Och.
2009.
Efficient minimum error ratetraining and minimum bayes-risk decoding for trans-lation hypergraphs and lattices.
In Proceedings ofACL-IJCNLP, Suntec, Singapore, August.Zhifei Li and Sanjeev Khudanpur.
2009.
Efficientextraction of oracle-best translations from hyper-graphs.
In Proceedings of HLT-NAACL Short Pa-pers.Adam Lopez.
2008.
Statistical machine translation.ACM Comput.
Surv., 40(3).Preslav Nakov, Francisco Guzmn, and Stephan Voge.2013.
A tale about pro and monsters.
In Proceedingsof ACL Short Papers.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Comput.
Linguist., 29(1):19?51, March.Franz Joseph Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,pages 311?318, Philadephia, USA, July.Patrick Simianer, Stefan Riezler, and Chris Dyer.
2012.Joint feature selection in distributed stochastic learn-ing for large-scale discriminative training in smt.
InProceedings of ACL, pages 11?21, Jeju Island, Ko-rea, July.Andreas Stolcke.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP,volume 30, pages 901?904.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Risk decoding for statistical machine translation.
InProceedings of EMNLP, pages 620?629, Honolulu,Hawaii, October.Taro Watanabe, Jun Suzuki, Hajime Tsukada, andHideki Isozaki.
2007.
Online large-margin trainingfor statistical machine translation.
In Proceedings ofEMNLP-CoNLL.Taro Watanabe.
2012.
Optimized online rank learningfor machine translation.
In Proceedings of NAACL-HLT, pages 253?262, Montr?eal, Canada, June.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-violation perceptron and forced decod-ing for scalable mt training.
In Proceedings ofEMNLP 2013.Hao Zhang, Liang Huang, Kai Zhao, and Ryan McDon-ald.
2013.
Online learning with inexact hypergraphsearch.
In Proceedings of EMNLP 2013.Kai Zhao, Liang Huang, Haitao Mi, and Abe Itty-cheriah.
2014.
Hierarchical mt training using max-violation perceptron.
In Proceedings of ACL, Balti-more, Maryland, June.1952
