Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 175?184,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsTopicCheck: Interactive Alignment for Assessing Topic Model StabilityJason Chuang?jason@chuang.caMargaret E. Roberts?Political ScienceU.
California, San Diegomeroberts@ucsd.eduBrandon M. Stewart?GovernmentHarvard Universitybstewart@fas.harvard.eduRebecca Weiss?CommunicationStanford Universityrjweiss@stanford.eduDustin TingleyGovernmentHarvard Universitydtingley@gov.harvard.eduJustin GrimmerPolitical ScienceStanford Universityjgrimmer@stanford.eduJeffrey HeerComputer Science & Eng.University of Washingtonjheer@uw.eduAbstractContent analysis, a widely-applied social sci-ence research method, is increasingly beingsupplemented by topic modeling.
However,while the discourse on content analysis cen-ters heavily on reproducibility, computer sci-entists often focus more on scalability and lesson coding reliability, leading to growing skep-ticism on the usefulness of topic models forautomated content analysis.
In response, weintroduce TopicCheck, an interactive tool forassessing topic model stability.
Our contri-butions are threefold.
First, from establishedguidelines on reproducible content analysis,we distill a set of design requirements on howto computationally assess the stability of anautomated coding process.
Second, we devisean interactive alignment algorithm for match-ing latent topics from multiple models, and en-able sensitivity evaluation across a large num-ber of models.
Finally, we demonstrate thatour tool enables social scientists to gain novelinsights into three active research questions.1 IntroductionContent analysis ?
the examination and systematiccategorization of written texts (Berelson, 1952) ?
isa fundamental and widely-applied research methodin the social sciences and humanities (Krippendorff,2004a), found in one third of all articles publishedin major communication journals (Wimmer and Do-minick, 2010).
Initial reading and coding, two labor-?Work completed while at Stanford University and the Uni-versity of Washington, and submitted while at the Allen Insti-tute for Artificial Intelligence.
?These authors contributed equally to this paper.intensive steps in the analysis process, are increas-ingly replaced by computational approaches such asstatistical topic modeling (Grimmer, 2013; McFar-land et al, 2013; Roberts et al, 2014a).However, while the discourse on content analysisoverwhelmingly centers around the reproducibilityand generalizability of a coding scheme (Krippen-dorff, 2004b; Lombard et al, 2002), computer sci-entists tend to focus more on increasing the scaleof analysis and less on establishing coding reliabil-ity.
Machine-generated latent topics are often takenon faith to be a truthful and consistent representa-tion of the underlying corpus, but in practice ex-hibit significant variations among models or mod-eling runs.
These unquantified uncertainties fuelgrowing skepticism (Schmidt, 2012) and hamper thecontinued adoption (Grimmer and Stewart, 2011) oftopic models for automated content analysis.In response, we introduce TopicCheck, an interac-tive tool for assessing the stability of topic models.Our threefold contributions are as follows.First, from established guidelines on reproduciblecontent analysis, we distill a set of design require-ments on how to computationally assess the stabil-ity of an automated coding process.
We advocate forthe use of multiple models for analysis, a user-drivenapproach to identify acceptable levels of coding un-certainty, and providing users with the capability toinspect model output at all levels of detail.Second, we devise an interactive up-to-one align-ment algorithm for assessing topic model stability.Through repeated applications of a topic model togenerate multiple outputs, our tool allows users toinspect whether the model consistently uncover the175same set of concepts.
We allow users to interactivelydefine groupings of matching topics, and presentthe aligned topics using an informative tabular lay-out, so that users can quickly identify stable topicalgroupings as well as any inconsistencies.Finally, in three case studies, we demonstrate thatour tool allows social scientists to gain novel in-sights into active and ongoing research questions.We provide an in-depth look at the multi-modality oftopic models.
We document how text pre-processingalters topical compositions, causing shifts in defini-tions and the removal of select topics.
We reporton how TopicCheck supports the validity of newly-proposed communication research methods.2 BackgroundManual approaches to extract information from tex-tual data ?
reading the source documents and codi-fying notable concepts ?
do not scale.
For example,Pew Research Center produces the News CoverageIndex (2014) to measure the quality of news report-ing in the United States.
Intended to track 1,450newspapers nationwide, their purely manual effortsonly cover 20 stories per day.
Researchers stand tolose rich details in their data when their attention islimited to a minuscule fraction of the available texts.Critical of approaches that ?
[make] restrictive as-sumptions or [are] prohibitively costly,?
Quinn et al(2010) discuss the use of topic models (Blei et al,2003) to enable large-scale text analysis by usingmachine-generated latent topics to approximate pre-viously manually-crafted codes.
Automated contentanalysis has enabled groundbreaking massive stud-ies (Grimmer, 2013; McFarland et al, 2013; Robertset al, 2014a).
While this initial uptake of topic mod-els is encouraging, an over-emphasis on scalabilityand the use of a single model for analysis invitesskepticism and threatens continued adoption.2.1 Coding Reliability & Growing SkepticismCoding reliability is critical to content analysis.When social scientists devise a coding scheme, theymust clearly articulate the definition of their codesin such a way that any person can consistently applythe given codes to all documents in a corpus.Despite high labor cost, content analysis is typi-cally conducted with multiple coders in order to es-tablish coding reliability; the proper application ofreliability measures is heavily discussed and debatedin the literature (Krippendorff, 2004b; Lombard etal., 2002).
In contrast, software packages (McCal-lum, 2013;?Reh?u?rek and Sojka, 2010) and graphicaltools (Chaney and Blei, 2014; Chuang et al, 2012b)have made topic models accessible, cheap to com-pute, easy to deploy, but they almost always presentusers with a single model without any measure ofuncertainty; we find few studies on topic model sen-sitivity and no existing tool to support such analyses.Schmidt (2012) summarizes the view among dig-ital humanists, a group of early adopters of topicmodels, on the experience of working with uncer-tain modeling results: ?A poorly supervised ma-chine learning algorithm is like a bad research as-sistant.
It might produce some unexpected constel-lations that show flickers of deeper truths; but it willalso produce tedious, inexplicable, or misleading re-sults.
.
.
.
[Excitement] about the use of topic modelsfor discovery needs to be tempered with skepticismabout how often the unexpected juxtapositions.
.
.will be helpful, and how often merely surprising.
?Researchers increasingly voice skepticism aboutthe validity of using single models for analysis.
Ina comprehensive survey of automatic content anal-ysis methods, Grimmer et al (2011) highlight theneed to validate models through close reading andmodel comparison, and advise against the use ofsoftware that ?simply provide the researcher withoutput?
with no capability to ensure the output isconceptually valid and useful.
Chuang et al (2012a)report that findings from one-off modeling effortsmay not sustain under scrutiny.
Schmidt (2012) ar-gues that computer-aided text analysis should incor-porate competing models or ?humanists are betteroff applying zero computer programs.
?2.2 Uncertainties in Topic ModelsWhile topic models remove some issues associ-ated with human coding, they also introduce newsources of uncertainties.
We review three factors re-lated to our case studies: multi-modality, text pre-processing, and human judgment of topical quality.Roberts et al (2014b) examine the multi-modaldistributions of topic models that arise due to thenon-convex nature of the underlying optimization.They characterize the various local solutions, and176demonstrate that the spread of topics can lead to con-tradictory analysis outcomes.
The authors note thatoptimal coding may not necessarily correspond tomodels that yield the highest value of the objectivefunction, but there is currently a paucity of computa-tional tools to inspect how the various modes differ,help researchers justify why one local mode mightbe preferred over another on the basis of their do-main knowledge, or for an independent researcherto validate another?s modeling choices.Fokkens et al (2013) report widespread repro-ducibility failures in natural language processingwhen they replicate ?
and fail to reproduce ?
theresults reported on two standard experiments.
Theauthors find that minor decisions in the modelingprocess can impact evaluation results, including twofactors highly relevant to topic modeling: differ-ences in text pre-processing and corpus vocabulary.The word intrusion test (Chang et al, 2009; Lauet al, 2014) is considered the current state-of-the-art approach to assess topical quality, and captureshuman judgment more accurately than other topicalcoherence measures (Stevens et al, 2012; Wallach etal., 2009).
However, in this approach, users inspectonly a single latent topic at a time without access tothe overall set of topics.
As a part of this paper, weinvestigate whether exposure to multiple competingmodels affects human judgment, and whether modelconsistency impacts topical coherence.2.3 Reproducibility of a Coding ProcessWhile no single definition exists for the processof content analysis, a frequently-cited and wide-applied template is provided by Krippendorff (1989;2004b) who recommends four steps to safeguard thereproducibility of a coding process.
Practitionersmust demonstrate coder reliability, a decisive agree-ment coefficient, an acceptable level of agreement,and test individual variables.To the best of our knowledge, our paper is the firstto convert guidelines on reproducible human codinginto software design requirements on validating au-tomated content analysis.
Our interactive alignmentalgorithm is the first implementation of these guide-lines.
Our case studies represent the first reportson the impact of computationally quantifying topicmodel uncertainties, situated within the context ofreal-world ongoing social science research.Much of the research on topic modeling focuseson model designs (Blei et al, 2004; Blei and Laf-ferty, 2006; Rosen-Zvi et al, 2004) or inference al-gorithms (Anandkumar et al, 2012).
Our tool iscomplementary to this large body of work, and sup-ports real-world deployment of these techniques.
In-teractive topic modeling (Hu et al, 2014) can play akey role to help users not only verify model consis-tency but actively curate high-quality codes; its in-clusion is beyond the scope of a single conferencepaper.
While supervised learning (Settles, 2011) hasbeen applied to content analysis, it represents the ap-plication of a pre-defined coding scheme to a textcorpus, which is different from the task of devisinga coding scheme and assessing its reliability.3 Validation Tool Design RequirementsA measure of coding reproducibility is whether atopic model can consistently uncover the same setof latent topics.
We assume that users have a largenumber of topic model outputs, presumed to be iden-tical, and that the users wish to examine unexpectedvariations among the outputs.
To guide tool devel-opment, we first identify software design require-ments, to meet the standards social scientists needto demonstrate producible coding.3.1 Topical Mapping & Up-to-One AlignmentA key difference exists between measuring inter-coder agreement and assessing topic model varia-tions.
In a manual coding process, human coders areprovided code identifiers; responses from differentcoders can be unambiguously mapped onto a com-mon scheme.
No such mapping exists among theoutput from repeated runs of a topic model.
Valida-tion tools must provide users with effective meansto generate topical mapping.However, the general alignment problem of op-timally mapping multiple topics from one model tomultiple topics in another model is both ill-definedand computationally intractable.
Since our tool is tosupport the comparison of similar ?
and supposedlyidentical ?
model output, we impose the followingconstraint.
A latent topic belonging to a model mayalign with up to one latent topic in another model.We avoid the more restrictive constraint of one-to-one alignment.
Forcing a topic to always map ontoanother topic may cause highly dissimilar topics to177be grouped together, obscuring critical mismatches.Instead, up-to-one mapping allows for two poten-tial outcomes, both of which correspond directlyto the intended user task: recognize consistent pat-terns across the models (when alignment occurs) andidentify any deviations (when alignment fails).3.2 Guidelines Adapted for Topic ModelsWe synthesize the following four requirements fromKrippendorff?s guidelines (2004b).To calculate the equivalent of coder reliability,we advocate the use of multiple models to deter-mine modeling consistency, which may be deter-mined from the repeated applications of the sametopic model, a search through the parameter spaceof a model, or the use of multiple models.Selecting an appropriate agreement coefficient de-pends on the underlying data type, such as binary,multivariate, ordered, or continuous codes (Cohen,1960; Holsti, 1969; Krippendorff, 1970; Osgood,1959; Scott, 1995).
No widely-accepted similaritymeasure exists for aligning latent topics, which areprobability distributions over a large vocabulary.
Weargue that validation tools must be sufficiently mod-ular, in order to accept any user-defined topicalsimilarity measure for aligning latent topics.Acceptable level of agreement depends on the pur-pose of the analysis, and should account for thecosts of drawing incorrect conclusions from a cod-ing scheme.
For example, do ?human lives hangon the results of a content analysis??
(Krippendorff,2004b).
Validation tools must allow users to set theappropriate acceptable level of agreement, andhelp users determine ?
rather than dictate ?
whentopic models match and what constitutes reasonablevariations in the model output.Finally, Krippendorff points out that aggregatedstatistics can obscure critical reliability failures, andpractitioners must test individual variables.
We in-terpret this recommendation as the need to presentusers with not a single overall alignment scorebut details at all levels: models, topics, and con-stituent words within each latent topic.4 Interactive Topical AlignmentWe introduce TopicCheck, an implementation of ourdesign specifications.
At the core of this tool is aninteractive topical alignment algorithm.4.1 Hierarchical Clustering with ConstraintsOur algorithm can be considered as hierarchical ag-glomerative clustering with up-to-one mapping con-straints.
As input, it takes in three arguments: a listof topic models, a topical similarity measure, and amatching criterion.
As output, it generates a list oftopical groups, where each group contains a list oftopics with at most one topic from each model.At initialization, we create a topical group forevery topic in every model.
We then iterativelymerge the two most similar groups based on theuser-supplied topical similarity measure, providedthat the groups satisfy the user-specified matchingcriterion and the mapping constraints.
When no newgroups can be formed, the algorithm terminates andreturns a sorted list of final topical groups.During the alignment process, the following twoinvariants are guaranteed: Every topic is always as-signed to exactly one group; every group contains atmost one topic from each model.
A topic model mconsists of a list of latent topics.
A latent topic t isrepresented by a probability distribution over words.A topical group g also consists of a list of latent top-ics.
Let |m|, |t|, and |g| denote the number of mod-els, topics, and groups respectively.
We create a totalof |g| = |m| ?
|t| initial topical groups.
Although|g| decreases by 1 after each merge, |g| ?
|t| at alltimes.
At the end of alignment, |g| = |t| if and onlyif perfect alignment occurs and every group containsexactly one topic from each model.Users may supply any topical similarity measurethat best suits their analysis needs.
We select cosinesimilarity for our three case studies, though our soft-ware is modular and accepts any input.
As a firstimplementation, we apply single-linkage clusteringcriteria when comparing the similarity of two topicalgroups.
Single-linkage clustering is computationallyefficient (Sibson, 1973), so that users may interactwith the algorithm and receive feedback in real-time;our procedure generalizes to other linkage criteriasuch as complete-linkage or average-linkage.At each merge step, the most similar pair of top-ical groups are identified.
If they meet the match-ing criteria and the mapping constraints, the pair iscombined into a new group.
Otherwise, the algo-rithm iteratively examines the next most similar pairuntil either a merge occurs or when all pairs are ex-178Figure 1: This chart shows topics uncovered from 13,250 political blogs (Eisenstein and Xing, 2010) by 50 structuraltopic models (Roberts et al, 2013).
Latent topics are represented as rectangles; bar charts within the rectanglesrepresent top terms in a topic.
Topics belonging to the same model are arranged in a column; topics assigned to thesame group are arranged in a row.
This chart is completely filled with topics only if perfect alignment occurs.
Whentopics in a model fail to align with topics in other models, empty cells appear in its column.
Similarly, when topics in agroup are not consistently uncovered by all models, empty cells appear in its row.
Hovering over a term highlights allother occurrences of the same term.
Top terms belonging to each topical group are shown on the right; they representthe most frequent words over all topics in the group, by summing their probability distributions.Figure 2: Continued from Figure 1, users may decrease the similarity threshold to generate additional groupings oftopics that are less consistent, uncovered by as few as 3 of the 50 modeling runs.hausted, at which point the procedure terminates.Users can specify a similarity threshold, belowwhich topical groups are considered to differ toomuch to be matched.
Two groups are allowed tomerge only if both of the following conditions aremet: their similarity is above the user-defined sim-ilarity threshold and every topic in the combinedgroup belongs to a different model.4.2 Tabular Layout and User InteractionsWe devise a tabular layout to present the alignmentoutput at all levels of detail: groups, models, topics,179and words.
Users can interact with the algorithm,redefine matching criteria, and inspect the alignedmodels interactively in real-time.We arrange topical groups as rows and topic mod-els as columns as shown in Figure 1.
A topic as-signed to group giand belonging to model mjisplaced at the intersection of row i and column j.Our up-to-one mapping ensures at most one topicper each cell.
A table of size |g| ?
|m| will onlybe completely filled with topics if perfect alignmentoccurs.
When topics in model mjfail to align withtopics in other models, empty cells appear in columnj.
Similarly, when topics in group giare not consis-tently uncovered by all models, empty cells appearin row i.
Within each topic, we show the probabilitydistribution of its constituent words as a bar chart.Users define three parameters in our tool.
First,they may set the matching criteria, and define howaggressively the topics are merged into groups.
Sec-ond, users may alter the number of topical groupsto reveal.
Rather than displaying numerous sparsegroups, the tool shows only the top groups as deter-mined by their topical weight.
Topics in all remain-ing groups are placed at the bottom of the table andmarked as ungrouped.
Third, users may adjust thenumber of top terms to show, as a trade-off betweendetails vs. overview.
Increasing the number of termsallows users to inspect the topics more carefully, butthe cells take up more screen space, reducing thenumber of visible groups.
Decreasing the numberof terms reduces the size of each cell, allowing usersto see more groups and observe high-level patterns.The tabular layout enables rapid visual assess-ment of consistency within a model or a group.We further facilitate comparisons via brushing andlinking (Becker and Cleveland, 1987).
When usershover over a word on the right hand side or over a barwithin the bar charts, we highlight all other occur-rences of the same word.
For example, in Figure 1,hovering over the term econom reveals that the wordis common in three topical groups.5 Deployment and Initial FindingsWe implemented our alignment algorithm and userinterface in JavaScript, so they are easily accessi-ble within a web browser; topical similarity is com-puted on a Python-backed web server.
We reportuser responses and initial findings from deployingthe tool on three social science research projects.Interactive versions of the projects are available athttp://content-analysis.info/naacl.5.1 A Look at Multi-Modal SolutionsWe deployed TopicCheck on topic models generatedby Roberts et al (2014b) to examine how model out-put clusters into local modes.
As the models are pro-duced by 50 runs of an identical algorithm with allpre-processing, parameters, and hyper-parametersheld constant, we expect minimal variations.As shown in Figure 1, we observe that the top twotopical groups, about Barack Obama and John Mc-Cain respectively, are consistently uncovered acrossall runs.
The third topical group, about the Iraqi andAfghani wars (defined by a broader set of terms) isalso consistently generated by 49 of the 50 runs.Toward the bottom of the chart, we observesigns of multi-modality.
Topical groups #15 to #17represent variations of topics about the economy.Whereas group #15 is about the broader economy,groups #16 and #17 focus on taxes and the finan-cial crisis, respectively.
Half of the runs producedthe broader economy topic; the other runs generatedonly one or two of the specialized subtopics.
No sin-gle model uncovered all three, suggesting that theinference algorithm converged to one of two distinctlocal optimal solutions.
In Figure 2, by lowering thematching criteria and revealing additional groups,we find that the model continues to produce inter-esting topics such as those related to global warm-ing (group #24) or women?s rights (group #25), butthese topics are not stable across the multiple modes.5.2 Text Pre-Processing & Replication IssuesWe conducted an experiment to investigate the ef-fects of rare word removal using TopicCheck.
Asa part of our research, we had collected 12,000news reports from five different international newssources over a period of ten years, to study sys-tematic differences in news coverage on the rise ofChina, between western and Chinese media.While many modeling decisions are involved inour analysis, we choose rare word removal for tworeasons.
First, though the practice is standard, to thebest of our knowledge, we find no systematic studieson how aggressively one should cull the vocabulary.180Figure 3: While rare word removal is generally considered to have limited impact on topic model output, we findevidence to the contrary.
By varying the removal threshold, for this corpus of international news reports on the riseof China, we observe that topics such as group #11 on the Beijing Olympics begin to disappear.
Topics about HongKong appear sporadically.
On top of the inconsistency issues, different pre-processing settings lead to drifts in topicdefinitions.
For milder removal thresholds (toward the left), group #13 discusses Hong Kong within the context ofTaiwan and Macau.
With more aggressive filtering (toward the right), group #14 shifts into discussions about HongKong itself such as one country two systems and the special administrative region.
Unchecked, these seemingly minortext pre-processing decisions may eventually lead researchers down different paths of analysis.Second, as latent topics are typically defined throughtheir top words, filtering words that occur only in asmall fraction of the documents is generally consid-ered to have limited impact on model output.We trained structural topic models (Roberts etal., 2013) based on a subset of the corpus with2,398 documents containing approximately 20,000unique words.
We applied 10 different settingswhere we progressively removed a greater numberof rare terms beyond those already filtered by thedefault settings while holding all other parametersconstant.
The number of unique words retained bythe models were 1,481 (default), 904, 634, 474, 365,. .
., down to 124 for the 10 settings.
We generated6 runs of the model at each setting, for a total of 60runs.
Removed words are assigned a value of 0 inthe topic vector when computing cosine similarity.We observe significant changes to the model out-put across the pre-processing settings, as shown inFigure 3.
The six models on the far left (columns 1to 6) represent standard processing; rare word re-moval ranges from the mildest (columns 7 to 12)to the most aggressive (columns 55 to 60) as thecolumns move from left to right across the chart.While some topical groups (e.g., #1 on the com-munist party) are stable across all settings, manyothers fade in and out.
Group #11 on the BeijingOlympics is consistent under standard processingand the mildest removal, but disappears completelyafterward.
We find two topical groups about HongKong that appear sporadically.
On top of the in-stability issues, we observe that their content driftsacross the settings.
With milder thresholds, topicalgroup #13 discusses Hong Kong within the contextof Taiwan and Macau.
With more aggressive filter-ing, topical group #14 shifts into discussions aboutHong Kong itself such as one country two systemsand the special administrative region.
Unchecked,these minor text pre-processing decisions may leadresearchers down different paths of analysis.5.3 News Coverage & Topical CoherenceAgenda-setting refers to observations by McCombset al (1972) that the media play an important rolein dictating issues of importance for voters, and byIyengar et al (1993) that news selection bias candetermine how the public votes.
Studying agenda-setting requires assessing the amount of coveragepaid to specific issues.
Previous manual coding ef-forts are typically limited to either a single eventor subsampled so thinly that they lose the abilityto consistently track events over time.
Large-scaleanalysis (e.g., for an entire federal election) remainsbeyond the reach of most communication scholars.As part of our research, we apply topic modelingto closed-captioning data from over 200,000 hoursof broadcasts on all mainstream news networks, totrack the full spectrum of topics across all media out-181Figure 4: To enable large-scale studies of agenda-setting, we applied topic modeling to closed-captioning of over200,000 hours of broadcasts, to estimate coverage in mainstream news networks.
Through TopicCheck, the researchersfind consistent topical groups that correspond to known major news categories.
Group #9 represents topics aboutadvertisements and valuable data to study the relationships between broadcasters and advertisers.lets.
We conduct word intrusion tests (Chang et al,2009) on Amazon Mechanical Turk, and obtain over50,000 user ratings to identify high quality topics.However, to establish topic modeling as a valid re-search method, we must demonstrate the reliabilityof how we include or exclude topics in our analyses.By applying TopicCheck to 32 runs of the sametopic model, as shown in Figure 4, we confirm thatthe consistent topical groupings capture at least fourmajor known news categories: weather (such asgroup #5), finance (group #3), major events (group#7 on the Trayvon Martin shooting), and natural dis-asters (group #11 on Hurricane Katrina).
We findadditional evidence supporting the use of topic mod-els, including the consistent appearance of adver-tising topics (group #9 on the sales of prescriptionmedicine to senior citizens, a major demographic ofthe broadcast news audience).
These topics may en-able studies on the relationship between broadcast-ers and advertisers, an important but difficult ques-tion to address because few previous studies have theresources to codify advertisement content.However, event-specific topics tend to appear lessconsistently (such as group #24 on Russia, its con-flict with Ukraine, and the Sochi Olympics).
Wenote the lack of consistent topics on supreme courtcases, an expected but missing news category, whichwarrants more in-depth investigations.We compare human judgment of topical qualitywhen examining multiple models and those basedon word intrusion tests.
We calculate the aggregatedtopical coherence scores for each topical grouping.We find that consistent topical groups tend to receivehigher coherence scores.
However, topics about nat-ural disasters receive low scores with a high variance(avg 0.5371; stdev 0.2497); many of them wouldhave previously been excluded from analysis.6 DiscussionsTo many social scientists, statistical models aremeasurement tools for inspecting social phenom-ena, such as probing recurring language use in atext corpus with topic models.
In this light, instru-ments with known performance characteristics ?including well-quantified uncertainties and propercoverage ?
are more valuable than potentially pow-erful but inconsistent modeling approaches.Our initial findings suggest that a single topicmodel may not capture all perspectives on a dataset,as evident in the multiple local solutions about theeconomy, Hong Kong, and natural disasters in thethree case studies respectively.
By exposing modelstability, our tool can help researchers validate mod-eling decisions, and caution against making too gen-eral a claim about any single modeling result.We hypothesize that the low coherence scores fortopics about natural disasters might derive from twocauses.
First, news media might cover an event dif-ferently (e.g., focusing on economic vs. humanitar-ian issues during Hurricane Katrina).
Second, un-182folding events may naturally have less stable vocab-ularies.
In both cases, detecting and pinpointing re-porting bias is central to the study of agenda-setting.These observations suggest that for certain applica-tions, identifying consistent topics across multiplemodels may be equally critical as, if not more than,enforcing topical coherence within a single model.Increasingly, text analysis relies on data-depen-dent modeling decisions.
Rare word removal cansubstantively alter analysis outcomes, but selectingan appropriate threshold requires inspecting the con-tent of a text corpus.
TopicCheck can help archivethe exact context of analysis, allowing researchersto justify ?
and readers to verify and challenge ?modeling decisions through access to data.Finally, topic modeling has dramatically loweredthe costs associated with content analysis, allowinghundreds of models to be built in parallel.
The cur-rent intended user task for TopicCheck is to validatethe stability of presumably identical models.
Weplan to develop additional tools to help social scien-tists design better models, and actively explore theeffects of alternative coding schemes.7 ConclusionWe present TopicCheck for assessing topic modelstability.
Through its development, we demonstratethat existing research on reproducible manual codi-fication can be transferred and applied to computa-tional approaches such as automated content analy-sis via topic modeling.
We hope this work will helpcomputer scientists and social scientists engage indeeper conversations about research reproducibilityfor large-scale computer-assisted text analysis.AcknowledgmentsThis research was supported in part by a grant fromthe Brown Institute for Media Innovation.ReferencesAnima Anandkumar, Yi kai Liu, Daniel J. Hsu, Dean PFoster, and Sham M Kakade.
2012.
A spectral algo-rithm for latent dirichlet alocation.
In Neural Infor-mation Processing Systems (NIPS), pages 917?925.Richard A. Becker and William S. Cleveland.
1987.Brushing scatterplots.
Technometrics, 29(2):127?142.Bernard Berelson.
1952.
Content analysis in communi-cation research.
Free Press.David M. Blei and John D. Lafferty.
2006.
Dynamictopic models.
In International Conference on MachineLearning (ICML), pages 113?120.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3(1):993?1022.David M. Blei, Thomas L. Griffiths, Michael I. Jordan,and Joshua B. Tenenbaum.
2004.
Hierarchical topicmodels and the nested chinese restaurant process.
InNeural Information Processing Systems (NIPS).Allison June-Barlow Chaney and David M. Blei.
2014.Visualizing topic models.
In International Conferenceon Weblogs and Social Media (ICWSM), pages 419?422.Jonathan Chang, Jordan Boyd-Graber, Chong Wang,Sean Gerrish, and David M. Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InNeural Information Processing Systems (NIPS), pages288?296.Jason Chuang, Christopher D. Manning, and JeffreyHeer.
2012a.
Interpretation and trust: Design-ing model-driven visualizations for text analysis.
InConference on Human Factors in Computing Systems(CHI), pages 443?452.Jason Chuang, Christopher D. Manning, and JeffreyHeer.
2012b.
Termite: Visualization techniques forassessing textual topic models.
In Advanced Visual In-terfaces (AVI).Jacob Cohen.
1960.
A coefficient of agreement for nom-inal scales.
Educational and Psychological Measure-ment, 20:37?46.Jacob Eisenstein and Eric Xing.
2010.
The CMU 2008Political Blog Corpus.
Carnegie Mellon University.Antske Fokkens, Marieke van Erp, Marten Postma, TedPedersen, Piek Vossen, and Nuno Freire.
2013.
Off-spring from reproduction problems: What replicationfailure teaches us.
In Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages1691?1701.Justin Grimmer and Brandon M. Stewart.
2011.
Textas data: The promise and pitfalls of automatic contentanalysis methods for political texts.
Political Analysis,21(3):267?297.Justin Grimmer.
2013.
Appropriators not position takers:The distorting effects of electoral incentives on con-gressional representation.
American Journal of Politi-cal Science, 57(3):624?642.Ole R. Holsti.
1969.
Content analysis for the socialsciences and humanities.
Addison-Wesley PublishingCompany.Yuening Hu, Jordan Boyd-Graber, Brianna Satinoff, andAlison Smith.
2014.
Interactive topic modeling.
Ma-chine Learning, 95(3):423?469.183Shanto Iyengar and Adam Simon.
1993.
News cover-age of the gulf crisis and public opinion: A study ofagenda-setting, priming, and framing.
Communica-tion Research, 20(3):365?383.Klaus Krippendorff.
1970.
Bivariate agreement coef-ficients for reliability of data.
In E. R. Borgatta andG.
W. Bohrnstedt, editors, Sociological methodology,pages 139?150.
John Wiley & Sons.Klaus Krippendorff.
1989.
Content analysis.
InE.
Barnouw, G. Gerbner, W. Schramm, T. L. Worth,and L. Gross, editors, International encyclopedia ofcommunication.
Oxford University Press.Klaus Krippendorff.
2004a.
Content analysis: An intro-duction to its methodology.
Sage, 2nd edition.Klaus Krippendorff.
2004b.
Reliability in content analy-sis: Some common misconceptions and recommenda-tions.
Human Communication Research, 30(3):411?433.Jey Han Lau, David Newman, and Timothy Baldwin.2014.
Machine reading tea leaves: Automaticallyevaluating topic coherence and topic model quality.In Conference of the European Chapter of the Asso-ciation for Computational Linguistics (EACL), pages530?539.Matthew Lombard, Jennifer Snyder-Duch, andCheryl Campanella Bracken.
2002.
Content analysisin mass communication: Assessment and reportingof intercoder reliability.
Human CommunicationResearch, 28(4):587?604.Andrew McCallum.
2013.
MALLET: Amachine learning for language toolkit.http://mallet.cs.umass.edu.Maxwell E. McCombs and Donald L. Shaw.
1972.
Theagenda-setting function of mass media.
Public Opin-ion Quarterly, 36(5):176?187.Daniel A. McFarland, Daniel Ramage, Jason Chuang,Jeffrey Heer, and Christopher D. Manning.
2013.
Dif-ferentiating language usage through topic models.
Po-etics: Special Issue on Topic Models and the CulturalSciences, 41(6):607?625.C.
E. Osgood.
1959.
The representational model andrelevant research.
In I. de Sola Pool, editor, Trends incontent analysis, pages 33?88.
University of IllinoisPress.Ted Pedersen.
2008.
Empiricism is not a matter of faith.Computational Linguistics, 34(3):465?470.Pew Research Journalism Project.
2014.News coverage index methodology.http://www.journalism.org/news index methodology/99/.Kevin M. Quinn, Burt L. Monroe, Michael Colaresi,Michael H. Crespin, and Dragomir R. Radev.
2010.How to analyze political attention with minimal as-sumptions and costs.
American Journal of PoliticalScience, 54(1):209?228.Radim?Reh?u?rek and Petr Sojka.
2010.
Software frame-work for topic modelling with large corpora.
In LRECWorkshop on New Challenges for NLP Frameworks,pages 45?50.Margaret E. Roberts, Brandon M. Stewart, Dustin Tin-gley, and Edoardo M. Airoldi.
2013.
The structuraltopic model and applied social science.
In NIPS Work-shop on Topic Models.Margaret E. Roberts, Brandon Stewart, Dustin Tingley,Chris Lucas, Jetson Leder-Luis, Bethany Albertson,Shana Gadarian, and David Rand.
2014a.
Topic mod-els for open-ended survey responses with applicationsto experiments.
American Journal of Political Science.Forthcoming.Margaret E. Roberts, Brandon M. Stewart, and DustinTingley.
2014b.
Navigating the local modes of bigdata: The case of topic models.
In R. Michael Alvarez,editor, Data Science for Politics, Policy and Govern-ment.
In Press.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers, andPadhraic Smyth.
2004.
The author-topic model forauthors and documents.
In Conference on Uncertaintyin Artificial Intelligence (UAI), pages 487?494.Benjamin M. Schmidt.
2012.
Words alone: Dismantlingtopic models in the humanities.
Journal of Digital Hu-manities, 2(1).William A. Scott.
1995.
Reliability of content analy-sis:: The case of nominal scale coding.
Public Opin-ion Quarterly, 19(3):321?325.Burr Settles.
2011.
Closing the loop: Fast, interactivesemi-supervised annotation with queries on featuresand instances.
In Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 1467?1478.Robin Sibson.
1973.
SLINK: an optimally efficient al-gorithm for the single-link cluster method.
The Com-puter Journal, 16:30?34.Keith Stevens, Philip Kegelmeyer, David Andrzejewski,and David Buttler.
2012.
Exploring topic coherenceover many models and many topics.
In Conferenceon Empirical Methods on Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 952?961.Hanna M. Wallach, Iain Murray, Ruslan Salakhutdinov,and David Mimno.
2009.
Evaluation methods fortopic models.
In International Conference on MachineLearning (ICML), pages 1105?1112.Roger Wimmer and Joseph Dominick.
2010.
Mass Me-dia Research: An Introduction.
Cengage Learning.184
