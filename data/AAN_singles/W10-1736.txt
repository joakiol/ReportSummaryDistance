Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 244?251,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsHead Finalization: A Simple Reordering Rule for SOV LanguagesHideki Isozaki, Katsuhito Sudoh, Hajime Tsukada, Kevin DuhNTT Communication Science Laboratories, NTT Corporation2-4 Hikaridai, Seikacho, Sorakugun, Kyoto, 619-0237, Japan{isozaki,sudoh,tsukada,kevinduh}@cslab.kecl.ntt.co.jpAbstractEnglish is a typical SVO (Subject-Verb-Object) language, while Japanese is a typ-ical SOV language.
Conventional Statis-tical Machine Translation (SMT) systemswork well within each of these languagefamilies.
However, SMT-based translationfrom an SVO language to an SOV lan-guage does not work well because theirword orders are completely different.
Re-cently, a few groups have proposed rule-based preprocessing methods to mitigatethis problem (Xu et al, 2009; Hong et al,2009).
These methods rewrite SVO sen-tences to derive more SOV-like sentencesby using a set of handcrafted rules.
In thispaper, we propose an alternative single re-ordering rule: Head Finalization.
Thisis a syntax-based preprocessing approachthat offers the advantage of simplicity.
Wedo not have to be concerned about part-of-speech tags or rule weights because thepowerful Enju parser allows us to imple-ment the rule at a general level.
Our ex-periments show that its result, Head FinalEnglish (HFE), follows almost the sameorder as Japanese.
We also show that thisrule improves automatic evaluation scores.1 IntroductionStatistical Machine Translation (SMT) is usefulfor building a machine translator between a pair oflanguages that follow similar word orders.
How-ever, SMT does not work well for distant languagepairs such as English and Japanese, since Englishis an SVO language and Japanese is an SOV lan-guage.Some existing methods try to solve this word-order problem in language-independent ways.They usually parse input sentences and learn a re-ordering decision at each node of the parse trees.For example, Yamada and Knight (2001), Quirk etal.
(2005), Xia and McCord (2004), and Li et al(2007) proposed such methods.Other methods tackle this problem in language-dependent ways (Katz-Brown and Collins, 2008;Collins et al, 2005; Nguyen and Shimazu, 2006).Recently, Xu et al (2009) and Hong et al (2009)proposed rule-based preprocessing methods forSOV languages.
These methods parse input sen-tences and reorder the words using a set of hand-crafted rules to get SOV-like sentences.If we could completely reorder the words in in-put sentences by preprocessing to match the wordorder of the target language, we would be able togreatly reduce the computational cost of SMT sys-tems.In this paper, we introduce a single reorderingrule: Head Finalization.
We simply move syntac-tic heads to the end of the corresponding syntacticconstituents (e.g., phrases and clauses).
We useonly this reordering rule, and we do not have toconsider part-of-speech tags or rule weights be-cause the powerful Enju parser allows us to im-plement the rule at a general level.Why do we think this works?
The reason issimple: Japanese is a typical head-final language.That is, a syntactic head word comes after non-head (dependent) words.
SOV is just one as-pect of head-final languages.
In order to imple-ment this idea, we need a parser that outputs syn-tactic heads.
Enju is such a parser from theUniversity of Tokyo (http://www-tsujii.is.s.u-tokyo.ac.jp/enju).
We discuss other parsersin section 5.There is another kind of head: semantic heads.Hong et al (2009) used Stanford parser (de Marn-effe et al, 2006), which outputs semantic head-based dependencies; Xu et al (2009) also used thesame representation.The use of syntactic heads and the numberof dependents are essential for the simplicity of244Head Finalization (See Discussion).
Our methodsimply checks whether a tree node is a syntactichead.
We do not have to consider what we aremoving and how to move it.
On the other hand, Xuet al had to introduce dozens of weighted rules,probably because they used the semantic head-based dependency representation without restric-tion on the number of dependents.The major difference between our method andthe above conventional methods, other than itssimplicity, is that our method moves not only verbsand adjectives but also functional words such asprepositions.2 Head FinalizationFigure 1 shows Enju?s XML output for the simplesentence: ?John hit a ball.?
The tag <cons>indicates a nonterminal node and <tok> indicatesa terminal node or a word (token).
Each node hasa unique id.
Head information is given by thenode?s head attribute.
For instance, node c0?s headis node c3, and c3 is a VP, or verb phrase.
Thus,Enju treats not only words but also non-terminalnodes as heads.Enju outputs at most two child nodes for eachnode.
One child is a head and the other is a depen-dent.
c3?s head is c4, which is VX, or a fragment ofa verb phrase.
c4?s head is t1 or hit, which is VBDor a past-tense verb.
The upper picture of Figure 2shows the parse tree graphically.
Here, ?
indicatesan edge that is linked from a ?head.
?Our Head Finalization rule simply swaps twochildren when the head child appears before thedependent child.
In the upper picture of Fig.
2, c3has two children c4 and c5.
Here, c3?s head c4appears before c5, so c4 and c5 are swapped.The lower picture shows the swapped result.Then we get John a ball hit, which has thesame word order as its Japanese translation jon wabohru wo utta except for the functional words a,wa, and wo.We have to add Japanese particles wa (topicmarker) or ga (nominative case marker) for Johnand wo (objective case marker) for ball to get anacceptable Japanese sentence.It is well known that SMT is not good at gen-erating appropriate particles from English, whitchdoes not have particles.
Particle generation wastackled by a few research groups (Toutanova andSuzuki, 2007; Hong et al, 2009).Here, we use Enju?s output to generate seeds?sentence id=?s0?
parse status=?success??
?cons id=?c0?
cat=?S?
xcat=??
head=?c3??
?cons id=?c1?
cat=?NP?
xcat=??
head=?c2??
?cons id=?c2?
cat=?NX?
xcat=??
head=?t0??
?tok id=?t0?
cat=?N?
pos=?NNP?base=?john??John?/tok??/cons??/cons?
?cons id=?c3?
cat=?VP?
xcat=??
head=?c4??
?cons id=?c4?
cat=?VX?
xcat=??
head=?t1??
?tok id=?t1?
cat=?V?
pos=?VBD?
base=?hit?arg1=?c1?
arg2=?c5??hit?/tok??/cons?
?cons id=?c5?
cat=?NP?
xcat=??
head=?c7??
?cons id=?c6?
cat=?DP?
xcat=??
head=?t2?
?tok id=?t2?
cat=?D?
pos=?DT?
base=?a?arg1=?c7??a?/tok??/cons?
?cons id=?c7?
cat=?NX?
xcat=??
head=?t3??
?tok id=?t3?
cat=?N?
pos=?NN?base=?ball??ball?/tok??/cons??/cons??/cons??/cons?.
?/sentence?Figure 1: Enju?s XML output (some attributes areremoved for readability).t0Johnt1hitt2at3ballc7?c6?c5?c4?c3?c2?c1?c0 Original English?t0Johnjon (wa)t1hituttat2a?t3ballbohru (wo)c7?c6?c5?c4?c3?c2?c1?c0 Head Final English?Figure 2: Head Finalization of a simple sentence(?
indicates a head).2452John5went7to9the10police12because15Mary17lost19his20wallet1?
14?8?
18?6?4?
16?13?11?3?0 Original English?2Johnjon (wa)5Marymeari (ga)19hiskare no20walletsaifu (wo)17lostnakushita12becausenode9the?10policekeisatsu7toni5wentitta1?
14?
8?18?6?4?16?13?11?3 ?0 Head Final English?Figure 3: Head-Finalizing a complex sentence.for particles.
As Fig.
1 shows, the verb hit hasarg1="c1" and arg2="c5".
This indicates that c1(John) is the subject of hit and c5 (a ball) isthe object of hit.
We add seed words va1 afterarg1 and va2 after arg2.
Then, we obtain Johnva1 a ball va2 hit.
We do not have to addarg2 for be because be?s arg2 is not an object buta complement.
We introduced the idea of particleseed words independently but found that it is verysimilar to Hong et al (2009)?s method for Korean.Figure 3 shows Enju?s parse tree for amore complicated sentence ?John went to thepolice because Mary lost his wallet.?
Forbrevity, we hide the terminal nodes, and we re-moved the nonterminal nodes?
prefix c.Conventional Rule-Based Machine Translation(RBMT) systems swap X and Y of ?X because Y?and move verbs to the end of each clause.
Then weget ?Mary his wallet lost because John the policeto went.?
Its word-to-word translation is a fluentJapanese sentence: meari (ga) kare no saifu (wo)nakushita node jon (wa) keisatsu ni itta.On the other hand, our Head Finalization withparticle seed words yields a slightly different wordorder ?John va1 Mary va1 his wallet va2 lostbecause the police to went.?
Its word-to-wordtranslation is jon wa meari ga kare no saifu wonakushita node keisatsu ni itta.
This is also an ac-ceptable Japanese sentence.This difference comes from the syntactic roleof ?because.?
In our method, Enju states thatbecause is a dependent of went, whereas RBMTsystems treat because as a clause conjunction.When we use Xu et al?s preprocessing method,?because?
moves to the beginning of the sentence.We do not know a good monotonic translation ofthe result.Preliminary experiments show that HFE looksgood as a first approximiation of Japanese wordorder.
However, we can make it better by intro-ducing some heuristic rules.
(We did not see thetest set to develop these heuristic rules.
)From a preliminary experiment, we found thatcoordination expressions such as A and B and Aor B are reordered as B and A and B or A. Al-though A and B have syntactically equal positions,the order of these elements sometimes matters.Therefore, we decided to stop swapping them atcoordination nodes, which are indicated cat andxcat attributes of the Enju output.
We call thisthe coordination exception rule.
In addition,we avoid Enju?s splitting of numerical expressionssuch as ?12,345?
and ?(1)?
because this splittingleads to inappropriate word orders.2463 ExperimentsIn order to show how closely our Head Finaliza-tion makes English follow Japanese word order,we measured Kendall?s ?
, a rank correlation co-efficient.
We also measured BLEU (Papineni etal., 2002) and other automatic evaluation scores toshow that Head Finalization can actually improvethe translation quality.We used NTCIR7 PAT-MT?s Patent corpus (Fu-jii et al, 2008).
Its training corpus has 1.8 mil-lion sentence pairs.
We used MeCab (http://mecab.sourceforge.net/) to segment Japanesesentences.3.1 Rough evaluation of reorderingFirst, we examined rank correlation between HeadFinal English sentences produced by the Head Fi-nalization rule and Japanese reference sentences.Since we do not have handcrafted word alignmentdata for an English-to-Japanese bilingual corpus,we used GIZA++ (Och and Ney, 2003) to get au-tomatic word alignment.Based on this automatic word alignment, wemeasured Kendall?s ?
for the word order betweenHFE sentences and Japanese sentences.
Kendall?s?
is a kind of rank correlation measure defined asfollows.
Suppose a list of integers such as L = [2,1, 3, 4].
The number of all integer pairs in this listis 4C2 = 4 ?
3/(2 ?
1) = 6.
The number of in-creasing pairs is five: (2, 3), (2, 4), (1, 3), (1, 4),and (3, 4).
Kendall?s ?
is defined by?
= #increasing pairs#all pairs?
2?
1.In this case, we get ?
= 5/6?
2?
1 = 0.667.For each sentence in the training data,we calculate ?
based on a GIZA++ align-ment file, en-ja.A3.final.
(We also triedja-en.A3.final, but we got similar results.)
Itlooks something like this:John hit a ball .NULL ({3}) jon ({1}) wa ({}) bohru ({4})wo ({}) utta ({2}) .
({5})Numbers in ({ }) indicate corresponding En-glish words.
The article ?a?
has no correspond-ing word in Japanese, and such words are listedin NULL ({ }).
From this alignment information,we get an integer list [1, 4, 2, 5].
Then, we get?
= 5/4C2 ?
2?
1 = 0.667.For HFE in Figure 2, we will get the followingalignment.John va1 a ball va2 hit .NULL ({3}) jon ({1}) wa ({2}) bohru ({4})wo ({5}) utta ({6}) .
({7})Then, we get [1, 2, 4, 5, 6, 7] and ?
= 1.0.
Weuse ?
or the average of ?
over all training sentencesto observe the tendency.Sometimes, one Japanese word corresponds toan English phrase:John went to Costa Rica .NULL ({}) jon ({1}) wa ({}) kosutarika ({4 5})ni ({3}) itta ({2}) .
({6})We get [1, 4, 5, 3, 2, 6] from this alignment.When the same word (or derivative words) ap-pears twice or more in a single English sentence,two or more non-consecutive words in the Englishsentence are aligned to a single Japanese word:rate of change of speedNULL ({}) sokudo ({5}) henka ({3})no ({2 4}) wariai ({1})We excluded the ambiguously aligned words (24) from the calculation of ?
.
We use only [5, 3,1] and get ?
= ?1.0.
The exclusion of thesewords will be criticized by statisticians, but eventhis rough calculation of ?
sheds light on the weakpoints of Head Finalization.Because of this exclusion, the best value ?
=1.0 does not mean that we obtained the perfectword ordering, but low ?
values imply failures.
Insection 4, we use ?
to analyze failures.By examining low ?
sentences, we found thatpatent documents have a lot of expressions suchas ?motor 2.?
These are reordered (2 motor) andslightly degrade ?
.
We did not notice this problemuntil we handled the patent corpus because theseexpressions are rare in other documents such asnews articles.
Here, we added a rule to keep theseexpressions.We did not use any dictionary in our experi-ment, but if we add dictionary entries to the train-ing data, it raises ?
because most entries are short.One-word entries do not affect ?
because we can-not calculate ?
.
Most multi-word entries are shortnoun phrases that are not reordered (?
= 1.0).Therefore, we should exclude dictionary entriesfrom the calculation of ?
.3.2 Quality of translationIt must be noted that the rank correlation does notdirectly measure the quality of translation.
There-fore, we also measured BLEU and other automaticevaluation scores of the translated sentences.
Weused Moses (Koehn, 2010) for Minimum ErrorRate Training and decoding.2470%5%10%15%20%-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0?
of English sentences0%5%10%15%20%-1.0 -0.8 -0.6 -0.4 -0.2 0.0 0.2 0.4 0.6 0.8 1.0?
of Head Finalized English sentencesFigure 4: Distribution of ?We used the development set (915 sentences) inthe NTCIR7 PAT-MT PSD data as well as the for-mal run test set (1,381 sentences).In the NTCIR7 PAT-MT workshop held in 2008,its participants used different methods such as hi-erarchical phrase-based SMT, RBMT, and EBMT(Example-Based Machine Translation).
However,the organizers?
Moses-based baseline system ob-tained the best BLEU score.4 ResultsFirst, we show ?
values to evaluate word order,and then we show BLEU and other automatic eval-uation scores.4.1 Rank correlationThe original English sentences have ?
= 0.451.Head Finalization improved it to 0.722.
Figure4 shows the distribution of ?
for all training sen-tences.
HFE reduces the percentage of low ?
sen-tences: 49.6% of the 1.8 million HFE sentenceshave ?
?
0.8 and 15.1% have ?
= 1.0.We also implemented Xu et al?s method withthe Stanford parser 1.6.2.
Its ?
was 0.624.
Therate of the sentences with ?
?
0.8 was 30.6% andthe rate of ?
= 1.0 was 4.3%.We examined low ?
sentences of our methodand found the following reasons for low ?
values.?
The sentence pair is not an exact one-to-onetranslation.
A Japanese reference sentencefor ?I bought the cake.?
can be some-thing like ?The cake I bought.?
or ?Theperson who bought the cake is me.??
Mistakes in Enju?s tagging or parsing.
Weencountered certain POS tag mistakes:?
VBZ/NNS mistake: ?advances?
of ?.
.
.device advances along .
.
.?
is VBZ,main cause counttagging/parsing mistakes 12VBN/VBD mistake (4)VBZ/NNS mistake (2)comma or and (2)inexact translation 7wrong alignment 1Table 1: Main causes of 20 worst sentencesbut NNS is assigned.?
VBN/VBD mistake: ?encoded?
of?.
.
.
the error correction encodeddata is supplied .
.
.?
is VBN, butVBD is assigned.These tagging mistakes lead to global parsingmistakes.
In addition, just like other parsers,Enju tends to make mistakes when a sentencehas a comma or ?and.??
Mistakes/Ambiguity of GIZA++ automaticword alignment.
Ambiguity happens whena single sentence has two or more occur-rences of a word or derivatives of a word(e.g., difference/different/differential).
As wedescribed above, ambiguously aligned wordsare removed from calculation of ?
, and smallreordering mistakes in other words are em-phasized.We analyzed the 20 worst sentences with ?
<?0.5 when we used only 400,000 sentences forGIZA++.
Their causes are summarized in Table1.
In general, low ?
sentences have two or morecauses, but here we show only the most influen-tial cause for each sentence.
This table shows thatmistakes in tagging and parsing are major causesof low ?
values.
When we used all of 1.8 million248Method BLEU WER TERproposed (0) 30.79 0.663 0.554proposed (3) 30.97 0.665 0.554proposed (6) 31.21 0.660 0.549proposed (9) 31.11 0.661 0.549proposed (12) 30.98 0.662 0.551proposed (15) 31.00 0.662 0.552no va (6) 30.99 0.669 0.559Organizer 30.58 0.755 0.592Table 2: Automatic Evaluation of TranslationQuality (Numbers in parentheses indicate distor-tion limits).sentence pairs, only 11 sentences had ?
< ?0.5among the 1.8 million sentences.4.2 Automatic Evaluation of TranslationQualityIn general, it is believed that translation betweenEnglish and Japanese requires a large distortionlimit (dl), which restricts how far a phrase canmove.
SMT reasearchers working on E-J or J-E translation often use dl=?1 (unlimited) as adefault value, and this takes a long translationtime.For PATMT J-E translation, Katz-Brown andCollins (2008) showed that dl=unlimited is thebest and it requires a very long translation time.For PATMT E-J translation, Kumai et al (2008)claimed that they achieved the best result ?whenthe distortion limit was 20 instead of ?1.
?Table 2 compares the single-reference BLEUscore of the proposed method and that of theMoses-based system by the NTCIR-7 PATMTorganizers.
This organizers?
system was betterthan all participants (Fujii et al, 2008) in termsof BLEU.
Here, we used Bleu Kit (http://www.mibel.cs.tsukuba.ac.jp/norimatsu/bleu kit/) following the PATMT?s overviewpaper (Fujii et al, 2008).
The table shows thatdl=6 gives the best result, and even dl=0 (noreordering in Moses) gives better scores than theorganizers?
Moses.Table 2 also shows Word Error Rates (WER)and Translation Error Rates (TER) (Snover et al,2006).
Since they are error rates, smaller is better.Although the improvement of BLEU is not veryimpressive, the score of WER is greatly reduced.This difference comes from the fact that BLEUmeasures only local word order, while WER mea-Method ROUGE-L IMPACT PERproposed (6) 0.480 0.369 0.390no va (6) 0.475 0.368 0.398Organizer 0.403 0.339 0.384Table 3: Improvement in word ordersures global word order.
Another line ?no va?stands for our method without vas or particleseeds.
Without particle seeds, all scores slightlydrop.Echizen-ya et al (2009) showed that IMPACTand ROUGE-L are highly correlated to humanevaluation in evaluating J-E patent translation.Therefore, we also used these evaluation methodshere for E-J translation.
Table 3 shows that theproposed method is also much better than the or-ganizers?
Moses in terms of these measures.
With-out particle seeds, these scores also drop slightly.On the other hand, Position-independent WordError Rate (PER), which completely disregardsword order, does not change very much.
Thesefacts indicate that our method improves word or-der, which is the most important problem in E-Jtranslation.The organizers?
Moses uses dl=unlimited, andit has been reported that its MERT training tooktwo weeks.
On the other hand, our MERT trainingwith dl=6 took only eight hours on a PC: XeonX5570 2.93 GHz.
Our method takes extra time toparse sentences by Enju, but it is easy to run theparser in parallel.5 DiscussionOur method used an HPSG parser, which givesrich information, but it is not easy to build such aparser.
It is much easier to build word dependencyparsers and Penn Treebank-style parsers.
In orderuse these parsers, we have to add some heuristicrules.5.1 Word Dependency ParsersAt first, we thought that we could substitute a worddependency parser for Enju by simply rephrasinga head with a modified word.
Xu et al (2009)used a semantic head-based dependency parser fora similar purpose.
Even when we use a syntac-tic head-based dependency parser instead, we en-countered their ?excessive movement?
problem.A straightforward application of their ruleschanges2493John5hit7the8ball10but13Sam15threw17the18ball16?14?12?11?9?6?4?2?1?0?xcat="COOD"cat="COOD"Figure 5: Head Finilization does not mix upclauses(0) John hit the ball but Sam threw the ball.to(1) John the ball but Sam the ball threw hit.Here, the two clauses are mixed up.
To preventthis, they disallow any movement across punctua-tion and conjunctions.
Then they get a better re-sult:(2) John the ball hit but Sam the ball threw.When we used Enju, these clauses were notmixed up.
Enju-based Head Finalization gave thesame word order as (2):(3) John va1 ball va2 hit but Sam va1 ball va2throw.Figure 5 shows Enju?s parse tree.
When Head Fi-nalization swaps the children of a mother node,the children do not move beyond the range ofthe mother node.
Therefore, Head Finalizationbased on Enju does not mix up the first clauseJohn hit the ball covered by Node 1 with thesecond clause Sam threw the ball covered byNode 11.
Moreover, our coordination exceptionrule keeps the order of these clauses.
Thus, non-terminal nodes in Enju?s output are useful to pro-tect clauses.When we use a word-dependency parser, we as-sume that the modified words are heads.
Further-more, the Head Finalization rule is rephrased as?move modified words after modifiers.?
There-fore, hit is moved after threw just like (2), andthe two clauses become mixed up.
Consequently,we need a heuristic rule like Xu?s.5.2 Penn Treebank-style parsersWe also tried Charniak-Johnson?s parser (Char-niak and Johnson, 2005).
PyInputTree(http://www.cs.brown.edu/?dmcc/software/PyInputTree/) gives heads.
Enju outputs atmost two children for a mother node, but PennTreebank-style parsers do not have such a limita-tion on the number of children.
This fact causes aproblem.When we use Enju, ?This toy is popular inJapan?
is reordered as ?This toy va1 Japan inpopular is.?
Its monotonic translation is fluent:kono omocha wa nihon de ninki ga aru.On the other hand, Charniak-Johnson?s parseroutputs the following S-expression for this sen-tence (we added asterisks (*) to indicate heads).
(S (NP (DT This) (NN* toy))(VP* (AUX* is)(ADJP (JJ* popular))(PP (IN* in) (NP (NNP* Japan)))))Simply moving heads to the end introduces?Japan in?
between ?is?
and ?popular?
: this toyva1 popular Japan in is.
It is difficult to translatethis monotonically because of this interruption.Reversing the children order (Xu et al, 2009)reconnects is and popular.
We get ?This toy(va1) Japan in popular is?
from the follow-ing reversed S-expression.
(S (NP (DT This) (NN* toy))(VP* (PP (IN* in) (NP (NNP* Japan)))(ADJP (JJ* popular))(AUX* is)))5.3 Limitation of Head FinalizationHead Finalization gives a good first approximationof Japanese word order in spite of its simplicity.However, it is not perfect.
In fact, a small distor-tion limit improved the performance.Sometimes, the Japanese language does nothave an appropriate word for monotonic transla-tion.
For instance, ?I have no time?
becomes?I va1 no time va2 have.?
Its monotonic trans-lation is ?watashi wa nai jikan wo motteiru,?but this sentence is not acceptable.
An acceptableliteral translation is ?watashi wa jikan ga nai.
?Here, ?no?
corresponds to ?nai?
at the end of thesentence.6 ConclusionTo solve the word-order problem between SVOlanguages and SOV langugages, we introduceda new reordering rule called Head Finalization.This rule is simple, and we do not have to considerPOS tags or rule weights.
We also showed that thisreordering improved automatic evaluation scoresof English-to-Japanese translation.
Improvementof the BLEU score is not very impressive, butother evaluation scores (WER, TER, LOUGE-L,and IMPACT) are greatly improved.250However, Head Finalization requires a sophis-ticated HPSG tagger such as Enju.
We showedthat severe failures are caused by Enju?s POS tag-ging mistakes.
We discussed the problems of otherparsers and how to solve them.Our future work is to build our own parser thatmakes fewer errors and to apply Head Finalizationto other SOV languages such as Korean.AcknowledgementsWe would like to thank Dr. Yusuke Miyao forhis useful advice on the usage of Enju.
We alsothank anonymous reviewers for their valuable sug-gestions.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proc.
of the Annual Meeting of the As-sociation of Computational Linguistics (ACL), pages173?180.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of the Annual Meeting of theAssociation of Computational Linguistics (ACL).Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProc.
of the Language Resources and EvaluationConference (LREC), pages 449?454.Hiroshi Echizen-ya, Terumasa Ehara, Sayori Shimo-hata, Atsushi Fujii, Masao Utiyama, Mikio Ya-mamoto, Takehito Utsuro, and Noriko Kando.
2009.Meta-evaluation of automatic evaluation methodsfor machine translation using patent translation datain NTCIR-7.
In Proceedings of the 3rd Workshop onPatent Translation, pages 9?16.Atsushi Fujii, Masao Utiyama, Mikio Yamamoto, andTakehito Utsuro.
2008.
Overview of the patenttranslation task at the NTCIR-7 workshop.
In Work-ing Notes of the NTCIR Workshop Meeting (NTCIR),pages 389?400.Gumwon Hong, Seung-Wook Lee, and Hae-ChangRim.
2009.
Bridging morpho-syntactic gap be-tween source and target sentences for English-Korean statistical machine translation.
In Proc.
ofACL-IJCNLP, pages 233?236.Jason Katz-Brown and Michael Collins.
2008.
Syn-tactic reordering in preprocessing for Japanese ?English translation: MIT system description forNTCIR-7 patent translation task.
In Working Notesof the NTCIR Workshop Meeting (NTCIR).Philipp Koehn, 2010.
MOSES, Statistical MachineTranslation System, User Manual and Code Guide.Hiroyuki Kumai, Hirohiko Segawa, and YasutsuguMorimoto.
2008.
NTCIR-7 patent translation ex-periments at Hitachi.
In Working Notes of the NT-CIR Workshop Meeting (NTCIR), pages 441?444.Chi-Ho Li, Dongdong Zhang, Mu Li, Ming Zhou,Minghui Li, and Yi Guan.
2007.
A probabilisticapproach to syntax-based reordering for statisticalmachine translation.
In Proc.
of the Annual Meet-ing of the Association of Computational Linguistics(ACL), pages 720?727.Thai Phuong Nguyen and Akira Shimazu.
2006.Improving phrase-based statistical machine transla-tion with morphosyntactic transformation.
MachineTranslation, 20(3):147?166.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proc.
of theAnnual Meeting of the Association of ComputationalLinguistics (ACL), pages 311?318.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically in-formed phrasal SMT.
In Proc.
of the Annual Meet-ing of the Association of Computational Linguistics(ACL), pages 271?279.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas.Kristina Toutanova and Hisami Suzuki.
2007.
Gener-ating case markers in machine translation.
In Proc.of NAACL-HLT, pages 49?56.Fei Xia and Michael McCord.
2004.
Improvinga statistical MT system with automatically learnedrewrite patterns.
In Proc.
of the International Con-ference on Computational Linguistics (COLING),pages 508?514.Peng Xu, Jaeho Kang, Michael Ringgaard, and FranzOch.
2009.
Using a dependency parser to improveSMT for Subject-Object-Verb languages.
In Proc.of NAACL-HLT, pages 245?253.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
of theAnnual Meeting of the Association of ComputationalLinguistics (ACL), pages 523?530.251
