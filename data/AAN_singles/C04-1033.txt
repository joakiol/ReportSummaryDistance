An NP-Cluster Based Approach to Coreference ResolutionXiaofeng Yang??
Jian Su?
Guodong Zhou?
Chew Lim Tan?
?Institute for Infocomm Research21 Heng Mui Keng Terrace,Singapore, 119613{xiaofengy,sujian,zhougd}@i2r.a-star.edu.sg?
Department of Computer ScienceNational University of Singapore,Singapore, 117543{yangxiao,tancl}@comp.nus.edu.sgAbstractTraditionally, coreference resolution is doneby mining the reference relationships be-tween NP pairs.
However, an individual NPusually lacks adequate description informa-tion of its referred entity.
In this paper,we propose a supervised learning-based ap-proach which does coreference resolution byexploring the relationships between NPs andcoreferential clusters.
Compared with indi-vidual NPs, coreferential clusters could pro-vide richer information of the entities for bet-ter rules learning and reference determina-tion.
The evaluation done on MEDLINEdata set shows that our approach outper-forms the baseline NP-NP based approachin both recall and precision.1 IntroductionCoreference resolution is the process of linkingas a cluster1 multiple expressions which referto the same entities in a document.
In recentyears, supervised machine learning approacheshave been applied to this problem and achievedconsiderable success (e.g.
Aone and Bennett(1995); McCarthy and Lehnert (1995); Soon etal.
(2001); Ng and Cardie (2002b)).
The mainidea of most supervised learning approaches is torecast this task as a binary classification prob-lem.
Specifically, a classifier is learned and thenused to determine whether or not two NPs in adocument are co-referring.
Clusters are formedby linking coreferential NP pairs according to acertain selection strategy.
In this way, the identi-fication of coreferential clusters in text is reducedto the identification of coreferential NP pairs.One problem of such reduction, however,is that the individual NP usually lacks ade-quate descriptive information of its referred en-tity.
Consequently, it is often difficult to judgewhether or not two NPs are talking about the1In this paper the term ?cluster?
can be interchange-ably used as ?chain?, while the former better emphasizesthe equivalence property of coreference relationship.same entity simply from the properties of thepair alone.
As an example, consider the pair of anon-pronoun and its pronominal antecedent can-didate.
The pronoun itself gives few clues for thereference determination.
Using such NP pairswould have a negative influence for rules learn-ing and subsequent resolution.
So far, severalefforts (Harabagiu et al, 2001; Ng and Cardie,2002a; Ng and Cardie, 2002b) have attempted toaddress this problem by discarding the ?hard?pairs and select only those confident ones fromthe NP-pair pool.
Nevertheless, this eliminat-ing strategy still can not guarantee that the NPsin ?confident?
pairs bear necessary descriptioninformation of their referents.In this paper, we present a supervisedlearning-based approach to coreference resolu-tion.
Rather than attempting to mine the ref-erence relationships between NP pairs, our ap-proach does resolution by determining the linksof NPs to the existing coreferential clusters.
Inour approach, a classifier is trained on the in-stances formed by an NP and one of its possi-ble antecedent clusters, and then applied dur-ing resolution to select the proper cluster for anencountered NP to be linked.
As a coreferen-tial cluster offers richer information to describean entity than a single NP in the cluster, wecould expect that such an NP-Cluster frameworkwould enhance the resolution capability of thesystem.
Our experiments were done on the theMEDLINE data set.
Compared with the base-line approach based on NP-NP framework, ourapproach yields a recall improvement by 4.6%,with still a precision gain by 1.3%.
These resultsindicate that the NP-Cluster based approach iseffective for the coreference resolution task.The remainder of this paper is organized asfollows.
Section 2 introduces as the baseline theNP-NP based approach, while Section 3 presentsin details our NP-Cluster based approach.
Sec-tion 4 reports and discusses the experimental re-sults.
Section 5 describes related research work.Finally, conclusion is given in Section 6.2 Baseline: the NP-NP basedapproach2.1 Framework descriptionWe built a baseline coreference resolution sys-tem, which adopts the common NP-NP basedlearning framework as employed in (Soon et al,2001).Each instance in this approach takes the formof i{NPj , NPi}, which is associated with a fea-ture vector consisting of 18 features (f1 ?
f18) asdescribed in Table 2.
Most of the features comefrom Soon et al (2001)?s system.
Inspired by thework of (Strube et al, 2002) and (Yang et al,2004), we use two features, StrSim1 (f17) andStrSim2 (f18), to measure the string-matchingdegree of NPj and NPi.
Given the following sim-ilarity function:Str Simlarity(Str1, Str2) = 100?
|Str1 ?
Str2|Str1StrSim1 and StrSim2 are computedusing Str Similarity(SNPj , SNPi) andStr Similarity(SNPi , SNPj ), respectively.
HereSNP is the token list of NP, which is obtainedby applying word stemming, stopword removaland acronym expansion to the original string asdescribed in Yang et al (2004)?s work.During training, for each anaphor NPj in agiven text, a positive instance is generated bypairing NPj with its closest antecedent.
A setof negative instances is also formed by NPj andeach NP occurring between NPj and NPi.When the training instances are ready, a clas-sifier is learned by C5.0 algorithm (Quinlan,1993).
During resolution, each encountered nounphrase, NPj , is paired in turn with each preced-ing noun phrase, NPi.
For each pair, a test-ing instance is created as during training, andthen presented to the decision tree, which re-turns a confidence value (CF)2 indicating thelikelihood that NPi is coreferential to NPj .
Inour study, two antecedent selection strategies,Most Recent First (MRF) and Best First (BF),are tried to link NPj to its a proper antecedentwith CF above a threshold (0.5).
MRF (Soonet al, 2001) selects the candidate closest to theanaphor, while BF (Aone and Bennett, 1995; Ng2The confidence value is obtained by using thesmoothed ratio p+1t+2 , where p is the number of positiveinstances and t is the total number of instances containedin the corresponding leaf node.and Cardie, 2002b) selects the candidate withthe maximal CF.2.2 Limitation of the approachNevertheless, the problem of the NP-NP basedapproach is that the individual NP usually lacksadequate description information about its re-ferred entity.
Consequently, it is often difficultto determine whether or not two NPs refer tothe same entity simply from the properties ofthe pair.
See the the text segment in Table 1,for example,[1 A mutant of [2 KBF1/p50] ], unable tobind to DNA but able to form homo- or [3 het-erodimers] , has been constructed.
[4 This protein] reduces or abolishes the DNAbinding activity of wild-type proteins of [5 thesame family ([6 KBF1/p50] , c- and v-rel)].
[7 This mutant] also functions in vivo as atransacting dominant negative regulator:.
.
.Table 1: An Example from the data setIn the above text, [1 A mutant of KBF1/p50],[4 This protein] and [7 This mutant] are anno-tated in the same coreferential cluster.
Accord-ing to the above framework, NP7 and its closestantecedent, NP4, will form a positive instance.Nevertheless, such an instance is not informa-tive in that NP4 bears little information relatedto the entity and thus provides few clues to ex-plain its coreference relationship with NP7.In fact, this relationship would be clear if [1 Amutant of KBF1/p50], the antecedent of NP4,is taken into consideration.
NP1 gives a de-tailed description of the entity.
By comparingthe string of NP7 with this description, it is ap-parent that NP7 belongs to the cluster of NP1,and thus should be coreferential to NP4.
Thissuggests that we use the coreferential cluster,instead of its single element, to resolve an NPcorrectly.
In our study, we propose an approachwhich adopts an NP-Cluster based framework todo resolution.
The details of the approach aregiven in the next section.3 The NP-Cluster based approachSimilar to the baseline approach, our approachalso recasts coreference resolution as a binaryclassification problem.
The difference, however,is that our approach aims to learn a classifierwhich would select the most preferred cluster,instead of the most preferred antecedent, for anencountered NP in text.
We will give the frame-work of the approach, including the instance rep-Features describing the relationships between NPj and NPi1.
DefNp 1 1 if NPj is a definite NP; else 02.
DemoNP 1 1 if NPj starts with a demonstrative; else 03.
IndefNP 1 1 if NPj is an indefinite NP; else 04.
Pron 1 1 if NPj is a pronoun; else 05.
ProperNP 1 1 if NPj is a proper NP; else 06.
DefNP 2 1 if NPi is a definite NP; else 07.
DemoNP 2 1 if NPi starts with a demonstrative; else 08.
IndefNP 2 1 if NPi is an indefinite NP; else 09.
Pron 2 1 if NPi is a pronoun; else 010.
ProperNP 2 1 if NPi is a proper NP; else 011.
Appositive 1 if NPi and NPj are in an appositive structure; else 012.
NameAlias 1 if NPi and NPj are in an alias of the other; else 013.
GenderAgree 1 if NPi and NPj agree in gender; else 014.
NumAgree 1 if NPi and NPj agree in number; else 015.
SemanticAgree 1 if NPi and NPj agree in semantic class; else 016.
HeadStrMatch 1 if NPi and NPj contain the same head string; else 017.
StrSim 1 The string similarity of NPj against NPi18.
StrSim 2 The string similarity of NPi against NPjFeatures describing the relationships between NPj and cluster Ck19.
Cluster NumAgree 1 if Ck and NPj agree in number; else 020.
Cluster GenAgree 1 if Ck and NPj agree in gender; else 021.
Cluster SemAgree 1 if Ck and NPj agree in semantic class; else 022.
Cluster Length The number of elements contained in Ck23.
Cluster StrSim The string similarity of NPj against Ck24.
Cluster StrLNPSim The string similarity of NPj against the longest NP in CkTable 2: The features in our coreference resolution system (Features 1 ?
18 are also used in thebaseline system using NP-NP based approach)resentation, the training and the resolution pro-cedures, in the following subsections.3.1 Instance representationAn instance in our approach is composed of threeelements like below:i{NPj , Ck, NPi}where NPj , like the definition in the baseline,is the noun phrase under consideration, while Ckis an existing coreferential cluster.
Each clustercould be referred by a reference noun phrase NPi,a certain element of the cluster.
A cluster wouldprobably contain more than one reference NPsand thus may have multiple associated instances.For a training instance, the label is positive ifNPj is annotated as belonging to Ck, or negativeif otherwise.In our system, each instance is represented asa set of 24 features as shown in Table 2.
Thefeatures are supposed to capture the propertiesof NPj and Ck as well as their relationships.
Inthe table we divide the features into two groups,one describing NPj and NPi and the other de-scribing NPj and Ck.
For the former group, wejust use the same features set as in the baselinesystem, while for the latter, we introduce 6 morefeatures:Cluster NumAgree, Cluster GenAgreeand Cluster SemAgree: These three fea-tures mark the compatibility of NPj and Ckin number, gender and semantic agreement,respectively.
If NPj mismatches the agreementwith any element in Ck, the correspondingfeature is set to 0.Cluster Length: The number of NPs in thecluster Ck.
This feature reflects the globalsalience of an entity in the sense that the morefrequently an entity is mentioned, the more im-portant it would probably be in text.Cluster StrSim: This feature marks the stringsimilarity between NPj and Ck.
SupposeSNPj is the token set of NPj , we computethe feature value using the similarity functionStr Similarity(SNPj , SCk), whereSCk =?NPi?CkSNPiCluster StrLNPSim: It marks the stringmatching degree of NPj and the noun phrasein Ck with the most number of tokens.
Theintuition here is that the NP with the longeststring would probably bear richer description in-formation of the referent than other elements inthe cluster.
The feature is calculated using thesimilarity function Str Similarity(SNPj , SNPk),whereNPk = arg maxNPi?Ck |SNPi |3.2 Training procedureGiven an annotated training document, we pro-cess the noun phrases from beginning to end.For each anaphoric noun phrase NPj , we considerits preceding coreferential clusters from right toleft3.
For each cluster, we create only one in-stance by taking the last NP in the cluster asthe reference NP.
The process will not terminateuntil the cluster to which NPj belongs is found.To make it clear, consider the example in Ta-ble 1 again.
For the noun phrase [7 This mu-tant], the annotated preceding coreferential clus-ters are:C1: { .
.
.
, NP2, NP6 }C2: { .
.
.
, NP5 }C3: { NP1, NP4 }C4: { .
.
.
, NP3 }Thus three training instances are generated:i{ NP7, C1, NP6 }i{ NP7, C2, NP5 }i{ NP7, C3, NP4 }Among them, the first two instances are la-belled as negative while the last one is positive.After the training instances are ready, we useC5.0 learning algorithm to learn a decision treeclassifier as in the baseline approach.3.3 Resolution procedureThe resolution procedure is the counterpart ofthe training procedure.
Given a testing docu-ment, for each encountered noun phrase, NPj ,we create a set of instances by pairing NPj witheach cluster found previously.
The instances arepresented to the learned decision tree to judgethe likelihood that NPj is linked to a cluster.The resolution algorithm is given in Figure 1.As described in the algorithm, for each clus-ter under consideration, we create multiple in-stances by using every NP in the cluster as thereference NP.
The confidence value of the cluster3We define the position of a cluster as the position ofthe last NP in the cluster.algorithm RESOLVE (a testing document d)ClusterSet = ?
;//suppose d has N markable NPs;for j = 1 to Nforeach cluster in ClusterSetCFcluster = maxNPi?clusterCFi(NPj ,cluster,NPi)select a proper cluster, BestCluster, accordingto a ceterin cluster selection strategy;if BestCluster != NULLBestCluster = BestCluster ?
{NPj};else//create a new clusterNewCluster = { NPj };ClusterSet = ClusterSet ?
{NewCluster};Figure 1: The clusters identification algorithmis the maximal confidence value of its instances.Similar to the baseline system, two cluster selec-tion strategies, i.e.
MRF and BF, could be ap-plied to link NPj to a proper cluster.
For MRFstrategy, NPj is linked to the closest cluster withconfidence value above 0.5, while for BF, it islinked to the cluster with the maximal confidencevalue (above 0.5).3.4 Comparison of NP-NP andNP-Cluster based approachesAs noted above, the idea of the NP-Cluster basedapproach is different from the NP-NP based ap-proach.
However, due to the fact that in ourapproach a cluster is processed based on its refer-ence NPs, the framework of our approach couldbe reduced to the NP-NP based framework ifthe cluster-related features were removed.
Fromthis point of view, this approach could be con-sidered as an extension of the baseline approachby applying additional cluster features as theproperties of NPi.
These features provide richerdescription information of the entity, and thusmake the coreference relationship between twoNPs more apparent.
In this way, both ruleslearning and coreference determination capabili-ties of the original approach could be enhanced.4 Evaluation4.1 Data collectionOur coreference resolution system is a compo-nent of our information extraction system inbiomedical domain.
For this purpose, an anno-tated coreference corpus have been built 4, which4The annotation scheme and samples are avail-able in http://nlp.i2r.a-star.edu.sg/resources/GENIA-coreferenceMRF BFExperiments R P F R P FBaseline 80.2 77.4 78.8 80.3 77.5 78.9AllAnte 84.4 70.2 76.6 85.7 71.4 77.9Our Approach 84.4 78.2 81.2 84.9 78.8 81.7Table 3: The performance of different coreference resolution systemsconsists of totally 228 MEDLINE abstracts se-lected from the GENIA data set.
The aver-age length of the documents in collection is 244words.
One characteristic of the bio-literatureis that pronouns only occupy about 3% amongall the NPs.
This ratio is quite low comparedto that in newswire domain (e.g.
above 10% forMUC data set).A pipeline of NLP components is applied topre-process an input raw text.
Among them,NE recognition, part-of-speech tagging and textchunking adopt the same HMM based enginewith error-driven learning capability (Zhou andSu, 2002).
The NE recognition componenttrained on GENIA (Shen et al, 2003) canrecognize up to 23 common biomedical entitytypes with an overall performance of 66.1 F-measure (P=66.5% R=65.7%).
In addition, toremove the apparent non-anaphors (e.g., em-bedded proper nouns) in advance, a heuristic-based non-anaphoricity identification module isapplied, which successfully removes 50.0% non-anaphors with a precision of 83.5% for our dataset.4.2 Experiments and discussionsOur experiments were done on first 100 docu-ments from the annotated corpus, among them70 for training and the other 30 for testing.Throughout these experiments, default learningparameters were applied in the C5.0 algorithm.The recall and precision were calculated auto-matically according to the scoring scheme pro-posed by Vilain et al (1995).In Table 3 we compared the performance ofdifferent coreference resolution systems.
Thefirst line summarizes the results of the baselinesystem using traditional NP-NP based approachas described in Section 2.
Using BF strategy,Baseline obtains 80.3% recall and 77.5% preci-sion.
These results are better than the work byCastano et al (2002) and Yang et al (2004),which were also tested on the MEDLINE dataset and reported a F-measure of about 74% and69%, respectively.In the experiments, we evaluated another NP-NP based system, AllAnte.
It adopts a similarlearning framework as Baseline except that dur-ing training it generates the positive instances byparing an NP with all its antecedents instead ofonly the closest one.
The system attempts to usesuch an instance selection strategy to incorpo-rate the information from coreferential clusters.But the results are nevertheless disappointing:although this strategy boosts the recall by 5.4%,the precision drops considerably by above 6% atthe same time.
The overall F-measure is evenlower than the baseline systems.The last line of Table 3 demonstrates the re-sults of our NP-Cluster based approach.
For BFstrategy, the system achieves 84.9% recall and78.8% precision.
As opposed to the baseline sys-tem, the recall rises by 4.6% while the precisionstill gains slightly by 1.3%.
Overall, we observethe increase of F-measure by 2.8%.The results in Table 3 also indicate that theBF strategy is superior to the MRF strategy.A similar finding was also reported by Ng andCardie (2002b) in the MUC data set.To gain insight into the difference in the per-formance between our NP-Cluster based systemand the NP-NP based system, we compared thedecision trees generated in the two systems inFigure 2.
In both trees, the string-similarityfeatures occur on the top portion, which sup-ports the arguments by (Strube et al, 2002)and (Yang et al, 2004) that string-matching is acrucial factor for NP coreference resolution.
Asshown in the figure, the feature StrSim 1 in lefttree is completely replaced by the Cluster StrSimand Cluster StrLNPSim in the right tree, whichmeans that matching the tokens with a clusteris more reliable than with a single NP.
More-over, the cluster length will also be checked whenthe NP under consideration has low similarityagainst a cluster.
These evidences prove thatthe information from clusters is quite importantfor the coreference resolution on the data set.The decision tree visualizes the importance ofthe features for a data set.
However, the tree islearned from the documents where coreferentialclusters are correctly annotated.
During resolu-HeadMatch = 0::...NameAlias = 1: 1 (22/1): NameAlias = 0:: :...Appositive = 0: 0 (13095/265): Appositive = 1: 1 (15/4)HeadMatch = 1::...StrSim_1 > 71::...DemoNP_1 = 0: 1 (615/29): DemoNP_1 = 1:: :...NumAgree = 0: 0 (5): NumAgree = 1: 1 (26)StrSim_1 <= 71::...DemoNP_2 = 1: 1 (12/2)DemoNP_2 = 0::...StrSim_2 <= 77: 0 (144/17)StrSim_2 > 77::...StrSim_1 <= 33: 0 (42/11)StrSim_1 > 33: 1 (38/11)HeadMatch = 1::...Cluster_StrSim > 66: 1 (663/36): Cluster_StrSim <= 66:: :...StrSim_2 <= 85: 0 (140/14): StrSim_2 > 85:: :...Cluster_StrLNPSim > 50: 1 (16/1): Cluster_StrLNPSim <= 50:: :...Cluster_Length <= 5: 0 (59/17): Cluster_Length > 5: 1 (4)HeadMatch = 0::...NameAlias = 1: 1 (22/1)NameAlias = 0::...Appositive = 1: 1 (15/4)Appositive = 0::...StrSim_2 <= 54::..StrSim_2 > 54::..Figure 2: The resulting decision trees for the NP-NP and NP-Cluster based approachesFeatures R P Ff1?21 80.3 77.5 78.9f1?21, f22 84.1 74.4 79.0f1?21, f23 84.7 78.8 81.6f1?21, f24 84.3 78.0 81.0f1?21, f23, f22 84.9 78.6 81.6f1?21, f23, f24 84.9 78.9 81.8f1?21, f23, f24, f22 84.9 78.8 81.7Table 4: Performance using combined features(fi refers to the i(th) feature listed in Table 2)tion, unfortunately, the found clusters are usu-ally not completely correct, and as a result thefeatures important in training data may not bealso helpful for testing data.
Therefore, in theexperiments we were concerned about which fea-tures really matter for the real coreference res-olution.
For this purpose, we tested our systemusing different features and evaluated their per-formance in Table 4.
Here we just considered fea-ture Cluster Length (f22), Cluster StrSim (f23)and Cluster StrLNPSim (f24), as Figure 2 hasindicated that among the cluster-related featuresonly these three are possibly effective for resolu-tion.
Throughout the experiment, the Best-Firststrategy was applied.As illustrated in the table, we could observethat:1.
Without the three features, the system isequivalent to the baseline system in termsof the same recall and precision.2.
Cluster StrSim (f23) is the most effectiveas it contributes most to the system per-formance.
Simply using this feature booststhe F-measure by 2.7%.3.
Cluster StrLNPSim (f24) is also effective byimproving the F-measure by 2.1% alone.When combined with f23, it leads to thebest F-measure.4.
Cluster Length (f22) only brings 0.1% F-measure improvement.
It could barelyincrease, or even worse, reduces the F-measure when used together with the theother two features.5 Related workTo our knowledge, our work is the firstsupervised-learning based attempt to do coref-erence resolution by exploring the relationshipbetween an NP and coreferential clusters.
In theheuristic salience-based algorithm for pronounresolution, Lappin and Leass (1994) introducea procedure for identifying anaphorically linkedNP as a cluster for which a global salience valueis computed as the sum of the salience values ofits elements.
Cardie and Wagstaff (1999) haveproposed an unsupervised approach which alsoincorporates cluster information into considera-tion.
Their approach uses hard constraints topreclude the link of an NP to a cluster mismatch-ing the number, gender or semantic agreements,while our approach takes these agreements to-gether with other features (e.g.
cluster-length,string-matching degree,etc) as preference factorsfor cluster selection.
Besides, the idea of cluster-ing can be seen in the research of cross-documentcoreference, where NPs with high context simi-larity would be chained together based on certainclustering methods (Bagga and Biermann, 1998;Gooi and Allan, 2004).6 ConclusionIn this paper we have proposed a supervisedlearning-based approach to coreference resolu-tion.
Rather than mining the coreferential re-lationship between NP pairs as in conventionalapproaches, our approach does resolution by ex-ploring the relationships between an NP and thecoreferential clusters.
Compared to individualNPs, coreferential clusters provide more infor-mation for rules learning and reference determi-nation.
In the paper, we first introduced the con-ventional NP-NP based approach and analyzedits limitation.
Then we described in details theframework of our NP-Cluster based approach,including the instance representation, trainingand resolution procedures.
We evaluated our ap-proach in the biomedical domain, and the experi-mental results showed that our approach outper-forms the NP-NP based approach in both recall(4.6%) and precision (1.3%).While our approach achieves better perfor-mance, there is still room for further improve-ment.
For example, the approach just resolvesan NP using the cluster information available sofar.
Nevertheless, the text after the NP wouldprobably give important supplementary infor-mation of the clusters.
The ignorance of suchinformation may affect the correct resolution ofthe NP.
In the future work, we plan to work outmore robust clustering algorithm to link an NPto a globally best cluster.ReferencesC.
Aone and S. W. Bennett.
1995.
Evaluatingautomated and manual acquistion of anaphoraresolution strategies.
In Proceedings of the33rd Annual Meeting of the Association forCompuational Linguistics, pages 122?129.A.
Bagga and A. Biermann.
1998.
Entity-basedcross document coreferencing using the vectorspace model.
In Proceedings of the 36th An-nual Meeting of the Association for Computa-tional Linguisticsthe 17th International Con-ference on Computational Linguistics, pages79?85.C.
Cardie and K. Wagstaff.
1999.
Noun phrasecoreference as clustering.
In Proceedings ofthe Joint Conference on Empirical Methods inNLP and Very Large Corpora.J.
Castano, J. Zhang, and J. Pustejovsky.
2002.Anaphora resolution in biomedical literature.In International Symposium on Reference Res-olution, Alicante, Spain.C.
Gooi and J. Allan.
2004.
Cross-documentcoreference on a large scale corpus.
In Pro-ceedings of 2004 Human Language Technologyconference / North American chapter of theAssociation for Computational Linguistics an-nual meeting.S.
Harabagiu, R. Bunescu, and S. Maiorano.2001.
Text knowledge mining for coreferenceresolution.
In Proceedings of the 2nd An-nual Meeting of the North America Chapter ofthe Association for Compuational Linguistics,pages 55?62.S.
Lappin and H. Leass.
1994.
An algorithmfor pronominal anaphora resolution.
Compu-tational Linguistics, 20(4):525?561.J.
McCarthy and Q. Lehnert.
1995.
Using de-cision trees for coreference resolution.
In Pro-ceedings of the 14th International Conferenceon Artificial Intelligences, pages 1050?1055.V.
Ng and C. Cardie.
2002a.
Combining sam-ple selection and error-driven pruning for ma-chine learning of coreference rules.
In Proceed-ings of the conference on Empirical Methodsin Natural Language Processing, pages 55?62,Philadelphia.V.
Ng and C. Cardie.
2002b.
Improving ma-chine learning approaches to coreference res-olution.
In Proceedings of the 40th AnnualMeeting of the Association for ComputationalLinguistics, pages 104?111, Philadelphia.J.
R. Quinlan.
1993.
C4.5: Programs for ma-chine learning.
Morgan Kaufmann Publishers,San Francisco, CA.D.
Shen, J. Zhang, G. Zhou, J. Su, andC.
Tan.
2003.
Effective adaptation of hid-den markov model-based named-entity recog-nizer for biomedical domain.
In Proceedings ofACL03 Workshop on Natural Language Pro-cessing in Biomedicine, Japan.W.
Soon, H. Ng, and D. Lim.
2001.
A ma-chine learning approach to coreference resolu-tion of noun phrases.
Computational Linguis-tics, 27(4):521?544.M.
Strube, S. Rapp, and C. Muller.
2002.
Theinfluence of minimum edit distance on refer-ence resolution.
In Proceedings of the Confer-ence on Empirical Methods in Natural Lan-guage Processing, pages 312?319, Philadel-phia.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly,and L. Hirschman.
1995.
A model-theoreticcoreference scoring scheme.
In Proceedings ofthe Sixth Message understanding Conference(MUC-6), pages 45?52, San Francisco, CA.Morgan Kaufmann Publishers.X.
Yang, G. Zhou, J. Su, and C. Tan.
2004.
Im-proving noun phrase coreference resolution bymatching strings.
In Proceedings of the 1st In-ternational Joint Conference on Natural Lan-guage Processing, Hainan.G.
Zhou and J. Su.
2002.
Named Entity recog-nition using a HMM-based chunk tagger.
InProceedings of the 40th Annual Meeting ofthe Association for Computational Linguis-tics, Philadelphia.
