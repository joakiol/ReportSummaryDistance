Towards a Resource for Lexical Semantics:A Large German Corpus with Extensive Semantic AnnotationKatrin Erk and Andrea Kowalski and Sebastian Pado?
and Manfred PinkalDepartment of Computational LinguisticsSaarland UniversitySaarbru?cken, Germany{erk, kowalski, pado, pinkal}@coli.uni-sb.deAbstractWe describe the ongoing construction ofa large, semantically annotated corpusresource as reliable basis for the large-scale acquisition of word-semantic infor-mation, e.g.
the construction of domain-independent lexica.
The backbone of theannotation are semantic roles in the framesemantics paradigm.
We report expe-riences and evaluate the annotated datafrom the first project stage.
On this ba-sis, we discuss the problems of vaguenessand ambiguity in semantic annotation.1 IntroductionCorpus-based methods for syntactic learning andprocessing are well-established in computationallinguistics.
There are comprehensive and carefullyworked-out corpus resources available for a num-ber of languages, e.g.
the Penn Treebank (Marcus etal., 1994) for English or the NEGRA corpus (Skutet al, 1998) for German.
In semantics, the sit-uation is different: Semantic corpus annotation isonly in its initial stages, and currently only a few,mostly small, corpora are available.
Semantic an-notation has predominantly concentrated on wordsenses, e.g.
in the SENSEVAL initiative (Kilgarriff,2001), a notable exception being the Prague Tree-bank (Hajic?ova?, 1998) .
As a consequence, mostrecent work in corpus-based semantics has taken anunsupervised approach, relying on statistical meth-ods to extract semantic regularities from raw cor-pora, often using information from ontologies likeWordNet (Miller et al, 1990).Meanwhile, the lack of large, domain-independent lexica providing word-semanticinformation is one of the most serious bottlenecksfor language technology.
To train tools for theacquisition of semantic information for such lexica,large, extensively annotated resources are necessary.In this paper, we present current work of theSALSA (SAarbru?cken Lexical Semantics Annota-tion and analysis) project, whose aim is to providesuch a resource and to investigate efficient methodsfor its utilisation.
In the current project phase, thefocus of our research and the backbone of the an-notation are semantic role relations.
More specif-ically, our role annotation is based on the Berke-ley FrameNet project (Baker et al, 1998; Johnsonet al, 2002).
In addition, we selectively annotateword senses and anaphoric links.
The TIGER corpus(Brants et al, 2002), a 1.5M word German newspa-per corpus, serves as sound syntactic basis.Besides the sparse data problem, the most seri-ous problem for corpus-based lexical semantics isthe lack of specificity of the data: Word meaning isnotoriously ambiguous, vague, and subject to con-textual variance.
The problem has been recognisedand discussed in connection with the SENSEVALtask (Kilgarriff and Rosenzweig, 2000).
Annotationof frame semantic roles compounds the problem asit combines word sense assignment with the assign-ment of semantic roles, a task that introduces vague-ness and ambiguity problems of its own.The problem can be alleviated by choosing a suit-able resource as annotation basis.
FrameNet roles,which are local to particular frames (abstract sit-uations), may be better suited for the annotationtask than the ?classical?
thematic roles concept witha small, universal and exhaustive set of roles likeagent, patient, theme: The exact extension of therole concepts has never been agreed upon (Fillmore,1968).
Furthermore, the more concrete frame se-mantic roles may make the annotators?
task easier.The FrameNet database itself, however, cannot betaken as evidence that reliable annotation is pos-sible: The aim of the FrameNet project is essen-tially lexicographic and its annotation not exhaus-tive; it comprises representative examples for the useof each frame and its frame elements in the BNC.While the vagueness and ambiguity problem maybe mitigated by the using of a ?good?
resource, itwill not disappear entirely, and an annotation formatis needed that can cope with the inherent vaguenessof word sense and semantic role assignment.Plan of the paper.
In Section 2 we briefly intro-duce FrameNet and the TIGER corpus that we useas a basis for semantic annotation.
Section 3 givesan overview of the aims of the SALSA project, andSection 4 describes the annotation with frame se-mantic roles.
Section 5 evaluates the first annotationresults and the suitability of FrameNet as an anno-tation resource, and Section 6 discusses the effectsof vagueness and ambiguity on frame semantic roleannotation.
Although the current amount of anno-tated data does not allow for definitive judgements,we can discuss tendencies.2 ResourcesSALSA currently extends the TIGER corpus by se-mantic role annotation, using FrameNet as a re-source.
In the following, we will give a shortoverview of both resources.FrameNet.
The FrameNet project (Johnson et al,2002) is based on Fillmore?s Frame Semantics.
Aframe is a conceptual structure that describes a situ-ation.
It is introduced by a target or frame-evokingelement (FEE).
The roles, called frame elements(FEs), are local to particular frames and are the par-ticipants and props of the described situations.The aim of FrameNet is to provide a comprehen-sive frame-semantic description of the core lexiconof English.
A database of frames contains theframes?
basic conceptual structure, and names anddescriptions for the available frame elements.
Alexicon database associates lemmas with the framesthey evoke, lists possible syntactic realizations ofFEs and provides annotated examples from theBNC.
The current on-line version of the framedatabase (Johnson et al, 2002) consists of almost400 frames, and covers about 6,900 lexical entries.Frame: REQUESTFE ExampleSPEAKER Pat urged me to apply for the job.ADDRESSEE Pat urged me to apply for the job.MESSAGE Pat urged me to apply for the job.TOPIC Kim made a request about changing the title.MEDIUM Kim made a request in her letter.Frame: COMMERCIAL TRANSACTION (C T)BUYER Jess bought a coat.GOODS Jess bought a coat.SELLER Kim sold the sweater.MONEY Kim paid 14 dollars for the ticket.PURPOSE Kim bought peppers to cook them.REASON Bob bought peppers because he was hungry.Figure 1: Example frame descriptions.Figure 1 shows two frames.
The frame REQUESTinvolves a FE SPEAKER who voices the request,an ADDRESSEE who is asked to do something, theMESSAGE, the request that is made, the TOPIC thatthe request is about, and the MEDIUM that is used toconvey the request.
Among the FEEs for this frameare the verb ask and the noun request.
In the frameCOMMERCIAL TRANSACTION (henceforth C T), aBUYER gives MONEY to a SELLER and receivesGOODS in exchange.
This frame is evoked e.g.
bythe verb pay and the noun money.The TIGER Corpus.
We are using the TIGERCorpus (Brants et al, 2002), a manually syntacti-cally annotated German corpus, as a basis for ourannotation.
It is the largest available such cor-pus (80,000 sentences in its final release comparedto 20,000 sentences in its predecessor NEGRA)and uses a rich annotation format.
The annotationscheme is surface oriented and comparably theory-neutral.
Individual words are labelled with POSinformation.
The syntactic structures of sentencesare described by relatively flat trees providing in-formation about grammatical functions (on edge la-bels), syntactic categories (on node labels), and ar-gument structure of syntactic heads (through theuse of dependency-oriented constituent structures,which are close to the syntactic surface).
An exam-ple for a syntactic structure is given in Figure 2.3 Project overviewThe aim of the SALSA project is to construct a largesemantically annotated corpus and to provide meth-ods for its utilisation.Corpus construction.
In the first phase of theproject, we annotate the TIGER corpus in part man-Figure 2: A sentence and its syntactic structure.ually, in part semi-automatically, having tools pro-pose tags which are verified by human annotators.In the second phase, we will extend these tools forthe weakly supervised annotation of a much largercorpus, using the TIGER corpus as training data.Utilisation.
The SALSA corpus is designed tobe utilisable for many purposes, like improving sta-tistical parsers, and extending methods for informa-tion extraction and access.
The focus in the SALSAproject itself is on lexical semantics, and our firstuse of the corpus will be to extract selectional pref-erences for frame elements.The SALSA corpus will be tagged with the fol-lowing types of semantic information:FrameNet frames.
We tag all FEEs that oc-cur in the corpus with their appropriate frames, andspecify their frame elements.
Thus, our focus isdifferent from the lexicographic orientation of theFrameNet project mentioned above.
As we tag allcorpus instances of each FEE, we expect to en-counter a wider range of phenomena.
which Cur-rently, FrameNet only exists for English and is stillunder development.
We will produce a ?light ver-sion?
of a FrameNet for German as a by-productof the annotation, reusing as many as possible ofthe semantic frame descriptions from the EnglishFrameNet database.
Our first results indicate thatthe frame structure assumed for the description ofthe English lexicon can be reused for German, withminor changes and extensions.Word sense.
The additional value of word sensedisambiguation in a corpus is obvious.
However,exhaustive word sense annotation is a highly time-consuming task.
Therefore we decided for a selec-tive annotation policy, annotating only the heads offrame elements.
GermaNet, the German WordNetversion, will be used as a basis for the annotation.request         conversationSPKRFEEADDMSGFEE FEETOPICINTLC_1Figure 3: Frame annotation.Coreference.
Similarly, we will selectively anno-tate coreference.
If a lexical head of a frame elementis an anaphor, we specify the antecedent to make themeaning of the frame element accessible.4 Frame AnnotationAnnotation schema.
To give a first impression offrame annotation, we turn to the sentence in Fig.
2:(1) SPD fordert Koalition zu Gespra?ch u?ber Re-form auf.
(SPD requests that coalition talk about reform.)Fig.
3 shows the frame annotation associated with(1).
Frames are drawn as flat trees.
The root node islabelled with the frame name.
The edges are labelledwith abbreviated FE names, like SPKR for SPEAKER,plus the tag FEE for the frame-evoking element.
Theterminal nodes of the frame trees are always nodesof the syntactic tree.
Cases where a semantic unit(FE or FEE) does not form one syntactic constituent,like fordert .
.
.
auf in the example, are representedby assignment of the same label to several edges.Sentence (1), a newspaper headline, contains atleast two FEEs: auffordern and Gespra?ch.
auf-fordern belongs to the frame REQUEST (see Fig.
1).In our example the SPEAKER is the subject NP SPD,the ADDRESSEE is the direct object NP Koalition,and the MESSAGE is the complex PP zu Gespra?chu?ber Reform.
So far, the frame structure follows thesyntactic structure, except for that fact that the FEE,as a separable prefix verb, is realized by two syntac-tic nodes.
However, it is not always the case thatframe structure parallels syntactic structure.
Thesecond FEE Gespra?ch introduces the frame CON-VERSATION.
In this frame two (or more) groupstalk to one another and no participant is construedas only a SPEAKER or only an ADDRESSEE.
Inour example the only NP-internal frame element isthe TOPIC (?what the message is about?)
u?ber Re-form, whereas the INTERLOCUTOR-1 (?the promi-nent participant in the conversation?)
is realized bythe direct object of auffordern.As shown in Fig.
3, frames are annotated as treesof depth one.
Although it might seem semanticallymore adequate to admit deeper frame trees, e.g.
toallow the MSG edge of the REQUEST frame in Fig.3 to be the root node of the CONVERSATION tree,as its ?real?
semantic argument, the representationof frame structure in terms of flat and independentsemantic trees seems to be preferable for a numberof practical reasons: It makes the annotation processmore modular and flexible ?
this way, no frame an-notation relies on previous frame annotation.
Thecloseness to the syntactic structure makes the an-notators?
task easier.
Finally, it facilitates statisticalevaluation by providing small units of semantic in-formation that are locally related to syntax.Difficult cases.
Because frame elements mayspan more than one sentence, like in the case ofdirect speech, we cannot restrict ourselves to an-notation at sentence level.
Also, compound nounsrequire annotation below word level.
For ex-ample, the word ?Gagenforderung?
(demand forwages) consists of ?-forderung?
(demand), which in-vokes the frame REQUEST, and a MESSAGE element?Gagen-?.
Another interesting point is that one wordmay introduce more than one frame in cases of co-ordination and ellipsis.
An example is shown in (2).In the elliptical clause only one fifth for daughters,the elided bought introduces a C T frame.
So we letthe bought in the antecedent introduce two frames,one for the antecedent and one for the ellipsis.
(2) Ein Viertel aller Spielwaren wu?rden fu?r So?hneerworben, nur ein Fu?nftel fu?r To?chter.
(One quarter of all toys are bought for sons, only one fifthfor daughters.
)Annotation process.
Frame annotation proceedsone frame-evoking lemma at a time, using subcor-pora containing all instances of the lemma withsome surrounding context.
Since most FEEs arepolysemous, there will usually be several frames rel-evant to a subcorpus.
Annotators first select a framefor an instance of the target lemma.
Then they assignframe elements.At the moment the annotation uses XML tags onbare text.
The syntactic structure of the TIGER-sentences can be accessed in a separate viewer.
Anannotation tool is being implemented that will pro-vide a graphical interface for the annotation.
It willdisplay the syntactic structure and allow for a graph-ical manipulation of semantic frame trees, in a simi-lar way as shown in Fig.
3.Extending FrameNet.
Since FrameNet is farfrom being complete, there are many word sensesnot yet covered.
For example the verb fordern,which belongs to the REQUEST frame, additionallyhas the reading challenge, for which the current ver-sion of FrameNet does not supply a frame.5 Evaluation of Annotated DataMaterials.
Compared to the pilot study we previ-ously reported (Erk et al, 2003), in which 3 annota-tors tagged 440 corpus instances of a single frame,resulting in 1,320 annotation instances, we now dis-pose of a considerably larger body of data.
It con-sists of 703 corpus instances for the two framesshown in Figure 1, making up a total of 4,653 an-notation instances.
For the frame REQUEST, weobtained 421 instances with 8-fold and 114 with7-fold annotation.
The annotated lemmas com-prise auffordern (to request), fordern, verlangen (todemand), zuru?ckfordern (demand back), the nounForderung (demand), and compound nouns endingwith -forderung.
For the frame C T we have 30, 40and 98 instances with 5-, 3-, and 2-fold annotationrespectively.
The annotated lemmas are kaufen (tobuy), erwerben (to acquire), verbrauchen (to con-sume), and verkaufen (to sell).Note that the corpora we are evaluating do notconstitute a random sample: At the moment, wecover only two frames, and REQUEST seems to berelatively easy to annotate.
Also, the annotation re-sults may not be entirely predictive for larger sam-ple sizes: While the annotation guidelines were be-ing developed, we used REQUEST as a ?calibration?frame to be annotated by everybody.
As a result, insome cases reliability may be too low because de-tailed guidelines were not available, and in othersit may be too high because controversial instanceswere discussed in project meetings.Results.
The results in this section refer solely tothe assignment of fully specified frames and frameelements.
Underspecification is discussed at lengthframes average best worstREQUEST 96.83% 100% 90.73%COMM.
97.11% 98.96% 88.71%elements average best worstREQUEST 88.86% 95.69% 66.57%COMM.
74.25% 90.30% 69.33%Table 1: Inter-annotator agreement on frames (top)and frame elements (below).in Section 6.
Due to the limited space in this pa-per, we only address the question of inter-annotatoragreement or annotation reliability, since a reliableannotation is necessary for all further corpus uses.Table 1 shows the inter-annotator agreement onframe assignment and on frame element assignment,computed for pairs of annotators.
The ?average?column shows the total agreement for all annotationinstances, while ?best?
and ?worst?
show the fig-ures for the (lemma-specific) subcorpora with high-est and lowest agreement, respectively.
The upperhalf of the table shows agreement on the assignmentof frames to FEEs, for which we performed 14,410pairwise comparisons, and the lower half showsagreement on assigned frame elements (29,889 pair-wise comparisons).
Agreement on frame elements is?exact match?
: both annotators have to tag exactlythe same sequence of words.
In sum, we found thatannotators agreed very well on frames.
Disagree-ment on frame elements was higher, in the range of12-25%.
Generally, the numbers indicated consider-able differences between the subcorpora.To investigate this matter further, we computedthe Alpha statistic (Krippendorff, 1980) for our an-notation.
Like the widely used Kappa, ?
is a chance-corrected measure of reliability.
It is defined as?
= 1 ?
observed disagreementexpected disagreementWe chose Alpha over Kappa because it also indi-cates unreliabilities due to unequal coder preferencefor categories.
With an ?
value of 1 signifying totalagreement and 0 chance agreement, ?
values above0.8 are usually interpreted as reliable annotation.Figure 4 shows single category reliabilities forthe assignment of frame elements.
The graphsshows that not only did target lemmas vary intheir difficulty, but that reliability of frame ele-ment assignment was also a matter of high varia-tion.
Firstly, frames introduced by nouns (Forderungand -forderung) were more difficult to annotate thanverbs.
Secondly, frame elements could be assignedto three groups: frame elements which were al-ways annotated reliably, those whose reliability washighly dependent on the FEE, and the third groupwhose members were impossible to annotate reli-ably (these are not shown in the graphs).
In theREQUEST frames, SPEAKER, MESSAGE and AD-DRESSEE belong to the first group, at least for verbalFEEs.
MEDIUM is a member of the second group,and TOPIC was annotated at chance level (?
?
0).In the COMMERCE frame, only BUYER and GOODSalways show high reliability.
SELLER can only be re-liably annotated for the target verkaufen.
PURPOSEand REASON fall into the third group.5.1 DiscussionInterpretation of the data.
Inter-annotator agree-ment on the frames shown in Table 1 is very high.However, the lemmas we considered so far wereonly moderately ambiguous, and we might see lowerfigures for frame agreement for highly polysemousFEEs like laufen (to run).For frame elements, inter-annotator agreementis not that high.
Can we expect improvement?The Prague Treebank reported a disagreement ofabout 10% for manual thematic role assignment( ?Zabokrtsky?, 2000).
However, in contrast to ourstudy, they also annotated temporal and local modi-fiers, which are easier to mark than other roles.One factor that may improve frame elementagreement in the future is the display of syntacticstructure directly in the annotation tool.
Annotatorswere instructed to assign each frame element to asingle syntactic constituent whenever possible, butcould only access syntactic structure in a separateviewer.
We found that in 35% of pairwise frame ele-ment disagreements, one annotator assigned a singlesyntactic constituent and the other did not.
Since atotal of 95.6% of frame elements were assigned tosingle constituents, we expect an increase in agree-ment when a dedicated annotation tool is available.As to the pronounced differences in reliability be-tween frame elements, we found that while mostcentral frame elements like SPEAKER or BUYERwere easy to identify, annotators found it harder toagree on less frequent frame elements like MEDIUM,PURPOSE and REASON.
The latter two with their0.60.81auffordern fordern verlangen Forderung -forderungalphavalueaddresseemediummessagespeaker0.60.81erwerben kaufen verkaufenalphavaluebuyersellermoneygoodsFigure 4: Alpha values for frame elements.
Left: REQUEST.
Right: COMMERCIAL TRANSACTION.particularly low agreement (?
< 0.8) contribute to-wards the low overall inter-annotator agreement ofthe C T frame.
We suspect that annotators saw toofew instances of these elements to build up a reli-able intuition.
However, the elements may also beinherently difficult to distinguish.How can we interpret the differences in frame el-ement agreement across target lemmas, especiallybetween verb and noun targets?
While frame ele-ments for verbal targets are usually easy to identifybased on syntactic factors, this is not the case fornouns.
Figure 3 shows an example: Should SPDbe tagged as INTERLOCUTOR-2 in the CONVERSA-TION frame?
This appears to be a question of prag-matics.
Here it seems that clearer annotation guide-lines would be desirable.FrameNet as a resource for semantic role an-notation.
Above, we have asked about the suitabil-ity of FrameNet for semantic role annotation, andour data allow a first, though tentative, assessment.Concerning the portability of FrameNet to otherlanguages than English, the English frames workedwell for the German lemmas we have seen so far.For C T a number of frame elements seem to bemissing, but these are not language-specific, likeCREDIT (for on commission and in installments).The FrameNet frame database is not yet complete.How often do annotators encounter missing frames?The frame UNKNOWN was assigned in 6.3% of theinstances of REQUEST, and in 17.6% of the C T in-stances.
The last figure is due to the overwhelm-ing number of UNKNOWN cases in verbrauchen, forwhich the main sense we encountered is ?to use upa resource?, which FrameNet does not offer.Is the choice of frame always clear?
And canframe elements always be assigned unambiguously?Above we have already seen that frame element as-signment is problematic for nouns.
In the next sec-tion we will discuss problematic cases of frame as-signment as well as frame element assignment.6 Vagueness, Ambiguity andUnderspecificationAnnotation Challenges.
It is a well-known prob-lem from word sense annotation that it is often im-possible to make a safe choice among the set of pos-sible semantic correlates for a linguistic item.
Inframe annotation, this problem appears on two lev-els: The choice of a frame for a target is a choiceof word sense.
The assignment of frame elements tophrases poses a second disambiguation problem.An example of the first problem is the Ger-man verb verlangen, which associates with both theframe REQUEST and the frame C T. We found sev-eral cases where both readings seem to be equallypresent, e.g.
sentence (3).
Sentences (4) and (5) ex-emplify the second problem.
The italicised phrase in(4) may be either a SPEAKER or a MEDIUM and theone in (5) either a MEDIUM or not a frame elementat all.
In our exhaustive annotation, these problemsare much more virulent than in the FrameNet corpus,which consists mostly of prototypical examples.
(3) Gleichwohl versuchen offenbar Assekuranzen,[das Gesetz] zu umgehen, indem sie von Nicht-deutschen mehr Geld verlangen.
(Nonetheless insurance companies evidently try to cir-cumvent [the law] by asking/demanding more moneyfrom non-Germans.
)(4) Die nachhaltigste Korrektur der Programmatikfordert ein Antrag.
.
.
(The most fundamental policy correction is requested bya motion.
.
.
)(5) Der Parteitag billigte ein Wirtschaftskonzept, indem der Umbau gefordert wird.
(The party congress approved of an economic concept inwhich a change is demanded.
)Following Kilgarriff and Rosenzweig (2000), wedistinguish three cases where the assignment of asingle semantic tag is problematic: (1), cases inwhich, judging from the available context informa-tion, several tags are equally possible for an ambigu-ous utterance; (2), cases in which more than one tagapplies at the same time, because the sense distinc-tion is neutralised in the context; and (3), cases inwhich the distinction between two tags is systemati-cally vague or unclear.In SALSA, we use the concept of underspecifica-tion to handle all three cases: Annotators may assignunderspecified frame and frame element tags.
Whilethe cases have different semantic-pragmatic status,we tag all three of them as underspecified.
This is inaccordance with the general view on underspecifica-tion in semantic theory (Pinkal, 1996).
Furthermore,Kilgarriff and Rosenzweig (2000) argue that it is im-possible to distinguish those casesAllowing underspecified tags has several advan-tages.
First, it avoids (sometimes dubious) decisionsfor a unique tag during annotation.
Second, it is use-ful to know if annotators systematically found it hardto distinguish between two frames or two frame ele-ments.
This diagnostic information can be used forimproving the annotation scheme (e.g.
by removingvague distinctions).
Third, underspecified tags mayindicate frame relations beyond an inheritance hier-archy, horizontal rather than vertical connections.
In(3), the use of underspecification can indicate thatthe frames REQUEST and C T are used in the samesituation, which in turn can serve to infer relationsbetween their respective frame elements.Evaluating underspecified annotation.
In theprevious section, we disregarded annotation casesinvolving underspecification.
In order to evalu-ate underspecified tags, we present a method ofcomputing inter-annotator agreement in the pres-ence of underspecified annotations.
Represent-ing frames and frame elements as predicates thateach take a sequence of word indices as theirargument, a frame annotation can be seen as apair (CF,CE) of two formulae, describing theframe and the frame elements, respectively.
With-out underspecification, CF is a single predicateand CE is a conjunction of predicates.
For theCONVERSATION frame of sentence (1), CF hasthe form CONVERSATION(Gespra?ch)1 , and CE isINTLC 1(Koalition) ?
TOPIC(u?ber Reform).
Un-derspecification is expressed by conjuncts that aredisjunctions instead of single predicates.
Table 2shows the admissible cases.
For example, the CEof (4) contains the conjunct SPKR(ein Antrag) ?MEDIUM(ein Antrag).
Our annotation scheme guar-antees that every FE name appears in at most oneconjunct of CE.
Exact agreement means that ev-ery conjunct of annotator A must correspond to aconjunct by annotator B, and vice versa.
For partialagreement, it suffices that for each conjunct of A,one disjunct matches a disjunct in a conjunct of B,and conversely.frame annotationF(t) single frame: F is assigned to t(F1(t)?F2(t)) frame disjunction: F1 or F2 isassigned to tframe element annotationE(s) single frame element: E is as-signed to s(E1(s)?E2(s)) frame element disjunction: E1or E2 is assigned to s(E(s)?NOFE(s)) optional element: E1 or noframe element is assigned to s(E(s)?E(s1ss2)) underspecified length: frameelement E is assigned to sor the longer sequence s1ss2,which includes sTable 2: Types of conjuncts.
F is a frame name, Ea frame element name, and t and s are sequences ofword indices (t is for the target (FEE))Using this measure of partial agreement, we nowevaluate underspecified annotation.
The most strik-ing result is that annotators made little use of under-specification.
Frame underspecification was used in0.4% of all frames, and frame element underspecifi-cation for 0.9% of all frame elements.
The frame el-ement MEDIUM, which was rarely assigned outside1We use words instead of indices for readability.underspecification, accounted for roughly half of allunderspecification in the REQUEST frame.
63% ofthe frame element underspecifications are cases ofoptional elements, the third class in the lower half ofTable 2.
(Partial) agreement on underspecified tagswas considerably lower than on non-underspecifiedtags, both in the case of frames (86%) and in thecase of frame elements (54%).
This was to be ex-pected, since the cases with underspecified tags arethe more difficult and controversial ones.
Since un-derspecified annotation is so rare, overall frame andframe element agreement including underspecifiedannotation is virtually the same as in Table 1.It is unfortunate that annotators use underspecifi-cation only infrequently, since it can indicate inter-esting cases of relatedness between different framesand frame elements.
However, underspecificationmay well find its main use during the merging ofindependent annotations of the same corpus.
Notonly underspecified annotation, also disagreementbetween annotators can point out vague and ambigu-ous cases.
If, for example, one annotator has as-signed SPEAKER and the other MEDIUM in sentence(4), the best course is probably to use an underspec-ified tag in the merged corpus.7 ConclusionWe presented the SALSA project, the aim of whichis to construct and utilize a large corpus reliablyannotated with semantic information.
While theSALSA corpus is designed to be utilizable for manypurposes, our focus is on lexical semantics, in or-der to address one of the most serious bottlenecksfor language technology today: the lack of large,domain-independent lexica.In this paper we have focused on the annotationwith frame semantic roles.
We have presented theannotation scheme, and we have evaluated first an-notation results, which show encouraging figures forinter-annotator agreement.
We have discussed theproblem of vagueness and ambiguity of the data andproposed a representation for underspecified tags,which are to be used both for the annotation and themerging of individual annotations.Important next steps are: the design of a tool forsemi-automatic annotation, and the extraction of se-lectional preferences from the annotated data.Acknowledgments.
We would like to thank thefollowing people, who helped us with their sugges-tions and discussions: Sue Atkins, Collin Baker,Ulrike Baldewein, Hans Boas, Daniel Bobbert,Sabine Brants, Paul Buitelaar, Ann Copestake,Christiane Fellbaum, Charles Fillmore, Gerd Flied-ner, Silvia Hansen, Ulrich Heid, Katja Markert andOliver Plaehn.
We are especially indebted to MariaLapata, whose suggestions have contributed to thecurrent shape of the project in an essential way.
Anyerrors are, of course, entirely our own.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceedings ofCOLING-ACL, Montreal, Canada.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolfgang Lez-ius, and George Smith.
2002.
The TIGER treebank.
InProceedings of the Workshop on Treebanks and LinguisticTheories, Sozopol, Bulgaria.Katrin Erk, Andrea Kowalski, and Manfred Pinkal.
2003.
Acorpus resource for lexical semantics.
In Proceedings ofIWCS5, pages 106?121, Tilburg, The Netherlands.Charles J. Fillmore.
1968.
The case for case.
In Bach andHarms, editors, Universals in Linguistic Theory, pages 1?88.Holt, Rinehart, and Winston, New York.Eva Hajic?ov a?.
1998.
Prague Dependency Treebank: From An-alytic to Tectogrammatical Annotation.
In Proceedings ofTSD?98, pages 45?50, Brno, Czech Republic.C.
R. Johnson, C. J. Fillmore, M. R. L. Petruck, C. F. Baker,M.
Ellsworth, J. Ruppenhofer, and E. J.
Wood.
2002.FrameNet: Theory and Practice.
http://www.icsi.berkeley.edu/?framenet/book/book.html.Adam Kilgarriff and Joseph Rosenzweig.
2000.
Frameworkand results for English Senseval.
Computers and the Hu-manities, 34(1-2).Adam Kilgarriff, editor.
2001.
SENSEVAL-2, Toulouse.Klaus Krippendorff.
1980.
Content Analysis.
Sage.M.
Marcus, G. Kim, M.A.
Marcinkiewicz, R. MacIntyre,A.
Bies, M. Gerguson, K. Katz, and B. Schasberger.
1994.The Penn Treebank: Annotating predicate argument struc-ture.
In Proceedings of the ARPA HLT Workshop.G.
Miller, R. Beckwith, C. Fellbaum, D. Gros, and K. Miller.1990.
Introduction to WordNet: An on-line lexical database.International Journal of Lexicography, 3(4):235?44.Manfred Pinkal.
1996.
Vagueness, ambiguity, and underspeci-fication.
In Proceedings of SALT?96, pages 185?201.Wojciech Skut, Brigitte Krenn, Thorsten Brants, and HansUszkoreit.
1998.
A linguistically interpreted corpus of Ger-man newspaper text.
In Proceedings of LREC?98, Granada.Zdene?k ?Zabokrtsk y?.
2000.
Automatic functor assignmentin the Prague Dependency Treebank.
In Proceedings ofTSD?00, Brno, Czech Republic.
