Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 777?784,Sydney, July 2006. c?2006 Association for Computational LinguisticsLeft-to-Right Target Generation for Hierarchical Phrase-basedTranslationTaro Watanabe Hajime Tsukada Hideki Isozaki2-4, Hikaridai, Seika-cho, Soraku-gun,Kyoto, JAPAN 619-0237{taro,tsukada,isozaki}@cslab.kecl.ntt.co.jpAbstractWe present a hierarchical phrase-basedstatistical machine translation in which atarget sentence is efficiently generated inleft-to-right order.
The model is a classof synchronous-CFG with a Greibach Nor-mal Form-like structure for the projectedproduction rule: The paired target-sideof a production rule takes a phrase pre-fixed form.
The decoder for the target-normalized form is based on an Early-style top down parser on the source side.The target-normalized form coupled withour top down parser implies a left-to-right generation of translations which en-ables us a straightforward integration withngram language models.
Our model wasexperimented on a Japanese-to-Englishnewswire translation task, and showed sta-tistically significant performance improve-ments against a phrase-based translationsystem.1 IntroductionIn a classical statistical machine translation, a for-eign language sentence f J1 = f1, f2, ... fJ is trans-lated into another language, i.e.
English, eI1 =e1, e2, ..., eI by seeking a maximum likely solutionof:e?I1 = argmaxeI1Pr(eI1| f J1 ) (1)= argmaxeI1Pr( f J1 |eI1)Pr(eI1) (2)The source channel approach in Equation 2 inde-pendently decomposes translation knowledge intoa translation model and a language model, respec-tively (Brown et al, 1993).
The former repre-sents the correspondence between two languagesand the latter contributes to the fluency of English.In the state of the art statistical machine transla-tion, the posterior probability Pr(eI1| f J1 ) is directlymaximized using a log-linear combination of fea-ture functions (Och and Ney, 2002):e?I1 = argmaxeI1exp(?Mm=1 ?mhm(eI1, f J1 ))?e?
I?1exp(?Mm=1 ?mhm(e?
I?1 , f J1 )) (3)where hm(eI1, f J1 ) is a feature function, such asa ngram language model or a translation model.When decoding, the denominator is dropped sinceit depends only on f J1 .
Feature function scalingfactors ?m are optimized based on a maximumlikely approach (Och and Ney, 2002) or on a directerror minimization approach (Och, 2003).
Thismodeling allows the integration of various fea-ture functions depending on the scenario of howa translation is constituted.A phrase-based translation model is one of themodern approaches which exploits a phrase, acontiguous sequence of words, as a unit of transla-tion (Koehn et al, 2003; Zens and Ney, 2003; Till-man, 2004).
The idea is based on a word-basedsource channel modeling of Brown et al (1993):It assumes that eI1 is segmented into a sequenceof K phrases e?K1 .
Each phrase e?k is transformedinto ?fk.
The translated phrases are reordered toform f J1 .
One of the benefits of the modeling isthat the phrase translation unit preserves localizedword reordering.
However, it cannot hypothesizea long-distance reordering required for linguisti-cally divergent language pairs.
For instance, whentranslating Japanese to English, a Japanese SOVstructure has to be reordered to match with an En-777glish SVO structure.
Such a sentence-wise move-ment cannot be realized within the phrase-basedmodeling.Chiang (2005) introduced a hierarchical phrase-based translation model that combined thestrength of the phrase-based approach and asynchronous-CFG formalism (Aho and Ullman,1969): A rewrite system initiated from a startsymbol which synchronously rewrites paired non-terminals.
Their translation model is a binarizedsynchronous-CFG, or a rank-2 of synchronous-CFG, in which the right-hand side of a productionrule contains at most two non-terminals.
The formcan be regarded as a phrase translation pair withat most two holes instantiated with other phrases.The hierarchically combined phrases provide asort of reordering constraints that is not directlymodeled by a phrase-based model.Rules are induced from a bilingual corpus with-out linguistic clues first by extracting phrase trans-lation pairs, and then by generalizing extractedphrases with holes (Chiang, 2005).
Even in aphrase-based model, the number of phrases ex-tracted from a bilingual corpus is quadratic tothe length of bilingual sentences.
The grammarsize for the hierarchical phrase-based model willbe further exploded, since there exists numerouscombination of inserting holes to each rule.
Thespuriously increasing grammar size will be prob-lematic for decoding without certain heuristics,such as a length based thresholding.The integration with a ngram language modelfurther increases the cost of decoding especiallywhen incorporating a higher order ngram, such as5-gram.
In the hierarchical phrase-based model(Chiang, 2005), and an inversion transductiongrammar (ITG) (Wu, 1997), the problem is re-solved by restricting to a binarized form where atmost two non-terminals are allowed in the right-hand side.
However, Huang et al (2005) reportedthat the computational complexity for decodingamounted to O(J3+3(n?1)) with n-gram even usinga hook technique.
The complexity lies in mem-orizing the ngram?s context for each constituent.The order of ngram would be a dominant factorfor higher order ngrams.As an alternative to a binarized form, wepresent a target-normalized hierarchical phrase-based translation model.
The model is a class of ahierarchical phrase-based model, but constrainedso that the English part of the right-hand sideis restricted to a Greibach Normal Form (GNF)-like structure: A contiguous sequence of termi-nals, or a phrase, is followed by a string of non-terminals.
The target-normalized form reduces thenumber of rules extracted from a bilingual corpus,but still preserves the strength of the phrase-basedapproach.
An integration with ngram languagemodel is straightforward, since the model gener-ates a translation in left-to-right order.
Our de-coder is based on an Earley-style top down pars-ing on the foreign language side.
The projectedEnglish-side is generated in left-to-right order syn-chronized with the derivation of the foreign lan-guage side.
The decoder?s implementation is takenafter a decoder for an existing phrase-based modelwith a simple modification to account for produc-tion rules.
Experimental results on a Japanese-to-English newswire translation task showed signif-icant improvement against a phrase-based model-ing.2 Translation ModelA weighted synchronous-CFG is a rewrite systemconsisting of production rules whose right-handside is paired (Aho and Ullman, 1969):X ?
?
?, ?,??
(4)where X is a non-terminal, ?
and ?
are strings ofterminals and non-terminals.
For notational sim-plicity, we assume that ?
and ?
correspond to theforeign language side and the English side, re-spectively.
?
is a one-to-one correspondence forthe non-terminals appeared in ?
and ?.
Startingfrom an initial non-terminal, each rule rewritesnon-terminals in ?
and ?
that are associated with?.Chiang (2005) proposed a hierarchical phrase-based translation model, a binary synchronous-CFG, which restricted the form of production rulesas follows:?
Only two types of non-terminals allowed: Sand X.?
Both of the strings ?
and ?
must contain atleast one terminal item.?
Rules may have at most two non-terminalsbut non-terminals cannot be adjacent for theforeign language side ?.The production rules are induced from a bilingualcorpus with the help of word alignments.
To al-leviate a data sparseness problem, glue rules are778added that prefer combining hierarchical phrasesin a serial manner:S ?
?S 1 X2 , S 1 X2?
(5)S ?
?X 1 , X1?
(6)where boxed indices indicate non-terminal?s link-ages represented in ?.Our model is based on Chiang (2005)?s frame-work, but further restricts the form of productionrules so that the aligned right-hand side ?
followsa GNF-like structure:X ??
?, ?b?,??
(7)where ?b is a string of terminals, or a phrase,and beta is a (possibly empty) string of non-terminals.
The foreign language at right-hand side?
still takes an arbitrary string of terminals andnon-terminals.
The use of a phrase ?b as a pre-fix keeps the strength of the phrase-base frame-work.
A contiguous English side coupled witha (possibly) discontiguous foreign language sidepreserves a phrase-bounded local word reordering.At the same time, the target-normalized frame-work still combines phrases hierarchically in a re-stricted manner.The target-normalized form can be regarded asa type of rule in which certain non-terminals arealways instantiated with phrase translation pairs.Thus, we will be able to reduce the number of rulesinduced from a bilingual corpus, which, in turn,help reducing the decoding complexity.The contiguous phrase-prefixed form generatesEnglish in left-to-right order.
Therefore, a decodercan easily hypothesize a derivation tree integratedwith a ngram language model even with higher or-der.Note that we do not imply arbitrarysynchronous-CFGs are transformed into thetarget normalized form.
The form simply restrictsthe grammar extracted from a bilingual corpusexplained in the next section.2.1 Rule ExtractionWe present an algorithm to extract productionrules from a bilingual corpus.
The procedure isbased on those for the hierarchical phrase-basedtranslation model (Chiang, 2005).First, a bilingual corpus is annotated with wordalignments using the method of Koehn et al(2003).
Many-to-many word alignments are in-duced by running a one-to-many word alignmentmodel, such as GIZA++ (Och and Ney, 2003), inboth directions and by combining the results basedon a heuristic (Koehn et al, 2003).Second, phrase translation pairs are extractedfrom the word alignment corpus (Koehn et al,2003).
The method exhaustively extracts phrasepairs ( f j+mj , ei+ni ) from a sentence pair ( f J1 , eI1) thatdo not violate the word alignment constraints a:?
(i?, j?)
?
a : j?
?
[ j, j + m], i?
?
[i, i + n]?
(i?, j?)
?
a : j?
?
[ j, j + m], i?
< [i, i + n]?
(i?, j?)
?
a : j?
< [ j, j + m], i?
?
[i, i + n]Third, based on the extracted phrases, productionrules are accumulated by computing the ?holes?for contiguous phrases (Chiang, 2005):1.
A phrase pair ( ?f , e?)
constitutes a ruleX ??
?f , e??2.
A rule X ?
?
?, ??
and a phrase pair ( ?f , e?)
s.t.?
= ??
?f???
and ?
= e??e??
constitutes a ruleX ????
X k ??
?, e??
X k ?
?Following Chiang (2005), we applied constraintswhen inducing rules with non-terminals:?
At least one foreign word must be aligned toan English word.?
Adjacent non-terminals are not allowed forthe foreign language side.2.2 Phrase-based RulesThe rule extraction procedure described in Section2.1 is a corpus-based, therefore will be easily suf-fered from a data sparseness problem.
The hier-archical phrase-based model avoided this problemby introducing the glue rules 5 and 6 that com-bined hierarchical phrases sequentially (Chiang,2005).We use a different method of generalizing pro-duction rules.
When production rules without non-terminals are extracted in step 1 of Section 2.1,X ??
?f , e??
(8)then, we also add production rules as follows:X ??
?f X 1 , e?
X 1?
(9)X ?
?X 1 ?f , e?
X 1?
(10)X ?
?X 1 ?f X 2 , e?
X 1 X 2?
(11)X ?
?X 2 ?f X 1 , e?
X 1 X 2?
(12)779The international terrorism also is a possible threat in JapanReference translation: ?International terrorism is a threateven to Japan?
(a) Translation by a phrase-based model.
(b) A derivation tree representation for Figure 1(a).Indices innon-terminal X represent the order to perform rewriting.Figure 1: An example of Japanese-to-English translation by a phrase-based model.We call them phrase-based rules, since four typesof rules are generalized directly from phrase trans-lation pairs.The class of rules roughly corresponds to the re-ordering constraints used in a phrase-based modelduring decoding.
Rules 8 and 9 are sufficient to re-alize a monotone decoding in which phrase trans-lation pairs are simply combined sequentially.With rules 10 and 11, the non-terminal X 1 behavesas a place holder where certain number of foreignwords are skipped.
Therefore, those rules real-ize a window size constraint used in many phrase-based models (Koehn et al, 2003).
The rule 12further gives an extra freedom for the phrase pairreordering.
The rules 8 through 12 can be in-terpreted as ITG-constraints where phrase trans-lation pairs are hierarchically combined either ina monotonic way or in an inverted manner (Zensand Ney, 2003; Wu, 1997).
Thus, by controllingwhat types of phrase-based rules employed in agrammar, we will be able to simulate a phrase-based translation model with various constraints.This reduction is rather natural in that a finite statetransducer, or a phrase-based model, is a subclassof a synchronous-CFG.Figure 1(a) shows an example Japanese-to-English translation by a phrase-based model de-scribed in Section 5.
Using the phrase-based rules,the translation results is represented as a derivationtree in Figure 1(b).3 DecodingOur decoder is an Earley-style top down parser onthe foreign language side with a beam search strat-egy.
Given an input sentence f J1 , the decoder seeksfor the best English according to Equation 3 us-ing the feature functions described in Section 4.The English output sentence is generated in left-to-right order in accordance with the derivation ofthe foreign language side synchronized with thecardinality of already translated foreign word po-sitions.The decoding process is very similar to thosedescribed in (Koehn et al, 2003): It starts from aninitial empty hypothesis.
From an existing hypoth-esis, new hypothesis is generated by consuminga production rule that covers untranslated foreignword positions.
The score for the newly generatedhypothesis is updated by combining the scores offeature functions described in Section 4.
The En-glish side of the rule is simply concatenated toform a new prefix of English sentence.
Hypothe-ses that consumed m foreign words are stored in apriority queue Qm.Hypotheses in Qm undergo two types of prun-ing: A histogram pruning preserves at most M hy-potheses inQm.
A threshold pruning discards a hy-potheses whose score is below the maximum scoreof Qm multiplied with a threshold value ?.
Rulesare constrained by their foreign word span of anon-terminal.
For a rule consisting of more thantwo non-terminals, we constrained so that at leastone non-terminal should span at most ?
words.The decoder is characterized as a weightedsynchronous-CFG implemented with a push-downautomaton rather a weighted finite state transducer(Aho and Ullman, 1969).
Each hypothesis main-tains following knowledge:?
A prefix of English sentence.
For space ef-ficiency, the prefix is represented as a wordgraph.?
Partial contexts for each feature function.For instance, to compute a 5-gram languagemodel feature, we keep the consecutive lastfour words of an English prefix.780?
A stack that keeps track of the uncovered for-eign word spans.
The stack for an initial hy-pothesis is initialized with span [1, J].When extending a hypothesis, the associated stackstructure is popped.
The popped foreign wordspan [ jl, jr] is used to locate the rules for uncov-ered foreign word positions.
We assume that thedecoder accumulates all the applicable rules froma large database and stores the extracted rules in achart structure.
The decoder identifies what rulesto consume when extending a hypothesis using thechart structure.
A new hypothesis is created withan updated stack by pushing foreign non-terminalspans: For each rule spanning [ jl, jr] at foreign-side with non-terminal spans of [kl1, kr1], [kl2, kr2], ...,the non-terminal spans are pushed in the reverseorder of the projected English side.
For example,A rule with foreign word non-terminal spans:X ?
?X 2 : [kl2, kr2] ?f X 1 : [kl1, kr1], e?
X 1 X 2?will update a stack by pushing the foreign wordspans [kl2, kr2] and [kl1, kr1] in order.
This orderingassures that, when popped, the English-side willbe generated in left-to-right order.
A hypothesiswith an empty stack implies that the hypothesishas covered all the foreign words.Figure 2 illustrates the decoding process for thederivation tree in Figure 1(b).
Starting from theinitial hypothesis of [1, 11], the stack is updated inaccordance with non-terminal?s spans.
The spanis popped and the rule with the foreign word pan[1, 11] is looked up from the chart structure.
Thestack structure for the newly created hypothesis isupdated by pushing non-terminal spans [4, 11] and[1, 2].Our decoder is based on an in-house devel-oped phrase-based decoder which uses a bit vec-tor to represent uncovered foreign word positionsfor each hypothesis.
We basically replaced thebit vector structure to the stack structure: Al-most no modification was required for the wordgraph structure and the beam search strategy im-plemented for a phrase-based modeling.
The useof a stack structure directly models a synchronous-CFG formalism realized as a push-down automa-tion, while the bit vector implementation is con-ceptualized as a finite state transducer.
The costof decoding with the proposed model is cubic toforeign language sentence length.Rules Stack[1, 11]X : [1, 11]?
?X 1 : [1, 2] X 2 : [4, 11], The X 1 X 2?
[1, 2][4, 11]X : [1, 2]?
?X 1 : [2, 2], international X 1?
[2, 2][4, 11]X : [2, 2]?
?
, terrorism?
[4, 11]X : [4, 11]?
?X 2 : [4, 5] X 1 : [7, 11], also X 1 X 2?
[7, 11][4, 5]X : [7, 11]?
?X 1 : [7, 9] , is a X 1?
[7, 9][4, 5]X : [7, 9]?
?X 1 : [9, 9], possible X 1?
[9, 9][4, 5]X : [9, 9]?
?
, threat?
[4, 5]X : [4, 5]?
?X 1 : [4, 4] , in X 1?
[4, 4]X : [4, 4]?
?
, Japan?Figure 2: An example decoding process of Fig-ure 1(b) with a stack to keep track of foreign wordspans.4 Feature FunctionsThe decoder for our translation model uses a log-linear combination of feature functions, or sub-models, to seek for the maximum likely translationaccording to Equation 3.
This section describesthe models experimented in Section 5, mainlyconsisting of count-based models, lexicon-basedmodels, a language model, reordering models andlength-based models.4.1 Count-based ModelsMain feature functions h?
( f J1 |eI1,D) andh?
(eI1| f J1 ,D) estimate the likelihood of twosentences f J1 and eI1 over a derivation tree D.We assume that the production rules in D areindependent of each other:h?
( f J1 |eI1,D) = log???,???D?(?|?)
(13)?(?|?)
is estimated through the relative frequencyon a given bilingual corpus.?(?|?)
= count(?, ?)??
count(?, ?
)(14)where count(?)
represents the cooccurrence fre-quency of rules ?
and ?.The relative count-based probabilities for thephrase-based rules are simply adopted from theoriginal probabilities of phrase translation pairs.4.2 Lexicon-based ModelsWe define lexically weighted feature functionshw( f J1 |eI1,D) and hw(eI1| f J1 ,D) applying the inde-pendence assumption of production rules as in781Equation 13.hw( f J1 |eI1,D) = log???,???Dpw(?|?)
(15)The lexical weight pw(?|?)
is computed from wordalignments a inside ?
and ?
(Koehn et al, 2003):pw(?|?, a) =|?|?i=11|{ j|(i, j) ?
a}|??
(i, j)?at(?
j|?i)(16)where t(?)
is a lexicon model trained from the wordalignment annotated bilingual corpus discussed inSection 2.1.
The alignment a also includes non-terminal correspondence with t(X k |X k ) = 1.
If weobserved multiple alignment instances for ?
and ?,then, we take the maximum of the weights.pw(?|?)
= maxapw(?|?, a) (17)4.3 Language ModelWe used mixed-cased n-gram language model.
Incase of 5-gram language model, the feature func-tion is expressed as follows:hlm(eI1) = log?ipn(ei|ei?4ei?3ei?2ei?1) (18)4.4 Reordering ModelsIn order to limit the reorderings, two feature func-tions are employed based on the backtracking ofrules during the top-down parsing on foreign lan-guage side.hh(eI1, f J1 ,D) =?Di?back(D)height(Di) (19)hw(eI1, f J1 ,D) =?Di?back(D)width(Di) (20)where back(D) is a set of subtrees backtrackedduring the derivation of D, and height(Di) andwidth(Di) refer the height and width of subtreeDi,respectively.
In Figure 1(b), for instance, a rule ofX 1 with non-terminals X 2 and X 4 , two rules X 2and X 3 spanning two terminal symbols should bebacktracked to proceed to X 4 .
The rationale is thatpositive scaling factors prefer a deeper structurewhereby negative scaling factors prefer a mono-tonized structure.4.5 Length-based ModelsThree trivial length-based feature functions wereused in our experiment.hl(eI1) = I (21)hr(D) = rule(D) (22)hp(D) = phrase(D) (23)Table 1: Japanese/English news corpusJapanese Englishtrain sentence 175,384dictionary + 1,329,519words 8,373,478 7,222,726vocabulary 297,646 397,592dev.
sentence 1,500words 47,081 39,117OOV 45 149test sentence 1,500words 47,033 38,707OOV 51 127Table 2: Phrases/rules extracted from theJapanese/English bilingual corpus.
Figures do notinclude phrase-based rules.# rules/phrasesPhrase 5,433,091Normalized-2 6,225,630Normalized-3 6,233,294Hierarchical 12,824,387where rule(D) and phrase(D) are the numberof production rules extracted in Section 2.1 andphrase-based rules generalized in Section 2.2, re-spectively.
The English length feature functioncontrols the length of output sentence.
Two featurefunctions based on rule?s counts are hypothesizedto control whether to incorporate a production ruleor a phrase-based rule into D.5 ExperimentsThe bilingual corpus used for our experiments wasobtained from an automatically sentence alignedJapanese/English Yomiuri newspaper corpus con-sisting of 180K sentence pairs (refer to Table1) (Utiyama and Isahara, 2003).
From one-to-one aligned sentences, 1,500 sentence pairs weresampled for a development set and a test set1.Since the bilingual corpus is rather small, es-pecially for the newspaper translation domain,Japanese/English dictionaries consisting of 1.3Mentries were added into a training set to alleviatean OOV problem2.Word alignments were annotated by a HMMtranslation model (Och and Ney, 2003).
After1Japanese sentences were segmented by MeCab availablefrom http://mecab.sourceforge.jp.2The dictionary entries were compiled from JE-DICT/JNAMEDICT and an in-house developed dictionary.782the annotation via Viterbi alignments with refine-ments, phrases translation pairs and productionrules were extracted (refer to Table 2).
We per-formed the rule extraction using the hierarchi-cal phrase-based constraint (Hierarchical) and ourproposed target-normalized form with 2 and 3non-terminals (Normalized-2 and Normalized-3).Phrase translation pairs were also extracted forcomparison (Phrase).
We did not threshold theextracted phrases or rules by their length.
Ta-ble 2 shows that Normalized-2 extracted slightlylarger number of rules than those for phrase-based model.
Including three non-terminals didnot increase the grammar size.
The hierarchicalphrase-based translation model extracts twice aslarge as our target-normalized formalism.
Thetarget-normalized form is restrictive in that non-terminals should be consecutive for the English-side.
This property prohibits spuriously extractedproduction rules.Mixed-casing 3-gram/5-gram language modelswere estimated from LDC English GigaWord 2 to-gether with the 100K English articles of Yomiurinewspaper that were used neither for developmentnor test sets 3.We run the decoder for the target-normalizedhierarchical phrase-based model consisting of atmost two non-terminals, since adding rules withthree non-terminals did not increase the grammarsize.
ITG-constraint simulated phrase-based ruleswere also included into our grammar.
The foreignword span size was thresholded so that at least onenon-terminal should span at most 7 words.Our phrase-based model employed all featurefunctions for the hierarchical phrase-based systemwith additional feature functions:?
A distortion model that penalizes the re-ordering of phrases by the number of wordsskipped | j ?
( j?
+ m?)
?
1|, where j is the for-eign word position for a phrase f j+mj trans-lated immediately after a phrase for f j?+m?j?
(Koehn et al, 2003).?
Lexicalized reordering models constrain thereordering of phrases whether to favor mono-tone, swap or discontinuous positions (Till-man, 2004).The phrase-based decoder?s reordering was con-strained by ITG-constraints with a window size of3We used SRI ngram language modeling toolkit with lim-ited vocabulary size.Table 3: Results for the Japanese-to-Englishnewswire translation task.BLEU NIST[%]Phrase 3-gram 7.14 3.215-gram 7.33 3.19Normalized-2 3-gram 10.00 4.115-gram 10.26 4.207.The translation results are summarized in Table3.
Two systems were contrasted by 3-gram and 5-gram language models.
Results were evaluated byngram precision based metrics, BLEU and NIST,on the casing preserved single reference test set.Feature function scaling factors for each systemwere optimized on BLEU score under the devel-opment set using a downhill simplex method.
Thedifferences of translation qualities are statisticallysignificant at the 95% confidence level (Koehn,2004).
Although the figures presented in Table3 are rather low, we found that Normalized-2 re-sulted in statistically significant improvement overPhrase.
Figure 3 shows some translation resultsfrom the test set.6 ConclusionThe target-normalized hierarchical phrase-basedmodel is based on a more general hierarchicalphrase-based model (Chiang, 2005).
The hier-archically combined phrases can be regarded asan instance of phrase-based model with a placeholder to constraint reordering.
Such reorder-ing was realized either by an additional constraintfor decoding, such as window constraints, IBMconstraints or ITG-constraints (Zens and Ney,2003), or by lexicalized reordering feature func-tions (Tillman, 2004).
In the hierarchical phrase-based model, such reordering is explicitly repre-sented in each rule.As experimented in Section 5, the use of thetarget-normalized form reduced the grammar size,but still outperformed a phrase-based system.Furthermore, the target-normalized form coupledwith our top down parsing on the foreign lan-guage side allows an easier integration with ngramlanguage model.
A decoder can be implementedbased on a phrase-based model by employing astack structure to keep track of untranslated for-eign word spans.The target-normalized form can be interpreted783Reference: Japan needs to learn a lesson from history to ensure that it not repeat its mistakes .Phrase: At the same time , it never mistakes that it is necessary to learn lessons from the history of criminal .Normalized-2: It is necessary to learn lessons from history so as not to repeat similar mistakes in the future .Reference: The ministries will dispatch design and construction experts to China to train local engineers and toresearch technology that is appropriate to China?s economic situation .Phrase: Japan sent specialists to train local technicians to the project , in addition to the situation in China andits design methods by exception of study .Normalized-2: Japan will send experts to study the situation in China , and train Chinese engineers , constructiondesign and construction methods of the recipient from .Reference: The Health and Welfare Ministry has decided to invoke the Disaster Relief Law in extending reliefmeasures to the village and the city of Niigata .Phrase: The Health and Welfare Ministry in that the Japanese people in the village are made law .Normalized-2: The Health and Welfare Ministry decided to apply the Disaster Relief Law to the village in Niigata .Figure 3: Sample translations from two systems: Phrase and Normalized-2as a set of rules that reorders the foreign lan-guage to match with English language sequen-tially.
Collins et al (2005) presented a methodwith hand-coded rules.
Our method directly learnssuch serialization rules from a bilingual corpuswithout linguistic clues.The translation quality presented in Section 5are rather low due to the limited size of the bilin-gual corpus, and also because of the linguistic dif-ference of two languages.
As our future work,we are in the process of experimenting our modelfor other languages with rich resources, such asChinese and Arabic, as well as similar languagepairs, such as French and English.
Additionalfeature functions will be also investigated thatwere proved successful for phrase-based modelstogether with feature functions useful for a tree-based modeling.AcknowledgementWe would like to thank to our colleagues, espe-cially to Hideto Kazawa and Jun Suzuki, for usefuldiscussions on the hierarchical phrase-based trans-lation.ReferencesAlfred V. Aho and Jeffrey D. Ullman.
1969.
Syntaxdirected translations and the pushdown assembler.
J.Comput.
Syst.
Sci., 3(1):37?56.Peter F. Brown, Stephen A. Della Pietra, VincentJ.
Della Pietra, and Robert L. Mercer.
1993.The mathematics of statistical machine translation:Parameter estimation.
Computational Linguistics,19(2):263?311.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Proc.of ACL 2005, pages 263?270, Ann Arbor, Michigan,June.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of ACL 2005, pages 531?540,Ann Arbor, Michigan, June.Liang Huang, Hao Zhang, and Daniel Gildea.
2005.Machine translation as lexicalized parsing withhooks.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology, pages 65?73,Vancouver, British Columbia, October.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL 2003, pages 48?54, Edmonton, Canada.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
of EMNLP2004, pages 388?395, Barcelona, Spain, July.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proc.
of ACL 2002,pages 295?302.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proc.
of ACL2003, pages 160?167.Christoph Tillman.
2004.
A unigram orienta-tion model for statistical machine translation.
InHLT-NAACL 2004: Short Papers, pages 101?104,Boston, Massachusetts, USA, May 2 - May 7.Masao Utiyama and Hitoshi Isahara.
2003.
Reliablemeasures for aligning Japanese-English news arti-cles and sentences.
In Proc.
of ACL 2003, pages72?79.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Comput.
Linguist., 23(3):377?403.Richard Zens and Hermann Ney.
2003.
A comparativestudy on reordering constraints in statistical machinetranslation.
In Proc.
of ACL 2003, pages 144?151.784
