An Empirical Study of Information Synthesis TasksEnrique Amigo?
Julio Gonzalo V?
?ctor Peinado Anselmo Pen?as Felisa VerdejoDepartamento de Lenguajes y Sistemas Informa?ticosUniversidad Nacional de Educacio?n a Distanciac/Juan del Rosal, 16 - 28040 Madrid - Spain{enrique,julio,victor,anselmo,felisa}@lsi.uned.esAbstractThis paper describes an empirical study of the ?In-formation Synthesis?
task, defined as the process of(given a complex information need) extracting, or-ganizing and inter-relating the pieces of informationcontained in a set of relevant documents, in order toobtain a comprehensive, non redundant report thatsatisfies the information need.Two main results are presented: a) the creationof an Information Synthesis testbed with 72 reportsmanually generated by nine subjects for eight com-plex topics with 100 relevant documents each; andb) an empirical comparison of similarity metrics be-tween reports, under the hypothesis that the bestmetric is the one that best distinguishes betweenmanual and automatically generated reports.
A met-ric based on key concepts overlap gives better re-sults than metrics based on n-gram overlap (such asROUGE) or sentence overlap.1 IntroductionA classical Information Retrieval (IR) system helpsthe user finding relevant documents in a given textcollection.
In most occasions, however, this is onlythe first step towards fulfilling an information need.The next steps consist of extracting, organizing andrelating the relevant pieces of information, in or-der to obtain a comprehensive, non redundant reportthat satisfies the information need.In this paper, we will refer to this process as In-formation Synthesis.
It is normally understood asan (intellectually challenging) human task, and per-haps the Google Answer Service1 is the best gen-eral purpose illustration of how it works.
In this ser-vice, users send complex queries which cannot beanswered simply by inspecting the first two or threedocuments returned by a search engine.
These are acouple of real, representative examples:a) I?m looking for information concerning the history of textcompression both before and with computers.1http://answers.google.comb) Provide an analysis on the future of web browsers, ifany.Answers to such complex information needs areprovided by experts which, commonly, search theInternet, select the best sources, and assemble themost relevant pieces of information into a report,organizing the most important facts and providingadditional web hyperlinks for further reading.
ThisInformation Synthesis task is understood, in GoogleAnswers, as a human task for which a search engineonly provides the initial starting point.
Our mid-term goal is to develop computer assistants that helpusers to accomplish Information Synthesis tasks.From a Computational Linguistics point of view,Information Synthesis can be seen as a kind oftopic-oriented, informative multi-document sum-marization, where the goal is to produce a singletext as a compressed version of a set of documentswith a minimum loss of relevant information.
Un-like indicative summaries (which help to determinewhether a document is relevant to a particular topic),informative summaries must be helpful to answer,for instance, factual questions about the topic.
Inthe remainder of the paper, we will use the term?reports?
to refer to the summaries produced in anInformation Synthesis task, in order to distinguishthem from other kinds of summaries.Topic-oriented multi-document summarizationhas already been studied in other evaluation ini-tiatives which provide testbeds to compare alterna-tive approaches (Over, 2003; Goldstein et al, 2000;Radev et al, 2000).
Unfortunately, those stud-ies have been restricted to very small summaries(around 100 words) and small document sets (10-20 documents).
These are relevant summarizationtasks, but hardly representative of the InformationSynthesis problem we are focusing on.The first goal of our work has been, therefore,to create a suitable testbed that permits qualitativeand quantitative studies on the information synthe-sis task.
Section 2 describes the creation of such atestbed, which includes the manual generation of 72reports by nine different subjects across 8 complextopics with 100 relevant documents per topic.Using this testbed, our second goal has been tocompare alternative similarity metrics for the Infor-mation Synthesis task.
A good similarity metricprovides a way of evaluating Information Synthe-sis systems (comparing their output with manuallygenerated reports), and should also shed some lighton the common properties of manually generated re-ports.
Our working hypothesis is that the best metricwill best distinguish between manual and automati-cally generated reports.We have compared several similarity metrics, in-cluding a few baseline measures (based on docu-ment, sentence and vocabulary overlap) and a state-of-the-art measure to evaluate summarization sys-tems, ROUGE (Lin and Hovy, 2003).
We also intro-duce another proximity measure based on key con-cept overlap, which turns out to be substantially bet-ter than ROUGE for a relevant class of topics.Section 3 describes these metrics and the experi-mental design to compare them; in Section 4, we an-alyze the outcome of the experiment, and Section 5discusses related work.
Finally, Section 6 draws themain conclusions of this work.2 Creation of an Information SynthesistestbedWe refer to Information Synthesis as the processof generating a topic-oriented report from a non-trivial amount of relevant, possibly interrelated doc-uments.
The first goal of our work is the generationof a testbed (ISCORPUS) with manually producedreports that serve as a starting point for further em-pirical studies and evaluation of information synthe-sis systems.
This section describes how this testbedhas been built.2.1 Document collection and topic setThe testbed must have a certain number of featureswhich, altogether, differentiate the task from currentmulti-document summarization evaluations:Complex information needs.
Being Informa-tion Synthesis a step which immediately follows adocument retrieval process, it seems natural to startwith standard IR topics as used in evaluation con-ferences such as TREC2, CLEF3 or NTCIR4.
Thetitle/description/narrative topics commonly used insuch evaluation exercises are specially well suitedfor an Information Synthesis task: they are complex2http://trec.nist.gov3http://www.clef-campaign.org4http://research.nii.ac.jp/ntcir/and well defined, unlike, for instance, typical webqueries.We have selected the Spanish CLEF 2001-2003news collection testbed (Peters et al, 2002), be-cause Spanish is the native language of the subjectsrecruited for the manual generation of reports.
Outof the CLEF topic set, we have chosen the eighttopics with the largest number of documents man-ually judged as relevant from the assessment pools.We have slightly reworded the topics to change thedocument retrieval focus (?Find documents that...?
)into an information synthesis wording (?Generate areport about...?).
Table 1 shows the eight selectedtopics.C042: Generate a report about the invasion of Haiti by UN/USsoldiers.C045: Generate a report about the main negotiators of theMiddle East peace treaty between Israel and Jordan, givingdetailed information on the treaty.C047: What are the reasons for the military intervention ofRussia in Chechnya?C048: Reasons for the withdrawal of United Nations (UN)peace- keeping forces from Bosnia.C050: Generate a report about the uprising of Indians inChiapas (Mexico).C085: Generate a report about the operation ?Turquoise?, theFrench humanitarian program in Rwanda.C056: Generate a report about campaigns against racism inEurope.C080: Generate a report about hunger strikes attempted inorder to attract attention to a cause.Table 1: Topic setThis set of eight CLEF topics has two differenti-ated subsets: in a majority of cases (first six topics),it is necessary to study how a situation evolves intime; the importance of every event related to thetopic can only be established in relation with theothers.
The invasion of Haiti by UN and USA troops(C042) is an example of such a topic.
We will referto them as ?Topic Tracking?
(TT) reports, becausethey resemble the kind of topics used in such task.The last two questions (56 and 80), however, re-semble Information Extraction tasks: essentially,the user has to detect and describe instances ofa generic event (cases of hunger strikes and cam-paigns against racism in Europe); hence we will re-fer to them as ?IE?
reports.Topic tracking reports need a more elaboratedtreatment of the information in the documents, andtherefore are more interesting from the point of viewof Information Synthesis.
We have, however, de-cided to keep the two IE topics; first, because theyalso reflect a realistic synthesis task; and second, be-cause they can provide contrastive information ascompared to TT reports.Large document sets.
All the selected CLEFtopics have more than one hundred documentsjudged as relevant by the CLEF assessors.
For ho-mogeneity, we have restricted the task to the first100 documents for each topic (using a chronologi-cal order).Complex reports.
The elaboration of a com-prehensive report requires more space than is al-lowed in current multi-document summarization ex-periences.
We have established a maximum of fiftysentences per summary, i.e., half a sentence per doc-ument.
This limit satisfies three conditions: a) itis large enough to contain the essential informationabout the topic, b) it requires a substantial compres-sion effort from the user, and c) it avoids defaultingto a ?first sentence?
strategy by lazy (or tired) users,because this strategy would double the maximumsize allowed.We decided that the report generation would bean extractive task, which consists of selecting sen-tences from the documents.
Obviously, a realisticinformation synthesis process also involves rewrit-ing and elaboration of the texts contained in the doc-uments.
Keeping the task extractive has, however,two major advantages: first, it permits a direct com-parison to automatic systems, which will typicallybe extractive; and second, it is a simpler task whichproduces less fatigue.2.2 Generation of manual reportsNine subjects between 25 and 35 years-old were re-cruited for the manual generation of reports.
Allof them self-reported university degrees and a largeexperience using search engines and performing in-formation searches.All subjects were given an in-place detailed de-scription of the task in order to minimize divergentinterpretations.
They were told that, in a first step,they had to generate reports with a maximum of in-formation about every topic within the fifty sentencespace limit.
In a second step, which would takeplace six months afterwards, they would be exam-ined from each of the eight topics.
The only docu-mentation allowed during the exam would be the re-ports generated in the first phase of the experiment.Subjects scoring best would be rewarded.These instructions had two practical effects: first,the competitive setup was an extra motivation forachieving better results.
And second, users tried totake advantage of all available space, and thus mostreports were close to the fifty sentences limit.
Thetime limit per topic was set to 30 minutes, which istight for the information synthesis task, but preventsthe effects of fatigue.We implemented an interface to facilitate the gen-eration of extractive reports.
The system displays alist with the titles of relevant documents in chrono-logical order.
Clicking on a title displays the fulldocument, where the user can select any sentence(s)and add them to the final report.
A different framedisplays the selected sentences (also in chronolog-ical order), together with one bar indicating the re-maining time and another bar indicating the remain-ing space.
The 50 sentence limit can be temporarilyexceeded and, when the 30 minute limit has beenreached, the user can still remove sentences fromthe report until the sentence limit is reached back.2.3 QuestionnairesAfter summarizing every topic, the following ques-tionnaire was filled in by every user:?
Who are the main people involved in the topic??
What are the main organizations participating in thetopic??
What are the key factors in the topic?Users provided free-text answers to these ques-tions, with their freshly generated summary at hand.We did not provide any suggestions or constraintsat this point, except that a maximum of eight slotswere available per question (i.e.
a maximum of8X3 = 24 key concepts per topic, per user).This is, for instance, the answer of one user forthe topic 42 about the invasion of Haiti by UN andUSA troops in 1994:People OrganizationsJean Bertrand Aristide ONU (UN)Clinton EEUU (USA)Raoul Cedras OEA (OAS)Philippe BiambiMichel Josep FrancoisFactorsmilitares golpistas (coup attempting soldiers)golpe militar (coup attempt)restaurar la democracia (reinstatement of democracy)Finally, a single list of key concepts is gener-ated for each topic, joining all the different answers.Redundant concepts (e.g.
?war?
and ?conflict?
)were inspected and collapsed by hand.
These listsof key concepts constitute the gold standard for thesimilarity metric described in Section 3.2.5.Besides identifying key concepts, users also filledin the following questionnaire:?
Were you familiarized with the topic??
Was it hard for you to elaborate the report??
Did you miss the possibility of introducing annotationsor rewriting parts of the report by hand??
Do you consider that you generated a good report??
Are you tired?Out of the answers provided by users, the mostremarkable facts are that:?
only in 6% of the cases the user missed ?a lot?the possibility of rewriting/adding commentsto the topic.
The fact that reports are made ex-tractively did not seem to be a significant prob-lem for our users.?
in 73% of the cases, the user was quite or verysatisfied about his summary.These are indications that the practical con-straints imposed on the task (time limit and extrac-tive nature of the summaries) do not necessarilycompromise the representativeness of the testbed.The time limit is very tight, but the temporal ar-rangement of documents and their highly redundantnature facilitates skipping repetitive material (somepieces of news are discarded just by looking at thetitle, without examining the content).2.4 Generation of baseline reportsWe have automatically generated baseline reports intwo steps:?
For every topic, we have produced 30 tentativebaseline reports using DUC style criteria:?
18 summaries consist only of picking thefirst sentence out of each document in 18different document subsets.
The subsetsare formed using different strategies, e.g.the most relevant documents for the query(according to the Inquery search engine),one document per day, the first or last 50documents in chronological order, etc.?
The other 12 summaries consist of a)picking the first n sentences out of a setof selected documents (with different val-ues for n and different sets of documents)and b) taking the full content of a few doc-uments.
In both cases, document sets areformed with similar criteria as above.?
Out of these 30 baseline reports, we have se-lected the 10 reports which have the highestsentence overlap with the manual summaries.The second step increases the quality of the base-lines, making the task of differentiating manual andbaseline reports more challenging.3 Comparison of similarity metricsFormal aspects of a summary (or report), suchas legibility, grammatical correctness, informative-ness, etc., can only be evaluated manually.
How-ever, automatic evaluation metrics can play a usefulrole in the evaluation of how well the informationfrom the original sources is preserved (Mani, 2001).Previous studies have shown that it is feasible toevaluate the output of summarization systems au-tomatically (Lin and Hovy, 2003).
The process isbased in similarity metrics between texts.
The firststep is to establish a (manual) reference summary,and then the automatically generated summaries areranked according to their similarity to the referencesummary.The challenge is, then, to define an appropriateproximity metric for reports generated in the infor-mation synthesis task.3.1 How to compare similarity metrics withouthuman judgments?
The QARLAestimationIn tasks such as Machine Translation and Summa-rization, the quality of a proximity metric is mea-sured in terms of the correlation between the rank-ing produced by the metric, and a reference rankingproduced by human judges.
An optimal similaritymetric should produce the same ranking as humanjudges.In our case, acquiring human judgments aboutthe quality of the baseline reports is too costly, andprobably cannot be done reliably: a fine-grainedevaluation of 50-sentence reports summarizing setsof 100 documents is a very complex task, whichwould probably produce different rankings fromdifferent judges.We believe there is a cheaper and more robustway of comparing similarity metrics without usinghuman assessments.
We assume a simple hypothe-sis: the best metric should be the one that best dis-criminates between manual and automatically gen-erated reports.
In other words, a similarity metricthat cannot distinguish manual and automatic re-ports cannot be a good metric.
Then, all we needis an estimation of how well a similarity metric sep-arates manual and automatic reports.
We proposeto use the probability that, given any manual reportMref , any other manual report M is closer to Mrefthan any other automatic report A:QARLA(sim) = P (sim(M,Mref ) > sim(A,Mref ))where M,Mref ?M, A ?
Awhere M is the set of manually generated re-ports, A is the set of automatically generated re-ports, and ?sim?
is the similarity metric being eval-uated.We refer to this value as the QARLA5 estimation.QARLA has two interesting features:?
No human assessments are needed to computeQARLA.
Only a set of manually producedsummaries and a set of automatic summaries,for each topic considered.
This reduces thecost of creating the testbed and, in addition,eliminates the possible bias introduced by hu-man judges.?
It is easy to collect enough data to achieve sta-tistically significant results.
For instance, ourtestbed provides 720 combinations per topicto estimate QARLA probability (we havenine manual plus ten automatic summaries pertopic).A good QARLA value does not guarantee thata similarity metric will produce the same rankingsas human judges, but a good similarity metric musthave a good QARLA value: it is unlikely thata measure that cannot distinguish between manualand automatic summaries can still produce high-quality rankings of automatic summaries by com-parison to manual reference summaries.3.2 Similarity metricsWe have compared five different metrics using theQARLA estimation.
The first three are meant asbaselines; the fourth is the standard similarity met-ric used to evaluate summaries (ROUGE); and thelast one, introduced in this paper, is based on theoverlapping of key concepts.3.2.1 Baseline 1: Document co-selection metricThe following metric estimates the similarity of tworeports from the set of documents which are repre-sented in both reports (i.e.
at least one sentence ineach report belongs to the document).DocSim(Mr,M) =|Doc(Mr) ?Doc(M)||Doc(Mr)|where Mr is the reference report, M a second re-port and Doc(Mr), Doc(M) are the documents towhich the sentences in Mr,M belong to.5Quality criterion for reports evaluation metrics3.2.2 Baselines 2 and 3: Sentence co-selectionThe more sentences in common between two re-ports, the more similar their content will be.
We canmeasure Recall (how many sentences from the ref-erence report are also in the contrastive report) andPrecision (how many sentences from the contrastivereport are also in the reference report):SentenceSimR(Mr,M) =|S(Mr) ?
S(M)||S(Mr)|SentenceSimP (Mr,M) =|S(Mr) ?
S(M)||S(M)|where S(Mr), S(M) are the sets of sentences inthe reports Mr (reference) and M (contrastive).3.2.3 Baseline 4: PerplexityA language model is a probability distribution overword sequences obtained from some training cor-pora (see e.g.
(Manning and Schutze, 1999)).
Per-plexity is a measure of the degree of surprise of atext or corpus given a language model.
In our case,we build a language model LM(Mr) for the refer-ence report Mr, and measure the perplexity of thecontrastive report M as compared to that languagemodel:PerplexitySim(Mr,M) =1Perp(LM(Mr),M)We have used the Good-Turing discount algo-rithm to compute the language models (Clarksonand Rosenfeld, 1997).
Note that this is also a base-line metric, because it only measures whether thecontent of the contrastive report is compatible withthe reference report, but it does not consider the cov-erage: a single sentence from the reference reportwill have a low perplexity, even if it covers only asmall fraction of the whole report.
This problemis mitigated by the fact that we are comparing re-ports of approximately the same size and withoutrepeated sentences.3.2.4 ROUGE metricThe distance between two summaries can be estab-lished as a function of their vocabulary (unigrams)and how this vocabulary is used (n-grams).
Fromthis point of view, some of the measures used in theevaluation of Machine Translation systems, such asBLEU (Papineni et al, 2002), have been importedinto the summarization task.
BLEU is based in theprecision and n-gram co-ocurrence between an au-tomatic translation and a reference manual transla-tion.
(Lin and Hovy, 2003) tried to apply BLEU asa measure to evaluate summaries, but the resultswere not as good as in Machine Translation.
In-deed, some of the characteristics that define a goodtranslation are not related with the features of a goodsummary; then Lin and Hovy proposed a recall-based variation of BLEU, known as ROUGE.
Theidea is the same: the quality of a proposed sum-mary can be calculated as a function of the n-gramsin common between the units of a model summary.The units can be sentences or discourse units:ROUGEn =?C?
{MU}?n-gram?C Countm?C?
{MU}?n-gram?C Countwhere MU is the set of model units, Countm isthe maximum number of n-grams co-ocurring in apeer summary and a model unit, and Count is thenumber of n-grams in the model unit.
It has beenestablished that unigram and bigram based metricspermit to create a ranking of automatic summariesbetter (more similar to a human-produced ranking)than n-grams with n > 2.For our experiment, we have only considered un-igrams (lemmatized words, excluding stop words),which gives good results with standard summaries(Lin and Hovy, 2003).3.2.5 Key concepts metricTwo summaries generated by different subjects maydiffer in the documents that contribute to the sum-mary, in the sentences that are chosen, and even inthe information that they provide.
In our Informa-tion Synthesis settings, where topics are complexand the number of documents to summarize is large,it is likely to expect that similarity measures basedon document, sentence or n-gram overlap do notgive large similarity values between pairs of man-ually generated summaries.Our hypothesis is that two manual reports, even ifthey differ in their information content, will have thesame (or very similar) key concepts; if this is true,comparing the key concepts of two reports can be abetter similarity measure than the previous ones.In order to measure the overlap of key conceptsbetween two reports, we create a vector ~kc for everyreport, such that every element in the vector repre-sents the frequency of a key concept in the report inrelation to the size of the report:kc(M)i =freq(Ci,M)|words(M)|being freq(Ci,M) the number of times thekey concept Ci appears in the report M , and|words(M)| the number of words in the report.The key concept similarity NICOS (Nuclear In-formative Concept Similarity) between two reportsM and Mr can then be defined as the inverse of theEuclidean distance between their associated conceptvectors:NICOS(M,Mr) =1| ~kc(Mr)?
~kc(M)|In our experiment, the dimensions of kc vectorscorrespond to the list of key concepts provided byour test subjects (see Section 2.3).
This list is ourgold standard for every topic.4 Experimental resultsFigure 1 shows, for every topic (horizontal axis),the QARLA estimation obtained for each similaritymetric, i.e., the probability of a manual report beingcloser to other manual report than to an automaticreport.
Table 2 shows the average QARLA measureacross all topics.Metric TT topics IE topicsPerplexity 0.19 0.60DocSim 0.20 0.34SentenceSimR 0.29 0.52SentenceSimP 0.38 0.57ROUGE 0.54 0.53NICOS 0.77 0.52Table 2: Average QARLAFor the six TT topics, the key concept similarityNICOS performs 43% better than ROUGE, and allbaselines give poor results (all their QARLA proba-bilities are below chance, QARLA < 0.5).
A non-parametric Wilcoxon sign test confirms that the dif-ference between NICOS and ROUGE is highly sig-nificant (p < 0.005).
This is an indication that theInformation Synthesis task, as we have defined it,should not be studied as a standard summarizationproblem.
It also confirms our hypothesis that keyconcepts tend to be stable across different users, andmay help to generate the reports.The behavior of the two Information Extraction(IE) topics is substantially different from TT topics.While the ROUGE measure remains stable (0.53versus 0.54), the key concept similarity is muchworse with IE topics (0.52 versus 0.77).
On theother hand, all baselines improve, and some of them(SentenceSim precision and perplexity) give betterresults than both ROUGE and NICOS.Of course, no reliable conclusion can be obtainedfrom only two IE topics.
But the observed differ-ences suggest that TT and IE may need differentapproaches, both to the automatic generation of re-ports and to their evaluation.Figure 1: Comparison of similarity metrics by topicOne possible reason for this different behavior isthat IE topics do not have a set of consistent keyconcepts; every case of a hunger strike, for instance,involves different people, organizations and places.The average number of different key concepts is18.7 for TT topics and 28.5 for IE topics, a differ-ence that reveals less agreement between subjects,supporting this argument.5 Related workBesides the measures included in our experiment,there are other criteria to compare summaries whichcould as well be tested for Information Synthesis:Annotation of relevant sentences in a corpus.
(Khandelwal et al, 2001) propose a task, called?Temporal Summarization?, that combines summa-rization and topic tracking.
The paper describes thecreation of an evaluation corpus in which the mostrelevant sentences in a set of related news were an-notated.
Summaries are evaluated with a measurecalled ?novel recall?, based in sentences selected bya summarization system and sentences manually as-sociated to events in the corpus.
The agreement ratebetween subjects in the identification of key eventsand the sentence annotation does not correspondwith the agreement between reports that we haveobtained in our experiments.
There are, at least, tworeasons to explain this:?
(Khandelwal et al, 2001) work on an averageof 43 documents, half the size of the topics inour corpus.?
Although there are topics in both experiments,the information needs in our testbed are morecomplex (e.g.
motivations for the invasion ofChechnya)Factoids.
One of the problems in the evalua-tion of summaries is the versatility of human lan-guage.
Two different summaries may contain thesame information.
In (Halteren and Teufel, 2003),the content of summaries is manually represented,decomposing sentences in factoids or simple facts.They also annotate the composition, generalizationand implication relations between extracted fac-toids.
The resulting measure is different from un-igram based similarity.
The main problem of fac-toids, as compared to other metrics, is that they re-quire a costly manual processing of the summariesto be evaluated.6 ConclusionsIn this paper, we have reported an empirical studyof the ?Information Synthesis?
task, defined as theprocess of (given a complex information need) ex-tracting, organizing and relating the pieces of infor-mation contained in a set of relevant documents, inorder to obtain a comprehensive, non redundant re-port that satisfies the information need.We have obtained two main results:?
The creation of an Information Synthesistestbed (ISCORPUS) with 72 reports manuallygenerated by 9 subjects for 8 complex topicswith 100 relevant documents each.?
The empirical comparison of candidate metricsto estimate the similarity between reports.Our empirical comparison uses a quantitative cri-terion (the QARLA estimation) based on the hy-pothesis that a good similarity metric will be able todistinguish between manual and automatic reports.According to this measure, we have found evidencethat the Information Synthesis task is not a standardmulti-document summarization problem: state-of-the-art similarity metrics for summaries do not per-form equally well with the reports in our testbed.Our most interesting finding is that manuallygenerated reports tend to have the same key con-cepts: a similarity metric based on overlapping keyconcepts (NICOS) gives significantly better resultsthan metrics based on language models, n-gram co-ocurrence and sentence overlapping.
This is an in-dication that detecting relevant key concepts is apromising strategy in the process of generating re-ports.Our results, however, has also some intrinsic lim-itations.
Firstly, manually generated summaries areextractive, which is good for comparison purposes,but does not faithfully reflect a natural process ofhuman information synthesis.
Another weakness isthe maximum time allowed per report: 30 minutesseems too little to examine 100 documents and ex-tract a decent report, but allowing more time wouldhave caused an excessive fatigue to users.
Our vol-unteers, however, reported a medium to high satis-faction with the results of their work, and in someoccasions finished their task without reaching thetime limit.ISCORPUS is available at:http://nlp.uned.es/ISCORPUSAcknowledgmentsThis research has been partially supported by agrant of the Spanish Government, project HERMES(TIC-2000-0335-C03-01).
We are indebted to E.Hovy for his comments on an earlier version ofthis paper, and C. Y. Lin for his assistance with theROUGE measure.
Thanks also to our volunteers fortheir valuable cooperation.ReferencesP.
Clarkson and R. Rosenfeld.
1997.
Statisticallanguage modeling using the CMU-Cambridgetoolkit.
In Proceeding of Eurospeech ?97,Rhodes, Greece.J.
Goldstein, V. O. Mittal, J. G. Carbonell, andJ.
P. Callan.
2000.
Creating and EvaluatingMulti-Document Sentence Extract Summaries.In Proceedings of Ninth International Confer-ences on Information Knowledge Management(CIKM?00), pages 165?172, McLean, VA.H.
V. Halteren and S. Teufel.
2003.
Examin-ing the Consensus between Human Summaries:Initial Experiments with Factoids Analysis.
InHLT/NAACL-2003 Workshop on Automatic Sum-marization, Edmonton, Canada.V.
Khandelwal, R. Gupta, and J. Allan.
2001.
AnEvaluation Corpus for Temporal Summarization.In Proceedings of the First International Confer-ence on Human Language Technology Research(HLT 2001), Tolouse, France.C.
Lin and E. H. Hovy.
2003.
Automatic Evalua-tion of Summaries Using N-gram Co-ocurrenceStatistics.
In Proceeding of the 2003 LanguageTechnology Conference (HLT-NAACL 2003), Ed-monton, Canada.I.
Mani.
2001.
Automatic Summarization, vol-ume 3 of Natural Language Processing.
JohnBenjamins Publishing Company, Amster-dam/Philadelphia.C.
D. Manning and H. Schutze.
1999.
Foundationsof statistical natural language processing.
MITPress, Cambridge Mass.P.
Over.
2003.
Introduction to DUC-2003: An In-trinsic Evaluation of Generic News Text Summa-rization Systems.
In Proceedings of Workshop onAutomatic Summarization (DUC 2003).K.
Papineni, S. Roukos, T. Ward, and W. Zhu.2002.
Bleu: a method for automatic evalua-tion of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318, Philadelphia.C.
Peters, M. Braschler, J. Gonzalo, and M. Kluck,editors.
2002.
Evaluation of Cross-LanguageInformation Retrieval Systems, volume 2406 ofLecture Notes in Computer Science.
Springer-Verlag, Berlin-Heidelberg-New York.D.
R. Radev, J. Hongyan, and M. Budzikowska.2000.
Centroid-Based Summarization of Mul-tiple Documents: Sentence Extraction, Utility-Based Evaluation, and User Studies.
In Proceed-ings of the Workshop on Automatic Summariza-tion at the 6th Applied Natural Language Pro-cessing Conference and the 1st Conference of theNorth American Chapter of the Association forComputational Linguistics, Seattle, WA, April.
