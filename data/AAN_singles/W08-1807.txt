Coling 2008: Proceedings of the 2nd workshop on Information Retrieval for Question Answering (IR4QA), pages 50?57Manchester, UK.
August 2008Using lexico-semantic information for query expansion in passageretrieval for question answeringLonneke van der PlasLATLUniversity of GenevaSwitzerlandlonneke.vanderplas@lettres.unige.chJo?rg TiedemannAlfa-InformaticaUniversity of GroningenThe Netherlandsj.tiedemann@rug.nlAbstractIn this paper we investigate the use of sev-eral types of lexico-semantic informationfor query expansion in the passage retrievalcomponent of our QA system.
We haveused four corpus-based methods to acquiresemantically related words, and we haveused one hand-built resource.
We eval-uate our techniques on the Dutch CLEFQA track.1 In our experiments expansionsthat try to bridge the terminological gapbetween question and document collectiondo not result in any improvements.
How-ever, expansions bridging the knowledgegap show modest improvements.1 IntroductionInformation retrieval (IR) is used in most QA sys-tems to filter out relevant passages from large doc-ument collections to narrow down the search foranswer extraction modules in a QA system.
Accu-rate IR is crucial for the success of this approach.Answers in paragraphs that have been missed byIR are lost for the entire QA system.
Hence, highperformance of IR especially in terms of recall isessential.
Furthermore, high precision is desirableas IR scores are used for answer extraction heuris-tics and also to reduce the chance of subsequentextraction errors.Because the user?s formulation of the questionis only one of the many possible ways to state theinformation need that the user might have, there isc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.1The Cross-Language Evaluation Forum (http://clef-qa.itc.it/)often a discrepancy between the terminology usedby the user and the terminology used in the doc-ument collection to describe the same concept.
Adocument might hold the answer to the user?s ques-tion, but it will not be found due to the TERMI-NOLOGICAL GAP.
Moldovan et al (2002) showthat their system fails to answer many questions(25.7%), because of the terminological gap, i.e.keyword expansion would be desirable but is miss-ing.
Query expansion techniques have been devel-oped to bridge this gap.However, we believe that there is more than justa terminological gap.
There is also a KNOWLEDGEGAP.
Documents are missed or do not end up highin the ranks, because additional world knowledgeis missing.
We are not speaking of synonyms here,but words belonging to the same subject field.
Forexample, when a user is looking for informationabout the explosion of the first atomic bomb, inhis/her head a subject field is active that could in-clude: war, disaster, World War II.We have used three corpus-based methodsto acquire semantically related words: theSYNTAX-BASED METHOD, the ALIGNMENT-BASED METHOD, and the PROXIMITY-BASEDMETHOD.
The nature of the relations betweenwords found by the three methods is very differ-ent.
Ranging from free associations to synonyms.Apart from these resources we have used cate-gorised named entities, such as Van Gogh IS-Apainter and synsets from EWN as candidate ex-pansion terms.In this paper we have applied several types oflexico-semantic information to the task of queryexpansion for QA.
We hope that the synonymsretrieved automatically, and in particular the syn-onyms retrieved by the alignment-based method,as these are most precise, will help to overcome the50terminological gap.
With respect to the knowledgegap, we expect that the proximity-based methodwould be most helpful as well as the list of cate-gorised named entities.
For example, knowing thatMonica Seles is a tennis player helps to find rele-vant passages regarding this tennis star.2 Related workThere are many ways to expand queries and ex-pansions can be acquired from several sources.For example, one can make use of collection-independent resources, such as EWN.
In contrast,collection-dependent knowledge structures are of-ten constructed automatically based on data fromthe collection.The results from using collection-independent,hand-built sources are varied.
Moldovan et al(2003) show that using a lexico-semantic feed-back loop that feeds lexico-semantic alternationsfrom WordNet as keyword expansions to the re-trieval component of their QA system incrementsthe scores by 15%.
Also, Pasc?a and Harabagiu(2001) show substantial improvements when us-ing lexico-semantic information from WordNet forkeyword alternation on the morphological, lexicaland semantic level.
They evaluated their system onquestion sets of TREC-8 and TREC-9.
For TREC-8 they reach a precision score of 55.3% with-out including any alternations for question key-words, 67.6% if lexical alternations are allowedand 73.7% if both lexical and semantic alternationsare allowed.However, Yang and Chua (2003) report thatadding additional terms from WordNet?s synsetsand glosses adds more noise than information tothe query.
Also, Voorhees (1993) concludes thatexpanding by automatically generated synonymsets from EWN can degrade results.In Yang et al (2003) the authors use externalknowledge extracted from WordNet and the Webto expand queries for QA.
Minor improvementsare attained when the Web is used to retrieve alist of nearby (one sentence or snippet) non-trivialterms.
When WordNet is used to rank the retrievedterms, the improvement is reduced.
The best re-sults are reached when structure analysis is addedto knowledge from the Web and WordNet.
Struc-ture analysis determines the relations that hold be-tween the candidate expansion terms to identifysemantic groups.
Semantic groups are then con-nected by conjunction in the Boolean query.Monz (2003) ran experiments using pseudo rel-evance feedback for IR in a QA system.
The authorreports dramatic decreases in performance.
He ar-gues that this might be due to the fact that thereare usually only a small number of relevant doc-uments.
Another reason he gives is the fact thathe used the full document to fetch expansion termsand the information that allows one to answer thequestion is expressed very locally.A global technique that is most similar to oursuses syntactic context to find suitable terms forquery expansion (Grefenstette, 1992; Grefenstette,1994).
The author reports that the gain is mod-est: 2% when expanded with nearest neighboursfound by his system and 5 to 6%, when apply-ing stemming and a second loop of expansionsof words that are in the family of the augmentedquery terms.2 Although the gain is greater thanwhen using document co-occurrence as context,the results are mixed, with expansions improvingsome query results and degrading others.Also, the approach by Qiu and Frei (1993) isa global technique.
They automatically constructa similarity thesaurus, based on what documentsterms appear in.
They use word-by-document ma-trices, where the features are document IDs, to de-termine the similarity between words.
Expansionsare selected based on the similarity to the queryconcept, i.e.
all words in the query together, andnot based on the single words in the query inde-pendently.
The results they get are promising.Pantel and Ravichandran (2004) have used amethod that is not related to query expansion,but yet very related to our work.
They have se-mantically indexed the TREC-2002 IR collectionwith the ISA-relations found by their system for179 questions that had an explicit semantic answertype, such as What band was Jerry Garcia with?They show small gains in performance of the IRoutput using the semantically indexed collection.Recent work (Shen and Lapata, 2007; Kaisserand Webber, 2007) that falls outside the scope ofthis paper, but that is worth mentioning success-fully applies semantic roles to question answering.3 Lexico-semantic informationWe have used several types of lexico-semanticinformation as sources for candidate expansionterms.
The first three are automatically acquired2i.e.
words that appear in the same documents and thatshare the first three, four or five letters.51from corpora by means of distributional methods.?
Nearest neighbours from proximity-baseddistributional similarity?
Nearest neighbours from syntax-based distri-butional similarity?
Nearest neighbours from alignment-baseddistributional similarityThe idea behind distributional methods is rootedin the DISTRIBUTIONAL HYPOTHESIS (Harris,1968).
Similar words appear in similar context.The way words are distributed over contexts tellsus something about their meaning.
Context canbe defined in several ways.
The way the contextis defined determines the type of lexico-semanticknowledge we will retrieve.For example, one can define the context of aword as the n words surrounding it.
In that caseproximity to the head word is the determiningfactor.
We refer to these methods that use un-structured context as PROXIMITY-BASED METH-ODS.
The nearest neighbours resulting from suchmethods are rather unstructured as well.
They aremerely associations between words, such as babyand cry.
We have used the 80 million-word corpusof Dutch newspaper text (the CLEF corpus) that isalso part of the document collection in the QA taskto retrieve co-occurrences within sentences.Another approach is one in which the contextof a word is determined by syntactic relations.
Inthis case, the head word is in a syntactic relationto a second word and this second word accom-panied by the syntactic relation form the contextof the head word.
We refer to these methods asSYNTAX-BASED METHODS.
We have used severalsyntactic relations to acquire syntax-based contextfor our headwords.
This method results in nearestneighbours that at least belong to the same seman-tic and syntactic class, for example baby and son.We have used 500 million words of newspaper text(the TwNC corpus parsed by Alpino (van Noord,2006)) of which the CLEF corpus is a subset.A third method we have used is theALIGNMENT-BASED METHOD.
Here, trans-lations of word, retrieved from the automaticword alignment of parallel corpora are used todetermine the similarity between words.
Thismethod results in even more tightly related data,as it mainly finds synonyms, such as infant andbaby.
We have used the Europarl corpus (Koehn,2003) to extract word alignments from.3By calculating the similarity between the con-texts words are found in, we can retrieve aranked list of nearest neighbours for any head-word.
We gathered nearest neighbours for afrequency-controlled list of words, that was stillmanageable to retrieve.
We included all words(nouns, verbs, adjectives and proper names) witha frequency of 150 and higher in the CLEF cor-pus.
This resulted in a ranked list of nearest neigh-bours for the 2,387 most frequent adjectives, the5,437 most frequent nouns, the 1,898 most fre-quent verbs, and the 1,399 most frequent propernames.
For all words we retrieved a ranked listof its 100 nearest neighbours with accompanyingsimilarity score.In addition to the lexico-semantic informationresulting from the three distributional methods weused:?
Dutch EuroWordNet (Vossen, 1998)?
Categorised named entitiesWith respect to the first resource we can beshort.
We selected the synsets of this hand-builtlexico-semantic resource for nouns, verbs, adjec-tives and proper names.The categorised named entities are a by-productof the syntax-based distributional method.
Fromthe example in (1) we extract the apposition rela-tion between Van Gogh and schilder ?painter?
todetermine that the named entity Van Gogh belongsto the category of painters.
(1) Van Gogh, de beroemde schilder huurdeeen atelier, Het Gele huis, in Arles.
?Van Gogh, the famous painter, rented astudio, The Yellow House, in Arles.
?We used the data of the TwNC corpus (500Mwords) and Dutch Wikipedia (50M words) to ex-tract apposition relations.
The data is skewed.
TheNetherlands appears with 1,251 different labels.To filter out incorrect and highly unlikely labels(often the result of parsing errors) we determinedthe relative frequency of the combination of thenamed entity and a category with regard to the fre-quency of the named entity overall.
All categorisednamed entities with relative frequencies under 0.053In van der Plas and Tiedemann (2006) there is more in-formation on the syntax-based and alignment-based distribu-tional methods.52Lex.
info Nouns Adj Verbs ProperProximity 5.3K 2.4K 1.9K 1.2KSyntax 5.4K 2.3K 1.9K 1.4KAlign 4.0K 1.2K 1.6KCat.
NEs 218KEWN 44.9K 1.5K 9.0K 1.4KTable 1: Number of words for which lexico-semantic information is availablewere discarded.
This cutoff made the number ofunwanted labels considerably lower.In Table 1 we see the amount of informationthat is contained in individual lexico-semantic re-sources.
It is clear from the numbers that thealignment-based method does not provide near-est neighbours for all head words selected.
Only4.0K nouns from the 5.4K retrieve nearest neigh-bours.
The data is sparse.
Also, the alignment-based method does not have any nearest neigh-bours for proper names, due to decisions we madeearlier regarding preprocessing: All words weretransformed to lowercase.The proximity-based method also misses a num-ber of words, but the number is far less impor-tant.
The amount of information the lists of cate-gorised named entities provide is much larger thanthe amount of information comprised in the listprovided by distributional methods.
EWN alsoprovides more information than the distributionalmethods, except for adjectives.44 MethodologyIn order to test the performance of the var-ious lexico-semantic resources we ran severaltests.
The baseline is running a standard full-textretrieval engine using Apache Lucene (Jakarta,2004).
Documents have been lemmatised and stopwords have been removed.We applied the nearest neighbours resultingfrom the three distributional methods as describedin section 3.
For all methods we selected the top-5 nearest neighbours that had a similarity score ofmore than 0.2 as expansions.For EWN all words in the same synset (for allsenses) were added as expansions.
Since all syn-onyms are equally similar, we do not have similar-ity scores for them to be used in a threshold.The categorised named entities were not onlyused to expand named entities with the corre-4Note that the number of nouns from EWN is the result ofsubtracting the proper names.sponding label, but also to expand nouns withnamed entities.
In the first case all labels wereselected.
The maximum is not more than 18 la-bels.
In the second case some nouns get manyexpansions.
For example, a noun, such as vrouw?woman?, gets 1,751 named entities as expansions.We discarded nouns with more than 50 expansions,as these were deemed too general and hence notvery useful.The last two settings are the same for the expan-sions resulting from distributional methods and thelast two types of lexico-semantic information.?
Expansions were added as root forms?
Expansions were given a weight such that allexpansions for one original keyword add upto 0.5.5 EvaluationFor evaluation we used data collected from theCLEF Dutch QA tracks.
The CLEF text collec-tion contains 4 years of newspaper text, approxi-mately 80 million words and Dutch Wikipedia, ap-proximately 50 million words.
We used the ques-tion sets from the competitions of the Dutch QAtrack in 2003, 2004, and 2005 (774 in total).
Ques-tions in these sets are annotated with valid answersfound by the participating teams including IDs ofsupporting documents in the given text collection.We expanded these list of valid answers where nec-essary.We calculated for each run the Mean ReciprocalRank (MRR).5 The MRR measures the percentageof passages for which a correct answer was foundin the top-k passages returned by the system.
TheMRR score is the average of 1/R where R is therank of the first relevant passage computed overthe 20 highest ranked passages.
Passages retrievedwere considered relevant when one of the possibleanswer strings was found in that passage.6 ResultsIn Table 2 the MRR (Mean Reciprocal Rank) isgiven for the various expansion techniques.
Scoresare given for expanding the several syntactic cat-egories, where possible.
The baseline does not5We used MRR instead of other common evaluation mea-sures because of its stronger correlation with the overall per-formance of our QA system than, for example, coverage andredundancy (see Tiedemann and Mur (2008)).53MRRSynCat EWN Syntax Align Proxi Cat.NEsNouns 51.52 51.15 51.21 51.38 51.75Adj 52.33 52.27 52.38 51.71Verbs 52.40 52.33 52.21 52.62Proper 52.59 50.16 53.94 55.68All 51.65 51.21 51.02 53.36 55.29Table 2: MRR scores for the IR component withquery expansion from several sources#questions (+/-)SynCat EWN Syntax Align Proxi Cat.NEsNouns 27/50 28/61 17/58 64/87 17/37Adj 3/6 1/2 1/2 31/47Verbs 31/51 5/10 8/32 51/56Proper 3/2 30/80 76/48 157/106All 56/94 56/131 25/89 161/147 168/130Table 3: Number of questions that receive a higher(+) or lower (-) RR when using expansions fromseveral sourcesmake use of any expansion for any syntactic cat-egory and amounts to 52.36.In Table 3 the number of questions that get ahigher and lower reciprocal rank (RR) after ap-plying the individual lexico-semantic resources aregiven.
Apart from expansions on adjectives andproper names from EWN, the impact of the expan-sion is substantial.
The fact that adjectives haveso little impact is due to the fact that there are notmany adjectives among the query terms.6The negligible impact of the proper names fromEWN is surprising since EWN provides more en-tries for proper names than the proximity-basedmethod (1.2K vs 1.4K, as can be seen in 1).
Theproximity-based method clearly provides informa-tion about proper names that are more relevant forthe corpus used for QA, as it is built from a subsetof that same corpus.
This shows the advantage ofusing corpus-based methods.
The impact of the ex-pansions resulting from the syntax-based methodlies in between the two previously mentioned ex-pansions.
It uses a corpus of which the corpus usedfor QA is a subset.The type of expansions that result from theproximity-based method have a larger effect onthe performance of the system than those result-ing from the syntax-based method.
In Chapter 5 ofvan der Plas (2008) we explain in greater detail thatthe proximity-based method uses frequency cut-6Moreover, the adjectives related to countries, such asGerman and French and their expansion Germany, France arehandled by a separate list.offs to keep the co-occurrence matrix manageable.The larger impact of the proximity-based nearestneighbours is probably partly due to this decision.The cutoffs for the alignment-based and syntax-based method have been determined after evalu-ations on EuroWordNet (Vossen, 1998) (see alsovan der Plas (2008)).The largest impact results from expandingproper names with categorised named entities.
Weknow from Table 1 in section 3, that this resourcehas 70 times more data than the proximity-basedresource.For most of the resources the number of ques-tions that show a rise in RR is smaller than thenumber of questions that receive a lower RR, ex-cept for the expansion of proper names by the cat-egorised named entities and the proximity-basedmethod.The expansions resulting from the syntax-basedmethod do not result in any improvements.
Asexpected, the expansion of proper names fromthe syntax-based method hurts the performancemost.
Remember that the nearest neighbours of thesyntax-based method often include co-hyponyms.For example, Germany would get The Netherlandsand France as nearest neighbours.
It does not seemto be a good idea to expand the word Germanywith other country names when a user, for exam-ple, asks the name of the Minister of Foreign Af-fairs of Germany.
However, also the synonymsfrom EWN and the alignment-based method do notresult in improvements.The categorised named entities provide the mostsuccessful lexico-semantic information, whenused to expand named entities with their categorylabel.
The MRR is augmented by almost 3,5%.
Itis clear that using the same information in the otherdirection, i.e.
to expand nouns with named enti-ties of the corresponding category hurts the scores.The proximity-based nearest neighbours of propernames raises the MRR scores with 1,5%.Remember from the introduction that we madea distinction between the terminological gap andthe knowledge gap.
The lexico-semantic re-sources that are suited to bridge the terminolog-ical gap, such as synonyms from the alignment-based method and EWN, do not result in improve-ments in the experiments under discussion.
How-ever, the lexico-semantic resources that may beused to bridge the knowledge gap, i.e.
associationsfrom the proximity-based method and categorised54CLEF scoreEWN Syntax Align Proxi Cat.NEs Baseline46.3 47.0 46.6 47.6 47.9 46.8Table 4: CLEF scores of the QA system with queryexpansion from several sourcesnamed entities, do result in improvements of theIR component.To determine the effect of query expansion onthe QA system as a whole we determined the av-erage CLEF score when using the various typesof lexico-semantic information for the IR com-ponent.
The CLEF score gives the precision ofthe first (highest ranked) answer only.
For EWN,the syntax-based, and the alignment-based nearestneighbours we have used all expansions for all syn-tactic categories together.
For the proximity-basednearest neighbours and the categorised named en-tities we have limited the expansions to the propernames as these performed rather well.The positive effect of using categorised namedentities and proximity-based nearest neighboursfor query expansion is visible in the CLEF scoresas well, although less apparent than in the MRRscores from the IR component in Table 2.6.1 Error analysisLet us first take a look at the disappointing re-sults regarding the terminological gap, before wemove to the more promising results related to theknowledge gap.
We expected that the expansionsof verbs would be particularly helpful to overcomethe terminological gap that is large for verbs, sincethere is much variation.
We will give some exam-ples of expansion from EWN and the alignment-based method.
(2) Wanneer werd het Verdrag van Rome getekend?
?When was the Treaty of Rome signed?
?Align: teken ?sign??
typeer ?typify?, onderteken ?sign?EWN: teken ?sign?
?
typeer ?typify?, kentekenen ?charac-terise?, kenmerk ?characterise?, schilder ?paint?, kenschets?characterise?, signeer ?sign?, onderteken ?sign?, schets?sketch?, karakteriseer ?characterise?.For the example in (2) both the alignment-basedexpansions and the expansion from EWN result ina decrease in RR of 0.5.
The verb teken ?sign?
isambiguous.
We see three senses of the verb repre-sented in the EWN list, i.e.
drawing, characteris-ing, and signing as in signing an official document.One out of the two expansions for the alignment-based method and 2 out of 9 for EWN are in princi-ple synonyms of teken ?sign?
in the right sense forthis question.
However, the documents that holdthe answer to this question do not use synonymsfor the word teken.
The expansions only introducenoise.We found a positive example in (3).
The RRscore is improved by 0.3 for both the alignment-based expansions and the expansions from EWN,when expanding explodeer ?explode?
with ontplof?blow up?.
(3) Waar explodeerde de eerste atoombom?
?Where did the first atomic bomb explode?
?Align: explodeer ?explode?
?
ontplof ?blow up?.EWN: explodeer ?explode??
barst los ?burst?, ontplof ?blowup?, barst uit ?crack?, plof ?boom?.To get an idea of the amount of terminologi-cal variation between the questions and the doc-uments, we determined the optimal expansionwords for each query, by looking at the wordsthat appear in the relevant documents.
When in-specting these, we learned that there is in fact lit-tle to be gained by terminological variation.
Inthe 25 questions we inspected we found 1 near-synonym only that improved the scores: gekke-koeienziekte ?mad cow disease?
?
Creutzfeldt-Jacob-ziekte ?Creutzfeldt-Jacob disease?.The fact that we find only few synonyms mightbe related to a point noted by Mur (2006): Someof the questions in the CLEF track that we use forevaluation look like back formulations.After inspecting the optimal expansions, wewere under the impression that most of the expan-sions that improved the scores were related to theknowledge gap, rather than the terminological gap.We will now give some examples of good and badexpansions related to the knowledge gap.The categorised named entities result in the bestexpansions, followed by the proximity-based ex-pansions.
In (4) an example is given for which cat-egorised named entities proved very useful:(4) Wie is Keith Richard?
?Who is Keith Richard??Cat.
NEs: Keith Richard ?
gitarist ?guitar player?, lid?member?, collega ?colleague?, Rolling Stones-gitarist?Rolling Stones guitar player?, Stones-gitarist ?Stones guitarplayer?.It is clear that this type of information helps a lotin answering the question in (4).
It contains theanswer to the question.
The RR for this questiongoes from 0 to 1.
We see the same effect for the55question Wat is NASA?
?What is NASA?
?.It is a known fact that named entities are an im-portant category for QA.
Many questions ask fornamed entities or facts related to named entities.From these results we can see that adding the ap-propriate categories to the named entities is usefulfor IR in QA.The categorised named entities were not alwayssuccessful.
In (5) we show that the proximity-based expansion proved more helpful in somecases.
(5) Welke bevolkingsgroepen voerden oorlog inRwanda?
?What populations waged war in Rwanda?
?Proximity: Rwanda?
Za?
?re, Hutu, Tutsi, Ruanda, Rwandees?Rwandese?.Cat.
NEs: Rwanda ?
bondgenoot ?ally?, land ?country?,staat ?state?, buurland ?neighbouring country?.In this case the expansions from the proximity-based method are very useful (except for Zaire),since they include the answer to the question.
Thatis not always the case, as can be seen in (6).
How-ever, the expansions from the categorised namedentities are not very helpful in this case either.
(6) Wanneer werd het Verdrag van Rome getekend?
?When was the treaty of Rome signed?
?Proximity: Rome ?
paus ?pope?, Italie?, bisschop ?bishop?,Italiaans ?Italian?, Milaan ?Milan?.Cat.
NEs: Rome ?
provincie ?province?, stad ?city?,hoofdstad ?capital?, gemeente ?municipality?.IR does identify Verdrag van Rome ?Treaty ofRome?
as a multi-word term, however it adds theindividual parts of multi-word terms as keywordsas a form of compound analysis.
It might be bet-ter to expand the multi-word term only and notits individual parts to decrease ambiguity.
Ver-drag van Rome ?Treaty of Rome?
is not found inthe proximity-based nearest neighbours, because itdoes not include multi-word terms.Still, it is not very helpful to expand the wordRome with pope for this question that has nothingto do with religious affairs.
We can see this as aproblem of word sense disambiguation.
The as-sociation pope belongs to Rome in the religioussense, the place where the Catholic Church isseated.
Rome is often referred to as the CatholicChurch itself, as in Henry VIII broke from Rome.Gonzalo et al (1998) showed in an experiment,where words were manually disambiguated, thata substantial increase in performance is obtainedwhen query words are disambiguated, before theyare expanded.We tried to take care of these ambiguities byusing an overlap method.
The overlap methodselects expansions that were found in the near-est neighbours of more than two query words.Unfortunately, as Navigli and Velardi (2003),who implement a similar technique, using lexico-semantic information from WordNet, note, theCOMMON NODES EXPANSION TECHNIQUE worksvery badly.
Also, Voorhees (1993) who uses asimilar method to select expansions concludes thatthe method has the tendency to select very generalterms that have more than one sense themselves.In future work we would like to implement themethod by Qiu and Frei (1993), as discussed insection 2, that uses a more sophisticated techniqueto combine the expansions of several words in thequery.7 ConclusionWe can conclude from these experiments on queryexpansion for passage retrieval that query expan-sion with synonyms to overcome the terminolog-ical gap is not very fruitful.
We believe that thenoise introduced by ambiguity of the query termsis stronger than the positive effect of adding lexi-cal variants.
This is in line with findings by Yangand Chua (2003).
On the contrary, Pasc?a andHarabagiu (2001) were able to improve their QAsystem by using lexical and semantic alternationsfrom WordNet using feedback loops.The disappointing results might also be due tothe small amount of terminological variation be-tween questions and document collection.However, adding extra information with regardto the subject field of the query, query expansionsthat bridge the knowledge gap, proved slightlybeneficial.
The proximity-based expansions aug-ment the MRR scores with 1.5%.
Most successfulare the categorised named entities.
These expan-sions were able to augment the MRR scores withnearly 3.5%.The positive effect of using categorised namedentities and proximity-based nearest neighboursfor query expansion is visible in the CLEF scoresfor the QA system overall as well.
However, theimprovements are less apparent than in the MRRscores from the IR component.56AcknowledgementsThis research was carried out in the projectQuestion Answering using Dependency Relations,which is part of the research program for Interac-tive Multimedia Information eXtraction, IMIX, fi-nanced by NWO, the Dutch Organisation for Scien-tific Research and partly by the European Commu-nity?s Seventh Framework Programme (FP7/2007-2013) under grant agreement n 216594 (CLASSICproject: www.classic-project.org).ReferencesGonzalo, J., F. Verdejo, I. Chugur, and J. Cigarran.1998.
Indexing with WordNet synsets can improvetext retrieval.
In Proceedings of the COLING/ACLWorkshop on Usage of WordNet for NLP.Grefenstette, G. 1992.
Use of syntactic context to pro-duce term association lists for text retrieval.
In Pro-ceedings of the Annual International Conference onResearch and Development in Information Retrieval(SIGIR).Grefenstette, G. 1994.
Explorations in automatic the-saurus discovery.
Kluwer Academic Publishers.Harris, Z. S. 1968.
Mathematical structures of lan-guage.
Wiley.Jakarta, Apache.
2004.
Apache Lucene - a high-performance, full-featured text search engine library.http://lucene.apache.org/java/docs/index.html.Kaisser, M. and B. Webber.
2007.
Question answeringbased on semantic roles.
In Proceedings of de ACLworkshop on deep linguistic processing.Koehn, P. 2003.
Europarl: A multilingual corpus forevaluation of machine translation.Moldovan, D., M. Passc?a, S. Harabagiu, and M. Sur-deanu.
2002.
Performance issues and error analysisin an open-domain question answering system.
InProceedings of the 40th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL).Moldovan, D., M. Pasc?a, S. Harabagiu, and M. Sur-deanu.
2003.
Performance issues and error analysisin an open-domain question answering system.
ACMTransactions on Information Systems., 21(2):133?154.Monz, C. 2003.
From Document Retrieval to QuestionAnswering.
Ph.D. thesis, University of Amsterdam.Mur, J.
2006.
Increasing the coverage of answer ex-traction by applying anaphora resolution.
In FifthSlovenian and First International Language Tech-nologies Conference (IS-LTC).Navigli, R. and P. Velardi.
2003.
An analysis ofontology-based query expansion strategies.
In Pro-ceedings of the Workshop on Adaptive Text Extrac-tion and Mining (ATEM), in the 14th European Con-ference on Machine Learning (ECML 2003).Pantel, P. and D. Ravichandran.
2004.
Automati-cally labeling semantic classes.
In Proceedings ofthe Conference on Human Language Technology andEmpirical Methods in Natural Language Processing(HLT/EMNLP).Pasc?a, M. and S Harabagiu.
2001.
The informative roleof wordnet in open-domain question answering.
InProceedings of the NAACL 2001 Workshop on Word-Net and Other Lexical Resources.Qiu, Y. and H.P.
Frei.
1993.
Concept-based query ex-pansion.
In Proceedings of the Annual InternationalConference on Research and Development in Infor-mation Retrieval (SIGIR), pages 160?169.Shen, D. and M. Lapata.
2007.
Using semantic rolesto improve question answering.
In Proceedings ofEMNLP.Tiedemann, J. and J. Mur.
2008.
Simple is best: Exper-iments with different document segmentation strate-gies for passage retrieval.
In Proceedings of theColing workshop Information Retrieval for QuestionAnswering.
To appear.van der Plas, L. and J. Tiedemann.
2006.
Findingsynonyms using automatic word alignment and mea-sures of distributional similarity.
In Proceedings ofCOLING/ACL.van der Plas, Lonneke.
2008.
Automatic lexico-semantic acquisition for question answering.
Ph.D.thesis, University of Groningen.
To appear.van Noord, G. 2006.
At last parsing is now operational.In Actes de la 13eme Conference sur le TraitementAutomatique des Langues Naturelles.Voorhees, E.M. 1993.
Query expansion using lexical-semantic relations.
In Proceedings of the AnnualInternational Conference on Research and Develop-ment in Information Retrieval (SIGIR).Vossen, P. 1998.
EuroWordNet a multilingual databasewith lexical semantic networks.Yang, H. and T-S. Chua.
2003.
Qualifier: question an-swering by lexical fabric and external resources.
InProceedings of the Conference on European Chap-ter of the Association for Computational Linguistics(EACL).Yang, H., T-S. Chua, Sh.
Wang, and Ch-K. Koh.
2003.Structured use of external knowledge for event-basedopen domain question answering.
In Proceedingsof the Annual International Conference on Researchand Development in Information Retrieval (SIGIR).57
