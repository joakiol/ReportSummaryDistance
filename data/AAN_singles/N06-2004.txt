Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 13?16,New York, June 2006. c?2006 Association for Computational LinguisticsMeasuring Semantic Relatedness Using People and WordNetBeata Beigman KlebanovSchool of Computer Science and EngineeringThe Hebrew University, Jerusalem, Israelbeata@cs.huji.ac.ilAbstractIn this paper, we (1) propose a new datasetfor testing the degree of relatedness be-tween pairs of words; (2) propose a newWordNet-based measure of relatedness, andevaluate it on the new dataset.1 IntroductionEstimating the degree of semantic relatedness be-tween words in a text is deemed important innumerous applications: word-sense disambigua-tion (Banerjee and Pedersen, 2003), story segmen-tation (Stokes et al, 2004), error correction (Hirstand Budanitsky, 2005), summarization (Barzilay andElhadad, 1997; Gurevych and Strube, 2004).Furthermore, Budanitsky and Hirst (2006) notedthat various applications tend to pick the same mea-sures of relatedness, which suggests a certain com-monality in what is required from such a measure bythe different applications.
It thus seems worthwhileto develop such measures intrinsically, before puttingthem to application-based utility tests.The most popular, by-now-standard testbed isRubenstein and Goodenough?s (1965) list of 65 nounpairs, ranked by similarity of meaning.
A 30-pairsubset (henceforth, MC) passed a number of repli-cations (Miller and Charles, 1991; Resnik, 1995), andis thus highly reliable.Rubenstein and Goodenough (1965) view simi-larity of meaning as degree of synonymy.
Researchershave long recognized, however, that synonymy is onlyone kind of semantic affinity between words in atext (Halliday and Hasan, 1976), and expressed awish for a dataset for testing a more general notionof semantic relatedness.11?.
.
.
similarity of meaning is not the same thing assemantic relatedness.
However, there is at present nolarge dataset of human judgments of semantic related-This paper proposes and explores a new related-ness dataset.
In sections 2-3, we briefly introducethe experiment by Beigman Klebanov and Shamir(henceforth, BS), and use the data to induce related-ness scores.
In section 4, we propose a new WordNet-based measure of relatedness, and use it to explorethe new dataset.
We show that it usually does bet-ter than competing WordNet-based measures (sec-tion 5).
We discuss future directions in section 6.2 DataAiming at reader-based exploration of lexical cohe-sion in texts, Beigman Klebanov and Shamir con-ducted an experiment with 22 students, each reading10 texts: 3 news stories, 4 journalistic and 3 fictionpieces (Beigman Klebanov and Shamir, 2006).
Peo-ple were instructed to read the text first, and thengo over a separately attached list of words in orderof their appearance in the text, and ask themselves,for every newly mentioned concept, ?which previ-ously mentioned concepts help the easy accommoda-tion of the current concept into the evolving story,if indeed it is easily accommodated, based on thecommon knowledge as perceived by the annotator?
(Beigman Klebanov and Shamir, 2005); this preced-ing helper concept is called an anchor.
People wereasked to mark all anchoring relations they could find.The rendering of relatedness between two conceptsis not tied to any specific lexical relation, but ratherto common-sense knowledge, which has to do with?knowledge of kinds, of associations, of typical sit-uations, and even typical utterances?.2 The phe-nomenon is thus clearly construed as much broaderthan degree-of-synonymy.Beigman Klebanov and Shamir (2006) provide re-liability estimation of the experimental data usingness?
(Hirst and Budanitsky, 2005); ?To our knowledge,no datasets are available for validating the results of se-mantic relatedness metric?
(Gurevych, 2005).2according to Hirst (2000), cited in the guidelines13statistical analysis and a validation experiment, iden-tifying reliably anchored items with their strong an-chors, and reliably un-anchored items.
Such analysisprovides high-validity data for classification; how-ever, much of the data regarding intermediate de-grees of relatedness is left out.3 Relatedness ScoresOur idea is to induce scores for pairs of anchoreditems with their anchors (henceforth, AApairs)using the cumulative annotations by 20 people.3Thus, an AApair written by all 20 people scores 20,and that written by just one person scores 1.
Thescores would correspond to the perceived relatednessof the pair of concepts in the given text.In Beigman Klebanov and Shamir?s (2006) coreclassification data, no distinctions are retained be-tween pairs marked by 19 or 13 people.
Now weare interested in the relative relatedness, so it is im-portant to handle cases where the BS data mightunder-rate a pair.
One such case are multi-worditems; we remove AApairs with suspect multi-wordelements.4 Further, we retain only pairs that belongto open-class parts of speech (henceforth, POS), asfunctional categories contribute little to the lexicaltexture (Halliday and Hasan, 1976).
The Size col-umn of table 1 shows the number of AApairs foreach BS text, after the aforementioned exclusions.The induced scores correspond to cumulativejudgements of a group of people.
How well do theyrepresent the people?s ideas?
One way to measuregroup homogeneity is leave-one-out estimation, asdone by Resnik (1995) for MC data, attaining thehigh average correlation of r = 0.88.
In the currentcase, however, every specific person made a binarydecision, whereas a group is represented by scores 1to 20; such difference in granularity is problematicfor correlation or rank order analysis.Another way to measure group homogeneity is tosplit it into subgroups and compare scores emergingfrom the different subgroups.
We know fromBeigman Klebanov and Shamir?s (2006) analysis thatit is not the case that the 20-subject group clustersinto subgroups that systematically produced differ-ent patterns of answers.
This leads us to expect rel-ative lack of sensitivity to the exact splits into sub-groups.To validate this reasoning, we performed 100 ran-dom choices of two 9-subject4 groups, calculated thescores induced by the two groups, and computed3Two subjects were revealed as outliers and their datawas removed (Beigman Klebanov and Shamir, 2006).4See Beigman Klebanov (2006) for details.Pearson correlation between the two lists.
Thus, forevery BS text, we have a distribution of 100 coeffi-cients, which is approximately normal.
Estimationsof ?
and ?
of these distributions are ?
= .69 ?
.82(av.
0.75), ?
= .02?
.03 for the different BS texts.To summarize: although the homogeneity is lowerthan for MC data, we observe good average inter-group correlations with little deviation across the 100splits.
We now turn to discussion of a relatednessmeasure, which we will evaluate using the data.4 Gic: WordNet-based MeasureMeasures using WordNet taxonomy are state-of-the-art in capturing semantic similarity, attainingr=.85 ?.89 correlations with the MC dataset (Jiangand Conrath, 1997; Budanitsky and Hirst, 2006).However, they fall short of measuring relatedness,as, operating within a single-POS taxonomy, theycannot meaningfully compare kill to death.
This isa major limitation with respect to BS data, whereonly about 40% of pairs are nominal, and less than10% are verbal.
We develop a WordNet-based mea-sure that would allow cross-POS comparisons, usingglosses in addition to the taxonomy.One family of WordNet measures are methodsbased on estimation of information content (hence-forth, IC) of concepts, as proposed in (Resnik, 1995).Resnik?s key idea in corpus-based information con-tent induction using a taxonomy is to count everyappearance of a concept as mentions of all its hy-pernyms as well.
This way, artifact#n#1, althoughrarely mentioned explicitly, receives high frequencyand low IC value.
We will count a concept?s men-tion towards all its hypernyms AND all words5 thatappear in its own and its hypernyms?
glosses.
Analo-gously to artifact, we expect properties mentioned inglosses of more general concepts to be less informa-tive, as those pertain to more things (ex., visible,a property of anything that is-a physical object).The details of the algorithm for information con-tent induction from taxonomy and gloss information(ICGT ) are given in appendix A.To estimate the semantic affinity between twosenses A and B, we average the ICGT values of the3 words with the highest ICGT in the overlap of A?sand B?s expanded glosses (the expansion follows thealgorithm in appendix A).65We induce IC values on (POS-tagged baseform) words rather than senses.
Ongoing glosssense-tagging projects like eXtended WordNet(http://xwn.hlt.utdallas.edu/links.html) would allowsense-based calculation in the future.6The number 3 is empirically-based; the idea is tocounter-balance (a) the effect of an accidental match of a14Data Size Gic BP Data Size Gic BPBS-1 1007 .29 .19 BS-6 536 .24 .19BS-2 776 .37 .16 BS-7 917 .22 .10BS-3 1015 .22 .09 BS-8 529 .24 .12BS-4 512 .34 .39 BS-9 509 .31 .16BS-5 1020 .25 .11 BS10 417 .36 .19Table 1: Dataset sizes and correlations of Gic, BPwith human ratings.
r > 0.16 is significant atp < .05; r > .23 is significant at p < .01.
Averagecorrelation (AvBS) is r=.28 (Gic), r=.17 (BP).If A?
(the word of which A is a sense) appearsin the expanded gloss of B, we take the maximumbetween the ICGT (A?)
and the value returned bythe 3-smoothed calculation.
To compare two words,we take the maximum value returned by pairwisecomparisons of their WordNet senses.7The performance of this measure is shown underGic in table 1.
Gic manages robust but weak corre-lations, never reaching the r = .40 threshold.5 Related WorkWe compare Gic to another WordNet-based measurethat can handle cross-POS comparisons, proposedby Banerjee and Pedersen (2003).
To compare wordsenses A and B, the algorithm compares not onlytheir glosses, but also glosses of items standing invarious WordNet relations with A and B.
For ex-ample, it compares the gloss of A?s meronym to thatof B?s hyponym.
We use the default configurationof the measure in WordNet::Similarity-0.12 package(Pedersen et al, 2004), and, with a single exception,the measure performed below Gic; see BP in table 1.As mentioned before, taxonomy-based similaritymeasures cannot fully handle BS data.
Table 2 usesnominal-only subsets of BS data and the MC nominalsimilarity dataset to show that (a) state-of-the-artWordNet-based similarity measure JC8 (Jiang andConrath, 1997; Budanitsky and Hirst, 2006) doesvery poorly on the relatedness data, suggesting thatnominal similarity and relatedness are rather differ-ent things; (b) Gic does better on average, and ismore robust; (c) Gic yields on MC to gain perfor-mance on BS, whereas BP is no more inclined to-single word which is relatively rarely used in glosses; (b)the multitude of low-IC items in many of the overlapsthat tend to downplay the impact of the few higher-ICmembers of the overlap.7To speed the processing up, we use first 5 WordNetsenses of each item for results reported here.8See formula in appendix B.
We use (Pedersen etal., 2004) implementation with a minor alteration ?
seeBeigman Klebanov (2006).wards relatedness than JC.Data Gic BP JC Data Gic BP JCBS-1 .38 .18 .21 BS-6 .25 .16 .22BS-2 .53 .18 .37 BS-7 .23 .10 .04BS-3 .21 .04 .01 BS-8 .32 .10 .00BS-4 .28 .38 .33 BS-9 .24 .17 .27BS-5 .12 .07 .16 BS10 .41 .25 .25AvBS .30 .16 .19 MC .78 .80 .86Table 2: MC and nominal-only subsets of BS: corre-lations of various measures with the human ratings.Table 3 illustrates the relatedness vs. similaritydistinction.
Whereas, taxonomically speaking, sonis more similar to man, as reflected in JC scores,people marked family and mother as much strongeranchors for son in BS-2; Gic follows suit.AApair Human Gic JCson ?
man 2 0.355 22.3son ?
family 13 0.375 16.9son ?
mother 16 0.370 20.1Table 3: Relatendess vs. similarity6 Conclusion and Future WorkWe proposed a dataset of relatedness judgementsthat differs from the existing ones9 in (1) size ?about 7000 items, as opposed to up to 350 in existingdatasets; (2) cross-POS data, as opposed to purelynominal or verbal; (3) a broad approach to semanticrelatedness, not focussing on any particular relation,but grounding it in the reader?s (idea of) commonknowledge; this as opposed to synonymy-based simi-larity prevalent in existing databases.We explored the new data with WordNet-basedmeasures, showing that (1) the data is different incharacter from a standard similarity dataset, andvery challenging for state-of-the-art methods; (2) theproposed novel WordNet-based measure of related-ness usually outperforms its competitor, as well asa state-of-the-art similarity measure when the latterapplies.In future work, we plan to explore distributionalmethods for modeling relatedness, as well as theuse of text-based information to improve correlationswith the human data, as judgments are situated inspecific textual contexts.9Though most widely used, MC is not the only avail-able dataset; we will address other datasets in a subse-quent paper.15ReferencesSatanjeev Banerjee and Ted Pedersen.
2003.
Ex-tended gloss overlaps as a measure of semanticrelatedness.
In Proceedings of IJCAI.Regina Barzilay and Michael Elhadad.
1997.
Usinglexical chains for text summarization.
In Pro-ceedings of ACL Intelligent Scalable Text Summa-rization Workshop.Beata Beigman Klebanov and Eli Shamir.
2005.Guidelines for annotation of concept mention pat-terns.
Technical Report 2005-8, Leibniz Center forResearch in Computer Science, The Hebrew Uni-versity of Jerusalem, Israel.Beata Beigman Klebanov and Eli Shamir.
2006.Reader-based exploration of lexical cohesion.
Toappear in Language Resources and Evaluation.Springer, Netherlands.Beata Beigman Klebanov.
2006.
Using people andWordNet to measure semantic relatedness.
Tech-nical Report 2006-17, Leibniz Center for Researchin Computer Science, The Hebrew University ofJerusalem, Israel.Alexander Budanitsky and Graeme Hirst.
2006.Evaluating WordNet-based measures of semanticdistance.
Computational Linguistics, 32(1):13?47.Iryna Gurevych and Michael Strube.
2004.
Semanticsimilarity applied to spoken dialogue summariza-tion.
In Proceedings of COLING.Iryna Gurevych.
2005.
Using the structure of a con-ceptual network in computing semantic related-ness.
In Proceedings of IJCNLP.M.A.K.
Halliday and Ruqaiya Hasan.
1976.
Cohe-sion in English.
Longman Group Ltd.Graeme Hirst and Alexander Budanitsky.
2005.Correcting real-word spelling errors by restoringlexical cohesion.
Natural Language Engineering,11(1):87?111.Graeme Hirst.
2000.
Context as a spurious concept.In Proceedings of CICLING.Jay Jiang and David Conrath.
1997.
Semantic simi-larity based on corpus statistics and lexical taxon-omy.
In Proceedings on International Conferenceon Research in Computational Linguistics.George Miller and Walter Charles.
1991.
Contex-tual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):1?28.Ted Pedersen, Siddharth Patwardhan, and JasonMichelizzi.
2004.
WordNet::Similarity-measuringthe relatedness of concepts.
In Proceedings ofNAACL.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
InProceedings of IJCAI.Herbert Rubenstein and John Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.Nicola Stokes, Joe Carthy, and Alan F. Smeaton.2004.
SeLeCT: A lexical cohesion based news storysegmentation system.
Journal of AI Communica-tions, 17(1):3?12.A Gloss&Taxonomy IC (ICGT )We refer to POS-tagged base form items as ?words?throughout this section.
For every word-sense W inWordNet database for a given POS:1.
Collect all content words from the gloss of W ,excluding examples, including W ?
- the POS-tagged word of which W is a sense.2.
If W is part of a taxonomy, expand its gloss,without repetitions, with words appearing inthe glosses of all its super-ordinate concepts,up to the top of the hierarchy.
Thus, the ex-panded gloss for airplane#n#1 would containwords from the glosses of the relevant senses ofaircraft , vehicle, transport, etc.3.
Add W ?s sense count to all words in its ex-panded gloss.10Each POS database induces its own counts on eachword that appeared in the gloss of at least one of itsmembers.
When merging the data from the differ-ent POS, we scale the aggregated counts, such thatthey correspond to the proportion of the given wordin the POS database where it was the least informa-tive.
The standard log-frequency calculation trans-forms these counts into taxonomy-and-gloss based in-formation content (ICGT ) values.B JC measure of similarityIn the formula, IC is taxonomy-only based informa-tion content, as in (Resnik, 1995), LS is the lowestcommon subsumer of the two concepts in the Word-Net hierarchy, and Max is the maximum distance11between any two concepts.JC(c1, c2) = Max?
(IC(c1)+IC(c2)?2?IC(LS(c1, c2))To make JC scores comparable to Gic?s [0,1] range,the score can be divided by Max.
Normalization hasno effect on correlations.10We do add-1-smoothing on WordNet sense counts.11This is about 26 for WordNet-2.0 nominal hierar-chy with add-1-smoothed SemCor database; see BeigmanKlebanov (2006) for details.16
