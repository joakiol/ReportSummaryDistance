Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 53?58,Baltimore, Maryland, USA, June 27, 2014.c?2014 Association for Computational LinguisticsMUCK: A toolkit for extracting and visualizing semantic dimensions oflarge text collectionsRebecca WeissStanford UniversityStanford, CA, 94305rjweiss@stanford.eduAbstractUsers with large text collections are of-ten faced with one of two problems; ei-ther they wish to retrieve a semantically-relevant subset of data from the collectionfor further scrutiny (needle-in-a-haystack)or they wish to glean a high-level un-derstanding of how a subset compares tothe parent corpus in the context of afore-mentioned semantic dimensions (forest-for-the-trees).
In this paper, I describeMUCK1, an open-source toolkit that ad-dresses both of these problems through adistributed text processing engine with aninteractive visualization interface.1 IntroductionAs gathering large text collections grows increas-ingly feasible for non-technical users, individu-als such as journalists, marketing/communicationsanalysts, and social scientists are accumulatingvast quantities of documents in order to addresskey strategy or research questions.
But thesegroups often lack the technical skills to work withlarge text collections, in that the conventional ap-proaches they employ (content analysis and indi-vidual document scrutiny) are not suitable for thescale of the data they have gathered.
Thus, usersrequire tools with the capability to filter out irrel-evant documents while drilling-down to the docu-ments that they are most interested in investigatingwith closer scrutiny.
Furthermore, they require thecapability to then evaluate their subset in context,as the contrast in attributes between their subsetand the full corpora can often address many rele-vant questions.This paper introduces a work-in-progress: thedevelopment of a toolkit that aids non-technical1Mechanical Understanding of Contextual Knowledgeusers of large text collections by combining se-mantic search and semantic visualization methods.The purpose of this toolkit is two-fold: first, toease the technical burden of working with large-scale text collections by leveraging semantic infor-mation for the purposes of filtering a large collec-tion of text down to the select sample documentsthat matter most to the user; second, to allow theuser to visually explore semantic attributes of theirsubset in comparison to the rest of the text collec-tion.Thus, this toolkit comprises two components:1. a distributed text processing engine that de-creases the cost of annotating massive quan-tities of text data for natural language infor-mation2.
an interactive visualization interface that en-ables exploration of the collection along se-mantic dimensions, which then affords sub-sequent document selection and subset-to-corpora comparisonThe text processing engine is extensible, en-abling the future development of plug-ins to al-low for tasks beyond the included natural languageprocessing tasks, such that future users can em-bed any sentence- or document-level task to theirprocessing pipeline.
The visualization interface isbuilt upon search engine technologies to decreasesearch result latency to user requests, enabling ahigh level of interactivity.2 Related workThe common theme of existing semantic searchand semantic visualization methods is to enablethe user to gain greater, meaningful insight into thestructure of their document collections through theuse of transparent, trustworthy methods (Chuanget al., 2012; Ramage et al., 2009).
The desired in-sight can change depending on the intended task.53For some applications, users are understood tohave a need to find a smaller, relevant subset ofarticles (or even a single article) in a vast collec-tion of documents, which we can refer to as aneedle-in-a-haystack problem.
For others, userssimply require the ability to gain a broad but de-scriptive summary of a semantic concept that de-scribes these text data, which we can refer to as aforest-for-the-trees problem.For example, marketers and social scientists of-ten study news data, as the news constitute a vi-tally important source of information that guidethe agendas of marketing strategy and informmany theories underlying social behavior.
How-ever, their interests are answered at the level ofsentences or documents that contain the conceptsor entities that they care about.
This need is oftennot met through simple text querying, which canreturn too many or too few relevant documents andsentences.
This is an example of a needle-in-a-haystack problem, which has been previously ad-dressed through the application of semantic search(Guha et al., 2003).
Much of the literature onsemantic search, in which semantic informationsuch as named entity, semantic web data, or simpledocument categories are added to the individual-level results of a simple query in order to bolsterthe relevance of resulting query hits.
This typeof information has proven to be useful in filteringout irrelevant content for a wide array of informa-tion retrieval tasks (Blanco et al., 2011; Pound etal., 2010; Hearst, 1999b; Hearst, 1999a; Liu et al.,2009; Odijk et al., 2012).Remaining in the same narrative, once a sub-set of relevant documents has been created, theseusers may wish to see how the semantic charac-teristics of their subset contrast to the parent col-lection from which it was drawn.
A marketer mayhave a desire to see how the tone of coverage innews related to their client?s brand compares tothe news coverage of other brands of a similartype.
A social scientist may be interested to seeif one news organization covers more politiciansthan other news organizations.
This is an exam-ple of a forest-for-the-trees problem.
This type ofproblem has been addressed through the applica-tion of semantic visualization, which can be use-ful for trend analysis and anomaly detection in textcorpora (Fisher et al., 2008; Chase et al., 1998;Hearst and Karadi, 1997; Hearst, 1995; Ando etal., 2000).The toolkit outlined in this paper leverages bothof these techniques in order to facilitate the user?sability to gain meaningful insight into various se-mantic attributes of their text collection while alsoretrieving semantically relevant documents.3 Overview of System From UserPerspectiveThe ordering of a user?s experience with thistoolkit is as follows:1.
Users begin with a collection of unstructuredtext documents, which must be made avail-able to the system (e.g., on a local or networkdrive or as a list of URLs for remote content)2.
Users specify the types of semantic detail rel-evant to their analysis (named entities, senti-ment, etc.
), and documents are then parsed,annotated, and indexed.3.
Users interact with the visualization in or-der to create the subset of documents or sen-tences they are interested in according to se-mantic dimensions of relevance4.
Once a view has been adequately configuredusing the visual feedback, users are able to re-trieve the documents or sentences referencedin the visualization from the document storeItems 2 and 3 are further elaborated in the sec-tions on the backend and frontend.4 BackendThe distributed processing engine is driven by atask planner, which is a framework for chainingper-document tasks.
As diagrammed in figure 1,the system creates and distributes text processingtasks needed to satisfy the user?s level of semanticinterest according to the dependencies between thevarious integrated third-party text processing li-braries.
Additionally, this system does not possessdependencies on additional third-party large-scaleprocessing frameworks or message queueing sys-tems, which makes this toolkit useful for relativelylarge (i.e.
millions of documents) collections as itdoes not require configuration of other technolo-gies beyond maintaining a document store2and asearch index.2http://www.mongodb.com54index task plannerlocalfilesURLlistRSSfeedlistdocumentextractionlocal worker pool1 n 2 ?remote worker pool1 n 2 ?documentstorefront endtask resolverFigure 1: The architecture of the backend system.Task planner and resolver system The se-mantic information extraction process occurs viadefining a series of tasks for each document.
Thisinstantiates a virtual per-document queues of pro-cessing tasks.
These queues are maintained bya task planner and resolver, which handles all ofthe distribution of processing tasks through theuse of local or cloud resources3.
This processingmodel enables non-technical users to describe acomputationally-intensive, per-document process-ing pipeline without having to perform any tech-nical configuration beyond specifying the level ofprocessing detail output desired.NLP task Currently, this system only incor-porates the full Stanford CoreNLP pipeline4,which processes each document into its (likely)constituent sentences and tokens and annotateseach sentence and token for named entities,parts-of-speech, dependency relations, and senti-ment (Toutanova et al., 2003; Finkel et al., 2005;De Marneffe et al., 2006; Raghunathan et al.,2010; Lee et al., 2011; Lee et al., 2013; Re-casens et al., 2013; Socher et al., 2013).
This ex-traction process is extensible, meaning that futuretasks can be defined and included in the processingqueue in the order determined by the dependen-cies of the new processing technology.
Additionaltasks at the sentence- or document-level, such assimple text classification using the Stanford Clas-sifier (Manning and Klein, 2003), are included inthe development roadmap.3http://aws.amazon.com4Using most recent version as of writing (v3.1)5 FrontendA semantic dimension of interest is mapped to adimension of the screen as a context pane, as di-agrammed in figure 2.
Corpora-level summariesfor each dimension are provided within each con-text pane for each semantic category, whereas thesubset that the user interactively builds is visual-ized in the focus pane of the screen.
By brushingeach of semantic dimensions, the user can drill-down to relevant data while also maintaining anunderstanding of the semantic contrast betweentheir subset and the parent corpus.This visualization design constitutes a multiple-view system (Wang Baldonado et al., 2000), wherea single conceptual entity can be viewed from sev-eral perspectives.
In this case, the semantic con-cepts extracted from the data can be portrayed inseveral ways.
This system maps semantic dimen-sions to visualization components using the fol-lowing interaction techniques:Navigational slaving Users must first make aninitial selection for data by querying for a spe-cific item of interest; a general text query (idealfor phrase matching), a named entity, or even anentity that served in a specific dependency relation(such as the dependent of an nsubj relation).
Thisselection propagates through the remaining com-ponents of the interface, such that the remainingsemantic dimensions are manipulated in the con-text of the original query.Focus + Context Users can increase their under-standing of the subset by zooming into a relevant55focus panecontext pane(dimension 2)context pane(dimension 1)context pane (dimension 3 )primary navigational slaving pane (query)brush  brushfilter filterfilterfilterfilter filterbrush  brushfilterFigure 2: The wireframe of the frontend system.selection in a semantic dimension (e.g.
time).Brushing Users can further restrict their sub-set by highlighting categories or ranges of interestin semantic dimensions (e.g.
document sources,types of named entities).
Brushing technique isdetermined by whether the semantic concept iscategorical or continuous.Filtering The brushing and context panes serveas filters, which restrict the visualized subset toonly documents containing the intersection of allbrushed characteristics.This visualization design is enabled through theuse of a distributed search engine5, which enablesthe previously defined interactivity through threebehaviors:Filters Search engines enable the restrictionof query results according to whether a querymatches the parameters of a filter, such as whethera field contains text of a specific pattern.Facets Search engines also can return subsets ofdocuments structured along a dimension of inter-est, such as by document source types (if such in-formation was originally included in the index).Aggregations Aggregations allow for bucketingof relevant data and metrics to be calculated per5http://www.elasticsearch.combucket.
This allows the swift retrieval of docu-ments in a variety of structures, providing the hi-erarchical representation required for visualizinga subset along multiple semantic dimensions de-fined above.Nesting All of these capabilities can be stackedupon each other, allowing for the multiple viewsystem described above.The visualization components are highly inter-active, since the application is built upon a two-way binding design paradigm6between the DOMand the RESTful API of the index (Bostock et al.,2011).6 Discussion and future workThis paper presents a work-in-progress on the de-velopment of a system that enables the extractionand visualization of large text collections along se-mantic dimensions.
This system is open-sourceand extensible, so that additional per-documentprocessing tasks for future semantic extractionprocedures can be easily distributed.
Additionally,this system does not possess requirements beyondmaintaining a document store and a search index.6http://www.angularjs.org56ReferencesRie Kubota Ando, Branimir K Boguraev, Roy J Byrd,and Mary S Neff.
2000.
Multi-document summa-rization by visualizing topical content.
In Proceed-ings of the 2000 NAACL-ANLP Workshop on Auto-matic Summarization, pages 79?98.
Association forComputational Linguistics.Roi Blanco, Harry Halpin, Daniel M Herzig, Pe-ter Mika, Jeffrey Pound, Henry S Thompson, andT Tran Duc.
2011.
Entity search evaluation overstructured web data.
In Proceedings of the 1st inter-national workshop on entity-oriented search work-shop (SIGIR 2011), ACM, New York.Michael Bostock, Vadim Ogievetsky, and Jeffrey Heer.2011.
D3data-driven documents.
Visualizationand Computer Graphics, IEEE Transactions on,17(12):2301?2309.Penny Chase, Ray D?Amore, Nahum Gershon, RodHolland, Rob Hyland, Inderjeet Mani, Mark May-bury, Andy Merlino, and Jim Rayson.
1998.
Se-mantic visualization.
In ACL-COLING Workshopon Content Visualization and Intermedia Represen-tation.Jason Chuang, Daniel Ramage, Christopher Manning,and Jeffrey Heer.
2012.
Interpretation and trust:Designing model-driven visualizations for text anal-ysis.
In Proceedings of the SIGCHI Conference onHuman Factors in Computing Systems, pages 443?452.
ACM.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al.
2006.
Generat-ing typed dependency parses from phrase structureparses.
In Proceedings of LREC, volume 6, pages449?454.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 363?370.
Association for Computational Lin-guistics.Danyel Fisher, Aaron Hoff, George Robertson, andMatthew Hurst.
2008.
Narratives: A visualizationto track narrative events as they develop.
In VisualAnalytics Science and Technology, 2008.
VAST?08.IEEE Symposium on, pages 115?122.
IEEE.Ramanathan Guha, Rob McCool, and Eric Miller.2003.
Semantic search.
In Proceedings of the 12thinternational conference on World Wide Web, pages700?709.
ACM.Marti A Hearst and Chandu Karadi.
1997.
Cat-a-cone:an interactive interface for specifying searches andviewing retrieval results using a large category hi-erarchy.
In ACM SIGIR Forum, volume 31, pages246?255.
ACM.Marti A Hearst.
1995.
Tilebars: visualization of termdistribution information in full text information ac-cess.
In Proceedings of the SIGCHI conference onHuman factors in computing systems, pages 59?66.ACM Press/Addison-Wesley Publishing Co.Marti A Hearst.
1999a.
Untangling text data mining.In Proceedings of the 37th annual meeting of theAssociation for Computational Linguistics on Com-putational Linguistics, pages 3?10.
Association forComputational Linguistics.Marti A Hearst.
1999b.
The use of categories andclusters for organizing retrieval results.
In Natu-ral language information retrieval, pages 333?374.Springer.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve coref-erence resolution system at the conll-2011 sharedtask.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 28?34.
Association for ComputationalLinguistics.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolutionbased on entity-centric, precision-ranked rules.Shixia Liu, Michelle X Zhou, Shimei Pan, WeihongQian, Weijia Cai, and Xiaoxiao Lian.
2009.
Interac-tive, topic-based visual text summarization and anal-ysis.
In Proceedings of the 18th ACM conferenceon Information and knowledge management, pages543?552.
ACM.Christopher Manning and Dan Klein.
2003.
Opti-mization, maxent models, and conditional estima-tion without magic.
In Proceedings of the 2003 Con-ference of the North American Chapter of the As-sociation for Computational Linguistics on HumanLanguage Technology: Tutorials-Volume 5, pages8?8.
Association for Computational Linguistics.Daan Odijk, Ork de Rooij, Maria-Hendrike Peetz,Toine Pieters, Maarten de Rijke, and StephenSnelders.
2012.
Semantic document selection.In Theory and Practice of Digital Libraries, pages215?221.
Springer.Jeffrey Pound, Peter Mika, and Hugo Zaragoza.
2010.Ad-hoc object retrieval in the web of data.
In Pro-ceedings of the 19th international conference onWorld wide web, pages 771?780.
ACM.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nathanael Chambers, Mihai Surdeanu, DanJurafsky, and Christopher Manning.
2010.
A multi-pass sieve for coreference resolution.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 492?501.Association for Computational Linguistics.57Daniel Ramage, Evan Rosen, Jason Chuang, Christo-pher D Manning, and Daniel A McFarland.
2009.Topic modeling for the social sciences.
In NIPS2009 Workshop on Applications for Topic Models:Text and Beyond, volume 5.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013.
The life and death of dis-course entities: Identifying singleton mentions.
InProceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 627?633.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 1631?1642.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.
Association for Compu-tational Linguistics.Michelle Q Wang Baldonado, Allison Woodruff, andAllan Kuchinsky.
2000.
Guidelines for using multi-ple views in information visualization.
In Proceed-ings of the working conference on Advanced visualinterfaces, pages 110?119.
ACM.58
