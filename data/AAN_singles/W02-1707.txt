Towards a web-based centre on Swedish Language TechnologyPetter KARLSTR?MGraduate School of Language Technology,G?teborg UniversityBox 200405 30, G?teborg, Swedenpetter.karlstrom@gslt.hum.gu.seRobin COOPERGraduate School of Language Technology,G?teborg UniversityBox 200405 30, G?teborg, Swedenrobin.cooper@gslt.hum.gu.seAbstractWe present an upcoming web-based centre forinformation and documentation on languagetechnology (LT) in Sweden and/or in Swedish.Some ways to collect and represent data usingdata harvesting and XML are suggested.
Usersproviding data may choose one of four inputmodes, two of which will allow the user to editexisting documents at her own site.
Thus wehope to facilitate data entry and inter-sitecommunication, and reduce the amount of staledata.
The entry modes use annotated HTML andan emerging XML-application, respectively.In conclusion, we propose that an XML-application in the field of LT would facilitate forthe LT community to share information, and thatautomatic data harvesting will emerge as animportant technique for building informationcentres in the future.IntroductionThe website Slate is an information centre onlanguage technology (LT) developed in Swedenand/or for the Swedish language.
It is thetechnical/academic part of a collaborative effortbetween the National Graduate School ofLanguage Technology in Sweden (GSLT) andSvenska spr?kn?mnden (the official organizationfor matters concerning the Swedish language).Spr?kn?mnden will provide the second part,containing popular information.
Some materialfor the popular part will be taken over from theproject Svenska.se at the Swedish Institute forComputer Science, SICS.
The two parts will asfar as possible have the same structure andcross-reference each other.
We will inform onthe following matters, in both areas:?
Research projects.?
Industrial projects.?
Educational programmes.?
Software.?
LT developed in Sweden for otherlanguages than Swedish.?
Swedish organizations and individualswho have an interest in LT.?
Employment oppurtunities in the area ofLT.Information will be provided in both English andSwedish, depending on whether translateddocuments exist.We cooperate via NorDokNet with centra inDenmark, Finland, Iceland and Norway.
We alsohave contact with COLLATE at DFKI andfollow their way of organizing information onLT in LT-World.
Several more LanguageTechnologists and LT-projects will contributewith information, notably the Ph.
D. students inGSLT who have agreed to participate insoftware tests.1 RationaleSince the breakthrough of the Internet, and morespecifically the World Wide Web (WWW, web),an information centre is commonly viewed assome sort of web-based service.
In its mostsimple form, such a centre consists of acollection of links, manually updated by awebmaster or editor.
Large, more sophisticatedwebsites usually utilize some kind of databasebackend in order to separate content fromdisplay information, to facilitate searching andto provide simple, site-standardized means fordata entry and editing.One of the fundamental principles of the WWWis the idea of globally reachable documents, afact given in the acronym WWW itself.
Givenits global nature, the WWW naturally grows tocontain a very large amount of data.
In order toresolve the issue of actually finding anything inthis vast network, a need for search engines,portals, and information centres has arisen.
Ourwebsite is a response to such a need for aninformation centre on language technology inSweden.The success of an information centre largelydepend on the perceived quality of its contents,and how easily those contents are found.
Textualquality is not what we will discuss in this paper.Suffice to say that quality depends on, amongother things, information being correct, up todate, and comprehensive.
For our purposes, thisis a data entry/edit issue, which we will address.1.1 Data entryAs stated above, a common way to managewebsite is by means of a database that generatesHTML web-pages which a user may view in herweb browser.
The input to the database may beprovided by other users, sometimes by means ofa web-form (Fig 1).LanguageTechnologistWeb FormdatabaseWebsiteUser!InterestingWorkFig.
1.
An  information centre/portal Arrowssymbolize data-flowNow, the Language Technologist in this pictureis probably more interested in his InterestingWork than filling out a web-form (we hope!).
Hemay already have filled out several such forms,may not see the use of the website in question,or may have done so already and be unwilling toupdate old information.Therefore we suggest an alternative way toprovide information (Fig 2).
The alternativebuilds on the notion that people, projects,companies, etc.
already have or want to havewebsites or homepages of their own.
Most of theinformation is thus already available there.
Wewill try to harvest this information by asking thedata-providers to do one of four things:a) Annotate their own HTML websitewith tags for our data-harvester.b) Provide information in an XML-format, compatible with ours.c) Submit a web-form.d) Contact our web-editor for manualentry.The last two entry modes are a step back to oldermethods, and are provided as a cautionarymeasure for two reasons.
First, not all data-providers will adopt to the new way of data-entry.
A cause for not adopting is that our XML-application and attributed HTML-tags willprobably undergo several changes and demandthat early adopters change their documentsaccordingly.
The second reason is of pragmaticnature, namely that we need to build our sitebefore the data-harvesting application and theXML-application is finished.InterestingWeb page(AnnotatedHTML/XML)SlatebotLanguageTechnologistdatabaseUserAnnotated file(XML)WebsiteOther websiteUser!InterestingWorkFig 2.
An information centre utilizing dataharvesting.
Arrows symbolize data-flowThere are three benefits with entry modes a) andb):1.
Data entry is facilitated by making useof existing web-pages.2.
The amount of stale data is reduced byupdating the database as the data-provider updates her own website.3.
The ability to output in XML format.This will allow for other websites, forexample those in NorDokNet and LT-World to use their own harvesters onSlate?s material.2 Data harvestingWe will now describe what documents that willbe harvested should look like, and how ourharvester will work.
Understanding this part ofthe paper requires some knowledge of HTMLand XML2.1 Document structureIn order for our harvesting engine to retrieve andstore documents properly they need to conformto some standards.
Contributions may be inHTML or XML.
In the case of HTML thecontributing site will need to annotate existingdocuments with tags that tell us what is what.We have chosen to use the META and SPANtags since they may be used as containers forgeneric meta-information.
Use of the META-tagis uncontroversial, since it is commonly used forproviding data for search engines.
The SPANtag is more commonly used for styleinformation, but may be used to tailor HTML toones own needs and tastes, according to W3C(1999).
Some may still see the use of this tag ascontroversial, since we use it for semantic, notstyle information.
We argue that this is not anissue because the tags will not affect the websitein question (presuming the information wasalready there), whether users implement a style-sheet or not.
Alternatives might have beenHTML-comments or scripting languages.
Thedrawbacks with these methods are that they mayforce data-providers to use invalid HTML andthat they make it harder to make use of existinginformation.As for XML, documents will have to conform toa DTD under development by us in cooperationwith NorDokNet.
We also have contact with LT-World, in order to resolve compatibility issueswith their information centre before they appear.Since websites using XML is not yetcommonsight, documents will usually have to bewritten from scratch.
No web browsers currentlysupport XML satisfactorily, so the documentsare not yet of much use outside of Slate.Therefore, this method is aimed towards earlyadopters who see the future benefits of XMLand wish to participate in that development.2.2 ExamplesAn HTML file that we should retrieve and fileunder people should contain this META-tag:<meta http-equiv="slate"content="people">somewhere in its head.
The document body maycontain constructs like this:I am a  <span class="title">Ph.D.student</span> in computationallinguistics at <spanclass="affiliation">the Departmentof Computer and Information Scienceat Svedala University</span>.Note that this is a real-life example (somewhatanonymized and edited).
The Ph.
D. student?sexisting web-page was annotated with SPAN-tags attributed to classes in the Slatedatabase/XML-application.
If the Ph.
D. studentchanges affilitiation, he will probably want toupdate his own webpage.
The updatedinformation will be automatically retrieved bySlateBot.XML files are more rigid since they follow aDTD.
Their structure should not be surprising.Being XML-documents they contain adeclaration and a few containers:<?xml version="1.0" encoding="ISO-8859-1" standalone="no" ?><!DOCTYPE people SYSTEM"http://slate.gslt.hum.gu.se/people/people.dtd"><people><person><first_name>John</first_name><last_name>Doe</last_name>?</person></people>The XML-compliant reader may note that ourstructure does not adhere to any standards suchas RDF or OLAC.
This will be attended to inlater versions (see Discussion, below).2.3 Harvesting engineThe core parts of our data harvesting engine,SlateBot, have been implemented and alpha testshave commenced using a very small databaseand XML-application listing people.SlateBot is an application being developed inPerl, in order to acommodate for entry modes a)and b) above.
The application consists of fivemain parts, making use of corresponding Perl-modules:?
HTML head-parser?
HTML parser?
Link extractor?
XML parser?
Database InterfaceFor input, it takes a list of links of participatingsites.
The list is maintained by our webmaster.For each entry in the list, it makes a HTTP GETrequest, just like a regular web-browser.
It thenchecks whether the document in question iswritten in HTML or XML, and runs either theHTML parsers and the link extractor or theXML parser.
The information returned from theparsers is then translated into XML and updatedby the database interface.The reason for having three different HTMLparsers (including the link extractor) is, ofcourse, that they do different things.
The headparser only parses information in the HTML-HEAD part of the document, in order to checkwhether the document belongs in our database atall, and if so, in what main category.
The HTMLparser looks for SPAN tags, and returns thosethat correspond to our categories.
Finally, thelink extractor searches the document  for links toother documents.
This is provided for futuredevelopment, in case we need our harvester tofollow links in documents.The XML parser is simpler, because we canassume that the XML file conforms to our DTD.We simply parse the file to find out which of ourcategories are implemented in the document, andsend those to the database interface.2.4 TestsWe have made some preliminary tests of theengine.
Ph.
D. students from GSLT and a fewothers were asked to provide information onthemselves in either of three of the four entry-modes above.
Web-forms were left out of thefirst tests, because we needed to ascertain thatwe could delete the database withoutconsequenses.The Ph.
D. students at GSLT come from a widerange of backgrounds, so it was expected thatsome would be more interested in XML thanothers.
Three of the entries we received were inthe form of links to HTML-files and three werelinks to XML-files.
One reply was in the form ofan e-mail request for entry.
Naturally, theinformation base is too small to draw any realconclusions concerning user preferences, but itdoes seem that we are headed in the rightdirection in providing the different modes.The tests have helped us iron out a few of theunforeseen bugs that come with all softwaredevelopment, and we are now ready to enlargethe database model.3 Other related projects andstandardsWe are not alone in realizing the benefits withmeta-data.
In fact, there are quite a lot ofbuzzwords and hype surrounding XML-development.
These are some projects andstandards that we should keep in mind,particularly with respect to data-harvesting andfuture development.
Please note that this is in noway a complete list, due to the novelty of thefield.3.1 Semantic WebThe Semantic Web is an attempt to giveinformation on the web meaning, in order tofacilitate searching, automation, etc (W3C,2001).3.2 Resource Description FrameworkThe Resource Description Framework, RDF is alanguage for providing metadata to support theSemantic Web.
It can be used for describingresources that can be located via a URI or otheridentifier (W3C, 2001).
RDF is developed sideby side with the Dublin Core (below), and bothstandards may be used in one document.3.3 Dublin CoreThe Dublin Core Metadata Element Set  (DublinCore, DC) is, as its name implies, a set ofelements for metadata.
There are fifteenelements, strictly defined using a set of tenattributes (DCMI, 1999).
The elements?
mainuses are for information or service resources,e.g.
bibliographies and card catalogs.3.4 DocBookDocBook is an SGML or XML format fortechnical documentation.
It is intended forauthoring, and can be converted to other formatsfor reading (Harold and Means, 2001).3.5 Open Archives InitiativeThe Open Archives Initiative, OAI is anexperimental  init iat ive for efficientdissemination of content.
It uses the DublinCore.
Historically, the main intention of OAIwas providing a meta-data language for e-prints,but this has been expanded to related domains aswell.
There is an OAI protocol for data-harvesting.
Version 2.0 of that protocol isscheduled for release in June 2002 (OAI, 2001).3.6 OLACOLAC, the Open Language ArchivesCommuni ty  is a community who developmethods for digital archiving of languageresources and a network for housing andaccessing such services (OLAC, 2001).
Theyuse methods very similar to ours, though aimedat language in general rather than languagetechnology.
The Dublin Core forms the basis oftheir meta-data set.
Alpha tests havecommenced, and the project has recently beenlaunched in Europe (May 2002).3.7 Web services and SOAPWeb services is a way to share applicationmethods over the Internet, by means of somestandard interface such as Simple Object AccessProtocol, SOAP (W3C, 2000).
The methodsimplemented may for example provide access toa site?s data.4 Discussion and future developmentAs stated above, our tests were carried out withthe help of a rather small group of subjects, andusing a small database.
In order for us to expandthe database and the XML-application, we see aneed for a more standardized XML for LT. Astandard would be of use to all in thecommunity, since it could be used for anythingfrom printed matters (via translation to forexample LaTex) to web pages and informationcentres such as ours.
The standard would ideallybe developed as a collaborative effort in the LTcommunity, perhaps building on othersuccessful standards such as DocBook or DublinCore.
Coordinating something of that magnitudeis a rather large task, and not within the scope ofour project.
We will take the approach ofdeveloping some XML in NorDokNet, whilekeeping our eyes open towards the world, inparticular stay in contact with LT-World.
Innon-formal discussions with representativesfrom DFKI, there is a growing interest ininterchange of data and data-harvesting, and abelief that the work of OLAC could prove veryuseful.In order to reach data-providers unwilling orunable to edit XML files, an editing tool wouldprobably prove helpful.
We will not developsuch a tool in the immediate future, since otherparts of Slate are more prioritized.
With someluck, general XML-editing tools may appear onthe market or better still as open source projects.In addition, browser support for XML (withstyle sheets) would probably encourage users.However, our control of browser developmentis, of course, nonexistent.As seen above, there are other, larger projectsand standards we should keep in mind andprobably adapt to.
The XML given in theexamples should thus be treated as just examplesto demonstrate the capabilities of SlateBot.Emerging global standards will be incorporatedas consensus will dictate.ConclusionData harvesting seems to be a feasible way ofcollecting information to an information centre,technically speaking.
Our notion that differentdata-providers will make use of different entry-modes seems to be true but need to be testedmore.
We believe that XML together withharvesting engines and other access methods canand will change the way we view the web, andthat the LT community could and should takeadvantage of this.AcknowledgementsOur thanks go to all other participants inNorDokNet and the Ph.
D. students of GSLT forproviding feedback and testing.ReferencesNote: all URIs to web-pages were tested on01.07.2002.
The documents may have beenmoved or removed after this date.DCMI org., (1999)  Dublin Core Metadata ElementSet, Version 1.1: Reference Description, web-publication, http://dublincore.org/documents/dces/Harold, E. R. and Means, S. W. (2001)  XML In aNutshell, O?Reilly & AssociatesOAI org.
(2001)  OAI FAQ, web-publication,http://www.openarchives.org/documents/FAQ.htmlOLAC org.
(2001)  OLAC Metadata Set, w e b -publ ica t ion ,  h t t p : / / w w w .
l a n g u a g e -archives.org/OLAC/olacms.htmlW3C org.
(1999)  HTML 4.01 Specification, web-publication, http://www.w3c.org/TR/html401/W3C org.
(2000)  Simple Object Access Protocol( S O A P )  1 .
1 ,  w e b - p u b l i c a t i o n ,http://www.w3.org/TR/SOAP/W3C org.
(2001) Semantic Web Activity Statement,web-publication,http://www.w3.org/2001/sw/ActivityOther websites mentioned in the textLT-World, http://www.lt-world.org/NorDokNet, http://www.nordoknet.org/Slate, http://slate.gslt.hum.gu.se/
