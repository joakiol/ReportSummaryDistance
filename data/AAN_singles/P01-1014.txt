Towards Automatic Classification of Discourse Elements in EssaysJill BursteinETS TechnologiesMS 18EPrinceton, NJ 08541USAJburstein@etstechnologies.comDaniel MarcuISI/USC4676 AdmiraltyWayMarina del Rey,CA, USAMarcu@isi.eduSlava AndreyevETS TechnologiesMS 18EPrinceton, NJ 08541USAsandreyev@etstechnologies.comMartin ChodorowHunter College, TheCity University ofNew YorkNew York, NY USAMartin.chodorow@hunter.cuny.eduAbstractEducators are interested in essayevaluation systems that includefeedback about writing features thatcan facilitate the essay revisionprocess.
For instance, if the thesisstatement of a student?s essay could beautomatically identified, the studentcould then use this information toreflect on the thesis statement withregard to its quality, and its relationshipto other discourse elements in theessay.
Using a relatively small corpusof manually annotated data, we useBayesian classification to identifythesis statements.
This method yieldsresults that are much closer to humanperformance than the results producedby two baseline systems.1 IntroductionAutomated essay scoring technology canachieve agreement with a single human judgethat is comparable to agreement between twosingle human judges (Burstein, et al1998; Foltz,et al1998; Larkey, 1998; and Page andPeterson, 1995).
Unfortunately, providingstudents with just a score (grade) is insufficientfor instruction.
To help students improve theirwriting skills, writing evaluation systems needto provide feedback that is specific to eachindividual?s writing and that is applicable toessay revision.The factors that contribute to improvementof student writing include refined sentencestructure, variety of appropriate word usage, andorganizational structure.
The improvement oforganizational structure is believed to be criticalin the essay revision process toward overallimprovement of essay quality.
Therefore, itwould be desirable to have a system that couldindicate as feedback to students, the discourseelements in their essays.
Such a system couldpresent to students a guided list of questions toconsider about the quality of the discourse.For instance, it has been suggested by writingexperts that if the thesis statement1 of a student?sessay could be automatically provided, thestudent could then use this information to reflecton the thesis statement and its quality.
Inaddition, such an instructional application couldutilize the thesis statement to discuss other typesof discourse elements in the essay, such as therelationship between the thesis statement and theconclusion, and the connection between thethesis statement and the main points in theessay.
In the teaching of writing, in order tofacilitate the revision process, students are oftenpresented with ?Revision Checklists.?
A revisionchecklist is a list of questions posed to thestudent to help the student reflect on the qualityof his or her writing.
Such a list might posequestions such as:a) Is the intention of my thesis statementclear?1A thesis statement is generally defined as thesentence that explicitly identifies the purpose of thepaper or previews its main ideas.
See the LiteracyEducation On-line (LEO) site athttp://leo.stcloudstate.edu.
(Annotator 1) ?In my opinion student should do what they want to do because they feel everythingand they can't have anythig they feel because they probably feel to do just because other people do it not theywant it.
(Annotator 2) I think doing what students want is good for them.
I sure they want to achieve in thehighest place but most of the student give up.
They they don?t get what they want.
To get what they want, theyhave to be so strong and take the lesson from their parents Even take a risk, go to the library, and study hard bydoing different thing.Some student they do not get what they want because of their family.
Their family might be carelessabout their children so this kind of student who does not get support, loving from their family might not getwhat he wants.
He just going to do what he feels right away.So student need a support from their family they has to learn from them and from their background.
Ilearn from my background I will be the first generation who is going to gradguate from university that is what Iwant.
?Figure 1: Sample student essay with human annotations of thesis statements.b) Does my thesis statement responddirectly to the essay question?c) Are the main points in my essayclearly stated?d) Do the main points in my essay relateto my original thesis statement?If these questions are expressed in generalterms, they are of little help; to be useful, theyneed to be grounded and need to referexplicitly to the essays students write(Scardamalia and Bereiter, 1985; White 1994).The ability to automatically identify andpresent to students the discourse elements intheir essays can help them focus and reflect onthe critical discourse structure of the essays.In addition, the ability for the application toindicate to the student that a discourse elementcould not be located, perhaps due to the ?lackof clarity?
of this element, could also behelpful.
Assuming that such a capability wasreliable, this would force the writer to thinkabout the clarity of an intended discourseelement, such as a thesis statement.Using a relatively small corpus of essaydata where thesis statements have beenmanually annotated, we built a Bayesianclassifier using the following features:sentence position; words commonly used inthesis statements; and discourse features,based on Rhetorical Structure Theory (RST)parses (Mann and Thompson, 1988 andMarcu, 2000).
Our results indicate that thisclassification technique may be used towardautomatic identification of thesis statements inessays.
Furthermore, we show that thismethod generalizes across essay topics.2 What Are Thesis Statements?A thesis statement is defined as the sentence thatexplicitly identifies the purpose of the paper orpreviews its main ideas (see footnote 1).
Thisdefinition seems straightforward enough, andwould lead one to believe that even for people toidentify the thesis statement in an essay would beclear-cut.
However, the essay in Figure 1 is acommon example of the kind of first-draft writingthat our system has to handle.
Figure 1 shows astudent response to the essay question:Often in life we experience a conflict inchoosing between something we "want" to doand something we feel we "should" do.
In youropinion, are there any circumstances in whichit is better for people to do what they  "want" todo rather than what they feel they "should" do?Support your position with evidence from yourown experience or your observations of otherpeople.The writing in Figure 1 illustrates one kind ofchallenge in automatic identification of discourseelements, such as thesis statements.
In this case,the two human annotators independently chosedifferent text as the thesis statement (the two textshighlighted in bold and italics in Figure 1).
In thiskind of first-draft writing, it is not uncommon forwriters to repeat ideas, or express more than onegeneral opinion about the topic, resulting in textthat seems to contain multiple thesis statements.Before building a system that automaticallyidentifies thesis statements in essays, we wanted todetermine whether the task was well-defined.
Incollaboration with two writing experts, a simplediscourse-based annotation protocol wasdeveloped to manually annotate discourseelements in essays for a single essay topic.This was the initial attempt to annotate essaydata using discourse elements generallyassociated with essay structure, such as thesisstatement, concluding statement, and topicsentences of the essay?s main ideas.
Thewriting experts defined the characteristics ofthe discourse labels.
These experts thenannotated 100 essay responses to one EnglishProficiency Test (EPT) question, called TopicB, using a PC-based interface implemented inJava.We computed the agreement between thetwo human annotators using the kappacoefficient (Siegel and Castellan, 1988), astatistic used extensively in previous empiricalstudies of discourse.
The kappa statisticmeasures pairwise agreement among a set ofcoders who make categorial judgments,correcting for chance expected agreement.The kappa agreement between the twoannotators with respect to the thesis statementlabels was 0.733 (N=2391, where 2391represents the total number of sentencesacross all annotated essay responses).
Thisshows high agreement based on research incontent analysis (Krippendorff, 1980) thatsuggests that values of kappa higher than 0.8reflect very high agreement and values higherthan 0.6 reflect good agreement.
Thecorresponding z statistic was 27.1, whichreflects a confidence level that is much higherthan 0.01, for which the corresponding z valueis 2.32 (Siegel and Castellan, 1988).In the early stages of our project, it wassuggested to us that thesis statements reflectthe most important sentences in essays.
Interms of summarization, these sentenceswould represent indicative, generic summaries(Mani and Maybury, 1999; Marcu, 2000).
Totest this hypothesis (and estimate the adequacyof using summarization technology foridentifying thesis statements), we carried outan additional experiment.
The sameannotation tool was used with two differenthuman judges, who were asked this time toidentify the most important sentence of eachessay.
The agreement between human judgeson the task of identifying summary sentenceswas significantly lower: the kappa was 0.603(N=2391).
Tables 1a and 1b summarize the resultsof the annotation experiments.Table 1a shows the degree of agreementbetween human judges on the task of identifyingthesis statements and generic summary sentences.The agreement figures are given using the kappastatistic and the relative precision (P), recall (R),and F-values (F), which reflect the ability of onejudge to identify the sentences labeled as thesisstatements or summary sentences by the otherjudge.
The results in Table 1a show that the task ofthesis statement identification is much betterdefined than the task of identifying importantsummary sentences.
In addition, Table 1b indicatesthat there is very little overlap between thesis andgeneric summary sentences: just 6% of thesummary sentences were labeled by human judgesas thesis statement sentences.
This stronglysuggests that there are critical differences betweenthesis statements and summary sentences, at leastin first-draft essay writing.
It is possible that thesisstatements reflect an intentional facet (Grosz andSidner, 1986) of language, while summarysentences reflect a semantic one (Martin, 1992).More detailed experiments need to be carried outthough before proper conclusions can be derived.Table 1a: Agreement between human judges onthesis and summary sentence identification.Metric ThesisStatementsSummarySentencesKappa 0.733 0.603P (1 vs. 2) 0.73 0.44R (1 vs. 2) 0.69 0.60F (1 vs. 2) 0.71 0.51Table 1b: Percent overlap between human labeledthesis statements and summary sentences.Thesis statements  vs.Summary sentencesPercent Overlap 0.06The results in Table 1a provide an estimate foran upper bound of a thesis statement identificationalgorithm.
If one can build an automatic classifierthat identifies thesis statements at recall andprecision levels as high as 70%, the performanceof such a classifier will be indistinguishable fromthe performance of humans.3 A Bayesian Classifier forIdentifying Thesis Statements3.1 Description of the ApproachWe initially built a Bayesian classifier forthesis statements using essay responses to oneEnglish Proficiency Test (EPT) test question:Topic B.McCallum and Nigam (1998) discuss twoprobabilistic models for text classification thatcan be used to train Bayesian independenceclassifiers.
They describe the multinominalmodel as being the more traditional approachfor statistical language modeling (especially inspeech recognition applications), where adocument is represented by a set of wordoccurrences, and where probability estimatesreflect the number of word occurrences in adocument.
In using the alternative,multivariate Bernoulli model, a document isrepresented by both the absence and presenceof features.
On a text classification task,McCallum and Nigam (1998) show that themultivariate Bernoulli model performs wellwith small vocabularies, as opposed to themultinominal model which performs betterwhen larger vocabularies are involved.Larkey (1998) uses the multivariate Bernoulliapproach for an essay scoring task, and herresults are consistent with the results ofMcCallum and Nigam (1998) (see also Larkeyand Croft (1996) for descriptions of additionalapplications).
In Larkey (1998), sets of essaysused for training scoring models typicallycontain fewer than 300 documents.Furthermore, the vocabulary used across thesedocuments tends to be restricted.Based on the success of Larkey?sexperiments, and McCallum and Nigam?sfindings that the multivariate Bernoulli modelperforms better on texts with smallvocabularies, this approach would seem to bethe likely choice when dealing with data setsof essay responses.
Therefore, we haveadopted this approach in order to build a thesisstatement classifier that can select from anessay the sentence that is the most likelycandidate to be labeled as thesis statement.22In our research, we trained classifiers using aclassical Bayes approach too, where two classifierswere built: a thesis classifier and a non-thesisIn our experiments, we used three generalfeature types to build the classifier: sentenceposition; words commonly occurring in thesisstatements; and RST labels from outputs generatedby an existing rhetorical structure parser (Marcu,2000).We trained the classifier to predict thesisstatements in an essay.
Using the multivariateBernoulli formula, below, this gives us the logprobability that a sentence (S) in an essay belongsto the class (T) of sentences that are thesisstatements.
We found that it helped performanceto use a Laplace estimator to deal with cases wherethe probability estimates were equal to zero.i ii iilog(P(T | S)) =log(P(T)) +log(P(A | T) /P(A)),log(P(A | T) /P(A )),iiif S contains Aif S does not contain A?????
?In this formula, P(T) is the prior probability that asentence is in class T, P(Ai|T) is the conditionalprobability of a sentence having feature Ai , giventhat the sentence is in T, and P(Ai) is the priorprobability that a sentence contains feature Ai,P( iA |T) is the conditional probability that asentence does not have feature Ai, given that it isin T, and P( iA ) is the prior probability that asentence does not contain feature Ai.3.2 Features Used to Classify ThesisStatements3.2.1 Positional FeatureWe found that the likelihood of a thesis statementoccurring at the beginning of essays was quite highin the human annotated data.
To account for this,we used one feature that reflected the position ofeach sentence in an essay.classifier.
In the classical Bayes implementation, eachclassifier was trained only on positive feature evidence,in contrast to the multivariate Bernoulli approach thattrains classifiers both on the absence and presence offeatures.
Since the performance of the classical Bayesclassifiers was lower than the performance of theBernoulli classifier, we report here only theperformance of the latter.3.2.2 Lexical FeaturesAll words from human annotated thesisstatements were used to build the Bayesianclassifier.
We will refer to these words as thethesis word list.
From the training data, avocabulary list was created that included oneoccurrence of each word used in all resolvedhuman annotations of thesis statements.
Allwords in this list were used as independentlexical features.
We found that the use ofvarious lists of stop words decreased theperformance of our classifier, so we did notuse them.3.2.3 Rhetorical Structure TheoryFeaturesAccording to RST (Mann and Thompson,1988), one can associate a rhetorical structuretree to any text.
The leaves of the treecorrespond to elementary discourse units andthe internal nodes correspond to contiguoustext spans.
Each node in a tree is characterizedby a status (nucleus or satellite) and arhetorical relation, which is a relation thatholds between two non-overlapping textspans.
The distinction between nuclei andsatellites comes from the empiricalobservation that the nucleus expresses what ismore essential to the writer?s intention than thesatellite; and that the nucleus of a rhetoricalrelation is comprehensible independent of thesatellite, but not vice versa.
When spans areequally important, the relation is multinuclear.Rhetorical relations reflect semantic,intentional, and textual relations that holdbetween text spans as is illustrated in Figure 2.For example, one text span may elaborate onanother text span; the information in two textspans may be in contrast; and the informationin one text span may provide background forthe information presented in another text span.Figure 2 displays in the style of Mann andThompson (1988) the rhetorical structure treeof a text fragment.
In Figure 2, nuclei arerepresented using straight lines; satellitesusing arcs.
Internal nodes are labeled withrhetorical relation names.We built RST trees automatically for eachessay using the cue-phrase-based discourse parserof Marcu (2000).
We then associated with eachsentence in an essay a feature that reflected thestatus of its parent node (nucleus or satellite), andanother feature that reflected its rhetorical relation.For example, for the last sentence in Figure 2 weassociated the status satellite and the relationelaboration because that sentence is the satelliteof an elaboration relation.
For sentence 2, weassociated the status nucleus and the relationelaboration because that sentence is the nucleusof an elaboration relation.We found that some rhetorical relationsoccurred more frequently in sentences annotated asthesis statements.
Therefore, the conditionalprobabilities for such relations were higher andprovided evidence that certain sentences werethesis statements.
The Contrast relation shown inFigure 2, for example, was a rhetorical relationthat occurred more often in thesis statements.Arguably, there may be some overlap betweenwords in thesis statements, and rhetorical relationsused to build the classifier.
The RST relations,however, capture long distance relations betweentext spans, which are not accounted by the wordsin our thesis word list.3.3 Evaluation of the Bayesian classifierWe estimated the performance of our system usinga six-fold cross validation procedure.
Wepartitioned the 93 essays that were labeled by bothhuman annotators with a thesis statement into sixgroups.
(The judges agreed that 7 of the 100 essaysthey annotated had no thesis statement.)
Wetrained six times on 5/6 of the labeled data andevaluated the performance on the other 1/6 of thedata.The evaluation results in Table 2 show the averageperformance of our classifier with respect to theresolved annotation (Alg.
wrt.
Resolved), usingtraditional recall (R), precision (P), and F-value (F)metrics.
For purposes of comparison, Table 2 alsoshows the performance of two baselines: therandom baseline    classifies    the     thesisstatementsFigure 2:  Example of RST tree.randomly; while the position baseline assumesthat the thesis statement is given by the firstsentence in each essay.Table 2: Performance of the thesis statementclassifier.System vs. system P R FRandom baselinewrt.
Resolved0.06 0.05 0.06Position baseline wrt.Resolved0.26 0.22 0.24Alg.
wrt.
Resolved 0.55 0.46 0.501 wrt.
2 0.73 0.69 0.711 wrt.
Resolved 0.77 0.78 0.782 wrt.
Resolved 0.68 0.74 0.714 Generality of the Thesis StatementIdentifierIn commercial settings, it is crucial that aclassifier such as the one discussed in Section 3generalizes across different test questions.
Newtest questions are introduced on a regular basis;so it is important that a classifier that works wellfor a given data set works well for other datasets as well, without requiring additionalannotations and training.For the thesis statement classifier it wasimportant to determine whether the positional,lexical, and RST-specific features are topicindependent, and thus generalizable to new testquestions.
If so, this would indicate that wecould annotate thesis statements across a numberof topics, and re-use the algorithm on additionaltopics, without further annotation.
We asked awriting expert to manually annotate the thesisstatement in approximately 45 essays for 4additional test questions: Topics A, C, D and E.The annotator completed this task using thesame interface that was used by the twoannotators in Experiment 1.To test generalizability for each of the fiveEPT questions, the thesis sentences selected by awriting expert were used for building theclassifier.
Five combinations of 4 prompts wereused to build the classifier in each case, and theresulting classifier was then cross-validated onthe fifth topic, which was treated as test data.To evaluate the performance of each of theclassifiers, agreement was calculated for each?cross-validation?
sample (single topic) bycomparing the algorithm selection to our writingexpert?s thesis statement selections.
Forexample, we trained on Topics A, C, D, and E,using the thesis statements selected manually.This classifier was then used to select,automatically, thesis statements for Topic B.  Inthe evaluation, the algorithm?s selection wascompared to the manually selected set of thesisstatements for Topic B, and agreement wascalculated.
Table 3 illustrates that in all but onecase, agreement exceeds both baselines fromTable 2.
In this set of manual annotations, thehuman judge almost always selected onesentence as the thesis statement.
This is whyPrecision, Recall, and the F-value are oftenequal in Table 3.Table 3: Cross-topic generalizability of the thesisstatement classifier.TrainingTopicsCV Topic P R  FABCD   E 0.36 0.36 0.36ABCE   D 0.49 0.49 0.49ABDE   C 0.45 0.45 0.45ACDE   B 0.60 0.59 0.59BCDE   A 0.25 0.24 0.25Mean  0.43 0.43 0.435 Discussion and ConclusionsThe results of our experimental work indicatethat the task of identifying thesis statements inessays is well defined.
The empirical evaluationof our algorithm indicates that with a relativelysmall corpus of manually annotated essay data,one can build a Bayes classifier that identifiesthesis statements with good accuracy.
Theevaluations also provide evidence that thismethod for automated thesis selection in essaysis generalizable.
That is, once trained on a fewhuman annotated prompts, it can be applied toother prompts given a similar population ofwriters, in this case, writers at the collegefreshman level.
The larger implication is thatwe begin to see that there are underlyingdiscourse elements in essays that can beidentified, independent of the topic of the testquestion.
For essay evaluation applications thisis critical since new test questions arecontinuously being introduced into on-line essayevaluation applications.Our results compare favorably with resultsreported by Teufel and Moens (1999) who alsouse Bayes classification techniques to identifyrhetorical arguments such as aim andbackground in scientific texts, although the textswe are working with are extremely noisy.Because EPT essays are often produced forhigh-stake exams, under severe time constraints,they are often ungrammatical, repetitive, andpoorly organized at the discourse level.Current investigations indicate that thistechnique can be used to reliably identify otheressay-specific discourse elements, such as,concluding statements, main points ofarguments, and supporting ideas.
In addition,we are exploring how we can use estimatedprobabilities as confidence measures of thedecisions made by the system.
If the confidencelevel associated with the identification of athesis statement is low, the system wouldinstruct the student that no explicit thesisstatement has been found in the essay.AcknowledgementsWe would like to thank our annotationexperts, Marisa Farnum, Hilary Persky, ToddFarley, and Andrea King.ReferencesBurstein, J., Kukich, K. Wolff, S. Lu, C.Chodorow, M, Braden-Harder, L. and HarrisM.D.
(1998).
Automated Scoring Using AHybrid Feature Identification Technique.Proceedings of ACL, 206-210.Foltz, P. W., Kintsch, W., and Landauer, T..(1998).
The Measurement of Textual Coherencewith Latent Semantic Analysis.
DiscourseProcesses, 25(2&3), 285-307.Grosz B. and Sidner, C. (1986).
Attention,Intention, and the Structure of Discourse.Computational Linguistics, 12 (3), 175-204.Krippendorff K. (1980).
Content Analysis:An Introduction to Its Methodology.
Sage Publ.Larkey, L. and Croft, W. B.
(1996).Combining Classifiers in Text Categorization.Proceedings of  SIGIR,  289-298.Larkey, L. (1998).
Automatic Essay GradingUsing Text Categorization Techniques.Proceedings of SIGIR, pages 90-95.Mani, I. and Maybury, M. (1999).
Advancesin Automatic Text Summarization.
The MITPress.Mann, W.C. and Thompson, S.A.(1988).Rhetorical Structure Theory: Toward aFunctional Theory of Text Organization.
Text8(3), 243?281.Martin, J.
(1992).
English Text.
System andStructure.
John Benjamin Publishers.Marcu, D. (2000).
The Theory and Practiceof Discourse Parsing and Summarization.
TheMIT Press.McCallum, A. and Nigam, K. (1998).
AComparison of Event Models for Naive BayesText Classification.
The AAAI-98 Workshop on"Learning for Text Categorization".Page, E.B.
and Peterson, N. (1995).
Thecomputer moves into essay grading: updatingthe ancient test.
Phi Delta Kappa, March, 561-565.Scardamalia, M. and Bereiter, C. (1985).Development of Dialectical Processes inComposition.
In Olson, D. R., Torrance, N. andHildyard, A.
(eds), Literacy, Language, andLearning: The nature of consequences ofreading and writing.
Cambridge UniversityPress.Siegel S. and Castellan, N.J. (1988).Nonparametric Statistics for the BehavioralSciences.
McGraw-Hill.Teufel , S. and Moens, M. (1999).
Discourse-level argumentation in scientific articles.Proceedings of the ACL99 Workshop onStandards and Tools for Discourse Tagging.White E.M. (1994).
Teaching and AssessingWriting.
Jossey-Bass Publishers, 103-108.
