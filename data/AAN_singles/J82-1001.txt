Phrase Structure Trees Bear More  Fruitthan You Wou ld  Have Thought  1Aravind K. JoshiDepartment of Computer and Information ScienceThe Moore School/D2University of PennsylvaniaPhiladelphia, PA 19104Leon S. LevyBell Telephone LaboratoriesWhippany, NJ 07981In this paper we will present several results concerning phrase structure trees.
Theseresults show that phrase structure trees, when viewed in certain ways, have much moredescriptive power than one would have thought.
We have given a brief account of localconstraints on structural descriptions and an intuitive proof of a theorem about localconstraints.
We have compared the local constraints approach to some aspects of Gazdar'sframework and that of Peters and Ritchie and of Karttunen.
We have also presented someresults on skeletons (phrase structure trees without labels) which show that phrase structuretrees, even when deprived of the labels, retain in a certain sense all the structural informa-tion.
This result has implications for grammatical inference procedures.1.
IntroductionThere is renewed interest in examining the descrip-tive as well as generative power of phrase structuregrammars.
The primary motivation for this interesthas come from the recent investigations in alternativesto transformational grammars (e.g., Bresnan 1978;Kaplan and Bresnan 1979; Gazdar 1978, 1979a,1979b; Peters 1980; Kart tunen 1980).
2 Some ofthese approaches require amendments to phrase struc-ture grammars (especially Gazdar 1978, 1979a,1979b; Peters 1980; Karttunen 1980) that increasetheir descriptive power without increasing their gener-ative power.
Gazdar wants to restrict the power tothat of context-free languages.
Others are not com-l This work was partially supported by NSF grant MCS 79-08401 and MCS 81-07290.This paper is a revised and expanded version of a paper pres-ented at the 18th Annual  Meeting of the Association for Computa-tion Linguistics, University of Pennsylvania,  Philadelphia, June1980.Thanks are extended to the two referees of this paper and toMichael McCord for their valuable comments,  which helped in theimprovement of both the content and the presentation of the mate-rial herein.pletely precise on this aspect.
Berwick has shown thatthe Kaplan-Bresnan system is nearly equivalent o the2 Since this paper was submitted for publication, a number ofpapers have appeared that should be of interest to its readers.
"Phrase Linking Grammars"  by S. Peters and R.W.
Ritchie de-scribes their system (Technical Report, Department of Linguistics,University of Texas at Austin, 1982).
Strong adequacy of context-free grammars has been discussed by J. Bresnan, R.M.
Kaplan, S.Peters, and A. Zaenan in "Cross-Serial  Dependencies in Dutch"  (toappear in Linguistic Inquiry in 1982).
This paper shows thatcontext- free grammars are not strongly adequate (i.e., they areunable to provide the appropriate structural descriptions) to charac-terize cross-serial dependencies in Dutch.
In a recent paper ( "Howmuch context-sensit ivity is needed to provide reasonable structuraldescriptions: A tree adjoining system for generating phrase struc-ture trees," presented at the Parsing Workshop, Ohio State Univer-sity, May 1982; also Technical Report, Department of Computerand Information Science, University of Pennsylvania,  1982), A.Joshi discusses weak and strong adequacy of grammars and propos-es a tree adjoining grammar (TAG)  that appears to be stronglyadequate and has only slightly more power than context-free gram-mars.
Joshi has also given a rough characterization of a class ofcontext-sensit ive language (MCSL) that appears to be suitable tocharacterize natural languages.
Languages of TAG belong toMCSL, and languages of PLG's  of Peters and Ritchie also belong tothis class.
(TAG's  use a l inking device similar to that in thePLG's.
)Copyright 1982 by the Association for Computational  Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires a fee and/or  specific permission.0362-613X/82/010001-11501.00American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 1Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruitso-called indexed grammars.
The power of the phraselinking grammars of Peters and Ritchie is not com-pletely known at this time.The notion of node admissibility plays an importantrole in these formulations.
The earliest reference tonode admissibility appears in Chomsky 1965 (p.215); he suggests the possibility of constructing a rew-riting system where the rewriting of a symbol is deter-mined not only by the symbol being rewritten but alsoby the dominating category symbol.
In his analysis ofthe base component of a transformation grammar,McCawley 1968 suggested that the appropriate role ofcontext-sensitive rules in the base component of atransformational grammar can be viewed as node ad-missibility conditions on the base trees.
The basecomponent is thus a set of labeled trees satisfyingcertain conditions.
Peters and Ritchie 1969 made thisnotion precise and proved an important result, whichroughly states that the weak generative power of acontext-sensitive grammar is that of a context-freegrammar, if the rules are used as node admissibilityconditions.
Later Joshi and Levy 1977 made a sub-stantial extension of this result and showed that, if thenode admissibility conditions include Boolean combi-nations of proper analysis predicates and dominationpredicates, the weak generative capacity is still that ofa context-free grammar.Besides the notion of node admissibility, Gazdarintroduces two other notions in his framework(Generalized Phrase Structure Grammars, GPSG).These are (1) categories with holes and an associatedset of derived rules and linking rules, and (2) meta-rules for deriving rules from one another.
The cate-gories with holes and the associated rules do not in-crease the weak generative power beyond that ofcontext-free grammars.
The metarules, unless con-strained in some fashion, will increase the generativepower, because, for example, a metarule can generatean infinite set of context-free rules that can generate astrictly context-sensitive language.
(The language{anbncn/n>l} can be generated in this way.)
Themetarules in the actual grammars written in the GPSGframework so far are constrained enough so that theydo not increase the generative power.Besides node admissibility conditions, Peters 1980introduces a device for "linking" nodes (see also Kart-tunen 1980).
A lower node can be "linked" to a nodehigher in the tree and becomes "visible" while thesemantic interpretation is carried out at the lowernode.
The idea here is to let the context-free grammarovergenerate and the semantic interpretation weed outill-formed structures.
Karttunen 1980 has developed aparser using this idea.Kaplan and Bresnan 1979 have proposed an inter-mediate level of representation called functional struc-ture.
This level serves to filter structures generated bya phrase structure grammar.
Categories with holes arenot used in their framework.
In this paper we will notbe concerned with the Kaplan-Bresnan system.In Section 2 we briefly review Gazdar's proposal,especially his notion of categories with holes.
We givea short historical review of this notion.In Section 3 we briefly describe our work on localconstraints on structural descriptions (Joshi and Levy1977; Joshi, Levy, and Yueh 1980).
We give an intui-tive explanation of these results.In Section 4 we propose some extensions of ourresults and discuss them in the context of some longdistance rules.
We also describe Peters's 1980 ap-proach and present some suggestions for "context-sensitive" compositional semantics.In Section 5 we briefly present the framework ofPeters and Karttunen and compare it with that of Gaz-dar and of ourselves.In Section 6 we briefly discuss our results concern-ing a characterization of structural descriptions entire-ly in terms of trees without labels.2.
Gazdar ' s  Formulat ionGazdar 1979 has introduced categories with holesand some associated rules in order to allow for thebase generation of "unbounded" dependencies.
LetV N be the set of basic nonterminal symbols.
Then wedefine a set D(V N) of derived nonterminal symbols asfollows.D(VN) = {a /~ I ct, fl e V N }For example, if S and NP are the only two nonter-minal symbols, then D(V N) would consist of S /S ,S /NP,  NP/NP, and NP/S.
The intended interpreta-tion of a derived category (slashed category or a cate-gory with a hole) is as follows: A node labeled a/18will dominate subtrees identical to those that can bedominated by a, except that somewhere in every sub-tree of the a/18 type there will occur a node of theform ~/fl dominating a resumptive pronoun, a trace, orthe empty string, and every node linking a/fl and /3//3will be of the form a/t9.
Thus a/18 labels a node oftype a that dominates material containing a hole of thetype/3 (i.e., an extraction site in a movement analysis).For example, S/N P is a sentence that has an N P miss-ing somewhere.
The derived rules allow the propaga-tion of a hole and the linking rules allow the introduc-tion of a category with a hole.
For example, given therule (1)\ [ s  NPVP\ ]  3 (1)3 This is the same as the ruleS-,,.
NPVPbut written as a node admissibility condition.2 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruitwe will get two derived rules (2) and (3)\[S/NP NP/NP VP\] (2)\[S/NP NP VP/NP\] (3)An example of a linking rule is a rule (rule schema)that introduces a category with a hole as needed fortopicalization.\[s ~ S/a\] (4)For a = P P this becomes\[s PP S/PP \] (5)This rule will induce a structure like (6).
The tech-nique of categories with holes and the associated e-rived and linking rules allows unbounded ependenciesto be accounted for in the phrase structure representa-tion; however, this is accomplished at the expense ofproliferation of categories of the type a/\[3 (see alsoKarttunen 1980).
Later, in Section 3, we will presentan alternate way of representing (6) by means of localconstraints and some of their generalizations.JPPP NPI Ito BillSS/PPNP VPlPPI / \Mary V NPI IgavePP/PPIa book ?
(6)The notion of categories with holes is not com-pletely new.
In his 'String Analysis of LanguageStructure', Harris 1956, 1962 introduces categoriessuch as S - NP or S Np (like S /NP  of Gazdar) toaccount for moved constituents.
He does not howeverseem to provide, at least not explicitly, machinery forcarrying the "hole" downwards.
He also has rules inhis f ramework for introducing categories with holes.Thus, in his framework, something like (6) would beaccomplished by allowing for a sentence form (a cen-ter string) of the form (7) (not entirely his notation).NP V ?-NP (7)?
= Object or Complement of VSager, who has constructed a very substantial parserstarting from some of these ideas and extending themsignificantly, has allowed for the propagation of the'hole' resulting in structures very similar to those ofGazdar.
She has also used the notion of categorieswith holes in order to carry out some coordinate struc-ture computation.
For example, Sager allows for thecoordination of S/a and S/a but not S and S/a.
(SeeSager 1967 for an early reference to her work.
)Gazdar  is the first, however, to incorporate thenotion of categories with holes and the associatedrules in a formal framework for his syntactical theoryand also to exploit it in a systematic manner for ex-plaining coordinate structure phenomena.3.
Local Const ra in tsIn this section we briefly review our work on localconstraints.
Although this work has already appeared(Joshi and Levy 1977, Joshi, Levy, and Yueh 1980)and attracted some attention recently, the demonstra-tion of our results has remained somewhat inaccessibleto many due to the technicalities of the tree automatatheory.
In this paper we present an intuitive accountof these results in terms of interacting finite state ma-chines.The method of local constraints is an attempt todescribe context-free languages in an apparentlycontext-sensitive form that helps to retain the intuitiveinsights about the grammatical structure.
This form ofdescription, while apparently context-sensit ive, is infact context-free.3.1 Def in i t ion  of Local Const ra in tsContext-sensit ive grammars, in general, are morepowerful (with respect to weak generative capacity)than context-free grammars.
A fascinating result ofPeters and Ritchie 1969 is that if a context-sensitivegrammar G is used for "analysis" then the language"analyzed" by G is context-free.
First, we describewhat we mean by the use of a context-sensitive gram-mar G for "analysis".
Given a tree t, we define theset of proper analyses of t. Roughly speaking, a properanalysis of a tree is a slice across the tree.
More pre-cisely, the following recursive definition applies:American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 3Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More  FruitDefinition 3.1.
The set of proper analyses of a tree t,denoted Pt, is defined as follows.
(i) If t = q~ (the empty tree), thenPt = Cp.
( i i )  I f  t =Athen Pt = {A} u P(to).P(t 1) .
.
.
.
.
P(t n)where to, t 1 .... t n are trees, and '.'
denotes conca-tenation (of sets).Example  3.1p11rA~#2 (pl ,P2e V*).The contextual condition associated with such a"vertical" proper analysi~ is called a dominationpredicate.The general form of a local constraint combines theproper analysis and domination predicates as follows:Definition 3.2.
A local constraint rule is a rule of theformA -* u /C  Awhere C A is a Boolean combination of proper analysisand domination predicates.In transformational linguistics the context-sensitiveand domination predicates are used to describe condi-tions on transformations; hence we have referred tothese local constraints elsewhere as local transforma-tions.S/ \A B'= / \  IC d EC ept = {S, AB, AE, Ae, CdB, CdE, Cde, cdB, cdE,cde}.Let G be a context-sensitive grammar; i.e., its rulesare of the formA -" ~/~r__~where A cV - E (V is the alphabet and E is the set ofterminal symbols), w e V + (set of non-null strings onV) and ~r, 4~ E V* (set of all strings onV) .
If ~r andare both null, then the rule is a context-free rule.
Atree t is said to be "analyzable" with respect to G iffor each node of t some rule of G "holds".
It is obvi-ous how to check whether a context-free rule holds ofa node or not.
A context-sensitive rule A -- ~/~r ~holds of a node labeled A if the string correspondingto the immediate descendants of that node is t~ andthere is a proper analysis of t of the form p17rAdpp 2that "passes through" the node, (Pl,P2 E V*).
We callthe contextual condition qr ff a proper analysispredicate.Similar to these context-sensitive rules, which allowus to specify context on the "right" and "left", weoften need rules to specify context on the " top"  or"bottom".
Given a node labeled A in a tree t, we saythat DOM(~r q0, qr, ~ E V*, holds of a node labeledA if there is a path from the root of the tree to thefrontier, which passes through the node labeled A, andis of the form3.2 Resul ts  on Local Const ra in tsTheorem 3.1 (Joshi and Levy 1977) Let G be afinite set of local constraint rules and z(G) the set oftrees analyzable by G. (It is assumed here that thetrees in +(G) are sentential trees; i.e., the root node ofa tree in ~-(G) is labeled by the start symbol, S, andthe terminal nodes are labeled by terminal symbols.
)Then the string language L(z(G)) = {xlx is the yieldof t and t E ~-(G)} is context-free.Example  3.2 Let  V = {S,T ,a ,b ,c ,e}  and Y.
= {a ,b ,c ,e} ,and G be a finite set of local constraint rules:1.
S - -e2.
S --- aT3.
T~aS4.
S -- bTc / (a  )A  DOM (T )5.
T -~ bSc / (a  ) A DOM (S)In rules 1, 2, and 3, the context is null, and these rulesare context-free.
In rule 4 (and in rule 5), the const-raint requires an 'a'  on the left, and the node dominat-ed (immediately) by a T (and by an S in rule 5).The language generated by G can be derived by G l:S - *e  S - -aT  1S --, aT T --- aS 1T --,- aS T 1 --,.
bScS 1 --,.
bTcIn G 1 there are additional nonterminals S 1 and T 1 thatenable the context checking of the local constraintsgrammar, G, in the generation process.It is easy to see that, under the homomorphism thatremoves subscripts on the nonterminals T1 and S 1,each tree generable in G 1 is analyzable in G. Also,each tree analyzable in G has a homomorphic pre-image in G 1.The methods used in the proof of the theorem usetree automata to check the local constraint predicates,4 Amer ican Journal  of Computational Linguistics, Volume 8, Number  1, January-March 1982Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruitsince tree automata used as recognizers accept onlytree sets whose yield languages are context-free.We now give an informal introduction to the ideasof (bottom-up) tree automata.
Tree automata processlabeled trees, where there is a left-to-right ordering onthe successors of a node in the tree.
When all thesuccessors of a node v have been assigned states, thena state is assigned to v by a rule that depends on thelabel of v and (he states of the successors of v consid-ering their left-to-right ordering.
Note that the auto-maton may immediately assign states to the nodes onthe frontier of the tree since these nodes .have no suc-cessors.
If the set of states is partitioned into finaland non-final states, then a tree is accepted by theautomaton if the state assigned to the root is a finalstate.
A set of trees accepted by a tree automaton iscalled a recognizable set.
Note that the automaton mayoperate non-deterministically, in which case, as usual,a tree is accepted if there is some set of state assign-ments leading to its acceptance.The importance of tree automata is that they arerelated to the sets of derivation trees of context-freegrammars.
Specifically, if T is the set of derivationtrees of a context-free grammar, G, then there is a treeautomaton that recognizes T. Conversely, if T is theset of trees recognized by a tree automaton, A, then Tmay be systematically relabeled as the set of deriva-tion trees of a context-free grammar.The basic idea presented in detail in Joshi and Levy1977 is that, because tree automata have nice closureproperties (closure under union, intersection, and con-catenation), they can do the computations required tocheck the local constraints.Another way of looking at the checking of a la-beled tree by a tree automaton is as follows.
We im-agine a finite state machine sitting at each node of atree.
The role of the finite state machine is to checkthat a correct rule application is made at the node it ischecking.
Initially, the nodes on the frontier areturned on and signal their parent nodes.
At any othernode in the tree, the machine at that node is turned onas soon as all its direct descendants are active.
Assum-ing that at each node the machine for that node haschecked that the rule applied there was one of therules of the context-free grammar we are looking for,then when the root node of the tree signals that it hascorrectly checked the root we know that the tree is aproper tree for the given context-free grammar.When checking for local constraints, a machine at agiven node not only passes information to its parent,as described above, but also passes information aboutthose parts of the local constraints, corresponding tothe given node as well as all its descendants, that havenot yet been satisfied.
The point is that this informa-tion is always bounded and hence a finite number ofstates are adequate to code this information.The fact that the closure properties hold can beseen as follows.
Consider a slightly more general situ-ation.
We consider an A machine and a B machine ateach node.
Depending on the connections betweenthese A and B machines, we obtain additional results.For example, as each A machine passes information toits parent, it may also pass information to the B ma-chine, but the \[3 machine will not pass informationback to the A machine.
The tree is accepted if the Bmachine .at the root node of the tree ends up in a finalstate.
Although this seems to be a more complicatedmodel, it can in fact be subsumed in our first modeland is the basis of an informal proof that the recogni-tion rules are closed under intersection, since the Amachine and the \[3 machine can check different rules.An important point is that the local constraint on arule applied at a given node may only be verified bythe checking automata t some distant ancestor of thatnode.
In particular, in the case of a proper analysisconstraint, it can only be verified at a node sufficientlyhigh in the tree to dominate the entire string specifiedin the constraint.The perceptive reader may now be wondering whatreplaces all these hypothetical finite state machineswhen the set of trees corresponds to a context-freegrammar.
Well, if we were to convert our local const-raints grammar into a standard form context-freegrammar, we would require a larger nonterminal set.In effect this larger nonterminal set is an encoding ofthe finite state information stored at the nodes.The intuitive explanation presented in this sectionis, in fact, a complete characterization f recognizabili-ty.
Given a context-free grammar, one can specify thefinite state machine to be posted at each node of atree to check the tree.
And conversely, given the fin-ite state machine description, one can derive theequivalent context-free grammar.The essence of the local constraints formulation isto paraphrase the finite state checking at the nodes ofthe tree in terms of patterns or predicates.4.
Some General izationsThe result of Theorem 3.1 can be generalized invarious ways.
Generalizations in (i) and (ii) below areimmediate.
(i) Variables can be included in the constraint.Thus, for example, a local constraint rule can beof the formA --,,.
w I BCDXE FYGwhere A,B,C,D.E,F,G are nonterminals, w is astring of terminals and/or  nonterminals, and Xand Y are variables that range over arbitrarystrings of terminals and nonterminals.American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 5Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruit(ii) Finite tree fragments can be included in theconstraint.
Thus, for example, a local constraintrule can be of the formA- , -w BG , PP/ \  IXC D P NPItoAnother useful generalization has the followingessential character.
(iii) Predicates that relate nodes mentioned in the prop-er analysis predicates and domination predicates(associated with a rule), as well as nodes in fin-ite tree fragments dominated by these nodes,can be included in the constraint.
Unfortunate-ly, at this time we are unable to give a precisecharacterization of this generalization.
The fol-lowing two predicates are special cases of thisgeneralization, and Theorem 3.1 holds for thesetwo cases.
(a) COMMANDCOM (A B C)B immediately dominates A and B dominates C,not necessarily immediately.
Usually B is an Snode.
(b) LEFTMOSTSISTERLMS(A B)A is the leftmost sister of BExample  4.1Let us consider (6) in Section 2 (topicalization).Consider the following fragment of a local const-raints grammar, G. (Only some of the relevant rulesare shown.
)S .-,,.
NPVPVP .-,.
V NP PPS' --- PP SV -.,.
give I NP PPPP--,,.
c I PP1 X V 2 NP 3 __A DOM (S' 4 S 5 Y VP 6 )A COM (PP1 S'4 VP6)--/k LMS (PP1 $5)The last rule has a proper analysis predicate, a domi-nation predicate, and COMMAND and LEFTMOST-SISTER predicates whose arguments satisfy the re-quirements mentioned in (iii) above (i.e., they relatenodes mentioned in proper analysis predicates anddomination predicates).
The indexing makes thisclear.Structure (7) will be well-formed.
Compare (7)with (6) in Section 1 (Gazdar's  framework) and with(8) in Section 5 (Peters's and Karttunen's framework).JPPI/ \P NPI Ito BittSs/NP VP 6/Mory  V 2 NP 3 PPgave  o book ?
(7)6 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982Arav ind K. Joshi  and Leon S. Levy Phrase St ructure  Trees Bear  More  Fruit4.1 Local Constraints in SemanticsSince a local constraint rule has a context-free partand contextual constraint part, it is possible to definecontext-sensit ive composit ional semantics in the fol-lowing manner.For a context-free rule of the formA--,- BCif o(A), o(B), o(C) are the 'semantic'  translations as-sociated with A, B, and C, respectively, then o(A) is acomposition of o(B) and o(C).For a local constraint rule of the formA- , -  BC I Pwhere A -* BC is the context-free part and P is thecontextual constraint, we can have o(A) as a compos-ition of o(B) and o(C), which depends on P. This ideahas been pursued in the context of programming lan-guages (Joshi, Levy, and Yueh 1980).
Whether suchan approach would be useful for natural language is anopen question.
(An additional comment appears inSection 5.)5.
Linked Nodes 4 (Peters's and Kartunnen's frame-work)Peters 1980 and Kartunnen 1980 have proposed adevice for linking nodes to handle unbounded depen-dencies.
Thus, for example, instead of (6) or (7), wehave (8).S !/ (8) .
f -pp  $,- / \  / \// P NP NP VPI I I / II to Bi l l  Mary V NP ",,\ gave a bookISThe dotted line that loops from the VP node back tothe moved constituent is a way of indicating the loca-tion of the gap in the object position under the VP.The link also indicates that there is certain dependencybetween the gap and the dislocated element.
Both inour approach and that of Peters and Karttunen, prolif-eration of categories as in Gazdar's  approach is avoid-ed.
Further, for Peters and Karttunen, while carrying4 We give a very informal description of a linked tree.
Aprecise definition can be found in S. Peters and R.W.
Ritchie,Phrase Linking Grammars, Technical Report, Department of Lin-guistics, University of Texas at Austin, 1982.out bottom-up semantic translation, the moved constit-uent is "visible" at the VP node.
In our approach, this"visibil ity" can be obtained if the translation is madeto depend on the contextual constraint which, ofcourse, has already been checked prior to the transla-tion.
This is the essence of our suggestion in Section4.1.Karttunen 1980 has constructed a parser incorpo-rating the device of linked nodes.
Karttunen also dis-cusses the problem of complex patterns of moved con-stituents and their associated gaps or resumptive pro-nouns.
This is not easy to handle in Gazdar's  frame-work without multiplying the categories even further,e.g., by providing categories uch as S /NP  NP, etc.
5Karttunen handles this problem by essentially incorpo-rating the checking of the patterns of gaps and fillersin the parser, i.e., in the control structure of the par-ser.Our approach can be regarded as somewhat inter-mediate between Gazdar 's  and that of Peters andKarttunen in the following sense.
We avoid multipli-cation of categories as do Peters and Karttunen.
Onthe other hand, the relationship between the movedconstituent and the gap is expressed in the grammaritself (more in the spirit of Gazdar) instead of in theparser (more precisely, in the data structure created bythe parser) as in the Peters and Karttunen approach.We have not pursued the topic of multiple gaps andfillers in our framework but, obviously, in it we wouldopt for Karttunen's uggestion of checking the const-raints on the patterns of gaps and fillers in the parseritself.
It could not be done by local constraints alonebecause local constraints essentially do the work of thelinks in the Peters and Karttunen framework.6.
Skeletal Structural Descriptions (Skeletons)In Section 4, we showed how local constraints al-lowed us to prevent proliferation of categories.
Wecan dispense with the local constraints and constructan equivalent context-free grammar that would havepotentially a very large number of categories.
Whilepursuing the relation between 'structure' and the sizeof the nonterminal vocabulary (i.e., the syntactic cate-gories), we were led to the following surprising result:the actual labels, in a sense, carry no information.
(This result was also used by us in developing someheuristics for converting a context-free grammar into amore compact but equivalent local constraints gram-mar.
We will not describe this use of our result in thepresent paper.
(For further information, see Joshi,Levy, and Yueh 1980.
)First we need some definitions.
A phrase structuretree without labels will be called a skeletal structural5 S /NP NP means an S tree with two NP type holes.Amer ican  Journal of  Computat ional  Linguistics, Volume 8, Number 1, January-March 1982 7Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More Fruitdescription or a skeleton.
A skeleton exhibits all of thegrouping structure without naming the syntactic cate-gories.
For example, (9) is a skeleton.
The structuraldescription is characterized only by the shape of thetree and not the associated labels.
The only symbolsappearing in the structure are the terminal symbols(more precisely, the preterminal symbols and the ter-minal symbols, in the linguistic context, as in (10);however, for the rest of the discussion, we will takeskeletons to mean trees with terminal symbols only).Let G be a context-free grammar and let T G be theset of sentential derivation trees (structural descrip-tions) of G. Let S G be the skeletons of G, i.e., alltrees in T G with the labels removed.It is possible to show that for every context-freegrammar G we can construct a skeletal generatingsystem (consisting of skeletons and skeletal rewritingrules) that generates exactly SG; i.e., all category la-bels can be eliminated while retaining the structuralinformation (Levy and Joshi 1979).BittJove / \a book/ \to Mary(9)/NIBil l  V ?gave DET N P NI I Ia book to Mary(10)8 American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More FruitExample  6.1G : S ~ aSbS --,.
abSG:/ \ / I  \a b a , b/Xa b(1) (2)A skeletal generating system can be constructed asfollows.
We have a finite set of initial skeletons and afinite set of skeletal rewriting rules whose left-handand right-hand sides are skeletons.Initial Skeletons/ \a bSkeletal rewriting rulesa b a , b/ \a bIn this system, generation proceeds from an initialskeleton through a sequence of intermediate skeletonsto the desired skeleton.
Clearly, because of the defini-tion of a skeleton and the nature of the skeletal rewrit-ing rules, the rules must always apply to one of thelowermost configurations in a skeleton that matcheswith the left-hand side of a rule.
Thus the derivationof the skeleton (3) in S G would be as in (11).
Theconfigurations encircled by a dotted line are the onesto which the skeletal rule is applied./ IXa ?
b/1\a ,b/ \a b?
?
?
(3)  = = ?In the above example, there was only one nonter-minal; hence the result is obvious.
Following is asomewhat more complicated example.Example  6.2G: E -~aE - .
(E)E - -E+E/~(+ )A~ (* )A~ (* )E -~E* E /~(+ )G is a local constraints grammar.
Clearly there is acontext- free grammar G' that is equivalent to G.Rather than taking a complicated context-free gram-mar and then exhibiting the equivalent skeletal gram-mar, we will take the local constraints grammar G andexhibit a skeletal grammar equivalent o G. This willallow us to present a complicated example withoutmaking the resulting skeletal grammar too unwieldy.Also, this example will give some idea about the rela-tionship between local constraints grammars and skele-tal grammars; in particular, the skeletal rewriting rulesindirectly encode the local constraints in the rules inExample 6.2.We have eliminated all labels by introducing struc-tural rewriting rules and defining the derivation asproceeding from skeleton to skeleton rather than fromstring to string.
This result clearly brings out the rela-tionship between the grouping structure and the syn-tactic categories labeling the nodes.f ?
~ ?
?'
/ \  ' / IN  / I  Nl a  b l  a .
.
.
a ,, , j !
\  , , , -7 \ ' .
.
(a  b,, , / \a b(11)American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 9Aravind K. Joshi  and Leon S. Levy Phrase Structure Trees Bear More  FruitSkeletal grammar equivalent to G:Initial Skeletons:/ \  /i\t 1 1 IG Q Q QSkeletal Rewriting Rules:/ \i ' iQ Q/ \?
4.
?IQ O.
/ i \  I TQ Q/ \(1 GG/',~ Ti ' i  ?Q Q/ \?
?
? "
/ \j .
j  oQ Q./!\.I ./l\.?
I I0 0/ \  T r ' r  ?Q Q/ \  / \' T ' ~ ' / i \ '?
i ' iC1 Q/ \  /\?
i ' TQ Q10 Amer ican Journal  of Computat iona l  Linguist ics,  Vo lume 8, Number  1, January-March 1982Aravind K. Joshi and Leon S. Levy Phrase Structure Trees Bear More FruitSince skeletons pay attention to grouping only, thisresult may be psycholinguistically important becauseour first intuition about the structure of a sentence ismore likely to be in terms of the grouping structureand not in terms of the corresponding syntactic cate-gories, especially those beyond the preterminal cate-gories.The theory of skeletons may also provide someinsight into the problem of grammatical inference.
Fora finite state string automaton, it is well know that ifthe number of states is 2k then, if we are presentedwith all acceptable stings of length <2k, the finitestate automaton is completely determined.
We have asimilar situation with the skeletons.
First, it can beshown that for each skeletal set S G (i.e., the set ofskeletons of a context-free grammar) we can constructa bottom-up tree automaton that recognizes preciselyS G (Levy and Joshi 1978).
Further, if the number ofstates of this automaton is k, then the set of all ac-ceptable sets of skeletons of depth _<2k completelydetermines 5G (Levy and Joshi 1979).
Using skele-tons (i.e., string with their grouping structure) ratherthan just strings as input to a grammatical inferencemachine is an idea worth pursuing further.6.
Conc lus ion:We have presented several results concerningphrase structure trees that show that phrase structuretrees when viewed in certain ways have much moredescriptive power than one would have thought.
Wehave given a brief account of our work on local const-raints and presented an intuitive proof.
We have alsocompared it to some aspects of the framework of Gaz-dar and that of Peters and Karttunen.
We have alsoshown that phrase structure trees, even when deprivedof the labels, retain in a certain sense all the structuralinformation.
This result has implications for grammat-ical inference procedures.ReferencesBresnan, J.W.
1978 A Realistic transformational grammar.
InHalle, M., Bresnan, J. and Miller, G.A., Ed., Linguistic Theoryand Psychological Reality.
The MIT Press, Cambridge, Mass.Chomsky, N. 1965 Aspects of the Theory of Syntax, The MIT Press,Cambridge, Mass.Gazdar, G.J.M.
1978 English as a context-free language, unpubl-ished manuscript.Gazdar, G.J.M.
1981 Unbounded ependencies and coordinatestructure.
Linguistic Inquiry 12, 2.Gazdar.
G.J.M.
1982 Phrase Structure Grammar.
To appear inJaeobson, P. and Pullam, G.K., Ed., The Nature of SyntacticRepresentation.
Reidel, Boston, Mass.Harris, Z.S.
1956 String analysing of language structure, manu-script.
Also in manuscripts around 1958 and published in 1962by Mouton and Co., The Hague.Joshi, A.K.
and Levy, L.S.
1977 Constraints on structural descrip-tions.
SlAM Journal of Computing.Joshi, A.K., Levy, L.S., and Yueh, K. 1980 Local constraints inthe syntax and semantics of programming language.
Journal ofTheoretical Computer Science.Kaplan, R. and Bresnan, J.W.
1979 A formal system for grammat-ical representation.
To appear in Bresnan, J.W., Ed., The Men-tal Representation of Grammatical Relations.
The MIT Press,Mass.Karttunen, L. 1980 Unbounded ependencies in phrase structuregrammar: slash categories versus dotted lines.
Paper presentedat the Third Amsterdam Colloquium: Formal Methods in theStudy of Language, Amsterdam.Levy, L.S.
and Joshi, A.K.
1978 Skeletal structural descriptions.Information and ControlMcCawley, J.D.
1967 Concerning the base component of a trans-formational grammar.
Foundations of Language, Vol.
4.
Reprint-ed in McCawley, J.D., Grammar and Meaning, Academic Press,New York, NY, 1968.Peters, S. 1980 (Talk presented at the Workshop on Alternativesto Transformation Grammars, Stanford University, January.
)Peters, S. and Ritchie, R.W.
1969 Context-sensitive immediateconstituent analysis.
Proc.
ACM Symposium on Theory ofComputing.Sager, N. 1967 Syntactic analysis of natural anguages.
Advancesin Computers, Vol.
8 ed.
M. Alt and M. Rubinoff, AcademicPress, New York, NY.Aravind K. Joshi is a professor of computer andinformation science, and that department's chairman, atthe University of Pennsylvania, Philadelphia.
He re-ceived the Ph.D. degree in electrical engineering fromthe University of Pennsylvania in 1960.Leon S. Levy is a member of the technical staff ofBell Telephone Laboratories at Whippany, New Jersey.He received the Ph.D. degree in Computer Science fromthe University of Pennsylvania in 1970.American Journal of Computational Linguistics, Volume 8, Number 1, January-March 1982 11
