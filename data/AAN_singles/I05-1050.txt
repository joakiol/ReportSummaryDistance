Automatic Extraction of Fixed MultiwordExpressionsCampbell Hore, Masayuki Asahara, and Yu?ji MatsumotoGraduate School of Information Science,Nara Institute of Science and Technology,8916-5 Takayama, Ikoma, Nara 630-0192, Japan{campbe-h, masayu-a, matsu}@is.naist.jphttp://cl.naist.jpAbstract.
Fixed multiword expressions are strings of words which to-gether behave like a single word.
This research establishes a method forthe automatic extraction of such expressions.
Our method involves threestages.
In the first, a statistical measure is used to extract candidate bi-grams.
In the second, we use this list to select occurrences of candidateexpressions in a corpus, together with their surrounding contexts.
Theseexamples are used as training data for supervised machine learning, re-sulting in a classifier which can identify target multiword expressions.The final stage is the estimation of the part of speech of each extractedexpression based on its context of occurence.
Evaluation demonstratedthat collocation measures alone are not effective in identifying target ex-pressions.
However, when trained on one million examples, the classifieridentified target multiword expressions with precision greater than 90%.Part of speech estimation had precision and recall of over 95%.1 Introduction1.1 Multiword ExpressionsFor natural language processing purposes, a naive definition of a word in Englishis ?a sequence of letters delimited by spaces ?.
By this definition, the expressionad hoc, which originally came from Latin, consists of two ?words?, ad and hoc.However, in isolation hoc is not a meaningful English word.
It is always precededby ad.
This suggests that treating these two words as if they together forma single ?word with spaces?
more closely models their behaviour in text.
Asequence of words which for one reason or another is more sensibly treated as asingle lexical item, rather than as individual words, is known as a multiwordexpression (MWE).
In other words, an MWE is a sequence of words whichtogether behave as though they were a single word.MWEs are not limited to imported foreign phrases such as ad hoc.
They covera large range of expression types including proper nouns such as New York, verb-particle constructions such as to call up (i.e.
to telephone someone), and light Supported by the Japanese government?s MEXT scholarship programme.R.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
565?575, 2005.c?
Springer-Verlag Berlin Heidelberg 2005566 C. Hore, M. Asahara, and Y. Matsumotoverbs such as to make a mistake.
The justification for treating such expressionsas MWEs is that their linguistic properties are odd in some way as comparedto ?normal?
expressions: either their part of speech or their meaning is unpre-dictable despite full knowledge about the parts of speech or meanings of theirconstituent words.1.2 Fixed Multiword ExpressionsBy fixed, we mean that this particular type of MWE consists of a contiguous se-quence of words.
Other MWE types can consist of discontinuous word sequences.For example, the verb-particle construction to call up takes an indirect object,the person who receives the telephone call.
This person can appear after theverb-particle construction (e.g.
?I called up Mohammad?)
but it can also ap-pear in the middle of the verb-particle construction, (e.g.
?I called Mohammadup?).
In contrast, fixed MWEs consist of contiguous word sequences.
For exam-ple, by and large cannot be modified by insertion of other words (e.g.
*?by andvery large?
).1.3 Multiword Expressions in ParsingThe aim of our research is the development of a method for the automatic extrac-tion and part of speech estimation of fixed MWEs.
The ability to identify thistype of MWE in texts is of potential use in a wide variety of natural languageprocessing tasks because it should enable an improvement in the precision of sen-tence parsing.
Sentence parsing is frequently the first step in more sophisticatedlanguage processing tasks, so an increase in parsing precision should improveresults in a large number of natural language processing applications.A parser generally takes a sentence as input, together with the parts of speechof the tokens1 in the sentence.
The parser then attempts to estimate the mostprobable syntactic structure for the sentence.
Some kinds of MWE have thepotential to disrupt this process because the part of speech of the MWE as awhole, cannot be predicted on the basis of the parts of speech of its constituentwords.
For example, the part of speech sequence for the expression by and largeis Preposition + Conjunction + Adjective, which is a sequence almost certainlyunique to this expression.
A parser which is not explicitly informed about byand large will therefore struggle to cope with this part of speech sequence, andmay incorrectly try to group one or more parts of the expression with words inthe surrounding context.The solution to this problem of syntactically unpredictable MWEs is to addthem to the dictionary used by the parser.
When the parser comes across asequence of words that matches an MWE in its dictionary, it can use the MWE?spart of speech to parse the sentence treating the MWE as a single lexical item.1 We use the term tokens here rather than words because texts actually contain manyspace delimited character strings which are not normally thought of as words, suchas numbers and punctuation.
In addition, some purely alphabetic character stringsare not words in their own right, as is the case for hoc, mentioned above.Automatic Extraction of Fixed Multiword Expressions 567In reality, some sequences of words are an MWE only in specific contexts.For example, in the main is an MWE when it means ?overall?
or ?mostly?
asseen in the sentence ?In the main, the biggest improvements have been in childhealth?.
In contrast, in a sentence such as ?Village hotels ought to be in the mainsquare, not at the outskirts of a village?
the word sequence in the main is not anMWE, and can therefore be treated normally by the parser as separate words.1.4 Target ProblemsWe approach the task of extracting fixed MWEs by decomposing it into threesub-problems, as illustrated in Fig.
1.
The first (?3.1, described in Section 3.1) issimple collocation extraction.
We use a standard collocation measure to extractas many candidate bigram MWEs from the corpus as possible.Fig.
1.
Flowchart of processingThe second problem (?3.2) is refinement of the list of candidate MWEs.Many of the candidates are not target multiword expressions.
Distinguishingbetween word sequences that are MWEs, and those that never are, representsone sub-task.
Hereafter, we refer to word sequences which are never MWEs asnon-MWEs.
Some word sequences have dual identities.
In one context a word568 C. Hore, M. Asahara, and Y. Matsumotosequence may be an MWE, but in another, it may be just a normal, literal wordsequence.
For example, a child forced to help wash the family car, and actingpetulantly, might be scolded ?Don?t kick the bucket!?.
In this case kick thebucket is a normal, compositional phrase; its meaning can be understood basedon the literal meaning of its constituent words.
When kick the bucket is used as aeuphemism for ?to die?
however, its meaning is non-compositional, and thus inthis context it is an MWE.
In this paper we refer to occurrences of literal wordsequences which have the appearance of being an MWE as pseudo-MWEs.
Wedeal with these two sub-tasks simultaneously, using supervised machine learning.The third problem we tackle (?3.3) is estimation of the part of speech ofMWEs.
This problem is also solved using supervised machine learning.
In thisresearch we limit ourselves to MWEs containing only two words (i.e.
bigrams).In the future, we plan to generalize the method so that it works with MWEs ofarbitrary length.2 Related WorkCollocation extraction has been covered extensively in the literature.
One ofthe earliest attempts to automatically extract collocations from a corpus wasundertaken by Church and Hanks [1].
The statistical measure they used to iden-tify collocations was based on mutual information.
Smadja [2] developed a toolcalled Xtract for the extraction of collocations.
His definition of a collocationdiffered slightly from that of Church and Hanks because he claimed expressionssuch as doctors and nurses are not real collocations, just words related by virtueof their shared domain or semantics.
Thanopoulos, Fakotakis and Kokkinakisin [3] reviewed the statistical measures most frequently used for collocation ex-traction, and evaluated them by comparing their performance with that of twonew measures of their own.
Their first novel measure, Mutual Dependency (MD)is pointwise mutual information minus self-information.
The second measure at-tempts to introduce a slight frequency bias by combining the t-score with mutualdependency.
Although frequency alone is not sufficient evidence of collocationalstatus, they argue that candidate collocations that have a high frequency aremore likely to be valid than those that are very rare.While collocations have received attention over a number of years, MWEshave only relatively recently emerged as a research topic within natural lan-guage processing.
In consequence, there are relatively few articles specificallyabout MWEs.
Sag et al in [4] gave a linguistic categorisation of the differ-ent types of MWE, and described ways of representing them efficiently withina computational framework2.
Although MWEs as a whole have yet to receivewidespread investigation, attention has been paid to specific types of MWE.
Forexample, verb-particle constructions have been the subject of several studies (seefor example [5] and [6]).2 Head-driven Phrase Structure Grammar (HPSG).Automatic Extraction of Fixed Multiword Expressions 5693 MethodIn order to extract information about fixed MWEs from a corpus, we use a threestage process.
In the first stage we identify a list of candidate MWEs based on thestatistical behaviour of the tokens in the corpus.
Two words whose probability ofappearing together is greater than that which would be expected based on theirindividual frequencies, are considered to constitute a potential MWE and areextracted for later processing.
In other words, stage one is collocation extraction.In the second stage, we use this list of candidate MWEs as the basis forextracting from the corpus examples of candidates together with their contexts.These examples are then used as training data for supervised machine learningresulting in a classifier capable of distinguishing between, on the one hand, trueMWEs, and on the other, non-MWEs and pseudo-MWEs.In the final stage we use supervised machine learning to train a classifier toperform MWE part of speech assignment.
By examining the context surroundingan MWE, it is possible for the classifier to determine the most likely part ofspeech for the MWE in that context.3.1 Collocation ExtractionCollocation extraction was performed using one of the statistical measures dis-cussed in [3].
The measures we experimented with were: frequency, ?-square [7],log-likelihood ratio [8], t-score [7], mutual information (MI) [1], mutual depen-dency (MD ?
mutual information minus self-information) [3], and log-frequencybiased mutual dependency (LFMD ?
a combination of the t-score and mutualdependency) [3].
The equations of these measures are shown in Fig.
2.We compared the resulting ranked lists of bigrams with a list of target MWEsextracted from the British National Corpus (BNC)3.
The target list was pro-duced by starting with a list of all MWEs tagged as such in the BNC, andremoving MWEs with a frequency of less than five, and MWEs with a partof speech of noun, or adjective.
This reduction was performed for two reasons.Firstly, many MWE instances in the BNC can be considered noise in that theycontain spelling variants, and features of spoken language.
Secondly, a colloca-tion consisting entirely of a combination of one or more nouns and adjectives isalmost certainly a noun phrase, or part of a noun phrase.
Noun phrases tend tobe easily identifiable as such by parsers, and so are not relevant to our researchaim.
By removing the above MWEs we were able to reduce computational costsat later stages of processing.3.2 VerificationVerification was performed with the aim of extracting a much higher quality listof candidate MWEs from the list of candidate MWEs produced in the collocationstage.3 http://www.natcorp.ox.ac.uk/570 C. Hore, M. Asahara, and Y. Matsumotot-scoret =x ?
?
?s2NWhere x is the sample mean, s2 is the sample variance, N is the sample size and ?
isthe distribution?s mean.
?-square?2 =(fw1w2 ?
fw1fw2 )2fw1fw2+(fw1w2 ?
fw1fw2)2fw1fw2+(fw1w2 ?
fw1fw2)2fw1fw2+(fw1w2 ?
fw1fw2)2fw1fw2Where f is the frequency of an event, w1w2 is the sequence of events (in this casewords) w1 then w2, and w1 is the negation of the event w1.log-likelihood ratio?2 log ?
= 2 ?
log L(H1)L(H0)Where L(H) is the likelihood of hypothesis H assuming a binomial distribution.pointwise mutual information (PMI)I(w1, w2) = log2P (w1w2)P (w1) ?
P (w2)Where P (w) is the probability of a given word.mutual dependency (MD)D(w1, w2) = log2P 2(w1w2)P (w1) ?
P (w2)Where P (w) is the probability of a given word.log-frequency biased mutual dependency (LFMD)DLF (w1, w2) = D(w1, w2) + log2P (w1w2)Where P (w) is the probability of a given word.Fig.
2.
Collocation measuresFeatures.
In order to train a classifier, a decision must be made about whichfeatures to include in the training data.
We decided to use the tokens and theirparts of speech from a context window of three tokens to the left and three tokensto the right of each candidate MWE.
We also used the tokens in the candidateMWE itself and their parts of speech.
The cutoff value of three is somewhatarbitrary, but most lexical dependencies can be assumed to be relatively local,so we can assume that it is large enough to capture the most useful informationavailable from the context.Automatic Extraction of Fixed Multiword Expressions 571Contexts were not allowed to cross sentence boundaries.
In cases where theavailable context surrounding a candidate MWE was shorter than the three tokenwindow, we inserted the appropriate number of dummy tokens and dummy partsof speech to fill up the deficit: ?BOS?
(Beginning Of Sentence) tokens in the leftcontext, and ?EOS?
(End Of Sentence) tokens in the right context.Part of Speech Tagging.
The part of speech information was provided bytagging the corpus using the part of speech tagger TnT4.
The BNC is a part ofspeech tagged corpus, but retagging was necessary because although each MWEin the BNC is tagged with a part of speech, its constituent tokens are not.
Thetokens which make up each MWE must therefore be tagged with individual partsof speech.
It might be argued that combining the original BNC tagging with theTnT tagging of the words in the MWEs would have produced more accuratetraining data, but in a real world application, the part of speech information inthe classifier?s input data will be produced entirely using a tagger such as TnT.By using the same part of speech tagger at both the training, and applicationstages, any systematic tagging mistakes will hopefully (at least in part) be learntand compensated for by the classifier.
The tagger was trained on a subset of BNCfiles containing just under 5.5 million tokens.
It was then tested on a differentset of files, containing approximately 5.3 million tokens.
The tagger?s precisionwhen tested on this data was 94.7%.Training.
The corpus used for training the classifier was a sub-corpus of theBNC containing approximately ninety million tokens covering all domains in thecorpus.
Examples used for training were the occurrences in the training corpusof the top 10,000 bigrams identified using the t-score and LFMD collocationmeasures.
Most of the bigrams in the corpus were negative examples, eithernon-MWEs, or pseudo-MWEs.We used TinySVM5 to create a binary classifier.
Training was performed(using the software?s default settings) on the training corpus.
Several trainingruns were performed using different amounts of data in order to investigate therelationship between volume of training data and the resulting model?s perfor-mance.Testing.
Another sub-corpus of the BNC, independent of that used in training,was used for testing the classifier.
This testing corpus contained approximatelysix million tokens and included texts from each domain in the corpus.3.3 Part of Speech Estimation of Multiword ExpressionsWe treated the estimation of the part of speech of a given MWE as a classificationtask using the same approach as we used for the classification of true and falseMWEs.
We trained a separate classifier for each target part of speech.
A positive4 http://www.coli.uni-sb.de/?thorsten/tnt/5 http://chasen.org/?taku/software/TinySVM/572 C. Hore, M. Asahara, and Y. Matsumototraining example was an occurrence of an MWE with the target part of speech.A negative example was an occurrence of any MWE with a non-target part ofspeech.
The features used were the token and part of speech of three tokens tothe left and to the right of the target MWE, as well as the tokens and parts ofspeech of the words in the MWE itself.
The training and testing corpora werethe same as used at the verification stage described above (Section 3.2).
Thiswas acceptable because the two tasks are independent of each other.We chose the target parts of speech (adverbs, prepositions and conjunctions)because these relatively closed class, high frequency types are expected to bemost useful in applications like parsing.
We also experimented with an openclass type (nouns) for comparison.
Verbs could not be tested because there wereinsufficient numbers of them in the testing corpus.
There are few fixed MWEverbs, so a scarcity of data was not surprising.4 Results and Discussion4.1 Collocation ExtractionResults for collocation extraction (Table 1) show that standard collocation mea-sures perform poorly in the task of extracting the target MWEs.
Even whenTable 1.
Precision and recall for top 100, 1,000 and 10,000 candidate multiword ex-pressions extracted using different collocation measuresMeasure Cutoff Precision Recall F-measure10,000 0.009 0.251 0.017freq 1,000 0.032 0.091 0.047100 0.010 0.003 0.00410,000 0.009 0.257 0.017t-score 1,000 0.037 0.106 0.055100 0.030 0.009 0.01310,000 0.006 0.169 0.011?2 1,000 0.013 0.037 0.019100 0.020 0.006 0.00910,000 0.004 0.117 0.008log-like 1,000 0.008 0.023 0.012100 0.000 0.000 0.00010,000 0.003 0.083 0.006MI 1,000 0.002 0.006 0.003100 0.000 0.000 0.00010,000 0.003 0.091 0.006MD 1,000 0.003 0.009 0.004100 0.000 0.000 0.00010,000 0.008 0.229 0.015LFMD 1,000 0.017 0.049 0.025100 0.080 0.023 0.036Automatic Extraction of Fixed Multiword Expressions 573calculated based on the top 10,000 candidate collocations, recall is only 26%(using the t-score).A limitation in our approach to measuring collocation extraction may bepartly to blame for the poor results in this task.
Our target list consisted of alltarget MWEs, irrespective of their length.
Since the collocations extracted werelimited to bigrams, some of these may in fact be only part of a larger MWE inour target list.Nevertheless, it may be that collocation measures are relatively ineffective atextracting fixed MWEs.
Collocation measures are most effective when appliedto expressions such as noun compounds.
Many of the target MWEs contain highfrequency function words such as prepositions, and thus are atypical of the typesof expressions for which collocation measures were originally developed.4.2 VerificationVerification of candidate MWEs produced better results (Table 2).
For example,a classifier trained using one million examples, had precision of 96.56%, andrecall of 89.11% giving an F-measure of 92.69.Table 2.
Performance of verifier using models trained on different quantities of dataMeasure Examples Precision (%) Recall (%) F-measureLFMD 1,000,000 96.56 89.11 92.69100,000 96.35 79.87 87.3410,000 93.66 56.04 70.121,000 92.83 11.56 20.56Initial review of classification results suggests a number of sources of error.Tagging errors seem to cause many of the false negative results.
Proper nounstend to be tagged as ?unclassified words?
which is intelligent in as much asit bundles all unusual words the tagger is unsure of together, but it results inincorrect tagging which prevents the classifier identifying true MWEs.
Similarly,capitalisation of words in titles results in incorrect tagging of ordinary words asproper nouns.
One title in particular Sport in Short occurs multiple times in thecorpus, resulting in numerous errors.False positives seem to be caused by proper nouns (e.g.
Kuala Lumpur) andforeign words (e.g.
Vive L?Empereur).
Both false positives and false negativesseem to occur often in the context of punctuation, suggesting that this presentsa particular difficulty for the classifier.Interestingly, some false positives are in fact substrings of longer MWEs.Because we focused on bigrams in this paper, MWEs of longer than two tokenswere ignored when assessing whether a candidate MWE was a true or falseMWE.
However, some of these candidate MWEs were in fact substrings of alonger MWE.
The classifier may therefore be recognising that a given substring574 C. Hore, M. Asahara, and Y. Matsumotooccurs in a context typical of MWEs, and is identifying the MWE substring asbeing a MWE in its own right.
A fuller implementation which extracts MWEslonger than two tokens might therefore be expected to eliminate this sourceof error.In spite of the occasional error, applying a classifier to the context surround-ing a candidate MWE seems to offer an effective means of distinguishing trueMWEs from non-MWEs and pseudo-MWEs.4.3 Part of Speech EstimationEvaluation of the part of speech classifiers shows them to be an effective means ofestimating an MWE?s part of speech based on its context of occurrence (Table 3).As we might expect, the classifier for nouns performed best, with near perfectrecall and high precision.
The conjunctions classifier performed least well withrecall in particular being lower than that achieved for other parts of speech.This may reflect a greater variability in the contexts surrounding conjunctiveMWEs.
Conjunctions often play a discursive role in sentences, so evidence ofan expression being a conjunction or not might be found at a higher level oflinguistic analysis than the immediate lexical context used in our experiment.Table 3.
Part of speech estimation resultsPart of speech Precision (%) Recall (%) F-measurePrepositions 98.06 98.40 98.23Conjunctions 97.10 95.37 96.23Adverbs 98.73 98.72 98.72Nouns 98.88 99.25 99.075 Future WorkIn this work we have focused on bigrams.
We hope to generalise our approach,so that MWEs of length greater than two can be extracted and assigned a partof speech.We plan to evaluate the performance of the BNC models described aboveon another corpus to determine their flexibility.
Specifically, we plan to use acorpus of North American English such as the Penn Treebank, in the hope ofdemonstrating the models?
ability to handle American as well as British English.We also plan to check the effect on parsing accuracy of using the extractedmultiword expressions in the input to a parser such as Collins?
[9] orYamada?s [10].6 ConclusionIn this research we aimed to identify a method for the automatic extraction andpart of speech estimation of fixed MWEs.
Knowledge about fixed MWEs hasAutomatic Extraction of Fixed Multiword Expressions 575the potential to improve the accuracy of numerous natural language processingapplications.
Generating such a list therefore represents an important naturallanguage processing task.Our method uses a collocation measure to produce a list of candidate bigrams.These candidates are then used to select training data for a classifier.
The trainedclassifier was successfully able to distinguish between contexts containing a trueMWE, from contexts containing a pseudo-MWE or no MWE at all.
The classifiertrained on one million example candidates identified using LFMD had precisionof 96.56%, and recall of 89.11%, giving an F-measure of 92.69%.
Part of speechclassifiers were then trained and tested.
The classifiers were able to identify thecorrect part of speech for an MWE with a precision and recall of over 95%.These results show that the local context surrounding an MWE containssufficient information to identify its presence, and estimate its part of speech.If this information is detailed enough, we may be able to perform additionalprocessing steps.
For example, it may be possible to distinguish between specificsub-types of fixed MWE.
The present method needs to be generalised so it candeal with MWEs of any length, not just bigrams.
We plan to explore these issuesin future research.References1.
Church, K., Hanks, P.: Word association norms, mutual information, and lexicog-raphy.
Computational Linguistics 16 (1990) 22?292.
Smadja, F.: Retrieving collocations from text: Xtract.
Computational Linguistics19 (1993) 143?177 *Special Issue on Using Large Corpora: I.3.
Thanopoulos, A., Fakotakis, N., Kokkinakis, G.: Comparative evaluation of col-location extraction metrics.
In: International Conference on Language Resourcesand Evaluation (LREC-2002).
(2002) 620?6254.
Sag, I., Baldwin, T., Bond, F., Copestake, A., Flickinger, D.: Multiword expres-sions: A pain in the neck for NLP.
In: Proceedings of the Third International Con-ference on Intelligent Text Processing and Computational Linguistics (CICLING2002), Mexico City, Mexico, CICLING (2002) 1?155.
Baldwin, T., Villavicencio, A.: Extracting the unextractable: A case study on verb-particles.
In Roth, D., van den Bosch, A., eds.
: Proceedings of the 6th Conferenceon Natural Language Learning (CoNLL-2002), Taipei, Taiwan (2002) 98?1046.
Villavicencio, A.: Verb-particle constructions and lexical resources.
In Bond, F.,Korhonen, A., McCarthy, D., Villavicencio, A., eds.
: Proceedings of the ACL 2003Workshop on Multiword Expressions: Analysis, Acquisition and Treatment, ACL(2003) 57?647.
Manning, C.D., Schu?tze, H.: Foundations of Statistical Natural Language Process-ing.
The MIT Press, Cambridge, Massachusetts (1999)8.
Dunning, T.: Accurate methods for the statistics of surprise and coincidence.Computational Linguistics 19 (1993) 61?749.
Collins, M.: Head-Driven Statistical Models for Natural Language Parsing.
PhDthesis, University of Pennsylvania (1999)10.
Yamada, H., Matsumoto, Y.: Statistical dependency analysis with support vectormachines.
In: IWPT 2003: 8th International Workshop on Parsing Technologies.
(2003) 195?206
