Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43?51,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsEvaluating the Impact of Coder Errors on Active LearningInes RehbeinComputational LinguisticsSaarland Universityrehbein@coli.uni-sb.deJosef RuppenhoferComputational LinguisticsSaarland Universityjosefr@coli.uni-sb.deAbstractActive Learning (AL) has been proposed as atechnique to reduce the amount of annotateddata needed in the context of supervised clas-sification.
While various simulation studiesfor a number of NLP tasks have shown thatAL works well on goldstandard data, there issome doubt whether the approach can be suc-cessful when applied to noisy, real-world datasets.
This paper presents a thorough evalua-tion of the impact of annotation noise on ALand shows that systematic noise resulting frombiased coder decisions can seriously harm theAL process.
We present a method to filter outinconsistent annotations during AL and showthat this makes AL far more robust when ap-plied to noisy data.1 IntroductionSupervised machine learning techniques are still themainstay for many NLP tasks.
There is, how-ever, a well-known bottleneck for these approaches:the amount of high-quality data needed for train-ing, mostly obtained by human annotation.
ActiveLearning (AL) has been proposed as a promising ap-proach to reduce the amount of time and cost for hu-man annotation.
The idea behind active learning isquite intuitive: instead of annotating a large numberof randomly picked instances we carefully select asmall number of instances that are maximally infor-mative for the machine learning classifier.
Thus asmaller set of data points is able to boost classifierperformance and to yield an accuracy comparable tothe one obtained when training the same system ona larger set of randomly chosen data.Active learning has been applied to several NLPtasks like part-of-speech tagging (Ringger et al,2007), chunking (Ngai and Yarowsky, 2000), syn-tactic parsing (Osborne and Baldridge, 2004; Hwa,2004), Named Entity Recognition (Shen et al,2004; Laws and Schu?tze, 2008; Tomanek and Hahn,2009), Word Sense Disambiguation (Chen et al,2006; Zhu and Hovy, 2007; Chan and Ng, 2007),text classification (Tong and Koller, 1998) or statis-tical machine translation (Haffari and Sarkar, 2009),and has been shown to reduce the amount of anno-tated data needed to achieve a certain classifier per-formance, sometimes by as much as half.
Most ofthese studies, however, have only simulated the ac-tive learning process using goldstandard data.
Thissetting is crucially different from a real world sce-nario where we have to deal with erroneous dataand inconsistent annotation decisions made by thehuman annotators.
While simulations are an indis-pensable instrument to test different parameters andsettings, it has been shown that when applying ALto highly ambiguous tasks like e.g.
Word SenseDisambiguation (WSD) with fine-grained sense dis-tinctions, AL can actually harm the learning process(Dang, 2004; Rehbein et al, 2010).
Dang suggeststhat the lack of a positive effect of AL might be dueto inconsistencies in the human annotations and thatAL cannot efficiently be applied to tasks which needdouble blind annotation with adjudication to insurea sufficient data quality.
Even if we take a more opti-mistic view and assume that AL might still be usefuleven for tasks featuring a high degree of ambiguity,it remains crucial to address the problem of annota-tion noise and its impact on AL.43In this paper we present a thorough evaluation ofthe impact of annotation noise on AL.
We simulatedifferent types of coder errors and assess the effecton the learning process.
We propose a method to de-tect inconsistencies and remove them from the train-ing data, and show that our method does alleviate theproblem of annotation noise in our experiments.The paper is structured as follows.
Section 2 re-ports on recent research on the impact of annota-tion noise in the context of supervised classification.Section 3 describes the experimental setup of oursimulation study and presents results.
In Section 4we present our filtering approach and show its im-pact on AL performance.
Section 5 concludes andoutlines future work.2 Related WorkWe are interested in the question whether or not ALcan be successfully applied to a supervised classifi-cation task where we have to deal with a consider-able amount of inconsistencies and noise in the data,which is the case for many NLP tasks (e.g.
sen-timent analysis, the detection of metaphors, WSDwith fine-grained word senses, to name but a few).Therefore we do not consider part-of-speech tag-ging or syntactic parsing, where coders are expectedto agree on most annotation decisions.
Instead,we focus on work on AL for WSD, where inter-coder agreement (at least for fine-grained annotationschemes) usually is much lower than for the formertasks.2.1 Annotation NoiseStudies on active learning for WSD have been lim-ited to running simulations of AL using gold stan-dard data and a coarse-grained annotation scheme(Chen et al, 2006; Chan and Ng, 2007; Zhu andHovy, 2007).
Two exceptions are Dang (2004) andRehbein et al (2010) who both were not able toreplicate the positive findings obtained for AL forWSD on coarse-grained sense distinctions.
A pos-sible reason for this failure is the amount of annota-tion noise in the training data which might misleadthe classifier during the AL process.
Recent work onthe impact of annotation noise on a machine learningtask (Reidsma and Carletta, 2008) has shown thatrandom noise can be tolerated in supervised learn-ing, while systematic errors (as caused by biased an-notators) can seriously impair the performance of asupervised classifier even if the observed accuracyof the classifier on a test set coming from the samepopulation as the training data is as high as 0.8.Related work (Beigman Klebanov et al, 2008;Beigman Klebanov and Beigman, 2009) has beenstudying annotation noise in a multi-annotator set-ting, distinguishing between hard cases (unreliablyannotated due to genuine ambiguity) and easy cases(reliably annotated data).
The authors argue thateven for those data points where the annotatorsagreed on one particular class, a proportion of theagreement might be merely due to chance.
Fol-lowing this assumption, the authors propose a mea-sure to estimate the amount of annotation noise inthe data after removing all hard cases.
Klebanovet al (2008; 2009) show that, according to theirmodel, high inter-annotator agreement (?)
achievedin an annotation scenario with two annotators is noguarantee for a high-quality data set.
Their model,however, assumes that a) all instances where anno-tators disagreed are in fact hard cases, and b) that forthe hard cases the annotators decisions are obtainedby coin-flips.
In our experience, some amount ofdisagreement can also be observed for easy cases,caused by attention slips or by a deviant interpre-tation of some class(es) by one of the annotators,and the annotation decision of an individual annota-tor cannot so much be described as random choice(coin-flip) but as systematically biased selection,causing the types of errors which have been shownto be problematic for supervised classification (Rei-dsma and Carletta, 2008).Further problems arise in the AL scenario wherethe instances to be annotated are selected as a func-tion of the sampling method and the annotationjudgements made before.
Therefore, Beigman andKlebanov Beigman (2009)?s approach of identify-ing unreliably annotated instances by disagreementis not applicable to AL, as most instances are anno-tated only once.2.2 Annotation Noise and Active LearningFor AL to be succesful, we need to remove system-atic noise in the training data.
The challenge we faceis that we only have a small set of seed data and noinformation about the reliability of the annotations44assigned by the human coders.Zhu et al (2008) present a method for detectingoutliers in the pool of unannotated data to preventthese instances from becoming part of the trainingdata.
This approach is different from ours, wherewe focus on detecting annotation noise in the man-ually labelled training data produced by the humancoders.Schein and Ungar (2007) provide a systematic in-vestigation of 8 different sampling methods for ALand their ability to handle different types of noisein the data.
The types of noise investigated are a)prediction residual error (the portion of squared er-ror that is independent of training set size), and b)different levels of confusion among the categories.Type a) models the presence of unknown featuresthat influence the true probabilities of an outcome: aform of noise that will increase residual error.
Typeb) models categories in the data set which are intrin-sically hard to disambiguate, while others are not.Therefore, type b) errors are of greater interest to us,as it is safe to assume that intrinsically ambiguouscategories will lead to biased coder decisions andresult in the systematic annotation noise we are in-terested in.Schein and Ungar observe that none of the 8sampling methods investigated in their experimentachieved a significant improvement over the randomsampling baseline on type b) errors.
In fact, en-tropy sampling and margin sampling even showed adecrease in performance compared to random sam-pling.
For AL to work well on noisy data, we needto identify and remove this type of annotation noiseduring the AL process.
To the best of our knowl-edge, there is no work on detecting and removingannotation noise by human coders during AL.3 Experimental SetupTo make sure that the data we use in our simula-tion is as close to real-world data as possible, we donot create an artificial data set as done in (Scheinand Ungar, 2007; Reidsma and Carletta, 2008) butuse real data from a WSD task for the German verbdrohen (threaten).1 Drohen has three different wordsenses which can be disambiguated by humans with1The data has been provided by the SALSA project:http://www.coli.uni-saarland.de/projects/salsaa high accuracy.2 This point is crucial to our setup.To control the amount of noise in the data, we needto be sure that the initial data set is noise-free.For classification we use a maximum entropyclassifier.3 Our sampling method is uncertainty sam-pling (Lewis and Gale, 1994), a standard samplingheuristic for AL where new instances are selectedbased on the confidence of the classifier for predict-ing the appropriate label.
As a measure of uncer-tainty we use Shannon entropy (1) (Zhang and Chen,2002) and the margin metric (2) (Schein and Ungar,2007).
The first measure considers the model?s pre-dictions q for each class c and selects those instancesfrom the pool where the Shannon entropy is highest.?
?cqc log qc (1)The second measure looks at the difference be-tween the largest two values in the prediciton vectorq, namely the two predicted classes c, c?
which are,according to our model, the most likely ones for in-stance xn, and selects those instances where the dif-ference (margin) between the two predicted proba-bilities is the smallest.
We discuss some details ofthis metric in Section 4.Mn = |P (c|xn) ?
P (c?|xn)| (2)The features we use for WSD are a combinationof context features (word token with window size 11and POS context with window size 7), syntactic fea-tures based on the output of a dependency parser4and semantic features based on GermaNet hyper-onyms.
These settings were tuned to the target verbby (Rehbein et al, 2009).
All results reported beloware averages over a 5-fold cross validation.3.1 Simulating Coder Errors in ALBefore starting the AL trials we automatically sepa-rate the 2,500 sentences into test set (498 sentences)and pool (2,002 sentences),5 retaining the overalldistribution of word senses in the data set.
We in-sert a varying amount of noise into the pool data,2In a pilot study where two human coders assigned labels toa set of 100 sentences, the coders agreed on 99% of the data.3http://maxent.sourceforge.net4The MaltParser: http://maltparser.org5The split has been made automatically, the unusual num-bers are caused by rounding errors.45test poolALrand ALbias% errors 0% 0% 30% 30%drohen1-salsa 126 506 524 514Comittment 129 520 522 327Run risk 243 976 956 1161Total 498 2002 2002 2002Table 1: Distribution of word senses in pool and test setsstarting from 0% up to 30% of noise, increasing by2% in each trial.We assess the impact of annotation noise on ac-tive learning in three different settings.
In the firstsetting, we randomly select new instances from thepool (random sampling; rand).
In the second setting,we randomly replace n percent of all labels (from 0to 30) in the pool by another label before startingthe active learning trial, but retain the distribution ofthe different labels in the pool data (active learningwith random errors); (Table 1, ALrand, 30%).
Inthe third setting we simulate biased decisions by ahuman annotator.
For a certain fraction (0 to 30%)of instances of a particular non-majority class, wesubstitute the majority class label for the gold label,thereby producing a more skewed distribution thanin the original pool (active learning with biased er-rors); (Table 1, ALbias, 30%).For all three settings (rand, ALrand, ALbias) andeach degree of noise (0-30%), we run active learningsimulations on the already annotated data, simulat-ing the annotation process by selecting one new, pre-labelled instance per trial from the pool and, insteadof handing them over to a human coder, assigningthe known (possibly erroneous) label to the instanceand adding it to the training set.
We use the samesplit (test, pool) for all three settings and all degreesof noise, with identical test sets for all trials.3.2 ResultsFigure 1 shows active learning curves for the differ-ent settings and varying degrees of noise.
The hori-zontal black line slightly below 0.5 accuracy showsthe majority baseline (the performance obtainedwhen always assigning the majority class).
For alldegrees of randomly inserted noise, active learning(ALrand) outperforms random sampling (rand) at anearly stage in the learning process.
Looking at thebiased errors (ALbias), we see a different picture.With a low degree of noise, the curves for ALrandand ALbias are very similar.
When inserting morenoise, performance for ALbias decreases, and witharound 20% of biased errors in the pool AL performsworse than our random sampling baseline.
In therandom noise setting (ALrand), even after inserting30% of errors AL clearly outperforms random sam-pling.
Increasing the size of the seed data reducesthe effect slightly, but does not prevent it (not shownhere due to space limitations).
This confirms thefindings that under certain circumstances AL per-forms worse than random sampling (Dang, 2004;Schein and Ungar, 2007; Rehbein et al, 2010).
Wecould also confirm Schein and Ungar (2007)?s obser-vation that margin sampling is less sensitive to cer-tain types of noise than entropy sampling (Table 2).Because of space limitations we only show curvesfor margin sampling.
For entropy sampling, the gen-eral trend is the same, with results being slightlylower than for margin sampling.4 Detecting Annotation NoiseUncertainty sampling using the margin metric se-lects instances for which the difference betweenclassifier predictions for the two most probableclasses c, c?
is very small (Section 3, Equation 2).When selecting unlabelled instances from the pool,this metric picks examples which represent regionsof uncertainty between classes which have yet to belearned by the classifier and thus will advance thelearning process.
Our human coder, however, is notthe perfect oracle assumed in most AL simulations,and might also assign incorrect labels.
The filter ap-proach has two objectives: a) to detect incorrect la-bels assigned by human coders, and b) to preventthe hard cases (following the terminology of Kle-banov et al (2008)) from becoming part of the train-ing data.We proceed as follows.
Our approach makes useof the limited set of seed data S and uses heuris-tics to detect unreliably annotated instances.
Weassume that the instances in S have been validatedthoroughly.
We train an ensemble of classifiers Eon subsets of S, and use E to decide whether or nota newly annotated instance should be added to theseed.46error=2%Training sizeAccuracy0 250 600 9500.40.50.60.70.8randal_randal_biaserror=6%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=10%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=14%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=18%Training sizeAccuracy0 250 600 9500.40.50.60.70.8randal_randal_biaserror=22%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=26%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=30%Training sizeAccuracy0 250 600 9500.40.50.60.70.8Figure 1: Active learning curves for varying degrees of noise, starting from 0% up to 30% for a training size up to1200 instances (solid circle (black): random sampling; filled triangle point-up (red): AL with random errors; cross(green): AL with biased errors)47filter % error 0 4 8 12 16 20 24 28 30- rand 0.763 0.752 0.736 0.741 0.726 0.708 0.707 0.677 0.678entropy - ALrand 0.806 0.786 0.779 0.743 0.752 0.762 0.731 0.724 0.729entropy y ALrand 0.792 0.786 0.777 0.760 0.771 0.748 0.730 0.729 0.727margin - ALrand 0.795 0.795 0.782 0.771 0.758 0.755 0.737 0.719 0.708margin y ALrand 0.800 0.785 0.773 0.777 0.765 0.766 0.734 0.735 0.718entropy - ALbias 0.806 0.793 0.759 0.748 0.702 0.651 0.625 0.630 0.622entropy y ALbias 0.802 0.781 0.777 0.735 0.702 0.678 0.687 0.624 0.616margin - ALbias 0.795 0.789 0.770 0.753 0.706 0.684 0.656 0.634 0.624margin y ALbias 0.787 0.781 0.787 0.768 0.739 0.700 0.671 0.653 0.651Table 2: Accuracy for the different sampling methods without and with filtering after adding 500 instances to the seeddataThere are a number of problems with this ap-proach.
First, there is the risk of overfitting S. Sec-ond, we know that classifier accuracy in the earlyphase of AL is low.
Therefore, using classifier pre-dictions at this stage to accept or reject new in-stances could result in poor choices that might harmthe learning proceess.
To avoid this and to gener-alise over S to prevent overfitting, we do not directlytrain our ensemble on instances from S. Instead, wecreate new feature vectors Fgen on the basis of thefeature vectors Fseed in S. For each class in S, weextract all attribute-value pairs from the feature vec-tors for this particular class.
For each class, we ran-domly select features (with replacement) from Fseedand combine them into a new feature vector Fgen,retaining the distribution of the different classes inthe data.
As a result, we obtain a more general set offeature vectors Fgen with characteristic features be-ing distributed more evenly over the different featurevectors.In the next step we train n = 5 maximum en-tropy classifiers on subsets of Fgen, excluding theinstances last annotated by the oracle.
Each subsetis half the size of the current S. We use the ensembleto predict the labels for the new instances and, basedon the predictions, accept or reject these, followingthe two heuristics below (also see Figure 2).1.
If all n ensemble classifiers agree on one labelbut disagree with the oracle ?
reject.2.
If the sum of the margins predicted by the en-semble classifiers is below a particular thesholdtmargin ?
reject.The threshold tmargin was set to 0.01, based on aqualitative data analysis.AL with Filtering:Input: annotated seed data S,unannotated pool PAL loop:?
train classifier C on S?
let C predict labels for data in P?
select new instances from P according tosampling method, hand over to oracle forannotationRepeat: after every c new instancesannotated by the oracle?
for each class in S, extract sets offeatures Fseed?
create new, more general feature vectorsFgen from this set (with replacement)?
train an ensemble E of n classifiers ondifferent subsets of FgenFiltering Heuristics:?
if all n classifier in E agree on labelbut disagree with oracle:?
remove instance from seed?
if margin is less than threshold tmargin:?
remove instance from seedUntil doneFigure 2: Heuristics for filtering unreliable data points(parameters used: initial seed size: 9 sentences, c = 10,n = 5, tmargin = 0.01)48In each iteration of the AL process, one new in-stance is selected using margin sampling.
The in-stance is presented to the oracle who assigns a label.Then the instance is added to the seed data, thus in-fluencing the selection of the next data point to beannotated.
After 10 new instances have been added,we apply the filter technique which finally decideswhether the newly added instances will remain inthe seed data or will be removed.Figure 3 shows learning curves for the filter ap-proach.
With increasing amount of errors in thepool, a clear pattern emerges.
For both samplingmethods (ALrand, ALbias), the filtering step clearlyimproves results.
Even for the noisier data sets withup to 26% of errors, ALbias with filtering performsat least as well as random sampling.4.1 Error AnalysisNext we want to find out what kind of errors thesystem could detect.
We want to know whether theapproach is able to detect the errors previously in-serted into the data, and whether it manages to iden-tify hard cases representing true ambiguities.To answer these questions we look at one fold ofthe ALbias data with 10% of noise.
In 1,200 AL it-erations the system rejected 116 instances (Table 3).The major part of the rejections was due to the ma-jority vote of the ensemble classifiers (first heuris-tic, H1) which rejects all instances where the en-semble classifiers agree with each other but disagreewith the human judgement.
Out of the 105 instancesrejected by H1, 41 were labelled incorrectly.
Thismeans that we were able to detect around half of theincorrect labels inserted in the pool.11 instances were filtered out by the marginthreshold (H2).
None of these contained an incor-errors inserted in pool 173err.
instances selected by AL 93instances rejected by H1+H2 116instances rejected by H1 105true errors rejected by H1 41instances rejected by H2 11true errors rejected by H2 0Table 3: Error analysis of the instances rejected by thefiltering approachrect label.
On first glance H2 seems to be more le-nient than H1, considering the number of rejectedsentences.
This, however, could also be an effect ofthe order in which we apply the filters.The different word senses are evenly distributedover the rejected instances (H1: Commitment 30,drohen1-salsa 38, Run risk 36; H2: Commitment 3,drohen1-salsa 4, Run risk 4).
This shows that thereis less uncertainty about the majority word sense,Run risk.It is hard to decide whether the correctly labelledinstances rejected by the filtering method wouldhave helped or hurt the learning process.
Simplyadding them to the seed data after the conclusionof AL would not answer this question, as it wouldmerely tell us whether they improve classificationaccuracy further, but we still would not know whatimpact these instances would have had on the selec-tion of instances during the AL process.5 ConclusionsThis paper shows that certain types of annotationnoise cause serious problems for active learning ap-proaches.
We showed how biased coder decisionscan result in an accuracy for AL approaches whichis below the one for random sampling.
In this case,it is necessary to apply an additional filtering stepto remove the noisy data from the training set.
Wepresented an approach based on a resampling of thefeatures in the seed data and guided by an ensembleof classifiers trained on the resampled feature vec-tors.
We showed that our approach is able to detecta certain amount of noise in the data.Future work should focus on finding optimal pa-rameter settings to make the filtering method morerobust even for noisier data sets.
We also plan to im-prove the filtering heuristics and to explore furtherways of detecting human coder errors.
Finally, weplan to test our method in a real-world annotationscenario.6 AcknowledgmentsThis work was funded by the German ResearchFoundation DFG (grant PI 154/9-3).
We would liketo thank the anonymous reviewers for their helpfulcomments and suggestions.49error=2%Training sizeAccuracy0 250 600 9500.40.50.60.70.8randALrandALrand_fALbiasALbias_ferror=6%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=10%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=14%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=18%Training sizeAccuracy0 250 600 9500.40.50.60.70.8randALrandALrand_fALbiasALbias_ferror=22%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=26%Training sizeAccuracy0 250 600 9500.40.50.60.70.8error=30%Training sizeAccuracy0 250 600 9500.40.50.60.70.8Figure 3: Active learning curves for varying degrees of noise, starting from 0% up to 30% for a training size up to1200 instances (solid circle (black): random sampling; open circle (red): ALrand; cross (green): ALrand with filtering;filled triangle point-up (black): ALbias; plus (blue): ALbias with filtering)50ReferencesBeata Beigman Klebanov and Eyal Beigman.
2009.From annotator agreement to noise models.
Compu-tational Linguistics, 35:495?503, December.Beata Beigman Klebanov, Eyal Beigman, and DanielDiermeier.
2008.
Analyzing disagreements.
In Pro-ceedings of the Workshop on Human Judgements inComputational Linguistics, HumanJudge ?08, pages2?7, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Yee Seng Chan and Hwee Tou Ng.
2007.
Domain adap-tation with active learning for word sense disambigua-tion.
In Proceedings of ACL-2007.Jinying Chen, Andrew Schein, Lyle Ungar, and MarthaPalmer.
2006.
An empirical study of the behavior ofactive learning for word sense disambiguation.
In Pro-ceedings of NAACL-2006, New York, NY.Hoa Trang Dang.
2004.
Investigations into the role oflexical semantics in word sense disambiguation.
PhDdissertation, University of Pennsylvania, Pennsylva-nia, PA.Gholamreza Haffari and Anoop Sarkar.
2009.
Activelearning for multilingual statistical machine transla-tion.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 1 - Volume 1, pages 181?189.
Association for Computational Linguistics.Rebecca Hwa.
2004.
Sample selection for statisticalparsing.
Computational Linguistics, 30(3):253?276.Florian Laws and H. Schu?tze.
2008.
Stopping crite-ria for active learning of named entity recognition.In Proceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), Manch-ester, UK, August.David D. Lewis and William A. Gale.
1994.
A sequentialalgorithm for training text classifiers.
In Proceedingsof ACM-SIGIR, Dublin, Ireland.Grace Ngai and David Yarowsky.
2000.
Rule writingor annotation: cost-efficient resource usage for basenoun phrase chunking.
In Proceedings of the 38th An-nual Meeting on Association for Computational Lin-guistics, pages 117?125, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Miles Osborne and Jason Baldridge.
2004.
Ensemble-based active learning for parse selection.
In Proceed-ings of HLT-NAACL 2004.Ines Rehbein, Josef Ruppenhofer, and Jonas Sunde.2009.
Majo - a toolkit for supervised word sense dis-ambiguation and active learning.
In Proceedings ofthe 8th Workshop on Treebanks and Linguistic Theo-ries (TLT-8), Milano, Italy.Ines Rehbein, Josef Ruppenhofer, and Alexis Palmer.2010.
Bringing active learning to life.
In Proceed-ings of the 23rd International Conference on Compu-tational Linguistics (COLING 2010), Beijing, China.Dennis Reidsma and Jean Carletta.
2008.
Reliabilitymeasurement without limits.
Computational Linguis-tics, 34:319?326.Eric Ringger, Peter McClanahan, Robbie Haertel, GeorgeBusby, Marc Carmen, James Carroll, Kevin Seppi, andDeryle Lonsdale.
2007.
Active learning for part-of-speech tagging: Accelerating corpus annotation.
InProceedings of the Linguistic Annotation Workshop,Prague.Andrew I. Schein and Lyle H. Ungar.
2007.
Active learn-ing for logistic regression: an evaluation.
MachineLearning, 68:235?265.Dan Shen, Jie Zhang, Jian Su, Guodong Zhou, and Chew-Lim Tan.
2004.
Multi-criteria-based active learningfor named entity recognition.
In Proceedings of the42nd Annual Meeting on Association for Computa-tional Linguistics, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Katrin Tomanek and Udo Hahn.
2009.
Reducing classimbalance during active learning for named entity an-notation.
In Proceedings of the 5th International Con-ference on Knowledge Capture, Redondo Beach, CA.Simon Tong and Daphne Koller.
1998.
Support vectormachine active learning with applications to text clas-sification.
In Proceedings of the Seventeenth Interna-tional Conference on Machine Learning (ICML-00),pages 287?295.Cha Zhang and Tsuhan Chen.
2002.
An active learn-ing framework for content-based information retrieval.IEEE Transactions on Multimedia, 4(2):260?268.Jingbo Zhu and Edward Hovy.
2007.
Active learning forword sense disambiguation with methods for address-ing the class imbalance problem.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning, Prague, Czech Republic.Jingbo Zhu, Huizhen Wang, Tianshun Yao, and Ben-jamin K. Tsou.
2008.
Active learning with samplingby uncertainty and density for word sense disambigua-tion and text classification.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics (Coling 2008), Manchester, UK.51
