Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 379?382,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsJAIST: Clustering and Classification based Approachesfor Japanese WSDKiyoaki Shirai Makoto NakamuraJapan Advanced Institute of Science and Technology{kshirai,mnakamur}@jaist.ac.jpAbstractThis paper reports about our three par-ticipating systems in SemEval-2 JapaneseWSD task.
The first one is a clusteringbased method, which chooses a sense for,not individual instances, but automaticallyconstructed clusters of instances.
The sec-ond one is a classification method, whichis an ordinary SVM classifier with simpledomain adaptation techniques.
The last isan ensemble of these two systems.
Resultsof the formal run shows the second systemis the best.
Its precision is 0.7476.1 IntroductionThis paper reports about our systems inSemEval-2 Japanese Word Sense Disambiguation (WSD)task (Okumura et al, 2010).
This task is a lexi-cal sample task for Japanese WSD and has the fol-lowing two characteristics.
First, a balanced word-sense tagged corpus is used for the task.
Since itconsists of sub-corpora of several domains or gen-res, domain adaptation might be required.
Second,the task takes into account not only the instanceshaving a sense in the given set but also the in-stances having a sense not found in the set (called?new sense?).
Participants are required to identifynew senses of words in this task.The second characteristics of the task is mainlyconsidered in our system.
A clustering basedapproach is investigated to identify new senses.Our system first constructs a set of clusters ofgiven word instances using unsupervised cluster-ing techniques.
This is motivated by the fact thatthe new sense is not defined in the dictionary, andsense induction without referring to the dictionarywould be required.
Clusters obtained would besets of instances having the same sense, and someof them would be new sense instances.
Then eachcluster is judged whether instances in it have a newsense or not.
An ordinary classification-based ap-proach is also considered.
That is, WSD classifiersare trained by a supervised learning algorithm.Furthermore, simple techniques considering gen-res of sub-corpora are incorporated into both ourclustering and classification based systems.The paper continues as follows, Section 2 de-scribes our three participating systems, JAIST-1,JAIST-2 and JAIST-3.
The results of these systemsare reported and discussed in Section 3.
Finally weconclude the paper in Section 4.2 Systems2.1 JAIST-1: Clustering based WSD SystemJAIST-1 was developed by a clustering basedmethod.
The overview of the system is shown inFigure 1.
It consists of two procedures: (A) clus-ters of word instances are constructed so that theinstances of the same sense are merged, (B) thensimilarity between a cluster and a sense in a dic-tionary is measured in order to determine sensesof instances in each cluster.Corpus??????
(service)S  ????????????
?help that people who work in ashop give youS  ????????????
?help that is provided by abusiness to customersS  ??
?volunteer workDictionaryinstance(sentence)(A) (B)213Figure 1: Overview of JAIST-12.1.1 Clustering of Word InstancesAs previous work applying clustering techniquesfor sense induction (Schu?tze, 1998; Agirre andSoroa, 2007), each instance is represented by afeature vector.
In JAIST-1, the following 4 vectorsare used for clustering.Collocation Vector This vector reflects colloca-tion including the target instance.
Words or POSsappearing just before and after the target instanceare used as features, i.e.
they correspond to one di-mension in the vector.
The weight of each featureis 1 if the feature exists for the instance, or 0 if not.Context Vector The vector reflects words in thecontext of the target instance.
All content wordsappearing in the context are used as features.
Thewindow size of the context is set to 50.
Further-more, related words are also used as features to en-379rich the information in the vector.
Related wordsare defined as follows: first topics of texts are au-tomatically derived by Latent Dirichlet Allocation(LDA) (Blei et al, 2003), then words which are themost closely associated with each topic are formedinto a ?related word set?.
If one word in a relatedword set appears in the context, other words inthat set alo have a positive weight in the vector.More concretely, the weight of each feature is de-termined to be 1 if the word appears in the contextor 0.5 if the word does not appear but is in the re-lated word set.Association Vector Similarly to context vector,this reflects words in the context of the target in-stance, but data sparseness is alleviated in a differ-ent manner.
In advance, the co-occurrence matrixA is constructed from a corpus.
Each row and col-umn in A corresponds to one of the most frequent10,000 content words.
Each element ai,jin thematrix is P (wi|wj), conditional probability repre-senting how likely it is that two words wiand wjwill occur in the same document.
Now j-th col-umn in A can be regarded as the co-occurrencevector of wj, ~o(wj).
Association vector is a nor-malized vector of sum of ~o(wj) for all words inthe context.Topic Vector Unlike other vectors, this vector re-flects topics of texts.
The topics zjautomaticallyderived by PLSI (Probabilistic Latent Semantic In-dexing) are used as features.
The weight for zjinthe vector is P (zj|di) estimated by Folding-in al-gorithm (Hofmann, 1999), where diis the docu-ment containing the instance.
Topic vector is mo-tivated by the well-known fact that word senses arehighly associated with the topics of documents.Target instances are clustered by the agglomera-tive clustering algorithm.
Similarities between in-stances are calculated by cosine measure of vec-tors.
Furthermore, pairs of instances in differentgenre sub-corpora are treated as ?cannot-link?, sothat they will not be merged into the same cluster.Clustering procedure is stopped when the num-ber of instances in a cluster become more than athreshold Nc.
Ncis set to 5 in the participatingsystem.The clustering is performed 4 times using 4 dif-ferent feature vectors.
Then the best one is chosenfrom the 4 sets of clusters obtained.
A set of clus-ter C (={Ci}) is evaluated by E(C)E(C) =?icoh(Ci) (1)where ?cohesiveness?
coh(Ci) for each cluster Ciis defined by (2).coh(Ci) =1|Ci||Ci|?j=1rel-sim(~vij, ~gi)=1|Ci||Ci|?j=1sim(~vij, ~gi)maxjsim(~vij, ~gi)(2)~vijis an instance vector in the cluster Ci, while ~giis an average vector of Ci.
rel-sim(~vij, ~gi) meansthe relative similarity between the instance vectorand average vector.
Intuitively, coh(Ci) evaluateshow likely instances in the cluster are similar eachother.
C such that E(C) is maximum is chosen asthe final set of clusters.2.1.2 Similarity between Clusters and SensesAfter clustering, similarity between a cluster Ciand a sense Sjin the dictionary, sim(Ci, Sj), iscalculated for WSD.
Ciand Sjare represented bycluster vector ~ciand sense vector ~sj, respectively.Then cosine measure between these two vectors iscalculated as sim(Ci, Sj).The cluster vector ~ciis defined as (3):~ci=1N?eik?Ci?tl?eik~o(tl) (3)In (3), eikstands for an instance in the cluster Ci,tlwords appearing in the context of eik, ~o(tl) co-occurrence vector of tl(similar one used in asso-ciation vector), and N the constant for normaliza-tion.
So ~ciis similar to association vector, but theco-occurrence vectors of words in the contexts ofall instances in the cluster are summed.The sense vector ~sjis defined as in (4).~sj=1N??
?tk?Dj~o(tk) +?tl?Ejwe?
~o(tl)??
(4)Djstands for definition sentences of the sense Sjin the Japanese dictionary Iwanami Kokugo Jiten(the sense inventory in this task), while Eja set ofexample sentences of Sj.
Here Ejincludes bothexample sentences from the dictionary and onesexcerpted from a sense-tagged corpus, the train-ing data of this task.
weis the parameter puttingmore weight on words in example sentences thanin definition sentences.
We set we= 2.0 throughthe preliminary investigation.Based on sim(Ci, Sj), the system judgeswhether the cluster is a collection of new380sense instances.
Suppose that MaxSimiismaxjsim(Ci, Sj), the maximum similarity be-tween the cluster and the sense.
If MaxSimiissmall, the cluster Ciis not similar to any definedsenses, so instances in Cicould have a new sense.The system regards that the sense of instances inCiis new when MaxSimiis less than a thresh-old Tns.
Otherwise, it regards the sense of in-stances in Cias the most similar sense, Sjsuchthat j = argmaxjsim(Ci, Sj).The threshold Tnsfor each target word is deter-mined as follows.
First the training data is equallysubdivided into two halves, the development dataDdevand the training data Dtr.
Next, JAIST-1 isrun for instances in Ddev, while example sentencesin Dtrare used as Ejin (4) when sense vectors areconstructed.
For words where new sense instancesexist in Ddev, Tnsis optimized for the accuracyof new sense detection.
For words where no newsense instances are found in Ddev, Tnsis deter-mined by the minimum of MaxSimias follows:Tns= (miniMaxSimi) ?
?
(5)Since even the cluster of which MaxSimiis min-imum represents not a new but a defined sense, theminimum of MaxSimiis decreased by ?.
To de-termine ?, the ratiosMaxSimiof clusters of new sensesMaxSimiof clusters of defined senses(6)are investigated for 5 words1.
Since we found theratios are more than 0.95, we set ?
to 0.95.2.2 JAIST-2: SVM Classifier with SimpleDomain AdaptationOur second system JAIST-2 is the classificationbased method.
It is a WSD classifier trained bySupport Vector Machine (SVM).
SVM is widelyused for various NLP tasks including JapaneseWSD (Shirai and Tamagaki, 2004).
In this system,new sense is treated as one of the sense classes.Thus it would never choose ?new sense?
for anyinstances when no new sense instance is found inthe training data.
We used the LIBSVM package2to train the SVM classifiers.
Linear kernel is usedwith default parameters.The following conventional features of WSDare used for training the SVM classifiers.1Among 50 target words in this task, there exist newsense instances of only ?kanou?
(possibility) in Ddev.
So wechecked 4 more words, other than target words.2http://www.csie.ntu.edu.tw/?cjlin/libsvm/?
W (0),W (?1),W (?2),W (+1),W (+2)P (?1), P (?2), P (+1), P (+2)Words and their POSs appearing before or af-ter a target instance.
A number in parenthesesindicates the position of a word from a targetinstance.
W (0) means a target instance itself.?
W (?2)&W (?1),W (+1)&W (+2),W (?1)&W (+1)P (?2)&P (?1), P (+1)&P (+2), P (?1)&P (+1)Pairs of words (or their POSs) near a targetinstance.?
Base form of content words appearing in thecontext (bag-of-words).The data used in this task is a set of documentswith 4 different genre codes: OC (Web page),OW (white paper), PB (book) and PN (newspa-per).
The training data consists of documents of3 genres OW, PB and PN, while the test data con-tains all 4 genres.
Considering domain adaptation,each feature fiis represented as fi+g when SVMclassifiers are trained.
g is one of the genre codes{OW,PB,PN} if fiis derived from the docu-ments of only one genre g in the training data, oth-erwise g is ?multi?.
For instances in the test data,only features fi+gtand fi+multi are used, wheregtis the genre code of the document of the targetinstance.
If gtis OC (which is not included in thetraining data), however, all features are used.
Theabove method aims at distinguishing genre intrin-sic features and improving the WSD performanceby excluding features which might be associatedwith different genres.2.3 JAIST-3: Ensemble of Two SystemsThe third system combines clustering basedmethod (JAIST-1) and classification based method(JAIST-2).
The basic idea is that JAIST-1 be usedonly for reliable clusters, otherwise JAIST-2 isused.
Here ?reliable cluster?
means a cluster suchthat MaxSimiis high.
The greater the similar-ity between the cluster and the sense is, the morelikely the chosen sense is correct.
Furthermore,JAIST-1 is used for new sense detection.
The de-tailed procedure in JAIST-3 is:1.
If JAIST-1 judges a cluster to be a collectionof new sense instances, output ?new sense?for instances in that cluster.2.
For instances in the top Nclclusters ofMaxSimi,output senses chosen by JAIST-1.3.
Otherwise output senses chosen by JAIST-2.381For the optimization of Ncl, Ddevand Dtr, eachis a half of the training data described in Subsec-tion 2.1, are used.
Dtris used for training SVMclassifiers (JAIST-2).
Then Nclis determined sothat the precision of WSD on Ddevis optimized.In the participating system, Nclis set to 1.3 EvaluationTable 1 shows the results of our participating sys-tems and the baseline system MFS, which alwaysselects the most frequent sense in the trainingdata.
The column WSD reveals the precision (P)of word sense disambiguation, while the columnNSD shows accuracy (A), precision (P) and recall(R) of new sense detection.Table 1: ResultsWSD NSDP A P RMFS 0.6896 0.9844 0 0JAIST-1 0.6864 0.9512 0.0337 0.0769JAIST-2 0.7476 0.9872 1 0.1795JAIST-3 0.7208 0.9532 0.0851 0.2051JAIST-1 is the clustering based method.
Perfor-mance of the clustering is also evaluated: Puritywas 0.9636, Inverse-Purity 0.1336 and F-measure0.2333.
Although this system was designed fornew sense detection, it seems not to work well.It could correctly find only three new sense in-stances.
The main reason is that there were fewinstances of the new sense in the test data.
Among2,500 instances (50 instances of each word, for 50target word), only 39 instances had the new sense.Our system supposes that considerable number ofnew sense instances exist in the corpus, and tries togather them into clusters.
However, JAIST-1 wasable to construct only one cluster containing mul-tiple new sense instances.
The proposed method isinadequate for new sense detection when the num-ber of new sense instances is quite small.For domain adaptation, features which are in-trinsic to different genres were excluded for testinstances in JAIST-2.
When we trained the systemusing all features, its precision was 0.7516, whichis higher than that of JAIST-2.
Thus our methoddoes not work at all.
This might be caused by re-moving features that were derived from differentgenre sub-corpora, but effective for WSD.
Moresophisticated ways to remove ineffective featureswould be required.JAIST-3 is the ensemble of JAIST-1 and JAIST-2.
Although a little improvement is found by com-bining two different systems in our preliminary ex-periments, however, the performance of JAIST-3was worse than JAIST-2 because of the low per-formance of JAIST-1.
We compared WSD pre-cision of three systems for 50 individual targetwords, and found that JAIST-2 is almost alwaysthe best.
The only exceptional case was the targetword ?ookii?(big).
For this adjective, the precisionof JAIST-1, JAIST-2 and JAIST-3 were 0.74, 0.16and 0.18, respectively.
The precision of SVM clas-sifiers (JAIST-2) is quite bad because of the differ-ence of text genres.
All 50 test instances of thisword were excerpted from Web sub-corpus, whichwas not included in the training data.
Furthermore,word sense distributions of test and training datawere totally different.
JAIST-1 works better insuch a case.
Thus clustering based method mightbe an alternative method for WSDwhen sense dis-tribution in the test data is far from the trainingdata.4 ConclusionThe paper reports the participating systems inSemEval-2 Japanese WSD task.
Clustering basedmethod was designed for new sense detection,however, it was ineffective when there were fewnew sense instances.
In future, we would like toexamine the performance of our method when it isapplied to a corpus including more new senses.ReferencesEneko Agirre and Aitor Soroa.
2007.
Semeval-2007task 02: Evaluating word sense induction and dis-crimination systems.
In Proceedings of the 4thInternational Workshop on Semantic Evaluations,pages 7?12.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research, 3:993?1022.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
In Proceedings of the SIGIR, pages 50?57.Manabu Okumura, Kiyoaki Shirai, Kanako Komiya,and Hikaru Yokono.
2010.
Semeval-2010 task:Japanese WSD.
In Proceedings of the SemEval-2010: 5th International Workshop on SemanticEvaluations.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123.Kiyoaki Shirai and Takayuki Tamagaki.
2004.
Wordsense disambiguation using heterogeneous languageresources.
In Proceedings of the First IJCNLP,pages 614?619.382
