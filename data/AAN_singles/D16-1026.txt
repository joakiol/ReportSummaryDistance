Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 268?277,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsZero-Resource Translation withMulti-Lingual Neural Machine TranslationOrhan Firat?Middle East Technical Universityorhan.firat@ceng.metu.edu.trBaskaran SankaranIBM T.J. Watson Research CenterYaser Al-onaizanIBM T.J. Watson Research CenterFatos T. Yarman VuralMiddle East Technical UniversityKyunghyun ChoNew York UniversityAbstractIn this paper, we propose a novel finetuningalgorithm for the recently introduced multi-way, multilingual neural machine translatethat enables zero-resource machine transla-tion.
When used together with novel many-to-one translation strategies, we empiricallyshow that this finetuning algorithm allows themulti-way, multilingual model to translate azero-resource language pair (1) as well as asingle-pair neural translation model trainedwith up to 1M direct parallel sentences of thesame language pair and (2) better than pivot-based translation strategy, while keeping onlyone additional copy of attention-related pa-rameters.1 IntroductionA recently introduced neural machine transla-tion (Forcada and N?eco, 1997; Kalchbrenner andBlunsom, 2013; Sutskever et al, 2014; Cho et al,2014) has proven to be a platform for new opportu-nities in machine translation research.
Rather thanword-level translation with language-specific pre-processing, neural machine translation has found towork well with statistically segmented subword se-quences as well as sequences of characters (Chunget al, 2016; Luong and Manning, 2016; Sennrichet al, 2015b; Ling et al, 2015).
Also, recentworks show that neural machine translation providesa seamless way to incorporate multiple modalities?
Work carried out while the author was at IBM Research.other than natural language text in translation (Lu-ong et al, 2015a; Caglayan et al, 2016).
Further-more, neural machine translation has been foundto translate between multiple languages, achievingbetter translation quality by exploiting positive lan-guage transfer (Dong et al, 2015; Firat et al, 2016;Zoph and Knight, 2016).In this paper, we conduct in-depth investiga-tion into the recently proposed multi-way, multilin-gual neural machine translation (Firat et al, 2016).Specifically, we are interested in its potential forzero-resource machine translation, in which theredoes not exist any direct parallel examples betweena target language pair.
Zero-resource translation hasbeen addressed by pivot-based translation in tradi-tional machine translation research (Wu and Wang,2007; Utiyama and Isahara, 2007; Habash and Hu,2009), but we explore a way to use the multi-way,multilingual neural model to translate directly froma source to target language.In doing so, we begin by studying different trans-lation strategies available in the multi-way, multi-lingual model in Sec.
3?4.
The strategies includea usual one-to-one translation as well as variantsof many-to-one translation for multi-source transla-tion (Zoph and Knight, 2016).
We empirically showthat the many-to-one strategies significantly outper-form the one-to-one strategy.We move on to zero-resource translation by firstevaluating a vanilla multi-way, multilingual modelon a zero-resource language pair, which revealedthat the vanilla model cannot do zero-resource trans-lation in Sec.
6.1.
Based on the many-to-one strate-gies we proposed earlier, we design a novel finetun-268ing strategy that does not require any direct paral-lel corpus between a target, zero-resource languagepair in Sec.
5.2, which uses the idea of generating apseudo-parallel corpus (Sennrich et al, 2015a).
Thisstrategy makes an additional copy of the attentionmechanism and finetunes only this small set of pa-rameters.Large-scale experiments with Spanish, Frenchand English show that the proposed finetuning strat-egy allows the multi-way, multilingual neural trans-lation model to perform zero-resource translation aswell as a single-pair neural translation model trainedwith up to 1M true parallel sentences.
This resultre-confirms the potential of the multi-way, multilin-gual model for low/zero-resource language transla-tion, which was earlier argued by Firat et al (2016).2 Multi-Way, MultilingualNeural Machine TranslationRecently Firat et al (2016) proposed an extensionof attention-based neural machine translation (Bah-danau et al, 2015) that can handle multi-way, mul-tilingual translation with a shared attention mech-anism.
This model was designed to handle multi-ple source and target languages.
In this section, webriefly overview this multi-way, multilingual model.For more detailed exposition, we refer the reader to(Firat et al, 2016).2.1 Model DescriptionThe goal of multi-way, multilingual model is tobuild a neural translation model that can translate asource sentence given in one of N languages intoone of M target languages.
Thus to handle thoseN source and M target languages, the model con-sists of N encoders and M decoders.
Unlike theselanguage-specific encoders and decoders, only a sin-gle attention mechanism is shared across all M ?Nlanguage pairs.Encoder An encoder for the n-th source languagereads a source sentence X = (x1, .
.
.
, xTx) as asequence of linguistic symbols and returns a set ofcontext vectors Cn = {hn1 , .
.
.
,hnTx}.
The encoderis usually implemented as a bidirectional recurrentnetwork (Schuster and Paliwal, 1997), and each con-text vector hnt is a concatenation of the forward andreverse recurrent networks?
hidden states at time t.Without loss of generality, we assume that the di-mensionalities of the context vector for all sourcelanguages are all same.Decoder and Attention Mechanism A decoderfor the m-th target language is a conditional recur-rent language model (Mikolov et al, 2010).
At eachtime step t?, it updates its hidden state byzmt?
= ?m(zmt?
?1, y?mt?
?1, cmt?
),based on the previous hidden state zmt?
?1, previoustarget symbol y?mt?
?1 and the time-dependent contextvector cmt?
.
?m is a gated recurrent unit (GRU, (Choet al, 2014)).The time-dependent context vector is computedby the shared attention mechanism as a weightedsum of the context vectors from the encoder Cn:cmt?
= UTx?t=1?m,nt,t?
hnt + b, (1)where?m,nt,t?
?
exp(fscore(Wnhnt ,Wmzmt?
?1, y?mt??1)).
(2)The scoring function fscore returns a scalar and is im-plemented as a feedforward neural network with asingle hidden layer.
For more variants of the atten-tion mechanism for machine translation, see (Luonget al, 2015b).The initial hidden state of the decoder is initial-ized aszm0 = ?minit(Wnhnt ).
(3)With the new hidden state zmt?
, the probability dis-tribution over the next symbol is computed byp(yt = w|y?<t, Xn) ?
exp(gmw (zmt , cmt ,Emy [y?t?1]),(4)where gmw is a decoder specific parametric func-tion that returns the unnormalized probability for thenext target symbol being w.2.2 LearningTraining this multi-way, multilingual model doesnot require multi-way parallel corpora but only a269set of bilingual corpora.
For each bilingual pair, theconditional log-probability of a ground-truth transla-tion given a source sentence is maximize by adjust-ing the relevant parameters following the gradient ofthe log-probability.3 Translation Strategies3.1 One-to-One TranslationIn the original paper by Firat et al (2016), only onetranslation strategy was evaluated, that is, one-to-one translation.
This one-to-one strategy works ona source sentence given in one language by takingthe encoder of that source language, the decoder ofa target language and the shared attention mecha-nism.
These three components are glued together asif they form a single-pair neural translation modeland translates the source sentence into a target lan-guage.We however notice that this is not the only transla-tion strategy available with the multi-way, multilin-gual model.
As we end up with multiple encoders,multiple decoders and a shared attention mecha-nism, this model naturally enables us to exploit asource sentence given in multiple languages, lead-ing to a many- to-one translation strategy which wasproposed recently by Zoph and Knight (2016) in thecontext of neural machine translation.Unlike (Zoph and Knight, 2016), the multi-way,multilingual model is not trained with multi-wayparallel corpora.
This however does not necessar-ily imply that the model cannot be used in this way.In the remainder of this section, we propose twoalternatives for doing multi-source translation withthe multi-way, multilingual model, which eventuallypave the way towards zero-resource translation.3.2 Many-to-One TranslationIn this section, we consider a case where a sourcesentence is given in two languages, X1 and X2.However, any of the approaches described below ap-plies to more than two source languages trivially.In this multi-way, multilingual model, multi-source translation can be thought of as averagingtwo separate translation paths.
For instance, in thecase of Es+Fr to En, we want to combine Es?Enand Fr?En so as to get a better English translation.We notice that there are two points in the multi-way,multilingual model where this averaging may hap-pen.Early Average The first candidate is to averag-ing two translation paths when computing the time-dependent context vector (see Eq.
(1).)
At each timet in the decoder, we compute a time-dependent con-text vector for each source language, c1t and c2t re-spectively for the two source languages.
In this earlyaveraging strategy, we simply take the average ofthese two context vectors:ct =c1t + c2t2 .
(5)Similarly, we initialize the decoder?s hidden state tobe the average of the initializers of the two encoders:z0 =12(?init(?1init(h1Tx1 )) + ?init(?2init(h2Tx1 ))),(6)where ?init is the decoder?s initializer (see Eq.
(3).
)Late Average Alternatively, we can average thosetwo translation paths (e.g., Es?En and Fr?En) atthe output level.
At each time t, each translation pathcomputes the distribution over the target vocabulary,i.e., p(yt = w|y<t, X1) and p(yt = w|y<t, X2).
Wethen average them to get the multi-source output dis-tribution:p(yt = w|y<t, X1, X2) = (7)12(p(yt = w|y<t, X1) + p(yt = w|y<t)).An advantage of this late averaging strategy over theearly averaging one is that this can work even whenthose two translation paths were not from a singlemultilingual model.
They can be two separatelytrained single-pair models.
In fact, if X1 and X2 aresame and the two translation paths are simply twodifferent models trained on the same language pair?direction, this is equivalent to constructing an en-semble, which was found to greatly improve transla-tion quality (Sutskever et al, 2014; Jean et al, 2015)Early+Late Average The two strategies abovecan be further combined by late-averaging the out-put distributions from the early averaged model andthe late averaged one.
We empirically evaluate thisearly+late average strategy as well.2704 Experiments: Translation Strategies andMulti-Source TranslationBefore continuing on with zero-resource machinetranslation, we first evaluate the translation strate-gies described in the previous section on multi-source translation, as these translation strategiesform a basic foundation on which we extend themulti-way, multilingual model for zero-resourcemachine translation.4.1 SettingsWhen evaluating the multi-source translation strate-gies, we use English, Spanish and French, and focuson a scenario where only En-Es and En-Fr parallelcorpora are available.4.1.1 CorporaEn-Es We combine the following corpora to form34.71m parallel Es-En sentence pairs: UN (8.8m),Europarl-v7 (1.8m), news-commentary-v7 (150k),LDC2011T07-T12 (2.9m) and internal technical-domain data (21.7m).En-Fr We combine the following corpora to form65.77m parallel En-Fr sentence pairs: UN (9.7m),Europarl-v7 (1.9m), news-commentary-v7 (1.2m),LDC2011T07-T10 (1.6m), ReutersUN (4.5m), in-ternal technical-domain data (23.5m) and GigawordR2 (20.66m).Evaluation Sets We use newstest-2012 andnewstest-2013 from WMT as development and testsets, respectively.Monolingual Corpora We do not use any addi-tional monolingual corpus.Preprocessing All the sentences are tokenized us-ing the tokenizer script from Moses (Koehn et al,2007).
We then replace special tokens, such asnumbers, dates and URL?s with predefined markers,which will be replaced back with the original to-kens after decoding.
After using byte pair encoding(BPE, (Sennrich et al, 2015b)) to get subword sym-bols, we end up with 37k, 43k and 45k unique tokensfor English, Spanish and French, respectively.
Fortraining, we only use sentence pairs in which bothsentences are only up to 50 symbols long.See Table 1 for the detailed statistics.# Sents Train Dev?
Test?En-Es 34.71m 3003 3000En-Fr 65.77m 3003 3000En-Es-Fr 11.32m 3003 3000Table 1: Data statistics.
?
: newstest-2012.
?
: newstest-20134.2 Models and TrainingWe start from the code made publicly available as apart of (Firat et al, 2016)1.
We made two changesto the original code.
First, we replaced the decoderwith the conditional gated recurrent network withthe attention mechanism as outlines in (Firat andCho, 2016).
Second, we feed a binary indicator vec-tor of which encoder(s) the source sentence was pro-cessed by to the output layer of each decoder (gmw inEq.
(4)).
Each dimension of the indicator vector cor-responds to one source language, and in the case ofmulti-source translation, there may be more than onedimensions set to 1.We train the following models: four single-pairmodels (Es?En and Fr?En) and one multi-way,multilingual model (Es,Fr,En?Es,Fr,En).
As pro-posed by Firat et al (2016), we share one attentionmechanism for the latter case.Training We closely follow the setup from (Firatet al, 2016).
Each symbol is represented as a 620-dimensional vector.
Any recurrent layer, be it in theencoder or decoder, consists of 1000 gated recurrentunits (GRU, (Cho et al, 2014)), and the attentionmechanism has a hidden layer of 1200 tanh units(fscore in Eq.
(2)).
We use Adam (Kingma and Ba,2015) to train a model, and the gradient at each up-date is computed using a minibatch of at most 80sentence pairs.
The gradient is clipped to have thenorm of at most 1 (Pascanu et al, 2012).
We early-stop any training using the T-B score on a develop-ment set2.4.3 One-to-One TranslationWe first confirm that the multi-way, multilingualtranslation model indeed works as well as single-pair models on the translation paths that were con-sidered during training, which was the major claim1https://github.com/nyu-dl/dl4mt-multi2T-B score is defined as TER?BLEU2 which we found to bemore stable than either TER or BLEU alone for the purpose ofearly-stopping (Zhao and Chen, 2009).271Multi SingleSrc Trgt Dev Test Dev Test(a) Es En 30.73 28.32 29.74 27.48(b) Fr En 26.93 27.93 26.00 27.21(c) En Es 30.63 28.41 31.31 28.90(d) En Fr 22.68 23.41 22.80 24.05Table 2: One-to-one translation qualities using the multi-way,multilingual model and four separate single-pair models.Multi SingleDev Test Dev Test(a) Early 31.89 31.35 ?
?
(b) Late 32.04 31.57 32.00 31.46(c) E+L 32.61 31.88 ?
?Table 3: Many-to-one quality (Es+Fr?En) using three transla-tion strategies.
Compared to Table 2 (a?b) we observe a signif-icant improvement (up to 3+ BLEU), although the model wasnever trained in these many-to-one settings.
The second columnshows the quality by the ensemble of two separate single-pairmodels.in (Firat et al, 2016).
In Table 2, we present the re-sults on four language pair-directions (Es?En andFr?En).It is clear that the multi-way, multilingual modelindeed performs comparably on all the four caseswith less parameters (due to the shared attentionmechanism.)
As observed earlier in (Firat et al,2016), we also see that the multilingual model per-forms better when a target language is English.4.4 Many-to-One TranslationWe consider translating from a pair of source sen-tences in Spanish (Es) and French (Fr) to English(En).
It is important to note that the multilingualmodel was not trained with any multi-way parallelcorpus.
Despite this, we observe that the early aver-aging strategy improves the translation quality (mea-sured in BLEU) by 3 points in the case of the test set(compare Table 2 (a?b) and Table 3 (a).)
We con-jecture that this happens as training the multilingualmodel has implicitly encouraged the model to find acommon context vector space across multiple sourcelanguages.The late averaging strategy however outperformsthe early averaging in both cases of multilingualmodel and a pair of single-pair models (see Ta-ble 3 (b)) albeit marginally.
The best quality wasobserved when the early and late averaging strate-gies were combined at the output level, achieving upto +3.5 BLEU (compare Table 2 (a) and Table 3 (c).
)We emphasize again that there was no multi-wayparallel corpus consisting of Spanish, French andEnglish during training3.
The result presentedin this section shows that the multi-way, multilin-gual model can exploit multiple sources effectivelywithout requiring any multi-way parallel corpus, andwe will rely on this property together with the pro-posed many-to-one translation strategies in the latersections where we propose and investigate zero-resource translation.5 Zero-Resource Translation StrategiesThe network architecture of multi-way, multilingualmodel suggests the potential for translating betweentwo languages without any direct parallel corpusavailable.
In the setting considered in this paper(see Sec.
4.1,) these translation paths correspond toEs?Fr, as only parallel corpora used for trainingwere Es?En and Fr?En.The most naive approach for translating along azero-resource path is to simply treat it as any otherpath that was included as a part of training.
Thiscorresponds to the one-to-one strategy from Sec.
3.1.In our experiments, it however turned out that thisnaive approach does not work at all, as can be seenin Table 4 (a).In this section, we investigate this potential ofzero-resource translation with the multi-way, mul-tilingual model in depth.
More specifically, wepropose a number of approaches that enable zero-resource translation without requiring any additionalbilingual or multi-way corpora.5.1 Pivot-based TranslationThe first set of approaches exploits the fact that thetarget zero-resource translation path can be decom-posed into a sequence of high-resource translationpaths (Wu and Wang, 2007; Utiyama and Isahara,2007; Habash and Hu, 2009).
For instance, in our3We do not assume the availability of annotation on multi-way parallel sentence pairs.
It is likely that there will be somesentence (or a set of very close variants of a single sentence)translated into multiple languages (eg.
Europarl).
One may de-cide to introduce a mechanism for exploiting these (Zoph andKnight, 2016), or as we present here, it may not be necessary atall to do so.272case, Es?Fr can be decomposed into a sequence ofEs?En and En?Fr.
In other words, we translate asource sentence (Es) into a pivot language (En) andthen translate the English translation into a targetlanguage (Fr), all within the same multi-way, mul-tilingual model trained by using bilingual corpora.One-to-One Translation The most basic ap-proach here is to perform each translation path inthe decomposed sequence independently from eachother.
This one-to-one approach introduces only aminimal computational complexity (the multiplica-tive factor of two.)
We can further improve this one-to-one pivot-based translation by maintaining a setof k-best translations from the first stage (Es?En),but this increase the overall computational complex-ity by the factor of k, making it impractical in prac-tice.
We therefore focus only on the former approachof keeping the best pivot translation in this paper.Many-to-One Translation With the multi-way,multilingual model considered in this paper, we canextend the naive one-to-one pivot-based strategy byreplacing the second stage (En?Fr) to be many-to-one translation from Sec.
4.4 using both the origi-nal source language and the pivot language as a pairof source languages.
We first translate the sourcesentence (Es) into English, and use both the originalsource sentence and the English translation (Es+En)to translate into the final target language (Fr).Both approaches described and proposed abovedo not require any additional action on an already-trained multilingual model.
They are simply differ-ent translation strategies specifically aimed at zero-resource translation.5.2 Finetuning with Pseudo Parallel CorpusThe failure of the naive zero-resource translationearlier (see Table 4 (a)) suggests that the context vec-tors returned by the encoder are not compatible withthe decoder, when the combination was not includedduring training.
The good translation qualities of thetranslation paths included in training however im-ply that the representations learned by the encodersand decoders are good.
Based on these two obser-vations, we conjecture that all that is needed for azero-resource translation path is a simple adjustmentthat makes the context vectors from the encoder tobe compatible with the target decoder.
Thus, wepropose to adjust this zero-resource translation pathhowever without any additional parallel corpus.First, we generate a small set of pseudo bilin-gual pairs of sentences for the zero-resource lan-guage pair (Es?Fr) in interest.
We randomly selectN sentences pairs from a parallel corpus betweenthe target language (Fr) and a pivot language (En)and translate the pivot side (En) into the source lan-guage (Es).
Then, the pivot side is discarded, and weconstruct a pseudo parallel corpus consisting of sen-tence pairs of the source and target languages (Es-Fr).We make a copy of the existing attention mech-anism, to which we refer as target-specific atten-tion mechanism.
We then finetune only this target-specific attention mechanism while keeping all theother parameters of the encoder and decoder intact,using the generated pseudo parallel corpus.
We donot update any other parameters in the encoder anddecoder, because they are already well-trained (evi-denced by high translation qualities in Table 2) andwe want to avoid disrupting the well-captured struc-tures underlying each language.Once the model has been finetuned with thepseudo parallel corpus, we can use any of the trans-lation strategies described earlier in Sec.
3 for thefinetuned zero-resource translation path.
We ex-pect a similar gain by using many-to-one translation,which we empirically confirm in the next section.6 Experiments:Zero-Resource Translation6.1 Without Finetuning6.1.1 SettingsWe use the same multi-way, multilingual modeltrained earlier in Sec.
4.2 to evaluate the zero-resource translation strategies.
We emphasize herethat this model was trained only using Es-En andFr-En bilingual parallel corpora without any Es-Frparallel corpus.We evaluate the proposed approaches to zero-resource translation with the same multi-way, multi-lingual model from Sec.
4.1.
We specifically selectthe path from Spanish to French (Es?Fr) as a targetzero-resource translation path.273Pivot Many-to-1 Dev Test(a) < 1 < 1(b) ?
20.64 20.4(c) ?
Early 9.24 10.42(d) ?
Late 18.22 19.14(e) ?
E+L 13.29 14.56Table 4: Zero-resource translation from Spanish (Es) to French(Fr) without finetuning, using multi-way, multilingual model.When pivot is ?, English is used as a pivot language.6.1.2 Result and AnalysisAs mentioned earlier, we observed that the multi-way, multilingual model cannot directly translatebetween two languages when the translation pathbetween those two languages was not included intraining (Table 4 (a).)
On the other hand, themodel was able to translate decently with the pivot-based one-to-one translation strategy, as can be seenin Table 4 (b).
Unsurprisingly, all the many-to-one strategies resulted in worse translation quality,which is due to the inclusion of the useless transla-tion path (direct path between the zero-resource pair,Es-Fr).
Another interesting trend we observe is theEarly+Late averaging (Table 4 (e)) seems to per-form worse than Late averaging (Table 4 (d)) alone,opposite of the results in Table 3 (b-c).
We conjec-ture that, by simply averaging two model outputs (asin E+L), when one of them is drastically worse thanthe other, has the effect of pulling down the perfor-mance of final results.
But early averaging can stillrecover from this deficiency, upto some extent, sincethe decoder output probability function gmw (Eq.
(4).
)is a smooth function not only using the averagedcontext vectors (Eq.
(5).
).These results clearly indicate that the multi-way,multilingual model trained with only bilingual par-allel corpora is not capable of direct zero-resourcetranslation as it is.6.2 Finetuning with a Pseudo Parallel Corpus6.2.1 SettingsThe proposed finetuning strategy raises a numberof questions.
First, it is unclear how many pseudosentence pairs are needed to achieve a decent trans-lation quality.
Because the purpose of this finetuningstage is simply to adjust the shared attention mecha-nism so that it can properly bridge from the source-side encoder to the target-side decoder, we expect itto work with only a small amount of pseudo pairs.We validate this by creating pseudo corpora of dif-ferent sizes?1k, 10k, 100k and 1m.Second, we want to know how detrimental itis to use the generated pseudo sentence pairscompared to using true sentence pairs betweenthe target language pair.
In order to answerthis question, we compiled a true multi-way par-allel corpus by combining the subsets of UN(7.8m), Europarl-v7 (1.8m), OpenSubtitles-2013(1m), news-commentary-v7 (174k), LDC2011T07(335k) and news-crawl (310k), and use it to finetunethe model4.
This allows us to evaluate the effect ofthe pseudo and true parallel corpora on finetuningfor zero-resource translation.Lastly, we train single-pair models translating di-rectly from Spanish to French by using the true par-allel corpora.
These models work as a baselineagainst which we compare the multi-way, multilin-gual models.Training Unlike the usual training procedure de-scribed in Sec.
4.2, we compute the gradient for eachupdate using 60 sentence pairs only, when finetuningthe model with the multi-way parallel corpus (eitherpseudo or true.
)6.2.2 Result and AnalysisTable 5 summarizes all the result.
The most im-portant observation is that the proposed finetuningstrategy with pseudo-parallel sentence pairs outper-forms the pivot-based approach (using the early av-eraging strategy from Sec.
4.4) even when we usedonly 10k such pairs (compare (b) and (d).)
As we in-crease the size of the pseudo-parallel corpus, we ob-serve a clear improvement.
Furthermore, these mod-els perform comparably to or better than the single-pair model trained with 1M true parallel sentencepairs, although they never saw a single true bilin-gual sentence pair of Spanish and French (compare(a) and (d).
)Another interesting finding is that it is only ben-eficial to use true parallel pairs for finetuning themulti-way, mulitilingual models when there areenough of them (1m or more).
When there are onlya small number of true parallel sentence pairs, we4See the last row of Table 1.274Pseudo Parallel Corpus True Parallel CorpusPivot Many-to-1 1k 10k 100k 1m 1k 10k 100k 1m(a) Single-Pair Models Dev ?
?
?
?
?
?
11.25 21.32Test ?
?
?
?
?
?
10.43 20.35(b) ?
No Finetuning Dev: 20.64, Test: 20.4 ?
(c) Dev 0.28 10.16 15.61 17.59 0.1 8.45 16.2 20.59Test 0.47 10.14 15.41 17.61 0.12 8.18 15.8 19.97(d) ?
Early Dev 19.42 21.08 21.7 21.81 8.89 16.89 20.77 22.08Test 19.43 20.72 21.23 21.46 9.77 16.61 20.40 21.7(e) ?
Early+ Dev 20.89 20.93 21.35 21.33 14.86 18.28 20.31 21.33Late Test 20.5 20.71 21.06 21.19 15.42 17.95 20.16 20.9Table 5: Zero-resource translation from Spanish (Es) to French (Fr) with finetuning.
When pivot is ?, English is used as a pivotlanguage.
Row (b) is from Table 4 (b).even found using pseudo pairs to be more benefi-cial than true ones.
This effective as more apparent,when the direct one-to-one translation of the zero-resource pair was considered (see (c) in Table 5.
)This applies that the misalignment between the en-coder and decoder can be largely fixed by usingpseudo-parallel pairs only, and we conjecture that itis easier to learn from pseudo-parallel pairs as theybetter reflect the inductive bias of the trained modeland as the pseudo- parallel corpus is expected to bemore noisy, this may be an implicit regularizationeffect.
When there is a large amount of true parallelsentence pairs available, however, our results indi-cate that it is better to exploit them.Unlike we observed with the multi-source trans-lation in Sec.
3.2, we were not able to see any im-provement by further averaging the early-averagedand late-average decoding schemes (compare (d)and (e).)
This may be explained by the fact that thecontext vectors computed when creating a pseudosource (e.g., En from Es when Es?Fr) already con-tains all the information about the pseudo source.
Itis simply enough to take those context vectors intoaccount via the early averaging scheme.These results clearly indicate and verify the po-tential of the multi-way, multilingual neural trans-lation model in performing zero-resource machinetranslation.
More specifically, it has been shown thatthe translation quality can be improved even withoutany direct parallel corpus available, and if there is asmall amount of direct parallel pairs available, thequality may improve even further.7 Conclusion:Implications and LimitationsImplications There are two main results in thispaper.
First, we showed that the multi-way, multilin-gual neural translation model by Firat et al (2016)is able to exploit common, underlying structuresacross many languages in order to better translatewhen a source sentence is given in multiple lan-guages.
This confirms the usefulness of positive lan-guage transfer, which has been believed to be an im-portant factor in human language learning (Odlin,1989; Ringbom, 2007), in machine translation.
Fur-thermore, our result significantly expands the ap-plicability of multi-source translation (Zoph andKnight, 2016), as it does not assume the availabilityof multi-way parallel corpora for training and reliesonly on bilingual parallel corpora.Second, the experiments on zero-resource trans-lation revealed that it is not necessary to have a di-rect parallel corpus, or deep linguistic knowledge,between two languages in order to build a machinetranslation system.
Importantly we observed thatthe proposed approach of zero-resource translationis better both in terms of translation quality anddata efficiency than a more traditional pivot-basedtranslation (Wu and Wang, 2007; Utiyama and Isa-hara, 2007).
Considering that this is the first attemptat such zero-resource, or extremely low-resource,translation using neural machine translation, we ex-pect a large progress in near future.275Limitations Despite the promising empirical re-sults presented in this paper, there are a number ofshortcomings that needs to addressed in follow-upresearch.
First, our experiments have been done onlywith three European languages?Spanish, French andEnglish.
More investigation with a diverse set of lan-guages needs to be done in order to make a moresolid conclusion, such as was done in (Firat et al,2016; Chung et al, 2016).
Furthermore, the effect ofvarying sizes of available parallel corpora on the per-formance of zero-resource translation must be stud-ied more in the future.Second, although the proposed many-to-onetranslation is indeed generally applicable to anynumber of source languages, we have only testeda source sentence in two languages.
We expecteven higher improvement with more languages, butit must be tested thoroughly in the future.Lastly, the proposed finetuning strategy requiresthe model to have an additional set of parameters rel-evant to the attention mechanism for a target, zero-resource pair.
This implies that the number of pa-rameters may grow linearly with respect to the num-ber of target language pairs.
We expect future re-search to address this issue by, for instance, mix-ing in the parallel corpora of high-resource languagepairs during finetuning as well.AcknowledgmentsOF thanks Iulian Vlad Serban and Georgiana Dinufor insightful discussions.
KC thanks the supportby Facebook, Google (Google Faculty Award 2016)and NVidia (GPU Center of Excellence 2015-2016).ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In ICLR 2015.Ozan Caglayan, Walid Aransa, Yaxing Wang, MarcMasana, Mercedes Garc??a-Mart?
?nez, Fethi Bougares,Lo?
?c Barrault, and Joost van de Weijer.
2016.Does multimodality help human and machine fortranslation and image captioning?
arXiv preprintarXiv:1605.09186.Kyunghyun Cho, Bart Van Merrie?nboer, Caglar Gul-cehre, Dzmitry Bahdanau, Fethi Bougares, HolgerSchwenk, and Yoshua Bengio.
2014.
Learning phraserepresentations using RNN encoder-decoder for statis-tical machine translation.
arXiv:1406.1078.Junyoung Chung, Kyunghyun Cho, and Yoshua Bengio.2016.
A character-level decoder without explicit seg-mentation for neural machine translation.
In ACL.Daxiang Dong, Hua Wu, Wei He, Dianhai Yu, andHaifeng Wang.
2015.
Multi-task learning for multi-ple language translation.
ACL.Orhan Firat and Kyunghyun Cho.
2016.
DL4MT-Tutorial: Conditional gated recurrent unit with atten-tion mechanism.Orhan Firat, Kyunghyun Cho, and Yoshua Bengio.
2016.Multi-way, multilingual neural machine translationwith a shared attention mechanism.
In NAACL.Mikel L Forcada and Ramo?n P N?eco.
1997.
Recur-sive hetero-associative memories for translation.
InBiological and Artificial Computation: From Neuro-science to Technology, pages 453?462.
Springer.Nizar Habash and Jun Hu.
2009.
Improving arabic-chinese statistical machine translation using english aspivot language.
In Proceedings of the Fourth Work-shop on Statistical Machine Translation, StatMT ?09,pages 173?181.
Association for Computational Lin-guistics.Se?bastien Jean, Orhan Firat, Kyunghyun Cho, RolandMemisevic, and Yoshua Bengio.
2015.
Montrealneural machine translation systems for wmt?15.
InProceedings of the Tenth Workshop on Statistical Ma-chine Translation, pages 134?140, Lisbon, Portugal,September.
Association for Computational Linguis-tics.Nal Kalchbrenner and Phil Blunsom.
2013.
Recur-rent continuous translation models.
In EMNLP, pages1700?1709.Diederik Kingma and Jimmy Ba.
2015.
Adam: Amethod for stochastic optimization.
The InternationalConference on Learning Representations (ICLR).Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, et al 2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of the45th annual meeting of the ACL on interactive posterand demonstration sessions, pages 177?180.
Associa-tion for Computational Linguistics.Wang Ling, Isabel Trancoso, Chris Dyer, and Alan WBlack.
2015.
Character-based neural machine transla-tion.
arXiv:1511.04586.Minh-Thang Luong and Christopher D Manning.2016.
Achieving open vocabulary neural ma-chine translation with hybrid word-character models.arXiv:1604.00788.Minh-Thang Luong, Quoc V Le, Ilya Sutskever, OriolVinyals, and Lukasz Kaiser.
2015a.
Multi-tasksequence to sequence learning.
arXiv preprintarXiv:1511.06114.276Minh-Thang Luong, Hieu Pham, and Christopher DManning.
2015b.
Effective approaches to attention-based neural machine translation.
arXiv preprintarXiv:1508.04025.Tomas Mikolov, Martin Karafia?t, Lukas Burget, Jan Cer-nocky`, and Sanjeev Khudanpur.
2010.
Recurrent neu-ral network based language model.
INTERSPEECH,2:3.Terence Odlin.
1989.
Language Transfer.
CambridgeUniversity Press.
Cambridge Books Online.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2012.
On the difficulty of training recurrent neuralnetworks.
arXiv preprint arXiv:1211.5063.Ha?kan Ringbom.
2007.
Cross-linguistic similarity inforeign language learning, volume 21.Mike Schuster and Kuldip K Paliwal.
1997.
Bidirec-tional recurrent neural networks.
Signal Processing,IEEE Transactions on, 45(11):2673?2681.Rico Sennrich, Barry Haddow, and Alexandra Birch.2015a.
Improving neural machine translationmodels with monolingual data.
arXiv preprintarXiv:1511.06709.Rico Sennrich, Barry Haddow, and Alexandra Birch.2015b.
Neural machine translation of rare words withsubword units.
arXiv preprint arXiv:1508.07909.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural networks.In NIPS, pages 3104?3112.Masao Utiyama and Hitoshi Isahara.
2007.
A compar-ison of pivot methods for phrase-based statistical ma-chine translation.
In HLT-NAACL, pages 484?491.Hua Wu and Haifeng Wang.
2007.
Pivot languageapproach for phrase-based statistical machine transla-tion.
Machine Translation, 21(3):165?181.Bing Zhao and Shengyuan Chen.
2009.
A simplexarmijo downhill algorithm for optimizing statisticalmachine translation decoding parameters.
In HLT-NAACL, pages 21?24.Barret Zoph and Kevin Knight.
2016.
Multi-source neu-ral translation.
In NAACL.277
