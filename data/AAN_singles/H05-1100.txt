Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 795?802, Vancouver, October 2005. c?2005 Association for Computational LinguisticsMorphology and Reranking for the Statistical Parsing of SpanishBrooke CowanMIT CSAILbrooke@csail.mit.eduMichael CollinsMIT CSAILmcollins@csail.mit.eduAbstractWe present two methods for incorporat-ing detailed features in a Spanish parser,building on a baseline model that is a lex-icalized PCFG.
The first method exploitsSpanish morphology, and achieves an F1constituency score of 83.6%.
This is animprovement over 81.2% accuracy for thebaseline, which makes little or no use ofmorphological information.
The secondmodel uses a reranking approach to addarbitrary global features of parse trees tothe morphological model.
The rerankingmodel reaches 85.1% F1 accuracy on theSpanish parsing task.
The resulting modelfor Spanish parsing combines an approachthat specifically targets morphological in-formation with an approach that makesuse of general structural features.1 IntroductionInitial methods for statistical parsing were mainlydeveloped through experimentation on English datasets.
Subsequent research has focused on apply-ing these methods to other languages.
There hasbeen widespread evidence that new languages ex-hibit linguistic phenomena that pose considerablechallenges to techniques originally developed forEnglish; because of this, an important area of cur-rent research concerns how to model these phenom-ena more accurately within statistical approaches.
Inthis paper, we investigate this question within thecontext of parsing Spanish.
We describe two meth-ods for incorporating detailed features in a Spanishparser, building on a baseline model that is a lexical-ized PCFG originally developed for English.Our first model uses morphology to improvethe performance of the baseline model.
Englishis a morphologically-impoverished language, whilemost of the world?s languages exhibit far richer mor-phologies.
Spanish is one of these languages.
Forinstance, the forms of Spanish nouns, determiners,and adjectives reflect both number and gender; pro-nouns reflect gender, number, person, and case.
Fur-thermore, morphological constraints may be mani-fested at the syntactic level: certain constituents of anoun phrase are constrained to agree in number andgender, and a verb is constrained to agree in num-ber and person with its subject.
Hence, morphol-ogy gives us important structural cues about how thewords in a Spanish sentence relate to one another.The mechanism we employ for incorporating mor-phology into the PCFG model (the Model 1 parserin (Collins, 1999)) is the modification of its part-of-speech (POS) tagset; in this paper, we explain howthis mechanism allows the parser to better capturemorphological constraints.All of the experiments in this paper are carriedout using a freely-available Spanish treebank pro-duced by the 3LB project (Navarro et al, 2003).This resource contains around 3,500 hand-annotatedtrees encoding ample morphological information.We could not use all of this information and ade-quately train the resulting parameters due to lim-ited training data.
Hence, we used developmentdata to test the performance of several models, eachincorporating a subset of morphological informa-tion.
The highest-accuracy model on the devel-opment set uses the mode and number of verbs,as well as the number of adjectives, determiners,nouns, and pronouns.
On test data, it reachesF1 accuracy of 83.6%/83.9%/79.4% for labeledconstituents, unlabeled dependencies, and labeleddependencies, respectively.
The baseline model,which makes almost no use of morphology, achieves81.2%/82.5%/77.0% in these same measures.We use the morphological model from the afore-mentioned experiments as a base parser in a secondset of experiments.
Here we investigate the efficacyof a reranking approach for parsing Spanish by using795arbitrary structural features.
Previous work in sta-tistical parsing (Collins and Koo, 2005) has shownthat applying reranking techniques to the n-best out-put of a base parser can improve parsing perfor-mance.
Applying an exponentiated gradient rerank-ing algorithm (Bartlett et al, 2004) to the n-best out-put of our morphologically-informed Spanish pars-ing model gives us similar improvements.
Using thereranking model combined with the morphologicalmodel raises performance to 85.1%/84.7%/80.2%F1 accuracy for labeled constituents, unlabeled de-pendencies, and labeled dependencies.2 Related WorkThe statistical parsing of English has surpassed 90%accuracy in the precision and recall of labeled con-stituents (e.g., (Collins, 1999; Charniak and John-son, 2005)).
A recent proliferation of treebanks invarious languages has fueled research in the pars-ing of other languages.
For instance, work hasbeen done in Chinese using the Penn Chinese Tree-bank (Levy and Manning, 2003; Chiang and Bikel,2002), in Czech using the Prague Dependency Tree-bank (Collins et al, 1999), in French using theFrench Treebank (Arun and Keller, 2005), in Ger-man using the Negra Treebank (Dubey, 2005; Dubeyand Keller, 2003), and in Spanish using the UAMSpanish Treebank (Moreno et al, 2000).
The best-reported F1 constituency scores from this work foreach language are 79.9% (Chinese (Chiang andBikel, 2002)), 81.0% (French (Arun and Keller,2005), 76.2% (German (Dubey, 2005)), and 73.8%(Spanish (Moreno et al, 2000)).
The authors in(Collins et al, 1999) describe an approach that gives80% accuracy in recovering unlabeled dependenciesin Czech.1The project that is arguably most akin to the workpresented in this paper is that on Spanish parsing(Moreno et al, 2000).
However, a direct compari-son of scores is complicated by the fact that we haveused a different corpus as well as larger training andtest sets (2,800- vs. 1,500-sentence training sets, and700- vs. 40-sentence test sets).1Note that cross-linguistic comparison of results is compli-cated: in addition to differences in corpus annotation schemesand sizes, there may be significant differences in linguistic char-acteristics.Category AttributesAdjective gender, number, participleDeterminer gender, number, person, possessorNoun gender, numberVerb gender, number, person, mode, tensePreposition gender, number, formPronoun gender, number, person, case, possessorTable 1: A list of the morphological features from which wecreated our models.
For brevity, we only list attributes with atleast two values.
See (Civit, 2000) for a comprehensive list ofthe morphological attributes included in the Spanish treebank.3 ModelsThis section details our two approaches for addingfeatures to a baseline parsing model.
First, we de-scribe how morphological information can be addedto a parsing model by modifying the POS tagset.Second, we describe an approach that reranks then-best output of the morphologically-rich parser, us-ing arbitrary, general features of the parse trees asadditional information.3.1 Adding Morphological InformationThe mechanism we employ for incorporating mor-phological information is the modification of thePOS tagset of a lexicalized PCFG2 ?
the Model 1parser described in (Collins, 1999) (hereafterModel 1).
Each POS tagset can be thought of as aparticular morphological model or a subset of mor-phological attributes.
Table 1 shows the complete setof morphological features we considered for Span-ish.
There are 22 morphological features in total inthis table; different POS sets can be created by de-ciding whether or not to include each of these 22features; hence, there are 222 different morpholog-ical models we could have created.
For instance,one particular model might capture the modal infor-mation of verbs.
In this model, there would be sixPOS tags for verbs (one for each of indicative, sub-junctive, imperative, infinitive, gerund, and partici-ple) instead of just one.
A model that captured boththe number and mode of verbs would have 18 verbalPOS tags, assuming three values (singular, plural,and neutral) for the number feature.The Effect of the Tagset on Model 1 Modifyingthe POS tagset alows Model 1 to better distinguish2Hand-crafted head rules are used to lexicalize the trees.796S(corri?,v)NP(gatos,n) VP(corri?,v)Figure 1: An ungrammatical dependency: the plural noun gatosis unlikely to modify the singular verb corrio?.events that are unlikely from those that are likely, onthe basis of morphological evidence.
An examplewill help to illustrate this point.Model 1 relies on statistics conditioned on lexi-cal headwords for practically all parameters in themodel.
This sensitivity to headwords is achieved bypropagating lexical heads and POS tags to the non-terminals in the parse tree.
Thus, any statistic basedon headwords may also be sensitive to the associatedPOS tag.
For instance, consider the subtree in Fig-ure 1.
Note that this structure is ungrammatical be-cause the subject, gatos (cats), is plural, but the verb,corrio?
(ran), is singular.
In Model 1, the probabilityof generating the noun phrase (NP) with headwordgatos and headtag noun (n) is defined as follows:3P (gatos, n, NP | corrio?, v, S, VP) =P1(n, NP | corrio?, v, S, VP)?P2(gatos | n, NP, corrio?, v, S, VP)The parser smooths parameter values using backed-off statistics, and in particular smooths statisticsbased on headwords with coarser statistics based onPOS tags alone.
This allows the parser to effectivelyuse POS tags as a way of separating different lexi-cal items into subsets or classes depending on theirsyntactic behavior.
In our example, each term is es-timated as follows:P1(n, NP | corrio?, v, S, VP) =?1,1P?1,1(n, NP | corrio?, v, S, VP) +?1,2P?1,2(n, NP | v, S, VP) +?1,3P?1,3(n, NP | S, VP)andP2(gatos | n, NP, corrio?, v, S, VP) =?2,1P?2,1(gatos | n, NP, corrio?, v, S, VP) +?2,2P?2,2(gatos | n, NP, v, S, VP) +?2,3P?2,3(gatos | n)3Note that the parsing model includes other features such asdistance which we omit from the parameter definition for thesake of brevity.Here the P?i,j terms are maximum likelihood es-timates derived directly from counts in the train-ing data.
The ?i,j parameters are defined so that?1,1+?1,2+?1,3 = ?2,1+?2,2+?2,3 = 1.
They con-trol the relative contribution of each level of back-offto the final estimate.Note that thus far our example has not includedany morphological information in the POS tags.
Be-cause of this, we will see that there is a danger ofthe estimates P1 and P2 both being high, in spiteof the dependency being ungrammatical.
P1 will behigh because all three estimates P?1,1, P?1,2 and P?1,3will most likely be high.
Next, consider P2.
Of thethree estimates P?2,1, P?2,2, and P?2,3, only P?2,1 retainsthe information that the noun is plural and the verbis singular.
Thus P2 will be sensitive to the morpho-logical clash between gatos and corrio?
only if ?2,1 ishigh, reflecting a high level of confidence in the es-timate of P?2,3.
This will only happen if the context?corrio?, v, S, VP?
is seen frequently enough for ?2,1to take a high value.
This is unlikely, given that thiscontext is quite specific.
In summary, the impover-ished model can only capture morphological restric-tions through lexically-specific estimates based onextremely sparse statistics.Now consider a model that incorporates morpho-logical information ?
in particular, number infor-mation ?
in the noun and verb POS tags.
gatos willhave the POS pn, signifying a plural noun; corrio?will have the POS sv, signifying a singular verb.All estimates in the previous equations will reflectthese POS changes.
For example, P1 will now beestimated as follows:P1(pn, NP | corrio?, sv, S, VP) =?1,1P?1,1(pn, NP | corrio?, sv, S, VP) +?1,2P?1,2(pn, NP | sv, S, VP) +?1,3P?1,3(pn, NP | S, VP)Note that the two estimates P?1,1 and P?1,2 includean (unlikely) dependency between the POS tags pnand sv.
Both of these estimates will be 0, assum-ing that a plural noun is never seen as the subject ofa singular verb.
At the very least, the context ?sv,S, VP?
will be frequent enough for P?1,2 to be a re-liable estimate.
The value for ?1,2 will therefore behigh, leading to a low estimate for P1, thus correctlyassigning low probability to the ungrammatical de-797pendency.
In summary, the morphologically-richmodel can make use of non-lexical statistics such asP?1,2(pn, NP | sv, S, VP) which contain dependen-cies between POS tags and which will most likelybe estimated reliably by the model.3.2 The Reranking ModelIn the reranking model, we use an n-best version ofthe morphologically-rich parser to generate a num-ber of candidate parse trees for each sentence intraining and test data.
These parse trees are thenrepresented through a combination of the log prob-ability under the initial model, together with a largenumber of global features.
A reranking model usesthe information from these features to derive a newranking of the n-best parses, with the hope of im-proving upon the baseline model.
Previous ap-proaches (e.g., (Collins and Koo, 2005)) have useda linear model to combine the log probability un-der a base parser with arbitrary features derived fromparse trees.
There are a variety of methods for train-ing the parameters of the model.
In this work, weuse the algorithm described in (Bartlett et al, 2004),which applies the large-margin training criterion ofsupport vector machines (Cortes and Vapnik, 1995)to the reranking problem.The motivation for the reranking model is that awide variety of features, which can essentially besensitive to arbitrary context in the parse trees, canbe incorporated into the model.
In our work, we in-cluded all features described in (Collins and Koo,2005).
As far as we are aware, this is the first timethat a reranking model has been applied to parsinga language other than English.
One goal was to in-vestigate whether the improvements seen on Englishparsing can be carried across to another language.We have found that features in (Collins and Koo,2005), initially developed for English parsing, alsogive appreciable gains in accuracy when applied toSpanish.4 DataThe Spanish 3LB treebank is a freely-available re-source with about 3,500 sentence/tree pairs that wehave used to train our models.
The average sen-tence length is 28 tokens.
The data is taken from38 complete articles and short texts.
Roughly 27%Non-Terminal Significanceaq adjectivecc conjunctionCOORD coordinated phraseESPEC determinerGRUP base noun phraseGV verb phraseMORF impersonal pronounp pronounPREP base prepositional phraseRELATIU relative pronoun phrases adjectival phraseSN noun phraseSP prepositional phraseSADV adverbial phraseS sentencesps prepositionv verbTable 2: The non-terminals and preterminals from the Spanish3LB corpus used in this paper.of the texts are news articles, 27% scientific articles,14% narrative, 11% commentary, 11% sports arti-cles, 6% essays, and 5% articles from weekly maga-zines.
The trees contain information about both con-stituency structure and syntactic functions.4.1 PreprocessingIt is well-known that tree representation influencesparsing performance (Johnson, 1998).
Prior to train-ing our models, we made some systematic modifica-tions to the corpus trees in an effort to make it eas-ier for Model 1 to represent the linguistic phenom-ena present in the trees.
For the convenience of thereader, Table 2 gives a key to the non-terminal labelsin the 3LB treebank that are used in this section andthe remainder of the paper.Relative and Subordinate Clauses Cases of rela-tive and subordinate clauses appearing in the corpustrees have the basic structure of the example in Fig-ure 2a.
Figure 2b shows the modifications we im-pose on such structures.
The modified structure hasthe advantage that the SBAR selects the CP node asits head, making the relative pronoun que the head-word for the root of the subtree.
This change allows,for example, better modeling of verbs that select forparticular complementizers.
In addition, the newsubtree rooted at the S node now looks like a top-level sentence, making sentence types more uniformin structure and easier to model statistically.
Addi-tionally, the new structure differentiates phrases em-798RELATIU?CPapquienSP?CPspsPREP?CPconsiderabanvGVtodosSNGRUPpCPSBAR?SStodosPREPspsaSNGRUPp considerabanvGVSSPRELATIUpquien(a)(b)Figure 2: Figure (a) is the original structure from the 3LB tree-bank for the phrase a quien todos consideraban or whom ev-eryone considered.
We transform structures like (a) into (b) byinserting SBAR and CP nodes, and by marking all non-terminalsbelow the CP with a -CP tag.bedded in the complementizers of SBARs from thoseused in other contexts, allowing relative pronounslike quien in Figure 2 to surface as lexical head-words when embedded in larger phrases beneath theCP node.4Coordination In the treebank, coordinated con-stituents and their coordinating conjunction areplaced as sister nodes in a flat structure.
We enhancethe structure of such subtrees, as in Figure 3.
Ourstructure helps to rule out unlikely phrases such ascats and dogs and; the model trained with the orig-inal treebank structures will assign non-zero proba-bility to ill-formed structures such as these.5 ExperimentsOur models were trained using a training set con-sisting of 80% of the data (2,801 sentence/tree pairs,75,372 words) available to us in the 3LB treebank.We reserved the remaining 20% (692 sentences,19,343 words) to use as unseen data in a test set.We selected these subsets with two criteria in mind:first, respecting the boundaries of the texts by plac-ing articles in their entirety into either one subset orthe other; and second, maintaining, in each subset,the same proportion of genres found in the originalset of trees.
During development, we used a cross-4This is achieved through our head rules.
(a)(b)civilesparlamentarios yparlamentariosCOORDy civilesss?CC1s s?CC2saqsCOORD ssaq cc aqaqccFigure 3: In the 3LB corpus, phrases involving coordination,are represented with a flat structure as in (a).
For coordinationinvolving a non-terminal X (X = s in the example), we insertnew nodes X-CC1 and X-CC2 to form the structure in (b).validation approach on the training set to test differ-ent models.
We divided the 2,800 training data treesinto 14 different development data sets, where eachof these data sets consisted of 2,600 training sen-tences and 200 development sentences.
We took theaverage over the results of the 14 splits to gauge theeffectiveness of the model being tested.To evaluate our models, we considered the recov-ery of labeled and unlabeled dependencies as well aslabeled constituents.
Unlabeled dependencies cap-ture how the words in a sentence depend on one an-other.
Formally, they are tuples {headchild index,modifier index}, where the indices indicate positionin the sentence.
Labeled dependencies include thelabels of the modifier, headchild, and parent non-terminals as well.
The root of the tree has a specialdependency: {head index} in the unlabeled case and{TOP, headchild index, root non-terminal} in the la-beled case.
The labeled constituents in a tree are allof the non-terminals and, for each, the positions ofthe words it spans.
We use the standard definitionsof precision, recall, and F-measure.55When extracting dependencies, we replaced all non-punctuation POS labels with a generic label TAG to avoid con-flating tagging errors with dependency errors.
We also includedthe structural changes that we imposed during preprocessing.Results for constituent precision and recall were computed af-ter we restored the trees to the original treebank structure.799Labeled Dep Unlabeled Dep Labeled Const<=70 words <=40 WordsModel Prec/Rec Gain Prec/Rec Gain Prec Rec Prec Rec1 Baseline 76.0 ?
82.1 ?
81.6 80.4 82.6 81.42 n(P,N,V) 78.4 2.4 83.6 1.5 83.1 82.5 84.1 83.43 n(A,D,N,P,V) 78.2 2.2 83.5 1.4 83.3 82.4 84.2 83.34 n(V) 77.8 1.8 82.9 0.8 82.3 81.6 83.1 82.25 m(V) 78.4 2.4 83.1 1.0 82.8 82.0 83.8 82.96 t(V) 77.6 1.6 82.7 0.6 82.4 81.4 83.2 82.37 p(V) 78.1 2.1 83.3 1.2 82.9 82.0 83.8 82.88 g(V) 76.3 0.3 82.2 0.1 81.6 80.6 82.7 81.79 n(A,D,N,V,P)+m(V) 79.0 3.0 84.0 1.9 83.9 83.2 84.7 84.110 n(P,N,V)+m(V) 78.9 2.9 83.7/83.8 1.6/1.7 83.6 82.8 84.6 83.711 n(A,D,N,V,P)+m(V)+p(V) 78.7 2.7 83.6 1.5 83.6 82.9 84.4 83.812 n(A,D,N,V,P)+p(V) 78.4 2.4 83.5/83.6 1.4/1.5 83.3 82.6 84.2 83.513 n(A,D,N,V,P)+g(A,D,N,V,P) 78.1 2.1 83.2 1.1 83.1 82.5 83.9 83.4Table 3: Results after training morphological models during development.
When precision and recall differ in labeled or unlabeleddependencies, both scores are shown.
Row 1 shows results on a baseline model containing almost no morphological information.The subsequent rows represent a subset of the models with which we experimented: n(P,N,V) uses number for pronouns, nouns,and verbs; n(A,D,N,P,V) uses number for adjectives, determiners, nouns, pronouns, and verbs; n(V) uses number for verbs; m(V)uses mode for verbs; t(V) uses tense for verbs; p(V) uses person for verbs; g(V) uses gender for verbs; the models in rows 9?12are combinations of these models, and in row 13, n(A,D,N,V,P) combines with g(A,D,N,V,P), which uses gender for adjectives,determiners, nouns, verbs, and pronouns.
The results of the best-performing model are in bold.Labeled Dep Unlabeled Dep Labeled Const<=70 words <=40 WordsModel Prec/Rec Prec/Rec Prec Rec Prec Rec1 Baseline 77.0 82.5 81.7 80.8 83.1 82.02 n(A,D,N,V,P)+m(V) 79.4 83.9 83.9 83.4 85.1 84.43 RERANK 80.2 84.7 85.2 85.0 86.3 85.9Table 4: Results after running the morphological and reranking models on test data.
Row 1 is our baseline model.
Row 2 is themorphological model that scored highest during development.
Row 3 gives the accuracy of the reranking approach, when appliedto n-best output from the model in Row 2.5.1 The Effects of MorphologyIn our first experiments, we trained over 50 mod-els, incorporating different morphological informa-tion into each in the way described in Section 3.1.Prior to running the parsers, we trained the POS tag-ger described in (Collins, 2002).
The output fromthe tagger was used to assign a POS label for un-known words.
We only attempted to parse sentencesunder 70 words in length.Table 3 describes some of the models we triedduring development and gives results for each.
Ourbaseline model, which we used to evaluate the ef-fects of using morphology, was Model 1 (Collins,1999) with a simple POS tagset containing almostno morphological information.
The morphologi-cal models we show are meant to be representativeof both the highest-scoring models and the perfor-mance of various morphological features.
For in-stance, we found that, in general, gender had only aslight impact on the performance of the parser.
Notethat gender is not a morphological attribute of Span-ish verbs, and that the inclusion of verbal features,particularly number, mode, and person, generatedthe strongest-performing models in our experiments.Table 4 shows the results of running two mod-els on the test set: the baseline model and the best-performing morphological model from the develop-ment stage.
This model uses the number and modeof verbs, as well as the number of adjectives, deter-miners, nouns, and pronouns.The results in Tables 3 and 4 show that addingsome amount of morphological information to aparsing model is beneficial.
We found, however, thatadding more information does not always lead to im-proved performance (see, for example, rows 11 and13 in Table 3).
Presumably this is because the tagsetgrows too large.Table 5 takes a closer look at the performance800of the best-performing morphological model in therecovery of particular labeled dependencies.
Thebreakdown shows the top 15 dependencies in thegold-standard trees across the entire training set.Collectively, these dependencies represent around72% of the dependencies seen in this data.We see an extraordinary gain in the recovery ofsome of these dependencies when we add morpho-logical information.
Among these are the two in-volving postmodifiers to verbs.
When examining theoutput of the morphological model, we found thatmuch of this gain is due to the fact that there are twonon-terminal labels used in the treebank that specifymodal information of verbs they dominate (infiniti-vals and gerunds): with insufficient morphologicalinformation, the baseline parser was unable to dis-tinguish regular verb phrases from these more spe-cific verb phrases.Some dependencies are particularly difficult forthe parser, such as that in which SBAR modifiesa noun ({GRUP TAG SBAR R}).
We found thataround 20% of cases of this type in the training setinvolve structures like el proceso de negociones que(in English the process of negotiation that).
Thistype of structure is inherently difficult to disam-biguate.
In Spanish, such structures may be morecommon than in English, since phrases involvingnominal modifiers to nouns, like negotiation pro-cess, are always formed as noun + de + noun.5.2 Experiments with RerankingIn the reranking experiments, we follow the proce-dure described in (Collins and Koo, 2005) for cre-ation of a training set with n-best parses for eachsentence.
This method involves jack-knifing thedata: the training set of 2,800 sentences was parsedin 200-sentence chunks by an n-best morphologi-cal parser trained on the remaining 2,600 sentences.This ensured that each sentence in the training datahad n-best output from a baseline model that wasnot trained on that sentence.
We used the optimalmorphological model (n(A,D,N,V,P)+m(V)) to gen-erate the n-best lists, and we used the feature set de-scribed in (Collins and Koo, 2005).
The test resultsare given in Table 4.66Note that we also created development sets for develop-ment of the reranking approach, and for cross-validation of thesingle parameter C in approach of (Bartlett et al, 2004).Dependency Count Model Prec/RecDeterminer modifier 9680 BL 95.0/95.4SN GRUP ESPEC L (15.5%) M 95.4/95.7Complement of SP 9052 BL 92.4/92.9SP PREP SN R (14.5%) M 93.2/93.9SP modifier to noun 4500 BL 83.9/78.1GRUP TAG SP R (7.2%) M 82.9/79.9Subject 3106 BL 77.7/86.1S GV SN L (5.0%) M 83.1/87.5Sentential head 2758 BL 75.0/75.0TOP S (4.4%) M 79.7/79.7S modifier under SBAR 2728 BL 83.3/82.1SBAR CP S R (4.4%) M 86.0/84.7SP modifier to verb 2685 BL 62.4/78.8S GV SP R (4.3%) M 72.6/82.5SN modifier to verb 2677 BL 71.6/75.6S GV SN R (4.3%) M 81.0/83.0Adjective postmodifier 2522 BL 76.3/83.6GRUP TAG s R (4.0%) M 76.4/83.5Adjective premodifier 980 BL 79.2/80.0GRUP TAG s L (1.6%) M 80.1/79.3SBAR modifier to noun 928 BL 62.2/60.6GRUP TAG SBAR R (1.4%) M 61.3/60.8Coordination 895 BL 65.2/72.7S-CC2 S coord L (1.4%) M 66.7/74.2Coordination 870 BL 52.4/56.1S-CC1 S-CC2 S L (1.4%) M 60.3/63.6Impersonal pronoun 804 BL 93.3/96.4S GV MORF L (1.3%) M 92.0/95.6SN modifier to noun 736 BL 47.3/39.5GRUP TAG SN R (1.2%) M 51.7/50.8Table 5: Labeled dependency accuracy for the top 15 depen-dencies (representing around 72% of all dependencies) in thegold-standard trees across all training data.
The first columnshows the type and subtype, where the subtype is specified asthe 4-tuple {parent non-terminal, head non-terminal, modifiernon-terminal, direction}; the second column shows the countfor that subtype and the percent of the total that it represents(where the total is 62,372) .
The model BL is the baseline, andM is the morphological model n(A,D,N,V,P)+m(V).5.3 Statistical SignificanceWe tested the significance of the labeled precisionand recall results in Table 4 using the sign test.When applying the sign test, for each sentence inthe test data we calculate the sentence-level F1 con-stituent score for the two parses being compared.This indicates whether one model performs betteron that sentence than the other model, or whetherthe two models perform equally well, informationused by the sign test.
All differences were found tobe statistically significant at the level p = 0.01.77When comparing the baseline model to the morphologicalmodel on the 692 test sentences, F1 scores improved on 314sentences, and became worse on 164 sentences.
When com-paring the baseline model to the reranked model, 358/157 sen-8016 Conclusions and Future WorkWe have developed a statistical parsing model forSpanish that performs at 85.1% F1 constituency ac-curacy.
We find that an approach that explicitlyrepresents some of the particular features of Span-ish (i.e., its morphology) does indeed help in pars-ing.
Moreover, this approach is compatible withthe reranking approach, which uses general fea-tures that were first developed for use in an En-glish parser.
In fact, our best parsing model com-bines both the language-specific morphological fea-tures and the non-specific reranking features.
Themorphological features are local, being restricted todependencies between words in the parse tree; thereranking features are more global, relying on largerportions of parse structures.
Thus, we see our finalmodel as combining the strengths of two comple-mentary approaches.We are curious to know the extent to which aclose analysis of the dependency errors made by thebaseline parser can be corrected by the developmentof features tailored to addressing these problems.Some preliminary investigation of this suggests thatwe see much higher gains when using generic fea-tures than these more specific ones, but we leave athorough investigation of this to future work.
An-other avenue for future investigation is to try using amore sophisticated baseline model such as Collins?Model 2, which incorporates both subcategorizationand complement/adjunct information.
Finally, wewould like to use the Spanish parser in an applica-tion such as machine translation.AcknowledgementsWe would like to thank Xavier Carreras for point-ing us to the Spanish 3LB treebank and MontserratCivit for providing access to the data and answeringquestions about it.
We also gratefully acknowledgethe support of the National Science Foundation un-der grants 0347631 and 0434222.tences had improved/worse parses.
When comparing the mor-phological model to the reranked model, 199/106 sentences hadimproved/worse parses.ReferencesAbhishek Arun and Frank Keller.
2005.
Lexicalization incrosslinguistic probabilistic parsing: the case of French.ACL 2005, Ann Arbor, MI.Peter Bartlett, Michael Collins, Ben Taskar, and DavidMcAllester.
2004.
Exponentiated gradient algorithms forlarge-margin structured classification.
Proceedings of NIPS2004.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-finen-best parsing and MaxEnt discriminative reranking.
ACL2005, Ann Arbor, MI.David Chiang and Daniel M. Bikel.
2002.
Recovering latentinformation in treebanks.
Proceedings of COLING-2002,pages 183?189.Montserrat Civit Torruella.
2000.
Gu?
?a para la anotacio?n mor-fosinta?ctica del corpus CLiC-TALP.
X-Tract Working Paper,WP-00/06.Michael Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
University of Pennsylvania.Michael Collins, Jan Hajic, Lance Ranshaw, and Christoph Till-man.
1999.
A statistical parser for Czech.
ACL 99.Michael Collins.
2002.
Discriminative training methods forhidden Markov models: theory and experiments with per-ceptron algorithms.
EMNLP 2002.Michael Collins and Terry Koo.
2005.
Discriminative Rerank-ing for Natural Language Parsing.
Computational Linguis-tics, 31(1):25?69.C.
Cortes and V. Vapnik.
1995.
Support Vector Networks.
Ma-chine Learning, 20:273?297.Amit Dubey and Frank Keller.
2003.
Probabilistic parsing forGerman using sister-head dependencies.
ACL 2003, pp.
96?103.Amit Dubey.
2005.
What to do when lexicalization fails: pars-ing German with suffix analysis and smoothing.
ACL 2005,Ann Arbor, MI.Mark Johnson.
1998.
PCFG Models of Linguistic Tree Repre-sentations.
Computational Linguistics, 24(4):613?632.Roger Levy and Christopher Manning.
2003.
Is it harder toparse Chinese, or the Chinese Treebank?
ACL 2003, pp.439?446.Antonio Moreno, Ralph Grishman, Susana Lo?pez, FernandoSa?nchez, and Satoshi Sekine.
2000.
A treebank of Span-ish and its application to parsing.
The Proceedings of theWorkshop on Using Evaluation within HLT Programs: Re-sults and Trends, Athens, Greece.Borja Navarro, Montserrat Civit, Ma.
Anto`nia Mart?
?, RaquelMarcos, and Bele?n Ferna?ndez.
2003.
Syntactic, semanticand pragmatic annotation in Cast3LB.
Shallow Processingof Large Corpora (SProLaC), a Workshop of Corpus Lin-guistics, 2003, Lancaster, UK.802
