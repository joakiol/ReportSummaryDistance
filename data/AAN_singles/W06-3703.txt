Speech to Speech Translation for Medical Triage in KoreanFarzad Ehsani, Jim Kimzey, Demitrios Master, Karen Sudre Hunil ParkEngineering DepartmentSehda, Inc.
Independent ConsultantMountain View, CA 94043 Seoul, Korea{farzad,jkimzey,dlm,karen}@sehda.com phunil@hotmail.comAbstractS-MINDS is a speech translation engine,which allows an English speaker to communi-cate with a non-English speaker easily withina question-and-answer, interview-style format.It can handle limited dialogs such as medicaltriage or hospital admissions.
We have builtand tested an English-Korean system for do-ing medical triage with a translation accuracyof 79.8% (for English) and 78.3% (for Ko-rean) for all non-rejected utterances.
We willgive an overview of the system building proc-ess and the quantitative and qualitatively sys-tem performance.1 IntroductionSpeech translation technology has the potential togive nurses and other clinicians immediate accessto consistent, easy-to-use, and accurate medicalinterpretation for routine patient encounters.
Thiscould improve safety and quality of care for pa-tients who speak a different language from that ofthe healthcare provider.This paper describes the building and testing of aspeech translation system, S-MINDS (SpeakingMultilingual Interactive Natural Dialog System),built in less than 4 months from specification to thetest scenario described.
Although this paper showsa number of areas for improvement in the S-MINDS system, it does demonstrate that buildingand deploying a successful speech translation sys-tem is becoming possible and perhaps even com-mercially viable.2 BackgroundSehda is focused on creating speech translationsystems to overcome language barriers in health-care settings in the U.S.
The number of people inthe U.S. who speak a language other than Englishis large and growing, and Spanish is the mostcommonly spoken language next to English.
Ac-cording to the 2000 census, 18% of the U.S. popu-lation aged 5 and older (47 million people) did notspeak English at home.1 This represents a 48% in-crease from the 1990 figure.
In 2000, 8% of thepopulation (21 million) was Limited English Profi-cient (LEP).
More than 65% of the LEP population(almost 14 million people) spoke Spanish.A body of research shows that language barriersimpede access to care, compromise quality, andincrease the risk of adverse outcomes.
Althoughtrained medical interpreters and bilingual health-care providers are effective in overcoming suchlanguage barriers, the use of semi-fluent healthcareprofessionals and ad hoc interpreters causes moreinterpreter errors and lower quality of care (Flores2005).One study analyzed the problem of language barri-ers for hospitalized inpatients.
The study, whichfocused on pediatric patients, sought to determinewhether patients whose families have a languagebarrier are more likely to incur serious medicalerrors than patients without a language barrier(Cohen et al, 2005).
The study?s conclusion wasthat patients of LEP families had a twofold in-creased risk for serious medical incident comparedwith patients whose families did not have a lan-guage barrier.
It is important to note that the LEP1   US Census Bureau, 2000patients in this study were identified as needinginterpreters during their inpatient stay and medicalinterpreters were available.Although the evidence favors using trained medi-cal interpreters, there is a gap between best prac-tice and reality.
Many patients needing aninterpreter do not get one, and many must use adhoc interpreters.
In a study of 4,161 uninsured pa-tients who received care in 23 hospitals in 16 cit-ies, more than 50% who needed an interpreter didnot get one (Andrulis et al, 2002).Another study surveyed 59 residents in a pediatricresidency program in an urban children?s hospital(O?Leary and Hampers, 2003).
Forty of the 59 resi-dents surveyed spoke little or no Spanish.
Again, itis important to note that this hospital had in-housemedical interpreters.
Of this group of nonproficientresidents:?
100% agreed that the hospital interpreterswere effective; however, 75% ?never?
oronly ?sometimes?
used the hospital inter-preters.?
53% used their inadequate language skillsin the care of patients ?often?
or ?everyday.??
53% believed the families ?never?
or only?sometimes?
understood their child?s diag-nosis.?
43% believed the families ?never?
or only?sometimes?
understood discharge instruc-tions.?
40% believed the families ?never?
or only?sometimes?
understood the follow-upplan.?
28% believed the families ?never?
or only?sometimes?
understood the medications.?
53% reported calling on their Spanish-proficient colleagues ?often?
or ?everyday?
for help.?
80% admitted to avoiding communicationwith non-English-speaking families.The conclusion of the study was as follows: ?De-spite a perception that they are providing subopti-mal communication, nonproficient residents rarelyuse professional interpreters.
Instead, they tend torely on their own inadequate language skills, im-pose on their proficient colleagues, or avoid com-munication with Spanish-speaking families withLEP.
?Virtually every study on language barriers suggeststhat these residents are not unique.
Physicians andstaff at several hospitals have told Sehda that theyare less likely to use a medical interpreter or tele-phone-based interpreter because it takes too longand is too inconvenient.
Sehda believes that tobridge this gap requires 2-way speech translationsolutions that are immediately available, easy touse, accurate, and consistent in interpretation.The need for speech translation exists in health-care, and a lot of work has been done in speechtranslation over the past two decades.
Carnegie-Mellon University has been experimenting withspoken language translation in its JANUS projectsince the late 1980s (Waibel et al, 1996).
TheUniversity of Karlsruhe, Germany, has also beeninvolved in an expansion of JANUS.
In 1992, thesegroups joined ATR in the C-STAR consortium(Consortium for Speech Translation Advanced Re-search) and in January 1993 gave a successful pub-lic demonstration of telephone translation betweenEnglish, German and Japanese, within the limiteddomain of conference registrations (Woszczyna,1993).
A number of other large companies andlaboratories including NEC (Isotani, et al, 2003) inJapan, the Verbmobil Consortium (Wahlster,2000), NESPOLE!
Consortium (Florian et al,2002), AT&T (Bangalore and Riccardi, 2001), andATR have been making their own research effort(Yasuda et al, 2003).
LC-Star and TC-Star are tworecent European efforts to gather the data and theindustrial requirements to enable pervasive speech-to-speech translation (Zhang, 2003).
Most recently,the DARPA TransTac program (previously knownas Babylon) has been focusing on developing de-ployable systems for English to Iraqi Arabic.3 System DescriptionUnlike other systems that try to solve the speechtranslation problem with the assumption that thereis a moderate amount of data available, S-MINDSfocuses on rapid building and deployment ofspeech translation systems in languages where lit-tle or no data is available.
S-MINDS allows theuser to communicate easily in a question-and-answer, interview-style conversation across lan-guages in limited domains such as border control,hospital admissions or medical triage, or other nar-row interview fields.S-MINDS uses a number of voice-independentspeech recognition engines with the usage depend-ent on the languages and the particular domain.These engines include Nuance 8.52, SRI EduSpeak2.03, and Entropic?s HTK-based engine.4 There is adialog/translation creation tool that allows us tocompile and run our created dialogs with any ofthese engines.
This allows our developers to befree from the nuances of any particular engine thatis deployed.
S-MINDS uses a combination ofgrammars and language models with these engines,depending on the task and the availability of train-ing data.
In the case of the system described in thisdocument, we were using Nuance 8.5 for bothEnglish and Korean speech recognition.We use our own semantic parser, which identifieskeywords and phrases that are tagged by the user;these in turn are fed into an interpretation engine.Because of the limited context, we can achievehigh translation accuracy with the interpretationengine.
However, as the name suggests, this enginedoes not directly translate users?
utterances butinterprets what they say and paraphrases theirstatements.
Finally, we use a voice generation sys-tem (which splices human recordings) along withthe Festival TTS engine to output the translations.This has been recently replaced by the CepstralTTS engine.Additionally, S-MINDS includes a set of tools tomodify and augment the existing system with addi-tional words and phrases in the field in a matter ofa few minutes.The initial task given to us was a medical disasterrecovery scenario that might occur near an Ameri-can military base in Korea.
We were given about270 questions and an additional 90 statements thatmight occur on the interviewer side.
Since our sys-tem is an interview-driven system (sometimes re-ferred to as ?1.5-way?
), the second-languageperson is not given the option of initiating conver-sations.
The questions and statements given to uscovered several domains related to the task above,including medical triage, force protection at the2   http://www.nuance.com/nuancerecognition/3   http://www.speechatsri.com/products/eduspeak.shtml4   http://htk.eng.cam.ac.uk/installation gate, and some disaster recovery ques-tions.
In addition to the 270 assigned questions, wecreated 120 of our own in order to make the do-mains more complete.3.1 Data CollectionSince we assumed that we could internally gener-ate the English language data used to ask the ques-tion but not the language data on the Korean side,our entire focus for the data collection task was onKorean.
As such, we collected about 56,000 utter-ances from 144 people to answer the 390 questionsdescribed above.
This data collection was con-ducted over the course of 2 months via a tele-phone-based computer system that the nativeKorean speakers could call.
The system first intro-duced the purpose of the data collection and thenpresented the participants with 12 different scenar-ios.
The participants were then asked a subset ofthe questions after each of the scenarios.
One ad-vantage of the phone-based system ?
in addition tothe savings in administrative costs ?
was that theparticipants were free to do the data collection anytime during the day or night, from any location.The system also allowed participants to hang upand call back at a later time.
The participants werepaid only if they completed all the scenarios.Of this data, roughly 7% was unusable and wasthrown away.
Another 31% consisted of one-wordanswers (like ?yes?).
The rest of the data consistedof utterances 2 to 25 words long.
Approximately85% of the usable data was used for training; theremainder was used for testing.The transcription of the data started one week afterthe start of the data collection, and we startedbuilding the grammars three weeks later.3.2  System DevelopmentWe have an extensive set of tools that allow non-specialists, with a few days of training, to buildcomplete mission-oriented domains.
In this project,we used three bilingual college graduates who hadno knowledge of linguistics.
We spent the first 10days training them and the next two weeks closelysupervising their work.
Their work involved takingthe sentences that were produced from the datacollection and building grammars for them untilthe ?coverage?
of our grammars ?
that is, the num-ber of utterances from the training set that our sys-tem would handle ?
was larger than a set threshold(generally set between 80% and 90%).
Because ofthe scarcity of Korean-language data, we built thissystem based entirely on grammar language mod-els rather than statistical language models.
Gram-mars are generally more rigid than statisticallanguage models, and as such grammars tend tohave higher in-domain accuracy and much lowerout-of-domain accuracy5 than statistical languagemodels.
This means that the system performancewill depend greatly upon on how well our gram-mars cover the domains.The semantic tagging and the paraphrase transla-tions were built simultaneously with the grammars.This involved finding and tagging the semanticclasses as well as the key concepts in each utter-ance.
Frame-based translations were performed bydoing concept and semantic transfer.
Because ourtools allowed the developers to see the resultingframe translations right away, they were able tomake fixes to the system as they were building it;hence, the system-building time was greatly re-duced.We used about 15% of the collected telephone datafor batch testing.
Before deployment, our averageword accuracy on the batch results was 92.9%.
Thetranslation results were harder to measure directly,mostly because of time constraints.3.3 System TestingWe tested our system with 11 native Koreanspeakers, gathering 968 utterances from them.
Theresults of the test are shown in Table 1.
Most of thevalid rejected utterances occurred because partici-pants spoke too softly, too loudly, before theprompt, or in English.
Note that there was one ut-terance with bad translation; that and a number ofother problems were fixed before the actual fieldtesting.5   Note that there are many factors effecting both gram-mar-based and statistical language model based speechrecognition, including noise, word perplexity, acousticconfusability, etc.
The statement above has been truewith some of the experiments that we have done, but wecan not claim that it is universally true.Category PercentageTotal Recognized Correctly 82.0%Total Recognized Incorrectly 5.8%Total Valid Rejection 8.0%Total Invalid Rejected   4.1%Total unclear translations 0.1%Table 1: Korean-to-English system testing re-sults for the 11 native Korean speakers.4 Experimental SetupA military medical group used S-MINDS during amedical training exercise in January 2005 in Carls-bad, California.
The testing of speech translationsystems was integrated into the exercise to assessthe viability of such systems in realistic situations.The scenario involved a medical aid station nearthe front lines treating badly injured civilians.
Themedical facilities were designed to quickly triageseverely wounded patients, provide life-savingsurgery if necessary, and transfer the patients to asafer area as soon as possible.4.1 User TrainingOften the success or failure of these interactivesystems is determined by how well the users aretrained on the systems?
features.Training and testing on S-MINDS took place fromNovember 2004 through January 2005.
The train-ing had three parts: a system demonstration in No-vember, two to three hours of training per personin December, and another three-hour training ses-sion in January.
About 30 soldiers were exposed toS-MINDS during this period.
Because of the tsu-nami in Southeast Asia, many of the people whoattended the November demo and December train-ing were not available for the January training andthe exercise.
Nine service members used S-MINDS during the exercise.
Most of them had at-tended only the training session in January.4.2 Test ScenariosKorean-speaking ?patients?
arrived by military am-bulance.
They were received into one of three tentswhere they were (notionally) triaged, treated, andprepared for surgery.
The tents were about 20 feetwide by 25 feet deep, and each had six to eight cotsfor patients.
The tents had lights and electricity.The environment was noisy, sandy, and ?bloody.
?The patients?
makeup coated our handsets by theend of the day.
There were many soldiers availableto help and watch.
Nine service members used S-MINDS during a four-hour period.All of the ?patients?
spoke both English and Ko-rean.
A few ?patients?
were native Korean speak-ers, and two were American service members whospoke Korean fairly fluently but with an accent.The ?patients?
were all presented as severely in-jured from burns, explosions, and cuts and in needof immediate trauma care.The ?patients?
were instructed to act as if they werein great pain.
Some did, and they sounded quiterealistic.
In fact, their recorded answers to ques-tions were sometimes hard for a native Koreanspeaker to understand.
The background noise in thetents was quite loud (because of the number ofpeople involved, screaming patients and closequarters).
Although we did not directly measurethe noise; we estimate it ranged from 65 to 75 deci-bels.4.3 Physical and Hardware SetupS-MINDS is a flexible system that can be config-ured in different ways depending on the needs ofthe end user.
Because of the limited time availablefor training, the users were trained on a singlehardware setup, tailored to our understanding ofhow the exercises would be conducted.
Diagramsavailable before the exercises showed that eachtent would have a ?translation station?
where Ko-rean-speaking patients would be brought.
The ex-perimenters (two of the authors) had expected thatthe tents would be positioned at least 40 feet apart.In reality, the tents were positioned about 5 feetapart, and there was no translation station.Our original intent was to use S-MINDS on a SonyU-50 tablet computer mounted on a computerstand with a keyboard and mouse at the translationstation, and for a prototype wireless device ?
basedon a Bluetooth-like technology to eliminate theneed for wires between the patient and the system?
that we had built previously.
However, becauseof changes in the conduct of the exercise, the ex-perimenters had to step in and quickly set up twoof the S-MINDS systems without the wireless sys-tem (because of the close proximity of the tents)and without the computer stands.
The keyboardsand mice were also removed so that the S-MINDSsystems could be made portable.
The medicsworked in teams of two; one medic would hold thecomputer and headset for the injured patient whilethe other medic conducted the interview.5 ResultsThe nine participants used our system to commu-nicate with ?patients?
over a four-hour period.
Weanalyzed qualitative problems with using the sys-tem and quantitative results of translation accu-racy.5.1 Problems with System UsageWe observed a number of problems in the test sce-narios with our system.
These represent some ofthe more common problems with the S-MINDSsystem.
The authors suspect these may be endemicof all such systems.5.1.1 Inadequate Training on the SystemUsers were trained to use the wireless units, whichinterfered with each other when used in close prox-imity.
For the exercise, we had to set up the unitswithout the wireless devices because the users hadnot been trained on this type of setup.
As a result,service members were forced to use a differentsystem from the one they were trained on.Also, the users had difficulty navigating to theright domain.
S-MINDS has multiple domainseach optimized for a particular scenario (medicaltriage, pediatrics, etc.
), but the user training did notinclude navigation among domains.5.1.2 User Interface IssuesThe user interface and the system?s user feedbackmessages caused unnecessary confusion with theinterviewers.
The biggest problem was that thesystem responded with, ?I?m sorry, I didn?t hearthat clearly?
whenever a particular utterancewasn?t recognized.
This made the users think theyshould just repeat their utterance over and over.
Infact, the problem was that they were saying some-thing that were out of domain or did not fit anydialogs in S-MINDS, so no matter how many timesthey repeated the phrase, it would not be recog-nized.
This caused the users significant frustration.5.2.
Quantative AnalysisDuring the system testing, there were 363 recordedinteractions for the English speakers.
Unfortu-nately, the system was not set up to record the ut-terances that had a very low confidence score (asdetermined by the Nuance engine), and the userwas asked to repeat those utterances again.
Here isthe rough breakdown for all of the English interac-tions:?
52.5% were translated correctly into Ko-rean?
34.2% were rejected by the system?
13.3% had misrecognition or mistranslationerrorsThis means that S-MINDS tried to recognize andtranslate 65.8% of the English utterances and ofthose 79.8% were correctly translated.
A more de-tailed analysis is presented in Figure 1.Figure 1: Detailed breakdown for the Englishutterances and percentage breakdown foreach category.The Korean speakers?
responses to each of thequestions that were recognized and translated areanalyzed in Figure 2.
Note that the accuracy for thenon-rejected responses is 78.3%.Figure 2: Detailed breakdown of the recogni-tion for the Korean utterances and percentagebreakdown for each category.6 DiscussionAlthough these results are less than impressive, aclose evaluation pointed to three areas where aconcentration of effort would significantly improvetranslation accuracy and reduce mistranslations.These areas were:1) Data collection with English speakers to in-crease coverage on the dialogs.a) 34% of the things the soldiers said werethings S-MINDS was not designed totranslate.b) We had assumed that our existing Englishsystem would have adequate coveragewithout any additional data collection.2) User verification on low-confidence results.3) Improved feedback prompts when a phrase isnot recognized; for example:a) One user said, ?Are you allergic to any al-lergies??
three times before he caught him-self and said, ?Are you allergic to anymedications?
?b) Another user said, ?How old are you?
?seven times before realizing he needed toswitch to a different domain, where he wasable to have the phrase translated.c) Another user repeated, ?What is yourname??
nine times before giving up on thephrase (this phrase wasn?t in the S-MINDSKorean medical mission set).Beyond improving the coverage, the system?s pri-mary problem seemed to be in the voice user inter-face since even the trained users had a difficulttime in using the system.Statements +Questions(100%)Concepts inDialog (90%)Concepts notin Dialog(10%)Rejected(7.4%)IncorrectTransl.
(2.5%)In Grammar(64.7%)Not in Gram-mar (25.3%)CorrectTransl.
(50%)Rejected(8.3%)CorrectTransl.
(2.5%)Rejected(14.9%)IncorrectTransl.
(8.0%)IncorrectTransl.
(2.8%)Wrong topicSelect (3.6%)KoreanResponses(100%)TranslatedCorrectly(63.4%)Mistranslated(4.2%)Could NotHear(13.4%)Rejected(19.0%)The attempt at realism in playing out a high-traumascenario may have detracted from the effectivenessof the event as a test of the systems?
abilities undermore routine (but still realistic) conditions.7 New ResultsBased on the results of this experiment, we had asecondary deployment in a medical setting for avery similar system.We applied what we had learned to that setting andachieved better results in a few areas.
For example:1.
Data collection in English helped tremen-dously.
S-MINDS recognized about 40%more concepts than it had been able to rec-ognize using only grammars created bysubject-matter experts.2.
Verbal verification of the recognized utter-ance was added to system, and that im-proved the user confidence, although toomuch verification tended to frustrate theusers.3.
Feedback prompts were designed to givemore specific feedback, which seemed toreduce user frustration and the number ofmistakes.Overall, the system performance seemed to im-prove.
We continue to gather data on this task, andwe believe that this is going to enable us to identifythe next set of problems that need to be solved.8 AcknowledgementThis research was funded in part by the LASERACTD.
We specially wish to thank Mr. Pete Fisherof ARL for his generous support and his participa-tion in discussions related to this project.ReferencesAndrulis Dennis, Nanette Goodman, Carol Pryor(2002), ?What a Difference an Interpreter Canmake?
April 2002.
Access Project,www.accessproject.org/downloads/c_LEPreportENG.pdfBangalore, S. and G. Riccardi, (2001), ?A FiniteState Approach to Machine Translation,?
NorthAmerican ACL 2001, Pittsburgh.Cohen, L, F. Rivara, E. K. Marcuse, H. McPhillips,and R. Davis, (2005), ?Are Language BarriersAssociated With Serious Medical Events inHospitalized Pediatric Patients?
?, Pediatrics,September 1, 2005; 116(3): 575 - 579Flores Glenn, (2005), ?The Impact of Medical In-terpreter Services on the Quality of Health Care:A Systematic Review,?
Medical Care Researchand Review, Vol.
62, No.
3, pp.
255-299Florian M., et.
al.
(2002), ?Enhancing the Usabilityand Performance of NESPOLE!
: a Real-WorldSpeech-to-Speech Translation System?, HLT2002, San Diego, California U.S., March 2002.Isotani, R., Kiyoshi Yamabana, Shinichi Ando,Ken Hanazawa, Shin-ya Ishikawa and Ken-ichiISO  (2003), ?Speech-to-Speech TranslationSoftware on PDAs for Travel Conversation,?NEC Research and Development, Apr.
2003,Vol.44, No.2.O?Leary and Hampers (2003) ?The Truth AboutLanguage Barriers: One Residency Program'sExperience,?
Pediatrics, May 1, 2003; 111(5):pp.
569 - 573.Keiji Yasuda, Eiichiro Sumita, Seiichi Yamamoto,Genichiro Kikui, Masazo Yanagida, ?Real-TimeEvaluation Architecture for MT Using MultipleBackward Translations,?
Recent Advances inNatural Language Processing, pp.
518-522,Sep., 2003Wahlster, W. (2000), Verbmobil: Foundations ofSpeech-to-Speech Translation.
Springer.Waibel, A., (1996), ?Interactive Translation ofConversational Speech,?
IEEE Computer, July1996, 29-7, pp.
41-48.Woszczyna, et al, (1993), ?Recent Advances inJANUS: A Speech Translation System,?DARPA Speech and Natural Language Work-shop 1993, session 6 ?
MT.Zhang, Ying, (2003), ?Survey of Current SpeechTranslation Research,?
Found on Web:http://projectile.is.cs.cmu.edu/research/public/talks/ speechTranslation/sst-survey-joy.pdf
