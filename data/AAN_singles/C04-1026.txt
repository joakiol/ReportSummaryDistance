A Relational Syntax-Semantics Interface Based on Dependency GrammarRalph Debusmann Denys Duchier?
Alexander Koller Marco Kuhlmann Gert Smolka Stefan ThaterSaarland University, Saarbr?cken, Germany ?LORIA, Nancy, France{rade|kuhlmann|smolka}@ps.uni-sb.de, duchier@loria.fr, {koller|stth}@coli.uni-sb.deAbstractWe propose a syntax-semantics interface thatrealises the mapping between syntax and se-mantics as a relation and does not make func-tionality assumptions in either direction.
Thisinterface is stated in terms of Extensible De-pendency Grammar (XDG), a grammar formal-ism we newly specify.
XDG?s constraint-basedparser supports the concurrent flow of informa-tion between any two levels of linguistic rep-resentation, even when only partial analyses areavailable.
This generalises the concept of under-specification.1 IntroductionA key assumption of traditional syntax-semanticsinterfaces, starting with (Montague, 1974), is thatthe mapping from syntax to semantics is functional,i.
e. that once we know the syntactic structure of asentence, we can deterministically compute its se-mantics.Unfortunately, this assumption is typically notjustified.
Ambiguities such as of quantifier scopeor pronominal reference are genuine semantic am-biguities; that is, even a syntactically unambigu-ous sentence can have multiple semantic readings.Conversely, a common situation in natural languagegeneration is that one semantic representation canbe verbalised in multiple ways.
This means that therelation between syntax and semantics is not func-tional at all, but rather a true m-to-n relation.There is a variety of approaches in the litera-ture on syntax-semantics interfaces for coping withthis situation, but none of them is completely sat-isfactory.
One way is to recast semantic ambiguityas syntactic ambiguity by compiling semantic dis-tinctions into the syntax (Montague, 1974; Steed-man, 1999; Moortgat, 2002).
This restores function-ality, but comes at the price of an artificial blow-up of syntactic ambiguity.
A second approach is toassume a non-deterministic mapping from syntaxto semantics as in generative grammar (Chomsky,1965), but it is not always obvious how to reversethe relation, e. g. for generation.
For LFG, the oper-ation of functional uncertaintainty allows for a re-stricted form of relationality (Kaplan and MaxwellIII, 1988).
Finally, underspecification (Egg et al,2001; Gupta and Lamping, 1998; Copestake et al,2004) introduces a new level of representation,which can be computed functionally from a syntac-tic analysis and encapsulates semantic ambiguity ina way that supports the enumeration of all semanticreadings by need.In this paper, we introduce a completely rela-tional syntax-semantics interface, building upon theunderspecification approach.
We assume a set oflinguistic dimensions, such as (syntactic) immedi-ate dominance and predicate-argument structure; agrammatical analysis is a tuple with one componentfor each dimension, and a grammar describes a setof such tuples.
While we make no a priori function-ality assumptions about the relation of the linguisticdimensions, functional mappings can be obtainedas a special case.
We formalise our syntax-seman-tics interface using Extensible Dependency Gram-mar (XDG), a new grammar formalism which gen-eralises earlier work on Topological DependencyGrammar (Duchier and Debusmann, 2001).The relational syntax-semantics interface is sup-ported by a parser for XDG based on constraint pro-gramming.
The crucial feature of this parser is thatit supports the concurrent flow of possibly partial in-formation between any two dimensions: once addi-tional information becomes available on one dimen-sion, it can be propagated to any other dimension.Grammaticality conditions and preferences (e. g. se-lectional restrictions) can be specified on their nat-ural level of representation, and inferences on eachdimension can help reduce ambiguity on the oth-ers.
This generalises the idea of underspecifica-tion, which aims to represent and reduce ambiguitythrough inferences on a single dimension only.The structure of this paper is as follows: in Sec-tion 2, we give the general ideas behind XDG, itsformal definition, and an overview of the constraint-based parser.
In Section 3, we present the relationalsyntax-semantics interface, and go through exam-ples that illustrate its operation.
Section 4 showshow the semantics side of our syntax-semantics in-terface can be precisely related to mainstream se-mantics research.
We summarise our results andpoint to further work in Section 5.2 Extensible Dependency GrammarThis section presents Extensible DependencyGrammar (XDG), a description-based formalismfor dependency grammar.
XDG generalizes previ-ous work on Topological Dependency Grammar(Duchier and Debusmann, 2001), which focussedon word order phenomena in German.2.1 XDG in a NutshellXDG is a description language over finite labelledgraphs.
It is able to talk about two kinds of con-straints on these structures: The lexicon of an XDGgrammar describes properties local to individualnodes, such as valency.
The grammar?s principlesexpress constraints global to the graph as a whole,such as treeness.
Well-formed analyses are graphsthat satisfy all constraints.An XDG grammar allows the characterisationof linguistic structure along several dimensions ofdescription.
Each dimension contains a separategraph, but all these graphs share the same set ofnodes.
Lexicon entries synchronise dimensions byspecifying the properties of a node on all dimen-sions at once.
Principles can either apply to a singledimension (one-dimensional), or constrain the rela-tion of several dimensions (multi-dimensional).Consider the example in Fig.
1, which shows ananalysis for a sentence of English along two dimen-sions of description, immediate dominance (ID) andlinear precedence (LP).
The principles of the under-lying grammar require both dimensions to be trees,and the LP tree to be a ?flattened?
version of the IDtree, in the sense that whenever a node v is a tran-sitive successor of a node u in the LP tree, it mustalso be a transitive successor of u in the ID tree.
Thegiven lexicon specifies the potential incoming andrequired outgoing edges for each word on both di-mensions.
The word does, for example, accepts noincoming edges on either dimension and must there-fore be at the root of both the ID and the LP tree.
It isrequired to have outgoing edges to a subject (subj)and a verb base form (vbse) in the ID tree, needsfillers for a subject (sf) and a verb complement field(vcf) in the LP tree, and offers an optional field fortopicalised material (tf).
All these constraints aresatisfied by the analysis, which is thus well-formed.2.2 FormalisationFormally, an XDG grammar is built up of dimen-sions, principles, and a lexicon, and characterises aset of well-formed analyses.A dimension is a tuple D = (Lab,Fea,Val,Pri) ofa set Lab of edge labels, a set Fea of features, a setVal of feature values, and a set of one-dimensionalsubjvbseobjwhat does John eatsfvcfwhat does John eattfword inID outID inLP outLPwhat {obj?}
{} {tf?}
{}does {} {subj,vbse} {} {tf?,sf,vcf}John {subj?,obj?}
{} {sf?,of?}
{}eat {vbse?}
{obj} {vcf?}
{}Figure 1: XDG analysis of ?what does John eat?principles Pri.
A lexicon for the dimension D is aset Lex ?
Fea ?
Val of total feature assignments (orlexical entries).
A D-structure, representing an anal-ysis on dimension D, is a triple (V,E,F) of a set Vof nodes, a set E ?V ?V ?Lab of directed labellededges, and an assignment F : V ?
(Fea ?
Val) oflexical entries to nodes.
V and E form a graph.
Wewrite StrD for the set of all possible D-structures.The principles characterise subsets of StrD that havefurther dimension-specific properties, such as beinga tree, satisfying assigned valencies, etc.
We assumethat the elements of Pri are finite representations ofsuch subsets, but do not go into details here; someexamples are shown in Section 3.2.An XDG grammar ((Labi,Feai,Vali,Prii)ni=1,Pri,Lex) consists of n dimensions, multi-dimensionalprinciples Pri, and a lexicon Lex.
An XDG analysis(V,Ei,Fi)ni=1 is an element of Ana = Str1???
?
?Strnwhere all dimensions share the same set of nodes V .Multi-dimensional principles work just like one-dimensional principles, except that they specifysubsets of Ana, i. e. couplings between dimensions(e. g. the flattening principle between ID and LP inSection 2.1).
The lexicon Lex ?
Lex1 ?
??
?
?
Lexnconstrains all dimensions at once.
An XDG analysisis licenced by Lex iff (F1(w), .
.
.
,Fn(w)) ?
Lex forevery node w ?V .In order to compute analyses for a given input, wemodel it as a set of input constraints (Inp), whichagain specify a subset of Ana.
The parsing prob-lem for XDG is then to find elements of Ana thatare licenced by Lex and consistent with Inp andPri.
Note that the term ?parsing problem?
is tradi-tionally used only for inputs that are sequences ofwords, but we can easily represent surface realisa-tion as a ?parsing?
problem in which Inp specifies asemantic dimension; in this case, a ?parser?
wouldcompute analyses that contain syntactic dimensionsfrom which we can read off a surface sentence.2.3 Constraint SolverThe parsing problem of XDG has a natural read-ing as a constraint satisfaction problem (CSP) (Apt,2003) on finite sets of integers; well-formed anal-yses correspond to the solutions of this problem.The transformation, whose details we omit due tolack of space, closely follows previous work on ax-iomatising dependency parsing (Duchier, 2003) andincludes the use of the selection constraint to effi-ciently handle lexical ambiguity.We have implemented a constraint solver forthis CSP using the Mozart/Oz programming system(Smolka, 1995; Mozart Consortium, 2004).
Thissolver does a search for a satisfying variable assign-ment.
After each case distinction (distribution), itperforms simple inferences that restrict the rangesof the finite set variables and thus reduce the sizeof the search tree (propagation).
The successfulleaves of the search tree correspond to XDG anal-yses, whereas the inner nodes correspond to partialanalyses.
In these cases, the current constraints aretoo weak to specify a complete analysis, but theyalready express that some edges or feature valuesmust be present, and that others are excluded.
Partialanalyses will play an important role in Section 3.3.Because propagation operates on all dimensionsconcurrently, the constraint solver can frequentlyinfer information about one dimension from infor-mation on another, if there is a multi-dimensionalprinciple linking the two dimensions.
These infer-ences take place while the constraint problem is be-ing solved, and they can often be drawn before thesolver commits to any single solution.Because XDG allows us to write grammars withcompletely free word order, XDG solving is an NP-complete problem (Koller and Striegnitz, 2002).This means that the worst-case complexity of thesolver is exponential, but the average-case complex-ity for the hand-crafted grammars we experimentedwith is often better than this result suggests.
Wehope there are useful fragments of XDG that wouldguarantee polynomial worst-case complexity.3 A Relational Syntax-Semantics InterfaceNow that we have the formal and processing frame-works in place, we can define a relational syntax-semantics interface for XDG.
We will first showhow we encode semantics within the XDG frame-work.
Then we will present an example grammar(including some principle definitions), and finallygo through an example that shows how the rela-tionality of the interface, combined with the con-currency of the constraint solver, supports the flowof information between different dimensions.3.1 Representing MeaningWe represent meaning within XDG on two dimen-sions: one for predicate-argument structure (PA),every student reads a booksubjdetobjdetevery student reads a bookagargpatargi.
ID-tree ii.
PA-structuresevery student reads a booksrrsevery student reads a booksrriii.
scope treesFigure 2: Two analyses for the sentence ?every stu-dent reads a book.
?and one for scope (SC).
The function of the PA di-mension is to abstract over syntactic idiosyncrasiessuch as active-passive alternations or dative shifts,and to make certain semantic dependencies e. g. incontrol constructions explicit; it deals with conceptssuch as agent and patient, rather than subject and ob-ject.
The purpose of the SC dimension is to reflectthe structure of a logical formula that would repre-sent the semantics, in terms of scope and restriction.We will make this connection explicit in Section 4.In addition, we assume an ID dimension as above.We do not include an LP dimension only for ease ofpresentation; it could be added completely orthogo-nally to the three dimensions we consider here.While one ID structure will typically correspondto one PA structure, each PA structure will typicallybe consistent with multiple SC structures, becauseof scope ambiguities.
For instance, Fig.
2 shows theunique ID and PA structures for the sentence ?Ev-ery student reads a book.?
These structures (and theinput sentence) are consistent with the two possi-ble SC-structures shown in (iii).
Assuming a David-sonian event semantics, the two SC trees (togetherwith the PA-structure) represent the two readings ofthe sentence:?
?e.
?x.student(x) ??y.book(y)?
read(e,x,y)?
?e.?y.book(y)?
?x.student(x) ?
read(e,x,y)3.2 A Grammar for a Fragment of EnglishThe lexicon for an XDG grammar for a small frag-ment of English using the ID, PA, and SC dimensionsis shown in Fig.
3.
Each row in the table specifies a(unique) lexical entry for each part of speech (deter-miner, common noun, proper noun, transitive verband preposition); there is no lexical ambiguity inthis grammar.
Each column specifies a feature.
Themeaning of the features will be explained togetherinID outID inPA outPA inSC outSCDET {subj?,obj?,pcomp?}
{det!}
{ag?,pat?,arg?}
{quant!}
{r?,s?,a?}
{r!,s!
}CN {det?}
{prep?}
{quant?}
{mod?}
{r?,s?,a?}
{}PN {subj?,obj?,pcomp?}
{prep?}
{ag?,pat?,arg?}
{mod?}
{r?,s?,a?}
{r?,s!
}TV {} {subj!,obj!,prep?}
{} {ag!,pat!, instr?}
{r?,s?,a?}
{}PREP {prep?}
{pcomp!}
{mod?, instr?}
{arg!}
{r?,s?,a?}
{a!
}link codom contradomDET {quant 7?
{det}} {quant 7?
{r}} {}CN,PN {mod 7?
{prep}} {} {mod 7?
{a}}TV {ag 7?
{subj},pat 7?
{obj}, instr 7?
{prep}} {} {ag 7?
{s},pat 7?
{s}, instr 7?
{a}}PREP {arg 7?
{pcomp}} {} {arg 7?
{s}}Figure 3: The example grammar fragmentwith the principles that use them.The ID dimension uses the edge labels LabID ={det,subj,obj,prep,pcomp} resp.
for determinedcommon noun,1 subject, object, preposition, andcomplement of a preposition.
The PA dimensionuses LabPA = {ag,pat,arg,quant,mod, instr}, resp.for agent, patient, argument of a modifier, commonnoun pertaining to a quantifier, modifier, and instru-ment; and SC uses LabSC = {r,s,a} resp.
for restric-tion and scope of a quantifier, and for an argument.The grammar also contains three one-dimen-sional principles (tree, dag, and valency), andthree multi-dimensional principles (linking, co-dominance, and contra-dominance).Tree and dag principles.
The tree principle re-stricts ID and SC structures to be trees, and thedag principle restricts PA structures to be directedacyclic graphs.Valency principle.
The valency principle, whichwe use on all dimensions, states that the incom-ing and outgoing edges of each node must obey thespecifications of the in and out features.
The possi-ble values for each feature ind and outd are subsetsof Labd ?
{!,?,?}.
`!
specifies a mandatory edgewith label `, `?
an optional one, and `?
zero or more.Linking principle.
The linking principle for di-mensions d1,d2 constrains how dependents on d1may be realised on d2.
It assumes a feature linkd1,d2whose values are functions that map labels fromLabd1 to sets of labels from Labd2 , and is specifiedby the following implication:vl?d1 v?
?
?l?
?
linkd1,d2(v)(l) : vl?
?d2 v?Our grammar uses this principle with the link fea-ture to constrain the realisations of PA-dependents inthe ID dimension.
In Fig.
2, the agent (ag) of readsmust be realised as the subject (subj), i. e.1We assume on all dimensions that determiners are theheads of common nouns.
This makes for a simpler relationshipbetween the syntactic and semantic dimensions.reads ag?PA every ?
readssubj?
ID everySimilarly for the patient and the object.
Thereis no instrument dependent in the example, so thispart of the link feature is not used.
An ergative verbwould use a link feature where the subject realisesthe patient; Control and raising phenomena can alsobe modelled, but we cannot present this here.Co-dominance principle.
The co-dominanceprinciple for d1,d2 relates edges in d1 to dominancerelations in the same direction in d2.
It assumes afeature codomd1,d2 mapping labels in Labd1 to setsof labels in Labd2 and is specified asvl?d1 v?
?
?l?
?
codomd1,d2(v)(l) : vl???
?d2v?Our grammar uses the co-dominance principle ondimension PA and SC to express, e. g., that thepropositional contribution of a noun must end up inthe restriction of its determiner.
For example, for thedeterminer every of Fig.
2 we have:every quant?
PA student ?
everyr??
?SCstudentContra-dominance principle.
The contra-domi-nance principle is symmetric to the co-dominanceprinciple, and relates edges in d1 to dominanceedges into the opposite direction in d2.
It assumesa feature contradomd1,d2 mapping labels of Labd1 tosets of labels from Labd2 and is specified asvl?d1 v?
??l?
?
contradomd1,d2(v)(l) : v?l???
?d2vOur grammar uses the contra-dominance principleon dimensions PA and SC to express, e. g., that pred-icates must end up in the scope of the quantifierswhose variables they refer to.
Thus, for the transi-tive verb reads of Fig.
2, we have:reads ag?PA every ?
everys??
?SCreadsreads pat?PA a ?
as??
?SCreadsMary saw a student with a bookagpatquantargquantMary saw a student with a bookssrsrMary saw a student with a booksubjobjdetargdetprepMary saw a student with a bookagpatquantargquantMary saw a student with a booksrsrMary saw a student with a booksubjobjdetargdetinstrasMary saw a student with a bookagpatargargquantMary saw a student with a bookssrsrMary saw a student with a booksubjobjdetargdetmodprepai.
Partial analysisii.
verb attachmentiii.
noun attachmentIDPASCFigure 4: Partial description (left) and two solutions (right) for ?Mary saw a student with a book.
?3.3 Syntax-Semantics InteractionIt is important to note at this point that the syntax-semantics interface we have defined is indeed re-lational.
Each principle declaratively specifies a setof admissible analyses, i. e. a relation between thestructures for the different dimensions, and the anal-yses that the complete grammar judges grammaticalare simply those that satisfy all principles.
The roleof the lexicon is to provide the feature values whichparameterise the principles defined above.The constraint solver complements this relation-ality by supporting the use of the principles to moveinformation between any two dimensions.
If, say,the left-hand side of the linking principle is found tobe satisfied for dimension d1, a propagator will inferthe right-hand side and add it to dimension d2.
Con-versely, if the solver finds that the right-hand sidemust be false for d2, the negation of the left-handside is inferred for d1.
By letting principles interactconcurrently, we can make some very powerful in-ferences, as we will demonstrate with the examplesentence ?Mary saw a student with a book,?
somepartial analyses for which are shown in Fig.
4.Column (i) in the figure shows the state after theconstraint solver finishes its initial propagation, atthe root of the search tree.
Even at this point, the va-lency and treeness principles have conspired to es-tablish an almost complete ID-structure.
By the link-ing principle, the PA-structure has been determinedsimilarly closely.
The SC-structure is still mostly un-determined, but by the co- and contra-dominanceprinciples, the solver has already established thatsome nodes must dominate others: A dotted edgewith label s in the picture means that the solverknows there must be a path between these two nodeswhich starts with an s-edge.
In other words, thesolver has computed a large amount of semantic in-formation from an incomplete syntactic analysis.Now imagine some external source tells us thatwith is a mod-child of student on PA, i. e. the anal-ysis in (iii).
This information could come e. g. froma statistical model of selectional preferences, whichwill judge this edge much more probable than aninstr-edge from the verb to the preposition (ii).Adding this edge will trigger additional inferencesthrough the linking principle, which can now inferthat with is a prep-child of student on ID.
In the otherdirection, the solver will infer more dominances onSC.
This means that semantic information can beused to disambiguate syntactic ambiguities, and se-mantic information such as selectional preferencescan be stated on their natural level of representation,rather than be forced into the ID dimension directly.Similarly, the introduction of new edges on SCcould trigger a similar reasoning process whichwould infer new PA-edges, and thus indirectly alsonew ID-edges.
Such new edges on SC could comefrom inferences with world or discourse knowledge(Koller and Niehren, 2000), scope preferences, orinteractions with information structure (Duchier andKruijff, 2003).4 Traditional SemanticsOur syntax-semantics interface represents seman-tic information as graphs on the PA and SC dimen-sions.
While this looks like a radical departure fromtraditional semantic formalisms, we consider thesegraphs simply an alternative way of presenting moretraditional representations.
We devote the rest of thepaper to demonstrating that a pair of a PA and a SCstructure can be interpreted as a Montague-style for-mula, and that a partial analysis on these two di-mensions can be seen as an underspecified semanticdescription.4.1 Montague-style InterpretationIn order to extract a standard type-theoretic expres-sion from an XDG analysis, we assign each node vtwo semantic values: a lexical value L(v) represent-ing the semantics of v itself, and a phrasal valueP(v) representing the semantics of the entire SC-subtree rooted at v. We use the SC-structure to de-termine functor-argument relationships, and the PA-structure to establish variable binding.We assume that nodes for determiners and propernames introduce unique individual variables (?in-dices?).
Below we will write ??v??
to refer to the in-dex of the node v, and we write ?` to refer to thenode which is the `-child of the current node in theappropriate dimension (PA or SC).
The semantic lex-icon is defined as follows; ?L(w)?
should be read as?L(v), where v is a node for the word w?.L(a) = ?P?Q?e.
?x(P(x)?Q(x)(e))L(book) = book?L(with) = ?P?x.(with?(???arg??
)(x)?P(x))L(reads) = read?(???pat??)(???ag??
)Lexical values for other determiners, commonnouns, and proper names are defined analogously.Note that we do not formally distinguish eventvariables from individual variables.
In particular,L(with) can be applied to either nouns or verbs,which both have type ?e, t?.We assume that no node in the SC-tree has morethan one child with the same edge label (which ourgrammar guarantees), and write n(`1, .
.
.
, `k) to in-dicate that the node n has SC-children over the edgelabels `1, .
.
.
, `k. The phrasal value for n is defined(in the most complex case) as follows:P(n(r,s)) = L(n)(P(?r))(?
??n?
?.P(?s))This rule implements Montague?s rule of quan-tification (Montague, 1974); note that ?
??n??
is abinder for the variable ??n??.
Nodes that have nos-children are simply functionally applied to thephrasal semantics of their children (if any).By way of example, consider the left-hand SC-structure in Fig.
2.
If we identify each node by theword it stands for, we get the following phrasal@@every?@@a?
@@read varvarstudentbookrssrevery student reads a bookrrssevery student reads a bookagargpatargFigure 5: A partial SC-structure and its correspond-ing CLLS description.value for the root of the tree:L(a)(L(book))(?x.L(every)(L(student)(?y.read?
(y)(x)))),where we write x for ??a??
and y for ??every??.
Thearguments of read?
are x and y because every anda are the arg and pat children of reads on the PA-structure.
After replacing the lexical values by theirdefinitions and beta-reduction, we obtain the fa-miliar representation for this semantic reading, asshown in Section 3.1.4.2 UnderspecificationIt is straightforward to extend this extraction oftype-theoretic formulas from fully specified XDGanalyses to an extraction of underspecified seman-tic descriptions from partial XDG analyses.
We willbriefly demonstrate this here for descriptions in theCLLS framework (Egg et al, 2001), which sup-ports this most easily.
Other underspecification for-malisms could be used too.Consider the partial SC-structure in Fig.
5, whichcould be derived by the constraint solver for thesentence from Fig.
2.
We can obtain a CLLS con-straint from it by first assigning to each node ofthe SC-structure a lexical value, which is now a partof the CLLS constraint (indicated by the dotted el-lipses).
Because student and book are known to be r-daughters of every and a on SC, we plug their CLLSconstraints into the r-holes of their mothers?
con-straints.
Because we know that reads must be dom-inated by the s-children of the determiners, we addthe two (dotted) dominance edges to the constraint.Finally, variable binding is represented by the bind-ing constraints drawn as dashed arrows, and can bederived from PA exactly as above.5 ConclusionIn this paper, we have shown how to build a fully re-lational syntax-semantics interface based on XDG.This new grammar formalism offers the grammardeveloper the possibility to represent different kindsof linguistic information on separate dimensionsthat can be represented as graphs.
Any two dimen-sions can be linked by multi-dimensional principles,which mutually constrain the graphs on the two di-mensions.
We have shown that a parser based onconcurrent constraint programming is capable of in-ferences that restrict ambiguity on one dimensionbased on newly available information on another.Because the interface we have presented makesno assumption that any dimension is more ?basic?than another, there is no conceptual difference be-tween parsing and generation.
If the input is the sur-face sentence, the solver will use this informationto compute the semantic dimensions; if the input isthe semantics, the solver will compute the syntacticdimensions, and therefore a surface sentence.
Thismeans that we get bidirectional grammars for free.While the solver is reasonably efficient for many(hand-crafted) grammars, it is an important goalfor the future to ensure that it can handle large-scale grammars imported from e.g.
XTAG (XTAGResearch Group, 2001) or induced from treebanks.One way in which we hope to achieve this is to iden-tify fragments of XDG with provably polynomialparsing algorithms, and which contain most usefulgrammars.
Such grammars would probably have tospecify word orders that are not completely free,and we would have to control the combinatoricsof the different dimensions (Maxwell and Kaplan,1993).
One interesting question is also whether dif-ferent dimensions can be compiled into a single di-mension, which might improve efficiency in somecases, and also sidestep the monostratal vs. multi-stratal distinction.The crucial ingredient of XDG that make rela-tional syntax-semantics processing possible are thedeclaratively specified principles.
So far, we haveonly given some examples for principle specifi-cations; while they could all be written as Hornclauses, we have not committed to any particularrepresentation formalism.
The development of sucha representation formalism will of course be ex-tremely important once we have experimented withmore powerful grammars and have a stable intuitionabout what principles are needed.At that point, it would also be highly interest-ing to define a (logic) formalism that generalisesboth XDG and dominance constraints, a fragment ofCLLS.
Such a formalism would make it possible totake over the interface presented here, but use dom-inance constraints directly on the semantics dimen-sions, rather than via the encoding into PA and SCdimensions.
The extraction process of Section 4.2could then be recast as a principle.AcknowledgementsWe thank Markus Egg for many fruitful discussionsabout this paper.ReferencesK.
Apt.
2003.
Principles of Constraint Programming.Cambridge University Press.N.
Chomsky.
1965.
Aspects of the Theory of Syntax.MIT Press, Cambridge, MA.A.
Copestake, D. Flickinger, C. Pollard, and I. Sag.2004.
Minimal recursion semantics.
an introduction.Journal of Language and Computation.
To appear.D.
Duchier and R. Debusmann.
2001.
Topological de-pendency trees: A constraint-based account of linearprecedence.
In ACL 2001, Toulouse.D.
Duchier and G.-J.
M. Kruijff.
2003.
Informationstructure in topological dependency grammar.
InEACL 2003.D.
Duchier.
2003.
Configuration of labeled trees un-der lexicalized constraints and principles.
Researchon Language and Computation, 1(3?4):307?336.M.
Egg, A. Koller, and J. Niehren.
2001.
The ConstraintLanguage for Lambda Structures.
Logic, Language,and Information, 10:457?485.V.
Gupta and J. Lamping.
1998.
Efficient linear logicmeaning assembly.
In COLING/ACL 1998.R.
M. Kaplan and J. T. Maxwell III.
1988.
An algorithmfor functional uncertainty.
In COLING 1988, pages297?302, Budapest/HUN.A.
Koller and J. Niehren.
2000.
On underspecifiedprocessing of dynamic semantics.
In Proceedings ofCOLING-2000, Saarbr?cken.A.
Koller and K. Striegnitz.
2002.
Generation as depen-dency parsing.
In ACL 2002, Philadelphia/USA.J.
T. Maxwell and R. M. Kaplan.
1993.
The interfacebetween phrasal and functional constraints.
Compu-tational Linguistics, 19(4):571?590.R.
Montague.
1974.
The proper treatment of quantifica-tion in ordinary english.
In Richard Thomason, editor,Formal Philosophy.
Selected Papers of Richard Mon-tague, pages 247?271.
Yale University Press, NewHaven and London.M.
Moortgat.
2002.
Categorial grammar and formal se-mantics.
In Encyclopedia of Cognitive Science.
Na-ture Publishing Group, MacMillan.
To appear.Mozart Consortium.
2004.
The Mozart-Oz website.http://www.mozart-oz.org/.G.
Smolka.
1995.
The Oz Programming Model.
InComputer Science Today, Lecture Notes in ComputerScience, vol.
1000, pages 324?343.
Springer-Verlag.M.
Steedman.
1999.
Alternating quantifier scope inCCG.
In Proc.
37th ACL, pages 301?308.XTAG Research Group.
2001.
A lexicalized tree adjoin-ing grammar for english.
Technical Report IRCS-01-03, IRCS, University of Pennsylvania.
