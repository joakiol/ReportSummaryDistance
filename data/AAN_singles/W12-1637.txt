Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 270?274,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsA Study in How NLU Performance Can Affect the Choice of DialogueSystem ArchitectureAnton Leuski and David DeVaultUSC Institute for Creative Technologies12015 Waterfront Drive, Playa Vista, CA 90094{leuski,devault}@ict.usc.eduAbstractThis paper presents an analysis of how thelevel of performance achievable by an NLUmodule can affect the optimal modular designof a dialogue system.
We present an evalua-tion that shows how NLU accuracy levels im-pact the overall performance of a system thatincludes an NLU module and a rule-based di-alogue policy.
We contrast these performancelevels with the performance of a direct classifi-cation design that omits a separate NLU mod-ule.
We conclude with a discussion of the po-tential for a hybrid architecture incorporatingthe strengths of both approaches.1 IntroductionRecently computer-driven conversational charactersor virtual humans have started finding real-life ap-plications ranging from education to health servicesand museums (Traum et al, 2005; Swartout et al,2006; Kenny et al, 2009; Jan et al, 2009; Swartoutet al, 2010).
As proliferation of these systems in-creases, there is a growing demand for the designand construction of virtual humans to be made moreefficient and accessible to people without extensivelinguistics and computer science backgrounds, suchas writers, designers, and educators.
We are specif-ically interested in making the language processingand dialogue management components in a virtualhuman easier for such potential authors to develop.Some system building steps that can be challengingfor such authors include annotating the meaning ofuser and system utterances in a semantic formalism,developing a formal representation of informationstate, and writing detailed rules that govern dialoguemanagement.We are generally interested in the extent to whichthese various authoring steps are necessary in orderto achieve specific levels of system performance.
Inthis paper, we present a case study analysis of theperformance of two alternative architectures for aspecific virtual human.
The two architectures, whichhave been developed and evaluated in prior work(DeVault et al, 2011b; DeVault et al, 2011a), differsubstantially in their semantic annotation and policyauthoring requirements.
We describe these architec-tures and our evaluation corpus in Section 2.
Wefocus our new analysis specifically on how the over-all performance of one of the architectures, whichuses a natural language understanding (NLU) mod-ule and hand-authored rules for the dialogue policy,depends on the performance of the NLU module.
InSection 3, we describe our finding that, dependingon the attainable level of NLU accuracy, this modu-lar approach may or may not perform better than asimpler direct classification design that omits a sep-arate NLU module and has a lower annotation andrule authoring burden.
In Section 4, we present aninitial exploration of whether a hybrid architecturemay be able to combine these approaches?
strengths.2 Summary of Data Set and Prior ResultsThis work is part of an ongoing research effortinto techniques for developing high quality dialoguepolicies using a relatively small number of sampledialogues and low annotation requirements (DeVaultet al, 2011b; DeVault et al, 2011a).
This sectionbriefly summarizes our prior work and data set.2702.1 Data SetFor our experiments we use the dataset describedin (DeVault et al, 2011b).
It contains 19 Wiz-ard of Oz dialogues with a virtual human calledAmani (Gandhe et al, 2009).
The user plays therole of an Army commander whose unit has been at-tacked by a sniper.
The user interviews Amani, whowas a witness to the incident and has some informa-tion about the sniper.
Amani is willing to tell theinterviewer what she knows, but she will only re-veal certain information in exchange for promises ofsafety, secrecy, and money (Artstein et al, 2009).Each dialogue turn in the data set includes a singleuser utterance followed by the response chosen by ahuman Amani role player.
There are a total of 296turns, for an average of 15.6 turns/dialogue.
Userutterances are modeled using 46 distinct speech act(SA) labels.
The dataset alo defines a different setof 96 unique SAs (responses) for Amani.
Six ex-ternal referees analyzed each user utterance and se-lected a single character response out of the 96 SAs.Thus the dataset defines a one-to-many mapping be-tween user utterances and alternative system SAs.2.2 Evaluation MetricWe evaluate the dialogue policies in our experi-ments through 19-fold cross-validation of our 19 di-alogues.
In each fold, we hold out one dialogue anduse the remaining 18 as training data.
To measurepolicy performance, we count an automatically pro-duced system SA as correct if that SA was chosen bythe original wizard or at least one external referee forthat dialogue turn.
We then count the proportion ofthe correct SAs among all the SAs produced acrossall 19 dialogues, and use this measure of weak accu-racy to score dialogue policies.We can use the weak accuracy of one referee,measured against all the others, to establish a per-formance ceiling for this metric.
This score is .79;see DeVault et al (2011b).2.3 Baseline SystemsWe consider two existing baseline systems in our ex-periments here.
The first system (Rules-NLU-SA)consists of a statistical NLU module that maps a userutterance to a single user SA label, and a rule-baseddialogue policy hand-crafted by one of the authors.The NLU uses a maximum-entropy model (Bergeret al, 1996) to classify utterances as one of the userSAs using shallow text features.
Training this modelrequires a corpus of user utterances that have beensemantically annotated with the appropriate SA.We developed our rule-based policy by manu-ally writing the simple rules needed to implementAmani?s dialogue policy.
Given a user SA labelAt for turn t, the rules for determining Amani?s re-sponse Rt take one of three forms:(a)ifAt = SAi thenRt = SAj(b)ifAt = SAi ?
?kAt?k = SAl thenRt = SAj(c)ifAt = SAi ?
?
?kAt?k = SAl thenRt = SAjThe first rule form specifies that a given user SAshould always lead to a given system response.
Thesecond and third rule forms enable the system?s re-sponse to depend on the user having previously per-formed (or not performed) a specific SA.
One thesystem developers, who is also a computational lin-guist, created the current set of 42 rules in about 2hours.
There are 30 rules of form (a), 6 rules of form(b), and 6 rules of form (c).The second baseline system (RM-Text) is a sta-tistical classifier that selects system SAs by analyz-ing shallow features of the user utterances and sys-tem responses.
We use the Relevance Model (RM)approach pioneered by Lavrenko et al (2002) forcross-lingual information retrieval and adapted toquestion-answering by Leuski et al (2006).
Thismethod does not require semantic annotation or ruleauthoring; instead, the necessary training data is de-fined by linking user utterances directly to the appro-priate system responses (Leuski and Traum, 2010).Table 1 summarizes the performance for the base-line systems (DeVault et al, 2011a).
The NLU mod-ule accuracy is approximately 53%, and the weakaccuracy of .58 for the corresponding system (Rules-NLU-SA) is relatively low when compared to theRM system at .71.
For comparison we provide athird data point: for Rules-G-SA, we assume thatour NLU is 100% accurate and always returns thecorrect (?gold?)
SA label.
We then run the rule-based dialogue policy on those labels.
The thirdcolumn (Rules-G-SA) shows the resulting weak ac-curacy value, .79, which is comparable to the weakaccuracy score achieved by the human referees (De-Vault et al, 2011b).271Rules-NLU-SA RM-Text Rules-G-SA.58 .71 .79Table 1: Weak accuracy results for baseline systems.Rules-GRM-TextRules-NLU-SA50 60 70 80 90 100556065707580Simulated NLU Accuracy H%LWeakAccuracyH%LFigure 1: Weak accuracy of the Rules system as a func-tion of simulated NLU accuracy.3 NLU Accuracy and System PerformanceWe conducted two experiments.
In the first, we stud-ied the effect of NLU accuracy on the performanceof the Rules-NLU-SA system.
One of our goals wasto find how accurate the NLU would have to be forthe Rules-NLU-SA system to outperform RM-Text.To investigate this, we simulated NLU perfor-mance at different accuracy levels by repeatedlysampling to create a mixture of the SAs from thetrained NLU classifier and from the correct (gold)set of SAs.
Specifically, we set a fixed value p rang-ing from 0 to 1 and then iterate over all dialogueturns in the held out dialogue, selecting the the cor-rect SA label with probability p or the trained NLUmodule?s output with probability 1 ?
p. Using thesampled set of SA labels, we compute the result-ing simulated NLU accuracy, run the Rules dialoguepolicy, and record the weak accuracy result.
We re-peat the process 25 times for each value of p. We letp range from 0 to 1 in increments of .05 to explore arange of simulated accuracy levels.Figure 1 shows simulated NLU accuracy and thecorresponding dialogue policy weak accuracy as apoint in two dimensions.
The points form a cloudwith a clear linear trend that starts at approximately53% NLU accuracy where it intersects with theRules-NLU-SA system performance and then goesup to the Rules-G performance at 100% NLU accu-racy.
The correlation is strong with R2 = 0.97.1The existence of a mostly linear relationship com-ports with the fact that most of the policy rules (30of 42), as described in Section 2.3, are of form (a).For such rules, each individual correct NLU speechact translates directly into a single correct systemresponse, with no dependence on the system hav-ing understood previous user utterances correctly.In contrast, selecting system responses that complywith rules in forms (b) and (c) generally requirescorrect understanding of multiple user utterances.Such rules create a nonlinear relationship betweenpolicy performance and NLU accuracy, but theserules are relatively few in number for Amani.The estimated linear trend line (in purple) crossesthe RM-Text system performance at approximately82% NLU accuracy.
This result suggests that ourNLU component would need to improve from itscurrent accuracy of 53% to approximately 82% ac-curacy for the Rules-NLU-SA system to outperformthe RM-Text classifier.
This represents a very sub-stantial increase in NLU accuracy that, in practice,could be expected to require a significant effort in-volving utterance data collection, semantic annota-tion, and optimization of machine learning for NLU.4 Hybrid SystemIn our second experiment we investigated the po-tential to integrate the Rules-NLU-SA and RM-Textsystems together for better performance.
Our ap-proach draws on a confidence score ?
from the NLUmaximum-entropy classifier; specifically, ?
is theprobability assigned to the most probable user SA.Figure 2 shows an analysis of NLU accuracy,Rules-NLU-SA, and RM-Text that is restricted tothose subsets of utterances for which NLU confi-dence ?
is greater than or equal to some threshold ?
.Two important aspects of this figure are (1) that rais-ing the minimum confidence threshold also raisesthe NLU accuracy on the selected subset of utter-ances; and (2) that there is a threshold NLU confi-dence level beyond which Rules-NLU-SA seems to1This type of analysis of dialogue system performance interms of internal component metrics is somewhat similar to theregression analysis in the PARADISE framework (Walker et al,2000).
However, here we are not concerned with user satis-faction, but are instead focused solely on the modular system?sability to reproduce a specific well-defined dialogue policy.2720.4 0.5 0.6 0.7 0.8 0.9 1.065707580859095NLU Confidence H?LAccuracyH%LRM-TextRules-NLU-SA H?
?
?LNLU Accuracy H?
?
?LFigure 2: Weak accuracy of Rules-NLU-SA and RM-Text on utterance subsets for which NLU confidence?
?
?
.
We also indicate the corresponding NLU accu-racy at each threshold.
In all cases a rolling average of 30data points is shown to more clearly indicate the trends.RM-TextRules-NLU-SA0.70 0.75 0.80 0.85 0.90 0.95 1.005560657075015304560NLU Confidence H?
LWeakAccuracyH%LProp.ofNLUSAsw???
?MixHybridFigure 3: Weak accuracy of the Hybrid system as a func-tion of the NLU confidence score.outperform RM-Text.
This confidence level is ap-proximately 0.95, and it identifies a subset of userutterances for which NLU accuracy is 83.3%.
Theseresults therefore suggest that NLU confidence canbe useful in identifying utterances for which NLUspeech acts are more likely to be accurate and Rules-NLU-SA is more likely to perform well.To explore this further, we implemented a hy-brid system that chooses between Rules-NLU-SA orRM-Text as follows.
If the confidence score is highenough (?
?
?
, for some fixed threshold ?
), the Hy-brid system uses the NLU output to run the Rulesdialogue policy to select the system SA; otherwise,it discards the NLU SA, and applies the RM classi-fier to select the system response directly.Figure 3 shows the plot of the Hybrid system per-formance as a function of the threshold value ?
.We see that with sufficiently high threshold value(?
?
0.95) the Hybrid system outperforms boththe Rules-NLU-SA and the RM-Text systems.
Thesecond line, labeled ?Mix?
and plotted against thesecondary (right) axis, shows the proportion of theNLU SAs with the confidence score that exceed thethreshold (?
?
?
).
It indicates how often the Hybridsystem prefers the Rules-NLU-SA output over theRM-Text system output.
We observe that approxi-mately 42 of the NLU outputs over all 296 dialogueturns (15%) have confidence values ?
?
0.95.
How-ever, for most of these dialogue turns the outputs forthe Rules-NLU-SA and RM-Text dialogue policiesare the same.
While we observe a small improve-ment in the Hybrid system weak accuracy valuesover the RM-Text system at thresholds of 0.95 andhigher, the difference is not statistically significant.Despite the lack of statistical significance in theinitial Hybrid results in this small data set, we inter-pret the complementary evidence from both experi-ments, which support the potential for Rules-NLU-SA to perform well when NLU accuracy is high, andthe potential for a hybrid system to identify a subsetof utterances that are likely to be understood accu-rately at run-time, as indicating that a hybrid designis a promising avenue for future work.5 Conclusions and Future WorkWe presented a case study analysis of how the levelof performance that is achievable in an NLU modulecan provide perspective on the design choices for amodular dialogue system.
We found that NLU accu-racy must be substantially higher than it currently isin order for the Rules-NLU-SA design, which car-ries a greater annotation and rule authoring burden,to deliver better performance than the simpler RM-Text design.
We also presented evidence that a hy-brid architecture could be a promising direction.AcknowledgmentsThe project or effort described here has been spon-sored by the U.S. Army Research, Development,and Engineering Command (RDECOM).
State-ments and opinions expressed do not necessarily re-flect the position or the policy of the United StatesGovernment, and no official endorsement should beinferred.273ReferencesRon Artstein, Sudeep Gandhe, Michael Rushforth, andDavid R. Traum.
2009.
Viability of a simple dialogueact scheme for a tactical questioning dialogue system.In DiaHolmia 2009: Proceedings of the 13th Work-shop on the Semantics and Pragmatics of Dialogue,page 43?50, Stockholm, Sweden, June.Adam L. Berger, Stephen D. Della Pietra, and VincentJ.
D. Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
ComputationalLinguistics, 22(1):39?71.David DeVault, Anton Leuski, and Kenji Sagae.
2011a.An evaluation of alternative strategies for implement-ing dialogue policies using statistical classification andrules.
In Proceedings of the 5th International JointConference on Natural Language Processing (IJC-NLP), pages 1341?1345, Nov.David DeVault, Anton Leuski, and Kenji Sagae.
2011b.Toward learning and evaluation of dialogue policieswith text examples.
In Proceedings of the 12th annualSIGdial Meeting on Discourse and Dialogue, pages39?48.Sudeep Gandhe, Nicolle Whitman, David R. Traum, andRon Artstein.
2009.
An integrated authoring tool fortactical questioning dialogue systems.
In 6th Work-shop on Knowledge and Reasoning in Practical Dia-logue Systems, Pasadena, California, July.Dusan Jan, Antonio Roque, Anton Leuski, Jackie Morie,and David R. Traum.
2009.
A virtual tour guide forvirtual worlds.
In Zso?fia Ruttkay, Michael Kipp, An-ton Nijholt, and Hannes Ho?gni Vilhja?lmsson, editors,IVA, volume 5773 of Lecture Notes in Computer Sci-ence, pages 372?378.
Springer.Patrick G. Kenny, Thomas D. Parsons, and Albert A.Rizzo.
2009.
Human computer interaction in virtualstandardized patient systems.
In Proceedings of the13th International Conference on Human-ComputerInteraction.
Part IV, pages 514?523, Berlin, Heidel-berg.
Springer-Verlag.Victor Lavrenko, Martin Choquette, and W. Bruce Croft.2002.
Cross-lingual relevance models.
In Proceed-ings of the 25th annual international ACM SIGIR con-ference on Research and development in informationretrieval, pages 175?182, Tampere, Finland.Anton Leuski and David Traum.
2010.
NPCEditor: Atool for building question-answering characters.
InProceedings of The Seventh International Conferenceon Language Resources and Evaluation (LREC).Anton Leuski, Ronakkumar Patel, David Traum, andBrandon Kennedy.
2006.
Building effective ques-tion answering characters.
In Proceedings of the 7thSIGdial Workshop on Discourse and Dialogue, Syd-ney, Australia, July.W.
Swartout, J. Gratch, R. W. Hill, E. Hovy, S. Marsella,J.
Rickel, and D. Traum.
2006.
Toward virtual hu-mans.
AI Mag., 27(2):96?108.William R. Swartout, David R. Traum, Ron Artstein,Dan Noren, Paul E. Debevec, Kerry Bronnenkant, JoshWilliams, Anton Leuski, Shrikanth Narayanan, andDiane Piepol.
2010.
Ada and grace: Toward realis-tic and engaging virtual museum guides.
In Jan M.Allbeck, Norman I. Badler, Timothy W. Bickmore,Catherine Pelachaud, and Alla Safonova, editors, IVA,volume 6356 of Lecture Notes in Computer Science,pages 286?300.
Springer.David Traum, William Swartout, Jonathan Gratch,Stacy Marsella, Patrick Kenney, Eduard Hovy, ShriNarayanan, Ed Fast, Bilyana Martinovski, Rahul Bha-gat, Susan Robinson, Andrew Marshall, Dagen Wang,Sudeep Gandhe, and Anton Leuski.
2005.
Dealingwith doctors: Virtual humans for non-team interac-tion training.
In Proceedings of ACL/ISCA 6th SIGdialWorkshop on Discourse and Dialogue, Lisbon, Portu-gal, September.Marilyn Walker, Candace Kamm, and Diane Litman.2000.
Towards developing general models of usabilitywith PARADISE.
Nat.
Lang.
Eng., 6(3-4):363?377.274
