The Problem of Computing the Most Probable Tree inData-Oriented Parsing and Stochastic Tree GrammarsRens BodInstitute for Logic, Language and ComputationDepartment ofComputational LinguisticsUniversity of AmsterdamSpuistraat 134, 1012 VB AmsterdamThe Netherlandsrens@mars.let.uva.nlAbstractWe deal with the question as to whether thereexists a polynomial time algorithm for computingthe most probable parse tree of a sentence generatedby a data-oriented parsing (DOP) model.
(Scha,1990; Bod, 1992, 1993a).
Therefore we describeDOP as a stochastic tree-substitution grammar(STSG).
In STSG, a tree can be generated byexponentially many derivations involving differentelementary trees.
The probability of a tree is equalto the sum of the probabilities of all itsderivations.We show that in STSG, in contrast withstochastic context-free grammar, the Viterbialgorithm cannot be used for computing a mostprobable tree of a string.
We propose a simplemodification of Viterbi which allows by means ofa "se lect - random" search to est imate the mostprobable tree of a string in polynomial time.Experiments with DOP on ATIS show thatonly in 68% of the cases, the most probablederivation of a string generates the most probabletree of that string.
Therefore, the parse accuracyobtained by the most probable trees (96%) isdramatically higher than the parse accuracy obtainedby the most probable derivations (65%).It is still an open question whether themost probable tree of a string can bedeterministically computed in polynomial time.1 Data-Oriented ParsingA Data-Oriented Parsing model (Scha, 1990; Bod,1992, 1993a) is characterized by a corpus of analyzedlanguage utterances, together with a set of operationsthat combine sub-analyses from the corpus into newanalyses.
We will limit ourselves in this paper tocorpora with purely syntactic annotations.
For thesemantic dimension of DOP, the reader is referred to(van den Berg et al, 1994).
Consider the imaginaryexample corpus consisting of only two trees in figure1.
We will assume one operation for combiningsubtrees.
This operation is called "composition", andis indicated by the infix operator o.
The compositionof t and u, tou, yields a copy of t in which itsleftmost nonterminal leaf node has been identifiedwith the roof node of u (i.e., u is substituted on theleftmost nonterminal leaf node of t).
For reasons ofsimplicity we will write in the following (tou)ov as:louov.S SANP VPI A  uP,hov II ,.o ww=.
A v/ : , ;  A A A  i. ,Loo.oc o.oo o.othe dress P NPI /N  on the rackFigure 1.
Example corpus of two trees.Now the (ambiguous) sentence "She displayed thedress on the table" can be parsed by combiningsubtrees from the corpus.
For instance:S o NP  oA /<, "Aip  V ~  the dress on I the tableshe VP PPA V NPIdisplayedNP VPshe Via pp/displa~ the dress on the tableFigure 2.
Derivation and parse tree for "She displayed thedress on the table"104As the reader may easily ascertain, a differentderivation may yield a different parse tree.
However, adifferent derivation may also very well yield the sameparse tree; for instance:S o VP o NPNP VP the dressdisplayedshe VP PPp NPon the tableFigure 3.
Different derivation generating the same parsetree for "She displayed the dress on the table"OrS o VP o NPNP VP VP pp the dressI / x  /x ,  she V NP p I~Pdispl!yed !n the ab le /NFigure 4.
Another derivation generating the same parsetree for "She displayed the dress on the table"Thus, a parse tree can have several derivationsinvolving different subtrees.
Using the corpus for ourstochastic estimations, we estimate the probability ofsubstituting a certain subtree on a specific node as theprobab i l i ty  of selecting this subtree among allsubtrees in the corpus that could be substituted onthat node.
1 The probability of a derivation can becomputed as the product of the probabilities of thesubstitutions that it involves.
The probability of aparse tree is equal to the probability that any of itsderivations occurs, which is the sum of theprobabilities of all derivations of that parse tree.Finally, the probability of a word string is equal tothe sum of the probabilities of all its parse trees.2 DOP as a Stochastic Tree-Subst i tut ion GrammarIn order to deal with the problem of computing themost probable parse tree of a string, it is convenientto describe DOP as a "Stochastic Tree-SubstitutionGrammar" (STSG).
STSG can be seen as ageneralization over DOP, where the elementary tree Sof STSG are the subtrees of DOP, and theprobabilities of the elementary trees are the1Very small frequencies are smoothed by Good-Turing.substitution-probabilities of the correspondingsubtrees ofDOP (Bod, 1993c).A Stochastic Tree-Substitution Grammar G is a five-tuple < VN, VT-, S, R, P> whereVu is a finite set of nonterminal symbols.Vr is a finite set of terminal symbols.S ~ VN is the distinguished symbol.R is a finite set of elementary trees whose top nodesand interior nodes are labeled by nonterminal symbolsand whose yield nodes are labeled by terminal ornonterminal symbols.P is a function which assigns to every elementary treet ~ R a probability p(t) .
For a tree t with a root a,p(t) is interpreted as the probability of substituting ton a.
We require, therefore, that 0 < p(t)  <- 1 and~-~t:root(t)=Ct p(t) = 1.If t l  and t2 are trees such that the l e f tmostnonterminal yield node of t l  is equal to the root of t2,then tlot 2 is the tree that results from substituting t 2for this leftmost nonterminal yield node in t l .
Thepartial function o is called leftmost substitution.
Forreasons of conciseness we will use the termsubstitution for leftmost substitution.A leftmost derivation generated by an STSG Gis a tuple of trees <t 1 ..... tn> such that t I ..... t n areelements of R, the root of t I is labeled by S and theyield of t l  .... otn is labeled by terminal symbols.
Theset of leftmost derivations generated by G is thusgiven by Derivations(G) = { <t I ..... tn> I tl ..... tn ~ R^ root ( t1 )  = S A y ie ld ( t lo .
.
.o tn )  ~ Vr +}.
Forconvenience we will use the term derivation forleftmost derivation.
A derivation <tl ..... tn> is calleda derivation of tree T, iff tlo...ot n = T. A derivation<t l  ..... tn> is called a derivation of string s, iffyield(t1 .... ?tn) = s. The probability of a derivation<t I ..... in> is defined as p(t l )  ?
... ?
p(tn).A parse tree generated by an STSG G is a treeT such that there is a derivation <t l  ..... tn>Der ivat ions (G)  for which t l  .
.
.
.
.
tn = T. The set ofparse trees, or tree language, generated by G is givenbyParses(G) = {T I3  <t I ..... tn> ~ Der ivat ions(G):t l  .
.
.
.
.
tn = T}.
For reasons of conciseness we willoften use the terms parse or tree for a parse tree.
Aparse whose yield is equal to string s, is called a parseof s. The probability of a parse is defined as the sumof the probabilities of all its derivations.A s t r ing  generated by an STSG G is anelement of Vr + such that there is a parse generated byG whose yield is equal to the string.
The set ofstrings, or string language, generated by G is givenby Str ings(G) = {sl 3 T:  T~ Parses(G)  ^  s =yield(T)}.
The probability of a string is defined as thesum of the probabilities of all its parses.
This meansthat the probability of a string is also equal to thesum of the probabilities of all its derivations.1053 For the input string abcd, the following derivationforest is then obtained:Comput ing a most probableparse tree in STSGIn order to deal with the problem of computing themost probable parse tree of a sentence, we willdistinguish between parsing and disambiguation.
Byparsing we mean the creation of a parse forest for aninput sentence.
By disambiguation we mean theselection of the most probable parse 2from the forest.The creation of a parse forest is an intermediate stepfor computing the most probable parse.3.1 ParsingFrom the way STSG combines elementary trees bymeans of substitution, it follows that an inputsentence can be parsed by the same algorithms as(S)CFGs.
Every elementary tree t is used as acontext-free rewrite rule root(t) --~ yield(t).
Given achart parsing algorithm, an input sentence of length ncan be parsed in n 3 time.In order to obtain a chart-like forest for asentence parsed in STSG, we need to label the well-formed substrings in the chart not only with thesyntactic ategories of that substring but with the fullelementary trees t that correspond to the use of thederived rules root(t) ---~yield(t).
Note that in a chart-like forest generated by an STSG, different derivationsthat generate a same tree do not collapse.
We willtherefore talk about a derivation forest generated by anSTSG (cf.
Sima'an et al, 1994).The following formal example illustrateswhat a derivation forest of a string may look like.
Inthe example, we leave out the probabilities, which areneeded only in the disambiguation process.
The visualrepresentation comes from (Kay, 1980): every entry(i,j) in the chart is indicated by an edge and spans thewords between the i-th and the j-th position of asentence.
Every edge is labeled with the elementarytrees that denote the underlying phrase.
The example-STSG consists of the following elementary trees:S /XcAB/XcS S A AA d c a bB B C AAIa b dFigure 5.
Elementary trees of an example-STSG2 Although theoretically there can be more than onemost probable parse for a sentence, in practice a systemthat employs a non-trivial treebank tends to generateexactly one most probable parse for a given inputsentence.s /kAc.A A/k ?
~ X.c C./~  .
:  ?
I?
~ " .tt a 1 b 2 ?
3 4 4Figure 6.
Derivation forest for abedNote that different derivations in the forest generatethe same tree.
By exhaustively unpacking the forest,four different derivations generating two different reesare obtained.
We may ask whether we can pack theforest by col lapsing spurious derivat ions.Unfortunately, no efficient procedure is known thataccomplishes this (remember that there can beexponentially many derivations for one tree).3.2 DisambiguationCubic time parsing does not guarantee cubic timedisambiguation, asa sentence may have exponentiallymany parses and any such parse may haveexponentially many derivations.
Therefore, in order tofind the most probable parse of a sentence, it is notefficient o compare the probabilities of the parses byexhaustively unpacking the chart.
Even fordetermining the probability of one parse, it is notefficient o add the probabilities of all derivations ofthat parse.3.2.1 Viterbi optimization is not feasiblefor finding the most probable parseThere exists a heuristic optimization algorithm,known as Viterbi optimization, which selects on thebasis of an SCFG the most probable derivation of asentence in cubic time (Viterbi, 1967; Fujisaki et al,1989; Jelinek et al, 1990).
In STSG, however, themost probable derivation does not necessarily generatethe most probable parse, as the probability of a parseis defined as the sum of the probabilities of all itsderivations.
Thus, there is an important question as towhether we can adapt he Viterbi algorithm for findingthe most probable parse.To understand the difficulty of the problem,we look in more detail at the Viterbi algorithm.
Thebasic idea of the Viterbi algorithm is the earlypruning of low probability subderivations in abottom-up fashion.
Two different subderivations ofthe same part of the sentence and whose resulting106subparses have the same root can both be developed(if at all) to derivations of the whole sentence in thesame ways.
Therefore, if one of these twosubderivations has a lower probability, then it can beeliminated.
This is illustrated by a formal example infigure 7.
Suppose that during bottom-up arsing ofthe string abcd the following two subderivations dland d2 have been generated for the substring abc.
(Actually represented are their resulting subparses.
)A AA \ ,  abeFigure 7.
Two subparses for the string abcdIf the probability of dl is higher than the probabilityof d2, we can eliminate d2 if we are only interested infinding the most probable derivation of abcd.
But ifwe are interested in finding the most probable parse ofabcd (generated by STSG), we are not allowed toeliminate d2.
This can be seen by the following.Suppose that we have the additional elementary treegiven in figure 8.SaFigure 8.
Elementary tree.This elementary tree may be developed to the sametree that can be developed by d2, but not to the treethat can be developed by dl.
And since the probabilityof a parse tree is equal to the sum of the probabilitiesof all its derivations, it is still possible that d 2contributes to the generation of the most probableparse.
Therefore we are not allowed to eliminate d2.This counter-example does not prove thatthere is no heuristic optimization that allowspolynomial time selection of the most probable parse.But it makes clear that a "select-best" search, asaccomplished by Viterbi, is not adequate for findingthe most probable parse in STSG.
So far, it isunknown whether the problem of finding the mostprobable parse in a deterministic way is inherentlyexponential or not (cf.
Sima'an et al, 1994).
Oneshould of course ask how often in practice the mostprobable derivation produces the most probable parse,but this can only be answered by means ofexperiments on real life corpora.
Experiments on theATIS corpus (see session 4) show that only in 68%of the cases the most probable derivation of a sentencegenerates the most probable parse of that sentence.Moreover, the parse accuracy obtained by the mostprobable parse is dramatically higher than the parseaccuracy obtained by the parse generated by the mostprobable derivation.3.2.2 Estimating the most probable parseby Monte Carlo searchWe will leave it as an open question whether the mostprobable parse can be deterministically derived inpolynomial time.
Here we will ask whether thereexists a polynomial time approximation procedurethat estimates the most probable parse with anestimation error that can be made arbitrarily small.We have seen that a "select-best" search, asaccomplished by Viterbi, can be used for finding themost probable derivation but not for finding the mostprobable parse.
If we apply instead of a select-bestsearch, a "select-random" search, we can generate arandom derivation.
By iteratively generating a largenumber of random derivations we can estimate themost probable parse as the parse which results mostoften from these random derivations (since theprobability of a parse is the probability that any of itsderivations occurs).
The most probable parse can beestimated as accurately as desired by making thenumber of random samples as large as desired.According to the Law of Large Numbers, the mostoften generated parse converges to the most probableparse.
Methods that estimate the probability of anevent by taking random samples are known as MonteCarlo methods (Hammersley & Handscomb, 1964).
3The selection of a random derivation isaccomplished in a bottom-up fashion analogous toViterbi.
Instead of selecting the most probablesubderivation at each node-sharing in the chart, arandom subderivation is selected (i.e.
sampled) at eachnode-sharing (that is, a subderivation that has n timesas large a probability as another subderivation shouldalso have n times as large a chance to be chosen asthis other subderivation).
Once sampled at the S-node,the random derivation of the whole sentence can beretrieved by tracing back the choices made at eachnode-sharing.
Of course, we may postpone samplinguntil the S-node, such that we sample directly fromthe distribution of all S-derivations.
But this wouldtake exponential time, since there may beexponentially many derivations for the wholesentence.
By sampling bottom-up at every node whereambiguity appears, the maximum number of differentsubderivations ateach node-sharing is bounded to aconstant (the total number of rules of that node), andtherefore the time complexity of generating a randomderivation of an input sentence is equal to the timecomplexity of finding the most probable derivation,O(n3).
This is exemplif ied by the followingalgorithm.3 Note that Monte Carlo estimation of the most probableparse is more reliable than the estimation of the mostprobable parse by generating the n most probablederivations by Viterbi, since it might be that the mostprobable parse is exclusively generated by many lowprobable derivations.
The Monte Carlo method isguaranteed to converge to the most probable parse.107Sampling a random 0?riva~ion from a derivation forestGiven a derivation forest, of a sentence of n words,consisting of labeled entries (i,j) that span the wordsbetween the i-th and the j-th position of the sentence.Every entry is labeled with linked elementary trees,together with their probabilities, that constitutesubderivations of the underlying subsentence.Sampling a derivation from the chart consists ofchoosing at every labeled entry (bottom-up, breadth-fu'st) a random subderivation of each root-node:fork := 1 tondofor i  := 0 to n-k dofor chart-entry (i,i+k) dofor each root-node X doselect 4a random subderivation of root Xeliminate the other subderivationsWe now have an algorithm that selects a randomderivation from a derivation forest.
Converting thisderivation into a parse tree gives a first estimation forthe most probable parse.
Since one random sample isnot a reliable estimate, we sample a large number ofrandom derivations and see which parse is generatedmost frequently.
This is exemplified by the followingalgorithm.
(Note that we might also estimate themost probable derivation by random sampling,namely by counting which derivation is sampled mostoften; however, the most probable derivation can bemore effectively generated by Viterbi.
)Eslimating the most probable parse (MPP)Given a derivation forest for an input sentence:repeat until the MPP convergessample a random derivation from the foreststore the parse generated by the random derivationMPP := the most frequently occurring parseThere is an important question as to how long theconvergence of the most probable parse may take.
Isthere a tractable upper bound on the number ofderivations that have to be sampled from the forestbefore stability in the top of the parse distributionoccurs?
The answer is yes: the worst case timecomplexity of achieving a maximum estimation errore by means of random sampling is O(e-2),independently of the probability distribution.
This is aclassical result from sampling theory (cf.
Hammersleyand Handscomb, 1964), and follows directly fromChebyshev's inequality.
In practice, it means that the4 Let { (e 1, Pl),  (e2, P2) ... .
.
(en, Pn) } be a probabilitydistribution of events el,  e2, ..., en; an event e i is said tobe randomly selected iff its probability of being selectedis equal to Pi.
In order to allow for "direct sampling", onemust convert the probability distribution into acorresponding sample space for which holds that thefrequency of occurrence 3\] of each event e i is a positiveinteger equal to Npi, where N is the size of the samplespace.error e is inversely proportional to the square-root ofthe number of random samples N and therefore, toreduce e by a factor of k, the number of samples Nneeds to be increased k2-fold.
In practical experiments(see ?4), we will limit the number of samples to apre-determined, sufficiently large bound N.What is the theoretical worst case timecomplexity of parsing and disambiguation together?That is, given an STSG and an input sentence, whatis the maximal time cost of finding the most probableparse of a sentence?
If  we use a CKY-parser, thecreation of a derivation forest for a sentence of nwords takes O(n 3) time.
Taking also into account hesize G of an STSG (defined as the sum of the lengthsof the yields of all its elementary trees), the timecomplexity of creating a derivation forest isproportional to Gn 3.
The time complexity ofdisambiguation is both proportional to the cost ofsampling a derivation, i.e.
Gn 3, and to the cost of theconvergence by means of iteration, which is e -2.
Tiffsmeans that the time complexity of disambiguation isgiven by O(Gn3e-2).
The total time complexity ofparsing and disambiguation is equal to O(Gn 3) +O(Gn3e -2) = O(Gn3e'2).
Thus, there exists a tractableprocedure that estimates the most probable parse of aninput sentence.Notice that although the Monte Carlodisambiguation algorithm estimates the mostprobable parse of a sentence in polynomial time, it isnot in the class of polynomial time decidablealgorithms.
The Monte Carlo algorithm cannot decidein polynomial time what is the most probable parse;it can only make the error-probability of the estimatedmost probable parse arbitrarily small.
As such, theMonte Carlo algorithm is a probabilistic algorithmbelonging to the class of Bounded error ProbabilisticPolynomial time (BPP) algorithms.We hypothesize that Monte Carlodisambiguation is also relevant for other stochasticgrammars.
It turns out that all stochastic extensionsof CFGs that are stochastically richer than SCFGneed exponential time algorithms for finding a mostprobable parse tree (cf.
Briscoe & Carroll, 1992;Black et al, 1993; Magerman & Weir, 1992; Schabes& Waters, 1993).
To our knowledge, it has neverbeen studied whether there exist BPP-algorithms forthese models.
Alhough it is beyond the scope of ourresearch, we conjecture that there exists a MonteCarlo disambiguation algorithm for at least StochasticTree-Adjoining Grammar (Schabes, 1992).3.2.3 Psychological relevance of MonteCarlo d isambiguat ionAs has been noted, an important difference betweenthe Viterbi algorithm and the Monte Carlo algorithmis, that with the latter we never have 100%confidence.
In our opinion, this should not be seen asa disadvantage.
In fact, absolute confidence about themost probable parse does not have any significance,as the probability assigned to a p~se is already anestimation of its actual probability.
One may ask asto whether Monte Carlo is appropriate for modeling108human sentence perception.
The following lists someproperties of Monte Carlo disambiguation that maybe of psychological interest:1.
As mentioned above, Monte Carlo never provides100% confidence about the best analysis.
Thiscorresponds to the psychological observation thatpeople never have absolute confidence about theirinterpretation f an ambiguous sentence.2.
Although conceptually Monte Carlo uses the totalspace of possible analyses, it tends to sample only themost likely ones.
Very unlikely analyses may only besampled after considerable time, but it is notguaranteed that all analyses are found in finite time.This matches with experiments on human sentenceperception where very implausible analyses are onlyperceived with great difficulty and after considerabletime.3.
Monte Carlo does not necessarily give the sameresults for different sequences of samples, especially ifdifferent analyses in the top of the distribution arealmost equally likely.
In the case there is more thanone most probable analysis, Monte Carlo does notconverge to one analysis but keeps alternating,however large the number of samples is made.
Inexperiments with human sentence perception, it hasoften been shown that different analyses can beperceived for one sentence.
And in case these analysesare equally plausible, people perceive so-calledfluctuation effects.
This fluctuation phenomenon isalso well-known in the perception of ambiguousvisual patterns.4.
Monte Carlo can be made parallel in a verystraightforward way: N samples can be computed byN processing units, where equal outputs arereinforced.
The more processing units are employed,the better the estimation.
However, since the numberof processing units is finite, there is never absoluteconfidence.
This has some similarity with the ParallelDistributed Processing paradigm for haman (language)processing (Rumelhart & McClelland, 1986).4 ExperimentsIn this section, we report on experiments with animplementation f DOP that parses and disambiguatespart-of-speech strings.
In (Bod, 1995) it is shown howDOP is extended to parse word strings that possiblycontain unknown words.4.1 The test environmentFor our experiments, we used a manually correctedversion of the Air Travel Information System (ATIS)spoken language corpus (Hemphill et al, 1990)annotated in the Pennsylvania Treebank (Marcus etal., 1993).
We employed the "blind testing" method,dividing the corpus into a 90% training set and a 10%test set by randomly selecting sentences.
The 675trees from the training set were converted into theirsubtrees together with their relative frequencies,yielding roughly 4"105 different subtrees.
The 75part-of-speech sequences from the test set served asinput strings that were parsed and disambiguated usingthe subtrees from the training set.
As motivated in(Bed, 1993b), we use the notion of parse accuracy asour accuracy metric, defined as the percentage of thetest strings for which the most probable parse isidentical to the parse in the test set.4.2 Accuracy as a function of subtree-depthIt is one of the most essential features of DOP, thatarbitrarily large subtrees are taken into considerationto estimate the probability of a parse.
In order to testthe usefulness of this feature, we performed ifferentexperiments constraining the depth of the subtrees.The following table shows the results of sevenexperiments for different maximum depths of thetraining set subtrees.
The accuracy refers to the parseaccuracy at 400 randomly sampled parses, and isrounded off to the nearest integer.
The CPU timerefers to the average CPU time per string employedby a Spark II.depth ofsubtrees1_<2_<3<4<5<6unboundedparseaccuracy52 %87 %92 %93 %93 %95 %96 %CPU time(hours).04 h.21 h.72 h1.6 h1.9 h2.2 h3.5 hTable 1.
Parse results on the ATIS corpusThe table shows a dramatic increase in parse accuracywhen enlarging the maximum depth of the subtreesfrom 1 to 2.
(Remember that for depth one, DOP isequivalent to a stochastic context-free grammar.)
Theaccuracy keeps increasing, at a slower rate, when thedepth is enlarged further.
The highest accuracy isobtained by using all subtrees from the training set:72 out of the 75 sentences from the test set are parsedcorrectly.
Thus, the accuracy increases if largersubtrees are used, though the CPU time increasesconsiderably as well.4.3 Does the most probable derivationgenerate the most probable parse?Another important feature of DOP is that theprobability of a resulting parse tree is computed as thesum of the probabilities of all its derivations.Although the most probable parse of a sentence is notnecessarily generated by the most probable derivationof that sentence, there is a question as to how oftenthese two coincide.
In order to study this, we alsocalculated the derivation accuracy, defined as thepercentage of the test strings for which the parsegenerated by the most probable derviation is identicalto the parse in the test set.
The following table showsthe derivation accuracy against he parse accuracy forthe 75 test set strings from the ATIS corpus, usingdifferent maximum depths for the corpus subtrees.109depth  ofsubt rees1-<2-<3-<4-<5-<6unboundedder ivat ionaccuracy52%47%49%57%60%65%65%parseaccuracy52%87%92%93%93%95%96%Table 2.
Derivation accuracy vs. parse accuracyThe table shows that the derivation accuracy is equalto the parse accuracy if the depth of the subtrees isconstrained to 1.
This is not surprising, as for depth1, DOP is equivalent with SCFG where every parse isgenerated by exactly one derivation.
What isremarkable, is, that the derivation accuracy decreases ifthe depth of the subtrees i  enlarged to 2.
If the depthis enlarged further, the derivation accuracy increasesagain.
The highest derivation accuracy is obtained byusing all subtrees from the corpus (65%), but remainsfar behind the highest parse accuracy (96%).
Fromthis table we conclude that if we.are interested in themost probable analysis of a string we must not lookat the probability of the process of achieving thatanalysis but at the probability of the result of thatprocess.4.4 The significance of once-occurringsubtreesThere is an important question as to whether we canreduce the "grammar constant" of DOP by eliminatingvery infrequent subtrees, without affecting the parseaccuracy.
In order to study this question, we start witha test result.
Consider the test set sentence "Arrangethe fl ight code of the flight from Denverto Dallas Worth in descending order", whichhas the following parse in the test set:(s (NP *)(VP VB/Arrange(NP (NP DT/the NN/flight NN/code)(PP IN/of(NP (NP DT/the NN/flight)(PP (PP IN/from(NP NP/Denver))(PP TO/to(NP NP/DallasNP/Worth))))))(PP IN/in(NP (VP VBG/descending)NN/order))).
))The corresponding p-o-s sequence of this sentence isthe test set string "vB DT NN NN IN DT NN IN NPTO NP NP IN VBG NN".
At subtree-depth < 2, thefollowing most probable parse was estimated for thisstring (where for reasons of readability the words areadded to the p-o-s tags):(s (NP *)(VP VB/Arrange(NP (Np DT/the NN/flight NN/code)(PP IN/of(NP (NP DT/the NN/flight)(PP (PP IN/from(NP NP/Denver))(PP TO/to(NP NP/DallasNP/Worth)))(PP IN/in(NP (VP VBG/descending)NN/order)))))).
))In this parse, we see that the prepositional phrase "indescending order" is incorrectly attached to the NP"the f l ight"  instead of to the verb "arrange".
Thiswrong attachment may be explained by the highrelative frequencies of the following subtrees of depth2 (that appear in structures of sentences like "Show methe transportation from SFO to downtown SanFrancisco in August", where the PP  "in August"is attached to the NP  "the transportat ion",  andnot to the verb "show"):NP NP NP NPPP PP PPPP IN PPNP PP INNPOnly if the maximum depth was enlarged to 4,subtrees like the following were available, which ledto the estimation of the correct ree.VP VBNP NPPPPP INNP VP VBGNNIt is interesting to note that this subtree occurs onlyonce in the training set.
Nevertheless, it induces thecorrect parsing of the test string.
This seems tocontradict the fact that probabilities based on sparsedata are not reliable.
Since many large subtrees areonce-occumng events (hapaxes), there seems to be apreference in DOP for an occurrence-based approach ifenough context is provided: large subtrees, even ifthey occur once, tend to contribute to the generationof the correct parse, since they provide muchcontextual information.
Although these subtrees havelow probabilities, they tend to induce the correct parsebecause fewer subtrees are needed to construct a parse.Additional experiments seemed to confirmthis hypothesis.
Throwing away all hapaxes yieldedan accuracy of 92%, which is a decrease of 4%.Distinguishing between small and large hapaxes,showed that the accuracy was not affected byeliminating the hapaxes of depth 1 (however, as anadvantage, the convergence seemed to get slightlyfaster).
Eliminating hapaxes larger than depth 1,decreased the accuracy.
The following table shows theparse accuracy after eliminating once-occurringsubtrees of different maximum depths.110depth ofhapaxes1<2_<3_<4<_5<_6unboundedparseaccuracy96%95%95%93%92%92%92%Table 3.
Parse accuracy after eliminating once-occurringsubtreesConclusionsWe have shown that in DOP and STSG the Viterbialgorithm cannot be used for computing a mostprobable tree of a string.
We developed a modificationof Viterbi which allows by means of an iterativeMonte Carlo search to estimate the most probable treeof a string in polynomial time.
Experiments on ATISshowed that only in 68% of the cases, the mostprobable derivation of a string generates the mostprobable tree of that string, and that he parse accuracyis dramatically higher than the derivation accuracy.We conjectured that the Monte Carlo algorithm canalso be applied to other stochastic grammars forcomputing the most probable tree of a string.
Thequestion as to whether the most probable tree of astring can also be deterministically derived inpolynomial time is still unsolved.AcknowledgmentsThe author is indebted to Remko Scha for valuablecomments on an earlier version of this paper, and toKhalil Sima'an for useful discussions.ReferencesM.
van den Berg, R. Bod & R. Scha, 1994.
"ACorpus-Based Approach to Semantic Interpretation",Proceedings Ninth Amsterdam Colloquium,Amsterdam.E.
Black, R. Garside and G. Leech, 1993.Statistically-Driven Computer Grammars o/English:The IBM/Lancaster Approach, Rodopi: Amsterdam-Atlanta.R.
Bod, 1992.
"A Computational Model ofLanguage Performance: Data Oriented Parsing",Proceedings COLING'92, Nantes.R.
Bod, 1993a.
"Using an Annotated Corpus as aStochastic Grammar", Proceedings European Chapterfo the ACL'93, Utrecht.R.
Bod, 1993b.
"Monte Carlo Parsing",Proceedings Third International Workshop on ParsingTechnologies, Tilburg/Durbuy.R.
Bod, 1993c.
"Data Oriented Parsing as a GeneralFramework for Stochastic Language Processing", in:K.Sikkel & A. Nijholt (eds.
), Parsing NaturalLanguage, TWLT6, Twente University.R.
Bod, 1995.
Enriching Linguistics withStatistics: Performance Models of Natural Language.PhD-thesis, University of Amsterdam (forthcoming).T.
Briscoe and J. Carroll, 1993.
"GeneralizedProbabilistic LR Parsing of Natural Language(Corpora) with Unification-Based Grammars",Computational Linguistics 19(1), 25-59.T.
Fujisaki, F. Jelinek, J. Cocke, E. Black and T.Nishino, 1989.
"A Probabilistic Method for SentenceDisambiguation", Proceedings 1st Int.
Workshop onParsing Technologies, Pittsburgh.J.M.
Hammersley and D.C. Handscomb, 1964.Monte Carlo Methods, Chapman and Hall, London.C.T.
Hemphill, J.J. Godfrey and G.R.
Doddington,1990.
"The ATIS spoken language systems pilotcorpus".
Proceedings DARPA Speech and NaturalLanguage Workshop, Hidden Valley, MorganKaufmann.F.
Jelinek, J.D.
Lafferty and R.L.
Mercer, 1990.Basic Methods of Probabilistic Context FreeGrammars, Technical Report IBM RC 16374(#72684), Yorktown Heights.M.
Kay, 1980.
Algorithmic Schemata nd DataStructures in Syntactic Processing.
Report CSL-80-12, Xerox PARC, Palo Alto, Ca.D.
Magerman and C. Weir, 1992.
"Efficiency,Robustness and Accuracy in Picky Chart Parsing",Proceedings A CL'92, Newark, Delaware.M.
Marcus, B. Santorini and M. Marcinkiewicz,1993.
"Building a Large Annotated Corpus ofEnglish: the Penn Treebank", ComputationalLinguistics 19(2).D.
Rumelhart and J. McClelland, 1986.
ParallelDistributed Processing, The MIT Press, Cambridge,Mass.R.
Scha, 1990.
"Language Theory and LanguageTechnology; Competence and Performance" (inDutch), in Q.A.M.
de Kort & G.L.J.
Leerdam (eds.
),Computertoepassingen in de Neerlandistiek, Almere:Landelijke Vereniging van Neerlandici (LVVN-jaarboek).Y.
Schabes, 1992.
"Stochastic Lexicalized Tree-Adjoining Grammars", Proceedings COLING'92,Nantes.Y.
Schabes and R, Waters, 1993.
"StochasticLexicalized Context Free Grammars", ProceedingsThird International Workshop on ParsingTechnologies, Tilburg/Durbuy.K.
Sima'an, R. Bod, S. Krauwer and R. Scha,1994.
"Efficient Disambiguation by means ofStochastic Tree Substitution Grammars", ProceedingsInternational Conference on New Methods inLanguage Processing, UMIST, Manchester.A.
Viterbi, 1967.
"Error bounds for convolutionalcodes and an asymptotically optimum decodingalgorithm", IEEE Trans.
Information Theory, IT-13,260-269.111
