Squibs and DiscussionsNonminimal Derivations in Unification-basedParsingNoriko Tomuro*DePaul UniversitySteven L. Lytinen tDePaul UniversityShieber's abstract parsing algorithm (Shieber 1992)for unification grammars is an extension ofEarley's algorithm (Earley 1970)for context-free grammars to feature structures.
In this paper,we show that, under certain conditions, Shieber ' salgorithm produces what we call a nonminimalderivation: aparse tree which contains additional features that are not in the licensing productions.While Shieber's definition of parse tree allows for such nonminimal derivations, we claim that theyshould be viewed as invalid.
We describe the sources of the nonminimal derivation problem, andpropose aprecise definition of minimal parse tree, as well as a modification to Shieber's algorithmwhich ensures minimality, although at some computational cost.1.
IntroductionUnification grammar is a term often used to describe a family of feature-based gram-mar formalisms, including GPSG (Gazdar et al 1985), PATR-II (Shieber 1986), DCG(Pereira and Warren 1980), and HPSG (Pollard and Sag 1994).
In an effort to formalizethe common elements of unification-style grammars, Shieber (1992) developed a logicfor describing them, and used this logic to define an abstract parsing algorithm.
Thealgorithm uses the same set of operations as Earley's (1970) algorithm for context-freegrammars, but modified for unification grammars.In this paper, we show that, under certain conditions, Shieber's algorithm producesunintended, spurious parses in addition to the intended ones.
We call these spuriousparses nonminimal derivations (or nonminimal parse trees), because they containextra features which are not in the productions that license the parse, aWe claim thatsuch nonminimal derivations are invalid.
The basis of our claim is that the unifica-tion operation as set union preserves minimality; thus any correct unification-basedparsing algorithm should produce parses that contain all and only features from thelicensing productions (i.e., minimal derivations or minimal parse trees).
Nonminimalderivations are also undesirable in practice because, given a parse tree, we cannot ellwhether a particular feature should be in the model or not unless we reconstruct thewhole tree.Despite the nonminimal derivations, Shieber (1992) proved the correctness of hisalgorithm.
As it turned out, his definition of parse tree, which his proof relied on, was* School of Computer  Science, Telecommunications and Information Systems, Chicago, IL 60604.
E-mail:tomuro@cs.depaul.edut School of Computer  Science, Telecommunications and Information Systems, Chicago, IL 60604.
E-maihlytinen@cs.depaul.edu1 In this paper, we use "nonminimal  derivations" synonymous ly  with "nonminimal  parses".
Normal lythe notions of derivation and parse tree are different.
However, in this paper we focus on parse trees asthe final result of derivation, thus we mean that a derivation is nonminimal  when its result is anonminimal  parse, in contrast o a minimal  derivation which produces a minimal  parse.
Unfortunately,formal definitions of min imal  and nonmin imal  derivations are outside the scope of this short paper;interested readers are encouraged to read Tomuro (1999).
(~) 2001 Association for Computat ional  LinguisticsComputational Linguistics Volume 27, Number 2((cat) - S/ (1 cat) =" NP.J (2 cat) -- VPP0 = (2, ~0 : \] (head) -- (2 head)/ / head subj) - (1 head}~, (head agr> - (1 head agr>((cat) - VPJ <1 cat/----" VP2 : (1, q)2 : ~ (head) -- (1 head} >I, (head type) - intrans ,/Figure 1Examples of productions.
((cat) -- NPpl = ("John",~l : ~ (head agr pers) - 3rd } >~, (head agr num)-  singJ((cat) -- V "\]~l (head agr pers) - 3rd / P3 = ("sleeps", ,I~ 3 : \] (head agr num) - sing )I, (head tense} - presnot constrain ing enough to disal low nonmin imal  derivat ions.
To solve this twofo ldprob lem,  we propose  an alternate def init ion of min ima l  parse tree for unif icat ion gram-mars,  and present  a modi f icat ion to Shieber 's  a lgor i thm which ensures minimality.It is impor tant  to note that the same spur ious  parses also occur in context-freeparsing,  specif ically in Ear ley's  algor i thm.
However ,  since the only in format ion a con-st ituent carries in context-free grammar  is the grammar  symbol ,  the spur ious  der iva-t ions only produce  exactly the same results as the normal  ones.
When the a lgor i thmis extended to unif icat ion grammar ,  however ,  these spur ious  parses are a prob lem.2.
Unification Grammar and Parse TreesShieber (1992) defines a unif icat ion grammar  as a 3-tuple (G, P, p0), where  ~ is thevocabu lary  of the grammar ,  P is the set of product ions ,  and P0 E P is the start pro-duct ion.
G contains L, a set of labels  (feature names);  C, a set of constants (featurevalues); and W, a set of terminals.
There are two k inds of product ions  in P: phrasaland lexical.
A phrasa l  product ion  is a 2-tuple (a, ~),  where  a is the arity of the rule (thenumber  of r ight-hand-s ide \[RHS\] constituents),  and ~ is a logical formula.
Typically,q~ is a conjunct ion of equat ions of the form pl - p2 or pl -" c, where  pl, p2 E L* arepaths,  and c E C. In an equat ion,  any  path  wh ich  begins wi th  an integer i (1 < i < a)represents the ith RHS const i tuent of the rule.
2 A lexical p roduct ion  is a 2-tuple (w, ~),where  w E W and q~ is the same as above,  except that there are no RHS constituents.F igure 1 shows some example  phrasa l  and lexical product ions  (P0 cor responds  to thecontext-free rule S --+ NP  VP and is the start product ion) .
Then a mode l  M relates toa formula  q~ by  a satisfaction relat ion ~ as usual  (M ~ ~), and when q~ is the formulain a product ion  p = (a, ~),  p is said to l icense M.Based on the logic above,  Shieber defines a parse tree and the language of ag rammar  expressed in his formal ism.
To define a val id parse tree, he first def ines theset of possible parse trees I1 = Ui>_0 Hi for a g iven grammar  G, where  each Eli is def inedas follows:DefinitionA parse tree r is a mode l  that is a member  of the infinite un ion of sets of bounded-depth  parse trees FI = Ui_>0 I1i, where  each IIi is def ined as:2 Shieber (1992) also uses a path that begins with 0 for the left-hand-side (LHS) constituent of a rule.
Inthis paper, we omit the 0 arcs and place the features of the LHS constituent directly at the root.
Thischange does not affect the formalism for the purpose of this paper.278Tomuro and Lytinen Nonminimal Derivations..rio is the set of models 7- for which there is a lexical productionp = <w, q)) E G such that 7- ~ 4<I I i ( i  > 0) is the set of models 7- for which there is a phrasal productionp = (a, q~) C G such that 7- ~ ~ and, for all 1 < i < a, 7-/{i) is defined and7-/<i} C Uj<iIIy.In the second condition, the extraction operator, denoted by / ,  retrieves the featurestructure found at the end of a particular path; so for instance 7-/<1) retrieves the firstsubconstituent on the RHS of the production that licenses 7-.
In the definition above,II0 contains all models that satisfy any lexical production in the grammar, while Hicontains all models that satisfy a phrasal production, and whose subconstituents areall i n  UjGi I\]j.To specify what constitutes a valid parse for a particular sentence, the next step isto define the yield of a parse tree.
It is defined recursively as follows: if 7- is licensed bysome lexical production p = {w, q~/, then the yield of 7- is w; or if 7- is licensed by somephrasal production {a, q~} and O~ 1 .
.
.
.
.
(X a are the yields of 7-/(1) .
.
.
.
.
7-/<a) respectively,then the yield of 7- is ~1 ...  %.Finally, Shieber defines a valid parse tree 7- c II for sentence Wl .
.
.
wn as follows:o2.The yield of 7- is Wl .
.
.
Wn7- is licensed by the start production poNotice that this definition allows extra features in a parse tree, because a parse tree7- is defined by the satisfaction relation (7- ~ ~), which allows the existence of featuresin the model that are not in the licensing production's formula.
Given this definition,for any valid parse tree 7-, we can construct another parse tree 7-' by simply adding anarbitrary (nonnumeric) feature to any node in 7-.
Such a parse tree T' is nonminimalbecause extra features are nonminimal with respect o the minimal features in thelicensing productions.
We will return to the issue of minimal and nonminimal parsetrees in Section 4.3.
The Abstract Parsing AlgorithmBased on the logic described above, Shieber defines an abstract parsing algorithm as aset of four logical deduction rules.
Each rule derives a new item, from previous itemsand/or  productions in the grammar.
An item is a 5-tuple {i,j, p, M, d), where i and j areindices into the sentence and specify which words in the sentence have been used toconstruct the item; p is the production used to construct the item; M is a model; and dis the position of the "dot"; i.e., how many subconstituents in p have been completedso far.The logical rules of the abstract algorithm are shown in Figure 2.
The Initial Itemrule produces the first item, and is constructed from the start production P0.
It spansnone of the input (i and j are both 0), and its model is the minimal model (ram) of P0.The Prediction rule is essentially the top-down rewriting of the expectation (asubconstituent just after the dot) in a prior item.
In this rule, the extraction of M/(d  +1 / retrieves the d + 1st submodel in M (i.e., expectation).
The function p, which isleft underspecified as a parameter in the abstract algorithm, filters out some featurespredefined in the various instantiations of the algorithm.
Here, it is applied to theexpectation, by which it effectively controls the top-down predictive power of the279Computational Linguistics Volume 27, Number 2INITIAL ITEM: {O,O, po, mm(~o),O)PREDICTION:SCANNING:li, j,p = la, ~l,M,d)(j,j, p', p(M/(d+l)) t3 mm(~'), 0) ' where d K a and p' = (a',O') ?
P(i,j,p = (a, ~},M,d}{i,j+lip, M t_l (mm(~2') \ {d+l ) ) ,d+l}  ' where  d < a and  (wj+l, O'} ?
PCOMPLETION: li'j'P = la' ~l 'M'd) (j,k,p' = (a',/I~'),M',a' / where d < aI {i, kip, M El (M' \ {d+l) ,d+l)Figure 2Shieber's parsing operations.I0 = (O,O, po, mm(420),O)11 = (O, 1,po, Ml,1)12 = (1,1,p2,M2,0 II3 = (1,2,p2,M3,1)I4 = (0, 2, p0, M4, 2)ag 5 ?yP?pers~ -n~ p mtrans3rd singFigure 3Items produced in the parse of John sleeps, and the final parse.algorithm and provides flexibility to the instantiated algorithms.
Then the expectationis unified with a production (~'), which can consistently rewrite it.
By this operation,some features in the expectation may be propagated own in the production.The remaining two rules advance the dot in a prior item, by unifying the sub-constituent to the right of the dot with either a lexical item from the input string (theScanning rule) or some other completed higher-level item (the Completion rule).
Bothrules perform the correct unification by utilizing the embedding operator (signifiedby \), which places a model M under a path p (M\p).We illustrate these operators with a simple step-by-step example parse.
Considerthe grammar that consists of the rules presented in Figure 1.
Using this grammar,Figure 3 shows the parse of the sentence John sleeps.
First, the Initial Item operatoris applied, producing item I0, whose model is mm(~o).
Next, the Scanning operatorscans the word John, producing 11.
The Prediction operator then produces 12.
Next,the word sleeps is scanned (since the first subconstituent of the model in 12 is a V),producing 13.
Finally, since the item in 13 is complete (d = 1, the arity of productionp2), Completion is applied to items 11 and/3, producing 14.
Model M4 is the final parseof the sentence.4.
Nonminimal DerivationsIn Section 2, we noted that Shieber's definition of parse trees allows them to be non-minimal.
We consider these to be invalid based on a principle that, since the unificationoperation as set union preserves minimality (as proved in Shieber, \[1992\]), repeatedapplications of unification using licensing productions hould result in parses thatcontain features only from those productions and nothing more.
In this section, we280Tomuro and Lytinen Nonminimal Derivations((cat) - VPI (1 cat) -- VPp4 = (2,~4: { (2 cat} -- ADV }| (head) - (1 head) /( (head modified) - true )Figure 4A phrasal production that results in a nonminimal derivation.I~ = (1,1,p4,M~, 0}/~' = (1,1, p2,M~r, 0)I~ = (1,2,p2,M~, 1)I~ = {0, 2, p0, M~, 2}M4 ~ .
"cat~-43c S a l tNP nea~/ l~Vub~V tag~.. 7se.t~pe~modifiedper~ n~ lntrans t}ue3rd singFigure 5Nonminimal derivation of John sleeps.formally define minimal and nonminimal parse trees, and show an example in whichnonminimal parse trees are produced by Shieber's algorithm.Our definition of minimal parse tree is to a large extent similar to Shieber's def-inition, but to ensure minimality, our definition uses the equality relation instead ofD, and inductively specifies a minimal parse tree bottom-up.DefinitionGiven a grammar G, a minimal parse tree r admitted by G is a model that is a memberof the infinite union of sets of bounded-depth parse trees 11' = Oi>0 IIl, where each171 is defined as:.2.For each lexical production p = (w, ~b) E G, mm(~) E 11'o.For each phrasal production p = (a, ~} E G, let rl .
.
.
.
.
ra E Uj<i I1;.
Ifr = mm(~) l i t1\(1) t3.. .
I lr l \(a}, then r E 1I;.It is obvious that 1I' is a subset of 17 in Shieber's definition.
Then, a nonminimal parsetree is defined as a model that is a member of the difference of the two sets (II - 1I').
3Here is a simple example in which a nonminimal parse is produced in Shieber'salgorithm.
Say that we add the production in Figure 4 to the grammar in the previoussection.
The intent of this production is to mark the verb with the feature modified if anadverb follows.
Using this grammar, Shieber's algorithm will produce a nonminimalparse for the sentence John sleeps, in addition to the minimal parse shown in theprevious section.
4 The nonminimal parse, shown in Figure 5, arises as follows: afterscanning John, Prediction can produce items I~ and I~', first using production p4 (thusinserting /head modified} - true into the model), and then P2.
Scanning the word3 Note that using subsumption (which we will discuss in Section 5) here does not work, for instance bysaying "a model r"  is a nonminimal parse tree if r "  E 17 and there exists r '  E II such that r '  _< r"",because some r" 's  are minimal.
See the example in Section 5.4 Here, we are assuming that the filtering function/9 is the identity function.281Computational Linguistics Volume 27, Number 2sleeps then produces I~ from I~ I.
Completion then can be applied directly to 11 and 11 byskipping a completion using I~ and I~, thereby producing item I~.
The feature modifiedremains in I~, even though an adverb was never encountered in the sentence.
Thefinal parse M~, shown in Figure 5, is clearly nonminimal according to our definitionbecause of this feature.Note that the example grammar can be changed to prevent he nonminimal parse,by moving the feature modified off of the head path in ff~4 (i.e., (modified / - trueinstead of (head modified / - true), sHowever, the point of the example is not to arguewhether or not well-designed grammars will produce erroneous parses.
A formallydefined parser (see the discussion below) should in principle produce correct parsesregardless of the grammar used; otherwise, the grammar formalism (i.e., Shieber's logicfor unification grammars) must be revised and properly constrained to allow only thekinds of productions with which the parser produces correct results.In general, nonminimal derivations may arise whenever two or more predictionsthat are not mutually exclusive can be produced at the same point in the sentence;i.e., two prediction items (i, i, p, M, 0 / and (i, i, p', M ~, 0 / are produced such that MM / and M and M ~ are unifiable.
In the example, items 12 = (1,1, p2, M2, 0/ and I~ --(1,1, P4, M~, 0) (as well as I2 and I~ ~ = (1,1, p2, M~ ~, 0/) are two such items.
Since the twopredictions did not have any conflicting features from the beginning, a situation mayoccur where a completion generated from one prediction can fill the other predictionwithout causing conflict.
When this happens, features that were in the other predictionbut not the original one become nonminimal in the resulting model.As to what causes nonminimal situations, we speculate that there are a numberof possibilRies.
First, nonminimal derivations occur when a prediction is filled by acomplete item that was not generated from the prediction.
This mismatch will nothappen if parsing is done in one direction only (e.g.
purely top-down or bottom-upparsing).
Thus, the mixed-direction parsing strategy is a contributing factor.Second, wrong complete items are retrieved because Shieber's item-based algo-rithm makes all partial results available during parsing, as if they are kept in a globalstructure (such as a chart in chart parsing).
But if the accessibility of items were some-how restricted, prediction-completion mismatch would not happen.
In this respect,other chart-based algorithms for unification grammars which adopt mixed-directionparsing strategy, including head-corner parsing (van Noord 1997) and left-corner pars-ing (Alshawi 1992), are subject o the same problem.Third, extra features can only appear when the grammar contains rules whichinteract in a certain way (such as rules P2 and P4 above).
If the grammar containedno such rules, or if p (the filtering function applied in Prediction) filtered out thosefeatures, even the prediction-completion mismatch would not produce nonminimalderivations.As we stated in the beginning of this section, we consider nonminimal parses tobe invalid on the basis of minimality.
It then immediately follows that any parsingalgorithm that produces nonminimal parses is considered to be unsound; in particular,Shieber's algorithm is unsound.
However, since nonminimal parse trees have the sameyield as their minimal counterparts, his algorithm does indeed recognize xactly thelanguage of a given grammar.
So, Shieber's algorithm is sound as a recognizer, 6 butnot as a transducer or parser (as in van Noord, \[1997\]) where the correctness of outputmodels (i.e., parse trees) is critical.
In other words, Shieber's algorithm is correct up to5 Note that adding (head modified) -- false to ~2 (VP --* V) or ~3 (sleeps) isnot feasible, because theycannot specify the modified feature at their level,6 In fact, Shieber hints at this: "The process of parsing (more properly, recognition)..." (Shieber 1992, 78).282Tomuro and Lytinen Nonminimal Derivationslicensing, but incorrect on the basis of a stronger criteria of minimality.
Thus, to guar-antee correctness based on minimality, we need another algorithm; such an a lgor i thmis exactly the solution to the nonmin imal  der ivat ion problem.5.
Practical TechniquesBefore present ing our solution to the nonmin imal  der ivat ion problem, we discussseveral possible practical techniques to get around the prob lem in implemented sys-tems.
These are known techniques, which have been appl ied to solve other problemsin unif ication-based systems.
However ,  most  of them only offer partial solutions tothe nonmin imal  derivat ion problem.
First, whenever  Shieber's a lgor i thm produces anonmin imal  derivation, it also produces a corresponding minimal  der ivat ion (Tomuro1999).
Thus, one possible solution is to use subsumpt ion  to discard items that are morespecific than any other items that are produced.
Subsumpt ion  has often been used inunif ication-based systems to pack items or models  (e.g., A lshawi  1992).
However,s imple subsumpt ion  may filter out val id parses for some grammars,  thus sacrificingcompleteness.
7Another  possibil ity is to filter out problematic features in the Prediction step byusing the funct ion p. However,  automatic detection of such features (i.e., automaticderivat ion of p) is undecidable for the same reason as the prediction nonterminationproblem (caused by left recursion) for unif ication grammars  (Shieber 1985).
Manualdetection is also problematic:  when a grammar  is large, part icular ly if semantic fea-tures are included, complete detection is nearly impossible.
As for the techniquesdeveloped so far which (partially) solve predict ion nonterminat ion (e.g., Shieber 1985;Haas 1989; Samuelsson 1993), they do not apply  to nonmin imal  derivations becausenonmin imal  derivations may arise wi thout  left recursion or recursion in genera l  s Oneway  is to define p to filter out all features except the context-free backbone of predic-tions.
However ,  this severely restricts the range of possible instantiations of Shieber'salgorithm.
9A third possibil ity is to manual ly  fix the grammar  so that nonmin imal  derivationsdo not occur, as we noted in Section 4.
However ,  this approach is problematic for thesame reason as the manua l  der ivat ion of p ment ioned above.6.
Modified AlgorithmFinally, we propose an algor i thm that does not produce nonmin imal  derivations.
It is amodif icat ion of Shieber's a lgor i thm that incorporates parent pointers.
Figure 6 shows7 For example, when there are two predictions M1 and M2 for category C and a production p whereM1 : {<cat> -- C, <x> - a}, M2 : {<cat> - C, <y> - b}, and p = <1, {<cat> - C, <1 cat> "- D, <x> - a}>respectively, the resulting model M2 ~ = {<cat> - C, <1 cat> - D, <x> -- a, <y> -- b} will have strictly moreinformation than the other esulting model MI' = {<cat> ~ C, <1 cat> - D, <x> - a}, although bothmodels are minimal.8 We do not show any particular example here, but if we change the left-recursive VP rule in the earlierexample to a non-left-recursive rule, for instance VP --* VP2 ADV, and add some rules, a nonrninimalparse will indeed arise.Note also that some (but not all) cases of prediction ontermination will produce nonminimalderivations.
Those cases occur when there is a prediction for a category, and repeated applications ofsome left-recursive rule(s) generate predictions for the same category that are not mutually exclusive tothe original prediction or each other.9 In head-corner parsing, Sikkel (1997) proposes the use of transitive features: features that propagateonly through ead arcs.
However, this method oes not solve nonminimal derivations either, becauseproblematic features may be subfeatures of a head (such as the example case shown earlier), which willnot be filtered.283Computational Linguistics Volume 27, Number 2INITIAL ITEM:PREDICTION:(id, nil, (O,O, po, mm( ~o),O) ' where id is a new symbol(id, pid, (i,j,p = (a, ~),M,d) )(id', id, (j,j, p', p(M/ (d+l) ) U mm(~I,'), 0))'where id I is a new symbol, and d ( a and pl = (ar,~t) C PSCANNING:COMPLETION:(id, pid, (i,j,p = (a, ~),M,d) )(id, pid, (i,j+l,p,M U mm( ~') \ (d+l),d+l)) ' where d< a and (wj+D ~') E P(id, pid,(i,j,p,M,d)) (id",id,(j,k,p',M',a')) where d < a(ia, pie, (i,k,p, UU (U' \ (d+l)),d+l)) 'Figure 6Shieber's parsing operations modified.the modified algorithm.
In the figure, an item is represented by a nested 3-tuple, wherethe first argument is the self index, the second is the parent index/pointer, and thethird is the old 5-tuple used in Shieber's original algorithm.
A parent pointer, then,is set in Prediction--the r sulting item has the index of the antecedent item (id) asits parent.
By generating a new symbol for the self index in every Prediction item(id'), parent pointers in those items are threaded to form a prediction path.
Then inCompletion, the parent pointer is used to restrict he antecedent items: the completeitem (on the right) must have the prior expectation (on the left) as its parent (id),thereby ensuring a prediction path to be precisely restored.While this modified algorithm offers a complete solution on the level of logic, ithas some undesirable implications in implemented systems.
The most prominent oneis that the parent pointer scheme makes implementation f memoizat ion rather diffi-cult.
Normally, memoization is used to avoid storing duplicate items that are identical;however, in the modified algorithm, many items that are otherwise identical will havedifferent parent pointers, thereby changing the polynomial time algorithm (O(n3); Ear-ley \[1970\]) to an exponential one.
To avoid computational inefficiency, a way must bedevised for items that are identical except for parent poInters to share information,especially models, and thus avoid the expense of duplicate identical unification opera-tions.
One possibility is to represent the 5-tuple from Shieber's original algorithm by aseparate structure and have an index to it in the new 3-tuple item.
This way, not onlycan the items be shared, they can still be memoized in the usual way as well.
Anotherpossibility is to adopt an efficiency technique along the line of selective memoization(van Noord 1997).
Implementation a d empirical analysis is our future research.Whatever the practical performance will turn out to be, it is important to notethat the proposed algorithm is a formal solution that guarantees minimality for anygrammar defined in Shieber's logic.
Moreover the algorithm preserves the same gen-erality and flexibility as Shieber's: a mixed top-down, bottom-up arsing with thefiltering function p to allow various instantiations of the algorithm to characterizetheir algorithms.ReferencesAlshawi, H., editor.
1992.
The Core LanguageEngine.
MIT Press.Earley, J.
1970.
An efficient context-freeparsing algorithm.
Communications oftheACM, 13(2).Gazdar, G., E. Klein, G. Pullum, and I. Sag.1985.
Generalized Phrase Structure Grammar.Blackwell Publishing.Haas, A.
1989.
A parsing algorithm forunification grammar.
ComputationalLinguistics, 15(4):219-232.284Tomuro and Lytinen Nonminimal DerivationsPereira, F. and D. Warren.
1980.
Definiteclause grammars for language analysis.Arti~'cial Intelligence, 13:231-278.Pollard, C. and I.
Sag.
1994.
Head-drivenPhrase Structure Grammar.
CSLI.
Universityof Chicago Press.Samuelsson, C. 1993.
Avoidingnon-termination in unification grammars.In Natural Language Understanding and LogicProgramming IV.Shieber, S. 1985.
Using restriction to extendparsing algorithms for complex-feature-based formalisms.
In Proceedings ofthe 23rdAnnual Meeting, Association forComputational Linguistics.Shieber, S. 1986.
An Introduction toUniX'cation-Based Approaches to Grammar.CSLI.
University of Chicago Press.Shieber, S. 1992.
Constraint-based GrammarFormalisms.
MIT Press.Sikkel, K. 1997.
Parsing Schemata.Springer?Verlag.Tomuro, N. 1999.
Left-Corner ParsingAlgorithm for UniX'cation Grammars.
Ph.D.thesis, DePaul University.van Noord, G. 1997.
An efficientimplementation f the head-corner parser.Computational Linguistics, 23(3):425-456.285
