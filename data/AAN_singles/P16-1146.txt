Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1546?1556,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsOptimizing Spectral Learning for ParsingShashi Narayan and Shay B. CohenSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LE, UK{snaraya2,scohen}@inf.ed.ac.ukAbstractWe describe a search algorithm for opti-mizing the number of latent states whenestimating latent-variable PCFGs withspectral methods.
Our results show thatcontrary to the common belief that thenumber of latent states for each nontermi-nal in an L-PCFG can be decided in isola-tion with spectral methods, parsing resultssignificantly improve if the number of la-tent states for each nonterminal is globallyoptimized, while taking into account in-teractions between the different nontermi-nals.
In addition, we contribute an empiri-cal analysis of spectral algorithms on eightmorphologically rich languages: Basque,French, German, Hebrew, Hungarian, Ko-rean, Polish and Swedish.
Our resultsshow that our estimation consistently per-forms better or close to coarse-to-fineexpectation-maximization techniques forthese languages.1 IntroductionLatent-variable probabilistic context-free gram-mars (L-PCFGs) have been used in the natural lan-guage processing community (NLP) for syntacticparsing for over a decade.
They were introducedin the NLP community by Matsuzaki et al (2005)and Prescher (2005), with Matsuzaki et al us-ing the expectation-maximization (EM) algorithmto estimate them.
Their performance on syntac-tic parsing of English at that stage lagged behindstate-of-the-art parsers.Petrov et al (2006) showed that one of thereasons that the EM algorithm does not estimatestate-of-the-art parsing models for English is thatthe EM algorithm does not control well for themodel size used in the parser ?
the number of la-tent states associated with the various nontermi-nals in the grammar.
As such, they introduced acoarse-to-fine technique to estimate the grammar.It splits and merges nonterminals (with latent stateinformation) with the aim to optimize the likeli-hood of the training data.
Together with othertypes of fine tuning of the parsing model, this ledto state-of-the-art results for English parsing.In more recent work, Cohen et al (2012) de-scribed a different family of estimation algorithmsfor L-PCFGs.
This so-called ?spectral?
family oflearning algorithms is compelling because it offersa rigorous theoretical analysis of statistical conver-gence, and sidesteps local maxima issues that arisewith the EM algorithm.While spectral algorithms for L-PCFGs arecompelling from a theoretical perspective, theyhave been lagging behind in their empirical resultson the problem of parsing.
In this paper we showthat one of the main reasons for that is that spectralalgorithms require a more careful tuning proce-dure for the number of latent states than that whichhas been advocated for until now.
In a sense, therelationship between our work and the work ofCohen et al (2013) is analogous to the relation-ship between the work by Petrov et al (2006) andthe work by Matsuzaki et al (2005): we suggesta technique for optimizing the number of latentstates for spectral algorithms, and test it on eightlanguages.Our results show that when the number of la-tent states is optimized using our technique, theparsing models the spectral algorithms yield per-form significantly better than the vanilla-estimatedmodels, and for most of the languages ?
better thanthe Berkeley parser of Petrov et al (2006).As such, the contributions of this parser are two-fold:?
We describe a search algorithm for optimiz-1546ing the number of latent states for spectrallearning.?
We describe an analysis of spectral algo-rithms on eight languages (until now the re-sults of L-PCFG estimation with spectral al-gorithms for parsing were known only forEnglish).
Our parsing algorithm is ratherlanguage-generic, and does not require sig-nificant linguistically-oriented adjustments.In addition, we dispel the common wisdom thatmore data is needed with spectral algorithms.
Ourmodels yield high performance on treebanks ofvarying sizes from 5,000 sentences (Hebrew andSwedish) to 40,472 sentences (German).The rest of the paper is organized as follows.In ?2 we describe notation and background.
?3further investigates the need for an optimizationof the number of latent states in spectral learn-ing and describes our optimization algorithm, asearch algorithm akin to beam search.
In ?4 we de-scribe our experiments with natural language pars-ing for Basque, French, German, Hebrew, Hungar-ian, Korean, Polish and Swedish.
We conclude in?5.2 Background and NotationWe denote by [n] the set of integers {1, .
.
.
, n}.An L-PCFG is a 5-tuple (N , I,P, f, n) where:?
N is the set of nonterminal symbols in thegrammar.
I ?
N is a finite set of intermi-nals.
P ?
N is a finite set of preterminals.We assume thatN = I ?
P , and I ?
P = ?.Hence we have partitioned the set of nonter-minals into two subsets.?
f :N ?
N is a function that maps each non-terminal a to the number of latent states ituses.
The set [ma] includes the possible hid-den states for nonterminal a.?
[n] is the set of possible words.?
For all a ?
I, b ?
N , c ?
N , h1?
[ma],h2?
[mb], h3?
[mc], we have a binarycontext-free rule a(h1)?
b(h2) c(h3).?
For all a ?
P , h ?
[ma], x ?
[n], we have alexical context-free rule a(h)?
x.The estimation of an L-PCFG requires an as-signment of probabilities (or weights) to each ofthe rules a(h1) ?
b(h2) c(h3) and a(h) ?
x,and also an assignment of starting probabilities foreach a(h), where a ?
I and h ?
[ma].
Estima-tion is usually assumed to be done from a set ofparse trees (a treebank), where the latent states arenot included in the data ?
only the ?skeletal?
treeswhich consist of nonterminals in N .L-PCFGs, in their symbolic form, are relatedto regular tree grammars, an old grammar formal-ism, but they were introduced as statistical mod-els for parsing with latent heads more recentlyby Matsuzaki et al (2005) and Prescher (2005).Earlier work about L-PCFGs by Matsuzaki et al(2005) used the expectation-maximization (EM)algorithm to estimate the grammar probabilities.Indeed, given that the latent states are not ob-served, EM is a good fit for L-PCFG estimation,since it aims to do learning from incomplete data.This work has been further extended by Petrov etal.
(2006) to use EM in a coarse-to-fine fashion:merging and splitting nonterminals using the la-tent states to optimize the number of latent statesfor each nonterminal.Cohen et al (2012) presented a so-called spec-tral algorithm to estimate L-PCFGs.
This algo-rithm uses linear-algebraic procedures such as sin-gular value decomposition (SVD) during learning.The spectral algorithm of Cohen et al builds onan estimation algorithm for HMMs by Hsu et al(2009).1Cohen et al (2013) experimented withthis spectral algorithm for parsing English.
A dif-ferent variant of a spectral learning algorithm forL-PCFGs was developed by Cohen and Collins(2014).
It breaks the problem of L-PCFG estima-tion into multiple convex optimization problemswhich are solved using EM.The family of L-PCFG spectral learning algo-rithms was further extended by Narayan and Co-hen (2015).
They presented a simplified versionof the algorithm of Cohen et al (2012) that es-timates sparse grammars and assigns probabili-ties (instead of weights) to the rules in the gram-mar, and as such does not suffer from the prob-lem of negative probabilities that arise with theoriginal spectral algorithm (see discussion in Co-hen et al, 2013).
In this paper, we use the algo-rithms by Narayan and Cohen (2015) and Cohen1A related algorithm for weighted tree automata (WTA)was developed by Bailly et al (2010).
However, the con-version from L-PCFGs to WTA is not straightforward, andinformation is lost in this conversion.
See also (Rabusseau etal., 2016).1547VPVchasedNPDtheNcatSNPDtheNmouseVPFigure 1: The inside tree (left) and outside tree(right) for the nonterminal VP in the parse tree(S (NP (D the) (N mouse)) (VP (Vchased) (NP (D the) (N cat)))) forthe sentence ?the mouse chased the cat.
?et al (2012), and we compare them against state-of-the-art L-PCFG parsers such as the Berkeleyparser (Petrov et al, 2006).
We also compare ouralgorithms to other state-of-the-art parsers whereelaborate linguistically-motivated feature specifi-cations (Hall et al, 2014), annotations (Crabb?e,2015) and formalism conversions (Fern?andez-Gonz?alez and Martins, 2015) are used.3 Optimizing Spectral EstimationIn this section, we describe our optimization algo-rithm and its motivation.3.1 Spectral Learning of L-PCFGs andModel SizeThe family of spectral algorithms for latent-variable PCFGs rely on feature functions that aredefined for inside and outside trees.
Given a tree,the inside tree for a node contains the entire sub-tree below that node; the outside tree contains ev-erything in the tree excluding the inside tree.
Fig-ure 1 shows an example of inside and outside treesfor the nonterminal VP in the parse tree of the sen-tence ?the mouse chased the cat?.With L-PCFGs, the model dictates that an in-side tree and an outside tree that are connected ata node are statistically conditionally independentof each other given the node label and the latentstate that is associated with it.
As such, one canidentify the distribution over the latent states for agiven nonterminal a by using the cross-covariancematrix of the inside and the outside trees, ?a.
Formore information on the definition of this cross-covariance matrix, see Cohen et al (2012) andNarayan and Cohen (2015).The L-PCFG spectral algorithms use singularvalue decomposition (SVD) on ?ato reduce thedimensionality of the feature functions.
If ?aiscomputed from the true L-PCFG distribution thenthe rank of ?a(the number of non-zero singularvalues) gives the number of latent states accordingto the model.In the case of estimating ?afrom data gener-ated from an L-PCFG, the number of latent statesfor each nonterminal can be exposed by cappingit when the singular values of ?aare smaller thansome threshold value.
This means that spectral al-gorithms give a natural way for the selection of thenumber of latent states for each nonterminal a inthe grammar.However, when the data from which we esti-mate an L-PCFG model are not drawn from an L-PCFG (the model is ?incorrect?
), the number ofnon-zero singular values (or the number of singu-lar values which are large) is no longer sufficientto determine the number of latent states for eachnonterminal.
This is where our algorithm comesinto play: it optimizes the number of latent searchfor each nonterminal by applying a search algo-rithm akin to beam search.3.2 Optimizing the Number of Latent StatesAs mentioned in the previous section, the numberof non-zero singular values of ?agives a criterionto determine the number of latent states mafor agiven nonterminal a.
In practice, we cap manotto include small singular values which are close to0, because of estimation errors of ?a.This procedure does not take into account theinteractions that exist between choices of latentstate numbers for the various nonterminals.
Inprinciple, given the independence assumptionsthat L-PCFGs make, choosing the nonterminalsbased only on the singular values is ?statisticallycorrect.?
However, because in practice the mod-eling assumptions that we make (that natural lan-guage parse trees are drawn from an L-PCFG) donot hold, we can improve further the accuracy ofthe model by taking into account the nonterminalinteraction.
Another source of difficulty in choos-ing the number of latent states based the singu-lar values of ?ais sampling error: in practice, weare using data to estimate ?a, and as such, evenif the model is correct, the rank of the estimatedmatrix does not have to correspond to the rank of?aaccording to the true distribution.
As a mat-ter of fact, in addition to neglecting small singularvalues, the spectral methods of Cohen et al (2013)and Narayan and Cohen (2015) also cap the num-ber of latent states for each nonterminal to an up-1548Inputs: An input treebank divided into training and devel-opment set.
A basic spectral estimation algorithm S with itsdefault setting.
An integer k denoting the size of the beam.An integer m denoting the upper bound on the number oflatent states.Algorithm:(Step 0: Initialization)?
Set Q, a queue of size k, to be empty.?
Estimate an L-PCFG GS: (N , I,P, fS, n) using S.?
Initialize f = fS, a function that maps each nontermi-nal a ?
N to the number of latent states.?
Let L be a list of nonterminals (a1, .
.
.
, aM) such thatai?
N for which to optimize the number of latentstates.?
Let s be the F1score for the above L-PCFG GSon thedevelopment set.?
Put in Q the element (s, 1, f, coarse).?
The queue is ordered by s, the first element of tuples,in the queue.
(Step 1: Search, repeat until termination happens)?
Dequeue the queue into (s, j, f, t) where j is the indexin the input nonterminal list L.?
If j = (M + 1), return f .?
If t is coarse then for each m0?
{1, 5, 10, .
.
.
,m}:?
Let f0be such that ?a 6= ajf0(a) = f(a) andf0(aj) = m0.?
Train an L-PCFG G0using S but with f0.?
Let s0be the F1score for G0on the developmentset.?
Enqueue into Q: (s0, j, f0, refine).?
If t is refine then for each m0?
{f(a) + ` | ` ?
{?4,?3,?2,?1, 0, 1, 2, 3, 4}}:?
Let f0be such that ?a 6= ajf0(a) = f(a) andf0(aj) = m0.?
Train an L-PCFG G0using S but with f0.?
Let s0be the F1score for G0on the developmentset.?
Enqueue into Q: (s0, j + 1, f0, coarse).Figure 2: A search algorithm for finding the opti-mal number of latent states.per bound to keep the grammar size small.Petrov et al (2006) improves over the estima-tion described in Matsuzaki et al (2005) by takinginto account the interactions between the nonter-minals and their latent state numbers in the train-ing data.
They use the EM algorithm to split andmerge nonterminals using the latent states, and op-timize the number of latent states for each nonter-minal such that it maximizes the likelihood of atraining treebank.
Their refined grammar success-fully splits nonterminals to various degrees to cap-ture their complexity.
We take the analogous stepwith spectral methods.
We propose an algorithmwhere we first compute ?aon the training dataand then we optimize the number of latent statesfor each nonterminal by optimizing the PARSE-VAL metric (Black et al, 1991) on a developmentset.Our optimization algorithm appears in Figure 2.The input to the algorithm is training and develop-ment data in the form of parse trees, a basic spec-tral estimation algorithm S in its default setting,an upper bound m on the number of latent statesthat can be used for the different nonterminals anda beam size k which gives a maximal queue sizefor the beam.
The algorithm aims to learn a func-tion f that maps each nonterminal a to the numberof latent states.
It initializes f by estimating a de-fault grammar GS: (N , I,P, fS, n) using S andsetting f = fS.
It then iterates over a ?
N , im-proving f such that it optimizes the PARSEVALmetric on the development set.The state of the algorithm includes a queue thatconsists of tuples of the form (s, j, f, t) where fis an assignment of latent state numbers to eachnonterminal in the grammar, j is the index of anonterminal to be explored in the input nontermi-nal list L, s is the F1score on the development setfor a grammar that is estimated with f and t is atag that can either be coarse or refine.The algorithm orders these tuples by s in thequeue, and iteratively dequeues elements from thequeue.
Then, depending on the label t, it eithermakes a refined search for the number of latentstates for aj, or a more coarse search.
As such,the algorithm can be seen as a variant of a beamsearch algorithm.The search algorithm can be used with anytraining algorithm for L-PCFGs, including the al-gorithms of Cohen et al (2013) and Narayan andCohen (2015).
These methods, in their default set-ting, use a function fSwhich maps each nonter-minal a to a fixed number of latent states maituses.
In this case, S takes as input training data,in the form of a treebank, decomposes into in-side and outside trees at each node in each tree inthe training set; and reduces the dimensionality ofthe inside and outside feature functions by running1549lang.
Basque French German-N German-T Hebrew Hungarian Korean Polish Swedishtrainsent.
7,577 14,759 18,602 40,472 5,000 8,146 23,010 6,578 5,000tokens 96,565 443,113 328,531 719,532 128,065 170,221 301,800 66,814 76,332lex.
size 25,136 27,470 48,509 77,219 15,971 40,775 85,671 21,793 14,097#nts 112 222 208 762 375 112 352 198 148devsent.
948 1,235 1,000 5,000 500 1,051 2,066 821 494tokens 13,893 38,820 17,542 76,704 11,305 30,010 25,729 8,391 9,339testsent.
946 2,541 1,000 5,000 716 1,009 2,287 822 666tokens 11,477 75,216 17,585 92,004 17,002 19,913 28,783 8,336 10,675Table 1: Statistics about the different datasets used in our experiments for the training (?train?
), development (?dev?)
and test(?test?)
sets.
?sent.?
denotes the number of sentences in the dataset, ?tokens?
denotes the total number of words in the dataset,?lex.
size?
denotes the vocabulary size in the training set and ?#nts?
denotes the number of nonterminals in the training set afterbinarization.SVD on the cross-covariance matrix ?aof the in-side and the outside trees, for each nonterminal a.Cohen et al (2013) estimate the parameters of theL-PCFG up to a linear transformation using f(a)non-zero singular values of ?a, whereas Narayanand Cohen (2015) use the feature representationsinduced from the SVD step to cluster instances ofnonterminal a in the training data into f(a) clus-ters; these clusters are then treated as latent statesthat are ?observed.?
Finally, Narayan and Cohenfollow up with a simple frequency count maxi-mum likelihood estimate to estimate the parame-ters in the L-PCFG with these latent states.An important point to make is that the learningalgorithms of Narayan and Cohen (2015) and Co-hen et al (2013) are relatively fast,2in comparisonto the EM algorithm.
They require only one iter-ation over the data.
In addition, the SVD step ofS for these learning algorithms is computed justonce for a large m. The SVD of a lower rank canthen be easily computed from that SVD.4 ExperimentsIn this section, we describe our setup for parsingexperiments on a range of languages.4.1 Experimental SetupDatasets We experiment with nine treebanksconsisting of eight different morphologically richlanguages: Basque, French, German, Hebrew,Hungarian, Korean, Polish and Swedish.
Table 1shows the statistics of 9 different treebanks withtheir splits into training, development and test sets.Eight out of the nine datasets (Basque, French,German-T, Hebrew, Hungarian, Korean, Polish2It has been documented in several papers that the fam-ily of spectral estimation algorithms is faster than algorithmssuch as EM, not just for L-PCFGs.
See, for example, Parikhet al (2012).and Swedish) are taken from the workshop onStatistical Parsing of Morphologically Rich Lan-guages (SPMRL; Seddah et al, 2013).
The Ger-man corpus in the SPMRL workshop is taken fromthe TiGer corpus (German-T, Brants et al, 2004).We also experiment with another German cor-pus, the NEGRA corpus (German-N, Skut et al,1997), in a standard evaluation split.3Words inthe SPMRL datasets are annotated with their mor-phological signatures, whereas the NEGRA cor-pus does not contain any morphological informa-tion.Data preprocessing and treatment of rarewords We convert all trees in the treebanks to abinary form, train and run the parser in that form,and then transform back the trees when doing eval-uation using the PARSEVAL metric.
In addition,we collapse unary rules into unary chains, so thatour trees are fully binarized.
The column ?#nts?in Table 1 shows the number of nonterminals af-ter binarization in the various treebanks.
Beforebinarization, we also drop all functional informa-tion from the nonterminals.
We use fine tags forall languages except Korean.
This is in line withBj?orkelund et al (2013).4For Korean, there are2,825 binarized nonterminals making it impracti-cal to use our optimization algorithm, so we usethe coarse tags.Bj?orkelund et al (2013) have shown that themorphological signatures for rare words are usefulto improve the performance of the Berkeley parser.3We use the first 18,602 sentences as a training set, thenext 1,000 sentences as a development set and the last 1,000sentences as a test set.
This corresponds to an 80%-10%-10%split of the treebank.4In their experiments Bj?orkelund et al (2013) found thatfine tags were not useful for Basque also; they did not find aproper explanation for that.
In our experiments, however, wefound that fine tags were useful for Basque.
To retrieve thefine tags, we concatenate coarse tags with their refinementfeature (?AZP?)
values.1550In our preliminary experiments with na?
?ve spectralestimation, we preprocess rare words in the train-ing set in two ways: (i) we replace them with theircorresponding POS tags, and (ii) we replace themwith their corresponding POS+morphological sig-natures.
We follow Bj?orkelund et al (2013) andconsider a word to be rare if it occurs less than 20times in the training data.
We experimented bothwith a version of the parser that does not ignoreand does ignore letter cases, and discovered thatthe parser behaves better when case is not ignored.Spectral algorithms: subroutine choices Thelatent state optimization algorithm will workwith either the clustering estimation algorithm ofNarayan and Cohen (2015) or the spectral algo-rithm of Cohen et al (2013).
In our setup, wefirst run the latent state optimization algorithmwith the clustering algorithm.
We then run thespectral algorithm once with the optimized f fromthe clustering algorithm.
We do that because theclustering algorithm is significantly faster to itera-tively parse the development set, because it leadsto sparse estimates.Our optimization algorithm is sensitive to theinitialization of the number of latent states as-signed to each nonterminals as it sequentially goesthrough the list of nonterminals and chooses latentstate numbers for each nonterminal, keeping latentstate numbers for other nonterminals fixed.
In oursetup, we start our search algorithm with the bestmodel from the clustering algorithm, controllingfor all hyperparameters; we tune f , the functionwhich maps each nonterminal to a fixed numberof latent states m, by running the vanilla versionwith different values of m for different languages.Based on our preliminary experiments, we set mto 4 for Basque, Hebrew, Polish and Swedish; 8for German-N; 16 for German-T, Hungarian andKorean; and 24 for French.We use the same features for the spectral meth-ods as in Narayan and Cohen (2015) for German-N. For the SPMRL datasets we do not use the headfeatures.
These require linguistic understanding ofthe datasets (because they require head rules forpropagating leaf nodes in the tree), and we discov-ered that simple heuristics for constructing theserules did not yield an increase in performance.We use the kmeans function in Matlab todo the clustering for the spectral algorithm ofNarayan and Cohen (2015).
We experimentedwith several versions of k-means, and discoveredthat the version that works best in a set of prelimi-nary experiments is hard k-means.5Decoding and evaluation For efficiency, weuse a base PCFG without latent states to prunemarginals which receive a value less than 0.00005in the dynamic programming chart.
This isjust a bare-bones PCFG that is estimated usingmaximum likelihood estimation (with frequencycount).
The parser takes part-of-speech taggedsentences as input.
We tag the German-N data us-ing the Turbo Tagger (Martins et al, 2010).
Forthe languages in the SPMRL data we use the Mar-Mot tagger of M?ueller et al (2013) to jointly pre-dict the POS and morphological tags.6The parseritself can assign different part-of-speech tags towords to avoid parse failure.
This is also particu-larly important for constituency parsing with mor-phologically rich languages.
It helps mitigate theproblem of the taggers to assign correct tags whenlong-distance dependencies are present.For all results, we report the F1measureof the PARSEVAL metric (Black et al, 1991).We use the EVALB program7with the parame-ter file COLLINS.prm (Collins, 1999) for theGerman-N data and the SPMRL parameter file,spmrl.prm, for the SPMRL data (Seddah et al,2013).In this setup, the latent state optimization algo-rithm terminates in few hours for all datasets ex-cept French and German-T.
The German-T datahas 762 nonterminals to tune over a large develop-ment set consisting of 5,000 sentences, whereas,the French data has a high average sentence lengthof 31.43 in the development set.8Following Narayan and Cohen (2015), we fur-ther improve our results by using multiple spec-tral models where noise is added to the underlyingfeatures in the training set before the estimation ofeach model.9Using the optimized f , we estimate5To be more precise, we use the Matlab function kmeanswhile passing it the parameter ?start?=?sample?
to ran-domly sample the initial centroid positions.
In our experi-ments, we found that default initialization of centroids differsin Matlab14 (random) and in Matlab15 (kmeans++).
Our es-timation performs better with random initialization.6See Bj?orkelund et al (2013) for the performance of theMarMot tagger on the SPMRL datasets.7http://nlp.cs.nyu.edu/evalb/8To speed up tuning on the French data, we drop sentenceswith length >46 from the development set, dropping its sizefrom 12,35 to 1,006.9We only use the algorithm of Narayan and Cohen (2015)for the noisy model estimation.
They have shown that de-coding with noisy models performs better with their sparse1551lang.
Basque French German-N German-T Hebrew Hungarian Korean Polish SwedishBkvan 69.2 79.9 - 81.7 87.8 83.9 71.0 84.1 74.5rep 84.3 79.7 - 82.7 89.6 89.1 82.8 87.1 75.5Clvan (pos) 69.8 73.9 75.7 78.3 88.0 81.3 68.7 90.3 70.9van (rep) 78.6 73.7 - 78.8 88.1 84.7 76.5 90.4 71.4opt 81.2?76.7 77.8 81.7 90.1 87.2 79.2 92.0 75.2Spvan 78.1 78.0 77.6 82.0 89.2 87.7 80.6 91.7 73.4opt 79.0 78.1?79.0?82.9?90.3?87.8?80.9?91.7?75.5?Bk multiple 87.4 82.5 - 85.0 90.5 91.1 84.6 88.4 79.5Cl multiple 83.4 79.9 82.7 85.1 90.6 89.0 80.8 92.5 78.3Hall et al ?14 83.7 79.4 - 83.3 88.1 87.4 81.9 91.1 76.0Crabb?e ?15 84.0 80.9 - 84.1 90.7 88.3 83.1 92.8 77.9Table 2: Results on the development datasets.
?Bk?
makes use of the Berkeley parser with its coarse-to-fine mechanism tooptimize the number of latent states (Petrov et al, 2006).
For Bk, ?van?
uses the vanilla treatment of rare words using signaturesdefined by Petrov et al (2006), whereas ?rep.?
uses the morphological signatures instead.
?Cl?
uses the algorithm of Narayanand Cohen (2015) and ?Sp?
uses the algorithm of Cohen et al (2013).
In Cl, ?van (pos)?
and ?van (rep)?
are vanilla estima-tions (i.e., each nonterminal is mapped to fixed number of latent states) replacing rare words by POS or POS+morphologicalsignatures, respectively.
The best of these two models is used with our optimization algorithm in ?opt?.
For Sp, ?van?
usesthe best setting for unknown words as Cl.
Best result in each column from the first seven rows is in bold.
In addition, ourbest performing models from rows 3-7 are marked with?.
?Bk multiple?
shows the best results with the multiple models usingproduct-of-grammars procedure (Petrov, 2010) and discriminative reranking (Charniak and Johnson, 2005).
?Cl multiple?
givesthe results with multiple models generated using the noise induction and decoded using the hierarchical decoding (Narayan andCohen, 2015).
Bk results are not available on the development dataset for German-N. For others, we report Bk results fromBj?orkelund et al (2013).
We also include results from Hall et al (2014) and Crabb?e (2015).lang.
Basque French German-N German-T Hebrew Hungarian Korean Polish SwedishBk 74.7 80.4 80.1 78.3 87.0 85.2 78.6 86.8 80.6Clvan 79.6 74.3 76.4 74.1 86.3 86.5 76.5 90.5 76.4opt 81.4?75.6 78.0 76.0 87.2 88.4 78.4 91.2 79.4Spvan 79.9 78.7 78.4 78.0 87.8 89.1 80.3 91.8 78.4opt 80.5 79.1?79.4?78.2?89.0?89.2?80.0?91.8?80.9?Bk multiple 87.9 82.9 84.5 81.3 89.5 91.9 84.3 87.8 84.9Cl multiple 83.4 80.4 82.7 80.4 89.2 89.9 80.3 92.4 82.8Hall et al ?14 83.4 79.7 - 78.4 87.2 88.3 80.2 90.7 82.0F&M ?15 85.9 78.8 - 78.7 89.0 88.2 79.3 91.2 82.8Crabb?e ?15 84.9 80.8 - 79.3 89.7 90.1 82.7 92.7 83.2Table 3: Results on the test datasets.
?Bk?
denotes the best Berkeley parser result reported by the shared task organizers(Seddah et al, 2013).
For the German-N data, Bk results are taken from Petrov (2010).
?Cl van?
shows the performance of thebest vanilla models from Table 2 on the test set.
?Cl opt?
and ?Sp opt?
give the result of our algorithm on the test set.
We alsoinclude results from Hall et al (2014), Crabb?e (2015) and Fern?andez-Gonz?alez and Martins (2015).80 models for each of noise induction mechanismsin Narayan and Cohen: Dropout, Gaussian (ad-ditive) and Gaussian (multiplicative).
To decodewith multiple noisy models, we train the MaxEntreranker of Charniak and Johnson (2005).10Hi-erarchical decoding with ?maximal tree coverage?over MaxEnt models, further improves our accu-racy.
See Narayan and Cohen (2015) for more de-tails on the estimation of a diverse set of models,and on decoding with them.estimates than the dense estimates of Cohen et al (2013).10Implementation: https://github.com/BLLIP/bllip-parser.
More specifically, we used theprograms extract-spfeatures, cvlm-lbfgs andbest-indices.
extract-spfeatures uses head fea-tures, we bypass this for the SPMRL datasets by creating adummy heads.cc file.
cvlm-lbfgs was used with thedefault hyperparameters from the Makefile.4.2 ResultsTable 2 and Table 3 give the results for the variouslanguages.11Our main focus is on comparing thecoarse-to-fine Berkeley parser (Petrov et al, 2006)to our method.
However, for the sake of com-pleteness, we also present results for other parsers,such as parsers of Hall et al (2014), Fern?andez-Gonz?alez and Martins (2015) and Crabb?e (2015).In line with Bj?orkelund et al (2013), our pre-liminary experiments with the treatment of rarewords suggest that morphological features areuseful for all SPMRL languages except French.Specifically, for Basque, Hungarian and Korean,improvements are significantly large.Our results show that the optimization of the11See more in http://cohort.inf.ed.ac.uk/lpcfg/.1552preterminals interminals alllanguage?ixi?iyidiv.
#nts?ixi?iyidiv.
#nts?ixi?iyidiv.
#ntsBasque 311 419 196 169 91 227 152 31 402 646 348 200French 839 715 476 108 1145 1279 906 114 1984 1994 1382 222German-N 425 567 416 109 323 578 361 99 748 1145 777 208German-T 1251 890 795 378 1037 1323 738 384 2288 2213 1533 762Hebrew 434 442 182 279 169 544 393 96 603 986 575 375Hungarian 457 415 282 87 186 261 129 25 643 676 411 112Korean 1077 980 547 331 218 220 150 21 1295 1200 697 352Polish 252 311 197 135 132 180 86 63 384 491 283 198Swedish 191 284 127 106 85 345 266 42 276 629 393 148Table 4: A comparison of the number of latent states for the different nonterminals before and after running our latent statenumber optimization algorithm.
The index i ranges over preterminals and interminals, with xidenoting the number of latentstates for nonterminal i with the vanilla version of the estimation algorithm and yidenoting the number of latent states fornonterminal i after running the optimization algorithm.
The divergence figure (?div.?)
is a calculation of?i|xi?
yi|.number of latent states with the clustering andspectral algorithms indeed improves these algo-rithms performance, and these increases general-ize to the test sets as well.
This was a pointof concern, since the optimization algorithm goesthrough many points in the hypothesis space ofparsing models, and identifies one that behaves op-timally on the development set ?
and as such itcould overfit to the development set.
However, thisdid not happen, and in some cases, the increase inaccuracy of the test set after running our optimiza-tion algorithm is actually larger than the one forthe development set.While the vanilla estimation algorithms (with-out latent state optimization) lag behind the Berke-ley parser for many of the languages, once thenumber of latent states is optimized, our parsingmodels do better for Basque, Hebrew, Hungar-ian, Korean, Polish and Swedish.
For German-T we perform close to the Berkeley parser (78.2vs.
78.3).
It is also interesting to compare theclustering algorithm of Narayan and Cohen (2015)to the spectral algorithm of Cohen et al (2013).In the vanilla version, the spectral algorithm doesbetter in most cases.
However, these differencesare narrowed, and in some cases, overcome, whenthe number of latent states is optimized.
Decod-ing with multiple models further improves our ac-curacy.
Our ?Cl multiple?
results lag behind ?Bkmultiple.?
We believe this is the result of the needof head features for the MaxEnt models.12Our results show that spectral learning isa viable alternative to the use of expectation-12Bj?orkelund et al (2013) also use the MaxEnt rarankerwith multiple models of the Berkeley parser, and in their casealso the performance after the raranking step is not alwayssignificantly better.
See footnote 10 on how we create dummyhead-features for our MaxEnt models.maximization coarse-to-fine techniques.
As wediscuss later, further improvements have been in-troduced to state-of-the-art parsers that are orthog-onal to the use of a specific estimation algorithm.Some of them can be applied to our setup.4.3 Further AnalysisIn addition to the basic set of parsing results, wealso wanted to inspect the size of the parsing mod-els when using the optimization algorithm in com-parison to the vanilla models.
Table 4 gives thisanalysis.
In this table, we see that in most cases,on average, the optimization algorithm chooses toenlarge the number of latent states.
However, forGerman-T and Korean, for example, the optimiza-tion algorithm actually chooses a smaller modelthan the original vanilla model.We further inspected the behavior of theoptimization algorithm for the preterminals inGerman-N, for which the optimal model chose (onaverage) a larger number of latent states.
Table 5describes this analysis.
We see that in most cases,the optimization algorithm chose to decrease thenumber of latent states for the various pretermi-nals, but in some cases significantly increases thenumber of latent states.13Our experiments dispel another ?common wis-dom?
about spectral learning and training datasize.
It has been believed that spectral learningdo not behave very well when small amounts ofdata are available (when compared to maximumlikelihood estimation algorithms such as EM) ?however we see that our results do better than theBerkeley parser for several languages with small13Interestingly, most of the punctuation symbols, such as$?LRB?, $.
and $,, drop their latent state number to a sig-nificantly lower value indicating that their interactions withother nonterminals in the tree are minimal.1553preterminal freq.
b. a. preterminal freq.
b. a. preterminal freq.
b. a. preterminal freq.
b. a.PWAT 64 2 2 TRUNC 614 8 1 PIS 1,628 8 8 KON 8,633 8 30XY 135 3 1 VAPP 363 6 4 $*LRB* 13,681 8 6 PPER 4,979 8 100NP|NN 88 2 1 PDS 988 8 8 ADJD 6,419 8 60 $.
17,699 8 3VMINF 177 3 5 AVP|ADV 211 4 11 KOUS 2,456 8 1 APPRART 6,217 8 15PTKA 162 3 1 FM 578 8 3 PIAT 1,061 8 8 ADJA 18,993 8 10VP|VVINF 409 6 2 VVIMP 76 2 1 NP|PPER 382 6 1 APPR 26,717 8 7PRELAT 94 2 1 KOUI 339 5 2 VVPP 5,005 8 20 VVFIN 13,444 8 3AP|ADJD 178 3 1 VAINF 1,024 8 1 PP|PROAV 174 3 1 $, 16,631 8 1APPO 89 2 2 PRELS 2,120 8 40 VAFIN 8,814 8 1 VVINF 4,382 8 10PWS 361 6 1 CARD 6,826 8 8 PTKNEG 1,884 8 8 ART 35,003 8 10KOKOM 800 8 37 NE 17,489 8 6 PTKZU 1,586 8 1 ADV 15,566 8 8VP|VVPP 844 8 5 PRF 2,158 8 1 VVIZU 479 7 1 PIDAT 1,254 8 20PWAV 689 8 1 PDAT 1,129 8 1 PPOSAT 2,295 8 6 NN 68,056 8 12APZR 134 3 2 PROAV 1,479 8 10 PTKVZ 1,864 8 3 VMFIN 3,177 8 1Table 5: A comparison of the number of latent states for each preterminal for the German-N model, before (?b.?)
running thelatent state number optimization algorithm and after running it (?a.?).
Note that some of the preterminals denote unary rulesthat were collapsed (the nonterminals in the chain are separated by |).
We do not show rare preterminals with b. and a. bothbeing 1.training datasets, such as Basque, Hebrew, Pol-ish and Hungarian.
The source of this commonwisdom is that ML estimators tend to be statis-tically ?efficient:?
they extract more informationfrom the data than spectral learning algorithms do.Indeed, there is no reason to believe that spectralalgorithms are statistically efficient.
However, it isnot clear that indeed for L-PCFGs with the EMalgorithm, the ML estimator is statistically effi-cient either.
MLE is statistically efficient underspecific assumptions which are not clearly satis-fied with L-PCFG estimation.
In addition, whenthe model is ?incorrect,?
(i.e.
when the data isnot sampled from L-PCFG, as we would expectfrom natural language treebank data), spectral al-gorithms could yield better results because theycan mimic a higher order model.
This can beunderstood through HMMs.
When estimating anHMM of a low order with data which was gener-ated from a higher order model, EM does quitepoorly.
However, if the number of latent states(and feature functions) is properly controlled withspectral algorithms, a spectral algorithm wouldlearn a ?product?
HMM, where the states in thelower order model are the product of states of ahigher order.14State-of-the-art parsers for the SPMRL datasetsimprove the Berkeley parser in ways which are or-thogonal to the use of the basic estimation algo-rithm and the method for optimizing the numberof latent states.
They include transformations ofthe treebanks such as with unary rules (Bj?orkelundet al, 2013), a more careful handling of unknownwords and better use of morphological informa-14For example, a trigram HMM can be reduced to a bigramHMM where the states are products of the original trigramHMM.tion such as decorating preterminals with such in-formation (Bj?orkelund et al, 2014; Sz?ant?o andFarkas, 2014), with careful feature specifications(Hall et al, 2014) and head-annotations (Crabb?e,2015), and other techniques.
Some of these tech-niques can be applied to our case.5 ConclusionWe demonstrated that a careful selection of thenumber of latent states in a latent-variable PCFGwith spectral estimation has a significant effecton the parsing accuracy of the L-PCFG.
We de-scribed a search procedure to do this kind ofoptimization, and described parsing results foreight languages (with nine datasets).
Our resultsdemonstrate that when comparing the expectation-maximization with coarse-to-fine techniques toour spectral algorithm with latent state optimiza-tion, spectral learning performs better on six of thedatasets.
Our results are comparable to other state-of-the-art results for these languages.
Using a di-verse set of models to parse these datasets furtherimproves the results.AcknowledgmentsThe authors would like to thank David McCloskyfor his help with running the BLLIP parser andhis comments on the paper and also the threeanonymous reviewers for their helpful comments.We also thank Eugene Charniak, DK Choe andGeoff Gordon for useful discussions.
Finally,thanks to Djam?e Seddah for providing us withthe SPMRL datasets and to Thomas M?uller andAnders Bj?orkelund for providing us the MarMotmodels.
This research was supported by an EP-SRC grant (EP/L02411X/1) and an EU H2020grant (688139/H2020-ICT-2015; SUMMA).1554ReferencesRapha?el Bailly, Amaury Habrard, and Franc?ois Denis.2010.
A spectral approach for probabilistic gram-matical inference on trees.
In Proceedings of Inter-national Conference on Algorithmic Learning The-ory.Anders Bj?orkelund,?Ozlem C?etino?glu, Rich?ard Farkas,Thomas M?ueller, and Wolfgang Seeker.
2013.
(Re)ranking meets morphosyntax: State-of-the-artresults from the SPMRL 2013 shared task.
In Pro-ceedings of the Fourth Workshop on Statistical Pars-ing of Morphologically-Rich Languages.Anders Bj?orkelund,?Ozlem C?etino?glu, AgnieszkaFale?nska, Rich?ard Farkas, Thomas M?uller, Wolf-gang Seeker, and Zsolt Sz?ant?o.
2014.
Introducingthe IMS-Wroc?aw-Szeged-CIS entry at the SPMRL2014 shared task: Reranking and morphosyntaxmeet unlabeled data.
In Proceedings of the FirstJoint Workshop on Statistical Parsing of Morpho-logically Rich Languages and Syntactic Analysis ofNon-Canonical Languages.Ezra W. Black, Steven Abney, Daniel P. Flickinger,Claudia Gdaniec, Ralph Grishman, Philip Harri-son, Donald Hindle, Robert J. P. Ingria, Freder-ick Jelinek, Judith L. Klavans, Mark Y. Liberman,Mitchell P. Marcus, Salim Roukos, Beatrice San-torini, and Tomek Strzalkowski.
1991.
A procedurefor quantitatively comparing the syntactic coverageof English grammars.
In Proceedings of DARPAWorkshop on Speech and Natural Language.Sabine Brants, Stefanie Dipper, Peter Eisenberg, Sil-via Hansen-Schirra, Esther K?onig, Wolfgang Lezius,Christian Rohrer, George Smith, and Hans Uszko-reit.
2004.
TIGER: Linguistic interpretation of aGerman corpus.
Research on Language and Com-putation, 2(4):597?620.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of ACL.Shay B. Cohen and Michael Collins.
2014.
A prov-ably correct learning algorithm for latent-variablePCFGs.
In Proceedings of ACL.Shay B. Cohen, Karl Stratos, Michael Collins, Dean F.Foster, and Lyle Ungar.
2012.
Spectral learning oflatent-variable PCFGs.
In Proceedings of ACL.Shay B. Cohen, Karl Stratos, Michael Collins, Dean P.Foster, and Lyle Ungar.
2013.
Experiments withspectral learning of latent-variable PCFGs.
In Pro-ceedings of NAACL.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania.Benoit Crabb?e.
2015.
Multilingual discriminative lex-icalized phrase structure parsing.
In Proceedings ofEMNLP.Daniel Fern?andez-Gonz?alez and Andr?e F. T. Martins.2015.
Parsing as reduction.
In Proceedings of ACL-IJCNLP.David Hall, Greg Durrett, and Dan Klein.
2014.
Lessgrammar, more features.
In Proceedings of ACL.Daniel Hsu, Sham M. Kakade, and Tong Zhang.
2009.A spectral algorithm for learning hidden Markovmodels.
In Proceedings of COLT.Andr?e F. T. Martins, Noah A. Smith, Eric P. Xing,M?ario A. T. Figueiredo, and Pedro M. Q. Aguiar.2010.
TurboParsers: Dependency parsing by ap-proximate variational inference.
In Proceedings ofEMNLP.Takuya Matsuzaki, Yusuke Miyao, and Junichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InProceedings of ACL.Thomas M?ueller, Helmut Schmid, and HinrichSch?utze.
2013.
Efficient higher-order CRFs formorphological tagging.
In Proceedings of EMNLP.Shashi Narayan and Shay B. Cohen.
2015.
Diversityin spectral learning for natural language parsing.
InProceedings of EMNLP.Ankur P. Parikh, Le Song, Mariya Ishteva, GabiTeodoru, and Eric P. Xing.
2012.
A spectral al-gorithm for latent junction trees.
In Proceedings ofthe Twenty-Eighth Conference on Uncertainty in Ar-tificial Intelligence.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofCOLING-ACL.Slav Petrov.
2010.
Products of random latent variablegrammars.
In Proceedings of HLT-NAACL.Detlef Prescher.
2005.
Head-driven PCFGs withlatent-head statistics.
In Proceedings of IWPT.Guillaume Rabusseau, Borja Balle, and Shay B. Cohen.2016.
Low-rank approximation of weighted tree au-tomata.
In Proceedings of The 19th InternationalConference on Artificial Intelligence and Statistics.Djam?e Seddah, Reut Tsarfaty, Sandra K?ubler, MarieCandito, Jinho D. Choi, Rich?ard Farkas, Jen-nifer Foster, Iakes Goenaga, Koldo Gojenola Gal-letebeitia, Yoav Goldberg, Spence Green, NizarHabash, Marco Kuhlmann, Wolfgang Maier, JoakimNivre, Adam Przepi?orkowski, Ryan Roth, WolfgangSeeker, Yannick Versley, Veronika Vincze, MarcinWoli?nski, Alina Wr?oblewska, and Eric Villemontede la Cl?ergerie.
2013.
Overview of the SPMRL2013 shared task: A cross-framework evaluation ofparsing morphologically rich languages.
In Pro-ceedings of the Fourth Workshop on Statistical Pars-ing of Morphologically-Rich Languages.1555Wojciech Skut, Brigitte Krenn, Thorsten Brants, andHans Uszkoreit.
1997.
An annotation scheme forfree word order languages.
In Proceedings of ANLP.Zsolt Sz?ant?o and Rich?ard Farkas.
2014.
Special tech-niques for constituent parsing of morphologicallyrich languages.
In Proceedings of EACL.1556
