An Empirical Evaluation of LFG-DOPRens BodInfonnatics Research Institute, University of Leeds, Leeds LS2 9JT, &Institute for Logic, Language and Computation, University of Amsterdammns@scs.leeds.ac.ukAbst ractThis paper presents an empirical assessment of the LFG-DOP model introduced by Bed & Kaplan (1998).
Theparser we describe uses fragments l'rom LFG-aunotatedsentences to parse new sentences and Monte Carlotechniques to compute the most probable parse.
Whileour main goal is to test Bed & Kaplan's model, we willalso test a version of LFG-DOP which treats generalizedfragments as previously unseen events.
Experiments withthe Verbmobil and Itomecentre corpora show that ourversion of LFG-DOP outperforms Bed & Kaplan'smodel, and that LFG's functional information improvesthe parse accuracy of Iree structures.1 IntroductionWe present an empirical ewduation of the LFG-DOPmodel introduced by Bed & Kaplan (1998).
LFG-DOP isa Data-Oriented Parsing (DOP) model (Bed 1993, 98)based on the syntactic representations of Lexical-Functional Grammar (Kaplan & Bresnan 1982).
A DOPmodel provides linguistic representations lot- an tmlimit-cd set of sentences by generalizing from a given corpttsof annotated exemphu's, it operates by decomposing thegiven representations into (arbitrarily large) fi'agmentsand recomposing those pieces to analyze new sentences.The occurrence-frequencies of the fragments are used todetermine the most probable analysis of a sentence.So far, DOP models have been implelnented forphrase-structure trees and logical-semantic represent-ations (cf.
Bed 1993, 98; Sima'an 1995, 99; Bonnema elal.
1997; Goodman 1998).
However, these DOP modelsare limited in that they cannot accotmt for underlyingsyntactic and semantic dependencies that are notreflected directly in a surface tree.
DOP models for anumber of richer representations have been explored(van den Berg et al 1994; Tugwell 1995), but theseapproaches have remained context-free in theirgenerative power.
In contrast, Lexical-FunctionalGrammar (Kaplan & Bresnan 1982) is known to bebeyond context-free.
In Bed & Kaplan (1998), a firstDOP model was proposed based on representationsdefined by LFG theory ("LFG-DOP").
I This model wasI DOP models have recently also been proposed for Tree-Adjoining Grammar and Head-driven Phrase StructureGrammar (cf.
Neumann & Flickinger 1999).studied fi'om a mathematical perspective by Cormons(1999) who also accomplished a first simple experinacntwith LFG-DOP.
Next, Way (1999) studied LFG-DOP asan architecture for machine translation.
The currentpaper contains tile first extensive empMeal evaluationof LFG-DOP on the currently available LFG-annotatcdcorpora: the Verbmobil corpus and the Itomecentrecorpus.
Both corpora were annotated at Xerox PARC.Out" parser uses fragments from LFG-annotatedsentences to parse new sentences, and Monte Carlolechniques to compute the most probable parse.Although our main goal is to lest Bed & Kaplan's LFG-l)OP model, we will also test a modified version o1'LFG-DOP which uses a different model for computingfragment probabilities.
While Bed & Kaplan treat allfragments probabil istically equal regardless whetherthey contain generalized features, we will propose amore fine-grained probabil ity model which treatsfragments with generalized features as previouslyunseen events and assigns probabil i t ies to thesefi'agments by means of discotmting.
The experimentsindicate that our probability model outperforms Bed &Kaplan's probabil ity model on the Verbmobil andHomecentre corpora.The rest of this paper is organized as follows:we first summarize the LFG-DOP model and go into ourproposed extension.
Next, we explain the Monte Carloparsing technique for estimating lhe most probable LFG-parse o1' a sentence.
In section 3, we test our parser onsentences from the LFG-annotated corpora.2 Summary of LFG-DOP and an ExtensionIn accordance with Bed (1998), a particular DOP modelis described by specifying settings for the following fourparameters:?
a formal definition of a well-formed representation fortltlcl'allcc (lllalys~s,?
a set of decomposition operations that divide a givenutterance analysis into a set of.fragments,?
a set of composition operations by which suchfragments may be recombined to derive an analysis of anew utterance, and?
a probabili O, model that indicates how the probabilityof a new utterance analysis is computed.62In defining a l)OP model for l .exicaI-FuuctionalGrammar representations, Bed & Kaphm (1998) givethe following sctlings for l)OP's four parameters.2.1 RepresentationsThe representations u ed by LFG-I)OP are direclly takenfrom LFG: they consist of a c-structure, an f-strt|ett|reand a mapping q~ between them (sue Kaplan & thesnan1982).
The fol lowing figure shows an examplerepresentation for the utterance Kim eats.
(We leave outsomK features to keep the example simple.
)~/~~ I"RH' 'Kim' l" K im caL~; l'Rlil) 'L I{(,SUIH) 'Figttt'c I.
A representation Ibr Kim eat.~Bed & Kaphm also introduce the notion of accessibilitywhich they later use for defining the decompositionoperations of LFG-DOP:An f-struc{t|re unit./'is ?p-accessible f|om a node n iffeither ;t is ~l)-Iinked to j' (that is, f=  ~l)(;;) ) or./' iscontained within d)(n) (that is, there is a chain (31atlribu(es that leads from qb(n) Iof).According to tire IA;G representation theory, c-st|'ucturesand f-structures must salisfy certain fo|n|al well-formedness conditions.
A c-strt|cture/f-structu|e pair is aval id LFG represcntalion only if it satisfies theNonbranching Dominance, UniquelleSs, Coherence andCompleteness conditions (see Kaplan & Bresnan 1982).2.2 Decomposition operations and FragmelflSThe l'x'agmellts for LFG-I)OP consist (3t" connectedsublrees whose nodes are in <l~-correslxmdence with Ihecor|'epondi|lg sul>units of f-structures.
To give a precisedefinition of L1;G-I)OP fragments, it is convenient orecall the decomposition operations employed by thesimpler "Tree-l)OP" model which is based on phrase-structt|,'e treks only (Bed 1998):(I) Root: the Root operation selects any node of a trekto be the root o1' the new subtree and erases allnodes except the selected node and the nodes itdominates.
(2) Frontier: the Frontier operation then chooses a set(possibly en|pty) of nodes in tile new subtreedifferent from its ,'oot and erases all subtreesdominated by the chosen nodes.Bed & Kaplau extend Tree-1)OP's Root and Frontieroperations o that they also apply to the nodes of the c-structure in L1;G, while respecting the fundamentalprinciples of c-structure/f-structure correspotKlence.When a node is selected by the Root operation,all nodes outside of that node's subtree are erased, jttstas in Tree-DOP.
Further, I'or LFG-DOP, all d?
l inksleaving the erased nodes arc removed and all f-structuretraits that are not (~-accessible from the remaining nodesare erased.
For example, if Root selects the NP in figure1, then the f-structure corresponding lo the S node iserased, giving figure 2 as a possible fl'agment:lqgurc 2.
An LFG-DOI ~ fragment obtained by RootIn addition tile Root  Ol)eratioll deletes from theremaining f-st,ucturo all semantic forlns lhat are local tof-structu|'es thai correspond to el'used c-slructure nodes,and it thereby also maintains the fundamental two-wayconnection between words and meanings.
Thus, if Rootselects the VP node so thai the NP is erased, the subjectsemantic form "Kim" is also deleted:i fcats  I'RI;I) 'Ca|(SUIH)'Figure 3.
Another I.FG-I)O1 > fragmentAs with Tree-1)OP, the Frontier operation thel~ selects aset of f,outier nodes and deletes all subtrees theydominate.
Like Root, it also removes Ihe q~ links of thedeleted nodes and erases any semantic form thatcOlleSpolldS to ~.llly (3|" those nodes.
Frontier does notdelete any other f-structure fealures, however.
Forinstance, if tire NP in figure I is sclec{ed as a l't'onlim"node, Frontier erases the predicate "Kim" from thefragment:-PI(I~I) 'eat(SUll J) '  C~IISFigure 4.
A I:ronlier-gcneratcd fragmentFinally, Bed & Kaplan present a third decompositionoperation, Discard, defined to conslruct generalizationsof the fragments upplied by Root and Frontier.
Discardacts to delete combinations of altrilmte-value pairssubject Io Ihe following condition: Discard does notdelete pairs whose values ~-correspond to relnaining c-63structure nodes.
Discard produces fragments uch as infigure 5, where the subject's number in figure 3 has beendeleted:eatsSUItJ \[ \]'l'liNSli PRt!SI'Rlil) 'eat(suB J)'Figure 5.
A l)iscard-generated lYagment2.3 The composition operationIn LFG-DOP the operation for combining fragments,indicated by o, is carried out in two steps.
First the c-structures are combined by left-most substitution subjectto the category-matching condition, .just as in Tree-DOP(cf.
Bed 1993, 98).
This is followed by the recursiveunil'ication of the f-structures corresponding to thematching nodes.
A derivation for an LFG-DOPrepresentation R is a sequence o1' fragments the first ofwhich is labeled with S and for which the itcrativeapplication of the composition operation produces R.The two-stage composit ion operation isillustrated by a simple example.
We therefore assume acorpus containing the representation i  figure 1 for thesentence Kim eats  and the representation i  figure 6 forthe sentence John fell.?
: I I'RH,'J?h.'
\]li'17 :7: n 'fall(SUl~l)'Figure 6.
Corpus represeutation for John fellFigure 7 shows the effect of the LFG-DOP compositionoperation using two fragments from this corpus, resultingin a representation for the new sentence Kim fell.I'RID 'Killl' =fell \[ PRI:,) 'fl~H(SUIJJ)' iStJBJ PRED 'K\[III' 11- Kim fi.er PRI{')  f Ill(SUB J)Figure 7.
Illustration of the composition operationThis representation satisfies the well- formednessconditions and is therefore valid.
Note that the sentenceKim fell can be parsed by fragments that are generatedby the decomposition operations Root and Frontier only,without using generalized fragments (i.e.
fragmentsgenerated by tile Discard operation).
Bed & Kaplan(1998) call a sentence "granunatical with respect o acorpus" if it call be parsed without general izedfragments.
Generalized fragments are needed ouly toparse sentences that a,e "ungrammatical with respect othe corpus".2.4 Probability modelsAs in Tree-DOP, an LFG-DOP representation R cantypically be derived in many different ways.
If eachderivation D has a probability P(D), then the probabilityof deriving R is the sum of the individual derivationprobabilities, as shown in (1):(1) P(R) = ~I.)
derives R P(D)An LFG-DOP derivation is produced by a stochasticprocess which starts by randomly choosing a fraglnentwhose c-structure is labeled with the initial category(e.g.
S).
At each subsequent step, a next fragment ischosen at random from among the fragments that can becomposed with the current subanalysis.
The chosenfragment is composed with the current subanalysis toproduce a new one; the process stops when an analysisresults with no non-te,'minal leaves.
We will call the setof composable fragments at a certain step in thestochastic process the competition set at that step.
LetCP(f l  CS) denote the probability of choosing a fragmentf f rom a competition set CS containing J; then theprobability of a derivation D = <fJ,f2 ...Jk> isC2) P(<ag,f, ...fk>) = H i  cpq} I csi)where the compet i t io ,  l~robability CP0el CS) is ex-pressed in terms of fragment probabilities Pq):(3) CP(f I CS) - PCt)Z,,,~ cs P(/+')Bed & Kaplan give three definitions of increasingcomplexity for the competition set: the first definitiongroups all fi'agments that only satisfy the Category-matching condition o1' the composition operation (thusleaving out the Uniqueness, Coherence and Complete-uess conditions); the second definition groups allfragments which satisfy both Category-matching andUniqueness; and the third defi,fition groups all fragmentswhich satisfy Category-matching, Uniqueness andCoherence.
Bed & Kaplan point out that theCompleteness condition cannot be enforced at each stepof the stochastic derivation process.
It is a property ofthe final representation which can only be enforced bysampling valid representations from the outpt, t of thestochastic process.In this paper, we will only deal with the thirddefinition of competition set, as it selects only those64l'ragments at each derivation step that may finally restdtin a valid LFG representation, lhus reducing lhe off-linevalidity checking just to the Completeness condition.Notice that the computation o1' the competitionprobability in (3) still reqttires a del:inition for thefragment probabil ity P(f).
Bed & Kaplan define theprobability of a fragment simply as its relative frequencyin the bag of all fragments generated from the corpus.Thus Bed & Kaplan do not distinguish betweenRoot~Frontier-generated l 'ragments a11d Discard-generated l'ragments, the latter being generalizationsover Root~Frontier-generated fragments.
Although Bed& Kaplan illustrate with a simple example that theirprobability model exhibits a preference for the mostspecific representation containing the fewest featuregeneralizations (mainly because specific representationstend to have more derivations than general izedrepresentations), they do not perform an empiricalevaluation el' lheir model.
We will assess their model onthe LFG-annotated Verbmobil and Itomecentre corporain section 3 of this paper.However, we will also assess an alternativedefinition of fragment probability which is a refinementof Bed & Kaplan's model.
This definition doesdistinguish between fragments upplied by Root~Frontierand fragments upplied by Discard.
We will treat thefirst type el' fragments as seen events, and the secondtype of fragments as previously unseen events.
We thuscreate two separate bags corresponding to two separatedistributions: a bag with l'ragments generated by Rootand Frontier, and a bag with l'ragments generated byDiscard.
We assign probability mass to the fragments ofeach bag by means of discounting: the relativel'requencies of seen events are discounted and thegained probability mass is reserved for Ihe bag el' unseenevents (cf.
Ney et al 1997).
We accolnplish lhis by avery simple estimalor: lhe Turing-Good estimator (Good1953) which computes lhe probability mass of unseenevents as n l /N  where n I is the ,mmber of singletonevents and N is the total number of seen events.
Thisprobability mass is assigned to the bag o1' Discard-generated fragments.
The remaining mass (1 -n l /N) i sassigned to the bag o1' Root~Front ier-generatedl 'ragments.
Thus tim total probabi l i ty mass isredistributed over tim seen and unseen fragments.
Theprobability of each l'ragment is then computed as itsrelative frequency 2 in its bag multiplied by the prol)a-bility mass assigned to this bag.
Let Ill denote thefrequency of a fragment f, then its probability is givenby:2 Bed (2000) discusses ome alternative fragment probabilityestimators, e.g.
based on maximum likelihood.
(4) P(/'ll'is generated byRootlFrontier) =( I  - n i /N)Ifl"~-"J': .fis generated by Rool/Fro,,tier I:'l(5) PUlJis generated byDiscard)  =Ill 0q/N)"~'f:fisgene,'aledby Discard I.rlNote that this probability model assigns less probabilitymass to Discard-generated fragments than Bed &Kaphm's model.
For each Root~Frontier-generatedfragment there are exponential ly many Discard-generated fragments (exponential in the number offeatures the fragment contains), which means that inBed & Kaphm's model the Discard-generated IYagnaentsabsorb a vast amount of probability mass.
Our model, onthe other hand, assigns a fixed probability mass to thedistribution of Discard-generated l 'ragments andlherefore the exponential explosion of these fi'agmentsdoes not affect the probabilities of Root~Frontier-generated fragments.3 Testing tile LFG-DOP model3.1 Computing tile most probable analysisIn his PhD-thesis, C(nmous (1999) describes a parsingalgorithnl for I~FG-DOP which is based on tile Tree-DOPparsing teclmique given in Bed (1998).
Cormons firstconverts LFG-representat ions into more compactindexed lrees: each node in the c-structure is assignedan index which refers to the ~-c(wresponding f-struclureunit.
For example, the rcpresentalion in figure 6 isindexed as(S. 1 (NP.2 John.2)(VP.
1 fell.
1 ))where1 --> \[ (SUB J=2)(TENSE = PAST)(PRED = fall(SUBJ)) \]2 --> \[ (PRED = John)(NUM = SG) \]The indexed trees are then fragmented by applying theTree-DOP decomposition operations described in section2.
Next, the LFG-DOP decomposition operations Root,Frontier and Discard are applied to the f-structure unitsthat correspond to the indices in the e-structure subtrees.ltaving obtained the set of LFG-DOP fragments in thisway, each test sentence is parsed by a bottom-up chartparser using initially the indexed subtrees only.
Thusonly the Category-matching condition is enforced during65the chart-parsing process.
Tile Uniqueness andCoherence conditions of the corresponding f-structureunits are enforced during tile disambiguation (or chart-decoding) process.
Disambiguation is accomplished bycomputing a large number o1' random derivations fromthe chart; this technique is known as "Monte Carlodisambiguation" and has been extensively described illthe literature (e.g.
Bed 1998; Chappelier & Rajman1998; Goodman 1998).
Sampling a random deriwttionl'rom the chart consists of choosing at random one o1' thefragments fi'om the sel of composable fragments at everylabeled chart-entry (ill a top-down, leftmost order so asto maintain (he LFG-DOP derivation order).
Thus thecompetition set of composable fragments is computedon the fly at each derivation step during the MonteCarlo sampling process by grouping the f-structure unitsthat unify and that are coherent with the subderivationbuilt so far.As mentioned in 2.4, the Completenesscondition can only be checked after the derivationprocess.
Incomplete derivations are simply removedfrom the sampling distribution.
After sampling a largenumber of random derivations that satisfy the LFGvalidity requirements, the most probable analysis isestimated by the analysis which results most often fromthe sampled erivations.
For our experiments in section3.2, we used a sample size of N = 10,000 derivationswhich corresponds to a maximal standard error o" of0.005 (o_< 1/(2~/N), see Bed 1998).3.2 Experiments with LFG-DOPWe tested LFG-DOP on two LFG-anuotated corpora: theVerbmobil corpus, which contains appointment planningdialogues, and the Homecentre corpus, which containsXerox printer documentation.
Both corpora have beenannotated by Xerox PARC.
They contain packed LFG-representations (Maxwell & Kaplan 1991) of thegrammatical parses of each sentence together with anindication which of these parses is the correct one.
Theparses are represented in a binary form and weredebinarized using software provided to us by XeroxPARC.
3 For our experiments we only used the correctparses of each sentence resulting ill 540 Verbmobilparses and 980 Homecentre parses.
Each corpus wasdivided into a 90% trai,ting set and a 10% test set.
Thisdivision was random except for one constraint: hat allthe words ill the test set actually occurred in the trainingset.
The sentences from the test set were parsed anddisambiguated by means of the fragments from thetraining set.
Due to memory limitations, we limited thedepth of the indexed subtrees to 4.
Because of the small3 Thanks to Hadar Shemtov for providing us wi~.h the relevantsoftware.size of the corpora we averaged our results on 10different raining/test et splits.
Besides an exact matchaccuracy metric, we also used a more fine-grainedmetric based ell the well-known PARSEVAL metricsthat evaluate phrase-structure tr es (Black et al 1991).The PARSEVAL metrics compare a proposed parse Pwith the corresponding correct treebank parse 7" asfollows:# correct constituents in1'Precision =# constituents inP# correct constituents in PRecall =# constituents in 7In order to apply these metrics to LFG analyses, weextend the PARSEVAL notion of "correct constituent" inthe following way: a constituent in P is correct if thereexists a constituent ill T of the same label that spans thesame words and that ?l)-corresponds to the same f-structure unit.We illustrate the evaluation metrics with asimple example.
In the next figure, a proposed parse P iscompared with the correct parse T for the test sentenceKim fell .
The proposed parse is incorrect since it has theincorrect feature value for the TENSE attribute.
Thus, il'this were the only test sentence, the exact match wouldbe 0%.
The precision, on the other hand, is higher than0% as it compares the parse on a constiluent basis.
Boththe proposed parse and the correct parse contain threeconstituents: S, NP and VP.
While all three constituentsill P have the same label and span the same words as inT, only the NP constituent in P also maps to the same f-structure unit as ill T. The precision is thus equal to 1/3.Note that in this example the recall is equal to theprecision, but this need not always be the case., SIIIIJ \[ PI~EI)'KiI11'J1- Wun fell" PRI!')
'fall(SUllJ)'Proposed parse PCorrect parse 7"Ill out" expm'iments we are first of all interested illcomparing the performance of Bed & Kaplan'sprobability model against our probability model (itsexplained ill section 2.4).
Moreover, we also want to66study tile contribution of l)iscard-gelmrated fragments tothe parse accuracy.
We therefore created for eachtraining sol two sets of fragments: one which containsall fragments (up to depth 4) and one which excludesthe fragmenls generated by Discard.
The exclusion ofthe \])iscard-generated fragments means lhat allprolmbility mass goes to tile fraglnents generated byRoot and I"rot+lier in which case our model is equivalentto Bed & Kaplan's.
The following two tables present heresults of our experiments where +l)iscard refers to lilt+'full set of fragments and -Discard refers to the fragmentset without Discard-generated fragments.Exact Match l'lecision Recall+I)iscad-I)iscard +Discard-Discard +I)iscard-l)iscafdI~ud&Kaplam98 I.I+,4 3525{ 13.89~ 76.0~,{ 11.5~,~ 7-1.95{OurModcl 35.9~A 35.2rA 77.YA 76.0cA 76.,F,4 74.9c/~Table I. l{xperimental results on tile Verbmobil colpuslixact Match Ihvcisi0n Recall+Discard q)iscm,I +l)iscmxl q)iscm'd 4I)iscm,I q)iscm'dl~0d&Kaplan98 2.7~{ 37.9g 17.15i 77.85{ 15.5+,~ 77.2gOurM0del 38.4~/~ 37.9c7< 80.05~ 77.85{ 78.65{ 77.2gTable 2. l+,xperiincntal ,esults on the l lomccentre corpusThe tables show that Bed & Kaplan's model scoresexlremely bad if all fragments are used: the exacl lnatchis only 1.1% on tile Verbmobil corptts alld 2.7% on thel \ ]o l l lecentre corpus,  whereas our model  scoresrespectively 35.9% and 38.4% on these corpora.
Also thenlore fine-grained precision and recall scores of P, od &Kaplan's model are quite low: e.g.
13.8% and 11.5% onthe Verbmobil corpus, where our tnodel obtains 77.5%and 76.4%.
We l'()ulld ()tit Ihat even for the few lestsenlences that occur literally in tile training set, Bed &Kaplan's model does not always generate tile correctanalysis, whereas our model does.
Inlerestingly, lheaccuracy of P, od & Kaphm's model is much higher ifDiscard-generated fragments are excluded.
This suggeststhat treating generalized fragments l)robabilisl,ically inthe same way as ungeueralized fragments ix ha,mful.Connons (1999) has made a mathematical obserwllionwhich also shows that generalized fragments can get toomuch probability mass.The tables also show l,hat Otll + way ()f assigningprobabilities to Discard-generated fragments leads onlyIo a sl ight accuracy increase (compared to tileexperiments ill which l)iscard-generated fragments a,'eexcluded).
According to paired t-testing none of thesediffefences ill accuracy were statistically signil icant.This suggests that Discard-goner;lied fragtnents do notsignificanlly conl,'ibute to tile parse accuracy, or thatperhaps these fragments are too ntllllerous to be reliablyestimated Ol1 the basis of our small corpora.
We alsovaried the f~lolmbilil,y mass assigned to Discard-generated llaglllenls: eXCel)l, for very small (_< 0 .01)orlarge values (_>.0.88), which led to an accuracydecrease, there was no significant change.
4It is difficult to say how good or bad our resultsare with respect to other approaches.
The only otherpublished results on tile LFG-annotated Verbmobil andllomecetll,re corpora are by Johnson el, al.
(1999) andJohnson & P, iezler (2000) who use a log-linear model loestimale probabilities.
But while we first parse the lestsentences with l'ragnlenls froln tile training set andsul)sequently c()tnpule the IllOSl, pr()bal)le parse, Jollns()llcl al.
directly use the packed lJ+G-representations fromlhe lest set to select the most probable parse, Ihetel)ycompletely skipping the parsing phase (Mark Johnson,p.o.).
Moreover,  42% of the Verbmobil sentences and51% of the l lomecentre sentences arc unaml+iguous (i.e.their packed IAVG-representations contain only oneanalysis), which makes J()hns<.m et als lask completelytrivial for these sentences.
In our apl)foaeh, all toslSelltences wefe alllbiguous, Iestdling i l l  :.l I/IUC\]I nlol'edifficult lask.
A quantitative comparison between ourmodel and Johnson el, al.
's is therefore meaningless.l: inally, we are interested in the impact offunct ional  structures on predict ing lhe co l ree lconstituet~t structures.
We therefore removed all f-structure ut/ils from the fragtnetll,s (Ihus yielding a Trce-1)O1' model) and compared the results against ourversion of LFG-I)() I  ~ (which inclttde tile Discard-generated fraglnenls).
We ewlluated tile parse accuracyon tile l,ree-slructures only, using exact match togetherwith tile PARSIr+VAI, measures.
We used tile sametraining/test set splits as in the previot, s experiments andlimited tile maximum sublree depth again to 4.
Thelollowing tables show the results.Exact Match Precision P.ccallTrcc-I)Ol ~ 46.6% 88.9% 86.7%LFG-I)OI' 50.8% 90.3% 88A%Table 3.
C-structure accuracy on the Verbmobil1 Although generalized tfagnlcnls thus seem statisticallyunimportant fen these cm-pora, they rclnail~ ilnportant forparsing ungrammatical sentences (which was the originalmotivation for including them -- see Bed & Kaplan 1998).67Exact Match Precision RecallTree-DOP 49.0% 93.4% 92.1%LFG-DOP 53.2% 95.8% 94.7%Table 4.
C-structure accuracy on the HomecentreThe results indicate that LFG-DOP's functionalstructures help to improve the parse accuracy of tree-structures.
In other words, LFG-DOP outperforms Tree-DOP if evaluated on tree-structures only.
According topaired t-tests the differences in accuracy were statis-tically significant.4 ConclusionWe have given an empirical assessment of the LFG-DOP model introduced by Bed & Kaplan (1998).
Wedeveloped a new probability model for LFG-DOP whichtreats fragments with generalized features as previouslyunseen events.
The experiments howed that ourprobability model outperforms Bed & Kaplan's model onthe Verbmobil and Homeceutre corpora.
Moreover, Bed& Kaplan's model turned out to be inadequate in dealingwith generalized fragments.
We also established that thecontribution of generalized fragments to the parseaccuracy in our model is minimal and statisticallyinsignil'icant.
Finally, we showed that LFG's l'unctionalstructures contribute to significantly higher parseaccuracy on tree structures.
This suggests that our modelmay be successfully used to exploit the functionalannotations in the Penn Treebank (Marcus et al 1994),provided that these annotations can be converted intoLFG-style l'unctional structures.
As future research, wewant to test LFG-DOP using log-linear models, as suchmodels maximize the likelihood o1' the traiuing corpus.ReferencesM.
van den Berg, R. Bed and R. Scha, 1994.
"A Corpus-BasedApproach to Semantic Interpretation", ProceedingsNinth Amsterdam Colloquium, Amsterdam, TheNetherlands.E.
Black et al, 1991.
"A Procedure for QuantitativelyComparing the Syntactic Coverage of English",Proceedings DARPA Speech and Natural LanguageWorkshop, Pacific Grove, Morgan Kaufinann.R.
Bed, 1993.
"Using an Annotated Language Corpus as aVirtual Stochastic Grammar", Proceedings AAAl'93,Washington D.C.R.
Bed, 1998.
Beyond Grammar, CSLI Publications, CambridgeUniversity Press.R.
Bed, 2000.
"Parsing with the Shortest t)erivation", Proceed-ings COLING-2000, Saarbrticken, Gerlnany.R.
Bed and R. Kaplan, 1998.
"A Probabilistic Corpus-l)rivenModel for Lexical Functional Analysis", ProceedingsCOLING-ACL'98, Montreal, Canada.R.
Botmema, R. Bed and R. Scha, 1997.
"A DOP Model forSemantic Interpretation", Proceedings AClJEACL-97,Madrid, Spain.J.
Chal~pelier and M. P, ajman, 1998.
"Extraction stochastiqued'arbres d'analyse pour le mod61e DOP", ProceedingsTALN'98, Paris, France.B.
Cormons, 1999.
Analyse t dd.~ambiguisation: U e approche hbase de corpus (Data-Oriented l'arsing) pour lesr@resentations lexicales fonctionnelles.
Phi) thesis,Universit6 de Rennes, France.I.
Good, 1953.
"The Population Frequencies of Species and theEstimation of Population Parameters", Biometrika 40,237-264.J.
Goodman, 1998.
Palwing Inside-Out, PhD thesis, HarwudUniversity, Mass.M.
Johnson, S. Geman, S. Canon, Z. Chi and S. Pdezler, 1999.
"Estimators for Stochastic Unification-Based Gram-mars", Proceedings ACL'99, Maryland.M.
Johnson and S. Riezler, 2000.
"Exploiting AuxiliaryI)istributions in Stochastic Unification-Based Gram-mars", Proceedings ANLP-NAACL-2000, Seattle,Washington.R.
Kaplan, and J. Bresnan, 1982.
"Lexical-FunctionalGrammar: A Formal System for Grammaticall~,epresentation", in J. Bresnan (ed.
), The MentalRepresentation of Grammatical Relations, The MITPress, Cambridge, Mass.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre, A. Bies,M.
Ferguson, K. Katz and B. Schasberger, 1994.
"ThePenn Treebank: Annotating Predicate ArgumentStructure".
In: ARPA Human Language TechnologyWorkshop, I I 0-115.J.
Maxwell and R. Kaplan, 1991.
"A Method for DisjunctiveConstraint Satisfaction", in M. Tomita (ed.
), Currentlssttes in Parsing Technology, Kluwer AcademicPublishers.G.
Neumann and D. Flickinger, 1999.
"Learning StochasticLexicalized Tree Grammars from HPSG", I)FKITechnical Report, Saarbrackcn, Germany.H.
Ney, S. Martin and F. Wessel, 1997.
"Statistical LanguageModeling Using Leaving-One-Out", in S. Young & G.Bloothooft (eds.
), Corpus-Based Methods in Languageand Speech Processing, Kluwer Academic Publishers.K.
Sima'an, 1995.
"An optimized algorithm for Data OrientedParsing", in R. Mitkov and N. Nicolov (cds.
), RecentAdvances in Natural Language Plvcessing 1995, vohune136 of Current Issues in Linguistic Theot3,.
JohnBenjamins, Amsterdam.K.
Sima'an, 1999.
Learning E\[ficient Disambiguation.
PhDthesis, ILLC dissertation series number 1999-02.Utrecht / Amsterdam.D.
Tugwell, 1995.
"A State-Transition Grammar for l)ata-Oriented Parsing", PIwceedings Ettropean Chapter of theACL95, Dublin, Ireland.A.
Way, 1999.
"A Hybrid Architecture for Robust MT usingLFG-DOP", Journal of Experimental and 77teoreticalArtificial Intelligence I I (Special Issue on Memory-Based Language Processing)68
