A Language Independent Algorithm forSingle and Multiple Document SummarizationRada Mihalcea and Paul TarauDepartment of Computer Science and EngineeringUniversity of North Texas{rada,tarau}@cs.unt.eduAbstractThis paper describes a method for lan-guage independent extractive summariza-tion that relies on iterative graph-basedranking algorithms.
Through evalua-tions performed on a single-documentsummarization task for English and Por-tuguese, we show that the method per-forms equally well regardless of the lan-guage.
Moreover, we show how a meta-summarizer relying on a layered appli-cation of techniques for single-documentsummarization can be turned into an ef-fective method for multi-document sum-marization.1 IntroductionAlgorithms for extractive summarization are typi-cally based on techniques for sentence extraction,and attempt to identify the set of sentences that aremost important for the overall understanding of agiven document.
Some of the most successful ap-proaches consist of supervised algorithms that at-tempt to learn what makes a good summary bytraining on collections of summaries built for a rela-tively large number of training documents, e.g.
(Hi-rao et al, 2002), (Teufel and Moens, 1997).
How-ever, the price paid for the high performance ofsuch supervised algorithms is their inability to eas-ily adapt to new languages or domains, as new train-ing data are required for each new data type.
Inthis paper, we show that a method for extractivesummarization relying on iterative graph-based al-gorithms, as previously proposed in (Mihalcea andTarau, 2004) can be applied to the summarizationof documents in different languages without any re-quirements for additional data.
Additionally, wealso show that a layered application of this single-document summarization method can result into anefficient multi-document summarization tool.Earlier experiments with graph-based ranking al-gorithms for text summarization, as previously re-ported in (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004), were either limited to single-document English summarization, or they were ap-plied to English multi-document summarization,but in conjunction with other extractive summariza-tion techniques that did not allow for a clear evalua-tion of the impact of the graph algorithms alone.
Inthis paper, we show that a method exclusively basedon graph-based algorithms can be successfully ap-plied to the summarization of single and multipledocuments in any language, and show that the re-sults are competitive with those of state-of-the-artsummarization systems.The paper is organized as follows.
Section 2briefly overviews two iterative graph-based rankingalgorithms, and shows how these algorithms can beapplied to single and multiple document summa-rization.
Section 3 describes the data sets used inthe summarization experiments and the evaluationmethodology.
Experimental results are presented inSection 4, followed by discussions, pointers to re-lated work, and conclusions.2 Iterative Graph-based Algorithms forExtractive SummarizationIn this section, we shortly describe two graph-basedranking algorithms and their application to the taskof extractive summarization.
Ranking algorithms,such as Kleinberg?s HITS algorithm (Kleinberg,1999) or Google?s PageRank (Brin and Page, 1998),have been traditionally and successfully used inWeb-link analysis (Brin and Page, 1998), social net-works, and more recently in text processing appli-cations (Mihalcea and Tarau, 2004), (Mihalcea etal., 2004), (Erkan and Radev, 2004).
In short, agraph-based ranking algorithm is a way of decid-ing on the importance of a vertex within a graph, bytaking into account global information recursivelycomputed from the entire graph, rather than relying19only on local vertex-specific information.
The ba-sic idea implemented by the ranking model is thatof ?voting?
or ?recommendation?.
When one vertexlinks to another one, it is basically casting a vote forthat other vertex.
The higher the number of votesthat are cast for a vertex, the higher the importanceof the vertex.Let G = (V,E) be a directed graph with the setof vertices V and set of edges E, where E is a sub-set of V ?V .
For a given vertex Vi, let In(Vi) be theset of vertices that point to it (predecessors), and letOut(Vi) be the set of vertices that vertex Vi pointsto (successors).PageRank.
PageRank (Brin and Page, 1998)is perhaps one of the most popular ranking algo-rithms, and was designed as a method for Web linkanalysis.
Unlike other graph ranking algorithms,PageRank integrates the impact of both incomingand outgoing links into one single model, and there-fore it produces only one set of scores:PR(Vi) = (1?
d) + d ?
?Vj?In(Vi)PR(Vj)|Out(Vj)| (1)where d is a parameter set between 0 and 1.HITS.
HITS (Hyperlinked Induced TopicSearch) (Kleinberg, 1999) is an iterative algorithmthat was designed for ranking Web pages accordingto their degree of ?authority?.
The HITS algo-rithm makes a distinction between ?authorities?
(pages with a large number of incoming links) and?hubs?
(pages with a large number of outgoinglinks).
For each vertex, HITS produces two setsof scores ?
an ?authority?
score, and a ?hub?
score:HITSA(Vi) =?Vj?In(Vi)HITSH(Vj) (2)HITSH(Vi) =?Vj?Out(Vi)HITSA(Vj) (3)For each of these algorithms, starting from arbitraryvalues assigned to each node in the graph, the com-putation iterates until convergence below a giventhreshold is achieved.
After running the algorithm,a score is associated with each vertex, which rep-resents the ?importance?
or ?power?
of that vertexwithin the graph.In the context of Web surfing or citation analy-sis, it is unusual for a vertex to include multiple orpartial links to another vertex, and hence the orig-inal definition for graph-based ranking algorithmsis assuming unweighted graphs.
However, whenthe graphs are built starting with natural languagetexts, they may include multiple or partial links be-tween the units (vertices) that are extracted fromtext.
It may be therefore useful to integrate intothe model the ?strength?
of the connection betweentwo vertices Vi and Vj as a weight wij added tothe corresponding edge that connects the two ver-tices.
The ranking algorithms are thus adapted toinclude edge weights, e.g.
for PageRank the scoreis determined using the following formula (a similarchange can be applied to the HITS algorithm):PRW (Vi) = (1?d)+d?
?Vj?In(Vi)wji PRW (Vj)?Vk?Out(Vj)wkj(4)[1] Watching the new movie, ?Imagine: John Lennon,?
wasvery painful for the late Beatle?s wife, Yoko Ono.
[2] ?The only reason why I did watch it to the end is becauseI?m responsible for it, even though somebody else made it,?she said.
[3] Cassettes, film footage and other elements of the acclaimedmovie were collected by Ono.
[4] She also took cassettes of interviews by Lennon, whichwere edited in such a way that he narrates the picture.
[5] Andrew Solt (?This Is Elvis?)
directed, Solt and David L.Wolper produced and Solt and Sam Egan wrote it.
[6] ?I think this is really the definitive documentary of JohnLennon?s life,?
Ono said in an interview.123456 0.160.300.46 0.15[1.34][1.75][0.70][0.74][0.52][0.91]0.150.290.320.15Figure 1: Graph of sentence similarities built on asample text.
Scores reflecting sentence importanceare shown in brackets next to each sentence.While the final vertex scores (and therefore rank-ings) for weighted graphs differ significantly ascompared to their unweighted alternatives, the num-ber of iterations to convergence and the shape of theconvergence curves is almost identical for weightedand unweighted graphs.2.1 Single Document SummarizationFor the task of single-document extractive summa-rization, the goal is to rank the sentences in a giventext with respect to their importance for the overallunderstanding of the text.
A graph is therefore con-structed by adding a vertex for each sentence in thetext, and edges between vertices are established us-ing sentence inter-connections.
These connections20are defined using a similarity relation, where ?simi-larity?
is measured as a function of content overlap.Such a relation between two sentences can be seenas a process of ?recommendation?
: a sentence thataddresses certain concepts in a text gives the readera ?recommendation?
to refer to other sentences inthe text that address the same concepts, and there-fore a link can be drawn between any two such sen-tences that share common content.The overlap of two sentences can be determinedsimply as the number of common tokens betweenthe lexical representations of two sentences, or itcan be run through syntactic filters, which onlycount words of a certain syntactic category.
More-over, to avoid promoting long sentences, we use anormalization factor, and divide the content overlapof two sentences with the length of each sentence.The resulting graph is highly connected, with aweight associated with each edge, indicating thestrength of the connections between various sen-tence pairs in the text.
The graph can be repre-sented as: (a) simple undirected graph; (b) directedweighted graph with the orientation of edges setfrom a sentence to sentences that follow in the text(directed forward); or (c) directed weighted graphwith the orientation of edges set from a sentence toprevious sentences in the text (directed backward).After the ranking algorithm is run on the graph,sentences are sorted in reversed order of their score,and the top ranked sentences are selected for inclu-sion in the extractive summary.
Figure 1 shows anexample of a weighted graph built for a sample textof six sentences.2.2 Multiple Document SummarizationMulti-document summaries are built using a ?meta?summarization procedure.
First, for each documentin a given cluster of documents, a single documentsummary is generated using one of the graph-basedranking algorithms.
Next, a ?summary of sum-maries?
is produced using the same or a differentranking algorithm.
Figure 2 illustrates the meta-summarization process used to generate a multi-document summary starting with a cluster of Ndocuments.Unlike single documents ?
where sentences withhighly similar content are very rarely if at all en-countered ?
it is often the case that clusters of mul-tiple documents, all addressing the same or relatedtopics, would contain very similar or even identicalsentences.
To avoid such pairs of sentences, whichmay decrease the readability and the amount of in-formation conveyed by a summary, we introduce amaximum threshold on the sentence similarity mea-sure.
Consequently, in the graph construction stage,no link (edge) is added between sentences (ver-tices) whose similarity exceeds this threshold.
InSingle?documentsummarization Single?document summarizationSummary Document 1Summary Document 2Summary Document N......Single?documentsummarizationSingle?documentsummarizationDocument 1 Document 2 Document NMeta?documentMulti?document summaryFigure 2: Generation of a multi-document summaryusing meta-summarization.the experiments reported in this paper, this similar-ity threshold was empirically set to 0.5.3 Materials and Evaluation MethodologySingle and multiple English document summariza-tion experiments are run using the summarizationtest collection provided in the framework of theDocument Understanding Conference (DUC).
Inparticular, we use the data set of 567 news arti-cles made available during the DUC 2002 evalu-ations (DUC, 2002), and the corresponding 100-word summaries generated for each of these doc-uments (single-document summarization), or the100-word summaries generated for each of the 59document clusters formed on the same data set(multi-document summarization).
These are thesummarization tasks undertaken by other systemsparticipating in the DUC 2002 document summa-rization evaluations.To test the language independence aspect of thealgorithm, in addition to the English test collection,we also use a Brazilian Portuguese data set con-sisting of 100 news articles and their correspond-ing manually produced summaries.
We use theTeMa?rio test collection (Pardo and Rino, 2003),containing newspaper articles from online Braziliannewswire: 40 documents from Jornal de Brasil and60 documents from Folha de Sa?o Paulo.
The doc-uments were selected to cover a variety of domains(e.g.
world, politics, foreign affairs, editorials), andmanual summaries were produced by an expert inBrazilian Portuguese.
Unlike the summaries pro-duced for the English DUC documents ?
which hada length requirement of approximately 100 words,the length of the summaries in the TeMa?rio dataset is constrained relative to the length of the corre-21sponding documents, i.e.
a summary has to accountfor about 25-30% of the original document.
Con-sequently, the automatic summaries generated forthe documents in this collection are not restricted to100 words, as in the English experiments, but arerequired to have a length comparable to the corre-sponding manual summaries, to ensure a fair evalu-ation.For evaluation, we are using the ROUGE evalu-ation toolkit1, which is a method based on Ngramstatistics, found to be highly correlated with humanevaluations (Lin and Hovy, 2003a).
The evaluationis done using the Ngram(1,1) setting of ROUGE,which was found to have the highest correlationwith human judgments, at a confidence level of95%.4 Experimental ResultsThe extractive summarization algorithm is evalu-ated in the context of: (1) A single-document sum-marization task, where a summary is generated foreach of the 567 English news articles provided dur-ing the Document Understanding Evaluations 2002(DUC, 2002), and for each of the 100 Portuguesedocuments in the TeMa?rio data set; and (2) A multi-document summarization task, where a summary isgenerated for each of the 59 document clusters inthe DUC 2002 data.
Since document clusters andmulti-document summaries are not available for thePortuguese documents, a multi-document summa-rization evaluation could not be conducted on thisdata set.
Note however that the multi-documentsummarization tool is based on the single-documentsummarization method (see Figure 2), and thus highperformance in single-document summarization isexpected to result into a similar level of perfor-mance in multi-document summarization.4.1 Single Document Summarization forEnglishFor single-document summarization, we evaluatethe extractive summaries produced using each ofthe two graph-based ranking algorithms describedin Section 2 (HITS and PageRank).
Table 1shows the results obtained for the 100-words au-tomatically generated summaries for the EnglishDUC 2002 data set.
The table shows results us-ing the two graph algorithms described in Section2 when using graphs that are: (a) undirected, (b)directed forward, or (c) directed backward2.For a comparative evaluation, Table 2 shows theresults obtained on this data set by the top 5 (out1ROUGE is available at http://www.isi.edu/?cyl/ROUGE/.2Note that the first two rows in the table are in fact redun-dant, since the ?hub?
variation of the HITS algorithm can bederived from its ?authority?
counterpart by reversing the edgeorientation in the graphs.GraphAlgorithm Undir.
Forward BackwardHITSWA 49.12 45.84 50.23HITSWH 49.12 50.23 45.84PageRankW 49.04 42.02 50.08Table 1: Results for English single-document sum-marization.of 15) performing systems participating in the sin-gle document summarization task at DUC 2002.
Italso lists the baseline performance, computed for100-word summaries generated by taking the firstsentences in each article.Top 5 systems (DUC, 2002)S27 S31 S28 S21 S29 Baseline50.11 49.14 48.90 48.69 46.81 47.99Table 2: Results for top 5 DUC 2002 single docu-ment summarization systems, and baseline.4.2 Single Document Summarization forPortugueseThe single-document summarization tool was alsoevaluated on the TeMa?rio collection of Portuguesenewspaper articles.
We used the same graph set-tings as in the English experiments: graph-basedranking algorithms consisting of either HITS orPageRank, relying on graphs that are undirected,directed forward, or directed backward.
As men-tioned in Section 3, the length of each automaticallygenerated summary was constrained to match thelength of the corresponding manual summary, for afair comparison.
Table 3 shows the results obtainedon this data set, evaluated using the ROUGE evalu-ation toolkit.
A baseline was also computed, usingthe first sentences in each document, and evaluatedat 0.4963.GraphAlgorithm Undir.
Forward BackwardHITSWA 48.14 48.34 50.02HITSWH 48.14 50.02 48.34PageRankW 49.39 45.74 51.21Table 3: Results for Portuguese single-documentsummarization.4.3 Multiple Document SummarizationWe evaluate multi-document summaries gener-ated using combinations of the graph-based rank-ing algorithms that were found to work best inthe single document summarization experiments ?PageRankW and HITSWA , on undirected or di-rected backward graphs.
Although the single docu-ment summaries used in the ?meta?
summarization22process may conceivably be of any size, in this eval-uation their length is limited to 100 words.As mentioned earlier, different graph algorithmscan be used for producing the single document sum-mary and the ?meta?
summary; Table 4 lists theresults for multi-document summarization experi-ments using various combinations of graph algo-rithms.
For comparison, Table 5 lists the results ob-tained by the top 5 (out of 9) performing systemsin the multi-document summarization task at DUC2002, and a baseline generated by taking the firstsentence in each article.Since no multi-document clusters and associ-ated summaries were available for the other lan-guage considered in our experiments, the multi-document summarization experiments were con-ducted only on the English data set.
However, sincethe multi-doc summarization technique consists ofa layered application of single-document summa-rization, we believe that the performance achievedin single-document summarization for Portuguesewould eventually result into similar performancefigures when applied to the summarization of clus-ters of documents.Top 5 systems (DUC, 2002)S26 S19 S29 S25 S20 Baseline35.78 34.47 32.64 30.56 30.47 29.32Table 5: Results for top 5 DUC 2002 multi-document summarization systems, and baseline.4.4 DiscussionThe graph-based extractive summarization algo-rithm succeeds in identifying the most importantsentences in a text (or collection of texts) based oninformation exclusively drawn from the text itself.Unlike other supervised systems, which attempt tolearn what makes a good summary by training oncollections of summaries built for other articles, thegraph-based method is fully unsupervised, and re-lies only on the given texts to derive an extractivesummary.For single document summarization, theHITSWA and PageRankW algorithms, run ona graph structure encoding a backward directionacross sentence relations, provide the best per-formance.
These results are consistent acrosslanguages ?
with similar performance figuresobserved on both the English DUC data set andon the Portuguese TeMa?rio data set.
The settingthat is always exceeding the baseline by a largemargin is PageRankW on a directed backwardgraph, with clear improvements over the simple(but powerful) first-sentence selection baseline.Moreover, comparative evaluations performedwith respect to other systems participating in theDUC 2002 evaluations revealed the fact that theperformance of the graph-based extractive summa-rization method is competitive with state-of-the-artsummarization systems.Interestingly, the ?directed forward?
setting isconsistently performing worse than the baseline,which can be explained by the fact that both datasets consist of newspaper articles, which tend toconcentrate the most important facts toward the be-ginning of the document, and therefore disfavor aforward direction set across sentence relations.For multiple document summarization, the best?meta?
summarizer is the PageRankW algorithmapplied on undirected graphs, in combination witha single summarization system using the HITSWAranking algorithm, for a performance similar to theone of the best system in the DUC 2002 multi-document summarization task.The results obtained during all these experimentsprove that graph-based ranking algorithms, previ-ously found successful in Web link analysis and so-cial networks, can be turned into a state-of-the-arttool for extractive summarization when applied tographs extracted from texts.
Moreover, the methodwas also shown to be language independent, lead-ing to similar results when applied to the summa-rization of documents in different languages.The better results obtained by algorithms likeHITSWA and PageRank on graphs containing onlybackward edges are likely to come from the fact thatrecommendations flowing toward the beginning ofthe text take advantage of the bias giving highersummarizing value of sentences occurring at the be-ginning of the document.Another important aspect of the method is thatit gives a ranking over all sentences in a text (ora collection of texts) ?
which means that it can beeasily adapted to extracting very short summaries,or longer more explicative summaries.4.5 Related WorkExtractive summarization is considered an impor-tant first step for more sophisticated automatic textsummarization.
As a consequence, there is a largebody of work on algorithms for extractive summa-rization undertaken as part of the DUC evaluationexercises (http://www-nlpir.nist.gov/projects/duc/).Previous approaches include supervised learning(Hirao et al, 2002), (Teufel and Moens, 1997), vec-torial similarity computed between an initial ab-stract and sentences in the given document, intra-document similarities (Salton et al, 1997), or graphalgorithms (Mihalcea and Tarau, 2004), (Erkan andRadev, 2004), (Wolf and Gibson, 2004).
It is alsonotable the study reported in (Lin and Hovy, 2003b)discussing the usefulness and limitations of auto-matic sentence extraction for text summarization,23Single document ?Meta?
summarization algorithmsummarization algo.
PageRankW -U PageRankW -DB HITSWA -U HITSWA -DBPageRankW -U 35.52 34.99 34.56 34.65PageRankW -DB 35.02 34.48 35.19 34.39HITSWA -U 33.68 32.59 32.12 34.23HITSWA -DB 35.72 35.20 34.62 34.73Table 4: Results for multi-document summarization (U = Undirected; DB = Directed Backward)which emphasizes the need of accurate tools forsentence extraction as an integral part of automaticsummarization systems.5 ConclusionsIntuitively, iterative graph-based ranking algo-rithms work well on the task of extractive summa-rization because they do not only rely on the localcontext of a text unit (vertex), but they rather takeinto account information recursively drawn fromthe entire text (graph).
Through the graphs it buildson texts, a graph-based ranking algorithm identifiesconnections between various entities in a text, andimplements the concept of recommendation.
A textunit recommends other related text units, and thestrength of the recommendation is recursively com-puted based on the importance of the units makingthe recommendation.
In the process of identifyingimportant sentences in a text, a sentence recom-mends another sentence that addresses similar con-cepts as being useful for the overall understandingof the text.
Sentences that are highly recommendedby other sentences are likely to be more informa-tive for the given text, and will be therefore given ahigher score.In this paper, we showed that a previously pro-posed method for graph-based extractive summa-rization can be successfully applied to the sum-marization of documents in different languages,without any requirements for additional knowl-edge or corpora.
Moreover, we showed how ameta-summarizer relying on a layered applicationof techniques for single-document summarizationcan be turned into an effective method for multi-document summarization.
Experiments performedon standard data sets have shown that the results ob-tained with this method are comparable with thoseof state-of-the-art systems for automatic summa-rization, while at the same time providing the bene-fits of a robust language independent algorithm.AcknowledgmentsWe are grateful to Lucia Helena Machado Rino formaking available the TeMa?rio summarization testcollection and for her help with this data set.ReferencesS.
Brin and L. Page.
1998.
The anatomy of a large-scalehypertextual Web search engine.
Computer Networksand ISDN Systems, 30(1?7).DUC.
2002.
Document understanding conference 2002.http://www-nlpir.nist.gov/projects/duc/.G.
Erkan and D. Radev.
2004.
Lexpagerank: Prestige inmulti-document text summarization.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, Barcelona, Spain, July.T.
Hirao, Y. Sasaki, H. Isozaki, and E. Maeda.
2002.Ntt?s text summarization system for duc-2002.
InProceedings of the Document Understanding Confer-ence 2002.J.M.
Kleinberg.
1999.
Authoritative sources in a hyper-linked environment.
Journal of the ACM, 46(5):604?632.C.Y.
Lin and E.H. Hovy.
2003a.
Automatic evalua-tion of summaries using n-gram co-occurrence statis-tics.
In Proceedings of Human Language TechnologyConference (HLT-NAACL 2003), Edmonton, Canada,May.C.Y.
Lin and E.H. Hovy.
2003b.
The potential and lim-itations of sentence extraction for summarization.
InProceedings of the HLT/NAACL Workshop on Auto-matic Summarization, Edmonton, Canada, May.R.
Mihalcea and P. Tarau.
2004.
TextRank ?
bringingorder into texts.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2004), Barcelona, Spain.R.
Mihalcea, P. Tarau, and E. Figa.
2004.
PageR-ank on semantic networks, with application to wordsense disambiguation.
In Proceedings of the 20st In-ternational Conference on Computational Linguistics(COLING 2004), Geneva, Switzerland.T.A.S.
Pardo and L.H.M.
Rino.
2003.
TeMario: a cor-pus for automatic text summarization.
Technical re-port, NILC-TR-03-09.G.
Salton, A. Singhal, M. Mitra, and C. Buckley.
1997.Automatic text structuring and summarization.
Infor-mation Processing and Management, 2(32).S.
Teufel and M. Moens.
1997.
Sentence extractionas a classification task.
In ACL/EACL workshop on?Intelligent and scalable Text summarization?, pages58?65, Madrid, Spain.F.
Wolf and E. Gibson.
2004.
Paragraph-, word-,and coherence-based approaches to sentence ranking:A comparison of algorithm and human performance.In Proceedings of the 42nd Meeting of the Associa-tion for Computational Linguistics, Barcelona, Spain,July.24
