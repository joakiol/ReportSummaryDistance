Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1386?1390,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsSimple task-specific bilingual word embeddings?Stephan GouwsStellenbosch Universitystephan@ml.sun.ac.zaAnders S?gaardUniversity of Copenhagensoegaard@hum.ku.dkAbstractWe introduce a simple wrapper method thatuses off-the-shelf word embedding algorithmsto learn task-specific bilingual word em-beddings.
We use a small dictionary ofeasily-obtainable task-specific word equiva-lence classes to produce mixed context-targetpairs that we use to train off-the-shelf em-bedding models.
Our model has the advan-tage that it (a) is independent of the choiceof embedding algorithm, (b) does not re-quire parallel data, and (c) can be adapted tospecific tasks by re-defining the equivalenceclasses.
We show how our method outper-forms off-the-shelf bilingual embeddings onthe task of unsupervised cross-language part-of-speech (POS) tagging, as well as on thetask of semi-supervised cross-language supersense (SuS) tagging.1 IntroductionUsing multi-layered neural networks to learn wordembeddings has become standard in NLP (Turian etal., 2010; Guo et al, 2014).
While there is still somecontroversy whether such methods are superior toolder methods (Levy and Goldberg, 2014; Baroniet al, 2014), there is little doubt that continuousword representations can potentially solve some ofthe data sparsity problems inherent in NLP.Most research on word embeddings has focusedon learning representations for the words in a sin-gle language, making syntactically or semanticallysimilar words appear close in the embedding space.Embeddings have been applied to many tasks, from?The authors contributed equally to this work.
The secondauthor is funded by the ERC Starting Grant LOWLANDS No.313695.named entity recognition (Turian et al, 2010) to de-pendency parsing (Bansal et al, 2014).
It has fur-thermore been shown that weakly supervised em-bedding algorithms can also lead to huge improve-ments for tasks like sentiment analysis (Tang et al,2014).
In this work, we also use weak or distant su-pervision, relying on small dictionary seeds.This paper, however, considers the problem oflearning bilingual word embeddings, i.e., word em-beddings such that similar words in two differentlanguages end up close in the embedding space.Such bilingual word embeddings can potentially beused for better cross-language transfer of NLP mod-els, as we show in this paper.
Previous work on bilin-gual word embeddings have defined similar wordsas translation equivalents and evaluated embeddingsin the context of document classification tasks (Kle-mentiev et al, 2012; Kocisky et al, 2014).
In thispaper, we present a simple wrapper method to ex-isting monolingual word embedding algorithms thatcan learn task-specific bilingual embeddings, e.g.,for POS tagging, named entity recognition, or sen-timent analysis.
Our algorithm is simpler and per-forms better on the tasks where we could compareperformance to existing algorithms.
Also, we notethat our approach, unlike existing algorithms (Kle-mentiev et al, 2012), is as fast as learning monolin-gual embeddings.Our contributions In this paper we introduce anew approach for learning bilingual word embed-dings and revisit the task of unsupervised cross-language POS tagging (Das and Petrov, 2011).
Ourbilingual embedding model, which we call BilingualAdaptive Reshuffling with Individual Stochastic Al-ternatives (BARISTA), takes two (non-parallel) cor-pora and a small dictionary as input.
The dictio-1386nary is essentially a list of words in the two lan-guages that are equivalent with respect to some task,e.g., English car and French maison (?house?)
areboth nouns, and hence ?equivalent?
in POS tagging;English clerk and chauffeur are both persons, andhence ?equivalent?
in SuS tagging; house and mai-son are equivalent in machine translation.
BARISTAhas the advantage that it (a) is independent of thechoice of embedding algorithm, (b) does not requireparallel data, and (c) can be adapted to specific tasksby using appropriate dictionaries.
We use the bilin-gual embeddings directly to train a target languagePOS tagger on source language training data.
In-stead of lexical features, we use the bilingual embed-dings.
We show our bilingual embedding methodoutperforms using off-the-shelf bilingual embed-dings on this task, and that our system is competitiveto state-of-the-art approaches for cross-languagePOS tagging.
Finally, we show that the same embed-dings also lead to significantly better performance insemi-supervised cross-language SuS tagging.
Thecode will be made publicly available at https://github.com/gouwsmeister/barista.2 Our approachStandard monolingual neural language models areunsupervised models that train on raw text, learningword features that enable the model to predict thenext word (the target) from a sequence of words (thecontext).
In the process, the model learns to clusterwords into soft equivalence classes (words that havesimilar distributions).Several authors have proposed bilingual cluster-ing and embedding algorithms based on parallel data(T?ackstr?om et al, 2012; Klementiev et al, 2012;Zou et al, 2013; Kocisky et al, 2014; Hermannand Blunsom, 2014b).
These authors have all eval-uated their embeddings on document classificationand machine translation, and not yet structured pre-diction tasks like POS/SuS tagging or syntactic pars-ing.
A notable exception is Hermann and Blunsom(2014a), who do not rely on parallel data and do notuse word alignments, but they still use comparabledata and sentence alignments, and they only evalu-ate their embeddings in document classification.The assumption that large amounts of paralleldata exists for a language pair of interest is some-times too strong (Hermann and Blunsom, 2014a).On the other hand, we often have access to smallsamples of near-equivalences from knowledge basesof various forms.
For example, for POS tagging,we often have access to small-to-sizeable crowd-sourced tag dictionaries (e.g.
Wiktionary).
For SuStagging, which is the other example considered inthis paper, we sometimes have access to WordNetsor similar resources.
If we have such resourcesfor both source and target language, we can extractword-equivalences from them and use these to learnbilingual embeddings using our proposed method.In this paper, we experiment with usingboth equivalences based on word alignments (e.g.,house?
maison), and equivalences based on knowl-edge bases (e.g., car ?
maison).
Crucially, our ap-proach to learning bilingual embeddings only as-sumes a small seed of equivalences, no parallel data.It then uses these to produce a set of mixed context-target pairs.Our input is a source corpus Csand a target cor-pus Ct, as well as a set of bilingual equivalences R?.We begin by shuffling the concatenation of CsandCt.
We then pass over this mixed corpus, and foreach word w, if {w?| w,w??
R?}
is non-emptyand of cardinality k, i.e., w is in the seed list ofequivalences, we replace w with w?with probability1/2k.
In other words, we flip a coin whether to re-place w, and then randomly choose one of its equiva-lences as our replacement.
For example, using trans-lation equivalence classes, one could generate any ofthe following mixed texts from the English sentencebuild the house: construire the house, build la mai-son, build the maison, etc., or any other combinationof English and French words with the English wordorder.
With POS equivalence classes, any of thewords in build the house can be replaced with wordswith overlapping syntactic categories, e.g., build thevoiture.3 ExperimentsIn our experiments we balance the source and tar-get corpora, by subsampling from the bigger cor-pus.
The vocabularies for all models are kept un-restricted, and result in around 1M words per lan-guage pair.
We train with a window of 4 words oneither side of the target word, using linear discount-ing of the initial learning rate of 0.1.
These parame-ters were set on the Spanish POS data (see ?3.2).
We1387Figure 1: t-SNE visualization of BARISTA embeddingstrained with POS classes.use the CBOW model in word2vec for training theword embeddings.1Both our POS tagging evaluation datasets, as wellas the Wiktionaries,2are mapped to Google?s univer-sal tagset (Petrov et al, 2011).
We use the deriveddictionaries for extracting bilingual POS equiva-lence classes.
For SuS tagging, we only considerEnglish-Danish and extract equivalence classes fromPrinceton WordNet and DanNet.
For translationequivalents, we made use of Google Translate.3.1 Qualitative EvaluationIn our main experiments (?3.2?3.3), we use datafrom Wikipedia, but we first present a qualitativeevaluation of English-German bilingual embeddingslearned from the smaller Europarl corpus.3Note thatwhile this is parallel data, we do not exploit its par-allel nature.POS classes The embeddings learned fromEnglish-German Europarl using POS classes fromWiktionary were visualized using the t-SNE tech-nique (Van der Maaten and Hinton, 2008), and areshown in Fig 1.
The model learns to cluster wordsvery distinctively by their POS tag.
Monolingualword embedding models with short context win-dows typically cluster by POS, but here we see thesame effect for bilingual embeddings, i.e.
wordsfrom both languages with the same POS tag clus-ter together.
Individual words, on the other hand, donot appear to retain the fine-grained relationships we1https://code.google.com/p/word2vec/2https://code.google.com/p/wikily-supervised-pos-tagger/3http://www.statmt.org/europarl/(a)(b)Figure 2: t-SNE visualizations of BARISTA embeddingsubspaces for (a) prepositions and (b) modals.normally observe in word embeddings (where simi-lar words cluster closer together).
The question thusis whether such embeddings are more or less usefulfor cross-language POS tagging than bilingual em-beddings based on translation equivalencies.Translation classes Next, we induced English-German bilingual embeddings on Europarl usingtranslations obtained from Google Translate.
Wederived a dictionary of the top 20k most frequentwords in English, translated into German.
The em-beddings are shown in Fig 2.
The visualizationsshow that the models are able to extract very fine-grained bilingual relationships, and some clustersstill correspond to POS.3.2 Cross-language part-of-speech taggingNext we evaluated the embeddings in the context ofunsupervised cross-language POS tagging (Das andPetrov, 2011).
The goal is to train a tagger ?
in ourcase, we use SEARN (Daume et al, 2009), follow-ing (Johannsen et al, 2014)?
on labeled English datadecorated with bilingual embeddings, and then eval-uate the model on another target language.
We usedata from Danish, German, Spanish, Italian, Dutch,Portuguese, and Swedish.
Training and test data,which are the same used in Das and Petrov (2011),were converted to use the same 12 universal parts ofspeech proposed by Petrov et al (2011).All results in this section were obtained by train-ing bilingual embedding models on the publicly-1388Language TC-Perc Random Klmtv POS-50 POS-300 Tr-50 Tr-300 DP B-KSpanish 80.6 81.8 79.8 82.4 81 81.6 82.6 84.2 80.2German 80.4 82.7 82.8 81.8 84.1 82.6 84.8 82.8 81.3Danish 63 68.9 - 68.9 72.4 71.8 78.4 83.2 69.1Swedish 71.6 73.7 - 75 76 75.4 77.5 80.5 70.1Italian 80.1 81.3 - 82.1 80.9 82.1 80.7 86.8 68.1Dutch 74.5 77.2 - 78.3 77.4 78.7 80.3 79.5 65.1Portuguese 76.9 78.1 - 77.3 76.1 80.6 80.5 87.9 78.4Avg 75.3 77.7 - 78 78.3 79 80.7 83.6 73.2Table 1: Cross-language POS tagging.
TC-Perc: type-constrained structured perceptron.available, pre-tokenized versions of Wikipedia,4andthen using the trained embeddings as features in apublicly available implementation of the structuredperceptron.5We use ortographic features, as wellas the embedding vector of the target word.
In ad-dition, we use type constraints from Wiktionary toprune the search lattice during decoding (T?ackstr?omet al, 2013).
We scaled the embedding features inthe same way as Turian et al (2010) with scalingparameter 0.01 (set on Spanish POS data).Our baseline method is a type-constrained struc-tured perceptron with only ortographic features,which are expected to transfer across languages.We also experiment with using random embeddings,as well as the embeddings provided by Klemen-tiev et al (2012) (Klmtv).6Our results are dis-played in Table 1.
POS-X refer to X-dimensionalBARISTA embeddings trained with POS equiva-lence classes, and Tr-X is X-dimensional BARISTAembeddings trained using translation equivalenceclasses.
We note that random embeddings improveover our baseline, suggesting that the random fea-tures act as regularizers.
The embeddings providedby Klementiev et al (2012) seem to lead to worseperformance than random embeddings, presumablybecause they capture mostly semantic (topic) simi-larity.
We also compare our results to those reportedby Berg-Kirkpatrick et al (2010) (B-K) and Das andPetrov (2011) (DP), but note that their approachesrequire in-sample unlabeled data, and in the lattercase, also parallel bilingual data.4https://sites.google.com/site/rmyeid/projects/polyglot5https://github.com/coastalcph/rungsted6http://people.mmci.uni-saarland.de/?aklement/data/distrib/While training with POS classes improves overthe random baseline, training with translation equiv-alence classes gives even better performance.
Forboth approaches, using more embedding featuresimproves the performance (500 dimensions did notimprove significantly over 300).
Our model is gener-ally better than Berg-Kirkpatrick et al (2010) ?
butworse than Das and Petrov (2011).3.3 Cross-language super sense taggingFinally, we also tried using the BARISTA-embeddings for English-Danish (with parametersstill set on Spanish POS) on another task, namelysuper sense tagging (Ciaramita and Altun, 2006).We train a system on a mixture of 1000 randomlysampled sentences from English SemCor7and 320labeled Danish sentences (see below) and compareusing bilingual embeddings trained with equiva-lence classes from English and Danish wordnets(WN-300), to embeddings trained using translationequivalence classes (Tr-300).
We use 300 dimen-sional embeddings.
We use a POS-sensitive mostfrequent sense baseline (MFS), as well as structuredperceptron model trained only with ortographic andPOS features, as well as MFS features (Johannsenet al, 2014).
Our metric is a weighted averageover F1-scores for the (41) semantic classes.
Notethat using the knowledge base is superior to usingtranslation equivalences, but both embeddingsare superior to both our baselines.
The Danishtraining (newswire only) and test data (six differentdomains) ?
is made publicly available.87http://web.eecs.umich.edu/?mihalcea/downloads.html#semcor8https://github.com/coastalcph/noda2015_sst1389Baselines BARISTAMFS TC-Searn Tr-300 WN-300blogs 49.1 46.7 50.5 61.9forum 44.5 41.2 45.0 53.9magazine 46.5 45.2 50.4 51.5newswire 48.4 45.4 52.7 60.9reviews 48.4 44.9 50.4 55.8speech 51.1 48.4 51.5 58.4Avg 48.1 45.4 50.5 58.3Table 2: Cross-language SuS tagging.4 ConclusionsWe presented a simple approach, BARISTA, to learn-ing bilingual embeddings.
BARISTA has the advan-tages that it (a) is independent of the choice of em-bedding algorithm, (b) does not require parallel data,and (c) can be adapted to specific tasks by using ap-propriate dictionaries.
Our embeddings proved use-ful for cross-language POS/SuS tagging.ReferencesMohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.Tailoring continuous word representations for depen-dency parsing.
In ACL.Marco Baroni, Georgiana Dinu, and German Kruszewski.2014.
Don?t count, predict!
a systematic compari-son of context-counting vs. context-predicting seman-tic vectors.
In ACL.Taylor Berg-Kirkpatrick, Alexandre Cote, John DeNero,and Dan Klein.
2010.
Painless unsupervised learningwith features.
In NAACL.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informationextraction with a supersense sequence tagger.
In Proc.of EMNLP, pages 594?602, Sydney, Australia, July.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In ACL.Hal Daume, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.
Machine Learn-ing, 75:297?325.Jiang Guo, Wanxiang Che, Haifeng Wang, and Ting Liu.2014.
Revisiting embedding features for simple semi-supervised learning.
In EMNLP.Karl Hermann and Phil Blunsom.
2014a.
Multilingualdistributed representations without word alignment.
InICLR.Karl Hermann and Phil Blunsom.
2014b.
Multilingualmodels for compositional distributed semantics.
InACL.Anders Johannsen, Dirk Hovy, Hector Martinez Alonso,and Anders S?gaard.
2014.
More or less supervisedsuper-sense tagging of twitter.
In *SEM.Alexandre Klementiev, Ivan Titov, and Binod Bhattarai.2012.
Inducing crosslingual distributed representa-tions of words.
In COLING.Tomas Kocisky, Moritz Hermann, and Phil Blunsom.2014.
Learning bilingual word representations bymarginalizing alignments.
In ACL.Omer Levy and Yoav Goldberg.
2014.
Neural word em-beddings as implicit matrix factorization.
In NIPS.Slav Petrov, Dipanjan Das, and Ryan McDonald.2011.
A universal part-of-speech tagset.
CoRRabs/1104.2086.Oscar T?ackstr?om, Ryan McDonald, and Jakob Uszkoreit.2012.
Cross-lingual word clusters for direct transferof linguistic structure.
In NAACL.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan Mc-Donald, and Joakim Nivre.
2013.
Token and typeconstraints for cross-lingual part-of-speech tagging.TACL, 1:1?12.Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,and Bing Qin.
2014.
Learning sentiment-specificword embedding for Twitter sentiment classification.In ACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In ACL.Laurens Van der Maaten and Geoffrey Hinton.
2008.
Vi-sualizing data using t-sne.
Journal of Machine Learn-ing Research, 9(2579-2605):85.Will Zou, Richard Socher, Daniel Cer, and Chris Man-ning.
2013.
Bilingual word embeddings for phrase-based machine translation.
In EMNLP.1390
