Proceedings of the ACL 2007 Demo and Poster Sessions, pages 25?28,Prague, June 2007. c?2007 Association for Computational LinguisticsNICT-ATR Speech-to-Speech Translation SystemEiichiro Sumita Tohru Shimizu Satoshi NakamuraNational Institute of Information and Communications Technology&ATR Spoken Language Communication Research Laboratories2-2-2 Hikaridai, Keihanna Science City, Kyoto 619-0288, Japaneiichiro.sumita, tohru.shimizu & satoshi.nakamura@atr.jpAbstractThis paper describes the latest version ofspeech-to-speech translation systems de-veloped by the team of NICT-ATR for overtwenty years.
The system is now ready tobe deployed for the travel domain.
A newnoise-suppression technique notably im-proves speech recognition performance.Corpus-based approaches of recognition,translation, and synthesis enable coverageof a wide variety of topics and portabilityto other languages.1 IntroductionSpeech recognition, speech synthesis, and machinetranslation research started about half a centuryago.
They have developed independently for a longtime until speech-to-speech translation researchwas proposed in the 1980?s.
The feasibility ofspeech-to-speech translation was the focus of re-search at the beginning because each componentwas difficult to build and their integration seemedmore difficult.
After groundbreaking work for twodecades, corpus-based speech and language proc-essing technology have recently enabled theachievement of speech-to-speech translation that isusable in the real world.This paper introduces (at ACL 2007) the state-of-the-art speech-to-speech translation system de-veloped by NICT-ATR, Japan.2 SPEECH-TO-SPEECH TRANSLA-TION SYSTEMA speech-to-speech translation system is very largeand complex.
In this paper, we prefer to describerecent progress.
Detailed information can be foundin [1, 2, 3] and their references.2.1 Speech recognitionTo obtain a compact, accurate model from corporawith a limited size, we use MDL-SSS [4] andcomposite multi-class N-gram models [5] foracoustic and language modeling, respectively.MDL-SSS is an algorithm that automatically de-termines the appropriate number of parameters ac-cording to the size of the training data based on theMaximum Description Length (MDL) criterion.Japanese, English, and Chinese acoustic modelswere trained using the data from 4,200, 532, and536 speakers, respectively.
Furthermore, thesemodels were adapted to several accents, e.g., US(the United States), AUS (Australia), and BRT(Britain) for English.
A statistical language modelwas trained by using large-scale corpora (852 ksentences of Japanese, 710 k sentences of English,510 k sentences of Chinese) drawn from the traveldomain.Robust speech recognition technology in noisysituations is an important issue for speech transla-tion in real-world environments.
An MMSE(Minimum mean square error) estimator for logMel-spectral energy coefficients using a GMM(Gaussian Mixture Model) [6] is introduced forsuppressing interference and noise and for attenu-ating reverberation.Even when the acoustic and language modelsare trained well, environmental conditions such asvariability of speakers, mismatches between thetraining and testing channels, and interferencefrom environmental noise may cause recognitionerrors.
These utterance recognition errors can berejected by tagging them with a low confidencevalue.
To do this we introduce generalized word25posterior probability (GWPP)-based recognitionerror rejection for the post processing of the speechrecognition [7, 8].2.2 Machine translationThe translation modules are automatically con-structed from large-scale corpora: (1) TATR, aphrase-based SMT module and (2) EM, a simplememory-based translation module.
EM matches agiven source sentence against the source languageparts of translation examples.
If an exact match isachieved, the corresponding target language sen-tence will be output.
Otherwise, TATR is called up.In TATR, which is built within the framework offeature-based exponential models, we used the fol-lowing five features: phrase translation probabilityfrom source to target; inverse phrase translationprobability; lexical weighting probability fromsource to target; inverse lexical weighting prob-ability; and phrase penalty.Here, we touch on two approaches of TATR:novel word segmentation for Chinese, and lan-guage model adaptation.We used a subword-based approach for wordsegmentation of Chinese [9].
This word segmenta-tion is composed of three steps.
The first is a dic-tionary-based step, similar to the word segmenta-tion provided by LDC.
The second is a subword-based IOB tagging step implemented by a CRFtagging model.
The subword-based IOB taggingachieves a better segmentation than character-based IOB tagging.
The third step is confidence-dependent disambiguation to combine the previoustwo results.
The subword-based segmentation wasevaluated with two different data from the SighanBakeoff and the NIST machine translation evalua-tion workshop.
With the data of the second SighanBakeoff1, our segmentation gave a higher F-scorethan the best published results.
We also evaluatedthis segmentation in a translation scenario usingthe data of NIST translation evaluation 2  2005,where its BLEU score3 was 1.1% higher than thatusing the LDC-provided word segmentation.The language model that is used plays an impor-tant role in SMT.
The effectiveness of the language1 http://sighan.cs.uchicago.edu/bakeoff2005/2http://www.nist.gov/speech/tests/mt/mt05eval_official_results_release_20050801_v3.html3http://www.nist.gov/speech/tests/mt/resources/scoring.htmmodel is significant if the test data happen to havethe same characteristics as those of the trainingdata for the language models.
However, this coin-cidence is rare in practice.
To avoid this perform-ance reduction, a topic adaptation technique is of-ten used.
We applied this adaptation technique tomachine translation.
For this purpose, a ?topic?
isdefined as clusters of bilingual sentence pairs.
Inthe decoding, for a source input sentence, f, a topicT is determined by maximizing P(f|T).
To maxi-mize P(f|T) we select cluster T that gives the high-est probability for a given translation source sen-tence f. After the topic is found, a topic-dependentlanguage model P(e|T) is used instead of P(e), thetopic-independent language model.
The topic-dependent language models were tested usingIWSLT06 data 4 .
Our approach improved theBLEU score between 1.1% and 1.4%.
The paper of[10] presents a detailed description of this work.2.3 Speech synthesisAn ATR speech synthesis engine called XIMERAwas developed using large corpora (a 110-hourcorpus of a Japanese male, a 60-hour corpus of aJapanese female, and a 20-hour corpus of a Chi-nese female).
This corpus-based approach makes itpossible to preserve the naturalness and personalityof the speech without introducing signal processingto the speech segment [11].
XIMERA?s HMM(Hidden Markov Model)-based statistical prosodymodel is automatically trained, so it can generate ahighly natural F0 pattern [12].
In addition, the costfunction for segment selection has been optimizedbased on perceptual experiments, thereby improv-ing the naturalness of the selected segments [13].3 EVALUATION3.1 Speech and language corporaWe have collected three kinds of speech and lan-guage corpora: BTEC (Basic Travel ExpressionCorpus), MAD (Machine Aided Dialog), and FED(Field Experiment Data) [14, 15, 16, and 17].
TheBTEC Corpus includes parallel sentences in twolanguages composed of the kind of sentences onemight find in a travel phrasebook.
MAD is a dialogcorpus collected using a speech-to-speech transla-tion system.
While the size of this corpus is rela-tively limited, the corpus is used for adaptation and4 http://www.slt.atr.jp/IWSLT2006/26evaluation.
FED is a corpus collected in KansaiInternational Airport uttered by travelers using theairport.3.2 Speech recognition systemThe size of the vocabulary was about 35 k in ca-nonical form and 50 k with pronunciation varia-tions.
Recognition results are shown in Table 1 forJapanese, English, and Chinese with a real-timefactor5 of 5.
Although the speech recognition per-formance for dialog speech is worse than that forread speech, the utterance correctness excludingerroneous recognition output using GWPP [8] wasgreater than 83% in all cases.BTEC MAD FEDCharacteristics Read speechDialogspeech(Office)Dialogspeech(Airport)# of speakers 20 12 6# of utterances 510 502 155# of word tokens 4,035 5,682 1,108Average length 7.9 11.3 7.1Perplexity 18.9 23.2 36.2Japanese 94.9 92.9 91.0English 92.3 90.5 81.0Word ac-curacyChinese 90.7 78.3 76.5All 82.4 62.2 69.0Utterancecorrect-nessNot re-jected 87.1 83.9 91.4Table 1 Evaluation of speech recognition3.3 Machine TranslationThe mechanical evaluation is shown, where thereare sixteen reference translations.
The performanceis very high except for English-to-Chinese (Table2).BLEUJapanese-to-English 0.6998English-to-Japanese 0.7496Japanese-to-Chinese 0.6584Chinese-to-Japanese 0.7400English-to-Chinese 0.5520Chinese-to-English 0.6581Table 2 Mechanical evaluation of translation5 The real time factor is the ratio to an utterance time.The translation outputs were ranked A (perfect),B (good), C (fair), or D (nonsense) by professionaltranslators.
The percentage of ranks is shown inTable 3.
This is in accordance with the aboveBLEU score.A AB ABCJapanese-to-English 78.4 86.3 92.2English-to-Japanese 74.3 85.7 93.9Japanese-to-Chinese 68.0 78.0 88.8Chinese-to-Japanese 68.6 80.4 89.0English-to-Chinese 52.5 67.1 79.4Chinese-to-English 68.0 77.3 86.3Table 3 Human Evaluation of translation4 System presented at ACL 2007The system works well in a noisy environment andtranslation can be performed for any combinationof Japanese, English, and Chinese languages.
Thedisplay of the current speech-to-speech translationsystem is shown below.Figure 1  Japanese-to-English Display of NICT-ATR Speech-to-Speech Translation System5 CONCLUSIONThis paper presented a speech-to-speech transla-tion system that has been developed by NICT-ATRfor two decades.
Various techniques, such as noisesuppression and corpus-based modeling for bothspeech processing and machine translation achieverobustness and portability.The evaluation has demonstrated that our systemis both effective and useful in a real-world envi-ronment.27References[1] S. Nakamura, K. Markov, H. Nakaiwa, G. Kikui, H.Kawai, T. Jitsuhiro, J. Zhang, H. Yamamoto, E.Sumita, and S. Yamamoto.
The ATR multilingualspeech-to-speech translation system.
IEEE Trans.
onAudio, Speech, and Language Processing, 14, No.2:365?376, 2006.
[2] T. Shimizu, Y. Ashikari, E. Sumita, H. Kashioka,and S. Nakamura, ?Development of client-serverspeech translation system on a multi-lingual speechcommunication platform,?
Proc.
of the InternationalWorkshop on Spoken Language Translation, pp.
213-216, Kyoto, Japan, 2006.
[3] R. Zhang, H. Yamamoto, M. Paul, H. Okuma, K.Yasuda, Y. Lepage, E. Denoual, D. Mochihashi, A.Finch, and E. Sumita, ?The NiCT-ATR StatisticalMachine Translation System for the IWSLT 2006Evaluation,?
Proc.
of the International Workshop onSpoken Language Translation, pp.
83-90, Kyoto, Ja-pan , 2006.
[4] T. Jitsuhiro, T. Matsui, and S. Nakamura.
Automaticgeneration of non-uniform context-dependent HMMtopologies based on the MDL criterion.
In Proc.
ofEurospeech, pages 2721?2724, 2003.
[5] H. Yamamoto, S. Isogai, and Y. Sagisaka.
Multi-class composite N-gram language model.
SpeechCommunication, 41:369?379, 2003.
[6] M. Fujimoto and Y. Ariki.
Combination of temporaldomain SVD based speech enhancement and GMMbased speech estimation for ASR in noise - evalua-tion on the AURORA II database and tasks.
In Proc.of Eurospeech, pages 1781?1784, 2003.
[7] F. K. Soong, W. K. Lo, and S. Nakamura.
Optimalacoustic and language model weight for minimizingword verification errors.
In Proc.
of ICSLP, pages441?444, 2004[8] W. K. Lo and F. K. Soong.
Generalized posteriorprobability for minimum error verification of recog-nized sentences.
In Proc.
of ICASSP, pages 85?88,2005.
[9] R. Zhang, G. Kikui, and E. Sumita, ?Subword-basedtagging by conditional random fields for Chineseword segmentation,?
in Companion volume to theproceedings of the North American chapter of theAssociation for Computational Linguistics (NAACL),2006, pp.
193?196.
[10] H. Yamamoto and E. Sumita, ?Online languagemodel task adaptation for statistical machine transla-tion (in Japanese),?
in FIT2006, Fukuoka, Japan,2006, pp.
131?134.
[11] H. Kawai, T. Toda, J. Ni, and M. Tsuzaki.
XI-MERA: A new TTS from ATR based on corpus-based technologies.
In Proc.
of 5th ISCA SpeechSynthesis Workshop, 2004.
[12] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobaya-shi, and T. Kitamura.
Speech parameter generationalgorithms for HMM-based speech synthesis.
In Proc.of ICASSP, pages 1215?1218, 2000.
[13] T. Toda, H. Kawai, and M. Tsuzaki.
Optimizingsub-cost functions for segment selection based onperceptual evaluation in concatenative speech syn-thesis.
In Proc.
of ICASSP, pages 657?660, 2004.
[14] T. Takezawa and G. Kikui.
Collecting machine ?translation-aided bilingual dialogs for corpus-basedspeech translation.
In Proc.
of Eurospeech, pages2757?2760, 2003.
[15] G. Kikui, E. Sumita, T. Takezawa, and S. Yama-moto.
Creating corpora for speech-to-speech transla-tion.
In Proc.
Of Eurospeech, pages 381?384, 2003.
[16] T. Takezawa and G. Kikui.
A comparative study onhuman communication behaviors and linguistic char-acteristics for speech-to-speech translation.
In Proc.of LREC, pages 1589?1592, 2004.
[17] G. Kikui, T. Takezawa, M. Mizushima, S. Yama-moto, Y. Sasaki, H. Kawai, and S. Nakamura.
Moni-tor experiments of ATR speech-to-speech translationsystem.
In Proc.
of Autumn Meeting of the Acousti-cal Society of Japan, pages 1?7?10, 2005, in Japa-nese.28
