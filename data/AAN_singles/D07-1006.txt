Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
51?60, Prague, June 2007. c?2007 Association for Computational LinguisticsGetting the structure right for word alignment: LEAFAlexander FraserISI / University of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292fraser@isi.eduDaniel MarcuISI / University of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292marcu@isi.eduAbstractWord alignment is the problem of annotatingparallel text with translational correspon-dence.
Previous generative word alignmentmodels have made structural assumptionssuch as the 1-to-1, 1-to-N, or phrase-basedconsecutive word assumptions, while previ-ous discriminative models have either madesuch an assumption directly or used featuresderived from a generative model making oneof these assumptions.
We present a new gen-erative alignment model which avoids thesestructural limitations, and show that it iseffective when trained using both unsuper-vised and semi-supervised training methods.1 IntroductionSeveral generative models and a large number ofdiscriminatively trained models have been proposedin the literature to solve the problem of automaticword alignment of bitexts.
The generative propos-als have required unrealistic assumptions about thestructure of the word alignments.
Two assumptionsare particularly common.
The first is the 1-to-N as-sumption, meaning that each source word generateszero or more target words, which requires heuristictechniques in order to obtain alignments suitable fortraining a SMT system.
The second is the consec-utive word-based ?phrasal SMT?
assumption.
Thisdoes not allow gaps, which can be used to particularadvantage by SMT models which model hierarchi-cal structure.
Previous discriminative models haveeither made such assumptions directly or used fea-tures from a generative model making such an as-sumption.
Our objective is to automatically producealignments which can be used to build high qualitymachine translation systems.
These are presumablyclose to the alignments that trained bilingual speak-ers produce.
Human annotated alignments oftencontain M-to-N alignments, where several sourcewords are aligned to several target words and the re-sulting unit can not be further decomposed.
Sourceor target words in a single unit are sometimes non-consecutive.In this paper, we describe a new generative modelwhich directly models M-to-N non-consecutiveword alignments.
The rest of the paper is organizedas follows.
The generative story is presented, fol-lowed by the mathematical formulation.
Details ofthe unsupervised training procedure are described.The generative model is then decomposed into fea-ture functions used in a log-linear model which istrained using a semi-supervised algorithm.
Experi-ments show improvements in word alignment accu-racy and usage of the generated alignments in hier-archical and phrasal SMT systems results in an in-creased BLEU score.
Previous work is discussedand this is followed by the conclusion.2 LEAF: a generative word alignmentmodel2.1 Generative storyWe introduce a new generative story which enablesthe capture of non-consecutive M-to-N alignmentstructure.
We have attempted to use the same la-bels as the generative story for Model 4 (Brown et51al., 1993), which we are extending.Our generative story describes the stochastic gen-eration of a target string f (sometimes referred toas the French string, or foreign string) from a sourcestring e (sometimes referred to as the English string),consisting of l words.
The variable m is the lengthof f .
We generally use the index i to refer to sourcewords (ei is the English word at position i), and j torefer to target words.Our generative story makes the distinction be-tween different types of source words.
There arehead words, non-head words, and deleted words.Similarly, for target words, there are head words,non-head words, and spurious words.
A head wordis linked to zero or more non-head words; each non-head word is linked to from exactly one head word.The purpose of head words is to try to provide a ro-bust representation of the semantic features neces-sary to determine translational correspondence.
Thisis similar to the use of syntactic head words in sta-tistical parsers to provide a robust representation ofthe syntactic features of a parse sub-tree.A minimal translational correspondence consistsof a linkage between a source head word and a targethead word (and by implication, the non-head wordslinked to them).
Deleted source words are not in-volved in a minimal translational correspondence, asthey were ?deleted?
by the translation process.
Spu-rious target words are also not involved in a min-imal translational correspondence, as they sponta-neously appeared during the generation of other tar-get words.Figure 1 shows a simple example of the stochas-tic generation of a French sentence from an Englishsentence, annotated with the step number in the gen-erative story.1.
Choose the source word type.for each i = 1, 2, ..., l choose a word type?i = ?1 (non-head word), ?i = 0 (deletedword) or ?i = 1 (head word) according to thedistribution g(?i|ei)let ?0 = 12.
Choose the identity of the head word for eachnon-head word.for each i = 1, 2, ..., l if ?i = ?1 choose a?linked from head word?
value ?i (the positionof the head word which ei is linked to) accord-ing to the distribution w?1(?i ?
i|classe(ei))for each i = 1, 2, ..., l if ?i = 1 let ?i = ifor each i = 1, 2, ..., l if ?i = 0 let ?i = 0for each i = 1, 2, ..., l if ?
?i 6= 1 return ?fail-ure?3.
Choose the identity of the generated target headword for each source head word.for each i = 1, 2, ..., l if ?i = 1 choose ?i1according to the distribution t1(?i1|ei)4.
Choose the number of words in a target ceptconditioned on the identity of the source headword and the source cept size (?i is 1 if the ceptsize is 1, and 2 if the cept size is greater).for each i = 1, 2, ..., l if ?i = 1 choose a For-eign cept size ?i according to the distributions(?i|ei, ?i)for each i = 1, 2, ..., l if ?i < 1 let ?i = 05.
Choose the number of spurious words.choose ?0 according to the distributions0(?0|?i ?i)let m = ?0 +?li=1 ?i6.
Choose the identity of the spurious words.for each k = 1, 2, ..., ?0 choose ?0k accordingto the distribution t0(?0k)7.
Choose the identity of the target non-headwords linked to each target head word.for each i = 1, 2, ..., l and for each k =2, 3, ..., ?i choose ?ik according to the distribu-tion t>1(?ik|ei, classh(?i1))8.
Choose the position of the target head and non-head words.for each i = 1, 2, ..., l and for each k =1, 2, ..., ?i choose a position piik as follows:?
if k = 1 choose pii1 accord-ing to the distribution d1(pii1 ?c?i |classe(e?i), classf (?i1))?
if k = 2 choose pii2 according to the dis-tribution d2(pii2 ?
pii1|classf (?i1))52source absolutely [comma] they do not want to spend that moneyword type (1) DEL.
DEL.
HEAD non-head HEAD HEAD non-head HEAD HEAD HEADlinked from (2) THEY do NOT|| WANT to SPEND{{ THAT MONEYhead(3) ILS PAS DESIRENT DEPENSER CET ARGENTcept size(4) 1 2 1 1 1 1num spurious(5) 1spurious(6) aujourd?huinon-head(7) ILS PAS "" ne DESIRENT DEPENSER CET ARGENTplacement(8) aujourd?hui ILS ne DESIRENT PASww DEPENSER CET ARGENTspur.
placement(9) ILS ne DESIRENT PASww DEPENSER CET ARGENT aujourd?huiFigure 1: Generative story example, (number) indicates step number?
if k > 2 choose piik according to the dis-tribution d>2(piik ?
piik?1|classf (?i1))if any position was chosen twice, return ?fail-ure?9.
Choose the position of the spuriously generatedwords.for each k = 1, 2, ..., ?0 choose a position pi0kfrom ?0 ?
k + 1 remaining vacant positions in1, 2, ...,m according to the uniform distributionlet f be the string fpiik = ?ikWe note that the steps which return ?failure?
arerequired because the model is deficient.
Deficiencymeans that a portion of the probability mass in themodel is allocated towards generative stories whichwould result in infeasible alignment structures.
Ourmodel has deficiency in the non-spurious target wordplacement, just as Model 4 does.
It has addi-tional deficiency in the source word linking deci-sions.
(Och and Ney, 2003) presented results sug-gesting that the additional parameters required to en-sure that a model is not deficient result in inferiorperformance, but we plan to study whether this isthe case for our generative model in future work.Given e, f and a candidate alignment a, whichrepresents both the links between source and tar-get head-words and the head-word connections ofthe non-head words, we would like to calculatep(f, a|e).
The formula for this is:p(f, a|e) =[l?i=1g(?i|ei)][l?i=1?
(?i,?1)w?1(?i ?
i|classe(ei))][l?i=1?
(?i, 1)t1(?i1|ei)][l?i=1?
(?i, 1)s(?i|ei, ?i)][s0(?0|l?i=1?i)][?0?k=1t0(?0k)][l?i=1?i?k=2t>1(?ik|ei, classh(?i1))][l?i=1?i?k=1Dik(piik)]where:?
(i, i?)
is the Kronecker delta function which isequal to 1 if i = i?
and 0 otherwise.
?i is the position of the closest English head wordto the left of the word at i or 0 if there is no suchword.53classe(ei) is the word class of the English word atposition i, classf (fj) is the word class of the Frenchword at position j, classh(fj) is the word class ofthe French head word at position j.p0 and p1 are parameters describing the proba-bility of not generating and of generating a targetspurious word from each non-spurious target word,p0 + p1 = 1.m?
=l?i=1?i (1)s0(?0|m?)
=(m??0)pm??
?00 p?01 (2)Dik(j) =??????????????
?d1(j ?
c?i |classe(e?i), classf (?ik))if k = 1d2(j ?
pii1|classf (?ik))if k = 2d>2(j ?
piik?1|classf (?ik))if k > 2(3)?i = min(2,l?i?=1?(?i?
, i)) (4)ci ={ ceiling(?
?ik=1 piik/?i) if ?i 6= 00 if ?i = 0 (5)The alignment structure used in many other mod-els can be modeled using special cases of this frame-work.
We can express the 1-to-N structure of mod-els like Model 4 by disallowing ?i = ?1, while for1-to-1 structure we both disallow ?i = ?1 and de-terministically set ?i = ?i.
We can also specializeour generative story to the consecutive word M-to-Nalignments used in ?phrase-based?
models, thoughin this case the conditioning of the generation deci-sions would be quite different.
This involves addingchecks on source and target connection geometry tothe generative story which, if violated, would return?failure?
; naturally this is at the cost of additionaldeficiency.2.2 Unsupervised Parameter EstimationWe can perform maximum likelihood estimation ofthe parameters of this model in a similar fashionto that of Model 4 (Brown et al, 1993), describedthoroughly in (Och and Ney, 2003).
We use Viterbitraining (Brown et al, 1993) but neighborhood es-timation (Al-Onaizan et al, 1999; Och and Ney,2003) or ?pegging?
(Brown et al, 1993) could alsobe used.To initialize the parameters of the generativemodel for the first iteration, we use bootstrappingfrom a 1-to-N and a M-to-1 alignment.
We use theintersection of the 1-to-N and M-to-1 alignmentsto establish the head word relationship, the 1-to-Nalignment to delineate the target word cepts, and theM-to-1 alignment to delineate the source word cepts.In bootstrapping, a problem arises when we en-counter infeasible alignment structure where, for in-stance, a source word generates target words but nolink between any of the target words and the sourceword appears in the intersection, so it is not clearwhich target word is the target head word.
To ad-dress this, we consider each of the N generated tar-get words as the target head word in turn and assignthis configuration 1/N of the counts.For each iteration of training we search for theViterbi solution for millions of sentences.
Evidencethat inference over the space of all possible align-ments is intractable has been presented, for a sim-ilar problem, in (Knight, 1999).
Unlike phrase-based SMT, left-to-right hypothesis extension usinga beam decoder is unlikely to be effective because inword alignment reordering is not limited to a smalllocal window and so the necessary beam would bevery large.
We are not aware of admissible or inad-missible search heuristics which have been shown tobe effective when used in conjunction with a searchalgorithm similar to A* search for a model predict-ing over a structure like ours.
Therefore we usea simple local search algorithm which operates oncomplete hypotheses.
(Brown et al, 1993) defined two local search op-erations for their 1-to-N alignment models 3, 4 and5.
All alignments which are reachable via theseoperations from the starting alignment are consid-ered.
One operation is to change the generation de-cision for a French word to a different English word(move), and the other is to swap the generation de-cision for two French words (swap).
All possibleoperations are tried and the best is chosen.
This isrepeated.
The search is terminated when no opera-54tion results in an improvement.
(Och and Ney, 2003)discussed efficient implementation.In our model, because the alignment structure isricher, we define the following operations: moveFrench non-head word to new head, move Englishnon-head word to new head, swap heads of twoFrench non-head words, swap heads of two Englishnon-head words, swap English head word links oftwo French head words, link English word to Frenchword making new head words, unlink English andFrench head words.
We use multiple restarts to try toreduce search errors.
(Germann et al, 2004; Marcuand Wong, 2002) have some similar operations with-out the head word distinction.3 Semi-supervised parameter estimationEquation 6 defines a log-linear model.
Each featurefunction hm has an associated weight ?m.
Givena vector of these weights ?, the alignment searchproblem, i.e.
the search to return the best alignmenta?
of the sentences e and f according to the model, isspecified by Equation 7.p?
(f, a|e) = exp(?m ?mhm(a, e, f))?a?,f ?
exp(?m ?mhm(a?, e, f ?))(6)a?
= argmaxa?m?mhm(f, a, e) (7)We decompose the new generative model pre-sented in Section 2 in both translation directionsto provide the initial feature functions for our log-linear model, features 1 to 10 and 16 to 25 in Table1.We use backoffs for the translation decisions (fea-tures 11 and 26 and the HMM translation tableswhich are features 12 and 27) and the target cept sizedistributions (features 13, 14, 28 and 29 in Table 1),as well as heuristics which directly control the num-ber of unaligned words we generate (features 15 and30 in Table 1).We use the semi-supervised EMD algorithm(Fraser and Marcu, 2006b) to train the model.
Theinitial M-step bootstraps parameters as described inSection 2.2 from a M-to-1 and a 1-to-N alignment.We then perform the D-step following (Fraser andA B CDnnnnnnnnnnnnnn E@@@@@@@~~~~~~~A B CDnnnnnnnnnnnnnn E@@@@@@@~~~~~~~Figure 2: Two alignments with the same transla-tional correspondenceMarcu, 2006b).
Given the feature function param-eters estimated in the M-step and the feature func-tion weights ?
determined in the D-step, the E-stepsearches for the Viterbi alignment for the full train-ing corpus.We use 1 ?
F-Measure as our error criterion.
(Fraser and Marcu, 2006a) established that it is im-portant to tune ?
(the trade-off between Precisionand Recall) to maximize performance.
In workingwith LEAF, we discovered a methodological prob-lem with our baseline systems, which is that twoalignments which have the same translational cor-respondence can have different F-Measures.
An ex-ample is shown in Figure 2.To overcome this problem we fully interlinked thetransitive closure of the undirected bigraph formedby each alignment hypothesized by our baselinealignment systems1.
This operation maps the align-ment shown to the left in Figure 2 to the alignmentshown to the right.
This operation does not changethe collection of phrases or rules extracted from ahypothesized alignment, see, for instance, (Koehn etal., 2003).
Working with this fully interlinked rep-resentation we found that the best settings of ?
were?
= 0.1 for the Arabic/English task and ?
= 0.4 forthe French/English task.4 Experiments4.1 Data SetsWe perform experiments on two large alignmentstasks, for Arabic/English and French/English datasets.
Statistics for these sets are shown in Table 2.All of the data used is available from the Linguis-tic Data Consortium except for the French/English1All of the gold standard alignments were fully interlinkedas distributed.
We did not modify the gold standard alignments.551 chi(?i|ei) source word type 9 d2(4j|classf (fj)) movement for left-most targetnon-head word2 ?
(4i|classe(ei)) choosing a head word 10 d>2(4j|classf (fj)) movement for subsequent targetnon-head words3 t1(fj |ei) head word translation 11 t(fj |ei) translation without dependency on word-type4 s(?i|ei, ?i) ?i is number of words in target cept 12 t(fj |ei) translation table from final HMM iteration5 s0(?0|Pi ?i) number of unaligned target words 13 s(?i|?i) target cept size without dependency onsource head word e6 t0(fj) identity of unaligned target words 14 s(?i|ei) target cept size without dependency on ?i7 t>1(fj |ei, classh(?i1)) non-head word translation 15 target spurious word penalty8 d1(4j|classe(e?
), classf (fj)) movement for targethead words16-30 (same features, other direction)Table 1: Feature functionsgold standard alignments which are available fromthe authors.4.2 ExperimentsTo build all alignment systems, we start with 5 iter-ations of Model 1 followed by 4 iterations of HMM(Vogel et al, 1996), as implemented in GIZA++(Och and Ney, 2003).For all non-LEAF systems, we take the best per-forming of the ?union?, ?refined?
and ?intersection?symmetrization heuristics (Och and Ney, 2003) tocombine the 1-to-N and M-to-1 directions resultingin a M-to-N alignment.
Because these systems donot output fully linked alignments, we fully link theresulting alignments as described at the end of Sec-tion 3.
The reader should recall that this does notchange the set of rules or phrases that can be ex-tracted using the alignment.We perform one main comparison, which is ofsemi-supervised systems, which is what we will useto produce alignments for SMT.
We compare semi-supervised LEAF with a previous state of the artsemi-supervised system (Fraser and Marcu, 2006b).We performed translation experiments on the align-ments generated using semi-supervised training toverify that the improvements in F-Measure result inincreases in BLEU.We also compare the unsupervised LEAF sys-tem with GIZA++ Model 4 to give some idea ofthe performance of the unsupervised model.
Wemade an effort to optimize the free parameters ofGIZA++, while for unsupervised LEAF there areno free parameters to optimize.
A single iterationof unsupervised LEAF2 is compared with heuristic2Unsupervised LEAF is equivalent to using the log-linearmodel and setting ?m = 1 for m = 1 to 10 and m = 16 to 25,symmetrization of GIZA++?s extension of Model 4(which was run for four iterations).
LEAF was boot-strapped as described in Section 2.2 from the HMMViterbi alignments.Results for the experiments on the French/Englishdata set are shown in Table 3.
We ran GIZA++for four iterations of Model 4 and used the ?re-fined?
heuristic (line 1).
We ran the baseline semi-supervised system for two iterations (line 2), and incontrast with (Fraser and Marcu, 2006b) we foundthat the best symmetrization heuristic for this sys-tem was ?union?, which is most likely due to ouruse of fully linked alignments which was discussedat the end of Section 3.
We observe that LEAFunsupervised (line 3) is competitive with GIZA++(line 1), and is in fact competitive with the baselinesemi-supervised result (line 2).
We ran the LEAFsemi-supervised system for two iterations (line 4).The best result is the LEAF semi-supervised system,with a gain of 1.8 F-Measure over the LEAF unsu-pervised system.For French/English translation we use a state ofthe art phrase-based MT system similar to (Och andNey, 2004; Koehn et al, 2003).
The translation testdata is described in Table 2.
We use two trigram lan-guage models, one built using the English portion ofthe training data and the other built using additionalEnglish news data.
The BLEU scores reported inthis work are calculated using lowercased and tok-enized data.
For semi-supervised LEAF the gain of0.46 BLEU over the semi-supervised baseline is notstatistically significant (a gain of 0.78 BLEU wouldbe required), but LEAF semi-supervised comparedwith GIZA++ is significant, with a gain of 1.23BLEU.
We note that this shows a large gain in trans-while setting ?m = 0 for other values of m.56ARABIC/ENGLISH FRENCH/ENGLISHA E F ETRAININGSENTS 6,609,162 2,842,184WORDS 147,165,003 168,301,299 75,794,254 67,366,819VOCAB 642,518 352,357 149,568 114,907SINGLETONS 256,778 158,544 60,651 47,765ALIGN DISCR.SENTS 1,000 110WORDS 26,882 37,635 1,888 1,726LINKS 39,931 2,292ALIGN TESTSENTS 83 110WORDS 1,510 2,030 1,899 1,716LINKS 2,131 2,176TRANS.
DEV SENTS 728 (4 REFERENCES) 833 (1 REFERENCE)WORDS 18,255 22.0K TO 24.6K 20,562 17,454TRANS.
TEST SENTS 1,056 (4 REFERENCES) 2,380 (1 REFERENCE)WORDS 28,505 35.8K TO 38.1K 58,990 49,182Table 2: Data setslation quality over that obtained using GIZA++ be-cause BLEU is calculated using only a single refer-ence for the French/English task.Results for the Arabic/English data set are alsoshown in Table 3.
We used a large gold standardword alignment set available from the LDC.
We ranGIZA++ for four iterations of Model 4 and used the?union?
heuristic.
We compare GIZA++ (line 1)with one iteration of the unsupervised LEAF model(line 2).
The unsupervised LEAF system is worsethan four iterations of GIZA++ Model 4.
We be-lieve that the features in LEAF are too high dimen-sional to use for the Arabic/English task without thebackoffs available in the semi-supervised models.The baseline semi-supervised system (line 3) wasrun for three iterations and the resulting alignmentswere combined with the ?union?
heuristic.
We ranthe LEAF semi-supervised system for two iterations.The best result is the LEAF semi-supervised system(line 4), with a gain of 5.4 F-Measure over the base-line semi-supervised system.For Arabic/English translation we train a state ofthe art hierarchical model similar to (Chiang, 2005)using our Viterbi alignments.
The translation testdata used is described in Table 2.
We use two tri-gram language models, one built using the Englishportion of the training data and the other built usingadditional English news data.
The test set is from theNIST 2005 translation task.
LEAF had the best per-formance scoring 1.43 BLEU better than the base-line semi-supervised system, which is statisticallysignificant.5 Previous WorkThe LEAF model is inspired by the literature on gen-erative modeling for statistical word alignment andparticularly by Model 4 (Brown et al, 1993).
Muchof the additional work on generative modeling of 1-to-N word alignments is based on the HMM model(Vogel et al, 1996).
(Toutanova et al, 2002) and(Lopez and Resnik, 2005) presented a variety of re-finements of the HMM model particularly effectivefor low data conditions.
(Deng and Byrne, 2005)described work on extending the HMM model us-ing a bigram formulation to generate 1-to-N align-ment structure.
The common thread connectingthese works is their reliance on the 1-to-N approx-imation, while we have defined a generative modelwhich does not require use of this approximation, atthe cost of having to rely on local search.There has also been work on generative modelsfor other alignment structures.
(Wang and Waibel,1998) introduced a generative story based on ex-tension of the generative story of Model 4.
Thealignment structure modeled was ?consecutive Mto non-consecutive N?.
(Marcu and Wong, 2002)defined the Joint model, which modeled consec-utive word M-to-N alignments.
(Matusov et al,2004) presented a model capable of modeling 1-to-N and M-to-1 alignments (but not arbitrary M-to-N alignments) which was bootstrapped from Model4.
LEAF directly models non-consecutive M-to-Nalignments.One important aspect of LEAF is its symmetry.
(Och and Ney, 2003) invented heuristic symmetriza-57FRENCH/ENGLISH ARABIC/ENGLISHSYSTEM F-MEASURE (?
= 0.4) BLEU F-MEASURE (?
= 0.1) BLEUGIZA++ 73.5 30.63 75.8 51.55(FRASER AND MARCU, 2006B) 74.1 31.40 79.1 52.89LEAF UNSUPERVISED 74.5 72.3LEAF SEMI-SUPERVISED 76.3 31.86 84.5 54.34Table 3: Experimental Resultstion of the output of a 1-to-N model and a M-to-1model resulting in a M-to-N alignment, this was ex-tended in (Koehn et al, 2003).
We have used in-sights from these works to help determine the struc-ture of our generative model.
(Zens et al, 2004)introduced a model featuring a symmetrized lexi-con.
(Liang et al, 2006) showed how to train twoHMM models, a 1-to-N model and a M-to-1 model,to agree in predicting all of the links generated, re-sulting in a 1-to-1 alignment with occasional rare 1-to-N or M-to-1 links.
We improve on these works bychoosing a new structure for our generative model,the head word link structure, which is both sym-metric and a robust structure for modeling of non-consecutive M-to-N alignments.In designing LEAF, we were also inspired bydependency-based alignment models (Wu, 1997;Alshawi et al, 2000; Yamada and Knight, 2001;Cherry and Lin, 2003; Zhang and Gildea, 2004).
Incontrast with their approaches, we have a very flat,one-level notion of dependency, which is bilinguallymotivated and learned automatically from the paral-lel corpus.
This idea of dependency has some sim-ilarity with hierarchical SMT models such as (Chi-ang, 2005).The discriminative component of our work isbased on a plethora of recent literature.
This lit-erature generally views the discriminative modelingproblem as a supervised problem involving the com-bination of heuristically derived feature functions.These feature functions generally include the predic-tion of some type of generative model, such as theHMM model or Model 4.
A discriminatively trained1-to-N model with feature functions specifically de-signed for Arabic was presented in (Ittycheriah andRoukos, 2005).
(Lacoste-Julien et al, 2006) createda discriminative model able to model 1-to-1, 1-to-2 and 2-to-1 alignments for which the best resultswere obtained using features based on symmetricHMMs trained to agree, (Liang et al, 2006), andintersected Model 4.
(Ayan and Dorr, 2006) de-fined a discriminative model which learns how tocombine the predictions of several alignment algo-rithms.
The experiments performed included Model4 and the HMM extensions of (Lopez and Resnik,2005).
(Moore et al, 2006) introduced a discrimi-native model of 1-to-N and M-to-1 alignments, andsimilarly to (Lacoste-Julien et al, 2006) the best re-sults were obtained using HMMs trained to agreeand intersected Model 4.
LEAF is not bound bythe structural restrictions present either directly inthese models, or in the features derived from thegenerative models used.
We also iterate the gener-ative/discriminative process, which allows the dis-criminative predictions to influence the generativemodel.Our work is most similar to work using discrim-inative log-linear models for alignment, which issimilar to discriminative log-linear models used forthe SMT decoding (translation) problem (Och andNey, 2002; Och, 2003).
(Liu et al, 2005) presenteda log-linear model combining IBM Model 3 trainedin both directions with heuristic features which re-sulted in a 1-to-1 alignment.
(Fraser and Marcu,2006b) described symmetrized training of a 1-to-N log-linear model and a M-to-1 log-linear model.These models took advantage of features derivedfrom both training directions, similar to the sym-metrized lexicons of (Zens et al, 2004), includingfeatures derived from the HMM model and Model4.
However, despite the symmetric lexicons, thesemodels were only able to optimize the performanceof the 1-to-N model and the M-to-1 model sepa-rately, and the predictions of the two models re-quired combination with symmetrization heuristics.We have overcome the limitations of that work bydefining new feature functions, based on the LEAFgenerative model, which score non-consecutive M-to-N alignments so that the final performance crite-rion can be optimized directly.586 ConclusionWe have found a new structure over which we canrobustly predict which directly models translationalcorrespondence commensurate with how it is usedin hierarchical SMT systems.
Our new generativemodel, LEAF, is able to model alignments whichconsist of M-to-N non-consecutive translational cor-respondences.
Unsupervised LEAF is comparablewith a strong baseline.
When coupled with a dis-criminative training procedure, the model leads toincreases between 3 and 9 F-score points in align-ment accuracy and 1.2 and 2.8 BLEU points in trans-lation accuracy over strong French/English and Ara-bic/English baselines.7 AcknowledgmentsThis work was partially supported under the GALEprogram of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011-06-C-0022.
Wewould like to thank the USC Center for High Per-formance Computing and Communications.ReferencesYaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John D. Lafferty, I. Dan Melamed, DavidPurdy, Franz J. Och, Noah A. Smith, and DavidYarowsky.
1999.
Statistical machine translation, finalreport, JHU workshop.Hiyan Alshawi, Srinivas Bangalore, and Shona Douglas.2000.
Learning dependency translation models as col-lections of finite state head transducers.
Computa-tional Linguistics, 26(1):45?60.Necip Fazil Ayan and Bonnie J. Dorr.
2006.
A maxi-mum entropy approach to combining word alignments.In Proceedings of HLT-NAACL, pages 96?103, NewYork.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The mathematics ofstatistical machine translation: Parameter estimation.Computational Linguistics, 19(2):263?311.Colin Cherry and Dekang Lin.
2003.
A probabilitymodel to improve word alignment.
In Proceedings ofACL, pages 88?95, Sapporo, Japan.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL, pages 263?270, Ann Arbor, MI.Yonggang Deng and William Byrne.
2005.
Hmm wordand phrase alignment for statistical machine trans-lation.
In Proceedings of HLT-EMNLP, Vancouver,Canada.Alexander Fraser and Daniel Marcu.
2006a.
Measuringword alignment quality for statistical machine transla-tion.
In Technical Report ISI-TR-616, ISI/Universityof Southern California.Alexander Fraser and Daniel Marcu.
2006b.
Semi-supervised training for statistical word alignment.
InProceedings of COLING-ACL, pages 769?776, Syd-ney, Australia.Ulrich Germann, Michael Jahr, Kevin Knight, DanielMarcu, and Kenji Yamada.
2004.
Fast decoding andoptimal decoding for machine translation.
ArtificialIntelligence, 154(1-2):127?143.Abraham Ittycheriah and Salim Roukos.
2005.
A max-imum entropy word aligner for Arabic-English ma-chine translation.
In Proceedings of HLT-EMNLP,pages 89?96, Vancouver, Canada.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25(4):607?615.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofHLT-NAACL, pages 127?133, Edmonton, Canada.Simon Lacoste-Julien, Dan Klein, Ben Taskar, andMichael Jordan.
2006.
Word alignment via quadraticassignment.
In Proceedings of HLT-NAACL, pages112?119, New York, NY.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL,New York.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linearmodels for word alignment.
In Proceedings of ACL,pages 459?466, Ann Arbor, MI.Adam Lopez and Philip Resnik.
2005.
Improved hmmalignment models for languages with scarce resources.In Proceedings of the ACL Workshop on Building andUsing Parallel Texts, pages 83?86, Ann Arbor, MI.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine trans-lation.
In Proceedings of EMNLP, pages 133?139,Philadelphia, PA.Evgeny Matusov, Richard Zens, and Hermann Ney.2004.
Symmetric word alignments for statisticalmachine translation.
In Proceedings of COLING,Geneva, Switzerland.59Robert C. Moore, Wen-Tau Yih, and Andreas Bode.2006.
Improved discriminative bilingual word align-ment.
In Proceedings of COLING-ACL, pages 513?520, Sydney, Australia.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proceedings of ACL, pages295?302, Philadelphia, PA.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1):19?51.Franz J. Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30(1):417?449.Franz J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL,pages 160?167, Sapporo, Japan.Kristina Toutanova, H. Tolga Ilhan, and Christopher D.Manning.
2002.
Extensions to hmm-based statisticalword alignment models.
In Proceedings of EMNLP,Philadelphia, PA.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proceedings of COLING, pages 836?841,Copenhagen, Denmark.Ye-Yi Wang and Alex Waibel.
1998.
Modeling withstructures in statistical machine translation.
In Pro-ceedings of COLING-ACL, volume 2, pages 1357?1363, Montreal, Canada.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Kenji Yamada and Kevin Knight.
2001.
A syntax-basedstatistical translation model.
In Proceedings of ACL,pages 523?530, Toulouse, France.Richard Zens, Evgeny Matusov, and Hermann Ney.2004.
Improved word alignment using a symmetriclexicon model.
In Proceedings of COLING, Geneva,Switzerland.Hao Zhang and Daniel Gildea.
2004.
Syntax-basedalignment: Supervised or unsupervised?
In Proceed-ings of COLING, Geneva, Switzerland.60
