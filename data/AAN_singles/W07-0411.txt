Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 80?87,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsDependency-Based Automatic Evaluation for Machine TranslationKarolina Owczarzak Josef van Genabith Andy WayNational Centre for Language TechnologySchool of Computing, Dublin City UniversityDublin 9, Ireland{owczarzak,josef,away}@computing.dcu.ieAbstractWe present a novel method for evaluatingthe output of Machine Translation (MT),based on comparing the dependencystructures of the translation and referencerather than their surface string forms.
Ourmethod uses a treebank-based, wide-coverage, probabilistic Lexical-FunctionalGrammar (LFG) parser to produce a set ofstructural dependencies for eachtranslation-reference sentence pair, andthen calculates the precision and recall forthese dependencies.
Our dependency-based evaluation, in contrast to mostpopular string-based evaluation metrics,will not unfairly penalize perfectly validsyntactic variations in the translation.
Inaddition to allowing for legitimatesyntactic differences, we use paraphrasesin the evaluation process to account forlexical variation.
In comparison withother metrics on 16,800 sentences ofChinese-English newswire text, ourmethod reaches high correlation withhuman scores.
An experiment with twotranslations of 4,000 sentences fromSpanish-English Europarl shows that, incontrast to most other metrics, our methoddoes not display a high bias towardsstatistical models of translation.1 IntroductionSince their appearance, string-based evaluationmetrics such as BLEU (Papineni et al, 2002) andNIST (Doddington, 2002) have been the standardtools used for evaluating MT quality.
Both score acandidate translation on the basis of the number ofn-grams shared with one or more referencetranslations.
Automatic measures are indispensablein the development of MT systems, because theyallow MT developers to conduct frequent, cost-effective, and fast evaluations of their evolvingmodels.These advantages come at a price, though: anautomatic comparison of n-grams measures onlythe string similarity of the candidate translation toone or more reference strings, and will penalizeany divergence from them.
In effect, a candidatetranslation expressing the source meaningaccurately and fluently will be given a low score ifthe lexical and syntactic choices it contains, eventhough perfectly legitimate, are not present in atleast one of the references.
Necessarily, this scorewould differ from a much more favourable humanjudgement that such a translation would receive.The limitations of string comparison are thereason why it is advisable to provide multiplereferences for a candidate translation in BLEU- orNIST-based evaluations.
While Zhang and Vogel(2004) argue that increasing the size of the test setgives even more reliable system scores thanmultiple references, this still does not solve theinadequacy of BLEU and NIST for sentence-levelor small set evaluation.
In addition, in practiceeven a number of references do not capture thewhole potential variability of the translation.Moreover, when designing a statistical MT system,the need for large amounts of training data limitsthe researcher to collections of parallel corporasuch as Europarl (Koehn, 2005), which providesonly one reference, namely the target text; and thecost of creating additional reference translations ofthe test set, usually a few thousand sentences long,is often prohibitive.
Therefore, it would bedesirable to find an evaluation method that acceptslegitimate syntactic and lexical differences80between the translation and the reference, thusbetter mirroring human assessment.In this paper, we present a novel method thatautomatically evaluates the quality of translationbased on the dependency structure of the sentence,rather than its surface form.
Dependencies abstractaway from the particulars of the surface string (andCFG tree) realization and provide a ?normalized?representation of (some) syntactic variants of agiven sentence.
The translation and reference filesare analyzed by a treebank-based, probabilisticLexical-Functional Grammar (LFG) parser (Cahillet al, 2004), which produces a set of dependencytriples for each input.
The translation set iscompared to the reference set, and the number ofmatches is calculated, giving the precision, recall,and f-score for that particular translation.In addition, to allow for the possibility of validlexical differences between the translation and thereferences, we follow Kauchak and Barzilay(2006) and Owczarzak et al (2006) in adding anumber of paraphrases in the process of evaluationto raise the number of matches between thetranslation and the reference, leading to a higherscore.Comparing the LFG-based evaluation methodwith other popular metrics: BLEU, NIST, GeneralText Matcher (GTM) (Turian et al, 2003),Translation Error Rate (TER) (Snover et al,2006)1, and METEOR (Banerjee and Lavie, 2005),we show that combining dependencyrepresentations with paraphrases leads to a moreaccurate evaluation that correlates better withhuman judgment.The remainder of this paper is organized asfollows: Section 2 gives a basic introduction toLFG; Section 3 describes related work; Section 4describes our method and gives results of twoexperiments on different sets of data: 4,000sentences from Spanish-English Europarl and16,800 sentences of Chinese-English newswire textfrom the Linguistic Data Consortium?s (LDC)Multiple Translation project; Section 5 discussesongoing work; Section 6 concludes.1 As we focus on purely automatic metrics, we omitHTER (Human-Targeted Translation Error Rate) here.2 Lexical-Functional GrammarIn Lexical-Functional Grammar (Bresnan, 2001)sentence structure is represented in terms ofc(onstituent)-structure and f(unctional)-structure.C-structure represents the surface string word orderand the hierarchical organisation of phrases interms of CFG trees.
F-structures are recursivefeature (or attribute-value) structures, representingabstract grammatical relations, such as subj(ect),obj(ect), obl(ique), adj(unct), approximating topredicate-argument structure or simple logicalforms.
C-structure and f-structure are related interms of functional annotations (attribute-valuestructure equations) in c-structure trees, describingf-structures.While c-structure is sensitive to surface wordorder, f-structure is not.
The sentences Johnresigned yesterday and Yesterday, John resignedwill receive different tree representations, butidentical f-structures, shown in (1).
(1) C-structure:                         F-structure:SNP                      VP|JohnV               NP-TMP|                      |resigned       yesterdaySUBJ        PRED   johnNUM    sgPERS   3PRED       resignTENSE     pastADJ      {[PRED   yesterday]}SNP       NP       VP|                 |            |Yesterday  John        V|resignedSUBJ        PRED   johnNUM    sgPERS   3PRED       resignTENSE     pastADJ      {[PRED   yesterday]}Notice that if these two sentences were atranslation-reference pair, they would receive aless-than-perfect score from string-based metrics.For example, BLEU with add-one smoothing2gives this pair a score of barely 0.3781.The f-structure can also be described as a flatset of triples.
In triples format, the f-structure in (1)could be represented as follows: {subj(resign,john), pers(john, 3), num(john, sg), tense(resign,2 We use smoothing because the original BLEU giveszero points to sentences with fewer than one four-gram.81past), adj(resign, yesterday), pers(yesterday, 3),num(yesterday, sg)}.Cahill et al (2004) presents Penn-II Treebank-based LFG parsing resources.
Her approachdistinguishes 32 types of dependencies, includinggrammatical functions and morphologicalinformation.
This set can be divided into two majorgroups: a group of predicate-only dependenciesand non-predicate dependencies.
Predicate-onlydependencies are those whose path ends in apredicate-value pair, describing grammaticalrelations.
For example, for the f-structure in (1),predicate-only dependencies would include:{subj(resign, john), adj(resign, yesterday)}.3In parser evaluation, the quality of the f-structures produced automatically can be checkedagainst a set of gold standard sentences annotatedwith f-structures by a linguist.
The evaluation isconducted by calculating the precision and recallbetween the set of dependencies produced by theparser, and the set of dependencies derived fromthe human-created f-structure.
Usually, twoversions of f-score are calculated: one for all thedependencies for a given input, and a separate onefor the subset of predicate-only dependencies.In this paper, we use the parser developed byCahill et al (2004), which automatically annotatesinput text with c-structure trees and f-structuredependencies, reaching high precision and recallrates.
43 Related workThe insensitivity of BLEU and NIST to perfectlylegitimate syntactic and lexical variation has beenraised, among others, in Callison-Burch et al(2006), but the criticism is widespread.
Even thecreators of BLEU point out that it may notcorrelate particularly well with human judgment atthe sentence level (Papineni et al, 2002).
A side3 Other predicate-only dependencies include:apposition,  complement, open complement,coordination, determiner, object, second object,oblique, second oblique, oblique agent, possessive,quantifier, relative clause, topic, relative clausepronoun.
The remaining non-predicate dependenciesare: adjectival degree, coordination surface form, focus,complementizer forms: if, whether, and that, modal,number, verbal particle, participle, passive, person,pronoun surface form, tense, infinitival clause.4 http://lfg-demo.computing.dcu.ie/lfgparser.htmleffect of this phenomenon is that BLEU is lessreliable for smaller data sets, so the advantage itprovides in the speed of evaluation is to someextent counterbalanced by the time spent bydevelopers on producing a sufficiently large testset in order to obtain a reliable score for theirsystem.Recently a number of attempts to remedy theseshortcomings have led to the development of otherautomatic MT evaluation metrics.
Some of themconcentrate mainly on word order, like GeneralText Matcher (Turian et al, 2003), whichcalculates precision and recall for translation-reference pairs, weighting contiguous matchesmore than non-sequential matches, or TranslationError Rate (Snover et al, 2005), which computesthe number of substitutions, inserts, deletions, andshifts necessary to transform the translation text tomatch the reference.
Others try to accommodateboth syntactic and lexical differences between thecandidate translation and the reference, like CDER(Leusch et al, 2006), which employs a version ofedit distance for word substitution and reordering;or METEOR (Banerjee and Lavie, 2005), whichuses stemming and WordNet synonymy.
Kauchakand Barzilay (2006) and Owczarzak et al (2006)use paraphrases during BLEU and NIST evaluationto increase the number of matches between thetranslation and the reference; the paraphrases areeither taken from WordNet5 in Kauchak andBarzilay (2006) or derived from the test set itselfthrough automatic word and phrase alignment inOwczarzak et al (2006).
Another metric makinguse of synonyms is the linear regression modeldeveloped by Russo-Lassner et al (2005), whichmakes use of stemming, WordNet synonymy, verbclass synonymy, matching noun phrase heads, andproper name matching.
Kulesza and Schieber(2004), on the other hand, train a Support VectorMachine using features like proportion of n-grammatches and word error rate to judge a giventranslation?s distance from human-level quality.Nevertheless, these metrics use only string-based comparisons, even while taking intoconsideration reordering.
By contrast, ourdependency-based method concentrates onutilizing linguistic structure to establish acomparison between translated sentences and theirreference.5 http://wordnet.princeton.edu/824 LFG f-structure in MT evaluationThe process underlying the evaluation of f-structure quality against a gold standard can beused in automatic MT evaluation as well: we parsethe translation and the reference, and then, for eachsentence, we check the set of translationdependencies against the set of referencedependencies, counting the number of matches.
Asa result, we obtain the precision and recall scoresfor the translation, and we calculate the f-score forthe given pair.
Because we are comparing twooutputs that were produced automatically, there isa possibility that the result will not be noise-free.To assess the amount of noise that the parsermay introduce we conducted an experiment where100 English Europarl sentences were modified byhand in such a way that the position of adjunctswas changed, but the sentence remainedgrammatical and the meaning was not changed.This way, an ideal parser should give both thesource and the modified sentence the same f-structure, similarly to the case presented in (1).
Themodified sentences were treated like a translationfile, and the original sentences played the part ofthe reference.
Each set was run through the parser.We evaluated the dependency triples obtained fromthe ?translation?
against the dependency triples forthe ?reference?, calculating the f-score, and appliedother metrics (TER, METEOR, BLEU, NIST, andGTM) to the set in order to compare scores.
Theresults, inluding the distinction between f-scoresfor all dependencies and predicate-onlydependencies, appear in Table 1.baseline modifiedTER 0.0 6.417METEOR   1.0 0.9970BLEU 1.0000 0.8725NIST 11.5232 11.1704 (96.94%)GTM 100 99.18dep f-score  100 96.56dep_preds f-score 100 94.13Table 1.
Scores for sentences with reordered adjunctsThe baseline column shows the upper bound for agiven metric: the score which a perfect translation,word-for-word identical to the reference, wouldobtain.6 In the other column we list the scores thatthe metrics gave to the ?translation?
containingreordered adjunct.
As can be seen, the dependencyand predicate-only dependency scores are lowerthan the perfect 100, reflecting the noiseintroduced by the parser.To show the difference between the scoringbased on LFG dependencies and other metrics inan ideal situation, we created another set of ahundred sentences with reordered adjuncts, but thistime selecting only those reordered sentences thatwere given the same set of dependencies by theparser (in other words, we simulated having theideal parser).
As can be seen in Table 2, othermetrics are still unable to tolerate legitimatevariation in the position of adjuncts, because thesentence surface form differs from the reference;however, it is not treated as an error by the parser.baseline modifiedTER 0.0 7.841METEOR   1.0 0.9956BLEU 1.0000 0.8485NIST 11.1690 10.7422 (96.18%)GTM 100 99.35dep f-score  100 100dep_preds f-score 100 100Table 2.
Scores for sentences with reordered adjuncts inan ideal situation4.1 Initial experiment ?
EuroparlIn the first experiment, we attempted to determinewhether the dependency-based measure is biasedtowards statistical MT output, a problem that hasbeen observed for n-gram-based metrics likeBLEU and NIST.
Callison-Burch et al (2006)report that BLEU and NIST favour n-gram-basedMT models such as Pharaoh (Koehn, 2004), so thetranslations produced by rule-based systems scorelower on the automatic evaluation, even thoughhuman judges consistently rate their output higherthan Pharaoh?s translation.
Others repeatedly6 Two things have to be noted here: (1) in case of NISTthe perfect score differs from text to text, which is whywe provide the percentage points as well, and (2) in caseof TER the lower the score, the better the translation, sothe perfect translation will receive 0, and there is noupper bound on the score, which makes this particularmetric extremely difficult to directly compare withothers.83observed this tendency in previous research aswell; in one experiment, reported in Owczarzak etal.
(2006), where the rule-based systemLogomedia7 was compared with Pharaoh, BLEUscored Pharaoh 0.0349 points higher, NIST scoredPharaoh 0.6219 points higher, but human judgesscored Logomedia output 0.19 points higher (on a5-point scale).4.1.1 Experimental designIn order to check for the existence of a bias in thedependency-based metric, we created a set of4,000 sentences drawn randomly from the Spanish-English subset of Europarl (Koehn, 2005), and weproduced two translations: one by a rule-basedsystem Logomedia, and the other by the standardphrase-based statistical decoder Pharaoh, usingalignments produced by GIZA++8 and the refinedword alignment strategy of Och and Ney (2003).The translations were scored with a range ofmetrics: BLEU, NIST, GTM, TER, METEOR, andthe dependency-based method.4.1.2 Adding synonymsBesides the ability to allow syntactic variants asvalid translations, a good metric should also beable to accept legitimate lexical variation.
Weintroduced synonyms and paraphrases into theprocess of evaluation, creating new best-matchingreferences for the translations using eitherparaphrases derived from the test set itself(following Owczarzak et al (2006)) or WordNetsynonyms (as in Kauchak and Barzilay (2006)).Bitext-derived paraphrasesOwczarzak et al (2006) describe a simple way toproduce a list of paraphrases, which can be usefulin MT evaluation, by running word alignmentsoftware on the test set that is being evaluated.Paraphrases derived in this way are specific to thedomain at hand and contain low-level syntacticvariants in addition to word-level synonymy.Using the standard GIZA++ software and therefined word alignment strategy of Och and Ney(2003) on our test set of 4,000 Spanish-Englishsentences, the method generated paraphrases forjust over 1100 items.
These paraphrases served to7 http://www.lec.com/8 http://www.fjoch.com/GIZA++create new individual best-matching references forthe Logomedia and Pharaoh translations.
Due tothe small size of the paraphrase set, only about20% of reference sentences were actually modifiedto better reflect the translation.
This, in turn, led tolittle difference in scores.WordNet synonymsTo maximize the number of matches between atranslation and a reference, Kauchak and Barzilay(2006) use WordNet synonyms during evaluation.In addition, METEOR also has an option ofincluding WordNet in the evaluation process.
As inthe case of bitext-derived paraphrases, we usedWordNet synonyms to create new best-matchingreferences for each of the two translations.
Thistime, given the extensive database containingsynonyms for over 150,000 items, around 70% ofreference sentences were modified: 67% forPharaoh, and 75% for Logomedia.
Note that thenumber of substitutions is higher for Logomedia;this confirms the intuition that the translationproduced by Pharaoh, trained on the domain whichis also the source of the reference text, will needfewer lexical replacements than Logomedia, whichis based on a general non-domain-specific model.4.1.3 ResultsTable 3 shows the difference between the scoreswhich Pharaoh?s and Logomedia?s translationsobtained from each metric: a positive numbershows by how much Pharaoh?s score was higherthan Logomedia?s, and a negative number reflectsLogomedia?s higher score (the percentages areabsolute values).
As can be seen, all the metricsscored Pharaoh higher, inlcuding METEOR andthe dependency-based method that were boostedwith WordNet.
The values in the table are sorted indescending order, from the largest to the lowestadvantage of Pharaoh over Logomedia.Interestingly, next to METEOR boosted withWordNet, it is the dependency-based method, andespecially the predicates-only version, that showsthe least bias towards the phrase-based translation.In the next step, we selected from this set smallersubsets of sentences that were more and moresimilar in terms of translation quality (asdetermined by a sentence?s BLEU score).
As thesimilarity of the translation quality increased, mostmetrics lowered their bias, as is shown in Table 4.The first column shows the case where thesentences chosen differed at the most by 0.0584points BLEU score; in the second column thedifference was lowered to 0.01; and in the thirdcolumn to 0.005.
The numbers following the hashsigns in the header row indicate the number ofsentences in a given set.metric PH score ?
LM scoreTER 1.997BLEU 7.16%NIST 6.58%dep 4.93%dep+paraphr 4.80%GTM 3.89%METEOR 3.80%dep_preds 3.79%dep+paraphr_preds 3.70%dep+WordNet 3.55%dep+WordNet_preds 2.60%METEOR+WordNet 1.56%Table 3.
Difference between scores assigned to Pharaohand Logomedia.
Positive numbers show by how muchPharaoh?s score was higher than Logomedia?s.
Legend:dep = dependency f-score, paraph = paraphrases, _preds =predicate-only f-score.~ 0.05 #1692 ~ 0.01 #567 ~ 0.005 #335NIST 2.29% NIST 1.76% NIST 1.48%BLEU 0.95% BLEU 0.42% BLEU 0.59%GTM 0.94% GTM 0.29% GTM -0.09%d+p 0.67% d 0.04% d+p -0.15%d 0.61% d+p 0.02% d -0.24%d+WN -0.29% d+WN -0.78% d+WN -0.99%d+p_pr -0.70% M -0.99% d+p_pr -1.30%d_pr -0.75% d_pr -1.37% d_pr -1.43%M -1.03% d+p_pr -1.38% M -1.57%d+WN_pr -1.43% d+WN_pr -1.97% d+WN_pr -1.94%M+WN -2.51% M+WN -2.21% M+WN -2.74%TER -1.579 TER -1.228 TER -1.739Table 4.
Difference between scores assigned to Pharaohand Logomedia for sets of increasing similarity.
Positivenumbers show Pharaoh?s advantage, negative numbersshow Logomedia?s advantage.
Legend: d = dependency f-score, p = paraphrases, _pr = predicate-only f-score, M =METEOR, WN = WordNet.These results confirm earlier suggestions thatthe predicate-only version of the dependency-based evaluation is less biased in favour of thestatistical MT system than the version that includesall dependency types.
Adding a sufficient numberof lexical choices reduces the bias even further;although again, paraphrases generated from the testset only are too few to make a significantdifference.
Similarly to METEOR, thedependency-based method shows on the wholelower bias than other metrics.
However, we cannotbe certain that the underlying scores vary linearlywith each other and with human judgements, as wehave no framework of reference such as humansegment-level assessment of translation quality inthis case.
Therefore, the correlation with humanjudgement is analysed in our next experiment.4.2 Correlation with human judgement ?MultiTransTo calculate how well the dependency-basedmethod correlates with human judgement, and howit compares to the correlation shown by othermetrics, we conducted an experiment on Chinese-English newswire text.4.2.1 Experimental designWe used the data from the Linguistic DataConsortium Multiple Translation Chinese (MTC)Parts 2 and 4.
The data consists of multipletranslations of Chinese newswire text, four human-produced references, and segment-level humanscores for a subset of the translation-referencepairs.
Although a single translated segment wasalways evaluated by more than one judge, thejudges used a different reference every time, whichis why we treated each translation-reference-human score triple as a separate segment.
In effect,the test set created from this data contained 16,800segments.
As in the previous experiment, thetranslation was scored using BLEU, NIST, GTM,TER, METEOR, and the dependency-basedmethod.4.2.2 ResultsWe calculated Pearson?s correlation coefficient forsegment-level scores that were given by eachmetric and by human judges.
The results of thecorrelation are shown in Table 5.
Note that thecorrelation for TER is negative, because in TERzero is the perfect score, in contrast to othermetrics where zero is the worst possible score;however, this time the absolute values can beeasily compared to each other.
Rows are ordered85by the highest value of the (absolute) correlationwith the human score.First, it seems like none of the metrics is verygood at reflecting human fluency judgments; thecorrelation values in the first column aresignificantly lower than the correlation withaccuracy.
However, the dependency-based methodin almost all its versions has decidedly the highestcorrelation in this area.
This can be explained bythe method?s sensitivity to the grammaticalstructure of the sentence: a more grammaticaltranslation is also a translation that is more fluent.H_FL  H_AC  H_AVEd+WN 0.168 M+WN 0.294 M+WN 0.255d   0.162 M   0.278 d+WN 0.244d+WN_pr 0.162 NIST 0.273 M   0.242BLEU 0.155 d+WN 0.266 NIST 0.238d_pr 0.154 GTM 0.260 d   0.236M+WN 0.153 d  0.257 GTM 0.230M   0.149 d+WN_pr 0.232 d+WN_pr 0.220NIST 0.146 d_pr 0.224 d_pr 0.212GTM 0.146 BLEU 0.199 BLEU 0.197TER -0.133 TER -0.192 TER -0.182Table 5.
Pearson?s correlation between human scores andevaluation metrics.
Legend: d = dependency f-score, _pr =predicate-only f-score, M = METEOR, WN = WordNet,H_FL = human fluency score, H_AC = human accuracyscore, H_AVE = human average score.9Second, and somewhat surprisingly, in thisdetailed examination the relative order of themetrics changed.
The predicate-only version of thedependency-based method appears to be lessadequate for correlation with human scores than itsnon-restricted versions.
As to the correlation withhuman evaluation of translation accuracy, ourmethod currently falls short of METEOR and evenNIST.
This is caused by the fact that bothMETEOR and NIST assign relatively littleimportance to the position of a specific word in asentence, therefore rewarding the translation forcontent rather than linguistic form.
For ourdependency-based method, the noise introduced bythe parser might be the reason for low correlation:if even one side of the translation-reference paircontains parsing errors, this may lead to a lessreliable score.
An obvious solution to this problem,9 In general terms, an increase of 0.015 between any twoscores is significant with a 95% confidence interval.which we are examining at the moment, is toinclude a number of best parses for each side of theevaluation.High correlation with human judgements offluency and lower correlation with accuracy resultsin a high second place for our dependency-basedmethod when it comes to the average correlationcoefficient.
The WordNet-boosted dependency-based method scores only slightly lower thanMETEOR with WordNet.
These results are veryencouraging, especially as we see a number ofways the dependency-based method could befurther developed.5 Current and future workWhile the idea of a dependency-based method is anatural step in the direction of a deeper linguisticanalysis for MT evaluation, it does require an LFGgrammar and parser for the target language.
Thereare several obvious areas for improvement withrespect to the method itself.
First, we would alsolike to adapt the process of translation-referencedependency comparison to include n-best parsersfor the input sentences, as well as some basictransformations which would allow an even deeperlogical analysis of input (e.g.
passive to activevoice transformation).Second, we want to repeat bothexperiments using a paraphrase set derived from alarge parallel corpus, rather than the test set, asdescribed in Owczarzak et al (2006).
Whileretaining the advantage of having a similar size toa corresponding set of WordNet synonyms, this setwill also capture low-level syntactic variations,which can increase the number of matches and thecorrelation with human scores.Finally, we want to take advantage of thefact that the score produced by the dependency-based method is the proportional average of f-scores for a group of up to 32 (but usually farfewer) different dependency types.
We plan toimplement a set of weights, one for eachdependency type, trained in such a way as tomaximize the correlation of the final dependency f-score with human evaluation.6 ConclusionsIn this paper we present a novel way ofevaluating MT output.
So far, all metrics relied on86comparing translation and reference on a stringlevel.
Even given reordering, stemming, andsynonyms for individual words, current methodsare still far from reaching human ability to assessthe quality of translation.
Our method comparesthe sentences on the level of their grammaticalstructure, as exemplified by their f-structuredependency triples produced by an LFG parser.The dependency-based method can be furtheraugmented by using paraphrases or WordNetsynonyms, and is available in full version andpredicate-only version.
In our experiments weshowed that the dependency-based methodcorrelates higher than any other metric with humanevaluation of translation fluency, and shows highcorrelation with the average human score.
The useof dependencies in MT evaluation is a rather newidea and requires more research to improve it, butthe method shows potential to become an accurateevaluation metric.AcknowledgementsThis work was partly funded by Microsoft IrelandPhD studentship  2006-8  for the first author of thepaper.
We would also like to thank our reviewersfor their insightful comments.
All remaining errorsare our own.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation withImproved Correlation with Human Judgments.Proceedings of the ACL 2005 Workshop on Intrinsicand Extrinsic Evaluation Measures for MT and/orSummarization: 65-73.Joan Bresnan.
2001.
Lexical-Functional Syntax,Blackwell, Oxford.Aoife Cahill, Michael Burke, Ruth O?Donovan, Josefvan Genabith, and Andy Way.
2004.
Long-DistanceDependency Resolution in Automatically AcquiredWide-Coverage PCFG-Based LFG Approximations,In Proceedings of ACL-04: 320-327Chris Callison-Burch, Miles Osborne and PhilippKoehn.
2006.
Re-evaluating the role of BLEU inMachine Translation Research.
Proceedings ofEACL 2006: 249-256George Doddington.
2002.
Automatic Evaluation of MTQuality using N-gram Co-occurrence Statistics.Proceedings of HLT 2002: 138-145.David Kauchak and Regina Barzilay.
2006.Paraphrasing for Automatic Evaluation.
Proceedingsof HLT-NAACL 2006: 45-462.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translationmodels.
Proceedings of the AMTA 2004 Workshopon Machine Translation: From real users toresearch: 115-124.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
Proceedings of MTSummit 2005: 79-86.Alex Kulesza and Stuart M. Shieber.
2004.
A learningapproach to improving sentence-level MT evaluation.In Proceedings of the TMI 2004: 75-84.Gregor Leusch, Nicola Ueffing and Hermann Ney.2006.
CDER: Efficient MT Evaluation Using BlockMovements.
Proceedings of EACL 2006: 241-248.Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Modes.Computational Linguistics, 29:19-51.Karolina Owczarzak, Declan Groves, Josef vanGenabith, and Andy Way.
2006.
Contextual Bitext-Derived Paraphrases in Automatic MT Evaluation.Proceedings of the HLT-NAACL 2006 Workshop onStatistical Machine Translation: 86-93.Kishore Papineni, Salim Roukos, Todd Ward, andWeiJing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofACL 2002: 311-318.Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.2005.
A Paraphrase-based Approach to MachineTranslation Evaluation.
Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, Universityof Maryland, College Park, MD.Mathew Snover, Bonnie Dorr, Richard Schwartz, JohnMakhoul, Linnea Micciula.
2006.
A Study ofTranslation Error Rate with Targeted HumanAnnotation.
Proceedings of AMTA 2006: 223-231.Joseph P. Turian, Luke Shen, and I. Dan Melamed.2003.
Evaluation of Machine Translation and ItsEvaluation.
Proceedings of MT Summit 2003: 386-393.Ying Zhang and Stephan Vogel.
2004.
Measuringconfidence intervals for the machine translationevaluation metrics.
Proceedings of TMI 2004: 85-94.87
