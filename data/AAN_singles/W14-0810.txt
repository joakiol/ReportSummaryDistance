Proceedings of the 10th Workshop on Multiword Expressions (MWE 2014), pages 62?66,Gothenburg, Sweden, 26-27 April 2014.c?2014 Association for Computational LinguisticsMickey Mouse is not a Phrase: Improving Relevance in E-Commerce withMultiword ExpressionsPrathyusha Senthil Kumar, Vamsi Salaka, Tracy Holloway King, and Brian JohnsonSearch ScienceeBay, Inc.San Jose, CA, USA{ prathykumar, vsalaka, tracyking, bjohnson } @ebay.comAbstractWe describe a method for detectingphrases in e-commerce queries.
The keyinsight is that previous buyer purchasingbehavior as well as the general distribu-tion of phrases in item titles must be usedto select phrases.
Many multiword ex-pression (mwe) phrases which might beuseful in other situations are not suitablefor buyer query phrases because relevantitems, as measured by purchases, do notcontain these terms as phrases.1 Phrase MWE in e-Commerce SearchProcessing buyers?
queries is key for successfule-commerce.
As with web search queries, e-commerce queries are shorter and have differentsyntactic patterns than standard written language.For a given query, the system must provide suffi-cient recall (i.e.
return all items relevant to the buy-ers?
query, regardless of the tokens used) and suffi-cient precision (i.e.
exclude items which are tokenmatches but not relevant for the query).
This paperlooks at how identifying phrases in buyer queriescan help with recall and precision in e-commerceat eBay.
We focus primarily on precision, whichis the harder problem to solve.Phrases are a sub-type of mwe: one wherethe tokens of the mwe appear strictly adjacent toone another and in a specified order ((Sag et al.,2002)?s words with spaces).The eBay product search engine takes buyerqueries and retrieves items relevant to the buyer?spurchasing intent.
The items are listed in cate-gories (e.g.
women?s dresses) and each item has atitle provided by the seller.
The buyer can chooseto sort the items by most relevant (e.g.
similarto web search ranking) or deterministically (e.g.price low to high).
There are versions of the e-commerce site for different countries such as US,UK, Germany, France, Poland, etc.
and so thequery processing is language-specific according tosite.
Here we report on incorporating phrases intoEnglish for the US and German for Germany.2 Controlling Retrieval via QueryPhrasesThe query processing system has three core capa-bilities1which expand tokens in the buyer?s queryinto other forms.
Both single and multiple to-kens can be expanded.
Token-to-token expan-sions (Jammalamadaka and Salaka, 2012) includeacronyms, abbreviations, inflectional variants (e.g.hats to hat), and space synonyms (e.g.
ray ban torayban).
Category expansions expand tokens toall items in a given category (e.g.
womens shoesretrieves all items in the Womens?
Shoes cate-gory).
Finally, attribute expansions map tokensto structured data (e.g.
red retrieves any item withColor=Reds in its structured data).
These expan-sions are used to increase the number of relevantitems brought back for a specific buyer query.Precision issues occur when a buyer?s query re-turns an item that is a spurious match.
For exam-ple, the query diamond ring size 10 matches allthe tokens in the title ?10 kt gold, size 7 diamondring?
even though it is not a size 10 ring.Recall issues occur when relevant items are notreturned for a buyer?s query.
The core capabilitiesof token-to-token mappings, category mappings,and attribute mapping largely address this.
How-ever, some query tokens are not covered by thesecapabilities.
For example, the query used cars forsale contains the tokens for sale which rarely oc-cur in e-commerce item titles.1Here we ignore tokenization, although the quality of thetokenizer affects the quality of all remaining components(Manning et al., 2008).622.1 Hypothesis: Phrasing within QueriesTo address these precision and recall issues, weprovide special treatment for phrases in queries.To address the precision issue where spuriousitems are returned, we require certain token se-quences to be treated as phrases.
For example, size10 will be phrased and hence only match itemswhose titles have those tokens in that order.
Toaddress the recall issue, we identify queries whichcontain phrases that can be dropped.
For exam-ple, in the query used cars for sale the tokens forsale can be dropped; similarly for German kaufen(buy) in the query waschtrockner kaufen (washer-dryer buy).
For the remainder of the paper we willuse the terminology:?
REQUIRED PHRASES: Token sequences re-quired to be phrases when used in queries(e.g.
apple tv)?
DROPPED PHRASES: Phrases which allowsub-phrase deletion (e.g.
used cars for sale)The required-phrases approach must be highconfidence since it will block items from being re-turned for the buyer?s query.We first mined candidate phrases for requiredphrases and for dropped phrases in queries.
Fromthis large set of candidates, we then used pastbuyer behavior to determine whether the candi-date was viable for application to queries (see(Ramisch et al., 2008) on mwe candidate evalu-ation in general).
As we will see, many phraseswhich seem to be intuitively well-formed mwecannot be used as e-commerce query phrases be-cause they would block relevant inventory frombeing returned (see (Diab et al., 2010) on mwe inNLP applications).The phrases which pass candidate selectionare then incorporated into the existing query ex-pansions (i.e.
token-to-token mappings, categorymappings, attribute mappings).
The phrases are anew type of token-to-token mapping which requirethe query tokens to appear in order and adjacent,i.e.
as a mwe phrase, or to be dropped.2.2 Phrase Candidate SelectionThe first stage of the algorithm is candidate selec-tion: from all the possible buyer query n-grams wedetermine which are potential mwe phrase candi-dates.
We use a straight-forward selection tech-nique in order to gather a large candidate set; atthis stage we are concerned with recall, not preci-sion, of the phrases.First consider required phrases.
For a givensite (US and Germany here), we consider all thebi- and tri-grams seen in buyer queries.
Sincee-commerce queries are relatively short, evenshorter than web queries, we do not considerlonger n-grams.
The most frequent of these arethen considered candidates.
Manual inspectionof the candidate set shows a variety of mwe se-mantic types.
As expected in the e-commerce do-main, these contain primarily nominal mwe: brandnames, product types, and measure phrases (see(?O S?eaghdha and Copestake, 2007) on identifyingnominal mwe).
Multiword verbs are non-existentin buyer queries and relatively few adjectives arecandidates (e.g.
navy blue, brand new).Next consider dropped phrases.
These arestop words specialized to the e-commerce domain.They are mined from behavioral logs by lookingat query-to-query transitions.
We consider querytransitions where buyers drop a word or phrase inthe transition and show increased engagement af-ter the transition.
For example, buyers issue thequery used cars for sale followed by the queryused cars and subsequently engage with the searchresults (e.g.
view or purchase items).
The most fre-quent n-grams identified by this approach are can-didates for dropped phrases and are contextuallydropped, i.e.
they are dropped when they are partsof specific larger phrases.
Query context is impor-tant because for sale should not be dropped whenpart of the larger phrase plastic for sale signs.2.3 Phrase Selection: Sorry MickeyOnce we have candidate phrases, we use buyerbehavioral data (Carterette et al., 2012) to deter-mine which phrases to require in buyer queries.For each query which contains a given phrase (e.g.for the candidate phrase apple tv consider queriessuch as apple tv, new apple tv, apple tv remote)we see which items were purchased.
Item titlesfrom purchased items which contain the phraseare referred to as ?phrase bought?
while item ti-tles shown in searches are ?phrase impressed?.
Weare interested only in high confidence phrases andso focus on purchase behavior: this signal is rela-tively sparse but is the strongest indicator of buyerinterest.
To determine the candidates, we want tocompute the conditional probability of an item be-ing bought (B(ought)) given a phrase (Ph(rase)).P (B|Ph) =P (Ph|B) ?
P (B)P (Ph)(1)63However, this is computationally intensive in thatall items retrieved for a query must be considered.In equation 1, P(Ph|B) is easy to compute sinceonly bought items are considered; P(Ph) can be ap-proximated by the ratio of phrases to non-phrasesfor bought items; P(B) is a constant and hence canbe ignored.
So, we use the following two metricsbased on these probabilities:?
SALE EFFICIENCY: Probability of phrases inbought items, P(Ph|B) > 95%.
Ensures qual-ity and acts as an upper bound for the ex-pected loss (equation 2).?
LIFT: Ensures phrasing has a positive rev-enue impact and handles presentation bias(equation 3).First consider sale efficiency:P (Ph|B) =P (Ph?B)P (B)=n(ph bought)n(bought)(2)One drawback of sale efficiency P(Ph|B) is datasparsity.
There is a high false positive rate inidentifying phrases when the frequency of boughtitems is low since it is hard to distinguish sig-nal from noise with a strict threshold.
We usedBeta-Binomial smoothing to avoid this (Schuck-ers, 2003; Agarwal et al., 2009).
Conceptually,by incorporating Beta-Binomial smoothing, wemodel the number of phrases bought as a binomialprocess and use the Beta distribution, which is itsconjugate prior, for smoothing the sale efficiency.However the sale efficiency as captured by theconditional probability of being bought as a phrase(equation 2) does not take into account the dis-tribution of the phrases in the retrieved set.
Forexample for the phrase apple tv, 80% of the im-pressed items contained the phrase while 99%of the bought items contained the phrase, whichmakes it an excellent phrase.
However, for mountrushmore 99% of the impressed items containedthe phrase while only 97% of the bought itemscontained the phrase.
This implies that the proba-bility of being bought as a phrase for mount rush-more is high because of presentation bias (i.e.
thevast majority of token matches contain phrases)and not because the phrase itself is an indicatorof relevance.
To address the issue of presentationbias in P(Ph|B), we use the following lift metric:P (Ph|B)?
P (Ph)P (Ph)> 0 (3)Lift (equation 3) measures the buyers?
tendencyto purchase phrase items.
For a good phrase thisvalue should be high.
For example, for apple tvthis value is +23.13% while for mount rushmore itis ?1.8%.
We only consider phrases that have apositive lift.Examples of English phrases for buyer queriesinclude apple tv, bubble wrap, playstation 3, 4 x4, tank top, nexus 4, rose gold, 1 gb, hot pack, 20v, kindle fire, hard rock and new balance and Ger-man phrases include geflochtene schnur (braidedline) and energiespar regler (energy-saving con-troller).
These form a disparate semantic set in-cluding brand names (new balance), product types(bubble wrap), and units of measure (1 gb).Consider the phrases which were not selectedbecause a significant percentage of the buyer de-mand was for items where the tokens appearedeither in a different order or not adjacent.
Theseinclude golf balls, hard drive and mickey mouse.You might ask, what could possibly be a strongerphrase in American English than mickey mouse?Closer examination of the buyer behavioral datashows that many buyers are using queries with thetokens mickey mouse to find and purchase mickeyand minnie mouse items.
The introduction of andminnie in the item titles breaks the query phrase.3 Experiment ResultsWe selected phrase candidates for two sites: TheUS and Germany.
These sites were selected be-cause there was significant query and purchasingdata which alleviates data sparsity issues and be-cause the language differences allowed us to testthe general applicability of the approach.2We created query assets which contained theexisting production assets and modified them toinclude the required phrases and the droppedphrases.
The relative query frequency of requiredphrases (blue) vs. dropped phrases (red) in eachexperiment is shown in Figure 2.US GermanyFigure 2: Impacted Query Frequency: red=drop-ped; blue=requiredFor US and Germany, 10% of users were ex-2English and German are closely related languages.
Weplan to apply mwe phrases to Russian and French.64Figure 1: US Phrase Query Impressions: Head-vs.-tail queriesposed to the new phrase assets, while a 10% con-trol3were exposed to the existing production as-sets.
The test was run for two weeks.
We mea-sured the number of items bought in test vs. con-trol, the revenue, and the behavior of new users.Bought items and revenue are both measured todetermine whether changes in purchases are com-ing from better deals (e.g.
bought items might in-crease while revenue is constant) or improved dis-covery (e.g.
more items are bought at the sameprice).
New user success is measured because newusers are generally sensitive to irrelevant items be-ing returned for their queries; the required phrasemwe in this experiment target this use case.As a result of the phrase experiment, in theUS, revenue, bought items, and new user engage-ment increased statistically significantly (p<0.1).The German test showed directionally similar re-sults but was only statistically significant for newbuyers.
We cannot show proprietary business re-sults, but both experiences are now in productionin place of the previous query processing.
Thegraph in Figure 1 shows the distribution of head-vs.-tail queries for the US with some sample af-fected head queries.4 Discussion and ConclusionWe described a relatively straight-forward methodfor detecting phrases in buyer queries.
The keyinsight is that previous buyer purchasing behavioras well as the distribution of phrases in item titlesmust be used to select which candidate phrasesto keep in the final analysis.
Many mwe phraseswhich might be useful in other situations (e.g.3Technically there were two 5% controls which werecompared to determine variability within the control group.our friend mickey mouse (?2.3)) are not suitablefor buyer queries because many relevant items, asmeasured by purchases, do not contain these to-kens phrases (e.g.
mickey and minnie mouse).Among the rejected candidate phrases, thehigher confidence ones are likely to be suitable forranking of the results even though they could notbe used to filter out results.
This is an area of ac-tive research: what mwe phrases can improve theranking of e-commerce results, especially giventhe presence of the phrase in the buyer query?Another method to increase phrase coverage is toconsider contextualized phrases, whereby tokensequences may be a phrase in one query but notin another.The experiments here were conducted on twoof our largest sites, thereby avoiding data spar-sity issues.
We have used the same algorithmon smaller sites such as Australia: the resultingrequired phrases and dropped phrases look rea-sonable but have not been tested experimentally.An interesting question is whether phrases fromsame-language sites (e.g.
UK, Australia, Canada,US) can be combined or whether a site with morebehavioral data can be used to learn phrases forsmaller sites.
The later has been done for Canadausing US data.In sum, mwe phrases improved eBay e-commerce, but it was important to use domain-specific data in choosing the relevant phrases.
Thissuggests that the utility of universal vs. domainspecific mwe is an area requiring investigation.65ReferencesDeepak Agarwal, Bee-Chung Chen, and PradheepElango.
2009.
Spatio-temporal models for esti-mating click-through rate.
In Proceedings of the18th International Conference on World Wide Web.ACM.Ben Carterette, Evangelos Kanoulas, Paul Clough, andMark Sanderson, editors.
2012.
Information Re-trieval Over Query Sessions.
Springer LectureNotes in Computer Science.Mona Diab, Valia Kordoni, and Hans Uszkoreit.
2010.Multiword expressions: From theory to applica-tions.
Panel at MWE2010.Ravi Chandra Jammalamadaka and Vamsi Salaka.2012.
Synonym mining and usage in e-commerce.Presented at ECIR.Christopher D. Manning, Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Diarmuid?O S?eaghdha and Ann Copestake.
2007.
Co-occurrence contexts for noun compound interpreta-tion.
In Proceedings of the Workshop on A BroaderPerspective on Multiword Expressions, pages 57?64.Association for Computational Linguistics.Carlos Ramisch, Paulo Schreiner, Marco Idiart, andAline Villavicencio.
2008.
An evaluation of meth-ods for the extraction of multiword expressions.
InTowards a Shared Task for Multiword Expressions,pages 50?53.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann A.Copestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for NLP.
In Pro-ceedings of the Third International Conference onComputational Linguistics and Intelligent Text Pro-cessing, CICLing ?02, pages 1?15.
Springer-Verlag.Michael E. Schuckers.
2003.
Using the beta-binomialdistribution to assess performance of a biometricidentification device.
International Journal of Im-age and Graphics, pages 523?529.66
