Proceedings of the 10th Conference on Parsing Technologies, pages 11?22,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsEvaluating Impact of Re-training a Lexical Disambiguation Modelon Domain Adaptation of an HPSG ParserTadayoshi Hara1 Yusuke Miyao1 Jun?ichi Tsujii1;2;31Department of Computer Science, University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo 113-0033 Japan2School of Computer Science, University of ManchesterPOBox 88, Sackville St, MANCHESTER M60 1QD, UK3NaCTeM(National Center for Text Mining)Manchester Interdisciplinary Biocentre, University of Manchester131 Princess St, MANCHESTER M1 7DN, UKE-mail: fharasan, yusuke, tsujiig@is.s.u-tokyo.ac.jpAbstractThis paper describes an effective approachto adapting an HPSG parser trained on thePenn Treebank to a biomedical domain.
Inthis approach, we train probabilities of lex-ical entry assignments to words in a tar-get domain and then incorporate them intothe original parser.
Experimental resultsshow that this method can obtain higherparsing accuracy than previous work on do-main adaptation for parsing the same data.Moreover, the results show that the combi-nation of the proposed method and the exist-ing method achieves parsing accuracy that isas high as that of an HPSG parser retrainedfrom scratch, but with much lower trainingcost.
We also evaluated our method in theBrown corpus to show the portability of ourapproach in another domain.1 IntroductionDomain portability is an important aspect of the ap-plicability of NLP tools to practical tasks.
There-fore, domain adaptation methods have recently beenproposed in several NLP areas, e.g., word sense dis-ambiguation (Chan and Ng, 2006), statistical pars-ing (Lease and Charniak, 2005; McClosky et al,2006), and lexicalized-grammar parsing (Johnsonand Riezler, 2000; Hara et al, 2005).
Their aim wasto re-train a probabilistic model for a new domain atlow cost, and more or less successfully improved theaccuracy for the domain.In this paper, we propose a method for adaptingan HPSG parser (Miyao and Tsujii, 2002; Ninomiyaet al, 2006) trained on the WSJ section of the PennTreebank (Marcus et al, 1994) to a biomedical do-main.
Our method re-trains a probabilistic model oflexical entry assignments to words in a target do-main, and incorporates it into the original parser.The model of lexical entry assignments is a log-linear model re-trained with machine learning fea-tures only of word n-grams.
Hence, the cost for there-training is much lower than the cost of trainingthe entire disambiguation model from scratch.In the experiments, we used an HPSG parser orig-inally trained with the Penn Treebank, and evaluateda disambiguation model re-trained with the GENIAtreebank (Kim et al, 2003), which consists of ab-stracts of biomedical papers.
We varied the size ofa training corpus, and measured the transition of theparsing accuracy and the cost required for parameterestimation.
For comparison, we also examined otherpossible approaches to adapting the same parser.
Inaddition, we applied our approach to the Brown cor-pus (Kucera and Francis, 1967) in order to examineportability of our approach.The experimental results revealed that by sim-ply re-training the probabilistic model of lexical en-try assignments we achieve higher parsing accuracythan with a previously proposed adaptation method.In addition, combined with the existing adaptationmethod, our approach achieves accuracy as high asthat obtained by re-training the original parser fromscratch, but with much lower training cost.
In thispaper, we report these experimental results in detail,and discuss how disambiguation models of lexicalentry assignments contribute to domain adaptation.In recent years, it has been shown that lexical in-11formation plays a very important role for high accu-racy of lexicalized grammar parsing.
Bangalore andJoshi (1999) indicated that, correct disambiguationwith supertagging, i.e., assignment of lexical entriesbefore parsing, enabled effective LTAG (Lexical-ized Tree-Adjoining Grammar) parsing.
Clark andCurran (2004a) showed that supertagging reducedcost for training and execution of a CCG (Combina-tory Categorial Grammar) parser while keeping ac-curacy.
Clark and Curran (2006) showed that a CCGparser trained on data derived from lexical categorysequences alone was only slightly less accurate thanone trained on complete dependency structures.
Ni-nomiya et al (2006) also succeeded in significantlyimproving speed and accuracy of HPSG parsing byusing supertagging probabilities.
These results indi-cate that the probability of lexical entry assignmentsis essential for parse disambiguation.Such usefulness of lexical information has alsobeen shown for domain adaptation methods.
Leaseand Charniak (2005) showed how existing domain-specific lexical resources on a target domain may beleveraged to augment PTB-training: part-of-speechtags, dictionary collocations, and named-entities.Our findings basically follow the above results.
Thecontribution of this paper is to provide empirical re-sults of the relationships among domain variation,probability of lexical entry assignment, training datasize, and training cost.
In particular, this paper em-pirically shows how much in-domain corpus is re-quired for satisfiable performance.In Section 2, we introduce an HPSG parser anddescribe an existing method for domain adaptation.In Section 3, we show our methods of re-traininga lexical disambiguation model and incorporatingit into the original model.
In Section 4, we exam-ine our method through experiments on the GENIAtreebank.
In Section 5, we examine the portabilityof our method through experiments on the Browncorpus.
In Section 6, we showed several recent re-searches related to domain adaptation.2 An HPSG ParserHPSG (Pollard and Sag, 1994) is a syntactic the-ory based on lexicalized grammar formalism.
InHPSG, a small number of grammar rules describegeneral construction rules, and a large number ofHEAD nounSUBCAT <>HEAD verbSUBCAT <verb>HEAD verbSUBCAT <noun>Grammar Rule31UnificationHEADSUBCAT < >12HEADSUBCAT < >32HEADSUBCAT < >HEAD nounSUBCAT <>HEAD verbSUBCAT <verb>HEAD verbSUBCAT <noun>John has comeHEAD verbSUBCAT <noun>HEAD nounSUBCAT <>HEAD verbSUBCAT <verb>HEAD verbSUBCAT <noun>Lexical EntriesJohn has comeJohn has comeFigure 1: Parsing a sentence ?John has come.
?HEAD verbSUBCAT <noun>HEAD nounSUBCAT <>HEAD verbSUBCAT <verb>HEAD verbSUBCAT <noun>John has comeHEAD verbSUBCAT <>Figure 2: An HPSG parse tree for a sentence ?Johnhas come.
?lexical entries express word-specific characteristics.The structures of sentences are explained using com-binations of grammar rules and lexical entries.Figure 1 shows an example of HPSG parsing ofthe sentence ?John has come.?
First, as shown at thetop of the figure, an HPSG parser assigns a lexicalentry to each word in this sentence.
Next, a gram-mar rule is assigned and applied to lexical entries.
Atthe middle of this figure, the grammar rule is appliedto the lexical entries for ?has?
and ?come.?
We thenobtain the structure represented at the bottom of thefigure.
After that, the application of grammar rulesis done iteratively, and then we can finally obtain theparse tree as is shown in Figure 2.
In practice, sincetwo or more parse candidates can be given for onesentence, a disambiguation model gives probabili-ties to these candidates, and a candidate given thehighest probability is then chosen as a correct parse.12The HPSG parser used in this study is Ninomiyaet al (2006), which is based on Enju (Miyao andTsujii, 2005).
Lexical entries of Enju were extractedfrom the Penn Treebank (Marcus et al, 1994), whichconsists of sentences collected from The Wall StreetJournal (Miyao et al, 2004).
The disambiguationmodel of Enju was trained on the same treebank.The disambiguation model of Enju is based ona feature forest model (Miyao and Tsujii, 2002),which is a log-linear model (Berger et al, 1996) onpacked forest structure.
The probability, pE(tjw),of producing the parse result t for a given sentencew = hw1; :::; wui is defined aspE(tjw) =1ZsYiplex(lijw; i)  qsyn(tjl);Zs=Xt2T (w)Yiplex(lijw; i)  qsyn(tjl)where l = hl1; :::; lui is a list of lexical entries as-signed to w, plex(lijw; i) is a probabilistic modelgiving the probability that lexical entry liis assignedto word wi, qsyn(tjl) is an unnormalized log-linearmodel of tree construction and gives the possibil-ity that parse candidate t is produced from lexicalentries l, and T (w) is a set of parse candidates as-signed to w. With a treebank of a target domain astraining data, model parameters of plexand qsynareestimated so as to maximize the log-likelihood of thetraining data.Probabilistic model plexis defined as a log-linearmodel as follows.plex(lijw; i) =1ZwiexpXjjfj(li;w; i)!
;Zwi=Xli2L(wi)expXjjfj(li;w; i)!
;where L(wi) is a set of lexical entries which canbe assigned to word wi.
Before training this model,L(wi) for all wiare extracted from the training tree-bank.
The feature function fj(li;w; i) represents thecharacteristics of li, w and wi, while correspondingjis its weight.
For the feature functions, instead ofusing unigram features adopted in Miyao and Tsujii(2005), Ninomiya et al (2006) used ?word trigram?and ?POS 5-gram?
features which are listed in Ta-ble 1.
With the revised Enju model, they achievedTable 1: Features for the probabilities of lexical en-try selectionsurrounding words w 1w0w1(word trigram)surrounding POS tags p 2p 1p0p1p2(POS 5-gram)combinations w 1w0; w0w1; p 1w0; p0w0;p1w0; p0p1p2p3; p 2p 1p0;p 1p0p1; p0p1p2; p 2p 1;p 1p0; p0p1; p1p2parsing accuracy as high as Miyao and Tsujii (2005),with around four times faster parsing speed.Johnson and Riezler (2000) suggested the pos-sibility of the method for adapting a stochasticunification-based grammar including HPSG to an-other domain.
They incorporated auxiliary distribu-tions as additional features for an original log-linearmodel, and then attempted to assign proper weightsto the new features.
With this approach, they suc-ceeded in decreasing to a degree indistinguishablesentences for a target grammar.Our previous work proposed a method for adapt-ing an HPSG parser trained on the Penn Treebankto a biomedical domain (Hara et al, 2005).
Were-trained a disambiguation model of tree construc-tion, i.e., qsyn, for the target domain.
In this ap-proach, qsynof the original parser was used as areference distribution (Jelinek, 1998) of another log-linear model, and the new model was trained using atarget treebank.
Since re-training used only a smalltreebank of the target domain, the cost was small andparsing accuracy was successfully improved.3 Re-training of a Disambiguation Modelof Lexical Entry AssignmentsOur idea of domain adaptation is to train a disam-biguation model of lexical entry assignments for thetarget domain and then incorporate it into the origi-nal parser.
Since Enju includes the disambiguationmodel of lexical entry assignments as plex, we canimplement our method in Enju by training anotherdisambiguation model p0lex(lijw; i) of lexical entryassignments for the biomedical domain, and then re-placing the original plexwith the newly trained p0lex.In this paper, for p0lex, we train a disambigua-tion model plex mix(lijw; i) of lexical entry assign-ments.
plex mixis a maximum entropy model andthe feature functions for it is the same as plexas13given in Table 1.
With these feature functions, wetrain plex mixon the treebanks both of the originaland biomedical domains.In the experiments, we examine the contributionof our method to parsing accuracy.
In addition, weimplement several other possible methods for com-parison of the performances.baseline: use the original model of EnjuGENIA only: execute the same method of trainingthe disambiguation model of Enju, using onlythe GENIA treebankMixture: execute the same method of training thedisambiguation model of Enju, using both ofthe Penn Treebank and the GENIA treebank (akind of smoothing method)HMT05: execute the method proposed in our pre-vious work (Hara et al, 2005)Our method: replace plexin the original modelwith plex mix, while leaving qsynas it isOur method (GENIA): replace plexin the originalmodel with plex genia, which is a probabilisticmodel of lexical entry assignments trained onlywith the GENIA treebank, while leaving qsynas it isOur method + GENIA: replace plexin the originalmodel with plex mixand qsynwith qsyn genia,which is a disambiguation model of tree con-struction trained with the GENIA treebankOur method + HMT05: replace plexin the orig-inal model with plex mixand qsynwith themodel re-trained with our previous method(Hara et al, 2005) (the combination of ourmethod and the ?HMT05?
method)baseline (lex): use only plexas a disambiguationmodelGENIA only (lex): use only plex geniaas a disam-biguation model, which is a probabilistic modelof lexical entry assignments trained only withthe GENIA treebankMixture (lex): use only plex mixas a disambigua-tion modelThe ?baseline?
method does no adaptation to thebiomedical domain, and therefore gives lower pars-ing accuracy for the domain than for the original do-main.
This method is regarded as the baseline ofthe experiments.
The ?GENIA only?
method reliessolely on the treebank for the biomedical domain,and therefore it cannot work well with the small tree-bank.
The ?Mixture?
method is a kind of smoothingmethod using all available training data at the sametime, and therefore the method can give the highestaccuracy of the three, which would be regarded asthe ideal accuracy with the naive methods.
However,training this model is expected to be very costly.The ?baseline (lex),?
?GENIA only (lex),?
and?Mixture (lex)?
approaches rely solely on models oflexical entry assignments, and show lower accuracythan those that contain both of models of lexical en-try assignments and tree constructions.
These ap-proaches can be utilized as indicators of importanceof combining the two types of models.Our previous work (Hara et al, 2005) showed thatthe model trained with the ?HMT05?
method cangive higher accuracy than the ?baseline?
method,even with the small amount of the treebanks in thebiomedical domain.
The model also takes much lesscost to train than with the ?Mixture?
method.
How-ever, they reported that the method could not give ashigh accuracy as the ?Mixture?
method.4 Experiments with the GENIA Corpus4.1 Experimental SettingsWe implemented the models shown in Section 3,and then evaluated the performance of them.
Theoriginal parser, Enju, was developed on Section 02-21 of the Penn Treebank (39,832 sentences) (Miyaoand Tsujii, 2005; Ninomiya et al, 2006).
Fortraining those models, we used the GENIA tree-bank (Kim et al, 2003), which consisted of 1,200abstracts (10,848 sentences) extracted from MED-LINE.
We divided it into three sets of 900, 150, and150 abstracts (8,127, 1,361, and 1,360 sentences),and these sets were used respectively as training, de-velopment, and final evaluation data.
The methodof Gaussian MAP estimation (Chen and Rosenfeld,1999) was used for smoothing.
The meta parameter of the Gaussian distribution was determined so asto maximize the accuracy on the development set.14                                             fffiflffi!
"#$ %  %&' (  )'* +   ,   +   ,    -  .+   ,    /  +   ,    / )'* #$ %  - % ( . %&-% ( .'
(   -% ( .Figure 3: Corpus size vs. accuracy for various methodsIn the following experiments, we measured theaccuracy of predicate-argument dependencies onthe evaluation set.
The measure is labeled preci-sion/recall (LP/LR), which is the same measure asprevious work (Clark and Curran, 2004b; Miyao andTsujii, 2005) that evaluated the accuracy of lexical-ized grammars on the Penn Treebank.The features for the examined approaches wereall the same as the original disambiguation model.In our previous work, the features for ?HMT05?were tuned to some extent.
We evened out the fea-tures in order to compare various approaches underthe same condition.
The lexical entries for trainingeach model were extracted from the treebank usedfor training the model of lexical entry assignments.We compared the performances of the given mod-els from various angles, by focusing mainly on theaccuracy against the cost.
For each of the models,we measured the accuracy transition according tothe size of the GENIA treebank for training and ac-cording to the training time.
We changed the sizeof the GENIA treebank for training: 100, 200, 300,400, 500, 600, 700, 800, and 900 abstracts.
Figure3 and 4 show the F-score transition according to thesize of the training set and the training time amongthe given models respectively.
Table 2 and Table 3show the parsing performance and the training costobtained when using 900 abstracts of the GENIAtreebank.
Note that Figure 4 does not include theresults of the ?Mixture?
method because only themethod took too much training cost as shown inTable 3.
It should also be noted that training timein Figure 4 includes time required for both trainingand development tests.
In Table 2, accuracies withmodels other than ?baseline?
showed the significantdifferences from ?baseline?
according to stratifiedshuffling test (Cohen, 1995) with p-value < 0:05.In the rest of this section we analyze these exper-imental results by focusing mainly on the contribu-tion of re-training lexical entry assignment models.We first observe the results with the naive or existingapproaches.
On the basis of these results, we evalu-ate the impact of our method.
We then explore thecombination of our method with other methods, andanalyze the errors for our future research.4.2 Exploring Naive or Existing ApproachesWithout adaptation, Enju gave the parsing accuracyof 86.39 in F-score, which was 3.42 point lower than15                                fffiflffi   !"
#$%& ' ( )   *# +( )   *# +ffi   !
"( )   *# + , ffi   !
"( )   *# + ,& ' ffi   !"
#$%$ -' - ) $ -Figure 4: Training time vs. accuracy for various methodsthat Enju gave for the original domain, the PennTreebank.
This is the baseline of the experiments.Figure 3 shows that, for less than about 4,500training sentences, the ?GENIA only?
method couldnot obtain as high parsing accuracy as the ?baseline?method.
This result would indicate that the trainingdata would not be sufficient for re-training the wholedisambiguation model from scratch.
However, ifwe prepared more than about 4,500 sentences, themethod could give higher accuracy than ?baseline?with low training cost (see Figure 4).
On the otherhand, the ?Mixture?
method could obtain the high-est level of the parsing accuracy for any size of theGENIA treebank.
However, Table 3 shows that thismethod required too much training cost.
It would bea major barrier for further challenges for improve-ment with various additional parameters.The ?HMT05?
method could give higher accu-racy than the ?baseline?
method for any size of thetraining sentences although the accuracy was lowerthan the ?Mixture?
method.
The method could alsobe carried out in much smaller training time andlower cost than the ?Mixture?
method.
These pointswould be the benefits of the ?HMT05?
method.
Onthe other hand, when we compared the ?HMT05?method with the ?GENIA only?
method, for thelarger size of the training corpus, the ?HMT05?method was defeated by the ?GENIA only?
methodin parsing accuracy and training cost.4.3 Impact of Re-training a LexicalDisambiguation ModelWhen we focused on our method, it could constantlygive higher accuracy than the ?baseline?
and the?HMT05?
methods.
These results would indicatethat, for an individual method, re-training a model oflexical entry assignments might be more critical todomain adaptation than re-training that of tree con-struction.
In addition, for the small treebank, ourmethod could give as high accuracy as the ?Mixture?method with much lower training cost.
Our methodwould be a very satisfiable approach when appliedwith a small treebank.
It should be noted that the re-trained lexical model could not solely give the ac-curacy as high as our method (see ?Mixture (lex)?in Figure 3).
The combination of a re-trained lexi-cal model and a tree construction model would havegiven such a high performance.When we compared the training time for our16Table 2: Parsing accuracy and time for various methodsFor GENIA Corpus For Penn TreebankLP LR F-score Time LP LR F-score Timebaseline 86.71 86.08 86.39 476 sec.
89.99 89.63 89.81 675 sec.GENIA only 88.99 87.91 88.45 242 sec.
72.07 45.78 55.99 2,441 sec.Mixture 90.01 89.87 89.94 355 sec.
89.93 89.60 89.77 767 sec.HMT05 88.47 87.89 88.18 510 sec.
88.92 88.61 88.76 778 sec.Our method 89.11 88.97 89.04 327 sec.
89.96 89.63 89.79 713 sec.Our method (GENIA) 86.06 85.15 85.60 542 sec.
70.18 44.88 54.75 3,290 sec.Our method + GENIA 90.02 89.88 89.95 320 sec.
88.11 87.77 87.94 718 sec.Our method + HMT05 90.23 90.08 90.15 377 sec.
89.31 88.98 89.14 859 sec.baseline (lex) 85.93 85.27 85.60 377 sec.
87.52 87.13 87.33 553 sec.GENIA only (lex) 87.42 86.28 86.85 197 sec.
71.49 45.41 55.54 1,928 sec.Mixture (lex) 88.43 88.18 88.31 258 sec.
87.49 87.12 87.30 585 sec.Table 3: Training cost of various methodsTraining time Memory usedbaseline 0 sec.
0.00 GByteGENIA only 14,695 sec.
1.10 GByteMixture 238,576 sec.
5.05 GByteHMT05 21,833 sec.
1.10 GByteOur method 12,957 sec.
4.27 GByteOur method (GENIA) 1,419 sec.
0.94 GByteOur method + GENIA 42,475 sec.
4.27 GByteOur method + HMT05 31,637 sec.
4.27 GBytebaseline (lex) 0 sec.
0.00 GByteGENIA only (lex) 1,434 sec.
1.10 GByteMixture (lex) 13,595 sec.
4.27 GBytemethod with the one for the ?HMT05?
method,our method required less time than the ?HMT05?method.
This would be because our method requiredonly the re-training of the very simple model, that is,a probabilistic model of lexical entry assignments.It should be noted that our method would notwork only with in-domain treebank.
The ?Ourmethod (GENIA)?
and the ?GENIA only (lex)?methods could hardly give as high parsing accuracyas the ?baseline?
method.
Although, for the largersize of the GENIA treebank, the methods couldobtain a little higher accuracy than the ?baseline?method, the benefit was very little.
These resultswould indicate that only the treebank in the targetdomain would be insufficient for adaptation.
Fig-ure 5 shows the coverage of each training corpus forthe GENIA treebank, which would also support theabove observation.
It shows that the GENIA tree-bank could not solely cover so much sentences inthe GENIA corpus as the combination of the PennTreebank and the GENIA treebank. 	   Figure 5: Corpus size vs. coverage of each trainingset for the GENIA corpusTable 4: Coverage of each training set% of covered sentencesTraining set for GENIA for PTBGENIA treebank 77.54 % 25.66 %PTB treebank 70.45 % 84.12 %GENIA treebank + PTB treebank 82.74 % 84.86 %4.4 Effectiveness of Combining Lexical andSyntactic Disambiguation ModelsWhen we compared the ?Our method + HMT05?and ?Our method + GENIA?
methods with the?Mixture?
method, the former two models couldgive as the high parsing accuracies as the latter onefor any size of the training corpus.
In particular,for the maximum size, the ?Our method + HMT05?models could give a little higher parsing accuracythan the ?Mixture?
method.
This difference was17Table 5: Errors in various methodsTotal errors = Common errors with baseline + Specific errorsGENIA only 2,889 = 1,906 (65.97%) + 983 (34.03%)Mixture 2,653 = 2,177 (82.06%) + 476 (17.94%)HMT05 3,063 = 2,470 (80.64%) + 593 (19.36%)Our method 2,891 = 2,405 (83.19%) + 486 (16.81%)Our method (GENIA) 3,153 = 2,070 (65.65%) + 1,083 (34.35%)Our method + GENIA 2,650 = 2,056 (77.58%) + 594 (22.42%)Our method + HMT05 2,597 = 1,943 (74.82%) + 654 (25.18%)baseline 3,542Total errors = Common errors with baseline (lex) + Specific errorsGENIA only (lex) 3,320 = 2,509 (75.57%) + 811 (24.43%)Mixture (lex) 3,100 = 2,769 (89.32%) + 331 (10.68%)baseline (lex) 3,757Table 6: Types of disambiguation errors# of errorsOnly forError cause Common Baseline AdaptedAttachment ambiguityprepositional phrase 12 12 6relative clause 0 1 0adjective 4 2 2adverb 1 3 1verb phrase 10 3 1subordinate clause 0 2 0Argument/modifier distinctionto-infinitive 0 0 7Lexical ambiguitypreposition/modifier 0 3 0verb subcategorization frame 5 0 6participle/adjective 0 2 0Test set errorsErrors of treebank 2 0 0Other types of error causesComma 10 8 4Noun phrase identification 21 5 8Coordination/insertion 6 3 5Zero-pronoun resolution 8 1 0Others 1 1 2shown to be significant according to stratified shuf-fling test with p-value < 0.10, which might suggestthe beneficial impact of the ?Our method + HMT05?method.
In addition, Figure 4 and Table 3 showthat training the ?Our method + HMT05?
or ?Ourmethod + GENIA?
model required much less timeand PC memory than training the ?Mixture?
model.According to the above observation, we would beable to say that the ?Our method + HMT05?
methodmight be the most ideal among the given methods.The ?Our method + HMT05?
and ?Our method+ GENIA?
methods showed the different perfor-mances in the point that the former could obtainhigh parsing accuracy with less training time thanthe latter.
This would come from the fact that thelatter method trained qsyn geniasolely with lexicalentries in the GENIA treebank, while the former onetrained qsynwith rich lexical entries borrowed fromqlex mix.
Rich lexical entries would decrease un-known lexical entries, and therefore would improvethe effectiveness of making the feature forest model.On the other hand, the difference in lexical entrieswould not seem to affect so much on the contribu-tion of tree construction model to the parsing accu-racy.
In our experiments, the parameters for a treeconstruction model such as feature functions werenot adjusted thoroughly, which might possibly blurthe benefits of the rich lexical entries.4.5 Error AnalysisTable 5 shows the comparison of the number of er-rors for various models with that for the originalmodel in parsing the GENIA corpus.
For each ofthe methods, the table gives the numbers of commonerrors with the original Enju model and the onesspecific to that method.
If possible, we would likeour methods to decrease the errors in the originalEnju model while not increasing new errors.
The ta-ble shows that our method gave the least percentageof newly added errors among the approaches exceptfor the methods utilizing only lexical entry assign-ments models.
On the other hand, the ?Our method+ HMT05?
approach gave over 25 % of newly addederrors, although we considered above that the ap-proach gave the best performance.In order to explore this phenomenon, we observed18the errors for the ?Our method + HMT05?
and thebaseline models, and then classified them into sev-eral types.
Table 6 shows manual classification ofcauses of errors for the two models in 50 sentences.In the classification, one error often propagated andresulted in multiple errors of predicate argument de-pendencies.
The numbers in the table include suchdouble counting.
It would be desirable that the er-rors in the rightmost column were less than the onesin the middle column, which means that the ?Ourmethod + HMT05?
approach did not produce moreerrors specific to the approach than the baseline.With the ?Our method + HMT05?
approach,errors for ?attachment ambiguity?
decreased as awhole.
Errors for ?comma?
and lexical ambiguitiesof ?preposition/modifier?
and ?participle/adjective?also decreased.
For these attributes, the approachcould learn in the training phase lexical properties ofcontinuous words with the lexical entry assignmentmodel, and syntactic relations of separated wordswith the tree construction model.
On the other hand,the errors for ?to-infinitive argument/modifier dis-tinction?
and ?verb subcategorization frame ambi-guity?
considerably increased.
These two types oferrors have close relation to each other because thefailure to recognize verb subcategorization framestends to cause the failure to recognize the syntacticrole of the to-infinitives.
We must research furtheron these errors in our future work.When we focused on ?noun phrase identifica-tion,?
most of the errors did not differ betweenthe two models.
In the biomedical domain, therewould be many technical terms which could not becorrectly identified solely with the disambiguationmodel, which would possibly result in such manyuntouched errors.
In order to properly cope withthese terms, we might have to introduce some kindsof dictionaries or named entity recognition methods.5 Experiments with the Brown Corpus5.1 Brown CorpusWe applied our methods to the Brown corpus(Kucera and Francis, 1967) and examined the porta-bility of our method.
The Brown corpus consists of15 domains, and the Penn Treebank gives bracketedversion of the corpus for the 8 domains containing19,395 sentences (Table 7).Table 7: Domains in the Brown corpuslabel domain sentencesCF popular lore 2,420CG belles lettres 2,546CK general fiction 3,172CL mystery and detective fiction 2,745CM science fiction 615CN adventure and western fiction 3,521CP romance and love story 3,089CR humor 812All total of all the above domains 19,395For the target of adaptation, we utilized the do-main containing all of these 8 domains as a total fic-tion domain (labelled ?All?)
as well as the individualones.
As in the experiments with the GENIA Tree-bank, we divided sentences for each domain intothree parts, 80% for training, 10% for develepmenttest, and 10% for final test.
For the ?All?
domain, wemerged all training sets, all development test sets,and all final test sets for the 8 domains respectively.Table 8 and 9 show the parsing accuracy and train-ing time for each domain with the various methodsshown in Section 3.
The methods are fundamen-tally the same as in the experiments with the GE-NIA corpus except that the target corpus is replacedwith the Brown corpus.
In order to avoid confusion,we replaced ?GENIA?
in the names of the meth-ods with ?Brown.?
Each of the bold numbers inTable 8 means that it was the best accuracy givenfor the target domain.
It should be noted that the?CM?
and ?CR?
domain contains very small tree-bank, and therefore we must consider that the resultswith these domains would not be so useful.5.2 Evaluation of Portability of Our MethodWhen we focus on the ?ALL?
domain, the ap-proaches other than the baseline succeeded to givehigher parsing accuracy than the baseline.
Thiswould show that these approaches were effective notonly for the GENIA corpus but also for the Browncorpus.
The ?Mixture?
method gave the highest ac-curacy which was 3.41 point higher than the base-line.
The ?Our method + HMT05?
approach alsogave the accuracy as high as the ?Mixture?
method.In addition, as is the case with the GENIA corpus,the approach could be trained with much less timethan the ?Mixture?
method.
Not only for these two19Table 8: Parsing accuracy for the Brown corpusF-scoreALL CF CG CK CL CM CN CP CRbaseline 83.09 85.75 85.38 81.12 77.53 85.30 82.84 85.18 76.63Brown only 84.84 77.65 78.92 75.72 70.56 50.02 78.38 79.10 50.34Mixture 86.50 86.59 85.94 82.49 78.66 84.82 84.28 86.85 76.45HMT05 83.79 85.80 84.98 81.48 76.91 85.25 83.50 85.66 77.15Our method 86.14 86.73 85.74 82.77 77.95 85.40 84.23 86.90 76.71Our method (GENIA) 84.71 78.49 79.63 75.43 70.86 50.24 78.49 79.69 51.82Our method + GENIA 86.00 86.12 85.41 83.22 77.10 83.39 84.21 85.77 76.91Our method + HMT05 86.44 86.76 85.85 82.90 77.70 85.61 84.43 86.87 77.48baseline (lex) 82.19 84.69 83.85 80.25 76.32 83.42 81.29 84.13 77.33Brown only (lex) 83.92 77.12 77.81 75.06 70.35 49.95 77.06 78.84 50.63Mixture (lex) 85.29 85.47 84.18 81.88 77.22 83.98 82.67 85.65 77.58Table 9: Consumed time for various methods for the Brown corpusConsumed time for training (sec.
)ALL CF CG CK CL CM CN CP CRbaseline 0 0 0 0 0 0 0 0 0Brown only 42,614 4,115 3,763 2,478 2,162 925 2,362 2,695 1,226Mixture 383,557 190,449 159,490 156,299 210,357 131,335 170,108 224,045 184,251HMT05 30,933 6,003 4,830 4,186 5,010 1,681 4,411 5,069 1,588Our method 15,912 11,053 10,988 11,151 10,782 10,158 11,075 10,594 10,284Our method (Brown) 3,273 312 373 310 249 46 321 317 86Our method + Brown 130,434 24,633 21,848 20,171 19,184 11,995 19,164 20,922 13,461Our method + HMT05 54,355 17,722 16,627 15,229 14,914 12,226 15,760 16,175 11,724baseline (lex) 0 0 0 0 0 0 0 0 0Brown only (lex) 3,001 317 373 308 251 47 321 317 86Mixture (lex) 21,148 11,128 11,251 11,094 10,728 10,466 11,151 10,897 10,537methods, the experimental results for the ?All?
do-main showed the tendency similar to the GENIAcorpus as a whole, except for the less improvementwith the ?HMT05?
method.When we focus on the individual domains, ourmethod could successfully obtain higher parsing ac-curacy than the baseline for all the domains.
More-over, for the ?CP?
domain, our method could givethe highest parsing accuracy among the methods.These results would support the portability of re-training the model for lexical entry assignment.
The?Our method + HMT05?
approach, which gave thehighest performance for the GENIA corpus, alsogave accuracy improvement for the all domainswhile it did not give so much impact for the ?CL?domain.
The ?Mixture?
approach, which utilizedthe same lexical entry assignment model, could ob-tain 0.94 point higher parsing accuracy than the?Our method + HMT05?
approach.
Table 10, whichshows the lexical coverage with each domains, doesnot seem to indicate the noteworthy difference inlexical entry coverage between the ?CL?
and theother domains.
As mentioned in the error analysisin Section 4, the model of tree construction mightaffect the performance in some way.
In our futurework, we must clarify the mechanism of this resultand would like to further improve the performance.6 Related WorkFor recent years, domain adaptation has been stud-ied extensively.
This section explores how our re-search is relevant to the previous works.Our previous work (Hara et al, 2005) and thisresearch mainly focused on how to draw as muchbenefit from a smaller amount of in-domain anno-tated data as possible.
Titov and Henderson (2006)also took this type of approach.
They first trained aprobabilistic model on original and target treebanksand used it to define a kernel over parse trees.
Thiskernel was used in a large margin classifier trainedon a small set of data only from the target domain,and the classifier was then used for reranking the top20Table 10: Coverage of each training set for the Brown corpus% of covered sentences for the target corpusTraining set ALL CF CG CK CL CM CN CP CRTarget treebank 74.99 % 49.13 % 50.00 % 47.97 % 49.08 % 29.66 % 53.51 % 64.01 % 8.57%PTB treebank 70.02 % 72.09 % 68.93 % 66.42 % 68.87 % 78.62 % 70.00 % 77.59 % 47.14 %Target + PTB 79.77 % 74.71 % 71.47 % 71.59 % 70.45 % 80.00 % 72.70 % 80.39 % 52.86 %parses on the target domain.On the other hand, several studies have exploredhow to draw useful information from unlabelled in-domain data.
Roark and Bacchiani (2003) adapted alexicalized PCFG by using maximum a posteriori(MAP) estimation for handling unlabelled adapta-tion data.
In the field of classifications, Blitzer et al(2006) utilized unlabelled corpora to extract featuresof structural correspondences, and then adapted aPOS-tagger to a biomedical domain.
Steedman etal.
(2003) utilized a co-training parser for adapta-tion and showed that co-training is effective evenacross domains.
McClosky et al (2006) adapted are-ranking parser to a target domain by self-trainingthe parser with unlabelled data in the target domain.Clegg and Shepherd (2005) combined several ex-isting parsers with voting schemes or parse selec-tion, and then succeeded to gain the improvementof performance for a biomedical domain.
Althoughunsupervised methods can exploit large in-domaindata, the above studies could not obtain the accu-racy as high as that for an original domain, evenwith the sufficient size of the unlabelled corpora.On the other hand, we showed that our approachcould achieve this goal with about 6,500 labelledsentences.
However, when 6,500 labelled can not beprepared, it might be worth while to explore the po-tentiality of combining the above unsupervised andour supervised methods.When we focuses on biomedical domains, therehave also been various works which coped withdomain adaptation.
Biomedical sentences containmany technical terms which cannot be easily recog-nized without expert knowledge, and this damagesperformances of NLP tools directly.
In order to solvethis problem, two types of approaches have beensuggested.
The first approach is to utilize existingdomain-specific lexical resources.
Lease and Char-niak (2005) utilized POS tags, dictionary colloca-tions, and named entities for parser adaptation, andthen succeeded to achieve accuracy improvement.The second approach is to expand lexical entries fora target domain.
Szolovits (2003) extended a lexicaldictionary for a target domain by predicting lexicalinformation for words.
They transplanted lexical in-discernibility of words in an original domain into atarget domain.
Pyysalo et al (2004) showed the ex-perimental results that this approach improved theperformance of a parser for Link Grammar.
Sinceour re-trained model of lexical entry assignmentswas shown to be unable to cope with this problemproperly (shown in Section 4), the combination ofthe above approaches with our approach would beexpected to bring further improvement.7 ConclusionsThis paper presented an effective approach to adapt-ing an HPSG parser trained on the Penn Treebankto a biomedical domain.
We trained a probabilis-tic model of lexical entry assignments in a targetdomain and then incorporated it into the originalparser.
The experimental results showed that thisapproach obtains higher parsing accuracy than theexisting approach of adapting the structural modelalone.
Moreover, the results showed that, the com-bination of our method and the existing approachcould achieve parsing accuracy that is as high as thatobtained by re-training an HPSG parser for the targetdomain from scratch, but with much lower trainingcost.
With this model, the parsing accuracy for thetarget domain improved by 3.84 f-score points, us-ing a domain-specific treebank of 8,127 sentences.Experiments showed that 6,500 sentences are suffi-cient for achieving as high parsing accuracy as thebaseline for the original domain.In addition, we applied our method to the Browncorpus in order to evaluate the portability of ourmethod.
Experimental results showed that the pars-ing accuracy for the target domain improved by 3.35f-score points.
On the other hand, when we focused21on some individual domains, that combination ap-proach could not give the desirable results.In future work, we would like to explore furtherperformance improvement of our approach.
For thefirst step, domain-specific features such as namedentities could be much help for solving unsuccess-ful recognition of technical terms.AcknowledgmentThis research was partially supported by Grant-in-Aid for Specially Promoted Research 18002007.ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: An approach to almost parsing.
Compu-tational Linguistics, 25(2).A.
L. Berger, S. A. Della Pietra, and V. J. DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?71.J.
Blitzer, R. McDonald, and F. Pereira.
2006.
Domainadaptation with structural correspondence learning.
InProc.
EMNLP 2006.Y.
S. Chan and H. T. Ng.
2006.
Estimating class priorsin domain adaptation for word sense disambiguation.In Proc.
21st COLING and 44th ACL.S.
Chen and R. Rosenfeld.
1999.
A gaussian prior forsmoothing maximum entropy models.
Technical Re-port CMUCS-99-108, Carnegie Mellon University.S.
Clark and J. R. Curran.
2004a.
The importance of su-pertagging for wide-coverage CCG parsing.
In Proc.COLING-04.S.
Clark and J. R. Curran.
2004b.
Parsing the WSJ usingCCG and log-linear models.
In Proc.
42nd ACL.S.
Clark and J. R. Curran.
2006.
Partial training for alexicalized-grammar parser.
In Proc.
NAACL-06.A.
B. Clegg and A. Shepherd.
2005.
Evaluating andintegrating treebank parsers on a biomedical corpus.In Proc.
the ACL Workshop on Software.P.
R. Cohen.
1995.
Empirical Methods for Artificial In-telligence.
MIT Press.T.
Hara, Y. Miyao, and J. Tsujii.
2005.
Adapting a prob-abilistic disambiguation model of an HPSG parser to anew domain.
In Proc.
IJCNLP 2005.F.
Jelinek.
1998.
Statistical Methods for Speech Recog-nition.
The MIT Press.M.
Johnson and S. Riezler.
2000.
Exploiting auxiliarydistributions in stochastic unification-based grammars.In Proc.
1st NAACL.J.
D. Kim, T. Ohta, Y. Teteisi, and J. Tsujii.
2003.
GE-NIA corpus - a semantically annotated corpus for bio-textmining.
Bioinformatics, 19(suppl.
1):i180?i182.H.
Kucera and W. N. Francis.
1967.
ComputationalAnalysis of Present-Day American English.
BrownUniversity Press, Providence, RI.M.
Lease and E. Charniak.
2005.
Parsing biomedicalliterature.
In In Proc.
IJCNLP 2005.M.
Marcus, G. Kim, M. A. Marcinkiewicz, R. MacIntyre,A.
Bies, M. Ferguson, K. Katz, and B. Schasberger.1994.
The Penn Treebank: Annotating predicate argu-ment structure.
In ARPA HLT Workshop.D.
McClosky, E. Charniak, and M. Johnson.
2006.Reranking and self-training for parser adaptation.
InProc.
21st COLING and 44th ACL.Y.
Miyao and J. Tsujii.
2002.
Maximum entropy estima-tion for feature forests.
In Proc.
HLT 2002.Y.
Miyao and J. Tsujii.
2005.
Probabilistic disam-biguation models for wide-coverage HPSG parsing.
InProc.
ACL 2005.Y.
Miyao, T. Ninomiya, and J. Tsujii.
2004.
Corpus-oriented grammar development for acquiring a Head-driven Phrase Structure Grammar from the Penn Tree-bank.
In Proc.
IJCNLP-04.T.
Ninomiya, T. Matsuzaki, Y. Tsuruoka, Y. Miyao, andJ.
Tsujii.
2006.
Extremely lexicalized models for ac-curate and fast HPSG parsing.
In Proc.
EMNLP 2006.C.
Pollard and I.
A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.S.
Pyysalo, F. Ginter, T. Pahikkala, J. Koivula, J. Boberg,J.
Jrvinen, and T. Salakoski.
2004.
Analysisof Link Grammar on biomedical dependency cor-pus targeted at protein-protein interactions.
In Proc.BioNLP/NLPBA 2004.B.
Roark and M. Bacchiani.
2003.
Supervised and unsu-pervised PCFG adaptation to novel domains.
In Proc.HLT-NAACL 2003.M.
Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhen, S. Baker, and J. Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In Proc.
European ACL (EACL).P.
Szolovits.
2003.
Adding a medical lexicon to an En-glish parser.
In AMIA Annu Symp Proc.Ivan Titov and James Henderson.
2006.
Porting statisti-cal parsers with data-defined kernels.
In Proc.
CoNLL-2006.22
