Proceedings of NAACL HLT 2007, pages 147?154,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsWorst-Case Synchronous Grammar RulesDaniel Gildea and Daniel ?Stefankovic?Computer Science Dept.University of RochesterRochester, NY 14627AbstractWe relate the problem of finding the bestapplication of a Synchronous Context-Free Grammar (SCFG) rule during pars-ing to a Markov Random Field.
Thisrepresentation allows us to use the the-ory of expander graphs to show that thecomplexity of SCFG parsing of an inputsentence of length N is ?
(N cn), for agrammar with maximum rule length n andsome constant c. This improves on theprevious best result of ?
(N c?n).1 IntroductionRecent interest in syntax-based methods for statis-tical machine translation has lead to work in pars-ing algorithms for synchronous context-free gram-mars (SCFGs).
Generally, parsing complexity de-pends on the length of the longest rule in the gram-mar, but the exact nature of this relationship has onlyrecently begun to be explored.
It has been knownsince the early days of automata theory (Aho andUllman, 1972) that the languages of string pairs gen-erated by a synchronous grammar can be arranged inan infinite hierarchy, with each rule size ?
4 pro-ducing languages not possible with grammars re-stricted to smaller rules.
For any grammar withmaximum rule size n, a fairly straightforward dy-namic programming strategy yields an O(Nn+4) al-gorithm for parsing sentences of length N .
How-ever, this is often not the best achievable complexity,and the exact bounds of the best possible algorithmsare not known.
Satta and Peserico (2005) showedthat a permutation can be defined for any length nsuch that tabular parsing strategies must take at least?
(N c?n), that is, the exponent of the algorithm isproportional to the square root of the rule length.In this paper, we improve this result, showing thatin the worst case the exponent grows linearly withthe rule length.
Using a probabilistic argument, weshow that the number of easily parsable permuta-tions grows slowly enough that most permutationsmust be difficult, where by difficult we mean that theexponent in the complexity is greater than a constantfactor times the rule length.
Thus, not only do thereexist permutations that have complexity higher thanthe square root case of Satta and Peserico (2005),but in fact the probability that a randomly chosenpermutation will have higher complexity approachesone as the rule length grows.Our approach is to first relate the problem offinding an efficient parsing algorithm to finding thetreewidth of a graph derived from the SCFG rule?spermutation.
We then show that this class of graphsare expander graphs, which in turn means that thetreewidth grows linearly with the graph size.2 Synchronous Parsing StrategiesWe write SCFG rules as productions with onelefthand side nonterminal and two righthand sidestrings.
Nonterminals in the two strings are linkedwith superscript indices; symbols with the same in-dex must be further rewritten synchronously.
For ex-ample,X ?
A(1) B(2) C(3) D(4), A(1) B(2) C(3) D(4)(1)is a rule with four children and no reordering, whileX ?
A(1) B(2) C(3) D(4), B(2) D(4) A(1) C(3)(2)147Algorithm 1 BottomUpParser(grammar G, input strings e, f )for x0, xn such that 1 < x0 < xn < |e| in increasing order of xn ?
x0 dofor y0, yn such that 1 < y0 < yn < |f | in increasing order of yn ?
y0 dofor Rules R of form X ?
X(1)1 ...X(n)n , X(pi(1))pi(1) ...X(pi(n))pi(n) in G dop = P (R) maxx1..xn?1y1..yn?1?i?
(Xi, xi?1, xi, ypi(i)?1, ypi(i))?
(X,x0, xn, y0, yn) = max{?
(X,x0, xn, y0, yn), p}end forend forend forexpresses a more complex reordering.
In general,we can take indices in the first grammar dimen-sion to be consecutive, and associate a permutation?
with the second dimension.
If we use Xi for0 ?
i ?
n as a set of variables over nonterminalsymbols (for example, X1 and X2 may both standfor nonterminal A), we can write rules in the gen-eral form:X0 ?
X(1)1 ...X(n)n , X(pi(1))pi(1) ...X(pi(n))pi(n)Grammar rules also contain terminal symbols, but astheir position does not affect parsing complexity, wefocus on nonterminals and their associated permuta-tion ?
in the remainder of the paper.
In a probabilis-tic grammar, each rule R has an associated proba-bility P (R).
The synchronous parsing problem con-sists of finding the tree covering both strings havingthe maximum product of rule probabilities.1We assume synchronous parsing is done by stor-ing a dynamic programming table of recognizednonterminals, as outlined in Algorithm 1.
We referto a dynamic programming item for a given nonter-minal with specified boundaries in each language asa cell.
The algorithm computes cells by maximiz-ing over boundary variables xi and yi, which rangeover positions in the two input strings, and specifybeginning and end points for the SCFG rule?s childnonterminals.The maximization in the inner loop of Algo-rithm 1 is the most expensive part of the proce-dure, as it would take O(N2n?2) with exhaustive1We describe our methods in terms of the Viterbi algorithm(using the max-product semiring), but they also apply to non-probabilistic parsing (boolean semiring), language modeling(sum-product semiring), and Expectation Maximization (withinside and outside passes).search; making this step more efficient is our fo-cus in this paper.
The maximization can be donewith further dynamic programming, storing partialresults which contain some subset of an SCFG rule?srighthand side nonterminals that have been recog-nized.
A parsing strategy for a specific SCFG ruleconsists of an order in which these subsets shouldbe combined, until all the rule?s children have beenrecognized.
The complexity of an individual parsingstep depends on the number of free boundary vari-ables, each of which can take O(N) values.
It isoften helpful to visualize parsing strategies on thepermutation matrix corresponding to a rule?s per-mutation ?.
Figure 1 shows the permutation matrixof rule (2) with a three-step parsing strategy.
Eachpanel shows one combination step along with theprojections of the partial results in each dimension;the endpoints of these projections correspond to freeboundary variables.
The second step has the high-est number of distinct endpoints, five in the verticaldimension and three horizontally, meaning parsingcan be done in time O(N8).As an example of the impact that the choice ofparsing strategy can make, Figure 2 shows a per-mutation for which a clever ordering of partial re-sults enables parsing in time O(N10) in the lengthof the input strings.
Permutations having this patternof diagonal stripes can be parsed using this strat-egy in time O(N10) regardless of the length n ofthe SCFG rule, whereas a na?
?ve strategy proceedingfrom left to right in either input string would taketime O(Nn+3).2.1 Markov Random Fields for CellsIn this section, we connect the maximization ofprobabilities for a cell to the Markov Random Field148{A,B,C,D}{A,B,C}{A,B}{A} {B}{C}{D}x0 x1 x2 x3 x4y0y1y2y3y4ABCDx0 x1 x2 x3 x4y0y1y2y3y4ABCDx0 x1 x2 x3 x4y0y1y2y3y4ABCDFigure 1: The tree on the left defines a three-step parsing strategy for rule (2).
In each step, the two subsetsof nonterminals in the inner marked spans are combined into a new chart item with the outer spans.
Theintersection of the outer spans, shaded, has now been processed.
Tic marks indicate distinct endpoints of thespans being combined, corresponding to the free boundary variables.
(MRF) representation, which will later allow us touse algorithms and complexity results based on thegraphical structure of MRFs.
A Markov RandomField is defined as a probability distribution2 over aset of variables x that can be written as a product offactors fi that are functions of various subsets xi ofx.
The probability of an SCFG rule instance com-puted by Algorithm 1 can be written in this func-tional form:?R(x) = P (R)?ifi(xi)wherex = {xi, yi} for 0 ?
i ?
nxi = {xi?1, xi, ypi(i)?1, ypi(i)}and the MRF has one factor fi for each child nonter-minal Xi in the grammar rule R. The factor?s valueis the probability of the child nonterminal, which canbe expressed as a function of its four boundaries:fi(xi) = ?
(Xi, xi?1, xi, ypi(i)?1, ypi(i))For reasons that are explained in the followingsection, we augment our Markov Random Fieldswith a dummy factor for the completed parent non-terminal?s chart item.
Thus there is one dummy fac-tor d for each grammar rule:d(x0, xn, y0, yn) = 1expressed as a function of the four outer boundaryvariables of the completed rule, but with a constant2In our case unnormalized.Figure 2: A parsing strategy maintaining two spansin each dimension is O(N10) for any length permu-tation of this general form.value of 1 so as not to change the probabilities com-puted.Thus an SCFG rule with n child nonterminals al-ways results in a Markov Random Field with 2n+2variables and n+ 1 factors, with each factor a func-tion of exactly four variables.Markov Random Fields are often represented asgraphs.
A factor graph representation has a nodefor each variable and factor, with an edge connect-ing each factor to the variables it depends on.
An ex-ample for rule (2) is shown in Figure 3, with roundnodes for variables, square nodes for factors, and adiamond for the special dummy factor.2.2 Junction TreesEfficient computation on Markov Random Fieldsis performed by first transforming the MRF intoa junction tree (Jensen et al, 1990; Shafer andShenoy, 1990), and then applying the standard149dy0 y1 y2 y3 y4f1 f2 f3 f4x0 x1 x2 x3 x4Figure 3: Markov Random Field for rule (2).message-passing algorithm for graphical modelsover this tree structure.
The complexity of the mes-sage passing algorithm depends on the structure ofthe junction tree, which in turn depends on the graphstructure of the original MRF.A junction tree can be constructed from a MarkovRandom Field by the following three steps:?
Connect all variable nodes that share a factor,and remove factor nodes.
This results in thegraphs shown in Figure 4.?
Choose a triangulation of the resulting graph,by adding chords to any cycle of length greaterthan three.?
Decompose the triangulated graph into a tree ofcliques.We call nodes in the resulting tree, correspondingto cliques in the triangulated graph, clusters.
Eachcluster has a potential function, which is a functionof the variables in the cluster.
For each factor in theoriginal MRF, the junction tree will have at least onecluster containing all of the variables on which thefactor is defined.
Each factor is associated with onesuch cluster, and the cluster?s potential function isset to be the product of its factors, for all combina-tions of variable values.
Triangulation ensures thatthe resulting tree satisfies the junction tree property,which states that for any two clusters containing thesame variable x, all nodes on the path connecting theclusters also contain x.
A junction tree derived fromthe MRF of Figure 3 is shown in Figure 5.The message-passing algorithm for graphicalmodels can be applied to the junction tree.
The algo-y0 y1 y2 y3 y4x0 x1 x2 x3 x4y0 y1 y2 y3 y4x0 x1 x2 x3 x4Figure 4: The graphs resulting from connectingall interacting variables for the identity permutation(1, 2, 3, 4) (top) and the (2, 4, 1, 3) permutation ofrule (2) (bottom).rithm works from the leaves of the tree inward, alter-nately multiplying in potential functions and maxi-mizing over variables that are no longer needed, ef-fectively distributing the max and product operatorsso as to minimize the interaction between variables.The complexity of the message-passing is O(nNk),where the junction tree contain O(n) clusters, k isthe maximum cluster size, and each variable in thecluster can take N values.However, the standard algorithm assumes that thefactor functions are predefined as part of the input.In our case, however, the factor functions themselvesdepend on message-passing calculations from othergrammar rules:fi(xi) = ?
(Xi, xi?1, xi, ypi(i)?1, ypi(i))= maxR?:Xi?
?,?P (R?)
maxx?:x?0=xi?1,x?n?=xiy?0=ypi(i?1),y?n?=ypi(i)?R?(x?)
(3)We must modify the standard algorithm in orderto interleave computation among the junction treescorresponding to the various rules in the grammar,using the bottom-up ordering of computation fromAlgorithm 1.
Where, in the standard algorithm, eachmessage contains a complete table for all assign-ments to its variables, we break these into a sepa-rate message for each individual assignment of vari-ables.
The overall complexity is unchanged, becauseeach assignment to all variables in each cluster isstill considered only once.The dummy factor d ensures that every junction150x0 x3 x4 y0 y2 y3 y4x0 x2 x3 y0 y1 y2 y3 y4x0 x1 x2 y1 y2 y3 y4Figure 5: Junction tree for rule (2).tree we derive from an SCFG rule has a cluster con-taining all four outer boundary variables, allowingefficient lookup of the inner maximization in (3).Because the outer boundary variables need not ap-pear throughout the junction tree, this technique al-lows reuse of some partial results across differentouter boundaries.
As an example, consider messagepassing on the junction tree of shown in Figure 5,which corresponds to the parsing strategy of Fig-ure 1.
Only the final step involves all four bound-aries of the complete cell, but the most complex stepis the second, with a total of eight boundaries.
Thisefficient reuse would not be achieved by applyingthe junction tree technique directly to the maximiza-tion operator in Algorithm 1, because we would befixing the outer boundaries and computing the junc-tion tree only over the inner boundaries.3 Treewidth and Tabular ParsingThe complexity of the message passing algorithmover an MRF?s junction tree is determined by thetreewidth of the MRF.
In this section we show that,because parsing strategies are in direct correspon-dence with valid junction trees, we can use treewidthto analyze the complexity of a grammar rule.We define a tabular parsing strategy as any dy-namic programming algorithm that stores partial re-sults corresponding to subsets of a rule?s child non-terminals.
Such a strategy can be represented as arecursive partition of child nonterminals, as shownin Figure 1(left).
We show below that a recursivepartition of children having maximum complexity kat any step can be converted into a junction tree hav-ing k as the maximum cluster size.
This implies thatfinding the optimal junction tree will give a parsingstrategy at least as good as the strategy of the opti-mal recursive partition.A recursive partition of child nonterminals can beconverted into a junction tree as follows:?
For each leaf of the recursive partition, whichrepresents a single child nonterminal i, cre-ate a leaf in the junction tree with the cluster(xi?1, xi, ypi(i)?1, ypi(i)) and the potential func-tion fi(xi?1, xi, ypi(i)?1, ypi(i)).?
For each internal node in the recursive parti-tion, create a corresponding node in the junc-tion tree.?
Add each variable xi to all nodes in the junctiontree on the path from the node for child nonter-minal i?
1 to the node for child nonterminal i.Similarly, add each variable ypi(i) to all nodesin the junction tree on the path from the nodefor child nonterminal ?
(i) ?
1 to the node forchild nonterminal ?
(i).Because each variable appears as an argument ofonly two factors, the junction tree nodes in which itis present form a linear path from one leaf of the treeto another.
Since each variable is associated onlywith nodes on one path through the tree, the result-ing tree will satisfy the junction tree property.
Thetree structure of the original recursive partition im-plies that the variable rises from two leaf nodes tothe lowest common ancestor of both leaves, and isnot contained in any higher nodes.
Thus each nodein the junction tree contains variables correspond-ing to the set of endpoints of the spans defined bythe two subsets corresponding to its two children.The number of variables at each node in the junctiontree is identical to the number of free endpoints atthe corresponding combination in the recursive par-tition.Because each recursive partition corresponds to ajunction tree with the same complexity, finding thebest recursive partition reduces to finding the junc-tion tree with the best complexity, i.e., the smallestmaximum cluster size.Finding the junction tree with the smallest clus-ter size is equivalent to finding the input graph?streewidth, the smallest k such that the graph can beembedded in a k-tree.
In general, this problem wasshown to be NP-complete by Arnborg et al (1987).However, because the treewidth of a given rule lowerbounds the complexity of its tabular parsing strate-gies, parsing complexity for general rules can be151bounded with treewidth results for worst-case rules,without explicitly identifying the worst-case permu-tations.4 Treewidth Grows LinearlyIn this section, we show that the treewidth of thegraphs corresponding to worst-case permutationsgrowths linearly with the permutation?s length.
Ourstrategy is as follows:1.
Define a 3-regular graph for an input permu-tation consisting of a subset of edges from theoriginal graph.2.
Show that the edge-expansion of the 3-regulargraph grows linearly for randomly chosen per-mutations.3.
Use edge-expansion to bound the spectral gap.4.
Use spectral gap to bound treewidth.For the first step, we define H = (V,E) as a ran-dom 3-regular graph on 2n vertices obtained as fol-lows.
Let G1 = (V1, E1) and G2 = (V2, E2) becycles, each on a separate set of n vertices.
Thesetwo cycles correspond to the edges (xi, xi+1) and(yi, yi+1) in the graphs of the type shown in Fig-ure 4.
Let M be a random perfect matching be-tween V1 and V2.
The matching represents the edges(xi, ypi(i)) produced from the input permutation ?.Let H be the union of G1, G2, and M .
While Hcontains only some of the edges in the graphs de-fined in the previous section, removing edges cannotincrease the treewidth.For the second step of the proof, we use a proba-bilistic argument detailed in the next subsection.For the third step, we will use the following con-nection between the edge-expansion and the eigen-value gap (Alon and Milman, 1985; Tanner, 1984).Lemma 4.1 Let G be a k-regular graph.
Let ?2 bethe second largest eigenvalue of G. Let h(G) be theedge-expansion of G. Thenk ?
?2 ?h(G)22k .Finally, for the fourth step, we use a relation be-tween the eigenvalue gap and treewidth for regu-lar graphs shown by Chandran and Subramanian(2003).Lemma 4.2 Let G be a k-regular graph.
Let n bethe number of vertices of G. Let ?2 be the secondlargest eigenvalue of G. Thentw(G) ??
n4k (k ?
?2)??
1Note that in our setting k = 3.
In order to useLemma 4.2 we will need to give a lower bound onthe eigenvalue gap k ?
?2 of G.4.1 Edge ExpansionThe edge-expansion of a set of vertices T is the ra-tio of the number of edges connecting vertices in Tto the rest of the graph, divided by the number ofvertices in T ,|E(T, V ?
T )||T |where we assume that |T | ?
|V |/2.
The edge ex-pansion of a graph is the minimum edge expansionof any subset of vertices:h(G) = minT?V|E(T, V ?
T )|min{|T |, |V ?
T |} .Intuitively, if all subsets of vertices are highly con-nected to the remainder of the graph, there is no wayto decompose the graph into minimally interactingsubgraphs, and thus no way to decompose the dy-namic programming problem of parsing into smallerpieces.Let(nk)be the standard binomial coefficient, andfor ?
?
R, let( n?
?)=???
?k=0(nk).We will use the following standard inequality validfor 0 ?
?
?
n:( n?
?)?(ne?)?
(4)Lemma 4.3 With probability at least 0.98 the graphH has edge-expansion at least 1/50.Proof :Let ?
= 1/50.
Assume that T ?
V is a set with asmall edge-expansion, i. e.,|E(T, V ?
T )| ?
?|T |, (5)152and |T | ?
|V |/2 = n. Let Ti = T ?
Vi and letti = |Ti|, for i = 1, 2.
We will w.l.o.g.
assumet1 ?
t2.
We will denote as ?i the number of spans ofconsecutive vertices from Ei contained in T .
Thus2?i = |E(Ti, Vi ?
Ti)|, for i = 1, 2.
The spanscounted by ?1 and ?2 correspond to continuous spanscounted in computing the complexity of a chart pars-ing operation.
However, unlike in the diagrams inthe earlier part of this paper, in our graph theoreticargument there is no requirement that T select onlycorresponding pairs of vertices from V1 and V2.There are at least 2(?1+?2)+t2?t1 edges betweenT and V ?
T .
This is because there are 2?i edgeswithin Vi at the left and right boundaries of the ?ispans, and at least t2?
t1 edges connecting the extravertices from T2 that have no matching vertex in T1.Thus from assumption (5) we havet2 ?
t1 ?
?
(t1 + t2)which in turn impliest1 ?
t2 ?1 + ?1?
?
t1.
(6)Similarly, using (6), we have?1 + ?2 ?
?2 (t1 + t2) ??1?
?
t1.
(7)That is, for T to have small edge expansion,the vertices in T1 and T2 must be collected into asmall number of spans ?1 and ?2.
This limit on thenumber of spans allows us to limit the number ofways of choosing T1 and T2.
Suppose that t1 isgiven.
Any pair T1, T2 is determined by the edgesin E(T1, V1 ?
T1), and E(T2, V2 ?
T2), and twobits (corresponding to the possible ?swaps?
of Tiwith Vi ?
Ti).
Note that we can choose at most2?1 + 2?2 ?
t1 ?
2?/(1?
?)
edges in total.
Thus thenumber of choices of T1 and T2 is bounded above by4 ?
( 2n?
2?1??
t1).
(8)For a given choice of T1 and T2, for T to havesmall edge expansion, there must also not be toomany edges that connect T1 to vertices in V2 ?
T2.Let k be the number of edges between T1 and T2.There are at least t1 + t2 ?
2k edges between T andV ?
T and from assumption (5) we havet1 + t2 ?
2k ?
?
(t1 + t2)Thusk ?
(1?
?)
t1 + t22 ?
(1?
?)t1.
(9)The probability that there are ?
(1?
?
)t1 edges be-tween T1 and T2 is bounded by( t1?
?t1)( t2n)(1??
)t1where the first term selects vertices in T1 connectedto T2, and the second term upper bounds the proba-bility that the selected vertices are indeed connectedto T2.
Using 6, we obtain a bound in terms of t1alone:( t1?
?t1)(1 + ?1?
?
?t1n)(1??
)t1, (10)Combining the number of ways of choosing T1and T2 (8) with the bound on the probability that theedges M from the input permutation connect almostall the vertices in T1 to vertices from T2 (10), andusing the union bound over values of t1, we obtainthat the probability p that there exists T ?
V withedge-expansion less than ?
is bounded by:2?n/2??t1=04?
( 2n?
2?1??
t1)( t1?
?t1)(1 + ?1?
?
?t1n)(1??
)t1(11)where the factor of 2 is due to the assumption t1 ?t2.The graph H is connected and hence T has at leastone out-going edge.
Therefore if t1 + t2 ?
1/?, theedge-expansion of T is at least ?.
Thus a set withedge-expansion less than ?
must have t1 + t2 ?
1/?,which, by (6), implies t1 ?
(1 ?
?)/(2?).
Thus thesum in (11) can be taken for t from ?
(1 ?
?)/(2?
)?153to ?n/2?.
Using (4) we obtainp ?
8?n/2??t1=?
1??2?
???(2ne2?1??
t1)2?1??
t1 ( t1e?t1)?t1(1 + ?1?
?
?t1n)(1??)t1]=8?n/2??t1=?
1??2?
?((e(1?
?)?)2?1??
(e?)?
(1 + ?1?
?)1??
( t1n)1???
2?1??)t1.
(12)We will use t1/n ?
1/2 and plug ?
= 1/50 into(12).
We obtainp ?
8?
?t1=250.74t1 ?
0.02.While this constant bound on p is sufficient forour main complexity result, it can further be shownthat p approaches zero as n increases, from the factthat the geometric sum in (12) converges, and eachterm for fixed t1 goes to zero as n grows.This completes the second step of the proof asoutlined at the beginning of this section.
The con-stant bound on the edge expansion implies a constantbound on the eigenvalue gap (Lemma 4.1), which inturn implies an ?
(n) bound on treewidth (Lemma4.2), yielding:Theorem 4.4 Tabular parsing strategies for Syn-chronous Context-Free Grammars containing ruleswith all permutations of length n require time?
(N cn) in the input string length N for some con-stant c.We have shown our result without explicitly con-structing a difficult permutation, but we close withone example.
The zero-based permutations of lengthp, where p is prime, ?
(i) = i?1 mod p for 0 <i < p, and ?
(0) = 0, provide a known family ofexpander graphs (see Hoory et al (2006)).5 ConclusionWe have shown in the exponent in the complex-ity of polynomial-time parsing algorithms for syn-chronous context-free grammars grows linearly withthe length of the grammar rules.
While it is veryexpensive computationally to test whether a speci-fied permutation has a parsing algorithm of a certaincomplexity, it turns out that randomly chosen per-mutations are difficult with high probability.Acknowledgments This work was supported byNSF grants IIS-0546554, IIS-0428020, and IIS-0325646.ReferencesAlbert V. Aho and Jeffery D. Ullman.
1972.
The The-ory of Parsing, Translation, and Compiling, volume 1.Prentice-Hall, Englewood Cliffs, NJ.N.
Alon and V.D.
Milman.
1985.
?1, isoperimetricinequalities for graphs and superconcentrators.
J. ofCombinatorial Theory, Ser.
B, 38:73?88.Stefen Arnborg, Derek G. Corneil, and AndrzejProskurowski.
1987.
Complexity of finding embed-dings in a k-tree.
SIAM Journal of Algebraic and Dis-crete Methods, 8:277?284, April.L.S.
Chandran and C.R.
Subramanian.
2003.
A spectrallower bound for the treewidth of a graph and its conse-quences.
Information Processing Letters, 87:195?200.Shlomo Hoory, Nathan Linial, and Avi Wigderson.
2006.Expander graphs and their applications.
Bull.
Amer.Math.
Soc., 43:439?561.Finn V. Jensen, Steffen L. Lauritzen, and Kristian G. Ole-sen. 1990.
Bayesian updating in causal probabilis-tic networks by local computations.
ComputationalStatistics Quarterly, 4:269?282.Giorgio Satta and Enoch Peserico.
2005.
Some com-putational complexity results for synchronous context-free grammars.
In Proceedings of HLT/EMNLP, pages803?810, Vancouver, Canada, October.G.
Shafer and P. Shenoy.
1990.
Probability propaga-tion.
Annals of Mathematics and Artificial Intelli-gence, 2:327?353.R.M.
Tanner.
1984.
Explicit construction of concentra-tors from generalized n-gons.
J. Algebraic DiscreteMethods, 5:287?294.154
