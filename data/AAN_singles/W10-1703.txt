Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 17?53,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsFindings of the 2010 Joint Workshop onStatistical Machine Translation and Metrics for Machine TranslationChris Callison-BurchJohns Hopkins Universityccb@cs.jhu.eduPhilipp KoehnUniversity of Edinburghpkoehn@inf.ed.ac.ukChristof MonzUniversity of Amsterdamc.monz@uva.nlKay Peterson and Mark PrzybockiNational Institute of Standards and Technologykay.peterson,mark.przybocki@nist.govOmar F. ZaidanJohns Hopkins Universityozaidan@cs.jhu.eduAbstractThis paper presents the results of theWMT10 and MetricsMATR10 sharedtasks,1 which included a translation task,a system combination task, and an eval-uation task.
We conducted a large-scalemanual evaluation of 104 machine trans-lation systems and 41 system combina-tion entries.
We used the ranking of thesesystems to measure how strongly auto-matic metrics correlate with human judg-ments of translation quality for 26 metrics.This year we also investigated increasingthe number of human judgments by hiringnon-expert annotators through Amazon?sMechanical Turk.1 IntroductionThis paper presents the results of the sharedtasks of the joint Workshop on statistical Ma-chine Translation (WMT) and Metrics for MA-chine TRanslation (MetricsMATR), which washeld at ACL 2010.
This builds on four previ-ous WMT workshops (Koehn and Monz, 2006;Callison-Burch et al, 2007; Callison-Burch et al,2008; Callison-Burch et al, 2009), and one pre-vious MetricsMATR meeting (Przybocki et al,2008).
There were three shared tasks this year:a translation task between English and four otherEuropean languages, a task to combine the out-put of multiple machine translation systems, anda task to predict human judgments of translationquality using automatic evaluation metrics.
The1The MetricsMATR analysis was not complete in time forthe publication deadline.
An updated version of paper will bemade available on http://statmt.org/wmt10/ priorto July 15, 2010.performance on each of these shared task was de-termined after a comprehensive human evaluation.There were a number of differences betweenthis year?s workshop and last year?s workshop:?
Non-expert judgments ?
In addition to hav-ing shared task participants judge translationquality, we also collected judgments fromnon-expert annotators hired through Ama-zon?s Mechanical Turk.
By collecting a largenumber of judgments we hope to reduce theburden on shared task participants, and to in-crease the statistical significance of our find-ings.
We discuss the feasibility of using non-experts evaluators, by analyzing the cost, vol-ume and quality of non-expert annotations.?
Clearer results for system combination ?This year we excluded Google translationsfrom the systems used in system combina-tion.
In last year?s evaluation, the large mar-gin between Google and many of the othersystems meant that it was hard to improve onwhen combining systems.
This year, the sys-tem combinations perform better than theircomponent systems more often than last year.?
Fewer rule-based systems ?
This year therewere fewer rule-based systems submitted.
Inpast years, University of Saarland compiled alarge set of outputs from rule-based machinetranslation (RBMT) systems.
The RBMTsystems were not submitted this year.
Thisis unfortunate, because they tended to outper-form the statistical systems for German, andthey were often difficult to rank properly us-ing automatic evaluation metrics.The primary objectives of this workshop are toevaluate the state of the art in machine transla-17tion, to disseminate common test sets and pub-lic training data with published performance num-bers, and to refine evaluation methodologies formachine translation.
As with past years, all of thedata, translations, and human judgments producedfor our workshop are publicly available.2 We hopethey form a valuable resource for research into sta-tistical machine translation, system combination,and automatic evaluation of translation quality.2 Overview of the shared translation andsystem combination tasksThe workshop examined translation between En-glish and four other languages: German, Span-ish, French, and Czech.
We created a test set foreach language pair by translating newspaper arti-cles.
We additionally provided training data andtwo baseline systems.2.1 Test dataThe test data for this year?s task was createdby hiring people to translate news articles thatwere drawn from a variety of sources from mid-December 2009.
A total of 119 articles were se-lected, in roughly equal amounts from a varietyof Czech, English, French, German and Spanishnews sites:3Czech: iDNES.cz (5), iHNed.cz (1), Lidov-ky (16)French: Les Echos (25)Spanish: El Mundo (20), ABC.es (4), CincoDias (11)English: BBC (5), Economist (2), WashingtonPost (12), Times of London (3)German: Frankfurter Rundschau (11), Spie-gel (4)The translations were created by the profes-sional translation agency CEET4.
All of the trans-lations were done directly, and not via an interme-diate language.2.2 Training dataAs in past years we provided parallel corpora totrain translation models, monolingual corpora to2http://statmt.org/wmt10/results.html3For more details see the XML test files.
The docidtag gives the source and the date for each document in thetest set, and the origlang tag indicates the original sourcelanguage.4http://www.ceet.eu/train language models, and development sets totune parameters.
Some statistics about the train-ing materials are given in Figure 1.2.3 Baseline systemsTo lower the barrier of entry for newcomers tothe field, we provided two open source toolkitsfor phrase-based and parsing-based statistical ma-chine translation (Koehn et al, 2007; Li et al,2009).2.4 Submitted systemsWe received submissions from 33 groups from 29institutions, as listed in Table 1, a 50% increaseover last year?s shared task.We also evaluated 2 commercial off the shelfMT systems, and two online statistical machinetranslation systems.
We note that these companiesdid not submit entries themselves.
The entries forthe online systems were done by translating thetest data via their web interfaces.
The data usedto train the online systems is unconstrained.
It ispossible that part of the reference translations thatwere taken from online news sites could have beenincluded in the online systems?
language models.2.5 System combinationIn total, we received 153 primary system submis-sions along with 28 secondary submissions.
Thesewere made available to participants in the sys-tem combination shared task.
Based on feedbackthat we received on last year?s system combina-tion task, we provided two additional resources toparticipants:?
Development set: We reserved 25 articlesto use as a dev set for system combination.These were translated by all participatingsites, and distributed to system combinationparticipants along with reference translations.?
n-best translations: We requested n-bestlists from sites whose systems could producethem.
We received 20 n-best lists accompa-nying the system submissions.Table 2 lists the 9 participants in the systemcombination task.3 Human evaluationAs with past workshops, we placed greater em-phasis on the human evaluation than on the auto-matic evaluation metric scores.
It is our contention18Europarl Training CorpusSpanish?
English French?
English German?
EnglishSentences 1,650,152 1,683,156 1,540,549Words 47,694,560 46,078,122 50,964,362 47,145,288 40,756,801 43,037,967Distinct words 173,033 95,305 123,639 95,846 316,365 92,464News Commentary Training CorpusSpanish?
English French?
English German?
English Czech?
EnglishSentences 98,598 84,624 100,269 94,742Words 2,724,141 2,432,064 2,405,082 2,101,921 2,505,583 2,443,183 2,050,545 2,290,066Distinct words 69,410 46,918 53,763 43,906 101,529 47,034 125,678 45,306United Nations Training CorpusSpanish?
English French?
EnglishSentences 6,222,450 7,230,217Words 213,877,170 190,978,737 243,465,100 216,052,412Distinct words 441,517 361,734 402,491 412,815109 Word Parallel CorpusFrench?
EnglishSentences 22,520,400Words 811,203,407 668,412,817Distinct words 2,738,882 2,861,836CzEng Training CorpusCzech?
EnglishSentences 7,227,409Words 72,993,427 84,856,749Distinct words 1,088,642 522,770Europarl Language Model DataEnglish Spanish French GermanSentence 1,843,035 1,822,021 1,855,589 1,772,039Words 50,132,615 51,223,902 54,273,514 43,781,217Distinct words 99,206 178,934 127,689 328,628News Language Model DataEnglish Spanish French German CzechSentence 48,653,884 3,857,414 15,670,745 17,474,133 13,042,040Words 1,148,480,525 106,716,219 382,563,246 321,165,206 205,614,201Distinct words 1,451,719 548,169 998,595 1,855,993 1,715,376News Test SetEnglish Spanish French German CzechSentences 2489Words 62,988 65,654 68,107 62,390 53,171Distinct words 9,457 11,409 10,775 12,718 15,825Figure 1: Statistics for the training and test sets used in the translation task.
The number of words andthe number of distinct words is based on the provided tokenizer.19ID ParticipantAALTO Aalto University, Finland (Virpioja et al, 2010)CAMBRIDGE Cambridge University (Pino et al, 2010)CMU Carnegie Mellon University?s Cunei system (Phillips, 2010)CMU-STATXFER Carnegie Mellon University?s statistical transfer system (Hanneman et al, 2010)COLUMBIA Columbia UniversityCU-BOJAR Charles University Bojar (Bojar and Kos, 2010)CU-TECTO Charles University Tectogramatical MT (Z?abokrtsky?
et al, 2010)CU-ZEMAN Charles University Zeman (Zeman, 2010)DCU Dublin City University (Penkale et al, 2010)DFKI Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz (Federmann et al, 2010)EU European Parliament, Luxembourg (Jellinghaus et al, 2010)EUROTRANS commercial MT provider from the Czech RepublicFBK Fondazione Bruno Kessler (Hardmeier et al, 2010)GENEVA University of GenevaHUICONG Shanghai Jiao Tong University (Cong et al, 2010)JHU Johns Hopkins University (Schwartz, 2010)KIT Karlsruhe Institute for Technology (Niehues et al, 2010)KOC Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)LIG LIG Lab, University Joseph Fourier, Grenoble (Potet et al, 2010)LIMSI LIMSI (Allauzen et al, 2010)LIU Linko?ping University (Stymne et al, 2010)LIUM University of Le Mans (Lambert et al, 2010)NRC National Research Council Canada (Larkin et al, 2010)ONLINEA an online machine translation systemONLINEB an online machine translation systemPC-TRANS commercial MT provider from the Czech RepublicPOTSDAM Potsdam UniversityRALI RALI - Universite?
de Montre?al (Huet et al, 2010)RWTH RWTH Aachen (Heger et al, 2010)SFU Simon Fraser University (Sankaran et al, 2010)UCH-UPV Universidad CEU-Cardenal Herrera y UPV (Zamora-Martinez and Sanchis-Trilles, 2010)UEDIN University of Edinburgh (Koehn et al, 2010)UMD University of Maryland (Eidelman et al, 2010)UPC Universitat Polite`cnica de Catalunya (Henr?
?quez Q. et al, 2010)UPPSALA Uppsala University (Tiedemann, 2010)UPV Universidad Polite?cnica de Valencia (Sanchis-Trilles et al, 2010)UU-MS Uppsala University - Saers (Saers et al, 2010)Table 1: Participants in the shared translation task.
Not all groups participated in all language pairs.20ID ParticipantBBN-COMBO BBN system combination (Rosti et al, 2010)CMU-COMBO-HEAFIELD CMU system combination (Heafield and Lavie, 2010)CMU-COMBO-HYPOSEL CMU system combo with hyp.
selection (Hildebrand and Vogel, 2010)DCU-COMBO Dublin City University system combination (Du et al, 2010)JHU-COMBO Johns Hopkins University system combination (Narsale, 2010)KOC-COMBO Koc University, Turkey (Bicici and Kozat, 2010; Bicici and Yuret, 2010)LIUM-COMBO University of Le Mans system combination (Barrault, 2010)RWTH-COMBO RWTH Aachen system combination (Leusch and Ney, 2010)UPV-COMBO Universidad Polite?cnica de Valencia (Gonza?lez-Rubio et al, 2010)Table 2: Participants in the system combination task.Language Pair Sentence Ranking Edited Translations Yes/No JudgmentsGerman-English 5,212 830 824English-German 6,847 755 751Spanish-English 5,653 845 845English-Spanish 2,587 920 690French-English 4,147 925 921English-French 3,981 1,325 1,223Czech-English 2,688 490 488English-Czech 6,769 1,165 1,163Totals 37,884 7,255 6,905Table 3: The number of items that were collected for each task during the manual evaluation.
An itemis defined to be a rank label in the ranking task, an edited sentence in the editing task, and a yes/nojudgment in the judgment task.21that automatic measures are an imperfect substi-tute for human assessment of translation quality.Therefore, we define the manual evaluation to beprimary, and use the human judgments to validateautomatic metrics.Manual evaluation is time consuming, and it re-quires a large effort to conduct it on the scale ofour workshop.
We distributed the workload acrossa number of people, including shared-task partic-ipants, interested volunteers, and a small numberof paid annotators.
More than 120 people partic-ipated in the manual evaluation5, with 89 peopleputting in more than an hour?s worth of effort, and29 putting in more than four hours.
A collectivetotal of 337 hours of labor was invested.6We asked people to evaluate the systems?
outputin two different ways:?
Ranking translated sentences relative to eachother.
This was our official determinant oftranslation quality.?
Editing the output of systems without dis-playing the source or a reference translation,and then later judging whether edited transla-tions were correct.The total number of judgments collected for thedifferent modes of annotation is given in Table 3.In all cases, the output of the various translationsystems were judged on equal footing; the outputof system combinations was judged alongside thatof the individual system, and the constrained andunconstrained systems were judged together.3.1 Ranking translations of sentencesRanking translations relative to each other is a rea-sonably intuitive task.
We therefore kept the in-structions simple:Rank translations from Best to Worst rel-ative to the other choices (ties are al-lowed).5We excluded data from three errant annotators, identifiedas follows.
We considered annotators completing at least 3screens, whose P (A) with others (see 3.2) is less than 0.33.Out of seven such annotators, four were affiliated with sharedtask teams.
The other three had no apparent affiliation, andso we discarded their data, less than 5% of the total data.6Whenever an annotator appears to have spent more thanten minutes on a single screen, we assume they left their sta-tion and left the window open, rather than actually needingmore than ten minutes.
In those cases, we assume the timespent to be ten minutes.Each screen for this task involved judging trans-lations of three consecutive source segments.
Foreach source segment, the annotator was shown theoutputs of five submissions.
For each of the lan-guage pairs, there were more than 5 submissions.We did not attempt to get a complete ordering overthe systems, and instead relied on random selec-tion and a reasonably large sample size to makethe comparisons fair.Relative ranking is our official evaluation met-ric.
Individual systems and system combinationsare ranked based on how frequently they werejudged to be better than or equal to any other sys-tem.
The results of this are reported in Section 4.Appendix A provides detailed tables that containpairwise comparisons between systems.3.2 Inter- and Intra-annotator agreement inthe ranking taskWe were interested in determining the inter- andintra-annotator agreement for the ranking task,since a reasonable degree of agreement must ex-ist to support our process as a valid evaluationsetup.
To ensure we had enough data to measureagreement, we purposely designed the sampling ofsource segments shown to annotators so that itemswere likely to be repeated, both within an annota-tor?s assigned tasks and across annotators.
We didso by assigning an annotator a batch of 20 screens(each with three ranking sets; see 3.1) that were tobe completed in full before generating new screensfor that annotator.Within each batch, the source segments for nineof the 20 screens (45%) were chosen from a smallpool of 60 source segments, instead of being sam-pled from the larger pool of 1,000 source segmentsdesignated for the ranking task.7 The larger poolwas used to choose source segments for nine otherscreens (also 45%).
As for the remaining twoscreens (10%), they were chosen randomly fromthe set of eighteen screens already chosen.
Fur-thermore, in the two ?local repeat?
screens, thesystem choices were also preserved.Heavily sampling from a small pool of sourcesegments ensured we had enough data to measureinter-annotator agreement, while purposely mak-ing 10% of each annotator?s screens repeats of pre-viously seen sets in the same batch ensured we7Each language pair had its own 60-sentence pool, dis-joint from other language pairs?
pools, but ach of the 60-sentence pools was a subset of the 1,000-sentence pool.22INTER-ANNOTATOR AGREEMENTP (A) KWith references 0.658 0.487Without references 0.626 0.439WMT ?09 0.549 0.323INTRA-ANNOTATOR AGREEMENTP (A) KWith references 0.755 0.633Without references 0.734 0.601WMT ?09 0.707 0.561Table 4: Inter- and intra-annotator agreement forthe sentence ranking task.
In this task, P (E) is0.333.had enough data to measure intra-annotator agree-ment.We measured pairwise agreement among anno-tators using the kappa coefficient (K), which is de-fined asK =P (A)?
P (E)1?
P (E)where P (A) is the proportion of times that the an-notators agree, and P (E) is the proportion of timethat they would agree by chance.For inter-annotator agreement for the rankingtasks we calculated P (A) by examining all pairsof systems which had been judged by two or morejudges, and calculated the proportion of time thatthey agreed thatA > B, A = B, orA < B. Intra-annotator agreement was computed similarly, butwe gathered items that were annotated on multipleoccasions by a single annotator.Table 4 gives K values for inter-annotator andintra-annotator agreement.
These give an indi-cation of how often different judges agree, andhow often single judges are consistent for repeatedjudgments, respectively.
The exact interpretationof the kappa coefficient is difficult, but accordingto Landis and Koch (1977), 0?
.2 is slight, .2?
.4is fair, .4 ?
.6 is moderate, .6 ?
.8 is substantialand the rest is almost perfect.Based on these interpretations the agreementfor sentence-level ranking is moderate for inter-annotator agreement and substantial for intra-annotator agreement.
These levels of agreementare higher than in previous years, partially due tothe fact that that year we randomly included thereferences along the system outputs.
In general,judges tend to rank the reference as the best trans-lation, so people have stronger levels of agreementwhen it is included.
That said, even when compar-isons involving reference are excluded, we still seean improvement in agreement levels over last year.3.3 Editing machine translation outputIn addition to simply ranking the output of sys-tems, we also had people edit the output of MTsystems.
We did not show them the referencetranslation, which makes our edit-based evalu-ation different from the Human-targeted Trans-lation Edit Rate (HTER) measure used in theDARPA GALE program (NIST, 2008).
Ratherthan asking people to make the minimum numberof changes to the MT output in order capture thesame meaning as the reference, we asked them toedit the translation to be as fluent as possible with-out seeing the reference.
Our hope was that thiswould reflect people?s understanding of the out-put.The instructions given to our judges were as fol-lows:Correct the translation displayed, mak-ing it as fluent as possible.
If no correc-tions are needed, select ?No correctionsneeded.?
If you cannot understand thesentence well enough to correct it, select?Unable to correct.
?A screenshot is shown in Figure 2.
This year,judges were shown the translations of 5 consec-utive source sentences, all produced by the samemachine translation system.
In last year?s WMTevaluation they were shown only one sentence at atime, which made the task more difficult becausethe surrounding context could not be used as anaid to understanding.Since we wanted to prevent judges from see-ing the reference before editing the translations,we split the test set between the sentences usedin the ranking task and the editing task (becausethey were being conducted concurrently).
More-over, annotators edited only a single system?s out-put for one source sentence to ensure that their un-derstanding of it would not be influenced by an-other system?s output.3.4 Judging the acceptability of edited outputHalfway through the manual evaluation period, westopped collecting edited translations, and insteadasked annotators to do the following:23Edit Machine Translation OutputsInstructions:You are shown several machine translation outputs.Your task is to edit each translation to make it as fluent as possible.It is possible that the translation is already fluent.
In that case, select No corrections needed.If you cannot understand the sentence well enough to correct it, select Unable to correct.The sentences are all from the same article.
You can use the earlier and later sentencesto help understand a confusing sentence.Your edited translations           The machine translationsThe shortage of snow in mountain worries the hoteliersEdited     No corrections needed     Unable tocorrect         ResetThe shortage of snow in mountainworries the hoteliersThe deserted tracks are not putting down problem only at the exploitantsof skilift.Edited     No corrections needed     Unable tocorrect         ResetThe deserted tracks are notputting down problem only at theexploitants of skilift.The lack of snow deters the people to reserving their stays at the ski inthe hotels and pension.Edited     No corrections needed     Unable tocorrect         ResetThe lack of snow deters the peopleto reserving their stays at the skiin the hotels and pension.Thereby, is always possible to track free bedrooms for all the dates inwinter, including Christmas and Nouvel An.Edited     No corrections needed     Unable tocorrect         ResetThereby, is always possible totrack free bedrooms for all thedates in winter, includingChristmas and Nouvel An.We have many of visit on our siteFigure 2: This screenshot shows what an annotator sees when beginning to edit the output of a machinetranslation system.24Indicate whether the edited transla-tions represent fully fluent and meaning-equivalent alternatives to the referencesentence.
The reference is shown withcontext, the actual sentence is bold.In addition to edited translations, unedited itemsthat were either marked as acceptable or as incom-prehensible were also shown.
Judges gave a sim-ple yes/no indication to each item.4 Translation task resultsWe used the results of the manual evaluation toanalyze the translation quality of the different sys-tems that were submitted to the workshop.
In ouranalysis, we aimed to address the following ques-tions:?
Which systems produced the best translationquality for each language pair??
Did the system combinations produce bettertranslations than individual systems??
Which of the systems that used only the pro-vided training materials produced the besttranslation quality?Table 5 shows the best individual systems.
Wedefine the best systems as those which had noother system that was statistically significantlybetter than them under the Sign Test at p ?
0.1.Multiple systems are listed as the winners formany language pairs because it was not possible todraw a statistically significant difference betweenthe systems.
There is no individual system clearlyoutperforming all other systems across the differ-ent language pairs.
With the exception of French-English and English-French one can observe thattop-performing constrained systems did as well asthe unconstrained system ONLINEB.Table 6 shows the best combination systems.For all language directions, except Spanish-English, one can see that the system combina-tion runs outperform the individual systems andthat in most cases the differences are statisticallysignificant.
While this is to be expected, systemcombination is not guaranteed to improve perfor-mance as some of the lower ranked combinationruns show, which are outperformed by individualsystems.
Also note that except for Czech-Englishtranslation the online systems ONLINEA and ON-LINEB where not included for the system combi-nation runsUnderstandabilityOur hope is that judging the acceptability of editedoutput as discussed in Section 3 gives some indi-cation of how often a system?s output was under-standable.
Figure 3 gives the percentage of timesthat each system?s edited output was judged tobe acceptable (the percentage also factors in in-stances when judges were unable to improve theoutput because it was incomprehensible).This style of manual evaluation is experimentaland should not be taken to be authoritative.
Somecaveats about this measure:?
There are several sources of variance that aredifficult to control for: some people are betterat editing, and some sentences are more dif-ficult to edit.
Therefore, variance in the un-derstandability of systems is difficult to pindown.?
The acceptability measure does not stronglycorrelate with the more established method ofranking translations relative to each other forall the language pairs.5 Shared evaluation task overviewIn addition to allowing the analysis of subjectivetranslation quality measures for different systems,the judgments gathered during the manual evalu-ation may be used to evaluate how well the au-tomatic evaluation metrics serve as a surrogate tothe manual evaluation processes.
NIST began run-ning a ?Metrics for MAchine TRanslation?
chal-lenge (MetricsMATR), and presented their find-ings at a workshop at AMTA (Przybocki et al,2008).
This year we conducted a joint Metrics-MATR and WMT workshop, with NIST runningthe shared evaluation task and analyzing the re-sults.In this year?s shared evaluation task 14 differentresearch groups submitted a total of 26 differentautomatic metrics for evaluation:Aalto University of Science and Technology(Dobrinkat et al, 2010)?
MT-NCD ?
A machine translation metricbased on normalized compression distance(NCD), a general information-theoretic mea-sure of string similarity.
MT-NCD mea-sures the surface level similarity between twostrings with a general compression algorithm.More similar strings can be represented with25French-English551?755 judgments per systemSystem C?
?othersLIUM ??
Y 0.71ONLINEB ?
N 0.71NRC ??
Y 0.66CAMBRIDGE ??
Y +GW 0.66LIMSI ?
Y +GW 0.65UEDIN Y 0.65RALI ??
Y +GW 0.65JHU Y 0.59RWTH ??
Y +GW 0.55LIG Y 0.53ONLINEA N 0.52CMU-STATXFER Y 0.51HUICONG Y 0.51DFKI N 0.42GENEVA Y 0.27CU-ZEMAN Y 0.21English-French664?879 judgments per systemSystem C?
?othersUEDIN ??
Y 0.70ONLINEB ?
N 0.68RALI ??
Y +GW 0.66LIMSI ??
Y +GW 0.66RWTH ??
Y +GW 0.63CAMBRIDGE ?
Y +GW 0.63LIUM Y 0.63NRC Y 0.62ONLINEA N 0.55JHU Y 0.53DFKI N 0.40GENEVA Y 0.35EU N 0.32CU-ZEMAN Y 0.26KOC Y 0.26Czech-English788?868 judgments per systemSystem C?
?othersONLINEB ?
N 0.7UEDIN ?
Y 0.61CMU Y 0.55CU-BOJAR N 0.55AALTO Y 0.43ONLINEA N 0.37CU-ZEMAN Y 0.22German-English723?879 judgments per systemSystem C?
?othersONLINEB ?
N 0.73KIT ??
Y +GW 0.72UMD ??
Y 0.68UEDIN ?
Y 0.66FBK ?
Y +GW 0.66ONLINEA ?
N 0.63RWTH Y +GW 0.62LIU Y 0.59UU-MS Y 0.55JHU Y 0.53LIMSI Y +GW 0.52UPPSALA Y 0.51DFKI N 0.50HUICONG Y 0.47CMU Y 0.46AALTO Y 0.42CU-ZEMAN Y 0.36KOC Y 0.23English-German1284?1542 judgments per systemSystem C?
?othersONLINEB ?
N 0.70DFKI ?
N 0.62UEDIN ??
Y 0.62KIT ?
Y 0.60ONLINEA N 0.59FBK ?
Y 0.56LIU Y 0.55RWTH Y 0.51LIMSI Y 0.51UPPSALA Y 0.47JHU Y 0.46SFU Y 0.34KOC Y 0.30CU-ZEMAN Y 0.28English-Czech1375?1627 judgments per systemSystem C?
?othersONLINEB ?
N 0.70CU-BOJAR ?
N 0.66PC-TRANS ?
N 0.62UEDIN ??
Y 0.62CU-TECTO Y 0.60EUROTRANS N 0.54CU-ZEMAN Y 0.50SFU Y 0.45ONLINEA N 0.44POTSDAM Y 0.44DCU N 0.38KOC Y 0.33Spanish-English1448?1577 judgments per systemSystem C?
?othersONLINEB ?
N 0.70UEDIN ??
Y 0.69CAMBRIDGE Y +GW 0.61JHU Y 0.61ONLINEA N 0.54UPC ?
Y 0.51HUICONG Y 0.50DFKI N 0.45COLUMBIA Y 0.45CU-ZEMAN Y 0.27English-Spanish540?722 judgments per systemSystem C?
?othersONLINEB ?
N 0.71ONLINEA ?
N 0.69UEDIN ?
Y 0.61DCU N 0.61DFKI ?
N 0.55JHU ?
Y 0.55UPV ?
Y 0.55CAMBRIDGE ?
Y +GW 0.54UHC-UPV ?
Y 0.54SFU Y 0.40CU-ZEMAN Y 0.23KOC Y 0.19Systems are listed in the order of how often their translations were ranked higher than or equal to any other system.
Ties arebroken by direct comparison.C?
indicates constrained condition, meaning only using the supplied training data, standard monolingual linguistic tools, andoptionally the LDC?s GigaWord, which was allowed this year (entries that used the GigaWord are marked +GW).?
indicates a win in the category, meaning that no other system is statistically significantly better at p-level?0.1 in pairwisecomparison.?
indicates a constrained win, no other constrained system is statistically better.For all pairwise comparisons between systems, please check the appendix.Table 5: Official results for the WMT10 translation task, based on the human evaluation (ranking trans-lations relative to each other)26French-English589?716 judgments per comboSystem ?othersRWTH-COMBO ?
0.77CMU-HYP-COMBO ?
0.77DCU-COMBO ?
0.72LIUM ?
0.71CMU-HEA-COMBO ?
0.70UPV-COMBO ?
0.68NRC 0.66CAMBRIDGE 0.66UEDIN ?
0.65LIMSI ?
0.65JHU-COMBO 0.65RALI 0.65LIUM-COMBO 0.64BBN-COMBO 0.64RWTH 0.55English-French740?829 judgments per comboSystem ?othersRWTH-COMBO ?
0.75CMU-HEA-COMBO ?
0.74UEDIN 0.70KOC-COMBO ?
0.68UPV-COMBO 0.66RALI ?
0.66LIMSI 0.66RWTH 0.63CAMBRIDGE 0.63Czech-English766?843 judgments per comboSystem ?othersCMU-HEA-COMBO ?
0.71ONLINEB ?
0.7BBN-COMBO ?
0.70RWTH-COMBO ?
0.65UPV-COMBO ?
0.63JHU-COMBO 0.62UEDIN 0.61German-English743?835 judgments per comboSystem ?othersBBN-COMBO ?
0.77RWTH-COMBO ?
0.75CMU-HEA-COMBO 0.73KIT ?
0.72UMD ?
0.68JHU-COMBO 0.67UEDIN ?
0.66FBK 0.66CMU-HYP-COMBO 0.65UPV-COMBO 0.64RWTH 0.62KOC-COMBO 0.59English-German1340?1469 judgments per comboSystem ?othersRWTH-COMBO ?
0.65DFKI ?
0.62UEDIN ?
0.62KIT ?
0.60CMU-HEA-COMBO ?
0.59KOC-COMBO 0.59FBK ?
0.56UPV-COMBO 0.55English-Czech1405?1496 judgments per comboSystem ?othersDCU-COMBO ?
0.75ONLINEB ?
0.70RWTH-COMBO 0.70CMU-HEA-COMBO 0.69UPV-COMBO 0.68CU-BOJAR 0.66KOC-COMBO 0.66PC-TRANS 0.62UEDIN 0.62Spanish-English1385?1535 judgments per comboSystem ?othersUEDIN ?
0.69CMU-HEA-COMBO ?
0.66UPV-COMBO ?
0.66BBN-COMBO 0.62JHU-COMBO 0.55UPC 0.51English-Spanish516?673 judgments per comboSystem ?othersCMU-HEA-COMBO ?
0.68KOC-COMBO 0.62UEDIN ?
0.61UPV-COMBO 0.60RWTH-COMBO 0.59DFKI ?
0.55JHU 0.55UPV 0.55CAMBRIDGE ?
0.54UPV-NNLM ?
0.54System combinations are listed in the order of how often their translations were ranked higher than or equal to any other system.Ties are broken by direct comparison.
We show the best individual systems alongside the system combinations, since the goalof combination is to produce better quality translation than the component systems.?
indicates a win for the system combination meaning that no other system or system combination is statistically signifi-cantly better at p-level?0.1 in pairwise comparison.?
indicates an individual system that none of the system combinations beat by a statistically significant margin at p-level?0.1.For all pairwise comparisons between systems, please check the appendix.Note: ONLINEA and ONLINEB were not included among the systems being combined in the system combination shared tasks,except in the Czech-English and English-Czech conditions, where ONLINEB was included.Table 6: Official results for the WMT10 system combination task, based on the human evaluation (rank-ing translations relative to each other)27System % Yes Yes count No count N/A count Total count *** en-cz ***ref 0.97 63 2 0 65 en-czdcu-c 0.58 29 21 0 50 en-czonlineB 0.55 22 18 0 40 en-czrwth-c 0.49 56 59 0 115 en-czkoc-c 0.45 29 36 0 65 en-czpc-trans 0.43 26 34 0 60 en-czupv-c 0.42 23 32 0 55 en-czcu-bojar 0.4 20 30 0 50 en-czeurotrans 0.4 18 27 0 45 en-czuedin 0.34 24 46 0 70 en-czcu-tecto 0.34 29 55 1 85 en-czcmu-hea-c 0.29 13 32 0 45 en-czsfu 0.24 14 44 0 58 en-czpotsdam 0.24 13 42 0 55 en-czcu-zeman 0.21 15 55 0 70 en-czkoc 0.21 21 79 0 100 en-czonlineA 0.2 13 52 0 65 en-czdcu 0.19 13 57 0 70 en-cz0.1260077028*** en-de ***ref 0.94 47 3 0 50 en-deonlineA 0.8 20 5 0 25 en-dekoc-c 0.68 17 8 0 25 en-deuppsala 0.65 26 14 0 40 en-deuedin 0.62 50 30 0 80 en-dekit 0.62 37 23 0 60 en-deupv-c 0.57 30 23 0 53 en-deonlineB 0.52 21 19 0 40 en-dedfki 0.52 13 12 0 25 en-dekoc 0.51 18 17 0 35 en-delimsi 0.51 18 16 1 35 en-deliu 0.51 28 27 0 55 en-derwth 0.5 15 15 0 30 en-derwth-c 0.49 22 23 0 45 en-dejhu 0.48 12 13 0 25 en-decmu-hea-c 0.47 14 16 0 30 en-defbk 0.4 4 6 0 10 en-desfu 0.31 11 24 0 35 en-decu-zeman 0.19 10 40 3 53 en-de0.1364453014System % Yes Yes count No count N/A count Total count *** en-es ***ref 0.83 48 10 0 58 en-esonlineB 0.58 25 18 0 43 en-esupv 0.5 20 20 0 40 en-esrwth-c 0.46 13 15 0 28 en-esdcu 0.42 16 22 0 38 en-eskoc 0.4 17 24 1 42 en-esupv-nnlm 0.39 15 23 0 38 en-esonlineA 0.38 11 18 0 29 en-esjhu 0.38 17 27 1 45 en-eskoc-c 0.38 20 33 0 53 en-esuedin 0.36 12 21 0 33 en-esupb-c 0.32 13 27 0 40 en-escmu-hea-c 0.32 16 34 0 50 en-escamb 0.3 12 27 1 40 en-esdfki 0.29 7 17 0 24 en-escu-zeman 0.29 16 39 0 55 en-essfu 0.26 9 25 0 34 en-es0.0845946216System % Yes Yes count No count N/A count Total count *** en-fr ***ref 0.91 64 4 2 70 en-frrwth-c 0.54 27 23 0 50 en-fronlineB 0.52 47 42 1 90 en-frupv-c 0.51 34 33 0 67 en-frkoc-c 0.48 32 34 0 66 en-fruedin 0.48 30 32 1 63 en-frrali 0.47 21 24 0 45 en-frrwth 0.45 25 30 0 55 en-frlium 0.43 20 27 0 47 en-frcamb 0.42 26 36 0 62 en-fronlineA 0.41 15 22 0 37 en-frlimsi 0.37 26 44 0 70 en-frjhu 0.37 27 46 0 73 en-frnrc 0.36 13 23 0 36 en-frcmu-hea-c 0.32 22 47 0 69 en-frgeneva 0.31 32 70 0 102 en-freu 0.3 13 30 0 43 en-frdfki 0.28 16 42 0 58 en-frkoc 0.21 12 44 1 57 en-frcu-zeman 0.17 11 52 0 63 en-fr0.1045877454System % Yes Yes count No count N/A count Total count *** cz-en ***ref 1.00 33 0 0 33 cz-encu-bojar 0.6 3 2 0 5 cz-enupv-c 0.43 15 20 0 35 cz-encmu-hea-c 0.35 14 26 0 40 cz-enrwth-c 0.32 16 34 0 50 cz-enonlineB 0.3 12 28 0 40 cz-enbbn-c 0.28 17 43 0 60 cz-enuedin 0.28 11 28 1 40 cz-enaalto 0.27 8 22 0 30 cz-enjhu-c 0.26 13 37 0 50 cz-enonlineA 0.2 6 24 0 30 cz-encmu 0.17 5 25 0 30 cz-encu-zeman 0.09 4 40 1 45 cz-en0.1292958787System % Yes Yes count No count N/A count Total count *** de-en ***ref 0.98 44 1 0 45 de-enumd 0.8 8 2 0 10 de-enbbn-c 0.67 10 5 0 15 de-enonlineB 0.65 13 7 0 20 de-encmu-hea-c 0.52 12 11 0 23 de-enjhu-c 0.51 18 17 0 35 de-enupv-c 0.51 18 16 1 35 de-enfbk 0.5 20 20 0 40 de-enuppsala 0.5 20 19 1 40 de-enlimsi 0.46 30 34 1 65 de-enkit 0.45 18 22 0 40 de-enliu 0.44 19 24 0 43 de-enuedin 0.44 11 14 0 25 de-endfki 0.4 12 18 0 30 de-enonlineA 0.4 6 9 0 15 de-enrwth 0.4 14 21 0 35 de-encmu-hyp-c 0.37 11 19 0 30 de-enhuicong 0.36 9 16 0 25 de-enkoc-c 0.36 9 14 2 25 de-enrwth-c 0.36 10 18 0 28 de-enkoc 0.31 11 23 1 35 de-encu-zeman 0.3 12 28 0 40 de-enuu-ms 0.26 13 37 0 50 de-enjhu 0.26 9 26 0 35 de-encmu 0.24 6 19 0 25 de-enaalto 0.07 1 14 0 15 de-en0.1512635669System % Yes Yes count No count N/A count Total count *** es-en ***ref 0.98 39 0 1 40 es-enonlineB 0.71 39 15 1 55 es-enonlineA 0.64 32 18 0 50 es-enupv-c 0.6 36 24 0 60 es-enhuicong 0.54 27 23 0 50 es-enjhu 0.54 35 30 0 65 es-encmu-hea-c 0.52 26 23 1 50 es-enbbn-c 0.51 36 33 1 70 es-enuedin 0.51 33 30 2 65 es-enjhu-c 0.47 28 31 1 60 es-endfki 0.46 16 18 1 35 es-enupc 0.43 28 36 1 65 es-encu-zeman 0.4 18 26 1 45 es-encamb 0.36 25 45 0 70 es-encolumbia 0.29 19 46 0 65 es-en0.1104436607System % Yes Yes count No count N/A count Total count *** fr-en ***ref 0.91 32 3 0 35 fr-encmu-hyp-c 0.7 21 9 0 30 fr-enuedin 0.58 23 17 0 40 fr-enbbn-c 0.56 14 10 1 25 fr-enrwth-c 0.53 16 14 0 30 fr-enonlineB 0.51 28 27 0 55 fr-encamb 0.5 20 19 1 40 fr-enrali 0.48 31 34 0 65 fr-enlium 0.46 23 27 0 50 fr-endcu-c 0.45 15 16 2 33 fr-enlig 0.45 9 11 0 20 fr-encmu-statxfer 0.44 11 14 0 25 fr-ennrc 0.43 15 20 0 35 fr-endfki 0.4 8 12 0 20 fr-enjhu 0.4 10 14 1 25 fr-enjhu-c 0.4 22 30 3 55 fr-enupv-c 0.4 14 20 1 35 fr-enlium-c 0.4 27 41 0 68 fr-encmu-hea-c 0.35 14 26 0 40 fr-enlimsi 0.35 14 26 0 40 fr-enonlineA 0.33 20 40 0 60 fr-enhuicong 0.32 13 25 2 40 fr-encu-zeman 0.24 6 19 0 25 fr-engeneva 0.24 6 19 0 25 fr-enrwth 0.2 1 4 0 5 fr-en0.114347550600.250.50.751refdcu-conlineBrwth-ckoc-cpc-transupv-ccu-bojareurotransuedincu-tectocmu-hea-csfupotsdamcu-zemankoconlineAdcu.19.2.21.21.24.24.29.34.34.4.4.42.43.45.49.55.58.97English-Czech00.250.50.751refonlineAkoc-cuppsalauedinkitupv-conlineBdfkikoclimsiliurwthrwth-cjhucmu-hea-cfbksfucu-zeman.19.31.4.47.48.49.5.51.51.51.52.52.57.62.62.65.68.8.94English-German00.250.50.751refonlineBupvrwth-cdcukocupv-nnlmonlineAjhukoc-cuedinupb-ccmu-hea-ccambdfkicu-zsfu.26.29.29.3.32.32.36.38.38.38.39.4.42.46.5.58.83English-Spanish00.250.50.751refrwth-conlineBupv-ckoc-cuedinralirwthliumcambonlineAlimsijhunrccmu-hea-cgenevaeudfkikoccu-zeman.17.21.28.3.31.32.36.37.37.41.42.43.45.47.48.48.51.52.54.91English-French00.250.50.751refumdbbn-conlineBcmu-hea-cjhu-cupv-cfbkuppsalalimsikitliuuedindfkionlineArwthcmu-hyp-chuicongkoc-crwth-ckoccu-zemanuu-msjhucmuaalto.07.24.26.26.3.31.36.36.36.37.4.4.4.44.44.45.46.5.5.51.51.52.65.67.8.98German-English00.250.50.751refcu-bojarupv-ccmu-hea-crwth-conlineBbbn-cuedinaaltojhu-conlineAcmucu-zeman.09.17.2.26.27.28.28.3.32.35.43.61.0Czech-English00.250.50.751refonlineBonlineAupv-chuicongjhucmu-hea-cbbn-c45jhu-cdfkiupccu-zemancambcolumbia.29.36.4.43.46.47.51.51.52.54.54.6.64.71.98Spanish-English00.250.50.751refcmu-hyp-cuedinbbn-crwth-conlineBcambraliliumdcu-cligcmu-statxfernrcdfkijhujhu-cupv-clium-ccmu-hea-climsionlineAhuicongcu-zemangenevarwth.2.24.24.32.33.35.35.4.4.4.4.4.43.44.45.45.46.48.5.51.53.56.58.7.91French-EnglishFigure 3: The percent of time that each system?s edited output was judged to be an acceptable translation.These numbers also include judgments of the system?s output when it was marked either incomprehen-sible or acceptable and left unedited.
Note that the reference translation was edited alongside the systemoutputs.
Error bars show one positive and one negative standard deviation for the systems in that lan-guage pair.28a shorter description when concatenated be-fore compression than when concatenated af-ter compression.
MT-NCD does not requireany language specific resources.?
MT-mNCD ?
Enhances MT-NCD with flex-ible word matching provided by stemmingand synonyms.
It works analogously toM-BLEU and M-TER and uses METEOR?saligner module to find relaxed word-to-wordalignments.
MT-mNCD exploits EnglishWordNet data and increases correlation to hu-man judgments for English over MT-NCD.Due to a processing issue inherent to the metric,the scores reported were generated excluding thefirst segment of each document.
Also, a separateissue was found for the MT-mNCD metric, and ac-cording to the developer the scores reported herewould like change with a correction of the issue.BabbleQuest International8?
Badger 2.0 full ?
Uses the Smith-Watermanalignment algorithm with Gotoh improve-ments to measure segment similarity.
Thefull version uses a multilingual knowledgebase to assign a substitution cost which sup-ports normalization of word infection andsimilarity.?
Badger 2.0 lite ?
The lite version uses defaultgap, gap extension and substitution costs.City University of Hong Kong (Wong and Kit,2010)?
ATEC 2.1 ?
This version of ATEC extendsthe measurement of word choice and word or-der by various means.
The former is assessedby matching word forms at linguistic levels,including surface form, stem, sense and se-mantic similarity, and further by weightingthe informativeness of both matched and un-matched words.
The latter is quantified interm of the discordance of word position andword sequence between an MT output and itsreference.Due to a version discrepancy of the metric, finalscores for ATECD-2.1 differ from those reportedhere, but only minimally.8http://www.babblequest.com/badger2Carnegie Mellon University (Denkowski andLavie, 2010)?
METEOR-NEXT-adq ?
Evaluates a machinetranslation hypothesis against one or morereference translations by calculating a simi-larity score based on an alignment betweenthe hypothesis and reference strings.
Align-ments are based on exact, stem, synonym,and paraphrase matches between words andphrases in the strings.
Metric parameters aretuned to maximize correlation with humanjudgments of translation quality (adequacyjudgments).?
METEOR-NEXT-hter ?
METEOR-NEXTtuned to HTER.?
METEOR-NEXT-rank ?
METEOR-NEXTtuned to human judgments of rank.Columbia University9?
SEPIA ?
A syntactically-aware machinetranslation evaluation metric designed withthe goal of assigning bigger weight to gram-matical structural bigrams with long surfacespans that cannot be captured with surface n-gram metrics.
SEPIA uses a dependency rep-resentation produced for both hypothesis andreference(s).
SEPIA is configurable to allowusing different combinations of structural n-grams, surface n-grams, POS tags, depen-dency relations and lemmatization.
SEPIA isa precision-based metric and as such employsclipping and length penalty to minimize met-ric gaming.Charles University Prague (Bojar and Kos,2010)?
SemPOS ?
Computes overlapping of autose-mantic (content-bearing) word lemmas in thecandidate and reference translations given afine-grained semantic part of speech (sem-pos) and outputs average overlapping scoreover all sempos types.
The overlapping is de-fined as the number of matched lemmas di-vided by the total number of lemmas in thecandidate and reference translations havingthe same sempos type.9http://www1.ccls.columbia.edu/?SEPIA/29?
SemPOS-BLEU ?
A linear combination ofSemPOS and BLEU with equal weights.BLEU is computed on surface forms of au-tosemantic words that are used by SemPOS,i.e.
auxiliary verbs or prepositions are nottaken into account.Dublin City University (He et al, 2010)?
DCU-LFG ?
A combination of syntactic andlexical information.
It measures the similar-ity of the hypothesis and reference in termsof matches of Lexical Functional Grammar(LFG) dependency triples.
The matchingmodule can also access the WordNet syn-onym dictionary and Snover?s paraphrasedatabase10.University of Edinburgh (Birch and Osborne,2010)?
LRKB4 ?
A novel metric which directly mea-sures reordering success using Kendall?s taupermutation distance metrics.
The reorderingcomponent is combined with a lexical metric,capturing the two most important elementsof translation quality.
This simple combinedmetric only has one parameter, which makesits scores easy to interpret.
It is also fastto run and language-independent.
It usesKendall?s tau permutation.?
LRHB4 ?
LRKB4, replacing Kendall?s taupermutation distance metric with the Ham-ming distance permutation distance metric.Due to installation issues, the reported submittedscores for these two metrics have not been verifiedto produce identical scores at NIST.Harbin Institute of Technology, China?
I-letter-BLEU ?
Normal BLEU based on let-ters.
Moreover, the maximum length of N-gram is decided by the average length foreach sentence, respectively.?
I-letter-recall ?
A geometric mean of N-gramrecall based on letters.
Moreover, the maxi-mum length of N-gram is decided by the av-erage length for each sentence, respectively.10Available at http://www.umiacs.umd.edu/?snover/terp/.?
SVM-RANK ?
Uses support vector ma-chines rank models to predict an orderingover a set of system translations with lin-ear kernel.
Features include Meteor-exact,BLEU-cum-1, BLEU-cum-2, BLEU-cum-5,BLEU-ind-1, BLEU-ind-2, ROUGE-L re-call, letter-based TER, letter-based BLEU-cum-5, letter-based ROUGE-L recall, andletter-based ROUGE-S recall.National University of Singapore (Liu et al,2010)?
TESLA-M ?
Based on matching of bags ofunigrams, bigrams, and trigrams, with con-sideration of WordNet synonyms.
The matchis done in the framework of real-valued lin-ear programming to enable the discounting offunction words.?
TESLA ?
Built on TESLA-M, this metricalso considers bilingual phrase tables to dis-cover phrase-level synonyms.
The featureweights are tuned on the development datausing SVMrank.Stanford University?
Stanford ?
A discriminatively trainedstring-edit distance metric with varioussimilarity-matching, synonym-matching, anddependency-parse-tree-matching features.The model resembles a Conditional RandomField, but performs regression instead ofclassification.
It is trained on Arabic, Chi-nese, and Urdu data from the MT-Eval 2008dataset.Due to installation issues, the reported scores forthis metric have not been verified to produce iden-tical scores at NIST.University of Maryland11?
TER-plus (TERp) ?
An extension of theTranslation Edit Rate (TER) metric that mea-sures the number of edits between a hypoth-esized translation and a reference translation.TERp extends TER by using stemming, syn-onymy, and paraphrases as well as tunableedit costs to better measure the distance be-tween the two translations.
This versionof TERp improves upon prior versions byadding brevity and length penalties.11http://www.umiacs.umd.edu/?snover/terp30Scores were not submitted along with this metric,and due to installation issues were not produced atNIST in time to be included in this report.University Polite`cnica de Catalunya/Universityde Barcelona (Comelles et al, 2010)?
DR ?
An arithmetic mean over a set ofthree metrics based on discourse representa-tions, respectively computing lexical overlap,morphosyntactic overlap, and semantic treematching.?
DRdoc ?
Is analogous to DR but, instead ofoperating at the segment level, it analyzessimilarities over whole document discourserepresentations.?
ULCh ?
An arithmetic mean over aheuristically-defined set of metrics operat-ing at different linguistic levels (ROUGE,METEOR, and measures of overlap betweenconstituent parses, dependency parses, se-mantic roles, and discourse representations).University of Southern California, ISI?
BEwT-E ?
Basic Elements with Transfor-mations for Evaluation, is a recall-orientedmetric that compares basic elements, smallportions of contents, between the two trans-lations.
The basic elements (BEs) consistof content words and various combinationsof syntactically-related words.
A variety oftransformations are performed to allow flexi-ble matching so that words and syntactic con-structions conveying similar content in dif-ferent manners may be matched.
The trans-formations cover synonymy, preposition vs.noun compounding, differences in tenses,etc.
BEwT-E was originally created for sum-marization evaluation and is English-specific.?
Bkars ?
Measures overlap between charactertrigrams in the system and reference trans-lations.
It is heavily weighted toward recalland contains a fragmentation penalty.
Bkarsproduces a score both with and without stem-ming (using the Snowball package of stem-mers) and averages the results together.
It isnot English-specific.Scores were not submitted for BEwT-E; the run-time required for this metric to process the WMT-10 data set prohibited the production of scores intime for publication.6 Evaluation task resultsThe results reported here are preliminary; a finalrelease of results will be published on the WMT10website before July 15, 2010.
Metric developerssubmitted metrics for installation at NIST; theywere also asked to submit metric scores on theWMT10 test set alng with their metrics.
Notall developers submitted scores, and not all met-rics were verified to produce the same scores assubmitted at NIST in time for publication.
Anysuch caveats are reported with the description ofthe metrics above.The results reported here are limited to a com-parison of metric scores on the full WMT10test set with human assessments on the human-assessed subset.
An analysis comparing the hu-man assessments with the automatic metrics runonly on the human-assessed subset will follow ata later date.The WMT10 system output used to generatethe reported metric scores was found to have im-properly escaped characters for a small number ofsegments.
While we plan to regenerate the met-ric scores with this issue resolved, we do not ex-pect this to significantly alter the results, given thesmall number of segments affected.6.1 System Level Metric ScoresThe tables in Appendix B list the metric scoresfor the language pairs processed by each metric.These first four tables present scores for transla-tions out of English into Czech, French, Germanand Spanish.
In addition to the metric scores ofthe submitted metrics identified above, we alsopresent (1) the ranking of the system as deter-mined by the human assessments; and (2) themetrics scores for two popular baseline metrics,BLEU as calculated by NIST?s mteval software12and the NIST score.
For each method of systemmeasurement the absolute highest score is identi-fied by being outlined in a box.Similarly, the remaining tables in Appendix Blist the metric scores for the submitted metrics andthe two baseline metrics, and the ranking basedon the human assessments for translations into En-glish from Czech, French, German and Spanish.As some metrics employ language-specific re-sources, not all metrics produced scores for all lan-guage pairs.12ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v13a-20091001.tar.gz31cz-enfr-ende-enes-enavgSemPOS .78 .77 .60 .95 .77IQmt-DRdoc .61 .79 .65 .98 .76SemPOS-BLEU .75 .70 .61 .96 .75i-letter-BLEU .71 .70 .60 .98 .75NIST .85 .72 .55 .86 .74TESLA .70 .70 .60 .97 .74MT-NCD .71 .72 .58 .95 .74Bkars .71 .67 .58 .98 .74ATEC-2.1 .71 .67 .59 .96 .73meteor-next-rank .69 .68 .60 .96 .73IQmt-ULCh .70 .64 .60 .99 .73IQmt-DR .68 .67 .60 .97 .73meteor-next-hter .71 .66 .59 .95 .73meteor-next-adq .69 .67 .60 .96 .73badger-2.0-lite .70 .70 .56 .94 .73DCU-LFG .69 .69 .58 .96 .73badger-2.0-full .69 .70 .57 .94 .73SEPIA .71 .70 .57 .92 .73SVM-rank .66 .65 .61 .98 .73i-letter-recall .65 .64 .61 .98 .72TESLA-M .67 .67 .57 .95 .72BLEU-4-v13a .69 .68 .52 .90 .70LRKB4 .63 .62 .53 .89 .67LRHB4 .62 .65 .50 .87 .66MT-mNCD .69 .64 .52 .70 .64Stanford .58 .19 .60 .46 .46Table 7: The system-level correlation of the au-tomatic evaluation metrics with the human judg-ments for translation into English.It is noticeable that system combinations are of-ten among those achieving the highest scores.6.2 System-Level CorrelationsTo assess the performance of the automatic met-rics, we correlated the metrics?
scores with the hu-man rankings at the system level.
We assigned aconsolidated human-assessment rank to each sys-tem based on the number of times that the givensystem?s translations were ranked higher than orequal to the translations of any other system inthe manual evaluation of the given language pair.We then compared the ranking of systems by thehuman assessments to that provided by the au-tomatic metric system level scores on the com-plete WMT10 test set for each language pair, us-ing Spearman?s ?
rank correlation coefficient.
Thecorrelations are shown in Table 7 for translationsto English, and Table 8 out of English, with base-line metrics listed at the bottom.
The highest cor-relation for each language pair and the highestoverall average are bolded.Overall, correlations are higher for translationsto English than compared to translations from En-glish.
For all language pairs, there are a numberof new metrics that yield noticeably higher corre-en-czen-fren-deen-esavgSVM-rank .29 .54 .68 .67 .55TESLA-M .27 .49 .74 .66 .54LRKB4 .39 .58 .47 .71 .54i-letter-recall .28 .51 .61 .66 .52LRHB4 .39 .59 .41 .63 .51i-letter-BLEU .26 .49 .56 .65 .49ATEC-2.1 .38 .52 .44 .62 .49badger-2.0-full .37 .58 .41 .59 .49Bkars .22 .54 .52 .66 .48BLEU-4-v13a .35 .58 .39 .57 .47badger-2.0-lite .32 .57 .41 .59 .47TESLA .09 .62 .66 .50 .47meteor-next-rank .34 .59 .39 .51 .46Stanford .34 .48 .70 .32 .46MT-NCD .17 .54 .51 .61 .46NIST .30 .52 .41 .50 .43MT-mNCD .26 .49 .17 .43 .34SemPOS .31 n/a n/a n/a .31SemPOS-BLEU .29 n/a n/a n/a .29Table 8: The system-level correlation of the au-tomatic evaluation metrics with the human judg-ments for translation out of English.lations with human assessments than either of thetwo included baseline metrics.
In particular, Bleuperformed in the bottom half of the into-Englishand out-of-English directions.6.3 Segment-Level Metric AnalysisThe method employed to collect human judgmentsof rank preferences at the segment level producesa sparse matrix of decision points.
It is unclearwhether attempts to normalize the segment levelrankings to 0.0?1.0 values, representing the rela-tive rank of a system per segment given the num-ber of comparisons it is involved with, is proper.An intuitive display of how well metrics mirror thehuman judgments may be shown via a confusionmatrix.
We compare the human ranks to the ranksas determined by a metric.
Below, we show an ex-ample of the confusion matrix for the SVM-rankmetric which had the highest summed diagonal(occurrences when a particular rank by the met-ric?s score exactly matches the human judgments)for all segments translated into English.
The num-bers provided are percentages of the total count.The summed diagonal constitutes 39.01% of allcounts in this example matrix.
The largest cell isthe 1/1 ranking cell (top left).
We included thereference translation as a system in this analysis,which is likely to lead to a lot of agreement on thehighest rank between humans and automatic met-rics.32Metric Human RankRank 1 2 3 4 51 12.79 4.48 2.75 1.82 0.922 2.77 7.94 5.55 3.79 2.23 1.57 4.29 6.74 5.4 4.464 0.97 2.42 3.76 4.99 6.55 0.59 1.54 1.84 3.38 6.55No allowances for ties were made in this analy-sis.
That is, if a human ranked two system transla-tions the same, this analysis expects the metrics toprovide the same score in order to get them bothcorrect.
Future analysis could relax this constraint.As not all human rankings start with the highestpossible rank of ?1?
(due to ties and withholdingjudgment on a particular system output being al-lowed), we set the highest automatic metric rankto the highest human rank and shifted the lowermetric ranks down accordingly.Table 9 shows the summed diagonal percent-ages of the total count of all datapoints for all met-rics that WMT10 scores were available for, bothcombined for all languages to English (X-English)and separately for each language into English.The results are ordered by the highest percent-age for the summed diagonal on all languagesto English combined.
There are quite noticeablechanges in ranking of the metrics for the separatelanguage pairs; further analysis into the reasonsfor this will be necessary.We plan to also analyze metric performance fortranslation into English.7 Feasibility of Using Non-ExpertAnnotators in Future WMTsIn this section we analyze the data that we col-lected data by posting the ranking task on Ama-zon?s Mechanical Turk (MTurk).
Although we didnot use this data when creating the official results,our hope was that it may be useful in future work-shops in two ways.
First, if we find that it is pos-sible to obtain a sufficient amount of data of goodquality, then we might be able to reduce the timecommitment expected from the system develop-ers in future evaluations.
Second, the additionalcollected labels might enable us to detect signifi-cant differences between systems that would oth-erwise be insignificantly different using only thedata from the volunteers (which we will now referto as the ?expert?
data).7.1 Data collectionTo that end, we prepared 600 ranking sets for eachof the eight language pairs, with each set con-taining five MT outputs to be ranked, using thesame interface used by the volunteers.
We postedthe data to MTurk and requested, for each one,five redundant assignments, from different work-ers.
Had all the 5?
8?
600 = 24,000 assignmentsbeen completed, we would have obtained 24,000?
5 = 120,000 additional rank labels, comparedto the 37,884 labels we collected from the volun-teers (Table 3).
In actuality, we collected closer to55,000 rank labels, as we discuss shortly.To minimize the amount of data that is of poorquality, we placed two requirements that must besatisfied by any worker before completing any ofour tasks.
First, we required that a worker have anexisting approval rating of at least 85%.
Second,we required a worker to reside in a country wherethe target language of the task can be assumed tobe the spoken language.
Finally, anticipating alarge pool of workers located in the United States,we felt it possible for us to add a third restrictionfor the *-to-English language pairs, which is that aworker must have had at least five tasks previouslyapproved on MTurk.13 We organized the rankingsets in groups of 3 per screen, with a monetary re-ward of $0.05 per screen.When we created our tasks, we had no expecta-tion that all the assignments would be completedover the tasks?
lifetime of 30 days.
This was in-deed the case (Table 10), especially for languagepairs with a non-English target language, due toworkers being in short supply outside the US.Overall, we see that the amount of data collectedfrom non-US workers is relatively small (left halfof Table 10), whereas the pool of US-based work-ers is much larger, leading to much higher com-pletion rates for language pairs with English as thetarget language (right half of Table 10).
This is inspite of the additional restriction we placed on USworkers.13We suspect that newly registered workers on MTurk al-ready start with an ?approval rating?
of 100%, and so requir-ing a high approval rating alone might not guard against newworkers.
It is not entirely clear if our suspicion is true, but ourpast experiences with MTurk usually involved a noticeablyfaster completion rate than what we experienced this timearound, indicating our suspicion might very well be correct.33Metric *-English Czech-English French-English German-English Spanish-EnglishSVM-rank 39.01 41.21 36.07 38.81 40.3i-letter-recall 38.85 41.71 36.19 38.8 39.5MT-NCD 38.77 42.55 35.31 38.7 39.48i-letter-BLEU 38.69 40.54 36.05 38.82 39.64meteor-next-rank 38.5 40.1 34.41 39.25 40.05meteor-next-adq 38.27 39.58 34.41 39.5 39.35meteor-next-hter 38.21 38.61 34.1 39.13 40.18Bkars 37.98 40.1 35.08 38.6 38.52Stanford 37.97 39.87 36.19 38.27 38.09ATEC-2.1 37.95 40.06 34.96 38.6 38.53TESLA 37.57 38.68 34.38 38.67 38.36NIST 37.47 39.54 35.54 37.13 38.2SemPOS 37.21 38.8 37.39 35.73 37.69SemPOS-BLEU 37.16 38.05 36.57 37.11 37.21badger-2.0-full 37.12 37.5 36 36.21 38.62badger-2.0-lite 37.08 37.2 35.88 36.23 38.69SEPIA 37.06 38.98 34.6 36.46 38.52BLEU-4-v13a 36.71 37.83 34.84 36.44 37.81LRHB4 36.14 38.35 34.65 34.24 37.93TESLA-M 36.13 37.01 34 35.79 37.6LRKB4 36.12 38.72 33.47 35.25 37.63IQmt-ULCh 35.86 37.64 33.95 35.81 36.45IQmt-DR 35.77 36.27 34.43 34.43 37.74DCU-LFG 34.72 36.38 32.29 33.87 36.49MT-mNCD 34.51 34.93 31.78 35.73 35.13IQmt-DRdoc 31.9 33.85 28.99 32.9 32.18Table 9: The segment-level performance for metrics for the into-English direction.en-de en-es en-fr en-cz de-en es-en fr-en cz-enLocation DE ES/MX FR CZ US US US USCompleted 1 time 37% 38% 29% 19% 3.5% 1.5% 14% 2.0%Completed 2 times 18% 14% 12% 1.5% 6.0% 5.5% 19% 4.5%Completed 3 times 2.5% 4.5% 0.5% 0.0% 8.5% 11% 20% 10%Completed 4 times 1.5% 0.5% 0.5% 0.0% 22% 19% 23% 17%Completed 5 times 0.0% 0.5% 0.0% 0.0% 60% 63% 22% 67%Completed ?
once 59% 57% 42% 21% 100% 99% 96% 100%Label count 2,583 2,488 1,578 627 12,570 12,870 9,197 13,169(% of expert data) (38%) (96%) (40%) (9%) (241%) (228%) (222%) (490%)Table 10: Statistics for data collected on MTurk for the ranking task.
In total, 55,082 rank labels werecollected across the eight language pairs (145% of expert data).
Each language pair had 600 sets, andwe requested each set completed by 5 different workers.
Since each set provides 5 labels, we could havepotentially obtained 600 ?
5 ?
5 = 15,000 labels for each language pair.
The Label count row indicatesto what extent that potential was met (over the 30-day lifetime of our tasks), and the ?Completed...?
rowsgive a breakdown of redundancy.
For instance, the right-most column indicates that, in the cz-en group,2.0% of the 600 sets were completed by only one worker, while 67% of the sets were completed by 5workers, with 100% of the sets completed at least once.
The total cost of this data collection effort wasroughly $200.34INTER-ANNOTATOR AGREEMENTP (A) K K?With references 0.466 0.198 0.487Without references 0.441 0.161 0.439INTRA-ANNOTATOR AGREEMENTP (A) K K?With references 0.539 0.309 0.633Without references 0.538 0.307 0.601Table 11: Inter- and intra-annotator agreement forthe MTurk workers on the sentence ranking task.
(As before, P (E) is 0.333.)
For comparison, werepeat here the kappa coefficients of the experts(K?
), taken from Table 4.7.2 Quality of MTurk dataIt is encouraging to see that we can collect a largeamount of rank labels from MTurk.
That said, westill need to guard against data from bad work-ers, who are either not being faithful and click-ing randomly, or who might simply not be compe-tent enough.
Case in point, if we examine inter-and intra-annotator agreement on the MTurk data(Table 11), we see that the agreement rates aremarkedly lower than their expert counterparts.Another indication of the presence of bad work-ers is a low reference preference rate (RPR),which we define as the proportion of time a ref-erence translation wins (or ties in) a comparisonwhen it appears in one.
Intuitively, the RPRshould be quite high, since it is quite rare that anMT output ought to be judged better than the refer-ence.
This rate is 96.5% over the expert data, butonly 83.7% over the MTurk data.
Compare thisto a randomly-clicking RPR of 66.67% (becausethe two acceptable answers are that the referenceis either better than a system?s output or tied withit).Also telling would be the rate at which MTurkworkers agree with experts.
To ensure that we ob-tain enough overlapping data to calculate such arate, we purposely select one-sixth14 of our rank-ing sets so that the five-system group is exactly onethat has been judged by an expert.
This way, atleast one-sixth of the comparisons obtained froman MTurk worker?s labels are comparisons for14This means that on average Turkers ranked a set of sys-tem outputs that had been ranked by experts on every otherscreen, since each screen?s worth of work had three sets.which we already have an expert judgment.
Whenwe calculate the rate of agreement on this data,we find that MTurk workers agree with the ex-pert workers 53.2% of the time, or K = 0.297, andwhen references are excluded, the agreement rateis 50.0%, or K = 0.249.
Ideally, we would wantthose values to be in the 0.4?0.5 range, since thatis where the inter-annotator kappa coefficient liesfor the expert annotators.7.3 Filtering MTurk data by agreement withexpertsWe can use the agreement rate with experts toidentify MTurk workers who are not performingthe task as required.
For each worker w of the669 workers for whom we have such data, wecompute the worker?s agreement rate with the ex-perts, and from it a kappa coefficient Kexp(w) forthat worker.
(Given that P (E) is 0.333, Kexp(w)ranges between?0.5 and +1.0.)
We sort the work-ers based on Kexp(w) in ascending order, and ex-amine properties of the MTurk data as we removethe lowest-ranked workers one by one (Figure 4).We first note that the amount of data we ob-tained from MTurk is so large, that we could af-ford to eliminate close to 30% of the labels, andwe would still have twice as much data than us-ing the expert data alone.
We also note that twoworkers in particular (the 103rd and 130th to beremoved) are likely responsible for the majorityof the bad data, since removing their data leads tonoticeable jumps in the reference preference rateand the inter-annotator agreement rate (right twocurves of Figure 4).
Indeed, examining the data forthose two workers, we find that their RPR valuesare 55.7% and 51.9%, which is a clear indicationof random clicking.15Looking again at those two curves shows de-grading values as we continue to remove workersin large droves, indicating a form of ?overfitting?to agreement with experts (which, naturally, con-tinues to increase until reaching 1.0; bottom leftcurve).
It is therefore important, if one were to fil-ter out the MTurk data by removing workers thisway, to choose a cutoff carefully so that no crite-rion is degraded dramatically.In Appendix A, after reporting head-to-headcomparisons using only the expert data, we alsoreport head-to-head comparisons using the expert15In retrospect, we should have performed this type ofanalysis as the data was being collected, since such workerscould have been identified early on and blocked.35-204060801001201401600 100 200 300 400 500 600 700# Workers RemovedMTurkDataRemaining(%ofExpertData)-0.10.20.30.40.50.60.70.80.91.00 100 200 300 400 500 600 700# Workers RemovedAgreementwithExpertData(kappa)82%84%86%88%90%92%94%96%98%100%0 100 200 300 400 500 600 700# Workers RemovedReferencePreferenceRate0.100.150.200.250.300 100 200 300 400 500 600 700# Workers RemovedInter-AnnotatorAgreement(kappa)Figure 4: The effect of removing an increasing number of MTurk workers.
The order in which workersare removed is by Kexp(w), the kappa agreement coefficient with expert data (excluding references).data combined with the MTurk data, in order tobe able to detect more significant differences be-tween the systems.
We choose the 300-workerpoint as a reasonable cutoff point before combin-ing the MTurk data with the expert data, basedon the characteristics of the MTurk data at thatpoint: a high reference preference rate, high inter-annotator agreement, and, critically, a kappa co-efficient vs. expert data of 0.449, which is closeto the expert inter-annotator kappa coefficient of0.439.7.4 Feasibility of using only MTurk dataIn the previous subsection, we outlined an ap-proach by which MTurk data can be filtered outusing expert data.
Since we were to combine thefiltered MTurk data with the expert data to ob-tain more significant differences, it was reason-able to use agreement with experts to quantify theMTurk workers?
competency.
However, we alsowould like to know whether it is feasible to use theMTurk data alone.
Our aim here is not to boost thedifferences we see by examining expert data, butto eliminate our reliance on obtaining expert datain the first place.We briefly examined some simple ways of fil-tering/combining the MTurk data, and measuredthe Spearman rank correlations obtained from theMTurk data (alone), as compared to the rankingsobtained using the expert data (alone), and reportthem in Table 12.
(These correlations do not in-clude the references.
)We first see that even when using the MTurkdata untouched, we already obtain relatively highcorrelation with expert ranking (?Unfiltered?
).This is especially true for the *-to-English lan-guage pairs, where we collected much more datathan English-to-*.
In fact, the relationship be-tween the amount of data and the correlation val-ues is very strong, and it is reasonable to expectthe correlation numbers for English-to-* to catchup had more data been collected.We also measure rank correlations when apply-ing some simple methods of cleaning/weightingMTurk data.
The first method (?Voting?)
is per-forming a simple vote whenever redundant com-parisons (i.e.
from different workers) are avail-able.
The second method (?Kexp-filtered?)
first re-moves labels from the 300 worst workers accord-ing to agreement with experts.
The third method36(?RPR-filtered?)
first removes labels from the 62worst workers according to their RPR.
The num-bers 300 and 62 were chosen since those are thepoints at which the MTurk data reaches the levelof expert data in the inter-annotator agreement andRPR of the experts.The fourth and fifth methods (?Weighted byKexp?
and ?Weighted by K(RPR)?)
do not re-move any data, instead assigning weights to work-ers based on their agreement with experts and theirRPR, respectively.
Namely, for each worker, theweight assigned by the fourth method is Kexp forthat worker, and the weight assigned by the fifthmethod is K(RPR) for that worker.Examining the correlation coefficients obtainedfrom those methods (Table 12), we see mixed re-sults, and there is no clear winner among thosemethods.
It is also difficult to draw any conclusionas to which method performs best when.
However,it is encouraging to see that the two RPR-basedmethods perform well.
This is noteworthy, sincethere is no need to use expert data to weight work-ers, which means that it is possible to evaluate aworker using inherent, ?built-in?
properties of thatworker?s own data, without resorting to makingcomparisons with other workers or with experts.8 SummaryAs in previous editions of this workshop we car-ried out an extensive manual and automatic eval-uation of machine translation performance fortranslating from European languages into English,and vice versa.The number of participants grew substantiallycompared to previous editions of the WMT work-shop, with 33 groups from 29 institutions partic-ipating in WMT10.
Most groups participated inthe translation task only, while the system combi-nation task attracted a somewhat smaller numberof participantsUnfortunately, fewer rule-based systems partic-ipated in this year?s edition of WMT, comparedto previous editions.
We hope to attract morerule-based systems in future editions as they in-crease the variation of translation output and forsome language pairs, such as German-English,tend to outperform statistical machine translationsystems.This was the first time that the WMT workshopwas held as a joint workshop with NIST?s Metric-sMATR evaluation initiative.
This joint effort wasvery productive as it allowed us to focus more onthe two evaluation dimensions: manual evaluationof MT performance and the correlation betweenmanual metrics and automated metrics.This year was also the first time we have in-troduced quality assessments by non-experts.
Inprevious years all assessments were carried outthrough peer evaluation exclusively consisting ofdevelopers of machine translation systems, andthereby people who are used to machine transla-tion output.
This year we have facilitated Ama-zon?s Mechanical Turk to investigate two as-pects of manual evaluation: How stable are man-ual assessments across different assessor profiles(experts vs. non-experts) and how reliable arequality judgments of non-expert users?
Whilethe intra- and inter-annotator agreements betweennon-expert assessors are considerably lower thanfor their expert counterparts, the overall rankingsof translation systems exhibit a high degree of cor-relation between experts and non-experts.
Thiscorrelation can be further increased by applyingvarious filtering strategies reducing the impact ofunreliable non-expert annotators.As in previous years, all data sets generated bythis workshop, including the human judgments,system translations and automatic scores, are pub-licly available for other researchers to analyze.16AcknowledgmentsThis work was supported in parts by the Euro-MatrixPlus project funded by the European Com-mission (7th Framework Programme), the GALEprogram of the US Defense Advanced ResearchProjects Agency, Contract No.
HR0011-06-C-0022, and the US National Science Foundation un-der grant IIS-0713448.ReferencesAlexandre Allauzen, Josep M. Crego, lknur Durgar El-Kahlout, and Francois Yvon.
2010.
Limsi?s statisti-cal translation systems for wmt?10.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 29?34, Upp-sala, Sweden, July.
Association for ComputationalLinguistics.Lo?
?c Barrault.
2010.
Many: Open source mt systemcombination at wmt?10.
In Proceedings of the JointFifth Workshop on Statistical Machine Translation16http://www.statmt.org/wmt09/results.html37Label Unfiltered Voting Kexp-filtered RPR-filtered Weighted by Weighted bycount Kexp K(RPR)en-de 2,583 0.862 0.779 0.818 0.862 0.868 0.862en-es 2,488 0.759 0.785 0.797 0.797 0.768 0.806en-fr 1,578 0.826 0.840 0.791 0.814 0.802 0.814en-cz 627 0.833 0.818 0.354 0.833 0.851 0.828de-en 12,570 0.914 0.925 0.920 0.931 0.933 0.926es-en 12,870 0.934 0.969 0.965 0.987 0.978 0.987fr-en 9,197 0.880 0.865 0.920 0.919 0.907 0.917cz-en 13,169 0.951 0.909 0.965 0.944 0.930 0.944Table 12: Spearman rank coefficients for the MTurk data across the various language pairs, using differ-ent methods to clean the data or weight workers.
(These correlations were computed after excluding thereferences.)
Kexp is the kappa coefficient of the worker?s agreement rate with experts, with P (A) = 0.33.K(RPR) is the kappa coefficient of the worker?s RPR (see 7.2), with P (A) = 0.66.
In Kexp-filtering,42% of labels remain, after removing 300 workers.
In K(RPR)-filtering, 69% of labels remain, afterremoving 62 workers.and MetricsMATR, pages 252?256, Uppsala, Swe-den, July.
Association for Computational Linguis-tics.Ergun Bicici and S. Serdar Kozat.
2010.
Adaptivemodel weighting and transductive regression for pre-dicting best system combinations.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 257?262, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Ergun Bicici and Deniz Yuret.
2010.
L1 regularizedregression for reranking and system combination inmachine translation.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 263?270, Uppsala, Swe-den, July.
Association for Computational Linguis-tics.Alexandra Birch and Miles Osborne.
2010.
Lrscore forevaluating lexical and reordering quality in mt.
InProceedings of the Joint Fifth Workshop on Statisti-cal Machine Translation and MetricsMATR, pages302?307, Uppsala, Sweden, July.
Association forComputational Linguistics.Ondrej Bojar and Kamil Kos.
2010.
2010 failuresin english-czech phrase-based mt.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 35?41, Upp-sala, Sweden, July.
Association for ComputationalLinguistics.Chris Callison-Burch and Mark Dredze.
2010.
Creat-ing speech and language data with amazons mechan-ical turk.
In Proceedings NAACL-2010 Workshop onCreating Speech and Language Data With AmazonsMechanical Turk, Los Angeles.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2007.
(Meta-) evaluation of machine translation.
In Pro-ceedings of the Second Workshop on Statistical Ma-chine Translation (WMT07), Prague, Czech Repub-lic.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on StatisticalMachine Translation (WMT08), Colmbus, Ohio.Chris Callison-Burch, , Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the findingsof the 2009 workshop on statistical machine trans-lation.
In Proceedings of the Fourth Workshop onStatistical Machine Translation (WMT09), Athens,Greece.Chris Callison-Burch.
2009.
Fast, cheap, and cre-ative: Evaluating translation quality using amazon?smechanical turk.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-2009), Singapore.Elisabet Comelles, Jesus Gimenez, Lluis Marquez,Irene Castellon, and Victoria Arranz.
2010.Document-level automatic mt evaluation based ondiscourse representations.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 308?313, Uppsala,Sweden, July.
Association for Computational Lin-guistics.Hui Cong, Zhao Hai, Lu Bao-Liang, and Song Yan.2010.
An empirical study on development set se-lection strategy for machine translation learning.In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 42?46, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Michael Denkowski and Alon Lavie.
2010.
Meteor-next and the meteor paraphrase tables: Improved38evaluation support for five target languages.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 314?317, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Marcus Dobrinkat, Tero Tapiovaara, Jaakko Va?yrynen,and Kimmo Kettunen.
2010.
Normalized compres-sion distance based measures for metricsmatr 2010.In Proceedings of the Joint Fifth Workshop on Statis-tical Machine Translation and MetricsMATR, pages318?323, Uppsala, Sweden, July.
Association forComputational Linguistics.Jinhua Du, Pavel Pecina, and Andy Way.
2010.
Anaugmented three-pass system combination frame-work: Dcu combination system for wmt 2010.
InProceedings of the Joint Fifth Workshop on Statisti-cal Machine Translation and MetricsMATR, pages271?276, Uppsala, Sweden, July.
Association forComputational Linguistics.Vladimir Eidelman, Chris Dyer, and Philip Resnik.2010.
The university of maryland statistical ma-chine translation system for the fifth workshop onmachine translation.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 47?51, Uppsala, Sweden,July.
Association for Computational Linguistics.Christian Federmann, Andreas Eisele, Yu Chen, SabineHunsicker, Jia Xu, and Hans Uszkoreit.
2010.Further experiments with shallow hybrid mt sys-tems.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 52?56, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Jesu?s Gonza?lez-Rubio, Germa?n Sanchis-Trilles, Joan-Andreu Sa?nchez, Jesu?s Andre?s-Ferrer, GuillemGasco?, Pascual Mart?
?nez-Go?mez, Martha-AliciaRocha, and Francisco Casacuberta.
2010.
The upv-prhlt combination system for wmt 2010.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 277?281, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Greg Hanneman, Jonathan Clark, and Alon Lavie.2010.
Improved features and grammar selection forsyntax-based mt.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation andMetricsMATR, pages 57?62, Uppsala, Sweden, July.Association for Computational Linguistics.Christian Hardmeier, Arianna Bisazza, and MarcelloFederico.
2010.
Fbk at wmt 2010: Word lattices formorphological reduction and chunk-based reorder-ing.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 63?67, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Yifan He, Jinhua Du, Andy Way, and Josef van Gen-abith.
2010.
The dcu dependency-based metric inwmt-metricsmatr 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 324?328, Uppsala, Swe-den, July.
Association for Computational Linguis-tics.Kenneth Heafield and Alon Lavie.
2010.
Cmu multi-engine machine translation for wmt 2010.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 68?73, Uppsala, Sweden, July.
Association for Compu-tational Linguistics.Carmen Heger, Joern Wuebker, Matthias Huck, GregorLeusch, Saab Mansour, Daniel Stein, and HermannNey.
2010.
The rwth aachen machine translationsystem for wmt 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 74?78, Uppsala, Sweden,July.
Association for Computational Linguistics.Carlos A.
Henr?
?quez Q., Marta Ruiz Costa-jussa`, Vi-das Daudaravicius, Rafael E. Banchs, and Jose?
B.Marin?o.
2010.
Using collocation segmentation toaugment the phrase table.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 79?83, Uppsala,Sweden, July.
Association for Computational Lin-guistics.Almut Silja Hildebrand and Stephan Vogel.
2010.Cmu system combination via hypothesis selectionfor wmt?10.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metric-sMATR, pages 282?285, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Ste?phane Huet, Julien Bourdaillet, Alexandre Patry,and Philippe Langlais.
2010.
The rali machinetranslation system for wmt 2010.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 84?90, Upp-sala, Sweden, July.
Association for ComputationalLinguistics.Michael Jellinghaus, Alexandros Poulis, and DavidKolovratn??k.
2010.
Exodus - exploring smt for euinstitutions.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metric-sMATR, pages 91?95, Uppsala, Sweden, July.
Asso-ciation for Computational Linguistics.Philipp Koehn and Christof Monz.
2006.
Manual andautomatic evaluation of machine translation betweenEuropean languages.
In Proceedings of NAACL2006 Workshop on Statistical Machine Translation,New York, New York.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the ACL-2007 Demo and Poster Ses-sions, Prague, Czech Republic.39Philipp Koehn, Barry Haddow, Philip Williams, andHieu Hoang.
2010.
More linguistic annotation forstatistical machine translation.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 96?101, Uppsala,Sweden, July.
Association for Computational Lin-guistics.Patrik Lambert, Sadaf Abdul-Rauf, and HolgerSchwenk.
2010.
Lium smt machine translationsystem for wmt 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 102?107, Uppsala, Swe-den, July.
Association for Computational Linguis-tics.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33:159?174.Samuel Larkin, Boxing Chen, George Foster, UlrichGermann, Eric Joanis, Howard Johnson, and RolandKuhn.
2010.
Lessons from nrcs portage system atwmt 2010.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Metric-sMATR, pages 108?113, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Gregor Leusch and Hermann Ney.
2010.
The rwthsystem combination system for wmt 2010.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 290?295, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren Thornton, Jonathan Weese, and Omar Zaidan.2009.
Joshua: An open source toolkit for parsing-based machine translation.
In Proceedings of theFourth Workshop on Statistical Machine Transla-tion, Athens, Greece, March.Zhifei Li, Chris Callison-Burch, Chris Dyer, Juri Gan-itkevitch, Ann Irvine, Sanjeev Khudanpur, LaneSchwartz, Wren Thornton, Ziyuan Wang, JonathanWeese, and Omar Zaidan.
2010.
Joshua 2.0: Atoolkit for parsing-based machine translation withsyntax, semirings, discriminative training and othergoodies.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Met-ricsMATR, pages 114?118, Uppsala, Sweden, July.Association for Computational Linguistics.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.2010.
Tesla: Translation evaluation of sentenceswith linear-programming-based analysis.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 329?334, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Sushant Narsale.
2010.
Jhu system combinationscheme for wmt 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 286?289, Uppsala, Swe-den, July.
Association for Computational Linguis-tics.Jan Niehues, Teresa Herrmann, Mohammed Mediani,and Alex Waibel.
2010.
The karlsruhe institutefor technology translation system for the acl-wmt2010.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 119?123, Uppsala, Sweden, July.
Associationfor Computational Linguistics.NIST.
2008.
Evaluation plan for gale go/no-go phase3 / phase 3.5 translation evaluations.
June 18, 2008.Sergio Penkale, Rejwanul Haque, Sandipan Dandapat,Pratyush Banerjee, Ankit K. Srivastava, Jinhua Du,Pavel Pecina, Sudip Kumar Naskar, Mikel L. For-cada, and Andy Way.
2010.
Matrex: The dcu mtsystem for wmt 2010.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 124?129, Uppsala, Swe-den, July.
Association for Computational Linguis-tics.Aaron Phillips.
2010.
The cunei machine translationplatform for wmt ?10.
In Proceedings of the JointFifth Workshop on Statistical Machine Translationand MetricsMATR, pages 130?135, Uppsala, Swe-den, July.
Association for Computational Linguis-tics.Juan Pino, Gonzalo Iglesias, Adria` de Gispert, GraemeBlackwood, Jamie Brunning, and William Byrne.2010.
The cued hifst system for the wmt10 trans-lation shared task.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation andMetricsMATR, pages 136?141, Uppsala, Sweden,July.
Association for Computational Linguistics.Marion Potet, Laurent Besacier, and Herve?
Blanchon.2010.
The lig machine translation system for wmt2010.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 142?147, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Mark Przybocki, Kay Peterson, and Sebastian Bron-sart.
2008.
Official results of the NIST 2008 ?Met-rics for MAchine TRanslation?
challenge (Metrics-MATR08).
In AMTA-2008 workshop on Metrics forMachine Translation, Honolulu, Hawaii.Antti-Veikko Rosti, Bing Zhang, Spyros Matsoukas,and Richard Schwartz.
2010.
Bbn system descrip-tion for wmt10 system combination task.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 296?301, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Markus Saers, Joakim Nivre, and Dekai Wu.
2010.Linear inversion transduction grammar alignmentsas a second translation path.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 148?152, Uppsala,40Sweden, July.
Association for Computational Lin-guistics.Germa?n Sanchis-Trilles, Jesu?s Andre?s-Ferrer, GuillemGasco?, Jesu?s Gonza?lez-Rubio, Pascual Mart?
?nez-Go?mez, Martha-Alicia Rocha, Joan-AndreuSa?nchez, and Francisco Casacuberta.
2010.Upv-prhlt english?spanish system for wmt10.
InProceedings of the Joint Fifth Workshop on Statisti-cal Machine Translation and MetricsMATR, pages153?157, Uppsala, Sweden, July.
Association forComputational Linguistics.Baskaran Sankaran, Ajeet Grewal, and Anoop Sarkar.2010.
Incremental decoding for phrase-based sta-tistical machine translation.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 197?204, Uppsala,Sweden, July.
Association for Computational Lin-guistics.Lane Schwartz.
2010.
Reproducible results in parsing-based machine translation: The jhu shared task sub-mission.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Met-ricsMATR, pages 158?163, Uppsala, Sweden, July.Association for Computational Linguistics.Sara Stymne, Maria Holmqvist, and Lars Ahrenberg.2010.
Vs and oovs: Two problems for translationbetween german and english.
In Proceedings of theJoint Fifth Workshop on Statistical Machine Trans-lation and MetricsMATR, pages 164?169, Uppsala,Sweden, July.
Association for Computational Lin-guistics.Jo?rg Tiedemann.
2010.
To cache or not to cache?experiments with adaptive models in statistical ma-chine translation.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation andMetricsMATR, pages 170?175, Uppsala, Sweden,July.
Association for Computational Linguistics.Sami Virpioja, Jaakko Va?yrynen, Andre Man-sikkaniemi, and Mikko Kurimo.
2010.
Apply-ing morphological decompositions to statistical ma-chine translation.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation andMetricsMATR, pages 176?181, Uppsala, Sweden,July.
Association for Computational Linguistics.Zdene?k Z?abokrtsky?, Martin Popel, and David Marec?ek.2010.
Maximum entropy translation model independency-based mt framework.
In Proceedingsof the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, pages 182?187, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Billy Wong and Chunyu Kit.
2010.
The parameter-optimized atec metric for mt evaluation.
In Pro-ceedings of the Joint Fifth Workshop on StatisticalMachine Translation and MetricsMATR, pages 335?339, Uppsala, Sweden, July.
Association for Com-putational Linguistics.Francisco Zamora-Martinez and Germa?n Sanchis-Trilles.
2010.
Uch-upv english?spanish system forwmt10.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Met-ricsMATR, pages 188?192, Uppsala, Sweden, July.Association for Computational Linguistics.Daniel Zeman.
2010.
Hierarchical phrase-based mtat the charles university for the wmt 2010 sharedtask.
In Proceedings of the Joint Fifth Workshop onStatistical Machine Translation and MetricsMATR,pages 193?196, Uppsala, Sweden, July.
Associationfor Computational Linguistics.41A Pairwise system comparisons by human judgesTables 13?20 show pairwise comparisons between systems for each language pair.
The numbers in eachof the tables?
cells indicate the percentage of times that the system in that column was judged to be betterthan the system in that row.
Bolding indicates the winner of the two systems.
The difference between100 and the sum of the complimentary cells is the percent of time that the two systems were judged tobe equal.Because there were so many systems and data conditions the significance of each pairwise compar-ison needs to be quantified.
We applied the Sign Test to measure which comparisons indicate genuinedifferences (rather than differences that are attributable to chance).
In the following tables ?
indicates sta-tistical significance at p ?
0.10, ?
indicates statistical significance at p ?
0.05, and ?
indicates statisticalsignificance at p ?
0.01, according to the Sign Test.B Automatic scoresThe tables on pages 33?32 give the automatic scores for each of the systems.C Pairwise system comparisons for combined expert and non-expert dataTables 21?20 show pairwise comparisons between systems for the into English direction when non-expert judgments have been added.The number of pairwise comparisons at the ?
level of significance increases from 48 to 50, and thenumber at the ?
level of significants increases from 79 to 80 (basically same number).
However, the?
level of significance went up considerably, from 280 to 369.
That?s a 31% increase.
75 of ?
arecomparisons involving the reference, then the non-reference ?
count went up from 205 to 294, a 43%increase.REFCAMBRIDGECMU-STATXFERCU-ZEMANDFKIGENEVAHUICONGJHULIGLIMSILIUMNRCONLINEAONLINEBRALIRWTHUEDINBBN-COMBOCMU-HEAFIELD-COMBOCMU-HYPOSEL-COMBODCU-COMBOJHU-COMBOLIUM-COMBORWTH-COMBOUPV-COMBOREF ?
.00?
.00?
.00?
.00?
.00?
.04?
.03?
.00?
.00?
.00?
.00?
.04?
.00?
.04?
.00?
.00?
.00?
.00?
.05?
.06?
.03?
.09?
.04?
.04?CAMBRIDGE .79?
?
.36 .16?
.12?
.23?
.27 .43 .26?
.38 .24 .3 .28 .51 .34 .23 .37 .24 .32 .46 .24 .29 .45 .59?
.44CMU-STATXFER .84?
.58 ?
.16?
.48 .14?
.19 .39 .33 .54 .54?
.50?
.36 .50 .70?
.55?
.50 .46 .58?
.67?
.50 .56?
.48 .58?
.52?CU-ZEMAN 1.00?
.77?
.72?
?
.76?
.37 .73?
.74?
.79?
.77?
.77?
.81?
.75?
.94?
.86?
.77?
.89?
.67 .77?
.79?
.81?
.81?
.77?
.96?
.86?DFKI 1.00?
.72?
.45 .12?
?
.32 .48 .50 .52 .53 .56 .65 .53 .62 .55 .43 .61?
.50 .68?
.73?
.70?
.60 .59?
.72?
.71?GENEVA 1.00?
.69?
.76?
.48 .56 ?
.47 .71?
.79?
.72?
.79?
.71?
.68?
.76?
.83?
.57 .86?
.72?
.71?
.69?
.76?
.65?
.88?
.96?
.70HUICONG .86?
.54 .29 .12?
.26 .37 ?
.48 .31 .43 .63?
.62?
.53 .55 .53?
.44 .50 .55 .52 .68?
.52?
.51 .52?
.57 .53JHU .83?
.39 .42 .13?
.33 .19?
.3 ?
.3 .36 .56?
.56?
.47 .52 .46 .29 .36 .42 .42 .59?
.50 .31 .43 .29 .37LIG .97?
.63?
.36 .15?
.37 .18?
.40 .60 ?
.62?
.57?
.39 .35 .54?
.46 .33 .34 .38 .54?
.48?
.42 .44 .50 .61?
.56LIMSI .96?
.41 .23 .19?
.31 .17?
.32 .50 .28?
?
.35 .42 .21 .62?
.25 .21 .33 .22 .42 .35 .43 .32 .26 .35 .41LIUM .83?
.33 .21?
.13?
.41 .05?
.13?
.15?
.09?
.3 ?
.39 .19 .36 .43 .26 .23?
.28 .29 .45 .28 .26 .28 .33 .28NRC .96?
.3 .10?
.10?
.32 .24?
.15?
.22?
.22 .33 .43 ?
.26 .58 .26 .24 .3 .50 .36 .45 .47?
.23 .38 .36?
.35ONLINEA .96?
.55 .57 .14?
.42 .16?
.42 .4 .39 .53 .52 .47 ?
.52?
.46 .36 .64 .57 .59 .50 .59 .42 .46 .43 .48ONLINEB .87?
.37 .33 .03?
.29 .12?
.31 .26 .16?
.12?
.39 .35 .20?
?
.33 .38 .17?
.36 .29 .21 .33 .3 .3 .32 .21?RALI .89?
.45 .15?
.06?
.35 .04?
.12?
.42 .35 .46 .32 .42 .39 .52 ?
.32 .31 .26 .43 .41 .27 .43 .40 .63?
.26RWTH .91?
.46 .21?
.05?
.51 .36 .44 .46 .53 .39 .48 .48 .39 .48 .48 ?
.39 .38 .39 .52 .46 .53?
.52 .50?
.25UEDIN .96?
.40 .33 .03?
.28?
.03?
.28 .29 .49 .38 .61?
.3 .32 .50?
.34 .24 ?
.42 .33 .43 .48 .18?
.13 .27 .38BBN-C .90?
.48 .46 .29 .39 .22?
.27 .27 .46 .43 .28 .35 .33 .39 .29 .34 .26 ?
.28 .44?
.33 .26 .62?
.36 .28CMU-HEA-C .89?
.50 .23?
.14?
.30?
.21?
.26 .25 .17?
.33 .43 .16 .36 .43 .26 .29 .24 .24 ?
.48 .27 .13 .25 .30 .15CMU-HYP-C .81?
.17 .19?
.11?
.19?
.19?
.14?
.14?
.19?
.40 .23 .18 .29 .46 .35 .29 .21 .15?
.17 ?
.26 .18 .07?
.32 .21DCU-C .88?
.27 .25 .11?
.22?
.24?
.20?
.28 .21 .35 .50 .10?
.31 .44 .27 .29 .22 .21 .2 .30 ?
.12?
.26 .26 .08JHU-C .86?
.48 .16?
.16?
.33 .21?
.35 .41 .32 .44 .39 .35 .39 .37 .26 .19?
.50?
.23 .32 .43 .40?
?
.36 .27 .39LIUM-C .87?
.41 .36 .13?
.31?
.08?
.21?
.48 .31 .47 .44 .24 .39 .52 .28 .28 .33 .27?
.25 .67?
.26 .44 ?
.54?
.48RWTH-C .88?
.18?
.13?
.04?
.22?
.04?
.14 .24 .25?
.3 .33 .05?
.43 .50 .30?
.13?
.23 .14 .18 .21 .19 .23 .11?
?
.24UPV-C .92?
.25 .12?
.10?
.16?
.3 .25 .34 .29 .31 .34 .29 .39 .65?
.39 .36 .3 .45 .27 .36 .23 .16 .24 .28 ?> others .90 .44 .31 .13 .33 .18 .29 .37 .34 .42 .44 .38 .37 .51 .41 .31 .38 .35 .38 .48 .39 .36 .40 .46 .37>= others .98 .66 .51 .21 .42 .27 .51 .59 .53 .65 .71 .66 .52 .71 .65 .55 .65 .64 .70 .77 .72 .65 .64 .77 .68Table 13: Sentence-level ranking for the WMT10 French-English News Task42REFCAMBRIDGECU-ZEMANDFKIEUGENEVAJHUKOCLIMSILIUMNRCONLINEAONLINEBRALIRWTHUEDINCMU-HEAFIELD-COMBOKOC-COMBORWTH-COMBOUPV-COMBOREF ?
.08?
.02?
.00?
.04?
.08?
.13?
.06?
.09?
.09?
.07?
.16?
.11?
.12?
.12?
.12?
.05?
.07?
.08?
.09?CAMBRIDGE .82?
?
.16?
.24?
.15?
.07?
.35 .10?
.42 .36 .43 .27 .67?
.46 .39 .44 .40 .46 .48?
.40CU-ZEMAN .98?
.82?
?
.47 .54?
.62?
.71?
.41 .79?
.82?
.70?
.67?
.85?
.90?
.75?
.72?
.92?
.82?
.88?
.82?DFKI .95?
.66?
.31 ?
.46 .25?
.78?
.36 .59 .62?
.75?
.65?
.45 .56?
.75?
.69?
.71?
.63?
.57 .65?EU .96?
.78?
.30?
.41 ?
.55 .68?
.16?
.76?
.72?
.82?
.67?
.63?
.86?
.78?
.78?
.76?
.76?
.75?
.71?GENEVA .86?
.81?
.23?
.55?
.34 ?
.65?
.25?
.65?
.70?
.69?
.66?
.77?
.71?
.70?
.89?
.75?
.63?
.84?
.75?JHU .77?
.42 .15?
.22?
.22?
.22?
?
.06?
.58?
.47 .52?
.49 .70?
.61?
.53 .64?
.53?
.65?
.68?
.50KOC .85?
.67?
.4 .58 .55?
.69?
.82?
?
.76?
.85?
.81?
.72?
.86?
.82?
.86?
.85?
.77?
.77?
.74?
.79?LIMSI .84?
.23 .08?
.29 .09?
.30?
.21?
.08?
?
.33 .37 .17?
.51 .40 .29 .45 .49 .40 .61?
.28LIUM .85?
.39 .07?
.32?
.11?
.21?
.44 .07?
.46 ?
.44 .4 .32 .44 .37 .64?
.35 .40 .35 .42NRC .91?
.43 .15?
.20?
.11?
.25?
.21?
.09?
.31 .45 ?
.32 .48 .44 .49 .61?
.52?
.30 .58?
.40ONLINEA .80?
.51 .21?
.33?
.23?
.15?
.41 .14?
.60?
.42 .54 ?
.52?
.56?
.36 .67?
.61?
.45 .50 .44ONLINEB .87?
.23?
.08?
.43 .23?
.11?
.12?
.08?
.27 .36 .43 .25?
?
.38 .31 .33 .52 .33?
.46 .29RALI .83?
.38 .05?
.27?
.11?
.15?
.22?
.10?
.36 .44 .49 .31?
.50 ?
.38 .44 .42 .37 .38 .34RWTH .76?
.33 .11?
.12?
.15?
.17?
.34 .05?
.34 .44 .29 .42 .49 .40 ?
.56 .48 .44 .53?
.50UEDIN .84?
.29 .20?
.17?
.12?
.09?
.19?
.07?
.33 .23?
.24?
.24?
.56 .31 .3 ?
.36?
.27 .51 .18?CMU-HEAFIELD-COMBO .90?
.23 .04?
.23?
.18?
.12?
.22?
.11?
.32 .41 .20?
.23?
.28 .31 .31 .11?
?
.29 .24 .3KOC-COMBO .91?
.26 .08?
.31?
.17?
.28?
.20?
.07?
.23 .26 .19 .36 .57?
.37 .32 .32 .42 ?
.38 .34RWTH-COMBO .85?
.21?
.02?
.36 .16?
.07?
.12?
.07?
.16?
.3 .30?
.4 .34 .32 .06?
.26 .35 .16 ?
.21?UPV-COMBO .87?
.38 .08?
.30?
.19?
.19?
.37 .11?
.39 .24 .33 .37 .44 .27 .34 .46?
.35 .28 .50?
?> others .87 .43 .15 .30 .22 .25 .38 .13 .44 .45 .46 .41 .53 .49 .44 .52 .53 .45 .53 .45>= others .92 .63 .26 .40 .32 .35 .53 .26 .66 .63 .62 .55 .68 .66 .63 .70 .74 .68 .75 .66Table 14: Sentence-level ranking for the WMT10 English-French News Task43REFAALTOCMUCU-ZEMANDFKIFBKHUICONGJHUKITKOCLIMSILIUONLINEAONLINEBRWTHUEDINUMDUPPSALAUU-MSBBN-COMBOCMU-HEAFIELD-COMBOCMU-HYPOSEL-COMBOJHU-COMBOKOC-COMBORWTH-COMBOUPV-COMBOREF ?
.00?
.03?
.00?
.06?
.03?
.00?
.00?
.05?
.00?
.00?
.03?
.06?
.09?
.06?
.00?
.09?
.03?
.03?
.14?
.03?
.06?
.03?
.03?
.06?
.00?AALTO 1.00?
?
.50 .31 .60 .69?
.39 .41 .71?
.31 .45 .60?
.59?
.65?
.66?
.64?
.81?
.45 .41 .69?
.72?
.75?
.55 .55?
.76?
.57?CMU .93?
.31 ?
.29 .49 .57?
.38 .50 .74?
.13?
.44 .59?
.57?
.59?
.60?
.67?
.59?
.41 .50 .68?
.67?
.46 .64?
.55?
.67?
.54?CU-ZEMAN 1.00?
.44 .56 ?
.58 .64?
.17 .44 .75?
.38 .50 .54?
.76?
.79?
.73?
.72?
.72?
.50?
.73?
.78?
.80?
.68?
.72?
.62?
.68?
.73?DFKI .92?
.25 .32 .27 ?
.53 .36 .46 .65?
.07?
.50 .47 .47 .69?
.56 .35 .55 .58 .47 .67?
.61?
.52 .47 .38 .67?
.51FBK .97?
.20?
.16?
.14?
.38 ?
.11?
.31 .45 .10?
.22?
.36 .50 .57?
.37 .43 .40 .12?
.17?
.48?
.43 .35 .38 .22 .38 .39HUICONG .93?
.35 .28 .46 .43 .75?
?
.52 .69?
.16?
.39 .42 .64?
.79?
.31 .51?
.78?
.27 .41 .49 .74?
.68?
.60?
.37 .68?
.56?JHU .86?
.34 .29 .16 .43 .31 .26 ?
.61?
.15?
.35 .36 .45 .69?
.52?
.56?
.64?
.27 .36 .70?
.53 .47 .66?
.52 .68?
.44KIT .89?
.21?
.10?
.14?
.29?
.33 .19?
.14?
?
.03?
.27 .21?
.36 .46 .17?
.29 .24 .25?
.25?
.48 .23?
.31 .38 .2 .36 .12?KOC .96?
.58 .77?
.48 .70?
.77?
.58?
.71?
.97?
?
.77?
.90?
.72?
.82?
.76?
.84?
.81?
.84?
.66?
.83?
.87?
.79?
.77?
.75?
.93?
.71?LIMSI 1.00?
.23 .28 .35 .35 .53?
.33 .45 .41 .19?
?
.49 .48 .63?
.49 .63?
.52 .36 .29 .73?
.53?
.45 .59?
.29 .56?
.59?LIU .88?
.12?
.15?
.16?
.39 .21 .46 .36 .61?
.00?
.27 ?
.44 .63?
.49 .45 .53 .27?
.33 .67?
.55?
.46 .44 .32 .37 .55ONLINEA .92?
.15?
.23?
.24?
.42 .34 .21?
.35 .50 .10?
.32 .36 ?
.41 .4 .44 .37 .32 .34 .36 .4 .47 .3 .26 .48 .41ONLINEB .68?
.18?
.29?
.17?
.26?
.24?
.18?
.23?
.33 .18?
.23?
.27?
.34 ?
.3 .15?
.29 .24?
.15?
.44 .28 .33?
.20?
.21?
.38 .3RWTH .88?
.17?
.20?
.20?
.37 .49 .41 .23?
.61?
.16?
.4 .3 .43 .56 ?
.39 .50 .26 .49 .37 .29 .34 .41 .26 .44 .2UEDIN .89?
.14?
.22?
.13?
.62 .34 .18?
.22?
.39 .03?
.17?
.3 .44 .67?
.42 ?
.39 .15?
.14?
.52?
.40 .36 .43 .26 .41 .38UMD .91?
.07?
.14?
.08?
.36 .34 .11?
.25?
.48 .16?
.24 .34 .52 .56 .41 .45 ?
.16?
.21?
.41 .28 .29 .43 .29 .25 .23UPPSALA .97?
.32 .34 .17?
.36 .54?
.23 .37 .70?
.00?
.41 .62?
.56 .68?
.57 .64?
.59?
?
.2 .63?
.69?
.51?
.60?
.33 .69?
.63?UU-MS .82?
.22 .43 .14?
.45 .51?
.19 .21 .68?
.14?
.39 .52 .60 .64?
.44 .53?
.61?
.28 ?
.36 .58?
.52?
.53?
.30 .64?
.44BBN-C .86?
.25?
.10?
.07?
.27?
.17?
.23 .18?
.35 .07?
.15?
.12?
.32 .41 .3 .19?
.22 .15?
.27 ?
.39 .06?
.23?
.11?
.21 .18?CMU-HEA-C .87?
.14?
.15?
.08?
.29?
.33 .04?
.26 .53?
.00?
.20?
.24?
.44 .31 .46 .23 .53 .15?
.13?
.27 ?
.40 .2 .14?
.22 .28CMU-HYP-C .94?
.25?
.24 .14?
.44 .3 .15?
.26 .47 .08?
.45 .31 .42 .67?
.24 .36 .46 .14?
.21?
.50?
.32 ?
.43 .28 .51?
.42JHU-C .97?
.34 .11?
.20?
.29 .34 .29?
.03?
.38 .12?
.07?
.29 .55 .67?
.34 .32 .23 .24?
.24?
.48?
.40 .32 ?
.27 .37 .31KOC-C .88?
.00?
.23?
.21?
.53 .44 .29 .22 .43 .08?
.36 .50 .53 .63?
.39 .37 .39 .28 .19 .64?
.61?
.38 .55 ?
.48?
.46RWTH-C .82?
.09?
.06?
.29?
.25?
.25 .18?
.18?
.24 .03?
.19?
.26 .36 .54 .25 .26 .33 .06?
.14?
.29 .22 .23?
.3 .17?
?
.13?UPV-C .97?
.17?
.21?
.17?
.36 .36 .23?
.19 .67?
.20?
.18?
.29 .41 .40 .40 .38 .48 .17?
.31 .50?
.43 .27 .27 .27 .65?
?> others .91 .23 .25 .20 .39 .42 .24 .30 .53 .11 .31 .38 .47 .59 .42 .43 .48 .27 .30 .53 .49 .42 .44 .31 .51 .41>= others .96 .42 .46 .36 .50 .66 .47 .53 .72 .23 .52 .59 .63 .73 .62 .66 .68 .51 .55 .77 .73 .65 .67 .59 .75 .64Table 15: Sentence-level ranking for the WMT10 German-English News TaskREFCU-ZEMANDFKIFBKJHUKITKOCLIMSILIUONLINEAONLINEBRWTHSFUUEDINUPPSALACMU-HEAFIELD-COMBOKOC-COMBORWTH-COMBOUPV-COMBOREF ?
.03?
.06?
.01?
.02?
.05?
.00?
.00?
.01?
.04?
.03?
.01?
.01?
.01?
.02?
.01?
.01?
.05?
.06?CU-ZEMAN .97?
?
.85?
.67?
.62?
.78?
.58?
.70?
.64?
.80?
.85?
.64?
.52 .80?
.61?
.79?
.69?
.76?
.73?DFKI .89?
.14?
?
.36?
.24?
.38 .30?
.27?
.36?
.36?
.55 .35?
.21?
.41 .39 .46 .38?
.47 .37?FBK .97?
.30?
.59?
?
.35?
.42 .12?
.36 .48 .48 .64?
.39 .29?
.46 .30?
.44 .46 .48 .38JHU .98?
.27?
.72?
.57?
?
.59?
.30?
.51 .53 .56?
.65?
.43 .39 .66?
.45 .56 .61?
.52 .47KIT .92?
.18?
.55 .42 .29?
?
.23?
.32 .32?
.43 .53?
.41 .27?
.43 .23?
.41 .41 .42 .37KOC 1.00?
.37?
.64?
.82?
.62?
.70?
?
.74?
.74?
.74?
.82?
.63?
.48 .62?
.65?
.73?
.67?
.81?
.71?LIMSI .95?
.27?
.68?
.39 .45 .49 .17?
?
.49 .74?
.70?
.51 .28?
.58?
.32 .51 .53?
.52?
.31LIU .95?
.32?
.59?
.4 .36 .58?
.21?
.37 ?
.39 .74?
.33?
.23?
.55?
.36?
.49 .42 .46 .38ONLINEA .95?
.16?
.55?
.4 .36?
.45 .21?
.23?
.50 ?
.56?
.38 .23?
.41 .23?
.48 .4 .50 .33?ONLINEB .92?
.12?
.42 .26?
.27?
.33?
.14?
.23?
.21?
.32?
?
.24?
.14?
.39 .19?
.29?
.27?
.36 .32?RWTH .98?
.33?
.61?
.51 .47 .46 .30?
.33 .52?
.55 .71?
?
.33?
.57?
.45 .40 .51?
.47 .46SFU .98?
.42 .77?
.66?
.51 .69?
.48 .68?
.69?
.72?
.77?
.56?
?
.82?
.53 .65?
.69?
.73?
.62?UEDIN .94?
.17?
.51 .4 .31?
.49 .34?
.25?
.30?
.52 .52 .36?
.10?
?
.33?
.31 .42 .38 .22?UPPSALA .97?
.36?
.55 .51?
.47 .70?
.25?
.46 .57?
.67?
.71?
.41 .38 .54?
?
.53?
.42 .58?
.40CMU-HEAFIELD-COMBO .96?
.17?
.49 .36 .36 .37 .21?
.35 .49 .42 .64?
.38 .28?
.48 .28?
?
.35 .46 .35KOC-COMBO .99?
.27?
.56?
.32 .27?
.32 .23?
.32?
.41 .55 .64?
.30?
.21?
.37 .36 .41 ?
.34 .36RWTH-COMBO .92?
.17?
.50 .34 .35 .41 .09?
.25?
.38 .4 .54 .38 .20?
.42 .19?
.28 .35 ?
.16?UPV-COMBO .93?
.23?
.58?
.38 .36 .51 .23?
.50 .49 .57?
.60?
.42 .28?
.51?
.3 .38 .46 .48?
?> others .95 .24 .57 .44 .37 .48 .24 .39 .45 .51 .63 .40 .27 .51 .34 .45 .44 .49 .39>= others .98 .28 .62 .56 .46 .60 .30 .51 .55 .59 .70 .51 .34 .62 .47 .59 .59 .65 .55Table 16: Sentence-level ranking for the WMT10 English-German News Task44REFCAMBRIDGECOLUMBIACU-ZEMANDFKIHUICONGJHUONLINEAONLINEBUEDINUPCBBN-COMBOCMU-HEAFIELD-COMBOJHU-COMBOUPV-COMBOREF ?
.00?
.01?
.01?
.01?
.00?
.00?
.00?
.00?
.00?
.01?
.02?
.05?
.01?
.04?CAMBRIDGE .95?
?
.23?
.14?
.34?
.31?
.41 .34 .62?
.45?
.35 .40?
.42 .22?
.44COLUMBIA .97?
.58?
?
.25?
.52 .45 .59?
.53?
.65?
.60?
.47 .56?
.55?
.45 .58?CU-ZEMAN .96?
.71?
.59?
?
.60?
.68?
.79?
.66?
.75?
.80?
.66?
.79?
.78?
.69?
.75?DFKI .97?
.51?
.37 .23?
?
.43 .59?
.52?
.66?
.62?
.48 .53?
.55?
.55?
.64?HUICONG .95?
.50?
.34 .21?
.41 ?
.45 .50 .66?
.61?
.39 .50?
.59?
.40 .52?JHU .98?
.39 .22?
.12?
.30?
.33 ?
.37 .56?
.51?
.34 .39 .34?
.22?
.34ONLINEA .96?
.46 .37?
.23?
.32?
.38 .44 ?
.59?
.53?
.4 .50 .36 .30?
.54?ONLINEB .88?
.25?
.21?
.16?
.23?
.21?
.27?
.23?
?
.35 .24?
.28?
.34?
.22?
.36UEDIN .96?
.31?
.28?
.10?
.25?
.19?
.25?
.31?
.48 ?
.23?
.27?
.31 .23?
.2UPC .94?
.47 .4 .20?
.41 .33 .43 .46 .66?
.56?
?
.50?
.52?
.48?
.49?BBN-COMBO .95?
.26?
.31?
.09?
.32?
.34?
.33 .37 .54?
.44?
.33?
?
.35 .24?
.34CMU-HEAFIELD-COMBO .91?
.39 .21?
.08?
.34?
.22?
.16?
.42 .57?
.45 .31?
.31 ?
.14?
.27JHU-COMBO .95?
.40?
.32 .15?
.36?
.31 .44?
.50?
.66?
.50?
.32?
.47?
.43?
?
.43?UPV-COMBO .92?
.35 .28?
.16?
.27?
.23?
.38 .28?
.47 .30 .28?
.26 .35 .25?
?> others .95 .41 .30 .15 .33 .32 .39 .39 .56 .48 .34 .41 .43 .32 .43>= others .99 .61 .45 .27 .45 .50 .61 .54 .70 .69 .51 .62 .66 .55 .66Table 17: Sentence-level ranking for the WMT10 Spanish-English News TaskREFCAMBRIDGECU-ZEMANDCUDFKIJHUKOCONLINEAONLINEBSFUUEDINUPVUCH-UPVCMU-HEAFIELD-COMBOKOC-COMBORWTH-COMBOUPV-COMBOREF ?
.00?
.02?
.07?
.15?
.07?
.02?
.11?
.14?
.07?
.07?
.03?
.06?
.09?
.06?
.03?
.07?CAMBRIDGE .91?
?
.28?
.45 .38 .45 .11?
.52 .61?
.21?
.52 .47 .35 .54 .51 .39 .49CU-ZEMAN .95?
.70?
?
.79?
.75?
.85?
.49 .83?
.82?
.74?
.87?
.67?
.85?
.81?
.80?
.70?
.74?DCU .93?
.32 .21?
?
.45 .32 .09?
.70?
.59 .24?
.48 .38 .29 .32 .36 .24 .14?DFKI .80?
.41 .15?
.45 ?
.38 .12?
.64?
.57 .4 .57 .31 .41 .59 .50 .48 .47JHU .90?
.37 .10?
.52 .56 ?
.17?
.67?
.67?
.26?
.34 .3 .49 .54 .53?
.47 .35KOC .98?
.87?
.47 .88?
.73?
.76?
?
.76?
.87?
.67?
.83?
.86?
.90?
.87?
.90?
.86?
.86?ONLINEA .82?
.42 .08?
.30?
.18?
.24?
.20?
?
.49 .36 .25?
.17?
.25?
.45 .30?
.29 .18?ONLINEB .76?
.26?
.10?
.32 .37 .22?
.10?
.34 ?
.21?
.28 .24?
.32 .33 .22?
.19?
.27?SFU .91?
.54?
.19?
.67?
.51 .63?
.27?
.64 .72?
?
.74?
.57?
.68?
.77?
.71?
.64?
.46UEDIN .91?
.3 .08?
.4 .38 .34 .14?
.71?
.49 .09?
?
.34 .4 .58 .33 .3 .31UPV .94?
.34 .07?
.41 .53 .54 .07?
.73?
.61?
.27?
.45 ?
.37 .51 .44 .38 .48?UCH-UPV .90?
.55 .07?
.58 .51 .41 .08?
.69?
.52 .24?
.51 .46 ?
.47 .41 .49 .49CMU-HEAFIELD-COMBO .83?
.29 .13?
.37 .38 .35 .07?
.48 .54 .08?
.29 .26 .28 ?
.17?
.21?
.21KOC-COMBO .88?
.27 .15?
.40 .42 .24?
.03?
.62?
.60?
.15?
.41 .27 .34 .53?
?
.3 .40RWTH-COMBO .92?
.36 .21?
.52 .33 .31 .10?
.55 .65?
.14?
.37 .22 .41 .52?
.48 ?
.31UPV-COMBO .91?
.32 .13?
.69?
.4 .32 .09?
.76?
.52?
.36 .38 .19?
.31 .45 .35 .28 ?> others .89 .39 .15 .48 .44 .41 .14 .61 .58 .29 .46 .36 .42 .51 .44 .39 .40>= others .93 .54 .23 .61 .55 .55 .19 .69 .71 .40 .61 .55 .54 .68 .62 .59 .60Table 18: Sentence-level ranking for the WMT10 English-Spanish News Task45REFAALTOCMUCU-BOJARCU-ZEMANONLINEAONLINEBUEDINBBN-COMBOCMU-HEAFIELD-COMBOJHU-COMBORWTH-COMBOUPV-COMBOREF ?
.04?
.02?
.03?
.00?
.02?
.00?
.03?
.03?
.04?
.01?
.04?
.02?AALTO .88?
?
.49 .51 .22?
.38 .64?
.55?
.57?
.71?
.64?
.65?
.59?CMU .97?
.35 ?
.4 .14?
.18?
.59?
.49?
.45?
.57?
.50?
.34 .43CU-BOJAR .90?
.33 .43 ?
.12?
.20?
.64?
.45 .45 .54?
.42 .42 .41CU-ZEMAN .99?
.60?
.77?
.75?
?
.56?
.81?
.78?
.88?
.79?
.84?
.84?
.76?ONLINEA .92?
.46 .68?
.59?
.28?
?
.65?
.54?
.72?
.75?
.58?
.57?
.66?ONLINEB .97?
.27?
.28?
.21?
.10?
.17?
?
.25?
.32 .22 .21?
.32 .28UEDIN .95?
.28?
.26?
.38 .07?
.22?
.49?
?
.60?
.52?
.33 .31 .32BBN-COMBO .92?
.31?
.20?
.39 .08?
.15?
.41 .16?
?
.27 .25 .3 .26CMU-HEAFIELD-COMBO .90?
.13?
.23?
.25?
.07?
.15?
.31 .23?
.34 ?
.18?
.35 .28JHU-COMBO .93?
.20?
.19?
.33 .08?
.25?
.48?
.39 .38 .52?
?
.37 .42RWTH-COMBO .92?
.18?
.37 .38 .13?
.25?
.34 .28 .43 .40 .26 ?
.25UPV-COMBO .96?
.25?
.36 .41 .11?
.27?
.45 .35 .37 .44 .31 .34 ?> others .93 .28 .36 .38 .11 .23 .49 .38 .47 .48 .38 .40 .40>= others .98 .43 .55 .55 .22 .37 .70 .61 .70 .71 .62 .65 .63Table 19: Sentence-level ranking for the WMT10 Czech-English News TaskREFCU-BOJARCU-TECTOCU-ZEMANDCUEUROTRANSKOCONLINEAONLINEBPC-TRANSPOTSDAMSFUUEDINCMU-HEAFIELD-COMBODCU-COMBOKOC-COMBORWTH-COMBOUPV-COMBOREF ?
.04?
.04?
.03?
.01?
.05?
.03?
.08?
.04?
.04?
.03?
.02?
.02?
.04?
.08?
.04?
.07?
.04?CU-BOJAR .87?
?
.46 .27?
.12?
.28?
.16?
.17?
.44 .4 .11?
.27?
.41 .28 .52?
.28 .42 .43CU-TECTO .88?
.36 ?
.30?
.23?
.38 .17?
.28?
.56?
.44 .29?
.27?
.36 .45 .51?
.4 .58?
.35CU-ZEMAN .91?
.58?
.51?
?
.38 .49 .19?
.39 .62?
.63?
.36 .41 .48 .51?
.58?
.48?
.54?
.55?DCU .98?
.73?
.52?
.43 ?
.59?
.22?
.47 .74?
.63?
.47?
.53?
.56?
.77?
.77?
.62?
.76?
.71?EUROTRANS .88?
.61?
.47 .33 .30?
?
.10?
.33 .51 .54?
.25?
.27?
.49 .57?
.59?
.49 .57?
.60?KOC .93?
.69?
.67?
.54?
.49?
.77?
?
.54?
.71?
.70?
.51?
.55?
.64?
.72?
.78?
.65?
.76?
.78?ONLINEA .91?
.62?
.57?
.51 .39 .44 .24?
?
.66?
.62?
.39 .43 .55?
.60?
.61?
.59?
.73?
.61?ONLINEB .91?
.31 .29?
.27?
.13?
.33 .14?
.19?
?
.44 .22?
.09?
.39 .19 .34 .24?
.22?
.39PC-TRANS .88?
.45 .43 .24?
.26?
.29?
.21?
.24?
.49 ?
.22?
.27?
.37 .43 .55?
.33?
.49 .41POTSDAM .88?
.60?
.51?
.40 .27?
.59?
.25?
.47 .63?
.64?
?
.45 .52?
.56?
.69?
.61?
.70?
.68?SFU .95?
.52?
.56?
.4 .30?
.61?
.27?
.39 .65?
.64?
.29 ?
.55?
.54?
.76?
.53?
.70?
.60?UEDIN .94?
.39 .44 .33 .23?
.32 .20?
.26?
.32 .49 .25?
.26?
?
.43 .57?
.18 .46?
.42CMU-HEAFIELD-COMBO .91?
.42 .39 .23?
.10?
.27?
.14?
.19?
.23 .35 .24?
.19?
.28 ?
.48?
.28 .34 .29DCU-COMBO .84?
.23?
.27?
.23?
.03?
.31?
.10?
.21?
.42 .31?
.15?
.10?
.16?
.20?
?
.18?
.27?
.22?KOC-COMBO .91?
.37 .49 .25?
.10?
.39 .17?
.32?
.42?
.55?
.17?
.27?
.26 .33 .41?
?
.32 .22RWTH-COMBO .88?
.29 .34?
.28?
.05?
.26?
.10?
.17?
.48?
.43 .16?
.15?
.24?
.33 .46?
.36 ?
.29UPV-COMBO .92?
.37 .52 .22?
.09?
.25?
.10?
.19?
.28 .47 .15?
.25?
.33 .24 .49?
.34 .39 ?> others .91 .45 .44 .32 .20 .39 .16 .29 .49 .49 .25 .28 .40 .43 .54 .39 .50 .45>= others .96 .66 .60 .50 .38 .54 .33 .44 .70 .62 .44 .45 .62 .69 .75 .66 .70 .68Table 20: Sentence-level ranking for the WMT10 English-Czech News Task46!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$( !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$""#)* +,-.
+,/- +,/.
+,-0 +,/+ +,-1 +,-0 +,-.
+,-1 +,/+ +,-/ +,2- +,3/ +,24 +,./ +,/3 +,--''567*8'* +,4+ +,/1 !
"#$ !
"#% !
"#% !
"#% !
"#% !"#!
!
"## !
"#& !
"#' !
"() !
"'& !
"*& !
"%% !
"#$ !
"%)78&69:67*8'* !
"&' +,/1 +,// +,/- +,/- +,/.
+,/.
+,/+ +,/- !
"#& +,/+ !
"() !
"'& +,.
; !
"%% !
"#$ +,-178& +,// +,// +,/.
+,/2 +,/2 +,/3 +,/2 +,-; +,/2 +,/- +,-1 +,24 !
"'& +,.. +,-3 +,/- +,-;7&6'*<"= +,// +,/; +,/.
+,/+ +,/3 +,-0 +,/+ +,-/ +,/3 +,/.
+,-4 +,2/ +,3/ +,.+ +,.4 +,/.
+,-;7&6>?8"5 +,22 +,-0 +,/2 +,-/ +,-; +,-/ +,-/ +,.0 +,-/ +,-; +,-2 +,23 +,3.
+,2+ +,.+ +,/+ +,-3<@&67*8'* +,;2 +,/4 !
"#$ !
"#% !
"#% +,/.
!
"#% +,-1 +,/.
+,// +,-0 +,21 !
"'& +,./ +,-2 +,/.
+,-4*5#A5?B +,.4 +,/- +,/2 +,-0 +,/+ +,-1 +,/+ +,-2 +,-4 +,-0 +,-- +,2- +,3- +,24 +,.- +,-0 +,-2*5#A5?C +,4+ !
"#) !
"#$ !
"#% +,/.
+,/.
+,/.
+,-0 +,/- +,/; +,/+ !
"() !
"'& +,.
; +,-.
+,/.
+,-;=D)@67*8'* +,;/ +,/1 +,// !
"#% !
"#% +,/.
+,/.
+,-0 +,/.
+,/; +,-0 !
"() !
"'& +,./ +,-2 +,// +,-1&?EA5 +,;3 +,/4 +,/- +,/2 +,/.
+,/3 +,/2 +,-1 +,/.
+,// +,-0 +,21 +,3; +,.- +,-2 +,// +,-1&FG67*8'* +,;.
+,/1 +,// !
"#% !
"#% +,/.
+,/.
+,-0 +,/- +,/; +,/+ !
"() !
"'& +,./ +,-.
+,// +,-1!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$""#)* +,-0 +,/2 -,04 !
"%' +,+2 +,+4 +,24 +,-2 +,2/ +,/2 +,/2 +,31 ;,2+''567*8'* +,/- !
"#& /,-; !
"%+ !"!
* +,33 !
"** !
"%+ !"*!
!
"#+ !
"#+ !
"(# &"'!78&69:67*8'* +,/- +,/4 /,-.
!
"%+ !"!
* +,+0 !
"** +,-1 !"*!
+,/4 +,/4 !
"(# ;,0178& +,/+ +,/.
/,3+ +,-/ +,+2 +,+0 +,.3 +,-; +,21 +,/- +,/- +,22 ;,;;7&6'*<"= +,/2 +,// /,2.
+,-/ +,+2 +,+/ +,.+ +,-/ +,24 +,// +,// +,30 ;,2-7&6>?8"5 +,-4 +,-0 -,44 +,.
; +,+2 +,+.
+,2.
+,.4 +,22 +,-0 +,-0 +,3- /,-/<@&67*8'* +,/.
+,// /,2/ +,-; +,+2 +,+.
+,.2 +,-; +,20 +,/; +,/; +,2.
4,++*5#A5?B +,-0 +,/3 -,14 +,-2 +,+2 +,+4 +,24 +,-.
+,2/ +,/2 +,/2 +,31 ;,3.
*5#A5?C !
"## !
"#& /,-3 !
"%+ !"!
* !
"'$ +,.2 +,-4 !"*!
!
"#+ !
"#+ +,2- ;,3.=D)@67*8'* +,/- +,/; /,-+ !
"%+ !"!
* +,+4 +,.2 +,-4 +,20 +,/4 +,/4 +,2- 4,+;&?EA5 +,/.
!
"#& /,-+ !
"%+ !"!
* +,3- +,.2 +,-4 +,20 +,/4 +,/4 +,2.
;,;-&FG67*8'* +,/- !
"#& #"%& !
"%+ !"!
* +,+4 !
"** +,-4 +,20 !
"#+ !
"#+ +,2- ;,00="5HIJK)"5L*=E MNJFO(#?P?=(=?7"## KQR(J"5H MNKSB CNDM6N CH"=%RNMNTJ("EU RNMNTJ(="5HRNMNTJ(@)?= KNVOB K?8VTK,-./012345670888((66((((((R?)=A7%(%&'8A)?E()*(WOKM(R?)=A7%RBMJ(2+3+X(F#&%()D*('"%?#A5?(8?)=A7%(!CSNY("5E(WOKM$,((K7*=?%(L*=(Z"##Z(!?5[=?(\RM3+()?%)%?)$("5E(Z%&'Z(!%&'%?)(*L()@?(@&8"5#]("%%?%%?E(E")"$("=?
(%@*D5, SJ^C- SJ9C-WOKMCSNY(G3.
"RM6W_I RM68W_I C"E`?=(L&## C"E`?=(#A)?
BMN_(2,3I=E*7 YS_@O(#?P?=(CSNYK?8VTK(CSNY I_Y6S:aMNKSB(R!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$( !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$'')*+,-', ./01 ./0.
%&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 %&'% %&'* ./2.+"-'56789 ./00 ./0.
%&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* ./1.
./1; %&'* ./2.+-&*<=*+,-', ./>.
./0.
%&'( %&') %&'* ./23 %&') %&+, %&'- %&.. %&/* ./14 ./1; %&'* ./2.+-&*?
@A*+,-', %&(( ./0.
%&'( ./2: %&'* ./23 %&') %&+, %&'- %&.. %&/* %&-/ %&-) ./2> ./2.+-&*%B"BCD95 ./24 ./2: ./22 ./22 ./21 ./1; ./20 ./2: ./24 ./E; %&/( %&.
( %&-.
./20 ./1:+&*F9-") ./E4 ./2E ./22 ./24 ./2.
./10 ./2E ./21 ./1: ./E1 %&/' %&,.
%&.
( ./23 ./117+&*+,-', ./>E %&+/ %&'( %&') %&'* %&'- %&') ./0E %&'- %&.. %&/* %&-.
%&'% %&'* %&'/7G6 ./1E ./20 ./23 ./2E ./24 ./10 ./23 ./21 ./1; ./E2 ./4> ./3E ./1.
./22 ./1089)9H" ./E> ./22 ./2E ./1; ./1; ./11 ./2.
./24 ./10 ./E3 ./41 ./E> ./3> ./23 ./11?&6+,)8 ./24 ./20 ./22 ./22 ./21 ./2.
./20 ./2: ./24 ./E: ./4> ./3E ./11 ./22 ./1>I?&*+,-', ./02 ./0.
%&'( ./2: %&'* ./23 ./2: ./04 ./23 ./3E %&/* ./1.
./1: %&'* ./2.I?& ./2; ./0.
%&'( ./2> ./20 ./2E ./2: ./0.
./23 ./34 %&/* ./3: ./1> ./2> ./1;#68 ./23 ./2; ./20 ./22 ./22 ./24 ./20 ./2; ./2E ./3.
./4> ./30 ./12 ./20 ./1:#6-%6 ./02 ./2; ./20 ./2> ./20 ./2E ./2> ./0.
./23 ./34 ./4> ./3: ./1> ./2> ./1:#6&-*+,-', ./01 ./0.
%&'( ./2: ./2> ./23 ./2: ./04 ./23 %&.. %&/* ./14 ./1; ./2> ./1;#6&- ./>4 ./0.
%&'( ./2: ./2> ./23 %&') ./04 %&'- ./3E %&/* ./14 ./1; %&'* ./2.
)5+ ./00 ./0.
%&'( ./2> ./20 ./2E ./2: ./04 ./23 ./3E %&/* ./3; ./1: ./2> ./1;,)#6)9J ./2E ./2; ./20 ./22 ./22 ./2.
./20 ./2: ./24 ./3.
./4> ./30 ./11 ./21 ./10,)#6)9K ./>4 ./0.
%&'( ./2: %&'* ./23 ./2: ./04 %&'- ./3E %&/* ./1E ./1; ./2> ./1;5"#6 ./02 ./0.
./20 ./2> ./20 ./2E ./2: ./04 %&'- ./3E %&/* ./1.
./1: ./2> ./1;5LB?
*+,-', %&(( ./0.
%&'( ./2; %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* ./2.5LB?
./22 ./2; ./20 ./2> ./20 ./2E ./2: ./0.
./23 ./34 ./4> ./3: ./10 %&'( ./1;&976) ./02 ./0.
./20 ./2> ./2> ./23 ./2: ./04 %&'- ./3E %&/* ./1.
./1; %&'* ./1;&AH*+,-', ./0: ./0.
%&'( ./2: %&'* %&'- %&') %&+, %&'- %&.. %&/* ./1E %&'% %&'* %&'/!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$'')*+,-', %&'* ./0.
2/:.
./24 %&%.
./.E %&.
( ./2.
%&.- %&+/ ./3.
(&*/+"-'56789 ./2> ./0.
2/:.
./24 %&%.
./.2 %&.
( ./2.
./33 %&+/ ./3.
>/02+-&*<=*+,-', %&'* ./0.
2/:4 %&', %&%.
./.3 %&.
( %&'/ ./31 %&+/ ./3.
>/0;+-&*?
@A*+,-', %&'* ./0.
2/:1 %&', %&%.
./.3 %&.
( %&'/ %&.- %&+/ ./3.
>/0;+-&*%B"BCD95 ./22 ./2> 2/20 ./1: %&%.
./.3 ./31 ./1; ./34 ./2: ./E2 >/40+&*F9-") ./24 ./21 2/3.
./1.
./.E ./.E ./E> ./1.
./E> ./22 ./4; 0/4E7+&*+,-', %&'* %&+/ 2/:> ./2E %&%.
./.3 %&.
( %&'/ %&.- %&+/ %&./ >/>:7G6 ./24 ./20 2/10 ./10 ./.E ./.1 ./34 ./12 ./E; ./22 ./4; 0/.489)9H" ./1; ./22 2/E; ./12 ./.E ./.2 ./E; ./13 ./E> ./21 ./4> 2/0>?&6+,)8 ./22 ./2: 2/0.
./1: %&%.
./.3 ./33 ./1> ./34 ./2> ./E1 0/;;I?&*+,-', ./2> ./0.
2/>> ./24 %&%.
./.4 ./30 ./2.
./33 ./0.
./E; >/04I?& ./2> ./2; 2/>1 ./2.
%&%.
./.
: ./32 ./2.
./3E ./0.
./E: >/11#68 ./22 ./2: 2/0E ./1: %&%.
./.2 ./31 ./1; ./34 ./2: ./E0 >/43#6-%6 ./20 ./2; 2/>E ./24 %&%.
./.2 ./30 ./2.
./33 ./2; ./E> >/34#6&-*+,-', ./2> ./0.
2/>> ./24 %&%.
./.3 %&.
( %&'/ ./33 ./0.
./E; >/>.#6&- %&'* %&+/ '&** %&', %&%.
./.0 %&.
( %&'/ %&.- %&+/ ./E; >/2;)5+ ./2> ./0.
2/>; ./2.
%&%.
./.1 ./30 ./2.
./33 ./0.
./E; >/23,)#6)9J ./22 ./2: 2/21 ./2.
%&%.
./.
: ./33 ./1: ./34 ./2: ./E2 >/E;,)#6)9K %&'* ./0.
2/:.
%&', %&%.
%&// %&.
( %&'/ %&.- %&+/ ./E; >/>35"#6 ./2> ./0.
2/:.
%&', %&%.
./.0 ./30 ./2.
./33 ./0.
./E: >/115LB?
*+,-', %&'* %&+/ 2/:> %&', %&%.
./.4 %&.
( %&'/ %&.- %&+/ ./3.
>/>35LB?
./20 ./2; 2/>3 ./2.
%&%.
./.1 ./30 ./2.
./33 ./0.
./E: >/3;&976) ./2> ./0.
2/:E ./24 %&%.
./.1 ./30 ./2.
./33 %&+/ ./E: >/11&AH*+,-', %&'* %&+/ '&** %&', %&%.
./.3 %&.
( ./2.
%&.- %&+/ ./3.
>/010123456738#9:5;(((**(((((M9B56+%(%&'-6N97(B,(OPQR(M9B56+%MJRS(E.4.T(A#&%(BL,('"%9#6)9(-9B56+%(!KUVW(")7(OPQR$/((Q+,59%(D,5(X"##X(!9)Y59(ZMR4.
(B9%B%9B$(")7(X%&'X(!%&'%9B(,D(B?9(?&-")#@("%%9%%97(7"B"$("59(%?,L)/5")[ MR*O\] MR*-O\] K"7895(D&## K"7895(#6B9 ]\W*U=^ US_K1 US<K1Q9-`aQ Q9-`aQ(KUVWP(#9N95(KUVW P(#9N95(59+"## QbM(S")[ RVQUJ(M RVQUJ QB")D,57MVRVaS("7c MVRVaS(?B95 MVRVaS(5")[ QV`PJJRV\(E/4KUVW(43" OPQRRVSA ]S ]57,+ WU\?
KVLR*V K["5%47!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$( !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$""#)* +,-.
+,/- +,/0 +,/+ +,-1 +,-0 +,/+ +,/.
+,-2 +,./ +,3/ +,./ +,0/ +,-4 +,02''567*8'* !
"## !
"$% +,// !
"$$ !
"$& +,/+ +,/2 +,/1 +,/.
!
"'( +,39 +,02 +,-2 !
"$& !
"&)78&6:;67*8'* +,90 +,/4 +,// !
"$$ !
"$& +,/+ +,/2 +,/1 +,/.
+,0+ +,39 +,0/ +,-/ !
"$& !
"&)78&6<=>67*8'* +,2/ +,/4 +,// +,/- +,/0 +,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-378& +,-2 +,/- +,/3 +,/+ +,/+ +,-- +,/3 +,/0 +,-9 +,.2 +,32 +,0+ +,04 +,/3 +,047&6?
@8"5 +,02 +,/3 +,/0 +,-9 +,-9 +,-.
+,-4 +,/+ +,-/ +,.0 +,3- +,.+ +,0.
+,/+ +,02ABC +,/+ +,/- +,/.
+,-1 +,-4 +,-- +,/3 +,/.
+,-9 +,.- +,32 +,.1 +,0/ +,/+ +,02DE +,22 +,/4 +,/- +,/3 +,/3 +,-1 +,// +,/9 +,/+ +,.9 +,32 +,03 +,-.
+,/.
+,01<&C7*5F +,-9 +,/0 +,/3 +,-1 +,-4 +,-0 +,/+ +,/.
+,-9 +,./ +,3/ +,./ +,02 +,/+ +,02G<&67*8'* +,29 +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3G<& +,/0 +,// +,/.
+,/.
+,/3 +,-/ +,/3 +,/- +,-9 +,.9 +,32 +,.1 +,09 +,-1 +,09EC) +,9.
+,/4 +,/- +,/0 +,/.
+,-1 +,// +,/9 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-+E*767*8'* +,/1 +,/2 +,/- +,/0 +,/.
+,-4 +,/0 +,/2 +,-1 +,.4 +,32 +,03 +,-3 +,/.
+,-+E*7 +,.0 +,-1 +,-9 +,-/ +,-/ +,09 +,-0 +,-/ +,-3 +,.3 +,30 +,.3 +,.4 +,-1 +,0/#C8%C +,/.
+,/9 +,/- +,/.
+,/3 +,-4 +,/- +,/2 +,/+ +,.4 +,32 +,0.
+,-3 +,/.
+,01#C& +,/1 +,/9 +,/- +,/3 +,/+ +,-9 +,/0 +,// +,-1 +,.9 +,32 +,0.
+,-3 +,/.
+,01*5#C5@H +,20 +,/9 +,/- +,/.
+,/3 +,-4 +,/0 +,// +,-1 +,.9 +,32 +,03 +,-+ +,-1 +,02*5#C5@I +,90 !
"$% !
"$* !
"$$ !
"$& !
"$) +,/9 !"*!
!
"$' !
"'( !
"(+ !
"'# !
"&# !
"$& +,-3JK)<67*8'* +,9/ +,/4 +,// +,/- !
"$& +,/+ +,/2 +,/1 +,/.
+,0+ +,39 +,0/ +,-/ !
"$& +,-3JK)< +,2.
+,/4 +,// +,/0 +,/.
+,-1 +,// +,/9 +,/3 +,.1 +,39 +,0- +,-- +,/0 +,-3&@AC5 +,22 +,/4 +,/- +,/0 +,/.
+,/+ +,// +,/9 +,/3 +,.1 +,32 +,00 +,-0 +,/0 +,01&8A +,24 +,/4 +,// +,/0 +,/.
+,/+ +,/2 +,/4 +,/3 +,.1 +,39 +,00 +,-- +,/0 +,-+&>>%"#" +,/3 +,// +,/0 +,/+ +,/+ +,-/ +,/3 +,/- +,-4 +,.2 +,3/ +,.4 +,04 +,/3 +,04&>L67*8'* +,2- +,/4 +,// +,/- +,/0 +,/+ +,// +,/4 +,/3 +,.1 +,32 +,0- +,-- +,/0 +,-3&&68% +,// +,// +,/.
+,/+ +,-1 +,-/ +,/3 +,/0 +,-9 +,.2 +,3/ +,.1 +,09 +,/3 +,04!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$""#)* +,/+ +,/.
/,+3 +,-3 +,+.
+,+3 +,.1 +,-.
+,./ +,/0 +,31 2,04''567*8'* +,// +,/9 /,-2 +,-4 !"!'
+,+- +,0/ +,-9 +,0+ +,/4 !
")* #")+78&6:;67*8'* +,// +,/9 /,-9 +,-4 !"!'
+,+0 +,0- +,-2 +,0+ +,/4 !
")* 9,3178&6<=>67*8'* +,/- +,/9 /,-2 +,-9 !"!'
+,+- +,00 +,-2 +,.1 +,/4 +,./ 9,+178& +,/+ +,/0 /,+4 +,-.
+,+.
+,+0 +,0+ +,-- +,.2 +,/.
+,.3 2,-97&6?
@8"5 +,-4 +,/3 -,14 +,09 +,+.
+,+.
+,.2 +,01 +,.0 +,/3 +,32 /,93ABC +,-1 +,/0 /,3.
+,-/ +,+.
+,+2 +,03 +,-/ +,.4 +,/0 +,39 /,1/DE +,/- +,/9 /,-0 +,-9 !"!'
+,+.
+,00 +,-2 +,.4 +,/4 +,.. 2,29<&C7*5F +,/+ +,/0 /,33 +,-.
+,+.
+,+.
+,0+ +,-0 +,.2 +,/.
+,31 2,30G<&67*8'* +,/- +,/9 /,-0 +,-9 !"!'
+,+.
+,0- +,-2 +,.1 +,/4 +,./ 9,30G<& +,/3 +,/0 /,3.
+,-.
+,+.
+,+/ +,03 +,-- +,.9 +,/- +,.3 2,90EC) +,/- +,/4 /,/3 +,-4 !"!'
+,+/ +,0- +,-9 +,0+ +,/4 +,.- 2,13E*767*8'* +,/.
+,// /,03 +,-/ +,+.
+,+.
+,00 +,-/ +,.4 +,/2 +,.- 2,44E*7 +,-0 +,-4 -,9.
+,02 +,+.
+,+3 +,.9 +,-+ +,.- +,-9 +,32 /,01#C8%C +,/0 +,/2 /,0/ +,-9 +,+.
+,+- +,0.
+,-2 +,.4 +,/9 +,.0 2,90#C& +,/0 +,/2 /,0- +,-2 +,+.
+,+0 +,0.
+,-2 +,.4 +,/2 +,.. 2,2+*5#C5@H +,/0 +,// /,.- +,-2 +,+.
+,3+ +,00 +,-2 +,.1 +,/2 +,.3 2,44*5#C5@I !
"$* !"*!
$"#( !
"$) !"!'
!
"(# !
"'* !
"&% !"')
!
"*( !
")* 9,34JK)<67*8'* +,// +,/4 /,/+ +,-4 !"!'
+,+.
+,0/ +,-9 +,0+ +,/1 !
")* 9,.+JK)< +,/- +,/9 /,-- +,-9 !"!'
+,+0 +,00 +,-2 +,.1 +,/4 +,.- 2,12&@AC5 +,/- +,/4 /,/0 +,-1 !"!'
+,+2 +,0- +,-4 +,0+ +,/4 +,.0 2,4+&8A +,// +,/4 /,/+ +,-4 !"!'
+,+9 +,00 +,-9 +,.1 +,/4 +,.0 2,99&>>%"#" +,/3 +,/- /,.3 +,-0 +,+.
+,+.
+,03 +,-- +,.9 +,/- +,.3 2,/3&>L67*8'* +,/- +,/9 /,-/ +,-4 !"!'
+,+0 +,0- +,-/ +,.1 +,/4 +,./ 9,3/&&68% +,/+ +,/- /,34 +,-- +,+.
+,+.
+,03 +,-/ +,.9 +,/- +,.+ 2,0.,-./0123145678(((((66(((((M@)JC7%(%&'8CN@A()*(OPQR(M@)JC7%MHRS(.+3+T(>#&%()K*('"%@#C5@(8@)JC7%(!IUVW("5A(OPQR$,((Q7*J@%(X*J(Y"##Y(!
@5ZJ@([MR3+()@%)%@)$("5A(Y%&'Y(!%&'%@)(*X()<@(<&8"5#=("%%@%%@A(A")"$("J@(%<*K5,J"5E MR6O\] MR68O\] I"AF@J(X&## I"AF@J(#C)@ ]\W6U;^ US_I- US:I-Q@8`aQ Q@8`aQ(IUVWP(#@N@J(IUVW P(#@N@J(J@7"## QbM(S"5E RVQUH(M RVQUH Q)"5X*JAMVRVaS("Ac MVRVaS(<)@J MVRVaS(J"5E QV`PHHRV\(.,3IUVW(30" OPQRRVS> ]S ]JA*7 WU\< IVKR6V IE"J%!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$( !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$'')*+,-', ./01 ./02 ./34 ./35 ./35 ./36 ./0.
./07 ./33 ./76 ./24 ./67 ./37 ./35 !
"#$+"-'89:;< ./02 ./0.
./34 ./34 ./3= ./37 ./35 ./01 ./33 ./76 ./24 ./62 ./31 ./35 !"#$+-&*>?
*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0.
./07 ./33 ./76 ./24 ./62 ./31 ./35 !
"#$+,#&-'9" ./63 ./34 ./30 ./33 ./36 ./3.
./30 ./35 ./31 ./7.
./2= ./73 ./6= ./3= ./64+&*@<-") ./1= ./33 ./34 ./31 ./31 ./64 ./36 ./30 ./3.
./1= ./23 ./13 ./62 ./33 ./63:A9 ./63 ./34 ./33 ./36 ./37 ./65 ./30 ./34 ./32 ./14 ./24 ./73 ./60 ./30 ./6=B&9+,); ./3.
./3= ./30 ./30 ./33 ./32 ./34 ./02 ./37 ./72 ./2= ./73 ./6= ./3= ./64CB&*+,-', ./33 ./0.
./34 ./34 ./34 ./37 ./35 ./01 ./36 ./77 ./24 ./6.
./32 ./34 ./3.CB& ./02 ./0.
./3= ./3= ./3= ./37 ./35 ./01 ./36 ./77 ./24 ./75 ./3.
./34 ./65,)#9)"D ./36 ./0.
./3= ./3= ./3= ./37 ./34 ./02 ./36 ./71 ./2= ./75 ./64 ./33 ./60,)#9)<E !"%!
!
"&' !
"#( !
"&) !
"&$ !
"#& !
"&) !
"&# !
"#% !
"'& !")!
!
"*% !
"## ./35 ./3.&<:9) ./05 ./02 ./3= ./34 ./34 ./36 ./0.
./07 ./33 ./77 ./24 ./61 ./31 ./35 !
"#$&F+ ./32 ./35 ./30 ./33 ./33 ./32 ./34 ./0.
./37 ./72 ./2= ./7= ./64 ./34 ./65&FG*+,-', ./00 ./02 ./34 ./35 ./34 ./36 ./0.
./07 ./33 ./76 ./24 ./61 ./37 !"&!
!"#$!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$'')*+,-', ./35 ./02 3/51 ./31 !"!'
./.6 ./74 ./3.
./76 ./01 ./71 =/56+"-'89:;< ./34 ./0.
3/43 ./32 !"!'
./.= ./7= ./3.
./77 ./02 ./72 =/==+-&*>?
*+,-', ./35 ./02 3/5.
./31 !"!'
./.6 ./7= ./3.
./76 ./01 ./72 =/4=+,#&-'9" ./33 ./3= 3/37 ./6= ./.1 ./.6 ./76 ./60 ./7.
./3= ./1= =/25+&*@<-") ./36 ./30 3/60 ./61 ./.1 ./.7 ./14 ./62 ./1= ./30 ./11 0/03:A9 ./36 ./34 3/07 ./3.
./.1 ./.4 ./76 ./6= ./72 ./34 ./17 0/0.B&9+,); ./3= ./35 3/=7 ./65 !"!'
./.3 ./76 ./64 ./72 ./35 ./10 =/7=CB&*+,-', ./34 ./0.
3/47 ./32 !"!'
./.2 ./7= ./3.
./77 ./02 ./7.
=/42CB& ./34 ./0.
3/47 ./32 !"!'
./.4 ./70 ./65 ./77 ./02 ./15 =/=1,)#9)"D ./3= ./35 3/=.
./31 !"!'
./22 ./70 ./3.
./71 ./02 ./14 =/==,)#9)<E !
"&$ !
"&' &"!'
!
"#& !"!'
!
")# !"*!
!
"#' !
"'& !
"&* !"''
+"'(&<:9) ./35 ./01 3/5= ./36 !"!'
./21 ./74 ./32 ./76 ./01 ./7.
=/04&F+ ./30 ./0.
3/=3 ./3.
!"!'
./.7 ./73 ./64 ./71 ./0.
./10 =/13&FG*+,-', ./35 ./01 3/5= ./37 !"!'
./.6 ./74 ./3.
./76 ./01 ./72 =/41,-./01234/560127((((**(((((H<I89+%(%&'-9J<:(I,(KLMN(H<I89+%HDNO(1.2.P(F#&%(IQ,('"%<#9)<(-<I89+%(!ERST("):(KLMN$/((M+,8<%(U,8(V"##V(!<)W8<(XHN2.
(I<%I%<I$("):(V%&'V(!%&'%<I(,U(IB<(B&-")#Y("%%<%%<:(:"I"$("8<(%B,Q)/8")Z HN*K[\ HN*-K[\ E":;<8(U&## E":;<8(#9I< \[T*R?]
RO^E6 RO>E6M<-_`M M<-_`M(ERSTL(#<J<8(ERST L(#<J<8(8<+"## MaH(O")Z NSMRD(H NSMRD MI")U,8:HSNS`O(":b HSNS`O(BI<8 HSNS`O(8")Z MS_LDDNS[(1/2ERST(27" KLMNNSOF \O \8:,+ TR[B ESQN*S EZ"8%48!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$( !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$)*&+,-+).*'.
/012 /034 /035 /03/ /056 /076 !
"#$ /076 /052 /038 /055)&+'.9": /011 /036 /035 /052 /051 /071 /057 /071 /056 /03/ /057)&+;<);.
/01/ /036 /037 /051 /057 /075 /05/ /077 /05= /054 /058)&+><*"?
/03/ /033 /03= /051 /057 /078 /072 /075 /05= /051 /072@)&+).*'.
!
"%$ !
"$& !
"$$ !
"$' !
"#( !
")& !
"#$ !
")( !
"$' !
"$* !
"#+@)& /074 /035 /038 /051 /057 /07= /072 /077 /058 /052 /057<&:.
;:"?% /035 /035 /038 /057 /058 /0=2 /076 /078 /074 /054 /05=A.)+).*'.
/011 /036 /035 /052 /051 /076 /057 /076 /056 /038 /053A.)
/077 /03= /052 /055 /058 /0=2 /071 /077 /072 /052 /05=.
?#B?<C /055 /033 /038 /053 /058 /078 /074 /077 /058 /054 /05=.
?#B?<D /06/ /034 /035 /052 /051 /076 /055 /076 /054 /038 /053E)+;:"?% /01= /033 /038 /055 /058 /078 /076 /0=2 /071 /052 /05=E.
;%@"* /055 /033 /038 /053 /05= /078 /074 /075 /05= /052 /05=:F;G+).*'.
/06/ /036 /037 /03/ /056 /074 /055 /076 /052 /038 /053%H& /053 /033 /03= /055 /058 /078 /074 /077 /058 /052 /057&<@B?
/01= /036 /037 /054 /053 /071 /05= /076 /056 /03/ /055&EI+).*'.
/014 /034 /033 /03/ /056 /074 !
"#$ /076 /052 /038 /053!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$)*&+,-+).*'.
!
"#+ /056 504/ !
"$$ +/0/= /0/3 /037 /086 301=)&+'.9": /053 /051 506/ /037 ,!"!'
/0/1 /038 /081 3056)&+;<);.
/057 /053 501= /037 ,!"!'
/0/2 /03/ /087 30=3)&+><*"?
/058 /05= 5071 /03/ ,!"!'
/0/3 /056 /08= 5021@)&+).*'.
/056 !
"#( #"&!
!
"$$ ,!"!'
!
"'# !
"$# !
"'& $"&+@)& /05/ /058 50=3 /056 ,!"!'
/0/3 /051 /087 5023<&:.
;:"?% /05/ /05= 505= /038 ,!"!'
/0/5 /051 /08/ 5055A.)+).*'.
/053 /051 506/ /037 ,!"!'
/0/= /038 /086 3034A.)
/074 /05/ 50=5 /057 ,!"!'
/0/7 /055 /08= 5037.?#B?<C /058 /057 5054 /038 ,!"!'
/0/1 /056 /08= 5013.?#B?<D !
"#+ !
"#( 5046 /035 ,!"!'
/08/ /037 /086 3031E)+;:"?% /058 /057 503= /03/ ,!"!'
/0/3 /056 /08/ 5031E.
;%@"* /05/ /05= 5074 /052 ,!"!'
/0/7 /051 /08= 5065:F;G+).*'.
!
"#+ /056 5048 !
"$$ ,!"!'
/0/5 /038 /084 304=%H& /058 /057 5055 /03/ ,!"!'
/0/5 /056 /088 5014&<@B?
/053 /051 5067 /035 ,!"!'
/088 /03= /081 3057&EI+).*'.
!
"#+ !
"#( 5045 /035 ,!"!'
/0/1 /037 /084 3044:"?AJKL;"?H.
:@ MNKEO(#<P<:(:<)"## LQR(K"?A MNLSC DNFM+N DA":%RNMNTK("@U RNMNTK(:"?ARNMNTK(G;<: LNVOC L<*VTL-./0123,45673888((++((((((R<;:B)%(%&'*B;<@(;.(WOLM(R<;:B)%RCMK(=/8/X(E#&%(;F.('"%<#B?<(*<;:B)%(!DSNY("?@(WOLM$0((L).:<%(H.:(Z"##Z(!<?[:<(\RM8/(;<%;%<;$("?
@(Z%&'Z(!%&'%<;(.H(;G<(G&*"?#]("%%<%%<@(@";"$(":<(%G.F?0 SK^D5 SK,D5WOLMDSNY(I87"RM+W_J RM+*W_J D"@`<:(H&## D"@`<:(#B;< CMN_(=08J:@.)
YS_GO(#<P<:(DSNYL<*VTL(DSNY J_Y+S-aMNLSC(R!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$( !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$)"*'+,-./ 0123 0145 0146 !
"#$ 0144 0164 0167 0148 0140)*&9:;9)<*'< 0156 0148 !
"## !
"#$ !
"#% !
"&$ !
"#' !"%!
!
"#()&9=/*"> 01?2 014@ 0143 0165 0162 0138 0163 0146 0163-A, 0160 0146 014@ 0140 0167 0137 0166 0144 0162/& 013?
0143 014@ 0168 0165 0138 0166 0142 0162./>/B" 0134 0144 014@ 0167 0168 0135 016?
0146 0166CD& 0143 0145 0143 0146 0143 0166 0167 0148 0168E<)9)<*'< 0128 !
"#) !
"## !
"#$ !
"#% !
"&$ !
"#' !"%!
!
"#(E<) 01?2 0143 014@ 0167 0168 0135 0163 0144 0162#,*%, 0122 0148 0146 0144 0146 0164 0167 0147 0140#,&* 0123 0148 0146 0142 0144 0162 !
"#' !"%!
014@>+) 012?
0145 0146 0146 0143 0164 0167 0148 0167<>#,>/F 0144 0145 0143 0146 0143 0166 0168 0148 0167<>#,>/G 0128 0148 0146 !
"#$ 0144 0162 !
"#' !"%!
014@+"#, 0122 0148 0146 0142 0144 0162 0140 !"%!
014@+HID9)<*'< !
"$# !
"#) !
"## !
"#$ !
"#% !
"&$ !
"#' !"%!
!
"#(+HID 0123 0148 0146 0144 0146 0164 0140 0147 014@&/-,> 0150 0148 0146 0142 0144 0164 0140 !"%!
014@&JB9)<*'< 0122 !
"#) !
"## !
"#$ !
"#% !
"&$ !
"#' !"%!
!"#(!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$)"*'+,-./ 0144 0145 4143 014?
9010?
0102 0148 01?8 5165)*&9:;9)<*'< !
"#$ 0147 4156 0146 *!"!'
0103 !"%!
!"+!
514?
)&9=/*"> 0140 014?
4103 0160 90103 0103 014?
01@2 4163-A, 014@ 0146 41?7 0162 9010?
0102 0146 01?0 21@2/& 0140 0146 41?
@ 0162 9010?
0103 0143 01@7 4176./>/B" 0140 014?
41@0 0168 90103 0104 014?
01@8 2106CD& 0144 0145 414@ 0140 9010?
0104 0145 01?6 2124E<)9)<*'< !
"#$ 0147 4152 0146 *!"!'
0100 !"%!
01?7 5143E<) 0167 014@ 4102 0164 9010?
010?
014?
01?0 21@6#,*%, 0142 0148 4125 0146 9010?
0102 0148 01?5 51?0#,&* !
"#$ 0147 4152 0146 *!"!'
0105 0147 01?7 5138>+) 0142 0148 412@ 014?
*!"!'
0102 0148 01?5 51?
?<>#,>/F 0144 0145 4148 014?
9010?
0105 0148 01?4 2177<>#,>/G !
"#$ !"%!
#",' 0146 9010?
!
"'& !"%!
01?8 513?+"#, !
"#$ 0147 4153 0146 *!"!'
0102 0147 01?8 513?+HID9)<*'< !
"#$ 0147 4157 !
"## *!"!'
0106 !"%!
!"+!
512?+HID 0142 0148 4127 0143 *!"!'
0102 0147 01?8 51?8&/-,> 0142 0148 4150 0146 *!"!'
0107 0147 01?8 51?6&JB9)<*'< !
"#$ 0147 4158 !
"## *!"!'
0104 !"%!
!"+!
$"%#-./0123*456.738(((99(((((K/I+,)%(%&'*,L/-(I<(MNOP(K/I+,)%KFPQ(?0@0R(J#&%(IH<('"%/#,>/(*/I+,)%(!GSTU(">-(MNOP$1((O)<+/%(V<+(W"##W(!/>X+/(YKP@0(I/%I%/I$(">-(W%&'W(!%&'%/I(<V(ID/(D&*">#Z("%%/%%/-(-"I"$("+/(%D<H>1+">E KP9M[\ KP9*M[\ G"-./+(V&## G"-./+(#,I/ \[U9S;] SQ^G6 SQ:G6O/*_`O O/*_`O(GSTUN(#/L/+(GSTU N(#/L/+(+/)"## OaK(Q">E PTOSF(K PTOSF OI">V<+-KTPT`Q("-b KTPT`Q(DI/+ KTPT`Q(+">E OT_NFFPT[(?1@GSTU(@3" MNOPPTQJ \Q \+-<) US[D GTHP9T GE"+%49!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$( !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$)*&+,-+).*'.
/012 /011 /034 /034 /035 /064 /074 !
"#$ !
"%&)&+89*": /07; /035 !
"&' /037 /03< /06/ /077 /036 /06<=>?
/057 /016 /035 /033 /033 /063 /073 /031 /066@A /015 /011 /034 /031 /031 /065 /075 /031 /063BC& /035 /016 /034 /035 /031 /063 /071 /036 /06<A?D /05/ /011 /034 /034 /035 /065 /075 /031 /066A.)+).*'.
/012 /011 /034 /034 /035 /064 /075 /035 !"%&A.)
/06/ /01/ /031 /037 /037 /06< /077 /033 /067#?*%?
/01< /013 /035 /031 /033 /061 /071 /031 /066#?& /011 /013 /034 /035 /031 /061 /071 /033 /066.:#?
:9E /012 /013 /035 /035 /031 /061 /071 /033 /067.:#?
:9F !"$!
!
"&( /03; !
"#) !
"#$ !
"%* !"')
!
"#$ !"%&GHDC+).*'.
/051 /011 /034 /034 /035 /064 /074 /035 !
"%&GHDC /01< /013 /034 /035 /031 /061 /075 /031 /063%I& /063 /01< /031 /033 /036 /072 /07< /062 /07;&9=?
: /057 /011 /034 /035 /031 /065 /075 /035 /063&JJ%"#" /034 /013 /034 /031 /033 /061 /071 /031 /063&JK+).*'.
/011 /011 /034 /034 !
"#$ /064 /074 /031 /063!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$)*&+C9"L9#=+).*'.
/034 /03; 30;5 /031 /0/6 /0/1 /016 /0<; 102;)&+89*": /03< /037 3067 /066 /0/6 /0/6 /034 /0<7 10/6=>?
/031 /034 3042 /035 !
"!# !
"+' /01/ /0<3 107;@A /035 /034 3045 /031 /0/6 /0/6 /017 /0<5 10;1BC& /033 /031 3017 /037 /0/6 /0/4 /01/ /0<1 1042A?D /034 /03; 30;6 /031 /0/6 /0/5 /016 /0<4 102;A.)+).*'.
/035 /034 3044 /031 /0/6 /0// /016 /0<4 102;A.)
/03< /037 303/ /062 /0/6 /0/7 /035 /0<3 10<2#?*%?
/031 /035 3055 /036 /0/6 /0/1 /01< /0<5 104/#?& /035 /03; 3045 /031 /0/6 /0/3 /017 /0<5 10;1.:#?
:9E /035 /03; 30;7 /035 !
"!# /0/2 /017 /0<1 1051.:#?
:9F !
"#* !"&!
&"!& !
"#) !
"!# /07/ !
"&& !
"+* ("+)GHDC+).*'.
/035 /03; 3044 /031 /0/6 /0/6 /016 /0<4 50/5GHDC /031 /034 304< /033 /0/6 /0/1 /017 /0<5 10;5%I& /06; /062 6023 /06; /0/6 /0/6 /031 /0<< 30;7&9=?
: /034 /03; 30;1 /035 /0/6 /0/; /016 /0<4 1023&JJ%"#" /031 /035 3051 /036 /0/6 /0/6 /01< /0<5 1043&JK+).*'.
/035 /034 3041 /033 /0/6 /0/6 /016 /0<4 50</,-./012345678-(((((++(((((M9DG?)%(%&'*?N9=(D.(OPQR(M9DG?)%MERS(7/</T(J#&%(DH.('"%9#?:9(*9DG?
)%(!FUVW(":=(OPQR$0((Q).G9%(I.G(X"##X(!9:YG9(ZMR</(D9%D%9D$(":=(X%&'X(!%&'%9D(.I(DC9(C&*":#[("%%9%%9=(="D"$("G9(%C.H:0G":A MR+O\] MR+*O\] F"=^9G(I&## F"=^9G(#?D9 ]\W+U-_ US`F3 US,F3Q9*abQ Q9*abQ(FUVWP(#9N9G(FUVW P(#9N9G(G9)"## QcM(S":A RVQUE(M RVQUE QD":I.G=MVRVbS("=d MVRVbS(CD9G MVRVbS(G":A QVaPEERV\(70<FUVW(<6" OPQRRVSJ ]S ]G=.)
WU\C FVHR+V FA"G%!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$( !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$)"*'+,-./ 0123 0124 0125 0126 0122 0132 0172 0126 0120)*&89:8);*'; 015< 0150 !
"#$ 0124 012< 013< !
"%& 012< 0127)&8=/*"> 017?
0122 !
"#$ 012@ 0120 013@ 0177 0127 013?-)& 015@ 0150 !
"#$ 012< 0126 0136 0172 012< 012@-A, 0122 0126 0123 0123 0127 0137 017?
0123 0136BC& 0122 0124 0125 0126 0122 0132 0172 0125 013<D;)8);*'; 0157 0150 !
"#$ 012< 0126 0136 !
"%& 012< 012@D;) 01@4 0123 012?
012@ 0134 01?< 017@ 012?
0135;>#,>/E 0154 0150 !
"#$ 012< 0126 013< 0172 012< 012@;>#,>/F !
"$' !
"&' !
"#$ !"&!
!
"#( !
")( !
"%& !"&!
!
"#*+GHC8);*'; 0124 0124 0125 0124 012< 013< !
"%& 012< 012@%I& 0130 012< 0122 0122 0123 0137 0173 012?
0132&/-,> 015@ 0150 0125 012< 0126 0136 0172 012< 012@&J'8);*'; 0150 0150 !
"#$ 0124 012< 013< !
"%& 012< 012@&JK8>>#* 0123 0124 0125 0125 0122 0132 0173 0126 0134&JK 0122 0124 0125 0126 0125 0135 0172 0126 0120!
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$ !
"##$ !%&'$)"*'+,-./ 0126 0124 2162 0132 !"!'
0100 0150 0174 6120)*&89:8);*'; 0124 015@ 2140 013< !"!'
0100 0157 !
"*% 6145)&8=/*"> 0123 0125 213@ 0130 0100 0100 0125 017@ 5123-)& 0124 015@ 2140 0136 !"!'
0100 015@ 01?0 6167-A, 0123 0126 2122 0137 0100 0100 0126 0173 51<?BC& 0126 0124 2167 0132 !"!'
0100 0150 017< 612<D;)8);*'; 012< 0150 21<3 0135 !"!'
0100 015@ 01?0 6164D;) 012@ 012?
21@4 01?5 0100 0100 012?
017@ 51?6;>#,>/E 0124 0157 2146 013< !"!'
0100 0157 01?0 6152;>#,>/F !"&!
!
"&* &"'% !
")( !"!'
0100 0153 !
"*% 61<6+GHC8);*'; 0124 015@ 21<4 0136 !"!'
0100 0150 01?
@ $"($%I& 0123 0122 21?2 0137 0100 0100 0125 0173 61@<&/-,> 012< 015@ 21<5 0136 !"!'
!"!'
015@ 01?0 6155&J'8);*'; 012< 0150 21<< 0136 !"!'
0100 015@ 01?
@ 614?&JK8>>#* 0126 0124 216@ 0133 !"!'
0100 0124 017< 61?4&JK 012< 0150 21<0 0132 !"!'
0100 0150 0174 613<+,-./012345,/016((((88(((((L/H+,)%(%&'*,M/-(H;(NOPQ(L/H+,)%LEQR(70@0S(J#&%(HG;('"%/#,>/(*/H+,)%(!FTUV(">-(NOPQ$1((P);+/%(I;+(W"##W(!/>X+/(YLQ@0(H/%H%/H$(">-(W%&'W(!%&'%/H(;I(HC/(C&*">#Z("%%/%%/-(-"H"$("+/(%C;G>1+">D LQ8N[\ LQ8*N[\ F"-./+(I&## F"-./+(#,H/ \[V8T:] TR^F3 TR9F3P/*_`P P/*_`P(FTUVO(#/M/+(FTUV O(#/M/+(+/)"## PaL(R">D QUPTE(L QUPTE PH">I;+-LUQU`R("-b LUQU`R(CH/+ LUQU`R(+">D PU_OEEQU[(71@FTUV(@?"
NOPQQURJ \R \+-;) VT[C FUGQ8U FD"+%50REFAALTOCMUCU-BOJARCU-ZEMANONLINEAONLINEBUEDINBBN-CCMU-HEA-CJHU-CRWTH-CUPV-CREF ?
.03?
.02?
.03?
.01?
.03?
.02?
.05?
.02?
.06?
.03?
.05?
.03?AALTO .93?
?
.54?
.54?
.23?
.36 .58?
.56?
.65?
.69?
.64?
.67?
.62?CMU .94?
.30?
?
.47 .14?
.22?
.52?
.41 .50?
.57?
.45?
.44 .38CU-BOJAR .94?
.26?
.38 ?
.10?
.22?
.61?
.47?
.46 .55?
.42 .49?
.44CU-ZEMAN .98?
.58?
.73?
.77?
?
.55?
.79?
.71?
.84?
.80?
.77?
.79?
.75?ONLINEA .94?
.41 .61?
.57?
.23?
?
.68?
.63?
.71?
.71?
.63?
.54?
.61?ONLINEB .93?
.30?
.31?
.26?
.10?
.17?
?
.32?
.35 .31 .22?
.29?
.38UEDIN .91?
.27?
.35 .34?
.11?
.18?
.47?
?
.54?
.50?
.35 .29 .35BBN-C .95?
.21?
.22?
.36 .06?
.17?
.38 .26?
?
.32 .24?
.31?
.26?CMU-HEA-C .90?
.17?
.19?
.23?
.09?
.18?
.32 .27?
.34 ?
.31?
.31?
.30?JHU-C .93?
.19?
.30?
.35 .09?
.24?
.50?
.34 .47?
.45?
?
.41?
.36RWTH-C .91?
.16?
.35 .29?
.12?
.27?
.41?
.37 .42?
.42?
.23?
?
.24?UPV-C .94?
.24?
.40 .36 .09?
.28?
.39 .32 .46?
.47?
.33 .36?
?> others .93 .26 .37 .38 .11 .24 .47 .40 .49 .49 .38 .41 .40>= others .97 .42 .56 .55 .25 .39 .67 .62 .70 .70 .61 .65 .62Table 21: Sentence-level ranking for the WMT10 Czech-English News Task (Combining expert andnon-expert Mechanical Turk judgments)51REFAALTOCMUCU-ZEMANDFKIFBKHUICONGJHUKITKOCLIMSILIUONLINEAONLINEBRWTHUEDINUMDUPPSALAUU-MSBBN-CCMU-HEA-CCMU-HYPO-CJHU-CKOC-CRWTH-CUPV-CREF ?
.00?
.02?
.00?
.07?
.04?
.03?
.00?
.06?
.04?
.00?
.02?
.07?
.07?
.07?
.02?
.09?
.03?
.03?
.10?
.04?
.04?
.03?
.02?
.07?
.06?AALTO 1.00?
?
.43 .39 .48 .60?
.38 .41 .74?
.18?
.42 .57?
.50?
.63?
.55?
.68?
.79?
.42 .33 .71?
.61?
.66?
.54 .51?
.66?
.56?CMU .95?
.34 ?
.19?
.45 .52?
.38 .50 .63?
.17?
.51?
.55?
.56?
.66?
.55?
.60?
.56?
.30 .40 .62?
.64?
.49?
.58?
.46 .64?
.46?CU-ZEMAN 1.00?
.44 .64?
?
.43 .72?
.31 .45?
.69?
.36 .55 .62?
.75?
.75?
.78?
.75?
.75?
.48?
.56?
.79?
.82?
.72?
.68?
.63?
.67?
.84?DFKI .92?
.29 .33 .35 ?
.37 .40 .34 .59 .08?
.42 .50 .49 .64?
.35 .44 .44 .50 .41 .70?
.61?
.57 .46 .47 .62?
.44FBK .93?
.26?
.23?
.17?
.49 ?
.12?
.30 .52?
.08?
.20?
.45?
.41 .62?
.44 .44 .48?
.18?
.25?
.53?
.47 .38 .38 .22?
.41 .51?HUICONG .92?
.34 .39 .37 .38 .71?
?
.53?
.67?
.18?
.51?
.47 .60?
.65?
.49?
.55?
.78?
.35 .41 .56?
.77?
.74?
.58?
.41 .65?
.57?JHU .92?
.35 .30 .17?
.52 .45 .25?
?
.58?
.16?
.43 .38 .57?
.60?
.54?
.60?
.70?
.29 .25 .65?
.75?
.56?
.62?
.49?
.66?
.48?KIT .90?
.14?
.16?
.14?
.35 .28?
.19?
.16?
?
.03?
.29?
.20?
.35 .53?
.21?
.24?
.30 .20?
.22?
.44 .29 .38 .35 .24 .40 .24?KOC .95?
.66?
.71?
.51 .75?
.80?
.58?
.68?
.93?
?
.75?
.87?
.72?
.74?
.74?
.81?
.81?
.78?
.66?
.89?
.85?
.80?
.80?
.72?
.91?
.73?LIMSI .99?
.26 .24?
.32 .45 .61?
.25?
.38 .50?
.10?
?
.50?
.55?
.69?
.52?
.57?
.57?
.29?
.22?
.60?
.52?
.42 .47?
.37 .60?
.56?LIU .87?
.17?
.20?
.14?
.34 .22?
.31 .38 .66?
.04?
.27?
?
.51?
.53?
.52?
.53?
.51 .20?
.33 .64?
.59?
.48?
.48 .51 .37 .53?ONLINEA .90?
.25?
.29?
.18?
.34 .43 .23?
.28?
.49 .08?
.32?
.30?
?
.44 .38 .40 .42 .32?
.35?
.39 .47 .51 .27?
.35 .43 .40ONLINEB .76?
.22?
.24?
.14?
.27?
.27?
.25?
.25?
.32?
.22?
.21?
.28?
.32 ?
.27?
.21?
.30?
.23?
.15?
.41 .31 .40 .23?
.16?
.42 .29RWTH .89?
.22?
.23?
.13?
.49 .35 .29?
.21?
.62?
.15?
.32?
.29?
.46 .57?
?
.39 .49 .25 .38 .41 .27 .34 .36 .27 .48?
.22?UEDIN .91?
.15?
.20?
.12?
.49 .35 .24?
.22?
.49?
.04?
.22?
.30?
.46 .62?
.43 ?
.39 .11?
.15?
.45 .33 .40 .45 .33 .34 .33UMD .91?
.12?
.23?
.06?
.35 .29?
.11?
.16?
.47 .14?
.23?
.35 .40 .55?
.36 .47 ?
.16?
.17?
.44 .29?
.27 .37 .26 .27 .24?UPPSALA .94?
.30 .41 .23?
.35 .53?
.26 .37 .66?
.03?
.54?
.71?
.57?
.65?
.45 .72?
.67?
?
.25 .59?
.69?
.49?
.63?
.33 .60?
.64?UU-MS .83?
.28 .42 .24?
.41 .49?
.28 .42 .68?
.10?
.55?
.48 .55?
.63?
.49 .56?
.60?
.32 ?
.52?
.58?
.61?
.64?
.46?
.64?
.50?BBN-C .90?
.15?
.16?
.10?
.22?
.17?
.22?
.18?
.41 .06?
.16?
.21?
.35 .45 .30 .26 .34 .13?
.20?
?
.42?
.14?
.27 .11?
.25 .21?CMU-HEA-C .83?
.20?
.18?
.07?
.29?
.32 .06?
.10?
.49 .05?
.26?
.21?
.41 .33 .37 .43 .58?
.10?
.14?
.18?
?
.33 .32 .11?
.34 .24?CMU-HYPO-C .96?
.24?
.20?
.07?
.37 .33 .12?
.21?
.40 .10?
.41 .26?
.40 .54 .25 .37 .44 .13?
.17?
.49?
.31 ?
.34 .23?
.51?
.45JHU-C .97?
.33 .22?
.18?
.31 .30 .27?
.18?
.33 .12?
.19?
.33 .59?
.60?
.39 .32 .30 .19?
.20?
.44 .29 .34 ?
.21?
.36 .23KOC-C .93?
.11?
.31 .17?
.41 .50?
.25 .27?
.44 .11?
.42 .36 .47 .68?
.43 .41 .40 .33 .18?
.59?
.57?
.46?
.47?
?
.52?
.43RWTH-C .87?
.20?
.10?
.21?
.25?
.27 .15?
.23?
.24 .02?
.20?
.30 .34 .47 .27?
.34 .36 .14?
.20?
.33 .26 .21?
.24 .20?
?
.17?UPV-C .93?
.14?
.20?
.10?
.42 .29?
.25?
.25?
.57?
.20?
.22?
.33?
.39 .45 .47?
.40 .50?
.24?
.28?
.44?
.42?
.27 .34 .28 .56?
?> others .92 .25 .28 .18 .39 .41 .25 .30 .52 .12 .34 .39 .47 .57 .42 .46 .51 .27 .28 .52 .49 .45 .44 .34 .50 .42>= others .96 .46 .49 .35 .53 .62 .45 .51 .71 .24 .54 .58 .63 .72 .63 .66 .70 .50 .51 .75 .73 .68 .67 .59 .74 .64Table 22: Sentence-level ranking for the WMT10 German-English News Task (Combining expert andnon-expert Mechanical Turk judgments)REFCAMBRIDGECOLUMBIACU-ZEMANDFKIHUICONGJHUONLINEAONLINEBUEDINUPCBBN-CCMU-HEA-CJHU-CUPV-CREF ?
.05?
.01?
.02?
.03?
.03?
.01?
.02?
.04?
.03?
.04?
.03?
.07?
.05?
.04?CAMBRIDGE .90?
?
.24?
.11?
.35?
.26?
.43 .35 .50?
.45?
.33?
.40 .46 .28?
.41COLUMBIA .97?
.61?
?
.25?
.47 .44 .61?
.53?
.62?
.59?
.48?
.59?
.57?
.45?
.57?CU-ZEMAN .92?
.73?
.59?
?
.62?
.66?
.71?
.65?
.75?
.79?
.58?
.75?
.78?
.71?
.72?DFKI .95?
.50?
.41 .21?
?
.46 .56?
.52?
.65?
.62?
.47 .52?
.56?
.52?
.60?HUICONG .93?
.57?
.34 .21?
.36 ?
.47?
.43 .67?
.58?
.40 .51?
.62?
.46?
.52?JHU .94?
.39 .22?
.16?
.30?
.32?
?
.41 .52?
.47?
.37 .41 .33?
.28 .35ONLINEA .92?
.45 .35?
.24?
.34?
.41 .41 ?
.60?
.58?
.38 .55?
.46 .36 .57?ONLINEB .87?
.34?
.24?
.15?
.21?
.19?
.33?
.25?
?
.34?
.26?
.34?
.37?
.24?
.40UEDIN .94?
.33?
.26?
.12?
.24?
.22?
.25?
.25?
.50?
?
.25?
.28?
.32?
.25?
.26UPC .89?
.45?
.36?
.23?
.39 .37 .42 .48 .62?
.57?
?
.54?
.51?
.50?
.53?BBN-C .91?
.33 .25?
.11?
.32?
.30?
.34 .31?
.51?
.41?
.30?
?
.36 .26?
.31CMU-HEA-C .89?
.37 .20?
.10?
.29?
.23?
.23?
.35 .50?
.44?
.31?
.34 ?
.23?
.31JHU-C .89?
.39?
.31?
.17?
.37?
.33?
.38 .42 .63?
.47?
.31?
.42?
.42?
?
.37?UPV-C .91?
.35 .30?
.16?
.29?
.26?
.32 .28?
.44 .35 .27?
.27 .30 .24?
?> others .92 .42 .29 .16 .33 .32 .39 .37 .54 .48 .34 .42 .44 .35 .43>= others .97 .62 .45 .29 .46 .50 .61 .52 .68 .68 .51 .64 .65 .58 .66Table 23: Sentence-level ranking for the WMT10 Spanish-English News Task (Combining expert andnon-expert Mechanical Turk judgments)52REFCAMBRIDGECMU-STATXFERCU-ZEMANDFKIGENEVAHUICONGJHULIGLIMSILIUMNRCONLINEAONLINEBRALIRWTHUEDINBBN-CCMU-HEA-CCMU-HYPO-CDCU-CJHU-CLIUM-CRWTH-CUPV-CREF ?
.02?
.00?
.00?
.00?
.00?
.05?
.02?
.00?
.00?
.00?
.02?
.06?
.02?
.04?
.02?
.04?
.03?
.02?
.05?
.05?
.04?
.05?
.06?
.02?CAMBRIDGE .82?
?
.42 .16?
.12?
.35 .31 .45 .21?
.47 .29 .38 .28?
.54 .43 .33 .38 .28 .39 .45?
.24 .25 .34 .54?
.37CMU-STATXFER .91?
.50 ?
.17?
.41 .17?
.28 .44 .36 .48?
.56?
.57?
.47 .56?
.70?
.49 .50 .47 .61?
.68?
.55?
.50 .42 .52?
.51?CU-ZEMAN 1.00?
.74?
.71?
?
.74?
.46 .67?
.73?
.73?
.74?
.75?
.76?
.75?
.89?
.78?
.66?
.83?
.74?
.87?
.73?
.80?
.83?
.77?
.95?
.82?DFKI 1.00?
.77?
.48 .17?
?
.27?
.49 .52 .48 .64?
.69?
.67?
.47 .62?
.53 .47 .64?
.60?
.73?
.72?
.79?
.58?
.66?
.73?
.74?GENEVA .98?
.58 .70?
.44 .59?
?
.55?
.67?
.70?
.70?
.77?
.73?
.63?
.81?
.81?
.69?
.77?
.73?
.62?
.66?
.75?
.60?
.73?
.88?
.67?HUICONG .89?
.53 .34 .13?
.34 .30?
?
.41 .36 .43 .70?
.56?
.57 .59?
.56?
.43 .55?
.45 .51?
.64?
.48 .49 .49 .53?
.57?JHU .88?
.36 .38 .11?
.34 .25?
.35 ?
.33?
.46 .49?
.48 .40 .50 .40 .34 .36 .39 .33 .59?
.54?
.41 .42 .40 .41LIG .98?
.65?
.34 .18?
.44 .26?
.39 .56?
?
.60?
.55?
.51?
.45 .54?
.53 .39 .38 .52?
.54?
.53?
.51?
.53?
.55 .51 .58?LIMSI .98?
.40 .24?
.23?
.23?
.15?
.29 .38 .25?
?
.28 .38 .27?
.64?
.35 .30 .41 .27 .33 .49 .45 .37 .28 .45 .39LIUM .90?
.40 .19?
.12?
.30?
.11?
.11?
.26?
.15?
.36 ?
.36 .25?
.37 .39 .26 .29 .24 .34 .49?
.34 .33 .34 .31 .38NRC .93?
.31 .06?
.15?
.29?
.23?
.20?
.32 .16?
.38 .36 ?
.23?
.53 .36 .24?
.31 .44 .37 .47?
.45?
.29 .39 .38 .42ONLINEA .92?
.60?
.47 .15?
.44 .22?
.32 .46 .34 .57?
.52?
.60?
?
.52?
.34 .44 .57?
.56 .51 .51 .64?
.46 .51 .41 .60ONLINEB .85?
.35 .32?
.09?
.33?
.10?
.29?
.31 .25?
.17?
.40 .34 .24?
?
.38 .32?
.28 .39 .30 .42 .37 .41 .35 .32 .22?RALI .90?
.31 .19?
.10?
.38 .10?
.17?
.47 .35 .38 .33 .38 .48 .48 ?
.29?
.31 .29 .38 .40 .38 .34 .31 .57?
.21?RWTH .93?
.43 .33 .12?
.47 .26?
.39 .40 .47 .35 .45 .49?
.44 .53?
.54?
?
.44?
.42 .48 .51?
.54?
.48?
.49 .50?
.26UEDIN .92?
.42 .32 .10?
.22?
.10?
.28?
.30 .42 .30 .55 .36 .23?
.43 .33 .20?
?
.41 .24 .52?
.46 .25 .22 .27 .37BBN-C .92?
.49 .33 .24?
.28?
.18?
.40 .39 .28?
.45 .27 .27 .36 .39 .35 .35 .31 ?
.26 .45?
.43 .26 .58?
.36 .28CMU-HEA-C .90?
.41 .21?
.06?
.23?
.29?
.28?
.27 .22?
.39 .40 .22 .39 .43 .29 .30 .40 .28 ?
.43 .28 .15?
.25 .26 .16CMU-HYPO-C .84?
.18?
.20?
.14?
.20?
.22?
.21?
.19?
.16?
.31 .22?
.21?
.36 .38 .34 .27?
.22?
.16?
.24 ?
.36 .23 .10?
.33 .24DCU-C .92?
.27 .24?
.12?
.17?
.23?
.30 .29?
.24?
.32 .43 .22?
.28?
.41 .23 .27?
.28 .22 .23 .25 ?
.23 .23 .24 .17JHU-C .88?
.47 .26 .10?
.33?
.24?
.36 .34 .24?
.41 .39 .40 .42 .39 .34 .25?
.42 .28 .37?
.38 .39 ?
.37 .32 .38?LIUM-C .90?
.48 .42 .13?
.25?
.20?
.33 .50 .30 .44 .37 .34 .37 .52 .43 .34 .33 .22?
.34 .56?
.33 .43 ?
.49?
.44RWTH-C .89?
.22?
.19?
.03?
.23?
.12?
.19?
.23 .27 .30 .36 .19 .47 .54 .26?
.16?
.27 .19 .26 .28 .16 .22 .16?
?
.22UPV-C .89?
.27 .15?
.10?
.16?
.29?
.30?
.31 .25?
.36 .42 .24 .32 .64?
.46?
.34 .27 .44 .33 .44 .23 .17?
.31 .24 ?> others .91 .43 .32 .14 .31 .21 .31 .39 .31 .42 .44 .40 .38 .52 .43 .33 .40 .37 .40 .49 .43 .38 .4 .44 .39>= others .97 .64 .51 .24 .40 .31 .50 .59 .50 .63 .68 .65 .51 .68 .65 .55 .66 .63 .69 .75 .71 .64 .62 .74 .67Table 24: Sentence-level ranking for the WMT10 French-English News Task (Combining expert andnon-expert Mechanical Turk judgments)53
