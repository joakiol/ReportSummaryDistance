Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 281?288Manchester, August 2008Using Web-Search Results to Measure Word-Group SimilarityAnn Gledson and John KeaneSchool of Computer Science,University of ManchesterOxford Road, Manchester, UK M13 9PL{ann.gledson,john.keane}@manchester.ac.ukAbstractSemantic relatedness between words isimportant to many NLP tasks, and nu-merous measures exist which use a vari-ety of resources.
Thus far, such work isconfined to measuring similarity betweentwo words (or two texts), and only ahandful utilize the web as a corpus.
Thispaper introduces a distributional similar-ity measure which uses internet searchcounts and also extends to calculating thesimilarity within word-groups.
Theevaluation results are encouraging: forword-pairs, the correlations with humanjudgments are comparable with state-of-the-art web-search page-count heuristics.When used to measure similarities withinsets of 10 words, the results correlatehighly (up to 0.8) with those expected.Relatively little comparison has beenmade between the results of differentsearch-engines.
Here, we compare ex-perimental results from Google, Win-dows Live Search and Yahoo and findnoticeable differences.1 IntroductionThe propensity of words to appear together intexts, also known as their distributional similarityis an important part of Natural Language Proc-essing (NLP):?The need to determine semantic related-ness?
between two lexically expressedconcepts is a problem that pervades much of[NLP].?
(Budanitsky and Hirst 2006)?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.Such requirements are evident in word sense dis-ambiguation (WSD) (Patwardhan et-al 2003),spelling correction (Budanitsky and Hirst 2006)and Lexical Chaining (Morris and Hirst 1991).As well as measuring the co-occurrence ofword-pairs, it is also considered useful to extendthese measures to calculate the likelihood of setsof words to appear together.
For example, Carac-ciolo et-al (2004) evaluate two established topicarea detection (TAD) algorithms and indicatetext homogeneity as a document feature affectingthe results.
Furthermore, Gliozzo et-al (2004)and McCarthy et-al (2007) highlight the impor-tance of topic features for WSD and Navigli andVelardi (2005) report differing WSD results for 3types of text: unfocussed, mid-technical (eg fi-nance articles) and overly technical (eg interop-erability and computer networks).
Measures ofword-group similarity can be used to calculatethe level of topic cohesion in texts, and the po-tential for this to be used to benefit NLP areassuch as WSD and TAD has been indicated inGledson and Keane (2008).We consider web-searching an important partof measuring similarity, as it provides up-to-dateinformation on word co-occurrence frequenciesin the largest available collection of English lan-guage documents.
We propose a simple measurethat uses internet search counts by measuring thedecline in the number of hits as more words(from the word-set to be measured) are appendedto a query string using the ?AND?
operator.
Tooffset against the effect of individual word hit-counts, the above gradient is compared to that ofthe individual word hit counts ?
arranged in de-scending order of hits returned.The paper is structured as follows: in Section2 we describe related work in the areas of wordsimilarity and the use of search-engine counts inNLP.
Section 3 outlines the algorithm to be usedfor our similarity measure which can utilise anysearch-engine that is web-service enabled.
Sec-281tion 4 describes and discusses the results of theevaluation techniques used, one for evaluatingword-pair similarities, which compares with pre-vious work and human judgements, and three forevaluating word-group similarities.
Section 5outlines the conclusions drawn from these ex-periments and Section 6 discusses further work.2 Related WorkThe most commonly used similarity measuresare based on the WordNet lexical database (egBudanitsky and Hirst 2006, Hughes and Ramage2007) and a number of such measures have beenmade publicly available (Pedersen et-al 2004).The problem with such methods is that they areconfined to measuring words in terms of the lex-ical connections manually assigned to them.Language is evolving continuously and the con-tinual maintenance and updating of such data-bases is highly labour intensive, therefore, suchlexical resources can never be fully up-to-date.
Inaddition, the lexical connections made betweenwords and concepts do not cover all possible re-lations between words.
An important relationshipbetween words is distributional similarity andBudanitsky and Hirst (2006) conclude that thecapture of these ?non-classical?
relationships isan important area of future research.Weeds and Weir (2006) use a ?co-occurrenceretrieval method to compute word relatedness,which is described as analogous to the precisionand recall metrics of document retrieval.
Theyobserve the ?plausibility?
of substituting word-1for word-2 within verb/object-noun grammaticalrelationships.
This method is restricted to docu-ment retrieval from the BNC corpus, due to thepre-requisite that words are part-of-speechtagged and that some grammatical parsing is per-formed.
The similarity measure that we have de-veloped is simpler and does not rely on pre-processing of texts.
This distributional similaritymeasure calculates the propensity of words toappear together, regardless of part-of-speech orgrammatical functions.We assert that the world-wide-web can beused to capture distributional similarity, and isless likely to suffer from problems of coverage,found in smaller corpora.
The web as a corpushas been successfully used for many areas inNLP (Kilgarriff and Grefenstette 2003) such asWSD (Mihalcea and Moldovan 1999), obtainingfrequencies for bigrams (Keller and Lapata 2003)and noun compound bracketing (Nakov andHearst 2005).
Such reliance on web search-engine results does come with caveats, the mostimportant (in this context) being that the reportedhit counts may not be entirely trustworthy (Kil-garriff 2007).Strube and Ponzetto?s (2006) use the Wikipe-dia database, which includes a taxonomy of cate-gories, and they adapt ?well established semanticrelatedness measures originally developed forWordNet?.
They achieve a correlation coefficientof 0.48 with human judgments, which is stated asbeing higher than a Google-only and WordNet-only based measure for the largest of their testdatasets (the 353 word-pairs of 353-TC)Chen et-al (2006) use the snippet?s returnedfrom web-searches, in order to perform wordsimilarity measurement that captures ?new usag-es?
of ever evolving, ?live?
languages.
A doublechecking model is utilized which combines thenumber of occurrences of the first word in thesnippets of the second word, and vice-versa.
Thiswork achieves a correlation coefficient of 0.85with the Miller and Charles (1998) dataset of 28word-pairs, but to achieve the best results, 600-700 snippets are required for each word-pair,requiring extra text-processing and searching.The work most similar to ours is the set ofpage-counting techniques of Bollegala et-al(2007).
They combine the use of web-searchpage-counts with the analysis of the returned textsnippets and achieve impressive correlations withhuman judgment (0.834 with the Miller andCharles dataset).
The text-snippet analysis ismore complex than that of Chen et-al (2006), asthe optimal number of snippets is over 1000, andtheir process involves complex pattern matching,which may not scale well to measuring word-groups similarity.
Their page counting heuristicsutilize 4 popular co-occurrence methods: Jac-card, Overlap (Simpson), Dice and PMI (Point-wise mutual information) but again, these tech-niques are not designed to scale up to largernumbers of input words.Mihalcea et-al (2006) and Gabrilovich andMarkovitch (2007) achieve good results whencomparing texts, the former utilizing the inverse-document frequency heuristic and the latter in-dexing the entire Wikipedia database and com-paring vector representations of the two inputs.None of the above work is adapted to be usedon single groups of words as a measure of topiccohesion.
They could be adapted by combiningsimilarity results between many pairs, but thismight similarly have a high computational cost.In addition, no comparisons of different web-search engines are made when using web-counts.282As the use of web-counts is precarious (Kigarriff2007), these types of comparisons are of highpractical value.3 Similarity MeasureThe proposed measure can be used with anyweb-service enabled search-engine and in ourexperiments we compare three such search-engines: Yahoo [Yahoo], Windows Live Search[Live-Search] and Google [Google].
The simi-larity measure WebSim[search-engine] is calculated foreach document d as follows:Step 1: Add the required set of n lemmas to theLemmas list.Step 2: Using an internet search-engine, obtainthe hit counts of each member of Lemmas.Step 3: Order the resulting list of n lemma/hit-counts combinations in descending order of hit-counts and save lemma/hit combinations to In-divHitsDesc.Step 4: For each lemma of IndivHitsDesc, saveto CombiHitsDesc preserving the ordering.Step 5: For each member of CombiHitsDesc:CombiHitsDesci, obtain the hit counts of the as-sociated lemma, along with the concatenatedlemmas of all preceding list members of Combi-HitsDesc (CombiHitsDesc[0] to CombiHits-Desc[i-1]).
This list of lemmas are concatenatedtogether using ?
AND ?
as the delimiter.Step 6: Calculate the gradients of the best-fitlines for the hit-counts of IndivHitsDesc andCombiHitsDesc: creating gradIndiv and grad-Combi respectively.Step 7: WebSim[search-engine] is calculated for d asgradIndiv minus gradCombi.As WebSim[search-engine] is taken as the differencebetween the two descending gradients, the meas-ure is more likely to reveal the affects of theprobability of the set of lemmas co-occurring inthe same documents, rather than by influencessuch as duplicate documents.
If the decline in hit-counts from IndivHitsDesc[i-1] to IndivHits-Desc[i] is high, then the decline in the number ofhits from CombiHitsDesc[i-1] to CombiHits-Desc[i] is also expected to be higher, and theconverse, for lower drops is also expected.
Devi-ations from these expectations are reflected in thefinal homogeneity measure and are assumed tobe caused by the likelihood of lemmas co-occurring together in internet texts.Search-engines are required that publish a setof web-services for a fully automated process.The Google, Yahoo and Windows Live Searchsearch-engines have been selected and the resultsof each measure are compared.
In response toimportant problems highlighted by Kilgarriff(2007) relating to the use of web counts in NLPexperiments: firstly, a measure is proposed be-tween words that does not require the pre-annotation of part-of-speech information anddoes not rely on query syntax / meta-language.Secondly, our measure relies on the use of web-search page counts (as opposed to word instancecounts) as we are measuring the likelihood of co-occurrence in the same text.
Finally, measuresare taken to try to avoid the problem of arbitrarysearch-engine counts.
For example, each measureis the result of a comparison between the declinerates of 2 sets of hit counts and the full set ofqueries for each input text are taken within a 20second interval (for groups of 10 words).
In addi-tion, for the Google and Yahoo measures theweb-service request parameter includes optionsto avoid the return of the same web-page(Google: ?filter_alike_results?
; Yahoo: ?Al-lowSimilar?
).4 EvaluationFour methods of evaluation are used to verifythat the similarity measure is capable of measur-ing similarity between words.
These methods areselected to be as varied as possible, to provide afuller understanding of the usefulness of ourtechnique and to verify that it is working as ex-pected.4.1 Word-pairsThe results of Rubenstein and Goodenough(1965) (65 word-pairs), Miller and Charles(1998) (30 word-pair subset of Rubenstein andGoodenough 1965) and Finkelstein et-al (2002)(353 word-pairs) are used to evaluate the abilityof the proposed method to calculate the similaritybetween word-pairs.
These results sets list thesimilarity scores of the word-pairs as assigned byhumans.
Although this does provide a usefulbenchmark, extremely high, or perfect correla-tions are unrealistic, as those involved in the ex-periments were asked to think about the wordsassociation in terms of lexical relations such assynonymy as opposed to the broader idea of dis-tributional similarity.
Nevertheless, in conjunc-tion with other evaluation techniques, these com-parisons can still be useful, as some correlationwould be expected if our measure was function-ing as required.In addition, our results are compared with thepage-count based similarity scores of Bollegala283Measure Correlation (Pearson?s R)WebSim[YAHOO] -0.57**WebSim[LIVE-SEARCH] -0.60**WebSim[GOOGLE] -0.43**Google-Jaccard (Strube & Ponzetto 2006) 0.41pl (path-lengths)  (Strube & Ponzetto 2006) 0.56wup  (Strube & Ponzetto 2006) 0.52lch (Strube & Ponzetto 2006) 0.54** Significant at the 0.01 level (2-tailed)Italics: Statistical significance not specified in Strube and Ponzetto (2006)Table 1: Correlation with Human Ratings on Rubenstein-Goodenough datasetMeasure Correlation (Pearson?s R)WebSim[YAHOO] -0.55**WebSim[LIVE-SEARCH] -0.53**WebSim[GOOGLE] -0.39*Jaccard  (Bollegala et-al 2007) 0.26Dice  (Bollegala et-al 2007) 0.27Overlap  (Bollegala et-al 2007) 0.38PMI  (Bollegala et-al 2007) 0.55pl (Strube & Ponzetto 2006) 0.49** Significant at the 0.01 level (2-tailed)* Significant at the 0.05 level (2-tailed)Italics: Statistical significance not specified in Bollegala et-al (2007) or Strube and Ponzetto (2006)Table 2: Correlation with Human Ratings on Miller-Charles?
datasetMeasure Correlation (Pearson?s R)WebSim[YAHOO] -0.37**WebSim[LIVE-SEARCH] -0.40**WebSim[GOOGLE] -0.119*Google-Jaccard (Strube & Ponzetto 2006) 0.18wup (Strube & Ponzetto 2006) 0.48lch (Strube & Ponzetto 2006) 0.48** Significant at the 0.01 level (2-tailed)* Significant at the 0.05 level (2-tailed)Italics: Statistical significance not specified in Strube and Ponzetto (2006)Table 3: Correlation with Human Ratings on 353-TC datasetet-al (2007) and the best performing Wikipedia-based measures of Strube and Ponzetto (2006).The former gauge similarity using a number ofpopular co-occurrence measures adapted forweb-search page-counts and their method is con-sidered closest to our approach.
The individualresults for each word-pair are shown in Tables 1,2 and 3.
They indicate that moderate correlationsexists, particularly in the Rubenstein and Goode-nough set.
WebSim[YAHOO] and WebSim[LIVESEARCH] significantly outperform the Web-Sim[GOOGLE] method.
This may be due toGoogle?s results being more erratic.
Google?sreturned counts were sometimes found to in-crease as extra ?AND?
clauses were added to thequery string.
This is perhaps because of the wayin which Google combines the results of severalsearch-engine hubs.
(This was accommodated toa degree by setting any negative scores to zero.
)All three of the methods were comparable withthe results of Bollegala et-al (2007), with theWebSim[LIVE SEARCH] measure performing at thehighest levels and WebSim[YAHOO] producing thehighest of all measures on the Miller and Charles(1998) dataset.
(Unfortunately, Bollegala et-al(2007) do not compare their results with the Ru-benstein-Goodenough and 353-TC dataset.)
Inthe largest (353-TC) dataset, the WebSim[YAHOO]and WebSim[LIVE SEARCH] results were found to be,lower, but comparable with the best of Strubeand Ponzetto?s (2006) Wikipedia-based resultsand significantly higher than their Jaccard meas-ure, adapted to use Google web-search page-counts.284Set WordsA law, crime, perpetrator, prison, sentence, judge, jury, police, justice, criminalB astrology, ?star sign", Libra, Gemini, Sagittarius, zodiac, constellation, Leo, Scorpio, birthdayC ocean, "hydrothermal vent", fauna, botanical, biogeography, tube-worm, Atlantic, species, biology, habitatD economy, credit, bank, inflation, "interest rate", finance, capital, expenditure, market, profitE health, diet, swimming, fitness, exercise, "heart disease", stroke, fruit, jogging, workF football, stadium, striker, trophy, match, referee, pitch, crowd, manager, kickG education, exam, teacher, timetable, classroom, pupils, homework, results, parents, teenagerH country, election, president, vote, leader, population, opposition, party, government, countJ computer, "hard drive", keyboard, connection, monitor, RAM, speaker, "mother board", CPU, internetK "film star", drugs, money, millionaire, fame, music, actor, debut, beauty, paparazziM "house prices", monthly, mortgage, home, borrowing, "buy-to-let", figures, homeowners, lending, trendN inmates, overcrowding, prisoners, custody, release, warden, cell, bars, violence, detentionTable 4: Manually selected test sets4.2 Word-groupsIn order to indicate whether the proposed meas-ure is capable of measuring similarity amongst aset of words, a broad range of evaluation meth-ods is required.
No human evaluations exist, andto produce such a data-set would be difficult dueto the larger quantity of data to evaluate.
Thefollowing three methods are used:Manual Selection of Word-groupsThe manual selection of word-groups, for testingthe proposed measure is an important part of itsevaluation as it is possible to construct word-setsof varying similarity, so that expected results canbe predicted and then compared with actual re-sults.
In addition, the datasets created can be eas-ily manipulated to range from highly homoge-nous / similar sets of words to extremely hetero-geneous ?
where words are highly unlikely toappear together.
The latter is achieved by sys-tematically merging the groups together, until acomplete mixture of words from different word-sets is produced.The method used to compile the words groupsis as follows: firstly, groups of words with aknown propensity to appear together were se-lected by browsing popular internet web-sites(see Table 4 for the words contained in each set).Secondly for each of these original sets, a seriesof 5 measures is taken, the first with all wordsfrom the original set (to illustrate, this might berepresented as AAAAAAAAAA ?
where eachletter represent a word from the set shown), thistherefore is the most homogeneous group.
Thentwo words from this set are replaced with twofrom a new set (eg B) (AAAAAAAABB).
Thena further two words (again originally from set A)are replaced with two more words from anothernew set (eg from Set C) (AAAAAABBCC), andso on, until the final set of 10 words to be meas-ured consists of 5 pairs of words, each from 5different sets (AABBCCDDEE).
These stepswere first performed for sets A, B, C, D and Eand then for sets F, G, H, J and K respectively.The results were compared (using Pearson?s cor-relation) against the expected results.
For exam-ple: AAAAAAAAAA = 10 points,AAAAAAAABB = 8 points, AAAAAABBCC =6 points, AAAABBCCDD = 4 points andAABBCCDDEE = 2 points.On the whole, the word-groups contained ineach of these two sets of sets are considered to beheterogeneous (eg A is dissimilar to B, C, D andE etc).
To introduce more ?blurring?
of these cat-egories, a third set of sets was measured, consist-ing of the word-sets A, D, H, M and N. It wasconsidered more conceivable that the words inthese sets could be found in the same documents.The expected results for this set of measureswere modified slightly to become:AAAAAAAAAA = 8 points, AAAAAAAABB= 7 points, AAAAAABBCC = 6 points,AAAABBCCDD = 5 points andAABBCCDDEE = 4 points.
This was done toreflect the fact that the differences between thehighest and lowest were expected to be less.MeasureCorrelation (Pearson?s R)Heteroge-neous Setsonly:ABCDEFGHJKHomogenousSets Included:ABCDEFGHJK+ADHMNWebSim[YAHOO] -.80** -.68**WebSim[LIVE-SEARCH] -.65** -.57**WebSim[GOOGLE] -.70** -.64****Significant at the .01 level (2-tailed)Table 5: Correlation with expected scores formanually selected sets285As illustrated in Table 5, the ?HeterogeneousSets?
group similarity scores were found to corre-late very highly with those expected, with theWebSim[YAHOO] measure achieving .80.
The Web-Sim[GOOGLE] measure was also found to improvewhen tested on groups of words.The measures were found to perform less wellwhen the third set of word-sets, containing moreclosely related members, was introduced (Table5, final column).
This indicates that the measuresare better at distinguishing between highly ho-mogeneous and highly heterogeneous word-sets,but appear less proficient at distinguishing be-tween word-sets with moderate levels of topichomogeneity.WordNet::Domains Selection of Word-groupsThe WordNet Domains package2 (Magnini andCavaglia 2000) assigns domains to each sense ofthe WordNet electronic dictionary.
Therefore, foreach domain a relevant list of words can be ex-tracted.
The domains are arranged hierarchically,allowing sets of words with a varied degree oftopic homogeneity to be selected.
For example,for a highly heterogeneous set, 10 words can beselected from any domain, including factotum(level-0: the non-topic related category).
For aslightly less heterogeneous set, words might beselected randomly from a level-1 category (eg?Applied_Science?
), and any of the categories itsubsumes (eg Agriculture, Architecture, Build-ings etc).
The levels range from level-0 (facto-tum) to level-4; we merge levels 3 and 4 as level-4 domains are relatively few and are viewed assimilar to level-3.
This combined set is hence-forth known as level-3.Measure Correlation (Pearson?s R)All Extreme OnlyWebSim[YAHOO] -0.46** -0.80**WebSim[LIVE-SEARCH] -0.06 -0.23**WebSim[GOOGLE] -0.42** -0.71****Significant at the .01 level (2-tailed)Table 6: Correlation with expected scores forWordNet::Domains selected setsFor our experiments, we collected 2 randomsamples of 10 words for every WordNet domain(167 domains) and then increased the number ofsets from level-0 to level-2 domains, to make thenumber of sets from each level more similar.
Thefinal level counts are: levels 0 to 2 have 100word-sets each and level 3 has 192 word-sets.The word-sets contain 10 words each.
We then2 We use version 3.2 released Feb 2007assign an expected score to each set, equal to itsdomain level.Table 6 displays the resulting correlation be-tween the WebSim scores and the expected scores(column: ?All?).
In the previous section, we ob-served that the measures are less competent atdistinguishing between moderate levels of ho-mogeneity, and the WordNet::Domain test setscontain many sets which could be described ashaving moderate homogeneity.
To further testthis, we repeated the above WordNet::Domaintests, but included only those sets of level-0 andlevel-3.
The results displayed in the final columnof Table 6 and provide further evidence that thismight be the case, as the correlations are signifi-cantly higher for these more extreme test sets.SENSEVAL 2 & 3 DataThe selection of the most frequent words fromnatural language documents is another importantpart of our evaluation, as it is representative ofreal-world data-sets.
As it is anticipated that ourresults could be of use for WSD, we opted tomeasure the topic homogeneity of a publiclyavailable set of documents from a well estab-lished WSD competition.
The documents of theSENSEVAL 2 & 33 English all-words WSD taskwere divided into 73 sub-documents, each con-taining approximately 50 content-words.
Thestop-words and non-topical 4  content words ofeach sub-document were then removed and theremaining words lemmatised and aggregated bylemma.
This list of lemmas was then arranged indescending order of the number of occurrences inthe sub-document.
The top-10 most frequentlemmas were selected as input to the similaritymeasure.
The results for each set alng with de-scriptions of the documents used are displayed inTable 7.The scientific documents (d01) could beviewed as the most homogeneous, with theearthquake accounts (d002) and the work of fic-tion (d000) being considered the most heteroge-neous.
With this in mind, it is evident in Table 7that the WebSim[MSN] measure performed the leastwell and other two methods performed as ex-pected, and with identical rankings.Further analysis is required to compare theseresults with the WSD results for the same3 See http://www.senseval.org/4 Non-topical words are those that are found in over 25% ofSemcor documents or have their correct sense(s) belongingto the ?Factotum?
category in the WordNet Domains pack-age by Magnini and Cavaglia (2000).286Text Description AverageWebSim[YAHOO]AverageWebSim[GOOGLE]AverageWebSim[MSN]d00 Change-Ringing: ?
History of  Campanology, Churches, Social HistoryTypical set = {bell, church, tower, "change ringing", English, peculiar-ity, rest, stone, faithful, evensong}1.52 (4) 1.10 (4) 1.11 (5)d01 Cancer Cell Research: Typical Set = {cancer, gene, parent, protein,cell, growth, "suppressor gene", individual, scientist, beneficiary}1.19 (1) .93 (1) .89 (2)d02 Education: Typical Set = {child, belief, nature, parent, education, me-dium, politician, "elementary school", creativity}1.36 (3) 1.05 (3) 1.03 (3)d000 Fiction: Typical set = {stranger, drinking, occasion, street, "movingvan", intersection, brake, guy, mouth, truck}1.55 (5) 1.18 (5) 1.03 (3)d001 US Presidential Elections: Typical set = {district, candidate, half-century, election, percentage, president, vote, hopeful, reminder, ticket}1.29 (2) .96 (2) .85 (1)d002 California Earthquake ?
First hand accounts / storiesTypical Set = {computer, quake, subscriber, earthquake, resident, sol-ace, "personal computer", hundred, "check in", "bulletin board"}1.63 (6) 1.32 (6) 1.13 (6)Rankings for each measure are shown in bracketsTable 7: Average group similarity measures of the SENSEVAL 2 and 3 datasetsdocuments, as performed by Gledson and Keane(2008), and this is highlighted as part of the fur-ther work.5 ConclusionsWe proposed a simple web-based similaritymeasure which relies on page-counts only, canbe utilized to measure the similarity of entire setsof words in addition to word-pairs and can useany web-service enabled search engine.
Whenused to measure similarities between two words,our technique is comparable with other state-of-the art web-search page-counting techniques (andoutperforms most).
The measure is found to cor-relate to a moderate level (the highest being .60correlation) with human judgments.
When usedto measure similarities between sets of 10 words,the results are similarly encouraging and showthe expected variations for word-groups withdifferent levels of homogeneity.
Where word-groups are manually created, with known expec-tations of the similarities between each word-set,correlations with these expected results are ashigh as .80.
Noticeable differences between theperformances of each of the 3 search-engines, foreach evaluation method, are evident.
Google per-forms poorly for the word-pair similarity meas-ures and Yahoo and Live Search both performsubstantially better.
For the word-set compari-sons, Google performance improves (perhaps asthe erratic single counts are stabilised as largersets of words are used), but Yahoo is again supe-rior and MSN performs much less well.
Overall,our results indicate that the Yahoo measure is themost consistent and reliable.6 Further WorkThe group similarity measure calculates the pro-pensity of words to co-occur with one-another,which can be described as a measure of the topichomogeneity of the set of input words.
Gledsonand Keane (2008) propose the use of furthermeasures of topic homogeneity using a variety ofavailable resources.
These measures are com-pared with the results of WSD approaches relianton topic features and low to moderate correla-tions are found to exist.
It is also proposed thatuseful correlations with other NLP techniquesutilising topical features (such as TAD andMalapropism Detection) might also exist.The word-group similarity measures per-formed better for extreme levels of topic homo-geneity.
The measures must be improved to en-able them to distinguish between moderate ho-mogeneity levels.
This may be achieved by com-bining our simple measure with other word simi-larity/relatedness techniques such as the use ofWordNet Domains, or Lexical Chaining.It is expected that polysemy counts of wordsinfluence the outcome of these experiments, es-pecially for the word-pairs which have less dataand are more susceptible to erratic counts.
Re-sults might be improved by measuring and off-setting these effects.In addition, an upper limit of word-set cardi-nality should be determined, which is the maxi-mum number of input words that can be meas-ured.
Further testing is necessary using a range ofset cardinalities, to obtain optimal values.287ReferencesBollegala, D., Matsuo, Y., Ishizuka, M., 2007.
Meas-uring Semantic Similarity between Words UsingWeb Search Engines.
In Proceedings of World-Wide-Web Conference 2007 (Track: SemanticWeb), Banff, Alberta, Canada.
pp.
757-766.Budanitsky, A. and Hirst, G. 2006.
Evaluating Word-Net-based Measures of Lexical Semantic Related-ness.
Computational Linguistics , 32(1), pp.
13--47.Caracciolo, C., Willem van Hage and Maarten deRijke, 2004.
Towards Topic Driven Access to FullText Documents, in Research and Advanced Tech-nology for Digital Libraries, LNCS, 3232, pp 495-500Chen, Hsin-Hsi, Ming-Shun Lin and Yu-Chuan Wei,2006.
Novel Association Measures Using WebSearch with Double Checking.
In Proc.s 21st Intl.Conference on Computational Linguistics and 44thAnnual Meeting of the ACL, pp.
1009-1016Finkelstein, Lev, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin, 2002.
Placing Search in Context: TheConcept Revisited, ACM Transactions on Informa-tion Systems,  20(1), pp.
116-131, JanuaryGabrilovich, Evgeniy and Shaul Markovitch, 2007.Computing Semantic Relatedness using Wikipedia-based Explicit Semantic Analysis.
In Proc.s Intl.Joint Conference on Artificial Intelligence 2007,Hyderabad, India.Gledson, Ann and John Keane.
2008.
Measuring topichomogeneity and its application to word sense dis-ambiguation.
In Procs 22nd Intl Conference onComputational Linguistics (COLING), Manchester.
(To Appear)Gliozzo, Alfio, Carlo Strapparava and Ido Dagan,2004.
Unsupervised and Supervised Exploitation ofSemantic Domains in Lexical Disambiguation,Computer Speech and LanguageHughes, T. and D. Ramage, 2007.
Lexical SemanticRelatedness with Random Graph Walks, In Proc.sof the 2007 Joint Conference on Empirical Meth-ods in Natural Language Processing and Compu-tational Natural Language Learning, pp.
581-589,Prague.Keller, F. and Lapata, M., 2003.
Using the Web toObtain Frequencies for Unseen Bigrams, Computa-tional Linguistics, 29(3)Kilgarriff, Adam, 2007.
Googleology is Bad Science.Computational Linguistics, 33(1), pp.
147?151.Kilgarriff, A and Grefenstette, G., 2003.
Web as Cor-pus, In Introduction to the special issue on the webas corpus, Computational Linguistics, 29(3), pp.333--347Magnini, B. and Cavagli?, Gabriela.
2000.
IntegratingSubject Field Codes into WordNet.In Proceedings of LREC-2000, Athens, Greece,2000, pp 1413-1418.McCarthy, Diana, Rob Koeling, Julie Weeds and JohnCarroll, 2007.
Unsupervised Acquisition of Pre-dominant Word Senses, Computational Linguistics,33(4), pp.
553-590.Mihalcea, R and Moldovan, D., 1999.
A method forword sense disambiguation of unrestricted txt.
InProceedings of the 37th Meeting of ACL, pp 152-158Mihalcea, Rada, Courtney Corley and Carlo Strappa-rava, 2006.
Corpus-based and Knowledge-basedMeasures of Text Semantic Similarity, In Proc.s ofAmerican Association for Artificial Intelligence2006Miller, G. and Charles, W., 1998.
Contextual corre-lates of semantic similarity.
Language and Cogni-tive Processes, 6(1), pp.
1--28.Morris, J. and Hirst, G., 1991.
Lexical CohesionComputed by Thesaural Relations as an Indicatorof the Structure of Text, Computational Linguis-tics, 17(1), pp.
21-48Nakov, P. and Hearst, M. 2005.
Search Engine Statis-tics Beyond the n-gram: Application to NounCompound Bracketing, In Proceedings of the 9thConference on CoNLL, pp.
17--24, Ann Arbor,JuneNavigli, Roberto, Paola Velardi, 2005.
Structural Se-mantic Interconnections: A Knowledge-Based Ap-proach to Word Sense Disambiguation, IEEETransactions on Pattern Analysis and Machine In-telligence, 27(7),  pp.
1075-1086,  July.Patwardhan, S., Banerjee, S. and Pedersen, T., (2003).Using Measures of Semantic Relatedness for WordSense Disambiguation, LNCS 2588 - CICLing2003, pp.
241--257.Pedersen, Ted, Siddharth Patwardhan and Jason Mi-chelizzi, 2004.
WordNet::Similarity ?
Measuringthe Relatedness of Concepts, In Proc.s 19th Nation-al Conference on Artificial Intelligence.Rubenstein, H. and Goodenough, J., 1965.
Contextualcorrelates of synonymy.
Communications of theACM, 8, pp.
627--633.Strube, Michael and Paolo Ponzetto, 2006.
WikiRe-late!
Computing Semantic Relatedness Using Wi-kipedia, In Proc.s of American Association for Ar-tificial Intelligence 2006Weeds, Julie and, David Weir, 2006.
Co-occurrenceRetrieval: A Flexible Framework for Lexical Dis-tributional Similarity.
Computational Linguistics,31(4), pp 433-475.288
