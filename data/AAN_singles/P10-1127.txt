Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1250?1258,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsGenerating image descriptions using dependency relational patternsAhmet AkerUniversity of Sheffielda.aker@dcs.shef.ac.ukRobert GaizauskasUniversity of Sheffieldr.gaizauskas@dcs.shef.ac.ukAbstractThis paper presents a novel approachto automatic captioning of geo-taggedimages by summarizing multiple web-documents that contain information re-lated to an image?s location.
The summa-rizer is biased by dependency pattern mod-els towards sentences which contain fea-tures typically provided for different scenetypes such as those of churches, bridges,etc.
Our results show that summaries bi-ased by dependency pattern models leadto significantly higher ROUGE scores thanboth n-gram language models reported inprevious work and also Wikipedia base-line summaries.
Summaries generated us-ing dependency patterns also lead to morereadable summaries than those generatedwithout dependency patterns.1 IntroductionThe number of images tagged with location infor-mation on the web is growing rapidly, facilitatedby the availability of GPS (Global Position Sys-tem) equipped cameras and phones, as well as bythe widespread use of online social sites.
The ma-jority of these images are indexed with GPS coor-dinates (latitude and longitude) only and/or haveminimal captions.
This typically small amount oftextual information associated with the image is oflimited usefulness for image indexing, organiza-tion and search.
Therefore methods which couldautomatically supplement the information avail-able for image indexing and lead to improved im-age retrieval would be extremely useful.Following the general approach proposed byAker and Gaizauskas (2009), in this paper wedescribe a method for automatic image caption-ing or caption enhancement starting with only ascene or subject type and a set of place names per-taining to an image ?
for example ?church, {St.Paul?s,London}?.
Scene type and place names canbe obtained automatically given GPS coordinatesand compass information using techniques such asthose described in Xin et al (2010) ?
that task isnot the focus of this paper.Our method applies only to images of static fea-tures of the built or natural landscape, i.e.
objectswith persistent geo-coordinates, such as buildingsand mountains, and not to images of objects whichmove about in such landscapes, e.g.
people, cars,clouds, etc.
However, our technique is suitable notonly for image captioning but in any applicationcontext that requires summary descriptions of in-stances of object classes, where the instance is tobe characterized in terms of the features typicallymentioned in describing members of the class.Aker and Gaizauskas (2009) have argued thathumans appear to have a conceptual model ofwhat is salient regarding a certain object type (e.g.church, bridge, etc.)
and that this model informstheir choice of what to say when describing an in-stance of this type.
They also experimented withrepresenting such conceptual models using n-gramlanguage models derived from corpora consistingof collections of descriptions of instances of spe-cific object types (e.g.
a corpus of descriptions ofchurches, a corpus of bridge descriptions, and soon) and reported results showing that incorporat-ing such n-gram language models as a feature in afeature-based extractive summarizer improves thequality of automatically generated summaries.The main weakness of n-gram language mod-els is that they only capture very local informationabout short term sequences and cannot model longdistance dependencies between terms.
For exam-ple one common and important feature of objectdescriptions is the simple specification of the ob-ject type, e.g.
the information that the object Lon-don Bridge is a bridge or that the Rhine is a river.If this information is expressed as in the first lineof Table 1, n-gram language models are likely to1250Table 1: Example of sentences which express the type of an object.London Bridge is a bridge...The Rhine (German: Rhein; Dutch: Rijn; French: Rhin; Romansh: Rain;Italian: Reno; Latin: Rhenus West Frisian Ryn) is one of the longest andmost important rivers in Europe...reflect it, since one would expect the tri-gram is abridge to occur with high frequency in a corpus ofbridge descriptions.
However, if the type predica-tion occurs with less commonly seen local context,as is the case for the object Rhine in the secondrow of Table 1 ?
most important rivers ?
n-gramlanguage models may well be unable to identify it.Intuitively, what is important in both these casesis that there is a predication whose subject is theobject instance of interest and the head of whosecomplement is the object type: London Bridge ...is ... bridge and Rhine ... is ... river.
Sentencesmatching such patterns are likely to be importantones to include in a summary.
This intuition sug-gests that rather than representing object type con-ceptual models via corpus-derived language mod-els as do Aker and Gaizauskas (2009), we do so in-stead using corpus-derived dependency patterns.We pursue this idea in this paper, our hy-pothesis being that information that is importantfor describing objects of a given type will fre-quently be realized linguistically via expressionswith the same dependency structure.
We explorethis hypothesis by developing a method for deriv-ing common dependency patterns from object typecorpora (Section 2) and then incorporating thesepatterns into an extractive summarization system(Section 3).
In Section 4 we evaluate the approachboth by scoring against model summaries and viaa readability assessment.
Since our work aims toextend the work of Aker and Gaizauskas (2009)we reproduce their experiments with n-gram lan-guage models in the current setting so as to permitaccurate comparison.Multi-document summarizers face the problemof avoiding redundancy: often, important infor-mation which must be included in the summaryis repeated several times across the document set,but must be included in the summary only once.We can use the dependency pattern approach toaddress this problem in a novel way.
The com-mon approach to avoiding redundancy is to use atext similarity measure to block the addition of afurther sentence to the summary if it is too simi-lar to one already included.
Instead, since specificdependency patterns express specific types of in-Table 2: Object types and the number of articles in each object type cor-pus.
Object types which are bold are covered by the evaluation image set.village 39970, school 15794, city 14233, organization 9393, university7101, area 6934, district 6565, airport 6493, island 6400, railway station5905, river 5851, company 5734, mountain 5290, park 3754, college 3749,stadium 3665, lake 3649, road 3421, country 3186, church 3005, way2508, museum 2320, railway 2093, house 2018, arena 1829, field 1731,club 1708, shopping centre 1509, highway 1464, bridge 1383, street 1352,theatre 1330, bank 1310, property 1261, hill 1072, castle 1022, forest 995,court 949, hospital 937, peak 906, bay 899, skyscraper 843, valley 763, ho-tel 741, garden 739, building 722, market 712, monument 679, port 651,sea 645, temple 625, beach 614, square 605, store 547, campus 525, palace516, tower 496, cemetery 457, volcano 426, cathedral 402, glacier 392,residence 371, dam 363, waterfall 355, gallery 349, prison 348, cave 341,canal 332, restaurant 329, path 312, observatory 303, zoo 302, coast 298,statue 283, venue 269, parliament 258, shrine 256, desert 248, synagogue236, bar 229, ski resort 227, arch 223, landscape 220, avenue 202, casino179, farm 179, seaside 173, waterway 167, tunnel 167, ruin 166, chapel 165,observation wheel 158, basilica 157, woodland 154, wetland 151, cinema144, gate 142, aquarium 136, entrance 136, opera house 134, spa 125,shop 124, abbey 108, boulevard 108, pub 92, bookstore 76, mosque 56formation we can group the patterns into groupsexpressing the same type of information and then,during sentence selection, ensure that sentencesmatching patterns from different groups are se-lected in order to guarantee broad, non-redundantcoverage of information relevant for inclusion inthe summary.
We report work experimenting withthis idea too.2 Representing conceptual models2.1 Object type corporaWe derive n-gram language and dependency pat-tern models using object type corpora made avail-able to us by Aker and Gaizauskas.
Aker andGaizauskas (2009) define an object type corpus asa collection of texts about a specific static objecttype such as church, bridge, etc.
Objects can benamed locations such as Eiffel Tower.
To refer tosuch names they use the term toponym.
To buildsuch object type corpora the authors categorizedWikipedia articles places by object type.
The ob-ject type of each article was identified automati-cally by running Is-A patterns over the first fivesentences of the article.
The authors report 91%accuracy for their categorization process.
Themost populated of the categories identified (in to-tal 107 containing articles about places around theworld) are shown in Table 2.2.2 N-gram language modelsAker and Gaizauskas (2009) experimented withuni-gram and bi-gram language models to capturethe features commonly used when describing anobject type and used these to bias the sentence se-lection of the summarizer towards the sentencesthat contain these features.
As in Song and Croft(1999) they used their language models in a gener-1251ative way, i.e.
they calculate the probability that asentence is generated based on a n-gram languagemodel.
They showed that summarizer biased withbi-gram language models produced better resultsthan those biased with uni-gram models.
We repli-cate the experiments of Aker and Gaizauskas andgenerate a bi-gram language model for each objecttype corpus.
In later sections we use LM to referto these models.2.3 Dependency patternsWe use the same object type corpora to derivedependency patterns.
Our patterns are derivedfrom dependency trees which are obtained usingthe Stanford parser1.
Each article in each ob-ject type corpus was pre-processed by sentencesplitting and named entity tagging2.
Then eachsentence was parsed by the Stanford dependencyparser to obtain relational patterns.
As with thechain model introduced by Sudo et al (2001) ourrelational patterns are concentrated on the verbsin the sentences and contain n+1 words (the verband n words in direct or indirect relation with theverb).
The number n is experimentally set to twowords.For illustration consider the sentence shown inTable 3 that is taken from an article in the bridgecorpus.
The first two rows of the table show theoriginal sentence and its form after named entitytagging.
The next step in processing is to replaceany occurrence of a string denoting the object typeby the term ?OBJECTTYPE?
as shown in the thirdrow of Table 3.
The final two rows of the tableshow the output of the Stanford dependency parserand the relational patterns identified for this ex-ample.
To obtain the relational patterns from theparser output we first identified the verbs in theoutput.
For each such verb we extracted two fur-ther words being in direct or indirect relation to thecurrent verb.
Two words are directly related if theyoccur in the same relational term.
The verb built-4,for instance, is directly related to DATE-6 becausethey both are in the same relational term prep-in(built-4, DATE-6).
Two words are indirectly re-lated if they occur in two different terms but arelinked by a word that occurs in those two terms.The verb was-3 is, for instance, indirectly relatedto OBJECTTYPE-2 because they are both in dif-ferent terms but linked with built-4 that occurs in1http://nlp.stanford.edu/software/lex-parser.shtml2For performing shallow text analysis the OpenNLP tools(http://opennlp.sourceforge.net/) were used.Table 3: Example sentence for dependency pattern.Original sentence: The bridge was built in 1876 by W. W.After NE tagging: The bridge was built in DATE by W. W.Input to the parser: The OBJECTTYPE was built in DATE by W. W.Output of the parser: det(OBJECTTYPE-2, The-1), nsubjpass(built-4, OBJECTTYPE-2), auxpass(built-4, was-3), prep-in(built-4, DATE-6),nn(W-10, W-8), agent(built-4, W-10)Patterns: The OBJECTTYPE built, OBJECTTYPE was built, OBJECT-TYPE built DATE, OBJECTTYPE built W, was built DATE, was built Wboth terms.
E.g.
for the term nsubjpass(built-4,OBJECTTYPE-2) we use the verb built and ex-tract patterns based on this.
OBJECTTYPE is indirect relation to built and The is in indirect rela-tion to built through OBJECTTYPE.
So a patternfrom these relations is The OBJECTTYPE built.The next pattern extracted from this term is OB-JECTTYPE was built.
This pattern is based on di-rect relations.
The verb built is in direct relationto OBJECTTYPE and also to was.
We continuethis until we cover all direct relations with built re-sulting in two more patterns (OBJECTTYPE builtDATE and OBJECTTYPE built W).
It should benoted that we consider all direct and indirect rela-tions while generating the patterns.Following these steps we extracted relationalpatterns for each object type corpus along with thefrequency of occurrence of the pattern in the en-tire corpus.
The frequency values are used by thesummarizer to score the sentences.
In the follow-ing sections we will use the term DpM to refer tothese dependency pattern models.2.3.1 Pattern categorizationIn addition to using dependency patterns as mod-els for biasing sentence selection, we can also usethem to control the kind of information to be in-cluded in the final summary (see Section 3.2).
Wemay want to ensure that the summary containsa sentence describing the object type of the ob-ject, its location and some background informa-tion.
For example, for the object Eiffel Tower weaim to say that it is a tower, located in Paris, de-signed by Gustave Eiffel, etc.
To be able to doso, we categorize dependency patterns accordingto the type of information they express.We manually analyzed human written descrip-tions about instances of different object types andrecorded for each sentence in the descriptions thekind of information it contained about the object.We analyzed descriptions of 310 different objectswhere each object had up to four different humanwritten descriptions (Section 4.1).
We categorizedthe information contained in the descriptions into1252the following categories:?
type: sentences containing the ?type?
information ofthe object such as XXX is a bridge?
year: sentences containing information about when theobject was built or in case of mountains, for instance,when it was first climbed?
location: sentences containing information aboutwhere the object is located?
background: sentences containing some specific in-formation about the object?
surrounding: sentences containing information aboutwhat other objects are close to the main object?
visiting: sentences containing information about e.g.visiting times, etc.We also manually assigned each dependencypattern in each corpus-derived model to one of theabove categories, provided it occurred five or moretimes in the object type corpora.
The patterns ex-tracted for our example sentence shown in Table 3,for instance, are all categorized by year categorybecause all of them contain information about thefoundation date of an object.3 SummarizerWe adopted the same overall approach to sum-marization used by Aker and Gaizauskas (2009)to generate the image descriptions.
The summa-rizer is an extractive, query-based multi-documentsummarization system.
It is given two inputs: atoponym associated with an image and a set ofdocuments to be summarized which have been re-trieved from the web using the toponym as a query.The summarizer creates image descriptions in athree step process.
First, it applies shallow textanalysis, including sentence detection, tokeniza-tion, lemmatization and POS-tagging to the giveninput documents.
Then it extracts features fromthe document sentences.
Finally, it combines thefeatures using a linear weighting scheme to com-pute the final score for each sentence and to cre-ate the final summary.
We modified the approachto feature extraction and the way the summarizeracquires the weights for feature combination.
Thefollowing subsections describe how feature extrac-tion/combination is done in more detail.3.1 Feature ExtractionThe original summarizer reported in Aker andGaizauskas (2009) uses the following features toscore the sentences:?
querySimilarity: Sentence similarity to the query (to-ponym) (cosine similarity over the vector representa-tion of the sentence and the query).?
centroidSimilarity: Sentence similarity to the centroid.The centroid is composed of the 100 most frequentlyoccurring non stop words in the document collection(cosine similarity over the vector representation of thesentence and the centroid).?
sentencePosition: Position of the sentence within itsdocument.
The first sentence in the document gets thescore 1 and the last one gets 1n where n is the numberof sentences in the document.?
starterSimilarity: A sentence gets a binary score if itstarts with the query term (e.g.
Westminster Abbey, TheWestminster Abbey, The Westminster or The Abbey) orwith the object type, e.g.
The church.
We also allowgaps (up to four words) between the and the query tocapture cases such as The most magnificent Abbey, etc.?
LMSim3: The similarity of a sentence S to an n-gramlanguage model LM (the probability that the sentenceS is generated by LM).In our experiments we extend this feature set bytwo dependency pattern related features: DpMSimand DepCat.DpMSim is computed in a similar fashion toLMSim feature.
We assign each sentence a depen-dency similarity score.
To compute this score, wefirst parse the sentence on the fly with the Stan-ford parser and obtain the dependency patterns forthe sentence.
We then associate each dependencypattern of the sentence with the occurrence fre-quency of that pattern in the dependency patternmodel (DpM).
DpMSim is then computed as givenin Equation 1.
It is a sum of all occurrence fre-quencies of the dependency patterns detected in asentence S that are also contained in the DpM.DpMSim(S,DpM) =?p?SfDpM (p) (1)The second feature, DepCat, uses dependencypatterns to categorize the sentences rather thanranking them.
It can be used independently fromother features to categorize each sentence by oneof the categories described in Section 2.3.1.
To dothis, we obtain the relational patterns for the cur-rent sentence, check whether for each such patternwhether it is included in the DpM, and, if so, weadd to the sentence the category the pattern wasmanually associated with.
It should be noted thata sentence can have more than one category.
Thiscan occur, for instance, if the sentence contains in-formation about when something was built and atthe same time where it is located.
It is also impor-tant to mention that assigning sentences categoriesdoes not change the order in the ranked list.We use DepCat to generate an automated sum-mary by first including sentences containing thecategory ?type?, then ?year?
and so on until the3In Aker and Gaizauskas (2009) this feature is called mod-elSimilarity.1253summary length is violated.
The sentences are se-lected according to the order in which they occurin the ranked list.
From each of the first three cat-egories (?type?, ?year?
and ?location?)
we take asingle sentence to avoid redundancy.
The same isapplied to the final two categories (?surrounding?and ?visiting?).
Then, if length limit is not vio-lated, we fill the summary with sentences from the?background?
category until the word limit of 200words is reached.
Here the number of added sen-tences is not limited.
Finally, we order the sen-tences by first adding the sentences from the firstthree categories to the summary, then the ?back-ground?
related sentences and finally the last twosentences from the ?surrounding?
and ?visiting?categories.
However, in cases where we have notreached the summary word limit because of un-covered categories, i.e.
there were not, for in-stance, sentences about ?location?, we add to theend of the summary the next top sentence from theranked list that was not taken.3.2 Sentence SelectionTo compute the final score for each sentence Akerand Gaizauskas (2009) use a linear function withweighted features:Sscore = (n?i=1featurei ?
weighti) (2)We use the same approach, but whereas the fea-ture weights they use are experimentally set ratherthan learned, we learn the weights using linear re-gression instead.
We used 23 of the 310 imagesfrom our image set (see Section 4.1) to train theweights.
The image descriptions from this data setare used as model summaries.Our training data contains for each image aset of image descriptions taken from the Virtual-Tourist travel community web-site 4.
From thisweb-site we took all existing image descriptionsabout a particular image or object.
Note that someof these descriptions about a particular object wereused to derive the model summaries for that ob-ject (see Section 4.1).
Assuming that model sum-maries contain the most relevant sentences aboutan object we perform ROUGE comparisons be-tween the sentences in all the image descriptionsand the model summaries, i.e.
we pair each sen-tence from all image descriptions about a particu-lar place with every sentence from all the model4www.virtualtourist.comsummaries for that particular object.
Sentenceswhich are exactly the same or have common partswill score higher in ROUGE than sentences whichdo not have anything in common.
In this way, wehave for each sentence from all existing image de-scriptions about an object a ROUGE score5 indi-cating its relevance.
We also ran the summarizerfor each of these sentences to compute the valuesfor the different features.
This gives informationabout each feature?s value for each sentence.
Thenthe ROUGE scores and feature score values for ev-ery sentence were input to the linear regression al-gorithm to train the weights.Given the weights, Equation 2 is used to com-pute the final score for each sentence.
The finalsentence scores are used to sort the sentences inthe descending order.
This sorted list is then usedby the summarizer to generate the final summaryas described in Aker and Gaizauskas (2009).4 EvaluationTo evaluate our approach we used two different as-sessment methods: ROUGE (Lin, 2004) and man-ual readability.
In the following we first describethe data sets used in each of these evaluations, andthen we present the results of each assessment.4.1 Data setsFor evaluation we use the image collection de-scribed in Aker and Gaizauskas (2010).
The imagecollection contains 310 different images with man-ually assigned toponyms.
The images cover 60of the 107 object types identified from Wikipedia(see Table 2).
For each image there are up tofour short descriptions or model summaries.
Themodel summaries were created manually based onimage descriptions taken from VirtualTourist andcontain a minimum of 190 and a maximum of 210words.
An example model summary about the Eif-fel Tower is shown in Table 4.
23 of this imagecollection was used to train the weights and theremaining 13 (105 images) for evaluation.To generate automatic captions for the im-ages we automatically retrieved the top 30 relatedweb-documents for each image using the Yahoo!search engine and the toponym associated with theimage as a query.
The text from these documentswas extracted using an HTML parser and passedto the summarizer.
The set of documents we usedto generate our summaries excluded any Virtual-Tourist related sites, as these were used to generate5We used ROUGE 1.1254Table 4: Model, Wikipedia baseline and starterSimilarity+LMSim+DepCat summary for Eiffel Tower.Model Summary Wikipedia baseline summary starterSimilarity+LMSim+DepCat summaryThe Eiffel Tower is the most famous place in Paris.
Itis made of 15,000 pieces fitted together by 2,500,000rivets.
It?s of 324 m (1070 ft) high structure andweighs about 7,000 tones.
This world famous land-mark was built in 1889 and was named after its de-signer, engineer Gustave Alexandre Eiffel.
It is nowone of the world?s biggest tourist places which is vis-ited by around 6,5 million people yearly.
There arethree levels to visit: Stages 1 and 2 which can bereached by either taking the steps (680 stairs) or thelift, which also has a restaurant ?Altitude 95?
and aSouvenir shop on the first floor.
The second floor alsohas a restaurant ?Jules Verne?.
Stage 3, which is atthe top of the tower can only be reached by using thelift.
But there were times in the history when Tour Eif-fel was not at all popular, when the Parisians thoughtit looked ugly and wanted to pull it down.
The Eif-fel Tower can be reached by using the Mtro throughTrocadro, Ecole Militaire, or Bir-Hakeim stops.
Theaddress is: Champ de Mars-Tour Eiffel.The Eiffel Tower (French: Tour Eiffel, [tur efel])is a 19th century iron lattice tower located on theChamp de Mars in Paris that has become both aglobal icon of France and one of the most recog-nizable structures in the world.
The Eiffel Tower,which is the tallest building in Paris, is the singlemost visited paid monument in the world; millionsof people ascend it every year.
Named after its de-signer, engineer Gustave Eiffel, the tower was builtas the entrance arch for the 1889 World?s Fair.
Thetower stands at 324 m (1,063 ft) tall, about thesame height as an 81-story building.
It was thetallest structure in the world from its completionuntil 1930, when it was eclipsed by the ChryslerBuilding in New York City.
Not including broad-cast antennas, it is the second-tallest structure inFrance, behind the Millau Viaduct, completed in2004.
The tower has three levels for visitors.
Tick-ets can be purchased to ascend either on stairs orlifts to the first and second levels.The Eiffel Tower, which is the tallest building inParis, is the single most visited paid monument in theworld; millions of people ascend it every year.
Thetower is located on the Left Bank of the Seine River,at the northwestern extreme of the Parc du Champde Mars, a park in front of the Ecole Militaire thatused to be a military parade ground.
The tower wasmet with much criticism from the public when it wasbuilt, with many calling it an eyesore.
Counting fromthe ground, there are 347 steps to the first level, 674steps to the second level, and 1,710 steps to the smallplatform on the top of the tower.
Although it wasthe world?s tallest structure when completed in 1889,the Eiffel Tower has since lost its standing both asthe tallest lattice tower and as the tallest structure inFrance.
The tower has two restaurants: Altitude 95,on the first floor 311ft (95m) above sea level; andthe Jules Verne, an expensive gastronomical restau-rant on the second floor, with a private lift.Table 5: ROUGE scores for each single feature and Wikipedia baseline.Recall centroidSimilarity sentencePosition querySimilarity starterSimilarity LMSim DpMSim*** WikiR2 .0734 .066 .0774 .0869 .0895 .093 .097RSU4 .12 .11 .12 .137 .142 .145 .14the model summaries.4.2 ROUGE assessmentIn the first assessment we compared the automat-ically generated summaries against model sum-maries written by humans using ROUGE (Lin,2004).
Following the Document UnderstandingConference (DUC) evaluation standards we usedROUGE 2 (R2) and ROUGE SU4 (RSU4) as eval-uation metrics (Dang, 2006) .
ROUGE 2 gives re-call scores for bi-gram overlap between the auto-matically generated summaries and the referenceones.
ROUGE SU4 allows bi-grams to be com-posed of non-contiguous words, with a maximumof four words between the bi-grams.As baselines for evaluation we used two dif-ferent summary types.
Firstly, we generatedsummaries for each image using the top-rankednon Wikipedia document retrieved in the Yahoo!search results for the given toponyms.
From thisdocument we create a baseline summary by select-ing sentences from the beginning until the sum-mary reaches a length of 200 words.
As a secondbaseline we use the Wikipedia article for a giventoponym from which we again select sentencesfrom the beginning until the summary length limitis reached.First, we compared the baseline summariesagainst the VirtualTourist model summaries.
Thecomparison shows that the Wikipedia baselineROUGE scores (R2 .097***, RSU4 .14***) aresignificantly higher than the first document ones(R2 0.042, RSU4 .079) 6.
Thus, we will focuson the Wikipedia baseline summaries to draw con-clusions about our automatic summaries.
Table 4shows the Wikipedia baseline summary about theEiffel Tower.Secondly, we separately ran the summarizerover the top ten documents for each single featureand compared the automated summaries againstthe model ones.
The results of this comparisonare shown in Table 5.Table 5 shows that the dependency model fea-ture (DpMSim) contributes most to the summaryquality according to the ROUGE metrics.
It is alsosignificantly better than all other feature scoresexcept the LMSim feature.
Compared to LMSimROUGE scores the DpMSim feature offers only amoderate improvement.
The same moderate im-provement we can see between the DpMSim RSU4and the Wiki RSU4.
The lowest ROUGE scoresare obtained if only sentence position (sentecePo-sition) is used.To see how the ROUGE scores change whenfeatures are combined with each other we per-formed different combinations of the features,ran the summarizer for each combination andcompared the automated summaries against themodel ones.
In the different combinations we6To assess the statistical significance of ROUGE scoredifferences between multiple summarization results we per-formed a pairwise Wilcoxon signed-rank test.
We use thefollowing conventions for indicating significance level in thetables: *** = p < .0001, ** = p < .001, * = p < .05 and nostar indicates non-significance.1255Table 6: ROUGE scores of feature combinations which score moderatelyor significantly higher than dependency pattern model (DpMSim) feature andWikipedia baseline.Recall starterSimilarity+ LMSimstarterSimilarity+ LMSim + Dep-Cat***DpmSim WikiR2 .095 .102 .093 .097RSU4 .145 .155 .145 .14also included the dependency pattern categoriza-tion (DepCat) feature explained in Section 3.1.Table 6 shows the results of feature combinationswhich score moderately or significantly higherthan the dependency pattern model (DpMSim) fea-ture score shown in Table 5.The results showed that combining DpMSimwith other features did not lead to higher ROUGEscores than those produced by that feature alone.The summaries categorized by dependency pat-terns (starterSimilarity+LMSim+DepCat) achievesignificantly higher ROUGE scores than theWikipedia baseline.
For both ROUGE R2 andROUGE SU4 the significance is at level p <.0001.
Table 4 shows a summary about theEiffel Tower obtained using this starterSimilar-ity+LMSim+DepCat feature.
Table 5 also showsthe ROUGE scores of the feature combinationstarterSimilarity and LMSim used without the de-pendency categorization (DepCat) feature.
It canbe seen that this combination without the depen-dency patterns lead to lower ROUGE scores inROUGE 2 and only moderate improvement inROUGE SU4 if compared with Wikipedia base-line ROUGE scores.4.3 Readability assessmentWe also evaluated our summaries using a read-ability assessment as in DUC and TAC.
DUC andTAC manually assess the quality of automaticallygenerated summaries by asking human subjects toscore each summary using five criteria ?
gram-maticality, redundancy, clarity, focus and structurecriteria.
Each criterion is scored on a five pointscale with high scores indicating a better result(Dang, 2005).For this evaluation we used the same 105 im-ages as in the ROUGE evaluation.
As the ROUGEevaluation showed that the dependency patterncategorization (DepCat) renders the best resultswhen used in feature combination starterSimilar-ity + LMSim + DepCat, we further investigatedthe contribution of dependency pattern categoriza-tion by performing a readability assessment onsummaries generated using this feature combina-tion.
For comparison we also evaluated sum-maries which were not structured by dependencypatterns (starterSimilarity + LMSim) and also theWikipedia baseline summaries.We asked four people to assess the summaries.Each person was shown all 315 summaries (105from each summary type) in a random way andwas asked to assess them according to the DUCand TAC manual assessment scheme.
The resultsare shown in Table 7.We see from Table 7 that using dependency pat-terns to categorize the sentences and produce astructured summary helps to obtain better readablesummaries.
Looking at the 5 and 4 scores the ta-ble shows that the dependency pattern categorizedsummaries (SLMD) have better clarity (85% of thesummaries), are more coherent (74% of the sum-maries), contain less redundant information (83%of the summaries) and have better grammar (92%of the summaries) than the ones without depen-dency categorization (80%, 70%, 60%, 84%).The scores of our automated summaries werebetter than the Wikipedia baseline summaries inthe grammar feature.
However, in other featuresthe Wikipedia baseline summaries obtained betterscores than our automated summaries.
This com-parison show that there is a gap to fill in order toobtain better readable summaries.5 Related WorkOur approach has an advantage over related workin automatic image captioning in that it requiresonly GPS information associated with the image inorder to generate captions.
Other attempts towardsautomatic generation of image captions generatecaptions based on the immediate textual context ofthe image with or without consideration of imagerelated features such as colour, shape or texture(Deschacht and Moens, 2007; Mori et al, 2000;Barnard and Forsyth, 2001; Duygulu et al, 2002;Barnard et al, 2003; Pan et al, 2004; Feng and La-pata, 2008; Satoh et al, 1999; Berg et al, 2005).However, Marsch & White (2003) argue that thecontent of an image and its immediate text havelittle semantic agreement and this can, accordingto Purves et al (2008), be misleading to imageretrieval.
Furthermore, these approaches assumethat the image has been obtained from a document.In cases where there is no document associatedwith the image, which is the scenario we are prin-cipally concerned with, these techniques are notapplicable.1256Table 7: Readability evaluation results: Each cell shows the percentage of summaries scoring the ranking score heading the column for each criterion in therow as produced by the summary method indicated by the subcolumn heading ?
Wikipedia baseline (W), starterSimilarity + LMSim (SLM) and starterSimilarity +LMSim + DepCat (SLMD).
The numbers indicate the percentage values averaged over the four people.5 4 3 2 1Criterion W SLM SLMD W SLM SLMD W SLM SLMD W SLM SLMD W SLM SLMDclarity 72.6 50.5 53.6 21.7 30.0 31.4 1.2 6.7 5.7 4.0 10.2 6.0 0.5 2.6 3.3focus 72.1 49.3 51.2 20.5 26.0 25.2 3.8 10.0 10.7 3.3 10.0 10.5 0.2 4.8 2.4coherence 67.1 39.0 48.3 23.6 31.4 26.9 4.8 12.4 11.9 3.3 10.2 9.8 1.2 6.9 3.1redundancy 69.8 42.9 55.0 21.7 17.4 28.8 2.4 4.5 4.3 5.0 27.1 8.8 1.2 8.1 3.1grammar 48.6 55.7 62.9 32.9 29.0 30.0 5.0 3.1 1.9 11.7 12.1 5.2 1.9 0 0Dependency patterns have been exploited invarious language processing applications.
In in-formation extraction, for instance, dependencypatterns have been used to extract relevant in-formation from text resources (Yangarber et al,2000; Sudo et al, 2001; Culotta and Sorensen,2004; Stevenson and Greenwood, 2005; Bunescuand Mooney, 2005; Stevenson and Greenwood,2009).
However, dependency patterns have notbeen used extensively in summarization tasks.
Weare aware only of the work described in Nobata etal.
(2002) who used dependency patterns in com-bination with other features to generate extracts ina single document summarization task.
The au-thors found that when learning weights in a simplefeature weigthing scheme, the weight assigned todependency patterns was lower than that assignedto other features.
The small contribution of the de-pendency patterns may have been due to the smallnumber of documents they used to derive theirdependency patterns ?
they gathered dependencypatterns from only ten domain specific documentswhich are unlikely to be sufficient to capture re-peated features in a domain.6 Discussion and ConclusionWe have proposed a method by which dependencypatterns extracted from corpora of descriptions ofinstances of particular object types can be used in amulti-document summarizer to automatically gen-erate image descriptions.
Our evaluations showthat such an approach yields summaries whichscore more highly than an approach which uses asimpler representation of an object type model inthe form of a n-gram language model.When used as the sole feature for sentence rank-ing, dependency pattern models (DpMSim) pro-duced summaries with higher ROUGE scores thanthose obtained using the features reported in Akerand Gaizauskas (2009).
These dependency pat-tern models also achieved a modest improvementover Wikipedia baseline ROUGE SU4.
Further-more, we showed that using dependency patternsin combination with features reported in Aker andGaizauskas to produce a structured summary ledto significantly better results than Wikipedia base-line summaries as assessed by ROUGE.
However,human assessed readability showed that there isstill scope for improvement.These results indicate that dependency patternsare worth investigating for object focused auto-mated summarization tasks.
Such investigationsshould in particular concentrate on how depen-dency patterns can be used to structure informa-tion within the summary, as our best results wereachieved when dependency patterns were used forthis purpose.There are a number of avenues to pursue in fu-ture work.
One is to explore how dependency pat-terns could be used to produce generative sum-maries and/or perform sentence trimming.
An-other is to investigate how dependency patternsmight be automatically clustered into groups ex-pressing similar or related facts, rather than rely-ing on manual categorization of dependency pat-terns into categories such as ?type?, ?year?, etc.as was done here.
Evaluation should be extendedto investigate the utility of the automatically gen-erated image descriptions for image retrieval.
Fi-nally, we also plan to analyze automated ways forlearning information structures (e.g.
what is theflow of facts to describe a location) from existingimage descriptions to produce better summaries.7 AcknowlegmentThe research reported was funded by the TRIPODproject supported by the European Commissionunder the contract No.
045335.
We would liketo thank Emina Kurtic, Mesude Bicak, Edina Kur-tic and Olga Nesic for participating in our manualevaluation.
We also would like to thank TrevorCohn and Mark Hepple for discussions and com-ments.ReferencesA.
Aker and R. Gaizauskas.
2009.
Summary Gener-ation for Toponym-Referenced Images using Object1257Type Language Models.
International Conferenceon Recent Advances in Natural Language Process-ing (RANLP),2009.A.
Aker and R. Gaizauskas.
2010.
Model Summariesfor Location-related Images.
In Proc.
of the LREC-2010 Conference.K.
Barnard and D. Forsyth.
2001.
Learning the seman-tics of words and pictures.
In International Confer-ence on Computer Vision, volume 2, pages 408?415.Vancouver: IEEE.K.
Barnard, P. Duygulu, D. Forsyth, N. de Freitas,D.M.
Blei, and M.I.
Jordan.
2003.
Matching wordsand pictures.
The Journal of Machine Learning Re-search, 3:1107?1135.T.L.
Berg, A.C. Berg, J. Edwards, and DA Forsyth.2005.
Whos in the Picture?
In Advances in NeuralInformation Processing Systems 17: Proc.
Of The2004 Conference.
MIT Press.R.C.
Bunescu and R.J. Mooney.
2005.
A shortestpath dependency kernel for relation extraction.
InProceedings of the conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, pages 724?731.
Association forComputational Linguistics Morristown, NJ, USA.A.
Culotta and J. Sorensen.
2004.
Dependency TreeKernels for Relation Extraction.
In Proceedings ofthe 42nd Meeting of the Association for Compu-tational Linguistics (ACL?04), Main Volume, pages423?429, Barcelona, Spain, July.H.T.
Dang.
2005.
Overview of DUC 2005.
DUC 05Workshop at HLT/EMNLP.H.T.
Dang.
2006.
Overview of DUC 2006.
NationalInstitute of Standards and Technology.K.
Deschacht and M.F.
Moens.
2007.
Text Analy-sis for Automatic Image Annotation.
Proc.
of the45th Annual Meeting of the Association for Compu-tational Linguistics.
East Stroudsburg: ACL.P.
Duygulu, K. Barnard, JFG de Freitas, and D.A.Forsyth.
2002.
Object Recognition as MachineTranslation: Learning a Lexicon for a Fixed Im-age Vocabulary.
In Seventh European Conferenceon Computer Vision (ECCV), 4:97?112.X.
Fan, A. Aker, M. Tomko, P. Smart, M Sanderson,and R. Gaizauskas.
2010.
Automatic Image Cap-tioning From the Web For GPS Photographs.
InProc.
of the 11th ACM SIGMM International Con-ference on Multimedia Information Retrieval, Na-tional Constitution Center, Philadelphia, Pennsylva-nia.Y.
Feng and M. Lapata.
2008.
Automatic Image An-notation Using Auxiliary Text Information.
Proc.of Association for Computational Linguistics (ACL)2008, Columbus, Ohio, USA.C.Y.
Lin.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
Proc.
of the Workshopon Text Summarization Branches Out (WAS 2004),pages 25?26.E.E.
Marsh and M.D.
White.
2003.
A taxonomy ofrelationships between images and text.
Journal ofDocumentation, 59:647?672.Y.
Mori, H. Takahashi, and R. Oka.
2000.
Automaticword assignment to images based on image divisionand vector quantization.
In Proc.
of RIAO 2000:Content-Based Multimedia Information Access.C.
Nobata, S. Sekine, H. Isahara, and R. Grishman.2002.
Summarization system integrated with namedentity tagging and ie pattern discovery.
In Proc.
ofthe LREC-2002 Conference, pages 1742?1745.J.Y.
Pan, H.J.
Yang, P. Duygulu, and C. Faloutsos.2004.
Automatic image captioning.
In Multime-dia and Expo, 2004.
ICME?04.
IEEE InternationalConference on, volume 3.RS Purves, A. Edwardes, and M. Sanderson.
2008.Describing the where?improving image annotationand search through geography.
1st Intl.
Workshopon Metadata Mining for Image Understanding, Fun-chal, Madeira-Portugal.S.
Satoh, Y. Nakamura, and T. Kanade.
1999.
Name-It:naming and detecting faces in news videos.
Multi-media, IEEE, 6(1):22?35.F.
Song and W.B.
Croft.
1999.
A general languagemodel for information retrieval.
In Proc.
of theeighth international conference on Information andknowledge management, pages 316?321.
ACM NewYork, NY, USA.M.
Stevenson and M.A.
Greenwood.
2005.
A seman-tic approach to IE pattern induction.
In Proc.
of the43rd Annual Meeting on Association for Computa-tional Linguistics, pages 379?386.
Association forComputational Linguistics Morristown, NJ, USA.M.
Stevenson and M. Greenwood.
2009.
Depen-dency Pattern Models for Information Extraction.Research on Language and Computation, 7(1):13?39.K.
Sudo, S. Sekine, and R. Grishman.
2001.
Auto-matic pattern acquisition for Japanese informationextraction.
In Proc.
of the first international con-ference on Human language technology research,page 7.
Association for Computational Linguistics.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic acquisition of domainknowledge for information extraction.
In Proc.
ofthe 18th International Conference on ComputationalLinguistics (COLING 2000), pages 940?946.
Saar-briicken, Germany, August.1258
