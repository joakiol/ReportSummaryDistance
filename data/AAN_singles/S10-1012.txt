Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 69?74,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsSemEval-2010 Task: Japanese WSDManabu OkumuraTokyo Institute of Technologyoku@pi.titech.ac.jpKiyoaki ShiraiJapan Advanced Institute of Science and Technologykshirai@jaist.ac.jpKanako KomiyaTokyo University of Agriculture and Technologykkomiya@cc.tuat.ac.jpHikaru YokonoTokyo Institute of Technologyyokono@lr.pi.titech.ac.jpAbstractAn overview of the SemEval-2 JapaneseWSD task is presented.
It is a lexicalsample task, and word senses are definedaccording to a Japanese dictionary, theIwanami Kokugo Jiten.
This dictionaryand a training corpus were distributed toparticipants.
The number of target wordswas 50, with 22 nouns, 23 verbs, and 5adjectives.
Fifty instances of each targetword were provided, consisting of a to-tal of 2,500 instances for the evaluation.Nine systems from four organizations par-ticipated in the task.1 IntroductionThis paper reports an overview of the SemEval-2 Japanese Word Sense Disambiguation (WSD)task.
It can be considered an extension of theSENSEVAL-2 Japanese monolingual dictionary-based task (Shirai, 2001), so it is a lexical sam-ple task.
Word senses are defined according tothe Iwanami Kokugo Jiten (Nishio et al, 1994), aJapanese dictionary published by Iwanami Shoten.It was distributed to participants as a sense inven-tory.
Our task has the following two new charac-teristics:1.
All previous Japanese sense-tagged corporawere from newspaper articles, while sense-tagged corpora were constructed in Englishon balanced corpora, such as Brown corpusand BNC corpus.
The first balanced corpusof contemporary written Japanese (BCCWJcorpus) is now being constructed as part ofa national project in Japan (Maekawa, 2008),and we are now constructing a sense-taggedcorpus based on it.
Therefore, the task willuse the first balanced Japanese sense-taggedcorpus.Because a balanced corpus consists of docu-ments from multiple genres, the corpus canbe divided into multiple sub-corpora of agenre.
In supervised learning approacheson word sense disambiguation, because wordsense distribution might vary across differentsub-corpora, we need to take into account thegenres of training and test corpora.
There-fore, word sense disambiguation on a bal-anced corpus requires tackling a kind of do-main (genre) adaptation problem (Chang andNg, 2006; Agirre and de Lacalle, 2008).2.
In previous WSD tasks, systems have beenrequired to select a sense from a given set ofsenses in a dictionary for a word in one con-text (an instance).
However, the set of sensesin the dictionary is not always complete.
Newword senses sometimes appear after the dic-tionary has been compiled.
Therefore, someinstances might have a sense that cannot befound in the dictionary?s set.
The task willtake into account not only the instances thathave a sense in the given set but also the in-stances that have a sense that cannot be foundin the set.
In the latter case, systems shouldoutput that the instances have a sense that isnot in the set.Training data, a corpus that consists of threegenres (books, newspaper articles, and white pa-pers) and is manually annotated with sense IDs,was also distributed to participants.
For the evalu-ation, we distributed a corpus that consists of fourgenres (books, newspaper articles, white papers,and documents from a Q&A site on the WWW)with marked target words as test data.
Participantswere requested to assign one or more sense IDs toeach target word, optionally with associated prob-abilities.
The number of target words was 50, with22 nouns, 23 verbs, and 5 adjectives.
Fifty in-stances of each target word were provided, con-69sisting of a total of 2,500 instances for the evalua-tion.In what follows, section two describes the de-tails of the data used in the Japanese WSD task.Section three describes the process to constructthe sense tagged data, including the analysis of aninter-annotator agreement.
Section four briefly in-troduces participating systems and section five de-scribes their results.
Finally, section six concludesthe paper.2 DataIn the Japanese WSD task, three types of data weredistributed to all participants: a sense inventory,training data, and test data1.2.1 Sense InventoryAs described in section one, word senses aredefined according to a Japanese dictionary, theIwanami Kokugo Jiten.
The number of headwordsand word senses in the Iwanami Kokugo Jiten is60,321 and 85,870.As described in the task description ofSENSEVAL-2 Japanese dictionary task (Shirai,2001), the Iwanami Kokugo Jiten has hierarchi-cal structures in word sense descriptions.
TheIwanami Kokugo Jiten has at most three hierarchi-cal layers.2.2 Training DataAn annotated corpus was distributed as the train-ing data.
It consists of 240 documents of threegenres (books, newspaper articles, and white pa-pers) from the BCCWJ corpus.
The annotated in-formation in the training data is as follows:?
Morphological informationThe document was annotated with morpho-logical information (word boundaries, a part-of-speech (POS) tag, a base form, and a read-ing) for all words.
All the morphological in-formation was automatically annotated usingchasen2 with unidic and was manually post-edited.1Due to space limits, we unfortunately cannot present thestatistics of the training and test data, such as the numberof instances in different genres, the number of instances fora new word sense, and the Jensen Shannon (JS) divergence(Lin, 1991; Dagan et al, 1997) between the word sense dis-tributions of two different genres.
We hope we will presentthem in another paper in the near future.2http://chasen-legacy.sourceforge.jp/?
Genre codeEach document was assigned a code indicat-ing its genre from the aforementioned list.?
Word sense IDs3,437 word types in the data were annotatedfor sense IDs, and the data contain 31,611sense-tagged instances that include 2,500 in-stances for the 50 target words.
Words as-signed with sense IDs satisfied the followingconditions:1.
The Iwanami Kokugo Jiten gave theirsense description.2.
Their POSs were either a noun, a verb,or an adjective.3.
They were ambiguous, that is, therewere more than two word senses forthem in the dictionary.Word sense IDs were manually annotated.2.3 Test DataThe test data consists of 695 documents of fourgenres (books, newspaper articles, white papers,and documents from a Q&A site on the WWW)from the BCCWJ corpus, with marked targetwords.
The documents used for the training andtest data are not mutually exclusive.
The num-ber of overlapping documents between the train-ing and test data is 185.
The instances used for theevaluation were not provided as the training data3.The annotated information in the test data is as fol-lows:?
Morphological informationSimilar to the training data, the documentwas annotated with morphological informa-tion (word boundaries, a POS tag, a baseform, and a reading) for all words.
All mor-phological information was automatically an-notated using chasen with unidic and wasmanually post-edited.?
Genre codeAs in the training data, each document wasassigned a code indicating its genre from theaforementioned list.?
Word sense IDsWord sense IDs were manually annotated for3The word sense IDs for them were hidden from the par-ticipants.70the target words4.The number of target words was 50, with 22nouns, 23 verbs, and 5 adjectives.
Fifty instancesof each target word were provided, consisting of atotal of 2,500 instances for the evaluation.3 Word Sense TaggingExcept for the word sense IDs, the data describedin section two was developed by the National In-stitute of Japanese Language.
However, the wordsense IDs were newly annotated on the data.
Thissection presents the process of annotating the wordsense IDs, and the analysis of the inter-annotatoragreement.3.1 Sampling Target WordsWhen we chose target words, we considered thefollowing conditions:?
The POSs of target words were either a noun,a verb, or an adjective.?
We chose words that occurred more than 50times in the training data.?
The relative ?difficulty?
in disambiguatingthe sense of words was taken into account.The difficulty of the word w was defined bythe entropy of the word sense distributionE(w) in the test data (Kilgarriff and Rosen-zweig, 2000).
Obviously, the higher E(w) is,the more difficult the WSD for w is.?
The number of instances for a new sense wasalso taken into account.3.2 Manual AnnotationNine annotators assigned the correct word senseIDs for the training and test data.
All of them had acertain level of linguistic knowledge.
The processof manual annotation was as follows:1.
An annotator chose a sense ID for each wordseparately in accordance with the followingguidelines:?
One sense ID was to be chosen for eachword.?
Sense IDs at any layers in the hierarchi-cal structures were assignable.4They were hidden from the participants during the for-mal run.?
The ?new word sense?
tag was to bechosen only when all sense IDs were notabsolutely applicable.2.
For the instances that had a ?new word sense?tag, another annotator reexamined carefullywhether those instances really had a newsense.Because a fragment of the corpus was tagged bymultiple annotators in a preliminary annotation,the inter-annotator agreement between the two an-notators in step 1 was calculated with Kappa statis-tics.
It was 0.678.4 Evaluation MethodologyThe evaluation was returned in the following twoways:1.
The outputted sense IDs were evaluated, as-suming the ?new sense?
as another sense ID.The outputted sense IDs were compared tothe given gold standard word senses, and theusual precision measure for supervised wordsense disambiguation systems was computedusing the scorer.
The Iwanami Kokugo Jitenhas three levels for sense IDs, and we usedthe middle-level sense in the task.
Therefore,the scoring in the task was ?middle-grainedscoring.?2.
The ability of finding the instances of newsenses was evaluated, assuming the taskas classifying each instance into a ?knownsense?
or ?new sense?
class.
The outputtedsense IDs (same as in 1.)
were compared tothe given gold standard word senses, and theusual accuracy for binary classification wascomputed, assuming all sense IDs in the dic-tionary were in the ?known sense?
class.5 Participating SystemsIn the Japanese WSD task, 10 organizations reg-istered for participation.
However, only the ninesystems from four organizations submitted the re-sults.
In what follows, we outline them with thefollowing description:1. learning algorithm used,2.
features used,3.
language resources used,714.
level of analysis performed in the system,5.
whether and how the difference in the textgenre was taken into account,6.
method to detect new senses of words, if any.Note that most of the systems used supervisedlearning techniques.?
HIT-11.
Naive Bayes, 2.
Word form/POS of thetarget word, word form/POS before or afterthe target word, content words in the con-text, classes in a thesaurus for those words inthe context, the text genre, 3.
?Bunrui-Goi-Hyou?, a Japanese thesaurus (National Insti-tute of Japanese Language, 1964), 4.
Mor-phological analysis, 5.
A genre is included inthe features.
6.
Assuming that the posteriorprobability has a normal distribution, the sys-tem judges those instances deviating from thedistribution at the 0.05 significance level as anew word sense?
JAIST-11.
Agglomerative clustering, 2.
Bag-of-words in context, etc.
3.
None, 4.
Mor-phological analysis, 5.
The system does notmerge example sentences in different genresub-corpus into a cluster.
6.
First, the systemmakes clusters of example sentences, thenmeasures the similarity between a cluster anda sense in the dictionary, finally regarding thecluster as a collection of new senses whenthe similarity is small.
For WSD, the systemchooses the most similar sense for each clus-ter, then it considers all the instances in thecluster to have that sense.?
JAIST-21.
SVM, 2.
Word form/POS before or afterthe target word, content words in the context,etc.
3.
None, 4.
Morphological analysis, 5.The system was trained with the feature setwhere features are distinguished whether ornot they are derived from only one genre sub-corpus.
6.
?New sense?
is treated as one of thesense classes.?
JAIST-3The system is an ensemble of JAIST-1 andJAIST-2.
The judgment of a new sense is per-formed by JAIST-1.
The output of JAIST-1 ischosen when the similarity between a clusterand a sense in the dictionary is sufficientlyhigh.
Otherwise, the output of JAIST-2 isused.?
MSS-1,2,31.
Maximum entropy, 2.
Three wordforms/lemmas/POSs before or after the targetword, bigrams, and skip bigrams in the con-text, bag-of-words in the document, a classof the document categorized by a topic clas-sifier, etc.
3.
None, 4.
None, 5.
For each tar-get word, the system selected the genre anddictionary examples combinations for train-ing data, which got the best results in cross-validation.
6.
The system calculated the en-tropy for each target word given by the Maxi-mum Entropy Model (MEM).
It assumed thathigh entropy (when probabilities of classesare uniformly dispersed) was indicative of anew sense.
The threshold was tuned by usingthe words with a new sense tag in the trainingdata.
Three official submissions correspondto different thresholds.?
RALI-1, RALI-21.
Naive Bayes, 2.
Only the ?writing?
ofthe words (inside of <mor> tag), 3.
TheMainichi 2005 corpus of NTCIR, parsed withchasen+unidic, 4.
None, 5.
Not taken into ac-count, 6.
?New sense?
is only used when it isevident in the training dataFor more details, please refer to their descriptionpapers.6 Their ResultsThe evaluation results of all the systems are shownin tables 1 and 2.
?Baseline?
for WSD indicatesthe results of the baseline system that used SVMwith the following features:?
Morphological featuresBag-of-words (BOW), Part-of-speech (POS),and detailed POS classification.
We extractthese features from the target word itself andthe two words to the right and left of it.?
Syntactic features?
If the POS of a target word is a noun,extract the verb in a grammatical depen-dency relation with the noun.72Table 1: Results: Word sense disambiguationPrecisionBaseline 0.7528HIT-1 0.6612JAIST-1 0.6864JAIST-2 0.7476JAIST-3 0.7208MSS-1 0.6404MSS-2 0.6384MSS-3 0.6604RALI-1 0.7592RALI-2 0.7636Table 2: Results: New sense detectionAccuracy Precision RecallBaseline 0.9844 - 0HIT-1 0.9132 0.0297 0.0769JAIST-1 0.9512 0.0337 0.0769JAIST-2 0.9872 1 0.1795JAIST-3 0.9532 0.0851 0.2051MSS-1 0.9416 0.1409 0.5385MSS-2 0.9384 0.1338 0.5385MSS-3 0.9652 0.2333 0.5385RALI-1 0.9864 0.7778 0.1795RALI-2 0.9872 0.8182 0.2308?
If the POS of a target word is a verb, ex-tract the noun in a grammatical depen-dency relation with the verb.?
Figures in Bunrui-Goi-Hyou4 and 5 digits regarding the content word tothe right and left of the target word.The baseline system did not take into account anyinformation on the text genre.
?Baseline?
for newsense detection (NSD) indicates the results of thebaseline system, which outputs a sense in the dic-tionary and never outputs the new sense tag.
Pre-cision and recall for NSD are shown just for refer-ence.
Because relatively few instances for a newword sense were found (39 out of 2500), the taskof the new sense detection was found to be ratherdifficult.Tables 3 and 4 show the results for nouns, verbs,and adjectives.
In our comparison of the base-line system scores for WSD, the score for nounswas the biggest, and the score for verbs was thesmallest (table 3).
However, the average entropyof nouns was the second biggest (0.7257), and thatTable 3: Results for each POS (Precision): Wordsense disambiguationNoun Verb AdjectiveBaseline 0.8255 0.6878 0.732HIT-1 0.7436 0.5739 0.7JAIST-1 0.7645 0.5957 0.76JAIST-2 0.84 0.6626 0.732JAIST-3 0.8236 0.6217 0.724MSS-1 0.7 0.5504 0.792MSS-2 0.6991 0.5470 0.792MSS-3 0.7218 0.5713 0.8RALI-1 0.8236 0.6965 0.764RALI-2 0.8127 0.7191 0.752Table 4: Results for each POS (Accuracy): Newsense detectionNoun Verb AdjectiveBaseline 0.97 0.9948 1HIT-1 0.8881 0.9304 0.944JAIST-1 0.9518 0.9470 0.968JAIST-2 0.9764 0.9948 1JAIST-3 0.9564 0.9470 0.968MSS-1 0.9355 0.9409 0.972MSS-2 0.9336 0.9357 0.972MSS-3 0.96 0.9670 0.98RALI-1 0.9745 0.9948 1RALI-2 0.9764 0.9948 1of verbs was the biggest (1.194)5.We set up three word classes, Ddiff(E(w) ?1), Dmid(0.5 ?
E(w) < 1), and Deasy(E(w) <0.5).
Ddiff, Dmid, and Deasyconsist of 20, 19and 11 words, respectively.
Tables 5 and 6 showthe results for each word class.
The results ofWSD are quite natural in that the higher E(w) is,the more difficult WSD is, and the more the per-formance degrades.7 ConclusionThis paper reported an overview of the SemEval-2Japanese WSD task.
The data used in this task willbe available when you contact the task organizerand sign a copyright agreement form.
We hopethis valuable data helps many researchers improvetheir WSD systems.5The average entropy of adjectives was 0.6326.73Table 5: Results for entropy classes (Precision):Word sense disambiguationDeasyDmidDdiffBaseline 0.9418 0.7411 0.66HIT-1 0.8436 0.6832 0.54JAIST-1 0.8782 0.7158 0.553JAIST-2 0.9509 0.7484 0.635JAIST-3 0.92 0.7368 0.596MSS-1 0.8291 0.6558 0.522MSS-2 0.8273 0.6558 0.518MSS-3 0.8345 0.6905 0.536RALI-1 0.9455 0.7653 0.651RALI-2 0.94 0.7558 0.674Table 6: Results for Entropy classes (Accuracy):New sense detectionDeasyDmidDdiffBaseline 1 0.9737 0.986HIT-1 0.8909 0.9095 0.929JAIST-1 0.9672 0.9505 0.943JAIST-2 1 0.9811 0.986JAIST-3 0.9673 0.9558 0.943MSS-1 0.9818 0.9221 0.938MSS-2 0.98 0.9221 0.931MSS-3 0.9873 0.9611 0.957RALI-1 1 0.9789 0.986RALI-2 1 0.9811 0.986AcknowledgmentsWe would like to thank all the participants and theannotators for constructing this sense tagged cor-pus.ReferencesEneko Agirre and Oier Lopez de Lacalle.
2008.
On ro-bustness and domain adaptation using svd for wordsense disambiguation.
In Proc.
of COLING?08.Yee Seng Chang and Hwee Tou Ng.
2006.
Estimatingclass priors in domain adaptation for wsd.
In Proc.of ACL?06.Ido Dagan, Lillian Lee, and Fernando Pereira.
1997.Similarity-based methods for word sense disam-biguation.
In Proceedings of the Thirty-Fifth An-nual Meeting of the Association for ComputationalLinguistics and Eighth Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 56?63.A.
Kilgarriff and J. Rosenzweig.
2000.
English sense-val: Report and results.
In Proc.
LREC?00.J.
Lin.
1991.
Divergence measures based on the shan-non entropy.
IEEE Transactions on InformationTheory, 37(1):145?151.Kikuo Maekawa.
2008.
Balanced corpus of con-temporary written japanese.
In Proceedings of the6th Workshop on Asian Language Resources (ALR),pages 101?102.National Institute of Japanese Language.
1964.
Bun-ruigoihyou.
Shuuei Shuppan.
In Japanese.Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizu-tani.
1994.
Iwanami Kokugo Jiten Dai Go Han.Iwanami Publisher.
In Japanese.Kiyoaki Shirai.
2001.
Senseval-2 japanese dictionarytask.
In Proceedings of SENSEVAL-2: Second Inter-national Workshop on Evaluating Word Sense Dis-ambiguation Systems, pages 33?36.74
