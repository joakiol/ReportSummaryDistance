Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 32?42,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsModelling Annotator Bias with Multi-task Gaussian Processes:An Application to Machine Translation Quality EstimationTrevor Cohn and Lucia SpeciaDepartment of Computer ScienceUniversity of SheffieldSheffield, United Kingdom{t.cohn,l.specia}@sheffield.ac.ukAbstractAnnotating linguistic data is often a com-plex, time consuming and expensive en-deavour.
Even with strict annotationguidelines, human subjects often deviatein their analyses, each bringing differentbiases, interpretations of the task and lev-els of consistency.
We present novel tech-niques for learning from the outputs ofmultiple annotators while accounting forannotator specific behaviour.
These tech-niques use multi-task Gaussian Processesto learn jointly a series of annotator andmetadata specific models, while explicitlyrepresenting correlations between modelswhich can be learned directly from data.Our experiments on two machine trans-lation quality estimation datasets showuniform significant accuracy gains frommulti-task learning, and consistently out-perform strong baselines.1 IntroductionMost empirical work in Natural Language Pro-cessing (NLP) is based on supervised machinelearning techniques which rely on human anno-tated data of some form or another.
The annota-tion process is often time consuming, expensive,and prone to errors; moreover there is often con-siderable disagreement amongst annotators.In general, the predominant perspective to dealwith these data annotation issues in previous workhas been that there is a single underlying groundtruth, and that the annotations collected are noisyand/or biased samples of this.
The challenge isthen one of quality control, in order to processthe data by filtering, averaging or similar to dis-til the truth.
We posit that this perspective istoo limiting, especially with respect to linguis-tic data, where each individual?s idiolect and lin-guistic background can give rise to many different?
and yet equally valid ?
truths.
Particularly inhighly subjective annotation tasks, the differencesbetween annotators cannot be captured by simplemodels such as scaling all instances of a certainannotator by a factor.
They can originate froma number of nuanced aspects.
This is the case,for example, of annotations on the quality of sen-tences generated using machine translation (MT)systems, which are often used to build quality es-timation models (Blatz et al, 2004; Specia et al,2009) ?
our application of interest.In addition to annotators?
own perceptions andexpectations with respect to translation quality, anumber of factors can affect their judgements onspecific sentences.
For example, certain anno-tators may prefer translations produced by rule-based systems as these tend to be more grammati-cal, while others would prefer sentences producedby statistical systems with more adequate lexicalchoices.
Likewise, some annotators can be biasedby the complexity of the source sentence: lengthysentences are often (subconsciously) assumed tobe of low quality by some annotators.
An ex-treme case is the judgement of quality throughpost-editing time: annotators have different typingspeeds, as well as levels of expertise in the taskof post-editing, proficiency levels in the languagepair, and knowledge of the terminology used inparticular sentences.
These variations result intime measurements that are not comparable acrossannotators.
Thus far, the use of post-editing timehas been done on an per-annotator basis (Specia,2011), or simply averaged across multiple transla-tors (Plitt and Masselot, 2010), both strategies farfrom ideal.Overall, these myriad of factors affecting qual-ity judgements make the modelling of multipleannotators a very challenging problem.
Thisproblem is exacerbated when annotations areprovided by non-professional annotators, e.g.,through crowdsourcing ?
a common strategy used32to make annotation cheaper and faster, however atthe cost of less reliable outcomes.Most related work on quality assurance for dataannotation has been developed in the context ofcrowdsourcing.
Common practices include fil-tering out annotators who substantially deviatefrom a gold-standard set or present unexpectedbehaviours (Raykar et al, 2010; Raykar and Yu,2012), or who disagree with others using, e.g., ma-jority or consensus labelling (Snow et al, 2008;Sheng et al, 2008).
Another relevant strand ofwork aims to model legitimate, systematic biasesin annotators (including both non-experts and ex-perts), such as the fact that some annotators tendto be more negative than others, and that someannotators use a wider or narrower range of val-ues (Flach et al, 2010; Ipeirotis et al, 2010).However, with a few exceptions in Computer Vi-sion (e.g., Whitehill et al (2009), Welinder et al(2010)), existing work disregard metadata and itsimpact on labelling.In this paper we model the task of predicting thequality of sentence translations using datasets thathave been annotated by several judges with differ-ent levels of expertise and reliability, containingtranslations from a variety of MT systems and ona range of different types of sentences.
We ad-dress this problem using multi-task learning inwhich we learn individual models for each context(the task, incorporating the annotator and othermetadata: translation system and the source sen-tence) while also modelling correlations betweentasks such that related tasks can mutually informone another.
Our use of multi-task learning allowsthe modelling of a diversity of truths, while alsorecognising that they are rarely independent of oneanother (annotators often agree) by explicitly ac-counting for inter-annotator correlations.Our approach is based on Gaussian Processes(GPs) (Rasmussen and Williams, 2006), a ker-nelised Bayesian non-parametric learning frame-work.
We develop multi-task learning models byrepresenting intra-task transfer simply and explic-itly as part of a parameterised kernel function.
GPsare an extremely flexible probabilistic frameworkand have been successfully adapted for multi-tasklearning in a number of ways, e.g., by learningmulti-task correlations (Bonilla et al, 2008), mod-elling per-task variance (Groot et al, 2011) or per-annotator biases (Rogers et al, 2010).
Our methodbuilds on the work of Bonilla et al (2008) byexplicitly modelling intra-task transfer, which islearned automatically from the data, in order to ro-bustly handle outlier tasks and task variances.
Weshow in our experiments on two translation qual-ity datasets that these multi-task learning strate-gies are far superior to training individual per-taskmodels or a single pooled model, and moreoverthat our multi-task learning approach can achievesimilar performance to these baselines using onlya fraction of the training data.In addition to showing empirical performancegains on quality estimation applications, an im-portant contribution of this paper is in introduc-ing Gaussian Processes to the NLP community,1a technique that has great potential to further per-formance in a wider range of NLP applications.Moreover, the algorithms proposed herein can beadapted to improve future annotation efforts, andsubsequent use of noisy crowd-sourced data.2 Quality EstimationQuality estimation (QE) for MT aims at providingan estimate on the quality of each translated seg-ment ?
typically a sentence ?
without access to ref-erence translations.
Work in this area has becomeincreasingly popular in recent years as a conse-quence of the widespread use of MT among real-world users such as professional translators.
Ex-amples of applications of QE include improvingpost-editing efficiency by filtering out low qual-ity segments which would require more effort andtime to correct than translating from scratch (Spe-cia et al, 2009), selecting high quality segmentsto be published as they are, without post-editing(Soricut and Echihabi, 2010), selecting a trans-lation from either an MT system or a translationmemory for post-editing (He et al, 2010), select-ing the best translation from multiple MT sys-tems (Specia et al, 2010), and highlighting sub-segments that need revision (Bach et al, 2011).QE is generally addressed as a machine learn-ing task using a variety of linear and kernel-basedregression or classification algorithms to inducemodels from examples of translations describedthrough a number of features and annotated forquality.
For an overview of various algorithms andfeatures we refer the reader to the WMT12 sharedtask on QE (Callison-Burch et al, 2012).While initial work used annotations derived1We are not strictly the first, Polajnar et al (2011) usedGPs for text classification.33from automatic MT evaluation metrics (Blatz etal., 2004) such as BLEU (Papineni et al, 2002)at training time, it soon became clear that humanlabels result in significantly better models (Quirk,2004).
Current work at sentence level is thus basedon some form of human supervision.As typical of subjective annotation tasks, QEdatasets should contain multiple annotators to leadto models that are representative.
Therefore, workin QE faces all common issues regarding variabil-ity in annotators?
judgements.
The following are afew other features that make our datasets particu-larly interesting:?
In order to minimise annotation costs, trans-lation instances are often spread among anno-tators, such that each instance is only labelledby one or a few judges.
In fact, for a sizeabledataset (thousands of instances), the annota-tion of a complete dataset by a single judgemay become infeasible.?
It is often desirable to include alternativetranslations of source sentences produced bymultiple MT systems, which requires multi-ple annotators for unbiased judgements, par-ticularly for labels such as post-editing time(a translation seen a second time will requireless editing effort).?
For crowd-sourced annotations it is often im-possible to ensure that the same annotatorswill label the same subset of cases.These features ?
which are also typical of manyother linguistic annotation tasks ?
make the learn-ing process extremely challenging.
Learning mod-els from datasets annotated by multiple annotatorsremains an open challenge in QE, as we show inSection 4.
In what follows, we present our QEdatasets in more detail.2.1 DatasetsWe use two freely available QE datasets to experi-ment with the techniques proposed in this paper:2WMT12: This dataset was distributed as part ofthe WMT12 shared task on QE (Callison-Burch etal., 2012).
It contains 1, 832 instances for train-ing, and 422 for test.
The English source sen-tences are a subset of WMT09-12 test sets.
TheSpanish MT outputs were created using a standardPBSMT Moses engine.
Each instance was anno-tated with post-editing effort scores from highest2Both datasets can be downloaded from http://www.dcs.shef.ac.uk/?lucia/resources.html.effort (score 1) to lowest effort (score 5), whereeach score identifies an estimated percentage ofthe MT output that needs to be corrected.
Thepost-editing effort scores were produced indepen-dently by three professional translators based ona previously post-edited translation by a fourthtranslator.
In an attempt to accommodate for sys-tematic biases among annotators, the final effortscore was computed as the weighted average be-tween the three PE-effort scores, with more weightgiven to the judges with higher standard deviationfrom their own mean score.
This resulted in scoresspread more evenly in the [1, 5] range.WPTP12: This dataset was distributed by Ko-ponen et al (2012).
It contains 299 English sen-tences translated into Spanish using two or moreof eight MT systems randomly selected from allsystem submissions for WMT11 (Callison-Burchet al, 2011).
These MT systems range from on-line and customised SMT systems to commercialrule-based systems.
Translations were post-editedby humans while time was recorded.
The labelsare the number of seconds spent by a translatorediting a sentence normalised by source sentencelength.
The post-editing was done by eight na-tive speakers of Spanish, including five profes-sional translators and three translation students.Only 20 translations were edited by all eight an-notators, with the remaining translations randomlydistributed amongst them.
The resulting datasetcontains 1, 624 instances, which were randomlysplit into 1, 300 for training and 300 for test.
Ac-cording to the analysis in (Koponen et al, 2012),while on average certain translators were found tobe faster than others, their speed in post-editingindividual sentences varies considerably, i.e., cer-tain translators are faster at certain sentences.
Toour knowledge, no previous work has managed tosuccessfully model the prediction of post-editingtime from datasets with multiple annotators.3 Gaussian Process RegressionMachine learning models for quality estimationtypically treat the problem as regression, seekingto model the relationship between features of thetext input and the human quality judgement as acontinuous response variable.
Popular choices in-clude Support Vector Machines (SVMs), whichhave been shown to perform well for quality es-timation (Callison-Burch et al, 2012) using non-linear kernel functions such as radial basis func-34tions.
In this paper we consider Gaussian Pro-cesses (GP) (Rasmussen and Williams, 2006), aprobabilistic machine learning framework incor-porating kernels and Bayesian non-parametrics,widely considered state-of-the-art for regression.Despite this GPs have not been used widely to datein statistical NLP.
GPs are particularly suitable formodelling QE for a number of reasons: 1) theyexplicitly model uncertainty, which is rife in QEdatasets; 2) they allow fitting of expressive kernelsto data, in order to modulate the effect of featuresof varying usefulness; and 3) they can naturallybe extended to model correlated tasks using multi-task kernels.
We now give a brief overview of GPs,following Rasmussen and Williams (2006).In our regression task3 the data consists of npairs D = {(xi, yi)}, where xi ?
RF is a F -dimensional feature vector and yi ?
R is the re-sponse variable.
Each instance is a translation andthe feature vector encodes its linguistic features;the response variable is a numerical quality judge-ment: post editing time or likert score.
As usual,the modelling challenge is to automatically predictthe value of y based on the x for unseen test input.GP regression assumes the presence of a la-tent function, f : RF ?
R, which maps fromthe input space of feature vectors x to a scalar.Each response value is then generated from thefunction evaluated at the corresponding data point,yi = f(xi) + ?, where ?
?
N (0, ?2n) is addedwhite-noise.
Formally f is drawn from a GP prior,f(x) ?
GP(0, k(x,x?
)),which is parameterised by a mean (here, 0) anda covariance kernel function k(x,x?).
The ker-nel function represents the covariance (i.e., sim-ilarities in the response) between pairs of datapoints.
Intuitively, points that are in close proxim-ity should have high covariance compared to thosethat are further apart, which constrains f to be asmoothly varying function of its inputs.
This intu-ition is embodied in the squared exponential ker-nel (a.k.a.
radial basis function or Gaussian),k(x,x?)
= ?2f exp(?12(x?
x?)TA?1(x?
x?
))(1)where ?2f is a scaling factor describing the overalllevels of variance, and A = diag(a) is a diagonal3Our approach generalises to classification, ranking (ordi-nal regression) or various other training objectives, includingmixtures of objectives.
In this paper we use regression forsimplicity of exposition and implementation.matrix of length scales, encoding the smoothnessof functions f with respect to each feature.
Non-uniform length scales allow for different degreesof smoothness of f in each dimension, such thate.g., for unimportant features f is relatively flatwhereas for very important features f is jagged,such that a small change in the feature value hasa large effect.
When the values of a are learnedautomatically from data, as we do herein, this isreferred to as the automatic relevance determina-tion (ARD) kernel.Given the generative process defined above, weformulate prediction as Bayesian inference underthe posterior, namelyp(y?|x?,D) =?fp(y?|x?, f)p(f |D)where x?
is a test input and y?
is its responsevalue.
The posterior p(f |D) reflects our updatedbelief over possible functions after observing thetraining set D, i.e., f should pass close to the re-sponse values for each training instance (but neednot fit exactly due to additive noise).
This is bal-anced against the smoothness constraints that arisefrom the GP prior.
The predictive posterior can besolved analytically, resulting iny?
?
N(kT?
(K + ?2nI)?1y, (2)k(x?,x?)?
kT?
(K + ?2nI)?1k?
)where k?
= [k(x?,x1) k(x?,x2) ?
?
?
k(x?,xn)]Tare the kernel evaluations between the test pointand the training set, and {Kij = k(xi,xj)} isthe kernel (gram) matrix over the training points.Note that the posterior in Eq.
2 includes not onlythe expected response (the mean) but also the vari-ance, encoding the model?s uncertainty, which isimportant for integration into subsequent process-ing, e.g., as part of a larger probabilistic model.GP regression also permits an analytic for-mulation of the marginal likelihood, p(y|X) =?f p(y|X, f)p(f), which can be used for modeltraining (X are the training inputs).
Specifically,we can derive the gradient of the (log) marginallikelihood with respect to the model hyperparam-eters (i.e., a, ?n, ?s etc.)
and thereby find the typeII maximum likelihood estimate using gradient as-cent.
Note that in general the marginal likelihoodis non-convex in the hyperparameter values, andconsequently the solutions may only be locally op-timal.
Here we bootstrap the learning of complexmodels with many hyperparameters by initialising35with the (good) solutions found for simpler mod-els, thereby avoiding poor local optima.
We referthe reader to Rasmussen and Williams (2006) forfurther details.At first glance GPs resemble SVMs, which alsoadmit kernels such as the popular squared expo-nential kernel in Eq.
1.
The key differences arethat GPs are probabilistic models and support ex-act Bayesian inference in the case of regression(approximate inference is required for classifica-tion (Rasmussen and Williams, 2006)).
MoreoverGPs provide greater flexibility in fitting the ker-nel hyperparameters even for complex compositekernels.
In typical usage, the kernel hyperparam-eters for an SVM are fit using held-out estima-tion, which is inefficient and often involves ty-ing together parameters to limit the search com-plexity (e.g., using a single scale parameter inthe squared exponential).
Multiple-kernel learning(Go?nen and Alpayd?n, 2011) goes some way to ad-dressing this problem within the SVM framework,however this technique is limited to reweightinglinear combinations of kernels and has high com-putational complexity.3.1 Multi-task Gaussian Process ModelsUntil now we have considered a standard regres-sion scenario, where each training point is labelledwith a single output variable.
In order to modelmultiple different annotators jointly, i.e., multi-task learning, we need to extend the model to han-dle many tasks.
Conceptually, we can considerthe multi-task model drawing a latent function foreach task, fm(x), where m ?
1, ...,M is the taskidentifier.
This function is then used to explainthe response values for all the instances for thattask (subject to noise).
Importantly, for multi-tasklearning to be of benefit, the prior over {fm} mustcorrelate the functions over different tasks, e.g., byimposing similarity constraints between the valuesfor fm(x) and fm?
(x).We can consider two alternative perspectivesfor framing the multi-task learning problem: ei-ther isotopic where we associate each input pointx with a vector of outputs, y ?
RM , one foreach of the M tasks; or heterotopic where someof the outputs are missing, i.e., tasks are not con-strained to share the same data points (Alvarez etal., 2011).
Given the nature of our datasets, weopted for the heterotopic approach, which can han-dle both singly annotated and multiply annotateddata.
This can be implemented by augmentingeach input point with an additional task identityfeature, which is paired with a single y response,and integrated into a GP model with the standardtraining and inference algorithms.4In moving to a task-augmented data representa-tion, we need to revise our kernel function.
We usea separable multi-task kernel (Bonilla et al, 2008;Alvarez et al, 2011) of the formk((x, d), (x?, d?
))= kdata(x,x?)Bd,d?
, (3)where kdata(x,x?)
is a standard kernel over the in-put points, typically a squared exponential (seeEq.
1), and B ?
RD?D is a positive semi-definitematrix encoding task covariances.
We developa series of increasingly complex choices for B,which we compare empirically in Section 4.2:Independent The simplest case is whereB = I ,i.e., all pairs of different tasks have zero covari-ance.
This corresponds to independent modellingof each task, although all models share the samedata kernel, so this setting is not strictly equiva-lent to independent training with independent per-task data kernels (with different hyperparameters).Similarly, we might choose to use a single noisevariance, ?2n, or an independent noise variance hy-perparameter per task.Pooled Another extreme is B = 1, which ig-nores the task identity, corresponding to poolingthe multi-task data into one large set.
Groot etal.
(2011) present a method for applying GPs formodelling multi-annotator data using this pool-ing kernel with independent per-task noise terms.They show on synthetic data experiments that thisapproach works well at extracting the signal fromnoise-corrupted inputs.Combined A simple approach for B is aweighted combination of Independent and Pool,i.e., B = 1+ aI , where the hyperparameter a ?
0controls the amount of inter-task transfer betweeneach task and the global ?pooled?
task.5 For dis-similar tasks, a high value of a allows each task tobe modelled independently, while for more simi-lar tasks low a allows the use of a large pool of4Note that the separable kernel (Eq.
3) gives rise to blockstructured kernel matrices which permit various optimisa-tions (Bonilla et al, 2008) to reduce the computational com-plexity of inference, e.g., the matrix inversion in Eq.
2.5Note that larger values of a need not affect the overallmagnitude of k, which can be down-scaled by the ?2f factorin the data kernel (Eq.
1).36similar data.
A scaled version of this kernel hasbeen shown to correspond to mean regularisationin SVMs when combined with a linear data ker-nel (Evgeniou et al, 2006).
A similar multi-taskkernel was proposed by Daume?
III (2007), usinga linear data kernel and a = 1, which has shownto result in excellent performance across a rangeof NLP problems.
In contrast to these earlier ap-proaches, we learn the hyperparameter a directly,fitting the relative amounts of inter- versus intra-task transfer to the dataset.Combined+ We consider an extension to theCombined kernel, B = 1 + diag(a), ad ?
0in which each task has a different hyperparametermodulating its independence from the global pool.This additional flexibility can be used, e.g., to al-low individual outlier annotators to be modelledindependently of the others, by assigning a highvalue to ad.
In contrast, Combined ties togetherthe parameters for all tasks, i.e., all annotators areassumed to have similar quality in that they devi-ate from the mean to the same degree.3.2 Integrating metadataThe approaches above assume that the data is splitinto an unstructured set of M tasks, e.g., by anno-tator.
However, it is often the case that we haveadditional information about each data instance inthe form of metadata.
In our quality estimationexperiments we consider as metadata the MT sys-tem which produced the translation, and the iden-tity of the source sentence being translated.
Manyother types of metadata, such as the level of expe-rience of the annotator, could also be used.
Oneway of integrating such metadata would be to de-fine a separate task for every observed combina-tion of metadata values, in which case we treat themetadata as a task descriptor.
Doing so naivelywould however incur a significant penalty, as eachtask will have very few training instances result-ing in inaccurate models, even with the inter-taskkernel approaches defined above.We instead extend the task-level kernels to usethe task descriptors directly to represent task cor-relations.
Let B(i) be a square covariance matrixfor the ith task descriptor ofM , with a column androw for each value (e.g., annotator identity, trans-lation system, etc.).
We redefine the task level ker-nel using paired inputs (x,m), where m are thetask descriptors,k((x,m), (x?,m?
))= kdata(x,x?
)M?i=1B(i)mi,m?i .This is equivalent to using a structured task-kernelB = B(1) ?
B(3) ?
?
?
?
?
B(M) where ?
is theKronecker product.
Using this formulation we canconsider any of the above choices for B appliedto each task descriptor.
In our experiments weconsider the Combined and Combined+ kernels,which allow the model to learn the relative impor-tance of each descriptor in terms of independentmodelling versus pooling the data.4 Multi-task Quality Estimation4.1 Experimental SetupFeature sets: In all experiments we use 17 shal-low QE features that have been shown to performwell in previous work.
These were used by ahighly competitive baseline entry in the WMT12shared task, and were extracted here using the sys-tem provided by that shared task.6 They includesimple counts, e.g., the tokens in sentences, aswell as source and target language model proba-bilities.
Each feature was scaled to have zero meanand unit standard deviation on the training set.Baselines: The baselines use the SVM regres-sion algorithm with radial basis function kerneland parameters ?,  and C optimised through grid-search and 5-fold cross validation on the trainingset.
This is generally a very strong baseline: inthe WMT12 QE shared task, only five out of 19submissions were able to significantly outperformit, and only by including many complex additionalfeatures, tree kernels, etc.
We also present ?, atrivial baseline based on predicting for each testinstance the training mean (overall, and for spe-cific tasks).GP: All GP models were implemented using theGPML Matlab toolbox.7 Hyperparameter optimi-sation was performed using conjugate gradient as-cent of the log marginal likelihood function, withup to 100 iterations.
The simpler models were ini-tialised with all hyperparameters set to one, whilemore complex models were initialised using the6The software used to extract these (and other) fea-tures can be downloaded from http://www.quest.dcs.shef.ac.uk/7http://www.gaussianprocess.org/gpml/code37Model MAE RMSE?
0.8279 0.9899SVM 0.6889 0.8201Linear ARD 0.7063 0.8480Squared exp.
Isotropic 0.6813 0.8146Squared exp.
ARD 0.6680 0.8098Rational quadratic ARD 0.6773 0.8238Matern(5,2) 0.6772 0.8124Neural network 0.6727 0.8103Table 1: Single-task learning results on theWMT12 dataset, trained and evaluated againstthe weighted averaged response variable.
?
is abaseline which predicts the training mean, SVMuses the same system as the WMT12 QE task, andthe remainder are GP regression models with dif-ferent kernels (all include additive noise).solution for a simpler model.
For instance, mod-els using ARD kernels were initialised from anequivalent isotropic kernel (which ties all the hy-perparameters together), and independent per-tasknoise models were initialised from a single noisemodel.
This approach was more reliable than ran-dom restarts in terms of accuracy and runtime ef-ficiency.Evaluation: We evaluate predictive accuracyusing two measures: mean absolute error,MAE = 1N?Ni=1 |yi ?
y?i| and root mean squareerror, RMSE =?1N?Ni=1 (yi ?
y?i)2, where yiare the gold standard response values and y?i arethe model predictions.4.2 ResultsOur experiments aim to demonstrate the efficacyof GP regression, both the single task and multi-task settings, compared to competitive baselines.WMT12: Single task We start by comparingGP regression with alternative approaches usingthe WMT12 dataset on the standard task of pre-dicting a weighted mean quality rating (as it wasdone in the WMT12 QE shared task).
Table 1shows the results for baseline approaches and theGP models, using a variety of different kernels(see Rasmussen and Williams (2006) for details ofthe kernel functions).
From this we can see that allmodels do much better than the mean baseline andthat most of the GP models have lower error thanthe state-of-the-art SVM.
In terms of kernels, thelinear kernel performs comparatively worse thannon-linear kernels.
Overall the squared exponen-Model MAE RMSE?
0.8541 1.0119Independent SVMs 0.7967 0.9673EasyAdapt SVM 0.7655 0.9105Independent 0.7061 0.8534Pooled 0.7252 0.8754Pooled & {N} 0.7050 0.8497Combined 0.6966 0.8448Combined & {N} 0.6975 0.8476Combined+ 0.6975 0.8463Combined+ & {N} 0.7046 0.8595Table 2: Results on the WMT12 dataset, trainedand evaluated over all three annotator?s judge-ments.
Shown above are the training mean base-line ?, single-task learning approaches, and multi-task learning models, with the columns showingmacro average error rates over all three responsevalues.
All systems use a squared exponentialARD kernel in a product with the named task-kernel, and with added noise (per-task noise is de-noted {N}, otherwise has shared noise).tial ARD kernel has the best performance underboth measures of error, and for this reason we usethis kernel in our subsequent experiments.WMT12: Multi-task We now turn to the multi-task setting, where we seek to model each of thethree annotators?
predictions.
Table 2 presentsthe results.
Note that here error rates are mea-sured over all of the three annotators?
judgements,and consequently are higher than those measuredagainst their average response in Table 1.
For com-parison, taking the predictions of the best model,Combined, in Table 2 and evaluating its averagedprediction has a MAE of 0.6588 vs. the averagedgold standard, significantly outperforming the bestmodel in Table 1.There are a number of important findings in Ta-ble 2.
First, the independently trained models dowell, outperforming the pooled model with fixednoise, indicating that naively pooling the data iscounter-productive and that there are annotator-specific biases.
Including per-annotator noise tothe pooled model provides a boost in performance,however the best results are obtained using theCombined kernel which brings the strengths ofboth the independent and pooled settings.
Thereare only minor differences between the differentmulti-task kernels, and in this case per-annotatornoise made little difference.
An explanation forthe contradictory findings about the importance38of independent noise is that differences betweenannotators can already be explained by the MTLmodel using the multi-task kernel, and need not beexplained as noise.The GP models significantly improve overthe baselines, including an SVM trained inde-pendently and using the EasyAdapt method formulti-task learning (Daume?
III, 2007).
WhileEasyAdapt showed an improvement over the in-dependent SVM, it was a long way short of theGP models.
A possible explanation is that inEasyAdapt the multi-task sharing parameter is setat a = 1, which may not be appropriate for thetask.
In contrast the Combined GP model learneda value of a = 0.01, weighting the value of pool-ing much more highly than independent training.A remaining question is how these approachescope with smaller datasets, where issues of datasparsity become more prevalent.
To test this, wetrained single-task, pooled and multi-task modelson randomly sub-sampled training sets of differ-ent sizes, and plot their error rates in Figure 1.As expected, for very small datasets pooling out-performs single task learning, however for modestsized datasets of ?
90 training instances poolingwas inferior.
For all dataset sizes multi-task learn-ing is superior to the other approaches, makingmuch better use of small and large training sets.The MTL model trained on 500 samples had anMAE of 0.7082?
0.0042, close to the best resultsfrom the full dataset in Table 2, despite using 19as much data: here we use 13 as many traininginstances where each is singly (cf.
triply) anno-tated.
The same experiments run with multiply-annotated instances showed much weaker perfor-mance, presumably due to the more limited sam-ple of input points and poorer fit of the ARD ker-nel hyperparameters.
This finding suggests thatour multi-task learning approach could be used tostreamline annotation efforts by reducing the needfor extensive multiple annotations.WPTP12 This dataset involves predicting thepost-editing time for eight annotators, where weseek to test our model?s capability to use addi-tional metadata.
We model the logarithm of theper-word post-editing time, in order to make theresponse variable more comparable between an-notators and across sentences, and generally moreGaussian in shape.
In Table 3 immediately wecan see that the baseline of predicting the train-ing mean is very difficult to beat, and the trained50 100 150 200 250 300 350 400 450 5000.70.720.740.760.780.80.82Training examplesSTLMTLPooledFigure 1: Learning curve comparing MAE for dif-ferent training methods on the WMT12 dataset,all using a squared exponential ARD data kerneland tied noise parameter.
The MTL model uses theCombined task kernel.
Each point is the averageof 5 runs, and the error bars show ?1 s.d.systems often do worse.
Partitioning the databy annotator (?A) gives the best baseline result,while there is less information from the MT sys-tem or sentence identity.
Single-task learning per-forms only a little better than these baselines, al-though some approaches such as the naive pool-ing perform terribly.
This suggests that the tasksare highly different to one another.
Interestingly,adding the per-task noise models to the pooling ap-proach greatly improves its performance.The multi-task learning methods performed bestwhen using the annotator identity as the task de-scriptor, and less well for the MT system and sen-tence pair, where they only slightly improved overthe baseline.
However, making use of all these lay-ers of metadata together gives substantial furtherimprovements, reaching the best result with Com-binedA,S,T .
The effect of adding per-task noise tothese models was less marked than for the pooledmodels, as in the WMT12 experiments.
Inspectingthe learned hyperparameters, the combined mod-els learned a large bias towards independent learn-ing over pooling, in contrast to the WMT12 exper-iments.
This may explain the poor performance ofEasyAdapt on this dataset.5 ConclusionThis paper presented a novel approach for learningfrom human linguistic annotations by explicitlytraining models of individual annotators (and pos-sibly additional metadata) using multi-task learn-ing.
Our method using Gaussian Processes is flex-ible, allowing easy learning of inter-dependencesbetween different annotators and other task meta-39Model MAE RMSE?
0.5596 0.7053?A 0.5184 0.6367?S 0.5888 0.7588?T 0.6300 0.8270Pooled SVM 0.5823 0.7472IndependentA SVM 0.5058 0.6351EasyAdapt SVM 0.7027 0.8816SINGLE-TASK LEARNINGIndependentA 0.5091 0.6362IndependentS 0.5980 0.7729Pooled 0.5834 0.7494Pooled & {N} 0.4932 0.6275MULTI-TASK LEARNING: AnnotatorCombinedA 0.4815 0.6174CombinedA & {N} 0.4909 0.6268Combined+A 0.4855 0.6203Combined+A & {N} 0.4833 0.6102MULTI-TASK LEARNING: Translation systemCombinedS 0.5825 0.7482MULTI-TASK LEARNING: Sentence pairCombinedT 0.5813 0.7410MULTI-TASK LEARNING: CombinationsCombinedA,S 0.4988 0.6490CombinedA,S & {NA,S} 0.4707 0.6003Combined+A,S 0.4772 0.6094CombinedA,S,T 0.4588 0.5852CombinedA,S,T & {NA,S} 0.4723 0.6023Table 3: Results on the WPTP12 dataset, usingthe log of the post-editing time per word as theresponse variable.
Shown above are the trainingmean and SVM baselines, single-task learning andmulti-task learning results (micro average).
Thesubscripts denote the task split: annotator (A), MTsystem (S) and sentence identity (T).data.
Our experiments showed how our approachoutperformed competitive baselines on two ma-chine translation quality regression problems, in-cluding the highly challenging problem of predict-ing post-editing time.In future work we plan to apply these techniquesto new datasets, particularly noisy crowd-sourceddata with much large numbers of annotators, aswell as a wider range of task types and mixturesthereof (regression, ordinal regression, ranking,classification).
We also have preliminary positiveresults for more advanced multi-task kernels, e.g.,general dense matrices, which can induce clustersof related tasks.Our multi-task learning approach has muchwider application.
Models of individual annota-tors could be used to train machine translationsystems to optimise an annotator-specific qualitymeasure, or in active learning for corpus annota-tion, where the model can suggest the most ap-propriate instances for each annotator or the bestannotator for a given instance.
Further, our ap-proach contributes to work based on cheap and fastcrowdsourcing of linguistic annotation by min-imising the need for careful data curation andquality control.AcknowledgementsThis work was funded by PASCAL2 Harvest Pro-gramme, as part of the QuEst project: http://www.quest.dcs.shef.ac.uk/.
The au-thors would like to thank Neil Lawerence andJames Hensman for advice on Gaussian Processes,the QuEst participants, particularly Jose?
Guil-herme Camargo de Souza and Eva Hassler, and thethree anonymous reviewers.ReferencesMauricio A. Alvarez, Lorenzo Rosasco, and Neil D.Lawrence.
2011.
Kernels for vector-valued func-tions: A review.
Foundations and Trends in MachineLearning, 4(3):195?266.Nguyen Bach, Fei Huang, and Yaser Al-Onaizan.2011.
Goodness: a method for measuring machinetranslation confidence.
In the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 211?219, Portland, Oregon.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, Alberto40Sanchis, and Nicola Ueffing.
2004.
Confidence Es-timation for Machine Translation.
In the 20th Inter-national Conference on Computational Linguistics(Coling 2004), pages 315?321, Geneva.Edwin Bonilla, Kian Ming Chai, and ChristopherWilliams.
2008.
Multi-task gaussian process pre-diction.
In Advances in Neural Information Process-ing Systems (NIPS).Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In the SixthWorkshop on Statistical Machine Translation, pages22?64, Edinburgh, Scotland.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In the Seventh Workshopon Statistical Machine Translation, pages 10?51,Montre?al, Canada.Hal Daume?
III.
2007.
Frustratingly easy domain adap-tation.
In the 45th Annual Meeting of the Associ-ation for Computational Linguistics, Prague, CzechRepublic.Theodoros Evgeniou, Charles A. Micchelli, and Massi-miliano Pontil.
2006.
Learning multiple tasks withkernel methods.
Journal of Machine Learning Re-search, 6(1):615.Peter A. Flach, Sebastian Spiegler, Bruno Gole?nia, Si-mon Price, John Guiver, Ralf Herbrich, Thore Grae-pel, and Mohammed J. Zaki.
2010.
Novel toolsto streamline the conference review process: experi-ences from SIGKDD?09.
SIGKDD Explor.
Newsl.,11(2):63?67, May.Mehmet Go?nen and Ethem Alpayd?n.
2011.
Multi-ple kernel learning algorithms.
Journal of MachineLearning Research, 12:2211?2268.Perry Groot, Adriana Birlutiu, and Tom Heskes.
2011.Learning from multiple annotators with gaussianprocesses.
In Proceedings of the 21st internationalconference on Artificial neural networks - VolumePart II, ICANN?11, pages 159?164, Espoo, Finland.Yifan He, Yanjun Ma, Josef van Genabith, and AndyWay.
2010.
Bridging smt and tm with transla-tion recommendation.
In the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 622?630, Uppsala, Sweden.Panagiotis G. Ipeirotis, Foster Provost, and Jing Wang.2010.
Quality management on amazon mechanicalturk.
In Proceedings of the ACM SIGKDD Work-shop on Human Computation, HCOMP ?10, pages64?67, Washington DC.Maarit Koponen, Wilker Aziz, Luciana Ramos, andLucia Specia.
2012.
Post-editing time as a mea-sure of cognitive effort.
In Proceedings of theAMTA 2012 Workshop on Post-editing Technologyand Practice, WPTP 2012, San Diego, CA.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In the 40th An-nual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, Pennsyl-vania.Mirko Plitt and Franc?ois Masselot.
2010.
A productiv-ity test of statistical machine translation post-editingin a typical localisation context.
Prague Bull.
Math.Linguistics, 93:7?16.Tamara Polajnar, Simon Rogers, and Mark Girolami.2011.
Protein interaction detection in sentences viagaussian processes; a preliminary evaluation.
Int.
J.Data Min.
Bioinformatics, 5(1):52?72, February.Christopher B. Quirk.
2004.
Training a sentence-levelmachine translation confidence metric.
In Proceed-ings of the International Conference on LanguageResources and Evaluation, volume 4 of LREC 2004,pages 825?828, Lisbon, Portugal.Carl E. Rasmussen and Christopher K.I.
Williams.2006.
Gaussian processes for machine learning,volume 1.
MIT press Cambridge, MA.Vikas C. Raykar and Shipeng Yu.
2012.
Eliminatingspammers and ranking annotators for crowdsourcedlabeling tasks.
J. Mach.
Learn.
Res., 13:491?518.Vikas C. Raykar, Shipeng Yu, Linda H. Zhao, Ger-ardo Hermosillo Valadez, Charles Florin, Luca Bo-goni, and Linda Moy.
2010.
Learning from crowds.J.
Mach.
Learn.
Res., 99:1297?1322.Simon Rogers, Mark Girolami, and Tamara Polajnar.2010.
Semi-parametric analysis of multi-rater data.Statistics and Computing, 20(3):317?334.Victor S. Sheng, Foster Provost, and Panagiotis G.Ipeirotis.
2008.
Get another label?
Improving dataquality and data mining using multiple, noisy la-belers.
In Proceedings of the 14th ACM SIGKDD,KDD?08, pages 614?622, Las Vegas, Nevada.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages254?263, Honolulu, Hawaii.Radu Soricut and Abdessamad Echihabi.
2010.Trustrank: Inducing trust in automatic translationsvia ranking.
In the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 612?621, Uppsala, Swe-den, July.Lucia Specia, Marco Turchi, Nicola Cancedda, MarcDymetman, and Nello Cristianini.
2009.
Estimat-ing the Sentence-Level Quality of Machine Trans-lation Systems.
In the 13th Annual Meeting ofthe European Association for Machine Translation(EAMT?2009), pages 28?37, Barcelona.41Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.Machine translation evaluation versus quality esti-mation.
Machine Translation, pages 39?50.Lucia Specia.
2011.
Exploiting Objective Annotationsfor Measuring Translation Post-editing Effort.
In the15th Annual Meeting of the European Associationfor Machine Translation (EAMT?2011), pages 73?80, Leuven.Peter Welinder, Steve Branson, Serge Belongie, andPietro Perona.
2010.
The Multidimensional Wis-dom of Crowds.
In Advances in Neural InformationProcessing Systems, volume 23, pages 2424?2432.Jacob Whitehill, Paul Ruvolo, Ting-fan Wu, JacobBergsma, and Javier Movellan.
2009.
Whose voteshould count more: Optimal integration of labelsfrom labelers of unknown expertise.
Advances inNeural Information Processing Systems, 22:2035?2043.42
