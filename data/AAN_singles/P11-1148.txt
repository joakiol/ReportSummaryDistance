Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1476?1485,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLatent Semantic Word Sense Induction and DisambiguationTim Van de CruysRCEALUniversity of CambridgeUnited Kingdomtv234@cam.ac.ukMarianna ApidianakiAlpage, INRIA & Univ Paris DiderotSorbonne Paris Cite?, UMRI-00175013 Paris, Francemarianna.apidianaki@inria.frAbstractIn this paper, we present a unified model forthe automatic induction of word senses fromtext, and the subsequent disambiguation ofparticular word instances using the automati-cally extracted sense inventory.
The inductionstep and the disambiguation step are based onthe same principle: words and contexts aremapped to a limited number of topical dimen-sions in a latent semantic word space.
The in-tuition is that a particular sense is associatedwith a particular topic, so that different sensescan be discriminated through their associationwith particular topical dimensions; in a similarvein, a particular instance of a word can be dis-ambiguated by determining its most importanttopical dimensions.
The model is evaluated onthe SEMEVAL-2010 word sense induction anddisambiguation task, on which it reaches state-of-the-art results.1 IntroductionWord sense induction (WSI) is the task of automati-cally identifying the senses of words in texts, with-out the need for handcrafted resources or manuallyannotated data.
The manual construction of a senseinventory is a tedious and time-consuming job, andthe result is highly dependent on the annotators andthe domain at hand.
By applying an automatic proce-dure, we are able to only extract the senses that areobjectively present in a particular corpus, and it al-lows for the sense inventory to be straightforwardlyadapted to a new domain.Word sense disambiguation (WSD), on the otherhand, is the closely related task of assigning a senselabel to a particular instance of a word in context,using an existing sense inventory.
The bulk of WSDalgorithms up till now use pre-defined sense inven-tories (such as WordNet) that often contain fine-grained sense distinctions, which poses serious prob-lems for computational semantic processing (Ideand Wilks, 2007).
Moreover, most WSD algorithmstake a supervised approach, which requires a signifi-cant amount of manually annotated training data.The model presented here induces the senses ofwords in a fully unsupervised way, and subsequentlyuses the induced sense inventory for the unsuper-vised disambiguation of particular occurrences ofwords.
The induction step and the disambiguationstep are based on the same principle: words andcontexts are mapped to a limited number of topicaldimensions in a latent semantic word space.
Thekey idea is that the model combines tight, synonym-like similarity (based on dependency relations) withbroad, topical similarity (based on a large ?bag ofwords?
context window).
The intuition in this is thatthe dependency features can be disambiguated bythe topical dimensions identified by the broad con-textual features; in a similar vein, a particular in-stance of a word can be disambiguated by determin-ing its most important topical dimensions (based onthe instance?s context words).The paper is organized as follows.
Section 2presents some previous research on distributionalsimilarity and word sense induction.
Section 3 givesan overview of our method for word sense inductionand disambiguation.
Section 4 provides a quantita-tive evaluation and comparison to other algorithmsin the framework of the SEMEVAL-2010 word sense1476induction and disambiguation (WSI/WSD) task.
Thelast section draws conclusions, and lays out a num-ber of future research directions.2 Previous Work2.1 Distributional similarityAccording to the distributional hypothesis of mean-ing (Harris, 1954), words that occur in similar con-texts tend to be semantically similar.
In the spiritof this by now well-known adage, numerous algo-rithms have sprouted up that try to capture the se-mantics of words by looking at their distribution intexts, and comparing those distributions in a vectorspace model.One of the best known models in this respect islatent semantic analysis ?
LSA (Landauer and Du-mais, 1997; Landauer et al, 1998).
In LSA, a term-document matrix is created, that contains the fre-quency of each word in a particular document.
Thismatrix is then decomposed into three other matriceswith a mathematical factorization technique calledsingular value decomposition (SVD).
The most im-portant dimensions that come out of the SVD are saidto represent latent semantic dimensions, accordingto which nouns and documents can be representedmore efficiently.
Our model also applies a factoriza-tion technique (albeit a different one) in order to finda reduced semantic space.Context is a determining factor in the nature ofthe semantic similarity that is induced.
A broad con-text window (e.g.
a paragraph or document) yieldsbroad, topical similarity, whereas a small contextyields tight, synonym-like similarity.
This has leada number of researchers to use the dependency rela-tions that a particular word takes part in as contex-tual features.
One of the most important approachesis Lin (1998).
An overview of dependency-basedsemantic space models is given in Pado?
and Lapata(2007).2.2 Word sense inductionThe following paragraphs provide a succinctoverview of word sense induction research.
A thor-ough survey on word sense disambiguation (includ-ing unsupervised induction algorithms) is presentedin Navigli (2009).Algorithms for word sense induction can roughlybe divided into local and global ones.
Local WSIalgorithms extract the different senses of a word ona per-word basis, i.e.
the different senses for eachword are determined separately.
They can be furthersubdivided into context-clustering algorithms andgraph-based algorithms.
In the context-clusteringapproach, context vectors are created for the differ-ent instances of a particular word, and those con-texts are grouped into a number of clusters, repre-senting the different senses of the word.
The con-text vectors may be represented as first or second-order co-occurrences (i.e.
the contexts of the targetword are similar if the words they in turn co-occurwith are similar).
The first one to propose this ideaof context-group discrimination was Schu?tze (1998),and many researchers followed a similar approachto sense induction (Purandare and Pedersen, 2004).In the graph-based approach, on the other hand, aco-occurrence graph is created, in which nodes rep-resent words, and edges connect words that appearin the same context (dependency relation or contextwindow).
The senses of a word may then be discov-ered using graph clustering techniques (Widdowsand Dorow, 2002), or algorithms such as HyperLex(Ve?ronis, 2004) or Pagerank (Agirre et al, 2006).
Fi-nally, Bordag (2006) recently proposed an approachthat uses word triplets to perform word sense induc-tion.
The underlying idea is the ?one sense per col-location?
assumption, and co-occurrence triplets areclustered based on the words they have in common.Global algorithms take an approach in which thedifferent senses of a particular word are determinedby comparing them to, and demarcating them from,the senses of other words in a full-blown word spacemodel.
The best known global approach is the oneby Pantel and Lin (2002).
They present a globalclustering algorithm ?
coined clustering by commit-tee (CBC) ?
that automatically discovers word sensesfrom text.
The key idea is to first discover a set oftight, unambiguous clusters, to which possibly am-biguous words can be assigned.
Once a word hasbeen assigned to a cluster, the features associatedwith that particular cluster are stripped off the word?svector.
This way, less frequent senses of the wordmay be discovered.Van de Cruys (2008) proposes a model for senseinduction based on latent semantic dimensions.
Us-ing an extension of non-negative matrix factoriza-1477tion, the model induces a latent semantic spaceaccording to which both dependency features andbroad contextual features are classified.
Using thelatent space, the model is able to discriminate be-tween different word senses.
The model presentedbelow is an extension of this approach: whereas themodel described in Van de Cruys (2008) is only ableto perform word sense induction, our model is ca-pable of performing both word sense induction anddisambiguation.3 Methodology3.1 Non-negative Matrix FactorizationOur model uses non-negative matrix factorization ?NMF (Lee and Seung, 2000) in order to find latentdimensions.
There are a number of reasons to preferNMF over the better known singular value decompo-sition used in LSA.
First of all, NMF allows us to min-imize the Kullback-Leibler divergence as an objec-tive function, whereas SVD minimizes the Euclideandistance.
The Kullback-Leibler divergence is bettersuited for language phenomena.
Minimizing the Eu-clidean distance requires normally distributed data,and language phenomena are typically not normallydistributed.
Secondly, the non-negative nature of thefactorization ensures that only additive and no sub-tractive relations are allowed.
This proves partic-ularly useful for the extraction of semantic dimen-sions, so that the NMF model is able to extract muchmore clear-cut dimensions than an SVD model.
Andthirdly, the non-negative property allows the result-ing model to be interpreted probabilistically, whichis not straightforward with an SVD factorization.The key idea is that a non-negative matrix A isfactorized into two other non-negative matrices, Wand HAi?j ?Wi?kHk?j (1)where k is much smaller than i, j so that both in-stances and features are expressed in terms of a fewcomponents.
Non-negative matrix factorization en-forces the constraint that all three matrices must benon-negative, so all elements must be greater than orequal to zero.Using the minimization of the Kullback-Leiblerdivergence as an objective function, we want tofind the matrices W and H for which the Kullback-Leibler divergence between A and WH (the multipli-cation of W and H) is the smallest.
This factoriza-tion is carried out through the iterative applicationof update rules.
Matrices W and H are randomlyinitialized, and the rules in 2 and 3 are iteratively ap-plied ?
alternating between them.
In each iteration,each vector is adequately normalized, so that all di-mension values sum to 1.Ha?
?
Ha??iWiaAi?(WH)i?
?kWka(2)Wia ?Wia??Ha?Ai?(WH)i?
?vHav(3)3.2 Word sense inductionUsing an extension of non-negative matrix factoriza-tion, we are able to jointly induce latent factors forthree different modes: words, their window-based(?bag of words?)
context words, and their depen-dency relations.
Three matrices are constructed thatcapture the pairwise co-occurrence frequencies forthe different modes.
The first matrix contains co-occurrence frequencies of words cross-classified bydependency relations, the second matrix containsco-occurrence frequencies of words cross-classifiedby words that appear in the noun?s context window,and the third matrix contains co-occurrence frequen-cies of dependency relations cross-classified by co-occurring context words.
NMF is then applied to thethree matrices and the separate factorizations are in-terleaved (i.e.
the results of the former factorizationare used to initialize the factorization of the next ma-trix).
A graphical representation of the interleavedfactorization algorithm is given in figure 1.The procedure of the algorithm goes as follows.First, matrices W, H, G, and F are randomly initial-ized.
We then start our first iteration, and computethe update of matrix W (using equation 3).
MatrixW is then copied to matrix V, and the update ofmatrix G is computed (using equation 2).
The trans-pose of matrix G is again copied to matrix U, andthe update ofF is computed (again using equation 2).As a last step, matrix F is copied to matrix H, andwe restart the iteration loop until a stopping criterion(e.g.
a maximum number of iterations, or no moresignificant change in objective function; we used the1478= xW H= xV G= xU FjiskijkAwords xdependency relationsBwords xcontext wordsCcontext words xdependency relationsk kkkijisjs sFigure 1: A graphical representation of the interleavedNMF algorithmformer one) is reached.1 When the factorization isfinished, the three different modes (words, window-based context words and dependency relations) areall represented according to a limited number of la-tent factors.Next, the factorization that is thus created is usedfor word sense induction.
The intuition is that a par-ticular, dominant dimension of an ambiguous wordis ?switched off?, in order to reveal other possiblesenses of the word.
Formally, we proceed as follows.Matrix H indicates the importance of each depen-dency relation given a topical dimension.
With thisknowledge, the dependency relations that are respon-sible for a certain dimension can be subtracted fromthe original noun vector.
This is done by scalingdown each feature of the original vector accordingto the load of the feature on the subtracted dimen-sion, using equation 4.t = v(u1 ?
hk) (4)Equation 4 multiplies each dependency feature ofthe original noun vector v with a scaling factor, ac-cording to the load of the feature on the subtracteddimension (hk ?
the vector of matrix H that corre-sponds to the dimension we want to subtract).
u1 isa vector of ones with the same length as hk.
The re-sult is vector t, in which the dependency features rel-1Note that this is not the only possibly way of interleavingthe different factorizations, but in our experiments we found thatdifferent constellations lead to similar results.evant to the particular topical dimension have beenscaled down.In order to determine which dimension(s) are re-sponsible for a particular sense of the word, themethod is embedded in a clustering approach.
First,a specific word is assigned to its predominant sense(i.e.
the most similar cluster).
Next, the dominantsemantic dimension(s) for this cluster are subtractedfrom the word vector, and the resulting vector isfed to the clustering algorithm again, to see if otherword senses emerge.
The dominant semantic dimen-sion(s) can be identified by folding vector c ?
repre-senting the cluster centroid ?
into the factorization(equation 5).
This yields a probability vector b overlatent factors for the particular centroid.b = cHT (5)A simple k-means algorithm is used to com-pute the initial clustering, using the non-factorizeddependency-based feature vectors (matrix A).
k-means yields a hard clustering, in which each nounis assigned to exactly one (dominant) cluster.
In thesecond step, we determine for each noun whetherit can be assigned to other, less dominant clusters.First, the salient dimension(s) of the centroid towhich the noun is assigned are determined.
The cen-troid of the cluster is computed by averaging the fre-quencies of all cluster elements except for the tar-get word we want to reassign.
After subtracting thesalient dimensions from the noun vector, we checkwhether the vector is reassigned to another clustercentroid.
If this is the case, (another instance of) thenoun is assigned to the cluster, and the second stepis repeated.
If there is no reassignment, we continuewith the next word.
The target element is removedfrom the centroid to make sure that only the dimen-sions associated with the sense of the cluster are sub-tracted.
When the algorithm is finished, each nounis assigned to a number of clusters, representing itsdifferent senses.We use two different methods for selecting the fi-nal number of candidate senses.
The first method,NMFcon , takes a conservative approach, and onlyselects candidate senses if ?
after the subtraction ofsalient dimensions ?
another sense is found that ismore similar2 to the adapted noun vector than the2We use the cosine measure for our similarity calculations.1479dominant sense.
The second method, NMFlib , ismore liberal, and also selects the next best clustercentroid as candidate sense until a certain similaritythreshold ?
is reached.33.3 Word sense disambiguationThe sense inventory that results from the inductionstep can now be used for the disambiguation of in-dividual instances as follows.
For each instance ofthe target noun, we extract its context words, i.e.
thewords that co-occur in the same paragraph, and rep-resent them as a probability vector f .
Using matrixG from our factorization model (which representscontext words by semantic dimensions), this vectorcan be folded into the semantic space, thus represent-ing a probability vector over latent factors for theparticular instance of the target noun (equation 6).d = fGT (6)Likewise, the candidate senses of the noun (repre-sented as centroids) can be folded into our seman-tic space using matrix H (equation 5).
This yieldsa probability distribution over the semantic dimen-sions for each centroid.
As a last step, we com-pute the Kullback-Leibler divergence between thecontext vector and the candidate centroids, and se-lect the candidate centroid that yields the lowest di-vergence as the correct sense.
The disambiguationprocess is represented graphically in figure 2.3.4 ExampleLet us clarify the process with an example for thenoun chip.
The sense induction algorithm finds thefollowing candidate senses:41. cache, CPU, memory, microprocessor, proces-sor, RAM, register2.
bread, cake, chocolate, cookie, recipe, sand-wich3.
accessory, equipment, goods, item, machinery,material, product, supplies3Experimentally (examining the cluster output), we set ?
=0.24Note that we do not use the word sense to hint at a lexico-graphic meaning distinction; rather, sense in this case should beregarded as a more coarse-grained and topic-related entity.G'ksscontext vectorkcluster centroidjcluster centroidjcluster centroidjH'kjkkkFigure 2: Graphical representation of the disambiguationprocessEach candidate sense is associated with a centroid(the average frequency vector of the cluster?s mem-bers), that is folded into the semantic space, whichyields a ?semantic fingerprint?, i.e.
a distributionover the semantic dimensions.
For the first sense,the ?computer?
dimension will be the most impor-tant.
Likewise, for the second and the third sense the?food?
dimension and the ?manufacturing?
dimensionwill be the most important.5Let us now take a particular instance of the nounchip, such as the one in (1).
(1) An N.V. Philips unit has created a com-puter system that processes video images3,000 times faster than conventional systems.Using reduced instruction - set comput-ing, or RISC, chips made by Intergraph ofHuntsville, Ala., the system splits the im-age it ?sees?
into 20 digital representations,each processed by one chip.Looking at the context of the particular instance ofchip, a context vector is created which representsthe semantic content words that appear in the sameparagraph (the extracted content words are printedin boldface).
This context vector is again foldedinto the semantic space, yielding a distribution overthe semantic dimensions.
By selecting the lowest5In the majority of cases, the induced dimensions indeedcontain such clear-cut semantics, so that the dimensions can berightfully labeled as above.1480Kullback-Leibler divergence between the semanticprobability distribution of the target instance and thesemantic probability distributions of the candidatesenses, the algorithm is able to assign the ?computer?sense of the target noun chip.4 Evaluation4.1 DatasetOur word sense induction and disambiguationmodel is trained and tested on the dataset of theSEMEVAL-2010 WSI/WSD task (Manandhar et al,2010).
The SEMEVAL-2010 WSI/WSD task is basedon a dataset of 100 target words, 50 nouns and 50verbs.
For each target word, a training set is pro-vided from which the senses of the word have tobe induced without using any other resources.
Thetraining set for a target word consists of a set oftarget word instances in context (sentences or para-graphs).
The complete training set contains 879,807instances, viz.
716,945 noun and 162,862 verb in-stances.The senses induced during training are used fordisambiguation in the testing phase.
In this phase,the system is provided with a test set that consistsof unseen instances of the target words.
The testset contains 8,915 instances in total, of which 5,285nouns and 3,630 verbs.
The instances in the testset are tagged with OntoNotes senses (Hovy et al,2006).
The system needs to disambiguate these in-stances using the senses acquired during training.4.2 Implementational detailsThe SEMEVAL training set has been part of speechtagged and lemmatized with the Stanford Part-Of-Speech Tagger (Toutanova and Manning, 2000;Toutanova et al, 2003) and parsed with Malt-Parser (Nivre et al, 2006), trained on sections 2-21 of the Wall Street Journal section of the PennTreebank extended with about 4000 questions fromthe QuestionBank6 in order to extract dependencytriples.
The SEMEVAL test set has only been taggedand lemmatized, as our disambiguation model doesnot use dependency triples as features (contrary tothe induction model).6http://maltparser.org/mco/english_parser/engmalt.htmlWe constructed two different models ?
one fornouns and one for verbs.
For each model, the matri-ces needed for our interleaved NMF factorization areextracted from the corpus.
The noun model was builtusing 5K nouns, 80K dependency relations, and 2Kcontext words (excluding stop words) with highestfrequency in the training set, which yields matricesof 5K nouns ?
80K dependency relations, 5K nouns?
2K context words, and 80K dependency relations?
2K context words.
The model for verbs was con-structed analogously, using 3K verbs, and the samenumber of dependency relations and context words.For our initial k-means clustering, we set k = 600for nouns, and k = 400 for verbs.
For the under-lying interleaved NMF model, we used 50 iterations,and factored the model to 50 dimensions.4.3 Evaluation measuresThe results of the systems participating in theSEMEVAL-2010 WSI/WSD task are evaluated bothin a supervised and in an unsupervised manner.The supervised evaluation in the SEMEVAL-2010WSI/WSD task follows the scheme of the SEMEVAL-2007 WSI task (Agirre and Soroa, 2007), with somemodifications.
One part of the test set is used as amapping corpus, which maps the automatically in-duced clusters to gold standard senses; the other partacts as an evaluation corpus.
The mapping betweenclusters and gold standard senses is used to tag theevaluation corpus with gold standard tags.
The sys-tems are then evaluated as in a standard WSD task,using recall.In the unsupervised evaluation, the inducedsenses are evaluated as clusters of instances whichare compared to the sets of instances tagged withthe gold standard senses (corresponding to classes).Two partitions are thus created over the test set ofa target word: a set of automatically generated clus-ters and a set of gold standard classes.
A number ofthese instances will be members of both one goldstandard class and one cluster.
Consequently, thequality of the proposed clustering solution is evalu-ated by comparing the two groupings and measuringtheir similarity.Two evaluation metrics are used during the unsu-pervised evaluation in order to estimate the qualityof the clustering solutions, the V-Measure (Rosen-berg and Hirschberg, 2007) and the paired F-1481Score (Artiles et al, 2009).
V-Measure assesses thequality of a clustering by measuring its homogeneity(h) and its completeness (c).
Homogeneity refers tothe degree that each cluster consists of data pointsprimarily belonging to a single gold standard class,while completeness refers to the degree that eachgold standard class consists of data points primarilyassigned to a single cluster.
V-Measure is the har-monic mean of h and c.VM =2 ?
h ?
ch+ c(7)In the paired F-Score (Artiles et al, 2009) eval-uation, the clustering problem is transformed into aclassification problem (Manandhar et al, 2010).
Aset of instance pairs is generated from the automati-cally induced clusters, which comprises pairs of theinstances found in each cluster.
Similarly, a set of in-stance pairs is created from the gold standard classes,containing pairs of the instances found in each class.Precision is then defined as the number of commoninstance pairs between the two sets to the total num-ber of pairs in the clustering solution (cf.
formula 8).Recall is defined as the number of common instancepairs between the two sets to the total number ofpairs in the gold standard (cf.
formula 9).
Preci-sion and recall are finally combined to produce theharmonic mean (cf.
formula 10).P =|F (K) ?
F (S)||F (K)|(8)R =|F (K) ?
F (S)||F (S)|(9)FS =2 ?
P ?RP +R(10)The obtained results are also compared to twobaselines.
The most frequent sense (MFS) baselinegroups all testing instances of a target word into onecluster.
The Random baseline randomly assigns aninstance to one of the clusters.7 This baseline is exe-cuted five times and the results are averaged.7The number of clusters in Random was chosen to beroughly equal to the average number of senses in the gold stan-dard.4.4 Results4.4.1 Unsupervised evaluationIn table 1, we present the performance of a numberof algorithms on the V-measure.
We compare ourV-measure scores with the scores of the best-rankedsystems in the SEMEVAL 2010 WSI/WSD task, bothfor the complete data set and for nouns and verbsseparately.
The fourth column shows the averagenumber of clusters induced in the test set by eachalgorithm.
The MFS baseline has a V-Measure equalto 0, since by definition its completeness is 1 and itshomogeneity is 0.NMFcon ?
our model that takes a conservative ap-proach in the induction of candidate senses ?
doesnot beat the random baseline.
NMFlib ?
our modelthat is more liberal in inducing senses ?
reaches bet-ter results.
With 11.8%, it scores similar to otheralgorithms that induce a similar average number ofclusters, such as Duluth-WSI (Pedersen, 2010).Pedersen (2010) has shown that the V-Measuretends to favour systems producing a higher numberof clusters than the number of gold standard senses.This is reflected in the scores of our models as well.VM (%) all noun verb #clHermit 16.2 16.7 15.6 10.78UoY 15.7 20.6 8.5 11.54KSU KDD 15.7 18.0 12.4 17.50NMFlib 11.8 13.5 9.4 4.80Duluth-WSI 9.0 11.4 5.7 4.15Random 4.4 4.2 4.6 4.00NMFcon 3.9 3.9 3.9 1.58MFS 0.0 0.0 0.0 1.00Table 1: Unsupervised V-measure evaluation on SE-MEVAL test setMotivated by the large divergences in the sys-tem rankings on the different metrics used in theSEMEVAL-2010 WSI/WSD task, Pedersen evaluatedthe metrics themselves.
His evaluation relied onthe assumption that a good measure should assignlow scores to random baselines.
Pedersen showedthat the V-Measure continued to improve as random-ness increased.
We agree with Pedersen?s conclu-sion that the V-Measure results should be interpretedwith caution, but we still report the results in order1482to perform a global comparison, on all metrics, ofour system?s performance to the systems that partic-ipated to the SEMEVAL task.Contrary to V-Measure, paired F-score is a fairlyreliable measure and the only one that managed toidentify and expose random baselines in the abovementioned metric evaluation.
This means that therandom systems used for testing were ranked lowwhen a high number of random senses was used.In table 2, the paired F-Score of a number of al-gorithms is given.
The paired F-Score penalizes sys-tems when they produce a higher number of clusters(low recall) or a lower number of clusters (low pre-cision) than the gold standard number of senses.
Weagain compare our results with the scores of the best-ranked systems in the SEMEVAL-2010 WSI/WSDTASK.FS (%) all noun verb #clMFS 63.5 57.0 72.7 1.00Duluth-WSI-SVD-Gap 63.3 57.0 72.4 1.02NMFcon 60.2 54.6 68.4 1.58NMFlib 45.3 42.2 49.8 5.42Duluth-WSI 41.1 37.1 46.7 4.15Random 31.9 30.4 34.1 4.00Table 2: Unsupervised paired F-score evaluation on SE-MEVAL testsetNMFcon reaches a score of 60.2%, which is againsimilar to other algorithms that induce the same av-erage number of clusters.
NMFlib scores 45.3%, in-dicating that the algorithm is able to retain a rea-sonable F-Score while at the same time inducing asignificant number of clusters.
This especially be-comes clear when comparing its score to the otheralgorithms.4.4.2 Supervised evaluationIn the supervised evaluation, the automatically in-duced clusters are mapped to gold standard senses,using the mapping corpus (i.e.
one part of the testset).
The obtained mapping is used to tag the evalu-ation corpus (i.e.
the other part of the test set) withgold standard tags, which means that the methodsare evaluated in a standard WSD task.Table 3 shows the recall of our algorithms in thesupervised evaluation, again compared to other algo-rithms evaluated in the SEMEVAL-2010 WSI/WSDtask.SR (%) all noun verb #SNMFlib 62.6 57.3 70.2 1.82UoY 62.4 59.4 66.8 1.51Duluth-WSI 60.5 54.7 68.9 1.66NMFcon 60.3 54.5 68.8 1.21MFS 58.7 53.2 66.6 1.00Random 57.3 51.5 65.7 1.53Table 3: Supervised recall for SEMEVAL testset, 80%mapping, 20% evaluationNMFlib gets 62.6%, which makes it the best scor-ing algorithm on the supervised evaluation.
NMFconreaches 60.3%, which again indicates that it is in thesame ballpark as other algorithms that induce a sim-ilar average number of senses.Some doubts have been cast on the representative-ness of the supervised recall results as well.
Accord-ing to Pedersen (2010), the supervised learning al-gorithm that underlies this evaluation method tendsto converge to the Most Frequent Sense (MFS) base-line, because the number of senses that the classi-fier assigns to the test instances is rather low.
Wethink these shortcomings indicate the need for thedevelopment of new evaluation metrics, capable ofproviding a more accurate evaluation of the perfor-mance of WSI systems.
Nevertheless, these metricsstill constitute a useful testbed for comparing the per-formance of different systems.5 Conclusion and future workIn this paper, we presented a model based on latentsemantics that is able to perform word sense induc-tion as well as disambiguation.
Using latent topi-cal dimensions, the model is able to discriminate be-tween different senses of a word, and subsequentlydisambiguate particular instances of a word.
Theevaluation results indicate that our model reachesstate-of-the-art performance compared to other sys-tems that participated in the SEMEVAL-2010 wordsense induction and disambiguation task.
Moreover,our global approach is able to reach similar perfor-mance on an evaluation set that is tuned to fit theneeds of local approaches.
The evaluation set con-1483tains an enormous amount of contexts for only asmall number of target words, favouring methodsthat induce senses on a per-word basis.
A globalapproach like ours is likely to induce a more bal-anced sense inventory using an unbiased corpus, andis likely to outperform local methods when such anunbiased corpus is used as input.
We therefore thinkthat the global, unified approach to word sense in-duction and disambiguation presented here providesa genuine and powerful solution to the problem athand.We conclude with some issues for future work.First of all, we would like to evaluate the approachpresented here using a more balanced and unbiasedcorpus, and compare its performance on such a cor-pus to local approaches.
Secondly, we would alsolike to include grammatical dependency informationin the disambiguation step of the algorithm.
For now,the disambiguation step only uses a word?s contextwords; enriching the feature set with dependency in-formation is likely to improve the performance ofthe disambiguation.AcknowledgmentsThis work is supported by the Scribo project, fundedby the French ?po?le de compe?titivite??
System@tic,and by the French national grant EDyLex (ANR-09-CORD-008).ReferencesEneko Agirre and Aitor Soroa.
2007.
SemEval-2007Task 02: Evaluating word sense induction and discrim-ination systems.
In Proceedings of the fourth Interna-tional Workshop on Semantic Evaluations (SemEval),ACL, pages 7?12, Prague, Czech Republic.Eneko Agirre, David Mart?
?nez, Ojer Lo?pez de Lacalle,and Aitor Soroa.
2006.
Two graph-based algo-rithms for state-of-the-art WSD.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP-06), pages 585?593, Syd-ney, Australia.Marianna Apidianaki and Tim Van de Cruys.
2011.
AQuantitative Evaluation of Global Word Sense Induc-tion.
In Proceedings of the 12th International Con-ference on Intelligent Text Processing and Computa-tional Linguistics (CICLing), published in SpringerLecture Notes in Computer Science (LNCS), volume6608, pages 253?264, Tokyo, Japan.Javier Artiles, Enrique Amigo?, and Julio Gonzalo.
2009.The role of named entities in web people search.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP-09), pages534?542, Singapore.Stefan Bordag.
2006.
Word sense induction: Triplet-based clustering and automatic evaluation.
In Proceed-ings of the 11th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL-06), pages 137?144, Trento, Italy.Zellig S. Harris.
1954.
Distributional structure.
Word,10(23):146?162.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In Proceedings of the Human Lan-guage Technology / North American Association ofComputational Linguistics conference (HLT-NAACL-06), pages 57?60, New York, NY.Nancy Ide and Yorick Wilks.
2007.
Making Sense AboutSense.
In Eneko Agirre and Philip Edmonds, editors,Word Sense Disambiguation, Algorithms and Applica-tions, pages 47?73.
Springer.Thomas Landauer and Susan Dumais.
1997.
A solutionto Plato?s problem: The Latent Semantic Analysis the-ory of the acquisition, induction, and representation ofknowledge.
Psychology Review, 104:211?240.Thomas Landauer, Peter Foltz, and Darrell Laham.
1998.An Introduction to Latent Semantic Analysis.
Dis-course Processes, 25:295?284.Daniel D. Lee and H. Sebastian Seung.
2000.
Algo-rithms for non-negative matrix factorization.
In Ad-vances in Neural Information Processing Systems, vol-ume 13, pages 556?562.Dekang Lin.
1998.
Automatic Retrieval and Clusteringof Similar Words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics (COLING-ACL98), volume 2, pages768?774, Montreal, Quebec, Canada.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dligach,and Sameer S. Pradhan.
2010.
SemEval-2010 Task14: Word Sense Induction & Disambiguation.
In Pro-ceedings of the fifth International Workshop on Seman-tic Evaluation (SemEval), ACL-10, pages 63?68, Upp-sala, Sweden.Roberto Navigli.
2009.
Word Sense Disambiguation: aSurvey.
ACM Computing Surveys, 41(2):1?69.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.
Malt-parser: A data-driven parser-generator for dependencyparsing.
In Proceedings of the fifth InternationalConference on Language Resources and Evaluation(LREC-06), pages 2216?2219, Genoa, Italy.1484Sebastian Pado?
and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33(2):161?199.Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,pages 613?619, Edmonton, Alberta, Canada.Ted Pedersen.
2010.
Duluth-WSI: SenseClusters Ap-plied to the Sense Induction Task of SemEval-2.
InProceedings of the fifth International Workshop on Se-mantic Evaluations (SemEval-2010), pages 363?366,Uppsala, Sweden.Amruta Purandare and Ted Pedersen.
2004.
WordSense Discrimination by Clustering Contexts in Vec-tor and Similarity Spaces.
In Proceedings of the Con-ference on Computational Natural Language Learning(CoNLL), pages 41?48, Boston, MA.Andrew Rosenberg and Julia Hirschberg.
2007.
V-measure: A conditional entropy-based external clus-ter evaluation measure.
In Proceedings of the Joint2007 Conference on Empirical Methods in NaturalLanguage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 410?420,Prague, Czech Republic.Hinrich Schu?tze.
1998.
Automatic Word Sense Discrim-ination.
Computational Linguistics, 24(1):97?123.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the Knowledge Sources Used in a Maxi-mum Entropy Part-of-Speech Tagger.
In Proceedingsof the Joint SIGDAT Conference on Empirical Meth-ods in Natural Language Processing and Very LargeCorpora (EMNLP/VLC-2000), pages 63?70.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.In Proceedings of the Human Language Technology/ North American Association of Computational Lin-guistics conference (HLT-NAACL-03, pages 252?259,Edmonton, Canada.Tim Van de Cruys.
2008.
Using Three Way Data forWord Sense Discrimination.
In Proceedings of the22nd International Conference on Computational Lin-guistics (COLING-08), pages 929?936, Manchester,UK.Jean Ve?ronis.
2004.
Hyperlex: lexical cartography forinformation retrieval.
Computer Speech & Language,18(3):223?252.Dominic Widdows and Beate Dorow.
2002.
A GraphModel for Unsupervised Lexical Acquisition.
In Pro-ceedings of the 19th International Conference on Com-putational Linguistics (COLING-02), pages 1093?1099, Taipei, Taiwan.1485
