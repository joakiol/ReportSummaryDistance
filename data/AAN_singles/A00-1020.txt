Multilingual Coreference ResolutionSanda M.  Harabag iuSouthern Methodist  UniversityDallas, TX  75275-0122sanda@seas, smu.
eduSteven J .
Ma ioranoIPOWashington,  D.C. 20505maiorano@cais, comAbst rac tIn this paper we present a new, multi-lingual data-driven method for coreferenceresolution as implemented in the SWIZZLEsystem.
The results obtained after trainingthis system on a bilingual corpus of Englishand Romanian tagged texts, outperformedcoreference r solution in each of the indi-vidual anguages.1 I n t roduct ionThe recent availability of large bilingual corpora hasspawned interest in several areas of multilingual textprocessing.
Most of the research as focused onbilingual terminology identification, either as par-allel multiwords forms (e.g.
the ChampoUion sys-tem (Smadja et a1.1996)), technical terminology (e.g.the Termight system (Dagan and Church, 1994) orbroad-coverage translation lexicons (e.g.
the SABLEsystem (Resnik and Melamed, 1997)).
In addition,the Multilingual Entity Task (MET) from the TIP-STER program 1 (http://www-nlpir.nist.gov/related-projeets/tipster/met.htm) challenged the partici-pants in the Message Understanding Conference(MUC) to extract named entities across everal for-eign language corpora, such as Chinese, Japaneseand Spanish.In this paper we present a new application ofaligned multilinguai texts.
Since coreference r so-lution is a pervasive discourse phenomenon causingperformance impediments in current IE systems, weconsidered a corpus of aligned English and Roma-nian texts to identify coreferring expressions.
Ourtask focused on the same kind of coreference asconsidered in the past MUC competitions, namely1The TIPSTER Text Program was a DARPA-Iedgovernment effort o advance the state of the art in textprocessing technologies.the identity coreference.
Identity coreference linksnouns, pronouns and noun phrases (including propernames) to their corresponding antecedents.We created our bilingual collection by translatingthe MUC-6 and MUC-7 coreference training textsinto Romanian using native speakers.
The train-ing data set for Romanian coreference used, wher-ever possible, the same coreference identifiers as theEnglish data and incorporated additional tags asneeded.
Our claim is that by adding the wealthof coreferential features provided by multilingualdata, new powerful heuristics for coreference r solu-tion can be developed that outperform onolingualcoreference r solution systems.For both languages, we resolved coreference byusing SWIZZLE, our implementation f a bilingualcoreference r solver.
SWIZZLE is a multilingual en-hancement of COCKTAIL (Harabagiu and Maiorano,1999), a coreference r solution system that operateson a mixture of heuristics that combine semanticand textual cohesive information 2.
When COCKTAILwas applied separately on the English and the Ro-manian texts, coreferring links were identified foreach English and Romanian document respectively.When aligned referential expressions corefer withnon-aligned anaphors, SWIZZLE derived new heuris-tics for coreference.
Our experiments show thatSWIZZLE outperformed COCKTAIL on both Englishand Romanian test documents.The rest of the paper is organized as follows.
Sec-tion 2 presents COCKTAIL, a monolingnai coreferenceresolution system used separately on both the En-glish and Romanian texts.
Section 3 details thedata-driven approach used in SWIZZLE and presentssome of its resources.
Section 4 reports and discussesthe experimental results.
Section 5 summarizes the2The name of COCKTAIL is a pun on CogNIAC be-cause COCKTAIL combines a larger number of heuristicsthan those reported in (Baldwin, 1997).
SWIZZLE, more-over, adds new heuristics, discovered from the bilingualaligned corpus.142conclusions.2 COCKTAILCurrently, some of the best-performing andmost robust coreference r solution systems employknowledge-based techniques.
Traditionally, thesetechniques have combined extensive syntactic, se-mantic, and discourse knowledge.
The acquisitionof such knowledge is time-consuming, difficult, anderror-prone.
Nevertheless, recent results show thatknowledge-poor methods perform with amazing ac-curacy (cf.
(Mitkov, 1998), (Kennedy and Boguraev,1996) (Kameyama, 1997)).
For example, CogNIAC(Baldwin, 1997), a system based on seven orderedheuristics, generates high-precision resolution (over90%) for some cases of pronominal reference.
Forthis research, we used a coreference r solution sys-tem ((Harabagiu and Malorano, 1999)) that imple-ments different sets of heuristics corresponding tovarious forms of coreference.
This system, calledCOCKTAIL, resolves coreference by exploiting severaltextual cohesion constraints (e.g.
term repetition)combined with lexical and textual coherence cues(e.g.
subjects of communication verbs are morelikely to refer to the last person mentioned in thetext).
These constraints are implemented as a set ofheuristics ordered by their priority.
Moreover, theCOCKTAIL framework uniformly addresses the prob-lem of interaction between different forms of coref-erence, thus making the extension t  multilingualcoreference very natural.2.1 Data -Dr iven  Coreference Reso lu t ionIn general, we define a data-driven methodology asa sequence of actions that captures the data pat-terns capable of resolving a problem with both ahigh degree of precision and recall.
Our data-drivenmethodology reported here generated sets of heuris-tics for the coreference r solution problem.
Precisionis the number of correct references out of the totalnumber of coreferences resolved, whereas the recallmeasures the number of resolved references out ofthe total number of keys, i.e., the annotated coref-erence data.The data-driven methodology used in COCKTAIL iscentered around the notion of a coreference chain.Due to the transitivity of coreference relations, kcoreference r lations having at least one common ar-gument generate k + 1 core/erring expressions.
Thetext position induces an order among coreferring ex-pressions.
A coreference structure is created whena set of coreferring expressions are connected in anoriented graph such that each node is related onlyto one of its preceding nodes.
In turn, a corefer-ence chain is the coreference structure in which ev-ery node is connected to its immediately precedingnode.
Clearly, multiple coreference structures for thesame set of coreferring expressions can be mappedto a single coreference chain.
As an example, bothcoreference structures illustrated in Figure l(a) and(c) are cast into the coreference chain illustrated inFigure l(b).TEXT TEXT TEXTi [ ](a) (b) (c)Figure 1: Three coreference structures.Given a corpus annotated with coreference data,the data-driven methodology first generates allcoreference chains in the data set and then con-siders all possible combinations of coreference re-lations that would generate the same coreferencechains.
For a coreference chain of length l withnodes nl, n2, ... nt+l, each node nk ( l<k~/)  canbe connected to any of the l - k nodes precedingit.
From this observation, we find that a numberof 1 x 2 x ... x (l - k)... x I = l!
coreference struc-tures can generate the same coreference chain.
Thisresult is very important, since it allows for the auto-matic generation of coreference data.
For each coref-erence relation T~ from an annotated corpus we cre-ated a median of (l - 1)!
new coreference r lations,where l is the length of the coreference hain contain-ing relation 7~.
This observation gave us the possi-bility of expanding the test data provided by thecoreference keys available in the MUC-6 and MUC-7 competitions (MUC-6 1996), (MUC-7 1998).
TheMUC-6 coreference annotated corpus contains 1626coreference r lations, while the MUC-7 corpus has2245 relations.
The average length of a coreferencechain is 7.21 for the MUC-6 data, and 8.57 for theMUC-7 data.
We were able to expand the numberof annotated coreference relations to 6,095,142 forthe MUC-6 corpus and to 8,269,403 relations for theMUC-7 corpus; this represents an expansion factorof 3,710.
We are not aware of any other automatedway of creating coreference annotated ata, and webelieve that much of the COCKTAIL's impressive per-formance is due to the plethora of data provided bythis method.143Heuristics for 3rd person pronounsoHeuristie 1-Pronoun(H1Pron)Search in the same sentence for the same3rd person pronoun Pros'if (Pron' belongs to coreference chain CC)and there is an element from CC which isclosest o Pron in Text, Pick that element.else Pick Pron'.oHeuristic 2-Pronoun(H2Pron)Search for PN, the closest proper name from Pronif (PN agrees in number and gender with Pros)if (PN belongs to coreference chain CC)then Pick the element from CC which isclosest o Pros in Text.else Pick PN.o Heuristic 3- Pronoun( H3Pron )Search for Noun, the closest noun from Prosif (Noun agrees in number and gender with Pros)i f  (Noun belongs to coreference chain CC)and there is an element from CC which isclosest o Pros in Text, Pick that element.else Pick NounHeuristics for nominal referenceo Heuristic 1-Nominal(HINom )if (Noun is the head of an appositive)then Pick the preceding NP.oHeuristic 2-Nominal(H2Nom)if (Noun belongs to an NP, Search for NP'such that Noun'=same_name(head(NP),head(NP'))orNoun'--same_name(adjunct(NP), adjunct(NP')))then if (Noun' belongs to coreference chain CC)then Pick the element from CC which isclosest o Noun in Text.else Pick Noun'.oHeuristic 3-Nominal(H3Nom)if Noun is the head of an NPthen Search for proper name PNsuch that head(PN)=Nounif (PN belongs to coreference chain CC)and there is an element from CC which isclosest o Noun in Text, Pick that element.else Pick PN.Table 1: Best performing heuristics implemented in COCKTAIL2.2 Knowledge-Poor  Core ferenceReso lu t ionThe result of our data-driven methodology is theset of heuristics implemented in COCKTAIL whichcover both nominal and pronoun coreference.
Eachheuristic represents a pattern of coreference thatwas mined from the large set of coreference data.COCKTAIL uses knowledge-poor methods because (a)it is based only on a limited number of heuristicsand (b) text processing is limited to part-of-speechtagging, named-entity recognition, and approximatephrasal parsing.
The heuristics from COCKTAIL canbe classified along two directions.
First of all, theycan be grouped according to the type of corefer-ence they resolve, e.g., heuristics that resolve theanaphors of reflexive pronouns operate differentlythan those resolving bare nominals.
Currently, inCOCKTAIL there are heuristics that resolve five typesof pronouns (personal, possessive, reflexive, demon-strative and relative) and three forms of nominals(definite, bare and indefinite).Secondly, for each type of coreference, there arethree classes of heuristics categorized according totheir suitability to resolve coreference.
The firstclass is comprised of strong indicators of coreference.This class resulted from the analysis of the distribu-tion of the antecedents in the MUC annotated ata.For example, repetitions of named entities and ap-positives account for the majority of the nominalcoreferences, and, therefore, represent anchors forthe first class of heuristics.The second class of coreference covers cases inwhich the arguments are recognized to be seman-tically consistent.
COCKTAIL's test of semantic on-sistency blends together information available fromWordNet and statistics gathered from Treebank.Different consistency checks are modeled for each ofthe heuristics.Example of the application of heuristic H2PronMr.
Adams1, 69 years old, is the retired chairmanof Canadian-based Emco Ltd., a maker of plumbingand petroleum equipment; he1 has served on theWoolworth board since 1981.Example of the application of heuristic H3Pron"We have got to stop pointing our fingers at thesekids2 who have no future," he said, "and reach ourhands out to them2.Example of the application of heuristic H2NomThe chairman and the chief executive officer3of Woolworth Corp. have temporarily relinquishedtheir posts while the retailer conducts its investi-gation into alleged accounting irregularities4.Woolworth's board named John W. Adams, anoutsider, to serve as interim chairman and executiveofficer3, while a special committee, appointed bythe board last week and led by Mr. Adams,investigates the alleged irregularities4.Table 2: Examples of coreference resolution.
Thesame annotated index indicates coreference.The third class of heuristics resolves coreferenceby coercing nominals.
Sometimes coercions involveonly derivational morphology - linking verbs withtheir nominalizations.
On other occasions, coercionsare obtained as paths of meronyms (e.g.
is-part re-lations) and hypernyms (e.g.
is-a relations).
Con-144.sistency checks implemented for this class of coref-erence are conservative: ither the adjuncts must beidentical or the adjunct of the referent must be lessspecific than the antecedent.
Table 1 lists the topperforming heuristics of COCKTAIL for pronominaland nominal coreference.
Examples of the heuristicsoperation on the MUC data are presented presentedin Table 2.
Details of the top performing heuris-tics of COCKTAIL were reported in (Harabagiu andMaiorano, 1999).2.3 Bootstrapping for CorefereneeResolutionOne of the major drawbacks of existing corefer-ence resolution systems is their inability to recog-nize many forms of coreference displayed by manyreal-world texts.
Recall measures of current systemsrange between 36% and 59% for both knowledge-based and statistical techniques.
Knowledge based-systems would perform better if more coreferenceconstraints were available whereas tatistical meth-ods would be improved if more annotated data wereavailable.
Since knowledge-based techniques out-perform inductive methods, we used high-precisioncoreference heuristics as knowledge seeds for ma-chine learning techniques that operate on largeamounts of unlabeled ata.
One such techniqueis bootstrapping, which was recently presented in(Riloff and Jones 1999), (Jones et a1.1999) as anideal framework for text learning tasks that haveknowledge seeds.
The method oes not require largetraining sets.
We extended COCKTAIL by using meta-bootstrapping of both new heuristics and clusters ofnouns that display semantic onsistency for corefer-ence.The coreference heuristics are the seeds of ourbootstrapping framework for coreference r solution.When applied to large collections of texts, theheuristics determine classes of coreferring expres-sions.
By generating coreference chains out of allthese coreferring expressions, often new heuristicsare uncovered.
For example, Figure 2 illustrates theapplication of three heuristics and the generation ofdata for a new heuristic rule.
In COCKTAIL, after aheuristic is applied, a new coreference chain is cal-culated.
For the example illustrated in Figure 2, ifthe reference of expression A is sought, heuristic H1indicates expression B to be the antecedent.
Whenthe coreference chain is built, expression A is di-rectly linked to expression D, thus uncovering a newheuristic H0.As a rule of thumb, we do not consider a newheuristic unless there is massive vidence of its cov-erage in the data.
To measure the coverage we usethe FOIL_Gain measure, as introduced by the FOILinductive algorithm (Cameron-Jones and Quinlan1993).
Let Ho be the new heuristic and/-/1 a heuris-tic that is already in the seed set.
Let P0 be the num-ber of positive coreference examples of Hn~w (i.e.the number of coreference r lations produced by theheuristic that can be found in the test data) and nothe number of negative xamples of/-/new (i.e.
thenumber of relations generated by the heuristic whichcannot be found in the test data).
Similarly, Pl andnl are the positive and negative xamples of Ha.The new heuristics are scored by their FOIL_Gaindistance to the existing set of heuristics, and the bestscoring one is added to the COCKTAIL system.
TheFOIL_Gain formula is:l og2- - -~ ) FOIL_Gain(H1, Ho) = k(log2 Pl nl  Po -k nowhere k is the number of positive examples cov-ered by both//1 and Ho.
Heuristic Ho is added tothe seed set if there is no other heuristic providinglarger FOIL_Gain to any of the seed heuristics.H3 .
j .
.
.~ IB  B\ [~  HO - New HeuristicFigure 2: Bootstrapping new heuristics.Since in COCKTAIL, semantic onsistency of core-ferring expressions is checked by comparing the sim-ilarity of noun classes, each new heuristic deter-mines the adjustment of the similarity threshold ofall known coreferring noun classes.
The steps ofthe bootstrapping algorithm that learns both newheuristics and adjusts the similarity threshold ofcoreferential expressions i :MUTUAL BOOTSTRAPPING LOOP1.
Score all candidate heuristics with FOIL_Gain2.
Best_h--closest candidate to heuristics(COCKTAIL)3.
Add Best_h to heuristics(COCKTAIL),f. Adjust semantic similarity threshold for semanticconsistency o\[ coreferring nouns5.
Goto step 1 if the precision and recall did notdegrade under minimal performance.
(Riloff and Jones 1999) note that the bootstrap-ping algorithm works well but its performance candeteriorate rapidly when non-coreferring data enteras candidate heuristics.
To make the algorithm orerobust, a second level of bootstrapping can be intro-duced.
The outer bootstrapping mechanism, called145recta-bootstrapping compiles the results of the inner(mutual) bootstrapping process and identifies the kmost reliable heuristics, where k is a number de-termined experimentally.
These k heuristics are re-tained and the rest of them are discarded.3 SWIZZLE3.1 Mul t iHngua l  Core ference  DataTo study the performance of a data-driven multi-lingual coreference r solution system, we prepared acorpus of Romanian texts by translating the MUC-6and MUC-7 coreference training texts.
The transla-tions were performed by a group of four Romaniannative speakers, and were checked for style by a cer-tified translator from Romania.
In addition, the Ro-manian texts were annotated with coreference keys.Two rules were followed when the annotations weredone:o 1: Whenever an expression ER represents a trans-lation of an expression EE from the correspondingEnglish text, if Es  is tagged as a coreference keywith identification umber ID, then the Romanianexpression ER is also tagged with the same ID num-ber.
This rule allows for translations in which thetextual position of the referent and the antecedenthave been swapped.o2: Since the translations often introduce newcoreferring expressions in the same chain, the newexpressions are given new, unused ID numbers.For example, Table 3 lists corresponding Englishand Romanian fragments of coreference chains fromthe original MUC-6 Wall Street Journal documentDOCNO: 930729-0143.Table 3 also shows the original MUC coreferenceSGML annotations.
Whenever present, the REF tagindicates the ID of the antecedent, whereas the MINtag indicates the minimal reference xpression.3.2 Lex ica l  ResourcesThe multilingual coreference resolution method im-plemented in SWIZZLE incorporates the heuristics de-rived from COKCTAIL's monolingual coreference res-olution processing in both languages.
To this end,COCKTAIL required both sets of texts to be taggedfor part-of-speech and to recognize the noun phrases.The English texts were parsed with Brill's part-of-speech tagger (Brill 1992) and the noun phrases wereidentified by the grammar ules implemented in thephrasal parser of FASTUS (Appelt et al, 1993).
Cor-responding resources are not available in Romanian.To minimize COCKTAIL's configuration for process-ing Romanian texts, we implemented a Romanianpart-of-speech rule-based tagger that used the sameEconomic adviser Gene Sperling described<COREF ID="29" TYPE=' IDENT" REF-"30">i t</COREF> as "a true full-court press" to pass<COREF ID="31" TYPE="IDENT" REF="26"MIN='bilr '  >the <COREF ID="32"TYPE="IDENT" REF-----"10" MIN="reduction"><COREF ID="33" TYPE="IDENT" REF="12">def i c i t</COREF>-reduct ion</COREF>bill, the final version of which is now beinghammered out by <COREF ID=" 43" >House</COREF> and <COREF ID="41" >Senate</COREF>negot ia tors</COREF>.<COREF ID=" 34" TYPE=" IDENT" REF-"  2" >The executives</COREF>' backing - however tepid- gives the administration a way to counter<COREF ID="35" TYPE="IDENT" REF="36">bus iness</COREF:> critics of <COREF ID="500"TYPE="IDENT" REF="31" MIN="package"STATUS=" OPT" >the  overall package</COREF>,.
.
.Consilierul cu probleme conomice Gene Sperling adescris-<COREF ID=" 29" TYPE="IDENT"REF="30" >o</COREF> ca pe un efort deavengur~ menit s~ promoveze <COREF ID=" 1125"TYPE="IDENT" REF="26" MIN="legea">legea</COREF> pentru <COREF TYPE="IDENT"REF=" 10" MIN="reducerea" > reducerea</COREF> <COREF ID=" 33" TYPE=" IDENT"REF=" 12"> deficitului n bugetul SUA</COREF>.Versiunea finals a acestei <COREF ID="1126"TYPE="IDENT" REF="l125" MIN="legi">legi</COI~EF> este desfiin~at~ chiax in acestezile in cadrul dezbaterilor ce au loc in<COREF ID="43" >Camera  Reprezentat lv i lor</CORJ~F> ?i in <COREF ID="41">Senat</COREF></COREF>.Sprijinirea <COREF ID="127" TYPE="IDENT"REF=" 1126" MIN="legii" >leg i i>/COREF>de c~tre speciali~ti in economic - de?iin manier~ moderat~ - ofer~ administra~iei omodalitate de a contrabalansa criticile aduse<COREF ID="500" TYPE="IDENT" REF="31"MIN=" legii" STATUS=" OPT" >leg i i</COREF>de c~tre companiile americane,...Table 3: Example of parallel English and Romaniantext annotated for coreference.
The elements from acoreference chain in the respective texts are under-lined.
The English text has only two elements in thecoreference chain, whereas the Romanian text con-tains four different elements.
The two additional ele-ments of the Romanian coreference chain are deriveddue to (1) the need to translate the relative clausefrom the English fragment into a separate sentencein Romanian; and (2) the reordering of words in thesecond sentence.146tags as generated by the Brill tagger.
In addition,we implemented rules that identify noun phrases inRomanian.To take advantage of the aligned corpus, SWIZZLEalso relied on bilingual exical resources that helptranslate the referential expressions.
For thispurpose, we used a core Romanian WordNet(Harabagiu, 1999) which encoded, wherever possi-ble, links between the English synsets and their Ro-manian counterparts.
This resource also incorpo-rated knowledge derived from several bilingual dic-tionaries (e.g.
(Bantam, 1969)).Having the parallel coreference annotations, wecan easily identify their translations because theyhave the same identification coreference k y. Look-ing at the example given in Table 3, the expres-sion "legii', with ID=500 is the translation of theexpression "package", having the same ID in theEnglish text.
However, in the test set, the REFfields are intentionally voided, entrusting COCKTAILto identify the antecedents.
The bilingual corefer-ence resolution performed in SWIZZLE, however, re-quires the translations ofthe English and Romanianantecedents.
The principles guiding the translationsof the English and Romanian antecedents (AE-Rand A R-E, respectively) are:?
Circularity: Given an English antecedent, due tosemantic ambiguity, it can belong to several EnglishWordNet sysnsets.
For each such sysnset S/~ we con-sider the Romanian corresponding sysnet(s) Sff.
Wefilter out all Sff that do not contain A E-R.
If onlyone Romanian sysnset is left, then we identified atranslation.
Otherwise, we start from the Roma-nian antecedent, find all synsets SR to which it be-longs, and obtain the corresponding English sysnetsS F. Similarly, all English synsets not containingthe English antecedent are filtered out.
If only onesynset remains, we have again identified a transla-tion.
Finally, in the last case, the intersection ofthe multiple synsets in either language generates alegal translation.
For example, the English synsetS E ={bill, measure} translates into the Romaniansynset S R ={lege}.
First, none of the dictionarytranslations of bill into Romanian (e.g.
politE, bac-notE, afi~) translate back into any of the elementsof S E. However the translation of measure into theRomanian lege translates back into bill, its synonym.?
Semantic density: Given an English and a Roma-nian antecedent, to establish whether they are trans-lations of one another, we disambiguate them by firstcollapsing all sysnsets that have common elements.Then we apply the circularity principle, relying onthe semantic alignment encoded in the RomanianWordNet.
When this core lexical database was firstimplemented, several other principles were applied.In our experiment, we were satisfied with the qual-ity of the translations recognized by following onlythese two principles.3.3 Mult i l ingual Coreference ResolutionThe SWIZZLE system was run on a corpus of 2335referential expressions in English (927 from MUC-6 and 1408 from MUC-7) and 2851 Romanian ex-pressions (1219 from MUC-6 and 1632 from MUC-7).
Initially, the heuristics implemented in COCKTAILwere applied separately to the two textual collec-tions.
Several special cases arose.English Text.
.
.
.
:rr-Z, la ,on .
.
.
.
.
.
.TranslationRomanian Text"~eferenceFigure 3: Case 1 of multilingual coreferenceCase 1, which is the ideal case, is shown in Fig-ure 3.
It occurs when two referential expressionshave antecedents hat are translations of one an-other.
This situation occurred in 63.3% of the refer-ential expressions from MUC-6 and in 58.7% of theMUC-7 references.
Over 50% of these are pronounsor named entities.
However, all the non-ideal casesare more interesting for SWIZZLE, since they portknowledge that enhances system performance.Coref.
English Text chainsE .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.H 4 ~  .................TranslationER ~ .. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.TranslationRomanian Text?
~ ......... RA'~  ?
(R)RRER: English reference RR: Romanian referenceEA: English antecedent RA: Romanian antecedentET: English translation RT: Romanian translationof Romanian antecedent of English antecedentFigure 4: Case 2 of multilingual coreferenceCase 2 occurs when the antecedents are not trans-lations, but belong to or corefer with elements ofsome coreference chains that were already estab-lished.
Moreover, one of the antecedents is textually147closer to its referent.
Figure 4 illustrates the casewhen the English antecedent is closer to the referentthan the Romanian one.SWIZZLE Solutions: (1) If the heuristic H(E) usedto resolve the reference inthe English text has higherpriority than H(R), which was used to resolve thereference from the Romanian text, then we firstsearch for RT, the Romanian translation of EA, theEnglish antecedent.
In the next step, we add heuris-tic H1 that resolves RR into RT, and give it a higherpriority than H(R).
Finally, we also add heuristic H2that links RTto RA when there is at least one trans-lation between the elements of the coreference hainscontaining EA and ET respectively.
(2) If H(R) has higher priority than H(E), heuris-tic H3 is added while H(E) is removed.
We also add//4 that relates ER to ET, the English translation ofRA.Case 3 occurs when at least one of the antecedentsstarts a new coreference chain (i.e., no coreferringantecedent can be found in the current chains).SWIZZLE Solution: If one of the antecedentscorefers with an element from a coreference chain,then the antecedent in the opposite language is itstranslation.
Otherwise, SNIZZLE chooses the an-tecedent returned by the heuristic with highest pri-ority.4 Resu l tsThe foremost contribution of SWIZZLE was that itimproved coreference r solution over both Englishand Romanian texts when compared to monolingualcoreference r solution performance in terms of preci-sion and recall.
Also relevant was the contribution ofSNIZZLE to the process of understanding the culturaldifferences expressed in language and the way thesedifferences influence coreference r solution.
Becausewe do not have sufficient space to discuss this issuein detail here, let us state, in short, that English ismore economical than Romanian in terms of referen-tial expressions.
However the referential expressionsin Romanian contribute to the resolution of some ofthe most difficult forms of coreference in English.4.1 Precis ion and RecallTable 4 summarizes the precision results for bothEnglish and Romanian coreference.
The results in-dicate that the English coreference is more pre-cise than the Romanian coreference, but SNIZZLEimproves coreference r solution in both languages.There were 64% cases when the English coreferencewas resolved by a heuristic with higher priority thanthe corresponding heuristic for the Romanian coun-terpart.
This result explains why there is better pre-cision enhancement for the English coreference.EnglishRomanianSWIZZLE onEnglishSWIZZLE onRomanianNominal Pronominal73% 89%66% 78%76% 93%71?/o 82%Table 4: Coreference precisionTotal84%72%87%76%EnglishRomanianSWIZZLE onEnglishSWIZZLE onRomanianNominal69%63%66%61%Pronominal Total89% 78%83% 72%87% 77%80% 70%Table 5: Coreference r callTable 5 also illustrates the recall results.
Theadvantage of the data-driven coreference r solutionover other methods is based on its better ecall per-formance.
This is explained by the fact that thismethod captures a larger variety of coreference pat-terns.
Even though other coreference r solution sys-tems perform better for some specific forms of refer-ence, their recall results are surpassed by the data-driven approach.
Multilingual coreference in turnimproves more the precision than the recall of themonolingual data-driven coreference systems.In addition, Table 5 shows that the English coref-erence results in better ecall than Romanian coref-erence.
However, the recall shows a decrease for bothlanguages for SNIZZLE because imprecise coreferencelinks are deleted.
As is usually the case, deletingdata lowers the recall.
All results were obtained byusing the automatic scorer program developed forthe MUC evaluations.5 Conc lus ionsWe have introduced a new data-driven method formultilingual coreference r solution, implemented inthe SWIZZLE system.
The results of this methodare encouraging since they show clear improvementsover monolingual coreference r solution.
Currently,we are also considering the effects of a bootstrap-ping algorithm for multilingual coreference r solu-tion.
Through this procedure we would learn con-currently semantic onsistency knowledge and bet-ter performing heuristic rules.
To be able to de-velop such a learning approach, we must first developa method for automatic recognition of multilingualreferential expressions.148We also believe that a better performance valu-ation of SidIZZLE can be achieved by measuring itsimpact on several complex applications.
We intendto analyze the performance of SIdIZZLE when it isused as a module in an IE system, and separately ina Question/Answering system.Acknowledgements  This paper is dedicated to thememory of our friend Megumi Kameyama, who in-spired this work.Re ferencesDouglas E. Appelt, Jerry R. Hobbs, John Bear, DavidIsrael, Megumi Kameyama nd Mabry Tyson.
1993.The SRI MUC-5 JV-FASTUS Information ExtractionSystem.
In Proceedings of the Fifth Message Under-standing Conference (MUC-5).Brack Baldwin.
1997.
CogNIAC: high precision corefer-ence with limited knowledge and linguistic resources.In Proceedings of the ACL '97/EACL '97 Workshop onOperational factors in practical, robust anaphora res-olution, pages 38-45, Madrid, Spain.Andrei Bantam.
1969.
Dic~ionar Rom?n-Englez, Enlgez-Romfi~.
Editura ~tiin~ific~, Bucure~ti.David Bean and Ellen Riloff.
1999.
Corpus-Based I en-tification of Non-Anaphoric Noun Phrases.
In Pro-ceedings of the 37th Conference of the Assosiation forComputatioanl Linguistics (A CL-99), pages 373-380.Eric Brill.
A simple rule-based part of speech tagger.
InProceedings of the Third Conference on Applied Nat-ural Language Processing, pages 152-155, 1992.Joseph F. Cameron-Jones and Ross Quinlan.
1993.Avoiding Pitfalls When Learning Recursive Theories.In Proceedings of the 13th International Joint Confer-ence on Artificial Intelligence (IJCAI-93), pages 1050-1055.Claire Cardie and Kiri Wagstaff.
1999.
Noun phrasecoreference as clustering.
In Proceedings of the JointConference on Empirical Methods in NLP and VeryLarge Corpora, pages 82-89.Niyu Ge, John Gale and Eugene Charniak.
1998.Anaphora Resolution: A Multi-Strategy Approach.
InProceedings of the 6th Workshop on Very Large Cor-pora, (COLING/A CL '98).Ido Dagan and Ken W. Church.
1994.
TERMIGHT:Identifying and translating technical terminology.
InProceedings of the ~th ACL Conference on AppliedNatural Language Processing (ANLP-94).Sanda M. Harabagiu.
1999.
Lexical Acquisition for aRomanian WordNet.
Proceeding of the 3rd EuropeanSummer School on Computational Linguistics.Sanda M. Harabagiu and Steve J. Maiorano.
1999.Knowledge-Lean Coreference Resolution and its Re-lation to Textual Cohesion and Coherence.
In Pro-ceedings of the Workshop on the Relation of Dis-course/Dialogue Structure and Reference, ACL'98,pages 29-38.Jerry R. Hobbs.
Resolving pronoun references.
Lingua,44:311-338.Andrew Kehler.
1997.
Probabilistic Coreference in In-formation Extraction.
In Proceedings of the SecondConference on Empirical Methods in Natural Lan-guage Processing (SIGDAT), pages 163-173.Shalom Lappin and Herbert Leass.
1994.
An algorithmfor pronominal anaphora resolution.
ComputationalLinguistics, 20(4):535-562.Rosie Jones, Andrew McCallum, Kevin Nigam and EllenRiloff.
1999.
Bootstrapping for Text Learning Tasks.In Proceedings of the IJCAI-99 Workshop on TextMining: Foundations, Techniques, and Applications.Megumi Kameyama.
1997.
Recognizing ReferentialLinks: An Information Extraction Perspective.
InProceedings of the Workshop on Operational Factorsin Practical, Robust Anaphora Resolution for Un-restricted Texts, (ACL-97/EACL-97), pages 46-53,Madrid, Spain.Christopher Kennedy and Branimir Bogureav.
1996.Anaphora for everyone: Pronominal anaphora reso-lution without a parser.
In Proceedings of the 16thInternational Conference on Computational Linguis-tics (COLING-96).George A. Miller.
1995.
WordNet: A Lexical Database.Communication of the A CM, 38(11):39-41.Ruslan Mitkov.
1998.
Robust pronoun resolutionwith limited knowledge.
In Proceedings of COLING-ACL'98, pages 869-875.1996.
Proceedings of the Sixth Message UnderstandingConference (MUC-6),Morgan Kaufmann, San Mateo,CA.1998.
Proceedings of the Seventh Message Understand-ing Conference (MUC-7) ,Morgan Kaufmann, SanMateo, CA.Philip Resnik and I. Dan Melamed.
1997.
Semi-Automatic Acquisition of Domain-Specific TranslationLexicons.
In Proceedings of the 5th ACL Conferenceon Applied Natural Language Processing (ANLP-97).Ellen Riloff and Rosie Jones.
1999.
Learning Dictionar-ies for Information Extraction by Multi-Level Boot-strapping.
In Proceedings of the Sixteenth NationalConference on Artificial Intelligence (AAAI-99).Frank Smadja, Katheleen R. McKeown and VasileiosHatzivassiloglou.
1996.
Translating collocations forbilingual exicons: A statistical approach.
Computa-tional Linguistics , 21(1):1-38.149
