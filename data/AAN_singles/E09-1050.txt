Proceedings of the 12th Conference of the European Chapter of the ACL, pages 433?441,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsLightly Supervised Transliteration for Machine TranslationAmit KirschenbaumDepartment of Computer ScienceUniversity of Haifa31905 Haifa, Israelakirsche@cs.haifa.ac.ilShuly WintnerDepartment of Computer ScienceUniversity of Haifa31905 Haifa, Israelshuly@cs.haifa.ac.ilAbstractWe present a Hebrew to English transliter-ation method in the context of a machinetranslation system.
Our method uses ma-chine learning to determine which termsare to be transliterated rather than trans-lated.
The training corpus for this purposeincludes only positive examples, acquiredsemi-automatically.
Our classifier reducesmore than 38% of the errors made by abaseline method.
The identified terms arethen transliterated.
We present an SMT-based transliteration model trained with aparallel corpus extracted from Wikipediausing a fairly simple method which re-quires minimal knowledge.
The correct re-sult is produced in more than 76% of thecases, and in 92% of the instances it is oneof the top-5 results.
We also demonstrate asmall improvement in the performance ofa Hebrew-to-English MT system that usesour transliteration module.1 IntroductionTransliteration is the process of converting termswritten in one language into their approximatespelling or phonetic equivalents in another lan-guage.
Transliteration is defined for a pair of lan-guages, a source language and a target language.The two languages may differ in their script sys-tems and phonetic inventories.
This paper ad-dresses transliteration from Hebrew to English aspart of a machine translation system.Transliteration of terms from Hebrew into En-glish is a hard task, for the most part because of thedifferences in the phonological and orthographicsystems of the two languages.
On the one hand,there are cases where a Hebrew letter can be pro-nounced in multiple ways.
For example, Hebrew!?
can be pronounced either as [b] or as [v].
Onthe other hand, two different Hebrew sounds canbe mapped into the same English letter.
For exam-ple, both !?
and !?
are in most cases mapped to [t].A major difficulty stems from the fact that in theHebrew orthography (like Arabic), words are rep-resented as sequences of consonants where vow-els are only partially and very inconsistently rep-resented.
Even letters that are considered as rep-resenting vowels may sometimes represent conso-nants, specifically !?
[v]/[o]/[u] and !?
[y]/[i].
As aresult, the mapping between Hebrew orthographyand phonology is highly ambiguous.Transliteration has acquired a growing inter-est recently, particularly in the field of MachineTranslation (MT).
It handles those terms where notranslation would suffice or even exist.
Failing torecognize such terms would result in poor perfor-mance of the translation system.
In the contextof an MT system, one has to first identify whichterms should be transliterated rather than trans-lated, and then produce a proper transliteration forthese terms.
We address both tasks in this work.Identification of Terms To-be Transliterated(TTT) must not be confused with recognition ofNamed Entities (NE) (Hermjakob et al, 2008).On the one hand, many NEs should be translatedrather than transliterated, for example:1m$rd hm$p@immisrad hamishpatimministry-of the-sentences?Ministry of Justice?1To facilitate readability, examples are presented with in-terlinear gloss, including an ASCII representation of Hebreworthography followed by a broad phonemic transcription, aword-for-word gloss in English where relevant, and the cor-responding free text in English.
The following table presentsthe ASCII encoding of Hebrew used in this paper:!?
!?
!?
!?
!?
!?
!?
!?
!?
!?
!|?a b g d h w z x @ i k!?
!|?
!|?
!?
!?
!|?
!|?
!?
!?
!?
!
?l m n s & p c q r $ t433him htikwnhayam hatichonthe-sea the-central?the Mediterranean Sea?On the other hand, there are terms that are notNEs, such as borrowed words or culturally specificterms that are transliterated rather than translated,as shown by the following examples:aqzis@ncializmeqzistentzializm?Existentialism?
@littalit?Tallit?As these examples show, transliteration cannotbe considered the default strategy to handle NEsin MT and translation does not necessarily applyfor all other cases.Candidacy for either transliteration or transla-tion is not necessarily determined by orthographicfeatures.
In contrast to English (and many otherlanguages), proper names in Hebrew are not cap-italized.
As a result, the following homographsmay be interpreted as either a proper name, a noun,or a verb:alwnalon?oak?alwnalun?I will sleep?alwnalon?Alon?
(name)One usually distinguishes between two types oftransliteration (Knight and Graehl, 1997): For-ward transliteration, where an originally Hebrewterm is to be transliterated to English; and Back-ward transliteration, in which a foreign term thathas already been transliterated into Hebrew is tobe recovered.
Forward transliteration may result inseveral acceptable alternatives.
This is mainly dueto phonetic gaps between the languages and lackof standards for expressing Hebrew phonemes inEnglish.
For example, the Hebrew term cdiq maybe transliterated as Tzadik, Tsadik, Tsaddiq, etc.On the other hand, backward transliteration is re-strictive.
There is usually only one acceptable wayto express the transliterated term.
So, for exam-ple, the name wiliam can be transliterated onlyto William and not, for example, to Viliem, eventhough the Hebrew character w may stand for theconsonant [v] and the character a may be vow-elized as [e].We approach the task of transliteration in thecontext of Machine Translation in two phases.First, we describe a lightly-supervised classifierthat can identify TTTs in the text (section 4).
Theidentified terms are then transliterated (section 5)using a transliteration model based on StatisticalMachine Translation (SMT).
The two modules arecombined and integrated in a Hebrew to EnglishMT system (section 6).The main contribution of this work is the actualtransliteration module, which has already been in-tegrated in a Hebrew to English MT system.
Theaccuracy of the transliteration is comparable withstate-of-the-art results for other language pairs,where much more training material is available.More generally, we believe that the method we de-scribe here can be easily adapted to other languagepairs, especially those for which few resources areavailable.
Specifically, we did not have access toa significant parallel corpus, and most of the re-sources we used are readily available for manyother languages.2 Previous WorkIn this section we sketch some related works, fo-cusing on transliteration from Hebrew and Arabic,and on the context of machine translation.Arbabi et al (1994) present a hybrid algorithmfor romanization of Arabic names using neuralnetworks and a knowledge based system.
The pro-gram applies vowelization rules, based on Arabicmorphology and stemming from the knowledgebase, to unvowelized names.
This stage, termedthe broad approach, exhaustively yields all validvowelizations of the input.
To solve this over-generation, the narrow approach is then used.
Inthis approach, the program uses a neural networkto filter unreliable names, that is, names whosevowelizations are not in actual use.
The vowelizednames are converted into a standard phonetic rep-resentation which in turn is used to produce var-ious spellings in languages which use Roman al-phabet.
The broad approach covers close to 80%of the names given to it, though with some extra-neous vowelization.
The narrow approach coversover 45% of the names presented to it with higherprecision than the broad approach.This approach requires a vast linguistic knowl-edge in order to create the knowledge base of vow-elization rules.
In addition, these rules are appli-cable only to names that adhere to the Arabic mor-phology.Stalls and Knight (1998) propose a method forback transliteration of names that originate in En-glish and occur in Arabic texts.
The method uses asequence of probabilistic models to convert nameswritten in Arabic into the English script.
First,434an Arabic name is passed through a phonemicmodel producing a network of possible Englishsound sequences, where the probability of eachsound is location dependent.
Next, phonetic se-quences are transformed into English phrases.
Fi-nally, each possible result is scored according to aunigram word model.
This method translates cor-rectly about 32% of the tested names.
Those nottranslated are frequently not foreign names.This method uses a pronunciation dictionaryand is therefore restricted to transliterating onlywords of known pronunciation.
Both of the abovemethods perform only unidirectional translitera-tion, that is, either forward- or backward- translit-eration, while our work handles both.Al-Onaizan and Knight (2002) describe a sys-tem which combines a phonetic based model witha spelling model for transliteration.
The spellingbased model directly maps sequences of Englishletters into sequences of Arabic letters without theneed of English pronunciation.
The method uses atranslation model based on IBM Model 1 (Brownet al, 1993), in which translation candidates ofa phrase are generated by combining translationsand transliterations of the phrase components, andmatching the result against a large corpus.
Thesystem?s overall accuracy is about 72% for top-1results and 84% for top-20 results.This method is restricted to transliterating NEs,and performs best for person names.
As notedabove, the TTT problem is not identical to theNER problem.
In addition, the method requires alist of transliteration pairs from which the translit-eration model could be learned.Yoon et al (2007) use phonetic distinctivefeatures and phonology-based pseudo featuresto learn both language-specific and language-universal transliteration characteristics.
Distinc-tive features are the characteristics that define theset of phonemic segments (consonants, vowels) ina given language.
Pseudo features capture soundchange patterns that involve the position in the syl-lable.
Distinctive features and pseudo features areextracted from source- and target-language train-ing data to train a linear classifier.
The classifiercomputes compatibility scores between Englishsource words and target-language words.
Whenseveral target-language strings are transliterationcandidates for a source word, the one with thehighest score is selected as the transliteration.
Themethod was evaluated using parallel corpora ofEnglish with each of four target languages.
NEswere extracted from the English side and werecompared with all the words in the target lan-guage to find proper transliterations.
The baselinepresented for the case of transliteration from En-glish to Arabic achieves Mean Reciprocal Rank(MRR) of 0.66 and this method improves its re-sults by 7%.
This technique involves knowledgeabout phonological characteristics, such as elisionof consonants based on their position in the word,which requires expert knowledge of the language.In addition, conversion of terms into a phonemicrepresentation poses hurdles in representing shortvowels in Arabic and will have similar behavior inHebrew.
Moreover, English to Arabic transliter-ation is easier than Arabic to English, because inthe former, vowels should be deleted whereas inthe latter they should be generated.Matthews (2007) presents a model for translit-eration from Arabic to English based on SMT.The parallel corpus from which the translationmodel is acquired contains approximately 2500pairs, which are part of a bilingual person namescorpus (LDC2005G02).
This biases the model to-ward transliterating person names.
The languagemodel presented for that method consisted of 10Kentries of names which is, again, not complete.This model also uses different settings for maxi-mum phrase length in the translation model anddifferent n-gram order for the language model.
Itachieves an accuracy of 43% when transliteratingfrom Arabic to English.Goldwasser and Roth (2008) introduce a dis-criminative method for identifying NE transliter-ation pairs in English-Hebrew.
Given a word pair(ws, wt), where ws is an English NE, the systemdetermines whether wt, a string in Hebrew, is itstransliteration.
The classification is based on pair-wise features: sets of substrings are extracted fromeach of the words, and substrings from the two setsare then coupled to form the features.
The accu-racy of correctly identifying transliteration pairsin top-1 and top-5 was 52% and 88%, respec-tively.
Whereas this approach selects most suitabletransliteration out of a list of candidates, our ap-proach generates a list of possible transliterationsranked by their accuracy.Despite the importance of identifying TTTs,this task has only been addressed recently.
Gold-berg and Elhadad (2008) present a loosely super-vised method for non contextual identification of435transliterated foreign words in Hebrew texts.
Themethod is a Naive-Bayes classifier which learnsfrom noisy data.
Such data are acquired by over-generation of transliterations for a set of words ina foreign script, using mappings from the phone-mic representation of words to the Hebrew script.Precision and recall obtained are 80% and 82%,respectively.
However, although foreign wordsare indeed often TTTs, many originally Hebrewwords should sometimes be transliterated.
Asexplained in section 4, there are words in He-brew that may be subject to either translation ortransliteration, depending on the context.
A non-contextual approach would not suffice for our task.Hermjakob et al (2008) describe a method foridentifying NEs that should be transliterated inArabic texts.
The method first tries to find amatching English word for each Arabic word in aparallel corpus, and tag the Arabic words as eithernames or non-names based on a matching algo-rithm.
This algorithm uses a scoring model whichassigns manually-crafted costs to pairs of Arabicand English substrings, allowing for context re-strictions.
A number of language specific heuris-tics, such as considering only capitalized wordsas candidates and using lists of stop words, areused to enhance the algorithm?s accuracy.
Thetagged Arabic corpus is then divided: One part isused to collect statistics about the distribution ofname/non-name patterns among tokens, bigramsand trigrams.
The rest of the tagged corpus isused for training using an averaged perceptron.The precision of the identification task is 92.1%and its recall is 95.9%.
This work also presentsa novel transliteration model, which is integratedinto a machine translation system.
Its accuracy,measured by the percentage of correctly translatednames, is 89.7%.Our work is very similar in its goals and theoverall framework, but in contrast to Hermjakobet al (2008) we use much less supervision, and inparticular, we do not use a parallel corpus.
We alsodo not use manually-crafted weights for (hundredsof) bilingual pairs of strings.
More generally, ourtransliteration model is much more language-pairneutral.3 Resources and MethodologyOur work consists of of two sub-tasks: IdentifyingTTTs and then transliterating them.
Specifically,we use the following resources for this work: Forthe identification task we use a large un-annotatedcorpus of articles from Hebrew press and web-forums (Itai and Wintner, 2008) consisting of 16million tokens.
The corpus is POS-tagged (Bar-Haim et al, 2008).
We bootstrap a training cor-pus for one-class SVM (section 4.2) using a listof rare Hebrew character n-grams (section 4.1) togenerate a set of positive, high-precision exam-ples for TTTs in the tagged corpus.
POS tags forthe positive examples and their surrounding tokensare used as features for the one-class SVM (sec-tion 4.2).For the transliteration itself we use a list thatmaps Hebrew consonants to their English counter-parts to extract a list of Hebrew-English transla-tion pairs from Wikipedia (section 5.2).
To learnthe transliteration model we utilize Moses (sec-tion 5) which is also used for decoding.
Decod-ing also relies on a target language model, whichis trained by applying SRILM to Web 1T corpus(section 5.1).Importantly, the resources we use for this workare readily available for a large number of lan-guages and can be easily obtained.
None of theserequire any special expertise in linguistics.
Cru-cially, no parallel corpus was used.4 What to transliterateThe task in this phase, then, is to determine foreach token in a given text whether it should betranslated or transliterated.
We developed a setof guidelines to determine which words are to betransliterated.
For example, person names are al-ways transliterated, although many of them havehomographs that can be translated.
Foreign words,which retain the sound patterns of their originallanguage with no semantic translation involved,are also (back-)transliterated.
On the other hand,names of countries may be subject to translationor transliteration, as demonstrated in the follow-ing examples:crpttsarfat?France?sprdsfarad?Spain?qwngwkongo?Congo?We use information obtained from POS tagging(Bar-Haim et al, 2008) to address the problem ofidentifying TTTs.
Each token is assigned a POSand is additionally marked if it was not found in alexicon (Itai et al, 2006).
As a baseline, we tag fortransliteration Out Of Vocabulary (OOV) tokens.436Our evaluation metric is tagging accuracy, that is,the percentage of correctly tagged tokens.4.1 Rule-based taggingMany of the TTTs do appear in the lexicon,though, and their number will grow with the avail-ability of more language resources.
As notedabove, some TTTs can be identified based on theirsurface forms; these words are mainly loan words.For example, the word brwdqsting (broadcasting)contains several sequences of graphemes that arenot frequent in Hebrew (e.g., ng in a word-finalposition).We manually generated a list of such features toserve as tagging rules.
To create this list we useda few dozens of character bigrams, about a dozentrigrams and a couple of unigrams and four-grams,that are highly unlikely to occur in words of He-brew origin.
Rules associate n-grams with scoresand these scores are summed when applying therules to tokens.
A typical rule is of the form: if?1?2 are the final characters of w, add c to thescore of w, where w is a word in Hebrew, ?1 and?2 are Hebrew characters, and c is an positive in-teger.
A word is tagged for transliteration if thesum of the scores associated with its substrings ishigher than a predefined threshold.We apply these rules to a large Hebrew corpusand create an initial set of instances of terms that,with high probability, should be be transliteratedrather than translated.
Of course, many TTTs, es-pecially those whose surface forms are typical ofHebrew, will be missed when using this taggingtechnique.
Our solution is to learn the contexts inwhich TTTs tend to occur, and contrast these con-texts with those for translated terms.
The underly-ing assumption is that the former contexts are syn-tactically determined, and are independent of theactual surface form of the term (and of whether ornot it occurs in the lexicon).
Since the result ofthe rule-based tagging is considered as examplesof TTTs, this automatically-annotated corpus canbe used to extract such contexts.4.2 Training with one class classifierThe above process provides us with 40279 exam-ples of TTTs out of a total of more than 16 mil-lion tokens.
These examples, however, are onlypositive examples.
In order to learn from the in-complete data we utilized a One Class Classifier.Classification problems generally involve two ormore classes of objects.
A function separatingthese classes is to be learned and used by the clas-sifier.
One class classification utilizes only targetclass objects to learn a function that distinguishesthem from any other objects.SVM (Support Vector Machine) (Vapnik, 1995)is a classification technique which finds a linearseparating hyperplane with maximal margins be-tween data instances of two classes.
The sepa-rating hyperplane is found for a mapping of datainstances into a higher dimension, using a ker-nel function.
Scho?lkopf et al (2000) introducean adaptation of the SVM methodology to theproblem of one-class classification.
We used one-class SVM as implemented in LIBSVM (Changand Lin, 2001).
The features selected to representeach TTT were its POS and the POS of the tokenpreceding it in the sentence.
The kernel functionwhich yielded the best results on this problem wasa sigmoid with standard parameters.4.3 ResultsTo evaluate the TTT identification model we cre-ated a gold standard, tagged according to theguidelines described above, by a single lexicog-rapher.
The testing corpus consists of 25 sen-tences from the same sources as the training cor-pus and contains 518 tokens, of which 98 areTTTs.
We experimented with two different base-lines: the na?
?ve baseline always decides to trans-late; a slightly better baseline consults the lexicon,and tags as TTT any token that does not occur inthe lexicon.
We measure our performance in errorrate reduction of tagging accuracy, compared withthe latter baseline.Our initial approach consisted of consultingonly the decision of the one-class SVM.
How-ever, since there are TTTs that can be easily iden-tified using features obtained from their surfaceform, our method also examines each token usingsurface-form features, as described in section 4.1.If a token has no surface features that identify itas a TTT, we take the decision of the one-classSVM.
Table 1 presents different configurations weexperimented with, and their results.
The first twocolumns present the two baselines we used, as ex-plained above.
The third column (OCS) shows theresults based only on decisions made by the OneClass SVM.
The penultimate column shows the re-sults obtained by our method combining the SVMwith surface-based features.
The final columnpresents the Error Rate Reduction (ERR) achieved437when using our method, compared to the base-line of transliterating OOV words.
As can be ob-served, our method increases classification accu-racy: more than 38% of the errors over the base-line are reduced.Na?
?ve Baseline OCS Our ERR79.9 84.23 88.04 90.26 38.24Table 1: TTT identification results (% of the in-stances identified correctly)The importance of the recognition process isdemonstrated in the following example.
The un-derlined phrase was recognized correctly by ourmethod.kbwdw habwd $l bn arikvodo heavud shel ben ariHis-honor the-lost of Ben Ari?Ben Ari?s lost honor ?Both the word ben and the word ari have literalmeanings in Hebrew (son and lion, respectively),and their combination might be interpreted as aphrase since it is formed as a Hebrew noun con-struct.
Recognizing them as transliteration candi-dates is crucial for improving the performance ofMT systems.5 How to transliterateOnce a token is classified as a TTT, it is sent tothe transliteration module.
Our approach handlesthe transliteration task as a case of phrase-basedSMT, based on the noisy channel model.
Accord-ing to this model, when translating a string f in thesource language into the target language, a stringe?
is chosen out of all target language strings e if ithas the maximal probability given f (Brown et al,1993):e?
= arg maxe{Pr(e|f)}= arg maxe{Pr(f |e) ?
Pr(e)}where Pr(f |e) is the translation model and Pr(e)is the target language model.
In phrase-basedtranslation, f is divided into phrases f?1 .
.
.
f?I ,and each source phrase f?i is translated into targetphrase e?i according to a phrase translation model.Target phrases may then be reordered using a dis-tortion model.We use SMT for transliteration; this approachviews transliteration pairs as aligned sentences andcharacters are viewed as words.
In the case ofphrase-based SMT, phrases are sequences of char-acters.
We used Moses (Koehn et al, 2007), aphrase-based SMT toolkit, for training the transla-tion model (and later for decoding).
In order to ex-tract phrases, bidirectional word level alignmentsare first created, both source to target and targetto source.
Alignments are merged heuristically ifthey are consistent, in order to extract phrases.5.1 Target language modelWe created an English target language model fromunigrams of Web 1T (Brants and Franz, 2006).The unigrams are viewed as character n-grams tofit into the SMT system.
We used SRILM (Stol-cke, 2002) with a modified Kneser-Ney smooth-ing, to generate a language model of order 5.5.2 Hebrew-English translation modelNo parallel corpus of Hebrew-English transliter-ation pairs is available, and compiling one man-ually is time-consuming and labor-intensive.
In-stead, we extracted a parallel list of Hebrew andEnglish terms from Wikipedia and automaticallygenerated such a corpus.
The terms are paral-lel titles of Wikipedia articles and thus can safelybe assumed to denote the same entity.
In manycases these titles are transliterations of one an-other.
From this list we extracted transliterationpairs according to similarity of consonants in par-allel English and Hebrew entries.The similarity measure is based only on conso-nants since vowels are often not represented at allin Hebrew.
We constructed a table relating He-brew and English consonants, based on commonknowledge patterns that relate sound to spelling inboth languages.
Sound patterns that are not part ofthe phoneme inventory of Hebrew but are nonethe-less represented in Hebrew orthography were alsoincluded in the table.
Every entry in the mappingtable consists of a Hebrew letter and a possibleLatin letter or letter sequences that might matchit.
A typical entry is the following:$:SH|S|CHsuch that SH, S or CH are possible candidates formatching the Hebrew letter $.Both Hebrew and English titles in Wikipediamay be composed of several words.
However,words composing the entries in each of the lan-guages may be ordered differently.
Therefore, ev-ery word in Hebrew is compared with every word438in English, assuming that titles are short enough.The example in Table 2 presents an aligned pair ofmulti-lingual Wikipedia entries with high similar-ity of consonants.
This is therefore considered as atransliteration pair.
In contrast, the title empty setwhich is translated to hqbwch hriqh shows a lowsimilarity of consonants.
This pair is not selectedfor the training corpus.g r a t e f u l d e a dg r i i @ p w l d dTable 2: Titles of Wikipedia entriesOut of 41914 Hebrew and English terms re-trieved from Wikipedia, more than 20000 were de-termined as transliteration pairs.
Out of this set,500 were randomly chosen to serve as a test set,500 others were chosen to serve as a developmentset, and the rest are the training set.
Minimumerror rate training was done on the developmentset to optimize translation performance obtainedby the training phase.2 For decoding, we prohib-ited Moses form performing character reordering(distortion).
While reordering may be needed fortranslation, we want to ensure the monotone na-ture of transliteration.5.3 ResultsWe applied Moses to the test set to get a list oftop-n transliteration options for each entry in theset.
The results obtained by Moses were furtherre-ranked to take into account their frequency asreflected in the unigrams of Web 1T (Brants andFranz, 2006).
The re-ranking method first nor-malizes the scores of Moses?
results to the rangeof [0, 1].
The respective frequencies of these re-sults in Web1T corpus are also normalized to thisrange.
The score s of each transliteration op-tion is a linear combination of these two elements:s = ?sM + (1?
?
)sW , where sM is the normal-ized score obtained for the transliteration optionby Moses, and sW is its normalized frequency.?
is empirically set to 0.75.
Table 3 summarizesthe proportion of the terms transliterated correctlyacross top-n results as achieved by Moses, andtheir improvement after re-ranking.We further experimented with two methods forreducing the list of transliteration options to themost prominent ones by taking a variable numberof candidates rather than a fixed number.
This is2We used moses-mert.pl in the Moses package.Results Top-1 Top-2 Top-5 Top-10Moses 68.4 81.6 90.2 93.6Re-ranked 76.6 86.6 92.6 93.6Table 3: Transliteration results (% of the instancestransliterated correctly)important for limiting the search space of MT sys-tems.
The first method (var1) measures the ratiobetween the scores of each two consecutive op-tions and generates the option that scored loweronly if this ratio exceeds a predefined threshold.We found that the best setting for the thresholdis 0.75, resulting in an accuracy of 88.6% andan average of 2.32 results per token.
Our sec-ond method (var2) views the score as a probabil-ity mass, and generates all the results whose com-bined probabilities are at most p. We found thatthe best value for p is 0.5, resulting in an accuracyof 87.4% and 1.92 results per token on average.Both methods outperform the top-2 accuracy.Table 4 presents a few examples from thetest set that were correctly transliterated by ourmethod.
Some incorrect transliterations aredemonstrated in Table 5.Source Transliterationnp$ nefeshhlmsbrgr hellmesbergersmb@iwn sambationhiprbwlh hyperbola$prd shepardba$h bachetxt$pswt hatshepsutbrgnch berganzaali$r elissarg?wbani giovanniTable 4: Transliteration examples generated cor-rectly from the test set6 Integration with machine translationWe have integrated our system as a module in aMachine Translation system, based on Lavie etal.
(2004a).
The system consults the TTT clas-sifier described in section 4 for each token, beforetranslating it.
If the classifier determines that thetoken should be transliterated, then the transliter-ation procedure described in section 5 is appliedto the token to produce the transliteration results.439Source Transliteration Targetrbindrnt rbindrant rabindranathaswirh asuira essaouirakmpi@ champit chamaephytebwdlr bodler baudelairelwrh laura lorrehwlis ollies hollieswnwm onom venomTable 5: Incorrect transliteration examplesWe provide an external evaluation in the form ofBLEU (Papineni et al, 2001) and Meteor (Lavieet al, 2004b) scores for SMT with and without thetransliteration module.When integrating our method in the MT systemwe use the best transliteration options as obtainedwhen using the re-ranking procedure described insection 5.3.
The translation results for all condi-tions are presented in Table 6, compared to thebasic MT system where no transliteration takesplace.
Using the transliteration module yields astatistically significant improvement in METEORscores (p < 0.05).
METEOR scores are most rel-evant since they reflect improvement in recall.
TheMT system cannot yet take into consideration theweights of the transliteration options.
Translationresults are expected to improve once these weightsare taken into account.System BLEU METEORBase 9.35 35.33127Top-1 9.85 38.37584Top-10 9.18 37.95336var1 8.72 37.28186var2 8.71 37.11948Table 6: Integration of transliteration module inMT system7 ConclusionsWe presented a new method for transliteration inthe context of Machine Translation.
This methodidentifies, for a given text, tokens that shouldbe transliterated rather than translated, and ap-plies a transliteration procedure to the identifiedwords.
The method uses only positive exam-ples for learning which words to transliterate andachieves over 38% error rate reduction when com-pared to the baseline.
In contrast to previous stud-ies this method does not use any parallel corporafor learning the features which define the translit-erated terms.
The simple transliteration scheme isaccurate and requires minimal resources which aregeneral and easy to obtain.
The correct transliter-ation is generated in more than 76% of the cases,and in 92% of the instances it is one of the top-5results.We believe that some simple extensions couldfurther improve the accuracy of the translitera-tion module, and these are the focus of currentand future research.
First, we would like to useavailable gazetteers, such as lists of place andperson names available from the US census bu-reau, http://world-gazetteer.com/ orhttp://geonames.org.
Then, we considerutilizing the bigram and trigram parts of Web1T (Brants and Franz, 2006), to improve theTTT identifier with respect to identifying multi-token expressions which should be transliterated.In addition, we would like to take into accountthe weights of the different transliteration optionswhen deciding which to select in the translation.Finally, we are interested in applying this moduleto different language pairs, especially ones withlimited resources.AcknowledgmentsWe wish to thank Gennadi Lembersky for his helpin integrating our work into the MT system, aswell as to Erik Peterson and Alon Lavie for pro-viding the code for extracting bilingual article ti-tles from Wikipedia.
We thank Google Inc. and theLDC for making the Web 1T corpus available tous.
Dan Roth provided good advice in early stagesof this work.
This research was supported byTHE ISRAEL SCIENCE FOUNDATION (grantNo.
137/06); by the Israel Internet Association; bythe Knowledge Center for Processing Hebrew; andby the Caesarea Rothschild Institute for Interdis-ciplinary Application of Computer Science at theUniversity of Haifa.ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Translat-ing named entities using monolingual and bilingualresources.
In ACL ?02: Proceedings of the 40th An-nual Meeting on Association for Computational Lin-guistics, pages 400?408, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Mansur Arbabi, Scott M. Fischthal, Vincent C. Cheng,440and Elizabeth Bart.
1994.
Algorithms for arabicname transliteration.
IBM Journal of Research andDevelopment, 38(2):183?194.Roy Bar-Haim, Khalil Sima?an, and Yoad Winter.2008.
Part-of-speech tagging of Modern Hebrewtext.
Natural Language Engineering, 14(2):223?251.Thorsten Brants and Alex Franz.
2006.
Web 1T 5-gram version 1.1.
Technical report, Google Re-seach.Peter F. Brown, Stephen Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathe-matic of statistical machine translation: Parameterestimation.
Computational Linguistics, 19(2):263?311.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Yoav Goldberg and Michael Elhadad.
2008.
Identifica-tion of transliterated foreign words in hebrew script.In CICLing, pages 466?477.Dan Goldwasser and Dan Roth.
2008.
Active sampleselection for named entity transliteration.
In Pro-ceedings of ACL-08: HLT, Short Papers, pages 53?56, Columbus, Ohio, June.
Association for Compu-tational Linguistics.Ulf Hermjakob, Kevin Knight, and Hal Daume?
III.2008.
Name translation in statistical machine trans-lation - learning when to transliterate.
In Proceed-ings of ACL-08: HLT, pages 389?397, Columbus,Ohio, June.
Association for Computational Linguis-tics.Alon Itai and Shuly Wintner.
2008.
Language re-sources for Hebrew.
Language Resources and Eval-uation, 42(1):75?98, March.Alon Itai, Shuly Wintner, and Shlomo Yona.
2006.
Acomputational lexicon of contemporary hebrew.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC-2006),pages 19?22, Genoa, Italy.Kevin Knight and Jonathan Graehl.
1997.
Machinetransliteration.
In Proceedings of the 35th AnnualMeeting of the Association for Computational Lin-guistics, pages 128?135, Madrid, Spain.
Associationfor Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Alon Lavie, Erik Peterson, Katharina Probst, ShulyWintner, and Yaniv Eytani.
2004a.
Rapid prototyp-ing of a transfer-based Hebrew-to-English machinetranslation system.
In Proceedings of the 10th In-ternational Conference on Theoretical and Method-ological Issues in Machine Translation, pages 1?10,Baltimore, MD, October.Alon Lavie, Kenji Sagae, and Shyamsundar Jayara-man.
2004b.
The significance of recall in automaticmetrics for mt evaluation.
In Robert E. Frederkingand Kathryn Taylor, editors, AMTA, volume 3265 ofLecture Notes in Computer Science, pages 134?143.Springer.David Matthews.
2007.
Machine transliteration ofproper names.
Master?s thesis, School of Informat-ics, University of Edinburgh.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
In ACL ?02: Pro-ceedings of the 40th Annual Meeting on Associa-tion for Computational Linguistics, pages 311?318,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Bernhard Scho?lkopf, Alex J. Smola, RobertWilliamson, and Peter Bartlett.
2000.
Newsupport vector algorithms.
Neural Computation,12:1207?1245.Bonnie Glover Stalls and Kevin Knight.
1998.
Trans-lating names and technical terms in Arabic text.In Proceedings of the COLING/ACL Workshop onComputational Approaches to Semitic Languages,pages 34?41.Andreas Stolcke.
2002.
SRILM ?
an extensible lan-guage modeling toolkit.
In Proceedings Interna-tional Conference on Spoken Language Processing(ICSLP 2002), pages 901?904.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Su-Youn Yoon, Kyoung-Young Kim, and RichardSproat.
2007.
Multilingual transliteration using fea-ture based phonetic method.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 112?119, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.441
