Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 401?408,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic learning of textual entailments with cross-pair similaritiesFabio Massimo ZanzottoDISCoUniversity of Milano-BicoccaMilan, Italyzanzotto@disco.unimib.itAlessandro MoschittiDepartment of Computer ScienceUniversity of Rome ?Tor Vergata?Rome, Italymoschitti@info.uniroma2.itAbstractIn this paper we define a novel similaritymeasure between examples of textual en-tailments and we use it as a kernel func-tion in Support Vector Machines (SVMs).This allows us to automatically learn therewrite rules that describe a non trivial setof entailment cases.
The experiments withthe data sets of the RTE 2005 challengeshow an improvement of 4.4% over thestate-of-the-art methods.1 IntroductionRecently, textual entailment recognition has beenreceiving a lot of attention.
The main reason isthat the understanding of the basic entailment pro-cesses will allow us to model more accurate se-mantic theories of natural languages (Chierchiaand McConnell-Ginet, 2001) and design importantapplications (Dagan and Glickman, 2004), e.g.,Question Answering and Information Extraction.However, previous work (e.g., (Zaenen et al,2005)) suggests that determining whether or nota text T entails a hypothesis H is quite complexeven when all the needed information is explic-itly asserted.
For example, the sentence T1: ?Atthe end of the year, all solid companies pay divi-dends.?
entails the hypothesis H1: ?At the end ofthe year, all solid insurance companies pay divi-dends.?
but it does not entail the hypothesis H2:?At the end of the year, all solid companies paycash dividends.
?Although these implications are uncontrover-sial, their automatic recognition is complex if werely on models based on lexical distance (or sim-ilarity) between hypothesis and text, e.g., (Corleyand Mihalcea, 2005).
Indeed, according to suchapproaches, the hypotheses H1 and H2 are verysimilar and seem to be similarly related to T1.
Thissuggests that we should study the properties anddifferences of such two examples (negative andpositive) to derive more accurate entailment mod-els.
For example, if we consider the following en-tailment:T3 ?
H3?T3 ?All wild animals eat plants that havescientifically proven medicinal proper-ties.
?H3 ?All wild mountain animals eat plantsthat have scientifically proven medici-nal properties.
?we note that T3 is structurally (and somehow lex-ically similar) to T1 and H3 is more similar to H1than to H2.
Thus, from T1 ?
H1 we may extractrules to derive that T3 ?
H3.The above example suggests that we should relynot only on a intra-pair similarity between T andH but also on a cross-pair similarity between twopairs (T ?,H ?)
and (T ?
?,H ??).
The latter similaritymeasure along with a set of annotated examples al-lows a learning algorithm to automatically derivesyntactic and lexical rules that can solve complexentailment cases.In this paper, we define a new cross-pair similar-ity measure based on text and hypothesis syntactictrees and we use such similarity with traditionalintra-pair similarities to define a novel semantickernel function.
We experimented with such ker-nel using Support Vector Machines (Vapnik, 1995)on the test tests of the Recognizing Textual En-tailment (RTE) challenges (Dagan et al, 2005;Bar Haim et al, 2006).
The comparative resultsshow that (a) we have designed an effective wayto automatically learn entailment rules from ex-amples and (b) our approach is highly accurate andexceeds the accuracy of the current state-of-the-art401models (Glickman et al, 2005; Bayer et al, 2005)by about 4.4% (i.e.
63% vs. 58.6%) on the RTE 1test set (Dagan et al, 2005).In the remainder of this paper, Sec.
2 illustratesthe related work, Sec.
3 introduces the complexityof learning entailments from examples, Sec.
4 de-scribes our models, Sec.
6 shows the experimentalresults and finally Sec.
7 derives the conclusions.2 Related workAlthough the textual entailment recognition prob-lem is not new, most of the automatic approacheshave been proposed only recently.
This has beenmainly due to the RTE challenge events (Dagan etal., 2005; Bar Haim et al, 2006).
In the followingwe report some of such researches.A first class of methods defines measures ofthe distance or similarity between T and H ei-ther assuming the independence between words(Corley and Mihalcea, 2005; Glickman et al,2005) in a bag-of-word fashion or exploiting syn-tactic interpretations (Kouylekov and Magnini,2005).
A pair (T,H) is then in entailment whensim(T,H) > ?.
These approaches can hardlydetermine whether the entailment holds in the ex-amples of the previous section.
From the point ofview of bag-of-word methods, the pairs (T1,H1)and (T1,H2) have both the same intra-pair simi-larity since the sentences of T1 and H1 as well asthose of T1 and H2 differ by a noun, insurance andcash, respectively.
At syntactic level, also, we can-not capture the required information as such nounsare both noun modifiers: insurance modifies com-panies and cash modifies dividends.A second class of methods can give a solutionto the previous problem.
These methods generallycombine a similarity measure with a set of possi-ble transformations T applied over syntactic andsemantic interpretations.
The entailment betweenT and H is detected when there is a transformationr ?
T so that sim(r(T ),H) > ?.
These trans-formations are logical rules in (Bos and Markert,2005) or sequences of allowed rewrite rules in (deSalvo Braz et al, 2005).
The disadvantage is thatsuch rules have to be manually designed.
More-over, they generally model better positive implica-tions than negative ones and they do not considererrors in syntactic parsing and semantic analysis.3 Challenges in learning from examplesIn the introductory section, we have shown that,to carry out automatic learning from examples, weneed to define a cross-pair similarity measure.
Itsdefinition is not straightforward as it should detectwhether two pairs (T ?,H ?)
and (T ?
?,H ??)
realizethe same rewrite rules.
This measure should con-sider pairs similar when: (1) T ?
and H ?
are struc-turally similar to T ??
and H ?
?, respectively and (2)the lexical relations within the pair (T ?,H ?)
arecompatible with those in (T ?
?,H ??).
Typically, Tand H show a certain degree of overlapping, thus,lexical relations (e.g., between the same words)determine word movements from T to H (or viceversa).
This is important to model the syntac-tic/lexical similarity between example pairs.
In-deed, if we encode such movements in the syntac-tic parse trees of texts and hypotheses, we can useinteresting similarity measures defined for syntac-tic parsing, e.g., the tree kernel devised in (Collinsand Duffy, 2002).To consider structural and lexical relation simi-larity, we augment syntactic trees with placehold-ers which identify linked words.
More in detail:- We detect links between words wt in T that areequal, similar, or semantically dependent on wordswh in H .
We call anchors the pairs (wt, wh) andwe associate them with placeholders.
For exam-ple, in Fig.
1, the placeholder 2?
indicates the(companies,companies) anchor between T1 andH1.
This allows us to derive the word movementsbetween text and hypothesis.- We align the trees of the two texts T ?
and T ??
aswell as the tree of the two hypotheses H ?
and H ?
?by considering the word movements.
We find acorrect mapping between placeholders of the twohypothesis H ?
and H ??
and apply it to the tree ofH ??
to substitute its placeholders.
The same map-ping is used to substitute the placeholders in T ?
?.This mapping should maximize the structural sim-ilarity between the four trees by considering thatplaceholders augment the node labels.
Hence, thecross-pair similarity computation is reduced to thetree similarity computation.The above steps define an effective cross-pairsimilarity that can be applied to the example inFig.
1: T1 and T3 share the subtree in bold start-ing with S ?
NP VP.
The lexicals in T3 and H3are quite different from those T1 and H1, but wecan rely on the structural properties expressed bytheir bold subtrees.
These are more similar to thesubtrees of T1 and H1 than those of T1 and H2,respectively.
Indeed, H1 and H3 share the pro-duction NP ?
DT JJ NN NNS while H2 and H3 do402T1 T3SPPINAtNP 0NP 0DTtheNN 0end0PPINofNP 1DTtheNN 1year1,,NP 2DTallJJ 2solid2?NNS 2companies2?VP 3VBP 3pay3NP 4NNS 4dividends4SNP aDTAllJJ awilda?NNS aanimalsa?VP bVBP beatbNP cplantsc ... propertiesH1 H3SPPINAtNP 0NP 0DTtheNN 0end0PPINofNP 1DTtheNN 1year1,,NP 2DTallJJ 2solid2?NNinsuranceNNS 2companies2?VP 3VBP 3pay3NP 4NNS 4dividends4SNP aDTAllJJ awilda?NNmountainNNS aanimalsa?VP bVBP beatbNP cplantsc ... propertiesH2 H3SPPAt ... yearNP 2DTallJJ 2solid2?NNS 2companies2?VP 3VBP 3pay3NP 4NNcashNNS 4dividends4SNP aDTAllJJ awilda?NNmountainNNS aanimalsa?VP bVBP beatbNP cplantsc ... propertiesFigure 1: Relations between (T1,H1), (T1,H2), and (T3,H3).not.
Consequently, to decide if (T3,H3) is a validentailment, we should rely on the decision madefor (T1,H1).
Note also that the dashed lines con-necting placeholders of two texts (hypotheses) in-dicate structurally equivalent nodes.
For instance,the dashed line between 3 and b links the mainverbs both in the texts T1 and T3 and in the hy-potheses H1 and H3.
After substituting 3 with band 2 with a , we can detect if T1 and T3 sharethe bold subtree S ?
NP 2 VP 3 .
As such subtreeis shared also by H1 and H3, the words within thepair (T1,H1) are correlated similarly to the wordsin (T3,H3).The above example emphasizes that we needto derive the best mapping between placeholdersets.
It can be obtained as follows: let A?
and A?
?be the placeholders of (T ?,H ?)
and (T ?
?,H ??
), re-spectively, without loss of generality, we consider|A?| ?
|A?
?| and we align a subset of A?
to A??.
Thebest alignment is the one that maximizes the syn-tactic and lexical overlapping of the two subtreesinduced by the aligned set of anchors.More precisely, let C be the set of all bijectivemappings from a?
?
A?
: |a?| = |A?
?| to A?
?, anelement c ?
C is a substitution function.
Wedefine as the best alignment the one determinedby cmax = argmaxc?C(KT (t(H ?, c), t(H ?
?, i))+KT (t(T ?, c), t(T ?
?, i)) (1)where (a) t(S, c) returns the syntactic tree of thehypothesis (text) S with placeholders replaced bymeans of the substitution c, (b) i is the identitysubstitution and (c) KT (t1, t2) is a function thatmeasures the similarity between the two trees t1and t2 (for more details see Sec.
4.2).
For ex-ample, the cmax between (T1,H1) and (T3,H3)is {( 2?
, a?
), ( 2?
, a?
), ( 3 , b ), ( 4 , c )}.4 Similarity ModelsIn this section we describe how anchors are foundat the level of a single pair (T,H) (Sec.
4.1).
Theanchoring process gives the direct possibility of403implementing an inter-pair similarity that can beused as a baseline approach or in combination withthe cross-pair similarity.
This latter will be imple-mented with tree kernel functions over syntacticstructures (Sec.
4.2).4.1 Anchoring and Lexical SimilarityThe algorithm that we design to find the anchorsis based on similarity functions between words ormore complex expressions.
Our approach is in linewith many other researches (e.g., (Corley and Mi-halcea, 2005; Glickman et al, 2005)).Given the set of content words (verbs, nouns,adjectives, and adverbs) WT and WH of the twosentences T and H , respectively, the set of anchorsA ?
WT ?WH is built using a similarity measurebetween two words simw(wt, wh).
Each elementwh ?
WH will be part of a pair (wt, wh) ?
A if:1) simw(wt, wh) 6= 02) simw(wt, wh) = maxw?t?WT simw(w?t, wh)According to these properties, elements in WHcan participate in more than one anchor and con-versely more than one element in WH can belinked to a single element w ?
WT .The similarity simw(wt, wh) can be defined us-ing different indicators and resources.
First of all,two words are maximally similar if these have thesame surface form wt = wh.
Second, we can useone of the WordNet (Miller, 1995) similarities in-dicated with d(lw, lw?)
(in line with what was donein (Corley and Mihalcea, 2005)) and different rela-tion between words such as the lexical entailmentbetween verbs (Ent) and derivationally relationbetween words (Der).
Finally, we use the edit dis-tance measure lev(wt, wh) to capture the similar-ity between words that are missed by the previousanalysis for misspelling errors or for the lack ofderivationally forms not coded in WordNet.As result, given the syntactic categorycw ?
{noun, verb, adjective, adverb} andthe lemmatized form lw of a word w, the simi-larity measure between two words w and w?
isdefined as follows:simw(w,w?)
=????????????????
?1 if w = w?
?lw = lw?
?
cw = cw??
((lw, cw), (lw?
, cw? ))
?
Ent?
((lw, cw), (lw?
, cw? ))
?
Der?lev(w, w?)
= 1d(lw, lw? )
if cw = cw?
?
d(lw, lw? )
> 0.20 otherwise(2)It is worth noticing that, the above measure is nota pure similarity measure as it includes the entail-ment relation that does not represent synonymy orsimilarity between verbs.
To emphasize the contri-bution of each used resource, in the experimentalsection, we will compare Eq.
2 with some versionsthat exclude some word relations.The above word similarity measure can be usedto compute the similarity between T and H .
Inline with (Corley and Mihalcea, 2005), we defineit as:s1(T,H) =?
(wt,wh)?Asimw(wt, wh)?
idf(wh)?wh?WHidf(wh)(3)where idf(w) is the inverse document frequencyof the word w. For sake of comparison, weconsider also the corresponding more classicalversion that does not apply the inverse documentfrequencys2(T,H) =?
(wt,wh)?Asimw(wt, wh)/|WH | (4)?From the above intra-pair similarities, s1and s2, we can obtain the baseline cross-pairsimilarities based on only lexical information:Ki((T ?, H ?
), (T ?
?,H ??))
= si(T ?, H ?)?
si(T ?
?,H ??
), (5)where i ?
{1, 2}.
In the next section we define anovel cross-pair similarity that takes into accountsyntactic evidence by means of tree kernel func-tions.4.2 Cross-pair syntactic kernelsSection 3 has shown that to measure the syn-tactic similarity between two pairs, (T ?,H ?
)and (T ?
?,H ??
), we should capture the number ofcommon subtrees between texts and hypothesesthat share the same anchoring scheme.
The bestalignment between anchor sets, i.e.
the bestsubstitution cmax, can be found with Eq.
1.
As thecorresponding maximum quantifies the alignmentdegree, we could define a cross-pair similarity asfollows:Ks((T ?,H ?
), (T ?
?,H ??))
= maxc?C(KT (t(H ?, c), t(H ?
?, i))+KT (t(T ?, c), t(T ?
?, i)), (6)where as KT (t1, t2) we use the tree kernel func-tion defined in (Collins and Duffy, 2002).
Thisevaluates the number of subtrees shared by t1 andt2, thus defining an implicit substructure space.Formally, given a subtree space F ={f1, f2, .
.
.
, f|F|}, the indicator function Ii(n)is equal to 1 if the target fi is rooted atnode n and equal to 0 otherwise.
A tree-kernel function over t1 and t2 is KT (t1, t2) =?n1?Nt1?n2?Nt2?
(n1, n2), where Nt1 and Nt2are the sets of the t1?s and t2?s nodes, respectively.In turn ?
(n1, n2) =?|F|i=1 ?l(fi)Ii(n1)Ii(n2),404where 0 ?
?
?
1 and l(fi) is the number of lev-els of the subtree fi.
Thus ?l(fi) assigns a lowerweight to larger fragments.
When ?
= 1, ?
isequal to the number of common fragments rootedat nodes n1 and n2.
As described in (Collins andDuffy, 2002), ?
can be computed in O(|Nt1 | ?|Nt2 |).The KT function has been proven to be a validkernel, i.e.
its associated Gram matrix is positive-semidefinite.
Some basic operations on kernelfunctions, e.g.
the sum, are closed with respectto the set of valid kernels.
Thus, if the maximumheld such property, Eq.
6 would be a valid ker-nel and we could use it in kernel based machineslike SVMs.
Unfortunately, a counterexample il-lustrated in (Boughorbel et al, 2004) shows thatthe max function does not produce valid kernels ingeneral.However, we observe that: (1)Ks((T ?,H ?
), (T ?
?,H ??))
is a symmetric func-tion since the set of transformation C are alwayscomputed with respect to the pair that has thelargest anchor set; (2) in (Haasdonk, 2005), itis shown that when kernel functions are notpositive semidefinite, SVMs still solve a dataseparation problem in pseudo Euclidean spaces.The drawback is that the solution may be onlya local optimum.
Therefore, we can experimentEq.
6 with SVMs and observe if the empiricalresults are satisfactory.
Section 6 shows that thesolutions found by Eq.
6 produce accuracy higherthan those evaluated on previous automatic textualentailment recognition approaches.5 Refining cross-pair syntactic similarityIn the previous section we have defined the intraand the cross pair similarity.
The former does notshow relevant implementation issues whereas thelatter should be optimized to favor its applicabilitywith SVMs.
The Eq.
6 improvement depends onthree factors: (1) its computation complexity; (2)a correct marking of tree nodes with placeholders;and, (3) the pruning of irrelevant information inlarge syntactic trees.5.1 Controlling the computational costThe computational cost of cross-pair similarity be-tween two tree pairs (Eq.
6) depends on the size ofC .
This is combinatorial in the size of A?
and A??,i.e.
|C| = (|A?|?
|A??|)!|A??|!
if |A?| ?
|A??|.
Thuswe should keep the sizes of A?
and A??
reasonablysmall.To reduce the number of placeholders, we con-sider the notion of chunk defined in (Abney, 1996),i.e., not recursive kernels of noun, verb, adjective,and adverb phrases.
When placeholders are in asingle chunk both in the text and hypothesis weassign them the same name.
For example, Fig.
1shows the placeholders 2?
and 2?
that are substi-tuted by the placeholder 2 .
The placeholder re-duction procedure also gives the possibility of re-solving the ambiguity still present in the anchorset A (see Sec.
4.1).
A way to eliminate the am-biguous anchors is to select the ones that reducethe final number of placeholders.5.2 Augmenting tree nodes with placeholdersAnchors are mainly used to extract relevant syn-tactic subtrees between pairs of text and hypoth-esis.
We also use them to characterize the syn-tactic information expressed by such subtrees.
In-deed, Eq.
6 depends on the number of commonsubtrees between two pairs.
Such subtrees arematched when they have the same node labels.Thus, to keep track of the argument movements,we augment the node labels with placeholders.The larger number of placeholders two hypothe-ses (texts) match the larger the number of theircommon substructures is (i.e.
higher similarity).Thus, it is really important where placeholders areinserted.For example, the sentences in the pair (T1,H1)have related subjects 2 and related main verbs3 .
The same occurs in the sentences of the pair(T3,H3), respectively a and b .
To obtain suchnode marking, the placeholders are propagated inthe syntactic tree, from the leaves1 to the targetnodes according to the head of constituents.
Theexample of Fig.
1 shows that the placeholder 0climbs up to the node governing all the NPs.5.3 Pruning irrelevant information in largetext treesOften only a portion of the parse trees is relevantto detect entailments.
For instance, let us considerthe following pair from the RTE 2005 corpus:1To increase the generalization capacity of the tree ker-nel function we choose not to assign any placeholder to theleaves.405T ?
H (id: 929)T ?Ron Gainsford, chief executive of theTSI, said: ?It is a major concern to usthat parents could be unwittingly expos-ing their children to the risk of sun dam-age, thinking they are better protectedthan they actually are.
?H ?Ron Gainsford is the chief executive ofthe TSI.
?Only the bold part of T supports the implication;the rest is useless and also misleading: if we usedit to compute the similarity it would reduce the im-portance of the relevant part.
Moreover, as we nor-malize the syntactic tree kernel (KT ) with respectto the size of the two trees, we need to focus onlyon the part relevant to the implication.The anchored leaves are good indicators of rel-evant parts but also some other parts may be veryrelevant.
For example, the function word not playsan important role.
Another example is given by theword insurance in H1 and mountain in H3 (seeFig.
1).
They support the implication T1 ?
H1and T1 ?
H3 as well as cash supports T1 ; H2.By removing these words and the related struc-tures, we cannot determine the correct implica-tions of the first two and the incorrect implicationof the second one.
Thus, we keep all the words thatare immediately related to relevant constituents.The reduction procedure can be formally ex-pressed as follows: given a syntactic tree t, the setof its nodes N(t), and a set of anchors, we builda tree t?
with all the nodes N ?
that are anchors orancestors of any anchor.
Moreover, we add to t?the leaf nodes of the original tree t that are directchildren of the nodes in N ?.
We apply such proce-dure only to the syntactic trees of texts before thecomputation of the kernel function.6 Experimental investigationThe aim of the experiments is twofold: we showthat (a) entailment recognition rules can be learnedfrom examples and (b) our kernel functions oversyntactic structures are effective to derive syntac-tic properties.
The above goals can be achieved bycomparing the different intra and cross pair simi-larity measures.6.1 Experimental settingsFor the experiments, we used the RecognizingTextual Entailment Challenge data sets, which wename as follows:- D1, T1 and D2, T2, are the development andthe test sets of the first (Dagan et al, 2005) andsecond (Bar Haim et al, 2006) challenges, respec-tively.
D1 contains 567 examples whereas T1,D2 and T2 have all the same size, i.e.
800 train-ing/testing instances.
The positive examples con-stitute the 50% of the data.- ALL is the union of D1, D2, and T1, which wealso split in 70%-30%.
This set is useful to test ifwe can learn entailments from the data prepared inthe two different challenges.- D2(50%)?
and D2(50%)??
is a random split ofD2.
It is possible that the data sets of the two com-petitions are quite different thus we created thishomogeneous split.We also used the following resources:- The Charniak parser (Charniak, 2000) and themorpha lemmatiser (Minnen et al, 2001) to carryout the syntactic and morphological analysis.- WordNet 2.0 (Miller, 1995) to extract both theverbs in entailment, Ent set, and the derivation-ally related words, Der set.- The wn::similarity package (Pedersen etal., 2004) to compute the Jiang&Conrath (J&C)distance (Jiang and Conrath, 1997) as in (Corleyand Mihalcea, 2005).
This is one of the best fig-ure method which provides a similarity score inthe [0, 1] interval.
We used it to implement thed(lw, lw?)
function.- A selected portion of the British National Cor-pus2 to compute the inverse document frequency(idf ).
We assigned the maximum idf to words notfound in the BNC.- SVM-light-TK3 (Moschitti, 2006) which en-codes the basic tree kernel function, KT , in SVM-light (Joachims, 1999).
We used such softwareto implement Ks (Eq.
6), K1, K2 (Eq.
5) andKs + Ki kernels.
The latter combines our newkernel with traditional approaches (i ?
{1, 2}).6.2 Results and analysisTable 1 reports the results of different similaritykernels on the different training and test splits de-scribed in the previous section.
The table is orga-nized as follows:The first 5 rows (Experiment settings) report theintra-pair similarity measures defined in Section4.1, the 6th row refers to only the idf similaritymetric whereas the following two rows report thecross-pair similarity carried out with Eq.
6 with(Synt Trees with placeholders) and without (OnlySynt Trees) augmenting the trees with placehold-ers, respectively.
Each column in the Experiment2http://www.natcorp.ox.ac.uk/3SVM-light-TK is available at http://ai-nlp.info.uniroma2.it/moschitti/406Experiment Settingsw = w?
?
lw = lw?
?
cw = cw??
?
?
?
?
?
?
?cw = cw?
?
d(lw, lw? )
> 0.2?
?
?
?
?
?
((lw , cw), (lw?
, cw? ))
?
Der?
?
?
?
((lw , cw), (lw?
, cw? ))
?
Ent?
?
?
?lev(w, w?)
= 1 ?
?
?idf ?
?
?
?
?
?Only Synt Trees?Synt Trees with placeholders?Datasets?Train:D1-Test:T1?
0.5388 0.5813 0.5500 0.5788 0.5900 0.5888 0.6213 0.6300?Train:T1-Test:D1?
0.5714 0.5538 0.5767 0.5450 0.5591 0.5644 0.5732 0.5838?Train:D2(50%)?-Test:D2(50%)???
0.6034 0.5961 0.6083 0.6010 0.6083 0.6083 0.6156 0.6350?Train:D2(50%)??
-Test:D2(50%)??
0.6452 0.6375 0.6427 0.6350 0.6324 0.6272 0.5861 0.6607?Train:D2-Test:T2?
0.6000 0.5950 0.6025 0.6050 0.6050 0.6038 0.6238 0.6388Mean 0.5918 0.5927 0.5960 0.5930 0.5990 0.5985 0.6040 0.6297(?
0.0396 ) (?
0.0303 ) (?
0.0349 ) (?
0.0335 ) (?
0.0270 ) (?
0.0235 ) (?
0.0229 ) (?
0.0282 )?Train:ALL(70%)-Test:ALL(30%)?
0.5902 0.6024 0.6009 - 0.6131 0.6193 0.6086 0.6376?Train:ALL-Test:T2?
0.5863 0.5975 0.5975 0.6038 - - 0.6213 0.6250Table 1: Experimental results of the different methods over different test settingssettings indicates a different intra-pair similaritymeasure built by means of a combination of basicsimilarity approaches.
These are specified with thecheck sign ?.
For example, Column 5 refers to amodel using: the surface word form similarity, thed(lw, lw?)
similarity and the idf .The next 5 rows show the accuracy on the datasets and splits used for the experiments and thenext row reports the average and Std.
Dev.
overthe previous 5 results.
Finally, the last two rowsreport the accuracy on ALL dataset split in 70/30%and on the whole ALL dataset used for trainingand T2 for testing.
?From the table we note the following aspects:- First, the lexical-based distance kernels K1 andK2 (Eq.
5) show accuracy significantly higher thanthe random baseline, i.e.
50%.
In all the datasets(except for the first one), the simw(T,H) simi-larity based on the lexical overlap (first column)provides an accuracy essentially similar to the bestlexical-based distance method.- Second, the dataset ?Train:D1-Test:T1?
allowsus to compare our models with the ones of the firstRTE challenge (Dagan et al, 2005).
The accuracyreported for the best systems, i.e.
58.6% (Glick-man et al, 2005; Bayer et al, 2005), is not signif-icantly different from the result obtained with K1that uses the idf .- Third, the dramatic improvement observed in(Corley and Mihalcea, 2005) on the dataset?Train:D1-Test:T1?
is given by the idf rather thanthe use of the J&C similarity (second vs. thirdcolumns).
The use of J&C with the idf decreasesthe accuracy of the idf alone.- Next, our approach (last column) is significantlybetter than all the other methods as it provides thebest result for each combination of training andtest sets.
On the ?Train:D1-Test:T1?
test set, itexceeds the accuracy of the current state-of-the-art models (Glickman et al, 2005; Bayer et al,2005) by about 4.4 absolute percent points (63%vs.
58.6%) and 4% over our best lexical simi-larity measure.
By comparing the average on alldatasets, our system improves on all the methodsby at least 3 absolute percent points.- Finally, the accuracy produced by Synt Trees withplaceholders is higher than the one obtained withOnly Synt Trees.
Thus, the use of placeholdersis fundamental to automatically learn entailmentsfrom examples.6.2.1 Qualitative analysisHereafter we show some instances selectedfrom the first experiment ?Train:T1-Test:D1?.They were correctly classified by our overallmodel (last column) and miss-classified by themodels in the seventh and in the eighth columns.The first is an example in entailment:T ?
H (id: 35)T ?Saudi Arabia, the biggest oil pro-ducer in the world, was once a sup-porter of Osama bin Laden and hisassociates who led attacks against theUnited States.
?H ?Saudi Arabia is the world?s biggest oilexporter.
?It was correctly classified by exploiting exampleslike these two:T ?
H (id: 929)T ?Ron Gainsford, chief executive of theTSI, said: ...?H ?Ron Gainsford is the chief executive ofthe TSI.
?T ?
H (id: 976)T ?Harvey Weinstein, the co-chairman ofMiramax, who was instrumental in pop-ularizing both independent and foreignfilms with broad audiences, agrees.
?H ?Harvey Weinstein is the co-chairmanof Miramax.
?407The rewrite rule is: ?X, Y, ...?
implies ?X is Y?.This rule is also described in (Hearst, 1992).A more interesting rule relates the followingtwo sentences which are not in entailment:T ; H (id: 2045)T ?Mrs.
Lane, who has been a Directorsince 1989, is Special Assistant to theBoard of Trustees and to the Presidentof Stanford University.
?H ?Mrs.
Lane is the president of StanfordUniversity.
?It was correctly classified using instances like thefollowing:T ; H (id: 2044)T ?Jacqueline B. Wender is Assistant tothe President of Stanford University.
?H ?Jacqueline B. Wender is the Presidentof Stanford University.
?T ; H (id: 2069)T ?Grieving father Christopher Yavelowhopes to deliver one million letters tothe queen of Holland to bring his chil-dren home.
?H ?Christopher Yavelow is the queen ofHolland.
?Here, the implicit rule is: ?X (VP (V ...) (NP (to Y)...)?
does not imply ?X is Y?.7 ConclusionsWe have presented a model for the automaticlearning of rewrite rules for textual entailmentsfrom examples.
For this purpose, we devised anovel powerful kernel based on cross-pair simi-larities.
We experimented with such kernel us-ing Support Vector Machines on the RTE testsets.
The results show that (1) learning entailmentsfrom positive and negative examples is a viable ap-proach and (2) our model based on kernel meth-ods is highly accurate and improves on the currentstate-of-the-art entailment systems.In the future, we would like to study approachesto improve the computational complexity of ourkernel function and to design approximated ver-sions that are valid Mercer?s kernels.ReferencesSteven Abney.
1996.
Part-of-speech tagging and partial pars-ing.
In G.Bloothooft K.Church, S.Young, editor, Corpus-based methods in language and speech.
Kluwer academicpublishers, Dordrecht.Roy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Gi-ampiccolo, Bernardo Magnini, and Idan Szpektor.
2006.The II PASCAL RTE challenge.
In RTE Workshop,Venice, Italy.Samuel Bayer, John Burger, Lisa Ferro, John Henderson, andAlexander Yeh.
2005.
MITRE?s submissions to the euPASCAL RTE challenge.
In Proceedings of the 1st RTEWorkshop, Southampton, UK.Johan Bos and Katja Markert.
2005.
Recognising textual en-tailment with logical inference.
In Proc.
of HLT-EMNLPConference, Canada.S.
Boughorbel, J-P. Tarel, and F. Fleuret.
2004.
Non-mercerkernel for svm object recognition.
In Proceedings ofBMVC 2004.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
of the 1st NAACL,Seattle, Washington.Gennaro Chierchia and Sally McConnell-Ginet.
2001.Meaning and Grammar: An introduction to Semantics.MIT press, Cambridge, MA.Michael Collins and Nigel Duffy.
2002.
New ranking al-gorithms for parsing and tagging: Kernels over discretestructures, and the voted perceptron.
In Proceedings ofACL02.Courtney Corley and Rada Mihalcea.
2005.
Measuring thesemantic similarity of texts.
In Proc.
of the ACL Workshopon Empirical Modeling of Semantic Equivalence and En-tailment, Ann Arbor, Michigan.Ido Dagan and Oren Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.
In Proceedings of the Workshop on LearningMethods for Text Understanding and Mining, Grenoble,France.Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005.The PASCAL RTE challenge.
In RTE Workshop,Southampton, U.K.Rodrigo de Salvo Braz, Roxana Girju, Vasin Punyakanok,Dan Roth, and Mark Sammons.
2005.
An inferencemodel for semantic entailment in natural language.
InProc.
of the RTE Workshop, Southampton, U.K.Oren Glickman, Ido Dagan, and Moshe Koppel.
2005.
Webbased probabilistic textual entailment.
In Proceedings ofthe 1st RTE Workshop, Southampton, UK.Bernard Haasdonk.
2005.
Feature space interpretation ofSVMs with indefinite kernels.
IEEE Trans Pattern AnalMach Intell.Marti A. Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proc.
of the 15th CoLing,Nantes, France.Jay J. Jiang and David W. Conrath.
1997.
Semantic simi-larity based on corpus statistics and lexical taxonomy.
InProc.
of the 10th ROCLING, Tapei, Taiwan.Thorsten Joachims.
1999.
Making large-scale svm learningpractical.
In Advances in Kernel Methods-Support VectorLearning.
MIT Press.Milen Kouylekov and Bernardo Magnini.
2005.
Tree editdistance for textual entailment.
In Proc.
of the RANLP-2005, Borovets, Bulgaria.George A. Miller.
1995.
WordNet: A lexical database forEnglish.
Communications of the ACM, November.Guido Minnen, John Carroll, and Darren Pearce.
2001.
Ap-plied morphological processing of English.
Natural Lan-guage Engineering.Alessandro Moschitti.
2006.
Making tree kernels practicalfor natural language learning.
In Proceedings of EACL?06,Trento, Italy.Ted Pedersen, Siddharth Patwardhan, and Jason Michelizzi.2004.
Wordnet::similarity - measuring the relatedness ofconcepts.
In Proc.
of 5th NAACL, Boston, MA.Vladimir Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer.Annie Zaenen, Lauri Karttunen, and Richard Crouch.
2005.Local textual inference: Can it be defined or circum-scribed?
In Proc.
of the ACL Workshop on EmpiricalModeling of Semantic Equivalence and Entailment, AnnArbor, Michigan.408
