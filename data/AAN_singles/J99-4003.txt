Speech Repairs, Intonational Phrases, andDiscourse Markers: Modeling Speakers'Utterances in Spoken DialoguePeter  A.  Heeman*Oregon Graduate InstituteJames  F. A l len  tUniversity of RochesterInteractive spoken dialogue provides many new challenges for natural anguage understandingsystems.
One of the most critical challenges i simply determining the speaker's intended utter-ances: both segmenting a speaker's turn into utterances and determining the intended words ineach utterance.
Even assuming perfect word recognition, the latter problem is complicated by theoccurrence of speech repairs, which occur where speakers go back and change (or repeat) somethingthey just said.
The words that are replaced or repeated are no longer part of the intended utterance,and so need to be identified.
Segmenting turns and resolving repairs are strongly intertwinedwith a third task: identifying discourse markers.
Because of the interactions, and interactionswith POS tagging and speech recognition, we need to address these tasks together and early on inthe processing stream.
This paper presents a statistical language model in which we rede~'ne thespeech recognition problem so that it includes the identification of POS tags, discourse markers,speech repairs, and intonational phrases.
By solving these simultaneously, we obtain better esultson each task than addressing them separately.
Our model is able to identify 72% of turn-internalintonational boundaries with a precision of 71%, 97% of discourse markers with 96% precision,and detect and correct 66% of repairs with 74% precision.1.
IntroductionConsider the following example from the Trains corpus (Heeman and Allen 1995).Example 1 (d93-13.3 utt63)um it'll be there it'll get to Dansville at three a.m. and then you wanna do you taketho- want to take those back to Elmira so engine E two with three boxcars will be backin Elmira at six a.m. is that what you wanna doIn order to understand what the speaker was trying to say, the reader probably seg-mented the above into a number  of sentence-like segments, utterances, as follows.Example 1 Revisitedum it'll be there it'll get to Dansville at three a.m.and then you wanna do you take tho- want to take those back to Elmiraso engine E two with three boxcars will be back in Elmira at six a.m.is that what you wanna do* Computer Science and Engineering, P.O.
Box 91000, Portland, OR 97291.
E-mail: heeman@cse.ogi.edut Department of Computer Science, Rochester, NY 14627.
E-mail: james@cs.rochester.edu(~) 1999 Association for Computational LinguisticsComputational Linguistics Volume 25, Number 4Even this does not fully capture what the speaker was intending to convey.
Thefirst and second utterances contain speech repairs, where the speaker goes back andchanges (or repeats) something she just said.
In the first, the speaker changed it'll bethere to it'll get to; in the second, she changed you wanna to do you take tho-, which shethen further evised.
The speaker's intended utterances are thus as follows: 1Example 1 Revisited Againum it'll get to Dansville at three a.m.and then do you want to take those back to Elmiraso engine E two with three boxcars will be back in Elmira at six a.m.is that what you wanna doThe tasks of segmenting speakers' turns into utterance units and resolving speechrepairs are strongly intertwined with a third task: identifying whether words, suchas so, well, and right, are part of the sentential content or are being used as discoursemarkers to relate the current speech to the preceding context.
In the example above,the second and third utterances begin with discourse markers.1.1 Utterance Units and Intonational PhrasesAs illustrated above, understanding a speaker's turn necessitates segmenting it intoindividual utterance units.
However, there is no consensus as to how to define anutterance unit (Traum and Heeman 1997).
The manner in which speakers break theirspeech into intonational phrases undoubtedly plays a major role in its definition.
Into-national phrase ndings are signaled through variations in the pitch contour, segmentallengthening, and pauses.
Beach (1991) demonstrated that hearers can use intonationalinformation early on in sentence processing to help resolve syntactic ambiguities.Bear and Price (1990) showed that a parser can use automatically extracted intona-tional phrasing to reduce ambiguity and improve efficiency.
Ostendorf, Wightman,and Veilleux (1993) used hand-labeled intonational phrasing to do syntactic disam-biguation and achieved performance comparable to that of human listeners.
Due totheir significance, we will focus on the task of detecting intonational phrase bound-aries.1.2 Speech RepairsThe on-line nature of spoken dialogue forces conversants o sometimes start speak-ing before they are sure of what they want to say.
Hence, the speaker might needto go back and repeat or modify what she just said.
Of course there are many dif-ferent reasons why speakers make repairs; but whatever the reason, speech repairsare a normal occurrence in spoken dialogue.
In the Trains corpus, 23% of speakerturns contain at least one repair and 54% of turns with at least 10 words contain arepair.Fortunately for the hearer, speech repairs tend to have a standard form.
As illus-trated by the following example, they can be divided into three intervals, or stretchesof speech: the reparandum, editing term, and alteration.
21 The speech that was revised cannot simply be thrown out since it might contain i formation, such asthe identity of an anaphoric reference asthe following example shows: Peter was well he wasfired.528Heeman and Allen Modeling Speakers' UtterancesExample 2 (d92a-2.1 utt29)that's the one with the bananas I mean that's taking the bananasreparandum ip editing terms alterationThe reparandum is the stretch of speech that the speaker is replacing, and can endwith a word fragment, where the speaker interrupts herself during the middle of aword.
The end of the reparandum is the interruption point and is often accompaniedby a disruption in the intonational contour.
This can be optionally followed by theediting term, which can consist of filled pauses, such as um or uh or cue phrases,such as I mean, well, or let's see.
Reparanda nd editing terms account for 10% of thewords in the Trains corpus.
The last part is the alteration, which is the speech thatthe speaker intends as the replacement for the reparandum.
In order for the hearer todetermine the intended utterance, he must detect he repair and determine the extentof the reparandum and editing term.
We refer to this latter process as correcting thespeech repair.
In the example above, the speaker's intended utterance is that's the onethat's taking the bananas.Hearers seem to be able to effortlessly understand speech with repairs in it, evenwhen multiple repairs occur in a row.
In laboratory experiments, Martin and Strange(1968) found that attending to speech repairs and the content of an utterance aremutually inhibitory, and Bard and Lickley (1997) found that subjects have difficultyremembering the actual words in the reparandum.
Listeners must be resolving repairsvery early on in processing the speech.
Earlier work by Lickley and colleagues (Lick-ley, Shillcock, and Bard 1991; Lickley and Bard 1992) strongly suggests that there areprosodic cues across the interruption point that hearers make use of in detecting re-pairs.
However, little progress has been made in detecting speech repairs based solelyon acoustical cues (cf.
Bear, Dowding, and Shriberg 1992; Nakatani and Hirschberg1994; O'Shaughnessy 1994; Shriberg, Bates, and Stolcke 1997).1.2.1 Classification of Speech Repairs.
Psycholinguistic work in speech repairs andin understanding the implications that they pose for theories of speech production(e.g.
Levelt 1983; Blackmer and Mitton 1991; Shriberg 1994) has come up with a num-ber of classification systems.
Categories are based on how the reparandum and al-teration differ, for instance whether the alteration repeats the reparandum, makes itmore appropriate, or fixes an error in the reparandum.
Such an analysis can shedlight on where in the production system the error and its repair originated.
Ourconcern, however, is in computationally resolving repairs.
The relevant features arethose that the hearer has access to and can make use of in detecting and correctinga repair.
Following loosely in the footsteps of the work of Hindle (1983), we dividethem into the following categories: fresh starts, modification repairs, and abridgedrepairs.Fresh starts occur where the speaker abandons the current utterance and startsagain, where the abandonment seems to be acoustically signaled either in the editingterm or at the onset of the alteration.
Example 3 illustrates a fresh start where thespeaker abandons the partial utterance I need to send, and replaces it by the questionhow many boxcars can one engine take.2 Our notation is adapted from Levelt (1983).
We follow Shriberg (1994) and Nakatani and Hirschberg(1994) in using reparandum to refer to the entire interval being replaced.
We use alteration in the sameway.529Computational Linguistics Volume 25, Number 4Example 3 (d93-14.3 utt2)I need to send let's see how many boxcars can one engine takereparandum ip editing terms alterationFor fresh starts, there can sometimes be little or even no correlation between thereparandum and alteration.
Although it is usually easy to determine the reparandumonset, initial discourse markers and preceding intonational phrases can prove prob-lematic.The second type are modification repairs, which comprise the remainder of repairswith a nonempty reparandum.
The example below illustrates this type of repair.Example 4 (d92a-1.2 utt40)you can carry them both onJreparandum lptow both on the same engineYalterationModification repairs tend to have strong word correspondences between the reparan-dum and alteration, which can help the hearer determine the reparandum onset aswell as signal that a repair occurred.
In the example above, there are word matches onthe instances of both and on, and a replacement of the verb carry by tow.
Modificationrepairs can in fact consist solely of the reparandum being repeated by the alteration.The third type are the abridged repairs.
These repairs consist of an editing term,but with no reparandum, asthe following example illustrates.Example 5 (d93-14.3 utt42)we need to um manage to get the bananas to Dansville more quicklyT vIp editing termsFor these repairs, the hearer has to determine that an editing term occurred, whichcan be difficult for phrases uch as let's see or well since they can also have a sententialinterpretation.
The hearer also has to determine that the reparandum is empty.
As theexample above illustrates, this is not necessarily a trivial task because of the spuriousword correspondences between eed to and manage to.1.3 Discourse MarkersPhrases uch as so, now, firstly, moreover, and anyways can be used as discourse mark-ers (Schiffrin 1987).
Discourse markers are conjectured to give the hearer informationabout the discourse structure, and so aid the hearer in understanding how the newspeech or text relates to what was previously said and for resolving anaphoric refer-ences (Hirschberg and Litman 1993).
Although discourse markers, such as firstly, andmoreover, are not commonly used in spoken dialogue (Brown and Yule 1983), a lot ofother markers are employed.
These markers are used to achieve a variety of effects:such as signal an acknowledgment or acceptance, hold a turn, stall for time, signal aspeech repair, or signal an interruption i  the discourse structure or the return fromone.Although Schiffrin defines discourse markers as bracketing units of speech, sheexplicitly avoids defining what the unit is.
We feel that utterance units are the building530Heeman and Allen Modeling Speakers' Utterancesblocks of spoken dialogue and that discourse markers operate at this level to relatethe current utterance to the discourse context or to signal a repair in an utterance.
Inthe following example, and then helps signal that the upcoming speech is adding newinformation, while so helps indicate a summary is about to be made.Example 6 (d92a-1.2 utt47)and then while at Dansville take the three boxcarsso that's total of five1.4 InteractionsThe tasks of identifying intonational phrases and discourse markers and detecting andcorrecting speech repairs are highly intertwined, and the solution to each task dependson the solution for the others.1.4.1 Intonational Phrases and Speech Repairs.
Phrase boundaries and interruptionpoints of speech repairs share a number of features that can be used to identify them:there is often a pause at these events as well as lengthening of the final syllable beforethem.
Even correspondences, traditionally associated with speech repairs, can crossphrase boundaries (indicated with "%"), as the following example shows.Example 7 (d93-8.3 utt73)that's all you need %you only need one boxcar %Second, the reparandum onset for repairs, especially fresh starts, often occurs at theonset of an intonational phrase, and reparanda usually do not span phrase boundaries.Third, deciding if filled pauses and cue phrases hould be treated as abridged repairscan only be done by taking into account whether they are midutterance or not (cf.Shriberg and Lickley 1993), which is associated with intonational phrasing.1.4.2 Intonational Phases and Discourse Markers.
Discourse markers tend to be usedat utterance boundaries, and hence have strong interactions with intonational phrasing.In fact, Hirschberg and Litman (1993) found that discourse markers tend to occur at thebeginning of intonational phrases, while sentential usages tend to occur midphrase.Example 8below illustrates so being used midutterance as a subordinating conjunction,not as a discourse marker.Example 8 (d93-15.2 utt9)it takes an hour to load them %just so you know %Now consider the third turn of the following example in which the system isnot using no as a quantifier to mean that there are not any oranges available, butas a discourse marker in signaling that the user misrecognized oranges as orangejuice.531Computational Linguistics Volume 25, Number 4Table 1Frequency of discourse markers in the editing term of speech repairsand as the alteration onset.Abridged Modification Fresh StartsNumber of repairs 423 1,301 671DM in editing term 36 60 155DM as alteration onset 8 126 147Either 41 179 269Example 9 (d93-11.1 utt109-111)system: so so we have three boxcars of oranges at Cominguser: three boxcars of orange juice at Comingsystem: no um orangesThe discourse marker interpretation is facilitated by the phrase boundary betweenno and oranges, especially since the determiner reading of no would be very unlikelyto have a phrase boundary separating it from the noun it modifies.
Likewise, therecognition of no as a discourse marker makes it more likely that there will be aphrase boundary following it.1.4.3 Speech Repairs and Discourse Markers.
Discourse markers are often used inthe editing term to help signal that a repair occurred, and can be used to help de-termine if it is a fresh start (cf.
Hindle 1983; Levelt 1983), as the following exampleillustrates.Example 10 (d92a-1.3 utt75)we have the orange juice in two ohreparandum zp ethow many did we needRealizing that oh is being used as a discourse marker helps facilitate the detection ofthe repair, and vice versus.
This holds even if the discourse marker is not part of theediting term, but is the first word of the alteration.
Table 1 shows the frequency withwhich discourse markers co-occur with speech repairs.
We see that a discourse markeris either part of the editing term or is the alteration onset for 40% of fresh starts and14% of modification repairs.
Discourse markers also play a role in determining theonset for fresh starts, since they are often utterance initial.1.5 Interactions with POS Tagging and Speech RecognitionNot only are the tasks of identifying intonational phrases and discourse markers andresolving speech repairs intertwined, but these tasks are also intertwined with iden-tifying the lexical category or part of speech (POS) of each word, and the speechrecognition problem of predicting the next word given the previous context.Just as POS taggers for text take advantage of sentence boundaries, it is naturalto assume that tagging spontaneous speech would benefit from modeling intonationalphrases and speech repairs.
This is especially true for repairs, since their occurrencedisrupts the local context hat is needed to determine the POS tags (Hindle 1983).
In532Heeman and Allen Modeling Speakers' Utterancesthe example below, both instances of load are being used as verbs; however, since thesecond instance follows a preposition, it could easily be mistaken for a noun.Example 11 (d93-12.4 utt44)by the time we load in load the bananasT reparandum ~pHowever, by realizing that the second instance of load is being used in a repair andcorresponds to the first instance of load, its POS tag becomes obvious.
Conversely,since repairs disrupt the local syntactic ontext, this disruption, as captured by thePOS tags, can be used as evidence that a repair occurred, as shown by the followingexample.Example 12 (d93-13.1 utt90)I can run trains on the in the opposite directionreparandum alterationHere we have a preposition following a determiner, an event hat only happens acrossthe interruption point of a speech repair.Just as there are interactions with POS tagging, the same holds for the speechrecognition problem of predicting the next word given the previous context.
For thelexical context can run trains on the, it would be very unlikely that the word in would benext.
It is only by modeling the occurrence of repairs and their word correspondencesthat we can account for the speaker's words.There are also interactions with intonational phrasing.
In the example below, afterasking the question what time do we have to get done by, the speaker efines this to bewhether they have to be done by two p.m.
The result, however, is that there is arepetition of the word by, but separated by a phrase boundary.Example 13 (d93-18.1 utt58)what time do we have to get done by %by two p.m. %By modeling the intonational phrases, POS taggers and speech recognition languagemodels would be expecting a POS tag and word that can introduce a new phrase.1.6 Modeling Speakers' UtterancesIn this paper, we address the problem of modeling speakers' utterances in spokendialogue, which involves identifying intonational phrases and discourse markers anddetecting and correcting speech repairs.
We propose that these tasks can be done usinglocal context and early in the processing stream.
Hearers are able to resolve speechrepairs and intonational phrase boundaries very early on, and hence there must beenough cues in the local context o make this feasible.
We redefine the speech recog-nition problem so that it includes the resolution of speech repairs and identificationof intonational phrases, discourse markers, and POS tags, which results in a statisticallanguage model that is sensitive to speakers' utterances.
Since all tasks are being re-solved in the same model, we can account for the interactions between the tasks in a533Computational Linguistics Volume 25, Number 4Timing l a I , ,~m6,n:  k /It takes no t i t~ it, cf,nple .
r  decemlde cars ~ .
.
.-..u...~I?
takes 1 hcmr t .
le~d t,e unh~d any amcmm ?~?
carge, ?,11 a t ramManuf~luring O J: One be,xcar oranges c,,nwrts into .me tanker hxld.
Any a,~,,mt can be made in e~le hmtr,Figure 1Map used by the system in collecting the Trains corpus.framework that can compare alternative hypotheses for the speaker's turn.
Not onlydoes this allow us to model the speaker's utterance, but it also results in an improvedlanguage model, evidenced by both improved POS tagging and in better estimatingthe probability of the next word.
Furthermore, speech repairs and phrase boundarieshave acoustic orrelates, such as pauses between words.
By resolving speech repairsand identifying intonational phrases during speech recognition, these acoustic cues,which otherwise would be treated as noise, can give evidence as to the occurrence ofthese events, and further improve speech recognition results.Resolving the speaker's utterances early on will not only help a speech recognizerdetermine what was said, but it will also help later processing, such as syntactic andsemantic analysis.
The literature (e.g., Bear and Price 1990; Ostendorf, Wightman, andVeilleus 1993) already indicates the usefulness of intonational information for syntacticprocessing.
Resolving speech repairs will further simplify syntactic and semantic un-derstanding of spontaneous speech, since it will remove the apparent ill-formednessthat speech repairs cause.
This will also make it easier for these processes to cope withthe added syntactic and semantic variance that spoken dialogue seems to license.1.7 Overview of the PaperWe next describe the Trains corpus and the annotation of speech repairs, intonationalphrases, discourse markers, and POS tags.
We then introduce a language model thatincorporates POS tagging and discourse marker identification.
We then augment itwith speech repair and intonational phrase detection, repair correction, and silenceinformation, and give a sample run of the model.
We then evaluate the model byanalyzing the effects that each component of the model has on the other components.Finally, we compare our work with previous work and present he conclusions andfuture work.2.
The Trains CorpusOne of the goals that we are pursuing at the University of Rochester is the developmentof a conversationally proficient planning assistant, which assists a user in constructinga plan to achieve some task involving the manufacture and shipment of goods in arailroad freight system (Allen et al 1995).
In order to do this, we need to know whatkinds of phenomena occur in such dialogue.
To this end, we have collected a corpusof human-human dialogues (Heeman and Allen 1995).
The person playing the role ofthe system was given the map in Figure 1.
The user was also given a map, but lacking534Heeman and Allen Modeling Speakers' UtterancesTable 2Size of the Trains corpus.Dialogues 98 Intonational Phrases 10,947Speaker Turns 6,163 Turn-Internal Phrase Boundaries 5,535Words 58,298 Abridged Repairs 423Fragments 756 Modification Repairs 1,302Filled Pauses 1,498 Fresh Starts 671Discourse Markers 8,278 Editing Terms 1,128the distances and timing information.
The collection procedure was designed to makethe setting as close to human-computer interaction as possible, but was not a Wizardof Oz scenario, where one person pretends to be a computer; ather, both participantsknow that they are speaking to a real person.
Thus these dialogues provide a snapshotinto an ideal human-computer interface that is able to engage in fluent conversation.Table 2 gives details about the Trains corpus.
The corpus consists of six and a halfhours of speech produced by 34 different speakers olving 20 different problems.The Trains corpus provides natural examples of dialogue usage that spoken dia-logue systems need to handle in order to carry on a dialogue with a user.
For instance,the corpus contains instances of overlapping speech, back-channel responses, and turntaking: phenomena that do not occur in collections of single speaker utterances, uchas ATIS (MADCOW 1992).
The Trains corpus also differs from the Switchboard corpus(Godfrey, Holliman, and McDaniel 1992) in that it is task oriented and has a limiteddomain, making it a more realistic domain for studying the types of conversationsthat people would want to have with a computer.2.1 Word TranscriptionTable 3 gives a dialogue from the Trains corpus.
Overlapping speech is indicated bythe "+" markings.
Each word was transcribed using its orthographic spelling, unlessit was mispronounced and the speaker subsequently repairs the mispronunciation.Contractions, including words such as wanna, were transcribed as single words.
Wordfragments were annotated by spelling as much of the word as can be heard followedby a dash.
If it was clear what word the speaker was saying, then the rest of the wordwas enclosed in parentheses before the dash.2.2 POS and Discourse Marker Annotat ionsOur POS tagset is based on the Penn tagset (Marcus, Santorini, and Marcinkiewicz1993), but modified to include tags for discourse markers and end-of-turns, and toprovide richer syntactic information (Heeman 1997).
Table 4 lists our tagset with dif-ferences from the Penn tagset marked in bold.
Contractions are annotated using "A"to conjoin the tag for each part; for instance, can't is annotated as MDARB.Discourse marker usage is captured by the POS tags.
The tag AC marks singleword acknowledgments, such as okay, right, mm-hm, and no.
The tag CC_D marksdiscourse conjuncts, uch as and, so, and but.
The tag RB_D marks discourse adverbials,such as then, now, actually, first, and anyway.
Finally, UH_D marks interjections, uchas oh, well, hm, and mm.
Verbs used as discourse markers, such as wait, and see, arenot given special markers, but are annotated as VB.
No attempt has been made atanalyzing multiword discourse markers, such as by the way and you know; however,phrases uch as oh really and and then are treated as two individual discourse markers.535Computational Linguistics Volume 25, Number 4Table 3Transcription of dialogue d93-12.2.Problem 1-BTransport 2 boxcars of bananas to Corning by 11 AM.
It is now midnight.turn1 s:turn2 u:turn3 s:turn4 u:turn5 s:turn6 u:turn7 s:turn8 u:turn9 s:turn10 u:turn11 s:turn12 u:turn13 s:turn14 u:turn15 s:turn16 u:turn17 s:turn18 u:turn19 s:turn20 u:turn21 s:hello can I help youI need to take two boxcars of bananas um from Avon to Coming by eleven a.m.so two boxcars of whatbananasbananas to whereCorningto Coming okayum so the first thing we need to do is to get the uh boxcars to uh Avonokay so there's boxcars in Dansville and there's boxcars in Bathokay is Dansville the shortest routeyepokay how long will it take from to to have the oh I need it ooh how long will ittake to get from Avon to Dansvillethree hoursokay so I'll need to go from Avon to Dansville with the engine to pick up twoboxcarsokay so we'll g- we'll get to Dansville at three a.m.okay I need to return to Avon to load the boxcarsokay so we'll get back to Avon at six a.m. and we'll load them which takes an hourso that'll be done by seven a.m.and then we need to travel to uh Corningokay so the quickest way to Corning is through Dansville which will take fourhours so we'll get there at + eleven a.m. ++ eleven + a.m. okay it's doablegreatTable 4Part-of-speech tags used in annotating the Trains corpus.AC Acknowledgement HAVEP Present ense of haveBE Base form of be HAVEZ 3rd person sing.
presentBED Past tense of be JJ AdjectiveBEG Present participle of be JJR Relative AdjectiveBEN Past participle of be JJS Superlative AdjectiveBEP Present ense of be MD ModalBEZ 3rd person sing.
present NN NounCC Coordinating conjunct NNS Plural nounCC_D Discourse connective NNP Proper NounCD Cardinal number NNPS Plural proper NounDO Base form of do PDT Pre-determinerDOD Past tense of do POS PossessiveDOP Present ense of do PPREP Pre-prepositionDOZ 3rd person sing.
present PREP PrepositionDP Pro-form PRP Personal pronounDT Determiner PRP$ Possessive pronounEX Existential there RB AdverbHAVE Base form of have RBR Relative AdverbHAVED Past tense of have RBS Superlative AdverbRB_D Discourse adverbialRP Reduced particleSC Subordinating conjunctTO To-infinitiveTURN Turn markerUH_D Discourse interjectionUH_FP Filled pauseVB Base form of verb (otherthan do, be, or have)VBD Past tenseVBG Present participleVBN Past participleVBP Present enseVBZ 3rd person sing.
presentWDT Wh-determinerWP Wh-pronounWRB Wh-adverbWP$ Possessive Wh-pronoun2.3 Speech Repair Annotat ionsOur  repair  annotat ion scheme, def ined in Table 5, is based on the one proposed  byBear et al (1993), but  extended to better deal  w i th  ambiguous  and over lapp ing  re-536Heeman and Allen Modeling Speakers' UtterancesTable 5Labels used for annotating speech repairs.ipripr:modipr:canipr:abrsrr<miriXFpietInterruption point of a speech repair.
Index r is used to distinguish between multiplerepairs.
Indices are in multiples of 10 and all word correspondence for the repair aregiven a unique index between the repair index and the next highest repair index.Repair indices of 0 are not marked, as Example 14 illustrates.The mod suffix indicates a modification repair; mod+ indicates uncertainty as to thetype of repair.The can suffix indicates a fresh start (or cancel); can+ marks ambiguous repairs.The abr suffix indicates an abridged repair.Denotes the onset of the reparandum of a fresh start.Used to label word correspondences in which the two words are identical.
The indexi is used both to coindex the two words and to associate them with the repair index.Used to label word correspondences in which one word replaces another.Word deletion or insertion.
It is indexed by the repair index.Multiword correspondence, such as replacement of a pronoun by a longer description.Used to label the editing term that follows the interruption point.pairs (Heeman 1997).
Like their scheme, ours allows the annotator to capture wordcorrespondences between the reparandum and alteration.
Below, we give a repair an-notation.Example 14 (d93-15.2 utt42)engine two from Elmi(ra)- or engine three from Elmiraml r2 m3 m4 T et ml  r2 m3 m4ip:mod+In this example, the reparandum is engine two from Elmi(ra)-, the editing term is or,and the alteration is engine three from Elmira.
The word matches on engine and from areannotated with m and the replacement of two by three is annotated with r. As withthe POS tags, "A" can be used in annotating contracted words.
32.4 Intonat ion Annotat ionsFor our intonation annotation, we have annotated the intonational phrase boundaries,using the ToBI (Tones and Break Indices) definition (Silverman et al 1992).
Intonationalphrases are determined by both the pitch contour and the perceived juncture betweeneach pair of words, where the perceived juncture takes into account both interwordpauses and preboundary lengthening (normalized uration of the final consonants).Labeling with the full ToBI annotation scheme is very time-consuming; hence, welabeled only the intonational phrase boundaries.3.
POS-based Language ModelIn this section, we present a speech recognition language model that incorporates POStagging.
Here, POS tags are viewed as part of the output of the speech recognizerrather than as intermediate objects.
Not only is this syntactic information eeded for3 Shriberg (1994) also extends the scheme of Bear et al (1993) to deal with overlapping repairs.537Computational Linguistics Volume 25, Number 4modeling the occurrence of speech repairs and intonational phrases, but it will also beuseful for higher-level syntactic and semantic processes.
Incorporating POS taggingcan also be seen as a first step in tightening the coupling between speech recognitionand natural anguage processing so as to be able to make use of richer knowledge ofnatural anguage than simple word-based language models provide.3.1 Word-based Language ModelsThe goal of speech recognition is to find the most probable sequence of words I;V giventhe acoustic signal A (Jelinek 1985).1~ = arg mwax Pr(W\]A) (1)Using Bayes' rule, we rewrite the above equation in the following manner.Pr(A\] W) Pr(W) l~ = arg max (2)w Pr(A)Since Pr(A) is independent of the choice of W, we can simplify the above as follows.I,V = arg rn~x Pr(A\[W) Pr(W) (3)The first term, Pr(A\[W), is the acoustic model and the second term, Pr(W), is the lan-guage model.
We can rewrite W explicitly as the sequence of words WIW2W3... WN,where N is the number of words in the sequence.
For expository ease, we use W/,jto refer to Wi... Wj.
We now use the definition of conditional probabilities to rewritePr(W1,N) as follows.NPr(W1,N) = I I  Pr(WilWl,i-1) (4)i=1The above equation gives us the probability of the word sequence as the productof the probability of each word given its previous lexical context.
This probabilitydistribution must be estimated.
The simplest approach to estimating the probability ofan event given a context is to use a training corpus to compute the relative frequencyof the event given the context.
However, no matter how large the corpus is, there willalways be event-context pairs that have not been seen, or have been seen too rarely toaccurately estimate the probability.
To alleviate this problem, one must partition thecontexts into equivalence classes and use these to compute the relative frequencies.
Acommon technique is to partition the context based on the last n - 1 words, Wi-n+l,i-1,which is referred to as an n-gram language model.
One can also mix in smaller-sizelanguage models to use when there is not enough data to support he larger context.Two common approaches for doing this are interpolated estimation (Jelinek and Mercer1980) and the backoff approach (Katz 1987).3.2 Incorporating POS Tags and Discourse Marker IdentificationPrevious attempts to incorporate POS tags into a language model view the POS tagsas intermediate objects and sum over all POS possibilities (Jelinek 1985).Pr(W1,N) = ~Pr(W1,NP1,N)P1,NN= ~ HPr(WiIWI,i-IPI,i) Pr(PilWl,i-lPl,i-1)P1,N i=1(5)538Heeman and Allen Modeling Speakers' UtterancesHowever, this throws away valuable information that is needed by later processing.Instead, we redefine the speech recognition problem so as to include finding the bestPOS and discourse marker sequence along with the best word sequence.
For the wordsequence W, let D be a POS sequence that can include discourse marker tags.
The goalof the speech recognition process is to now solve the following:61b = argmaxPr(WDIA )= argmaxPr(AIWD ) Pr(WD) (6)The first term Pr(AIWD ) is the acoustic model, which can be approximated byPr(AIW ).The second term Pr(WD) is the POS-based language model and accounts for both thesequence of words and their POS assignment.
We rewrite this term as follows:Pr(W1,ND1,N)N= I-IPr(WiDilWl,i_lDl,i_l)i=1N= l-I Pr(WilWl,i-lDl,i) Pr(DilWl,i-lDl,i-1)i=1(7)Equation 7 involves two probability distributions that need to be estimated.
Theseare the same distributions that are needed by previous POS-based language models(Equation 5) and POS taggers (Church 1988; Charniak et al 1993).
However, theseapproaches simplify the context so that the lexical probability is just conditioned onthe POS category of the word, and the POS probability is conditioned on just thepreceding POS tags, which leads to the following two approximations.Pr(WiIWl,i_lDl,i) ~ Pr(WilDi) (8)Pr(DiIWu_lDl,i_l) ~ Pr(DiIDu_l) (9)However, to successfully incorporate POS information, we need to account for the fullrichness of the probability distributions, as will be demonstrated in Section 3.4.4.3.3 Estimating the ProbabilitiesTo estimate the probability distributions, we follow the approach of Bahl et al (1989)and use a decision tree learning algorithm (Breiman et al 1984) to partition the contextinto equivalence classes.
The algorithm starts with a single node.
It then finds a ques-tion to ask about the node in order to partition the node into two leaves, each moreinformative as to which event occurred than the parent node.
Information-theoreticmetrics, such as minimizing entropy, are used to decide which question to propose.The proposed question is then verified using held-out data: if the split does not leadto a decrease in entropy according to the held-out data, the split is rejected and thenode is not further explored.
This process continues with the new leaves and results ina hierarchical partitioning of the context.
After the tree is grown, relative frequenciesare calculated for each node, and these probabilities are then interpolated with theirparent node's probabilities using a second held-out dataset.Using the decision tree algorithm to estimate probabilities i attractive since the al-gorithm can choose which parts of the context are relevant, and in what order.
Hence,this approach lends itself more readily to allowing extra contextual information to be539Computational Linguistics Volume 25, Number 4Figure 2Binary classification tree that encodes the POS tags for the decision tree algorithm.included, such as both the word identities and POS tags, and even hierarchical cluster-ings of them.
If the extra information is not relevant, it will not be used.
The approachof using decision trees will become ven more critical in the next two sections, wherethe probability distributions will be conditioned on even richer context.3.3.1 Simple Questions.
One of the most important aspects of using a decision treealgorithm is the form of the questions that it is allowed to ask.
We allow two basictypes of information to be used as part of the context: numeric and categorical.
For anumeric variable N, the decision tree searches for questions of the form "is N >= n' ,where n is a numeric constant.
For a categorical variable C, it searches over questionsof the form: "is C E S", where S is a subset of the possible values of C. We also allowcomposite questions (Bahl et al 1989), which are Boolean combinations of elementaryquestions.3.3.2 Questions about POS Tags.
The context hat we use for estimating the prob-abilities includes both word identities and POS tags.
To make effective use of thisinformation, we allow the decision tree algorithm to generalize between words andPOS tags that behave similarly.
To learn which ones behave similarly, Black et al(1992) and Magerman (1994) used the clustering algorithm of Brown et al (1992) tobuild a hierarchical classification tree.
Figure 2 gives the tree that we built for thePOS tags.
The algorithm starts with each POS tag in a separate class and iterativelyfinds two classes to merge that results in the smallest loss of information about POSadjacency.
This continues until only a single class remains.
The order in which classeswere merged, however, gives a binary tree with the root corresponding to the entire540Heeman and Allen Modeling Speakers' Utterances~ <low> 2 them 157 me 85 us 176 they 89we 766648Figure 3Binary classification tree that encodes the personal pronouns (PRP).tagset, each leaf to a single POS tag, and intermediate nodes to groupings of the tagsthat are statistically similar.
The path from the root to a tag gives the binary encodingfor the tag.
For instance, the binary encoding of VBG in Figure 2 is 01011100.
Thedecision tree algorithm can ask which partition a tag belongs to by asking questionsabout its binary encoding.3.3.3 Questions about Word Identities.
For handling word identities, one could followthe approach used for handling the POS tags (e.g., Black et al 1992; Magerman 1994)and view the POS tags and word identities as two separate sources of information.Instead, we view the word identities as a further efinement of the POS tags.
We startthe clustering algorithm with a separate class for each word and each tag that it takeson.
Classes are only merged if the tags are the same.
The result is a word classificationtree for each tag.
This approach means that the trees will not be polluted by wordsthat are ambiguous as to their tag, as exemplified by the word loads, which is usedin the corpus as a third-person present ense verb VBZ and as a plural noun NNS.Furthermore, this approach simplifies the clustering task because the hand annotationsof the POS tags resolve a lot of the difficulty that the algorithm would otherwise haveto learn.
Hence, effective trees can be built even when only a small amount of data isavailable.Figure 3 shows the classification tree for the personal pronouns (PRP).
For ref-erence, we also list the number of occurrences of each word for the POS tag.
In thefigure, we see that the algorithm distinguished between the subjective pronouns L we,and they, and the objective pronouns me, us, and them.
The pronouns you and it cantake both cases and were probably clustered according to their most common usage inthe corpus.
The class low is used to group singleton words, which do not have enoughtraining data to allow effective clustering.
In using the word identities with the deci-sion tree algorithm, we restrict he algorithm from asking word questions when thePOS tag for the word is not uniquely determined by previous questions.3.3.4 Example Decision Tree.
Figure 4 illustrates the top part of the tree that wasgrown for estimating the probability distribution of the POS tag of the current word.The question on the root node "is D)_ 1 = 0 V D2_1 = 1" is asking whether the POStag of the previous word has a 0 as the first bit or a 1 as the second bit of its binaryencoding.
If the answer is no then the bottom branch is followed, which correspondsto the following partition.Di-1 c (CC, PREP, JJ, JJS, JJR, CD, DT, PRP$, WDT}Following the bottom branch of the decision tree, we see that the next question is "isD31 ----1", which gives a true partition of Di-1 E {JJ, JJS, JJR, CD, DT, PRP$,WDT}.Following the top branch, we see that the next question is "is D4_l = 1", whose true541Computational Linguistics Volume 25, Number 4,s ~h- -~-v - - -~~isD 1 - -0A  D 2 --1 A D 3 --1 ~-1 - " " - '~2~-  " "/ ~ ..._..-.--~ J~f/ k i s  D 1 ------0 A D2 ~ A D3 =1 - "  ~-1 ZZ_- ~ i -1 -  ~-I..L~L.--w-~ ~ ?
.
*/ N i s D ~  ~-----OAD?
=I\ -i - l N .
.Figure 4The top part of the decision tree used for estimating the POS probability distribution.partition is Di-1 E {DT, PRP$, WDT}.
The next question along the top branch is "isD5 -1"  which gives a true partition of Di-1 = WDT.
As indicated in the figure, this i _1 - -~is a leaf node, and so no suitable question was found to ask of this context.3.4 ResultsTo test our POS-based language model, we ran two experiments.
The first set examinesthe effect of using richer contexts for estimating the word and POS probability distri-butions.
The second set measures whether modeling discourse usage leads to betterlanguage modeling.
Before we give the results, we explain the methodology that weuse throughout the experiments.3.4.1 Experimental Setup.
In order to make the best use of our limited data, we testedour model using a sixfold cross-validation procedure.
We divided the dialogues intosix partitions and tested each partition with a model built from the other partitions.We divided the dialogues for each pair of speakers as evenly between the six parti-tions as possible.
Changes in speaker are marked in the word transcription with thespecial token <turn>.
The end-of-turn marker is not included in the POS results, butis included in the perplexity results.
We treat contractions, such as that'll and gonna, asseparate words, treating them as that and 'll for the first example, and going and ta forthe second.
We also changed all word fragments into a common token <fragment>.Since current speech recognition rates for spontaneous speech are quite low, wehave run the experiments on the hand-collected transcripts.
In searching for the bestsequence of POS tags for the transcribed words, we follow the technique proposedby Chow and Schwartz (1989) and only keep a small number of alternative paths bypruning the low probability paths after processing each word.3.4.2 Perplexity.
A way to measure the effectiveness of the language model is tomeasure the perplexity that it assigns to a test corpus (Bahl et al 1977).
Perplexityis an estimate of how well the language model is able to predict the next word ofa test corpus in terms of the number of alternatives that need to be considered ateach point.
For word-based language models, with estimated probability distributionof Pr(wilwl,i_l) , the perplexity of a test set Wl,N is calculated as 2 H, where H is thei ~ - - - -1  log2  Pr (w i lw l , i -1 )  ?
entropy, which is defined as H = -~542Heeman and Allen Modeling Speakers' UtterancesTable 6Using richer histories to estimate probabilities.Pr(Wi\[Di) Pr(Wi\[Di-2,iWi-2,i-1)Pr(Di\[Di-2,i-1) Pr(Di\[Di-2,i-lWi-2,i-1)POS Errors 1,778 1,711POS Error Rate 3.04 2.93DM Errors 690 630DM Error Rate 8.33 7.61DM Recall 95.86 96.75DM Precision 95.79 95.68Word Perplexity 43.22 24.04Branching Perplexity 47.25 26.35Branching Perplexity.
Our POS-based model is not only predicting the next word, butits POS tag as well.
To estimate the branching factor, and thus the size of the searchspace, we use the following formula for the entropy, where di is the POS tag forword wi.1 NH = - ~ ~ log 2 ISr(wi\[wl,i_ldl,i)I~r(di\[Wl,i_ldl,i_l) (10)i=1Word Perplexity.
In order to compare a POS-based model against a word-based lan-guage model, we should not penalize the POS-based model for incorrect POS tags.Hence, we should ignore them when defining the perplexity and base the perplexitymeasure on Pr(wi\[wl,i_l).
However, for our model, this probability is not estimated.Hence, we must rewrite it in terms of the probabilities that we do estimate.
To do this,our only recourse is to sum over all possible POS sequences.1 N Y'~I)li I3r(wiDi\]Wl,i-lDl,i-1)ISr(wl,i-lDl,i-1)H= -~ ~log  2 ' ^ (11)i=1 EDI,i-1 Pr(Wl,i-lD1, i-1)3.4.3 Recall and Precision.
We report results on identifying discourse markers interms of recall, precision and error rate.
The recall rate is the number of times that thealgorithm correctly identifies an event over the total number of times that it actuallyoccurred.
The precision rate is the number of times the algorithm correctly identifiesit over the total number of times it identifies it.
The error rate is the number of errorsin identifying an event over the number of times that the event occurred.3.4.4 Using Richer Histories.
Table 6 shows the effect of varying the richness of theinformation that the decision tree algorithm is allowed to use in estimating the POS andword probabilities.
The second column uses the approximations given in Equation 8and 9 and the third column uses the full context.
The results show that adding theextra context has the biggest effect on the perplexity measures, decreasing the wordperplexity by 44.4% from 43.22 to 24.04.
The effect on POS tagging is less pronounced,but still gives a reduction of 3.8%.
We also see a 8.7% improvement in identifyingdiscourse markers.
Hence, in order to use POS tags in a speech recognition languagemodel, we need to use a richer context for estimating the probabilities than what istypically used.
In other work (Heeman 1999), we show that our POS-based modelresults in lower perplexity and word error rate than a word-based model.543Computational Linguistics Volume 25, Number 4Table 7Effect of modeling discourse markers withspecial POS tags.WP WDPOS Errors 1,219 1,189POS Error Rate 2.09 2.04Word Perplexity 24.20 24.04Branching Perplexity 26.08 26.353.4.5 Modeling Discourse Markers.
Table 7 shows the effect of modeling discoursemarkers by using special POS tags.
In column two, we give the results of a model inwhich we use a POS tagset hat does not distinguish discourse marker usage (P).
Thediscourse conjuncts CC_D are collapsed into CC, discourse adverbials RB_D into RB,and acknowledgments AC and discourse interjections UH_D into UH_FP.
The thirdcolumn gives the results of the model in which we use our tagset hat does distinguishdiscourse marker usage (D).
To ensure a fair comparison, we do not penalize POSerrors that result from a confusion between discourse and sentential usages.
We seethat modeling discourse markers results in a perplexity reduction from 24.20 to 24.04and reduces the number of POS errors from 1,219 to 1,189, giving a 2.5% error ratereduction.
Although the improvements in perplexity and POS tagging are small, theyindicate that there are interactions, and hence discourse markers hould be resolvedat the same time as POS tagging and speech recognition word prediction.4.
Identifying Speech Repairs and Intonational PhrasesIn the previous section, we presented a POS-based language model that uses spe-cial tags to denote discourse markers.
However, this model does not account for theoccurrence of speech repairs and intonational phrases.
Ignoring these events whenbuilding a statistical language model will lead to probabilistic estimates for the wordsand POS tags that are less precise, since they mix contexts that cross intonationalboundaries and interruption points of speech repairs with fluent stretches of speech.However, there is not a reliable signal for detecting the interruption point of speechrepairs (Bear, Dowding, and Shriberg 1992) nor the occurrence of intonational phrases.Rather, there are a number of different sources of information that give evidence asto the occurrence of these events.
These sources include the presence of pauses, filledpauses, cue phrases, discourse markers, word fragments, word correspondences, andsyntactic anomalies.
Table 8 gives the number of occurrences for some of these fea-tures for each word in the corpus that is not turn-final nor part of the editing term ofa speech repair.
Each word is classified by whether it immediately precedes the inter-ruption point of a fresh start, modification, or abridged repair, or ends an intonationalphrase.
All other words are categorized as fluent.
The first row gives the number ofoccurrences of these events.
The second row reports whether the word is a fragment.The third and fourth give the number of times the word is followed by a filled pauseor discourse marker, respectively.
The fifth and sixth rows report whether the word isfollowed by a pause that is less than or greater than 0.5 seconds, respectively.
Pausedurations were computed automatically with a speech recognizer constrained to theword transcription (Entropic Research Laboratory, Inc. 1994).
The next row reportswhether there is a word match that crosses the word with at most two interveningwords, and the next row, those with at most five intervening words.544Heeman and Allen Modeling Speakers' UtterancesTable 8Occurrence of featuresboundaries.that signal speech repairs and intonationalFluent Abridged Modification Fresh IntonationalFeature Speech Repairs Repairs Starts BoundariesAll 43,439 423 1,301 671 5,211Fragments 7 0 481 150 0Filled Pauses 97 374 114 71 358Short Pauses 4,415 146 711 313 1,710Long Pauses 1,537 121 176 186 1,622Matching (2) 2,629 27 869 197 373Matching (5) 11,479 94 1,517 575 1,375From the table, it is clear that none of the cues on their own is a reliable indicator ofspeech repairs or intonational boundaries.
For instance, 44.5% (1,622/3,642) of all longpauses occur after an intonational boundary and 13.3% occur after the interruptionpoint of a speech repair.
Conversely, 31.1% (1,622/5,211) of intonational boundariesare followed by a pause while 20.2% of all repairs are followed by a long pause.Hence, pauses alone do not give a complete picture of whether a speech repair orintonational boundary occurred.
The same holds for filled pauses, which can occurboth after the interruption point of a speech repair and in fluent speech, namelybetween utterances or after utterance-initial discourse markers.
Word matchings canalso be spurious, as evidenced by the 27 word matches with at most two interveningwords across abridged repairs, as well as the matchings across intonational boundariesand fluent speech.
Even syntactic ill-formedness atthe interruption point is not alwaysguaranteed, asthe following example illustrates.Example 15 (d93-13.2 utt53)load two boxes of boxcars with orangesreparandum zpHence using parser failures to find repairs (cf.
Dowding et al 1993) will not be robust.In this section, we augment our POS-based language model so that it also detectsintonational boundaries and speech repairs, along with their editing terms.
Althoughnot all speech repairs have obvious syntactic anomalies, the probability distributionsfor words and POS tags are going to be different depending on whether they followthe interruption point of a speech repair, an intonational boundary, or fluent speech.So, it makes ense to take the speech repairs and intonational boundaries into accountby directly modeling them when building the language model, which automaticallygives us a means of detecting these events and better prediction of the speech thatfollows.
To model the occurrence of intonational boundaries and speech repairs, weintroduce three extra variables into the language model.
The repair tag Ri, the editingterm tag Ei and the intonation tag Ii.
These utterance tags capture the discontinuitiesin the speaker's turn, and we use these discontinuities tobetter model the speech thatfollows.4.1 Speech RepairsThe repair tag indicates the occurrence ofspeech repairs.
However, we not only want toknow whether arepair occurred, but also the type of repair: whether it is a modification545Computational Linguistics Volume 25, Number 4repair, a fresh start, or an abridged repair.
The type of repair is important since thestrategy that a hearer uses to correct the repair depends on the type of repair.
Forfresh starts, the hearer must determine the beginning of the current utterance.
Formodification repairs, the hearer can make use of the correspondences between thereparandum and alteration to determine the reparandum onset.
For abridged repairs,there is no reparandum, and so simply knowing that it is abridged gives the correction.For repairs that do not have an editing term, the interruption point is wherethe local context is disrupted, and hence is the logical place to tag such repairs.
Forrepairs with an editing term, there are two choices for marking the speech repair:either directly following the end of the reparandum, or directly preceding the onsetof the alteration.
The following example illustrates these two choices, marking themwith Mod?.Example 16 (d92a-5.2 utt34)so we'll pick up a tank of Mod?
uh Mod?
the tanker of orangesThe editing term by itself does not completely determine the type of repair.
The al-teration also helps to disambiguate the repair.
Hence, we delay hypothesizing aboutthe repair type until the end of the editing term, which should keep our search-spacesmaller, since we do not need to keep alternative repair type interpretations while pro-cessing the editing term.
This leads to the following definition of the repair variableRi for the transition between word Wi-1 and Wi:ModRi = CanAbrnullif Wi is the alteration onset of a modification repairif Wi is the alteration onset of a fresh start (or cancel)if Wi is the alteration onset of an abridged repairotherwise4.2 Editing TermsEditing terms are problematic for tagging speech repairs since they separate the endof the reparandum from the alteration onset, thus separating the discontinuity thatgives evidence that a fresh start or modification repair occurred.
For abridged repairs,they separate the word that follows the editing term from the context hat is needed todetermine the identity of the word and its POS tag.
If editing terms could be identifiedwithout having to consider the context, we could skip over them, but still use them aspart of the context for deciding the repair tag (cf.
Heeman and Allen 1994).
However,this assumption is not valid for words that are ambiguous as to whether they are anediting term, such as let me see.
Even filled pauses are problematic since they are notnecessarily part of the editing term of a repair.
To model editing terms, we use thevar iab le  E i to indicate the type of editing term transition between word W/_ 1 and Wi.PushETEi : Popnullif Wi-1 is not part of an editing term but Wi isif Wi-1 and Wi are both part of an editing termif Wi-1 is part of an editing term but Wi is notif neither Wi-1 nor Wi are part of an editing termBelow, we give an example and show all non-null editing term and repair tags.Example 17 (d93-10.4 utt30)that'll get there at four a.m. Push oh ET sorry Pop Mod at eleven a.m.546Heeman and Allen Modeling Speakers' Utterances4.3 Intonational PhrasesThe final variable is Ii, which marks the occurrence of intonational phrase boundaries.% if Wi-1 ends an intonational phraseli = null otherwiseThe intonation variable is separate from the editing term and repair variables ince itis not restricted by the value of the other two.
For instance, an editing term could endan intonational phrase, especially on the end of a cue phrase such as let's see, as canthe reparandum, asExample 18 below demonstrates.Example 18 (d92a-2.1 utt29)that's the one with the bananas % Push I ET mean Pop Mod that's taking the bananas4.4 Redefining the Speech Recognition ProblemWe now redefine the speech recognition problem so that its goal is to find the sequenceof words and the corresponding POS, intonation, editing term, and repair tags that ismost probable given the acoustic signal.= argmaxPr(WDREI IA )WDREI= arg max Pr(A IWDREI) Pr(WDREI)WDREI(12)The second term is the language model probability, and can be rewritten as follows.Pr( W1,ND1,NR1,NE1,NI1,N )N= ~-IPr(WiDiRiEililWl,i-lDl,i-lRl,i-lEl,i-lI1, i-1)i=1N~- I I  Pr(li lWl,i-lDl,i-lal,i-lEl,i-lI l, i-1)i=1Pr(EilWl,i-lDl,i-lRl,i-lEl,i-ll l,i)Pr (Ril Wl,i-lDl,i-lRl,i-lEl,ill,i)Pr( Dil Wl,i- l Dl,i- l al,iEl,ill,i )Pr(WilWl,i_lDl,iRl,iEl,iIl,i) (13)4.5 Representing the ContextEquation 13 requires five probability distributions to be estimated.
The context foreach includes all of the words, POS, intonation, repair, and editing term tags thathave been hypothesized, each as a separate piece of information.
In principal, wecould give this to the decision tree algorithm and let it decide what information touse in constructing equivalence classes.
However, repairs, editing terms, and evenintonation phrases do not occur in the same abundance as fluent speech and arenot as constrained.
Hence, it will be difficult to model the discontinuities that theyintroduce into the context.547Computational Linguistics Volume 25, Number 4Consider the following example of a speech repair without an editing term.Example 19 (d92-1 utt53)engine E two picks Mod takes the two boxcarsWhen predicting the first word of the alteration takes, it is inappropriate o ask aboutthe preceding words, such as picks, without realizing that there is a modification repairin between.
The same also holds for intonational boundaries and editing term pushesand pops.
In the example below, a question should only be asked about is in therealization that it ends an intonational phrase.Example 20 (d92a-1.2 utt3)you'll have to tell me what the problem is % I don't have their labelsAlthough the intonation, repair, and editing term tags are part of the context and socan be used in partitioning it, the question is whether this will happen.
The prob-lem is that null intonation, repair, and editing term tags dominate the training ex-amples.
So, we are bound to run into contexts in which there are not enough into-national phrases and repairs for the algorithm to learn the importance of using thisinformation, and instead might blindly subdivide the context based on some subdi-vision of the POS tags.
The solution is analogous to what is done in POS taggingof written text: we give a view of the words and POS tags with the non-null re-pair, non-null intonation, and editing term push and pop tags inserted.
By insertingthese tags into the word and POS sequence, it will be more difficult for the learn-ing algorithm to ignore them.
It also allows these tags to be grouped with other tagsthat behave in a similar way, such as change in speaker turn, and discourse mark-ers.Now consider the following examples, which both start with so we need to.Example 21 (d92a-2.2 utt6)so we need to Push urn Pop Abr get a tanker of OJ to AvonExample 22 (d93-11.1 utt46)so we need to get the three tankersThis is then followed by the verb get, except he first has an editing term in between.However, in predicting this word, the editing term hinders the decision tree algorithmfrom generalizing with nonabridged examples.
The same thing happens with freshstarts and modification repairs.
To allow generalizations between repairs with an edit-ing term and those without, we need a view of the context with completed editingterms removed (cf.
Stolcke and Shriberg 1996b).Part of the context given to the decision tree is the words and POS tags with thenon-null utterance tags inserted (i.e., %) and completed editing terms removed.
Werefer to this as the utterance context, since it incorporates the utterance informationthat has been hypothesized.
Consider the following example.548Heeman and Allen Modeling Speakers' Utterances/ l e a f/ i suD~ =1~is  ~Dl _==13 A uD2 ,=0  / ~-~'- ~-~ ~ , .-----~--~4~ .
1 ~,  * n3 1 "" " is uDi_ =1 A uDi_l=l V uDi_l=O1 / .
.
.
.
~ ~-~-------""is uD i_~=O V uD i_l=O A ET-stateE{nul~is uD 4 .
~I/V u W 1 ~---O(NN) s l../ i~  ~o~..._ ,_ I =0is uD i ~V uD i i=1N i s  uDS_~tat 'eE{  fp} V uO~_l=lFigure 5Top part of the decision tree used for estimating the probability distribution of the intonationtag.Example 23 (d93-18.1 utt47)it takes one Push you ET know Pop Mod two hours %The utterance context for the POS tag of you is "it/PRP takes/VBP one/CD Push.
"The context for the editing term Pop is "it/PRP takes/VBP one/CD Push you/PRPknow/VBP."
The utterance context for the repair tag has the editing term cleaned up:"it/PRP takes/VBP one/CD" (we also give it the context with the editing term notcleaned up).
The context for the POS tag of two is "it/PRP takes/VBP one/CD Mod.
"We also include two variables that indicate whether we are processing an editingterm without forcing it to look for an editing term Push in the utterance context: ETostate indicates whether we are processing an editing term and whether a cue phrasewas seen; and ET-prev indicates the number of editing term words seen so far.
Figure 5gives the top part of the decision tree that was grown for the intonation tag, whereuW and uD are the utterance context.5.
Correcting Speech RepairsThe previous ection focused on the detection of speech repairs, editing terms, andintonational phrases.
But for repairs, we have only addressed half of the problem;the other half is determining the extent of the reparandum.
Hindle (1983) and Kikuiand Morimoto (1994) both separate the task of correcting a repair from detectingit by assuming that there is an acoustic editing signal that marks the interruptionpoint of speech repairs (as well as access to the POS tags and utterance boundaries).Although the model of the previous ection detects repairs, this model is not effectiveenough.
In fact, we feel that one of its crucial shortcomings is that it does not take intoconsideration the task of correcting repairs (Heeman, Loken-Kim, and Allen 1996).Since hearers are often unaware of speech repairs (Martin and Strange 1968), theymust be able to correct hem as the utterance is unfolding and as an indistinguishableevent from detecting them and recognizing the words involved.Bear, Dowding, and Shriberg (1992) proposed that multiple information sourcesneed to be combined in order to detect and correct speech repairs.
One of these sources549Computational Linguistics Volume 25, Number 4Table 9Occurrences of common repair structures.x.
362 mmr.mmr 10 mm.mxm 4 mxmx.mm 2m.m 249 m.xm 10 xmmm.mmm 3 mmm.mmxm 2r.r 136mxx.m 8 mrx.mr 3 mm.xxmm 2mm.mm 85 mmmx.mmm 8 mrr.mrr 3 mm.mxxm 2mx.m 76 m.xxm 8 mrmx.mrm 3 xr.r 2mmx.mm 35 mrm.mrm 7 mmmmm.mmmmm 3 xmx.m 2mr.mr 29 mx.xm 6 mm.xmm 3 xmmx.mm 2mmm.mmm 22 xm.m 5 mmmmx.mmmm 2 rr.rr 2rx.r 20 mmmmr.mmmmr 5 mrmm.mrmm 2 rm.rxm 2rm.rm 20rmm.rmm 4 mmmxx.mmm 2 r.xr 2xx.
12 mmxx.mm 4 mmm.xxxmmm 2mmmm.mmmm 12 mmn~.mmmr 4 mmm.mxmm 2includes a pattern-matching routine that looks for simple cases of word correspon-dences that could indicate a repair.
However, pattern matching is too limited to cap-ture the variety of word correspondence patterns that speech repairs exhibit (Heemanand Allen 1994).
For example, the 1,302 modification repairs in the Trains corpus takeon 160 different repair structures, even when we exclude word fragments and editingterms.
Of these, only 47 occurred at least twice, and these are listed in Table 9.
Eachword in the repair is represented by its correspondence type: m for word match, r forreplacement, and x for deletions and insertions.
A period "."
marks the interruptionpoint.
For example, the structure of the repair given in Example 14 (engine two fromElmi(ra)- or engine three from Elmira) would be mrm.mrm.To remedy the limitation of Bear, Dowding, and Shriberg (1992), we proposed thatthe word correspondences between the reparandum and alteration could be found bya set of well-formedness rules (Heeman and Allen 1994; Heeman, Loken-Kim, andAllen 1996).
Potential repairs found by the rules were passed to a statistical languagemodel (a predecessor f the model of Section 4), which pruned out false positives.
Partof the context for the statistical model was the proposed repair structure found by thewell-formedness rules.
However, the alteration of a repair, which makes up half of therepair structure, occurs after the interruption point and hence should not be used topredict he occurrence of a repair.
Hence this model was of limited use for integrationinto a speech recognizer.Recently, Stolcke and Shriberg (1996) presented a word-based model for speechrecognition that models simple word deletion and repetition patterns.
They used theprediction of the repair to clean up the context and to help predict what word will occurnext.
Although their model is limited to simple types of repairs, it provides a startingpoint for incorporating speech repair correction into a statistical language model.5.1 Sources of InformationThere are several sources of information that give evidence as to the extent of thereparandum of speech repairs.
Probably the most widely used is the presence of wordcorrespondences between the reparandum and alteration, both at the word level andat the level of syntactic onstituents (Levelt 1983; Hindle 1983; Bear, Dowding, andShriberg 1992; Heeman and Allen 1994; Kikui and Morimoto 1994).
Second, there tendsto be a fluent transition from the speech that precedes the onset of the reparandum tothe alteration (Kikui and Morimoto 1994).
This source is very important for repairs thatdo not have initial retracing, and is the mainstay of the "parser-first" approach (e.g.,550Heeman and Allen Modeling Speakers' UtterancesDowding et al 1993)--keep trying alternative corrections until one of them parses.Third, there are certain regularities for where speakers restart.
Reparandum onsetstend to be at constituent boundaries (Nooteboom 1980), and in particular, at bound-aries where a coordinated constituent can be placed (Levelt 1983).
Hence, reparandumonsets can be partially predicted without even looking at the alteration.5.2 Our ApproachMost previous approaches to correcting speech repairs have taken the standpoint offinding the best reparandum given the neighboring words.
Instead, we view the prob-lem as finding the reparandum that best predicts the following words.
Since speechrepairs are often accompanied by word correspondences (Levelt 1983; Hindle 1983;Bear, Dowding, and Shriberg 1992; Heeman and Allen 1994; Kikui and Morimoto1994), the actual reparandum will better predict the words in the alteration of therepair.
Consider the following example:Example 24 (d93-3.2 utt45)which engine are we are we takingTreparandum lpIn this example, if we predicted that a modification repair occurred and that thereparandum consists of are we, then the probability of are being the first word of thealteration would be very high, since it matches the first word of the reparandum.Conversely, if we are not predicting a modification repair with reparandum are we,then the probability of seeing are would be much lower.
The same reasoning holds forpredicting the next word, we: it is much more likely under the repair interpretation.
So,as we process the words of the alteration, the repair interpretation will better accountfor the words that follow it, strengthening the interpretation.When predicting the first word of the alteration, we can also make use of thesecond source of evidence identified in the previous ection: the context provided bythe words that precede the reparandum.
Consider the following repair in which thefirst two words of the alteration are inserted.Example 25 (d93-16.2 utt66)and two tankers to of OJ to Dansvillev Treparandum zpHere, if we know the reparandum is to, then we know that the first word of the reparan-dum must be a fluent continuation of the speech before the onset of the reparandum.In fact, we see that the repair interpretation (with the correct reparandum onset) pro-vides better context for predicting the first word of the alteration than a hypothesisthat predicts either the wrong reparandum onset or predicts no repair at all.
Hence, bypredicting the reparandum of a speech repair, we no longer need to predict he onsetof the alteration on the basis of the ending of the reparandum, as we did in Section 4.5.Such predictions are based on limited amounts of training data since only examplesof speech repairs can be used.
Rather, by first predicting the reparandum, we can useexamples of fluent transitions to help predict the first word of the alteration.We can also make use of the third source of information.
When we initially hy-pothesize the reparandum onset, we can take into account he a priori probability551Computational Linguistics Volume 25, Number 4that it will occur at that point.
In the following example, the words should and theare preferred by Levelt's coordinated constituent rule (Levelt 1983), and hence shouldhave a higher score.
Exceptions to the rule, such as this one, should have a lowerscore.Example 26two boxcars(d93-10.4 utt30)of orange juice should er of oranges hould be made into orange juice ?
,JYreparandum lpTo incorporate correction processing into our language model, we need to addextra variables.
After we predict a repair, we need to predict he reparandum onset.Knowing the reparandum onset then allows us to predict he word correspondencesbetween the reparandum and alteration, thus allowing us to use the repair to betterpredict he words and their POS tags that make up the alteration.5.3 Reparandum OnsetAfter we predict a modification repair or a fresh start, we need to predict he reparan-dum onset.
Consider the following two examples of modification repairs.Example 27 (d93-16.3 utt9)to fill the engine?
"T Yreparandum lpthe boxcars with bananasExample 28 (d93-25.6 utt31)drop off the one tankerreparandum l~pthe two tankersAlthough the examples differ in the length of the reparandum, their reparanda bothstart at the onset of a noun phrase.
This same phenomena also exists for fresh startswhere reparandum onsets are likely to follow an intonational boundary, the beginningof the turn, or a discourse marker.
In order to allow generalizations across differentreparandum lengths, we query each potential onset o see how likely it is as the onset.For Ri E {Mod, Can} and j < i, we define Oq as follows:Onset Wj is the reparandum onset of repair R iOq = null otherwiseWe normalize the probabilities to ensure that  ~-~j(Oij = Onset) = 1.Just as we exclude the editing terms of previous repairs from the utterance wordsand POS tags, so we exclude the reparanda of previous repairs.
Consider the followingexample of overlapping repairs, repairs in which the reparanda and alterations cannotbe separated.552Heeman and Allen Modeling Speakers' UtterancesExample 29 (d93-16.3 utt4)what's the shortest route from engine fromT l ~p zpfor engine two at ElmiraThe reparandum of the first repair is from engine.
In predicting the reparandum of thesecond, we work from the cleaned up context: what's the shortest route from.The context used in estimating how likely a word is as the reparandum onsetincludes which word we are querying.
We also include the utterance words and POStags that precede the proposed reparandum onset, thus allowing the decision tree tocheck if the onset is at a suitable constituent boundary.
Since reparanda rarely extendover more than one utterance, we include three variables that help indicate whetheran utterance boundary is being crossed.
The first indicates the number of intonationalphrase boundaries embedded in the proposed reparandum.
The second indicates thenumber of discourse markers in the reparandum.
Discourse markers at the beginningof the reparandum are not included, and if discourse markers appear consecutively,the group is only counted once.
The third indicates the number of filled pauses in thereparandum.Another source of information is the presence of other repairs in the turn.
In theTrains corpus, 35.6% of nonabridged repairs overlap.
If a repair overlaps a previousone then its reparandum onset is likely to co-occur with the alteration onset of theprevious repair (Heeman 1997).
Hence we include a variable that indicates whetherthere is a previous repair, and if there is, whether the proposed onset coincides with,precedes, or follows the alteration onset of the preceding repair.5.4 The Active RepairDetermining word correspondences is complicated by the occurrence of overlappingrepairs.
To keep our approach simple, we allow at most one previous word to licensethe correspondence.
Consider again Example 29.
Here, one could argue that the wordfor corresponds to the word from from either the reparandum of the first or secondrepair.
In either case, the correspondence to the word engine is from the reparandumof the first repair.
Our approach is to first decide which repair the correspondence willbe to and then decide which word of that repair's reparandum will license the currentword.
We always choose the most recent repair that has words in its reparandumthat have not yet licensed a correspondence (other than a word fragment).
Hence, theactive repair for predicting the word for is the second repair, while the active repairfor predicting engine is the first repair.
For predicting the word two, neither the firstnor second repair has any unlicensed words in its reparandum, and hence two willnot have an active repair.
In future work, we plan to choose between the reparan-dum of alternative speech repairs, as allowed by the annotation scheme (Heeman1997).5.5 Licensing a CorrespondenceIf we are in the midst of processing a repair, we can use the reparandum to helppredict the current word Wi and its POS tag Di.
In order to do this, we need todetermine which word in the reparandum of the active repair will license the currentword.
As illustrated in Figure 6, word correspondences for speech repairs tend toexhibit a cross serial dependency (Heeman and Allen 1994); in other words, if wehave a correspondence b tween Wj in the reparandum and Wk in the alteration, anycorrespondence with a word in the alteration after Wk will be to a word that is after wj.553Computational Linguistics Volume 25, Number 4we'llpick up a tank of uh the tanker of oranges' I t t tFigure 6Cross serial correspondences between reparandum and alteration.This regularity does have exceptions, as the following example illustrates; however,we currently do not support such correspondences.Example 30 (d93-19.4 utt37)can we havereparandum z~pwe can have three engines in Corning at the same timeSince we currently do not support such exceptions, this means that if there is alreadya correspondence forthe repair, then the licensing word will follow the last correspon-dence in the reparandum.The licensing word might need to skip over words due to deleted words in thereparandum or inserted words in the alteration.
In the example below, the word towis licensed by carry, but the word them must be skipped over before processing thelicensing between the two instances of both.Example 31 (d92a-1.2 utt40)you can carry them both on "1Yreparandum lptow both on the same engineThe next example illustrates the opposite problem: the word two has no correspon-dence with any word in the reparandum.Example 32 (d93-15.4 utt45)and fill my boxcars fully of oranges \]"reparandum lpmy two boxcars full of orangesFor words that have no correspondence, we define the licensing word as the firstavailable word in the alternation, in this case boxcars.
We leave it to the correspon-dence variable to encode that there is no correspondence.
This gives us the followingdefinition for the correspondence licensor, Lq, where i is the current word and j runsover all words in the reparandum of the active repair that come after the last word inthe reparandum with a correspondence.CorrLq = CorrnullWj licenses the current wordW i is an inserted word and Wj is first available word in reparandumotherwiseJust as with the reparandum onset, we estimate the probability by querying eacheligible word.
The context for this query includes information about the proposedword, namely its POS tag, as well as the utterance POS and word context prior tothe current word, the type of repair and the reparandum length.
We also include554Heeman and Allen Modeling Speakers' Utterancesinformation about he repair structure that has been found so far.
If the previous wordwas a word match, there is a good chance that the current word will involve a wordmatch to the next word.
The rest of the features are the number of words skipped inthe reparandum and alteration since the last correspondence, the number of wordssince the onset of the reparandum and alteration, and the number of words to the endof the reparandum.5.6 The Word CorrespondenceNow that we have decided which word in the reparandum will potentially license thecurrent word, we need to predict he type of correspondence.
Wefocus on correspon-dences involving exact word match (identical POS tag and word), word replacements(same POS tag), or no such correspondence.mCi = rXnullWi is a word match of the word indicated by LiWi is a word replacement of the word indicated by LiWi has no correspondence (inserted word)No active repairThe context used for estimating the correspondence variable is exactly the same asthat used for estimating the licensor.5.7 Redefining the Speech Recognition ProblemNow that we have introduced the correction tags, we redefine the speech recognitionproblem so that it includes finding the most probable corrections tags.WDCLOREI arg max Pr( WDCLOREIIA )WDCLOREIarg max Pr(A\] WDCLOREI) Pr(WDCLOREI) (14)WDCLOREIThe second term is the language model and can be rewritten as we did for Equation 12.We have already discussed the context used for estimating the three new proba-bility distributions.
We also have a richer context for estimating the other five distri-butions.
For these, we take advantage of the new definition of the utterance word andPOS tags, which now accounts for the reparanda of repairs.
Consider the followingexample.Example 33 (d93-13.1 utt64)pick up and load two um the two boxcars on engine tworeparandum lpIn processing the word the, if we hypothesized that it follows a modification repairwith editing term um and reparandum two, then we can now generalize with fluentexamples, uch as the following, in hypothesizing its POS tag and the word identity.Example 34 (d93-12.4 utt97)and to make the orange juice and load the tankersThus, we can make use of the second knowledge source of Section 5.1.555Computational Linguistics Volume 25, Number 4Cleaning up fresh starts requires a slightly different treatment.
Fresh starts aban-don the current utterance, and hence the alteration starts a new utterance.
But thisnew utterance will start differently than most utterances in that it will not begin withinitial filled pauses, or phrases uch as let's see, since these would have been countedas part of the editing term of the fresh start.
Hence, when we clean up the reparandaof fresh starts, we leave the fresh start marker Can, just as we do for intonationalboundaries.For predicting the word and POS tags, we have an additional source of informa-tion, namely the values of the correspondence licensor and the correspondence type.Rather than use these two variables as part of the context hat we give the decisiontree algorithm, we use these tags to override the decision tree probability.
If a wordreplacement or word match was hypothesized, we assign all of the POS probabilityto the appropriate POS tag.
If a word match was hypothesized, we assign all of theword probability to the appropriate word.6.
Acoustic CuesSilence, as well as other acoustic information, can also give evidence as to whether anintonational phrase, speech repair, or editing term occurred, as was shown in Table 8.In this section, we revise the language model to incorporate this information.6.1 Redefining the Speech Recognition ProblemIn the same way that speech recognizers hypothesize l xical items, they also hypoth-esize pauses.
Rather than insert these into the word sequence (e.g., Zeppenfeld et al1997), we define the variable Si to be the amount of silence between words Wi_ 1 andWi.
We incorporate this information by redefining the speech recognition problem.^ ^ .
.
.
.
.
.
.WPCLOREIS = argmax Pr(AIWDCLOREIS ) Pr(WDCLOREIS) (15)WDCLOREISAgain, the first term is the acoustic model, which one can approximate by Pr(AIWS ),and thus reduce it to a traditional coustic model.
The second term is the new languagemodel, which we rewrite as follows:Pr( W1,ND1,NC1,NL1,NO1,NR1,NE1,NI1,NS1,N )N= I-\[ Pr(WiDiCiLiOiRiEiliSilWl,i-lDl,i-lCl,i-lLl,i-lOl,i-lRl,i-lEl,i-lI l, i-lSl,i-1)i=1We expand the silence variable first so that we can use it as part of the context inestimating the tags for the remaining variables.We now have an extra probability in our model, namely the probability of Sigiven the previous context.
The variable Si will take on values in accordance withthe minimum time samples that the speech recognizer uses.
To deal with limitedamounts of training data, one could collapse these durations into larger intervals.
Notethat including this probability impacts the perplexity computation.
Usually, predictionof silence durations is not included in the perplexity calculation.
In order to allowcomparisons between the perplexity rates of the model that includes ilence durationsand ones that do not, we exclude the probability of Si in the perplexity calculation.6.2 Using Silence as Part of the ContextWe now need to include the silence durations as part of the context for predicting thevalues of the other variables.
However, it is just for the intonation, repair, and editing556Heeman and Allen Modeling Speakers' Utterances9F luent  - -7 Tone  .
.
.
.
.Mod i f i ca t ion  .
.
.
.
.F resh  S tar t  .........6 Push  .
.
.
.
.. .
.
.
.
.
.
.
.
.
Pop  .
.
.
.. .
.
.
.
.
?
.
.
- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~: : .
..........: ,  .
/? '
; -~"- ' := = - ~::CE .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
./i'f .
.
.
.
.
.
.
.
.
.
.
.
.
.
.............................I.
i i i0 5 1 1.5 2 2 ,5Figure 7Preference for utterance tags given the length of silence.term variables that this information is most appropriate.
We could let the decision treealgorithm use the silence duration as part of the context in estimating the probabilitydistributions.
However, our attempts at doing this have not met with success, perhapsbecause asking questions about the silences fragments the training data and hencemakes it difficult to model the influence of the other aspects of the context.
Instead,we treat the silence information as being independent from the other context.
Belowwe give the derivation for the intonation variable.
For expository ease, we defineContexti to be the prior context for deciding the probabilities for word Wi.Contexti = Wl , i _ lD l , i _ lC l , i _ l L l , i _ lO l , i _ lR l , i _ lE l , i _ l l l , i _ lS l , i _ lThe derivation is as follows.Pr (Iil SiContexti ) Pr(ContextiSiili) Pr(Ii) Pr( SiContexti)Pr(Contextilli) Pr(Silli) Pr(Ii)Pr(Si) Pr( Contexti).
Pr(IiiSi) Pr(IiiC?ntexti) (16)The second line involved the assumptions that Contexti and Si are independent andthat Contexti and Si are independent given Ii.
The first assumption is obviously toostrong.
If the previous word is a noun it is more likely that there will be a silence afterit than if the previous word was an article.
However, the assumptions allow us tomodel the silence information independently from the other context, which gives usmore data to estimate its effect.
The result is that we use the factor Pr(IilSi) to adjust heprobabilities computed by the decision tree algorithm, which does not use the silencedurations.
We guard against shortcomings by normalizing the adjusted probabilitiesto ensure that they sum to one.To compute Pr(IiiSi), we group the silence durations into 30 intervals and thensmooth the counts using a Gaussian filter.
We do the same adjustment for the editingterm and repair variables.
For the editing term variable, we only do the adjustmentif the intonation tag is null, due to a lack of data in which editing terms co-occurwith intonational phrasing.
For the repair variable, we only do the adjustment if theintonation tag is null and the editing term tag is not a push or pop.
Figure 7 givesthe adjustments for the resulting six equivalence classes of utterance tags.
The ratio557Computational Linguistics Volume 25, Number 4between the curves gives the preference for one class over another, for a given silenceduration.
Silence durations were automatically obtained from a word aligner (EntropicResearch Laboratory, Inc. 1994).Silences between speaker turns are not used in computing the preference factor,nor is the preference factor used at such points.
The end of the speaker's turn isdetermined jointly by both the speaker and the hearer.
So when building a system thatis designed to participate in a conversation, these silence durations will be partiallydetermined by the system's turn-taking strategy.
We also do not include the silencedurations after word fragments ince these silences were hand-computed.7.
ExampleThis section illustrates the workings of the algorithm.
As in Section 3.4.1, the algorithmis constrained to the word transcriptions and incrementally considers all possible in-terpretations (those that do not get pruned), proceeding one word at a time.
Sinceresolving speech repairs is the most complicated part of our model, we focus on thisusing the following example of overlapping repairs.Example 35 (d92a-2.1 utt95)okay % uh and that will take a total of um let's see total of s- of seven hoursT " " ~ lp~:mod reparandum ~p:mod ~ reparandumRather than try to show all of the competing hypotheses, we focus on the correctinterpretation, which, for this example, happens to be the winning interpretation.
Wecontrast the probabilities of the correct ags with those of its competitors.
For reference,we give a simplified view of the context hat is used for each probability.
Full resultsof the algorithm will be given in the next section.7.1 Predicting "um" as the Onset of an Editing TermBelow, we give the probabilities involved in the correct interpretation of the word umgiven the correct interpretation of the words okay uh and that will take a total of.
We startwith the intonation variable.
The correct ag of null is significantly preferred over thealternative, mainly because intonational boundaries rarely follow prepositions.Pr(Iw=null \[ a total of) = 0.9997Pr(h0=% I a total of) = 0.0003For I10 = null, we give the alternatives for the editing term tag.
Since an editing termis not in progress, the only possible values are Push and null.Pr(E10=Push \]a total of) = 0.242Pr(E10=null \[ a total of) = 0.758With El0 = Push, the only allowable repair tag is null.
Since no repair has been started,the reparandum onset O10 must be null.
Similarly, since no repair is in progress,L10, the correspondence licensor, and C10, the correspondence type, must both benull.We next hypothesize the POS tag.
Below we list all of the tags that have a prob-ability greater than 1%.
Since we are starting an editing term, we see that POS tags558Heeman and Allen Modeling Speakers' Utterancesassociated with the first word of an editing term have a high probability, such asUFLFP for um, AC for okay, CC_D for or, UH_D for well, and VB for the let in let's see.For D10Pr(D10=UH_FP \[ a total of Push) = 0.731Pr(D10--AC \[ a total of Push) = 0.177Pr(D10=CC_D I a total of Push) = 0.026Pr(D10=UH_D \[ a total of Push) = 0.020Pr(D10--VB I a total of Push) = 0.026set to UH_FP, the word choices are um, uh, and er.Pr(W10=um \[ a total of Push UH_FP) = 0.508Pr(W10=uh \[ a total of Push UH_FP) -- 0.488Pr(Wl0=er I a total of Push UH_FP) = 0.004Given the correct interpretation of the previous words, the probability of the filledpause um along with the correct ags is 0.090.7.2 Predicting "total" as the Alteration OnsetWe now give the probabilities involved in the second instance of total, which is thealteration onset of the first repair, whose editing term um let's see, which ends anintonational phrase, has just finished.
Again we start with the intonation variable.Pr(I14=% \[ a total of Push um let's see) = 0.902Pr(I14=null I a total of Push um let's see) = 0.098For  I14 -~ %, the editing term probabilities are given below.
Since an editing term is inprogress, the only possibilities are that it is continued or that it has ended.Pr(E14=Pop \[ a total of Push um let's see %) = 0.830Pr(E14=ET I a total of Push um let's see %) -- 0.170For E14 ~ Pop, we give the probabilities for the repair variable.
Since an editing termhas just ended, the null tag for the repair variable is ruled out.
Note the modificationinterpretation receives a score approximately one third of that of a fresh start.
However,the repair interpretation catches up after the alteration is processed.Pr(R14=Mod I a total of Push um let's see % Pop) = 0.228Pr(R14=Can I a total of Push um let's see % Pop) = 0.644Pr(R14=Abr I a total of Push um let's see % Pop) -- 0.128For R14 = Mod, we give the probabilities assigned to the possible reparandum onsets.559Computational Linguistics Volume 25, Number 4For each, we give the proposed reparandum onset, X, and the words that precede it.Pr(O14,x=OnsetPr(O14,x=OnsetPr(O14,x=OnsetPr(O14,x=OnsetPr(O14,x=OnsetPr(O14,x=OnsetPr(O14,x=OnsetPr(O14,x=OnsetPr(O14,x=OnsetW=take a total X=of R=Mod) = 0.589W=will take a X=total R=Mod) = 0.126W=that will take X--a R=Mod) = 0.145W=and that will X=take R=Mod) = 0.023W=uh and that X=will R=Mod) = 0.016W=% uh and X=that R=Mod) = 0.047IW=okay % uh X=and R=Mod) = 0.047\]W=<turn> okay % X=uh R=Mod) = 0.003\]W=<turn> X=okay R--Mod) = 0.003With total as the reparandum onset, there are two possibilities for which word of thereparandum will license the current word---either the word total or of.Pr(L10,x=Corr \]W=will take a X=total R=Mod) = 0.973Pr(L10,x=Corr \]W=will take a X=of R=Mod) = 0.027With total as the correspondence licensor, we need to decide the type of correspon-dence: whether it is a word match, word replacement, or otherwise.Pr(C14=m \]W=will take a L=total R=Mod) = 0.5882Pr(C14=r \[W=will take a L=total R=Mod) = 0.1790Pr(C14---x \]W=will take a L=total R=Mod) = 0.2328For the correct interpretation, the word correspondence is a word match with theword total and POS tag NN.
Hence, the POS tag and identity of the current wordare both fixed and hence have a probability of 1.
Given the correct interpretation fthe previous words, the probability of the word total along with the correct ags is0.0111.8.
ResultsIn this section, we present the results of running our model on the Trains corpus.
Thissection not only shows the feasibility of the model, but also supports the thesis thatthe tasks of resolving speech repairs, identifying intonational phrases and discoursemarkers, POS tagging, and speech recognition language modeling must be accom-plished in a single model to account for the interactions between these tasks.
We startwith the models that we presented in Section 3, and vary which variables of Section 4,5, and 6 that we include.
All results in this section were obtained using the sixfoldcross-validation procedure described in Section 3.4.1.8.1 POS Tagging, Perplexity, and Discourse MarkersTable 10 shows that POS tagging, word perplexity, and discourse markers benefitfrom modeling intonational phrases and speech repairs.
The second column givesthe results of the POS-based language model of Section 3.
The third column addsintonational phrase detection, which reduces the POS error rate by 3.8%, improves560Heeman and Allen Modeling Speakers' UtterancesTable 10Comparison of POS tagging, discourse marker identification, and perplexity rates.WD WDI WDCLORE WDCLOREI WDCLOREISPOS Errors 1,711 1,646 1,688 1,652 1,563POS Error Rate 2.93 2.82 2.89 2.83 2.68DM Errors 630 587 645 611 533DM Error Rate 7.61 7.09 7.79 7.38 6.43Word Perplexity 24.04 23.91 23.17 22.96 22.35Branching Perplexity 26.35 30.61 27.69 31.59 30.26discourse marker identification by 6.8%, and reduces perplexity slightly from 24.04 to23.91.
These improvements are of course at the expense of the branching perplexity,which increases from 26.35 to 30.61.
Column four augments the POS-based modelwith speech repair detection and correction, which improves POS tagging and reducesword perplexity by 3.6%, while only increasing the branching perplexity from 26.35 to27.69.
Although we are adding five variables to the speech recognition problem, mostof the extra ambiguity is resolved by the time the word is predicted.
Thus, correctionscan be sufficiently resolved by the first word of the alteration.
Column five combinesthe models of columns three and four and results in a further improvement in wordperplexity.
POS tagging and discourse marker identification do not seem to benefitfrom combining the two processes, but both rates remain better than those obtainedfrom the base model.Column six adds silence information.
Silence information is not directly used todecide the POS tags, the discourse markers, nor what words are involved; rather, itgives evidence as to whether an intonational boundary, speech repair, or editing termoccurred.
As the following sections how, silence information improves the perfor-mance on these tasks, and this translates into better language modeling, resulting ina further decrease in perplexity from 22.96 to 22.35, giving an overall perplexity re-duction of 7.0% over the POS-based model.
We also see a significant improvementin POS tagging with an error rate reduction of 9.5% over the POS-based model, anda reduction in the discourse marker error rate of 15.4%.
As we further improve themodeling of the user's utterance, we should expect o see further improvements inthe language model.8.2 Intonat iona l  PhrasesTable 11 demonstrates that modeling intonational phrases benefits from modeling si-lence information, speech repairs, and discourse markers.
Column two gives the baseresults of modeling intonational phrase boundaries.
Column three adds silence infor-mation, which reduces the error rate for turn-internal boundaries by 9.1%.
Columnfour adds speech repair detection, which further educes the error rate by 3.5%.
Col-umn five adds speech repair correction.
Curiously, this actually slightly increases theerror rate for intonational boundaries but the rate is still better than not modeling re-pairs at all (column four).
The final result for within-turn boundaries i a recall rate of71.8%, with a precision of 70.8%.
The last column subtracts out the discourse markermodeling by using the POS tagset P of Section 3.4.5, which collapses discourse markerusage with sentential usages.
Removing the modeling of discourse markers results ina 2.0% degradation i identifying turn-internal boundaries and 7.2% for end-of-turnboundaries.561Computational Linguistics Volume 25, Number 4Table 11Comparison of errors in detecting intonational phrase boundaries.WDI WDIS WDREIS WDCLOREIS WPCLOREISWithin Turn 3,585 3,259 3,145 3,199 3,262End of Tum 439 439 436 433 464All Boundaries 4,024 3,698 3,581 3,632 3,726Table 12Comparison of errors in detecting speech repairs.WDRE WDCLORE WDCLOREI WDCLOREIS WPCLOREISAll Repairs 1,106 982 909 839 879Exact Repairs 1,496 1,240 1,185 1,119 1,169Abridged 161 187 173 170 183Modification 747 512 489 459 497Fresh Starts 588 541 523 490 4898.3 Detecting Speech RepairsWe now demonstrate that detecting speech repairs benefits from modeling speechrepair correction, intonational phrases, silences, and discourse markers.
We use twomeasures to compare speech repair detection.
The first measure, referred to as AllRepairs, ignores errors that result from improperly identifying the type of repair, andhence scores a repair as correctly detected as long as it was identified as either anabridged repair, a modification repair, or a fresh start.
For experiments that includespeech repair correction, we further relax this rule.
When multiple repairs have con-tiguous reparanda, we count all repairs involved (of the hand-annotations) as correctas long as the combined reparandum is correctly identified.
Hence, for Example 29given earlier, as long as the overall reparandum was identified as from engine from,both of the hand-annotated repairs are counted as correct.We argued earlier that the proper identification of the type of repair is neces-sary for successful correction.
Hence, the second measure, Exact Repairs, counts arepair as being correctly identified only if the type of the repair is also properlydetermined.
Under this measure, a flesh start detected as a modification repair iscounted as a false positive and as a missed repair.
Just as with All Repairs, for modelsthat include speech repair correction, if a misidentified repair is correctly corrected,then it is counted as correct.
We also give a breakdown of this measure by repairtype.The results are given in Table 12.
The second column gives the base results for de-tecting speech repairs.
The third column adds speech repair correction, which improvesthe error rate from 46.2% to 41.0%, a reduction of 11.2%.
Part of this improvement isattributed to better scoring of overlapping repairs.
However, from an analysis of theresults, we found that this could account for at most 32 of the 124 fewer errors.
Hence,a reduction of at least 8.3% is directly attributed to incorporating speech repair cor-rection.
The fourth column adds intonational phrasing, which reduces the error ratefor detecting repairs from 41.0% to 37.9%, a reduction of 7.4%.
The fifth column addssilence information, which further educes the error rate to 35.0%, a reduction of 7.7%.Part of this improvement is a result of improved intonational phrase modeling, and562Heeman and Allen Modeling Speakers' UtterancesTable 13Comparison of errors in correcting speech repairs.WDCLORE WDCLOREI WDCLOREIS WPCLOREISAll Repairs 1,506 1,411 1,363 1,435Abridged 187 175 172 185Modification 616 563 535 607Fresh Starts 703 673 656 643part is a result of using pauses to detect speech repairs.
This gives a final recall rate of76.8% with a precision of 86.7%.
In the last column, we show the effect of removingthe modeling of discourse markers, which increases the error rate of detecting repairsby 4.8%.8.4 Correcting Speech RepairsTable 13 shows that correcting speech repairs benefits from modeling intonationalphrasing, silences, and discourse markers.
Column two gives the base results for cor-recting repairs, which is a recall rate of 61.9% and a precision of 71.4%.
Note thatabridged and modification repairs are corrected at roughly the same rate but the cor-rection of fresh starts proves particularly problematic.
Column three adds intonationalphrase modeling.
Just as with detecting repairs, we see that this improves correctingeach type of repair, with the overall error rate decreasing from 62.9 to 58.9, a reductionof 6.3%.
From Table 12, we see that only 73 fewer errors were made in detecting repairsafter adding intonational phrase modeling, while 95 fewer errors were made in cor-recting them.
Thus adding intonation phrases leads to better correction of the detectedrepairs.
Column four adds silence information, which further educes the error rate to56.9%, a reduction of 3.4%.
This gives a final recall rate of 65.9% with a precision of74.3%.
The last column subtracts out discourse marker modeling, which degrades thecorrection error rate by 5.2%.
From Table 12, 40 errors were introduced in detectingrepairs by removing discourse marker modeling, while 72 errors were introduced incorrecting them.
Thus modeling discourse markers leads to better correction of thedetected repairs.8.5 Collapsing Repair DistinctionsOur classification scheme distinguishes between fresh starts and modification repairs.Table 14 contrasts the full model (column 3) with one that collapses modificationrepairs and fresh starts (column 2).
To ensure a fair comparison, the reported de-tection rates do not penalize incorrect identification of the repair type.
We find thatdistinguishing fresh starts and modification repairs results in a 7.0% improvement indetecting repairs and a 6.6% improvement in correcting them.
Hence, the two typesof repairs differ enough both in how they are signaled and the manner in which theyare corrected that it is worthwhile to model them separately.
Interestingly, we alsosee that distinguishing between fresh starts and modification repairs improves into-national phrase identification by 1.9%.
This improvement is undoubtedly attributableto the fact that the reparandum onset of fresh starts interacts more strongly with into-national boundaries than does the reparandum onset of modification repairs.
As forperplexity and POS tagging, there was virtually no difference, xcept a slight increasein branching perplexity for the full model.563Computational Linguistics Volume 25, Number 4Table 14Effect of collapsing modification repairs and fresh starts.Collapsed DistinctErrors in Detecting Speech Repairs 902 839Errors in Correcting Speech Repairs 1,460 1,363Errors in Identifying Within-Turn Boundaries 3,260 3,199Table 15Speech repair detection and correction results for full model.Detection CorrectionRecall Precision Error Rate Recall Precision Error RateAll Repairs 76.79 86.66 35.01 65.85 74.32 56.88Abridged 75.88 82.51 40.18 75.65 82.26 40.66Modification 80.87 83.37 35.25 77.95 80.36 41.09Fresh Starts 48.58 69.21 73.02 36.21 51.59 97.76Modification and Fresh Starts 73.69 83.85 40.49 63.76 72.54 60.369.
ComparisonComparing the performance of our model to others that have been proposed is prob-lematic.
First, there are differences in corpora.
The Trains corpus is a collection ofdialogues between two people, both of whom realize that they are talking to an-other person.
The ATIS corpus (MADCOW 1992), on the other hand, is a collectionof human-computer dialogues.
The rate of repairs in this corpus is much lower andalmost all speaker turns consists of just one contribution.
The Switchboard corpus(Godfrey, Holliman, and McDaniel 1992) is a collection of human-human dialogues,which are much less constrained and about a much wider domain.
Even more extremeare corpora of professionally read speech.
A second problem is that different systemsemploy different inputs; for instance, does the input include POS tags, utterance seg-mentation, or hand-transcriptions of the words that were uttered?
We also note thatthis work is the first proposal that combines the detection and correction of speechrepairs, the identification of intonational phrases and discourse markers, and POS tag-ging, in a framework that is amenable to speech recognition.
Hence our comparisonis with systems that address only part of the problem.9.1 Speech RepairsTable 15 gives the results of the full model for detecting and correcting speech re-pairs.
The overall correction recall rate is 65.9% with a precision of 74.3%.
In thetable, we also report the results for each type of repair using the Exact Repair met-ric.
To facilitate comparisons with approaches that do not distinguish between mod-ification repairs and fresh starts, we give the combined results of these two cate-gories.Bear, Dowding, and Shriberg (1992) investigated the use of pattern matching ofthe word correspondences, global and local syntactic and semantic ill-formedness,and acoustic ues as evidence for detecting speech repairs.
They tested their patternmatcher on a subset of the ATIS corpus from which they removed "all trivial" repairs,repairs that involve only the removal of a word fragment or a filled pause.
For their564Heeman and Allen Modeling Speakers' Utterancespattern-matching results, they achieved a detection recall rate of 76% with a precisionof 62%, and a correction recall rate of 44% with a precision of 35%.
They also triedcombining syntactic and semantic knowledge in a "parser-first" approach--first try toparse the input and if that fails, invoke repair strategies based on word patterns in theinput.
In a test set containing 26 repairs Dowding et al 1993, they obtained a detectionrecall rate of 42% with a precision of 85%, and a correction recall rate of 31% with aprecision of 62%.Nakatani and Hirschberg (1994) proposed that speech repairs should be detectedin a "speech-first" model using acoustic-prosodic cues, without relying on a wordtranscription.
In order to test their theory, they built a decision tree using a trainingcorpus of 148 turns of speech.
They used hand-transcribed prosodic-acoustic featuressuch as silence duration, energy, and pitch, as well as traditional text-first cues suchas presence of word fragments, filled pauses, word matches, word replacements, POStags, and position of the word in the turn, and obtained a detection recall rate of 86%with a precision of 91%.
The cues they found relevant were duration of pauses betweenwords, word fragments, and lexical matching within a window of three words.
Notethat in their corpus, 73% of the repairs were accompanied by a word fragment, asopposed to 32% of the modification repairs and fresh starts in the Trains corpus.Hence, word fragments are a stronger indicator of speech repairs in their corpus thanin the Trains corpus.
Also note that their training and test sets only included turnswith speech repairs; hence their "findings hould be seen more as indicative of therelative importance of various predictors of \[speech repair\] location than as a true testof repair site location" (page 1612).Stolcke and Shriberg (1996b) incorporated repair resolution into a word-basedlanguage model.
They limited the types of repair to single and double word repetitionsand deletions, deletions from the beginning of the sentence, and filled pauses.
Inpredicting a word, they summed over the probability distributions for each type ofrepair (including no repair at all).
For hypotheses that include a repair, the predictionof the next word was based upon a cleaned up representation f the context, and tookinto account whether a single or double word repetition was predicted.
Surprisingly,they found that this model actually degrades performance, in terms of perplexity andword error rate.
They attributed this to their treatment of filled pauses: utterance-medial filled pauses hould be cleaned up before predicting the next word, whereasutterance-initial ones should be left intact, a distinction that we make in our model bymodeling intonational phrases.Siu and Ostendorf (1996) extended a language model to account for three rolesthat words such as filled pauses can play in an utterance: utterance-initial, part ofa nonabridged repair, or part of an abridged repair.
By using training data withthese roles marked and a function-specific variable n-gram model (i.e., use a differ-ent context for the probability estimates depending on the function of the word),and summing over each possible role, they achieved a perplexity reduction of 82.9 to81.1.9.2 Utterance Units and Intonational PhrasesWe now contrast our intonational phrase results with the results of other researchersin phrases, or other definitions of utterance units.
Table 16 gives our performance.Most methods for detecting phrases use end-of-turn as a source of evidence; however,this is jointly determined by both participants.
Hence, a dialogue system, designed toparticipate in the conversation, will not be able to take advantage of this information.For this reason, we focus on turn-internal intonational phrase boundaries.565Computational Linguistics Volume 25, Number 4Table 16Intonational phrase results for full model.Recall Precision Error RateWithin Turn 71.76 70.82 57.79End of Turn 98.05 94.17 8.00All Boundaries 84.76 82.53 33.17Wightman and Ostendorf (1994) used preboundary lengthening, pausal durations,and other acoustic ues to automatically abel intonational phrases and word accents.They trained a decision tree to estimate the probability of a phrase boundary giventhe acoustic ontext.
These probabilities were fed into a Markov model whose state isthe boundary type of the previous word.
For training and testing their algorithm, theyused a single-speaker corpus of news stories read by a public radio announcer.
Withthis speaker-dependent model, they achieved a recall rate of 78.1% and a precision of76.8%.
4However, it is unclear how well this will adapt to spontaneous speech, whererepairs might interfere with the cues that they use, and to speaker independent testing.Wang and Hirschberg (1992) also looked at detecting intonational phrases.
Usingautomatically labeled features, including POS tag of the current word, category ofthe constituent being built, distance from last boundary, and word accent, they builtdecision trees to classify each word as to whether it has an intonational boundary.Note that they do not model interactions with other tasks, such as POS tagging.
Withthis approach, they achieved a recall rate of 79.5% and a precision rate of 82.7% on asubset of the ATIS corpus.
Excluding end-of-turn data gives a recall rate of 72.2% anda precision of 76.2%.
These results group speech repairs with intonational boundariesand do not distinguish between them.
In their corpus, there were 424 disfluenciesand 405 turn-internal boundaries.
The performance of the decision tree that does notclassify disfluencies as intonational boundaries i significantly worse.
However, theseresults were achieved with one-tenth the data of the Trains corpus.Kompe et al (1995) combined acoustic cues with a statistical language modelto find intonational phrases.
They combined normalized syllable duration, length ofpauses, pitch contour, and energy using a multilayered perceptron that estimates theprobability Pr(vilci), where vi indicates if there is a boundary after the current wordand ci is the acoustic features of the neighboring six syllables.
This score is combinedwith the score from a statistical language model, which determines the probability ofthe word sequence with the hypothesized phrase boundary inserted using a backoffstrategy.Pr ( VilCi ) Pr ~ (... wi- 1WiV iWi+ 1Wi+ 2 " " " )Building on this work, Mast et al (1996) segmented speech into speech acts as the firststep in automatically classifying them and achieved a recognition accuracy of 92.5%on turn-internal boundaries using Verbmobil dialogues.
This translates into a recallrate of 85.0%, a precision of 53.1%, and an error rate of 90.1%.
Their model, whichemploys rich acoustic modeling, does not account for interactions with speech repairsor discourse markers, nor does it redefine the speech recognition language model.Meteer and Iyer (1996) investigated whether modeling linguistic segments, seg-ments with a single independent clause, improves language modeling.
They computed4 Derivations of recall and precision rates are given in detail  in Heeman (1997).566Heernan and Allen Modeling Speakers" Utterancesthe probability of the sequence of words with the hypothesized segment boundariesinserted into the sequence.
Working on the Switchboard corpus, they found that pre-dicting linguistic boundaries improved perplexity from 130 to 127.
Similar to thiswork, Stolcke and Shriberg (1996a) investigated how the language model can find theboundaries.
Their best results were obtained by using POS tags as part of the input,as well as the word identities of certain word classes, in particular, filled pauses, con-junctions, and certain discourse markers.
However, this work does not incorporateautomatic POS tagging and discourse marker identification.9.3 Discourse MarkersThe full model results in 533 errors in discourse marker identification, giving an er-ror rate of 6.43%, a recall of 97.26%, and a precision of 96.32%.
Although numerousresearchers have noted the importance of discourse markers in determining discoursestructure, there has not been a lot of work in actually identifying them.Hirschberg and Litman (1993) examined how intonational information can distin-guish between discourse and sentential interpretation for a set of ambiguous lexicalitems.
They used hand-transcribed intonational features and only examined iscoursemarkers that were one word long, as we have.
They found that discourse usages wereeither an intermediate phrase by themselves (or in a phrase consisting entirely of am-biguous tokens), or they are first in an intermediate phrase (or preceded by otherambiguous tokens) and are either de-accented or have a low word accent.
In a mono-logue of approximately 12,500 words, their model achieved a recall rate of 63.1% witha precision of 88.3%.
Many of the errors occurred on coordinate conjuncts, such asand, or, and but, which proved problematic for annotating as well, since "the discoursemeanings of conjunction as described in the literature ... seem to be quite similar tothe meanings of sentential conjunction" (page 518).Litman (1996) used machine learning techniques to identify discourse markers.The best set of features for predicting discourse markers were lengths of intonationaland intermediate phrase, positions of token in intonational nd intermediate phrase,composition of intermediate phrase (token is alone in intermediate phrase or phraseconsists entirely of potential discourse markers), and identity of the token.
The algo-rithm achieved a success rate of 85.5%, which translates into a discourse marker errorrate of 37.3%, in comparison to the rate of 45.3% for Hirschberg and Litman (1993).Direct comparisons with our error rate of 6.4% are problematic since our corpus is fivetimes as large and we use task-oriented human-human dialogues, which include a lotof turn-initial discourse markers for coordinating mutual belief.
In any event, the workof Litman and Hirschberg indicates the usefulness of modeling intermediate phraseboundaries and word accents.
Conversely, our approach does not force decisions to bemade independently and does not assume intonational nnotations as input; rather,we identify discourse markers as part of the task of searching for the best assignmentof discourse markers along with POS tags, speech repairs, and intonational phrases.10.
Conc lus ion and Future WorkIn this paper, we redefined the speech recognition language model so that it alsoidentifies POS tags, intonational phrases, and discourse markers, and resolves peechrepairs.
This language model allows the speech recognizer to model the speaker'sutterances, rather than simply the words involved.
This allows it to better account forthe words involved and allows it to return a more meaningful analysis of the speaker'sturn for later processing.
The model incorporates identifying intonational phrases,discourse markers, and POS tags, and detecting and correcting speech repairs; hence,567Computational Linguistics Volume 25, Number 4interactions that exist between these tasks, as well as the task of predicting the nextword, can be modeled.Constraining our model to the hand-transcription, it is able to identify 71.8% of allturn-internal intonational boundaries with a precision of 70.8%, identify 97.3% of alldiscourse markers with a precision of 96.3%, and detect and correct 65.9% of all speechrepairs with a precision of 74.3%.
These results are partially attributable to accountingfor the interaction between these tasks: modeling intonation phrases improves peechrepair detection by 7.4% and correction by 6.3%; modeling speech repairs improvesintonational phrase identification by 3.5%; modeling repair correction improves re-pair detection by 8.3%; modeling repairs and intonational phrases improves discoursemarker identification by 15.4%; and removing the modeling of discourse markers de-grades intonational phrase identification by 2.0%, speech repair detection by 4.8%,and speech repair correction by 5.2%.
Speech repairs and intonational phrases creatediscontinuities that traditional speech recognition language models and POS taggershave difficulty modeling.
Modeling speech repairs and intonational phrases results ina 9.5% improvement in POS tagging and a 7.0% improvement in perplexity.
Part ofthis improvement is from exploiting silences to give evidence of the speech repairsand intonational phrase boundaries.More work still needs to be done.
First, with the exception of pauses, we do notconsider acoustic cues.
This is a rich source of information for detecting (and dis-tinguishing between) intonational phrases, interruption points of speech repairs, andeven discourse markers.
It would also help in determining the reparandum onset offresh starts, which tend to occur at intonational boundaries.
Acoustic modeling is alsoneeded to identify word fragments.
The second area is extending the model to incor-porate higher level syntactic and semantic processing.
This would not only allow usto give a much richer output from the model, but it would also allow us to accountfor interactions between this higher-level knowledge and modeling speakers' utter-ances, especially in detecting the ill-formedness that often occurs with speech repairs.It would also aid in finding richer correspondences between the reparandum andalteration, such as between the noun phrase and pronoun in the following example.Example 36 (d93-14.3 utt27)the engine can take as many ~ ,~ it can take up to three loaded boxcars?
?
yYre,aran,~ume ~ zp et alerationThe third area of future research is to show that our model works on other languages.Although the model encodes the basic structure of speech repairs, intonational phrases,and discourse markers, actual parameters are learned from a training corpus.
Prelim-inary work on a Japanese corpus indicates that the model is not language specific(Heeman and Loken-Kim 1999).
The fourth and most important area is to incorporateour work into a speech recognizer.
We have already used our POS-based model torescore word-graphs, which results in a one percent absolute reduction in word errorrate in comparison to a word-based model (Heeman 1999).
Our full model, which ac-counts for intonational phrases and speech repairs, should lead to a further reduction,as well as return a richer understanding of the speech.AcknowledgmentsWe wish to thank Allen Black, JohnDowding, K.H.
Loken-Kim, TsuyoshiMorimoto, Massimo Poesio, Eric Ringger,Len Schubert, Elizabeth Shriberg, MichaelTanenhaus, and David Traum.
We also wish568Heeman and Allen Modeling Speakers' Utterancesto thank Eva Bero, Bin Li, Greg Mitchell,Andrew Simchik, and Mia Stern for theirhelp in transcribing and giving us usefulcomments on the annotation schemes.Funding gratefully received from NSERCCanada, NSF under grant IRI-9623665,DARPA--Rome Laboratory under researchcontract F30602-95-1-0025, ONR/DARPAunder grant N00014-92-J-1512, ONR undergrant N0014-95-1-1088, ATR InterpretingTelecommunications Laboratory and CNET,France T616com.ReferencesAllen, James F., Lenhart K. Schubert, GeorgeFerguson, Peter Heeman, Chung HeeHwang, Tsuneaki Kato, Marc Light,Nathaniel Martin, Bradford Miller,Massimo Poesio, and David Traum.
1995.The Trains project: A case study inbuilding a conversational planning agent.Journal of Experimental nd Theoretical AI,7:7-48.Bahl, Lalit R., J. K. Baker, Frederick Jelinek,and Robert L. Mercer.
1977.
Perplexity--Ameasure of the difficulty of speechrecognition tasks.
In Proceedings ofthe 94thMeeting of the Acoustical Society of America.Bahl, Lalit R., Peter E Brown, PeterV.
deSouza, and Robert L. Mercer.
1989.
Atree-based statistical language model fornatural language speech recognition.
IEEETransactions on Acoustics, Speech, and SignalProcessing, 36(7):1001-1008.Bard, Ellen G. and Robin J. Lickley.
1997.On not remembering disfluencies.
InProceedings ofthe 5th European Conference onSpeech Communication a d Technology,pages 2855-2858.Beach, Cheryl M. 1991.
The interpretation fprosodic patterns at points of syntacticstructure ambiguity: Evidence for cuetrading relations.
Journal of Memory andLanguage, 30(6):644-663.Bear, John, John Dowding, and ElizabethE.
Shriberg.
1992.
Integrating multipleknowledge sources for detection andcorrection of repairs in human-computerdialogue.
In Proceedings ofthe 30th AnnualMeeting, pages 56-63.
Association forComputational Linguistics.Bear, John, John Dowding, ElizabethE.
Shriberg, and Patti Price.
1993.
Asystem for labeling self-repairs in speech.Technical Note 522, SRI International.Bear, John and Patti Price.
1990.
Prosody,syntax, and parsing.
In Proceedings ofthe28th Annual Meeting, pages 17-22.Association for ComputationalLinguistics.Black, Ezra, Fred Jelinek, John Lafferty,David Magerman, Robert Mercer, andSalim Roukos.
1992.
Towardshistory-based grammars: Using richermodels for probabilistic parsing.
InProceedings ofthe DARPA Speech andNatural Language Workshop, ages 134-139.Morgan Kaufman.Blackmer, Elizabeth R. and Janet L. Mitton.1991.
Theories of monitoring and thetiming of repairs in spontaneous speech.Cognition, 39:173-194.Breiman, Leo, Jerome H. Friedman,Richard A. Olshen, and Charles J. Stone.1984.
Classification and Regression Trees.Wadsworth & Brooks.Brown, Gillian and George Yule.
1983.Discourse Analysis.
Cambridge UniversityPress.Brown, Peter F., Vincent J. Della Pietra,Peter V. deSouza, Jenifer C. Lai, andRobert L. Mercer.
1992.
Class-basedn-gram models of natural anguage.Computational Linguistics, 18(4):467--479.Charniak, Eugene, C. Hendrickson,N.
Jacobson, and M. Perkowitz.
1993.Equations for part-of-speech tagging.
InProceedings ofthe National Conference onArtificial Intelligence, pages 784-789.Chow, Yen-Lu and Richard Schwartz.
1989.The n-best algorithm: An efficientprocedure for finding top n sentencehypotheses.
In Proceedings ofthe DARPASpeech and Natural Language Workshop,pages 199-202.Church, Kenneth.
1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
In Proceedings ofthe 2ndConference on Applied Natural LanguageProcessing, pages 136-143.Dowding, John, Jean M. Gawron, DougAppelt, John Bear, Lynn Cherny, RobertMoore, and Douglas Moran.
1993.Gemini: A natural anguage system forspoken-language understanding.
InProceedings ofthe 31st Annual Meeting,pages 54-61.
Association forComputational Linguistics.Entropic Research Laboratory, Inc. 1994.Aligner Reference Manual.
Version 1.3.Godfrey, John J., Edward C. Holliman, andJane McDaniel.
1992.
SWITCHBOARD:Telephone speech corpus for research anddevelopment.
In Proceedings oftheInternational Conference on Audio, Speech andSignal Processing, pages 517-520.Heeman, Peter A.
1997.
Speech Repairs,Intonational Boundaries and DiscourseMarkers: Modeling Speakers' Utterances inSpoken Dialogue.
Doctoral dissertation,Department ofComputer Science,569Computational Linguistics Volume 25, Number 4University of Rochester.Heeman, Peter A.
1999.
POS tags anddecision trees for language modeling.
InProceedings ofthe Joint SIGDAT Conferenceon Empirical Methods in Natural LanguageProcessing and Very Large Corpora, pages129-137.Heeman, Peter A. and James F. Allen.
1994.Detecting and correcting speech repairs.In Proceedings ofthe 32nd Annual Meeting,pages 295-302.
Association forComputational Linguistics.Heeman, Peter A. and James F. Allen.
1995.The Trains Spoken Dialog Corpus.
CD-ROM,Linguistics Data Consortium.Heeman, Peter A. and K. H. Loken-Kim.1999.
Detecting and correcting speechrepairs in Japanese.
In Proceedings oftheICPhS Satellite Meeting on Disfluency inSpontaneous Speech, pages 43-46.Heeman, Peter A., K. H. Loken-Kim, andJames F. Allen.
1996.
Combining thedetection and correction of speech repairs.In Proceedings ofthe 4th InternationalConference on Spoken Language Processing,pages 358-361.Hindle, Donald.
1983.
Deterministic parsingof syntactic non-fluencies.
In Proceedings ofthe 21st Annual Meeting, pages 123-128.Association for ComputationalLinguistics.Hirschberg, Julia and Diane Litman.
1993.Empirical studies on the disambiguationof cue phrases.
Computational Linguistics,19(3):501-530.Jelinek, Frederick.
1985.
Self-organizedlanguage modeling for speechrecognition.
Technical report, IBMT.J.
Watson Research Center.Jelinek, Frederick.
and Robert L. Mercer.1980.
Interpolated estimation of Markovsource parameters from sparse data.
InProceedings ofthe Workshop on PatternRecognition i  Practice, pages 381-397.Katz, Slava M. 1987.
Estimation ofprobabilities from sparse data for thelanguage model component of a speechrecognizer.
IEEE Transactions on Acoustics,Speech, and Signal Processing, 35(3):400-401.Kikui, Gen-ichiro and Tsuyoshi Morimoto.1994.
Similarity-based i entification ofrepairs in Japanese spoken language.
InProceedings ofthe 3rd InternationalConference on Spoken Language Processing,pages 915-918.Kompe, Ralf, Andreas Kiet~ling, HeinrichNiemann, Elmar NOth, E. Gi~nterSchukat-Talamazzini, A. Zottmann, andAnton Batliner.
1995.
Prosodic scoring ofword hypotheses graphs.
In Proceedings ofthe 4th European Conference on SpeechCommunication a d Technology, pages1333-1336.Levelt, Willem J.
1983.
Monitoring andself-repair in speech.
Cognition, 14:41-104.Lickley, Robin J. and Ellen G. Bard.
1992.Processing disfluent speech: Recognizingdisfluency before lexical access.
InProceedings ofthe 2nd InternationalConference on Spoken Language Processing,pages 935-938.Lickley, Robin J., Richard C. Shillcock, andEllen G. Bard.
1991.
Processing disfluentspeech: How and when are disfluenciesfound?
In Proceedings ofthe 2nd EuropeanConference on Speech Communication a dTechnology, pages 1499-1502.Litman, Diane J.
1996.
Cue phraseclassification using machine learning.Journal of Arti~cial Intelligence Research,5:53-94.MADCOW.
1992.
Multi-site data collectionfor a spoken language corpus.
InProceedings ofthe DARPA Workshop onSpeech and Natural Language Processing,pages 7-14.Magerman, David M. 1994.
Natural LanguageParsing as Statistical Pattern Recognition.Doctoral dissertation, Department ofComputor Science, Stanford University.Marcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313-330.Martin, J. G. and W. Strange.
1968.
Theperception of hesitation in spontaneousspeech.
Perception and Psychophysics,53:1-15.Mast, Marion, Ralf Kompe, Stefan Harbeck,Andreas Kiet~ling, Heinrich Niemann,Elmar NOth, E. GiintherSchukat-Talamazzini, and Volker Warnke.1996.
Dialogue act classification with thehelp of prosody.
In Proceedings ofthe 4thInternational Conference on Spoken LanguageProcessing, pages 1728-1731.Meteer, Marie and Rukmini Iyer.
1996.Modeling conversational speech forspeech recognition.
In Proceedings oftheConference on Empirical Methods in NaturalLanguage Processing, pages 33-47.Nakatani, Christine H. and Julia Hirschberg.1994.
A corpus-based study of repair cuesin spontaneous speech.
Journal of theAcoustical Society of America,95(3):1603-1616.Nooteboom, S. G. 1980.
Speaking andunspeaking: Detection and correction ofphonological and lexical errors.
InVictoria A. Fromkin, editor, Errors inLinguistic Performance: Slips of the Tongue,570Heeman and Allen Modeling Speakers' UtterancesEar, Pen, and Hand.
Academic Press, pages86-97.O'Shaughnessy, Douglas.
1994.
Correctingcomplex false starts in spontaneousspeech.
In Proceedings ofthe InternationalConference on Audio, Speech and SignalProcessing, pages 349-352.Ostendorf, Marl, Colin Wightman, andNanette Veilleux.
1993.
Parse scoring withprosodic information: Ananalysis/synthesis approach.
ComputerSpeech and Language, 7(2):193-210.Schiffrin, Deborah.
1987.
Discourse Markers.Cambridge University Press.Shriberg, Elizabeth E. 1994.
Preliminaries to aTheory of Speech Disfluencies.
Doctoraldissertation, University of California atBerkeley.Shriberg, Elizabeth E., Rebecca Bates, andAndreas Stolcke.
1997.
A prosody-onlydecision-tree model for disfluencydetection.
In Proceedings ofthe 5th EuropeanConference on Speech Communication a dTechnology, pages 2383-2386.Shriberg, Elizabeth E. and Robin J. Lickley.1993.
Intonation of clause-internal filledpauses.
Phonetica, 50(3):172-179.Silverman, Ken, Mary Beckman, JohnPitrelli, Mari Ostendorf, Colin Wightman,Patti Price, Janet Pierrehumbert, and JuliaHirschberg.
1992.
ToBI: A standard forlabelling English prosody.
In Proceedings ofthe 2nd International Conference on SpokenLanguage Processing, pages 867-870.Siu, Man-hung and Marl Ostendorf.
1996.Modeling disfluencies in conversationalspeech.
In Proceedings ofthe 4thInternational Conference on Spoken LanguageProcessing, pages 382-391.Stolcke, Andreas and Elizabeth E. Shriberg.1996a.
Automatic linguistic segmentationof conversational speech.
In Proceedings ofthe 4th International Conference on SpokenLanguage Processing, pages 1001-1004.Stolcke, Andreas and Elizabeth E. Shriberg.1996b.
Statistical language modeling forspeech disfluencies.
In Proceedings oftheInternational Conference on Audio, Speech andSignal Processing, pages 405-408.Traum, David R. and Peter A. Heeman.1997.
Utterance units in spoken dialogue.In Elisabeth Maier, Marion Mast, andSusann LuperFoy, editors, DialogueProcessing in Spoken Language Systems,Lecture Notes in Artificial Intelligence.Springer-Verlag, Heidelberg, pages125-140.Wang, Michelle Q. and Julia Hirschberg.1992.
Automatic lassification ofintonational phrase boundaries.
ComputerSpeech and Language, 6:175-196.Wightman, Colin and Mari Ostendorf.
1994.Automatic labeling of prosodic patterns.IEEE Transactions on Speech and AudioProcessing, 2(4):469-481.Zeppenfeld, Torsten, Michael Finke, KlausRies, Martin Westphal, and Alex Waibel.1997.
Recognition of conversationaltelephone speech using the Janus speechengine.
In Proceedings ofthe InternationalConference on Audio, Speech and SignalProcessing, pages 1815-1818.571
