Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2297?2305,Berlin, Germany, August 7-12, 2016. c?2016 Association for Computational LinguisticsAutomatic Labeling of Topic Models Using Text SummariesXiaojun Wan and Tianming WangInstitute of Computer Science and Technology, The MOE Key Laboratory of ComputationalLinguistics, Peking University, Beijing 100871, China{wanxiaojun, wangtm}@pku.edu.cnAbstractLabeling topics learned by topic models isa challenging problem.
Previous studieshave used words, phrases and images tolabel topics.
In this paper, we propose touse text summaries for topic labeling.Several sentences are extracted from themost related documents to form the sum-mary for each topic.
In order to obtainsummaries with both high relevance, cov-erage and discrimination for all the topics,we propose an algorithm based on sub-modular optimization.
Both automatic andmanual analysis have been conducted ontwo real document collections, and wefind 1) the summaries extracted by ourproposed algorithm are superior over thesummaries extracted by existing popularsummarization methods; 2) the use ofsummaries as labels has obvious ad-vantages over the use of words andphrases.1 IntroductionStatistical topic modelling plays very importantroles in many research areas, such as text mining,natural language processing and information re-trieval.
Popular topic modeling techniques in-clude Latent Dirichlet Allocation (LDA) (Blei etal., 2003) and Probabilistic Latent Semantic Anal-ysis (pLSA) (Hofmann, 1999).
These techniquescan automatically discover the abstract ?topics?that occur in a collection of documents.
Theymodel the documents as a mixture of topics, andeach topic is modeled as a probability distributionover words.Although the discovered topics?
word distribu-tions are sometimes intuitively meaningful, a ma-jor challenge shared by all such topic models is toaccurately interpret the meaning of each topic(Mei et al, 2007).
The interpretation of each topicis very important when people want to browse,understand and leverage the topic.
However, it isusually very hard for a user to understand the dis-covered topics based only on the multinomial dis-tribution of words.
For example, here are the topterms for a discovered topic: {fire miles areanorth southern people coast homes south damagenorthern river state friday central water rain highcalifornia weather}.
It is not easy for a user tofully understand this topic if the user is not veryfamiliar with the document collection.
The situa-tion may become worse when the user faces witha number of discovered topics and the sets of topterms of the topics are often overlapping with eachother on many practical document collections.In order to address the above challenge, a fewprevious studies have proposed to use phrases,concepts and even images for labeling the discov-ered topics (Mei et al, 2007; Lau et al, 2011;Hulpus et al, 2013; Aletras and Stevenson, 2013).For example, we may automatically extract thephrase ?southern california?
to represent the ex-ample topic mentioned earlier.
These topic labelscan help the user to understand the topics to someextent.
However, the use of phrases or concepts astopic labels are not very satisfactory in practice,because the phrases or concepts are still very short,and the information expressed in these short labelsis not adequate for user?s understanding.
The casewill become worse when some ambiguous phraseis used or multiple discrete phrases with poor co-herence are used for a topic.
To address the draw-backs of the above short labels, we need to pro-vide more contextual information and considerusing long text descriptions to represent the topics.The long text descriptions can be used inde-pendently or used as beneficial complement to theshort labels.
For example, below is part of thesummary label produced by our proposed methodand it provides much more contextual informationfor understanding the topic.Showers and thunderstorms developed in parchedareas of the southeast , from western northcarolina into south central alabama , northcentral and northeast texas and the central andsouthern gulf coast .
?
The quake was felt over a2297large area , extending from santa rosa , about 60miles north of san francisco , to the santa cruzarea 70 miles to the south ?.
Fourteen homeswere destroyed in baldwin park 20 miles northeastof downtown los angeles and five were damagedalong with five commercial buildings when 75mph gusts snapped power lines , igniting a fire atallan paper co. , fire officials said .
?The contributions of this paper are summarizedas follows:1) We are the first to invesitage using textsummaries for topic labeling;2) We propose a summarization algorithmbased on submodular optimization to extractsummaries with both high relevance, coverageand discrimination for all topics.3) Automatic and manual analysis reveals theusefulness and advantages of the summaries pro-duced by our algorithm.2 Related Work2.1 Topic LabelingAfter topics are discovered by topic modelingtechniques, these topics are conventionally repre-sented by their top N words or terms (Blei et al,2003; Griffiths and Steyvers, 2004).
The words orterms in a topic are ranked based on the condi-tional probability p(??|??)
in that topic.
It issometimes not easy for users to understand eachtopic based on the terms.
Sometimes topics arepresented with manual labeling for exploring re-search publications (Wang and McCallum, 2006;Mei et al, 2006), and the labeling process is timeconsuming.In order to make the topic representations moreinterpretable and make the topics easier to under-stand, there are a few studies proposing to auto-matically find phrases, concepts or even imagesfor topic labeling.
Mei et al (2007) proposed touse phrases (chunks or ngrams) for topic labelingand cast the labeling problem as an optimizationproblem involving minimizing Kullback-Leibler(KL) divergence between word distributions andmaximizing mutual information between a labeland a topic model.
Lau et al (2011) also usedphrases as topic labels and they proposed to usesupervised learning techniques for ranking candi-date labels.
In their work, candidate labels includethe top-5 topic terms and a few noun chunks ex-tracted from related Wikipedia articles.
Mao et al(2012) proposed two effective algorithms that au-tomatically assign concise labels to each topic ina hierarchy by exploiting sibling and parent-childrelations among topics.
Kou et al (2015) pro-posed to map topics and candidate labels (phrases)to word vectors and letter trigram vectors in orderto find which candidate label is more semanticallyrelated to that topic.
Hulpus et al (2013) took anew approach based on graph centrality measuresto topic labelling by making use of structured dataexposed by DBpedia.
Different from the aboveworks, Aletras and Stevenson (2013) proposed touse images for representing topics, where candi-date images for each topic are retrieved from theweb and the most suitable image is selected by us-ing a graph-based algorithm.
In a very recentstudy (Aletras et al, 2015), 3 different topic rep-resentations (lists of terms, textual phrase labelsand images labels) are compared in a documentretrieval task, and results show that textual phraselabels are easier for users to interpret than termlists and image labels.The phrase-based labels in the above works arestill very short and are sometimes not adequate forinterpreting the topics.
Unfortunately, none ofprevious works has investigated using textualsummaries for representing topics yet.2.2 Document SummarizationThe task of document summarization aims to pro-duce a summary with a length limit for a givendocument or document set.
The task has been ex-tensively investigated in the natural language pro-cessing and information retrieval fields, and mostprevious works focus on directly extracting sen-tences from a news document or collection toform the summary.
The summary can be used forhelping users quickly browse and understand adocument or document collection.Typical multi-document summarization meth-ods include the centroid-based method (Radev etal., 2004), integer linear programming (ILP) (Gil-lick et al, 2008), sentence-based LDA (Chang andChien, 2009), submodular function maximization(Lin and Bilmes, 2010; Lin and Bilmes, 2011),graph based methods (Erkan and Radev, 2004;Wan et al, 2007; Wan and Yang, 2008), and su-pervised learning based methods (Ouyang et al,2007; Shen et al, 2007).
Though different sum-marization methods have been proposed in recentyears, the submodular function maximizationmethod is still one of the state-of-the-art summa-rization methods.
Moreover, the method is easy tofollow and its framework is very flexible.
One candesign specific submodular functions for address-ing special summarization tasks, without alteringthe overall greedy selection framework.2298Though various summarization methods havebeen proposed, none of existing works has inves-tigated or tried to adapt document summarizationtechniques for the task of automatic labeling oftopic models.3 Problem FormulationGiven a set of latent topics extracted from a textcollection and each topic is represented by a mul-tinomial distribution over words, our goal is toproduce understandable text summaries as labelsfor interpreting all the topics.
We now give twouseful definitions for later use.Topic: Each topic  ?
is a probability distribu-tion of words {??(?)}??
?, where V is the vocab-ulary set, and we have ?
??(?)
= 1???
.Topic Summary: In this study, a summary foreach topic ?
is a set of sentences extracted fromthe document collection and it can be used as alabel to represent the latent meaning of ?.
Typi-cally, the length of the summary is limited to 250words, as defined in recent DUC and TAC confer-ences.Like the criteria for the topic labels in (Mei etal., 2007), the topic summary for each topic needsto meet the following two criteria:High Relevance: The summary needs to be se-mantically relevant to the topic, i.e., the summaryneeds to be closely relevant to all representativedocuments of the topic.
The higher the relevanceis, the better the summary is.
This criterion is in-tuitive because we do not expect to obtain a sum-mary unrelated to the topic.High Coverage: The summary needs to coveras much semantic information of the topic as pos-sible.
The summary usually consists of severalsentences, and we do not expect all the sentencesto focus on the same piece of semantic infor-mation.
A summary with high coverage will cer-tainly not contain redundant information.
This cri-terion is very similar to the diversity requirementof multi-document summarization.Since we usually produce a set of summariesfor all the topics discovered in a document collec-tion.
In order to facilitate users to understand allthe topics, the summaries need to meet the follow-ing additional criterion:High Discrimination: The summaries for dif-ferent topics need to have inter-topic discrimina-tion.
If the summaries for two or more topics arevery similar with each other, users can hardly un-derstand each topic appropriately.
The higher theinter-topic discrimination is, the better the sum-maries are.4 Our MethodOur proposed method is based on submodular op-timization, and it can extract summaries with bothhigh relevance, coverage and discrimination forall topics.
We choose the framework of submodu-lar optimization because the framework is veryflexible and different objectives can be easily in-corporated into the framework.
The overall frame-work of our method consists of two phases: can-didate sentence selection, and topic summary ex-traction.
The two phrases are described in the nexttwo subsections, respectively.4.1 Candidate Sentence SelectionThere are usually many thousands of sentences ina document collection for topic modelling, and allthe sentences are more or less correlated with eachtopic.
If we use all the sentences for summary ex-traction, the summarization efficiency will bevery low.
Moreover, many sentences are not suit-able for summarization because of their low rele-vance with the topic.
Therefore, we filter out thelarge number of unrelated sentences and treat theremaining sentences as candidates for summaryextraction.For each topic ?
, we compute the Kullback-Leibler (KL) divergence between the word distri-butions of the topic and each sentence s in thewhole document collection as follows:??
(?, ?
)= ?
??(?)
?
?????(?)??
(?, ?)
???(?)???????
?where ??(?)
is the probability of word w in topic?.
TW denotes the set of top 500 words in topic ?according to the probability distribution.
SW de-notes the set of words in sentence s after removingstop words.
??
(?, ?)
denotes the frequency ofword w in sentence s, and ???(?)
denotes thelength of sentence s after removing stop words.For a word w which does not appear in SW, we set??
(?, ?)
???(?)?
to a very small value (0.00001 inthis study).Then we rank the sentences by an increasing or-der of the divergence scores and keep the top 500sentences which are most related to the topic.These 500 sentences are treated as candidate sen-tences for the subsequent summarization step foreach topic.
Note that different topics have differ-ent candidate sentence sets.4.2 Topic Summary ExtractionOur method for topic summary extraction is basedon submodular optimization.
For each topic ?
as-sociated with the candidate sentence set V, our2299method aims to find an optimal summary ???
fromall possible summaries by maximizing a scorefunction under budget constraint:???
= ?????????{?(?)}s.t.
???(?)
?
?where ???(?)
denotes the length of summary E.Here E is also used to denote the set of sentencesin the summary.
L is a predefined length limit, i.e.250 words in this study.?(?)
is the score function to evaluate the over-all quality of summary E. Usually, ?(?)
is re-quired to be a submodular function, so that we canuse a simple greedy algorithm to find the near-op-timal summary with theoretical guarantee.
For-mally, for any ?
?
?
?
?\?, we have?(?
+ ?)
?
?(?)
?
?(?
+ ?)
?
?(?
)which means that the incremental ?value?
of v de-creases as the context in which v is consideredgrows from A to B.In this study, the score function ?(?)
is decom-posed into three parts and each part evaluates oneaspect of the summary:?(?)
= ???(?)
+ ???(?)
+ ???(?
)where ???(?)
, ???(?)
and ???(?)
evaluatethe relevance, coverage and discrimination ofsummary E respectively.
We will describe them indetails respectively.4.2.1 Relevance FunctionInstead of intuitively measuring relevance be-tween the summary and the topic via the KL di-vergence between the word distributions of them,we consider to measure the relevance of summaryE for topic ?
by the relevance of the sentences inthe summary to all the candidate sentences for thetopic as follows:???(?
)= ?
min?{????(?
?, ?
), ?????(?
?, ?)??????}???
?where V represents the candidate sentence set fortopic  ?, and E is used to represent the sentenceset of the summary.
???(?
?, ?)
is the standard co-sine similarity between sentences ??
?and s.  ?
?
[0,1] is a threshold co-efficient.The above function is a monotone submodularfunction because ?(?)
= ????
(?, ?)
where ?
?
0is a concave non-decreasing function.?
???(?
?, ?)???
measures how similar E is to sen-tence ??
and then ?
???(?
?, ?)???
is the largestvalue that ?
???(?
?, ?)???
can achieve.
Therefore,??
is saturated by E when ?
???(?
?, ?)
??????
???(?
?, ?)???
.
When ?
?is already saturated byE in this way, any new sentence very similar to ?
?cannot further improve the overall relevance of E,and this sentence is less possible to be added tothe summary.4.2.2 Coverage FunctionWe want the summary to cover as many topicwords as possible and contain as many differentsentences as possible.
The coverage function isthus defined as follows:???(?)
= ?
?
?
{??(?)
?
????
(?, ?)???}???
?where  ?
?
0 is a combination co-efficient.The above function is a monotone submodularfunction and it encourages the summary E to con-tain many different words, rather than a small setof words.
Because ?(?)
= ??
where ?
?
0 is aconcave non-decreasing function, we have ?(?
+?)
?
?(?)
+ ?(?).
The value of the function willbe larger when we use x and y to represent twofrequency values of two different words respec-tively than that when we use (?
+ ?)
to representthe frequency value of a single word.
Therefore,the use of this function encourages the coverageof more different words in the summary.
In otherwords, the diversity of the summary is enhanced.4.2.3 Discrimination FunctionThe function for measuring the discrimination be-tween the summary E of topic ?
and all other top-ics {??}
is defined as follows:???(?)
= ????
?
???(?)
?
??
(?, ?)
?????????
?where ?
?
0 is a combination co-efficient.The above function is still a monotone submod-ular function.
The negative sign indicates that thesummary E of topic ?
needs to be as irrelevantwith any other topic as possible, and thus makingdifferent topic summaries have much differences.4.2.4 Greedy SelectionSince ???(?
), ???(?)
and ???(?)
are all sub-modular functions, ?(?)
is also a submodularfunction.
In order to find a good approximation tothe optimal summary, we use a greedy algorithmsimilar to (Lin and Bilmes, 2010) to select sen-tence one by one and produce the final summary,as shown in Algorithm 1.2300Algorithm 1 Greedy algorithm for summaryextraction1: ?
?
?2: ?
?
?3: while ?
?
?
do4:    ???
?
??????????(??{?})??(?)???(?
)?5:    ?
?
?
?
{???}
if ?
???(?)
+ ???(???)
?
????and?(?
?
{?})
?
?(?)
?
06:    ?
?
?
?
{???
}7:  end while8:  return ?In the algorithm, ???(?)
denotes the length ofsentence s and  ?
> 0 is the scaling factor.
At eachiteration, the sentence with the largest ratio of ob-jective function gain to scaled cost is found in step4, and if adding the sentence can increase the ob-jective function value while not violating thelength constraint, it is then selected into the sum-mary and otherwise bypassed.5 Evaluation and Results5.1 Evaluation SetupWe used two document collections as evaluationdatasets, as in (Mei et al 2007): AP news andSIGMOD proceedings.
The AP news dataset con-tains a set of 2250 AP news articles, which areprovided by TREC.
There is a total of 43803 sen-tences in the AP news dataset and the vocabularysize is 37547 (after removing stop words).
TheSIGMOD proceeding dataset contains a set of2128 abstracts of SIGMOD proceedings betweenthe year 1976 and 2015, downloaded from theACM digital library.
There is a total of 15211sen-tences in the SIGMOD proceeding dataset and thevocabulary size is 13688.For topic modeling, we adopted the most popu-lar LDA to discover topics in the two datasets, re-spectively.
Particularly, we used the LDA moduleimplemented in the MALLET toolkit1.
Withoutloss of generality, we extracted 25 topics from theAP news dataset and 25 topics from the SIGMODproceeding dataset.The parameter values of our proposed summa-rization method is either directly borrowed fromprevious works or empirically set as follows: ?
=0.05, ?
= 250, ?
= 300 and ?
= 0.15.1  http://mallet.cs.umass.edu/We have two goals in the evaluation: compari-son of different summarization methods for topiclabeling, and comparison of different kinds of la-bels (summaries, words, and phrases).In particular, we compare our proposed summa-rization method (denoted as Our Method) withthe following typical summarization methods andall of them extract summaries from the same can-didate sentence set for each topic:MEAD: It uses a heuristic way to obtain eachsentence?s score by summing the scores based ondifferent features (Radev et al, 2004): centroid-based weight, position and similarity with firstsentence.LexRank: It constructs a graph based on thesentences and their similarity relationships andthen applies the PageRank algorithm for sentenceranking (Erkan and Radev, 2004).TopicLexRank: It is an improved version ofLexRank by considering the probability distribu-tion of top 500 words in a topic as a prior vector,and then applies the topic-sensitive PageRank al-gorithm for sentence ranking, similar to (Wan2008).Submodular(REL): It is based on submodularfunction maximization but only the relevancefunction is considered.Submodular(REL+COV): It is based on sub-modular function maximization and combinestwo functions: the relevance function and the cov-erage function.We also compare the following three differentkinds of labels:Word label: It shows ten topic words as labelsfor each topic, which is the most intuitive inter-pretation of the topic.Phrase label: It uses three phrases as labels foreach topic, and the phrase labels are extracted byusing the method proposed in (Mei et al, 2007),which is very closely related to our work and con-sidered a strong baseline in this study.Summary Label:  It uses a topic summary witha length of 250 words to label each topic and thesummary is produced by our proposed method.5.2 Evaluation Results5.2.1 Automatic Comparison of SummarizationMethodsIn this section, we compare different summariza-tion methods with the following automaticmeasures:2301KL divergence between word distributionsof summary and topic: For each summarizationmethod, we compute the KL divergence betweenthe word distributions of each topic and the sum-mary for the topic, then average the KL diver-gence across all topics.
Table 1 shows the results.We can see that our method and Submodu-lar(REL+COV) have the lowest KL divergencewith the topic, which means our method can pro-duce summaries relevant to the topic representa-tion.Topic word coverage: For each summarizationmethod, we compute the ratio of the words cov-ered by the summary out of top 20 words for eachtopic, and then average the ratio across all topics.We use top 20 words instead of 500 words be-cause we want to focus on the most importantwords.
The results are shown in Table 2.
We cansee that our method has almost the best coverageratio and the produced summary can cover mostimportant words in a topic.AP  SIGMODMEAD 0.832503  1.470307LexRank 0.420137  1.153163TopicLexRank 0.377587  1.112623Submodular(REL) 0.43264  1.002964Submodular(REL+COV) 0.349807  0.991071Our Method 0.360306  0.907193Table 1.
Comparison of KL divergence between worddistributions of summary and topicAP  SIGMODMEAD 0.422246  0.611355LexRank 0.651217  0.681728TopicLexRank 0.678515  0.692066Submodular(REL) 0.62815  0.713159Submodular(REL+COV) 0.683998  0.723228Our Method 0.673585  0.74572Table 2.
Comparison of the ratio of the coveredwords out of top 20 topic wordsAP SIGMODaverage max average maxMEAD 0.026961 0.546618 0.078826 0.580055LexRank 0.019466 0.252074 0.05635 0.357491TopicLexRank 0.022548 0.283742 0.062034 0.536886Submodu-lar(REL)0.028035 0.47012 0.07522 0.52629Submodular(REL+COV)0.023206 0.362795 0.048872 0.524863Our Method 0.010304 0.093017 0.024551 0.116905Table 3.
Comparison of the average and max similar-ity between different topic summariesSimilarity between topic summaries: Foreach summarization method, we compute the co-sine similarity between the summaries of any twotopics, and then obtain the average similarity andthe maximum similarity.
Seen from Table 3, thetopic summaries produced by our method has thelowest average and maximum similarity with eachother, and thus the summaries for different topicshave much difference.5.2.2 Manual Comparison of SummarizationMethodsIn this section, we compare our summarizationmethod with three typical summarization methods(MEAD, TopicLexRank and Submodular(REL))manually.
We employed three human judges toread and rank the four summaries produced foreach topic by the four methods in three aspects:relevance between the summary and the topicwith the corresponding sentence set, the contentcoverage (or diversity) in the summary and thediscrimination between different summaries.
Thehuman judges were encouraged to read a fewclosely related documents for better understand-ing each topic.
Note that the judges did not knowwhich summary was generated by our method andwhich summaries were generated by the baselinemethods.
The rank k for each summary rangesfrom 1 to 4 (1 means the best, and 4 means theworst; we allow equal ranks), and the score is thus(4-k).
We average the scores across all summariesand all judges and the results on the two datasetsare shown in Tables 4 and 5, respectively.
In thetable, the higher the score is, the better the corre-sponding summaries are.
We can see that our pro-posed method outperforms all the three baselinesover almost all metrics.rele-vancecover-agediscrimina-tionMEAD 1.03 0.8 1.13TopicLexRank 1.9 1.6 1.83Submodu-lar(REL)2.23 2 2.07Our Method 2.33 2.4 2.33Table 4.
Manual comparison of different summariza-tion methods on AP news datasetrele-vancecover-agediscrimina-tionMEAD 1.6 1.4 1.83TopicLexRank 1.77 2.1 2.1Submodu-lar(REL)2.07 2.1 2.03Our Method 2.43 2.17 2.1Table 5.
Manual comparison of different summariza-tion methods on SIGMOD proceeding dataset5.2.3 Manual Comparison of Different Kinds ofLabelsIn this section, we manually compare the threekinds of labels: words, phrases and summary, as2302mentioned in Section 5.1.
Similarly, the three hu-man judges were asked to read and rank the threekinds of labels in the same three aspects: rele-vance between the label and the topic with the cor-responding sentence set, the content coverage (ordiversity) in the label and the discrimination be-tween different labels.
The rank k for each kind oflabels ranges from 1 to 3 (1 means the best, and 3means the worst; we allow equal ranks), and thescore is thus (3-k).
We average the scores acrossall labels and all judges and the results on the twodatasets are shown in Tables 6 and 7, respectively.It is clear that the summary labels produced by ourproposed method have obvious advantages overthe conventional word labels and phrase labels.The summary labels have better evaluation resultson relevance, coverage and discrimination.rele-vancecover-agediscrimina-tionWord label 0.67 0.67 1.11Phrase label 1 0.87 1.4Summary la-bel1.83 1.87 1.9Table 6.
Manual comparison of different kinds of la-bels on AP news datasetrele-vancecover-agediscrimina-tionWord label 0.87 0.877 1.27Phrase label 1.4 1.53 1.43Summary la-bel1.8 1.97 1.9Table 7.
Manual comparison of different kinds of la-bels on AP news dataset5.2.4 Example AnalysisIn this section, we demonstrate some running ex-amples on the SIGMOD proceeding dataset.
Twotopics and the three kinds of labels are shown be-low.
For brevity, we only show the first 100 wordsof the summaries to users unless they want to seemore.
We can see that the word labels are veryconfusing, and the phrase labels for the two topicsare totally overlapping with each other and haveno discrimination.
Therefore, it is hard to under-stand the two topics by looking at the word orphrase labels.
Fortunately, by carefully readingthe topic summaries, we can understand what thetwo topics are really about.
In this example, thefirst topic is about data analysis and data integra-tion, while the second topic is about data privacy.Though the summary labels are much longer thanthe word labels or phrase labels, users can obtainmore reliable information after reading the sum-mary labels and the summaries can help users tobetter understand each topic and also know thedifference between different topics.In practice, the different kinds of labels can beused together to allow users to browse topic mod-els in a level-wise matter, as described in next sec-tion.Topic 1 on SIGMOD proceeding dataset:word label: data analysis scientific set process analyzetool insight interest scenariophrase label: data analysis ;  data integration ;  datasetsummary label: The field of data analysis seek toextract value from data for either business or scientificbenefit .
?
Nowadays data analytic application areaccessing more and more data from distributed datastore , creating a large amount of data traffic on thenetwork .
?these service will access data fromdifferent data source type and potentially need toaggregate data from different data source type withdifferent data format ?.Various data model will bediscussed , including relational data , xml data , graph-structured data , data stream , and workflow ?.Topic 2 on SIGMOD proceeding dataset:word label: user information attribute model privacyquality record result individual providephrase label: data set ;  data analysis ;  dataintegrationsummary label: An essential element for privacymetric is the measure of how much adversaries canknow about an individual ' sensitive attribute ( sa ) ifthey know the individual ' quasi-identifier ( qi) ?.Wepresent an automated solution that elicit userpreference on attribute and value , employing differentdisambiguation technique ranging from simplekeyword matching , to more sophisticated probabilisticmodel ?.Privgene need significantly less perturbationthan previous method , and it achieve higher overallresult quality , even for model fitting task where ga isnot the first choice without privacy consideration ?.5.2.5 Discussion of Practical UseAlthough the summary labels produced by ourmethod have higher relevance, coverage and dis-crimination than the word labels and the phraselabels, the summary labels have one obviousshortcoming of consuming more reading time ofusers, because the summaries are much longerthan the words and phrases.
The feedback fromthe human judges also reveals the above problemand all the three human judges said they need totake more than five times longer to read the sum-maries.
Therefore, we want to find a better way tomake use of the summary label in practice.In order to consider both the shorter readingtime of the phrase labels and the better quality of2303the summary labels, we can use both of the twokinds of labels in the following hierarchical way:For each topic, we first present only the phraselabel to users, and if they can easily know aboutthe topic after they read the phrase label, the sum-mary label will not be shown to them.
Whereas, ifusers cannot know well about the topic based onthe phrase label, or they need more informationabout the topic, they may choose to read the sum-mary label for better understanding the topic.Only the first 100 words of the summary label areshown to users, and the rest words will be shownupon request.
In this way, the summary label isused as an important complement to the phrase la-bel, and the burden of reading the longer summarylabel can be greatly alleviated.6 Conclusions and Future WorkIn this study, we addressed the problem of topiclabeling by using text summaries.
We propose asummarization algorithm based on submodularoptimization to extract representative summariesfor all the topics.
Evaluation results demonstratethat the summaries produced by our proposed al-gorithm have high relevance, coverage and dis-crimination, and the use of summaries as labelshas obvious advantages over the use of words andphrases.In future work, we will explore to make use ofall the three kinds of labels together to improvethe users?
experience when they want to browse,understand and leverage the topics.In this study, we do not consider the coherenceof the topic summaries because it is really verychallenging to get a coherent summary by extract-ing different sentences from a large set of differentdocuments.
In future work, we will try to make thesummary label more coherent by considering thediscourse structure of the summary and leveragingsentence ordering techniques.AcknowledgmentsThe work was supported by National Natural Sci-ence Foundation of China (61331011), NationalHi-Tech Research and Development Program(863 Program) of China (2015AA015403) andIBM Global Faculty Award Program.
We thankthe anonymous reviewers and mentor for theirhelpful comments.ReferencesNikolaos Aletras, and Mark Stevenson.
2013.Representing topics using images.
HLT-NAACL.Nikolaos Aletras, Timothy Baldwin, Jey Han Lau, andMark Stevenson.
2015.
Evaluating topicrepresentations for exploring document collections.Journal of the Association for Information Scienceand Technology (2015).David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal ofmachine Learning research 3: 993-1022.Ying-Lang Chang and Jen-Tzung Chien.
2009.
LatentDirichlet learning for document summarization.Proccedings of IEEE International Conference onAcoustics, Speech and Signal Processing(ICASSP2009).G?ne?
Erkan and Dragomir R. Radev.
2004.LexPageRank: Prestige in multi-document textsummarization.
In Proceedings of EMNLP.Dan Gillick, Benoit Favre, and Dilek Hakkani-Tur.2008.
The ICSI summarization system at TAC 2008.In Proceedings of the Text UnderstandingConference.Thomas Hofmann.
1999.
Probabilistic latent semanticindexing.
Proceedings of the 22nd annualinternational ACM SIGIR conference on Researchand development in information retrieval.
ACM.Ioana Hulpus, Conor Hayes, Marcel Karnstedt, andDerek Greene.
2013.
Unsupervised graph-basedtopic labelling using dbpedia.
Proceedings of thesixth ACM international conference on Web searchand data mining.
ACM.Wanqiu Kou, Fang Li, and Timothy Baldwin.
2015.Automatic labelling of topic models using wordvectors and letter trigram vectors.
InformationRetrieval Technology.
Springer InternationalPublishing, 253-264.Jey Han Lau, Karl Grieser, David Newman, andTimothy Baldwin.
2011.
Automatic labelling oftopic models.
Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies-Volume 1.
Association for ComputationalLinguistics.Hui Lin and Jeff Bilmes.
2010.
Multi-documentsummarization via budgeted maximization ofsubmodular functions.
Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics.
Association forComputational Linguistics.Hui Lin and Jeff Bilmes.
2011.
A class of submodularfunctions for document summarization.Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies-Volume 1.
Association forComputational Linguistics.Qiaozhu Mei, Chao Liu, Hang Su, and ChengXiangZhai.
2006.
A probabilistic approach tospatiotemporal theme pattern mining on weblogs.In Proceedings of the 15th international conferenceon World Wide Web, pp.
533-542.
ACM.2304Qiaozhu Mei, Xuehua Shen, and ChengXiang Zhai.2007.
Automatic labeling of multinomial topicmodels.
Proceedings of the 13th ACM SIGKDDinternational conference on Knowledge discoveryand data mining.
ACM.Xian-Ling Mao, Zhao-Yan Ming, Zheng-Jun Zha, Tat-Seng Chua, Hongfei Yan, and Xiaoming Li.
2012.Automatic labeling hierarchical topics.
InProceedings of the 21st ACM internationalconference on Information and knowledgemanagement, pp.
2383-2386.
ACM.You Ouyang, Sujian Li, and Wenjie Li.
2007.Developing learning strategies for topic-basedsummarization.
Proceedings of the sixteenth ACMconference on Conference on information andknowledge management.
ACM.Dragomir R. Radev, Hongyan Jing, Ma?gorzata Sty?,and Daniel Tam.
2004.
Centroid-basedsummarization of multiple documents.
InformationProcessing & Management 40, no.
6: 919-938.Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, andZheng Chen.
2007.
Document summarization usingConditional Random Fields.
In IJCAI, vol.
7, pp.2862-2867.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
Proceedings of the NationalAcademy of Sciences 101.suppl 1: 5228-5235.Xiaojun Wan.
2008.
Using only cross-documentrelationships for both generic and topic-focusedmulti-document summarizations.
InformationRetrieval 11.1: 25-49.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007.Manifold-ranking based topic-focused multi-document summarization.
In IJCAI, vol.
7, pp.2903-2908.Xiaojun Wan and Jianwu Yang.
2008.
Multi-documentsummarization using cluster-based link analysis.Proceedings of the 31st annual international ACMSIGIR conference on Research and development ininformation retrieval.
ACM.Xuerui Wang and Andrew McCallum.
2006.
Topicsover time: a non-Markov continuous-time model oftopical trends.
Proceedings of the 12th ACMSIGKDD international conference on Knowledgediscovery and data mining.
ACM.2305
