SenseClusters - Finding Clusters that Represent Word SensesAmruta Purandare and Ted PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN 55812{pura0010,tpederse}@d.umn.eduhttp://senseclusters.sourceforge.netAbstractSenseClusters is a freely available word sensediscrimination system that takes a purely unsu-pervised clustering approach.
It uses no knowl-edge other than what is available in a raw un-structured corpus, and clusters instances of agiven target word based only on their mutualcontextual similarities.
It is a complete sys-tem that provides support for feature selec-tion from large corpora, several different con-text representation schemes, various clusteringalgorithms, and evaluation of the discoveredclusters.1 IntroductionMost words in natural language have multiple possiblemeanings that can only be determined by considering thecontext in which they occur.
Given instances of a tar-get word used in a number of different contexts, wordsense discrimination is the process of grouping these in-stances into clusters that refer to the same word mean-ing.
Approaches to this problem are often based onthe strong contextual hypothesis of (Miller and Charles,1991), which states that two words are semantically re-lated to the extent that their contextual representationsare similar.
Hence the problem of word sense discrimi-nation reduces to that of determining which contexts of agiven target word are related or similar.SenseClusters creates clusters made up of the contextsin which a given target word occurs.
All the instances ina cluster are contextually similar to each other, making itmore likely that the given target word has been used withthe same meaning in all of those instances.
Each instancenormally includes 2 or 3 sentences, one of which containsthe given occurrence of the target word.SenseClusters was originally intended to discriminateamong word senses.
However, the methodology of clus-tering contextually (and hence semantically) similar in-stances of text can be used in a variety of natural languageprocessing tasks such as synonymy identification, textsummarization and document classification.
SenseClus-ters has also been used for applications such as email sort-ing and automatic ontology construction.In the sections that follow we will describe the basicfunctionality supported by SenseClusters.
In general pro-cessing starts by selecting features from a corpus of text.Then these features are used to create an appropriate rep-resentation of the contexts that are to be clustered.
There-after the actual clustering takes place, followed by an op-tional evaluation stage that compares the discovered clus-ters to an existing gold standard (if available).2 Feature SelectionSenseClusters distinguishes among the different contextsin which a target word occurs based on a set of featuresthat are identified from raw corpora.
SenseClusters usesthe Ngram Statistics Package (Banerjee and Pedersen,2003), which is able to extract surface lexical featuresfrom large corpora using frequency cutoffs and variousmeasures of association, including the log?likelihood ra-tio, Pearson?s Chi?Squared test, Fisher?s Exact test, theDice Coefficient, Pointwise Mutual Information, etc.SenseClusters currently supports the use of unigram,bigram, and co-occurrence features.
Unigrams are indi-vidual words that occur above a certain frequency cutoff.These can be effective discriminating features if they areshared by a minimum of 2 contexts, but not shared by allcontexts.
Very common non-content words are excludedby providing a stop?list.Bigrams are pairs of words that occur above a givenfrequency cutoff and that have a statistically significantscore on a test of association.
There may optionally beintervening words between them that are ignored.
Co?occurrences are bigrams that include the target word.
Ineffect co?occurrences localize the scope of the unigramfeatures by selecting only those words that occur withinsome number of positions from the target word.SenseClusters allows for the selection of lexical fea-tures either from a held out corpus of training data, orfrom the same data that is to be clustered, which we referto as the test data.
Selecting features from separate train-ing data is particularly useful when the amount of the testdata to be clustered is too small to identify interestingfeatures.The following is a summary of some of the optionsprovided by SenseClusters that make it possible for a userto customize feature selection to their needs:?training FILE A held out file of training data to beused to select features.
Otherwise, features will be se-lected from the data to be clustered.
?token FILE A file containing Perl regular expressionsthat defines the tokenization scheme.
?stop FILE A file containing a user provided stoplist.
?feature STRING The feature type to be selected.Valid options include unigrams, bigrams, and co-occurrences.
?remove N Ignore features that occur less N times.
?window M Allow up to M-2 words to intervene be-tween pairs of words when identifying bigram and co-occurrence features.
?stat STRING The statistical test of association toidentify bigram and co?occurrence features.
Valid valuesinclude any of the tests supported by the Ngram StatisticsPackage.3 Context RepresentationOnce features are selected, SenseClusters creates a vectorfor each test instance to be discriminated where each se-lected feature is represented by an entry/index.
Each vec-tor shows if the feature represented by the correspondingindex occurs or not in the context of the instance (binaryvectors), or how often the feature occurs in the context(frequency vectors).
This is referred to as a first ordercontext vector, since this representation directly indicateswhich features make up the contexts.
Here we are follow-ing (Pedersen and Bruce, 1997), who likewise took thisapproach to feature representation.
(Schu?tze, 1998) utilized second order context vectorsthat represent the context of a target word to be discrim-inated by taking the average of the first order vectors as-sociated with the unigrams that occur in that context.
InSenseClusters we have extended this idea such that thesefirst order vectors can also be based on co?occurrence orbigram features from the training corpus.Both the first and second order context vectors repre-sent the given instances as vectors in a high dimensionalword space.
This approach suffers from two limitations.First, there may be synonyms represented by separate di-mensions in the space.
Second, and conversely, a singledimension in the space might be polysemous and associ-ated with several different underlying concepts.
To com-bat these problems, SenseClusters follows the lead of LSI(Deerwester et al, 1990) and LSA (Landauer et al, 1998)and allows for the conversion of word level feature spacesinto a concept level semantic space by carrying out di-mensionality reduction with Singular Value Decomposi-tion (SVD).
In particular, the package SVDPACK (Berryet al, 1993) is integrated into SenseClusters to allow forfast and efficient SVD.4 ClusteringClustering can be carried out using either a first or sec-ond order vector representation of instances.
SenseClus-ters provides a seamless interface to CLUTO, a Cluster-ing Toolkit (Karypis, 2002), which implements a rangeof clustering techniques suitable for both representations,including repeated bisections, direct, nearest neighbor,agglomerative, and biased agglomerative.The first or second order vector representations of con-texts can be directly clustered using vector space meth-ods provided in CLUTO.
As an alternative, each contextvector can be represented as a point in similarity spacesuch that the distance between it and any other contextvector reflects the pairwise similarity of the underlyinginstances.SenseClusters provides support for a number of simi-larity measures, such as simple matching, the cosine, theJaccard coefficient, and the Dice coefficient.
A similar-ity matrix created by determining all pairwise measuresof similarity between contexts can be used as an inputto CLUTO?s clustering algorithms, or to SenseClusters?own agglomerative clustering implementation.5 EvaluationSenseClusters produces clusters of instances where eachcluster refers to a particular sense of the given targetword.
SenseClusters supports evaluation of these clus-ters in two ways.
First, SenseClusters provides externalevaluation techniques that require knowledge of correctsenses or clusters of the given instances.
Second, thereare internal evaluation methods provided by CLUTO thatreport the intra-cluster and inter-cluster similarity.5.1 External EvaluationWhen a gold standard clustering of the instances is avail-able, SenseClusters builds a confusion matrix that showsS1 S2 S3 S4 S5 S6C0: 2 3 3 1 99 3 111C1: 11 5 43 11 11 8 89C2: 1 19 7 19 208 7 261C3: 3 15 13 7 37 12 87C4: 6 5 8 16 143 8 186C5: 37 18 8 18 186 20 287C6: 17 7 11 59 14 13 121C7: 4 9 13 14 163 12 215C8: 54 20 15 6 16 35 146C9: 29 51 12 18 11 35 156164 152 133 169 888 153 1659Figure 1: Confusion Matrix: Prior to MappingS3 S5 S6 S4 S1 S2C1: 43 11 8 11 11 5 89C2: 7 208 7 19 1 19 261C5: 8 186 20 18 37 18 287C6: 11 14 13 59 17 7 121C8: 15 16 35 6 54 20 146C9: 12 11 35 18 29 51 156C0:* 3 99 3 1 2 3 111C3:* 13 37 12 7 3 15 87C4:* 8 143 8 16 6 5 186C7:* 13 163 12 14 4 9 215133 888 153 169 164 152 1659Figure 2: Confusion Matrix: After Mappingthe distribution of the known senses in each of the dis-covered clusters.
A gold standard most typically exists inthe form of sense?tagged text, where each sense tag canbe considered to represent a different cluster that couldbe discovered.In Figure 1, the rows C0 ?
C9 represent ten discoveredclusters while the columns represent six gold-standardsenses.
The value of cell (i,j) shows the number of in-stances in the ith discovered cluster that actually belongto the gold standard sense represented by the jth column.Note that the bottom row represents the true distributionof the instances across the senses, while the right handcolumn shows the distribution of the discovered clusters.To carry out evaluation of the discovered clusters,SenseClusters finds the mapping of gold standard sensesto discovered clusters that would result in maximally ac-curate discrimination.
The problem of assigning sensesto clusters becomes one of re-ordering the columns of theconfusion matrix to maximize the diagonal sum.
Thus,each possible re-ordering shows one assignment schemeand the sum of the diagonal entries indicates the totalnumber of instances in the discovered clusters that wouldbe in their correct sense given that alignment.
This corre-sponds to several well known problems, among them theAssignment Problem in Operations Research and findingthe maximal matching of a bipartite graph.Figure 2 shows that cluster C1 maps most closely tosense S3, while discovered cluster C2 corresponds bestto sense S5, and so forth.
The clusters marked with *are not assigned to any sense.
The accuracy of discrim-ination is simply the sum of the diagonal entries of therow/column re-ordered confusion matrix divided by thetotal number of instances clustered (435/1659 = 26%).Precision can also be computed by dividing the total num-ber of correctly discriminated instances by the numberof instances in the six clusters mapped to gold standardsenses (435/1060 = 41%).5.2 Internal EvaluationWhen gold?standard sense tags of the test instances arenot available, SenseClusters relies on CLUTO?s internalevaluation metrics to report the intra-cluster and inter-cluster similarity.
There is also a graphical componentto CLUTO known as gCLUTO that provides a visualiza-tion tool.
An example of gCLUTO?s output is provided inFigure 3, which displays a mountain view of the clustersshown in tables 1 and 2.This particular visualization illustrates the case whenthe gold?standard data has fewer senses (6) than the ac-tual number requested (10).
CLUTO and SenseClustersboth require that the desired number of clusters be speci-fied prior to clustering.
In this example we requested 10,and the mountain view reveals that there were really only5 to 7 actual distinct senses.
In unsupervised word sensediscrimination, the user will usually not know the actualnumber of senses ahead of time.
One possible solutionto this problem is to request an arbitrarily large numberof clusters and rely on such visualizations to discover thetrue number of senses.
In future work, we plan to sup-port mechanisms that automatically determine the opti-mal number of clusters/senses to be found.6 Summary of Unique FeaturesThe following are some of the distinguishing characteris-tics of SenseClusters.Feature Types SenseClusters supports the flexible se-lection of a variety of lexical features, including uni-grams, bigrams, co-occurrences.
These are selected bythe Ngram Statistics Package using statistical tests of as-sociation or frequency cutoffs.Context Representations SenseClusters supports twodifferent representations of context, first order contextvectors as used by (Pedersen and Bruce, 1997) andsecond order context vectors as suggested by (Schu?tze,1998).
The former is a direct representation of the in-stances to be clustered in terms of their features, whileFigure 3: Mountain View from gCLUTOthe latter uses an indirect representation that averages thefirst order vector representations of the features that makeup the context.Clustering SenseClusters seamlessly integratesCLUTO, a clustering package that provides a widerange of clustering algorithms and criteria functions.CLUTO also provides evaluation functions that reportthe inter-cluster and intra-cluster similarity, the mostdiscriminating features characterizing each cluster,a dendogram tree view, and a 3D mountain view ofclusters.
SenseClusters also provides a native imple-mentation of single link, complete link, and average linkclustering.Evaluation SenseClusters supports the evaluation ofdiscovered clusters relative to an existing gold standard.If sense?tagged text is available, this can be immediatelyused as such a gold standard.
This evaluation reports pre-cision and recall relative to the gold standard.LSA Support SenseClusters provides all of the func-tionality needed to carry out Latent Semantic Analysis.LSA converts a word level feature space into a conceptlevel semantic space that smoothes over differences dueto polysemy and synonymy among words.Efficiency SenseClusters is optimized to deal with alarge amount of data both in terms of the number of textinstances being clustered and the number of features usedto represent the contexts.Integration SenseClusters transparently incorporatesseveral specialized tools, including CLUTO, the NgramStatistics Package, and SVDPACK.
This provides a widenumber of options and high efficiency at various stepslike feature selection, feature space dimensionality reduc-tion, clustering and evaluation.Availability SenseClusters is an open source softwareproject that is freely distributed under the GNU PublicLicense (GPL) via http://senseclusters.sourceforge.net/SenseClusters is an ongoing project, and there are al-ready a number of published papers based on its use (e.g.,(Purandare, 2003), (Purandare and Pedersen, 2004)).7 AcknowledgmentsThis work has been partially supported by a National Sci-ence Foundation Faculty Early CAREER Developmentaward (Grant #0092784).ReferencesS.
Banerjee and T. Pedersen.
2003.
The design, imple-mentation, and use of the Ngram Statistics Package.In Proceedings of the Fourth International Conferenceon Intelligent Text Processing and Computational Lin-guistics, pages 370?381, Mexico City, February.M.
Berry, T. Do, G. O?Brien, V. Krishna, and S. Varad-han.
1993.
SVDPACK (version 1.0) user?s guide.Technical Report CS-93-194, University of Tennesseeat Knoxville, Computer Science Department, April.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41:391?407.G.
Karypis.
2002.
CLUTO - a clustering toolkit.
Tech-nical Report 02-017, University of Minnesota, Depart-ment of Computer Science, August.T.K.
Landauer, P.W.
Foltz, and D. Laham.
1998.
An in-troduction to latent semantic analysis.
Discourse Pro-cesses, 25:259?284.G.A.
Miller and W.G.
Charles.
1991.
Contextual corre-lates of semantic similarity.
Language and CognitiveProcesses, 6(1):1?28.T.
Pedersen and R. Bruce.
1997.
Distinguishing wordsenses in untagged text.
In Proceedings of the Sec-ond Conference on Empirical Methods in Natural Lan-guage Processing, pages 197?207, Providence, RI,August.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and sim-ilarity spaces.
In Proceedings of the Conference onComputational Natural Language Learning, Boston,MA.A.
Purandare.
2003.
Discriminating among word sensesusing mcquitty?s similarity analysis.
In Proceedingsof the HLT-NAACL 2003 Student Research Workshop,pages 19?24, Edmonton, Alberta, Canada, May 27 -June 1.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.
