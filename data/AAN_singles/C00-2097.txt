Compi l ing  Language Mode ls  f rom a L ingu is t i ca l ly  Mot ivatedUn i f i ca t ion  GrammarManny Rayner t>, Beth Ann Hockey t, Frankie James tElizabeth Owen Bratt ++, Sharon Goldwater ++ and Jean Mark Gawron ~tResea.rch Inst i tute forAdvanced Computer  ScienceMail Stop 19-39NASA Ames Research CenterMoffett Field, CA 94035-1000AbstractSystems now exist which are able to con:pileunification gralmnars into language models thatcan be included in a speech recognizer, but itis so far unclear whether non-trivial linguisti-cally principled gralnlnars can be used for thispurpose.
We describe a series of experimentswhich investigate the question empirica.lly, byincrementally constructing a grammar and dis-covering what prot)lems emerge when succes-sively larger versions are compiled into finitestate graph representations and used as lan-guage models for a medium-vocabulary recog-nition task.1 Introduction ~Construction of speech recognizers for n:ediuln-vocabulary dialogue tasks has now becolne animportant I)ractical problem.
The central taskis usually building a suitable language model,and a number of standard methodologies havebecome established.
Broadly speaking, thesefall into two main classes.
One approach isto obtain or create a domain corpus, and frolnit induce a statistical anguage model, usuallysome kind of N-gram grammar; the alternativeis to manually design a grammar which specifiesthe utterances the recognizer will accept.
Thereare many theoretical reasons to prefer the firstcourse if it is feasible, but in practice there is of-ten no choice.
Unless a substantial domain cor-pus is available, the only method that stands achance of working is hand-construction f an ex-i The majority of the research reported was performedat I{IACS under NASA Cooperative Agreement~ NumberNCC 2-1006.
The research described in Section 3 wassupported by the Defense Advanced Research ProjectsAgency under Con~racl~ N66001-94 C-6046 with theNaval Command, Control, and Ocean Surveillance Cen-ter.SRI International333 Ravenswood AveMenlo Park, CA 94025*netdecisionsWell ington HouseEast RoadCambr idge CB1 1BHEnglandplicit grammar based on the grammar-writer'sintuitions.If the application is simple enough, experi-ence shows that good grammars of this kindcan be constructed quickly and efficiently usingcommercially available products like ViaVoiceSDK (IBM 1999) or the Nuance Toolkit (Nu-ance 1999).
Systems of this kind typically al-low specification of some restricted subset of theclass of context-free grammars, together withannotations that permit the grammar-writer toassociate selnantic values with lexical entriesand rules.
This kind of framework is fl:lly ad-equate for small grammars.
As the gran:marsincrease in size, however, the limited expres-sive power of context-free language notation be-conies increasingly burdensome.
The grainn:a,rtends to beconie large and unwieldy, with manyrules appearing in multiple versions that con-stantly need to be kept in step with each other.It represents a large developn:ent cost, is hardto maintain, and does not usually port well tonew applications.It is tempting to consider the option of mov-ing towards a :::ore expressive grammar tbrmal-isln, like unification gramnm.r, writing the orig-inal grammar in unification grammar form andcoml)iling it down to the context-free notationrequired by the underlying toolkit.
At leastone such system (Gemilfi; (Moore ct al 1997))has been implemented and used to build suc-cessful and non-trivial applications, most no-tably ComnmndTalk (Stent ct al 1999).
Gem-ini accepts a slightly constrained version of theunification grammar formalism originally usedin the Core Language Engine (Alshawi 1992),and compiles it into context-free gran:nmrs inthe GSL formalism supported by the NuanceToolkit.
The Nuance Toolkit con:piles GSLgran:mars into sets of probabilistic finite state670gra.phs (PFSGs), which form the final bmguagemodel.The relative success of the Gemilfi systemsuggests a new question.
Ulfification grammarsha.re been used many times to build substantialgeneral gramlnars tbr English and other na.tu-ra\[ languages, but the language model orientedgra.mln~rs o far developed fi)r Gemini (includ-ing the one for ColnmandTalk) have a.ll beendomain-sl)ecific.
One naturally wonders howfeasible it is to take yet another step in the di-rection of increased genera.lity; roughly, whatwe want to do is start with a completely gen-eral, linguistically motivated gramma.r, combineit with a domain-specific lexicon, and compilethe result down to a domain-specitic context-free grammar that can be used as a la.nguagemodel.
If this 1)tetra.mine can be rea.lized, it iseasy to believe that the result would 1)e a.n ex-tremely useful methodology tbr rapid construc-tion of la.nguage models.
It is i lnportant o notetha.t there are no obvious theoretical obstaclesin our way.
The clailn that English is context-free has been respectable since a.t least the early8(Is (Pullum and Gazda.r 1982) 'e, and the idea.of using unification grammar as a. compact wa 5,of tel)resenting an ulMerlying context-fl'e~e, lan-guage is one of the main inotivations for GPSG(Gazdar et al1985) and other formalislns basedon it.
The real question is whether the goal ispractically achievable, given the resource limi-tations of current technology.In this l)a.1)er, we describe work aimed at thetarget outlined above, in which we used theGemini system (described in more detail in Sec-tion 2) to a.ttempt o compile a. va.riety of lin-guistically principled unification gralnlna.rs intola.ngua.ge lnodels.
Our first experiments (Sec-tion 3) were pertbrmed on a. large pre-existingunification gramlna.r.
These were unsuccessful,for reasons that were not entirely obvious; inorder to investigate the prol)lem more system-atically, we then conducted a second series ofexperilnents (Section 4), in which we increlnen-tally 1)uilt up a smMler gra.lnlna.r.
By monitor-ing; the behavior of the compilation process andthe resulting langua.ge model as the gra.lmnar~s2~1e m'e aware l, hal, this claim is most~ 1)robably notl;rue for natural languages ill gelmraI (lh'csnall cl al1987), but furl~hcr discussion of t.his point is beyond I.hescope of t, llC paper.cover~ge was expanded, we were a.ble to iden-tit~ the point a,t which serious problems beganto emerge (Section 5).
In the fina.1 section, wesummarize and suggest fltrther directions.2 Tile Genfini Language ModelCompi le rTo lnake the paper nlore self-contained, this sec-tion provides some background on the methodused by Gemini to compile unifica.tion grain-mars into CFGs, and then into language mod-els.
The ha.sic idea.
is the obvious one: enu-mera.te all possible instantiations of the feal;uresin the grammar rules and lexicon entries, andthus tra.nsform esch rule and entry in the ()rig-inal unification grammar into a set of rules inthe derived CFG.
For this to be possible, therelevant fe~ttul'es Inust be constrained so thatthey can only take values in a finite predefinedrange.
The finite range restriction is inconve-nient for fea.tures used to build semantic repre-sentations, and the tbrmalism consequently dis-tinguishes syntactic and semantic features; se-lmmtic features axe discarded a.t the start of thecompilation process.A naive iml)lelnentation of the basic lnethodwould be iml)raetical for any but the small-est a.nd simplest grammars, and considera.bleingemfity has been expended on various opti-mizations.
Most importantly, categories axe ex-panded in a demand-driven fa.shion, with inferlnatiotl being percolated 1)oth t)otton>up (fromthe lexicon) and top-down (fl'om the grammar'sstart symbol).
This is done in such a. waythat potentially valid colnl)inations of featureinstantiations in rules are successively filteredout if they are not licensed by the top-downand bottom-ul) constra.ints.
Ranges of featurevalues are also kept together when possible, sothat sets of context-free rules produced by themdve algorithm may in these cases be mergedinto single rules.By exploiting the structure of the gram-mar a.nd lexicon, the demand-driven expansionlnethod can often effect substa.ntial reductionsin the size of the derived CFG.
(For the typeof grammar we consider in this paper, the re-duction is typically by ,~ fa.etor of over 102?
).The downside is that even an app~trently slnallcha.nge in the syntactic t>atures associated witha.
rule may have a large eIfect on the size of671the CFG, if it opens up or blocks an impor-tant percolation path.
Adding or deleting lexi-con entries can also have a significant effect onthe size of the CFG, especially when there areonly a small number of entries in a given gram-matical category; as usual, entries of this typebehave from a software ngineering standpointlike grammar ules.The language model compiler also performsa number of other non-trivial transformations.The most important of these is related to thefact that Nuance GSL grammars are not al-lowed to contain left-recursive rules, and left-recursive unification-grammar rules must con-sequently be converted into a non-left-recursivefort::.
Rules of this type do not however occurin the gramlnars described below, and we conse-quently omit further description of the method.3 Initial ExperimentsOur initial experiments were performed on arecent unification grammar in the ATIS (AirTravel Information System) domain, developedas a linguistically principled grammar with adomain-specific lexicon.
This grammar wascre~ted for an experiment COl::t)aring cover-age and recognition performance of a hand-written grammar with that of a.uto:::atically de-rived recognition language models, as increas-ing amounts of data from the ATIS corpuswere made available for each n:ethod.
Exam-ples of sentences covered by this gralnlnar are"yes", "on friday", "i want to fly from bostonto denver on united airlines on friday septem-ber twenty third", "is the cheapest one wayfare from boston to denver a morning flight",and "what flight leaves earliest from boston tosan francisco with the longest layover in den-ver".
Problems obtaining a working recognitiongrammar from the unification grammar endedour original experiment prematurely, and ledus to investigate the factors responsible for thepoor recognition performance.We explored several ikely causes of recogni-tion trouble: number of rules, ::umber of vocab-ulary items, size of node array, perplexity, andcomplexity of the grammar, measured by aver-age and highest number of transitions per graphin the PFSG form of the grammar.We were able to in:mediately rule out sim-ple size metrics as the cause of Nuance's diffi-culties with recognition.
Our smallest air travelgrammar had 141 Gemini rules and 1043 words,producing a Nuance grammar with 368 rules.This compares to the Con:mandTalk grammar,which had 1231 Gemini rules and 1771 words,producing a Nuance gran:n:ar with 4096 rules.To determine whether the number of thewords in the grammar or the structure ofthe phrases was responsible for the recognitionproblems, we created extreme cases of a Word+grammar (i.e.
a grammar that constrains theinput to be any sequence of the words in thevocabulary) and a one-word-per-category gram-mar.
We found that both of these variantsof our gralmnar produced reasonable recogni-tion, though the Word+ grammar was very in-accurate.
However, a three-words-per-categorygrammar could not produce snccessflfl speechrecognition.Many thature specifications can lnake a gram-mar ::tore accurate, but will also result in alarger recognition grammar due to multiplica-tion of feature w~lues to derive the categoriesof the eontext-fl'ee grammar.
We experimentedwith various techniques of selecting features tobe retained in the recognition grammar.
As de-scribed in the previous ection, Gemini's defaultmethod is to select only syntactic features andnot consider semantic features in the recogni-tion grammar.
We experimented with selectinga subset of syntactic features to apply and withapplying only se:nantic sortal features, and nosyntactic features.
None of these grammars pro-duced successful speech recognition./.Fro::: these experiments, we were unable toisolate any simple set of factors to explain whichgrammars would be problematic for speechrecognition.
However, the numbers of transi-tions per graph in a PFSG did seem suggestiveof a factor.
The ATIS grammar had a high of1184 transitions per graph, while the semanticgrammar of CommandTalk had a high of 428transitions per graph, and produced very rea-sonable speech recognition.Still, at; the end of these attempts, it beca.meclear that we did not yet know the precise char-acteristic that makes a linguistically motivatedgrammar intractable for speech recognition, northe best way to retain the advantages of thehand-written grammar approach while provid-ing reasonable speech recognition.6724 Incrementa l  GrammarDeve lopmentIn our second series of experiments, we in-crelnenta.lly developed a. new grammar fronts('ra.tch.
The new gra.mma.r is basica.lly a s('a.led-down and a.dapted version of tile Core Lan-guage Engine gramme\ for English (Puhnan1!
)92; Rayner 1993); concrete development worka.nd testing were organized a.round a. speech in-terfa c(; to a. set; of functionalities oflhred by asimple simula,tion of the Space Shuttle (Rather,Hockey gll(l James 2000).
Rules and lexicalentries were added in sma.ll groups, typically2-3 rules or 5 10 lexical entries in one incre-ment.
After each round of exl)a.nsion , we testedto make sure that the gramlnar could still 1)ecompiled into a. usa.bh; recognizer, a.nd a.t sev-ere.1 points this suggested changes in our iln-1)\]ementation strategy.
The rest of this sectiondescribes tile new grmmnar in nlore detail.4.1 Overv iew of  Ru lesThe current versions of the grammar and lexi-con contain 58 rules a.nd 30J.
Ulfinflectesl entriesrespectively.
They (:over the tbllowing phenom-el i  :~IZ1.
Top-level utl;er~tnces: declarative clauses,WH-qtlestions, Y-N questions, iml)erat;ives,etlil)tical NPs and I)Ps, int(;rject.ions.~.. / \ ]  9 \,~ H-lnovement of NPs and PPs.3.
The fbllowing verb types: intr~nsi-tive, silnple transitive, PP con:plen-mnt,lnodaJ/a.uxiliary, -ing VP con-q)len:ent, par-ticleq-NP complement, sentential comple-lnent, embedded question complement.4.
PPs: simple PP, PP with postposition("ago")~ PP lnodifica,tion of VP and NP.5.
Relat;ive clauses with both relative NP pro-1101111 ("tit(; telnperature th,tt I measured )and relative PP ("the (loci: where I am").6.
Numeric determiners, time expressions,and postmodification of NP 1)y nun:eric ex-pressions.7.
Constituent conjunction of NPs andcl~ulses.Tilt following examl)le sentences illustratecurrent covera,ge: 3 '-.
, ':how ~d)out scenariothree.?
", "wha, t is the temperature?
", "mea-sure the pressure a,t flight deck", "go to tilecrew ha.tch a.nd (:lose it", "what were ten:per-a.tttt'e a, nd pressure a.t iifteen oh five?
", "is thetelnpera.ture going ttp'.
~', "do the fi?ed sensorssa.y tha.t the pressure is decreasing.
, "find outwhen the pressure rea.ched fifteen p s i .
.
.
.
wh~t 1is the pressure that you mea.sured?
", "wha.t isthe tempera.lure where you a.re?
", ?~(:a.n you findout when the fixed sensors ay the temperatureat flight deck reached thirty degrees celsius?
".4.2 Unusua l  Features  o f  the  GrammarMost of the gramn:~u', as already sta.ted, isclosely based on the Core Language Eng!negra.nlnla.r.
\?e briefly sllnllna.rize the main di-vergences between the two gramnlars.4.2.1 I nvers ionThe new gramlna, r uses a. novel trea.tment ofinversion, which is p~trtly designed to simplifythe l)l'ocess of compiling a, fea,ture gl'anllna, r intocontext-free form.
The CLE grammar's trea.t-l l tent of invers ion uses a, movement account, inwhich the fronted verb is lnoved to its notionalpla.ce in the VP through a feature.
So, tbrexample, the sentence "is pressure low?"
willin the origina.1 CLE gramma.r ha.re the phrase-structure::\[\[iS\]l" \ [p ressure \ ]N / ,  \[\[\]V \[IO\V\]AI),\]\]V'\]'\],'g"in whk:h the head of th(, VP is a V gap coin-dexed with tile fronted main verb 1,~ .Our new gra.mn:ar, in contrast, hal:dles in-version without movement, by making the con>bination of inverted ver\]) and subject into a.VBAR constituent.
A binary fea.ture invsubjpicks o:ll; these VBARs, a.nd there is a. question-forma,tion rule of tilt formS --> VP : E invsub j=y\ ]Continuing the example, the new gram-mar a.ssigns this sentence tilt simpler phrase-structure"\[\[\[is\] v \[press:ire\] N*'\] v .A .
\[\[low\] J\] V.\] S"4.2.2 Sorta l  Const ra in tsSortal constra,ints are coded into most gr~un:nnrrules as synta.ctic features in a straight-forwardlna.nner, so they are available to the compilation673process which constructs the context-free gram-mar, ~nd ultimately tile language model.
Thecurrent lexicon allows 11 possible sortal valuestbr nouns, and 5 for PPs.We have taken the rather non-standard stepof organizing tile rules for PP modification sothat a VP or NP cannot be modified by twoPPs  of the same sortal type.
The principal mo-tivation is to tighten the language model withregard to prepositions, which tend to be pho-netically reduced and often hard to distinguishfrom other function words.
For example, with-out this extra constraint we discovered that anutterance likemeasure temperature at flight deckand lower deckwould frequently be misrecognized asmeasure temperature at flight deck inlower deck5 Exper iments  with IncrementalG r am 111 ar  SOur intention when developing the new gram-mar was to find out just when problems beganto emerge with respect to compilation of tan-gm~ge models.
Our initial hypothesis was thatthese would l)robably become serious if the rulesfor clausal structure were reasonably elaborate;we expected that the large number of possibleways of combining modal and auxiliary verbs,question forlnation, movement, and sententialcomplements would rapidly combine to producean intractably loose language model.
Interest-ingly, this did not prove to be the case.
In-stead, the rules which appear to be the primaryca.use of difficulties are those relating to relativeclauses.
We describe the main results in Sec-tion 5.1; quantitative results on recognizer per-tbrmance are presented together in Section 5.2.5.1 Main FindingsWe discovered that addition of the single rulewhich allowed relative clause modification of anNP had a dr~stic effect on recognizer perfor-lnance.
The most obvious symptoms were thatrecognition became much slower and the size ofthe recognition process much larger, sometimescausing it to exceed resource bounds.
The falsereject rate (the l)roportion of utterances whichfell below the recognizer's mininmnl confidencetheshold) also increased substantially, thoughwe were surprised to discover no significant in-crea.se in the word error rate tbr sentences whichdid produce a recognition result.
To investi-gate tile cause of these effects, we examined theresults of perfornfing compilation to GSL andPFSG level.
The compilation processes are suchthat symbols retain mnemonic names, so that itis relatively easy to find GSL rules and gral)hsused to recognize phrases of specified gralnmat-ical categories.At the GSL level, addition of the relativeclause rule to the original unification grammaronly increased the number of derived Nuancerules by about 15%, from 4317 to 4959.
The av-erage size of the rules however increased muchmore a.
It, is easiest o measure size at the level ofPFSGs, by counting nodes and transitions; wefound that the total size of all the graphs had in-creased from 48836 nodes and 57195 tra.nsitionsto 113166 nodes and 140640 transitions, rathermore than doubling.
The increase was not dis-tributed evenly between graphs.
We extractedfigures for only the graphs relating to specificgrammatical categories; this showed that, thenumber of gra.1)hs fbr NPs had increased from94 to 258, and lnoreover that the average sizeof each NP graph had increased fronl 21 nodesand 25.5 transitions to 127 nodes and 165 tra.nsi-tions, a more than sixfold increase.
The graphsfor clause (S) phrases had only increased innumber froln 53 to 68.
They ha.d however alsogreatly increased in average size, from 171 nodesand 212 transitions to 445 nodes and 572 tran-sitions, or slightly less than a threefold increase.Since NP and S are by far the most importantcategories in the grammar, it is not strange thatthese large changes m~tke a great difference tothe quality of the language model, and indi-rectly to that of speech recognition.Colnparing the original unification gramlnarand the compiled CSL version, we were able tomake a precise diagnosis.
The problem with therelative clause rules are that they unify featurevalues in the critical S and NP subgralnlnars;this means that each constrains the other, lead-ing to the large observed increase in the sizeand complexity of the derived Nuance grammar.aGSL rules are written in all notat ion which allowsdisjunction and Klccne star.674Specifically, agreement ilffbrmation and sortalcategory are shared between the two daugh-ter NPs in the relative clause modification rule,which is schematically as follows:Igp: \[agr=A, sort=S\]  --+NP: \[agr=A, sort=S\]REL:\[agr=A, sort=S\]These feature settings ~re needed in order to gettile right alternation in pairs likethe robot that *measure/measuresthe teml)erature \[agr\]the *deck/teml)era.ture tha.t youmeasured \[sort\]We tested our hypothesis by colnlnenting ()litthe agr and sor t  features in the above rule.This completely solves the main 1)robh;in of ex-1)lesion in the size of the PFSG representation;tile new version is only very slightly larger thantile one with no relative clause rule (50647 nodesand 59322 transitions against 48836 nodes and57195 transitions) Most inL1)ortantty, there isno great increase in the number or average sizeof the NP and S graphs.
NP graphs increase innumber froin 94 to 130, and stay constant in a.v-era ge size.
; S graphs increase in number f}om 53to 64, and actually decrease, in aa;erage size to13,5 nodes and 167 transitions.
Tests on st)eech(l~t;a. show that recognition quality is nea~rly :liesa.me as for the version of the recognizer whichdoes not cover relative clauses.
Although speedis still significantly degraded, the process sizehas been reduced sufficiently that the 1)roblen:swith resource bounds disappear.It would be rea.sonal)le 1:o expect tim: remov-ing the explosion in the PFSG ret)resentationwould result in mL underconstrained languagemodel for the relative clause paxt of the gram-mar, causing degraded 1)erformance on utter-ances containing a, relative clause.
Interestingly,this does not appear to hapl)en , though recog-nition speed under the new grammar is signif-icaatly worse for these utterances COml)ared toutterances with no relative clause.5.2 Recogn i t ion  Resu l tsThis section summarizes our empirical recog-nition results.
With the help of the NuanceToolkit batchrec  tool, we evah:ated three ver-sions of the recognizer, which differed only withrespect to tile language model, no_re ls  usedthe version of the language model derived fl'onI agranLn:a.r with the relative clause rule removed;re l s  is the version derived from the fltll gram-lnar; and un l inked  is the colnl)romise version,which keeps the relative clause rule but removesthe critical features.
We constructed a corpusof 41 utterances, of mean length 12.1 words.The utterances were chosen so that the first, 31were within the coverage of all three versionsof the grammar; the last 10 contained relativeclauses, and were within the coverage of re :sand un: inked but :tot of no_rels .
Each utter-anee was recorded by eight different subjects,none of whom had participated in developmentof the gra.mmar or recognizers.
Tests were runon a dual-processor SUN Ultra60 with 1.5 GBof RAM.The recognizer was set, to reject uttera.nces iftheir a.ssociated confidence measure fell underthe default threshold.
Figures 1 and 2 sum-marize the re.suits for the first 31 utterances(no relative clauses) and the last 10 uttera:Lces(relative clauses) respectively.
Under '?RT',we give inean recognition speed (averaged oversubjects) e?pressed as a multiple of real time;'PRe.j' gives the false reject rate, the :heart l)er -centage of utterances which were reiected ue tolow confidence measures; 'Me:n' gives the lnean1)ercentage of uttera.nces which fhiled due to the.recognition process exceeding inemory resourcebounds; and 'WER,' gives the mean word er-ror rate on the sentences that were neither re-jected nor failed due to resource bound prob-lems.
Since the distribution was highly skewed,all mea.ns were calculated over the six subjectsrenm.i:fing after exclusion of the extreme highand low values.Looking first at Figure 1, we see that re l s  isclearly inferior to no_re ls  on tile subset of thecorpus which is within the coverage of both ver-sions: nea.rly twice as many utterances are re-jected due to low confidence values or resource1)roblems, and recognition speed is about fivetimes slower, un l inked  is in contrast :tot sig-nificantly worse than no_re ls  in terms of recog-nition performance, though it is still two and ahalf times slower.Figure 2 compares re l s  and un l inked on theutterances containing a relative clause.
It seemsreasona.ble to say that recognition performance675I C4ran"nar I I FR .i I IWER 1no_rels 1.04 9.0% - 6.0%re l s  4.76 16.1% 1.1% 5.7%un l inked  2.60 9.6% - 6.5%Figure 1: Evaluation results for 31 utterancesnot containing relative clauses, averaged across8 subjects excluding extreme values.Grammar xRT FRej Men: WER\ ]re l s  4.60 26.7% 1.6% 3.5%\]un l inked 5.29 20.0% - 5.4%JFigure 2: Evaluation results for i0 utter~mcescontaining relative clauses, averaged across 8subjects excluding extreme values.is comparable for the two versions: rels haslower word error rate, but also rqjects moreutterances.
Recognition speed is marginallylower for unl inked,  though it is not clear to uswhether the difference is significant given thehigh variability of the data.6 Conc lus ions  and  Fur therD i rec t ions  jWe found the results presented above surpris-ing and interesting.
When we 1)egal: our pro-gramme of attempting to compile increasinglylarger linguistically based unification grammarsinto language models, we had expected to see asteady combinatorial increase, which we guessedwould be most obviously related to complexclause structure.
This did not turn out to be thecase.
Instead, the serious problems we encoun-tered were caused by a small number of crit-ical rules, of which the one for relative clausemodification was by the far the worst.
It wasnot immediately obvious how to deal with theproblem, but a careful analysis revealed a rea-sonable con:promise solution, whose only draw-back was a significant but undisastrous degra-dation in recognition speed.It seems optimistic to hope that the rela-tive clause problem is the end of the story; theobvious way to investigate is by continuing toexpand the gramlnar in the same incrementalfashion, and find out what happens next.
Weintend to do this over the next few months, andexpect in due course to be able to l)resent fur-ther results.ReferencesH.
Alshawi.
1992.
The Core Language Engine.Cambridge, Massachusetts: The MIT Press.J.
Bresnan, R.M.
Kapla.n, S. Peters and A. Za-enen.
Cross-Serial Dependencies in Dutch.1987.
In W. J. Savitch et al(eds.
), The For-real Complexity of Natural Languagc, Reidel,Dordrecht, pages 286-319.G.
Gazdar, E. Klein, G. Pullum and I. Sag.1985.
Generalized Phrase Structure Gram-mar Basil Blackwell.IBM.
1999.
ViaVoice SDK tbr Windows, ver-sion 1.5.R.
Moore, J. Dowding, H. Bratt, J.M.
Gawron,Y.
Gorfl:, and A. Cheyer.
1997.
Com-mandTalk: A Spoken-Language Interfacetbr Battlefield Simulations.
Proceedingsof the Fifth Conference on Applied Nat-uraI Languagc Processing, pages 1-7,Washington, DC.
Available online fromhttp ://www.
ai.
sri.
com/natural-language/project s/arpa-sl s / commandt alk.
html.Nuance Communications.
1999.
Nuance SpeechRecognition System Developer's Manv, aI, Ver-sion 6.2G.
Pullum and G. Gazdar.
1982.
Natural Lan-guages and Context-Free Languages.
Lin-guistics and Philosophy, 4, pages 471-504.S.G.
Puhnan.
1992.
Unification-Based Synta.c-tic Analysis.
In (Alshawi 1992)M. Rayner.
1993.
English Linguistic Coverage.In M.S.
Agn~s et al 1993.
Spoken LanguageTranslator: First Year Report.
SRI Techni-cal Report CRC-043.
Available online fromhttp ://www.
sri.
com.M.
Rayner, B.A.
Hockey and F. James.
2000.Turning Speech into Scripts.
To appear inP~vceedings of the 2000 AAAI Spring Sym-posium on Natural Language Dialogues withPractical Robotic DevicesA.
Stent, J. Dowding, J.M.
Gawron, E.O.Bratt, and R. Moore.
1999.
The Coin-mandTalk Spoken Dialogue System.
P'rv-cecdings of the 37th Annual Meeting of theACL, pages 183-190.
Available online fromht tp  ://www.
a i .
s r i .
com/natura l - language/p ro jec t  s /a rpa-s  :s  / commandt a:k.  html.676
