Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 49?57,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsActive Dual Supervision: Reducing the Costof Annotating Examples and FeaturesPrem MelvilleIBM T.J. Watson Research CenterYorktown Heights, NY 10598pmelvil@us.ibm.comVikas SindhwaniIBM T.J. Watson Research CenterYorktown Heights, NY 10598vsindhw@us.ibm.comAbstractWhen faced with the task of building machinelearning or NLP models, it is often worthwhileto turn to active learning to obtain human an-notations at minimal costs.
Traditional activelearning schemes query a human for labels ofintelligently chosen examples.
However, hu-man effort can also be expended in collectingalternative forms of annotations.
For example,one may attempt to learn a text classifier bylabeling class-indicating words, instead of, orin addition to, documents.
Learning from twodifferent kinds of supervision brings a new,unexplored dimension to the problem of ac-tive learning.
In this paper, we demonstratethe value of such active dual supervision inthe context of sentiment analysis.
We showhow interleaving queries for both documentsand words significantly reduces human effort?
more than what is possible through tradi-tional one-dimensional active learning, or bypassive combinations of supervisory inputs.1 IntroductionAs a canonical running example for the theme ofthis paper, consider the problem of sentiment anal-ysis (Pang and Lee, 2008).
Given a piece of text asinput, the desired output is a polarity score that indi-cates whether this text expresses a positive or nega-tive opinion towards a topic of interest.
From a ma-chine learning viewpoint, this problem may be posedas a typical binary text classification task.
Senti-ment, however, is often conveyed with subtle lin-guistic mechanisms such as sarcasm, negation andthe use of highly domain-specific and contextualcues.
This brings a multi-disciplinary flavor to theproblem, drawing interest from both Natural Lan-guage Processing and Machine Learning communi-ties.Many methodologies proposed in these disci-plines share a common limitation that their perfor-mance is bounded by the amount and quality of la-beled data.
However, they differ conceptually inthe type of human effort they require.
On onehand, supervised machine learning techniques re-quire human effort in acquiring labeled examples,which requires reading documents and annotatingthem with their aggregate sentiment.
On the otherhand, dictionary-based NLP systems require humaneffort in collecting labeled features: for example,in the domain of movie reviews, words that evokepositive sentiment (e.g., ?mesmerizing?, ?thrilling?etc) may be labeled positive, while words that evokenegative sentiment (e.g., ?boring?,?disappointing?
)may be labeled negative.
This kind of annotationrequires a human to condense prior linguistic expe-rience with a word into a sentiment label that reflectsthe net emotion that the word evokes.We refer to the general setting of learning fromboth labels on examples and features as dual super-vision.
This setting arises more broadly in taskswhere in addition to labeled documents, it is fre-quently possible to provide domain knowledge in theform of words, or phrases (Zaidan and Eisner, 2008)or even more sophisticated linguistic features, thatassociate strongly with a class.
Recent work (Drucket al, 2008; Sindhwani and Melville, 2008) hasdemonstrated that the presence of word supervisioncan greatly reduce the number of labeled documents49required to build high quality text classifiers.In general, these two sources of supervision arenot mutually redundant, and have different annota-tion costs, human response quality, and degrees ofutility towards learning a dual supervision model.This leads naturally to the problem of active dualsupervision, or, how to optimally query a human or-acle to simultaneously collect document and featureannotations, with the objective of building the high-est quality model with the lowest cost.
Much of themachine learning literature on active learning hasfocused on one-sided example-only annotation forclassification problems.
Less attention has been de-voted to simultaneously acquiring alternative formsof supervisory domain knowledge, such as the kindroutinely encountered in NLP.
Our contribution maybe viewed as a step in this direction.2 Dual supervisionMost work in supervised learning has focused onlearning from examples, each represented by a setof feature values and a class label.
In dual super-vision we consider an additional aspect, by way oflabels of features, which convey prior knowledge onassociations of features to particular classes.
Sincewe deal only with text classification in this paper, allfeatures represent term-frequencies of words, and assuch we use feature and word interchangeably.The active learning schemes we explore in this pa-per are broadly applicable to any learner that cansupport dual supervision, but here we focus on ac-tive learning for the Pooling Multinomials classi-fier (Melville et al, 2009) described below.
In con-current related work, we propose active dual su-pervision schemes for a class of graph-based andkernel-based dual supervision methods (Sindhwaniet al, 2009).2.1 Pooling MultinomialsThe Pooling Multinomials classifier was introducedby Melville et al (2009) as an approach to incorpo-rate prior lexical knowledge into supervised learn-ing for better sentiment detection.
In the context ofsentiment analysis, lexical knowledge is available interms of the prior sentiment-polarity of words.
Froma dual supervision point of view, this knowledge canbe seen as labeled features, since the lexicon effec-tively provides associations of a set of words withthe positive or negative class.Pooling Multinomials classifies unlabeled exam-ples just as in multinomial Na?
?ve Bayes classifica-tion (McCallum and Nigam, 1998), by predictingthe class with the maximum likelihood, given byargmaxcjP (cj)?i P (wi|cj); where P (cj) is theprior probability of class cj , and P (wi|cj) is theprobability of word wi appearing in a document ofclass cj .
In the absence of background knowledgeabout the class distribution, we estimate the classpriors P (cj) solely from the training data.
However,unlike regular Na?
?ve Bayes, the conditional prob-abilities P (wi|cj) are computed using both the la-beled examples and the labeled features.Pooling distributions is a general approach forcombining information from multiple sources or ex-perts; where experts are typically represented interms of probability distributions (Clemen and Win-kler, 1999).
Here, we only consider the special caseof combining multinomial distributions from twosources ?
namely, the labeled examples and labeledfeatures.
The multinomial parameters of such mod-els can be easily combined using the linear opin-ion pool (Clemen and Winkler, 1999), in whichthe aggregate probability is given by P (wi|cj) =?Pe(wi|cj) + (1 ?
?
)Pf (wi|cj); where Pe(wi|cj)and Pf (wi|cj) represent the probability assigned byusing the example labels and feature labels respec-tively, and ?
is the weight for combining these dis-tributions.
The weight indicates a level of confi-dence in each source of information, and Melvilleet al (2009) explore ways of automatically selectingthis weight.
However, in order to not confound ourresults with the choice of weight-selection mecha-nism, here we make the simplifying assumption thatthe two experts based on instance and feature labelsare equally valuable, and as such set ?
to 0.5.To learn a model from the labeled examples wecompute conditionals Pe(wi|cj) based on observedterm frequencies, as in standard Na?
?ve Bayes classi-fication.
In addition, for Pooling Multinomials weneed to construct a multinomial model represent-ing the labeled features in the background knowl-edge.
For this, we assume that the feature-class as-sociations provided by labeled features are implic-itly arrived at by human experts by examining manypositive and negative sentiment documents.
So we50attempt to select the parameters Pf (wi|cj) of themultinomial distributions that would generate suchdocuments.
The exact values of these condition-als are presented below.
Their derivation is not di-rectly pertinent to the subject of this paper, but canbe found in (Melville et al, 2009).Given:V ?
the vocabulary, i.e., set of words in our domainP ?
set of words labeled as positiveN ?
set of words labeled as negativeU ?
set of unknown words, i.e.
V ?
(N ?
P)m ?
size of vocabulary, i.e.
|V|p ?
number of positive words, i.e.
|P|n ?
number of negative words, i.e.
|N |All words in the vocabulary can be divided intothree categories ?
words with a positive label, nega-tive label, and unknown label.
We refer to the prob-ability of any positive term appearing in a positivedocument simply as Pf (w+|+).
Similarly, we referto the probability of any negative term appearing in anegative document as Pf (w?|?
); and the probabil-ity of an unknown word in a positive or negative con-text as Pf (wu|+) and Pf (wu|?)
respectively.
Thegenerative model for labeled features can then be de-fined by:Pf (w+|+) = Pf (w?|?)
= 1p + nPf (w+|?)
= Pf (w?|+) = 1p + n ?1rPf (wu|+) = n(1?
1/r)(p + n)(m?
p?
n)Pf (wu|?)
= p(1?
1/r)(p + n)(m?
p?
n)where, the polarity level, r, is a measure of howmuch more likely it is for a positive term to occurin a positive document compared to a negative term.The value of r is set to 100 in our experiments, asdone in (Melville et al, 2009).2.2 Learning from example vs. feature labelsDual supervision makes it possible to learn from la-beled examples and labeled features simultaneously;and, as in most supervised learning tasks, one wouldexpect more labeled data of either form to lead tomore accurate models.
In this section we explore theinfluence of increased number of instance labels andfeature labels independently, and also in tandem.For these, and all subsequent experiments, weuse 10-fold cross-validation on the publicly avail-able data of movie reviews provided by Pang etal.
(2002).
This data consists of 1000 positiveand 1000 negative reviews from the Internet MovieDatabase; where positive labels were assigned to re-views that had a rating above 3.5 stars and negativelabels were assigned to ratings of 2 stars and below.We use a bag-of-words representation of reviews,where each review is represented by the term fre-quencies of the 5000 most frequent words across allreviews, excluding stop-words.In order to study the effect of increasing numberof labels we need to simulate a human oracle label-ing data.
In the case of examples this is straight-forward, since all examples in the Movies datasethave labels.
However, in the case of features, wedo not have a gold-standard set of feature labels.
Soin order to simulate human responses to queries forfeature labels, we construct a feature oracle in thefollowing manner.
The information gain of wordswith respect to the known true class labels in thedataset is computed using binary feature represen-tations.
Next, out of the 5000 total words, the top1000 as ranked by information gain are assigned alabel.
This label is the class in which the word ap-pears more frequently.
The oracle returns a ?dontknow?
response for the remaining words.
Thus, thisoracle simulates a human domain expert who is ableto recognize and label the most relevant task-specificwords, and also reject a word that falls below the rel-evance threshold.
For instance, in sentiment classi-fication, we would expect a ?don?t know?
responsefor non-polar words.We ran experiments beginning with a classifierprovided with labels for 10 randomly selected in-stances and 10 randomly selected features.
We thencompare three schemes - Instances-then-features,Features-then-instances, and Passive Interleaving.As the name suggests, Instances-then-features, isprovided labels for randomly selected instances untilall instances have been labeled, and then switches tolabeling features.
Similarly, Features-then-instancesacquires labels for randomly selected features firstand then switches to getting instance labels.
InPassive Interleaving we probabilistically switch be-51tween issuing queries for randomly chosen instanceand feature labels.
In particular, at each step wechoose to query for an instance with probability0.36, otherwise we query for a feature label.
Theinstance-query rate of 0.36 is selected based on theratio of available instances (1800) to available fea-tures (5000).
The results of these learning curves arepresented in Fig.
1.
Note that the x-axis in the figurecorresponds to the number of queries issued.
As dis-cussed earlier, in the case of features, the oracle mayrespond to a query with a class label or may issuea ?don?t know?
response, indicating that no label isavailable.
As such, the number of feature-querieson the x-axis does not correspond to the numberof actual known feature labels.
We would expectthat on average 1 in 5 feature-label queries promptsa response from the feature oracle that results in aknown feature label being provided.At the end of the learning curves, each methodhas labels for all available instances and features;and as such, the last points of all three curves areidentical.
The results show that fixing the numberof labeled features, and increasing the number of la-beled instances steadily improves classification ac-curacy.
This is what one would expect from tra-ditional supervised learning curves.
More interest-ingly, the results also indicate that we can fix thenumber of instances, and improve accuracy by la-beling more features.
Finally, results on Passive In-terleaving show that, though both feature labels andexample labels are beneficial by themselves, dual su-pervision which exploits the interaction of examplesand features does in fact benefit from acquiring bothtypes of labels concurrently.For all results above, we are selecting instancesand/or features to be labeled uniformly at random.Based on previous work in active learning one wouldexpect that we can select instances to be labeledmore efficiently, by having the learner decide whichinstances it is most likely to benefit from.
The resultsin this section suggests that actively selecting fea-tures to be labeled may also be beneficial.
Further-more, the Passive Interleaving results suggest that anideal active dual supervision scheme would activelyselect both instances and features for labeling.
Webegin by exploring active learning for feature labelsin the next section, and then consider the simultane-ous selection of instances and features in Sec.
4.5055606570758085900  1000  2000  3000  4000  5000  6000  7000AccuracyNumber of queriesInstances-then-featuresFeatures-then-instancesPassive InterleavingFigure 1: Comparing the effect of instance and featurelabel acquisition in dual supervision.3 Acquiring feature labelsTraditional active learning has primarily focused onselecting unlabeled instances to be labeled.
Thedual-supervision setting now provides us with an ad-ditional dimension to active learning, where labelsmay also be acquired for features.
In this sectionwe look at the novel task of active learning appliedonly to feature-label acquisition.
In Section 4 westudy the more general task of active dual supervi-sion, where both instance and feature labels may beacquired concurrently.3.1 Feature uncertainty vs. certaintyA very common approach to active learning for in-stances is Uncertainty Sampling (Lewis and Catlett,1994).
In this approach we acquire labels for in-stances that the current model is most uncertainabout.
Uncertainty Sampling is founded on theheuristic that uncertain instances are close to the cur-rent classification boundary, and acquiring the cor-rect labels for them are likely to help refine the loca-tion of this boundary.
Despite its simplicity, Uncer-tainty Sampling is usually quite effective in practice;which raises the question of whether one can applythe same principle to feature-label acquisition.
Inthis case, we want to select unlabeled features thatthe current model is most uncertain about.Much like instance uncertainty, feature uncer-tainty can be measured in different ways, depend-ing on the underlying method used for dual super-vision.
For instance, if the learner produces a lin-52ear classifier as in (Sindhwani and Melville, 2008),we could use the magnitude of the weights on thefeatures as a measure of uncertainty ?
where lowerweights indicate less certainty.
Since Pooling Multi-nomials builds a multinomial Na?
?ve Bayes model,we can directly use the model?s conditional proba-bilities of each word (feature) given a class.For ease of exposition we refer to the two classesin binary classification as postive (+) and negative(-), without loss of generality.
Given the probabili-ties of word f belonging to the positive and negativeclass, P (f |+) and P (f |?
), we can determine theuncertainty of a feature using the absolute value ofthe log-odds ratio, i.e.,abs(log(P (f |+)P (f |?
)))(1)The smaller this value, the more uncertain the modelis about the feature?s class association.
In every it-eration of active learning we can select the featureswith the lowest certainty scores.
We refer to this ap-proach as Feature Uncertainty.Though Uncertainty Sampling for features seemslike an appealing notion, it may not lead to bettermodels.
If a classifier is uncertain about a feature,it may have insufficient information about this fea-ture and may indeed benefit from learning its la-bel.
However, it is also quite likely that a featurehas a low certainty score because it does not carrymuch discriminative information about the classes.In the context of sentiment detection, one would ex-pect that neutral/non-polar words will appear to beuncertain words.
For example, words such as ?the?which are unlikely to help in discriminating betweenclasses, are also likely to be considered the most un-certain.
As we shortly report, on the movies dataset,Feature Uncertainty ends up wasting queries on suchwords ending up with performance inferior to ran-dom feature queries.
What works significantly bet-ter is an alternative strategy which acquires labelsfor features in the descending order of the score inEq 1.
We refer to this approach as Feature Certainty.3.2 Expected feature utilityThe intuition underlying the feature certainty heuris-tic is that it serves to confirm or correct the orienta-tion of model probabilities on different words duringthe active learning process.
One can argue that fea-ture certainty is also suboptimal in that queries maybe wasted simply confirming confident predictions,which is of limited utility to the model.
An alterna-tive to using a certainty-based heuristic, is to directlyestimate the expected value of acquiring each fea-ture label.
Such Expected Utility (Estimated RiskMinimization) approaches have been applied suc-cessfully to traditional active learning (Roy and Mc-Callum, 2001), and to active feature-value acquisi-tion (Melville et al, 2005).
In this section we de-scribe how this Expected Utility framework can beadapted for feature-label acquisition.At every step of active learning for features, thenext best feature to label is one that will result inthe highest improvement in classifier performance.Since the true label of the unlabeled features areunknown prior to acquisition, it is necessary to es-timate the potential impact of every feature queryfor all possible outcomes.1 Hence, the decision-theoretic optimal policy is to ask for feature labelswhich, once incorporated into the data, will result inthe highest increase in classification performance inexpectation.If fj is the label of the j-th feature, and qj is thequery for this feature?s label, then the Expected Util-ity of a feature query qj can be computed as:EU(qj) =K?k=1P (fj = ck)U(fj = ck) (2)Where P (fj = ck) is the probability that fj will belabeled with class ck, and U(fj = ck) is the util-ity to the model of knowing that fj has the labelck.
In practice, the true values of these two quan-tities are unknown, and the main challenge of anyExpected Utility approach is to accurately estimatethese quantities from the data currently available.A direct way to estimate the utility of a feature la-bel to classification, is to measure classification ac-curacy on the training set of a model built using thisfeature label.
However, small changes in the modelthat may result from a acquiring a single additionalfeature label may not be reflected by a change in ac-curacy.
As such, we use a more fine-grained mea-sure of classifier performance, Log Gain, which is1In the case of binary polarity classification, the possibleoutcomes are a positive or negative label for a queried feature.53computed as follows.
For a model induced from atraining set T , let P?
(ck|xi) be the probability es-timated by the model that instance xi belongs toclass ck; and I is an indicator function such thatI(ck, xi) = 1 if ck is the correct class for xi andI(ck, xi) = 0, otherwise.
Log Gain is then definedas:LG(xi) = ?K?k=1I(ck)P?
(ck|xi) (3)Then the utility of a classifier, U , can be measuredby summing the Log Gain for all instances in thetraining set T .
A lower value of Log Gain indi-cates a better classifier performance.
For a deeperdiscussion of this measure see (Saar-Tsechansky etal., 2008).In Eq.
2, apart from the measure of utility, wealso do not know the true probability distributionof labels for the feature under consideration.
Thistoo can be estimated from the training data, by see-ing how frequently the word appears in documentsof each class.
In a multinomial Na?
?ve Bayes modelwe already collect these statistics in order to deter-mine the conditional probability of a class given aword, i.e.
P (fj |ck).
We can use these probabilitiesto get an estimate of the feature label distribution,P?
(fj = ck) = P (fj |ck)?Kk=1 P (fj |ck).Given the estimated values of the feature-labeldistribution and the utility of a particular featurequery outcome, we can now estimate the ExpectedUtility of each unknown feature, and select the fea-tures with the highest Expected Utility to modeling.Though theoretically appealing, this approach isquite computationally intensive if applied to evalu-ate all unknown features.
In the worst case it re-quires building and evaluating models for each pos-sible outcome of each unlabeled feature query.
Ifyou have m features and K classes, this approachrequires training O(mK) classifiers.
However, thecomplexity of the approach can be significantly al-leviated by only applying Expected Utility evalua-tion to a sub-sample of all unlabeled features.
Giventhe large number of features with no true class la-bels, selecting a sample of available features uni-formly at random may be sub-optimal.
Instead weselect a sample of features based on Feature Cer-tainty.
In particular we select the top 100 unknownfeatures that the current model is most certain about,and identify the features in this pool with the highestExpected Utility.
We refer to this approach as Ex-pected Feature Utility.
We use Feature Certainty tosub-sample the available feature queries, since thisapproach is more likely to select features for whichthe label is known by the Oracle.3.3 Active learning with feature labelsWe ran experiments comparing the three differentactive learning approaches described above.
Inthese, and all subsequent experiments, we beginwith a model trained on 10 labeled features and 100labeled instances, which were randomly selected.From our prior efforts of manually labeling suchdata, we find this to be a reasonable initial setting.The experiments in this section focus only on theselection of features to be labeled.
So, in each itera-tion of active learning we select the next 10 feature-label queries, based on Feature Uncertainty, FeatureCertainty, or Expected Feature Utility.
As a baseline,we also compare to the performance of a model thatselects features uniformly at random.
Our results arepresented in Fig.
2.5055606570750  50  100  150  200  250  300  350  400AccuracyNumber of queriesExpected Feature UtilityFeature CertaintyRandom FeatureFeature UncertaintyFigure 2: Comparing different active learning approachesfor acquiring feature labels.The results show that Feature Uncertainty, whichis a direct analog of Uncertainty Sampling, actu-ally performs worse than random sampling.
Manyuncertain features may actually not be very usefulin discriminating between the classes, and selectingthem can be systematically worse than selecting uni-formly at random.
However, the converse approach54of Feature Certainty does remarkably well.
Thismay be because polarized words are better for learn-ing, but it is also likely that querying for such wordsincreases the likelihood of selecting one whose labelis known to the oracle.The results on Expected Feature Utility show thatestimating the expected impact of potential labelsfor features does in fact perform much better thanfeature certainty.
The results confirm that despiteour crude estimations in Eq.
2, Expected FeatureUtility is an effective approach to active learning offeature labels.
Furthermore, we demonstrate that byapplying the approach to only a small sub-sample ofcertain features, we are able to make this methodcomputationally feasible to use in practice.
In-creasing the size of the sample of candidate featurequeries is likely to improve performance, at the costof increased time in selecting queries.4 Active dual supervisionIn the previous section we demonstrated that ac-tively selecting informative features to be labeled issignificantly better than random selection.
In thissection, we look at the complementary task of se-lecting instances to be labeled, and combined activelearning for both forms of supervision.Selecting unlabeled examples for learning hasbeen a well-studied problem, and we use Uncer-tainty Sampling (Lewis and Catlett, 1994), whichhas been shown to be a computationally efficientand effective approach in the literature.
In particularwe select unlabeled examples to be labeled in orderof decreasing uncertainty, where uncertainty is mea-sured in terms of the margin, as done in (Melvilleand Mooney, 2004).
The margin on an unlabeled ex-ample is defined as the absolute difference betweenthe class probabilities predicted by the classifier forthe given example, i.e., |P (+|x)?
P (?|x)|.
We re-fer to the selection of instances based on this uncer-tainty as Instance Uncertainty, in order to distinguishit from Feature Uncertainty.We ran experiments as before, comparing selec-tion of instances using Instance Uncertainty and se-lection of features using Expected Feature Utility.In addition, we also combine these to methods byinterleaving feature and instance selection.
In par-ticular, we first order instances in decreasing orderof uncertainty, and features in terms of decreasingExpected Feature Utility.
We then probabilisticallyselect instances or features from the top of theselists, where, as before, the probability of selectingan instance is 0.36.
Recall that this probability cor-responds to the ratio of available instances (1800)and features (5000).
We refer to this approach as Ac-tive Interleaving, in contrast to Passive Interleaving,which we also present as a baseline.
Recall that Pas-sive Interleaving corresponds to probabilistically in-terleaving queries for randomly chosen, not activelychosen, examples and features.
Our results are pre-sented in Fig.
3.We observe that, Instance Uncertainty performsbetter than Passive Interleaving, which in turn is bet-ter than random selection of only instances or fea-tures ?
as seen in Fig.
1.
However, effectively se-lecting features labels, via Expected Feature Util-ity, does even better than actively selecting only in-stances.
Finally, selecting instance and features si-multaneously via Active Interleaving performs bet-ter than active learning of features or instances sep-arately.
Active Interleaving is indeed very effective,reaching an accuracy of 77% with only 500 queries,while Passive Interleaving requires more than 4000queries to reach the same performance ?
as evi-denced by Fig.
1505560657075800  50  100  150  200  250  300  350  400AccuracyNumber of queriesActive InterleavingExpected Feature UtilityInstance UncertaintyPassive InterleavingFigure 3: Comparing Active Interleaving to alternativelabel acquisition strategies.5 Related workActive learning in the context of dual supervisionmodels is a new area of research with very little prior55work, to the best of our knowledge.
Most prior workhas focused on pooled-based active learning, whereexamples from an unlabeled pool are selected for la-beling (Cohn et al, 1994; Tong and Koller, 2000).
Incontrast, active feature-value acquisition (Melvilleet al, 2005) and budgeted learning (Lizotte et al,2003) focus on estimating the value of acquiringmissing features, but do not deal with the task oflearning from feature labels.
In contrast, Raghavanand Allan (2007) and Raghavan et al (2006) studythe problem of tandem learning where they combineuncertainty sampling for instances along with co-occurence based interactive feature selection.
God-bole et al (2004) propose notions of feature uncer-tainty and incorporate the acquired feature labels,into learning by creating one-term mini-documents.Learning from labeled examples and features viadual supervision, is itself a new area of research.Sindhwani et al (2008) use a kernel-based frame-work to build dual supervision into co-clusteringmodels.
Sindhwani and Melville (2008) apply sim-ilar ideas for graph-based sentiment analysis.
Therehave also been previous attempts at using only fea-ture supervision, mostly along with unlabeled doc-uments.
Much of this work (Schapire et al, 2002;Wu and Srihari, 2004; Liu et al, 2004; Dayaniket al, 2006) has focused on using labeled featuresto generate pseudo-labeled examples that are thenused with well-known models.
In contrast, Drucket al (2008) constrain the outputs of a multinomiallogistic regression model to match certain referencedistributions associated with labeled features.6 Perspectives and future workThough Active Interleaving is a very effective ap-proach to active dual supervision, there is still a lotof room for improvement.
Firstly, Active Interleav-ing relies on Uncertainty Sampling for the selectionof instances.
Though Uncertainty Sampling has theadvantage of being fast and effective, there exist ap-proaches that lead to better models with fewer ex-amples ?
usually at the cost of computation time.One such method, estimating error reduction (Royand McCallum, 2001), is a direct analog of Ex-pected Feature Utility applied to instance selection.One would expect that an improvement in instanceselection, should directly improve any method thatcombines instance and feature label selection.
Sec-ondly, Active Interleaving uses the simple approachof probabilistically choosing to select an instance orfeature for each subsequent query.
However, a moreintelligent active scheme should be able to assess ifan instance or feature would be more beneficial ateach step.
Furthermore, we do not currently con-sider the cost of acquiring labels.
Presumably la-beling a feature versus labeling an instance couldincur very different costs ?
which could be mone-tary costs or time taken for each annotation.
Fortu-nately, the Expected Utility method is very flexible,and allows us to address all these issues within a sin-gle framework.
We can specifically estimate the ex-pected utility of different forms of annotation, perunit cost.
For instance, Provost et al (2007) usesuch an approach to estimate the utility of acquir-ing class labels and feature values (not labels) perunit cost, within one unified framework.
A similarmethod can be applied for a holistic approach to ac-tive dual supervision, where the Expected Utility ofan instance or feature label query q, can be computedas EU(q) = ?Kk=1 P (q = ck)U(q=ck)?q ; where ?q iscost of the query q, and utility U can be computed asin Eq.
3.
By evaluating instances and features on thesame scale, and by measuring utility per unit cost ofacquisition, such a framework should enable us tohandle the trade-off between the costs and benefitsof the different types of acquisitions.
The primarychallenge in the success of this approach is to accu-rately and efficiently estimate the different quantitiesin the equation above, using only the training datacurrently available.
These are directions for futureexploration.7 ConclusionsThis paper is a preliminary foray into active dual su-pervision.
We have demonstrated that not only iscombining example and feature labels beneficial formodeling, but that actively selecting the most infor-mative examples and features for labeling can sig-nificantly reduce the burden of annotating such data.In future work, we would like to explore more effec-tive solutions to the problem, and also to corroborateour results on a larger number of datasets and underdifferent experimental settings.56ReferencesR.
T. Clemen and R. L. Winkler.
1999.
Combining prob-ability distributions from experts in risk analysis.
RiskAnalysis, 19:187?203.D.
Cohn, L. Atlas, and R. Ladner.
1994.
Improving gen-eralization with active learning.
Machine Learning,15(2):201?221.Aynur Dayanik, David D. Lewis, David Madigan,Vladimir Menkov, and Alexander Genkin.
2006.
Con-structing informative prior distributions from domainknowledge in text classification.
In SIGIR.G.
Druck, G. Mann, and A. McCallum.
2008.
Learn-ing from labeled features using generalized expecta-tion criteria.
In SIGIR.S.
Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.2004.
Document classification through interactive su-pervision of document and term labels.
In PKDD.David D. Lewis and Jason Catlett.
1994.
Heteroge-neous uncertainty sampling for supervised learning.
InProc.
of 11th Intl.
Conf.
on Machine Learning (ICML-94), pages 148?156, San Francisco, CA, July.
MorganKaufmann.Bing Liu, Xiaoli Li, Wee Sun Lee, and Philip Yu.
2004.Text classification by labeling words.
In AAAI.Dan Lizotte, Omid Madani, and Russell Greiner.
2003.Budgeted learning of naive-Bayes classifiers.
In UAI.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for naive Bayes text classifi-cation.
In AAAI Workshop on Text Categorization.Prem Melville and Raymond J. Mooney.
2004.
Diverseensembles for active learning.
In Proc.
of 21st Intl.Conf.
on Machine Learning (ICML-2004), pages 584?591, Banff, Canada, July.Prem Melville, Maytal Saar-Tsechansky, Foster Provost,and Raymond Mooney.
2005.
An expected utility ap-proach to active feature-value acquisition.
In ICDM.Prem Melville, Wojciech Gryc, and Richard Lawrence.2009.
Sentiment analysis of blogs by combining lexi-cal knowledge with text classification.
In KDD.Bo Pang and Lilian Lee.
2008.
Opinion mining and sen-timent analysis.
Foundations and Trends in Informa-tion Retrieval: Vol.
2: No 1, pp 1-135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
sentiment classification using ma-chine learning techniques.
In EMNLP.Foster Provost, Prem Melville, and Maytal Saar-Tsechansky.
2007.
Data acquisition and cost-effectivepredictive modeling: Targeting offers for electroniccommerce.
In ICEC ?07: Proceedings of the ninth in-ternational conference on Electronic commerce.Hema Raghavan, Omid Madani, and Rosie Jones.
2006.Active learning with feedback on features and in-stances.
J. Mach.
Learn.
Res., 7:1655?1686.H.
Raghavan, O. Madani, and R. Jones.
2007.
An inter-active algorithm for asking and incorporating featurefeedback into support vector machines.
In SIGIR.Nicholas Roy and Andrew McCallum.
2001.
Towardoptimal active learning through sampling estimation oferror reduction.
In ICML.Maytal Saar-Tsechansky, Prem Melville, and FosterProvost.
2008.
Active feature-value acquisition.
InManagement Science.Robert E. Schapire, Marie Rochery, Mazin G. Rahim, andNarendra Gupta.
2002.
Incorporating prior knowl-edge into boosting.
In ICML.Vikas Sindhwani and Prem Melville.
2008.
Document-word co-regularization for semi-supervised sentimentanalysis.
In ICDM.Vikas Sindhwani, Jianying Hu, and Alexandra Mo-jsilovic.
2008.
Regularized co-clustering with dualsupervision.
In NIPS.Vikas Sindhwani, Prem Melville, and Richard Lawrence.2009.
Uncertainty sampling and transductive experi-mental design for active dual supervision.
In ICML.Simon Tong and Daphne Koller.
2000.
Support vec-tor machine active learning with applications to textclassification.
In Proc.
of 17th Intl.
Conf.
on MachineLearning (ICML-2000).Xiaoyun Wu and Rohini Srihari.
2004.
Incorporatingprior knowledge with weighted margin support vectormachines.
In KDD.O.
F. Zaidan and J. Eisner.
2008.
Modeling annotators:A generative approach to learning from annotator ra-tionales.
In EMNLP.57
