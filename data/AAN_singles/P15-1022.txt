Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 219?228,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsOnline Multitask Learning for Machine Translation Quality EstimationJos?e G. C. de Souza(1,2), Matteo Negri(1), Elisa Ricci(1), Marco Turchi(1)(1)FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy(2)University of Trento, Italy{desouza,negri,eliricci,turchi}@fbk.euAbstractWe present a method for predicting ma-chine translation output quality geared tothe needs of computer-assisted translation.These include the capability to: i) con-tinuously learn and self-adapt to a streamof data coming from multiple translationjobs, ii) react to data diversity by ex-ploiting human feedback, and iii) leveragedata similarity by learning and transferringknowledge across domains.
To achievethese goals, we combine two supervisedmachine learning paradigms, online andmultitask learning, adapting and unifyingthem in a single framework.
We showthe effectiveness of our approach in a re-gression task (HTER prediction), in whichonline multitask learning outperforms thecompetitive online single-task and poolingmethods used for comparison.
This in-dicates the feasibility of integrating in aCAT tool a single QE component capa-ble to simultaneously serve (and continu-ously learn from) multiple translation jobsinvolving different domains and users.1 IntroductionEven if not perfect, machine translation (MT) isnow getting reliable enough to support and speed-up human translation.
Thanks to this progress,the work of professional translators is graduallyshifting from full translation from scratch to MTpost-editing.
Advanced computer-assisted trans-lation (CAT) tools1provide a natural frameworkfor this activity by proposing, for each segment ina source document, one or more suggestions ob-tained either from a translation memory (TM) orfrom an MT engine.
In both cases, accurate mech-anisms to indicate the reliability of a suggestion1See for instance the open source MateCat tool (Federicoet al, 2014).are extremely useful to let the user decide whetherto post-edit a given suggestion or ignore it andtranslate the source segment from scratch.
How-ever, while scoring TM matches relies on standardmethods based on fuzzy matching, predicting thequality of MT suggestions at run-time and withoutreferences is still an open issue.This is the goal of MT quality estimation (QE),which aims to predict the quality of an automatictranslation as a function of the estimated numberof editing operations or the time required for man-ual correction (Specia et al, 2009; Soricut andEchihabi, 2010; Bach et al, 2011; Mehdad et al,2012).
So far, QE has been mainly approachedin controlled settings where homogeneous train-ing and test data is used to learn and evaluate staticpredictors.
Cast in this way, however, it does notfully reflect (nor exploit) the working conditionsposed by the CAT framework, in which:1.
The QE module is exposed to a continuousstream of data.
The amount of such data andthe tight schedule of multiple, simultaneoustranslation jobs prevents from (theoreticallyfeasible but impractical) complete re-trainingprocedures in a batch fashion and advocatefor continuous learning methods.2.
The input data can be diverse in nature.
Con-tinuous learning should be sensitive to suchdifferences, in a way that each translation joband user is supported by a reactive model thatis robust to variable working conditions.3.
The input data can show similarities withprevious observations.
Continuous learningshould leverage such similarities, so that QEcan capitalize from all the previously pro-cessed segments even if they come from dif-ferent domains, genres or users.While previous QE research disregarded thesechallenges or addressed them in isolation, our219work tackles them in a single unifying frameworkbased on the combination of two paradigms: on-line and multitask learning.
The former providescontinuous learning capabilities that allow the QEmodel to be robust and self-adapt to a stream ofpotentially diverse data.
The latter provides themodel with the capability to exploit the similari-ties between data coming from different sources.Along this direction our contributions are:?
The first application of online multitasklearning to QE, geared to the challengesposed by CAT technology.
In this framework,our models are trained to predict MT qualityin terms of HTER (Snover et al, 2006).2?
The extension of current online multitasklearning methods to regression.
Prior worksin the machine learning field applied thisparadigm to classification problems, but itsuse for HTER estimation requires real-valuedpredictions.
To this aim, we propose a newregression algorithm that, at the same time,handles positive and negative transfer andperforms online weight updates.?
A comparison between online multitask andalternative, state-of-the-art online learningstrategies.
Our experiments, carried out in arealistic scenario involving a stream of datafrom four domains, lead to consistent resultsthat prove the effectiveness of our approach.2 Related WorkIn recent years, sentence-level QE has beenmainly investigated in controlled evaluation sce-narios such as those proposed by the shared tasksorganized within the WMT workshop on SMT(Callison-Burch et al, 2012; Bojar et al, 2013;Bojar et al, 2014).
In this framework, systemstrained from a collection of (source, target, label)instances are evaluated based on their capabilityto predict the correct label3for new, unseen testitems.
Compared to our application scenario, theshared tasks setting differs in two main aspects.2The HTER is the minimum edit distance between a trans-lation suggestion and its manually post-edited version in the[0,1] interval.
Edit distance is calculated as the number ofedits (word insertions, deletions, substitutions, and shifts) di-vided by the number of words in the reference.3Possible label types include post-editing effort scores(e.g.
1-5 Likert scores indicating the estimated percentageof MT output that has to be corrected), HTER values, andpost-editing time (e.g.
seconds per word).First, the data used are substantially homogeneous(usually they come from the same domain, and tar-get translations are produced by the same MT sys-tem).
Second, training and test are carried out asdistinct, sequential phases.
Instead, in the CAT en-vironment, a QE component should ideally serve,adapt to and continuously learn from simultaneoustranslation jobs involving different MT engines,domains, genres and users (Turchi et al, 2013).These challenges have been separately ad-dressed from different perspectives in few recentworks.
Huang et al (2014) proposed a methodto adaptively train a QE model for document-specific MT post-editing.
Adaptability, however,is achieved in a batch fashion, by re-training an adhoc QE component for each document to be trans-lated.
The adaptive approach proposed by Turchiet al (2014) overcomes the limitations of batchmethods by applying an online learning protocolto continuously learn from a stream of (potentiallyheterogeneous) data.
Experimental results suggestthe effectiveness of online learning as a way to ex-ploit user feedback to tailor QE predictions to theirquality standards and to cope with the heterogene-ity of data coming from different domains.
How-ever, though robust to user and domain changes,the method is solely driven by the distance com-puted between predicted and true labels, and itdoes not exploit any notion of similarity betweentasks (e.g.
domains, users, MT engines).On the other way round, task relatedness is suc-cessfully exploited by Cohn and Specia (2013),who apply multitask learning to jointly learn fromdata obtained from several annotators with differ-ent levels of expertise and reliability.
A similar ap-proach is adopted by de Souza et al (2014a), whoapply multitask learning to cope with situations inwhich a QE model has to be trained with scarcedata from multiple domains/genres, different fromthe actual test domain.
The two methods signifi-cantly outperform both individual single-task (in-domain) models and single pooled models.
How-ever, operating in batch learning mode, none ofthem provides the continuous learning capabilitiesdesirable in the CAT framework.The idea that online and multitask learning cancomplement each other if combined is suggestedby (de Souza et al, 2014b), who compared the twolearning paradigms in the same experimental set-ting.
So far, however, empirical evidence of thiscomplementarity is still lacking.2203 Online Multitask Learning for QEOnline learning takes place in a stepwise fash-ion.
At each step, the learner processes an instance(in our case a feature vector extracted from sourceand target sentences) and predicts a label for it (inour case an HTER value).
After the prediction, thelearner receives the ?true?
label (in our case the ac-tual HTER computed from a human post-edition)and computes a loss that indicates the distance be-tween the predicted and the true label.
Before go-ing to the next step, the weights are updated ac-cording to the suffered loss.Multitask learning (MTL) aims to simultane-ously learn models for a set of possibly relatedtasks by exploiting their relationships.
By do-ing this, improved generalization capabilities areobtained over models trained on the differenttasks in isolation (single-task learning ?
STL).The relationships among tasks are provided by ashared structure, which can encode three typesof relationships based on their correlation (Zhangand Yeung, 2010).
Positive correlation indicatesthat the tasks are related and knowledge transfershould lead to similar model parameters.
Negativecorrelation indicates that the tasks are likely to beunrelated and knowledge transfer should force anincrease in the distance between model parame-ters.
No correlation indicates that the tasks are in-dependent and no knowledge transfer should takeplace.
In our case, a task is a set of (instance, la-bel) pairs obtained from source sentences comingfrom different translation jobs, together with theirtranslations produced by several MT systems andthe relative post-editions from various translators.In this paper the terms task and domain are usedinterchangeably.Early MTL methods model only positive cor-relation (Caruana, 1997; Argyriou et al, 2008),which results in a positive knowledge transfer be-tween all the tasks, with the risk of impairing eachother?s performance when they are unrelated ornegatively correlated.
Other methods (Jacob etal., 2009; Zhong and Kwok, 2012; Yan et al,2014) cluster tasks into different groups and shareknowledge only among those in the same cluster,thus implicitly identifying outlier tasks.
A thirdclass of algorithms considers all the three types ofrelationships by learning task interaction via thecovariance of task-specific weights (Bonilla et al,2008; Zhang and Yeung, 2010).
All these meth-ods, however, learn the task relationships in batchmode.
To overcome this limitation, recent workspropose the ?lifelong learning?
paradigm (Eatonand Ruvolo, 2013; Ruvolo and Eaton, 2014), inwhich all the instances of a task are given tothe learner sequentially and the previously learnedtasks are leveraged to improve generalization forfuture tasks.
This approach, however, is not ap-plicable to our scenario as it assumes that all theinstances of each task are processed as separateblocks.In this paper we propose a novel MTL algorithmfor QE that learns the structure shared by differ-ent tasks in an online fashion and from an inputstream of instances from all the tasks.
To this aim,we extend the online passive aggressive (PA) al-gorithm (Crammer et al, 2006) to the multitaskscenario, learning a set of task-specific regressionmodels.
The multitask component of our methodis given by an ?interaction matrix?
that defines towhich extent each encoded task can ?borrow?
and?lend?
knowledge from and to the other tasks.
Op-posite to previous methods (Cavallanti et al, 2010)that assume fixed dependencies among tasks, wepropose to learn the interaction matrix instance-by-instance from the data.
To this aim we followthe recent work of Saha et al (2011), extending itto a regression setting.
The choice of PA is mo-tivated by practical reasons.
Indeed, by provid-ing the best trade-off between accuracy and com-putational time (He and Wang, 2012) comparedto other algorithms such as OnlineSVR (Parrella,2007), it represents a good solution to meet the de-mand of efficiency posed by the CAT framework.3.1 Passive Aggressive AlgorithmPA follows the typical online learning proto-col. At each round t the learner receives an in-stance, xt?
Rd(d is the number of features),and predicts the label y?taccording to a functionparametrized by a set weights wt?
Rd.
Next,the learner receives the true label yt, computes the-insensitive loss, `, measuring the deviation be-tween the prediction y?tand the true label ytandupdates the weights.
The weights are updated bysolving the optimization problem:wt= argminwCPA(w) + C?
(1)s.t.
`(w, (xt, yt)) ?
?
and ?
?
0where CPA(w) =12||w ?
wt?1||2and `is the-insensitive hinge loss defined as:221`(w, (x, y)) ={0, if |y ?w ?
x| ?
|y ?w ?
x| ?
, otherwise(2)The loss is zero when the absolute difference be-tween the prediction and the true label is smalleror equal to , and grows linearly with this differ-ence otherwise.
The  parameter is given as inputand regulates the sensitivity to mistakes.
The slackvariable ?
acts as an upper-bound to the loss, whilethe C parameter is introduced to control the ag-gressiveness of the weights update.
High C valueslead to more aggressive weight updates.
However,when the labels present some degree of noise (acommon situation in MT QE), they might causethe learner to drastically change the weight vectorin a wrong direction.
In these situations, setting Cto small values is desirable.
As shown in (Cram-mer et al, 2006), a closed form solution for theweights update in Eq.1 can be derived as:wt= wt?1+ sgn(yt?
y?t)?txt(3)with ?t= min(C,`t||xt||2) and `t= `(w, (xt, yt)).3.2 Passive Aggressive MTL AlgorithmOur Passive Aggressive Multitask Learning(PAMTL) algorithm extends the traditional PA forregression to multitask learning.
Our approach isinspired by the Online Task Relationship Learningalgorithm proposed by Saha et al (2011) which,however, is only defined for classification.The learning process considers one instance ateach round t. The random sequence of instancesbelongs to a fixed set ofK tasks and the goal of thealgorithm is to learnK linear models, one for eachtask, parametrized by weight vectors?wt,k, k ?
{1, .
.
.
,K}.
Moreover, the algorithm also learnsa positive semidefinite matrix ?
?
RK?K, mod-eling the relationship among tasks.
Algorithm 1summarizes our approach.
At each round t, thelearner receives a pair (xt, it) where xt?
Rdis aninstance and it?
{1, .
.
.
,K} is the task identifier.Each incoming instance is transformed to a com-pound vector ?t= [0, .
.
.
, 0,xt, 0, .
.
.
, 0] ?
RKd.Then, the algorithm predicts the HTER score cor-responding to the label y?
by using the weight vec-tor?wt.
The weight vector is a compound vector?wt= [?wt,1, .
.
.
,?wt,K] ?
RKd, where?wt,k?Rd, k ?
{1, .
.
.
,K}.
Next, the learner receivesthe true HTER label y and computes the loss `(Eq.
2) for round t.Algorithm 1 PA Multitask Learning (PAMTL)Input: instances from K tasks, number of rounds R > 0, > 0, C > 0Output: w and ?, learned after T roundsInitialization: ?
=1K?
Ik, w = 0for t = 1 to T doreceive instance (xt, it)compute ?tfrom xtpredict HTER y?t= (w?Tt?
?t)receive true HTER label ytcompute `t(Eq.
2)compute ?t= min(C,`t||?t||2)/*update weights*/w?t= w?t?1+ sgn(yt?
y?t)?t(?t?1?
Id)?1?t/*update task matrix*/if t > R thenupdate ?twith Eq.
6 or Eq.
7end ifend forWe propose to update the weights by solving:w?t,?t= argminw,?
0CMTL(w,?)
+ C?
+D(?,?t?1)s.t.
`(w, (xt, yt)) ?
?, ?
?
0 (4)The first term models the joint dependenciesbetween the task weights and the interactionmatrix and it is defined as CMTL(w,?)
=12(w ??wt)T??
(w ?
?wt), where ?
?= ?
?Id.
The function D(?)
represents the diver-gence between a pair of positive definite matri-ces.
Similar to (Saha et al, 2011), to defineD(?)
we also consider the family of Bregman di-vergences and specifically the LogDet and theVon Neumann divergences.
Given two matri-ces X,Y ?
Rn?n, the LogDet divergence isDLD(X,Y) = tr(XY?1) ?
log |XY?1| ?
n,while the Von Neumann divergence is computedasDV N(X,Y) = tr(X logX?Y logY?X+Y).The optimization process to solve Eq.4 is per-formed with an alternate scheme: first, with afixed ?, we compute w; then, given w we opti-mize for ?.
The closed-form solution for updatingw, which we derived similarly to the PA update(Crammer et al, 2006), becomes:w?t= w?t?1+ sgn(yt?
y?t)?t(?t?1?
Id)?1?t(5)In practice, the interaction matrix works as a learn-ing rate when updating the weights of each task.Similarly, following previous works (Tsuda et al,2005), the update steps for the interaction matrix?
can be easily derived.
For the Log-Det diver-gence we have:?t= (?t?1+ ?
sym(W?Tt?1W?t?1))?1(6)222while for the Von Neumann we obtain:?t= exp(log?t?1?
?
sym(W?Tt?1W?t?1)) (7)where?Wt?
Rd?Kis a matrix obtained bycolumn-wise reshaping the weight vector?wt,sym(X) = (X + XT)/2 and ?
is the learningrate parameter.
The sequence of steps to compute?tand?wtis summarized in Algorithm 1.
Impor-tantly, the weight vector is updated at each roundt, while ?tis initialized to a diagonal matrix andit is only computed after R iterations.
In this way,at the beginning, the tasks are assumed to be in-dependent and the task-specific regression mod-els are learned in isolation.
Then, after R rounds,the interaction matrix is updated and the weightsare refined considering tasks dependencies.
Thisleads to a progressive increase in the correlationof weight vectors of related tasks.
In the follow-ing, PAMTLvnrefers to PAMTL with the VonNeumann updates and PAMTLldto PAMTL withLogDet updates.4 Experimental SettingIn this section, we describe the data used in our ex-periments, the features extracted from the sourceand target sentences, the evaluation metric and thebaselines used for comparison.Data.
We experiment with English-Frenchdatasets coming from Technology EntertainmentDesign talks (TED), Information Technologymanuals (IT) and Education Material (EM).
Alldatasets provide a set of tuples composed by(source, translation and post-edited translation).The TED dataset is distributed in the Trace cor-pus4and includes, as source sentences, the sub-titles of several talks spanning a range of topicspresented in the TED conferences.
Translationswere generated by two different MT systems: aphrase-based statistical MT system and a commer-cial rule-based system.
Post-editions were col-lected from four different translators, as describedby Wisniewski et al (2013).The IT manuals data come from two languageservice providers, henceforth LSP1 and LSP2.The ITLSP1tuples belong to a software manualtranslated by an SMT system trained using theMoses toolkit (Koehn et al, 2007).
The post-editions were produced by one professional trans-4http://anrtrace.limsi.fr/trace_postedit.tar.bz2Domain No.
Vocab.
Avg.
Snt.tokens Size LengthTED src 20,048 3,452 20TED tgt 21,565 3,940 22ITLSP1src 12,791 2,013 13ITLSP1tgt 13,626 2,321 13EM src 15,327 3,200 15EM tgt 17,857 3,149 17ITLSP2src 15,128 2,105 13ITLSP2tgt 17,109 2,104 14Table 1: Data statistics for each domain.lator.
The ITLSP2data includes a software man-ual from the automotive industry; its source sen-tences are translated with an adaptive proprietaryMT system and post-edited by several profes-sional translators.
The EM corpus is also pro-vided by LSP2 and regards educational material(e.g.
courseware and assessments) of various textstyles.
The translations and post-editions are pro-duced in the same way as for ITLSP2.
The ITLSP2and the EM datasets are derived from the Au-todesk Post-Editing Data corpus.5In total, we end up with four domains (TED,ITLSP1, EM and ITLSP2), which allows us to eval-uate the PAMTL algorithm in realistic conditionswhere the QE component is exposed to a contin-uous stream of heterogeneous data.
Each domainis composed by 1,000 tuples formed by: i) the En-glish source sentence, ii) its automatic translationin French, and iii) a real-valued quality label ob-tained by computing the HTER between the trans-lation and the post-edition with the TERCpp opensource tool.6Table 1 reports some macro-indicators (num-ber of tokens, vocabulary size, average sentencelength) that give an idea about the similarities anddifferences between domains.
Although they con-tain data from different software manuals, similarvocabulary size and sentence lengths for the twoIT domains seem to reflect some commonalities intheir technical style and jargon.
Larger values forTED and EM evidence a higher lexical variabilityin the topics that compose these domains and theexpected stylistic differences featured by speechtranscriptions and non-technical writing.
Over-all, these numbers suggest a possible dissimilar-5https://autodesk.app.box.com/Autodesk-PostEditing6http://sourceforge.net/projects/tercpp/223Figure 1: Validation curves for the R parameter.ity between ITLSP1and ITLSP2and the other twodomains, which might make knowledge transferacross them more difficult and QE model reactiv-ity to domain changes particularly important.Features.
Our models are trained using the 17baseline features proposed in (Specia et al, 2009),extracted with the online version of the QuEst fea-ture extractor (Shah et al, 2014).
These featurestake into account the complexity of the source sen-tence (e.g.
number of tokens, number of transla-tions per source word) and the fluency of the trans-lation (e.g.
language model probabilities).
Theirdescription is available in (Callison-Burch et al,2012).
The results of previous WMT QE sharedtasks have shown that these features are particu-larly competitive in the HTER prediction task.Baselines.
We compare the performance ofPAMTL against three baselines: i) pooling mean,ii) pooling online single task learning (STLpool)and iii) in-domain online single task learning(STLin).
The pooling mean is obtained by assign-ing a fixed prediction value to each test point.
Thisvalue is the average HTER computed on the entirepool of training data.
Although assigning the sameprediction to each test instance would be uselessin real applications, we compare against the meanbaseline since it is often hard to beat in regressiontasks, especially when dealing with heterogeneousdata distributions (Rubino et al, 2013).The two online single task baselines implementthe PA algorithm described in Section 3.1.
Thechoice of PA is to make them comparable to ourmethod, so that we can isolate more precisely thecontribution of multitask learning.
STLpoolresultsare obtained by a single model trained on the entireFigure 2: Learning curves for all the domains,computed by calculating the mean MAE (?)
of thefour domains.pool of available training data presented in randomorder.
STLinresults are obtained by separatelytraining one model for each domain.
These repre-sent two alternative strategies for the integration ofQE in the CAT framework.
The former would al-low a single model to simultaneously support mul-tiple translation jobs in different domains, withoutany notion about their relations.
The latter wouldlead to a more complex architecture, organized asa pool of independent, specialized QE modules.Evaluation metric.
The performance of our re-gression models is evaluated in terms of mean ab-solute error (MAE), a standard error measure forregression problems commonly used also for QE(Callison-Burch et al, 2012).
The MAE is the av-erage of the absolute errors ei= |y?i?
yi|, wherey?iis the prediction of the model and yiis the truevalue for the ithinstance.
As it is an error mea-sure, lower values indicate better performance (?
).5 Results and DiscussionIn this Section we evaluate the proposed PAMTLalgorithm.
First, by analyzing how the number ofrounds R impacts on the performance of our ap-proach, we empirically find the value that will beused to train the model.
Then, the learned modelis run on test data and compared against the base-lines.
Performance is analyzed both by averag-ing the MAE results computed on all the domains,and by separately discussing in-domain behavior.Finally, the capability of the algorithm to learntask correlations and, in turn, transfer knowledgeacross them, is analysed by presenting the correla-224Figure 3: Learning curves showing MAE (?)
variations for each domain.tion matrix of the task weights.For the evaluation, we uniformly sample 700 in-stances from each domain for training, leaving theremaining 300 instances for test.
The training setsof all the domains are concatenated and shuffledto create a random sequence of points.
To inves-tigate the impact of different amounts of data onthe learning process, we create ten subsets of 10to 100% of the training data.
We optimize the pa-rameters of all the models with a grid search pro-cedure using 5-fold cross-validation.
This processis repeated for 30 different train/test splits over thewhole data.
Results are presented with 95% confi-dence bands.7Analysis of the R parameter.
We empiricallystudy the influence of the number of instances re-quired to start updating the interaction matrix (theR parameter in Algorithm 1).
For that, we per-form a set of experiments where R is initializedwith nine different values (expressed as percent-age of training data).
Figure 1 shows the val-idation curves obtained in cross-validation overthe training data using the LogDet and Von Neu-mann updates.
The curves report the performance(MAE) difference between STLinand PAMTLld7Confidence bands are used to show whether performancedifferences between the models are statistically significant.
(black curve) and STLinand PAMTLvn(greycurve).
The higher the difference, the better.
ThePAMTLvncurve differs from PAMTLldone onlyfor small values ofR (< 20), showing that the twodivergences are substantially equivalent.
It is in-teresting to note that with only 20% of the trainingdata (R = 20), PAMTL is able to find a stableset of weights and to effectively update the inter-action matrix.
Larger values of R harm the perfor-mance, indicating that the interaction matrix up-dates require a reasonable amount of points to reli-ably transfer knowledge across tasks.
We use thisobservation to set R for our final experiment, inwhich we evaluate the methods over the test data.Evaluation on test data.
Global evaluation re-sults are summarized in Figure 2, which showsfive curves: one for each baseline (Mean, STLin,STLpool) and two for the proposed online mul-titask method (PAMTLvnand PAMTLld).
Thecurves are computed by calculating the averageMAE achieved with different amounts of data oneach domain?s test set.The results show that PAMTLldand PAMTLvnhave similar trends (confirming the substantialequivalence previously observed), and that bothoutperform all the baselines in a statistically sig-nificant manner.
This holds for all the training set225sizes we experimented with.
The maximum im-provement over the baselines (+1.3 MAE) is ob-served with 60% of the training data when com-paring PAMTLvnwith STLin.
Even if this is thebest baseline, also with 100% of the data its resultsare not competitive and of limited interest with re-spect to our application scenario (the integration ofeffective QE models in the CAT framework).
In-deed, despite the STLindownward error trend, it?sworth remarking that an increased competitive-ness would come at the cost of: i) collecting largeamounts of annotated data and ii) integrating themodel in a complex CAT architecture organizedas a pool of independent QE components.
Underthe tested conditions, it is also evident that the al-ternative strategy of using a single QE componentto simultaneously serve multiple translation jobs isnot viable.
Indeed, STLpoolis the worst perform-ing baseline, with a constant distance of around 2MAE points from the best PAMTL model for al-most all the training set sizes.
The fact that, withincreasing amounts of data, the STLpoolpredic-tions get close to those of the simple mean base-line indicates its limitations to cope with the noiseintroduced by a continuous stream of diverse data.The capability to handle such stream by exploit-ing task relationships makes PAMTL a much bet-ter solution for our purposes.Per-domain analysis.
Figure 3 shows the MAEresults achieved on each target domain by the mostcompetitive baseline (STLin) and the proposed on-line multitask method (PAMTLvn, PAMTLld).For all the domains, the behavior of PAMTLldand PAMTLvnis consistent and almost identi-cal.
With both divergences, the improvement ofPAMTL over online single task learning becomesstatistically significant when using more than 30%of the training data (210 instances).
Interestingly,in all the plots, with 20% of the training data(140 instances for each domain, i.e.
a total of560 instances adding data from all the domains),PATML results are comparable to those achievedby STLinwith 80% of the training data (i.e.
560in-domain instances).
This confirms that PATMLcan effectively leverage data heterogeneity, andthat a limited amount of in-domain data is suf-ficient to make it competitive.
Nevertheless, forall domains except EM, the PATML and STLincurves converge to comparable performance whentrained with 100% of the data.
This is not surpris-ing if we consider that EM has a varied vocabularyFigure 4: Correlation among the weights predictedby PATMLvnusing all the training data.
(see Table 1), which may be evidence of the pres-ence of different topics, increasing its similaritywith other domains.
The same assumption shouldalso hold for TED, given that its source sentencesbelong to talks about different topics.
The resultsfor the TED domain, however, do not present thesame degree of improvement as for EM.To better understand the relationships learnedby the PAMTL models, we compute the corre-lation between the weights inferred for each do-main (as performed by Saha et al (2011)).
Fig-ure 4 shows the correlations computed on the taskweights learned by PATMLvnwith all the train-ing data.
In the matrix, EM is the domain thatpresents the highest correlation with all the others.Instead, TED and ITLSP2are the less correlatedwith the other domains (even though, being closeto the other IT domain, ITLSP2can share knowl-edge with it).
This explains why the improvementmeasured on TED is smaller compared to EM.
Al-though there is no canonical way to measure cor-relation among domains, the weights correlationmatrix and the improvements achieved by PAMTLshow the capability of the method to identify taskrelationships and exploit them to improve the gen-eralization properties of the model.6 ConclusionWe addressed the problem of developing qual-ity estimation models suitable for integration incomputer-assisted translation technology.
In thisframework, on-the-fly MT quality prediction for astream of heterogeneous data coming from differ-ent domains/users/MT systems represents a majorchallenge.
On one side, processing such streamcalls for supervised solutions that avoid the bot-226tleneck of periodically retraining the QE modelsin a batch fashion.
On the other side, handlingdata heterogeneity requires the capability to lever-age data similarities and dissimilarities.
Whileprevious works addressed these two problems inisolation, by proposing approaches respectivelybased on online and multitask learning, our so-lution unifies the two paradigms in a single on-line multitask approach.
To this aim, we devel-oped a novel regression algorithm, filling a gapleft by current online multitask learning methodsthat only operate in classification mode.
Our ap-proach, which is based on the passive aggressivealgorithm, has been successfully evaluated againststrong online single-task competitors in a scenarioinvolving four domains.
Our future objective isto extend our evaluation to streams of data com-ing from a larger number of domains.
Findingreasonably-sized datasets for this purpose is cur-rently difficult.
However, we are confident that thegradual shift of the translation industry towardshuman MT post-editing will not only push for fur-ther research on these problems, but also providedata for larger scale evaluations in a short time.To allow for replicability of our results andpromote further research on QE, the features ex-tracted from our data, the computed labels andthe source code of the method are available athttps://github.com/jsouza/pamtl.AcknowledgementsThis work has been partially supported by the EC-funded H2020 project QT21 (grant agreement no.645452).
The authors would like to thank Dr.Ventsislav Zhechev for his support with the Au-todesk Post-Editing Data corpus.ReferencesAndreas Argyriou, Theodoros Evgeniou, and Massim-iliano Massimo Pontil.
2008.
Convex multi-taskfeature learning.
Machine Learning, 73(3):243?272, January.Nguyen Bach, F. Huang, and Y. Al-Onaizan.
2011.Goodness: A method for measuring machine trans-lation confidence.
In 49th Annual Meeting of theAssociation for Computational Linguistics.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In EighthWorkshop on Statistical Machine Translation, pages1?44, Sofia, Bulgaria, August.Ondrej Bojar, Christian Buck, Christian Federmann,Barry Haddow, Philipp Koehn, Johannes Leveling,Christof Monz, Pavel Pecina, Matt Post, HerveSaint-Amand, Radu Soricut, Lucia Specia, and Ale?sTamchyna.
2014.
Findings of the 2014 Workshopon Statistical Machine Translation.
In Proceedingsof the Ninth Workshop on Statistical Machine Trans-lation, pages 12?58, Baltimore, USA, June.Edwin Bonilla, Kian Ming Chai, and ChristopherWilliams.
2008.
Multi-task Gaussian Process Pre-diction.
In Advances in Neural Information Process-ing Systems 20: NIPS?08.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the 7th Work-shop on Statistical Machine Translation, pages 10?51, Montr?eal, Canada, June.Rich Caruana.
1997.
Multitask learning.
In MachineLearning, pages 41?75.Giovanni Cavallanti, N Cesa-Bianchi, and C Gentile.2010.
Linear algorithms for online multitask clas-sification.
The Journal of Machine Learning Re-search, 11:2901?2934.Trevor Cohn and Lucia Specia.
2013.
ModellingAnnotator Bias with Multi-task Gaussian Processes:An application to Machine Translation Quality Es-timation.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguis-tics, pages 32?42, Sofia, Bulgaria, August.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
OnlinePassive-Aggressive Algorithms.
The Journal of Ma-chine Learning Research, 7:551?585.Jos?e G. C. de Souza, Marco Turchi, and Matteo Ne-gri.
2014a.
Machine Translation Quality Estima-tion Across Domains.
In Proceedings of COLING2014, the 25th International Conference on Compu-tational Linguistics: Technical Papers, pages 409?420, Dublin, Ireland, August.Jos?e G. C. de Souza, Marco Turchi, and Matteo Negri.2014b.
Towards a Combination of Online and Mul-titask Learning for MT Quality Estimation: a Pre-liminary Study.
In Proceedings of Workshop on In-teractive and Adaptive Machine Translation in 2014(IAMT 2014), Vancouver, BC, Canada, October.Eric Eaton and PL Ruvolo.
2013.
ELLA: An efficientlifelong learning algorithm.
In Proceedings of the30th International Conference on Machine Learn-ing, pages 507?515, Atlanta, Georgia, USA, June.Marcello Federico, Nicola Bertoldi, Mauro Cettolo,Matteo Negri, Marco Turchi, Marco Trombetti,Alessandro Cattelan, Antonio Farina, Domenico227Lupinetti, Andrea Martines, Alberto Massidda, Hol-ger Schwenk, Lo?
?c Barrault, Frederic Blain, PhilippKoehn, Christian Buck, and Ulrich Germann.
2014.THE MATECAT TOOL.
In Proceedings of COL-ING 2014, the 25th International Conference onComputational Linguistics: System Demonstrations,pages 129?132, Dublin, Ireland, August.Fei Huang, Jian-Ming Xu, Abraham Ittycheriah, andSalim Roukos.
2014.
Adaptive HTER Estimationfor Document-Specific MT Post-Editing.
In Pro-ceedings of the 52nd Annual Meeting of the Asso-ciation for Computational Linguistics (Volume 1:Long Papers), pages 861?870, Baltimore, Maryland,June.Laurent Jacob, Jean-philippe Vert, Francis R Bach, andJean-philippe Vert.
2009.
Clustered Multi-TaskLearning: A Convex Formulation.
In D Koller,D Schuurmans, Y Bengio, and L Bottou, editors,Advances in Neural Information Processing Systems21, pages 745?752.
Curran Associates, Inc.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zenz, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InACL 2007 Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.Yashar Mehdad, Matteo Negri, and Marcello Fed-erico.
2012.
Match without a Referee: Eval-uating MT Adequacy without Reference Transla-tions.
In Proceedings of the Machine TranslationWorkshop (WMT2012), pages 171?180, Montr?eal,Canada, June.Francesco Parrella.
2007.
Online support vector re-gression.
Master?s Thesis, Department of Informa-tion Science, University of Genoa, Italy.Raphael Rubino, Jos?e G. C. de Souza, and Lucia Spe-cia.
2013.
Topic Models for Translation QualityEstimation for Gisting Purposes.
In Machine Trans-lation Summit XIV, pages 295?302.Paul Ruvolo and Eric Eaton.
2014.
Online Multi-TaskLearning via Sparse Dictionary Optimization.
InProceedings of the 28th AAAI Conference on Arti-ficial Intelligence (AAAI-14), Qu?ebec City, Qu?ebec,Canada, July.Avishek Saha, Piyush Rai, Hal Daum?e, and SureshVenkatasubramanian.
2011.
Online Learning ofMultiple Tasks and their Relationships.
In Proceed-ings of the 14th International Conference on Ar-tificial Intelligence and Statistics (AISTATS), FortLauderdale, FL, USA, April.Kashif Shah, Marco Turchi, and Lucia Specia.
2014.An Efficient and User-friendly Tool for MachineTranslation Quality Estimation.
In Proceedings ofthe Ninth International Conference on Language Re-sources and Evaluation, Reykjavik, Iceland, May.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Studyof Translation Edit Rate with Targeted Human An-notation.
In Association for Machine Translation inthe Americas, Cambridge, MA, USA, August.Radu Soricut and A Echihabi.
2010.
Trustrank: In-ducing trust in automatic translations via ranking.
InProceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics, number July,pages 612?621.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Estimat-ing the Sentence-Level Quality of Machine Transla-tion Systems.
In Proceedings of the 13th AnnualConference of the EAMT, pages 28?35, Barcelona,Spain, May.Koji Tsuda, Gunnar R?atsch, and Manfred K Warmuth.2005.
Matrix exponentiated gradient updates for on-line learning and bregman projection.
In Journal ofMachine Learning Research, pages 995?1018.Marco Turchi, Matteo Negri, and Marcello Federico.2013.
Coping with the Subjectivity of HumanJudgements in MT Quality Estimation.
In Proceed-ings of the Eighth Workshop on Statistical MachineTranslation (WMT), pages 240?251, Sofia, Bulgaria,August.Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. deSouza, and Matteo Negri.
2014.
Adaptive Qual-ity Estimation for Machine Translation.
In Proceed-ings of the 52nd Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 710?720, Baltimore, Maryland, USA,June.Guillaume Wisniewski, Anil Kumar Singh, Natalia Se-gal, and Franc?ois Yvon.
2013.
Design and Anal-ysis of a Large Corpus of Post-Edited Translations:Quality Estimation, Failure Analysis and the Vari-ability of Post-Edition.
In Machine TranslationSummit XIV, pages 117?124.Yan Yan, Elisa Ricci, Ramanathan Subramanian,Gaowen Liu, and Nicu Sebe.
2014.
Multitask lin-ear discriminant analysis for view invariant actionrecognition.
IEEE Transactions on Image Process-ing, 23(12):5599?5611.Yu Zhang and Dit-yan Yeung.
2010.
A Convex Formu-lation for Learning Task Relationships in Multi-TaskLearning.
In Proceedings of the Twenty-Sixth Con-ference Annual Conference on Uncertainty in Artifi-cial Intelligence (UAI-10), pages 733?742, CatalinaIsland, CA, USA, July.Leon Wenliang Zhong and James T. Kwok.
2012.Convex multitask learning with flexible task clus-ters.
In Proceedings of the 29 th International Con-ference on Machine Learning, Edinburgh, Scotland,June.228
