Proceedings of the 14th European Workshop on Natural Language Generation, pages 147?151,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsAutomatic Voice Selection in Japanese based on Various LinguisticInformationRyu Iida and Takenobu TokunagaDepartment of Computer Science, Tokyo Institute of TechnologyW8-73, 2-12-1 Ookayama Meguro Tokyo, 152-8552 Japan{ryu-i,take}@cl.cs.titech.ac.jpAbstractThis paper focuses on a subtask of natu-ral language generation (NLG), voice se-lection, which decides whether a clause isrealised in the active or passive voice ac-cording to its contextual information.
Au-tomatic voice selection is essential for re-alising more sophisticated MT and sum-marisation systems, because it impactsthe readability of generated texts.
How-ever, to the best of our knowledge, theNLG community has been less concernedwith explicit voice selection.
In this pa-per, we propose an automatic voice se-lection model based on various linguisticinformation, ranging from lexical to dis-course information.
Our empirical evalua-tion using a manually annotated corpus inJapanese demonstrates that the proposedmodel achieved 0.758 in F-score, outper-forming the two baseline models.1 IntroductionGenerating a readable text is the primary goalin natural language generation (NLG).
To realisesuch text, we need to arrange discourse entities(e.g.
NPs) in appropriate positions in a sentenceaccording to their discourse salience.
Consider thetwo following Japanese texts, each of which con-sists of two sentences.
(1) Tomi-wa kouenj-ni it-ta .Tomi-TOP parkj-IOBJ go-PAST(Tomi went to a parkj .
)Karei-wa sokoj-de ookina inu-ni oikake-rareta .hei-TOP therej-LOC big dog-IOBJ chase-PASSIVE/PAST(Hei was chased by a big dog therej .
)(2) Tomi-wa kouenj-ni it-ta .Tomi-TOP parkj-IOBJ go-PAST(Tomi went to a parkj .
)Ookina inu-ga sokoj-de karei-o oikake-ta .big dog-SUBJ therej-LOC hei-OBJ chase-PAST(A big dog chased himi therej .
)In (1), ?Tomi?
is topicalised in the first sentence,and then it appears at the subject position in thesecond sentence.
In contrast, the same argu-ment, i.e.
?hei?
is realised at the object positionin the second sentence of text (2).
Intuitively,text (1) is relatively more natural than text (2).Thus, given the two predicate argument relations,go(SUBJ:Tomi, IOBJ:parkj) and chase(SUBJ:bigdog, OBJ:Tomi, IOBJ:parkj), a generation systemshould choose text (1).The realisation from a semantic representation(e.g.
predicate argument structures) to an actualtext has been mainly developed in the area of nat-ural language generation (Reiter and Dale, 2000),and has been applied to various NLP applicationssuch as multi-document summarisation (Radevand McKeown, 1998) and tutoring systems (DiEugenio et al 2005).
During the course of atext generation process, various kinds of decisionsshould be made, including decisions on textualcontent, clustering the content of each clause, dis-course structure of the clauses, lexical choices,types of referring expressions and syntactic struc-tures.
Since these different kinds of decisions areinterrelated to each other, it is not a trivial prob-lem to find an optimal order among these deci-sions.
This issue has been much discussed in termsof architecture of generation systems.
Although avariety of architectures has been proposed in thepast, e.g.
an integrated architecture (Appelt, 1985)and a revision-based architecture (Inui et al 1994;Robin, 1994), a pipeline architecture is consideredas a consensus architecture in which decisionsare made in a predetermined order (Reiter, 1994).Voice selection is a syntactic decision that tends tobe made in a later stage of the pipeline architec-ture, even though it influences various decisions,such as discourse structure and lexical choice.
Un-like referring expression generation, voice selec-tion has received less attention and been less dis-cussed in the past.
Against this background, this147research tackles the problem of voice selectionconsidering a wide range of linguistic informationthat is assumed to be already decided in the pre-ceding stages of a generation process.The paper is organised as follows.
We firstoverview the related work in Section 2, and thenpropose a voice selection model based on the fourkinds of information that impact voice selectionin Section 3.
Section 4 then demonstrates the re-sults of empirical evaluation using the NAIST TextCorpus (Iida et al 2007) as training and evalu-ation data sets.
Finally, Section 5 concludes anddiscusses our future directions.2 Related workThe task of automatic voice selection has beenmainly developed in the NLG community.
How-ever, it has attracted less attention compared withother major NLG problems, such as generating re-ferring expressions.
There is less work focusingsingly on voice selection, but not entirely with-out exception, such as Abb et al(1993).
In theirwork, passivisation is performed by taking into ac-count both linguistic and extra-linguistic informa-tion.
The linguistic information explains passivi-sation in an incremental generation process; realis-ing the most salient discourse entity in short termmemory as a subject eventually leads to passivi-sation.
In contrast, extra-linguistic information isused to move a less salient entity to a subject posi-tion when an explicit agent is missing in the text.Although these two kinds of information seem ad-equate for explaining passivisation, their applica-bility was not examined in empirical evaluations.Sheikha and Inkpen (2011) focused attention onvoice selection in the generation task distinguish-ing formal and informal sentences.
In their work,passivisation is considered as a rhetorical tech-nique for conveying formal intentions.
However,they did not discuss passivisation in terms of dis-course coherence.3 Voice selection modelWe recast the voice selection task into a binaryclassification problem, i.e.
given a predicate withits arguments and its preceding context, we clas-sify the predicate into either an active or passiveclass, taking into account predicate argument rela-tions and the preceding context of the predicate.As shown in examples (1) and (2) in Section 1,several factors have an impact on voice selectionin a text.
In this work, we take into account thefollowing four information as features The detailsof the feature set are shown in Table 1.Passivisation preference of each verb An im-portant factor of voice selection is the preferencefor how frequently a verb is used in passive sen-tences.
This means each verb has a potential ten-dency of being used in passive sentences in a do-main.
For example, the verb ?yosou-suru (to ex-pect)?
tends to be realised in the passive in thenewspaper domain because Japanese journaliststend to write their opinions objectively by omittingthe agent role.
To take into account this preferenceof verb passivisation, we define a preference scoreby the following formula:scorepas(vi) =freqpas(vi)freqall(vi)?
log freqall(vi) (1)where vi is a verb in question1, freqall(vi) isthe frequency of vi appearing in corpora, andfreqpas(vi) is the frequency of vi with the passivemarker, (ra)reru.
The logarithm of freqall(vi) ismultiplied due to avoiding the overestimation ofthe score for less frequent instances.
In the evalua-tion, the preference score was calculated based onthe frequency of each verb in the 12 years worthof newspaper articles, which had been morpho-syntactically analysed by a Japanese morpholog-ical analyser Mecab3 and a dependency parserCaboCha4.Syntactic decisions As described in Section 1,various kinds of decisions are interrelated to voiceselection.
Particularly, syntactic decisions includ-ing voice selection directly impact sentence struc-ture.
Therefore, we introduce syntactic informa-tion except for voice selection which prescribeshow an input predicate-argument structure will berealised in an actual text.Semantic category of arguments Animacy ofthe arguments of a predicate has an impact on theirsyntactic positions.
Unlike in English, inanimatesubjects tend to be avoided in Japanese.
In orderto capture this tendency, we use the semantic cate-gory of the arguments of the verb in question (e.g.1Note that the preference needs to be defined for eachword sense.
However, we here ignore the difference of sensesbecause selecting a correct verb sense for a given context isstill difficult.1Bunsetsu is a basic unit in Japanese, consisting of at leastone content word and more than zero functional words.2http://nlp.cs.nyu.edu/irex/index-e.html3https://code.google.com/p/mecab/4https://code.google.com/p/cabocha/148type feature definitionPRED scorepas passivisation preference score defined in equation (1).lexical lemma of P .func lemma of functional words following P , excluding passive markers.SYN sent end 1 if P appears in the last bunsetsu1-unit in a sentence; otherwise 0.adnom 1 if P appears in an adnominal clause; otherwise 0.first sent (last sent) 1 if P appears in the first (last) sentence of a text; otherwise 0.subj(obj,iobj) embedded 1 if the head of the adnominal clause including P is semantic subject (object, indi-rect object) of P ; otherwise 0.ARG subj(obj,iobj) ne named entity class (based on IREX2) of the subject (object, indirect object) of P .subj(obj,iobj) sem semantic class of the subject (object, indirect object) of P in terms of Japaneseontology, nihongo goi taikei (Ikehara et al 1997).COREF subj(obj,iobj) exo 1 if the subject (object, indirect object) of P is unrealised and it is annotated asexophoric; otherwise 0.subj(obj,iobj) srl order order of the subject (object, indirect object) of P in the SRL.subj(obj,iobj) srl rank rank of the subject (object, indirect object) of P in the SRL.subj(obj,iobj) coref num number of discourse entities in the coreference chain including P?s subject (object,indirect object) in the preceding context.P stands for the predicate in question.
The four feature types (PRED, SYN, ARG and COREF) correspond to each informationdescribed in Section 3.Table 1: Feature set for voice selectionnamed entity labels provided by CaboCha, such asPerson and Organisation, and the ontological in-formation defined in a Japanese ontology, nihongogoi taikei (Ikehara et al 1997)) as features.Coreference and anaphora of arguments Asdiscussed in discourse theories such as CenteringTheory (Grosz et al 1995), arguments which havebeen already most salient in the preceding contexttend to be placed at the beginning of a sentence forreducing the cognitive cost of reading, as argued inFunctional Grammar (Halliday and Matthiessen,2004).
In order to consider the characteristic, weemploy an extension of Centering Theory (Groszet al 1995), proposed by Nariyama (2002) forimplementing the COREF type features in Table 1.She proposed a generalised version of the forwardlooking-center list, called the Salient ReferenceList (SRL), which stores all salient discourse en-tities (e.g.
NP) in the preceding contexts in the or-der of their saliency.
A highly ranked argument?sentity in the SRL tends to be placed in the sub-ject position, resulting in a passive sentence if thatsalient entity has a THEME role in the predicate-argument structure.
To capture this characteristic,the order and rank of discourse entities in the SRLare used as features5.In addition, as described in Abb et al(1993),if the agent filler of a predicate is underspecified,the passive voice is preferred so as to unfocus theunderspecified agent.
Likewise, if the argument5In Table 1 ?
* srl rank?
stands for how highly the argu-ment?s referent ranked out of the discourse entities in theSRL, while ?
* srl order?
stands for which slot (e.g.
TOP slotor SUBJ slot, etc.)
stores the argument?s referent.
(in this case, the agent filler) of a predicate is ex-ophoric, the passive voice is selected.4 ExperimentsWe conducted an empirical evaluation using man-ually annotated newspaper articles in Japanese.
Toestimate the feature weights of each classifier, weused MEGAM6, an implementation of the Maxi-mum Entropy model, with default parameter set-tings.
We also used SVM7 with a polynomial ker-nel for explicitly handling the dependency of theproposed features.4.1 Data and baseline modelsFor training and evaluation, we used the NAISTText Corpus (Iida et al 2007).
Because the cor-pus contains manually annotated predicate argu-ment relations and coreference relations, we usedthose for the inputs of voice selection.
In our prob-lem setting, we conducted an intrinsic evaluation;given manually annotated predicate argument re-lations and coreference relations of arguments, amodel determines whether a predicate in questionis actually realised in the passive or active voicein the original text.
The performance is measuredbased on recall, precision and F-score of correctlydetecting passive voice.
For evaluation, we di-vided the texts in the corpus into two sets; one isused for training and the other for evaluation.
Thedetails of this division are shown in Table 2.We employed two baseline models for compar-6http://www.cs.utah.edu/?hal/megam/7http://svmlight.joachims.org/149#articles #predicates #passive predicatestraining 1,753 65,592 4,974 (7.6%)test 696 24,884 1,891 (7.6%)Table 2: Data set division for evaluationR P F?
= 0.1 0.768 0.269 0.399?
= 0.2 0.573 0.357 0.440?
= 0.3 0.403 0.450 0.425?
= 0.4 0.293 0.512 0.373?
= 0.5 0.161 0.591 0.253?
= 0.6 0.091 0.692 0.162?
= 0.7 0.060 0.717 0.111?
= 0.8 0.030 0.851 0.058?
= 0.9 0.014 1.000 0.027Table 3: Effect of threshold ?
for scorepasison.
One is based on the passivisation preferenceof each verb.
The model uses only scorepas(vi)defined in equation (1), that is, it selects the pas-sive voice if the score is more than the thresholdparameter ?
; otherwise, it selects the active voice.The other baseline model is based on the infor-mation that the existence of an exophoric subjectresults in selecting the passive voice.
To capturethis characteristic, the model classifies a verb inquestion as passive if the annotated subject is ex-ophoric; otherwise, it selects the active voice.4.2 ResultsWe first evaluated performance of the first baselinemodel with various ?.
The results are shown inTable 3, demonstrating that the baseline achievedits best F-score when ?
is 0.2.
Therefore, we setthe ?
to 0.2 in the following comparison.Table 4 shows the results of the baselines andproposed models.
To investigate the impact ofeach feature type, we conducted feature ablationwhen using the maximum entropy model (ME:* inTable 4).
Table 4 shows that the model using thefeature type PRED achieves the best performanceamong the four models when using a single featuretype.
In addition, by adding feature type(s), the F-score monotonically improves.
Finally, the resultsof the model using the PRED, ARG and COREF fea-tures achieved the best F-score, 0.605, out of thetwo baselines and models based on the maximumentropy model.
It indicates that each of the fea-tures except SYN feature contributes to improvingperformance in a complementary manner.Furthermore, the results of the model usingSVM with the second degree polynomial kernelshow better performance than any model based onmodel R P Fbaseline1: scorepas ?
0.2 0.573 0.357 0.440baseline2: exophora 0.493 0.329 0.395ME: PRED 0.270 9.612 0.374ME: SYN 0.000 N/A N/AME: ARG 0.095 0.516 0.161ME: COREF 0.092 0.574 0.159ME: PRED+SYN 0.282 0.618 0.387ME: PRED+ARG 0.380 0.647 0.479ME: PRED+COREF 0.480 0.762 0.589ME: SYN+ARG 0.133 0.558 0.215ME: SYN+COREF 0.147 9.618 9.238ME: ARG+COREF 0.267 0.661 0.380ME: PRED+SYN+ARG 0.397 0.656 0.494ME: PRED+SYN+COREF 0.485 0.760 0.592ME: PRED+ARG+COREF 0.506 0.752 0.605ME: SYN+ARG+COREF 0.281 0.673 0.397ME: ALL 0.507 0.747 0.604SVM(linear): ALL 0.456 0.792 0.579SVM(poly-2d): ALL 0.679 0.858 0.758Table 4: Results of automatic voice selectionthe maximum entropy model.
This means that thecombination of features is important in this taskbecause of the dependency among the four kindsof information introduced in Section 3.5 ConclusionThis paper focused on the task of automatic voiceselection in text generation, taking into accountfour kinds of linguistic information: passivisa-tion preference of verbs, syntactic decisions, se-mantic category of the arguments of a predicate,and coreference or anaphoric relations of the argu-ments.
For empirical evaluation of voice selectionin Japanese, we used the predicate argument re-lations and coreference relations annotated in theNAIST Text Corpus (Iida et al 2007).
Integrat-ing the four kinds of linguistic information intoa machine learning-based approach contributed toimproving F-score by about 0.3, compared to thebest baseline model, which utilises only the pas-sivisation preference.
Finally, we achieved 0.758in F-score by combining features using SVM.As future work, we are planning to incorpo-rate the proposed voice selection model into natu-ral language generation models for more sophisti-cated text generation.
In particular, generating re-ferring expressions and voice selection are closelyrelated because both tasks utilise similar linguisticinformation (e.g.
salience and semantic informa-tion of arguments) for generation.
Therefore, ournext challenge is to solve problems about gener-ating referring expressions and voice selection si-multaneously by using optimisation techniques.150ReferencesB.
Abb, M. Herweg, and K. Lebeth.
1993.
The incre-mental generation of passive sentences.
In Proceed-ings of the 6th EACL, pages 3?11.Douglas E. Appelt.
1985.
Planning English referringexpressions.
Artificial Intelligence, 26(1):1?33.Barbara Di Eugenio, Davide Fossati, Dan Yu, SusanHaller, and Michael Glass.
2005.
Natural languagegeneration for intelligent tutoring systems: A casestudy.
In Proceedings of the 2005 conference on Ar-tificial Intelligence in Education: Supporting Learn-ing through Intelligent and Socially Informed Tech-nology, pages 217?224.B.
J. Grosz, A. K. Joshi, and S. Weinstein.
1995.Centering: A framework for modeling the local co-herence of discourse.
Computational Linguistics,21(2):203?226.M.
A. K. Halliday and C. Matthiessen.
2004.
An Intro-duction to Functional Grammar.
Routledge.R.
Iida, M. Komachi, K. Inui, and Y. Matsumoto.
2007.Annotating a Japanese text corpus with predicate-argument and coreference relations.
In Proceedingof the ACL Workshop ?Linguistic Annotation Work-shop?, pages 132?139.S.
Ikehara, M. Miyazaki, S. Shirai A. Yokoo,H.
Nakaiwa, K. Ogura, Y. Ooyama, and Y. Hayashi.1997.
Nihongo Goi Taikei (in Japanese).
IwanamiShoten.Kentaro Inui, Takenobu Tokunaga, and HozumiTanaka.
1994.
Text revision: A model and itsimplementation.
In Aspects of Automated NaturalLanguage Generation: Proceedings of the 6th Inter-national Natural Language Generation Workshop,pages 215?230.S.
Nariyama.
2002.
Grammar for ellipsis resolutionin Japanese.
In Proceedings of the 9th InternationalConference on Theoretical and Methodological Is-sues in Machine Translation, pages 135?145.D.
R. Radev and K. R. McKeown.
1998.
Generat-ing natural language summaries from multiple on-line sources.
Computational Linguistics, 24(3):469?500.E.
Reiter and R. Dale.
2000.
Building Natural Lan-guage Generation Systems.
Cambridge UniversityPress.Ehud Reiter.
1994.
Has a consensus NL generationarchitecture appeared, and is it psycholinguisticallyplausible?
In Proceedings of the Seventh Interna-tional Workshop on Natural Language Generation,pages 163?170.Jacques Robin.
1994.
Revision-based Generationof Natural Language Summaries Providing His-torical Background ?
Corpus-based Analysis, De-sign, Implementation and Evaluation.
Ph.D. thesis,Columbia University.F.
Abu Sheikha and D. Inkpen.
2011.
Generation offormal and informal sentences.
In Proceedings ofthe 13th European Workshop on Natural LanguageGeneration, pages 187?193.151
