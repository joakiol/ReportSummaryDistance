Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 106?115,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAutomatic Creation of Arabic Named Entity Annotated Corpus UsingWikipediaMaha Althobaiti, Udo Kruschwitz, and Massimo PoesioSchool of Computer Science and Electronic EngineeringUniversity of EssexColchester, UK{mjaltha, udo, poesio}@essex.ac.ukAbstractIn this paper we propose a new methodology to ex-ploit Wikipedia features and structure to automati-cally develop an Arabic NE annotated corpus.
EachWikipedia link is transformed into an NE type ofthe target article in order to produce the NE an-notation.
Other Wikipedia features - namely redi-rects, anchor texts, and inter-language links - areused to tag additional NEs, which appear withoutlinks in Wikipedia texts.
Furthermore, we have de-veloped a filtering algorithm to eliminate ambiguitywhen tagging candidate NEs.
Herein we also in-troduce a mechanism based on the high coverage ofWikipedia in order to address two challenges partic-ular to tagging NEs in Arabic text: rich morphologyand the absence of capitalisation.
The corpus cre-ated with our new method (WDC) has been used totrain an NE tagger which has been tested on differ-ent domains.
Judging by the results, an NE taggertrained on WDC can compete with those trained onmanually annotated corpora.1 IntroductionSupervised learning techniques are well knownfor their effectiveness to develop Named EntityRecognition (NER) taggers (Bikel et al., 1997;Sekine and others, 1998; McCallum and Li, 2003;Benajiba et al., 2008).
The main disadvantage ofsupervised learning is that it requires a large an-notated corpus.
Although a substantial amountof annotated data is available for some languages,for other languages, including Arabic, more workis needed to enrich their linguistic resources.
Infact, changing the domain or just expanding theset of classes always requires domain-specific ex-perts and new annotated data, both of which costtime and effort.
Therefore, current research fo-cuses on approaches that require minimal humanintervention to facilitate the process of moving theNE classifiers to new domains and to expand NEclasses.Semi-supervised and unsupervised learning ap-proaches, along with the automatic creation oftagged corpora, are alternatives that avoid manu-ally annotated data (Richman and Schone, 2008;Althobaiti et al., 2013).
The high coverage andrich informational structure of online encyclope-dias can be exploited for the automatic creation ofdatasets.
For example, many researchers have in-vestigated the use of Wikipedia?s structure to clas-sify Wikipedia articles and to transform links intoNE annotations according to the link target type(Nothman et al., 2008; Ringland et al., 2009).In this paper we present our approach to au-tomatically derive a large NE annotated corporafrom Arabic Wikipedia.
The key to our methodlies in the exploitation of Wikipedia?s concepts,specifically anchor texts1and redirects, to handlethe rich morphology in Arabic, and thereby elim-inate the need to perform any deep morphologi-cal analysis.
In addition, a capitalisation probabil-ity measure has been introduced and incorporatedinto the approach in order to replace the capitalisa-tion feature that does not exist in the Arabic script.This capitalisation measure has been utilised in or-der to filter ambiguous Arabic NE phrases duringannotation process.The remainder of this paper is structured as fol-lows: Section 2 illustrates structural informationabout Wikipedia.
Section 3 includes backgroundinformation on NER, including recent work.
Sec-tion 4 summarises the proposed methodology.Sections 5, 6, and 7 describe the proposed algo-rithm in detail.
The experimental setup and theevaluation results are reported and discussed inSection 8.
Finally, the conclusion features com-ments regarding our future work.1The terms ?anchor texts?
and ?link labels?
are used inter-changeably in this paper.1062 The Structure of WikipediaWikipedia is a free online encyclopedia projectwritten collaboratively by thousands of volunteers,using MediaWiki2.
Each article in Wikipedia isuniquely identified by its title.
The title is usuallythe most common name for the entity explainedin the article.2.1 Types of Wikipedia Pages2.1.1 Content PagesContent pages (aka Wikipedia articles) contain themajority of Wikipedia?s informative content.
Eachcontent page describes a single topic and has aunique title.
In addition to the text describing thetopic of the article, content pages may contain ta-bles, images, links and templates.2.1.2 Redirect PagesA redirect page is used if there are two or morealternative names that can refer to one entityin Wikipedia.
Thus, each alternative name ischanged into a title whose article contains a redi-rect link to the actual article for that entity.
For ex-ample, ?UK?
is an alternative name for the ?UnitedKingdom?, and consequently, the article with thetitle ?UK?
is just a pointer to the article with thetitle ?United Kingdom?.2.1.3 List of PagesWikipedia offers several ways to group articles.One method is to group articles by lists.
The itemson these lists include links to articles in a particu-lar subject area, and may include additional infor-mation about the listed items.
For example, ?listof scientists?
contains links to articles of scientistsand also links to more specific lists of scientists.2.2 The Structure of Wikipedia Articles2.2.1 CategoriesEvery article in the Wikipedia collection shouldhave at least one category.
Categories should beon vital topics that are useful to the reader.
Forexample, the Wikipedia article about the UnitedKingdom in Wikipedia is associated with a set ofcategories that includes ?Countries bordering theAtlantic Ocean?, and ?Countries in Europe?.2An open source wiki package written in PHP2.2.2 InfoboxAn infobox is a fixed-format table added to thetop right-hand or left-hand corner of articles toprovide a summary of some unifying parametersshared by the articles.
For instance, every scientisthas a name, date of birth, birthplace, nationality,and field of study.2.3 LinksA link is a method used by Wikipedia to link pageswithin wiki environments.
Links are enclosed indoubled square brackets.
A vertical bar, the ?pipe?symbol, is used to create a link while labelling itwith a different name on the current page.
Look atthe following two examples,1 - [[a]] is labelled ?a?
on the current page andlinks to taget page ?a?.2 - [[a|b]] is labelled ?b?
on the current page, butlinks to target page ?a?.In the second example, the anchor text (aka linklabel) is ?a?, while ?b?, a link target, refers to thetitle of the target article.
In the first example, theanchor text shown on the page and the title of thetarget article are the same.3 Related WorkCurrent NE research seeks out adequate alter-natives to traditional techniques such that theyrequire minimal human intervention and solvedeficiencies of traditional methods.
Specificdeficiencies include the limited number of NEclasses resulting from the high cost of setting upcorpora, and the difficulty of adapting the systemto new domains.One of these trends is distant learning, whichdepends on the recruitment of external knowledgeto increase the performance of the classifier, orto automatically create new resources used in thelearning stage.Kazama and Torisawa (2007) exploitedWikipedia-based features to improve their NEmachine learning recogniser?s F-score by threepercent.
Their method retrieved the correspondingWikipedia entry for each candidate word sequencein the CoNLL 2003 dataset and extracted a cate-gory label from the first sentence of the entry.The automatic creation of training data hasalso been investigated using external knowledge.An et al.
(2003) extracted sentences containinglisted entities from the web, and produced a1.8 million Korean word dataset.
Their corpus107performed as well as manually annotated trainingdata.
Nothman et al.
(2008) exploited Wikipediato create a massive corpus of named entityannotated text.
They transformed Wikipedia?slinks into named entity annotations by classifyingthe target articles into standard entity types3.Compared to MUC, CoNLL, and BBN corpora,their Wikipedia-derived corpora tend to performbetter than other cross-corpus train/test pairs.Nothman et al.
(2013) automatically createdmassive, multilingual training annotations fornamed entity recognition by exploiting the textand internal structure of Wikipedia.
They firstcategorised each Wikipedia article into namedentity types, training and evaluating on 7,200manually-labelled Wikipedia articles across ninelanguages: English, German, French, Italian,Polish, Spanish, Dutch, Portuguese, and Russian.Their cross-lingual approach achieved up to 95%accuracy.
They transformed Wikipedia?s linksinto named entity annotations by classifying thetarget articles into standard entity types.
Thistechnique produced reasonable annotations, butwas not immediately able to compete with exist-ing gold-standard data.
They better aligned theirautomatic annotations to the gold standard corpusby deducing additional links and heuristicallytweaking the Wikipedia corpora.
Following thisapproach, millions of words in nine languageswere annotated.
Wikipedia-trained models wereevaluated against CONLL shared task data andother gold-standard corpora.
Their method out-performed Richman and Schone (2008) and Mikaet al.
(2008), and achieved scores 10% higherthan models trained on newswire when tested onmanually annotated Wikipedia text.Alotaibi and Lee (2013) automatically de-veloped two NE-annotated sets from ArabicWikipedia.
The corpora were built using themechanism that transforms links into NE an-notations, by classifying the target articles intonamed entity types.
They used POS-tagging,morphological analysis, and linked NE phrases todetect other mentions of NEs that appear withoutlinks in text.
By contrast, our method does notrequire POS-tagging or morphological analysisand just identifies unlinked NEs by matchingphrases from an automatically constructed andfiltered alternative names with identical terms in3The terms ?type?, ?class?
and ?category?
are used inter-changeably in this paper.the articles texts, see Section 6.
The first datasetcreated by Alotaibi and Lee (2013) is calledWikiFANE(whole) and contains all sentencesretrieved from the articles.
The second set, whichis called WikiFANE(selective), is constructed byselecting only the sentences that have at least onenamed entity phrase.4 Summary of the ApproachAll of our experiments were conducted on the26 March 2013 Arabic version of the Wikipediadump4.
A parser was created to handle the medi-awiki markup and to extract structural informationfrom the Wikipedia dump such as a list of redirectpages along with their target articles, a list of pairscontaining link labels and their target articles inthe form ?anchor text, target article?, and essentialinformation for each article (e.g., title, body text,categories, and templates).Many of Wikipedia?s concepts such as links, an-chor texts, redirects, and inter-language links havebeen exploited to transform Wikipedia into a NEannotated corpus.
More details can be found inthe next sections.
Generally, the following stepsare necessary to develop the dataset:1.
Classify Wikipedia articles into a specific setof NE types.2.
Identify matching text in the title and the firstsentence of each article and label the match-ing phrases according to the article type.3.
Label linked phrases in the text according tothe NE type of the target article.4.
Compile a list of alternative titles for articlesand filter out ambiguous ones.5.
Identify matching phrases in the list and theWikipedia text.6.
Filter sentences to prevent noisy sentencesbeing included in the corpus.We explain each step in turn in the following sec-tions.5 Classifying Wikipedia Articles into NECategoriesCategorising Wikipedia articles is the initial stepin producing NE training data.
Therefore, allWikipedia articles need to be classified into aspecific set of named entity types.4http://dumps.wikimedia.org/arwiki/1085.1 The Dataset and AnnotationIn order to develop a Wikipedia document clas-sifier, we used a set of 4,000 manually classi-fied Wikipedia articles that are available free on-line5.
The set was manually classified using theACE (2008) taxonomy and a new class (Product).Therefore, there were eight coarse-grained cate-gories in total: Facility, Geo-Political, Location,Organisation, Person, Vehicle, Weapon, and Prod-uct.
As our work adheres to the CoNLL definition,we mapped these classified Wikipedia articles intoCoNLL NE types ?
namely person, location, or-ganisation, miscellaneous, or other ?
based on theCoNLL 2003 annotation guidelines (Chinchor etal., 1999).5.2 The Classification of Wikipedia ArticlesMany researchers have already addressed the taskof classifying Wikipedia articles into named entitytypes (Dakka and Cucerzan, 2008; Tardif et al.,2009).
Alotaibi and Lee (2012) is the only studythat has experimented with classifying the Arabicversion of Wikipedia into NE classes.
They haveexplored the use of Naive Bayes, MultinomialNaive Bayes, and SVM for classifying Wikipediaarticles, and achieved a F-score ranging from 78%and 90% using different language-dependent andindependent features.We conducted three experiments that used asimple bag-of-words features extracted from dif-ferent portions of the Wikipedia document andmetadata.
We summarise the portions of the doc-ument included in each experiment below:Exp1: Experiment 1 involved tokens from thearticle title and the entire article body.Exp2: Rich metadata in Wikipedia proved ef-fective for the classification of articles (Tardif etal., 2009; Alotaibi and Lee, 2012).
Therefore, inExperiment 2 we included tokens from categories,templates ?
specifically ?Infobox?
?
as well as to-kens from the article title and first sentence of thedocument.Exp3: Experiment 3 involved the same set oftokens as experiment 2 except that categories andinfobox features were marked with suffixes to dif-ferentiate them from tokens extracted from the ar-ticle body text.
This step of distinguishing tokensbased on their location in the document improvedthe accuracy of document?s classification (Tardifet al., 2009; Alotaibi and Lee, 2012).5www.cs.bham.ac.uk/?fsa081/In order to optimise features, we implemented afiltered version of the bag-of-words article repre-sentation (e.g., removing punctuation marks andsymbols) to classify the Arabic Wikipedia doc-uments instead of using a raw dataset (Alotaibiand Lee, 2012).
In addition, the same studyshows the high impact of applying tokenisation6as opposed to the neutral effect of using stem-ming.
We used the filtered features proposed inthe study of Alotaibi and Lee (2012), which in-cluded removing punctuation marks, symbols, fil-tering stop words, and normalising digits.
We ex-tended the features, however, by utilising the to-kenisation scheme that involves separating con-junctions, prepositions, and pronouns from eachword.The feature set has been represented using TermFrequency-Inverse Document Frequency (TF ?IDF ).
This representation method is a numeri-cal statistic that reflects how important a token isto a document.5.3 The Results of Classifying the WikipediaArticlesAs for the learning process, our Wikipedia doc-uments classifier was trained using Liblinear7.80% of the 4,000 hand-classified Wikipediaarticles were dedicated to the training stage, while20% were specified to test the classifier.
Table1 is a comparison of the precision, recall, andF-measure of the classifiers that resulted from thethree experiments.
The Exp3 classifier performedbetter than the other classifiers.
Therefore, it wasselected to classify all of the Wikipedia articles.At the end of this stage, we obtained a list ofpairs containing each Wikipedia article and itsNE Type.
We stored this list in a database inpreparation for the next stage: developing theNE-tagged training corpus.Table 1: The results of the three Wikipedia docu-ment classifiers.6It is also called decliticization or segmentation.7www.csie.ntu.edu.tw/?cjlin/liblinear/1096 The Annotation Process6.1 Utilising the Titles of Articles and LinkTargetsIdentifying corresponding words in the article ti-tle and the entire body of text and then tagging thematching phrases with the NE-type can be a riskyprocess, especially for terms with more than onemeaning.
For example, the title of the article de-scribing the city (?A?, ?Cannes?
)8can also, in Ara-bic, refer to the past verb (?A?, ?was?).
The portionof the Wikipedia article unlikely to produce errorsduring the matching process is the first sentence,which usually contains the definition of the termthe Wikipedia article is written about (Zesch et al.,2007).When identifying matching terms in the arti-cle title and the first sentence, we found that ar-ticle titles often contain abbreviations, while thefirst sentence spells out entire words.
This pat-tern makes it difficult to identify matching termsin the title and first sentence, and frequently ap-pears in biographical Wikipedia articles.
For ex-ample, one article is entitled (?P@Q?
@ Q?K.?K.
@, ?AbuBakr Al-Razi?
), but the first sentence states the fullname of the person: (?P@Q?
@ AKQ?P?K.??m'?K.Y?m?Q?K.?K.
@,?Abu Bakr Mohammad Bin Yahia Bin Zakaria Al-Razi?).
Therefore, we decided to address the prob-lem with partial matching.
In this case, the sys-tem should first identify all corresponding wordsin the title and the first sentence.
Second, the sys-tem should annotate them and all words that fallbetween, provided that:?
the sequence of the words in the article titleand the text are the same in order to avoiderrors in tagging.
For example, if the title ofthe article is (Q??AJ?
@ Q?E, ?The River Thames?
),but the first sentence reads (.
.
.
.
??
??KQ?E ??Q??AJ?
@, ?The Thames is a river flowing throughsouthern England....?
), then the text will notbe properly tagged.?
the number of tokens located betweenmatched tokens is less than or equal to five9.Figure 1 shows one example of partial matching.8Throughout the entire paper, Arabic words are repre-sented as follows: ( Arabic word,?English translation?
).9An informal experiment showed that the longest properArabic names are 5 to 7 tokens in length.Figure 1: Example of Partial MatchingThe next step is to transform the links be-tween Wikipedia articles into NE annotations ac-cording to the link target type.
Therefore, thelink ([[ A?AK.?
@ ?
@PAK.| A?AK.?
@]]/[[Barack Obama|Obama]])would be changed to ( A?AK.?
@ PER) (Obama PER),since the link target (Barack Obama) is the title ofan article about person.
By the end of this stage,all NE anchor texts (anchor texts referring to NEarticles) on Wikipedia should be annotated basedon the NE-type of the target article.6.2 Dictionaries of Alternative NamesDepending only on NE anchor texts in order toderive and annotate data from Wikipedia resultsin a low-quality dataset, as Wikipedia containsa fair amount of NEs mentioned without links.This can be attributed to the fact that each termon Wikipedia is more likely to be linked onlyon its first appearance in the article (Nothman etal., 2008).
These unlinked NE phrases can befound simply by identifying the matching termsin the list of linked NE phrases10and the text.The process is not as straightforward as it seems,however, because identifying corresponding termsmay prove ineffective, especially in the case ofmorphologically rich language in which unlinkedNE phrases are sometimes found agglutinated toprefixes and conjunctions.
In order to detect un-linked and inflected forms of NEs in Wikipediatext, we extended the list of articles titles that wereused in the previous step to find and match the pos-sible NEs in the text by including NE anchor texts.Adding NE anchor texts to the list assists in find-ing possible morphologically inflected NEs in thetext while eliminating the need for any morpho-10The list of anchor texts that refer to NE articles110logical analysis.
Table 2 shows examples from thedictionary of NE anchor texts.Table 2: Examples from the dictionary of NE An-chor Texts.Spelling variations resulting from variedtransliteration of foreign named entities in somecases prevent the accurate matching and identifi-cation of some unlinked NEs, if only the list ofNE anchor texts is used.
For example, ( @Q?m.'@, ?Eng-land?)
has been written five different ways: ( ?Q?m.'@,@Q?
?K @, ?Q?
?K @, @Q?
?K @, ?Q?
?K @).
Therefore, we compileda list of the titles of redirected pages that sendthe reader to articles describing NEs.
We referto these titles in this paper as NE redirects.
Weconsider to the lists of NE redirects and anchortexts a list of alternative names, since they can beused as alternative names for article titles.The list of alternative names is used to findunlinked NEs in the text by matching phrasesfrom the list with identical terms in the articlestexts.
This list is essential for managing spellingand morphological variations of unlinked NEs, aswell as misspelling.
Consequently, the processincreases the coverage of NE tags augmentedwithin the plain texts of Wikipedia articles.6.2.1 Filtering the Dictionaries of AlternativeNamesOne-word alternative names: Identifyingmatching phrases in the list of alternative namesand the text inevitably results in a lower qualitycorpus due to noisy names.
The noisy alternativenames usually occur with meaningful namedentities.
For example, the article on the person(?
?B@ ???
@YJ.?
?K.
@, ?Abu Abdullah Alamyn?)
has analternative name consisting only of his lastname (?
?B@, ?Alameen?
), which means ?custo-dian?.
Therefore, annotating every occurrence of?Alamyn?
as PER would lead to incorrect taggingand ambiguity.
The same applies to the city withthe name ( ?YKYm.?
'@, ?Aljadydah?
), which literallymeans ?new?.
Thus, the list of alternative namesshould be filtered to omit one-word NE phrasesthat usually have a meaning and are ambiguouswhen taken out of context.In order to solve this problem, we introduceda capitalisation probability measure for Arabicwords, which are never capitalised.
This involvedfinding the English gloss for each one-word alter-native name and then computing its probabilityof being capitalised using the English Wikipedia.To find the English gloss for Arabic words, weexploited Wikipedia Arabic-to-English cross-lingual links that provided us with a reasonablenumber of Arabic and corresponding Englishterms.
If the English gloss for the Arabic wordcould not be found using inter-language links, weresorted to an online translator.
Before translatingthe Arabic word, a light stemmer was used toremove prefixes and conjunctions in order toget the translation of the word itself without itsassociated affixes.
Otherwise, the Arabic word(XCJ.??)
would be translated as (in the country).The capitalisation probability was computed asfollows:Pr[EN ] =f(EN)isCapitalisedf(EN)isCapitalised+f(EN)notCapitalisedwhere: EN is the English gloss of the alter-native name; f(EN)isCapitalisedis the numberof times the English gloss EN is capitalised inEnglish Wikipedia; and f(EN)notCapitalisedisthe number of times the English gloss EN is notcapitalised in English Wikipedia.This way, we managed to build a list of Arabicwords and their probabilities of being capitalised.It is evident that the meaningful one-word NEsusually achieve a low probability.
By specifyinga capitalisation threshold constraint, we preventedsuch words from being included in the list ofalternative names.
After a set of experiments, wedecided to use the capitalisation threshold equalto 0.75.Multi-word alternative names: Multi-wordalternative names (e.g., X??m??????
/?MusTafaeMahmud?
), ?XA?
Y?g@ /?Ahmad Adel?)
rarely causeerrors in the automatic annotation process.Wikipedians, however, at times append personaland job titles to the person?s name contained inthe anchor text, which refers to the article aboutthat person.
Examples of such anchor texts are(Y?@P?K.Y?m?
?G.X ??
Ag, ?Ruler of Dubai Muhammadbin Rashid?)
and (Y?@P?K.Y?m?Z@PP??
@ ??m.?
?KP, ?Presi-dent of the Council of Ministers Muhammad bin111Rashid?).
As a result, the system will mistakenlyannotate words like Dubai, Council, Ministersas PER.
Our solution to this problem is to omitthe multi-word alternative name, if any of itswords belong to the list of apposition words,which usually appear adjacent to NEs such as(?KP, ?President?
), (QKP?, ?Minister?
), and (??
Ag,?Ruler?).
The filtering algorithm managed toexclude 22.95% of the alternative names from theoriginal list.
Algorithm 1 shows pseudo code ofthe filtering algorithm.Algorithm 1: Filtering Alternative NamesInput: A set L = {l1, l2, .
.
.
, ln} of all alternativenames of Wikipedia articlesOutput: A set RL = {rl1, rl2, .
.
.
, rln} of reliablealternative names1 for i?
1 to n do2 T ?
split liinto tokens3 if (T.size() >= 2) then/*All tokens of T do notbelong to apposition list*/4 if (!
containAppositiveWord(T)) then5 add lito the set RL6 else7 lightstem?
findLightStem(li)8 englishgloss?
translate(lightstem)/*Compute CapitalisationProbability for Englishgloss*/9 capprob?
compCapProb(englishgloss)10 if (capprob> 0.75) then11 add lito the set RLThe dictionaries derived from Wikipedia byexploiting Wikipedia?s structure and adopting thefiltering algorithm is shown in Table 3.Table 3: Dictionaries derived from Wikipedia.6.3 Post-processingThe goal of Post-processing was to address someissues that arose during the annotation process asa result of different domains, genres, and con-ventions of entity types.
For example, national-ities and other adjectival forms of nations, reli-gions, and ethnic groups are considered MISC inthe CoNLL NER task in the English corpus, whilethe Spanish corpus consider them NOT named en-tities (Nothman et al., 2013).
As far as we know,almost all Arabic NER datasets that followed theCoNLL style and guidelines in the annotation pro-cess consider nationalities NOT named entities.On Wikipedia all nationalities are linked to ar-ticles about the corresponding countries, whichmakes the annotation tool tag them as LOC.
Wedecided to consider them NOT named entities inaccordance with the CoNLL-style Arabic datasets.Therefore, in order to resolve this issue, we com-piled a list of nationalities, and other adjectivalforms of religion and ethnic groups, so that anyanchor text matching an entry in the list was re-tagged as a NOT named entity.The list of nationalities and apposition wordsused in section 6.2.1 were compiled by exploitingthe ?List of?
articles in Wikipedia such as list ofpeople by nationality, list of ethnic groups, list ofadjectival forms of place names, and list of titles.Some English versions of these ?List of?
pageshave been translated into Arabic, either becausethey are more comprehensive than the Arabic ver-sion, or because there is no corresponding page inArabic.7 Building the CorpusAfter the annotation process, the last step wasto incorporate sentences into the corpus.
Thisresulted in obtaining an annotated dataset witharound ten million tokens.
However, in order toobtain a corpus with a large number of tags with-out affecting its quality, we created a dataset calledWikipedia-derived corpus (WDC), which includedonly sentences with at least three annotated namedentity tokens.
The WDC dataset contains 165,119sentences consisting of around 6 million tokens.The annotation style of the WDC dataset followedthe CoNLL format, where each token and its tagare placed together in the same file in the form< token > \s < tag >.
The NE boundaryis specified using the BIO representation scheme,where B- indicates the beginning of the NE, I-refers to the continuation (Inside) of the NE, andO indicates that the word is not a NE.
The WDCdataset is available online to the community of re-searchers1111https://www.dropbox.com/sh/27afkiqvlpwyfq0/1hwWGqAcTL1128 Experimental EvaluationTo evaluate the quality of the methodology, weused WDC as training data to build an NER model.Then we tested the resulting classifier on datasetsfrom different domains.8.1 DatasetsFor the evaluation purposes, we used threedatasets: ANERcorp, NEWS, and TWEETS.ANERcorp is a news-wire domain dataset builtand tagged especially for the NER task by Bena-jiba et al.
(2007).
It contains around 150k tokensand is available for free.
We tested our method-ology on the ANERcorp test corpus because it iswidely used in the literature for comparing withexisting systems.
The NEWS dataset is also anews-wire domain dataset collected by Darwish(2013) from the RSS feed of the Arabic versionof news.google.com from October 2012.
TheRSS consists of the headline and the first 50 to100 words in the news articles.
This set containsapproximately 15k tokens.
The third test set wasextracted randomly from Twitter and contains aset of 1,423 tweets authored in November 2011.It has approximately 26k tokens (Darwish, 2013).8.2 Our Supervised ClassifierAll experiments to train and build a probabilisticclassifier were conducted using Conditional Ran-dom Fields (CRF)12.
Regarding the features usedin all our experiments, we selected the most suc-cessful features from Arabic NER work (Benajibaet al., 2008; Abdul-Hamid and Darwish, 2010;Darwish, 2013).
These features include:?
The words immediately before and after thecurrent word in their raw and stemmed forms.?
The first 1, 2, 3, 4 characters in a word.?
The last 1, 2, 3, 4 characters in a word.?
The appearance of the word in the gazetteer.?
The stemmed form of the word.The gazetteer used contains around 5,000 entriesand was developed by Benajiba et al.
(2008).
Alight stemmer was used to determine the stemform of the word by using simple rules to re-move conjunctions, prepositions, and definite ar-ticles (Larkey et al., 2002).12http://www.chokkan.org/software/crfsuite/8.3 Training the Supervised Classifier onManually-annotated DataThe supervised classifier in Section 8.2 wastrained on the ANERcorp training set.
We refer tothe resulting model as the ANERcorp-Model.
Ta-ble 4 shows the results of the ANERcorp-Modelon the ANERcorp test set.
The table also showsthe results of the state-of-the-art supervised clas-sifier ?ANERcorp-Model(SoA)?
developed by Dar-wish (2013) when trained and tested on the samedatasets used for ANERcorp-Model.Table 4: The results of Supervised Classifiers.8.4 ResultsWe compared a system trained on WDC withthe systems trained by Alotaibi and Lee (2013)on two datasets, WikiFANE(whole) and Wiki-FANE(selective), which are also automatically col-lected from Arabic Wikipedia.
The evaluation pro-cess was conducted by testing them on the AN-ERcorp set.
The results shown in Table 5 provethat the methodology we proposed in this paperproduces a dataset that outperforms the two otherdatasets in terms of recall and F-measure.Table 5: Comparison of the system trained onWDC dataset with the systems trained on Wiki-FANE datasets.Table 6 compares the results of the ANERcorp-Model and the WDC-Model when testing them ondatasets from different domains.
Firstly, We de-cided to test the ANERcorp-Model and the WDC-Model on Wikipedia.
Thus, a subset, contain-ing around 14k tokens, of WDC set was allocatedfor testing purpose.
The results in Table 6 showsthat WDC classifier outperforms the F-score of thenews-based classifier by around 48%.The obvi-ous difference in the performance of the two clas-sifiers can be attributed to the difference in an-notation convention for different domains.
Forexample, many key words in Arabic Wikipedia,113which appear in the text along with NEs (e.g.,??
?Ag./university,?JKY?/ city,?
?Q??/company), are usu-ally considered part of NE names.
So, the phrase?Shizuoka Prefecture?
that is mentioned in someArabic Wikipedia articles is considered an entityand linked to an article that talks about Shizuoka,making the system annotate all words in the phraseas NEs as follows: (??
?Am?B-LOC A??Q?
I-LOC/Shizuoka B-LOC Prefecture I-LOC).
On the otherhand, in ANERcorp corpus, only the the word af-ter the keyword (?KB?, ?Prefecture?)
is consideredNE.
In addition, although sport facilities (e.g., sta-diums) are categorized in Wikipedia as location,some of them are not even considered entities inANERcorp test corpus.Secondly, the ANERcorp-Model and the WDC-Model were tested on the ANERcorp test data.The point of this comparison is to show how wellthe WDC dataset works on a news-wire domain,which is more specific than Wikipedia?s open do-main.
The table shows that the ANERcorp-modeloutperforms the F-score of the WDC-Model byaround 13 points.
However, in addition to the factthat training and test datasets for the ANERcorp-Model are drawn from the same domain, 69% ofNEs in the test data were seen in the training set(Darwish, 2013).Thirdly, the ANERcorp-Model and the WDC-Model were tested on NEWS corpus, which is alsoa news-wire based dataset.
The results from Ta-ble 6 reveal the quality of the WDC dataset on theNEWS corpus.
The WDC-Model achieves rela-tively similar results to the ANERcorp-Model, al-though the latter has the advantage of being trainedon a manually annotated corpus extracted from thesimilar domain of the NEWS test set.Finally, testing the ANERcorp-Model and theWDC-Model on data extracted from a social net-works like Twitter proves that models trained onopen-domain datasets like Wikipedia perform bet-ter on social network text than classifiers trainedon domain-specific datasets, as shown in Table 6.In order to show the effect of combining ourcorpus (WDC) with a manually annotated datasetfrom a different domain, we merged WDC with theANERcorp dataset.
Table 7 shows the results of asystem trained on the combined corpus when test-ing it on three test sets.
The system trained on thecombined corpus achieves results that fall betweenthe results of the systems trained on each corpusseparately when testing them on the ANERcorpTable 6: The F-scores of ANERcorp-Model andWDC-Model on ANERcorp, NEWS, & TWEETSdatasets.test set and NEWS test set.
On the other hand,the results of the system trained on the combinedcorpus when tested on the third test set (TWEETS)show no significant improvement.Table 7: The results of combining WDC with AN-ERcorp dataset.9 Conclusion and Future WorkWe have presented a methodology that requiresminimal time and human intervention to gener-ate an NE-annotated corpus from Wikipedia.
Theevaluation results showed the high quality of thedeveloped corpus WDC, which contains around6 million tokens representing different genres, asWikipedia is considered an open domain.
Further-more, WDC outperforms other NE corpora gen-erated automatically from Arabic Wikipedia by 8to 12 points in terms of F-measure.
Our methodol-ogy can easily be adapted to extend to new classes.Therefore, in future we intend to experiment withfiner-grained NE hierarchies.
In addition, we planto carry out some domain adaptation experimentsto handle the difference in annotation conventionfor different domains.ReferencesAhmed Abdul-Hamid and Kareem Darwish.
2010.Simplified feature set for Arabic named entity recog-114nition.
In Proceedings of the 2010 Named EntitiesWorkshop, pages 110?115.
Association for Compu-tational Linguistics.Fahd Alotaibi and Mark Lee.
2012.
Mapping ArabicWikipedia into the Named Entities Taxonomy.
InCOLING (Posters), pages 43?52.Fahd Alotaibi and Mark Lee.
2013.
Automatically De-veloping a Fine-grained Arabic Named Entity Cor-pus and Gazetteer by utilizing Wikipedia.
In IJC-NLP.Maha Althobaiti, Udo Kruschwitz, and Massimo Poe-sio.
2013.
A Semi-supervised Learning Approachto Arabic Named Entity Recognition.
In RANLP,pages 32?40.Joohui An, Seungwoo Lee, and Gary Geunbae Lee.2003.
Automatic acquisition of named entity taggedcorpus from world wide web.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 2, pages 165?168.
Asso-ciation for Computational Linguistics.Yassine Benajiba, Paolo Rosso, and Jos?e MiguelBened??ruiz.
2007.
Anersys: An Arabic NamedEntity Recognition System based on Maximum En-tropy.
In Computational Linguistics and IntelligentText Processing, pages 143?153.
Springer.Yassine Benajiba, Mona Diab, and Paolo Rosso.
2008.Arabic Named Entity Recognition using optimizedfeature sets.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 284?293.
Association for ComputationalLinguistics.Daniel M Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: a high-performance learning name-finder.
In Proceedingsof the fifth conference on Applied natural languageprocessing, pages 194?201.
Association for Compu-tational Linguistics.Nancy Chinchor, Erica Brown, Lisa Ferro, and PattyRobinson.
1999.
1999 Named Entity RecognitionTask Definition.
MITRE and SAIC.Wisam Dakka and Silviu Cucerzan.
2008.
Augment-ing Wikipedia with Named Entity Tags.
In IJCNLP,pages 545?552.Kareem Darwish.
2013.
Named Entity Recognitionusing Cross-lingual Resources: Arabic as an Exam-ple.
In ACL.Junichi Kazama and Kentaro Torisawa.
2007.
Exploit-ing Wikipedia as external knowledge for named en-tity recognition.
In Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 698?707.L.S.
Larkey, L. Ballesteros, and M.E.
Connell.
2002.Improving stemming for Arabic information re-trieval: light stemming and co-occurrence analysis.In Annual ACM Conference on Research and De-velopment in Information Retrieval: Proceedings ofthe 25 th annual international ACM SIGIR confer-ence on Research and development in informationretrieval, volume 11, pages 275?282.Andrew McCallum and Wei Li.
2003.
Early results fornamed entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings of the seventh conference on Naturallanguage learning at HLT-NAACL 2003-Volume 4,pages 188?191.
Association for Computational Lin-guistics.Peter Mika, Massimiliano Ciaramita, Hugo Zaragoza,and Jordi Atserias.
2008.
Learning to Tag and Tag-ging to Learn: A Case Study on Wikipedia.
vol-ume 23, pages 26?33.Joel Nothman, James R Curran, and Tara Murphy.2008.
Transforming Wikipedia into Named Entitytraining data.
In Proceedings of the Australian Lan-guage Technology Workshop, pages 124?132.Joel Nothman, Nicky Ringland, Will Radford, TaraMurphy, and James R Curran.
2013.
Learn-ing multilingual Named Entity Recognition fromWikipedia.
Artificial Intelligence, 194:151?175.Alexander E Richman and Patrick Schone.
2008.
Min-ing Wiki Resources for Multilingual Named EntityRecognition.
In ACL, pages 1?9.Nicky Ringland, Joel Nothman, Tara Murphy, andJames R Curran.
2009.
Classifying articles inEnglish and German Wikipedia.
In AustralasianLanguage Technology Association Workshop 2009,page 20.Satoshi Sekine et al.
1998.
NYU: Description of theJapanese NE system used for MET-2.
In Proc.
of theSeventh Message Understanding Conference (MUC-7), volume 17.Sam Tardif, James R. Curran, and Tara Murphy.2009.
Improved Text Categorisation for WikipediaNamed Entities.
In Proceedings of the AustralasianLanguage Technology Association Workshop, pages104?108.Torsten Zesch, Iryna Gurevych, and Max M?uhlh?auser.2007.
Analyzing and accessing Wikipedia as a lex-ical semantic resource.
pages 197?205.
Tuebingen,Germany: Gunter Narr, T?ubingen.115
