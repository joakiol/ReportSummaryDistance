Consistent Validation of Manual andAutomatic Sense Annotations withthe Aid of Semantic GraphsRoberto Navigli?Universita` di Roma ?La Sapienza?The task of annotating texts with senses from a computational lexicon is widely recognized to becomplex and often subjective.
Although strategies like interannotator agreement and voting canbe applied to deal with the divergences between sense taggers, the consistency of sense choiceswith respect to the reference dictionary is not always guaranteed.In this article, we introduce Valido, a visual tool for the validation of manual and auto-matic sense annotations.
The tool employs semantic interconnection patterns to smooth possibledivergences and support consistent decision making.1.
IntroductionSense tagging is the task of assigning senses chosen from a computational lexicon towords in context.
This is a task where both machines and humans find it difficult toreach an agreement.
The problem depends on a variety of factors, ranging from theinherent subjectivity of the task to the granularity of sense discretization, coverage ofthe reference dictionary, etc.The problem of validation is even amplified when sense tags are collected throughacquisition interfaces like the Open Mind Word Expert (Chklovski and Mihalcea 2002),due to the unknown source of the contributions of possibly unskilled volunteers.Strategies like voting for automatic sense annotations and the use of interannotatoragreement with adjudication for human sense assignments only partially solve the issueof disagreement.
Especially when there is no clear preference towards a certain wordsense, the final choice made by a judge can be subjective, if not arbitrary.
This is a casewhere analyzing the intrinsic structure of the reference lexicon is essential for producinga consistent decision.
A lexicographer is indeed expected to review a number of relateddictionary entries in order to adjudicate a sense coherently.
This work can be tedious,time-consuming, and often incomplete due to the complex structure of the resource.
Asa result, inconsistent choices can be made.In this article, we present Valido, a tool for supporting the validation of both manualand automatic sense annotations through the use of semantic graphs, particularly ofsemantic interconnection patterns (Navigli and Velardi 2005).?
Dipartimento di Informatica, Universita` di Roma ?La Sapienza,?
Via Salaria, 113 - 00198 Roma, Italia.E-mail: navigli@di.uniroma1.it.?
2006 Association for Computational LinguisticsComputational Linguistics Volume 32, Number 22.
Semantic Networks and Semantic Interconnection PatternsSemantic networks are a graphical notation developed to represent knowledge expli-citly as a set of conceptual entities and their interrelationships.
The availability of wide-coverage computational lexicons like WordNet (Fellbaum 1998), as well as semanticallyannotated corpora like SemCor (Miller et al 1993), has certainly contributed to theexploration and exploitation of semantic graphs for several tasks like the analysis oflexical text cohesion (Morris and Hirst 1991), word sense disambiguation (Agirre andRigau 1996; Mihalcea and Moldovan 2001), and ontology learning (Navigli and Velardi2004), etc.Recently, a knowledge-based algorithm for word sense disambiguation calledstructural semantic interconnections (SSI, http://lcl.di.uniroma1.it/ssi) (Navigli andVelardi 2004, 2005), has been shown to provide interesting insights into the choice ofword senses by providing structural justifications in terms of semantic graphs.
Givena word context and a lexical knowledge base (LKB), obtained by integrating WordNetwith annotated corpora and collocation resources (Navigli 2005), SSI selects a semanticgraph including those word senses having a higher degree of interconnection, accordingto a measure of connectivity.A semantic interconnection pattern is a relevant sequence of edges selected ac-cording to a context-free grammar, i.e., a path connecting a pair of word senses(dark nodes in Figure 1), possibly including a number of intermediate concepts (lightnodes in Figure 1).
For example, if the context of words to be disambiguated is [cross-v,street-n, intersection-n], the senses chosen by SSI with respect to WordNet are [cross-v#1,street#2, intersection#2],1 supported, among others, by the pattern intersection#2?
?part?of road#1 ?
?kind?of thoroughfare#1 ?
?kind?of street#2.
Semantic interconnection patternsare inspired by several works on semantic relatedness and similarity (Rada et al 1989;Hirst and St-Onge 1998; Mihalcea and Moldovan 2001).An excerpt of the manually written context-free grammar encoding semantic inter-connection patterns for the WordNet lexicon is reported in Table 1.
For further detailsthe reader can refer to Velardi 2005.3.
Supporting Validation with Semantic Interconnection PatternsThe validation task can be defined as follows: Let w be a word in a sentence ?,previously annotated by a set of annotators A = {a1, a2, ..., an} each providing a sensefor w, and let S = {s1, s2, ..., sm} ?
Senses(w) be the set of senses chosen for w by theannotators in A, where Senses(w) is the set of senses of w in the reference inven-tory (e.g., WordNet).
A validator is asked to validate, that is, to adjudicate a senses ?
Senses(w) for a word w over the others.
Notice that s is a word sense for w in thesense inventory, but is not necessarily in S, although it is likely to be.
Also note thatthe annotators in A can be either human or automatic, depending upon the purpose ofthe exercise.Based on SSI, we developed a visual tool, Valido (http://lcl.di.uniroma1.it/valido),to support the validator in the difficult task of assessing the quality and suitability ofsense annotations.
The tool takes as input a corpus of documents whose sentences are1 We indicate a word sense with the convention w-p#i, where w is a word, p its part of speech (n fornouns, a for adjectives, v for verbs, r for adverbs) and i its sense number in the reference inventory.For readability, in the following we omit the noun part of speech.274Navigli Consistent Validation of Manual and Automatic Sense AnnotationsFigure 1Structural interconnection patterns for the sentence We crossed the street near the intersection whensense #2 of street is chosen, as suggested by the validation policy (?
).tagged by one or more annotators with word senses from the WordNet inventory.
Theuser can then browse the sentences and adjudicate a choice over the others in case ofdisagreement among the annotators.
To the end of facilitating the user in the validationtask, the tool highlights each word in a sentence with different colors, namely, green forwords having a full agreement, red for words where no agreement can be found, andorange for those words to which a validation policy can be applied.Table 1An excerpt of the context-free grammar for the recognition of semantic interconnections.S ?
S?S1|S?S2|S?S3 (start rule)S?
?
eNOMINALIZATION|ePERTAINYMY| (part-of-speech jump)S1 ?
eKIND?OFS1|ePART?OFS1|eKIND?OF|ePART?OF (hyperonymy/meronymy)S2 ?
eKIND?OFS2|eRELATEDNESSS2|eKIND?OF|eRELATEDNESS (hypernymy/relatedness)S3 ?
eSIMILARITYS3|eANTONYMYS3|eSIMILARITY|eANTONYMY (adjectives)275Computational Linguistics Volume 32, Number 2A validation policy is a strategy for suggesting a default sense choice to the val-idator in case of disagreement.
Initially, the validator can choose one of four vali-dation policies to be applied to those words with disagreement on which sense toassign:(?)
majority voting: If there exists a sense s ?
S such that|{a ?
A | a annotated w with s}||A| ?12 ,s is proposed as the preferred sense for w.(?)
majority voting + SSI: The same as the previous policy, with the additionthat if there exists no sense chosen by a majority of annotators, SSI isapplied to w, and the sense chosen by the algorithm, if any, is proposedto the validator.(?)
SSI: The SSI algorithm is applied to w, and the chosen sense, if any, isproposed to the validator.(?)
no validation: w is left untagged.Notice that for policies (?)
and (?)
Valido applies the SSI algorithm to w in thecontext of its sentence ?
by taking into account for disambiguation only the senses in S(i.e., the set of senses chosen by the annotators).
In general, given a set of words withdisagreement W ?
?, SSI is applied to W using as a fixed context the agreed senseschosen for the words in ?
\ W.Also note that the suggestion of a sense choice, marked in orange based on thevalidation policy, is just a proposal and can be freely modified by the validator, asexplained hereafter.Before starting the interface, the validator can also choose whether to add a vir-tual annotator aSSI to the set of annotators A.
This virtual annotator tags each wordw ?
?
with the sense chosen by the application of the SSI algorithm to ?.
As a re-sult, the selected validation policy will be applied to the new set of annotators A?
=A ?
{aSSI}.
This is useful especially when |A| = 1 (e.g., in the automatic application ofa single word sense disambiguation system), that is, when validation policies are ofno use.Figure 1 illustrates the interface of the tool: In the top pane the sentence at handis shown, marked with colors as explained above.
The main pane shows the semanticinterconnections between senses for which either there is a full agreement or the chosenvalidation policy can be applied.
When the user clicks on a word w, the left pane reportsthe sense inventory for w, including information about the hypernym, definition, andusage for each sense of w. The validator can then click on a sense and see how thesemantic graph shown in the main pane changes after the selection, possibly resultingin a different number and strength of semantic interconnection patterns supporting thatsense choice.In the following subsections, we describe the application of the Valido tool to thevalidation of manual and automatic annotations, and we discuss cases of uncertainapplicability of the tool.276Navigli Consistent Validation of Manual and Automatic Sense Annotations3.1 Validating Manual AnnotationsIn the following, we illustrate the tool by presenting two examples of validation of amanual annotation (the validation policy ?
was selected).Figure 1 shows the senses chosen by the validators for the following sentence:(a) We crossed the street near the intersection.Sense #2 of intersection and sense #1 of cross are marked in green in the top pane,meaning that the annotators fully agreed on those choices.
On the other hand, sense#2 of street is marked in orange, due to a disagreement between the annotators, onepreferring sense #1.
Such an inconsistency is reported on the left pane, showing thedictionary definitions of the two senses.
The validator can then visualize in the same orin a new window the semantic graphs concerning conflicting sense choices, comparingthe interconnection patterns available for sense #1 and #2 of street.After evaluating the respective semantic interconnections, the validator can eitherconfirm the human annotator?s choice, accept the SSI interpretation, or assess the se-mantic interconnection patterns resulting from different sense choices (reported in theleft pane of Figure 1).It is worth mentioning that all the occurrences of the phrase cross the street in theSemCor corpus are tagged with the first sense of street [defined as a thoroughfare (usuallyincluding sidewalks) that is lined with buildings], but it is clear, from the definition of thesecond sense (the part of a thoroughfare between the sidewalks; the part of the thoroughfare onwhich vehicles travel; ?be careful crossing the street?
), that a pedestrian crosses that part ofthe thoroughfare between the sidewalks.
Though questionable, this is a subtlety madeexplicit in the dictionary and reinforced by the usage example of sense #2 above.
Thetool reflects this fact, showing that both senses are connected with other word senses incontext, the first sense having a smaller degree of overall connectivity.2As a second example, consider the WordNet definition of motorcycle:(b) Motorcycle: a motor vehicle with two wheels and a strong frameIn the Gloss Word Sense Disambiguation task at Senseval-3 (Litkowski 2004), thehuman annotators assigned the first sense to the word frame (a structure supporting orcontaining something), unintentionally neglecting that the dictionary encodes a specificsense of frame concerning the structure of objects (e.g., vehicles, buildings, etc.).
In fact,a chassis#3 is a kind of frame#6 (the internal supporting structure that gives an artifact itsshape), and is also part of a motor vehicle#1.
While regular polysemy holds between sense#1 and #6, there is no justification for the former choice, as it does not refer to vehiclesat all (as reflected by the lack of semantic interconnection patterns concerning frame#1).The tool applies the validation policy and suggests sense #6 to the validator.From these two real-world cases, it is evident that Valido can point at inconsistent,although acceptable, choices made by human annotators due, among others, to the finegranularity of the sense inventory and to regular polysemy.
In Section 4 we present anexperiment showing that this claim still holds on a larger scale.2 In the case of a large, connected graph, a pruned version is shown, and a link is available for viewing amore complete, extended version of the graph.277Computational Linguistics Volume 32, Number 2Apart from tagging mistakes, most of the cases of disagreement between manualannotators is due to the fine granularity of the lexicon inventory.
We recognize thatsubtle distinctions, like those encoded in WordNet, are rarely useful in any NLP appli-cation, but, as a matter of fact, WordNet is at the moment the de facto standard withinthe research community, as no other computational lexicon of that size and complexityis freely available.3.2 Validating Automatic AnnotationsWhile the task of manual annotation is mostly restricted to lexicographers, automaticannotations of texts (especially Web pages) are gaining a huge popularity in the Seman-tic Web vision (Berners-Lee 1999).
In order to perform automatic tagging, one or moreword sense disambiguation systems are applied, resulting in a semantically enhancedresource.
Unfortunately, even when dealing with restricted sense inventories or selecteddomains, automated systems can make mistakes in the sense assignment, also due tothe difficulty in training a supervised program with a sufficient number of annotatedinstances and again the fine granularity of the dictionary inventory.The recognition of intuitive and convincing interconnection patterns reinforces aconsistent choice of senses throughout the discourse, a desirable condition for guaran-teeing semantic coherence.
For example, semantic interconnections can help deal withpartially justifiable, but incorrect, interpretations for words in context.
Consider forinstance the sentence from the Senseval-3 English all-words competition:(c) The driver stopped swearing at them, turned on his heel and went back tohis truck.A partial interpretation of driver and heel can be provided in the golf domain (aheel#6 is part of a driver#5).
This can be a reasonable choice for a word sense disam-biguator, but the overall semantic graph exposes a poor structural quality.
A differentchoice of senses pointed out by Valido (driver as an operator of a vehicle and heel asthe back part of the foot) provides a more interconnected structure (among others,driver#1 ?
?related?to motor vehicle#1 ?
?kind?of truck#1, turn ?
v#1 ?
?related?to heel#2, etc.
).3.3 Weaknesses of the ApproachIt can happen that semantic interconnection patterns proposed by the validation toolconvey weak suggestions due to the lack of structure in the lexical knowledge baseused to extract patterns like those in Table 1.
In that case, the validator is expected toreject the possible suggestion and make a more reasonable choice.
As a result, if nointeresting proposal is provided to the validator, it is less likely that the final choice willbe inconsistent with the lexicon structure.
Typical examples are:(d) A payment was made last week.
(e) I spent three days in that hospital.WordNet encodes two senses of payment: the sum of money paid (sense #1) and theact of paying money (sense #2).
Such regular polysemy makes it hard to converge on asense choice for payment in sentence (d).
This difficulty is also manifested in the anno-tations of similar expressions involving make and payment within SemCor.
Furthermore,278Navigli Consistent Validation of Manual and Automatic Sense AnnotationsTable 2Precision and recall of the Valido tool in the appropriateness of its suggestions for 360 words.Part of speech Precision Recall F1 measureNouns 73.83% (79/107) 65.83% (79/120) 69.60%Adjectives 89.29% (25/28) 20.83% (25/120) 33.78%Verbs 82.14% (69/84) 57.50% (69/120) 67.65%Total 79.00% (173/219) 48.05% (173/360) 59.76%apart from the distinction between the act of doing the action and the amount of moneypaid, there are not many structural suggestions that allow us to distinguish betweenthe two senses.
Semantic interconnection patterns cannot help the validator here, butany choice will not violate the structural consistency of the lexicon.
As for sentence (e),WordNet encodes two senses for hospital: the building where patients receive treatment(sense #1) and the medical institution (sense #2).
This case is diametrically oppositein that here WordNet encodes much information about both senses, but such ?noisy?knowledge does not help discriminate.
As a result, a number of semantic interconnec-tion patterns are presented to the validator, indicating the relevance of both senses fortagging, but no evidence in favor of the choice of sense #1 (which is most appropriatein the sentence).4.
EvaluationWe performed an evaluation of the tool on SemCor (Miller et al 1993), a selection ofdocuments from the Brown Corpus where each content word is annotated with concepts(specifically, synsets) from the WordNet inventory.The objective of our evaluation is to show that Valido constitutes good supportfor a validator in detecting bad or inconsistent annotations.
A total of 360 sentencesof average length (9 or 10 content words) were uniformly selected from the set ofdocuments in the SemCor corpus.
The average ambiguity of an arbitrary word in thedata set was 5.77, while the average ambiguity of the most ambiguous word in asentence was 8.70.For each sentence ?
= w1w2 .
.
.wn annotated in SemCor with the sensessw1 sw2 .
.
.
swn (swi ?
Senses(wi), i ?
{1, 2, .
.
.
, n}), we identified the most ambiguous wordwi ?
?, and randomly chose a different sense swi for that word, that is, swi ?
Senses(wi) \{swi}.
The experiment simulates in vitro a situation in which, for each sentence, theannotators agree on which sense to assign to all the words but one, where one annotatorprovides an appropriate sense and the other selects a different sense.
The random factorguarantees an approximation to the uniform distribution in the test set of all the possibledegrees of disagreement between sense annotators (ranging from regular polysemy tohomonymy).We applied Valido with validation policy (?)
to the annotated sentences and evalu-ated the performance of the tool in suggesting the appropriate choice for the words withdisagreement.
We assessed precision (the number of correct suggestions over the overallnumber of suggestions from the Valido tool), recall (the number of correct suggestionsover the total number of words to be validated), and the F1 measure( 2prp+r).The results are reported in Table 2 for nouns, adjectives, and verbs (we neglectedadverbs, as very few interconnections can be found for them).
The experiment shows279Computational Linguistics Volume 32, Number 2that evidences of inconsistency are provided by the tool with good precision (and agood F1 measure, especially for nouns and verbs, beating the random baseline of 50%).Notice that this test differs from the typical evaluation of word sense disambiguationtasks, like the Senseval exercises (http://www.senseval.org), in that we are assessinghighly polysemous (possibly, very fine grained) words.
Comparing the results with asmart baseline, like the most frequent sense heuristic, is not feasible in this experiment,as the frequency of WordNet senses was calculated on the same data set (i.e., SemCor).Notice anyway that beating a baseline is not necessarily our objective if we are not ableto provide justifications (like semantic graphs) of which the human validator can takeadvantage in order to take the final decision.The low recall resulting for parts of speech other than nouns (mainly, adjectives)is due to a lack of connectivity in the lexical knowledge base, especially when dealingwith connections across different parts of speech.
This is a problem already discussed inNavigli and Velardi (2005) and partially taken into account in Navigli (2005).
Valido canindeed be used as a tool to collect new, consistent collocations that could grow the LKBfrom which the semantic interconnection patterns are extracted, possibly in an iterativeprocess.
We plan to investigate this topic in the near future.5.
ConclusionsIn this article we discussed a tool, Valido, for supporting validators in the difficult taskof assessing the quality of both manual and automatic sense assignments.
The validatorcan analyze the correctness of a sense choice in terms of its structural semantic inter-connections (SSI) with respect to the other word senses chosen in context.
The use ofsemantic interconnection patterns to support validation allows one to smooth possibledivergences between the annotators and to corroborate choices consistent with the LKB.Furthermore, the method is independent of the adopted lexicon (i.e., WordNet), in thatpatterns can be derived from any sufficiently rich ontological resource.
Moreover, theapproach allows the validator to discover mistakes in the lexicon: For instance, thesemantic graphs analyzed in a number of experiments helped us find out that a Swisscanton#1 is not a Chinese city (canton#1) but a division of a country (canton#2), that a malehorse should be a kind of horse, that carelessness is not a kind of attentiveness, but ratherthe contrary, and so on.
These inconsistencies of WordNet 2.0 were promptly reportedto the resource maintainers, and most of them have been corrected in the latest versionof the lexicon.Finally, we would like to point out that, in the future, the tool could also be usedduring the annotation phase by taggers looking for suggestions based on the structureof the LKB, with the result of improving the coherence and awareness of the decisionsto be taken.ReferencesAgirre, Eneko and German Rigau.
1996.Word sense disambiguation usingconceptual density.
In Proceedingsof COLING 1996, pages 16?22,Copenhagen, Denmark.Berners-Lee, Tim.
1999.
Weaving the Web.Harper, San Francisco, CA.Chklovski, Tim and Rada Mihalcea.
2002.Building a sense tagged corpus withOpen Mind Word Expert.
In Proceedingsof ACL 2002 Workshop on Word SenseDisambiguation: Recent Successes and FutureDirections, pages 116?122, Philadelphia, PA.Fellbaum, Christiane, editor.
1998.
WordNet:An Electronic Lexical Database.
MIT Press,Cambridge, MA.Hirst, Graeme and David St-Onge.
1998.Lexical chains as representations ofcontext for the detection and correction280Navigli Consistent Validation of Manual and Automatic Sense Annotationsof malapropisms.
In C. Fellbaum, editor,WordNet: An Electronic Lexical Database.The MIT Press, Cambridge, MA,pages 305?332.Litkowski, Kenneth C. 2004.
SENSEVAL-3task: Word-sense disambiguation ofWordNet glosses.
In Proceedings of ACL2004 SENSEVAL-3 Workshop, pages 13?16,Barcelona, Spain.Mihalcea, Rada and Dan Moldovan.
2001.eXtended WordNet: Progress report.
InProceedings of NAACL Workshop on WordNetand Other Lexical Resources, pages 95?100,Pittsburgh, PA.Miller, George, Claudia Leacock, TengiRandee, and Ross Bunker.
1993.
Asemantic concordance.
In Proceedings 3rdDARPA Workshop on Human LanguageTechnology, Plainsboro, NJ.Morris, Jane and Graeme Hirst.
1991.
Lexicalcohesion computed by thesaural relationsas an indicator of the structure of text.Computational Linguistics, 17(1):21?48.Navigli, Roberto.
2005.
Semi-automaticextension of large-scale linguisticknowledge bases.
In Proceedings of18th FLAIRS International Conference,pages 548?553, Clearwater Beach, FL,May 16?18, 2005.Navigli, Roberto and Paola Velardi.
2004.Learning domain ontologies fromdocument warehouses and dedicatedwebsites, Computational Linguistics,30(2):151?179.Navigli, Roberto and Paola Velardi.2005.
Structural semantic interconnections:a knowledge-based approach toword sense disambiguation.
IEEETransactions on Pattern Analysis andMachine Intelligence (PAMI), 27(7):1075?1086.Rada, Roy, Hafedh Mili, Ellen Bickell, andMaria Blettner.
1989.
Development andapplication of a metric on semantic nets.IEEE Transactions on Systems, Man andCybernetics, 19(1):17?30.281
