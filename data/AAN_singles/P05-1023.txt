Proceedings of the 43rd Annual Meeting of the ACL, pages 181?188,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsData-Defined Kernels for Parse RerankingDerived from Probabilistic ModelsJames HendersonSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, United Kingdomjames.henderson@ed.ac.ukIvan TitovDepartment of Computer ScienceUniversity of Geneva24, rue Ge?ne?ral DufourCH-1211 Gene`ve 4, Switzerlandivan.titov@cui.unige.chAbstractPrevious research applying kernel meth-ods to natural language parsing have fo-cussed on proposing kernels over parsetrees, which are hand-crafted based on do-main knowledge and computational con-siderations.
In this paper we propose amethod for defining kernels in terms ofa probabilistic model of parsing.
Thismodel is then trained, so that the param-eters of the probabilistic model reflect thegeneralizations in the training data.
Themethod we propose then uses these trainedparameters to define a kernel for rerank-ing parse trees.
In experiments, we usea neural network based statistical parseras the probabilistic model, and use theresulting kernel with the Voted Percep-tron algorithm to rerank the top 20 parsesfrom the probabilistic model.
This methodachieves a significant improvement overthe accuracy of the probabilistic model.1 IntroductionKernel methods have been shown to be very ef-fective in many machine learning problems.
Theyhave the advantage that learning can try to optimizemeasures related directly to expected testing perfor-mance (i.e.
?large margin?
methods), rather thanthe probabilistic measures used in statistical models,which are only indirectly related to expected test-ing performance.
Work on kernel methods in naturallanguage has focussed on the definition of appropri-ate kernels for natural language tasks.
In particu-lar, most of the work on parsing with kernel meth-ods has focussed on kernels over parse trees (Collinsand Duffy, 2002; Shen and Joshi, 2003; Shen etal., 2003; Collins and Roark, 2004).
These kernelshave all been hand-crafted to try reflect propertiesof parse trees which are relevant to discriminatingcorrect parse trees from incorrect ones, while at thesame time maintaining the tractability of learning.Some work in machine learning has taken an al-ternative approach to defining kernels, where thekernel is derived from a probabilistic model of thetask (Jaakkola and Haussler, 1998; Tsuda et al,2002).
This way of defining kernels has two ad-vantages.
First, linguistic knowledge about parsingis reflected in the design of the probabilistic model,not directly in the kernel.
Designing probabilisticmodels to reflect linguistic knowledge is a processwhich is currently well understood, both in terms ofreflecting generalizations and controlling computa-tional cost.
Because many NLP problems are un-bounded in size and complexity, it is hard to specifyall possible relevant kernel features without havingso many features that the computations become in-tractable and/or the data becomes too sparse.1 Sec-ond, the kernel is defined using the trained param-eters of the probabilistic model.
Thus the kernel isin part determined by the training data, and is auto-matically tailored to reflect properties of parse treeswhich are relevant to parsing.1For example, see (Henderson, 2004) for a discussion ofwhy generative models are better than models parameterized toestimate the a posteriori probability directly.181In this paper, we propose a new method for de-riving a kernel from a probabilistic model which isspecifically tailored to reranking tasks, and we ap-ply this method to natural language parsing.
For theprobabilistic model, we use a state-of-the-art neuralnetwork based statistical parser (Henderson, 2003).The resulting kernel is then used with the Voted Per-ceptron algorithm (Freund and Schapire, 1998) toreranking the top 20 parses from the probabilisticmodel.
This method achieves a significant improve-ment over the accuracy of the probabilistic modelalone.2 Kernels Derived from ProbabilisticModelsIn recent years, several methods have been proposedfor constructing kernels from trained probabilisticmodels.
As usual, these kernels are then used withlinear classifiers to learn the desired task.
As well assome empirical successes, these methods are moti-vated by theoretical results which suggest we shouldexpect some improvement with these classifiers overthe classifier which chooses the most probable an-swer according to the probabilistic model (i.e.
themaximum a posteriori (MAP) classifier).
There isguaranteed to be a linear classifier for the derivedkernel which performs at least as well as the MAPclassifier for the probabilistic model.
So, assuminga large-margin classifier can optimize a more ap-propriate criteria than the posterior probability, weshould expect the derived kernel?s classifier to per-form better than the probabilistic model?s classifier,although empirical results on a given task are neverguaranteed.In this section, we first present two previous ker-nels and then propose a new kernel specifically forreranking tasks.
In each of these discussions weneed to characterize the parsing problem as a classi-fication task.
Parsing can be regarded as a mappingfrom an input space of sentences x?X to a struc-tured output space of parse trees y?Y .
On the basisof training sentences, we learn a discriminant func-tion F : X ?
Y ?
R. The parse tree y with thelargest value for this discriminant function F (x, y)is the output parse tree for the sentence x.
We focuson the linear discriminant functions:Fw(x, y) = <w,?
(x, y)>,where ?
(x, y) is a feature vector for the sentence-tree pair, w is a parameter vector for the discrim-inant function, and <a, b> is the inner product ofvectors a and b.
In the remainder of this section, wewill characterize the kernel methods we consider interms of the feature extractor ?
(x, y).2.1 Fisher KernelsThe Fisher kernel (Jaakkola and Haussler, 1998) isone of the best known kernels belonging to the classof probability model based kernels.
Given a genera-tive model of P (z|??)
with smooth parameterization,the Fisher score of an example z is a vector of partialderivatives of the log-likelihood of the example withrespect to the model parameters:???
(z) = (?logP (z|??)?
?1, .
.
.
, ?logP (z|??)?
?l ).This score can be regarded as specifying how themodel should be changed in order to maximize thelikelihood of the example z.
Then we can define thesimilarity between data points as the inner productof the corresponding Fisher scores.
This kernel isoften referred to as the practical Fisher kernel.
Thetheoretical Fisher kernel depends on the Fisher in-formation matrix, which is not feasible to computefor most practical tasks and is usually omitted.The Fisher kernel is only directly applicable tobinary classification tasks.
We can apply it to ourtask by considering an example z to be a sentence-tree pair (x, y), and classifying the pairs into cor-rect parses versus incorrect parses.
When we use theFisher score ???
(x, y) in the discriminant function F ,we can interpret the value as the confidence that thetree y is correct, and choose the y in which we arethe most confident.2.2 TOP KernelsTsuda (2002) proposed another kernel constructedfrom a probabilistic model, called the Tangent vec-tors Of Posterior log-odds (TOP) kernel.
Their TOPkernel is also only for binary classification tasks, so,as above, we treat the input z as a sentence-tree pairand the output category c ?
{?1,+1} as incor-rect/correct.
It is assumed that the true probabilitydistribution is included in the class of probabilis-tic models and that the true parameter vector ??
isunique.
The feature extractor of the TOP kernel for182the input z is defined by:???
(z) = (v(z, ??),?v(z,??)?
?1, .
.
.
, ?v(z,??)?
?l ),where v(z, ??)
= logP (c=+1|z, ??)
?logP (c=?1|z, ??
).In addition to being at least as good as theMAP classifier, the choice of the TOP kernel fea-ture extractor is motivated by the minimization ofthe binary classification error of a linear classifier<w,???
(z)> + b. Tsuda (2002) demonstrates thatthis error is closely related to the estimation error ofthe posterior probability P (c=+1|z, ??)
by the esti-mator g(<w,???
(z)> + b), where g is the sigmoidfunction g(t) = 1/(1 + exp (?t)).The TOP kernel isn?t quite appropriate for struc-tured classification tasks because ???
(z) is motivatedby binary classificaton error minimization.
In thenext subsection, we will adapt it to structured classi-fication.2.3 A TOP Kernel for RerankingWe define the reranking task as selecting a parse treefrom the list of candidate trees suggested by a proba-bilistic model.
Furthermore, we only consider learn-ing to rerank the output of a particular probabilisticmodel, without requiring the classifier to have goodperformance when applied to a candidate list pro-vided by a different model.
In this case, it is naturalto model the probability that a parse tree is the bestcandidate given the list of candidate trees:P (yk|x, y1, .
.
.
, ys) =P (x,yk)?tP (x,yt),where y1, .
.
.
, ys is the list of candidate parse trees.To construct a new TOP kernel for reranking, weapply an approach similar to that used for the TOPkernel (Tsuda et al, 2002), but we consider the prob-ability P (yk|x, y1, .
.
.
, ys, ??)
instead of the proba-bility P (c=+1|z, ??)
considered by Tsuda.
The re-sulting feature extractor is given by:???
(x, yk) = (v(x, yk, ??),?v(x,yk,??)?
?1, .
.
.
, ?v(x,yk,??)?
?l ),where v(x, yk, ??)
= logP (yk|y1, .
.
.
, ys, ??)
?log?t6=k P (yt|y1, .
.
.
, ys, ??).
We will call this ker-nel the TOP reranking kernel.3 The Probabilistic ModelTo complete the definition of the kernel, we needto choose a probabilistic model of parsing.
Forthis we use a statistical parser which has previouslybeen shown to achieve state-of-the-art performance,namely that proposed in (Henderson, 2003).
Thisparser has two levels of parameterization.
The firstlevel of parameterization is in terms of a history-based generative probability model, but this level isnot appropriate for our purposes because it definesan infinite number of parameters (one for every pos-sible partial parse history).
When parsing a givensentence, the bounded set of parameters which arerelevant to a given parse are estimated using a neuralnetwork.
The weights of this neural network formthe second level of parameterization.
There is a fi-nite number of these parameters.
Neural networktraining is applied to determine the values of theseparameters, which in turn determine the values ofthe probability model?s parameters, which in turndetermine the probabilistic model of parse trees.We do not use the complete set of neural networkweights to define our kernels, but instead we define athird level of parameterization which only includesthe network?s output layer weights.
These weightsdefine a normalized exponential model, with the net-work?s hidden layer as the input features.
When wetried using the complete set of weights in some smallscale experiments, training the classifier was morecomputationally expensive, and actually performedslightly worse than just using the output weights.Using just the output weights also allows us to makesome approximations in the TOP reranking kernelwhich makes the classifier learning algorithm moreefficient.3.1 A History-Based Probability ModelAs with many other statistical parsers (Ratnaparkhi,1999; Collins, 1999; Charniak, 2000), Henderson(2003) uses a history-based model of parsing.
Hedefines the mapping from phrase structure trees toparse sequences using a form of left-corner parsingstrategy (see (Henderson, 2003) for more details).The parser actions include: introducing a new con-stituent with a specified label, attaching one con-stituent to another, and predicting the next word ofthe sentence.
A complete parse consists of a se-quence of these actions, d1,..., dm, such that per-forming d1,..., dm results in a complete phrase struc-ture tree.Because this mapping to parse sequences is183one-to-one, and the word prediction actions ina complete parse d1,..., dm specify the sentence,P (d1,..., dm) is equivalent to the joint probability ofthe output phrase structure tree and the input sen-tence.
This probability can be then be decomposedinto the multiplication of the probabilities of eachaction decision di conditioned on that decision?sprior parse history d1,..., di?1.P (d1,..., dm) = ?iP (di|d1,..., di?1)3.2 Estimating Decision Probabilities with aNeural NetworkThe parameters of the above probability model arethe P (di|d1,..., di?1).
There are an infinite num-ber of these parameters, since the parse historyd1,..., di?1 grows with the length of the sentence.
Inother work on history-based parsing, independenceassumptions are applied so that only a finite amountof information from the parse history can be treatedas relevant to each parameter, thereby reducing thenumber of parameters to a finite set which can beestimated directly.
Instead, Henderson (2003) usesa neural network to induce a finite representationof this unbounded history, which we will denoteh(d1,..., di?1).
Neural network training tries to findsuch a history representation which preserves all theinformation about the history which is relevant to es-timating the desired probability.P (di|d1,..., di?1) ?
P (di|h(d1,..., di?1))Using a neural network architecture called SimpleSynchrony Networks (SSNs), the history representa-tion h(d1,..., di?1) is incrementally computed fromfeatures of the previous decision di?1 plus a finiteset of previous history representations h(d1,..., dj),j < i ?
1.
Each history representation is a finitevector of real numbers, called the network?s hiddenlayer.
As long as the history representation for po-sition i ?
1 is always included in the inputs to thehistory representation for position i, any informationabout the entire sequence could be passed from his-tory representation to history representation and beused to estimate the desired probability.
However,learning is biased towards paying more attention toinformation which passes through fewer history rep-resentations.To exploit this learning bias, structural locality isused to determine which history representations areinput to which others.
First, each history representa-tion is assigned to the constituent which is on the topof the parser?s stack when it is computed.
Then ear-lier history representations whose constituents arestructurally local to the current representation?s con-stituent are input to the computation of the correctrepresentation.
In this way, the number of represen-tations which information needs to pass through inorder to flow from history representation i to his-tory representation j is determined by the structuraldistance between i?s constituent and j?s constituent,and not just the distance between i and j in theparse sequence.
This provides the neural networkwith a linguistically appropriate inductive bias whenit learns the history representations, as explained inmore detail in (Henderson, 2003).Once it has computed h(d1,..., di?1), the SSNuses a normalized exponential to estimate a proba-bility distribution over the set of possible next deci-sions di given the history:P (di|d1,..., di?1, ?)
?exp(<?di ,h(d1,...,di?1)>)?t?N(di?1)exp(<?t,h(d1,...,di?1)>),where by ?t we denote the set of output layerweights, corresponding to the parser action t,N(di?1) defines a set of possible next parser actionsafter the step di?1 and ?
denotes the full set of modelparameters.We trained SSN parsing models, using the on-lineversion of Backpropagation to perform the gradientdescent with a maximum likelihood objective func-tion.
This learning simultaneously tries to optimizethe parameters of the output computation and the pa-rameters of the mappings h(d1,..., di?1).
With multi-layered networks such as SSNs, this training is notguaranteed to converge to a global optimum, but inpractice a network whose criteria value is close tothe optimum can be found.4 Large-Margin OptimizationOnce we have defined a kernel over parse trees, gen-eral techniques for linear classifier optimization canbe used to learn the given task.
The most sophis-ticated of these techniques (such as Support Vec-tor Machines) are unfortunately too computationallyexpensive to be used on large datasets like the PennTreebank (Marcus et al, 1993).
Instead we use a184method which has often been shown to be virtu-ally as good, the Voted Perceptron (VP) (Freund andSchapire, 1998) algorithm.
The VP algorithm wasoriginally applied to parse reranking in (Collins andDuffy, 2002) with the Tree kernel.
We modify theperceptron training algorithm to make it more suit-able for parsing, where zero-one classification lossis not the evaluation measure usually employed.
Wealso develop a variant of the kernel defined in sec-tion 2.3, which is more efficient when used with theVP algorithm.Given a list of candidate trees, we train the clas-sifier to select the tree with largest constituent F1score.
The F1 score is a measure of the similaritybetween the tree in question and the gold standardparse, and is the standard way to evaluate the accu-racy of a parser.
We denote the k?th candidate treefor the j?th sentence xj by yjk.
Without loss of gener-ality, let us assume that yj1 is the candidate tree withthe largest F1 score.The Voted Perceptron algorithm is an ensem-ble method for combining the various intermediatemodels which are produced during training a per-ceptron.
It demonstrates more stable generalizationperformance than the normal perceptron algorithmwhen the problem is not linearly separable (Freundand Schapire, 1998), as is usually the case.We modify the perceptron algorithm by introduc-ing a new classification loss function.
This modifi-cation enables us to treat differently the cases wherethe perceptron predicts a tree with an F1 score muchsmaller than that of the top candidate and the caseswhere the predicted and the top candidates have sim-ilar score values.
The natural choice for the lossfunction would be ?
(yjk, yj1) = F1(yj1) ?
F1(yjk),where F1(yjk) denotes the F1 score value for theparse tree yjk.
This approach is very similar to slackvariable rescaling for Support Vector Machines pro-posed in (Tsochantaridis et al, 2004).
The learningalgorithm we employed is presented in figure 1.When applying kernels with a large training cor-pus, we face efficiency issues because of the largenumber of the neural network weights.
Even thoughwe use only the output layer weights, this vectorgrows with the size of the vocabulary, and thus canbe large.
The kernels presented in section 2 all leadto feature vectors without many zero values.
Thisw = 0for j = 1 .. nfor k = 2 .. sif <w,?
(xj , yjk)> > <w, ?
(xj , yj1)>w = w + ?
(yjk, yj1)(?
(xj , yj1)?
?
(xj , yjk))Figure 1: The modified perceptron algorithmhappens because we compute the derivative of thenormalization factor used in the network?s estima-tion of P (di|d1,..., di?1).
This normalization factordepends on the output layer weights correspondingto all the possible next decisions (see section 3.2).This makes an application of the VP algorithm in-feasible in the case of a large vocabulary.We can address this problem by freezing thenormalization factor when computing the featurevector.
Note that we can rewrite the model log-probability of the tree as:logP (y|?)
=?i log (exp(<?di ,h(d1,...,di?1)>)?t?N(di?1)exp(<?t,h(d1,...,di?1)>)) =?i(<?di , h(d1,..., di?1)>)?
?i log?t?N(di?1) exp(<?t, h(d1,..., di?1)>).We treat the parameters used to compute the firstterm as different from the parameters used to com-pute the second term, and we define our kernel onlyusing the parameters in the first term.
This meansthat the second term does not effect the derivativesin the formula for the feature vector ?
(x, y).
Thusthe feature vector for the kernel will contain non-zero entries only in the components correspondingto the parser actions which are present in the candi-date derivation for the sentence, and thus in the firstvector component.
We have applied this techniqueto the TOP reranking kernel, the result of which wewill call the efficient TOP reranking kernel.5 The Experimental ResultsWe used the Penn Treebank WSJ corpus (Marcus etal., 1993) to perform empirical experiments on theproposed parsing models.
In each case the input tothe network is a sequence of tag-word pairs.2 We re-port results for two different vocabulary sizes, vary-ing in the frequency with which tag-word pairs must2We used a publicly available tagger (Ratnaparkhi, 1996) toprovide the tags.185occur in the training set in order to be included ex-plicitly in the vocabulary.
A frequency threshold of200 resulted in a vocabulary of 508 tag-word pairs(including tag-unknown word pairs) and a thresholdof 20 resulted in 4215 tag-word pairs.
We denotethe probabilistic model trained with the vocabularyof 508 by the SSN-Freq?200, the model trained withthe vocabulary of 4215 by the SSN-Freq?20.Testing the probabilistic parser requires using abeam search through the space of possible parses.We used a form of beam search which prunes thesearch after the prediction of each word.
We set thewidth of this post-word beam to 40 for both testingof the probabilistic model and generating the candi-date list for reranking.
For training and testing ofthe kernel models, we provided a candidate list con-sisting of the top 20 parses found by the generativeprobabilistic model.
When using the Fisher kernel,we added the log-probability of the tree given by theprobabilistic model as the feature.
This was not nec-essary for the TOP kernels because they already con-tain a feature corresponding to the probability esti-mated by the probabilistic model (see section 2.3).We trained the VP model with all three kernelsusing the 508 word vocabulary (Fisher-Freq?200,TOP-Freq?200, TOP-Eff-Freq?200) but only the ef-ficient TOP reranking kernel model was trained withthe vocabulary of 4215 words (TOP-Eff-Freq?20).The non-sparsity of the feature vectors for other ker-nels led to the excessive memory requirements andlarger testing time.
In each case, the VP model wasrun for only one epoch.
We would expect some im-provement if running it for more epochs, as has beenempirically demonstrated in other domains (Freundand Schapire, 1998).To avoid repeated testing on the standard testingset, we first compare the different models with theirperformance on the validation set.
Note that the val-idation set wasn?t used during learning of the kernelmodels or for adjustment of any parameters.Standard measures of accuracy are shown in ta-ble 1.3 Both the Fisher kernel and the TOP kernelsshow better accuracy than the baseline probabilistic3All our results are computed with the evalb program fol-lowing the standard criteria in (Collins, 1999), and using thestandard training (sections 2?22, 39,832 sentences, 910,196words), validation (section 24, 1346 sentence, 31507 words),and testing (section 23, 2416 sentences, 54268 words) sets(Collins, 1999).LR LP F?=1SSN-Freq?200 87.2 88.5 87.8Fisher-Freq?200 87.2 88.8 87.9TOP-Freq?200 87.3 88.9 88.1TOP-Eff-Freq?200 87.3 88.9 88.1SSN-Freq?20 88.1 89.2 88.6TOP-Eff-Freq?20 88.2 89.7 88.9Table 1: Percentage labeled constituent recall (LR),precision (LP), and a combination of both (F?=1) onvalidation set sentences of length at most 100.model, but only the improvement of the TOP kernelsis statistically significant.4 For the TOP kernel, theimprovement over baseline is about the same withboth vocabulary sizes.
Also note that the perfor-mance of the efficient TOP reranking kernel is thesame as that of the original TOP reranking kernel,for the smaller vocabulary.For comparison to previous results, table 2 liststhe results on the testing set for our best model(TOP-Efficient-Freq?20) and several other statisti-cal parsers (Collins, 1999; Collins and Duffy, 2002;Collins and Roark, 2004; Henderson, 2003; Char-niak, 2000; Collins, 2000; Shen and Joshi, 2004;Shen et al, 2003; Henderson, 2004; Bod, 2003).First note that the parser based on the TOP efficientkernel has better accuracy than (Henderson, 2003),which used the same parsing method as our base-line model, although the trained network parameterswere not the same.
When compared to other kernelmethods, our approach performs better than thosebased on the Tree kernel (Collins and Duffy, 2002;Collins and Roark, 2004), and is only 0.2% worsethan the best results achieved by a kernel method forparsing (Shen et al, 2003; Shen and Joshi, 2004).6 Related WorkThe first application of kernel methods to parsingwas proposed by Collins and Duffy (2002).
Theyused the Tree kernel, where the features of a tree areall its connected tree fragments.
The VP algorithmwas applied to rerank the output of a probabilisticmodel and demonstrated an improvement over thebaseline.4We measured significance with the randomized signifi-cance test of (Yeh, 2000).186LR LP F?=1?Collins99 88.1 88.3 88.2Collins&Duffy02 88.6 88.9 88.7Collins&Roark04 88.4 89.1 88.8Henderson03 88.8 89.5 89.1Charniak00 89.6 89.5 89.5TOP-Eff-Freq?20 89.1 90.1 89.6Collins00 89.6 89.9 89.7Shen&Joshi04 89.5 90.0 89.8Shen et al03 89.7 90.0 89.8Henderson04 89.8 90.4 90.1Bod03 90.7 90.8 90.7* F?=1 for previous models may have rounding errors.Table 2: Percentage labeled constituent recall (LR),precision (LP), and a combination of both (F?=1) onthe entire testing set.Shen and Joshi (2003) applied an SVM basedvoting algorithm with the Preference kernel definedover pairs for reranking.
To define the Preferencekernel they used the Tree kernel and the Linear ker-nel as its underlying kernels and achieved state-of-the-art results with the Linear kernel.In (Shen et al, 2003) it was pointed out thatmost of the arbitrary tree fragments allowed by theTree kernel are linguistically meaningless.
The au-thors suggested the use of Lexical Tree AdjoiningGrammar (LTAG) based features as a more linguis-tically appropriate set of features.
They empiri-cally demonstrated that incorporation of these fea-tures helps to improve reranking performance.Shen and Joshi (2004) proposed to improve mar-gin based methods for reranking by defining themargin not only between the top tree and all theother trees in the candidate list but between all thepairs of parses in the ordered candidate list for thegiven sentence.
They achieved the best results whentraining with an uneven margin scaled by the heuris-tic function of the candidates positions in the list.One potential drawback of this method is that itdoesn?t take into account the actual F1 score of thecandidate and considers only the position in the listordered by the F1 score.
We expect that an im-provement could be achieved by combining our ap-proach of scaling updates by the F1 loss with theall pairs approach of (Shen and Joshi, 2004).
Useof the F1 loss function during training demonstratedbetter performance comparing to the 0-1 loss func-tion when applied to a structured classification task(Tsochantaridis et al, 2004).All the described kernel methods are limited tothe reranking of candidates from an existing parserdue to the complexity of finding the best parse givena kernel (i.e.
the decoding problem).
(Taskar etal., 2004) suggested a method for maximal mar-gin parsing which employs the dynamic program-ming approach to decoding and parameter estima-tion problems.
The efficiency of dynamic program-ming means that the entire space of parses can beconsidered, not just a candidate list.
However, notall kernels are suitable for this method.
The dy-namic programming approach requires the featurevector of a tree to be decomposable into a sum overparts of the tree.
In particular, this is impossible withthe TOP and Fisher kernels derived from the SSNmodel.
Also, it isn?t clear whether the algorithmremains tractable for a large training set with longsentences, since the authors only present results forsentences of length less than or equal to 15.7 ConclusionsThis paper proposes a method for deriving a ker-nel for reranking from a probabilistic model, anddemonstrates state-of-the-art accuracy when thismethod is applied to parse reranking.
Contrary tomost of the previous research on kernel methods inparsing, linguistic knowledge does not have to be ex-pressed through a list of features, but instead can beexpressed through the design of a probability model.The parameters of this probability model are thentrained, so that they reflect what features of trees arerelevant to parsing.
The kernel is then derived fromthis trained model in such a way as to maximize itsusefulness for reranking.We performed experiments on parse reranking us-ing a neural network based statistical parser as boththe probabilistic model and the source of the listof candidate parses.
We used a modification ofthe Voted Perceptron algorithm to perform rerankingwith the kernel.
The results were amongst the bestcurrent statistical parsers, and only 0.2% worse thanthe best current parsing methods which use kernels.We would expect further improvement if we useddifferent models to derive the kernel and to gener-187ate the candidates, thereby exploiting the advantagesof combining multiple models, as do the better per-forming methods using kernels.In recent years, probabilistic models have becomecommonplace in natural language processing.
Webelieve that this approach to defining kernels wouldsimplify the problem of defining kernels for thesetasks, and could be very useful for many of them.In particular, maximum entropy models also use anormalized exponential function to estimate proba-bilities, so all the methods discussed in this paperwould be applicable to maximum entropy models.This approach would be particularly useful for taskswhere there is less data available than in parsing, forwhich large-margin methods work particularly well.ReferencesRens Bod.
2003.
An efficient implementation of a newDOP model.
In Proc.
10th Conf.
of European Chap-ter of the Association for Computational Linguistics,Budapest, Hungary.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proc.
1st Meeting of North AmericanChapter of Association for Computational Linguistics,pages 132?139, Seattle, Washington.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures and the voted perceptron.
In Proc.40th Meeting of Association for Computational Lin-guistics, pages 263?270.Michael Collins and Brian Roark.
2004.
Incrementalparsing with the perceptron algorithm.
In Proc.
42thMeeting of Association for Computational Linguistics,Barcelona, Spain.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania, Philadelphia, PA.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proc.
17th Int.
Conf.
on Ma-chine Learning, pages 175?182, Stanford, CA.Yoav Freund and Robert E. Schapire.
1998.
Largemargin classification using the perceptron algorithm.In Proc.
of the 11th Annual Conf.
on ComputationalLearning Theory, pages 209?217, Madisson WI.James Henderson.
2003.
Inducing history representa-tions for broad coverage statistical parsing.
In Proc.joint meeting of North American Chapter of the Asso-ciation for Computational Linguistics and the HumanLanguage Technology Conf., pages 103?110, Edmon-ton, Canada.James Henderson.
2004.
Discriminative training ofa neural network statistical parser.
In Proc.
42ndMeeting of Association for Computational Linguistics,Barcelona, Spain.Tommi S. Jaakkola and David Haussler.
1998.
Ex-ploiting generative models in discriminative classi-fiers.
Advances in Neural Information Processes Sys-tems 11.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19(2):313?330.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Proc.
Conf.
on Empir-ical Methods in Natural Language Processing, pages133?142, Univ.
of Pennsylvania, PA.Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34:151?175.Libin Shen and Aravind K. Joshi.
2003.
An SVM basedvoting algorithm with application to parse reranking.In Proc.
of the 7th Conf.
on Computational NaturalLanguage Learning, pages 9?16, Edmonton, Canada.Libin Shen and Aravind K. Joshi.
2004.
Flexible marginselection for reranking with full pairwise samples.
InProc.
of the 1st Int.
Joint Conf.
on Natural LanguageProcessing, Hainan Island, China.Libin Shen, Anoop Sarkar, and Aravind K. Joshi.
2003.Using LTAG based features in parse reranking.
InProc.
of Conf.
on Empirical Methods in Natural Lan-guage Processing, Sapporo, Japan.Ben Taskar, Dan Klein, Michael Collins, Daphne Koller,and Christopher Manning.
2004.
Max-margin pars-ing.
In Proc.
Conf.
on Empirical Methods in NaturalLanguage Processing, Barcelona, Spain.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and structuredoutput spaces.
In Proc.
21st Int.
Conf.
on MachineLearning, pages 823?830, Banff, Alberta, Canada.K.
Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg,and K. Muller.
2002.
A new discriminative ker-nel from probabilistic models.
Neural Computation,14(10):2397?2414.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of the result differences.
In Proc.17th International Conf.
on Computational Linguis-tics, pages 947?953, Saarbruken, Germany.188
