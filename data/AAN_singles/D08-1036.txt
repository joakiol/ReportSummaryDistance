Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 344?352,Honolulu, October 2008. c?2008 Association for Computational LinguisticsA comparison of Bayesian estimators forunsupervised Hidden Markov Model POS taggersJianfeng GaoMicrosoft ResearchRedmond, WA, USAjfgao@microsoft.comMark JohnsonBrown UniveristyProvidence, RI, USAMark?Johnson@Brown.eduAbstractThere is growing interest in applying Bayesiantechniques to NLP problems.
There are anumber of different estimators for Bayesianmodels, and it is useful to know what kinds oftasks each does well on.
This paper comparesa variety of different Bayesian estimators forHidden Markov Model POS taggers with var-ious numbers of hidden states on data sets ofdifferent sizes.
Recent papers have given con-tradictory results when comparing Bayesianestimators to Expectation Maximization (EM)for unsupervised HMM POS tagging, and weshow that the difference in reported results islargely due to differences in the size of thetraining data and the number of states in theHMM.
We invesigate a variety of samplers forHMMs, including some that these earlier pa-pers did not study.
We find that all of Gibbssamplers do well with small data sets and fewstates, and that Variational Bayes does wellon large data sets and is competitive with theGibbs samplers.
In terms of times of conver-gence, we find that Variational Bayes was thefastest of all the estimators, especially on largedata sets, and that explicit Gibbs sampler (bothpointwise and sentence-blocked) were gener-ally faster than their collapsed counterparts onlarge data sets.1 IntroductionProbabilistic models now play a central role in com-putational linguistics.
These models define a prob-ability distribution P(x) over structures or analysesx.
For example, in the part-of-speech (POS) tag-ging application described in this paper, which in-volves predicting the part-of-speech tag ti of eachword wi in the sentence w = (w1, .
.
.
, wn), thestructure x = (w, t) consists of the words w in asentence together with their corresponding parts-of-speech t = (t1, .
.
.
, tn).In general the probabilistic models used in com-putational linguistics have adjustable parameters ?which determine the distribution P(x | ?).
In thispaper we focus on bitag Hidden Markov Models(HMMs).
Since our goal here is to compare algo-rithms rather than achieve the best performance, wekeep the models simple by ignoring morphology andcapitalization (two very strong cues in English) andtreat each word as an atomic entity.
This means thatthe model parameters ?
consist of the HMM state-to-state transition probabilities and the state-to-wordemission probabilities.In virtually all statistical approaches the parame-ters ?
are chosen or estimated on the basis of trainingdata d. This paper studies unsupervised estimation,so d = w = (w1, .
.
.
, wn) consists of a sequenceof words wi containing all of the words of trainingcorpus appended into a single string, as explainedbelow.Maximum Likelihood (ML) is the most commonestimation method in computational linguistics.
AMaximum Likelihood estimator sets the parametersto the value ??
that makes the likelihood Ld of thedata d as large as possible:Ld(?)
= P(d | ?)??
= argmax?Ld(?
)In this paper we use the Inside-Outside algo-rithm, which is a specialized form of Expectation-344Maximization, to find HMM parameters which (atleast locally) maximize the likelihood function Ld.Recently there is increasing interest in Bayesianmethods in computational linguistics, and the pri-mary goal of this paper is to compare the perfor-mance of various Bayesian estimators with eachother and with EM.A Bayesian approach uses Bayes theorem to fac-torize the posterior distribution P(?
| d) into thelikelihood P(d | ?)
and the prior P(?).P(?
| d) ?
P(d | ?)
P(?
)Priors can be useful because they can express pref-erences for certain types of models.
To take anexample from our POS-tagging application, mostwords belong to relatively few parts-of-speech (e.g.,most words belong to a single POS, and while thereare some words which are both nouns and verbs,very few are prepositions and adjectives as well).One might express this using a prior which prefersHMMs in which the state-to-word emissions aresparse, i.e., each state emits few words.
An appro-priate Dirichlet prior can express this preference.While it is possible to use Bayesian inference tofind a single model, such as the Maximum A Pos-teriori or MAP value of ?
which maximizes theposterior P(?
| d), this is not necessarily the bestapproach (Bishop, 2006; MacKay, 2003).
Instead,rather than commiting to a single value for the pa-rameters ?
many Bayesians often prefer to workwith the full posterior distribution P(?
| d), as thisnaturally reflects the uncertainty in ?
?s value.In all but the simplest models there is no knownclosed form for the posterior distribution.
However,the Bayesian literature describes a number of meth-ods for approximating the posterior P(?
| d).
MonteCarlo sampling methods and Variational Bayes aretwo kinds of approximate inference methods thathave been applied to Bayesian inference of unsu-pervised HMM POS taggers (Goldwater and Grif-fiths, 2007; Johnson, 2007).
These methods can alsobe used to approximate other distributions that areimportant to us, such as the conditional distributionP(t | w) of POS tags (i.e., HMM hidden states) tgiven words w.This recent literature reports contradictory resultsabout these Bayesian inference methods.
John-son (2007) compared two Bayesian inference algo-rithms, Variational Bayes and what we call here apoint-wise collapsed Gibbs sampler, and found thatVariational Bayes produced the best solution, andthat the Gibbs sampler was extremely slow to con-verge and produced a worse solution than EM.
Onthe other hand, Goldwater and Griffiths (2007) re-ported that the same kind of Gibbs sampler producedmuch better results than EM on their unsupervisedPOS tagging task.
One of the primary motivationsfor this paper was to understand and resolve the dif-ference in these results.
We replicate the results ofboth papers and show that the difference in their re-sults stems from differences in the sizes of the train-ing data and numbers of states in their models.It turns out that the Gibbs sampler used in theseearlier papers is not the only kind of sampler forHMMs.
This paper compares the performance offour different kinds of Gibbs samplers, VariationalBayes and Expectation Maximization on unsuper-vised POS tagging problems of various sizes.
Ourgoal here is to try to learn how the performance ofthese different estimators varies as we change thenumber of hidden states in the HMMs and the sizeof the training data.In theory, the Gibbs samplers produce streamsof samples that eventually converge on the trueposterior distribution, while the Variational Bayes(VB) estimator only produces an approximation tothe posterior.
However, as the size of the trainingdata distribution increases the likelihood functionand therefore the posterior distribution becomes in-creasingly peaked, so one would expect this varia-tional approximation to become increasingly accu-rate.
Further the Gibbs samplers used in this papershould exhibit reduced mobility as the size of train-ing data increases, so as the size of the training dataincreases eventually the Variational Bayes estimatorshould prove to be superior.However the two point-wise Gibbs samplers in-vestigated here, which resample the label of eachword conditioned on the labels of its neighbours(amongst other things) only require O(m) steps persample (where m is the number of HMM states),while EM, VB and the sentence-blocked Gibbs sam-plers require O(m2) steps per sample.
Thus forHMMs with many states it is possible to perform oneor two orders of magnitude more iterations of the345point-wise Gibbs samplers in the same run-time asthe other samplers, so it is plausible that they wouldyield better results.2 Inference for HMMsThere are a number of excellent textbook presen-tations of Hidden Markov Models (Jelinek, 1997;Manning and Schu?tze, 1999), so we do not presentthem in detail here.
Conceptually, a Hidden MarkovModel uses a Markov model to generate the se-quence of states t = (t1, .
.
.
, tn) (which will be in-terpreted as POS tags), and then generates each wordwi conditioned on the corresponding state ti.We insert endmarkers at the beginning and endof the corpus and between sentence boundaries,and constrain the estimators to associate endmarkerswith a special HMM state that never appears else-where in the corpus (we ignore these endmarkersduring evaluation).
This means that we can formallytreat the training corpus as one long string, yet eachsentence can be processed independently by a first-order HMM.In more detail, the HMM is specified by a pair ofmultinomials ?t and ?t associated with each state t,where ?t specifies the distribution over states t?
fol-lowing t and ?t specifies the distribution over wordsw given state t.ti | ti?1 = t ?
Multi(?t)wi | ti = t ?
Multi(?t)(1)The Bayesian model we consider here puts a fixeduniform Dirichlet prior on these multinomials.
Be-cause Dirichlets are conjugate to multinomials, thisgreatly simplifies inference.
?t | ?
?
Dir(?
)?t | ??
?
Dir(??
)A multinomial ?
is distributed according to theDirichlet distribution Dir(?)
iff:P(?
| ?)
?m?j=1?
?j?1jIn our experiments we set ?
and ??
to the uniformvalues (i.e., all components have the same value ?
or??
), but it is possible to estimate these as well (Gold-water and Griffiths, 2007).
Informally, ?
controlsthe sparsity of the state-to-state transition probabil-ities while ??
controls the sparsity of the state-to-word emission probabilities.
As ??
approaches zerothe prior strongly prefers models in which each stateemits as few words as possible, capturing the intu-ition that most word types only belong to one POSmentioned earlier.2.1 Expectation MaximizationExpectation-Maximization is a procedure that iter-atively re-estimates the model parameters (?,?
),converging on a local maximum of the likelihood.Specifically, if the parameter estimate at iteration `is (?(`),?
(`)), then the re-estimated parameters at it-eration `+ 1 are:?
(`+1)t?|t = E[nt?,t]/E[nt] (2)?
(`+1)w|t = E[n?w,t]/E[nt]where n?w,t is the number of times word w occurswith state t, nt?,t is the number of times state t?
fol-lows t and nt is the number of occurences of state t;all expectations are taken with respect to the model(?(`),?
(`)).The experiments below used the Forward-Backward algorithm (Jelinek, 1997), which is a dy-namic programming algorithm for calculating thelikelihood and the expectations in (2) in O(nm2)time, where n is the number of words in the train-ing corpus and m is the number of HMM states.2.2 Variational BayesVariational Bayesian inference attempts to find afunction Q(t,?,?)
that minimizes an upper bound(3) to the negative log likelihood.?
log P(w)= ?
log?Q(t,?,?
)P(w, t,?,?
)Q(t, ?, ?)
dt d?
d??
??Q(t,?,?)
log P(w, t,?,?)Q(t,?,?)
dt d?
d?
(3)The upper bound (3) is called the Variational FreeEnergy.
We make a ?mean-field?
assumption thatthe posterior can be well approximated by a factor-ized model Q in which the state sequence t does notcovary with the model parameters ?,?:P(t,?,?
| w) ?
Q(t,?,?)
= Q1(t)Q2(?,?
)346P(ti|w, t?i, ?, ??)
?
(n?wi,ti + ?
?nti + m???)
(nti,ti?1 + ?nti?1 + m?)
(nti+1,ti + I(ti?1 = ti = ti+1) + ?nti + I(ti?1 = ti) + m?
)Figure 1: The conditional distribution for state ti used in the pointwise collapsed Gibbs sampler, which conditions onall states t?i except ti (i.e., the counts n do not include ti).
Here m?
is the size of the vocabulary, m is the number ofHMM states and I(?)
is the indicator function (i.e., equal to one if its argument is true and zero otherwise),The calculus of variations is used to minimize theKL divergence between the desired posterior distri-bution and the factorized approximation.
It turnsout that if the likelihood and conjugate prior be-long to exponential families then the optimal Q1 andQ2 do too, and there is an EM-like iterative pro-cedure that finds locally-optimal model parameters(Bishop, 2006).This procedure is especially attractive for HMMinference, since it involves only a minor modifica-tion to the M-step of the Forward-Backward algo-rithm.
MacKay (1997) and Beal (2003) describeVariational Bayesian (VB) inference for HMMs.
Ingeneral, the E-step for VB inference for HMMs isthe same as in EM, while the M-step is as follows:??
(`+1)t?|t = f(E[nt?,t] + ?
)/f(E[nt] +m?)
(4)??
(`+1)w|t = f(E[n?w,t] + ??
)/f(E[nt] + m???
)f(v) = exp(?
(v))where m?
and m are the number of word types andstates respectively, ?
is the digamma function andthe remaining quantities are as in (2).
This meansthat a single iteration can be performed in O(nm2)time, just as for the EM algorithm.2.3 MCMC sampling algorithmsThe goal of Markov Chain Monte Carlo (MCMC)algorithms is to produce a stream of samples fromthe posterior distribution P(t | w,?).
Besag (2004)provides a tutorial on MCMC techniques for HMMinference.A Gibbs sampler is a simple kind of MCMCalgorithm that is well-suited to sampling high-dimensional spaces.
A Gibbs sampler for P(z)where z = (z1, .
.
.
, zn) proceeds by sampling andupdating each zi in turn from P(zi | z?i), wherez?i = (z1, .
.
.
, zi?1, zi+1, .
.
.
, zn), i.e., all of thez except zi (Geman and Geman, 1984; Robert andCasella, 2004).We evaluate four different Gibbs samplers in thispaper, which vary along two dimensions.
First, thesampler can either be pointwise or blocked.
A point-wise sampler resamples a single state ti (labeling asingle word wi) at each step, while a blocked sam-pler resamples the labels for all of the words in asentence at a single step using a dynamic program-ming algorithm based on the Forward-Backward al-gorithm.
(In principle it is possible to use blocksizes other than the sentence, but we did not explorethis here).
A pointwise sampler requires O(nm)time per iteration, while a blocked sampler requiresO(nm2) time per iteration, where m is the numberof HMM states and n is the length of the trainingcorpus.Second, the sampler can either be explicit or col-lapsed.
An explicit sampler represents and sam-ples the HMM parameters ?
and ?
in addition tothe states t, while in a collapsed sampler the HMMparameters are integrated out, and only the states tare sampled.
The difference between explicit andcollapsed samplers corresponds exactly to the dif-ference between the two PCFG sampling algorithmspresented in Johnson et al (2007).An iteration of the pointwise explicit Gibbs sam-pler consists of resampling ?
and ?
given the state-to-state transition counts n and state-to-word emis-sion counts n?
using (5), and then resampling eachstate ti given the corresponding word wi and theneighboring states ti?1 and ti+1 using (6).
?t | nt,?
?
Dir(nt +?
)?t | n?t,??
?
Dir(n?t +??
)(5)P(ti | wi, t?i,?,?)
?
?ti|ti?1?wi|ti?ti+1|ti (6)The Dirichlet distributions in (5) are non-uniform;nt is the vector of state-to-state transition counts int leaving state t in the current state vector t, while347n?t is the vector of state-to-word emission counts forstate t. See Johnson et al (2007) for a more detailedexplanation, as well as an algorithm for samplingfrom the Dirichlet distributions in (5).The samplers that Goldwater and Griffiths (2007)and Johnson (2007) describe are pointwise collapsedGibbs samplers.
Figure 1 gives the sampling distri-bution for this sampler.
As Johnson et al (2007)explains, samples of the HMM parameters ?
and ?can be obtained using (5) if required.The blocked Gibbs samplers differ from the point-wise Gibbs samplers in that they resample the POStags for an entire sentence at a time.
Besag (2004)describes the well-known dynamic programmingalgorithm (based on the Forward-Backward algo-rithm) for sampling a state sequence t given thewords w and the transition and emission probabil-ities ?
and ?.At each iteration the explicit blocked Gibbs sam-pler resamples ?
and ?
using (5), just as the explicitpointwise sampler does.
Then it uses the new HMMparameters to resample the states t for the trainingcorpus using the algorithm just mentioned.
This canbe done in parallel for each sentence in the trainingcorpus.The collapsed blocked Gibbs sampler is astraight-forward application of the Metropolis-within-Gibbs approach proposed by Johnson et al(2007) for PCFGs, so we only sketch it here.
Weiterate through the sentences of the training data, re-sampling the states for each sentence conditionedon the state-to-state transition counts n and state-to-word emission counts n?
for the other sentencesin the corpus.
This is done by first computing theparameters ??
and ??
of a proposal HMM using (7).?
?t?|t =nt?,t + ?nt + m?(7)?
?w|t =n?w,t + ?
?nt + m?
?Then we use the dynamic programming sampler de-scribed above to produce a proposal state sequencet?
for the words in the sentence.
Finally, we usea Metropolis-Hastings accept-reject step to decidewhether to update the current state sequence for thesentence with the proposal t?, or whether to keep thecurrent state sequence.
In practice, with all but thevery smallest training corpora the acceptance rate isvery high; the acceptance rate for all of our collapsedblocked Gibbs samplers was over 99%.3 EvaluationThe previous section described six different unsu-pervised estimators for HMMs.
In this sectionwe compare their performance for English part-of-speech tagging.
One of the difficulties in evalu-ating unsupervised taggers such as these is map-ping the system?s states to the gold-standard parts-of-speech.
Goldwater and Griffiths (2007) proposedan information-theoretic measure known as the Vari-ation of Information (VI) described by Meila?
(2003)as an evaluation of an unsupervised tagging.
How-ever as Goldwater (p.c.)
points out, this may not bean ideal evaluation measure; e.g., a tagger which as-signs all words the same single part-of-speech tagdoes disturbingly well under Variation of Informa-tion, suggesting that a poor tagger may score wellunder VI.In order to avoid this problem we focus here onevaluation measures that construct an explicit map-ping between the gold-standard part-of-speech tagsand the HMM?s states.
Perhaps the most straight-forward approach is to map each HMM state to thepart-of-speech tag it co-occurs with most frequently,and use this mapping to map each HMM state se-quence t to a sequence of part-of-speech tags.
But asClark (2003) observes, this approach has several de-fects.
If a system is permitted to posit an unboundednumber of states (which is not the case here) it canachieve a perfect score on by assigning each wordtoken its own unique state.We can partially address this by cross-validation.We divide the corpus into two equal parts, and fromthe first part we extract a mapping from HMM statesto the parts-of-speech they co-occur with most fre-quently, and use that mapping to map the states ofthe second part of the corpus to parts-of-speech.
Wecall the accuracy of the resulting tagging the cross-validation accuracy.Finally, following Haghighi and Klein (2006) andJohnson (2007) we can instead insist that at mostone HMM state can be mapped to any part-of-speechtag.
Following these authors, we used a greedy algo-rithm to associate states with POS tags; the accuracyof the resulting tagging is called the greedy 1-to-1348All?
50 All?
17 120K ?
50 120K ?
17 24K ?
50 24K ?
17EM 0.40527 0.43101 0.29303 0.35202 0.18618 0.28165VB 0.46123 0.51379 0.34679 0.36010 0.23823 0.36599GSe,p 0.47826 0.43424 0.36984 0.44125 0.29953 0.36811GSe,b 0.49371 0.46568 0.38888 0.44341 0.34404 0.37032GSc,p 0.49910?
0.45028 0.42785 0.43652 0.39182 0.39164GSc,b 0.49486?
0.46193 0.41162 0.42278 0.38497 0.36793Figure 2: Average greedy 1-to-1 accuracy of state sequences produced by HMMs estimated by the various estimators.The column heading indicates the size of the corpus and the number of HMM states.
In the Gibbs sampler (GS) resultsthe subscript ?e?
indicates that the parameters ?
and ?
were explicitly sampled while the subscript ?c?
indicates thatthey were integrated out, and the subscript ?p?
indicates pointwise sampling, while ?b?
indicates sentence-blockedsampling.
Entries tagged with a star indicate that the estimator had not converged after weeks of run-time, but wasstill slowly improving.All?
50 All?
17 120K ?
50 120K ?
17 24K ?
50 24K ?
17EM 0.62115 0.64651 0.44135 0.56215 0.28576 0.46669VB 0.60484 0.63652 0.48427 0.36458 0.35946 0.36926GSe,p 0.64190 0.63057 0.53571 0.46986 0.41620 0.37165GSe,b 0.65953 0.65606 0.57918 0.48975 0.47228 0.37311GSc,p 0.61391?
0.67414 0.65285 0.65012 0.58153 0.62254GSc,b 0.60551?
0.65516 0.62167 0.58271 0.55006 0.58728Figure 3: Average cross-validation accuracy of state sequences produced by HMMs estimated by the various estima-tors.
The table headings follow those used in Figure 2.All?
50 All?
17 120K ?
50 120K ?
17 24K ?
50 24K ?
17EM 4.47555 3.86326 6.16499 4.55681 7.72465 5.42815VB 4.27911 3.44029 5.00509 3.19670 4.80778 3.14557GSe,p 4.24919 3.53024 4.30457 3.23082 4.24368 3.17076GSe,b 4.04123 3.46179 4.22590 3.20276 4.29474 3.10609GSc,p 4.03886?
3.52185 4.21259 3.17586 4.30928 3.18273GSc,b 4.11272?
3.61516 4.36595 3.23630 4.32096 3.17780Figure 4: Average Variation of Information between the state sequences produced by HMMs estimated by the variousestimators and the gold tags (smaller is better).
The table headings follow those used in Figure 2.All?
50 All?
17 120K ?
50 120K ?
17 24K ?
50 24K ?
17EM 558 346 648 351 142 125VB 473 123 337 24 183 20GSe,p 2863 382 3709 63 2500 177GSe,b 3846 286 5169 154 4856 139GSc,p ?
34325 44864 40088 45285 43208GSc,b ?
6948 7502 7782 7342 7985Figure 5: Average number of iterations until the negative logarithm of the posterior probability (or likelihood) changesby less than 0.5% (smaller is better) per at least 2,000 iterations.
No annealing was used.349explicit, pointwiseexplicit, blockedcollapsed, pointwisecollapsed,blockedAll data, 50 states, ?
= ??
= 0.1computing time (seconds)?logposteriorprobability500004000030000200001000008.1e+068.05e+068e+067.95e+067.9e+067.85e+06explicit, pointwiseexplicit, blockedcollapsed, pointwisecollapsed,blockedAll data, 50 states, ?
= ??
= 0.1computing time (seconds)Greedy1-to-1accuracy500004000030000200001000000.580.560.540.520.50.480.460.440.420.4Figure 6: Variation in (a) negative log likelihood and (b) 1-to-1 accuracy as a function of running time on a 3GHzdual quad-core Pentium for the four different Gibbs samplers on all data and 50 hidden states.
Each iteration tookapproximately 96 sec.
for the collapsed blocked sampler, 7.5 sec.
for the collapsed pointwise sampler, 25 sec.
for theexplicit blocked sampler and 4.4 sec.
for the explicit pointwise sampler.350accuracy.The studies presented by Goldwater and Griffiths(2007) and Johnson (2007) differed in the number ofstates that they used.
Goldwater and Griffiths (2007)evaluated against the reduced tag set of 17 tags de-veloped by Smith and Eisner (2005), while Johnson(2007) evaluated against the full Penn Treebank tagset.
We ran all our estimators in both conditions here(thanks to Noah Smith for supplying us with his tagset).Also, the studies differed in the size of the corporaused.
The largest corpus that Goldwater and Grif-fiths (2007) studied contained 96,000 words, whileJohnson (2007) used all of the 1,173,766 wordsin the full Penn WSJ treebank.
For that reasonwe ran all our estimators on corpora containing24,000 words and 120,000 words as well as the fulltreebank.We ran each estimator with the eight differentcombinations of values for the hyperparameters ?and ??
listed below, which include the optimalvalues for the hyperparameters found by Johnson(2007), and report results for the best combinationfor each estimator below 1.?
?
?1 11 0.50.5 10.5 0.50.1 0.10.1 0.00010.0001 0.10.0001 0.0001Further, we ran each setting of each estimator atleast 10 times (from randomly jittered initial start-ing points) for at least 1,000 iterations, as Johnson(2007) showed that some estimators require many it-erations to converge.
The results of our experimentsare summarized in Figures 2?5.1We found that on some data sets the results are sensitive tothe values of the hyperparameters.
So, there is a bit uncertaintyin our comparison results because it is possible that the valueswe tried were good for one estimator and bad for others.
Un-fortunately, we do not know any efficient way of searching theoptimal hyperparameters in a much wider and more fine-grainedspace.
We leave it to future work.4 Conclusion and future workAs might be expected, our evaluation measures dis-agree somewhat, but the following broad tendanciesseem clear.
On small data sets all of the Bayesianestimators strongly outperform EM (and, to a lesserextent, VB) with respect to all of our evaluationmeasures, confirming the results reported in Gold-water and Griffiths (2007).
This is perhaps not toosurprising, as the Bayesian prior plays a compara-tively stronger role with a smaller training corpus(which makes the likelihood term smaller) and theapproximation used by Variational Bayes is likely tobe less accurate on smaller data sets.But on larger data sets, which Goldwater et aldidnot study, the results are much less clear, and dependon which evaluation measure is used.
ExpectationMaximization does surprisingly well on larger datasets and is competitive with the Bayesian estimatorsat least in terms of cross-validation accuracy, con-firming the results reported by Johnson (2007).Variational Bayes converges faster than all of theother estimators we examined here.
We found thatthe speed of convergence of our samplers dependsto a large degree upon the values of the hyperparam-eters ?
and ?
?, with larger values leading to muchfaster convergence.
This is not surprising, as the ?and ??
specify how likely the samplers are to con-sider novel tags, and therefore directly influence thesampler?s mobility.
However, in our experiments thebest results are obtained in most settings with smallvalues for ?
and ?
?, usually between 0.1 and 0.0001.In terms of time to convergence, on larger datasets we found that the blocked samplers were gen-erally faster than the pointwise samplers, and thatthe explicit samplers (which represented and sam-pled ?
and ?)
were faster than the collapsed sam-plers, largely because the time saved in not com-puting probabilities on the fly overwhelmed the timespent resampling the parameters.Of course these experiments only scratch the sur-face of what is possible.
Figure 6 shows thatpointwise-samplers initially converge faster, but areovertaken later by the blocked samplers.
Inspiredby this, one can devise hybrid strategies that inter-leave blocked and pointwise sampling; these mightperform better than both the blocked and pointwisesamplers described here.351ReferencesMatthew J. Beal.
2003.
Variational Algorithms for Ap-proximate Bayesian Inference.
Ph.D. thesis, GatsbyComputational Neuroscience unit, University CollegeLondon.Julian Besag.
2004.
An introduction to Markov ChainMonte Carlo methods.
In Mark Johnson, Sanjeev P.Khudanpur, Mari Ostendorf, and Roni Rosenfeld, ed-itors, Mathematical Foundations of Speech and Lan-guage Processing, pages 247?270.
Springer, NewYork.Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In 10th Conference of the European Chapter ofthe Association for Computational Linguistics, pages59?66.
Association for Computational Linguistics.Stuart Geman and Donald Geman.
1984.
Stochastic re-laxation, Gibbs distributions, and the Bayesian restora-tion of images.
IEEE Transactions on Pattern Analysisand Machine Intelligence, 6:721?741.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speech tag-ging.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages744?751, Prague, Czech Republic, June.
Associationfor Computational Linguistics.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings ofthe Human Language Technology Conference of theNAACL, Main Conference, pages 320?327, New YorkCity, USA, June.
Association for Computational Lin-guistics.Frederick Jelinek.
1997.
Statistical Methods for SpeechRecognition.
The MIT Press, Cambridge, Mas-sachusetts.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 139?146,Rochester, New York, April.
Association for Compu-tational Linguistics.Mark Johnson.
2007.
Why doesn?t EM find goodHMM POS-taggers?
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 296?305.David J.C. MacKay.
1997.
Ensemble learning for hiddenMarkov models.
Technical report, Cavendish Labora-tory, Cambridge.David J.C. MacKay.
2003.
Information Theory, Infer-ence, and Learning Algorithms.
Cambridge Univer-sity Press.Chris Manning and Hinrich Schu?tze.
1999.
Foundationsof Statistical Natural Language Processing.
The MITPress, Cambridge, Massachusetts.Marina Meila?.
2003.
Comparing clusterings by the vari-ation of information.
In Bernhard Scho?lkopf and Man-fred K. Warmuth, editors, COLT 2003: The SixteenthAnnual Conference on Learning Theory, volume 2777of Lecture Notes in Computer Science, pages 173?187.Springer.Christian P. Robert and George Casella.
2004.
MonteCarlo Statistical Methods.
Springer.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 354?362, Ann Arbor, Michigan, June.
Associa-tion for Computational Linguistics.352
