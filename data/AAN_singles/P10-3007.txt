Proceedings of the ACL 2010 Student Research Workshop, pages 37?42,Uppsala, Sweden, 13 July 2010.c?2010 Association for Computational LinguisticsHow spoken language corpora can refinecurrent speech motor training methodologiesDaniil Umanski, Niels O. SchillerLeiden Institute for Brain and CognitionLeiden University, The Netherlandsdaniil.umanski@gmail.comN.O.Schiller@hum.leidenuniv.nlFederico SangatiInstitute for Logic,Language and ComputationUniversity of Amsterdam, the Netherlandsf.sangati@uva.nlAbstractThe growing availability of spoken lan-guage corpora presents new opportunitiesfor enriching the methodologies of speechand language therapy.
In this paper, wepresent a novel approach for construct-ing speech motor exercises, based on lin-guistic knowledge extracted from spokenlanguage corpora.
In our study with theDutch Spoken Corpus, syllabic inventorieswere obtained by means of automatic syl-labification of the spoken language data.Our experimental syllabification methodexhibited a reliable performance, and al-lowed for the acquisition of syllabic tokensfrom the corpus.
Consequently, the syl-labic tokens were integrated in a tool forclinicians, a result which holds the poten-tial of contributing to the current state ofspeech motor training methodologies.1 IntroductionSpoken language corpora are often accessed bylinguists, who need to manipulate specifically de-fined speech stimuli in their experiments.
How-ever, this valuable resource of linguistic informa-tion has not yet been systematically applied forthe benefit of speech therapy methodologies.
Thisis not surprising, considering the fact that spokenlanguage corpora have only appeared relatively re-cently, and are still not easily accessible outsidethe NLP community.
Existing applications forselecting linguistic stimuli, although undoubtedlyuseful, are not based on spoken language data,and are generally not designed for utilization byspeech therapists per se (Aichert et al, 2005).
Asa first attempt to bridge this gap, a mechanism isproposed for utilizing the relevant linguistic in-formation to the service of clinicians.
In coor-dination with speech pathologists, the domain ofspeech motor training was identified as an appro-priate area of application.
The traditional speechmotor programs are based on a rather static inven-tory of speech items, and clinicians do not haveaccess to a modular way of selecting speech tar-gets for training.Therefore, in this project, we deal with develop-ing an interactive interface to assist speech thera-pists with constructing individualized speech mo-tor practice programs for their patients.
The prin-cipal innovation of the proposed system in re-gard to existing stimuli selection applications istwofold: first, the syllabic inventories are derivedfrom spoken word forms, and second, the selec-tion interface is integrated within a broader plat-form for conducting speech motor practice.2 Principles of speech motor practice2.1 Speech Motor DisordersSpeech motor disorders (SMD) arise from neuro-logical impairments in the motor systems involvedin speech production.
SMD include acquired anddevelopmental forms of dysarthria and apraxia ofspeech.
Dysarthria refers to the group of disor-ders associated with weakness, slowness and in-ability to coordinate the muscles used to producespeech (Duffy, 2005).
Apraxia of speech (AOS)is referred to the impaired planning and program-ming of speech (Ziegler , 2008).
Fluency dis-orders, namely stuttering and cluttering, althoughnot always classified as SMD, have been exten-sively studied from the speech motor skill perspec-tive (Van Lieshout et al, 2001).2.2 Speech Motor TrainingThe goal of speech therapy with SMD patients isestablishing and maintaining correct speech mo-tor routines by means of practice.
The process oflearning and maintaining productive speech mo-tor skills is referred to as speech motor training.37An insightful design of speech motor training ex-ercises is crucial in order to achieve an optimallearning process, in terms of efficiency, retention,and transfer levels (Namasivayam, 2008).Maas et al (2008) make the attempt to relate find-ings from research on non-speech motor learningprinciples to the case of speech motor training.They outline a number of critical factors in the de-sign of speech motor exercises.
These factors in-clude the training program structure, selection ofspeech items, and the nature of the provided feed-back.It is now generally agreed that speech motor exer-cises should involve simplified speech tasks.
Theuse of non-sense syllable combinations is a gener-ally accepted method for minimizing the effects ofhigher-order linguistic processing levels, with theidea of tapping as directly as possible to the motorcomponent of speech production (Smits-Bandstraet al, 2006) .2.3 Selection of speech itemsThe main considerations in selecting speech itemsfor a specific patient are functional relevance andmotor complexity.
Functional relevance refersto the specific motor, articulatory or phoneticdeficits, and consequently to the treatment goalsof the patient.
For example, producing correctstress patterns might be a special difficulty for onepatient, while producing consonant clusters mightbe challenging for another.
Relative motor com-plexity of speech segments is much less defined inlinguistic terms than, for example, syntactic com-plexity (Kleinow et al, 2000).
Although the part-whole relationship, which works well for syntacticconstructions, can be applied to syllabic structuresas well (e.g., ?flake?
and ?lake?
), it may not be themost suitable strategy.However, in an original recent work, Zieglerpresented a non-linear probabilistic model ofthe phonetic code, which involves units from asub-segmental level up to the level of metricalfeet (Ziegler , 2009).
The model is verified onthe basis of accuracy data from a large sample ofapraxic speakers, and thus provides a quantitiveindex of a speech segment?s motor complexity.Taken together, it is evident that the task of se-lecting sets of speech items for an individualized,optimal learning process is far from obvious, andmuch can be done to assist the clinicians with go-ing through this step.3 The role of the syllableThe syllable is the primary speech unit used instudies on speech motor control (Namasivayam,2008).
It is also the basic unit used for con-structing speech items in current methodologiesof speech motor training (Kent, 2000).
Sincethe choice of syllabic tokens is assumed to affectspeech motor learning, it would be beneficial tohave access to the syllabic inventory of the spokenlanguage.
Besides the inventory of spoken sylla-bles, we are interested in the distribution of sylla-bles across the language.3.1 Syllable frequency effectsThe observation that syllables exhibit an exponen-tial distribution in English, Dutch and German hasled researchers to infer the existence of a ?men-tal syllabary?
component in the speech productionmodel (Schiller et al, 1996).
Since this hypothesisassumes that production of high frequency sylla-bles relies on highly automated motor gestures, itbears direct consequences on the utility of speechmotor exercises.
In other words, manipulating syl-lable sets in terms of their relative frequency is ex-pected to have an effect on the learning process ofnew motor gestures.
This argument is supportedby a number of empirical findings.
In a recentstudy, Staiger et al report that syllable frequencyand syllable structure play a decisive role with re-spect to articulatory accuracy in the spontaneousspeech production of patients with AOS (Staigeret al, 2008).
Similarly, (Laganaro, 2008) con-firms a significant effect of syllable frequency onproduction accuracy in experiments with speakerswith AOS and speakers with conduction aphasia.3.2 Implications on motor learningIn that view, practicing with high-frequency sylla-bles could promote a faster transfer of skills to ev-eryday language, as the most ?required?
motor ges-tures are being strengthened.
On the other hand,practicing with low-frequency syllables could po-tentially promote plasticity (or ?stretching? )
of thespeech motor system, as the learner is required toassemble motor plans from scratch, similar to theprocess of learning to pronounce words in a for-eign language.
In the next section, we describeour study with the Spoken Dutch Corpus, and il-lustrate the performed data extraction strategies.384 A study with the Spoken Dutch CorpusThe Corpus Gesproken Nederlands (CGN) is alarge corpus of spoken Dutch1.
The CGN con-tains manually verified phonetic transcriptions of53,583 spoken forms, sampled from a wide vari-ety of communication situations.
A spoken formreports the phoneme sequence as it was actuallyuttered by the speaker as opposed to the canonicalform, which represents how the same word wouldbe uttered in principle.4.1 Motivation for accessing spoken formsIn contrast to written language corpora, such asCELEX (Baayenet al, 1996), or even a corpuslike TIMIT (Zue et al, 1996), in which speak-ers read prepared written material, spontaneousspeech corpora offer an access to an informal, un-scripted speech on a variety of topics, includingspeakers from a range of regional dialects, age andeducational backgrounds.Spoken language is a dynamic, adaptive, and gen-erative process.
Speakers most often deviate fromthe canonical pronunciation, producing segmentreductions, deletions, insertions and assimilationsin spontaneous speech (Mitterer, 2008).
The workof Greenberg provides an in-depth account on thepronunciation variation in spoken English.
A de-tailed phonetic transcription of the Switchboardcorpus revealed that the spectral properties ofmany phonetic elements deviate significantly fromtheir canonical form (Greenberg, 1999).In the light of the apparent discrepancy betweenthe canonical forms and the actual spoken lan-guage, it becomes apparent that deriving syllabicinventories from spoken word forms will approxi-mate the reality of spontaneous speech productionbetter than relying on canonical representations.Consequently, it can be argued that clinical ap-plications will benefit from incorporating speechitems which optimally converge with the ?live?
re-alization of speech.4.2 Syllabification of spoken formsThe syllabification information available in theCGN applies only to the canonical forms of words,and no syllabification of spoken word forms exists.The methods of automatic syllabification havebeen applied and tested exclusively on canonicalword forms (Bartlett, 2007).
In order to obtainthe syllabic inventory of spoken language per se,1(see http://lands.let.kun.nl/cgn/)a preliminary study on automatic syllabificationof spoken word forms has been carried out.
Twomethods for dealing with the syllabification taskwere proposed, the first based on an n-gram modeldefined over sequences of phonemes, and the sec-ond based on statistics over syllable units.
Bothalgorithms accept as input a list of possible seg-mentations of a given phonetic sequence, and re-turn the one which maximizes the score of the spe-cific function they implement.
The list of possiblesegmentations is obtained by exhaustively gener-ating all possible divisions of the sequence, satis-fying the condition of keeping exactly one vowelper segment.4.3 Syllabification MethodsThe first method is a reimplementation of the workof (Schmid et al, 2007).
The authors describe thesyllabification task as a tagging problem, in whicheach phonetic symbol of a word is tagged as ei-ther a syllable boundary (?B?)
or as a non-syllableboundary (?N?).
Given a set of possible segmenta-tions of a given word, the aim is to select the one,viz.
the tag sequence?bn1, which is more proba-ble for the given phoneme sequence pn1, as shownin equation (1).
This probability in equations (3)is reduced to the joint probability of the two se-quences: the denominator of equation (2) is in factconstant for the given list of possible syllabifica-tions, since they all share the same sequence ofphonemes.
Equation (4) is obtained by introduc-ing a Markovian assumption of order 3 in the waythe phonemes and tags are jointly generated?bn1= argmaxbn1P (bn1|pn1) (1)= argmaxbn1P (bn1, pn1)/P (pn1) (2)= argmaxbn1P (bn1, pn1) (3)= argmaxbn1n+1?i=1P (bi, pi|bi?1i?3, pi?1i?3) (4)The second syllabification method relies onstatistics over the set of syllables unit and bi-gram (bisegments) present in the training corpus.Broadly speaking, given a set of possible segmen-tations of a given phoneme sequence, the algo-rithm, selects the one which maximizes the pres-ence and frequency of its segments.39CorpusPhonemes SyllablesBoundaries Words Boundaries WordsCGN Dutch 98.62 97.15 97.58 94.99CELEX Dutch 99.12 97.76 99.09 97.70CELEX German 99.77 99.41 99.51 98.73CELEX English 98.86 97.96 96.37 93.50Table 1: Summary of syllabification results on canonical word forms.4.4 ResultsThe first step involved the evaluation of the twoalgorithms on syllabification of canonical wordforms.
Four corpora comprising three differentlanguages (English, German, and Dutch) wereevaluated: the CELEX2 corpora (Baayenet al,1996) for the three languages, and the SpokenDutch Corpus (CGN).
All the resources includedmanually verified syllabification transcriptions.
A10-fold cross validation on each of the corpora wasperformed to evaluate the accuracy of our meth-ods.
The evaluation is presented in terms of per-centage of correct syllable boundaries2, and per-centage of correctly syllabified words.Table 1 summarizes the obtained results.
For theCELEX corpora, both methods produce almostequally high scores, which are comparable to thestate of the art results reported in (Bartlett, 2007).For the Spoken Dutch Corpus, both methodsdemonstrate quite high scores, with the phoneme-level method showing an advantage, especiallywith respect to correctly syllabified words.4.5 Data extractionThe process of evaluating syllabification of spo-ken word forms is compromised by the fact thatthere exists no gold annotation for the pronuncia-tion data in the corpus.
Therefore, the next stepinvolved applying both methods on the data setand comparing the two solutions.
The results re-vealed that the two algorithms agree on 94.29%of syllable boundaries and on 90.22% of wholeword syllabification.
Based on the high scores re-ported for lexical word forms syllabification, anagreement between both methods most probablyimplies a correct solution.
The ?disagreement?
setcan be assumed to represent the class of ambigu-ous cases, which are the most problematic for au-tomatic syllabification.
As an example, consider2Note that recall and precision coincide since the numberof boundaries (one less than the number of vowels) is con-stant for different segmentations of the same word.the following pair of possible syllabification, onwhich the two methods disagree: ?bEl-kOm-pjut?vs ?bEl-kOmp-jut?3.Motivated by the high agreement score, we haveapplied the phoneme-based method on the spo-ken word forms in the CGN, and compiled a syl-labic inventory.
In total, 832,236 syllable tokenswere encountered in the corpus, of them 11,054unique syllables were extracted and listed.
Thefrequencies distribution of the extracted syllabary,as can be seen in Figure 1, exhibits an exponentialcurve, a result consistent with earlier findings re-ported in (Schiller et al, 1996).
According to ourstatistics, 4% of unique syllable tokens account for80% of all extracted tokens, and 10% of uniquesyllables account for 90% respectively.
For eachextracted syllable, we have recorded its structure,frequency rank, and the articulatory characteristicsof its consonants.
Next, we describe the speechitems selection tool for clinicians.Figure 1: Syllable frequency distribution over thespoken forms in the Dutch Spoken Corpus.The x-axis represents 625 ranked frequency bins.The y-axis plots the total number of syllable to-kens extracted for each frequency bin.3A manual evaluation of the disagreement set revealed aclear advantage for the phoneme-based method405 An interface for cliniciansIn order to make the collected linguistic informa-tion available for clinicians, an interface has beenbuilt which enables clinicians to compose individ-ual training programs.
A training program con-sists of several training sessions, which in turnconsists of a number of exercises.
For each ex-ercise, a number of syllable sets are selected, ac-cording to the specific needs of the patient.
Themain function of the interface, thus, deals withselection of customized syllable sets, and is de-scribed next.
The rest of the interface deals withthe different ways in which the syllable sets canbe grouped into exercises, and how exercises arescheduled between treatment sessions.5.1 User-defined syllable setsThe process starts with selecting the number ofsyllables in the current set, a number between oneand four.
Consequently, the selected number of?syllable boxes?
appear on the screen.
Each boxallows for a separate configuration of one syllablegroup.
As can be seen in Figure 2, a syllable boxcontains a number of menus, and a text grid at thebottom of the box.Figure 2: A snapshot of the part of the interfaceallowing configuration of syllable setsHere follows the list of the parameters which theuser can manipulate, and their possible values:?
Syllable Type4?
Syllable Frequency54CV, CVC, CCV, CCVC, etc.5Syllables are divided in three rank groups - high,medium, and low frequency.?
Voiced - Unvoiced consonant6?
Manner of articulation7?
Place of articulation8Once the user selects a syllable type, he/she canfurther specify each consonant within that syllabletype in terms of voiced/unvoiced segment choiceand manner and place of articulation.
For the sakeof simplicity, syllable frequency ranks have beendivided in three rank groups.
Alternatively, theuser can bypass this criterion by selecting ?any?.As the user selects the parameters which define thedesired syllable type, the text grid is continuouslyfilled with the list of syllables satisfying these cri-teria, and a counter shows the number of syllablescurrently in the grid.Once the configuration process is accomplished,the syllables which ?survived?
the selection willconstitute the speech items of the current exercise,and the user proceeds to select how the syllablesets should be grouped, scheduled and so on.6 Final remarks6.1 Future directionsA formal usability study is needed in order toestablish the degree of utility and satisfaction withthe interface.
One question which demands inves-tigation is the degrees of choice that the selectiontool should provide.
With too many variablesand hinges of choice, the configuration processfor each patient might become complicated andtime consuming.
Therefore, a usability studyshould provide guidelines for an optimal designof the interface, so that its utility for clinicians ismaximized.Furthermore, we plan to integrate the proposedinterface within an computer-based interactiveplatform for speech therapy.
A seamless integra-tion of a speech items selection module withinbiofeedback games for performing exercises withthese items seems straight forward, as the selecteditems can be directly embedded (e.g., as textsymbols or more abstract shapes) in the graphicalenvironment where the exercises take place.6when applicable7for a specific consonant.
Plosives, Fricatives, Sonorants8for a specific consonant.
Bilabial, Labio-Dental, Alveo-lar, Post-Alveolar, Palatal, Velar, Uvular, Glottal41AcknowledgmentsThis research is supported with the ?Mosaic?
grantfrom The Netherlands Organisation for ScientificResearch (NWO).
The authors are grateful forthe anonymous reviewers for their constructivefeedback.ReferencesAichert, I., Ziegler, W. 2004.
Syllable frequency andsyllable structure in apraxia of speech.
Brain andLanguage, 88, 148-159.Aichert, I., Marquardt, C., Ziegler, W. 2005.
Fre-quenzen sublexikalischer Einheiten des Deutschen:CELEX-basierte Datenbanken.
Neurolinguistik, 19,55-81Baayen R.H., Piepenbrock R. and Gulikers L. 1996.CELEX2.
Linguistic Data Consortium, Philadel-phia.Bartlett, S. 2007.
Discriminative approach to auto-matic syllabication.
Masters thesis, Departmentof-Computing Science, University of Alberta.Duffy, J.R 2005.
Motor speech disorder: Substrates,Differential Diagnosis, and Management.
(2nd Ed.)507-524.
St. Louis, MO: Elsevier MosbyGreenberg, S. 1999.
Speaking in shorthanda syllable-centric perspective for understanding pronunciationvariation.
Speech Comm., 29(2-4):159-176Kent, R. 2000.
Research on speech motor controland its disorders, a review and prospectives.
SpeechComm., 29(2-4):159-176 J.Kleinow, J., Smith, A.
2000.
Inuences of length andsyntactic complexity on the speech motor stabilityof the uent speech of adults who stutter.
Journalof Speech, Language, and Hearing Research, 43,548559.Laganaro, M. 2008.
Is there a syllable frequency effectin aphasia or in apraxia of speech or both?
Aphasi-ology, Volume 22, Number 11, November 2008 , pp.1191-1200(10)Maas, E., Robin, D.A., Austermann Hula, S.N., Freed-man, S.E., Wulf, G., Ballard, K.J., Schmidt, R.A.2008.
Principles of Motor Learning in Treatmentof Motor Speech Disorders American Journal ofSpeech-Language Pathology, 17, 277-298.Mitterer, H. 2008.
How are words reduced in sponta-neous speech?
In A. Botinis (Ed.
), Proceedings ofthe ISCA Tutorial and Research Workshop on Ex-perimental Linguistics (pages 165-168).
Universityof Athens.Namasivayam, A.K., van Lieshout, P. 2008.
Investi-gating speech motor practice and learning in peoplewho stutter Journal of Fluency Disorders 33 (2008)3251Schiller, N. O., Meyer, A. S., Baayen, R. H., Levelt, W.J.
M. 1996.
A Comparison of Lexeme and SpeechSyllables in Dutch.
Journal of Quantitative Linguis-tics, 3, 8-28.Schmid H., M?obius B. and Weidenkaff J.
2007.
Tag-ging Syllable Boundaries With Joint N-Gram Mod-els.
Proceedings of Interspeech-2007 (Antwerpen),pages 2857-2860.Smits-Bandstra, S., DeNil, L. F., Saint-Cyr, J.
2006.Speech and non-speech sequence skill learning inadults who stutter.
Journal of Fluency Disorders,31,116136.Staiger, A., Ziegler, W. 2008.
Syllable frequency andsyllable structure in the spontaneous speech produc-tion of patients with apraxia of speech.
Aphasiol-ogy, Volume 22, Number 11, November 2008 , pp.1201-1215(15)Tjaden, K. 2000.
Exploration of a treatment techniquefor prosodic disturbance following stroke training.Clinical Linguistics and Phonetics 2000, Vol.
14,No.
8, Pages 619-641Riley, J., Riley, G. 1995.
Speech motor improvementprogram for children who stutter.
In C.W.
Stark-weather, H.F.M.
Peters (Eds.
), Stuttering (pp.269-272) New York: ElsevierVan Lieshout, P. H. H. M. 2001.
Recent developmentsin studies of speech motor control in stuttering.
In B.Maassen, W. Hulstijn, R. D. Kent, H. F. M. Peters,P.
H. H. M. Van Lieshout (Eds.
), Speech motor con-trol in normal and disordered speech(pp.
286290).Nijmegen, The Netherlands:Vantilt.Ziegler W. 2009.
Modelling the architecture of pho-netic plans: Evidence from apraxia of speech.
Lan-guage and Cognitive Processes 24, 631 - 661Ziegler W. 2008.
Apraxia of speech.
In: GoldenbergG, Miller B (Eds.
), Handbook of Clinical Neurology,Vol.
88 (3rd series), pp.
269 - 285.
Elsevier.
LondonZue, V.W.
and Seneff, S. 1996.
?Transcription andalignment of the TIMIT database.
In Recent Re-search Towards Advanced Man-Machine InterfaceThrough Spoken Language.
H. Fujisaki (ed.
), Am-sterdam: Elsevier, 1996, pp.
515-525.42
