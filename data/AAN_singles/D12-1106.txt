Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1157?1168, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsA coherence model based on syntactic patternsAnnie LouisUniversity of PennsylvaniaPhiladelphia, PA 19104, USAlannie@seas.upenn.eduAni NenkovaUniversity of PennsylvaniaPhiladelphia, PA 19104, USAnenkova@seas.upenn.eduAbstractWe introduce a model of coherence whichcaptures the intentional discourse structure intext.
Our work is based on the hypothesis thatsyntax provides a proxy for the communica-tive goal of a sentence and therefore the se-quence of sentences in a coherent discourseshould exhibit detectable structural patterns.Results show that our method has high dis-criminating power for separating out coherentand incoherent news articles reaching accura-cies of up to 90%.
We also show that our syn-tactic patterns are correlated with manual an-notations of intentional structure for academicconference articles and can successfully pre-dict the coherence of abstract, introductionand related work sections of these articles.1 IntroductionRecent studies have introduced successful automaticmethods to predict the structure and coherence oftexts.
They include entity approaches for local co-herence which track the repetition and syntactic re-alization of entities in adjacent sentences (Barzilayand Lapata, 2008; Elsner and Charniak, 2008) andcontent approaches for global coherence which viewtexts as a sequence of topics, each characterized by aparticular distribution of lexical items (Barzilay andLee, 2004; Fung and Ngai, 2006).
Other work hasshown that co-occurrence of words (Lapata, 2003;Soricut and Marcu, 2006) and discourse relations(Pitler and Nenkova, 2008; Lin et al 2011) also pre-dict coherence.Early theories (Grosz and Sidner, 1986) positedthat there are three factors which collectively con-tribute to coherence: intentional structure (purposeof discourse), attentional structure (what items arediscussed) and the organization of discourse seg-ments.
The highly successful entity approaches cap-ture attentional structure and content approaches arerelated to topic segments but intentional structurehas largely been neglected.
Every discourse has apurpose: explaining a concept, narrating an event,critiquing an idea and so on.
As a result each sen-tence in the article has a communicative goal and thesequence of goals helps the author achieve the dis-course purpose.
In this work, we introduce a modelto capture coherence from the intentional structuredimension.
Our key proposal is that syntactic pat-terns are a useful proxy for intentional structure.This idea is motivated from the fact that cer-tain sentence types such as questions and definitionshave distinguishable and unique syntactic structure.For example, consider the opening sentences of twodescriptive articles1 shown in Table 1.
Sentences(1a) and (2a) are typical instances of definition sen-tences.
Definitions are written with the concept tobe defined expressed as a noun phrase followed bya copular verb (is/are).
The predicate contains twoparts: the first is a noun phrase reporting the conceptas part of a larger class (eg.
an aqueduct is a watersupply), the second component is a relative clauselisting unique properties of the concept.
These areexamples of syntactic patterns related to the com-municative goals of individual sentences.
Similarly,sentences (1b) and (2b) which provide further de-tails about the concept also have some distinguish-1Wikipedia articles on ?Aqueduct?
and ?Cytokine Recep-tors?11571a) An aqueduct is a water supply or navigable channelconstructed to convey water.b) In modern engineering, the term is used for any systemof pipes, canals, tunnels, and other structures used forthis purpose.2a) Cytokine receptors are receptors that binds cytokines.b) In recent years, the cytokine receptors have come todemand more attention because their deficiency has now beendirectly linked to certain debilitating immunodeficiency states.Table 1: The first two sentences of two descriptive arti-clesing syntactic features such as the presence of a top-icalized phrase providing the focus of the sentence.The two sets of sentences have similar sequence ofcommunicative goals and so we can expect the syn-tax of adjacent sentences to also be related.We aim to characterize this relationship on abroad scale using a coherence model based entirelyon syntax.
The model relies on two assumptionswhich summarize our intuitions about syntax and in-tentional structure:1.
Sentences with similar syntax are likely to havethe same communicative goal.2.
Regularities in intentional structure will bemanifested in syntactic regularities between ad-jacent sentences.There is also evidence from recent work that sup-ports these assumptions.
Cheung and Penn (2010)find that a better syntactic parse of a sentence can bederived when the syntax of adjacent sentences is alsotaken into account.
Lin et al(2009) report that thesyntactic productions in adjacent sentences are pow-erful features for predicting which discourse relation(cause, contrast, etc.)
holds between them.
Cocco etal.
(2011) show that significant associations exist be-tween certain part of speech tags and sentence typessuch as explanation, dialog and argumentation.In our model, syntax is represented either as parsetree productions or a sequence of phrasal nodes aug-mented with part of speech tags.
Our best perform-ing method uses a Hidden Markov Model to learnthe patterns in these syntactic items.
Sections 3 and5 discuss the representations and their specific im-plementations and relative advantages.
Results showthat syntax models can distinguish coherent and in-coherent news articles from two domains with 75-90% accuracies over a 50% baseline.
In addition,the syntax coherence scores turn out complementaryto scores given by lexical and entity models.We also study our models?
predictions on aca-demic articles, a genre where intentional structureis widely studied.
Sections in these articles havewell-defined purposes and we find recurring sen-tence types such as motivation, citations, descrip-tion, and speculations.
There is a large body of work(Swales, 1990; Teufel et al 1999; Liakata et al2010) concerned with defining and annotating thesesentence types (called zones) in conference articles.In Section 6, we describe how indeed some patternscaptured by the syntax-based models are correlatedwith zone categories that were proposed in prior lit-erature.
We also present results on coherence pre-diction: our model can distinguish the introductionsection of conference papers from its perturbed ver-sions with over 70% accuracy.
Further, our modelis able to identify conference from workshop paperswith good accuracies, given that we can expect thesearticles to vary in purpose.2 Evidence for syntactic coherenceWe first present a pilot study that confirms that ad-jacent sentences in discourse exhibit stable patternsof syntactic co-occurrence.
This study validates oursecond assumption relating the syntax of adjacentsentences.
Later in Section 6, we examine syntac-tic patterns in individual sentences (assumption 1)using a corpus of academic articles where sentenceswere manually annotated with communicative goals.Prior work has reported that certain grammaticalproductions are repeated in adjacent sentences moreoften than would be expected by chance (Reitter etal., 2006; Cheung and Penn, 2010).
We analyze allco-occurrence patterns rather than just repetitions.We use the gold standard parse trees from thePenn Treebank (Marcus et al 1994).
Our unit ofanalysis is a pair of adjacent sentences (S1, S2) andwe choose to use Section 0 of the corpus which has99 documents and 1727 sentence pairs.
We enumer-ate all productions that appear in the syntactic parseof any sentence and exclude those that appear lessthan 25 times, resulting in a list of 197 unique pro-ductions.
Then all ordered pairs2 (p1, p2) of pro-ductions are formed.
For each pair, we compute2(p1, p2) and (p2, p1) are considered as different pairs.1158p1, p2 Sentence 1 Sentence 2NP?
NP NP-ADV The two concerns said they entered into a definitive Also on the takeover front, Jaguar?s ADRs roseQP?
CD CD merger agreement under which Ratners will begin a tender 1/4 to 13 7/8 on turnover of [4.4 million]QP.offer for all of Weisfield?s common shares for [$57.50 each]NP.VP?
VB VP ?The refund pool may not [be held hostage through another?
[Commonwealth Edison]NP-SBJ said it is alreadyNP-SBJ?
NNP NNP round of appeals]VP,?
Judge Curry said.
appealing the underlying commission order andis considering appealing Judge Curry?s order.NP-LOC?
NNP ?It has to be considered as an additional risk for the investor,?
[?Cray Computer will be a concept?S-TPC-1?
NP-SBJ VP said Gary P. Smaby of Smaby Group Inc., [Minneapolis]NP-LOC.
?stock,?
]S-TPC-1 he said.Table 2: Example sentences for preferred production sequences.
The span of the LHS of the corresponding productionis indicated by [] braces.the following: c(p1p2) = number of sentence pairswhere p1 ?
S1 and p2 ?
S2; c(p1?p2) = num-ber of pairs where p1 ?
S1 and p2 6?
S2; c(?p1p2)and c(?p1?p2) are computed similarly.
Then weperform a chi-square test to understand if the ob-served count c(p1p2) is significantly (95% confi-dence level) greater or lesser than the expected valueif occurrences of p1 and p2 were independent.Of the 38,809 production pairs, we found that1,168 pairs occurred in consecutive sentences sig-nificantly more often than chance and 172 appearedsignificantly fewer times than expected.
In Table 2we list, grouped in three simple categories, the 25pairs of the first kind with most significant p-values.Some of the preferred pairs are indeed repetitionsas pointed out by prior work.
But they form only asmall fraction (5%) of the total preferred productionpairs indicating that there are several other classesof syntactic regularities beyond priming.
Some ofthese other sequences can be explained by the factthat these articles come from the finance domain:they involve productions containing numbers andquantities.
An example for this type is shown in Ta-ble 2.
Finally, there is also a class that is not repe-titions or readily observed as domain-specific.
Themost frequent one reflects a pattern where the firstsentence introduces a subject and predicate and thesubject in the second sentence is pronominalized.Examples for two other patterns are given in Table2.
For the sequence (VP ?
VB VP | NP-SBJ ?
NNPNNP), a bare verb is present in S1 and is often asso-ciated with modals.
In the corpus, these statementsoften present hypothesis or speculation.
The follow-ing sentence S2 has an entity, a person or organiza-tion, giving an explanation or opinion on the state-ment.
This pattern roughly correponds to a SPECU-LATE followed by ENDORSE sequence of intentions.p1 p2 c(p1p2)?
Repetition ?VP?
VBD SBAR VP?
VBD SBAR 83QP?
$ CD CD QP?
$ CD CD 18NP?
$ CD -NONE- NP?
$ CD -NONE- 16NP?
QP -NONE- NP?
QP -NONE- 15NP-ADV?
DT NN NP-ADV?
DT NN 10NP?
NP NP-ADV NP?
NP NP-ADV 7?
Quantities/Amounts ?NP?
QP -NONE- QP?
$ CD CD 16QP?
$ CD CD NP?
QP -NONE- 15NP?
NP NP-ADV NP?
QP -NONE- 11NP-ADV?
DT NN NP?
QP -NONE- 11NP?
NP NP-ADV NP-ADV?
DT NN 9NP?
$ CD -NONE- NP-ADV?
DT NN 8NP-ADV?
DT NN NP?
$ CD -NONE- 8NP-ADV?
DT NN NP?
NP NP-ADV 8NP?
NP NP-ADV QP?
CD CD 6?
Other ?S?
NP-SBJ VP NP-SBJ?
PRP 290VP?
VBD SBAR PP-TMP?
IN NP 79S?
NP-SBJ-1 VP VP?
VBD SBAR 43VP?
VBD NP VP?
VBD VP 31VP?
VB VP NP-SBJ?
NNP NNP 27NP-SBJ-1?
NNP NNP VP?
VBD NP 13VP?
VBZ NP S?
PP-TMP , NP-SBJ VP .
8NP-SBJ?
JJ NNS VP?
VBP NP 8NP-PRD?
NP PP NP-PRD?
NP SBAR 7NP-LOC?
NNP S-TPC-1?
NP-SBJ VP 6Table 3: Top patterns in productions from WSJSimilarly, in all the six adjacent sentence pairs fromour corpus containing the items (NP-LOC ?
NNP | S-TPC-1 ?
NP-SBJ VP), p1 introduces a location name,and is often associated with the title of a person ororganization.
The next sentence has a quote fromthat person, where the quotation forms the topical-ized clause in p2.
Here the intentional structure isINTRODUCE X / STATEMENT BY X.In the remainder of the paper we formalize ourrepresentation of syntax and the derived model ofcoherence and test its efficacy in three domains.3 Coherence models using syntaxWe first describe the two representations of sentencestructure we adopted for our analysis.3 Next, we3Our representations are similar to features used for rerank-ing in parsing.
Our first representation corresponds to ?rules?features (Charniak and Johnson, 2005; Collins and Koo, 2005),and our second representation is related to ?spines?
(Carreras etal., 2008) and edge annotation(Huang, 2008).1159present two coherence models: a local model whichcaptures the co-occurrence of structural features inadjacent sentences and a global one which learnsfrom clusters of sentences with similar syntax.3.1 Representing syntaxOur models rely exclusively on syntactic cues.
Wederive representations from constituent parses of thesentences, and terminals (words) are removed fromthe parse tree before any processing is done.
Theleaf nodes in our parse trees are part of speech tags.Productions: In this representation we view eachsentence as the set of grammatical productions, LHS?
RHS, which appear in the parse of the sen-tence.
As we already pointed out, the right-hand side(RHS) contains only non-terminal nodes.
This rep-resentation is straightforward, however, some pro-ductions can be rather specific with long right handsides.
Another apparent drawback of this represen-tation is that it contains sequence information onlyabout nodes that belong to the same constituent.d-sequence: In this representation we aim to pre-serve more sequence information about adjacentconstituents in the sentence.
The simplest approachwould be to represent the sentence as the sequenceof part of speech (POS) tags but then we lose allthe abstraction provided by higher level nodes intree.
Instead, we introduce a more general represen-tation, d-sequence where the level of abstraction canbe controlled using a parameter d. The parse tree istruncated to depth at most d, and the leaves of theresulting tree listed left to right form the d-sequencerepresentation.
For example, in Figure 1, the linedepicts the cutoff at depth 2.Next the representation is further augmented; allphrasal nodes in the d-sequence are annotated (con-catenated) with the left-most leaf that they domi-nate in the full non-lexicalized parse tree.
This isshown as suffixes on the S, NP and VP nodes inthe figure.
Such annotation conveys richer informa-tion about the structure of the subtree below nodesin the d-sequence.
For example, ?the chairs?, ?hischairs?, ?comfortable chairs?
will be represented asNPDT, NPPRP$ and NPJJ.
In the resulting representa-tions, sentences are viewed as sequences of syntacticwords (w1,w2...,wk), k ?
p, where p is the length ofthe full POS sequence and each wi is either POS tagor a phrasal node+POS tag combination.Figure 1: Example for d-sequence representationIn our example, at depth-2, the quotation sentencegets the representation (w1=?
, w2=SDT , w3=, , w4=?
,w5=NPNNP , w6=VPVBD , w7=.)
where the actual quoteis omitted.
Sentences that contain attributions arelikely to appear more similar to each other whencompared using this representation in contrast torepresentations derived from word or POS sequence.The depth-3 sequence is also indicated in the figure.The main verb of a sentence is central to its struc-ture, so the parameter d is always set to be greaterthan that of the main verb and is tuned to optimizeperformance for coherence prediction.3.2 Implementing the modelWe adapt two models of coherence to operate overthe two syntactic representations.3.2.1 Local co-occurrence modelThis model is a direct extension from our pilotstudy.
It allows us to test the assumption that coher-ent discourse is characterized by syntactic regulari-ties in adjacent sentences.
We estimate the proba-bilities of pairs of syntactic items from adjacent sen-tences in the training data and use these probabilitiesto compute the coherence of new texts.The coherence of a text T containing n sentences(S1...Sn) is computed as:P (T ) =n?i=2|Si|?j=11|Si?1||Si?1|?k=1p(Sji |Ski?1)where Syx indicates the yth item of Sx.
Itemsare either productions or syntactic word unigramsdepending on the representation.
The conditionalprobabilities are computed with smoothing:1160Cluster a Cluster bADJP ?
JJ PP | VP ?
VBZ ADJP VP ?
VB VP | VP ?
MD VP[1] This method VP-[is ADJP-[capable of sequence-specific [1] Our results for the difference in reactivity VP-[candetection of DNA with high accuracy]-ADJP]-VP .
VP-[be linked to experimental observations]-VP]-VP .
[2] The same VP-[is ADJP-[true for synthetic polyamines [2] These phenomena taken together VP-[can VP-[be consideredsuch as polyallylamine]-ADJP]-VP .
as the signature of the gelation process]-VP]-VP .Table 4: Example syntactic similarity clusters.
The top two descriptive productions for each cluster are also listed.p(wj |wi) =c(wi, wj) + ?Cc(wi) + ?C ?
|V |wherewi andwj are syntactic items and c(wi, wj) isthe number of sentences that contain the item wi im-mediately followed by a sentence that contains wj .|V | is the vocabulary size for syntactic items.3.2.2 Global structureNow we turn to a global coherence approachthat implements the assumption that sentences withsimilar syntax have the same communicative goalas well as captures the patterns in communicativegoals in the discourse.
This approach uses a Hid-den Markov Model (HMM) which has been a popu-lar implementation for modeling coherence (Barzi-lay and Lee, 2004; Fung and Ngai, 2006; Elsneret al 2007).
The hidden states in our model de-pict communicative goals by encoding a probabilitydistribution over syntactic items.
This distributiongives higher weight to syntactic items that are morelikely for that communicative goal.
Transitions be-tween states record the common patterns in inten-tional structure for the domain.In this syntax-HMM, states hk are created byclustering the sentences from the documents in thetraining set by syntactic similarity.
For the pro-ductions representation of syntax, the features forclustering are the number of times a given produc-tion appeared in the parse of the sentence.
For thed-sequence approach, the features are n-grams ofsize one to four of syntactic words from the se-quence.
Clustering was done by optimizing for av-erage cosine similarity and was implemented usingthe CLUTO toolkit (Zhao et al 2005).
C clustersare formed and taken as the states of the model.
Ta-ble 4 shows sentences from two clusters formed onthe abstracts of journal articles using the productionsrepresentation.
One of them, cluster (a), appearsto capture descriptive sentences and cluster (b) in-volves mostly speculation type sentences.The emission probabilities for each state are mod-eled as a (syntactic) language model derived fromthe sentences in it.
For productions representa-tion, this is the unigram distribution of produc-tions from the sentences in hk.
For d-sequences,the distribution is computed for bigrams of syntac-tic words.
These language models use Lidstonesmoothing with constant ?E .
The probability for asentence Sl to be generated from state hk, pE(Sl|hk)is computed using these syntactic language models.The transition probability pM from a state hi tostate hj is computed as:pM (hj |hi) =d(hi, hj) + ?Md(hi) + ?M ?
Cwhere d(hi) is the number of documents whose sen-tences appear in hi and d(hi, hj) is the number ofdocuments which have a sentence in hi which is im-mediately followed by a sentence in hj .
In addi-tion to the C states, we add one initial hS and onefinal hF state to capture document beginning andend.
Transitions from hS to any state hk recordshow likely it is for hk to be the starting state for doc-uments of that domain.
?M is a smoothing constant.The likelihood of a text with n sentences is givenby P (T ) =?h1...hn?nt=1 pM (ht|ht?1)pE(St|ht).All model parameters?the number of clustersC, smoothing constants ?C , ?E , ?M and d ford-sequences?are tuned to optimize how well themodel can distinguish coherent from incoherent ar-ticles.
We describe these settings in Section 5.1.4 Content and entity grid modelsWe compare the syntax model with content modeland entity grid methods.
These approaches are themost popular ones from prior work and also allowus to test the complementary nature of syntax with1161lexical statistics and entity structure.
This sectionexplains how we implemented these approaches.Content models introduced by Barzilay and Lee(2004) and Fung and Ngai (2006) use lexicallydriven HMMs to capture coherence.
The hiddenstates represent the topics of the domain and en-code a probability distribution over words.
Transi-tions between states record the probable successionof topics.
We built a content model using our HMMimplementation.
Clusters are created using word bi-gram features after replacing numbers and propernames with tags NUM and PROP.
The emissions aregiven by a bigram language model on words fromthe clustered sentences.
Barzilay and Lee (2004)also employ an iterative clustering procedure beforefinalizing the states of the HMM but our methodonly uses one-step clustering.
Despite the differ-ence, the content model accuracies for our imple-mentation are quite close to that from the original.For the entity grid model, we follow the gen-erative approach proposed by Lapata and Barzilay(2005).
A text is converted into a matrix, where rowscorrespond to sentences, in the order in which theyappear in the article.
Columns are created one foreach entity appearing in the text.
Each cell (i,j) isfilled with the grammatical role ri,j of the entity jin sentence i.
We computed the entity grids usingthe Brown Coherence Toolkit4.
The probability ofthe text (T ) is defined using the likely sequence ofgrammatical role transitions.P (T ) =m?j=1n?i=1p(ri,j |ri?1,j ...ri?h,j)for m entities and n sentences.
Parameter h controlsthe history size for transitions and is tuned duringdevelopment.
When h = 1, for example, only thegrammatical role for the entity in the previous sen-tence is considered and earlier roles are ignored.5 Evaluating syntactic coherenceWe follow the common approach from prior workand use pairs of articles, where one has the originaldocument order and the other is a random permuta-tion of the sentences from the same document.
Sincethe original article is always more coherent than arandom permutation, a model can be evaluated using4http://www.cs.brown.edu/~melsner/manual.htmlthe accuracy with which it can identify the originalarticle in the pair, i.e.
it assigns higher probabilityto the original article.
This setting is not ideal buthas become the de facto standard for evaluation ofcoherence models (Barzilay and Lee, 2004; Elsneret al 2007; Barzilay and Lapata, 2008; Karamaniset al 2009; Lin et al 2011; Elsner and Charniak,2011).
It is however based on a reasonable assump-tion as recent work (Lin et al 2011) shows that peo-ple identify the original article as more coherent thanits permutations with over 90% accuracy and asses-sors also have high agreement.
Later, we presentan experiment distinguishing conference from work-shop articles as a more realistic evaluation.We use two corpora that are widely employed forcoherence prediction (Barzilay and Lee, 2004; El-sner et al 2007; Barzilay and Lapata, 2008; Lin etal., 2011).
One contains reports on airplane acci-dents from the National Transportation Safety Boardand the other has reports about earthquakes from theAssociated Press.
These articles are about 10 sen-tences long.
These corpora were chosen since withineach dataset, the articles have the same intentionalstructure.
Further, these corpora are also standardones used in prior work on lexical, entity and dis-course relation based coherence models.
Later inSection 6, we show that the models perform well onthe academic genre and longer articles too.For each of the two corpora, we have 100 arti-cles for training and 100 (accidents) and 99 (earth-quakes) for testing.
A maximum of 20 random per-mutations were generated for each test article to cre-ate the pairwise data (total of 1986 test pairs for theaccident corpus and 1956 for earthquakes).5 Thebaseline accuracy for random prediction is 50%.The articles were parsed using the Stanford parser(Klein and Manning, 2003).5.1 Accuracy of the syntax modelFor each model, the relevant parameters were tunedusing 10-fold cross validation on the training data.In each fold, 90 documents were used for trainingand evaluation was done on permutations from theremaining articles.
After tuning, the final model wastrained on all 100 articles in the training set.5We downloaded the permutations from http://people.csail.mit.edu/regina/coherence/CLsubmission/1162Table 5 shows the results on the test set.
Thebest number of clusters and depth for d-sequencesare also indicated.
Overall, the syntax models workquite well, with accuracies at least 15% or more ab-solute improvement over the baseline.In the local co-occurrence approach, both pro-ductions and d-sequences provide 72% accuracy forthe accidents corpus.
For the earthquake corpus,the accuracies are lower and the d-sequence methodworks better.
The best depth setting for d-sequenceis rather small: depth of main verb (MVP) + 2 (or 1),and indicates that a fairly abstract level of nodes ispreferred for the patterns.
For comparison, we alsoprovide results using just the POS tags in the modeland this is worse than the d-sequence approach.The global HMM model is better than the localmodel for each representation type giving 2 to 38%better accuracies.
Here we see a different trend forthe d-sequence representation, with better results forgreater depths.
At such depths (8 and 9) below themain verb, the nodes are mostly POS tags.Overall both productions and d-sequence workcompetitively and give the best accuracies when im-plemented with the global approach.5.2 Comparison with other approachesFor our implementations of the content and entitygrid models, the best accuracies are 71% on the ac-cidents corpus and 85% on the earthquakes one, sim-ilar to the syntactic models.Ideally, we would like to combine models but wedo not have separate training data.
So we performthe following classification experiment which com-bines the predictions made by different models onthe test set.
Each test pair (article and permutation)forms one example and is given a class value of 0 or1 depending on whether the first article in the pairis the original one or the second one.
The exampleis represented as an n-dimensional vector, where nis the number of models we wish to combine.
Forinstance, to combine content models and entity grid,two features are created: one of these records the dif-ference in log probabilities for the two articles fromthe content model, the other feature indicates the dif-ference in probabilities from the entity grid.A logistic regression classifier is trained to pre-dict the class using these features.
The test pairs arecreated such that an equal number of examples haveModel Accidents EarthquakeParameter Acc Parameter AccA.
Local co-occurrenceProdns 72.8 55.0d-seq dep.
MVP+2 71.8 dep.
MVP+1 65.1POS 61.3 42.6B.
HMM-syntaxProdns clus.
37 74.6 clus.
5 93.8d-seq dep.
MVP+8 82.2 dep.
MVP+9 86.5clus.
8 clus.
45C.
Other approachesEgrid history 1 67.6 history 1 82.2Content clus.
48 71.4 clus.
23 84.5Table 5: Accuracies on accident and earthquake corporaModel Accid.
Earthq.Content + Egrid 76.8 90.7Content + HMM-prodn 74.2 95.3Content + HMM-d-seq 82.1 90.3Egrid + HMM-prodn 79.6 93.9Egrid + HMM-d-seq 84.2 91.1Egrid + Content + HMM-prodn 79.5 95.0Egrid + Content + HMM-d-seq 84.1 92.3Egrid + Content + HMM-prodn 83.6 95.7+ HMM-d-seqTable 6: Accuracies for combined approachesclass 0 and 1, so the baseline accuracy is 50%.
Werun this experiment using 10-fold cross validation onthe test set after first obtaining the log probabilitiesfrom individual models.
In each fold, the training isdone using the pairs from 90 articles and tested onpermutations from the remaining 10 articles.
Theseaccuracies are reported in Table 6.
When the accu-racy of a combination is better than that using any ofits smaller subsets, the value is bolded.We find that syntax supplements both content andentity grid methods.
While on the airplane corpussyntax only combines well with the entity grid, onthe earthquake corpus, both entity and content ap-proaches give better accuracies when combined withsyntax.
However, adding all three approaches doesnot outperform combinations of any two of them.This result can be due to the simple approach thatwe tested for combination.
In prior work, contentand entity grid methods have been combined gen-eratively (Elsner et al 2007) and using discrimina-tive training with different objectives (Soricut and1163Marcu, 2006).
Such approaches might bring outthe complementary strengths of the different aspectsbetter and we leave such analysis for future work.6 Predictions on academic articlesThe distinctive intentional structure of academic ar-ticles has motivated several proposals to define andannotate the communicative purpose (argumentativezone) of each sentence (Swales, 1990; Teufel et al1999; Liakata et al 2010).
Supervised classifierswere also built to identify these zones (Teufel andMoens, 2000; Guo et al 2011).
So we expect thatthese articles form a good testbed for our models.
Inthe remainder of the paper, we examine how unsu-pervised patterns discovered by our approach relateto zones and how well our models predict coherencefor articles from this genre.We employ two corpora of scientific articles.ART Corpus: contains a set of 225 Chemistry jour-nal articles that were manually annotated for inten-tional structure (Liakata and Soldatova, 2008).
Eachsentence was assigned one of 11 zone labels: Result,Conclusion, Objective, Method, Goal, Background,Observation, Experiment, Motivation, Model, Hy-pothesis.
For our study, we use the annotation ofthe introduction and the abstract sections.
We dividethe data into training, development and test sets.
Forabstracts, we have 75, 50 and 100 for these sets re-spectively.
For introductions, this split is 75, 31, 82.6ACL Anthology Network (AAN) Corpus: Radevet al(2009) provides the full text of publicationsfrom ACL venues.
These articles do not have anyzone annotations.
The AAN corpus is producedfrom OCR analysis and no section marking is avail-able.
To recreate these, we use the Parscit tagger7(Councill et al 2008).
We use articles from years1999 to 2011.
For training, we randomly choose 70articles from ACL and NAACL main conferences.Similarly, we obtain a development corpus of 36ACL-NAACL articles.
We create two test sets: onehas 500 ACL-NAACL conference articles and an-other has 500 articles from ACL-sponsored work-shops.
We only choose articles in which all threesections?abstract, introduction and related work?6Some articles did not have labelled ?introduction?
sectionsresulting in fewer examples for this setup.7http://aye.comp.nus.edu.sg/parsCit/could be successfully identified using Parscit.8This data was sentence-segmented using MxTer-minator (Reynar and Ratnaparkhi, 1997) and parsedwith the Stanford Parser (Klein and Manning, 2003).For each corpus and each section, we train all oursyntactic models: the two local coherence modelsusing the production and d-sequence representationsand the HMM models with the two representations.These models are tuned on the respective develop-ment data, on the task of differentiating the originalfrom a permuted section.
For this purpose, we cre-ated a maximum of 30 permutations per article.6.1 Comparison with ART Corpus zonesWe perform this analysis using the ART corpus.
Thezone annotations present in this corpus allow us todirectly test our first assumption in this work, thatsentences with similar syntax have the same com-municative goal.For this analysis, we use the the HMM-prodmodel for abstracts and the HMM-d-seq model forintroductions.
These models were chosen becausethey gave the best performance on the ART corpusdevelopment sets.9 We examine the clusters cre-ated by these models on the training data and checkwhether there are clusters which strongly involvesentences from some particular annotated zone.For each possible pair of cluster and zone (Ci,Zj), we compute c(Ci, Zj): the number of sentencesin Ci that are annotated as zone Zj .
Then we use achi-square test to identify pairs for which c(Ci, Zj)is significantly greater than expected (there is a ?pos-itive?
association between Ci and Zj) and pairswhere c(Ci, Zj) is significantly less than chance (Ciis not associated with Zj).
A 95% confidence levelwas used to determine significance.The HMM-prod model for abstracts has 9 clusters(named Clus0 to 8) and the HMM-d-seq model forintroductions has 6 clusters (Clus0 to 5).
The pair-ings of these clusters with zones which turned out tobe significant are reported in Table 7.
We also re-port for each positively associated cluster-zone pair,the following numbers: matches c(Ci, Zj), preci-sion c(Ci, Zj)/|Ci| and recall c(Ci, Zj)/|Zj |.8We also exclude introduction and related work sectionslonger than 50 sentences and those shorter than 4 sentencessince they often have inaccurate section boundaries.9Their test accuracies are reported in the next section.1164Abstracts (HMM-prod 9 clusters)Positive associations matches prec.
recallClus5 - Model 7 17.1 43.8Clus7 - Objective 27 27.6 32.9Clus7 - Goal 16 16.3 55.2Clus0 - Conclusion 15 50.0 12.1Clus6 - Conclusion 27 51.9 21.8Not associated: Clus7 - Conclusion,Clus8 - ConclusionIntroductions (HMM-d-seq 6 clusters)Positive associations matches prec.
recallClus2-Background 161 64.9 14.2Clus3-Objective 37 7.9 38.5Clus4-Goal 29 9.8 32.6Clus4-Hypothesis 12 4.1 52.2Clus5-Motivation 61 12.9 37.4Not associated: Clus1 - Motivation, Clus2 - Goal,Clus4 - Background, Clus 5 - ModelTable 7: Cluster-Zone mappings on the ART CorpusThe presence of significant associations validateour intuitions that syntax provides clues about com-municative goals.
Some clusters overwhelminglycontain the same zone, indicated by high precision,for example 64% of sentences in Clus2 from intro-duction sections are background sentences.
Otherclusters have high recall of a zone, 55% of all goalsentences from the abstracts training data is capturedby Clus7.
It is particularly interesting to see thatClus7 of abstracts captures both objective and goalzone sentences and for introductions, Clus4 is a mixof hypothesis and goal sentences which intuitivelyare closely related categories.6.2 Original versus permuted sectionsWe also explore the accuracy of the syntax modelsfor predicting coherence of articles from the test setof ART corpus and the 500 test articles from ACL-NAACL conferences.
We use the same experimen-tal setup as before and create pairs of original andpermuted versions of the test articles.
We created amaximum of 20 permutations for each article.
Thebaseline accuracy is 50% as before.For the ART corpus, we also built an oracle modelof annotated zones.
We train a first order MarkovChain to record the sequence of zones in the trainingarticles.
For testing, we assume that the oracle zoneis provided for each sentence and use the model topredict the likelihood of the zone sequence.
Resultsfrom this model represent an upper bound becausean accurate hypothesis of the communicative goal isavailable for each sentence.The accuracies are presented in Table 8.
Overall,the HMM-d-seq model provides the best accuracies.The highest results are obtained for ACL introduc-tion sections (74%).
These results are lower thanthat obtained on the earthquake/accident corpus butthe task here is much harder: the articles are longerand the ACL corpus also has OCR errors which af-fect sentence segmentation and parsing accuracies.When the oracle zones are known, the accuracies aremuch higher on the ART corpus indicating that theintentional structure of academic articles is very pre-dictive of their coherence.6.3 Conference versus workshop papersFinally, we test whether the syntax-based model candistinguish the structure of conference from work-shop articles.
Conferences publish more completeand tested work and workshops often present pre-liminary studies.
Workshops are also venues to dis-cuss a focused and specialized topic.
So the wayinformation is conveyed in the abstracts and intro-ductions would vary in these articles.We perform this analysis on the ACL corpus andno permutations are used, only the original text ofthe 500 articles each in the conference and work-shop test sets.
While permutation examples providecheap training/test data, they have a few unrealisticproperties.
For example, both original and permutedarticles have the same length.
Further some permu-tations could result in an outstandingly incoherentsample which is easily distinguished from the origi-nal articles.
So we use the conference versus work-shop task as another evaluation of our model.We designed a classification experiment for thistask which combines features from the different syn-tax models that were trained on the ACL conferencetraining set.
We include four features indicating theperplexity of an article under each model (Local-prod, Local-d-seq, HMM-prod, HMM-d-seq).
Weuse perplexity rather than probability because thelength of the articles vary widely in contrast to theprevious permutation-based tests, where both per-mutation and original article have the same length.We compute perplexity as P (T )?1/n, where n isthe number of words in the article.
We also obtainthe most likely state sequence for the article under1165Data Section Test pairs Local-prod Local-d-seq HMM-prod HMM-d-seq Oracle zonesART CorpusAbstract 1633 57.0 52.9 64.1 55.0 80.8Intro 1640 44.5 54.6 58.1 64.6 94.0ACL ConferenceAbstract 8815 44.0 47.2 58.2 63.7Intro 9966 54.5 53.0 64.4 74.0Rel.
wk.
10,000 54.6 54.4 57.3 67.3Table 8: Accuracy in differentiating permutation from original sections on ACL and ART test sets.HMM-prod and HMM-d-seq models using Viterbidecoding.
Then the proportion of sentences fromeach state of the two models are added as features.We also add some fine-grained features from thelocal model.
We represent sentences in the train-ing set as either productions or d-sequence items andcompute pairs of associated items (xi, xj) from ad-jacent sentences using the same chi-square test asin our pilot study.
The most significant (lowest p-values) 30 pairs (each for production and d-seq) aretaken as features.10 For a test article, we computefeatures that represent how often each pair is presentin the article such that xi is in Sm and xj is in Sm+1.We perform this experiment for each section andthere are about 90 to 140 features for the differentsections.
We cast the problem as a binary classifi-cation task: conference articles belong to one classand workshop to the other.
Each class has 500 ar-ticles and so the baseline random accuracy is 50%.We perform 10-fold cross validation using logisticregression.
Our results were 59.3% accuracy for dis-tinguishing abstracts of conference verus workshop,50.3% for introductions and 55.4% for related work.For abstracts and related work, these accuracies aresignificantly better than baseline (95% confidencelevel from a two-sided paired t-test comparing theaccuracies from the 10 folds).
It is possible that in-troductions in either case, talk in general about thefield and importance of the problem addressed andhence have similar structure.Our accuracies are not as high as on permutationsexamples because the task is clearly harder.
It mayalso be the case that the prediction is more difficultfor certain papers than for others.
So we also ana-lyze our results by the confidence provided by theclassifier for the predicted class.
We consider onlythe examples predicted above a certain confidencelevel and compute the accuracy on these predictions.10A cutoff is applied such that the pair was seen at least 25times in the training data.Conf.
Abstract Intro Rel wk>= 0.5 59.3 (100.0) 50.3 (100.0) 55.4 (100.0)>= 0.6 63.8 (67.2) 50.8 (71.1) 58.6 (75.9)>= 0.7 67.2 (32.0) 54.4 (38.6) 63.3 (52.8)>= 0.8 74.0 (10.0) 51.6 (22.0) 63.0 (25.7)>= 0.9 91.7 (2.0) 30.6 (5.0) 68.1 (7.2)Table 9: Accuracy (% examples) above each confidencelevel for the conference versus workshop task.These results are shown in Table 9.
The proportionof examples under each setting is also indicated.When only examples above 0.6 confidence are ex-amined, the classifier has a higher accuracy of 63.8%for abstracts and covers close to 70% of the exam-ples.
Similarly, when a cutoff of 0.7 is applied to theconfidence for predicting related work sections, weachieve 63.3% accuracy for 53% of examples.
Sowe can consider that 30 to 47% of the examples inthe two sections respectively are harder to tell apart.Interestingly however even high confidence predic-tions on introductions remain incorrect.These results show that our model can success-fully distinguish the structure of articles beyond justclearly incoherent permutation examples.7 ConclusionOur work is the first to develop an unsupervisedmodel for intentional structure and to show thatit has good accuracy for coherence prediction andalso complements entity and lexical structure of dis-course.
This result raises interesting questions abouthow patterns captured by these different coherencemetrics vary and how they can be combined usefullyfor predicting coherence.
We plan to explore theseideas in future work.
We also want to analyze genredifferences to understand if the strength of these co-herence dimensions varies with genre.AcknowledgementsThis work is partially supported by a Google re-search grant and NSF CAREER 0953445 award.1166ReferencesRegina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34(1):1?34.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofNAACL-HLT, pages 113?120.Xavier Carreras, Michael Collins, and Terry Koo.
2008.Tag, dynamic programming, and the perceptron forefficient, feature-rich parsing.
In Proceedings ofCoNLL, pages 9?16.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of ACL, pages 173?180.Jackie C.K.
Cheung and Gerald Penn.
2010.
Utilizingextra-sentential context for parsing.
In Proceedings ofEMNLP, pages 23?33.Christelle Cocco, Raphae?l Pittier, Franc?ois Bavaud, andAris Xanthos.
2011.
Segmentation and clustering oftextual sequences: a typological approach.
In Pro-ceedings of RANLP, pages 427?433.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31:25?70.Isaac G. Councill, C. Lee Giles, and Min-Yen Kan. 2008.Parscit: An open-source crf reference string parsingpackage.
In Proceedings of LREC, pages 661?667.Micha Elsner and Eugene Charniak.
2008.
Coreference-inspired coherence modeling.
In Proceedings of ACL-HLT, Short Papers, pages 41?44.Micha Elsner and Eugene Charniak.
2011.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of ACL-HLT, pages 125?129.Micha Elsner, Joseph Austerweil, and Eugene Charniak.2007.
A unified local and global model for discoursecoherence.
In Proceedings of NAACL-HLT, pages436?443.Pascale Fung and Grace Ngai.
2006.
One story, oneflow: Hidden markov story models for multilingualmultidocument summarization.
ACM Transactions onSpeech and Language Processing, 3(2):1?16.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 3(12):175?204.Yufan Guo, Anna Korhonen, and Thierry Poibeau.
2011.A weakly-supervised approach to argumentative zon-ing of scientific documents.
In Proceedings ofEMNLP, pages 273?283.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings ofACL-HLT, pages 586?594, June.Nikiforos Karamanis, Chris Mellish, Massimo Poesio,and Jon Oberlander.
2009.
Evaluating centering forinformation ordering using corpora.
ComputationalLinguistics, 35(1):29?46.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL, pages423?430.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and representa-tions.
In Proceedings of IJCAI.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedings ofACL, pages 545?552.Maria Liakata and Larisa Soldatova.
2008.
Guidelinesfor the annotation of general scientific concepts.
JISCProject Report.Maria Liakata, Simone Teufel, Advaith Siddharthan, andColin Batchelor.
2010.
Corpora for the conceptualisa-tion and zoning of scientific papers.
In Proceedings ofLREC.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the PennDiscourse Treebank.
In Proceedings of EMNLP,pages 343?351.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2011.
Au-tomatically evaluating text coherence using discourserelations.
In Proceedings of ACL-HLT, pages 997?1006.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.Emily Pitler and Ani Nenkova.
2008.
Revisiting read-ability: A unified framework for predicting text qual-ity.
In Proceedings of EMNLP, pages 186?195.Dragomir R. Radev, Mark Thomas Joseph, Bryan Gib-son, and Pradeep Muthukrishnan.
2009.
A Bibliomet-ric and Network Analysis of the field of ComputationalLinguistics.
Journal of the American Society for Infor-mation Science and Technology.David Reitter, Johanna D. Moore, and Frank Keller.2006.
Priming of Syntactic Rules in Task-OrientedDialogue and Spontaneous Conversation.
In Proceed-ings of the 28th Annual Conference of the CognitiveScience Society, pages 685?690.Jeffrey C. Reynar and Adwait Ratnaparkhi.
1997.
Amaximum entropy approach to identifying sentenceboundaries.
In Proceedings of the fifth conference onApplied natural language processing, pages 16?19.Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In Pro-ceedings of COLING-ACL, pages 803?810.1167John Swales.
1990.
Genre analysis: English in academicand research settings, volume 11.
Cambridge Univer-sity Press.Simone Teufel and Marc Moens.
2000.
What?s yoursand what?s mine: determining intellectual attributionin scientific text.
In Proceedings of EMNLP, pages 9?17.Simone Teufel, Jean Carletta, and Marc Moens.
1999.An annotation scheme for discourse-level argumen-tation in research articles.
In Proceedings of EACL,pages 110?117.Ying Zhao, George Karypis, and Usama Fayyad.2005.
Hierarchical clustering algorithms for docu-ment datasets.
Data Mining and Knowledge Discov-ery, 10:141?168.1168
