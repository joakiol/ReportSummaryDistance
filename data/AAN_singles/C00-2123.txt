Word Re-order ing  and  DP-based  Search  in S ta t i s t i ca l  Mach ineTrans la t ionChristoph Til lmann and Hermann NeyLehrstuhl fiir Informatik VI, Computer Science DepartmentRWTH Aachen - University of TechnologyD-52056 Aachen, Germany{t illmann, ney}@inf ormat ik.
rwth-aachen, deAbstractIn this paper, we describe a search procedure for sta-tistical machine translation (MT) based on dynmnicprogramming (DP).
Starting from a DP-based solu-tion to the traveling salesman problem, we presenta novel technique to restrict the possible word re-ordering between source and target language in or-der to achieve an efficient search algorithm.
A searchrestriction especially useful for tile translation di-rection from German to English is presented.
Theexperimental tests are carried out on the Verbmo-bil task (Germm>English, 8000-word vocabulary),which is a limited-domain spoken-language task.1 IntroductionThe goal of machine translation is tile translationof a text given in some source language into a tar-gel: language.
We are given a source string f J  =fl ...fj.-.f.l of length J, wlfich is to be translated intoa target string c\[ = cl ...ei...el of length I. Amongall possible target strings, we will choose the stringwith the highest probability:dI = argmax {Pr(e{lff)} q-- argmax {Pr (e l ) .P r ( f f le l )  } (1)The argmax operation denotes the search problem,i.e.
the generation of the output sentence in the tar-get language.
Pr(c~) is the language model of timtarget language, whereas Pr(fi'le*l) is the transla-tion model.
Our approach uses word-to-word epen-dencies between source and target words.
The modelis often further restricted so that each source wordis assigned to exactly one target word (Brown et al,1993; Ney et al, 2000).
These alignment modelsare similar to the concept of hidden Markov models(HMM) in speech recognition.
The alignment map-ping is j --+ i = aj from source position j to targetposition i = aj.
The use of this alignment modelraises major problems if a source word has to bealigned to several target words, e.g.
when translat-ing German compound nouns.
A siuLple extensionwill be used to handle this problem.In Section 2, we briefly review our approach to sta-tistical machine translation.
In Section 3, we in-troduce our novel concept to word re-ordering anda DP-based search, which is especially suitable forthe translation direction fl'om German to English.This approach is compared to another re-orderingscheme presented in (Berger et al, 1996).
In Sec-tion 4, we present the t)erformance measures usedand give translation results on the Verbmobil task.2 Basic ApproachIn this section, we briefly review our translation ap-proach.
In Eq.
(1), Pr(C~l) is the language model,which is a trigrain language model in this case.
Forthe translation model Pr(fil le{), we go on the as-sunlption that each source word is aligned to ex-actly one target word.
The alignment model usestwo kinds of parameters: alignment probabilitiesp(ajlaj_l, I, J), where the probability of alignmentaj for position j deI)ends on tile previous alignmentposition aj-i  (Ney et al, 2000) and lexicon proba.-bilities p(\]~le~j).
When aligning the words in par-allel texts (tbr language pairs like Spanish-English,French-Englisll, Italian-German,...), we typically ob-serve a strong localization effect.
In many cases,there is an even stronger estriction: over large por-tions of tile source string, the alignment is monotone.2.1 Inverted AlignmentsTo explicitly handle the word re-ordering betweenwords in source and target language, we use the con-cept of the so-called inverted aligmnents as given in(Ney et al, 2000).
An inverted alignment is definedas follows:inverted alignment: i -+ j = bi.Target positions i are mapped to source positions bi.What is important and is not expressed by the nota-tion is the so-called coverage constraint: each sourceposition j should be 'hit' exactly once by the pathof the inverted aligmnent b~ = bL...bi...bi.
Using theinverted alignments in the maximum approximation,850we obtain as search criterion:/ f l  i - I  sn} .
v ( ,Z l5  ?ell k i= l' }} ?
max  I I  z,.5.
v(f,,,le*)\] ---hi  i=1= max V( J I I ) -maxI i I el 'hi i=1"p(bi\[bi-l,-\[,'\])'P(fbilCi)\] }},where the two products over i have l)een merged into) i -1  a single product ()vet i. I (cilei_~) is tim trigramlanguage model probability.
The inverted alignmentprobability p(bi\[bi-l, I, .1) and the lexicon probabil-ity p(J'~,~ led are obtained by relative fl'equency es-timal;es frosll the Viterbi alignment path after thefinal training iteration.
The details are given in(Och art(1 Ney, 2000).
The sentence length prob-ability p(J\[1) is omitted without any loss in per-tbrmance.
For the inverted alignment probabilityp(bi\[bi-~, I J), we drop the dependence on the tar-get sentence length I.2.2  Word Jo in ingThe baseline alignment model does not pernfit hat asource word is aligned to two or more target words,e.g.
%r the translation direction from German to\]?,nglish, the German (:Oml)ound I IOlStl  'Zahnarztter-rain' causes ira>blares, because it must be translatedby the two target words dcntist'.s appoi'ntmcnt.
Weuse a solution to this 1)roblenl similar to the onepresented in (()ell el; al., 1999), where target wordsare joined during training.
The word joining is (lottoon the basis of a likelihood criterion.
An extendedlexicon model is defined, and its likelihood is com-pared to a baseline lexicon model, which takes onlysingle-word ependencies into aecollnt.
E.g.
when'Zahnarzttermin' is aligned to dentist'.s, the extendedlexicon model might learn that 'Zahnarzttcrmin' ac-tually has to be aligned to both dentist's and ap-pointment.
In the following, we assmne that thisword joining has been carried out.
"I DP  Algor i thm for StatisticalMachine Translationin order to handle the necessary word re-ordering as&n optimization problem within our dynmnic pro-gramming approach, we describe a solution to thetraveling salesnmn problem (TSP) which is basedon dynamic programming (Held, Karp, 1962).
Thetraveling salesman problem is an oi)timization prob-lem which is defined as follows: given are a set ofMayoffourththeollyouvisitnotcancolleaguemycasethisInO O O O O O O O O O ~_ .
.
.
.OO O O O O O O O O /O~ O O OO O O O O O O O O/O O O OO O O O O O O O ?
O O O OO O O O O O O O// O O O O OO O O O O O O/ /O  O O O O Oo o o o o ooo Oo Oo Oo oO oO oO o o o ??_...
o.... o ...o._...o.....?O O O O O/~ O O O O O O OO O O9/7 .O  O O O O O O O OO O f  O O O O O O O O O OoO oOoOOoOOoOOI I I I 1 I I I I I I 1 II d F k rn K S a v M n b. i a a e o i m i ~ i ee / n i / e e i C ss I n n I r h ue e t t Cm g e he I/ anFigm'e 1: Re-ordering for the Gerlnan verbgroul).cities S = Sl ,--- ,  s ,  and tbr each pair of cities si, sjthe cost dij > 0 for traveling flom city s: to city.sj.
We arc.'
looking for the shortest tour visitingall cities exactly once while starting and ending incity sl.
A straightforward way to find the short-est tour is by trying all possible permutations of then cities.
The resulting algorithm has a complexityof O(n!).
Itowever, dynamic progrmnming can beused to find tile shortest our in exponential time,namely in O(n 2-2'~), using the algorithm by Ileld andKarp.
The approach recursively evahlates a quantityQ(C,j), where C is the set; of already visited citiesand sj is the last visited city.
Subsets C of increas-ing cardinality c are processed.
The algorithm worksdue to the fact that not all permutations of citieshave to be considered explicitly.
For a given partialhypothesis (C, j), the order in which the cities in (2have beast visited cast be ignored (except j) ,  only thescore for the best path reaching j has to be stored.This algorithm can be applied to statistical machinetranslation.
Using the concept of inverted align-ments, we explicitly take care of the coverage con-straint by introducing a coverage set C of source sen-tence positions that have been already processed.The advantage is that we can recombine search hy-potheses by dynmnic programming.
The cities ofthe traveling salesman probleIn correspond to source851Table 1: DP algorithm tbr statistical machine translation.input: source string fl..-f j .
..f. linitializationfor each cardinality c = 1, 2,.
- ?, J dotbr each pair (C,j), where j C C and ICl = c dotbr each tat'get word e E Emax5,e  t !Q~,(e,C,j) =p(fjle) {p(jlj',J).p(5) .pa(ele',e" ) -O~,,(e',C\ {j},j')}j lEC\{j}words f j  in the input string of length J.
For thefinal translation each source position is consideredexactly once.
Subsets of partial hypotheses withcoverage sets C of increasing cardinality c are pro-cessed.
For a trigrmn language model, the partialhyl)otheses are of tile form (c', c,(2, j) ,  e', c are thelast two target words, C is a coverage set for tile al-ready covered source positions and j is the last posi-tion visited.
Each distance in the traveling salesmanproblem now corresponds to the negative logarithmof tile product of the translation, alignment and lan-guage model probabilities.
The following auxiliaryquantity is defined:Qc~((?,~ C~j)  :.~.
probability of tile best partialhypothesis (c~, b~), whereC = {bt:\]k = 1, - - .
, i} ,  bi = j ,(2 i ~- C a ILd  C i _  1 ~ e I.The type of alignment we have considered so far re-quires the stone length tbr source and target sen-tence, i.e.
I = J.  Evidently, this is an unrealisticassumption, therefore we extend the concept of in-verted alignments as follows: Wtmn adding a newposition to the coverage set C, we might generate i-ther 5 = 0 or a = 1 new target words.
For 5 = 1, anew target language word is generated using the tri-gram language model p(e\[e', e").
For 5 = 0, no newtarget word is generated, while an additional sourcesentence position is covered.
A modified languageinodel probability pa(e\[c', c") is defined as follows:1.0 i f a=0Pa(ele"e") = p(e\]e',e") i fa  = 1We associate a distribution p(5) with the two cases5 = 0 and 5 = 1 and set p(5 = 1) = 0.7.The above auxiliary quantity satisfies tile followingrecursive DP equation:Qe, (c ,C , j )  =4.
mein 5.
Kollege~ ~  ,Q ~Verb_ .~  F in: l~--_ J J1.
In 7.nicht 9.
Sie2.
diesem 8. besuehen 10. am3.
Fall 11. vierten6.
kann 12.
Mai 13..Figure 2: Order in which source positions are visitedtbr the example given in Fig.1.P(fJIe) "max {P(JlJ','/)'P(5)" ~,c I!i' ec\ {j}?
pa( le', e")- Qe,, c \ {j}, j ') }.The DP equation is evaluated recursively for eachhypothesis (e ' ,e,C, j ) .
TILe resulting algorithm isdepicted in Table 1.
The complexity of the algorithmis O(E  a ?
j2 .2 .1) ,  where E is the size of the targetlanguage vocabulary.3.1 Word  Re-Order ing  w i th  VerbgroupRest r i c t ions :  Quas i -monotone  SearchThe above search space is still too large to allowthe translation of a inedium length input sentence.On the other hand, only very restricted re-orderingsare necessary, e.g.
for the translation direction fi'om852Table 2: Coverage set hyllothesis extensions for the IBM re-ordering.Predecessor eovera~e, set \[ I Successor coverage set({1,...,,, ,} \ ,l') ({1,... , , , ,} ,0({ \ ] , - -  .
, , , t} \ {l,/1 } , l ' )  -- ({1 , ' ' ' ,  'L~, } \ {/l} ,/)\ ,l') ({1,.--,,,,,} \ ,0({ I , .
.
.
, , , -  J} \ ,l') ({1,--.,,,,} \ ,m)Oerlnan to English the monotonicity constraint isviolated mainly with respect; to the German verb-group.
In German, the verbgroui) usually consistsof a left and a right verbal brace., whereas in En-glish the words of the verbgroul) usually tbrm a se-quence of consecutive words.
Our new al)t)roach,which is ('alle.d quas i -monotone  search, proce.ssesthe source sentence monotonically, while explicitlytaking into account the positions of the (-lel'lnanverbgroup.A typical situation is shown in Figure \].
Whentranslating the sentence monotonically fl'om left toright, the translation of the German finite verb'kmm', which is the left verbal brace in this case,is postponed mttil the German noun phrase 'meint(ollege' is translated, which is the subject of thesentence.
The.n, the.
German infinitive 'besuclmn'and the negation particle 'nicht;' are translated.
Thetrai ls\]at\oi l  of erie posit ion in the source sentencenmy be postponed ti)r up to L = 3 source positions,and the translation of u I) to two source positionsl ink be anticittated for at most 1~ = l0 source l)osi-tions.
To formalize the attl)roach, we introduce fourverbgroup stat;es S:?
hfitial (Z): A contiguous, initial block <)f s<mrcel)ositions is covered.?
Skitlped (K;): The translation of up to one wordmay be l)OStl)oned .?
Verl> (V): The translation of Ul> (;o two wordsmay be antMl)ated., Final (Y:): The rest of the sentence is pro-cessed monotonically taking accoullt of tit(; al:ready covered positions.\Vhile processing the source sentence monotonically,the iifitial state Z is entere.d whenever there are nommovered positions to the left of the rightmost cov-ered position.
The sequence of states needed tocarry out the.
word re-ordering example in Fig.
1is given in Fig.
2.
The 13 positions of the sourcesentence are processed in the order shown.
A posi-tion is presented by the word at that position.
Usingthese states, we define partial hypothesis extensions,which are of the following type:(S',C \ { j} , j ' )  --9 (S,C, j ) ,Not only the cove.rage set C and the posit\oilS j , j ' ,but also the verbgroup states S, S' are taken into ac-count.
~1~) be short, we omit the target words c, e' inthe tbrinulation of the search hypotheses.
There are13 types of extensions needed to describe the verb-group re-ordering.
The details are given in (Till-mann, 2000).
For each extension a new position isadded to the coverage set.
Covering the first lul-covered position in the source sentence, we use thelanguage model probatfility p(e\[$, $).
IIere, $ is thesentence boundary symbol, which is thought o be at;position 0 in the target sentence.
Tile search startsin the hyl)othesis (Z, {~}, 0).
{~} denotes the emptyset, where, no source sentence t)osition is covered.The following recursive quation is evaluated:= (2){gj\[j', 3).
p(5).
( ld, e").
p(fj lc) ?
ntax ~,c II?
max Oc, (c ' ,S ' ,C  \ {j},j')??.
(st,/) )(S t ,C \ t j} , f f )~(?
, ,C , j )j ' cc \u}The search ends in the hypotheses (Z, {1, .
- .
,  d}, j).
{1, .
.
.
,  d} de.notes a coverage se.t including all posi-tions from the starting 1)osition I to position J andj C {d-  L , - .
- ,  .\]}.
The final score is obtaiiled from:,nax P($l", c').
G '  (c, Z, { 1, .
.
- ,  d}, .it,c ,c  Ij c{a  :.,..
.
,a}where p($lc, c/) denotes the trigram language model,which predicts the sentence boundary $ at tim endof the target sentence.
The complexity of the quasi-monotone search is O(\]'J a-,l- (\[~2-t-L-1~)).
The proofis given ill (Tilhnann, 2000).3.2 Re-order ing  w i th  IBM StyleRest r i c t ionsWe compare our new api)roach with tim word re-ordering used in the IBM translation approach(Berger et al, 1996).
A detailed descrit)tion of tilesearch procedure used is given in this patent.
Sourcesentence words are aligned with hypothesized targetsentence words, where the choice of a new sourceword, which has not been aligned with a target wordyet, is restricted I .
A procedural definitioll to restrictl In the approach described in (Berger et al, 1996), a mor-phological analysis is carried out and word morphenles ratherthin, full-form words are used during the search, ltere, weprocess only flfll-forin words within the trmmlation proce.durc.853the number of pernmtations carried out for the wordre-ordering is given.
During the search process, apartial hyt)othesis i  extended by choosing a sourcesentence position, which has not been aligned with atarget sentence t)osition yet.
Only one of the first npositions which are not already aligned in a partialhyt)othesis may be chosen, where n is set to 4.
Tilerestriction can be expressed in terms of the nmn-ber of uncovered source sentence positions to theleft of the rightmost position m in the coverage set.This munber must be less than or equal to n - 1.Otherwise for the predecessor search hyt)othesis, wewonld have chosen a position that would not havebeen among the first n uncovered t)ositions.Ignoring the identity of the target language wordse and c', the possible partial hypothesis extensionsdue to the IBM restrictions are shown in Table 2.In general, m, l, l' ~k {/1,12,/3} and in line umber 3and 4, l' must be chosen not to violate the abovere-ordering restriction.
Note that in line 4 the last;visited position for tile successor hypothesis mustbe m. Otherwise, there will be four uncovered po-sitions tbr the t)redecessor hypothesis violating therestriction.
A dynamic programming recursion sin>ilar to the one in Eq.
2 is evaluated.
In this case, wehave no finite-state restrictions for the search space.Tile search stm'ts in hyi)othesis ({0}, 0) and ends inthe hyt)otheses ({1 , .
.
.
, J} , j ) ,  with j C {1 , .
.
- ,d} .This approach leads to a search procedure with com-plexity O(E  a .
j4).
The proof is given in (Tilhnann,2000).4 Exper imenta l  Resu l t s4.1 The  Task and the CorpusWe have tested tim translation system Oil the Verb-mobil task (Wahlster 1993).
The Verbmobil task isan appointment scheduling task.
Two subjects areeach given a calendar and they are asked to schedulea meeting.
The translation direction is from Ger-man to English.
A summary of the corpus used inthe experiments i given in Table 3.
The perplexityfor the trigrmn language model used is 26.5.
Al-though the ultimate goal of tile Verbmobil projectis the translation of spoken language, the input usedfor the translation experinmnts reported on in thispaper is the (more or less) correct orthographic tran-scription of the spoken sentences.
Thus, the effectsof spontaneous speech are t)resent in the corpus, e.g.the syntactic structure of tile sentence is rather lessrestricted, however the effect of sl)eech recognitionerrors is not covered.For the experiments, we use a simt)le pret)rocessingstep.
German city names are replaced by categorymarkers.
The translation search is carried out withtlm category markers and tlm city names are resub-stituted into the target sentence as a postt)rocessingstep.Table 3: Training and test; conditions for the Verb-mobil task (*number of words without punctuationmarks).\ [German EnglishTraining: SelltencesWordsWords*Vocabulary SizeSingletonsTest-147: SentencesWordsPerplexity58073519523 549921418979 4536327939 46483454 16991471968 2173- 26.5Table 4: Multi-reference word error rate (roWER)and subjective sentence rror rate (SSER) for threedifferent search t)rocedures.Search CPU timeMethod \[sec\]MonS 0.9QmS 10.6IbnlS 28.6roWER SSER\[yo\] \[y0\]42.0 30.534.4 23.838.2 26.24.2 Per formance MeasuresThe following two error criteria are used ill our ex-t)erinmnts:?
roWER: multi-reference WER:We use the Levenshtein distance between tileautomatic translation and several referencetranslations as a measure of tile translation er-rors.
On average, 6 reference translations perautomatic translation are availal)le.
Tile Lev-enshtein distance between the automatic trans-lation and each of tile reference translations iscomt)uted, and the minimum Levenshtein dis-tance is taken.
This measure has the advantageof being completely automatic.?
SSER: subjective sentence rror rate:For a more detailed analysis, tile translationsare judged i)y a tminan test 1)erson.
For the er--ror counts, a range from 0.0 to 1.0 is used.
Anerror count of 0.0 is assigned to a perfect rans-lation, and an error count of 1.0 is assigned toa semantically and syntactically wrong transb>tion.4.3 Translat ion Exper imentsFor tile translation experiments, Eq.
2 is recursivelyevahlated.
We apply a beam search concet)t as inst)eech recognition.
However there is no global prun-ing.
Search hypotheses are i)rocessed separately ac-cording to their coverage set d. The best scored854hyi)othesis tbr each coverage set is comlmted:Om, , , , , , ( c )  = c,c',6",jThe hyl)othesis (d, e, $, C, j) is t)1'1151(;(l if:Q~,(e,S,C, j )  < to.O,~cam(C),where to is a threshold to control the mmlber of sur-viving hypotheses.
Additionally, for a given coverageset, at most 250 different hypotheses are kept dur-ing the search process, and the number of difl'erentwords to |)e hyl)othesized by a source word is lim-ited.
For each source word f ,  the list of its possibletranslations c is sorte(1 according to p(.flc) ?
p.,,.,,i(c),where Puui(e) is the unigrmn probability of the En-glish word c. It is sufficient o consi(ter only the best50 words.We show translation results for three at)l)roaches:tile monotone search (MonS):  where no word re-ordering is allowed (Tillmann, 1997), the quasi-monotone search (QmS) as 1)resented in this palseramt the IBM style ( IbmS) search as described inSection 3.2.TMsle 4: shows translation results tbr the three ap-I)roaches.
The eomlsuting time is given in terms ofCPU time per sentence (on a 450-MIlz l?entimn-III-PC).
Itere, the printing threshold to = 10.0 is used.q_5:anslation errors are reported in terms of multi-reference word error rate (roWER) and subjectives(mtenee rror rate (SSER).
The monotone searchtserforms worst in terms of both elTror rates 5IsWI~;I~.mid SSEIL The (;OSlll)lstislg time is low, sitlce 51o 5e-ordering is (:arried o551,.
'\]'he quasi-inonotone s archi)e1foI'551s t)est, in ter551s of l)oth error rates roWERand SSh;R. Additionally, it; works about 3 times asfast as the II3M style sem:eh.
For our demonstra-tion system, we typically use the pruning thresholdto = 5.0 to speed Ul) the search by a factor 5 whileallowing for a sm?fll degradation i  translation accu-racy.The effect of the pruning threshold to is shown inTable 5.
The coml)uting time, the number of searcherrors, and the mull;i-reference WEll, (roWER) areshown as a flmction of to.
The negative logarithmof to is reporte(t. The translation scores for the hy-i)otheses generated with different threshohl valuesto are compared to the translation scores obtainedwith a conservatively large threshold to = 10.0.
Foreach test series, we count tile mlml)er of sentenceswhose score is worse than the corresponding score ofthe tent; series with the conserw~tively large thresh-old to = 10.0, and this mm:ber is reported as thenumber of search errors, l)epending on the thresh-old to, the search algorithm may miss the globallyof)timal path which typically results in additionaltranslation errors.
Decreasing the threshold resultsin higher mWEl l  due to additional search errors.Table 5: Effectof sere'e\ errors (147 sentences).Search to I CPU time #searchMethod \[ \[sec\] errorQmS 0.0 0.07 1081.0 0.13 852.5 0.35 445.0 1.92 410.0 10.6 0lbmS 0.0 0.14 1081.0 0.3 842.5 0.8 455.0 4.99 710.0 28.52 0of the beam threshold on the nmnberroWER\[%142.637.836.634.634.543.439.539.138.338.2Table 6 shows example translations obtained by thethree, difli;rent appro~mhes.
Again, the monotonesearch performs worst.
In the second and thirdtranslation examples, the 1bins word re-orderingperforms worse than the QmS word re-ordering,since it; can not take l)roperly into at:count he wordre-ordering (115(; to the, German verbgroul).
TileGerman finite verbs 'l)in' (second exmnple) and'kSnnten' (third exmnt)le) are too far away from thet)ersonal pronouns 'ich' and 'Sic' (6 respectively 5source sentence positions).
In the last example, theless restrictive IbmS word re-ordering leads to a bet-ter translation, although the QmS translation is stillaceeptabh'.5 Conclusionin this pal)er, we have presented a new, efficientDP-based search procedure for statistical machinetranslation.
The approach assumes that, the word re-ordering is restricted to a few positions in the sourcesentence.
The approach as been successfiflly testedon the 8000-word Verbmobil task.
l'hture exten-sions of the system might include: 1) An e?tendedtranslation model, where we use.
more context o pre-(lict a source word.
2) An trot)roved language model,which takes into at:count syntactic structure, e.g.
toensure that a l)roper English verbgrout) in generated.3) A tight coupling with the speech recognizer out-lint.AcknowledgementsThis work has been supported as part of tile Verb-mobil project (contract number 01 IV 601 A) bythe Certain5 Federal Ministry of Education, Science,Research and Technology and as part of the Eutransproject (ESPRIT project number 30268) by the Eu-rot)can Comimmity.855Table 6: Example Translations for the Verbmobil task.Input: Ja ,  wunderbar.
KSnnen wir macllen.MonS: Yes, wonderflfl.
Can we do .QmS: Yes, wonderflfl.
We can do that .IbInS: Yes, wonderflfl.
We can do that .Input: Das ist zu knapt) , weil ich ab dem dritten in Kaiserslautern bin .
Genaugenommen imr atn dritten .Wie wSre es denn mn ghln Samstag , dem zehnten Februar ?MonS: That is too t ight,  becmlse I froln the third in Kaiserslautern.
hi fact only on the third .How about 5hm Saturday, the tenth of February "?QmS: That is too t ight, because Imn froln tim third in Kaiserslautern.
In fact only on the third .Ahm how about Saturday, February the tenth ?IbmS: That is too tight , froln the third because I will be in Kaiserslautern.
In fact only on the third.Atlln how about Saturday, February the tenth ?Input: Wenn Sie dann noch den siel)zehnten kSnnten , wSre das toll , j a .MonS: If you then also the seventeenth could, would be the great , yes .QmS: If you could then also the seventeenth , tlmt would be great,  yes .1binS: Then if you could even take seventeenth , that would be great, yes .Illtmt: .Ja, das kommt mir sehr gelegen.
Machen wir es dann am besten so.MonS: Yes, that suits me perfectly .
Do we should best like that .QmS: Yes , that suits me fine .
We do it like that then best; .IbmS: Yes , that suits me fine .
We should best do it like tlmt .ReferencesA.
L. Berger, P. F. Brown, S. A. Della Pietra,V.
J. Della Pietra, J. R. Gillett, A. S. Kehler,R.
L. Mercer.
1996.
Language Translation appa-ratus and method of using context-based transla-tion models.
United States Patent, Patent Num-ber 5510981, April.P.
F. Brown, V. J. Della Pietra, S. A. Della Pietra,and R. L. Mercer.
1993.
The Mathematics of Sta-tistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, vol.
19, no.
2,pp.
263-311.M.
Held, R. M. Karp.
1962.
A Dynmnic Progrmn-ruing Approach to Sequencing Problems.
&SIAM,vol.
10, no.
1, pp.
196-210.H.
Ney, S. Niessen, F. J. Och, H. Sawaf, C. Tilhnmm,S.
Vogel.
2000.
Algorittuns for Statistical %'ansla-tion of Spoken Language.
IEEE Transactions onSpeech and Audio Processing, vol.
8, no.
1, pp.
24-36.F.
J. Och, C. Tilhnann, H. Ney.
1999. hnprovcdAlignment Models for Statistical Machine ~IYans-lation.
In Proc.
of the ,loint SIGDAT Conferenceon Empirical Methods in Natural Language Pro-ccssing and Very Large Corpora (EMNLP99), pp.20-28, University of Maryland, College Park, MD,USA, June.F.
J. Och and Ney, H. 2000.
A comparison of align-ment models for statistical machine translation, hiProceedings of COLING 2000: The 18th Interna-tional Conference on Computational Linguistics,Saarbr/ieken, Germany, July-August.C.
Tilltnann.
2000.
Complexity of the Different Word Re-ordering Approaches.
Thedocument can be found under the URLhttp ://www-i6.
Informat ik.
RWTg-Aachen.
de/Colleagues/tilli/.
Aachen University of' Tech-nology, Aachen, Germany, June.C.
Tilhnann, S. Vogel, H. Ney and A. Zubiaga.
1997.A DP based Search Using Monotone Alignmentsin Statistical Translation.
In Proc.
of the 35th An-nual Conf.
of the Association for" ComputationalLinguistics, pp.
289 296, Madrid, Spain, July.W.
Wahlster.
1993.
Verbmol)ih Translation of Face-to-Face Dialogs.
MT Summit IV, pp.
127-135,Kobe, Japan.856
