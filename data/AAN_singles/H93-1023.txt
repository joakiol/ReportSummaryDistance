Topic and Speaker Identificationvia Large Vocabulary Continuous Speech RecognitionBarbara Peskin, Larry Gillick, Yoshiko Ito, Stephen Lowe, Robert Roth, Francesco Scattone,James Baker, Janet Baker, John Bridle, Melvyn Hunt, Jeremy OrloffDragon Systems, Inc.320 Nevada StreetNewton, Massachusetts 02160ABSTRACTIn this paper we exhibit a novel approach to the problemsof topic and speaker identification that makes use of a largevocabulary continuous speech recognizer.
We present a theo-retical framework which formulates the two tasks as comple-mentary problems, and describe the symmetric way in whichwe have implemented their solution.
Results of trials of themessage identification systems using the Switchboard corpusof telephone conversations are reported.1.
INTRODUCTIONThe task of topic identification is to select from a setof possibilities the topic that is most likely to representthe subject matter covered by a sample of speech.
Simi-larly, speaker identification requires electing from a listof possibilities the speaker most likely to have producedthe speech.
In this paper, we present a novel approachto the problems of topic and speaker identification whichuses a large vocabulary continuous speech recognizer asa preprocessor f the speech messages.The motivation for developing improved message identi-fication systems derives in part from the increasing re-liance on audio databases such as arise from voice mail,for example, and the consequent eed to extract informa-tion from them.
Technology that is capable of searchingsuch a database of recorded speech and classifying mate-rial by subject matter or by speaker would have substan-tial value, much as text-based information retrieval tech-nology has for textual corpora.
Several approaches totheproblems of topic and speaker identification have alreadyappeared in the literature.
For example, an approach totopic identification using wordspotting is described in \[1\]and approaches tothe speaker identification problem arereported in \[2\] and \[3\].Dragon Systems' approach to the message identificationtasks depends crucially on the existence of a large vo-cabulary continuous speech recognition system.
We viewthe tasks of topic and speaker identification as comple-mentary problems: for topic identification, the speaker isirrelevant and only the subject matter is of interest; forspeaker identification, the reverse is true.
For efficiencyof computation, in either case we first use a speaker-independent topic-independent recognizer to transcribethe speech messages.
The resulting output is then scoredusing topic-sensitive or speaker-sensitive models.This approach to the problem of message identificationis based on the belief that the contextual informationused in a full-scale recognition is invaluable in extract-ing reliable data from difficult speech channels.
For ex-ample, unlike standard approaches to topic identifica-tion through spotting a small collection of topic-specificwords, the approach via continuous speech recognitionshould more reliably detect keywords because of theacoustic and language model context available to therecognizer.
Moreover, with large vocabulary recognition,the list of keywords is no longer limited to a small setof highly topic-specific (but generally infrequent) words,and instead can grow to include much (or even all) of therecognition vocabulary.
The use of contextual informa-tion makes the message systems ufficiently robust hatthey are able to operate ven with vocabulary sizes andnoise environments that would make speech recognitionextremely difficult for other applications.To test our message identification systems, we havebeen using the "Switchboard" corpus of recorded tele-phone messages \[4\] collected by Texas Instruments andnow available through the Linguistic Data Consortium.This collection of roughly 2500 messages includes con-versations involving several hundred speakers.
Peoplewho volunteered to participate in this program wereprompted with a subject o discuss (chosen from a setthat they had previously specified as acceptable) andwere expected to talk for at least five minutes.
We re-port results of topic identification tests involving mes-sages on ten different opics using four and a half min-utes of speech and speaker identification tests involving24 speakers with test intervals containing as little as 10seconds of speech.In the next section, we describe the theoretical frame-work on which our message identification systems arebased and discuss the dual nature of the two problems.We then describe how this theory is implemented in thecurrent message processing systems.
Preliminary tests of119the systems using the Switchboard corpus are reportedin Section 4.
We close with a discussion of the test re-sults and plans for further research.2.
THEORET ICAL  FRAMEWORKOur approach to the topic and speaker identificationtasks is based on modelling speech as a stochastic pro-cess.
For each of the two problems, we assume that agiven stream of speech is generated by one of severalpossible stochastic sources, one corresponding to each ofthe possible topics or to each of the possible speakersin question.
We are required to judge from the acous-tic data which topic (or speaker) is the most probablesource.Standard statistical theory provides us with the optimalsolution to such a classification problem.
We denote thestring of acoustic observations by A and introduce therandom variable T to designate which stochastic modelhas produced the speech, where T may take on the valuesfrom 1 to n for the n possible sources.
If we let Pi denotethe prior probability of stochastic source i and assumethat all classification errors have the same cost, then weshould choose the source T = i for which= argmax Pi P(A\[ T = i).iWe assume, for the purposes of this work, that all priorprobabilities are equal, so that the classification problemreduces imply to choosing the source i for which theconditional probability of the acoustics given the sourceis maximized.In principle, to compute each of the probabilitiesP(A I T = i) we would have to sum over all possibletranscriptions W of the speech:P(A I T = i) = Z P(A, W\[ T = i).WIn practice, such a collection of computations is unwieldyand so we make several simplifying approximations tolimit the computational burden.
First, we estimate theabove sum only by its single largest term, i.e.
we ap-proximate the probability P(A I T = i) by the jointprobabiltiy of A and the single most probable word se-quence W = W~a x.
Of course, generating such an op-timal word sequence is exactly what speech recognitionis designed to do.
Thus, for the problem of topic iden-tification, we could imagine running n different speechrecognizers, each modelling a different opic, and thencompare the resulting probabilities P(A, W~a x I T = i)corresponding to each of the n optimal transcriptionsW~ x.
Similarly, for speaker identification, we would runn different speaker-dependent recognizers, each trainedon one of the possible speakers, and compare the result-ing scores.This approach, though simpler, still requires us to makemany complete recognition passes across the speech sam-ple.
We further educe the computational burden by in-stead producing only a single transcription of the speechto be classified, by using a recognizer whose models areboth topic-independent and speaker-independent.
Oncethis single transcription W = Wm~ is obtained, we needonly compute the probabilities P(A, Wmax \[ T = i) cor-responding to each of the stochastic sources T = i.Rewriting P(A, Wmax I T = i) asP(A I Wmax, T = i) * P(Wmax I T = i),we see that the problem of computing the desiredprobability factors into two components.
The first,P(A \[ W, T), we can think of as the contribution ofthe acoustic model, which assigns probabilities to acous-tic observations generated from a given string of words.The second factor, P(W \[ T), encodes the contribu-tion of the language model, which assigns probabilitiesto word strings without reference to the acoustics.Now for the problem of topic identification, we wish todetermine which of several possible topics is most likelythe subject of a given sample of speech.
Nothing isknown about the speaker.
We therefore assume that thesame speaker-independent acoustic model holds for alltopics; i.e.
for the topic identification task, we assumethat P(A I W, T) does not depend on T. But we needn different language models P(W I T = i), i = 1, .
.
.
,  n.From the above factorization, it is then clear that in com-paring scores from the different sources, only this latterterm matters.Symmetrically, for the speaker identification problem, wemust choose which of several possible speakers is mostlikely to have produced a given sample of speech.
Whilein practice, different speakers may well talk about dif-ferent subjects and in different styles, we assume forthe speaker identification task that the language modelP(W \[ T) is independent of T. But n different acousticmodels P(A \[ W, T = i) are required.
Thus only thefirst factor matters for speaker identification.As a result, once the speaker-independent topic-independent recognizer has generated a transcript of thespeech message, the task of the topic classifier is simplyto score the transcription using each of n different lan-guage models.
Similarly, for speaker identification thetask reduces to computing the likelihood of the acousticdata given the transcription, using each of n differentacoustic models.1203.
THE MESSAGE IDENTIF ICAT IONSYSTEMWe now examine how this theory is implemented in eachof the major components of Dragon's message identi-fication system: the continuous speech recognizer, thespeaker classifier, and the topic classifier.3.1.
The Speech RecognizerIn order to carry out topic and speaker identification asdescribed above, it is necessary to have a large vocab-ulary continuous speech recognizer that can operate ineither speaker-independent or speaker-dependent mode.Dragon's peech recognizer has been described exten-sively elsewhere (\[5\], \[6\]).
Briefly, the recognizer is atime-synchronous hidden Markov model (HMM) basedsystem.
It makes use of a set of 32 signal-processingparameters: 1 overall amplitude term, 7 spectral param-eters, 12 mel-cepstral parameters, and 12 mel-cepstraldifferences.
Each word pronunciation is represented asasequence of phoneme models called PICs (phonemes-in-context) designed to capture coarticulatory effects dueto the preceding and succeeding phonemes.
Because itis impractical to model all the triphones that could inprinciple arise, we model only the most common onesand back off to more generic forms when a recognitionhypothesis calls for a PIG which has not been built.
ThePIGs themselves are modelled as linear ttMMs with oneor more nodes, each node being specified by an outputdistribution and a double exponential duration distribu-tion.
We are currently modelling the output distribu-tions of the states as tied mixtures of double exponen-tial distributions.
The recognizer employs a rapid matchmodule which returns a short list of words that mightbegin in a given frame whenever the recognizer hypoth-esizes that a word might be ending.
During recognition,a digram language model with unigram backoff is used.We have recently begun transforming our basic set of32 signal-processing parameters u ing the IMELDA trans-form \[7\], a transformation constructed via linear discrim-inant analysis to select directions in parameter spacethat are most useful in distinguishing between desig-nated classes while reducing variation within classes.
Forthe speaker-independent r cognizer, we sought directionswhich maximize average variation between phonemeswhile being relatively insensitive to differences within thephoneme class, such as might arise from different speak-ers, telephone channels, etc.
Since the IMELDA trans-form generates a new set of parameters ordered with re-spect to their value in discriminating classes, directionswith little discriminating power between phonemes canbe dropped.
We use only the top 16 IMELDA parametersfor speaker-independent r cognition.
A different IMELDAtransform, in many ways dual to this one, was employedby the speaker classifier, as described below.For speaker-independent r cognition, we also normalizethe average speech spectra cross conversations via blinddeconvolution prior to performing the IM~LDA trans-form, in order to further reduce channel differences.
Afixed number of frames are removed from the beginningand end of each speech segment before computing the av-erage to minimize the effect of silence on the long-termspeech spectrum.Finally, we are now building separate male and femaleacoustic models and using the result of whichever modelscores better.
While in principle, one would have toperform a complete recognition pass with both sets ofmodels and choose the better scoring, we have found thatone can fairly reliably determine the model which betterfits the data after recognizing only a few utterances.
Theremainder of the speech can then be recognized usingonly the better model.3.2.
The Speaker ClassifierGiven the transcript generated by the speaker-independent recognizer, the job of the speaker classifieris to score the speech data using speaker-specific sets ofacoustic models, assuming that the transcript providesthe correct ext; i.e.
it must calculate the probabilitiesP(A \[ W, T = i) discussed above.
Dragon's continuousspeech recognizer is capable of running in such a "scor-ing" mode.
This step is much faster than performing afull recognition, since the recognizer only has to hypoth-esize different ways of mapping the speech data to therequired text - a frame-by-frame phonetic labelling werefer to as a "segmentation" of the script - and need notentertain hypotheses on alternate word sequences.In principle, the value of P(A \[ W, 7") should be com-puted as the sum over all possible segmentations of theacoustic data, but, as usual, we approximate his proba-bility using only the largest erm in the sum, correspond-ing to the maximum likelihood segmentation.
Whileone could imagine letting each of the speaker-dependentmodels choose the segmentation that is best for them, inour current version of the speaker classifier we have cho-sen to compute this "best" segmentation ce and for allusing the same speaker-independent r cognizer responsi-ble for generating the initial transcription.
This ensuresthat the comparison of different speakers is relative tothe same alignment of the speech and may yield an ac-tual advantage in performance, given the imprecision ofour probability models.Thus, the job of the speaker classifier educes to scor-ing the speech data given both a fixed transcription121and a specified mapping of individual speech frames toPICs.
'Ib perform this scoring, we use a "matched set"of tied mixture acoustic models - a collection of speaker-dependent models each trained on speech from one of thetarget speakers but constructed with exactly the samecollection of PICs to keep the scoring directly compara-ble.
Running in "scoring" mode, we then produce a setof scores corresponding to the negative log likelihood ofgenerating the acoustics given the segmentation foreachof the speaker-dependent acoustic models.
The speechsample is assigned to the lowest scoring model.In constructing speaker scoring models, we derived anew "speaker sensitive" IMELDA transformation, de-signed to enhance differences between speakers.
Thetransform was computed using only voiced speech seg-ments of the test speakers (and, correspondingly, onlyvoiced speech was used in the scoring).
As is commonin using the IMELDA strategy, we dropped parameterswith the least discriminating power, reducing our orig-inal 32 signal-processing parameters to a new set of 24IMELDA parameters.
These were the parameters u ed tobuild the speaker scoring models.
It is worth remark-ing that, because these parameters were constructed toemphasize differences between speakers rather than be-tween phonemes, it was particularly important that thephoneme-level segmentation used in the scoring be setby the original recognition models.3.3.
The  Top ic  C lass i f ie rOnce the speaker-independent r cognizer has generateda transcription of the speech, the topic classifier needonly score the transcript using language models trainedon each of the possible topics.
The current opic scor-ing algorithm uses a simple (unigram) multinomial prob-ability model based on a collection of topic-dependent"keywords".
Thus digrams are not used for topic scor-ing although they are used during recognition.
For eachtopic, the probability of occurrence of each keyword isestimated from training material on that topic.
Non-keyword members of the vocabulary are assigned to acatch-all "other" category whose probability is also esti-mated.
Transcripts are then scored by adding in a nega-tive log probability for every recognized word, and run-ning totals are kept for each of the topics.
The speechsample is assigned to the topic with the lowest cumula-tive score.We have experimented with two different methods ofkeyword selection.
The first method is based on com-puting the chi-squared statistic for homogeneity basedon the number of times a given word occurs in the train-ing data for each of the target opics.
This method as-sumes that the number of occurrences ofthe word withina topic follows a binomial distribution, i.e.
that there isa "natural frequency" for each word within each topicclass.
The words of the vocabulary can then be rankedaccording to the P-value resulting from this chi-squaredtest.
Presumably, the smaller the P-value, the more use-ful the word should be for topic identification.
Key-word lists of different lengths are obtained by selectingall words whose P-value falls below a given threshold.Unfortunately, this method oes not do a good job of ex-cluding function words and other high frequency words,such as "uh" or "oh", which are of limited use for topicclassification.
Consequently, this method requires theuse of a human-generated "stop list" to filter out theseunwanted entries.
The problem lies chiefly in the falsityof the binomial assumption: one expects a great deal ofvariability in the frequency of words, even among mes-sages on the same topic, and natural variations in theoccurrence rates of these very high frequency words canresult in exceptionally small P-values.The second method is designed to address this problemby explicitly modelling the variability in word frequencyamong conversations in the same topic instead of onlyvariations between topics.
It also uses a chi-squared testto sort the words in the vocabulary by P-value.
Butnow for each word we construct a two-way table sortingtraining messages from each topic into classes based onwhether the word in question occurs at a low, a moder-ate, or a high rate.
(If the word occurs in only a small mi-nority of messages, it becomes necessary to collapse thethree categories to two.)
Then we compute the P-valuerelative to the null hypothesis that the distribution ofoccurrence rates is the same for each of the topic classes.Hence this method explicitly models the variability inoccurrence rates among documents in a nonparametricway.
This method oes seem successful at automaticallyexcluding most function words when stringent P-valuethresholds are set, and as the threshold isrelaxed and thekeyword lists allowed to grow, function words are slowlyintroduced at levels more appropriate to their utility intopic identification.
Hence, this method eliminates theneed for human editing of the keyword lists.4.
TEST ING ON SWITCHBOARDDATATo gauge the performance of our message classificationsystem, we turned to the Switchboard corpus of recordedtelephone conversations.
The recognition task is partic-ularly challenging for Switchboard messages, ince theyinvolve spontaneous conversational speech across noisyphone lines.
This made the Switchboard corpus a par-ticularly goodplatform for testing the message identifi-cation systems, allowing us to assess the ability of the122continuous speech recognizer to extract information use-ful to the message classifiers even when the recognitionitself was bound to be highly errorful.To create our "Switchboard" recognizer, male and fe-male speaker-independent acoustic models were trainedusing a total of about 9 hours of Switchboard messages(approximately 140 message halves) from 8 male and 8female speakers not involved in the test sets.
We foundthat it was necessary to hand edit the training messagesin order to remove such extraneous noises as cross-talk,bursts of static, and laughter.
We also corrected badtranscriptions and broke up long utterances into shorter,more manageable pieces.Models for about 4800 PICs were constructed.
We choseto construct only one-node models for the Switchboardtask, both to reduce the number of parameters to beestimated given the limited training data and to mini-mize the penalty for reducing or skipping phonemes inthe often rapid speech of many Switchboard speakers.
Avocabulary of 8431 words (all words occurring at least 4times in the training data) and a digram language modelwere derived from a set of 935 transcribed Switchboardmessages involving roughly 1.4 million words of text andcovering nearly 60 different opics.
Roughly a third ofthe language model training messages were on one of the10 topics used for the topic identification task.For the speaker identification trials, we used a set of 24test speakers, 12 male and 12 female.
Speaker-dependentscoring models were constructed for each of the 24speakers using the same PIC set as for the speaker-independent recognizer.
PIC models were trained using5 to 10 hand-edited message halves (about 16 minutesof speech) from each speaker.The speaker identification test material involved 97 mes-sage halves and included from 1 to 6 messages for eachtest speaker.
We tested on speech segments from thesemessages that contained 10, 30, and 60 seconds of speech.The results of the speaker identification tests were sur-prisingly constant across the three duration lengths.Even for segments containing as little as 10 seconds ofspeech, 86 of the 97 message halves, or 88.7%, were cor-rectly classified.
When averaged equally across peakers,this gave 90.3% accuracy.
The results from the three trialruns are summarized in Table 1.
It is worth remarkingthat even the few errors that were made tended to beconcentrated in a few difficult speakers; for 17 of the 24speakers, the performance was always perfect, and foronly 2 speakers was more than one message ver mis-classified.Given the insensitivity of these results to speech dura-tion, we decided to further limit the amount of speechavailable to the speaker classifier.
The test segmentsused in the speaker test were actually concatenations ofsmaller speech intervals, ranging in length from as littleas 1.5 to as much as 50.2 seconds.
We rescored usingthese individual fragments as the test pieces.
1 Resultsremained excellent.
For example, when testing only thepieces of length under 3 seconds, 42 of the 46 pieces,or 91.3%, were correctly classified (90.9% when speakerswere equally weighted).
These pieces represented only 19of the 24 speakers, but did include our most problematicspeakers.
For segments of length less than 5 seconds, 177of the 201 pieces (88.1%, or 89.4% when the 24 speakerswere equally weighted) were correctly classified.speechinterval(seconds)weighted weightedby message by speaker(%) (%)10 88.7 90.330 88.7 90.660 87.6 89.9Table 1: Speaker identification accuracy for 97-messageSwitchboard test.For the topic identification task, we used a test set of 120messages, 12 conversations on each of 10 different topics.Topics included such subjects as "air pollution", "pets",and "public education", and involved several topics (forexample, "gun control" and "crime") with significantcommon ground.
For topic identification, we plannedto use the entire speech message, but for uniformity allmessages were truncated after 5 minutes and the first 30seconds of each was removed because of concern that thisinitial segment might be artificially rich in keywords.Keywords were selected from the same training messagesused for constructing the recognizer's language model.This collection yielded just over 30 messages on each ofthe ten topics, for a total of about 50,000 words of train-ing text per topic.
Because this is relatively little for es-timating reliable word frequencies, word counts for eachtopic were heavily smoothed using counts from all othertopics.
We found that it was best to use a 5-to-1 smooth-ing ratio; i.e.
data specific to the topic were counted fivetimes as heavily as data from the other nine topics.Keyword lists of lengths ranging from about 200 wordsto nearly 5000 were generated using the second methodof keyword selection.
We also tried using the entire 8431-1The initial speaker-independent recognition and segmentationwere not, however, re-run so that such decisions as gender deter-minatlon were inherited from the larger test.123word recognition vocabulary as the "keyword" list.
Theresults of the initial runs, given in the second column ofTable 2, were disappointing: performance f ll between70% and 75% in all cases.#keywords original (%) recalibrated (%)203 70.0 71.71127 71.772.5 265585.087.54658 74:2 87.58431 72.5 88.3Table 2: Topic identification accuracy for 120-messageSwitchboard test.It is worth noting that, as it was designed to, the newkeyword selection routine succeeded in automatically ex-cluding virtually all function words from the 203-wordlist.
For comparison, we also ran some keyword lists se-lected using our original method and filtered through ahuman-generated "stop list".
The performance was sim-ilar: for example, a list of 211 keywords resulted in anaccuracy of 67.5%.The problem for the topic classifier was that scores formessages from different opics were not generally com-parable due to differences in the acoustic onfusability ofthe keywords.
When tested on the true transcripts of thespeech messages, the topic classifier did extremely well,missing only 2 or 3 messages out of the 120 with any ofthe keyword lists.
Unfortunately, when run on the recog-nized transcriptions, ome topics (most notably "pets",with its preponderance of monosyllabic keywords) neverreceived competitive scores.In principle, this problem could be corrected by esti-mating keyword frequencies not from true transcriptionsof training data but from their recognized counterparts.Unfortunately, this is a fairly expensive approach, re-quiring that the full training corpus be run through therecognizer.
Instead, we took a more expedient course.
Inthe process of evaluating our Switchboard recognizer, wehad run recognition on over a hundred messages on top-ics other than the ten used in the topic identification test.For each of these off-topic messages, we computed scoresbased on each of the test topic language models to esti-mate the (per word) handicap that each test topic shouldreceive.
When the 120 test messages were rescored us-ing this adjustment, he results improved dramaticallyfor all but the smallest list (where the keywords weretoo sparse for scores to be adequately estimated).
Theimproved results are given in the last column of Table 2.5.
CONCLUSIONSAs the Switchboard testing demonstrates, message iden-tification via large vocabulary continuous peech recog-nition is a successful strategy even in challenging speechenvironments.
Although the quality of the recognitionas measured by word accuracy rates was very low for thistask - only 22% of the words were correctly transcribed -the recognizer was still able to extract sufficient informa-tion to reliably identify speech messages.
This supportsour belief in the advantages of using articulatory andlanguage model context.We were surprised not to find a more pronounced benefitfrom using large numbers of keywords for the topic iden-tification task.
Our prior experience had indicated thatthere were small but significant gains as the number ofkeywords grew and, although such a pattern is perhapssuggested by the results in Table 2, the gains (beyondthose in the recalibration estimates) are too small tobe considered significant.
It is possible that with bet-ter modelling of keyword frequencies or by introducingacoustic distinctiveness a a keyword selection criterion,such improvements might be realized.Given the strong performance of both of our identifi-cation systems, we also look forward to exploring howmuch we can restrict he amount of training and testingmaterial and still maintain the quality of our results.Re ferences1.
R.C.
Rose, E.I.
Chang, and R.P.
Lipmann, "Techniquesfor Information Retrieval from Voice Messages," Proc.ICASSP-91, Toronto, May 1991.2.
A.L.
Higgins and L.G.
Bahler, "Text IndependentSpeaker Verification by Discriminator Counting," Proc.ICASSP-91, Toronto, May 1991.3.
L.P. Netsch and G.R.
Doddington, "Speaker VerificationUsing Temporal Decorrelation Post-Processing," Proc.ICASSP-9P, San Francisco, March 1992.4.
J.J. Godfrey, E.G.
Holliman, and J. McDaniel,"SWITCHBOARD: Telephone Speech Corpus for Re-search and Development," Proc.
ICASSP-9P, San Fran-cisco, March 1992.5.
J.K. Baker et al, "Large Vocabulary Recognition of WallStreet Journal Sentences at Dragon Systems," Proc.DARPA Speech and Natural Language Workshop, Har-riman, New York, February 1992.6.
R. Roth et al, "Large Vocabulary Continuous SpeechRecognition of Wall Street Journal Data," Proc.ICASSP-93, Minneapolis, Minnesota, April 1993.7.
M.J. Hunt, D.C. Bateman, S.M.
Richardson, and A.Piau, "An Investigation of PLP and IMELDA Acous-tic Representations and of their Potential for Combina-tion," Proc.
ICASSP-91, Toronto, May 1991.124
