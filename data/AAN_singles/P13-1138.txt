Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1406?1415,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA Statistical NLG Framework for Aggregated Planning and RealizationRavi Kondadadi?, Blake Howald and Frank SchilderThomson Reuters, Research & Development610 Opperman Drive, Eagan, MN 55123firstname.lastname@thomsonreuters.comAbstractWe present a hybrid natural language gen-eration (NLG) system that consolidatesmacro and micro planning and surface re-alization tasks into one statistical learn-ing process.
Our novel approach is basedon deriving a template bank automaticallyfrom a corpus of texts from a target do-main.
First, we identify domain specificentity tags and Discourse RepresentationStructures on a per sentence basis.
Eachsentence is then organized into semanti-cally similar groups (representing a do-main specific concept) by k-means cluster-ing.
After this semi-automatic processing(human review of cluster assignments), anumber of corpus?level statistics are com-piled and used as features by a rankingSVM to develop model weights from atraining corpus.
At generation time, a setof input data, the collection of semanti-cally organized templates, and the modelweights are used to select optimal tem-plates.
Our system is evaluated with au-tomatic, non?expert crowdsourced and ex-pert evaluation metrics.
We also introducea novel automatic metric ?
syntactic vari-ability ?
that represents linguistic variationas a measure of unique template sequencesacross a collection of automatically gener-ated documents.
The metrics for generatedweather and biography texts fall within ac-ceptable ranges.
In sum, we argue that ourstatistical approach to NLG reduces theneed for complicated knowledge-based ar-chitectures and readily adapts to differentdomains with reduced development time.?
*Ravi Kondadadi is now affiliated with Nuance Commu-nications, Inc.1 IntroductionNLG is the process of generating natural-soundingtext from non-linguistic inputs.
A typical NLGsystem contains three main components: (1) Doc-ument (Macro) Planning - deciding what contentshould be realized in the output and how it shouldbe structured; (2) Sentence (Micro) planning -generating a detailed sentence specification andselecting appropriate referring expressions; and(3) Surface Realization - generating the final textafter applying morphological modifications basedon syntactic rules (see e.g., Bateman and Zock(2003), Reiter and Dale (2000) and McKeown(1985)).
However, document planning is arguablyone of the most crucial components of an NLGsystem and is responsible for making the texts ex-press the desired communicative goal in a coher-ent structure.
If the document planning stage fails,the communicative goal of the generated text willnot be met even if the other two stages are perfect.While most traditional systems simplify develop-ment by using a pipelined approach where (1-3)are executed in a sequence, this can result in er-rors at one stage propagating to successive stages(see e.g., Robin and McKeown (1996)).
We pro-pose a hybrid framework that combines (1-3) byconverting data to text in one single process.Most NLG systems fall into two broadcategories: knowledge-based and statistical.Knowledge-based systems heavily depend on hav-ing domain expertise to come up with hand-crafted rules at each stage of a pipeline.
Althoughknowledge-based systems can produce high qual-ity text, they are (1) very expensive to build, in-volving a lot of discussion with the end users of thesystem for the document planning stage alone; (2)have limited linguistic coverage, as it is time con-suming to capture linguistic variation; and (3) onehas to start from scratch for each new domain be-cause the developed components cannot be reused.1406Statistical systems, on the other hand, are fairlyinexpensive, more adaptable and rely on havinghistorical data for the given domain.
Coverage islikely to be high if more historical data is avail-able.
The main disadvantage with statistical sys-tems is that they are more prone to errors and theoutput text may not be coherent as there are lessconstraints on the generated text.Our framework is a hybrid of statistical andtemplate-based systems.
Many knowledge-basedsystems use templates to generate text.
A tem-plate structure contains ?gaps?
that are filled togenerate the output.
The idea is to create a lotof templates from the historical data and selectthe right template based on some constraints.
Tothe best of our knowledge, this is the first hy-brid statistical-template-based system that com-bines all three stages of NLG.
Experiments withdifferent variants of our system (for biography andweather subject matter domains) demonstrate thatour system generates reasonable texts.Also, in addition to the standard metrics usedto evaluate NLG systems (e.g., BLEU, NIST, etc.
),we present a unique text evaluation metric calledsyntactic variability to measure the linguistic vari-ation of generated texts.
This metric applies to thedocument collection level and is based on com-puting the number of unique template sequencesamong all the generated texts.
A higher numberindicates the texts are more variable and natural-sounding whereas a lower number shows they aremore redundant.
We argue that this metric is use-ful for evaluating template-based systems and forany type of text generation for domains where lin-guistic variability is favored (e.g., the user is ex-pected to go through more than one document inthe same session).The main contributions of this paper are (1) Astatistical NLG system that combines documentand sentence planning and surface realization intoone single process; and (2) A new metric ?
syntac-tic variability ?
is proposed to measure the syntac-tic and morphological variability of the generatedtexts.
We believe this is the first work to proposean automatic metric to measure linguistic variabil-ity of generated texts in NLG.Section 2 provides an overview of related workon NLG.
We present our main system in Section 3.The system is evaluated and discussed in Section4.
Finally, we conclude in Section 5 and point outfuture directions of research.2 BackgroundTypically, knowledge-based NLG systems are im-plemented by rules and, as mentioned above, havea pipelined architecture for the document andsentence planning stages and surface realization(Hovy, 1993; Moore and Paris, 1993).
However,document planning is arguably the most impor-tant task (Sripada et al, 2001).
It follows that ap-proaches to document planning are rule-based aswell and, concomitantly, are usually domain spe-cific.
For example, Bouayad-Agha, et al (2011)proposed document planning based on an ontol-ogy knowledge base to generate football sum-maries.
For rule?based systems, rules exist forselecting content to grammatical choices to post-processing (e.g., pronoun generation).
These rulesare often tailored to a given system, with inputfrom multiple experts; consequently, there is ahigh associated development cost (e.g., 12 personmonths for the SUMTIME-METEO system (Belz,2007)).Statistical approaches can reduce extensive de-velopment time by relying on corpus data to?learn?
rules for one or more components of anNLG system (Langkilde and Knight, 1998).
Forexample, Duboue and McKeown (2003) proposeda statistical approach to extract content selectionrules for biography descriptions.
Further, statisti-cal approaches should be more adaptable to differ-ent domains than their rule-based equivalents (An-geli et al, 2012).
For example, Barzilay and Lap-ata (2005) formulated content selection as a clas-sification task to produce football summaries andKelly et al (2009) extended Barzilay and Lapata?sapproach for generating match reports for cricket.The present work builds on Howald et al(2013) where, in a given corpus, a combination ofdomain specific named entity tagging and cluster-ing sentences (based on semantic predicates) wereused to generate templates.
However, while thesystem consolidated both sentence planning andsurface realization with this approach (describedin more detail in Section 3), the document planwas given via the input data and sequencing infor-mation was present in training documents.
For thepresent research, we introduce a similar methodthat leverages the distributions of document?levelfeatures in the training corpus to incorporate astatistical document planning component.
Con-sequently, we are able to create a streamlinedstatistical NLG architecture that balances natural1407human?like variability with appropriate and accu-rate information.3 MethodologyIn order to generate text for a given domain oursystem runs input data through a statistical rankingmodel to select a sequence of templates that bestfit the input data (E).
In order to build the rank-ing model, our system takes historical data (cor-pus) for the domain through four components: (A)preprocessing; (B) ?conceptual unit?
creation; (C)collecting statistics; and (D) ranking model build-ing (summarized in Figure 1).
In this section, wedescribe each component in detail.Figure 1: System Architecture.3.1 PreprocessingThe first component processes the given corpus toextract templates.
We assume that each documentin the corpus is classified to a specific domain.Preprocessing involves uncovering the underlyingsemantic structure of the corpus and using this asa foundation for template creation (Lu et al, 2009;Lu and Ng, 2011; Konstas and Lapata, 2012).We first split each document in the corpus intosentences and create a shallow Discourse Repre-sentation Structure (following Discourse Repre-sentation Theory (Kamp and Reyle, 1993)) of eachsentence.
The DRS consists of semantic predi-cates and named entity tags.
We use Boxer se-mantic analyzer (Bos, 2008) to extract semanticpredicates such as EVENT or DATE.
In parallel,domain specific named entity tags are identifiedand, in conjunction with the semantic predicates,are used to create templates.
We developed thenamed-entity tagger for the weather domain our-selves.
To tag entities in the biography domain,we used OpenCalais (www.opencalais.com).
Forexample, in the biography in (1), the conceptualmeaning (semantic predicates and domain-specificentities) of sentences (a-b) are represented in (c-d).The corresponding templates are showing in (e-f).
(1) Sentencea.
Mr. Mitsutaka Kambe has been serving as Managing Di-rector of the 77 Bank, Ltd. since June 27, 2008.b.
He holds a Bachelor?s in finance from USC and a MBAfrom UCLA.Conceptual Meaningc.
SERVING | TITLE | PERSON | COMPANY | DATEd.
HOLDS | DEGREE | SUBJECT | INSTITUTION| EVENTTemplatese.
[person] has been serving as [title] of the [company]since [date].f.
[person] holds a [degree] in [subject] from [institution]and a [degree] from [institution].The outputs of the preprocessing stage are the tem-plate bank and predicate information for each tem-plate in the corpus.13.2 Creating Conceptual UnitsThe next step is to create conceptual units for thecorpus by clustering templates.
This is a semi-automatic process where we use the predicate in-formation for each template to compute similar-ity between templates.
We use k-means clusteringwith k (equivalent to the number of semantic con-cepts in the domain) set to an arbitrarily high value(100) to over-generate (using the WEKA toolkit(Witten and Frank, 2005)).
This allows for easiermanual verification of the generated clusters andwe merge them if necessary.
We assign a uniqueidentifier called a CuId (Conceptual Unit Identi-fier) to each cluster, which represents a ?concep-tual unit?.
We associate each template in the cor-pus to a corresponding CuId.
For example, in (2),using the templates in (1e-f), the identified namedentities are assigned to a clustered CuId (2a-b).
(2) Conceptual Unitsa.
{CuId : 000} ?
[person] has been serving as [title] of the[company] since [date].b.
{CuId : 001} ?
[person] holds a [degree] in [subject]from [institution] and a [degree] from [institution].At this stage, we will have a set of conceptualunits with corresponding template collections (seeHowald et al (2013) for a further explanation ofSections 3.1-3.2).1A similar approach to the clustering of semantic contentis found in Duboue and McKeown (2003), where text withstopwords removed were used as semantic input.
Boxer pro-vides a similar representation with the addition of domaingeneral tags.
However, to contrast our work from Duboueand McKeown, which focused on content selection, we arefocused on learning templates from the semantic representa-tions for the complete generation system (covering contentselection, aggregation, sentence and document planning).14083.3 Collecting Corpus StatisticsAfter identifying the different conceptual units andthe template bank, we collect a number of statisticsfrom the corpus:?
Frequency distribution of templates overall and per po-sition?
Frequency distribution of CuIds overall and per posi-tion?
Average number of entity tags by CuId as well as theentity distribution by CuId?
Average number of entity tags by position as well asthe entity distribution by position?
Average number of words per CuId.?
Average number of words per CuId and position com-bination.?
Average number of words per position?
Frequency distribution of the main verbs by position?
Frequency distribution of CuId sequences (bigrams andtrigrams only) overall and per position?
Frequency distribution of template sequences (bigramsand trigrams only) overall and per position?
Frequency distribution of entity tag sequences overalland per position?
The average, minimum, maximum number of CuIdsacross all documentsAs discussed in the next section, these statisticsare turned into features used for building a rankingmodel in the next component.3.4 Building a ranking modelThe core component of our system is a statisticalmodel that ranks a set of templates for a givenposition (sentence 1, sentence 2, ..., sentence n)based on the input data.
The input data in ourtasks was extracted from a training document; thisserves as a temporary surrogate to a database.
Thetask is to learn the ranks of all the templates fromall CuIds at each position.To generate the training data, we first filter thetemplates that have named entity tags not specifiedin the input data.
This will make sure the gener-ated text does not have any unfilled entity tags.
Wethen rank templates according to the Levenshteinedit distance (Levenshtein, 1966) from the tem-plate corresponding to the current sentence in thetraining document (using the top 10 ranked tem-plates in training for ease of processing effort).
Weexperimented with other ranking schemes such asentity-based similarity (similarity between entitysequences in the templates) and a combination ofedit-distance based and entity-based similarities.We obtained better results with edit distance.
Foreach template, we generate the following featuresto build the ranking model.
Most of the featuresare based on the corpus statistics mentioned above.?
CuId given position: This is a binary feature wherethe current CuId is either the same as the most frequentCuId for the position (1) or not (0).?
Overlap of named entities: Number of common enti-ties between current CuId and most likely CuId for theposition?
Prior template: Probability of the sequence of tem-plates selected at the previous position and the currenttemplate (iterated for the last three positions).?
Prior CuId: Probability of the sequence of the CuIdselected at the previous position and the current CuId(iterated for the last three positions).?
Difference in number of words: Absolute differencebetween number of words for current template and av-erage number of words for the CuId?
Difference in number of words given position: Ab-solute difference between number of words for cur-rent template and average number of words for CuIdat given position?
Percentage of unused data: This feature representsthe portion of the unused input so far.?
Difference in number of named entities: Absolutedifference between the number of named entities in thecurrent template and the average number of named en-tities for the current position?
Most frequent verb for the position: Binary valuedfeature where the main verb of the template belongs tothe most frequent verb group given the position is eitherthe same (1) or not (0).?
Average number of words used: Ratio of number ofwords in the generated text so far to the average numberof words.?
Average number of entities: Ratio of number ofnamed entities in the generated text so far to the av-erage number of named entities.?
Most likely CuId given position and previous CuId:Binary feature indicating if the current CuId is mostlikely given the position and the previous CuId.?
Similarity between the most likely template in CuIdand current template: Edit distance between the cur-rent template and the most likely template for the cur-rent CuId.?
Similarity between the most likely template in CuIdgiven position and current template: Edit distancebetween the current template and the most likely tem-plate for the current CuId at the current position.We used a linear kernel for a ranking SVM(Joachims, 2002) (cost set to total queries) to learnthe weights associated with each feature for thedifferent domains.3.5 GenerationAt generation time, our system has a set of in-put data, a semantically organized template bank(collection of templates organized by CuId) and amodel from training on the documents for a givendomain.
We first filter out those templates thatcontain a named entity tag not present in the in-put data.
Then, we compute a score for each of theremaining templates from the feature values andthe feature weights from the model.
The templatewith the highest overall score is selected and filledwith matching entity tags from the input data and1409appended to the generated text.Before generating the next sentence, we trackthose entities used in the initial sentence gener-ation and decide to either remove those entitiesfrom the input data or keep the entity for one ormore additional sentence generations.
For exam-ple, in the biography discourses, the name of theperson may occur only once in the input data, butit may be useful for creating good texts to havethat person?s name available for subsequent gen-erations.
To illustrate in (3), if we remove JamesSmithton from the input data after the initial gen-eration, an irrelevant sentence (d) is generated asthe input data will only have one company afterthe removal of James Smithton and the model willonly select a template with one company.
If wekeep James Smithton, then the generations in (a-b)are more cohesive.
(3) Use more than oncea.
Mr. James Smithton was appointed CFO at FordwayInternation in April.b.
Previously, Mr. Smithton was CFO of the KeyesDevelopment Group.Use once and removec.
Mr. James Smithton was appointed CFO at FordwayInternation in April.d.
Keyes Development Group is a venture capital firm.Deciding on what type of entities and how toremove them is different for each domain.
For ex-ample, some entities are very unique to a text andshould not be made available for subsequent gen-erations as doing so would lead to unwanted re-dundancies (e.g., mentioning the name of currentcompany in a biography discourse more than onceas in (3)) and some entities are general and shouldbe retained.
Our system possesses the ability tomonitor the data usage from historical data and wecan set parameters (based on the distribution of en-tities) on the usage to ensure coherent generationsfor a given domain.Once the input data has been modified (i.e., anentity have been removed, replaced or retained),it serves as the new input data for the next sen-tence generation.
This process repeats until reach-ing the minimum number of sentences for the do-main (determined from the training corpus statis-tic) and then continues until all of the remaininginput data is consumed (and not to exceed the pre-determined maximum number of sentences, alsodetermined from the training corpus statistic).4 Evaluation and DiscussionIn this section, we first discuss the corpus dataused to train and generate texts.
Then, the re-sults of both automatic and human evaluations ofour system?s generations against the original andbaseline texts are considered as a means of de-termining performance.
For all experiments re-ported in this section, the baseline system selectsthe most frequent conceptual unit at the given po-sition, chooses the most likely template for theconceptual unit, and fills the template with inputdata.
The above process is repeated until the num-ber of sentences is less than or equal to the averagenumber of sentences for the given domain.4.1 DataWe ran our system on two different domains: cor-porate officer and director biographies and off-shore oil rig weather reports from the SUMTIME-METEO corpus ((Reiter et al, 2005)).
The biogra-phy domain includes 1150 texts ranging from 3-17sentences and the weather domain includes 1045weather reports ranging from 1-6 sentences.2 Weused a training-test(generation) split of 70/30.
(4) provides generation comparisons for thesystem ( DocSys), baseline ( DocBase) and orig-inal ( DocOrig) randomly selected text snippetsfrom each domain.
The variability of the gener-ated texts ranges from a close similarity to slightlyshorter - not an uncommon (Belz and Reiter,2006), but not necessarily detrimental, observationfor NLG systems (van Deemter et al, 2005).
(4) Weather DocOriga.
Another weak cold front will move ne to Cornwall by laterFriday.Weather DocSysb.
Another weak cold front will move ne to Cornwall duringFriday.Weather DocBasec.
Another weak cold front from ne through the Cornwall willremain slow moving.Bio DocOrigd.
He previously served as Director of Sales Planning andManager of Loan Center.Bio DocSyse.
He previously served as Director of Sales in Loan Centerof the Company.Bio DocBase2The SUMTIME-METEO project is a common benchmark in NLG.
However, we provide no comparison betweenour system and SUMTIME-METEO as our system utilized thegenerated forecasts from SUMTIME-METEO?s system as thehistorical data.
We cannot compare with other statistical gen-eration systems like (Belz, 2007) as they only focussed on thepart of the forecasts the predicts wind characteristics whereasour system generates the complete forecasts.1410f.
He previously served as Director of Sales of the Company.The DocSys and DocBase generations arelargely grammatical and coherent on the surfacewith some variance, but there are graded semanticvariations (e.g., Director of Sales Planning vs. Di-rector of Sales (4g-h) and move ne to Cornwall vs.from ne through the Cornwall).
Both automaticand human evaluations are required in NLG to de-termine the impact of these variances on the under-standability of the texts in general (non-experts)and as they are representative of particular subjectmatter domains (experts).
The following sectionsdiscuss the evaluation results.4.2 Automatic MetricsWe used BLEU?4 (Papineni et al, 2002), METEOR(v.1.3) (Denkowski and Lavie, 2011) to evaluatethe texts at document level.
Both BLEU?4 andMETEOR originate from machine translation re-search.
BLEU?4 measures the degree of 4-gramoverlap between documents.
METEOR uses a un-igram weighted f?score less a penalty based onchunking dissimilarity.
These metrics only eval-uate the text on a document level but fail to iden-tify ?syntactic repetitiveness?
across documents ina document collection.
This is an important char-acteristic of a document collection to avoid banal-ity.
To address this issue, we propose a new auto-matic metric called syntactic variability.
In orderto compute this metric, each document should berepresented as a sequence of templates by associ-ating each sentence in the document with a tem-plate in the template bank.
Syntactic variability isdefined as the percentage of unique template se-quences across all generated documents.
It rangesbetween 0 and 1.
A higher value indicates thatmore documents in the collection are linguisticallydifferent from each other and a value closer to zeroshows that most of documents have the similarlanguage despite different input data.3As indicated in Figure 2, the BLEU-4 scores arelow for all DocSys and DocBase generations (ascompared to DocOrig) for each domain.
How-ever, the METEOR scores, while low overall (rang-ing from .201-.437) are noticeably increased overBLEU-4 (which ranges from .199-.320).Given the nature of each metric, the results in-dicate that the generated and baseline texts have3Of course, syntactic and semantic repetitiveness could becaptured by syntactic variability, but only if this is the natureof the underlying historical data - financial texts tend to befairly repetitive.Figure 2: Automatic Evaluations.very different surface realizations compared to theoriginals (low BLEU-4), but are still capturing thecontent of the originals (higher METEOR).
BothBLEU?4 and METEOR measure the similarity ofthe generated text to the original text, but fail topenalize repetitiveness across texts, which is ad-dressed by the syntactic variability metric.
Thereis no statistically significant difference betweenDocSys and DocBase generations for METEORand BLEU?4.4 However, there is a statisticallysignificant difference in the syntactic variabilitymetric for both domains (weather - ?2=137.16,d.f.=1, p<.0001; biography - ?2=96.641, d.f.=1,p<.0001) - the variability of the DocSys gener-ations is greater than the DocBase generations,which shows that texts generated by our systemare more variable than the baseline texts.The use of automatic metrics is a common eval-uation method in NLG, but they must be recon-ciled against non?expert and expert level evalua-tions.4.3 Non-Expert Human EvaluationsTwo sets of crowdsourced human evaluation tasks(run on CrowdFlower) were constructed to com-pare against the automatic metrics: (1) an under-standability evaluation of the entire text on a three-point scale: Fluent = no grammatical or infor-mative barriers; Understandable = some gram-matical or informative barriers; Disfluent = sig-nificant grammatical or informative barriers; and(2) a sentence?level preference between sentencepairs (e.g., ?Do you prefer Sentence A (from Do-cOrig) or the corresponding Sentence B (fromDocBase/DocSys)?
).4BLEU?4: weather - ?2=1.418, d.f.=1, p=.230; biography- ?2=0.311, d.f.=1, p=.354.
METEOR: weather - ?2=1.016,d.f.=1, p=.313; biography - ?2=0.851, d.f.=1, p=.354.1411Over 100 native English speakers contributed,each one restricted to providing no more than50 responses and only after they successfully an-swered 4 ?gold data?
questions correctly.
We alsoomitted those evaluators with a disproportionatelyhigh response rate.
No other data was collected onthe contributors (although geographic data (coun-try, region, city) and IP addresses were available).For the sentence?level preference task, the pair or-derings were randomized to prevent click bias.For the text?understandability task, 40 docu-ments were chosen at random from the DocOrigtest set alng with the corresponding 40 Doc-Sys and DocBase generations (240 documents to-tal/120 for each domain).
8 judgments per doc-ument were solicited from the crowd (1920 to-tal judgments, 69.51 average agreement) and aresummarized in Figures 3 and 4 (biography andweather respectively).If the system is performing well and the rank-ing model is actually contributing to increasedperformance, the accepted trend should be thatthe DocOrig texts are more fluent and preferredcompared to both the DocSys and DocBase sys-tems.
However, the differences between DocOrigand DocSys will not be significant, the differencesbetween DocOrig and DocBase and DocSys andDocBase will be significantly different.Figure 3: Biography Text Evaluations.Focusing on fluency ratings, it is expected thatthe DocOrig generations will have the highest flu-ency (as they are human generated).
Further, if theDocSys is performing well, it is expected that thefluency rating will be less than the DocOrig andhigher than DocBase.
Figure 3, which shows thebiography text evaluations, demonstrates this ac-ceptable distribution of performances.For the weather discourses, as evident fromFigure 4, the acceptable trend holds between theDocSys and DocBase generations, and the Doc-Sys generation fluency is actually slightly higherthan DocOrig.
This is possibly because the Do-cOrig texts are from a particular subject matter -weather forecasts for offshore oil rigs in the U.K.- which may be difficult for people in general tounderstand.
Nonetheless, the demonstrated trendis favorable to our system.In terms of significance, there are no statisti-cally significant differences between the systemsfor weather (DocOrig vs. DocSys - ?2=.347,d.f.=1, p=.555; DocOrig vs. DocBase - ?2=.090,d.f.=1, p=.764; DocSys vs. DocBase - ?2=.790,d.f.=1, p=.373).
While this is a good result forcomparing DocOrig and DocSys generations, it isnot for the other pairs.
However, numerically, thetrend is in the right direction despite not beingable to demonstrate significance.
For biography,the trend fits nicely both numerically and in termsof statistical significance (DocOrig vs. DocSys -?2=5.094, d.f.=1, p=.024; DocOrig vs. DocBase -?2=35.171, d.f.=1, p<.0001; DocSys vs. DocBase- ?2=14.000, d.f.=1, p<.0001).Figure 4: Weather Text Evaluations.For the sentence preference task, equivalentsentences across the 120 documents were chosenat random (80 sentences from biography and 74sentences from weather).
8 judgments per com-parison were solicited from the crowd (3758 to-tal judgments, 75.87 average agreement) and aresummarized in Figures 5 and 6 (biography andweather, respectively).Similar to the text?understandability task, anacceptable performance pattern should include theDocOrig texts being preferred to both DocSys andDocBase generations and the DocSys generationpreferred to the DocBase.
The closer the Doc-Sys generation is to the DocOrig, the better Doc-Sys is performing.
The biography domain illus-1412Figure 5: Biography Sentence Evaluations.trates this scenario (Figure 5) where the results aresimilar to the text-understandability experiments.In contrast, for weather domain, sentences fromDocBase system were preferred to our system?s(Figure 6).
We looked at the cases where thepreferences were in favor of DocBase.
It appearsthat because of high syntactic variability, our sys-tem can produce quite complex sentences where asthe non-experts seem to prefer shorter and simplersentences because of the complexity of the text.In terms of significance, there are no statisti-cally significant differences between the systemsfor weather (DocOrig vs. DocSys - ?2=6.48,d.f.=1, p=.011; DocOrig vs. DocBase - ?2=.720,d.f.=1, p=.396; DocSys vs. DocBase - ?2=.720,d.f.=1, p=.396).
The trend is different compared tothe fluency metric above in that the DocBase sys-tem is outperforming the DocOrig generations toan almost statistically significant difference - theremaining comparisons follow the trend.
We be-lieve that this is for similar reasons stated above- i.e., the generation may be a more digestibleversion of a technical document.
More problem-atic is the results of the biography evaluations.Here there is a statistically significant differencebetween the DocSys and DocOrig and no sta-tistically significant difference between the Doc-Sys and DocBase generations (DocOrig vs. Doc-Sys - ?2=76.880, d.f.=1, p<.0001; DocOrig vs.DocBase - ?2=38.720, d.f.=1, p<.0001; DocSysvs.
DocBase - ?2=.720, d.f.=1, p=.396).
Again,this distribution of preferences is numerically sim-ilar to the trend we would like to see, but the sta-tistical significance indicates that there is someground to make up.
Expert evaluations are po-tentially informative for identifying specific short-comings and how best to address them.Figure 6: Weather Sentence Evaluations.4.4 Expert Human EvaluationsWe performed expert evaluations for the biogra-phy domain only as we do not have access toweather experts.
The four biography reviewers arejournalists who write short biographies for newsarchives.For the biography domain, evaluations of thetexts were largely similar to the evaluations ofthe non-expert crowd (76.22 average agreementfor the sentence?preference task and 72.95 for thetext?understandability task).
For example, the dis-fluent ratings were highest for the DocBase gen-erations and lowest for the DocOrig generations.Also, the fluent ratings were highest for the Do-cOrig generations, and while the combined flu-ent and understandable are higher for DocSys ascompared to DocBase, the DocBase generationshad a 10% higher fluent score (58.22%) as com-pared to the DocSys fluent score (47.97%).
Basedon notes from the reviewers, the succinctness ofthe the DocBase generations are preferred in someways as they are in keeping with certain editorialstandards.
This is further reflected in the sentencepreferences being 70% in favor of the DocBasegenerations as compared to the DocSys genera-tions (all other sentence comparisons were consis-tent with the non-expert crowd).These expert evaluations provide much neededclarity to the NLG process.
Overall, our systemis generating clearly acceptable texts.
Further,there are enough parameters inherent in the systemto tune to different domain expectations.
This isan encouraging result considering that no expertswere involved in the development of the system -a key contrast to many other existing (especiallyrule-based) NLG systems.14135 Conclusions and Future WorkWe have presented a hybrid (template-based andstatistical), single?staged NLG system that gen-erates natural sounding texts and is domain?adaptable.
Our experiments with both ex-perts and non?experts demonstrate that thesystem-generated texts are comparable to human?authored texts.
The development time to adaptour system to new domains is small compared toother NLG systems; around a week to adapt thesystem to weather and biography domains.
Mostof the development time was spent on creating thedomain-specific entity taggers for the weather do-main.
The development time would be reduced tohours if the historical data for a domain is readilyavailable with the corresponding input data.The main limitation of our system is that it re-quires significant historical data.
Our system doesconsolidate many traditional components (macro-and micro-planning, lexical choice and aggrega-tion),5 but the system cannot be applied to the do-mains with no historical data.
The quality and thelinguistic variability of the generated text is di-rectly proportional to the amount of historical dataavailable.We also presented a new automatic metric toevaluate generated texts at document collectionlevel to identify boilerplate texts.
This metriccomputes ?syntactic repetitiveness?
by countingthe number of unique template sequences acrossthe given document collection.Future work will focus on extending our frame-work by adding additional features to the modelthat could improve the quality of the generatedtext.
For example, most NLG pipelines have aseparate component responsible for referring ex-pression generation (Krahmer and van Deemter,2012).
While we address the associated concernof data consumption in Section 3.5, we currentlydo not have any features that would handle refer-ring expression generation.
We believe that thisis possible by identifying referring expressions intemplates and adding features to the model to givehigher scores to the templates having relevant re-ferring expressions.
We also would like to inves-tigate using all the top-scored templates insteadof the highest-scoring template.
This would helpachieve better syntactic-variability scores by pro-ducing more natural-sounding texts.5Lexical choice and aggregation are ?handled?
insofar astheir existence in the historical data.AcknowledgmentsThis research is made possible by ThomsonReuters Global Resources (TRGR) with particu-lar thanks to Peter Pircher, Jaclyn Sprtel and BenHachey for significant support.
Thank you alsoto Khalid Al-Kofahi for encouragment, LeszekMichalak and Andrew Lipstein for expert evalua-tions and three anonymous reviewers for construc-tive feedback.ReferencesGabor Angeli, Percy Liang, and Dan Klein.
2012.
Asimple domain-independent probabilistic approachto generation.
In Proceedings of the 2010 Confer-ence on Empirical Methods for Natural LanguageProcessing (EMNLP 2010), pages 502?512.Regina Barzilay and Mirella Lapata.
2005.
Collectivecontent selection for concept-to-text generation.
InProceedings of the 2005 Conference on EmpiricalMethods for Natural Language Processing (EMNLP2005), pages 331?338.John Bateman and Michael Zock.
2003.
Naturallanguage generation.
In R. Mitkov, editor, OxfordHandbook of Computational Linguistics, Researchin Computational Semantics, pages 284?304.
Ox-ford University Press, Oxford.Anja Belz and Ehud Reiter.
2006.
Comparing au-tomatic and human evaluation of NLG systems.
InProceedings of the European Association for Com-putational Linguistics (EACL?06), pages 313?320.Anja Belz.
2007.
Probabilistic generation of weatherforecast texts.
In Proceedings of Human LanguageTechnologies 2007: The Annual Conference of theNorth American Chapter of the Association forComputational Linguistics (NAACL-HLT?07), pages164?171.Johan Bos.
2008.
Wide-coverage semantic analysiswith Boxer.
In J. Bos and R. Delmonte, editors,Semantics in Text Processing.
STEP 2008 Confer-ence Proceedings, volume 1 of Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.Nadjet Bouayad-Agha, Gerard Casamayor, and LeoWanner.
2011.
Content selection from an ontology-based knowledge base for the generation of foot-ball summaries.
In Proceedings of the 13th Eu-ropean Workshop on Natural Language Generation(ENLG), pages 72?81.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the EMNLP 2011 Workshop on Statisti-cal Machine Translation, pages 85?91.1414Pablo A. Duboue and Kathleen R. McKeown.
2003.Statistical acquisition of content selection rules fornatural language generation.
In Proceedings of the2003 Conference on Empirical Methods for NaturalLanguage Processing (EMNLP 2003), pages 2003?2007.Eduard H. Hovy.
1993.
Automated discourse gener-ation using discourse structure relations.
ArtificialIntelligence, 63:341?385.Blake Howald, Ravi Kondadadi, and Frank Schilder.2013.
Domain adaptable semantic clustering instatistical nlg.
In Proceedings of the 10th Inter-national Conference on Computational Semantics(IWCS 2013), pages 143?154.
Association for Com-putational Linguistics, March.Thorsten Joachims.
2002.
Learning to Classify TextUsing Support Vector Machines.
Kluwer.Hans Kamp and Uwe Reyle.
1993.
From Discourseto Logic; An Introduction to Modeltheoretic Seman-tics of Natural Language, Formal Logic and DRT.Kluwer, Dordrecht.Colin Kelly, Ann Copestake, and Nikiforos Karama-nis.
2009.
Investigating content selection for lan-guage generation using machine learning.
In Pro-ceedings of the 12th European Workshop on NaturalLanguage Generation (ENLG), pages 130?137.Ioannis Konstas and Mirella Lapata.
2012.
Concept-to-text generation via discriminative reranking.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics, pages 369?378.Emiel Krahmer and Kees van Deemter.
2012.
Com-putational generation of referring expression: A sur-vey.
Computational Linguistics, 38(1):173?218.Irene Langkilde and Kevin Knight.
1998.
Generationthat exploits corpus-based statistical knowledge.
InProceedings of the 36th Annual Meeting of the As-sociation for Computational Linguistics (ACL?98),pages 704?710.Vladimir Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
So-viet Physics Doklady, 10:707?710.Wei Lu and Hwee Tou Ng.
2011.
A probabilisticforest-to-string model for language generation fromtyped lambda calculus expressions.
In Proceed-ings of the 2011 Conference on Empirical Methodsfor Natural Language Processing (EMNLP 2011),pages 1611?1622.Wei Lu, Hwee Tou Ng, and Wee Sun Lee.
2009.
Nat-ural language generation with tree conditional ran-dom fields.
In Proceedings of the 2009 Conferenceon Empirical Methods for Natural Language Pro-cessing (EMNLP 2009), pages 400?409.Kathleen R. McKeown.
1985.
Text Generation: UsingDiscourse Strategies and Focus Constraints to Gen-erate Natural Language Text.
Cambridge UniversityPress.Johanna D. Moore and Cecile L. Paris.
1993.
Planningtext for advisory dialogues: Capturing intentionaland rhetorical information.
Computational Linguis-tics, 19(4):651?694.Kishore Papineni, Slim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics (ACL?02), pages 311?318.Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge Univer-sity Press.Ehud Reiter, Somayajulu Sripada, Jim Hunter, and JinYu.
2005.
Choosing words in computer-generatedweather forecasts.
Artificial Intelligence, 167:137?169.Jacques Robin and Kathy McKeown.
1996.
Exmpiri-cally designing and evaluating a new revision-basedmodel for summary generation.
Artificial Intelli-gence, 85(1-2).Somayajulu Sripada, Ehud Reiter, Jim Hunter, andJin Yu.
2001.
A two-stage model for contentdetermination.
In Proceedings of the 8th Euro-pean Workshop on Natural Language Generation(ENLG), pages 1?8.Kees van Deemter, Marie?t Theune, and Emiel Krahmer.2005.
Real vs. template-based natural language gen-eration: a false opposition?
Computational Linguis-tics, 31(1):15?24.Ian Witten and Eibe Frank.
2005.
Data Mining: Prac-tical Machine Learning Techniques with Java Imple-mentation (2nd Ed.).
Morgan Kaufmann, San Fran-cisco, CA.1415
