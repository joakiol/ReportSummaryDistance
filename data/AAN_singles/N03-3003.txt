Language choice models for microplanning and readabilitySandra WilliamsDepartment of Computing ScienceUniversity of AberdeenAberdeen AB24 3UE, UKswilliam@csd.abdn.ac.ukAbstractThis paper describes the construction of lan-guage choice models for the microplanning ofdiscourse relations in a Natural LanguageGeneration system that attempts to generateappropriate texts for users with varying levelsof literacy.
The models consist of constraintsatisfaction problem graphs that have been de-rived from the results of a corpus analysis.The corpus that the models are based on waswritten for good readers.
We adapted themodels for poor readers by allowing certainconstraints to be tightened, based on psycho-linguistic evidence.
We describe how the de-sign of microplanner is evolving.
We discussthe compromises involved in generating morereadable textual output and implications ofour design for NLG architectures.
Finally wedescribe plans for future work.1  IntroductionGenerator for Individual Reading Levels (GIRL) is aNatural Language Generation (NLG) system that gener-ates feedback reports for adults during a web-based lit-eracy assessment.
The inputs to GIRL are answers toquestions in a literacy assessment.
GIRL currently gen-erates a feedback report after each of eight skill-basedtests in the assessment.
An example output report, gen-erated after the spelling test, is shown in Figure 1.GIRL is being developed with the aim of tailoringits output texts to the individual reading skills of read-ers.
Our particular focus is on adults who have poorreading skills due to a number of reasons includingmissed school, dyslexia, poor eyesight, memory prob-lems, etc.
Poor literacy is a major problem in the UKwhere up to one fifth of the adult population is function-ally illiterate (Moser 1999).Using Kintsch and Vipond?s (1979) definition, werelate readability to performance on the reading task(i.e.
reading speed, ability to answer comprehensionquestions and ability to recall content).
We measuredthe first two of these in preliminary experiments thattested outputs from GIRL on both good and bad readers(Williams et al 2003).Sally Test,SPELLINGYou finished the SPELLING test, well done.You got eleven out of fifteen, so you need to practise.Sometimes you could not spell longer words.
Forexample, you did not click on: necessary.Many people find learning to spell hard, but you can do it.If you practise reading, then your skills will improve.Figure 1.
A feedback report generated by GIRLOur research is focused on decisions GIRL makes atthe discourse level.
A previous project, PSET  (Devlinand Tait 1998, Devlin et al 2000), has already madesome progress towards lexical-level and syntax-levelsimplifications for poor readers.
In GIRL, it is at thediscourse level that choices are made that affect sen-tence length and selection of discourse cue phrases(phrases that render discourse relations explicit to thereader, e.g.
?for example?, ?so?
and ?if?, in Figure 1).These choices are made in a module called the micro-planner (see Reiter and Dale 2000).The inputs to the microplanner are a model of auser?s reading ability and a tree-structured documentplan (Reiter and Dale 2000) that includes discourse re-lations.
In GIRL, discourse relations are schemas ar-ranged in a discourse tree structure.
Each schema hasslots for semantic roles filled by daughter text spans, ordaughter relations.
For instance, the condition  relationhas two semantic roles: a condition and a consequent.Figure 2 shows a discourse relation tree structure withits corresponding schema.
The root relation, R1, is aconcession  (type: concession), with one daughter rela-Edmonton, Ma -June 2003Student Research Workshop , pp.
13-18Proceedings of HLT-NAACL 2003tion, R2, filling the ?concession?
slot and a text spandaughter, S1, filling the ?statement?
slot.
R2 is a condi-tion relation with two text span daughters: S3 filling the?condition?
slot and S2 the ?consequent?
slot.Figure 2.
Discourse relation tree structure and schema.The task of GIRL?s microplanner is to decide on theordering of the daughters, how they should be packedinto sentences (aggregation), whether there should bepunctuation between the daughters, whether discoursecue phrases should be present and, if so, which ones andwhere they should be placed.
The microplanner willultimately adapt the choices it makes to the readinglevel of individual users (readers) from user modelsbuilt from users?
answers to up to ninety questions froma literacy test.
Our current implementation only consid-ers two generic types of user - ?good readers?
and ?badreaders?.Suppose the input to the microplanner is a discourseplan containing the discourse relation tree in Figure 2.
Itshould be able to calculate that this could be generatedin a number of different ways.
Just a few of them are:  You made four mistakes.
But you can learn to fill in forms ifyou practise.  Although you made four mistakes, you can learn to fill informs ... just as soon as you practise.  You made four mistakes.
But if you practise, you can learnto fill in forms.  If you practise, you can learn to fill in forms.
You made fourmistakes, though.and it should be able to choose which of these is themost appropriate for poor readers.The remainder of this paper describes what we be-lieve is a novel approach to building language choicemodels for microplanning.
We explain how these mod-els evolved (section 2) and the implications of this de-sign (section 3).
Section 4 draws conclusions from thecurrent work and outlines our plans for future work.2  Constructing the microplannerThis section describes the stages in the construction ofthe microplanner.
Each stage is based on empirical evi-dence.
Firstly, we acquired knowledge about how hu-man writers linguistically realise specific discourserelations by carrying out a corpus analysis (see Williamsand Reiter 2003).
Secondly, we selected the best methodfor representing this knowledge and built choice modelsfrom the corpus analysis data.
Then, because the corpuswas written for good readers, we had to adapt the mod-els for poor readers.
For this, we used results from psy-cholinguistic studies, including results from our ownpreliminary experiments (see Williams et al 2003).Finally, these individual parts were combined to pro-duce the finished microplanner.2.1 Reconfiguring our corpus analysis resultsWe analysed seven discourse relations  (Williams andReiter 2003), including concession, condition, elabora-tion-additional, evaluation, example, reason  and re-statement, using the RST Discourse Treebank corpus(Carlson et al 2002).
We analysed one hundred in-stances of each relation noting the following six fea-tures:  L1: length of the first text span (in words).  L2: length of the second text span (in words).  O: ordering of the text spans.  Ps: position(s) of discourse cue phrase(s).  P: between-text-span punctuation.  C: discourse cue phrase(s).An example to demonstrate these features is the conces-sion relation in the last example given above: ?If youpractise, you can learn to fill in forms.
You made fourmistakes, though.?
Here, L1  is ten words (this includesthe whole of the condition daughter), L2  is five words,O  is concession-statement, Ps is after the statement, P  isa full stop and C  is ?though?.These features were chosen on the basis of previouswork (Moser and Moore 1996) and because they influ-ence sentence length and lexical choice which areknown to be important factors in readability.
The analy-sis revealed some of the ways in which human authorsselect these features when writing for good readers.These provided a partial specification for modellingdiscourse-level choices that should be available in anRELATIONid: R1type: concessionconcession:statement:RELATIONid: R2type: conditioncondition:consequent:TEXT SPANid:    S1text: ?you made four mistakes?TEXT SPANid:     S3text: ?you practise ?TEXT SPANid:    S2text: ?you can learnto fill in forms?S3 S2S1R2R1NLG system.
Furthermore the analysis demonstratedthat the features are interdependent.The results from our corpus analysis (Williams andReiter 2003) were simplified.
The numbers of values forsome features were cut down by re-classifying them asmembers of a smaller number of categories.
Lengthbecame either ?long?
or ?short?.
The data for each rela-tion was split into two, so that roughly half the L1 in-stances fell into the ?short?
category (e.g.
forconcession, short = 1-15 words, long = >15 words).Between-text-span punctuation was divided into justthree categories: none, non-sentence-breaking, and sen-tence-breaking.
The restatement relation was an excep-tion because it had such a large proportion of open-parentheses (62%) that an extra category was created.
Inrestatement, it seems that punctuation is often used in-stead of a cue phrase to signal the relation.
The cuephrase feature was left with larger numbers of values toprovide GIRL with the maximum number of choices forlexical selection.The data was reconfigured as sets of 6-tuples.
Eachrepresents a set of values for one instance of a relation:i.e.
<L1,L2,O,Ps,P,C>.
For instance, the concessionrelation described above would be represented as<short, short, concession-statement, after_statement, fullstop, ?though?>.
We thus created seven hundred 6-tuples in total, one hundred per relation.
For each rela-tion, these were sorted, duplicates were counted andsuperfluous duplicates removed.
Of the resulting unique6-tuples, some were rejected and are not used in thecurrent language choice models.
For example, in theconcession choice  model forty-six unique 6-tuples cover100% of the corpus data and sixteen were rejected, re-sulting in a coverage of 75%.
For condition, forty-sevenunique 6-tuples cover 100% but only twenty-six wereincluded and these cover 72%.Figure 3.
Some cue phrases, with those not in thecurrent language models marked with asterisksThe reason why some tuples were rejected is be-cause GIRL?s present shallow approaches to syntacticand semantic processing cannot generate them.
It cannotcurrently generate embedded discourse text spans, norcan it generate discourse cue phrases in mid-text-spanpositions.
Both of these would require the implementa-tion of deeper syntactic processing.
Certain 6-tuplescontain discourse cue phrases that would not makesense when generated unless we implement deeper se-mantic processing.
Figure 3 shows some examples ofthese.
Cue phrases marked with asterisks have beenrejected from the current language models because theyrequire deeper processing.Our current method for reconfiguring the data ismanual, using existing spreadsheet, database and statis-tics packages.
We are investigating how it could beautomated, given that some decisions, such as which 6-tuples to reject, require human judgement.2.2 Building CSP graphs for good readersHaving reconfigured the results of our corpus analy-sis, we searched for the best way to model the choicesthey represent.
We tried exploring both discriminantanalysis statistics and machine learning of decision treesin attempts to identify which feature(s) would mostclearly divide the data into groups.
For most discourserelations, the positions of discourse cue phrases werethe most discriminating features.The most crucial characteristic of the choice modelswe were attempting to build was that they should reflectthe interdependencies of the features found in the corpusanalysis.
For instance, in most relations the selection ofbetween-span punctuation is dependent on the length ofthe first text span.
For some relations (not all), thismeans that as the first text span gets longer, the be-tween-span punctuation tends to change from no punc-tuation, to comma, to full stop.
Similarly, the selectionof punctuation depends on the order of text spans, par-ticularly with the condition  relation.
If the order is con-dition-consequent, there tends to be a comma betweentext spans, if the order is consequent-condition, there isoften no punctuation.
And so on with interdependenciesbetween all the other features.The best representation we have found to date thatfits this requirement is constraint satisfaction problem(CSP) graphs.
Power (2000) demonstrated that CSPscould be used to map rhetorical structures (i.e.
discourserelation trees) to text structures (paragraphs, sentences,etc.).
Our task is similar to Power?s, but we emphasisedifferent processes, such as cue phrase choice, ourchoice models are based on empirical evidence, and wehave the additional criteria that the representationsshould be adaptable for different reading abilities.
Itturned out that CSP graphs were ideal for this purpose,since we exploit CSP?s notion of ?tightening?
the con-straints in our solution for adapting the models for poorreaders (see section 2.3).IfAs soon asOnce              you practise, you will improve.
* Until* Should* Without* Unless* GivenifYou will improve    as long as      you practise.as soon asonly if* should* unless*untilWe used the Java Constraint Library (JCL 2.1) fromthe Artificial Intelligence Laboratory at the Swiss Fed-eral Institute of Technology in Lausanne (Torrens 2002)which we found to be portable, relatively bug-free andeasy to plug straight into our system which is writtenentirely in Java.We built computer models representing the six key fea-tures of discourse relations and their interdependentvalues.
One CSP graph was built for each of the sevendiscourse relations.
The structure of the graphs is ex-actly the same for each relation with six nodes and fif-teen connections linking every node to all the others.This structure is illustrated in figure 4.Figure 4.
CSP graph representing a discourse relation.The nodes in the graph in figure 4 are CSP domain vari-ables.
Each represents one of the six features.
The num-bers of values for each node varies for each relation.Constraints between the variables were represented as?good lists?.
Both values and constraints were codeddirectly from the 6-tuple data.
Good lists contain pairsof values that are ?legal?
for two variables.
For in-stance, a connection between L1 and P might containthe pair <short, non-sentence-breaking> in its good list,meaning: if the length of the first text span in the rela-tion is short, put non-sentence-breaking punctuation,such as a comma, between the text spans.
The numbersof pairs in the ?good lists?
attached to each of the fifteenconnections varies for each relation.We used pairs of ?legal?
values in the CSP goodlists because the corpus analysis is too small to predictthe probabilities of triples.
We are currently working onexpanding the size of our corpus analysis.
We wantedthe CSP graphs to generate solutions that gave as good acoverage of the 6-tuples included in the models as pos-sible, but we did not want to overgenerate instances thatdid not occur in the analysis.
This required delicate bal-ancing of the two factors.2.3  Adapting the models for poor readersbased on psycholinguistic evidenceThe language choice models were adapted for poorreaders by tightening the constraints.
We studied thepsycholinguistic and educational literature to determinehow they should be tightened.
We also carried out pre-liminary experiments of our own (Williams et al 2003)which indicated that certain discourse-level featuresaffect readability for poor readers more than good read-ers.
Selecting more common discourse cue phrases andthe placing punctuation between discourse segmentswere both particularly helpful for poor readers.Existing psycholinguistic research on reading haslittle to say about adults with poor literacy.
It has tendedto focus on proficient adult readers (University stu-dents), rather than on the problems of adult learnerreaders.
Where it has investigated the development ofreading skills, it has tended to focus on children, ratherthan adults.
Educationalists maintain that the readingskill profiles of adults with poor literacy are differentfrom those of children.
?Normal?
children tend to de-velop reading skills evenly, whereas adults who arefunctionally illiterate tend to have developed unevenly(Strucker 1997).
Yet another problem is that it tends tofocus on single words, single sentences, or pairs of sen-tences, that are presented to a reader out-of-context,rather than in multiple-sentence documents.There are some exceptions, however.
Devlin andTait (1998) found that the readability of newspaper textswas increased for seven out of ten aphasic readers whenthey replaced infrequent words with more frequentsynonyms.
Leijten and Van Waes (2001) reported thatelderly readers?
comprehension and recall improvedwhen they were presented with causal discourse struc-tures containing explicit discourse cue phrases and ex-plicit headings.
Degand et al (1999) observed thatremoval of even a few cue phrases affects comprehen-sion and recall of the entire content.
The last two studieswere with adult readers from the general public with(presumably) varying levels of reading ability.To sum up, use of cue phrases, selection of commoncue phrases and use of between-span punctuation allseem to help bad readers.
We therefore chose to tightenthe constraints to favour solutions with these features.Frequencies for cue phrases were obained from apart-of-speech (POS) search  (Aston and Burnard 1998)in the 100 million word British National Corpus.Phrases like ?for example?
are annotated with a singlepart-of-speech in the BNC.
Some results are shown inTable 1.
Cue phrases do not all have the same POS, andthey are not, of course, exact synonyms, so it is not al-ways possible to substitute one for another even if bothare from the same relation.
?Such as?
can not always besubstituted for ?for instance?, but ?for example?
is aclose synonym and it is possible to do a substitution.We tightened constraints, where possible, to favourwords that occur in the Dolch lists used by adult literacytutors.
These list the most commonly occurring functionwords that beginner readers are taught to sight read.Another danger with substituting common phrasesfor less common ones is that the most common phrasesL2O PsL1P  Care also the most ambiguous.
The cue phrases ?but?
and?and?
both occurred in four relations (concession, elabo-ration-additional, evaluation and reason) out of sevenin the corpus analysis and these are relations with verydifferent meanings.
These problems require further in-vestigation.Cue phrase BNC freq.
Dolch listalthough 42,758 -and  2,615,144 yesbecause 83,181 yesbut 443,164 yesfor example 23,643 yesfor instance 7,344 -if 230,892 yesstill 67,106 -though 33,337 -Table 1.
Cue, BNC frequency & Dolch list presence.2.4  Putting it all together ?
the microplannerFigure 4 shows the main components of the microplan-ner.
The inputs are a model of the user?s reading ability(marked ?user model?)
and a document plan containingdiscourse relation trees, marked ?DocPlan?.
Both arebuilt by system modules occurring earlier than the mi-croplanner in the processing sequence.
The documentplan in figure 4 is the same as shown above in figure 2.Working bottom-up, a CSP graph for the current rela-tion is retrieved from the CSP graph knowledge baseand the constraints are tightened or relaxed according tothe user model.
The CSP Solver (Torrens 2002) thenuses simple backtracking search to find all solutions forthe relation.
The solutions found by the CSP Solver arepassed through a filter which currently picks the mostfrequently occurring one for good readers and the onewith overall shortest sentences for poor readers.
Theoutput is a schema that the next module of GIRL uses toconstruct messages.It does not always output the most coherent solution.For instance, the output shown in figure 5 would resultin a final output of ?You made four mistakes.
But if youpractise, you can learn to fill in forms?.
Adjacent dis-course cue phrases do not improve coherency.
The mi-croplanner is still under development, however, futureimprovements, possibly including backtracking, willimprove readability, possibly including coherence con-siderations, such as focus and reference.3  DiscussionAdditional functionality would need to be added to the?filter?
module to choose solutions that optimise dis-course coherence.
Additional nodes might be required inthe constraint graphs.
The simple string content of dis-course relations would have to be replaced by semanticrepresentations.
If it were, the simple pipeline architec-ture would no longer be appropriate, since it currentlydepends on knowing the final length of the strings.On the other hand, when generating text for badreaders, we might have to sacrifice some of these, sincethey might impact on readability.
Ellipsis, for instance,may not be good for bad readers.
Ellipsis is one waythat conciseness can be achieved during aggregation.Current opinion in the NLG community is that aggrega-tion for conciseness is ?a good thing?.
Reape and Mel-lish (1999) even suggest that an NLG system should?aggregate whenever possible?.
But conciseness may beless comprehensible for poor readers.
The sentences inA, below, could be aggregated as in B.A.
Spelling is hard.
But spelling is important.B.
Spelling is hard but important.However, in B a single sentence is longer and thecognitive load for poor readers in working out the el-lipse could be higher.
A little repetition and redundancymight actually turn out to be beneficial!Figure 5.
The microplannerDocPlanUSER MODEL CSP GraphKnowledgeBaseTighten/RelaxConstraintsMICROPLANNERFILTEROutputR2: <short, short,S3-S2, beforeS3,comma, ?if?>R1: <short, long,S1-R2, beforeR2,full stop, ?but?>CSP SolverS3 S2S1R2R1L1 L2OPPsC4  Conclusions and future workThis paper described how we used the results of a cor-pus analysis to build language choice models for a mi-croplanner.
We discussed the creation of constraintsatisfaction problem graphs for our default ?goodreader?
models and how we adapted the models for poorreaders.
Our ?poor reader?
models are based on psy-cholinguistic evidence, including evidence from ourown preliminary experiments.
We discussed some ofthe compromises involved in generating more readabletextual output and the impacts that further developmentcould have on GIRL?s architecture.Plans for future work include expanding the size ofour corpus analysis and automating at least some of theanalysis and data reconfiguration.
We plan further de-velopment of the microplanner to prevent incoherentsolutions being generated.
Further on, we plan to takediscourse coherence considerations into account.We have plans to carry out additional reading ex-periments with good and bad readers to investigatewhether the constraints we tighten to adjust the lan-guage models for poor readers actually produce morereadable results.
We will generate texts under the de-fault ?good reader?
models and under the constrained,poor reader, models.
We will measure reading speedsand comprehension as in our preliminary experiment.
(Williams et al 2003).
We predict that, as we foundthen, good readers will perform equally well on bothmodels and poor readers will perform better on the con-strained models.
We will also carry out user satisfactionevaluations and carry out evaluation surveys with pro-fessional basic skills (adult literacy) tutors.ReferencesGuy Aston and Lou Burnard.
1998.
The BNC Hand-book: Exploring the British National Corpus withSARA.
Edinburgh University Press.Lynn Carlson, D. Marcu, and M Okurowski.
2002.Building a Discourse-Tagged Corpus in the Frame-work of Rhetorical Structure Theory.
Kuppevelt andSmith (eds.)
Current Directions in Discourse andDialogue, Kluwer.Liesbeth Degand, N. Lef?vre and Y. Bestgen.
1999.
Theimpact of connectives and anaphoric expressions onexpository discourse comprehension.
Document De-sign: Journal of Research and Problem Solving inOrganizational Communication, 1 pp.
39-51.Siobhan Devlin and John Tait.
1998.
The Use of a Psy-cholinguistic Database in the Simplification of Textfor Aphasic Readers.
Linguistic Databases.
J. Ner-bonne (ed.)
CSLI Publications.Siobhan Devlin, Y. Canning, J. Tait, J. Carroll, G. Min-nen and D. Pearce.
2000.
An AAC aid for aphasicpeople with reading difficulties.
Proceeding of the 9thBiennial Conference of the International Society forAugmentative and Alternative Communication.Walter Kintsch and Douglas Vipond.
1979.
ReadingComprehension and Readability in Educational Prac-tice and Psychological Theory.
L.Nilsson (ed.)
Per-spectives on Memory Research.
Lawrence Erlbaum.Mari?lle Leijten and Luuk Van Waes.
2001.The impactof text structure and linguistic markers on the textcomprehension of elderly people.
W. Spooren and L.van Waes (eds.)
Proceedings of MultidisciplinaryApproaches to Discourse.Claus Moser.
1999.
Improving literacy and numeracy: afresh start.
Report of the working group chaired bySir Claus Moser.Megan Moser and Johanna Moore.
1996.
On the corre-lation of cues with discourse structure: results from acorpus study.
Unpublished manuscript.Richard Power.
2000.
Mapping Rhetorical Structures toText Structures by Constraint Satisfaction.
Informa-tion Technology Research Institute, Technical ReportITRI-00-01, University of Brighton.Mike Reape and Chris Mellish.
1999.
Just what is ag-gregation anyway?
Proceedings of the Seventh Euro-pean Workshop on Natural Language Generation.Ehud Reiter and Robert Dale.
2000.
Building Natural-Language Generation Systems.
Cambridge Univer-sity Press.John Strucker.
1997.
What silent reading tests alonecan?t tell you: two case studies in adult reading dif-ferences.
Focus on Basics, Vol.
1B, National Centerfor the Study of Adult Learning and Literacy(NCSALL), Harvard University.Marc Torrens.
2002.
Java Constraint Library 2.1.
Artifi-cial Intelligence Laboratory, Swiss Federal Instituteof Technology.
GNU Lesser Public Licence.Sandra Williams and Ehud Reiter.
2003.
A corpusanalysis of discourse relations for Natural LanguageGeneration.
To appear in proceedings of Corpus Lin-guistics 2003.Sandra Williams, Ehud Reiter and Liesl Osman.
2003.Experiments with discourse-level choices and read-ability.
To appear in proceedings of the 9th EuropeanWorkshop on Natural Language Generation.
