Report on the Second NLG Challenge onGenerating Instructions in Virtual Environments (GIVE-2)Alexander KollerSaarland Universitykoller@mmci.uni-saarland.deKristina StriegnitzUnion Collegestriegnk@union.eduAndrew GargettSaarland Universitygargett@mmci.uni-saarland.deDonna ByronNortheastern Universitydbyron@ccs.neu.eduJustine CassellNorthwestern Universityjustine@northwestern.eduRobert DaleMacquarie UniversityRobert.Dale@mq.edu.auJohanna MooreUniversity of EdinburghJ.Moore@ed.ac.ukJon OberlanderUniversity of EdinburghJ.Oberlander@ed.ac.ukAbstractWe describe the second installment of theChallenge on Generating Instructions inVirtual Environments (GIVE-2), a sharedtask for the NLG community which tookplace in 2009-10.
We evaluated sevenNLG systems by connecting them to 1825users over the Internet, and report the re-sults of this evaluation in terms of objec-tive and subjective measures.1 IntroductionThis paper reports on the methodology and resultsof the Second Challenge on Generating Instruc-tions in Virtual Environments (GIVE-2), whichwe ran from August 2009 to May 2010.
GIVEis a shared task for the NLG community whichwe ran for the first time in 2008-09 (Koller et al,2010).
An NLG system in this task must generateinstructions which guide a human user in solvinga treasure-hunt task in a virtual 3D world, in realtime.
For the evaluation, we connect these NLGsystems to users over the Internet, which makesit possible to collect large amounts of evaluationdata cheaply.While the GIVE-1 challenge was a success, inthat it evaluated five NLG systems on data from1143 game runs in the virtual environments, itwas limited in that users could only move andturn in discrete steps in the virtual environments.This made the NLG task easier than intended; oneof the best-performing GIVE-1 systems generatedinstructions of the form ?move three steps for-ward?.
The primary change in GIVE-2 comparedto GIVE-1 is that users could now move and turnfreely, which makes expressions like ?three steps?meaningless, and makes it hard to predict the pre-cise effect of instructing a user to ?turn left?.We evaluated seven NLG systems from six in-stitutions in GIVE-2 over a period of three monthsfrom February to May 2010.
During this time,we collected 1825 games that were played byusers from 39 countries, which is an increase ofover 50% over the data we collected in GIVE-1.
We evaluated each system both on objec-tive measures (success rate, completion time, etc.
)and subjective measures which were collected byasking the users to fill in a questionnaire.
Wecompletely revised the questionnaire for the sec-ond challenge, which now consists of relativelyfine-grained questions that can be combined intomore high-level groups for reporting.
We also in-troduced several new objective measures, includ-ing the point in the game in which users lostor cancelled, and an experimental ?back-to-base?task intended to measure how much users learnedabout the virtual world while interacting with theNLG system.Plan of the paper.
The paper is structured as fol-lows.
In Section 2, we describe and motivate theGIVE-2 Challenge.
In section 3, we describe theevaluation method and infrastructure.
Section 4reports on the evaluation results.
Finally, we con-clude and discuss future work in Section 5.2 The GIVE ChallengeGIVE-2 is the second installment of the GIVEChallenge (?Generating Instructions in Virtual En-vironments?
), which we ran for the first time in2008-09.
In the GIVE scenario, subjects try tosolve a treasure hunt in a virtual 3Dworld that theyhave not seen before.
The computer has a com-plete symbolic representation of the virtual world.The challenge for the NLG system is to gener-ate, in real time, natural-language instructions thatwill guide the users to the successful completionof their task.Users participating in the GIVE evaluationstart the 3D game from our website at www.give-challenge.org.
They then see a 3DFigure 1: What the user sees when playing withthe GIVE Challenge.game window as in Fig.
1, which displays instruc-tions and allows them to move around in the worldand manipulate objects.
The first room is a tuto-rial room where users learn how to interact withthe system; they then enter one of three evaluationworlds, where instructions for solving the treasurehunt are generated by an NLG system.
Users caneither finish a game successfully, lose it by trig-gering an alarm, or cancel the game.
This result isstored in a database for later analysis, along with acomplete log of the game.In each game world we used in GIVE-2, playersmust pick up a trophy, which is in a wall safe be-hind a picture.
In order to access the trophy, theymust first push a button to move the picture to theside, and then push another sequence of buttons toopen the safe.
One floor tile is alarmed, and play-ers lose the game if they step on this tile withoutdeactivating the alarm first.
There are also a num-ber of distractor buttons which either do nothingwhen pressed or set off an alarm.
These distractorbuttons are intended to make the game harder and,more importantly, to require appropriate referenceto objects in the game world.
Finally, game worldscontained a number of objects such as chairs andflowers that did not bear on the task, but wereavailable for use as landmarks in spatial descrip-tions generated by the NLG systems.The crucial difference between this task andthe (very similar) GIVE-1 task was that in GIVE-2, players could move and turn freely in the vir-tual world.
This is in contrast to GIVE-1, whereplayers could only turn by 90 degree increments,and jump forward and backward by discrete steps.This feature of the way the game controls were setup made it possible for some systems to do verywell in GIVE-1 with only minimal intelligence,using exclusively instructions such as ?turn right?and ?move three steps forward?.
Such instructionsare unrealistic ?
they could not be carried over toinstruction-giving in the real world ?, and our aimwas to make GIVE harder for systems that reliedon them.3 MethodFollowing the approach from the GIVE-1 Chal-lenge (Koller et al, 2010), we connected the NLGsystems to users over the Internet.
In each gamerun, one user and one NLG system were paired up,with the system trying to guide the user to successin a specific game world.3.1 Software infrastructureWe adapted the GIVE-1 software to the GIVE-2setting.
The GIVE software infrastructure (Kolleret al, 2009a) consists of three different mod-ules: The client, which is the program which theuser runs on their machine to interact with thevirtual world (see Fig.
1); a collection of NLGservers, which generate instructions in real-timeand send them to the client; and a matchmaker,which chooses a random NLG server and virtualworld for each incoming connection from a clientand stores the game results in a database.The most visible change compared to GIVE-1was to modify the client so it permitted free move-ment in the virtual world.
This change further ne-cessitated a number of modifications to the inter-nal representation of the world.
To support the de-velopment of virtual worlds for GIVE, we changedthe file format for world descriptions to be muchmore readable, and provided an automatic toolfor displaying virtual worlds graphically (see thescreenshots in Fig.
2).3.2 Recruiting subjectsParticipants were recruited using email distribu-tion lists and press releases posted on the Internetand in traditional newspapers.
We further adver-tised GIVE at the Cebit computer expo as part ofthe Saarland University booth.
Recruiting anony-mous experimental subjects over the Internet car-ries known risks (Gosling et al, 2004), but weshowed in GIVE-1 that the results obtained forthe GIVE Challenge are comparable and more in-formative than those obtained from a laboratory-World 1 World 2 World 3Figure 2: The three GIVE-2 evaluation worlds.based experiment (Koller et al, 2009b).We also tried to leverage social networks for re-cruiting participants by implementing and adver-tising a Facebook application.
Because of a soft-ware bug, only about 50 participants could be re-cruited in this way.
Thus tapping the true poten-tial of social networks for recruiting participantsremains a task for the next installment of GIVE.3.3 Evaluation worldsFig.
2 shows the three virtual worlds we used in theGIVE-2 evaluation.
Overall, the worlds were moredifficult than the worlds used in GIVE-1, wheresome NLG-systems had success rates around 80%in some of the worlds.
As for GIVE-1, the threeworlds were designed to pose different challengesto the NLG systems.
World 1 was intended to bemore similar to the development world and lastyear?s worlds.
It did have rooms with more thanone button of the same color, however, these but-tons were not located close together.
World 2 con-tained several situations which required more so-phisticated referring expressions, such as roomswith several buttons of the same color (some ofthem close together) and a grid of buttons.
Fi-nally, World 3 was designed to exercise the sys-tems?
navigation instructions: one room containeda ?maze?
of alarm tiles, and another room twolong rows of buttons hidden in ?booths?
so thatthey were not all visible at the same time.3.4 TimelineAfter the GIVE-2 Challenge was publicized inJune 2009, fifteen researchers and research teamsdeclared their interest in participating.
We dis-tributed a first version of the software to theseteams in August 2009.
In the end, six teams sub-mitted NLG systems (two more than in GIVE-1);one team submitted two independent NLG sys-tems, bringing the total number of NLG systemsup to seven (two more than in GIVE-1).
Thesewere connected to a central matchmaker that ranfor a bit under three months, from 23 February to17 May 2010.3.5 NLG systemsSeven NLG systems were evaluated in GIVE-2:?
one system from the Dublin Institute of Tech-nology (?D?
in the discussion below);?
one system from Trinity College Dublin(?T?);?
one system from the Universidad Com-plutense de Madrid (?M?);?
one system from the University of Heidelberg(?H?);?
one system from Saarland University (?S?);?
and two systems from INRIA Grand-Est inNancy (?NA?
and ?NM?
).Detailed descriptions of these systems as wellas each team?s own analysis of the evalua-tion results can be found at http://www.give-challenge.org/research.4 ResultsWe now report the results of GIVE-2.
We startwith some basic demographics; then we discussobjective and subjective evaluation measures.
Thedata for the objective measures are extracted fromthe logs of the interactions; whereas the data forthe subjective measures are obtained from a ques-tionnaire which asked subjects to rate various as-pects of the NLG system they interacted with.Notice that some of our evaluation measures arein tension with each other: For instance, a sys-tem which gives very low-level instructions mayallow the user to complete the task more quickly(there is less chance of user errors), but it will re-quire more instructions than a system that aggre-gates these.
This is intentional, and emphasizesour desire to make GIVE a friendly comparativechallenge rather than a competition with a clearwinner.4.1 DemographicsOver the course of three months, we collected1825 valid games.
This is an increase of almost60% over the number of valid games we collectedin GIVE-1.
A game counted as valid if the gameclient did not crash, the game was not marked as atest game by the developers, and the player com-pleted the tutorial.Of these games, 79.0% were played by malesand 9.6% by females; a further 11.4% did notspecify their gender.
These numbers are compa-rable to GIVE-1.
About 42% of users connectedfrom an IP address in Germany; 12% from the US,8% from France, 6% from Great Britain, and therest from 35 further countries.
About 91% of theparticipants who answered the question self-ratedtheir English language proficiency as ?good?
orbetter.
About 65% of users connected from vari-ous versions of Windows, the rest were split aboutevenly between Linux and MacOS.4.2 Objective measuresThe objective measures are summarize in Fig.
3.In addition to calculating the percentage of gamesusers completed successfully when being guidedby the different systems, we measured the timeuntil task completion, the distance traveled untiltask completion, and the number of actions (suchas pushing a button to open a door) executed.
Fur-thermore, we counted howmany instructions usersreceived from each system, and how many wordsthese instructions contained on average.
All objec-tive measures were collected completely unobtru-sively, without requiring any action on the user?spart.
To ensure comparability, we only countedsuccessfully completed games.task success: Did the player get the trophy?duration: Time in seconds from the end of the tu-torial until the retrieval of the trophy.distance: Distance traveled (measured in distanceunits of the virtual environment).actions: Number of object manipulation actions.instructions: Number of instructions producedby the NLG system.words per instruction: Average number ofwords the NLG system used per instruction.Figure 3: Objective measures.Fig.
4 shows the results of these objective mea-sures.
Task success is reported as the percent-age of successfully completed games.
The othermeasures are reported as the mean number of sec-onds/distance units/actions/instructions/words perinstruction, respectively.
The figure also assignssystems to groups A, B, etc.
for each evaluationmeasure.
For example, users interacting with sys-tems in group A had a higher task success rate,needed less time, etc.
than users interacting withsystems in group B.
If two systems do not sharethe same letter, the difference between these twosystems is significant with p < 0.05.
Significancewas tested using a ?2-test for task success andANOVAs for the other objective measures.
Thesewere followed by post-hoc tests (pairwise ?2 andTukey) to compare the NLG systems pairwise.In terms of task success, the systems fall prettyneatly into four groups.
Note that systems D andT had very low task success rates.
That meansthat, for these systems, the results for the other ob-jective measures may not be reliable because theyare based on just a handful of games.
Anotheraspect in which systems clearly differed is howmany words they used per instruction.
Interest-ingly, the three systems with the best task successrates also produced the most succinct instructions.The distinctions between systems in terms of theother measures is less clear.4.3 Subjective measuresThe subjective measures were obtained from re-sponses to a questionnaire that was presented tousers after each game.
The questionnaire askedusers to rate different statements about the NLGD H M NA NM S Ttasksuccess9% 11% 13% 47% 30% 40% 3%A ABC C CD Dduration888 470 407 344 435 467 266A A A A AB B B B BCdistance231 164 126 162 167 150 89A A A A A AB B B B Bactions25 22 17 17 18 17 14A A A A A A Ainstructions349 209 463 224 244 244 78A A A A A AB Bwords perinstruction15 11 16 6 10 6 18A ABCDE EFigure 4: Results for the objective measures.system using a continuous slider.
The slider posi-tion was translated to a number between -100 and100.
Figs.
7 and 6 show the statements that userswere asked to rate as well as the results.
Theseresults are based on all games, independent of thesuccess.
We report the mean rating for each item,and, as before, systems that do not share a letter,were found to be significantly different (p< 0.05).We used ANOVAs and post-hoc Tukey tests to testfor significance.
Note that some items make a pos-itive statement about the NLG system (e.g., Q1)and some make a negative statement (e.g., Q2).For negative statements, we report the reversedscores, so that in Figs.
7 and 6 greater numbers arealways better, and systems in group A are alwaysbetter than systems in group B.In addition to the items Q1?Q22, the ques-tionnaire contained a statement about the over-all instruction quality: ?Overall, the system gaveme good directions.?
Furthermore notice that theother items fall into two categories: items that as-sess the quality of the instructions (Q1?Q15) anditems that assess the emotional affect of the in-teraction (Q16?Q22).
The ratings in these cate-D H M NA NM S Toverallqualityquestion-33 -18 -12 36 18 19 -25AB BC C C Cqualitymeasures(summed)-183 -148 -18 373 239 206 -44A A AB B B Bemotionalaffectmeasures(summed)-130 -103 -90 20 -5 0 -88A A A AB B B B BC C C C CFigure 5: Results for item assessing overall in-struction quality and the aggregated quality andemotional affect measures.gories can be aggregated into just two ratings bysumming over them.
Fig.
5 shows the results forthe overall question and the aggregated ratings forquality measures and emotional affect measures.The three systems with the highest task successrate get rated highest for overall instruction qual-ity.
The aggregated quality measure also singlesout the same group of three systems.4.4 Further analysisIn addition to the differences between NLG sys-tems, some other factors also influence the out-comes of our objective and subjective measures.As in GIVE-1, we find that there is a significantdifference in task success rate for different evalua-tion worlds and between users with different levelsof English proficiency.
Fig.
8 illustrates the effectof the different evaluation worlds on the task suc-cess rate for different systems, and Fig.
9 showsthe effect that a player?s English skills have on thetask success rate.
As in GIVE-1, some systemsseem to be more robust than others with respect tochanges in these factors.None of the other factors we looked at (gender,age, and computer expertise) have a significant ef-fect on the task success rate.
With a few excep-tions the other objective measures were not influ-enced by these demographic factors either.
How-ever, we do find a significant effect of age on thetime and number of actions a player needs to re-trieve the trophy: younger players are faster andneed fewer actions.
And we find that women travela significantly shorter distance than men on theirway to the trophy.
Interestingly, we do not findD H M NA NM S TQ1: The system used words and phrasesthat were easy to understand.45 26 41 62 54 58 46A A A AB B B BC C CQ2: I had to re-read instructions to under-stand what I needed to do.-26 -9 3 40 8 19 0AB B B BC C CD DQ3: The system gave me useful feedbackabout my progress.-17 -30 -31 9 11 -13 -27A AB B B BC C C CQ4: I was confused about what to do next.-35 -27 -18 29 9 5 -31AB BC C C CQ5: I was confused about which directionto go in.-32 -20 -16 21 8 3 -25A AB BC C C CQ6: I had no difficulty with identifyingthe objects the system described for me.-21 -11 -5 18 13 20 -21A A AB BC C C CQ7: The system gave me a lot of unnec-essary information.-22 -9 6 15 10 10 -6A A A AB B B BC C CD D DD H M NA NM S TQ8: The system gave me too much infor-mation all at once.-28 -8 9 31 8 21 15A A AB B B BC CQ9: The system immediately offered helpwhen I was in trouble.-15 -13 -13 32 3 -5 -23AB B B B BC C C CQ10: The system sent instructions toolate.15 15 9 38 39 14 8A AB B B B BQ11: The system?s instructions were de-livered too early.15 5 21 39 12 30 28A A AB B B BC C C CD D D DQ12: The system?s instructions were vis-ible long enough for me to read them.-67 -21 -19 6 -14 0 -18A AB B BC C C CDQ13: The system?s instructions wereclearly worded.-20 -9 1 32 23 26 6A A AB B BC C CD DQ14: The system?s instructions soundedrobotic.16 -6 8 -4 -1 5 1A A A A A AB B B B B BQ15: The system?s instructions wererepetitive.-28 -26 -11 -31 -28 -26 -23A A A A AB B B B B BFigure 7: Results for the subjective measures assessing the quality of the instructions.D H M NA NM S TQ16: I really wanted to find that trophy.-10 -13 -9 -11 -8 -7 -12A A A A A A AQ17: I lost track of time while solving theoverall task.-13 -18 -21 -16 -18 -11 -20A A A A A A AQ18: I enjoyed solving the overall task.-21 -23 -20 -8 -4 -5 -21A A A A A AB B B B BQ19: Interacting with the system was re-ally annoying.-14 -20 -12 8 -2 -2 -14A A AB B B B BC C C CQ20: I would recommend this game to afriend.-36 -39 -31 -30 -25 -24 -31A A A A A A AQ21: The system was very friendly.0 -1 5 30 20 19 5A A AB B B BC C C CD D D DQ22: I felt I could trust the system?s in-structions.-21 -6 -3 37 23 21 -13A A AB B B BFigure 6: Results for the subjective measures as-sessing the emotional affect of the instructions.Figure 8: Effect of the evaluation worlds on thesuccess rate of the NLG systems.Figure 9: Effect of the players?
English skills onthe success rate of the NLG systems.a significant effect of gender on the time playersneed to retrieve the trophy as in GIVE-1 (althoughthe mean duration is somewhat higher for femalethan for male players; 481 vs. 438 seconds).5 ConclusionIn this paper, we have described the setup and re-sults of the Second GIVE Challenge.
Altogether,we collected 1825 valid games for seven NLG sys-tems over a period of three months.
Given that thisis a 50% increase over GIVE-1, we feel that thisfurther justifies our basic experimental methodol-ogy.
As we are writing this, we are preparing de-tailed results and analyses for each participatingteam, which we hope will help them understandand improve the performance of their systems.The success rate is substantially worse in GIVE-2 than in GIVE-1.
This is probably due to theFigure 10: Points at which players lose/cancel.harder task (free movement) explained in Sec-tion 2 and to the more complex evaluation worlds(see Section 3.3).
It was our intention to makeGIVE-2 more difficult, although we did not antic-ipate such a dramatic drop in performance.
GIVE-2.5 next year will use the same task as GIVE-2 andwe hope to see an increase in task success as theparticipating research teams learn from this year?sresults.It is also noticeable that players gave mostlynegative ratings in response to statements aboutimmersion and engagement (Q16-Q20).
We dis-cussed last year how to make the task more engag-ing on the one hand and how to manage expecta-tions on the other hand, but none of the suggestedsolutions ended up being implemented.
It seemsthat we need to revisit this issue.Another indication that the task may not be ableto capture participants is that the vast majority ofcancelled and lost games end in the very begin-ning.
To analyze at what point players lose or giveup, we divide the game into phases demarcatedby manipulations of buttons that belong to the 6-button safe sequence.
Fig.
10 illustrates in whichphase of the game players lose or cancel.We are currently preparing the GIVE-2.5 Chal-lenge, which will take place in 2010-11.
GIVE-2.5will be very similar to GIVE-2, so that GIVE-2systems will be able to participate with only mi-nor changes.
In order to support the developmentof GIVE-2.5 systems, we have collected a multi-lingual corpus of written English and German in-structions in the GIVE-2 environment (Gargett etal., 2010).
We expect that GIVE-3 will then extendthe GIVE task substantially, perhaps in the direc-tion of full dialogue or of multimodal interaction.Acknowledgments.
GIVE-2 was only possiblethrough the support and hard work of a number ofcolleagues, especially Konstantina Garoufi (whohandled the website and other publicity-related is-sues), Ielka van der Sluis (who contributed to thedesign of the GIVE-2 questionnaire), and severalstudent assistants who programmed parts of theGIVE-2 system.
We thank the press offices ofSaarland University, the University of Edinburgh,and Macquarie University for their helpful pressreleases.
We also thank the organizers of Gener-ation Challenges 2010 and INLG 2010 for theirsupport and the opportunity to present our results,and the seven participating research teams for theircontributions.ReferencesAndrew Gargett, Konstantina Garoufi, AlexanderKoller, and Kristina Striegnitz.
2010.
The GIVE-2 corpus of giving instructions in virtual environ-ments.
In Proceedings of the 7th InternationalConference on Language Resources and Evaluation(LREC), Malta.S.
D. Gosling, S. Vazire, S. Srivastava, and O. P. John.2004.
Should we trust Web-based studies?
A com-parative analysis of six preconceptions about Inter-net questionnaires.
American Psychologist, 59:93?104.A.
Koller, D. Byron, J. Cassell, R. Dale, J. Moore,J.
Oberlander, and K. Striegnitz.
2009a.
The soft-ware architecture for the first challenge on generat-ing instructions in virtual environments.
In Proceed-ings of the EACL-09 Demo Session.Alexander Koller, Kristina Striegnitz, Donna Byron,Justine Cassell, Robert Dale, Sara Dalzel-Job, Jo-hanna Moore, and Jon Oberlander.
2009b.
Validat-ing the web-based evaluation of NLG systems.
InProceedings of ACL-IJCNLP 2009 (Short Papers),Singapore.Alexander Koller, Kristina Striegnitz, Donna Byron,Justine Cassell, Robert Dale, Johanna Moore, andJon Oberlander.
2010.
The first challenge ongenerating instructions in virtual environments.
InE.
Krahmer and M. Theune, editors, EmpiricalMethods in Natural Language Generation, volume5790 of LNCS, pages 337?361.
Springer.
