Proceedings of the Second ACL Workshop on Syntax and Structure in Statistical Translation (SSST-2), pages 28?36,ACL-08: HLT, Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsGeneralizing local translation modelsMichael SubotinLaboratory for Computational Linguistics and Information ProcessingDepartment of LinguisticsUniversity of MarylandCollege Park, MD 20742msubotin@umiacs.umd.eduAbstractWe investigate translation modeling based onexponential estimates which generalize essen-tial components of standard translation mod-els.
In application to a hierarchical phrase-based system the simplest generalization al-lows its models of lexical selection and re-ordering to be conditioned on arbitrary at-tributes of the source sentence and its anno-tation.
Viewing these estimates as approxi-mations of sentence-level probabilities moti-vates further elaborations that seek to exploitgeneral syntactic and morphological patterns.Dimensionality control with `1 regularizersmakes it possible to negotiate the tradeoff be-tween translation quality and decoding speed.Putting together and extending several recentadvances in phrase-based translation we ar-rive at a flexible modeling framework that al-lows efficient leveraging of monolingual re-sources and tools.
Experiments with featuresderived from the output of Chinese and Arabicparsers and an Arabic lemmatizer show signif-icant improvements over a strong baseline.1 IntroductionEffective handling of large and diverse inventoriesof feature functions is one of the most pressingopen problems in machine translation.
While min-imum error training (Och, 2003) has by now be-come a standard tool for interpolating a small num-ber of aggregate scores, it is not well suited forlearning in high-dimensional feature spaces.
At thesame time, although recent years have seen consid-erable progress in development of general methodsfor large-scale prediction of complex outputs (Bak?ret al, 2007), their application to language transla-tion has presented considerable challenges.
Sev-eral studies have shown that large-margin methodscan be adapted to the special complexities of thetask (Liang et al, 2006; Tillmann and Zhang, 2006;Cowan et al, 2006) .
However, the capacity of thesealgorithms to improve over state-of-the-art baselinesis currently limited by their lack of robust dimen-sionality reduction.
Performance gains are closelytied to the number and variety of candidate featuresthat enter into the model, and increasing the size ofthe feature space not only slows down training interms of the number of iterations required for con-vergence, but can also considerably reduce decod-ing speed, leading to run-time costs that may be un-acceptable in industrial settings.
Vector space re-gression has shown impressive performance in othertasks involving string-to-string mappings (Cortes etal., 2007), but its application to language transla-tion presents a different set of open problems (Wanget al, 2007).
Other promising formalisms, whichhave not yet produced end-to-end systems compet-itive with standard baselines, include the approachdue to Turian et al(2006), the hidden-state syn-chronous grammar-based exponential model studiedby Blunsom et al(2008), and a similar model in-corporating target-side n-gram features proposed inSubotin (2008).Taken together the results of these studies pointto a striking overarching conclusion: the humblerelative frequency estimate of phrase-based mod-els makes for a surprisingly strong baseline.
Thepresent paper investigates a family of models that28capitalize on this practical insight to allow efficientoptimization of weights for a virtually unlimitednumber of features.
We take as a point of depar-ture the observation that the essential translationmodel scores comprising standard decoding deci-sion rules can be recovered as special cases of amore general family of models.
As we discuss be-low, they are equal to maximum likelihood solu-tions for locally normalized ?piecewise?
approxi-mations to sentence-level probabilities, where wordalignment is used to determine the subset of fea-tures observed in each training example.
The casesfor which such solutions have a closed form corre-spond to particular restrictions placed on the featurespace.
Thus, relative frequency phrase models canbe obtained by limiting the feature space to indica-tor functions for the phrase pairs consistent with analignment.
By removing unnecessary restrictions werestore the full flexibility of local exponential mod-els, including their ability to use features dependingon arbitrary aspects of the source sentence and itsannotation.
The availability of robust algorithms fordimensionality reduction with `1 regularizers (Ng,2004) means that we can start with a virtually un-limited number of candidate features and negotiatethe tradeoff between translation quality and decod-ing speed in a way appropriate for a given setting.A further attractive property of locally normalizedmodels is the modest computational cost of theirtraining and ease of its parallelization.
This is par-ticularly so for the models we concentrate on in thispaper, defined so that parameter estimation decom-poses into a large number of small optimization sub-problems which can be solved independently.Several variants of these models beyond rela-tive frequencies have appeared in the literature be-fore.
Maximum entropy estimation for transla-tion of individual words dates back to Berger etal (1996), and the idea of using multi-class classi-fiers to sharpen predictions normally made throughrelative frequency estimates has been recently rein-troduced under the rubric of word sense disambigua-tion and generalized to substrings (Chan et al2007;Carpuat and Wu 2007a; Carpuat and Wu 2007b).Maximum entropy models for non-lexicalized re-ordering rules for a phrase-based system with CKYdecoding has been described by Xiong et al(2006).Some of our experiments, where exponential modelsconditioned on the source sentence and its parse an-notation are associated with all rewrite rules in a hi-erarchical phrase-based system (Chiang, 2007) andall word-level probabilities in standard lexical mod-els, may be seen as a synthesis of these ideas.The broader perspective of viewing the product ofsuch local probabilities as a particular approxima-tion of sentence-level likelihood points the way be-yond multi-class classification, and this type of gen-eralization is the main original contribution of thepresent work.
Training a classifier to predict the tar-get phrase for every source phrase is equivalent toconjoining all contextual features of the model withan indicator function for the surface form of somerule in the grammar.
We can also use features basedon less specific representation of a rule.
Of par-ticular importance for machine translations are rep-resentations which generalize reordering informa-tion beyond identity of individual words ?
a type ofgeneralization that presents a challenge in hierarchi-cal phrase-based translation.
With generalized localmodels this can be accomplished by adding featurestracking only ordering patterns of rules.
We exper-iment with a case of such models which allows usto preserve decomposition of parameter estimationinto independent subproblems.Besides varying the structure of the feature space,we can also extend the range of normalization forthe exponential models beyond target phrases co-occurring with a given source phrase in the phrasetable.
This choice is especially natural for richly in-flected languages, since it enables us to model mul-tiple levels of morphological representation at onceand estimate probabilities for rules whose surfaceforms have not been observed in training.
We applya simple variant of this approach to Arabic-Englishlexical models.Experimental results across eight test sets in twolanguage pairs support the intuition that featuresconjoined with indicator functions for surface formsof rules yield higher gains for test sets with bettercoverage in training data, while features based onless specific representations become more useful fortest sets with lower baselines.The types of features explored in this paper rep-resent only a small portion of available options, andmuch practical experimentation remains to be done,particularly in order to find the most effective ex-29tensions of the feature space beyond multiclass clas-sification.
However, the results reported here showconsiderable promise and we believe that the flexi-bility of these models combined with their computa-tional efficiency makes them potentially valuable asan extension for a variety of systems using transla-tion models with local conditional probabilities andas a feature selection method for globally trainedmodels.2 Hierarchical phrase-based translationWe take as our starting point David Chiang?s Hierosystem, which generalizes phrase-based translationto substrings with gaps (Chiang, 2007).
Considerfor instance the following set of context-free ruleswith a single non-terminal symbol:?A , A ?
?
?A1 A2 , A1 A2 ?
?A , A ?
?
?
d?
A1 ide?esA2 , A1 A2 ideas ?
?A , A ?
?
?
incolores , colorless ?
?A , A ?
?
?
vertes , green ?
?A , A ?
?
?
dormentA , sleepA ?
?A , A ?
?
?
furieusement , furiously ?It is one of many rule sets that would suffice togenerate the English translation 1b for the Frenchsentence 1a.1a.
d?
incolores ide?es vertes dorment furieusement1b.
colorless green ideas sleep furiouslyAs shown by Chiang (2007), a weighted gram-mar of this form can be collected and scored bysimple extensions of standard methods for phrase-based translation and efficiently combined with alanguage model in a CKY decoder to achieve largeimprovements over a state-of-the-art phrase-basedsystem.
The translation is chosen to be the target-side yield of the highest-scoring synchronous parseconsistent with the source sentence.
Although a va-riety of scores interpolated into the decision rule forphrase-based systems have been investigated overthe years, only a handful have been discovered to beconsistently useful, as is in our experience also thecase for the hierarchical variant.
Setting aside spe-cialized components such as number translators, weconcentrate on the essential sub-models1 comprising1To avoid confusion with features of the exponential modelsdescribed below we shall use the term ?model?
for the termsthe translation model: the phrase models and lexicalmodels.3 Local exponential translation models3.1 Relative frequency solutionsStandard phrase models associate conditional proba-bilities with subparts of translation hypotheses, usu-ally computed as relative frequencies of counts ofextracted phrases.2 Let ry be the target side of a ruleand rx its source side.
The weight of the rule in the?reverse?
phrase model would then be computed asp(ry|rx) =count(?rx, ry?)?ry?
count(?rx, ry??
)(1)When used to score a translation hypothesis cor-responding to some synchronous parse tree T , thephrase model may be conceived as an approxima-tion of the probability of a target sentence Y given asource sentence Xp(Y |X) ?
?r?Tp(ry|rx) (2)Although there is nothing in current learning the-ory that would prompt one to expect that expressionsof this form should be effective, their surprisinglystrong performance in machine translation in an em-pirical observation borne out by many studies.
Inorder to build on this practical insight it is useful togain a clearer understanding of their formal proper-ties.We start by writing out an expression for the like-lihood of training data which would give rise to max-imum likelihood solutions like those in eq.
1.
Con-sider a feature vector whose components are indi-cator functions for rules in the grammar, and letus define an exponential model for a sentence pair(Xm, Ym) of the formp (Ym|Xm) ??r?
(Xm,Ym)p(ry|rx) (3)=?r?
(Xm,Ym)exp{w ?
fr(Xm, Ym)}?r?
:rx=r?x exp{w ?
fr?
(Xm, Ym)}(4)interpolated using MERT.2Chiang (2007) uses a heuristic estimate of fractional countsin these computations.
For completeness we report both vari-ants in the experiments.30where fr(Xm, Ym) is a restriction of the featurevector such that all of its entries except for the onecorresponding to the rule r are zero and the summa-tion is over all rules in the grammar with the samesource side.
As can be verified by writing out thelikelihood for the training set and setting its gradi-ent to zero, maximum likelihood estimation basedon eq.
4 yields estimates equal to relative frequencysolutions.
In fact, because its normalization fac-tors have non-zero parameters in common only forrules which share the same source phrase, param-eter estimation decomposes into independent opti-mization subproblems, one for each source phrasein the grammar.
However, recovering relative fre-quencies of the needed form requires further atten-tion to the relationship between the definition of fea-ture functions and phrase extraction.
Computationof phrase models in machine translation crucially re-lies on a form of feature selection not widely knownin other contexts.
A rule is considered to be ob-served in a sentence pair only if it is consistent withpredictions of a word alignment model according toheuristics for alignment combination and phrase ex-traction.
The standard recipes in translation model-ing can thus be seen to include a feature selectionprocedure that applies individually to each trainingexample.3.2 Classifier solutionsWe can now generalize these relative frequencyestimates by relaxing the restrictions they implic-itly place on the form of permissible feature func-tions.
The simplest elaboration involves allow-ing indicator functions for rules to be conjoinedwith indicator functions for arbitrary attributes ofthe source sentence or its annotation.
This pre-serves a decomposition of parameter estimation ofoptimization subproblems associated with individ-ual source phrase, but effectively replaces proba-bilities p(ry|rx) in eqs.
2 and 3 with probabili-ties conditioned on the source phrase together withsome of its source-side context.
We may, for ex-ample, conjoin an indicator function for the rule?A , A ?
?
?
d?
A1 ide?esA2 , A1 A2 ideas ?
with afunction telling us whether a part-of-speech taggerhas identified the word at the left edge of the source-side gap A2 as an adjective, which would provideadditional evidence for the target side of this rule.Combining a grammar-based formalism with con-textual features raises a subtle question of whetherrules which have gaps at the edges and can match atmultiple positions of a training example should becounted as having occurred together with their re-spective contextual features once for each possiblematch.
To avoid favoring monotone rules, whichtend to match at many positions, over reorderingrules, which tend to match at a single span, we ran-domly sample only one of such multiple matches fortraining.Unlike conventional phrase models, contextually-conditioned probabilities cannot be stored in a pre-computed phrase table.
Instead, we store informa-tion about features and their weights and computethe normalization factors at run-time at the pointwhen they are first needed by the decoder.At the expense of more complicated decodingprocedures we could also apply the same line ofreasoning to generalize the ?noisy channel?
phrasemodel p(rx|ry) to be conditioned on local target-side context in a translation hypothesis, possiblycombining target-side annotation of the training setwith surface form of rules.
We do not pursue thiselaboration in part because we are skeptical about itspotential for success.
The current state of machinetranslation rarely permits constructing well-formedtranslations, so that most of the contextual featureson the target side would be rarely if at all observedin the training data, resulting in sparse and noisy es-timates.
Furthermore, we have yet to find a casewhere relative frequency estimates p(rx|ry) make auseful contribution to the system when contextually-conditioned ?reverse?
probabilities are used, sug-gesting that viewing translation modeling as approx-imating sentence-level probabilities p(Y |X) may bea more fruitful avenue in the long term.For translation with phrases without gaps classi-fier solutions of eq.
4 are equivalent to a maximumentropy variant of the phrase sense disambigua-tion approach studied by Carpuat & Wu (2007b).These solutions are also closely related to the ap-proximation known as piecewise training in graph-ical model literature (Sutton and McCallum, 2005;Sutton and Minka, 2006) and independently statedin a more general form by Pe?rez-Cruz et al(2007).Aside from formal differences between feature tem-plates defined by graphical models and grammars,31which are beyond the scope of our discussion, thereare several further contrasts between these studiesand standard practice in machine translation in howthe learned parameters are used to make predic-tions.
Unlike inference in piecewise-trained graphi-cal models, where all parameters for a given outputare added together without normalization, featuresthat enter into the score for a translation hypothe-sis are restricted to be consistent with a single syn-chronous parse and the local probabilities are nor-malized in decoding as in training.3.3 Lexical modelsThe use of conditional probabilities in standard lex-ical models also gives us a straightforward way togeneralize them in the same way as phrase models.Consider the lexical model pw(ry|rx), defined fol-lowing Koehn et al(2003), with a denoting the mostfrequent word alignment observed for the rule in thetraining set.pw(ry|rx) =n?i=11|j|(i, j) ?
a|?
(i,j)?ap(wyi |wxj )(5)We replace p(wyi |wxj ) with context-conditionedprobabilities, computed similarly to eq.
4, but at thelevel of individual words.
Our experience suggeststhat, unlike the analogous phrase model, the stan-dard lexical model pw(rx|ry) is not made redundantby this elaboration, and we use its baseline variantin all our experiments.
While this approach seeks tomake the most of practical insights underlying state-of-the-art baselines, it is of course not the only wayto combine rule-based and word-based features.
Seefor example Sutton & Minka (2006) for a discussionof alternatives that are closer in spirit to the idea ofapproximating global probabilities.3.4 Further generalizationsAn immediate practical benefit of interpreting rela-tive frequency and classifier estimates of translationmodels as special cases is the possibility of gener-alizing them further by introducing additional fea-tures based on less specific representations of rulesand words.Among the least specific and most potentially use-ful representations of hierarchical phrases are thoselimited to the patterns formed by gaps and words,allowing the model to generalize reordering infor-mation beyond individual tokens.
We study twotypes of ordering patterns.
For rules with two gapswe form features by conjoining contextual indicatorfunctions with functions indicating whether the gappattern is monotone or inverting.
We also use an-other type of ordering features, representing the pat-tern formed by gaps and contiguous subsequencesof words.
For example, the rule with the right-handside ?
d?
A1 ide?esA2 , A1 A2 ideas ?might be asso-ciated with the pattern ?
aA1 aA2 , A1 A2 a ?.
Be-cause some source-side patterns of this type applyto many different rules it is no longer possible to de-compose parameter estimation into small indepen-dent optimization subproblems.
For practical conve-nience we enforce decomposition in the experimentsreported below in the following way.
We define indi-cator functions for sequences of closed-class wordsand the most frequent part-of-speech tag for open-class words on the source side.
For the rule aboveand a simple tag-set the pattern tracked by such anindicator function would be d?
A1 N A2 .
We requireall reordering features to be conjoined with an indi-cator function of this type, ensuring that each cor-responds to a separate optimization subproblem.
Wefurther split larger optimization subproblems, so thatparameters for identical reordering features are insome cases estimated separately for different subsetsof rules.Morphological inflection provides motivation foranother class of features not bound to surface repre-sentations.
In this paper we explore a particularlysimple example of this approach, adding featuresconjoined with indicator functions for Arabic lem-mas to the lexical models in Arabic-English trans-lation.
This preserves decomposition of parameterestimation, with subproblems now associated withindividual lemmas rather than words.
Lemma-basedfeatures suggest another extension of the modelingframework.
Instead of computing the sums in nor-malization factors over all English words aligned toa given Arabic token in the training data, we let thesum range over all English words aligned to Arabicwords sharing its lemma.
This also defines probabil-ities for Arabic words whose surface forms have notbeen observed in training, although we do not takeadvantage of estimates for out-of-vocabulary words32in the experiments below.3.5 RegularizationWe apply `1 regularization (Ng, 2004; Gao et al,2007) to make learning more robust to noise andcontrol the effective dimensionality of the featurespace by subtracting a weighted sum of absolute val-ues of parameter weights from the log-likelihood ofthe training dataw?
= argmaxwLL(w) ?
?iCi|wi| (6)We optimize the objective using a variant of theorthant-wise limited-memory quasi-Newton algo-rithm proposed by Andrew & Gao (2007).3 All val-ues Ci are set to 1 in most of the experiments below,although we apply stronger regularization (Ci = 3)to reordering features.
Tuning regularization trade-offs individually for different feature types is an at-tractive option, but our experiments suggest that us-ing cross-entropy on a held-out portion of trainingdata for that purpose does not help performance.We leave investigation of the alternatives for futurework.4 Experiments4.1 Data and methodsWe apply the models to Arabic-English andChinese-English translation, with training sets con-sisting of 108,268 and 1,017,930 sentence pairs, re-spectively.4 All conditions use word alignmentsproduced by sequential iterations of IBM model 1,HMM, and IBM model 4 in GIZA++ , followed3Our implementation of the algo-rithm as a SciPy routine is available athttp://www.umiacs.umd.edu/?msubotin/owlqn.py4The Arabic-English data came from Arabic News Transla-tion Text Part 1 (LDC2004T17), Arabic English Parallel NewsText (LDC2004T18), and Arabic Treebank English Translation(LDC2005E46).
Chinese-English data came from Xinhua Chi-nese English Parallel News Text Version 1 beta (LDC2002E18),Chinese Treebank English Parallel Corpus (LDC2003E07),Chinese English News Magazine Parallel Text (LDC2005T10),FBIS Multilanguage Texts (LDC2003E14), Chinese NewsTranslation Text Part 1 (LDC2005T06), and the HKNews por-tion of Hong Kong Parallel Text (LDC2004T08).
Some sen-tence pairs were not included in the training sets due to largelength discrepancies.by ?diag-and?
symmetrization (Koehn et al, 2003).Thresholds for phrase extraction and decoder prun-ing were set to values typical for the baseline sys-tem (Chiang, 2007).
Unaligned words at the outeredges of rules or gaps were disallowed.
A trigramlanguage model with modified interpolated Kneser-Ney smoothing (Chen and Goodman, 1998) wastrained by the SRILM toolkit on the Xinhua por-tion of the Gigaword corpus and the English side ofthe parallel training set.
Evaluation was based onthe BLEU score with 95% bootstrap confidence in-tervals for the score and difference between scores,calculated by scripts in version 11a of the NIST dis-tribution.
The 2002 NIST MT evaluation sets wasused for development.
The 2003, 2004, 2005, and2006 sets were used for testing.The decision rule was based on the standard log-linear interpolation of several models, with weightstuned byMERT on the development set (Och, 2003).The baseline consisted of the language model, twophrase translation models, two lexical models, anda brevity penalty.
In the runs where generalized ex-ponential models were used they replaced both ofthe baseline phrase translation models.
The featureset used for exponential phrase models in the exper-iments included all the rules in the grammar and allaligned word pairs for lexical models.
Elementarycontextual features were based on Viterbi parses ob-tained from the Stanford parser.
Word features in-cluded identities of word unigrams and bigrams ad-jacent to a given rule, possibly including rule words.Part-of-speech features included similar ngrams upto the length of 3 and the tags for rule tokens.
Thesefeatures were collected for training by a straightfor-ward extension of rule extraction algorithms imple-mented in the baseline system for each possible lo-cation of ngrams with respect to the rule: namely, atthe outer edges of the rule and at the edges of anygaps that it has.
Our models also include a subsetof contextual features formed by pairwise combina-tions of these elementary features.
A final type ofcontextual features in these experiments was the se-quence of the highest nodes in the parse tree that fillthe span of the rule and the sequences that fill itsgaps.
We used an in-house Arabic tokenizer basedon a Java implementation of Buckwalter?s morpho-logical analyzer and incorporating simple statisticsfrom the Penn Arabic treebank, also extending it to33perform lemmatization.The total number of candidate features thus de-fined is very large, and we use a number of sim-ple heuristics to reduce it prior to training.
Theyare not essential to the estimates and were chosenso that the models could be trained in a few hourson a small cluster.
With the exception of discardingall except the 10 most frequent target phrases ob-served with each source phrase,5 which benefits per-formance, we expect that relaxing these restrictionswould improve the score.
These limitations includedcount-based thresholds on the frequency of contex-tual features included into the model, the frequencyof rules and reordering patterns conjoined with otherfeatures, and the size of optimization subproblems towhich contextual features are added.
We don?t con-join contextual features to rules whose source phraseterminals are all punctuation symbols.
For subprob-lems of size exceeding a certain threshold, we trainon a subsample of available training instances.
Forthe Chinese-English task we do not add reorder-ing features to problems with low-entropy distribu-tions of inversion and reordering patterns and dis-card rules with two non-terminals altogether if theentropy of their reordering patterns falls under athreshold.
None of these restrictions were applied tothe baselines.
Finally, we solve only those optimiza-tion subproblems which include parameters neededin the development and training sets.
This leads to areduction of costs that is similar to phrase table fil-tering and likewise does not affect the solution.
Atdecoding time all features for the translation modelsand their weights are accessed from a disk-mappedtrie.4.2 Results and discussionThe results are shown in tables 1 and 2.
For both lan-guage pairs we had a choice between using a base-line that is computed in the same way as the other ex-ponential models, with the exception of its use of rel-ative frequency estimates and a baseline that incor-porates averaged fractional counts for phrase mod-els and lexical models, as used by Chiang (2007).For the sake of completeness we report both (thoughwithout performing statistical comparisons between5This has prompted us to add an additional target-side tokento lexical models, which subsumes the discarded items under asingle category.Condition MT03 MT04 MT05 MT06Rel.
freq.
48.24 43.92 47.53 37.94Frac.
48.34 45.68 47.95 39.41Context 49.47* 45.65 48.76 39.49+lex 50.42* 46.07* 49.66* 39.32+lex+lemma 49.86* 47.02* 49.29* 40.81*Table 1: Arabic-English translation, BLEU scores ontesting.
Conditions include two baselines: simple rela-tive frequency (rel.
freq.)
and fractional estimates (frac.
).Experimental conditions: contextual features in phrasemodels (context); same and contextual features in lexi-cal models (+lex); same and lemma based features in lex-ical models (+lex+lemma).
Stars mark statistically sig-nificant improvements over the fractional baseline whichproduced a higher score on the dev-test MT02 set thanthe other baseline (59.75 vs. 59.66).Condition MT03 MT04 MT05 MT06Rel.
freq.
32.62 27.53 30.50 22.78Frac.
32.56 27.98 30.42 23.16Context 33.16* 28.35* 31.52* 23.67*+lex 33.50* 28.14* 31.98* 23.05+lex+reord 33.12* 28.27* 31.73* 23.45*Table 2: Chinese-English translation, BLEU scores ontesting.
Conditions include two baselines: simple rela-tive frequency (rel.
freq.)
and fractional estimates (frac.
).Experimental conditions: contextual features in phrasemodels (context); same and contextual features in lexi-cal models (+lex); same and reordering features in phrasemodels (+lex+reord).
Stars mark statistically significantimprovements over the simple relative frequency baselinewhich produced a higher score on the dev-test MT02 setthan the other baseline (33.62 vs. 33.53).34them).
Statistical tests for experimental conditionswere performed in comparison to the baseline whichachieved higher score on the test-dev MT02 set: thefractional count baseline for Arabic-English and thesimple relative count baseline for Chinese-English.We test models with classifier solutions for phrasemodels alone and for phrase models together withlexical models in both language pairs.
For Arabic-English translation we also experiment with addingfeatures based on lemmas to lexical models, whilefor Chinese-English we add ?reordering?
features ?features based on the ordering pattern of gaps forrules with two gaps and features based on orderingof gaps and words for rules with a single gap.For both language pairs the results show con-sistent distinctions in behavior of different mod-els between the test sets giving rise to generallyhigher scores (MT03 and MT05) and generallylower scores (MT04 and MT06).
The fractionalcounts seem to be consistently more helpful fortest sets with poorer coverage, although the rea-son for this is not immediately clear.
For exponen-tial models the two type of sets present two pos-sible sources of difference.
The lower-performingsets have poorer coverage in the training data, andthey also may suffer from lower-quality annotation,since the training sets for both the translation mod-els and the annotation tools are dominated by textin the same, newswire domain.
Overall, the use offeatures based on surface forms is more beneficialfor MT03 and MT05.
Indeed, using lexical modelswith contextual features in addition to phrase modelshurts performance on MT06 for Arabic-English andon both MT04 and MT06 for Chinese-English.
Incontrast, using features based on less specific repre-sentations is more beneficial on test sets with poorercoverage, while hurting performance on MT03 andMT05.
This agrees with our intuitions and also sug-gests that the differences in coverage of training datafor the translation models may be playing a moreimportant role in these trends than coverage for an-notation tools.5 ConclusionWe have outlined a framework for translationmodeling that synthesizes several recent advancesin phrase-based machine translation and suggestsmany other ways to leverage sub-token representa-tions of words as well as syntactic and morpholog-ical annotation tools, of which the experiments re-ported here explore only a small fraction.
Indeed,the range and practicality of the available options isperhaps its most attractive feature.
The inital resultsare promising and we are optimistic that continuedexploration of this class of models will uncover evenmore effective uses.AcknowledgmentsI would like to thank David Chiang, Chris Dyer, LiseGetoor, Kevin Gimpel, Adam Lopez, Nitin Mad-nani, Smaranda Muresan, Noah Smith, Amy Wein-berg and especially Philip Resnik for discussions re-lating to this work.
I am also grateful to David Chi-ang for sharing source code of the Hiero translationsystem and to the two anonymous reviewers for theirconstructive comments.ReferencesGalen Andrew and Jianfeng Gao.
2007.
Scalable train-ing of L1-regularized log-linear models.
In Proc.ICML 2007Go?khan H. Bak?r, Thomas Hofmann, BernhardScho?lkopf, Alexander J. Smola, Ben Taskar andS.
V. N. Vishwanathan, eds.
2007.
PredictingStructured Data.
MIT Press.Adam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing Computational Linguistics,22(1).Phil Blunsom, Trevor Cohn and Miles Osborne.
2008.Discriminative Synchronous Transduction for Statisti-cal Machine Translation In proc.
ACL 2008.Marine Carpuat and Dekai Wu.
2007a.
Improving Sta-tistical Machine Translation using Word Sense Disam-biguation In Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL 2007)..Marine Carpuat and Dekai Wu.
2007b.
How PhraseSense Disambiguation outperforms Word Sense Dis-ambiguation for Statistical Machine Translation.
In11th Conference on Theoretical and MethodologicalIssues in Machine Translation (TMI 2007).Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proc.
ACL.Stanley F. Chen and Joshua T. Goodman.
1998.
An Em-pirical Study of Smoothing Techniques for Language35Modeling.
Technical Report TR-10-98, Computer Sci-ence Group, Harvard University.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201-228.Corinna Cortes, Mehryar Mohri, and Jason Weston.2007.
A General Regression Framework for LearningString-to-String Mappings.
In Predicting StructuredData.
MIT Press.Brooke Cowan, Ivona Kucerova, and Michael Collins.2006.
A Discriminative Model for Tree-to-Tree Trans-lation.
In proceedings of EMNLP 2006.J.
Gao, G. Andrew, M. Johnson and K. Toutanova 2007.A Comparative Study of Parameter Estimation Meth-ods for Statistical Natural Language Processing.
InProc.
ACL 2007.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
Proceed-ings of the Human Language Technology Conference(HLT-NAACL 2003).P.
Liang, Alexandre Bouchard-Cote, D. Klein and B.Taskar.
2006.
An End-to-End Discriminative Ap-proach to Machine Translation.
In Association forComputational Linguistics (ACL06).A.
Y. Ng.
2004.
Feature selection, L1 vs. L2 regular-ization, and rotational invariance In Proceedings ofthe Twenty-first International Conference on MachineLearningFranz Josef Och.
2003.
Minimum Error Rate Train-ing for Statistical Machine Translation.
In ACL 2003:Proc.
of the 41st Annual Meeting of the Association forComputational Linguistics.F.
Pe?rez-Cruz, Z. Ghahramani and M. Pontil.
2007.
Ker-nel Conditional Graphical Models In Predicting Struc-tured Data.
MIT Press.Michael Subotin.
2008.
Exponential models for machinetranslation.
Generals paper, Department of Linguis-tics, University of Maryland.Charles Sutton and Andrew McCallum.
2005.
Piecewisetraining for undirected models.
In Conference on Un-certainty in Artificial Intelligence (UAI).Charles Sutton and Tom Minka.
2006.
Local Trainingand Belief Propagation.
Microsoft Research TechnicalReport TR-2006-121..Christoph Tillmann and Tong Zhang 2006.
A Dis-criminative Global Training Algorithm for StatisticalMT.
In Association for Computational Linguistics(ACL06).Joseph Turian, Benjamin Wellington, and I. DanMelamed 2006.
Scalable Discriminative Learning forNatural Language Parsing and Translation In Proceed-ings of the 20th Annual Conference on Neural Infor-mation Processing Systems (NIPS).ZhuoranWang, John Shawe-Taylor, and Sandor Szedmak2007.
Kernel Regression Based Machine Translation.In Proceedings of NAACL HLT.D.
Xiong, Q. Liu, and S. Lin.
2006.
Maximum entropybased phrase reordering model for statistical machinetranslation.
In Proceedings of the 21st internationalConference on Computational Linguistics and the 44thAnnual Meeting of the ACL.36
