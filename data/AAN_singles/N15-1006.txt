Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 53?63,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsAn Incremental Algorithm for Transition-based CCG ParsingBharat Ram Ambati1, Tejaswini Deoskar1, Mark Johnson2, Mark Steedman11ILCC, School of Informatics, University of Edinburgh2Department of Computing, Macquarie Universitybharat.ambati@ed.ac.uk, mark.johnson@mq.edu.au, {tdeoskar,steedman}@inf.ed.ac.ukAbstractIncremental parsers have potential advantagesfor applications like language modeling formachine translation and speech recognition.We describe a new algorithm for incrementaltransition-based Combinatory CategorialGrammar parsing.
As English CCGbankderivations are mostly right branching andnon-incremental, we design our algorithmbased on the dependencies resolved ratherthan the derivation.
We introduce two new ac-tions in the shift-reduce paradigm based on theidea of ?revealing?
(Pareschi and Steedman,1987) the required information during pars-ing.
On the standard CCGbank test data, ouralgorithm achieved improvements of 0.88%in labeled and 2.0% in unlabeled F-score overa greedy non-incremental shift-reduce parser.1 IntroductionCombinatory Categorial Grammar (CCG) (Steed-man, 2000) is an efficiently parseable, yet lin-guistically expressive grammar formalism.
Inaddition to predicate-argument structure, CCGelegantly captures the unbounded dependenciesfound in grammatical constructions like relativiza-tion, coordination etc.
Availability of the EnglishCCGbank (Hockenmaier and Steedman, 2007) hasenabled the creation of several robust and accuratewide-coverage CCG parsers (Hockenmaier andSteedman, 2002; Clark and Curran, 2007; Zhangand Clark, 2011).
While the majority of CCGparsers use chart-based approaches (Hockenmaierand Steedman, 2002; Clark and Curran, 2007), therehas been some work on developing shift-reduceparsers for CCG (Zhang and Clark, 2011; Xu et al,2014).
Most of these parsers model normal-formCCG derivations (Eisner, 1996), which are mostlyright-branching trees : hence are not incrementalin nature.
The dependency models of Clark andCurran (2007) and Xu et al (2014) model depen-dencies rather than derivations, but do not guaranteeincremental analyses.Besides being cognitively plausible (Marslen-Wilson, 1973), incremental parsing is more usefulthan non-incremental parsing for some applications.For example, an incremental analysis is requiredfor integrating syntactic and semantic informationinto language modeling for statistical machinetranslation (SMT) and automatic speech recognition(ASR) (Roark, 2001; Wang and Harper, 2003).This paper develops a new incremental shift-reduce algorithm for parsing CCG by building adependency graph in addition to the CCG derivationas a representation.
The dependencies in the graphare extracted from the CCG derivation.
A node canhave multiple parents, and hence we construct adependency graph rather than a tree.
Two new ac-tions are introduced in the shift-reduce paradigm for?revealing?
(Pareschi and Steedman, 1987) unbuiltstructure during parsing.
We build the dependencygraph in parallel to the incremental CCG derivationand use this graph for revealing, via these twonew actions.
On the standard CCGbank test data,our algorithm achieves improvements of 0.88% inlabeled F-score and 2.0% in unlabeled F-score overa greedy non-incremental shift-reduce algorithm.As our algorithm does not model derivations, butrather models transitions, we do not need a treebank53John likes mangoes from India madlyNP (S\NP)/NP NP (NP\NP)/NP NP (S\NP)\(S\NP)>NP\NP<NP>S\NP<S\NP<SFigure 1: Normal form CCG derivation.of incremental CCG derivations and can train on thedependencies in the existing treebank.
Our approachcan therefore be adapted to other languages withdependency treebanks, since CCG lexical categoriescan be easily extracted from dependency treebanks(Cakici, 2005; Ambati et al, 2013).The rest of the paper is arranged as follows.Section 2 gives a brief introduction to relatedwork in the areas of CCG parsing and incrementalparsing.
In section 3, we describe our incrementalshift-reduce parsing algorithm.
Details about theexperiments, evaluation metrices and analysis of theresults are in section 4.
We conclude with possiblefuture directions in section 5.2 Related WorkIn this section, we first give a brief introduction tovarious available CCG parsers.
Then we describeapproaches towards incremental and greedy parsing.2.1 CCG ParsersThere has been a significant amount of work ondeveloping chart-based parsers for CCG.
Bothgenerative (Hockenmaier and Steedman, 2002) anddiscriminative (Clark et al, 2002; Clark and Curran,2007; Auli and Lopez, 2011; Lewis and Steedman,2014) models have been developed.
As these parsersemploy a bottom-up chart-parsing strategy and usenormal-form CCGbank derivations which are right-branching, they are not incremental in nature.
In anSVO (Subject-Verb-Object) language, these parsersfirst attach the object to the verb and then the subject.Two major works in shift-reduce CCG parsingwith accuracies competitive with the widely usedClark and Curran (2007) parser (C&C) are Zhangand Clark (2011) and Xu et al (2014).
Zhang andClark (2011) used a global linear model traineddiscriminatively with the averaged perceptron(Collins, 2002) and beam search for their shift-reduce CCG parser.
Xu et al (2014) developed adependency model for shift-reduce CCG parsingusing a dynamic oracle technique.
Unlike the chartparsers, both these parsers can produce fragmentaryanalyses when a complete spanning analysis is notfound.
Both these shift-reduce parsers are moreincremental than standard chart based parsers.But, as they employ an arc-standard (Yamada andMatsumoto, 2003) shift-reduce strategy on CCG-bank, given an SVO language, these parsers are notguaranteed to attach the subject before the object.2.2 Incremental ParsersA strictly incremental parser is one which computesthe relationship between words as soon as theyare encountered in the input.
Shift-reduce CCGparsers rely either on CCGbank derivations (Zhangand Clark, 2011) which are non-incremental, oron dependencies (Xu et al, 2014) which could beincremental in simple cases, but do not guaranteeincrementality.
Hassan et al (2009) developed asemi-incremental CCG parser by transforming theEnglish CCGbank into left branching derivationtrees.
The strictly incremental version performedwith very low accuracy but a semi-incrementalversion gave a balance between incrementality andaccuracy.
There is also some work on incrementalparsing using grammar formalisms other than CCGlike phrase structure grammar (Collins and Roark,2004) and tree substitution grammar (Sangati andKeller, 2013).2.3 Greedy ParsersThere has been a significant amount of work ongreedy shift-reduce dependency parsing.
The Maltparser (Nivre et al, 2007) is one of the earliestparsers based on this paradigm.
Goldberg andNivre (2012) improved learning for greedy parsersby using dynamic oracles rather than a single statictransition sequence as the oracle.
In all the standardshift-reduce parsers, when two trees combine, onlythe top node (root) of each tree participates in theaction.
Sartorio et al (2013) introduced a techniquewhere in addition to the root node, nodes on the rightand left periphery respectively are also available forattachment in the parsing process.
A non-monotonicparsing strategy was introduced by Honnibal etal.
(2013), where an action taken during the parsingprocess is revised based on future context.54(1) S [ NPJohn(2) S [ NPJohn(S\NP)/NPlikes(3) S [ NPJohn(S\NP)/NPlikesNPmangoes(4) S [ NPJohn(S\NP)/NPlikesNPmangoes(NP\NP)/NPfrom(5) S [ NPJohn(S\NP)/NPlikesNPmangoes(NP\NP)/NPfromNPIndia(6) RR [ NPJohn(S\NP)/NPlikesNPmangoesNP\NPfrom(7) RR [ NPJohn(S\NP)/NPlikesNPmangoes(8) RR [ NPJohnS\NPlikes(9) S [ NPJohnS\NPlikes(S\NP)\(S\NP)madly(10) RR [ NPJohnS\NPlikes(11) RL [ SlikeslikesJohnmangoesfromIndiamadly(11)(10)(8)(7)(6)Figure 2: NonInc - Sequence of actions with parser configuration and the corresponding dependency graph.Though the performance of these greedy parsersis less accurate than related parsers using a beam(Zhang and Nivre, 2011), greedy parsers are inter-esting as they are very fast and are practically use-ful in large-scale applications such as parsing theweb and online machine translation or speech recog-nition.
In this work, we develop a new greedytransition-based algorithm for incremental CCGparsing, which is more incremental than Zhang andClark (2011) and Xu et al (2014) and more accu-rate than Hassan et al (2009).
Our algorithm is notstrictly incremental as we only produce derivationswhich are compatible with the Strict CompetenceHypothesis (Steedman, 2000) (details in ?3.2.3).3 AlgorithmsWe first describe the Zhang and Clark (2011) styleshift-reduce algorithm for CCG parsing.
Then weexplain our incremental algorithm based on the ?re-vealing?
technique for shift-reduce CCG parsing.3.1 Non Incremental Algorithm (NonInc)This is our baseline algorithm and is similar toZhang and Clark (2011)?s algorithm (henceforthNonInc).
It consists of an input buffer and a stackand has four major parsing actions.?
Shift - X (S) : Pushes a word from the inputbuffer to the stack and assigns a CCG categoryX.
This action performs category disambigua-tion as well, as X can be any of the categoriesassigned by a supertagger.?
Reduce Left - X (RL) : Pops the top two nodesfrom the stack, combines them into a new nodeand pushes it back onto the stack with a cate-gory X.
This corresponds to binary rules in theCCGbank (e.g.
CCG combinators like functionapplication, composition etc., and punctuationrules).
In this action the right node is the headand hence the left node is reduced.?
Reduce Right - X (RR) : This action is similarto the RL (Reduce Left -X) action, except thatin this action the right node is reduced since theleft node is the head.?
Unary - X (U) : Pops the top node from thestack, converts it into a new node with categoryX and pushes it back on the stack.
The headremains the same in this action.
This actioncorresponds to unary rules in the CCGbank(unary type-changing and type-raising rules).Figure 1 shows a normal-form CCG derivationfor an example sentence ?John likes mangoes fromIndia madly?.
Figure 2 shows the sequence of stepsusing the NonInc algorithm for parsing the sentence.For simplicity and space reasons, unary productionsleading to NP are not described.
From step 1through step 5, the first five words in the sentence(John, likes, mangoes, from, India) are shifted withcorresponding categories using shift actions (S).In step 6, (NP\NP)/NP:from and NP:Indiaare combined using the Reduce-Right (RR) actionto form NP\NP:from which is combined withNP:mangoes in step 7 to form NP:mangoes.Step 8 combines (S\NP)/NP:likes withNP:mangoes to form S\NP:likes using RR ac-tion.
Then the next word ?madly?
is shifted in step 9,which is then combined with S\NP:likes in step10.
In step 11, NP:John and S\NP:likes arecombined using Reduce-Left (RL) action leading toS:likes.
The parsing process terminates at thisstep as there are no more tokens in the input bufferand as there is only a single node left in the stack.55(1) S [ NPJohn(2) S [ NPJohn(S\NP)/NPlikes(3) RL [ S/NPlikes(4) S [ S/NPlikesNPmangoes(5) RR [ Slikes(6) S [ Slikes(NP\NP)/NPfrom(7) S [ Slikes(NP\NP)/NPfromNPIndia(8) RR [ SlikesNP\NPfrom(9) RRev [ Slikes(10) S [ Slikes(S\NP)\(S\NP)madly(11) LRev [ SlikeslikesJohnmangoesfromIndiamadly(3)(11)(5)(9)(8)Figure 3: RevInc - Sequence of actions with parser configuration and the corresponding dependency graph.We use indexed CCG categories (Clark et al,2002) and obtain the CCG dependencies after everyaction to build the dependency graph in parallelto the CCG derivation.
This is similar to Xu etal.
(2014) but differs from Zhang and Clark (2011),who extract the dependencies at the end after ob-taining a derivation for the entire sentence.
Figure2 also shows the dependency graph generated andthe arc labels give the step ID after which thedependency is generated.3.2 Revealing based Incremental Algorithm(RevInc)The NonInc algorithm described above is not incre-mental because it relies purely on the mostly right-branching CCG derivation.
In our example sentence,the verb (likes) combines with the subject (John)only at the end (step ID = 11) after all the remain-ing words in the sentence are processed, makingthe parse non-incremental.
In this section we de-scribe a new incremental algorithm based on a ?re-vealing?
technique (Pareschi and Steedman, 1987)which tries to build the most incremental derivation.3.2.1 RevealingPareschi and Steedman (1987)?s original versionof revealing was defined in terms of (implicitlyhigher-order) unification.
It was based on the fol-lowing observation.
If we think of categories asterms in a logic programming language, then whilewe usually think of CCG combinatory rules like thefollowing as applying with the two categories on theleftX/Y and Y as inputs, say instantiated as S/NPand NP , to define the category X on the right asS, in fact instantiating any two of those categoriesdefines the third.X/Y Y =?
XFor example, if we define X and X/Y as S andS/NP , we clearly define Y as NP .
They pro-posed to use unification-based revealing to recoverunbuilt constituents in from the result of overly-greedy incremental parsing.
A related second-order matching-based mechanism was used by(Kwiatkowski et al, 2010) to decompose logicalforms for semantic parser induction.The present incremental parser uses a relatedrevealing technique confined to the right periphery.Using CCG combinators and rules like type-raisingfollowed by forward composition, we combinenodes in the stack if there is a dependency betweenthem.
However, this can create problems for thenewly shifted node as its dependent might alreadyhave been reduced.
For instance, if the object?mangoes?
is reduced after it is shifted to the stack,then it won?t be available for the preposition phrase(PP) ?from India?
(of course, this goes for morecomplex NPs as well).
We have to extract ?man-goes?, which is hidden in the derivation, so as tomake the correct attachment to the PP.
This is whererevealing comes into play.
Mangoes is ?revealed?so that it is available to attach to the PP followingit, although it has already been reduced.
To handlethis, in addition to the four actions of the NonIncalgorithm, we introduce two new actions: LeftReveal (LRev) and Right Reveal (RRev).
For this,after every action, in addition to updating the stackwe also keep track of the dependencies resolvedand update the dependency graph accordingly1.
Inother words, we build the dependency graph for the1Xu et al (2014) also obtain CCG dependencies after everyaction.
But they don?t have a dependency graph which is up-dated based on the CCG derivation and used in the CCG parsing(in our case for LRev and RRev actions).56SlikesNP\NPfromR >S/NPlikesNPmangoes<NP>S(a) RRevSlikes(S\NP)\(S\NP)madlyR <NPJohnS\NPlikes<S\NP<S(b) LRevFigure 4: RRev and LRev actions.sentence in parallel to the CCG derivation.
As thesedependencies are extracted from the CCG deriva-tion, a node can have multiple parents and hence weconstruct a dependency graph rather than a tree.?
Left Reveal (LRev) : Pop the top two nodes inthe stack (left, right).
Identify the left node?schild with a subject dependency.
Abstract overthis child node and split the category of leftnode into two categories.
Combine the nodesusing CCG combinators accordingly.
VP mod-ifiers like VP coordination require this action.?
Right Reveal (RRev) : Pop the top two nodesin the stack (left, right).
Check the rightperiphery of the left node in the dependencygraph, extract all the nodes with compatibleCCG categories and identify all the possiblenodes that the right node can combine with.Abstract over this node (e.g.
object), split thecategory into two categories accordingly andcombine the nodes using CCG combinators.Constructions like NP coordination, and PPattachment require this action.3.2.2 Worked ExampleFigure 3 shows the sequence of steps for the ex-ample sentence described above.
In steps 1 and2, the first two words in the sentence: ?John?
and?likes?, are shifted from the input buffer to the stack.In addition to standard CCG combinators of appli-cation and composition, we also use type-raisingfollowed by forward composition2.
In step 3, thecategory of the left node ?John?, NP, is type-raisedto S/(S\NP) which is then combined with thecategory of right node ?likes?, (S\NP)/NP, usingforward composition operator to yield the categoryS/NP.
This step also updates the dependency graphwith an edge between ?John?
and ?likes?, where?likes?
is the parent and ?John?
is the child.
The2Type-raising followed by forward composition is treated asa single step.
Without this, after type-raising, the parser has tocheck all possible actions before applying forward composition,making it slower.next word ?mangoes?
is shifted in step 4 and com-bined with S/NP:likes in step 5 using RR actionyielding S:likes.
After this step, the dependencygraph will have ?likes?
as the root, with ?John?
and?mangoes?
as its children.
In this way, as our algo-rithm tries to be more incremental, both subject andobject arguments are resolved as soon as the corre-sponding tokens are shifted to the stack.In steps 6 and 7, the next two words ?from?and ?India?
are shifted to the stack.
Step 8 com-bines (NP\NP)/NP:from and NP:India usingRR action to form NP\NP:from.
Now, we ap-ply the RRev action in step 9 to correctly attach?from?
to ?mangoes?.
In RRev we first check theright periphery and identify a possible node to beattached, ?mangoes?, which is the object argumentof the verb ?likes?.
We abstract over this object andsplit the category in the following manner: If X isthe category of the left node and Y\Y is the cate-gory of the right node, then X is split into X/Y andY with corresponding heads.
The head of the leftnode will be the head of X/Y, and the dependencygraph helps in identifying the correct head for Y.Now, Y and Y\Y can be combined using the back-ward application rule to form Y, which can be com-bined with X/Y to form X back.
In our examplesentence, S:likes is split into S/NP:likes andNP:mangoes.
NP:mangoes is combined withNP\NP:from to form NP:mangoes, which in re-turn combines with S/NP:likes and forms backS:likes.
Figure 4(a) sketches this process.
Thisaction also updates the dependency graph with a de-pendency between ?mangoes?
and ?from?.The next word ?madly?
is shifted in step 10,after which the stack has two nodes S:likes and(S\NP)\(S\NP):madly.
We apply the LRevaction to combine these two nodes.
We abstract overthe subject of the left node, ?likes?, and split the cat-egory.
Here, S:likes is split into NP:John andS\NP:likes.
S\NP:likes is combined with(S\NP)\(S\NP):madly to form S\NP:likes,57which in return combines with NP:John and formsback S:likes.
The dependency graph is updatedwith a dependency between ?likes?
and ?madly?.Note that the final output is a standard CCG tree.Figure 4(b) shows this LRev action.3.2.3 AnalysisOur incremental algorithm uses a combinationof the CCG derivation and a dependency graphthat helps to ?reveal?
unbuilt structure in the CCGderivation by identifying heads of the revealedcategories.
For example in Figure-4a, in RRevaction, S:likes is split into S/NP:likes andNP:mangoes.
The splitting of categories is deter-ministic but the right periphery of the dependencygraph helps in identifying the head, which is ?man-goes?.
The theoretical idea of ?revealing?
is fromPareschi and Steedman (1987), but they used only atoy grammar without a model or empirical results.Checking the right periphery is similar to Sartorio etal.
(2013) and abstracting over the left or right argu-ment is similar to Dalrymple et al (1991).
Currently,we abstract only over arguments.
Adding a newaction to abstract over the verb as well will make ouralgorithm handle ellipses in the sentences like ?Johnlikes mangoes and Mary too?
similar to Dalrympleet al (1991) but we leave that for future work.Our system is monotonic in the sense that the setof dependency relationships grows monotonicallyduring the parsing process.
Our algorithm givesderivations almost as incremental as Hassan etal.
(2009) but without changing the lexical cate-gories and without backtracking.
The only changewe made to the CCGbank is making the main verbthe head of the auxiliary rather than the reverse as inCCGbank derivations.
In the right derivational treesof CCGbank, the main verb is the head for its rightside arguments and the auxiliary verb is the head forthe left side arguments in the derivation.
Not chang-ing the head rule would make our algorithm use thecostly reveal actions significantly more, which weavoid by changing the head direction.
3% of thetotal dependencies are affected by this modification.Though our algorithm can be completely incre-mental, we currently compromise incrementality inthe following cases:(a) no dependency between the nodes in the stack(b) unary type-changing and non-standard binaryrules(c) adjuncts like VP modifiers and coordinate con-structions like VP, sentential coordination.We find empirically that extending incrementalityto cover these cases actually reduces parsing perfor-mance significantly.
It also violates the Strict Com-petence Hypothesis (SCH) (Steedman, 2000), whichargues on evolutionary and developmental groundsthat the parser can only build constituents that are ty-pable by the competence grammar.
We explored theadjunct case of attaching only the preposition firstrather than creating a complete prepositional phraseand then attaching it to correct parent.
In our exam-ple sentence, this would be the case of attaching thepreposition ?from?
to its parent using RRev and thencombining the NP ?India?
accordingly as opposed tocreating the preposition phrase ?from India?
first andthen using RRev action to attach it to the correctparent.
Though the former is more incremental, itis inconsistent with the SCH.
The latter analysisis consistent with strict competence and also gavebetter parsing performance while compromising in-crementality only slightly.
The empirical impact ofthese differing degrees of incrementality on extrin-sic evaluation of our algorithm in terms of languagemodeling for SMT or ASR is left for future work.Using our incremental algorithm, we convertedthe CCGbank derivations into a sequence of shift-reduce actions.
We could convert around 98% of thederivations, which is the coverage of our algorithm,recovering around 99% dependencies.
Problematiccases are mainly the ones which involve non-standard binary rules, and punctuations with lexicalCCG categories other than ?conj?, used as a conjunc-tion, or ?,?
which is treated as a punctuation mark.4 Experiments and ResultsWe re-implemented Zhang and Clark (2011)?smodel for our experiments.
We used their globallinear model trained with the averaged perceptron(Collins, 2002).
We applied the early-update strat-egy of Collins and Roark (2004) while training.
Inthis strategy, when we don?t use a beam, decoding isstopped when the predicted action is different fromthe gold action and weights are updated accordingly.We use the feature set of Zhang and Clark (2011)(Z&C) for the NonInc algorithm.
This feature setcomprises of features over the top four nodes in the58stack and the next four words in the input buffer.Complete details of the feature set can be found intheir paper.
For our own model, RevInc, in additionto these features used for NonInc, we also providefeatures based on the right periphery of top nodein the stack.
For nodes in the right periphery, weprovide uni-gram and bi-gram features based on thenode?s CCG category.
For example, if S0 is the nodeon the top of the stack, B1 is the bottommost node inthe right periphery, and c represent the node?s CCGcategory, then B1c, and B1cS0c are the uni-gramand bi-gram features respectively.Unlike Z&C, we do not use a beam for our ex-periments, although we use a beam of 16 for com-parison of our results with their parser.
The lat-ter gives competitive results with the state-of-the-art CCG parsers.
Z&C and Xu et al (2014), useC&C?s generate script and unification mecha-nism respectively to extract dependencies for eval-uation.
C&C?s grammar doesn?t cover all the lex-ical categories and binary rules in the CCGbank.To avoid this, we adapted Hockenmaier?s scriptsused for extracting dependencies from the CCGbankderivations.
These are the two major divergences inour re-implementation from Z&C.4.1 Data and SettingsWe use standard CCGbank training (sections 02 ?21), development (section 00) and testing (section23) splits for our experiments.
All sentences inthe training set are used to train NonInc.
But forRevInc, we used 98% of the training set (the cover-age of our algorithm).
We use automatic POS-tagsand lexical CCG categories assigned using theC&C POS tagger and supertagger respectively fordevelopment and test data.
For training data, thesetags are assigned using ten-way jackknifing.
Also,for lexical CCG categories, we use a multitaggerwhich assigns k-best supertags to a word rather than1-best supertagging (Clark and Curran, 2004).
Thenumber of supertags assigned to a word dependson a ?
parameter.
Unlike Z&C, the default value of?
gave us better results rather than decreasing thevalue.
Not using a beam could be the reason for this.Following Z&C and Xu et al (2014), duringtraining, we also provide the gold CCG lexicalcategory to the list of CCG lexical categories for aword if it is not assigned by the supertagger.4.2 Connectedness and Waiting TimeBefore evaluating the performance of our algorithm,we introduce two measures of incrementality:connectedness and waiting time.
In a shift-reduceparser, a derivation is fully connected when all thenodes in the stack are connected leading to onlyone node in the stack at any point of time.
Wemeasure the average number of nodes in the stackbefore shifting a new token from input buffer tothe stack, which we call as connectedness.
Fora fully connected incremental parser like Hassanet al (2009), connectedness would be one.
Asour RevInc algorithm is not fully connected, thisnumber will be greater than one.
For example, ina noun phrase ?the big book?, when ?the?
and ?big?are in the stack, as there is no dependency betweenthese two words, our algorithm doesn?t combinethese two nodes resulting in having two nodes in thestack3.
Second column in Table 1 gives this numberfor both NonInc and RevInc algorithms.
Though ouralgorithm is not fully connected, connectedness ofour algorithm is significantly lower than the NonIncalgorithm as our algorithm is more incremental.Algorithm Connectedness Waiting TimeNonInc 4.62 2.98RevInc 2.15 0.69Table 1: Connectedness and waiting time.We define waiting time as the number of nodesthat need to be shifted to the stack before a de-pendency between any two nodes in the stack isresolved.
In our example sentence, there is a de-pendency between ?John?
and ?likes?.
For NonInc,this dependency is resolved only after all the fourremaining words in the sentence are shifted.
In otherwords, it has to wait for four more words beforethis dependency is resolved and hence the waitingtime is four.
Whereas, in our RevInc algorithm,this dependency is resolved immediately, withoutwaiting for more words to be shifted, and hencethe waiting time is zero.
The third column in Table1 gives the waiting time for both the algorithms.Since we compromised incrementality in cases likecoordination, waiting time for our RevInc algorithmis not zero but it is significantly lower than the3This is a case where the dependencies are not true to theCCG grammar, and make our algorithm less incremental thanSCH would allow.59Algorithm UP UR UF LP LR LF Cat Acc.NonInc (beam=1) 92.57 82.60 87.30 85.12 75.96 80.28 91.10RevInc (beam=1) 91.62 85.94 88.69 83.42 78.25 80.75 90.87NonInc (beam=16) 92.71 89.66 91.16 85.78 82.96 84.35 92.51Z&C (beam=16)* - - - 87.15 82.95 85.00 92.77Table 2: Performance on the development data.
*: These results are from the Z&C paper.NonInc algorithm and hence more incremental.This property is likely to be crucial for futureapplications in ASR and SMT language modeling.4.3 Results and AnalysisWe trained the perceptron for both NonInc andRevInc algorithms using the CCGbank training datafor 30 iterations, and the models which gave bestresults on development data are directly used for testdata.
Table 2 gives the unlabeled precision (UP), re-call (UR), F-score (UF) and labeled precision (LP),recall (LR), F-score (LF) results of both NonInc andRevInc approaches on the development data.
Lastcolumn in the table gives the category accuracy.
Weused the modified CCGbank for all experiments,including NonInc, for consistent comparisons.For NonInc, the modification decreased unlabeledF-score by 0.45%, without a major difference inlabeled F-score.Our incremental algorithm gives 1.39% and0.47% improvements over the NonInc algorithmin unlabeled and labeled F-scores respectively.
Forboth unlabeled and labeled scores, precision ofRevInc is slightly lower than NonInc but the recallof RevInc is much higher than NonInc resulting ina better F-score for RevInc.
As NonInc is not incre-mental and as it uses more context to the right whilemaking a decision, it makes more precise actions.But, on the other hand, if a node is reduced, it is notavailable for future actions.
This is not a problemfor our RevInc algorithm which is the reason forhigher recall.
For example, in the example sentence,?John likes mangoes from India madly?, if the object?mangoes?
is reduced after it got shifted to the stack,then in case of NonInc, the preposition phrase ?fromIndia?
can never be attached to ?mangoes?.
But,RevInc makes the correct attachment using RRevaction.
Category accuracy of NonInc is better thanRevInc, since NonInc can use more context beforetaking a complex action and is less prone to errorpropagation compared to RevInc.To compare these results in the perspective ofZ&C?s parser we also trained our NonInc parserwith a beam size of 16 similar to Z&C.
The secondlast row in Table 2 (NonInc (beam=16)) showsthese results and the last row presents the resultsfrom their paper.
Results with our implementationof Z&C are 0.65% lower than the published results,possibly due to the modification made in the headrule, and other minor differences like the supertag-ger beta value.
Unlabeled and labeled F-scores ofour RevInc parser are lower than these numbers.But, given that our RevInc parser doesn?t use anybeam, these margins are reasonable.We also analyzed the label-wise scores ofboth NonInc and RevInc.
In general, NonInc isbetter in precision and RevInc is better in recall.In the case of verbal arguments ((S\NP)/NP)and verbal modifiers ((S\NP)\(S\NP)), theF-score of RevInc is better than that of NonInc.But NonInc performed better than RevInc inthe case of preposition phrase (PP) attachments((NP\NP)/NP, ((S\NP)\(S\NP))/NP).
Morecontext is required for better PP attachment whichis provided by the fact that NonInc has a contextof several unreduced types for the model to workwith, whereas RevInc has fewer.
Whereas actionslike LRev are required to correctly attach the verbalmodifiers (?madly?)
if the subject argument (?John?
)of the verb (?likes?)
is reduced early.
Table 3 givesthe results of these CCG lexical categories.Category RevInc NonInc(NP\NP)/NP 81.36 83.21(NP\NP)/NP 78.66 82.94((S\NP)\(S\NP))/NP 65.09 66.98((S\NP)\(S\NP))/NP 62.69 65.89((S[dcl]\NP)/NP 78.96 78.29((S[dcl]\NP)/NP 76.71 75.22(S\NP)\(S\NP) 80.49 76.90Table 3: Label-wise F-score of RevInc and NonIncparsers (both with beam=1).
Argument slots in therelation are in bold.60Algorithm UP UR UF LP LR LF Cat Acc.NonInc (beam=1) 92.45 82.16 87.00 85.59 76.06 80.55 91.39RevInc (beam=1) 91.83 86.35 89.00 84.02 79.00 81.43 91.17NonInc (beam=16) 92.68 89.57 91.10 86.20 83.32 84.74 92.70Z&C (beam=16)* - - - 87.43 83.61 85.48 93.12Hassan et al 09* - - 86.31 - - - -Table 4: Performance on the test data.
*: These results are from their paper.We also analyzed the performance of the greedy(beam=1) NonInc and RevInc parsers in terms ofparsing speed (excluding pos tagger and supertag-ger time).
NonInc and RevInc parse 110 and 125sentences/second respectively.
Despite the complex-ity of the revealing actions, RevInc is faster thanthe NonInc.
Significant amount of parsing time isspent on the feature extraction step.
Features fromtop four nodes in the stack and their children are ex-tracted for both the algorithms.
Since the averageconnectedness of RevInc and NonInc are 4.62 and2.15 respectively, on average, all four nodes in thestack are processed for NonInc and only two nodesare processed for RevInc.
Because of this there issignificant reduction in the feature extraction stepfor RevInc compared to NonInc.
Also, the complexLRev and RRev actions only constituted 5% of thetotal actions in the parsing process.Table 4 presents the results of our approaches ontest data.
Our incremental algorithm, RevInc, gives2.0% and 0.88% improvements over NonInc in un-labeled and labeled F-scores respectively on the testdata.
Results of RevInc without a beam are rea-sonably close to the results of Z&C which uses abeam of 16.
We compare our results with Incre-mental+Lookahead model of Hassan et al (2009).They reported 86.31% unlabeled F-score on testdata which is 2.69% lower.
Note that these F-scores are not directly comparable since Hassanet al (2009) use simplified lexicalized CCG cate-gories.
Our evaluation is based on CCG dependen-cies which are different from dependencies in the de-pendency grammar.
Hence, we can?t directly com-pare our results with dependency parsers like Zhangand Nivre (2011) and Honnibal et al (2013).5 Conclusion and Future PlanWe have designed and implemented a new incre-mental shift-reduce algorithm based on a version ofrevealing for parsing CCG (Pareschi and Steedman,1987).
On the standard CCGbank test data, ouralgorithm achieved improvements of 0.88% and2.0% in labeled and unlabeled F-scores respectivelyover the baseline non-incremental shift-reducealgorithm.
We achieved this without changing anyCCG lexical categories and only changing a singlehead rule of making the main verb rather than theauxiliary verb the head.
Our algorithm modelstransitions rather than incremental derivations, andhence we don?t need an incremental CCGbank.
Ourapproach can therefore be adapted to languageswith dependency treebanks, since CCG lexicalcategories can be easily extracted from dependencytreebanks (Cakici, 2005; Ambati et al, 2013).
Wealso designed new measures of incrementality andshowed that our algorithm is more incremental thanthe standard shift-reduce CCG parsing algorithm.We expect to improve our current model in anumber of ways.
Providing information about lex-ical category probabilities (Auli and Lopez, 2011)assigned by the supertagger can be useful duringparsing.
We would like to explore the limited use ofa beam to handle lexical ambiguity by only keepinganalyses derived from distinct lexical categories inthe beam.
Following Xu et al (2014), we also planto explore a dynamic oracle strategy.
Ultimately,we intend to evaluate the impact of our incrementalparser extrinsically in terms of language modelingfor SMT or ASR.AcknowledgmentsWe thank Mike Lewis, Greg Coppola, FrancescoSartorio and Siva Reddy for helpful discussions.
Wealso thank the three anonymous reviewers for theiruseful suggestions.
This work was supported byERC Advanced Fellowship 249520 GRAMPLUS,EU IST Cognitive Systems IP Xperience and ARCDiscovery grant DP 110102506.61ReferencesBharat RamAmbati, Tejaswini Deoskar, andMark Steed-man.
2013.
Using CCG categories to improve Hindidependency parsing.
In Proceedings of the 51st An-nual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), pages 604?609,Sofia, Bulgaria.Michael Auli and Adam Lopez.
2011.
A Comparisonof Loopy Belief Propagation and Dual Decompositionfor Integrated CCG Supertagging and Parsing.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 470?480, Portland, Oregon, USA,June.Ruken Cakici.
2005.
Automatic induction of a CCGgrammar for Turkish.
In Proceedings of the ACL Stu-dent Research Workshop, pages 73?78, Ann Arbor,Michigan.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG parsing.In Proceedings of COLING-04, pages 282?288.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG andlog-linear models.
Computational Linguistics, 33.Stephen Clark, Julia Hockenmaier, and Mark Steedman.2002.
Building Deep Dependency Structures using aWide-Coverage CCG Parser.
In ACL, pages 327?334.Michael Collins and Brian Roark.
2004.
IncrementalParsing with the Perceptron Algorithm.
In Proceed-ings of the 42nd Meeting of the Association for Com-putational Linguistics (ACL?04), Main Volume, pages111?118, Barcelona, Spain, July.Michael Collins.
2002.
Discriminative training methodsfor hidden Markov models: theory and experimentswith perceptron algorithms.
In Proceedings of the con-ference on Empirical methods in natural language pro-cessing, EMNLP ?02, pages 1?8.Mary Dalrymple, Stuart M Shieber, and Fernando CNPereira.
1991.
Ellipsis and higher-order unification.Linguistics and philosophy, 14(4):399?452.Jason Eisner.
1996.
Efficient Normal-Form Parsing forCombinatory Categorial Grammar.
In Proceedings ofthe 34th Annual Meeting of the Association for Com-putational Linguistics, pages 79?86, Santa Cruz, Cali-fornia, USA, June.Yoav Goldberg and Joakim Nivre.
2012.
A DynamicOracle for Arc-Eager Dependency Parsing.
In Pro-ceedings of COLING 2012, pages 959?976, Mumbai,India, December.Hany Hassan, Khalil Sima?an, and Andy Way.
2009.Lexicalized Semi-Incremental Dependency Parsing.In Proceedings of the Recent Advances in NLP(RANLP?09), Borovets, Bulgaria.Julia Hockenmaier and Mark Steedman.
2002.
Gener-ative models for statistical parsing with CombinatoryCategorial Grammar.
In Proceedings of the 40th An-nual Meeting on Association for Computational Lin-guistics, ACL ?02, pages 335?342, Philadelphia, Penn-sylvania.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.Matthew Honnibal, Yoav Goldberg, and Mark John-son.
2013.
A Non-Monotonic Arc-Eager TransitionSystem for Dependency Parsing.
In Proceedings ofthe Seventeenth Conference on Computational NaturalLanguage Learning, pages 163?172, Sofia, Bulgaria,August.Tom Kwiatkowski, Luke Zettlemoyer, Sharon Goldwa-ter, and Mark Steedman.
2010.
Inducing probabilisticCCG grammars from logical form with higher-orderunification.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,pages 1223?1233, Cambridge, MA, October.Mike Lewis and Mark Steedman.
2014.
A* CCG Pars-ing with a Supertag-factored Model.
In Proceedings ofthe 2014 Conference on Empirical Methods in NaturalLanguage Processing, Doha, Qatar, October.W.
Marslen-Wilson.
1973.
Linguistic structure andspeech shadowing at very short latencies.
Nature.,244:522?533.Joakim Nivre, Johan Hall, Jens Nilsson, Atanas Chanev,G?ulsen Eryigit, Sandra K?ubler, Svetoslav Marinov,and Erwin Marsi.
2007.
Maltparser: A language-independent system for data-driven dependency pars-ing.
Natural Language Engineering, 13(2):95?135.Remo Pareschi and Mark Steedman.
1987.
A lazy wayto chart-parse with categorial grammars.
In Proceed-ings of the 25th Annual Meeting of the Associationfor Computational Linguistics, pages 81?88, Stanford,California, USA, July.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27:249?276.Federico Sangati and Frank Keller.
2013.
Incremen-tal Tree Substitution Grammar for Parsing and Sen-tence Prediction.
In Transactions of the Associationfor Computational Linguistics (TACL).Francesco Sartorio, Giorgio Satta, and Joakim Nivre.2013.
A Transition-Based Dependency Parser Using aDynamic Parsing Strategy.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics (Volume 1: Long Papers), pages 135?144,Sofia, Bulgaria, August.Mark Steedman.
2000.
The Syntactic Process.
MITPress, Cambridge, MA, USA.62Wen Wang and Mary Harper.
2003.
Language modelingusing a statistical dependency grammar parser.
In Pro-ceedings of the International Workshop on AutomaticSpeech Recognition and Understanding, US Virgin Is-lands.Wenduan Xu, Stephen Clark, and Yue Zhang.
2014.Shift-Reduce CCG Parsing with a Dependency Model.In Proceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 218?227, Baltimore, Maryland,June.Hiroyasu Yamada and Yuji Matsumoto.
2003.
StatisticalDependency Analysis with Support Vector Machines.In In Proceedings of IWPT, pages 195?206.Yue Zhang and Stephen Clark.
2011.
Shift-Reduce CCGParsing.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 683?692, Port-land, Oregon, USA, June.Yue Zhang and Joakim Nivre.
2011.
Transition-basedDependency Parsing with Rich Non-local Features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 188?193, Portland, Ore-gon, USA.63
