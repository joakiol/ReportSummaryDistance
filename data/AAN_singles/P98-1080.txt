Tagging Inflective Languages: Prediction of MorphologicalCategories for a Rich, Structured TagsetJan Haji~: and Barbora HladkfiInstitute of Formal and Applied Linguistics MFF UKCharles University, Prague, Czech Republic{hajic,hladka}~ufal.mff.cuni.czAbstrakt (~esky)(This  short abstract is in Czech.
For i l lustrationpurposes, it has been tagged by our tagger; errorsare pr inted under l ined and corrections are shown.
)Hlavnfm/AAIS7  .
.
.
.
1A- -p rob ldmem/NNIS7  .
.
.
.
.
A - -p~i/RR--6morfologickdm/AANS6 .
.
.
.
1A--zna~kov?nf/NNNS6 .
.
.
.
.
A--(/z: .
.
.
.
.
.
.
.
.
.
.n~kdy/Dbt~/Db 'zvandm/AAI_S6  .
.
.
.
IA - -mor fo log icko /A2  .
.
.
.
.
.
.
.
.
.
.- /Z :  .
.
.
.
.
.
.
.
.
.
.syntakt i ckd /AA IP1  .
.
.
.
1A--)/z: .
.
.
.
.
.
.
.
.
.
.j azyk f i /NNIP2  .
.
.
.
.
A - -s/RR--7bohatou/AAFS7 .
.
.
.
1A--f lexf /NNFS7 .
.
.
.
.
A - -, /Z:jako / J ,je /VB-S- - -3P -AA-nap~fklad/Db~egt ina /NNFS l  .
.
.
.
.
A - -nebo/ J  ~ru~t ina /NNFS 1 .
.
.
.
.
A - -, /Z:  .
.
.
.
.
.
.
.
.
.
.j e /VB-S - - -3P -AA--/Z: .
.
.
.
.
.
.
.
.
.
.p~i/P~--6 'omezend/AAFS6 .
.
.
.
1A--velikosti/NNFS2- .
.
.
.
A- -zdrojfl/NNIP2 .
.
.
.
.
A- -- /Z :-po~et/NNIS1 .
.
.
.
.
A - -mo~n~ch/AAFP2 .
.
.
.
IA - -zna~ek/NNFP2 .
.
.
.
.
A--,/Z : .
.
.
.
.
.
.
.
.
.
.kter37/P4YS1.jde /VB-S- - -3P -AA-obvykle/Dg .
.
.
.
.
.
.
1A--do/RR- -2Correct: NCorrect: NSCorrect: 6tisfc6/NNIP2 .
.
.
.
.
A--./Z: .
.
.
.
.
.
.
.
.
.
.Na~e/PSHS1-P1.metoda/NNFS1 .
.
.
.
.
A- -p~itom/Db.vyu~fv i /VB-S- - -3P-AA-exponenci i lnfho/AAIS2 .
.
.
.
1A--pravd~podobnostnfho/AAI $2.
.
.
.
1A--modelu/NNIS2 .
.
.
.
.
A- -zalo~endho/AAIS2 .
.
.
.
1A--na /P~- -6automaticky /Dg .
.
.
.
.
.
.
1A--vybran3~ch/AA_NP6 .
.
.
.
1A-- Correct: Irysech/NNIP6 .
.
.
.
.
A- -.
/Z:Parametry /NNIP l  .
.
.
.
.
A - -tohoto /PDZS2mode lu /NNIS2  .
.
.
.
.
A - -se /P7-X4po~ka j  f /VB-P - - -3P -AA-pomocf/NNFS7 .
.
.
.
.
A--  Correct: PSt--2,-jednoduch~ch/AAIP2 .
.
.
.
1A--odhad6/NNIP2 .
.
.
.
.
A--(/z: .
.
.
.
.
.
.
.
.
.
.trdnink/NNIS1 .
.
.
.
.
A- -je /VB-S- - -3P-AA-tak/Dbmnohem/Dbrychlej~f/AAES1 .
.
.
.
2A--  Correct: I, /Z:  .
.
.
.
.
.
.
.
.
.
.ne~./J,kdybychom/ J ,  -P - - -  1 .
.
.
.
.pou~i l i /VpMP- - -XR-AA-metodu/NNFS4 .
.
.
.
.
A - -max imi ln f /AAFS_4- - - -  IA - -  Correct: 2ent rop ie /NNFS2 .
.
.
.
.
A--)/z: .
.
.
.
.
.
.
.
.
.
.a/ J ' -  .
.
.
.
.
.
.
.
.
.p\[ i tom/Dbse/PT-X4.pHmo/Dg .
.
.
.
.
.
.
1A--min imal izu je /VB-S- - -3P-AA-po~et/NNIS_4- .
.
.
.
A- -chyb/NNFP2 .
.
.
.
.
A--./Z: .
.
.
.
.
.
.
.
.
.
.Correct: 1483AbstractThe major obstacle in morphological (sometimescalled morpho-syntactic, or extended POS) taggingof highly inflective languages, uch as Czech or Rus-sian, is - given the resources possibly available - thetagset size.
Typically, it is in the order of thou-sands.
Our method uses an exponential probabilis-tic model based on automatically selected features.The parameters of the model are computed usingsimple estimates (which makes training much fasterthan when one uses Maximum Entropy) to directlyminimize the error rate on training data.The results obtained so far not only show goodperformance on disambiguation of most of the indi-vidual morphological categories, but they also showa significant improvement on the overall predictionof the resulting combined tag over a HMM-based tagn-gram model, using even substantially ess trainingdata.1 I n t roduct ion1.1 Orthogonal i ty  of morphologicalcategories of inflective languagesThe major obstacle in morphological 1 tagging ofhighly inflective languages, such as Czech or Rus-sian, is - given the resources possibly available - thetagset size.
Typically, it is in the order of thou-sands.
This is due to the (partial) "orthogonality "2of simple morphological categories, which then mul-tiply when creating a "flat" list of tags.
However,the individual categories contain only a very smallnumber of different values; e.g., number has five (Sg,P1, Dual, Any, and "not applicable"), case nine etc.The "orthogonality" should not be taken to meancomplete independence, though.
Inflectional an-guages (as opposed to agglutinative languages suchas Finnish or Hungarian) typically combine severalcertain categories into one morpheme (suffix or end-ing).
At the same time, the morphemes display ahigh degree of ambiguity, even across major POScategories.For example, most of the Czech nouns can formsingular and plural forms in all seven cases, mostadjectives can (at least potentially) form all (4) gen-ders, both numbers, all (7) cases, all (3) degrees ofcomparison, and can be either of positive or nega-tive polarity.
That gives 336 possibilities (for ad-jectives), many of them homonymous on the sur-face.
On the other hand, pronouns and numerals do1 This type of tagging is sometimes called morpho-syntactictagging.
However, to stress that we are not dealing with syn-tactic categories such as Object or Attribute (but rather withmorphological categories such as Number or Case) we will usethe term "morphological" here.2By orthogonality we mean that all combinations ofvaluesof two (or more) categories are systematically possible, i.e.that every member of the cartesian product of the two (ormore) sets of values do appear in the language.not display such an orthogonality, and even adjec-tives are not fully orthogonal - an ancient "dual"number, happily living in modern Czech in the fem-inine, plural and instrumental case adds another 6sub-orthogonal possibilities to almost every adjec-tive.
Together, we employ 3127 plausible combina-tions (including style and diachronic variants).1.2 The individual categoriesThere are 13 morphological categories currently usedfor morphological tagging of Czech: part of speech,detailed POS (called "subpart of speech"), gender,number, case, possessor's gender, possessor's num-ber, person, tense, degree of comparison, negative-ness (affirmative/negative), voice (active/passive),and variant/register.The P0S category contains only the major part ofspeech values (noun (N), verb (V), adjective (A), pro-noun (P), verb (V), adjective (A), adverb (D), numeral(C), preposition (R), conjunction (J), interjection (I),particle (T), punctuation (Z), and "undefined" (X)).The "subpart of speech" (SUBPOS) contains detailsabout he major category mad has 75 different values.For example, verbs (POS: V) are divided into simplefinite form in present or future tense (B), conditional(c), infinitive (f), imperative (i), etc.
3All the categories vary in their size as well as intheir unigram entropy (see Table 1) computed usingthe standard entropy definitionHp = - ~ p(y)log(p(y)) (1)yEYwhere p is the unigram distribution estimate basedon the training data, and Y is the set of possiblevalues of the category in question.
This formula canbe rewritten as1 \[D\[Hp, t ) -  iDl~lOg(p(yi)) (21i=1where p is the unigram distribution, D is the dataand IDI its size, and yi is the value of the categoryin question at the i - th event (or position) in thedata.
The form (2) is usually used for cross-entropycomputation on data (such as test data) differentfrom those used for estimating p. The base of thelog function is always taken to be 2.1.3 The morphological  analyzerGiven the nature of inflectional languages, which cangenerate many (sometimes thousands of) forms for agiven lemma (or "dictionary entry"), it is necessaryto employ morphological nalysis before the taggingproper.
In Czech, there are as many as 5 differ-ent lemmas (not counting underlying derivations nor3The categories POS and SUBPOS are the only two categorieswhich are rather lexically (and not inflectionally) based.484Table h Most Difficult Individual MorphologicalCategoriesCategoryPOSSUBPOSGENDERNUMBERCASEPOSSGENDERPOSSNUMBERPERSONTENSEGRADENEGATIONVOICEVARNumberof values12751169535643310Unigram entropyHp (in bits)2.993.832.051.622.240.040.040.640.550.551.070.450.07word senses) and up to 108 different ags for an in-put word form.
The morphological nalyzer used forthis purpose (Hajji, in prep.
), (Haji~, 1994) coversabout 98% of running unrestricted text (newspaper,magazines, novels, etc.).
It is based on a lexiconcontaining about 228,000 lemmas and it can analyzeabout 20,000,000 word forms.2 The  Tra in ing  DataOur training data consists of about 130,000 tokensof newspaper and magazine text, manually double-tagged and then corrected by a single judge.Our training data consists of about 130,000 tokensof newspaper and magazine text, manually taggedusing a special-purpose tool which allows for easydisambiguation of morphological output.
The datahas been tagged twice, with manual resolution ofdiscrepancies (the discrepancy rate being about 5%,most of them being simple tagging errors rather thanopinion differences).One data item contains everal fields: the inputword form (token), the disambiguated tag, the set ofall possible tags for the input word form, the disam-biguated lemma, and the set of all possible lemmaswith links to their possible tags.
Out of these, weare currently interested in the form, its possible tagsand the disambiguated tag.
The lemmas are ignoredfor tagging purposes.
4The tag from the "disambiguated tag" field aswell as the tags from the "possible tags" field arefurther divided into so called subtags (by morpho-logical category).
In the set "possible tags field",4In fact, tagging helps in most cases to disambiguate helemmas.
Lemma disambiguation is a separate process follow-ing tagging.
The lemma disambiguation is a much simplerproblem - the average number of different lemmas per token(as output by the morphological nalyzer) is only 1.15.
Wedo not cover the lemma disambiguation procedure here.~--s  .
.
.
.
.
.
.
.
IR IR I - I -1461-1 -1 -1 -1 -1 - I - I - I I oaAAIS6  .
.
.
.
tA  N I A IA I IMNIS lS I - I - I - I - I  t /A / - / - / Ipoet ta , "ov&~milS6 .
.
.
.
.
A--lNINII/S12361-/-I-I-I-IAl-I-/Imodeluz: .
.
.
.
.
.
.
.
.
.
.
\[Zl : l - l - l - l - l - l - l - l - l - l - l - l \ ]  ,P4YS1 .
.
.
.
.
.
.
.
\ [P /4/ I?/S/14/ - / - / - / - / - / - / - / - / \ ]kZ,r~VpYS---IR-A A- lV /p /Y /S / - / - / - I I / P , I - /A / - / - / l s i~u lova l~IS4 .
.
.
.
.
A--\[N/N/I/S/14/-/-/-/-/-/A/-/-/\[v~rvojAANS2 .
.
.
.
IA--\[A/A/IMN/S/24/-/-/-/-/i/A/-/-/Isv~zov4hoh~NS2 .
.
.
.
.
A-- \[N/N/N/S/236/-/-/-/-/-/A/-/-/\]kllma~u\ ]~- -8  .
.
.
.
.
.
.
.
I~ IR I -1 -1461- I - I - I - I - I - I - I -311  vAAIm8 .
.
.
.
IA - - IA IA IF I~ IP1281-1 -1 -1 -111A l - l - l l P~ i~t lchIaWIP6 .
.
.
.
.
A - - IN IN IN IP lS l - l - l - l - l - lA l - l - l l dea , t i l e t l chFigure 1: Training Data: lit: on computer(adj.
)model, which was-simulating development of-worldclimate in next decadesthe ambiguity on the level of full (combined) tags ismapped onto so called "ambiguity classes" (AC-s)of subtags.
This mapping is generally not reversible,which means that the links across categories mightnot be preserved.
For example, the word form jenfor which the morphology generates three possibletags, namely, TT .
.
.
.
.
.
.
.
.
.
.
(particle "only"), andNNIS I  .
.
.
.
.
A - -  and  NNIS4  .
.
.
.
.
A - -  (noun, masc.inanimate, singular, nominative (1) or accusative(4) case; "yen" (the Japanese currency)), will beassigned six ambiguous ambiguity classes (NT, NT,- I ,  -S, -14, -h, for POS, subpart of speech, gen-der, number, case, and negation) and 7 unambiguousambiguity classes (all -).
An example of the train-ing data is presented in Fig.
1.
It contains threecolumns, separated by the vertical bar 0):1. the "truth" (the correct ag, i.e.
a sequence of13 subtags, each represented by a single charac-ter, which is the true value for each individualcategory in the order defined in Fig.
1 (lst col-umn: POS, 2nd: SUBPOS, etc.)2.
the 13-tuple of ambiguity classes, separated bya slash (/), in the same order; each ambiguityclass is named using the single character subtagsused for all the possible values of that category;3. the original word form.Please note that it is customary to number theseven grammatical cases in Czech: (instead of nam-ing them): "nominative" gets 1, "genitive" 2, etc.There are four genders, as the Czech masculine gen-der is divided into masculine animate (M) and inan-imate (I).Fig.
1 is a typical example of the ambiguities en-countered in a running text: little POS ambigu-ity, but a lot of gender, number and case ambiguity(columns 3 to 5).4853 The  Mode lInstead of employing the source-channel paradigmfor tagging (more or less explicitly present e.g.
in(Merialdo, 1992), (Church, 1988), (Hajji, Hladk~,1997)) used in the past (notwithstanding some ex-ceptions, such as Maximum Entropy and rule-basedtaggers), we are using here a "direct" approach tomodeling, for which we have chosen an exponentialprobabilistic model.
Such model (when predictingan event 5 y E Y in a context x) has the generalformPAC,e (YIX) = exp(~-~in----1 Aifi (y, x))Z(x) (3)where fi (Y, x) is the set (of size n) of binary-valued(yes/no) features of the event value being predictedand its context, hi is a "weigth" (in the exponentialsense) of the feature fi, and the normalization factorZ(x) is defined naturally asz(x) = exp( z x)) (4)yEY i----1~,Ve use a separate model for each ambiguity classAC (which actually appeared in the training data)of each of the 13 morphological c tegories 6.
Thefinal PAC (Yix) distribution is further smoothed usingunigram distributions onsubtags (again, separatelyfor each category).pAC(y\[x) = apAC,e(yIx) q- (1 -- a)PAC, I(y) (5)Such smoothing takes care of any unseen context;for ambiguity classes not seen in the training data,for which there is no model, we use unigram proba-bilities of subtags, one distribution per category.In the general case, features can operate on anyimaginable context (such as the speed of the windover Mt.
Washington, the last word of yesterdayTV news, or the absence of a noun in the next 1000words, etc.).
In practice, we view the context as aset of attribute-value pairs with a discrete range ofvalues (from now on, we will use the word "context"for such a set).
Every feature can thus be repre-sented by a set of contexts, in which it is positive.There is, of course, also a distinguished attribute forthe value of the variable being predicted (y); the restof the attributes i denoted by x as expected.
Valuesof attributes will be denoted by an overstrike (~, 5).The pool of contexts of prospective f atures is forthe purpose of morphological tagging defined as aSa subtag, i.e.
(in our case) the unique value of a morpho-logical category.6Every category is, of course, treated separately.
It meansthat e.g.
the ambiguity class 23 for category CASE (mean-ing that there is an ambiguity between genitive and dativecases) is different from ambiguity class 23 for category GRADEor PEI~0N.full cross-product of the category being predicted(y) and of the x specified as a combination of:1. an ambiguity class of a single category, whichmay be different from the category being pre-dicted, or2.
a word formand1.
the current position, or2.
immediately preceding (following) position intext, or3.
closest preceding (following) position (up tofour positions away) having a certain ambiguityclass in the POS categoryLet nowCategories = { POS, SUBPOS, GENDER,NUMBER, CASE, POSSGENDER,POSSNUMBER, PERSON, TENSE,GRADE, NEGATION, VOICE, VAR};then the feature function fcatAc,~,~(Y,X) ~ {0, 1}is well-defined iff6 CatAc (6)where Cat E Categories and CatAC is the ambi-guity class AC (such as AN, for adjective/noun am-biguity of the part of speech category) of a mor-phological category Cat (such as POS).
For exam-ple, the function fPOSaN,A,-~ is well-defined (A E{A,N}), whereas the function fCASE145,6,-?
is not(6 ?~ {1, 4, 5}).
We will introduce the notation of thecontext part in the examples of feature value com-putation below.
The indexes may be omitted if itis clear what category, ambiguity class, the value ofthe category being predicted and/or the context hefeature belongs to.The value of a well-defined feature 7 functionfca~Ac,y,~(Y, x) is determined byfCa~ac.y,~(Y, x) = 1 ~=~ = y A ?
C x.
(7)This definition excludes features which are positivefor more than one y in any context x.
This propertywill be used later in the feature selection algorithm.As an example of a feature, let's assume we arepredicting the category CASE from the ambiguityclass 145, i.e.
the morphology gives us the possibilityto assign nominative (1), accusative (4) or vocative(5) case.
A feature then is e.g.The resulting case is nominative (1) andthe following word form is pracu je  (lit.
(it) works)7From now on, we will assume that all features are well-defined.486l l l S l  .
.
.
.
1A- -  \[ A/AlIM/S/1451-/-/-I-IllAI-I-I I tvrd~'I~NIS l  .
.
.
.
.
A - - I  t~ /~ i / - I  ISl-141-1-1-21-1-1Al-I-IlbojFigure 2: Context where the featurefPOSNv,N,(POS_l=A,CASE-~=145) is positive (lit.heavy fighting).AAIS6  .
.
.
.
1A- - I  A/A/IMN/S/6/-/-/-/-/1/AI-I-/IprtdeBk6mt ro iS6  .
.
.
.
.
A - -  I t~VINolIYISI-OI-I-I-I-I-IAI-I-/II~rad6Figure 3: Context where the featurefPOSNv,N,(POS_l=A,CASE_l=145) is negative (lit.
(at the) Prague castle).denoted as  fCASE145,1,(FORM+1=pracuje), orThe resulting case is accusative (4) and theclosest preceding preposition's case has theambiguity class 46denoted as  fCASEa4s,4,(CASE-pos=R=46).The feature fPOSNv,N,(POS_l=A,CASE_l=145) willbe positive in the context of Fig.
2, but not in thecontext of Fig.
3.The full cross-product of all the possibilities out-lined above is again restricted to those featureswhich have actually appeared in the training datamore than a certain number of times.Using ambiguity classes instead of unique valuesof morphological categories for evaluating the (con-text part of the) features has the advantage of giv-ing us the possibility to avoid Viterbi search duringtagging.
This then allows to easily add lookahead(right) context.
8There is no "forced relationship" among categoriesof the same tag.
Instead, the model is allowed tolearn also from the same-position "context" of thesubtag being predicted.
However, when using themodel for tagging one can choose between two modesof operation: separate, which is the same modeused when training as described herein, and VTC(Valid Tag Combinations) method, which doesnot allow for impossible combinations of categories.See Sect.
5 for more details and for the impact onthe tagging accuracy.4 T ra in ing4.1 Feature WeightsThe usual method for computing the feature weights(the Ai parameters) is Maximum Entropy (Berger8It remains  to be seen whether  using the unique values -at least for the left context - and employing Viterbi wouldhelp.
The  results obtained so far suggest hat  probably notmuch,  and if yes, then it would restrict the number  of featuresselected rather than  increase tagging accuracy.& al., 1996).
This method is generally slow, as itrequires lot of computing power.Based on our experience with tagging as well aswith other projects involving statistical modeling,we assume that actually the weights are much lessimportant  than the features themselves.We therefore mploy very simple weight estima-tion.
It is based on the ratio of conditional proba-bility of y in the context defined by the feature fy,~and the uniform distribution for the ambiguity classAC.4.2 Feature Select ionThe usual guiding principle for selecting features ofexponential models is the Maximum Likelihood prin-ciple, i.e.
the probability of the training data is beingmaximized.
(or the cross-entropy of the model andthe training data is being minimized, which is thesame thing).
Even though we are eventually inter-ested in the final error rate of the resulting model,this might be the only solution in the usual source-channel setting where two independent models (alanguage model and a "translation" model of somesort - acoustic, real translation etc.)
are being used.The improvement of one model influences the errorrate of the combined model only indirectly.This is not the case of tagging.
Tagging can beseen as a "final application" problem for which weassume to have enough data at hand to train anduse just one model, abandoning the source-channelparadigm.
We have therefore used the error ratedirectly as the objective function which we try tominimize when selecting the model's features.
Thisidea is not new, but as far as we know it has beenimplemented in rule-based taggers and parsers, suchas (Brill, 1993a), (Brill, 1993b), (Brill, 1993c) and(Ribarov, 1996), but not in models based on proba-bility distributions.Let's define the set of contexts of a set of features:X(F) = {Z: 3~ Bf~,-~ 6 F}, (s)where F is some set of features of interest.The features can therefore be grouped togetherbased on the context hey operate on.
In the cur-rent implementation, we actually add features in"batches".
A "batch" of features i  defined as a setof features which share the same context Z (see thedefinition below).
Computationaly, adding featuresin batches is relatively cheap both time- and space-wise.For example, the featuresfPOSNv,N,(POS_I=A,CASE_I=I45)andfPOSNv,V,(POS_I=A,CASE_I=I45)487share the context (POS_I = A, CASE_, = 145).Let further?
FAC be the pool of features available for selec-tion.?
SAC be the set of features elected so far for amodel for ambiguity class AC,?
PSac (Yl d) the probability, using model (3-5)with features SAC, of subtag y in a context de-fined by position d in the training data, and?
FAC,~ be the set ("batch") of features haringthe same context ~, i.e.FAc,  = {f  FAc: : S = (9)Note that the size of AC is equal to the size ofany batch of features (\[AC\[ = \[FAc,~\[ for anyz).The selection process then proceeds as follows:1.
For all contexts ~ E X(FAc) do the following:2.
For all features f = fy,~ E FAc,5 compute theirassociated weights AI using the formula:A.~ = log(/3ac~(Y)),where= f~,~(Yd, Xd)(10)(11)3.
Compute the error rate of the training data bygoing through it and at each position d selectingthe best subtag by maximizing PSacUFAc.~(Yid)over all y E AC.4.
Select such a feature set FAC,~ which results inthe maximal improvement in the error rate ofthe training data and add all f e FAC,~ perma-nently to SAC; with SAC now extended, startfrom the beginning (unless the termination con-dition is met),5.
Termination condition: improvement in errorrate smaller than a preset minimum.The probability defined by the formula (11) caneasily be computed espite its ugly general form, asthe denominator is in fact the number of (positive)occurrences ofall the features from the batch definedby the context ~ in the training data.
It also helpsif the underlying ambiguity class AC is found onlyin a fraction of the training data, which is typicallythe case.
Also, the size of the batch (equal to \[AC\[)is usually very small.On top of rather oughly estimating the Af param-eters, we use another implementation shortcut here:we do not necessarily compute the best batch of fea-tures in each iteration, but rather add all (batchesof) features which improve the error rate by morethan a threshold 6.
This threshold is set to half thenumber of data items which contain the ambiguityclass AC at the beginning of the loop, and then is cutin half at every iteration.
The positive consequenceof this shortcut (which certainly adds some unnec-essary features) is that the number of iterations ismuch smaller than if the maximum is regularly com-puted at each iteration.5 Resu l t sWe have used 130,000 words as the training set and atest set of 1000 words.
There have been 378 differentambiguity classes (of subtags) across all categories.We have used two evaluation metrics: one whichevaluates each category separately and one "flat-list" error rate which is used for comparison withother methods which do not predict the morpho-logical categories separately.
We compare the newmethod with results obtained on Czech previously,as reported in (Hladk~, 1994) and (Hajie, Hladk~,1997).
The apparently high baseline when comparedto previously reported experiments i  undoubtedlydue to the introduction of multiple models based onambiguity classes.In all cases, since the percentage of text tokenswhich are at least wo-way ambiguous i about 55%,the error rate should be almost doubled if one wantsto know the error rate based on ambiguous wordsonly.The baseline, or "smoothing-only" error rate wasat 20.7 % in the test data and 22.18 % in the trainingdata.Table 2 presents the initial error rates for the indi-vidual categories computed using only the smooth-ing part of the model (n = 0 in equation 3).Training took slightly under 20 hours on a Linux-powered Pentium 90, with feature adding thresholdset to 4 (which means that a feature batch was notadded if it improved the absolute rror rate on train-ing data by 4 errors or less).
840 (batches) of fea-tures (which corresponds to about 2000 fully spec-ified features) have been learned.
The tagging it-self is (contrary to training) very fast.
The averagespeed is about 300 words/sec, on morphologicallyprepared ata on the same machine.
The results aresummarized in Table 3.There is no apparent overtraining yet.
However,it does appear when the threshold is lowered (wehave tested that on a smaller set of training dataconsisting of 35,000 words: overtraining started tooccur when the threshold was down to 2-3).Table 4 contains comparison of the results488CategoryPOSSUBPOSGENDERNUMBERCASEPOSSGENDERPOSSNUMBERPERSONTENSEGRADENEGATIONVOICEVAROveralltraining data test data1.101.066.355.3414.550.050.130.280.360.481.330.400.3022.182.11.16.14.214.50.00.10.00.10.31.00.10.320.7Table 2: Initial Error RateCategoryPOSSUBPOSGENDERNUMBERCASEPOSSGENDERPOSSNUMBERPERSONTENSEGRADENEGATIONVOICEVAROveralltraining data test data0.020.491.782.736.010.040.010.120.120.110.250.110.108.750.91.02.00.95.00.00.00.00.10.10.00.00.28.0Table 3: Resulting Error Rateachieved with the previous experiments on Czechtagging (Hajji, HladkA, 1997).
It shows that wegot more than 50% improvement on the best errorrate achieved so far.
Also the amount of trainingdata used was lower than needed for the HMM ex-periments.
We have also performed an experimentusing 35,000 training words which yielded by about4% worse results (88% combined tag accuracy).Finally, Table 5 compares results (given differ-ExperimentUnigram HMMRule-based (Brill's)Trigram HMMBigram HMMExponentialExponentialExponential, VTCtrainingdata size621,01537,892621,015621,01535,000130,000160,000best errorrate (in %)34.3020.2518.8618.4612.008.006.20Table 4: Comparing Various Methodsent training thresholds 9) obtained on larger train-ing data using the "separate" prediction method is-cussed so far with results obtained through a mod-ification, the key point of which is that it considersonly "Valid (sub)Tag Combinations (VTC)'.
Theprobability of a tag is computed as a simple productof subtag probabilities (normalized), thus assumingsubtag independence.
The "winner" is presented inboldface.
As expected, the overall error rate is al-ways better using the VTC method, but some of thesubtags are (sometimes) better predicted using the"separate" prediction method l?.
This could haveimportant practical consequences - if, for example,the POS or SUBPOS is all that's interesting.6 Conc lus ion  and  Fur ther  ResearchThe combined error rate results are still far belowthe results reported for English, but we believe thatthere is still room for improvement.
Moreover, split-ting the tags into subtags howed that "pure" part ofspeech (as well as the even more detailed "subpart"of speech) tagging ives actually better esults thanthose for English.We see several ways how to proceed to possiblyimprove the performance of the tagger (we are stilltalking here about the "single best tag" approach;the n-best case will be explored separately):?
Disambiguated tags (in the left context) plusViterbi search.
Some errors might be eliminatedif features asking questions about the disam-biguated context are being used.
The disam-biguated tags concentrate - or transfer - in-formation about the more distant context.
Itwould avoid "repeated" learning of the sameor similar features for different but related is-ambiguation problems.
The final effect on theoverall accuracy is yet to be seen.
Moreover,the transition function assumed by the Viterbialgorithm must be reasonably defined (approx-imated).?
Final re-estimation using maximum entropy.Let's imagine that after selecting all the featuresusing the training method described here werecompute the feature weights using the usualmaximum entropy objective function.
This willproduce better (read: more principled) weightestimates for the features already selected, butit might help as well as hurt the performance.?
Improved feature pool.
This is, according toour opinion, the source of major improvement.The error analysis hows that in many cases the9No overtraining occurred here either, but the results forthresholds 2-4 do not differ significantly.l?For English, using the Penn 23"eebank data, we have al-ways obtained better accuracy using the VTC method (andredefinition ofthe tag set based on 4 categories).489Threshold: 128 16 8 4 2Features learned: 23 213 772 1529 4571CategoryPOSSUBPOSGENDERNUMBERCASEPOSSGENDERPOSSNUMBERPERSONTENSEGRADENEGATIONVOICEVAROverallSep VTC1.50 1.321.24 1.404.50 4.063.46 2.9411.10 10.52O.08 0.100.14 0.040.28 0.180.36 0.180.88 1.000.62 0.260.38 0.180.26 0.1816.50 13.22Sep VTC0.86 0.780.78 0.843.00 2.802.62 2.407.74 7.660.08 0.120.04 0.040.14 0.160.16 0.140.70 0.300.34 0.360.16 0.140.24 0.2212.20 9.58Sep VTC0.66 0.600.70 0.642.40 2.141.86 1.725.30 5.340.08 0.040.04 0.000.16 0.100.10 0.120.44 0.300.28 0.260.10 0.120.14 0.148.42 6.98Sep VTC0.44 0.420.36 0.482.14 1.801.72 1.564.82 4.800.04 0.060.02 0.020.14 0.120.10 0.120.22 0.180.24 0.240.10 0.120.12 0.147.62 6.22Sep VTC0.36 0.440.30 0.482.08 1.901.80 1.504.88 4.840.02 0.040.00 0.000.12 0.060.I0 0.080.22 0.160.26 0.240.08 0.080.12 0.047.66 6.20Table 5: Resulting Error Rate in % (newspaper, training size: 160,000, test size: 5000 tokens)context o be used for disambiguation has notbeen used by the tagger simply because moresophisticated features have not been consideredfor selection.
An example of such a feature,which would possibly help to solve the very hardand relatively frequent problem of disambiguat-ing between ominative and accusative cases ofcertain nouns, would be a question "Is therea noun in nominative case only in the sameclause?"
- every clause may usually have onlyone noun phrase in nominative, constituting itssubject.
For such feature to work we will haveto correctly determine or at least approximatethe clause boundaries, which is obviously a non-trivial task by itself.7 AcknowledgementsVarious parts of this work has been supported bythe following grants: Open Foundation RSS/HESP195/1995, Grant Agency of the Czech Republic(GA(~R) 405/96/K214, and Ministry of EducationProject No.
VS96151.
The authors would also liketo thank Fred Jelinek of CLSP JHU Baltimore forvaluable comments and suggestions which helped toimprove this paper a lot.ReferencesAdam Berger, Stephen Della Pietra, Vincent DellaPietra.
1996.
Maximum Entropy Approach.
InComputational Linguistics, vol.
3, MIT Press,Cambridge, MA.Eric Brill.
1993a.
A Corpus Based Approach ToLanguage Learning.
PhD Dissertation, Depart-ment of Computer and Information Science, Uni-versity of Pennsylvania.Eric Brill.
1993b.
Automatic grammar induc-tion and parsing free text: A Transformation?Based Approach.
In: Proceedings of the 3rd In-ternational Workshop on Parsing Technologies,Tilburg, The Netherlands.Eric Brill.
1993c.
Transformation-Based Error-Driven Parsing.
In: Proceedings of the TwelfthNational Conference on Artificial Intelligence.Kenneth W. Church.
1988.
A stochastic parts pro-gram and noun phrase parser for unrestrictedtext.
In Proceedings of the Second Conferenceon Applied Natural Language Processing, pages136-143, Austin, Texas.
Association for Compu-tational Linguistics, Morristown, New Jersey.Jan Haji~.
1994.
Unification Morphology Grammar.PhD Dissertation.
MFF UK, Charles University,Prague.Jan Haji~.
In prep.
Automatic Processing of Czech:between Morphology and Syntax.
MFF UK,Charles University, Prague.Jan Hajji, Barbora Hladk& 1997.
Tagging of Inflec-tive Languages: a Comparison.
In Proceedings ofthe ANLP'97, pages 136-143, Washington, DC.Association for Computational Linguistics, Mor-ristown, New Jersey.Barbora Hladk& 1994.
Programov6 vybavenf prozpracov~ni velk~ch ~esk~ch textov~ch korpusfi.MSc Thesis, Institute of Formal and Applied Lin-guistics, Charles University, Prague, Czech Re-public.Bernard Merialdo.
1992.
Tagging Text With AProbabilistic Model.
Computational Linguistics,20(2):155-171Kiril Ribarov.
1996.
Automatick~.
tvorba gramatikyp~irozen6ho jazyka.
MSc Thesis, Institute of For-mal and Applied Linguistics, Charles University,Prague, Czech Republic.
In Czech.490
