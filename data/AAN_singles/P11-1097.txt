Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 965?975,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsPiggyback: Using Search Engines for Robust Cross-DomainNamed Entity RecognitionStefan Ru?dInstitute for NLPUniversity of StuttgartGermanyMassimiliano CiaramitaGoogle ResearchZu?richSwitzerlandJens Mu?ller and Hinrich Schu?tzeInstitute for NLPUniversity of StuttgartGermanyAbstractWe use search engine results to address a par-ticularly difficult cross-domain language pro-cessing task, the adaptation of named entityrecognition (NER) from news text to webqueries.
The key novelty of the method is thatwe submit a token with context to a searchengine and use similar contexts in the searchresults as additional information for correctlyclassifying the token.
We achieve strong gainsin NER performance on news, in-domain andout-of-domain, and on web queries.1 IntroductionAs statistical Natural Language Processing (NLP)matures, NLP components are increasingly used inreal-world applications.
In many cases, this meansthat some form of cross-domain adaptation is neces-sary because there are distributional differences be-tween the labeled training set that is available andthe real-world data in the application.
To addressthis problem, we propose a new type of featuresfor NLP data, features extracted from search en-gine results.
Our motivation is that search engineresults can be viewed as a substitute for the worldknowledge that is required in NLP tasks, but that canonly be extracted from a standard training set or pre-compiled resources to a limited extent.
For example,a named entity (NE) recognizer trained on news textmay tag the NE London in an out-of-domain webquery like London Klondike gold rush as a location.But if we train the recognizer on features derivedfrom search results for the sentence to be tagged,correct classification as person is possible.
This isbecause the search results for London Klondike goldrush contain snippets in which the first name Jackprecedes London; this is a sure indicator of a lastname and hence an NE of type person.We call our approach piggyback and search result-derived features piggyback features because we pig-gyback on a search engine like Google for solving adifficult NLP task.In this paper, we use piggyback features to ad-dress a particularly hard cross-domain problem, theapplication of an NER system trained on news toweb queries.
This problem is hard for two reasons.First, the most reliable cue for NEs in English, asin many languages, is capitalization.
But queriesare generally lowercase and even if uppercase char-acters are used, they are not consistent enough tobe reliable features.
Thus, applying NER systemstrained on news to web queries requires a robustcross-domain approach.News to queries adaptation is also hard becausequeries provide limited context for NEs.
In newstext, the first mention of a word like Ford is oftena fully qualified, unambiguous name like Ford Mo-tor Corporation or Gerald Ford.
In a short querylike buy ford or ford pardon, there is much less con-text than in news.
The lack of context and capitaliza-tion, and the noisiness of real-world web queries (to-kenization irregularities and misspellings) all makeNER hard.
The low annotator agreement we foundfor queries (Section 5) also confirms this point.The correct identification of NEs in web queriescan be crucial for providing relevant pages and adsto users.
Other domains have characteristics sim-ilar to web queries, e.g., automatically transcribedspeech, social communities like Twitter, and SMS.Thus, NER for short, noisy text fragments, in theabsence of capitalization, is of general importance.965NER performance is to a large extent determinedby the quality of the feature representation.
Lexical,part-of-speech (PoS), shape and gazetteer featuresare standard.
While the impact of different types offeatures is well understood for standard NER, fun-damentally different types of features can be usedwhen leveraging search engine results.
Returning tothe NE London in the query London Klondike goldrush, the feature ?proportion of search engine resultsin which a first name precedes the token of interest?is likely to be useful in NER.
Since using search en-gine results for cross-domain robustness is a new ap-proach in NLP, the design of appropriate features iscrucial to its success.
A significant part of this paperis devoted to feature design and evaluation.This paper is organized as follows.
Section 2 dis-cusses related work.
We describe standard NER fea-tures in Section 3.
One main contribution of thispaper is the large array of piggyback features thatwe propose in Section 4.
We describe the data setswe use and our experimental setup in Sections 5?6.The results in Section 7 show that piggyback fea-tures significantly increase NER performance.
Thisis the second main contribution of the paper.
We dis-cuss challenges of using piggyback features ?
due tothe cost of querying search engines ?
and present ourconclusions and future work in Section 8.2 Related workBarr et al (2008) found that capitalization of NEs inweb queries is inconsistent and not a reliable cue forNER.
Guo et al (2009) exploit query logs for NERin queries.
This is also promising, but the contextin search results is richer and potentially more infor-mative than that of other queries in logs.The insight that search results provide useful ad-ditional context for natural language expressions isnot new.
Perhaps the oldest and best known applica-tion is pseudo-relevance feedback which uses wordsand phrases from search results for query expansion(Rocchio, 1971; Xu and Croft, 1996).
Search countsor search results have also been used for sentimentanalysis (Turney, 2002), for transliteration (Grefen-stette et al, 2004), candidate selection in machinetranslation (Lapata and Keller, 2005), text similar-ity measurements (Sahami and Heilman, 2006), in-correct parse tree filtering (Yates et al, 2006), andparaphrase evaluation (Fujita and Sato, 2008).
Thespecific NER application we address is most similarto the work of Farkas et al (2007), but they mainlyused frequency statistics as opposed to what we viewas the main strength of search results: the ability toget additional contextually similar uses of the tokenthat is to be classified.Lawson et al (2010), Finin et al (2010), andYetisgen-Yildiz et al (2010) investigate how to bestuse Amazon Mechanical Turk (AMT) for NER.
Weuse AMT as a tool, but it is not our focus.NLP settings where training and test sets are fromdifferent domains have received considerable atten-tion in recent years.
These settings are difficult be-cause many machine learning approaches assumethat source and target are drawn from the same dis-tribution; this is not the case if they are from differ-ent domains.
Systems applied out of domain typi-cally incur severe losses in accuracy; e.g., Poibeauand Kosseim (2000) showed that newswire-trainedNER systems perform poorly when applied to emaildata (a drop of F1 from .9 to .5).
Recent work in ma-chine learning has made substantial progress in un-derstanding how cross-domain features can be usedin effective ways (Ben-David et al, 2010).
The de-velopment of such features however is to a large ex-tent an empirical problem.
From this perspective,one of the most successful approaches to adaptationfor NER is based on generating shared feature rep-resentations between source and target domains, viaunsupervised methods (Ando, 2004; Turian et al,2010).
Turian et al (2010) show that adapting fromCoNLL to MUC-7 (Chinchor, 1998) data (thus be-tween different newswire sources), the best unsuper-vised feature (Brown clusters) improves F1 from .68to .79.
Our approach fits within this line of workin that it empirically investigates features with goodcross-domain generalization properties.
The maincontribution of this paper is the design and evalu-ation of a novel family of features extracted fromthe largest and most up-to-date repository of worldknowledge, the web.Another source of world knowledge for NER isWikipedia: Kazama and Torisawa (2007) show thatpseudocategories extracted from Wikipedia help forin-domain NER.
Cucerzan (2007) uses Wikipediaand web search frequencies to improve NE disam-biguation, including simple web search frequencies966BASE: lexical and input-text part-of-speech features1 WORD(k,i) binary: wk = wi2 POS(k,t) binary: wk has part-of-speech t3 SHAPE(k,i) binary: wk has (regular expression) shape regexpi4 PREFIX(j) binary: w0 has prefix j (analogously for suffixes)GAZ: gazetteer features5 GAZ-Bl(k,i) binary: wk is the initial word of a phrase, consisting of l words, whose gaz.
category is i6 GAZ-Il(k,i) binary: wk is a non-initial word in a phrase, consisting of l words, whose gaz.
category is iURL: URL features7 URL-SUBPART N(w0 is substring of a URL)/N(URL)8 URL-MI(PER) 1/N(URL-parts)?
[[p?URL-parts]] 3MIu(p, PER)?MIu(p,O)?MIu(p,ORG)?MIu(p,LOC)LEX: local lexical features9 NEIGHBOR(k) 1/N(k-neighbors)?
[[v?k-neighbors]] log[NE-BNC(v, k)/OTHER-BNC(v, k)]10 LEX-MI(PER,d) 1/N(d-words)?
[[v?d-words]] 3MId(v, PER)?MId(v,O)?MId(v,ORG)?MId(v,LOC)BOW: bag-of-word features11 BOW-MI(PER) 1/N(bow-words)?
[[v?bow-words]] 3MIb(v, PER)?MIb(v,O)?MIb(v,ORG)?MIb(v,LOC)MISC: shape, search part-of-speech, and title features12 UPPERCASE N(s0 is uppercase)/N(s0)13 ALLCAPS N(s0 is all-caps)/N(s0)14 SPECIAL binary: w0 contains special character15 SPECIAL-TITLE N(s?1 or s1 in title contains special character)/(N(s?1)+N(s1))16 TITLE-WORD N(s0 occurs in title)/N(title)17 NOMINAL-POS N(s0 is tagged with nominal PoS)/N(s0)18 CONTEXT(k) N(sk is typical neighbor at position k of named entity)/N(s0)19 PHRASE-HIT(k) N(wk = sk, i.e., word at position k occurs in snippet)/N(s0)20 ACRONYM N(w?1w0 or w0w1 or w?1w0w1 occur as acronym)/N(s0)21 EMPTY binary: search result is emptyTable 1: NER features used in this paper.
BASE and GAZ are standard features.
URL, LEX, BOW and MISC arepiggyback (search engine-based) features.
See text for explanation of notation.
The definitions of URL-MI, LEX-MI,and BOW-MI for LOC, ORG and O are analogous to those for PER.
For better readability, we write?
[[x]] for?x.for compound entities.3 Standard NER featuresAs is standard in supervised NER, we train an NEtagger on a dataset where each token is representedas a feature vector.
In this and the following sectionwe present the features used in our study divided ingroups.
We will refer to the target token ?
the to-ken we define the feature vector for ?
as w0.
Its leftneighbor is w?1 and its right neighbor w1.
Table 1provides a summary of all features.Feature group BASE.
The first class of fea-tures, BASE, is standard in NER.
The binary fea-ture WORD(k,i) (line 1) is 1 iff wi, the ith word inthe dictionary, occurs at position k with respect tow0.
The dictionary consists of all words in the train-ing set.
The analogous feature for part of speech,POS(k,t) (line 2), is 1 iff wk has been tagged withPoS t, as determined by TnT tagger (Brants, 2000).We also encode surface properties of the word withsimple regular expressions, e.g., x-ray is encoded asx-x and 9/11 as d/dd (SHAPE, line 3).
For these fea-tures, k ?
{?1, 0, 1}.
Finally, we encode prefixesand suffixes, up to three characters long, for w0 (line4).Feature group GAZ.
Gazetteer features (lines 5& 6) are an efficient and effective way of buildingworld knowledge into an NER model.
A gazetteeris simply a list of phrases that belong to a par-ticular semantic category.
We use gazetteers from(i) GATE (Cunningham et al, 2002): countries,first/last names, trigger words; (ii) WordNet: the46 lexicographical labels (food, location, personetc.
); and (iii) Fortune 500: company names.
Thetwo gazetteer features are the binary features GAZ-Bl(k,i) and GAZ-Il(k,i).
GAZ-Bl (resp.
GAZ-Il) is 1967iff wk occurs as the first (resp.
non-initial or internal)word in a phrase of length l that the gazetteer lists asbelonging to category i where k ?
{?1, 0, 1}.4 Piggyback featuresFeature groups URL, LEX, BOW, and MISC arepiggyback features.
We produce these by segment-ing the input text into overlapping trigrams w1w2w3,w2w3w4, w3w4w5 etc.
Each trigram wi?1wiwi+1is submitted as a query to the search engine.
Forall experiments we used the publicly accessibleGoogle Web Search API.1 The search engine returnsa search result for the query consisting of, in mostcases, 10 snippets,2 each of which contains 0, 1 ormore hits of the search term wi.
We then computefeatures for the vector representation of wi based onthe snippets.
We again refer to the target token andits neighbors (i.e., the search string) as w?1w0w1.w0 is the token that is to be classified (PER, LOC,ORG, or O) and the previous word and the next wordserve as context that the search engine can exploit toprovide snippets in which w0 is used in the same NEcategory as in the input text.
O is the tag of a tokenthat is neither LOC, ORG nor PER.In the definition of the features, we refer to theword in the snippet that matches w0 as s0, wherethe match is determined based on edit distance.
Theword immediately to the left (resp.
right) of s0 in asnippet is called s?1 (resp.
s1).For non-binary features, we first calculate realvalues and then binarize them into 10 quantile bins.Feature group URL.
This group exploits NEinformation in URLs.
The feature URL-SUBPART(line 7) is the fraction of URLs in the search re-sult containing w0 as a substring.
To avoid spuriousmatches, we set the feature to 0 if length(w0) ?
2.For URL-MI (line 8), each URL in the search re-sult is split on special characters into parts (e.g., do-main and subdomains).
We refer to the set of allparts in the search result as URL-parts.
The valueof MIu(p,PER) is computed on the search results ofthe training set as the mutual information (MI) be-tween (i) w0 being PER and (ii) p occurring as partof a URL in the search result.
MI is defined as fol-1Now deprecated in favor of the new Custom Search API.2Less than 0.5% of the queries return fewer than 10 snippets.lows:MI(p,PER) =?i?{p?,p}?j?
{ ?PER,PER}P (i, j) log P (i, j)P (i)P (j)For example, for the URL-part p = ?staff?
(e.g.,in bigcorp.com/staff.htm), P (staff) is theproportion of search results that contain a URLwith the part ?staff?, P (PER) is the proportion ofsearch results where the search token w0 is PERand P (staff,PER) is the proportion of search resultswhere w0 is PER and one of the URLs returned bythe search engine has part ?staff?.The value of the feature URL-MI is the averagedifference between the MI of PER and the othernamed entities.
The feature is calculated in the sameway for LOC, ORG, and O.Our initial experiments that used binary featuresfor URL parts were not successful.
We then de-signed URL-MI to integrate all URL informationspecific to an NE class into one measurement ina way that gives higher weight to strong featuresand lower weight to weak features.
The innersum on line 8 is the sum of the three differencesMI(PER) ?
MI(O), MI(PER) ?
MI(ORG), andMI(PER)?MI(LOC).
Each of the three summandsindicates the relative advantage a URL part p givesto PER vs O (or ORG and LOC).
By averaging overall URL parts, one then obtains an assessment of theoverall strength of evidence (in terms of MI) for theNE class in question.Feature group LEX.
These features assess howappropriate the words occurring in w0?s local con-texts in the search result are for an NE class.For NEIGHBOR (line 9), we calculate for eachword v in the British National Corpus (BNC) thecount NE-BNC(v, k), the number of times it oc-curs at position k with respect to an NE; andOTHER-BNC(v, k), the number of times it occursat position k with respect to a non-NE.
We instan-tiate the feature for k = ?1 (left neighbor) andk = 1 (right neighbor).
The value of NEIGHBOR(k)is defined as the average log ratio of NE-BNC(v, k)and OTHER-BNC(v, k), averaged over the set k-neighbors, the set of words that occur at position kwith respect to s0 in the search result.In the experiments reported in this paper, we usea PoS-tagged version of the BNC, a balanced cor-pus of 100M words of British English, as a model968of word distribution in general contexts and in NEcontexts that is not specific to either target or sourcedomain.
In the BNC, NEs are tagged with just onePoS-tag, but there is no differentiation into subcat-egories.
Note that the search engine could be usedagain for this purpose; for practical reasons we pre-ferred a static resource for this first study wheremany design variants were explored.The feature LEX-MI interprets words occurringbefore or after s0 as indicators of named entitihood.The parameter d indicates the ?direction?
of the fea-ture: before or after.
MId(v,PER) is computed onthe search results of the training set as the MI be-tween (i) w0 being PER and (ii) v occurring close tos0 in the search result either to the left (d = ?1) orto the right (d = 1) of s0.
Close refers to a windowof 2 words.
The value of LEX-MI(PER,d) is thenthe average difference between the MI of PER andthe other NEs.
The definition for LEX-MI(PER,d)is given on line 10.
The feature is calculated in thesame way for LOC, ORG, and O.Feature group BOW.
The features LEX-MI con-sider a small window for cooccurrence informationand distinguish left and right context.
For BOW fea-tures, we use a larger window and ignore direction.Our aim is to build a bag-of-words representation ofthe contexts of w0 in the result snippets.MIb(v,PER) is computed on the search resultsof the training set as the MI between (i) w0 beingPER and (ii) v occurring anywhere in the search re-sult.
The value of BOW-MI(PER) is the average dif-ference between the MI of PER and the other NEs(line 11).
The average is computed over all wordsv ?
bow-words that occur in a particular search re-sult.
The feature is calculated in the same way forLOC, ORG, and O.Feature group MISC.
We collect the remainingpiggyback features in the group MISC.The UPPERCASE and ALLCAPS features (lines12&13) compute the fraction of occurrences of w0in the search result with capitalization of only thefirst letter and all letters, respectively.
We excludetitles: capitalization in titles is not a consistent cluefor NE status.The SPECIAL feature (line 14) returns 1 iff anycharacter of w0 is a number or a special character.NEs are often surrounded by special characters inweb pages, e.g., Janis Joplin - Summertime.
TheSPECIAL-TITLE feature (line 15) captures this bycounting the occurrences of numbers and specialcharacters in s?1 and s1 in titles of the search result.The TITLE-WORD feature (line 16) computes thefraction of occurrences of w0 in the titles of thesearch result.The NOMINAL-POS feature (line 17) calculatesthe proportion of nominal PoS tags (NN, NNS, NP,NPS) of s0 in the search result, as determined bya PoS tagging of the snippets using TreeTagger(Schmid, 1994).The basic idea behind the CONTEXT(k) feature(line 18) is that the occurrence of words of certainshapes and with certain parts of speech makes it ei-ther more or less likely that w0 is an NE.
For k = ?1(the word preceding s0 in the search result), we testfor words that are adjectives, indefinites, posses-sive pronouns or numerals (partly based on tagging,partly based on a manually compiled list of words).For k = 1 (the word following s0), we test for wordsthat contain numbers and special characters.
Thisfeature is complementary to the feature group LEXin that it is based on shape and PoS and does notestimate different parameters for each word.The feature PHRASE-HIT(?1) (line 19) calculatesthe proportion of occurrences of w0 in the search re-sult where the left neighbor in the snippet is equalto the word preceding w0 in the search string, i.e.,k = ?1: s?1 = w?1.
PHRASE-HIT(1) is theequivalent for the right neighbor.
This feature helpsidentify phrases ?
search strings containing NEs aremore likely to occur as a phrase in search results.The ACRONYM feature (line 20) computes theproportion of the initials of w?1w0 or w0w1 orw?1w0w1 occurring in the search result.
For ex-ample, the abbreviation GM is likely to occur whensearching for general motors dealers.The binary feature EMPTY (line 21) returns 1 iffthe search result is empty.
This feature enables theclassifier to distinguish true zero values (e.g., for thefeature ALLCAPS) from values that are zero becausethe search engine found no hits.5 Experimental dataIn our experiments, we train an NER classifier on anin-domain data set and test it on two different out-of-domain data sets.
We describe these data sets in969CoNLL trn CoNLL tst IEER KDD-D KDD-TLOC 4.1 4.1 1.9 11.9 10.6ORG 4.9 3.7 3.2 8.2 8.3PER 5.4 6.4 3.8 5.3 5.4O 85.6 85.8 91.1 74.6 75.7Table 2: Percentages of NEs in CoNLL, IEER, and KDD.this section and the NER classifier and the details ofthe training regime in the next section, Section 6.As training data for all models evaluated we usedthe CoNLL 2003 English NER dataset, a corpusof approximately 300,000 tokens of Reuters newsfrom 1992 annotated with person, location, organi-zation and miscellaneous NE labels (Sang and Meul-der, 2003).
As out-of-domain newswire evaluationdata3 we use the development test data from theNIST 1999 IEER named entity corpus, a dataset of50,000 tokens of New York Times (NYT) and Asso-ciated Press Weekly news.4 This corpus is annotatedwith person, location, organization, cardinal, dura-tion, measure, and date labels.
CoNLL and IEERare professionally edited and, in particular, properlycapitalized news corpora.
As capitalization is ab-sent from queries we lowercased both CoNLL andIEER.
We also reannotated the lowercased datasetswith PoS categories using the retrained TnT PoS tag-ger (Brants, 2000) to avoid using non-plausible PoSinformation.
Notice that this step is necessary asotherwise virtually no NNP/NNPS categories wouldbe predicted on the query data because the lower-case NEs of web queries never occur in properlycapitalized news; this causes an NER tagger trainedon standard PoS to underpredict NEs (1?3% positiverate).The 2005 KDD Cup is a query topic categoriza-tion task based on 800,000 queries (Li et al, 2005).5We use a random subset of 2000 queries as a sourceof web queries.
By means of simple regular ex-pressions we excluded from sampling queries thatlooked like urls or emails (?
15%) as they are easyto identify and do not provide a significant chal-3A reviewer points out that we use the terms in-domainand out-of-domain somewhat liberally.
We simply use ?differ-ent domain?
as a short-hand for ?different distribution?
withoutmaking any claim about the exact nature of the difference.4nltk.googlecode.com/svn/trunk/nltk data5www.sigkdd.org/kdd2005/kddcup.htmllenge.
We also excluded queries shorter than 10characters (4%) and longer than 50 characters (2%)to provide annotators with enough context, but notan overly complex task.
The annotation procedurewas carried out using Amazon Mechanical Turk.
Weinstructed workers to follow the CoNLL 2003 NERguidelines (augmented with several examples fromqueries that we annotated) and identify up to threeNEs in a short text and copy and paste them into abox with associated multiple choice menu with the4 CoNLL NE labels: LOC, MISC, ORG, and PER.Five workers annotated each query.
In a first roundwe produced 1000 queries later used for develop-ment.
We call this set KDD-D. We then expandedthe guidelines with a few uncertain cases.
In a sec-ond round, we generated another 1000 queries.
Thisset will be referred to as KDD-T. Because annota-tor agreement is low on a per-token basis (?
= .30for KDD-D, ?
= .34 for KDD-T (Cohen, 1960)),we remove queries with less than 50% agreement,averaged over the tokens in the query.
After thisfiltering, KDD-D and KDD-T contain 777 and 819queries, respectively.
Most of the rater disagreementinvolves the MISC NE class.
This is not surprisingas MISC is a sort of place-holder category that isdifficult to define and identify in queries, especiallyby untrained AMT workers.
We thus replaced MISCwith the null label O.
With these two changes, ?
was.54 on KDD-D and .64 on KDD-T.
This is sufficientfor repeatable experiments.6Table 2 shows the distribution of NE types in the5 datasets.
IEER has fewer NEs than CoNLL, KDDhas more.
PER is about as prevalent in KDD asin CoNLL, but LOC and ORG have higher percent-ages, reflecting the fact that people search frequentlyfor locations and commercial organizations.
Thesedifferences between source domain (CoNLL) andtarget domains (IEER, KDD) add to the difficultyof cross-domain generalization in this case.6 Experimental setupRecall that the input features for a token w0 con-sist of standard NER features (BASE and GAZ) andfeatures derived from the search result we obtain by6The two KDD sets, along with additional statistics on an-notator agreement requested by a reviewer, are available atifnlp.org/?schuetze/piggyback11.970running a search for w?1w0w1 (URL, LEX, BOW,and MISC).
Since the MISC NE class is not anno-tated in IEER and has low agreement on KDD inthe experimental evaluation we focus on the four-class (PER, LOC, ORG, O) NER problem on alldatasets.
We use BIO encoding as in the originalCoNLL task (Sang and Meulder, 2003).ALL LOC ORG PERCoNLLc1 l BASE GAZ 88.8?
91.9 77.9 93.0c2 l GAZ URL BOW MISC 86.4?
90.7 74.0 90.9c3 l BASE URL BOW MISC 92.3?
93.7 84.8 96.0c4 l BASE GAZ BOW MISC 91.1?
93.3 82.2 94.9c5 l BASE GAZ URL MISC 92.7?
94.9 84.5 95.9c6 l BASE GAZ URL BOW 92.3?
94.2 84.4 95.8c7 l BASE GAZ URL BOW MISC 93.0 94.9 85.1 96.4c8 l BASE GAZ URL LEX BOW MISC 92.9 94.7 84.9 96.5c9 c BASE GAZ 92.9 95.3 87.7 94.6IEERi1 l BASE GAZ 57.9?
71.0 37.7 59.9i2 l GAZ URL LEX BOW MISC 63.8?
76.2 26.0 75.9i3 l BASE URL LEX BOW MISC 64.9?
71.8 38.3 73.8i4 l BASE GAZ LEX BOW MISC 67.3 76.7 41.2 74.6i5 l BASE GAZ URL BOW MISC 67.8 76.7 40.4 75.8i6 l BASE GAZ URL LEX MISC 68.1 77.2 36.9 77.8i7 l BASE GAZ URL LEX BOW 66.6?
77.4 38.3 73.9i8 l BASE GAZ URL LEX BOW MISC 68.1 77.4 36.2 78.0i9 c BASE GAZ 68.6?
77.3 52.3 73.1KDD-Tk1 l BASE GAZ 34.6?
48.9 19.2 34.7k2 l GAZ URL LEX MISC 40.4?
52.1 15.4 50.4k3 l BASE URL LEX MISC 40.9?
50.0 20.1 48.0k4 l BASE GAZ LEX MISC 41.6?
55.0 25.2 45.2k5 l BASE GAZ URL MISC 43.0 57.0 15.8 50.9k6 l BASE GAZ URL LEX 40.7?
55.5 15.8 42.9k7 l BASE GAZ URL LEX MISC 43.8 56.4 17.0 52.0k8 l BASE GAZ URL LEX BOW MISC 43.8 56.5 17.4 52.3Table 3: Evaluation results.
l = text lowercased, c = orig-inal capitalization preserved.
ALL scores significantlydifferent from the best results for the three datasets (linesc7, i8, k7) are marked ?
(see text).We use SuperSenseTagger (Ciaramita and Altun,2006)7 as our NER tagger.
It is a first-order con-ditional HMM trained with the perceptron algo-7sourceforge.net/projects/supersensetagrithm (Collins, 2002), a discriminative model withexcellent efficiency-performance trade-off (Sha andPereira, 2003).
The model is regularized by aver-aging (Freund and Schapire, 1999).
For all modelswe used an appropriate development set for choos-ing the only hyperparameter, T , the number of train-ing iterations on the source data.
T must be tunedseparately for each evaluation because different tar-get domains have different overfitting patterns.We train our NER system on an 80% sample ofthe CoNLL data.
For our in-domain evaluation, wetune T on a 10% development sample of the CoNLLdata and test on the remaining 10%.
For our out-of-domain evaluation, we use the IEER and KDD testsets.
Here T is tuned on the corresponding develop-ment sets.
Since we do not train on IEER and KDD,these two data sets do not have training set portions.For each data set, we perform 63 runs, correspond-ing to the 26?1 = 63 different non-empty combina-tions of the 6 feature groups.
We report average F1,generated by five-trial training and evaluation, withrandom permutations of the training data.
We com-pute the scores using the original CoNLL phrase-based metric (Sang and Meulder, 2003).
As a bench-mark we use the baseline model with gazetteer fea-tures (BASE and GAZ).
The robustness of this sim-ple approach is well documented; e.g., Turian et al(2010) show that the baseline model (gazetteer fea-tures without unsupervised features) produces an F1of .778 against .788 of the best unsupervised wordrepresentation feature.7 Results and discussionTable 3 summarizes the experimental results.
Ineach column, the best numbers within a dataset forthe ?lowercased?
runs are bolded (see below for dis-cussion of the ?capitalization?
runs on lines c9 andi9).
For all experiments, we selected a subset of thecombinations of the feature groups.
This subset always includes the best results and a number of othercombinations where feature groups are added to orremoved from the optimal combination.Results for the CoNLL test set show that the 5feature groups without LEX achieve optimal per-formance (line c7).
Adding LEX improves perfor-mance on PER, but decreases overall performance(line c8).
Removing GAZ, URL, BOW and MISC971from line c7, causes small comparable decreases inperformance (lines c3?c6).
These feature groupsseem to have about the same importance in this ex-perimental setting, but leaving out BASE decreasesF1 by a larger 6.6% (lines c7 vs c2).The main result for CoNLL is that using piggy-back features (line c7) improves F1 of a standardNER system that uses only BASE and GAZ (linec1) by 4.2%.
Even though the emphasis of this pa-per is on cross-domain robustness, we can see thatour approach also has clear in-domain benefits.The baseline in line c1 is the ?lowercase?
base-line as indicated by ?l?.
We also ran a ?capitalized?baseline (?c?)
on text with the original capitalizationpreserved and PoS-tagged in this unchanged form.Comparing lines c7 and c9, we see that piggybackfeatures are able to recover all the performance thatis lost when proper capitalization is unavailable.
Linand Wu (2009) report an F1 score of 90.90 on theoriginal split of the CoNLL data.
Our F1 scores> 92% can be explained by a combination of ran-domly partitioning the data and the fact that the four-class problem is easier than the five-class problemLOC-ORG-PER-MISC-O.We use the t-test to compute significance on thetwo sets of five F1 scores from the two experimentsthat are being compared (two-tailed, p < .01 for t >3.36).8 CoNLL scores that are significantly differentfrom line c7 are marked with ?.For IEER, the system performs best for all sixfeature groups (line i8).
Runs significantly differentfrom i8 are marked ?.
When URL, LEX and BOWare removed from the set, performance does not de-crease, or only slightly (lines i4, i5, i6), indicatingthat these three feature groups are least important.In contrast, there is significant evidence for the im-portance of BASE, GAZ, and MISC: removing themdecreases performance by at least 1% (lines i2, i3,i7).
The large increase of ORG F1 when URL isnot used is surprising (41.2% on line i4, best per-formance).
The reason seems to be that URL fea-tures (and LEX to a lesser extent) do not generalizefor ORG.
Locations like Madrid in CoNLL are fre-quently tagged ORG when they refer to sports clubslike Real Madrid.
This is rare in IEER and KDD.8We make the assumption that the distribution of F1 scoresis approximately normal.
See Cohen (1995), Noreen (1989) fora discussion of how this affects the validity of the t-test.Compared to standard NER (using feature groupsBASE and GAZ), our combined feature set achievesa performance that is by more than 10% higher (linesi8 vs i1).
This demonstrates that piggyback featureshave robust cross-domain generalization properties.The comparison of lines i8 and i9 confirms that thefeatures effectively compensate for the lack of cap-italization, and perform almost as well as (althoughstatistically worse than) a model trained on capital-ized data.The best run on KDD-D was the run with featuregroups BASE, GAZ, URL, LEX and MISC.
On linek7, we show results for this run for KDD-T and forruns that differ by one feature group (lines k2?k6,k8).9 The overall best result (43.8%) is achievedwhen using all feature groups (line k8).
OmittingBOW results in the same score for ALL (line k7).Apparently, the local LEX features already capturemost useful cooccurrence information and lookingat a wider window (as implemented by BOW) is oflimited utility.
On lines k2?k6, performance gen-erally decreases on ALL and the three NE classeswhen dropping one of the five feature groups on linek7.
One notable exception is an increase for ORGwhen feature group URL is dropped (line k4, 25.2%,the best performance for ORG of all runs).
This is inline with our discussion of the same effect on IEER.The key take-away from our results on KDD-T isthat piggyback features are again (as for IEER) sig-nificantly better than standard feature groups BASEand GAZ.
Search engine based adaptation has an ad-vantage of 9.2% compared to standard NER (linesk7 vs k1).
An F1 below 45% may not yet be goodenough for practical purposes.
But even if additionalwork is necessary to boost the scores further, ourmodel is an important step in this direction.The low scores for KDD-T are also partially dueto our processing of the AMT data.
Our selectionprocedure is biased towards short entities whereasCoNLL guidelines favor long NEs.
We can addressthis by forcing AMT raters to be more consistentwith the CoNLL guidelines in the future.We summarize the experimental results as fol-lows.
Piggyback features consistently improve NERfor non-well-edited text when used together withstandard NER features.
While relative improve-9KDD-D F1 values were about 1% higher than for KDD-T.972ment due to piggyback features increases as out-of-domain data become more different from the in-domain training set, performance declines in abso-lute terms from .930 (CoNLL) to .681 (IEER) and.438 (KDD-T).8 ConclusionRobust cross-domain generalization is key in manyNLP applications.
In addition to surface and linguis-tic differences, differences in world knowledge posea key challenge, e.g., the fact that Java refers to alocation in one domain and to coffee in another.
Wehave proposed a new way of addressing this chal-lenge.
Because search engines attempt to make op-timal use of the context a word occurs in, hits shownto the user usually include other uses of the word insemantically similar snippets.
These snippets can beused as a more robust and domain-independent rep-resentation of the context of the word/phrase thanwhat is available in the input text.Our first contribution is that we have shown thatthis basic idea of using search engines for robustdomain-independent feature representations yieldssolid results for one specific NLP problem, NER.Piggyback features achieved an improvement of F1of about 10% compared to a baseline that uses BASEand GAZ features.
Even in-domain, we were ableto get a smaller, but still noticeable improvement of4.2% due to piggyback features.
These results arealso important because there are many applicationdomains with noisy text without reliable capitaliza-tion, e.g., automatically transcribed speech, tweets,SMS, social communities and blogs.Our second contribution is that we address a typeof NER that is of particular importance: NER forweb queries.
The query is the main source of in-formation about the user?s information need.
Queryanalysis is important on the web because under-standing the query, including the subtask of NER, iskey for identifying the most relevant documents andthe most relevant ads.
NER for domains like Twitterand SMS has properties similar to web queries.A third contribution of this paper is the release ofan annotated dataset for web query NER.
We hopethat this dataset will foster more research on cross-domain generalization and domain adaptation ?
inparticular for NER ?
and the difficult problem ofweb query understanding.This paper is about cross-domain generalization.However, the general idea of using search to providerich context information to NLP systems is applica-ble to a broad array of tasks.
One of the main hurdlesthat NLP faces is that the single context a token oc-curs in is often not sufficient for reliable decisions,be they about attachment, disambiguation or higher-order semantic interpretation.
Search makes dozensof additional relevant contexts available and can thusovercome this bottleneck.
In the future, we hope tobe able to show that other NLP tasks can also benefitfrom such an enriched context representation.Future work.
We used a web search engine in theexperiments presented in this paper.
Latencies whenusing one of the three main commercial search en-gines Bing, Google and Yahoo!
in our scenario rangefrom 0.2 to 0.5 seconds per token.
These executiontimes are prohibitive for many applications.
Searchengines also tend to limit the number of queries peruser and IP address.
To gain widespread acceptanceof the piggyback idea of using search results for ro-bust NLP, we therefore must explore alternatives tosearch engines.In future work, we plan to develop more efficientmethods of using search results for cross-domaingeneralization to avoid the cost of issuing a largenumber of queries to search engines.
Caching willbe of obvious importance in this regard.
Another av-enue we are pursuing is to build a specialized searchsystem for our application in a way similar to Ca-farella and Etzioni (2005).
While we need goodcoverage of a large variety of domains for our ap-proach to work, it is not clear how big the indexof the search engine must be for good performance.Conceivably, collections much smaller than those in-dexed by major search engines (e.g., the Google 1T5-gram corpus or ClueWeb09) might give rise to fea-tures with similar robustness properties.
It is impor-tant to keep in mind, however, that one of the keyfactors a search engine allows us to leverage is thenotion of relevance which might not be always pos-sible to model as accurately with other data.Acknowledgments.
This research was funded bya Google Research Award.
We would like to thankAmir Najmi, John Blitzer, Richa?rd Farkas, FlorianLaws, Slav Petrov and the anonymous reviewers fortheir comments.973ReferencesRie Kubota Ando.
2004.
Exploiting unannotated cor-pora for tagging and chunking.
In ACL, CompanionVolume, pages 142?145.Cory Barr, Rosie Jones, and Moira Regelson.
2008.
Thelinguistic structure of English web-search queries.
InEMNLP, pages 1021?1030.Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2010.
A theory of learning from differentdomains.
Machine Learning, 79:151?175.Thorsten Brants.
2000.
TnT ?
A statistical part-of-speech tagger.
In ANLP, pages 224?231.Michael J. Cafarella and Oren Etzioni.
2005.
A searchengine for natural language applications.
In WWW,pages 442?452.Nancy A. Chinchor, editor.
1998.
Proceedings of theSeventh Message Understanding Conference.
NIST.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informationextraction with a supersense sequence tagger.
In Pro-ceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, pages 594?602.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and Psychological Mea-surement, 20(1):37?46.Paul R. Cohen.
1995.
Empirical methods for artificialintelligence.
MIT Press, Cambridge, MA, USA.Michael Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In EMNLP, pages 1?8.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In EMNLP-CoNLL, pages 708?716.Hamish Cunningham, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
GATE:A framework and graphical development environmentfor robust NLP tools and applications.
In ACL, pages168?175.Richa?rd Farkas, Gyo?rgy Szarvas, and Ro?bert Orma?ndi.2007.
Improving a state-of-the-art named entity recog-nition system using the world wide web.
In IndustrialConference on Data Mining, pages 163?172.Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.Annotating named entities in twitter data with crowd-sourcing.
In NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechani-cal Turk, pages 80?88.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine Learning, 37:277?296.Atsushi Fujita and Satoshi Sato.
2008.
A probabilis-tic model for measuring grammaticality and similar-ity of automatically generated paraphrases of predicatephrases.
In COLING, pages 225?232.Gregory Grefenstette, Yan Qu, and David A. Evans.2004.
Mining the web to create a language modelfor mapping between English names and phrases andJapanese.
In Web Intelligence, pages 110?116.Jiafeng Guo, Gu Xu, Xueqi Cheng, and Hang Li.
2009.Named entity recognition in query.
In SIGIR, pages267?274.Jun?ichi Kazama and Kentaro Torisawa.
2007.
Exploit-ing Wikipedia as external knowledge for named entityrecognition.
In EMNLP-CoNLL, pages 698?707.Mirella Lapata and Frank Keller.
2005.
Web-based mod-els for natural language processing.
ACM Transac-tions on Speech and Language Processing, 2(1):1?31.Nolan Lawson, Kevin Eustice, Mike Perkowitz, andMeliha Yetisgen-Yildiz.
2010.
Annotating large emaildatasets for named entity recognition with mechani-cal turk.
In NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechani-cal Turk, pages 71?79.Ying Li, Zijian Zheng, and Honghua (Kathy) Dai.
2005.KDD CUP 2005 report: Facing a great challenge.SIGKDD Explorations Newsletter, 7:91?99.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clusteringfor discriminative learning.
In Proceedings of the JointConference of the 47th Annual Meeting of the ACL andthe 4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 1030?1038.Eric W. Noreen.
1989.
Computer-Intensive Methodsfor Testing Hypotheses : An Introduction.
Wiley-Interscience.Thierry Poibeau and Leila Kosseim.
2000.
Proper nameextraction from non-journalistic texts.
In CLIN, pages144?157.J.
J. Rocchio.
1971.
Relevance feedback in informa-tion retrieval.
In Gerard Salton, editor, The Smart Re-trieval System ?
Experiments in Automatic DocumentProcessing, pages 313?323.
Prentice-Hall.Mehran Sahami and Timothy D. Heilman.
2006.
A web-based kernel function for measuring the similarity ofshort text snippets.
In WWW, pages 377?386.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof CoNLL 2003 Shared Task, pages 142?147.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, pages 44?49.974Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proceedings of the2003 Conference of the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology - Volume 1, pages 134?141.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In ACL, pages384?394.Peter D. Turney.
2002.
Thumbs up or thumbs down?semantic orientation applied to unsupervised classifi-cation of reviews.
In ACL, pages 417?424.Jinxi Xu and W. Bruce Croft.
1996.
Query expansionusing local and global document analysis.
In SIGIR,pages 4?11.Alexander Yates, Stefan Schoenmackers, and Oren Et-zioni.
2006.
Detecting parser errors using web-basedsemantic filters.
In EMNLP, pages 27?34.Meliha Yetisgen-Yildiz, Imre Solti, Fei Xia, andScott Russell Halgrim.
2010.
Preliminary experiencewith Amazon?s Mechanical Turk for annotating medi-cal named entities.
In NAACL HLT 2010 Workshop onCreating Speech and Language Data with Amazon?sMechanical Turk, pages 180?183.975
