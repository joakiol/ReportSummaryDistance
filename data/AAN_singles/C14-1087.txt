Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 917?927, Dublin, Ireland, August 23-29 2014.Triple based Background Knowledge Ranking for Document EnrichmentMuyu Zhang, Bing Qin?, Ting Liu, Mao ZhengResearch Center for Social Computing and Information RetrievalHarbin Institute of Technology, China{myzhang,qinb,tliu,mzheng}@ir.hit.edu.cnAbstractDocument enrichment is the task of retrieving additional knowledge from external resource overwhat is available through source document.
This task is essential because of the phenomenonthat text is generally replete with gaps and ellipses since authors assume a certain amount ofbackground knowledge.
The recovery of these gaps is intuitively useful for better understandingof document.
Conventional document enrichment techniques usually rely on Wikipedia whichhas great coverage but less accuracy, or Ontology which has great accuracy but less cover-age.
In this study, we propose a document enrichment framework which automatically extracts?argument1, predicate, argument2?
triple from any text corpus as background knowledge, sothat to ensure the compatibility with any resource (e.g.
news text, ontology, and on-line ency-clopedia) and improve the enriching accuracy.
We first incorporate source document and back-ground knowledge together into a triple based document-level graph and then propose a globaliterative ranking model to propagate relevance score and select the most relevant knowledgetriple.
We evaluate our model as a ranking problem and compute the MAP and P&N score tovalidate the ranking result.
Our final result, a MAP score of 0.676 and P&20 score of 0.417outperform a strong baseline based on search engine by 0.182 in MAP and 0.04 in P&20.1 IntroductionDocument enrichment is the task to acquire background knowledge from external resources and recoverthe omitted information automatically for certain document.
This task is essential because authors usu-ally omit basic but well-known information to make the document more concise.
For example, authoromits ?Baghdad is the captain of Iraqi?
in the text of Figure 1 (a), which is well-known to readers.
Dur-ing reading process, these gaps will be automatically plugged effortlessly by the background knowledgein human brain.
However, the situation is different for machine because it lacks the ability to acquireand select the proper background knowledge, which limits the performances of certain NLP applica-tions.
Document enrichment has been proved helpful in these tasks such as web search (Pantel andFuxman, 2011), coreference resolution (Bryl et al., 2010), document cluster (Hu et al., 2009) and entitydisambiguation (Bunescu and Pasca, 2006; Sen, 2012).In the past, there are mainly two kinds of document enrichment researches according to the resourcethey relying on.
The first line of works make use of WikiPedia, the largest available on-line encyclopediaas resource and link the entity (e.g.
Baghdad) of document to its corresponding Wiki page (e.g.
Baghdad1in WikiPedia), so that to enrich the document with the context of Wiki page (Bunescu and Pasca, 2006;Cucerzan, 2007; Han et al., 2011; Kataria et al., 2011; Sen, 2012; He et al., 2013).
Despite the greatsuccess of these methods, there remain a great challenge that not all information in the linked Wiki pageis helpful to the understanding of corresponding document.
For example, the Wiki page of Baghdadcontains lots of information about city history and culture, which are not quite relevant to the semantic ofcontext in Figure 1 (a).
So treating the whole Wiki page as the enrichment to document may cause noise?Corresponding author.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1http://en.wikipedia.org/wiki/Baghdad917S1: Coalition may never know if   Iraqi   president   SaddamHussein survived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including Saddam Hussein and  his sons .BaghdadIraqi hasCapitalSaddam Hussein diedInQusay HusseinSaddam Hussein hasChildKadhimiyak1:k2:k3:GlobalRanking(a) Source document (b) Top-3 background knowledgeS1: Coalition may never know if   Iraqi   president   Saddam Husseinsurvived a U.S. air strike yesterday.S2: A B-1 bomber dropped four 2,000-pound bombs on a buildingin a residential area of   Baghdad  .S3: They had got an intelligence reports senior officials weremeeting there, possibly including  Saddam Hussein  and   his sons .
(a) Source document (b) Two relevant background knowledgeIraqBaghdadSaddam HusseinCaptainDied In KadhimiyaGlobalRankingk1:k2:Figure 1: An example of document enrichment with background knowledge: (a) source document talkingabout a U.S. air strike aiming at Saddam in Baghdad (b) two important relevant information, which isomitted in source document but acquired by our model and enriched as background knowledge .problem.
Another line of works rely on the Ontologies constructed with supervision or even manuallywhich have great accuracy but less coverage (Motta et al., 2000; Passant, 2007; Fodeh et al., 2011; Kumarand Salim, 2012).
Besides, these methods usually rely on special ontology which is rather difficult toconstruct and in turn limits the coverage and application of these methods.Ideally, we would wish to integrate both coverage and accuracy, where an triple based backgroundknowledge ranking model may help.
Our framework extracts knowledge from any corpus resource in-cluding WikiPedia to ensure coverage and present knowledge as ?argument1, predicate, argument2?triple to reduce noise.
This model ranks background knowledge triples according to their relevance tothe source document.
The key idea behind the model is that document is constructed by several units ofinformation, which can be extracted automatically.
For every background knowledge b extracted auto-matically from a relevant corpus, the more units are relevant to b and the more important they are, themore relevant b becomes to the source document.
Thus, we extract both source document informationand background knowledge automatically and present them together in a document-level graph.
Thenwe propagate the relevance score from the source document information to the background knowledgeduring an iterative process.
After convergence, we obtain the Top n relevant background knowledge,rather than retrieving all of them without filtering.To evaluate our model, we use ACE2corpus as source documents and output the ranked list of back-ground knowledge.
Then we train three annotators to check the ranking result and annotating whethercertain knowledge is relevant to corresponding source document separately.
We totally annotated morethan 7000 background knowledge by three annotators.
We evaluate their annotation consistence by com-puting the Fleiss?Kappa (Fleiss, 1971), a famous criterion in multi-annotator consistence evaluation.We achieve a Fleiss?Kappa of value 0.8066 in best situation and 0.7076 in average, which indicatesthe great consistence between three annotators.
The ranking result is evaluated with MAP score andP&N score (Voorhees et al., 2005).
We finally achieve aMAP score of 0.676 and P&20 score of 0.417in Top 20 background knowledge, which are higher by 0.182 and 0.04 than a strong baseline basedon search engine.
We also evaluate the effect of the automatically extraction to source document andbackground knowledge, which is key to the performance of our method in real application.2 Triple Graph based Document RepresentationWe believe that different parts of document are related to each other, rather than isolated.
Hence, wepropose a triple graph based document representation to incorporate source document information andbackground knowledge.
In this presentation, ?argument1, predicate, argument2?
triple serves as nodeand the edge between nodes indicates their semantic relevance.
In this part, we introduce triple graphand the way to extract source document information and background knowledge automatically.2.1 Motivation for triple presentationCompared to Wiki Page, triple based enrichment helps to reduce noise illustrated in Section 1.
Comparedto bag of words, triple based presentation help to reduce ambiguity of single word which is shown in2http://catalog.ldc.upenn.edu/LDC2006T06918B ar ac k O bam aH ar v ar d  U niv e r s it y W h it e  H ou s e(a) Ambiguity (b) DisambiguationB ar ac k O bam a,  e a r n ,  l a w  d e g r e eH ar v ar d  U niv e r s it y W h it e  H ou s eFigure 2: The motivation for the form of triple (a) relevance ambiguity of single word Obama, whichis related to Harvard and White House (b) disambiguation with the help of other triple elements, where?earn, law degree?
help to limit Obama to the graduate of Harvard.Figure 2.
Figre 2 (a) shows that the single word of Obama is related to multiple semantic informationsuch as Harvard University as a law graduate and White House as the president.
After introducing theinformation from other elements of the triple, ?earn, law degree?
help to disambiguate and limit Obamato the law graduate of Harvard University only in Figure 2 (b).
The form of triple has been used as thepresentation of knowledge in some researches such as knowledge base (Hoffart et al., 2013).2.2 Nodes in the GraphThere are two kinds of nodes in the triple graph: source document nodes (sd-nodes) and backgroundknowledge nodes (bk-nodes).
Both of them are extracted automatically with Open Information Ex-traction (Open IE) technology which focuses on extracting assertions from massive corpora without apre-specied vocabulary (Banko et al., 2007).
Open IE systems are unlexicalized-formed only in terms ofsyntactic tokens and closed-word classes, instead of specific nouns and verbs at all costs.There are existing Open IE systems such as TextRunner (Banko et al., 2007), WOE (Wu and Weld,2010), and StatSnowball (Zhu et al., 2009).
The output of these systems has been used to support manyNLP tasks such as learning selectional preference (Ritter et al., 2010), acquiring sense knowledge (Linet al., 2010), and recognizing entailment (Schoenmackers et al., 2010).
In this work, we use the famousOpen IE system Reverb (Etzioni et al., 2011), which is generated from TextRunner (Etzioni et al., 2008),to extract source document information and background knowledge automatically.
We use the newestversion of ReVerb (version 1.3) without modification, which is free download on-line3.Source document node (sd-node) Sd-nodes consists of the information extracted from source docu-ment automatically by open information extraction technology (Banko et al., 2007), especially Reverb,the famous Open IE system developed by University of Washington (Etzioni et al., 2011).
The outputof ReVerb is formed as ?argument1, predicate, argument2?, which is naturally presented as triple.
Inthis study, we use ACE corpus as source documents and all sd-nodes are extracted by ReVerb.
The setupof automatic extraction makes our method usable in many real applications.
To evaluate the effect ofautomatic extraction, we also use the golden annotation within ACE (Doddington et al., 2004) corpus assource document information and compare the performance that with automatic extraction.Background knowledge node (bk-node) Bk-nodes consist of the background knowledge extractedfrom external corpus resources automatically by Reverb too.
We do not rely on certain existed knowl-edge base and extract background knowledge from external corpus resources for corresponding sourcedocument.
This setup makes our methods usable in many real applications.
Although we do not rely onspecial knowledge base, we do adapt our method for the existed knowledge base such as YAGO (Hoffartet al., 2013) and compare the performance to evaluate the effect of different knowledge sources.2.3 Edges in the GraphThe edges between two nodes indicate their semantic relevance, which is evaluated in Section 3.1.
Thereare two kinds of edges: (1) sd-node to sd-node (2) sd-node to bk-node, both of them are undirected.Considering all the relevance score originating from sd-nodes, we connect no edge between bk-nodes.3http://reverb.cs.washington.edu/919Edges between sd-nodes All sd-nodes are extracted from the same document, so they should be relatedto each other.
We connect each pair of sd-nodes with an edge and set the weight of edge as their semanticrelevance computed in Section 3.1.
With this setup, we combine the source document as a whole wheredifferent parts affect each other through the edge.Edges between sd-node and bk-node The basic idea of our model is to propagate relevance scorefrom the sd-nodes to bk-nodes.
Hence, we connect each pair of sd-node and bk-node with an edge andset the weight of the edge as their relevance computed in Section 3.1.
These edges are all undirected,which indicates that bk-nodes also affect the relevance score of the sd-nodes during the ranking process.3 Global Ranking ModelIn this study, source document D is presented as the graph of sd-nodes.
For every background knowledgeb, the task of evaluating the relevance between b and D is naturally converted into evaluating the relevancebetween b and the graph of sd-nodes.
So the relevance between b and document D can be computed bypropagating the relevance score from every sd-node of D to b iteratively.
After the convergence, therelevance between b and D can be evaluated by the relevance score of b.
Intuitively, three factors affecttheir relevance:?
How many sd-nodes is b relevant to ??
How relevant is b to these sd-nodes??
How important are these sd-nodes ?For the first factor, b should be more relevant to source document D if more sd-nodes are relevantto b.
We capture this information by allowing b to receive relevance score from all the sd-nodes.
Forthe second factor, b should be more relevant to D if more relevant b is to sd-nodes.
We consider thisinformation by evaluating the relevance between b and every sd-node (Section 3.1).
For the last factor,important sd-nodes should have higher impact.
We consider this information by evaluating the impor-tance of sd-nodes and assigning higher initial value to importance ones (Section 3.3).
We combine allfactors in the global ranking process to select the top-n relevant background knowledge (Section3.2).3.1 Relevance Evaluation between NodesIn this section, we evaluate the semantic relevance between different nodes which is the weight of theedge between them.
We introduce Search Engine as a resource, which has been proven effective inrelevance evaluation (Gligorov et al., 2007).
This method is motivated by the phenomenon that thenumber of results returned by search engine for query p ?
qindicates the relevance between p and q.However, considering the different popularization of queries, this number alone can not accuratelyexpress their semantic relevance.
For example, query car ?
automobile gets 294, 300, 000 results,whereas query car?apple gets 683, 000, 000, which is 2 times higher than the previous one.
Obviously,automobile is more relevant to car rather than Apple.
The reason of this phenomenon is that appleis far more popular than automobile, which increase its possibility of co-occurrence with car.
So weconsider the number of results for p?q together with p and q withWebJaccard Coefficient (Bollegalaet al., 2007) to evaluate the relevance between p and q according to Formula 1, where H(p), H(q), andH(p ?
q) indicate the number of results for query p, p, and p ?
q.WebJaccard(p, q) ={0 if H(p ?
q) ?
CH(p?q)H(p)+H(q)?H(p?q)otherwise.
(1)To convert one ?argument1, predicate, argument2?
triple into query, we use argument1?argument2as the query for one triple.
We have tried argument1?
predicate ?
argument2which920is usually very sparse.
Besides, the combination of two arguments usually maintain better semantic com-pleteness of triple compared to other combinations according to our analysis.
So this setup aims to bal-ance completeness and sparseness.
Accordingly, two triples are combined as argument1?argument2?argument?1?
argument?2.
Considering the scale and noise in the Web data, it is possible for two wordsto appear together accidentally.
To reduce the adverse effects attributed to random co-occurrences, weset 0 to the WebJaccard Coefficient of query p?
q, if the number of result is less than a threshold C.3.2 Iterative Relevance PropagationHere we propose the relevance propagation based iterative process to evaluate the relevance between cer-tain background knowledge and source document.
Note that standard label propagation mainly focuseson classification task (Wang and Zhang, 2008).
However, we focus on a ranking problem where the bestranking result is computed during an iterative process in this study.
So we make two modifications tosuit the ranking problem better: not reseting the relevance score and introducing the propagation betweensource document information during iteration.Propagation possibility The edge between nodeiand nodejis weighted by r(i, j) to measure theirrelevance.
However, r(i, j) cannot completely present the propagation possibility because one node canbe equally relevant to all of its neighbors.
Thus, we define p(i, j) based on r(i, j) in formula 2 to indicatethe propagation possibility between nodeiand nodej.p(i, j) =r(i, j)?
?
(i, j)?k?Nr(k, j)?
?
(k, j)(2)N is the set of all nodes, ?
(i, j) denotes whether an edge exists between nodeiand nodelin the triple-graph or not, which indicates whether they may propagate to each other or not.
E is the set of edges.?
(i, j) ={1 if (i, j) ?
E0 otherwise(3)Iterative propagation There are n ?
n pairs of nodes, the p(i, j) of them is stored in a matrix P .we use~W = (w1, w2, ?
?
?
, wn) to denote the relevance score of all nodes, in which wiindicates therelevance between nodeiand source document D. Here the nodeican indicate both sd-nodes and bk-nodes because they are processed during one fellow step.
So that we keep updating both sd-nodes andbk-nodes and do not distinguish them explicitly.
The only difference between them is that we initializethe wiof sd-nodes as its importance to D (Section 3.1) while bk-nodes as 0 at the beginning.
We usematrix P together with ?
(i, j) to compute the~W during a iterative process, where~W is updated to~W?during the end of every iteration.
The matrix~W?is updated according to the following Formula 4:~W?=~W ?
P=~W ????
?p(1, 1) p(1, 2) ?
?
?
p(1, n)p(2, 1) p(2, 2) ?
?
?
p(2, n)?
?
?
?
?
?
?
?
?
?
?
?p(n, 1) p(n, 2) ?
?
?
p(n, n)????
(4)each wiin~W is updated to w?iaccording to the formula 5, where wiis propagated from all the otherwj(j 6= i) according to their propagation possibility p(j, i).
We also introduce the propagation frombk-nodes to sd-nodes, where bk-nodes serve as intermediate to help mining latent semantics.w?i= w1?
p(1, i) + w2?
p(2, i) + ?
?
?+ wn?
p(n, i)=?k?Nwk?
p(k, i)=?k?Nwk?
(r(i, j)?
?
(i, j)?k?Nr(k, j)?
?
(k, j))(5)9213.3 Importance Evaluation for sd-nodesThe main idea of our model is to propagate relevance score from sd-nodes to bk-nodes (Section 3.2).So the initialization of sd-node is important, which indicates the importance of different source docu-ment information.
This section solves this problem by evaluating the importance of sd-nodes to sourcedocument.
We use vjto denote the initialization of sd-nodes, which indicates the importance of nodej(nodej?
set of sd-nodes) to source document.
In this section, we propose a modified relevance propa-gation method to evaluate vjfor sd-notes.
We first construct a triple-graph consisting of sd-nodes only.Then we initialize the relevance score of sd-nodes according to a simple approach based on text fre-quency (Kohlsch?utter et al., 2010).
We use similar relevance propagation process without resetting therelevance score at the beginning of every iteration, until a global stable state is achieved.
Finally, wenormalize all the relevance scores to get~V , which indicates the importance of sd-nodes to the sourcedocument.
We return~V to the global ranking model (Section 3.2) as part of the input.
The initial impor-tance of bk-nodes is set as 0 at the beginning, which denotes that all bk-nodes are ir-relevant to sourcedocument before the starting of global ranking process.4 ExperimentWe treat our task as a ranking problem, which takes a document as input and output the ranked list ofbackground knowledge.
We evaluate our method as a ranking problem similarly to information retrievaltask and focus on the performances of models with different setups.4.1 Data PreparationThe experiment data consists of two parts: source document information and corresponding backgroundknowledge.
To select source documents, we use the ACE corpus (Doddington et al., 2004) for 2005 eval-uation4which consists of 599 articles from multiple sources.
We use ReVerb to extract these documentsinto multi-triples.
For background knowledge, we first retrieve relevant web pages with simply termmatching method and then extract these pages with ReVerb into a set of triples serving as backgroundknowledge.
To ensure the quality, we filter them according to the confidence given by ReVerb.Besides automatic extraction, we also adapt our system to the golden annotation of ACE as sourcedocument information and standard YAGO knowledge base5as background knowledge (Hoffart et al.,2013).
We compare its performance with that in fully automatic system and evaluate the effect of auto-matic extraction.
For better comparison with YAGO, we retrieve relevant pages from WikiPedia althoughour automatic extraction method is applicable to any corpus resources.For every outputted list, three trained annotators check the result and decide which background knowl-edge is relevant to source document.
They work separately and check the same list, so that we can e-valuate their annotation consistence.
They totally annotated more than 7000 background knowledge andachieved a Fleiss?Kappa value of 0.8066 in best situation and 0.7076 in average between three anno-tators, which is a good consistence between multi-annotator (Fleiss, 1971).
When collision happened,we choose the label selected by more annotators.4.2 Baseline systemAlthough we treat our task as a ranking problem, it is difficult to apply corresponding methods in tra-ditional ranking tasks such as information retrieval (IR) (Manning et al., 2008) and entity linking (EL)(Han et al., 2011; Kataria et al., 2011; Sen, 2012) directly in our task.
First, both IR and EL make use ofthe link structure between web or Wiki pages.
However, our task takes single document as input and nolink exists between documents which makes it difficult to apply IR and EL methods such as page rank(Page et al., 1999) and collective method (Han et al., 2011; Sen, 2012) in this task directly.
Second, ELusually evaluate the text similarity between certain document and target page in WikiPedia.
However,our task focuses on the ranking of ?argument1, predicate, argument2?
triple, which contains little textinformation.
Lack of text information also limits the application of corresponding methods in our task.4http://catalog.ldc.upenn.edu/LDC2006T065http://www.mpi-inf.mpg.de/yago-naga/yago922Setup MAP P&20Baseline 0.494 0.377AutoSD + AutoBK + NoInitial 0.504 0.378AutoSD + AutoBK + WithInitial 0.531 0.406GoldSD + AutoBK + NoInitial 0.564 0.417GoldSD + AutoBK + WithInitial 0.553 0.406GoldSD + YAGO + NoInitial 0.676 0.328GoldSD + YAGO + WithInitial 0.676 0.328Table 1: The result of our model in different setups: GoldSD indicates using annotation of ACE corpus assource document information; YAGO indicates using YAGO knowledge base as background knowledge;AutoSD and AutoBK means aotomatic extraction to source document and background knowledge; NoIni-tial and WithInitial means whether using different initial importance to source document information.For better comparison, we introduce search engine as resource which is proved effective in relevanceevaluation (Gligorov et al., 2007) and propose a search engine based strong baseline.
As illustratedbefore, the relevance Ribetween background knowledge biand source document D has been convertedinto the relevance between biand the triples of D. Hence, we compute Riby accumulating all rij, therelevance scores between biand every sd-node sjwith the same method in Section 3.1 (Ri=?sj?Srij,S is the set of sd-nodes).
Then we rank all background knowledge according to the value ofRiand outputthe ranked list as final result.
We extract source document and background knowledge automatically inthe baseline system, which makes it applicable in different setups.4.3 Experiment setupWe evaluate our model in different setups.
First, we extract both source document information andbackground knowledge automatically.
Second, we use golden annotation of ACE as source documentinformation but extract background knowledge automatically.
Third, we use golden annotation of ACEand introduce standard YAGO as background knowledge.
For all of them three, we evaluate the differentperformances with and without initial importance of sd-nodes(Section 3.3).
We evaluate the performancewith two famous criteria in ranking problem: MAP (Voorhees et al., 2005) requires more accuracyand focuses on the knowledge in higher position; P&N which require more coverage and pays moreattention to the number of relevant ones in Top N knowledge.
Note that we do not evaluate the Recallperformance because there can be millions of background knowledge to be ranked for every document.It is impossible to check all of them.
So we focus on the Top N candidates and evaluate the performancewith MAP and P&N .
In this study, we evaluate the Top 20 background knowledge triples which aremost easily to be viewed by users.4.4 Experiment ResultThe performance of our model is shown in Table 1.
Our search engine based baseline system achievea rather good performance: a MAP value of 0.494 and 0.377 in P&20.
At the same time, our modeloutperforms the baseline system in almost every setup and evaluation criterion.
The best performance ofMAP is achieved by GoldSD+YAGO (0.676), while the best performance of P&20 is achieved by GoldS-D+AutoBK (0.417).
To analyze the result further, we find that the initial importance, automatic extractionto source document, and to background knowledge have different effect on the final performance.4.4.1 Effect of automatic extraction to source documentWe use ACE corpus as source documents, which contain golden annotation to document information.So we can evaluate the effect of automatic extraction to source document by comparing the performancewith and without golden annotation.
The performance without golden annotation is shown in AutoS-D+AutoBK of Table 1, while the other one shown in GoldSD+AutoBK.
We can find that the performanceof GoldSD+AutoBK is better than that of AutoSD+AutoBK in both MAP and P&20, which indicates thatgolden annotation do help to improve the ranking result.923We further analyze the result and find an interesting phenomenon: these two systems performs greatlydifferent with the setup of NoInitial, but equally with the setup of WithInitial, which indicates that theperformance of AutoSD+AutoBK has been improved by evaluating the importance of source documentinformation (Section 3.3).
So we can naturally infer that, with a better importance evaluating methodin AutoSD+AutoBK, we may achieve similar performance compared to that in golden annotation.
Notethat, AutoSD+AutoBK is compatible with any corpus which is more useful in real applications.4.4.2 Effect of automatic extraction to background knowledgeWe evaluate the effect of automatic extraction to background knowledge by comparing the performancesbetween GoldSD+AutoBK and GoldSD+YAGO.
In GoldSD+AutoBK, the background knowledge is ex-tracted automatically with ReVerb, which has greater coverage but less accuracy.
In contrast, the GoldS-D+YAGO make use of YAGO as background knowledge, which is less coverage but better accuracy.
Thisdifference are reflected on the system performance, where GoldSD+YAGO achieves much better result inMAP, but much worse in P&20.
This is partly because that MAP focus on the background knowledge inhigher position which requires more accuracy, while P&20 pays more attention to the number of relevantbackground knowledge which require more coverage.In general, automatic extraction system has better coverage but less accuracy compared to YAGObased system.
However, automatic extraction to background knowledge may help in real applications byimproving coverage greatly.
Besides, the loss of accuracy is partly due to the technology of informationextraction which may be improved in the future.
In addition, we can also combine these two ways toacquire background knowledge to balance coverage and accuracy in the future.4.4.3 Effect of initial importance to source document informationInitial importance to source document information (Section 3.3) is important to the performance ofour models as shown in Table 1.
The model AutoSD+AutoBK+WithInitial outperforms the AutoS-D+AutoBK+NoInitial compared to other setups, which indicates the help of initial importance to theranking result.
Especially, initial importance to source document information helps most in the set-up of AutoSD+AutoBK, which is most useful in real applications.
So we can naturally infer that, byproposing better importance evaluating method, we may further improve the performance of AutoS-D+AutoBK+WithInitial, which will great helpful in the future application of this method.5 Related WorkDocument enrichment focuses on introducing external knowledge into source document.
There are main-ly two kinds of works in this topic according to the resource they relying on.
The first line of works makeuse of WikiPedia and enrich source document by linking the entity to its corresponding Wiki page (Bunes-cu and Pasca, 2006; Cucerzan, 2007).
In early stage, most researches rely on the similarity between thecontext of the mention and the definition of candidate entities by proposing different measuring crite-ria such as dot product, cosine similarity, KL divergence, Jaccard distance and more complicated ones(Bunescu and Pasca, 2006; Cucerzan, 2007; Zheng et al., 2010; Hoffart et al., 2011; Zhang et al., 2011).However, these methods mainly rely on text similarity but neglect the internal structure between mention-s.
So another kind of works explore the structure information with collective disambiguation (Kulkarniet al., 2009; Kataria et al., 2011; Sen, 2012; He et al., 2013).
These methods make use of structure infor-mation within context and resolve different mentions based on the coherence among decisions.
Despitethe success, the entity linking methods rely on WikiPedia which has great coverage but less accuracy.Another line of works try to improve the accuracy of enrichment by introducing ontologies (Mottaet al., 2000; Passant, 2007; Fodeh et al., 2011; Kumar and Salim, 2012) and structured knowledgesuch as WordNet (Nastase et al., 2010) and Mesh (Wang and Lim, 2008).
In these studies, resourcesusually provides word or phrase semantic information such as synonym (Sun et al., 2011) and antonym(Sansonnet and Bouchet, 2010).
However, these methods rely on special ontologies constructed withsupervision or even manually, which is difficult to expand and in turn limits the application of them.9246 Conclusion and Future WorkThis study presents a triple based background knowledge ranking model to acquire most relevant back-ground knowledge to certain source document.
We first develop a triple graph based document presen-tation to combine source document together with the background knowledge.
Then we propose a globaliterative ranking model to acquire Top n relevant knowledge, which provide additional information be-yond the source document.
Note that, both source document information and background knowledgeare extracted automatically which is useful in real application.
The experiments show that our modelachieves better results over a strong baseline, which indicates the effectiveness of our framework.Another interesting phenomenon is that YAGO based enrichment model achieved better ranking ac-curacy, but less coverage compared to automatic extraction model.
To combine these two sources ofbackground knowledge may help to overcome both coverage and accuracy problem.
So exploiting prop-er way to incorporate knowledge base and automatic extraction is an important topic in our future work.Finally, we believe that this background knowledge based document enriching technology may help inthose semantic based NLP applications such as coherence evaluation, coreference resolution and questionanswering.
In our future work, we will explore how to make use of these background knowledge in realapplications, hopefully to improve the performance significantly in the future.AcknowledgementsWe thank Muyun Yang and Jianhui Ji for their great help.
This work was supported by National NaturalScience Foundation of China(NSFC) via grant 61133012, the National 863 Leading Technology Re-search Project via grant 2012AA011102 and the National Natural Science Foundation of China SurfaceProject via grant 61273321.ReferencesMichele Banko, Michael J Cafarella, Stephen Soderland, Matthew Broadhead, and Oren Etzioni.
2007.
Openinformation extraction from the web.
In IJCAI, volume 7, pages 2670?2676.Danushka Bollegala, Yutaka Matsuo, and Mitsuru Ishizuka.
2007.
Measuring semantic similarity between wordsusing web search engines.
www, 7:757?766.Volha Bryl, Claudio Giuliano, Luciano Serafini, and Kateryna Tymoshenko.
2010.
Using background knowledgeto support coreference resolution.
In ECAI, volume 10, pages 759?764.Razvan C Bunescu and Marius Pasca.
2006.
Using encyclopedic knowledge for named entity disambiguation.
InEACL, volume 6, pages 9?16.Silviu Cucerzan.
2007.
Large-scale named entity disambiguation based on wikipedia data.
In EMNLP-CoNLL,volume 7, pages 708?716.George R Doddington, Alexis Mitchell, Mark A Przybocki, Lance A Ramshaw, Stephanie Strassel, and Ralph MWeischedel.
2004.
The automatic content extraction (ace) program-tasks, data, and evaluation.
In LREC.Citeseer.Oren Etzioni, Michele Banko, Stephen Soderland, and Daniel S Weld.
2008.
Open information extraction fromthe web.
Communications of the ACM, 51(12):68?74.Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam.
2011.
Open infor-mation extraction: The second generation.
In Proceedings of the Twenty-Second international joint conferenceon Artificial Intelligence-Volume Volume One, pages 3?10.
AAAI Press.Joseph L Fleiss.
1971.
Measuring nominal scale agreement among many raters.
Psychological bulletin, 76(5):378.Samah Fodeh, Bill Punch, and Pang-Ning Tan.
2011.
On ontology-driven document clustering using core semanticfeatures.
Knowledge and information systems, 28(2):395?421.Risto Gligorov, Warner ten Kate, Zharko Aleksovski, and Frank van Harmelen.
2007.
Using google distance toweight approximate ontology matches.
In Proceedings of the 16th international conference on World Wide Web,pages 767?776.
ACM.925Xianpei Han, Le Sun, and Jun Zhao.
2011.
Collective entity linking in web text: a graph-based method.
InProceedings of the 34th international ACM SIGIR conference on Research and development in InformationRetrieval, pages 765?774.
ACM.Zhengyan He, Shujie Liu, Mu Li, Ming Zhou, Longkai Zhang, and Houfeng Wang.
2013.
Learning entity repre-sentation for entity disambiguation.
Proc.
ACL2013.Johannes Hoffart, Mohamed Amir Yosef, Ilaria Bordino, Hagen F?urstenau, Manfred Pinkal, Marc Spaniol, BilyanaTaneva, Stefan Thater, and Gerhard Weikum.
2011.
Robust disambiguation of named entities in text.
In Pro-ceedings of the Conference on Empirical Methods in Natural Language Processing, pages 782?792.
Associationfor Computational Linguistics.Johannes Hoffart, Fabian M Suchanek, Klaus Berberich, and Gerhard Weikum.
2013.
Yago2: a spatially andtemporally enhanced knowledge base from wikipedia.
Artificial Intelligence, 194:28?61.Xiaohua Hu, Xiaodan Zhang, Caimei Lu, Eun K Park, and Xiaohua Zhou.
2009.
Exploiting wikipedia as externalknowledge for document clustering.
In Proceedings of the 15th ACM SIGKDD international conference onKnowledge discovery and data mining, pages 389?396.
ACM.Saurabh S Kataria, Krishnan S Kumar, Rajeev R Rastogi, Prithviraj Sen, and Srinivasan H Sengamedu.
2011.Entity disambiguation with hierarchical topic models.
In Proceedings of the 17th ACM SIGKDD internationalconference on Knowledge discovery and data mining, pages 1037?1045.
ACM.Christian Kohlsch?utter, Peter Fankhauser, and Wolfgang Nejdl.
2010.
Boilerplate detection using shallow textfeatures.
In Proceedings of the third ACM international conference on Web search and data mining, pages441?450.
ACM.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan, and Soumen Chakrabarti.
2009.
Collective annotation ofwikipedia entities in web text.
In Proceedings of the 15th ACM SIGKDD international conference on Knowledgediscovery and data mining, pages 457?466.
ACM.Yogan Jaya Kumar and Naomie Salim.
2012.
Automatic multi document summarization approaches.
Journal ofComputer Science, 8(1).Thomas Lin, Oren Etzioni, et al.
2010.
Identifying functional relations in web text.
In Proceedings of the2010 Conference on Empirical Methods in Natural Language Processing, pages 1266?1276.
Association forComputational Linguistics.Christopher D Manning, Prabhakar Raghavan, and Hinrich Sch?utze.
2008.
Introduction to information retrieval,volume 1.
Cambridge University Press Cambridge.Enrico Motta, Simon Buckingham Shum, and John Domingue.
2000.
Ontology-driven document enrichment:principles, tools and applications.
International Journal of Human-Computer Studies, 52(6):1071?1109.Vivi Nastase, Michael Strube, Benjamin B?orschinger, C?acilia Zirn, and Anas Elghafari.
2010.
Wikinet: A verylarge scale multi-lingual concept network.
In LREC.Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry Winograd.
1999.
The pagerank citation ranking: Bring-ing order to the web.Patrick Pantel and Ariel Fuxman.
2011.
Jigs and lures: Associating web queries with structured entities.
InProceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human LanguageTechnologies-Volume 1, pages 83?92.
Association for Computational Linguistics.Alexandre Passant.
2007.
Using ontologies to strengthen folksonomies and enrich information retrieval in we-blogs.
In Proceedings of International Conference on Weblogs and Social Media.Alan Ritter, Oren Etzioni, et al.
2010.
A latent dirichlet allocation method for selectional preferences.
In Proceed-ings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 424?434.
Associationfor Computational Linguistics.Jean-Paul Sansonnet and Franc?ois Bouchet.
2010.
Extraction of agent psychological behaviors from glosses ofwordnet personality adjectives.
In Proc.
of the 8th European Workshop on Multi-Agent Systems (EUMAS10).Stefan Schoenmackers, Oren Etzioni, Daniel S Weld, and Jesse Davis.
2010.
Learning first-order horn clausesfrom web text.
In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,pages 1088?1098.
Association for Computational Linguistics.926Prithviraj Sen. 2012.
Collective context-aware topic models for entity disambiguation.
In Proceedings of the 21stinternational conference on World Wide Web, pages 729?738.
ACM.Koun-Tem Sun, Yueh-Min Huang, and Ming-Chi Liu.
2011.
A wordnet-based near-synonyms and similar-lookingword learning system.
Educational Technology & Society, 14(1):121?134.Ellen M Voorhees, Donna K Harman, et al.
2005.
TREC: Experiment and evaluation in information retrieval,volume 63.
MIT press Cambridge.Xudong Wang and Azman O Lim.
2008.
Ieee 802.11 s wireless mesh networks: Framework and challenges.
AdHoc Networks, 6(6):970?984.Fei Wang and Changshui Zhang.
2008.
Label propagation through linear neighborhoods.
Knowledge and DataEngineering, IEEE Transactions on, 20(1):55?67.Fei Wu and Daniel S Weld.
2010.
Open information extraction using wikipedia.
In Proceedings of the 48th An-nual Meeting of the Association for Computational Linguistics, pages 118?127.
Association for ComputationalLinguistics.Wei Zhang, Yan Chuan Sim, Jian Su, and Chew Lim Tan.
2011.
Entity linking with effective acronym expansion,instance selection and topic modeling.
In Proceedings of the Twenty-Second international joint conference onArtificial Intelligence-Volume Volume Three, pages 1909?1914.
AAAI Press.Zhicheng Zheng, Fangtao Li, Minlie Huang, and Xiaoyan Zhu.
2010.
Learning to link entities with knowledgebase.
In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of theAssociation for Computational Linguistics, pages 483?491.
Association for Computational Linguistics.Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, and Ji-Rong Wen.
2009.
Statsnowball: a statistical approach toextracting entity relationships.
In Proceedings of the 18th international conference on World wide web, pages101?110.
ACM.927
