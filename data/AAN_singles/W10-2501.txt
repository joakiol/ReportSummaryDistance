Proceedings of the 2010 Workshop on Applications of Tree Automata in Natural Language Processing, ACL 2010, pages 1?9,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsPreservation of Recognizability forSynchronous Tree Substitution GrammarsZolta?n Fu?lo?pDepartment of Computer ScienceUniversity of SzegedSzeged, HungaryAndreas MalettiDepartament de Filologies Roma`niquesUniversitat Rovira i VirgiliTarragona, SpainHeiko VoglerFaculty of Computer ScienceTechnische Universita?t DresdenDresden, GermanyAbstractWe consider synchronous tree substitutiongrammars (STSG).
With the help of acharacterization of the expressive powerof STSG in terms of weighted tree bimor-phisms, we show that both the forward andthe backward application of an STSG pre-serve recognizability of weighted tree lan-guages in all reasonable cases.
As a con-sequence, both the domain and the rangeof an STSG without chain rules are recog-nizable weighted tree languages.1 IntroductionThe syntax-based approach to statistical machinetranslation (Yamada and Knight, 2001) becomesmore and more competitive in machine transla-tion, which is a subfield of natural language pro-cessing (NLP).
In this approach the full parse treesof the involved sentences are available to the trans-lation model, which can base its decisions on thisrich structure.
In the competing phrase-based ap-proach (Koehn et al, 2003) the translation modelonly has access to the linear sentence structure.There are two major classes of syntax-basedtranslation models: tree transducers and synchro-nous grammars.
Examples in the former classare the top-down tree transducer (Rounds, 1970;Thatcher, 1970), the extended top-down tree trans-ducer (Arnold and Dauchet, 1982; Galley et al,2004; Knight and Graehl, 2005; Graehl et al,2008; Maletti et al, 2009), and the extendedmulti bottom-up tree transducer (Lilin, 1981; En-gelfriet et al, 2009; Maletti, 2010).
The lat-ter class contains the syntax-directed transduc-tions of Lewis II and Stearns (1968), the gen-eralized syntax-directed transductions (Aho andUllman, 1969), the synchronous tree substitu-tion grammar (STSG) by Schabes (1990) and thesynchronous tree adjoining grammar (STAG) byAbeille?
et al (1990) and Shieber and Schabes(1990).
The first bridge between those two classeswere established in (Martin and Vere, 1970).
Fur-ther comparisons can be found in (Shieber, 2004)for STSG and in (Shieber, 2006) for STAG.One of the main challenges in NLP is the am-biguity that is inherent in natural languages.
Forinstance, the sentence ?I saw the man with thetelescope?
has several different meanings.
Someof them can be distinguished by the parse tree,so that probabilistic parsers (Nederhof and Satta,2006) for natural languages can (partially) achievethe disambiguation.
Such a parser returns a setof parse trees for each input sentence, and inaddition, each returned parse tree is assigned alikelihood.
Thus, the result can be seen as amapping from parse trees to probabilities wherethe impossible parses are assigned the probabil-ity 0.
Such mappings are called weighted tree lan-guages, of which some can be finitely representedby weighted regular tree grammars (Alexandrakisand Bozapalidis, 1987).
Those weighted treelanguages are recognizable and there exist algo-rithms (Huang and Chiang, 2005) that efficientlyextract the k-best parse trees (i.e., those with thehighest probability) for further processing.In this paper we consider synchronized tree sub-stitution grammars (STSG).
To overcome a techni-cal difficulty we add (grammar) nonterminals tothem.
Since an STSG often uses the nontermi-nals of a context-free grammar as terminal sym-bols (i.e., its derived trees contain both termi-nal and nonterminal symbols of the context-freegrammar), we call the newly added (grammar)nonterminals of the STSG states.
Substitution doesno longer take place at synchronized nonterminals(of the context-free grammar) but at synchronizedstates (one for the input and one for the outputside).
The states themselves will not appear in thefinal derived trees, which yields that it is sufficientto assume that only identical states are synchro-1nized.
Under those conventions a rule of an STSGhas the form q ?
(s, t, V, a) where q is a state,a ?
R?0 is the rule weight, s is an input tree thatcan contain states at the leaves, and t is an outputtree that can also contain states.
Finally, the syn-chronization is defined by V , which is a bijectionbetween the state-labeled leaves of s and t. Werequire that V only relates identical states.The rules of an STSG are applied in a step-wisemanner.
Here we use a derivation relation to definethe semantics of an STSG.
It can be understood asthe synchronization of the derivation relations oftwo regular tree grammars (Ge?cseg and Steinby,1984; Ge?cseg and Steinby, 1997) where the syn-chronization is done on nonterminals (or states) inthe spirit of syntax-directed transductions (LewisII and Stearns, 1968).
Thus each sentential formis a pair of (nonterminal-) connected trees.An STSG G computes a mapping ?G , calledits weighted tree transformation, that assigns aweight to each pair of input and output trees,where both the input and output tree may not con-tain any state.
This transformation is obtained asfollows: We start with two copies of the initialstate that are synchronized.
Given a connected treepair (?, ?
), we can apply the rule q ?
(s, t, V, a)to each pair of synchronized states q.
Such an ap-plication replaces the selected state q in ?
by s andthe corresponding state q in ?
by t. All the re-maining synchronized states and the synchronizedstates of V remain synchronized.
The result isa new connected tree pair.
This step charges theweight a.
The weights of successive applications(or steps) are multiplied to obtain the weight of thederivation.
The weighted tree transformation ?Gassigns to each pair of trees the sum of all weightsof derivations that derive that pair.Shieber (2004) showed that for every classicalunweighted STSG there exists an equivalent bi-morphism (Arnold and Dauchet, 1982).
The con-verse result only holds up to deterministic rela-belings (Ge?cseg and Steinby, 1984; Ge?cseg andSteinby, 1997), which remove the state informa-tion from the input and output tree.
It is this dif-ference that motivates us to add states to STSG.We generalize the result of Shieber (2004) andprove that every weighted tree transformation thatis computable by an STSG can also be computedby a weighted bimorphism and vice versa.Given an STSG and a recognizable weightedtree language ?
of input trees, we investigate un-der which conditions the weighted tree languageobtained by applying G to ?
is again recognizable.In other words, we investigate under which condi-tions the forward application of G preserves rec-ognizability.
The same question is investigated forbackward application, which is the correspondingoperation given a recognizable weighted tree lan-guage of output trees.
Since STSG are symmet-ric (i.e., input and output can be exchanged), theresults for backward application can be obtainedeasily from the results for forward application.Our main result is that forward application pre-serves recognizability if the STSG G is output-productive, which means that each rule of G con-tains at least one output symbol that is not a state.Dually, backward application preserves recogniz-ability if G is input-productive, which is the anal-ogous property for the input side.
In fact, those re-sults hold for weights taken from an arbitrary com-mutative semiring (Hebisch and Weinert, 1998;Golan, 1999), but we present the results only forprobabilities.2 Preliminary definitionsIn this contribution we will work with rankedtrees.
Each symbol that occurs in such a treehas a fixed rank that determines the number ofchildren of nodes with this label.
Formally, let?
be a ranked alphabet, which is a finite set ?together with a mapping rk?
: ?
?
N that asso-ciates a rank rk?(?)
with every ?
?
?.
We let?k = {?
?
?
| rk?(?)
= k} be the set contain-ing all symbols in ?
that have rank k. A ?-treeindexed by a set Q is a tree with nodes labeled byelements of ?
?
Q, where the nodes labeled bysome ?
?
?
have exactly rk?(?)
children and thenodes with labels ofQ have no children.
Formally,the set T?
(Q) of (term representations of) ?-treesindexed by a set Q is the smallest set T such that?
Q ?
T and?
?
(t1, .
.
.
, tk) ?
T for every ?
?
?k andt1, .
.
.
, tk ?
T .We generally write ?
instead of ?
() for all ?
?
?0.We frequently work with the set pos(t) of po-sitions of a ?-tree t, which is defined as fol-lows.
If t ?
Q, then pos(t) = {?
}, and ift = ?
(t1, .
.
.
, tk), thenpos(t) = {?}
?
{iw | 1 ?
i ?
k,w ?
pos(ti)} .Thus, each position is a finite (possibly empty) se-quence of natural numbers.
Clearly, each position2designates a node of the tree, and vice versa.
Thuswe identify nodes with positions.
As usual, a leafis a node that has no children.
The set of all leavesof t is denoted by lv(t).
Clearly, lv(t) ?
pos(t).The label of a position w ?
pos(t) is denotedby t(w).
Moreover, for every A ?
?
?
Q, letposA(t) = {w ?
pos(t) | t(w) ?
A} andlvA(t) = posA(t) ?
lv(t) be the sets of po-sitions and leaves that are labeled with an ele-ment of A, respectively.
Let t ?
T?
(Q) andw1, .
.
.
, wk ?
lvQ(t) be k (pairwise) differentleaves.
We write t[w1 ?
t1, .
.
.
, wk ?
tk] or justt[wi ?
ti | 1 ?
i ?
k] with t1, .
.
.
, tk ?
T?
(Q)for the tree obtained from t by replacing, for every1 ?
i ?
k, the leaf wi with the tree ti.For the rest of this paper, let ?
and ?
be twoarbitrary ranked alphabets.
To avoid consistencyissues, we assume that a symbol ?
that occurs inboth ?
and ?
has the same rank in ?
and ?
; i.e.,rk?(?)
= rk?(?).
A deterministic relabeling isa mapping r : ?
?
?
such that r(?)
?
?k forevery ?
?
?k.
For a tree s ?
T?, the relabeledtree r(s) ?
T?
is such that pos(r(s)) = pos(s)and(r(s))(w) = r(s(w)) for every w ?
pos(s).The class of tree transformations computed by de-terministic relabelings is denoted by dREL.A tree language (over ?)
is a subset of T?.
Cor-respondingly, a weighted tree language (over ?
)is a mapping ?
: T?
?
R?0.
A weighted treetransformation (over ?
and ?)
is a mapping?
: T?
?
T?
?
R?0.
Its inverse is the weightedtree transformation ?
?1 : T??T?
?
R?0, whichis defined by ?
?1(t, s) = ?
(s, t) for every t ?
T?and s ?
T?.3 Synchronous tree substitutiongrammars with statesLet Q be a finite set of states with a distinguishedinitial state qS ?
Q.
A connected tree pair is atuple (s, t, V, a) where s ?
T?
(Q), t ?
T?
(Q),and a ?
R?0.
Moreover, V : lvQ(s) ?
lvQ(t) isa bijective mapping such that s(u) = t(v) for ev-ery (u, v) ?
V .
We will often identify V with itsgraph.
Intuitively, a connected tree pair (s, t, V, a)is a pair of trees (s, t) with a weight a such thateach node labeled by a state in s has a correspond-ing node in t, and vice versa.
Such a connectedtree pair (s, t, V, a) is input-productive and output-productive if s /?
Q and t /?
Q, respectively.
LetConn denote the set of all connected tree pairs thatuse the index setQ.
Moreover, let Connp ?
Conncontain all connected tree pairs that are input- oroutput-productive.A synchronous tree substitution grammar G(with states) over ?, ?, and Q (for short: STSG),is a finite set of rules of the form q ?
(s, t, V, a)where q ?
Q and (s, t, V, a) ?
Connp.
We calla rule q ?
(s, t, V, a) a q-rule, of which q and(s, t, V, a) are the left-hand and right-hand side,respectively, and a is its weight.
The STSG G isinput-productive (respectively, output-productive)if each of its rules is so.
To simplify the followingdevelopment, we assume (without loss of general-ity) that two different q-rules differ on more thanjust their weight.1To make sure that we do not account essentiallythe same derivation twice, we have to use a deter-ministic derivation mode.
Since the choice is im-material, we use the leftmost derivation mode forthe output component t of a connected tree pair(s, t, V, a).
For every (s, t, V, a) ?
Conn suchthat V 6= ?, the leftmost output position is thepair (w,w?)
?
V , where w?
is the leftmost (i.e.,the lexicographically smallest) position of lvQ(t).Next we define derivations.
The derivation re-lation induced by G is the binary relation ?Gover Conn such that?
= (s1, t1, V1, a1)?G (s2, t2, V2, a2) = ?if and only if the leftmost output position of ?
is(w,w?)
?
V1 and there exists a rules1(w)?
(s, t, V, a) ?
Gsuch that?
s2 = s1[w ?
s] and t2 = t1[w?
?
t],?
V2 = (V1 \ {(w,w?)})
?
V ?
whereV ?
= {(ww1, w?w2) | (w1, w2) ?
V }, and?
a2 = a1 ?
a.A sequence D = (?1, .
.
.
, ?n) ?
Connn is aderivation of (s, t, V, a) ?
Conn from q ?
Q if?
?1 = (q, q, {(?, ?
)}, 1),?
?n = (s, t, V, a), and?
?i ?G ?i+1 for every 1 ?
i ?
n?
1.The set of all such derivations is denoted byDqG(s, t, V, a).For every q ?
Q, s ?
T?
(Q), t ?
T?
(Q), andbijection V : lvQ(s)?
lvQ(t), let?
qG(s, t, V ) =?a?R?0,D?DqG(s,t,V,a)a .1Formally, q ?
(s, t, V, a) ?
G and q ?
(s, t, V, b) ?
Gimplies a = b.3o 1?
o ?G,lo?e o6?1?
?o e?G,lo?e ?18?1???
e?G,lo?
?o o?
36?1???
?o o?G,lo?
?o ??
108?1???
??
o?G,lo???
??
324?1???
??
?Figure 1: Example derivation with the STSG G of Example 1.Finally, the weighted tree transformation com-puted by G is the weighted tree transformation?G : T?
?
T?
?
R?0 with ?G(s, t) = ?qSG (s, t, ?
)for every s ?
T?
and t ?
T?.
As usual, wecall two STSG equivalent if they compute the sameweighted tree transformation.
We observe thatevery STSG is essentially a linear, nondeletingweighted extended top-down (or bottom-up) treetransducer (Arnold and Dauchet, 1982; Graehl etal., 2008; Engelfriet et al, 2009) without (both-sided) epsilon rules, and vice versa.Example 1.
Let us consider the STSG G over?
= ?
= {?, ?}
and Q = {e, o} where qS = o,rk(?)
= 2, and rk(?)
= 0.
The STSG G consistsof the following rules where V = {(1, 2), (2, 1)}and id = {(1, 1), (2, 2)}:o?
(?
(o, e), ?
(e, o), V, 1/3) (?1)o?
(?
(e, o), ?
(o, e), V, 1/6) (?2)o?
(?
(e, o), ?
(e, o), id, 1/6) (?3)o?
(?, ?, ?, 1/3) (?4)e?
(?
(e, e), ?
(e, e), V, 1/2) (?5)e?
(?
(o, o), ?
(o, o), V, 1/2) (?6)Figure 1 shows a derivation induced by G. It caneasily be checked that ?G(s, t) = 16?3?2?3?3 wheres = ?(?
(?, ?
), ?)
and t = ?
(?, ?
(?, ?)).
More-over, ?G(s, s) = ?G(s, t).
If ?qG(s, t, ?)
6= 0 withq ?
{e, o}, then s and t have the same numberof ?-labeled leaves.
This number is odd if q = o,otherwise it is even.
Moreover, at every positionw ?
pos(s), the left and right subtrees s1 and s2are interchanged in s and t (due to V in the rules?1, ?2, ?5, ?6) except if s1 and s2 contain an evenand odd number, respectively, of ?-labeled leaves.In the latter case, the subtrees can be interchangedor left unchanged (both with probability 1/6).4 Recognizable weighted tree languagesNext, we recall weighted regular tree grammars(Alexandrakis and Bozapalidis, 1987).
To keepthe presentation simple, we identify WRTG withparticular STSG, in which the input and the out-put components are identical.
More precisely, aweighted regular tree grammar over ?
and Q (forshort: WRTG) is an STSG G over ?, ?, and Qwhere each rule has the form q ?
(s, s, id, a)where id is the suitable (partial) identity mapping.It follows that s /?
Q, which yields that we do nothave chain rules.
In the rest of this paper, we willspecify a rule q ?
(s, s, id, a) of a WRTG sim-ply by qa?
s. For every q ?
Q, we define theweighted tree language ?qG : T?
(Q) ?
R?0 gen-erated by G from q by ?qG(s) = ?qG(s, s, idlvQ(s))for every s ?
T?
(Q), where idlvQ(s) is the iden-tity on lvQ(s).
Moreover, the weighted tree lan-guage ?G : T?
?
R?0 generated by G is definedby ?G(s) = ?qSG (s) for every s ?
T?.A weighted tree language ?
: T?
?
R?0 isrecognizable if there exists a WRTG G such that?
= ?G .
We note that our notion of recognizabil-ity coincides with the classical one (Alexandrakisand Bozapalidis, 1987; Fu?lo?p and Vogler, 2009).Example 2.
We consider the WRTGK over the in-put alphabet ?
= {?, ?}
and P = {p, q} withqS = q, rk(?)
= 2, and rk(?)
= 0.
The WRTG Kcontains the following rules:q0.4?
?
(p, ?)
q0.6?
?
p1?
?
(?, q) (?1?
?3)Let s ?
T?
be such that ?K(s) 6= 0.
Then s is athin tree with zig-zag shape; i.e., there exists n ?
1such that pos(s) contains exactly the positions:?
(12)i for every 0 ?
i ?
bn?12 c, and?
(12)i1, (12)i2, and (12)i11 for every integer0 ?
i ?
bn?32 c.The integer n can be understood as the length ofa derivation that derives s from q.
Some example4????
?????
???
??
?weight: 0.6 weight: 0.24 weight: 0.096Figure 2: Example trees and their weight in ?Gwhere G is the WRTG of Example 2.trees with their weights are displayed in Figure 2.Proposition 3.
For every WRTG G there is anequivalent WRTG G?
in normal form, in which theright-hand side of every rule contains exactly onesymbol of ?.Proof.
We can obtain the statement by a trivial ex-tension to the weighted case of the approach usedin Lemma II.3.4 of (Ge?cseg and Steinby, 1984)and Section 6 of (Ge?cseg and Steinby, 1997).5 STSG and weighted bimorphismsIn this section, we characterize the expressivepower of STSG in terms of weighted bimorphisms.This will provide a conceptually clear pattern forthe construction in our main result (see Theo-rem 6) concerning the closure of recognizableweighted tree languages under forward and back-ward application.
For this we first recall tree ho-momorphisms.
Let ?
and ?
be two ranked al-phabets.
Moreover, let h : ?
?
T?
?
(N?
)?be a mapping such that h(?)
= (s, u1, .
.
.
, uk)for every ?
?
?k where s ?
T?
and all leavesu1, .
.
.
, uk ?
lv(s) are pairwise different.
Themapping h induces the (linear and complete) treehomomorphism h?
: T?
?
T?, which is defined byh?(?
(d1, .
.
.
, dk)) = s[u1 ?
d?1, .
.
.
, uk ?
d?k]for every ?
?
?k and d1, .
.
.
, dk ?
T?
withh(?)
= (s, u1, .
.
.
, uk) and d?i = h?
(di) for ev-ery 1 ?
i ?
k. Moreover, every (linear andcomplete) tree homomorphism is induced in thisway.
In the rest of this paper we will not distin-guish between h and h?
and simply write h insteadof h?.
The homomorphism h is order-preservingif u1 < ?
?
?
< uk for every ?
?
?k whereh(?)
= (s, u1, .
.
.
, uk).
Finally, we note thatevery ?
?
dREL can be computed by a order-preserving tree homomorphism.A weighted bimorphism B over ?
and ?
con-sists of a WRTG K over ?
and P and two tree ho-T?
R?0T?
?
T?
(hin, hout)?K?BFigure 3: Illustration of the semantics of the bi-morphism B.momorphismshin : T?
?
T?
and hout : T?
?
T?
.The bimorphism B computes the weighted treetransformation ?B : T?
?
T?
?
R?0 with?B(s, t) =?d?h?1in (s)?h?1out(t)?K(d)for every s ?
T?
and t ?
T?.Without loss of generality, we assume that ev-ery bimorphism B is presented by an WRTG K innormal form and an order-preserving output ho-momorphism hout.
Next, we prepare the relationbetween STSG and weighted bimorphisms.
LetG be an STSG over ?, ?, and Q.
Moreover, letB be a weighted bimorphism over ?
and ?
con-sisting of (i) K over ?
and P in normal form,(ii) hin, and (iii) order-preserving hout.
We saythat G and B are related if Q = P and thereis a bijection ?
: G ?
K such that, for everyrule ?
?
G with ?
= (q ?
(s, t, V, a)) and?(?)
= (pa?
?
(p1, .
.
.
, pk)) we have?
p = q,?
hin(?)
= (s, u1, .
.
.
, uk),?
hout(?)
= (t, v1, .
.
.
, vk),?
V = {(u1, v1), .
.
.
, (uk, vk)}, and?
s(ui) = pi = t(vi) for every 1 ?
i ?
k.Let G and B be related.
The following three easystatements can be used to prove that G and B areequivalent:1.
For every derivation D ?
DqG(s, t, ?, a) withq ?
Q, s ?
T?, t ?
T?, a ?
R?0, there existsd ?
T?
and a derivation D?
?
DqK(d, d, ?, a)such that hin(d) = s and hout(d) = t.2.
For every d ?
T?
and D?
?
DqK(d, d, ?, a)with q ?
Q and a ?
R?0, there exists aderivation D ?
DqG(hin(d), hout(d), ?, a).3.
The mentioned correspondence on deriva-tions is a bijection.Given an STSG G, we can easily construct aweighted bimorphism B such that G and B are re-lated, and vice versa.
Hence, STSG and weighted5bimorphisms are equally expressive, which gener-alizes the corresponding characterization result inthe unweighted case by Shieber (2004), which wewill state after the introduction of STSG?.Classical synchronous tree substitution gram-mars (STSG?)
do not have states.
An STSG?
canbe seen as an STSG by considering every substitu-tion site (i.e., each pair of synchronised nontermi-nals) as a state.2 We illustrate this by means of anexample here.
Let us consider the STSG?
G withthe following rules:?
(S(?,B?
), S(D?, ?))
with weight 0.2?
(B(?,B?
), D(?,D?))
with weight 0.3?
(B(?
), D(?))
with weight 0.4.The substitution sites are marked with ?.
Anyrule with root A can be applied to a substitutionsite A?.
An equivalent STSG G?
has the rules:?S, S?
?
(S(?, ?B,D?
), S(?B,D?, ?
), V, 0.2)?B,D?
?
(B(?, ?B,D?
), D(?, ?B,D?
), V ?, 0.3)?B,D?
?
(B(?
), D(?
), ?, 0.4) ,where V = {(2, 1)} and V ?
= {(2, 2)}.
It is easyto see that G and G?
are equivalent.Let ?
= {?, ?
?, ??
?, ?, ?}
where ?, ?
?, ???
?
?1and ?, ?
?
?0 (and ??
6= ???
and ?
6= ?).
We write?m(t) with t ?
T?
for the tree ?(?
?
?
?
(t) ?
?
? )
con-taining m occurrences of ?
above t. STSG?
have acertain locality property, which yields that STSG?cannot compute transformations like?
(s, t) =????
?1 if s = ??(?m(?))
= tor s = ???(?m(?))
= t0 otherwisefor every s, t ?
T?.
The non-local feature is thecorrespondence between the symbols ??
and ?
(inthe first alternative) and the symbols ???
and ?
(inthe second alternative).
An STSG that computes ?is presented in Figure 4.Theorem 4.
Let ?
be a weighted tree transforma-tion.
Then the following are equivalent.1.
?
is computable by an STSG.2.
?
is computable by a weighted bimorphism.3.
There exists a STSG?
G and deterministic re-labelings r1 and r2 such that?
(s, t) =?s?
?r?11 (s),t?
?r?12 (t)?G(s?, t?)
.2To avoid a severe expressivity restriction, several initialstates are allowed for an STSG?.The inverse of an STSG computable weightedtree transformation can be computed by an STSG.Formally, the inverse of the STSG G is the STSGG?1 = {(t, s, V ?1, a) | (s, t, V, a) ?
G}where V ?1 is the inverse of V .
Then ?G?1 = ?
?1G .6 Forward and backward applicationLet us start this section with the definition of theconcepts of forward and backward application of aweighted tree transformation ?
: T?
?
T?
?
R?0to weighted tree languages ?
: T?
?
R?0 and?
: T?
?
R?0.
We will give general definitionsfirst and deal with the potentially infinite sumslater.
The forward application of ?
to ?
is theweighted tree language ?(?)
: T?
?
R?0, whichis defined for every t ?
T?
by(?(?
))(t) =?s?T??
(s) ?
?
(s, t) .
(1)Dually, the backward application of ?
to ?
isthe weighted tree language ??1(?)
: T?
?
R?0,which is defined for every s ?
T?
by(??1(?
))(s) =?t?T??
(s, t) ?
?
(t) .
(2)In general, the sums in Equations (1) and (2) canbe infinite.
Let us recall the important propertythat makes them finite in our theorems.Proposition 5.
For every input-productive (resp.,output-productive) STSG G and every tree s ?
T?
(resp., t ?
T?
), there exist only finitely manytrees t ?
T?
(respectively, s ?
T?)
such that?G(s, t) 6= 0.Proof sketch.
If G is input-productive, then eachderivation step creates at least one input symbol.Consequently, any derivation for the input tree scan contain at most as many steps as there arenodes (or positions) in s. Clearly, there are onlyfinitely many such derivations, which proves thestatement.
Dually, we can obtain the statement foroutput-productive STSG.In the following, we will consider forward ap-plications ?G(?)
where G is an output-productiveSTSG and ?
is recognizable, which yields that (1)is well-defined by Proposition 5.
Similarly, weconsider backward applications ?
?1G (?)
where Gis input-productive and ?
is recognizable, whichagain yields that (2) is well-defined by Proposi-tion 5.
The question is whether ?G(?)
and ?
?1G (?
)6q0 ???q11??
?q1q0 ????q21???
?q2q1 ??q11?
?q1q2 ??q21?
?q2q1 ?
?1?
?q2 ?
?1?
?Figure 4: STSG computing the weighted tree transformation ?
with initial state q0.are again recognizable.
To avoid confusion, weoccasionally use angled parentheses as in ?p, q?instead of standard parentheses as in (p, q).
More-over, for ease of presentation, we identify the ini-tial state qS with ?qS, qS?.Theorem 6.
Let G be an STSG over ?, ?, and Q.Moreover, let ?
: T?
?
R?0 and ?
: T?
?
R?0be recognizable weighted tree languages.1.
If G is output-productive, then ?G(?)
is rec-ognizable.2.
If G is input-productive, then ?
?1G (?)
is rec-ognizable.Proof.
For the first item, let K be a WRTG over?
and P such that ?
= ?K.
Without loss of gen-erality, we suppose that K is in normal form.Intuitively, we take each rule q ?
(s, t, V, a)of G and run the WRTG K with every start state pon the input side s of the rule.
In this way, weobtain a weight b.
The WRTG will reach the stateleaves of s in certain states, which we then trans-fer to the linked states in t to obtain t?.
Finally, weremove the input side and obtain a rule ?p, q?ab?
t?for the WRTG L that represents the forward ap-plication.
We note that the same rule of L mightbe constructed several times.
If this happens, thenwe replace the several copies by one rule whoseweight is the sum of the weights of all copies.As already mentioned the initial state is ?qS, qS?.Clearly, this approach is inspired (and made rea-sonable) by the bimorphism characterization.
Wecan take the HADAMARD product of the WRTG ofthe bimorphism with the inverse image of ?K un-der its input homomorphism.
Then we can simplyproject to the output side.
Our construction per-forms those three steps at once.
The whole processis illustrated in Figure 5.Formally, we construct the WRTG L over ?
andP?Qwith the following rules.
Let p ?
P , q ?
Q,and t?
?
T?
(P ?
Q).
Then ?p, q?c?
t?
is a rulein L?, wherec =?(q?(s,t,V,a))?GV={(u1,v1),...,(uk,vk)}p1,...,pk?Pt?=t[vi?
?pi,t(vi)?|1?i?k]b=?pK(s[ui?pi|1?i?k])ab .This might create infinitely many rules in L?, butclearly only finitely many will have a weight dif-ferent from 0.
Thus, we can obtain the finite ruleset L by removing all rules with weight 0.The main statement to prove is the following:for every t ?
T?
(Q) with lvQ(t) = {v1, .
.
.
, vk},p, p1, .
.
.
, pk ?
P , and q ?
Q?s?T?(Q)u1,...,uk?lvQ(s)?pK(s?)
?
?
qG(s, t, V ) = ?
?p,q?L (t?)
,where?
V = {(u1, v1), .
.
.
, (uk, vk)},?
s?
= s[ui ?
pi | 1 ?
i ?
k], and?
t?
= t[vi ?
?pi, t(vi)?
| 1 ?
i ?
k].In particular, for t ?
T?
we obtain?s?T?
?pK(s) ?
?qG(s, t, ?)
= ?
?p,q?L (t) ,which yields(?G(?K))(t) =?s?T?
?K(s) ?
?G(s, t)=?s?T?
?qSK (s) ?
?qSG (s, t, ?
)= ?
?qS,qS?L (t) = ?L(t) .In the second item G is input-productive.
ThenG?1 is output-productive and ?
?1G (?)
= ?G?1(?
).Hence the first statement proves that ?
?1G (?)
isrecognizable.Example 7.
As an illustration of the constructionin Theorem 6, let us apply the STSG G of Exam-ple 1 to the WRTG K over ?
and P = {p, qS, q?
}and the following rules:qS25?
?
(p, q?)
qS35?
?p1?
?
(q?, qS) q?1?
?
.In fact, K is in normal form and is equivalent tothe WRTG of Example 2.
Using the constructionin the proof of Theorem 6 we obtain the WRTG Lover ?
and P ?Q with Q = {e, o}.
We will only7o??
?o o?
136???
?o o?q, o????
?q?, o?
?q, o??
136 ?
25???
?o o?q, o?136 ?25??????
?
?q, o?
?q?, o?Figure 5: Illustration of the construction in the proof of Theorem 6 using the WRTG K of Example 7:some example rule (left), run of K on the input side of the rule (middle), and resulting rule (right).q1115??
?q2 q3q1115??
?q3 q2q115??
?q213??
?
q315??
?q1 q2Figure 6: WRTG constructed in Example 7.
Werenamed the states and calculated the weights.show rules of L that contribute to ?L.
To the rightof each rule we indicate from which state ofK andwhich rule of G the rule was constructed.
?qS, o?16 ?25??
?
(?q?, o?, ?p, e?)
qS, ?2?qS, o?16 ?25??
?
(?p, e?, ?q?, o?)
qS, ?3?qS, o?13 ?35??
?
qS, ?4?q?, o?13 ?1??
?
q?, ?4?p, e?12 ?25??
?
(?qS, o?, ?q?, o?)
p, ?6The initial state ofL is ?qS, o?.
It is easy to see thatevery t ?
T?
such that ?L(t) 6= 0 is thin, whichmeans that |pos(t) ?
Nn| ?
2 for every n ?
N.7 Domain and rangeFinally, let us consider the domain and range of aweighted tree transformation ?
: T??T?
?
R?0.Again, we first give general definitions and dealwith the infinite sums that might occur in themlater.
The domain dom(?)
of ?
and the rangerange(?)
of ?
are defined by(dom(?
))(s) =?u?T??
(s, u) (3)(range(?
))(t) =?u?T??
(u, t) (4)for every s ?
T?
and t ?
T?.
Obviously,the domain dom(?)
is the range range(?
?1) ofthe inverse of ?
.
Moreover, we can express thedomain dom(?)
of ?
as the backward applica-tion ?
?1(1) where 1 is the weighted tree languagethat assigns the weight 1 to each tree.
Note that 1is recognizable for every ranked alphabet.We note that the sums in Equations (3) and (4)might be infinite, but for input-productive (re-spectively, output-productive) STSG G the do-main dom(?G) (respectively, the range range(?G))are well-defined by Proposition 5.
Using those ob-servations and Theorem 6 we can obtain the fol-lowing statement.Corollary 8.
Let G be an STSG.
If G is input-productive, then dom(?G) is recognizable.
More-over, if G is output-productive, then range(?G) isrecognizable.Proof.
These statements follow directly from The-orem 6 with the help of the observation thatdom(?G) = ?
?1G (1) and range(?G) = ?G(1).ConclusionWe showed that every output-productive STSGpreserves recognizability under forward applica-tion.
Dually, every input-productive STSG pre-serves recognizability under backward applica-tion.
We presented direct and effective construc-tions for these operations.
Special cases of thoseconstructions can be used to compute the domainof an input-productive STSG and the range of anoutput-productive STSG.
Finally, we presented acharacterization of the power of STSG in terms ofweighted bimorphisms.AcknowledgementsZOLTA?N FU?LO?P and HEIKO VOGLER were finan-cially supported by the TA?MOP-4.2.2/08/1/2008-0008 program of the Hungarian National Devel-opment Agency.
ANDREAS MALETTI was finan-cially supported by the Ministerio de Educacio?n yCiencia (MEC) grant JDCI-2007-760.8ReferencesAnne Abeille?, Yves Schabes, and Aravind K. Joshi.1990.
Using lexicalized TAGs for machine trans-lation.
In Proc.
13th CoLing, volume 3, pages 1?6.University of Helsinki, Finland.Alfred V. Aho and Jeffrey D. Ullman.
1969.
Transla-tions on a context-free grammar.
In Proc.
1st STOC,pages 93?112.
ACM.Athanasios Alexandrakis and Symeon Bozapalidis.1987.
Weighted grammars and Kleene?s theorem.Inf.
Process.
Lett., 24(1):1?4.Andre?
Arnold and Max Dauchet.
1982.
Morphismeset bimorphismes d?arbres.
Theoret.
Comput.
Sci.,20(1):33?93.Joost Engelfriet, Eric Lilin, and Andreas Maletti.2009.
Extended multi bottom-up tree transducers?
composition and decomposition.
Acta Inform.,46(8):561?590.Zolta?n Fu?lo?p and Heiko Vogler.
2009.
Weighted treeautomata and tree transducers.
In Manfred Droste,Werner Kuich, and Heiko Vogler, editors, Handbookof Weighted Automata, chapter 9, pages 313?403.Springer.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
HLT-NAACL 2004, pages 273?280.
ACL.Ferenc Ge?cseg and Magnus Steinby.
1984.
Tree Au-tomata.
Akade?miai Kiado?, Budapest, Hungary.Ferenc Ge?cseg and Magnus Steinby.
1997.
Tree lan-guages.
In Grzegorz Rozenberg and Arto Salomaa,editors, Handbook of Formal Languages, chapter 1,pages 1?68.
Springer.Jonathan S. Golan.
1999.
Semirings and their Appli-cations.
Kluwer Academic.Jonathan Graehl, Kevin Knight, and Jonathan May.2008.
Training tree transducers.
ComputationalLinguistics, 34(3):391?427.Udo Hebisch and Hanns J. Weinert.
1998.
Semirings?
Algebraic Theory and Applications in ComputerScience.
World Scientific.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
9th IWPT, pages 53?64.
ACL.Kevin Knight and Jonathan Graehl.
2005.
Anoverview of probabilistic tree transducers for naturallanguage processing.
In Proc.
6th CICLing, volume3406 of LNCS, pages 1?24.
Springer.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.HLT-NAACL 2003, pages 48?54.
ACL.Philip M. Lewis II and Richard Edwin Stearns.
1968.Syntax-directed transductions.
J. ACM, 15(3):465?488.Eric Lilin.
1981.
Proprie?te?s de clo?ture d?une extensionde transducteurs d?arbres de?terministes.
In Proc.6th CAAP, volume 112 of LNCS, pages 280?289.Springer.Andreas Maletti, Jonathan Graehl, Mark Hopkins,and Kevin Knight.
2009.
The power of ex-tended top-down tree transducers.
SIAM J. Comput.,39(2):410?430.Andreas Maletti.
2010.
Why synchronous tree substi-tution grammars?
In Proc.
HLT-NAACL 2010.
ACL.to appear.David F. Martin and Steven A. Vere.
1970.
On syntax-directed transduction and tree transducers.
In Proc.2nd STOC, pages 129?135.
ACM.Mark-Jan Nederhof and Giorgio Satta.
2006.
Proba-bilistic parsing strategies.
J. ACM, 53(3):406?436.William C. Rounds.
1970.
Mappings and grammarson trees.
Math.
Systems Theory, 4(3):257?287.Yves Schabes.
1990.
Mathematical and computa-tional aspects of lexicalized grammars.
Ph.D. thesis,University of Pennsylvania.Stuart M. Shieber and Yves Schabes.
1990.
Syn-chronous tree-adjoining grammars.
In Proc.
13thCoLing, pages 253?258.
ACL.Stuart M. Shieber.
2004.
Synchronous grammars astree transducers.
In Proc.
TAG+7, pages 88?95.
Si-mon Fraser University.Stuart M. Shieber.
2006.
Unifying synchronous treeadjoining grammars and tree transducers via bimor-phisms.
In Proc.
11th EACL, pages 377?384.
ACL.James W. Thatcher.
1970.
Generalized2 sequentialmachine maps.
J. Comput.
System Sci., 4(4):339?367.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proc.
39thACL, pages 523?530.
ACL.9
