Proceedings of the 2010 Workshop on Cognitive Modeling and Computational Linguistics, ACL 2010, pages 45?53,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsClose = Relevant?
The Role of Context in Efficient Language ProductionTing Qian and T. Florian JaegerDepartment of Brain and Cognitive SciencesUniversity of RochesterRochester, NY 14627 United States{tqian,fjaeger}@bcs.rochester.eduAbstractWe formally derive a mathematical modelfor evaluating the effect of context rele-vance in language production.
The modelis based on the principle that distant con-textual cues tend to gradually lose theirrelevance for predicting upcoming linguis-tic signals.
We evaluate our model againsta hypothesis of efficient communication(Genzel and Charniak?s Constant EntropyRate hypothesis).
We show that the devel-opment of entropy throughout discoursesis described significantly better by a modelwith cue relevance decay than by previ-ous models that do not consider context ef-fects.1 IntroductionIn this paper, we present a study on the effectof context relevance decay on the entropy of lin-guistic signals in natural discourses.
Context rele-vance decay refers to the phenomenon that contex-tual cues that are distant from an upcoming event(e.g.
production of a new linguistic signal) are lesslikely to be relevant to the event, as discourse con-tents that are close to one another are likely to besemantically related.
One can also view the wordsand sentences in a discourse as time steps, wheredistant context becomes less relevant simply dueto normal forgetting over time (e.g.
activation de-cay in memory).
The present study investigateshow this decaying property of discourse contextmight affect the development of entropy of lin-guistic signals in discourses.
We first introducethe background on efficient language productionand then propose our hypothesis.1.1 Background on Efficient LanguageProductionThe metaphor ?communication channel?, bor-rowed from Shannon?s information theory (Shan-non, 1948), can be conceived of as an abstract en-tity that defines the constraints of language com-munication (e.g.
ambient noise, distortions in ar-ticulation).
For error free communication to occur,the ensemble of messages that a speaker may uttermust be encoded in a system of signals whose en-tropy is under the capacity of the communicationchannel.
Entropy of these signals, in this context,correlates with the average number of upcomingmessages that the speaker can choose from for aparticular signal (e.g.
a word to be spoken) givenpreceding discourse context.
In other words, ifthe average number of choices given any linguis-tic signal exceeds the channel capacity, it cannotbe guaranteed that the receiver can correctly inferthe originally intended message.
Such transmis-sion errors will reduce the efficiency of languagecommunication.Keeping the entropy of linguistic signals be-low the channel capacity alone is not efficient, forone can devise a code where each signal corre-sponds to a distinct message.
With a unique choiceper signal, this encoding achieves an entropy ofzero at the cost of requiring a look-up table thatis too large to be possible (cf.
Zipf (1935), whomakes a similar argument for meaning and form).In fact, the most efficient code requires languageusers to encode messages into signals of the en-tropy bounded by the capacity of the channel.
Oneimplication of this efficient encoding is that overtime, the entropy of the signals is constant.
Oneof the first studies to investigate such constancyis Genzel and Charniak (2002), in which the au-thors proposed the Constant Entropy Rate (CER)hypothesis: in written text, the entropy per sig-nal symbol is constant across sentence positions indiscourses.
That is, if we view sentence positionsas a measure of time steps, then the entropy perword at each step should be the same in order toachieve efficient communication (word is selectedas the unit of signal, although it does not have to45be case; cf.
Qian and Jaeger (2009)).The difficulty in testing this direct prediction iscomputationally specifying the code used by hu-man speakers to obtain a context-sensitive esti-mate of the entropy per word.
An ngram modeloverestimates the entropy of upcoming messagesby relying on only the preceding n-1 words withina sentence, while in reality the upcoming messageis also constrained by extra-sentential context thataccumulates within a discourse.
The more extra-sentential context that the ngram model ignores,the higher estimate for entropy will be.
Hence,the CER hypothesis indirectly predicts that theentropy of signals, as estimated by ngrams, willincrease across sentence positions.
While somestudies have found the predicted positive correla-tion between sentence position and the per-wordentropy of signals estimated by ngrams, most ofthem assumed the correlation to be linear (Genzeland Charniak, 2002; Genzel and Charniak, 2003;Keller, 2004; Piantadosi and Gibson, 2008).
How-ever, in previous work, we found that a log-linearregression model was a better fit for empirical datathan a simple linear regression model based ondata of 12 languages (Qian and Jaeger, under re-view).
Why this would be case remained a puzzle.Our research question is closely related to thisindirect prediction of the Constant Entropy Ratehypothesis.
Intuitively, the number of possiblemessages that a speaker can choose from for anupcoming signal in a discourse is often restrictedby the presence of discourse context.
Contex-tual cues in the preceding discourse can make theupcoming content more predictable and thus ef-fectively reduces signal entropy.
As previouslymentioned, however, different contextual cues, de-pending on how long ago they were provided, havevarious degrees of effectiveness in reducing sig-nal entropy.
Thus we ask the question whetherthe decay of context relevance could explain thesublinear relation between entropy and discourseprogress that has been observed in previous stud-ies.We formally derive two nonlinear models fortesting our Relevance Decay Hypothesis (intro-duced next).
In addition to the constant entropy as-sumption in CER, our model assumed that the rel-evance of early sentences in the discourse system-atically decays as a function of discourse progress.Our models provide the best fit to the distributionof entropy of signals, suggesting the availabilityof discourse context can affect the planning of therest of a discourse.1.2 Relevance Decay HypothesisWe hypothesize the sublinear relation between theentropy of signals, when estimated out of dis-course context (hereafter, out-of-context entropyof signals) using an ngram model, and sentenceposition (Piantadosi and Gibson, 2008; Qian andJaeger, under review) is due to the role of dis-course context (hereafter, context).
Consider thefollowing example.
Assume that context at the kthsentence position comes from the 1 .
.
.
k ?
1 sen-tences in the past.
If k is large enough, contextfrom the early sentences 1 .
.
.
i (i  k) is essen-tially no longer relevant.
Rather, the nearby k ?
isentences are contributing most of the discoursecontext.
As a result, the constraint on the entropyof signals at sentence position k is mostly due tothe nearby window of k ?
i sentences.
Then if welook ahead to the (k + 1)th sentence position andfollow the same steps of reasoning, context at thatpoint also mostly comes from the nearby windowof k?
i sentences (i.e.
(k+ 1)?
(i+ 1) = k?
i).Hence, for later sentence positions, the differencein available context is minimal.
Consequently,their out-of-context entropy of signals increasesat a very small rate.
On the other hand, when kis fairly small, to the extent that the k ?
i win-dow covers the entire preceding discourse, all ofthe 1 .
.
.
k ?
1 sentences are contributing relevantcontext.
As k increases, the number of preced-ing sentences increases, which results in a moresignificant change in relevant context, but the rel-evance of each individual sentence decreases withits distance to k, which results in a sublinear pat-tern of relevant context with respect to sentenceposition overall.
As we will show, the relation ofout-of-context entropy of signals to sentence posi-tion follows from the relation of relevant contextto sentence position, exhibiting a sublinear formas well.The problem of interest here is to specify howquickly the relevance of a preceding sentence de-cays as a function of its distance to a target sen-tence position k. We experimented with two formsof decay functions ?
power law decay and expo-nential decay.
It has been established that manytypes of human behaviors can be well described bythe power function (Wixted and Ebbesen, 1991),so we mainly focus on building a model under the46Language Training Data Test Datain words in sentences in words in sentences per positionDanish 154,514 5,640 8,048 270 18Dutch 50,309 3,255 2,105 90 6English 597,698 23,295 31,276 1155 77French 229,461 9,300 11,371 435 29Italian 97,198 4,245 4,524 225 15Mandarin Chinese 145,127 4,875 4,310 150 10Norwegian 89,724 4,125 2,973 150 10Portuguese 170,342 5,340 9,044 240 16Russian 398,786 18,075 20,668 930 62Spanish (Latin-American) 1,363,560 41,160 67,870 2,070 138Spanish (European) 255,366 7,485 8,653 240 16Swedish 266,348 11,535 13,369 555 37Table 1: Number of words and sentences in the training and test data for each of the twelve languages.The last column gives the number of sentences at each sentence position (which is identical to the numberof documents contained in the corpora).power law, and examine if the model under the ex-ponential law yields any difference.
Under the as-sumptions of true entropy rate is constant acrosssentences, we predict that our models will bet-ter characterize the changes in estimated entropyof signals than general regression models that areblind to the role of context.2 Methods2.1 DataWe used the Reuters Corpus Volume 1 and 2(Lewis et al, 2004).
The corpus contains about810,000 English news articles and over 487,000news articles in thirteen languages.
Because of in-consistent annotation, we excluded the data fromthree languages, Chinese, German, and Japanese.For Chinese, we substituted the Treebank Cor-pus (Xue et al, 2005) for the Reuters data, leav-ing us with twelve languages: Danish, Dutch,English, French, Italian, Mandarin Chinese, Nor-wegian, Portuguese, Russian, European Spanish,Latin-American Spanish, and Swedish.
In orderto estimate out-of-context entropy per word (i.e.per signal symbol) for each sentence position, ar-ticles were divided into a training set (95% of allstories) for training language models and a test set(the remaining 5%) for analysis (see Table 1 fordetails).
Out-of-context entropy per word was es-timated by computing the average log probabilityof sentences at that position, normalized by theirlengths in words (i.e.
for an individual sentencetoken s, the term to be averaged is ?
log p(s)length(s) bits perword).
Standard trigram language models wereused to compute these probabilities (Clarkson andRosenfeld, 1997).
The majority of the 12 lan-guages belong to the Indo-European family, whileMandarin Chinese is a Sino-Tibetan language.2.2 Modeling Relevance Decay of ContextFormally, we define the relevance of context in thesame unit as entropy of signals ?
bits per word.Let r0 denote the entropy of signals that efficientlyencode the ensemble of messages a speaker canchoose from for any sentence position, a constantunder the assumption of CER.
According to Infor-mation Theory, r0 is equivalent to the uncertaintyassociated with any sentence position if context isconsidered.
Thus, in error free communication,linguistic signals presented at the kth sentence po-sition are said to have resolved the uncertainty atk and therefore are r0-bit relevant at the kth sen-tence position.
Then, at the (k+i)th sentence posi-tion, these linguistic signals have become contextby definition and their relevance has decayed tosome r bits.
Our models start from defining thevalue of r as a function of the distance betweencontext and a target sentence position.2.2.1 Power-law Decay ModelIf the relevance of a cue q (e.g.
a preceding sen-tence), which is originally r0-bit relevant at po-sition kq, decays at the rate following the powerfunction, its remaining relevance at target sentenceposition k is:relevancepow(k, q) = r0(k ?
kq + 1)??
(1)In Equation (1), k > kq and ?
is the decay rate.This means at position k, the relevance of the cue47from the (k?1)th sentence is r0?2?
?-bit relevant;the relevance of the cue from the (k?2)th sentenceis r0 ?
3?
?-bit relevant, and so on.
As a result, therelevance of discourse-specific context at positionk is the marginalization of all cues up to qk?1:contextpow(k) = r0?qi?
{q1...qk?1}(k ?
kqi + 1)??
(2)The general trend predicted by Equation (2)is that discourse-specific context increases morerapidly at the beginning of a discourse and muchmore slowly towards the end due to the relevancedecay of distant cues.
Rewriting Equation (2) ina closed-form formula so that a model can be fit-ted to data is not a trivial task without knowing therate ?, but the paradox is that ?
has to be estimatedfrom the data.
As a workaround, we approximatedthe value of Equation (2) by computing a definiteintegral of Equation (1), where ?i is a shorthandfor k ?
kq + 1:contextpow(k) ??
k1r0?i?
?d?i= r0(k1??
?
11?
?)
(3)Equation (3) uses an integral to approximate thesum of a series defined as a function.
The resultis usually acceptable as long as ?
is greater than1 so that the series defined by Equation 1 is con-vergent (this assumption is empirically supported;see Figure 5).
Note that Equation (3) producesthe desirable effect that upon encountering thefirst sentence of a discourse, no discourse-specificcontextual cues are available to the speaker (i.e.context(1) = 0).Now that we know the maximum relevance ofcontext at sentence position k, we can predict theamount of out-of-context entropy of signals r(k)based on the idea of uncertainty again.
There arenew linguistic signals that are r0-bit relevant incontext at any sentence position.
In addition, wenow know context(k) bits of relevant context arealso available.
Thus, the sum of r0 and context(k)defines the maximum amount of out-of-contextuncertainty that can be resolved at sentence posi-tion k. Therefore, the out-of-context entropy ofsignals at k is at most:rpow(k) = context(k) + r0 (4)= r0k1??
?
11?
?+ r0Whether speakers will utilize all available con-text as predicted by Equation (4) is another de-bate.
Here we adopt the view that speakers aremaximally efficient in that they do make use ofall available context.
Thus, we make the predic-tion that out-of-context entropy of signals, as ob-served empirically from data, can be described bythis model.
Figure 1 shows the behavior of thisfunction with various parameter sets.2 4 6 8 10 12 1456789101112Sentence PositionModel?Predicted Entropy per Wordr0 = 5.5,?
= 2r0 = 5.5,?
= 2.2r0 = 5,?
= 2r0 = 5,?
= 2.2Figure 1: Schematic plots of the behavior of out-of-context entropy of signals assuming the decayof the relevance of context is a power function.2.2.2 Exponential Decay ModelThe second model assumes the relevance of con-text decays exponentially.
Following the same no-tations as before, the relevance of a cue q at posi-tion k is:relevanceexp(k, q) = r0e??
(k?kq) (5)The major difference between the power func-tion and the exponential one is that the relevanceof a contextual cue drops more slowly in the expo-nential case (Anderson, 1995).
The relevance ofall discourse-specific context for a speaker at k is:contextexp(k) = r0k?1?i=1e?
?i (6)48Equation (6) is the sum of a geometric progres-sion series.
We can write Equation (6) in a closed-form:contextexp(k) =r0e?
?
1(1?
e?(k?1)?)
(7)As a result, the out-of-context entropy of signalsis:rexp(k) =r0e?
?
1(1?
e?(k?1)?)
+ r0 (8)Figure 2 schematically shows the behavior ofthis function.
One can notice this function con-verges against a ceiling more quickly than thepower function.
Thus, this model makes a slightlydifferent prediction from the power law model.2 4 6 8 10 12 1456789101112Sentence PositionModel?Predicted Entropy per Wordr0 = 5.5,?
= 0.6r0 = 5.5,?
= 0.8r0 = 5,?
= 0.6r0 = 5,?
= 0.8Figure 2: Schematic plots of the behavior of out-of-context entropy of signals assuming the decayof the relevance of context is an exponential func-tion.2.3 Nonlinear Regression AnalysisTo test whether the proposed models (i.e.
Equa-tions 4 and 8) better characterize the data, webuilt nonlinear regression models with document-specific random effects, where the out-of-contextentropy of signals, rij , is regressed on sentenceposition, kj .
Based on the power law model, wehaverij = (?1+b1i)kj1?
?2 ?
11?
?2+(?1+b1i)+ij (9)where ?1 corresponds to r0, the theoretical con-stant entropy of signals under an ideal encod-ing.
b1i represents the document-specific devia-tions from the overall mean.
?2 corresponds to ?,the mean rate at which the relevance of a past cuedecays, which is unfortunately not considered forrandom effects for the practical purpose of makingcomputation feasible in the current work.
Finally,ij represents the errors independently distributedas N (0, ?2), orthogonal to document specific de-viations.For the exponential model, the nonlinear modelis the following (symbols have the same interpre-tations as in Equation 9):rij =(?1 + b1i)e?2 ?
1(1?e?
(kj?1)?2)+(?1+b1i)+ij(10)Fitting data with the above nonlinear modelsrequires starting estimates for fixed-effect coeffi-cients (i.e.
?1s and ?2s).
Unfortunately, there areno principled methods for selecting these values.We heuristically selected 6 for ?1 and 2 for ?2 asstarting values for the power law model, and 4 and0.5 as starting values for the exponential model.3 ResultsWe examined the quality of the models and the pa-rameters in the models: r0, the within-context en-tropy rate, and ?, the rate of context decay.3.1 Model Quality ComparisonThe CER hypothesis indirectly predicts that out-of-context entropy of signals of sentence positions(bits per word) should increase throughout a dis-course.
The two models go one step further topredict specific sublinear increase patterns, basedon the speaker?s considerations of the relevance ofpast contextual cues.
We compared the quality ofmodels in terms of Bayesian Information Criterion(BIC) within languages.
A lower BIC score indi-cates a better fit.
As shown by Figure 3, we findour models best explain the data in 9 out of the 12languages, reporting lower BIC scores than boththe linear and log-linear models as reported in ourprevious work (Qian and Jaeger, under review).For Danish, English and Italian, although neitherof our models produced a better score than the log-linear model, the relative difference is small: 0.54on average (comparing to BIC scores on the orderof 102 to 103).49Danish Dutch English French Italian Mandarin Norwegian Portuguese Russian E.Spanish L.Spanish SwedishPower Law Exponential Loglinear Linear?20?1001020Figure 3: Our models yield superior BIC scores in most languages.
The y-axis shows the differencesbetween BIC scores of individual models for a language and mean BIC of the models for that language(E.Spanish = European Spanish; L.Spanish = Latin-American Spanish).Specifically, in terms of BIC scores, the power-law model is better than the linear model (t(11) =?3.98, p < 0.01), and the log-linear model(t(11) = ?3.10, p < 0.05).
The exponen-tial model is also better than the linear model(t(11) = ?3.98, p < 0.01), and the log-linearmodel (t(11) = ?3.18, p < 0.01).
The power-law model and the exponential model are not sig-nificantly different from each other (t(11) = 0.5,p > 0.5).3.2 Interpretation of ParametersConstant Entropy of Signals r0.
Both modelsare constructed in such a way that the first param-eter r0, in theory, corresponds to the theoreticalwithin-context entropy of signals of sentence po-sitions.
This parameter refers to how many bits perword are needed to encode the ensemble of mes-sages at a sentence position when context is takeninto account.
The CER hypothesis directly pre-dicts that this rate should be constant throughouta discourse.
Although we are unable to test thisprediction directly, it is nevertheless interesting tocompare whether these two independently devel-oped models yield the same estimates for this pa-rameter in each language.Figure 4 shows encouraging results.
Not onlythe estimates made by the power model are wellcorrelated with those by the exponential model,but also the slope of this correlation is equal to 1(t(10) = 1.01, p < 0.0001).
Since there are noreasons a priori to suspect that these two modelsll4.0 4.5 5.0 5.5 6.0 6.54.04.55.05.56.06.5Estimated Within?context Entropy per Word (Exponential Model)EstimatedWithin?context EntropyperWord(Power?law Model)llDanishDutchEnglishFrenchItalianMandarinNorwegianPortugueseRussianE.
SpanishL.
SpanishSwedishFigure 4: Estimates of r0 correlate between bothmodels with a slope of 1.would give the same estimates, this is a first step toconfirming the entropy per word in sentence pro-duction is indeed a tractable constant throughoutdiscourses.Among all languages, r0 has a mean of 5.0bits in both models, and a variance of 0.46 inthe power-law model and 0.48 in the exponentialmodel, both remarkably small.
The similarity inr0 between languages may lead one to speculatewhether the amount of uncertainty per word in dis-courses is largely the same regardless of the actuallanguage used by the speakers.
On the other hand,50the differences in r0 may reveal the specific prop-erties of different languages.
Meanwhile, precau-tions need to be taken in interpreting those esti-mates given that the corpora are of different sizes,and the ngram model is simplistic in nature.Decay Rate ?.
The second parameter ?
corre-sponds to the rate of relevance decay in both mod-els.
Since the base relevance r0 varies betweenlanguages, ?
can be more intuitively interpreted asto indicate the percentage of the original relevanceof a contextual cue still remains in n positions.
Inthe power-law model, for example, the context in-formation from a previous sentence in Danish, onaverage, is only 11.6% (2?3.10 = 0.116) as rele-vant.
Hence, the relevance of a contextual cue de-creases rather quickly for Danish.
Table 2 showsthis is in fact the general picture for all languageswe tested.Language Relevance of Context in Discourse (%)1 pos.
before 2 pos.
before 3 pos.
beforeDanish 11.6 3.3 1.4Dutch 10.4 2.8 1.1English 0.1 0.0 0.0French 8.5 2.0 0.7Italian 10.2 2.7 1.0Mandarin 7.7 1.7 0.6Norwegian 18.9 7.1 3.6Portuguese 5.5 1.0 0.3Russian 12.7 3.8 1.6E.
Spanish 0.8 0.0 0.0L.
Spanish 2.7 0.3 0.1Swedish 5.8 1.1 0.3Table 2: In the power model, relevance of a con-textual cue decays rather quickly for each lan-guage.The picture of ?
looks a little different in theexponential model.
The relevance percentage onaverage is significantly higher, which confirms anearlier point that the power function decreasesmore quickly than the exponential function.
Table3 shows a summary for the 12 languages.One may note that the decay rate varies greatlybetween languages under the prediction of bothmodels.
However, these number are only approxi-mations since the entropy estimated by the ngramlanguage model is far from psychological real-ity.
Furthermore, it is unlikely that speakers ofone language would exhibit the same decay rateof context relevance in their production, let alnespeakers of different languages, who may be sub-ject to language-specific constraints during pro-Language Relevance of Context in Discourse (%)1 pos.
before 2 pos.
before 3 pos.
beforeDanish 30.1 9.1 2.7Dutch 28.7 8.2 2.4English 9.6 0.9 0.1French 26.7 7.1 1.9Italian 28.7 8.2 2.4Mandarin 25.7 6.6 1.7Norwegian 42.3 17.9 7.6Portuguese 22.5 5.1 1.1Russian 34.6 12.0 4.2E.
Spanish 14.2 2.0 0.3L.
Spanish 18.6 3.5 0.6Swedish 23.7 5.6 1.3Table 3: In the exponential model, relevance of acontextual cue decays more slowly.duction.
Therefore, the variation in estimates of?
seems reasonable.Correlation between r0 and ?.
Interestingly, r0and ?
are highly correlated (r2 = 0.39, p < 0.05in the power model, Figure 5; r2 = 0.47, p < 0.01in the exponential model, Figure 6): a high rel-evance decay rate tends to be coupled with highwithin-context entropy of signals.
This unan-ticipated observation is in fact compatible withthe account of efficient language production: ahigh within-context entropy of signals indicatesthe base relevance of a contextual cue (i.e.
r0)is high.
It is then useful for its relevance to de-cay more quickly to allow the speaker to inte-grate context from other cues.
Otherwise, the to-tal amount of relevant context may presumablyoverload working memory.
However, our cur-rent results come from only cross-linguistic sam-ples.
Cross-validation in within-language samplesis needed for confirming this hypothesis.3.3 The Bigger PictureHaving obtained the estimates for r0 and ?, we arenow in a position to examine how out-of-contextentropy of signals increases as a function of sen-tence positions, given the estimates of these twoparameters.
As shown in Figure 7, the predictionsfrom both models are qualitative similar exceptthat 1) when the decay rate in the power-law modelis low, out-of-context entropy of signals convergesmore slowly than in the exponential model (Figure7, right panel); 2) when the decay rate in the powermodel is high, it almost converges as quickly asthe exponential model, and only minor differencesexist in their predictions (Figure 7, left panel).51ll4.0 4.5 5.0 5.5 6.046810Within?Context Entropy per WordRelevanceDecay RatellDanishDutchEnglishFrenchItalianMandarinNorwegianPortugueseRussianE.
SpanishL.
SpanishSwedishFigure 5: The rate of relevance decay is corre-lated with within-context entropy of signals in thepower-law model.ll4.5 5.0 5.5 6.01.01.52.0Within?Context Entropy per WordRelevanceDecay RatellDanishDutchEnglishFrenchItalianMandarinNorwegianPortugueseRussianE.
SpanishL.
SpanishSwedishFigure 6: The rate of relevance decay is correlatedwith within-context entropy of signals in the expo-nential model.Because of the nonlinearity in our models, itis not possible to report the results in an intuitivemanner as in ?an increase in sentence position cor-responds to an increase ofX bits of out-of-contextentropy per word?.
Instead, we can analyticallysolve for the derivative of the predicted out-of-context entropy of signals with respect to sentenceposition (Equation 4 and 8).
This gives us:rpower(k)?
= r0k??
(11)for the power-law model, showing the rate of in-crease in predicted out-of-context entropy of sig-nals is a monotonically decreasing power function,andlll lllllll llllllll llll llllll llllllllllll llllllllllllllllllll lllll llll lllllllllll llll2 4 6 8 1245678Sentence PositionOut?of?contextEntropy per Wordin DutchPower:?
= 3.27Exp:?
= 1.25 llllllllllll lllllllllllll llllll l ll lllllllllllllll llllllllllll llll llllllllllllllllllllll l lllllllll ll lllll l llllllllllllll2 4 6 8 1245678Sentence PositionOut?of?contextEntropy per Wordin NorwegianPower:?
= 2.4Exp:?
= 0.86Figure 7: Predicted out-of-context entropy of sig-nals by the power-law model (solid) and the expo-nential model (dashed) in Dutch and Norwegian,with the actual distributions plotted on the back-ground.rexp(k)?
=r0?e?
?
1(e?(k?1)?)
(12)for the exponential model, showing the rate of in-crease is a monotonically decreasing exponentialfunction.
These mathematical properties indeedmatch our observations in Figure 7.4 Discussion and Future WorkThe models introduced in this paper try to answerthis question: if the relevance of a contextual cuefor predicting an upcoming linguistic signal de-cays over the course of a discourse, how much un-certainty (entropy) is associated with each individ-ual sentence position?
We have shown under thatmodels that incorporate (power law or exponen-tial) cue relevance decay in most cases describethe relation of out-of-context entropy of signalsto sentence position are better accounted for thanpreviously suggested models.We are continuing to investigate along this line.Specifically, we are interested in finding the role ofsemantic memory in affecting the relevance decayof context.
To test that, we plan to implement aprobabilistic topic model, in which topic continu-ity between a preceding sentence and an upcom-ing sentence is quantitatively measured.
Thus, thedecay of contextual cues can be based on the esti-52mated semantic relatedness between sentences, inaddition to the abstract notion of rate as used inthis paper.Finally, our relevance decay model can be ap-plied to the domain of language processing aswell.
For instance, the distance between a con-textual cue and the target word may affect howquickly a comprehender can process the informa-tion conveyed by the word.
We plan to addressthese question in future work.5 ConclusionWe have presented a new approach for examin-ing the distribution of entropy of linguistic sig-nals in discourses, showing that not only the out-of-context entropy of signals increases sublinearlywith sentence position, but also the sublinear trendis better explained by our nonlinear models thanby log-linear models of previous work.
Our mod-els are built on the assumption that the relevanceof a contextual cue for predicting a linguistic sig-nal in the future decays with its distance to the tar-get, and predict the relation of out-of-context en-tropy of signals to sentence position in discourses.These results indirectly lend support to the hypoth-esis that speakers maintain a constant entropy ofsignals across sentence positions in a discourse.AcknowledgementsWe wish to thank Meredith Brown, Alex Fine andthree anonymous reviewers for their helpful com-ments on this paper.
This work was supported byNSF grant BCS-0845059 to TFJ.ReferencesJohn R. Anderson.
1995.
Learning and Memory: Anintegrated approach.
John Wiley & Sons.Philip R. Clarkson and Roni Rosenfeld.
1997.
Sta-tistical language modeling using the cmu-cambridgetoolkit.
In Proceedings of ESCA Eurospeech.Dimitry Genzel and Eugene Charniak.
2002.
Entropyrate constancy in text.
In ACL, pages 199?206.Dimitry Genzel and Eugene Charniak.
2003.
Variationof entropy and parse trees of sentences as a functionof the sentence number.
in.
In EMNLP, pages 65?72.Frank Keller.
2004.
The entropy rate principle as apredictor of processing effort: An evaluation againsteye-tracking data.
In EMNLP, pages 317?324.D.
D. Lewis, Y. Yang, T. Rose, and F Li.
2004.
Rcv1:A new benchmark collection for text categorizationresearch.
J Mach Learn Res, 5:361?397.Steve Piantadosi and Edwards Gibson.
2008.
Uniforminformation density in discourse: a cross-corpusanalysis of syntactic and lexical predictability.
InCUNY.Ting Qian and T. Florian Jaeger.
2009.
Evidencefor efficient language production in chinese.
InCogSci09, pages 851?856.Ting Qian and T. Florian Jaeger.
under review.
En-tropy profiles in language: A cross-linguistic inves-tigation.C.
E. Shannon.
1948.
A mathematical theory of com-munications.
Bell Labs Tech J, 27(4):623?656.J.
T. Wixted and E. B. Ebbesen.
1991.
On the form offorgetting.
Psychological Science, 2:409?415.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: Phrasestructure annotation of a large corpus.
Nat LangEng, 11:207?238.G.
K. Zipf.
1935.
Psycho-Biology of Languages.Houghton-Mifflin.53
