Proceedings of the Workshop on Multiword Expressions: from Parsing and Generation to the Real World (MWE 2011), pages 49?56,Portland, Oregon, USA, 23 June 2011. c?2011 Association for Computational LinguisticsMWU-aware Part-of-Speech Tagging with a CRF model and lexicalresourcesMatthieu ConstantUniversite?
Paris-Est, LIGM5, bd Descartes - Champs/Marne77454 Marne-la-Valle?e cedex 2, Francemconstan@univ-mlv.frAnthony SigogneUniversite?
Paris-Est, LIGM5, bd Descartes - Champs/Marne77454 Marne-la-Valle?e cedex 2, Francesigogne@univ-mlv.frAbstractThis paper describes a new part-of-speech tag-ger including multiword unit (MWU) identifi-cation.
It is based on a Conditional RandomField model integrating language-independentfeatures, as well as features computed fromexternal lexical resources.
It was imple-mented in a finite-state framework composedof a preliminary finite-state lexical analysisand a CRF decoding using weighted finite-state transducer composition.
We showed thatour tagger reaches state-of-the-art results forFrench in the standard evaluation conditions(i.e.
each multiword unit is already merged ina single token).
The evaluation of the taggerintegrating MWU recognition clearly showsthe interest of incorporating features based onMWU resources.1 IntroductionPart-of-speech (POS) tagging reaches excellentresults thanks to powerful discriminative multi-feature models such as Conditional Random Fields(Lafferty et al, 2001), Support Vector Machine(Gime?nez and Ma?rquez, 2004), Maximum Entropy(Ratnaparkhi, 1996).
Some studies like (Denis andSagot, 2009) have shown that featuring these modelsby means of external morphosyntactic resources stillimproves accuracy.
Nevertheless, current taggersrarely take multiword units such as compound wordsinto account, whereas they form very frequent lexi-cal units with strong syntactic and semantic particu-larities (Sag et al, 2001; Copestake et al, 2002) andtheir identification is crucial for applications requir-ing semantic processing.
Indeed, taggers are gen-erally evaluated on perfectly tokenized texts wheremultiword units (MWU) have already been identi-fied.Our paper presents a MWU-aware POS tagger(i.e.
a POS tagger including MWU recognition1).It is based on a Conditional Random Field (CRF)model that integrates features computed from large-coverage morphosyntactic lexicons and fine-grainedMWU resources.
We implemented it in a finite-stateframework composed of a finite-state lexical ana-lyzer and a CRF-decoder using weighted transducercomposition.In section 2, we will first describe statistical tag-ging based on CRF.
Then, in section 3, we willshow how to adapt the tagging models in order toalso identify multiword unit.
Next, section 4 willpresent the finite-state framework used to implementthe tagger.
Section 5 will focus on the description ofour working corpus and the set of lexical resourcesused.
In section 6, we then evaluate our tagger onFrench.2 Statistical POS tagging with LinearChain Conditional Random FieldsLinear chain Conditional Ramdom Fields (CRF) arediscriminative probabilistic models introduced by(Lafferty et al, 2001) for sequential labelling.
Givenan input sequence x = (x1, x2, ..., xN ) and an out-1This strategy somewhat resembles the popular approach ofjoint word segmentation and part-of-speech tagging for Chi-nese, e.g.
(Zhang and Clark, 2008).
Moreover, other similarexperiments on the same task for French are reported in (Con-stant et al, 2011).49put sequence of labels y = (y1, y2, ..., yN ), themodel is defined as follows:P?
(y|x) =1Z(x) .N?tK?k?k.fk(t, yt, yt?1, x)where Z(x) is a normalization factor dependingon x.
It is based on K features each of them be-ing defined by a binary function fk depending onthe current position t in x, the current label yt,the preceding one yt?1 and the whole input se-quence x.
The feature is activated if a given con-figuration between t, yt, yt?1 and x is satisfied (i.e.fk(t, yt, yt?1, x) = 1).
Each feature fk is associatedwith a weight ?k.
The weights are the parametersof the model.
They are estimated during the train-ing process by maximizing the conditional loglikeli-hood on a set of examples already labeled (trainingdata).
The decoding procedure consists in labellinga new input sequence with respect to the model, bymaximizing P (y|x) (or minimizing ?logP (y|x)).There exist dynamic programming procedures suchas Viterbi algorithm in order to efficiently explore alllabelling possibilities.Features are defined by combining different prop-erties of the tokens in the input sequence and the la-bels at the current position and the preceding one.Properties of tokens can be either binary or tex-tual: e.g.
token contains a digit, token is capital-ized (binary property), form of the token, suffix ofsize 2 of the token (textual property).
Most tag-gers exclusively use language-independent proper-ties ?
e.g.
(Ratnaparkhi, 1996; Toutanova et al,2003; Gime?nez and Ma?rquez, 2004; Tsuruoka etal., 2009).
It is also possible to integrate language-dependant properties computed from an externalbroad-coverage morphosyntactic lexicon, that arePOS tags found in the lexicon for the given token(e.g.
(Denis and Sagot, 2009)).
It is of great interestto deal with unknown words2 as most of them arecovered by the lexicon, and to somewhat filter thelist of candidate tags for each token.
We thereforeadded to our system a language-dependent property:a token is associated with the concatenation of itspossible tags in an external lexicon, i.e.
the am-bibuity class of the token (AC).2Unknown words are words that did not occur in the trainingdata.In practice, we can divide features fk in twofamilies: while unigram features (uk) do not de-pend on the preceding tag, i.e.
fk(t, yt, yt?1, x) =uk(t, yt, x), bigram features (bk) depend on bothcurrent and preceding tags, i.e.
fk(t, yt, yt?1, x) =bk(t, yt, yt?1, x).
In our practical case, bigramsexlusively depends on the two tags, i.e.
they are in-dependent from the input sequence and the currentposition like in the Hidden Markov Model (HMM)3.Unigram features can be sub-divided into internaland contextual ones.
Internal features provide solelycharacteristics of the current token w0: lexical form(i.e.
its character sequence), lowercase form, suf-fice, prefix, ambiguity classes in the external lexi-cons, whether it contains a hyphen, a digit, whetherit is capitalized, all capitalized, multiword.
Contex-tual features indicate characteristics of the surround-ings of the current token: token unigrams at relativepositions -2,-1,+1 and +2 (w?2, w?1, w+1,w+2); to-ken bigrams w?1w0, w0w+1 and w?1w+1; ambi-guity classes at relative positions -2,-1,+1 and +2(AC?2, AC?1, AC+1,AC+2).
The different featuretemplates used in our tagger are given in table 2.Internal unigram featuresw0 = X &t0 = TLowercase form of w0 = L &t0 = TPrefix of w0 = P with |P | < 5 &t0 = TSuffix of w0 = S with |S| < 5 &t0 = Tw0 contains a hyphen &t0 = Tw0 contains a digit &t0 = Tw0 is capitalized &t0 = Tw0 is all capital &t0 = Tw0 is capitalized and BOS4 &t0 = Tw0 is multiword &t0 = TLexicon tags AC0 of w0 = A & w0 is multiword &t0 = TContextual unigram featureswi = X , i ?
{?2,?1, 1, 2} &t0 = Twiwj = XY , (j, k) ?
{(?1, 0), (0, 1), (?1, 1)} &t0 = TACi = A & wi is multiword, i ?
{?2,?1, 1, 2} &t0 = TBigram featurest?1 = T ?
&t0 = TTable 1: Feature templates3 MWU-aware POS taggingMWU-aware POS tagging consists in identifyingand labelling lexical units including multiword ones.3Hidden Markov Models of order n use strong indepen-dance assumptions: a word only depends on its correspondingtag, and a tag only depends on its n previous tags.
In our case,n=1.50It is somewhat similar to segmentation tasks likechunking or Named Entity Recognition, that iden-tify the limits of chunk or Named Entity segmentsand classify these segments.
By using an IOB5scheme (Ramshaw and Marcus, 1995), this task isthen equivalent to labelling simple tokens.
Each to-ken is labeled by a tag in the form X+B or X+I,where X is the POS labelling the lexical unit the to-ken belongs to.
Suffix B indicates that the token is atthe beginning of the lexical unit.
Suffix I indicatesan internal position.
Suffix O is useless as the endof a lexical unit corresponds to the beginning of an-other one (suffix B) or the end of a sentence.
Suchprocedure therefore determines lexical unit limits, aswell as their POS.A simple approach is to relabel the training datain the IOB scheme and to train a new model with thesame feature templates.
With such method, most ofmultiword units present in the training corpus willbe recognized as such in a new text.
The main issueresides in the identification of unknown multiwordunits.
It is well known that statistically inferring newmultiword units from a rather small training corpusis very hard.
Most studies in the field prefer findingmethods to automatically extract, from very largecorpus, multiword lexicons, e.g.
(Dias, 2003; Caseliet al, 2010), to be integrated in Natural LanguageProcessing tools.In order to improve the number of new multiwordunits detected, it is necessary to plug the tagger tomultiword resources (either manually built or auto-matically extracted).
We incorporate new featurescomputed from such resources.
The resources thatwe use (cf.
section 5) include three exploitable fea-tures.
Each MWU encoded is obligatory assigneda part-of-speech, and optionally an internal sur-face structure and a semantic feature.
For instance,the organization name Banque de Chine (Bank ofChina) is a proper noun (NPP) with the semanticfeature ORG; the compound noun pouvoir d?achat(purchasing power) has a syntactic form NPN be-cause it is composed of a noun (N), a preposition (P)and a noun (N).
By applying these resources to texts,it is therefore possible to add four new propertiesfor each token that belongs to a lexical multiword5I: Inside (segment); O: Outside (segment); B: Beginning(of segment)unit: the part-of-speech of the lexical multiword unit(POS), its internal structure (STRUCT), its semanticfeature (SEM) and its relative position in the IOBscheme (POSITION).
Table 2 shows the encodingof these properties in an example.
The property ex-traction is performed by a longest-match context-free lookup in the resources.
From these properties,we use 3 new unigram feature templates shown intable 3: (1) one combining the MWU part-of-speechwith the relative position; (2) another one dependingon the internal structure and the relative position and(3) a last one composed of the semantic feature.FORM POS STRUCT POSITION SEM Translationun - - O - again - - O - gainde - - O - ofpouvoir NC NPN B - purchasingd?
NC NPN I -achat NC NPN I - powerde - - O - ofcelles - - O - the onesde - - O - ofla - - O - theBanque NPP - B ORG Bankde NPP - I ORG ofChine NPP - I ORG ChinaTable 2: New token properties depending on MultiwordresourcesNew internal unigram featuresPOS0/POSITION0 &t0 = TSTRUCT0/POSITION0 &t0 = TSEM0 &t0 = TTable 3: New features based on the MW resources4 A Finite-state FrameworkIn this section, we describe how we implemented aunified Finite-State Framework for our MWU-awarePOS tagger.
It is organized in two separate clas-sical stages: a preliminary resource-based lexicalanalyzer followed by a CRF-based decoder.
Thelexical analyzer outputs an acyclic finite-state trans-ducer (noted TFST) representing candidate taggingsequences for a given input.
The decoder is in chargeof selecting the most probable one (i.e.
the path inthe TFST which has the best probability).514.1 Weighted finite-state transducersFinite-state technology is a very powerful machin-ery for Natural Language Processing (Mohri, 1997;Kornai, 1999; Karttunen, 2001), and in particu-lar for POS tagging, e.g.
(Roche and Schabes,1995).
It is indeed very convenient because ithas simple factorized representations and interest-ing well-defined mathematical operations.
For in-stance, weighted finite-state transducers (WFST) areoften used to represent probabilistic models such asHidden Markov Models.
In that case, they map in-put sequences into output sequences associated withweights following a probability semiring (R+,+,?,0, 1) or a log semiring (R ?
{??,+?
},?log,+,+?, 0) for numerical stability6.
A WFST is a finite-state automaton which each transition is composedof an input symbol, an output symbol and a weight.A path in a WFST is therefore a sequence of consec-utive transitions of the WFST going from an initialstate to a final state, i.e.
it puts a binary relationbetween an input sequence and an output sequencewith a weight that is the product of the weights of thepath transitions in a probability semiring (the sumin the log semiring).
Note that a finite-state trans-ducer is a WFST with no weights.
A very nice oper-ation on WFSTs is composition (Salomaa and Soit-tola, 1978).
Let T1 be a WFST mapping an inputsequence x into an output sequence y with a weightw1(x, y), and T2 be another WFST mapping a se-quence y into a sequence z with a weight w2(y, z).The composition of T1 with T2 results in a WFST Tmapping x into z with a weight w1(x, y).w2(y, z) inthe probability semiring (w1(x, y) + w2(y, z) in thelog semiring).4.2 Lexical analysis and decodingThe lexical analyzer is driven by lexical resourcesrepresented by finite-state transducers like in (Sil-berztein, 2000) (cf.
section 5) and generates a TFSTcontaining candidate analyses.
Transitions of theTFST are labeled by a simple token (as input) anda POS tag (as output).
This stage allows for re-ducing the global ambiguity of the input sentence intwo different ways: (1) tag filtering, i.e.
each token6A semiring K is a 5-tuple (K,?,?, 0?, 1?)
where the set Kis equipped with two operations ?
and ?
; 0?
and 1?
are theirrespective neutral elements.
The log semiring is an image ofthe probability semiring via the ?log function.is only assigned its possible tags in the lexical re-sources; (2) segment filtering, i.e.
we only keep lex-ical multiword units present in the resources.
Thisimplies the use of large-coverage and fine-grainedlexical resources.The decoding stage selects the most probable pathin the TFST.
This involves that the TFST shouldbe weighted by CRF-based probabilities in orderto apply a shortest path algorithm.
Our weighingprocedure consists in composing a WFST encodingthe sentence unigram probabilities (unigram WFST)and a WFST encoding the bigram probabilities (bi-gram WFST).
The two WFSTs are defined over thelog semiring.
The unigram WFST is computed fromthe TFST.
Each transition corresponds to a (xt,yt)pair at a given position t in the sentence x.
So eachtransition is weighted by summing the weights ofthe unigram features activated at this position.
In ourpractical case, bigram features are independent fromthe sentence x.
The bigram WFST can therefore beconstructed once and for all for the whole taggingprocess, in the same way as for order-1 HMM tran-sition diagrams (Nasr and Volanschi, 2005).5 Linguistic resources5.1 French TreeBankThe French Treebank (FTB) is a syntactically an-notated corpus7 of 569,039 tokens (Abeille?
et al,2003).
Each token can be either a punctuationmarker, a number, a simple word or a multiwordunit.
At the POS level, it uses a tagset of 14 cate-gories and 34 sub-categories.
This tagset has beenoptimized to 29 tags for syntactic parsing (Crabbe?and Candito, 2008) and reused as a standard in aPOS tagging task (Denis and Sagot, 2009).
Belowis a sample of the FTB version annotated in POS., PONCT ,soit CC i.e.une DET aaugmentation NC raisede P of1 , 2 DET 1 , 2% NC %par rapport au P+D compared with themois NC precedingpre?ce?dent ADJ month7It is made of journalistic texts from Le Monde newspaper.52Multiword tokens encode multiword units of dif-ferent types: compound words and named enti-ties.
Compound words mainly include nominalssuch as acquis sociaux (social benefits), verbs suchas faire face a` (to face) adverbials like dans l?imme?diat (right now), prepositions such as en de-hors de (beside).
Some Named Entities are also en-coded: organization names like Socie?te?
suisse de mi-croe?lectronique et d?
horlogerie, family names likeStrauss-Kahn, location names like Afrique du Sud(South Africa) or New York.
For the purpose of ourstudy, this corpus was divided in three parts: 80%for training (TRAIN), 10% for development (DEV)and 10% for testing (TEST).5.2 Lexical resourcesThe lexical resources are composed of both mor-phosyntactic dictionaries and strongly lexicalizedlocal grammars.
Firstly, there are two general-language dictionaries of simple and multiwordforms: DELA (Courtois, 1990; Courtois et al, 1997)and Lefff (Sagot, 2010).
DELA has been devel-opped by a team of linguists.
Lefff has been au-tomatically acquired and then manually validated.It also resulted from the merge of different lexicalsources.
In addition, we applied specific manuallybuilt lexicons: Prolex (Piton at al., 1999) contain-ing toponyms ; others including organization namesand first names (Martineau et al, 2009).
Figures onthese dictionaries are detailed in table 4.Name # simple forms #MW formsDELA 690,619 272,226Lefff 553,140 26,311Prolex 25,190 97,925Organizations 772 587First names 22,074 2,220Table 4: Morphosynctatic dictionariesThis set of dictionaries is completed by a libraryof strongly lexicalized local grammars (Gross, 1997;Silberztein, 2000) that recognize different types ofmultiword units such as Named Entities (organiza-tion names, person names, location names, dates),locative prepositions, numerical determiners.
A lo-cal grammar is a graph representing a recursivefinite-state transducer, which recognizes sequencesbelonging to an algebraic language.
Practically, theydescribe regular grammars and, as a consequence,can be compiled into equivalent finite-state trans-ducers.
We used a library of 211 graphs.
We man-ually constructed from those available in the onlinelibrary GraalWeb (Constant and Watrin, 2007).5.3 Lexical resources vs. French TreebankIn this section, we compare the content of the re-sources described above with the encodings in theFTB-DEV corpus.
We observed that around 97,4%of lexical units encoded in the corpus (excludingnumbers and punctuation markers) are present in ourlexical resources (in particular, 97% are in the dic-tionaries).
While 5% of the tokens are unknown (i.e.not present in the training corpus), 1.5% of tokensare unknown and not present in the lexical resources,which shows that 70% of unknown words are cov-ered by our lexical resources.The segmentation task is mainly driven by themultiword resources.
Therefore, they should matchas much as possible with the multiword units en-coded in the FTB.
Nevertheless, this is practicallyvery hard to achieve because the definition of MWUcan never be the same between different people asthere exist a continuum between compositional andnon-compositional sequences.
In our case, we ob-served that 75.5% of the multiword units in the FTB-DEV corpus are in the lexical resources (87.5% in-cluding training lexicon).
This means that 12.5%of the multiword tokens are totally unknown and,as a consequence, will be hardly recognized.
An-other significant issue is that many multiword unitspresent in our resources are not encoded in the FTB.For instance, many Named Entities like dates, per-son names, mail addresses, complex numbers are ab-sent.
By applying our lexical resources8 in a longest-match context-free manner with the platform Unitex(Paumier, 2011), we manually observed that 30% ofthe multiword units found were not considered assuch in the FTB-DEV corpus.6 Experiments and EvaluationWe firstly evaluated our system for standard tag-ging without MWU segmentation and compare itwith other available statistical taggers that we alltrained on the FTB-TRAIN corpus.
We tested the8We excluded local grammars recognizing dates, personnames and complex numbers.53well-known TreeTagger (Schmid, 1994) based onprobabilistic decision trees, as well as TnT (Brants,2000) implementing second-order Hidden Markov.We also compared our system with two existingdiscriminative taggers: SVMTool (Gime?nez andMa?rquez, 2004) based on Support Vector Modelswith language-independent features; MElt (Denisand Sagot, 2009) based on a Maximum Entropymodel also incorporating language-dependent fea-ture computed from an external lexicon.
The lexiconused to train and test MElt included all lexical re-sources9 described in section 5.
For our CRF-basedsystem, we trained two models with CRF++10: (a)STD using language-independent template features(i.e.
excluding AC-based features); (b) LEX usingall feature templates described in table 2.
We noteCRF-STD and CRF-LEX the two related taggerswhen no preliminary lexical analysis is performed;CRF-STD+ and CRF-LEX+ when a lexical analy-sis is performed.
The lexical analysis in our exper-iment consists in assigning for each token its possi-ble tags found in the lexical resources11 .
Tokens notfound in the resources are assigned all possible tagsin the tagset in order to ensure the system robust-ness.
If no lexical analysis is applied, our systemconstructs a TFST representing all possible analyzesover the tagset.
The results obtained on the TESTcorpus are summed up in table 5.
Column ACC in-dicates the tagger accuracy in percentage.
We canobserve that our system (CRF-LEX+) outperformsthe other existing taggers, especially MElt whoseauthors claimed state-of-the-art results for French.We can notice the great interest of a lexical analysisas CRF-STD+ reaches similar results as a MaxEntmodel based on features from an external lexicon.We then evaluated our MWU-aware taggertrained on the TRAIN corpus whose complex tokenshave been decomposed in a sequence of simple to-kens and relabeled in the IOB representation.
Weused three different sets of feature templates lead-9Dictionaries were all put together, as well as with the resultof the application of the local grammars on the corpus.10CRF++ is an open-source toolkit to train and test CRF mod-els (http://crfpp.sourceforge.net/).
For training, we set the cut-off threshold for features to 2 and the C value to 1.
We also usedthe L2 regularization algorithm.11Practically, as the tagsets of the lexical resources and theFTB were different, we had to first map tags used in the dictio-naries into tags belonging to the FTB tagset.Tagger Model ACCTnT HMM 96.3TreeTagger Decision trees 96.4SVMTool SVM 97.2CRF-STD CRF 97.4MElt MaxEnt 97.6CRF-STD+ CRF 97.6CRF-LEX CRF 97.7CRF-LEX+ CRF 97.7Table 5: Comparison of different taggers for Frenching to three CRF models: CRF-STD,CRF-LEX andCRF-MWE.
The two first ones (STD and LEX) usethe same feature templates as in the previous ex-periment.
MWE includes all feature templates de-cribed in sections 2 and 3.
CRF-MWE+ indicatesthat a preliminary lexical analysis is performed be-fore applying CRF-MWE decoding.
The lexical anal-ysis is achieved by assigning all possible tags of sim-ple tokens found in our lexical resources, as well asadding, in the TFST, new transitions correspondingto MWU segments found in the lexical resources.We compared the three models with a baseline andSVMTool that have been learnt on the same trainingcorpus.
The baseline is a simple context-free lookupin the training MW lexicon, after a standard CRF-based tagging with no MW segmentation.
We eval-uated each MWU-aware tagger on the decomposedTEST corpus and computed the f-score, combiningprecision and recall12.
The results are synthesizedin table 6.
The SEG column shows the segmentationf -score solely taking into account the segment limitsof the identified lexical unit.
The TAG column alsoaccounts for the label assigned.
The first observationis that there is a general drop in the performances forall taggers, which is not a surprise as regards withthe complexity of MWU recognition (97.7% for thebest standard tagger vs. 94.4% for the best MWU-aware tagger).
Clearly, MWU-aware taggers whichmodels incorporate features based on external MWUresources outperform the others.
Nevertheless, thescores for the identification and the tagging of theMWUs are still rather low: 91%-precision and 71%recall.
We can also see that a preliminary lexicalanalysis slightly lower the scores, which is due to12f-score f = 2prp+r where p is precision and r is recall.54missing MWUs in the resources and is a side effectof missing encodings in the corpus.Tagger Model TAG SEGBaseline CRF 91.2 93.6SVMTool SVM 92.1 94.7CRF-STD CRF 93.7 95.8CRF-LEX CRF 93.9 95.9CRF-MWE CRF 94.4 96.4CRF-MWE+ CRF 94.3 96.3Table 6: Evaluation of MWU-aware taggingWith respect to the statistics given in section 5.3,it appears clearly that the evaluation of MWU-awaretaggers is somewhat biased by the fact that the def-inition of the multiword units encoded in the FTBand the ones listed in our lexical resources are notexactly the same.
Nevertheless, this evaluation thatis the first in this context, brings new evidenceson the importance of multiword unit resources forMWU-aware tagging.7 Conclusions and Future WorkThis paper presented a new part-of-speech tagger in-cluding multiword unit identification.
It is based ona CRF model integrating language-independent fea-tures, as well as features computed from externallexical resources.
It was implemented in a finite-state framework composed of a preliminary finite-state lexical analysis and a CRF decoding usingweighted finite-state transducer composition.
Thetagger is freely available under the LGPL license13.It allows users to incorporate their own lexicons inorder to easily integrate it in their own applications.We showed that the tagger reaches state-of-the-artresults for French in the standard evaluation environ-ment (i.e.
each multiword unit is already merged ina single token).
The evaluation of the tagger inte-grating MWU recognition clearly shows the interestof incorporating features based on MWU resources.Nevertheless, as there exist some differences in theMWU definitions between the lexical resources andthe working corpus, this first experiment requiresfurther investigations.
First of all, we could test ourtagger by incorporating lexicons of MWU automat-ically extracted from large raw corpora in order to13http://igm.univ-mlv.fr/?mconstan/research/softwaredeal with low recall.
We could as well combine thelexical analyzer with a Named Entity Recognizer.Another step would be to modify the annotations ofthe working corpus in order to cover all MWU typesand to make it more homogeneous with our defini-tion of MWU.
Another future work would be to testsemi-CRF models that are well-suited for segmenta-tion tasks.ReferencesA.
Abeille?, L. Cle?ment and F. Toussenel.
2003.
Buildinga treebank for French.
in A. Abeille?
(ed), Treebanks,Kluwer, Dordrecht.T.
Brants.
2000.
TnT - A Statistical Part-of-Speech Tag-ger.
In Proceedings of the Sixth Applied Natural Lan-guage Processing Conference (ANLP 2000), 224?231.H.
Caseli, C. Ramisch, M. das Graas Volpe Nunes, A.Villavicencio.
2010.
Alignment-based extractionof multiword expressions.
Language Resources andEvaluation, Springer, vol.
44(1), 59?77.M.
Constant, I. Tellier, D. Duchier, Y. Dupont, A. Si-gogne, S. Billot.
2011.
Inte?grer des connaissances lin-guistiques dans un CRF : application a` l?apprentissaged?un segmenteur-e?tiqueteur du franc?ais.
In Actes de laConfe?rence sur le traitement automatique des languesnaturelles (TALN?11).M.
Constant and P. Watrin.
2007.
Networking Mul-tiword Units.
In Proceedings of the 6th Interna-tional Conference on Natural Language Processing(GoTAL?08), Lecture Notes in Artificial Intelligence,Springer-Verlag, vol.
5221: 120 ?
125.A.
Copestake, F. Lambeau, A. Villavicencio, F. Bond, T.Baldwin, I.
A.
Sag and D. Flickinger.
2002.
Multi-word expressions: linguistic precision and reusability.In Proceedings of the Third conference on LanguageResources and Evaluation (LREC?
02), 1941 ?
1947.B.
Courtois.
1990.
Un syste`me de dictionnairese?lectroniques pour les mots simples du franc?ais.Langue Franc?aise, vol.
87: 1941 ?
1947.B.
Courtois, M. Garrigues, G. Gross, M. Gross, R.Jung, M. Mathieu-Colas, A. Monceaux, A. Poncet-Montange, M. Silberztein, R. Vive?s.
1990.
Dictio-nnaire e?lectronique DELAC : les mots compose?s bi-naires.
Technical report, LADL, University Paris 7,vol.
56.B.
Crabbe?
and M. -H. Candito.
2008.
Expe?riencesd?analyse syntaxique statistique du franais.
In Pro-ceedings of Traitement des Langues Naturelles (TALN2008).P.
Denis et B. Sagot.
2009.
Coupling an annotated cor-pus and a morphosyntactic lexicon for state-of-the-art55POS tagging with less human effort.
In Proceedingsof the 23rd Pacific Asia Conference on Language, In-formation and Computation (PACLIC 2009).G.
Dias.
2003.
Multiword Unit Hybrid Extraction.
Inproceedings of the Workshop on Multiword Expres-sions of the 41st Annual Meeting of the Associationof Computational Linguistics (ACL 2003), 41?49.J.
Gime?nez and L. Ma?rquez.
2004.
SVMTool: A gen-eral POS tagger generator based on Support VectorMachines.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation(LREC?04).M.
Gross.
2007.
The construction of local grammars.
InE.
Roche and Y. Schabes (eds.).
Finite-State LanguageProcessing.
The MIT Press, Cambridge, Mass.
329?352L.
Karttunen.
2001.
Applications of Finite-State Trans-ducers in Natural Language Processing.
In proceed-ings of the 5th International Conference on Implemen-tation and Application of Automata (CIAA 2000).
Lec-ture Notes in Computer Science.
vol.
2088, Springer,34?46A.
Kornai (Ed.).
1999.
Extended Finite State Models ofLanguage.
Cambridge University PressJ.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random Fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofthe Eighteenth International Conference on MachineLearning (ICML 2001), 282?289.C.
Martineau, T. Nakamura, L. Varga and Stavroula Voy-atzi.
2009.
Annotation et normalisation des entite?snomme?es.
Arena Romanistica.
vol.
4:234?243.M.
Mohri 1997.
Finite-state transducers in languageand speech processing.
Computational Linguistics 23(2):269?311.A.
Nasr, A. Volanschi.
2005.
Integrating a POS Taggerand a Chunker Implemented as Weighted Finite StateMachines.
Finite-State Methods and Natural Lan-guage Processing, Lecture Notes in Computer Sci-ence, vol.
4002, Springer 167?178.S.
Paumier.
2011.
Unitex 2.1 user manual.http://igm.univ-mlv.fr/?unitex.O.
Piton, D. Maurel, C. Belleil.
1999.
The Prolex DataBase : Toponyms and gentiles for NLP.
In proceedingsof the Third International Workshop on Applicationsof Natural Language to Data Bases (NLDB?99), 233?237.L.
A. Ramshaw and M. P. Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proceedingsof the 3rd Workshop on Very Large Corpora, 88 ?
94.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP 1996), 133 ?
142.E.
Roche, Y. Schabes.
1995.
Deterministic part-of-speech tagging with finite-state transducers.
Compu-tational Linguistics, MIT Press, vol.
21(2), 227?253I.
A.
Sag, T. Baldwin, F. Bond, A. Copestake, D.Flickinger.
2001.
Multiword Expressions: A Pain inthe Neck for NLP.
In Proceedings of the 3rd Interna-tional Conference on Intelligent Text Processing andComputational Linguistics (CICLing-2002), 1?15B.
Sagot.
2010.
The Lefff, a freely available, accurateand large-coverage lexicon for French.
In Proceed-ings of the 7th International Conference on LanguageResources and Evaluation (LREC?10).A.
Salomaa, M. Soittola.
1978.
Automata-Theoretic As-pects of Formal Power Series.
Springer-Verlag.H.
Schmid.
1994.
Probabilistic Part-of-Speech TaggingUsing Decision Trees.
Proceedings of InternationalConference on New Methods in Language Processing.M.
Silberztein.
2000.
INTEX: an FST toolbox.
Theoret-ical Computer Science, vol.
231 (1): 33?46.K.
Toutanova, D. Klein, C. D. Manning, Y. YoramSinger.
2003.
Feature-rich part-of-speech taggingwith a cyclic dependency network.
Proceedings ofHLT-NAACL 2003, 252 ?
259.Y.
Tsuruoka, J. Tsujii, S. Ananiadou.
2009.
Fast FullParsing by Linear-Chain Conditional Random Fields.Proceedings of the 12th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL 2009), 790?798.Y.
Zhang, S. Clark.
2008.
Joint Word Segmentation andPOS Tagging Using a Single Perceptron.
Proceedingsof ACL 2008, 888 ?
896.56
