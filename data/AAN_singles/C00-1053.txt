Deixis and Conjunction in Multimodal SystemsMichael JohnstonAT&T Labs - ResearchShannon Laboratory, 180 Park AveFlorham Park, NJ 07932, USAj ohnston@research ,  att.
comAbstractIn order to realize their full potential, multimodalinterfaces need to support not just input frommultiple modes, but single comnmnds optinmllydistributed across the available input modes.
Amultimodal anguage processing architecture isneeded to integrate semantic content from thedifferent modes.
Johnston 1998a proposes amodular approach to multimodal languageprocessing in which spoken language parsing iscompleted before lnultimodal parsing.
In thispaper, I will demonstrate the difficulties thisapproach faces as the spoken language parsingcomponent is expanded to provide a compositionalanalysis of deictic expressions.
I propose analternative architecture in which spoken andmultimodal parsing are tightly interleaved.
Thisarchitecture greatly simplifies the spoken languageparsing grm-nmar and enables predictiveinformation fiom spoken language parsing to drivethe application of multimodal parsing and gesturecombination rules.
I also propose a treatment ofdeictic numeral expressions that supports thebroad range of pen gesture combinations that canbe used to refer to collections of objects in theinterface.IntroductionMultimodal interfaces allow content to beconveyed between humans and machines overmultiple different channels uch speech, graphics,pen, and hand gesture.
This enables more naturaland efficient interaction since different kinds ofcontent are best suited to particular modes.
Forexample, spatial information is effectivelyconveyed using gesture for input (Oviatt 1997) and2d or 3d graphics for output (Towns et al1998).Multimodal interfaces also stand to play a criticalrole in the ongoing migration of interaction ontowireless portable computing devices, such asPDAs and next generation phones, which havelimited screen real estate and no keyboard.
Forsuch devices, complex graphical user interfacesare not feasible and speech and pen will be theprimary input lnodes.
I focus here on multimodalinterfaces which support speech and pen input.Pen input consists of gestures and drawings whichare made in electronic ink on the computer displayand processed by a gesture recognizer.
Speechinput is transcribed using a speech recognizer.This paper is concerned with therelationship between spoken language parsing andnmltimodal parsing, specifically whether theyshould be separate modular components, and therelated issue of determining the appropriate l velof constituent structure at which nmltimodalintegration should apply.
Johnston 1998aproposes a modular approach in which theindividual modes are parsed and assigned typedfeature structures representing their combinatoryproperties and semantic content.
Anmltidimensional chart parser then combines thesestructures in accordance with a unification-basedlnultimodal grammar.
This approach is outlined inSection 1.
Section 2 addresses the compositionalanalysis of deictic expressions and their interactionwith conjunction and other aspects of thegramnmr.
In Section 3, a new architecture ispresented in which spoken and multimodal parsingare interleaved.
Section 4 presents an analysis ofdeictic numeral expressions, and Section 5discusses certain constructions in whichmultimodal integration applies at higher levels ofconstituent structure than a simple deictic nounphrase.
I will draw examples from a nmltimodaldirectory and messaging application, specifically amultimodal variant of VPQ (Buntschuh et al1998).1 Unification-based nmltimodal parsingJohnston 1998a presents an approach to languageprocessing for multimodal systems in whichmultimodal integration strategies are specifieddeclaratively in a unification-based grammarformalism.
The basic architecture of the approachis given in Figure I.
The results of speechrecognition and gesture recognition are interpretedby spoken language processing (SLP) and gestureprocessing (GP) components respectively.
Theseassign typed feature structure representations362(Carpenter 1992) to speech and gesture and passthose on to a nmltimodal parsing component (MP).Tim typed feature structure formalism isaugmented with ftmctioual constraints (Wittenbnrg1993).
MP uses a multidimensional chart parser tocombine the interpretations of speech and gesturein accordance with a nmltimodal unil'ication-basedgrammar, determines the range of possiblelnultimodal interpretations, selects the one with thehighest joint probability, and passes it on forexecution.~ CommandsFigure 1 Modular architecture (Johnston 1998a)As an example of a multimodal command, in orderto reposition an object, a user might say 'move thishere' and make two gestures on the display.
Thespoken command 'move this here' needs tocombine with the two gestures, the first indicatingthe entity to be moved and the second indicatingwhere it should be moved to.
In cases where thespoken string needs to combine with more thanone gesture, it is assigned a multimodalsubcategorization list indicating the gestures itneeds, how they contribute to the meaning, and theconstraints on their combination.
For e.xample,SLI' assigns 'move this here' the feature structurein Figure 2.The mmsubcat: list indicates that thisinput needs to combine with two gestures.
Thespoken command is constrained to overlap with orfollow within five seconds of the first gesture.The second gesture must follow within fiveseconds of the first.
The first provides the entity tomove and second the new location.
GP assignsincolning estures feature structure representationsspecifying their semantic type and any object theyselect and passes these on to MP.
MP uses generalcombinatory schelnata for nmltimodalsubcategorization (Jolmston 1998a, p. 628) tocombine the gestures with the speech, saturate thenmltilnodal subcategorization list, and yield anexecutable command.cal.
:s tlbcat_COlllnlarKl limc:\[l\]\[-tyl~c:m!
?vc \]? )
....I ,?
.,.\[typc:cnti b, \] /I / I ,  ?
rtypc:localion\]/\[ mcat'?n :\[sclcclion:\[3\] j Jfirst: I |imc:\[4\] |conslraints:\[ovcrlap(\[1 ],\[4\])v fotlow(\[ 1 \],\[4\],5)\]mmsubcat: \[ \[cat:spatial gesture l\]i fa,.,,:l time:\[5\] //I c?nslraints:\[f?ll?v"(\[5\],\[4\],5)\] /\[rest:cud JFigure 2 Feature structure for 'move this here'Tiffs approach as many advantages.
It allows fora great degree of expressivity.
Combinations ofspeech with multiple gestures can be described ascan visual parsing of collections of gestures.Unlike many previous multilaodal systems, theapproach is not speech-driven, any piece ofcontent can come fiom any mode.
Anothersignificant advantage is the lnoclularity of spokenhmguage parsing (SLP) and multimodal parsing(MP).
More general rules regal'ding multimodalintegration are in MP while the specific speechgraMlllar used for an application is in SLP,enabling reuse of the multimodal parsing modulefor different applications.
This modularity alsoenables plug-and-play of different kinds of spokenlanguage parsers with the same multimodalparsing component.
SLP can be a traditional chartparser, a robust parser, or a stochastic parser(Gorin et al1997).
The modularity of SLP andMP also facilitates the adoption of a differentstrategy for string parsing t?om that used formultimodal parsing.
Traditional approaches tostring parsing, such as chart parsing (Kay 1980)assume the combining constituents o be discreteand in linear order.
This imposes significantconstraints on the combination of elelnents, greatlyreduces the number of Colabinations that need tobe considered, and facilitates prediction in parsing.In contrast, multimodal input is distributed overtwo or three spatial dimensions, peech, and time.Unlike words in a string, speech and gesture mayoverlap temporally and there is no singulardimension on which tim input is linear anddiscrete.
The constraints that drive parsing are363specific to the combining elements and there is notthe same general means for predictive parsing(Johnston 1998a).While the modularity of spoken languageprocessing and multimodal parsing in Johnston1998a has many advantages, the assumption thatall processing of the spoken string takes placebefore multimodal integration leads to significantdifficulties as the spoken language processingcomponent is expanded to handle more complexlanguage and to provide a compositional nalysisof spoken language containing deictics.2 Composit ional  analysis of  de ic t i csThe basic problem the approach faces is to providean analysis of spoken language in multimodalsystems which enables the appropriate multilnodalsubcategorization frame and associated constraintsto be built compositionally in the course of parsingthe spoken string.
Whatever the syntacticstructure of the spoken utterance, the essentialconstraint on the multimodal subcategorization isthat the list of subcategorized gestures match thelinear order of the deictic expressions in theutterance, and that the temporal constraints alsoreflect hat order.
This can be thought of in termsof lambda abstraction.
What we need to do isabstract over all of the unbound variables in thepredicate that will be instantiated by gesture.
Foran expression like 'move tiffs here' we generatethe abstraction.
2ge,,tio.2gh,catio,,.nlove(ge,,tio.,glocatio,,).In terms of the analysis above, this amounts toderiving the feature structure in Figure 2compositionally fi'om feature structures assignedto 'move', 'this', and 'here'.One way to accomplish this within themodular approach is to set up the spoken languageprocessing component so that it manipulates twosubcat lists: a regular spoken language subcat: listand a multimodal mmsubcat: list.
Informationabout needed gestures percolates through thesyntactic parse.
The verb 'move' is assigned timfeature structure in Figure 3.
It subcategorizes (inthe string) for an entity and for a location.
If thearguments are not deictic, for example 'move thesupplies to the island' the verb simply combineswith its arguments to yield a complete command.Deictic expressions are assigned structures whichsubcategorize for phrases which subcategorize forNPs (the deictic expression is essentially typeraised).
The structure for 'this' is given in Figure4.
Tim structure for 'here' is like that for 'this',except hat it selects for a verb subcategorizing fora location rather than an entity (subeat:f irst:subeat:first:eontent:type is location).-cat :vdeictic : notime :114\]I-t: 'pc : movecontent : Io 9ject :\[1 \]\[tzpe : ntity\]Llocation : \[2\]\[type : location  c.t,p 1\] \/ :\[ oi.e.t :lU\].
uUca': / \[first : \[cat : np /,'est : / keontent : \[21L L rest :end\[list :\[31 \]mmsubcat :/end :\[3\]Llasttime : \[4\]Figure 3 Feature structure for 'move'-cat:vdcictic:yescontent:\[ 1 ]lime:\[9\]subcat first:-cat:vtime:\[9\]deiclic:\[8\]content:\[ 1\]\[ \[cat:np \]\], .
\[fir,, t:/ .
?
\[lype:entily \]// su,,c,,,\] \[co,,te,.
:.selec,ion:\[2\]\]\]\[krcst:\[31 - \]rlist:\[4\]/ \[cat:spatiaI geslurc \]\] firsl:llimc:\[5\] \]\[ \[ .
\[lypc'a "e i \]\]I ICO l l tC I l l : |  i .
,  ? '
, ~, / Inlnlstlbcal:IclRl: rcst:16\]L ksciectmn.\[zlJjconslraints: \[is(\[8\],no)-->ove,lap(\[5\],\[7\])vfollow(\[7\] \[5\] 5)is(\[8\],yes)-->follow(\[5\] \[7\] 5)\]\[lasttime:\[7\]rest:\[3\]list:\[4\] \]mmsubcal end:\[6\] /lasttime:\[5\]\]Figure 4 Feature structure for 'this'In 'move this here', 'this' combines with the verbto its left, removing the first specification on thesubcat:  list of 'move' and adding a gesturespecification to the resulting mmsubcat:.
Then'here' composes to the left with 'move this'relnoving the next specification on the subeat: andadding another gesture specification to themmsubcat: I.
The constraint on the first gesturei Directionality features in subeat: used to control therelative positions of combining phrases are omitted hereto simplify tile exposition.364differs from that on the others.
The t'irst mustoverlap o1 precede the speech, while tile otherslnust follow the preceding gesture.
This isachieved with the feature deictie: which is set toyes when composition with the first deictic takesplace.
The setting of this t'eature determines whichof the temporal constraints applies (usingconditional constraints).
The lasttime: featurealways provides the time of the last entity in thesequence o1' inputs.
The mmsubcat:end: featureprovides access to the end of the currentmmsubcat: list.
Once the subcat: feature hasvalue end the mmsubcat:end: needs to be set toend and then the value of nunsubcat:list: is thesame as lhe msubcat: in Figure 2 and can bepassed on to the multimodal parser.So then, it is possible to set up tile speechparsing granlular so that it will build tile neededsubcategorization for gestures and modularitybetween specch parsing and multimodal parsingcan be maintained.
However, as more complexphenomena re considered tile resulting gramlnarbecomes more and more complex.
In tile exampleabove, the deictic NPs are pronouns ('lhis','here').
The grammar of noun phrases needs to beset up so that tile presence of a deictic determinermakes the whole phrase subcategorize for a verbas in 'move this large green one here'.
Mattersbecolne lnore complex as tile grammar isexpanded to handle conjunction, for example'move this and this he,w'.
An analysis of nolninalcol\junction can be set up in which the multimodalsubcategorization lists of conjuncts are combinedand assigned constraints uch that gestures arerequired in the order in which the dcictic words (orother phrases requiring gestures) appear.
If adeictic appears within a conjoined phrase, thatphrase is assigned a representation whichsubcategorizes for a verb (just as 'this' doesabove).
In 'move this and this there', 'this andthis' combines with 'move' then 'there' combineswith the result, yielding an expression whichsubcategorizes for three gestures.
The treatmentof possessives also needs to be expanded to handledeictics.
For example, in 'call this pelwon'smmtber', 'this l)etwon 's number' needs tosubcategorize for a verb which subcategorizes foxa nmnber while the multimodal subcategorizationis for a gesture on a person.
The possibility oflarger phrases mapping onto single gesturesfurther complicates matters.
For example, to allowlk~r 'move.fi'om here to there' with a line gesturewhich connects tile start and elld points, SLP willneed to assign multimodal subcategorization listwith a single line element to the whole phrase'from here to there', in addition to the otheranalysis in which this expression multimodallysubcategorizes for two gestures.
An alternative isto have a rule that breaks down any line into itsstart and end points.
The problem then is that youintroduce subpart points into the muitimodal chartthat could combine with other speech recognitionresults and lead to selection of the wrong parse ofthe multimodal input.
Keeping the points togetheras a line avoids this difficulty but complicates tileSLP grammar.
I return to these cases of largerphrases subcategorizing for single gestures inSection 5 below.If tile separation of natural languageparsing and multimodal integration is to bemaintained, the analysis of deictics 1 have shown,or one like it, has to permeate the whole of thenatural language grammar so that appropriatenmltimodal subcategorization frames can be builtin a general way.
This can be done, but as thecoverage of the natural anguage grammar grows,the analysis becomes increasingly baroque andhard to maintain.
To overcome these difficulties, Ipropose here a new architecture in which spokenlanguage parsing and multimodal parsing areinterleaved and multilnodal integration takes placeat the constituent structure level of simple deicticNPs.3 Interleaviug spoken language parsingand multimodal parsingThere are a nmnber of different ways in whichspoken language parsing (SLP) and multimodalparsing (MP) can be imerleaved: (1) SLPpopulates a chart with fragments, these are passedto MP which determines possible combinationswith gesture, the resulting combinations are passedback to SLP which continues until a parse of thestring is found, (2) SLP parses the incoming stringinto a series of fragments, these become edges inMP and are combined with gestures, MP isaugmented with rules from SLP which operate inMP in order to complete the analysis of the phrase,(3) SLP and MP are merged and there is one singlegralnmar covering both spoken language andmultimodal parsing (cf.
Johnston and Bangalore2000).
1 adopt here strategy (1) represented inFigure 5.365CommandsFigure 5 Interleaved architectureA significant advantage of (1) is that it limits thenumber of elements and combinations that need toconsidered by the nmltimodal parser.
Thecomplexity of the inultidilnensional parsingalgorithm is exponential in the worst case(Johnston 1998a) and so it is important to limit thenumber of elements that need to be considered.Another advantage of (1) over (2) and (3) is that asin the modular approach, the grammars areseparated, facilitating reuse of the multimodalcomponent for applications with different spokenCOlnmands.
Also, (2) has the problem that there isredundancy among the SLP and MP grammars,both need to have the grammars of verbsubcategorization, conjunction etc.Returning now to the example above,'move this here'.
The representation f 'move' isas before in Figure 3, except there is nommsubcat: feature.
The difference lies in therepresentation f the deictic expressions.
In thefirst pass of SLP, the deictic NP 'this' is assignedthe representation in Figure 6 (a).
I have used < >to represent the list-wdued mmsubcat: feature andthe constraints: feature is given in { }.
Thelocation deictic 'here' is assigned a similarrepresentation except hat its content:type: featurehas value location.
All deictic expressions (thosewith deictic: yes) are passed to MP.
MP uses ageneral subcategorization schema to combine'this' with an appropriate gesture, yielding therepresentation i  Figure 6 (b).
The multimodalsubcategorization schema changes the eat: featumfrom deictic_np to np when the mmsubcat: issaturated.
Much the same happens for 'here' andboth edges are passed back to SLP and added intothe chart (the chart: feature keeps track of theirlocation in the chart).
Now that the deictic NPshave been combined with gestures and convertedto NPs, spoken language parsing can proceed and'move' combines with 'this' and 'here' to yield anexecutable command which is then passed on toMP, which selects the optimal multimodalcommand and passes it on for execution.
Inexamples with conjunction such as 'move this andthis here', the deictic NPs am combined withgestures by MP belbre conjunction takes place inSLP, and so there is no need to complicate theanalysis of conjunction.cat : dcictic_npdeictic : yestime: \[1\[\[type: entity \]c?ntent : \[selection :\[21J/\[cat: spatial_gesture \ ] \/ I .
?
.
\[type:area 3/ \,:a> kso,o ,io,,.mmsubcat : tLtime :\[31 J\ /\{overlap(\[l\],\[3\]) v /\ fo l low( \ [1  \],\[3\]..5)} /chart : \[1,2\]\[cat : hi' \]\]deictic : no \]/L~/.
.
\[type : entity \ ] /  ""/":?"t?
"t \[ o'ootior, : \[o ioc,'dg4 .
H|mm,~ubcat : ( ) /\[chart :\[1,2\] \]Figure 6 Representation of 'this'In this approach, the level of constituent structureat which multilnodal integration applies is thesimple deictic NP.
It is preferable to integrate atthis level rather than the level of the deicticdeterminer, since other words in the simple NPwill place constraints on the choice andinterpretation of the gesture.
For example, 'thispetwon' is constrained to integrate with a gesture ata person while 'this number' is constrained tointegrate with a gesture at a number.4 Deictic numerical expressionsI turn now to the analysis of deictic expressionswith numerals.
An example command fi'om themultimodal messaging application domain is'email these four people'.
This could be handledby developing an analysis that assigns 'these fourpeople' a multimodal subcategorization whichselects for four spatial gestures at people: <Gpe,..,.o,,,Gm,..,.o,,.
Gp,.,,,.o,,.
Gp ........ ,>.
Similarly, 'these twoorganizations' would have tile followingmultimodal subcategorization: <Go,.~,,,,iz,tio,,,Go,.~,,,,iz,,~o,,>.
The multilnodal subcategorizationfiame will be saturated in MP throughcombination with the appropriate number ofindividual selection gestums.
The problem withthis approach is that it does not account for thewide range of different gesture patterns that can be366used to refer to a set of N objects on a disphty.Single objects may be selected using pointinggestures or circling (or underlining).
Circlinggestures can also be used to refer to sets of objectsand combinations of circling and pointing can beused to enumerate a set of entities.
Figure 7 showssome of the different ways that a set of fourobjects can be refened to using electronic ink.The graphical layout of objects on thescreen plays an ilnportant role in deterlnining thekind of gesture combinations that are likely.
Ifthree objects are close together and another furtheraway, the least effortl'ul gesture combination is tocircle the three and then circle oi point at theremaining one.
If all four are close together, thenit is easiest to make a single area gesturecontaining all four.
If other objects intervenebetween the objects to be selected, individualselections are lnore likely since there is less risk ofaccidentally selecting the intervening objects.
It isdesirable that multimodal systems be able tohandle the broad range of ways to selectcollections of entities so that users can utilize theand most natural gesture least effortfulcombination.no@030.63@ \ [ \ ]  mFigure 7 Gestures at collections of entitiesThe range of possible,gesture combinations can becaptured using multimodal subcategorization asabove, but this vastly complicates the SLPgrammar and leads to an explosion of ambiguity.Every time a numerical expression appears amultitude of alternative multimodalsubcategorization fralnes would need to beassigned to it.To address this problem, my approach is tounderspecify the particular configuration ofgestures in the multilnodal subcategorization o1'the deictic uumeral expression.
Instead ofsubcategorizing for a sequence of N gestures,'these N' subcategorizes for a collection ofplurality N : <G\[number:N\]>.
The expression'these fi~ttr people' has subcategorization<Gw.~.o,,\[mm,ber:4\]>.
An independent set of rolesfor gesture combination are used to enumerate allof the different ways to refer to a collection ofentities.
In simplil'ied form, the basic gesturecombination rule is as in Figure 8.G G G\[O,pc:lll \] \[~vt,~:lll -tvt, o:\[l\] \] \]mmtber : 12\] + 131/-> /'i"mhi": : I2\] / 1 liumber : 13\] /.sdeotio, i \[61 J \[.,'eleotio,, : 14\]_l \[.selection :\[ 51J{append(141, I51,161)}Figure 8 Gesture combination ruleThe rule is also constrained so that the combininggestures are adjacent in time and do not intersectwith each other.
The gesture combination ruleswill enumerate a broad range of possible gesturecollections (though not as many combiuations aswhen they are enumerated in the mullimodalsubcategorization frame).
The over-application ofthese rules can be prevented by using predictiveinformation from SLP; that is, if SLP parses 'these.four people' then these rules are applied to thegesture input in order to construct candidatecollections of four people.5 Integrat ion at higher levels of  const i tuentstructureIn the analysis developed above, multimodalinlegration takes place at the level of simpledeictic nominal expressions.
There are howevernmltimodal utterances where a single gesture mapsonto a higher level of constituent structure in thespoken language parse.
For example, 'move fromhere to there' could appear with two pointinggestures, but could also very well appear with aline gesture indicating the start and end of themove.
In this case, the integration coukt be kept atthe level of 'here' and 'there' by introducing a rulewhich splits line gestures into their componentstart and end points (Gli,,e ---) Gi,oim Gl,,,i,,t).
Theproblem with this approach is that it introducespoints that MP could then attempt to combine withother recognition results leading to an erroneousparse of the utterance.
To avoid this problem theSLP grammar can assign two possible analyses tothis string.
In one, both 'here' aud 'there' arepassed to MP for integration with point gestures.In the other, 'fi'om here to there' is parsed in SLP367and passed to MP for integration with a linegesture.
There are related examples withconjunction 'move this organization and thisdepartment here'.
An encircling esture could beused to identify 'this organization and thisdepartment' (especially if the pen is close to eachobject as the corresponding deictic phrase isuttered).
However, if in the general case we allowSLP to generate multiple analyzes of aconjunction, there will be an explosion of possiblepatterns generated, just as in the case of deicticnumeral expressions.
To overcome this difficulty,gesture decomposition rules can be used.
In orderto avoid errorful combinations with otherrecognition results, the application of these rules inMP needs to be driven by predictive informationfrom SLP; that is, in our example, if singlegestures cannot be found to combine with 'thisorganization' and 'this department', then thegesture decomposition rules are applied totemporally appropriate multiple selection gesturesto extract the needed individual selections.
Asimilar approach could be used to handle 'fi'omhere to there' with a controlled GI,-,,.
--~ @,o~,,t Gpoi, trule which only applies when required.ConclusionI have proposed an approach to nmltimodallanguage processing in which spoken languageparsing and nmltimodal parsing are more tightlycoupled than in the modular pipeliued approachtaken in Johnston 1998.
The spoken languageparsing component and nmltilnodal parsingcomponent cooperate in determining theinterpretation of nmltimodal utterances.
Thisenables multimodal integration to occur at a levelof constituent s ructure below the verbal utterancelevel specifically, the simple deictic noun phrase.This greatly simplifies the development of thespoken language parsing grammar as it is nolonger necessary construct a single multimodalsubcategorization list for the whole utterance.Following the modular approach of Johnston!
998a, the treatment of multimodalsubcategorization permeates the whole gramlnarcomplicating the analysis of verbsubcategorization, conjunction, possessives andinany other phenomena.
This new approach alsoenables more detailed inodeling of temporalconstraints in multi-gesture multimodal utterances.I have also argued that a deictic numeralexpression should multimodally subcategorize fora collection of entities and should beunderspecified with respect to the particularcombination of gestures used to pick out thecollection.
Possible combination patterns areenumerated by gesture composition rules.Communication between SLP and MP enablespredictive application of rules for gesturecomposition and decomposition which mightotherwise over-apply.ReferencesBuntschuh, B., Kamm, C., DiFabbrizio, G., Abella, A.,Mohri, M., Narayanan, S., Zeljkovic, I., Sharp, R.D.,Wright, J., Marcus, S., Shaffcr, J., Duncan, R. andWilpon, J.G.
1998.
VPQ: A spoken languageinterface to large scale directory information.
IllProceedings of lCSLP 98 (Sydney, Australia).Carpenter, R. 1992.
The logic of typed featurestructures.
Cambridge University Press, Cambridge,England.Gorin, A.L., Riccardi, G. and Wright, J.H.
1997.
"Howmay I help you?".
Speech Communication, vol.
23, p.113-127.Johnston, M. and S. Bangalore.
2000.
Finite-stateMultimodal Parsing and Understanding.
InProceedings of COLING-2000 (this volume).Johnston, M. 1998a.
Unification-based multimodalparsing.
In Proceedings of COLING-ACL 98, p. 624-630.Johnston, M. 1998b.
Multimodal anguage processing.In Proceedings of lCSLP 98 (Sydney, Australia).Johnston, M., Cohen, P.R., McGee, D., Oviatt, S.L.,Pittman, J.A., Smith, I.
1997.
Unification-basedmultimodal integration.
Proceedings of the 35thAmmal Meeting of the Associatiol~.\['or C mputationalLinguistics'.
Madrid, Spain.
p. 281-288.Kay, M. 1980.
Algoritlnn schemata and data structuresin syntactic processing.
In B. J. Grosz, K. S. Jones,and B. L. Webber (eds.)
Readings in NaturalLanguage P~vcessing, Morgan Kaufinann, 1986, p.35-70.Oviatt, S.L.
1997.
Multimodal interactive maps:Designing for human performance.
Human-Computerhzteraction, p. 93-129.Towns, S., Callaway, C., and Lester.
J.
1998.Generating coordinated natural language and 3danimations for complex spatial explanations.Proceedings of the Fifteenth National Conference onArtificial httelligence, p. 112-119.Wittenburg, K. 1993.
F-PATR: Functional constraintsfor unification-based grammars.
In Proceedings of31 't Annual meeting of the Association forComputational Linguistics, p. 216-223.368
