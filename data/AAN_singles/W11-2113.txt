Proceedings of the 6th Workshop on Statistical Machine Translation, pages 123?129,Edinburgh, Scotland, UK, July 30?31, 2011. c?2011 Association for Computational LinguisticsRegression and Ranking based Optimisation for Sentence Level MachineTranslation EvaluationXingyi Song and Trevor CohnThe Department of Computer ScienceUniversity of SheffieldSheffield, S1 4DP.
UK{xsong2,t.cohn}@shef.ac.ukAbstractAutomatic evaluation metrics are fundamen-tally important for Machine Translation, al-lowing comparison of systems performanceand efficient training.
Current evaluation met-rics fall into two classes: heuristic approaches,like BLEU, and those using supervised learn-ing trained on human judgement data.
Whilemany trained metrics provide a better matchagainst human judgements, this comes at thecost of including lots of features, leading tounwieldy, non-portable and slow metrics.
Inthis paper, we introduce a new trained met-ric, ROSE, which only uses simple featuresthat are easy portable and quick to compute.In addition, ROSE is sentence-based, as op-posed to document-based, allowing it to beused in a wider range of settings.
Results showthat ROSE performs well on many tasks, suchas ranking system and syntactic constituents,with results competitive to BLEU.
Moreover,this still holds when ROSE is trained on hu-man judgements of translations into a differentlanguage compared with that use in testing.1 IntroductionHuman judgements of translation quality are veryexpensive.
For this reason automatic MT evalu-ation metrics are used to as an approximation bycomparing predicted translations to human authoredreferences.
An early MT evaluation metric, BLEU(Papineni et al, 2002), is still the most commonlyused metric in automatic machine translation evalu-ation.
However, several drawbacks have been statedby many researchers (Chiang et al, 2008a; Callison-Burch et al, 2006; Banerjee and Lavie, 2005), mostnotably that it omits recall (substituting this with apenalty for overly short output) and not being easilyapplied at the sentence level.
Later heuristic metricssuch as METEOR (Banerjee and Lavie, 2005) andTER (Snover et al, 2006) account for both precisionand recall, but their relative weights are difficult todetermine manually.In contrast to heuristic metrics, trained met-rics use supervised learning to model directly hu-man judgements.
This allows the combinationof different features and can better fit specifictasks, such as evaluation focusing more on flu-ency/adequacy/relative ranks or post editing effort.Previous work includes approaches using classifica-tion (Corston-Oliver et al, 2001), regression (Alber-cht and Hwa, 2008; Specia and Gimenez, 2010), andranking (Duh, 2008).
Most of which achieved goodresults and better correlations with human judg-ments than heuristic baseline methods.Overall automatic metrics must find a balance be-tween several key issues: a) applicability to differ-ent sized texts (documents vs sentences), b) easyof portability to different languages, c) runtime re-quirements and d) correlation with human judge-ment data.
Previous work has typically ignored atleast one of these issues, e.g., BLEU which appliesonly to documents (A), trained metrics (Alberchtand Hwa, 2008; Specia and Gimenez, 2010) whichtend to ignore B and C.This paper presents ROSE, a trained metric whichis loosely based on BLEU, but seeks to further sim-plify its components such that it can be used for sen-tence level evaluation.
This contrasts with BLEUwhich is defined over large documents, and must123be coarsely approximated to allow sentence levelapplication.
The increased flexibility of ROSE al-lows the metric to be used in a wider range of situ-ations, including during decoding.
ROSE is a linearmodel with a small number of simple features, andis trained using regression or ranking against humanjudgement data.
A benefit of using only simple fea-tures is that ROSE can be trivially ported betweentarget languages, and that it can be run very quickly.Features include precision and recall over differentsized n-grams, and the difference in word countsbetween the candidate and the reference sentences,which is further divided into content word, func-tion word and punctuation.
An extended versionsalso includes features over Part of Speech (POS) se-quences.The paper is structured as follows: Related workon metrics for statistical machine translation is de-scribed in Section 2.
Four variations of ROSE andtheir features will be introduced in Section 3.
In sec-tion 4 we presents the result, showing how ROSEcorrelates well with human judgments on both sys-tem and sentence levels.
Conclusions are given atthe end of the paper.2 Related WorkThe defacto standard metric in machine translationis BLEU (Papineni et al, 2002).
This measuresn-gram precision (n normally equal to 1,2,3,4) be-tween a document of candidate sentences and aset of human authored reference documents.
Theidea is that high quality translations share many n-grams with the references.
In order to reduce re-peatedly generating the same word, BLEU clips thecounts of each candidate N-gram to the maximumcounts of that n-gram that in references, and witha brevity penalty to down-scale the score for out-put shorter than the reference.
In BLEU, each n-gram precision is given equal weight in geometricmean, while NIST (Doddington and George, 2002)extended BLEU by assigning more informative n-grams higher weight.However, BLEU and NIST have several draw-backs, the first being that BLEU uses a geometricmean over all n-grams which makes BLEU almostunusable for sentence level evaluations 1.
Secondly,1Note that various approximations exits (Lin and Och, 2004;BLEU and NIST both use the brevity penalty to re-place recall, but Banerjee and Lavie (2005) in exper-iments show that the brevity penalty is a poor sub-stitute for recall.Banerjee and Lavie (2005) proposed a METEORmetric, which that uses recall instead of the BP.Callison-Burch et al (2007; Callison-Burch et al(2008) show that METEOR does not perform well inout of English task.
This may because that Stemmeror WordNet may not available in some languages,which unable to model synonyms in these cases.
Inaddition, the performance also varies when adjustingweights in precision and recall.Supervised learning approaches have been pro-posed by many researchers (Corston-Oliver et al,2001; Duh, 2008; Albercht and Hwa, 2008; Spe-cia and Gimenez, 2010).
Corston-Oliver et al(2001) use a classification method to measure ma-chine translation system quality at the sentence levelas being human-like translation (good) or machinetranslated (bad).
Features extracted from referencesand machine translation include heavy linguistic fea-tures (requires parser).Quirk (2004) proposed a linear regression modelwhich is trained to match translation quality.
Alber-cht and Hwa (2008) introduced pseudo-referenceswhen data driven regression does not have enoughtraining data.
Most recently, Specia and Gimenez(2010) combined confidence estimation (withoutreference, just using the source) and reference-basedmetrics together in a regression framework to mea-sure sentence-level machine translation quality.Duh (2008) compared the ranking with the re-gression, with the results that with same feature set,ranking and regression have similar performance,while ranking can tolerate more training data noise.3 ModelROSE is a trained automatic MT evaluation metricthat works on sentence level.
It is defined as a linearmodel, and its weights will be trained by SupportVector Machine.
It is formulated asS = ?
?w ?f(?
?c ,?
?r ) (1)where ?
?w is the feature weights vector, f(?
?c ,?
?r ) isthe feature function which takes candidate transla-Chiang et al, 2008b)124tion (?
?c ) and reference (?
?c ), and returns the featurevector.
S is the response variable, measuring the?goodness?
of the candidate translation.
A higherscore means a better translation, although the mag-nitude is not always meaningful.We present two different method for training:a linear regression approach ROSE-reg, trained tomatch human evaluation score, and a ranking ap-proach ROSE-rank to match the relative ordering ofpairs of translations assigned by human judge.
Un-like ROSE-reg, ROSE-rank only gives relative scorebetween sentences, such as A is better than B. Thefeatures that used in ROSE will be listed in section3.1, and the regression and ranking training are de-scribed in section 3.23.1 ROSE FeaturesFeatures used in ROSE listed in Table 1 includestring n-gram matching, Word count and Part ofSpeech (POS).
String N-gram matching features, areused for measure how closely of the candidate sen-tence resembles the reference.
Both precision andrecall are considered.
Word count features measurelength differences between the candidate and refer-ence, which is further divided into function words,punctuation and content words.
POS features aredefined over POS n-gram matches between the can-didate and reference.3.1.1 String Matching FeaturesThe string matching features include n-gram pre-cision, n-gram recall and F1-measure.
N-gramprecision measures matches between sequence ofwords in the candidate sentence compared to the ref-erences,Pn =?n-gram??
?c Count(n-gram)Jn-gram ??
?r K?n-gram??
?c Count(ngram)(2)where Count are the occurrence counts of n-gramsin the candidate sentence, the numerator measuresthe number of predicted n-grams that also occur inthe reference.Recall is also used in ROSE, so clipping wasdeemed unnecessary in precision calculation, wherethe repeating words will increasing precision but atthe expense of recall.
F-measure is also included,which is the harmonic mean of precision and recall.ID Description1-4 n-gram precision, n=1...45-8 n-gram recall, n=1...49-12 n-gram f-measure, n=1...413 Average n-gram precision14 Words count15 Function words count16 Punctuation count17 Content words count18-21 n-gram POS precision, n=1...422-25 n-gram POS recall, n=1...426-29 n-gram POS f-measure, n=1...430-33 n-gram POS string mixed precision,n=1...4Table 1: ROSE Features.
The first column is the featurenumber.
The dashed line separates the core features fromthe POS extended features.With there are multiple references, the n-gram preci-sion error uses the same strategy as BLEU: n-gramsin candidate can match any of the references.
Forrecall, ROSE will match the n-grams in each refer-ence separately, and then choose the recall for thereference with minimum error.3.1.2 Word Count FeaturesThe word count features measure the length dif-ference between a candidate sentence and referencesentence.
In a sentence, content words are more in-formative than function words (grammatical words)and punctuation.
Therefore, the number of contentword candidate is a important indicator in evalua-tion.
In this case, besides measuring the length atwhole sentences, we also measure difference in thenumber of function words, punctuation and contentwords.
We normalise by the length of the refer-ence which allows comparability between short ver-sus long sentences.
In multiple reference cases wechoose the ratio that is closest to 1.3.1.3 Part of Speech FeaturesThe string matching features and word count fea-tures only measure similarities on the lexical level,but not over sentence structure or synonyms.
To addthis capability we also include Part of Speech (POS)features which work similar to the String Matchingfeatures, but using POS instead of words.
The fea-125tures measure precision, recall and F-measure overPOS n-grams (n=1...4).
In addition, we also includefeatures that mixed string and POS.The string/POS mixed feature is used for handlingsynonyms.
One problem in string n-gram match-ing is not being able to deal with the synonyms be-tween the candidate translation and the reference.One approach for doing so is to use an external re-source such as WordNet (Banerjee and Lavie, 2005),however this would limit the portability of the met-ric.
Instead we use POS as a proxy.
In most ofthe cases, synonyms share the same POS, so thiscan be rewarded by forming n-grams over a mix-ture of tokens and POS.
During the matching pro-cess, both words and its POS shall be considered, ifeither matches between reference and candidate, then-gram matches will be counted.Considering the example in table 2, candidate 1has better translation than candidate 2 and 3.
If onlythe string N-gram matching is used, that will give thesame score to candidate 1, 2 and 3.
The n-gram pre-cision scores obtained by all candidate sentences inthis example are: 2-gram = 1, 3-gram = 0.
However,we can at least distinguish candidate 1 is better thancandidate 3 if string POS mixed precision is used ,n-gram precision for candidate 1 will be: 2-gram =2, 3-gram = 1, which ranks candidate 1 better thancandidate 3.Examplereference: A/DT red/ADJ vehicle/NNcandidate 1: A/DT red/ADJ car/NNcandidate 2: A/DT red/ADJ rose/NNcandidate 3: A/DT red/ADJ red/ADJTable 2: Evaluation Example3.2 TrainingThe model was trained on human evaluation datain two different ways, regression and ranking.Theseboth used SVM-light (Joachims, 1999).
In the rank-ing model, the training data are candidate translationand their relative rankings were ranked by humanjudge for a given input sentence.
The SVM findsthe minimum magnitude weights that are able to cor-rectly rank training data which is framed as a seriesof constraints reflecting all pairwise comparisons.
Asoft-margin formulation is used to allow training er-rors with a penalty (Joachims, 2002).
For regres-sion, the training data is human annotation of post-edit effort (this will be further described in section4.1).
The Support vector Regression learns weightswith minimum magnitude that limit prediction er-ror to within an accepted range, again with a soft-margin formulation (Smola and Schlkopf, 2004).A linear kernel function will be used, becausenon-linear kernels are much slower to use and arenot decomposable.
Our experiments showed that thelinear kernel performed at similar accuracy to otherkernel functions (see section 4.2).4 Experimental SetupOur experiments test ROSE performance on docu-ment level with three different Kernel functions: lin-ear, polynomial and radial basis function.
Then wecompare four variants of ROSE with BLEU on bothsentence and system (document) level.The BLEU version we used here is NIST OpenMT Evaluation tool mteval version 13a, smooth-ing was disabled and except for the sentence levelevaluation experiment.
The system level evalua-tion procedure follows WMT08 (Callison-Burch etal., 2008), which ranked each system submitted onWMT08 in three types of tasks:?
Rank: Human judges candidate sentence rankin order of quality.
On the document level, doc-uments are ranked according to the proportionof candidate sentences in a document that arebetter than all of the candidates.?
Constituent: The constituent task is the sameas for ranking but operates over chosen syntac-tic constituents.?
Yes/No: WMT08 Yes/No task is to let humanjudge decide whether the particular part of asentence is acceptable or not.
Document levelYes/No ranks a document according to theirnumber of YES sentencesSpearman?s rho correlation was used to measurethe quality of the metrics on system level.
Four tar-get languages (English, German, French and Span-ish) were used in system level experiments.
ROSE-126reg and ROSE-rank were tested in all target lan-guage sets, but ROSE-regpos was only tested in theinto-English set as it requires a POS tagger.
On thesentence level, we compare sentences ranking thatranked by metrics against human ranking.
The eval-uation quality was examined by Kendall?s tau cor-relation, and tied results from human judges wereexcluded.Rank es-en fr-en de-en avgLinear 0.57 0.97 0.69 0.74Polynomial 0.62 0.97 0.71 0.76RBF 0.60 0.98 0.62 0.73ConstituentLinear 0.79 0.90 0.39 0.69Polynomial 0.80 0.89 0.41 0.70RBF 0.83 0.93 0.34 0.70Yes/NoLinear 0.92 0.93 0.67 0.84Polynomial 0.86 0.90 0.66 0.81RBF 0.87 0.93 0.65 0.82Table 3: ROSE-reg in with SVM kernel functionsMetric Kendall?s tauBLEU-smoothed 0.219ROSE-reg 0.120ROSE-regpos 0.164ROSE-rank 0.206ROSE-rankpos 0.172Table 4: Sentence Level Evaluation4.1 DataTraining data used for ROSE is from WMT10(Callison-Burch et al, 2010) human judged sen-tences.
A regression model was trained by sentenceswith human annotation for post editing effort.
Thethree levels used in WMT10 are ?OK?, ?EDIT?
and?BAD?, which we treat as response values of 3, 2and 1.
In total 2885 sentences were used in the re-gression training.
The ranking model was trained bysentences with human annotating sentence ranking,and tied results are allowed in training.
In this exper-iment, 1675 groups of sentences were used for train-ing, and each group contains five sentences, whichare manually ranked from 5 (best) to 1 (worst).
In or-der to test the ROSE?s ability to adapt the languagewithout training data, ROSE was only trained withEnglish data.The testing data on sentence level used in thispaper is human ranked sentences from WMT09(Callison-Burch et al, 2009).
Tied rankings were re-moved, leaving 1702 pairs.
We only consider trans-lations into English sentences.
On system level, thetesting data are the submissions for ?test2008?
testset in WMT08 (Callison-Burch et al, 2008).
ROSE,and BLEU were compared with human ranked sub-mitted system in ?RANK?, ?CONSTITUENT?
and?YES/NO?
tasks.English punctuation and 100 common functionwords list of four languages in this experiment weregenerated.
English POS was tagged by NLTK (Birdand Loper, 2004).4.2 Results and DiscussionTable 3 shows the results of ROSE-reg with threedifferent SVM Kernel functions.
Performance aresimilar among three different Kernel functions.However, the linear kernel is the fastest and simplestand there is no overall winner.
Therefore, linear Ker-nel function was used in ROSE.The results of Kendall?s tau on sentence levelevaluation are shown in Table 4.
According to Ta-ble 4 ROSE-rank has the highest score in all ver-sions of ROSE.
The score is close to the smoothedversion of BLEU.
Results also showed adding POSfeature helped in improving accuracy in the regres-sion model, but not in ranking, The reason for this isnot clear, but it may be due to over fitting.Table 5 and Table 6 are the Spearman?s rho in sys-tem ranking.
Table 5 is the task evaluation for trans-lation into English.
ROSE-rank performed the bestin the system ranking task.
Also, ROSE-regpos isthe best in the syntactic constituents task.
This maybecause of ROSE-rank is a ranking based metric andROSE-regpos incorporates POS that contains morelinguistic information.
Table 6 shows the results ofevaluating translations from English.
According tothe table, ROSE performs less accurately than forthe into-English tasks, but overall the ROSE scoresare similar to those of BLEU.127Rank es-en fr-en de-en avgBLEU 0.66 0.97 0.69 0.77ROSE-reg 0.57 0.97 0.69 0.74ROSE-rank 0.85 0.96 0.76 0.86ROSE-regpos 0.59 0.98 0.71 0.76ROSE-rankpos 0.83 0.96 0.69 0.82ConstituentBLEU 0.78 0.92 0.30 0.67ROSE-reg 0.79 0.90 0.39 0.69ROSE-rank 0.66 0.92 0.33 0.64ROSE-regpos 0.79 0.90 0.41 0.70ROSE-rankpos 0.64 0.93 0.31 0.63Yes/NoBLEU 0.99 0.96 0.66 0.87ROSE-reg 0.92 0.93 0.67 0.84ROSE-rank 0.78 0.96 0.61 0.78ROSE-regpos 0.97 0.93 0.66 0.85ROSE-rankpos 0.81 0.96 0.57 0.78Table 5: System Level evaluation that translation into En-glish5 ConclusionWe presented the ROSE metric to make up for sev-eral drawbacks of BLEU and other trained metrics.Features including string matching, words ratio andPOS were combined by the supervised learning ap-proach.
ROSE?s overall performance was close toBLEU on system level and sentence level.
However,it is better on tasks ROSE was specifically trained,such as ROSE-rank in the system level ranking taskand ROSE-regpos in the syntactic constituents task.Results also showed that when training data is notavailable in the right language ROSE produces rea-sonable results.Smoothed BLEU slightly outperformed ROSE insentence evaluation.
This might be due to the train-ing data not being expert judgments, and conse-quently very noisy.
In further work, we shall mod-ify the training method to better tolerate noise.
Inaddition, we will modify ROSE by substitute lessinformative features with more informative featuresin order to improve its performance and reduce overfitting.Rank es-en fr-en de-en avgBLEU 0.85 0.98 0.88 0.90ROSE-reg 0.75 0.98 0.93 0.89ROSE-rank 0.69 0.93 0.94 0.85ConstituentBLEU 0.83 0.87 0.35 0.68ROSE-reg 0.73 0.87 0.36 0.65ROSE-rank 0.72 0.78 0.32 0.61Yes/NoBLEU 0.75 0.97 0.89 0.87ROSE-reg 0.72 0.97 0.93 0.87ROSE-rank 0.82 0.96 0.87 0.88Table 6: System Level evaluation that translation fromEnglishReferencesJosha S. Albercht and Rebecca Hwa.
2008.
Regres-sion for machine translation evaluation at the sentencelevel.
Machine Translation, 22:1?27.Satanjeev Banerjee and Alon Lavie.
2005.
Meteor: Anautomatic metric for mt evaluation with improved cor-relation with human judgments.
Proceedings of theACL-05 Workshop.Steven Bird and Edward Loper.
2004.
Nltk: The naturallanguage toolkit.
In Proceedings of the ACL demon-stration session, pages 214?217, Barcelona, July.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of bleu in ma-chine translation research.
In In EACL, pages 249?256.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2007.
(meta-)evaluation of machine translation.
In Proceedings ofthe Second Workshop on Statistical Machine Transla-tion, pages 136?158, Prague, Czech Republic, June.Association for Computational Linguistics.Chris Callison-Burch, Cameron Fordyce, Philipp Koehn,Christof Monz, and Josh Schroeder.
2008.
Furthermeta-evaluation of machine translation.
In Proceed-ings of the Third Workshop on Statistical MachineTranslation, pages 70?106, Columbus, Ohio, June.Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28, Athens, Greece,March.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Kay Peterson, Mark Przybocki, and Omar Zaidan.1282010.
Findings of the 2010 joint workshop on sta-tistical machine translation and metrics for machinetranslation.
In Proceedings of the Joint Fifth Workshopon Statistical Machine Translation and MetricsMATR,pages 17?53, Uppsala, Sweden, July.
Association forComputational Linguistics.
Revised August 2010.David Chiang, Steve DeNeefe, Yee Seng Chan, andHwee Tou Ng.
2008a.
Decomposability of trans-lation metrics for improved evaluation and efficientalgorithms.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,EMNLP ?08, pages 610?619, Stroudsburg, PA, USA.Association for Computational Linguistics.David Chiang, Yuval Marton, and Philip Resnik.
2008b.Online large-margin training of syntactic and struc-tural translation features.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, EMNLP ?08, pages 224?233, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Simon Corston-Oliver, Michael Gamon, and ChrisBrockett.
2001.
A machine learning approach to theautomatic evaluation of machine translation.
In pro-ceedings of the Association for Computational Lin-guistics.Doddington and George.
2002.
Automatic evalua-tion of machine translation quality using n-gram co-occurrence statistics.
In Proceedings of the secondinternational conference on Human Language Tech-nology Research, HLT ?02, pages 138?145, San Fran-cisco, CA, USA.
Morgan Kaufmann Publishers Inc.Kevin Duh.
2008.
Ranking vs. regression in machinetranslation evaluation.
In In Proceedings of the ThirdWorkshop on Statistical Machine Translation, pages191?194, Columbus,Ohio,, June.T.
Joachims.
1999.
Making large-scale svm learningpractical.
Advances in Kernel Methods - Support Vec-tor Learning,.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In Proceedings of the ACM Con-ference on Knowledge Discovery and Data Mining(KDD).Chin-Yew Lin and Franz Josef Och.
2004.
Orange: amethod for evaluating automatic evaluation metrics formachine translation.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,COLING ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.C Quirk.
2004.
Training a sentence-level machine trans-lation confidence measure.
In In: Proceedings of theinternational conference on language resources andevaluation, pages 825?828, Lisbon, Portugal.Alex J. Smola and Bernhard Schlkopf.
2004.
A tuto-rial on support vector regression.
STATISTICS ANDCOMPUTING, 14:199?222.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and Ralph Weischedel.
2006.
A study oftranslation error rate with targeted human annotation.L.
Specia and J. Gimenez.
2010.
Combining confidenceestimation and reference-based metrics for segment-level mt evaluation.
In The Ninth Conference of theAssociation for Machine Translation in the Americas,Denver,Colorado.129
