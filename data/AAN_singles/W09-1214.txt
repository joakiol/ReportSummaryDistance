Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL): Shared Task, pages 91?96,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsThe Crotal SRL System : a Generic Tool Based on Tree-structured CRF?Erwan MoreauLIPN - CNRS UMR 7030 & Univ.
Paris 13Erwan.Moreau@lipn.univ-paris13.frIsabelle TellierLIFO - Univ.
Orle?ansIsabelle.Tellier@univ-orleans.frAbstractWe present the Crotal system, used in theCoNLL09 Shared Task.
It is based on XCRF,a highly configurable CRF library which cantake into account hierarchical relations.
Thissystem had never been used in such a contextthus the performance is average, but we areconfident that there is room for progression.1 IntroductionIn this paper we present the Crotal Semantic RoleLabelling (SRL) system, which has been used inthe CoNLL 2009 Shared Task (Hajic?
et al, 2009)1.This system is based on Conditional Random Fields(CRF) (Lafferty et al, 2001; Sutton and McCallum,2006): our idea is that we can use the provideddependency structure as the skeleton of a graphi-cal model expressing independence asumptions ina CRF model.
CRF are a powerful machine learn-ing technique that has been successfully applied toa large number of natural language tasks, mainlyto tag sequences.
Compared to classification tech-niques, CRF can easily take into account dependen-cies among annotations: it is therefore possible torepresent tree-like structures in the input of the al-gorithm.
Recently, CRF using tree structures wereused in (Finkel et al, 2008) in the case of parsing.Before participating to this Shared Task, our pro-totype had only been used to annotate function tagsin a French Treebank: these data were drastically?This work has been funded by the French National projectANR-07-MDCO-03 ?CRoTAL?.1We have participated in the SRL-only category.smaller, and the task was simpler.
Therefore CoNLL2009 ST is the first time the Crotal System is runfor a quite complex task, with so many data as in-put, and seven different languages (Catalan, Span-ish (Taule?
et al, 2008), Chinese (Palmer and Xue,2009), Czech (Hajic?
et al, 2006), English (Surdeanuet al, 2008), German (Burchardt et al, 2006) andJapanese (Kawahara et al, 2002)).
In this context,the performance we obtained seems reasonable: ouraverage F1-measure is 66.49% (evaluation dataset).One of the advantages we want to emphasiseabout our system is its genericity: the system doesnot need a lot of information as input (we mainlyuse pos and deprel columns, and the frame sets havenot been used), and it was able to achieve satisfy-ing results for the seven different languages usingnearly the same parameters (differences were essen-tially due to the volume of data, since it was some-times necessary to reduce the processing time).
Ofcourse, we hope to improve this prototype thanks tothis experience: it may become necessary to lose ingenericity in order to gain in performance, but ourgoal is to maintain as much as possible this advan-tage.In section 2 we explain the general architecturefor Crotal, then we explain how features are selectedin our system in section 3, and finally we detail anddiscuss the results in section 4.2 The Crotal System Architecture2.1 General principleThe system we propose is based on the public libraryXCRF (Gilleron et al, 2006; Jousse, 2007), which91implements CRF model(s) to learn to annotate treesrepresented by XML documents.
Of course, its per-formance depends on the way it is used, and espe-cially on how features are chosen to reliably repre-sent the labeled data.
In order to keep the systemas generic as possible, features are generated auto-matically and only a few parameters may vary.
Theglobal process has been divided into a sequence ofsteps, by creating clusters (one for each predicate,except the less frequent ones).
Indeed, one expectsthat the behaviour of the arguments for a given pred-icate is more regular than for all predicates put to-gether.
Moreover, the size of the training set forall seven languages allows such a clustering, and itwould even be difficult to process the whole set ofpredicates due to time and memory limitations.
Thusthe global process is2:1.
Data conversion from CoNLL format to XCRFformat:?
For each sentence containing n predicates,generate n different XML trees3.?
The tree is simply built following thedependencies (as provided by the headcolumn).
Therefore the possible non-projectivity of a tree is ignored, though theorder of words is of course prefered when-ever possible.
An artificial root node is al-ways added (useful for languages whereseveral roots are possible).?
In each such XML tree, there is only one(marked) predicate, and in the annotatedversion its arguments (extracted from thecorresponding column) and only them arereported in the corresponding nodes.Figure 1 shows the labeled XML tree obtainedfor a (part of) example sentence.2.
Clustering by lemma: all dependency trees hav-ing the same lemma as predicate are put to-gether if the number of such trees is at least a2Remark: unless stated otherwise, we will use terms?lemma?, ?POS tag?
?dependency relation?
or ?head?
to referto the information contained in the corresponding ?P-columns?for each word.
It is worth noticing that performance would bebetter using the ?real?
columns, but we have followed the in-structions given by the organizers.3Thus sentences with no predicate are skipped and severaltrees possibly correspond to the same sentence.given threshold (generally 3, also tested with2 to 5).
There is a special cluster for less fre-quent lemmas4.
Then, for each cluster, in train-ing mode the process consists of:(a) Generation of features for the argumentstraining step.
(b) The CRF model for arguments is trainedwith XCRF.
(c) Generation of features for the senses train-ing step.
(d) The CRF model for senses5 is trained withXCRF.In annotation mode, the CRF model for argu-ments is first applied to the input tree, then theCRF model for senses (if possible, an individ-ual evaluation is also computed).3.
Back conversion from XCRF format to CoNLLformat (in annotation mode).In the framework of this task, features generationis crucial for improving performance.
That is whywe will mainly focus on that point in the remainingof this paper.2.2 The XCRF LibraryXCRF (Gilleron et al, 2006; Jousse, 2007) is a pub-lic library which has been applied successfully toHTML documents in order to extract information ortranslate the tree structure into XML (Jousse, 2007).More recently we have applied it to annotate func-tion tags in a French Treebank.In a CRF model, a feature is a function (usuallyproviding a boolean result) whose value depends onthe annotations present in a special clique of thegraph, and on the value of the observed data.
Inour system, each feature is defined by a pair (C, T ),where:?
C is the set of annotations present in a givenclique, i.e.
a completely connected subgraphof the graphical structure between annotations.4This special cluster is used as a default case.
In particular,if an unknown lemma is encoutered during annotation, it willbe annotated using the model learned for this default cluster.5Steps 2c and 2d are skipped if the lemma has only one pos-sible sense (or no sense is needed, like in Japanese data and forsome Czech predicates).92Several solutions are possible to choose thisgraph.
In most of our experiments, we havechosen a graph where only the node-parentrelationship between nodes is taken into ac-count (denoted FT2), as illustrated by Figure2.
XCRF is also able to deal with simple one-node cliques (no dependency between annota-tion, denoted FT1) and node-parent-sibling re-lationship (denoted FT3).?
T = {t1, .
.
.
, tn} is a (possibly empty) setof boolean tests on the observation (i.e.
notdepending on the annotations).
Each ti is anatomic test6: for example, the test ?pos attributefor first left sibling is NNS?
is satisfied for node3 in fig.
1.
T is the conjunction of all ti.For example, let us define the following FT2 fea-ture (C, T ), that would be true for node 4 in fig.1: C is {apredparent = PRED ?
apredcurrent =C-A1} and T is {poschild1 = VB ?
deprelparent =VC}.3 Selecting FeaturesOur goal is somehow to ?learn?
features from thetraining set, in the sense that we do not explicitlydefine them but generate them from the corpus.
Themain parameters we use for generating a set of fea-tures are the following:?
The feature type n, with n ?
3.
All FT n?,with n?
?
n, are also considered, because somefunction tags possibly appear in FT n and not(or more rarely) in FT n + 1.?
Various kind of accessible information (decom-posed through two distinct parameters informa-tion and neighbourhood):?
Information: form, lemma, POS tags, de-pendency relation and various secondaryattributes (column features) are availablefor all nodes (i.e.
word), in every tree ex-tracted from the corpus.?
Neighbourhood: Given a current node, the?neighbourhood?
defines the set of nodes6A test is provided to XCRF as an XPath expression, whichwill be applied to the current node in the XML tree correspond-ing to the sentence.Sentence2, areVBP, ROOT1, ExportsNNS, SBJA13, thoughtVBN, VCPRED4, toTO, OPRDC-A15, haveVB, IM6, risenVBN, VC7, stronglyRB, MNR8, inIN, TMP9, AugustNNP, PMOD[...]Figure 1: a labeled example for the (part of) sentence?Exports are thought to have risen strongly in August[...]?
: the nodes are represented with their POS tags, andin bold face the corresponding annotation associated withthe predicate ?thought?
(label PRED was added duringpreprocessing, see 3.1)?
?A1 PREDC-A1???
??[...
]Figure 2: graph for a FT2-CRF for the annotation of thesentence of Figure 1 (where ?
means ?no annotation?
)93that will be observed to help deduce its an-notation: only this node, or also its parent,possibly its siblings, etc.?
The maximum number of (atomic) tests in theset T for these nodes: combining several testsmakes features more precise (conjunction), butalso more numerous.A few other parameters may be added to speed uplearning:?
minimum proportion for an argument labelwhich is present in the data to be taken into ac-count,?
minimum proportion for a feature which ispresent in the data to be included in the model,?
and maximum number of sentences to processby XCRF in the training step.We try to use as less linguistic knowledge as pos-sible, because we are interested in testing to whatextent the model is able to learn such knowledge byitself.
Moreover, we observe that using too manyfeatures and/or examples as input in XCRF requiresa lot of time and memory (sometimes too much), sowe have to restrict the selection to the most relevantkind of information in order to get a tractable ma-chinery.
This is why we use only POS tags (pos)and dependency relations (deprel) (as one can see infig.
1).
Finally the process of generating featuresconsists in parsing the training data in the follow-ing way: for each encoutered clique, all the possible(combinations of) tests concerning the given neigh-bourhood are generated, and each of them forms afeature together with the observed clique.3.1 Learning Argument RolesIn our system, the arguments and the sense of a pred-icate are trained (or annotated) one after the other:the former is always processed before the latter, thusthe dependency holds only in the direction from ar-guments to sense.
Therefore the training of argu-ments only relies on the observed trees (actuallyonly the neighbourhood considered and the argu-ments cliques).
In order to help the learner locatethe right arguments, a special label PRED is addedas ?argument?
to the node corresponding to the tar-get predicate: by this way cliques can more easilytake the tree structure into account in the neighbour-hood of the predicate.After some tests using the development set astest set, we observed that the following parameterswere the best suited to build a reliable CRF model(for the arguments) in a reasonable time (and thusused them to learn the final models): the neigh-bourhood consists in the node itself, its parent andgrand-parent, first and second siblings on both sidesand first child; the FT2 model performs quite cor-rectly (FT3 has been discarded because it wouldhave taken too much time), and at most two testsare included in a feature.3.2 Learning Predicate SensesThe step of predicting senses can use the argumentsthat have been predicted in the previous step.
In par-ticular, the list of all arguments that have been foundis added and may be used as a test in any feature.We did not use at all the frame sets provided withthe data: our system is based only on the sentences.This choice is mainly guided by our goal to build ageneric system, thus does not need a lot of input in-formation in various formats.
The lemma part of thepredicate is simply copied from the lemma column(this may cause a few errors due to wrong lemmas,as observed in the English data).The fact that sentences have been classified bylemma makes it convenient to learn/annotate senses:of course lemmas which can not have more than onesense are easily processed.
In the general case, wealso use XCRF to learn a model to assign senses foreach lemma, using the following parameters: thereis no need to use another model than FT1, since ineach tree there is only one (clearly identified) nodeto label; a close neighbourhood (parent, first left andright siblings and first child) and only two tests areenough to obtain satisfactory results.4 Results and Discussion4.1 General ResultsDue to limited time and resources, we had to relaxsome time-consuming constraints for some clustersof sentences (concerning mainly the biggest trainingsets, namely Czech and English): in some cases, the94threshold for a feature to be selected has been in-creased, resulting in a probably quite lower perfor-mance for these models.
Ideally we would also havedone more tests with all languages to fine-tune pa-rameters.
Nevertheless, we have obtained quite sat-isfying results for such a generic approach: the av-erage F1-measure is 66.49%, ranging from 57.75%(Japanese) to 72.14% (English).
These results showthat the system is generic enough to work quite cor-rectly with all seven languages7 .4.2 Internal EvaluationHere we report detailed results obtained in anno-tating the development set.
Since we process thetask in two distinct steps, we can evaluate bothseparately: for the arguments step, the F1-measureranges from 56.0% (Czech) to 61.8% (German), ex-cept for Japanese data where it is only 27%.
For thesenses step, the F1-measure is generally better: itranges from 61.5% for the Czech case8 to 93.3% forChinese.It is also interesting to observe the differencebetween using ?real?
indicators (i.e.
lemma, pos,deprel and head columns) versus predicted ones(i.e.
P-columns): for example, with German data(respectively Catalan data) the F1-measure reaches73.6% (resp.
70.8%) in the former case, but only61.8% (resp.
60.6%) in the latter case (for the argu-ment labeling step only).4.3 Impact of ParametersAt first we intended to use the most precise CRFmodel (namely FT3), but the fact that it generatesmany more features (thus taking too much time) to-gether with the fact that it does not improve perfor-mance a lot made impossible to use it for the wholedata.
More precisely, it was possible but only by set-ting restrictive values for other parameters (neigh-bourhood, thresholds), which would have decreasedperformance.
This is why we had to use FT2 as a7Actually detailed evaluation shows that the system does notdeal very well with Japanese, since locating arguments is harderin this language.8Counting only ?real senses?
: it is worth noticing that Czechdata were a bit different from the other languages concerningsenses, since most predicates do not have senses (not countedhere and easy to identify) and the set of possible senses is dif-ferent for each lemma.compromise, thus making possible to use better val-ues for the other parameters.
We have also tested us-ing 3 tests instead of only 2, but it does not improveperformance, or not enough to compensate for thehuge number of generated features, which requiresexcessive time and/or memory for XCRF learningstep.One of the most important parameters is theneighbourhood, since it specifies the location (andconsequently the amount) of the information takeninto account in the features.
We have tried differ-ent cases for both the argument labeling step and thesense disambiguation step: in the former case, ob-serving children nodes is useless, whereas observingthe parent and grand-parent nodes together with twosiblings in both left and right handside improves themodel.
On the contrary, in the senses step observingmore than close nodes is useless.
These facts are notsurprising, since arguments are generally hierarchi-cally lower than predicates in the dependency trees.We have also studied the problem of finding anoptimal threshold for the minimum number of sen-tences by cluster (all sentences in a given clusterhaving the same lemma for predicate): if this thresh-old is too low some clusters will not contain enoughexamples to build a reliable model, and if it is toohigh a lot of sentences will fall in the default clus-ter (for which the model could be less precise).
Butsurprisingly the results did not show any significantdifference between using a threshold of 2, 3 or 5:actually individual results differ, but the global per-formance remains the same.Finally a word has to be said about ?efficiency pa-rameters?
: the most important one is the minimumproportion for a generated feature to be included inthe final set of features for the model.
Clearly, thelower this threshold is, the better the performanceis.
Nevertheless, in the framework of a limited timetask, it was necessary to set a value of 0.0005% inmost cases, and sometimes a higher value (up to0.001%) for the big clusters: these values seem lowbut prevent including a lot of features (and probablysometimes useful ones).5 Problems, Discussion and Future WorkSince there was a time limit and the system was usedfor the first time for such a task, we had to face95several unexpected problems and solve them quiterapidly.
Therefore one may suppose that our systemcould perform better, provided more tests are done tofine-tune parameters, especially to optimize the bal-ance between efficiency and performance.
Indeed,there is a balance to find between the amount of in-formation (number of features and/or examples) andthe time taken by XCRF to process the training step.Generally speaking, performance increases with theamount of information, but practically XCRF cannot handle a huge number of features and/or exam-ples in a reasonable time.
This is why selecting the?right?
features as soon as possible is so important.Among various possible ways to improve the sys-tem, we should benefit from the fact that CRF do notneed a lot of examples as input to learn quite cor-rectly.
Informally, the XCRF library seems to havesome kind of ?optimal point?
: before this point themodel learned could be better, but beyond this pointtime and/or memory are excessive.
Thus one cantry for example to apply an iterative process using asufficiently low number of features at each step, toselect the more useful ones depending on the weightXCRF assigns to them.Since the Crotal system obtained reasonable re-sults in this ?non ideal?
context, we are quite confi-dent in the fact that it can be significantly improved.The CoNLL 09 Shared Task has been a good op-portunity to validate our approach with a non trivialproblem.
Even if the performance is not excellent,several important points are satisfying: this experi-ence shows that the system is able to handle such atask, and that it is generic enough to deal with verydifferent languages.ReferencesAljoscha Burchardt, Katrin Erk, Anette Frank, AndreaKowalski, Sebastian Pado?, and Manfred Pinkal.
2006.The SALSA corpus: a German corpus resource forlexical semantics.
In Proceedings of the 5th Interna-tional Conference on Language Resources and Evalu-ation (LREC-2006), Genoa, Italy.Jenny Rose Finkel, Alex Kleeman, and Christopher D.Manning.
2008.
Efficient, feature-based, condi-tional random field parsing.
In Proceedings of ACL-08:HLT, pages 959?967, Columbus, Ohio.
Associa-tion for Computational Linguistics.Re?mi Gilleron, Florent Jousse, Isabelle Tellier, and MarcTommasi.
2006.
Conditional random fields for xmltrees.
In Proceeding of ECML workshop on Miningand Learning in Graphs.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan ?Ste?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.Jan Hajic?, Jarmila Panevova?, Eva Hajic?ova?, PetrSgall, Petr Pajas, Jan ?Ste?pa?nek, Jir???
Havelka, MarieMikulova?, and Zdene?k ?Zabokrtsky?.
2006.
PragueDependency Treebank 2.0.
Linguistic Data Con-sortium, Philadelphia, Pennsylvania, USA.
URL:http://ldc.upenn.edu.
Cat.
No.
LDC2006T01, ISBN 1-58563-370-4.Florent Jousse.
2007.
Transformations d?Arbres XMLavec des Mode`les Probabilistes pour l?Annotation.Ph.D.
thesis, Universite?
Charles de Gaulle - Lille 3,October.Daisuke Kawahara, Sadao Kurohashi, and Ko?iti Hasida.2002.
Construction of a Japanese relevance-taggedcorpus.
In Proceedings of the 3rd InternationalConference on Language Resources and Evaluation(LREC-2002), pages 2008?2013, Las Palmas, CanaryIslands.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic mod-els for segmenting and labeling sequence data.
InICML?01: Proceedings of the 18th International Conf.on Machine Learning, pages 282?289.Martha Palmer and Nianwen Xue.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic and se-mantic dependencies.
In Proceedings of the 12th Con-ference on Computational Natural Language Learning(CoNLL-2008).Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.
MITPress.Mariona Taule?, Maria Anto`nia Mart?
?, and Marta Re-casens.
2008.
AnCora: Multilevel Annotated Corporafor Catalan and Spanish.
In Proceedings of the 6thInternational Conference on Language Resources andEvaluation (LREC-2008), Marrakesh, Morroco.96
