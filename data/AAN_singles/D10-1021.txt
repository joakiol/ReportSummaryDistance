Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 207?217,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsImproving Gender Classification of Blog AuthorsArjun Mukherjee Bing LiuDepartment of Computer ScienceUniversity of Illinois at Chicago851 South Morgan StreetChicago, IL 60607, USAamukherj@cs.uic.eduDepartment of Computer ScienceUniversity of Illinois at Chicago851 South Morgan StreetChicago, IL 60607, USAliub@cs.uic.eduAbstractThe problem of automatically classifying thegender of a blog author has important appli-cations in many commercial domains.
Exist-ing systems mainly use features such aswords, word classes, and POS (part-of-speech) n-grams, for classification learning.In this paper, we propose two new techniquesto improve the current result.
The first tech-nique introduces a new class of featureswhich are variable length POS sequence pat-terns mined from the training data using a se-quence pattern mining algorithm.
The secondtechnique is a new feature selection methodwhich is based on an ensemble of several fea-ture selection criteria and approaches.
Empir-ical evaluation using a real-life blog data setshows that these two techniques improve theclassification accuracy of the current state-of-the-art methods significantly.1 IntroductionWeblogs, commonly known as blogs, refer to on-line personal diaries which generally contain in-formal writings.
With the rapid growth of blogs,their value as an important source of informationis increasing.
A large amount of research workhas been devoted to blogs in the natural languageprocessing (NLP) and other communities.
Thereare also many commercial companies that exploitinformation in blogs to provide value-added ser-vices, e.g., blog search, blog topic tracking, andsentiment analysis of people?s opinions on prod-ucts and services.
Gender classification of blogauthors is one such study, which also has manycommercial applications.
For example, it can helpthe user find what topics or products are mosttalked about by males and females, and whatproducts and services are liked or disliked by menand women.
Knowing this information is crucialfor market intelligence because the informationcan be exploited in targeted advertising and alsoproduct development.In the past few years, several authors have stu-died the problem of gender classification in thenatural language processing and linguistic com-munities.
However, most existing works deal withformal writings, e.g., essays of people, the Reutersnews corpus and the British National Corpus(BNC).
Blog posts differ from such text in manyways.
For instance, blog posts are typically shortand unstructured, and consist of mostly informalsentences, which can contain spurious informationand are full of grammar errors, abbreviations,slang words and phrases, and wrong spellings.Due to these reasons, gender classification of blogposts is a harder problem than gender classifica-tion of traditional formal text.Recent work has also attempted gender classi-fication of blog authors using features such ascontent words, dictionary based content analysisresults, POS (part-of-speech) tags and feature se-lection along with a supervised learning algorithm(Schler et al, 2006; Argamon et al, 2007; Yanand Yan, 2006).
This paper improves these exist-ing methods by proposing two novel techniques.The first technique adds a new class of patternbased features to learning, which are not used inany existing work.
The patterns are frequent se-quences of POS tags which can capture complexstylistic characteristics of male and female au-thors.
We note that these patterns are very differ-ent from the traditional n-grams because the207patterns are of variable lengths and need to satisfysome criteria in order for them to represent signif-icant regularities.
We will discuss them in detailin Section 3.5.The second technique is a new feature selec-tion algorithm which uses an ensemble of featureselection criteria and methods.
It is well knownthat each individual feature selection criterion andmethod can be biased and tends to favor certaintypes of features.
A combination of them shouldbe able to capture the most useful or discrimina-tive features.Our experimental results based on a real lifeblog data set collected from a large number ofblog hosting sites show that the two new tech-niques enable classification algorithms to signifi-cantly improve the accuracy of the current state-of-the-art techniques (Argamon et al, 2007;Schler et al, 2006; Yan and Yan, 2006).
We alsocompare with two publicly available systems,Gender Genie (BookBlog, 2007) and GenderGuesser (Krawetz, 2006).
Both systems imple-mented variations of the method given in (Arga-mon et al, 2003).
Here, the improvement of ourtechniques is even greater.2 Related WorkThere have been several recent papers on genderclassification of blogs (e.g., Schler et al, 2006,Argamon et al, 2007; Yan and Yan, 2006; Now-son et al, 2005).
These systems use func-tion/content words, POS tag features, word classes(Schler et al, 2006), content word classes (Arga-mon et al, 2007), results of dictionary based con-tent analysis, POS unigram (Yan and Yan, 2006),and personality types (Nowson et al, 2005) tocapture stylistic behavior of authors?
writings forclassifying gender.
(Koppel et al 2002) also usedPOS n-grams together with content words on theBritish National Corpus (BNC).
(Houvardas andStamatatos, 2006) even applied character (ratherthan word or tag) n-grams to capture stylistic fea-tures for authorship classification of news articlesin Reuters.However, these works use only one or a subsetof the classes of features.
None of them uses allfeatures for classification learning.
Given thecomplexity of blog posts, it makes sense to applyall classes of features jointly in order to classifygenders.
Moreover, having many feature classes isvery useful as they provide features with variedgranularities and diversities.
However, this alsoresults in a huge number of features and many ofthem are redundant and may obscure classifica-tion.
Feature selection is thus needed.
Followingthe idea, this paper proposes a new ensemble fea-ture selection method which is capable of extract-ing good features from different feature classesusing multiple criteria.We also note some less relevant literature.
Forexample, (Tannen, 1990) deals with gender differ-ences in ?conversational style?
and in ?formalwritten essays?, and (Gefen and Straub, 1997)reports differences in perception of males and fe-males in the use of emails.Our new POS pattern features are related toPOS n-grams used in (Koppel et al, 2002; Arga-mon et al, 2007), which considered POS 3-grams,2-grams and unigrams as features.
As shown in(Baayen et.
al.
1996), POS n-grams are very ef-fective in capturing the fine-grained stylistic andheavier syntactic information.
In this work, we gofurther by finding POS sequence patterns.
As dis-cussed in the introduction, our patterns are entire-ly different from POS n-grams.
First of all, theyare of variable lengths depending on whateverlengths can catch the regularities.
They also needto satisfy some constraints to ensure that they tru-ly represent some significant regularity of male orfemale writings.
Furthermore, our POS sequencepatterns can take care of n-grams and capture ad-ditional sequence regularities.
These automatical-ly mined pattern features are thus morediscriminating for classification.3 Feature Engineering and MiningThere are different classes of features that havebeen experimented for gender classification, e.g.,F-measure, stylistic features, gender preferentialfeatures, factor analysis and word classes (Now-son et al, 2005; Schler et al, 2006; Corney et al,2002; Argamon et al, 2007).
We use all these ex-isting features and also propose a new class offeatures that are POS sequence patterns, whichreplace existing POS n-grams.
Also, as mentionedbefore, using all feature classes gives us featureswith varied granularities.
Upon extracting allthese classes of features, a new ensemble featureselection (EFS) algorithm is proposed to select asubset of good or discriminative features.208Below, we first introduce the existing features,and then present the proposed class of new patternbased features and how to discover them.3.1 F-measureThe F-measure feature was originally proposed in(Heylighen and Dewaele, 2002) and has been usedin (Nowson et al, 2005) with good results.
Notethat F-measure here is not the F-score or F-measure used in text classification or informationretrieval for measuring the classification or re-trieval effectiveness (or accuracy).F-measure explores the notion of implicitnessof text and is a unitary measure of text?s relativecontextuality (implicitness), as opposed to itsformality (explicitness).
Contextuality and formal-ity can be captured by certain parts of speech.
Alower score of F-measure indicates contextuality,marked by greater relative use of pronouns, verbs,adverbs, and interjections; a higher score of F-measure indicates formality, represented by great-er use of nouns, adjectives, prepositions, and ar-ticles.
F-measure is defined based on thefrequency of the POS usage in a text (freq.x belowmeans the frequency of the part-of-speech x):F = 0.5 * [(freq.noun + freq.adj + freq.prep +freq.art) ?
(freq.pron + freq.verb +freq.adv + freq.int) + 100](Heylighen and Dewaele, 2002) applied the F-measure to a corpus with known author gendersand found a distinct difference between the sexes.Females scored lower preferring a more contex-tual style while males scored higher preferring amore formal style.
F-measure values for male andfemale writings reported in (Nowson et al, 2005)also demonstrated a similar trend.
In our work, wealso use F-measure as one of the features.3.2 Stylistic FeaturesThese are features which capture people?s writingstyles.
The style of writing is typically capturedby three types of features: part of speech, words,and in the blog context, words such as lol, hmm,and smiley that appear with high frequency.
In thiswork, we use words and blog words as stylisticfeatures.
Part of speech features are mined usingour POS sequence pattern mining algorithm.
POSn-grams can also be used as features.
However,since we mine all POS sequence patterns and usethem as features, most discriminative POS n-grams are already covered.
In Section 5, we willalso show that POS n-grams do not perform aswell as our POS sequence patterns.3.3 Gender Preferential FeaturesGender preferential features consist of a set ofsignals that has been used in an email gender clas-sification task (Corney et al, 2002).
These fea-tures come from various studies that have beenundertaken on the issue of gender and languageuse (Schiffman, 2002).
It was suggested by thesestudies and also various other works that women?slanguage makes more frequent use of emotionallyintensive adverbs and adjectives like ?so?, ?terri-bly?, ?awfully?, ?dreadfully?
and women?s lan-guage is more punctuated.
On the other hand,men?s conversational patterns express ?indepen-dence?
(Corney et al, 2002).
In brief, the lan-guage expressed by males is more proactive atsolving problems while the language used by fe-males is more reactive to the contribution of oth-ers - agreeing, understanding and supporting.
Weused the gender preferential features listed in Ta-ble 1, which indicate adjectives and adverbs basedon the presence of suffixes and apologies as usedin (Corney et al, 2002).
The feature value as-signment will be discussed in Section 5.f1 words ending with ablef2 words ending with alf3 words ending with fulf4 words ending with iblef5 words ending with icf6 words ending with ivef7 words ending with lessf8 words ending with lyf9 words ending with ousf10 sorry wordsTable 1: Gender preferential features3.4 Factor Analysis and Word ClassesFactor or word factor analysis refers to the processof finding groups of similar words that tend tooccur in similar documents.
This process is re-ferred to as meaning extraction in (Chung andPennebaker, 2007).
Word lists for twenty factors,along with suggested labels/headings (for refer-ence) were used as features in (Argamon et al,2007).
Here we list some of those features (word209classes) in Table 2.
For the detailed list of suchword classes, the reader is referred to (Argamon etal., 2007).
We also used these word classes as fea-tures in our work.
In addition, we added threemore new word classes implying positive, nega-tive and emotional connotations and used them asfeatures in our experiments.
These are listed inTable 3.Factor WordsConversa-tionknow, people, think, person, tell, feel, friends, talk,new, talking, mean, ask, understand, feelings, care,thinking, friend, relationship, realize, question, an-swer, sayingHomewoke, home, sleep, today, eat, tired, wake, watch,watched, dinner, ate, bed, day, house, tv, early, bor-ing, yesterday, watching, sitFamilyyears, family, mother, children, father, kids, parents,old, year, child, son, married, sister, dad, brother,moved, age, young, months, three, wife, living, col-lege, four, high, five, died, six, baby, boy, spend,ChristmasFood /Clothesfood, eating, weight, lunch, water, hair, life, white,wearing, color, ice, red, fat, body, black, clothes,hot, drink, wear, blue, minutes, shirt, green, coffee,total, store, shoppingRomance forget, forever, remember, gone, true, face, spent, times, love, cry, hurt, wish, lovedTable 2: Words in factorsPositiveabsolutely, abundance, ace, active, admirable, adore,agree, amazing, appealing, attraction, bargain, beam-ing, beautiful, best, better, boost, breakthrough, breeze,brilliant, brimming, charming, clean, clear, colorful,compliment, confidence, cool, courteous, cuddly, daz-zling, delicious, delightful, dynamic, easy, ecstatic,efficient, enhance, enjoy, enormous, excellent, exotic,expert, exquisite, flair, free, generous, genius, great,graceful, heavenly, ideal, immaculate, impressive, in-credible, inspire, luxurious, outstanding, royal, speed,splendid, spectacular, superb, sweet, sure, supreme,terrific, treat, treasure, ultra, unbeatable, ultimate,unique, wow, zestNegativewrong, stupid, bad, evil, dumb, foolish, grotesque,harm, fear, horrible, idiot, lame, mean, poor, heinous,hideous, deficient, petty, awful, hopeless, fool, risk,immoral, risky, spoil, spoiled, malign, vicious, wicked,fright, ugly, atrocious, moron, hate, spiteful, meager,malicious, lackingEmotionaggressive, alienated, angry, annoyed, anxious, careful,cautious, confused, curious, depressed, determined,disappointed, discouraged, disgusted, ecstatic, embar-rassed, enthusiastic, envious,  excited,  exhausted,frightened, frustrated, guilty, happy,  helpless, hopeful,hostile, humiliated, hurt, hysterical,  innocent, interest-ed, jealous, lonely, mischievous,  miserable, optimistic,paranoid, peaceful, proud,  puzzled, regretful, relieved,sad, satisfied, shocked,  shy, sorry, surprised, suspi-cious, thoughtful, undecided,  withdrawnTable 3: Words implying positive, negative and emo-tional connotations3.5 Proposed POS Sequence Pattern Fea-turesWe now present the proposed POS sequence pat-tern features and the mining algorithm.
This re-sults in a new feature class.
A POS sequencepattern is a sequence of consecutive POS tags thatsatisfy some constraints (discussed below).
Weused (Tsuruoka and Tsujii, 2005) as our POS tag-ger.As shown in (Baayen et.
al., 1996), POS n-grams are good at capturing the heavy stylisticand syntactic information.
Instead of using allsuch n-grams, we want to discover all those pat-terns that represent true regularities, and we alsowant to have flexible lengths (not fixed lengths asin n-grams).
POS sequence patterns serve thesepurposes.
Its mining algorithm mines all such pat-terns that satisfy the user-specified minimum sup-port (minsup) and minimum adherence(minadherence) thresholds or constraints.
Thesethresholds ensure that the mined patterns representsignificant regularities.The main idea of the algorithm is to perform alevel-wise search for such patterns, which arePOS sequences with minsup and minadherence.The support of a pattern is simply the proportionof documents that contain the pattern.
If a patternappears too few times, it is probably spurious.
Asequence is called a frequent sequence if it satis-fies minsup.
The adherence of a pattern is meas-ured using the symmetrical conditionalprobability (SCP) given in (Silva et al, 1999).The SCP of a sequence with two elements |xy| isthe product of the conditional probability of eachgiven the other,)()(),()|()|(),(2yPxPyxPxyPyxPyxSCP ==Given a consecutive sequence of POS tags|x1?xn|, called a POS sequence of length n, a dis-persion point defines two subparts of the se-quence.
A sequence of length n contains n-1possible dispersion points.
The SCP of the se-quence |x1?xn| given the dispersion point (denotedby *) |x1?xn-1*xn| is:)()...()...()),...((112111nnnnn xPxxPxxPxxxSCP??
=The SCP measure can be extended so that allpossible dispersion points are accounted for.210Hence the fairSCP of the sequence |x1?xn| is giv-en by:??=+?=1111211)...()...(11)...()...
(niniinnxxPxxPnxxPxxfairSCPfairSCP measures the adherence strength of POStags in a sequence.
The higher the fairSCP value,the more dominant is the sequence.
Our POS se-quence pattern mining algorithm is given below.Input: Corpus D = {d | d is a document containing asequence of POS tags}, Tagset T = {t | t is a POStag}, and the user specified minimum support (min-sup) and minimum adherence (minadherence).Output: All POS sequence patterns (stored in SP)mined from D that satisfy minsup and minadhe-rence.Algorithm mine-POS-pats(D, T, minsup, minadhe-rence)1.
C1 ?
count each t (?
T) in D;2.
F1 ?
{f | f ?
C1 , f .count / n ?
minsup};    // n = |D|3.
SP1 ?
F1;4.  for (k = 2; k ?
MAX-length; k++)5.
Ck = candidate-gen(Fk-1);6. for each document d ?
D7.
for each candidate POS sequence c ?
Ck8.
if (c is contained in d)9. c.count++;10. endfor11.
endfor12.
Fk ?
{c ?
Ck | c.count / n ?
minsup};13 SPk ?
{f ?
Fk | fairSCP(f) ?
minadherence}14.  endfor15.
return SP ?
UkkSP ;Function candidate-gen(Fk-1)1.
Ck ?
?;2.
for each POS n-gram c ?
Fk-13.
for each t ?
T4.
c??
addsuffix(c, t);  // adds tag t to c as suffix5.
add c?
to Ck ;6.      endfor7.
endforWe now briefly explain the mine-POS-pats algo-rithm.
The algorithm is based on level-wisesearch.
It generates all POS patterns by makingmultiple passes over data.
In the first pass, itcounts the support of individual POS tags and de-termines which of them have minsup (line 2).Multiple occurrences of a tag in a document arecounted only once.
Those in F1 are called length 1frequent sequences.
All length 1 sequence patternsare stored in SP1.
Since adherence is not definedfor a single element, we have SP1 = F1 (line 3).
Ineach subsequent pass k until MAX-length (whichis the maximum length limit of the mined pat-terns), there are three steps:1.
Using Fk-1 (frequent sequences found in the (k-1) pass) as a set of seeds, the algorithm appliescandidate-gen() to generate all possibly fre-quent POS k-sequences (sequences of length k)(line 5).
Those infrequent sequences (which arenot in Fk-1) are discarded as adding more POStags will not make them frequent based on thedownward closure property in (Agrawal andSrikant, 1994).2.
D is then scanned to compute the actual sup-port count of each candidate in Ck (lines 6-11).3.
At the end of each scan, it determines whichcandidate sequences have minsup and minad-herence (lines 12 - 13).
We compute Fk and SPkseparately because adherence does not have thedownward closure property as the support.Finally, the algorithm returns the set of all se-quence patterns (line 15) that meet the minsup andminadherence thresholds.The candidate-gen() function generates all pos-sibly frequent k-sequences by adding each POStag t to c as suffix.
c is a k-1-sequence in Fk-1.In our experiments, we used MAX-length = 7,minsup = 30%, and minadherence = 20% to mineall POS sequence patterns.
All the mined patternsare used as features.Finally, it is worthwhile to note that mine-POS-pat is very similar to the well-known GSPalgorithm (Srikant and Agrawal, 1996).
Likewise,it has linear scale up with data size.
If needed, onecan use MapReduce (Dean and Ghemawat, 2004)with suitable modifications in mine-POS-pats tospeed things up by distributing to multiple ma-chines for large corpora.
Moreover, mining is apart of preprocessing of the algorithm and itscomplexity does not affect the final prediction, asit will be later shown that for model building andprediction, standard machine learning methods areused.4 Ensemble Feature SelectionSince all classes of features discussed in Section 3are useful, we want to employ all of them.
Thisresults in a huge number of features.
Many of211them are redundant and even harmful.
Featureselection thus becomes important.
There are twocommon approaches to feature selection: the filterand the wrapper approaches (Blum and Langley,1997; Kohavi and John, 1997).
In the filter ap-proach, features are first ranked based on a featureselection criterion such as information gain, chi-square (?2) test, and mutual information.
A set oftop ranked features are selected.
On the contrary,the wrapper model chooses features and adds tothe current feature pool based on whether the newfeatures improve the classification accuracy.Both these approaches have drawbacks.
Whilethe wrapper approach becomes very time consum-ing and impractical when the number of featuresis large as each feature is tested by building a newclassifier.
The filter approach often uses only onefeature selection criterion (e.g., information gain,chi-square, or mutual information).
Due to thebias of each criterion, using only a single one mayresult in missing out some good features whichcan rank high based on another criterion.
In thiswork, we developed a novel feature selection me-thod that uses multiple criteria, and combines boththe wrapper and the filter approaches.
Our methodis called ensemble feature selection (EFS).4.1 EFS AlgorithmEFS takes the best of both worlds.
It first uses anumber of feature selection criteria to rank thefeatures following the filter model.
Upon ranking,the algorithm generates some candidate featuresubsets which are used to find the final feature setbased on classification accuracy using the wrappermodel.
Since our framework generates much few-er candidate feature subsets than the total numberof features, using wrapper model with candidatefeature sets is scalable.
Also, since the algorithmgenerates candidate feature sets using multiplecriteria and all feature classes jointly, it is able tocapture most of those features which are discrimi-nating.
We now detail our EFS algorithm.The algorithm takes as input, a set of n featuresF = {f1, ?, fn}, a set of t feature selection criteria?
= {?1, ?, ?t}, a set of t thresholds ?
= {?1, ?,?t} corresponding to the criteria in ?, and a win-dow w. ?i is the base number of features to be se-lected for criterion ?i.
w is used to vary ?i (thus thenumber of features) to be used by the wrapperapproach.Algorithm: EFS (F, ?, ?, w)1. for each ?i ?
?2.
Rank all features in F based on criterion ?i andlet ?i denotes the ranked features3.
endfor4.
for i = 1 to t5.
Ci ?
?6.
for ?
= ?i ?
w to ?
= ?i + w7.
select first ?
features ?i from ?i and add ?i to Ciin order8.
endfor9.
endfor10.
// Ci = {?1,  ?, ?2w + 1}, where ?i is a set of fea-tures11.
OptCandFeatures ?
?;12.
Repeat steps 13 ?
1813. ?
?
?14.
for i = 1 to t15.
select and remove the first feature set ?i ?
Cifrom Ci in order16.
?
?
?
?
?i17.
endfor18.
add ?
to OptCandFeatures19.
// ?
is a set of features comprising of features in// feature sets ?i ?
Ci in the same position ?
i20.
until Ci = ?
?
i21.
for each ?
?
OptCandFeatures22.
?.score ?
accuracy of 10-fold CV on trainingdata on a chosen classifier (learning algo-rithm)23. endfor24.
returnscore.maxarg?
{ ?
| ?
?
OptCandFeatures}We now explain our EFS algorithm.
Using a set ofdifferent feature selection measures, ?, we rankall features in our feature pool, F, using the set ofcriteria (lines 1?3).
This is similar to the filter ap-proach.
In lines 4?9, we generate feature sets Ci, 1?
i ?
t for each of the t criteria.
Each set Ci con-tains feature subsets, and each subset ?i is the set oftop ?
features in ?i ranked based on criterion ?i inlines 1?2.
?
varies from ?i ?
w to ?i + w where ?i isthe threshold for criterion ?i and w the windowsize.
We vary ?
and generate 2w + 1 feature setsand add all such feature sets ?i to Ci (in lines 6?8)in order.
We do so because it is difficult to knowthe optimal threshold ?i for each criterion ?i.
Itshould be noted that ?adding in order?
ensures theordering of feature sets ?i  as shown in line 10,which will be later used to ?select and remove inorder?
in line 15.
In lines 11?20 we generate can-didate feature sets using Ci and add each such212candidate feature set ?
to OptCandFeatures.
Eachcandidate feature set ?
is a collection of topranked features based on multiple criteria.
It isgenerated by unioning the features in the first fea-ture subset ?i, which is then removed from Ci foreach criterion ?i (lines 14-17).
Each candidate fea-ture set is added to OptCandFeatures in line 18.Since each Ci has 2w+1 feature subsets ?i, thereare a total of 2w+1 candidate feature sets ?
inOptCandFeatures.
Lines 21?23 assign an accuracyto each candidate feature set ?
?
OptCandFeaturesby running 10-fold cross validation on the trainingdata using a chosen classifier with the features in?.
Finally, the optimal feature set ?
?
OptCand-Features is returned in line 24.An interesting question arising in the EFS al-gorithm is: How does one select the threshold ?ifor each criterion ?i and the window size w?
Intui-tively, suppose that for criterion ?i, the optimalsubset of features is Sopt_i based on some optimalthreshold ?i.
Then the final feature set is a collec-tion of all features f ?
Sopt_i ?
i.
However, findingsuch optimal feature set Sopt_i or optimal threshold?i is a difficult problem.
To counter this, we usethe window w to select various feature subsetsclose to the top ?i features in ?i.
Thus, the thre-shold values ?i and window size w should be ap-proximated by experiments.
In our experiments,we used ?i = top 1/20th of the features ranked in ?ifor ?
i and window size w = |F|/100, and got goodresults.
Fortunately, as we will see in Section 6.2,these parameters are not sensitive at all, and anyreasonably large size feature set seems to workequally well.Finally, we are aware that there are some exist-ing ensemble feature selection methods in the ma-chine learning literature (Gargant?
et al, 2007; Tuvet al, 2009).
However, they are very differentfrom our approach.
They mainly use ensembleclassification methods to help choose good fea-tures rather than combining different feature se-lection criteria and integrating different featureselection approaches as in our method.4.2 Feature Selection CriteriaThe set of feature selection criteria ?
= {?1?
?t}used in our work are those commonly used indi-vidual selection criteria in the filter approach.Let C ={c1, c2, ?, cm} denotes the set ofclasses, and F = {f1, f2, ?, fn} the set of features.We list the criteria in ?
used in our work below.Information Gain (IG): This is perhaps the mostcommonly used criterion, which is based on en-tropy.
The scoring function for information gainof a feature f is given by:?
?
?==+?=ffmiiiimii fcPfcPfPcPcPfIG, 11)|(log)|()()(log)()(Mutual Information (MI): This metric is com-monly used in statistical language modeling.
Themutual information MI(f, c) between a class c anda feature f is defined as:?
?=ff cc cPfPcfPcfPcfMI, , )()(),(log),(),(The scoring function generally used as the crite-rion is the max among all classes.
MI(f) = maxi{MI (f, ci)} (which we use).
The weighted averageover all classes can also be applied as the scoringfunction.
?2 Statistic: The ?2 statistic measures the lack ofindependence between a feature f and class c, andcan be compared to the ?2 distribution with onedegree of freedom.
We use a 2x2 contingency ta-ble of a feature f and a class c to introduce ?2 test.c cf W Xf  Y ZTable 4: Two-way contingency table of f and cIn the table, W denotes the number of documentsin the corpus in which feature f and class c co-occur, X  the number of documents in which f oc-curs without c, Y the number of documents inwhich c occurs without f, and Z the number ofdocuments in which neither c nor f occurs.
Thus,N = W + X + Y + Z is the total number of docu-ments in the corpus.
?2 test is defined as:))()()(()(),(22ZYXWZXYWYXWZNcf ++++?=?The scoring function using the ?2 statistic is eitherthe weighted average or max over all classes.
Inour experiments, we use the weighted average:?2(f) = ?=mi ii cfcP1 2 ),()( ?Cross Entropy (CE): This metric is similar tomutual information (Mladenic and Grobelnik,2131998):?==miii fPfcPfcPfPfCE1 )()|(log)|()()(Weight of Evidence for Text (WET): This crite-rion is based on the average absolute weight ofevidence (Mladenic and Grobelnik, 1998):|))|(1)(())(1)(|(log|)()()(1 fcPcPcPfcPfPcPfWETiiiimii ?
?=?=5 Feature Value AssignmentsAfter selecting features belonging to differentclasses, values are assigned differently to differentclasses of features.
There are three common waysof feature value assignments: Boolean, TF (TermFrequency) and TF-IDF (product of term and in-verted document frequency).
For details of featurevalue assignments, interested readers are referredto (Joachims, 1997).
While the Boolean schemeassigns a 1 to the feature value if the feature ispresent in the document and a 0 otherwise, the TFscheme assigns the relative frequency of the num-ber of times that the feature occurs in the docu-ment.
We did not use TF-IDF as it did not yieldgood results in our preliminary experiments.The feature value assignment to differentclasses of features is done as follows: The valueof F-measure was assigned based on its actualvalue.
Stylistic features such words, and blogwords were assigned values 1 or 0 in the Booleanscheme and the relative frequency in the TFscheme (we experimented with both schemes).Feature values for gender preferential featureswere also assigned in a similar way.
Factor andword class features were assigned values accord-ing to the Boolean or TF scheme if any of thewords belonging to the feature class exists (factoror word class appeared in that document).
EachPOS sequence pattern feature was assigned a val-ue according to the Boolean (or TF) scheme basedon the appearances of the pattern in the POStagged document.6 Experimental ResultsThis section evaluates the proposed techniquesand sees how they affect the classification accura-cy.
We also compare with the existing state-of-the-art algorithms and systems.
For algorithms,we compared with three representatives in (Arga-mon et al, 2007), (Schler et al, 2006) and (Yanand Yan, 2006).
Since they do not have publiclyavailable systems, we implemented them.
Each ofthem just uses a subset of the features used in oursystem.
Recall our system includes all their fea-tures and our own POS pattern based features.
Forsystems, we compared with two public domainsystems, Gender Genie (BookBlog, 2007) andGender Guesser (Krawetz, 2006), which imple-mented variations of the algorithm in (Argamonet.
al, 2003).We used SVM classification, SVM regression,and Na?ve Bayes (NB) as learning algorithms.Although SVM regression is not designed forclassification, it can be applied based on the out-put of positive or negative values.
It actuallyworked better than SVM classification for ourdata.
For SVM classification and regression, weused SVMLight (Joachims, 1999), and for NB weused (Borgelt, 2003).
In all our experiments, weused accuracy as the evaluation measure as thetwo classes (male and female) are roughly ba-lanced (see the data description below), and bothclasses are equally important.6.1 Blog Data SetTo keep the problem of gender classification ofinformal text as general as possible, we collectedblog posts from many blog hosting sites and blogsearch engines, e.g., blogger.com, technorati.com,etc.
The data set consists of 3100 blogs.
Each blogis labeled with the gender of its author.
The gend-er of the author was determined by visiting theprofile of the author.
Profile pictures or avatarsassociated with the profile were also helpful inconfirming the gender especially when the genderinformation was not available explicitly.
To en-sure quality of the labels, one group of studentscollected the blogs and did the initial labeling, andthe other group double-checked the labels by visit-ing the actual blog pages.
Out of 3100 posts, 1588(51.2%) were written by men and 1512 (48.8%)were written by women.
The average post lengthis 250 words for men and 330 words for women.6.2 ResultsWe used all features from different feature classes(Section 3) along with our POS patterns as our214pool of features.
We used ?
and w values stated inSection 4.1 and criteria mentioned in Section 4.2for our EFS algorithm.
EFS was compared withthree commonly used feature selection methodson SVM classification (denoted by SVM), SVMregression (denoted by SVM_R) and the NB clas-sifier.
The results are shown in Table 5.
All resultswere obtained through 10-fold cross validation.Also, the total number of features selected byIG, MI, ?2, and EFS were roughly the same.
Thus,the improvement in accuracy brought forth byEFS was chiefly due to the combination of fea-tures selected (based on multi-criteria).To measure the accuracy improvement of usingour POS patterns over common POS n-grams, wealso compared our results with those from POS n-grams (Koppel et al, 2002).
The comparison re-sults are given in Table 6.
Table 6 also includesresults to show the overall improvement in accu-racy with our two new techniques.
We tested oursystem without any feature selection and withoutusing the POS sequence patterns as features.The comparison results with existing algo-rithms and public domain systems using our real-life blog data set are tabulated in Table 7.Also, to see whether feature selection helps andhow many features are optimal, we varied ?
and wof the EFS algorithm and plotted the accuracy vs.no.
of features.
These results are shown in Figure1.FeatureSelectionValueAssignment NB SVM SVM_RIG Boolean 71.32 76.61 78.32IG TF 66.01 72.84 74.13MI  Boolean 72.01 78.62 79.48MI TF 70.86 73.14 74.58?2 Boolean 72.90 80.71 81.52?2 TF 71.84 73.57 75.24EFS Boolean 73.57 86.24 88.56EFS TF 72.82 82.05 83.53Table 5: Accuracies of SVM, SVM_R and NB withdifferent feature selection methodsSettings NB SVM SVM_RAll features 63.01 68.84 70.03All features, no POS patterns 60.73 65.17 66.17POS 1,2,3-grams + EFS 71.24 82.71 83.86POS Patterns + EFS 73.57 86.24 88.56Table 6: Accuracies of POS n-grams and POS patternswith or without EFS (Boolean value assignment)System Accuracy (%)Gender Genie 61.69Gender Guesser 63.78(Argamon et al, 2007) 77.86(Schler et al, 2006) 79.63(Yan and Yan, 2006) 68.75Our method 88.56Table 7: Accuracy comparison with other systems506070809010025 128210350180764682397426029No.
of featuresAccuracySVM Classification with EFSSVM Regression with EFSNa?ve Bayes with EFSFigure 1: Accuracy vs. no.
of features using EFS6.3 Observations and DiscussionsBased on the results given in the previous section,we make the following observations:?
SVM regression (SVM_R) performs the best(Table 5).
SVM classification (SVM) alsogives good accuracies.
NB did not do so well.?
Table 5 also shows that our EFS feature selec-tion method brings about 6-10% improvementin accuracy over the other feature selection me-thods based on SVM classification and SVMregression.
The reason has been explained inthe introduction section.
Paired t-tests showedthat all the improvements are statistically sig-nificant at the confidence level of 95%.
ForNB, the benefit is less (3%).?
Keeping all other parameters constant, Table 5also shows that Boolean feature values yieldedbetter results than the TF scheme across allclassifiers and feature selection methods.?
Row 1 of Table 6 tells us that feature selectionis very useful.
Without feature selection (Allfeatures), SVM regression only achieves 70%accuracy, which is way inferior to the 88.56%accuracy obtained using EFS feature selection.Row 2 shows that without EFS and withoutPOS sequence patterns, the results are evenworse.215?
Keeping all other parameters intact, Table 6also demonstrated the effectiveness of our POSpattern features over POS n-grams.
We havediscussed the reason in Section 3.2 and 3.5.?
From Tables 5 and 6, we can infer that theoverall accuracy improvement using EFS andall feature classes described in Section 3 isabout 15% for SVM classification and regres-sion and 10% for NB.
Also, using POS se-quence patterns with EFS brings about a 5%improvement over POS n-grams (Table 6).
Theimprovement is more pronounced for SVMbased methods than NB.?
Table 7 summarizes the accuracy improvementbrought by our proposed techniques over theexisting state-of-art systems.
Our techniqueshave resulted in substantial (around 9%) accu-racy improvement over the best of the existingsystems.
Note that (Argamon et al, 2007) usedLogistic Regression with word classes andPOS unigrams as features.
(Schler et al, 2006)used Winnow classifier with function words,content word classes, and POS features.
(Yanand Yan, 2006) used Naive Bayes with contentwords and blog-words as features.
For all thesesystems, we used their features and ran theiroriginal classifiers and also the three classifiersin this paper and report the best results.
Forexample, for (Argamon et al, 2007), we ranLogistic Regression and our three methods.SVM based methods always gave slightly bet-ter results.
We could not run Winnow due tosome technical issues.
SVM and SVM_R gavecomparable results to those given in their orig-inal papers.
These results again show that ourtechniques are useful.
All the gains are statisti-cally significant at the confidence level of95%.?
From Figure 1, we see that when the number offeatures selected is small (<100) the classifica-tion accuracy is lower than that obtained by us-ing all features (no feature selection).However, the accuracy increases rapidly as thenumber of selected features increases.
Afterobtaining the best case accuracy, it roughlymaintains the accuracy over a long range.
Theaccuracies then gradually decrease with the in-crease in the number of features.
This trend isconsistent with the prior findings in (Mladenic,1998; Rogati and Yang, 2002; Forman 2003;Riloff et al, 2006; Houvardas and Stamatatos,2006).It is important to note here that over a longrange of 2000 to 20000 features, the accuracyis high and stable.
This means that the thre-sholds of EFS are easy to set.
As long as theyare in the range, the accuracy will be good.Finally, we would like to mention that (Herringand Paolillo, 06) has used genre relationships withgender classification.
Their finding that subgenre?diary?
contains more ?female?
and subgenre ?fil-ter?
having more ?male?
stylistic features inde-pendent of the author gender, may obscure genderclassification as there are many factors to be con-sidered.
Herring and Paolillo referred only wordsas features which are not as fine grained as ourPOS sequence patterns.
We are also aware of oth-er factors influencing gender classification likegenre, age and ethnicity.
However, much of suchinformation is hard to obtain reliably in blogs.They definitely warren some future studies.
Also,EFS being a useful method for feature selection inmachine learning, it would be useful to performfurther experiments to investigate how well it per-forms on a variety of classification datasets.
Thisagain will be an interesting future work.7  ConclusionsThis paper studied the problem of gender classifi-cation.
Although there have been several existingpapers studying the problem, the current accuracyis still far from ideal.
In this work, we followedthe supervised approach and proposed two noveltechniques to improve the current state-of-the-art.In particular, we proposed a new class of featureswhich are POS sequence patterns that are able tocapture complex stylistic regularities of male andfemale authors.
Since there are a large numberfeatures that have been considered, it is importantto find a subset of features that have positive ef-fects on the classification task.
Here, we proposedan ensemble feature selection method which takesadvantage of many different types of feature se-lection criteria in feature selection.
Experimentalresults based on a real-life blog data set demon-strated the effectiveness of the proposed tech-niques.
They help achieve significantly higheraccuracy than the current state-of-the-art tech-niques and systems.216ReferencesAgrawal, R. and Srikant, R. 1994.
Fast Algorithms forMining Association Rules.
VLDB.
pp.
487-499.Argamon, S., Koppel, M., J Fine, AR Shimoni.
2003.Gender, genre, and writing style in formal writtentexts.
Text-Interdisciplinary Journal, 2003.Argamon, S., Koppel, M., Pennebaker, J. W., Schler, J.2007.
Mining the Blogosphere: Age, Gender andthe varieties of self-expression, First Monday, 2007- firstmonday.orgBaayen, H., H van Halteren, F Tweedie.
1996.
Outsidethe cave of shadows: Using syntactic annotation toenhance authorship attribution, Literary and Lin-guistic Computing, 11, 1996.Blum, A. and Langley, P. 1997.
Selection of relevantfeatures and examples in machine learning.
Artifi-cial Intelligence, 97(1-2):245-271.BookBlog, Gender Genie, Copyright 2003-2007,http://www.bookblog.net/gender/genie.htmlBorgelt, C. 2003.
Bayes Classifier Induction.http://www.borgelt.net/doc/bayes/bayes.htmlChung, C. K. and Pennebaker, J. W. 2007.
Revealingpeople?s thinking in natural language: Using an au-tomated meaning extraction method in open?endedself?descriptions, J. of Research in Personality.Corney, M., Vel, O., Anderson, A., Mohay, G. 2002.Gender Preferential Text Mining of E-mail Dis-course.
18th annual Computer Security Applica-tions Conference (ACSAC), 2002.J.
Dean and S. Ghemawat.
2004.
Mapreduce: Simpli-fied data processing on large clusters, OperatingSystems Design and Implementation, 2004.Forman, G., 2003.
An extensive empirical study of fea-ture selection metrics for text classification.
JMLR,3:1289 - 1306 , 2003.Gargant?, R. A., Marchiori, T. E., and Kowalczyk, S.R.
W., 2007.
A Genetic Algorithm to Ensemble Fea-ture Selection.
Masters Thesis.
Vrije Universiteit,Amsterdam.Gefen, D., D. W. Straub.
1997.
Gender differences inthe perception and use of e-mail: An extension tothe technology acceptance model.
MIS Quart.
21(4)389?400.Herring, S. C., & Paolillo, J. C. 2006.
Gender and ge-nre variation in weblogs, Journal of Sociolinguis-tics, 10 (4), 439-459.Heylighen, F., and Dewaele, J.
2002.
Variation in thecontextuality of language: an empirical measure.Foundations of Science, 7, 293?340.Houvardas, J. and Stamatatos, E. 2006.
N-gram Fea-ture Selection for Authorship Identification, Proc.
ofthe 12th Int.
Conf.
on Artificial Intelligence: Me-thodology, Systems, Applications, pp.
77-86.Joachims, T. 1999.
Making large-Scale SVM LearningPractical.
Advances in Kernel Methods - SupportVector Learning, B. Sch?lkopf and C. Burges andA.
Smola (ed.
), MIT-Press, 1999.Joachims, T. 1997.
Text categorization with supportvector machines, Technical report, LS VIII Number23, University of Dortmund, 1997Kohavi, R. and John, G. 1997.
Wrappers for featuresubset selection.
Artificial Intelligence, 97(1-2):273-324.Koppel, M., Argamon, S., Shimoni, A. R.. 2002.
Auto-matically Categorizing Written Text by AuthorGender.
Literary and Linguistic Computing.Krawetz, N. 2006.
Gender Guesser.
Hacker FactorSolutions.
http://www.hackerfactor.com/ Gender-Guesser.htmlMladenic, D. 1998.
Feature subset selection in textlearning.
In Proc.
of ECML-98, pp.
95?100.Mladenic, D. and Grobelnik, D.1998.
Feature selectionfor classification based on text hierarchy.
Proceed-ings of the Workshop on Learning from Text andthe Web, 1998Nowson, S., Oberlander J., Gill, A. J., 2005.
Gender,Genres, and Individual Differences.
In Proceedingsof the 27th annual meeting of the Cognitive ScienceSociety (p. 1666?1671).
Stresa, Italy.Riloff, E., Patwardhan, S., Wiebe, J.. 2006.
FeatureSubsumption for opinion Analysis.
EMNLP,Rogati, M. and Yang, Y.2002.
High performing andscalable feature selection for text classification.
InCIKM, pp.
659-661, 2002.Schiffman, H. 2002.
Bibliography of Gender and Lan-guage.
http://ccat.sas.upenn.edu/~haroldfs/ pop-cult/bibliogs/gender/genbib.htmSchler, J., Koppel, M., Argamon, S, and Pennebaker J.2006.
Effects of age and gender on blogging, InProc.
of the AAAI Spring Symposium Computa-tional Approaches to Analyzing Weblogs.Silva, J., Dias, F., Guillore, S., Lopes, G. 1999.
UsingLocalMaxs Algortihm for the Extraction of Conti-guous and Noncontiguous Multiword Lexical Units.Springer Lecture Notes in AI 1695, 1999Srikant, R. and Agrawal, R. 1996.
Mining sequentialpatterns: Generalizations and performance im-provements, In Proc.
5th Int.
Conf.
Extending Data-base Technology (EDBT?96), Avignon, France.Tannen, D. (1990).
You just don?t understand, NewYork: Ballantine.Tsuruoka, Y. and Tsujii, J.
2005.
Bidirectional Infe-rence with the Easiest-First Strategy for TaggingSequence Data, HLT/EMNLP 2005, pp.
467-474.Tuv, E., Borisov, A., Runger, G., and Torkkola, K.2009.
Feature selection with ensembles, artificialvariables, and redundancy elimination.
JMLR, 10.Yan, X., Yan, L. 2006.
Gender Classification of Web-log Authors.
Computational Approaches to Analyz-ing Weblogs, AAAI.217
