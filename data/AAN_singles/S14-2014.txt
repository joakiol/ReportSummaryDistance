Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 109?113,Dublin, Ireland, August 23-24, 2014.AT&T: The Tag&Parse Approach to Semantic Parsing of Robot SpatialCommandsSvetlana Stoyanchev, Hyuckchul Jung, John Chen, Srinivas BangaloreAT&T Labs Research1 AT&T Way Bedminster NJ 07921{sveta,hjung,jchen,srini}@research.att.comAbstractThe Tag&Parse approach to semanticparsing first assigns semantic tags to eachword in a sentence and then parses thetag sequence into a semantic tree.
Weuse statistical approach for tagging, pars-ing, and reference resolution stages.
Eachstage produces multiple hypotheses whichare re-ranked using spatial validation.
Weevaluate the Tag&Parse approach on a cor-pus of Robotic Spatial Commands as partof the SemEval Task6 exercise.
Our sys-tem accuracy is 87.35% and 60.84% withand without spatial validation.1 IntroductionIn this paper we describe a system participatingin the SemEval2014 Task-6 on Supervised Seman-tic Parsing of Robotic Spatial Commands.
It pro-duces a semantic parse of natural language com-mands addressed to a robot arm designed to moveobjects on a grid surface.
Each command directsa robot to change position of an object given acurrent configuration.
A command uniquely iden-tifies an object and its destination, for example?Move the turquoise pyramid above the yellowcube?.
System output is a Robot Control Lan-guage (RCL) parse (see Figure 1) which is pro-cessed by the robot arm simulator.
The Robot Spa-tial Commands dataset (Dukes, 2013) is used fortraining and testing.Our system uses a Tag&Parse approach whichseparates semantic tagging and semantic parsingstages.
It has four components: 1) semantic tag-ging, 2) parsing, 3) reference resolution, and 4)spatial validation.
The first three are trained usingLLAMA (Haffner, 2006), a supervised machinelearning toolkit, on the RCL-parsed sentences.This work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/For semantic tagging, we train a maximum en-tropy sequence tagger for assigning a semantic la-bel and value to each word in a sentence, such astype cube or color blue.
For parsing, we train aconstituency parser on non-lexical RCL semantictrees.
For reference resolution, we train a maxi-mum entropy model that identifies entities for ref-erence tags found by previous components.
All ofthese components can generate multiple hypothe-ses.
Spatial validation re-ranks these hypothesesby validating them against the input spatial con-figuration.
The top hypothesis after re-ranking isreturned by the system.Separating tagging and parsing stages has sev-eral advantages.
A tagging stage allows the systemflexibility to abstract from possible grammatical orspelling errors in a command.
It assigns a seman-tic category to each word in a sentence.
Words notcontributing to the semantic meaning are assigned?O?
label by the tagger and are ignored in the fur-ther processing.
Words that are misspelled can po-tentially receive a correct tag when a word simi-larity feature is used in building a tagging model.This will be especially important when process-ing output of spoken commands that may containrecognition errors.The remainder of the paper is organized thusly.In Section 2 we describe each of the componentsused in our system.
In Section 3 we describe theresults reported for SemEval2014 and evaluationof each system component.
We summarize ourfindings and present future work in Section 4.2 System2.1 Sequence TaggingA sequence tagging approach is used for condi-tional inference of tags given a word sequence.It is used for many natural language tasks, suchas part of speech (POS) and named entity tag-ging (Toutanova and others, 2003; Carreras et al.,2003).
We train a sequence tagger for assign-109Figure 1: RCL tree for a sentence Move the turquoise pyramid above the yellow cube.Word index tag labelMove 1 action movethe 2 O -turquoise 3 color cyanpyramid 4 type prismabove 5 relation abovethe 6 O -yellow 7 color yellowcube 8 type cubeTable 1: Tagging labels for a sentence Move theturquoise pyramid above the yellow cube.ing a combined semantic tag and label (such astype cube) to each word in a command.
The tagsused for training are extracted from the leaf-levelnodes of the RCL trees.
Table 2 shows tags andlabels for a sample sentence ?Move the turquoisepyramid above the yellow cube?
extracted fromthe RCL parse tree (see Figure 1).
In some cases,a label is the same as a word (yellow, cube) whilein other cases, it differs (turquoise - cyan, pyramid- prism).We train a sequence tagger using LLAMA max-imum entropy (maxent) classification (Haffner,2006) to predict the combined semantic tag andlabel of each word.
Neighboring words, immedi-ately neighboring semantic tags, and POS tags areused as features, where the POS tagger is anothersequence tagging model trained on the Penn Tree-bank (Marcus et al., 1993).
We also experimentedwith a tagger that assigns tags and labels in sep-arate sequence tagging models, but it performedpoorly.2.2 ParsingWe use a constituency parser for building RCLtrees.
The input to the parser is a sequence oftags assigned by a sequence tagger, such as ?ac-tion color type relation color type?
for the exam-ple in Figure 1.The parser generates multiple RCL parse treehypotheses sorted in the order of their likelihood.The likelihood of a tree T given a sequence of tagsT is determined using a probabilistic context freegrammar (PCFG) G:P (T |S) =?r?TPG(r) (1)The n-best parses are obtained using the CKYalgorithm, recording the n-best hyperedge back-pointers per constituent along the lines of (Huangand Chiang, 2005).
G was obtained and PGwasestimated from a corpus of non-lexical RCL treesgenerated by removing all nodes descendant fromthe tag nodes (action, color, etc.).
Parses may con-tain empty nodes not corresponding to any tag inthe input sequence.
These are hypothesized by theparser at positions in between input tags and in-serted as edges according to the PCFG, which hasprobabilistic rules for generating empty nodes.2.3 Reference ResolutionReference resolution identifies the most prob-able antecedent for each anaphor within atext (Hirschman and Chinchor, 1997).
It applieswhen multiple candidates antecedents are present.For example, in a sentence ?Pick up the red cubestanding on a grey cube and place it on top ofthe yellow one?, the anaphor it has two candidateantecedents corresponding to entity segments thered cube and a grey cube.
In our system, anaphorand antecedents are represented by reference tagsoccurring in one sentence.
A reference tag is ei-ther assigned by a sequence tagger to one of thewords (e.g.
to a pronoun) or is inserted into atree by the parser (e.g.
ellipsis).
We train a bi-nary maxent model for this task using LLAMA.The input is a pair consisting of an anaphor anda candidate antecedent, along with their features.110Features that are used include the preceding andfollowing words as well as the tags/labels of boththe anaphor and candidate antecedent.
The refer-ence resolution component selects the antecedentfor which the model returns the highest score.2.4 Spatial ValidationSemEval2014 Task6 provided a spatial plannerwhich takes an RCL command as an input anddetermines if that command is executable in thegiven spatial context.
At each step described in2.1?2.3, due to the statistical nature of our ap-proach, multiple hypotheses can be easily com-puted with different confidence values.
We usedthe spatial planner to validate the final output RCLcommands from the three steps by checking if theRCLs are executable or not.
We generate multi-ple tagger output hypotheses.
For each tagger out-put hypothesis, we generate multiple parser out-put hypotheses.
For each parser output hypothe-sis, we generate multiple reference resolution out-put hypotheses.
The resulting output hypothesesare ranked in the order of confidence scores withthe highest tagging output scores ranked first, fol-lowed by the parsing output scores, and, finally,reference resolution output scores.
The system re-turns the result of the top scored command that isvalid according to the spatial validator.In many applications, there can be a tool ormethod to validate tag/parse/reference outputsfully or partially.
Note that in our system the val-idation is performed after all output is generated.Tightly coupled validation, such as checking va-lidity of a tagged entity or a parse constituent,could help in computing hypotheses at each step(e.g., feature values based on possible entities oractions) and it remains as future work.3 ResultsIn this section, we present evaluation results on thethree subsets of the data summarized in Table 3.
Inthe TEST2500 data set, the models are trained onthe initial 2500 sentences of the Robot CommandsTreebank and evaluated on the last 909 sentences(this corresponds to the data split of the SemEvaltask).
In TEST500 data set, the models are trainedon the initial 500 sentences of the training set andevaluated on the last 909 test sentences.
We re-port these results to analyze the models?
perfor-mance on a reduced training size.
In DEV2500data set, models are trained on 90% of the initial2500 sentences and evaluated on 10% of the 2500# Dataset Avg # hyp Accuracy1 TEST2500 1-best 1 86.0%2 TEST2500 max-5 3.34 95.2%3 TEST500 1-best 1 67.9%4 TEST500 max-5 4.25 83.8%5 DEV2500 1-best 1 90.8%6 DEV2500 max-5 2.9 98.0%Table 3: Tagger accuracy for 1-best and maximumof 5-best hypotheses (max-5).sentences using a random data split.
We observethat sentence length and standard deviation of testsentences in the TEST2500 data set is higher thanon the training sentences while in the DEV2500data set training and test sentence length and stan-dard deviation are comparable.3.1 Semantic TaggingTable 3 presents sentence accuracy of the seman-tic tagging stage.
Tagging accuracy is evaluatedon 1-best and on max-5 best tagger outputs.
Inthe max-5 setting the number of hypotheses gen-erated by the tagger varies for each input with theaverage numbers reported in Table 3.
Tagging ac-curacy on TEST2500 using 1-best is 86.0%.
Con-sidering max-5 best tagging sequences, the accu-racy is 95.2%.
On the TEST500 data set taggingaccuracy is 67.9% and 83.8% on 1-best and max-5 best sequences respectively, approximately 8%points lower than on TEST2500 data set.
On theDEV2500 data set tagging accuracy is 90.8% and98.0% on 1-best and max-5 best sequences, 4.8%and 2.8% points higher than on the TEST2500data set.
The higher performance on DEV2500 incomparison to the TEST2500 can be explained bythe higher complexity of the test sentences in com-parison to the training sentences in the TEST2500data set.3.2 RCL ParsingParsing was evaluated using the EVALB scoringmetric (Collins, 1997).
Its 1-best F-measure accu-racy on gold standard TEST2500 and DEV2500semantic tag sequences was 96.17% and 95.20%,respectively.
On TEST500, its accuracy remained95.20%.
On TEST2500 with system provided in-put sequences, its accuracy was 94.79% for 869out of 909 sentences that were tagged correctly.3.3 System AccuracyTable 4 presents string accuracy of automaticallygenerated RCL parse trees on each data set.
The111Name Train #sent Train Sent.
len.
(stdev) Test #sent Test Sent.
Len.
(stdev)TEST2500 2500 13.44 (5.50) 909 13.96 (5.59)TEST500 500 14.62(5.66) 909 13.96 (5.59)DEV2500 2250 13.43 ( 5.53) 250 13.57 (5.27)Table 2: Number of sentences, average length and standard deviation of the data sets.results are obtained by comparing system outputRCL parse string with the reference RCL parsestring.
For each data set, we ran the systemwith and without spatial validation.
We ran RCLparser and reference resolution on automaticallyassigned semantic tags (Auto) and oracle tagging(Orcl).
We observed that some tag labels can beverified systematically and corrected them withsimple rules: e.g., change ?front?
to ?forward?because relation specification in (Dukes, 2013)doesn?t have ?front?
even though annotations in-cluded cases with ?front?
as relation.The system performance on TEST2500 dataset using automatically assigned tags and no spa-tial validation is 60.84%.
In this mode, the sys-tem uses 1-best parser and 1-best tagger output.With spatial validation, which allows the system tore-rank parser and tagger hypotheses, the perfor-mance increases by 27% points to 87.35%.
Thisindicates that the parser and the tagger componentoften produce a correct output which is not rankedfirst.
Using oracle tags without / with spatial vali-dation on TEST2500 data set the system accuracyis 67.55% / 94.83%, 7% points above the accuracyusing predicted tags.The system performance on TEST500 data setusing automatically assigned tags with / with-out spatial validation is 48.95% / 74.92%, ap-proximately 12% points below the performanceon TEST2500 (Row 1).
Using oracle tags with-out / with spatial validation the performance onTEST500 data set is 63.89% / 94.94%.
The per-formance without spatial validation is only 4% be-low TEST2500, while with spatial validation theperformance on TEST2500 and TEST500 is thesame.
These results indicate that most perfor-mance degradation on a smaller data set is due tothe semantic tagger.The system performance on DEV2500 data setusing automatically assigned tags without / withspatial validation is 68.0% / 96.80% (Row 5), 8%points above the performance on TEST2500 (Row1).
With oracle tags, the performance is 69.60%/ 98.0%, which is 2-3% points above TEST2500(Row 2).
These results indicate that most perfor-mance improvement on a better balanced data set# Dataset Tag Accuracy without / withspatial validation1 TEST2500 Auto 60.84 / 87.352 TEST2500 Orcl 67.55 / 94.833 TEST500 Auto 48.95 / 74.924 TEST500 Orcl 63.89 / 94.945 DEV2500 Auto 68.00 / 96.806 DEV2500 Orcl 69.60 / 98.00Table 4: System accuracy with and without spatialvalidation using automatically assigned tags andoracle tags (OT).DEV2500 is due to better semantic tagging.4 Summary and Future WorkIn this paper, we present the results of semanticprocessing for natural language robot commandsusing Tag&Parse approach.
The system first tagsthe input sentence and then applies non-lexicalparsing to the tag sequence.
Reference resolutionis applied to the resulting parse trees.
We com-pare the results of the models trained on the datasets of size 500 (TEST500) and 2500 (TEST2500)sentences.
We observe that sequence taggingmodel degrades significantly on a smaller data set.Parsing and reference resolution models, on theother hand, perform nearly as well on both train-ing sizes.
We compare the results of the modelstrained on more (DEV2500) and less (TEST2500)homogeneous training/testing data sets.
We ob-serve that a semantic tagging model is more sen-sitive to the difference between training and testset than parsing model degrading significantly aless homogeneous data set.
Our results show that1) both tagging and parsing models will benefitfrom an improved re-ranking, and 2) our parsingmodel is robust to a data size reduction while tag-ging model requires a larger training data set.In future work we plan to explore howTag&Parse approach will generalize in other do-mains.
In particular, we are interested in usinga combination of domain-specific tagging modelsand generic semantic parsing (Das et al., 2010) forprocessing spoken commands in a dialogue sys-tem.112ReferencesXavier Carreras, Llu?
?s M`arquez, and Llu?
?s Padr?o.2003.
A Simple Named Entity Extractor Using Ad-aBoost.
In Proceedings of the CoNLL, pages 152?157, Edmonton, Canada.Michael Collins.
1997.
Three Generative LexicalizedModels for Statistical Parsing.
In Proceedings of the35th Annual Meeting of the ACL, pages 16?23.Dipanjan Das, Nathan Schneider, Desai Chen, andNoah A. Smith.
2010.
Probabilistic Frame-Semantic Parsing.
In HLT-NAACL, pages 948?956.Kais Dukes.
2013.
Semantic Annotation of RoboticSpatial Commands.
In Language and TechnologyConference (LTC).Patrick Haffner.
2006.
Scaling large margin classifiersfor spoken language understanding.
Speech Com-munication, 48(3-4):239?261.Lynette Hirschman and Nancy Chinchor.
1997.
MUC-7 Coreference Task Definition.
In Proceedings ofthe Message Understanding Conference (MUC-7).Science Applications International Corporation.Liang Huang and David Chiang.
2005.
Better K-best Parsing.
In Proceedings of the Ninth Inter-national Workshop on Parsing Technology, Parsing?05, pages 53?64, Stroudsburg, PA, USA.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In Proceedings of the 2003 Conference of theNAACL on Human Language Technology - Volume1, pages 173?180.113
