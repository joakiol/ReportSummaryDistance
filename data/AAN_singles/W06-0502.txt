Proceedings of the 2nd Workshop on Ontology Learning and Population, pages 10?17,Sydney, July 2006. c?2006 Association for Computational LinguisticsMultilingual Ontology Acquisition from Multiple MRDsEric Nichols?, Francis Bond?, Takaaki Tanaka?, Sanae Fujita?, Dan Flickinger ??
Nara Inst.
of Science and Technology ?
NTT Communication Science Labs ?
Stanford UniversityGrad.
School of Information Science Natural Language Research Group CSLINara, Japan Keihanna, Japan Stanford, CAeric-n@is.naist.jp {bond,takaaki,sanae}@cslab.kecl.ntt.co.jp danf@csli.stanford.eduAbstractIn this paper, we outline the develop-ment of a system that automatically con-structs ontologies by extracting knowledgefrom dictionary definition sentences us-ing Robust Minimal Recursion Semantics(RMRS).
Combining deep and shallowparsing resource through the common for-malism of RMRS allows us to extract on-tological relations in greater quantity andquality than possible with any of the meth-ods independently.
Using this method,we construct ontologies from two differ-ent Japanese lexicons and one English lex-icon.
We then link them to existing, hand-crafted ontologies, aligning them at theword-sense level.
This alignment providesa representative evaluation of the qual-ity of the relations being extracted.
Wepresent the results of this ontology con-struction and discuss how our system wasdesigned to handle multiple lexicons andlanguages.1 IntroductionAutomatic methods of ontology acquisition have along history in the field of natural language pro-cessing.
The information contained in ontolo-gies is important for a number of tasks, for ex-ample word sense disambiguation, question an-swering and machine translation.
In this paper,we present the results of experiments conductedin automatic ontological acquisition over two lan-guages, English and Japanese, and from three dif-ferent machine-readable dictionaries.Useful semantic relations can be extracted fromlarge corpora using relatively simple patterns (e.g.,(Pantel et al, 2004)).
While large corpora oftencontain information not found in lexicons, even avery large corpus may not include all the familiarwords of a language, let alne those words occur-ring in useful patterns (Amano and Kondo, 1999).Therefore it makes sense to also extract data frommachine readable dictionaries (MRDs).There is a great deal of work on the creationof ontologies from machine readable dictionaries(a good summary is (Wilkes et al, 1996)), mainlyfor English.
Recently, there has also been inter-est in Japanese (Tokunaga et al, 2001; Nicholset al, 2005).
Most approaches use either a special-ized parser or a set of regular expressions tunedto a particular dictionary, often with hundreds ofrules.
Agirre et al (2000) extracted taxonomicrelations from a Basque dictionary with high ac-curacy using Constraint Grammar together withhand-crafted rules.
However, such a system is lim-ited to one language, and it has yet to be seenhow the rules will scale when deeper semantic re-lations are extracted.
In comparison, as we willdemonstrate, our system produces comparable re-sults while the framework is immediately applica-ble to any language with the resources to produceRMRS.
Advances in the state-of-the-art in pars-ing have made it practical to use deep processingsystems that produce rich syntactic and semanticanalyses to parse lexicons.
This high level of se-mantic information makes it easy to identify therelations between words that make up an ontol-ogy.
Such an approach was taken by the MindNetproject (Richardson et al, 1998).
However, deepparsing systems often suffer from small lexiconsand large amounts of parse ambiguity, making itdifficult to apply this knowledge broadly.Our ontology extraction system uses RobustMinimal Recursion Semantics (RMRS), a formal-ism that provides a high level of detail while, atthe same time, allowing for the flexibility of un-derspecification.
RMRS encodes syntactic infor-mation in a general enough manner to make pro-cessing of and extraction from syntactic phenom-ena including coordination, relative clause analy-10sis and the treatment of argument structure fromverbs and verbal nouns.
It provides a common for-mat for naming semantic relations, allowing themto be generalized over languages.
Because of this,we are able to extend our system to cover new lan-guages that have RMRS resourses available witha minimal amount of effort.
The underspecifica-tion mechanism in RMRS makes it possible for usto produce input that is compatible with our sys-tem from a variety of different parsers.
By select-ing parsers of various different levels of robustnessand informativeness, we avoid the coverage prob-lem that is classically associated with approachesusing deep-processing; using heterogeneous pars-ing resources maximizes the quality and quantityof ontological relations extracted.
Currently, oursystem uses input from parsers from three lev-els: with morphological analyzers the shallowest,parsers using Head-driven Phrase Structure Gram-mars (HPSG) the deepest and dependency parsersproviding a middle ground.Our system was initially developed for oneJapanese dictionary (Lexeed).
The use of the ab-stract formalism, RMRS, made it easy to extend toa different Japanese lexicon (Iwanami) and even alexicon in a different language (GCIDE).Section 2 provides a description of RMRS andthe tools used by our system.
The ontological ac-quisition system is presented in Section 3.
The re-sults of evaluating our ontologies by comparisonwith existing resources are given in Section 4.
Wediscuss our findings in Section 5.2 Resources2.1 The Lexeed Semantic Database ofJapaneseThe Lexeed Semantic Database of Japanese is amachine readable dictionary that covers the mostfamiliar open class words in Japanese as measuredby a series of psycholinguistic experiments (Kasa-hara et al, 2004).
Lexeed consists of all open classwords with a familiarity greater than or equal tofive on a scale of one to seven.
This gives 28,000words divided into 46,000 senses and defined with75,000 definition sentences.
All definition sen-tences and example sentences have been rewrittento use only the 28,000 familiar open class words.The definition and example sentences have beentreebanked with the JACY grammar (?
2.4.2).2.2 The Iwanami Dictionary of JapaneseThe Iwanami Kokugo Jiten (Iwanami) (Nishioet al, 1994) is a concise Japanese dictionary.A machine tractable version was made avail-able by the Real World Computing Project forthe SENSEVAL-2 Japanese lexical task (Shirai,2003).
Iwanami has 60,321 headwords and 85,870word senses.
Each sense in the dictionary con-sists of a sense ID and morphological information(word segmentation, POS tag, base form and read-ing, all manually post-edited).2.3 The Gnu Contemporary InternationalDictionary of EnglishThe GNU Collaborative International Dictionaryof English (GCIDE) is a freely available dic-tionary of English based on Webster?s RevisedUnabridged Dictionary (published in 1913), andsupplemented with entries from WordNet and ad-ditional submissions from users.
It currentlycontains over 148,000 definitions.
The versionused in this research is formatted in XML and isavailable for download from www.ibiblio.org/webster/.We arranged the headwords by frequency andsegmented their definition sentences into sub-sentences by tokenizing on semicolons (;).
Thisproduced a total of 397,460 pairs of headwordsand sub-sentences, for an average of slightly lessthan four sub-sentences per definition sentence.For corpus data, we selected the first 100,000 def-inition sub-sentences of the headwords with thehighest frequency.
This subset of definition sen-tences contains 12,440 headwords with 36,313senses, covering approximately 25% of the defi-nition sentences in the GCIDE.
The GCIDE hasthe most polysemy of the lexicons used in this re-search.
It averages over 3 senses per word definedin comparison to Lexeed and Iwanami which bothhave less than 2.2.4 Parsing ResourcesWe used Robust Minimal Recursion Semantics(RMRS) designed as part of the Deep Thoughtproject (Callmeier et al, 2004) as the formal-ism for our ontological relation extraction en-gine.
We used deep-processing tools from theDeep Linguistic Processing with HPSG Initiative(DELPH-IN: http://www.delph-in.net/) aswell as medium- and shallow-processing tools forJapanese processing (the morphological analyzer11ChaSen and the dependency parser CaboCha)from the Matsumoto Laboratory.2.4.1 Robust Minimal Recursion SemanticsRobust Minimal Recursion Semantics is a formof flat semantics which is designed to allow deepand shallow processing to use a compatible se-mantic representation, with fine-grained atomiccomponents of semantic content so shallow meth-ods can contribute just what they know, yet withenough expressive power for rich semantic contentincluding generalized quantifiers (Frank, 2004).The architecture of the representation is based onMinimal Recursion Semantics (Copestake et al,2005), including a bag of labeled elementary pred-icates (EPs) and their arguments, a list of scopingconstraints which enable scope underspecification,and a handle that provides a hook into the repre-sentation.The representation can be underspecified inthree ways: relationships can be omitted (suchas quantifiers, messages, conjunctions and so on);predicate-argument relations can be omitted; andpredicate names can be simplified.
Predicatenames are defined in such a way as to be ascompatible (predictable) as possible among differ-ent analysis engines, using a lemma pos subsensenaming convention, where the subsense is optionaland the part-of-speech (pos) for coarse-grainedsense distinctions is drawn from a small set of gen-eral types (noun, verb, sahen (verbal noun), .
.
.
).The predicate unten s (?U unten ?drive?
), forexample, is less specific than unten s 2 and thussubsumes it.
In order to simplify the combinationof different analyses, the EPs are indexed to thecorresponding character positions in the originalinput sentence.Examples of deep and shallow results for thesame sentence ?
k?U2d0 jido?sha wounten suru hito ?a person who drives a car (lit:car-ACC drive do person)?
are given in Figures 1and 2 (omitting the indexing).
Real predicates areprefixed by an under-bar ( ).
The deep parse givesinformation about the scope, message types andargument structure, while the shallow parse giveslittle more than a list of real and grammatical pred-icates with a hook.2.4.2 Deep Parsers (JACY, ERG and PET)For both Japanese and English, we used the PETSystem for the high-efficiency processing of typedfeature structures (Callmeier, 2000).
For Japanese,we used JACY (Siegel, 2000), for English we usedthe English Resource Grammar (ERG: Flickinger2000).1JACY The JACY grammar is an HPSG-basedgrammar of Japanese which originates from workdone in the Verbmobil project (Siegel, 2000) onmachine translation of spoken dialogues in the do-main of travel planning.
It has since been ex-tended to accommodate written Japanese and newdomains (such as electronic commerce customeremail and machine readable dictionaries).The grammar implementation is based on a sys-tem of types.
There are around 900 lexical typesthat define the syntactic, semantic and pragmaticproperties of the Japanese words, and 188 typesthat define the properties of phrases and lexicalrules.
The grammar includes 50 lexical rulesfor inflectional and derivational morphology and47 phrase structure rules.
The lexicon containsaround 36,000 lexemes.The English Resource Grammar (ERG) TheEnglish Resource Grammar (ERG: (Flickinger,2000)) is a broad-coverage, linguistically precisegrammar of English, developed within the Head-driven Phrase Structure Grammar (HPSG) frame-work, and designed for both parsing and gen-eration.
It was also originally launched withinthe Verbmobil (Wahlster, 2000) spoken languagemachine translation project for the particular do-mains of meeting scheduling and travel planning.The ERG has since been substantially extended inboth grammatical and lexical coverage, reaching80-90% coverage of sizeable corpora in two ad-ditional domains: electronic commerce customeremail and tourism brochures.The grammar includes a hand-built lexicon of23,000 lemmas instantiating 850 lexical types, ahighly schematic set of 150 grammar rules, and aset of 40 lexical rules, all organized in a rich multi-ple inheritance hierarchy of some 3000 typed fea-ture structures.
Like other DELPH-IN grammars,the ERG can be processed by several parsers andgenerators, including the LKB (Copestake, 2002)and PET (Callmeier, 2000).
Each successful ERGanalysis of a sentence or fragment includes a fine-grained semantic representation in MRS.For the task of parsing the dictionary defini-tions in GCIDE (the GNU Collaborative Interna-1Both grammars, the LKB and PET are available at<http://www.delph-in.net/>.12?????????????????????
?TEXT ?
k?U2d0TOP h1RELS???????????????????
?proposition m relLBL h1ARG0 e2 tense=presentMARG h3????
?jidousha n relLBL h4ARG0 x5??????
?udef relLBL h6ARG0 x5RSTR h7BODY h8?????????
?unten s relLBL h9ARG0 e11 tense=presentARG1 x10ARG2 x5??????
?hito n relLBL h12ARG0 x10??????
?udef relLBL h13ARG0 x10RSTR h14BODY h15???????
?proposition m relLBL h10001ARG0 e11 tense=presentMARG h16?????
?unknown relLBL h17ARG0 e2 tense=presentARG x10???????????????????
?HCONS {h3 qeq h17,h7 qeq h4,h14 qeq h12,h16 qeq h9}ING {h12 ing h10001}?????????????????????
?Figure 1: RMRS for the Sense 2 of doraiba- ?driver?
(Cabocha/JACY)????
?TEXT ?
k?U2d0TOP h9RELS????
?jidousha n relLBL h1ARG0 x2???
?o p relLBL h3ARG0 u4???
?unten s relLBL h5ARG0 e6???
?suru v relLBL h7ARG0 x8???
?hito n relLBL h9ARG0 x10?????????
?Figure 2: RMRS for the Sense 2 of doraiba- ?driver?
(ChaSen)tional Dictionary of English; see below), the ERGwas minimally extended to include two additionalfragment rules, for gap-containing VPs and PPs(idiosyncratic to this domain), and additional lex-ical entries were manually added for all missingwords in the alphabetically first 10,000 definitionsentences.These first 10,000 sentences were parsed andthen manually tree-banked to provide the train-ing material for constructing the stochastic modelused for best-only parsing of the rest of the defini-tion sentences.
Using POS-based unknown-wordguessing for missing lexical entries, MRSes wereobtained for about 75% of the first 100,000 defini-tion sentences.2.4.3 Medium Parser (CaboCha-RMRS)For Japanese, we produce RMRS from the de-pendency parser Cabocha (Kudo and Matsumoto,2002).
The method is similar to that of Spreyerand Frank (2005), who produce RMRS from de-tailed German dependencies.
CaboCha providesfairly minimal dependencies: there are three links(dependent, parallel, apposition) and they linkbase phrases (Japanese bunsetsu), marked withthe syntactic and semantic head.
The CaboCha-RMRS parser uses this information, along withheuristics based on the parts-of-speech, to produceunderspecified RMRSs.
CaboCha-RMRS is ca-pable of making use of HPSG resources, includ-ing verbal case frames, to further enrich its out-put.
This allows it to produce RMRS that ap-proaches the granularity of the analyses given byHPSG parsers.
Indeed, CaboCha-RMRS and JACYgive identical parses for the example sentence inFigure 1.
One of our motivations in including amedium parser in our system is to extract more re-lations that require special processing; the flexibil-ity of CaboCha-RMRS and the RMRS formalismmake this possible.2.4.4 Shallow Parser (ChaSen-RMRS)The part-of-speech tagger, ChaSen (Matsumotoet al, 2000) was used for shallow processing ofJapanese.
Predicate names were produced bytransliterating the pronunciation field and map-ping the part-of-speech codes to the RMRS supertypes.
The part-of-speech codes were also usedto judge whether predicates were real or gram-matical.
Since Japanese is a head-final language,the hook value was set to be the handle of theright-most real predicate.
This is easy to do forJapanese, but difficult for English.3 Ontology ConstructionWe adopt the ontological relation extraction algo-rithm used by Nichols et al (2005).
Its goal is toidentify the semantic head(s) of a dictionary def-inition sentence ?
the relation(s) that best sum-marize it.
The algorithm does this by traversingthe RMRS structure of a given definition sentencestarting at the HOOK (the highest-scoping seman-tic relationship) and following its argument struc-ture.
When the algorithm can proceed no fur-ther, it returns the a tuple consisting of the def-inition word and the word identified by the se-13mantic relation where the algorithm halted.
Ourextended algorithm has the following characteris-tics: sentences with only one content-bearing re-lation are assumed to identify a synonym; spe-cial relation processing (?
3.1) is used to gathermeta-information and identify ontological rela-tions; processing of coordination allows for ex-traction of multiple ontological relations; filteringby part-of-speech screens out unlikely relations(?
3.2).3.1 Special RelationsOccasionally, relations which provide ontologicalmeta-information, such as the specification of do-main or temporal expressions, or which help iden-tify the type of ontological relation present are en-countered.
Nichols et al (2005) identified theseas special relations.
We use a small number ofrules to determine where the semantic head is andwhat ontological relation should be extracted.
Asample of the special relations are listed in Ta-ble 1.
This technique follows in a long tradition ofspecial treatment of certain words that have beenshown to be particularly relevant to the task ofontology construction or which are semanticallycontent-free.
These words or relations have alsobe referred to as ?empty heads?, ?function nouns?,or ?relators?
in the literature (Wilkes et al, 1996).Our approach generalizes the treatment of thesespecial relations to rules that are portable for anyRMRS (modulo the language specific predicatenames) giving it portability that cannot be foundin approaches that use regular expressions or spe-cialized parsers.Special Predicate (s) OntologicalJapanese English Relationisshu, hitotsu form, kind, one hypernymryaku(shou) abbreviation abbreviationbubun, ichibu part, peice meronymmeishou name namekeishou ?polite name for?
name:honorificzokushou ?slang for?
name:slangTable 1: Special predicates and their associatedontological relationsAugmenting the system to work on English def-inition sentence simply entailed writing rules tohandle special relations that occur in English.
Oursystem currently has 26 rules for Japanese and 50rules for English.
These rules provide process-ing of relations like those found in Table 1, andthey also handle processing of coordinate struc-tures, such as noun phrases joined together withconjunctions such as and, or, and punctuation.3.2 Filtering by Part-of-SpeechOne of the problems encountered in expanding theapproach in Nichols et al (2005) to handle En-glish dictionaries is that many of the definitionsentences have a semantic head with a part-of-speech different than that of the definition word.We found that differing parts-of-speech often indi-cated an undesirable ontological relation.
One rea-son such relations can be extracted is when a sen-tence with a non-defining role, for example indi-cating usage, is encountered.
Definition sentencefor non-content-bearing words such as of or thealso pose problems for extraction.We avoid these problems by filtering by parts-of-speech twice in the extraction process.
First, weselect candidate sentences for extraction by veri-fying that the definition word has a content wordPOS (i.e.
adjective, adverb, noun, or verb).
Fi-nally, before we extract any ontological relation,we make sure that the definition word and the se-mantic head are in compatible POS classes.While adopting this strategy does reduce thenumber of total ontological relations that we ac-quire, it increases their reliability.
The addition ofa medium parser gives us more RMRS structuresto extract from, which helps compensate for anyloss in number.4 Results and EvaluationWe summarize the relationships acquired in Ta-ble 2.
The columns specify source dictionaryand parsing method while the rows show the rela-tion type.
These counts represent the total num-ber of relations extracted for each source andmethod combination.
The majority of relationsextracted are synonyms and hypernyms; however,some higher-level relations such as meronym andabbreviation are also acquired.
It should alsobe noted that both the medium and deep meth-ods were able to extract a fair number of spe-cial relations.
In many cases, the medium methodeven extracted more special relations than the deepmethod.
This is yet another indication of theflexibility of dependency parsing.
Altogether, weextracted 105,613 unique relations from Lexeed(for 46,000 senses), 183,927 unique relations fromIwanami (for 85,870 senses), and 65,593 uniquerelations from GCIDE (for 36,313 senses).
As canbe expected, a general pattern in our results is thatthe shallow method extracts the most relations intotal followed by the medium method, and finally14Relation Lexeed Iwanami GCIDEShallow Medium Deep Shallow Medium Deep Deephypernym 47,549 43,006 41,553 113,120 113,433 66,713 40,583synonym 12,692 13,126 9,114 31,682 32,261 18,080 21,643abbreviation 340 429 1,533 739meronym 235 189 395 202 472name 100 89 271 140Table 2: Results of Ontology Extractionthe deep method.4.1 Verification with Hand-craftedOntologiesBecause we are interested in comparing lexical se-mantics across languages, we compared the ex-tracted ontology with resources in both the sameand different languages.For Japanese we verified our results by com-paring the hypernym links to the manually con-structed Japanese ontology Goi-Taikei (GT).
It isa hierarchy of 2,710 semantic classes, defined forover 264,312 nouns Ikehara et al (1997).
The se-mantic classes are mostly defined for nouns (andverbal nouns), although there is some informationfor verbs and adjectives.
For English, we com-pared relations to WordNet 2.0 (Fellbaum, 1998).Comparison for hypernyms is done as follows:look up the semantic class or synset C for both theheadword (wi) and genus term(s) (wg).
If at leastone of the index word?s classes is subsumed by atleast one of the genus?
classes, then we considerthe relationship confirmed (1).?
(ch,cg) : {ch ?
cg;ch ?C(wh);cg ?C(wg)} (1)To test cross-linguistically, we looked up theheadwords in a translation lexicon (ALT-J/E (Ike-hara et al, 1991) and EDICT (Breen, 2004)) andthen did the confirmation on the set of translationsci ?
C(T (wi)).
Although looking up the transla-tion adds noise, the additional filter of the relation-ship triple effectively filters it out again.The total figures given in Table 3 do not matchthe totals given in Table 2.
These totals representthe number of relations where both the definitionword and semantic head were found in at least oneof the ontologies being used in this comparison.By comparing these numbers to the totals givenin Section 4, we can get an idea of the coverageof the ontologies being used in comparison.
Lex-eed has a coverage of approx.
55.74% ( 58,867105,613 ),with Iwanami the lowest at 48.20% ( 88,662183,927 ), andGCIDE the highest at 69.85% (45,81465,593 ).
It is clearthat there are a lot of relations in each lexicon thatare not covered by the hand-crafted ontologies.This demonstrates that machine-readable dictio-naries are still a valuable resource for constructingontologies.4.1.1 LexeedOur results using JACY achieve a confirmationrate of 66.84% for nouns only and 60.67% over-all (Table 3).
This is an improvement over bothTokunaga et al (2001), who reported 61.4% fornouns only, and Nichols et al (2005) who reported63.31% for nouns and 57.74% overall.
We alsoachieve an impressive 33,333 confirmed relationsfor a rate of 56.62% overall.
It is important tonote that our total counts include all unique re-lations regardless of source, unlike Nichols et al(2005) who take only the relation from the deepestsource whenever multiple relations are extracted.It is interesting to note that shallow processing outperforms medium with 22,540 verified relations(59.40%) compared to 21,806 (57.76%).
Thiswould seem to suggest that for the simplest task ofretrieving hyperynms and synonyms, more infor-mation than that is not necessary.
However, sincemedium and deep parsing obtain relations not cov-ered by shallow parsing and can extract special re-lations, a task that cannot be performed withoutsyntactic information, it is beneficial to use themas well.Agirre et al (2000) reported an error rate of2.8% in a hand-evaluation of the semantic rela-tions they automatically extracted from a machine-readable Basque dictionary.
In a similar hand-evaluation of a stratified sampling of relations ex-tracted from Lexeed, we achieved an error rateof 9.2%, demonstrating that our method is alsohighly accurate (Nichols et al, 2005).4.2 IwanamiIwanami?s verification results are similar to Lex-eed?s (Table 3).
There are on average around 3%more verifications and a total of almost 20,000more verified relations extracted.
It is particu-larly interesting to note that deep processing per-15Confirmed Relations in LexeedMethod / Relation hypernym synonym TotalShallow 58.55 % ( 16585 / 28328 ) 61.93 % ( 5955 / 9615 ) 59.40 % ( 22540 / 37943 )Medium 55.97 % ( 15431 / 27570 ) 62.61 % ( 6375 / 10182 ) 57.76 % ( 21806 / 37752 )Deep 54.78 % ( 4954 / 9043 ) 67.76 % ( 5098 / 7524 ) 60.67 % ( 10052 / 16567 )All 55.22 % ( 23802 / 43102 ) 60.46 % ( 9531 / 15765 ) 56.62 % ( 33333 / 58867 )Confirmed Relations in IwanamiMethod / Relation hypernym synonym TotalShallow 61.20 % ( 35208 / 57533 ) 63.57 % ( 11362 / 17872 ) 61.76 % ( 46570 / 75405 )Medium 60.69 % ( 35621 / 58698 ) 62.86 % ( 11037 / 17557 ) 61.19 % ( 46658 / 76255 )Deep 63.59 % ( 22936 / 36068 ) 64.44 % ( 8395 / 13027 ) 63.82 % ( 31331 / 49095 )All 59.36 % ( 40179 / 67689 ) 61.66 % ( 12931 / 20973 ) 59.90 % ( 53110 / 88662 )Confirmed Relations in GCIDEPOS / Relation hypernym synonym TotalAdjective 2.88 % ( 37 / 1283 ) 16.77 % ( 705 / 4203 ) 13.53 % ( 742 / 5486 )Noun 57.60 % ( 7518 / 13053 ) 50.71 % ( 3522 / 6945 ) 55.21 % ( 11040 / 19998 )Verb 24.22 % ( 3006 / 12411 ) 21.40 % ( 1695 / 7919 ) 23.12 % ( 4701 / 20330 )Total 39.48 % ( 10561 / 26747 ) 31.06 % ( 5922 / 19067 ) 35.98 % ( 16483 / 45814 )Table 3: Confirmed Relations, measured against GT and WordNetforms better here than on Lexeed (63.82% vs60.67%), even though the grammar was developedand tested on Lexeed.
There are two reasons forthis: The first is that the process of rewriting Lex-eed to use only familiar words actually makes thesentences harder to parse.
The second is that theless familiar words in Iwanami have fewer senses,and easier to parse definition sentences.
In anycase, the results support our claims that our onto-logical relation extraction system is easily adapt-able to new lexicons.4.3 GCIDEAt first glance, it would seem that GCIDE hasthe most disappointing of the verification resultswith overall verification of not even 36% and only16,483 relations confirmed.
However, on closerinspection one can see that noun hypernyms are arespectable 57.60% with over 55% for all nouns.These figures are comparable with the results weare obtaining with the other lexicons.
One shouldalso bear in mind that the definitions found inGCIDE can be archaic; after all this dictionarywas first published in 1913.
This could be onecause of parsing errors for ERG.
Despite these ob-stacles, we feel that GCIDE has a lot of poten-tial for ontological acquisition.
A dictionary ofits size and coverage will most likely contain rela-tions that may not be represented in other sources.One only has to look at the definition of ??
{? ?driver?/driver to confirm this; GT hastwo senses (?screwdriver?
and ?vehicle operator?
)Lexeed and Iwanami have 3 senses each (adding?golf club?
), and WordNet has 5 (including ?soft-ware driver?
), but GCIDE has 6, not including?software driver?
but including spanker ?a kind ofsail?.
It should be beneficial to propagate thesedifferent senses across ontologies.5 Discussion and Future WorkWe were able to successfully combine deep pro-cessing of various levels of depth in order toextract ontological information from lexical re-sources.
We showed that, by using a well definedsemantic representation, the extraction can be gen-eralized so much that it can be used on very differ-ent dictionaries from different languages.
This isan improvement on the common approach to usingmore and more detailed regular expressions (e.g.Tokunaga et al (2001)).
Although this provides aquick start, the results are not generally reusable.In comparison, the shallower RMRS engines areimmediately useful for a variety of other tasks.However, because the hook is the only syntacticinformation returned by the shallow parser, onto-logical relation extraction is essentially performedby this hook-identifying heuristic.
While this issufficient for a large number of sentences, it is notpossible to process special relations with the shal-low parser since none of the arguments are linkedwith the predicates to which they belong.
Thus, asTable 2 shows, our shallow parser is only capableof retrieving hypernyms and synonyms.
It is im-portant to extract a variety of semantic relations inorder to form a useful ontology.
This is one of thereasons why we use a combination of parsers of16different analytic levels rather than depending ona single resource.The other innovation of our approach is thecross-lingual evaluation.
As a by-product ofthe evaluation we enhance the existing resources(such as GT or WordNet) by linking them, sothat information can be shared between them.
Inthis way we can use the cross-lingual links to fillgaps in the monolingual resources.
GT and Word-Net both lack complete cover - over half the rela-tions were confirmed with only one resource.
Thisshows that the machine readable dictionary is auseful source of these relations.6 ConclusionIn this paper, we presented the results of experi-ments conducted in automatic ontological acqui-sition over two languages, English and Japanese,and from three different machine-readable dictio-naries.
Our system is unique in combining parsersof various levels of analysis to generate its inputsemantic structures.
The system is language ag-nostic and we give results for both Japanese andEnglish MRDs.
Finally, we presented evaluationof the ontologies constructed by comparing themwith existing hand-crafted English and Japaneseontologies.ReferencesEneko Agirre, Olatz Ansa, Xabier Arregi, Xabier Artola,Arantza Diaz de Ilarraza, Mikel Lersundi, David Martinez,Kepa Sarasola, and Ruben Urizar.
2000.
Extraction ofsemantic relations from a Basque monolingual dictionaryusing Constraint Grammar.
In EURALEX 2000.Shigeaki Amano and Tadahisa Kondo.
1999.
Nihongo-noGoi-Tokusei (Lexical properties of Japanese).
Sanseido.J.
W. Breen.
2004.
JMDict: a Japanese-multilingual dictio-nary.
In Coling 2004 Workshop on Multilingual LinguisticResources, pages 71?78.
Geneva.Ulrich Callmeier.
2000.
PET - a platform for experimenta-tion with efficient HPSG processing techniques.
NaturalLanguage Engineering, 6(1):99?108.Ulrich Callmeier, Andreas Eisele, Ulrich Scha?fer, andMelanie Siegel.
2004.
The DeepThought core architectureframework.
In Proceedings of LREC-2004, volume IV.Lisbon.Ann Copestake.
2002.
Implementing Typed Feature StructureGrammars.
CSLI Publications.Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.Sag.
2005.
Minimal Recursion Semantics.
An introduc-tion.
Research on Language and Computation, 3(4):281?332.Christine Fellbaum, editor.
1998.
WordNet: An ElectronicLexical Database.
MIT Press.Dan Flickinger.
2000.
On building a more efficient gram-mar by exploiting types.
Natural Language Engineering,6(1):15?28.
(Special Issue on Efficient Processing withHPSG).Anette Frank.
2004.
Constraint-based RMRS constructionfrom shallow grammars.
In 20th International Con-ference on Computational Linguistics: COLING-2004,pages 1269?1272.
Geneva.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai, AkioYokoo, Hiromi Nakaiwa, Kentaro Ogura, YoshifumiOoyama, and Yoshihiko Hayashi.
1997.
Goi-Taikei ?A Japanese Lexicon.
Iwanami Shoten, Tokyo.
5 vol-umes/CDROM.Satoru Ikehara, Satoshi Shirai, Akio Yokoo, and HiromiNakaiwa.
1991.
Toward an MT system without pre-editing?
effects of new methods in ALT-J/E ?.
In Third Ma-chine Translation Summit: MT Summit III, pages 101?106.
Washington DC.
(http://xxx.lanl.gov/abs/cmp-lg/9510008).Kaname Kasahara, Hiroshi Sato, Francis Bond, TakaakiTanaka, Sanae Fujita, Tomoko Kanasugi, and ShigeakiAmano.
2004.
Construction of a Japanese semantic lex-icon: Lexeed.
SIG NLC-159, IPSJ, Tokyo.
(in Japanese).Taku Kudo and Yuji Matsumoto.
2002.
Japanese depen-dency analysis using cascaded chunking.
In CoNLL 2002:Proceedings of the 6th Conference on Natural LanguageLearning 2002 (COLING 2002 Post-Conference Work-shops), pages 63?69.
Taipei.Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Matsuda,and Asahara.
2000.
Nihongo Keitaiso Kaiseki System:Chasen.
http://chasen.naist.jp/hiki/ChaSen/.Eric Nichols, Francis Bond, and Daniel Flickinger.
2005.
Ro-bust ontology acquisition from machine-readable dictio-naries.
In Proceedings of the International Joint Confer-ence on Artificial Intelligence IJCAI-2005, pages 1111?1116.
Edinburgh.Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizutani.1994.
Iwanami Kokugo Jiten Dai Go Han [IwanamiJapanese Dictionary Edition 5].
Iwanami Shoten, Tokyo.
(in Japanese).Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.2004.
Towards terascale knowledge acquisition.
In 20thInternational Conference on Computational Linguistics:COLING-2004, pages 771?777.
Geneva.Stephen D. Richardson, William B. Dolan, and Lucy Van-derwende.
1998.
MindNet: acquiring and structuring se-mantic information from text.
In 36th Annual Meetingof the Association for Computational Linguistics and 17thInternational Conference on Computational Linguistics:COLING/ACL-98, pages 1098?1102.
Montreal.Kiyoaki Shirai.
2003.
SENSEVAL-2 Japanese dictionarytask.
Journal of Natural Language Processing, 10(3):3?24.
(in Japanese).Melanie Siegel.
2000.
HPSG analysis of Japanese.
InWahlster (2000), pages 265?280.Kathrin Spreyer and Anette Frank.
2005.
The TIGER RMRS700 bank: RMRS construction from dependencies.
In Pro-ceedings of the 6th International Workshop on Linguisti-cally Interpreted Corpora (LINC 2005), pages 1?10.
JejuIsland, Korea.Takenobu Tokunaga, Yasuhiro Syotu, Hozumi Tanaka, andKiyoaki Shirai.
2001.
Integration of heterogeneous lan-guage resources: A monolingual dictionary and a the-saurus.
In Proceedings of the 6th Natural Language Pro-cessing Pacific Rim Symposium, NLPRS2001, pages 135?142.
Tokyo.Wolfgang Wahlster, editor.
2000.
Verbmobil: Foundations ofSpeech-to-Speech Translation.
Springer, Berlin, Germany.Yorick A. Wilkes, Brian M. Slator, and Louise M. Guthrie.1996.
Electric Words.
MIT Press.17
