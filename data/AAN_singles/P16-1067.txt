Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 706?714,Berlin, Germany, August 7-12, 2016. c?2016 Association for Computational LinguisticsUnsupervised Multi-Author Document Decomposition Based onHidden Markov ModelKhaled Aldebei Xiangjian He Wenjing JiaGlobal Big Data Technologies CentreUniversity of Technology SydneyAustralia{Khaled.Aldebei,Xiangjian.He,Wenjing.Jia}@uts.edu.auJie YangLab of Pattern Analysisand Machine IntelligenceShanghai Jiaotong UniversityChinaJieyang@sjtu.edu.cnAbstractThis paper proposes an unsupervisedapproach for segmenting a multi-author document into authorial com-ponents.
The key novelty is thatwe utilize the sequential patterns hid-den among document elements whendetermining their authorships.
Forthis purpose, we adopt Hidden MarkovModel (HMM) and construct a sequen-tial probabilistic model to capture thedependencies of sequential sentencesand their authorships.
An unsuper-vised learning method is developed toinitialize the HMM parameters.
Exper-imental results on benchmark datasetshave demonstrated the significant ben-efit of our idea and our approach hasoutperformed the state-of-the-arts onall tests.
As an example of its applica-tions, the proposed approach is appliedfor attributing authorship of a docu-ment and has also shown promising re-sults.1 IntroductionAuthorship analysis is a process of inspect-ing documents in order to extract autho-rial information about these documents.
Itis considered as a general concept that em-braces several types of authorship subjects, in-cluding authorship verification, plagiarismdetection and author attribution.
Author-ship verification (Brocardo et al, 2013; Pothaand Stamatatos, 2014) decides whether a givendocument is written by a specific author.
Pla-giarism detection (Stein et al, 2011; Keste-mont et al, 2011) seeks to expose the simi-larity between two texts.
However, it is un-able to determine if they are written by thesame author.
In author attribution (Juola,2006; Savoy, 2015), a real author of an anony-mous document is predicted using labeled doc-uments of a set of candidate authors.Another significant subject in author-ship analysis, which has received compara-tively less attention from research commu-nity, is authorship-based document decompo-sition (ABDD).
This subject is to group thesentences of a multi-author document to dif-ferent classes, of which each contains the sen-tences written by only one author.
Many ap-plications can take advantage of such a sub-ject, especially those in forensic investigation,which aim to determine the authorship of sen-tences in a multi-author document.
Further-more, this kind of subject is beneficial for de-tecting plagiarism in a document and definingcontributions of authors in a multi-author doc-ument for commercial purpose.
ABDD canalso be applied to identify which source (re-garded as an ?author?
in this paper) a partof a document is copied from when the doc-ument is formed by taking contents from var-ious sources.In despite of the benefits of ABDD, therehas been little research reported on this sub-ject.
Koppel et al (2011) are the first re-searchers who implemented an unsupervisedapproach for ABDD.
However, their approachis restricted to Hebrew documents only.
Theauthors of Akiva and Koppel (2013) addressedthe drawbacks of the above approach byproposing a generic unsupervised approach forABDD.
Their approach utilized distance mea-surements to increase the precision and accu-racy of clustering and classification phases, re-spectively.
The accuracy of their approachis highly dependent on the number of au-706thors.
When the number of authors increases,the accuracy of the approach is significantlydropped.
Giannella (2015) presented an im-proved approach for ABDD when the numberof authors of the document is known or un-known.
In his approach, a Bayesian segmenta-tion algorithm is applied, which is followed bya segment clustering algorithm.
However, theauthor tested his approach by using only doc-uments with a few transitions among authors.Furthermore, the accuracy of the approach isvery sensitive to the setting of its parameters.In Aldebei et al (2015), the authors presentedan unsupervised approach ABDD by exploit-ing the differences in the posterior probabili-ties of a Naive-Bayesian model in order to in-crease the precision and the classification ac-curacy, and to be less dependent on the num-ber of authors in comparing with the approachin Akiva and Koppel (2013).
Their work wastested on documents with up to 400 transi-tions among authors and the accuracy of theirapproach was not sensitive to the setting ofparameters, in contrast with the approach inGiannella (2015).
However, the performanceof their approach greatly depends on a thresh-old, of which the optimal value for an individ-ual document is not easy to find.Some other works have focused on segment-ing a document into components according totheir topics.
For applications where the top-ics of documents are unavailable, these topic-based solutions will fail.
In this paper, theABDD approach is independent of documents?topics.All of the existing works have assumed thatthe observations (i.e., sentences) are indepen-dent and identically distributed (i.i.d.).
Noconsideration has been given to the contextualinformation between the observations.
How-ever, in some cases, the i.i.d.
assumption isdeemed as a poor one (Rogovschi et al, 2010).In this paper, we will relax this assumptionand consider sentences of a document as a se-quence of observations.
We make use of thecontextual information hidden between sen-tences in order to identify the authorship ofeach sentence in a document.
In other words,the authorships of the ?previous?
and ?subse-quent?
sentences have relationships with theauthorship of the current sentence.
There-fore, in this paper, a well-known sequentialmodel, Hidden Markov Model (HMM), is usedfor modelling the sequential patterns of thedocument in order to describe the authorshiprelationships.The contributions of this article are summa-rized as follows.1.
We capture the dependencies betweenconsecutive elements in a document to iden-tify different authorial components and con-struct an HMM for classification.
It is forthe first time the sequential patterns hiddenamong document elements is considered forsuch a problem.2.
To build and learn the HMM model, anunsupervised learning method is first proposedto estimate its initial parameters, and it doesnot require any information of authors or doc-ument?s context other than how many authorshave contributed to write the document.3.
Different from the approach in Aldebeiet al (2015), the proposed unsupervised ap-proach no longer relies on any predeterminedthreshold for ABDD.4.
Comprehensive experiments are con-ducted to demonstrate the superior perfor-mance of our ideas on both widely-used ar-tificial benchmark datasets and an authenticscientific document.
As an example of its ap-plications, the proposed approach is also ap-plied for attributing authorship on a populardataset.
The proposed approach can not onlycorrectly determine the author of a disputeddocument but also provide a way for measur-ing the confidence level of the authorship de-cision for the first time.The rest of this article is organised as fol-lows.
Section 2 reviews the HMM.
Section 3presents the details of our proposed approach,including the processes for initialization andlearning of HMM parameters, and the Viterbidecoding process for classification.
Experi-ments are conducted in Section 4, followed bythe conclusion in Section 5.2 Overview of HMMIn this paper, we adopt the widely used se-quential model, the Hidden Markov Model(HMM) (Eddy, 1996), to classify sentences ofa multi-author document according to theirauthorship.
The HMM is a probabilistic707model which describes the statistical depen-dency between a sequence of observationsO = {o1, o2, ?
?
?
, oT } and a sequence of hid-den states Q = {q1, q2, ?
?
?
, qT }.
The obser-vations can either be discrete variables, whereeach oi takes a value from a set of M sym-bols W = {w1, ?
?
?
, wM }, or be continuousvariables.
On the other hand, each qi takesone possible value from a set of N symbols,S = {s1, ?
?
?
, sN }.The behaviour of the HMM can be deter-mined by three parameters shown as follows.1.
Initial state probabilities pi = {pi1, ?
?
?
, piN},where pin = p(q1 = sn) and sn ?
S, forn = 1, 2, ?
?
?
, N .2.
Emission probabilities B, where each emis-sion probability bn(ot) = p(ot|qt = sn), fort = 1, 2, ?
?
?
, T and n = 1, 2, ?
?
?
, N .3.
State transition probabilities A.
It is as-sumed that the state transition probabil-ity has a time-homogeneous property, i.e.,it is independent of the time t. Therefore,a probability p(qt = sl|qt?1 = sn) can berepresented as anl, for t = 1, 2, ?
?
?
, T andl, n = 1, 2, ?
?
?
, N .3 The Proposed ApproachThe ABDD proposed in this paper can be for-mulated as follows.
Given a multi-author doc-ument C, written by N co-authors, it is as-sumed that each sentence in the document iswritten by one of the N co-authors.
Further-more, each co-author has written long succes-sive sequences of sentences in the document.The number of authors N is known before-hand, while typically no information about thedocument contexts and co-authors is available.Our objective is to define the sentences of thedocument that are written by each co-author.Our approach consists of three steps shownas follows.1.
Estimate the initial values of the HMMparameters {pi , B, A} with a novel unsuper-vised learning method.2.
Learn the values of the HMM parametersusing the Baum ?
Welch algorithm (Baum,1972; Bilmes and others, 1998).3.
Apply the V iterbi algorithm (Forney Jr,1973) to find the most likely authorship of eachsentence.3.1 InitializationIn our approach, we assume that we do notknow anything about the document C and theauthors, except the number of co-authors ofthe document (i.e., N).
This approach appliesan HMM in order to classify each sentencein document C into a class corresponding toits co-author.
The step (see Sub-section 3.2)for learning of HMM parameters {pi , B, A} isheavily dependent on the initial values of theseparameters (Wu, 1983; Xu and Jordan, 1996;Huda et al, 2006).
Therefore, a good initialestimation of the HMM parameters can helpachieve a higher classification accuracy.We take advantage of the sequential infor-mation of data and propose an unsupervisedapproach to estimate the initial values of theHMM parameters.
The detailed steps of thisapproach are shown as follows.1.
The document C is divided into seg-ments.
Each segment has 30 successive sen-tences, where the ith segment comprises theith 30 successive sentences of the document.This will produce s segments, where s =Ceiling(|C|/30) with |C| representing the to-tal number of sentences in the document.
Thenumber of sentences in each segment (i.e., 30)is chosen in such a way that each segment islong enough for representing a particular au-thor?s writing style, and also the division of thedocument gives an adequate number of seg-ments in order to be used later for estimatingthe initial values of HMM parameters.2.
We select the words appearing in the doc-ument for more than two times.
This producesa set of D words.
For each segment, create aD-dimensional vector where the ith element inthe vector is one (zero) if the ith element in theselected word set does (not) appear in the seg-ment.
Therefore, s binary D-dimensional vec-tors are generated, and the set of these vectorsis denoted by X = {x1, ?
?
?
, xs}.3.
A multivariate Gaussian Mixture Models(GMMs) (McLachlan and Peel, 2004) is usedto cluster the D-dimensional vectors X into Ncomponents denoted by {s1, s2, ?
?
?
, sN}.
Notethat the number of components is equal to thenumber of co-authors of the document.
Basedon the GMMs, each vector, xi, gets a labelrepresenting the Gaussian component that thisvector xi is assigned to, for i = 1, 2, ?
?
?
, s.7084.
Again, we represent each segment asa binary vector using a new feature set con-taining all words appearing in the documentfor at least once.
Assuming the number ofelements in the new feature set is D?, s bi-nary D?-dimensional vectors are generated,and the set of these vectors is denoted byX ?
= {x?1, ?
?
?
, x?s}.
Each vector x?i will havethe same label of vector xi, for i = 1, 2, ?
?
?
, s.5.
We construct a Hidden Markov modelwith a sequence of observations O?
and its cor-responding sequence of hidden states Q?.
Inthis model, O?
represents the resulted segmentvectors X ?
of the previous step.
Formally, ob-servation o?i, is the ithbinary D?-dimensionalvector x?i, that represents the ithsegment ofdocument C. In contrast, Q?
represents thecorresponding authors of the observation se-quence O?.
Each q?i symbolizes the most likelyauthor of observation o?i.
According to Steps 3and 4 of this sub-section, each x?i represent-ing o?i takes one label from a set of N ele-ments, and the label represents its state, fori = 1, 2, ?
?
?
, s.By assigning the most likely states to all hid-den states (i.e., q?i, i = 1, 2, ?
?
?
, s), the statetransition probabilities A are estimated.As long as there is only one sequence ofstates in our model, the initial probability ofeach state is defined as the fraction of timesthat the state appears in the sequence Q?, sopin =Count(q?=sn)Count(q?)
, for n = 1, 2, ?
?
?
, N .6.
Given the sequence X ?, and the set of allpossible values of labels, the conditional prob-ability of feature fk in X ?
given a label sn,p(fk|sn), is computed, for k = 1, 2, ?
?
?
, D?
andn = 1, 2, ?
?
?
, N .7.
The document C is partitioned into sen-tences.
Let z = |C| represent the number ofsentences in the document.
We represent eachsentence as a binary feature vector using thesame feature set used in Step 4.
Therefore,z binary D?-dimensional vectors, denoted byO = {o1, ?
?
?
, oz}, are generated.
By using theconditional probabilities resulted in Step 6, theinitial values of B are computed as p(oi|sn)=?D?k=1 ofki p(fk|sn), where ofki represents thevalue of feature fk in sentence vector oi, fori = 1, 2, ?
?
?
, z and n = 1, 2, ?
?
?
, N .In this approach, we use add-one smooth-ing (Martin and Jurafsky, 2000) for avoidingzero probabilities of A and B. Furthermore,we take the logarithm function of the proba-bility in order to simplify its calculations.The initial values of the A, B and pi arenow available.
In next sub-section, the learn-ing process of these parameter values is per-formed.3.2 Learning HMMAfter estimating the initial values for the pa-rameters of HMM, we now find the parame-ter values that maximize likelihood of the ob-served data sequence (i.e., sentence sequence).The learning process of the HMM parametervalues is performed as follows.1.
Construct a Hidden Markov model witha sequence of observations, O, and a corre-sponding sequence of hidden states, Q. Inthis model, O represents the resulted sentencevectors (Step 7 in the previous Sub-section).Formally, the observation oi, is the ith binaryD?-dimensional vector and it represents theith sentence of document C. In contrast, Qrepresents the corresponding authors of obser-vation sequence O.
Each qi symbolizes themost likelihood author of observation oi, fori = 1, 2, ?
?
?
, z2.
The Baum-Welch algorithm is applied tolearn the HMM parameter values.
The algo-rithm, also known as the forward?backwardalgorithm (Rabiner, 1989), has two steps, i.e.,E-step and M-step.
The E-step finds the ex-pected author sequence (Q) of the observa-tion sequence (O), and the M-step updates theHMM parameter values according to the stateassignments.
The learning procedure startswith the initial values of HMM parameters,and then the cycle of these two steps contin-ues until a convergence is achieved in pi , B andA.The learned HMM parameter values will beused in the next sub-section in order to findthe best sequence of authors for the given sen-tences.3.3 Viterbi DecodingFor a Hidden Markov model, there are morethan one sequence of states in generating theobservation sequence.
The Viterbi decodingalgorithm (Forney Jr, 1973) is used to deter-mine the best sequence of states for generat-709ing observation sequence.
Therefore, by usingthe Hidden Markov model that is constructedin previous sub-section and the learned HMMparameter values, the Viterbi decoding algo-rithm is applied to find the best sequence ofauthors for the given sentences.4 ExperimentsIn this section, we demonstrate the perfor-mance of our proposed approach by conduct-ing experiments on benchmark datasets as wellas one authentic document.
Furthermore, anapplication on authorship attribution is pre-sented using another popular dataset.4.1 DatasetsThree benchmark corpora widely used for au-thorship analysis are used to evaluate our ap-proach.
Furthermore, an authentic documentis also examined.The first corpus consists of five Biblicalbooks written by Ezekiel, Isaiah, Jeremiah,Proverbs and Job, respectively.
All of thesebooks are written in Hebrew.
The five booksbelong to two types of literature genres.
Thefirst three books are related to prophecy liter-ature and the other two books are related toa wisdom literature.The second corpus consists of blogs writ-ten by the Nobel Prize-winning economistGary S. Becker and the renowned juristand legal scholar Richard A. Posner.
Thiscorpus, which is titled ?The Becker-PosnerBlogs?
(www.becker-posner-blog.com), con-tains 690 blogs.
On average, each blog has 39sentences talking about particular topic.
TheBecker-Posner Blogs dataset, which is consid-ered as a very important dataset for author-ship analysis, provides a good benchmark fortesting the proposed approach in a documentwhere the topics of authors are not distinguish-able.
For more challenging documents, Gian-nella (2015) has manually selected six single-topic documents from Becker-Posner blogs.Each document is a combination of Becker andPosner blogs that are talking about only onetopic.
The six merged documents with theirtopics and number of sentences of each alter-native author are shown in Table 1.The third corpus is a group of New YorkTimes articles of four columnists.
The arti-Topics Author order and number of sentencesper authorTenure (Ten) Posner(73), Becker(36), Posner(33),Becker(19)Senate Filibuster (SF) Posner(39), Becker(36), Posner(28),Becker(24)Tort Reform (TR) Posner(29), Becker(31), Posner(24)Profiling (Pro) Becker(35), Posner(19), Becker(21)Microfinance (Mic) Posner(51), Becker(37), Posner(44),Becker(33)Traffic Congestion (TC) Becker(57), Posner(33), Becker(20)Table 1: The 6 merged single-topic documentsof Becker-Posner blogs.cles are subjected to different topics.
In ourexperiments, all possible multi-author docu-ments of articles of these columnists are cre-ated.
Therefore, this corpus permits us to ex-amine the performance of our approach in doc-uments written by more than two authors.The fourth corpus is a very early draft of ascientific article co-authored by two PhD stu-dents each being assigned a task to write somefull sections of the paper.
We employ this cor-pus in order to evaluate the performance of ourapproach on an authentic document.
For thispurpose, we have disregarded its titles, authornames, references, figures and tables.
Afterthat, we get 313 sentences which are writtenby two authors, where Author 1 has written131 sentences and Author 2 has written 182sentences.4.2 Results on DocumentDecompositionThe performance of the proposed approach isevaluated through a set of comparisons withfour state-of-the-art approaches on the fouraforementioned datasets.The experiments on the first three datasets,excluding the six single-topic documents, areapplied using a set of artificially merged multi-author documents.
These documents are cre-ated by using the same method that has beenused by Aldebei et al (2015).
This methodaims to combine a group of documents of Nauthors into a single merged document.
Eachof these documents is written by only one au-thor.
The merged document process starts byselecting a random author from an author set.Then, the first r successive and unchosen sen-tences from the documents of the selected au-thor are gleaned, and are merged with the firstr successive and unchosen sentences from thedocuments of another randomly selected au-710thor.
This process is repeated till all sentencesof authors?
documents are gleaned.
The valueof r of each transition is selected randomlyfrom a uniform distribution varying from 1to V .
Furthermore, we follow Aldebei et al(2015) method and assign the value of 200 toV .Bible BooksWe utilize the bible books of five authors andcreate artificial documents by merging booksof any two possible authors.
This produces10 multi-author documents of which four havethe same type of literature and six have differ-ent type of literature.
Table 2 shows the com-parisons of classification accuracies of these 10documents by using our approach and the ap-proaches developed by Koppel et al (2011),Akiva and Koppel (2013)-500CommonWords,Akiva and Koppel (2013)-SynonymSet andAldebei et al (2015).Doc.
1 2 3 4 5DifferentEze-Job 85.8% 98.9% 95.0% 99.0%99.4%Eze-Prov 77.0% 99.0% 91.0% 98.0% 98.8%Isa-Prov 71.0% 95.0% 85.0% 98.0% 98.7%Isa-Job 83.0% 98.8% 89.0% 99.0% 99.4%Jer-Job 87.2% 98.2% 93.0% 98.0% 98.5%Jer-Prov 72.2% 97.0% 75.0% 99.0% 99.5%Overall 79.4% 97.8% 88.0% 98.5% 99.1%SameJob-Prov 85.0% 94.0% 82.0% 95.0% 98.2%Isa-Jer 72.0% 66.9% 82.9% 71.0% 72.1%Isa-Eze 79.0% 80.0% 88.0% 83.0% 83.2%Jer-Eze 82.0% 97.0% 96.0% 97.0% 97.3%Overall 79.5% 84.5% 87.2% 86.5% 87.7%Table 2: Classification accuracies of mergeddocuments of different literature or the sameliterature bible books using the approaches of1- Koppel et al (2011), 2- Akiva and Kop-pel (2013)-500CommonWords, 3- Akiva andKoppel (2013)-SynonymSet, 4- Aldebei et al(2015) and 5- our approach.As shown in Table 2, the results of our ap-proach are very promising.
The overall clas-sification accuracies of documents of the sameliterature or different literature are better thanthe other four state-of-the-art approaches.In our approach, we have proposed an un-supervised method to estimate the initial val-ues of the HMM parameters (i.e., pi , B andA) using segments.
Actually, the initial valuesof the HMM parameters are sensitive factorsto the convergence and accuracy of the learn-ing process.
Most of the previous works usingHMM have estimated these values by cluster-ing the original data, i.e., they have clusteredsentences rather than segments.
Figure 1 com-pares the results of using segments with theresults of using sentences for estimating theinitial parameters of HMM in the proposed ap-proach for the 10 merged Bible documents interms of the accuracy results and number ofiterations till convergence, respectively.
FromFigures 1, one can notice that the accuracyresults obtained by using segments for esti-mating the initial HMM parameters are sig-nificantly higher than using sentences for allmerged documents.
Furthermore, the num-ber of iterations required for convergence foreach merged document using segments is sig-nificantly smaller than using sentences.Figure 1: Comparisons between using seg-ments and using sentences in the unsupervisedmethod for estimating the initial values of theHMM of our approach in terms of accuracy(representd as the cylinders) and number of it-erations required for convergence (representedas the numbers above cylinders) using the 10merged Bible documents.Becker-Posner Blogs (Controlling for Topics)In our experiments, we represent Becker-Posner blogs in two different terms.
Thefirst term is as in Aldebei et al (2015) andAkiva and Koppel (2013) approaches, wherethe whole blogs are exploited to create onemerged document.
The resulted merged docu-ment contains 26,922 sentences and more than240 switches between the two authors.
We ob-tain an accuracy of 96.72% when testing ourapproach in the merged document.
The ob-tained result of such type of document, whichdoes not have topic indications to differentiatebetween authors, is delightful.
The first set ofcylinders labelled ?Becker-Posner?
in Figure 2shows the comparisons of classification accu-racies of our approach and the approaches ofAkiva and Koppel (2013) and Aldebei et al711(2015) when the whole blogs are used to cre-ate one merged document.
As shown in Figure2, our approach yields better classification ac-curacy than the other two approaches.Figure 2: Classification accuracy comparisonsbetween our approach and the approaches pre-sented in Akiva and Koppel (2013) and Alde-bei et al (2015) in Becker-Posner documents,and documents created by three or four NewYork Times columnists (TF = Thomas Fried-man, PK = Paul Krugman, MD = MaureeenDowd, GC = Gail Collins).The second term is as in the approach of Gi-annella (2015), where six merged single-topicdocuments are formed.
Due to comparativelyshorter lengths of these documents, the num-ber of resulted segments that are used forthe unsupervised learning in Sub-section 3.1is clearly not sufficient.
Therefore, instead ofsplitting each document into segments of 30sentences length each, we split it into segmentsof 10 sentences length each.
Figure 3 shows theclassification accuracies of the six documentsusing our approach and the approach pre-sented in Giannella (2015).
It is observed thatour proposed approach has achieved higherclassification accuracy than Giannella (2015)in all of the six documents.Figure 3: Classification accuracy comparisonsbetween our approach and the approach pre-sented in (Giannella, 2015) in the six single-topic documents of Becker-Posner blogs.New York Times Articles (N > 2)We perform our approach on New York Timesarticles.
For this corpus, the experiments canbe classified into three groups.
The first groupis for those merged documents that are createdby combining articles of any pair of the fourauthors.
The six resulted documents have onaverage more than 250 switches between au-thors.
The classification accuracies of thesedocuments are between 93.9% and 96.3%.
Itis notable that the results are very satisfactoryfor all documents.
For comparisons, the classi-fication accuracies of the same documents us-ing the approach presented in Aldebei et al(2015) range from 93.3% to 96.1%.
Further-more, some of these documents have producedan accuracy lower than 89.0% using the ap-proach of Akiva and Koppel (2013).The second group is for those merged doc-uments that are created by combining articlesof any three of the four authors.
The four re-sulted documents have on average more than350 switches among the authors.
The thirdgroup is for the document that are createdby combining articles of all four columnists.The resulted merged document has 46,851 sen-tences and more than 510 switches among au-thors.
Figure 2 shows the accuracies of the fiveresulted documents regarding the experimentsof the last two groups.
Furthermore, it showsthe comparisons of our approach and the ap-proaches presented in Aldebei et al (2015) andAkiva and Koppel (2013).
It is noteworthythat the accuracies of our approach are betterthan the other two approaches in all of the fivedocuments.Authentic DocumentIn order to demonstrate that our proposed ap-proach is applicable on genuine documents aswell, we have applied the approach on firstdraft of a scientific paper written by two Ph.D.students (Author 1 and Author 2) in our re-search group.
Each student was assigned atask to write some full sections of the paper.Author 1 has contributed 41.9% of the doc-ument and Author 2 contributed 58.1%.
Ta-ble 3 shows the number of correctly assignedsentences of each author and the classifica-tion accuracy resulted using the proposed ap-proach.
Table 3 also displays the authors?
con-tributions predicted using our approach.
As712AuthorClassificationAccuracyPredictedContribution1 98.5% 47.6%2 89.0% 52.4%Accuracy 93.0%Table 3: The classification accuracies and pre-dicted contributions of the two authors of thescientific paper using the proposed approach.shown in Table 3, the proposed approach hasachieved an overall accuracy of 93.0% for theauthentic document.4.3 Results on Authorship AttributionOne of the applications that can take advan-tage of the proposed approach is the author-ship attribution (i.e., determining a real au-thor of an anonymous document given a set oflabeled documents of candidate authors).
TheFederalist Papers dataset have been employedin order to examine the performance of ourapproach for this application.
This dataset isconsidered as a benchmark in authorship attri-bution task and has been used in many studiesrelated to this task (Juola, 2006; Savoy, 2013;Savoy, 2015).
The Federalist Papers consistof 85 articles published anonymously between1787 and 1788 by Alexander Hamilton, JamesMadison and John Jay to persuade the citizensof the State of New York to ratify the Con-stitution.
Of the 85 articles, 51 of them werewritten by Hamilton, 14 were written by Madi-son and 5 were written by Jay.
Furthermore,3 more articles were written jointly by Hamil-ton and Madison.
The other 12 articles (i.e.,articles 49-58 and 62-63), the famous ?anony-mous articles?, have been alleged to be writtenby Hamilton or Madison.To predict a real author of the 12 anony-mous articles, we use the first five undisputedarticles of both authors, Hamilton and Madi-son.
Note that we ignore the articles of Jay be-cause the anonymous articles are alleged to bewritten by Hamilton or Madison.
The five ar-ticles of Hamilton (articles 1 and 6-9) are com-bined with the five articles of Madison (articles10, 14 and 37-39) in a single merged documentwhere all the articles of Hamilton are insertedinto the first part of the merged document andall the articles of Madison are inserted intothe second part of the merged document.
Themerged document has 10 undisputed articlescovering eight different topics (i.e., each au-thor has four different topics).
Before applyingthe authorship attribution on the 12 anony-mous articles, we have tested our approach onthe resulted merged document and an accu-racy of 95.2% is achieved in this document.Note that, the authorial components in thisdocument are not thematically notable.For authorship attribution of the 12 anony-mous articles, we add one anonymous articleeach time on the middle of the merged docu-ment, i.e., between Hamilton articles part andMadison articles part.
Then, we apply our ap-proach on the resulted document, which has11 articles, to determine to which part the sen-tences of the anonymous article are classifiedto be sectences of Hamilton or Madison.
Asthe ground truth for our experiments, all ofthese 12 articles can be deemed to have beenwritten by Madison becuase the results of allrecent state-of-the-art studies testing on thesearticles on authorship attribution have clas-sified the articles to Madison?s.
Consistentwith the state-of-the-art approaches, these 12anonymous articles are also correctly classifiedto be Madison?s using the proposed approach.Actually, all sentences of articles 50,52-58 and62-63 are classified as Madison?s sentences,and 81% of the sentences of article 49 and 80%of article 51 are classified as Madison?s sen-tences.
These percentages can be deemed asthe confidence levels (i.e., 80% conferdence forarticles 49, 81% for 51, and 100% confidencesfor all other articles) in making our conclusionof the authorship contributions.5 ConclusionsWe have developed an unsupervised approachfor decomposing a multi-author documentbased on authorship.
Different from the state-of-the-art approaches, we have innovativelymade use of the sequential information hid-den among document elements.
For this pur-pose, we have used HMM and constructed asequential probabilistic model, which is usedto find the best sequence of authors that repre-sents the sentences of the document.
An unsu-pervised learning method has also been devel-oped to estimate the initial parameter valuesof HMM.
Comparative experiments conductedon benchmark datasets have demonstrated theeffectiveness of our ideas with superior perfor-713mance achieved on both artificial and authen-tic documents.
An application of the proposedapproach on authorship attribution has alsoachieved perfect results of 100% accuracies to-gether with confidence measurement for thefirst time.References[Akiva and Koppel2013] Navot Akiva and MosheKoppel.
2013.
A generic unsupervised methodfor decomposing multi-author documents.
Jour-nal of the American Society for Information Sci-ence and Technology, 64(11):2256?2264.
[Aldebei et al2015] Khaled Aldebei, Xiangjian He,and Jie Yang.
2015.
Unsupervised decomposi-tion of a multi-author document based on naive-bayesian model.
ACL, Volume 2: Short Papers,page 501.
[Baum1972] Leonard E Baum.
1972.
An equalityand associated maximization technique in sta-tistical estimation for probabilistic functions ofmarkov processes.
Inequalities, 3:1?8.
[Bilmes and others1998] Jeff A Bilmes et al 1998.A gentle tutorial of the em algorithm and its ap-plication to parameter estimation for gaussianmixture and hidden markov models.
Interna-tional Computer Science Institute, 4(510):126.
[Brocardo et al2013] Marcelo Luiz Brocardo, IssaTraore, Shatina Saad, and Isaac Woungang.2013.
Authorship verification for short messagesusing stylometry.
In Computer, Informationand Telecommunication Systems (CITS), 2013International Conference on, pages 1?6.
IEEE.
[Eddy1996] Sean R Eddy.
1996.
Hidden markovmodels.
Current opinion in structural biology,6(3):361?365.
[Forney Jr1973] G David Forney Jr. 1973.
Theviterbi algorithm.
Proceedings of the IEEE,61(3):268?278.
[Giannella2015] Chris Giannella.
2015.
An im-proved algorithm for unsupervised decomposi-tion of a multi-author document.
Journal of theAssociation for Information Science and Tech-nology.
[Huda et al2006] Md Shamsul Huda, RanadhirGhosh, and John Yearwood.
2006.
A variableinitialization approach to the em algorithm forbetter estimation of the parameters of hiddenmarkov model based acoustic modeling of speechsignals.
In Advances in Data Mining.
Applica-tions in Medicine, Web Mining, Marketing, Im-age and Signal Mining, pages 416?430.
Springer.
[Juola2006] Patrick Juola.
2006.
Authorship attri-bution.
Foundations and Trends in informationRetrieval, 1(3):233?334.
[Kestemont et al2011] Mike Kestemont, Kim Luy-ckx, and Walter Daelemans.
2011.
Intrinsic pla-giarism detection using character trigram dis-tance scores.
Proceedings of the PAN.
[Koppel et al2011] Moshe Koppel, Navot Akiva,Idan Dershowitz, and Nachum Dershowitz.2011.
Unsupervised decomposition of a docu-ment into authorial components.
In Proceed-ings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 1356?1364.
Association for Computational Linguis-tics.
[Martin and Jurafsky2000] James H Martin andDaniel Jurafsky.
2000.
Speech and languageprocessing.
International Edition.
[McLachlan and Peel2004] Geoffrey McLachlanand David Peel.
2004.
Finite mixture models.John Wiley & Sons.
[Potha and Stamatatos2014] Nektaria Potha andEfstathios Stamatatos.
2014.
A profile-basedmethod for authorship verification.
In ArtificialIntelligence: Methods and Applications, pages313?326.
Springer.
[Rabiner1989] Lawrence R Rabiner.
1989.
A tuto-rial on hidden markov models and selected ap-plications in speech recognition.
Proceedings ofthe IEEE, 77(2):257?286.
[Rogovschi et al2010] Nicoleta Rogovschi,Mustapha Lebbah, and Younes Bennani.2010.
Learning self-organizing mixture markovmodels.
Journal of Nonlinear Systems andApplications, 1:63?71.
[Savoy2013] Jacques Savoy.
2013.
The federal-ist papers revisited: A collaborative attributionscheme.
Proceedings of the American Society forInformation Science and Technology, 50(1):1?8.
[Savoy2015] Jacques Savoy.
2015.
Estimating theprobability of an authorship attribution.
Jour-nal of the Association for Information Scienceand Technology.
[Stein et al2011] Benno Stein, Nedim Lipka, andPeter Prettenhofer.
2011.
Intrinsic plagiarismanalysis.
Language Resources and Evaluation,45(1):63?82.
[Wu1983] CF Jeff Wu.
1983.
On the convergenceproperties of the em algorithm.
The Annals ofstatistics, pages 95?103.
[Xu and Jordan1996] Lei Xu and Michael I Jordan.1996.
On convergence properties of the em al-gorithm for gaussian mixtures.
Neural compu-tation, 8(1):129?151.714
