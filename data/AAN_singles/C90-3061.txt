Modelling Variations in Goal-Directed DialogueJ ean  Car let ta*Un ivers i ty  of Ed inburghDepa.
r tment  of Art i f ic ial  Inte l l igencej cc~a ipna .ed .ac .uk1 In t roduct ionThis research investigates human dialogue variations byhaving simulated agents converse about a simple mapnavigation task using a computational theory of humandialogue strategies.
The theory to be tested is thatagents have a variety of strategies which they use ingoM-directed ialogue for deciding what information toinclude, how to present it, how to construct references,and how to handle interactions.
Agents choose strate-gies for each of these a.speets of the dialogue depend-ing on the complexity of the current task, responsesfrom the partner, and the task's importance; in general,agents try to minimize the collaborative effort spent onthe task but still complete it satisfactorily.
The soft-ware allows simulated agents to be constructed usingany combination of strategies, howing how the strate-gies interact and allowing the decisions made in humandialogues about the same task to be modelled.
Cur-rently, the system works on a subset of the strategiesfound in Shadbolt \[6\], but a corpus of human dialoguesis being studied to construct, an improved theory of di-alogue strategies, and these strategies will be incorpo-rated into later versions of the system.2 The  D ia logue  DomainThe task domain \['or the dialogues involves navigatingaround a simple map containing approximately fifteenlandmarks.
Two participants are given maps which dif-fer slightly; each map may contain some features omit-ted on the other, and features in the same location onthe two maps may have different labels.
The first par-ticipant also has a route from the labelled start point toone map feature.
The second participant must draw the* Supported by a scholarship from the Marshall Aid Conunem-oration Commission.
Thanks to Chris Mellish for supervising meand Alison Cawsey for helpful comments,route on his or her map.
In this task, the participantsmust cooperate because neither of them knows enoughabout the other's map to be able to construct accuratedescriptions.
At the same time, small changes in themap test how participants hrmdle referential ambigui-ties, how information is carried from one part of thedialogue to the next, and when agents decide to replanrather than repair an existing plan.
Despite the possi-bilities for referential difficulties, this task minimizes theuse of real world knowledge as long as all participantsunderstand how to navigate.
The task is simple enoughto be completed in computer-simulated dialogue, butadmits the dialogue variations to be tested in the re-search.3 The  Cent ra l  IdeaThe central idea behind the research is that agents needmultiple strategies for engaging in goM-directed ia-logue because they do not necessarily know the bestway to communicate with a given partner.
Self \[5\] showsthat dialogue is crucial where neither agent has all of therelevant domain knowledge.
Dialogue is also necessaryfor any explanations where agents don't have accuratemodels of their partners \[3\].
Even if agents have allof the relevant domain knowledge, they may not knowhow best to present that knowledge, especially sinceexplanations are about exactly that part of the taskwhich is not mutually known to the dialogue partners\[1\].
Shadbolt \[6\] presents evidence that humans han-dle uncertainties about what information to give andhow to present it by having a set of strategies for eachaspect of the dialogue.
Then the agent can tailor ex-planations to a particular partner by using the strategythat best fits the situation.
For instance, human agentswho believe that much domain information will have tobe communicated structure their presentation carefullyand often elicit feedback from the partner, like partici-pant A of Sha.dbolt's \[6\] example 6.16:A: have you got wee palm trees aye?B: uhuA: right go just + a wee bit along to them haveyou got a swamp?n~ erA: right well just go + have you got a watt>fall7Al,~ents who believe that most domain in\[brmation iski~own to their partner are more likely to rely on inter-ru ptions fl'om the partner and replanning, a.s in example6.
:i 1:and "around".
The agents converse in an artificial lan-guage resembling their shared planning language, butsubstituting referring expressions for internal featureidentifiers.
Under these constraints, the agents use di-alogue strategies to decide on the content and form ofthe dialogue.
The existing system is a prototype de-.signed to show that incorporating such strategies canexplain some variations in human dialogue and makeagents more flexible.
An improved set of strategies isbeing extracted from the corpus of human dialogues.The end result of the project will be a theory of howcommunicative strategies control variations in dialogue,and software in which computer-simulated agents usethese strategies to complete the navigation task.A: and then q- go up about and over the bridgeB: I've not got a bridge I've got a lion's denand a woodA: have you got a river'?Ol~e way to make computer generated explanations lookm(,re natural is to plan them using strategies modelledon ~.he human ones.
Although strategies like these couldbe built into the way a system plans an explanation,making strategy choices explicit allows the strategiesthemselves to be investigated, providiug a way to testoul.
how variatio,~s affect the ensuing dialogue.
Thego~d of the present research is both to show how us-ing dialogue strategies can improve tile "naturalness"of computer-generated task explanations and to pro-vide insight into the dialogue strategies which humansuse and how they interact.4 The ProjectThe project involves creating a theory of human dia-logist strategies a.m:l modelling it.
usiug two cotnputerprocesses that converse.
Communication for the com-i)llter agents, bg~sed or~ the model in Power \[4\] andI\[oughtou \[2\], is simplified in a number of ways.
Aconvener wakes the agents in turn and interactions aremade by placing messages in mailboxes, leaving out thecomplications of turn-taking and interruptiom Ratherthan reason from "visual" images of the maps, agentsbegin with sets of beliefs about the positional relation-ship.~ among objects and share knowledge about bothdialogue conventions, expressed a.s interactio'n flames\[2\], ~md navigational concel)tS like "toward", "between",5 The ProgramThe current version of the software uses dialogue strate-gies adapted from Shadbolt \[6\].
tte lists seven dif~fercnt aspects of dialogue along which strategies maybe developed.
Agents may vary strategies tbr feedback(how they handle the partner's utterances), speciJicaolion (how they construct and resolve referring expres-sions), o~lology (how they decide from what featuresare available how to construct route descriptions), foe~zs(the amount of explMt focus intbrmation given), differ-once (the effort spent determining what the partner'sutterances mean), decenlering (whether intbrmation ispresented using the agent's or the partner's names tbrfe.atures), and hypolhesi~ formalion (the effort spentmaking hypotheses about the partner's knowledge).Agents choose strategies tbr each of these aspects de-pending on how explicit they want to be, which in turndepends on how likely the partner is to misunderstandeach aspect of the dialogue.
Some of Shadbolt's as-pects are interrelated; for instance, agents that provideexplicit information about the current focus do not needto construct referring expressions as carefully as agentswho provide no focusing information at all.
Our ownwork divides the strategies lightly differently so thatthey ea.n be divided into sets depending on whetherthey atfect planning the dialogue interaction, planningthe content, planning the presentation, or realizing ref-erences; the goal is to make the strategies ms modularas possible so that they can be modelled simply.
Eachsimulated agent takes on a set of strategies for tile dura-tion of' a dialogue.
Currently, the prototype varies howmuch intbrmation about tile structttre of the dialogue isexplicitly given, which features are included in a routedescription depending on a model of the partner's be-325liefs, how often an agent allows interactions from thepartner, and how much repair an agent is willing todo rather than replan a description.
The agents alsouse heuristics to prefer plans where the partner alreadyunderstands the plan's prerequisites.
The output of theprogram is a simulated ialogue where each agent keepsthe same strategies for the course of the dialogue; anobvious future step is to allow agents to adapt to aparticular partner or part of the task by varying theirstrategies within a dialogue.6 ExamplesThe system currently has several strategies which affecthow much structuring information is given in a dialogueand how often feedback is elicited from the partner.
Thefollowing dialogue, an English gloss of two simulatedagents conversing, shows how agent A might act if itbelieved that the maps had many differences:A: I'nl going to tell you how to get to theburied treasure.
I 'm going to tell you howto navigate the first part of the route.
Doyou have a pahn beach?B: Yes.A: Do you have a swamp?B: No.A: Do you have a waterfall?B: Yes.A: The swamp is between the pMm beach andthe waterfall.
OK?B: Yes.A: The route goes to the left of the pMm beachand around the swamp.
OK?B: Yes.If agent A believes that there will be few misunderstand-ings, or that B will understand enough to say what itmisunderstood, it might choose to give information firstand repair afterwards:7 Conc lus ionThe theory and program are designed to show how varbations that occur in human dialogue can be ex.plained interms of deciding among communicative strategies gov-erning the form of interaction, the content and presenta-tion of information, and the construction of referring ex-pressions.
The strategies found by examining a humancorpus of goM-directed ialogues are implemented in adialogue system where two computer processes using anartificial language and simplified turn-taking completethe task.
This approach is useful in itself for determin-ing what makes human dialogues eem natural} but italso has implications for human-computer interaction,since it is one step towards making computer dialogueswith humans operate flexibly to fit in with human ex-pectations.References\[1\]\[3\]\[4\]\[5\]\[6\]S. Garrod.
Explanations in dialogues as attempts toovercome problems in coordination.
In Proceedingsof the Third Alvey Explanation Workshop, Septem-ber 1987.G.
IIoughton.
The Production of Language in Di-alogue: A Computational Model.
PM) thesis, Uni-versity of Sussex, April 1986.J.
Moore and W. Swartout.
A reactive approach toexplanation.
In Proceedings of the I~'ouvth Interna-tional Workshop on Nat~tral Language Generation,1988.1L J. D. Power.
A Computer Model of Conversation.PhD thesis, University of Edinburgh, 1974.J.
Self.
Bypassing the intractable problem of stu-dent modelling.
In Intelligent Tutoring Systems,1988.N.
1{.
Shadbolt.
Constit~tting reference in naluraIlanguage: the problem of referential opacity.
PhDthesis, Edinburgh University, 1.984.A: The route goes to the left of the pahn beachand around the swamp.
OK?B: Where's the swamp?A: The swamp is between the pMm beach andthe waterfall.
OK?B: Yes.326
