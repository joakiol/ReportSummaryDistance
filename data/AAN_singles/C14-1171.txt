Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1813?1823, Dublin, Ireland, August 23-29 2014.Quality Estimation for Automatic Speech RecognitionMatteo Negri(1)Marco Turchi(1)Jos?e G. C. de Souza(1,2)Daniele Falavigna(1)(1)FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy(2)University of Trento, Italy{negri,turchi,desouza,falavi}@fbk.euAbstractWe address the problem of estimating the quality of Automatic Speech Recognition (ASR) out-put at utterance level, without recourse to manual reference transcriptions and when informationabout system?s confidence is not accessible.
Given a source signal and its automatic transcription,we approach this problem as a regression task where the word error rate of the transcribed utter-ance has to be predicted.
To this aim, we explore the contribution of different feature sets andthe potential of different algorithms in testing conditions of increasing complexity.
Results showthat our automatic quality estimates closely approximate the word error rate scores calculatedover reference transcripts, outperforming a strong baseline in all the testing conditions.1 IntroductionIn recent years, the increasing usage of large vocabulary continuous speech recognition (LVCSR) systemsto transcribe audio recordings from different sources (e.g.
Youtube videos, TV programs, DVD movies,meetings, etc) has sparked the need of accurate, fast and cost-effective methods to estimate the qualityof ASR output.
This need contrasts with the fact that, after decades of progress in ASR research, theestablished evaluation protocol is based on computing word error rate scores (WER)1over large testsets of hand-crafted reference transcriptions.
Indeed, despite its reliability, reference-based performanceassessment has an evident drawback represented by the cost of acquiring manual transcripts.
Besidesincreasing the cost-effectiveness of ASR evaluation routines, bypassing this bottleneck has several othermotivations.
From an application perspective, for instance, reference-free quality estimation methodscould be used to: i) decide at run-time whether a given input signal has been properly recognized (e.g.if a user spoken utterance needs to be repeated in a dialogue application), ii) decide if an automatictranscription is acceptable as is (e.g.
if manual revision is needed in an automatic subtitling application),or iii) select the best transcription among options from multiple ASR systems.When information about the inner workings of the system used to produce the transcriptions is acces-sible, current reference-free confidence estimation methods can supply ASR applications with reliableindicators about output reliability.
This condition, however, does not always hold in the aforementionedscenarios.
A clear motivating example is provided by the exponential growth of captioned TED Talksand Youtube videos,2for which no information is available about how transcriptions have been pro-duced.
In this case, neither reference-based methods, nor standard confidence measures can be appliedto obtain useful quality estimates.
Nevertheless, in this scenario, supplying reliable indicators of tran-scription quality has a huge market potential (e.g.
to reduce the costs of manual revision/translation)which motivates our research.Focusing on these compelling needs, this paper investigates the automatic prediction of ASR out-put quality when: i) manual reference transcripts are not available and ii) information about theThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1The word error rate is the minimum edit distance between an hypothesis and the reference transcription.
Edit distance iscalculated as the number of edits (word insertions, deletions, substitutions) divided by the number of words in the reference.2Since 2009, Youtube videos in English can be automatically captioned.
In 2012, for the 72 hours of video uploaded perminute, such functionality was already available for 10 languages.
Currently, more than 200 million Youtube videos have eitherautomatic or human-created captions (source: http://goo.gl/9swYSS).1813inner workings of the ASR system is not accessible.
Casting the problem as a supervised regressiontask, we experiment in a range of testing conditions on a well-known LVCSR setting (i.e.
the automatictranscription of TED talks).
In this framework, we analyse the performance of various models (i.e.
theircapability to predict utterance-level WER scores) as a function of the different learning algorithms used,the proposed features, and the amount of training data available.Our features are categorized according to the type of information they aim to capture.
Since the na-ture of the proposed features is a relevant aspect for the applicability of our approach, an importantdistinction is made between ?glass-box?
and ?black-box?
features, which are respectively informed andagnostic about systems?
internal decoding strategies.
The former can play an important role when all theintermediate processing steps are accessible (e.g.
in the selection of the best possible transcription hy-pothesis).
In contrast, black-box features have a wider applicability to situations where such informationis not available (e.g.
to estimate the quality of online video subtitles).Another important aspect relevant to our study is the relation between the accuracy of utterance-levelquality predictions and the degree of homogeneity of training and test data.
Indeed, as in any supervisedlearning framework, the similarity between training and test data has a direct impact on (classificationand regression) results.
In order to fully understand the potential of our approach, we hence measureperformance variations under different levels of similarity between the data used to train the regressorand the data used for evaluation.
To this aim, our experiments account for a range of possible conditions.These vary from the situation in which training and test are fully homogeneous (i.e.
same dataset, withtraining instances produced by the same ASR system) to the more challenging situation where trainingand test are not homogeneous (i.e.
different datasets, with training instances produced by different ASRsystems).
Our results, obtained with two different state-of-the-art algorithms for regression, demonstratethat in all such variable conditions our ASR quality estimation models lead to accurate predictions (i.e.close the word error rate scores calculated over reference transcripts).To the best of our knowledge, this paper represents the first extensive investigation on reference-free and system-agnostic automatic estimation of ASR output quality.
Along this direction, our maincontributions can be summarized as follows:1.
We propose a supervised, application-oriented approach to ASR quality estimation that bypassesthe need of manual reference transcriptions and is system-independent.2.
We evaluate our method with different learning algorithms and in different conditions, showing thatits estimates closely approximate the WER scores calculated over reference transcripts.3.
We perform feature analysis, isolating the contribution of each feature set in all the testing condi-tions.4.
We analyse the learning curves of our best models, investigating the relation between performanceresults and the amount of data needed for training.Overall, these contributions provide useful insights about the feasibility of automatic ASR quality esti-mation, opening interesting research avenues relevant for system development and for ASR applications.2 Related WorkAs a reference-free automatic evaluation method, our work introduces a valid application-oriented alter-native to the standard evaluation protocols used within current ASR evaluation campaigns such as IWSLT(Federico et al., 2011; Federico et al., 2012; Cettolo et al., 2013).3Besides that, our approach to ASRquality estimation (QE) also differs from the well-established confidence estimation (CE) techniquesproposed in previous ASR literature (Sukkar and Lee, 1996; Evermann and Woodland, 2000; Wessel etal., 2001; Sanchis et al., 2012; Seigel, 2013, inter alia).
Such difference firstly relies in the fact that,while in CE is the system itself that provides an indicator of the reliability of its output transcriptions,QE aims to provide an external and more objective measure of goodness through WER predictions.
A3See http://www.iwslt2013.org/ for details about the last edition of the IWSLT Workshop held in 2013.1814second (related) difference is that, in contrast with previous CE methods that heavily rely on informationabout the internal behaviour of the ASR system, our technique does not necessarily depend on the accessto such information.
This extends its applicability to scenarios (out of the scope of CE research) wherethe quality of transcriptions produced by (possibly unknown) ASR systems has to be evaluated/comparedsolely based on information about the input audio signals and the output transcriptions.An interesting approach exploiting ASR word accuracy estimates to automatically score the profi-ciency of non-native English speakers has been proposed by Yoon et al.
(2010).
To our knowledge thiswork is the most similar to the one presented here, although it differs in the application domain and sev-eral other aspects.
First of all, similar to CE methods, it makes some use of glass-box features derivedfrom knowledge about the ASR internal workings (e.g.
word confidence and acoustic/language modelprobabilities).
Secondly, the domain addressed is constrained to responses to prompted utterances, whilein this paper we address a large unconstrained domain, namely the automatic transcription of lectures(TED talks) covering different topics.
Finally, (Yoon et al., 2010) is based on a rather simple modelwhose performance is not carefully analysed from the learning point of view (e.g.
by comparing thecontribution different state-of-the-art algorithms) as we do here.The problem of automating system evaluation without a gold standard has been addressed also in otherNLP areas.
For instance, (Louis and Nenkova, 2013) recently addressed the assessment of machine-generated summaries without model summaries.
The strongest parallelism with our work, however,can be found in the Machine Translation (MT) evaluation field, where the goal of bypassing the needof manually-created reference translations has motivated a large body of research.4Quality estimationfor MT and ASR have a number of commonalities.
First, they both deal with a ?source?
(respectivelya sentence in a language L and an acoustic utterance) and an ?hypothesis?
whose quality has to beestimated without references (respectively a translation in a language L1 and an automatic transcriptionof the audio signal).
Second, they can be addressed at various granularities.
Indeed, ASR output qualityestimation is similar to its MT counterpart where research focused on quality predictions at word level(Ueffing and Ney, 2007; Bach et al., 2011), sentence level (Specia et al., 2009; Mehdad et al., 2012)and document level (Soricut and Echihabi, 2010).
Third, both tasks are suitable for supervised machinelearning methods, either for classification (Blatz et al., 2003; Quirk, 2004) or for regression (Specia etal., 2010; Specia, 2011).
Finally, both tasks motivate efforts in designing features capable to capturethe difficulty to process the source, the plausibility of the output hypothesis and (but not necessarily) theconfidence of the decoding process (Felice, 2012; Rubino et al., 2013b).3 ApproachWe approach the automatic estimation of ASR output quality as a supervised regression problem.
Givena training set of (signal, transcription, WER) instances, the task is to predict the WER of each instancein a test set of unseen (signal, transcription) pairs.Features.
As shown in Table 1, the features used in our experiments (68 in total) can be categorized infour main groups.
The first group (ASR features) includes several glass-box features proposed in previousliterature on ASR confidence estimation (Litman et al., 2000; Gabsdil and Lemon, 2004; Goldwater et al.,2010; Higgins et al., 2011).
These features are suitable only for the ideal situation in which informationabout systems?
internal decoding strategies is available (as in the experiments discussed in ?4.1).
We usethem as a term of comparison to evaluate the usefulness of the other three groups (signal, hybrid andtextual), which belong to the black-box type.
These features, which are totally uninformed about thedecoding process, have wider applicability to the system-independent ASR quality estimation tasks thatrepresent our target scenario (see Sections 4.2 and 4.3).
More in detail:?
ASR features aim to capture the confidence of the speech recognizer and the reliability of the wholedecoding process.
In our experiments, as we do not have access to decoders of other systems, theyare computed only for the ASR system developed in our labs (Falavigna et al., 2013).
These features4For a complete overview of the current approaches to MT quality estimation we refer the reader to the WMT12 and WMT13shared task reports (Callison-Burch et al., 2012; Bojar et al., 2013).1815are extracted both from word graphs (WGs) and n-best lists (n=100).
In Table 1 ?Total probabil-ity?
is the weighted sum of log Language Model (LM) and log Acoustic Model (AM) probabilities.LM probability is computed with a 4-gram backoff LM, trained over about 5 billion words usingthe IRSTLM toolkit (Federico et al., 2008) and the modified shift-beta smoothing method.
AMprobability is computed using a set of tied-state triphone Hidden Markov Models having, as outputstate density, a mixture of Gaussian probability densities with diagonal covariance matrices.
?Meanprobability?
is obtained dividing the total probability by the number of hypothesized ASR outputitems (words + silences).
Confidence scores are computed averaging time posterior word proba-bilities (Evermann and Woodland, 2000).
?Proportion of low confidence words?
is the fraction ofwords having confidence values ?
0.5.
The remaining ASR features are directly extracted fromword graphs and n-best lists scores.?
Signal features aim to capture the difficulty to transcribe a given input looking at the signal as awhole.
They are computed from raw vectors extracted through frame analysis (we employ 20msanalysis window and 10ms analysis step).
For each analysed window, 12 Mel Frequency CepstralCoefficients (MFCCs) are evaluated plus log energy.
Then, for each given segment, minimum,maximum and mean values of raw energy, as well as the mean MFCCs values and total segmentduration, are computed to form the signal feature vector.?
Hybrid features provide a more fine-grained way to capture the difficulty of transcribing the signal.This is done by considering information about word and silence/noise regions, as well as theirrespective duration.
These features are computed after having performed forced alignment betweenthe input audio signal and the corresponding automatic hypotheses.
Forced alignment is carriedout with our ASR system (Falavigna et al., 2013), in order to detect audio segments related towords, hesitations and silences in the hypothesis.
Pitch features have been computed with the Praatsoftware tool (Boersma and Weenink, 2005).?
Textual features aim to capture the plausibility (i.e.
the fluency) of an output transcription.
Tothis aim, we consider surface information (such as the number of words and the percentage ofnumbers/content-words/nouns/verbs in the hypothesis) as well as information about LM perplexityand probability of the hypothesis (both at the level of words and parts of speech)5.Feature selection is performed throughout all our experiments to maximize results and, at the sametime, analyse the contribution of the proposed features.
To this aim, we use Randomized Lasso, orstability selection (Meinshausen and B?uhlmann, 2010), which re-samples the training data several timesand fits a Lasso regression model on each sample.
Features that appear in a given number of samples areconsidered more informative for the task at hand, and hence retained (those marked in bold in Table 1are the most informative ones based on the experiments described in Sections 4.2 and 4.3).Learning algorithms.
To build our regression models we experimented with two non-parametric learn-ing approaches: Support Vector Machines (SVMs) (Shawe-Taylor and Cristianini, 2004) and ExtremelyRandomized Trees (XT) (Geurts et al., 2006).
SVMs are non-parametric deterministic algorithms thathave been widely used in several fields, in particular in NLP where they are the state-of-the-art for varioustasks.
Extra-Trees are a tree-based ensemble method for supervised classification and regression thatwere also successfully used for MT quality estimation (de Souza et al., 2013; de Souza et al., 2014a).
InXTs each tree can be parametrized differently.
When a tree is built, the node splitting step is done at ran-dom by picking the best split among a random subset of the input features.
The results of the individualtrees are combined by averaging their predictions.
Hyper-parameter optimization of the SVM (with Ra-dial basis function kernel ?
RBF) and XT models was performed using randomized search (Bergstra andBengio, 2012).
We used both learning methods as implemented in the Scikit-learn package (Pedregosaet al., 2011).5The PoS LM has been obtained by processing with the TreeTagger (Schmid, 1995) the same data used for the word LM.6Hesitations, such as ?uhm?, ?eh?
and ?ah?
are found through matches with a predefined list.
Consecutive repeated wordsin the same utterance are also considered as hesitations.1816ASR (16)Total probability of ASR output (w ?
logPLM+ logPAM), mean probability, totalacoustic probability, mean acoustic probability, mean confidence score, Std of confi-dence scores, confidence scores per second, proportion of low-confidence words, WGnode density, WG transition density, Mean/Std/Min n-best probability, Mean/Std/Minn-best acoustic probability.Signal (16)Total segment duration (sec), Mean/Min/Max raw energy (dB), mean MFCC[1, 2,3, 4, 5, 6, 7, 8, 9, 10, 11,12].Hybrid (26)SNR (dB), mean noise energy (dB), Mean/Min/Max word energy (dB), Min/Maxnoise energy (dB), (max word - min noise) energy (dB), # silences, ratio of silencesand words, # words per second, # silences per second, total duration of words(sec), total duration of silences (sec), mean duration of words (sec), mean durationof silences (sec), ratio of (tot duration silences) and (tot duration words), Std of wordduration (sec), Std of silence duration (sec), (tot duration words) - (tot durationsilences), Mean/Std/Min./Max.
pitch (Hz), # hesitations,6frequency of hesitations.Textual (10)Number of words, LM log probability of the hypothesis, LM log probability ofPOS of the hypothesis, LM log perplexity of POS of the hypothesis, Perplexity ofthe hypothesis, % of numbers in the hypothesis, % of tokens in the hypothesis whichdo not contain only a-z, % of content words in the hypothesis, % of nouns in thehypothesis, % of verbs in the hypothesisTable 1: Full list of the 68 features used in our experiments, divided into four groups.
The most predictiveblack-box features (resulting from feature selection in the ?4.3 experiments) are marked in bold.4 ExperimentsTo evaluate our approach we carried out three sets of experiments.
In each set our feature groups areanalysed: i) with the two learning algorithms, ii) in combination/isolation, iii) with/without feature se-lection.
The three sets differ in terms of the difficulty of the quality estimation task from the learningpoint of view.
To experiment with situations of increasing complexity, we alternate conditions in whichall the features (glass-box and black-box) can be used, training and test sets are non-/homogeneous, thequality estimator is trained on transcriptions generated by the same/different ASR systems.Data.
The data used in the experiments consists of the audio recordings delivered for the IWSLT 2013evaluation campaign (Cettolo et al., 2013).
One of the tasks of IWSLT 2013 is the automatic tran-scription of English TED talks, a global set of conferences whose audio/video recordings are publiclyavailable.
The main challenges for ASR in these talks include: the large variability of topics (hencea large, unconstrained vocabulary), the presence of non-native speakers and a rather informal speakingstyle.
Each IWSLT participant submitted one primary ASR output run for each of the talks included inthe test set plus some optional contrastive ASR outputs.
In addition, participants sent submissions forthe ASR tracks delivered for the 2012 evaluation campaign.
Our experiments have been carried out onthe primary submissions, sent by 8 participants, related to the 2012 (consisting in 11 different talks) and2013 (28 different talks) test sets.
The 2012 test set has a total duration of around 1h45sec, it contains1,118 reference sentences and 18,613 running words.
On such dataset, participants?
primary submissionsachieved a mean utterance WER ranging from 10.5% to 18.4% (in this work a WER score is computedfor each reference sentence, and mean utterance WER represents the average of sentence WERs).
The2013 test set has a total duration of around 3h55sec, it contains 2,238 reference sentences and 41,545 run-ning words.
On this dataset, primary participants?
submissions achieve a mean utterance WER rangingfrom 15.9% to 30.8%.In our experiments, we always use 1,118 utterances for training the regressor and 1,120 for testing.
Tothis aim, the IWSLT 2013 data is randomly sampled three times in training and test sets of such dimen-sions.
While for the 2012 test set manual utterance segmentation has been provided by the organizers, forthe 2013 data the participants had to employ their own automatic segmentation systems before decodingthe audio tracks (thus resulting in a different number of ASR sentence hypotheses for each team).
Hence,1817to ensure that each participant has the same number of ASR sentence hypotheses, an alignment with thereference manual segmentation has been performed in our experiments.Evaluation.
Our evaluation is carried out in terms of Mean Absolute Error (MAE), a standard metricfor regression problems.
The MAE is the average of the absolute errors ei= |fi?
yi|, where fiisthe prediction of the model and yiis the actual WER for the ithtest instance.
WER is calculated withthe NIST SCLITE Scoring Package.7As it is a measure of error, lower MAE scores indicate that ourpredictions are closer to the real WER calculated for each test instance against the reference transcripts.For each experiment, we report the mean and the standard deviation of the MAE achieved by the bestperforming QE models on the IWSLT 2013 test sets.Baseline.
Besides measuring performance in terms of global MAE, each model is compared against acommon baseline for regression tasks.
This baseline, which is particularly relevant in settings featuringdifferent data distributions between training and test sets, is calculated by labelling each test instancewith the mean WER score calculated on the training set.
Previous works, also in MT quality estimation,demonstrated that its results can be particularly hard to beat (Rubino et al., 2013a).4.1 Experiment 1In the first set of experiments we consider the easiest situation from the learning perspective.
In thissetting we predict the WER of transcriptions produced by our ASR system (denoted by X), whose innerworkings are known (thus enabling the use of glass-box features).
To investigate the relation betweenprediction accuracy and the degree of homogeneity of training and test data, we experiment both withsimilar datasets (disjoint training and test sampled from IWSLT13) and different datasets (IWSLT12for training and samples from IWSLT13 for test).
Results are reported in Table 2, where the notation?LetterYear - LetterYear?
indicates the systems and the datasets used for training and test (respectivelyour system X, and data from IWSLT12 and/or IWSLT13).Train - Test ALL (glass-box + BB COMB) ASR (glass-box) BB COMB (Signal+Hybrid+Textual) BaselineX13 - X13 11.56?0.29 SVR 12.11?0.29 XT 15.17?0.06 XT 19.84?0.06X12 - X13 12.61?0.13 XT 13.78?0.16 XT 16.78?0.18 XT 19.06?0.12Train - Test Signal Hybrid Textual BaselineX13 - X13 16.42?0.1 XT 17.61?0.12 XT 17.42?0.15 SVR 19.84?0.06X12 - X13 18.85?0.09?XT 18.39?0.22 XT 17.58?0.15 XT 19.06?0.12Table 2: MAE results using the same system on different datasets, with and without glass-box features.As can be seen from the table, the two models using ALL the features achieve the largest improvementsover the strong baseline used for comparison (up to 8.2 MAE points in the X13 - X13 setting).
This isnot surprising if we consider the high predictive power of ASR (glass-box) features that, when used inisolation, lead to a considerably lower MAE with respect to the other three groups.
However, it?s worthobserving that also the combination of only the black-box features (BB COMB) allows the QE predictorsto significantly outperform the baseline (up to 4.67 MAE points in X13 - X13).
Such improvements comefrom the joint contribution of each of the three groups, which achieve good results also in isolation.Indeed, except in one case where the gain over the baseline is not significant8(X12 - X13 with the Signalfeatures), their MAE reduction ranges between 0.67 (X12 - X13 Hybrid) and 3.42 MAE points (X13 -X13 Signal).
The good prediction capability of the black-box features is also shown by the fact that, whencombined with the glass-box features, they lead to improvements between 0.55 and 1.17 MAE pointsover the ASR features alone.
Considering the privileged condition of the (system-informed) glass-boxfeatures, this is a remarkable result that suggests some complementarity between the two groups.In general, our supervised approach is sensitive to the similarity between training and test.
This isevidenced by higher MAE results when non-homogeneous datasets (i.e.
X12 - X13) are processed.
In7http://www1.icsi.berkeley.edu/Speech/docs/sctk-1.2/sclite.htm8Statistical significance is measured by considering the overlap of confidence intervals defined by the standard deviationrange around the mean.
In our tables, the results marked with the ???
symbol are not significantly better than the baseline.1818terms of algorithms, XT generally performs better than SVR, in particular when the QE model is trainedand tested on non-homogeneous data.
This can be explained by their higher generalization capabilitydue to variance reductions as explained in (Hastie et al., 2009, Chapter 15).4.2 Experiment 2In this set of experiments we consider a situation of intermediate difficulty from the learning perspective.Our objective is to evaluate, on homogeneous datasets (sampled from IWSLT13), the output of ASRsystems whose inner workings are not known (hence only black-box features can be used).
To makeour analysis more complete, we also evaluate the performance of models trained on a given ASR systemto predict the WER of hypotheses produced by a different one.
This situation is closer to applicationscenarios in which the evaluated ASR system is unknown and different from the one used to train thequality estimator.
Two systems with very different performance are considered for this purpose: the bestand the worst according to the official IWSLT 2013 ranking (respectively denoted by A and Z).Train - Test BB COMB Signal Hybrid Textual BaselineA13 - A13 11.18?0.22 SVR 11.91?0.23 SVR 12.76?0.18 SVR 12.57?0.13 SVR 14.35?0.1Z13 - A13 16.01?0.23 SVR 18.04?0.22 SVR 17.24?0.22 SVR 18.01?0.2 XT 21.58?0.15Z13 - Z13 15.52?0.6 XT 16.94?0.41 XT 17.04?0.56 SVR 17.84?0.4 XT 19.65?0.43A13 - Z13 17.36?0.43 XT 18.7?0.53 XT 18.21?0.45 XT 19.38?0.45 XT 21.03?0.51Table 3: MAE results using different systems on the same dataset, without glass-box features.The results reported in Table 3 confirm that: i) the combination of black-box features (BB COMB)always leads to the best QE models, which significantly outperform the baseline, ii) the same holdsalso when each single group is used in isolation, iii) with less homogeneous training and test data, XTperforms generally better than SVR.In addition, it?s worth noting that when a QE model is trained and tested on data transcribed bythe same ASR system the results are significantly better (the MAE is always about 1.0 - 6.0 pointslower).
Indeed, as also shown by the same behaviour of our baseline, this condition is simpler and moresuitable for supervised learning methods.
This depends on the fact that each ASR system has its owncoherent behaviour, which results in transcriptions with similar characteristics that supervised modelsare able to learn (e.g.
recurring errors, similar WER distributions).
In contrast, when training and testdata are produced by different ASR systems, supervised learning becomes more difficult and the outputpredictions less reliable.
Each feature group is affected by this situation, but it is interesting to note thatthe Hybrid features are more robust than the other two groups to less homogeneous datasets.
This can beexplained by the fact that they are extracted after applying forced alignment by means of a third system,which is likely to normalise and reduce the difference between training and test data.
Overall, also inthis more complex scenario where the glass-box features cannot be used, our results demonstrate a goodprediction capability of the QE models, which are still able to beat a strong baseline.4.3 Experiment 3In the third set of experiments we consider the hardest case from the learning point of view.
In this settingthe evaluated ASR systems are unknown and training/test data are non homogeneous (i.e.
training fromIWSLT12, test from samples of IWSLT13).
Results are reported in Table 4.Train - Test BB COMB Signal Hybrid Textual BaselineA12 - A13 12.81?0.08 XT 13.57?0.13?XT 12.85?0.1 XT 13.25?0.23?XT 13.65?0.17Z12 - A13 14.78?0.1 SVR 15.66?0.09?XT 13.56?0.09 SVR 13.63?0.24 SVR 15.51?0.35Z12 - Z13 17.16?0.4 XT 19.34?0.32?XT 17.68?0.3 XT 19.59?0.11?XT 19.98?0.29A12 - Z13 19.83?0.23 XT 21.85?0.2 XT 20.68?0.13 XT 22.62?0.08 XT 23.04?0.18Table 4: MAE results using different systems on different dataset, without glass-box features.Also in the most challenging scenario our results substantially confirm the previous findings.
Indeed,except in one case (Z12 - A13), the following observations still hold: i) when used in combination, the18190 10 20 30 40 50 60 70 80 90 1001214161820222426% of Training DataMAEA12 ?
A13A12 ?
Z13Z12 ?
Z13Z12 ?
A13Figure 1: Learning curves for the best systems of ?Experiment 3?
(using BB COMB features).black-box features (BB COMB) lead to the best QE models, which significantly outperform the baseline,ii) this holds also when each single group is used in isolation (although not significantly in 5 out of 12settings), iii) with less homogeneous training and test data, XT performs generally better than SVR.Unsurprisingly, as also observed in the previous set of experiments, the low homogeneity of trainingand test data has an impact on the accuracy of the predictions.
The effect of training and testing onless homogeneous data produced by different systems is now clearly visible.
Except for the more robustHybrid features, which in the Z12 - A13 setting produce the best model, the results obtained with the twoother groups decreased to the point that their improvement over the baseline is often not significant.
Nev-ertheless, even under the challenging conditions posed by this realistic and application-oriented scenario,reference-free and system-agnostic ASR evaluation remains a feasible task.5 Feature Analysis and Learning CurvesIn order to gain additional insights about the effectiveness of our method, we performed a further analysisof the ?Experiment 3?
results.
In such challenging scenario, the most interesting from the applicationperspective, we first identified the most predictive features among those in the BB COMB set.
To thisaim, we collected the features that are always chosen by the feature selection algorithm proposed in ?3.The resulting list contains features from all the three black-box groups (marked in bold in Table 1).
Thisconfirms their complementarity in predicting the quality of a transcribed utterance.In the same setting, we also investigated the relation between the amount of data used to train ourmodels and the accuracy of their predictions.
To this aim, we measured performance variations whenthe same models (i.e.
those obtained with the BB COMB set) are trained on different amounts of data.For each training set, nine subsets were created (with 10%, 20%,..., 90% of the data) by sub-samplingsentences from a uniform distribution.
The process was iterated 5 times.
Each subset was used to buildthe relative QE regressor, which was then evaluated on our test sets.
Figure 1 shows the resulting learningcurves (each point is the average result of the 5 runs on each test set; the error bars show ?1std).
Ascan be seen from all the curves, after an initial fluctuation of the MAE, performance results with 40% ofthe training data are comparable with those obtained using the whole training set.
Moreover, it?s worthremarking that in three out of four cases the models trained with such amount of data already outperformthe baseline (for Z12 - A13 the MAE is only 0.01 point higher).
This suggests that reference-free, system-independent models for ASR quality estimation are able to provide informative predictions even with alimited amount (?400 manual transcripts) of training instances.18206 ConclusionWe investigated the problem of automatically predicting the word error rate of an automatically-transcribed utterance in a large vocabulary continuous speech recognition setting.
In such scenario,we proposed a supervised regression approach that bypasses the need of manual reference transcriptionsand does not necessarily depend on information about system?s confidence (first contribution of the pa-per).
Then, by evaluating models obtained with different state-of-the-art learning algorithms, we showedthat our automatic predictions outperform a strong baseline and closely approximate the WER scorescalculated over reference transcripts (second contribution).
Different feature groups have been proposedand their contribution has been analysed in a range of testing conditions of increasing difficulty (thirdcontribution).
This made possible to isolate informative features that significantly contribute to the per-formance of our quality estimation models, and to get useful insights about the potential of our approachwhen different sources of information (glass-box, black box features) are available.
Finally, analysingthe relation between prediction performance and the size of the training set, we showed that the resultsobtained with 40% of the data are already comparable to our best MAE (fourth contribution).Our analysis revealed a dependency between the performance of the quality estimation models andthe degree of homogeneity between training and test data.
This aspect is particularly relevant from theapplication perspective since in real working conditions the availability of large amounts of representa-tive training instances is far from being guaranteed.
In quality estimation for machine translation (a taskfeaturing strong similarities with ours), these issues have recently motivated studies on domain adapta-tion and online learning techniques (de Souza et al., 2014b; Turchi et al., 2014).
This suggests, as a firstdirection for future work, the investigation of approaches capable to better exploit the available trainingdata and mitigate the impact of large differences between training and test instances.AcknowledgementsThis work has been partially funded by the European project EU-BRIDGE (FP7-287658) and by theAutonomous Province of Trento, Italy, under the project Wikivoice (L.P. 6/1999).ReferencesNguyen Bach, Fei Huang, and Yaser Al-Onaizan.
2011.
Goodness: a Method for Measuring Machine TranslationConfidence.
In The 49th Annual Meeting of the Association for Computational Linguistics: Human LanguageTechnologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA, pages 211?219.
TheAssociation for Computer Linguistics.James Bergstra and Yoshua Bengio.
2012.
Random Search for Hyper-Parameter Optimization.
Journal of Ma-chine Learning Research, 13:281?305.John Blatz, Erin Fitzgerald, George Foster, Simona Gandrabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2003.
Confidence Estimation for Machine Translation.
Summer workshop final report,JHU/CLSP.Paul Boersma and David Weenink.
2005.
Praat: Doing Phonetics by Computer (Version 4.3.01).
Retrieved fromhttp://www.praat.org/.Ondrej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, ChristofMonz, Matt Post, Radu Soricut, and Lucia Specia.
2013.
Findings of the 2013 Workshop on Statistical MachineTranslation.
In Eighth Workshop on Statistical Machine Translation, WMT-2013, pages 1?44, Sofia, Bulgaria.Chris Callison-Burch, Philipp Koehn, Christof Monz, Matt Post, Radu Soricut, and Lucia Specia.
2012.
Findingsof the 2012 Workshop on Statistical Machine Translation.
In Proceedings of the Seventh Workshop on StatisticalMachine Translation (WMT?12), pages 10?51, Montr?eal, Canada.Mauro Cettolo, Jan Niehues, Sebastian St?uker, Luisa Bentivogli, and Marcello Federico.
2013.
Report on the 10thIWSLT Evaluation Campaign.
Proceedings of the International Workshop on Spoken Language Translation,Heidelberg, Germany.1821Jos?e G. C. de Souza, Christian Buck, Marco Turchi, and Matteo Negri.
2013.
FBK-UEdin participation to theWMT13 quality estimation shared task.
In Proceedings of the Eighth Workshop on Statistical Machine Trans-lation, pages 352?358, Sofia, Bulgaria, August.
Association for Computational Linguistics.Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Christian Buck, Marco Turchi, and Matteo Negri.
2014a.
FBK-UPV-UEdin participation in the WMT14 Quality Estimation shared-task.
In Proceedings of the Ninth Workshop onStatistical Machine Translation, Baltimore, MD, USA, June.Jos?e G. C. de Souza, Marco Turchi, and Matteo Negri.
2014b.
Predicting Machine Translation Quality Esti-mation Across Domains.
In Proceedings of the 25th International Conference on Computational Linguistics,COLING?14, Dublin, Ireland.Gunnar Evermann and Philip C. Woodland.
2000.
Large Vocabulary Decoding and Confidence Estimation UsingWord Posterior Probabilities.
In Proc.
of ICASSP, pages 2366?2369, Istanbul, Turkey, June.Daniele Falavigna, Roberto Gretter, Fabio Brugnara, Diego Giuliani, and Romain Serizel.
2013.
FBK@IWSLT2013 - ASR Tracks.
In Proceedings of the IWSLT 2013 workshop, Heidelberg, Germany.Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.
2008.
IRSTLM: an Open Source Toolkit for HandlingLarge Scale Language Models.
pages 1618?1621, Brisbane, Australia, September.Marcello Federico, Luisa Bentivogli, Michael Paul, and Sebastian St?uker.
2011.
Overview of the IWSLT 2011Evaluation Campaign.
In International Workshop on Spoken Language Translation, pages 11?27.Marcello Federico, Mauro Cettolo, Luisa Bentivogli, Michael Paul, and Sebastian St?uker.
2012.
Overview of theIWSLT 2012 Evaluation Campaign.
In Proc.
of the International Workshop on Spoken Language Translation,Hong Kong, HK, December.Mariano Felice.
2012.
Linguistic Indicators for Quality Estimation of Machine Translations.
Master?s thesis,University of Wolverhampton, UK.Malte Gabsdil and Oliver Lemon.
2004.
Combining acoustic and pragmatic features to predict recognition perfor-mance in spoken dialogue systems.
pages 344?351.Pierre Geurts, Damien Ernst, and Louis Wehenkel.
2006.
Extremely randomized trees.
Mach.
Learn., 63(1):3?42,April.Sharon Goldwater, Dan Jurafsky, and Christopher Manning.
2010.
Which words are hard to recognize?
Prosodic,lexical, and disfluency factors that increase speech recognition error rates.
52(3):181?200.Trevor Hastie, Robert Tibshirani, Jerome Friedman, T Hastie, J Friedman, and R Tibshirani.
2009.
The elementsof statistical learning, volume 2.
Springer.Derrick Higgins, Xiaoming Xi, Klaus Zechner, and David Williamson.
2011.
A three-stage approach for auto-mated scoring of spontaneous responses.
(25):282?306.Diane J. Litman, Julia B. Hirschberg, and Marc Swerts.
2000.
Predicting Automatic Speech Recognition Perfor-mance Using Prosodic Cues.
In Proceedings of NAACL, pages 218?225.Annie Louis and Ani Nenkova.
2013.
Automatically assessing machine summary content without a gold standard.Computational Linguistics, 39(2):267?300.Yashar Mehdad, Matteo Negri, and Marcello Federico.
2012.
Match without a Referee: Evaluating MT Adequacywithout Reference Translations.
In Proceedings of the Machine Translation Workshop (WMT2012).Nicolai Meinshausen and Peter B?uhlmann.
2010.
Stability selection.
Journal of the Royal Statistical Society:Series B (Statistical Methodology), 72(4):417?473.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss,V.
Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay.
2011.
Scikit-learn : Machine Learning in Python.
Journal of Machine Learning Research, 12:2825?2830.Christopher B. Quirk.
2004.
Training a Sentence-Level Machine Translation Confidence Measure.
In Proceedingsof LREC 2004.Raphael Rubino, Jos?e GC de Souza, Jennifer Foster, and Lucia Specia.
2013a.
Topic Models for TranslationQuality Estimation for Gisting Purposes.
In Proceedings of the Machine Translation Summit XIV.1822Raphael Rubino, Joachim Wagner, Jennifer Foster, Johann Roturier, Rasoul Samad Zadeh Kaljahi, and Fred Hol-lowood.
2013b.
DCU-Symantec at the WMT 2013 Quality Estimation Shared Task.
In Proceedings of theEighth Workshop on Statistical Machine Translation, pages 392?397.Alberto Sanchis, Alfons Juan, and Enrique Vidal.
2012.
A Word-Based Naive Bayes Classifier for ConfidenceEstimation in Speech Recognition.
20(12):565?574.Helmut Schmid.
1995.
Improvements in Part-of-Speech Tagging with an Application to German.
In Proceedingsof the ACL SIGDAT-Workshop, pages 47?50, Dublin, Ireland.Matthew Stephen Seigel.
2013.
Condence Estimation for Automatic Speech Recognition Hypotheses.
Universityof Cambridge.
PhD Thesis.John Shawe-Taylor and Nello Cristianini.
2004.
Kernel methods for pattern analysis.
Cambridge university press.Radu Soricut and Abdessamad Echihabi.
2010.
TrustRank: Inducing Trust in Automatic Translations via Ranking.In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, ACL ?10, pages612?621, Stroudsburg, PA, USA.
Association for Computational Linguistics.Lucia Specia, Nicola Cancedda, Marc Dymetman, Marco Turchi, and Nello Cristianini.
2009.
Estimating theSentence-Level Quality of Machine Translation Systems.
In Proceedings of the 13th Annual Conference of theEuropean Association for Machine Translation (EAMT?09), pages 28?35, Barcelona, Spain.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.
Machine Translation Evaluation versus Quality Estimation.Machine translation, 24(1):39?50.Lucia Specia.
2011.
Exploiting Objective Annotations for Measuring Translation Post-editing Effort.
Proceedingsof the 15th Conference of the European Association for Machine Translation, pages 73?80.Rafic Antoon Sukkar and Chin-Hui Lee.
1996.
Vocabulary Independent Discriminative Utterance Verification forNonkeyword Rejection in Subword Based Speech Recognition.
6(6):420?429.Marco Turchi, Antonios Anastasopoulos, Jos?e G. C. de Souza, and Matteo Negri.
2014.
Adaptive Quality Estima-tion for Machine Translation.
In Proceedings of the 52nd Annual Meeting of the Association for ComputationalLinguistics, ACL?14, Baltimore, MD, USA.
Association for Computational Linguistics.Nicola Ueffing and Hermann Ney.
2007.
Word-Level Confidence Estimation for Machine Translation.
Comput.Linguist., 33(1):9?40, March.Frank Wessel, Ralf Schl?uter, Klaus Macherey, and Hermann Ney.
2001.
Confidence Measures for Large Vocabu-lary Continuous Speech Recognition.
9(3):288?298.Su-Youn Yoon, Lei Chen, and Klaus Zechner.
2010.
Predicting word accuracy for the automatic speech recogni-tion of non-native speech.
In Proc.
of INTERSPEECH, pages 773?776, Makuhari,Chiba, Japan.1823
