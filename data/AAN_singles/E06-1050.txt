A Probabilistic Answer Type ModelChristopher PinchakDepartment of Computing ScienceUniversity of AlbertaEdmonton, Alberta, Canadapinchak@cs.ualberta.caDekang LinGoogle, Inc.1600 Amphitheatre ParkwayMountain View, CAlindek@google.comAbstractAll questions are implicitly associatedwith an expected answer type.
Unlikeprevious approaches that require a prede-fined set of question types, we presenta method for dynamically constructinga probability-based answer type modelfor each different question.
Our modelevaluates the appropriateness of a poten-tial answer by the probability that it fitsinto the question contexts.
Evaluationis performed against manual and semi-automatic methods using a fixed set of an-swer labels.
Results show our approach tobe superior for those questions classifiedas having a miscellaneous answer type.1 IntroductionGiven a question, people are usually able to forman expectation about the type of the answer, evenif they do not know the actual answer.
An accu-rate expectation of the answer type makes it mucheasier to select the answer from a sentence thatcontains the query words.
Consider the question?What is the capital of Norway??
We would ex-pect the answer to be a city and could filter outmost of the words in the following sentence:The landed aristocracy was virtually crushedby Hakon V, who reigned from 1299 to 1319,and Oslo became the capital of Norway, re-placing Bergen as the principal city of thekingdom.The goal of answer typing is to determinewhether a word?s semantic type is appropriate asan answer for a question.
Many previous ap-proaches to answer typing, e.g., (Ittycheriah et al,2001; Li and Roth, 2002; Krishnan et al, 2005),employ a predefined set of answer types and usesupervised learning or manually constructed rulesto classify a question according to expected an-swer type.
A disadvantage of this approach is thatthere will always be questions whose answers donot belong to any of the predefined types.Consider the question: ?What are tourist attrac-tions in Reims??
The answer may be many things:a church, a historic residence, a park, a famousintersection, a statue, etc.
A common method todeal with this problem is to define a catch-all class.This class, however, tends not to be as effective asother answer types.Another disadvantage of predefined answertypes is with regard to granularity.
If the typesare too specific, they are more difficult to tag.
Ifthey are too general, too many candidates may beidentified as having the appropriate type.In contrast to previous approaches that use a su-pervised classifier to categorize questions into apredefined set of types, we propose an unsuper-vised method to dynamically construct a proba-bilistic answer type model for each question.
Sucha model can be used to evaluate whether or nota word fits into the question context.
For exam-ple, given the question ?What are tourist attrac-tions in Reims?
?, we would expect the appropriateanswers to fit into the context ?X is a tourist attrac-tion.?
From a corpus, we can find the words thatappeared in this context, such as:A-Ama Temple, Aborigine, addition, AnakKrakatau, archipelago, area, baseball,Bletchley Park, brewery, cabaret, Cairo,Cape Town, capital, center, ...Using the frequency counts of these words inthe context, we construct a probabilistic modelto compute P (in(w,?
)|w), the probability for aword w to occur in a set of contexts ?, given anoccurrence of w. The parameters in this model areobtained from a large, automatically parsed, un-labeled corpus.
By asking whether a word wouldoccur in a particular context extracted from a ques-393tion, we avoid explicitly specifying a list of pos-sible answer types.
This has the added benefitof being easily adapted to different domains andcorpora in which a list of explicit possible answertypes may be difficult to enumerate and/or identifywithin the text.The remainder of this paper is organized as fol-lows.
Section 2 discusses the work related to an-swer typing.
Section 3 discusses some of the keyconcepts employed by our probabilistic model, in-cluding word clusters and the contexts of a ques-tion and a word.
Section 4 presents our probabilis-tic model for answer typing.
Section 5 comparesthe performance of our model with that of an or-acle and a semi-automatic system performing thesame task.
Finally, the concluding remarks in aremade in Section 6.2 Related WorkLight et al (2001) performed an analysis of theeffect of multiple answer type occurrences in asentence.
When multiple words of the same typeappear in a sentence, answer typing with fixedtypes must assign each the same score.
Light etal.
found that even with perfect answer sentenceidentification, question typing, and semantic tag-ging, a system could only achieve 59% accuracyover the TREC-9 questions when using their set of24 non-overlapping answer types.
By computingthe probability of an answer candidate occurringin the question contexts directly, we avoid havingmultiple candidates with the same level of appro-priateness as answers.There have been a variety of approaches to de-termine the answer types, which are also knownas Qtargets (Echihabi et al, 2003).
Most previousapproaches classify the answer type of a questionas one of a set of predefined types.Many systems construct the classification rulesmanually (Cui et al, 2004; Greenwood, 2004;Hermjakob, 2001).
The rules are usually triggeredby the presence of certain words in the question.For example, if a question contains ?author?
thenthe expected answer type is Person.The number of answer types as well as the num-ber of rules can vary a great deal.
For example,(Hermjakob, 2001) used 276 rules for 122 answertypes.
Greenwood (2004), on the other hand, used46 answer types with unspecified number of rules.The classification rules can also be acquiredwith supervised learning.
Ittycheriah, et al (2001)describe a maximum entropy based question clas-sification scheme to classify each question as hav-ing one of the MUC answer types.
In a similar ex-periment, Li & Roth (2002) train a question clas-sifier based on a modified version of SNoW usinga richer set of answer types than Ittycheriah et alThe LCC system (Harabagiu et al, 2003) com-bines fixed types with a novel loop-back strategy.In the event that a question cannot be classified asone of the fixed entity types or semantic conceptsderived from WordNet (Fellbaum, 1998), the an-swer type model backs off to a logic prover thatuses axioms derived form WordNet, along withlogic rules, to justify phrases as answers.
Thus, theLCC system is able to avoid the use of a miscel-laneous type that often exhibits poor performance.However, the logic prover must have sufficient ev-idence to link the question to the answer, and gen-eral knowledge must be encoded as axioms intothe system.
In contrast, our answer type modelderives all of its information automatically fromunannotated text.Answer types are often used as filters.
It wasnoted in (Radev et al, 2002) that a wrong guessabout the answer type reduces the chance for thesystem to answer the question correctly by asmuch as 17 times.
The approach presented hereis less brittle.
Even if the correct candidate doesnot have the highest likelihood according to themodel, it may still be selected when the answerextraction module takes into account other factorssuch as the proximity to the matched keywords.Furthermore, a probabilistic model makes it eas-ier to integrate the answer type scores with scorescomputed by other components in a question an-swering system in a principled fashion.3 ResourcesBefore introducing our model, we first describethe resources used in the model.3.1 Word ClustersNatural language data is extremely sparse.
Wordclusters are a way of coping with data sparsenessby abstracting a given word to a class of relatedwords.
Clusters, as used by our probabilistic an-swer typing system, play a role similar to that ofnamed entity types.
Many methods exist for clus-tering, e.g., (Brown et al, 1990; Cutting et al,1992; Pereira et al, 1993; Karypis et al, 1999).We used the Clustering By Committee (CBC)394Table 1: Words and their clustersWord Clusterssuite software, network, wireless, ...rooms, bathrooms, restrooms, ...meeting room, conference room, ...ghost rabbit, squirrel, duck, elephant, frog, ...goblins, ghosts, vampires, ghouls, ...punk, reggae, folk, pop, hip-pop, ...huge, larger, vast, significant, ...coming-of-age, true-life, ...clouds, cloud, fog, haze, mist, ...algorithm (Pantel and Lin, 2002) on a 10 GB En-glish text corpus to obtain 3607 clusters.
The fol-lowing is an example cluster generated by CBC:tension, anger, anxiety, tensions, frustration,resentment, uncertainty, confusion, conflict,discontent, insecurity, controversy, unease,bitterness, dispute, disagreement, nervous-ness, sadness, despair, animosity, hostility,outrage, discord, pessimism, anguish, ...In the clustering generated by CBC, a word maybelong to multiple clusters.
The clusters to whicha word belongs often represent the senses of theword.
Table 1 shows two example words and theirclusters.3.2 ContextsThe context in which a word appears often im-poses constraints on the semantic type of the word.This basic idea has been exploited by many pro-posals for distributional similarity and clustering,e.g., (Church and Hanks, 1989; Lin, 1998; Pereiraet al, 1993).Similar to Lin and Pantel (2001), we definethe contexts of a word to be the undirected pathsin dependency trees involving that word at eitherthe beginning or the end.
The following diagramshows an example dependency tree:Which city hosted the 1988 Winter Olympics?det subjobjNNNNdetThe links in the tree represent dependency rela-tionships.
The direction of a link is from the headto the modifier in the relationship.
Labels associ-ated with the links represent types of relations.In a context, the word itself is replaced with avariable X.
We say a word is the filler of a contextif it replaces X.
For example, the contexts for theword ?Olympics?
in the above sentence includethe following paths:Context of ?Olympics?
ExplanationX WinterNNWinter XX 1988NN1988 XX hostobjhost XX hostobjcitysubjcity hosted XIn these paths, words are reduced to their rootforms and proper names are reduced to their entitytags (we used MUC7 named entity tags).Paths allow us to balance the specificity of con-texts and the sparseness of data.
Longer paths typ-ically impose stricter constraints on the slot fillers.However, they tend to have fewer occurrences,making them more prone to errors arising fromdata sparseness.
We have restricted the path lengthto two (involving at most three words) and requirethe two ends of the path to be nouns.We parsed the AQUAINT corpus (3GB) withMinipar (Lin, 2001) and collected the frequencycounts of words appearing in various contexts.Parsing and database construction is performedoff-line as the database is identical for all ques-tions.
We extracted 527,768 contexts that ap-peared at least 25 times in the corpus.
An examplecontext and its fillers are shown in Figure 1.X host Olympicssubj objAfrica 2 grant 1 readiness 2AP 1 he 2 Rio de Janeiro 1Argentina 1 homeland 3 Rome 1Athens 16 IOC 1 Salt Lake City 2Atlanta 3 Iran 2 school 1Bangkok 1 Jakarta 1 S. Africa 1. .
.
.
.
.
.
.
.decades 1 president 2 Zakopane 4facility 1 Pusan 1government 1 race 1Figure 1: An example context and its fillers3.2.1 Question ContextsTo build a probabilistic model for answer typ-ing, we extract a set of contexts, called questioncontexts, from a question.
An answer is expectedto be a plausible filler of the question contexts.Question contexts are extracted from a questionwith two rules.
First, if the wh-word in a ques-tion has a trace in the parse tree, the question con-texts are the contexts of the trace.
For example, the395question ?What do most tourists visit in Reims?
?is parsed as:Whati do most tourists visit ei in Reims?detisubjdet objinThe symbol ei is the trace of whati.
Minipargenerates the trace to indicate that the word whatis the object of visit in the deep structure of thesentence.
The following question contexts are ex-tracted from the above question:Context ExplanationX visit touristobj subjtourist visits XX visit Reimsobj invisit X in ReimsThe second rule deals with situations wherethe wh-word is a determiner, as in the question?Which city hosted the 1988 Winter Olympics??
(the parse tree for which is shown in section 3.2).In such cases, the question contexts consist of asingle context involving the noun that is modifiedby the determiner.
The context for the above sen-tence is X citysubj, corresponding to the sentence?X is a city.?
This context is used because thequestion explicitly states that the desired answer isa city.
The context overrides the other contexts be-cause the question explicitly states the desired an-swer type.
Experimental results have shown thatusing this context in conjunction with other con-texts extracted from the question produces lowerperformance than using this context alone.In the event that a context extracted from a ques-tion is not found in the database, we shorten thecontext in one of two ways.
We start by replac-ing the word at the end of the path with a wildcardthat matches any word.
If this fails to yield en-tries in the context database, we shorten the con-text to length one and replace the end word withautomatically determined similar words instead ofa wildcard.3.2.2 Candidate ContextsCandidate contexts are very similar in form toquestion contexts, save for one important differ-ence.
Candidate contexts are extracted from theparse trees of the answer candidates rather than thequestion.
In natural language, some words maybe polysemous.
For example, Washington may re-fer to a person, a city, or a state.
The occurrencesof Washington in ?Washington?s descendants?
and?suburban Washington?
should not be given thesame score when the question is seeking a loca-tion.
Given that the sense of a word is largely de-termined by its local context (Choueka and Lusig-nan, 1985), candidate contexts allow the model totake into account the candidate answers?
sensesimplicitly.4 Probabilistic ModelThe goal of an answer typing model is to evalu-ate the appropriateness of a candidate word as ananswer to the question.
If we assume that a setof answer candidates is provided to our model bysome means (e.g., words comprising documentsextracted by an information retrieval engine), wewish to compute the value P (in(w,?Q)|w).
Thatis, the appropriateness of a candidate answer w isproportional to the probability that it will occur inthe question contexts ?Q extracted from the ques-tion.To mitigate data sparseness, we can introducea hidden variable C that represents the clusters towhich the candidate answer may belong.
As a can-didate may belong to multiple clusters, we obtain:P (in(w,?Q)|w) =XCP (in(w,?Q), C|w) (1)=XCP (C|w)P (in(w,?Q)|C,w) (2)Given that a word appears, we assume that it hasthe same probability to appear in a context as allother words in the same cluster.
Therefore:P (in(w,?Q)|C,w) ?
P (in(C,?Q)|C) (3)We can now rewrite the equation in (2) as:P (in(w,?Q)|w) ?XCP (C|w)P (in(C,?Q)|C) (4)This equation splits our model into two parts:one models which clusters a word belongs to andthe other models how appropriate a cluster is tothe question contexts.
When ?Q consists of multi-ple contexts, we make the na?
?ve Bayes assumptionthat each individual context ?Q ?
?Q is indepen-dent of all other contexts given the cluster C.P (in(w,?Q)|w) ?XCP (C|w)Y?Q?
?QP (in(C, ?Q)|C) (5)Equation (5) needs the parameters P (C|w) andP (in(C, ?Q)|C), neither of which are directlyavailable from the context-filler database.
We willdiscuss the estimation of these parameters in Sec-tion 4.2.3964.1 Using Candidate ContextsThe previous model assigns the same likelihood toevery instance of a given word.
As we noted insection 3.2.2, a word may be polysemous.
To takeinto account a word?s context, we can instead com-pute P (in(w,?Q)|w, in(w,?w)), where ?w is theset of contexts for the candidate word w in a re-trieved passage.By introducing word clusters as intermediatevariables as before and making a similar assump-tion as in equation (3), we obtain:P (in(w,?Q)|w, in(w,?w))=XCP (in(w,?Q), C|w, in(w,?w)) (6)?XCP (C|w, in(w,?w))P (in(C,?Q)|C) (7)Like equation (4), equation (7) partitions themodel into two parts.
Unlike P (C|w) in equation(4), the probability of the cluster is now based onthe particular occurrence of the word in the candi-date contexts.
It can be estimated by:P (C|w, in(w,?w))= P (in(w,?w)|w,C)P (w,C)P (in(w,?w)|w)P (w)(8)?Y?w?
?wP (in(w, ?w)|w,C)Y?w?
?wP (in(w, ?w)|w)?
P (C|w) (9)=Y?w?
?w?P (C|w, in(w, ?w))P (C|w)??
P (C|w) (10)4.2 Estimating ParametersOur probabilistic model requires the parametersP (C|w), P (C|w, in(w, ?
)), and P (in(C, ?
)|C),wherew is a word,C is a cluster thatw belongs to,and ?
is a question or candidate context.
This sec-tion explains how these parameters are estimatedwithout using labeled data.The context-filler database described in Sec-tion 3.2 provides the joint and marginal fre-quency counts of contexts and words (|in(?, w)|,|in(?, ?
)| and |in(w, ?)|).
These counts al-low us to compute the probabilities P (in(w, ?
)),P (in(w, ?
)), and P (in(?, ?)).
We can also com-pute P (in(w, ?
)|w), which is smoothed with add-one smoothing (see equation (11) in Figure 2).The estimation of P (C|w) presents a challenge.We have no corpus from which we can directlymeasure P (C|w) because word instances are notlabeled with their clusters.P (in(w, ?
)|w) = |in(w, ?
)|+ P (in(?, ?
))|in(w, ?
)|+ 1 (11)Pu(C|w) =(1|{C?|w?C?
}| if w ?
C,0 otherwise (12)P (C|w) =Xw??S(w)sim(w,w?)?
Pu(C|w?)X{C?|w?C?},w??S(w)sim(w,w?)?
Pu(C?|w?
)(13)P (in(C, ?
)|C) =Xw?
?CP (C|w?)?
|in(w?, ?
)|+ P (in(?, ?))Xw?
?CP (C|w?)?
|in(w?, ?
)|+ 1(14)Figure 2: Probability estimationWe use the average weighted ?guesses?
of thetop similar words of w to compute P (C|w) (seeequation 13).
The intuition is that if w?
and ware similar words, P (C|w?)
and P (C|w) tendto have similar values.
Since we do not knowP (C|w?)
either, we substitute it with uniform dis-tribution Pu(C|w?)
as in equation (12) of Fig-ure 2.
Although Pu(C|w?)
is a very crude guess,the weighted average of a set of such guesses canoften be quite accurate.The similarities between words are obtained asa byproduct of the CBC algorithm.
For each word,we use S(w) to denote the top-n most similarwords (n=50 in our experiments) and sim(w,w?
)to denote the similarity between words w and w?.The following is a sample similar word list for theword suit:S(suit) = {lawsuit 0.49, suits 0.47, com-plaint 0.29, lawsuits 0.27, jacket 0.25, coun-tersuit 0.24, counterclaim 0.24, pants 0.24,trousers 0.22, shirt 0.21, slacks 0.21, case0.21, pantsuit 0.21, shirts 0.20, sweater 0.20,coat 0.20, ...}The estimation for P (C|w, in(w, ?w)) is sim-ilar to that of P (C|w) except that instead of allw?
?
S(w), we instead use {w?|w?
?
S(w) ?in(w?, ?w)}.
By only looking at a particular con-text ?w, we may obtain a different distribution overC than P (C|w) specifies.
In the event that the dataare too sparse to estimate P (C|w, in(w, ?w)), wefall back to using P (C|w).P (in(C, ?
)|C) is computed in (14) by assum-ing each instance of w contains a fractional in-stance of C and the fractional count is P (C|w).Again, add-one smoothing is used.397System Median % Top 1% Top 5% Top 10% Top 50%Oracle 0.7% 89 (57%) 123 (79%) 131 (85%) 154 (99%)Frequency 7.7% 31 (20%) 67 (44%) 86 (56%) 112 (73%)Our model 1.2% 71 (46%) 106 (69%) 119 (77%) 146 (95%)no cand.
contexts 2.2% 58 (38%) 102 (66%) 113 (73%) 145 (94%)ANNIE 4.0% 54 (35%) 79 (51%) 93 (60%) 123 (80%)Table 2: Summary of Results5 Experimental Setup & ResultsWe evaluate our answer typing system by usingit to filter the contents of documents retrieved bythe information retrieval portion of a question an-swering system.
Each answer candidate in the setof documents is scored by the answer typing sys-tem and the list is sorted in descending order ofscore.
We treat the system as a filter and observethe proportion of candidates that must be acceptedby the filter so that at least one correct answer isaccepted.
A model that allows a low percentageof candidates to pass while still allowing at leastone correct answer through is favorable to a modelin which a high number of candidates must pass.This represents an intrinsic rather than extrinsicevaluation (Molla?
and Hutchinson, 2003) that webelieve illustrates the usefulness of our model.The evaluation data consist of 154 questionsfrom the TREC-2003 QA Track (Voorhees, 2003)satisfying the following criteria, along with the top10 documents returned for each question as iden-tified by NIST using the PRISE1 search engine.?
the question begins with What, Which, orWho.
We restricted the evaluation such ques-tions because our system is designed to dealwith questions whose answer types are oftensemantically open-ended noun phrases.?
There exists entry for the question in the an-swer patterns provided by Ken Litkowski2.?
One of the top-10 documents returned byPRISE contains a correct answer.We compare the performance of our prob-abilistic model with that of two other sys-tems.
Both comparison systems make use of asmall, predefined set of manually-assigned MUC-7 named-entity types (location, person, organiza-tion, cardinal, percent, date, time, duration, mea-sure, money) augmented with thing-name (proper1www.itl.nist.gov/iad/894.02/works/papers/zp2/zp2.html2trec.nist.gov/data/qa/2003 qadata/03QA.tasks/t12.pats.txtnames of inanimate objects) and miscellaneous(a catch-all answer type of all other candidates).Some examples of thing-name are Guinness Bookof World Records, Thriller, Mars Pathfinder, andGrey Cup.
Examples of miscellaneous answers arecopper, oil, red, and iris.The differences in the comparison systems iswith respect to how entity types are assigned to thewords in the candidate documents.
We make useof the ANNIE (Maynard et al, 2002) named entityrecognition system, along with a manual assigned?oracle?
strategy, to assign types to candidate an-swers.
In each case, the score for a candidate iseither 1 if it is tagged as the same type as the ques-tion or 0 otherwise.
With this scoring scheme pro-ducing a sorted list we can compute the probabilityof the first correct answer appearing at rankR = kas follows:P (R = k) =k?2Yi=0?t?
c?
it?
i?ct?
k + 1 (15)where t is the number of unique candidate answersthat are of the appropriate type and c is the numberof unique candidate answers that are correct.Using the probabilities in equation (15), wecompute the expected rank, E(R), of the first cor-rect answer of a given question in the system as:E(R) =t?c+1Xk=1kP (R = k) (16)Answer candidates are the set of ANNIE-identified tokens with stop words and punctuationremoved.
This yields between 900 and 8000 can-didates for each question, depending on the top 10documents returned by PRISE.
The oracle systemrepresents an upper bound on using the predefinedset of answer types.
The ANNIE system repre-sents a more realistic expectation of performance.The median percentage of candidates that areaccepted by a filter over the questions of our eval-uation data provides one measure of performanceand is preferred to the average because of the ef-fect of large values on the average.
In QA, a sys-tem accepting 60% of the candidates is not signif-icantly better or worse than one accepting 100%,398System MeasureQuestion TypeAll Location Person Organization Thing-Name Misc Other(154) (57) (17) (19) (17) (37) (7)Our modelMedian 1.2% 0.8% 2.0% 1.3% 3.7% 3.5% 12.2%Top 1% 71 34 6 9 7 13 2Top 5% 106 53 11 11 10 19 2Top 10% 119 55 12 17 10 22 3Top 50% 146 56 16 18 17 34 5OracleMedian 0.7% 0.4% 1.0% 0.3% 0.4% 16.0% 0.3%Top 1% 89 44 8 16 14 1 6Top 5% 123 57 17 19 17 6 7Top 10% 131 57 17 19 17 14 7Top 50% 154 57 17 19 17 37 7ANNIEMedian 4.0% 0.6% 1.4% 6.1% 100% 16.7% 50.0%Top 1% 54 39 5 7 0 0 3Top 5% 79 53 12 9 0 2 3Top 10% 93 54 13 11 0 12 3Top 50% 123 56 16 15 5 28 3Table 3: Detailed breakdown of performancebut the effect on average is quite high.
Anothermeasure is to observe the number of questionswith at least one correct answer in the top N% forvarious values of N .
By examining the number ofcorrect answers found in the topN%we can betterunderstand what an effective cutoff would be.The overall results of our comparison can befound in Table 2.
We have added the results ofa system that scores candidates based on their fre-quency within the document as a comparison witha simple, yet effective, strategy.
The second col-umn is the median percentage of where the highestscored correct answer appears in the sorted candi-date list.
Low percentage values mean the answeris usually found high in the sorted list.
The re-maining columns list the number of questions thathave a correct answer somewhere in the top N%of their sorted lists.
This is meant to show the ef-fects of imposing a strict cutoff prior to runningthe answer type model.The oracle system performs best, as it bene-fits from both manual question classification andmanual entity tagging.
If entity assignment isperformed by an automatic system (as it is forANNIE), the performance drops noticeably.
Ourprobabilistic model performs better than ANNIEand achieves approximately 2/3 of the perfor-mance of the oracle system.
Table 2 also showsthat the use of candidate contexts increases theperformance of our answer type model.Table 3 shows the performance of the oraclesystem, our model, and the ANNIE system brokendown by manually-assigned answer types.
Dueto insufficient numbers of questions, the cardinal,percent, time, duration, measure, and money typesare combined into an ?Other?
category.
Whencompared with the oracle system, our model per-forms worse overall for questions of all types ex-cept for those seeking miscellaneous answers.
Formiscellaneous questions, the oracle identifies alltokens that do not belong to one of the otherknown categories as possible answers.
For allquestions of non-miscellaneous type, only a smallsubset of the candidates are marked appropriate.In particular, our model performs worse than theoracle for questions seeking persons and thing-names.
Person questions often seek rare personnames, which occur in few contexts and are diffi-cult to reliably cluster.
Thing-name questions areeasy for a human to identify but difficult for au-tomatic system to identify.
Thing-names are a di-verse category and are not strongly associated withany identifying contexts.Our model outperforms the ANNIE system ingeneral, and for questions seeking organizations,thing-names, and miscellaneous targets in partic-ular.
ANNIE may have low coverage on organi-zation names, resulting in reduced performance.Like the oracle, ANNIE treats all candidates notassigned one of the categories as appropriate formiscellaneous questions.
Because ANNIE cannotidentify thing-names, they are treated as miscella-neous.
ANNIE shows low performance on thing-names because words incorrectly assigned typesare sorted to the bottom of the list for miscella-neous and thing-name questions.
If a correct an-swer is incorrectly assigned a type it will be sortednear the bottom, resulting in a poor score.3996 ConclusionsWe have presented an unsupervised probabilisticanswer type model.
Our model uses contexts de-rived from the question and the candidate answerto calculate the appropriateness of a candidate an-swer.
Statistics gathered from a large corpus oftext are used in the calculation, and the model isconstructed to exploit these statistics without be-ing overly specific or overly general.The method presented here avoids the use of anexplicit list of answer types.
Explicit answer typescan exhibit poor performance, especially for thosequestions not fitting one of the types.
They mustalso be redefined when either the domain or corpussubstantially changes.
By avoiding their use, ouranswer typing method may be easier to adapt todifferent corpora and question answering domains(such as bioinformatics).In addition to operating as a stand-alone answertyping component, our system can be combinedwith other existing answer typing strategies, es-pecially in situations in which a catch-all answertype is used.
Our experimental results show thatour probabilistic model outperforms the oracle anda system using automatic named entity recognitionunder such circumstances.
The performance ofour model is better than that of the semi-automaticsystem, which is a better indication of the expectedperformance of a comparable real-world answertyping system.AcknowledgmentsThe authors would like to thank the anonymous re-viewers for their helpful comments on improvingthe paper.
The first author is supported by the Nat-ural Sciences and Engineering Research Councilof Canada, the Alberta Ingenuity Fund, and the Al-berta Informatics Circle of Research Excellence.ReferencesP.F.
Brown, V.J.
Della Pietra, P.V.
deSouza, J.C. Lai, and R.L.Mercer.
1990.
Class-based n-gram Models of NaturalLanguage.
Computational Linguistics, 16(2):79?85.Y.
Choueka and S. Lusignan.
1985.
Disambiguation by ShortContexts.
Computer and the Humanities, 19:147?157.K.
Church and P. Hanks.
1989.
Word Association Norms,Mutual Information, and Lexicography.
In Proceedingsof ACL-89, pages 76?83, Vancouver, British Columbia,Canada.H.
Cui, K. Li, R. Sun, T-S. Chua, and M-K. Kan. 2004.
Na-tional University of Singapore at the TREC-13 QuestionAnswering Main Task.
In Notebook of TREC 2004, pages34?42, Gaithersburg, Maryland.D.R.
Cutting, D. Karger, J. Pedersen, and J.W.
Tukey.
1992.Scatter/Gather: A Cluster-based Approach to BrowsingLarge Document Collections.
In Proceedings of SIGIR-92, pages 318?329, Copenhagen, Denmark.A.
Echihabi, U. Hermjakob, E. Hovy, D. Marcu, E. Melz,and D. Ravichandran.
2003.
Multiple-Engine QuestionAnswering in TextMap.
In Proceedings of TREC 2003,pages 772?781, Gaithersburg, Maryland.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, Massachusetts.M.A.
Greenwood.
2004.
AnswerFinder: Question Answer-ing from your Desktop.
In Proceedings of the SeventhAnnual Colloquium for the UK Special Interest Groupfor Computational Linguistics (CLUK ?04), University ofBirmingham, UK.S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden,J.
Williams, and J. Bensley.
2003.
Answer Mining byCombining Extraction Techniques with Abductive Rea-soning.
In Proceedings of TREC 2003, pages 375?382,Gaithersburg, Maryland.U.
Hermjakob.
2001.
Parsing and Question Classification forQuestion Answering.
In Proceedings of the ACL Work-shop on Open-Domain Question Answering, Toulouse,France.A.
Ittycheriah, M. Franz, W-J.
Zhu, and A. Ratnaparkhi.2001.
Question Answering Using Maximum EntropyComponents.
In Proceedings of NAACL 2001, Pittsburgh,Pennsylvania.G.
Karypis, E.-H. Han, and V. Kumar.
1999.
Chameleon: AHierarchical Clustering Algorithm using Dynamic Model-ing.
IEEE Computer: Special Issue on Data Analysis andMining, 32(8):68?75.V.
Krishnan, S. Das, and S. Chakrabarti.
2005.
EnhancedAnswer Type Inference from Questions using SequentialModels.
In Proceedings of HLT/EMNLP 2005, pages315?322, Vancouver, British Columbia, Canada.X.
Li and D. Roth.
2002.
Learning Question Classifiers.In Proceedings of COLING 2002, pages 556?562, Taipei,Taiwan.M.
Light, G. Mann, E. Riloff, and E. Breck.
2001.
Analysesfor Elucidating Current Question Answering Technology.Natural Language Engineering, 7(4):325?342.D.
Lin and P. Pantel.
2001.
Discovery of Inference Rulesfor Question Answering.
Natural Language Engineering,7(4):343?360.D.
Lin.
1998.
Automatic Retrieval and Clustering of SimilarWords.
In Proceedings of COLING-ACL 1998, Montreal,Que?bec, Canada.D.
Lin.
2001.
Language and Text Analysis Tools.
In Pro-ceedings of HLT 2001, pages 222?227, San Diego, Cali-fornia.D.
Maynard, V. Tablan, H. Cunningham, C. Ursu, H. Sag-gion, K. Bontcheva, and Y. Wilks.
2002.
ArchitecturalElements of Language Engineering Robustness.
NaturalLanguage Engineering, 8(2/3):257?274.D.
Molla?
and B. Hutchinson.
2003.
Intrinsic versus ExtrinsicEvaluations of Parsing Systems.
In Proceedings of EACLWorkshop on Evaluation Initiatives in Natural LanguageProcessing, pages 43?50, Budapest, Hungary.P.
Pantel and D. Lin.
2002.
Document Clustering with Com-mittees.
In Proceedings of SIGIR 2002, pages 199?206,Tampere, Finland.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distributional Clus-tering of English Words.
In Proceedings of ACL 1992,pages 183?190.D.
Radev, W. Fan, H. Qi, H. Wu, and A. Grewal.
2002.
Prob-ablistic Question Answering on the Web.
In Proceedingsof the Eleventh International World Wide Web Conference.E.M.
Voorhees.
2003.
Overview of the TREC 2003 Ques-tion Answering Track.
In Proceedings of TREC 2003,Gaithersburg, Maryland.400
