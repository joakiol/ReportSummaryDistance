Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 160?168,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsA Reranking Model for Discourse Segmentation using Subtree FeaturesNgo Xuan Bach, Nguyen Le Minh, Akira ShimazuSchool of Information ScienceJapan Advanced Institute of Science and Technology1-1 Asahidai, Nomi, Ishikawa, 923-1292, Japan{bachnx,nguyenml,shimazu}@jaist.ac.jpAbstractThis paper presents a discriminative rerankingmodel for the discourse segmentation task, thefirst step in a discourse parsing system.
Ourmodel exploits subtree features to rerank N-best outputs of a base segmenter, which usessyntactic and lexical features in a CRF frame-work.
Experimental results on the RST Dis-course Treebank corpus show that our modeloutperforms existing discourse segmenters inboth settings that use gold standard Penn Tree-bank parse trees and Stanford parse trees.1 IntroductionDiscourse structure has been shown to have an im-portant role in many natural language applications,such as text summarization (Marcu, 2000; Louis etal., 2010), information presentation (Bateman et al,2001), question answering (Sun and Chai, 2007),and dialogue generation (Hernault et al, 2008).
Toproduce such kinds of discourse structure, severalattempts have been made to build discourse parsersin the framework of Rhetorical Structure Theory(RST) (Mann and Thompson, 1988), one of themost widely used theories of text structure.In the RST framework, a text is first divided intoseveral elementary discourse units (EDUs).
EachEDU may be a simple sentence or a clause in a com-plex sentence.
Consecutive EDUs are then put inrelation with each other to build a discourse tree.Figure 1 shows an example of a discourse tree withthree EDUs.
The goal of the discourse segmentationtask is to divide the input text into such EDUs.Figure 1: A discourse tree (Soricut and Marcu, 2003).The quality of the discourse segmenter con-tributes a significant part to the overall accuracy ofevery discourse parsing system.
If a text is wronglysegmented, no discourse parsing algorithm can builda correct discourse tree.Existing discourse segmenters usually exploit lex-ical and syntactic features to label each word in asentence with one of two labels, boundary or no-boundary.
The limitation of this approach is that itonly focuses on the boundaries of EDUs.
It cannotcapture features that describe whole EDUs.Recently, discriminative reranking has been usedsuccessfully in some NLP tasks such as POS tag-ging, chunking, and statistical parsing (Collins andKoo, 2005; Kudo et al, 2005; Huang, 2008; Fraseret al, 2009).
The advantage of the reranking methodis that it can exploit the output of a base model tolearn.
Based on that output, we can extract long-distance non-local features to rerank.In this paper, we present a reranking model forthe discourse segmentation task.
We show how touse subtree features, features extracted from wholeEDUs, to rerank outputs of a base model.
Exper-imental results on RST Discourse Treebank (RST-DT) (Carlson et al, 2002) show that our model out-160performs existing systems.The rest of this paper is organized as follows.
Sec-tion 2 summarizes related work.
Section 3 presentsour method.
Experimental results on RST-DT aredescribed in Section 4.
Finally, Section 5 gives con-clusions.2 Related WorkSeveral methods have been proposed to deal with thediscourse segmentation task.
Thanh et al (2004)present a rule-based discourse segmenter with twosteps.
In the first step, segmentation is done by us-ing syntactic relations between words.
The segmen-tation algorithm is based on some principles, whichhave been presented in Corston (1998) and Carlsonand Marcu (2001), as follows:1.
The clause that is attached to a noun phrasecan be recognised as an embedded unit.
If theclause is a subordinate clause, it must containmore than one word.2.
Coordinate clauses and coordinate sentencesof a complex sentence are EDUs.3.
Coordinate clauses and coordinate ellipticalclauses of verb phrases (VPs) are EDUs.
Co-ordinate VPs that share a direct object with themain VP are not considered as a separate dis-course segment.4.
Clausal complements of reported verbs andcognitive verbs are EDUs.The segmenter then uses cue phrases to correct theoutput of the first step.Tofiloski et al (2009) describe another rule-baseddiscourse segmenter.
The core of this segmenterconsists of 12 syntactic segmentation rules and somerules concerning a list of stop phrases, discourse cuephrases, and part-of-speech tags.
They also use alist of phrasal discourse cues to insert boundaries notderivable from the parser?s output.Soricut and Marcu (2003) introduce a statisti-cal discourse segmenter, which is trained on RST-DT to label words with boundary or no-boundarylabels.
They use lexical and syntactic features todetermine the probabilities of discourse boundariesP (bi|wi, t), where wi is the ith word of the inputsentence s, t is the syntactic parse tree of s, and bi ?
{boundary, no-boundary}.
Given a syntactic parsetree t, their algorithm inserts a discourse boundaryafter each word w for which P (boundary|w, t) >0.5.Another statistical discourse segmenter using arti-ficial neural networks is presented in Subba and DiEugenio (2007).
Like Soricut and Marcu (2003),they formulate the discourse segmentation task asa binary classification problem of deciding whethera word is the boundary or no-boundary of EDUs.Their segmenter exploits a multilayer perceptronmodel with back-propagation algorithm and is alsotrained on RST-DT.Hernault et al (2010) propose a sequential modelfor the discourse segmentation task, which considersthe segmentation task as a sequence labeling prob-lem rather than a classification problem.
They ex-ploit Conditional Random Fields (CRFs) (Laffertyet al, 2001) as the learning method and get state-of-the-art results on RST-DT.In our work, like Hernault et al (2010), we alsoconsider the discourse segmentation task as a se-quence labeling problem.
The final segmentationresult is selected among N-best outputs of a CRF-based model by using a reranking method with sub-tree features.3 Method3.1 Discriminative RerankingIn the discriminative reranking method (Collins andKoo, 2005), first, a set of candidates is generated us-ing a base model (GEN).
GEN can be any model forthe task.
For example, in the part-of-speech (POS)tagging problem, GEN may be a model that gener-ates all possible POS tags for a word based on a dic-tionary.
Then, candidates are reranked using a linearscore function:score(y) = ?
(y) ?Wwhere y is a candidate, ?
(y) is the feature vector ofcandidate y, and W is a parameter vector.
The finaloutput is the candidate with the highest score:F (x) = argmaxy?GEN(x)score(y)= argmaxy?GEN(x)?
(y) ?W.161To learn the parameter W we use the average per-ceptron algorithm, which is presented as Algorithm1.Algorithm 1 Average perceptron algorithm forreranking (Collins and Koo, 2005)1: Inputs: Training set {(xi, yi)|xi ?
Rn, yi ?C,?i = 1, 2, .
.
.
,m}2: Initialize: W ?
0,Wavg ?
03: Define: F (x) = argmaxy?GEN(x)?
(y) ?W4: for t = 1, 2, .
.
.
, T do5: for i = 1, 2, .
.
.
,m do6: zi ?
F (xi)7: if zi 6= yi then8: W ?W + ?(yi)?
?
(zi)9: end if10: Wavg ?Wavg + W11: end for12: end for13: Wavg ?Wavg/(mT )14: Output: Parameter vector Wavg.In the next sections we will describe our basemodel and features that we use to rerank candidates.3.2 Base ModelSimilar to the work of Hernault et al (2010), ourbase model uses Conditional Random Fields1 tolearn a sequence labeling model.
Each label is eitherbeginning of EDU (B) or continuation of EDU (C).Soricut and Marcu (2003) and Subba and Di Euge-nio (2007) use boundary labels, which are assignedto words at the end of EDUs.
Like Hernault et al(2010), we use beginning labels, which are assignedto words at the beginning of EDUs.
However, wecan convert an output with boundary, no-boundarylabels to an output with beginning, continuation la-bels and vice versa.
Figure 2 shows two examples ofsegmenting a sentence into EDUs and their correctlabel sequences.We use the following lexical and syntactic infor-mation as features: words, POS tags, nodes in parsetrees and their lexical heads and their POS heads2.When extracting features for word w, let r be the1We use the implementation of Kudo (Kudo, CRF++).2Lexical heads are extracted using Collins?
rules (Collins,1999).Figure 2: Examples of segmenting sentences into EDUs.word on the right-hand side of w and Np be the deep-est node that belongs to both paths from the root tow and r. Nw and Nr are child nodes of Np thatbelong to two paths, respectively.
Figure 3 showstwo partial lexicalized syntactic parse trees.
In thefirst tree, if w = says then r = it, Np = V P (says),Nw = V BZ(says), and Nr = SBAR(will).
Wealso consider the parent and the right-sibling of Npif any.
The final feature set for w consists of not onlyfeatures extracted from w but also features extractedfrom two words on the left-hand side and two wordson the right-hand side of w.Our feature extraction method is different fromthe method in previous work (Soricut and Marcu,2003; Hernault et al, 2010).
They define Nw as thehighest ancestor of w that has lexical head w and hasa right-sibling.
Then Np and Nr are defined as theparent and right-sibling of Nw.
In the first example,our method gives the same results as the previousone.
In the second example, however, there is nonode with lexical head ?done?
and having a right-sibling.
The previous method cannot extract Nw,Np, and Nr in such cases.
We also use some newfeatures such as the head node and the right-siblingnode of Np.3.3 Subtree Features for RerankingWe need to decide which kinds of subtrees are usefulto represent a candidate, a way to segment the inputsentence into EDUs.
In our work, we consider twokinds of subtrees: bound trees and splitting trees.The bound tree of an EDU, which spans fromword u to word w, is a subtree which satisfies twoconditions:162Figure 3: Partial lexicalized syntactic parse trees.1.
its root is the deepest node in the parse treewhich belongs to both paths from the root ofthe parse tree to u and w, and2.
it only contains nodes in two those paths.The splitting tree between two consecutive EDUs,from word u to word w and from word r to wordv, is a subtree which is similar to a bound tree, butcontains two paths from the root of the parse tree tow and r. Hence, a splitting tree between two con-secutive EDUs is a bound tree that only covers twowords: the last word of the first EDU and the firstword of the second EDU.
Bound trees will cover thewhole EDUs, while splitting trees will concentrateon the boundaries of EDUs.From a bound tree (similar to a splitting tree), weextract three kinds of subtrees: subtrees on the leftpath (left tree), subtrees on the right path (right tree),and subtrees consisting of a subtree on the left pathand a subtree on the right path (full tree).
In thethird case, if both subtrees on the left and right pathsdo not contain the root node, we add a pseudo rootnode.
Figure 4 shows the bound tree of EDU ?noth-ing was done?
of the second example in Figure 3,and some examples of extracted subtrees.Each subtree feature is then represented by astring as follows:?
A left tree (or a right tree) is represented byconcatenating its nodes with hyphens betweennodes.
For example, subtrees (b) and (e) in Fig-ure 4 can be represented as follows:S-NP-NN-nothing, andS-VP-VP-VBN-done.?
A full tree is represented by concatenating itsleft tree and right tree with string ### in themiddle.
For example, subtrees (g) and (h) inFigure 4 can be represented as follows:S-NP-NN###S-VP-VP-VBN, andNP-NN-nothing###VP-VP-VBN.The feature set of a candidate is the set of all sub-trees extracted from bound trees of all EDUs andsplitting trees between two consecutive EDUs.Among two kinds of subtrees, splitting trees canbe computed between any two adjacent words andtherefore can be incorporated into the base model.However, if we do so, the feature space will be verylarge and contains a lot of noisy features.
Becausemany words are not a boundary of any EDU, manysubtrees extracted by this method will never be-come a real splitting tree (tree that splits two EDUs).Splitting trees extracted in the reranking model willfocus on a small but compact and useful set of sub-trees.4 Experiments4.1 Data and Evaluation MethodsWe tested our model on the RST Discourse Treebankcorpus.
This corpus consists of 385 articles from thePenn Treebank, which are divided into a Trainingset and a Test set.
The Training set consists of 347articles (6132 sentences), and the Test set consists of38 articles (991 sentences).There are two evaluation methods that have beenused in previous work.
The first method measuresonly beginning labels (B labels) (Soricut and Marcu,2003; Subba and Di Eugenio, 2007).
The second163Figure 4: Subtree features.method (Hernault et al, 2010) measures both be-ginning and continuation labels (B and C labels)3.This method first calculates scores on B labels andscores on C labels, and then produces the average ofthem.
Due to the number of C labels being muchhigher than the number of B labels, the second eval-uation method yields much higher results.
In Her-nault et al (2010), the authors compare their systemswith previous work despite using different evalua-tion methods.
Such comparisons are not valid.
Inour work, we measure the performance of the pro-posed model using both methods.4.2 Experimental ResultsWe learned the base model on the Training set andtested on the Test set to get N-best outputs to rerank.To learn parameters of the reranking model, we con-ducted 5-fold cross-validation tests on the Trainingset.
In all experiments, we set N to 20.
To choosethe number of iterations, we used a development set,which is about 20 percent of the Training set.Table 1 shows experimental results when evaluat-ing only beginning (B) labels, in which SPADE isthe work of Soricut and Marcu (2003), NNDS is asegmenter that uses neural networks (Subba and DiEugenio, 2007), and CRFSeg is a CRF-based seg-menter (Hernault et al, 2010).
When using goldparse trees, our base model got 92.5% in the F1score, which improves 1.3% compared to the state-of-the-art segmenter (CRFSeg).
When using Stan-ford parse trees (Klein and Manning, 2003), ourbase model improved 1.7% compared to CRFSeg.It demonstrates the effectiveness of our feature ex-3Neither evaluation method counts sentence boundaries.Table 1: Performance when evaluating on B labelsModel Trees Pre(%) Re(%) F1(%)SPADE Penn 84.1 85.4 84.7NNDS Penn 85.5 86.6 86.0CRFSeg Penn 92.7 89.7 91.2Base Penn 92.5 92.5 92.5Reranking Penn 93.1 94.2 93.7CRFSeg Stanford 91.0 87.2 89.0Base Stanford 91.4 90.1 90.7Reranking Stanford 91.5 90.4 91.0Human - 98.5 98.2 98.3traction method in the base model.
As expected,our reranking model got higher results comparedto the base model in both settings.
The rerank-ing model got 93.7% and 91.0% in two settings,which improves 2.5% and 2.0% compared to CRF-Seg.
Also note that, when using Stanford parse trees,our reranking model got competitive results withCRFSeg when using gold parse trees (91.0% com-pared to 91.2%).Table 2 shows experimental results when evaluat-ing on both beginning and continuation labels.
Ourmodels also outperformed CRFSeg in both settings,using gold parse trees and using Stanford parse trees(96.6% compared to 95.3% in the first setting, and95.1% compared to 94.1% in the second setting).Both evaluation methods have a weak point inthat they do not measure the ability to find EDUsexactly.
We suggest that the discourse segmenta-tion task should be measured on EDUs rather thanboundaries of EDUs.
Under this evaluation scheme,our model achieved 90.0% and 86.2% when using164Table 2: Performance when evaluating on B and C labelsModel Trees Pre(%) Re(%) F1(%)CRFSeg Penn 96.0 94.6 95.3Base Penn 96.0 96.0 96.0Reranking Penn 96.3 96.9 96.6CRFSeg Stanford 95.0 93.2 94.1Base Stanford 95.3 94.7 95.0Reranking Stanford 95.4 94.9 95.1gold parse trees and Stanford parse trees, respec-tively.We do not compare our segmenter to systems de-scribed in Thanh et al (2004) and Tofiloski et al(2009).
Thanh et al (2004) evaluated their systemon only 8 texts of RST-DT with gold standard parsetrees.
They achieved 81.4% and 79.2% in the preci-sion and recall scores, respectively.
Tofiloski et al(2009) tested their system on only 3 texts of RST-DTand used different segmentation guidelines.
Theyreported a precision of 82.0% and recall of 86.0%when using Stanford parse trees.An important question is which subtree featureswere useful for the reranking model.
This questioncan be answered by looking at the weights of sub-tree features (the parameter vector learned by theaverage perceptron algorithm).
Table 3 shows 30subtree features with the highest weights in absolutevalue.
These features are thus useful for rerankingcandidates in the reranking model.
We can see thatmost subtree features at the top are splitting trees,so splitting trees have a more important role thanbound trees in our model.
Among three types ofsubtrees (left tree, right tree, and full tree), full treeis the most important type.
It is understandable be-cause subtrees in this type convey much informa-tion; and therefore describe splitting trees and boundtrees more precise than subtrees in other types.4.3 Error AnalysisThis section discusses the cases in which our modelfails to segment discourses.
Note that all errors be-long to one of two types, over-segmentation type(i.e., words that are not EDU boundaries are mis-taken for boundaries) and miss-segmentation type(i.e., words that are EDU boundaries are mistakenfor not boundaries).Table 4: Top error wordsWord Percentage among all errors (%)to 14.5and 5.8that 4.6the 4.6?
3.5he 2.3it 2.3of 2.3without 2.3?
1.7as 1.7if 1.7they 1.7when 1.7a 1.2Tabel 4 shows 15 most frequent words for whichour model usually makes a mistake and their per-centage among all segmentation errors.
Most errorsare related to coordinating conjunctions and subor-dinators (and, that, as, if, when), personal pronouns(he, it, they), determiners (the, a), prepositions (of,without), punctuations (quotes and hyphens), andthe word to.Figure 5 shows some errors made by our model.In these examples, gold (correct) EDU boundariesare marked by bracket squares ([]), while predictedboundaries made by our model are indicated by ar-rows (?
or ?).
A down arrow (?)
shows a boundarywhich is predicted correctly, while an up arrow (?
)indicates an over-segmentation error.
A boundarywith no arrow means a miss-segmentation error.
Forexample, in Sentence 1, we have a correct boundaryand an over-segmentation error.
Sentences 2 and 3show two over-segmentation errors, and sentences 4and 6 show two miss-segmentation errors.We also note that many errors occur right afterpunctuations (commas, quotes, hyphens, brackets,and so on).
We analyzed statistics on words thatappear before error words.
Table 5 shows 10 mostfrequent words and their percentage among all er-rors.
Overall, more than 35% errors occur right afterpunctuations.165Table 3: Top 30 subtree features with the highest weightsType of tree Type of subtree Subtree feature WeightSplitting tree Full tree NP###NP-VP 23.0125Splitting tree Full tree VP###S-VP 19.3044Splitting tree Full tree NP###VBN 18.3862Splitting tree Right tree VP -18.3723Splitting tree Full tree NP###SBAR 17.7119Splitting tree Full tree NP###NP-SBAR 17.0678Splitting tree Full tree NP###, -16.6763Splitting tree Full tree NP###VP 15.9934Splitting tree Left tree NP-VP 15.2849Splitting tree Full tree NP###NP 15.1657Splitting tree Right tree SBAR 14.6778Splitting tree Full tree NP###S-NP 14.4962Splitting tree Full tree NP###S 13.1656Bound tree Full tree S-PP###, 12.7428Splitting tree Full tree NP###NP-VP-VBN 12.5210Bound tree Full tree NP###NP -12.4723Bound tree Full tree VP###VP -12.1918Splitting tree Full tree NP-VP###S 12.1367Splitting tree Right tree NP-VP 12.0929Splitting tree Full tree NP-SBAR###VP 12.0858Splitting tree Full tree NP-SBAR-S###VP 12.0858Splitting tree Full tree VP###VP-VP -12.0338Bound tree Full tree VBG###.
11.9067Bound tree Right tree : 11.8833Bound tree Full tree VP###S -11.7624Bound tree Full tree S###VP -11.7596Bound tree Full tree ?###?
11.5524Bound tree Full tree S###, 11.5274Splitting tree Full tree NP###VP-VBN 11.3342Bound tree Left tree 0 11.2878Figure 5: Some errors made by our model.166Table 5: Most frequent words that appear before errorwordsWord Percentage among all errors (%), 24.9?
5.2?
2.3time 1.7) 1.2assets 1.2investors 1.2month 1.2plan 1.2was 1.25 ConclusionThis paper presented a reranking model for the dis-course segmentation task.
Our model exploits sub-tree features to rerank N-best outputs of a basemodel, which uses CRFs to learn.
Compared withthe state-of-the-art system, our model reduces 2.5%among 8.8% errors (28.4% in the term of error rate)when using gold parse trees, and reduces 2% among11% errors (18.2% in the term of error rate) whenusing Stanford parse trees.
In the future, we willbuild a discourse parser that uses the described dis-course segmenter.AcknowledgmentsThis work was partially supported by the 21st Cen-tury COE program ?Verifiable and Evolvable e-Society?, and Grant-in-Aid for Scientific Research,Education and Research Center for Trustworthy e-Society.The authors would like to thank the three anony-mous reviewers for the time they spent reading andreviewing this paper and Michael Strube for hiscomments during the revision process of the paper.ReferencesJ.
Bateman, J. Kleinz, T. Kamps, and K. Reichenberger.2001.
Towards Constructive Text, Diagram, and Lay-out Generation for Information Presentation.
Compu-tational Linguistics, 27(3), pp.
409-449.L.
Carlson and D. Marcu.
2001.
Discourse TaggingManual.
ISI Technical Report, ISI-TR-545.L.
Carlson, D. Marcu, and M.E.
Okurowski.
2002.RST Discourse Treebank.
Linguistic Data Consor-tium (LDC).M.
Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D. Thesis, Universityof Pennsylvania.S.
Corston-Oliver.
1998.
Computing Representations ofthe Structure of Written Discourse.
Ph.D. Thesis, Uni-versity of California, Santa Barbara.M.
Collins and T. Koo.
2005.
Discriminative Rerank-ing for Natural Language Parsing.
Computational Lin-guistics, 31(1), pp.
25-70.A.
Fraser, R. Wang, and H. Schu?tze.
2009.
Rich BitextProjection Features for Parse Reranking.
In Proceed-ings of the 12th Conference of the European Chapterof the ACL (EACL), pp.
282-290.H.
Hernault, P. Piwek, H. Prendinger, and M. Ishizuka.2008.
Generating Dialogues for Virtual Agents UsingNested Textual Coherence Relations.
In Proceedingsof IVA, pp.
139-145.H.
Hernault, D. Bollegala, and M. Ishizuka.
2010.
A Se-quential Model for Discourse Segmentation.
In Pro-ceedings of the 11th International Conference on Intel-ligent Text Processing and Computational Linguistics(CICLing), pp.
315-326.L.
Huang.
2008.
Forest Reranking: Discriminative Pars-ing with Non-Local Features.
In Proceedings of the46th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pp.
586-594D.
Klein and C. Manning.
2003.
Accurate UnlexicalizedParsing.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics (ACL),pp.
423-430.T.
Kudo.
CRF++: Yet Another CRF toolkit.
Available athttp://crfpp.sourceforge.net/T.
Kudo, J. Suzuki, and H. Isozaki.
2005.
Boosting-based parse reranking with subtree features.
In Pro-ceedings of the 43rd Annual Meeting of the Associ-ation for Computational Linguistics (ACL), pp.
189-196.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedings ofthe 18th International Conference on Machine Learn-ing (ICML), pp.282-289.A.
Louis, A. Joshi, and A. Nenkova 2010.
Discourseindicators for content selection in summarization.
InProceedings of the 11th annual SIGdial Meeting onDiscourse and Dialogue (SIGDIAL) , pp.147-156.W.C.
Mann and S.A. Thompson.
1988.
Rhetorical Struc-ture Theory.
Toward a Functional Theory of Text Or-ganization.
Text 8, pp.
243-281.D.
Marcu.
2000.
The Theory and Practice of DiscourseParsing and Summarization.
MIT Press, Cambridge.167R.
Soricut and D. Marcu.
2003.
Sentence Level Dis-course Parsing using Syntactic and Lexical Informa-tion.
In Proceedings of the North American Chap-ter of the Association for Computational Linguistics(NAACL), pp.
149-156.R.
Subba and B.
Di Eugenio.
2007.
Automatic DiscourseSegmentation using Neural Networks.
In Proceedingsof the Workshop on the Semantics and Pragmatics ofDialogue (SemDial), pp.
189-190.M.
Sun and J.Y.
Chai.
2007.
Discourse Processingfor Context Question Answering Based on Linguis-tic Knowledge.
Knowledge-Based Systems.
20(6), pp.511-526.H.L.
Thanh, G. Abeysinghe, and C. Huyck.
2004.
Auto-mated Discourse Segmentation by Syntactic Informa-tion and Cue Phrases.
In Proceedings of IASTED.M.
Tofiloski, J. Brooke, and M. Taboada.
2009.
ASyntactic and Lexical-Based Discourse Segmenter.
InProceedings of the Joint conference of the 47th AnnualMeeting of the Association for Computational Linguis-tics and the 4th International Joint Conference on Nat-ural Language Processing (ACL-IJCNLP), pp.
77-80.168
