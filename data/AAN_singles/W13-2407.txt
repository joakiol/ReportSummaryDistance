Proceedings of the 4th Biennial International Workshop on Balto-Slavic Natural Language Processing, pages 43?47,Sofia, Bulgaria, 8-9 August 2013. c?2010 Association for Computational LinguisticsGPKEX: Genetically Programmed Keyphrase Extractionfrom Croatian TextsMarko Bekavac and Jan S?najderUniversity of Zagreb, Faculty of Electrical Engineering and ComputingText Analysis and Knowledge Engineering LabUnska 3, 10000 Zagreb, Croatia{marko.bekavac2,jan.snajder}@fer.hrAbstractWe describe GPKEX, a keyphrase extrac-tion method based on genetic programming.We represent keyphrase scoring measuresas syntax trees and evolve them to pro-duce rankings for keyphrase candidates ex-tracted from text.
We apply and evalu-ate GPKEX on Croatian newspaper arti-cles.
We show that GPKEX can evolvesimple and interpretable keyphrase scoringmeasures that perform comparably to morecomplex machine learning methods previ-ously developed for Croatian.1 IntroductionKeyphrases are an effective way of summariz-ing document contents, useful for text categoriza-tion, document management, and search.
Unlikekeyphrase assignment, in which documents are as-signed keyphrases from a predefined taxonomy,keyphrase extraction selects phrases from the textof the document.
Extraction is preferred in caseswhen a taxonomy is not available or when its con-struction is not feasible, e.g., if the set of possiblekeyphrases is too large or changes often.
Manualkeyphrase extraction is extremely tedious and in-consistent, thus methods for automatic keyphraseextraction have attracted a lot of research interest.In this paper we describe GPKEX, a keyphraseextraction method based on genetic programming(GP), an evolutionary optimization technique in-spired by biological evolution (Koza and Poli,1992).
GP is similar to genetic algorithms exceptthat the individual solutions are expressions, ratherthan values.
We use GP to evolve keyphrase scor-ing measures, represented as abstract syntax trees.The advantage of using GP over black-box ma-chine learning methods is in the interpretability ofthe results: GP yields interpretable expressions,revealing the relevant features and their relation-ships, thus offering some insight into keyphraseusage.
Furthermore, GP can evolve simple scoringmeasures, providing an efficient alternative to morecomplex machine learning methods.We apply GPKEX to Croatian language and eval-uate it on a dataset of newspaper articles with man-ually extracted keyphrases.
Our results show thatGPKEX performs comparable to previous super-vised and unsupervised approaches for Croatian,but has the advantage of generating simple andinterpretable keyphrase scoring measures.2 Related WorkKeyphrase extraction typically consist of two steps:candidate extraction and candidate scoring.
Su-pervised approaches include decision tree models(Turney, 1999; Ercan and Cicekli, 2007), na?
?veBayes classifier (Witten et al 1999; McCallumand Nigam, 1998; Frank et al 1999), and SVM(Zhang et al 2006).
Unsupervised approachesinclude clustering (Liu et al 2009), graph-basedmethods (Mihalcea and Tarau, 2004), and languagemodeling (Tomokiyo and Hurst, 2003).
Manymore methods were proposed and evaluated withinthe SemEval shared task (Kim et al 2010).
Re-cent approaches (Jiang et al 2009; Wang and Li,2011; Eichler and Neumann, 2010) acknowledgekeyphrase extraction as a highly subjective task andframe it as a learning-to-rank problem.Keyphrase extraction for Croatian has been ad-dressed in both supervised and unsupervised set-ting.
Ahel et al(2009) use a na?
?ve Bayes clas-sifier with phrase position and tf-idf (term fre-quency/inverse document frequency) as features.Saratlija et al(2011) use distributional seman-tics to build topically related word clusters, fromwhich they extract keywords and expand them tokeyphrases.
Mijic?
et al(2010) use filtering basedon morphosyntactic tags followed by tf-idf scoring.43To the best of our knowledge, GPKEX is thefirst application of GP to keyphrase extraction.
Al-though we essentially approach the problem as aclassification task (we train on binary relevancejudgments), GPKEX produces continuous-valuedscoring measures, thus keyphrases can eventuallybe ranked and evaluated in a rank-based manner.3 GPKEXGPKEX (Genetically Programmed Keyphrase Ex-traction) consists of two steps: keyphrase candi-date extraction and the genetic programming ofkeyphrase scoring measures (KSMs).13.1 Step 1: Keyphrase candidate extractionKeyphrase candidate extraction starts with text pre-processing followed by keyphrase feature extrac-tion.
A keyphrase candidate is any sequence ofwords from the text that (1) does not span over(sub)sentence boundaries and (2) matches any ofthe predefined POS patterns (sequences of POStags).
The POS patterns are chosen based on theanalysis of the training set (cf.
Section 4).After the candidates have been extracted, eachcandidate is assigned 11 features.
We distinguishbetween three groups of features.
The first groupare the frequency-based features: the relative termfrequency (the ratio between the number of phraseoccurrences in a document and the total numberof phrases in the document), inverse document fre-quency (the ratio between the total number of doc-uments in the training set and the number of doc-uments in which the phrase occurs), and the tf-idfvalue.
These features serve to eliminate the irrele-vant and non-discriminative phrases.
The secondgroup are the position-based features: the positionof the first occurrence of a phrase in the text (i.e.,the number of phrases in the text preceding the firstoccurrence of the candidate phrase), the positionof the last occurrence, the occurrence in documenttitle, and the number of occurrences in the first, sec-ond, and the last third of the document.
These fea-tures serve to capture the relation between phraserelevance and the distribution of the phase withinthe document.
The last group of features concernsthe keyphrase surface form: its length and the num-ber of discriminative words it contains (these beingdefined as the 10 words from the document withthe highest tf-idf score).1GPKEX is freely available for download fromhttp://takelab.fer.hr/gpkex3.2 Step 2: Genetic programmingGenetic expressions.
Each keyphrase scoringmeasure (KSM) corresponds to one genetic expres-sion, represented as a syntax tree (see Fig.
1).
Weuse the above-described keyphrase features as outernodes of an expression.
For inner nodes we usebinary (+, ?, ?, and /) and unary operators (log ?,?
?10, ?/10, 1/?).
We randomly generate the initialpopulation of KSMs and use fitness-proportionateselection to guide the evolution process.Fitness function.
The fitness function scoresKSMs according to their ability to extract cor-rect keyphrases.
We measure this by comparingthe extracted keyphrases against the gold-standardkeyphrases (cf.
Section 4).
We experimented with anumber of fitness functions; simple functions, suchas Precision at n (P@n) or Mean Reciprocal Rank(MRR), did not give satisfactory results.
Instead,we define the fitness of a KSM s asf(s) =1|D|?d?D??
?|Ckd |minRank(Ckd )Ckd 6= ?,1minRank(C?d )otherwise(1)where D is the set of training documents, Ckd isthe set of correct keyphrases within top k-rankedkeyphrases extracted from document d ?
D, andminRank(Ckd ) is the highest rank (the smallestnumber) of keyphrase from set Ckd .
Parameter kdefines a cutoff threshold, i.e., keyphrase ranked be-low rank k are discarded.
If two KSMs extract thesame number of correct keyphrases in top k results,the one with the highest-ranked correct keyphrasewill be scored higher.
To ensure that the gradient ofthe fitness function is non-zero, a KSM that extractsno correct keyphrases within the first k results isassigned a score based on the complete set of cor-rectly extracted keyphrases (denoted C?d ).
The fit-ness scores are averaged over the whole documentcollection.
Based on preliminary experiments, weset the cutoff value to k = 15.Parsimony pressure.
Supervised models oftenface the problem of overfitting.
In GP, overfitting istypically controlled by parsimony pressure, a regu-larization term that penalizes complex expressions.We define the regularized fitness function asfreg =f1 +N/?
(2)where f is the non-regularized fitness functiongiven by (1), N is the number of nodes in the ex-pression, and parameter ?
defines the strength of44parsimony pressure.
Note that in both regularizedand non-regularized case we limit the size of anexpression to a maximum depth of 17, which isoften used as the limit (Riolo and Soule, 2009).Crossover and mutation.
Two expressions cho-sen for crossover exchange subtrees rooted at ran-dom nodes, resulting in a child expression withparts from both parent expressions.
We use a pop-ulation of 500 expressions and limit the numberof generations to 50, as we observe that resultsstagnate after that point.
To retain the quality ofsolution throughout the generations, we employ theelitist strategy and copy the best-fitted individualinto the next generation.
Moreover, we use muta-tion to prevent early local optimum trapping.
Weimplement mutation as a randomly grown subtreerooted at a randomly chosen node.
Each expressionhas a 5% probability of being mutated, with 10%probability of mutation at inner nodes.4 EvaluationData set and preprocessing.
We use the datasetdeveloped by Mijic?
et al(2010), comprising 1020Croatian newspaper articles provided by the Croat-ian News Agency.
The articles have been manuallyannotated by expert annotators, i.e., each documenthas an associated list of keyphrases.
The number ofextracted keyphrases per document varies between1 and 7 (3.4 on average).
The dataset is dividedin two parts: 960 documents each annotated by asingle annotator and 60 documents independentlyannotated by eight annotators.
We use the first partfor training and the second part for testing.Based on dataset analysis, we chose the follow-ing POS patterns for keyphrase candidate filtering:N, AN, NN, NSN, V, U (N ?
noun, A ?
adjective, S?
preposition, V ?
verb, U ?
unknown).
Although atotal of over 200 patterns would be needed to coverall keyphrases from the training set, we use onlythe six most frequent ones in order to reduce thenumber of candidates.
These patterns account forcca.
70% of keyphrases, while reducing the num-ber of candidates by cca.
80%.
Note that we choseto only extract keyphrases of three words or less,thereby covering 93% of keyphrases.
For lemmati-zation and (ambiguous) POS tagging, we use theinflectional lexicon from S?najder et al(2008), withadditional suffix removal after lemmatization.Evaluation methodology.
Keyphrase extractionis a highly subjective task and there is no agreed-upon evaluation methodology.
Annotators are ofteninconsistent: they extract different keyphrases andalso keyphrases of varying length.
What is more,an omission of a keyphrase by one of the annota-tors does not necessarily mean that the keyphraseis incorrect; it may merely indicate that it is less rel-evant.
To account for this, we use rank-based eval-uation measures.
As our method produces a rankedlist of keyphrases for each document, we can com-pare this list against a gold-standard keyphraseranking for each document.
We obtain the latterby aggregating the judgments of all annotators; themore annotators have extracted a keyphrase, thehigher its ranking will be.2 Following Zesch andGurevych (2009), we consider the morphologicalvariants when matching the keyphrases; however,we do not consider partial matches.To evaluate a ranked list of extracted keyphrases,we use the generalized average precision (GAP)measure proposed by Kishida (2005).
GAP gener-alizes average precision to multi-grade relevancejudgments: it takes into account both precision (allcorrect items are ranked before all incorrect ones)and the quality of ranking (more relevant items areranked before less relevant ones).Another way of evaluating against keyphrasesextracted by multiple annotators is to considerthe different levels of agreement.
We consider asstrong agreement the cases in which a keyphraseis extracted by at least five annotators, and asweak agreement the cases in which at least twoannotators have extracted a keyphrase.
For bothagreement levels separately, we compare the ex-tracted keyphrases against the manually extractedkeyphrases using rank-based IR measures of Pre-cision at Rank 10 (P@10) and Recall at Rank 10(R@10).
Because GP is a stochastic algorithm, toaccount for randomness we made 30 runs of eachexperiment and report the average scores.
On thesesamples, we use the unpaired t-test to determine thesignificance in performance differences.
As base-line to compare against GPKEX, we use keyphraseextraction based on tf-idf scores (with the samepreprocessing and filtering setup as for GPKEX).Tested configurations.
We tested four evolutionconfigurations.
Configuration A uses the param-eter setting described in Section 3.2, but withoutparsimony pressure.
Configurations B and C useparsimony pressure defined by (2), with ?
= 10002The annotated dataset is available under CC BY-NC-SAlicense from http://takelab.fer.hr/gpkex45Strong agreement Weak agreementConfig.
GAP P@10 R@10 P@10 R@10A 13.0 8.3 28.7 28.7 8.4B 12.8 8.2 30.2 28.4 8.5C 12.5 7.7 27.3 27.3 7.7D 9.9 5.1 25.9 20.4 7.3tf-idf 7.4 5.8 22.3 21.5 12.4UKE 6.0 5.8 32.6 15.3 15.8Table 1: Keyphrase ranking results.and ?
= 100, respectively.
Configuration D issimilar to A, but uses all POS patterns attested forkeyphrases in the dataset.Results.
Results are shown in Table 1.
Configu-rations A and B perform similarly across all evalu-ation measures (pairwise differences are not signif-icant at p<0.05, except for R@10) and outperformthe baseline (differences are significant at p<0.01).Configuration C is outperformed by configurationA (differences are significant at p<0.05).
Config-uration D outperforms the baseline, but is outper-formed by other configurations (pairwise differ-ences in GAP are significant at p<0.05), indicatingthat conservative POS filtering is beneficial.
SinceA and B perform similar, we conclude that apply-ing parsimony pressure in our case only marginallyimproved GAP (although it has reduced KSM sizefrom an average 30 nodes for configuration A toan average of 20 and 9 nodes for configurations Band C, respectively).
We believe there are two rea-sons for this: first, the increase in KSM complexityalso increases the probability that the KSM will bediscarded as not computable (e.g., the right subtreeof a ?/?
node evaluates to zero).
Secondly, our fit-ness function is perhaps not fine-grained enoughto allow more complex KSMs to emerge gradu-ally, as small changes in keyphrase scores do notimmediately affect the value of the fitness function.In absolute terms, GAP values are rather low.This is mostly due to wrong ranking, rather than theomission of correct phrases.
Furthermore, the pre-cision for strong agreement is considerably lowerthan for weak agreement.
This indicates that GP-KEX often assigns high scores to less relevantkeyphrases.
Both deficiencies may be attributedto the fact that we do not learn to rank, but train ondataset with binary relevance judgments.The best-performing KSM from configurationA is shown in Fig.
1 (simplified form).
Length isthe length of the phrase, First is the position of the1Tf?Tf + Tfidf ?
(Length + First) +Rarelog(log Length)Figure 1: The best-performing KSM expression.first occurrence, and Rare is the number of discrim-inative words in a phrase (cf.
Section 3.1).
Tfidf,First, and Rare features seem to be positively cor-related with keyphraseness.
This particular KSMextracts on average three correct keyphrases (weakagreement) within the first 10 results.Our results are not directly comparable to pre-vious work for Croatian (Ahel et al 2009; Mijic?et al 2010; Saratlija et al 2011) because we usea different dataset and/or evaluation methodology.However, to allow for an indirect comparison, were-evaluated the results of unsupervised keyphraseextraction (UKE) from Saratlija et al(2011); weshow the result in the last row of Table 1.
GPKEX(configuration A) outperforms UKE in terms ofprecision (GAP and P@10), but performs worsein terms of recall.
In terms of F1@10 (harmonicmean of P@10 and R@10), GPKEX performs bet-ter than UKE at the strong agreement level (12.9vs.
9.9), but worse at the weak agreement level(13.0 vs. 15.6).
For comparison, Saratlija et al(2011) report UKE to be comparable to supervisedmethod from Ahel et al(2009), but better than thetf-idf extraction method from Mijic?
et al(2010).5 ConclusionGPKEX uses genetically programmed scoring mea-sures to assign rankings to keyphrase candidates.We evaluated GPKEX on Croatian texts and showedthat it yields keyphrase scoring measures that per-form comparable to other machine learning meth-ods developed for Croatian.
Thus, scoring mea-sures evolved by GPKEX provide an efficient alter-native to these more complex models.
The focus ofthis work was on Croatian, but our method couldeasily be applied to other languages as well.We have described a preliminary study.
The nextstep is to apply GPKEX to directly learn keyphraseranking.
Using additional (e.g., syntactic) featuresmight further improve the results.46AcknowledgmentsThis work has been supported by the Ministry ofScience, Education and Sports, Republic of Croatiaunder the Grant 036-1300646-1986.
We thank thereviewers for their constructive comments.ReferencesRenee Ahel, B Dalbelo Bas?ic, and Jan S?najder.
2009.Automatic keyphrase extraction from Croatian news-paper articles.
The Future of Information Sciences,Digital Resources and Knowledge Sharing, pages207?218.Kathrin Eichler and Gu?nter Neumann.
2010.
DFKIKeyWE: Ranking keyphrases extracted from scien-tific articles.
In Proceedings of the 5th internationalworkshop on semantic evaluation, pages 150?153.Association for Computational Linguistics.Gonenc Ercan and Ilyas Cicekli.
2007.
Using lexicalchains for keyword extraction.
Information Process-ing & Management, 43(6):1705?1714.Eibe Frank, Gordon W. Paynter, Ian H. Witten, CarlGutwin, and Craig G. Nevill-Manning.
1999.Domain-specific keyphrase extraction.
In Proceed-ings of IJCAI ?99, pages 668?673.
Morgan Kauf-mann Publishers Inc.Xin Jiang, Yunhua Hu, and Hang Li.
2009.
A rankingapproach to keyphrase extraction.
In Proceedingsof the 32nd international ACM SIGIR conference onResearch and development in information retrieval,pages 756?757.
ACM.Su Nam Kim, Olena Medelyan, Min-Yen Kan, andTimothy Baldwin.
2010.
SemEval-2010 task 5: Au-tomatic keyphrase extraction from scientific articles.In Proceedings of the 5th International Workshop onSemantic Evaluation, pages 21?26.
Association forComputational Linguistics.Kazuaki Kishida.
2005.
Property of average precisionand its generalization: An examination of evaluationindicator for information retrieval experiments.
Na-tional Institute of Informatics.John R. Koza and Riccardo Poli.
1992.
Genetic Pro-gramming: On the programming of computers byMeans of Natural Selection.
MIT Press.Zhiyuan Liu, Peng Li, Yabin Zheng, and MaosongSun.
2009.
Clustering to find exemplar terms forkeyphrase extraction.
In Proceedings of EMNLP2009, pages 257?266, Singapore.
ACL.Andrew McCallum and Kamal Nigam.
1998.
A com-parison of event models for Na?
?ve Bayes text classi-fication.
In AAAI-98 workshop on learning for textcategorization, pages 41?48.
AAAI Press.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In Proceedings ofEMNLP, volume 4.
Barcelona, Spain.Jure Mijic?, B Dalbelo Bas?ic, and Jan S?najder.
2010.Robust keyphrase extraction for a large-scale Croa-tian news production system.
In Proceedings ofFASSBL, pages 59?66.Rick Riolo and Terence Soule.
2009.
Genetic Pro-gramming Theory and Practice VI.
Springer.Josip Saratlija, Jan S?najder, and Bojana Dalbelo Bas?ic?.2011.
Unsupervised topic-oriented keyphrase ex-traction and its application to Croatian.
In Text,Speech and Dialogue, pages 340?347.
Springer.Jan S?najder, Bojana Dalbelo Bas?ic?, and Marko Tadic?.2008.
Automatic acquisition of inflectional lexicafor morphological normalisation.
Information Pro-cessing & Management, 44(5):1720?1731.Takashi Tomokiyo and Matthew Hurst.
2003.
Alanguage model approach to keyphrase extraction.In Proceedings of the ACL 2003 workshop onMultiword expressions: analysis, acquisition andtreatment-Volume 18, pages 33?40.
Association forComputational Linguistics.Peter Turney.
1999.
Learning to extract keyphrasesfrom text.
Technical report, National ResearchCouncil, Institute for In- formation Technology.C.
Wang and S. Li.
2011.
CoRankBayes: Bayesianlearning to rank under the co-training frameworkand its application in keyphrase extraction.
In Pro-ceedings of the 20th ACM international conferenceon Information and knowledge management, pages2241?2244.
ACM.Ian H Witten, Gordon W Paynter, Eibe Frank, CarlGutwin, and Craig G Nevill-Manning.
1999.
Kea:Practical automatic keyphrase extraction.
In Pro-ceedings of the fourth ACM conference on Digitallibraries, pages 254?255.
ACM.Torsten Zesch and Iryna Gurevych.
2009.
Approxi-mate matching for evaluating keyphrase extraction.In Proceedings of the 7th International Conferenceon Recent Advances in Natural Language Process-ing, pages 484?489.Kuo Zhang, Hui Xu, Jie Tang, and Juanzi Li.
2006.Keyword extraction using support vector machine.In Advances in Web-Age Information Management,volume 4016 of LNCS, pages 85?96.
Springer Berlin/ Heidelberg.47
