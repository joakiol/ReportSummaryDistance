Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 1?11,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsEfficient Elicitation of Annotations for Human Evaluation of MachineTranslationKeisuke Sakaguchi?, Matt Post?, Benjamin Van Durme?
?Center for Language and Speech Processing?Human Language Technology Center of ExcellenceJohns Hopkins University, Baltimore, Maryland{keisuke,post,vandurme}@cs.jhu.eduAbstractA main output of the annual Workshopon Statistical Machine Translation (WMT)is a ranking of the systems that partici-pated in its shared translation tasks, pro-duced by aggregating pairwise sentence-level comparisons collected from humanjudges.
Over the past few years, therehave been a number of tweaks to the ag-gregation formula in attempts to addressissues arising from the inherent ambigu-ity and subjectivity of the task, as well asweaknesses in the proposed models andthe manner of model selection.We continue this line of work by adapt-ing the TrueSkillTMalgorithm ?
an onlineapproach for modeling the relative skillsof players in ongoing competitions, suchas Microsoft?s Xbox Live ?
to the hu-man evaluation of machine translation out-put.
Our experimental results show thatTrueSkill outperforms other recently pro-posed models on accuracy, and also cansignificantly reduce the number of pair-wise annotations that need to be collectedby sampling non-uniformly from the spaceof system competitions.1 IntroductionThe Workshop on Statistical Machine Translation(WMT) has long been a central event in the ma-chine translation (MT) community for the evalua-tion of MT output.
It hosts an annual set of sharedtranslation tasks focused mostly on the translationof western European languages.
One of its mainfunctions is to publish a ranking of the systemsfor each task, which are produced by aggregatinga large number of human judgments of sentence-level pairwise rankings of system outputs.
Whilethe performance on many automatic metrics is also# score range system1 0.638 1 UEDIN-HEAFIELD2 0.604 2-3 UEDIN0.591 2-3 ONLINE-B4 0.571 4-5 LIMSI-SOUL0.562 4-5 KIT0.541 5-6 ONLINE-A7 0.512 7 MES-SIMPLIFIED8 0.486 8 DCU9 0.439 9-10 RWTH0.429 9-11 CMU-T2T0.420 10-11 CU-ZEMAN12 0.389 12 JHU13 0.322 13 SHEF-WPROATable 1: System rankings presented as clusters(WMT13 French-English competition).
The scorecolumn is the percentage of time each system wasjudged better across its comparisons (?2.1).reported (e.g., BLEU (Papineni et al., 2002)), thehuman evaluation is considered primary, and is infact used as the gold standard for its metrics task,where evaluation metrics are evaluated.In machine translation, the longstanding dis-agreements about evaluation measures do not goaway when moving from automatic metrics to hu-man judges.
This is due in no small part to the in-herent ambiguity and subjectivity of the task, butalso arises from the particular way that the WMTorganizers produce the rankings.
The system-level rankings are produced by collecting pairwisesentence-level comparisons between system out-puts.
These are then aggregated to produce a com-plete ordering of all systems, or, more recently, apartial ordering (Koehn, 2012), with systems clus-tered where they cannot be distinguished in a sta-tistically significant way (Table 1, taken from Bo-jar et al.
(2013)).A number of problems have been noted withthis approach.
The first has to do with the na-ture of ranking itself.
Over the past few years, theWMT organizers have introduced a number of mi-nor tweaks to the ranking algorithm (?2) in reac-tion to largely intuitive arguments that have been1raised about how the evaluation is conducted (Bo-jar et al., 2011; Lopez, 2012).
While these tweakshave been sensible (and later corroborated), Hop-kins and May (2013) point out that this is essen-tially a model selection task, and should prop-erly be driven by empirical performance on held-out data according to some metric.
Instead of in-tuition, they suggest perplexity, and show that anovel graphical model outperforms existing ap-proaches on that metric, with less amount of data.A second problem is the deficiency of the mod-els used to produce the ranking, which work bycomputing simple ratios of wins (and, option-ally, ties) to losses.
Such approaches do not con-sider the relative difficulty of system matchups,and thus leave open the possibility that a systemis ranked highly from the luck of comparisonsagainst poorer opponents.Third, a large number of judgments need to becollected in order to separate the systems into clus-ters to produce a partial ranking.
The sheer size ofthe space of possible comparisons (all pairs of sys-tems times the number of segments in the test set)requires sampling from this space and distributingthe annotations across a number of judges.
Evenstill, the number of judgments needed to producestatistically significant rankings like those in Ta-ble 1 grows quadratically in the number of par-ticipating systems (Koehn, 2012), often forcingthe use of paid, lower-quality annotators hired onAmazon?s Mechanical Turk.
Part of the prob-lem is that the sampling strategy collects data uni-formly across system pairings.
Intuitively, weshould need many fewer annotations between sys-tems with divergent base performance levels, in-stead focusing the collection effort on system pairswhose performance is more matched, in order totease out the gaps between similarly-performingsystems.
Why spend precious human time on re-dundantly affirming predictable outcomes?To address these issues, we developed a varia-tion of the TrueSkill model (Herbrich et al., 2006),an adaptative model of competitions originally de-veloped for the Xbox Live online gaming commu-nity.
It assumes that each player?s skill level fol-lows a Gaussian distribution N (?, ?2), in which?
represents a player?s mean performance, and ?2the system?s uncertainty about its current estimateof this mean.
These values are updated after each?game?
(in our case, the value of a ternary judg-ment) in proportion to how surprising the outcomeis.
TrueSkill has been adapted to a number ofareas, including chess, advertising, and academicconference management.The rest of this paper provides an empiricalcomparison of a number of models of human eval-uation (?2).
We evaluate on perplexity and alsoon accuracy, showing that the two are not alwayscorrelated, and arguing for the primacy of the lat-ter (?3).
We find that TrueSkill outperforms othermodels (?4).
Moreover, TrueSkill also allows us todrastically reduce the amount of data that needs tobe collected by sampling non-uniformly from thespace of all competitions (?5), which also allowsfor greater separation of the systems into rankedclusters (?6).2 ModelsBefore introducing our adaptation of the TrueSkillmodel for ranking translation systems with humanjudgments (?2.3), we describe two comparisons:the ?Expected Wins?
model used in recent evalu-ations, and the Bayesian model proposed by Hop-kins and May (?2.2).As we described briefly in the introduction,WMT produces system rankings by aggregatingsentence-level ternary judgments of the form:(i, S1, S2, pi)where i is the source segment (id), S1and S2are the system pair drawn from a set of systems{S}, and pi ?
{<,>,=} denotes whether thefirst system was judged to be better than, worsethan, or equivalent to the second.
These ternaryjudgments are obtained by presenting judges witha randomly-selected input sentence and the refer-ence, followed by five randomly-selected transla-tions of that sentence.
Annotators are asked torank these systems from best (rank 1) to worst(rank 5), ties permitted, and with no meaning as-cribed to the absolute values or differences be-tween ranks.
This is done to accelerate data collec-tion, since it yields ten pairwise comparisons perranking.
Tens of thousands of judgments of thisform constitute the raw data used to compute thesystem-level rankings.
All the work described inthis section is computed over these pairwise com-parisons, which are treated as if they were col-lected independently.2.1 Expected WinsThe ?Expected Wins?
model computes the per-centage of times that each system wins in its2pairwise comparisons.
Let A be the completeset of annotations or judgments of the form{i, S1, S2, piR}.
We assume these judgments havebeen converted into a normal form where S1is ei-ther the winner or is tied with S2, and thereforepiR?
{<,=}.
Let ?
(x, y) be the Kronecker deltafunction.1We then define the function:wins(Si, Sj) =|A|?n=1?
(Si, S(n)1)?
(Sj, S(n)2)?
(pi(n)R, <)which counts the number of annotations for whichsystem Siwas ranked better than system Sj.
Wedefine a single-variable version that marginalizesover all annotations:wins(Si) =?Sj6=Siwins(Si, Sj)We also define analogous functions for loses andties.
Until the WMT11 evaluation (Callison-Burchet al., 2011), the score for each system Siwascomputed as follows:score(Si) =wins(Si) + ties(Si)wins(Si) + ties(Si) + loses(Si)Bojar et al.
(2011) suggested that the inclusion ofties biased the results, due to their large numbers,the underlying similarity of many of the models,and the fact that they are counted for both systemsin the tie, and proposed the following modifiedscoring function:score(Si) =1|{S}|?Sj6=Siwins(Si, Sj)wins(Si, Sj) + wins(Sj, Si)This metric computes an average relative fre-quency of wins, excluding ties, and was usedin WMT12 and WMT13 (Callison-Burch et al.,2012; Bojar et al., 2013).The decision to exclude ties isn?t withoutits problems; for example, an evaluation wheretwo systems are nearly always judged equivalentshould be relevant in producing the final rankingof systems.
Furthermore, as Hopkins and May(2013) point out, throwing out data to avoid bi-asing a model suggests a problem with the model.We now turn to a description of their model, whichaddresses these problems.1?
(x, y) ={1 if x = y0 o.w.2.2 The Hopkins and May (2013) modelRecent papers (Koehn, 2012; Hopkins and May,2013) have proposed models focused on the rel-ative ability of the competition systems.
Theseapproaches assume that each system has a meanquality represented by a Gaussian distribution witha fixed variance shared across all systems.
In thegraphical model formulation of Hopkins and May(2013), the pairwise judgments (i, S1, S2, pi) areimagined to have been generated according to thefollowing process:?
Select a source sentence i?
Select two systems S1and S2.
A systemSjis associated with a Gaussian distributionN (?Sj, ?2a), samples from which representthe quality of translations?
Draw two ?translations?, adding randomGaussian noise with variance ?2obsto simulatethe subjectivity of the task and the differencesamong annotators:q1?
N (?S1, ?2a) +N (0, ?2obs)q2?
N (?S2, ?2a) +N (0, ?2obs)?
Let d be a nonzero real number that definesa fixed decision radius.
Produce a rating piaccording to:2pi =??
?< q1?
q2> d> q2?
q1> d= otherwiseThe task is to then infer the posterior parameters,given the data: the system means ?Sjand, by ne-cessity, the latent values {qi} for each of the pair-wise comparison training instances.
Hopkins andMay do not publish code or describe details of thisalgorithm beyond mentioning Gibbs sampling, sowe used our own implementation,3and describe ithere for completeness.After initialization, we have training instancesof the form (i, S1, S2, piR, q1, q2), where all but theqiare observed.
At a high level, the sampler iter-ates over the training data, inferring values of q1and q2for each annotation together in a single stepof the sampler from the current values of the sys-tems means, {?j}.4At the end of each iteration,2Note that better systems have higher relative abilities{?Sj}.
Better translations subsequently have on-averagehigher values {qi}, which translate into a lower ranking pi.3github.com/keisks/wmt-trueskill4This worked better than a version of the sampler thatchanged one at a time.3these means are then recomputed by re-averagingall values of {qi} associated with that system.
Af-ter the burn-in period, the ?s are stored as samples,which are averaged when the sampling concludes.During each iteration, q1and q2are resampledfrom their corresponding system means:q1?
N (?S1, ?2a)q2?
N (?S2, ?2a)We then update these values to respect the annota-tion pi as follows.
Let t = q1?q2(S1is the winnerby human judgments), and ensure that the valuesare outside the decision radius, d:q?1={q1t ?
dq1+12(d?
t) otherwiseq?2={q2t ?
dq2?12(d?
t) otherwiseIn the case of a tie:q?1=?????????q1+12(d?
t) t ?
dq1t < dq1+12(?d?
t) t ?
?dq?2=?????????q2?12(d?
t) t ?
dq2t < dq2?12(?d?
t) t ?
?dThese values are stored for the current iterationand averaged at its end to produce new estimatesof the system means.
The quantity d?
t can be in-terpreted as a loss function, returning a high valuewhen the observed outcome is unexpected and alow value otherwise (Figure 1).2.3 TrueSkillPrior to 2012, the WMT organizers included refer-ence translations among the system comparisons.These were used as a control against which theevaluators could be measured for consistency, onthe assumption that the reference was almost al-ways best.
They were also included as data pointsin computing the system ranking.
Another ofBojar et al.
(2011)?s suggestions was to excludethis data, because systems compared more of-ten against the references suffered unfairly.
Thiscan be further generalized to the observation thatnot all competitions are equal, and a good modelshould incorporate some notion of ?match diffi-culty?
when evaluating system?s abilities.
Theinference procedure above incorporates this no-tion implicitly in the inference procedure, but themodel itself does not include a notion of matchdifficulty or outcome surprisal.A model that does is TrueSkill5(Herbrich et al.,2006).
TrueSkill is an adaptive, online system thatalso assumes that each system?s skill level followsa Gaussian distribution, maintaining a mean ?Sjfor each system Sjrepresenting its current esti-mate of that system?s native ability.
However, italso maintains a per-system variance, ?2Sj, whichrepresents TrueSkill?s uncertainty about its esti-mate of each mean.
After an outcome is observed(a game in which the result is a win, loss, or draw),the size of the updates is proportional to how sur-prising the outcome was, which is computed fromthe current system means and variances.
If a trans-lation from a system with a high mean is judgedbetter than a system with a greatly lower mean, theresult is not surprising, and the update size for thecorresponding system means will be small.
On theother hand, when an upset occurs in a competition,the means will receive larger updates.Before defining the update equations, we needto be more concrete about how this notion of sur-prisal is incorporated.
Let t = ?S1?
?S2, the dif-ference in system relative abilities, and let  be afixed hyper-parameter corresponding to the earlierdecision radius.
We then define two loss functionsof this difference for wins and for ties:vwin(t, ) =N (?+ t)?
(?+ t)vtie(t, ) =N (??
t)?N (?
t)?(?
t)?
?(??
t)where ?
(x) is the cumulative distribution functionand theN s are Gaussians.
Figures 1 and 2 displayplots of these two functions compared to the Hop-kins and May model.
Note how vwin(Figure 1) in-creases exponentially as ?S2becomes greater thanthe (purportedly) better system, ?S1.As noted above, TrueSkill maintains not onlyestimates {?Sj} of system abilities, but alsosystem-specific confidences about those estimates5The goal of this section is to provide an intuitive descrip-tion of TrueSkill as adapted for WMT manual evaluations,with enough detail to carry the main ideas.
For more details,please see Herbrich et al.
(2006).4?1.0 ?0.5 0.0 0.5 1.0t = ?S1?
?S20.00.51.01.5v(t,?
)TrueSkillHMFigure 1: TrueSkill?s vwinand the correspondingloss function in the Hopkins and May model asa function of the difference t of system means( = 0.5, c = 0.8 for TrueSkill, and d = 0.5 forHopkins and May model).
?1.5 ?1.0 ?0.5 0.0 0.5 1.0 1.5t = ?S1?
?S2?1.0?0.50.00.51.0v(t,?
)TrueSkillHMFigure 2: TrueSkills vtieand the correspondingloss function in the Hopkins and May model asa function of the difference t of system means( = 0.5, c = 0.3, and d = 0.5).{?Sj}.
These confidences also factor into the up-dates: while surprising outcomes result in largerupdates to system means, higher confidences (rep-resented by smaller variances) result in smallerupdates.
TrueSkill defines the following value:c2= 2?2+ ?2S1+ ?2S2which accumulates the variances along ?, anotherfree parameter.
We can now define the updateequations for the system means:?S1= ?S1+?2S1c?
v(tc,c)?S2= ?S2??2S2c?
v(tc,c)The second term in these equations captures theidea about balancing surprisal with confidence,described above.In order to update the system-level confidences,TrueSkill defines another set of functions, w, forthe cases of wins and ties.
These functions aremultiplicative factors that affect the amount ofchange in ?2:wwin(t, ) = vwin?
(vwin+ t?
)wtie(t, ) = vtie+(?
t) ?
N (?
t) + (+ t) ?
N (+ t)?(?
t)?
?(??
t)The underlying idea is that these functions cap-ture the outcome surprisal via v. This update al-ways decreases the size of the variances ?2, whichmeans uncertainty of ?
decreases as comparisonsgo on.
With these defined, we can conclude bydefining the updates for ?2S1and ?2S2:?2S1= ?2S1?[1??2S1c2?
w(tc,c)]?2S2= ?2S2?[1??2S2c2?
w(tc,c)]One final complication not presented here but rel-evant to adapting TrueSkill to the WMT setting:the parameter ?
and another parameter (not dis-cussed) ?
are incorporated into the update equa-tions to give more weight to recent matches.
This?latest-oriented?
property is useful in the gamingsetting for which TrueSkill was built, where play-ers improve over time, but is not applicable in theWMT competition setting.
To cancel this propertyin TrueSkill, we set ?
= 0 and ?
= 0.025 ?
|A| ?
?2in order to lessen the impact of the order in whichannotations are presented to the system.2.4 Data selection with TrueSkillA drawback of the standard WMT data collectionmethod is that it samples uniformly from the spaceof pairwise system combinations.
This is undesir-able: systems with vastly divergent relative abil-ity need not be compared as often as systems thatare more evenly matched.
Unfortunately, one can-not sample non-uniformly without knowing aheadof time which systems are better.
TrueSkill pro-vides a solution to this dilemma with its match-selection ability: systems with similar means andlow variances can be confidently considered to beclose matches.
This presents a strong possibilityof reducing the amount of data that needs to be5collected in the WMT competitions.
In fact, theTrueSkill formulation provides a way to computethe probability of a draw between two systems,which can be used to compute for a system Siaconditional distribution over matches with othersystems {Sj 6=i}.Formally, in the TrueSkill model, the match-selection (chance to draw) between two players(systems in WMT) is computed as follows:pdraw=?2?2c2?
exp(?(?a?
?b)22c2)However, our setting for canceling the ?latest-oriented?
property affects this matching qualityequation, where most systems are almost equallycompetitive (?
1).
Therefore, we modify the equa-tion in the following manner which simply de-pends on the difference of ?.p?draw=1exp(|?a?
?b|)TrueSkill selects the matches it would like tocreate, according to this selection criteria.
We dothis according to the following process:1.
Select a system S1(e.g., the one with thehighest variance)2.
Compute a normalized distribution overmatches with other systems pairs p?draw3.
Draw a system S2from this distribution4.
Draw a source sentence, and present to thejudge for annotation3 Experimental setup3.1 DatasetsWe used the evaluation data released by WMT13.6The data contains (1) five-way system rankingsmade by either researchers or Turkers and (2)translation data consisting of source sentences, hu-man reference translations, and submitted transla-tions.
Data exists for 10 language pairs.
More de-tails about the dataset can be found in the WMT2013 overview paper (Bojar et al., 2013).Each five-way system ranking was convertedinto ten pairwise judgments (?2).
We trained themodels using randomly selected sets of 400, 800,1,600, 3,200, and 6,400 pairwise comparisons,6statmt.org/wmt13/results.htmleach produced in two ways: selecting from all re-searchers, or split between researchers and Turk-ers.
An important note is that the training datadiffers according to the model.
For the ExpectedWins and Hopkins and May model, we sim-ply sample uniformly at random.
The TrueSkillmodel, however, selects its own training data (withreplacement) according to the description in Sec-tion 2.4.7For tuning hyperparameters and reporting testresults, we used development and test sets of 2,000comparisons drawn entirely from the researcherjudgments, and fixed across all experiments.3.2 PerplexityWe first compare the Hopkins and May model andTrueSkill using perplexity on the test data T , com-puted as follows:ppl(p|T ) = 2??
(i,S1,S2,pi)?Tlog2p(pi|S1,S2)where p is the model under consideration.
Theprobability of each observed outcome pi betweentwo systems S1and S2is computed by taking adifference of the Gaussian distributions associatedwith those systems:N (?
?, ?2?)
= N (?S1, ?2S1)?N (?S2, ?2S2)= N (?S1?
?S2, ?2S1+ ?2S2)This Gaussian can then be carved into three pieces:the area where S1loses, the middle area represent-ing ties (defined by a decision radius, r, whosevalue is fit using development data), and a thirdarea representing where S1wins.
By integratingover each of these regions, we have a probabilitydistribution over these outcomes:p(pi | S1, S2) =??????????????????0?
?N (?
?, ?2?)
if pi is >?r0N (?
?, ?2?)
if pi is =?
?rN (?
?, ?2?)
if pi is <We do not compute perplexity for the ExpectedWins model, which does not put any probabilitymass on ties.7We use a Python implementation of TrueSkill(github.com/sublee/trueskill).63.3 AccuracyPerplexity is often viewed as a neutral metric, butwithout access to unbounded training data or thetrue model parameters, it can only be approxi-mated.
Furthermore, it does not always corre-late perfectly with evaluation metrics.
As such,we also present accuracy results, measuring eachmodel?s ability to predict the values of the ternarypairwise judgments made by the annotators.
Theseare computed using the above equation, pickingthe highest value of p(pi) for all annotations be-tween each system pair (Si, Sj).
As with perplex-ity, we emphasize that these predictions are func-tions of the system pair only, and not the individualsentences under consideration, so the same out-come is always predicted for all sentences betweena system pair.3.4 Parameter TuningWe follow the settings described in Hopkins andMay (2013) for their model: ?a= 0.5, ?obs= 1.0,and d = 0.5.
In TrueSkill, in accordance with theHopkins and May model, we set the initial ?
and?
values for each system to 0 and 0.5 respectively,and  to 0.25.For test data, we tuned the ?decision ra-dius?
parameter r by doing grid search over{0.001, 0.01, 0.1, 0.3, 0.5}, searching for thevalue which minimized perplexity and maximizedaccuracy on the development set.
We do this foreach model and language pair.
When tuned byperplexity, r is typically either 0.3 or 0.5 for bothmodels and language pairs, whereas, for accuracy,the best r is either 0.001, 0.01, or 0.1.4 Results4.1 Model ComparisonFigure 3 shows the perplexity of the two mod-els with regard to the number of training compar-isons.
The perplexities in the figure are averagedover all ten language pairs in the WMT13 dataset.Overall, perplexities decrease according to the in-crease of training size.
The Hopkins and Mayand TrueSkill models trained on both researcherand Turker judgments are comparable, whereasthe Hopkins and May model trained on researcherjudgments alone shows lower perplexity than thecorresponding TrueSkill model.In terms of accuracy, we see that the TrueSkillmodel has the highest accuracies, saturating at justover 3,000 training instances (Figure 4).
TrueSkill1000 2000 3000 4000 5000 6000Training Data Size2.802.852.902.953.00PerplexityHM-allHM-resTS-allTS-resFigure 3: Model Perplexities for WMT13 dataset.?all?
indicates that models are trained on both re-searcher and Turker judgements, and ?res?
meansthat models are trained on only researcher judge-ments.outperforms Expected Win and the Hopkins andMay, especially when the training size is small(Table 2).
We also note that training on researcherjudgments alone (dashed lines) results in betterperformance than training on both researchers andTurker judgments.
This likely reflects both a bet-ter match between training and test data (recall thetest data consists of researcher judgments only),as well as the higher consistency of this data, asevidenced by the annotator agreement scores pub-lished in the WMT overview paper (Bojar et al.,2013).
Recall that the models only have accessto the system pair (and not the sentences them-selves), and thus make the same prediction for pifor a particular system pair, regardless of whichsource sentence was selected.
As an upper boundfor performance on this metric, Table 2 containsan oracle score, which is computed by selecting,for each pair of systems, the highest-probabilityranking.8Comparing the plots, we see there is not a per-fect relationship between perplexity and accuracyamong the models; the low perplexity does notmean the high accuracy, and in fact the order ofthe systems is different.4.2 Free-for-all matchesTrueSkill need not deal with judgments in pairsonly, but was in fact designed to be used in a vari-ety of settings, including N-way free-for-all games8Note that this might not represent a consistent rankingamong systems, but is itself an upper bound on the highest-scoring consistent ranking.71000 2000 3000 4000 5000 6000Training Data Size0.4600.4650.4700.4750.4800.4850.4900.4950.500AccuracyExpWin-allExpWin-resHM-allHM-resTS-allTS-resFigure 4: Model accuracies with different trainingdomain for WMT13 dataset.Train Size Exp-Win HM TrueSkill400 0.465 0.471 0.479800 0.471 0.475 0.483all 1600 0.479 0.477 0.4933200 0.486 0.489 0.4936400 0.487 0.490 0.495400 0.460 0.463 0.484800 0.475 0.473 0.488res 1600 0.481 0.482 0.4933200 0.492 0.494 0.4976400 0.495 0.496 0.497Upper Bound 0.525Table 2: Model accuracies: models are tuned byaccuracy instead of perplexity.
Upper bound iscomputed by selecting the most frequent choice(<,>,=) for each system pair.with many players all competing for first place.This adapts nicely to WMT?s actual collection set-ting.
Recall that annotators are presented with fivetranslations which are then ranked; we can treatthis setting as a 5-way free-for-all match.
Whilethe details of these updates are beyond the scope ofthis paper, they are presented in the original modeland are implemented in the toolkit we used.
Wethus also conducted experiments varying the valueof N from 2 to 5.The results are shown in Tables 3 and 4, whichhold constant the number of matches and pairwisejudgments, respectively.
When fixing the num-ber of matches, the 5-way setting is at an advan-tage, since there is much more information in eachmatch; in contrast, when fixing the number of pair-wise comparisons, the 5-way setting is at a dis-advantage, since many fewer competitions consti-# N=2 N=3 N=4 N=5400 0.479 0.482 0.491 0.492800 0.483 0.493 0.495 0.4951600 0.493 0.492 0.497 0.4953200 0.493 0.494 0.498 0.4976400 0.495 0.498 0.498 0.498Table 3: Accuracies when training with N-wayfree-for-all models, fixing the number of matches.# N=2 N=3 N=4 N=5400 0.479 0.475 0.470 0.459800 0.483 0.488 0.476 0.4661600 0.493 0.488 0.481 0.4813200 0.493 0.492 0.487 0.4896400 0.495 0.496 0.494 0.495Table 4: Accuracies when training with N-wayfree-for-all models, fixing the number of pairwisecomparisons.tute these comparisons.
The results bear this out,but also suggest that the standard WMT setting?
which extracts ten pairwise comparisons fromeach 5-way match and treats them independently?
works well.
We will not speculate further here,but provide this experiment purely to motivate po-tential future work.
Here we will focus our con-clusions to the pair-wise ranking scenario.5 Reduced Data Collection withNon-uniform Match SelectionAs mentioned earlier, a drawback of the selectionof training data for annotation is that it is sampleduniformly from the space of system pair compe-titions, and an advantage of TrueSkill is its abil-ity to instead compute a distribution over pairingsand thereby focus annotation efforts on competi-tive matches.
In this section, we report results inthe form of heat maps indicating the percentage ofpairwise judgments requested by TrueSkill acrossthe full cross-product of system pairs, using theWMT13 French-English translation task.Figure 5 depicts a system-versus-system heatmap for all judgments in the dataset.
Across thisfigure and the next two, systems are sorted alongeach axis by the final values of ?
inferred byTrueSkill during training, and the heat of eachsquare is proportional to the percentage of judg-ments obtained between those two systems.
Thediagonal reflects the fact that systems do not com-pete against themselves, and the stripe at row andcolumn 5 reflects a system that was entered late81 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.00.20.40.60.81.01.21.41.61.82.0Figure 5: Heat map for the ratio of pairwise judg-ments across the full cross-product of systems inthe WMT13 French-English translation task.1 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.00.20.40.60.81.01.21.41.61.82.0Figure 6: Heat map for the ratio of pairwise judg-ments across the full cross-product of systemsused in the first 20% of TrueSkill model.1 2 3 4 5 6 7 8 9 10 11 12 1312345678910111213 0.00.20.40.60.81.01.21.41.61.82.0Figure 7: Heat map for the ratio of pairwise judg-ments across the full cross-product of systemsused in the last 20% of TrueSkill model.into the WMT13 competition and thus had manyfewer judgments.
It is clear that these values areroughly uniformly distributed.
This figure servesas a sort of baseline, demonstrating the lack of pat-terns in the data-selection process.The next two figures focus on the data thatTrueSkill itself selected for its use from among allof the available data.
Figure 6 is a second heatmap presenting the set of system pairs selected byTrueSkill for the first 20% of its matches chosenduring training, while Figure 7 presents a heat mapof the last 20%.
The contrast is striking: whereasthe judgments are roughly uniformly distributed atthe beginning, the bulk of the judgments obtainedfor the last set are clustered along the diagonal,where the most competitive matches lie.Together with the higher accuracy of TrueSkill,this suggests that it could be used to decrease theamount of data that needs to be collected in futureWMT human evaluations by focusing the annota-tion effort on more closely-matched systems.6 ClusteringAs pointed out by Koehn (2012), a ranking pre-sented as a total ordering among systems con-ceals the closeness of comparable systems.
In theWMT13 competition, systems are grouped intoclusters, which is equivalent to presenting onlya partial ordering among the systems.
Clustersare constructed using bootstrap resampling to in-fer many system rankings.
From these rankings,rank ranges are then collected, which can be usedto construct 95% confidence intervals, and, in turn,to cluster systems whose ranges overlap.
We usea similar approach for clustering in the TrueSkillmodel.
We obtain rank ranges for each system byrunning the TrueSkill model 100 times,9throw-ing out the top and bottom 2 rankings for eachsystem, and clustering where rank ranges overlap.For comparison, we also do this for the other twomodels, altering the amount of training data from1k to 25k in increments of 1,000, and plotting thenumber of clusters that can be obtained from eachtechnique on each amount of training data.Figure 8 show the number of clusters accordingto the increase of training data for three models.TrueSkill efficiently split the systems into clusterscompared to other two methods.
Figure 9 and 10present the result of clustering two different size of9We also tried the sampling 1,000 times and the clusteringgranularities were the same.95000 10000 15000 20000 25000Pairwise Comparisons01234567Num.
ofClustersExpWinHMTSFigure 8: The number of clusters according tothe increase of training data for WMT13 French-English (13 systems in total).training data (1K and 25K pairwise comparisons)on the TrueSkill model, which indicates that therank ranges become narrow and generate clustersreasonably as the number of training samples in-creases.
The ranking and clusters are slightly dif-ferent from the official result (Table 1) mainly be-cause the official result is based on Expected Wins.One noteworthy observation is that the rankingof systems between Figure 9 and Figure 10 is thesame, further corroborating the stability and ac-curacy of the TrueSkill model even with a smallamount of data.
Furthermore, while the needto cluster systems forces the collection of sig-nificantly more data than if we wanted only toreport a total ordering, TrueSkill here producesnicely-sized clusters with only 25K pairwise com-parisons, which is nearly one-third large of thatused in the WMT13 campaign (80K for French-English, yielding 8 clusters).7 ConclusionModels of ?relative ability?
(Koehn, 2012; Hop-kins and May, 2013) are a welcome addition tomethods for inferring system rankings from hu-man judgments.
The TrueSkill variant presentedin this paper is a promising further development,both in its ability to achieve higher accuracy levelsthan alternatives, and in its ability to sample non-uniformly from the space of system pair match-ings.
It?s possible that future WMT evaluationscould significantly reduce the amount of data theyneed to collect, also potentially allowing them todraw from expert annotators alone (the developersuedin-h on.Buedin-w LIMSI KIT on.A MES-S DCU CMURWTH cu-z JHU Shef12345678910111213Figure 9: The result of clustering by TrueSkillmodel with 1K training data from WMT13French-English.
The boxes range from the lowerto upper quartile values, with means in the middle.The whiskers show the full range of each system?srank after the bootstrap resampling.uedin-h on.Buedin-w LIMSI KIT on.A MES-S DCU CMURWTH cu-z JHU Shef12345678910111213Figure 10: The result of clustering by TrueSkillmodel with 25K training data.
Dashed lines sep-arate systems with non-overlapping rank ranges,splitting the data into clusters.of the participating systems), without the need tohire non-experts on Mechanical Turk.One piece missing from the methods exploredand proposed in this paper is models of the actualtranslations being compared by judges.
Clearly,it is properties of the sentences themselves thatjudges use to make their judgments, a fact whichis captured only indirectly by modeling transla-tion qualities sampled from system abilities.
Thisobservation has been used in the developmentof automatic evaluation metrics (Song and Cohn,2011), and is something we hope to explore in fu-ture work for system ranking.10ReferencesOnd?rej Bojar, Milo?s Ercegov?cevi?c, Martin Popel, andOmar Zaidan.
2011.
A Grain of Salt for the WMTManual Evaluation.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, pages1?11, Edinburgh, Scotland, July.
Association forComputational Linguistics.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Work-shop on Statistical Machine Translation.
In Pro-ceedings of the Eighth Workshop on Statistical Ma-chine Translation, pages 1?44, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011Workshop on Statistical Machine Translation.
InProceedings of the Sixth Workshop on Statisti-cal Machine Translation, pages 22?64, Edinburgh,Scotland, July.
Association for Computational Lin-guistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation, pages10?51, Montr?eal, Canada, June.
Association forComputational Linguistics.Ralf Herbrich, Tom Minka, and Thore Graepel.
2006.TrueSkillTM: A Bayesian Skill Rating System.
InProceedings of the Twentieth Annual Conference onNeural Information Processing Systems, pages 569?576, Vancouver, British Columbia, Canada, Decem-ber.
MIT Press.Mark Hopkins and Jonathan May.
2013.
Models oftranslation competitions.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics (Volume 1: Long Papers), pages1416?1424, Sofia, Bulgaria, August.
Association forComputational Linguistics.Philipp Koehn.
2012.
Simulating Human Judgmentin Machine Translation Evaluation Campaigns.
InProceedings of the 9th International Workshop onSpoken Language Translation (IWSLT), pages 179?184, Hong Kong, China, December.
InternationalSpeech Communication Association.Adam Lopez.
2012.
Putting Human Assessments ofMachine Translation Systems in Order.
In Proceed-ings of the Seventh Workshop on Statistical MachineTranslation, pages 1?9, Montr?eal, Canada, June.
As-sociation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, July.
Association for ComputationalLinguistics.Xingyi Song and Trevor Cohn.
2011.
Regression andRanking based Optimisation for Sentence Level MTEvaluation.
In Proceedings of the Sixth Workshopon Statistical Machine Translation, pages 123?129,Edinburgh, Scotland, July.
Association for Compu-tational Linguistics.11
