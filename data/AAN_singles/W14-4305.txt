Proceedings of the SIGDIAL 2014 Conference, pages 32?40,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsInformation Navigation System Based on POMDP that Tracks User FocusKoichiro Yoshino Tatsuya KawaharaSchool of Informatics, Kyoto UniversitySakyo-ku, Kyoto, 606-8501, Japanyoshino@ar.media.kyoto-u.ac.jpAbstractWe present a spoken dialogue system fornavigating information (such as news ar-ticles), and which can engage in smalltalk.
At the core is a partially observ-able Markov decision process (POMDP),which tracks user?s state and focus of at-tention.
The input to the POMDP is pro-vided by a spoken language understanding(SLU) component implemented with lo-gistic regression (LR) and conditional ran-dom fields (CRFs).
The POMDP selectsone of six action classes; each action classis implemented with its own module.1 IntroductionA large number of spoken dialogue systems havebeen investigated and many systems are deployedin the real world.
Spoken dialogue applicationsthat interact with a diversity of users are avail-able on smart-phones.
However, current appli-cations are based on simple question answeringand the system requires a clear query or a def-inite task goal.
Therefore, next-generation dia-logue systems should engage in casual interactionswith users who do not have a clear intention or atask goal.
Such systems include a sightseeing nav-igation system that uses tour guide books or doc-uments in Wikipedia (Misu and Kawahara, 2010),and a news navigation system that introduces newsarticles updated day-by-day (Yoshino et al., 2011;Pan et al., 2012).
In this paper, we develop an in-formation navigation system that provides infor-mation even if the user request is not necessarilyclear and there is not a matching document in theknowledge base.
The user and the system converseon the current topic and the system provides po-tentially useful information for the user.Dialogue management of this kind of systemswas usually made in a heuristic manner and basedon simple rules (Dahl et al., 1994; Bohus and Rud-nicky, 2003).
There is not a clear principle norestablished methodology to design and implementcasual conversation systems.
In the past years, ma-chine learning, particularly reinforcement learn-ing, have been investigated for dialogue manage-ment.
MDPs and POMDPs are now widely usedto model and train dialogue managers (Levin etal., 2000; Williams and Young, 2007; Young etal., 2010; Yoshino et al., 2013b).
However, theconventional scheme assumes that the task and di-alogue goal can be clearly stated and readily en-coded in the RL reward function.
This is not truein casual conversation or when browsing informa-tion.Some previous work has tackled with this prob-lem.
In a conversational chatting system (Shibataet al., 2014), users were asked to make evalua-tion at the end of each dialogue session, to definerewards for reinforcement learning.
In a listen-ing dialogue system (Meguro et al., 2010), levelsof satisfaction were annotated in logs of dialoguesessions to train a discriminative model.
Theseapproaches require costly input from users or de-velopers, who provide labels and evaluative judg-ments.In this work, we present a framework in whichreward is defined for the quality of system actionsand also for encouraging long interactions, in con-trast to the conventional framework.
Moreover,user focus is tracked to make appropriate actions,which are more rewarded.2 Conversational InformationNavigation SystemIn natural human-human conversation, partici-pants have topics they plan to talk about, and theyprogress through the dialogue in accordance withthe topics (Schegloff and Sacks, 1973).
We callthis dialogue style ?information navigation.?
Anexample is shown in Figure 1.
First, the speaker32Dialogue statesSpeaker (system) Listener (user)Offer a topic Be interested in the topicPresent the detail Make a questionAnswer the question Be silentOffer a new topic (topic 2) Not be interested inOffer a new topic (topic 3)??
?Make a questionTopic 3Topic 2??
?Topic 1Figure 1: An example of information navigation.Story Telling(ST)System-initiativeModules of related topicsQuestion Answering(QA)User-initiativeProactiveinitiativePresentation(PP)System-Draw new topicRelated topicsTopicTopicTopicTopicTopicTopic Topic????????
?Selected topicModules of current topicTopic Presentation (TP)Topic NTopic 3Topic 2??
?Topic 1Other modulesGreeting(GR) Keep silence(KS)Figure 2: Overview of the information navigationsystem.offers a new topic and probes the interest of thelistener.
If the listener shows interest, the speakerdescribes details of the topic.
If the listener asksa specific question, the speaker answers the ques-tion.
On the other hand, if the listener is not inter-ested in the topic, the speaker avoids the details ofthat topic, and changes the topic.
Topics are oftentaken from current news.In our past work, we have developed a newsnavigation system (Yoshino et al., 2011) based onthis dialogue structure.
The system provides top-ics collected from Web news texts, and the usergets information according to his interests andqueries.2.1 System overviewAn overview of the proposed system is depictedin Figure 2.
The system has six modules, each ofwhich implements a class of actions.
Each moduletakes as input a recognized user utterance, an an-alyzed predicate-argument (P-A) structure and thedetected user focus.The system begins dialogues by selecting the?topic presentation (TP)?
module, which presentsa new topic selected from a news article.
The sys-tem chooses the next module based on the user?sresponse.
In our task, the system assumes thateach news article corresponds to a single topic,and the system presents a headline of news in theTP module.
If the user shows interest (positiveresponse) in the topic without any specific ques-tions, the system selects the ?story telling (ST)?module to give details of the news.
In the STmod-ule, the system provides a summary of the newsarticle by using lead sentences.
The system canalso provide related topics with the ?proactive pre-sentation (PP)?
module.
This module is invokedby system initiative; this module is not invoked byany user request.
If the user makes a specific ques-tion regarding the topic, the system switches to the?question answering (QA)?
module to answer thequestion.
This module answers questions on thepresented topic and related topics.The modules of PP and QA are based on a di-alogue framework which uses the similarity of P-A structures (Yoshino et al., 2011).
This frame-work defines the similarity of P-A structures be-tween user queries and news articles, and retrievesor recommends the appropriate sentence from thenews articles.
This method searches for appropri-ate information from automatically parsed docu-ments by referring to domain knowledge that isautomatically extracted from domain corpus.Transitions between the modules are allowed asshown in Figure 2.
The modules ?greeting (GR)?and ?keep silence (KS)?
are also implemented.GR module generates fixed greeting patterns byusing regular expression matching.
In terms ofdialogue flow, these modules can be used at anytime.2.2 User focus in information navigation?Focus?
in discourse is ?attentional state (that)contains information about the objects, properties,relations, and discourse intentions that are mostsalient at any given point.?
(Grosz and Sidner,1986).
The user has specific attention to an ob-ject if the user utterance contains the focus.
Inthis work, we define the user focus as ?the mainpiece of information of interest to the user.?
Itmakes a central component when making a replyor selecting relevant topics at the current dialoguestate.
For example, given ?Did Ichiro perform bril-33SYS, tp, ?Matsui, a part-time coach for the Giants, demonstrated 5 homeruns.
The Giants presidenttold him "Come back as a player!
?USR, st, 0, ?
Give me the detail?
?, ?Give:me/0 the:detail/0?SYS, st, ?Matsui joined the extra batting training in the Giants' camp, and demonstrated his battingto young players.
?USR, qa, 1, ?Will Matsui become the Giants' manager in the future?
?, ?Matsui/0 will:become/0the Giants' manager/1 in the future/0?SYS, qa, ?The director of Giants told him "Come back as a player!?
?USR, tp, 0, ?Next news, please.
?, ?Next:news/0 please/0?SYS, tp, ?
To be a ace pitcher, has Fujinami improved from the rookie year?
?Figure 3: An example of annotation for collected dialogue.
System utterances have a tuple of threeelements separated by a comma: speaker, called module and utterance.
User utterances have a tuple offour elements: speaker, the module the user request falls in, binary information of user focus, utteranceand user focus annotation on each phrase or P-A element.
(This example is translated from Japanese)liantly?,?
user focus is ?Ichiro?
because the sys-tem reply should include information on Ichiro.This information is annotated on content words ornamed entities in a user utterance.
In the POMDP,decisions are made based on whether any user fo-cus was detected in the user?s utterance.3 Spoken Language Understanding(SLU)In this section, we present the spoken language un-derstanding components of our system.
It detectsthe user?s focus and intention and provides theseto the dialogue manager.
These spoken languageunderstanding modules are formulated with a sta-tistical model to give likelihoods which are usedin POMDP.3.1 Dialogue dataWe collected 606 utterances (from 10 users) with arule-based dialogue system (Yoshino et al., 2011).We annotated two kinds of tags: user intention (6tags defined in Section 3.3), and focus informationdefined in Section 2.2.
An example of annotationis shown in Figure 3.
We highlighted annotationpoints in the bold font.To prepare the training data, each utterance waslabeled with one of the six modules, indicating thebest module to respond.
In addition, each phraseor P-A elements is labeled to indicated whether itis the user?s focus or not.
The user focus is deter-mined by the attributes (=specifications of wordsin the domain) and preference order of phrases toidentify the most appropriate information that theuser wants to know.
For example, in the seconduser utterance in Figure 3, the user?s focus is thephrase ?the Giants?
manager?.
These tags are an-notated by one person.3.2 User focus detection based on CRFTo detect the user focus, we use a conditionalrandom field (CRF)1.
The problem is defined asa sequential labeling of the focus labels to a se-quence of the phrases of the user utterance.
Fea-tures used are shown in the Table 1.
ORDER fea-tures are the order of the phrase in the sequenceand in the P-A structure.
We incorporate thesefeatures because the user focus often appears inthe first phrase of the user utterance.
POS fea-tures are part-of-speech (POS) tags and their pairsin the phrase.
P-A features are semantic role of theP-A structure.
We also incorporate the domain-dependent predicate-argument (P-A) scores thatare defined with an unsupervised method (Yoshinoet al., 2011).
The score is discretized to 0.01, 0.02,0.05, 0.1, 0.2, 0.5.Table 2 shows the accuracy of user focus de-tection, which was conducted via five-fold cross-validation.
?Phrase?
is phrase-base accuracy and?sentence?
indicates whether the presence of anyuser focus phrase was correctly detected (or not),regardless of whether the correct phrase was iden-tified.
This table indicates that WORD featuresare effective for detecting the user focus, but theyare not essential for in the sentence-level accuracy.In this paper, we aim for portability across do-mains; therefore the dialogue manager only usesthe sentence-level feature, so in our system we donot user the WORD features.3.3 User intention analysis based on LRThe module classifies the user intention from theuser utterance.
We define six intentions as below.?
TP: request to the TP module.1CRFsuite (Okazaki, 2007).34Table 1: Features of user focus detection.feature type featureORDER Rank in a sequence of phrasesRank in a sequence of elements of P-APOS POS tags in the phrasePOS tag sequencePOSORDER Pair of POS tag and its order in thephraseP-A Which semantic role the phrase hasWhich semantic roles exist on theutteranceP-AORDER Pair of semantic role and its order inthe utteranceP-A score P-A templates scoreWORD Words in the phrasePair of words in the phrasePair of word and its order in the phraseTable 2: Accuracy of user focus detection.Accuracyphrase 86.7%phrase + (WORD) 90.3%sentence (focus exist or not) 99.8%sentence (focus exist or not) + (WORD) 99.8%?
ST: request to the ST module.?
QA: request to the QA module.?
GR: greeting to the GR module.?
NR: silence longer than a threshold.?
II: irrelevant input due to ASR errors or noise.We adopt logistic regression (LR)-based dia-logue act tagging approach (Tur et al., 2006).
Theprobability of user intention o given an ASR resultof the user utterance h is defined as,P (o|h) =exp(?
?
?
(h, o))?oexp(?
?
?
(h, o)).
(1)Here, ?
is a vector of feature weights and ?
(h, o)is a feature vector.
We use POS, P-A and P-A tem-plates score as a feature set.
In addition, we add atypical expression feature (TYPICAL) to classifyTP, ST or GR tags.
For example, typical expres-sions in conversation are ?Hello?
or ?Go on,?
andthose in information navigation are ?News of theday?
or ?Tell me in detail.?
Features for the clas-sifier are shown in the Table 3.The accuracy of the classification in five-foldcross-validation is shown in Table 4.
The TYP-Table 3: Features of user intention analysis.feature type featurePOS Bag of POS tagsBag of POS bi-gramP-A Bag of semantic role labelsBag of semantic role labels bi-gramPair of semantic role label and its rankP-A score P-A templates scoreTYPICAL Occurrence of typical expressionsTable 4: Accuracy of user intention analysis.All features without TYPICALTP 100% 100%ST 75.3% 64.2%QA 94.1% 93.5%GR 100% 100%II 16.7% 16.7%All 92.1% 90.2%ICAL feature improves the classification accuracywhile keeping the domain portability.3.4 SLU for ASR outputASR and intention analysis involves errors.
Here,s is a true user intention and o is an observed in-tention.
The observation model P (o|s) is givenby the likelihood of ASR result P (h|u) (Komataniand Kawahara, 2000) and the likelihood of the in-tention analysis P (o|h),P (o|s) =?hP (o, h|s) (2)?
?hP (o|h)P (h|u).
(3)Here, u is an utterance of the user.
We combinethe N-best (N = 5) hypotheses of the ASR resulth.4 Dialogue Management for InformationNavigationThe conventional dialogue management for task-oriented dialogue systems is designed to reach atask goal as soon as possible (Williams and Young,2007).
In contrast, information navigation doesnot always have a clear goal, and the aim of infor-mation navigation is to provide as much relevantinformation as the user is interested in.
Therefore,our dialogue manager refers user involvement orengagement (=level of interest) and the user focus35(=object of interest).
This section describes thegeneral dialogue management based on POMDP,and then gives an explanation of the proposed dia-logue management using the user focus.4.1 Dialogue management based on POMDPThe POMDP-based statistical dialogue manage-ment is formulated as below.
The random vari-ables involved at a dialogue turn t are as follows:?
s ?
Is: user stateUser intention.?
a ?
K: system actionModule that the system selects.?
o ?
Is: observationObserved user state, including ASR and in-tention analysis errors.?
bsi= P (si|o1:t): beliefStochastic variable of the user state.?
pi: policy functionThis function determines a system action agiven a belief of user b. pi?is the optimal pol-icy function that is acquired by the training.?
r: reward functionThis function gives a reward to a pair of theuser state s and the system action a.The aim of the statistical dialogue management isto output an optimal system action a?tgiven a se-quence of observation o1:tfrom 1 to t time-steps.Next, we give the belief update that includes theobservation and state transition function.
The be-lief update of user state siin time-step t is definedas,bt+1s?j?
P (ot+1|s?j)?
??
?Obs.
?siP (s?j|si, a?k)?
??
?Trans.btsi.
(4)Obs.
is an observation function which is definedin Equation (3) and Trans.
is a state transitionprobability of the user state.
Once the system es-timates the belief btsi, the policy function outputsthe optimal action a?
as follows:a?
= pi?(bt).
(5)4.2 Training of POMDPWe applied Q-learning (Monahan, 1982; Watkinsand Dayan, 1992) to acquire the optimal policypi?.
Q-learning relies on the estimation of a Q-function, which maximizes the discounted sum offuture rewards of the system action atat a dialogueturn t given the current belief bt.
Q-learning isperformed by iterative updates on the training dia-logue data:Q(bt, at) ?
(1?
?
)Q(bt, at)+ ?
[R(st, at) + ?
maxat+1Q(bt+1, at+1)], (6)where ?
is a learning rate, ?
is a discount factor ofa future reward.
We experimentally decided ?
=0.01 and ?
= 0.9.
The optimal policy given by theQ-function is determined as,pi?
(bt) = argmaxatQ(bt, at).
(7)However, it is impossible to calculate the Q-function for all possible real values of belief b.Thus, we train a limited Q-function given by aGrid-based Value Iteration (Bonet, 2002).
The be-lief is given by a function,bsi={?
if s = i1?
?|Is|if s ?= i.
(8)Here, ?
is a likelihood of s = i that is outputof the intention analyzer, and we selected 11 dis-crete points from 0.0 to 1.0 by 0.1.
We also addedthe case of uniform distribution.
The observationfunction of the belief update is also given in a sim-ilar manner.4.3 Dialogue management using user focusOur POMDP-based dialogue managementchooses actions based on its belief in: the userintention s and the user focus f (0 or 1 ?
Jf).The observation o is controlled by hidden statesf and s that are decided by the state transitionprobabilities,P (ft+1|ft, st, at), (9)P (st+1|ft+1, ft, st, at).
(10)We constructed a user simulator by using the an-notated data described in Section 3.1.Equation (10) is also used for the state transitionprobability of the belief update.
The equation ofthe belief update (4) is extended by introducing theprevious user focus fland current user focus f?minformation,bt+1s?j= P (ot+1|s?j)?
??
?Obs.?
?iP (s?j|f?m, fl, si, a?k)?
??
?Trans.btsi,fl.
(11)36Table 5: Rewards in each turn.state focus action as f TP ST QA PP GR KSTP0+10 -10 -10 -10 -10 -101ST0-10 +10 -10 0 -10 -101QA0-10+10 +10 -10-10 -101 -10 +30 +10GR0-10 -10 -10 -10 +10 -101NR0 +10-10 -10-10-10 01 -10 +10II0-10 -10 -10 -10 -10 +101The resultant optimal policy is,a?
= pi?
(bt, fl).
(12)4.4 Definition of rewardsTable 5 defines a reward list at the end of a eachturn.
The reward of +10 is given to appropriateactions, 0 to acceptable actions, and -10 to inap-propriate actions.In Table 5, pairs of a state and its apparentlycorresponding action, TP and TP, ST and ST, QAand QA, GR and GR, and II and KS, have posi-tive rewards.
Rewards in bold fonts (+10) are de-fined for the following reasons.
If the user asks aquestion (QA) without a focus (e.g.
?What hap-pened on the game??
), the system can continue bystory telling (ST).
But when the question has a fo-cus, the system should answer the question (QA),which is highly rewarded (+30).
If the system can-not find an answer, it can present relevant informa-tion (PP).
When the user says nothing (NR), thesystem action should be decided by consideringthe user focus; present a new topic if the user isnot interested in the current topic (f=0) or presentan article related to the dialogue history (f=1).Reward of +200 is given if 20 turns are passed,to reward a long continued dialogue.
The user sim-ulator terminates the dialogue if the system selectsan inappropriate action (action of r = ?10) fivetimes, and a large penalty -200 is given to the sys-tem.5 Evaluations of DialogueWe evaluated the proposed system with two exper-iments; dialogue state tracking with real users andaverage reward with a user simulator.
For the eval-uation, we collected an additional 312 utterancesAverage of rewardsNoise-250-200-150-100-500501001502002503003500.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9w.
focus w.o.
focusFigure 4: Effect of introduction of the user focusin simulation.
(8 users, 24 dialogues) with the proposed dialoguesystem.5.1 Evaluation of dialogue manager withuser simulatorFirst, we evaluated the dialogue manager withuser simulation that is constructed from the train-ing corpus (Section 3.1).
In this evaluation, thesystem calculated average reward of 100,000 di-alogue sessions between the system and the usersimulator given a fixed noise rate.
Figure 4 showsthe effect of the user focus.
By introducing theuser focus, the system receives higher rewardsthan the model without the user focus.
Especially,the proposed model is more robust with a noiselevel of 10?30% that spoken dialogue systems of-ten encounter (Yoshino et al., 2013a).5.2 Evaluation of dialogue state trackingwith real usersDialogue state tracking (DST) is a task of track-ing the correct user state with a noisy input (e.g.ASR and NLU errors) (Williams et al., 2013).It tries to maximize the probability of the beliefof the correct states, but we evaluated the accu-racy of the 1-best result of the belief update.
Wealso evaluated the average reward of each dia-logue.
The baseline system is a rule-based dia-logue manager which we previously implemented.The baseline system is operated by a score ofthe question-answering module using predicate-argument structures (Yoshino et al., 2011) and reg-ular expressions for TP and GR modules.The DST accuracy and average reward areshown in Table 6.
This result shows that the pro-posed method tracks the dialogue state of the userwith a high accuracy.
Our statistical framework37Sys: Hello.Usr: Hello.
Tell me today's news.Sys: Tanaka showed a crazy split.
His colleagueSabathia was stunned.Usr: What is the crazy split?Sys: His split comes just like a four-seam, but itdrops quickly in front of the home plate.Usr: Where was he training?Sys: Tanaka practiced at the bullpen in the NewYork Yankees' camp on Feb. 20th.Usr: (pause)Sys: By the way, he also practiced a droptwo-seam on Feb. 18.Usr: Does his two-seam drop?Sys: The catcher said "Tanaka's two-seam notonly slides, but also drops.
"Usr: OK, any other news?Sys: Yankees' Ichiro is training strenuously!Usr: Tell me more.Sys: Ichiro came to the field an hour earlier thanusual and trained strenuously.
?=TP,    =0.8, (=TP)=0.800, =0=TP, (=TP =0)=0.8=QA,    =0.4, (=QA)=0.934, =1=QA, (=QA, =1)=0.9=QA,    =0.8, (=QA)=0.973, =1=QA, (=QA, =1)=1.0=NR,    =1.0, (=NR)=1.00, =1=PP, (=NR, =1)=1.0=QA,    =0.6, (=QA)=0.867, =1=QA, (=QA, =1)=0.9=TP,    =0.8, (=TP)=0.984, =0=TP, (=TP, =0)=1.0=ST,    =0.9, (=ST)=0.871, =0=ST, (=ST, =0)=0.9Dialogue 1Sys: Hello.Usr: Hello.
Tell me today's news?Sys: Matsui, a part-time coach for the Giants,demonstrated 5 homeruns.
The Giantspresident told him "Come back as a player!
"Usr: What happened?Sys: The godzilla cannon came back to the Giants?spring camp in Miyazaki.Usr: What is the Godzilla cannon?Sys: The Godzilla coach demonstrated hishome-run batting to young players.Usr: Next please.Sys: To be a ace pitcher, has Fujinami improvedfrom the rookie year?
?=TP,    =0.8, (=TP)=0.800, =0=TP, (=TP =0)=0.8=QA,    =0.8, (=QA)=0.532, =0=ST, (=QA, =0)=0.5=QA,    =0.8, (=QA)=0.806, =1=QA, (=QA, =1)=0.8=TP,    =0.8, (=TP)=0.986, =0=TP, (=TP, =0)=1.0Dialogue 2Figure 5: A dialogue example.
(This example is translated from Japanese)Table 6: Accuracy of dialogue state tracking.rule focus POMDPAccuracy of tracking 0.561 0.869(1-best) (=175/312) (=271/312)Average reward -22.9 188.6improved SLU accuracy and robustness againstASR errors, especially reducing confusions be-tween question answering (QA) and topic presen-tation (TP).
Moreover, belief update can detect theTP state even if the SLU incorrectly predicts QAor ST.5.3 Discussion of trained policyAn example dialogue is shown in Figure 5.
Inthe example, the system selects appropriate ac-tions even if the observation likelihood is low.
Atthe 4th turn of Dialogue 1 in this example, the sys-tem with the user focus responds with an action ofproactive presentation a=PP, but the system with-out the user focus responds with an action of topicpresentation a=TP.
At the 2nd turn of Dialogue 2,the user asks a question without a focus.
The con-fidence of s=QA is lowered by the belief update,and the system selects the story telling modulea=ST.
These examples show that the training re-sult (=learned policy) reflects our design describedin Section 4.4: It is better to make a proactive pre-sentation when the user is interested in the topic.6 ConclusionsWe constructed a spoken dialogue system for in-formation navigation ofWeb news articles updatedday-by-day.
The system presents relevant infor-38mation according to the user?s interest, by track-ing the user focus.
We introduce the user focusdetection model, and developed a POMDP frame-work which tracks user focus to select the appro-priate action class (module) of the dialogue sys-tem.
In experimental evaluations, the proposed di-alogue management approach determines the stateof the user more accurately than the existing sys-tem based on rules.
An evaluation with a user sim-ulator shows that including user focus in the dia-logue manager?s belief state improves robustnessto ASR/SLU errors.In future work, we plan to evaluate the systemwith a large number of real users on a variety ofdomains, and optimize the reward function for theinformation navigation task.AcknowledgmentsWe thank Dr. Jason Williams for his valuable anddetailed advice to improve this paper on SIGDIALmentoring program.
This work was supported byGrant-in-Aid for JSPS Fellows 25-4537.ReferencesDan Bohus and Alexander I. Rudnicky.
2003.
Raven-claw: Dialog management using hierarchical taskdecomposition and an expectation agenda.
In Pro-ceedings of the 8th European Conference on SpeechCommunication and Technology, pages 597?600.Blai Bonet.
2002.
An e-optimal grid-based algorithmfor partially observable Markov decision processes.In Proceedings of International Conference on Ma-chine Learning, pages 51?58.Deborah A. Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg.
1994.
Expanding the scope of the ATIStask: the ATIS-3 corpus.
In Proceedings of theworkshop on Human Language Technology, pages43?48.Barbara J. Grosz and Candace L. Sidner.
1986.
Atten-tion, intentions, and the structure of discourse.
Com-putational Linguistics, 12(3):175?204.Ryuichiro Higashinaka, Katsuhito Sudoh, and MikioNakano.
2006.
Incorporating discourse featuresinto confidence scoring of intention recognition re-sults in spoken dialogue systems.
Speech Communi-cation, 48(3):417?436.Tatsuya Kawahara.
2009.
New perspectives on spokenlanguage understanding: Does machine need to fullyunderstand speech?
In Proceedings of IEEE work-shop on Automatic Speech Recognition and Under-standing, pages 46?50.Kazunori Komatani and Tatsuya Kawahara.
2000.Flexible mixed-initiative dialogue management us-ing concept-level confidence measures of speechrecognizer output.
In Proceedings of the 18th con-ference on Computational linguistics, pages 467?473.Esther Levin, Roberto Pieraccini, and Wieland Eckert.2000.
A stochastic model of human-machine inter-action for learning dialog strategies.
IEEE Transac-tions on Speech and Audio Processing, 8(1):11?23.Toyomi Meguro, Ryuichiro Higashinaka, Yasuhiro Mi-nami, and Kohji Dohsaka.
2010.
Controllinglistening-oriented dialogue using partially observ-able markov decision processes.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 761?769.Teruhisa Misu and Tatsuya Kawahara.
2010.
Bayesrisk-based dialogue management for document re-trieval system with speech interface.
Speech Com-munication, 52(1):61?71.George E. Monahan.
1982.
State of the art?
a surveyof partially observable Markov decision processes:Theory, models, and algorithms.
Management Sci-ence, 28(1):1?16.Naoaki Okazaki.
2007.
CRFsuite: a fast implementa-tion of Conditional Random Fields (CRFs).Yi-Cheng Pan, Hung yi Lee, and Lin shan Lee.
2012.Interactive spoken document retrieval with sug-gested key terms ranked by a markov decision pro-cess.
IEEE Transactions on Audio, Speech, andLanguage Processing, 20(2):632?645.Emanuel A. Schegloff and Harvey Sacks.
1973.
Open-ing up closings.
Semiotica, 8(4):289?327.Tomohide Shibata, Yusuke Egashira, and Sadao Kuro-hashi.
2014.
Chat-like conversational system basedon selection of reply generating module with rein-forcement learning.
In Proceedings of the 5th In-ternational Workshop Series on Spoken Dialog Sys-tems.Gokhan Tur, Umit Guz, and Dilek Hakkani-Tur.
2006.Model adaptation for dialog act tagging.
In Pro-ceedings of IEEE workshop on Spoken LanguageTechnology, pages 94?97.
IEEE.Christopher JCH Watkins and Peter Dayan.
1992.
Q-learning.
Machine learning, 8(3):279?292.Jason D. Williams and Steve Young.
2007.
Par-tially observable Markov decision processes for spo-ken dialog systems.
Computer Speech & Language,21(2):393?422.Jason D. Williams, Antoine Raux, Deepak Ramachan-dran, and Alan Black.
2013.
The dialog state track-ing challenge.
In Proceedings of the 14th AnnualMeeting of the Special Interest Group on Discourseand Dialogue, pages 404?413.39Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-hara.
2011.
Spoken dialogue system based on infor-mation extraction using similarity of predicate argu-ment structures.
In Proceedings of the 12th AnnualMeeting of the Special Interest Group on Discourseand Dialogue, pages 59?66.Koichiro Yoshino, Shinsuke Mori, and Tatsuya Kawa-hara.
2013a.
Incorporating semantic information toselection of web texts for language model of spokendialogue system.
In Proceedings of IEEE Interna-tional Conference on Acoustic, Speech and SignalProcessing, pages 8252?8256.Koichiro Yoshino, Shinji Watanabe, Jonathan Le Roux,and John R. Hershey.
2013b.
Statistical dialoguemanagement using intention dependency graph.
InProceedings of the 6th International Joint Confer-ence on Natural Language Processing, pages 962?966.Steve Young, Milica Ga?si?c, Simon Keizer, Franc?oisMairesse, Jost Schatzmann, Blaise Thomson, andKai Yu.
2010.
The hidden information state model:A practical framework for POMDP-based spokendialogue management.
Computer Speech & Lan-guage, 24(2):150?174.40
