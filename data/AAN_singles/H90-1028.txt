Prel iminary ATIS Development at MITVictor Zue, James Glass, David Goodine, Hong Leung,Michael Phillips, Joseph Polifroni, and Stephanie SeneffRoom NE43-601Spoken Language Systems GroupLaboratory for Computer ScienceMassachusetts Institute of TechnologyCambridge, MA 02139IntroductionDARPA has recently initiated a plan for a common spokenlanguage task, to be developed independently b  all membersof the DARPA community, with the hope that it will providea mechanism leading to appropriate formal evaluation pro-cedures at the level of spoken language.
The task that wasselected for this purpose is the Air Travel Information Sys-tem (ATIS) task, based on selected tables from the OfficialAirline Guide (OAG).
It was decided that the first evaluationwould be limited in scope to deal with text input only, andto cover only sentences that could be understood unambigu-ously out of context.
Data have been recorded over the pastseveral months at Texas Instruments, using an interface thatinvolves a "wizard" who fully interprets the meaning of thesubject's entences, and generates database responses usinga menu driven data access ystem.We have been actively engaged in the last few months indeveloping the natural anguage and back end portions of theMIT version of the ATIS domain.
This paper describes ourprogress to date on this effort, including an evaluation of theperformance of the system on the recently released esignatedDARPA test set.
The remainder of this paper is organized asfollows.
First we will give a general description of the systemwe are developing, emphasizing those aspects that differ fromthe current general conception of the common task.
Next wewill describe in greater detail certain aspects of the back end,including knowledge representation, control strategy, the userinterface, and our preliminary treatment of discourse history.This is followed by a section describing changes made in theparser, in the areas of semantics, the interface with the back-end, and a preliminary new-word treatment.
This sectionalso includes a brief discussion of some interesting phenom-ena that occurred in the training sentences.
An evaluationsection follows, discussing our system's performance on bothtraining and test data, as well as a preliminary assessment ofthe perplexity of the system.
We conclude with a summaryof our results and our position on the nature of the commontask.General DescriptionOur conception of an ATIS system is somewhat differentfrom the one defined by the common task.
First, we wouldlike a domain that is sufficiently restrictive that we could hopeto cover most sentences spoken with high probability.
That is,it should be easy for the user to understand the limits of thedomain.
Secondly, we wanted a task that would be of prac-tical interest o a large number of people with minimal priortraining.
The core task of the OAG, booking flights, meetsthat requirement.
We believe that the task makes sense if itis the traveler, not the agent, who is seeking the information.The traveler would be willing to take the extra time to findout what meals are served and whether he/she could savemoney with a stopover in Denver.
The agent, on the otherhand, will typically have no knowledge of the prior prefer-ences of the client in these regards.We took several steps to limit the types of queries theuser would ask.
First of all, we omit the display of all codenumbers, flight code, fare code and connection code.
Con-necting flights axe displayed irectly as pairs of the two legsfor each flight, thus obviating the need for questions uch as"Show the flights under the connection code 456767."
Sec-ondly, in general we display only items of key interest o thetraveler.
For instance, the subject does not get a meal-codeor duration column unless they explicitly ask for them witha sentence such as, "What meals are served on flight twenty-nine."
We omitted most of the columns in the aircraft able,presuming that the majority of travelers would consider theinformation irrelevant.
The subject can ask general questionsabout the aircraft type for the flights of interest, but cannotask specific questions about range, compression, etc.We designed the system to be user-friendly as much aspossible.
We placed heavy emphasis on discourse, which wefound to be a very interesting research problem within thisdomain.
Since the system presumes that certain restrictionsfrom the discourse history still hold, we found that it wasimportant for the system to give the user feedback on itsunderstanding of the sentence in context.
To that end, thesystem answers in complete sentences of the form, "There axe4 direct flights from Boston to San Francisco leaving before3:00 p.m. serving lunch," prior to displaying the table.
Weare hoping that the user will be less inclined to speak "com-puterese" if the computer behaves a little more like a person.Knowledge RepresentationWe made a major paradigm shift in moving from VOY-AGER \[5\] to hTm in terms of back-end function operations, onethat was necessitated by the fact that the database should beaccessed only once, after all restrictions are in place.
WithinVOYAGER, low-level functions would access the database di-130rectly, passing on sets of objects to be filtered by later low-level functions.
Thus, for example the (restaurant) functionreturns the set of all restaurants, which might later be sub-setted by the (serve) function to include, for example, onlyrestaurants serving Chinese food.In ATIS, low level functions typically fill slots in an eventframe with appropriate semantic information.
Once the en-tire sentence has been processed and the history frames havebeen merged, an IDIL\[2\] 1 query is then constructed from thecompletely specified frame.
We had initially constructed thequery "on the fly," but we found that this led to much com-plexity at the end because of discourse ffects that would re-quire last minute alterations on the restrictions in the query.An ExampleAs in the VOYAGER domain, we have taken the viewpointthat the parse tree contains emantically oaded nodes at thelower levels, and these are in fact the only semantic infor-mation that is used by the interface between TINA\[3\] andthe back-end.
The presence of certain nodes within specifiedpositions in the hierarchy triggers the execution of particularfunctions whose action is typically to update slots in a seman-tic representation r event frame.
The event frame is passedas the first argument to each new function that is called, andthe final event, frame then has in place all of the appropriateinformation to be extracted from the sentence.We will illustrate how the ATIS system converts a sen-tence into an event frame by walking through a single exam-ple.
Consider the sentence, "Show me the flights on Septem-ber twenty ninth from San Francisco to Denver that leaveafter twelve and cost less than two hundred dollars."
The setof nested commands generated by an analysis of the parse treeis shown in Figure 1.
The innermost function is the (flight)command which was triggered by the presence of the seman-tic node A-FLIGHT within the event node FLIGHT-EVENT inthe parse tree 2.
It generates an event frame with appropriateslots for all of the types of information that might need to berecorded for a flight.The event frame is then passed in turn as the first argu-ment to a sequence of functions, each generated by a par-ticular post-modifier to the noun "flights."
The presence ofa FROM-PLACE node triggers a call to the function (from)with the arguments \[flight event\] 3 and the city \[San Fran-cisco\].
The (from) function fills the \[departure-place\] slot inthe event frame with the entry \[San Francisco\], and then re-turns the event frame.
The (to) function is then called withthe other city, \[Denver\] as the second argument, and it fillsan \[arrival-place\] slot with the appropriate information.
The(date) function then processes the number "twenty ninth"and the month "September" to update the \[date\] slot withan entry that includes Saturday as the day of the week, as-suming 1990 as the year.
The function (clock) is called on thellntelligent Database Interface Language, provided to us by re-searchers at Unisys as an intermediary toSQL.~These were found through a path that included the nodes \[request\],\[verb-phrase-show\] and [dir-object\].3returned by the function (flight).
(price(leave(hour(date(to(from(flight)(city \[San Francisco\]))(city \[Denver\] )(number \[twenty ninth\])(month \[September\]))\[after\](clock \[twelve\] nil nil)))\[less\](number \[two hundred\])\[dollars\] )F igure  1: Nested functions resulting from analy-sis of the parse tree for the sentence, "Show methe flights on September 29th from San Franciscoto Denver that leave after twelve and cost less thantwo hundred dollars.
"clock-time twelve 4, and it produces a time event-frame whichis provided as an argument, along with the keyword \[after\] toan (hour) function.
The (hour) function in turn sets a generic\[time\] slot in the event frame to a \[range\] frame representing"after twelve."
The verb "leave" triggers a (leave) functionwhich simply moves the entry in the \[time\] slot to a morespecific place, the \[departure-time\] slot.
Since the temporalmodifier occurred as an argument of the verb "leave," it is al-ways guaranteed that the appropriate time specifications willbe available in the generic \[time\] slot when the (leave) func-tion is called.
Finally, the verb phrase, "costing less than twohundred ollars" generates a call to the (price) function, withthe arguments \[flight-event\], a key-word "less," the number"200 "5 and the unit "dollars."
The price function generatesanother \[range\] frame specifying a price range, and puts itinto the \[fare\] slot of the \[flight-event\].The resulting \[flight-event\] then contains a number of filledslots, which permit it to generate a self description phrase ofthe form, "flights from San Francisco to Denver departingafter 12:00 on Saturday, September 29 costing less than 200dollars."
The final processing of this \[flight-event\] fillsin someadditional slots that were left unspecified by the subject.
Itpresumes, through a set of default conditions, that this mustbe direct flights, the time should be 12:00 P.M., and the pricerestriction should be on one-way fares.
Another option wouldbe to query the user for clarification at this time.
Finally,an IDIL query is constructed from all of the information inthe \[flight-event\], with the appropriate restrictions, and thedatabase is accessed.Discourse HistoryThe MIT ATIS system maintains a discourse history ofthe set of recently mentioned flight restrictions.
These re-strictions are merged into the current flight event object after4The null arguments are for A.M./P.M.
and time zone.%eturned by the function (number).131the new sentence has been completely analyzed.
The mergeprocedure allows the flight event to inherit from the historyall the features that have not been explicitly mentioned inthe current sentence.
Thus if the user says, "Show all Deltaflights from Boston to Dallas," followed by, "Show only themorning flights," the system will return all Delta flights fromBoston to Dallas leaving in the morning.
If the new sentencementions both a from-place and a to-place, the discourse his-tory is completely erased, under the assumption that the useris moving on to a new topic.
Likewise, whenever the user suc-cessfully books a flight or set of flights, the history is erased.The system refuses to book a set of flights upon request if thenumber of flights that meet the requirements is not consistentwith the number of flights the user wanted to book.
In thisfailure case the system does not erase the history.
The usercan digress to inquire about definitions of entries or headingsin a table, etc., in which case the history is ignored but noterased.
If the subject refers anaphorically to the flight set, asin "Which of these flights serve breakfast," the system filtersthe database directly on the set of flight codes available fromthe previous entence.Our discourse history mechanism was actually used on anumber of sentences among the type A training set.
Thesewere either compound sentences or two-sentence units spo-ken without a break.
A good example is the sentence pair,"I would like to take Delta flight number eighty-three to At-lanta.
What is the cheapest fare I can get?"
The transla-tion of the parse tree to function calls produced a list of twosets of nested functions, which, when evaluated, generate twoflight event frames, named \[Delta flight 83 to Atlanta\] and\[Direct flights costing minimum\].
The system first completesthe action on the first flight event, providing the user witha time table on Delta flight 83 to Atlanta.
It then uses thisinformation as discourse history to answer the second ques-tion, producing a table containing the Delta flight along withthe price and restrictions for the least expensive fare on thatflight.Natural Language IssuesIn this section we will discuss changes that were intro-duced within the natural language component as a directconsequence of issues that came up within the ATIS task.These include a significant modification to the way semanticfiltering is done, the introduction of a preliminary attempt ohandle new words in certain restricted contexts, and modifi-cations in the interface between T INA and the back-end.
Wewill also discuss some interesting linguistic phenomena thatoccurred in the spoken sentences of the subjects.Semant ic  F i l te r ingThe ATIS domain led to a significant modification in theway TINA handles emantic filtering.
The main change is that,when a new node in the parse tree carries semantic informa-tion, it or's in its semantic bits with the pre-existing bits,rather than simply replacing them.
In conjunction with thismechanism, all pre-existing semantic bits are erased withinthe domain of each new clause.
This has the effect of isolatingthe semantics of each clause 6This change was inspired by theinherent free order in the language of the arguments of verbssuch as leave and arrive.
These verbs typically take a FaOM-PLACE, a TO-PLACE, and an AT-TIME as arguments, but theycan occur in any order.
Due to the cross-pollenization f rulesin TINA, the context-free rules will end up licensing an infinitesequence of multiple arguments.
With the semantics or'd in,however, a proposed sibling can simply fail if its semantic bithas already been set.
This corresponds to saying that, withina clause, only a single fROM-PLACE is permissible.
We foundthis mechanism to be quite powerful, and we intend to modifyVOYAGER to also make use of it.New Word  Prob lemWe have developed a very preliminary capability for han-dling new words that show up in non-content positions in thesentence.
This is done by mapping any word not in the lexiconto a category called LOST, and then allowing certain selectedterminals to accept his LOST category as a suitable solution.These words are restricted to positions that either don't affectthe meaning of the sentence, or affect the meaning in such away that a parent node can infer the intended action.
Thatis, the system should be able to provide a "guess" at the cor-rect answer without actually knowing the explicit spelling ofthe unknown word.
Thus, if the user says, "Swell, now showme the flights again," and the system doesn't know the word"swell," it will allow it to map to the same terminal node as"okay," and then proceed to ignore it.
A semi-content wordwould also be acceptable as in, "Please explain the letter Q.
"The system allowed the unknown word "letter" to come un-der the same terminal node as the word "symbol."
A fewverbs, such as "show" are also allowed to map to unknownwords.
This mapping saved a sentence containing the un-known contraction, "what're," but it also gave a parse witha grossly incorrect meaning in another sentence by substitut-ing "show" for the verb "cancel!"
Although we have not donea formal study, there were a number of sentences that were"saved" by this simple strategy.This approach would only be effective for spoken input inconjunction with a new-word capability within the recognizer.One possibility has been proposed by researchers at BBN\[1\], where an all-phone general-word model is permitted tosucceed only on the condition that it overcomes a stiff word-score penalty.I n te r face  between TINA and  the  Back-endWe found that the methods for converting TINA's parsetree to a set of back-end functions that had been developedfor VOYAGER worked very well within the ATIS domain.
Wemade only one significant improvement, which was easy toimplement but had far-reaching effects.
This was to allowcertain node types to block access to their children by theirancestors, at the time of analysis of the parse tree to pro-duce back end function calls.
This mechanism serves to iso-late embedded clauses and noun phrases.
For example, aFARE-EVENT expects to process a postmodifier such as PRED-ADJUNCT for a sentence such as "Show all fares from BostonSThe current-focus and float-object retain semantic information fromoutside the clause that nodes within the clause may make use of.132to Denver."
However, it should not directly process PRED-ADJUNCT in the sentence, "Show all fares for flights fromBoston to Denver," where the PRED-ADJUNCT is embedded inthe object of the preposition for in a following FOR-PI- IKASE.With the blocking mechanism in place, the OBJECT-PREP ap-pears as a terminal to the FARE-EVENT noun phrase.
To fetchthe information in the PRED-.~DJUNCT it must go through apath that finds the series FOR-PItRASE, OBJECT-PREP, andFLIGHT-EVENT.
It is then the job of the FLIGHT-EVENT nodeto pick up the postmodifying PRED-ADJUNCT.
If the blockingis not done, then the post-modifier will be picked up multipletimes.There are several examples of conjunction within the ATISdomain, both at the clause level and at the noun-phraselevel.
We handled conjunction in a case-specific way, basedon whether the functions in the back-end would prefer to takemultiple arguments or be called multiple times.
For example,a sentence such as "Show all flights from Boston to Denverand show the fares," produces a list of two event-objects,one for each main clause.
It behaves just like a sequenceof two sentences, with the semantics of the first clause act-ing as discourse history when the second clause is processed.A sentence such as "Show a/1 flights from San Francisco orOakland," processes the function from-place on multiple ar-guments.
A sentence such as, "What do L, R, and T mean,"on the other hand, calls the code function three times, oncefor each code name.
The interface is flexible enough to handtailor the conjunction mechanism to match the functionalityof the conjoined elements.Interesting Sentence ConstructsThere were a surprising number of examples in this data-base of people violating the rules of syntax and semantics.One common error was to use a verb in the singular formwhen the subject contained multiple singular nouns, as in"What does A, L, R and T mean?"
We decided to allow sin-gular form optionally in the grammar in such a case.
How-ever, a sentence containing the phrase "all the arrival time"failed to parse on account of number violation between theplural "all" and the singular "time."
Similarly, the type-Atraining sentence, "What does the abbreviations under trans-port stand for," failed on number agreement between "does"and "abbreviations."
We decided to retain these number con-straints in the system, thus sacrificing these sentences.We were intrigued by the relatively frequent occurrenceof the verbs "leaving" and "arriving" in situations where theapparent subject of the verb should not carry a -..\[-ANIMATEfeature.
These sentences initially failed to parse on accountof subject/verb semantic agreement failure.
Some examplesare given in Table 1.
We pondered the problem for sometime, wondering whether a ticket, a flight-list, and a fareshould inherit some properties from an implicit flight-eventthat would give them mobility.
However, we finally cameto the realization that these forms are probably acting as adangling participle with an implied PRo subject, appearingat a higher level in the parse tree and therefore not modifyingthe main noun of the noun phrase.
When dealt with this way,the forms could be kept highly restrictive in that 1) they canonly occur as gerunds in the present-participle form, and 2)they cannot be followed by a set of additional modifiers forthe main noun.
According to these rules, sentences such as "Ineed a one-way ticket to Denver that leaves in the afternoon,"and "I need a one-way ticket leaving in the afternoon andcosting less than 200 dollars," would fail to parse.
Whenwe implemented this gerund form as a dangling participle,all of the sentences containing this odd construct could thenparse, because the CURRENT-FOCUS slot was empty at thetime the verb was processed, implying a null subject.
Thissolution also handles a similar phenomenon i sentences suchas, "Take me from San Francisco to Denver on Sept. 25thleaving after 7:00 P.M. but arriving before 10:00 A.M."Table 1: Some examples from the training data ofsentences with an unusual semantic phenomenon.?
I need a one-way ticket to Denver from San Franciscoon Sept. 29th leaving in the afternoon.?
May I see the airfare leaving from San Francisco toDFW, leaving at 12:00 P.M. and arriving at h17 P.M.??
Cost of a first class ticket Dallas to San Franciscodeparting August the 6th.?
Give me some flight information Dallas to SanFrancisco departing August 6th.Per formance  Eva luat ionFour independent releases of so-ca/led Type A sentenceswere distributed to the DARPA community over the past fewweeks, to be used for system development in preparation forthe first official DARPA formal spoken language valuation.Type A sentences are roughly defined as sentences containingno context-dependencies, andhaving answers that are unam-biguous and available within the database.
In this sectionwe will attempt o tabulate our progress on handling thesesentences, and we will also discuss our performance on theofficial test set.A record of our progress over time was kept by evaluatingthe developing system on each new set upon its arrival, bothin terms of parser coverage and agreement of the back-endresponses with the canonical answers.
A comparison betweenperformance on the new set and performance of the identicalsystem on the previous training set gives an indication of howwell the system is generalizing to unseen data.
The resultsare summarized in Figures 2 and 3.
Figure 2 gives the initialand final percent coverage of the parser, for each data set, aswell as an initial performance for the test set.
Figure 3 givesinitial, intermediate, and final performance for the percentagreement between our responses and the canonical ones.
Theintermediate stage represents the status of each release uponarrival of the next release.
The solid lines connect runs thatwere done on the same system.We saw very little convergence on the patterns of sen-tences spoken in each data set.
Both figures show a consis-tent trend downward in performance on unseen data.
We arequite concerned that rules created to deal with utterances inone release don't seem to generalize well to new releases.
Wewere eventually able to get parser coverage up to about 89%133overall and response performance up to about 85% overall,for the training data.A detailed analysis of the results for the 93 sentence testset is given in Figure 4.
71% of the sentences in the test setparsed, and 60% gave a correct answer to the comparator.
Wecategorized sentences in one of three bins: "parsed," "wordspot" and "failure."
The "word spot" category comprises sen-tences which failed to parse but for which we gave an answerby finding a key abbreviation or table heading and presum-ing that the user wanted a definition of it.
This step wasinspired by training sentences of the form, "What is trans-port stand for?"
that we didn't want our parser to be able tohandle, but could answer correctly by simply detecting theword "transport."
As can be seen from the table, this ideawas notably unsuccessful, in that only one of the 21 sentencesin this category obtained a correct answer.
This is in somesense reassuring, because it argues in favor of the extra effortrequired to fully parse a sentence.The most interesting category is the set of sentences thatparsed but for which we gave either no answer or an incorrectone.
There were a total of 12 sentences in this category, six ofwhich failed due to an inappropriate ranslation of the parsetree into back-end functions.
7 The remaining six failed dueto problems in the back end or inconsistencies between ourunderstanding of the "correct" answer and the true one.
Twosentences failed because we had assumed that "San Franciscoairport" meant only "SFO," whereas the canonical answerincluded "OAK" as well.
Another two failed due to a tablefault in translating from "Dallas Fort Worth" to "DFW."
Thefinal two failed because we were not properly informed of thecanonical definition of "advance purchase.
"'0,9o.100.80.60.40.20.o Initial?
F inal1 2 3 4 TestData  SetFigure 2: Graph of percent coverage of TINA as afunction of release number, for the four sequentialreleases and the test set.In order to assess the difficulty of this task for recognition,we computed the test-set perplexity for the condition whenprobabilities were obtained from the four releases of train-ing data, and the condition when all words were consideredequally likely.
The vocabulary size was 571 words, and theperplexity, without probabilities, was 166.5, or nearly 1/3 of?The four "no answer" sentences fell inthis bin.10080~' 6o00'~ 40 ?,92Oo InitialO Intermediate@ Finali 2 3 4 TestData  SetFigure 3: Graph of percent agreement between sys-tem responses and canonical answers, as a functionof release number, for the four sequential releasesand the test set.
The intermediate p rformance, ineach case, is the level of performance for that set atthe time of release of the next set.80- -  I 60 Wrong Answ~@Parsed Word Spot FailureFigure 4: Detailed analysis of the results for theTI test data.the full vocabulary, s However, when we trained on frequen-cies of occurrence in the training data, the perplexity wasreduced to only 10.8, a factor of 15 improvement.
We werevery much surprised by this result, as the VOYAGER.
domainonly gives a factor of three improvement, although ResourceManagement had yielded a factor of eight.
We suspect hatthis is due in part to the fact that the ATIS domain containsterminal categories uch as certain abbreviations that havelarge numbers of words but rarely occur.
In addition, therewere no sentence fragments and no sentences using indirectspeech such as "I want to go...." For instance, the sentence"Show me the flights," gives a perplexity reduction from 300to 5 with the use of probabilities, reflecting the fact that thisis a very common form.
*A word-pair grammar would give a further increase in perplexity.For Voyager the perplexity increased from 28 to 73 when long-distanceconstraints were ignored.134Data  Col lect ionWe have performed a preliminary data collection session,using a procedure that mimics closely the one we used forVOYAGER\[4\].
We told subjects to pretend they were a trav-eler planning a trip.
Their task was to work together withthe system to select particular flights that would meet theirrequirements.
They could book the selected flights, with theunderstanding that booking in real use means recording therelevant information so that a travel agent could completethe task.
We reminded them of the types of information thesystem contains (meals, fares, times, etc.).
The wizard's taskwas a simple one of typing in verbatim (minus false starts)what the user said.
The system answered as well as it could,and identified unknown words or points in the sentence whereit got stuck.
We found that over 100 sentences could be col-lected in an hour.
Preliminary examination of the sentencesshows that they are quite different from the ones collectedat TI, in particular with regard to indirect speech acts andanaphora.
We are encouraged by the results of our data col-lection procedure, and are hopeful that we can contribute tocollecting more spoken sentences in the future.SummaryWe have now completed an initial port of our natural an-guage system to the new ATIS domain, and we have devel-oped our first interface with a standard atabase using SQL.For the most part, methods used for the VOYAGER domaincould be effectively applied.
The most significant changewas to maintain an intermediate representation of the re-suits of processing a sentence in the form of a hierarchicaltree of frames recording the appropriate semantic informa-tion.
A final postprocessing function develops an SQL queryfrom the semantic representations, and fetches the appropri-ate database ntries.We spent considerable ffort on expanding the coverageof our system to handle forms that showed up in the fourreleases of Type A sentences from Texas Instruments.
As aconsequence of this effort, we were able to achieve an 89%coverage overall of the parser on these sentences, with 85%of them yielding an answer that conformed to that requiredby the comparator.
For the test release, 71% of the sentencescould parse, and 60% gave a correct response.We believe that the idea of a common task involving book-ing flights is a good one.
It would be useful to a great dealof people with no prior experience, and hence it could po-tentially be a profitable and practical enterprise.
However,we feel that it is essential for the success of such a systemthat it behave in a graceful way, such that the informationavailable to the user is clear, concise, and relevant.
Withthese ideas in mind, we have tried to emphasize those aspectsof the system that make it effective to the user, namely re-sponses in the form of fully descriptive sentences along withthe displays, minimal information in the tables so as not toconfuse the subject, and effective discourse capabilities.
Wewould like to produce a system that can be a useful aid forproviding the subject with the relevant information to selectflights to be booked.
It is our opinion that other aspects ofthe database, such as specific aircraft capabilities and groundtransportation, should be de-emphasized at first, pending thesuccessful closing of the loop to include spoken input withina narrower domain of booking flights.
Overall however, wehave found the process of developing an ATIS system to be re-warding and challenging.
We look forward to the time whenwe will be able to work with spoken input, thus integratingthe natural anguage component with a recognizer.Acknowledgement  sWe would like to thank Lynette Hirschman, Don McKay,and Andy Glick of Unisys for their help in installing ourdatabase management system and providing us with IDILsoftware to interface with it.References\[1\] Asadi, A., R. Schwartz, and J. Makhoul, "AutomaticDetection of New Words in a Large-VocabularyContinuous Speech Recognition System," Proceedings,ICASSP 90, Albuquerque, NM, 3-6 April 1990, pp.125-129.\[2\] O'Hare, A.
B., "The Intelligent Database Interface,"Technical Report, Unisys Paoli Research Center, 1989.\[3\] Seneff, S. "Probabilistic Parsing for Spoken LanguageApplications," paper presented at the InternationalWorkshop in Parsing Technologies, Pittsburgh, PA,28-31 August 1989.\[4\] Zue, V., N. Daly, J.
Glass, D. Goodine, H. Leung, M.Phillips, J. Polifroni, S. Seneff, M. Soclof, "TheCollection and Preliminary Analysis of a SpontaneousSpeech Database," paper presented at the SecondDARPA Speech and Natural Language Workshop,Harwichport, MA, 15-18 October 1989.\[5\] Zue, V., J.
Glass, D. Goodine, H. Leung, M. Phillips, J.Polifroni, and S. Seneff, "The VOYAGER SpeechUnderstanding System: A Progress Report," paperpresented at the Second DARPA Speech and NaturalLanguage Workshop, Harwichport, MA, 15-18 October1989.135
