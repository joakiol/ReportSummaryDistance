FrameNet-based Semantic Parsing using Maximum Entropy ModelsNamhee KwonInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292nkwon@isi.eduMichael FleischmanMessachusetts Institute ofTechnology,77 Massachusetts AveCambridge, MA 02139mbf@mit.eduEduard HovyInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292hovy@isi.eduAbstractAs part of its description of lexico-semanticpredicate frames or conceptual structures, theFrameNet project defines a set of semanticroles specific to the core predicate of asentence.
Recently, researchers have tried toautomatically produce semantic interpretationsof sentences using this information.
Buildingon prior work, we describe a new method toperform such interpretations.
We definesentence segmentation first and show howMaximum Entropy re-ranking helps achieve alevel of 76.2% F-score (answer among top-five candidates) or 61.5% (correct answer).1 IntroductionTo produce a semantic analysis has long been agoal of Computational Linguistics.
To do so,however, requires a representation of the semanticsof each predicate.
Since each predicate may have aparticular collection of semantic roles (agent,theme, etc.)
the first priority is to build a collectionof predicate senses with their associated roleframes.
This task is being performed in theFrameNet project based on frame semantics(Fillmore, 1976).Each frame contains a principal lexical item asthe target predicate and associated frame-specificroles, such as offender and buyer, called frameelements.
FrameNet I contains 1,462 distinctpredicates (927 verbs, 339 nouns, 175 adjectives)in 49,000 annotated sentences with 99,000annotated frame elements.
Given these, it wouldbe interesting to attempt an automatic sentenceinterpretation.We build semantic parsing based on FrameNet,treating it as a classification problem.
We split theproblem into three parts: sentence segmentation,frame element identification for each segment, andsemantic role tagging for each frame element.
Inthis paper, we provide a pipeline framework ofthese three phases, followed by a step of re-rankingfrom n-best lists of every phase for the final output.All classification and re-ranking are performed byMaximum Entropy.The top-five final outputs provide an F-score of76.2% for the correct frame element identificationand semantic role tagging.
The performance of thesingle best output is 61.5% F-score.The rest of the paper is organized as follows: wereview related work in Section 2, explainMaximum Entropy in Section 3, describe thedetailed method in Section 4, show the re-rankingprocess in Section 5, and conclude in Section 6.2 Related WorkThe first work using FrameNet for semanticparsing was done by Gildea and Jurafsky (G & J,2002) using conditional probabilistic models.They divide the problem into two sub-tasks: frameelement identification and frame elementclassification.
Frame element identificationidentifies the frame element boundaries in asentence, and frame element classificationclassifies each frame element into its appropriatesemantic role.
The basic assumption is that theframe element (FE) boundaries match the parseconstituents, and both identification andclassification are then done for each constituent1.In addition to the separate two phase model offrame element identification and role classification,they provide an integrated model that exhibitsimproved performance.
They define a frameelement group (FEG) as a set of frame elementroles present in a particular sentence.
Byintegrating FE identification with role labeling,allowing FEG priors and role labeling decision toaffect the determination of next FE identification,they accomplish F-score of 71.9% for FEidentification and 62.8% for both of FEidentification and role labeling.
However, sincethis integrated approach has an exponentialcomplexity in the number of constituents, theyapply a pruning scheme of using only the top m1 The final output performance measurement is limitedto the number of parse constituents matching theframe element boundaries.hypotheses on the role for each constituent (m =10).Fleischman et al(FKH, 2003) extend G & J?swork and achieve better performance in roleclassification for correct frame element boundaries.Their work improves accuracy from 78.5% to84.7%.
The main reasons for improvement arefirst the use of Maximum Entropy and second theuse of sentence-wide features such as Syntacticpatterns and previously identified frame elementroles.
It is not surprising that there is adependency between each constituent?s role in asentence and sentence level features reflecting thisdependency improve the performance.In this paper, we extend our previous work(KFH) by adopting sentence level features even forframe element identification.3 Maximum EntropyME models implement the intuition that the bestmodel is the one that is consistent with the set ofconstraints imposed by the evidence, but otherwiseis as uniform as possible (Berger et al 1996).
Wemodel the probability of a class c given a vector offeatures x according to the ME formulation below:)],(exp[1)|(0xcfZxcp iinix?=?=Here xZ  is normalization constant, ),( xcfi  is afeature function which maps each class and vectorelement to a binary value, n is the total number offeature functions, and i?
is a weight for thefeature function.
The final classification is just theclass with the highest probability given its featurevector and the model.It is important to note that the feature functionsdescribed here are not equivalent to the subsetconditional distributions that are used in G & J?smodel.
ME models are log-linear models in whichfeature functions map specific instances of featuresand classes to binary values.
Thus, ME is not herebeing used as another way to find weights for aninterpolated model.
Rather, the ME approachprovides an overarching framework in which thefull distribution of classes (semantic roles) givenfeatures can be modeled.4 ModelWe define the problem into three subsequentprocesses (see Figure 1): 1) sentence segmentation2) frame element identification, and 3) semanticrole tagging for the identified frame elements.
Inorder to use sentence-wide features for the FEidentification, a sentence should have a single non-overlapping constituent sequence instead of all theindependent constituents.
Sentence segmentationis applied before FE identification for this purpose.For each segment the classification into FE or notis performed in the FE identification phase, andfrom the FE-tagged constituents the semantic roleclassification is applied in the role tagging phase.He got up, bent briefly over her hand.
(He) (got up) (bent) (briefly) (over her hand)FE NO T FE FE(He) (briefly) (over her hand)Agent Manner PathInput sentence1) Sentence Segmentation:choose the highest constituentswhile separating target word2) Frame Element Boundary Identification:apply ME classification to classify each segmentinto classes of FE (frame element), T (target), NO (none)Extract the identified FEs:choose segments that are identified as FEs3) Semantic Role Tagging:apply ME classification to classify each FEInto classes of 120 semantic rolesOutput role: Agent (He), Manner (briefly), Path (over her hand)for the target ?bent?Fig.
1.
The sequence of steps on a sample sentence.4.1 Sentence SegmentationThe advantages of applying sentencesegmentation before FE identification areconsidered in two ways.
First we can utilizesentence-wide features, and second the number ofconstituents as FE candidates is reduced, whichreduces the convergence time in training.We segment a sentence with parse constituents2.During training, we split a sentence into true frameelements and the remainder.
After choosing frameelements as segments, we choose the highest levelconstituents in parse tree for other parts, and thenmake a complete sentence composed of a sequenceof constituent segments.
During testing, we needto consider all combinations of various levelconstituents.
We know the given target wordshould be a separate segment because a target wordis not a part of other FEs.
Since most frameelements tend to be among the higher levels of aparse tree, we decide to use the highestconstituents while separating the target word.Figure 2 shows an example of the segmentation for2 We use Michael Collins?s parser :http://www.cis.upenn.edu/~mcollins/an actual sentence in FrameNet with the targetword ?bent?.He got up bent briefly over her handPRP VBD RPPRTVBD RP IN PRP$ NNVPVPVPADVPNPPPNPSFig.
2.
A sample sentence segmentation: ?bent?
isa target predicate in a sentence and the shadedconstituent represents each segment.However, this segmentation for testing reducesthe FE coverage of constituents, which means ourFE classification performance is limited.
Table 1shows the FE coverage and the number ofconstituents for our development set.
The FEcoverage of individual constituents (86.36%)means the accuracy of the parser.
This limitationand will be discussed in detail in Section 4.4.Method Number of constituentsFE coverage(%)Individualconstituents  115,380 86.36Sentencesegmentation 29,688 77.25Table 1.
The number of constituents and FEcoverage for development set.4.2 Frame Element IdentificationFrame element identification is executed for thesequence of segments.
For the example sentence inFigure 2, ?
(He) (got up) (bent) (briefly) (over herhand)?, there are five segments and each segmenthas its own feature vector.
Maximum Entropyclassification into the classes of FE, Target, orNone is conducted for each.
Since the targetpredicate is given we don?t need to classify a targetword into a class, but we do not exclude it from thesegments because we want to get benefit of usingprevious segment?s features.The initial features are adopted from G & J andFKH, and most features are common to both offrame element identification and semantic roleclassification.
The features are:?
Target predicate (target): The targetpredicate, the principal word in a sentence, isthe feature that is provided by the user.Although there can be many predicates in asentence, only one predicate is defined at atime.?
Target identification (tar): The targetidentification is a binary value, indicatingwhether the given constituent is a target or not.Because we have a target word in a sequenceof segments, we provide this informationexplicitly.?
Constituent path (path): From the syntacticparse tree of a sentence, we extract the pathfrom each constituent to the target predicate.The path is represented by the nodes throughwhich one passes while traveling up the treefrom the constituent and then down throughthe governing category to the target word.
Forexample, ?over her hand?
in a sentence ofFigure 2 has a path PP?VP?VBD.?
Phrase Type (pt): The syntactic phrase type(e.g., NP, PP) of each constituent is alsoextracted from the parse tree.?
Syntactic Head (head): The syntactic head ofeach constituent is obtained based on MichaelCollins?s heuristic method3.
When the head isa proper noun, ?proper-noun?
substitutes forthe real head.
The decision if the head isproper noun is done by the part of speech tagin a parse tree.?
Logical Function (lf): The logical functions ofconstituents in a sentence are simplified intothree values: external argument, objectargument, other.
We follow the links in theparse tree from the constituent to the ancestorsuntil we meet either S or VP.
If the S is foundfirst, we assign external argument to theconstituent, and if the VP is found, we assignobject argument.
Otherwise, other is assigned.Generally, a grammatical function of externalargument is a subject, and that of objectargument is an object.
This feature is appliedonly to constituents whose phrase type is NP.?
Position (pos): The position indicates whethera constituent appears before or after the targetpredicate and whether the constituent has thesame parent as the target predicate or not.?
Voice (voice): The voice of a sentence (active,passive) is determined by a simple regularexpression over the surface form of thesentence.?
Previous class (c_n): The class information ofthe nth-previous constituent (target, frameelement, or none) is used to exploit thedependency between constituents.
Duringtraining, this information is provided by simply3 http://www.ai.mit.edu/people/mcollins/papers/headslooking at the true classes of the frame elementoccurring n-positions before the currentelement.
During testing, hypothesized classesof the n elements are used and Viterbi search isperformed to find the most probable tagsequence for a sentence.The combination of these features is used in MEclassification as feature sets.
The feature sets areoptimized by previous work and trial and errorexperiments.
Table 2 shows the lists of feature setsfor ?briefly?
in a sentence of ?He got up, bentbriefly over her hand?.
These feature sets containthe previous or next constituent?s features, forexample, pt_-1 represents the previousconstituent?s phrase type and lf_1 represents thenext constituent?s logical function.Feature Set Example Functionsf(c, target) f(c, ?bent?)
= 1f(c, target, pt) f(c, ?bent?,ADVP) = 1f(c, target, pt, lf) f(c, ?bent?,ADVP,other) = 1f(c, pt, pos, voice) f(c, ADVP,after_yes,active) = 1f(c, pt, lf) f(c, ADVP,other) = 1f(c, pt_-1, lf_-1) f(c, VBD_-1, other_-1) = 1f(c, pt_1, lf_1) f(c, PP_1, other_1) = 1f(c, pt_-1, pos_-1,voice) f(c, VBD_-1,t_-1,active) = 1f(c, pt_1, pos_1, voice) f(c, PP_1,after_yes_1, active) = 1f(c, head) f(c, ?briefly?)
= 1f(c, head, target) f(c, ?briefly?, ?bent?)
= 1f(c, path) f(c, ADVP?VP?VBD) = 1f(c, path_-1) f(c, VBD_-1) = 1f(c, path_1) f(c, PP?VP?VBD_1) = 1f(c, tar) f(c, 0) = 1f(c, c_-1) f(c, ?target?_-1) = 1f(c, c_-1,c_-2) f(c, ?target?_-1,?NO FE?_-2) = 1Table 2.
Feature sets used in ME frame elementidentification.
Example functions of ?briefly?from the sample sentence in Fig.2 are shown.4.3 Semantic Role ClassificationThe semantic role classification is executed onlyfor the constituents that are classified into FEs inthe previous FE identification phase.
MaximumEntropy classification is performed to classify eachFE into classes of semantic roles.Most features from the frame elementidentification in Section 4.2 are still used, and twoadditional features are applied.
The feature setsare in Table 3.?
Order (order): The relative position of aframe element in a sentence is given.
Forexample, in the sentence from Figure 2, thereare three frame elements, and the element?He?
has order 0, while ?over her hand?
hasorder 2.?
Syntactic pattern (pat): The sentence levelsyntactic pattern is generated from the parsetree by looking at the phrase type and logicalfunctions of each frame element in a sentence.For example, in the sentence from Figure 2,?He?
is an external argument Noun Phrase,?bent?
is a target predicate, and ?over herhand?
is an external argument PrepositionalPhrase.
Thus, the syntactic pattern associatedwith the sentence is [NP-ext, target, PP-ext].Feature Setsf(c, target) f(r, head)f(r, target, pt) f(r, head, target)f(r, target, pt, lf) f(r, head, target, pt)f(r, pt, pos, voice) f(r, order, syn)f(r, pt, pos, voice, target) f(r,target, order, syn)f(r, r_-1) f(r,r_-1,r_-2)Table 3.
Feature sets used in ME semantic roleclassification.4.4 Experiments and ResultsSince FrameNet II was published during ourresearch, we continued using FrameNet I (120semantic role categories).
We can, therefore,compare our results with previous research bymatching exactly the same data as used in G & Jand FKH.
We thank Dan Gildea for providing thefollowing data set: training (36,993 sentences /75,548 frame elements), development (4,000sentences / 8,167 frame elements), and held ourtest sets (3,865 sentences / 7,899 frame elements).We train the ME models using the GISalgorithm (Darroch and Ratcliff, 1972) asimplemented in the YASMET ME package (Och,2002).
For testing, we use the YASMETMEtagger (Bender et al 2003) to perform theViterbi search for choosing the most probable tagsequence for a sentence using the probabilitiesfrom training.
Feature weights are smoothed usingGaussian priors with mean 0 (Chen and Rosenfeld,1999).
The standard deviation of this distributionand the number of GIS iterations for training areoptimized on development set for each experiment.Table 4 shows the performance for test set.
Theevaluation is done for individual frame elements.To segment a sentence before FE identificationor role tagging improves the overall performance(from 57.6% to 60.0% in Table 4).
Since thesegmentation reduces the FE coverage of segments,we conduct the experiment with the manuallychosen segmentation to see how much thesegmentation helps the performance.
Here, weextract segments from the parse tree constituents,so the FE coverage is 86% for test set, whichmaches the parsing accuracy.
Table 5 shows theperformance of the frame element identification fortest set:  F-score is 77.2% that is much better than71.7% of our automatic segmentation.FE identification FE identification & Role tagging MethodPrec Rec F Prec Rec FG & Jseparatedmodel73.6 63.1 67.5 67.0 46.8 55.1FKHME model 73.6 67.9 70.6 60.0 55.4 57.6Our model(segmentation+ MEclassification)75.5 68.2 71.7 62.9 56.8 60.0Table 4.
Performance comparison for test set.Precision Recall F-score82.1 72.9 77.2Table 5.
Result of frame element identification onmanual segmentation of test set5 n-best Lists and Re-rankingAs stated, the sentence segmentation improvesthe performance by using sentence-wide features,but it drops the FE coverage of constituents.
Inorder to determine a good segmentation for asentence that does not reduce the FE coverage, weperform another experiment by using re-ranking.We obtain all possible segmentations for a givensentence, and conduct frame element identificationand semantic role classification for allsegmentations.
During both phases, we get n-bestlists with Viterbi search, and finally choose thebest output with re-ranking method.
Figure 3shows the overall framework of this task.5.1 Maximum Entropy Re-rankingWe model the probability of output r givencandidates?
feature sets {x1 .. xt} where t is the totalnumber of candidates and xj is a feature set of thejth candidate according to the following MEformulation:]})..{,(exp[1})..{|(01 1?==nitiixtxxrfZxxrp ?where Zx is a normalization factor, fi(r,{x1..xt}) is afeature function which maps each output and allcandidates?
feature sets to a binary value, n is thetotal number of feature functions, and ?i is theweight for a given feature function.
The weight ?iis associated with only each feature function whilethe weight in the ME classifier is associated withall possible classes as well as feature functions.The final decision is r having the highestprobability of p(r|{x1..xt}) from t number ofcandidates.As a feature set for each candidate, we use theME classification probability that is calculatedduring Viterbi search.
These probabilities areconditional probabilities given feature sets andthese feature sets depend on the previous output,for example, semantic role tagging is done for theidentified FEs in the previous phase.
For thisreason, the product of these conditionalprobabilities is used as a feature set.
)|(*)|(*)|()|( ferpsegfepssegpsrp =where s is a given sentence, seg is a segmentation,fe is a frame element identification, and r is thefinal semantic role tagging.
p(fe|seg) and p(r|fe)are produced from the ME classification butp(seg|s) is computed by a heuristic method and adevelopment set optimization experiment.
Theadopted p(seg|s) is composed of p(each segment?spart of speech tag | target?s part of speech tag),p(the number of total segments in a sentence | totalnumber of words in a sentence), and the average ofeach segment?s p(head word of FE | target).Two additional feature sets other than p(r|s) areapplied to get slight improvement for re-rankingperformance, which are average of p(parse treedepth of FE | target) and average of p(head wordof FE | target).5.2 Experiments and ResultsWe apply ME re-ranking in YASMET-MEpackage.
We train re-ranking model withdevelopment set after obtaining candidate lists forthe set.
For a simple cross validation, thedevelopment set is divided into a sub-training set(3,200 sentences) and a sub-development set (800sentences) by selecting every fifth sentence.Training for re-ranking is executed with the sub-training set and optimization is done with the sub-development set.
The final test is applied to testset.The possible number of segmentations is differentdepending on sentences, but the average number ofsegmentation lists is 15.24 for the development  set.For these segmentations, we compute 10-best5 listsfor the FE identification and 10-best lists for thesemantic role classification.4  To reduce the number of different segmenationswhile not dropping the FE coverage, the segmentationshaving too many segments for a long sentence areexcluded.5 The experiment showed 10-best lists outperformedother n-best lists where n is less than 10.
The biggernumber was not tested because of  huge number of lists.He craned over the balcony again but finally he seemed to sigh.1.
(He) (craned) (over) (the) (balcony) (again) (but) (finally) (he) (seemed) (to) (sigh).?6.
(He) (craned) (over) (the balcony) (again) (but) (finally) (he) (seemed) (to sigh).7.
(He) (craned) (over) (the balcony) (again) (but) (finally) (he) (seemed to sigh).?11.
(He) (craned) (over the balcony) (again) (but) (finally) (he) (seemed to sigh).12.
(He) (craned) (over the balcony) (again) (but) (finally he seemed to sigh).Input sentenceSentence Segmentation: segment a sentence into all possible combinations of constituents of aparse tree while separating target word (In this example, target word is ?craned?.
)Frame Element Identification:  apply ME classification to all segmentations and get n-best outputclassifying each segment into FE (frame element), T (target), or NO (none), then extract segments that areidentified as frame elements(1)(2)(4)1.1 (He)?6.1 (He) (the balcony)?11.1 (He) (over the balcony)?12.1 (He) (over the balcony)12.2 (He)12.3 (He) (over the balcony) (again)..(3)Semantic Role Classification: apply ME classification into 120 semantic roles and get n-best output for each1.1.1 Agent (He)?.6.1.1 Agent (He), BodyPart (the balcony)?.12.1.1 Agent (He), Goal (over the balcony)12.1.2 Agent (He), Path (over the balcony)12.1.3 Self-mover (He), Goal (over the balcony)?.Re-ranking : apply ME re-ranking and choose the best one from long listsFinal Output Agent (He), Path (over the balcony)Fig.
3.
The framework of the re-ranking method with an actual system output.
(1) contains different numberof segmentations depending on each sentence, (2) has mn number of lists when we obtain m possiblesegmentations in (1) and we get n-best FE identifications, (3) has mnn number of lists when we get n-bestrole classifications given mn lists (4) shows finally chosen output.Table 6 shows the performance of re-ranking.To evaluate the performance of top-n, the besttagging output for a sentence is chosen among n-lists and the performance is computed for that list.The top-5 lists show two interesting points: one isthat precision is very high, and the other is that F-score including role tagging is not much differentfrom F-score of only FE identification.
In otherwords, there are a few (not 120) confusing roles fora given frame element, and we have many frameelements that are not identified even in n-best lists.FE identification FE identification  & Role tagging Re-rankPrec Rec F Prec Rec FTop-1 77.4 66.0 71.2 66.7 57.0 61.5Top-2 81.8 69.2 75.0 75.6 64.0 69.4Top-5 86.8 72.4 78.0 83.7 69.9 76.2Table 6.
Re-ranking performance for test setTo improve our re-ranker, more featuresregarding these problems should be added, and amore principled method to obtain the probability ofsegmenations, p(seg) in Sectioin 5.1, needs to beinvestigated.Table 7 compares the final output with G & J?sbest result.
Our model is slightly worse than theirintegrated model, but it supports much furtherexperimentation in segmentation and re-ranking.FE identification FE indetification & Role tagging MethodPrec Rec F Prec Rec FG & Jintegratedmodel74.0 70.1 72.0 64.6 61.2 62.9Ourmodel w/re-ranking77.4 66.0 71.2 66.7 57.0 61.5Table 7.
The final output for test set.6 ConclusionWe describe a pipeline framework to analyzesentences into frame elements and semantic rolesbased on the FrameNet corpus.
The processincludes four steps: sentence segmentation, FEidentification, role classification, and final re-ranking of the n-best outputs.In future work, we will investigate ways toreduce the gap between the five-best outputperformance and the single best output.
Morefeatures should be extracted to improve re-rankingaccuracy.
Although the segmentation improves theperformance, since the final output is dominated bythe initial segmentation, we will explore a smartsegmentation method, possibly one not evenlimited to constituents.In addition to the provided syntactic features, wewill apply semantic features using ontology.Finally, the challenge is to apply this type of workto new predicates, ones not yet treated inFrameNet.
We are searching for methods toachieve this.ReferencesO.
Bender, K. Macherey, F.J. Och, and H. Ney.2003.
Comparison of Alignment Templates andMaximum Entropy Models for Natural LanguageProcessing.
Proc.
of EACL-2003.
Budapest,Hungary.A.
Berger, S. Della Pietra and V. Della Pietra,1996.
A Maximum Entropy Approach to NaturalLanguage Proc.
of Computational Linguistics,vol.
22, no.
1.S.F.
Chen and R. Rosenfeld.
1999.
A GaussianPrior for Smoothing Maximum Entropy Models.Technical Report CMUCS-99-108, CarnegieMellon University.M.
Collins.
1997.
Three Generative, LexicalizedModels for Statistical Parsing.
Proc.
of the 35thAnnual Meeting of the ACL.
pages 16-23,Madrid, Spain.J.
N. Darroch and D. Ratcliff.
1972.
GeneralizedIterative Scaling for Log-Linear Models.
Annalsof Mathematical Statistics, 43:1470-1480.C.Fillmore 1976.
Frame Semantics and the Natureof Language.
Annals of the New York Academyof Science Conference on the Origin andDevelopment of Language and Speech, Volume280 (pp.
20-32).M.
Fleischman, N. Kwon, and E. Hovy.
2003.Maximum Entropy Models for FrameNetClassification.
Proc.
of Empirical Methods inNatural Language Processing conference(EMNLP) 2003.
Sapporo, Japan.D.
Gildea and D. Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
ComputationalLinguistics, 28(3) 245-288 14.K.
Hacioglu, W. Ward.
2003.
Target worddetection and semantic role chunking usingsupport vector machines.
Proc.
of HLT-NAACL2003, Edmonton, Canada.F.J.
Och.
2002.
Yet Another Maxent Toolkit:YASMET www-i6.informatik.rwth-aachen.de/Colleagues/och/.S.
Pradhan, K. Haciolgu, W. Ward, J. Martin, D.Jurafsky.
2003.
Semantic Role Parsing: AddingSemantic Structure to Unstructured Text.
Proc ofof the International Conference on Data Mining(ICDM-2003), Melbourne, FLC.
Thompson, R. Levy, and C. Manning.
2003.
AGenerative Model for FrameNet Semantic RoleLabeling.
Proc.
of the Fourteenth EuropeanConference on Machine Learning, Croatia
