LICENSING AND TREE ADJOINING GRAMMAR INGOVERNMENT BINDING PARSINGRobert  Frank*Department  of  Computer  and Informat ion SciencesUnivers i ty o f  PennsylvaniaPhi ladelphia, PA 19104email:  f rank@ l inc.cis.upenn.eduAbstractThis paper presents an implemented, psychologically plau-sible parsing model for Government Binding theory gram-mars.
I make use of two main ideas: (1) a generaliza-tion of the licensing relations of \[Abney, 1986\] allows forthe direct encoding of certain principles of grammar (e.g.Theta Criterion, Case Filter) which drive structure build-ing; (2) the working space of the parser is constrainedto the domain determined by a Tree Adjoining Grammarelementary tree.
All dependencies and constraints are lo-caiized within this bounded structure.
The resultant parseroperates in linear time and allows for incremental semanticinterpretation a d determination f grammaticaiity.1 IntroductionThis paper aims to provide a psychologically plausiblemechanism for putting the knowledge which a speakerhas of the syntax of a language, the competence gram-mar, to use.
The representation f knowledge of languageI assume is that specified by Government Binding (GB)Theory introduced in \[Chomsky, 1981\].
GB, as a com-petence theory, emphatically does not specify the natureof the language processing mechanism.
In fact, "proofs"that transformational grammar is inadequate as a linguis-tic theory due to various performance measures are funda-mentally flawed since they suppose aparticular connectionbetween the grammar and parser \[Berwick and Weinberg,1984\].
Nonetheless, it seems desirable to maintain a fairlydirect connection between the linguistic competence and*I would like to thank the following for their valuable discussion a dsuggestions: Naoki Fukui, Jarnie Henderson, Aravind Joshi, Tony Kroch,Mitch Marcus, Michael Niv, Yves Schabes, Mark Steedman, Enric Vall-duv{.
This work was pa~ially supported by ARO Grants DAAL03-89-C0031 PRI and DAAG29-84-K-0061 and DARPA grant N00014-85-K-0018.
The author issupported by a Unisys doctoral fellowship.its processing.
Otherwise, claims of the psychological re-ality of this particular conception of competence becomeessentially vacuous ince they cannot be falsified but forthe data on which they are founded, i.e.
grammaticalityjudgments.
Thus, in building a model of language pro-cessing, I would like to posit as direct a link as is possiblebetween linguistic competence and the operations of theparser while still maintaining certain desirable computa-tional properties.What are the computational properties necessary forpsychological plausibility?
Since human syntactic pro-cessing is an effortless process, we should expect hat ittake place efficiently, perhaps in linear time since sen-tences do not become more difficult to process simplyas a function of their length.
Determinism, as proposedby Marcus \[1980\], seems desirable as well.
In addition,the mechanism should operate in an incremental fashion.Incrementality is evidenced in the human language pro-cessor in two ways.
As we hear a sentence, we buildup semantic representations without waiting until the sen-tence is complete.
Thus, the semantic processor shouldhave access to syntactic representations prior to an utter-ance's completion.
Additionally, we are able to perceiveungrammaticality n sentences almost immediately afterthe ill fonnedness occurs.
Thus, our processing mecha-nism should mimic this early detection of ungrammaticalinput.Unfortunately, a parser with the most transparent rela-tionship to the grammar, a "parsing as theorem proving"approach as proposed by \[Johnson, 1988\] and \[Stabler,1990\], does not fare well with respect o our computa-tional desiderata.
It suffers from the legacy of the com-putational properties of first order theorem proving, mostnotably undecidability, and is thus inadequate for our pur-poses.
The question, then, is how much we must repeatfrom this direct instantiatiou so that we can maintain therequisite properties.
In this paper, I attempt o provideiiian answer.
I propose a parsing model which representsthe principles of the grammar in a fairly direct manner,yet preserves efficiency and incrementality.
The modeldepends upon two key ideas.
First, I utilize the insightof \[Abney, 1986\] in the use of licensing relations as thefoundation for GB parsing.
By generalizing Abney's for-mulation of licensing, I can directly encode and enforcea particular class of the principles of GB theory and inso doing efficiently build phrase structure.
The principlesexpressible through licensing are not all of those positedby GB.
Thus, the others must be enforced using a differentmechanism.
Unfortunately, the unbounded size of the treecreated with licensing makes any such mechanism compu-tationally abhorrent.
In order to remedy this, I make useof the Tree Adjoining Grammar (TAG) framework \[Joshi,1985\] to limit the working space of the parser.
As theparser proceeds, its working slructure is bounded in size.If this bound is exceeded, we reduce this structure by oneof the operations provided by the TAG formalism, eithersubstitution or adjunction.
This results in two structures,each of which form independent elementary trees.
Inter-estingly, the domain of locality imposed by a TAG ele-mentary tree appears to be sufficient for the expression ofthe remaining rammatical principles.
Thus, we can checkfor the satisfaction of the remaining rammatical princi-ples in just the excised piece of structure and then send itoff for semantic interpretation.
Since this domain of con-straint checking is bounded in size, this process is doneefficiently.
This mechanism also works in an incrementalfashion.2 Abney's LicensingSince many grammatical constraints are concerned withthe licensing of elements, Abney \[1986\] proposes utiliz-ing licensing structure as a more concrete representationfor parsing.
This allows for more efficient processing yetmaintains "the spirit of the abstract grammar.
"Abney's notion of licensing requires that every elementin a structure be licensed by performing some syntac-tic function.
Any structure with unlicensed elements isill-formed.
Abney takes them role assignment to be thecanonical case of licensing and assumes that he propertiesof a general licensing relation should mirror those of thetaassignment, namely, that it be unique, local and lexical.The uniqueness proporty for them assignment requires thatan argument receives one and only one them role.
Corre-spondingly, licensing is unique: an element is licensed viaexactly one licensing relation.
Locality demands that thetaassignment, and correspondingly licensing, take place un-der a strict definition of government: sisterhood.
Finally,112IPNP will v p S M ~M ry tomorrow~T.
.
.~Figure 1: Abney's Licensing Relations in Clausal Struc-ture (S = subjecthood, F = functional selection, M = mod-ification, T = theta)theta ssignment is lexical in that it is the properties of thethe theta assigner which determine what theta assignmentrelations obtain.
Licensing will have the same property; itis the licenser that determines how many and what sort ofelements it licenses.Each licensing relation is a 3-tuple (D, Cat, Type).
D isthe direction in which licensing occurs.
Cat is the syntac-tic category of the element licensed by this relation.
Typespecifies the linguistic function accomplished by this li-censing relation.
This can be either functional selection,subjecthood, modification or theta-assignment.
Functionalselection is the relation which obtains between a func-tional head and the element for which it subcategorizes,i.e.
between C and IP, I and VP, D and NP.
Subjecthoodis the relation between a head and its "subject".
Moditica-tion holds between a head and adjunct.
Theta assignmentoccurs between a head and its subeategnrized lements.Figure 1 gives an example of the licensing relations whichmight obtain in a simple clause.
Parsing with these li-censing relations imply consists of determining, for eachlexieal item as it is read in a single left to right pass,where it is licensed in the previously constructed structureor whether it licenses the previous tructure.We can now re-examine Abney's claim that hese licens-ing relations allow him to retain "the spirit of the abstractgrammar."
Since licensing relations talk only of very lo-cal relationships, that occurring between sisters, this sys-tem cannot enforce the constraints of binding, control, andECP among others.
Abney notes this and suggests that hislicensing should be seen as a single module in a parsingsystem.
One would hope, though, that principles whichhave their roots in licensing, such as those of theta andcase theory, could receive natural treatments.
Unfortu-nately, this is not true.
Consider the theta criterion.
Whilethis licensing system is able to encode the portion of theconstraint that requires theta roles to be assigned uniquely,it fails to guarantee that all NPs (arguments) receive athetarole.
This is crucially not the case since NPs are some-times licensed not by them but by subject licensing.
Thus,the following pair will be indistinguishable:i.
It seems that the pigeon is deadii.
* Joe seems that the pigeon is deadBoth It and Joe will be appropriately licensed by a subjectlicensing relation associated with seems.
The case filteralso cannot be expressed since objects of ECM verbs are"licensed" by the lower clause as subject, yet alo requirecase.
Thus, the following distinction cannot accounted for:i. Carol asked Ben to swat the flyii.
* Carol tried Ben to swat the flyHere, in order to get the desired syntactic structure (withBen in the lower clause in both cases), Ben will need tobe licensed by the inflectional element o.
Since such alicensing relation eed be unique, the case assigning prop-erties of the matrix verbs will be irrelevant.
What seemsto have happened is that we have lost the modularity of thethe syntactic relations constrained by grammatical princi-ples.
Everything has been conltated onto one homoge-neous licensing structure.3 Generalized LicensingIn order to remedy these deficiencies, I propose a systemof Generalized Licensing.
In this system, every node isassigned two sets of licensing relations: gives and needs.Gives are similar to the Abney's licensing relations: theyare satisfied locally and determined lexically.
Needs spec-ify the ways in which a node must be licensed.1 A needof type them, for example, requires a node to be licensedby a theta relation.
In the current formulation, eeds differfrom gives in that they are always directionaUy unspeci-fied.
We can now represent the theta criterion by placingtheta gives on a theta assigner for each argument and thetaneeds on all DPs.
This encodes both that all them rolesmust be assigned and that all arguments must receive thetaroles.In Generalized Licensing, we allow a greater vocabu-lary of relation types: case, them assignment, modification,functional selection, predication, f-features, etc.
We canthen explicitly represent many of the relations which areposited in the grammar and preserve the modularity of thetheory.
As a result, however, certain elements can andmust be multiply licensed.
DPs, for instance, will haveneeds for both them and case as a result of the case filterand theta criterion.
We therefore relax the requirement that1These bear some similarity to the anti-relations of Abney, but areused in a rather different fashion.113all nodes be uniquely licensed.
Rather, we demand thatall gives and needs be uniquely "satisfied."
The unique-ness requirement in Abney's relations is now pushed ownto the level of individual gives and needs.
Once a giveor need is satisfied, it may not participate in any otherlicensing relationships.One further generalization which I make concerns thepositioning of gives and needs.
In Abney's system, licens-ing relations are associated with lexical heads and appliedto maximal projections of other heads.
Phrase structure isthus entirely parasitic upon the reconstruction f licensingstructure.
I propose to have an independent process oflexical projection.
A lexical item projects to the correctnumber of levels in its maximal projection, as determinedby theta structure, f-features, and other lexical properties.
2Gives and needs are assigned to each of these nodes.
Aswith Abney's system, licensing takes place under a strictnotion of government (sisterhood).
However, the projec-tion process allows licensing relations determined by ahead to take place over a somewhat larger domain thansisterhood to the head.
A DP's theta need resulting fromthe them criterion, for example, is present only at the max-imal projection level.
This is the node which stands in theappropriate structural relation to a theta give.
As a re-sult of this projection process, though, we must explicitlyrepresent s ructural relations during parsing.The reader may have noticed that multiple needs on anode might not all be satisfiable in one structural position.Consider the case of a DP subject which possesses boththeta and case needs.
The S-structure subject of the sen-tence receives its theta role from the verb, yet it receives itscase from the tense/agreement morpheme heading IP.
Thisis impossible, though, since given the structural correlateof the licensing relation, the DP would then be directlydominated both by IP and by VP.
Yet, it cannot be in ei-ther one of these positions alone, since we will then haveunsatisfied needs and hence an ill-formed structure.
Thus,our representation f grammatical principles and the con-straints on give and need satisfaction force us into adopt-ing a general notion of chain and more specifically the VPinternal subject hypothesis.
A chain consist of a list ofnodes (al .
.
.
.
,a~) such that they share gives and needs andeach ai c-commands each a~+l.
The first element in thechain, al, the head, is the only element which can havephonological content.
Others must be empty categories.Now, since the elements of the chain can occupy differ-ent structural positions, they may be governed and hencelicensed by distinct elements.
In the simple sentence:\[IP Johns tns/agr \[V' ti smile\]\]21 assume the relativized X-bar theory proposed in \[Fukui and Speas,1986\].the trace node which is an argument of smile forms a chainwith the DP John.
In its V' internal position, the thetaneed is satisfied by the theta give associated with the V.In subject position, the case need is satisfied by the casegive on the I' projection of the inflectional morphology.Now, how might we parse using these licensing rela-tions?
Abney's method is not sufficient since a singleinstance of licensing no longer guarantees that all of anode's licensing constraints are satisfied.
I propose a sim-ple mechanism, which generalizes Abney's approach: Weproceed left to right, project he current input token to itsmaximal projection p and add the associated gives andneeds to each of the nodes.
These are determined by ex-amination of information in the lexical entries (such asusing the theta grid to determine theta gives), examinationof language specific parameters (using head directionalityin order to determined irectionality of gives, for exam-pie), and consultation of UG parameters (for instance as aresult of the case filter, every DP maximal projection willhave an associated case need).
The parser then attempts tocombine this projection with previously built structure inone of two ways.
We may attach p as the sister of a noden on the right frontier of the developing structure, whenp is licensed by n either by a give in n and/or a need inthe node p. Another possibility is that the previously builtstructure is attached as sister to a node, rn, dominated bythe maximal projection p, by satisfying a give in rn and/ora need on the root of the previously built structure.
In thecase of multiple attachment possibilities, we order themaccording to some metric such as the one proposed byAbney, and choose the most highly ranked option.As structure is built, nodes in the tree with unsatisfiedgives and needs may become closed off from the rightfrontier of the working structure.
In such positions, theywill never become satisfied.
In the ease of a need in aninternal node n which is unsatisfied, we posit the existenceof an empty category rn, which will be attached later tothe structure such that (n, ra) form a chain.
We posit anelement to have been moved into a position exactly when itis licensed at that position yet its needs are not completelysatisfied.
After positing the empty category, we push itonto the trace stack.
When a node has an unsatisfied giveand no longer has access to the right frontier, we must positsome element, not phonologically represented in the input,which satisfies that give relation.
If there is an element onthe top of the trace stack which can satisfy this give, wepop it off the stack and attach it.
3 Of course, if the tracehas any remaining needs, it is returned to the Pace stacksince its new position is isolated from the right frontier.If no such element appears on top of the mace stack, we3Note that the use of this stack to recover filler-gap structures forbidsnon-nested dependencies a  in \[Fodor, 1978\].IP/ ~  8tree: <left, case, nomlaattve, 1>i needs: <th~, ?, ?> needs:<caae, non~ative, ~>Harry !
styes: <rlsht, ~anctioc.-select, VP, ?>Figure 2: Working Space after "Harry tns/agr"posit a non-mace mpty category of the appropriate type,if one exists in the language.
4Let's try this mechanism on the sentence "Harrylaughs."
The first token received is Harry and is projectedto DP.
No gives are associated with this node, but themand case needs are inserted into the need set as a resultof the them criterion and the case filter.
Next, tns/agr isread and projected to I", since it possesses f-features (cf.\[Fuktti and Speas, 1986\]).
Associated with the I ?
nodeis a rightward functional selection give of value V. Onthe I' node is a leftward nominative case give, from thef-features, and a leftward subject give, as a result of theExtended Projection Principle.
The previously constructedDP is attached as sister to the I' node, thereby satisfyingthe subject and case gives of the I' as well as the case needof the DP.
We are thus left with the structure in figure 2.
5Next, we see that the them need of the DP is inaccessiblefrom the right frontier, so we push an empty category DPwhose need set contains this unsatisfied theta need ontothe mace stack.
The next input token is the verb laugh.This is projected to a single bar level.
Since laugh assignsan external theta role, we insert a leftward theta give toa DP into the V' node.
This verbal projection is attachedas sister to I ?, satisfying the functional selection give ofI.
However, the theta give in V' remains unsatisfied andsince it is leftward, is inaccessible.
We therefore need toposit an empty category.
Since the DP trace on top of thetrace stack will accept his give, the trace stack is poppedand the trace is attached via Chomsky-adjunction to the4Such a simplistic approach to determining whether a trace or non-trace empty category should be inserted is dearly not correct.
For in-stance, in "tough movement"Alvin i is tough PRO to feed tithe proposed mechanism will insert the trace of Alvin in subject posi-tion rather than PRO.
It remains for future work to determine the exactmechanism by which such decisions are made.5In the examples which follow, gives are shown as 4-topics(D,T~tpe,Val, SatBI/) where D is the direction, T~tpe is the typeof licensing relation, Val is the licensing relation value and SatB~ isthe node which satisfies the give (marked as 7, if the relation is as yetunsatisfied).
Needs are 3-tuples (Type, Val, SatB~/) where these areas in the gives.
For purposes of readability, I remove previously satisfiedgives and needs from the fgure.
Of course, such information persists inthe parser's representation.114IP81ve~ oHarryneed= ~*~.eds: ,eh~,a, aS~,  *,>VIlaushFigure 4: Adjunction of auxiliary tree/~ into elementarytree ~ to produce 7Figure 3: Working space after "Harry tns/agr laugh"V' node yielding the structure in figure 3.
Since this nodeforms a chain with the subject DP, the theta need on thesubject DP is now satisfied.
We have now reached the endof our input.
The resulting structure is easily seen to bewell-formed since all gives and needs are satisfied.We have adopted a very particular view of traces: theirpositions in the structure must be independently motivatedby some other licensing relation.
Note, then, that we can-not analyze long distance dependencies through successivecyclic movement.
There is no licensing relation which willcause the intermediate traces to exist.
Ordinarily thesetraces exist only to allow a well-formed erivation, i.e.not ruled out by subjacency or by a barrier to antecedentgovernment.
Thus, we need to account for constraints onlong distance movement in another manner.
We will returnto this in the next section.The mechanism I have proposed allows a fairly directencoding for some of the principles of grammar such ascase theory, them theory, and the extended projection prin-ciple.
However, many other constraints of GB, such as theECP, control theory, binding theory, and bounding the-try, cannot be expressed perspicuously through licensing.Since we want our parser to maintain a fairly direct con-nection with the grammar, we need some additional mech-anism to ensure the satisfaction of these constraints.Recall, again, the computational properties we wanted tohold of our parsing model: efficiency and incrementality.The structure building process I have described has worstcase complexity O(n 2) since the set of possible attach-ments can grow linearly with the input.
While not enor-mously computationally intensive, this is greater that thelinear time bound we desire.
Also, checking for satisfac-tion of non-licensing constraints over unboundedly largestructures is likely to be quite inefficient.
There is alsothe question of when these other constraints are checked.To accord with incrementality, they must be checked assoon as possible, and not function as post-processing "fil-ters."
Unfortunately, it is not easily determinable when agiven constraint can apply such that further input will notchange the status of the satisfaction of a constraint.
Wedo not want to rule a structure ungrammatical simply be-cause it is incomplete.
Finally, it is unclear how we mightincorporate this mechanism which builds an ever largersyntactic structure into a model which performs emanticinterpretation i crementally.4 Limiting the Domain with TAGThese problems with our model are solved if we can placea limit on the size of the structures we construct.
Thenumber of licensing possibilities would be bounded yield-ing linear time for smacture construction.
Also, constraintchecking could be done in a constant amount of process-ing.
Unfortunately, the productivity of language requiresus to handle sentences of unbounded length and thus lin-guistic structures of unbounded size.TAG provides us with a way to achieve this paradise.TAG accomplishes linguistic description by factoring re-cursion from local dependencies \[Joshi, 1985\].
It positsa set of primitive structures, the elementary trees, whichmay be combined through the operations of adjunctionand substitution.
An elementary tree is a minimal non-recursive syntactic tree, a predication structure containingpositions for all arguments.
I propose that this is the pro-jection of a lexical head together with any of the associatedfunctional projections of which it is a complement.
Forinstance, a single elementary tree may contain the projec-tion of a V along with the I and C projections in which itis embedded.
6 Along the frontier of these trees may ap-pear terminal symbols (i.e.
lexical items) or non-terminals.The substitution operation is the insertion of one elemen-tary tree at a non-terminal of same type as the root on thefrontier of another elementary tree.
Adjunction allows theinsertion of one elementary tree of a special kind, an aux-iliary tree, at a node internal to another (cf.
figure 4).
In6This definition of TAG elementary t ees is consistent with the Lex-icalized TAG framework \[Schabes t al., 1988\] in that he lexical headmay be seen as the anchor of the elementary t ees.
For further detailsand consequences of this proposal on elementary t ee weU-fomaedness,see \[Frank, 1990\].115auxiliary trees, there is a single distinguished non-terminalon the frontier of the tree, the foot node, which is iden-tical in type to the root node.
Only adjunctions, and notsubstitutions, may occur at this node.TAG has proven useful as a formalism in which one canexpress linguistic generalizations since it seems to providea sufficient domain over which grammatical constraintscan be stated \[Kroch and Joshi, 1985\] \[Kroch and San-torini, 1987\].
Kroch, in two remarkable papers \[1986\] and\[1987\], has shown that even constraints on long distancedependencies, which intuitively demand a more "global"perspective, can be expressed using an entirely local (i.e.within a single elementary lee) formulation of the ECPand allows for the collapsing of the CED with the ECP.This analysis does not utilize intermediate races, but in-stead the link between filler and gap is "stretched" uponthe insertion of intervening structure during adjunctions.Thus, we are relieved of the problem that intermediatetraces are not licensed, since we do not require their exis-tence.Let us suppose a formulation of GB in which all princi-ples not enforced through generalized licensing are statedover the local domain of a TAG elementary tree.
Now,we can use the model described above to create structurescorresponding to single elementary trees.
However, werestrict he working space of the parser to contain only asingle structure of this size.
If we perform an attachmentwhich violates this "memory limitation," we are forced toreduce the structure in our working space.
We will dothis in one of two ways, corresponding to the two mech-anisms which TAG provides for combining structure.
Ei-ther we will undo a substitution or undo an adjunction.However, all chains are required to be localized in indi-vidual elementary tree.
Once an elementary tree is fin-ished, non-licensing constraints are checked and it is sentoff for semantic interpretation.
This is the basis for myproposed parsing model.
For details of the algorithm, see\[Frank, 1990\].
This mechanism operates in linear time anddeterministically, while maintaining coarse grained (i.e.clausal) incrementality for grammaticality determinationand semantic interpretation.Consider this model on the raising sentence "Harryseemed to kiss Sally."
We begin as before with "Harrytns/agr" yielding the structure in figure 2.
Before we re-ceive the next token of input, however, we see that theworking structure is larger than the domain of an elemen-tary tree, since the subject DP constitutes an independentpredication from the one determined by the projection ofI.
We therefore unsubstitute he subject DP and send it offto constraint checking and semantic interpretation.
At thispoint, we push a copy of the subject DP node onto thetrace stack due to its unsatisfied theta need.116IP~'~ ~ t~i I '  .
, .~  <am.. r. r> / "  J~6iw~ <risht, funclima-sdea.
VP, k> Iux./aSr SiVm~ ~ ~ 1P.
r>Figure 5: Working space after "Harry tus/agr seem"IPn~di: <lhela, ?, ?>x I 'sirra: <rl~t, there, u', t>  v ~' n~i tn~d.
:e .~ l  ~sivm: P ~ Jm~,t'nma-m~ vP.
r>Figure 6: Working space after "Harry tns/agr seem to"We continue with the verb seem which projects to V'and attaches as sister to I satisfying the functional selec-tion give yielding the structure in figure 5.
There remainsonly one elementary tree in working space so we neednot perform any domain reduction.
Next, to projects to I'since it lacks f-features to assign to its specifier.
This isattached as object of seem as in figure 6.
At this point,we must again perform adomain reduction operation sincethe upper and lower clauses form two separate lementarytrees.
Since the subject DP remains on the trace stack, itcannot yet be removed.
All dependencies must be resolvedwithina single elementary tree.
Hence, we must unadjointhe structure recursive on I' shown in figure 7 leaving thestructure in figure 8 in the working space.
This structureis sent off for constraint checking and semantic interpreta-tion.
We continue with kiss, projecting and attaching it asfunctionally selected sister of I and popping the DP fromthe trace stack to serve as external argument.
Finally, weI '  /NI V'tns/agr V I 'IFigure 7: Result of unadjunctionIP/ ~ ,  Stves: <le*%, subject, DP, i> &,iv~ e DPii \]I need.
: ~needs: <theta, ?
?> /<l'~ht, funct ton-select, itoVP, ?>Figure 8: Working space after unadjunctionconstrained, we might be able to retain the efficient natureof the current model.
Other strategies for resolving suchindeterminacies u ing statistical reasoning or hard codedrules or templates might also be possible, but these con-structs are not the sort of grammatical knowledge we havebeen considering here and would entail further abstractionfrom the competence grammar.Another problem with the parser has to do with theincompleteness of the algorithm.
Sentences such asIPv,V DPI kissFigure 9: Working Structure after entire sentenceproject and attach the DP Sally as sister of V, receivingboth them role and case in this position.
This DP is unsub-stituted in the same manner as the subject and is sent offfor further processing.
We are left finally with the struc-ture in figure 9, all of whose gives and needs are satisfied,and we are finished.This model also handles control constructions, bare in-finitives, ECM verbs and binding of anaphors, modifica-tion, genitive DPs and others.
Due to space constraints,these are not discussed here, but see \[Frank, 1990\].5 Problems and Future WorkBoris knew that Tom ate lunchwill not be parsed even though there exist well-formedsets of elementary trees which can derive them.
The prob-lem results from the fact that the left to right processingstrategy we have adopted is a bit too strict.
The comple-mentizer that will be attached as object of know, but Tomis not then licensed by any node on the right frontier.
Ul-timately, this DP is licensed by the tns/agr morpheme inthe lower clause whose IP projection is licensed throughfunctional selection by C. Similarly, the parser would havegreat difficulty handling head final languages.
Again, theseproblems might be solved using extra-grammatical de-vices, such as the attention shifting of \[Marcus, 1980\] orsome template matching mechanism, but this would entaila process of "compiling out" of the grammar that we havebeen trying to avoid.Finally, phonologically empty heads and head move-ment cause great difficulties for this mechanism.
Headsplay a crucial role in this "project and attach" scheme.Therefore, we must find a way of determining when andwhere heads occur when they are either dislocated or notpresent in the input string at all, perhaps in a similar man-ner to the mechanism for movement of maximal projec-tions I have proposed above.The parsing model which I have presented here is stillrather preliminary.
There are a number of areas which willrequire further development before this can be consideredcomplete.I have assumed that the process of projection is en-tirely determined from lexieal ookup.
It is clear, though,that lexical ambiguity abounds and that the assignment ofgives and needs to the projections of input tokens is notdeterminate.
An example of such indeterminacy has to dowith the assignment to argument maximal projections oftheta needs as a result of the them criterion.
DPs need notalways function as arguments, as I have been assuming.This problem might be solved by allowing for the state-ment of disjunctive constraints or a limited form of paral-lelism.
If the duration of such parallelism could be tightly6 ConclusionIn this paper, I have sketched a psychologically plausiblemodel for the use of GB grammars.
The currently im-plemented parser is a bit too simple to be truly robust,but the general approach presented here seems promising.Particularly interesting is that the computationally moti-vated use of TAG to constrain processing locality pro-vides us with insight on the nature of the meta-grammarof possible grammatical constraints.
Thus, if grammaticalprinciples are stated over such a bounded omain, we canguarantee the existence of a perspicuous model for theiruse, thereby lending credence to the cognitive reality ofthis competence grammar.117References\[Abney, 1986\] Steven Abney.
Licensing and parsing.
InProceedings of NELS 16, Amherst, MA.\[Berwick and Weinberg, 1984\] Robert Berwick and AmyWeinberg.
The Grammatical Basis of Linguistic Per-formance.
MIT Press, Cambridge, MA.\[Chomsky, 1981\] Noam Chomsky.
Lectures on Govern-ment and Binding.
Foris, Dordrecht.\[Fodor, 1978\] Janet D Fodor.
Parsing strategies and con-straints on transformations.
Linguistic Inquiry, 9.\[Frank, 1990\] Robert Frank.
Computation and LinguisticTheory: A Government Binding Theory Parser UsingTree Adjoning Grammar.
Master's thesis, Universityof Pennsylvania.\[Fukui and Speas, 1986\] NaokiFukui and Margaret Speas.
Specifiers and projec-tion.
In Naold Fukui, T. Rappaport, and E. Sagey,editors, MIT Working Papers in Linguistics 8, MITDepartment of Linguistics.\[Johnson, 1988\] Mark Johnson.
Parsing as deduction: theuse of knowledge of language.
In The MIT ParsingVolume, 1987-88, MIT Center for Cognitive Science.\[Joshi, 1985\] Aravind Joshi.
How much context-sensitivity isrequired to provide reasonable structuraldescriptions: tree adjoining rammars.
In D. Dowty,L.
Kartunnen, and A. Zwicky, editors, Natural Lan-guage Processing: Psycholinguistic, Computationaland Theoretical Perspectives, Cambridge UniversityPress.\[Kroch, 1986\] Anthony Kroch.
Unbounded ependenciesand subjacency in a tree adjoining grammar.
In A.Manaster-Ramer, editor, The Mathematics of Lan-guage, John Benjamins.\[Kroeh, 1987\] Anthony Kroch.
Assymetries in longdistance extraction in a tree adjoining grammar.manuscript, University of Pennsylvania.\[Kroch and Joshi, 1985\] Anthony Kroch and AravindJoshi.
The Linguistic Relevance of Tree AdjoiningGrammar.
Technical Report MS-CS-85-16, Univer-sity of Pennsylvania Department of Computer andInformation Sciences.
To appear in Linguistics andPhilosophy.\[Kroch and Santorini, 1987\] Anthony Kroch and BeatriceSantorini.
The derived constituent structure of the118west germanic verb raising construction.
In R. Frei-din, editor, Proceedings of the Princeton Conferenceon Comparative Grammar, MIT Press, Cambridge,MA.\[Marcus, 1980\] Mitchell Marcus.
A Theory of SyntacticRecognition for Natural Language.
MIT Press, Cam-bridge, MA.\[Schabes et al, 1988\] Yves Schabes, Anne Abeill6, andAravind K. Joshi.
Parsing strategies with 'lexical-ized' grammars: application to tree adjoining gram-mars.
In COLING Proceedings, Budapest.\[Stabler, 1990\] Edward Stabler.
Implementing overn-ment binding theories.
In Levine and Davis, ed-itors, Formal Linguistics: Theory and Implementa-tion.
forthcoming.
