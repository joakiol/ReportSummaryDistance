Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 387?393,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsLAYERED: Metric for Machine Translation EvaluationShubham GautamComputer Science & Engineering,IIT Bombayshubhamg@cse.iitb.ac.inPushpak BhattacharyyaComputer Science & Engineering,IIT Bombaypb@cse.iitb.ac.inAbstractThis paper describes the LAYERED met-ric which is used for the shared WMT?14metrics task.
Various metrics exist for MTevaluation: BLEU (Papineni, 2002), ME-TEOR (Alon Lavie, 2007), TER (Snover,2006) etc., but are found inadequate inquite a few language settings like, for ex-ample, in case of free word order lan-guages.
In this paper, we propose an MTevaluation scheme that is based on theNLP layers: lexical, syntactic and seman-tic.
We contend that higher layer met-rics are after all needed.
Results are pre-sented on the corpora of ACL-WMT, 2013and 2014.
We end with a metric which iscomposed of weighted metrics at individ-ual layers, which correlates very well withhuman judgment.1 IntroductionEvaluation is an integral component of machinetranslation (MT).
Human evaluation is difficultand time consuming so there is a need for a metricwhich can give the better evaluation in correlationto human judgement.
There are several existingmetrics such as: BLEU, METEOR etc.
but theseonly deal with the lexical layer combining bag ofwords and n-gram based approach.We present an analysis of BLEU and the higherlayer metrics on the ACL WMT 2013 corporawith 3 language pairs: French-English, Spanish-English and German-English.
For syntactic layer,we considered three metrics: Hamming score,Kendall?s Tau distance score and the spearmanrank score.
Syntactic layer metrics take care ofreordering within the words of the sentences sothese may play an important role when there isa decision to be made between two MT outputsentences of two different systems when both thesentences have same number of n-gram matcheswrt the reference sentence but there is a differ-ence in the ordering of the sentence.
We will dis-cuss these metrics in detail in the following sec-tions.
The next NLP layer in consideration is thesemantic layer which deals with the meaning ofthe sentences.
For semantic layer, we consideredtwo metrics: Shallow semantic score and Deep se-mantic score.
On semantic layer, we consideredentailment based measures to get the score.Ananthkrishnan et al.
(2007) mentioned someissues in automatic evaluation using BLEU.
Thereare some disadvantages of the existing metricsalso such as: BLEU does not take care of reorder-ing of the words in the sentence.
BLEU-like met-rics can give same score by permuting word or-der.
These metrics can be unreliable at the levelof individual sentences because there can be smallnumber of n-grams involved.
We would see in thispaper that the correlation of BLEU is lower com-pared to the semantic layer metrics.Section 2 presents the study of related work inMT evaluation.
Section 3 presents the importanceof each NLP layer in evaluation of MT output.
Itdiscusses the metrics that each layer contributes tothe achievement of the final result.
In section 4,various experiments are presented with each met-ric on the top 10 ranking systems of WMT 13corpora which are ranked on the basis of the hu-man ranking.
Each metric is discussed with thegraphical representation so that it would becomeclear to analyze the effect of each metric.
In sec-tion 5, spearman correlation of the metrics is cal-culated with human judgement and comparisonsare shown.
In section 6, we discuss the need of ametric which should be a combination of the met-rics presented in the above sections and present aweighted metric which is the amalgamation of themetrics at individual layers.
Section 7 presents theresults of the proposed metric on WMT 14 dataand compares it with other existing metrics.3872 Related WorkMachine translation evaluation has always re-mained as the most popular measure to judge thequality of a system output compared to the refer-ence translation.
Papineni (2002) proposed BLEUas an automatic MT evaluation metric which isbased on the n-gram matching of the referenceand candidate sentences.
This is still consideredas the most reliable metric and used widely inthe MT community for the determination of thetranslation quality.
BLEU averages the precisionfor unigram, bigram and up to 4-gram and ap-plies a length penalty if the generated sentenceis shorter than the best matching (in length) ref-erence translation.
Alternative approaches havebeen designed to address problems with BLEU.Doddington and George (2003) proposed NISTmetric which is derived from the BLEU evalua-tion criterion but differs in one fundamental as-pect: instead of n-gram precision, the informa-tion gain from each n-gram is taken into account.TER (Snover, 2006) tries to improve the hypothe-sis/reference matching process based on the edit-distance and METEOR (Alon Lavie, 2007) con-sidered linguistic evidence, mostly lexical similar-ity, for more intelligent matching.
Liu and Gildea(2005), Owczarzak et al.
(2007), and Zhang et al.
(2004) use syntactic overlap to calculate the sim-ilarity between the hypothesis and the reference.Pad?o and Galley (2009) proposed a metric thatevaluates MT output based on a rich set of textualentailment features.
There are different works thathave been done at various NLP layers.
Gim?eneztl al.
(2010) provided various linguistic measuresfor MT evaluation at different NLP layers.
DingLiu and Daniel Gildea (2005) focussed the studyon the syntactic features that can be helpful whileevaluation.3 Significance of NLP Layers in MTEvaluationIn this section, we discuss the different NLP layersand how these are important for evalution of MToutput.
We discuss here the significance of threeNLP layers: Lexical, Syntactic and Semantic lay-ers.3.1 Lexical LayerLexical layer emphasizes on the comparison of thewords in its original form irrespective of any lexi-cal corpora or any other resource.
There are somemetrics in MT evaluation which considers onlythese features.
Most popular of them is BLEU,this is based on the n-gram approach and consid-ers the matching upto 4-grams in the reference andthe candidate translation.
BLEU is designed toapproximate human judgement at a corpus level,and performs badly if used to evaluate the qualityof individual sentences.
Another important metricat this layer is TER (Translation Edit Rate) whichmeasures the number of edits required to changea system output into one of the references.
Forour experiments, we would consider BLEU as thebaseline metric on lexical layer.3.2 Syntactic LayerSyntactic layer takes care of the syntax of thesentence.
It mainly focusses on the reorderingof the words within a sentence.
Birch and Os-borne (2011) has mentioned some metrics on thislayer: Hamming score and Kendall?s Tau Dis-tance (KTD) score.
We additionally calculated thespearman rank score on this layer.
Scores are cal-culated first by giving ranking of words in the ref-erence sentence and then putting the rank numberof the word in the candidate sentence.
Now, wehave the relative ranking of the words of both thesentences, so final score is calculated.3.3 Semantic LayerSemantic layer goes into the meaning of the sen-tence, so we need to compare the dependency treeof the sentences.
At this layer, we used entailmentbased metrics for the comparison of dependencies.Pad?o and Galley (2009) illustrated the use of textentailment based features for MT evaluation.
Weintroduced two metrics at this layer: first is Shal-low semantic score, which is based on the depen-dencies generated by a shallow parser and thenthe dependency comparison is carried out.
Sec-ond is Deep semantic score, which goes more deepinto the semantic of the sentence.
For shallow se-mantic score, we used stanford dependency parser(Marie-Catherine et al., 2006) while for deep se-mantic score, we used UNL (Universal Network-ing Language)1dependency generator.Semantic layer may play an important rolewhen there are different words in two sentencesbut they are synonym of each other or are relatedto each other in some manner.
In this case, lexicaland syntactic layers can?t identify the similarity of1http://www.undl.org/unlsys/unl/unl2005/UW.htm388the sentences because there exist a need of somesemantic background knowledge which occurs atthe semantic layer.
Another important role of se-mantic layer is that there can be cases when thereis reordering of the phrases in the sentences, e.g.,active-passive voice sentences.
In these cases, de-pendencies between the words remain intact andthis can be captured through dependency tree gen-erated by the parser.4 ExperimentsWe conducted the experiments on WMT 13 cor-pora for French-English, Spanish-English andGerman-English language pairs.
We calculatedthe score of each metric for the top 10 rankingsystem (wmt, 2013) (as per human judgement) foreach language pair.Note:1.
In the graphs, metric score is multiplied by 100so that a better view can be captured.2.
In each graph, the scores of French-English (fr-en), Spanish-English (es-en) and German-English(de-en) language pairs are represented by red,black and blue lines respectively.4.1 BLEU ScoreFigure 1: BLEU ScoreWe can see from the graph of fig.
1 that for de-en and es-en language pair, BLEU is not able tocapture the phenomenon appropriately.
In fact, itis worse in de-en pair.
Because the graph shouldbe of decreasing manner i.e., as the rank of the sys-tem increases (system gets lower rank compared tothe previous one), the score should also decrease.4.2 Syntactic LayerBecause the BLEU score was not able to capturethe idealistic curve in the last section so we consid-ered the syntactic layer metrics.
This layer is con-sidered because it takes care of the reordering ofthe words within the sentence pair.
The idea hereis that if one candidate translation has lower re-ordering of words w.r.t.
reference translation thenit has higher chances of matching to the referencesentence.4.2.1 Hamming ScoreThe hamming distance measures the number ofdisagreements between two permutations.
Firstwe calculate the hamming distance and then cal-culate the fraction of words placed in the same po-sition in both sentences, finally we calculate thehamming score by subtracting the fraction from 1.It is formulated as follows:dh(pi, ?)
= 1?
?ni=1xin, xi={0; if pi(i) = ?
(i)1; otherwisewhere, n is the length of the permutation.Hamming scores for all three language pairsmentioned above are shown in fig.
2.
As we cansee from the graph that initially its not good for thetop ranking systems but it follows the ideal curvefor the discrimination of lower ranking systems forthe language pairs fr-en and es-en.Figure 2: Hamming Score4.2.2 Kendall?s Tau Distance (KTD)Kendall?s tau distance is the minimum number oftranspositions of two adjacent symbols necessaryto transform one permutation into another.
It rep-resents the percentage of pairs of elements which389share the same order between two permutations.
Itis defined as follows:dk(pi, ?)
= 1??
?ni=1?nj=1zijZwhere, zij={0; if pi(i) < pi(j) and ?
(i) < ?
(j)1; otherwiseThis can be used for measuring word order dif-ferences as the relative ordering of words has beentaken into account.
KTD scores are shown in fig.3.
It also follows the same phenomenon as thehamming score for fr-en and es-en pair but for de-en pair, it gives the worst results.Figure 3: KTD Score4.2.3 Spearman ScoreSpearman rank correlation coefficient is basicallyused for assessing how well the relationship be-tween two variables can be described using amonotonic function.
Because we are using syntac-tic layer metrics to keep track of the reordering be-tween two sentences, so this can be used by rank-ing the words of the first sentence (ranging from1 to n, where n is the length of the sentence) andthen checking where the particular word (with in-dex i) is present in the second sentence in terms ofranking.
Finally, we calculated the spearman scoreas follows:?
= 1?6?d2in(n2?
1)where, di= xi?
yiis the difference betweenthe ranks of words of two sentences.Spearman score lies between -1 to +1 so weconvert it to the range of 0 to +1 so that all themetrics would lie in the same range.4.3 Semantic LayerWe can see from the last two sections that therewere some loopholes on the metrics of both thelayers as can be seen in the graphical representa-tions.
So, there arises a need to go higher in thehierarchy.
The next one in the queue is semanticlayer which takes care of the meaning of the sen-tences.
At this layer, we considered two metrics.Both metrics are based on the concept of text en-tailment.
First we should understand, what is it?Text EntailmentAccording to wikipedia2, ?Textual entailment(TE) in natural language processing is a direc-tional relation between text fragments.
The rela-tion holds whenever the truth of one text fragmentfollows from another text.
In the TE framework,the entailing and entailed texts are termed text (t)and hypothesis (h), respectively.
?First, the dependencies for both reference (R)as well as candidate (C) translation are generatedusing the parser that is used (will vary in boththe following metrics).
Then, the entailment phe-nomenon is applied from R to C i.e., dependenciesof C are searched in the dependency graph of R.Matching number of dependencies are calculated,then a score is obtained as follows:ScoreR?C=No.
of matched dependencies of C in RTotal no.
of dependencies of C(1)Similarly, another score is also obtained by ap-plying the entailment phenomenon in the reverseddirection i.e.
from C to R as follows:ScoreC?R=No.
of matched dependencies of R in CTotal no.
of dependencies of R(2)Final score is obtained by taking the average ofthe above two scores as follows:Scorefinal=ScoreR?C+ ScoreC?R2(3)Now, we discuss how can we use this conceptin the metrics at semantic layer:4.3.1 Shallow Semantic ScoreThis metric uses the stanford dependency parser(Marie-Catherine et al., 2006) to generate the de-pendencies.
After getting the dependencies forboth reference (R) as well as candidate (C) trans-lation, entailment phenomenon is applied and thefinal score is obtained using eq.
(3).2http://wikipedia.org/390Figure 4: Shallow Semantic ScoreWe can see from fig.
4 that for French-Englishand Spanish-English pairs, the graph is very goodcompared to the other metrics at the lower layers.In fact, there is only one score in es-en pair thata lower ranking system gets better score than thehigher ranking system.4.3.2 Deep Semantic ScoreThis metric uses the UNL dependency graph gen-erator for taking care of the semantic of the sen-tence that shallow dependency generator is notable to capture.
Similar to the shallow seman-tic score, after getting the dependencies from theUNL, entailment score is calculated in both direc-tions i.e.
R?
C and C?
R.Figure 5: Deep Semantic ScoreFig.
5 shows that deep semantic score curvealso follows the same path as shallow semanticscore.
In fact, for Spanish-English pair, the pathis ideal i.e., the score is decreasing as the systemrank is increasing.5 Correlation with Human JudgementWe calculated spearman rank correlation coeffi-cient for the different scores calculated in the lastsection.
This score ranges from -1 to +1.
Form ta-Language Pair ?BLEU?Shallow?DeepFrench-English 0.95 0.96 0.92Spanish-English 0.89 0.98 1.00German-English 0.36 0.88 0.89Table 1: Correlation with BLEU Score, ShallowSemantic Score and Deep Semantic Scoreble 1, we can see that the correlation score is bet-ter with semantic layer metrics compared to theBLEU score (lower layer metrics).
In compar-ison to the WMT 13 results (wmt-result, 2013),?Shallowscore for French-English pair is interme-diate between the highest and lowest correlationsystem.
?Deepscore for Spanish-English is high-est among all the systems presented at WMT 13.So, it arises a need to take into account the seman-tic of the sentence while evaluating the MT output.6 Hybrid ApproachWe reached to a situation where we can?t ig-nore the score of any layer?s metric because eachmetric helps to capture some of the phenomenonwhich other may not capture.
So, we used a hy-brid approach where the final score of our pro-posed metric depends on the layered metrics.
Asalready said, we performed our experiments onACL-WMT 2013 corpora, but it provided only therank of the systems.
Due to availability of rankingof the systems, we used SVM-rank to learn the pa-rameters.Our final metric looks as follows:Final-Score = a*BLEU + b*Hamming + c*KTD+ d*Spearman + e*Shallow-Semantic-Score +f*Deep-Semantic-Scorewhere, a,b,c,d,e,f are parameters6.1 SVM-rankSVM-rank learns the parameters from the trainingdata and builds a model which contains the learnedparameters.
These parameters (model) can be usedfor ranking of a new set of data.ParametersWe made the training data of the French-English,Spanish-English and German-English language391MetricPearson Correlationfr-en de-en hi-en cs-en ru-en AverageLAYERED .973 .893 .976 .940 .843 .925BLEU .952 .831 .956 .908 .774 .884METEOR .975 .926 .457 .980 .792 .826NIST .955 .810 .783 .983 .785 .863TER .952 .774 .618 .977 .796 .823Table 2: Correlation with different metrics in WMT 14 Resultspairs.
Then we ran SVM-rank and obtained thescores for the parameters.So, our final proposed metric looks like:Final-Score = 0.26*BLEU + 0.13*Hamming +0.03*KTD + 0.04*Spearman + 0.28*Shallow-Semantic-Score + 0.26* Deep-Semantic-Score7 Performance in WMT 2014Table 2 shows the performance of our metric onWMT 2014 data (wmt-result, 2014).
It performedvery well in almost all language pairs and itgave the highest correlation with human in Hindi-English language pair.
On an average, our corre-lation was 0.925 with human considering all thelanguage pairs.
This way, we stood out on sec-ond position considering the average score whilethe first ranking system obtained the correlationof 0.942.
Its clear from table 2 that the proposedmetric gives the correlation better than the stan-dard metrics in most of the cases.
If we look atthe average score of the metrics in table 2 then wecan see that LAYERED obtains much higher scorethan the other metrics.8 ConclusionMachine Translation Evaluation is an excitingfield that is attracting the researchers from the pastfew years and the work in this field is enormous.We started with the need of using higher layermetrics while evaluating the MT output.
We un-derstand that it might be a little time consumingbut its efficient and correlation with human judge-ment is better with semantic layer metric com-pared to the lexical layer metric.
Because, eachlayer captures some linguistic phenomenon so wecan?t completely ignore the metrics at individuallayers.
It gives rise to a hybrid approach whichgives the weightage for each metric for the calcu-lation of final score.
We can see from the resultsof WMT 2014 that the correlation with LAYEREDmetric is better than the standard existing metricsin most of the language pairs.ReferencesAlexandra Birch, School of Informatics, University ofEdinburgh Reordering Metrics for Statistical Ma-chine Translation.
Phd Thesis, 2011.Alexandra Birch and Miles Osborne Reordering Met-rics for MT.
Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies - Volume 1, se-ries = HLT 2011.Alon Lavie and Abhaya Agarwal.
METEOR: An Auto-matic Metric for MT Evaluation with High Levels ofCorrelation with Human Judgments, Proceedings ofthe Second Workshop on Statistical Machine Trans-lation, StatMT 2007.Ananthakrishnan R and Pushpak Bhattacharyya and MSasikumar and Ritesh M Shah Some Issues in Auto-matic Evaluation of English-Hindi MT: More Bluesfor BLEU.
ICON, 2007.Doddington and George Automatic evaluation ofmachine translation quality using N-gram co-occurrence statistics, NIST.
Proceedings of the2nd International Conference on Human LanguageTechnology Research HLT 2002.Ding Liu and Daniel Gildea Syntactic Features forEvaluation of Machine Translation.
Workshop OnIntrinsic And Extrinsic Evaluation Measures ForMachine Translation And/or Summarization, 2005.Findings of the 2013 Workshop on Statistical MachineTranslation.
ACL-WMT 2013.Gim?enez, Jes?us and M`arquez, Llu?
?s Linguistic Mea-sures for Automatic Machine Translation Evalua-tion.
Machine Translation, December, 2010.Liu D, Gildea D Syntactic features for evaluation ofmachine translation.
ACL 2005 workshop on intrin-sic and extrinsic evaluation measures for machinetranslation and/or summarization.Owczarzak K, Genabith J, Way A Evaluating ma-chine translation with LFG dependencies.
MachineTranslation 21(2):95119.392Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
Generating Typed Depen-dency Parses from Phrase Structure Parses.
LREC2006.Matthew Snover and Bonnie Dorr and RichardSchwartz and Linnea Micciulla and John Makhoul.A Study of Translation Edit Rate with Targeted Hu-man Annotation, In Proceedings of Association forMachine Translation in the Americas, 2006.Papineni, Kishore and Roukos, Salim and Ward, Toddand Zhu, Wei-Jing.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, ACL 2002.Results of the WMT13 Metrics Shared Task.
ACL-WMT 2013.Results of the WMT14 Metrics Shared Task.
ACL-WMT 2014.Sebastian Pad?o and Michel Galley and Dan Jurafskyand Chris Manning Robust Machine TranslationEvaluation with Entailment Features.
Proceedingsof ACL-IJCNLP 2009, ACL 2009.Zhang Y, Vogel S, Waibel A Interpreting Bleu/NISTscores: how much improvement do we need to havea better system?.
In: Proceedings of the 4th interna-tional conference on language resources and evalua-tion.
Lisbon, Portugal.393
