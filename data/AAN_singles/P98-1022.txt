A Probabilistic Corpus-Driven Model for Lexical-Functional AnalysisRens BodDepartment ofComputational LinguisticsUniversity of AmsterdamSpuistraat 134, NL- 1012 VB Amsterdamrens.bod@let.uva.nlRonald KaplanXerox Palo Alto Research Center3333 Coyote Hill RoadPalo Alto, California 94304kaplan@parc.xerox.comAbstractWe develop a Data-Oriented Parsing (DOP) modelbased on the syntactic representations of Lexical-Functional Grammar (LFG).
We start by sum-marizing the original DOP model for tree represen-tations and then show how it can be extended withcorresponding functional structures.
The resultingLFG-DOP model triggers a new, corpus-based notionof grammaticality, and its probability models exhibitinteresting behavior with respect to specificity andthe interpretation of ill-formed strings.1.
IntroductionData-Oriented Parsing (DOP) models of naturallanguage embody the assumption that humanlanguage perception and production works withrepresentations of past language experiences, ratherthan with abstract grammar ules (cf.
Bod 1992, 95;Scha 1992; Sima'an 1995; Rajman 1995).
DOPmodels therefore maintain large corpora of linguisticrepresentations of previously occurring utterances.New utterances are analyzed by combining(arbitrarily large) fragments from the corpus; theoccurrence-frequencies of the fragments are used todetermine which analysis is the most probable one.
Inaccordance with the general DOP architectureoutlined by Bod (1995), a particular DOP model isdescribed by specifying settings for the following fourparameters:?
a formal definition of a well-formed represen-tation fo r  utterance analyses,?
a set of decompos i t ion  operat ions  that divide agiven utterance analysis into a set of fragments,?
a set of compos i t ion  operat ions  by which suchfragments may be recombined to derive ananalysis of a new utterance, and?
a definition of a probabi l i ty  model  that indicateshow the probability of a new utterance analysis iscomputed on the basis of the probabilities of thefragments that combine to make it up.Previous instantiations of the DOP architecture werebased on utterance-analyses r presented as surfacephrase-structure trees CTree-DOP", e.g.
Bod 1993;Rajman 1995; Sima'an 1995; Goodman 1996;Bonnema et al 1997).
Tree-DOP uses twodecomposition operations that produce connectedsubtrees of utterance representations: (1) the Rootoperation selects any node of a tree to be the root ofthe new subtree and erases all nodes except theselected node and the nodes it dominates; (2) theFront ie r  operation then chooses a set (possiblyempty) of nodes in the new subtree different from itsroot and erases all subtrees dominated by the chosennodes.
The only composition operation used by Tree-DOP is a node-substitution peration that replaces theleft-most nonterminal frontier node in a subtree with afragment whose root category matches the category ofthe frontier node.
Thus Tree-DOP provides tree-representations for new utterances by combiningfragments from a corpus of phrase structure trees.A Tree-DOP representation R can typicallybe derived in many different ways.
If each derivationD has a probability P(D), then the probability ofderiving R is the sum of the individual derivationprobabilities:P(R) = ~D derives R P(D)A Tree-DOP derivation D = <tl, t2 ... tk> is producedby a stochastic branching process.
It starts byrandomly choosing a fragment tl labeled with theinitial category (e.g.
S).
At each subsequent step, anext fragment is chosen at random from among theset of competitors for composition into the currentsubtree.
The process tops when a tree results with nononterminal leaves.
Let CP( t lCS)  denote theprobability of choosing a tree t from a competition setCS containing t. Then the probability of a derivationisP(<tl, t2 ... tk>) = l ' \ ] iCP(t i  I CSi )where the competition probability CP(t ICS) is givenbyCP(t I CS) = P(t) / :El, e CS P(t')Here, P(t) is the fragment probability for t in a givencorpus.
Let Ti-I = tj o t2 o ... o ti.1 be the subanalysisjust before the ith step of the process, let LNC(Ti.I )denote the category of the leftmost nonterminal ofTi-l ,  and let r(t) denote the root categ.ory of afragment .
Then the competition set at the i th step isCS i = { t : r ( t )=LNC(Ti .
1 ) }That is, the competition sets for Tree-DOP aredetermined by the category of the leftmostnonterminal of the current subanalysis.
This is not theonly possible definition of competition set.
AsManning and Carpenter (1997) have shown, thecompetition sets can be made dependent on thecomposition operation.
Their left-corner languagemodel would also apply to Tree-DOP, yielding adifferent definition for the competition sets.
But theproperties of such Tree-DOP models have not beeninvestigated.Experiments with Tree-DOP on the PennTreebank and the OVIS corpus show a consistentincrease in parse accuracy when larger and morecomplex subtrees are taken into account (cf.
Bod1993, 95, 98; Bonnema et al 1997; Sekine &Grishman 1995; Sima'an 1995).
However, Tree-DOPis limited in that it cannot account for underlyingsyntactic (and semantic) dependencies that are not145reflected directly in a surface tree.
All modernlinguistic theories propose more articulated represen-tations and mechanisms in order to characterize suchlinguistic phenomena.
DOP models for a number ofricher representations have been explored (van denBerg et al 1994; Tugwell 1995), but these approacheshave remained context-free in their generative power.In contrast, Lexical-Functional Grammar (Kaplan &Bresnan 1982; Kaplan 1989), which assignsrepresentations consisting of a surface constituent treeenriched with a corresponding functional structure, isknown to be beyond context-free.
In the current work,we develop a DOP model based on representationsdefined by LFG theory CLFG-DOP").
That is, weprovide a new instantiation for the four parameters ofthe DOP architecture.
We will see that this basicLFG-DOP model triggers a new, corpus-based notionof grammaticality, and that it leads to a differentclass of its probabil ity models which exhibitinteresting properties with respect o specificity andthe interpretation of ill-formed strings.2.
A DOP model based on Lexica l -Funct ionalrepresentationsRepresentationsThe definition of a well-formed representation forutterance-analyses follows from LFG theory, that is,every utterance is annotated with a c-structure, an f-structure and a mapping ?
between them.
The c-structure is a tree that describes the surfaceconstituent structure of an utterance; the f-structure isan attribute-value matrix marking the grammaticalrelations of subject, predicate and object, as well asproviding agreement features and semantic forms; andis a correspondence function that maps nodes of thec-structure into units of the f-structure (Kaplan &Bresnan 1982; Kaplan 1989).
The following figureshows a representation for the utterance Kim eats.
(We leave out some features to keep the examplesimple.
)(1)"XlPRED K,m\]\]TENSE PRES I?
PRED 'eat(SUB J)' \]Note that the ?
correspondence function gives anexplicit characterization of the relation between thesuperficial and underlying syntactic properties of anutterance, indicating how certain parts of the stringcarry information about particular units of underlyingstructure.
As such, it will play a crucial role in ourdefinition for the decomposition and compositionoperations of LFG-DOP.
In (1) we see for instancethat the NP node maps to the subject f-structure, andthe S and VP nodes map to the outermost f-structure.It is generally the case that the nodes in asubtree carry information only about the f-structureunits that the subtree's root gives access to.
Thenotion of accessibil ity is made precise in thefollowing definition:An f-structure unit f i s  ?-accessible from a node niff either n is C-linked to f (that is, f=  ?
(n) ) o r fis contained within ?
(n) (that is, there is a chainof attributes that leads from ?
(n) to f).All the f-structure units in (1) are C-accessible fromfor instance the S node and the VP node, but theTENSE and top-level PRED are not 0-accessible fromthe NP node.According to LFG theory, c-structures and f-structures must satisfy certain formal well-formednessconditions.
A c-structure/f-structure pair is a validLFG representation only if it satisfies the Non-branching Dominance, Uniqueness, Coherence andCompleteness conditions (Kaplan & Bresnan 1982).Nonbranching Dominance demands that no c-structurecategory appears twice in a nonbranching dominancechain; Uniqueness asserts that there can be at mostone value for any attribute in the f-structure;Coherence prohibits the appearance of grammaticalfunctions that are not governed by the lexicalpredicate; and Completeness requires that all thefunctions that a predicate governs appear as attributesin the local f-structure.Decomposition operationsMany different DOP models are compatible with thesystem of LFG representations.
In this paper weoutline a basic LFG-DOP model which extends theoperations of Tree-DOP to take correspondences andf-structure features into account.
The decompositionoperations for this model will produce fragments ofthe composite LFG representations.
These will consistof connected subtrees whose nodes are in C-correspondence with sub-units of f-structures.
Weextend the Root and Frontier decomposition opera-tions of Tree-DOP so that they also apply to the nodesof the c-structure while respecting the fundamentalprinciples of c-structure/f-structure correspondence.When a node is selected by the Rootoperation, all nodes outside of that node's subtree areerased, just as in Tree-DOP.
Further, for LFG-DOP,all ?
links leaving the erased nodes are removed andall f-structure units that are not C-accessible from theremaining nodes are erased.
Root thus maintains theintuitive correlation between nodes and theinformation in their corresponding f-structures.
Forexample, if Root selects the NP in (1), then the f-structure corresponding to the S node is erased, giving(2) as a possible fragment:(2)~ i  PRED 'Kim' !NP NUM SG \]In addition the Root operation deletes from theremaining f-structure all semantic forms that are localto f-structures that correspond to erased c-structurenodes, and it thereby also maintains the fundamentaltwo-way connection between words and meanings.Thus, if Root selects the VP node so that the NP iserased, the subject semantic form "Kim" is alsodeleted:146(3)SUB, \[,~u,., sG\] \]p~, .
, , .
.
.D -  TENSE PRESeats PRED 'eat(SUB J)'As with Tree-DOP, the Front ier  operation thenselects a set of frontier nodes and deletes all subtreesthey dominate.
Like Root, it also removes the ~ linksof the deleted nodes and erases any semantic formthat corresponds to any of those nodes.
Frontier doesnot delete any other f-structure features.
This reflectsthe fact that all features are C-accessible from thefragment's root even when nodes below the frontierare erased.
For instance, if the VP in (1) is selectedas a frontier node, Front ier  erases the predicate"eat(SUB J)" from the fragment:(4)Kimsu., IPRE?
:ml INo.TENSE PRESNote that the Root and Frontier operations retain thesubject's NUM feature in the VP-rooted fragment (3),even though the subject NP is not present.
Thisreflects the fact, usually encoded in particulargrammar ules or lexical entries, that verbs of Englishcarry agreement features for their subjects.
On theother hand, fragment (4) retains the predicate'sTENSE feature, reflecting the possibility that Englishsubjects might also carry information about theirpredicate's tense.
Subject-tense agreement asencoded in (4) is a pattern seen in some languages(e.g.
the split-ergativity pattern of languages likeHindi, Urdu and Georgian) and thus there is nouniversal principle by which fragments uch as (4)can be ruled out.
But in order to represent directly thepossibility that subject-tense agreement is not adependency of English, we also allow an S fragmentin which the TENSE feature is deleted, as in (5).
(5)KimFragment (5) is produced by a third decompositionoperation, Discard, defined to construct generali-zations of the fragments supplied by Root  andFrontier.
D iscard acts to delete combinations ofattribute-value pairs subject to the followingrestriction: Discard  does not delete pairs whosevalues C-correspond to remaining c-structure nodes.This condition maintains the essentialcorrespondences of LFG representations: if a c-structure and an f-structure are paired in one fragmentprovided by Root and Frontier, then Discard alsopairs that c-structure with all generalizations of thatfragment's f-structure.
Fragment (5) results fromapplying Discard  to the TENSE feature in (4).147Discard also produces fragments uch as (6), wherethe subject's number in (3) has been deleted:(6)V P ~ I  TENSE Re!ts~ \[ PRED 'eat(SUBJ)' JAgain, since we have no language-specific knowled-ge apart from the corpus, we have no basis for rulingout fragments like (6).
Indeed, it is quite intuitive toomit the subject's number in fragments derived fromsentences with past-tense verbs or modals.
Thus thespecification of Discard reflects the fact that LFGrepresentations, unlike LFG grammars, do notindicate unambiguously the c-structure source (orsources) of their f-structure feature values.The composition operationIn LFG-DOP the operation for combining fragments,again indicated by o, is carried out in two steps.
Firstthe c-structures are combined by left-most substitutionsubject o the category-matching condition, just as inTree-DOP.
This is followed by the recursiveunification of the f-structures corresponding to thematching nodes.
The result retains the ?correspondences of the fragments being combined.
Aderivation for an LFG-DOP representation R is asequence of fragments the first of which is labeledwith S and for which the iterative application of thecomposition operation produces R.We show in (7) the effect of the LFGcomposition operation using two fragments fromrepresentations of an imaginary corpus containing thesentences Kim eats and People ate.
The VP-rootedfragment is substituted for the VP in the firstfragment, and the second f-structure unifies with thefirst f-structure, resulting in a representation for thenew sentence Kim ate.
(7)SUBJ \[KimatePRED 'Kim'\] \]so I\]su., I I  I = TENSE PASTPRED 'eat(SUB J)'i uM so IITENSE PAST \[PRED 'eat(SUB J)' \]This representation satisfies the well-formednessconditions and is therefore valid.
Note that in LFG-DOP, as in Tree-DOP, the same representation maybe produced by several derivations involving differentfragments.Another valid representation for the sentence Kim atecould be composed from a fragment for Kim that doesnot preserve the number feature, leading to arepresentation which is unmarked for number.
Theprobability models we discuss below have thedesirable property that they tend to assign higherprobabilities to more specific representations.The following derivation produces a validrepresentation for the intuitively ungrammaticalsentence People eats:(8), I i ?w "L ,jpeopleeatssuB, II I =TENSE PRESPRED 'eat(SUB J)'people eatsSUBJ \[ NUMPRED '~?ple'lpL \]TENSE PRESPRED 'eat(SUB J)'This system of fragments and composition thusprovides a representational basis for a robust model oflanguage comprehension i that it assigns at leastsome representations to many strings that wouldgenerally be regarded as ill-formed.
A correlate of thisadvantage, however, is the fact that it does not offer adirect formal account of metalinguistic judgments ofgrammaticality.
Nevertheless, we can reconstruct thenotion of grammaticality by means of the followingdefinition:A sentence is grammatical with respect to a corpusif and only if it has at least one validrepresentation with at least one derivation whosefragments are produced only by Root and Frontierand not by Discard.Thus the system is robust in that it assigns threerepresentations (singular, plural, and unmarked as thesubject's number) to the string People eats, based onfragments for which the number feature of people,eats, or both has been discarded.
But unless thecorpus contains non-plural instances of people or non-singular instances of eats, there will be no Discard-free derivation and the string will be classified asungrammatical (with respect o the corpus).Probability modelsAs in Tree-DOP, an LFG-DOP representation R cantypically be derived in many different ways.
If eachderivation D has a probability P(D), then theprobability of deriving R is again the probability ofproducing it by any of its derivations.
This is the sumof the individual derivation probabilities:(9) P(R) = O derives R P(D)An LFG-DOP derivation is also produced by astochastic branching process which at each stepmakes a random selection from a competition set ofcompeting fragments.
Let CP(f l  CS) denote theprobability of choosing a fragment f from acompetition set CS containing f, then the probabilityof a derivation D = <fl,f2 ...fk> is(10) P(<fl,f2 ...fk>) = FIi CPffi I CSi)where as in Tree-DOP, CP(f I CS) is expressed interms of fragment probabilities P(f) by the formula( 11 ) CP(f I CS) = P(D / ~,fe cs  P(f)Tree-DOP is the special case where there are noconditions of validity other than the ones that areenforced at each step of the stochastic process by thecomposition operation.
This is not generally the caseand is certainly not the case for the CompletenessCondition of LFG representations: Completeness i  aproperty of a final representation that cannot beevaluated at any intermediate steps of the process.However, we can define probabilities for the validrepresentations by sampling only from suchrepresentations i  the output of the stochastic process.The probability of sampling a particular validrepresentation R is given by(12) P(R I R is valid) = P(R) / ~R' is valid P(R')This formula assigns probabilities to valid represent-ations whether or not the stochastic processguarantees validity.
The valid representions for aparticular utterance u are obtained by a furthersampling step and their probabilities are given by:(13) P(R I R is valid and yields u) =P(R) / ~R' is valid and yields u P(R~The formulas (9) through (13) will be part of anyLFG-DOP probability model.
The models will differonly in how the competition sets are defined, and thisin turn depends on which well-formedness conditionsare enforced on-line during the stochastic branchingprocess and which are evaluated by the off-linevalidity sampling process.One model, which we call M1, is a straight-forward extension of Tree-DOP's probability model.This computes the competition sets only on the basisof the category-matching condition, leaving all otherwell-formedness conditions for off-line sampling.
Thusfor M1 the competition sets are defined simply interms of the categories of a fragment's c-structure rootnode.
Suppose that Fi-I =f l  ?
f2 o ... ofi.1 is thecurrent subanalysis at the beginning of step i in theprocess, that LNC(Fi.1) denotes the category of theleftmost nonterminal node of the c-structure of F i.1,and that r(f) is now interpreted as the root-nodecategory of f s  c-structure component.
Then thecompetition set for the i th step is(14) CSi = { f :  r(0C)=LNC(Fi.1) }Since these competition sets depend only on thecategory of the leftmost nonterminal of the current c-structure, the competition sets group together allfragments with the same root category, independentof any other properties they may have or that aparticular derivation may have.
The competition148probability for a fragment can be expressed by theformula(15) CP(f) = p(f)/~Ef: r(f)=rff) P(\]")We see that the choice of a fragment at a particularstep in the stochastic process depends only on thecategory of its root node; other well-formednessproperties of the representation are not used inmaking fragment selections.
Thus, with this model thestochastic process may produce many invalidrepresentations; we rely on sampling of validrepresentations and the conditional probabilities givenby (12) and (13) to take the Uniqueness, Coherence,and Completeness Conditions into account.Another possible model (M2) defines thecompetition sets so that they take a second condition,Uniqueness, into account in addition to the root nodecategory.
For M2 the competing fragments at aparticular step in the stochastic derivation process arethose whose c-structures have the same root nodecategory as LNC(Fi.1 ) and also whose f-structures areconsistently unifiable with the f-structure of Fi.
1 .
Thusthe competition set for the ith step is(16) CSi = { f :  r(f)=LNC(Fi.1) andf i s  unifiablewith the f-structure of Fi-1 }Although it is still the case that the category-matching condition is independent of the derivation,the unifiabil ity requirement means that thecompetition sets vary according to the representationproduced by the sequence of previous steps in thestochastic process.
Unifiability must be determined ateach step in the process to produce a newcompetition set, and the competition probabilityremains dependent on the particular step:(17) CP(3~ I CSi) =P(fi) / ~f :  r(f)=r(fl) and f is  unifiable with/~.
I P(f)On this model we again rely on sampling and theconditional probabilities (12) and (13) to take just theCoherence and Completeness Conditions intoaccount.In model M3 we define the stochastic processto enforce three conditions, Coherence, Uniquenessand category-matching, so that it only producesrepresentations with well-formed c-structures thatcorrespond to coherent and consistent f-structures.
Thecompetition probabilities for this model are given bythe obvious extension of (17).
It is not possible,however, to construct a model in which theCompleteness Condition is enforced during thederivation process.
This is because the satisfiability ofthe Completeness Condition depends not only on theresults of previous steps of a derivation but also onthe following steps (see Kaplan & Bresnan 1982).This nonmonotonic property means that theappropriate step-wise competition sets cannot bedefined and that this condition can only be enforcedat the final stage of validity sampling.In each of these three models the category-matching condition is evaluated on-line during thederivation process while other conditions are eitherevaluated on-line or off-line by the after-the-factsampling process.
LFG-DOP is crucially differentfrom Tree-DOP in that at least one validity149requirement, the Completeness Condition, mustalways be left to the post-derivation process.
Notethat a number of other models are possible whichenforce other combinations of these three conditions.3.
Illustration and properties of LFG-DOPWe illustrate LFG-DOP using a very small corpusconsisting of the two simplified LFG representationsshown in (18):(18)"xSUBJ \[ PrOD 'pe?plel\]PRED 'walk(suB J)' JThe fragments from this corpus can be composed toprovide representations for the two observedsentences plus two new utterances, John walked andPeople fell.
This is sufficient o demonstrate hat theprobability models M1 and M2 assign differentprobabilities to particular epresentations.
We haveomitted the TENSE feature and the lexical categoriesN and V to reduce the number of the fragments wehave to deal with.
Applying the Root and Frontieroperators systematical ly to the first corpusrepresentation produces the fragments in the firstcolumn of (19), while the second column shows theadditional f-structure that is associated with each c-structure by the Discard operation.A total of 12 fragments are produced fromthis representation, and by analogy 12 fragments witheither PL or unmarked NUM values will also resultfrom People walked.
Note that the \[S NP VP\]fragment with the unspecified NUM value is producedfor both sentences and thus its corpus frequency is 2.There are 14 other S-rooted fragments, 4 NP-rootedfragments, and 4 VP-rooted fragments; each of theseoccurs only once.These fragments can be used to derive threedifferent representations for John walked (singular,plural, and unmarked as the subject's number).
Tofacilitate the presentation of our derivations andprobability calculations, we denote each fragment byan abbreviated name that indicates its c-structureroot-node category, the sequence of its frontier-nodelabels, and whether its subject's number is SG, PL, orunmarked (indicated by U).
Thus the first fragment in(19) is referred to as S/John-fell/SG and the unmarkedfragment hat Discard produces from it is referred toas S/John-fell/U.
Given this naming convention, wecan specify one of the derivations for John walked bythe expression S/NP-VP/U o NP/ John/SG oVP/walked/U, corresponding to an analysis in whichthe subject's number is marked as SG.
The fragmentVP/walked/U of course comes from People walked,the second corpus sentence, and does not appear in(19).
(19)~ ' f a l I ( S U B J ) '  J L PRED 'fall(SuB,I)' 1\[ PREDSUBJ NU'MJohnf e l l "  ~ PLIED 'falllsUBD'~ P ~  'Js~ hn\]NUMJohnls BJ IPREO'O dsu., I i !PRED 'fall(SUB J)'PRED 'fall(SUBJ)'\]PRED 'John\]Model M1 evaluates only the Tree-DOP root-categorycondition during the stochastic branching process, andthe competi t ion sets are f ixed independent of thederivation.
The probabil i ty of choosing the fragmentS/NP-VP/U,  given that an S-rooted f ragment isrequired, is always 2/16, its frequency divided by thesum of the frequencies of  all the S fragments.S imi la r ly ,  the probab i l i ty  of  then choos ingNP/John/SG to substitute at the NP frontier node is1/4, s ince the NP compet i t ion  set contains 4fragments each with frequency 1.
Thus, under modelM1 the probab i l i ty  of  produc ing  the completederivation S/NP-VP/U o NP/John/SG o VP/walked/Uis 2 /16x l /4x l /4=2/256.
This probabi l i ty  is smal lbecause it indicates the l ikel ihood of this derivationcompared to other derivations for John walked and forthe three other analyzable strings.
The computation ofthe other M1 derivation probabil it ies for John walkedis left to the reader.
There are 5 different derivationsfor the representation with SG number and 5 for thePL number, while there are only 3 ways of producingthe unmarked number U.
The conditional probabil it iesfor the particular epresentations (SG, PL, U) can becalculated by (9) and (13), and are given below.P(NUM=SG I valid and yield = John walked) = .353P(NUM=PL I valid and yield = John walked) = .353P(NUM=U I valid and yield = John walked) = .294150We see that the two specif ic representat ions areequally l ikely and each of them is more probable thanthe representation with unmarked NUM.Model  M2 produces a s l ight ly  d i f ferentdistr ibution of  probabil i t ies.
Under this model, theconsistency requirement is used in addit ion to theroot-category matching requirement o define thecompet i t ion sets at each step of  the branchingprocess.
This means that the first f ragment thatinstantiates the NUM feature to either SG or PLconstrains the compet i t ion sets for the fo l lowingchoices in a derivat ion.
Thus, having chosen theNP/John/SG fragment in the derivation S/NP-VP/U oNP/ John/SG o VP/walked/U,  only 3 VP fragmentsinstead of 4 remain in the competit ion set at the nextstep, since the VP/walked/PL fragment is no longeravailable.
The probabi l i ty for this derivation undermodel M2 is therefore 2 /16x l /4x l /3=2/192,  sl ightlyhigher than the probabi l i ty  assigned to it by M1.Table 1 shows the complete set of  derivations andtheir M2 probabilities for John walked.S/NP-VP/U o NP/JohrdSG o VP/walked/U SG 2/16 x 1/4 x 1/3S/NP-VP/SG ?
NP/John/SG o VP/walked/U SG 1/16 x 1/3 x 1/3S/NP-VP/SG ?
NP/John/U o VP/walked/U SG 1/16 x 1/3 x 1/3S/NP-walked/U oNP/John/SG SG 1/16 x 1/4S/John-VP/SG o VP/walked/U SG 1/16 x 1/3P(NUM=SG and yield = John walked) =351576 =.061P(NUM=SG Ivalid and yield = John walked) =701182 =.38S/NP-VP/U o NP/John/U o VP/walked/PL PL 2/16 x 1/4 x 1/4S/NP-VP/PL o NP/John/U oVP/walked/PL PL 1/16 x 1/3 x 1/3S/NP-VP/PL ?
NP/John/U o VP/walked/U PL 1/16 x 1/3 x 1/3S/NP-walked/PL o NP/JohrdU PL 1/16 x 1/3S/John-VP/U ?
VP/walked/PL PL 1/16 x 1/4P(NUM=PL and yield = John walked) = 33.5/576 = .058P(NUM=PL I valid and yield = John walked) = 671182 = .37S/NP-VP/U o NP/John/U o VP/walked/U U 2/16 x 1/4 x 1/4S/NP-walked/U o NP/John/U U 1/16 x 1/4S/John-VP/U o VP/walked/U U 1/16 x 1/4P(NUM=U and yield = John walked) =22.51576 =.039P(NUM=U I valid and yield = John walked) =451182 =.25Table 1: Model M2 derivations, ubject number features,and probabilities for John walkedThe total probabil ity for the derivations that produceJohn walked is .158, and the conditional probabil it iesfor the three representations are:P(NUM=SG I valid and yield = John walked) = .38P(NUM=PL I valid and yield = John walked) = .37P(NUM=U I valid and yield = John walked) = .25For model M2 the unmarked representation is lesslikely than under M1, and now there is a slight bias infavor of  the value SG over PL.
The SG value isfavored because it is carried by substitutions for theleft -most word of  the utterance and thus reducescompetit ion for subsequent choices.
The value PLwould be more probable for the sentence People fell.Thus both models give higher probabil ity to the morespecif ic representations.
Moreover,  M1 assigns thesame probabil ity to SG and PL, whereas M2 doesn't.M2 reflects a left-to-right bias (which might bepsycholinguistically interesting -- a so-called primacyeffect), whereas M1 is, like Tree-DOP, order indepen-dent.It turns out that all LFG-DOP probabilitymodels (M 1, M2 and M3) display a preference for themost specific representation.
This preference partlydepends on the number of derivations: specificrepresentations tend to have more derivations thangeneralized (i.e., unmarked) representations, andconsequently tend to get higher probabilities -- otherthings being equal.
However, this preference alsodepends on the number of feature values: the morefeature values, the longer the minimal derivationlength must be in order to get a preference for themost specific representation (Cormons, forthcoming).The bias in favor of more specific represen-tations, and consequently fewer Discard-producedfeature generalizations, is especially interesting forthe interpretation of ill-formed input strings.
Bod &Kaplan (1997) show that in analyzing an intuitivelyungrammatical string like These boys walks, there is aprobabilistic accumulation of evidence for the pluralinterpretation over the singular and unmarked one (forall models M1, M2 and M3).
This is because bothThese and boys carry the PL feature while only walksis a source for the SG feature, leading to morederivations for the PL reading of These boys walks.
Incase of "equal evidence" as in the ill-formed stringBoys walks, model M I assigns the same probability toPL and SG, while models M2 and M3 prefer the PLinterpretation due to their left-to-right bias.4.
Conclusion and computational issuesPrevious DOP models were based on context-free treerepresentations that cannot adequately represent alllinguistic phenomena.
In this paper, we gave a DOPmodel based on the more articulated representationsprovided by LFG theory.
LFG-DOP combines theadvantages of two approaches: the linguisticadequacy of LFG together with the robustness ofDOP.
LFG-DOP triggers a new, corpus-based notionof grammaticality, and its probability models exhibita preference for the most specific analysis containingthe fewest number of feature generalizations.The main goal of this paper was to providethe theoretical background of LFG-DOP.
As to thecomputational spects of LFG-DOP, the problem offinding the most probable representation f a sentenceis NP-hard even for Tree-DOP.
This problem may betackled by Monte Carlo sampling techniques (as inTree-DOP, cf.
Bod 1995) or by computing the Viterbin best derivations of a sentence.
Other optimizationheuristics may consist of restricting the fragmentspace, for example by putting an upper bound on thefragment depth, or by constraining the decompositionoperations.
To date, a couple of LFG-DOP implemen-tations are either operational (Cormons, forthcoming)or under development, and corpora with LFGrepresentations have recently been developed (atXRCE France and Xerox PARC).
Experiments withthese corpora will be presented in due time.AcknowledgmentsWe thank Joan Bresnan, Mary Dalrymple, MarkJohnson, Martin Kay, John Maxwell, Remko Scha,Khalil Sima'an, Andy Way and three anonymousreviewers for helpful comments.
We are most gratefulto Boris Cormons whose comments were particularlyhelpful.
This research was supported by NWO, theDutch Organization for Scientific Research.
Theinitial stages of this work were carried out while thesecond author was a Fellow of the NetherlandsInstitute for Advanced Study (NIAS).
Subsequentstages were also carried out while the first author wasa Consultant at Xerox PARC.ReferencesM.
van den Berg, R. Bod and R. Scha 1994.
"A Corpus-Based Approach to Semantic Interpretation", ProceedingsNinth Amsterdam Colloquium, Amsterdam, The Netherlands.R.
Bod 1992.
"A Computational Model of LanguagePerformance: Data Oriented Parsing", ProceedingsCOLING-92, Nantes, France.R.
Bod 1993.
"Using an Annotated Corpus as a StochasticGrammar", Proceedings EACL'93, Utrecht, The Netherlands.R.
Bod 1995.
Enriching Linguistics with Statistics:Performance Models of Natural Language, ILLCDissertation Series 1995-14, University of AmsterdamR.
Bod 1998.
"Spoken Dialogue Interpretation with the DOPModel", this proceedings.R.
Bod and R. Kaplan 1997.
"On Performance models forLexical-Functional Analysis", Paper presented at theComputational Psycholinguistics Conference 1997, Berkeley(Ca).R.
Bonnema, R. Bod and R. Scha 1997.
"A DOP Model forSemantic Interpretation", Proceedings ACL/EACL-97,Madrid, Spain.B.
Cormons, forthcoming.
Analyse t desambiguisation: U eapproche purement fi base de corpus (Data-Oriented Parsing)pour le formalisme des Grammaires LexicalesFonctionnelles, PhD thesis, Universit6 de Rennes, France.J.
Goodman 1996.
"Efficient Algorithms for Parsing the DOPModel", Proceedings Empirical Methods in NaturalLanguage Processing, Philadelphia, Pennsylvania.R.
Kaplan 1989.
"The Formal Architecture of Lexical-Functional Grammar", Journal of Information Science andEngineering, vol.
5, 305-322.R.
Kaplan and J. Bresnan 1982.
"Lexical-FunctionalGrammar: A Formal System for GrammaticalRepresentation", in J. Bresnan (ed.
), The MentalRepresentation f Grammatical Relations, The MIT Press,Cambridge, MA.C.
Manning and B. Carpenter 1997.
"Probabilistic parsingusing left comer language models", Proceedings IWPT'97,Boston (Mass.).M.
Rajman 1995.
"Approche Probabiliste de l'AnalyseSyntaxique", Traitement Automatique des Langues, vol.36(1-2).R.
Scha 1992.
"Virtuele Grammatica's en CreatieveAlgoritmen", Gramma/l'TT l(1).S.
Sekine and R. Grishman 1995.
"A Corpus-basedProbabilistic Grammar with Only Two Non-terminals",Proceedings Fourth International Workshop on ParsingTechnologies, Prague, Czech Republic.K.
Sima'an 1995.
"An optimized algorithm for Data OrientedParsing", in R. Mitkov and N. Nicolov (eds.
), RecentAdvances in Natural Language Processing 1995, JohnBenjamins, Amsterdam.D.
Tugwell 1995.
"A State-Transition Grammar for Data-Oriented Parsing", Proceedings European Chapter of theACL'95, Dublin, Ireland.151
