Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 440?445,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsCross-cultural Deception DetectionVer?onica P?erez-RosasComputer Science and EngineeringUniversity of North Texasveronicaperezrosas@my.unt.eduRada MihalceaComputer Science and EngineeringUniversity of Michiganmihalcea@umich.eduAbstractIn this paper, we address the task ofcross-cultural deception detection.
Usingcrowdsourcing, we collect three deceptiondatasets, two in English (one originatingfrom United States and one from India),and one in Spanish obtained from speakersfrom Mexico.
We run comparative experi-ments to evaluate the accuracies of decep-tion classifiers built for each culture, andalso to analyze classification differenceswithin and across cultures.
Our resultsshow that we can leverage cross-culturalinformation, either through translation orequivalent semantic categories, and builddeception classifiers with a performanceranging between 60-70%.1 IntroductionThe identification of deceptive behavior is a taskthat has gained increasing interest from researchersin computational linguistics.
This is mainly moti-vated by the rapid growth of deception in writtensources, and in particular in Web content, includingproduct reviews, online dating profiles, and socialnetworks posts (Ott et al, 2011).To date, most of the work presented on deceptiondetection has focused on the identification of deceitclues within a specific language, where English isthe most commonly studied language.
However, alarge portion of the written communication (e.g.,e-mail, chats, forums, blogs, social networks) oc-curs not only between speakers of English, but alsobetween speakers from other cultural backgrounds,which poses important questions regarding the ap-plicability of existing deception tools.
Issues suchas language, beliefs, and moral values may influ-ence the way people deceive, and therefore mayhave implications on the construction of tools fordeception detection.In this paper, we explore within- and across-culture deception detection for three different cul-tures, namely United States, India, and Mexico.Through several experiments, we compare the per-formance of classifiers that are built separately foreach culture, and classifiers that are applied acrosscultures, by using unigrams and word categoriesthat can act as a cross-lingual bridge.
Our resultsshow that we can achieve accuracies in the range of60-70%, and that we can leverage resources avail-able in one language to build deception tools foranother language.2 Related WorkResearch to date on automatic deceit detection hasexplored a wide range of applications such as theidentification of spam in e-mail communication,the detection of deceitful opinions in review web-sites, and the identification of deceptive behaviorin computer-mediated communication includingchats, blogs, forums and online dating sites (Penget al, 2011; Toma et al, 2008; Ott et al, 2011;Toma and Hancock, 2010; Zhou and Shi, 2008).Techniques used for deception detection fre-quently include word-based stylometric analysis.Linguistic clues such as n-grams, count of usedwords and sentences, word diversity, and self-references are also commonly used to identify de-ception markers.
An important resource that hasbeen used to represent semantic information for thedeception task is the Linguistic Inquiry and WordCount (LIWC) dictionary (Pennebaker and Francis,1999).
LIWC provides words grouped into seman-tic categories relevant to psychological processes,which have been used successfully to perform lin-guistic profiling of true tellers and liars (Zhou et al,2003; Newman et al, 2003; Rubin, 2010).
In addi-tion to this, features derived from syntactic ContextFree Grammar parse trees, and part of speech havealso been found to aid the deceit detection (Feng etal., 2012; Xu and Zhao, 2012).440While most of the studies have focused on En-glish, there is a growing interest in studying decep-tion for other languages.
For instance, (Fornaciariand Poesio, 2013) identified deception in Italian byanalyzing court cases.
The authors explored severalstrategies for identifying deceptive clues, such asutterance length, LIWC features, lemmas and partof speech patterns.
(Almela et al, 2012) studied thedeception detection in Spanish text by using SVMclassifiers and linguistic categories, obtained fromthe Spanish version of the LIWC dictionary.
Astudy on Chinese deception is presented in (Zhanget al, 2009), where the authors built a deceptivedataset using Internet news and performed machinelearning experiments using a bag-of-words repre-sentation to train a classifier able to discriminatebetween deceptive and truthful cases.It is also worth mentioning the work conductedto analyze cross-cultural differences.
(Lewis andGeorge, 2008) presented a study of deception insocial networks sites and face-to-face communi-cation, where authors compare deceptive behaviorof Korean and American participants, with a sub-sequent study also considering the differences be-tween Spanish and American participants (Lewisand George, 2009).
In general, research findingssuggest a strong relation between deception andcultural aspects, which are worth exploring withautomatic methods.3 DatasetsWe collect three datasets for three different cul-tures: United States (English-US), India (English-India), and Mexico (Spanish-Mexico).
Following(Mihalcea and Strapparava, 2009), we collect shortdeceptive and truthful essays for three topics: opin-ions on Abortion, opinions on Death Penalty, andfeelings about a Best Friend.For English-US and English-India, we use Ama-zon Mechanical Turk with a location restriction, sothat all the contributors are from the country of in-terest (US and India).
We collect 100 deceptive and100 truthful statements for each of the three topics.To avoid spam, each contribution is manually veri-fied by one of the authors of this paper.For Spanish-Mexico, while we initially attempted to collect dataalso using Mechanical Turk, we were not able toreceive enough contributions.
We therefore cre-ated a separate web interface to collect data, andrecruited participants through contacts of the pa-per?s authors.
The overall process was significantlymore time consuming than for the other two cul-tures, and resulted in fewer contributions, namely39+39 statements for Abortion, 42+42 statementsfor Death Penalty, and 94+94 statements for BestFriend.
For all three cultures, the participants firstprovided their truthful responses, followed by thedeceptive ones.Interestingly, for all three cultures, the averagenumber of words for the deceptive statements (62words) is significantly smaller than for the truthfulstatements (81 words), which may be explained bythe added difficulty of the deceptive process, andis in line with previous observations about the cuesof deception (DePaulo et al, 2003).4 ExperimentsThrough our experiments, we seek answers to thefollowing questions.
First, what is the perfor-mance for deception classifiers built for differentcultures?
Second, can we use information drawnfrom one culture to build a deception classifier foranother culture?
Finally, what are the psycholin-guistic classes most strongly associated with de-ception/truth, and are there commonalities or dif-ferences among languages?In all our experiments, we formulate the decep-tion detection task in a machine learning frame-work, where we use an SVM classifier to discrimi-nate between deceptive and truthful statements.14.1 What is the performance for deceptionclassifiers built for different cultures?We represent the deceptive and truthful statementsusing two different sets of features.
First we useunigrams obtained from the statements correspond-ing to each topic and each culture.
To select theunigrams, we use a threshold of 10, where all theunigrams with a frequency less than 10 are dropped.Since previous research suggested that stopwordscan contain linguistic clues for deception, no stop-word removal is performed.Experiments are performed using a ten-foldcross validation evaluation on each dataset.Usingthe same unigram features, we also perform cross-topic classification, so that we can better under-stand the topic dependence.
For this, we trainthe SVM classifier on training data consisting of amerge of two topics (e.g., Abortion + Best Friend)and test on the third topic (e.g., Death Penalty).
Theresults for both within- and cross-topic are shownin the last two columns of Table 1.1We use the SVM classifier implemented in the Wekatoolkit, with its default settings.441LIWC UnigramsTopic Linguistic Psychological Relativity Personal All Within-topic Cross-topicEnglish-USAbortion 72.50% 68.75% 44.37% 67.50% 73.03% 63.75% 80.36%Best Friend 75.98% 68.62% 58.33% 54.41% 73.03% 74.50% 60.78%Death Penalty 60.36% 54.50% 49.54% 50.45% 58.10% 58.10% 77.23%Average 69.61% 63.96% 50.75% 57.45% 69.05% 65.45% 72.79%English-IndiaAbortion 56.00% 48.50% 46.50% 48.50% 56.00% 46.00% 50.00%Best Friend 68.18% 68.62% 54.55% 53.18% 71.36% 60.45% 57.23%Death Penalty 56.00% 52.84% 57.50% 53.50% 63.50% 57.50% 54.00%Average 60.06% 59.19% 52.84% 51.72% 63.62% 54.65% 53.74%Spanish-MexicoAbortion 73.17% 67.07% 48.78% 51.22% 62.20% 52.46% 57.69%Best Friend 72.04% 74.19% 67.20% 54.30% 75.27% 66.66% 50.53%Death Penalty 73.17% 67.07% 48.78% 51.22% 62.20% 54.87% 63.41%Average 72.79% 69.45% 54.92% 52.25% 67.89% 57.99% 57.21%Table 1: Within-culture classification, using LIWC word classes and unigrams.
For LIWC, results areshown for within-topic experiments, with ten-fold cross validation.
For unigrams, both within-topic(ten-fold cross validation on the same topic) and cross-topic (training on two topics and testing on thethird topic) results are reported.Second, we use the LIWC lexicon to extract fea-tures corresponding to several word classes.
LIWCwas developed as a resource for psycholinguisticanalysis (Pennebaker and Francis, 1999).
The 2001version of LIWC includes about 2,200 words andword stems grouped into about 70 classes relevantto psychological processes (e.g., emotion, cogni-tion), which in turn are grouped into four broad cat-egories2namely: linguistic processes, psychologi-cal processes, relativity, and personal concerns.
Afeature is generated for each of the 70 word classesby counting the total frequency of the words belong-ing to that class.
We perform separate evaluationsusing each of the four broad LIWC categories, aswell as using all the categories together.
The re-sults obtained with the SVM classifier are shownin Table 1.Overall, the results show that it is possible todiscriminate between deceptive and truthful casesusing machine learning classifiers, with a perfor-mance superior to a random baseline which for alldatasets is 50% given an even class distribution.Considering the unigram results, among the threecultures considered, the deception discriminationworks best for the English-US dataset, and this isalso the dataset that benefits most from the largeramount of training data brought by the cross-topicexperiments.
In general, the cross-topic evaluationssuggest that there is no high topic dependence inthis task, and that using deception data from differ-2http://www.liwc.net/descriptiontable1.phpent topics can lead to results that are comparableto the within-topic data.
Interestingly, among thethree topics considered, the Best Friend topic hasconsistently the highest within-topic performance,which may be explained by the more personal na-ture of the topic, which can lead to clues that areuseful for the detection of deception (e.g., refer-ences to the self or personal relationships).Regarding the LIWC classifiers, the results showthat the use of the LIWC classes can lead to per-formance that is generally better than the one ob-tained with the unigram classifiers.
The explicit cat-egorization of words into psycholinguistic classesseems to be particularly useful for the languageswhere the words by themselves did not lead to verygood classification accuracies.
Among the fourbroad LIWC categories, the linguistic category ap-pears to lead to the best performance as comparedto the other categories.
It is notable that in Spanish,the linguistic category by itself provides results thatare better than when all the LIWC classes are used,which may be due to the fact that Spanish has moreexplicit lexicalization for clues that may be relevantto deception (e.g., verb tenses, formality).4.2 Can we use information drawn from oneculture to build a deception classifier inanother culture?In the next set of experiments, we explore the de-tection of deception using training data originatingfrom a different culture.
As with the within-culture442Topic Linguistic Psychological Relativity Personal All LIWC UnigramsTraining: English-US Test: English-IndiaAbortion 58.00% 51.00% 48.50% 51.50% 52.25% 57.89%Best Friend 66.36% 47.27% 48.64% 50.45% 59.54% 51.00%Death Penalty 54.50% 50.50% 50.00% 48.50% 53.5% 59.00%Average 59.62% 49.59% 49.05% 50.15% 55.10% 55.96%Training: English-India Test: English-USAbortion 71.32% 47.49% 43.38% 45.82% 62.50% 55.51%Best Friend 59.74% 49.35% 51.94% 49.36% 55.84% 53.20%Death Penalty 51.47% 44.11% 54.88% 50.98% 39.21% 50.71%Average 60.87% 46.65% 50.06% 48.72% 52.51% 54.14%Training: English-US Test: Spanish-MexicoAbortion 70.51% 46.15% 50.00% 52.56% 53.85% 61.53%Best Friend 69.35% 52.69% 51.08% 46.77% 67.74% 65.03%Death Penalty 54.88% 54.88% 53.66% 50.00% 62.19% 59.75%Average 64.92% 51.24% 51.58% 49.78% 61.26% 62.10%Training: English-India Test: Spanish-MexicoAbortion 48.72% 50.00% 47.44% 42.31% 43.58% 55.12 %Best Friend 68.28% 63.44% 56.45% 54.84% 60.75% 67.20%Death Penalty 60.98% 53.66% 54.88% 60.98% 59.75% 51.21%Average 59.32% 55.70% 52.92% 52.71% 54.69% 57.84%Table 2: Cross-cultural experiments using LIWC categories and unigramsexperiments, we use unigrams and LIWC features.For consistency across the experiments, given thatthe size of the Spanish dataset is different com-pared to the other two datasets, we always train onone of the English datasets.To enable the unigram based experiments, wetranslate the two English datasets into Spanish byusing the Bing API for automatic translation.3Asbefore, we extract and keep only the unigramswith frequency greater or equal to 10.
The resultsobtained in these cross-cultural experiments areshown in the last column of Table 2.In a second set of experiments, we use the LIWCword classes as a bridge between languages.
First,each deceptive or truthful statement is representedusing features based on the LIWC word classes.Next, since the same word classes are used in boththe English and the Spanish LIWC lexicons, thisLIWC-based representation is independent of lan-guage, and therefore can be used to perform cross-cultural experiments.
Table 2 shows the resultsobtained with each of the four broad LIWC cate-gories, as well as with all the LIWC word classes.We also attempted to combine unigrams andLIWC features.
However, in most cases, no im-provements were noticed with respect to the useof unigrams or LIWC features alone.
We are notreporting these results due to space limitation.These cross-cultural evaluations lead to several3http://http://http://www.bing.com/dev/en-us/dev-centerfindings.
First, we can use data from a cultureto build deception classifiers for another culture,with performance figures better than the randombaseline, but weaker than the results obtained withwithin-culture data.
An important finding is thatLIWC can be effectively used as a bridge for cross-cultural classification, with results that are com-parable to the use of unigrams, which suggeststhat such specialized lexicons can be used forcross-cultural or cross-lingual classification.
More-over, using only the linguistic category from LIWCbrings additional improvements, with absolute im-provements of 2-4% over the use of unigrams.
Thisis an encouraging result, as it implies that a seman-tic bridge such as LIWC can be effectively usedto classify deception data in other languages, in-stead of using the more costly and time consumingunigram method based on translations.4.3 What are the psycholinguistic classesmost strongly associated withdeception/truth?The final question we address is concerned withthe LIWC classes that are dominant in deceptiveand truthful text for different cultures.
We use themethod presented in (Mihalcea and Strapparava,2009), which consists of a metric that measures thesaliency of LIWC classes in deceptive versus truth-ful data.
Following their strategy, we first create acorpus of deceptive and truthful text using a mixof all the topics in each culture.
We then calculate443Class Score Sample words Class Score Sample wordsEnglish-USDeceptive TruthfulMetaph 1.77 Die,died,hell,sin,lord Insight 0.68 Accept,believe,understandOther 1.46 He,her,herself,him I 0.66 I,me,my,myself,You 1.41 Thou,you Optimism 0.65 accept, hope, top, bestOthref 1.18 He,her,herself,him We 0.55 Our,ourselves,us,we,Negemo 1.18 Afraid,agony,awful,bad Friends 0.46 Buddies,friendEnglish-IndiaDeceptive TruthfulNegate 1.49 Cannot,neither,no,none Past 0.78 Happened,helped,liked,listenedPhysical 1.46 Heart,ill,love,loved, I 0.66 I,me,mine,myFuture 1.42 Be,may,might,will Optimism 0.65 Accept,accepts,best,bold,Other 1.17 He,she, himself,herself We 0.55 Our,ourselves,us,weHumans 1.08 Adult,baby,children,human Friends 0.46 Buddies,companion,friend,palSpanish-MexicoDeceptive TruthfulCertain 1.47 Jam?as(never),siempre(always) Optimism 0.66 Aceptar(accept),animar(cheer)Humans 1.28 Beb?e(baby),persona(person) Self 0.65 Conmigo(me),tengo(have),soy(am)You 1.26 Eres(are),estas(be),su(his/her) We 0.58 Estamos(are),somos(be),tenemos(have)Negate 1.25 Jam?as(never),tampoco(neither) Friends 0.37 Amigo/amiga(friend),amistad(friendship)Other 1.22 Es(is),esta(are),otro(other) Past 0.32 Compartimos(share),vivimos(lived)Table 3: Top ranked LIWC classes for each culture, along with sample wordsthe dominance for each LIWC class, and rank theclasses in reversed order of their dominance score.Table 3 shows the most salient classes for eachculture, along with sample words.This analysis shows some interesting patterns.There are several classes that are shared among thecultures.
For instance, the deceivers in all culturesmake use of negation, negative emotions, and refer-ences to others.
Second, true tellers use more opti-mism and friendship words, as well as references tothemselves.
These results are in line with previousresearch, which showed that LIWC word classesexhibit similar trends when distinguishing betweendeceptive and non-deceptive text (Newman et al,2003).
Moreover, there are also word classes thatonly appear in some of the cultures; for example,time classes (Past, Future) appear in English-Indiaand Spanish-Mexico, but not in English-US, whichin turn contains other classes such as Insight andMetaph.5 ConclusionsIn this paper, we addressed the task of deceptiondetection within- and across-cultures.
Using threedatasets from three different cultures, each cover-ing three different topics, we conducted severalexperiments to evaluate the accuracy of deceptiondetection when learning from data from the sameculture or from a different culture.
In our evalua-tions, we compared the use of unigrams versus theuse of psycholinguistic word classes.The main findings from these experiments are:1) We can build deception classifiers for differentcultures with accuracies ranging between 60-70%,with better performance obtained when using psy-cholinguistic word classes as compared to simpleunigrams; 2) The deception classifiers are not sen-sitive to different topics, with cross-topic classifi-cation experiments leading to results comparableto the within-topic experiments; 3) We can usedata originating from one culture to train decep-tion detection classifiers for another culture; theuse of psycholinguistic classes as a bridge acrosslanguages can be as effective or even more effec-tive than the use of translated unigrams, with theadded benefit of making the classification processless costly and less time consuming.The datasets introduced in this paper are publiclyavailable from http://nlp.eecs.umich.edu.AcknowledgmentsThis material is based in part upon work supportedby National Science Foundation awards #1344257and #1355633 and by DARPA-BAA-12-47 DEFTgrant #12475008.
Any opinions, findings, and con-clusions or recommendations expressed in this ma-terial are those of the authors and do not necessarilyreflect the views of the National Science Founda-tion or the Defense Advanced Research ProjectsAgency.444References?A.
Almela, R.
Valencia-Garc?
?a, and P. Cantos.
2012.Seeing through deception: A computational ap-proach to deceit detection in written communication.In Proceedings of the Workshop on ComputationalApproaches to Deception Detection, pages 15?22,Avignon, France, April.
Association for Computa-tional Linguistics.B.
DePaulo, J. Lindsay, B. Malone, L. Muhlenbruck,K.
Charlton, and H. Cooper.
2003.
Cues to decep-tion.
Psychological Bulletin, 129(1).S.
Feng, R. Banerjee, and Y. Choi.
2012.
Syntacticstylometry for deception detection.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics: Short Papers - Volume2, ACL ?12, pages 171?175, Stroudsburg, PA, USA.Association for Computational Linguistics.T.
Fornaciari and M. Poesio.
2013.
Automatic decep-tion detection in italian court cases.
Artificial Intelli-gence and Law, 21(3):303?340.C.
Lewis and J. George.
2008.
Cross-cultural de-ception in social networking sites and face-to-facecommunication.
Comput.
Hum.
Behav., 24(6):2945?2964, September.C.
Lewis and Giordano G. George, J.
2009.
A cross-cultural comparison of computer-mediated decep-tive communication.
In Proceedings of Pacific AsiaConference on Information Systems.R.
Mihalcea and C. Strapparava.
2009.
The lie de-tector: Explorations in the automatic recognition ofdeceptive language.
In Proceedings of the Associa-tion for Computational Linguistics (ACL 2009), Sin-gapore.M.
Newman, J. Pennebaker, D. Berry, and J. Richards.2003.
Lying words: Predicting deception from lin-guistic styles.
Personality and Social PsychologyBulletin, 29.M.
Ott, Y. Choi, C. Cardie, and J. Hancock.
2011.Finding deceptive opinion spam by any stretch ofthe imagination.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Volume1, HLT ?11, pages 309?319, Stroudsburg, PA, USA.Association for Computational Linguistics.H.
Peng, C. Xiaoling, C. Na, R. Chandramouli, andP.
Subbalakshmi.
2011.
Adaptive context mod-eling for deception detection in emails.
In Pro-ceedings of the 7th international conference on Ma-chine learning and data mining in pattern recogni-tion, MLDM?11, pages 458?468, Berlin, Heidelberg.Springer-Verlag.J.
Pennebaker and M. Francis.
1999.
Linguistic in-quiry and word count: LIWC.
Erlbaum Publishers.V.
Rubin.
2010.
On deception and deception detec-tion: Content analysis of computer-mediated statedbeliefs.
Proceedings of the American Society for In-formation Science and Technology, 47(1):1?10.C.
Toma and J. Hancock.
2010.
Reading betweenthe lines: linguistic cues to deception in online dat-ing profiles.
In Proceedings of the 2010 ACM con-ference on Computer supported cooperative work,CSCW ?10, pages 5?8, New York, NY, USA.
ACM.C.
Toma, J. Hancock, and N. Ellison.
2008.
Separatingfact from fiction: An examination of deceptive self-presentation in online dating profiles.
Personalityand Social Psychology Bulletin, 34(8):1023?1036.Q.
Xu and H. Zhao.
2012.
Using deep linguistic fea-tures for finding deceptive opinion spam.
In Pro-ceedings of COLING 2012: Posters, pages 1341?1350, Mumbai, India, December.
The COLING2012 Organizing Committee.H.
Zhang, S. Wei, H. Tan, and J. Zheng.
2009.
Decep-tion detection based on svm for chinese text in cmc.In Information Technology: New Generations, 2009.ITNG ?09.
Sixth International Conference on, pages481?486, April.L.
Zhou and D. Shi, Y.and Zhang.
2008.
A statisti-cal language modeling approach to online deceptiondetection.
IEEE Trans.
on Knowl.
and Data Eng.,20(8):1077?1081, August.L Zhou, D. Twitchell, T Qin, J. Burgoon, and J. Nuna-maker.
2003.
An exploratory study into decep-tion detection in text-based computer-mediated com-munication.
In Proceedings of the 36th AnnualHawaii International Conference on System Sci-ences (HICSS?03) - Track1 - Volume 1, HICSS ?03,pages 44.2?, Washington, DC, USA.
IEEE Com-puter Society.445
