Survey ArticleUnsupervised Learning of MorphologyHarald Hammarstr?m?Radboud Universiteit and Max PlanckInstitute for Evolutionary AnthropologyLars Borin?
?University of GothenburgThis article surveys work on Unsupervised Learning of Morphology.
We define UnsupervisedLearning of Morphology as the problem of inducing a description (of some kind, even if onlymorpheme segmentation) of how orthographic words are built up given only raw text data ofa language.
We briefly go through the history and motivation of this problem.
Next, over 200items of work are listed with a brief characterization, and the most important ideas in the fieldare critically discussed.
We summarize the achievements so far and give pointers for futuredevelopments.1.
IntroductionMorphology is understood here in its usual sense in linguistics, namely, as referring to(the linguistic study and description of) the internal structure of words.
More specifi-cally, we understand morphology following Haspelmath (2002, page 2) as ?the studyof systematic covariation in the form and meaning of words.?
For our purposes, weassume that we have a way of identifying the text words of a language, ignoring thefact that the term word has eluded exhaustive cross-linguistic definition.
Similarly, weassume a number of commonly made distinctions in linguistic morphology, whose basicimport is indisputable, but where there is an ongoing discussion on exactly where todraw the boundaries with respect to particular phenomena in individual languages.Generally, a distinction is made between inflectional morphology and word forma-tion.
Inflectional morphology deals with the various realizations of the ?same?
lexicalword, depending on the particular syntactic context in which the word appears.
Typicalexamples of inflection are verbs agreeing with one or more of their arguments in theclause, or nouns inflected in particular case forms in order to show their syntactic rela-tion to other words in the phrase or clause, for example, showing which verb argumentthey express.
Word formation deals with the creation of new lexical words from existing?
Centre for Language Studies, Radboud Universiteit, Postbus 9103, 6500 HD Nijmegen, The Netherlands/Department of Linguistics, Max Planck Institute for Evolutionary Anthropology, Deutscher Platz 6,04103 Leipzig, Germany.
E-mail: h.hammarstrom@let.ru.nl.??
Spr?kbanken, Department of Swedish Language, University of Gothenburg, Box 200, SE-405 30G?teborg, Sweden.
E-mail: lars.borin@svenska.gu.se.Submission received: 2 March 2010; revised submission received: 19 August 2010; accepted for publication:4 October 2010.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 2ones, for example, agent nouns from verbs.
If the same kinds of mechanisms are used asin inflectional morphology (i.e., the resulting word is derived out of only one existingword), linguists talk about derivational morphology.
If two or more existing lexicalwords are combined in order to make up a new word, the terms compounding orincorporation are used, depending on the categories of the words involved.There is a fairly wide array of formal means available cross-linguistically for ex-pressing inflectional and derivational categories in languages.
Most commonly, how-ever, some form of affixation is involved?that is, some phonological material is addedto the end of the word (suffixation), to the beginning of the word (prefixation), or(much more rarely) inside the stem of the word (infixation).
Suffixes and prefixes (butrarely infixes) can form long chains, where the different positions, or ?slots,?
expressdifferent kinds of inflectional or derivational categories.
If a language has suffixingand/or prefixing?sometimes called concatenative morphology?it obviously followsthat text words in that language can be segmented into a sequence of morphologicalelements: a stem and a number of suffixes after the stem and/or prefixes before thestem.1Morphology is one of the oldest linguistic subdisciplines, and this brief presenta-tion by necessity omits many intricacies and greatly simplifies a vast scholarship.
(Forstandard, in-depth, introductions to this fascinating field, see, e.g., Nida [1949], Jensen[1990], Spencer and Zwicky [1998], or Haspelmath [2002].
)In language technology applications, a morphological component forms a bridgebetween texts and structured information about the vocabulary of a language.
Somekind of morphological analysis and/or generation thus forms a basic component inmany natural language processing applications.
Many languages have quite complexmorphological systems, with the number of potential inflected forms of a single lex-ical word running into the thousands, requiring a substantial amount of work if thelinguistic knowledge of the morphological component is to be defined manually.
Forthis reason, researchers often turn to machine learning approaches.
This survey articleis concerned with unsupervised approaches to morphology learning.For the purposes of the present survey, we use the following definition of Un-supervised Learning of Morphology (ULM).Input: Raw (unannotated, non-selective2) natural language text dataOutput: A description of the morphological structure (there are various levels to bedistinguished; see subsequent discussion) of the language of the input textWith: As little supervision (parameters, thresholds, human intervention, modelselection during development, etc.)
as possibleSome approaches have explicit or implicit biases towards certain kinds of lan-guages; they are nevertheless considered to be ULM for this survey.
Morphology maybe narrowly taken as to include only derivational and inflectional affixation, where thenumber of affixes a root may take is finite3 and the order of the affixes may not be1 The picture is less simple in reality, because affixation is often accompanied by so-calledmorphophonological changes?changes in the shape of the stem or affix involved, or both?which oftenhave the effect of blurring the boundaries between the elements.2 With the term non-selective we intend to exclude text data that requires manual selection (e.g., curatedsingular?plural pairs).3 The number of inflectional affixes is finite by definition.
The derivational affixes?especially in heavilyagglutinating languages?may be recursive, but are in practice finite.310Hammarstr?m and Borin Unsupervised Learning of Morphologypermuted.4 This survey also subsumes attempts that take a broader view includingclitics5 and compounding (and there seems to be no reason in principle to excludeincorporation and lexical affixes: see Mithun [1999], pages 37?67, for some examples).Many, but not all, approaches focus on concatenative morphology/compounding only.All works considered in this survey are designed to function on orthographicwords, that is, raw text data in an orthography that provides a ready-made segmen-tation of text into words.
Crucially, this excludes the rather large body of work thatonly targets word segmentation, that is, segmenting a sentence or a full utterance intowords (cf.
Goldsmith [2010] who also overviews word segmentation).
However, worksthat explicitly aim to treat both word segmentation and morpheme segmentation inone algorithm are included.
Hence, subsequent uses of the term segmentation in thepresent survey are to be understood as morpheme segmentation rather than wordsegmentation.
We prefer the term segmentation to analysis because, in general in ULM,the algorithm does not attempt to label the segments.There have been other approaches to machine learning of morphology than pureULM as defined here, the most popular ones being: approaches that require selective input, such as ?singular?plural pairs,?or ?all members of a paradigm?
(Garvin 1967; Klein and Dennison 1976;Golding and Thompson 1985; Wothke 1985; McClelland and Rumelhart1986; Brasington, Jones, and Biggs 1988; Tufis 1989; Zhang and Kim 1990;Borin 1991; Theron and Cloete 1997; Oflazer, McShane, and Nirenburg2001, for example) approaches where some (small) amount of annotated data, some(small) amount of existing rule sets, or resources such as a machine-readable dictionary or a parallel corpus, are mandatory (Yarowsky andWicentowski 2000; Yarowsky, Ngai, and Wicentowski 2001; Cucerzan andYarowsky 2002; Neuvel and Fulop 2002; Johnson and Martin 2003; Rogati,McCarley, and Yang 2003, for example)Such approaches are excluded from the present survey, unless the required data (e.g.,paradigm members) are extracted from raw text in an unsupervised manner as well.We also exclude the special case of the second approach where morphology learningmeans not ?learning the morphological system of a language,?
but rather ?learning theinflectional classes of out-of-vocabulary words,?
namely, approaches where an existingmorphological analysis component is used as the basis for guessing in which existingparadigm an unknown text word should belong (e.g., Antworth 1990; Mikheev 1997;Bharati et al 2001; Forsberg, Hammarstr?m, and Ranta 2006; Lind?n 2008; Lind?n 2009).One of the matters that varies the most between different authors is the desiredoutcome.
It is useful to set up the implicational hierarchy shown in Table 1 (which4 There are, however, rare cases of languages which allow the permutation of specific pairs of prefixes,such as Kagulu (Petzell 2007), Yimas and Karawari (Foley 1991, pages 31?32) as well as Chintang,Bantawa, and possibly other Kiranti languages where prefix ordering in general is very free (Rai 1984;Bickel et al 2007).5 Clitics are affix-like elements that attach to words in particular syntactic positions, rather than to wordsof particular categories as proper affixes do.
The English genitive -?s is sometimes classified as a clitic,because you can say things like The girl I met yesterday?s purse (the -?s attaches to the end of the nounphrase, regardless of the part of speech of the last word, an adverb in this case).
This could not happenwith an inflectional suffix like the plural -s: *The girl I met yesterdays cannot mean The girls I met yesterday.311Computational Linguistics Volume 37, Number 2Table 1Levels of power of morphological analysis.Form MeaningAffix list A list of the affixes?Same-stem decision Given two words, decide ifthey are affixations of thesame stemGiven two words, decide ifthey are affixations of thesame lexeme?Segmentation Given a word, segment itinto stem and affix(es)Morphological analysis A functional labeling for theaffixes in the segmentation?Inflection tables A list of the affixationpossibilities for all stemsParadigm list A list of the paradigms forall stem types, completewith functional labels forparadigm slots?Lexicon+Paradigm A list of the paradigms and a list of all stems with informationof which paradigm each stem belongs to?Justification A linguistically and methodologically informed motivationfor the morphological description of a languageneed of course not correspond to steps taken in an actual algorithm).
The divisionis implicational in the sense that if one can do the morphological analysis of a lowerlevel in the table, one can also easily produce the analysis of any of the levels above it.Reflecting a fundamental assumption underlying most ULM work, form and meaning(semantics) are kept separate in the table (see Section 2).
For example, if one can performsegmentation into stem and affixes, one can decide if two words are of the same stem(if meaning is disregarded) or the same lexeme (if meaning is taken into account).
Theconverse need not hold; it is perfectly possible to answer the question of whether twowords are of the same stem with high accuracy, without having to commit to what theactual stem should be.Many recent articles fail to deal properly with previous and related work, somereinvent heuristics that have been proposed earlier, and there is little modularizationtaking place.
Previous surveys and overviews for a general audience are Borin (1991),Batchelder (1997, pages 66?68), Powers (1998), Clark (2001, pages 80?82), Xanthos(2007, pages 95?107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat(2007, pages 116?136), Hammarstr?m (2007b, pages 10?15), Chan (2008, pages 48?60),Hammarstr?m (2009b, pages 14?21), Borin (2009), Goldsmith (2010), and, to a morelimited extent, the related-work sections of individual research papers.
Kurimo, Creutz,and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo andTurunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviewsof systems in the MorphoChallenge of the respective year.
However, we will try tobe more comprehensive than previous surveys and discuss the ideas in the fieldcritically.312Hammarstr?m and Borin Unsupervised Learning of MorphologyWe will not attempt a comparison in terms of accuracy figures as this is whollyimpossible, not only because of the great variation in goals but also because mostdescriptions do not specify their algorithm(s) in enough detail.
Fortunately, this aspect isbetter handled in controlled competitions, such as the Unsupervised Morpheme Analysis?MorphoChallenge6 which offers tasks of segmentation of Finnish, English, German,Arabic, and Turkish.2.
History and Motivation of ULMUsually and justifiedly, the work of Harris (1955, 1967) is given as the starting pointof ULM.
From another perspective, however, the same work by Harris can be said toequally represent the culmination of an endeavor in the linguistic school of thoughtknown as American structuralism, to formalize the process of linguistic descriptioninto so-called linguistic discovery procedures.The variety of American structuralism which concerned itself most with the formal-ization of linguistic discovery procedures is often connected with the name of LeonardBloomfield, and its core tenet may be succinctly summed up in Bloomfield?s oft-quoteddictum: ?The only useful generalizations about language are inductive generalizations?
(Bloomfield 1933, page 20).
The so-called ?extremist Post-Bloomfieldians?
took thisprogram a step further: ?From Bloomfield?s justified insistence on formal, rather thansemantic, features as the starting-point for linguistic analysis, this group (especiallyHarris) set up as a theoretical aim the description of linguistic structure exclusively interms of distribution?
(Hall 1987, page 156).The earliest reason for interest in ULM was thus?at least in part?methodologicaland arguably even ideological, but not (unlike at least some of the later ULM work)motivated by, for example, a desire to simulate language acquisition in humans.More or less simultaneously with but independently of Harris, the Russian linguistAndreev launched a program much like that of Harris.7 Andreev?s work is much lessknown than that of Harris?s, and for this reason we will describe it in some detail here.In a series of publications (Andreev 1959, 1963, 1965b, 1967), he develops an ?algorithmfor statistical-combinatory modeling of languages.?
This is part of a research programwhich, just like that of Harris, aims at eliminating semantics and considerations ofmeaning completely from the process of ?discovery?
of language structure.Thus, Andreev claims to be able to go from unsegmented transcribed speech all theway up to syntax, using basically one and the same approach grounded in text (corpus)statistics.
Given our focus on ULM, here we will be concerned only with his approachas applied to morphological segmentation.Andreev?s approach is much more explicitly based in text statistics?and to someextent in language typology?than Harris?s work.
The algorithm for morphologicalsegmentation is described in some detail in the works of Andreev and his colleagues.It relies on statistics of letter frequencies in a text corpus, and of average word lengthin characters and average sentence length in words.
From these statistics he calculates anumber of heuristic thresholds which are used to iteratively grow affix candidates fromcharacters at given positions in text words, and paradigm candidates from the resultingsegmentations.
Instead of looking at successor/predecessor counts or transition proba-bilities, Andreev looks at character positions in relation to word edges, from the first and6 Web site http://www.cis.hut.fi/morphochallenge2009/ accessed 10 September 2009.7 To our knowledge, Andreev never refers to Harris?s work.313Computational Linguistics Volume 37, Number 2the last character inwards no further than the average word length.
At each position, theamount of overrepresentation is calculated for each character found in this position insome word.
The overrepresentation (?correlative function?
in Andreev?s terminology)is defined as the relative frequency of the character in this word position divided byits relative frequency in the corpus.
The character?position combinations are used inorder of decreasing overrepresentation in an iterative see-saw procedure, where affixand stem candidates are collected in alternating iterations of the algorithm.
Andreev?sapproach reflects the same intuition as that of Harris; we would expect word-edgesequences of highly overrepresented characters to be flanked by marked differencesin predecessor or successor counts calculated according to Harris?s method.A concrete example of how Andreev?s method works (with the finer details omit-ted) is the following, originally presented by Andreeva (1963), but the presentation hereis partly based on that in Andreev (1967).In a 900,000-word corpus of electronics texts in Russian, the most overrepresentedletter was <j> (Russian <i?>) in the last position of the word, where it is eight times asfrequent as in the corpus as a whole (Andreeva 1963, page 49).
For the words ending in<j>, its most overrepresented predecessor was <o>, and using some thresholds derivedfrom corpus statistics, the first affix candidate found was -oj (Russian -<oi?>).Removingthis ending from all words in which it appears and matching the remainders of thewords (i.e., putative stems) against the other words of the corpus, yields a set of wordsfrom which additional suffix candidates emerge (including the null suffix).
This set ofwords is then iteratively reduced, using the admissible suffix candidates (those belowa certain length exceeding a heuristic threshold of overrepresentation) in each step, aslong as at least two stem candidates remain.
In other words: There must be at least twostems in the corpus appearing with all the suffix candidates.
In the Russian experimentreported by Andreeva (1963), a complete adjective paradigm was induced, with 12different suffixes.
The initial suffix candidate, -oj, has a high functional load and conse-quently a high text frequency: It is the most ambiguous of the Russian adjective suffixes,appearing in four different slots in the adjective paradigm, and is also homonymouswith a noun suffix.In Andreev (1965b) the method is tested extensively on Russian, which is the subjectof several papers in the volume, and a number of other languages: Albanian (Per?ikov1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (O?igova1965), English (Malahovskij 1965), Estonian (Hol?m 1965), French (Kordi 1965), German(Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965),and Vietnamese (Jaku?eva 1965).
As an aside, we may note that only after the turn ofthe millennium are we again seeing this variety of languages in ULM work.
Most ofthese studies are small-scale proof-of-concept experiments on corpora of varying sizes(from a few thousand words in many of the studies up to close to one million words forRussian).
The outcomes are more often than not quite small ?paradigmoid fragments,?that is, incomplete and not always in correspondence with traditional segmentations.It is noteworthy, however, that the method could not produce a single instance ofmorphological segmentation for Vietnamese (Jaku?eva 1965, page 228), which is as itshould be, because Vietnamese is often held forth as a language without morphology.The papers describing these experiments are short, and it is not always clear exactlywhat has been done.
In fact, computers are not mentioned at all in most of the papers;on the contrary, it is quite clear that at least some of the experiments have been carriedout manually.
In principle, because Andreev and the other authors in Andreev (1965b)describe the procedure in great detail, it should be possible to replicate some of the314Hammarstr?m and Borin Unsupervised Learning of Morphologyfindings (cf.
Altmann and Lehfeldt 1980, pages 195?198).
To our knowledge, therehas been one attempt to do this, by Cromm (1997), who reimplemented the methodand tested it on the German Bible, experimenting with various parameter settingsand also making some changes to the method itself.
He notes that several parametersthat Andreev provides mostly without motivation or comment in fact can be changedin a more accepting direction, leading to much increased recall without much lossin precision.
Unfortunately, however, in his short paper, Cromm does not provideenough information about the algorithm or the changes that he made to it, so that theRussian original is still the only publicly available source for the details of Andreev?sapproach.A very different, more practically oriented, motivation for ULM came in the 1980s,beginning with the supervised morphology learning ideas by Wothke (1985, 1986) andKlenk (1985a, 1985b) which later led to partly unsupervised methods (see the following).Because full natural language lexica, at the time, were too big to fit in working memory,these authors were looking for a way to analyze or stem running words in a ?nicht-lexikalisches?
manner, that is, without the storage and use of a large lexicon.
Thismotivation is now obsolete.The interest in purebred ULM was fairly low until about 1990, however, with onlya few works appearing between the mid 1960s and 1990.
Especially in the 1980s, thefocus in computational morphology was on the development of finite-state approacheswith hand-written rules, but in the course of the following decade, interest in ULMrose greatly, in the wake of a general increased attention during the 1990s to statisticaland information-theoretically informed approaches in natural language processing.In speech processing, the problem of word segmentation is ever-present, and as thecomputational tools for taking on this problem became increasingly sophisticated andincreasingly available not least as the result of a general development of computinghardware and software, researchers in linguistics and computational linguistics startedtaking a fresh look at the problems of word segmentation and ULM.The work of Goldsmith (2000, 2001, 2006) represents a kind of focal point here.
Hepulls together a number of strands from earlier work, sets them against a theoreticalbackground informed both by information theory (MDL) and linguistics, and usesthem specifically to address the problem of ULM?in particular, unsupervised learningof inflectional morphology?and not, for instance, that of word segmentation or ofstemming for information retrieval, and so forth.Further, there has been the idea that ULM could contribute to various open ques-tions in the field of first-language acquisition (see, e.g., Brent, Murthy, and Lundberg1995; Batchelder 1997; Brent 1999; Clark 2001; Goldwater 2007).
However, the connec-tion is still rather vague and even if ULM has matured, it is not clear what implications,if any, this has for child language acquisition.
Children have access to semantics andpragmatics, not just text strings, and it would be very surprising if such cues were notused at all in first language acquisition.
Further, if some ULM technique was shown tobe successful on some reasonably sized corpora, it does not automatically follow thatchildren can (and do, if they can) use the same technique.
Most current ULM techniquescrucially involve long series of number crunching that seem implausible for the child-learning setting.After the turn of the century, ULM has become something of a growth industryin language technology.
There are several reasons for this.
One obvious reason is agenerally increased interest in machine learning, both theoretically (as a research areainteresting in itself and as a possible tool for modeling human language acquisitionand language learning) and for pragmatic reasons, as a way to reduce the manual work315Computational Linguistics Volume 37, Number 2involved in the construction of the lexical and grammatical knowledge bases needed forthe realization of sophisticated language technology applications.8Another reason has to do with the acceptance of the world as multilingual and theunderstanding that language communities are very unequally endowed with languagetechnology resources.
There are on the order of 7,000 languages spoken in the worldtoday (Lewis 2009).
Their size in number of first-language speakers is very unevenlydistributed.
The top 30 languages in the world account for more than 60% of its popu-lation.
At the other end of the scale, we find that most languages are spoken by quitesmall communities:There are close to 7,000 languages in the world, and half of them have fewer than 7,000speakers each, less than a village.
What is more, 80% of the world?s languages havefewer than 100,000 speakers, the size of a small town.
(Ostler 2008, page 2)On the whole, small language communities will tend to have correspondingly smallfinancial and other resources that could be spent on the development of languagetechnology, but the cost of, for example, constructing a lexicon or a parser for a lan-guage is more or less constant, and not proportional to the number of speakers of thelanguage.At the same time, it has been observed over and over again that the use or non-useof a language in a particular situation?where the language could in principle be used,but where there is a choice available between two or more languages?is intimatelyconnected with the attitudes towards the language among the participants.
This isperhaps the most reliable determiner of language use, and not factors such as effort,lack of vocabulary, and so on, which in many cases seem to be post hoc rationalizationsmotivating a choice made on attitudinal grounds.
Another way of expressing this is thatlanguages are more or less prestigious in the eyes of their speakers, and that linguisticinferiority complexes seem to be common in the world.However, rather than taking status as an inherent and immutable characteristic of alanguage, we should see it for what it is, namely, a perceived characteristic, somethingthat lies in the eye of the beholder.
As such, it can be influenced by human action.Important for our purposes here is that it has been suggested that making availablemodern information and communication technologies for a language, including thecreation of linguistic resources and language technology for it, may serve to raise itsstatus (see, e.g., the papers in Saxena and Borin 2006).This, then, is another reason for pursuing ULM: to be able to provide languagetechnology to language communities lacking the requisite resources.
However, ULM,at least as understood for the purposes of this survey, requires a written language,which would still exclude a substantial majority of the world?s languages (Borin 2009).Note that the remainder?languages with a tradition of writing?are not on the wholesmall language communities; in the first instance, we are talking about the few hundredmost spoken languages in the world, for example, the 313 languages with at least onemillion native speakers (accounting for about 80% of the world?s population) surveyedby the Linguistic Data Consortium some years back in their Low-density language survey(Strassel, Maxwell, and Cieri 2003; Borin 2009).8 Another pragmatic, less savory reason is a general downplaying of linguistic knowledge in the languagetechnology research community (Reiter 2007).316Hammarstr?m and Borin Unsupervised Learning of MorphologyThe hope is often expressed in the literature that ULM and other unsupervisedmethods could be employed in order to rapidly and cheaply (in terms of human effort)bootstrap basic language technology resources for new languages.It should be noted that, even for larger languages, because of the human effortneeded to build computational morphological resources, many such implementationsare not released to the public domain.
Also, open domain texts will always contain afair share of (inflected) previously unknown words that are not in the lexicon.
Therehas to be strategy for such out-of-dictionary words?a ULM-solving algorithm is onepossibility.
The ULM problem as specified, therefore, still has a role to play for largerlanguages.Finally, and closely related to the preceding reason, ULM and other kinds of ma-chine learning of linguistic information are increasingly seen as providing potentialtools in language documentation.9It has been realized for some time that languages are disappearing at a rapid ratein the modern world (Krauss 1992, 2007).
Many linguists see this loss of linguisticdiversity as a disaster in the cultural and intellectual sphere on a par with the loss ofthe world?s biodiversity in the ecological sphere, only on a grander scale; languagesare going extinct more rapidly than species.
Enter language documentation (Gippert,Himmelmann, and Mosel 2006), which is construed as going well beyond traditionaldescriptive linguistic fieldwork, aspiring as it does to capture all aspects?linguistic,cultural, and social?of a language community?s day-to-day life, in video and audiorecordings of a wide range of sociocultural activities, in still images, and in representa-tive artifacts.
Basic linguistic descriptions of lexicon and grammar made on the basis oftranscribed recordings still form an important component of language documentation,however, and with the realization that languages are disappearing at a far faster ratethan linguists can document them, it is natural to look for ways of making this processless labor-intensive.10In summary, we have seen the following motivations for ULM (in chronologicalorder): Linguistic theory Elimination of the lexicon Child language acquisition Morphological engine bootstrapping Language description and documentation bootstrappingAs noted, the motivation of eliminating the lexicon is now obsolete, whereas the othersare active to various degrees.
By far the most popular motivation has been, and still is,that of inducing a morphological analyzer/segmentation from raw text data (with littlehuman intervention) in a well-described language.
However, as we have argued herein,the timing is right for the momentum to carry over also to under-described languages.9 To our knowledge, Eguchi (1987, page 168) is the first author to suggest ULM as one of severalcomputational aids to the language documentation fieldworker.10 For example, in the instructions for the recent large-scale language documentation effort BOLD:PNG?Basic Oral Language Documentation: Papua New Guinea?we read: ?Try not to spend more than an hourtranscribing a minute of text.?
and ?As before, try not to spend more than an hour translating a minuteof text.?
www.boldpng.info/bold/stage3.317Computational Linguistics Volume 37, Number 23.
Trends and Techniques in ULM3.1 Roadmap and Synopsis of Earlier StudiesA chronological listing of earlier work (with very short characterizations) is given inTable 2.
Several papers are co-indexed if they represent essentially the same line of workby essentially the same author(s).Given the number of algorithms proposed, it is impossible to go through the tech-niques and ideas individually.
However, we will attempt to cover the main trends andlook at some key questions in more detail.The problem has been approached in four fundamentally different ways, which wemay summarize in the following way.
(a) Border and Frequency: In this family of methods, if a substring occurswith a variety of substrings immediately adjacent to it, this is interpretedas evidence for a segmentation border.
In addition, frequent or somehowoverrepresented substrings are given a direct interpretation as candidatesfor segmentation.
A typical implementation is to subject the data to acompression formula of some kind, where frequent long substrings withclear borders offer the optimal compression gain.
The outcome of such acompression scheme gives the segmentation.
In addition, for thoseapproaches which also target paradigms, stem?suffix co-occurrencestatistics are gathered given the segmentation produced, rather than allpossible segmentations.
(b) Group and Abstract: In this family of methods, morphologically relatedwords are first grouped (clustered into sets, paired, shortlisted, etc.
)according to some metric, which is typically string edit distance, but mayinclude semantic features (Schone 2001), distributional similarity (Freitag2005), or frequency signatures (Wicentowski 2002).
The next step is toabstract some morphological pattern that recurs among the groups.
Suchemergent patterns provide enough clues for segmentation and cansometimes be formulated as rules or morphological paradigms.
(c) Features and Classes: In this family of methods, a word is seen as madeup of a set of features?n-grams in Mayfield and McNamee (2003) andMcNamee and Mayfield (2007), and initial/terminal/mid-substring inDe Pauw and Wagacha (2007).
Features which occur on many words havelittle selective power across the words, whereas features which occurseldom pinpoint a specific word or stem.
To formalize this intuition,Mayfield and McNamee and McNamee and Mayfield use TF-IDF, andDe Pauw and Wagacha use entropy.
Classifying an unseen word reducesto using its features to select which word(s) it may be morphologicallyrelated to.
This decides whether the unseen word is a morphologicalvariant of some other word, and allows extracting the ?variation?
bywhich they are related, such as an affix.
(d) Phonological Categories and Separation: In this family of methods, thephonemes (approximated by graphemes) are first classed into categories,foremostly, vowel versus consonant.
Thereafter, each word is separatedinto its vowel skeleton and its consonant skeleton, after which various318Hammarstr?m and Borin Unsupervised Learning of MorphologyTable 2Very brief roadmap of earlier studies.Model Superv.
Experimentation Learns what?Harris 1955, 1968, 1970 C T English SegmentationAndreev 1965a, Andreev 1967,Chapter 2, Per?ikov 1965;Melkumjan 1965; Fedulova 1965;O?igova 1965; Malahovskij 1965;Hol?m 1965; Kordi 1965; Fitialova1965; Fihman 1965a; Andreev1965a; Jakubajtis 1965; Panina1965; Fihman 1965b; Eliseeva1965; Jaku?eva 1965C T Vietnamese toHungarian (I)SegmentationGammon 1969 C T English SegmentationLehmann 1973, pages 71?93 C T French (I) Segmentationde Kock and Bossaert 1969, 1974,1978C T French/Spanish Lexicon+ ParadigmsFaulk and Gustavson 1990 C T English (I) SegmentationHafer and Weiss 1974 C T English (IR) SegmentationKlenk and Langer 1989 C T+SP German SegmentationLanger 1991 C T+SP German SegmentationRedlich 1993 C T English (I) SegmentationKlenk 1992, 1991 C T+SP Spanish SegmentationFlenner 1992, 1994, 1995 C T+SP Spanish SegmentationJan?en 1992 C T+SP French SegmentationJuola, Hall, and Boggs 1994 C T English SegmentationBrent 1993, 1999; Brent, Murthy,and Lundberg 1995; Snover 2002;Snover, Jarosz, and Brent 2002;Snover and Brent 2001, 2003C T English/Child-English/Polish/FrenchSegmentationDeligne and Bimbot 1997; Deligne1996C T English/French (I) SegmentationYvon 1996 C T French (I) SegmentationKazakov 1997; Kazakov andManandhar 1998, 2001C T French/English SegmentationJacquemin 1997 C T English SegmentationCromm 1997 C T German SegmentationGaussier 1999 C T French/English (I) Lexicon+ ParadigmsD?jean 1998a, 1998b C T Turkish/English/Korean/French/Swahili/Vietnamese (I)Affix ListsMedina Urrea 2000, 2003, 2006b C T Spanish Affix ListSchone and Jurafsky 2000, 2001a;Schone 2001C T English SegmentationGoldsmith 2000, 2001, 2006; Belkinand Goldsmith 2002; Goldsmith,Higgins, and Soglasnova 2001;Hu et al 2005b; Xanthos, Hu,and Goldsmith 2006C T English (I) Lexicon+ ParadigmsBaroni 2000, 2003 C T Child-English/EnglishAffix ListCho and Han 2002 C T Korean SegmentationSharma, Kalita, and Das 2002, 2003;Sharma and Das 2002C T Assamese Lexicon+ ParadigmsBaroni, Matiasek, and Trost 2002 C/NC T English/German (I) Related word pairsBati 2002 C/NC T Amharic Lexicon+ ParadigmsCreutz 2003, 2006; Creutz andLagus 2002, 2004, 2005a, 2005b,2005c, 2007; Creutz, Lagus, andVirpioja 2005; Hirsim?ki et al2003; Creutz et al 2005C T Finnish/Turkish/EnglishSegmentation319Computational Linguistics Volume 37, Number 2Table 2(continued)Model Superv.
Experimentation Learns what?Kontorovich, Don, and Singer 2003 C T English SegmentationMedina Urrea and D?az 2003;Medina-Urrea 2006a, 2008C T Chuj/Ral?muri/Czech Affix ListMayfield and McNamee 2003;McNamee and Mayfield 2007- - 8 West Europeanlanguages (IR)Same-stemZweigenbaum, Hadouche, andGrabar 2003; Hadouche 2002C T Medical French SegmentationPirrelli et al 2004; Pirrelli andHerreros 2007; Calderone 2008C T Italian/English/Arabic UnclearJohnson and Martin 2003b C T Inuktitut UnclearKatrenko 2004 C T Ukrainian Lexicon+ ParadigmsC?avar et al 2004a, 2004b; C?avar,Rodrigues, and Schrementi 2006;C?avar et al 2006C T Child-English UnclearRodrigues and C?avar 2005, 2007 NC T Arabic SegmentationMonson 2004, 2009; Monson et al2004, 2007a, 2007b, 2008, 2008a,2008bC T English/Spanish/Mapudungun (I)SegmentationYarowsky and Wicentowski 2000;Wicentowski 2002, 2004C/NC T 30-ish mostly Europeantype languagesSegmentation +Rewrite RulesGelbukh, Alexandrov, and Han 2004 C - English SegmentationArgamon et al 2004 C T English SegmentationGoldsmith et al 2005; Hu et al 2005a C/NC T Unclear UnclearBacchin, Ferro, and Melucci 2005,2002a, 2002b; Nunzio et al 2004C T Italian/English SegmentationOliver 2004, Chapter 4?5 C T Catalan ParadigmsBordag 2005a, 2005b, 2007, 2008 C T English/German SegmentationHammarstr?m 2005, 2006a, 2006b,2007b, 2009a, 2009bC - Maori to Warlpiri Same-stemBernhard 2005a, 2005b, 2006, 2007,2008C T Finnish/Turkish/English Segmentation+Related setsof wordsKeshava and Pitler 2005 C T Finnish/Turkish/English SegmentationJohnsen 2005 C T Finnish/Turkish/English SegmentationAtwell and Roberts 2005 C T Finnish/Turkish/English SegmentationDang and Choudri 2005 C T Finnish/Turkish/English Segmentationur Rehman and Hussain 2005 C T Finnish/Turkish/English SegmentationJordan, Healy, and Keselj 2005, 2006 C T Finnish/Turkish/English SegmentationGoldwater, Griffiths, and Johnson2005; Goldwater 2007;Naradowsky and Goldwater 2009C T English/Child-English SegmentationFreitag 2005 C T English SegmentationGolcher 2006 C - English/German Lexicon+ ParadigmsArabsorkhi and Shamsfard 2006 C T Persian SegmentationChan 2006, Chan 2008,pages 101?139C T English ParadigmsDemberg 2007 C/NC T English/German/Finnish/TurkishSegmentationDasgupta and Ng 2006, 2007a;2007b; Dasgupta 2007C T Bengali SegmentationDe Pauw and Wagacha 2007 C/NC T Gikuyu SegmentationTepper 2007; Tepper and Xia 2008 C/NC T+RR English/Turkish AnalysisXanthos 2007 NC T Arabic Lexicon+ ParadigmsMajumder et al 2007;Majumder, Mitra, and Pal2008C T French/Bengali/French/Bulgar-ian/HungarianAnalysisZeman 2008, 2009 C - Czech/English/German/FinnishSegmentation+ParadigmsKohonen, Virpioja, and Klami2008C T Finnish/Turkish/English Segmentation320Hammarstr?m and Borin Unsupervised Learning of MorphologyTable 2(continued)Model Superv.
Experimentation Learns what?Goodman 2008 C T Finnish/Turkish/English SegmentationGol?nia 2008 C T Turkish/Russian SegmentationPandey and Siddiqui 2008 C T Hindi Segmentation+ParadigmsJohnson 2008 C T Sesotho SegmentationSnyder and Barzilay 2008 C/NC T Hebrew/Arabic/Aramaic/EnglishSegmentationSpiegler et al 2008 C T Zulu SegmentationMoon, Erk, and Baldridge 2009 C T English/Uspanteko SegmentationPoon, Cherry, and Toutanova2009C T Arabic/Hebrew SegmentationAbbreviations: C = Concatenative; I = Impressionistic evaluation; IR = Evaluation only in terms of Information RetrievalPerformance; NC = Non-concatenative; RR = Hand-written rewrite rules; SP = Some manually curated segmentation points;T = Thresholds and Parameters to be set by a human.frequency techniques reminiscent of those of the (a) approaches canbe applied.
This strategy is targeted towards the special kind ofnon-concatenative morphology called intercalated morphology11 withthe observation that, empirically, in those (relatively few) languageswhich have intercalated morphology, it does seem to depend onvowel/consonant considerations.
In Xanthos (2007), the phonologicalcategories are inferred in an unsupervised manner (cf.
Goldsmith andXanthos 2009) whereas in Bati (2002) and Rodrigues and C?avar (2005,2007) they are seen as given by the writing system.The first two, (a) and (b), enjoy a fair amount of popularity in the reviewed collection ofwork, though (a) is much more common and was the only kind used up to about 1997.The last two, (c) and (d), have been utilized only by the sets of authors cited therein.Let us now look at some salient questions in more detail.
The following notationwill be used in formal statements: w, s, b, x, y, .
.
.
?
??
: lowercase-letter variables range over strings of somealphabet ?
and are variously called words, segments, strings, and so forth. W, S, .
.
.
?
??
: capital-letter variables range over sets ofwords/strings/segments. C, .
.
.
: capital-letter caligraphic variables range over multisets ofwords/strings/segments. | ?
|: is overloaded to denote both the length of a string and the cardinalityof a set. w[i]: denotes the character at position i in the string w. For example, ifw = hello then w[1] = h. w[i : j]: denotes the segment from position i to j (inclusive) of the string w.For example, if w = hello then w[1 : |w|] = hello.11 Also known as templatic morphology or root-and-pattern morphology.321Computational Linguistics Volume 37, Number 2 WC is used to denote the set of words in a corpus C. f pW (x) = |{z|xz ?
W}|: the (prefix) frequency of x, that is, the number ofwords in W with initial segment x. f sW (x) = |{z|zx ?
W}|: the (suffix) frequency of x, that is, the number ofwords in W with final segment x.Subscript letters are dropped when understood from the context.3.2 Border and Frequency Methods3.2.1 Letter Successor Varieties.
Most (if not all) authors trace the inspiration for theirborder heuristics back to Harris (1955).
In fact, Harris defines a family of heuristics,all based on letter successor/predecessor varieties.
They were originally presented asapplying to utterances made up of phoneme sequences (Harris 1955), but they applyjust the same to words, namely, grapheme sequences (Harris 1970).
The basic countingstrategy, labelled letter successor varieties (LSV) by Hafer and Weiss (1974), is as follows.Given a set of words W, the letter successor variety of a string x of length i is defined asthe number of distinct letters that occupy the i + 1st position in words that begin with xin W:LSV(x) = |{z[|x| + 1]|z = xy ?
W}|Table 3 shows an example of a letter successor count on a tiny contrived wordlist.We may define the letter predecessor variety (LPV) analogously.
For a given suffixx, the LPV(x) is the number of distinct letters that occupy the position immediatelypreceding x in the words of W that end in x. LSV/LPV counts for an example word areshown in Table 4.It should be noted that Harris (1955, page 192, footnote 4) explicitly targets thevariety in letter successors types (i.e., is only interested in which letters ever occur in thesuccessor position, as opposed to being interested in their frequencies).
For example, ifthere are two different letters occurring in successor position, one occurring a thousandtimes and the other once, Harris?s letter successor variety is still two?the same as ifthe two letters occurred once each.
Subsequent authors have suggested that the fullfrequency distribution of the token letter successors carries a better signal of morphemeboundary.
After all, if there is a significant token frequency skewing, this suggeststhat we are in the middle of coherent morpheme.
Moreover, mere type counts maybe influenced by phonotactic constraints (consonant after vowel, etc.
), which come outless significant in token frequency counts (Goldsmith 2006, page 6).
Already the earliestTable 3Example of LSV-counts for some example prefixes (bottom) based on a small example wordlist (top).W = {abide, able, abode, and, art, at, bat}x a ab abe .
.
.
{z|z = xy ?
W} {abide, able, abode, and, art, at} {abide, able, abode} ?
.
.
.LSV(x) 4 (b,n,r,t) 3 (i,l,o) 0 .
.
.322Hammarstr?m and Borin Unsupervised Learning of MorphologyTable 4LSV counts for d-, di-, dis-, .
.
.
, disturbance- and LPV counts for -e, -ce, -nce, .
.
.
, -disturbance.
Allfigures are computed on the Brown Corpus of English (Francis and Kucera 1964), using the27 letter alphabet [a ?
z] plus the apostrophe.
There are |W| = 42,353 word types in lowercase.LSV 13 20 21 6 1 1 3 1 1 1 1d i s t u r b a n c eLPV 0 1 1 1 1 1 1 19 6 12 25follow-ups to Harris (Gammon 1969; Hafer and Weiss 1974; Juola, Hall, and Boggs 1994)experiment with replacing the raw LSV/LPV counts with the entropy of the charactertoken distribution.
The character token distribution after a given segment can be seen asa probability distribution whose events are the characters of the alphabet.
The entropyof this probability distribution then measures how unpredictable the next character isafter a given segment.
In general, for a discrete random variable X with possible valuesx1, .
.
.
, xn, the expression for entropy takes the following form:H(X) = ?n?i=1p(xi) log2 p(xi)Thus, with alphabet ?, the letter successor entropy (LSE) for a prefix x is defined asLSE(x) = ??c?
?f p(xc)f p(x)log2f p(xc)f p(x)At least two authors (Golcher 2006; Hammarstr?m 2009b) have questioned entropy asthe appropriate measure for highlighting a morpheme boundary.
Entropy measureshow skewed the distribution is as a whole, that is, how deviant the most deviantmember is, in addition to the second member, the third, and so on.
If there is no mor-pheme boundary, the morpheme continues with (at least) one character.
So one deviant,highly predictable, character is necessary and sufficient to signal a non-break, and it isarguably irrelevant if there are second- and third-place, and so forth, highly predictablecharacters that also signal the absence of a morpheme boundary.
For example, thecharacter token distribution before -ng is shown in Table 5.
Obviously, the fact that ofthe 3,352 occurrences of -ng, 3,258 of them are preceded by -i-, says that the absence ofa morpheme boundary is highly likely.
Now, does it matter that also another 35 are -o-versus only 4 for -e-?
Entropy would also take into account the skewedness of -o- versus-e-, whereas for Hammarstr?m (2009b) and Golcher (2006) only the skewedness of themost skewed character (i.e., the character that potentially constitutes the morphemecontinuation) is interesting, in this example -i-.
Therefore, these approaches only use themaximally skewed character to predict the presence/absence of a morpheme boundary.The letter successor max-drop (LSM) for a prefix x is defined as the fraction not occupiedby its maximally skewed one-character continuation:LSM(x) = 1 ?
maxc?
?f p(xc)f p(x)323Computational Linguistics Volume 37, Number 2Table 5The character token distributon for the character immediately preceding -ng, computed on theBrown Corpus of English (Francis and Kucera 1964).-ng 3,352-n- 1 -l- l -h- l -e- 4 -u- 26 -a- 26 -o- 35 -i- 3,258Table 6Normalized LPV/LPE/LPM-scores for -e, -ce, -nce, .
.
.
, -disturbance.
All figures are computed onthe Brown Corpus of English (Francis and Kucera 1964), using the 27-letter alphabet [a ?
z] plusthe apostrophe.
There are |W| = 42, 353 word types in lowercase.d i s t u r b a n c eLPV 0.03 0.03 0.03 0.03 0.03 0.03 0.70 0.22 0.44 0.92LPE 0.0 0.0 0.0 0.0 0.0 0.0 0.74 0.28 0.38 0.81LPM 0.0 0.0 0.0 0.0 0.0 0.0 0.83 0.53 0.37 0.85Which one of LSV/LSE/LSM is the ?correct?
one?
The answer, of course, dependson one?s theory of affixation, for which the field has no single answer (see Section 3.6,subsequently).Empirically, however, the three measures are highly correlated.
To compare thethree, we normalize them to their maxima in order to get a ?border?
score ?
1.
Themaximum achievable LSV is the alphabet size, so the normalized LSV(x) = LSV(x)|?| .The maximum achievable LSE is a uniform distribution across the alphabet, so thenormalized LSE(x) =LSE(x)?|?|?
( 1|?| log21|?| ) .
The maximum achievable LSM is a uniformdistribution across the alphabet, so the normalized LSM(x) =LSM(x)1?
1|?|.
The predecessoranalogues LPV, LPE, LPM are obvious.
Table 6 shows an example word and itsnormalized predecessor scores of the three kinds.As in the example, the three different measures have nearly the same story to tellin general, at least for English.
For the three measures, Table 7 shows the Pearsonproduct-moment correlation coefficient between the LPH/LPE/LPM-values of all ter-minal segments, as well as the Pearson product-moment correlation coefficient betweenthe LPH/LPE/LPM-ranks of all terminal segments.
Most usages in the literature of theletter successor counts have been relative to other counts on the same language.
In suchcases, the rank correlations show that all three measures can be expected to have nearidentical effects.Table 7The Pearson product-moment correlation coefficient between LPH/LPE/LPM-values (r) andthe Pearson product-moment correlation coefficient between LPH/LPE/LPM-ranks (r-rank).All values are computed on the Brown Corpus of English (Francis and Kucera 1964), using the27-letter alphabet [a ?
z] plus the apostrophe.
There are |W| = 42, 353 word types in lowercase.LPH&LPE LPE&LPM LPM&LPHr 0.872 0.957 0.729r-rank 0.999 0.998 0.996324Hammarstr?m and Borin Unsupervised Learning of MorphologyA number of concrete ways to use LSV/LPVs for segmentation are suggested byHarris (1955) and Hafer and Weiss (1974); for instance:(a) Cutoff: By far the easiest way to segment a test word is first to pick somecutoff threshold k and then break the word wherever its successor (orpredecessor or both) variety reaches or exceeds k.(b) Peak and plateau: In the peak and plateau strategy, a cut in a word w ismade after a prefix x if and only if LSV(w[1 : |x| ?
1]) ?
LSV(x) ?LSV(w[1 : |x| + 1]); that is, if the successor count for x forms a local ?peak?or sits on a ?plateau?
of the LSV-sequence along the word.
(c) Complete word: A break is made after a word prefix (or before a wordsuffix) if that prefix (or suffix) is found to be a complete word in the corpusword list W.These and similar strategies have been discussed and evaluated in various settings inthe literature, and it is unlikely that any strategy based on LSV/LSE/LSM-counts alonewill produce high-precision results.
The example in Table 6 showing morpheme borderheuristics on a specific word illustrates the matter at heart.
Any intuitively plausibletheory of affixation should allow abundant combination of morphemes without respectto their phonological form, which predicts that high LSV/LSE/LSM values shouldemerge at morpheme boundaries.
However, there appears to be no reason why theconverse should hold?high LSV/LSE/LSM values could emerge in other places of theword as well.
Indeed, any frequent character at the end or beginning of a word mayalso be expected to show high LSV/LSE/LSM around it, such as the -e at the end ofdisturbance which has higher values than, for example, -ance.
Therefore, simply inferringthat high LSV/LSE/LSM values indicate a morpheme border is not a sound principle ingeneral.A different (but less successful, even when supervised) way to use character se-quence counts is that associated with Ursula Klenk and various colleagues (Klenk andLanger 1989; Klenk 1991, 1992; Langer 1991; Flenner 1992, 1994, 1995; Jan?en 1992).
Foreach character bigram c1c2, they record, with some supervision in the form of manualcuration, at what percentage there is a morpheme boundary before |c1c2, between c1|c2,after c1c2|, or none.
A new word can then be segmented by sliding a bigram windowand taking the split which satisfies the corresponding bigrams the best.
For example,given a word singing, if the window happens to be positioned at -gi- in the middle,the bigram splits ng|, g|i, and |in are relevant to deciding whether sing|ing is a goodsegmentation.
Exactly how to do the split by sliding the window and combining suchbigram split statistics is subject to a fair amount of discussion.
It became apparent,however, that the appropriateness of a bigram split is dependent on, for example, theposition in a word?-ed is likely at the end of a word, but hardly in any other position?and exception lists and cover-up rules had to be introduced, before the approach wasabandoned altogether.3.2.2 Frequency Heuristics.
For reasons just explained, most (if not all) recent authorsin the border-and-frequency tradition have incorporated another measure, comple-mentary to a morpheme border heuristic.
This measure is nearly always directly orindirectly related to frequency, that is, frequent segments of some kind are singled out.Frequency has been used in many different ways.
The simplest way is to look at theraw frequency of segments of any length, but, inevitably, this will sweep in any short325Computational Linguistics Volume 37, Number 2segment.
Indeed, better candidates for morphemic segmentation are segments whichare somehow overrepresented, that is, more frequent than random.
There are variousways to define this property as well, including the following.Overrepresentation as more-frequent-than-its-length: For a segment x of |x| charac-ters, it is overrepresented to the degree that it is more common than expectedfrom a segment of its length.
This applies to a segment in any position.f (x)|?||x|Overrepresentation as more-frequent-than-its-parts: For a segment x = c1c2 .
.
.
cn of ncharacters, it is overrepresented to the degree that it is more common than ex-pected from a co-occurrence of its parts.
This applies to a segment in any position.f (c1c2 .
.
.
cn)f (c1)f (c2) .
.
.
(cn)Overrepresentation as more-frequent-as-suffix: For a segment x, it is overrepresentedto the degree that its probability as a suffix is higher than in any other (non-final) position.
This applies to a segment in terminal position (but with obviousanalogues for other positions).With such measures, many authors have singled out affixes above a certain over-representation-value threshold or overrepresentation-rank threshold.Threshold values are unsatisfactory because typically there is no theory in whichto interpret them.
Although they may be set ad hoc with some success, such settingsdo not automatically generalize.
Such considerations have led many authors to devisecompression-inspired models for exploiting skewed frequencies.
In particular, severaldifferent sets of authors have invoked Minimum Description Length (MDL) as themotivation for a given formula to compress input data into a morphologically analyzedrepresentation.12The MDL principle is a general-purpose method of statistical inference.
It views thelearning/inference process as data compression: For a given set of hypotheses H anddata set D, we should try to find the hypothesis in H that compresses D most (Gr?nwald2007, pages 3?40).
Concretely, such a calculation can take the the following form.
If L(H)is the length, in bits, of the description of the hypothesis; and L(D|H) is the length, inbits, of the description of the data when encoded with the help of the hypothesis, thenMDL aims to minimize L(H) + L(D|H).In principle, all of the works that have invoked MDL in their ULM method act asfollows.
A particular way Q of describing morphological regularities is conceived thathas two components which we may call patterns P and data D. A coding scheme isdevised to describe any P and to describe any collection of actual words with somespecific P and D. A greedy search is done for a local minimum of the sum L(P) + L(D|P)to describe the set of words W (in some approaches) or the bag of word tokens C (in12 To our knowledge, Brent (1993) is the first author to do so for morphological segmentation.326Hammarstr?m and Borin Unsupervised Learning of Morphologyother approaches) of the input text data.13 To take one concrete example, Goldsmith?s(2006) particular way Q of describing morphological regularities is to allow for a list ofstems, a list of affixes, a list of signatures (structures indicating which stems may appearwith which affixes, i.e., a list of pointers to stems, and a list of pointers to suffixes).
Thesearch is then among different lists of stems, affixes, and signatures to see which is theshortest to account for the words of the corpus.
Further details of such coding schemesneed not concern us here, but for a range of options see, for example, Goldsmith (2001,2006), Xanthos, Hu, and Goldsmith (2006), Creutz and Lagus (2007), Argamon et al(2004), Arabsorkhi and Shamsfard (2006), C?avar et al (2004b), Baroni (2003), or Brent,Murthy, and Lundberg (1995).It should be noted that the label MDL, in at least the terminology of Gr?nwald(2007, pages 37?38), is infelicitous for such cases where the P, D-search is not amongdifferent description languages, but among varations within a fixed language Q. Forexample, in the stem-affixes-signatures way of description (a specific Q), the searchdoes not include other (possibly more parsimonious?)
ways of description that do notuse stems, affixes, or signatures at all.
For the MDL-label to apply with its full philo-sophical underpinnings, the scope must include any possible compression algorithm,namely, any Turing machine.
In this respect it is important to note that, compared to theschemes devised so far, Lempel-Ziv compression, another description language, shouldyield a superior compression (as, in fact, conceded by Baroni 2000, pages 146?147).MDL-inspired optimization schemes have achieved very competitive results in practice,however, and must be considered the leading paradigm to exploit skewed frequenciesfor morphological analysis.3.2.3 Paradigm Induction.
The next step after segmentation is to induce systematic alter-nation patterns, or (inflectional) paradigms,14 and this is usually done as an extensionof a border-and-frequency approach.
For purposes of ULM, a paradigm is typicallydefined as a maximally large set of affixes whose members systematically occur on anopen class of stems.
For a number of reasons, finding paradigms is a major challenge.The number of theoretically possible paradigms is exponential in the number of affixes(as paradigms are sets of affixes).
Paradigms do not need to be disjoint; in real languagesthey are typically not.
Rather, words in the same part of speech tend to share affixesacross paradigms (Carstairs 1983).
In addition, without any language-specific knowl-edge, basically the only evidence at hand is co-occurrence of stems and affixes (i.e.,when a word occurs in the corpus it evidences the co-occurrence of a [hypothetical]stem and suffix making up that word).
Paradigm induction would be an easy problemif all affixes that could legally appear on a word did appear on each such word in a rawtext corpus.
This is, as is well known, far from the case.
A typical corpus distributionis that a few lexemes appear very frequently but by far most lexemes appear once oronly a few times (Baayen 2001).
What this means for morphology is that most lexemeswill appear with only one or a minority of their possible affixes, even in languages withrelatively little morphology.
Of course, there is also the risk that some rare affix, forexample, the Classical Greek alternative medial 3p.
pl.
aorist imperative ending -???
?13 As most approaches define their task as capturing the set of legal morphological forms, their goal shouldbe to compress W, but see Goldwater (2007, pages 53?59) for arguments for compressing C.14 Note also that paradigm information can be fed back into the segmentation process in that some affixeswhich do poorly according to some paradigm-related measure (e.g., affixes that do not take part in manyparadigms) can be weeded out.327Computational Linguistics Volume 37, Number 2(Blomqvist and Jastrup 1998), may not appear at all even in a very large Classical Greekcorpus.More formally, consider a morphological paradigm (set of suffixes) P that is a trueparadigm according to linguistic analysis.
If k lexemes that are inflected according to Poccur in a corpus, each of the k lexemes will occur in 1 ?
i ?
|P| forms.
The number offorms i that a lexeme occurs in is likely not to be normally distributed.
Most lexemeswill occur in only one form, and only very few, if any, lexemes will occur in all |P|forms.
It appears that for most languages and most paradigms, the number of lexemesthat occur in i forms tends to decrease logarithmically in i (Chan 2008, pages 75?84).
Asan example, consider the three most common paradigms in Swedish and the frequencyof forms in Table 8.Works which have attempted nevertheless to tackle the matter of paradigms, atleast for languages with one-slot morphology, include Zeman 2008, 2009, Hammarstr?m(2009b), and Monson (2009).
They explicitly or implicitly make use of the following twoheuristics to narrow down the search space: Languages tend to have a small number of paradigms (where ?small?means fewer than 100 paradigms with at least 100 member stems each). Languages tend to have only small paradigms (where ?small?
meansfewer than 50), that is, the number of affixes in each paradigm is small.Agglutinative languages, which have several layers of affixes, can be saidto obey this generalization in the sense that each layer has few members,whereas conversely, the full paradigm achieves considerable sizecombinatorially.Although we know of no empirical evulation of them, in the impression of the presentauthors, the two heuristics appear to be cross-linguistically valid.Chan (2006) is an exceptionally clean study of inducing paradigms, assuming thatthe segmentation is already given.
The problem then takes the form of a matrix withTable 8The three most common paradigms in Swedish according to the SALDO lexicon andmorphological resources (Borin, Forsberg, and L?nngren 2008), as computed on the SUC 1.0corpus (Ejerhed and K?llgren 1997) of 55,000 word types.Adjective 1st decl Noun 3rd decl Verb 1st conj(e.g., gul ?yellow?)
(e.g., tid ?time?)
(e.g., lag- ?fix?
)-a 2022 -?
1619 -a 1001-?
1821 -en 1141 -ade 948-t 1572 -er 1072 -ar 883-e 221 -erna 583 -at 579-are 208 -s 310 -as 482-s 114 -ens 259 -ande 423-aste 90 -ernas 136 -ad 387-ast 46 -ers 40 -ades 273-as 39 -ats 207-es 13 -andes 5-ts 4 -ads 3-ares 1328Hammarstr?m and Borin Unsupervised Learning of Morphologystems on one axis and suffixes on the other axis.
Chan then makes use of knowntechniques from linear algebra, in particular Latent Dirichlet Allocation, to break the fullmatrix into smaller dense submatrices, which, when multiplied together, resemble thefull matrix.
There is only one humanly tuned threshold, namely, when to stop breakinginto smaller parts.3.3 Group and AbstractIn contrast to the methods that use a heuristic for finding morpheme boundaries, thegrouping methods are much less sensitive to continuous segments.
String edit distanceis the most straightforward metric for which to find pairs or sets of morphologicallyrelated words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone andJurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al 2005a; Bernhard 2006,pages 101?117; Bernhard 2007; Majumder et al 2007; Majumder, Mitra, and Pal 2008).In addition, as unsupervised methods for semantic clustering (e.g., Latent SemanticAnalysis) and distributional clustering became more mature, these could be included aswell (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002;Freitag 2005).
More remarkable, however, is that Yarowsky and Wicentowski (2000)and Wicentowski (2002, 2004) have shown that frequency signatures can also be usedto (heuristically) find morphologically related words.
The example they use is sangversus sing, whose relative frequency distribution in a corpus is 1,427/1,204 (or 1.19/1),whereas singed15 versus sing is 9/1,204 (Yarowsky and Wicentowski 2000, pages 209?210).
This way, sing can be heuristically said to be parallel to sang rather than singed,and indeed the distribution for singed versus singe (its true relative) is 9/2, that is, muchcloser to 1.Suppose now that groups of morphologically related words are somehow heuris-tically extracted.
For example, one group might be {play, player, played, playing} andanother might be {bark, barks, barked, barking}.
The next step would be to find what iscommon among several groups, not just one.
Abstracting morphological alternationsgiven a family of groups is a thorny issue.
For instance, Baroni, Matiasek, and Trost(2002) leave the matter largely in the exploration phase.
Wicentowski (2004) presents afinished theory based on constraining the abstraction to find patterns in terms of prefix,suffix, and stem alternations.The outstanding question for the group-and-abstract approaches, related not only togrouping but also to abstracting, is how to find one and the same morphological process(umlauting, adding a suffix, etc.)
that operates over a maximal number of groups.
Thesearch space is huge, considering not only the group space but also the large number ofpotential morphological processes itself.The group-and-abstract approaches are also characterized by the ubiquitous use ofad hoc thresholds.
However, there are clear advantages in that they are in principlecapable of handling non-concatenative morphology and in that issues of semantics (ofstems) are addressed from the beginning.The work by de Kock and Bossaert (1969, 1974, 1978), Yvon (1996), Medina Urrea(2003) and partly Moon, Erk, and Baldridge (2009) can favorably be seen as a mid-waybetween the border-and-frequency and group-and-abstract approaches as they rely on15 That is, the past tense of the verb singe.329Computational Linguistics Volume 37, Number 2Table 9Example feature values for the words ng??thi??
(I went) and tu?g??thi??
(we went) adapted fromDe Pauw and Wagacha (2007, page 1518).
B=-features describe a subset at the start of the wordform, E=-features indicate patterns at the end of the word, and I=-features describe patternsinside the word form.class featuresng??thi??
B=n B=ng B=ng??
B=ng?
?t B=ng?
?th B=ng?
?thi I=g I=g??
I=g?
?t I=g?
?th I=g??thiE=g??thi??
I=??
I=?
?t I=?
?th I=?
?thi E=??thi??
I=t I=th I=thi E=thi??
I=h I=hi E=hi?
?I=i E=i??tu?g??thi??
B=t B=tu?
B=tu?g B=tu?g??
B=tu?g?
?t B=tu?g?
?th B=tu?g?
?thi I=u?
I=u?g I=u?g??
I=u?g??tI=u?g?
?th I=u?g?
?thi E=u?g??thi??
I=g I=g??
I=g?
?t I=g?
?th I=g?
?thi E=g??thi??
I=??
I=??tI=?
?th I=?
?thi E=??thi??
I=t I=th I=thi E=thi??
I=h I=hi E=hi??
I=i E=i?
?sets of four members with a particular affixation arrangement (?squares?
),16 whoseexistence is governed much by the frequency of the affixes in question.3.4 Features and ClassesThe features-and-classes methods share with the group-and-abstract methods the virtueof not being tied to segmental morpheme choices.
As mentioned earlier, in this family ofmethods a word is seen as made up of a set of features which have no internal order?n-grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), andbeginning/terminal/internal segments in De Pauw and Wagacha (2007).For example, Table 9 shows two words and their features in Gi?ku?yu?, a tonal Bantulanguage of Kenya.
As designed by De Pauw and Wagacha (2007), initial (B=), middle(I=), or final (E=) segments of a given word constitute its features.
A majority of featuresenumerated this way will not be morphologically relevant, whereas a minority is.
Forexample, in this case, I=h is just an arbitrary character without morpheme status,whereas I=ng?
?thi happens to be equal to a stem.
The idea is that arbitrary features suchas I=h will be too common in the training data to provide a useful constraint, whereasa more specialized feature like I=ng?
?thi might indeed trigger useful morphologicalgeneralization properties.The input word list W thus transforms into a training set of word?feature pairs,which can be fed into a standard maximum entropy classifier.
The next step is, for eachword, to ask the classifier for the k closest classes, namely, words (which will include theword itself and k ?
1 others with significant feature overlap).
Clearly, such clusters maycapture relations that string-edit-distance clustering does not.
De Pauw and Wagacha16 Based on the famous Greenberg square, which is a concrete means of illustrating the minimalrequirement for postulating a paradigm: We need a minimum of two attested stems and two attestedsuffixes (or in the general case arbitrary morphological processes), where both stems must occur withboth suffixes:stemA+sx1 stemB+sx1stemA+sx2 stemB+sx2As it is used in linguistics, both the stems and the suffixes in the square must represent attestedform?meaning combinations.
This requirement is normally given up in the work reviewed here, whereonly the form of postulated stems and affixes is available, but not the meaning.
The Greenberg squaregoes back to the age-old linguistic notion of proportional analogy (Anttila 1977).330Hammarstr?m and Borin Unsupervised Learning of Morphology(2007, pages 1517?1518) further suggest how specific morphological information, suchas prefixes, tonal changes, etc., may be abstracted from such clusters.Clearly, feature-based methods provide an interesting new avenue for non-segmental and long-distance phenomena, but are so far largely unexplored and not freefrom thresholds and parameters.3.5 Phonological Categories and SeparationThese approaches specifically target the special kind of non-concatenative morphologycalled intercalated morphology (or templatic morphology or root-and-pattern mor-phology) famous mainly from Semitic languages, such as Arabic.
They start out byassuming that graphemes can be subdivided into those that take part in the root, andthose that take part in the pattern.
For the languages so far targeted, Arabic (Rodriguesand C?avar 2005, 2007; Xanthos 2007) and Amharic (Bati 2002), this is largely true, ora transcription is used where it is largely true.
Rodrigues and C?avar (2005, 2007) andBati (2002) hard-code the transition from the graphemic representation of a word toits (potential) root and pattern parts.
This can be said to constitute a strong languagespecific bias, tantamount to supervision.
Xanthos (2007), on the other hand, starts outonly by assuming that there exists a distinction between root and pattern graphemesand subsequently learns which graphemes are which.
See Goldsmith and Xanthos(2009) for an excellent survey on how to do this (something which falls under learn-ing phonological categories rather than morphology learning).
Basically, it is possibleonly because there are systematic combination constraints between different phonemes(approximated by graphemes); for example, vowels and consonants alternate in a verynon-random manner.Once each word is divided into its potential root and pattern, the morphologylearning problem is similar to morphology learning given roots and suffixes, that is,the typical model for learning concatenative morphology, where the task is to weed outnoise, to decide where patterns (?suffixes?)
start and end, which patterns are spurious,and so on.
All these authors who have addressed intercalated morphology use a variantof MDL (see the border-and-frequency techniques in Section 3.2).
The accuracy of ULMon languages with intercalated morphology appears to be similar to the accuracy onother languages (cf.
Section 4.3).3.6 General Strengths and WeaknessesA perhaps worrying tendency is that, despite extensive cross-citation, there is littletransfer between different groups of authors and there is a fair amount of duplicationof work.
The lack of a broadly accepted theoretical understanding is possibly relatedto this fact.
Few approaches have an abstract model of how words are formed, andthus cannot explain why (or why not) the heuristics employed fail, what kind of errorsare to be expected, and how the heuristics can be improved.
Nevertheless, a model forthe simplest kind of concatenative morphology is emerging, namely, that two sets ofrandom strings, B and S, combine in some way to form a set of words W. For Gelbukh,Alexandrov, and Han (2004), the segmentation task is to find minimal size |X| + |Y| suchthat W ?
{xy|x ?
X, y ?
Y}.
For example, if W = {ad, ae, bd, be, cd, ce}, then the minimalsize |X| + |Y| = 5 with X = {a, b, c} and Y = {d, e}.
For Bacchin, Ferro, and Melucci(2005) as well as in the word-segmentation version of Deligne (1996), the segmentationtask is to find a configuration of splits si for each wi = xiyi ?
W such that each xi and331Computational Linguistics Volume 37, Number 2yi occur in as many splits as possible.
More precisely, the product, over all words,of the number of splits for the parts x and y should be maximized.
Formally, let xiyibe the parts of wi induced by splits si and let p(x) = |{i|x = xi}| = |{wi|xyi = wi}| bethe number of words in which x equals the first part of the split and similarly letp?
(y) = |{i|y = yi}| = |{wi|xiy = wi}| be the number of words in which y equals the lastpart of the split.
Then the task is to find splits that maximize the following expression:arg max[s1,...,s|W|]?wi?Wp(xi) ?
p?
(yi)For example, if W = {ad, ae, bd, be, cd, ce, ggg}, then the configuration of splits a|d, a|e, b|d,b|e, c|d, c|e, g|gg yields the product (2 ?
3)6 ?
(1 ?
1).Brent (1999) devises a precise, but more elaborate, way of constructing W from Band S, but at the cost of a large search space, and whose global maximum is hard tocharacterize intuitively.
The same holds for the extension by Snover (2002).
Kontorovich,Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008),and Poon, Cherry, and Toutanova (2009) should also be noted for containing generativemodels.Most approaches, of any of the kinds (a)?
(d) described in Section 3.1, explicitly orimplicitly target languages which have (close to) one-slot morphology, that is, a word(or stem) typically takes not more than one prefix and not more than one suffix.
Many(indeed most; Dryer 2005) languages deviate more or less from this model.
At first,it may seem that multi-slot morphology can be handled by the same algorithms asone-slot morphology, by iterating the process used for one-slot morphology.
A decadeof ULM has shown that the matter is not so simple, because heuristics for one slotlanguages do not necessarily generalize to the outermost slot of a multi-slot language.The (c) and (d) approaches do not combine easily with the others but it is con-ceivable that the (a) and (b) type of approaches may be mutually enhancing.
Resultsfrom the (a) methods may serve to cut down the search space for the (b) methods,and the (b) methods may provide a way to circumvent thresholds for the (a) methods.There is also the possibility of serial combination where, for example, the (a) methodstarget concatenative morphology and the (b)?or (c)?methods attempt the remainingcases.
Presumably because most methods so far do not produce a clean, well-definedresult, various forms of hybridization of techniques by different authors have yet to besystematically explored.Lastly, there are scattered attempts to address morphophonological changes in aprincipled way, though so far these have been developed in close connection with aparticular segmentation method and target language (Schone 2001; Schone and Jurafsky2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepperand Xia 2008).4.
Discussion4.1 Language Dependence of ULMAs we mentioned in Section 3.6, most approaches have an explicit bias towards certainkinds of morphological systems, those for which we introduced the label ?one-slot mor-phology.?
This is of course not a problem, if the purpose is to bootstrap a morphologyfor some languages which happen to belong to the right type.
If the purpose is to saysomething about human language acquisition or language learning, or if the aim is to332Hammarstr?m and Borin Unsupervised Learning of Morphologydevise a method that should work with any language, such a bias naturally becomesproblematic.The two human learning analogues which have most frequently been proposedin the literature on ULM and other machine learning of morphology are those oflanguage acquisition and of linguistic analysis (e.g., as carried out as part of linguisticfieldwork).
Depending on which of the two we choose, the kinds of biases that we mayor may not allow become different.
Language acquisition in humans is oral (or sign, butfor practical reasons, we are leaving sign languages completely out of the discussionhere), so expecting written input with word delimiters would then be an inadmissiblebias.
ULM as delimited in Section 1 is definitely closer to linguistic analysis than tolanguage acquisition.It may be instructive at this point to see what kinds of knowledge are supposed tobe required in order to carry out the discovery procedures mentioned in Section 2:An analyst approaches a language which either he already knows in some practicalway or with which he sets about to familiarize himself?preferably in a languagelearning situation.
The analyst?s background is the sum total of his practical knowledgeof other languages, his previous analytical experience, and what he has learnedfrom the linguistic research of other people.
With this knowledge of the languageto be analyzed and with this background knowledge, he makes certain guessesabout the grammatical structure of the language.
He then submits these guesses to aseries of systematic checks in which he confirms, disproves, or modifies his originalguesses?and makes a few better guesses en route.
This systematic evaluation isbased on a theory of the structure of language, and the theory itself (while containingelements of creative thinking) is based on empirical study.
(Longacre 1964, page 12)Mutatis mutandis, the procedure described in this quote, contains most elements of ULMand related methods proposed in the literature.
Note that the quote just given stressesthe importance of the knowledge that the linguist brings to the analysis and which in-forms the whole analytical process.
This suggests that there may be a level of generalknowledge about language (in general or a useful subset of languages), or about lin-guistic analysis, or both, which would be useful to ULM in general, something like the?knowledge?
that white space is a word delimiter in written text, but on a higher level.One component of a research program on ULM would then be to formulate this kindof general knowledge in a way which makes sense given that the object of study is lan-guage, to test it, and to share it with the community of linguistic scholars.
A concreteillustration could be the way that the old notion of proportional analogy (Anttila 1977)is refined and formalized in various ways and used to test segmentation hypothesesin works on ULM from the earliest times onwards (e.g., the ?squares?
mentioned inSection 3.3).4.2 ULM and SemanticsAs traditionally conceived, an inflectional paradigm links a set of word forms tostructural descriptions expressed in terms of a stem carrying a lexical meaning andsome formal expression of one or more morphosyntactic categories (or grammaticalmeanings) taken from a closed, small set of such categories.
This bears emphasizing,because ULM work generally has been concerned only with the formal expression sideof morphology; that is, instead of the traditionaltable-s ?table N PL?333Computational Linguistics Volume 37, Number 2table-s ?table V 3SG?it will give us simplytable-stable-salthough it may tell us that the stem table appears in two paradigms.As far as we know, there have been no attempts to induce functional labels usingULM, although it is conceivable that the same kind of techniques used, for example,in order to cluster words semantically (e.g., Latent Semantic Indexing/Analysis orRandom Indexing), could be used also to classify the resulting morphs from a ULMsegmentation (cf.
Schone and Jurafsky [2001b] for a study of inducing part-of-speechclass labels in a setting similar to that of ULM).
The labelling problem can easily beconsidered independent of ULM by using a hand-segmented (or segmented by a hand-built morphological parser) input corpus.4.3 Is ULM of Any Use?As we said in Section 2, there is an explicit expectation frequently encountered in themore recent literature that ULM and other unsupervised methods could be employedin order to rapidly and cheaply (in terms of human effort) bootstrap basic languagetechnology resources for new languages.
However, looking at the literature, it seemsthat?at least in the area of inflectional morphology?the only approaches that haveso far produced substantial results are the old-fashioned, hand-coded grammar-basedones, such as the work described by Trosterud (2004), where finite-state morphologicalprocessors and constraint grammar-based disambiguation components are developedfor a number of related languages.
The fact that the languages are related is of greathelp when dealing with successive languages after the first one.
The morphologicalcomponent for the first language, North S?mi, required approximately 2.5 person-yearsof highly qualified linguistic expert work to reach the prototype stage, whereas theanalogous module for the closely related Lule S?mi was completed in an additionalsix months (Trosterud 2006).17 This and other work in the same vein reported in theliterature (e.g., by Artola-Zubillaga 2004 and Maxwell and David 2008) is characterizedby deep and long-lasting involvement by linguistic expertise and further often by thecreative use of digitized versions of conventional printed linguistic resources, especiallydictionaries.
The following observation is perhaps trivial, but bears stressing, becauseit is in fact often not heeded in practice: For this kind of approach to work, it isnecessary that tools for providing systems with linguistic knowledge use a conceptualapparatus and notation familiar to the linguists who are supposed to be working withthem.
Relevant to our purposes here, the same holds for any attempt to kickstart thedevelopment of a morphological analyzer by using ULM: If the expectation is thatthe output of ULM should be manually ?post-edited,?
this output must of course beintelligible to the linguist doing the post-editing.17 As pointed out by one anonymous reviewer, this suggests that with the right organization of informationflow among machine-learning components, ULM, too, could benefit from working with several closelyrelated languages simultaneously.334Hammarstr?m and Borin Unsupervised Learning of MorphologyMost ULM approaches reported in the literature are small proof-of-concept experi-ments, which generally founder on the lack of evaluation data.
The MorphoChallenge18series does provide adequate gold-standard evaluation data for Finnish, English, Ger-man, Arabic, and Turkish as well as task-based Information Retrieval (IR) evaluationdata for English, German, and Finnish.
It can be seen that ULM systems are matureenough to enhance IR, but so far, ULM systems are not close to full accuracy on thegold standard and outside commentators have generally been unimpressed with theseresults (e.g., Mahlow and Piotrowski 2009, page vi).
However, many (most?)
of thestrong-looking systems reported in the literature have not, for one reason or another,taken part in the MorphoChallenge.
Taking MorphoChallenge results and proof-of-concept reports together, it seems that high accuracy by ULM systems is presently onlyachievable if the language has small amounts of one-slot concatenative morphology,whereas for morphologically more complex languages, parameter tuning and/or loweraccuracy is to be expected.We are not yet in a position to assess whether there are other tasks than IR which, ingeneral, benefit significantly from (noisy) ULM, such as Speech Recognition (Hirsim?kiet al 2003, 2005, 2006; Kurimo et al 2006) or Machine Translation (Sereewattana 2003;Virpioja et al 2007; Bojar, Stran?
?k, and Zeman 2008; Kirik and Fishel 2008; De Gispertet al 2009; Fishel and Kirik 2010) because almost only the Morfessor system has beentested, and results are, if positive, not completely unambiguous.
One usage of noisyULM, at least, is for smoothing language identification models (Hammarstr?m 2007a;Ceylan and Kim 2009).Further, ULM approaches are data-hungry, which precludes their use with manylow-density languages.
There is much ongoing work addressing these issues, however,so we can probably expect some progress in this area (Bird 2009).4.4 Future DirectionsIn practice, the near future should define a high-accuracy threshold-minimal system forone-slot morphology languages, using refinements of ideas already extant.A major challenge, and the reason for duplication of work in the past, is to find atheory that explains why (or why not) a given algorithm works.
Further study of theo-retical properties of (stochastic) combining of string sets/bags are likely to hold the keyto the culmination of the border-and-frequency methods?not further experimentationwith ad hoc heuristics.
The recent increased interest in Bayesian generative models ingeneral in NLP may possibly serve as a catalyst.In the group-and-abstract paradigm, working with feature sets of a word, as inDe Pauw and Wagacha (2007), is an ingenious generalization that holds numerousadvantages over string edit distances.
Feature set comparisons are naturally definedover arbitrary collections, whereas string edit distances work on pairs of strings.
Manymorphologically related words differ in several characters and are therefore not particu-larly close in edit distance.
Features instead of edit distances provide a neat framework,based on global properties of the feature distribution, of capturing the fact that somecharacter mismatches do not really matter, whereas some character matches (althoughnot necessarily long) are very significant.For paradigm induction, it is clear that the ULM field has not made use of the largeliterature on clustering in other fields.
Chan (2006) is a step in this direction, but further18 Web site www.cis.hut.fi/morphochallenge2009/ accessed 10 September 2009.335Computational Linguistics Volume 37, Number 2steps are lacking; in particular, spectral clustering (of some kind) has not been exploredfor paradigm induction in ULM.
Also here, given the typical skewed stem distributionsand skewed suffix distributions (exemplified in Section 3.2), some theoretical work isneeded to determine its implications for clustering.Finally, we see ample opportunity for empirical investigations into lesser-knownlanguages for which data has become available only recently (Abney and Bird 2010).This would clarify the potential of ULM usefulness for underdescribed and under-resourced languages.5.
ConclusionAfter more than half a century of research, the field of ULM has made good progress (ashave many other areas of computational linguistics), but there is still a long way to gobefore it will become practically useful or even theoretically interesting to linguists.
Inthe terms of Table 1 in Section 1, the state of the art of ULM is somewhere in the region of?Segmentation?
and ?Inflection tables,?
if we are talking about linguistic form, but therehas been next to no progress at all when it comes to linguistic meaning (e.g., functionallabeling of affixes).In the early days of ULM, the expectation was that it should constitute?when even-tually achieved sometime in the future?a formalized version of a linguistic discoveryprocedure, that is, a knowledge-heavy enterprise.
Instead, recent successes in the areahave been largely contingent upon the rapid development in computational linguisticsof statistical and information-theoretic knowledge-light (but robust) methodologies.We believe (like Wintner 2009 for computational linguistics in general), however,that if ULM is to become a serious alternative to?or, equally likely, a natural componentof?manually built computational morphology systems for a wide and diverse range oflanguages, and especially if we are to make headway in the area of semantics, we needto see more interaction between the present approaches to ULM with the computationaltechniques and mathematical modeling tools they can bring to bear on the problem onthe one hand, and typologically informed linguistic research on morphology foundedon a vast store of knowledge and methodology refined over two millennia on the other.AcknowledgmentsThe authors wish to thank three anonymousreferees for helpful comments andsuggestions.ReferencesAbney, Steven and Steven Bird.
2010.
Thehuman language project: Building auniversal corpus of the world?s languages.In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics,pages 88?97, Uppsala.Altmann, Gabriel and Werner Lehfeldt.
1980.Einf?hrung in die quantitative Phonologie,volume 7 of Quantitative Linguistics.Bochum: Brockmeyer.Andreev, Nikolaj Dmitrievic?.
1959.Modelirovanije jazyka na base egostatistic?eskoj i teoretiko-mno?estvennojstruktury.
In Tezisy sove?c?anija pomatematic?eskoj lingvistike, 14?21 Aprelja 1959goda.
Ministerstvo vys?ego obrazovanijaSSSR, Leningrad, pages 15?22.Andreev, Nikolaj Dmitrievic?.
1963.Algoritmy statistiko-kombinatornogomodelirovanija morfologii, sintaksisa,slovoobrazovanija i semantiki.
InMaterialy po matematic?eskoj lingvistike ima?inomu perevodu: Sbornik II.
Izdatel?stvoLeningradskogo universiteta, Leningrad,pages 3?44.Andreev, Nikolaj Dmitrievic?.
1965a.
Opytstatistiko-kombinatornogo vydelenijapervogo morfologic?eskogo tipa vvengerskom jazyke.
In Nikolaj Dmitrievic?Andreev, editor, Statistiko-kombinatornoemodelirovanie jazykov.
Nauka, Leningrad,pages 205?211.Andreev, Nikolaj Dmitrievic?, editor.
1965b.Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad.336Hammarstr?m and Borin Unsupervised Learning of MorphologyAndreev, Nikolaj Dmitrievic?.
1967.Statistiko-kombinatornye metody vteoretic?eskom i prikladnom jazykovedenii.Nauka, Leningrad.Andreeva, L. D. 1963.
Statistiko-kombinatornoe vydelenie paradigmypervogo mofologic?eskogo tipa v russkomjazyke.
In Materialy po matematic?eskojlingvistike i ma?innomu pervodu: Sbornik II.Izdatel?stvo Leningradskogo universiteta,Leningrad, pages 45?60.Anttila, Raimo.
1977.
Analogy.
Mouton,The Hague.Antworth, Evan L. 1990.
PC-KIMMO: Atwo-level processor for morphological analysis.Occasional Publications in AcademicComputing 16.
Summer Institute ofLinguistics, Dallas.Arabsorkhi, Mohsen and MehrnoushShamsfard.
2006.
Unsupervised discoveryof Persian morphemes.
In Proceedings of the11th Conference of the European Chapter of theAssociation for Computational Linguistics,EACL 2006, pages 175?178, Trento.Argamon, Shlomo, Navot Akiva, AmihoodAmir, and Oren Kapah.
2004.
Efficientunsupervised recursive wordsegmentation using minimum descriptionlength.
In Proceedings of COLING 2004,pages 1058?1064, Geneva.Artola-Zubillaga, Xabier.
2004.
Laying lexicalfoundations for NLP: The case of Basqueat the ixa research group.
In SALTMILWorkshop at LREC 2004: First Steps inLanguage Documentation for MinorityLanguages, pages 9?18, Lisbon.Atwell, Eric and Andrew Roberts.
2005.Combinatory hybrid elementary analysisof text.
In Proceedings of MorphoChallenge2005, pages 37?41, Helsinki University ofTechnology, Helsinki.Baayen, Harald R. 2001.
Word FrequencyDistributions, volume 18 of Text, Speech,and Language Technology.
Kluwer,Dordrecht.Bacchin, Michela, Nicola Ferro, and MassimoMelucci.
2002a.
The effectiveness of agraph-based algorithm for stemming.In ICADL ?02: Proceedings of the 5thInternational Conference on Asian DigitalLibraries, volume 2555 of Lecture Notesin Computer Science, pages 117?128,Singapore.Bacchin, Michela, Nicola Ferro, and MassimoMelucci.
2002b.
University of Padua atCLEF 2002: Experiments to evaluate astatistical stemming algorithm.
In WorkingNotes for CLEF 2002: Cross-Language EvaluationForum Workshop, pages 161?168, Rome.Bacchin, Michela, Nicola Ferro, and MassimoMelucci.
2005.
A probabilistic model forstemmer generation.
Information Processingand Management, 41(1):121?137.Baroni, Marco.
2000.
Distributional Cues inMorpheme Discovery: A Computational Modeland Empirical Evidence.
Ph.D. thesis,University of California, Los Angeles.Baroni, Marco.
2003.
Distribution-drivenmorpheme discovery: A computational/experimental study.
Yearbook of Morphology,213?248.Baroni, Marco, Johannes Matiasek, andHarald Trost.
2002.
Unsuperviseddiscovery of morphologically relatedwords based on orthographic andsemantic similarity.
In Proceedings of theWorkshop on Morphological and PhonologicalLearning of ACL/SIGPHON-2002,pages 48?57, Philadelphia.Batchelder, E. O.
1997.
Computational Evidencefor the Use of Frequency Information inDiscovery of the Infant?s First Lexicon.
Ph.D.thesis, City University of New York.Bati, Tesfaye Bayu.
2002.
Automaticmorphological analyser: An experimentusing unsupervised and autosegmentalapproach.
Master?s thesis, Addis AbabaUniversity, Ethiopia.Belkin, Mikhail and John Goldsmith.2002.
Using eigenvectors of the bigramgraph to infer morpheme identity.
InMorphological and Phonological Learning:Proceedings of the 6th Workshop of the ACLSpecial Interest Group in ComputationalPhonology (SIGPHON), pages 41?47,Philadelphia, PA.Bernhard, Delphine.
2005a.
Segmentationmorphologique ?
partir de corpus.
Actesde TALN & R?CITAL 2005, volume 1,pages 555?564, Dourdan.Bernhard, Delphine.
2005b.
Unsupervisedmorphological segmentation based onsegment predictability and word segmentsalignment.
In Mikko Kurimo, MathiasCreutz, and Krista Lagus, editors,Unsupervised segmentation of words intomorphemes ?
Challenge 2005, pages 18?22,Helsinki University of Technology,Helsinki.Bernhard, Delphine.
2006.
Apprentissagede connaissances morphologiques pourl?acquisition automatique de ressourceslexicales.
Ph.D. thesis, Universit?
JosephFourier ?
Grenoble I.Bernhard, Delphine.
2007.
Apprentissagenon supervis?
de familles morphologiquespar classification ascendante hi?rarchique.In Actes de la 14e conf?rence sur le Traitement337Computational Linguistics Volume 37, Number 2Automatique des Langues Naturelles, TALN2007, volume 1, pages 367?376, Toulouse.Bernhard, Delphine.
2008.
Simple morphemelabelling in unsupervised morphemeanalysis.
In Carol Peters, Valentin Jijkoun,Thomas Mandl, Henning M?ller, DouglasW.
Oard, Anselmo Pe?as, Vivien Petras,and Diana Santos, editors, Advances inMultilingual and Multimodal InformationRetrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007,Budapest, Hungary, September 19?21,2007, Revised Selected Papers,pages 873?880.
Springer-Verlag, Berlin.Bharati, Akshar, S. M. Bendre Rajeev Sangal,Pavan Kumar, and Aishwarya.
2001.Unsupervised improvement ofmorphological analyzer for inflectionallyrich languages.
In Proceedings of the SixthNatural Language Processing Pacific RimSymposium (NLPRS-2001), pages 685?692,Tokyo.Bickel, Balthasar, Goma Banjade, MartinGaenszle, Elena Lieven, Netra Paudyal,Ichchha Rai, Manoj Rai, Novel Kishor Rai,and Sabine Stoll3.
2007.
Free prefix orderingin Chintang.
Language, 83(1):43?73.Bird, Steven.
2009.
Last words: Naturallanguage processing and linguisticfieldwork.
Computational Linguistics,35(3):469?474.Blomqvist, Jerker and Poul Ole Jastrup.
1998.Grekisk grammatik: Graesk grammatik,2 edition.
Akademisk Forlag, K?benhavn.Bloomfield, Leonard.
1933.
Language.
HenryHolt & Co, New York.Bojar, Ondr?ej, Pavel Stran?
?k, and DanielZeman.
2008.
English?Hindi translation in21 days.
In Proceedings of the ICON-2008NLP Tools Contest, pages 4?7, Pune.Bordag, S. 2005a.
Unsupervisedknowledge-free morpheme boundarydetection.
Paper presented at theProceedings of Recent Advances in NaturalLanguage Processing 2005 (RANLP ?05),Borovets.Bordag, Stefan.
2005b.
Two-step approach tounsupervised morpheme segmentation.
InMikko Kurimo, Mathias Creutz, and KristaLagus, editors, Unsupervised segmentationof words into morphemes ?
Challenge 2005,pages 23?27, Helsinki University ofTechnology, Helsinki.Bordag, Stefan.
2007.
Elements ofKnowledge-Free and Unsupervised LexicalAcquisition.
Ph.D. thesis, University ofLeipzig, Leipzig.Bordag, Stefan.
2008.
Unsupervised andknowledge-free morpheme segmentationand analysis.
In Carol Peters, ValentinJijkoun, Thomas Mandl, Henning M?ller,Douglas W. Oard, Anselmo Pe?as, VivienPetras, and Diana Santos, editors, Advancesin Multilingual and Multimodal InformationRetrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007,Budapest, Hungary, September 19?21,2007, Revised Selected Papers,pages 881?891.
Springer-Verlag, Berlin.Borin, Lars.
1991.
The Automatic Induction ofMorphological Regularities.
Ph.D. thesis,Uppsala University.Borin, Lars.
2009.
One in the bush:Low-density language technology.Research Reports from the Department ofSwedish, No.
GU-ISS-09-1.
University ofGothenburg.Borin, Lars, Markus Forsberg, and LennartL?nngren.
2008.
The hunting of theBLARK - SALDO, a freely available lexicaldatabase for Swedish language technology.In Joakim Nivre, Mats Dahll?f, and Be?taMegyesi, editors, Resourceful languagetechnology: Festschrift in honor of AnnaS?gvall Hein, volume 7 of Studia LinguisticaUpsaliensia.
Acta Universitatis Upsaliensis,Uppsala, pages 21?32.Brasington, Ron, Steve Jones, and ColinBiggs.
1988.
The automatic induction ofmorphological rules.
Literary and LinguisticComputing, 3(2):71?78.Brent, Michael.
1993.
Minimal generativeexplanations: A middle ground betweenneurons and triggers.
In Proceedings ofthe Fifteenth Annual Conference of theCognitive Science Society, pages 28?36,Boulder, CO.Brent, Michael R. 1999.
An efficient,probabilistically sound algorithm forsegmentation and word discovery.Machine Learning, 34:71?105.Brent, Michael R., S. Murthy, andA.
Lundberg.
1995.
Discoveringmorphemic suffixes: A case study inminimum description length induction.In Fifth International Workshop on ArtificialIntelligence and Statistics, pages 482?490.Fort Lauderdale, FL.Calderone, Basilio.
2008.
UnsupervisedLearning of Linguistic Structures.
Ph.D.thesis, Scuola Normale Superiore, Pisa.Carstairs, Andrew.
1983.
Paradigm economy.Journal of Linguistics, 19:115?125.C?avar, Damir, Joshua Herring, ToshikazuIkuta, Paul Rodrigues, and GiancarloSchrementi.
2004a.
On induction ofmorphology grammars and its role inbootstrapping.
In Proceedings of Formal338Hammarstr?m and Borin Unsupervised Learning of MorphologyGrammar 2004, pages 47?62, ESSLLI,Nancy, France.C?avar, Damir, Joshua Herring, ToshikazuIkuta, Paul Rodrigues, and GiancarloSchrementi.
2004b.
On statisticalparameter setting.
In Proceedings of theFirst Workshop on Psycho-ComputationalModels of Human Language Acquisition,pages 9?16, Geneva.C?avar, Damir, Jushua Herring, ToshikazuIkuta, Paul Rodrigues, and GiancarloSchrementi.
2006.
On unsupervisedgrammar induction from untaggedcorpora.
In P. Kaszubski, editor, PSiCL:Poznan?
Studies in ContemporaryLinguistics, volume 41.
Poznan?,Poland: Adam Mickiewicz University,pages 57?71.C?avar, Damir, Paul Rodrigues, andGiancarlo Schrementi.
2006.
Unsupervisedmorphology induction for part-of-speechtagging.
In Aviad Eilam, Tatjana Scheffler,and Joshua Tauberer, editors, Proceedings ofthe 29th Annual Penn Linguistics Colloquium,volume 12(1) of U. Penn Working Papers inLinguistics.
University of PennsylvaniaPress, Philadelphia, pages 29?41.Ceylan, Hakan and Yookyung Kim.
2009.Language identification of search enginequeries.
In ACL-IJCNLP ?09: Proceedingsof the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th InternationalJoint Conference on Natural LanguageProcessing of the AFNLP: Volume 2,pages 1066?1074, Morristown, NJ.Chan, Erwin.
2006.
Learning probabilisticparadigms for morphology in a latentclass model.
In Proceedings of the EighthMeeting of the ACL Special Interest Group onComputational Phonology and Morphology atHLT-NAACL 2006, pages 69?78, New YorkCity, NY.Chan, Erwin.
2008.
Structures andDistributions in Morphology Learning.
Ph.D.thesis, University of Pennsylvania,Philadelphia, PA.Cho, Sehyeong and Seung-Soo Han.
2002.Automatic stemming for indexing of anagglutinative language.
In T. Yakhno,editor, Advances in Information Systems,volume 2457 of Lecture Notes in ComputerScience.
Springer-Verlag, Berlin,pages 154?165.Clark, Alexander.
2001.
UnsupervisedLanguage Acquisition.
Ph.D. thesis,University of Sussex.Creutz, Mathias.
2003.
Unsupervisedsegmentation of words using priordistributions of morph length andfrequency.
In Proceedings of the ACL2003, pages 280?287, Sapporo.Creutz, Mathias.
2006.
Induction of theMorphology of Natural Language:Unsupervised Morpheme Segmentationwith Application to Automatic SpeechRecognition.
Ph.D. thesis, HelsinkiUniversity of Technology, Espoo,Finland.Creutz, Mathias and Krista Lagus.
2002.Unsupervised discovery of morphemes.In Proceedings of the 6th Workshop of theACL Special Interest Group in ComputationalPhonology (SIGPHON), pages 21?30,Philadelphia, PA.Creutz, Mathias and Krista Lagus.
2004.Induction of a simple morphologyfor highly-inflecting languages.
InProceedings of the 7th Meeting of the ACLSpecial Interest Group in ComputationalPhonology (SIGPHON), pages 43?51,Barcelona.Creutz, Mathias and Krista Lagus.
2005a.Inducing the morphological lexicon of anatural language from unannotated text.In Proceedings of the International andInterdisciplinary Conference on AdaptiveKnowledge Representation and Reasoning(AKRR ?05), pages 106?113, Espoo.Creutz, Mathias and Krista Lagus.
2005b.Morfessor in the Morpho Challenge.
InMikko Kurimo, Mathias Creutz, and KristaLagus, editors, Unsupervised segmentation ofwords into morphemes ?
Challenge 2005,pages 12?17, Helsinki University ofTechnology, Helsinki.Creutz, Mathias and Krista Lagus.
2005c.Unsupervised morpheme segmentationand morphology induction from textcorpora using morfessor 1.0.
Technicalreport A81, Publications in Computer andInformation Science, Helsinki Universityof Technology.Creutz, Mathias and Krista Lagus.
2007.Unsupervised models for morphemesegmentation and morphology learning.ACM Transactions on Speech and LanguageProcessing, 4(1?3):1?33.Creutz, Mathias, Krista Lagus, KristerLind?n, and Sami Virpioja.
2005.Morfessor and hutmegs: Unsupervisedmorpheme segmentation forhighly-inflecting and compoundinglanguages.
In Proceedings of the SecondBaltic Conference on Human LanguageTechnologies, pages 107?112, Tallinn.Creutz, Mathias, Krista Lagus, and SamiVirpioja.
2005.
Unsupervised morphologyinduction using morfessor.
In Finite State339Computational Linguistics Volume 37, Number 2Methods in Natural Language Processing: 5thInternational Workshop, FSMNLP 2005,pages 300?301, Helsinki.Cromm, Oliver.
1997.
Affixerkennung indeutschen wortformen: Ein nicht-lexikalisches segmentierungsverfahrennach N. D. Andreev.
LDV-Forum,14(2):4?13.Cucerzan, Silviu and David Yarowsky.2002.
Bootstrapping a multilingualpart-of-speech tagger in one person-day.In Proceedings of CoNLL-2002, pages 1?7,Taipei.Daelemans, Walter.
2004.
Computationallinguistics.
In Geert Booij, ChristianLehmann, Joachim Mugdan, and StavrosSkopetas, editors, Morphologie/Morphology:Ein internationales Handbuch zur Flexionund Wortbildung [An InternationalHandbook on Inflection and Word-Formation],volume 17.2 of Handb?cher zur Sprach- undKommunikationswissenschaft.
Mouton deGruyter, Berlin, pages 1893?1900.Dang, Minh Thang and Saad Choudri.
2005.Simple unsupervised morphology analysisalgorithm (SUMAA).
In Mikko Kurimo,Mathias Creutz, and Krista Lagus, editors,Proceedings of MorphoChallenge 2005,pages 47?51, Helsinki University ofTechnology, Helsinki.Dasgupta, Sajib.
2007.
Toward language-independent morphological segmentationand part-of-speech induction.
Master?sthesis, The University of Texas at Dallas.Dasgupta, Sajib and Vincent Ng.
2006.Unsupervised morphological parsing ofbengali.
Language Resources and Evaluation,3?4:311?330.Dasgupta, Sajib and Vincent Ng.
2007a.High-performance, language-independentmorphological segmentation.
In HumanLanguage Technologies 2007: The Conferenceof the North American Chapter of theAssociation for Computational Linguistic,pages 155?163, Rochester, NY,Association for ComputationalLinguistics.Dasgupta, Sajib and Vincent Ng.
2007b.Unsupervised word segmentationfor Bangla.
In Proceedings of the 5thInternational Conference on NaturalLanguage Processing (ICON 2007),pages 15?24, Hyderabad.De Gispert, Adri?, Sami Virpioja, MikkoKurimo, and William Byrne.
2009.Minimum bayes risk combination oftranslation hypotheses from alternativemorphological decompositions.
InProceedings of Human Language Technologies:The 2009 Annual Conference of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 73?76,Boulder, CO.de Kock, Josse and Walter Bossaert.
1969.Towards an automatic morphologicalsegmentation.
In International Conferenceon Computational Linguistics, COLING,pages 10?11, S?nga-S?by.de Kock, Josse and Walter Bossaert.
1974.Introducci?n a la ling?
?stica autom?tica en laslenguas Rom?nicas, volume 202 of Bibliotecarom?nica hisp?nica 2: Estudios y ensayos.Gredos, Madrid.de Kock, Josse and Walter Bossaert.
1978.
TheMorpheme: An Experiment in Quantitativeand Computational Linguistics.
Van Gorcum,Amsterdam.De Pauw, Guy and Peter W. Wagacha.
2007.Bootstrapping morphological analysis ofG??ku?yu?
using maximum entropy learning.In Proceedings of the 8th Annual Conferenceof the International Speech CommunicationAssociation (INTERSPEECH 2007),pages 1517?1520, Antwerp.D?jean, Herv?.
1998a.
Concepts et alorithmespour la d?couverte des structures formelles deslangues.
Ph.D. thesis, Universit?
de CaenBasse Normandie.D?jean, Herv?.
1998b.
Morphemes as anecessary concept for structures discoveryfrom untagged corpora.
In NeMLaP3/CoNLL98 Workshop on Paradigms andGrounding in Language Learning,pages 295?298, Philadephia, PA.Deligne, Sabine.
1996.
Mod?les de s?quences delongueurs variables: application au traitementdu langage ?crit et de la parole.
Ph.D. thesis,?cole Nationale Sup?rieure desT?l?communications, Paris.Deligne, Sabine and Fr?d?ric Bimbot.
1997.Inference of variable-length linguisticand acoustic units by multigrams.
SpeechCommunication, 23(3):223?241.Demberg, Vera.
2007.
A language-independent unsupervised model formorphological segmentation.
InProceedings of the 45th Annual Meetingof the Association of ComputationalLinguistics, pages 920?927, Prague.Dryer, Matthew S. 2005.
Prefixing versussuffixing in inflectional morphology.In Bernard Comrie, Matthew S. Dryer,David Gil, and Martin Haspelmath,editors, World Atlas of Language Structures.Oxford University Press, Oxford,pages 110?113.Eguchi, Paul K. 1987.
Fieldworker andcomputer: An end user?s view of computer340Hammarstr?m and Borin Unsupervised Learning of Morphologyethnology.
Senri Ethnological Studies,20:165?174.Ejerhed, Eva and Gunnel K?llgren.
1997.Stockholm Ume?
Corpus version 1.0,SUC 1.0.
Technical report, Departmentof Linguistics, Ume?
University.Eliseeva, K. A.
1965.
Statistiko-kombinatornoe modelirovaniepervogo tipa v ukrainskoj morfologii.In Nikolaj Dmitrievic?
Andreev, editor,Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad, pages 85?88.Faulk, R. D. and F. Goertzel Gustavson.
1990.Segmenting discrete data representingcontinuous speech input.
IBM SystemsJournal, 29(2):287?296.Fedulova, N. I.
1965.
Vydelenie pervogomorfologic?eskogo tipa v bolgarskomjazyke.
In Nikolaj Dmitrievic?
Andreev,editor, Statistiko-kombinatornoemodelirovanie jazykov.
Nauka,Leningrad, pages 110?115.Fihman, B. S. 1965a.
Vydelenie pervogomorfologic?eskogo tipa v jazyke hausa poalgoritmu statistiko-kombinatornogomodelirovanija.
In Nikolaj Dmitrievic?Andreev, editor, Statistiko-kombinatornoemodelirovanie jazykov.
Nauka, Leningrad,pages 189?195.Fihman, B. S. 1965b.
Vydelenie pervogomorfologic?eskogo tipa v jazyke suahili poalgoritmu statistiko-kombinatornogomodelirovanija.
In Nikolaj Dmitrievic?Andreev, editor, Statistiko-kombinatornoemodelirovanie jazykov.
Nauka, Leningrad,pages 196?204.Fishel, Mark and Harri Kirik.
2010.Linguistically motivated unsupervisedsegmentation for machine translation.
InProceedings of the Seventh InternationalLanguage Resources and Evaluation(LREC?10), pages 1741?1745, Valletta.Fitialova, I.
B.
1965.
Statistiko-kombinatornoevydelenie pervogo morfologic?eskogo tipav nemeckom jazyke.
In Nikolaj Dmitrievic?Andreev, editor, Statistiko-kombinatornoemodelirovanie jazykov.
Nauka, Leningrad,pages 158?171.Flenner, Gudrun.
1992.
Ein quantitativesMorphsegmentierungsverfahren f?rspanische Wortformen.
Ph.D. thesis,Georg-August-Universit?t G?ttingen.Flenner, Gudrun.
1994.
Ein quantitativesMorphsegmentierungssystem f?rspanische Wortformen.
In Ursula Klenk,editor, Computatio Linguae II: Aufs?tzezur algorithmischen und quantitativenAnalyse der Sprache, volume 83of Zeitschrift f?r Dialektologie undLinguistik: Beihefte.
Franz Steiner,Stuttgart, pages 31?62.Flenner, Gudrun.
1995.
QuantitativeMorphsegmentierung im Spanischenauf phonologischer Basis.
Spracheund Datenverarbeitung, 19(2):63?78.Foley, William.
1991.
The Yimas Language ofNew Guinea.
Stanford University Press,Stanford, CA.Forsberg, Markus, Harald Hammarstr?m,and Aarne Ranta.
2006.
Lexicon extractionfrom raw text data.
In Proceedings of the5th International Conference, FinTAL,pages 488?499, Turku.Francis, Nelson W. and Henry Kucera.
1964.Brown corpus.
Department of Linguistics,Brown University, Providence, RI.Freitag, Dayne.
2005.
Morphology inductionfrom term clusters.
In Proceedings of theNinth Conference on Computational NaturalLanguage Learning (CoNLL-2005),pages 128?135, Ann Arbor, MI.Gammon, Edward.
1969.
Quantitativeapproximations to the word.
InInternational Conference on ComputationalLinguistics, COLING, pages 1?28,S?nga-S?by.Garvin, Paul L. 1967.
The automation ofdiscovery procedure in linguistics.Language, 43(1):172?178.Gaussier, ?ric.
1999.
Unsupervised learningof derivational morphology frominflectional lexicons.
In Proceedings of theWorkshop on Unsupervised Learning inNatural Language Processing at the 37thAnnual Meeting of the Association forComputational Linguistics (ACL-1999),pages 24?30, Philadephia, PA.Gelbukh, Alexander F., Mikhail Alexandrov,and Sang-Yong Han.
2004.
Detectinginflection patterns in natural languageby minimization of morphological model.In Alberto Sanfeliu, Jos?
FranciscoMart?nez Trinidad, and Jes?s ArielCarrasco-Ochoa, editors, Proceedings ofProgress in Pattern Recognition, ImageAnalysis and Applications, 9th IberoamericanCongress on Pattern Recognition, CIARP ?04,volume 3287 of Lecture Notes in ComputerScience.
Springer-Verlag, Berlin,pages 432?438.Gippert, Jost, Nikolaus P. Himmelmann,and Ulrike Mosel, editors.
2006.
Essentialsof Language Documentation.
Mouton deGruyter, Berlin.Golcher, Felix.
2006.
Statistical textsegmentation with partial structureanalysis.
In Proceedings of KONVENS2006, pages 44?51, Konstanz.341Computational Linguistics Volume 37, Number 2Golding, Andrew and Henry S. Thompson.1985.
A morphology component forlanguage programs.
Linguistics,23:263?284.Goldsmith, John.
2000.
Linguistica: Anautomatic morphological analyzer.
InProceedings from the Main Session of theChicago Linguistic Society?s 36th Meeting,pages 125?139, Chicago, IL.Goldsmith, John.
2001.
Unsupervisedlearning of the morphology of naturallanguage.
Computational Linguistics,27(2):153?198.Goldsmith, John, Derrick Higgins, andSvetlana Soglasnova.
2001.
Automaticlanguage-specific stemming ininformation retrieval.
In Carol Peters,editor, Cross-Language Information Retrievaland Evaluation: Proceedings of the CLEF2000 Workshop, Lecture Notes inComputer Science.
Springer-Verlag,Berlin, pages 273?283.Goldsmith, John, Yu Hu, Irina Matveeva,and Colin Sprague.
2005.
A heuristicfor morpheme discovery based onstring edit distance.
TechnicalReport of Computer ScienceDepartment, University of Chicago, IL.TR-2005-4.Goldsmith, John and Aris Xanthos.
2009.Learning phonological categories.Language, 85(1):4?38.Goldsmith, John A.
2006.
An algorithm forthe unsupervised learning of morphology.Natural Language Engineering,12(4):353?371.Goldsmith, John A.
2010.
Segmentation andmorphology.
In Alexander Clark, ChrisFox, and Shalom Lappin, editors, Handbookof Computational Linguistics and NaturalLanguage Processing, Blackwell Handbooksin Linguistics.
Wiley-Blackwell, Oxford,pages 364?393.Goldwater, Sharon.
2007.
NonparametricBayesian Models of Lexical Acquisition.
Ph.D.thesis, Brown University.Goldwater, Sharon, Tom Griffiths, andMark Johnson.
2005.
Interpolating betweentypes and tokens by estimating power-lawgenerators.
In Advances in NeuralInformation Processing Systems 18 [NeuralInformation Processing Systems, NIPS 2005],pages 459?466, Vancouver.Gol?nia, Bruno.
2008.
Learning rules inmorphology of complex syntheticlanguages.
Master?s thesis, Universit?de Paris V.Goodman, Sarah A.
2008.
Morphologicalinduction through linguistic productivity.In Working Notes for the CLEF 2008Workshop, Aarhus.Gr?nwald, Peter D. 2007.
The MinimumDescription Length Principle: AdaptiveComputation and Machine Learning.MIT Press, Cambridge, MA.Hadouche, Fadila.
2002.
D?tection derelations morphologiques en corpus bas?esur les cooccurrences.
Master?s thesis,DESS, Centre de Recherche en Ing?nierieMultilingue, CRIM, France.Hafer, Margaret A. and Stephen F. Weiss.1974.
Word segmentation by lettersuccessor varieties.
Information Storageand Retrieval, 10:371?385.Hall, Jr., Robert A.
1987.
Bloomfield andsemantics.
In Robert A.
Hall, Jr., editor,Leonard Bloomfield: Essays on his Life andwork.
John Benjamins, Amsterdam,pages 155?160.Hammarstr?m, Harald.
2005.
A newalgorithm for unsupervised inductionof concatenative morphology.
In FiniteState Methods in Natural LanguageProcessing: 5th International Workshop,FSMNLP 2005, pages 288?289, Helsinki.Hammarstr?m, Harald.
2006a.
A naivetheory of morphology and an algorithmfor extraction.
In SIGPHON 2006: EighthMeeting of the Proceedings of the ACL SpecialInterest Group on Computational Phonology,pages 79?88, New York, NY.Hammarstr?m, Harald.
2006b.
Poor man?sstemming: Unsupervised recognition ofsame-stem words.
In Information RetrievalTechnology: Proceedings of the Third AsiaInformation Retrieval Symposium, AIRS2006, pages 323?337, Singapore.Hammarstr?m, Harald.
2007a.
A fine-grained model for language identification.In Proceedings of iNEWS-07 Workshop atSIGIR 2007, pages 14?20, Amsterdam.Hammarstr?m, Harald.
2007b.
Unsupervisedlearning of morphology: Survey, model,algorithm and experiments.
Thesis forthe Degree of Licentiate of Engineering,Department of Computer Science andEngineering, Chalmers University.Hammarstr?m, Harald.
2009a.
Poor man?sword-segmentation: Unsupervisedmorphological analysis for Indonesian.In Proceedings of the Third InternationalWorkshop on Malay and Indonesian LanguageEngineering (MALINDO), Singapore.Hammarstr?m, Harald.
2009b.
UnsupervisedLearning of Morphology and the Languagesof the World.
Ph.D. thesis, ChalmersUniversity of Technology and Universityof Gothenburg.342Hammarstr?m and Borin Unsupervised Learning of MorphologyHarris, Zellig.
1967.
Morpheme boundarieswithin words: Report on a computer test.In Transformations and Discourse AnalysisPapers 73.
Department of Linguistics,University of Pennsylvania, Philadelphia.Reprinted in Harris 1970.Harris, Zellig S. 1955.
From phoneme tomorpheme.
Language, 31(2):190?222.Harris, Zellig S. 1968.
Recurrent dependenceprocess: Morphemes by phonemeneighbors.
In Mathematical Structures ofLanguage, volume 21 of Interscience tracts inpure and applied mathematics.
Interscience,New York, pages 24?28.Harris, Zellig S. 1970.
Morpheme boundarieswithin words: Report on a computer test.In Zellig S. Harris, editor, Papers inStructural and Transformational Linguistics,volume 1 of Formal Linguistics Series.D.
Reidel, Dordrecht, pages 68?77.Haspelmath, Martin.
2002.
Understandingmorphology.
Arnold, London.Hirsim?ki, Teemu, Mathias Creutz, VesaSiivola, and Mikko Kurimo.
2003.Unlimited vocabulary speech recognitionbased on morphs discovered in anunsupervised manner.
In Proceedings ofEurospeech 2003, Geneva, pages 2293?2996.Geneva.Hirsim?ki, Teemu, Mathias Creutz,Vesa Siivola, and Mikko Kurimo.2005.
Morphologically motivatedlanguage models in speech recognition.In Proceedings of the Internationaland Interdisciplinary Conference onAdaptive Knowledge Representation andReasoning (AKRR ?05), pages 121?126,Espoo.Hirsim?ki, Teemu, Mathias Creutz, VesaSiivola, Mikko Kurimo, Sami Virpioja,and Janne Pylkk?nen.
2006.
Unlimitedvocabulary speech recognition withmorph language models applied toFinnish.
Computer Speech and Language,20(4):515?541.Hol?m, H. A.
1965.
Vydelenie pervogomorfologic?eskogo tipa v e?stonskom jazykena osnove statistiko-kombinatornogomodelirovanija.
In Nikolaj Dmitrievic?Andreev, editor, Statistiko-kombinatornoemodelirovanie jazykov.
Nauka, Leningrad,pages 212?224.Hu, Yu, Irina Matveeva, John Goldsmith,and Colin Sprague.
2005a.
Refining theSED heuristic for morpheme discovery:Another look at Swahili.
In Proceedingsof the Workshop on PsychocomputationalModels of Human Language Acquisition,pages 28?35, Ann Arbor, MI.Hu, Yu, Irina Matveeva, John Goldsmith, andColin Sprague.
2005b.
Using morphologyand syntax together in unsupervisedlearning.
In Proceedings of the Workshopon Psychocomputational Models of HumanLanguage Acquisition, pages 20?27,Ann Arbor, MI.Jacquemin, Christian.
1997.
Guessingmorphology from terms and corpora.In Proceedings, 20th Annual InternationalACM SIGIR Conference on Research andDevelopment in Information Retrieval(SIGIR ?97), pages 155?165,Philadelphia, PA.Jakubajtis, T. A.
1965.
Statistiko-kombinatornoe vydelenie pervogomorfologic?eskogo tipa v laty?skomjazyke.
In Nikolaj Dmitrievic?
Andreev,editor, Statistiko-kombinatornoemodelirovanie jazykov.
Nauka, Leningrad,pages 116?122.Jaku?eva, D. A.
1965.
Opyt primenenijaalgoritma statistiko-kombinatornogomodelirovanija k v?etnamskomu jazyku.In Nikolaj Dmitrievic?
Andreev, editor,Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad, pages 225?228.Jan?en, Axel.
1992.
Segmentierungfranz?sischer Wortformen ohne Lexikon.In Ursula Klenk, editor, ComputatioLinguae: Aufs?tze zur algorithmischenund quantitativen Analyse der Sprache,volume 73 of Zeitschrift f?r Dialektologieund Linguistik: Beihefte.
Franz Steiner,Stuttgart, pages 74?95.Jensen, John T. 1990.
Morphology.
WordStructure in Generative Grammar.
JohnBenjamins, Amsterdam.Johnsen, Lars G. 2005.
Morphologicallearning as principled argument.
In MikkoKurimo, Mathias Creutz, and Krista Lagus,editors, Proceedings of MorphoChallenge2005, pages 33?36, Helsinki University ofTechnology, Helsinki.Johnson, Howard and Joel Martin.
2003.Unsupervised learning of morphology forEnglish and Inuktitut.
In HLT-NAACL2003, Human Language TechnologyConference of the North American Chapterof the Association for ComputationalLinguistics, pages 43?45, Edmonton.Johnson, Mark.
2008.
Unsupervised wordsegmentation for Sesotho using adaptorgrammars.
In Proceedings of the TenthMeeting of ACL Special Interest Group onComputational Morphology and Phonology,pages 20?27, Columbus, OH.Jordan, Chris, John Healy, and Vlado Keselj.2005.
Swordfish: Using n-grams in an343Computational Linguistics Volume 37, Number 2unsupervised approach to morphologicalanalysis.
In Mikko Kurimo, MathiasCreutz, and Krista Lagus, editors,Proceedings of MorphoChallenge 2005,pages 42?46, Helsinki University ofTechnology, Helsinki.Jordan, Chris, John Healy, and Vlado Keselj.2006.
Swordfish: An unsupervised ngrambased approach to morphological analysis.In SIGIR ?06: Proceedings of the 29th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval, pages 657?658, New York, NY.Juola, Patrick, Chris Hall, and Adam Boggs.1994.
Corpus-based morphologicalsegmentation by entropy changes.
InThird Conference on the Cognitive Scienceof Natural Language Processing, Dublin.Katrenko, Sophia.
2004.
Towardsunsupervised learning of morphologyapplied to Ukrainian.
Student Session:16th European Summer School in Logic,Language and Information, pages 138?148,Nancy.Kazakov, Dimitar.
1997.
Unsupervisedlearning of na?ve morphology with geneticalgorithms.
In ECML?97 ?
Workshop Noteson Empirical Learning of Natural LanguageTasks, pages 105?112, Prague.Kazakov, Dimitar and Suresh Manandhar.1998.
A hybrid approach to wordsegmentation.
In Proceedings of the 8thInternational Workshop on Inductive LogicProgramming (ILP-98), pages 125?134,Madison, WI.Kazakov, Dimitar and Suresh Manandhar.2001.
Unsupervised learning of wordsegmentation rules with geneticalgorithms and inductive logicprogramming.
Machine Learning,43:121?162.Keshava, Samarth and Emily Pitler.
2005.
Asimpler, intuitive approach to morphemeinduction.
In Mikko Kurimo, MathiasCreutz, and Krista Lagus, editors,Proceedings of MorphoChallenge 2005,pages 28?32, Helsinki University ofTechnology, Helsinki.Kirik, Harri and Mark Fishel.
2008.Modelling linguistic phenomena withunsupervised morphology for improvingstatistical machine translation.
Paperpresented at the Workshop onUnsupervised Methods in NLP, heldin conjunction with SLTC?08, RoyalInstitute of Technology, Stockholm,Sweden.Klein, Sheldon and Terry A. Dennison.1976.
An interactive program forlearning the morphology of naturallanguages.
In Proceedings of the 3rdInternational Meeting on ComputationalLinguistics, pages 343?353, Debrecen.Klenk, Ursula.
1985a.
Ein nicht-lexikalischesVerfahren zur Erkennung spanischerWortst?mme.
In Ursula Klenk, editor,Strukturen und Verfahren in der maschinellenSprachverarbeitung.
AQ-Verlag, Dudweiler,pages 47?65.Klenk, Ursula.
1985b.
Recognition ofSpanish inflectional endings based on thedistribution of characters.
In Computersin Literary and Linguistic Computing:Proceedings of the Eleventh InternationalConference [L?ordinateur et les rechercheslitt?raires et linguistiques: actes de la XIeConf?rence internationale], pages 246?253,Louvain.Klenk, Ursula.
1991.
Verfahren derSegmentierung von W?rtern in Morphe:Mit einer Untersuchung zum Spanischen.In J?rgen Rolshoven und Dieter Seelbach,editor, Romanistische Computerlinguistik:Theorien und Implementationen, volume 266of Linguistische Arbeiten.
Niemeyer,T?bingen, pages 197?206.Klenk, Ursula.
1992.
Verfahrenmorphologischer Segmentierung und dieWortstruktur des Spanischen.
In UrsulaKlenk, editor, Computatio Linguae: Aufs?tzezur algorithmischen und quantitativenAnalyse der Sprache, volume 73 ofZeitschrift f?r Dialektologie und Linguistik:Beihefte.
Franz Steiner, Stuttgart,pages 110?124.Klenk, Ursula and Hagen Langer.
1989.Morphological segmentation without alexicon.
Literary and Linguistic Computing,4(4):247?253.Kohonen, Oskar, Sami Virpioja, andMikaela Klami.
2008.
Allomorfessor:Towards unsupervised morphemeanalysis.
In Carol Peters, ThomasDeselaers, Nicola Ferro, Julio Gonzalo,Gareth J. F. Jones, Mikko Kurimo,Thomas Mandl, Anselmo Pe?as, andVivien Petras, editors, EvaluatingSystems for Multilingual and MultimodalInformation Access, 9th Workshop ofthe Cross-Language Evaluation Forum,CLEF 2008, Aarthus, Denmark,September 17?19, 2008, Revised SelectedPapers, pages 975?982, Springer-Verlag,Berlin.Kontorovich, L., D. Don, and Y. Singer.2003.
A Markov model for the acquisitionof morphological structure.
Technicalreport CMU-CS-03-147, School of344Hammarstr?m and Borin Unsupervised Learning of MorphologyComputer Science, Carnegie MellonUniversity, June.Kordi, E. E. 1965.
Ishodnye dannyedlja statistiko-kombinatornogomodelirovanija morfologii sovremennogofrancuzckogo jazyka i vydeleniepervogo morfologic?eskogo tipa.
InNikolaj Dmitrievic?
Andreev, editor,Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad, pages 172?180.Krauss, Michael.
1992.
The world?slanguages in crisis.
Language, 68(1):4?10.Krauss, Michael E. 2007.
Mass languageextinction and documentation: The raceagainst time.
In O. Miyaoka, O. Sakiyama,and M. Krauss, editors, VanishingLanguages of the Pacific Rim.
OxfordUniversity Press, Oxford, pages 3?24.Kurimo, Mikko, Mathias Creutz, andVille Turunen.
2007.
Overview ofMorpho Challenge in CLEF 2007.
InWorking Notes for the CLEF 2007 Workshop,Budapest.Kurimo, Mikko, Mathias Creutz, andMatti Varjokallio.
2008a.
Morpho challengeevaluation by information retrievalexperiments.
In Carol Peters, ValentinJijkoun, Thomas Mandl, Henning M?ller,Douglas W. Oard, and Anselmo Pe?as,editors, Advances in Multilingual andMultimodal Information Retrieval, 8thWorkshop of the Cross-LanguageEvaluation Forum, CLEF 2007, Budapest,Hungary, September 19?21, 2007, RevisedSelected Papers, pages 991?998.Springer-Verlag, Berlin.Kurimo, Mikko, Mathias Creutz, and MattiVarjokallio.
2008b.
Morpho challengeevaluation using a linguistic goldstandard.
In Carol Peters, Valentin Jijkoun,Thomas Mandl, Henning M?ller, DouglasW.
Oard, and Anselmo Pe?as, editors,Advances in Multilingual and MultimodalInformation Retrieval, 8th Workshop ofthe Cross-Language Evaluation Forum,CLEF 2007, Budapest, Hungary,September 19?21, 2007, Revised SelectedPapers, pages 864?872.
Springer-Verlag,Berlin.Kurimo, Mikko, Antti Puurula, Ebru Arisoy,Vesi Siivola, Teemu Hirsim?ki, JannePylkk?nen, Tanel Alum?e, and MuratSara?lar.
2006.
Unlimited vocabularyspeech recognition for agglutinativelanguages.
In Proceedings of the MainConference on Human Language TechnologyConference of the North American Chapter ofthe Association for Computational Linguistics,pages 487?494, New York.Kurimo, Mikko and Ville Turunen.
2008.Unsupervised morpheme analysisevaluation by IR experiments?MorphoChallenge 2008.
In Working Notes for theCLEF 2008 Workshop, Aarhus.Kurimo, Mikko and Matti Varjokallio.2008.
Unsupervised morpheme analysisevaluation by a comparison to a linguisticgold standard?Morpho Challenge 2008.In Working Notes for the CLEF 2008Workshop, Aarhus.Langer, Hagen.
1991.
Ein automatischesMorphsegmentierungsverfahren f?rdeutsche Wortformen.
Ph.D. thesis,Georg-August-Universit?t zu G?ttingen.Lehmann, Hubert.
1973.
LinguistischeModellbildung und Methodologie.
MaxNiemeyer Verlag, T?bingen.Lewis, M. Paul, editor.
2009.
Ethnologue:Languages of the World.
Available atwww.ethnologue.com/.Lind?n, Krister.
2008.
A probabilistic modelfor guessing base forms of new words byanalogy.
In Proceedings of CICLing-2008: 9thInternational Conference on Intelligent TextProcessing and Computational Linguistics,pages 106?116, Springer, Berlin.Lind?n, Krister.
2009.
Entry generationby analogy?encoding new words formorphological lexicons.
NorthernEuropean Journal of Language Technology,1(1):1?25.Longacre, Robert E. 1964.
Grammar DiscoveryProcedures.
Mouton, The Hague.Mahlow, Cerstin and Michael Piotrowski.2009.
Preface.
In State of the Art inComputational Morphology: Proceedingsof the Workshop on Systems and Frameworksfor Computational Morphology, SFCM 2009,pages v?viii, Zurich.Majumder, Prasenjit, Mandar Mitra, andDipasree Pal.
2008.
Bulgarian, Hungarianand Czech stemming using YASS.
InAdvances in Multilingual and MultimodalInformation Retrieval: 8th Workshop ofthe Cross-Language Evaluation Forum,CLEF 2007, pages 49?56, Budapest.Majumder, Prasenjit, Mandar Mitra,Swapan K. Parui, Gobinda Kole, PabitraMitra, and Kalyankumar Datta.
2007.YASS: Yet another suffix stripper.ACM Transactions on InformationSystems, 25(4):18:1?20.Malahovskij, L. V. 1965.
Nac?al?nyj e?tapstatistiko-kombinatornogo modelirovanijamorfologii anglijskogo jazyka.
InNikolaj Dmitrievic?
Andreev, editor,Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad, pages 137?149.345Computational Linguistics Volume 37, Number 2Maxwell, Michael and Anne David.
2008.Joint grammar development by linguistsand computer scientists.
In Proceedings ofthe IJCNLP-08 Workshop on NLP for LessPrivileged Languages, pages 27?34,Hyderabad.Mayfield, James and Paul McNamee.
2003.Single n-gram stemming.
In SIGIR ?03:Proceedings of the 26th Annual InternationalACM SIGIR Conference on Research andDevelopment in Informaion Retrieval,pages 415?416, New York, NY.McClelland, James L. and David E.Rumelhart.
1986.
On learning the pasttenses of English verbs.
In ParallelDistributed Processing: Explorations in theMicrostructure of Cognition.
Volume 2:Psychological and Biological Models.
MITPress, Cambridge, MA, pages 216?271.McNamee, Paul.
2008.
Retrieval experimentsat Morpho Challenge 2008.
In WorkingNotes for the CLEF 2008 Workshop, Aarhus.McNamee, Paul and James Mayfield.
2007.N-gram morphemes for retrieval.
InWorking Notes for the CLEF 2007 Workshop,Budapest.Medina Urrea, Alfonso.
2000.
Automaticdiscovery of affixes by means of a corpus:A catalog of Spanish affixes.
Journal ofQuantitative Linguistics, 7(2):97?114.Medina Urrea, Alfonso.
2003.
Investigaci?ncuantitativa de afijos y cl?ticos del espa?ol deM?xico: Glutinometr?a en el Corpus delEspa?ol Mexicano Contempor?neo.Ph.D.
thesis, El Colegio de M?xico,M?xico, D.F.Medina-Urrea, Alfonso.
2006a.
Affixdiscovery by means of corpora:Experiments for Spanish, Czech, Ral?muliand Chuj.
In Alexander Mehler andReinhard K?hler, editors, Aspects ofAutomatic Text Analysis, volume 209 ofStudies in Fuzziness and Soft Computing.Springer, Berlin, pages 277?299.Medina Urrea, Alfonso.
2006b.
Towards theautomatic lemmatization of 16th centuryMexican Spanish: A stemming scheme forthe CHEM.
In Computational Linguistics andIntelligent Text Processing, 7th InternationalConference, CICLing 2006, pages 101?104,Mexico City.Medina-Urrea, Alfonso.
2008.
Affixdiscovery based on entropy and economymeasurements.
In Nicholas Gaylord,Alexis Palmer, and Elias Ponvert, editors,Computational Linguistics for Less-StudiedLanguages, volume X of Texas LinguisticsSociety.
CSLI Publications, Stanford, CA,pages 99?112.Medina Urrea, Alfonso and E. C. BuenrostroD?az.
2003.
Caracter?sticas cuantitativasde la flexi?n verbal del Chuj.
Estudios deLing?
?stica Aplicada, 38:15?31.Melkumjan, M. R. 1965.
Ishodnye dannyei statistiko-kombinatornoe vydelenieparadigmy pervogo morfologic?eskogotipa v armjanskom jazyke.
InNikolaj Dmitrievic?
Andreev, editor,Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad, pages 123?136.Mikheev, Andrei.
1997.
Automatic ruleinduction for unknown-word guessing.Computational Linguistics, 23(3):405?423.Mithun, Marianne.
1999.
The Languagesof Native North America.
CambridgeLanguage Surveys.
CambridgeUniversity Press, Cambridge.Monson, Christian.
2004.
A frameworkfor unsupervised natural languagemorphology induction.
In ACL 2004:Student Research Workshop, pages 67?72,Barcelona.Monson, Christian.
2009.
ParaMor: FromParadigm Structure to Natural LanguageMorphology Induction.
Ph.D. thesis,Carnegie Mellon University.Monson, Christian, Jaime Carbonell, AlonLavie, and Lori Levin.
2007a.
ParaMor:Finding paradigms across morphology.
InCarol Peters, Valentin Jijkoun, ThomasMandl, Henning M?ller, Douglas W. Oard,and Anselmo Pe?as, editors, Advances inMultilingual and Multimodal InformationRetrieval, 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007,Budapest, Hungary, September 19?21,2007, Revised Selected Papers,pages 892?899.
Springer-Verlag, Berlin.Monson, Christian, Jaime Carbonell, AlonLavie, and Lori Levin.
2007b.
ParaMor:Minimally supervised induction ofparadigm structure and morphologicalanalysis.
In Proceedings of Ninth Meetingof the ACL Special Interest Group inComputational Morphology and Phonology,pages 117?125, Prague.Monson, Christian, Jaime Carbonell, AlonLavie, and Lori Levin.
2008.
ParaMorand Morpho Challenge 2008.
Usingunsupervised paradigm acquisitionfor prefixes.
In Carol Peters, ThomasDeselaers, Nicola Ferro, Julio Gonzalo,Gareth J. F. Jones, Mikko Kurimo,Thomas Mandl, Anselmo Pe?as, andVivien Petras, editors, EvaluatingSystems for Multilingual and MultimodalInformation Access, 9th Workshop of theCross-Language Evaluation Forum,346Hammarstr?m and Borin Unsupervised Learning of MorphologyCLEF 2008, Aarthus, Denmark,September 17?19, 2008, Revised SelectedPapers, pages 967?974.
Springer-Verlag,Berlin.Monson, Christian, Alon Lavie, JaimeCarbonell, and Lori Levin.
2004.Unsupervised induction of naturallanguage morphology inflection classes.In SIGPHON 2004: Proceedings of theSeventh Meeting of the ACL Special InterestGroup in Computational Phonology,pages 52?61, Barcelona.Monson, Christian, Alon Lavie, JaimeCarbonell, and Lori Levin.
2008a.Evaluating an agglutinative segmentationmodel for ParaMor.
In Proceedings of theTenth Meeting of ACL Special Interest Groupon Computational Morphology and Phonology,pages 49?58, Columbus, OH.Monson, Christian, Ariadna Font Llitj?s,Vamshi Ambati, Lori Levin, Alon Lavie,Alison Alvarez, Roberto Aranovich,Jaime Carbonell, Robert Frederking,Erik Peterson, and Katharina Probst.2008b.
Linguistic structure and bilingualinformants help induce machinetranslation of lesser-resourced languages.In Proceedings of the Sixth InternationalLanguage Resources and Evaluation(LREC?08), pages 2854?2859, Marrakech.Moon, Taesun, Katrin Erk, and JasonBaldridge.
2009.
Unsupervisedmorphological segmentation andclustering with document boundaries.In Proceedings of the 2009 Conference onEmpirical Methods in Natural LanguageProcessing, pages 668?677, Singapore.Naradowsky, Jason and Sharon Goldwater.2009.
Improving morphology inductionby learning spelling rules.
In UCAI 2009,Proceedings of the 21st, InternationalJoint Conference on Artificial Intelligence,Pasadena, California, USA, July 11?17,2009, pages 1531?1537.Neuvel, Sylvain and Sean A. Fulop.
2002.Unsupervised learning of morphologywithout morphemes.
In Proceedings of theACL-02 Workshop on Morphological andPhonological Learning, Philadelphia,pages 31?40.Nida, Eugene A.
1949.
Morphology.
TheDescriptive Analysis of Words, 2nd edition.The University of Michigan Press, AnnArbor, MI.Nunzio, G. M. Di, N. Ferro, M. Melucci,and N. Orio.
2004.
Experiments to evaluateprobabilistic models for automatic stemmergeneration and query word translation.
InProceedings of the Cross-Language EvaluationForum (CLEF): Methodology and Metrics(CLEF 2003), pages 220?235.Oflazer, Kemal, Marjorie McShane, andSergei Nirenburg.
2001.
Bootstrappingmorphological analyzers by combininghuman elicitation and machine learning.Computational Linguistics, 27(1):59?85.Oliver, A.
2004.
Adquisici?
d?informaci?l?xica i morfosint?ctica a partir de corpussense anotar: aplicaci?
al rus i al croat.
Ph.D.thesis, Universitat de Barcelona.Ostler, Nicholas.
2008.
Is it globalization thatendangers languages?
In UNESCO/UNUConference: Globalization and Languages:Building our Rich Heritage, pages 206?211,Paris.O?igova, G. I.
1965.
Statistiko-kombinatornoemodelirovanie paradigmy pervogomorfologic?eskogo tipa v c?e?skom jazyke.In Nikolaj Dmitrievic?
Andreev, editor,Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad, pages 89?103.Pandey, Amaresh Kumar and Tanveer J.Siddiqui.
2008.
An unsupervised Hindistemmer with heuristic improvements.
InAND ?08: Proceedings of the Second Workshopon Analytics for Noisy Unstructured TextData, pages 99?105, New York, NY.Panina, N. A.
1965.
Opyt statistiko-kombinatornogo vydelenija paradigmypervogo morfologic?eskogo tipav fserbohorvatskom jazyke.
InNikolaj Dmitrievic?
Andreev, editor,Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad, pages 104?109.Per?ikov, V. F. 1965.
Iz opyta statistiko-kombinatornogo modelirovanija albanskojmorfologii.
In Nikolaj Dmitrievic?
Andreev,editor, Statistiko-kombinatornoe modelirovaniejazykov.
Nauka, Leningrad, pages 181?188.Petzell, Malin.
2007.
A lingustic descriptionof Kagulu.
Ph.D. thesis, G?teborgsUniversitet.Pirrelli, Vito, Basilio Calderone, IvanHerreros, and Michele Virgilio.
2004.Non-locality all the way through:Emergent global constraints in theItalian morphological lexicon.
InSIGPHON 2004: Proceedings of the SeventhMeeting of the ACL Special Interest Group inComputational Phonology, pages 52?61,Barcelona.Pirrelli, Vito and Ivan Herreros.
2007.Learning morphology by itself.
InProceedings of the Fifth MediterraneanMorphology Meeting (MMM5),pages 269?290, Fr?jus.Poon, Hoifung, Colin Cherry, and KristinaToutanova.
2009.
Unsupervised347Computational Linguistics Volume 37, Number 2morphological segmentation withlog-linear models.
In Proceedings ofNAACL ?09: The 2009 Annual Conferenceof the North American Chapter of theAssociation for Computational Linguistics,pages 209?217, Morristown, NJ.Powers, David M. W. 1998.
Reconciliation ofunsupervised clustering, segmentationand cohesion.
In NeMLaP3/CoNLL ?98Workshop on Paradigms and Grounding inLanguage Learning, Sydney, pages 307?310.Rai, Novel Kishore.
1984.
A Descriptive Studyof Bantawa.
Ph.D. thesis, Poona University.Redlich, A. Norman.
1993.
Redundancyreduction as a strategy for unsupervisedlearning.
Neural Computation, 5(2):289?304.Reiter, Ehud.
2007.
Last words: The shrinkinghorizons of computational linguistics.Computational Linguistics, 33(2):283?287.Roark, B. and Richard W. Sproat.
2007.Machine learning of morphology.
InComputational Approaches to Morphology andSyntax, volume 4 of Oxford Surveys inSyntax and Morphology.
Oxford UniversityPress, Oxford, pages 116?136.Rodrigues, Paul and Damir C?avar.
2005.Learning Arabic morphology usinginformation theory.
In The Panels 2005:Proceedings from the Annual Meeting of theChicago Linguistic Society, volume 41?2,pages 49?58, Chicago, IL.Rodrigues, Paul and Damir C?avar.
2007.Learning Arabic morphology usingstatistical constraint-satisfaction models.In Elabbas Benmamoun, editor,Perspectives on Arabic Linguistics: Papersfrom the Annual Symposium on ArabicLinguistics, pages 63?75, Urbana, IL.Rogati, Monica, Scott McCarley, and YimingYang.
2003.
Unsupervised learning ofArabic stemming using a parallel corpus.In Proceedings of the 41st Annual Meeting ofthe ACL, pages 391?398, Sapporo.Saxena, Anju and Lars Borin, editors.
2006.Lesser-Known Languages of South Asia:Status and Policies, Case Studies andApplications of Information Technology.Mouton de Gruyter, Berlin.Schone, Patrick.
2001.
Toward Knowledge-FreeInduction of Machine-Readable Dictionaries.Ph.D.
thesis, University of Colorado.Schone, Patrick and Daniel Jurafsky.
2000.Knowledge-free induction of inflectionalmorphologies using latent semanticanalysis.
In Conference on NaturalLanguage Learning 2000 (CoNLL-2000),pages 67?72, Lisbon.Schone, Patrick and Daniel Jurafsky.
2001a.Knowledge-free induction of inflectionalmorphologies.
In Proceedings of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 183?191,Pittsburgh, PA.Schone, Patrick and Daniel Jurafsky.
2001b.Language-independent induction of partof speech class labels using only languageuniversals.
In "Machine Learning: BeyondSupervision," Workshop at IJCAI-2001,pages 53?60, Seattle, WA.Sereewattana, Siriwan.
2003.
Unsupervisedsegmentation for statistical machinetranslation.
Master?s thesis, Universityof Edinburgh.Sharma, Utpal and Rajib Das.
2002.Classification of words based on affixevidence.
In International Conference onNatural Language Processing, ICON-2002,pages 31?39, Mumbai.Sharma, Utpal, Jugal Kalita, and Rajib Das.2002.
Unsupervised learning ofmorphology for building lexicon for ahighly inflectional language.
In Proceedingsof the 6th Workshop of the ACL SpecialInterest Group in Computational Phonology(SIGPHON), pages 1?10, Philadelphia.Sharma, Utpal, Jugal Kalita, and Rajib Das.2003.
Root word stemming by multipleevidence from corpus.
In Proceedingsof the 6th International Conference onComputational Intelligence and NaturalComputation (CINC), pages 1593?1596,Cary, NC.Snover, Matthew G. 2002.
An unsupervisedknowledge free algorithm for thelearning of morphology in naturallanguages.
Master?s thesis, Departmentof Computer Science, WashingtonUniversity.Snover, Matthew G. and Michael R. Brent.2001.
A Bayesian model for morphemeand paradigm identification.
In Proceedingsof the 39th Annual Meeting of the Associationfor Computational Linguistics (ACL-2001),pages 482?490.Snover, Matthew G. and Michael R. Brent.2003.
A probabilistic model for learningconcatenative morphology.
In S. Becker,S.
Thrun, and K. Obermayer, editors,Advances in Neural Information ProcessingSystems 15.
MIT Press, Cambridge, MA,pages 1513?1520.Snover, Matthew G., Gaja E. Jarosz, andMichael R. Brent.
2002.
Unsupervisedlearning of morphology using a noveldirected search algorithm: Taking the firststep.
In Proceedings of the ACL-02 Workshopon Morphological and Phonological Learning,pages 11?20, Philadelphia.348Hammarstr?m and Borin Unsupervised Learning of MorphologySnyder, Benjamin and Regina Barzilay.2008.
Unsupervised multilingual learningfor morphological segmentation.In Proceedings of ACL-08: HLT,pages 737?745, Columbus, OH.Spencer, Andrew and Arnold M. Zwicky,editors.
1998.
The Handbook of Morphology.Blackwell, Oxford.Spiegler, Sebastian, Bruno Gol?nia, KseniaShalonova, Peter Flach, and Roger Tucker.2008.
Learning the morphology of Zuluwith different degrees of supervision.
InSpoken Language Technology Workshop,2008 (SLT 2008), pages 9?12, Goa.Strassel, Stephanie, Mike Maxwell, andChristopher Cieri.
2003.
Linguisticresource creation for research andtechnology development: A recentexperiment.
ACM Transactions on AsianLanguage Processing, 2(2):101?117.Tepper, Michael.
2007.
Knowledge-liteinduction of underlying morphology: Ahybrid approach to learning morphemesusing context-sensitive rewrite rules.Master?s thesis, University of Washington.Tepper, Michael and Fei Xia.
2008.
A hybridapproach to the induction of underlyingmorphology.
In Proceedings of the ThirdInternational Joint Conference on NaturalLanguage Processing (IJCNLP 2008),pages 17?24, Hyderabad.Theron, Pieter and Ian Cloete.
1997.Automatic acquisition of two-levelmorphological rules.
In Fifth Conferenceon Applied Natural Language Processing,pages 103?110, Washington, DC.Trosterud, Trond.
2004.
Portingmorphological analysis anddisambiguation to new languages.In SALTMIL Workshop at LREC 2004:First Steps in Language Documentationfor Minority Languages, pages 90?92,Lisbon.Trosterud, Trond.
2006.
Grammaticallybased language technology for minoritylanguages.
In Anju Saxena and LarsBorin, editors, Lesser-Known Languagesof South Asia: Status and Policies, CaseStudies and Applications of InformationTechnology.
Mouton de Gruyter, Berlin,pages 293?315.Tufis, Dan.
1989.
It would be much easier ifWENT were GOED.
In Proceedings of theFourth Conference of the European Chapter ofthe ACL, pages 145?152, Manchester.ur Rehman, Khalid and Iftikhar Hussain.2005.
Unsupervised morphemessegmentation.
In Mikko Kurimo, MathiasCreutz, and Krista Lagus, editors,Proceedings of MorphoChallenge 2005,pages 52?56, Helsinki University ofTechnology, Helsinki.Virpioja, Sami, Jaakko J. V?yrynen, MathiasCreutz, and Markus Sadeniemi.
2007.Morphology-aware statistical machinetranslation based on morphs induced inan unsupervised manner.
In Proceedingsof Machine Translation Summit XI,pages 391?498, Copenhagen.Wicentowski, Richard.
2002.
Modelingand Learning Multilingual InflectionalMorphology in a Minimally SupervisedFramework.
Ph.D. thesis, Johns HopkinsUniversity, Baltimore, MD.Wicentowski, Richard.
2004.
Multilingualnoise-robust supervised morphologicalanalysis using the wordframe model.In Proceedings of the ACL Special InterestGroup on Computational Phonology(SIGPHON), pages 70?77, Barcelona.Wintner, Shuly.
2009.
Last words: Whatscience underlies natural languageengineering?
Computational Linguistics,35(4):641?644.Wothke, Klaus.
1986.
Machine learning ofmorphological rules by generalizationand analogy.
In Proceedings of the 11thConference on Computational Linguistics,pages 289?293, Morristown, NJ.Wothke, Klaus Christian.
1985.
MaschinelleErlernung und Simulation morphologischerAbleitungsregeln.
Ph.D. thesis, RheinischeFriedrich-Wilhelms-Universit?t zu Bonn.Xanthos, Aris.
2007.
Apprentissageautomatique de la morphologie: Le cas desstructures racine-sch?me.
Ph.D. thesis,Universit?
de Lausanne.Xanthos, Aris, Yu Hu, and John Goldsmith.2006.
Exploring variant definitions ofpointer length in MDL.
In Proceedings of theEighth Meeting of the ACL Special InterestGroup on Computational Phonology andMorphology at HLT-NAACL 2006,pages 32?40.Yarowsky, David, Grace Ngai, and RichardWicentowski.
2001.
Inducing multilingualtext analysis tools via robust projectionacross aligned corpora.
In Proceedings of theFirst International Conference on HumanLanguage Technology Research, Stroudsburg,PA, pages 1?8.Yarowsky, David and Richard Wicentowski.2000.
Minimally supervised morphologicalanalysis by multimodal alignment.
InProceedings of the 38th Annual Meetingof the Association for ComputationalLinguistics (ACL-2000), pages 207?216,Hong Kong.349Computational Linguistics Volume 37, Number 2Yvon, Fran?ois.
1996.
Prononcer par analogie:motivation, formalisation et evaluation.
Ph.D.thesis, ?cole Nationale Sup?rieure desT?l?communications, Paris.Zeman, Daniel.
2008.
Unsupervisedacquiring of morphological paradigmsfrom tokenized text.
In Advances inMultilingual and Multimodal InformationRetrieval: 8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007,pages 892?899, Budapest.Zeman, Daniel.
2009.
Using unsupervisedparadigm acquisition for prefixes.
In CarolPeters, Thomas Deselaers, Nicola Ferro,Julio Gonzalo, Gareth J. F. Jones, MikkoKurimo, Thomas Mandl, Anselmo Pe?as,and Vivien Petras, editors, EvaluatingSystems for Multilingual and MultimodalInformation Access, 9th Workshop of theCross-Language Evaluation Forum,CLEF 2008, Aarthus, Denmark,September 17?19, 2008, Revised SelectedPapers, pages 983?990.
Springer-Verlag,Berlin.Zhang, Byoung-Tak and Yung-Taek Kim.1990.
Morphological analysis andsynthesis by automated discovery andacquisition of linguistic rules.
In PapersPresented to the 13th International Conferenceon Computational Linguistics (COLING1990), volume 2, pages 431?436, Helsinki.Zweigenbaum, P., F. Hadouche, andN.
Grabar.
2003.
Apprentissage derelations morphologiques en corpus.In Actes de TALN 2003, pages 285?294,Batz-sur-mer.350
