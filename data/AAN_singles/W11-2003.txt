Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 8?17,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsWhich System Differences Matter?Using `1/`2 Regularization to Compare Dialogue SystemsJose?
P. Gonza?lez-Brenes and Jack MostowProject LISTENLanguage Technologies InstituteSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{joseg,mostow}@cs.cmu.eduAbstractWe investigate how to jointly explain the per-formance and behavioral differences of twospoken dialogue systems.
The Join Evalu-ation and Differences Identification (JEDI),finds differences between systems relevantto performance by formulating the problemas a multi-task feature selection question.JEDI provides evidence on the usefulness ofa recent method, `1/`p-regularized regres-sion (Obozinski et al, 2007).
We evaluateagainst manually annotated success criteriafrom real users interacting with five differentspoken user interfaces that give bus scheduleinformation.1 IntroductionThis paper addresses the problem of how to deter-mine which differences between two versions of asystem affect their behavior.
Researchers in Spo-ken Dialogue Systems (SDSs) can be perplexed as towhich of the differences between alternative systemsaffect performance metrics (Bacchiani et al, 2008).For example, when testing on real users at differ-ent periods of time, the variance of the performancemetrics might be higher than the difference betweensystems, causing (i) significantly different scores inidentical systems deployed at different times, and(ii) the same score on different systems (Gonza?lez-Brenes et al, 2009).We approach the problem of finding which systemdifferences matter by describing dialogues as featurevectors constructed from the logs of dialogs gener-ated by the SDSs interacting with real users.
Hence,we aim to identify features that jointly characterizethe system differences and the performance of theSDS being evaluated.
These features should be ableto (i) predict a performance metric and (ii) distin-guish between the two SDS being evaluated.The main contribution of this paper is a novel al-gorithm for detecting differences between two sys-tems that can explain performance.
Additionally, weprovide details on how to implement state-of-the-artmulti-task learning for SDSs.The rest of this manuscript is organized as fol-lows.
Section 2 reviews multi-task feature selection.Section 3 describes two algorithms to find whichsystem differences matter.
Section 4 describes thespecific SDS used to illustrate our algorithms.
Sec-tion 5 presents some experimental results.
Section 6reviews related prior work.
Section 7 presents someconcluding remarks and future work.
Appendix Aprovides implementation details of the multi-tasklearning approach we used.2 Feature SelectionIn this section we describe how we use regression toperform feature selection.
Feature selection meth-ods construct and select subsets of features in orderto build a good predictor.
We focus our attention onfeature selection methods that use complexity (regu-larization) penalties, because of their recent theoret-ical and experimental success (Yuan and Lin, 2006;Park and Hastie, 2007).
We provide a more rigorousdescription of how to implement this formulation asan optimization problem in Appendix A.We use labels to encode the output we want topredict.
For example, if our performance metric isbinary, we label successful dialogues with a +1, andunsuccesful dialogues with a ?1.
Given a trainingset consisting of labeled dialogues, we want to learn8a model that assigns a label to unseen dialogues.
Wefollow an approach called empirical risk minimiza-tion (Obozinski et al, 2007), that aims to minimizethe error of fitting the training data, while penalizingthe complexity of the model:Minimize Model loss + ?
Complexity (1)Here the hyper-parameter ?
controls the trade-offbetween a better fit to the training data (with a higherrisk of over-fitting it), and a simpler model, withfewer features selected (and less predictive power).We now review the two components of risk mini-mization, model loss and complexity penalty.2.1 Model LossWe model probabilistically the loss of our modelagainst the real-life phenomenon studied.
Given adialogue x, with correct label l, its loss using amodel ?
is:loss?
(y?, x) ?
P (y = l|x; reality)?
P (y?
= l|x;?
)(2)Here y?
is the predicted value of the event y.
Since lis the true label, P (y = l|x; reality) = 1.
To get theoverall loss of the model, we aggregate over the pre-diction loss of each of the dialogues in the trainingset by summing their individual loss calculated withEquation 2.
Let X = {x(1), x(2), .
.
.
x(n)} be the ndialogues in the training set.
Then the overall loss ofmodel ?
is:loss?
(y(1), x(1)) + ?
?
?+ loss?
(y(n), x(n))Since we use discrete labels, we use a logisticfunction to model their probability.
Let x1, .
.
.
xkbe the k features extracted from dialogue x. Thenthe logistic regression model is:P (y?
= +1|x;?)
=1Zexp(?1x1 + ?
?
?+ ?kxk)Here ?1...?k are the parameters of the model, andZ simply normalizes P to ensure that P is a validprobability function (the range of P should be 0 to1):Z = 1 + exp(?1x1 + ?
?
?+ ?kxk)Multi-task learning solves related regressionproblems at the same time using a shared representa-tion.
We now describe the risk-minimization formu-lation for multi-task learning.
Let ym be the valueof the performance metric.
Let ys be the label of thesystem that generated the dialogue.
The individualdialogue loss of using models ?m and ?s is:loss?m(y?m, x) + loss?s(y?s, x)2.2 Complexity PenaltiesWe consider a feature xi to be selected into themodel if its regression coefficient ?i is non-zero.Complexity penalties encourage selecting only a fewfeatures.
We review several commonly used penal-ties (Zou and Hastie, 2005):?
`2 Penalty.
Under some circumstances `2penalties perform better than other types ofpenalties (Zou and Hastie, 2005).
The `2penalty for a model ?
is:||?||`2 ??
(?1)2 + ?
?
?+ (?k)2?
`1 Penalty.
An `1 penalty induces sparsity bysetting many parameters of the model ?
to ex-actly zero (Tibshirani, 1996).||?||`1 ?
|?1|+ ?
?
?+ |?k|?
`1/`2 Penalty.
Yuan and Lin (2006) proposeda group penalty for penalizing groups of fea-tures simultaneously.
Previous work has shownthat grouping features between tasks encour-ages features to be used either by all tasks orby none (Turlach et al, 2005; Obozinski etal., 2007; Lounici et al, 2009; Puniyani et al,2010).
Our `1/`2 penalty is:?????
(?m1 )2 + (?s1)2???
?+ ...+?????
(?mk )2 + (?sk)2???
?3 Finding Features that PredictPerformance and System DifferencesWe find system differences that are predictive ofSDS performance, relying on:?
Describing dialogues as feature vectors.
Thebehavior of the systems must be describableby features extracted from the logs of the sys-tems.
A discussion of feature engineering fordialogue systems is found in (Gonza?lez-Brenesand Mostow, 2011).9?
Finding system differences.
The features of aclassifier that distinguishes between SDSs, canbe used to identify their differences (Gonza?lez-Brenes et al, 2009).
When comparing twoSDSs, we label the baseline system with ?1,and the alternate version with +1.?
Modeling performance.
Although our ap-proach does not depend on a specific perfor-mance metric, in this paper we use dialoguesuccess, a binary indicator that triggers thatthe user?s query was answered by the SDS.Task completion is cheaper to compute than di-alogue success, as it does not require a man-ual human labeled reference, but we considerthat dialogue success is a more accurate metric.Task completion is used in commercial applica-tions (Bacchiani et al, 2008), and has been ex-tensively studied in the literature (Walker et al,2001; Walker et al, 2002; Hajdinjak and Mi-helic, 2006; Levin and Pieraccini, 2006; Mo?lleret al, 2007; Mo?ller et al, 2008; Schmitt etal., 2010).
We encode success of dialogues bymanually annotating them with a binary vari-able that distinguishes if the user query is ful-filled by the SDS.We now present two algorithms to find what dif-ferences matter between systems.
We introduce Se-rial EvaluatioN Analysis (SERENA) as a scaffoldfor the Join Evaluation and Differences Identifica-tion (JEDI) algorithm.3.1 SERENA algorithmThe input to SERENA is a collection of log filescreated by two different SDSs and two functions thatrepresent the correct label for the regression tasks.In our case these functions should return binary la-bels (+1,?1): one task distinguishes between suc-cessful and unsuccessful dialogues, and the othertask distinguishes a baseline from an alternative SDSversion.
SERENA?s objective is to select featuresfrom one task, and use them to predict the other task.For example, SERENA selects features that predictdifferences between versions, and uses them to pre-dict performance.Algorithm 1 provides the pseudo-code for SER-ENA.
Line 1 builds the training set X from parsingthe logs of the SDSs.
Lines 2 and 3 create the outputAlgorithm 1 SERENA algorithmRequire: Logs1, Logs2 are the collections of SDSlogs of two systems.
task1, task2 are func-tions that return the value of a performance met-ric, and which system is being evaluated (?1 ifis the baseline, +1 otherwise).1: X?
extract features(Log1,Log2)2: yt1 ?
[task1(Logs1)task1(Logs2)]3: yt2 ?
[task2(Logs1)task2(Logs2)]4: // Select features that explain both tasks:5: for ?
= {0.1, 0.2, .
.
. }
do6: ?t1 ?
regression`1(X,yt1 , ?
)7: // Get feature weights:8: X?
?
X; where xk|?xk ?
X?, ?t1k 6= 09: ??
?
regression`2(X?,yt2 , ?c)10: end for11: return ?
?variables y for the regression tasks.
Line 6 returnsthe most predictive features using `1 regularizationas described in Section 2.
Line 8 builds a new train-ing set, removing the features that were not selectedin line 6.
Line 9 builds the final coefficients by fittinga `2-regularized model using a constant ?c.
We cal-culate the coefficients using an `2 penalty, becauseit has a better fit to the data (Zou and Hastie, 2005).Moreover, by using the same penalty, we control forthe idiosyncrasies different penalties have in param-eter learning.
In the experiments described in Sec-tion 5, all of our experiments are reported fitting a`2-regularized models.SERENA is not conmutative with regards to theorder of the tasks: selecting the features that predictperformance and using them to predict system dif-ferences is not the same as the reverse.
More impor-tantly, SERENA only searches in one of the tasks ata time.
We are interested in finding the features thatexplain both tasks simultaneously.
In the next sub-section we describe JEDI which makes use of recentadvances in multi-task feature selection in order tofind the features for both tasks at the same time.3.2 JEDI algorithmAlgorithm 2 provides the pseudo-code for JEDI.JEDI uses multi-task regression to find the fea-tures that affect performance and system differences10Algorithm 2 JEDI algorithmRequire: Logs1, Logs2 are the collections of SDSlogs of two systems.
task1, task2 are func-tions that return the value of a performance met-ric, and which system is being evaluated (?1 ifis the baseline, +1 otherwise).1: X?
extract features(Log1,Log2)2: yt1 ?
[task1(Logs1)task1(Logs2)]3: yt2 ?
[task2(Logs1)task2(Logs2)]4: // Select features that explain both tasks:5: for ?
= {0.1, 0.2, .
.
. }
do6: ?t1?t2 ?
regression`1/`2(X, ~yt1 , ~yt2 , ?
)7: // Get feature weights:8: X?
?
X; where xk|?xk ?
X?, ?t1k 6= 09: ??
?
regression`2(X?,yt2 , ?c)10: end for11: return ??jointly.
To ensure a sparse feature set across thetasks, it applies a `1/`2 penalty.
In this way, a featureis either selected to predict performance and systemdifferences, or it is not selected at all.
In the nextsection we review a concrete example of a SDS inwhich we apply SERENA and JEDI.4 Case StudyWe use data collected from Let?s Go!, an experi-mentary SDS available for research purposes1 thatprovides bus planning information to the PittsburghEast End community (Raux et al, 2006).
Figure 1shows a sample dialogue.We assembled three disjoint datasets to have di-alogues just before and after what we consideredimportant architectural differences of Let?s Go?s im-plementation.
By going through the change log, werealize that the magnitude of the differences varieswidely.
Table 1 describes the date the systems weredeployed, the description of the change in the SDS,and the number of dialogues in each set.
From the86 dialogues of ?Set A?, half are a random sam-ple of the logs before the acoustic model was re-trained (Log1), and the other half is a sample af-ter the acoustic model was retrained (Log2).
In thecase of the 84 dialogues of ?Set B?, half of the logs1http://www.dialrc.org/System: Welcome to the CMU Let?s Go!Bus Information System.
Howmay I help you?User: I?d like to go to Forbes and Mur-ray.System: To Forbes and Murray.
Whereare you leaving from?User: Carnegie Mellon.System: From Carnegie Mellon.
Thereare five buses running betweenCarnegie Mellon and Forbes andMurray: the 501, the 59U, the61A, the 61B, and the 61C.What else can I do for you?User: When is the next one?System: The next 61C going to Forbesand Murray will leave CarnegieMellon at 5:13 PM.Figure 1: Example of a Let?s Go dialogue, from (Raux etal., 2006)use an older version of the Olympus SDS infrastruc-ture (Bohus et al, 2007), and the other half usesOlympus II.
Since each system was deployed in adifferent period of time, we want to corroborate thatwe are modeling the differences among systems, andnot seasonal.
Hence, for control conditions, we alsochose a data set that contained no major change tothe system or to other conditions (Set C).Sets were built by randomly sampling from thecollection of logs.
They have the same number of di-alogues from each SDS version (baseline/alternate).Each dialogue was manually annotated to indicatewhether the user?s query was fulfilled, and we re-moved from our analysis the two dialogues that wereonly partially fulfilled.
The number of successful di-alogues is different from the number of unsuccessfuldialogues.We created a script to extract features from the logfiles of Let?s Go!.
The script has an explicit list offeatures to extract from the event logs, such as thewords that were identified by the Automatic SpeechRecognizer.
Although this script is dependent on ourspecific log format, it should be a simple program-ming task to adapt it to a different dialogue system,provided its logs are comprehensive enough.
The11Table 1: Dataset DescriptionSet Size Description DateA 86Baseline 8/05 10/05New acoustic model 12/05 2/05B 86Baseline 8/06 10/06New SDS architecture 6/07 7/07C 84Baseline 10/07 11/07No change 11/07 12/07script performs the standard transformation of cen-tering feature values as z-scores with mean zero andstandard deviation one.Table 2 summarizes the properties we are inter-ested to model.
Dialogue properties are the featuresthat summarize the behavior of the whole dialogue,and turn properties work at a finer-grain.
We encodeturn properties into features in the following way:?
Global average.
Turn properties are averagedover the entire dialogue.?
Beginning window.
Turn properties are aver-aged across an initial window.
Based on pre-liminary experiments, we defined the windowas the first 5 turns.?
State.
We relied on the fact that SDSs are of-ten engineered as finite state automata (Bohuset al, 2007).
Properties are averaged across thestates that belong to a specific dialogue state(for example, asking departure place).
Becausewe are interested in early identification of dif-ferences, we restricted state features to be in-side the beginning window.5 EvaluationWe assess the performance of our algorithms byevaluating the classification accuracy using the fea-tures selected.
To facilitate assessment of SDS, weonly consider models that select up to 15 features.Figure 2 reports mean classification accuracy usingfive-fold cross-validation.
Its first column describeshow well the features selected perform on detectingsystem differences, and the second column describeshow well they predict task success as a performancemetric.
We compare JEDI and SERENA against thefollowing approaches:Table 2: FeaturesDialogue Properties# of re-prompted turns# of turnsMean Dialogue lengthis evening?, is weekend?, 0-23 hourTurn PropertiesOccurrences of word w# of parse errors# of unrecognized words# of words# of repeated words# of unique wordsTurn lengthWords per minuteFailed prompts (number and percentage)Mean Utterance LengthBarge-in (in seconds)Machine-user pause (in seconds)User-machine pause (in seconds)Amplitude (power) statistics?
Majority classifier baseline.
A classifier thatalways selects the majority class (datasets Band C are not balanced in the number of suc-cessful dialogues).?
Same Task Classifier We report the classifica-tion accuracy of the model trained and testedon the same task.
Features are selected usingan `1 penalty, and the coefficients are estimatedwith `2-regularized logistic regression.
For ex-ample, in the column of the left, SERENA usesthe most predictive features of system differ-ences to predict success, while the same taskclassifier uses them to predict system differ-ences.
The same task classifier does not answer?which system differences matter?, it is just aninteresting benchmark.We used a one-sample t-test to check for sta-tistically significant differences against the classifi-cation accuracy of the majority classifier baseline.We used a paired-sample t-test to check for sig-nificant differences in classification accuracy be-tween classifiers.
Paired samples have the same ?hyper-parameter, which was described in the risk-120 5 10 150.40.50.60.70.80.91  System Differences# of featuresDatasetAClassificationAcc.0 5 10 150.40.50.60.70.80.91 Dialogue Success# of featuresClassificationAcc.0 5 10 150.40.50.60.70.80.91# of featuresDatasetBClassificationAcc.0 5 10 150.40.50.60.70.80.91# of featuresClassificationAcc.0 5 10 150.40.50.60.70.80.91# of featuresDatasetCClassificationAcc.0 5 10 150.40.50.60.70.80.91# of featuresClassificationAcc.Majority SERENA JEDI Same Task ClassifierFigure 2: Classification accuracy of different feature se-lection techniquesminimization formulation explained in Section 2.This hyper-parameter is related to the number of fea-tures selected ?
as ?
increases, the number of fea-tures selected decreases.
We use 5% as the signif-icance level at which to reject the null hypothesis.When checking for statistical differences, we testedon the range of ?s computed2.First we investigate the performance of the sim-pler algorithm SERENA.
For Dataset A, SERENAdoes not yield significant differences over the ma-jority classifier baseline.
For Dataset B, SERENAis significantly better than the majority classifier inpredicting system differences, but is significantlyworse for predicting success.
This means that the or-der in which we choose the tasks in SERENA affectsits performance.
SERENA performs significantlyworse in the Control Set C. We conclude that SER-2?
= {100, 30, 25, 20, 19, 18, .
.
.
, 1, 0.5, 0.25, 0.1}Table 3: Features selected in Dataset AFeature Suc.
Diff.
JEDISystem-user pause 5 5Weekend night?
3% of failed prompts 4?Forbes St.?
word 5 3User?s max.
power 5Table 4: Features selected in Dataset BFeature Suc.
Diff.
JEDI% of failed prompts 5 4User?s power std.dev.
5Weekend night?
3Unrecognized word 5Words/min.
4User-system pause 5Turn length 5 5ENA is not very reliable in predicting which systemdifferences matter.We now discuss how well JEDI is able to fill-in forthe deficiencies of SERENA.
As an ?upper-bound?,we will compare it to a classifier trained and testedin the same task.
This classifier significantly dom-inates over the majority baseline, even for the theControl Set C, where there were no changes in theSDS.
This suggests that the classifier might be pick-ing up on seasonal differences.
For Set A, JEDI per-forms significantly better than the majority classi-fier and than SERENA.
For Set B, there are no sig-nificant differences between the upper-bound clas-sifier and JEDI when predicting for changes in theSDS.
Again, JEDI dominates over SERENA and themajority baseline.
For the Control Set C, JEDI isnot statistically different from the majority baseline.This is the expected behavior, since the difference inperformance cannot be explained by the differencesbetween the SDS.
We hypothesize that the classifi-cation accuracy of JEDI could be used as a distancefunction between SDS: The closer the accuracy ofdistinguishing SDS is to 50%, the more similar theSDSs are.
Conversely, when JEDI is able to classifysystem differences closer to 100%, it is because theSDSs are more different.Tables 3 and 4 describe the features selected forSets A and B respectively.
The numbers indicate13in how many folds the feature was selected by JEDIand by classifiers trained to predict Success and SDSdifferences using five-fold cross validation.
The ?used is selected to contain the closest to five features(ties are resolved randomly).
We only report fea-tures that appeared in at least three folds.
In DatasetA we see that time of day is selected to predict di-alogue success.
Anecdotally, we have noticed thatmany users during weekend nights appear to be in-toxicated when calling the system.
JEDI does notselect ?is weekend night?
as a feature, because ithas little predictive power to detect system differ-ences.
In Dataset A, JEDI selects a speech recogni-tion feature (the token ?Forbes St?
was recognized),and an end-pointing feature.
Since in Dataset A, thedifference between systems correspond to a differ-ent acoustic model, these features make sense intu-itively.
In Dataset B, JEDI detected that the featuresmost predictive with system differences and successare percentage of failed prompts and the length ofthe turn.
The models for both systems make senseafter the fact.
However, neither model was knownbeforehand, nor did we know which of many fea-tures considered would turn out to be informative.Anecdotally, the documentation of the history ofchanges of Let?s Go!
is maintained manually.
Some-times, because of human error, this history is incom-plete.
The ability of JEDI to identify system differ-ences has been able to help completing the historyof changes (Gonza?lez-Brenes et al, 2009).6 Relation to Prior WorkThe scientific literature offers several performancemetrics to assess SDS performance (Polifroni et al,1992; Danieli and Gerbino, 1995; Bacchiani et al,2008; Suendermann et al, 2010).
SDS are eval-uated using different objective and subjective met-rics.
Examples of objective metrics are the meannumber of turns in the dialogue, and dialogue suc-cess.
Subjective evaluations study measure satisfac-tion through controlled user studies.
Ai et al (2007)studied the differences in using assessment metricswith real users and paid users.PARADISE, a notable example of a SDS subjec-tive evaluation, finds linear predictors of a satisfac-tion score using automatic and hand-labeled features(Hajdinjak and Mihelic, 2006; Walker et al, 2001),or only automatic features (Hastie et al, 2002).
Sat-isfaction scores are calibrated using surveys in con-trolled experiments (Mo?ller et al, 2007; Mo?ller etal., 2008).
Alternatively, Eckert et al (1998) pro-posed simulated users to evaluate SDSs.
Their per-formance metric has to be tuned with a subjectiveevaluation as well, in which they refer to the PAR-ADISE methodology.
Our approach does not re-quire user surveys to be calibrated.
Moreover, itwould be feasible to adapt JEDI to regress to PAR-ADISE, or other performance metrics.
Our work ex-tends previous studies that define performance met-rics, in proposing an algorithm that finds how systemdifferences are related to performance.7 Conclusions and Future WorkWe have presented JEDI, a novel algorithm that findsfeatures describing system differences relevant to asuccess metric.
This is a novel, automated ?glassbox?
assessment in the sense of linking changes inoverall performance to specific behavioral changes.JEDI is an application of feature selection using reg-ularized regression.We have presented empirical evidence suggestingthat JEDI?s use of multi-task feature selection per-forms better than single-task feature selection.
Fu-ture work could extend JEDI to quantify the vari-ability in performance explained by the differencesfound.
Common techniques in econometrics, suchas the Seemingly Unrelated Regressions (SUR) for-mulation (Zellner, 1962), may prove useful for this.In our approach we used a single binary evalu-ation criterion.
By using a different loss function,JEDI can be extended to allow continuous-valuedmetrics.
Moreover, previous work has argued thatevaluating SDSs should not be based on just a sin-gle criterion (Paek, 2001).
JEDI?s multi-task for-mulation can be extended to include more than oneperformance criterion at the same time, and mayprove helpful to understand trade-offs among differ-ent evaluation criteria.A Implementation Details of FeatureSelectionIn this appendix we review how to set-up multi-taskfeature selection as an optimization problem.14A.1 `1-Regularized Regression for Single-TaskFeature SelectionWe first review using regression with `1 regulariza-tion for single-task feature selection.
Given a train-ing set represented by X, denoting a n ?
k matrix,where n is the number of dialogues, and k is thenumber of features extracted for each dialogue, wewant to find the coefficients of the parameter vector~?, that can predict the output variables described inthe vector ~y of length n.For this, we find the parameter vector that mini-mizes the loss function J , penalized by a regulariza-tion term (Tibshirani, 1996):argmin~?J(X, ~?, ~y) + ?||~?||`1 (3)In the case of binary classification, outputs are bi-nary (any given y = ?1).
A commonly used lossfunction J is the Logistic Loss:Jlog(x, ?, y) ?11 + ey(x??
)(4)The `p-norm of a vector ~?
is defined as:||~?||`p ?
(k?i=1|?i|p)1/pThe `?-norm is defined as ||~?||`?
?max(?1, ?2, .
.
.
, ?k).The regularization term ||~?||`1 in Equation 3 con-trols model complexity: The higher the value of thehyper-parameter ?, the smaller number of featuresselected.
Conversely, the smaller the value of ?,the better the fit to the training data, with higherrisk of over-fitting it.
Thus, Equation 3 jointly per-forms feature selection and parameter estimation; itinduces sparsity by setting many coefficients of ~?to zero (Tibshirani, 1996).
Features with non-zerocoefficients are considered the features selected.A.2 `1-Regularized Regression for Multi-TaskFeature Selection`1 regularization can be used to learn a classifier foreach of T prediction task independently.
In our casewe are interested in only two prediction tasks: ver-sion and success.
We will index tasks with super-script t, and we define Xt as the n ?
k trainingdata for task t, used to predict the output variable ~yt.Learning each model separately yields the followingoptimization problem (Obozinski et al, 2007):argmin~?tT?t=1J(Xt, ~?t, ~yt) + ?||~?t||`1 (5)Solving this problem leads to individual sparsity ineach task (each ~?thas many zeros), but the modeldoes not enforce a common subset of features forall of the related output variables simultaneously(Turlach et al, 2005).
In the next subsection westudy how to achieve global sparsity across tasks.A.3 `1/`p-Regularized Regression forMulti-task Feature SelectionAlthough `1-regularization is very successful at se-lecting individual features, it does not perform ad-equately when a group of features should enteror leave the model simultaneously (Yuan and Lin,2006).
Group LASSO (Yuan and Lin, 2006), whichrelies on `1/`p-regularization to overcome this lim-itation, by allowing groups of feature entering orleaving the model simultaneously.
`1/`p regular-ization has been studied for multi-task learning bygrouping each of the k features across the T learningtasks (Turlach et al, 2005; Obozinski et al, 2007;Lounici et al, 2009; Puniyani et al, 2010).Let us define B as a n?
T matrix, whose tth col-umn is the parameter vector for the task t. For ex-ample, since we have two tasks B = [~?t=1, ~?t=2].Let ~?g denote the gth row of B.
In the context ofmulti-task learning, the `1/`p-norm of a matrix B isdefined as (Obozinski et al, 2007; Puniyani et al,2010):||B||`1/`p ?k?g=1||~?g||`p (6)Multi-task feature selection with `1/`p regular-ization is formulated as (Obozinski et al, 2007;Puniyani et al, 2010):argminBT?t=1J(Xt, ~?t, ~yt) + ?||B||`1/`2 (7)When T = 1, the multi-task problem of Equation 7reduces to the single-task problem of Equation 5.15A.4 Optimization procedurePuniyani et al (2010) describe that finding the pa-rameter coefficients B of Equation 7 can be achievedmore easily by transforming the problem into anequivalent single-task multivariate regression.
Wefollow their procedure to create ~yg, ~?g and Xg:1.
Concatenate the vectors ~yt?s into a single vec-tor ~yg of length n ?
T .
In our case, since wehave only two tasks (T = 2), we get the vector~yg =[ ~yt=1~yt=2].2.
Similarly, we concatenate the ~?t?s into a k?
Tvector ~?g, in our case ~?g =[ ~?t=1~?t=2].3.
Build a (n ?
T )?
(k ?
T ) block-diagonal matrixXg, where Xt?s are placed along the diagonal,and the rest of the elements are set to zero.
Inour case since we only have two tasks this isXg =[Xt=1 ??
Xt=2], where each ?
denotes an ?
k zero-matrix.
The expanded notation ofXg is:Xg ????????????
?xt=1(1)1 ... xt=1(1)k 0 ... 0............xt=1(n)1 ... xt=1(n)k 0 ... 00 ... 0 xt=2(1)1 ... xt=2(1)k............0 ... 0 xt=2(n)1 ...
xt=2(n)k???????????
?Thus, the multi-task learning problem from Equa-tion 7 is equivalent to (Yuan and Lin, 2006; Puniyaniet al, 2010):argminBJ(Xg, ~?g, ~yg) + ?||B||`1/`2 (8)In this work we solve this optimization problem us-ing an existing3 implementation of Block Coordi-nate Descent (Schmidt et al, 2008) that solves re-gression problems with a `1/`p penalty.AcknowledgmentsThis work was supported by the Institute of Ed-ucation Sciences, U.S. Department of Education,3Source code: http://www.cs.ubc.ca/?murphyk/Software/L1CRF/through Grant R305A080628 to Carnegie MellonUniversity.
The opinions expressed are those of theauthors and do not necessarily represent the views ofthe Institute or U.S. Department of Education.
Wethank the educators, students, and LISTENers whohelped generate, collect, and analyze our data, andthe reviewers for their helpful comments.
The firstauthor was partially supported by the Costa RicanMinistry of Science and Technology (MICIT).ReferencesH.
Ai, A. Raux, D. Bohus, M. Eskenazi, and D. Lit-man.
2007.
Comparing spoken dialog corpora col-lected with recruited subjects versus real users.
InProc.
of the 8th SIGdial workshop on Discourse andDialogue.M.
Bacchiani, F. Beaufays, J. Schalkwyk, M. Schuster,and B. Strope.
2008.
Deploying GOOG-411: Earlylessons in data, measurement, and testing.
In IEEEInternational Conference on Acoustics, Speech andSignal Processing, 2008.
ICASSP 2008, pages 5260?5263.D.
Bohus, A. Raux, T. Harris, M. Eskenazi, and A. Rud-nicky.
2007.
Olympus: an open-source frameworkfor conversational spoken language interface research.In HLT-NAACL 2007 Workshop on Bridging the Gap:Academic and Industrial Research in Dialog Technol-ogy.M.
Danieli and E. Gerbino.
1995.
Metrics for evaluat-ing dialogue strategies in a spoken language system.In Proceedings of the 1995 AAAI Spring Symposiumon Empirical Methods in Discourse Interpretation andGeneration, pages 34?39.W.
Eckert, E. Levin, and R. Pieraccini.
1998.
Automaticevaluation of spoken dialogue systems.
TWLT13: For-mal semantics and pragmatics of dialogue, pages 99?110.J.
P. Gonza?lez-Brenes and J. Mostow.
2011.
Classify-ing dialogue in high-dimensional space.
Transactionsof Speech and Language Processing; Special Issue onMachine Learning for Robust and Adaptive SpokenDialogue Systems.
In press.J.
P. Gonza?lez-Brenes, A. W. Black, and M. Eskenazi.2009.
Describing Spoken Dialogue Systems Differ-ences.
In International Workshop on Spoken DialogueSystems, Irsee, Germany.
Springer?Verlat.M.
Hajdinjak and F. Mihelic.
2006.
The PARADISEevaluation framework: Issues and findings.
Computa-tional Linguistics, 32(2):263?272.H.
W. Hastie, R. Prasad, and M. Walker.
2002.
Auto-matic evaluation: Using a date dialogue act tagger for16user satisfaction and task completion prediction.
In InLREC 2002, pages 641?648.E.
Levin and R. Pieraccini.
2006.
Value-based opti-mal decision for dialog systems.
In Spoken LanguageTechnology Workshop, 2006.
IEEE, pages 198 ?201.K.
Lounici, A.B.
Tsybakov, M. Pontil, and van de Geer.2009.
Taking advantage of sparsity in multi-task learn-ing.
In Conference on Learning Theory, volume 1050,page 9, Montreal, Quebec.S.
Mo?ller, P. Smeele, H. Boland, and J. Krebber.
2007.Evaluating spoken dialogue systems according to de-facto standards: A case study.
Computer Speech andLanguage, 21(1):26 ?
53.S.
Mo?ller, K.P.
Engelbrecht, and R. Schleicher.
2008.Predicting the quality and usability of spoken dialogueservices.
Speech Communication, 50(8-9):730?744.G.
Obozinski, B. Taskar, and M.I.
Jordan.
2007.
Multi-task feature selection.
In The Workshop of Struc-tural Knowledge Transfer for Machine Learning in the23rd International Conference on Machine Learning(ICML), Pittsburgh, PA.T.
Paek.
2001.
Empirical methods for evaluating dia-log systems.
In ACL 2001 Workshop on EvaluationMethodologies for Language and Dialogue systems,pages 3?10.M.Y.
Park and T. Hastie.
2007.
L1-regularization path al-gorithm for generalized linear models.
Journal of theRoyal Statistical Society: Series B (Statistical Method-ology), 69(19):659?677.J.
Polifroni, L. Hirschman, S. Seneff, and V. Zue.
1992.Experiments in evaluating interactive spoken languagesystems.
In Proceedings of the workshop on Speechand Natural Language, pages 28?33.
Association forComputational Linguistics.K.
Puniyani, S. Kim, and E.P.
Xing.
2010.
Multi-population GWA mapping via multi-task regularizedregression.
Bioinformatics, 26(12):208.A.
Raux, D. Bohus, B. Langner, A.W.
Black, and M. Es-kenazi.
2006.
Doing research on a deployed spokendialogue system: one year of Let?s Go!
experience.
InNinth International Conference on Spoken LanguageProcessing.
ISCA.M.
Schmidt, K. Murphy, G. Fung, and R. Rosales.
2008.Structure learning in random fields for heart motionabnormality detection.
In Computer Vision and Pat-tern Recognition, 2008.
CVPR 2008.
IEEE Conferenceon, pages 1 ?8.A.
Schmitt, M. Scholz, W. Minker, J. Liscombe, andD.
Suendermann.
2010.
Is it possible to predict taskcompletion in automated troubleshooters?
In INTER-SPEECH, pages 94?97.D.
Suendermann, J. Liscombe, R. Pieraccini, andK.
Evanini.
2010.
?How am I Doing??
: A new frame-work to effectively measure the performance of auto-mated customer care contact centers.
In A. Neustein,editor, Advances in Speech Recognition: Mobile Envi-ronments, Call Centers, and Clinics, pages 155?180.Springer.R.
Tibshirani.
1996.
Regression shrinkage and selectionvia the lasso.
Journal of the Royal Statistical Society.Series B (Methodological), 58(1):267?288.B.A.
Turlach, W.N.
Venables, and S.J.
Wright.
2005.Simultaneous variable selection.
Technometrics,47(3):349?363.M.
Walker, C. Kamm, and D. Litman.
2001.
Towards de-veloping general models of usability with PARADISE.Natural Language Engineering, 6(3):363?377.M.
A. Walker, I. Langkilde-Geary, H. W. Hastie,J.
Wright, and A. Gorin.
2002.
Automatically train-ing a problematic dialogue predictor for a spoken di-alogue system.
Journal of Artificial Intelligence Re-search, 16:293?319.M.
Yuan and Y. Lin.
2006.
Model selection and esti-mation in regression with grouped variables.
Journalof the Royal Statistical Society: Series B (StatisticalMethodology), 68(1):49?67.A.
Zellner.
1962.
An efficient method of estimatingseemingly unrelated regressions and tests for aggrega-tion bias.
Journal of the American Statistical Associa-tion, 57(298):pp.
348?368.H.
Zou and T. Hastie.
2005.
Regularization and vari-able selection via the Elastic Net.
Journal of the RoyalStatistical Society, 67:301?320.17
