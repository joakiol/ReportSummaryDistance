Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1477?1488, Dublin, Ireland, August 23-29 2014.Predicting Interesting Things in TextMichael GamonMicrosoft Corp.One Microsoft WayRedmond, WA 98052mgamon@microsoft.comArjun MukherjeeDepartment of ComputerScienceUniversity of HoustonHouston, TX 77004ar-jun4787@gmail.comPatrick PantelMicrosoft Corp.One Microsoft WayRedmond, WA 98052ppantel@microsoft.comAbstractWhile reading a document, a user may encounter concepts, entities, and topics that she is interested inexploring more.
We propose models of ?interestingness?, which aim to predict the level of interest a userhas in the various text spans in a document.
We obtain naturally occurring interest signals by observinguser browsing behavior in clicks from one page to another.
We cast the problem of predicting interesting-ness as a discriminative learning problem over this data.
We leverage features from two principal sources:textual context features and topic features that assess the semantics of the document transition.
We learnour topic features without supervision via probabilistic inference over a graphical model that captures thelatent joint topic space of the documents in the transition.
We train and test our models on millions of real-world transitions between Wikipedia documents as observed from web browser session logs.
On the taskof predicting which spans are of most interest to users, we show significant improvement over variousbaselines and highlight the value of our latent semantic model.1 IntroductionReading inevitably leads people to discover interesting concepts, entities, and topics.
Predicting whatinterests a user while reading a document has important applications ranging from augmenting the doc-ument with supplementary information, to ad placement, to content recommendation.
We define the taskof predicting interesting things (ITs) as ranking text spans in an unstructured document according towhether a user would want to know more about them.
This desire to learn more serves as our proxy forinterestingness.There are many types of observable behavior that indicate user interest in a text span.
The closest oneto our problem definition is found in web browsing, where users click from one document to anothervia named anchors.
The click process is generally governed by the user?s interest (modulo erroneousclicks).
As such, the anchor name can be viewed as a text span of interest for that user.
Furthermore, thefrequency with which users, in aggregate, click on an anchor serves as a good proxy for the level ofinterest1.What is perceived as interesting is influenced by many factors.
The semantics of the document andcandidate IT are important.
For example, we find that when users read an article about a movie, they aremore likely to browse to an article about an actor or character than to another movie or the director.Also, user profile and geo-temporal information are relevant.
For example, interests can differ depend-ing on the cultural and socio-economic background of a user as well as the time of the session (e.g.,weekday versus weekend, daytime versus late night, etc.
).1 Other naturally occurring expressions of user interest, albeit less fitting to our problem, are found in web search queries,social media engagement, and others.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1477Strictly speaking, human interestingness is a psychological and cognitive process (Varela et al., 1991).Clicks and long dwell times are salient observed behavioral signals of interestingness that have beenwell accepted in the information retrieval literature (Claypool et al., 2001; Mueller and Lockerd, 2001).In this paper, we utilize the observed user?s browsing behavior as a supervision signal for modelinginterestingness.
Specifically, we cast the prediction of ITs as a discriminative learning task.
We use aregression model to predict the likelihood of an anchor in a Wikipedia article to be clicked, which as wehave seen above can serve as a proxy for interestingness.
Based on an empirical study of a sample ofour data, we use features in our model from the document context (such as the position of the anchortext, frequency of the anchor text in the current paragraph, etc.)
as well as semantic features that aim tocapture the latent topic space of the documents in the browsing transition.
These semantic features areobtained in an unsupervised manner via a joint topic model of source and target documents in browsingtransitions.
We show empirical evidence that our discriminative model is effective in predicting ITs andwe demonstrate that the automatically learned latent semantic features contribute significantly to themodel performance.
The main contributions of this paper are:?
We introduce the task of predicting interesting things as identifying what a user likely wants tolearn more about while reading a document.?
We use browsing transitions as a proxy for interestingness and model our task using a discrimina-tive training approach.?
We propose a semantic probabilistic model of interestingness, which captures the latent aspectsthat drive a user to be interested in browsing from one document to another.
Features derived fromthis semantic model are used in our discriminative learner.?
We show empirical evidence of the effectiveness of our model on an application scenario.2 Related WorkSalience: A notion that might at first glance be confused with interestingness is that of salience (Paranjpe2009; Gamon et al.
2013).
Salience can be described as the centrality of a term to the content of adocument.
Put another way, it represents what the document is about.
Though salience and interesting-ness can interact, There are clear differences.
For example, in a news article about President Obama?svisit to Seattle, Obama is salient, yet the average user would probably not be interested in learning moreabout Obama while reading that article.Click Prediction: Click prediction models are used pervasively by search engines.
Query based clickprediction aims at computing the probability that a given document in a search-result page is clicked onafter a user enters some query (Joachims, 2002; Joachims et al., 2005; Agichtein et al., 2006; Guo et al.,2009a).
Click prediction for online advertising is a core signal for estimating the relevance of an ad to asearch result page or a document (Chatterjee et al., 2003; Broder et al., 2007; Craswell et al., 2008;Graepel et al., 2010).
Also related are personalized click models, e.g., (Shen et al., 2012), which useuser-specific click through rate (CTR).
Although these applications and our task share the use of CTRas a supervision signal, there is a key difference: Whereas in web search CTR is used as a predictor/fea-ture at runtime, our task specifically aims at predicting interestingness in the absence of web usagefeatures: Our input is completely unstructured and there is no assumption of prior user interaction data.Use of probabilistic models: Our semantic model is built over LDA (Blei et al., 2003) and has re-semblances to Link-LDA models (Erosheva et al., 2004) and Comment-LDA models (Yano et al., 2009).However, these are tailored for blogs and associated comment discussions which is very different fromour source to destination browsing transition logs.
Guo et al., (2009b) used probabilistic models fordiscovering entity classes from query logs and in (Lin et al., 2012), latent intents in entity centric searchwere explored.
Gao et al.
(2011) employ statistical machine translation to connect two types of content,learning semantic translation of queries to document titles.
None of the above models, however, aredirectly applicable to the joint topic mappings that are involved in source to destination browsing tran-sitions which are the focus of our work.Predicting Popular Content: Modeling interestingness is also related to predicting popular contentin the Web and content recommenders (Lerman and Hogg, 2010; Szabo and Huberman, 2010; Bandariet al., 2012).
In contrast to these tasks, we strive to predict what term a user is likely to be interested inwhen reading content.
We do not rely on prior browsing history, since we aim to predict interestingness1478in unstructured text with no interaction history.
We show in our experiments that a popularity signalalone is not a sufficient predictor for interestingness.3 The Interestingness TaskThe process of identifying interesting things (ITs) on a page consists of two parts: (1) generating candi-date things (e.g., entities, concepts, topics); and (2) scoring and ranking these according to interesting-ness.
In this paper, we fix step 1 and focus our effort on step 2, i.e., the assignment of an interestingnessscore to a candidate.
We believe that this scope is appropriate in order to understand the factors thatenter into what is perceived as interesting by a user.
Once we have gained an understanding of theinterestingness scoring problem, however, there are opportunities in identifying candidates automati-cally, which we leave for future work.In this section we begin by formally defining our task.
We then introduce our data set of naturallyoccurring interest signals, followed by an investigation of the factors that influence them.3.1 Formal Task DefinitionWe define our task as follows.
Let ??
be the set of all documents and ??
be the set of all candidate textspans in all documents in ??
, generated by some candidate generator.
Let ????
?
??
be the set of candi-dates in ??
?
??
.
We formally define the interestingness task as learning the function below, where??(?
?, ??)
is the interestingness of candidate ??
in 2:??:??
?
??
?
?
(1)3.2 Data SetUser browsing events on the web (i.e., a user clicking from one document to another) form a naturallyoccurring collection of interestingness signals.
That is when a user clicks on an anchor in a document,we can postulate that the user is interested in learning more about it, modulo erroneous clicks.We collect a large set of many millions of such user browsing events from session logs of a commer-cial web browser.
Specifically, we collect from these logs each occurrence of a user click from oneWikipedia page to another during a one month period, from all users in all parts of the world.
We referto each such event as a transition.
For each transition, our browser log provides metadata, including userprofile information, geo-location information and session information (e.g., time of click, source/targetdwell time, etc.)
Our data set includes millions of transitions between Wikipedia pages.For our task we require: (1) a mechanism for generating candidate things; (2) ample clicks to serveas a reliable signal of interestingness for training our models; and (3) accessible content.
Our focus onWikipedia satisfies all.
First, Wikipedia pages tend to contain many anchors, which can serve as the setof candidate things to be ranked.
Second, Wikipedia attracts enough traffic to obtain robust browsingtransition data.
Finally, Wikipedia provides full content3 dumps.
It is important here to note that ourchoice of Wikipedia as a test bed for our experiments does not restrict the general applicability of ourapproach: We propose a semantic model (Section 4.2) for mining latent features relevant to the phenom-enon of interestingness which is general and can be applied to generic Web document collections.Using uniform sampling, we split our data into three sets: a development set (20%), a training set(60%) and a test set (20%).
We further subdivide the test set by assigning each transition as belongingto the HEAD, TORSO, or TAIL, which we compute using inverse CDF sampling on the test set.
We doso by assigning the most frequently occurring transitions, accounting for 20% of the (source) traffic, tothe HEAD.
Similarly, the least frequently occurring transitions, accounting for 20% of the (source) traf-fic, are assigned to the TAIL.
The remaining transitions are assigned to the TORSO.
This three-waysplit reflects a common practice in the IR community and is based on the observation that web trafficfrequencies show a very skewed distribution, with a small set of web pages attracting a large amount oftraffic, and a very long tail of infrequently visited sites.
Different regions in that distribution often showmarked differences in behaviour, and models that are useful in one region are not necessarily as usefulin another.2 We fix ??(?
?, ??)
= 0 for all ??
?
???
?.3 We utilize the May 3, 2013 English Wikipedia dump from http://dumps.wikimedia.org, consisting of roughly 4.1 millionarticles.14793.3 What Factors Influence Interestingness?We manually inspected 200 random transitions from our development set.
Below, we summarize ourobservations.Only few things on a page are interesting: The average number of anchors on a Wikipedia page is 79.Of these, only very few are actually clicked by users.
For example, the Wikipedia article on the TVseries ?The Big Bang Theory?
leads to clicks on anchors linking to the pages of the series?
actors for90% of transitions (while these anchors account for only a small fraction of all unique anchors on thatpage).The semantics of source and destination pages is important: We manually determined the entity typeof the Wikipedia articles in our sample, according to schema.org classes.
49% of all source urls in ourdata sample are of the Creative Work category, reflecting the strong popular interest in movies(37%), actors (22%), artists (18%), and television series (8%).
The next three most prominent categoriesare Organization (12%), Person (11%) and Place (6%).
We observed that transitions are influ-enced by these categories.
For example, when the source article category is Movie, the most frequentlyclicked pages are of category Actor (63%) and Character (13%).
For source articles of theTVSeries category, Actor destination articles account for 86% of clicks.
Actor articles lead toclicks on Movie articles (45%) and other Actor articles (26%), whereas Artist articles lead toclicks on other Artist articles (29%), Movie articles (17%) and MusicRecording articles (18%).The structure of the source page plays a role: It is well known that the position of a link on a pageinfluences user click behavior: links that are higher on a page or in a more prominent position tend toattract more clicks.
We noticed similar trends in our data.The user plays a role: We hypothesized that users from different geographic and cultural backgroundsmight exhibit different interests, or that interests are time-bound (e.g., interests on weekends differ fromthose on week days, daytime from nighttime, etc.)
Initial experiments showed small effects of thesefactors, however, a more thorough analysis on a larger sample is necessary, which we leave for futurework.4 Modeling InterestingnessWe cast the problem of learning the interestingness function ??
(see Eq.
1) as a discriminative regressionlearning problem.
Below, we first describe this model, and then we introduce our semantic topic modelwhich serves to provide semantic features for the discriminative learner.4.1 Discriminative ModelAlthough our task is to predict ITs from unstructured documents, we can leverage the user interactionsin our data, described in Section 3.2 as our training signal.Given a source document ??
?
??
, and an anchor in s leading to destination document d, we use theaggregate click frequency of this anchor as a proxy for its interestingness, i.e.:??(?
?, ??)
= ??(??|??)
(2)where ??(??|??)
is the probability of a user clicking on the anchor to ??
when viewing ??3F4.
We use ??(??|??
)as our regression target computed from our training data.For our learning algorithm, we use boosted decision trees (Friedman, 1999).
We tune our hyperpa-rameters (i.e., number of iterations, learning rate, minimum instances in leaf nodes, and the maximumnumber of leaves) using cross-validation on the development set.
Each transition in our training data isrepresented as a vector of features, where the features fall into three basic families:1 Anchor features (Anc): position of the anchor in the document, frequency of the anchor, anchordensity in the paragraph, and whether the anchor text matches the title of the destination page.2 User session features (Ses): city, country, postal code, region, state and timezone of the user, aswell as day of week, hour, and weekend vs. workday of the occurrence of the transition.4 For notational convenience, we use ??(?
?, ??)
even though Eq.
1 defines its second argument as being a candidate text span.Here, it is implicit that d consists of both the target document and the anchor text (which serves as the candidate text span).14803 Semantic features: sourced in various experimental configurations from (1) Wikipedia page cate-gories as assigned by Wikipedia editors (Wiki) or from (2) an unsupervised joint topic transitionmodel (JTT) of source and destination pages (described in detail in the next section).In some experimental configurations we use Wikipedia page categories as semantic features.
We showin our experiments (see Section 5) that these are highly discriminative.
It is important to note that editor-labeled category information is available in the Wikipedia domain but not in others.
In other words, wecan use this information to verify that semantics indeed is influential for interestingness, but we shoulddesign our models to not rely on this.
We thus build an unsupervised semantic model of source anddestination pages, which serves the purpose of providing semantic information without any domain-specific annotation.4.2 The Semantics of InterestingnessAs indicated in Section 3, the semantics of source and destination pages, ??
and ?
?, influence the likeli-hood that a user is interested in ??
after viewing ??.
In this section we propose an unsupervised methodfor modeling the transition semantics between ??
and ??.
As outlined in the previous section, this modelthen serves to generate semantic features for our discriminative model of interestingness.Referring to the notations in Table 1, we start by positing a distribution over the joint latent transitiontopics (in the higher level of semantic space), ????
for each transition ??.
The corresponding source ??(??
)and destination ??(??)
articles of a given transition ??
are assumed to be admixtures of latent topics that areconditioned on the joint topic transition distribution, ????.
For ease of reference, we will refer to this modelas the Joint Transition Topic Model (JTT).
The variable names and their descriptions are provided inTable 1.
Figure 1 shows the plate notation of our model and the generative process:T?zd zsws wdK??
?Ns NdFigure 1.
Generative Process and Plate Notation of JTT.1.
For each topic ?
?, draw ????
~ ??????(??)2.
For each transition ??:a.
Draw the joint topic transition distribution, ????
~??????(??)b.
For each word token ??
?
{1?
????}:i.
Draw ????,????
~ ????????(????)ii.
Emit ????,????
~ ????????(????)c.
For each word token ??
?
{1?
????}:i.
Draw ????,????
~ ????????(????)ii.
Emit ????,????
~ ????????(????
)1481Variable Description Variable Description??
A transition ??
????
, ????
Set of all topics in src, dest pages??(??
), ??(??)
The src and dest pages of ??
??
?
?, ??
??
Set of all word tokens in src, dest pages????~??????(??)
Joint src/dest topic distribution ?
= {????
}Set of all latent joint transition topic dis-tributions???
?, ????
Latent topics of  ??(??
), ??(??)
?
= {????}
Set of all latent topics???
?, ????
Observed word tokens of  ??(??
), ??(??)
????,??
Contribution of topic ??
in transition ??????
~ ??????(?
?Latent topic-word distributions fortopic ??????,????
, ????,????
?
?th word of transition ??
in ??(??
), ??(??)?
?, ??
Dirichlet parameters for ?
?, ??
????,????
, ????,????
Latent topic of ?
?th word of ??
in ??(??
), ??(??)???
?, ????
No.
of terms in src and dest pgs of ??
????(??),????
No.
of words in ??(??)
assigned to topic ????
= {??}
Set of all transitions, ??
????(??),????
No.
of words in ??(??)
assigned to ????
No.
of topics ????,????
No.
of times word ??
assigned to ??
in ??
????
No.
of unique terms in the vocab.
????,????
No.
of times word ??
assigned to ??
in ??
?
?Table 1.
List of notations.Exact inference for JTT is intractable.
Hence, we use Markov Chain Monte Carlo (MCMC) Gibbs sam-pling.
Rao-Blackwellization (Bishop, 2006) is used to reduce sampling variance by collapsing latentvariables ??
and ??.
Owing to space constraints, we omit the full derivation details.
The full joint can bewritten succinctly as follows:??(????,????,???
?, ????)
= ?????????(??
),[ ]??
+ ????(??
),[ ]??
+ ?????(??)????=1?
= ????(???
?,[ ]??
+ ???
?,[ ]??
+??)??(??)????=1?
(3)Omission of a latter index in the count variables, denoted by [ ], corresponds to the row vector span-ning over the latter index.
The corresponding Gibbs conditional distributions for ????
and ????
are detailedbelow, where the subscript ??(?
?, ??)?
denotes the value of the expression excluding the counts of theterm (?
?, ??):???????,????
= ?
?| ?
?
??????(??),????
??(??,??
)+ ????(??),????
+???
??????(??),????
??(??,??
)+ ????(??),????
+???????=1??????,????
??(??,??
)+ ????,????
+???
??????,????
??(??,??
)+ ????,????
+???????=1(4)???????,????
= ?
?| ?
?
?????(??),????
+?????(??),????
??(??,??)+???
?????(??),????
+?????(??),????
??(??,??)+???????=1?????,????
+ ?????,????
??(??,??)+???
?????,????
+ ?????,????
??(??,??)+??????
?=1(5)We learn our joint topic model from a random traffic-weighted sample of 10,000 transitions, which arerandomly sampled from the development set outlined in Section 3.25.
The decision to use this sampleof 10,000 transitions is based on the observation that there were no statistically significant performancegains for models trained on more than 10k transitions.
The Dirichlet hyperparameters are set to ??
=50/??
and ??
= 0.1 according to the values suggested in (Griffiths and Steyvers, 2004).
The number oftopics, ?
?, is empirically set to 50.
We also conducted pilot experiments with other hyperparameter set-tings, larger transition sets and more topics but we found no substantial difference in the end-to-endperformance.
Although increasing the number of topics and modeling more volume usually results inlowering perplexities and better fitting in topic models (Blei et al., 2003), it can also result in redun-dancy in topics which may not be very useful for downstream applications (Chen et al., 2013).
For allreported experiments we use the posterior estimates of our joint model learned according to the abovesettings.
In our discriminative interestingness model, we use three classes of features from JTT to cap-ture the latent topic distributions of the source page, the destination page, and the joint topics for thattransition.
These correspond to source topic features (???
?, labeled as JTTsrc in charts), destination topicfeatures (???
?, labeled as JTTdst), and transition topic features (?, labeled as JTTtrans).
Each of thesethree sets comprises 50 features, for a total of 150.?
is the distribution over joint src and dst topics that5 Note that we use the development set to train our semantic model since it is ultimately used to generate features for our dis-criminative learner of Section 4.
Since the learner is trained using the training set, this strategy avoids overfitting our seman-tic model to the training set.1482appear in a particular transition.
????
and ????
are the actual topic assignments for individual words in srcand dst.
Upon learning the JTT model, for each K topics, we get a probability of that topic appearing inthe transition, in the src, and in the dst document (by taking the posterior point estimates for latentvariables  ?, ???
?, ????
respectively).
The GBDT implementation we use for our discriminative model per-forms binning of these real-valued features over an ensemble of DTs.5 ExperimentsWe evaluate our interestingness model on the task of proposing ??
anchors on a page that the user willfind interesting (highlighting task).
Recall the interestingness function ??
from Eq.
1.
In the highlightingtask, a user is reading a document ??
?
??
and is interested in learning more about a set of anchors.
Ourgoal in this task is to select ??
anchors that maximize the cumulative degree of interest of the user, i.e.:argmax??????=(??1,?,????|?????????)?
??(?
?, ????)???????????
(6)In other words, we consider the ideal selection to consist of the k most interesting anchors according to??(?
?, ??
).We compare the interestingness ranking of our models against a gold standard function, ??
?, com-puted from our test set.
Recall that we use the aggregate click frequency of an anchor as a proxy for itsinterestingness.
As such, the gold standard function for the test set is computed as:???(?
?, ??)
= ??(??|??)
(7)where ??(??|??)
is the probability of a user clicking on the anchor ??
when viewing ?
?.Given a source document ?
?, we measure the quality of a model?s interestingness ranking against theideal ranking defined above using the standard nDCG metric (Manning et al., 2008).
We use the inter-estingness score of the gold standard as the relevance score.Table 2 shows the nDCG results for two baselines and a range of different feature sets.
The first high-level observation is that the task is difficult, given the low baseline results.
Since there are many anchorson an average page, picking a random set of anchors yields very low nDCG scores.
The nDCG numbersof our baselines increase as we move from HEAD to TORSO to TAIL, due to the fact that the averagenumber of links per page (not unique) decreases in these sets from 170 to 94 to 416.
The second baselineillustrates that it is not sufficient to simply pick the top n anchors on a page.Next, we see that using our set of anchor features (see Section 4.1) in the regression model greatlyimproves performance over the baselines, with the strongest numbers on the HEAD set and decreasingeffectiveness in TORSO and TAIL.
This shows that the distribution of interesting anchors on a pagediffers according to the popularity of the source content, possibly also with the length of the page.
Ourbest performing model is the one using anchor features and all three sets of latent semantic features(Table 2, row 6; source, destination, and transition topics).The biggest improvement is obtained on the HEAD data.
This is not surprising given that the topicmodel is trained on a traffic weighted sample of Wikipedia articles and that HEAD pages tend to havemore content, making the identification of topics more reliable.
Regarding the individual contributionsof the latent semantic features (Table 2, rows 4, 5), destination features alone hurt performance on theHEAD set.
Latent semantic source features lead to a boost across the board, and the addition of latentsemantic transition topic features produces the best model, with gains especially pronounced on theHEAD data.
Figure 2 further shows the performance of our best configuration across ALL, HEAD,TORSO, and TAIL.
Interestingly, the TAIL exhibits better performance of the model than the TORSO(with the exception of nDCG at rank 3 or higher).
We hypothesize that this is because the average num-ber of anchors in a TAIL page is less than half of that in a TORSO page.6 Wikipedia editors tend to spend more time on more frequently viewed documents, hence they tend contain more content andmore anchors.1483nDCG % HEAD TORSO TAILn @1 @2 @5 @10 @1 @2 @5 @10 @1 @2 @5 @10Baseline: random 4.07 4.90 6.24 8.10 3.56 4.83 7.66 10.92 6.20 11.74 19.50 25.82Baseline: first n an-chors9.99 12.47 17.72 24.33 7.17 9.87 17.06 23.97 9.06 16.66 27.35 34.82Anc 21.46 22.50 25.30 29.47 13.85 16.80 22.85 28.20 10.88 19.16 29.33 36.48Anc+JTTdst 13.97 16.33 19.69 23.78 11.37 14.17 19.67 24.66 11.62 19.69 29.69 36.35Anc+JTTdst+JTTsrc 26.62 30.03 34.82 39.38 17.05 20.82 27.15 32.48 12.27 21.56 31.88 38.85Anc+JTT-dst+JTTsrc+JTTtrans34.49 35.21 38.01 41.80 18.32 21.69 28.03 33.22 13.06 21.68 32.13 38.01Table 2.
Highlighting performance (% nDCG @ n) for different feature sets across HEAD, TORSO,and TAIL.
Bold indicates statistically significant best systems (with 95% confidence).Not shown in these results are the effects of using user session features.
We consistently found thatthese features did not improve upon the configurations where anchor and JTT features are used.
We donot, however, rule out the potential of such features on this task, especially in light of our data analysisobservations from Section 3.3, which suggest an effect from these factors.
We leave a more in-depthstudy of the potential contribution of these types of features for future research.We now address the question how our unsupervised latent semantic features perform compared to theeditor-assigned categories for Wikipedia pages, for two reasons.
First, it is reasonable to consider themanually assigned Wikipedia categories as a (fine-grained) oracle for topic assignments.
Second, out-side of Wikipedia, we do not have the luxury of manually assigned categories/topics.
As illustrated inFigure 3, we found that Wikipedia categories outperform the JTT topic features, but the latter can re-cover about two thirds of the nDCG gain compared to Wikipedia categories.Finally, in the HEAD part of the data, we have enough historical clickthrough data that we coulddirectly leverage for prediction.
We conducted experiments where we used the prior probability ??(??|??
)obtained from the development data (both smoothed and unsmoothed).
Following this strategy we canachieve up to 65% nDCG@10 as shown in Figure 4 where the use of prior history (labeled ?History:Target | Source Prior?)
is compared to our best model and to baselines.
As stressed before, in most real-life applications, this is not a viable option since anchors or user-interaction logs are unavailable.
Evenin web browsing scenarios, the TORSO and TAIL have no or only very sparse histories.
Furthermore,the information is not available in a ?cold start?
scenario involving new and unseen pages.
We alsoexamined whether the general popularity of a target page is sufficient to predict an anchor?s interesting-ness, and we found that this signal performs better than the baselines, but significantly worse than ourmodels.
This series is labeled ?History: Target Prior?
in Figure 4.Figure 2.
NDCG comparison across overall performance (ALL) versus HEAD, TORSO, and TAILsubsets, on the Highlighting task.00.10.20.30.40.51 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20NDCGRankBest Configuration (Anc+JTT)NDCG @ Rank (ALL vs. HEAD vs. TORSO vs. TAIL)ALLHEADTORSOTAIL1484Figure 3.
JTT features versus Wikipedia category features on Highlighting task.Figure 4.
Highlighting task comparison between baselines, best configuration using JTT, and modelswith historical transitions.Our highlights task reflects the main goal of our paper, i.e., to predict interestingness in the context ofany document, whether it be a web page, an email, or a book.
A natural extension of our work, especiallyin our experimental setting with Wikipedia transitions, is to predict the next click of a user, i.e., clickprediction.There is a subtle but important difference between the two tasks.
Highlights aims to identify a set ofinteresting nuggets for a source document.
A user may ultimately click on only a subset of the nuggets,and perhaps not in the order of most interest.
Our experimental metric, nDCG, reflects this ranking taskwell.
Click prediction is an inherently more difficult task, where we focus on predicting exactly the nextclick of a specific user.
Unlike in the highlights task, there is no partial credit for retrieving other inter-esting anchors.
Only the exact clicked anchor is considered a correct result.
As such, we utilize a differ-ent metric than nDCG on this task.
We measure our model?s performance on the task of click predictionusing cumulative precision.
Given a unique transition event ?
(s,a,d) by a particular user at a particulartime, we present the transition, minus the gold anchor a and destination d, to our models, which in turnpredict an ordered list of most likely anchors on which the user will click.
The cumulative precision atk of a model, is 1 if any of the predicted anchors matched a, and 0 otherwise.00.10.20.30.40.51 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20NDCGRankSemantic Features: JTT vs. Wikipedia CategoriesNDCG @ Rank (ALL)Baseline: randomBaseline: firstAncAnc+Wiki_Src+Wiki_tarAnc+JTT_src+JTT_tar+JTT_trans00.10.20.30.40.50.60.71 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20NDCGRankInterest Models vs. Access to Historical TransitionsNDCG @ Rank (ALL)Baseline: randomBaseline: firstHistory: Target PriorHistory: Target |Source PriorAnc+JTT_src+JTT_tar+JTT_trans1485Table 3 outlines the results on this task and Figure 5 shows the corresponding chart for our bestconfiguration.
Note that in the click prediction task, the model performs best on the TAIL, followed byTORSO and HEAD.
This seems to be a reflection of the fact that in this harder task, the total number ofanchors per page is the most influential factor in model performance.CumulativePrecision % HEAD TORSO TAILn @1 @2 @5 @10 @1 @2 @5 @10 @1 @2 @5 @10Baseline: random 1.07 2.08 5.29 10.55 1.94 3.91 9.71 19.00 5.97 11.66 26.43 44.94Baseline: first n an-chors2.68 5.77 16.73 33.78 4.10 8.19 22.86 42.08 8.77 16.57 36.80 58.52Anc 8.40 12.55 22.04 34.22 8.70 14.37 27.56 42.68 10.59 19.08 38.27 59.04Anc+JTTdst 5.48 9.19 17.77 29.14 6.93 12.07 23.90 38.00 11.23 19.59 38.46 57.87Anc+JTTdst+JTTsrc 9.02 15.65 30.05 44.72 10.11 17.42 32.08 47.07 11.95 21.47 40.96 61.24Anc+JTT-dst+JTTsrc+JTTtrans11.53 18.43 31.93 45.36 10.86 18.19 32.96 47.66 12.64 21.58 41.27 61.28Table 3.
Click prediction results for different feature sets across HEAD, TORSO, and TAIL.
Bold indicates sta-tistically significant best systems (with 95% confidence).Figure 5.
Overall performance (ALL) versus HEAD, TORSO, and TAIL subsets on click prediction.6 Conclusion and Future DirectionsWe presented a notion of an IT on a page that is grounded in observable browsing behavior duringcontent consumption.
We implemented a model for prediction of interestingness that we trained andtested within the domain of Wikipedia.
The model design is generic and not tied to our experimentalchoice of the Wikipedia domain and can be applied to other domains.
Our model takes advantage ofsemantic features that we derive from a novel joint topic transition model.
This semantic model takesinto account the topic distributions for the source, destination, and transitions from source to destination.We demonstrated that the latent semantic features from our topic model contribute significantly to theperformance of interestingness prediction, to the point where they perform nearly as well as using editor-assigned Wikipedia categories as features.
We also showed that the transition topics improve resultsover just using source and destination semantic features alone.A number of future directions immediately suggest themselves.
First, for an application that marksinteresting ITs on an arbitrary page, we would need a detector for IT candidates.
A simple first approachwould be to use a state-of-the-art Named Entity Recognition (NER) system to cover at least a subset ofpotential candidates.
This does not solve the problem entirely, since we know that named entities arenot the only interesting nuggets ?
general terms and concepts can also be of interest to a reader.
On theother hand we do have reason to believe that entities play a very prominent role in web content con-sumption, based on the frequency with which entities are searched for (see, for example Lin et al.
2012and the references cited therein).
Using an NER system as a candidate generator would also allow us to00.20.40.60.811 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20CumulativePrecisionRankBest Configuration (Anc+JTT)Click Prediction - Cumulative Precision @ RankALLHEADTORSOTAIL1486add another potentially useful feature to our interestingness prediction model: the type of the entity.
Onecould also envision jointly modeling interestingness and candidate detection.A second point concerns the observation from the previous section on the different regularities thatseem to be at play according to the popularity and possibly the length of an article.
More detailed ex-periments are needed to tease out this influence and possibly improve the predictive power of the model.User session features did not contribute to model performance when used in conjunction with otherfeature families, but closer investigation of these features is warranted for more personalized models ofinterestingness.
Finally, a number of options regarding JTT  could be explored further.
Being trained ona traffic-weighted sample of articles, the topic model predominantly picks up on popular topics.
Thiscould be remedied by training on a non-weighted sample, or, more promisingly, on a larger non-weighted sample with a larger ?
?, i.e.
more permissible total topics.ReferencesAgichtein, E., Brill, E., and Dumais, S. 2006.
Improving web search ranking by incorporating user behavior infor-mation.
In Proceedings of SIGIR.
pp.
19-26.Bandari, R., Asur, S., and Huberman, B.
A.
2012.
The Pulse of News in Social Media: Forecasting Popularity.
InProceedings of ICWSM.Blei, D. M., Ng, A. Y., and Jordan, M. I.
2003.
Latent dirichlet allocation.
Journal of Machine Learning Re-search, 3, 993-1022.Broder, A., Fontoura, M., Josifovski, V., and Riedel, L. 2007.
A semantic approach to contextual advertising.
InProceedings of SIGIR.
pp.
559-566.Bishop, C.M.
2006.
Pattern Recognition and Machine Learning.
Springer.Chatterjee, P., Hoffman, D. L., and Novak, T. P. 2003.
Modeling the clickstream: Implications for web-basedadvertising efforts.
Marketing Science, 22(4), 520-541.Chen, Z., Mukherjee, A., Liu, B., Hsu, M., Castellanos, M. and Ghosh, R. 2013.
Leveraging Multi-Domain PriorKnowledge in Topic Models.
In Proceedings of IJCAI.
pp.
2071-2077.Claypool, M., Le, P., Wased, M., & Brown, D. 2001a.
Implicit interest indicators.
In Proceedings of the 6th inter-national conference on Intelligent user interfaces (pp.
33-40).
ACM.Craswell, N., Zoeter, O., Taylor, M., and Ramsey, B.
2008.
An experimental comparison of click position-biasmodels.
In Proceedings of WSDM.
pp.
87-94.Erosheva, E., Fienberg, S., and Lafferty, J.
2004.
Mixed membership models of scientific publications.
In Pro-ceedings of the National Academy of Sciences of the United States of America, 101(Suppl 1).
pp.
5220-5227.Friedman, J. H. 1999.
Greedy function approximation: A gradient boosting machine.
Annals of Statistics, 29:1189-1232, 1999.Gamon, M., Yano, T., Song, X., Apacible, J. and Pantel, P. 2013.
Identifying Salient Entities in Web Pages.
InProceedings CIKM.
pp.
2375-2380.Gao, J., Toutanova, K., and Yih, W. T. 2011.
Clickthrough-based latent semantic models for web search.
In Pro-ceedings of SIGIR.
pp.
675-684.Graepel, T., Candela, J.Q., Borchert, T., and Herbrich, R. 2010.
Web-scale Bayesian Click-Through Rate Predic-tion for Sponsored Search Advertising in Microsoft?s Bing Search Engine.
In Proceedings of ICML.
pp.
13-20.Griffiths, T.L and Steyvers, M. 2004.
Finding Scientific Topics.
Proceedings of the National Academy of Science,101, suppl 1, 5228-5235.Guo, F., Liu, C., Kannan, A., Minka, T., Taylor, M., Wang, Y.-M, and Faloutsos, C. 2009a.
Click chain model inweb search.
In Proceedings of WWW.
pp.
11-20.Guo, J., Xu, G., Cheng, X., and Li, H. 2009b.
Named entity recognition in query.
In Proceedings of SIGIR.
pp.267-274.Joachims, T. 2002.
Optimizing search engines using clickthrough data.
In Proceedings of KDD.
pp.
133-142.Joachims, T., Granka, L., Pan, B., Hembrooke, H. and Gay, G. 2005.
Accurately interpreting clickthrough data asimplicit feedback.
In Proceedings of SIGIR.
pp.
154-161.1487Lerman, K., and Hogg, T. 2010.
Using a model of social dynamics to predict popularity of news.
In Proceedingsof WWW.
pp.
621-630.Lin, T., Pantel, P., Gamon, M., Kannan, A., and Fuxman, A.
2012.
Active objects: actions for entity-centric search.In Proceedings of WWW.
pp.
589-598.Manning, C. D., Raghavan, P., and Schutze, H. 2008.
Introduction to Information Retrieval.
Cambridge UniversityPress.Mueller, F., & Lockerd, A.
2001.
Cheese: tracking mouse movement activity on websites, a tool for user modeling.In CHI'01 extended abstracts on Human factors in computing systems (pp.
279-280).
ACM.Paranjpe, D. 2009.
Learning document aboutness from implicit user feedback and document structure.
In Proceed-ings of CIKM.
pp.
365-374.Shen, S., Hu, B., Chen, W., and Yang, Q.
2012.
Personalized click model through collaborative filtering.
In Pro-ceedings of WSDM.
pp.
323-333.Szabo, G., and Huberman, B.
A.
2010.
Predicting the popularity of online content.
Com-munications of theACM, 53(8), 80-88.Varela, F. J., Thompson, E. T., & Rosch, E. 1991.
The embodied mind: Cognitive science and human experi-ence.
The MIT Press.Yano, T., Cohen, W. W., & Smith, N. A.
2009.
Predicting response to political blog posts with topic models.
InProceedings of NAACL.
pp.
477-485.1488
