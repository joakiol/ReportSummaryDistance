Refinements in BTG-based Statistical Machine TranslationDeyi Xiong, Min Zhang, Aiti AwHuman Language TechnologyInstitute for Infocomm Research21 Heng Mui Keng TerraceSingapore 119613{dyxiong, mzhang, aaiti}@i2r.a-star.edu.sgHaitao Mi, Qun Liu and Shouxun LinKey Lab of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesBeijing China, 100080{htmi, liuqun, sxlin}@ict.ac.cnAbstractBracketing Transduction Grammar (BTG)has been well studied and used in statisticalmachine translation (SMT) with promisingresults.
However, there are two major issuesfor BTG-based SMT.
First, there is no effec-tive mechanism available for predicting or-ders between neighboring blocks in the orig-inal BTG.
Second, the computational cost ishigh.
In this paper, we introduce two re-finements for BTG-based SMT to achievebetter reordering and higher-speed decod-ing, which include (1) reordering heuristicsto prevent incorrect swapping and reducesearch space, and (2) special phrases withtags to indicate sentence beginning and end-ing.
The two refinements are integrated intoa well-established BTG-based Chinese-to-English SMT system that is trained on large-scale parallel data.
Experimental results onthe NIST MT-05 task show that the proposedrefinements contribute significant improve-ment of 2% in BLEU score over the baselinesystem.1 IntroductionBracket transduction grammar was proposed by Wu(1995) and firstly employed in statistical machinetranslation in (Wu, 1996).
Because of its good trade-off between efficiency and expressiveness, BTG re-striction is widely used for reordering in SMT (Zenset al, 2004).
However, BTG restriction does notprovide a mechanism to predict final orders betweentwo neighboring blocks.To solve this problem, Xiong et al (2006)proposed an enhanced BTG with a maximum en-tropy (MaxEnt) based reordering model (MEBTG).MEBTG uses boundary words of bilingual phrasesas features to predict their orders.
Xiong etal.
(2006) reported significant performance im-provement on Chinese-English translation tasks intwo different domains when compared with bothPharaoh (Koehn, 2004) and the original BTG us-ing flat reordering.
However, error analysis of thetranslation output of Xiong et al (2006) revealsthat boundary words predict wrong swapping, espe-cially for long phrases although the MaxEnt-basedreordering model shows better performance thanbaseline reordering models.Another big problem with BTG-based SMT is thehigh computational cost.
Huang et al (2005) re-ported that the time complexity of BTG decodingwith m-gram language model is O(n3+4(m?1)).
If a4-gram language model is used (common in manycurrent SMT systems), the time complexity is ashigh as O(n15).
Therefore with this time complexitytranslating long sentences is time-consuming evenwith highly stringent pruning strategy.To speed up BTG decoding, Huang et al (2005)adapted the hook trick which changes the timecomplexity from O(n3+4(m?1)) to O(n3+3(m?1)).However, the implementation of the hook trick withpruning is quite complicated.
Another method to in-crease decoding speed is cube pruning proposed byChiang (2007) which reduces search space signifi-cantly.In this paper, we propose two refinements to ad-dress the two issues, including (1) reordering heuris-505tics to prevent incorrect swapping and reduce searchspace using swapping window and punctuation re-striction, and (2) phrases with special tags to indicatebeginning and ending of sentence.
Experimental re-sults show that both refinements improve the BLEUscore significantly on large-scale data.The above refinements can be easily implementedand integrated into a baseline BTG-based SMT sys-tem.
However, they are not specially designed forBTG-based SMT and can also be easily integratedinto other systems with different underlying trans-lation strategies, such as the state-of-the-art phrase-based system (Koehn et al, 2007), syntax-based sys-tems (Chiang et al, 2005; Marcu et al, 2006; Liu etal., 2006).The rest of the paper is organized as follows.
Insection 2, we review briefly the core elements ofthe baseline system.
In section 3 we describe ourproposed refinements in detail.
Section 4 presentsthe evaluation results on Chinese-to-English trans-lation based on these refinements as well as resultsobtained in the NIST MT-06 evaluation exercise.
Fi-nally, we conclude our work in section 5.2 The Baseline SystemIn this paper, we use Xiong et al (2006)?s sys-tem Bruin as our baseline system.
Their system hasthree essential elements which are (1) a stochasticBTG, whose rules are weighted using different fea-tures in log-linear form, (2) a MaxEnt-based reorder-ing model with features automatically learned frombilingual training data, (3) a CKY-style decoder us-ing beam search similar to that of Wu (1996).
Wedescribe the first two components briefly below.2.1 ModelThe translation process is modeled using BTG ruleswhich are listed as followsA ?
[A1, A2] (1)A ?
?A1, A2?
(2)A ?
x/y (3)The lexical rule (3) is used to translate source phrasex into target phrase y and generate a block A. Thetwo rules (1) and (2) are used to merge two consec-utive blocks into a single larger block in a straight orinverted order.To construct a stochastic BTG, we calculate ruleprobabilities using the log-linear model (Och andNey, 2002).
For the two merging rules (1) and (2),the assigned probability Prm(A) is defined as fol-lowsPrm(A) = ???
?
4?LMpLM (A1,A2) (4)where ?, the reordering score of block A1 andA2, is calculated using the MaxEnt-based reorderingmodel (Xiong et al, 2006) described in the next sec-tion, ??
is the weight of ?, and 4pLM (A1,A2) is theincrement of language model score of the two blocksaccording to their final order, ?LM is its weight.For the lexical rule (3), it is applied with a proba-bility Prl(A)Prl(A) = p(x|y)?1 ?
p(y|x)?2 ?
plex(x|y)?3?plex(y|x)?4 ?
exp(1)?5 ?
exp(|y|)?6?p?LMLM (y) (5)where p(?)
are the phrase translation probabilitiesin both directions, plex(?)
are the lexical translationprobabilities in both directions, exp(1) and exp(|y|)are the phrase penalty and word penalty, respec-tively and ?s are weights of features.
These featuresare commonly used in the state-of-the-art systems(Koehn et al, 2005; Chiang et al, 2005).2.2 MaxEnt-based Reordering ModelThe MaxEnt-based reordering model is defined ontwo consecutive blocks A1 and A2 together withtheir order o ?
{straight, inverted} according tothe maximum entropy framework.?
= p?
(o|A1, A2) = exp(?i ?ihi(o,A1, A2))?o exp(?i ?ihi(o,A1, A2))(6)where the functions hi ?
{0, 1} are model featuresand ?i are weights of the model features trained au-tomatically (Malouf, 2002).There are three steps to train a MaxEnt-based re-ordering model.
First, we need to extract reorderingexamples from unannotated bilingual data, then gen-erate features from these examples and finally esti-mate feature weights.506For extracting reordering examples, there are twopoints worth mentioning:1.
In the extraction of useful reordering examples,there is no length limitation over blocks com-pared with extracting bilingual phrases.2.
When enumerating all combinations of neigh-boring blocks, a good way to keep the numberof reordering examples acceptable is to extractsmallest blocks with the straight order whilelargest blocks with the inverted order .3 RefinementsIn this section we describe two refinements men-tioned above in detail.
First, we present fine-grained reordering heuristics using swapping win-dow and punctuation restriction.
Secondly, we inte-grate special bilingual phrases with sentence begin-ning/ending tags.3.1 Reordering HeuristicsWe conduct error analysis of the translation out-put of the baseline system and observe that Bruinsometimes incorrectly swaps two large neighboringblocks on the target side.
This happens frequentlywhen inverted order successfully challenges straightorder by the incorrect but strong support from thelanguage model and the MaxEnt-based reorderingmodel.
The reason is that only boundary wordsare used as evidences by both language model andMaxEnt-based reordering model when the decoderselects which merging rule (straight or inverted) tobe used 1.
However, statistics show that bound-ary words are not reliable for predicting the rightorder between two larger neighboring blocks.
Al-Onaizan and Papineni (2006) also proved that lan-guage model is insufficient to address long-distanceword reordering.
If a wrong inverted order is se-lected for two large consecutive blocks, incorrectlong-distance swapping happens.Yet another finding is that many incorrect swap-pings are related to punctuation marks.
First, thesource sequence within a pair of balanced punctua-tion marks (quotes and parentheses) should be kept1In (Xiong et al, 2006), the language model uses the left-most/rightmost words on the target side as evidences while theMaxEnt-based reordering model uses the boundary words onboth sides.Chinese: ??
: ??????????????????????????????
?Bruin: urgent action , he said : ?This is a veryserious situation , we can only hope that therewill be a possibility .
?Bruin+RH: he said : ?This is a very serious sit-uation , we can only hope that there will be thepossibility to expedite action .
?Ref: He said: ?This is a very serious situa-tion.
We can only hope that it is possible tospeed up the operation.
?Figure 1: An example of incorrect long-distanceswap.
The underlined Chinese words are incorrectlyswapped to the beginning of the sentence by theoriginal Bruin.
RH means reordering heuristics.within the punctuation after translation.
However,it is not always true when reordering is involved.Sometime the punctuation marks are distorted withthe enclosed words sequences being moved out.Secondly, it is found that a series of words is fre-quently reordered from one side of a structural mark,such as commas, semi-colons and colons, to theother side of the mark for long sentences contain-ing such marks.
Generally speaking, on Chinese-to-English translation, source words are translatedmonotonously relative to their adjacent punctuationmarks, which means their order relative to punctua-tion marks will not be changed.
In summary, punctu-ation marks place a strong constraint on word orderaround them.For example, in Figure 1, Chinese words ??????
are reordered to sentence beginning.
That isan incorrect long-distance swapping, which makesthe reordered words moved out from the balancedpunctuation marks ???
and ??
?, and incorrectlyprecede their previous mark ??
?.These incorrect swappings definitely jeopardizethe quality of translation.
Here we propose twostraightforward but effective heuristics to controland adjust the reordering, namely swapping windowand punctuation restriction.Swapping Window (SW): It constrains blockswapping in the following wayACTIVATE A ?
?A1, A2?
IF |A1s|+ |A2s| < sws507where |Ais| denotes the number of words on thesource side Ais of block Ai, sws is a pre-definedswapping window size.
Any inverted reordering be-yond the pre-defined swapping window size is pro-hibited.Punctuation Restriction (PR): If two neighbor-ing blocks include any of the punctuation marks p ?{?
?
?
?
?
?
?
?
?
?
?
?
}, the twoblocks will be merged with straight order.Punctuation marks were already used in pars-ing (Christine Doran, 2000) and statistical machinetranslation (Och et al, 2003).
In (Och et al,2003), three kinds of features are defined, all re-lated to punctuation marks like quotes, parenthesesand commas.
Unfortunately, no statistically signifi-cant improvement on the BLEU score was reportedin (Och et al, 2003).
In this paper, we considerthis problem from a different perspective.
We em-phasize that words around punctuation marks arereordered ungrammatically and therefore we posi-tively use punctuation marks as a hard decision torestrict such reordering around punctuations.
Thisis straightforward but yet results in significant im-provement on translation quality.The two heuristics described above can be usedtogether.
If the following conditions are satisfied,we can activate the inverted rule:|A1s|+ |A2s| < sws && P?
(A1s?A2s) = ?where P is the set of punctuation marks mentionedabove.The two heuristics can also speed up decoding be-cause decoding will be monotone within those spanswhich are not in accordance with both heuristics.For a sentence with n words, the total number ofspans is O(n2).
If we set sws = m (m < n),then the number of spans with monotone search isO((n?m)2).
With punctuation restriction, the non-monotone search space will reduce further.3.2 Phrases with Sentence Beginning/EndingTagsWe observe that in a sentence some phrases are morelikely to be located at the beginning, while otherphrases are more likely to be at the end.
This kind oflocation information with regard to the phrase posi-tion could be used for reordering.
A straightforwardway to use this information is to mark the begin-ning and ending of word-aligned sentences with ?s?and ?/s?
respectively.
This idea is borrowed fromlanguage modeling (Stolcke, 2002).
The corre-sponding tags at the source and target sentences arealigned to each other, i.e, the beginning tag of sourcesentences is aligned to the beginning tag of targetsentences, similarly for the ending tag.
Figure 2shows a word-aligned sentence pair annotated withthe sentence beginning and ending tag.During training, the sentence beginning and end-ing tags (?s?
and ?/s?)
are treated as words.
There-fore the phrase extraction and MaxEnt-based re-ordering training algorithm need not to be modified.Phrases with the sentence beginning/ending tag willbe extracted and MaxEnt-based reordering featureswith such tags will also be generated.
For example,from the word-aligned sentence pair in Figure 2, wecan extract tagged phrases like?s???
||| ?s?
Tibet ?s??
?/s?
||| achievements ?/s?and generate MaxEnt-based reordering features withtags likehi(o, b1, b2) ={ 1, b2.t1 = ?/s?, o = s0, otherwisewhere b1, b2 are blocks, t1 denotes the last sourceword, o = s means the order between two blocksis straight.
To avoid wrong alignments, we removetagged phrases where only the beginning/ending tagis extracted on either side of the phrases, such as?s?
||| ?s?
Those??/s?
||| ?/s?During decoding, we first annotate source sen-tences with the beiginning/ending tags, then trans-late them as what Bruin does.
Note that phraseswith sentence beginning/ending tags will be used inthe same way as ordinary phrases without such tagsduring decoding.
With the additional support of lan-guage model and MaxEnt-based reordering model,we observe that phrases with such tags are alwaysmoved to the beginning or ending of sentences cor-rectly.508?s?
??
??
??
??
??
??
?/s??s?
Tibet ?s financial work has gained remarkable achievements ?/s?Figure 2: A word-aligned sentence pair annotated with the sentence beginning and ending tag.4 EvaluationIn this section, we report the performance of the en-hanced Bruin on the NIST MT-05 and NIST MT-06Chinese-to-English translation tasks.
We describethe corpus, model training, and experiments relatedto the refinements described above.4.1 CorpusThe bilingual training data is derived from the fol-lowing various sources: the FBIS (LDC2003E14),Hong Kong Parallel Text (Hong Kong News andHong Kong Hansards, LDC2004T08), Xinhua News(LDC2002E18), Chinese News Translation TextPart1 (LDC2005T06), Translations from the Chi-nese Treebank (LDC2003E07), Chinese EnglishNews Magazine (LDC2005E47).
It contains 2.4Msentence pairs in total (68.1M Chinese words and73.8M English words).For the efficiency of minimum-error-rate training,we built our development set using sentences not ex-ceeding 50 characters from the NIST MT-02 evalu-ation test data (580 sentences).4.2 TrainingWe use exactly the same way and configuration de-scribed in (He et al, 2006) to preprocess the trainingdata, align words and extract phrases.We built two four-gram language models usingXinhua section of the English Gigaword corpus(181.1M words) and the English side of the bilin-gual training data described above respectively.
Weapplied modified Kneser-Ney smoothing as imple-mented in the SRILM toolkit (Stolcke, 2002).The MaxEnt-based reordering model is trainedusing the way of (Xiong et al, 2006).
The differenceis that we only use lexical features generated by tailwords of blocks, instead of head words, removingfeatures generated by the combination of two bound-ary words.Bleu(%) Secs/sentBruin 29.96 54.3sws RH1 RH12 RH1 RH125 29.65 29.95 42.6 41.210 30.55 31.27 46.2 41.815 30.26 31.40 48.0 42.220 30.19 31.42 49.1 43.2Table 1: Effect of reordering heuristics.
RH1 de-notes swapping window while RH12 denotes swap-ping window with the addition of punctuation re-striction.4.3 Translation ResultsTable 1 compares the BLEU scores 2 and the speedin seconds/sentence of the baseline system Bruinand the enhanced system with reordering heuristicsapplied.
The second row gives the BLEU score andthe average decoding time of Bruin.
The rows be-low row 3 show the BLEU scores and speed of theenhanced Bruin with different combinations of re-ordering heuristics.
We can clearly see that the re-ordering heuristics proposed by us have a two-foldeffect on the performance: improving the BLEUscore and decreasing the average decoding time.The example in Figure 1 shows how reorderingheuristics prevent incorrect long-distance swappingwhich is not in accordance with the punctuation re-striction.Table 1 also shows that a 15-word swapping win-dow is an inflexion point with the best tradeoff be-tween the decoding time and the BLEU score.
Wespeculate that in our corpus most reorderings hap-pen within a 15-word window.
We use the FBIScorpus to testify this hypothesis.
In this corpus, weextract all reordering examples using the algorithmof Xiong et al (2006).
Figure 3 shows the reorder-ing length distribution curve in this corpus.
Accord-2In this paper, all BLEU scores are case-sensitive and evalu-ated on the NIST MT-05 Chinese-to-English translation task ifthere is no special note.509010203040506070800510152025Percent(%)Reordering LengthFigure 3: Reordering length distribution.
The hor-izontal axis (reordering length) indicates the num-ber of words on the source side of two neighboringblocks which are to be swapped.
The vertical axisrepresents what proportion of reorderings with a cer-tain length is likely to be in all reordering exampleswith an inverted order.Bleu(%)Without Special Phrases 31.40With Special Phrases 32.01Table 2: Effect of integrating special phrases withthe sentence beginning/ending tag.ing to our statistics, reorderings within a windownot exceeding 15 words have a very high proportion,97.29%.
Therefore we set sws = 15 for later exper-iments.Table 2 shows the effect of integrating specialphrases with sentence beginning/ending tags intoBruin.
As special phrases accounts for only 1.95%of the total phrases used, an improvement of 0.6%in BLEU score is well worthwhile.
Further, the im-provement is statistically significant at the 99% con-fidence level according to Zhang?s significant tester(Zhang et al, 2004).
Figure 4 shows several exam-ples translated with special phrases integrated.
Wecan see that phrases with sentence beginning/endingtags are correctly selected and located at the rightplace.Table 3 shows the performance of two systems onthe NIST MT-05 Chinese test data, which are (1)System Refine MT-05 MT-06Bruin - 29.96 -EBruin RH 31.40 30.22EBruin RH+SP 32.01 -Table 3: Results of different systems.
The refine-ments RH, SP represent reordering heuristics andspecial phrases with the sentence beginning/endingtag, respectively.Bruin, trained on the large data described above; and(2) enhanced Bruin (EBruin) with different refine-ments trained on the same data set.
This table alsoshows the evaluation result of the enhanced Bruinwith reordering heuristics, obtained in the NIST MT-06 evaluation exercise.
35 ConclusionsWe have described in detail two refinements forBTG-based SMT which include reordering heuris-tics and special phrases with tags.
The refinementswere integrated into a well-established BTG-basedsystem Bruin introduced by Xiong et al (2006).
Re-ordering heuristics proposed here achieve a twofoldimprovement: better reordering and higher-speeddecoding.
To our best knowledge, we are the firstto integrate special phrases with the sentence be-ginning/ending tag into SMT.
Experimental resultsshow that the above refinements improve the base-line system significantly.For further improvements, we will investigatepossible extensions to the BTG grammars, e.g.learning useful nonterminals using unsupervisedlearning algorithm.AcknowledgementsWe would like to thank the anonymous review-ers for useful comments on the earlier version ofthis paper.
The first author was partially sup-ported by the National Science Foundations ofChina (No.
60573188) and the High TechnologyResearch and Development Program of China (No.2006AA010108) while he studied in the Institute ofComputing Technology, Chinese Academy of Sci-ences.3Full results are available at http://www.nist.gov/speech/tests/mt/doc/mt06eval official results.html.510With Special Phrases Without Special Phrases?s?
Japan had already pledged to provide 30 mil-lion US dollars of aid due to the tsunami victims ofthe country .
?/s?originally has pledged to provide 30 million USdollars of aid from Japan tsunami victimized coun-tries .?s?
the results of the survey is based on the re-sults of the chiefs of the Ukrainian National 50.96%cast by chiefs .
?/s?is based on the survey findings Ukraine 50.96% castby the chiefs of the chiefs of the country .?s?
and at the same time , the focus of the world havebeen transferred to other areas .
?/s?and at the same time , the global focus has shiftedhe.Figure 4: Examples translated with special phrases integrated.
The bold underlined words are special phraseswith the sentence beginning/ending tag.ReferencesYaser Al-Onaizan, Kishore Papineni.
2006.
DistortionModels for Statistical Machine Translation.
In Pro-ceedings of ACL-COLING 2006.David Chiang, Adam Lopez, Nitin Madnani, ChristofMonz, Philip Resnik, Michael Subotin.
2005.
TheHiero Machine Translation System: Extensions, Eval-uation, and Analysis.
In Proceedings of HLT/EMNLP,pages 779?786, Vancouver, October 2005.David Chiang.
2007.
Hierarchical Phrase-based Transla-tion.
In computational linguistics, 33(2).Christine Doran.
2000.
Punctuation in a LexicalizedGrammar.
In Proceedings of Workshop TAG+5, Paris.Zhongjun He, Yang Liu, Deyi Xiong, Hongxu Hou, QunLiu.
2006.
ICT System Description for the 2006TC-STAR Run #2 SLT Evaluation.
In Proceedings ofTC-STAR Workshop on Speech-to-Speech Translation,Barcelona, Spain.Liang Huang, Hao Zhang and Daniel Gildea.
2005.
Ma-chine Translation as Lexicalized Parsing with Hooks.In Proceedings of the 9th International Workshopon Parsing Technologies (IWPT-05), Vancouver, BC,Canada, October 2005.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of the Sixth Conference of theAssociation for Machine Translation in the Americas,pages 115?124.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.
InInternational Workshop on Spoken Language Transla-tion.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, Evan Herbst.
2007.
Moses: Open SourceToolkit for Statistical Machine Translation.
ACL2007, demonstration session, Prague, Czech Republic,June 2007.Yang Liu, Qun Liu, Shouxun Lin.
2006.
Tree-to-StringAlignment Template for Statistical Machine Transla-tion.
In Proceedings of ACL-COLING 2006.Robert Malouf.
2002.
A comparison of algorithms formaximum entropy parameter estimation.
In Proceed-ings of CoNLL-2002.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
SPMT: Statistical Ma-chine Translation with Syntactified Target LanguagePhraases.
In Proceedings of EMNLP.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statisti-cal machine translation.
In Proceedings of ACL 2002,pages 295?302.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, Dragomir Radev.
2003.
FinalReport of Johns Hopkins 2003 Summer Workshop onSyntax for Statistical Machine Translation.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing,volume 2, pages 901-904.Dekai Wu.
1995.
Stochastic inversion transductiongrammars, with application to segmentation, bracket-ing, and alignment of parallel corpora.
In Proceedingsof IJCAL 1995, pages 1328-1334, Montreal, August.511Dekai Wu.
1996.
A Polynomial-Time Algorithm for Sta-tistical Machine Translation.
In Proceedings of ACL1996.Deyi Xiong, Qun Liu and Shouxun Lin.
2006.
Maxi-mum Entropy Based Phrase Reordering Model for Sta-tistical Machine Translation.
In Proceedings of ACL-COLING 2006, pages 521?528.R.
Zens, H. Ney, T. Watanabe, and E. Sumita.
2004.
Re-ordering Constraints for Phrase-Based Statistical Ma-chine Translation.
In Proceedings of CoLing 2004,Geneva, Switzerland, pp.
205-211.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.
In-terpreting BLEU/NIST scores: How much improve-ment do we need to have a better system?
In Proceed-ings of LREC 2004, pages 2051?
2054.512
