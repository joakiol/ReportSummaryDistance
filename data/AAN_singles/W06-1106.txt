Proceedings of the Workshop on Linguistic Distances, pages 35?42,Sydney, July 2006. c?2006 Association for Computational LinguisticsSentence Comparisonusing Robust Minimal Recursion Semanticsand an OntologyRebecca Dridan}and Francis Bond}rdrid@csse.unimelb.edu.aubond@cslab.kecl.ntt.co.jp}The University of MelbourneNTT Communication Science Laboratories,Nippon Telegraph and Telephone CorporationAbstractWe design and test a sentence com-parison method using the frameworkof Robust Minimal Recursion Seman-tics which allows us to utilise the deepparse information produced by Jacy, aJapanese HPSG based parser and thelexical information available in our on-tology.
Our method was used for bothparaphrase detection and also for an-swer sentence selection for question an-swering.
In both tasks, results showedan improvement over Bag-of-Words, aswell as providing extra information use-ful to the applications.1 IntroductionComparison between sentences is required formany NLP applications, including questionanswering, paraphrasing, text summarizationand entailment tasks.
In this paper we showan RMRS (Robust Minimal Recursion Seman-tics, see Section 1.1) comparison algorithmthat can be used to compare sentences inany language that has RMRS generating toolsavailable.
Lexical resources of any languagecan be plugged in to give a more accurate andinformative comparison.The simplest and most commonly usedmethods of judging sentence similarity useword overlap { either looking for matchingword sequences, or comparing a Bag-of-Wordsrepresentation of each sentence.
Bag-of-Wordsdiscards word order, and any structure desig-nated by such, so that the cat snored and thedog slept is equivalent to the dog snored andthe cat slept.
Sequence matching on the otherhand requires exact word order matching andhence the game began quietly and the game qui-etly began are not considered a match.
Neithermethod allows for synonym matching.Hirao et al (2004) showed that they couldget a much more robust comparison usingdependency information rather than Bag-of-Words, since they could abstract away fromword order but still compare the importantelements of a sentence.
Using deep parsinginformation, such as dependencies, but alsodeep lexical resources where available, enablesa much more informative and robust compar-ison, which goes beyond lexical similarity.
Weuse the RMRS framework as our comparisonformat because it has the descriptive power toencode the full semantics, including argumentstructure.
It also enables easy combination ofdeep and shallow information and, due to itsat structure, is easy to manage computation-ally.1.1 Robust Minimal RecursionSemanticsRobust Minimal Recursion Semantics(RMRS) is a form ofat semantics which isdesigned to allow deep and shallow processingto use a compatible semantic representation,while being rich enough to support gener-alized quantiers (Frank, 2004).
The maincomponent of an RMRS representation isa bag of elementary predicates and theirarguments.An elementary predicate always has aunique label, a relation type, a relation nameand an ARG0 feature.
The example in Fig-ure 1 has a label of h5 which uniquely identi-es this predicate.
Relation types can eitherbe realpred for a predicate that relates di-rectly to a content word from the input text, orgpred for grammatical predicates which maynot have a direct referent in the text.
For ex-amples in this paper, a realpred is distin-guished by an underscore ( ) before the rela-tion name.The gpred relation names come from a35"unten slbl h5arg0 e6#Figure 1: Elementary predicate for-2 unten\drive"closed-set which specify common grammaticalrelations, but the realpred names are formedfrom the word in the text they relate to andthis is one way in which RMRS allows under-specication.
A full relation name is of theform lemma pos sense, where the pos (partof speech) is drawn from a small set of generaltypes including noun, verb and sahen (verbalnoun).
The sense is a number that identiesthe sense of the word within a particular gram-mar being used.
The POS and sense informa-tion are only used when available and hencethe unten s 1 is more specic but compati-ble with unten s or even unten.The arg0 feature (e6 in Figure 1) is thereferential index of the predicate.
Predicateswith the same arg0 are said to be referen-tially co-indexed and therefore have the samereferent in the text.A shallow parse might provide only the fea-tures shown in Figure 1, but a deep parse canalso give information about other argumentsas well as scoping constraints.
The featuresarg1..arg4 specify the indices of the semanticarguments of the relevant predicate, similar toPropBank's argument annotation (Kingsburyet al, 2002).
While the RMRS specicationdoes not dene semantic roles for the argnfeatures, in practice arg1 is generally used forthe agent and arg2 for the patient.
Fea-tures arg3 and arg4 have less consistency intheir roles.We will use (1) and (2) as examples of sim-ilar sentences.
They are denition sentencesfor one sense of ),'* doraiba- \driver",taken from two dierent lexicons.
(1) .30 & -2 !% 1jidosha wo unten suru hitocar acc drive do person\a person who drives a car"(2) .30 #" $ -2 /jidosha nado no unten shacar etc.
adn drive -er\a driver of cars etc.
"Examples of deep and shallow RMRS resultsfor (1) are given in Figure 2.
Deep results for(2) are given in Figure 3.2 AlgorithmThe matching algorithm is loosely based onRMRS comparison code included in the LKB(Copestake, 2002: hhttp://www.delph-in.net/lkb/i), which was used in Ritchie (2004),however that code used no outside lexical re-sources and we have substantially changed thematching algorithm.The comparison algorithm is language inde-pendent and can be used for any RMRS struc-tures.
It rst compares all elementary predi-cates from the RMRSs to construct a list ofmatch records and then examines, and poten-tially alters, the list of match records accord-ing to constraints encoded in the argn vari-ables.
Using the list of scored matches, thelowest scoring possible match set is found and,after further processing on that set, a similar-ity score is returned.
The threshold for de-ciding whether a pair of sentences should beconsidered similar or not can be determinedseparately for dierent applications.2.1 Matching PredicatesThe elementary predicates (EPs) of our RMRSstructures are divided into two groups - thosethat have a referent in the text, hereafterknown as content EPs, and those that don't.There are three kinds of content EP: real-preds, which correspond to content bearingwords that the grammar knows; gpreds witha carg (constant argument) feature, whichare used to represent proper names and num-bers; and gpreds with a predicate name start-ing with generic such as generic verb whichare used for unknown words that have onlybeen identied by their part of speech.
Allother EPs have no referent and are used toprovide information about the content EPs orabout the structure of the sentence as a whole.These non-content EPs can provide some use-ful information, but generally only in relationto other content EPs.Each content EP of the rst RMRS is com-pared to all content EPs in the second RMRS,as shown in Figure 4.Matches are categorised as exact, syn-onym, hypernym, hyponym or no matchand a numerical score is assigned.
The nu-36266666666666664text ',)%&+ $*top h1rels8>>>>>>><>>>>>>>>:"proposition m rellbl h1arg0 e2marg h3#"unknown rellbl h4arg0 e2arg x5#jidousha nlbl h6arg0 x7264udef rellbl h8arg0 x7rstr h9body h10375264unten slbl h11arg0 e13arg1 u12arg2 x7375hito nlbl h14arg0 x5264udef rellbl h15arg0 x5rstr h16body h17375"proposition m rellbl h10001arg0 e13marg h18#264topic rellbl h10002arg0 e19arg1 e13arg2 x53759>>>>>>>=>>>>>>>>;hcons fh3 qeq h4 ; h9 qeq h6 ; h16 qeq h14 ; h18 qeq h11ging fh11 ing h10002 ; h14 ing h10001g37777777777777526664text ',)%&+ $*top h9rels jidousha nlbl h1arg0 x2wo rellbl h3arg0 u4unten slbl h5arg0 e6suru rellbl h7arg0 e8hito nlbl h9arg0 x10 hcons fging fg37775Figure 2: Deep (top) and shallow (bottom) RMRS results for .30 & -2 !% 1266666666666664text ',) "!
# &+ (top h1rels8>>>>>>><>>>>>>>>:"proposition m rellbl h1arg0 e2marg h3#"unknown rellbl h4arg0 e2arg x5#jidousha nlbl h6arg0 x7264udef rellbl h8arg0 x7rstr h9body h10375"nado nlbl h10001arg0 u11arg1 x7#264udef rellbl h12arg0 x5rstr h13body h14375264unten slbl h15arg0 x5arg1 u16arg2 x7375"noun-relationlbl h17arg0 x5arg1 h18#"proposition m rellbl h18arg0 x5marg h19#"sha nlbl h10002arg0 u20arg1 x5#9>>>>>>>=>>>>>>>>;hcons fh3 qeq h4 ; h9 qeq h6 ; h13 qeq h17 ; h19 qeq h15ging fh6 ing h10001 ; h17 ing h10002g377777777777775Figure 3: RMRS representation for .30 #" $ -2 /foreach ep1 in contentEPs1foreach ep2 in contentEPs2(score, match) = match_EPs(ep1, ep2)if match != NO_MATCHadd_to_matches(ep1, ep2, score, match)endifdonedoneFigure 4: Predicate match pseudo-codemerical score represents the distance betweenthe two EPs, and hence an exact match isassigned a score of zero.The level of matching possible depends onthe lexical resources available.
With no extraresources, or only a dictionary to pick up or-thographic variants, the only match types pos-sible are exact and no match.
By addinga thesaurus, an ontology or a gazetteer, it isthen possible to return synonym, hypernymand hyponym match relations.
In our ex-periments we used the ontology described inSection 3.2.2, which provides all three extramatch types.
Adding a thesaurus only wouldenable synonym matching, while a gazetteercould be added to give, for example, Tokyo isa hyponym of city.Matches:hito_n - sha_n : HYPERNYM (2)jidosha_n - jidosha_n: EXACT (0)unten_s_2 - unten_s_2: EXACT (0)Figure 5: First pass match list for (1) and (2)At the end of the rst pass, a list of matchrecords shows all EP matches with their matchtype and score.
Each EP can have multiplepossible matches.
The output of comparing(1) and (2), with the RMRSes in Figures 2and 3, is shown in Figure 5.
This shows hito n(1 hito \person") tagged as a hypernym of37foreach match in matchesgpreds1 = get_gpreds_arg0(ep1{arg0})gpreds2 = get_gpreds_arg0(ep2{arg0})totalgpreds = len gpreds1 + len gpreds2foreach ep1 in gpreds1foreach ep2 in gpreds2if(match_gram_eps(ep1, ep2)remove(ep1, gpreds1)remove(ep2, gpreds2)endifdonedonegpreds_left = len gpreds1 + len gpreds2left = gpreds_left/totalgpredsmatch{score}+= left*gpredWeightdoneFigure 6: Matching ARG0ssha n (/ sha \-er" is a sux indicating a per-son, normally the agent of a verb: it is more re-strictive than English -er , in that it only refersto people).2.2 Constraints PassFor each possible match, all the non-contentEPs that have the same arg0 value as thecontent EPs in the match are examined, sincethese have the same referent.
If each non-content EP related to the content EP on oneside of the match can be matched to the non-content EPs related to the other content EP,no change is made.
If not, however, a penaltyis added to the match score, as shown in Fig-ure 6.
In our example, unten s 2 from the rstsentence has a proposition m rel referen-tially co-indexed, while the second unten s 2has a proposition m rel, a noun-relationand a udef rel, and so a small penalty isadded as shown in Figure 7.The second check in the constraint matchpass examines the arguments (arg1, arg2,arg3, arg4) of each of the matches.
It looksfor possible matches found between the EPslisted as argn for each match.
This check canresult in three separate results: both EPs havean argn but there is no potential match foundbetween the respective argn EPs, a potentialmatch has been found between the argn EPs,or only one of the EPs in the match has anargn feature.Where both EPs have an argn feature, thescore (distance) of the match is decreased orincreased depending on whether a match be-tween the argn variables was found.
Giventhat the RMRS denition does not specify aMatches:hito_n - sha_n : HYPERNYM (2.1)jidosha_n - jidosha_n: EXACT (0)unten_s_2 - unten_s_2: EXACT (0.05)Figure 7: Match listSlight penalty added to unten s 2 and hito nfor non-matching non-content EPs`meaning' for the argn variables, comparing,for example, arg1 variables from two dier-ent predicates may not necessarily be compar-ing the same semantic roles.
However, be-cause of the consistency found in arg1 andarg2 meaning this is still a useful check.
Ofcourse, if we are comparing the same relation,the args will all have the same meaning.
Thecomparison method allows for dierent penal-ties for each of arg1 to arg4, and also in-cludes a scaling factor so that mismatches inargs when comparing exact EP matches willhave more eect on the score than in nonexact matches.
If one EP does not havethe argn feature, no change is made to thescore.
This allows for the use of underspeci-ed RMRSs, in the case where the parse fails.At the end of this pass, the scores of thematches in the match list may have changedbut the number of matches is still the same.2.3 Constructing the SetsMatch sets are constructed by using a branch-and-bound decision tree.
Each match is con-sidered in order, and the tree is branched ifthe next match is possible, given the proceed-ing decisions.
Any branch which is more thantwo decisions away from the best score so faris pruned.
At the end of this stage, the lowestscoring match set is returned and then this isfurther processed.If no matches were found, processing stopsand a sentinel value is returned.
Otherwise,the non matching predicates are grouped to-gether by their arg0 value.
Scoping con-straints are checked and if any non matchingpredicate outscopes a content predicate it isadded to that grouping.
Hence if it outscopesa matching EP it becomes part of the match,otherwise it becomes part of a non-matchingEP group.Any group of grammatical EPs that sharesan arg0 but does not contain a content pred-icate is matched against any similar groupings38Best score is 0.799 for the match set:MATCHES:hito_n-sha_n: HYPERNYM:2.1jidousha_n-jidousha_n:EXACT:0unten_s_2-unten_s_2:EXACT:0.05proposition_m_rel-proposition_m_rel:EXACT:0UNMATCHED1:UNMATCHED2:u11: h10001:nado_nFigure 8: Verbose comparison outputin the other RMRS.
This type of match canonly be exact or no match and will makeonly a small dierence in the nal score.Content predicates that have not beenmatched by this stage are not processed anyfurther, although this is an area for furtherinvestigation.
Potentially negation and othermodiers could be processed at this point.2.4 OutputThe output of the comparison algorithm is anumeric score and also a representation of thenal best match found.The numerical score, using the default scor-ing parameters, ranges between 0 (perfectmatch) and 3.
As well as the no match score(-5), sentinel values are used to indicate miss-ing input data so it is possible to fall back toa shallow parse if the deep parse failed.Details of the match set are also returned forfurther processing or examination if the appli-cation requires.
This shows which predicateswere deemed to match, and with what score,and also shows the unmatched predicates.
Fig-ure 8 shows the output of our example com-parison.3 ResourcesWhile the comparison method is language in-dependent, the resources required are lan-guage specic. The resources fall in to twodierent categories: parsing and morpholog-ical analysis tools that produce the RMRSs,and lexical resources such as ontologies, dictio-naries and gazetteers for evaluating matches.3.1 ParsingJapanese language processing tools are freelyavailable.
We used the Japanese grammarJacy (Siegel and Bender, 2002), a deep parsingHPSG grammar that produces RMRSs for ourprimary input source.When parsing with Jacy failed, compar-isons could still be made with RMRS producedfrom shallow tools such as ChaSen (Mat-sumoto et al, 2000), a morphological analyseror CaboCha (Kudo and Matsumoto, 2002), aJapanese dependency parser.
Tools have beenbuilt to produced RMRS from the standardoutput of both those tools.The CaboCha output supplies similar de-pendency information to that of the Basic El-ements (BE) tool used by Hovy et al (2005b)for multi-document summarization.
Even thisintermediate level of parsing gives better com-parisons than either word or sequence overlap,since it is easier to compare meaningful ele-ments (Hovy et al, 2005a).3.2 Lexical ResourcesWhilst deep lexical resources are not availablefor every language, where they are available,they should be used to make comparisons moreinformative.
The comparison framework al-lows for dierent lexical resources to be addedto a pipeline.
The pipeline starts with a sim-ple relation name match, but this could be fol-lowed by a dictionary to extract orthographicvariants and then by ontologies such as Word-Net (Fellbaum, 1998) or GoiTaikei (Ikeharaet al, 1997), gazetteers or named entity recog-nisers to recognise names of people and places.The sections below detail the lexical resourceswe used within our experiments.3.2.1 The Lexeed Semantic DatabaseThe Lexeed Semantic Database of Japaneseis a machine readable dictionary that coversthe most familiar words in Japanese, basedon a series of psycholinguistic tests (Kasaharaet al, 2004).
Lexeed has 28,000 words dividedinto 46,000 senses and dened with 75,000 def-inition sentences.
Each entry includes a list oforthographic variants, and the pronunciation,in addition to the denitions.3.2.2 OntologyThe lexicon has been sense-tagged andparsed to give an ontology linking senses withvarious relations, principally hypernym andsynonym (Nichols et al, 2005).
For example,hhypernym, ),'* doraiba \driver", (,+ kurabu \club"i.
The ontology entries fornouns have been hand checked and corrected,including adding hypernyms for words where39the genus term in the denition was very gen-eral, e.g \a word used to refer insultingly tomen" where man is a more useful hypernymthan word for the dened term yarou.4 EvaluationWe evaluated the performance of the RMRScomparison method in two tasks.
First it wasused to indicate whether two sentences werepossible paraphrases.
In the second task, weused the comparison scores to select the mostlikely sentence to contain the answer to a ques-tion.4.1 ParaphrasingIn this task we compared denitions sen-tences for the same head word from two dier-ent Japanese dictionaries - the Lexeed dictio-nary (x3.2.1) and the Iwanami Kokugo Jiten(Iwanami: Nishio et al, 1994), the Japanesedictionary used in the SENSEVAL-2 Japaneselexical task (Shirai, 2002).There are 60,321 headwords and 85,870word senses in Iwanami.
Each sense in thedictionary consists of a sense ID and morpho-logical information (word segmentation, POStag, base form and reading, all manually post-edited).The denitions in Lexeed and Iwanami werelinked by headword and three Japanese nativespeakers assessed each potential pair of sensedenitions for the same head word to judgewhich denitions were describing the samesense.
This annotation not only describedwhich sense from each dictionary matched, butalso whether the denitions were equal, equiv-alent, or subsuming.The examples (1) and (2) are the denitionsof sense 2 of ),'* doraiba \driver" fromLexeed and Iwanami respectively.
They werejudged to be equivalent denitions by all threeannotators.4.1.1 MethodTest sets were built consisting of the Lexeedand Iwanami denition pairs that had been an-notated in the gold standard to be either non-matching, equal or equivalent.
Leaving outthose pairs annotated as having a subsump-tion relation made it a clearer task judgingbetween paraphrase or not, rather than ex-amining partial meaning overlap.
Ten sets of5,845 denition pairs were created, with eachset being equally split between matching andnon-matching pairs.
This gives data that is tosome extent semantically equivalent (the sameword sense is being dened), but with no guar-antee of syntactic equivalence.Comparisons were made between the rstsentence of each denition with both a Bag-of-Words comparison method and our RMRSbased method.
If RMRS output was not avail-able from Jacy (due to a failed parse), RMRSfrom CaboCha was used as a fall back shallowparse result.Scores were output and then the bestthreshold score for each method was calculatedon one of the 10 sets.
Using the calculatedthreshold score, pairs were classied as eithermatching or non-matching.
Pairs classied asmatching were evaluated as correct if the goldstandard annotation was either equal or equiv-alent.4.1.2 ResultsThe Bag-of-Words comparison got an av-erage accuracy over all sets of 73.9% with100% coverage.
A break down of the resultsshows that this method was more accurate(78%) in correctly classifying non-matchesthan matches (70%).
This is to be expectedsince it won't pick up equivalences where aword has been changed for its synonym.The RMRS comparison had an accuracywas 78.4% with almost 100% coverage, an im-provement over the Bag-of-Words.
The RMRSbased method was also more accurate overnon matches (79.9%) than matches (76.6%),although the dierence is not as large.
Con-sidering only those sentences with a parse fromJACY gave an accuracy of 81.1% but with acoverage of only 46.1%.
This shows that deepparsing improves precision, but must be usedin conjunction with a shallower fallback.To explore what eect the ontology was hav-ing on the results, another evaluation was per-formed without the ontology matching.
Thishad an accuracy of 77.3% (78.1% using Jacy,46.1% coverage).
This shows that the infor-mation available in the ontology denitely im-proves scores, but that even without that sortof deep lexical resource, the RMRS matchingcan still improve on Bag-of-Words using justsurface form abstraction and argument match-ing.404.2 Answer Sentence SelectionTo emulate a part of the question answeringpipeline, we used a freely available set of 2000Japanese questions, annotated with, amongother things, answer and answer document ID(Sekine et al, 2002).
The document IDs forthe answer containing documents refer to theMainichi Newspaper 1995 corpus which hasbeen used as part of the document collectionfor NTCIR's Question Answering Challenges.The documents range in length from 2 to 83sentences.4.2.1 MethodFor every question, we compared it to eachsentence in the answer document.
The sen-tence that has the best similarity to the ques-tion is returned as the most likely to con-tain the answer.
For this sort of compari-son, an entails option was added that changesthe similarity scoring method slightly so thatonly non-matches in the rst sentence increasethe score.
The rationale being that in Ques-tion Answering (and also in entailment), ev-erything present in the question (or hypoth-esis) should be matched by something in theanswer, but having extra, unmatched informa-tion in the answer should not be penalised.The task is evaluated by checking if the an-swer does exist in the sentence selected.
Thismeans that more than one sentence can be thecorrect answer for any question (if the answeris mentioned multiple times in the article).4.2.2 ResultsThe Bag-of-Words comparison correctlyfound a sentence containing the answer for62.5% of the 2000 questions.
The RMRS com-parison method gave a small improvement,with a result of 64.3%.
Examining the datashowed this to be much harder than the para-phrase task because of the language level in-volved.
In the paraphrasing task, the sen-tences averaged around 10 predicates each,while the questions and sentences in this taskaveraged over 3 times longer, with about 34predicates.
The words used were also lesslikely to be in the lexical resources both be-cause more formal, less familiar words wereused, and also because of the preponderanceof named entities.
Adding name lists of peo-ple, places and organisations would greatly im-prove the matching in this instance.5 Future Directions5.1 ApplicationsSince the comparison method was writtento be language independent, the next stageof evaluation would be to use it in a non-Japanese task.
The PASCAL RecognisingTextual Entailment (RTE) Challenge (Daganet al, 2005) is one recent English task whereparticipants used sentence comparison exten-sively.
While the task appears to call for in-ference and reasoning, the top 5 participat-ing groups used statistical methods and wordoverlap only.
Vanderwende et al (2005) did amanual evaluation of the test data and foundthat 37% could be decided on syntactic infor-mation alone, while adding a thesaurus couldincrease that coverage to 49%.
This meansthat RMRS comparison has the potential toperform well.
Not only does it improve onbasic word overlap, but it allows for easy ad-dition of a thesaurus or dictionary.
Further,because of the detailed match output avail-able, the method could be extended in postprocessing to encompass some basic inferencemethods.Aside from comparing sentences, the RMRScomparison can be used to compare the RMRSoutput of dierent tools for the same sentenceso that the compatibility of the outputs canbe evaluated and improved.5.2 ExtensionsOne immediate future improvement plannedis to add named entity lists to the lexical re-sources so that names of people and placescould be looked up.
This would allow partialmatches between, e.g., Clinton is a hyponymof person, which would be particularly usefulfor Question Answering.Another idea is to add a bilingual dictio-nary and try cross-lingual comparisons.
Asthe RMRS abstracts away much of the surfacespecic details, this might be useful for sen-tence alignment.To go beyond sentence by sentence compar-ison, we have plans to implement a methodfor multi-sentence comparisons by either com-bining the RMRS structures before compari-son, or post-processing the sentence compari-son outputs.
This could be particularly inter-esting for text summarization.416 ConclusionsDeep parsing information is useful for com-paring sentences and RMRS gives us a use-ful framework for utilising this informationwhen it is available.
Our RMRS compari-son was more accurate then basic word over-lap similarity measurement particularly in theparaphrase task where synonyms were of-ten used.
Even when the ontology was notused, abstracting away from surface form, andmatching arguments did give an improvement.Falling back to shallow parse methods in-creases the robustness which is often an issuefor tools that use deep processing, while stillallowing the use of the most accurate informa-tion available.The comparison method is language agnos-tic and can be used for any language that hasRMRS generating tools.
The output is muchmore informative than Bag-of-Words, mak-ing it useful in many applications that needto know exactly how a sentence matched oraligned.AcknowledgementsThis work was started when the rst author was a vis-itor at the NTT Communication Science Laboratories,Nippon Telegraph and Telephone Corporation.
Therst author was also supported by the Pam Todd schol-arship from St Hilda's College.
We would like to thankthe NTT Natural Language Research Group and twoanonymous reviewers for their valuable input.ReferencesAnn Copestake.
2002.
Implementing Typed FeatureStructure Grammars.
CSLI Publications.Ido Dagan, Oren Glickman, and Bernado Magnini.2005.
The PASCAL recognising textual entailmentchallenge.
In Proceedings of the PASCAL ChallengesWorkshop on Recognising Textual Entailment.Christine Fellbaum.
1998.
A semantic network of En-glish verbs.
In Christine Fellbaum, editor, WordNet:An Electronic Lexical Database, chapter 3, pages 70{104.
MIT Press.Anette Frank.
2004.
Constraint-based RMRS con-struction from shallow grammars.
In 20th Inter-national Conference on Computational Linguistics:COLING-2004, pages 1269{1272.
Geneva.Tsutomu Hirao, Jun Suzuki, Hideki Isozaki, andEisaku Maeda.
2004.
Dependency-based sentencealignment for multiple document summarization.
InProceedings of the COLING.Eduard Hovy, Junichi Fukumoto, Chin-Yew Lin, andLiang Zhao.
2005a.
Basic elements.
(http://www.isi.edu/cyl/BE).Eduard Hovy, Chin-Yew Lin, and Liang Zhao.
2005b.A BE-based multi-document summarizer with sen-tence compression.
In Proceedings of MultilingualSummarization Evaluation.Satoru Ikehara, Masahiro Miyazaki, Satoshi Shirai,Akio Yokoo, Hiromi Nakaiwa, Kentaro Ogura,Yoshifumi Ooyama, and Yoshihiko Hayashi.
1997.Goi-Taikei | A Japanese Lexicon.
Iwanami Shoten,Tokyo.
5 volumes/CDROM.Kaname Kasahara, Hiroshi Sato, Francis Bond,Takaaki Tanaka, Sanae Fujita, Tomoko Kanasugi,and Shigeaki Amano.
2004.
Construction of aJapanese semantic lexicon: Lexeed.
SIG NLC-159,IPSJ, Tokyo.
(in Japanese).Paul Kingsbury, Martha Palmer, and Mitch Marcus.2002.
Adding semantic annotation to the penn tree-bank.
In Proceedings of the Human Language Tech-nology 2002 Conference.Taku Kudo and Yuji Matsumoto.
2002.
Japanesedependency analysis using cascaded chunking.
InCoNLL 2002: Proceedings of the 6th Conferenceon Natural Language Learning 2002 (COLING 2002Post-Conference Workshops), pages 63{69.
Taipei.Yuji Matsumoto, Kitauchi, Yamashita, Hirano, Mat-suda, and Asahara.
2000.
Nihongo Keitaiso KaisekiSystem: Chasen, version 2.2.1 manual edition.http://chasen.aist-nara.ac.jp.Eric Nichols, Francis Bond, and Daniel Flickinger.2005.
Robust ontology acquisition from machine-readable dictionaries.
In Proceedings of the Inter-national Joint Conference on Articial IntelligenceIJCAI-2005, pages 1111{1116.
Edinburgh.Minoru Nishio, Etsutaro Iwabuchi, and Shizuo Mizu-tani.
1994.
Iwanami Kokugo Jiten Dai Go Han[Iwanami Japanese Dictionary Edition 5].
IwanamiShoten, Tokyo.
(in Japanese).Anna Ritchie.
2004.
Compatible RMRS representa-tions from RASP and the ERG.
Technical ReportUCAM-CL-TR-661.Satoshi Sekine, Kiyoshi Sudo, Yusuke Shinyama,Chikashi Nobata, Kiyotaka Uchimoto, and HitoshiIsahara.
2002.
NYU/CRL QA system, QAC ques-tion analysis and CRL QA data.
In Working Notesof NTCIR Workshop 3.Kiyoaki Shirai.
2002.
Construction of a word sensetagged corpus for SENSEVAL-2 Japanese dictio-nary task.
In Third International Conference onLanguage Resources and Evaluation (LREC-2002),pages 605{608.Melanie Siegel and Emily M. Bender.
2002.
E-cient deep processing of Japanese.
In Proceedingsof the 3rd Workshop on Asian Language Resourcesand International Standardization at the 19th Inter-national Conference on Computational Linguistics.Taipei.Lucy Vanderwende, Deborah Coughlin, and Bill Dolan.2005.
What syntax can contribute in entailmenttask.
In Proceedings of the PASCAL ChallengesWorkshop on Recognising Textual Entailment.42
