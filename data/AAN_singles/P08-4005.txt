Proceedings of the ACL-08: HLT Demo Session (Companion Volume), pages 17?19,Columbus, June 2008. c?2008 Association for Computational LinguisticsInteractive ASR Error Correction for Touchscreen DevicesDavid Huggins-DainesLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAdhuggins@cs.cmu.eduAlexander I. RudnickyLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAair@cs.cmu.eduAbstractWe will demonstrate a novel graphical inter-face for correcting search errors in the out-put of a speech recognizer.
This interfaceallows the user to visualize the word latticeby ?pulling apart?
regions of the hypothesisto reveal a cloud of words simlar to the ?tagclouds?
popular in many Web applications.This interface is potentially useful for dicta-tion on portable touchscreen devices such asthe Nokia N800 and other mobile Internet de-vices.1 IntroductionFor most people, dictating continuous speech is con-siderably faster than entering text using a keyboardor other manual input device.
This is particularlytrue on mobile devices which typically have no hard-ware keyboard whatsoever, a 12-digit keypad, or atbest a miniaturized keyboard unsuitable for touchtyping.However, the effective speed of text input usingspeech is significantly reduced by the fact that eventhe best speech recognition systems make errors.After accounting for error correction, the effectivenumber of words per minute attainable with speechrecognition drops to within the range attainable byan average typist (Moore, 2004).
Moreover, on amobile phone with predictive text entry, it has beenshown that isolated word dictation is actually slowerthan using a 12-digit keypad for typing SMS mes-sages (Karpov et al, 2006).2 DescriptionIt has been shown that multimodal error correctionmethods are much more effective than using speechalone (Lewis, 1999).
Mobile devices are increas-ingly being equipped with touchscreens which lendthemselves to gesture-based interaction methods.Therefore, we propose an interactive method ofvisualizing and browsing the word lattice using ges-tures in order to correct speech recognition errors.The user is presented with the decoding result in alarge font, either in a window on the desktop, or in afull-screen presentation on a touchscreen device.
Ifthe utterance is too long to fit on the screen, the usercan scroll left and right using touch gestures.
Theinitial interface is shown in Figure 1.Figure 1: Initial hypothesis viewWhere there is an error, the user can ?pull apart?the result using a touch stroke (or a multitouch ges-ture where supported), revealing a ?cloud?
of hy-pothesis words at that point in the utterance, asshown in Figure 2.It is also possible to expand the time interval overwhich the cloud is calculated by dragging sideways,resulting in a view like that in Figure 3.
The usercan then select zero or more words to add to the hy-pothesis string in place of the errorful text which was?exploded?, as shown in Figure 4.17Figure 2: Expanded word viewFigure 3: Word cloud expanded in timeThe word cloud is constructed by finding allwords active within a time interval whose log poste-rior probability falls within range of the most prob-able word.
Word posterior probabilities are cal-culated using the forward-backward algorithm de-scribed in (Wessel et al, 1998).
Specifically, given aword lattice in the form of a directed acyclic graph,whose nodes represent unique starting points t intime, and whose edges represent the acoustic likeli-hoods of word hypotheses wts spanning a given timeinterval (s, t), we can calculate the forward variable?t(w), which represents the joint probability of allword sequences ending in wts and the acoustic ob-servations up to time t, as:?t(w) = P (Os1, wts) =?vts?prev(w)P (w|v)P (wts)?s(v)Here, P (w|v) is the bigram probability of (v, w)obtained from the language model and P (wts) is theacoustic likelihood of the word model w given theobserved speech from time s to t, as approximatedby the Viterbi path score.Figure 4: Selecting replacement wordsLikewise, we can compute the backward variable?t(w), which represents the conditional probabil-ity of all word sequences beginning in wts and theacoustic observations from time t + 1 to the end ofthe utterance, given wts:?t(w) = P (OTt |wts) =?vet?succ(w)P (v|w)P (vet )?e(v)The posterior probability P (wts|OT1 ) can then beobtained by multiplication and normalization:P (wts|OT1 ) =P (wts, OT1 )P (OT1 )=?t(w)?t(w)P (OT1 )This algorithm has a straightforward extension totrigram language models which has been omittedhere for simplicity.This interface is inspired by the web browserzooming interface used on the Apple iPhone (Ap-ple, Inc., 2008), as well as the Speech Dasherlattice correction tool (Vertanen, 2004).
We feelthat it is potentially useful not only for auto-matic speech recognition, but also for machinetranslation and any other situation in whicha lattice representation of a possibly errorfulhypothesis is available.
A video of this in-terface in Ogg Theora format1 can be viewed athttp://www.cs.cmu.edu/?dhuggins/touchcorrect.ogg.1For Mac OS X: http://xiph.org/quicktime/download.htmlFor Windows: http://www.illiminable.com/ogg/downloads.html183 Script OutlineFor our demonstration, we will have availablea poster describing the interaction method beingdemonstrated.
We will begin by describing the mo-tivation for this work, followed by a ?silent?
demoof the correction method itself, using pre-recordedaudio.
We will then demonstrate live speech inputand correction using our own voices.
The audiencewill then be invited to test the interaction method ona touchscreen device (either a handheld computer ora tablet PC).4 RequirementsTo present this demo, we will be bringing two NokiaInternet Tablets as well as a laptop and possibly aTablet PC.
We have no requirements from the con-ference organizers aside from a suitable number ofpower outlets, a table, and a poster board.AcknowledgementsWe wish to thank Nokia for donating an N800 Inter-net Tablet used to develop this software.ReferencesE.
Karpov, I.
Kiss, J. Leppa?nen, J. Olsen, D. Oria, S.Sivadas and J. Tian 2006.
Short Message Sys-tem dictation on Series 60 mobile phones.
Work-shop on Speech in Mobile and Pervasive Environ-ments (SiMPE) in Conjunction with MobileHCI 2006.Helsinki, Finland.Keith Vertanen 2004.
Efficient Computer InterfacesUsing Continuous Gestures, Language Models, andSpeech.
M.Phil Thesis, University of Cambridge,Cambridge, UK.Apple, Inc. 2008. iPhone: Zoom-ing In to Enlarge Part of a Webpage.http://docs.info.apple.com/article.html?artnum=305899Roger K. Moore 2004.
Modelling Data Entry Rates forASR and Alternative Input Methods.
Proceedings ofInterspeech 2004.
Jeju, Korea.James R. Lewis 1999.
Effect of Error Correction Strat-egy on Speech Dictation Throughput Proceedings ofthe Human Factors and Ergonomics Society 43rd An-nual Meeting.Frank Wessel, Klaus Macherey, Ralf Schlu?ter 1998.
Us-ing Word Probabilities as Confidence Measures.
Pro-ceedings of ICASSP 1998.19
