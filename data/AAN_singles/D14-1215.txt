Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2008?2018,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsBrighter than Gold: Figurative Language in User Generated ComparisonsVlad Niculae and Cristian Danescu-Niculescu-MizilMPI-SWSCornell Universityvniculae@mpi-sws.org, cristian@mpi-sws.orgAbstractComparisons are common linguistic de-vices used to indicate the likeness of twothings.
Often, this likeness is not meantin the literal sense?for example, ?I sleptlike a log?
does not imply that logs ac-tually sleep.
In this paper we propose acomputational study of figurative compar-isons, or similes.
Our starting point is anew large dataset of comparisons extractedfrom product reviews and annotated forfigurativeness.
We use this dataset to char-acterize figurative language in naturallyoccurring comparisons and reveal linguis-tic patterns indicative of this phenomenon.We operationalize these insights and ap-ply them to a new task with high relevanceto text understanding: distinguishing be-tween figurative and literal comparisons.Finally, we apply this framework to ex-plore the social context in which figurativelanguage is produced, showing that simi-les are more likely to accompany opinionsshowing extreme sentiment, and that theyare uncommon in reviews deemed helpful.1 IntroductionIn argument similes are like songs in love; theydescribe much, but prove nothing.?
Franz KafkaComparisons are fundamental linguistic devicesthat express the likeness of two things?be it en-tities, concepts or ideas.
Given that their work-ing principle is to emphasize the relation betweenthe shared properties of two arguments (Bredin,1998), comparisons can synthesize important se-mantic knowledge.Often, comparisons are not meant to be under-stood literally.
Figurative comparisons are an im-portant figure of speech called simile.
Consider thefollowing two examples paraphrased from Ama-zon product reviews:(1) Sterling is much cheaper than gold.
(2) Her voice makes this song shine brighter than gold.In (1) the comparison draws on the relation be-tween the price property shared by the two metals,sterling and gold.
While (2) also draws on a com-mon property (brightness), the polysemantic use(vocal timbre vs. light reflection) makes the com-parison figurative.Importantly, there is no general rule separatingliteral from figurative comparisons.
More gen-erally, the distinction between figurative and lit-eral language is blurred and subjective (Hanks,2006).
Multiple criteria for delimiting the twohave been proposed in the linguistic and philo-sophical literature?for a comprehensive review,see Shutova (2010)?but they are not without ex-ceptions, and are often hard to operationalize in acomputational framework.
When considering thespecific case of comparisons, such criteria cannotbe directly applied.Recently, the simile has received increasing at-tention from linguists and lexicographers (Moon,2008; Moon, 2011; Hanks, 2013) as it becameclearer that similes need to be treated separatelyfrom metaphors since they operate on funda-mentally different principles (Bethlehem, 1996).Metaphors are linguistically simple structures hid-ing a complex mapping between two domains,through which many properties are transferred.For example the conceptual metaphor of life asa journey can be instantiated in many particularways: being at a fork in the road, reaching the endof the line (Lakoff and Johnson, 1980).
In contrast,the semantic context of similes tends to be veryshallow, transferring a single property (Hanks,2013).
Their more explicit syntactic structure al-lows, in exchange, for more lexical creativity.
AsHanks (2013) puts it, similes ?tend to license all2008sorts of logical mayhem.?
Moreover, the over-lap between the expressive range of similes andmetaphors is now known to be only partial: thereare similes that cannot be rephrased as metaphors,and the other way around (Israel et al., 2004).
Thissuggests that figurativeness in similes should bemodeled differently than in metaphors.
To furtherunderline the necessity of a computational modelfor similes, we give the first estimate of their fre-quency in the wild: over 30% of comparisons arefigurative.1We also confirm that a state of the artmetaphor detection system performs poorly whenapplied directly to the task of detecting similes.In this work we propose a computational studyof figurative language in comparisons.
To this end,we build the first large collection of naturally oc-curring comparisons with figurativeness annota-tion, which we make publicly available.
Usingthis resource we explore the linguistic patterns thatcharacterize similes, and group them in two con-ceptually distinctive classes.
The first class con-tains cues that are agnostic of the context in whichthe comparison appears (domain-agnostic cues).For example, we find that the higher the seman-tic similarity between the two arguments, the lesslikely it is for the comparison to be figurative?inthe examples above, sterling is semantically verysimilar to gold, both being metals, but song andgold are semantically dissimilar.
The second typeof cues are domain-specific, drawing on the in-tuition that the domain in which a comparison isused is a factor in determining its figurativeness.We find, for instance, that the less specific a com-parison is to the domain in which it appears, themore likely it is to be used in a figurative sense(e.g., in example (2), gold is very unexpected inthe musical domain).We successfully exploit these insights in a newprediction task relevant to text understanding: dis-criminating figurative comparisons from literalones.
Encouraged by the high accuracy of oursystem?which is within 10% of that obtained byhuman annotators?we automatically extend thefigurativeness labels to 80,000 comparisons occur-ring in product reviews.
This enables us to conducta fine-grained analysis of how comparison usageinteracts with their social context, opening up aresearch direction with applications in sentimentanalysis and opinion mining.
In particular we find1This estimate is based on the set of noun-noun compar-isons with non-identical arguments collected for this studyfrom Amazon.com product reviews.that figurative comparisons are more likely to ac-company reviews showing extreme sentiment, andthat they are uncommon in opinions deemed as be-ing helpful.
To the best of our knowledge, this isthe first time figurative language is tied to the so-cial context in which it appears.To summarize, the main contributions of thiswork are as follows:?
it introduces the first large dataset of compar-isons with figurativeness annotations (Sec-tion 3);?
it unveils new linguistic patterns characteriz-ing figurative comparisons (Section 4);?
it introduces the task of distinguishing figura-tive from literal comparisons (Section 5);?
it establishes the relation between figurativelanguage and the social context in which itappears (Section 6).2 Further Related WorkCorpus studies on figurative language in compar-isons are scarce, and none directly address thedistinction between figurative and literal compar-isons.
Roncero et al.
(2006) observed, by search-ing the web for several stereotypical comparisons(e.g., education is like a stairway), that similesare more likely to be accompanied by explana-tions than equivalent metaphors (e.g., educationis a stairway).
Related to figurativeness is irony,which Veale (2012a) finds to often be lexicallymarked.
By using a similar insight to filter outironic comparisons, and by assuming that the restare literal, Veale and Hao (2008) learn stereotyp-ical knowledge about the world from frequentlycompared terms.
A similar process has been ap-plied to both English and Chinese by Li et al.
(2012), thereby encouraging the idea that the tropebehaves similarly in different languages.
A relatedsystem is the Jigsaw Bard (Veale and Hao, 2011),a thesaurus driven by figurative conventional sim-iles extracted from the Google N-grams.
This sys-tem aims to build and generate canned expressionsby using items frequently associated with the sim-ile pattern above.
An extension of the principles ofthe Jigsaw Bard is found in Thesaurus Rex (Vealeand Li, 2013), a data-driven partition of words intoad-hoc categories.
Thesaurus Rex is constructedusing simple comparison and hypernym patterns2009and is able to provide weighted lists of categoriesfor given words.In text understanding systems, literal compar-isons are used to detect analogies between relatedgeographical places (Lofi et al., 2014).
Tandon etal.
(2014) use relative comparative patterns (e.g.,X is heavier than Y) to enrich a common-senseknowledge base.
Jindal and Liu (2006) extractgraded comparisons from various sources, withthe objective of mining consumer opinion aboutproducts.
They note that identifying objective vs.subjective comparisons?related to literality?isan important future direction.
Given that manycomparisons are figurative, a system that discrim-inates literal from figurative comparisons is essen-tial for such text understanding and informationretrieval systems.The vast majority of previous work on figu-rative language focused on metaphor detection.Tsvetkov et al.
(2014a) propose a cross-lingualsystem based on word-level conceptual featuresand they evaluate it on Subject-Verb-Object triplesand Adjective-Noun pairs.
Their features includeand extend the idea of abstractness used by Turneyet al.
(2011) for Adjective-Noun metaphors.
Hovyet al.
(2013) contribute an unrestricted metaphorcorpus and propose a method based on tree ker-nels.
Bridging the gap between metaphor identifi-cation and interpretation, Shutova and Sun (2013)proposed an unsupervised system to learn source-target domain mappings.
The system fits concep-tual metaphor theory (Lakoff and Johnson, 1980)well, at the cost of not being able to tackle figu-rative language in general, and similes in particu-lar, as similes do not map entire domains to oneanother.
Since similes operate on fundamentallydifferent principles than metaphors, our work pro-poses a computational approach tailored specifi-cally for comparisons.3 Background and Data3.1 Structure of a comparisonUnlike metaphors, which are generally unre-stricted, comparisons are more structured but alsomore lexically and semantically varied.
This en-ables a more structured computational representa-tion of which we take advantage.
The constituentsof a comparison according to Hanks (2012) are:?
the TOPIC, sometimes called tenor: it is usu-ally a noun phrase and acts as logical subject;?
the VEHICLE: it is the object of the compari-son and is also usually a noun phrase;?
the shared PROPERTY or ground: it expresseswhat the two entities have in common?it canbe explicit but is often implicit, left for thereader to infer;?
the EVENT (eventuality or state): usually averb, it sets the frame for the observation ofthe common property;?
the COMPARATOR: commonly a preposition(like) or part of an adjectival phrase (betterthan), it is the trigger word or phrase thatmarks the presence of a comparison.The literal example (1) would be segmented as:[Sterling /TOPIC] [is /EVENT] much [cheaper/PROPERTY] [than /COMPARATOR] [gold /VE-HICLE]3.2 AnnotationPeople resort to comparisons often when mak-ing descriptions, as they are a powerful way ofexpressing properties by example.
For this rea-son we collect a dataset of user-generated compar-isons in Amazon product reviews (McAuley andLeskovec, 2013), where users have to be descrip-tive and precise, but also to express personal opin-ion.
We supplement the data with a smaller set ofcomparisons from WaCky and WaCkypedia (Ba-roni et al., 2009) to cover more genres.
In pre-liminary work, we experimented with dependencyparse tree patterns for extracting comparisons andlabeling their parts (Niculae, 2013).
We use thesame approach, but with an improved set of pat-terns, to extract comparisons with the COMPARA-TORS like, as and than.2We keep only the matcheswhere the TOPIC and the VEHICLE are nouns, andthe PROPERTY, if present, is an adjective, whichis the typical case.
Also, the head words of theconstituents are constrained to occur in the distri-butional resources used (Baroni and Lenci, 2010;Faruqui and Dyer, 2014).32We process the review corpus with part-of-speech tag-ging using the IRC model for TweetNLP (Owoputi et al.,2013; Forsyth and Martell, 2007) and dependency parsingusing the TurboParser standard model (Martins et al., 2010).3Due to the strong tendency of comparisons with the sameTOPIC and VEHICLE to be trivially literal in the WaCkyexamples, we filtered out such examples from the Amazonproduct reviews.
We also filtered proper nouns using a capi-talization heuristic.2010We proceed to validate and annotate for figu-rativeness a random sample of the comparisonsextracted using the automated process describedabove.
The annotation is performed using crowd-sourcing on the Amazon Mechanical Turk plat-form, in two steps.
First, the annotators are askedto determine whether a displayed sentence is in-deed a comparison between the highlighted words(TOPIC and VEHICLE).
Sentences qualified bytwo out of three annotators as comparisons areused in the second round, where the task is torate how metaphorical a comparison is.
We usea scale of 1 to 4 following Turney et al.
(2011),and then binarize to consider scores of 1?2 as lit-eral and 3?4 as figurative.
Finally, in this work weonly consider comparisons where all three annota-tors agree on this binary notion of figurativeness.For both tasks, we provide guidelines mostly inthe form of examples and intuition, motivated onone hand by the annotators not having specializedknowledge, and on the other hand by the observa-tion that the literal-figurative distinction is subjec-tive.
All annotators have the master worker qual-ification, reside in the U.S. and completed a lin-guistic background questionnaire that verifies theirexperience with English.
In both tasks, controlsentences with confidently known labels are usedto filter low quality answers; in addition, we testannotators with a simple paraphrasing task shownto be effective for eliciting and verifying linguis-tic attention (Munro et al., 2010).
Both tasksseem relatively difficult for humans, with inter-annotator agreement given by Fleiss?
k of 0.48for the comparison identification task and of 0.54for the figurativeness annotation after binarization.This is comparable to 0.57 reported by Hovy et al.
(2013) for general metaphor labeling.
We showsome statistics about the collected data in Table 1.Overall, this is a costly process: out of 2400 auto-matically extracted comparison candidates, about60% were deemed by the annotators to be actualcomparisons and only 12% end up being selectedconfidently enough as figurative comparisons.Our dataset of human-filtered comparisons,with the scores given by the three annotators,is made publicly available to encourage furtherwork.4This also includes about 400 comparisonswhere the annotators do not agree perfectly on bi-nary figurativeness.
Such cases can be interest-ing to other analyses, even if we don?t consider4http://vene.ro/figurative-comparisons/Domain fig.
lit.
% fig.Books 177 313 36%Music 45 68 40%Electronics 23 105 18%Jewelery 9 126 7%WaCky 19 79 19%Total 273 609 31%Table 1: Figurativeness annotation results.
Onlycomparisons where all three annotators agree areconsidered.them in our experiments.
It is worth noting thatthe existing corpora annotated for metaphor can-not be directly used to study comparisons.
For ex-ample, in TroFi (Birke and Sarkar, 2006), a cor-pus of 6436 sentences annotated for figurative-ness, we only find 42 noun-noun comparisons withsentence-level (thus noisy) figurativeness labels.4 Linguistic InsightsWe now proceed to exploring the linguistic pat-terns that discriminate figurative from literal com-parisons.
We consider two broad classes of cues,which we discuss next.4.1 Domain-specific cuesFigurative language is often used for striking ef-fects, and comparisons are used to describe newthings in terms of something given (Hanks, 2013).Since the norms that define what is surprising andwhat is well-known vary across domains, we ex-pect that such contextual information should playan important role in figurative language detection.This is a previously unexplored dimension of figu-rative language, and Amazon product reviews of-fer a convenient testbed for this intuition since cat-egory information is provided.Specificity To estimate whether a compari-son can be considered striking in a particulardomain?whether it references images or ideasthat are unexpected in its context?we employ asimple measure of word specificity with respect toa domain: the ratio of the word frequency withinthe domain and the word frequency in all domainsbeing considered.5It should be noted that speci-ficity is not purely a function of the word, but5We measure specificity for the VEHICLE, PROPERTYand EVENT.2011(a) VEHICLE specificity.
(b) TOPIC-VEHICLE similarity.
(c) Imageability of the PROPERTY.Figure 1: Distribution of some of the features we use, across literal and figurative comparisons in the testset.
The profile of the plot is a kernel density estimation of the distribution, and the markers indicate themedian and the first and third quartiles.of the word and the context in which it appears.A comparison in the music domain that involvesmelodies is not surprising:But the title song really feels like a pretty blandvocal melody [...]But the same word can play a very different rolein another context, for example, book reviews:Her books are like sweet melodies that flowthrough your head.Indeed, the word melody has a specificity of 96%in the music domain and only of 3% in the booksdomain.An analysis on the labeled data confirms thatliteral comparisons do indeed tend to have moredomain-specific VEHICLES (Mann-Whitney Utest, p < 0.01) than figurative ones.
Further-more, the distribution of specificity across bothtypes of comparisons, as shown in Figure 1a, hasthe appearance of a mixture model of general andspecific words.
Figurative comparison VEHICLESlargely exhibit only the general component of themixture.6Domain label An analysis of the annotation re-sults reveals that the percentage of comparisonsthat are figurative differs widely across domains,as indicated in the last column in Table 1.
Thissuggests that simply knowing the domain of atext can serve to adjust some prior expectationabout figurative language presence and thereforeimprove detection.
We test this hypothesis using6The mass around 0.25 in Figure 1a is largely explainedby generic words such as thing, others, nothing, average andbarely specific words like veil, reputation, dream, garbage.a Z-test comparing all Amazon categories.
Withthe exception of books and music reviews, thathave similar ratios, all other pairs of categoriesshow significantly different proportions of figura-tive comparisons (p < 0.01).4.2 Domain-agnostic cuesLinguistic studies of figurative language suggestthat there is a fundamental generic notion of fig-urativeness.
We attempt to capture this notion inthe context of comparisons using syntactic and se-mantic information.Topic-Vehicle similarity The default role of lit-eral comparisons is to assert similarity of things.Therefore, we expect that a high semantic simi-larity between the TOPIC and the VEHICLE of acomparison is a sign of literal usage, as we pre-viously hypothesized in preliminary work (Nicu-lae, 2013).
To test this hypothesis, we computeTOPIC-VEHICLE similarity using DistributionalMemory (Baroni and Lenci, 2010), a freely avail-able distributional semantics resource that cap-tures word relationships through grammatical roleco-occurrence.By applying this measure to our data, we findthat there is indeed an important difference be-tween the distributions of TOPIC-VEHICLE simi-larity in figurative and literal comparisons (shownin Figure 1b); the means of the two distribu-tions are significantly different (Mann-Whitneyp < 0.01).Metaphor-inspired features We also seek tounderstand to what extent insights provided bycomputational work on metaphor detection can be2012more concrete less concretemore imageable cinnamon, kiss devil, happinessless imageable casque, pugilist aspect, howeverTable 2: Examples of words with high and lowconcreteness and imageability scores from theMRC Psycholinguistic Database.applied in the context of comparisons.
To that endwe consider features shown to provide state of theart performance in the task of metaphor detection(Tsvetkov et al., 2014a): abstractness, imageabil-ity and supersenses.Abstractness and imageability features are de-rived from the MRC Psycholinguistic Database(Coltheart, 1981), a dictionary based on manuallyannotated datasets of psycholinguistic norms.
Im-ageability is the property of a word to arouse amental image, be it in the form of a mental pic-ture, sound or any other sense.
Concreteness isdefined as ?any word that refers to objects, materi-als or persons,?
while abstractness, at the other endof the spectrum, is represented by words that can-not be usually experienced by the senses (Paivioet al., 1968).
Table 2 shows a few examples ofwords with high and low concreteness and image-ability scores.
Supersenses are a very coarse formof meaning representation.
Tsvetkov et al.
(2014a)used WordNet (Miller, 1995) semantic classesfor nouns and verbs, for example noun.body,noun.animal, verb.consumption, or verb.motion.For adjectives, Tsvetkov et al.
(2014b) developedand made available a novel classification in thesame spirit.7We compute abstractness, image-ability and supersenses for the TOPIC, VEHICLE,EVENT, and PROPERTY.8We concatenate thesefeatures with the raw vector representations of theconstituents, following Tsvetkov et al.
(2014a).We find that such features relate to figurativecomparisons in a meaningful way.
For example,out of all comparisons with explicit properties, fig-urative comparisons tend to have properties that7Following Tsvetkov et al.
(2014a) we train a classifier topredict these features from a vector space representation of aword.
We use the same cross-lingually optimized represen-tation from Faruqui and Dyer (2014) and a simpler classifier,a logistic regression, which we find to perform as well as therandom forests used in Tsvetkov et al.
(2014a).
We treat su-persense prediction as a multi-label problem and apply a one-versus-all transformation, effectively learning a linear classi-fier for each supersense.8If the PROPERTY is implicit, all corresponding featuresare set to zero.
An extra binary feature indicates whether thePROPERTY is explicit or implicit.are more imageable (Mann-Whitney p < 0.01), asillustrated by Figure 1c.
This is in agreement withHanks (2005), who observed that similes are char-acterized by their appeal to sensory imagination.Definiteness We introduce another simple buteffective syntactic cue that relates to concreteness:the presence of a definite article versus an indefi-nite one (or none at all).
We search for the indefi-nite articles a and an and the definite article the ineach component of a comparison.We find that similes tend to have indefinite arti-cles in the VEHICLE more often and definite arti-cles less often (Mann-Whitney p < 0.01).
In par-ticular, 59% of comparisons where the VEHICLEhas a indefinite article are figurative, as opposedto 13% of the comparisons where VEHICLE has adefinite article.5 Prediction TaskWe now turn to the task of predicting whether acomparison is figurative or literal.
Not only doesthis task allow us to assess and compare the effi-ciency of the linguistic cues we discussed, but it isalso highly relevant in the context of natural lan-guage understanding systems.We conduct a logistic regression analysis, andcompare the efficiency of the features derivedfrom our analysis to a bag of words baseline.In addition to the features inspired by the pre-viously described linguistic insights, we also tryto computationally capture the lexical usage pat-terns of comparisons using a version of bag ofwords adapted to the comparison structure.
In thisslotted bag of words system, features correspondto occurrence of words within constituents (e.g.,bright ?
PROPERTY).We perform a stratified split of our compari-son dataset into equal train and test sets (each setcontaining 408 comparisons, out of which 134 arefigurative),9and use a 5-fold stratified cross vali-dation over the training set to choose the optimalvalue for the logistic regression regularization pa-rameter and the type of regularization (?1or ?2) foreach feature set.109The entire analysis described in Section 4 is only con-ducted on the training set.
Also, in order to ensure that we areassessing the performance of the classifier on unseen com-parisons, we discard from our dataset all those with the sameTOPIC and VEHICLE pair.10We use the logistic regression implementationof liblinear (Fan et al., 2008) wrapped by thescikit-learn library (Pedregosa et al., 2011).2013Model # features Acc.
P R F1AUCBag of words 1970 0.79 0.63 0.84 0.72 0.87Slotted bag of words 1840 0.80 0.64 0.90 0.75 0.89Domain-agnostic cues 357 0.81 0.70 0.74 0.72 0.90only metaphor inspired 345 0.75 0.60 0.72 0.65 0.84Domain-specific cues 8 0.69 0.51 0.81 0.63 0.76All linguistic insight cues 365 0.86 0.76 0.83 0.79 0.92Full 2202 0.88 0.80 0.84 0.82 0.94Human - 0.96 0.92 0.96 0.94 -Table 3: Classification performance on the test set for the different sets of features we considered; humanperformance is shown for reference.Classifier performance The performance onthe classification task is summarized in Table 3.We note that the bag of words baseline is remark-ably strong, because of common idiomatic simi-les that can be captured through keywords.
Ourfull system (which relies on our linguistically in-spired cues discussed in Section 4 in addition toslotted bag of words) significantly outperforms thebag of words baseline and the slotted bag of wordssystem in terms of accuracy, F1score and AUC(p < 0.05),11suggesting that linguistic insightscomplement idiomatic simile matching.
Impor-tantly, a system using only our linguistic insightcues also significantly improves over the baselinein terms of accuracy and AUC and it is not signif-icantly different from the full system in terms ofperformance, in spite of having about an order ofmagnitude fewer features.
It is also worth notingthat the domain-specific cues play an importantrole in bringing the performance to this level bycapturing a different aspect of what it means for acomparison to be figurative.The features used by the state of the artmetaphor detection system of Tsvetkov et al.
(2014a), adapted to the comparison structure, per-form poorly by themselves and do not improvesignificantly over the baseline.
This is consis-tent with the theoretical motivation that figura-tiveness in comparisons requires special compu-tational treatment, as discussed in Section 1.
Fur-thermore, the linguistic insight features not onlysignificantly outperform the metaphor inspiredfeatures (p < 0.05), but are also better at exploit-ing larger amounts of data, as shown in Figure 2.11All statistical significance results in this paragraph areobtained from 5000 bootstrap samples.Figure 2: Learning curves.
Each point is obtainedby fitting a model on 10 random subsets of thetraining set.
Error bars show 95% confidence in-tervals.Comparison to human performance To gaugehow well humans would perform at the classifica-tion task on the actual test data, we perform an-other Amazon Mechanical Turk evaluation on 140examples from the test set.
For the evaluation,we use majority voting between the three anno-tators,12and compare to the agreed labels in thedataset.
Estimated human accuracy is 96%, plac-ing our full systemwithin 10% of human accuracy.Feature analysis The predictive analysis weperform allows us to investigate to what extent thefeatures inspired by our linguistic insights havediscriminative power, and whether they actuallycover different aspects of figurativeness.12Majority voting helps account for the noise inherent tocrowdsourced annotation, which is less accurate than profes-sional annotation.
Taking the less optimistic invididual turkeranswers, human performance is on the same level as our fullsystem.2014Feature Coef.
Example where the feature is positively activatedTOPIC-VEHICLE similarity ?11.3 the older man was wiser and stronger than the boyVEHICLE specificity ?5.8 the cord is more durable than the adapter [Electronics]VEHICLE imageability 4.9 the explanations are as clear as mudVEHICLE communication supersense ?4.6 the book reads like six short articlesVEHICLE indefiniteness 4.0 his fame drew foreigners to him like a magnetlife ?
VEHICLE 7.1 the hero is truly larger than life: godlike, yet flawedpicture ?
VEHICLE ?6.0 the necklace looks just like the pictureother ?
VEHICLE ?5.9 this one is just as nice as the otherothers ?
VEHICLE ?5.5 some songs are more memorable than otherscrap ?
VEHICLE 4.7 the headphones sounded like crapTable 4: Top 5 linguistic insight features (top) and slotted bag of words features (bottom) in the full modeland their logistic regression coefficients.
A positive coefficient means the feature indicates figurativeness.Table 4 shows the best linguistic insight andslotted bag of words features selected by the fullmodel.
The strongest feature by far is the seman-tic similarity between the TOPIC and the VEHI-CLE.
By itself, this feature gets 70% accuracy and61% F1score.The rest of the top features involve mostly theVEHICLE.
This suggests that the VEHICLE is themost informative element of a comparison when itcomes to figurativeness.
Features involving otherconstituents also get selected, but with slightlylower weights, not making it to the top.VEHICLE specificity is one of the strongest fea-tures, with positive values indicating literal com-parisons.
This confirms our intuition that domaininformation is important to discriminate figurativefrom literal language.Of the adapted metaphor features, the nouncommunication supersense and the imageabilityof the VEHICLE make it to the top.
Nouns withlow communication rating occurring in the train-ing set include puddles, arrangements, carbohy-drates while nouns with high communication rat-ing include languages and subjects.Presence of an indefinite article in the VEHICLEis a strong indicator of figurativeness.
By them-selves, the definiteness and indefiniteness featuresperform quite well, attaining 78% accuracy and67% F1score.The salient bag of words features correspond tospecific types of comparisons.
The words otherand others in the VEHICLE indicate comparisonsbetween the same kind of arguments, for exam-ple some songs are more memorable than others,and these are likely to be literal.
The word pic-ture is specific to the review setting, as productsare accompanied by photos, and for certain kindsof products, the resemblance of the product withthe image is an important factor for potential buy-ers.13The bag of words systems are furthermoreable to learn idiomatic comparisons by identify-ing common figurative VEHICLES such as life andcrap, corresponding to fixed expressions such aslarger than life.Error analysis Many of the errors made by ourfull system involve indirect semantic mechanismssuch as metonymy.
For example, the false pos-itive the typeface was larger than most booksreally means larger than the typefaces found inmost books, but without the implicit expansion themeaning can appear figurative.
A similar kind ofellipsis makes the example a lot [of songs] areeven better than sugar be wrongly classified asliteral.
Another source of error is polysemy.
Ex-amples like the rejuvelac formula is about 10 timesbetter than yogurt are misclassified because of themultiple meanings of the word formula, one beingclosely related to yogurt and food, but the morecommon ones being general and abstract, suggest-ing figurativeness.6 Social CorrelatesThe advantage of studying comparisons situatedin a social context is that we can understand howtheir usage interacts with internal and external hu-man factors.
An internal factor is the sentiment of13This feature is highly correlated with the domain: it ap-pears 25 times in the training set, 24 of which in the jewelerydomain and once in book reviews.2015(a) Figurative comparisons are more likely to be found in re-views with strongly polarized sentiment.
(b) Helpful reviews are less likely to contain figurative compar-isons.Figure 3: Interaction between figurative language and social context aspects.
Error bars show 95%confidence intervals.
The dashed horizontal line marks the average proportion of figurative comparisons.In Figure 3b the average proportion is different because we only consider reviews rated by at least 10readers.the user towards the reviewed product, indicatedby the star rating of the review.
An external factorpresent in the data is how helpful the review is per-ceived by other users.
In this section we analyzehow these factors interact with figurative languagein comparisons.To gain insight about fine grained interactionswith human factors at larger scale, we use our clas-sifier to find over 80,000 figurative and literal com-parisons from the same four categories.
The trendswe reveal also hold significantly on the manuallyannotated data.Sentiment While it was previously noted thatsimiles often transmit strong affect (Hanks, 2005;Veale, 2012a; Veale, 2012b), the connection be-tween figurativeness and sentiment was never em-pirically validated.
The setting of product reviewsis convenient for investigating this issue, sincethe star ratings associated with the reviews canbe used as sentiment labels.
We find that com-parisons are indeed significantly more likely tobe figurative when the users express strong opin-ions, i.e., in one-star or five-star reviews (Mann-Whitney p < 0.02 on the manually annotateddata).
Figure 3a shows how the proportion of fig-urative comparisons varies with the polarity of thereview.Helpfulness It is also interesting to understandto what extent figurative language relates to theexternal perception of the content in which it ap-pears.
We find that comparisons in helpful re-views14are less likely to be figurative.
Figure 3bshows a near-constant high ratio of figurative com-parisons among unhelpful and average reviews; ashelpfulness increases, figurative comparisons be-come less frequent.
We further validate that thiseffect is not a confound of the distribution of help-fulness ratings across reviews of different polarityby controlling for the star rating: given a fixed starrating, the proportion of figurative comparisons isstill lower in helpful (helpfulness over 50%) thanin unhelpful (helpfulness under 50%) reviews; thisdifference is significant (Mann-Whitney p < 0.01)for all classes of ratings except one-star.
Thesize of the manually annotated data does not al-low for star rating stratification, but the overall dif-ference is statistically significant (Mann-Whitneyp < 0.01).
This result encourages further exper-imentation to determine whether there is a causallink between the use of figurative language in usergenerated content and its external perception.7 Conclusions and Future WorkThis work proposes a computational study of fig-urative language in comparisons.
Starting froma new dataset of naturally occurring comparisonswith figurativeness annotation (which we makepublicly available) we explore linguistic patternsthat are indicative of similes.
We show that these14In order to have reliable helpfulness scores, we only con-sider reviews that have been rated by at least by ten readers.2016insights can be successfully operationalized in anew prediction task: distinguishing literal fromfigurative comparisons.
Our system reaches ac-curacy that is within 10% of human performance,and is outperforming a state of the art metaphordetection system, thus confirming the need fora computational approach tailored specifically tocomparisons.
While we take a data-driven ap-proach, our annotated dataset can be useful formore theoretical studies of the kinds of compar-isons and similes people use.We discover that domain knowledge is an im-portant factor in identifying similes.
This suggeststhat future work on automatic detection of figura-tive language should consider contextual parame-ters such as the topic and community where thecontent appears.Furthermore, we are the first to tie figurativelanguage to the social context in which it is pro-duced and show its relation to internal and exter-nal human factors such as opinion sentiment andhelpfulness.
Future investigation into the causaleffects of these interactions could lead to a betterunderstanding of the role of figurative language inpersuasion and rhetorics.In our work, we consider common noun TOP-ICS and VEHICLES and adjectival PROPERTIES.This is the most typical case, but supporting otherparts of speech?such as proper nouns, pronouns,and adverbs?can make a difference in many ap-plications.
Capturing compositional interactionbetween the parts of the comparison could lead tomore flexible models that give less weight to theVEHICLE.This study is also the first to estimate howprevalent similes are in the wild, and reports thatabout one third of the comparisons we consider arefigurative.
This is suggestive of the need to buildsystems that can properly process figurative com-parisons in order to correctly harness the semanticinformation encapsulated in comparisons.AcknowledgementsWe would like to thank Yulia Tsvetkov for con-structive discussion about figurative language andabout her and her co-authors?
work.
We are grate-ful for the suggestions of Patrick Hanks, Con-stantin Or?asan, Sylviane Cardey, Izabella Thomas,Ekaterina Shutova, Tony Veale, and Niket Tan-don.
We extend our gratitude to Julian McAuleyfor preparing and sharing the Amazon reviewdataset.
We are thankful to the anonymous review-ers, whose comments were like a breath of freshair.
We acknowledge the help of the Amazon Me-chanical Turk annotators and of the MPI-SWS stu-dents involved in pilot experiments.Vlad Niculae was supported in part by theEuropean Commission, Education & Training,Erasmus Mundus: EMMC 2008-0083, ErasmusMundus Masters in NLP & HLT.ReferencesMarco Baroni and Alessandro Lenci.
2010.
Dis-tributional memory: A general framework forcorpus-based semantics.
Computational Linguis-tics, 36(4):673?721.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky wide web:A collection of very large linguistically processedweb-crawled corpora.
Language Resources andEvaluation, 43(3):209?226.Louise Shabat Bethlehem.
1996.
Simile and figurativelanguage.
Poetics Today, 17(2):203?240.Julia Birke and Anoop Sarkar.
2006.
A clustering ap-proach for nearly unsupervised recognition of non-literal language.
In Proceedings of EACL.Hugh Bredin.
1998.
Comparisons and similes.
Lin-gua, 105(1):67?78.Max Coltheart.
1981.
The MRC psycholinguisticdatabase.
The Quarterly Journal of ExperimentalPsychology, 33(4):497?505.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilingualcorrelation.
In Proceedings of EACL.Eric N Forsyth and Craig H Martell.
2007.
Lexical anddiscourse analysis of online chat dialog.
In Proceed-ings of ICSC.Patrick Hanks.
2005.
Similes and sets: The Englishpreposition ?like?.
In R. Blatn?a and V. Petkevic, ed-itors, Languages and Linguistics: Festschrift for Fr.Cermak.
Charles University, Prague.Patrick Hanks.
2006.
Metaphoricity is grad-able.
Trends in Linguistic Studies and Monographs,171:17.Patrick Hanks.
2012.
The roles and structure of com-parisons, similes, and metaphors in natural language(an analogical system).
Presented at the StockholmMetaphor Festival.2017Patrick Hanks.
2013.
Lexical Analysis: Norms andExploitations.
MIT Press.Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-ney Sanders, and Eduard Hovy.
2013.
Identifyingmetaphorical word use with tree kernels.
In Pro-ceedings of the NAACL Workshop on Metaphors forNLP.M.
Israel, J.R. Harding, and V. Tobin.
2004.
On simile.Language, Culture, and Mind.
CSLI Publications.Nitin Jindal and Bing Liu.
2006.
Identifying compara-tive sentences in text documents.
In Proceedings ofSIGIR.George Lakoff and Mark Johnson.
1980.
Metaphorswe live by.
University of Chicago Press.Bin Li, Jiajun Chen, and Yingjie Zhang.
2012.
Webbased collection and comparison of cognitive prop-erties in English and Chinese.
In Proceedings of theJoint Workshop on Automatic Knowledge Base Con-struction and Web-scale Knowledge Extraction.Christoph Lofi, Christian Nieke, and Nigel Collier.2014.
Discriminating rhetorical analogies in socialmedia.
In Proceedings of EACL.Andr?e FT Martins, Noah A Smith, Eric P Xing, Pe-dro MQ Aguiar, and M?ario AT Figueiredo.
2010.Turbo parsers: Dependency parsing by approximatevariational inference.
In Proceedings of EMNLP.Julian McAuley and Jure Leskovec.
2013.
Hidden fac-tors and hidden topics: Understanding rating dimen-sions with review text.
In Proceedings of RecSys.George A Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Rosamund Moon.
2008.
Conventionalized as-similesin English: A problem case.
International Journalof Corpus Linguistics, 13(1):3?37.Rosamund Moon.
2011.
Simile and dissimilarity.Journal of Literary Semantics, 40(2):133?157.Robert Munro, Steven Bethard, Victor Kuperman,Vicky Tzuyin Lai, Robin Melnick, ChristopherPotts, Tyler Schnoebelen, and Harry Tily.
2010.Crowdsourcing and language studies: The new gen-eration of linguistic data.
In Proceedings of theNAACL Workshop on Creating Speech and Lan-guage Data with Amazon?s Mechanical Turk.Vlad Niculae.
2013.
Comparison pattern matching andcreative simile recognition.
In Proceedings of theJoint Symposium on Semantic Processing.Olutobi Owoputi, Brendan O?Connor, Chris Dyer,Kevin Gimpel, Nathan Schneider, and Noah ASmith.
2013.
Improved part-of-speech tagging foronline conversational text with word clusters.
InProceedings of NAACL-HLT.Allan Paivio, John C Yuille, and Stephen A Madigan.1968.
Concreteness, imagery, and meaningfulnessvalues for 925 nouns.
Journal of Experimental Psy-chology, 76(1p2):1.Fabian Pedregosa, Ga?el Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, OlivierGrisel, Mathieu Blondel, Peter Prettenhofer, RonWeiss, Vincent Dubourg, et al.
2011.
Scikit-learn:Machine learning in Python.
Journal of MachineLearning Research, 12:2825?2830.Carlos Roncero, John M Kennedy, and Ron Smyth.2006.
Similes on the internet have explanations.Psychonomic Bulletin & Review, 13(1):74?77.Ekaterina Shutova and Lin Sun.
2013.
Unsupervisedmetaphor identification using hierarchical graph fac-torization clustering.
In Proceedings of NAACL-HLT.Ekaterina Shutova.
2010.
Models of metaphor in NLP.In Proceedings of ACL.Niket Tandon, Gerard de Melo, and Gerhard Weikum.2014.
Smarter than you think: Acquiring compara-tive commonsense knowledge from the web.
In Pro-ceedings of AAAI.Yulia Tsvetkov, Leonid Boytsov, Anatole Gershman,Eric Nyberg, and Chris Dyer.
2014a.
Metaphor de-tection with cross-lingual model transfer.
In Pro-ceedings of ACL.Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, ArchnaBhatia, Manaal Faruqui, and Chris Dyer.
2014b.Augmenting English adjective senses with super-senses.
In Proceedings of LREC.Peter D Turney, Yair Neuman, Dan Assaf, and YohaiCohen.
2011.
Literal and metaphorical sense iden-tification through concrete and abstract context.
InProceedings of EMNLP.Tony Veale and Yanfen Hao.
2008.
A context-sensitiveframework for lexical ontologies.
Knowledge Engi-neering Review, 23(1):101?115.Tony Veale and Yanfen Hao.
2011.
Exploiting ready-mades in linguistic creativity: A system demonstra-tion of the Jigsaw Bard.
In Proceedings of ACL (Sys-tem Demonstrations).Tony Veale and Guofu Li.
2013.
Creating similarity:Lateral thinking for vertical similarity judgments.
InProceedings of ACL.Tony Veale.
2012a.
A computational exploration ofcreative similes.
Metaphor in Use: Context, Cul-ture, and Communication, 38:329.Tony Veale.
2012b.
A context-sensitive, multi-facetedmodel of lexico-conceptual affect.
In Proceedingsof ACL.2018
