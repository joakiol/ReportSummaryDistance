Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 810?820, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsImproving NLP through Marginalization of Hidden Syntactic StructureJason Naradowsky, Sebastian Riedel, and David A. SmithDepartment of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA, 01003, U.S.A.{narad, riedel, dasmith}@cs.umass.eduAbstractMany NLP tasks make predictions that are in-herently coupled to syntactic relations, but formany languages the resources required to pro-vide such syntactic annotations are unavail-able.
For others it is unclear exactly howmuch of the syntactic annotations can be ef-fectively leveraged with current models, andwhat structures in the syntactic trees are mostrelevant to the current task.We propose a novel method which avoidsthe need for any syntactically annotated datawhen predicting a related NLP task.
Ourmethod couples latent syntactic representa-tions, constrained to form valid dependencygraphs or constituency parses, with the predic-tion task via specialized factors in a Markovrandom field.
At both training and test time wemarginalize over this hidden structure, learn-ing the optimal latent representations for theproblem.
Results show that this approach pro-vides significant gains over a syntactically un-informed baseline, outperforming models thatobserve syntax on an English relation extrac-tion task, and performing comparably to themin semantic role labeling.1 IntroductionMany NLP tasks are inherently tied to syntax, andstate-of-the-art solutions to these tasks often rely onsyntactic annotations as either a source for usefulfeatures (Zhang et al2006, path features in relationextraction) or as a scaffolding upon which a morenarrow, specialized classification can occur (as of-ten done in semantic role labeling).
This decou-pling of the end task from its intermediate repre-sentation is sometimes known as the two-stage ap-proach (Chang et al2010) and comes with severaldrawbacks.
Most notably this decomposition pro-hibits the learning method from utilizing the labelsfrom the end task when predicting the intermediaterepresentation, a structure which must have somecorrelation to the end task to provide any benefit.Relying on intermediate representations that arespecifically syntactic in nature introduces its ownunique set of problems.
Large amounts of syntac-tically annotated data is difficult to obtain, costlyto produce, and often tied to a particular domainthat may vary greatly from that of the desired endtask.
Additionally, current systems often utilize onlya small amount of the annotation for any particulartask.
For instance, performing named entity recogni-tion (NER) jointly with constituent parsing has beenshown to improve performance on both tasks, butthe only aspect of the syntax which is leveraged bythe NER component is the location of noun phrases(Finkel and Manning, 2009).
By instead discover-ing a latent representation jointly with the end taskwe address all of these concerns, alleviating the needfor any syntactic annotations, while simultaneouslyattempting to learn a latent syntax relevant to boththe particular domain and structure of the end task.We phrase the joint model as factor graph andmarginalize over the hidden structure of the inter-mediate representation at both training and test time,to optimize performance on the end task.
Infer-ence is done via loopy belief propagation, makingthis framework trivially extensible to most graphstructures.
Computation over latent syntactic rep-810resentations is made tractable with the use of specialcombinatorial factors which implement unlabeledvariants of common dynamic-programming parsingalgorithms, constraining the hidden representationto realize valid dependency graphs or constituencytrees.We apply this strategy to two common NLP tasks,coupling a model for the end task prediction withlatent and general syntactic representations via spe-cialized logical factors which learn associations be-tween latent and observed structure.
In comparisonswith identical models which observe ?gold?
syntac-tic annotations, derived from off-the-shelf parsers orprovided with the corpora, we find that our hiddenmarginalization method is comparable in both tasksand almost every language tested, sometimes signifi-cantly outperforming models which observe the truesyntax.The following sections serves as a preliminary,introducing an inventory of factors and variablesfor constructing factor graph representations ofsyntactically-coupled NLP tasks.
Section 3 exploresthe benefits of this method on relation extraction(RE), where we compare the use dependency andconstituency structure as latent representations.
Wethen turn to a more established semantic role label-ing (SRL) task (?4) where we evaluate across a widerange of languages.2 Latent Pseudo-Syntactic StructureThe models presented in this paper are phrased interms of variables in an undirected graphical model,Markov random field.
More specifically, we imple-ment the model as a factor graph, a bipartite graphcomposed of factors and variables in which we canefficiently compute the marginal beliefs of any vari-able set with the sum-product algorithm for cyclicgraphs, loopy belief propagation,.
We now intro-duce the basic variable and factor components usedthroughout the paper.2.1 Latent Dependency StructureDependency grammar is a lexically-oriented syn-tactic formalism in which syntactic relationshipsare expressed as dependencies between individualwords.
Each non-root word specifies another asits head, provided that the resulting structure formsa valid directed graph, ie.
there are no cycles inthe graph.
Due to the flexibility of this representa-tion it is often used to describe free-word-order lan-guages, and increasingly preferred in NLP for morelanguage-in-use scenarios.
A dependency graph canbe modeled with the following nodes, as first pro-posed by Smith and Eisner (2008):?
Let {Link(i, j) : 0 ?
i ?
j ?
n, n 6= j}be O(n2) boolean variables corresponding tothe possible links in a dependency parse.
Li,j= true implies that there is a dependency fromparent i to child j.?
Let {LINK(i, j) : 0 ?
i ?
j ?
n, n 6= j}be O(n2) unary factors, each paired with a cor-responding Link(i, j) variable and expressingthe independent belief that Link(i, j) = true.2.2 Latent Constituency StructureAlternatively we can describe the more structuredconstituency formalism by setting up a representa-tion over span variables:?
Let {Span(i, j) : 0 ?
i < j ?
n} be O(n2)boolean variables such that Span(i, j) = trueiff there is a bracket spanning i to j 1.?
Let {SPAN(i, j) : 0 ?
i < j ?
n} be O(n2)unary factors, each attached to the correspond-ing Span(i, j) variable.
These factors score theindependent suitability of each span to appearin an unlabeled constituency tree.All boolean variables presented in this paper willbe paired to unary factors in this manner, whichwe will omit in future descriptions.
This encom-passes the necessary representational structure forboth syntactic formalisms, but nothing introducedup to this point guarantees that either of these rep-resentations will form a valid tree or DAG.2.3 Combinatorial FactorsNaively constraining these latent representationsthrough the introduction of many interconnectedternary factors is possible, but would likely be com-putationally intractable.
However, as observed in1In practice, we do not need to include variables for spansof width 1 or n, since they will always be true.811Smith and Eisner (2008), we can encapsulatingcommon dynamic programming algorithms withinspecial-purpose factors to efficiently globally con-strain variable configurations .
Since the outgoingmessages from such factors to a variable can be com-puted from the factor?s posterior beliefs about thatvariable, there is no difficulty in exchanging beliefsbetween these special-purpose factors and the restof the graph, and inference can proceed using thestandard sum-product or max-product belief prop-agation.
Here we present two combinatorial factorsthat provide efficient ways of constraining the modelto fit common syntactic frameworks.?
Let CKYTREE be a global combinatorial fac-tor, as used in previous work in efficient pars-ing (Naradowsky and Smith, 2012), attached toall the Span(i, j) variables.
This factor con-tributes a factor of 1 to the model?s score iff thespan variables collectively form a legal, binarybracketing and a factor of 0 otherwise.
It en-forces, therefore, a hard constraint on the vari-ables, computing beliefs via an unlabeled vari-ant of the inside-outside algorithm.?
Let DEP-TREE be a global combinatorial fac-tor, as presented in Smith and Eisner (2008),which attaches to all Link(i, j) variables andsimilarly contributes a factor of 1 iff the config-uration of Link variables forms a valid projec-tive dependency graph.
A graph is projective ifits edges do not cross.2.4 Marginal MAP InferenceIt is straightforward to train these latent variablemodels to maximize the marginal probability of theiroutputs, conditioning on their inputs, and marginal-izing out the latent syntactic variables.
To computefeature expectations, we can use marginal inferencetechniques such as sampling and sum-product beliefpropagation to compute marginal probabilities.A knottier problem arises when we want to findthe best assignment to the variables of interestwhile marginalizing out ?nuisance?
latent variables.This is the problem of marginal MAP inference?sometimes known as consensus decoding?whichhas been shown to be NP-hard and without a poly-nomial time approximation scheme (Sima?an, 1996;Casacuberta and Higuera, 2000).
In the NLP com-munity, these inference problems often arise whendealing with spurious ambiguity where multiplederivations can lead to the same derived structure.
Intree substitution grammars, for instance, there maybe many ways of combining elementary trees to pro-duce the same output tree; in machine translation,many different elementary phrases or elementarytree pairs might produce the same output string.
Forsyntactic parsing, Goodman (1996) proposed a vari-ational method for summing out spurious ambiguitythat was equivalent to minimum Bayes risk decoding(Goel and Byrne, 2000; Kumar and Byrne, 2004)with a constituent-recall loss function.
For MT,May and Knight (2006) proposed methods for de-terminizing tree automata to reduce ambiguity, andLi et al2009) proposed a variational method basedon n-gram loss functions.
More recently, Liu and Ih-ler (2011) analyzed message-passing algorithms formarginal MAP.In this paper, we adopt a simple minimum Bayesrisk decoding scheme.
First, we perform sum-product belief propagation on the full factor graph.Then, we maximize the expected accuracy of thevariables of interest, subject to any hard constraintson them (such as mutual exclusion among labels).
Insome cases with complex combinatorial constraints,this simple MBR scheme has proved more effec-tive than exact decoding over all variables (Auli andLopez, 2011).3 Relation ExtractionPerforming a syntax-based NLP task in most real-world scenarios requires that the incoming data firstbe parsed using a pre-trained parsing model.
Forsome tasks, like relation extraction, many data setslack syntactic annotation and these circumstancespersist even into the training phase.
In this sec-tion we explore such scenarios and contrast the useof parser-provided syntactic annotation to marginal-izing over latent representations of constituency ordependency syntax.
We show the hidden syntacticmodels are not just competitive with these ?oracle?models, but in some configurations can actually out-perform them.Relation extraction is the task of identifying se-mantic relations between sets of entities in text (as812illustrated in Fig.
1b), and a good proving groundfor latent syntactic methods for two reasons.
First,because entities share a semantic relationship, un-der most linguistic analyses these entities will alsoshare some syntactic relation.
Indeed, syntactic fea-tures have long been an extremely useful sourceof information for relation extraction systems (Cu-lotta and Sorensen, 2004; Mintz et al2009).
Sec-ondly, relation extraction has been a common taskfor pioneering efforts in processing data mined fromthe internet, and otherwise noisy or out-of-domaindata.
In particular, large noisily-annotated data setshave been generated by leveraging freely availableknowledge bases such as Freebase (Bollacker et al2008; Mintz et al2009).
Such data sets have beenutilized successfully for relation extraction from theweb (Bunescu and Mooney, 2007).3.1 ModelWe present a simple model for representing rela-tional structure, with the only variables present be-ing a set of boolean-valued variables representing anundirected dependency between two entities, and anadditional set of boolean label variables representingthe type label of the relation.?
Let {Rel(i, j : 0 ?
i < j ?
n} be O(n2)boolean variables such that Rel(i, j) = true iffthere is a relation spanning i to j.?
Let {Rel-Label(i, j, ?)
: ?
?
L, and 0 ?
i <j ?
n} be O(|L|n2) boolean variables suchthat Rel-Label(i, j, ?)
= true iff there is a re-lation spanning i to j with relation type ?.?
Let {ATMOST1(i, j) : 0 ?
i < j ?
n} beO(n2) factors, each coordinating the set L ofpossible nonterminal variables to the Rel vari-able at each i, j tuple, allowing a Rel-Labelvariable to be true iff all other label variablesare false and Rel(i, j) = true.Here the Rel(i, j) and Rel-Label(i, j) variablessimply express the representation of the problem,while the ATMOST1 factors are logical constraintsensuring that only one label will apply to a particu-lar relation.3.2 Coordination FactorsAn important contribution of this work is the intro-duction of a flexible, general framework for connect-ing the latent and observable partitions of the model.We accomplish this through the use of two addi-tional factors, each expressing the same basic logic,which learn when to coordinate and when to ignorecorrelations between the latent syntax and the endtask.
While here we specify binary and ternary ver-sions of these factors, they also generalize to higherdimensions.?
Let {D-CONNECT(i, j, k) : 0 ?
i < j ?n; 0 ?
k ?
n} be O(n3) factors coordinatingany number of dependency syntax Link(i, j)variables with representational variables on theend task, multiplying in 1 to the model scoreunless all variables are on, in which case it mul-tiplies a connective potential ?
derived fromits features.
Thus it functions logically as asoft NAND factor.
In this ternary formulation krepresents a hidden dependency head or pivotwhich is shared between two syntactic depen-dencies anchored at the indices of the entitiesin the relation (as illustrated in Fig.
1).?
Let {C-CONNECT(i, j) : 0 ?
i < j ?n} be O(n2) factors coordinating syntacticSpan(i, j) and relation arc Rel(i, j), identi-cally to D-CONNECT but with a 1-to-1 map-ping.
Intuitively the joint model might learn?
> 1, i.e., constituency spans and task predic-tion relations are more likely to be coterminous.The difficulty in working with latent dependencysyntax is that we posit that the RE variables do notshare a 1-to-1 mapping with variables in the hid-den representation.
We expect instead, accordingto linguistic intuition, that a relation between enti-ties at position i and j in the sentence should havecorresponding syntactic dependencies but that theyare likely to realize this by sharing the same headword (as depicted in Fig.1), a word whose identityshould help label the relation.
Therefore we intro-duce a special coordination factor, D-CONNECT asa ternary factor to capture the relationship betweenpairs of latent syntactic variables and a single rela-tion variable, pivoting on the same unknown headword.813Figure 1: Latent Dependency coupling for the RE task.The D-CONNECT factor expresses ternary connection re-lations because the shared head word of the proposed re-lation is unknown.
As is convention, variables are repre-sented by circles, factors by rectangles.We introduce six model scenarios.?
Baseline, simply the arc-factored model con-sisting only of Rel and corresponding Labelvariables for each entity.
Features on the re-lation factors, which are common to all modelconfigurations, are combinations of lexical in-formation (i.e., the words that form the entity,the pos-tags of the entities, etc.)
as well as thedistance between the relation.
This is a light-weight model and generally does not attemptto exhaustively leverage all possible provensources of useful features (Zhou et al2005)towards a higher absolute score, but rather toserve as a point of comparison to the modelswhich rely on syntactic information.?
Baseline-Ent, a variant of Baseline with addi-tional features which include combinations ofmention type, entity type, and entity sub-type.?
Oracle D-Parse, in which we also instantiate afull set of latent dependency syntax variables,and connect them to the baseline model us-ing D-CONNECT factors.
Syntax variables areclamped to their true values.?
Oracle C-Parse, the constituency syntax ana-logue of Oracle D-Parse.?
Hidden D-Parse, which is an extension of Or-acle D-Parse in which we connect all syntaxvariables to a DEP-TREE factor, syntax vari-ables are unobserved, and are learned jointlywith the end task.
The features for latent syntaxare a subset of those used in dependency pars-ing (McDonald et al2005).?
Hidden C-Parse, the constituency syntax ana-logue of Hidden D-Parse.
The feature set issimilar but bigrams are taken over the wordsdefining the constituent span, rather than thewords defining the head/modifier relation.Coordination factor features for the syntactically-informed models are particularly important.
Thisbecame evident in initial experiments where thebaseline was often able to outperform the hiddensyntactic model.
However, inclusion of entity andmention label features into the connection factorsprovides the model with greater reasoning overwhen to coordinate or ignore the relation predictionswith the underlying syntax.
These are a proper sub-set of the Baseline-Ent features.3.3 DataWe evaluate these models using the 2005 Auto-matic Content Extraction (ACE) data set (Walker,2006), using the English (dual-annotated) and Chi-nese (solely annotator #1 data set) sections.
Eachcorpus is annotated with entity mentions?tagged asPER, ORG, LOC, or MISC?and, where applica-ble, what type of relation exists between them (e.g.,coarse: PHYS; fine: Located).
But like most cor-pora available for the task, the burden of acquiringcorresponding syntactic annotation is left to the re-searcher.
In this situation it is common to turn toexisting pre-trained parsing models.We generate our data by first splitting the rawtext paragraphs into sentences.
Chinese sentences814ACE ResultsEnglish ChineseUnlabeled Labeled Unlabeled LabeledModel P R F1 P R F1 P R F1 P R F1Baseline 85.4 57.0 68.4 83.0 55.3 66.4 42.9 26.8 33.0 42.6 21.3 28.4Baseline-Ent 87.2 65.4 74.8 85.8 64.4 73.6 55.2 31.1 39.8 51.2 29.4 37.4Oracle D-Parse 89.3 67.4 76.8 89.3 66.2 75.4 60.0 32.6 42.2 58.1 31.3 40.7Hidden D-Parse 87.8 69.8 77.7 85.3 67.8 75.6 48.0 32.0 38.4 47.2 30.0 36.7Oracle C-Parse 89.1 68.7 77.6 87.5 67.5 76.2 66.8 37.8 48.3 63.8 37.0 46.8Hidden C-Parse 90.5 69.9 78.9 88.8 68.6 77.4 56.3 32.3 41.0 53.4 31.6 39.7Table 1: Relation Extraction Results.
Models using hidden constituency syntax provide significant gains over thesyntactically-uniformed baseline model in both languages, but the advantages of the latent syntax were mitigated onthe smaller Chinese data set.are also tokenized according to Penn Chinese Tree-bank standards (Xue et al2005).
The sentences arethen tagged and parsed using the Stanford CoreNLPtools, using the standard pre-trained models for tag-ging (Toutanvoa and Manning, 2000), and the fac-tored parsing model of Klein and Manning (2002).The distributed grammar is trained on a variety ofsources, including the standard Wallstreet Journalcorpus, but also biomedical, translation, and ques-tions.
We then apply entity and relation annota-tions noisily to the data, collapsing multi-word en-tities into one term.
We filter out sentences withfewer than two entities (and are thus incapable ofcontaining relations) and sentences with more than40 words (to keep the parses more reliable).
Thisyields 6966 sentences for English data, but unfortu-nately only 747 sentences for the Chinese.
Nine ofevery ten sentences comprise the training set, withevery tenth sentence reserved for test.3.4 ResultsWe train all models using 20 iterations of stochasticgradient descent, each with a maximum of 10 BP it-erations (though in practice we find convergence tooften occur much earlier).
The results are presentedin Table 1, showing precision, recall, and F-measurefor both labeled and unlabeled prediction.
For En-glish, not only is the hidden marginalization methoda suitable replacement for the syntactic trees pro-vided by pre-trained, state-of-the-art models, but inboth configurations we find that inducing an optimalhidden structure is preferable to the parser-producedannotations.
On Chinese, where the data set is atyp-ically small, we still observe improved performanceover the baseline in the constituency-based modelthough it is not able to match the observed syntaxmodel.Despite the intuition that both entities occupyroles as modifiers of the same verb, we find thatthe Hidden D-Parse model often fails to recover thecorrect latent structure, and that even when success-ful dependency parses are observed, the head wordis often not uniquely indicative of the relation type(as known is not strongly correlated with the relationtype EMPLOYS in the phrase: Shigeru Miyamoto,best known for his work at the video game companyNintendo).
Hence when it comes to relation extrac-tion, at least on our relatively small data sets, we findthe simplest approach to latent syntactic structure isthe best.We now turn to the task of semantic role label-ing to evaluate this method on a more establishedhand-annotated data set, and a more varied set oflanguages.4 Semantic Role LabelingThe task of semantic role labeling (SRL) aims todetect and label the semantic relationships betweenparticular words, most commonly verbs (referred toin the domain as predicates), and their arguments(Meza-Ruiz and Riedel, 2009).In a manner similar to RE, there is a strong corre-lation between the presence of an SRL relation andthere existing an underlying syntactic dependency,though this is not always expressed as directly as a1-to-1 correspondence.
This has historically moti-vated a reliance on syntactic annotation, and someof the most successful methods have simply applied815Pred5At Most 1sense.01sense.02sense.|S|.d.)
Sense PredictionArg5, 1Arg5, 2roleA0roleA1.
.
.c.)
Argument Predictionb.)
Syntactic LayerLink5, 1D-Connect5, 1a.)
Syntactic Combinatorial ConstraintDEP-TreeLink5, 2D-Connect5, 2Link5, 3D-Connect5, 3At Most 1Arg5, 3Link5, nroleA2.
.
.D-Connect5, nroleA-TM..Figure 2: A tiered graphic representing the three different SRL model configurations.
The baseline system is describedin the bottom (c & d), the separate panels highlighting the independent predictions of this model: sense labels areassigned in an entirely separate process from argument prediction.
Pruning in the model takes place primarily inthis tier, since we observe true predicates we only instantiate over these indices.
The middle tier (b.)
illustrates thesyntactic representation layer, and the connective factors between syntax and SRL.
In the observed syntax modelthe Link variables are clamped to their correct values, with no need for a factor to coordinate them to form a validtree.
Finally, the hidden model comprises all layers, including a combinatorial syntactic constraint (a.)
over syntacticvariables.
In this scenario all labels in (b.)
are hidden at both training and test time.feature-rich classifiers to the parsed trees.
Relatedwork has recognized the large annotation burden thetask demands, but aimed to keep the syntactic anno-tations and induce semantic roles (Lang and Lapata,2010).
In this section we will take the opposite ap-proach, disregarding the syntactic annotations whichwe argue are more costly to acquire, as they requiremore formal linguistic training to produce.4.1 ModelWe present a simple, flexible model for SRL inwhich sense predictions are made independently ofthe rest of the model, and argument predictions aremade independently of each other.
The model struc-ture is composed as depicted in Fig.
2.?
Let {Arg(i, j) : 0 ?
i < j ?
n} be O(n2)boolean variables such that Arg(i, j) = trueiff predicate i takes token j as an argument.?
Let {Role(i, j, ?)
: ?
?
L, and 0 ?
i <j ?
n} be O(|L|n2) boolean variables suchthat Role(i, j, ?)
= true iff Arg(i, j) is trueand takes the role label ?.?
Let {Sense(i, ?)
: ?
?
S, and 0 ?
i ?n} be O(|S|n) boolean variables such thatSense(i, ?)
= true iff predicate i has sense?.4.1.1 FeaturesAt the coarsest level both the SRL and RE modelsare specifying binary predictions between a pair ofindices in the sentence, and a set of labels for eachdependency that happens to be true.
Similarly weuse almost identical features in SRL as we did in816Figure 3: Examining the learned hidden representation for SRL.
In this example the syntactic dependency arcs derivedfrom gold standard syntactic annotations (left) are entirely disjoint from the correct predicate/arguments pairs (shownin the heatmaps by the squares outlined in black), and the observed syntax model fails to recover any of the correctpredictions.
In contrast, the hidden model structure (right) learns a representation that closely parallels the desired endtask predictions, helping it recover three of the four correct SRL predictions (shaded arcs: red corresponds to a correctprediction, with true labels GA, KARA, etc.
), and providing some evidence towards the fourth.
The dependency treecorresponding to the hidden structure is derived by edge-factored decoding: dependency variables whose beliefs> 0.5are classified as true (though some arcs not relevant to the SRL predictions are omitted for clarity).RE, with the sole exception that we incorporate theobservable lemma and morphological features intobigrams on predicate/argument pairs.
For sense pre-diction we rely only on unigram features taken in aclose (w = 2) window of the target predicate.For the coordinating factors we use subsets ofcombinations of word, part-of-speech, and capital-ization features taken between head and argument,and concatenate these with the distance and direc-tion between the predicate and argument.
We do notfind the performance of the system to be as sensi-tive to which features are present in the coordinatingfactors as we did in the RE task.4.2 DataWe evaluate our SRL model using the data set devel-oped for the CoNLL 2009 shared task competition(Hajic?
et al2009), which features seven languagesand provides an ideal opportunity to measure theability of the hidden structure to generalize acrosslanguages of disparate origin and varied character-istics.
It also provides the opportunity to observea variety of different annotation styles and biases,some of which our model was able to uncover as ill-suited to common models for the task.
The data it-self provides word, lemma, part-of-speech, and mor-phological feature information, along with gold de-pendency parses.
Words which denote predicates areidentified, and their (train time) arguments are pro-vided.
They are also annotated with a sense labelfor each predicate, which is scored as an additionalSRL dependency.
Thus the task involves predictingfor each predicate a set of argument dependenciesand the sense label associated with that predicate.817Unlabeled Labeled CoNLL 2009 F1Data Model P R F1 P R F1 MAX.
MEAN MED.CatalanBaseline 92.20 62.43 74.48 73.80 58.76 65.43Oracle Syn.
98.48 96.17 97.31 70.42 68.78 69.59 80.3 71.0 74.1Hidden Syn.
95.21 92.84 94.01 68.86 67.15 67.99ChineseBaseline 72.48 64.82 68.44 65.97 59.00 62.29Oracle Syn.
98.57 78.98 87.69 87.64 70.22 77.97 78.6 72.2 70.4Hidden Syn.
90.79 79.09 84.53 81.97 71.40 76.32CzechBaseline 97.73 56.50 71.61 84.80 48.80 61.84Oracle Syn.
98.62 81.25 89.09 92.94 68.25 74.84 85.4 72.4 71.7Hidden Syn.
92.39 89.35 90.85 74.41 71.96 73.16EnglishBaseline 92.46 71.56 80.68 84.56 65.45 73.78Oracle Syn.
96.75 82.25 88.91 85.48 72.67 78.55 85.6 75.6 72.1Hidden Syn.
95.06 79.06 86.32 83.82 69.72 76.12GermanBaseline 93.49 44.24 60.06 75.00 35.49 48.18Oracle Syn.
95.18 79.11 86.41 73.24 60.87 66.49 79.7 68.1 67.8Hidden Syn.
91.92 86.26 89.00 69.47 65.19 67.26JapaneseBaseline 91.64 43.36 58.87 80.41 38.05 51.66Oracle Syn.
93.84 48.15 63.64 90.06 46.21 61.08 78.2 62.7 72.0Hidden Syn.
90.88 73.47 81.25 73.42 59.36 65.65SpanishBaseline 82.90 39.47 53.48 67.64 32.21 43.64Oracle Syn.
98.96 94.19 96.52 70.68 67.27 68.93 80.5 70.4 73.4Hidden Syn.
96.15 90.53 93.25 68.81 64.79 66.74Table 2: SRL Results.
The hidden model excels on the unlabeled prediction results, often besting the scores obtainedusing the parses distributed with the CoNLL data sets.
These gains did not always translate to the labeled task wherepoor sense prediction hindered absolute performance.4.3 ResultsWe evaluate across a set of model configurationsanalogous to before.
All experiments used 30 itera-tions of SGD with a Gaussian prior, and a max 10 it-erations of BP to compute the marginals for each ex-ample.
In comparison to the CoNLL competition en-tries (Table 2, rightmost columns) our syntactically-informed models generally fall in the middle of therankings.
This is not surprising given the indepen-dent predictions of the model and the very general,language universal assumptions we have made in themodel structure and feature sets.
However, in termsof gauging the usefulness of the hidden syntacticmarginalization method the results are extremelycompelling, with only marginal differences betweenthe performance of the observed-syntax model, es-pecially relative to the baseline.And despite the simplicity of the model, we stillmanage to perform at state-of-the-art levels in afew instances, sometimes outperforming most of thecompetition entries without observing any syntax.The performance on Chinese is an example of this,with our system outperforming all but the best sys-tem, and the hidden syntactic model only slightlybehind.Abstracting away from the performance compar-isons against other systems, the unlabeled results arethe more revealing evidence for the use of hiddensyntactic structure.
Here the average hidden modelscore (88.89) almost outperforms the observed syn-tax model (90.22, and vs. 66.80 baseline), mostlydue to the large margins on the unlabeled Japanesescores.
The strong independence between senseprediction and argument prediction hinders perfor-mance on the labeled task, but on all languages wefind an extremely significant improvement exploit-ing hidden syntactic structure in comparison to thebaseline system?the hidden model recovers morethan 92% of the gap between the baseline and theobserved syntax model.
It is also interesting to notethat in the shared task competition the two languageswhich systems lost the most performance betweentheir parsing F1 and their SRL F1 were Japaneseand German.
As illustrated in Fig.
3, the corre-818spondence between syntax and SRL are extremely,and systematically, poor.
In this example our hid-den structure model was able to assign strong beliefsto the latent syntactic variables which correspond tothe correct predicate/argument pairs, allowing it tocorrectly identify three of the four SRL argumentswhen the joint model failed to recover one.5 Related WorkThis work is perhaps mostly closely related tothe Learning over Constrained Latent Representa-tions (LCLR) framework of Chang et al2010).Their abstract problem formulation is identical: bothparadigms seek to couple the end task to an interme-diate representation which is not accessible to thelearning algorithm.
However much of the intent,scale, and methodology is different.
LCLR aimsto provide a flexible latent structure for increasingthe representational power of the model in a use-ful way, and is demonstrated on tasks and domainswhere data availability is not a key concern.
In con-trast, while our hidden structure models may outper-form their observed syntax counterparts, our focusis as much on alleviating the burden of procuringlarge amounts of syntactic annotation as it is aboutincreasing the expressiveness of the model.
To thatend we constrain a more sophisticated latent repre-sentation and couple it to highly structured outputpredictions, opposed to binary classification prob-lems.
In methodology, we perform the more com-putationally intensive marginalization operation in-stead of maximizing.Marginalization of hidden structure is also funda-mental to other work, and featured most prominentlyin generative Bayesian latent variable models (Tehet al2006).
Our approach is trained discrimina-tively, affording the use of very rich feature sets andthe prediction of partial structures without needingto specify a full derivation.
Similar approaches havebeen used in more linear latent variable CRF-basedmodels (McCallum et al2005), but these must onlymarginalize only over hidden states of a much morecompact representation.
Naively extending this totree-based constraints would often be computation-ally inefficient, and we avoid intractability throughthe encapsulation of much of the dynamic program-ming machinery into specialized factors.
Moreover,using loopy belief propagation means that the in-ference method is not closely coupled to the taskstructure, and need not change when applying thismethod to other types of graphs.6 ConclusionWe have presented a novel method of couplingsyntactically-oriented NLP tasks to combinatorially-constrained hidden syntactic representations, andhave shown that marginalizing over this latent rep-resentation not only provides significant improve-ments over syntactically-uninformed baselines, butoccasionally improves performance when comparedto systems which observe syntax.
On the task ofrelation extraction we find that a constituency rep-resentation provides the most improvement over thebaseline, while in the SRL domain our model is ex-tremely competitive with the best reported results onChinese, and outperforms the model using the pro-vided parses on German and Japanese.We believe this method delivers very promisingresults in our presented tasks, opening the door tonew lines of research examining what types of con-straints and what configurations of hidden struc-ture are most beneficial for particular tasks and lan-guages.
Moreover, we present one type of coordinat-ing factor, as both D-CONNECT and C-CONNECTlogically express a soft NAND function, but moresophisticated coupling schemes are another naturaldirection to pursue.
Finally, we use sum-productvariant of belief propagation inference, but morespecialized inference schemes may show additionalbenefits.AcknowledgementsWe would like to thank Andrea Gesmundo for help inprocuring sections of the CoNLL 2009 shared task data.This work was supported in part by the Center for Intel-ligent Information Retrieval and in part by Army primecontract number W911NF-07-1-0216 and University ofPennsylvania subaward number 103-548106.
The Uni-versity of Massachusetts also gratefully acknowledgesthe support of Defense Advanced Research ProjectsAgency (DARPA) Machine Reading Program under AirForce Research Laboratory (AFRL) prime contract no.FA8750-09-C-0181.
Any opinions, findings, and conclu-sion or recommendations expressed in this material arethose of the authors and do not necessarily reflect theview of the DARPA, AFRL, or the US government.819ReferencesMichael Auli and Adam Lopez.
2011.
A comparison ofloopy belief propagation and dual decomposition forintegrated CCG supertagging and parsing.
In ACL,pages 470?480.Kurt Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a col-laboratively created graph database for structuring hu-man knowledge.
In SIGMOD, pages 1247?1250, NewYork, NY, USA.
ACM.Razvan C. Bunescu and Raymond J. Mooney.
2007.Learning to extract relations from the web using mini-mal supervision.
In ACL.Francisco Casacuberta and Colin De La Higuera.
2000.Computational complexity of problems on probabilis-tic grammars and transducers.
In ICGI, pages 15?24.M.
Chang, D. Goldwasser, D. Roth, and V. Srikumar.2010.
Discriminative learning over constrained latentrepresentations.
In NAACL.Aron Culotta and Jeffery Sorensen.
2004.
Dependencytree kernels for relation extraction.
In ACL, Barcelona,Spain.Jenny Rose Finkel and Christopher D. Manning.
2009.Joint parsing and named entity recognition.
InNAACL, pages 326?334.Vaibbhava Goel and William J. Byrne.
2000.
MinimumBayes risk automatic speech recognition.
ComputerSpeech and Language, 14(2):115?135.Joshua T. Goodman.
1996.
Parsing algorithms and met-rics.
In ACL, pages 177?183.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic dependen-cies in multiple languages.
In CoNLL: Shared Task,pages 1?18.Dan Klein and Chris Manning.
2002.
Fast exact infer-ence with a factored model for natural language pro-cessing.
In NIPS.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In HLT-NAACL, pages 169?176.Joel Lang and Mirella Lapata.
2010.
Unsupervised in-duction of semantic roles.
In HLT-NAACL, pages 939?947.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In ACL, pages 593?601.Qiang Liu and Alexander Ihler.
2011.
Variational algo-rithms for marginal MAP.
In UAI, pages 453?462.Jonathan May and Kevin Knight.
2006.
A better n-bestlist: Practical determinization of weighted finite treeautomata.
In HLT-NAACL, pages 351?358.Andrew McCallum, Kedar Bellare, and Fernando C. N.Pereira.
2005.
A conditional random field fordiscriminatively-trained finite-state string edit dis-tance.
In UAI, pages 388?395.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005.
Non-projective dependency pars-ing using spanning tree algorithms.
In HLT-EMNLP,pages 523?530.Ivan Meza-Ruiz and Sebastian Riedel.
2009.
Jointlyidentifying predicates, arguments and senses usingMarkov logic.
In NAACL.Mike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-sky.
2009.
Distant supervision for relation extractionwithout labeled data.
In ACL, pages 1003?1011.Jason Naradowsky and David A. Smith.
2012.
Combina-torial constraints for constituency parsing in graphicalnovels.
Technical report, University of MassachusettsAmherst.Khalil Sima?an.
1996.
Computational complexityof probabilistic disambiguation by means of tree-grammars.
In COLING, pages 1175?1180.David A. Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In EMNLP, pages 145?156.Y.
W. Teh, M. I. Jordan, M. J. Beal, and D. M. Blei.
2006.Hierarchical Dirichlet processes.
Journal of the Amer-ican Statistical Association, 101(476):1566?1581.Kristina Toutanvoa and Christopher D. Manning.
2000.Enriching the knowledge sources used in a maximumentropy part-of-speech tagger.
In EMNLP, pages 63?70.Christopher Walker.
2006.
Ace 2005 multilingual train-ing corpus.
number ldc2006t06.
In Linguistic DataConsortium.Nianwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
In Natural Lan-guage Engineering, pages 207?238.Min Zhang, Jie Zhang, and Jian Su.
2006.
Exploringsyntactic features for relation extraction using a con-volution tree kernel.
In NAACL, pages 288?295.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation extrac-tion.
In ACL, pages 427?434.820
