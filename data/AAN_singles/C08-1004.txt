Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 25?32Manchester, August 2008An Improved Hierarchical Bayesian Model of Languagefor Document ClassificationBen AllisonDepartment of Computer ScienceUniversity of SheffieldUKben@dcs.shef.ac.ukAbstractThis paper addresses the fundamentalproblem of document classification, andwe focus attention on classification prob-lems where the classes are mutually exclu-sive.
In the course of the paper we advo-cate an approximate sampling distributionfor word counts in documents, and demon-strate the model?s capacity to outperformboth the simple multinomial and more re-cently proposed extensions on the classifi-cation task.
We also compare the classi-fiers to a linear SVM, and show that pro-vided certain conditions are met, the newmodel allows performance which exceedsthat of the SVM and attains amongst thevery best published results on the News-groups classification task.1 IntroductionDocument classification is one of the key technolo-gies in the emerging digital world: as the amountof textual information existing in electronic formincreases exponentially, reliable automatic meth-ods to sift through the haystack and pluck out theoccasional needle are almost a necessity.Previous comparative studies of different classi-fiers (for example, (Yang and Liu, 1999; Joachims,1998; Rennie et al, 2003; Dumais et al, 1998))have consistently shown linear Support Vector Ma-chines to be the most appropriate method.
Gen-erative probabilistic classifiers, often representedby the multinomial classifier, have in these same?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.studies performed poorly, and this empirical evi-dence has been bolstered by theoretical arguments(Lasserre et al, 2006).In this paper we revisit the theme of genera-tive classifiers for mutually exclusive classificationproblems, but consider classifiers employing morecomplex models of language; as a starting pointwe consider recent work (Madsen et al, 2005)which relaxes some of the multinomial assump-tions.
We continue and expand upon the themeof that work, but identify some weaknesses bothin its theoretical motivations and practical applica-tions.
We demonstrate a new approximate modelwhich overcomes some of these concerns, anddemonstrate substantial improvements that such amodel achieves on four classification tasks, threeof which are standard and one of which is a newlycreated task.
We also show the new model to behighly competitive to an SVM where the previousmodels are not.
?2 of the paper describes previous work whichhas sought a probabilistic model of language andits application to document classification.
?3 de-scribes the models we consider in this paper, andgives details of parameter estimation.
?4 describesour evaluation of the models, and ?5 presents theresults of this evaluation.
?6 explores reasons forthe observed results, and finally ?7 ends with someconcluding remarks.2 Related WorkThe problem of finding an appropriate andtractable model for language is one which has beenstudied in many different areas.
In many cases, thefirst (and often only) model is one in which countsof words are modelled as binomial?
or Poisson?distributed random variables.
However, the use ofsuch distributions entails an implicit assumption25that the occurrence of words is the result of a fixednumber of independent trials?draws from a ?bagof words?
?where on each trial the probability ofsuccess is constant.Several authors, among them (Church and Gale,1995; Katz, 1996), observe empirically such mod-els are not always accurate predictors of actualword behaviour.
This moves them to suggest dis-tributions for word counts where the underlyingprobability varies between documents; thus the ex-pected behaviour of a word in a new document isa combination of predictions for all possible prob-abilities.
Other authors (Jansche, 2003; Eyhera-mendy et al, 2003; Lowe, 1999) use these sameideas to classify documents on the basis of subsetsof vocabulary, in the first and third cases with en-couraging results using small subsets (in the sec-ond case, the performance of the model is shownto be poor compared to the multinomial).When one moves to consider counts of all wordsin some vocabulary, the proper distribution ofthe whole vector of word counts is multinomial.
(Madsen et al, 2005) apply the same idea as forthe single word (binomial) case to the multino-mial, using the most convenient form of distribu-tion to represent the way the vector of multino-mial probabilities varies between documents, andreport encouraging results compared to the simplemultinomial.
However, we show that the use ofthe most mathematically convenient distribution todescribe the way the vector of probabilities variesentails some unwarranted and undesirable assump-tions.
This paper will first describe those assump-tions, and then describe an approximate techniquefor overcoming the assumptions.
We show that,combined with some alterations to estimation, themodels lead to a classifier able to outperform boththe multinomial classifier and a linear SVM.3 Probabilistic Models of Language forDocument ClassificationIn this section, we briefly describe the use of a gen-erative model of language as applied to the prob-lem of document classification, and also how weestimate all relevant parameters for the work whichfollows.In terms of notation, we use c?
to represent a ran-dom variable and c to represent an outcome.
Weuse roman letters for observed or observable quan-tities and greek letters for unobservables (i.e.
pa-rameters).
We write c?
?
?
(c) to mean that c?
hasprobability density (discrete or continuous) ?
(c),and write p(c) as shorthand for p(c?
= c).
Finally,we make no explicit distinction in notation be-tween univariate and multivariate quantities; how-ever, we use ?jto refer to the j-th component ofthe vector ?.We consider documents to be represented asvectors of count?valued random variables suchthat d = {d1...dv}.
For classification, interestcentres on the conditional distribution of the classvariable, given such a document.
Where docu-ments are to be assigned to one class only (as inthe case of this paper), this class is judged to bethe most probable class.
For generative classifierssuch as those considered here, the posterior distri-bution of interest is modelled from the joint distri-bution of class and document; thus if c?
is a variablerepresenting class and?d is a vector of word counts,then:p(c|d) ?
p(c) ?
p(d|c) (1)For the purposes of this work we also assume auniform prior on c?, meaning the ultimate decisionis on the basis of the document alone.Multinomial Sampling ModelA natural way to model the distribution ofcounts is to let p(d|c) be distributed multinomially,as proposed in (Guthrie et al, 1994; McCallumand Nigam, 1998) amongst others.
The multino-mial model assumes that documents are the resultof repeated trials, where on each trial a word is se-lected at random, and the probability of selectingthe j-th word from class c is ?cj.
However, in gen-eral we will not use the subscript c ?
we estimateone set of parameters for each possible class.Using multinomial sampling, the term p(d|c)has distribution:pmultinomial(d|?)
=(?jdj)!?j(dj!
)?j?djj(2)A simple Bayes estimator for ?
can be obtainedby taking the prior for ?
as a Dirichlet distribution,in which case the posterior is also Dirichlet.
De-note the total training data for the class in ques-tion as D = {(d11...d1v) ... (dk1...dkv)} (that is,counts of each of v words in k documents).
Thenif p(?)
?
Dirichlet(?1...?v), the mean of p(?|D)for the j-th component of ?
(which is the estimatewe use) is:26?
?j= E[?j|D] =?j+ nj?j?j+ n?
(3)where the njare the sufficient statistics?inij,and n?is?jnj.
We follow common practice anduse the standard reference Dirichlet prior, which isuniform on ?, such that ?j= 1 for all j.3.1 Hierarchical Sampling ModelsIn contrast to the model above, a hierarchical sam-pling model assumes that??
varies between docu-ments, and has distribution which depends uponparameters ?.
This allows for a more realisticmodel, letting the probabilities of using words varybetween documents subject only to some generaltrend.For example, consider documents about politics:some will discuss the current British Prime Minis-ter, Gordon Brown.
In these documents, the proba-bility of using the word brown (assuming case nor-malisation) may be relatively high.
Other politicsarticles may discuss US politics, for example, orthe UN, French elections, and so on, and these ar-ticles may have a much lower probability of usingthe word brown: perhaps just the occasional refer-ence to the Prime Minister.
A hierarchical modelattempts to model the way this probability variesbetween documents in the politics class.Starting with the joint distribution p(?, d|?)
andaveraging over all possible values that ?
may takein the new document gives:p(d|?)
=?p(?|?)p(d|?)
d?
(4)where integration is understood to be over theentire range of possible ?.
Intuitively, this allows??
to vary between documents subject to the restric-tion that??
?
p(?|?
), and the probability of observ-ing a document is the average of its probability forall possible ?, weighted by p(?|?).
The samplingprocess is 1) ?
is first sampled from p(?|?)
andthen 2) d is sampled from p(d|?
), leading to thehierarchical name for such models.Dirichlet Compound Multinomial SamplingModel(Madsen et al, 2005) suggest a form of (4)where p(?|?)
is Dirichlet-distributed, leading toa Dirichlet?Compound?Multinomial sampling dis-tribution.
The main benefit of this assumption isthat the integral of (4) can be obtained in closedform.
Thus p(d|?)
(using the standard ?
notationfor Dirichlet parameters) has distribution:pDCM(d|?)
=(?jdj)!?j(dj!)??(?j?j)?
(?jdj+ ?j)??j?
(?j+ dj)?
(?j)(5)Maximum likelihood estimates for the ?
are dif-ficult to obtain, since the likelihood for ?
is a func-tion which must be maximised for all componentssimultaneously, leading some authors to use ap-proximate distributions to improve the tractabilityof maximum likelihood estimation (Elkan, 2006).In contrast, we reparameterise the Dirichlet com-pound multinomial, and estimate some of the pa-rameters in closed form.We reparameterise the model in terms of ?
and?
?
?
is a vector of length v, and ?
is a con-stant which reflects the variance of ?.
Under thisparametrisation, ?j= ??j.
The estimate we usefor ?jis simply:??j=njn?
(6)where njand n?are defined above.
This simplymatches the first moment about the mean of thedistribution with the first moment about the meanof the sample.
Once again letting:D = {d1...dk} = {(d11...d1v)...(dk1...dkv)}denote the training data such that the diare indi-vidual document vectors and dijare counts of thej-th word in the i-th document, the likelihood for?
is:L(?)
=?i?(?j??j)?
(?jdij+ ??j)?j?(?
?j+ dij)?(?
?j)(7)This is a one?dimensional function, and as suchis much more simple to maximise using standardoptimisation techniques, for example as in (Minka,2000).As before, however, simple maximum likeli-hood estimates alone are not sufficient: if a wordfails to appear at all in D, the corresponding ?jwill be zero, in which case the distribution is im-proper.
The theoretically sound solution would be27to incorporate a prior on either ?
or (under ourparameterisation) ?
; however, this would lead tohigh computational cost as the resulting posteriorwould be complicated to work with.
(Madsen etal., 2005) instead set each ?
?jas the maximum like-lihood estimate plus some , in some ways echo-ing the estimation of ?
for the multinomial model.Unfortunately, unlike a prior this strategy has thesame effect regardless of the amount of trainingdata available, whereas any true prior would havediminishing effect as the amount of training dataincreased.
Instead, we supplement actual trainingdata with a pseudo?document in which every wordoccurs once (note this is quite different to setting = 1); this echoes the effect of a true prior on ?,but without the computational burden.A Joint Beta-Binomial Sampling ModelDespite its apparent convenience and theoreticalwell?foundedness, the Dirichlet compound multi-nomial model has one serious drawback, whichis emphasised by the reparameterisation.
Underthe Dirichlet, there is a functional dependence be-tween the expected value of ?j, ?jand its variance,where the relationship is regulated by the constant?.
Thus two words whose ?jare the same will alsohave the same variance in the ?j.
This is of concernsince different words have different patterns of use?
to use a popular turn of phrase, some words aremore ?bursty?
than others (see (Church and Gale,1995) for examples).
In practice, we may hopeto model different words as having the same ex-pected value, but drastically different variances ?unfortunately, this is not possible using the Dirich-let model.The difficulty with switching to a differentmodel is the evaluation of the integral in (4).
Theintegral is in fact in many thousands of dimensions,and even if it were possible to evaluate such anintegral numerically, the process would be excep-tionally slow.We overcome this problem by decomposing theterm p(d|?)
into a product of independent termsof the form p(dj|?j).
A natural way for each ofthese terms to be distributed is to let the probabilityp(dj|?j) be binomial and to let p(?j|?j) be beta?distributed.
The probability p(dj|?j) (where ?j={?j, ?j}, the parameters of the beta distribution) isthen:pbb(dj|?j, ?j) =(ndj)B(dj+ ?j, n?
dj+ ?j)B(?j, ?j)(8)where B(?)
is the Beta function.
The termp(d|?)
is then simply:pbeta?binomial(d|?)
=?jp(dj|?j) (9)This allows means and variances for each of the?jto be specified separately, but this comes at aprice: while the Dirichlet ensures that?j?j= 1for all possible ?, the model above does not.
Thusthe model is only an approximation to a true modelwhere components of ?
have independent meansand variances, and the requirements of the multi-nomial are fulfilled.
However, given the inflexibil-ity of the Dirichlet multinomial model, we arguethat such a sacrifice is justified.In order to estimate parameters of the Beta?Binomial model, we take a slight departure fromboth (Lowe, 1999) and (Jansche, 2003) who haveboth used a similar model previously for individualwords.
(Lowe, 1999) uses numerical techniquesto find maximum likelihood estimates of the ?jand ?j, which was feasible in that case because ofthe highly restricted vocabulary and two-classes.
(Jansche, 2003) argues exactly this point, and usesmoment?matched estimates; our estimation is sim-ilar to that, in that we use moment?matching, butdifferent in other regards.Conventional parameter estimates are affected(in some way or other) by the likelihood functionfor a parameter, and the likelihood function is suchthat longer documents exert a greater influence onthe overall likelihood for a parameter.
That is, wenote that if the true binomial parameter ?ijfor thej-th word in the i-th document were known, thenthe most sensible expected value for the distribu-tion over ?jwould be:E [?j] =1k?k?i=1?ij(10)Whereas the expected value of conventionalmethod?of?moments estimate is:E [?j] =k?i=1p (?ij)??
?ij(11)That is, a weighted mean of the maximum like-lihood estimates of each of the ?ij, with weights28given by p (?ij), i.e.
the length of the i-th docu-ment.
Similar effects would be observed by max-imising the likelihood function numerically.
Thisis to our minds undesireable, since we do note be-lieve that longer documents are necessarily morerepresentative of the population of all documentsthan are shorter ones (indeed, extremely long doc-uments are likeliy to be an oddity), and in any casethe goal is to capture variation in the parameters.This leads us to suggest estimates for parameterssuch that the expected value of the distribution isas in 10 but with the ?ij(which are unknown) re-placed with their maximum likelihood estimates,??ij.
We then use these estimates to specify the de-sired variance, leading to the simultaneous equa-tions:?j?j+ ?j=?i?
?ijk(12)?j?j(?j+ ?j)2(?j+ ?j+ 1)=?i(??ij?
E[?j])2k(13)As before, we supplement actual training doc-uments with a pseudo-document in which everyword occurs once to prevent any ?jbeing zero.4 Evaluating the ModelsThis section describes evaluation of the modelsabove on four text classification problems.The Newsgroups task is to classify postings intoone of twenty categories, and uses data originallycollected in (Lang, 1995).
The task involves a rel-atively large number of documents (approximately20,000) with roughly even distribution of mes-sages, giving a very low baseline of approximately5%.For the second task, we use a task derived fromthe Enron mail corpus (Klimt and Yang, 2004), de-scribed in (Allison and Guthrie, 2008).
Corpus isa nine?way email authorship attribution problem,with 4071 emails (between 174 and 706 emails perauthor)1.
The mean length of messages in the cor-pus is 75 words.WebKB is a web?page classification task, wherethe goal is to determine the webpage type of theunseen document.
We follow the setup of (McCal-lum and Nigam, 1998) and many thereafter, and1The corpus is available for download fromwww.dcs.shef.ac.uk/?ben.use the four biggest categories, namely student,faculty, course and project.
The resultingcorpus consists of approximately 4,200 webpages.The SpamAssassin corpus is made available forpublic use as part of the open-source Apache Spa-mAssassin Project2.
It consists of email dividedinto three categories: Easy Ham, which is emailunambiguously ham (i.e.
not spam), Hard Hamwhich is not spam but shares many traits withspam, and finally Spam.
The task is to apply theselabels to unseen emails.
We use the latest ver-sion of all datasets, and combine the easy ham andeasy ham 2 as well as spam and spam 2 sets toform a corpus of just over 6,000 messages.In all cases, we use 10?fold cross validationto make maximal use of the data, where foldsare chosen by random assignment.
We define?words?
to be contiguous whitespace?delimitedalpha?numeric strings, and perform no stemmingor stoplisting.For the purposes of comparison, we also presentresults using a linear SVM (Joachims, 1999),which we convert to multi?class problems usinga one?versus?all strategy shown to be amongstthe best performing strategies (Rennie and Rifkin,2001).
We normalise documents to be vectors ofunit length, and resolve decision ambiguities bysole means of distance to the hyperplane.
We alsonote that experimentation with non?linear kernelsshowed no consistent trends, and made very littledifference to performance.5 ResultsTable 1 displays results for the three models overthe four datasets.
We use the simplest measure ofclassifier performance, accuracy, which is simplythe total number of correct decisions over the tenfolds, divided by the size of the corpus.
In responseto a growing unease over the use of significancetests (because they have a tendency to overstatesignificance, as well as obscure effects of samplesize) we provide 95% intervals for accuracy as wellas the metric itself.
To calculate these, we view ac-curacy as an (unknown) parameter to a binomialdistribution such that the number of correctly clas-sified documents is a binomially distributed ran-dom variable.
We then calculate the Bayesian in-terval for the parameter, as described in (Brown etal., 2001), which allows immediate quantification2The corpus is available online athttp://spamassassin.apache.org/publiccorpus/29of uncertainty in the true accuracy after a limitedsample.As can be seen from the performance figures,no one classifier is totally dominant, although thereare obvious and substantial gains in using the Beta-Binomial model on the Newsgroups and Enrontasks when compared to all other models.
The Spa-mAssassin corpus shows the beta?binomial modeland the SVM to be considerably better than theother two models, but there is little to choose be-tween them.
The WebKB task, however, showsextremely unusual results: the SVM is head andshoulders above other methods, and of the genera-tive approaches the multinomial is clearly superior.In all cases, the Dirichlet model actually performsworse than the multinomial model, in contrast tothe observations of (Madsen et al, 2005).In terms of comparison with other work, we notethat the performance of our multinomial modelagrees with that in other work, including for exam-ple (Rennie et al, 2003; Eyheramendy et al, 2003;Madsen et al, 2005; Jansche, 2003).
Our Dirichletmodel performs worse than that in (Madsen et al,2005) (85% here compared to 89% in that work),which we attribute to their experimentation withalternate smoothing  as described in ?3.1.
Wenote however that the Beta-Binomial model herestill outperforms that work by some considerablemargin.
Finally, we note that our beta?binomialmodel outperforms that in (Jansche, 2003), whichwe attribute mainly to the altered estimate, but alsoto the partial vocabulary used in that work.
In fact,(Jansche, 2003) shows there to be little to sepa-rate the beta-binomial and multinomial models forlarger vocabularies, in stark contrast to the workhere, and this is doubtless due to the parameter es-timation.6 AnalysisOne might expect performance of a hierarchicalsampling model to eclipse that of the SVM becauseof the nature of the decision boundary, providedcertain conditions are met: the SVM estimatesa linear decision boundary, and the multinomialclassifier does the same.
However, the decisionboundaries for the hierarchical classifiers are non?linear, and can represent more complex word be-haviour, provided that sufficient data exist to pre-dict it.
However, unlike generic non?linear SVMs(which made little difference compared to a lin-ear SVM) the non?linear decision boundary herearises naturally from a model of word behaviour.For the hierarchical models, performance restson the ability to estimate both the rate of word oc-currence ?jand also the way that this rate variesbetween documents.
To reliably estimate variance(and arguably rate as well) would require words tooccur a sufficient number of times.
However, thissection will demonstrate that two of the datasetshave many words which do not occur with suf-ficient frequency to estimate parameters, and inthose two the linear SVM?s performance is morecomparable.We present two quantifications of word reuse tosupport our conclusions.
The first are frequencyspectra for each of the four corpora, shown in Fig-ure 1.
The two more problematic datasets appearin the top of the figure.
To generate the charts, wepool all documents from all classes in a each prob-lem, and count the number of words that appearonce, twice, and so on.
The x axis is the num-ber of times a word occurs, and the y axis the totalnumber of words which have that count.The WebKB corpus has the large majority ofwords occurring very few times (the mass of thedistribution is concentrated towards the left of thechart), while the SpamAssassin corpus is more rea-sonable and the Newsgroups corpus has by farthe most words which occur with substantial fre-quency (this correlates perfectly with the relativeperformances of the classifiers on these datasets).For the Enron corpus, it is somewhat harder to tell,since its size means no words occur with substan-tial frequency.We also consider the proportion of all word pairsin a corpus in which the first word is the same asthe second word.
If a corpus has n?words totalwith total counts n1...nvthen the statistic is:r =1(n?(n??
1)) /2?i(ni(ni?
1))/2.
(14)To measure differing tendencies to reuse words,we calculate the r statistic once for each class, andthen its mean across all classes in a problem (Ta-ble 2).
We note that the two corpora on which thehierarchical model dominates have much greatertendency for word reuse, meaning the extra pa-rameters can be esimated with greater accuracy.The SpamAssassin corpus is, by this measure, aharder task, but this is somewhat mitigated by themore even frequency distribution evidenced in Fig-ure 1; on the other hand, the WebKB corpus does30Newsgroups Enron Authors WebKB SpamAssassinMultinomial 85.66 ?
0.5 74.55 ?
1.34 85.69 ?
1.06 95.96 ?
0.5DCM 85.03 ?
0.51 74.43 ?
1.34 82.69 ?
1.15 91.47 ?
0.7Beta-Bin 91.65 ?
0.4?+83.54 ?
1.14?+84.81 ?
1.08 97.35 ?
0.4?SVM 88.8 ?
0.45?80 ?
1.23?92.68 ?
0.79?97.65 ?
0.38?Table 1: Performance of four classifiers on four tasks.
Error is 95% interval for accuracy.
Bold denotesbest performance on a task.
?denotes performance superior to multinomial which exceeds posterioruncertainty (i.e.
observed performance outside 95% interval).+denotes the same for the SVMFrequency Spectrum of Words in the SpamAssassin CorpusFrequencylog(Number ofwords with frequency)1101001000100000 500 1000 1500Frequency Spectrum of Words in the WebKB CorpusFrequencylog(Number ofwords with frequency)1101001000100000 500 1000 1500Frequency Spectrum of Words in the Newsgroups CorpusFrequencylog(Number ofwords with frequency)1101001000100000 500 1000 1500Frequency Spectrum of Words in the Enron Authors CorpusFrequencylog(Number ofwords with frequency)1101001000100000 500 1000 1500Figure 1: Frequency spectra for the four datasets.
y axis is on a logarithmic scalenot look promising for the hierarchical model byeither measure.7 ConclusionIn this paper, we have advocated the use of ajoint beta?binomial distribution for word counts indocuments for the purposes of classification.
Wehave shown that this model outperforms classifiersbased upon both multinomial and Dirichlet Com-pound Multinomial distributions for word counts.We have further made the case that, where cor-pora are sufficiently large as to warrant it, a gener-ative classifier employing a hierarchical samplingmodel outperforms a discriminative linear SVM.We attribute this to the capacity of the proposedmodel to capture aspects of word behaviour be-yond a simpler model.
However, in cases wherethe data contain many infrequent words and thetendency to reuse words is relatively low, default-ing to a linear classifier (either the multinomialfor a generative classifier, or preferably the lin-ear SVM) increases performance relative to a morecomplex model, which cannot be fit with sufficientprecision.ReferencesAllison, Ben and Louise Guthrie.
2008.
Authorship at-tribution of e-mail: Comparing classifiers over a newcorpus for evaluation.
In Proceedings of LREC?08.Brown, Lawrence D., Tony Cai, and Anirban Das-Gupta.
2001.
Interval estimation for a binomial pro-portion.
Statistical Science, 16(2):101?117, may.31Newsgroups Enron Authors WebKB SpamAssassinMean r 0.0090 0.0083 0.0047 0.0037Table 2: Mean r statistic for the four problemsChurch, K. and W. Gale.
1995.
Poisson mixtures.
Nat-ural Language Engineering, 1(2):163?190.Dumais, Susan, John Platt, David Heckerman, andMehran Sahami.
1998.
Inductive learning algo-rithms and representations for text categorization.
InCIKM ?98, pages 148?155.Elkan, Charles.
2006.
Clustering documents withan exponential-family approximation of the dirich-let compound multinomial distribution.
In Proceed-ings of the Twenty-Third International Conferenceon Machine Learning.Eyheramendy, S., D. Lewis, and D. Madigan.
2003.The naive bayes model for text categorization.
Arti-ficial Intelligence and Statistics.Guthrie, Louise, Elbert Walker, and Joe Guthrie.
1994.Document classification by machine: theory andpractice.
In Proceedings COLING ?94, pages 1059?1063.Jansche, Martin.
2003.
Parametric models of linguisticcount data.
In ACL ?03, pages 288?295.Joachims, Thorsten.
1998.
Text categorization withsupport vector machines: learning with many rele-vant features.
In N?edellec, Claire and C?eline Rou-veirol, editors, Proceedings of ECML-98, 10th Euro-pean Conference on Machine Learning, pages 137?142.Joachims, Thorsten.
1999.
Making large-scale svmlearning practical.
Advances in Kernel Methods -Support Vector Learning.Katz, Slava M. 1996.
Distribution of content wordsand phrases in text and language modelling.
Nat.Lang.
Eng., 2(1):15?59.Klimt, Bryan and Yiming Yang.
2004.
The enron cor-pus: A new dataset for email classification research.In Proceedings of ECML 2004, pages 217?226.Lang, Ken.
1995.
NewsWeeder: learning to filter net-news.
In Proceedings of the 12th International Con-ference on Machine Learning, pages 331?339.Lasserre, Julia A., Christopher M. Bishop, andThomas P. Minka.
2006.
Principled hybrids of gen-erative and discriminative models.
In CVPR ?06:Proceedings of the 2006 IEEE Computer SocietyConference on Computer Vision and Pattern Recog-nition, pages 87?94.Lowe, S. 1999.
The beta-binomial mixture model andits application to tdt tracking and detection.
In Pro-ceedings of the DARPA Broadcast News Workshop.Madsen, Rasmus E., David Kauchak, and CharlesElkan.
2005.
Modeling word burstiness using theDirichlet distribution.
In ICML ?05, pages 545?552.McCallum, A. and K. Nigam.
1998.
A comparisonof event models for na?
?ve bayes text classification.In Proceedings AAAI-98 Workshop on Learning forText Categorization.Minka, Tom.
2000.
Estimating a dirichlet distribution.Technical report, Microsoft Research.Rennie, Jason D. M. and Ryan Rifkin.
2001.
Improv-ing multiclass text classification with the SupportVector Machine.
Technical report, Massachusetts In-sititute of Technology, Artificial Intelligence Labora-tory.Rennie, J., L. Shih, J. Teevan, and D. Karger.
2003.Tackling the poor assumptions of naive bayes textclassifiers.Yang, Y. and X. Liu.
1999.
A re-examination of textcategorization methods.
In 22nd Annual Interna-tional SIGIR, pages 42?49, Berkley, August.32
