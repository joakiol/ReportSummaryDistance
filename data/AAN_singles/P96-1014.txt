Computing Optimal Descriptions for Optimality TheoryGrammars with Context-Free Position StructuresBruce TesarThe Rutgers Center for Cognit ive Science /The Linguistics Depar tmentRutgers UniversityPiscataway, NJ  08855 USAtesar@ruccs, rutgers, eduAbst rac tThis paper describes an algorithm forcomputing optimal structural descriptionsfor Optimality Theory grammars withcontext-free position structures.
Thisalgorithm extends Tesar's dynamic pro-gramming approach (Tesar, 1994) (Tesar,1995@ to computing optimal structuraldescriptions from regular to context-freestructures.
The generalization to context-free structures creates several complica-tions, all of which are overcome withoutcompromising the core dynamic program-ming approach.
The resulting algorithmhas a time complexity cubic in the lengthof the input, and is applicable to gram-mars with universal constraints that ex-hibit context-free locality.1 Comput ing  Optimal Descriptionsin Opt ima l i ty  TheoryIn Optimality Theory (Prince and Smolensky, 1993),grammaticality is defined in terms of optimization.For any given linguistic input, the grammaticalstructural description of that input is the descrip-tion, selected from a set of candidate descriptionsfor that input, that best satisfies a ranked set of uni-versal constraints.
The universal constraints oftenconflict: satisfying one constraint may only be pos-sible at the expense of violating another one.
Theseconflicts are resolved by ranking the universal con-straints in a strict dominance hierarchy: one viola-tion of a given constraint is strictly worse than anynumber of violations of a lower-ranked constraint.When comparing two descriptions, the one whichbetter satisfies the ranked constraints has higherHarmony.
Cross-linguistic variation is accounted forby differences in the ranking of the same constraints.The term linguistic input should here be under-stood as something like an underlying form.
Inphonology, an input might be a string of segmentalmaterial; in syntax, it might be a verb's argumentstructure, along with the arguments.
For exposi-tional purposes, this paper will assume linguistic in-puts to be ordered strings of segments.
A candidatestructural description for an input is a full linguis-tic description containing that input, and indicatingwhat the (pronounced) surface realization is.
An im-portant property of Optimality Theory (OT) gram-mars is that they do not accept or reject inputs;every possible input is assigned a description by thegrammar.The formal definition of Optimality Theory positsa function, Gen, which maps an input to a large (of-ten infinite) set of candidate structural descriptions,all of which are evaluated in parallel by the universalconstraints.
An OT grammar does not itself specifyan algorithm, it simply assigns a grammatical struc-tural description to each input.
However, one canask the computational question of whether efficientalgorithms exist to compute the description assignedto a linguistic input by a grammar.The most apparent computational challenge isposed by the allowance of faithfulness violations:the surface form of a structural description may notbe identical with the input.
Structural positionsnot filled with input segments constitute overpars-ing (epenthesis).
Input segments not parsed intostructural positions do not appear in the surface pro-nunciation, and constitute underparsing (deletion).To the extent hat underparsing and overparsing areavoided, the description is said to be faithful to theinput.
Crucial to Optimality Theory are faithful-ness constraints, which are violated by underparsingand overparsing.
The faithfulness constraints ensurethat a grammar will only tolerate deviations of thesurface form from the input form which are neces-sary to satisfy structural constraints dominating thefaithfulness constraints.Computing an optimal description means consid-ering a space of candidate descriptions that includestructures with a variety of faithfulness violations,and evaluating those candidates with respect o aranking in which structural and faithfulness con-straints may be interleaved.
This is parsing in thegeneric sense: a structural description is being as-101signed to an input.
It is, however, distinct fromwhat is traditionally thought of as parsing in com-putationM linguistics.
Traditional parsing attemptsto construct a grammatical description with a sur-face form matching the given input string exactly; ifa description cannot be fit exactly, the input string isrejected as ungrammatical.
Traditional parsing canbe thought of as enforcing faithfulness absolutely,with no faithfulness violations are allowed.
Partlyfor this reason, traditional parsing is usually under-stood as mapping a surface form to a description.
Inthe computation of optimal descriptions consideredhere, a candidate that is fully faithful to the inputmay be tossed aside by the grammar in favor of aless faithful description better satisfying other (dom-inant) constraints.
Computing an optimal descrip-tion in Optimality Theory is more naturally thoughtof as mapping an underlying form to a description,perhaps as part of the process of language produc-tion.Tesar (Tesar, 1994) (Tesar, 1995a) has devel-oped algorithms for computing optimal descriptions,based upon dynamic programming.
The details laidout in (Tesar, 1995a) focused on the case where theset of structures underlying the Gen function areformally regular.
In this paper, Tesar's basic ap-proach is adopted, and extended to grammars witha Gen function employing fully context-free struc-tures.
Using such context-free structures introducessome complications not apparent with the regularcase.
This paper demonstrates that the complica-tions can be dealt with, and that the dynamic pro-gramming case may be fully extended to grammarswith context-free structures.2 Context -F ree  Pos i t ion  S t ructureGrammarsTesar (Tesar, 1995a) formalizes Gen as a set ofmatchings between an ordered string of input seg-ments and the terminals of each of a set of positionstructures.
The set of possible position structuresis defined by a formal grammar, the position struc-ture grammar.
A position structure has as terminalsstructural positions.
In a valid structural descrip-tion, each structural position may be filled with atmost one input segment, and each input segmentmay be parsed into at most one position.
The linearorder of the input must be preserved in all candidatestructural descriptions.This paper considers Optimality Theory gram-mars where the position structure grammar iscontext-free; that is, the space of position structurescan be described by a formal context-free grammar.As an illustration, consider the grammar in Exam-ples 1 and 2 (this illustration is not intended to rep-resent any plausible natural language theory, butdoes use the "peak/margin" terminology sometimesemployed in syllable theories).
The set of inputsis {C,V} +.
The candidate descriptions of an inputconsist of a sequence of pieces, each of which has apeak (p) surrounded by one or more pairs of marginpositions (m).
These structures exhibit prototypi-cal context-free behavior, in that margin positionsto the left of a peak are balanced with margin po-sitions to the right.
'e' is the empty string, and 'S'the start symbol.Example  1 The Position Structure GrammarS :=~ F ieF =~ Y IYFY ~ P I MFMM ::~ mP =:~ pExample  2 The Constraints-(m/V) Do not parse V into a margin position-(p/C) Do not parse C into a peak positionPARSE Input segments must be parsedFILL m A margin position must be filledFILL p A peak position must be filledThe first two constraints are structurM, and man-date that V not be parsed into a margin position,and that C not be parsed into a peak position.
Theother three constraints are faithfulness constraints.The two structural constraints are satisfied by de-scriptions with each V in a peak position surroundedby matched C's in margin positions: CCVCC, V,CVCCCVCC, etc.
If the input string permits suchan analysis, it will be given this completely faithfuldescription, with no resulting constraint violations(ensuring that it will be optimal with respect o anyranking).Consider the constraint hierarchy in Example 3.Example  3 A Constraint Hierarchy{-(m/V), - (p/C) ,  PARSE} ~> {FILL p} > {FILL m}This ranking ensures that in optimal descriptions,a V will only be parsed as a peak, while a C will onlybe parsed as a margin.
Further, all input segmentswill be parsed, and unfilled positions will be includedonly as necessary to produce a sequence of balancedstructures.
For example, the input /VC/  receivesthe description 1 shown in Example 4.Example  4 The Optimal Description fo r /VC/S(F(Y(M(C),P(V),M(C))))The surface string for this description is CVC: thefirst C was "epenthesized" to balance with the onefollowing the peak V. This candidate is optimal be-cause it only violates FILL m, the lowest-ranked con-straint.Tesar identifies locality as a sufficient conditionon the universal constraints for the success of hisl In this paper, tree structures will be denoted withparentheses: a parent node X with child nodes Y and Zis denoted X(Y,Z).102approach.
For formally regular position structuregrammars, he defines a local constraint as one whichcan be evaluated strictly on the basis of two consec-utive positions (and any input segments filling thosepositions) in the linear position structure.
That ideacan be extended to the context-free case as follows.A local constraint is one which can be evaluatedstrictly on the basis of the information containedwithin a local region.
A local region of a descriptionis either of the following:?
a non4erminal nd the child non-terminals thatit immediately dominates;?
a non-terminal which dominates a terminalsymbol (position), along with the terminal andthe input segment (if present) filling the termi-nal position.It is important o keep clear the role of the posi-tion structure grammar.
It does not define the set ofgrammatical structures, it defines the Space of can-didate structures.
Thus, the computation ofdescrip-tions addressed in this paper should be distinguishedfrom robust, or error-correcting, parsing (Andersonand Backhouse, 1981, for example).
There, the in-put string is mapped to the grammatical structurethat is 'closest'; if the input completely matches astructure generated by the grammar, that structureis automatically selected.
In the OT case presentedhere, the full grammar is the entire OT system, ofwhich the position structure grammar is only a part.Error-correcting parsing uses optimization only withrespect o the faithfulness of pre-defined grammati-cal structures to the input.
OT uses optimization todefine grammaticality.3 The Dynamic Programming TableThe Dynamic Programming (DP) Table is here athree-dimensional, pyramid-shaped data structure.It resembles the tables used for context-free chartparsing (Kay, 1980) and maximum likelihood com-putation for stochastic ontext-free grammars (Lariand Young, 1990) (Charniak, 1993).
Each cell ofthe table contains a partial description (a part ofa structural description), and the Harmony of thatpartial description.
A partial description is muchlike an edge in chart parsing, covering a contigu-ous substring of the input.
A cell is identifiedby three indices, and denoted with square brackets(e.g., \[X,a,c\]).
The first index identifying the cell (X)indicates the cell category of the cell.
The other twoindices (a and c) indicate the contiguous ubstringof the input string covered by the partial descriptioncontained in the cell (input segments ia through ic).In chart parsing, the set of cell categories i pre-cisely the set of non-terminals in the grammar, andthus a cell contains a subtree with a root non-terminal corresponding tothe cell category, and withleaves that constitute precisely the input substringcovered by the cell.
In the algorithm presented here,the set of cell categories are the non-terminals of theposition structure grammar, along with a categoryfor each left-aligned substring of the right hand sideof each position grammar ule.
Example 5 gives theset of cell categories for the position structure gram-mar in Example 1.Example  5 The Set of Cell CategoriesS, F, Y, M, P, MFThe last category in Example 5, MF, comes fromthe rule Y =:~ MFM of Example 1, which has morethan two non-terminals on the right hand side.
Eachsuch category corresponds to an incomplete dge innormal chart parsing; having a table cell for eachsuch category eliminates the need for a separate datastructure containing edges.
The cell \[MF,a,c\] maycontain an ordered pair of subtrees, the first withroot M covering input \[a,b\], and the second withroot F covering input \[b+l,c\].The DP Table is perhaps best envisioned as a setof layers, one for each category.
A layer is a setof all cells in the table indexed by a particular cellcategory.Example  6 A Layer of the Dynamic ProgrammingTable for Category M (input i1"i3)\[U,l,3\]\[M,1,2\] \[M,2,3\]\[M,I,1\] \[M,2,2\] \[M,3,3\] Iil i2 i3For each substring length, there is a collection ofrows, one for each category, which will collectivelybe referred to as a level.
The first level contains thecells which only cover one input segment; the num-ber of cells in this level will he the number of inputsegments multiplied by the number of cell categories.Level two contains cells which cover input substringsof length two, and so on.
The top level contains onecell for each category.
One other useful partitionof the DP table is into blocks.
A block is a set ofall cells covering a particular input subsequence.
Ablock has one cell for each cell category.A cell of the DP Table is filled by comparing theresults of several operations, each of which try to filla cell.
The operation producing the partial descrip-tion with the highest Harmony actually fills the cell.The operations themselves are discussed in Section4.The algorithm presented in Section 6 fills the ta-ble cells level by level: first, all the cells coveringonly one input segment are filled, then the cells cov-ering two consecutive segments are filled, and soforth.
When the table has been completely filled,cell \[S,1,J\] will contain the optimal description ofthe input, and its Harmony.
The table may alsobe filled in a more left-to-right manner, bottom-up,in the spirit of CKY.
First, the cells covering onlysegment il, and then i2, are filled.
Then, the cells103covering the first two segments are filled, using theentries in the cells covering each of il and is.
Thecells of the next diagonal are then filled.4 The Operations SetThe Operations Set contains the operations used tofill DP Table cells.
The algorithm proceeds by con-sidering all of the operations that could be used to filla cell, and selecting the one generating the partialdescription with the highest Harmony to actuallyfill the cell.
There are three main types of opera-tions, corresponding to underparsing, parsing, andoverparsing actions.
These actions are analogous tothe three primitive actions of sequence comparison(Sankoff and Kruskal, 1983): deletion, correspon-dence, and insertion.The discussion that follows makes the assumptionthat the right hand side of every production is eithera string of non-terminals or a single terminal.
Eachparsing operation generates a new element of struc-ture, and so is associated with a position structuregrammar production.
The first type of parsing op-eration involves productions which generate a singleterminal (e.g., P:=~p).
Because we are assuming thatan input segment may only be parsed into at mostone position, and that a position may have at mostone input segment parsed into it, this parsing oper-ation may only fill a cell which covers exactly oneinput segment, in our example, cell \[P,I,1\] could befilled by an operation parsing il into a p position,giving the partial description P(p filled with il).The other kinds of parsing operations are matchedto position grammar productions in which a parentnon-terminal generates child non-terminals.
One ofthese kinds of operations fills the cell for a cate-gory by combining cell entries for two factor cat-egories, in order, so that the substrings covered byeach of them combine (concatenatively, with no over-lap) to form the input substring covered by thecell being filled.
For rule Y =~ MFM, there willbe an operation of this type combining entries in\[M,a,b\] and \[F,b+l,c\], creating the concatenatedstructure s \[M,a,b\]+\[F,b+l,c\], to fill \[MF,a,c\].
Thefinal type of parsing operation fills a cell for a cate-gory which is a single non-terminal on the left handside of a production, by combining two entries whichjointly form the entire right hand side of the pro-duction.
This operation would combining entriesin \[MF,a,c\] and \[M,c?l,d\], creating the structureY(\[MF,a,c\],\[M,c+l,d\]), to fill \[Y,a,d\].
Each of theseoperations involves filling a cell for a target cate-gory by using the entries in the cells for two factorcategories.The resulting Harmony of the partial descriptioncreated by a parsing operation will be the combina-2This partial description is not a single tree, but anordered pair of trees.
In general, such concatenatedstructures will be ordered lists of trees.tion of the marks assessed each of the partial descrip-tions for the factor categories, plus any additionalmarks incurred as a result of the structure added bythe production itself.
This is true because the con-straints must be local: any new constraint violationsare determinable on the basis of the cell category ofthe factor partial descriptions, and not any otherinternal details of those partial descriptions.All possible ways in which the factor categories,taken in order, may combine to cover the substring,must be considered.
Because the factor categoriesmust be contiguous and in order, this amounts toconsidering each of the ways in which the substringcan be split into two pieces.
This is reflected in theparsing operation descriptions given in Section 6.2.Underparsing operations are not matched with po-sition grammar productions.
A DP Table cell whichcovers only one input segment may be filled by anunderparsing operation which marks the input seg-ment as underparsed.
In general, any partial de-scription covering any substring of the input maybe extended to cover an adjacent input segment byadding that additional segment marked as under-parsed.
Thus, a cell covering a given substring oflength greater than one may be filled in two mirror-image ways via underparsing: by taking a partialdescription which covers all but the leftmost inputsegment and adding that segment as underparsed,and by taking a partial description which covers allbut the rightmost input segment and adding thatsegment as underparsed.Overparsing operations are discussed in Section 5.5 The Overparsing OperationsOverparsing operations consume no input; they onlyadd new unfilled structure.
Thus, a block of cells(the set of cells each covering the same input sub-string) is interdependent with respect o overparsingoperations, meaning that an overparsing operationtrying to fill one cell in the block is adding structureto a partial description from a different cell in thesame block.
The first consequence of this is that theoverparsing operations must be considered after theunderparsing and parsing operations for that block.Otherwise, the cells would be empty, and the over-parsing operations would have nothing to add on to.The second consequence is that overparsing oper-ations may need to be considered more than once,because the result of one overparsing operation (if itfills a cell) could be the source for another overpars-ing operation.
Thus, more than one pass through theoverparsing operations for a block may be necessary.In the description of the algorithm given in Section6.3, each Repeat-Until loop considers the overpars-ing operations for a block of cells.
The number ofloop iterations is the number of passes through theoverparsing operations for the block.
The loop iter-ations stop when none of the overparsing operations104is able to fill a cell (each proposed partial descriptionis less harmonic than the partial description alreadyin the cell).In principle, an unbounded number of overpars-ing operations could apply, and in fact descriptionswith arbitrary numbers of unfilled positions are con-tained in the output space of Gen (as formally de-fined).
The algorithm does not have to explicitlyconsider arbitrary amounts of overparsing, however.A necessary property of the faithfulness constraints,given constraint locality, is that a partial descriptioncannot have overparsed structures repeatedly addedto it until the resulting partial description falls intothe same cell category as the original prior to over-parsing, and be more Harmonic.
Such a sequence ofoverparsing operations can be considered a overpars-ing cycle.
Thus, the faithfulness constraints mustban overparsing cycles.
This is not solely a computa-tional requirement, but is necessary for the grammarto be well-defined: overparsing cycles must be har-monically suboptimal, otherwise arbitrary amountsof overparsing will be permitted in optimal descrip-tions.
In particular, the constraints hould preventoverparsing from adding an entire overparsed non-terminal more than once to the same partial descrip-tion while passing through the overparsing opera-tions.
In Example 2, the constraints FILL m andFILL p effectively ban overparsing cycles: no mat-ter where these constraints are ranked, a descriptioncontaining an overparsing cycle will be less harmonic(due to additional FILL violations) than the samedescription with the cycle removed.Given that the universal constraints meet this cri-terion, the overparsing operations may be repeatedlyconsidered for a given level until none of them in-crease the Harmony of the entries in any of the cells.Because each overparsing operation maps a partialdescription in one cell category to one for anothercell category, a partial description cannot undergomore consecutive overparsing operations than thereare cell categories without repeating at least one cellcategory, thereby creating a cycle.
Thus, the num-ber of cell categories places a constant bound on thenumber of passes made through the overparsing op-erations for a block.A single non-terminal may dominate an entiresubtree in which none of the syllable positions atthe leaves of the tree are filled.
Thus, the optimal"unfilled structure" for each non-terminal, and infact each cell category, must be determined, for useby the overparsing operations.
The optimal over-parsing structure for category X is denoted withIX,0\], and such an entity is referred to as a baseoverparsing structure.
A set of such structures mustbe computed, one for each category, before fillinginput-dependent DP table cells.
Because these val-ues are not dependent upon the input, base overpars-ing structures may be computed and stored in ad-vance.
Computing them is just like computing othercell entries, except that only overparsing operationsare considered.
First, consider (once) the overpars-ing operations for each non-terminal X which has aproduction rule permitting it to dominate a terminalx: each tries to set IX,0\] to contain the correspondingpartial description with the terminal x left unfilled.Next consider the other overparsing operations foreach cell, choosing the most Harmonic of those op-erations' partial descriptions and the prior value ofIX,0\].6 The  Dynamic  P rogrammingA lgor i thm6.1 Notat ionmaxH{} returns the argument with maximum Har-mony(i~) denotes input segment i~ underparsedX t is a non-terminalx t is a terminal+ denotes concatenation6.2 The OperationsUnderparsing Operations for \[X t,a,a\]:create (i~/+\[X*,0\]Underparsing Operations for IX t,a,c\]:create (ia)+\[X~,a+l,c\]create \[Xt,a,e-1\]+(ia)Parsing operations for \[X t,a,a\]:for each production Xt ::~ x kcreate Xt(x k filled with ia)Parsing operations for \[X*,a,c\],where c>a and all X are cell categories:for each production X t =~ XkX mfor b = a+l  to c-1create X* (\[Xk,a,b\],\[X'~,b+ 1 c\])for each production Xu :=~ X/:xmxn...where X t = XkX'~:for b=a+l  to c-1create \[Xk,a,b\]+\[X'~,b+l,c\]Overparsing operations for \[X t,0\]:for each production X t =~ x kcreate Xt(x k unfilled)for each production X t =~ XkX mcreate xt  (\[Xk,0\],\[Xm,0\])for each production X~ ~ XkXmXn...where X t -- xkxm:create \[Xk,0\]+\[Xm,0\]Overparsing operations for \[X t,a,a\]:same as for \[X*,a,c\]Overparsing operations for \[X t,a,c\]:for each production Xt ~ X kcreate X t (\[X k ,a,c\])105for each production Xt ::V xkx  "~create Xt (\[Xk,0\],\[X'~,a,c\])create X~ (\[Xk,a,c\],\[X'~,0\])for each production Xu :=~ XkXmX~...where X t = XkX'~:create \[Xk,a,c\]+\[Xm,0\]create \[Xk,0\]+\[Xm,a,c\]6.3 The Main  A lgor i thm/* create the base overparsing structures */RepeatFor each X t, Set \[Xt,0\] tomaxH{\[Xt,0\], overparsing ops for \[Xt,0\]}Until no IX t,0\] has changed uring a pass/* fill the cells covering only a single segment */For a = 1 to JFor each X t, Set \[Xt,a,a\] tomaxH{underparsing ops for \[Xt,a,a\]}For each X t, Set \[Xt,a,a\] tomaxH{\[Xt,a,a\], parsing ops for \[Xt,a,a\]}RepeatFor each X t, Set \[Xt,a,a\] tomaxH{\[Xt,a,a\], overparsing ops for \[Xt,a,a\]}Until no \[X t,a,a\] has changed uring a pass/* fill the rest of the cells */For d=l  to (J-l)For a=l  to (J-d)For each X t, Set \[Xt,a,a+d\] tomaxH{underparsing ops for \[Xt,a,a+d\]}For each X ~, Set \[Xt,a,a+d\]maxH{\[Xt,a,a+d\], parsing ops for \[Xt,a,a+d\]}RepeatFor each X t,Set \[Xt,a,a+d\] tomaxH{\[Xt,a,a+d\],overparsing ops for \[Xt,a,a+d\]}Until no \[Xt,a,a+d\] has changed uring a passReturn \[S,1,J\] as the optimal description6.4 Complex i tyEach block of cells for an input subsequence is pro-cessed in time linear in the length of the subse-quence.
This is a consequence of the fact that ingeneral parsing operations filling such a cell mustconsider all ways of dividing the input subsequenceinto two pieces.
The number of overparsing passesthrough the block is bounded from above by thenumber of cell categories, due to the fact that over-parsing cycles are suboptimal.
Thus, the numberof passes is bounded by a constant, for any fixedposition structure grammar.
The number of suchblocks is the number of distinct, contiguous inputsubsequences (equivalently, the number of cells in alayer), which is on the order of the square of thelength of the input.
If N is the length of the input,the algorithm has computational complexity O(N3).7 D iscuss ion7.1 Local i tyThat locality helps processing should he no greatsurprise to computationalists; the computationalsignificance of locality is widely appreciated.
Fur-ther, locality is often considered a desirable propertyof principles in linguistics, independent of computa-tional concerns.
Nevertheless, locality is a sufficientbut not necessary restriction for the applicability ofthis algorithm.
The locality restriction is really aspecial case of a more general sufficient condition.The general condition is a kind of "Markov"  prop-erty.
This property requires that, for any substringof the input for which partial descriptions are con-structed, the set of possible partial descriptions forthat substring may be partitioned into a finite setof classes, such that the consequences in terms ofconstraint violations for the addition of structure toa partial description may he determined entirely bythe identity of the class to which that partial de-scription belongs.
The special case of strict localityis easy to understand with respect to context-freestructures, because it states that the only informa-tion needed about a subtree to relate it to the restof the tree is the identity of the root non-terminal,so that the (necessarily finite) set of non-terminalsprovides the relevant set of classes.7.2 Underparsing and DerivationalRedundancyThe treatment of the underparsing operations givenabove creates the opportunity for the same par-tial description to be arrived at through several dif-ferent paths.
For example, suppose the input isia.
.
.
ibicid.
.
.
ie , and there is a constituent in \[X,a,b\]and a constituent \[Y,d,e\].
Further suppose the inputsegment ic is to be marked underparsed, so that thefinal description \[S,a,e\] contains \[X,a,b\] (i~) \[Y,d,e\].That description could be arrived at either by com-bining \[X,a,b\] and (ic) to fill \[X,a,c\], and then com-bine \[X,a,c\] and \[Y,d,e\], or it could be arrived at bycombining (i~) and \[Y,d,e\] to fill \[Y,c,e\], and thencombine \[X,a,b\] and \[Y,c,e\].
The potential confu-sion stems from the fact that an underparsed seg-ment is part of the description, but is not a properconstituent of the tree.This problem can be avoided in several ways.
Anobvious one is to only permit underparsings to beadded to partial descriptions on the right side.
Oneexception would then have to be made to permit in-put segments prior to any parsed input segments tobe underparsed (i.e., if the first input segment is un-derparsed, it has to be attached to the left side ofsome constituent because it is to the left of every-thing in the description).1068 Conclus ionsThe results presented here demonstrate that thebasic cubic time complexity results for processingcontext-free structures are preserved when Optimal-ity Theory grammars are used.
If Gen can be speci-fied as matching input segments to structures gener-ated by a context-free position structure grammar,and the constraints are local with respect o thosestructures, then the algorithm presented here maybe applied directly to compute optimal descriptions.9 AcknowledgmentsI would like to thank Paul Smolensky for his valu-able contributions and support.
I would also like tothank David I-Iaussler, Clayton Lewis, Mark Liber-man, Jim Martin, and Alan Prince for useful dis-cussions, and three anonymous reviewers for helpfulcomments.
This work was supported in part by anNSF Graduate Fellowship to the author, and NSFgrant IRI-9213894 to Paul Smolensky and GeraldineLegendre.Bruce Tesar.
1994.
Parsing in Optimality Theory:A dynamic programming approach.
Technical Re-port CU-CS-714-94, April 1994.
Department ofComputer Science, University of Colorado, Boul-der.Bruce Tesar.
1995a.
Computing optimal forms inOptimality Theory: Basic syllabification.
Tech-nical Report CU-CS-763-95, February 1995.
De-partment of Computer Science, University of Col-orado, Boulder.Bruce Tesar.
1995b.
Computational Optimality The-ory.
Unpublished Ph.D. Dissertation.
Departmentof Computer Science, University of Colorado,Boulder.
June 1995.A.J.
Viterbi.
1967.
Error bounds for convolutioncodes and an asymptotically optimal decodingalgorithm.
IEEE Trans.
on Information Theory13:260-269.Re ferencesS.
O. Anderson and R. C. Backhouse.
1981.
Lo-cally least-cost error recovery in Earley's algo-rithm.
A CM Transactions on Programming Lan-guages and Systems 3: 318-347.Eugene Charniak.
1993.
Statistical anguage learn-ing.
Cambridge, MA: MIT Press.Martin Kay.
1980.
Algorithmic schemata nd datastructures in syntactic processing.
CSL-80-12, Oc-tober 1980.K.
Lari and S. J.
Young.
1990.
The estimation ofstochastic context-free grammars using the inside-outside algorithm.
Computer Speech and Lan-guage 4: 35-36.Harry R. Lewis and Christos H. Papadimitriou.1981.
Elements of the theory of computation.
En-glewood Cliffs, New Jersey: Prentice-Hall, Inc.Alan Prince and Paul Smolensky.
1993.
Optimal-ity Theory: Constraint interaction in generativegrammar.
Technical Report CU-CS-696-93, De-partment of Computer Science, University of Col-orado at Boulder, and Technical Report TR-2,Rutgers Center for Cognitive Science, RutgersUniversity, New Brunswick, NJ.
March.
To ap-pear in the Linguistic Inquiry Monograph Series,Cambridge, MA: MIT Press.David Sankoff and Joseph Kruskal.
1983.
Timewarps, string edits, and macromolecules: The the-ory and practice of sequence comparison.
Reading,MA: Addison-Wesley.107
