Proceedings of the Demonstrations at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 77?80,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAnnotating by Proving using SemAnTEAssaf Toledo1Stavroula Alexandropoulou1Sophie Chesney2Robert Grimm1Pepijn Kokke1Benno Kruit3Kyriaki Neophytou1Antony Nguyen11- Utrecht University2- University College London3- University of Amsterdam{a.toledo,s.alexandropoulou,y.winter}@uu.nlsophie.chesney.10@ucl.ac.uk, {pepijn.kokke,bennokr}@gmail.com{r.m.grimm,k.neophytou,a.h.nguyen}@students.uu.nlYoad Winter1AbstractWe present SemAnTE, a platform formarking and substantiating a semantic an-notation scheme of textual entailment ac-cording to a formal model.
The plat-form introduces a novel approach to an-notation by providing annotators immedi-ate feedback whether the data they markare substantiated: for positive entailmentpairs, the system uses the annotations tosearch for a formal logical proof that val-idates the entailment relation; for negativepairs, the system verifies that a counter-model can be constructed.
By integratinga web-based user-interface, a formal lexi-con, a lambda-calculus engine and an off-the-shelf theorem prover, this platform fa-cilitates the creation of annotated corporaof textual entailment.
A corpus of severalhundred annotated entailments is currentlyin preparation using the platform and willbe available for the research community.1 IntroductionThe Recognizing Textual Entailment (RTE) chal-lenges (Dagan et al., 2006) advance the devel-opment of systems that automatically determinewhether an entailment relation obtains between anaturally occurring text T and a manually com-posed hypothesis H. The RTE corpus (Bar Haimet al., 2006; Giampiccolo et al., 2008), which iscurrently the only available resource of textual en-tailments, marks entailment candidates as posi-tive/negative.1For example:Example 1?
T: The book contains short stories by the fa-mous Bulgarian writer, Nikolai Haitov.?
H: Nikolai Haitov is a writer.2?
Entailment: PositiveThis categorization does not indicate the linguisticphenomena that underlie entailment or their con-tribution to inferential processes.
In default ofa gold standard identifying linguistic phenomenatriggering inferences, entailment systems can becompared based on their performance, but the in-ferential processes they employ to recognize en-tailment are not directly accessible and conse-quently cannot be either evaluated or improvedstraightforwardly.We address this problem by elucidating someof the central inferential processes underlying en-tailments in the RTE corpus, which we model for-mally within a standard semantic theory.
This al-lows us not only to indicate linguistic phenomenathat are involved in the recognition of entailmentby speakers, but also to provide formal proofs thatsubstantiate the annotations and explain how the1Pairs of sentences in RTE 1-3 are categorized in twoclasses: yes- or no-entailment; pairs in RTE 4-5 are cate-gorized in three classes: entailment, contradiction and un-known.
We label the judgments yes-entailment from RTE 1-3and entailment from RTE 4-5 as positive, and the other judg-ments as negative.2Pair 622 from the development set of RTE 2.77modeled phenomena interact and contribute to therecognition process.
In this sense the we adopt anAnnotating by Proving approach to textual entail-ment annotation.The annotation work is done using the Se-mAnTE (Semantic Annotation of Textual Entail-ment) platform, which incorporates a web-baseduser-interface, a formal lexicon, a lambda-calculusengine and an off-the-shelf theorem prover.
Weare currently using this platform to build a newcorpus of several hundred annotated entailmentscomprising both positive and negative pairs.
Wedecided to focus on the semantic phenomena ofappositive, restrictive and intersective modifica-tion as these semantic phenomena are prevalent inthe RTE datasets and can be annotated with highconsistency, and as their various syntactic expres-sions can be captured by a limited set of concepts.3In the future, we plan to extend this sematic modelto cover other, more complex phenomena.2 Semantic ModelTo model entailment in natural language, we as-sume that entailment describes a preorder on sen-tences.
Thus, any sentence trivially entails itself(reflexivity); and given two entailments T1?
H1and T2?
H2where H1and T2are identical sen-tences, we assume T1?
H2(transitivity).
Weuse a standard model-theoretical extensional se-mantics, based on the simple partial order on thedomain of truth-values.
Each model M assignssentences a truth-value in the set {0, 1}.
Such aTarskian theory of entailment is considered ade-quate if the intuitive entailment preorder on sen-tences can be described as the pairs of sentencesT and H whose truth-values [[T]]Mand [[H]]Msat-isfy [[T]]M?
[[H]]Mfor all models M .We use annotations to link between textualrepresentations in natural language and model-theoretic representations.
This link is establishedby marking the words and structural configura-tions in T and H with lexical items that encodesemantic meanings for the linguistic phenomenathat we model.
The lexical items are defined for-mally in a lexicon, as illustrated in Table 1 for ma-jor lexical categories over type:s e for entities, tfor truth-values, and their functional compounds.3This conclusion is based on an analysis of RTE 1-4, inwhich these modification phenomena were found to occur in80.65% of the entailments and were annotated with cross-annotator agreement of 68% on average.Category Type Example DenotationProper Name e Dan danIndef.
Article (et)(et) a ADef.
Article (et)e the ?Copula (et)(et) is ISNoun et book bookIntrans.
verb et sit sitTrans.
verb eet contain containPred.
Conj.
(et)((et)(et)) and ANDRes.
Adj.
(et)(et) short Rm(short)Exist.
Quant.
(et)(et)t some SOMETable 1: Lexicon IllustrationDenotations that are assumed to be arbitrary aregiven in boldface.
For example, the intransitiveverb sit is assigned the type et, which describesfunctions from entities to a truth-values, and itsdenotation sit is an arbitrary function of this type.By contrast, other lexical items have their denota-tions restricted by the given model M .
As illus-trated in Figure 1, the coordinator and is assignedthe type (et)((et)(et)) and its denotation is a func-tion that takes a function A of type et and returnsa function that takes a function B, also of type et,and returns a function that takes an entity x andreturns 1 if and only if x satisfies both A and B.A = IS = ?Aet.A?
= ?Aet.
{a A = (?xe.x = a)undefined otherwiseWHOA= ?Aet.?xe.?
(?y.y = x ?A(x))Rm= ?M(et)(et).?Aet.
?xe.M(A)(x) ?A(x)SOME = ?Aet.?Bet.
?x.A(x) ?B(x)AND = ?Aet.?Bet.
?xe.A(x) ?B(x)Figure 1: Functions in the LexiconBy marking words and syntactic constructionswith lexical items, annotators indicate the under-lying linguistic phenomena in the data.
Further-more, the formal foundation of this approach al-lows annotators to verify that the entailment re-lation (or lack thereof) that obtains between thetextual forms of T and H also obtains betweentheir respective semantic forms.
This verificationguarantees that the annotations are sufficient in thesense of providing enough information for recog-nizing the entailment relation based on the seman-tic abstraction.
For example, consider the simpleentailment Dan sat and sang?Dan sang and as-sume annotations of Dan as a proper name, satand sang as intransitive verbs and and as predi-cate conjunction.
The formal model can be usedto verify these annotations by constructing a proofas follows: for each model M :78[[Dan [sat [and sang]] ]]M= ((AND(sing))(sit))(dan) analysis= (((?Aet.?Bet.
?xe.A(x) ?B(x))(sing))(sit))(dan)def.
of AND= sit(dan) ?
sing(dan) func.
app.
to sing,sit and dan?
sing(dan) def.
of ?= [[Dan sang ]]Manalysis3 Platform ArchitectureThe platform?s architecture is based on a client-server model, as illustrated in Figure 2.Figure 2: Platform ArchitectureThe user interface (UI) is implemented as aweb-based client using Google Web Toolkit (Ol-son, 2007) and allows multiple annotators to ac-cess the RTE data, to annotate, and to substanti-ate their annotations.
These operations are doneby invoking corresponding remote procedure callsat the server side.
Below we describe the systemcomponents as we go over the work-flow of anno-tating Example 1.Data Preparation: We extract T -H pairs fromthe RTE datasets XML files and use the StanfordCoreNLP (Klein and Manning, 2003; Toutanovaet al., 2003; de Marneffe et al., 2006) to parse eachpair and to annotate it with part-of-speech tags.4Consequently, we apply a naive heuristic to mapthe PoS tags to the lexicon.5This process is called4Version 1.3.45This heuristic is naive in the sense of not disambiguatingverbs, adjectives and other types of terms according to theirsemantic features.
It is meant to provide a starting point forthe annotators to correct and fine-tune.as part of the platform?s installation and when an-notators need to simplify the original RTE data inorder to avoid syntactic/semantic phenomena thatthe semantic engine does not support.
For exam-ple, the bare plural short stories is simplified tosome short stories as otherwise the engine is un-able to determine the quantification of this noun.Annotation: The annotation is done by mark-ing the tree-leaves with entries from the lexicon.For example, short is annotated as a restrictivemodifier (MR) of the noun stories, and containsis annotated as a transitive verb (V 2).
In addition,annotators manipulate the tree structure to fix pars-ing mistakes and to add leaves that mark semanticrelations.
For instance, a leaf that indicates the ap-position between the famous Bulgarian writer andNikolai Haitov is added and annotated as WHOA.The server stores a list of all annotation actions.Figure 3 shows the tree-view, lexicon, prover andannotation history panels in the UI.Proving: When annotating all leaves and ma-nipulating the tree structures of T and H are done,the annotators use the prover interface to requesta search for a proof that indicates that their anno-tations are substantiated.
Firstly, the system useslambda calculus reductions to create logical formsthat represent the meanings of T and H in higher-order logic.
At this stage, type errors may be re-ported due to erroneous parse-trees or annotations.In this case an annotator will fix the errors and re-run the proving step.
Secondly, once all type er-rors are resolved, the higher-order representationsare lowered to first order and Prover9 (McCune,2010) is executed to search for a proof betweenthe logical expressions of T and H .6The proofsare recorded in order to be included in the corpusrelease.
Figure 4 shows the result of translating Tand H to an input to Prover9.4 Corpus PreparationWe have so far completed annotating 40 positiveentailments based on data from RTE 1-4.
The an-notation is a work in progress, done by four Masterstudents of Linguistics who are experts in the dataand focus on entailments whose recognition re-lies on a mixture of appositive, restrictive or inter-sective modification.
As we progress towards thecompilation of a corpus of several hundred pairs,we extend the semantic model to support more in-ferences with less phenomena simplification.6Version 2009-11A79Figure 3: User Interface Panels: Annotation History, Tree-View, Prover Interface and Lexicon Toolboxformulas(assumptions).all x0 (((writer(x0) & bulgarian(x0)) &famous writer bulgarian(x0))?
x0=c1).all x0 (((stories(x0) & short stories(x0)) & exists x1 (bystories short stories(x1, x0) & (x1=c1 & x1=NikolaiHaitov)))?
x0=c2).
all x0 (book(x0)?
x0=c3).contains(c2, c3).end of list.formulas(goals).exists x0 (writer(x0) & x0=Nikolai Haitov).end of list.Figure 4: Input for Theorem Prover5 ConclusionsWe introduced a new concept of an annotationplatform which implements an Annotating byProving approach.
The platform is currently inuse by annotators to indicate linguistic phenomenain entailment data and to provide logical proofsthat substantiate their annotations.
This methodguarantees that the annotations constitute a com-plete description of the entailment relation and canserve as a gold-standard for entailment recogniz-ers.
The new corpus will be publicly available.AcknowledgmentsThe work of Stavroula Alexandropoulou, RobertGrimm, Sophie Chesney, Pepijn Kokke, BennoKruit, Kyriaki Neophytou, Antony Nguyen, AssafToledo and Yoad Winter was supported by a VICIgrant number 277-80-002 by the Netherlands Or-ganisation for Scientific Research (NWO).ReferencesRoy Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro,Danilo Giampiccolo, Bernardo Magnini, and IdanSzpektor.
2006.
The second pascal recognisingtextual entailment challenge.
In Proceedings of theSecond PASCAL Challenges Workshop on Recognis-ing Textual Entailment.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
Machine Learning Challenges.
Evaluat-ing Predictive Uncertainty, Visual Object Classifi-cation, and Recognising Tectual Entailment, pages177?190.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the IEEE / ACL 2006 Workshop onSpoken Language Technology.
The Stanford NaturalLanguage Processing Group.Danilo Giampiccolo, Hoa Trang Dang, BernardoMagnini, Ido Dagan, and Elena Cabrio.
2008.
Thefourth pascal recognising textual entailment chal-lenge.
In TAC 2008 Proceedings.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics - Volume 1, ACL ?03, pages 423?430, Stroudsburg, PA, USA.
ACL.William McCune.
2010.
Prover9 and Mace4.
http://www.cs.unm.edu/?mccune/prover9/.Steven Douglas Olson.
2007.
Ajax on Java.
O?ReillyMedia.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology- Volume 1, NAACL ?03, pages 173?180, Strouds-burg, PA, USA.
ACL.80
