Lenient Default Unification for Robust Processingwithin Unification Based Grammar FormalismsTakashi NINOMIYA,??
Yusuke MIYAO,?
and Jun?ichi TSUJII???
Department of Computer Science, University of Tokyo?
CREST, Japan Science and Technology Corporatione-mail: {ninomi, yusuke, tsujii}@is.s.u-tokyo.ac.jpAbstractThis paper describes new default unification, lenientdefault unification.
It works efficiently, and givesmore informative results because it maximizes theamount of information in the result, while other de-fault unification maximizes it in the default.
We alsodescribe robust processing within the framework ofHPSG.
We extract grammar rules from the results ofrobust parsing using lenient default unification.
Theresults of a series of experiments show that parsingwith the extracted rules works robustly, and the cov-erage of a manually-developed HPSG grammar forPenn Treebank was greatly increased with a littleovergeneration.1 IntroductionParsing has often been considered to be crucialfor natural language processing, thus, efficient andwide coverage parsing has been extensively pur-sued in natural language literature.
This study aimsat robust processing within the Head-driven PhraseStructure Grammar (HPSG) to extend the cover-age of manually-developed HPSG grammars.
Themeaning of ?robust processing?
is not limited to ro-bust processing for ill-formed sentences found ina spoken language, but includes robust processingfor sentences which are well-formed but beyond thegrammar writer?s expectation.Studies of robust parsing within unification-basedgrammars have been explored by many researchers(Douglas and Dale, 1992; Imaichi and Matsumoto,1995).
They classified the errors found in analyzingill-formed sentences into several categories to makethem tractable, e.g., constraint violation, missing orextra elements, etc.
In this paper, we focus on re-covery from the constraint violation errors, which isa violation of feature values.
All errors in agreementfall into this category.
Since many of the grammat-ical components in HPSG are written as constraintsrepresented by feature structures, many of the er-rors are expected to be recovered by the recovery ofconstraint violation errors.This paper proposes two new types of defaultunification and describes their application to robustprocessing.
Default unification was originally stud-ied to develop a system of lexical semantics to dealwith the default inheritance in a lexicon, but it isalso desirable for the recovery of such constraint vi-olation errors due to the following merits: i) defaultunification is always well-defined, and ii) a featurestructure is relaxed such that the amount of infor-mation is maximized.
From the viewpoint of robustprocessing, an amount of lost information can be re-garded as a cost (i.e., penalty) of robust processing.In other words, default unification tries to minimizethe cost.
Given a strict feature structure F and adefault feature structure G, default unification is de-fined as unification that satisfies the following (writ-ten as F <unionsq G): 1) It is always defined.
2) All strictinformation is preserved.
That is, F v (F <unionsq G).
3) Itreduces to standard unification in the case of F andG being consistent.
That is, (F <unionsq G) = (F unionsqG) ifF unionsqG is defined.
With these definitions, Douglas?relaxation technique can be regarded as a sort of de-fault unification.
They classify constraints into nec-essary constraints and optional constraints, whichcan be regarded as strict information and default in-formation in the definition of default unification.Carpenter (1993) gave concise and comprehen-sive definitions of default unification.
However, theproblem in Carpenter?s default unification is that ittries to maximize the amount of information in a de-fault feature structure, not the result of default uni-fication.
Consider the case where a grammar ruleis the default feature structure and the daughters arethe strict feature structure.
The head feature prin-ciple can be described as the structure-sharing be-tween the values of the head feature in a motherand in a head daughter.
The set of constraints thatrepresent the head feature principle consists of onlyone element.
When we lose just one element in thehead feature principle, a large amount of informa-tion in the daughter?s substructure is not propagatedto its mother.
As Copestake (1993) mentioned, an-other problem in Carpenter?s default unification isthat the time complexity for finding the optimal an-swer of default unification is exponential becausewe have to verify the unifiability of the power set ofconstraints in a default feature structure.Here, we propose ideal lenient default unifica-tion, which tries to maximize the amount of infor-mation of a result, not the amount of default infor-mation.
Thus, the problem of losing a large amountof information in structure-sharing never arises.
Wealso propose lenient default unification whose algo-rithm is much more efficient than the ideal one.
Itstime complexity is linear to the size of the strict fea-ture structure and the default feature structure.
In-stead, the amount of information of a result derivedby lenient default unification is equal to or less thanthat of the ideal one.We apply lenient default unification to robust pro-cessing.
Given an HPSG grammar, our approachtakes two steps; i) extraction of grammar rules fromthe results of robust parsing using lenient defaultunification for applying the HPSG grammar rules(offline parsing), and ii) runtime parsing using theHPSG grammar with the extracted rules.
The ex-tracted rules work robustly since they reflect the ef-fects of recovery rules applied during offline robustparsing and the conditions in which they are ap-plied.Sections 3 and 4 describe our default unification.Our robust parsing is explained in Section 5.
Sec-tion 6 shows a series of experiments of robust pars-ing with default unification.2 BackgroundDefault unification has been investigated by manyresearchers (Bouma, 1990; Russell et al, 1991;Copestake, 1993; Carpenter, 1993; Lascarides andCopestake, 1999) in the context of developing lexi-cal semantics.
Here, we first explain the definitiongiven by Carpenter (1993) because his definition isboth concise and comprehensive.2.1 Carpenter?s Default UnificationCarpenter proposed two types of default unification,credulous default unification and skeptical defaultunification.
(Credulous Default Unification)F <unionsqc G ={F unionsqG????
G?
v G is maximal such thatF unionsqG?
is defined}(Skeptical Default Unification)F <unionsqs G = ?
(F <unionsqc G)F is called a strict feature structure, whose in-formation must not be lost, and G is called a de-fault feature structure, whose information might belost but as little as possible so that F and G can beunified.
A credulous default unification operationis greedy in that it tries to maximize the amount ofinformation it retains from the default feature struc-ture.
This definition returns a set of feature struc-tures rather than a unique feature structure.Skeptical default unification simply generalizesthe set of feature structures which results from cred-ulous default unification.
The definition of skepticaldefault unification leads to a unique result.
The de-fault information which can be found in every resultof credulous default unification remains.
Followingis an example of skeptical default unification.
[F: a] <unionsqs[F: 1 bG: 1H: c]= u{ [F: aG: bH: c],[F: 1 aG: 1H: c]}=[F: aG: ?H: c]2.2 Forced UnificationForced unification is another way to unify incon-sistent feature structures.
Forced unification alwayssucceeds by supposing the existence of the top type(the most specific type) in a type hierarchy.
Unifi-cation of any pair of types is defined in the type hi-erarchy, and therefore unification of any pair of fea-ture structures is defined.
One example is describedby Imaichi and Matsumoto (1995) (they call it cost-based unification).
Their unification always suc-ceeds by supposing the top type, and it also keepsthe information about inconsistent types.
Forcedunification can be regarded as one of the toughestrobust processing because it always succeeds andnever loses the information embedded in featurestructures.
The drawback of forced unification isthe postprocessing of parsing, i.e., feature structureswith top types are not tractable.
We write Funionsq f G forthe forced unification of F and G.3 Ideal Lenient Default UnificationIn this section, we explain our default unification,ideal lenient default unification.
Ideal lenient de-fault unification tries to maximize the amount ofinformation of the result, subsuming the result offorced unification.
In other words, ideal lenient de-fault unification tries to generate a result as similaras possible to the result of forced unification suchthat the result is defined in the type hierarchy with-out the top type.
Formally, we have:Definition 3.1 Ideal Lenient Default UnificationF <unionsqi G = ?
{F unionsqG??????G?
v f (Funionsq f G) is maximalsuch that F unionsqG?
is definedwithout the top type}where v f is a subsumption relation where the toptype is defined.From the definition of skeptical default unifica-tion, ideal lenient default unification is equivalentto F <unionsqs (Funionsq f G) assuming that skeptical default uni-fication does not add the default information that in-cludes the top type to the strict information.Consider the following feature structures.F =????F:[F:aG:bH:c]G:[F:aG:aH:c]???
?,G =[F: 1G: 1]In the case of Carpenter?s default unification, theresults of skeptical and credulous default unificationbecome as follows: F <unionsqs G = F,F <unionsqc G = {F}.
Thisis because G is generalized to the bottom featurestructure, and hence the result is equivalent to thestrict feature structure.With ideal lenient default unification, the resultbecomes as follows.F <unionsqi G =?????
?F:[F: 1 aG:bH: 2 c]G:[F: 1G:aH: 2]?????
?v f?
?F: 1[F:aG:>H:c]G: 1?
?Note that the result of ideal lenient default unifica-tion subsumes the result of forced unification.As we can see in the example, ideal lenient de-fault unification tries to keep as much informationof the structure-sharing as possible (ideal lenientdefault unification succeeds in preserving the struc-ture-sharing tagged as 1 and 2 though skeptical andcredulous default unification fail to capture it).4 Lenient Default UnificationThe optimal answer for ideal lenient default unifica-tion can be found by calculating F <unionsqs (Funionsq f G).
AsCopestake (1993) mentioned, the time complexityof skeptical default unification is exponential, andtherefore the time complexity of ideal lenient de-fault unification is also exponential.As other researchers pursued efficient default uni-fication (Bouma, 1990; Russell et al, 1991; Copes-take, 1993), we also propose another definition ofdefault unification, which we call lenient defaultunification.
An algorithm derived for it finds its an-swer efficiently.Given a strict feature structure F and a defaultfeature structure G, let H be the result of forced uni-fication, i.e., H = Funionsq f G. We define topnode(H)as a function that returns the fail points (the nodesthat are assigned the top type in H), f pnode(H)as a function that returns the fail path nodes (thenodes from which a fail point can be reached), andf pchild(H) as a a function that returns all the nodesthat are not fail path nodes but the immediate chil-dren of fail path nodes.Consider the following feature structures.F =?????F:F:[F:F:aG:G:bH:H:c]G:[G:[F:F:aG:G:aH:H:c]H:H:a]????
?,G =[F:F: 1?G:G: 1]Figure 1 shows F , G and H in the graph notation.This figure also shows the nodes that correspond totopnode(H), f pnode(H) and f pchild(H).   F = G =	H =	)(Htopnode?
)(Hfpnode? )(Hfpchild?fffigeneralize(H) = fifffiffff fiflffffFigure 1: F , G and H in the graph notationF unionsqH fails because some of the path value in Hconflict with F , or some of the path equivalence inH cause inconsistencies.
The basic ideas are thati) the inconsistency caused by path value specifica-tions can be removed by generalizing the types as-signed to the fail points in H, and that ii) the incon-sistency caused by path equivalence specificationscan be removed by unfolding the structure-sharingof fail path nodes in H.Let H be ?QH , q?H ,?H ,?H?, where Q is a set of afeature structure?s nodes, q?
is the root node, ?
(q) isa total node typing function, and ?
(pi,q) is a partialfunction that returns a node reached by followingpath pi from q.
We first give several definitions todefine a generalized feature structure.Definition 4.1toppath(H) = {pi|?q ?
topnode(H).
(q = ?H(pi, q?H))}f ail path(H) = {pi|?q ?
f pnode(H).
(q = ?H(pi, q?H))}ss(H) = {pi|?q ?
f pchild(H).
(q = ?H(pi, q?H))}I(H) =?
({F |?pi ?
toppath(H).
(F = PV (pi,?))})I?
(H) =?({F????
?pi ?
f ail path(H).
(F = PV (pi,?H(?H(pi, q?H))))})I??
(H) = I(H)unionsq I?
(H)PV (pi,?)
={the least feature structures wherepath value of pi is ?Let I(= ?QI, q?I,?I,?I?)
be I??(H).
The definitionof the generalized feature structure is given as fol-lows:Definition 4.2 (Generalization of H)generalize(H) = ?QH ?
, q?I ,?H ?
,?H ??
whereQH ?
= QH ?QI?H ?
(q) ={?H(q) if q ?
QH?I(q) if q ?
QI?H ?
( f ,q) ={?H( f ,q) if q ?
QH?I( f ,q) if q ?
QI?H ?
(pi, q?I) = ?
(pi, q?H) for all pi ?
ss(H)procedure generalize-sub(H,q)create a new state q?
?
QH ;?H (q?)
= ?H (q);if q ?
topnode(H) then?H (q?)
:= ?
;(?
is an appropriate typefor all the arcs that reach qH)return q?
;forall f ?
{ f |?H ( f ,q) is defined} dor = ?
( f ,q);if r ?
f pnode(H) then?
( f ,q?)
= generalize-sub(H,r);else?
( f ,q?)
= r;end-ifend-forallreturn q?
;procedure generalize(H = ?QH , q?H ,?H ,?H ?
)q = generalize-sub(H, q?H );return ?QH ,q,?H ,?H ?
;Figure 2: An algorithm that makes H more generalFigure 1 also shows the result of generalize(Funionsq f G).Finally, lenient default unification F <unionsq G is de-fined as follows:Definition 4.3 (Lenient Default Unification)F <unionsq G = F unionsqgeneralize(Funionsq f G)For F and G depicted in Figure 1, F <unionsq G becomes asfollows:F <unionsq G =??????
?F:F:[F: 1 aG:G:bH: 2 c]G:?
?G:[F: 1G:G:aH: 2]H:H:a????????
?Both ideal and non-ideal lenient default unifica-tion satisfy the following desiderata: 1) It is alwaysdefined (and produces a unique result).
2) All strictinformation is preserved.
That is, F v (F <unionsq G) v f(Funionsq f G).
3) F <unionsq G is defined without the top type.
4)It reduces to standard unification in the case F andG are consistent (unifiable).
That is, F <unionsq G = F unionsqGif F unionsqG is defined.AlgorithmOur algorithm for lenient default unification pro-ceeds in the following steps.
1) Calculate forcedunification of F and G (let H be Funionsq f G).
2) Findfail points and fail path nodes in H. 3) GeneralizeH so that F unionsqH can be unified.Figure 2 describes the algorithm that generalizesthe result of forced unification.1 The time complex-ity of the algorithm for finding F <unionsq G is linear to1In this paper, we assume acyclic feature structures.
Ouralgorithm never stops if a cyclic part in a feature structure is tobe generalized.
Acyclicity is easily enforced by requiring thatno path has a proper extension that leads to the same node.
Wealso assume outputs of default unification are not necessarilytotally-well typed since constraints of appropriateness condi-tions of types can propagate a node to its subnodes, and thisbehavior makes the definitions of default unification complex.Instead, totally-well typedness can be easily enforced by thetotal type inference function.the size of feature structures F and G because thetime complexity of each algorithm (the algorithmfor finding fail points, finding fail path nodes, andgeneralization) is linear to their size.ComparisonThe difference between ideal lenient default unifi-cation and lenient default unification can be exem-plified by the following example.
(skeptical default unification)[F: aG: aH: b]<unionsqs[F: 1G: 1H: 1]=[F: 2 aG: 2H: b](ideal lenient default unification)[F: aG: aH: b]<unionsqi[F: 1G: 1H: 1]=[F: 2 aG: 2H: b](lenient default unification)[F: aG: aH: b]<unionsq[F: 1G: 1H: 1]=[F: aG: aH: b]In the example above, the results of ideal lenientdefault unification and skeptical default unificationare the same.
In the case of lenient default unifi-cation, all the information embedded in the defaultis removed because all structure-sharings tagged as1 are on the paths that lead to the fail points in theresult of forced unification.
Lenient default unifica-tion is much more suspicious in removing informa-tion from the default than the ideal one.
Lenientdefault unification may remove structure-sharingsthat are irrelevant to unification failure.
Another de-fect of lenient default unification is that the bottomtype is assigned to nodes that correspond to the failpoints in the result of forced unification.
The typeassigned to their nodes should be more specific thanthe bottom type as the bottom type has no feature,i.e., all the arcs that go out from the fail point arecut.Though lenient default unification seems to havemany defects, lenient default unification has the ad-vantage of efficiency.
As we are thinking to use de-fault unification for practical robust processing, theefficiency is of great importance.
Furthermore, theresult of lenient default unification can be more in-formative than that of skeptical default unification inmany cases of practical applications.
For example,suppose that the grammar rule R and the daughtersDT R are given as follows.R =?
?MOTHER:HEAD: 1DTRS:[H: 2 [HEAD: 1 head]NH:SPR: 2]?
?DT R =[DTRS:[H:HEAD:CASE: objNH:SPR:HEAD:CASE: nom]]Suppose also that the type head has PHON:, CASE:,INV: and TENSE: as its features, and the type sign hasHEAD: and VAL:.
The result of skeptical default uni-fication DT R <unionsqs R becomes DT R. This is becauseall structure-sharings embedded in R are relevant tounification failure.
However, the result of lenientdefault unification is more informative.DT R <unionsqi R = DT R <unionsq R =??????????????????????????MOTHER:HEAD:???
?headPHON: 3CASE: ?INV: 4TENSE: 5????DTRS:??????????????????H:???????signHEAD:???
?headPHON: 3CASE: objINV: 4TENSE: 5???
?VAL: 6???????NH:SPR:??????signHEAD:??
?headPHON: 3CASE: nomINV: 4TENSE: 5??
?VAL: 6?????????????????????????????????????????????????
?The information of structure-sharing is preservedas much as possible.
In the example above,the structure-sharing tagged as 2 in the originalgrammar rule R is decomposed into the structure-sharings 3 , 4 , 5 , 6 .
That is, the structure-sharingtagged as 2 is preserved except HEAD:CASE:.5 Offline Robust Parsing and GrammarExtractionThis section describes a new approach to robustparsing using default unification.
Given an HPSGgrammar, our approach takes two steps; i) extractionof grammar rules from the result of offline robustparsing using default unification for applying theHPSG grammar rules, and ii) runtime parsing usingthe HPSG grammar with the extracted rules.
Offlineparsing is a training phase to extract grammar rules,and runtime parsing is a phase where we apply theextracted rules to practice.
The extracted rules workrobustly over corpora other than the training corpusbecause the extracted rules reflect the effects of de-fault unification that are applied during offline pars-ing.
Given an annotated corpus, our algorithm ex-tracts grammar rules that make the coverage of theHPSG grammar wider.In the offline parsing, constituents are generatedby default unification of daughters and grammarrules of the HPSG grammar2, where a head daugh-ter and a grammar rule are strict feature structuresand a non-head daughter is a default feature struc-ture.
With this construction, the information in agrammar rule and a head daughter is strictly pre-served and the information in a non-head daughteris partially lost (but, as little as possible).
The ideas2In HPSG, both constituents and grammar rules are repre-sented by feature structures.behind this construction are that (i) we had betterconstruct a mother node without the information ofthe non-head daughter rather than construct noth-ing (i.e., we had better construct a mother node byunifying only a head-daughter and a grammar rule),(ii) we had better construct a mother node with themaximal information of a non-head daughter ratherthan have no information of the non-head daughteradded.
Parse trees can be derived even if a parse treecannot be derived by normal unification.Offline robust parsing is based on A* algorithm,but we generate only parse trees which meet the fol-lowing conditions, 1) a generated parse tree mustbe consistent with an existing bracketed corpus, and2) the parsing cost of a generated parse tree mustbe minimum.
This means that i) we can limit asearch space, and that ii) the parsing result is valid inthe sense that it is consistent with the existing cor-pus.
The cost of a parse tree can be calculated byadding the cost of lenient default unification, whichis the amount of information that is lost by lenientdefault unification.
We regard it as the differencebetween the number of path values and structure-sharing in the results of a lenient default unificationand a forced unification.Grammar extraction is very concise.
When wefind a mother M in the result of offline parsing thatcannot be derived by using unification but can bederived by default unification, we regard M ?
L,Ras a new rule, where L and R are the daughtersof the mother.
The rules extracted in such a waycan reconstruct the mothers as does default uni-fication, and they reflect the condition of trigger-ing default unification, i.e., the extracted rules arenot frequently triggered because they can be ap-plied to feature structures that are exactly equiva-lent to their daughter?s part.
By collecting a numberof such rules,3 a grammar becomes wide-coveragewith some overgeneration.
They can be regardedas exceptions in a grammar, which are difficult tobe captured only by propagating information fromdaughters to a mother.This approach can be regarded as a kindof explanation-based learning (Samuelsson andRayner, 1991).
The explanation-based learningmethod is recently attracting researcher?s attention(Xia, 1999; Chiang, 2000) because their parsers arecomparative to the state-of-the-art parsers in termsof precision and recall.
In the context of unification-based grammars, Neumann (1994) has developed aparser running with an HPSG grammar learned byexplanation-based learning.
It should be also notedthat Kiyono and Tsujii (1993) exemplified the gram-mar extraction approach using offline parsing in the3Although the size of the grammar becomes very large, theextracted rules can be found by a hash algorithm very effi-ciently.
This tractability helps to use this approach in practicalapplications.Training Test Set Test SetCorpus A B# of sentences 5,903 1,480 100Avg.
length 23.59 23.93 6.63of sentencesTable 1: Corpus size and average length of sen-tences                                     	    	        Figure 4: The average number of edges whenTestSetB was parsedcontext of explanation-based learning.Finally, we need to remove some values in the ex-tracted rules because they contain too specific infor-mation.
For instance, a value of PHONOLOGY: repre-sents a list of phoneme strings of a phrasal structure.Without removing them, extracted rules cannot betriggered until when completely the same strings ap-pear in a text.46 Performance EvaluationWe measured the performance of our robust pars-ing algorithm by measuring coverage and degreeof overgeneration for the Wall Street Journal in thePenn Treebank (Marcus et al, 1993).
The trainingcorpus consists of 5,903 sentences selected from theWall Street Journal (Wall Street Journal 00 ?
02),and we prepared two sets of test corpora, TestSetAand TestSetB.
TestSetA consists of 1,480 sentences(Wall Street Journal 03) and is used for measur-ing coverage.5 TestSetB consists of 100 sentencesand is used for measuring the degree of overgenera-tion.
The sentences of TestSetB are the shortest 100sentences in TestSetA.
Table 1 shows the averagesentence length of each corpus.
Here, ?coverage?means the ratio of ?the number of sentences that arecovered by a grammar?
to ?the number of all sen-tences?.
Here, we say ?a sentence is covered?
whena sentence can be analyzed by a parser and the resultincludes trees that are consistent with brackets andPOS tags annotated in the Penn Treebank.Grammar rules were extracted by offline pars-ing with the XHPSG grammar (Tateisi et al, 1998),4In the experiment, we removed the values that correspondto the phoneme strings of phrasal structures, some of syntacticconstraints, and semantics of phrasal structures5There is no overlap between the training and test corpus.Phenomena (A) (B) (C) (D) (%)lack of lexical entry 118 32 86 72.9inconsistency between XHPSG and Penn Tree-bank44 13 31 70.5punctuation, quotation, parenthesis 36 15 21 58.3coordination 21 8 13 61.9apposition 16 6 10 62.5compound noun/adjective/adverb 14 3 11 78.6Adv modifying PP 12 2 10 83.3relative clause 12 5 7 58.3topicalization 11 0 11 100.0noun modifier 10 2 8 80.0omission 8 2 6 75.0parenthetical expression 7 3 4 57.1verb saying 7 1 6 85.7expression of frequency, NP + a + N 7 0 7 100.0present participle construction 6 3 3 50.0idiom 5 2 3 60.0violation of agreement 3 0 3 100.0adverbial noun 3 0 3 100.0present progressive, be + Adv + present pro-gressive3 1 2 66.7sentence modification from the beginning of asentence2 0 2 100.0be + complement sentence 2 0 2 100.0nominalization of adjective 1 0 1 100.0double numerals (NP + roughly + double + NP) 1 1 0 0.0total 349 99 250 71.6(A) ... frequency of phenomena that the XHPSG grammar fails to analyze(B) ... frequency of phenomena that the XHPSG grammar with the extracted rules fails toanalyze(C) = (A)?
(B) ... frequency of phenomena that cannot be analyzed by the XHPSG grammarbut can be analyzed by the XHPSG grammar with the extracted rules(D) = (C)/(A) ... the ratio of phenomena that are recoveredTable 2: Analysis of phenomena that are recoveredwhich is a translation into HPSG of the manually-developed XTAG English grammar (The XTAG Re-search Group , 1995).
The growth of the numberof extracted rules is shown in the left of Figure 3.The average cost per sentence in offline parsing was8.11.
This means the total number of nodes andstructure-sharing that are removed was less than 9for each sentence.
The coverage for the training cor-pus by offline parsing was 95.4%.The coverage was measured by using the XHPSGgrammar with the extracted rules.
The coverage forTestSetA and TestSetB is illustrated in the middleand right of Figure 3, respectively.
As seen in thefigure, the coverage for the Wall Street Journal grewfrom 24.7% to 65.3% for TestSetA and from 64% to88% for TestSetB.We measured the degree of overgeneration bymeasuring the number of edges, using a parserbased on A* algorithm.
Figure 4 shows the aver-age number of edges when TestSetB was parsed.From this figure and Figure 3, we can observe thatthe coverage grew from 64% to 88% by generatingjust 87.99 more edges (the number of edges grewfrom 240.68 to 328.67 in average).From the experiments, we can say that our ap-proach is effective in extending coverage with a lit-tle overgeneration.We have analyzed the phenomena that cannotbe analyzed by the original XHPSG grammar butcan be analyzed by the extracted rules in the first200 sentences in Wall Street Journal 03 of the testset.
Among the 200 sentences, the original XH-PSG grammar can cover 38 sentences (19% of thesentences) and the XHPSG grammar with the ex-tracted rules can analyze 131 sentences (65.5% ofthe sentences).
Table 2 shows the number of each                      	            		                                          	                                          Figure 3: The number of extracted rules (left), coverage for TestSetA (middle) and TestSetB (right)phenomenon that the original grammar fails to an-alyze ((A) in the table), and also shows the num-ber of each phenomenon that the XHPSG gram-mar with the extracted rules still fails to analyze((B) in the table).
As seen in the table, more than70% of phenomena that the original grammar can-not analyze were analyzed by our method.
Note thatmost of the phenomena that cannot be analyzed withthe extracted rules were lack of lexical entry, in-consistency between the grammar and the treebank,and complicated phenomena that are currently openproblems in the field of linguistics.Most of the lack of lexical entries failures werecaused by the lack of ?apostrophe s.?
This meansthat just by adding lexical entries for ?apostrophes?, we can cover almost half of this type of er-ror.
Among the words listed in the table, the XH-PSG grammar has no lexical entry for ?itself?
and?as (Adv)?.
As our method is only concerned withgrammar rules, our method cannot recover wordsthat have no lexical entry.
This means that if a sen-tence includes the word ?itself?, the sentence cannotbe recovered by our method.7 ConclusionWe proposed two new types of default unifica-tion, ideal and non-ideal lenient default unification.Ideal lenient default unification is desirable in thatit maximizes the amount of information in the re-sult, while other existing types of default unificationmaximize the amount of information in the default.Although non-ideal lenient default unification givesa less informative result than the ideal one, it worksefficiently and retains the desiderata the ideal onesatisfies.We also proposed a new approach to extend thecoverage of a grammar.
We extracted grammar rulesfrom the results of robust parsing using lenient de-fault unification.
A series of experiments showedthat the extracted rules work robustly, and the cov-erage of the XHPSG grammar for Penn Treebankgreatly increased with a little overgeneration.ReferencesG.
Bouma.
1990.
Defaults in unification grammar.
InProc.
of ACL-1990, pages 165?172.B.
Carpenter, 1993.
Inheritance, Defaults, and the Lexi-con, chapter Skeptical and credulous default unifica-tion with applications to templates and inheritance,pages 13?37.
Cambridge University Press, Cam-bridge.D.
Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.In Proc.
of ACL-2000, pages 456?463.A.
Copestake.
1993.
The representation of lexical se-mantic information.
Ph.D. thesis, University of Sus-sex.S.
Douglas and R. Dale.
1992.
Towards robust PATR.
InProc.
of COLING-1992, pages 468?474.O.
Imaichi and Y. Matsumoto.
1995.
Integration of syn-tactic, semantic and contextual information in pro-cessing grammatically ill-formed inputs.
In Proc.
ofIJCAI-1995, pages 1435?1440.M.
Kiyono and J. Tsujii.
1993.
Linguistic knowledgeacquisition from parsing failures.
In Proc.
of EACL-1993, pages 222?231.A.
Lascarides and A. Copestake.
1999.
Default repre-sentation in constraint-based frameworks.
Computa-tional Linguistics, 25(1):55?105.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: the Penn Treebank.
Computational Linguistics,19(2):313?330.G.
Neumann.
1994.
Application of explanation-basedlearning for efficient processing of constraint-basedgrammars.
In Proc.
of the 10th IEEE Conference onArtificial Intelligence for Applications, pages 208?215.The XTAG Research Group.
1995.
A Lexicalized TreeAdjoining Grammar for English.
Technical Report95-03, IRCS, University of Pennsylvania.G.
Russell, J. Carroll, and S. Warwick-Armstrong.
1991.Multiple default inheritance in a unification-based lex-icon.
In Proc.
of ACL-1991, pages 215?221.C.
Samuelsson and M. Rayner.
1991.
Quantitative eval-uation of explanation-based learning as an optimiza-tion tool for a large-scale natural language system.
InProc.
of IJCAI-1991, pages 609?615.Y.
Tateisi, K. Torisawa, Y. Miyao, and J. Tsujii.
1998.Translating the XTAG English grammar to HPSG.
InProc.
of TAG+4, pages 172?175.F.
Xia.
1999.
Extracting tree adjoining grammars frombracketed corpora.
In Proc.
of NLPRS-1999, pages398?403.
