PrefaceGeneration Challenges 2010 was the fourth round of shared-task evaluation compe-titions (STECs) that involve the generation of natural language; it followed the PilotAttribute Selection for Generating Referring Expressions Challenge in 2007 (AS-GRE?07) and Referring Expression Generation Challenges in 2008 (REG?08), andGeneration Challenges 2009 (GenChal?09).
More information about all these NLGSTEC activities can be found via the links on the Generation Challenges homepage(http://www.nltg.brighton.ac.uk/research/genchal10).Generation Challenges 2010 brought together three sets of STECs: the threeGREC Challenges, GREC Named Entity Generation (GREC-NEG), Named EntityReference Detection (GREC-NER), and Named Entity Reference Regeneration(GREC-Full), organised by Anja Belz and Eric Kow; the Challenge on Generat-ing Instructions in Virtual Environments (GIVE) organised by Donna Byron, Jus-tine Cassell, Robert Dale, Alexander Koller, Johanna Moore, Jon Oberlander, andKristina Striegnitz; and the new Question Generation (QG) tasks, organised byVasile Rus, Brendan Wyse, Mihai Lintean, Svetlana Stoyanchev and Paul Piwek.In the GIVE Challenge, participating teams developed systems which gener-ate natural-language instructions to users navigating a virtual 3D environment andperforming computer-game-like tasks.
The seven participating systems were eval-uated by measuring how quickly, accurately and efficiently users were able to per-form tasks with a given system?s instructions, as well as on subjective measures.Unlike the first GIVE Challenge, this year?s challenge allowed users to move andturn freely in the virtual environment, rather than in discrete steps, making the NLGtask much harder.
The evaluation report for the GIVE Challenge can be found inthis volume; the participants?
reports will be made available on the GIVE website(http://www.give-challenge.org/research) at a later stage.The GREC Tasks used the GREC-People corpus of introductory sections fromWikipedia articles on people.
In GREC-NEG, the task was to select referring ex-pressions for all mentions of all people in an article from given lists of alternatives(this was the same task as at GenChal?09).
The GREC-NER task combines named-entity recognition and coreference resolution, restricted to people entities; the aimfor participating systems is to identify all those types of mentions of people thatare annotated in the GREC-People corpus.
The aim for GREC-Full systems was toimprove the referential clarity and fluency of input texts.
Participants were free todo this in whichever way they chose.
Participants were encouraged, though notrequired, to create systems which replace referring expressions as and where nec-essary to produce as clear and fluent a text as possible.
This task could be viewedas combining the GREC-NER and GREC-NEG tasks.The first Question Generation challenge consisted of three tasks: Task A re-quired questions to be generated from paragraphs of texts; Task B required systemsto generate questions from sentences, and Task C was an Open Task track in whichany QG research involving evaluation could be submitted.
At the time of going topress, the QG tasks are still running; this volume contains a preliminary report fromthe organisers.In addition to the four shared tasks, Generation Challenges 2010 offered (i) anopen submission track in which participants could submit any work involving thedata from any of the shared tasks, while opting out of the competetive element, (ii)an evaluation track, in which proposals for new evaluation methods for the sharedtask could be submitted, and (iii) a task proposal track in which proposals for newshared tasks could be submitted.
We believe that these types of open-access tracksare important because they allow the wider research community to shape the focusand methodologies of STECs directly.We received three submissions in the Task Proposals track: an outline proposalfor tasks involving language generation under uncertainty (Lemon et al); a pro-posal for a shared task on improving text written by non-native speakers (Dale andKilgarriff); and a proposal for a surface realisation task (White et al).Once again, we successfully applied (with the help of support letters frommanyof last year?s participants and other HLT colleagues) for funding from the Engineer-ing and Physical Sciences Research Council (EPSRC), the main funding body forHLT in the UK.
This support helped with all aspects of organising Generation Chal-lenges 2010, and enabled us to create the new GREC-People corpus and to carry outextensive human evaluations, as well as to employ a dedicated research fellow (EricKow) to help with all aspects of Generation Challenges 2010.Preparations are already underway for a fifth NLG shared-task evaluation eventnext year, Generation Challenges 2011, which is likely to include a further run ofthe GIVE Task, a second run of the QG Challenge, and a pilot surface realisationtask.
We expect that results will be presented at ENLG?11.Just like our previous STECs, Generation Challenges 2010 would not have beenpossible without the contributions of many different people.
Wewould like to thankthe students of Oxford University, KCL, UCL, Brighton and Sussex Universitieswho participated in the evaluation experiments, as well as all other participants inour online data elicitation and evaluation exercises; the INLG?10 organisers, Ielkavan der Sluis, John Kelleher and Brian MacNamee; the research support team atBrighton University and the EPSRC for help with obtaining funding; and last butnot least, the participants in the shared tasks themselves.July 2010 Anja Belz, Albert Gatt and Alexander Koller
