Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 380?388,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsAssessing and Improving the Performance ofSpeech Recognition for Incremental SystemsTimo Baumann, Michaela Atterer, David SchlangenInstitut fu?r LinguistikUniversita?t PotsdamPotsdam, Germany{timo,atterer,das}@ling.uni-potsdam.deAbstractIn incremental spoken dialogue systems, par-tial hypotheses about what was said are re-quired even while the utterance is still ongo-ing.
We define measures for evaluating thequality of incremental ASR components withrespect to the relative correctness of the par-tial hypotheses compared to hypotheses thatcan optimize over the complete input, the tim-ing of hypothesis formation relative to the por-tion of the input they are about, and hypothesisstability, defined as the number of times theyare revised.
We show that simple incremen-tal post-processing can improve stability dra-matically, at the cost of timeliness (from 90%of edits of hypotheses being spurious down to10% at a lag of 320ms).
The measures arenot independent, and we show how system de-signers can find a desired operating point fortheir ASR.
To our knowledge, we are the firstto suggest and examine a variety of measuresfor assessing incremental ASR and improveperformance on this basis.1 IntroductionIncrementality, that is, the property of beginning toprocess input before it is complete, is often seen as adesirable property of dialogue systems (e.g., Allenet al (2001)), as it allows the system to (a) foldprocessing time (of modules such as parsers, or di-alogue managers) into the time taken by the utter-ance, and (b) react to partial results, for example bygenerating back-channel utterances or speculativelyinitiating potentially relevant database queries.Input to a spoken dialogue system normallypasses an automatic speech recognizer (ASR) as afirst processing module, thus the module?s incre-mentality determines the level of incrementality thatcan be reached by the system as a whole.
Usingan ASR system incrementally poses interesting chal-lenges, however.
Typically, ASRs use dynamic pro-gramming and the maximum likelihood hypothesisto find the word sequence with the lowest expectedlikelihood of the sequence containing errors (sen-tence error).
Due to the dynamic programming ap-proach, what is considered the best hypothesis abouta given stretch of the input signal can change duringthe recognition process, as more right context whichcan be used as evidence becomes available.In this paper, we argue that normally used met-rics for ASR evaluation such as word error rate mustbe complemented with metrics specifically designedfor measuring incremental performance, and offersome such metrics.
We show that there are varioussubproperties that are not independent of each other,and that trade-offs are involved if either of those isto be optimized.
Finally, we propose ways to im-prove incremental performance (as measured by ourmetrics) through the use of smoothing techniques.To our knowledge, incremental evaluation met-rics of ASR for incremental systems have not yetbeen covered in the literature.
Most closely related,Wachsmuth et al (1998) show results for an ASRwhich fixes its results after a given time ?
and re-port the corresponding word error rate (WER).
Thisunfortunately confounds the incremental and non-incremental properties of their ASR?s performance.The remainder of this paper is structured as fol-lows: In section 2, we give an overview of increme-nality with respect to ASR, and develop our evalua-380tion metrics.
Section 3 describes the setup and datathat we used in our experiments, and reports and dis-cusses some basic measures for different variants ofthe setup.
In section 4 we propose and discuss twoorthogonal methods that improve incremental per-formance: using right context and using messagesmoothing, which show different properties with re-gard to our measures.
Finally, in section 5 we sumup and point to future directions.2 Incrementality and Evaluation Measuresfor Incremental ASRIn a modular system, an incremental module is onethat generates (partial) responses while input is stillongoing and makes these available to other mod-ules (Kilger and Finkler, 1995).
ASR modules thatuse token passing (Young et al, 1989) can easilybe adapted to output a new, live hypothesis afterprocessing of every input frame (often that is ev-ery 10ms).
In an incremental system we are ableto get partial results from these hypotheses as soonas they become available ?
or rather as soon as theycan be trusted.
As mentioned above, hypotheses areonly tentative, and may be revised when more rightcontext becomes available.
Modules consuming theoutput of an incremental ASR hence must be ableto deal with such revisions.
There is a first trade-offhere: Depending on how costly revision is for latermodules (which after all may need to revise any hy-potheses which they themselves based on the now-revised input), it may be better to reduce the incre-mentality a bit ?
in the sense that partial informa-tion is produced less often, and hence new words forexample are recognised later ?
if that buys stability(fewer revisions).
Also, ignoring some incremen-tal results that are likely to be wrong may increasesystem performance.
Defining these notions moreprecisely is the aim of this section.2.1 Relative CorrectnessWe define a hypothesis at time t (hypt) as consist-ing of a sequence whypt of words predicted by theASR at time t.1 As an example figure 1 shows1In this paper, we only deal with one-best ASR.
We believethat there are no principled differences when generalising to n-best hypotheses, but will explore this in detail in future work.We also abstract away from changes in the hypothesised startand end times of the words in the sequence.
It often happens thatFigure 1: Live ASR hypotheses during incrementalrecognition.
Edit messages (see section 2.2) are shownon the right when words are added (?)
or revoked (?
).For the word ?zwei?
WFC and WFF (see section 2.3) areshown at the bottom.a sequence of incrementally produced hypotheses.
(Note that this is an artificial example, showing onlya few illustratory and interesting hypotheses.
In areal recognition system, the hypothesis frequency isof course much higher, with much repetition of sim-ilar hypotheses at consecutive frames.
)The question now is how we can evaluate thequality of a hypothesis at the time t it is produced.It is reasonable to only expect this hypothesis to saysomething (correct or not) about the input up to timet ?
unless we want the ASR to predict, in which casewe want it to make assumptions about times beyondt (see section 4.1).
There are two candidates for theyardstick against which the partial hypotheses couldbe compared: First, one could take the actually spo-ken words, computing measures such as word errorrate.
The other option, which is the one taken here,is to take as the gold standard the final hypothesisproduced by the ASR when it has all evidence avail-the ASR?s assumptions about the position of the word bound-aries change, even if the word sequence stays constant.
If, as weassume here, later modules do not use this timing information,we can consider two hypotheses that only differ in boundaryplacement as identical.381able (i.e., when the utterance is complete).
This ismore meaningful for our purpose, as it relates the in-formativity of the partial hypothesis to what can beexpected if the ASR can do all its internal optimisa-tions, and not to the factually correct sequence thatthe ASR might not be able to recognise even withall information present.
This latter problem is al-ready captured in the conventional non-incrementalperformance measures.In our metrics in this paper, we hence take as goldstandard (wgold) the final, non-incremental hypothe-sis of the ASR (which, to reiterate this point, mightbe factually incorrect, that is, might contain worderrors).
We define a module?s incremental responseat time t (whypt) as relatively correct (r-correct), iffit is equal to the non-incremental hypothesis up totime t: whypt t = wgoldt.
Hence, in figure 1 above,hypotheses 1, 2, 6, 7, 9 and 12 are r-correct.2 Wecall the normalised rate of r-correct responses of amodule its (average) r-correctness.As defined above, the criterion for r-correctnessis still pretty strict, as it demands of the ASR thatwords on the right edge are recognised even fromthe first frame on.
For example, whyp10 in figure 1is not r-correct, because wgold10 (that part of wgoldthat ends where whyp10 ends) already spans parts ofthe word ?drei?
which has not yet been picked upby the incremental recognition.
A relaxed notionof correctness hence is prefix-correctness, which re-quires only that whypt be a prefix of wgoldt.
(Hy-potheses 3 and 10 in figure 1 are p-correct, as are allr-correct hypotheses.)
It should be noted though thatp-correctness is too forgiving to be used directly asan optimization target: in the example in figure 1,a module that only ever produces empty hypotheseswould trivally achieve perfect p-correctness (as thisis always a prefix of wgold).2.2 Edit OverheadThe measures defined so far capture only static as-pects of the incremental performance of a moduleand do not say anything about the dynamics of therecognition process.
To capture this, we look atthe changes between subsequent partial hypotheses.There are three ways in which an hypothesis hypt+12The timing in hypothesis 7 is not correct ?
but this does notmatter to our notion of correctness (see footnote 1).can be different from hypt: there can be an extensionof the word sequence, a revokation, or a revision ofthe last words in the sequence.3 These differencescan be expressed as edit messages, where extendinga sequence by one word would require an add mes-sage (?
), deleting the last word in the sequence arevoke message (?
), and exchange of the last wordwould require two messages, one to revoke the oldand one to add the new word.4Now, an incrementally perfect ASR would onlygenerate extensions, adding new words at the rightedge; thus, there would be exactly as many edit mes-sages as there are words in wgold.
In reality, thereare typically many more changes, and hence manyspurious edits (see below for characteristic rates inour data).
We call the rate of spurious edits the editoverhead (EO).
For figure 1 above, this is 811 : Thereare 11 edits (as shown in the figure), while we?d ex-pect only 3 (one ?
for each word in the final result).Hence, 8 edits are spurious.This measure corresponds directly to the amountof unnecessary activity a consumer of the ASR?soutput performs when it reacts swiftly to words thatmay be revoked later on.
If the consumer is able torobustly cope with parallel hypotheses (for exampleby building a lattice-like structure), a high EO maynot be problematic, but if revisions are costly forlater modules (or even impossible because action hasalready been taken), we would like EO to be as lowas possible.
This can be achieved by not sending editmessages unconditionally as soon as words changein the ASR?s current hypothesis, using strategies asoutlined in section 4.
Obviously, deferring or sup-pressing messages results in delays, a topic to whichwe turn in the following section, where we definemeasures for the response time of ASR.2.3 Timing MeasuresSo far, our measures capture characteristics aboutthe complete recognition process.
We now turn tothe timing of the recognition of individual words.For this, we again take the output of the ASR whenall signal is present (i.e., wgold) as the basis.
There3As fourth and most frequent alternative, consecutive hy-potheses do not change at all.4Revision could also be seen as a third atomic operation,as in standard ASR evaluation (then called ?substitution?).
Tokeep things simple, we only regard two atomic operations.382are two things we may be interested in.
First, wemay want to know when is the first time that a certainword appears in the correct position in the sequence(or equivalently, when its first correct add edit mes-sage is sent), expressed in relation to its boundariesin wgold.
We measure this event, the first time thatthe ASR was right about a word, relative to its goldbeginning.
We call the measure word first correctresponse (WFC).
As a concrete example take hyp7in figure 1.
At this point, the word ?zwei?
is first hy-pothesised.
Compared to the beginning of the wordin wgold, this point (t7) has a delay of 1 frame (theframes are illustrated by the dashed lines).As explained above, it may very well be thecase that for a brief while another hypothesis, notr-correct w.r.t.
wgold, may be favoured (cf.
the word?zwar?
in the example in the figure).
Another mea-sure we hence might also be interested in is when ourword hypothesis starts remaining stable or, in otherwords, becomes final.
We measure this event rela-tive to the end of the word in the gold standard.
Wecall it word first final response (WFF).
In our exam-ple, again for ?zwei?, this is t9, which has a distanceof 0 to the right boundary of the word in wgold.In principle, we could use both anchor points (theleft vs. the right edge of a word) for either measureor use a word-relative scale, but for simplicity?s sakewe restrict ourselves to one anchor point each.Under normal conditions, we expect WFC to bepositive.
The better the incremental ASR, the closerto 0 it will be.
WFC is not a measure we can eas-ily optimize.
We would either have to enumeratethe whole language model or use external non-ASRknowledge to predict continuations of the word se-quence before the word in question has started.
Thiswould increase EO.
In principle, we are rather in-terested in accepting an increase in WFC, when wedelay messages in order to decrease EO.WFF however, can reach values below 0.
Itconverges towards the negative average of wordlength as an incremental ASR improves.
For non-incremental ASR it would be positive: the averagedistance beween the sentence end and word end.WFF is a measure we can strive to reduce by sendingfewer (especially fewer wrong) messages.Another property we might be interested in opti-mizing is the time it takes from the first correct hy-pothesis to stabilize to a final hypothesis.
We com-pute this correction time as the difference in timebetween WFF and WFC.5 A correction time of 0 in-dicates that there was no correction, i.e.
the ASRwasimmediately correct about a word, something whichwe would like to happen as often as possible.Note that these are measures for each word ineach processed utterance, and we will use distribu-tional parameters of these timing measures (meansand standard deviations) as metrics for the perfor-mance of the incremental setups described later.2.4 Summary of MeasuresIn this section, we first described measures that eval-uate the overall correctness of incrementally pro-duced ASR hypotheses, not taking into account theirsequential nature.
We then turned to the dynamics ofhow the current hypothesis evolves in a way whichwe consider important for a consumer of incremen-tal ASR, namely the overhead that results from editsto the hypothesis.
Finally, we looked at the timingof individual messages with regard to first correct(potentially unstable) occurrence (WFC) and stabil-ity (WFF).
In the next section, we use the measuresdefined here to characterize the incremental perfor-mance of our ASR, before we discuss ways to im-prove incremental performance in section 4.3 Setup, Corpora and Base MeasurementsWe use the large-vocabulary continuous-speechrecognition framework Sphinx-4 (Walker et al,2004) for our experiments, using the built-in Lex-Tree decoder, extended by us to provide incremen-tal results.
We built acoustic models for German,based on a small corpus of spontaneous instructionsin a puzzle building domain,6 and the Kiel corpusof read speech (IPDS, 1994).
We use a trigram lan-guage model that is based on the puzzle domain tran-scriptions.
As test data we use 85 recordings of twospeakers (unknown to the acoustic model) that speaksentences similar to those in the puzzle domain.We do not yet use recognition rescoring to opti-mize for word error rate, but just the ASR?s besthypotheses which optimize for low sentence error.Incremental rescoring mechanisms such as that of5In figure 1, the correction time for ?zwei?
is 9?
7 = 2.6Available from http://www.voxforge.org/home/downloads/speech/383SER (non-incremental) 68.2%WER (non-incremental) 18.8%r-correct (cropped) 30.9%p-correct (cropped) 53.1%edit overhead 90.5%mean word duration 0.378 sWFC: mean, stddev, median 0.276 s, 0.186 s, 0.230 sWFF: mean, stddev, median 0.004 s, 0.268 s, ?0.06 simmediately correct 58.6%Table 1: Base measurements on our dataRazik et al (2008) to optimize ASR performance areorthogonal to the approaches presented in section 4and could well be incorporated to further improveincremental performance.The individual recordings in our corpus are fairlyshort (5.5 seconds on average) and include a bit of si-lence at the beginning and end.
Obviously, recogniz-ing silence is much easier than recognizing words.To make our results more meaningful for continuousspeech, we crop away all ASR hypotheses from be-fore and after the active recognition process.7 Whilethis reduces our performance in terms of correctness(we crop away areas with nearly 100% correctness),it has no impact on the edit overhead, as the numberof changes in wcurr remains unchanged, and also noimpact on the timing measures as all word bound-aries remain the same.3.1 Base MeasurementsTable 1 characterises our ASR module (on our data)in terms of the metrics defined in section 2.
Addi-tionally we state sentence error rate, as the rate ofsentences that contain at least one error, and worderror rate computed in the usual way, as well asthe mean duration of words in our corpus (as non-incrementally measured for our ASR).We see that correctness is quite low.
This ismostly due to the jitter that the evolving current hy-pothesis shows in its last few frames, jumping backand forth between highly-ranked alternatives.
Also,our ASR only predicts words once there is acousticevidence for several phonemes and every phoneme(being modelled by 3 HMM states) must have a du-ration of at least 3 frames.
Thus, some errors rela-tive to the final hypothesis occur because the ASR7In figure 1, hypotheses 1, 2 and 3 would be cropped away.4050607080901000  0.2  0.4  0.6  0.8  1  1.2percentageof wordsthatarefinalcorrection time in sFigure 2: Distribution of correction times (WFF?WFC).only hypothesizes about words once they alreadyhave a certain duration (and hence preceding hy-potheses are not r-correct).
The difference betweenr-correctness and p-correctness (20% in our case)may be largely attributed to this fact.The edit overhead of 90.5% means that for ev-ery neccessary add message, there are nine superflu-ous (add or revoke) messages.
Thus, a consumer ofthe ASR output would have to recompute its resultsten times on average.
In an incremental system, thisconsumer might itself output messages and furtherrevise decisions as information from other modulesbecomes available, leading to a tremendous amountof changes in the system state.
As ASR is the firstmodule in an incremental spoken dialogue system,reducing the edit overhead is essential for overallsystem performance.On average, the correct hypothesis about a wordbecomes available 276ms after the word has started(WFC).
With a mean word duration of 378msthis means that information becomes available af-ter roughly 34 of the word have been spoken.
No-tice though that the median is somewhat lower thanthe mean, implying that this time is lower for mostwords and much higher for some words.
In fact, themaximum for WFC in our data is 1.38 s.On average, a word becomes final (i.e.
isnot changed anymore) when it has ended(mean(WFF) = 0.004).
Again, the median islower, indicating the unnormal distribution of WFF(more often lower, sometimes much higher).Of all words, 58.6% were immediately correctly3840204060801002 5 8 11LM weightR-CorrectnessP-CorrectnessEdit OverheadWERFigure 3: Correctness, Edit Overhead and Word ErrorRate (WER) with varied language model weight and un-altered audio.hypothesized by the ASR.
Figure 2 plots the per-centage of words with correction times equal to orlower than the time on the x-axis.
While this startsat the initial 58.6% of words that were immediatelycorrect, it rises above 90% for a correction time of320ms and above 95% for 550ms.
Inversely thismeans that we can be certain to 90% (or 95%) thata current correct hypothesis about a word will notchange anymore once it has not been revoked for320ms (or 550ms respectively).Knowing (or assuming with some certainty) thata hypothesis is final allows us, to commit ourselvesto this hypothesis.
This allows for reduced compu-tational overhead (as alternative hypotheses can beabandoned) and is crucial if action is to be taken thatcannot be revoked later on (as for example, initiat-ing a response from the dialogue system).
Figure 2allows us to choose an operating point for commit-ment with respect to hypothesis age and certainty.3.2 Variations of the SetupIn setting up our system we did not yet strive for best(non-incremental) performance; this would have re-quired much more training material and parametertweaking.
We were more interested here in explor-ing general questions related to incremental ASR,and in developing approaches to improve incremen-tal performance (see section 4), which we see as aproblem that is independent from that of improvingperformance measures like (overall) accuracy.To test how independent our measures are on de-020406080100orig -20 -15 -10 -5 0signal to noise ratio in dBR-CorrectnessP-CorrectnessEdit OverheadWERFigure 4: Correctness, Edit Overhead and Word ErrorRate (WER) with additive noise (LM weight set to 8).tails of the specific setting, such as quality of theaudio material and of the language model, we var-ied these factors systematically, by adding whitenoise to the audio and changing the language modelweight relative to the acoustic model.
We varied thenoise to produce signal to noise ratios ranging fromhardly audible (?20 dB), through annoying noise(?10 dB) to barely understandable audio (0 dB).Figure 3 gives an overview of the ASR-performance with different LM weights and figure 4with degraded audio signals.
Overall, we see thatr-correctness and EO change little with differentLM and AM performance and correspondigly de-graded WER.
A tendency can be seen that larger LMweights result in higher correctness and lower EO.
Alarger LM weight leads to less influence of acousticevents which dynamically change hypotheses, whilethe static knowledge from the LM becomes moreimportant.
Surprisingly, WER improved with theaddition of slight noise, which we assume is due todifferences in recording conditions between our testdata and the training data of the acoustic model.In the following experiments as well as in the datain table 1 above, we use a language model weight of8 and unaltered audio.4 Improving Incremental PerformanceIn the previous section we have shown how a stan-dard ASR that incrementally outputs partial hy-potheses after each frame processed performs withregard to our measures and showed that they remain385stable in different acoustic conditions and with dif-fering LM weights.
We now discuss ways of incre-mentally post-processing ASR hypotheses in orderto improve selected measures.We particularly look for ways to improve EO;that is, we want to reduce the amount of wrong hy-potheses and resulting spurious edits that deterio-rate later modules?
performance, while still being asquick as possible with passing on relevant hypothe-ses.
We are less concerned with correctness mea-sures, as they do not capture well the dynamic evo-lution, which is important for further processing ofthe incremental hypothesis.
We also discuss trade-offs that are involved in the optimization decisions.4.1 Right ContextAllowing the use of some right context is a com-mon strategy to cope with incremental data.
Forexample, our ASR already uses this strategy (withvery short right contexts) internally at word bound-aries to restrict the language model hypotheses toan acoustically plausible subset (Ortmanns and Ney,2000).
In the experiment described here, we allowthe ASR a larger right context of size ?
by takinginto account at time t the output of the ASR up totime t ?
?
only.
That is, what the ASR hypothe-sizes about the interval ]t ?
?, t] is considered tobe too immature and is discarded, and the hypothe-ses about the input up to t??
have the benefit of alookahead up to t. This reduces jitter, which is foundmostly to the very right of the incremental hypothe-ses.
Thus, we expect to reduce the edit overhead inproportion with ?.
On the other hand, allowing theuse of a right context leads to the current hypothe-sis lagging behind the gold standard.
Correspond-ingly, WFC increases by ?.
Obviously, using onlyinformation up to t ?
?
has averse effects on cor-rectness as well, as this measure evaluates the wordsequences up to wgoldt which may already containmore words (those recognised in ]t ?
?, t]).
Thus,to be more fair and to account for the lag when mea-suring the module?s correctness, we additionally de-fine fair r-correctness which restricts the evaluationup to time t??
: whyptt??
= wgoldt?
?.Figure 5 details the results for our data with rightcontext between 1.5 s and ?0.2 s. (The x-axis plots?
as negative values, with 0 being ?now?.
Resultsfor a right context (?)
of 1.2 can thus be found 1.2 to020406080100-1.6 -1.4 -1.2 -1 -0.8 -0.6 -0.4 -0.2  0  0.2right context in s (scale shows larger right contexts towards the left)(strict) R-Correctnessfair R-CorrectnessP-CorrectnessEdit OverheadWERFigure 5: Correctness (see text), Edit Overhead andfixed-WER for varying right contexts ?.the left of 0, at ?1.2.)
We see that at least in the fairmeasure, fixed lag performs quite well at improvingboth the module?s correctness and EO.
This is dueto the fact that ASR hypotheses become more andmore stable when given more right context.
Still,even for fairly long lags, many late edits still occur.To illustrate the effects of a system that does notsupport edits of hypotheses, but instead commitsright away, we plot WER that would be reached by asystem that always commits after a right context of?.
As can be seen in the figure, the WER remainshigher than the non-incremental WER (18.8%) evenfor fairly large right contexts.
Also, the WER plot byWachsmuth et al (1998) looks very similar to oursand likewise shows a sweet spot suitable as an oper-ating point with a right context of about 800ms.As expected, the analysis of timing measuresshows an increase with larger right contexts withtheir mean values quickly approaching ?
(or?
?meanword duration for WFF), which are thelower bounds when using right context.
Correspond-ingly, the percentage of immediately correct hy-potheses increases with right context reaching 90%for ?
= 580ms and 98% for ?
= 1060ms.Finally, we can extend the concept of right con-text into negative values, predicting the future, as itwere.
By choosing a negative right context, in whichwe extrapolate the last hypothesis state by ?
into thefuture, we can measure the correctness of our hy-potheses correctly predicting the close future, whichis always the case when the current word is still be-386ing spoken.
The graph shows that 15% of our hy-potheses will still be correct 100ms in the future and10% will still be correct for 170ms.
Unfortunately,there is little way to tell apart hypotheses that willsurvive and those which will soon be revised.4.2 Message SmoothingIn the previous section we reduced wrong edit mes-sages by avoiding most of the recognition jitter byallowing the ASR a right context of size ?, whichdirectly hurt timing measures by roughly the sameamount.
In this section, we look at the sequence ofpartial hypotheses from the incremental ASR, usingthe dynamic properties as cues.
We accomplish thisby looking at the edit messages relative to the cur-rently output word sequence.
But instead of sendingthem to a consumer directly (updating the externalword sequence), we require that an edit message bethe result of N consecutive hypotheses.
To illustratethe process with N = 2 we return to figure 1.
Noneof the words ?an?, ?ein?
or ?zwar?
would ever beoutput, because they are only present for one time-interval each.
Edit messages would be sent at thefollowing times: ?
(eins) at t7, ?
(zwei) at t10 (onlythen is ?zwei?
the result of two consecutive hypothe-ses) and ?
(drei) at t13.
While no words are revokedin the example, this still occurs when a revocation isconsecutively hypothesized for N frames.We get controversial results for this strategy, ascan be seen in figure 6: The edit overhead fallsrapidly, reaching 50% (for each message necessary,there is one superfluous message) with only 110ms(and correspondingly increasing WFC by the sametime) and 10% with 320ms.
The same thresh-olds are reached through the use of right context at530ms and 1150ms respectively as shown in fig-ure 5.
Likewise, the prefix correctness improve-ments are better than with using right context, butthe r-correctness is poor, even under the ?fair?
mea-sure.
We believe this is due to correct hypothesesbeing held back too long due to the hypothesis se-quence being interspersed with wrong hypotheses(which only last for few consecutive hypotheses)which reset the counter until the add message (forthe prevalent and potentially correct word) is sent.88This could be resolved by using some kind of majoritysmoothing instead of requiring a message to be the result of allconsecutive hypotheses.
We will investigate this in future work.020406080100-1 -0.8 -0.6 -0.4 -0.2  0smoothing in s (scale shows larger smoothings towards the left)(strict) R-Correctnessfair R-CorrectnessP-CorrectnessEdit OverheadFigure 6: Correctness and Edit Overhead for varyingsmoothing lengths.5 Conclusions and Further DirectionsWe have presented the problem of speech recogni-tion for incremental systems, outlined requirementsfor incremental speech recognition and showed mea-sures that capture how well an incremental ASR per-forms with regard to these measures.
We discussedthe measures and their implications in detail withour baseline system and showed that the incremen-tal measures remain stable regardless of the specificASR setting used.Finally, we presented ways for the online post-processing of incremental results, looking for waysto improve some of the measures defined, whilehurting the other measures as little as possible.Specifically, we were interested in generating lesswrong hypotheses at the cost of possible short de-lays.
While using right context shows improvementswith larger delays, using message smoothing seemsespecially useful for fast processing.
We think thesetwo approaches could be combined to good effect.Together with more elaborate confidence handling asystem could quickly generate hypotheses and thenrefine the associated confidences over time.
We willexplore this in future work.AcknowledgmentsThis work was funded by a DFG grant in the EmmyNoether programme.
We wish to thank the anony-mous reviewers for helpful comments.387ReferencesJames Allen, George Ferguson, and Amanda Stent.
2001.An architecture for more realistic conversational sys-tems.
In Proceedings of the Conference on IntelligentUser Interfaces, Santa Fe, USA.IPDS.
1994.
The Kiel Corpus of Read Speech.
CD-ROM.Anne Kilger and Wolfgang Finkler.
1995.
Incrementalgeneration for real-time applications.
Technical Re-port RR-95-11, DFKI, Saarbru?cken, Germany.Stefan Ortmanns and Hermann Ney.
2000.
Look-aheadtechniques for fast beam search.
Computer Speech &Language, 14:15?32.Joseph Razik, Odile Mella, Dominique Fohr, and Jean-Paul Haton.
2008.
Frame-Synchronous and LocalConfidenceMeasures for on-the-fly Automatic SpeechRecognition.
In Proceedings of Interspeech 2008.Sven Wachsmuth, Gernot A. Fink, and Gerhard Sagerer.1998.
Integration of parsing and incremental speechrecognition.
In Proceedings of the European Sig-nal Processing Conference, volume 1, pages 371?375,Rhodes, Greece.Willi Walker, Paul Lamere, Philip Kwok, Bhiksha Raj,Rita Singh, Evandro Gouvea, Peter Wolf, and JoeWoelfel.
2004.
Sphinx-4: A flexible open sourceframework for speech recognition.
Technical ReportSMLI TR2004-0811, Sun Microsystems Inc.Steve Young, NH Russell, and JHS Thornton.
1989.
To-ken passing: a simple conceptual model for connectedspeech recognition systems.
Cambridge UniversityEngineering Department Technical Report CUED/F-INFENG/TR, 38.388
