Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 150?160,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsGeneralized Transition-based Dependency Parsingvia Control ParametersBernd Bohnet, Ryan McDonald, Emily Pitler and Ji MaGoogle Inc.{bohnetbd,ryanmcd,epitler,maji}@google.comAbstractIn this paper, we present a generalizedtransition-based parsing framework whereparsers are instantiated in terms of a setof control parameters that constrain tran-sitions between parser states.
This gener-alization provides a unified framework todescribe and compare various transition-based parsing approaches from both a the-oretical and empirical perspective.
Thisincludes well-known transition systems,but also previously unstudied systems.1 IntroductionTransition-based dependency parsing is perhapsthe most successful parsing framework in use to-day (Nivre, 2008).
This is due to the fact that it canprocess sentences in linear time (Nivre, 2003); ishighly accurate (Zhang and Nivre, 2011; Bohnetand Nivre, 2012; Weiss et al, 2015); and has el-egant mechanisms for parsing non-projective sen-tences (Nivre, 2009).
As a result, there have beennumerous studies into different transition systems,each with varying properties and complexities(Nivre, 2003; Attardi, 2006; Nivre, 2008; Nivre,2009; G?omez-Rodr?
?guez and Nivre, 2010; Choiand Palmer, 2011; Pitler and McDonald, 2015).While connections between these transition sys-tems have been noted, there has been little workon developing frameworks that generalize the phe-nomena parsed by these diverse systems.
Such aframework would be beneficial for many reasons:It would provide a language from which we cantheoretically compare known transition systems; itcan give rise to new systems that could have fa-vorable empirical properties; and an implementa-tion of the generalization allows for comprehen-sive empirical studies.In this work we provide such a generalizedtransition-based parsing framework.
Our frame-work can be cast as transition-based parsing as itcontains both parser states as well as transitionsbetween these states that construct dependencytrees.
As in traditional transition-based parsing,the state maintains two data structures: a set of un-processed tokens (normally called the buffer); anda set of operative tokens (often called the stack).Key to our generalization is the notion of activetokens, which is the set of tokens in which newarcs can be created and/or removed from consid-eration.
A parser instantiation is defined by a setof control parameters, which dictate: the types oftransitions that are permitted and their properties;the capacity of the active token set; and the maxi-mum arc distance.We show that a number of different transi-tion systems can be described via this frame-work.
Critically the two most common systemsare covered ?
arc-eager and arc-standard (Nivre,2008).
But also Attardi?s non-projective (Attardi,2006), Kuhlmann?s hybrid system (Kuhlmann etal., 2011), the directed acyclic graph (DAG) parserof Sagae and Tsujii (2008), and likely others.More interestingly, the easy-first framework ofGoldberg and Elhadad (2010) can be described asan arc-standard system with an unbounded activetoken capacity.We present a number of experiments with animplementation of our generalized framework.One major advantage of our generalization (andits implementation) is that it allows for easy ex-ploration of novel systems not previously studied.In Section 5 we discuss some possibilities and pro-vide experiments for these in Section 6.2 Related WorkTransition-based dependency parsing can be char-acterized as any parsing system that maintains a150state as well as a finite set of operations that movethe system from one state to another (Nivre, 2008).In terms of modern statistical models that domi-nate the discourse today, the starting point is likelythe work of Kudo and Matsumoto (2000) and Ya-mada and Matsumoto (2003), who adopted theidea of cascaded chunking from Abney (1991) ina greedy dependency parsing framework.From this early work, transition-based pars-ing quickly grew in scope with the formalizationof the arc-eager versus arc-standard paradigms(Nivre, 2003; Nivre, 2008), the latter largely beingbased on well-known shift-reduce principles in thephrase-structure literature (Ratnaparkhi, 1999).The speed and empirical accuracy of these sys-tems ?
as evident in the widely used MaltParsersoftware (Nivre et al, 2006a) ?
led to the study ofa number of different transition systems.Many of these new transition systems attemptedto handle phenomena not covered by arc-eageror arc-standard transition systems, which inher-ently could only produce projective dependencytrees.
The work of Attardi (2006), Nivre (2009),G?omez-Rodr?
?guez and Nivre (2010), Choi andPalmer (2011), and Pitler and McDonald (2015)derived transition systems that could parse non-projective trees.
Each of these systems traded-offcomplexity for empirical coverage.
Additionally,Sagae and Tsujii (2008) developed transition sys-tems that could parse DAGs by augmentating thearc-standard and the arc-eager system.
Bohnet andNivre (2012) derived a system that could produceboth labeled dependency trees as well as part-of-speech tags in a joint transition system.
Takingthis idea further Hatori et al (2012) defined a tran-sition system that performed joint segmentation,tagging and parsing.In terms of empirical accuracy, from the earlysuccess of Nivre and colleagues (Nivre et al,2006b; Hall et al, 2007; Nivre, 2008), there hasbeen an succession of improvements in trainingand decoding, including structured training withbeam search (Zhang and Clark, 2008; Zhang andNivre, 2011), incorporating graph-based rescoringfeatures (Bohnet and Kuhn, 2012), the aformen-tioned work on joint parsing and tagging (Bohnetand Nivre, 2012), and more recently the adoptionof neural networks and feature embeddings (Chenand Manning, 2014; Weiss et al, 2015; Dyer et al,2015; Alberti et al, 2015).In terms of abstract generalizations of transi-tion systems, the most relevant work is that ofG?omez-Rodr?
?guez and Nivre (2013) ?
which weabbreviate GR&N13.
In that work, a generalizedframework is defined by first defining a set of basetransitions, and then showing that many transition-based systems can be constructed via compositionof these base transitions.
Like our framework, thiscovers common systems such as arc-eager and arc-standard, as well as easy-first parsing.
In partic-ular, arc construction in easy-first parsing can beseen as an action composed of a number of shifts,an arc action, and a number of un-shift actions.The primary conceptual difference between thatwork and the present study is the distinction be-tween complex actions versus control parameters.In terms of theoretical coverage, the frameworksare not equivalent.
For instance, our generaliza-tion covers the system of Attardi (2006), whereasGR&N13 cover transition systems where multiplearcs can be created in tandem.
In Section 7 wecompare the two generalizations.3 Generalized Transition-based ParsingA transition system must define a parser state aswell as a set of transitions that move the systemfrom one state to the next.
Correct sequencesof transitions create valid parse trees.
A parserstate is typically a tuple of data structures andvariables that represent the dependency tree con-structed thus far and, implicitly, possible validtransitions to the next state.In order to generalize across the parser statesof transition-based parsing systems, we preservetheir common parts and make the specific partsconfigurable.
In order to generalize across thetransitions, we divide the transitions into their ba-sic operations.
Each specific transition is then de-fined by composition of these basic operations.
Bydefining the properties of the data structures andthe composition of the basic operations, differenttransition systems can be defined and configuredwithin one single unified system.
As a conse-quence, we obtain a generalized parser that is ca-pable of executing a wide range of different tran-sition systems by setting a number of control pa-rameters without changing the specific implemen-tation of the generalized parser.3.1 Basic NotationIn the following, we use a directed unlabeled de-pendency tree T = ?V,A?
for a sentence x =151This is an example with two arcs(a) Arc-standard: is and example are eligible for arcs.This is an example with two arcs(b) Arc-eager: example and with are eligible for arcs.This is an example with two arcs(c) Easy-first: All unreduced tokens are active (bolded).Figure 1: A partially processed dependency treeafter having just added the arc (example, an) inthe arc-standard, arc-eager, and easy-first systems.Tokens in the operative token set O are shaded or-ange, while tokens in the unordered buffer U arein an unshaded box.
The bolded tokens are inACTIVE(O) and eligible for arcs (Section 3.4).w1, ..., wn, where Vx= {1, ..., n} and Vrx=Vx?
{r}, Ax?
Vrx?
Vx.
r is a placeholder forthe root node and set either to 0 (root to the leftof the sentence) or n + 1 (root to the right).
Thedefinition is mostly equivalent to that of K?ubler etal.
(2009) but deviates in the potential handling ofthe root on the right (Ballesteros and Nivre, 2013).The set of nodes Vxindex the words of a sen-tence x and Vrxincludes in addition the artificialroot node r. Let A be the arc set, i.e., (i, j) ?
Aiff there is a dependency from i to j.
We use asalternative notation (i?
j).
This is the arc set thealgorithm will create.For ease of exposition, we will only address un-labeled parsing.
However, for our experiments wedo implement a labeled parsing variant using thestandard convention of composing arc transitionswith corresponding arc labels.3.2 Generalized Parser StateLet U be an unordered set of buffered unprocessedtokens.
This set is identical to the buffer fromtransition-based parsing.
Following standard no-tation, we will use i|U to indicate that i is the left-most element of the set.Let O be an ordered set of operative tokens.Specifically, O is the set of tokens that 1) havebeen moved out of U , and 2) are not themselvesreduced.
The set O is similar in nature to the tra-ditional stack of transition-based parsing, but isnot restricted to stack operations.
As in transition-Transitions?Adds an arc from j to i, both ?
ACTIVE(O).??i,j(O,U,A)?
(O,U,A ?
(j ?
i))?Adds an arc from i to j, both ?
ACTIVE(O).??i,j(O,U,A)?
(O,U,A ?
(i?
j))?Removes token i ?
ACTIVE(O) from O.?
?i(O...i..., U,A)?
(O,U,A)?Moves the top token from U to the top of O.?+ (O, i|U,A)?
(O|i, U,A)Figure 2: Base generalized transitions over parserstates.based parsing we will use the notation O|i to indi-cate that i is the rightmost element of the set; O[n]is the n-th rightmost element of the set.
Figure 1shows the set of tokens in O within shaded boxesand U within unshaded boxes.3.3 Generalized TransitionsThe above discussion describes what a general-ized transition-based parser state looks like; it doesnot describe any transitions between these states,which is the core of transition-based parsing.
Inthis section we present a set of basic transitions,which themselves can be composed to make morecomplex transitions (similar to G?omez-Rodr?
?guezand Nivre (2013)).Let T = {?i,j,?i,j,?i,+} be the set of ba-sic transitions (Figure 2), which have analoguesto standard transition-based parsing.
These transi-tions come with the standard preconditions, i.e., aroot cannot be a modifier; each token can modifyat most one word1; a token can only be reduced ifit has a head; and a shift can only happen if the un-operative buffer is non-empty.
We will often referto these as LEFT-ARC (?i,j), RIGHT-ARC (?i,j),REDUCE (?i), and SHIFT (+).
We additionally re-fer to LEFT-ARC and RIGHT-ARC together as arc-creation actions.3.4 Control ParametersInstantiations of a transition system are defined viacontrol parameters.
We defined two sets of suchparameters.
The first, we call global parameters,dictates system wide behaviour.
The second, wecall transition parameters, dictates a specific be-haviour for each transition.1This precondition is not needed for DAG transition sys-tems.152Global Control ParametersWe have two parameters for the broader behaviourof the system.1.
Active token capacity K. The activeset of tokens ACTIVE(O) that can be op-erated on by the transitions is the set{O[min(|O|,K)], ..., O[1]}.
K additionallydetermines the size of O at the start of pars-ing.
E.g., if K = 2, then we populate Owith the first two tokens.
This is equivalent tomaking SHIFT deterministic while |O| < K.2.
Max arc distance D.
I.e., arcs can only be cre-ated between two active tokens O[i] and O[j]if |i?
j| ?
D.Transition Control ParamtersLet M(T ) be a multiset of transitions, such thatif t ?
M(T ), then t ?
T .
Note thatM(T ) is amultiset, and thus can have multiple transitions ofthe same type.
For each t ?
M(T ), our general-ization requires the following control parametersto be set (default in bold):1.
Bottom-up: B ?
{[t]rue, [f ]alse}.
Whethercreating an arc also reduces it.
Specificallywe will have two parameters, BLand BR,which specify whether LEFT/RIGHT-ARC ac-tions are bottom up.
We use the notation andsay B = true to mean BL= BR= true.For example BL= true indicates that ?i,jis immediately followed by a reduce ?i.2.
Arc-Shift: S ?
{[t]rue, [f ]alse}.
Whethercreating an arc also results in SHIFT.
Specif-ically we will have two parameters, SLandSR, which specify whether LEFT/RIGHT-ARC actions are joined with a SHIFT.
Weuse the notation and say S = true to meanSL= SR= true.
For example SL= trueindicates that ?i,jis immediately followedby +.3.
Periphery: P ?
{[l]eft, [r]ight,na}.
If atransition must operate on the left or right pe-riphery of the active token set ACTIVE(O).For arc-creation transitions, this means thatat least one of the head or modifier is on thespecified periphery.
If the value is na, thatmeans that the action is not constrained to beon the periphery.
Note, that when K ?
2, allarc-creation actions by default are on the pe-riphery.Each of these control parameters has a defaultvalue, which will be assumed if unspecified.
Notethat the relevance of these parameters is transitiondependent.
E.g., a SHIFT requires no such controlparameters and a REDUCE needs neither B nor S.These control parameters allow limited com-positionality of the basic transitions in Figure 2.Unlike G?omez-Rodr?
?guez and Nivre (2013), eachtransition includes at most one SHIFT, at most oneREDUCE, and at most one LEFT-ARC or RIGHT-ARC.
I.e., the most compositional transition is aLEFT/RIGHT-ARC with a REDUCE and/or a SHIFT.Even with this restriction, all of the transitionsystems covered by G?omez-Rodr?
?guez and Nivre(2013) can still be expressed in our generalization.3.5 Generalized Transition SystemTo summarize, a generalized transition is definedas follows:1.
A parser state: ?
= ?O,U,A?.2.
A set of basic transitions: T = {?i,j,?i,j,?i,+}.And each transition system instantiation must fur-ther define:1.
Values for global control parameters: K andD.2.
A multiset of valid transitionsM(T ), where?t ?M(T ), then t ?
T .3.
For each t ?M(T ) values for B, S, and P .4 Transition System InstantiationsThe instantiation of the transition system consistsof setting the capacityK and distanceD as well asthe transition control parameters.
In order to makethe comparison clearer, we will define typical tran-sition systems using the notation of Kuhlmann etal.
(2011).
Here, a parser state is a triple (?, ?,A),where ?|i is a stack with top element i, j|?
is abuffer whose next element is j, and A the set ofcreated arcs.
To make notation cleaner, we willdrop indexes whenever the context makes it clear.E.g., if the active token capacity is 2 (K = 2),then necessarily for ?i,j, i = 2 and j = 1 andwe can write?.
When K > 2 or D > 1, a basearc-creation action can be instantiated into multi-ple transitions that only differ by the indexes.
E.g.,when K = 3 and D = 2, ?
with P = nacan have three instantiations: ?3,2, ?2,1and153?3,1.
To keep exposition compact, in such cir-cumstances we use ?
to denote the set of index-pairs allowed by the given K, D and P values.4.1 Arc-standardArc-standard parsing is a form of shift-reduceparsing where arc-creations actions happen be-tween the top two elements on the stack:LEFT-ARC: (?|i|j, ?,A)?
(?|j, ?,A ?
(j ?
i))RIGHT-ARC: (?|i|j, ?,A)?
(?|i, ?,A ?
(i?
j))SHIFT: (?, i|?,A)?
(?|i, ?, A)This is easily handled in our generalization by hav-ing only two active tokens.
?
and ?
actionswith default parameter values are used to simulateLEFT-ARC and RIGHT-ARC respectively, and + isused to shift tokens from U to O.M(T ) base parameter valuesLEFT-ARC ?
{default}RIGHT-ARC ?
{default}SHIFT +capacity K 2arc distance D 1Note that when K = 2, arc-creations are by defi-nition always on the periphery.4.2 Arc-eagerArc-eager transition systems have been de-scribed in various ways.
Kuhlmann et al(2011) defines it as operations between tokensat the top of the stack and front of the buffer.LEFT-ARC: (?|i, j|?,A)?
(?, j|?,A ?
(j ?
i))RIGHT-ARC: (?|i, j|?,A)?
(?|i|j, ?,A ?
(i?
j))REDUCE: (?|i, ?, A)?
(?, ?,A)SHIFT: (?, i|?,A)?
(?|i, ?, A)In our generalization, we can simulate this by hav-ing two active tokens on the operative set, repre-senting the top of the stack and front of the buffer.SHIFT and LEFT-ARC are handled in the same wayas in the arc-standard system.
The RIGHT-ARC ac-tion is simulated by ?
with modified parametersto account for the fact that the action keeps themodifier in O and also shifts a token from U toO.
The REDUCE action is handled by ?
with theperiphery set to left so as to remove the secondrightmost token from O.M(T ) base parameter valuesLEFT-ARC ?
{default}RIGHT-ARC ?
{B = f, S = t}REDUCE ?
{P = l}SHIFT +K 2D 1Note that the artificial root must be on the right ofthe sentence to permit the reduce to operate at theleft periphery of the active token set.4.3 Easy-firstFor easy-first parsing (Goldberg and Elhadad,2010), the number of active tokens is infinite or,more precisely, equals to the number of tokens inthe input sentence, and arc-creation actions canhappen between any two adjacent tokens.M(T ) base parameter valuesLEFT-ARC ??
{default}RIGHT-ARC ??
{default}K ?D 1Note here ?
denotes the set of indexes {(i, j)|i ?|O|, j ?
1, i = j + 1}.
Thus, at each time stepthere are 2?|O| actions that need to be considered.Additionally, reduce is always composed with arc-creation and since K = ?, then there is nevera SHIFT operation as O is immediately populatedwith all tokens on the start of parsing.4.4 Kuhlmann et al (2011)Kuhlmann et al present a ?hybrid?
transition sys-tem where the RIGHT-ARC action is arc-standardin nature, but LEFT-ARC actions is arc-eager in na-ture, which is equivalent to the system of Yamadaand Matsumoto (2003).
We can get the same ef-fect as their system by allowing three active to-kens, representing the top two tokens of the stackand the front of the buffer.
Transitions can be han-dled similarly as the arc-standard system whereonly the periphery parameter need to be changedaccordingly.
This change also requires the root tobe on the right of the sentence.M(T ) base parameter valuesLEFT-ARC ?
{P = r}RIGHT-ARC ?
{P = l}SHIFT +K 3D 11544.5 Sagae and Tsujii (2008)Sagae and Tsuji present a model for projectiveDAG parsing by modifying the LEFT-ARC andRIGHT-ARC transitions of the arc-eager system.In their system, the LEFT-ARC transition does notremove the dependent, and RIGHT-ARC transitiondoes not shift any token from the input to the stack.LEFT-ARC: (?|i, j|?,A)?
(?|i, j|?,A ?
(j ?
i))RIGHT-ARC: (?|i, j|?,A)?
(?|i, j|?,A ?
(i?
j))REDUCE: (?|i, ?, A)?
(?, ?,A)SHIFT: (?, i|?,A)?
(?|i, ?, A)This can be easily simulated by modifying arc-eager system mentioned in Sec.
4.2 such that boththe LEFT-ARC and RIGHT-ARC transition keep thestack/buffer untouched.M(T ) base parameter valuesLEFT-ARC ?
{B = f}RIGHT-ARC ?
{B = f}REDUCE ?
{P = l}SHIFT +K 2D 14.6 Attardi (2006)Now we show how our framework can extend tonon-projective systems.
This is primarily con-trolled by the arc-distance parameter D.The base of the Attardi non-projective transitionsystem is the arc-standard system.
Attardi mod-ifies RIGHT/LEFT-ARC actions that operate overlarger contexts in the stack.
For simplicity of ex-position below we model the variant of the Attardisystem described in Cohen et al (2011).RIGHT-ARCN: (?|i1+N| .
.
.
|i2|i1, ?, A)?
(?|iN| .
.
.
|i2|i1, ?, A ?
(i1?
i1+N))LEFT-ARCN: (?|i1+N| .
.
.
|i2|i1, ?, A)?
(?|i1+N| .
.
.
|i2, ?, A ?
(i1+N?
i1))SHIFT: (?, i|?,A)?
(?|i, ?, A)Conceptually the Attardi system can be gener-alized to any value of N2, and Attardi specif-ically allows N=1,2,3.
Actions that create arcsbetween non-adjacent tokens permit limited non-projectivity.Thus, for Attardi, we set the number of activetokens to N+1, to simulate the top N+1 tokensof the stack, and set the max distance D to N to2Attardi (2006) also introduces Extract/Insert actions to atemporary buffer that he argues generalizes to all values of N.We don?t account for that specific generalization here.indicate that tokens up to N positions below thetop of the stack can add arcs with the top of thestack.M(T ) base parameter valuesLEFT-ARC ??
{P = r}RIGHT-ARC ??
{P = r}K N + 1D N (3 for Attardi (2006))The critical parameter for each arc action isthat P = r. This means that the right pe-ripheral active token always must participate inthe action, as does the right-most token of thestack for the original Attardi.
Here ?
denotes{(i, j)|j = 1, i?
j ?
D}.5 Novel Transition SystemsAny valid setting of the control parameters couldtheoretically define a new transition system, how-ever not all such combinations will be empiricallyreasonable.
We outline two potential novel transi-tion systems suggested by our framework, whichwe will experiment with in Section 6.
This is akey advantage of our framework (and implemen-tation) ?
it provides an easy experimental solutionto explore novel transition systems.5.1 Bounded Capacity Easy-firstEasy-first and arc-standard are similar since theyare both bottom-up and both create arcs betweenadjacent nodes.
The main difference lies in the ca-pacity K, which is 2 for arc-standard and ?
foreasy-first.
In addition, the shift action is neededby the arc-standard system.
Each system has someadvantages: arc-standard is faster, and somewhateasier to train, while easy-first can be more accu-rate (under identical learning settings).
Seen thisway, it is natural to ask: what happens if the ac-tive token range K is set to k, with 2 < k < ?
?We explore various values in the region betweenarc-standard and easy-first in Section 6.M(T ) base parameter valuesLEFT-ARC ??
{default}RIGHT-ARC ??
{default}SHIFT +K kD 15.2 Non-projective Easy-firstA simple observation is that by allowing D >1 makes any transition system naturally non-155projective.
One example would be a non-projective variant of easy-first parsing:M(T ) base parameter valuesLEFT-ARC ??
{default}RIGHT-ARC ??
{default}K ?D any value of NHere N denotes the maximum arc-creation dis-tance.We also note that one could potentially varyboth K and D simultaneously, giving a non-projective limited capacity easy-first system.6 Implementation and ExperimentsOur implementation uses a linear model?yi?yw ?f(x, y1, .
.
.
, yi) to assign a score to a sequence y =y1, y2, .
.
.
ymof parser transitions, given sentencex.
Model parameters are trained using the struc-tured perceptron with ?early update?
(Collins andRoark, 2004) and features follow that of Zhangand Nivre (2011).For the arc-standard and arc-eager transitionsystems, we use the static oracle to derive a singlegold sequence for a given sentence and its goldtree.
For systems where there is no such staticoracle, for example the easy-first system, we usethe method proposed by Ma et al (2013) to se-lect a gold sequence such that, for each update,the condition w ?
f(x, y?k) < w ?
f(x, yk) alwaysholds, which is required for perceptron conver-gence.
Here y?kdenotes the length k prefix of acorrect sequence and ykdenotes the highest scor-ing sequence in the beam.We carry out the experiments on the Wall StreetJournal using the standard splits for the trainingset (section 2-21), development set (section 22)and test set (section 23).
We converted the con-stituency trees to Stanford dependencies version3.3.0 (de Marneffe et al, 2006).
We used a CRF-based Part-of-Speech tagger to generate 5-foldjack-knifed Part-of-Speech tag annotation of thetraining set and used predicted tags on the devel-opment and test set.
The tagger reaches accuracyscores similar to the Stanford tagger (Toutanova etal., 2003) with 97.44% on the test set.
The unla-beled and labeled accuracy scores exclude punctu-ation marks.Obviously, there are many interesting instantia-tions for the generalized transition system.
In par-ticular, it would be interesting to investigate pars-ing performance of systems with different active0 10 20 30 40 50 60 708889909192KUASLASFigure 3: Labeled/unlabeled attachment scoreswith respect to the active capacity K.token size and arc-distance.
Before we investigatethese system in the next subsections, we presentthe performance on standard systems.6.1 Common SystemsThe results for arc-standard, arc-eager and easy-first (bottom of Table 1) show how standard sys-tems perform within our framework.
Easy-first?slabeled attachment score (LAS) is 0.46 higher thanthe LAS of arc-eager when using the same featureset.
These results are competitive with the state-of-the-art linear parsers, but below recent work onneural network parsers.
A future line of work is toadopt such training into our generalization.System UAS LAS bDyer et al (2015) 93.10 90.90 ?Weiss et al (2015) 93.99 92.05 ?Alberti et al (2015) 94.23 92.23 8Zhang and Nivre (2011)?
92.92 90.88 32Zhang and McDonald (2014) 93.22 91.02 ?Arc-standard (gen.) 92.81 90.68 32Arc-eager (gen.) 92.88 90.73 32Easy-first (gen.) 93.31 91.19 32Table 1: State-of-the-art comparison.
?
denotesour own re-implementation.
The systems in thefirst block on the top use neural networks.6.2 Bounded Capacity Easy-firstTable 1 shows that easy-first is more accurate thanarc-standard.
However, it is also more computa-tionally expensive.
By varying the number of ac-tive tokens, we can investigate whether there is asweet spot in the accuracy vs. speed trade-off.Figure 3 shows labeled/unlabeled accuracyscores on the development set for active token156sizes ranging from 2 to 32, all with beam size1.
Different from the original easy-first systemwhere all tokens are initialized as active tokens,in the bounded capacity system, a token can be ac-tive only after it has been shifted from U to O.We observe that the accuracy score increases re-markably by over a point when active token ca-pacity gets increased from 2 to 3, and peaks at aactive token capacity of 4.
Generally, more activetokens allows the parser to delay ?difficult?
deci-sions to later steps and choose the ?easy?
ones atearly steps.
Such behavior has the effect of limit-ing the extent of error propagation.
The result alsosuggests that a modification as simple as addingone more active token to the arc-standard systemcan yield significant improvement.With a larger active token capacity, we see aslight drop of accuracy.
This is likely related tothe parser having to predict when to perform ashift transition.
In comparison, the vanilla easy-first parser does not need to model this.6.3 Non-projective Easy-firstFor the experiments with non-projective easy-first,we use the Dutch and Danish CoNLL 2006 cor-pora.
To assess the performance, we applied theevaluation rules of the 2006 shared task.
In orderto make the non-projective systems perform well,we added to all feature templates the arc-distanceD.
In these experiments, we included in the train-ing an artificial root node on the right since Dutchas well a few sentences of the Danish corpus havemore than one root node.In the experiments, we use the easy-first settingwith infinite set of active tokens K and increasestepwise the arc-distance D. For training, we fil-ter out the sentences which contain non-projectivearcs not parseable with the selected setting.
Ta-ble 2 provides an overview of the performancewith increasing arc-distance.
At the bottom of thetable, we added accuracy scores for the boundedcapacity non-projective easy-first parser since wethink these settings provide attractive trade-offsbetween accuracy and complexity.The original easy-first performs O(n) featureextractions and has a runtime of O(n log(n)) as-suming a heap is used to extract the argmax at eachstep and feature extraction is done over local con-texts only (Goldberg and Elhadad, 2010).
For thenon-projective variant of easy-first with D = d,O(dn) feature extractions are required.
Thus, forthe unrestricted variant where K = D = ?,Danish DutchD K UAS LAS CUR NPS UAS LAS CUR NPS1 ?
90.22 85.51 41.30 84.37 79.11 75.41 32.45 63.552 ?
90.28 85.85 59.78 96.91 84.73 81.01 70.59 92.443 ?
90.68 86.07 65.22 98.82 85.03 81.65 77.99 99.014 ?
90.58 85.53 69.57 99.69 85.99 82.73 76.85 99.895 ?
90.84 86.11 65.22 99.88 85.21 81.93 76.09 99.966 ?
90.78 86.31 68.48 99.94 84.57 81.13 75.90 100.07 ?
90.64 85.91 63.04 100.0 85.07 82.01 77.04 100.04 5 90.74 85.87 66.30 99.69 86.51 82.91 76.66 99.895 6 91.00 86.21 72.83 99.88 86.03 82.73 76.09 99.96Table 2: Experiments with non-projective easy-first and bounded capacity easy-first with D thearc-distance, K the active token capacity (?
= alltokens of a sentence), UAS and LAS are the un-labeled and labeled accuracy scores, CUR is therecall of crossing edges and NPS shows the per-centage of sentences covered in the training setwhere 100% means all non-projective (and projec-tive) sentences in the training can be parsed andare included in training.O(n2) feature extractions are required.
Table 2explored more practical settings: when K = ?,D ?
7, the number of feature extractions is backto O(n) with a runtime of O(n log(n)), matchingthe original easy-first complexity.
When both Kand D are small constants as in the lower portionof Table 2, the runtime is O(n).7 Comparison with GR&N13The work most similar to ours is that of G?omez-Rodr?
?guez and Nivre (2013) (GR&N13).
They de-fine a divisible transition system with the principleto divide the transitions into elementary transitionsand then to compose from these elementary transi-tions complex transitions.
GR&N13 identified thefollowing five elementary transitions:SHIFT: (?, i|?,A)?
(?|i, ?, A)UNSHIFT: (?|i, ?, A)?
(?, i|?,A)REDUCE: (?|i, ?, A)?
(?, ?,A)LEFT-ARC: (?|i, j|?,A)?
(?|i, j|?,A ?
(j ?
i))RIGHT-ARC: (?|i, j|?,A)?
(?|i, j|?,A ?
(i?
j))The notion of function composition is used tocombine the elementary transitions to the complexones.
For instance, arc-standard would have threeactions: SHIFT; RIGHT-ARC ?REDUCE; LEFT-ARC ?REDUCE.The first difference we can note is the GR&N13cannot instantiate transition systems that producenon-projective trees.
This is surely a superficial157difference, as GR&N13 could easily add transi-tions with larger arc-distance or even SWAP ac-tions (Nivre, 2009).However, a less superficial difference is thatour generalization uses control parameters to con-struct instantiations of transition systems, insteadof solely via transition composition like GR&N13.Through this, our generalization results in a min-imal departure from ?standard?
representations ofthese systems.
While this may seem like a nota-tional difference, this is particularly a benefit withrespect to implementation, as previous techniquesfor classification and feature extraction can largelybe reused.For example, in GR&N13, the definition ofthe easy-first transition system (Goldberg and El-hadad, 2010) is complex, e.g., a RIGHT-ARC atposition i requires a compositional transition of iSHIFT actions, a RIGHT-ARC, a SHIFT, a REDUCE,then i UNSHIFT actions.
Note, that this means inany implementation of this generalization, the out-put space for a classifier will be very large.
Fur-thermore, the feature space would ultimately needto take the entire sentence into consideration, con-sidering that all compositional actions are centeredon the same state.In our transition system, on the other hand,easy-first operates almost as it does in its nativeform, where n LEFT-ARC and n RIGHT-ARC ac-tions are ranked relative to each other.
There areonly two actions, each instantiated for every loca-tion in the state.
Thus the output space and featureextraction are quite natural.This leads to straight-forward implementationsallowing for easy experimentation and discovery.Unlike GR&N13, we present empirical results forboth known transition systems as well as somenovel systems (Section 6).8 ConclusionWe presented a generalized transition system thatis capable of representing and executing a widerange of transition systems within one single im-plementation.
These transition systems includesystems such as arc-standard, arc-eager, easy-first.Transitions can be freely composed of elemen-tary operations.
The transition system shows per-fect alignment between the elementary operationson one hand and their preconditions and the oracleon the other hand.
We adjust the transition systemto work on a stack in a uniform way starting at anode on the stack and ending with the top node ofthe stack.
The results produced by this system aremore comparable as they can be executed with thesame classifier and feature extraction system.Finally, we would like to highlight two insightsthat the experiments provide.
First, a few more ac-tive tokens than two can boost the accuracy levelof an arc-standard transition system towards thelevel of an easy-first transition system.
These pars-ing systems maintain very nicely the linear com-plexity of the arc-standard transition system whilethey provide a higher accuracy similar to thoseof easy-first.
Second, non-projective trees can beparsed by allowing a larger arc-distance which isa simple way to allow for non-projective edges.We think that the transition systems with moreactive tokens or the combination with edges thatspan over more words provide very attractive tran-sition systems for possible future parsers.AcknowledgmentsWe would like to thank the anonymous reviewers,Slav Petrov and Michael Collins for valuable com-ments on earlier versions of this manuscript.ReferencesSteven Abney.
1991.
Parsing by chunks.
In RobertBerwick, Steven Abney, and Carol Tenny, editors,Principle-Based Parsing, pages 257?278.
Kluwer.Chris Alberti, David Weiss, Greg Coppola, and SlavPetrov.
2015.
Improved transition-based parsingand tagging with neural networks.
In Proceedingsof EMNLP 2015.Giuseppe Attardi.
2006.
Experiments with a multi-language non-projective dependency parser.
In Pro-ceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL), pages 166?170.Miguel Ballesteros and Joakim Nivre.
2013.
Goingto the roots of dependency parsing.
ComputationalLinguistics, 39(1):5?13.Bernd Bohnet and Jonas Kuhn.
2012.
The best ofboth worlds ?
a graph-based completion model fortransition-based parsers.
In Proceedings of the 13thConference of the European Chpater of the Associ-ation for Computational Linguistics (EACL), pages77?87.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-ceedings of the 2012 Joint Conference on Empirical158Methods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 1455?1465.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Empirical Methods in Natural LanguageProcessing.Jinho D Choi and Martha Palmer.
2011.
Getting themost out of transition-based dependency parsing.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers-Volume 2.Shay B Cohen, Carlos G?omez-Rodr?
?guez, and GiorgioSatta.
2011.
Exact inference for generative proba-bilistic non-projective dependency parsing.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages1234?1245.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages112?119.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th International Conference onLanguage Resources and Evaluation (LREC).Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of ACL 2015, pages334?343.
Association for Computational Linguis-tics.Yoav Goldberg and Michael Elhadad.
2010.
An effi-cient algorithm for easy-first non-directional depen-dency parsing.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL HLT), pages 742?750.Carlos G?omez-Rodr?
?guez and Joakim Nivre.
2010.A transition-based parser for 2-planar dependencystructures.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 1492?1501.Carlos G?omez-Rodr?
?guez and Joakim Nivre.
2013.Divisible transition systems and multiplanar de-pendency parsing.
Computational Linguistics,39(4):799?845.Johan Hall, Jens Nilsson, Joakim Nivre, G?ulsenEryi?git, Be?ata Megyesi, Mattias Nilsson, andMarkus Saers.
2007.
Single malt or blended?
Astudy in multilingual parser optimization.
In Pro-ceedings of the CoNLL Shared Task of EMNLP-CoNLL 2007, pages 933?939.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2012.
Incremental joint approachto word segmentation, pos tagging, and dependencyparsing in chinese.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 1045?1053.Sandra K?ubler, Ryan McDonald, and Joakim Nivre.2009.
Dependency Parsing.
Morgan and Claypool.Taku Kudo and Yuji Matsumoto.
2000.
Japanese de-pendency structure analysis based on support vec-tor machines.
In Proceedings of the Joint SIGDATConference on Empirical Methods in NLP and VeryLarge Corpora, pages 18?25.Marco Kuhlmann, Carlos G?omez-Rodr?
?guez, and Gior-gio Satta.
2011.
Dynamic programming algorithmsfor transition-based dependency parsers.
In Pro-ceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages673?682.Ji Ma, Jingbo Zhu, Tong Xiao, and Nan Yang.
2013.Easy-first pos tagging and dependency parsing withbeam search.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 110?114,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006a.Maltparser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (LREC), pages 2216?2219.Joakim Nivre, Johan Hall, Jens Nilsson, G?ulsenEryi?git, and Svetoslav Marinov.
2006b.
Labeledpseudo-projective dependency parsing with supportvector machines.
In Proceedings of the 10th Confer-ence on Computational Natural Language Learning(CoNLL), pages 221?225.Joakim Nivre.
2003.
An efficient algorithm for pro-jective dependency parsing.
In Proceedings of the8th International Workshop on Parsing Technologies(IWPT), pages 149?160.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34:513?553.Joakim Nivre.
2009.
Non-projective dependency pars-ing in expected linear time.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP (ACL-IJCNLP), pages 351?359.Emily Pitler and Ryan McDonald.
2015.
A linear-time transition system for crossing interval trees.
InHuman Language Technologies: The 2015 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics (NAACLHLT), pages 662?671.159Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
MachineLearning, 34:151?175.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-reducedependency DAG parsing.
In Proceedings of the22nd International Conference on ComputationalLinguistics (COLING), pages 753?760.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology- Volume 1, NAACL ?03, pages 173?180, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural net-work transition-based parsing.
In Proceedings ofACL 2015, pages 323?333.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statis-tical dependency analysis with support vector ma-chines.
In Proceedings of the 8th InternationalWorkshop on Parsing Technologies (IWPT), pages195?206.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-basedand transition-based dependency parsing.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing (EMNLP), pages562?571.Hao Zhang and Ryan McDonald.
2014.
Enforcingstructural diversity in cube-pruned dependency pars-ing.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 2: Short Papers), pages 656?661, Baltimore,Maryland, June.
Association for Computational Lin-guistics.Yue Zhang and Joakim Nivre.
2011.
Transition-basedparsing with rich non-local features.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics (ACL).160
