Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 488?495,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsThe utility of parse-derived features for automatic discourse segmentationSeeger Fisher and Brian RoarkCenter for Spoken Language Understanding, OGI School of Science & EngineeringOregon Health & Science University, Beaverton, Oregon, 97006 USA{fishers,roark}@cslu.ogi.eduAbstractWe investigate different feature sets forperforming automatic sentence-level dis-course segmentation within a general ma-chine learning approach, including featuresderived from either finite-state or context-free annotations.
We achieve the best re-ported performance on this task, and demon-strate that our SPADE-inspired context-freefeatures are critical to achieving this level ofaccuracy.
This counters recent results sug-gesting that purely finite-state approachescan perform competitively.1 IntroductionDiscourse structure annotations have been demon-strated to be of high utility for a number of NLPapplications, including automatic text summariza-tion (Marcu, 1998; Marcu, 1999; Cristea et al,2005), sentence compression (Sporleder and Lap-ata, 2005), natural language generation (Prasad etal., 2005) and question answering (Verberne et al,2006).
These annotations include sentence segmen-tation into discourse units along with the linkingof discourse units, both within and across sentenceboundaries, into a labeled hierarchical structure.
Forexample, the tree in Figure 1 shows a sentence-leveldiscourse tree for the string ?Prices have dropped butremain quite high, according to CEO Smith,?
whichhas three discourse segments, each labeled with ei-ther ?Nucleus?
or ?Satellite?
depending on how cen-tral the segment is to the coherence of the text.There are a number of corpora annotated withdiscourse structure, including the well-known RSTTreebank (Carlson et al, 2002); the DiscourseGraphBank (Wolf and Gibson, 2005); and the PennDiscourse Treebank (Miltsakaki et al, 2004).
Whilethe annotation approaches differ across these cor-pora, the requirement of sentence segmentation intoRootHHHHHNucleusHHHHNucleusPPPPPrices have droppedSatellitePPPPbut remain quite highSatellitePPPPPaccording to CEO SmithFigure 1: Example Nucleus/Satellite labeled sentence-leveldiscourse tree.sub-sentential discourse units is shared across all ap-proaches.
These resources have facilitated researchinto stochastic models and algorithms for automaticdiscourse structure annotation in recent years.Using the RST Treebank as training and evalua-tion data, Soricut and Marcu (2003) demonstratedthat their automatic sentence-level discourse pars-ing system could achieve near-human levels of ac-curacy, if it was provided with manual segmenta-tions and manual parse trees.
Manual segmenta-tion was primarily responsible for this performanceboost over their fully automatic system, thus mak-ing the case that automatic discourse segmentation isthe primary impediment to high accuracy automaticsentence-level discourse structure annotation.
Theirmodels and algorithm ?
subsequently packaged to-gether into the publicly available SPADE discourseparser1 ?
make use of the output of the Charniak(2000) parser to derive syntactic indicator featuresfor segmentation and discourse parsing.Sporleder and Lapata (2005) also used the RSTTreebank as training data for data-driven discourseparsing algorithms, though their focus, in contrastto Soricut and Marcu (2003), was to avoid context-free parsing and rely exclusively on features in theirmodel that could be derived via finite-state chunkersand taggers.
The annotations that they derive are dis-1http://www.isi.edu/publications/licensed-sw/spade/488course ?chunks?, i.e., sentence-level segmentationand non-hierarchical nucleus/span labeling of seg-ments.
They demonstrate that their models achievecomparable results to SPADE without the use of anycontext-free features.
Once again, segmentation isthe part of the process where the automatic algo-rithms most seriously underperform.In this paper we take up the question posed bythe results of Sporleder and Lapata (2005): howmuch, if any, accuracy reduction should we expectif we choose to use only finite-state derived fea-tures, rather than those derived from full context-free parses?
If little accuracy is lost, as their re-sults suggest, then it would make sense to avoid rel-atively expensive context-free parsing, particularlyif the amount of text to be processed is large or ifthere are real-time processing constraints on the sys-tem.
If, however, the accuracy loss is substantial,one might choose to avoid context-free parsing onlyin the most time-constrained scenarios.While Sporleder and Lapata (2005) demonstratedthat their finite-state system could perform as well asthe SPADE system, which uses context-free parsetrees, this does not directly answer the question ofthe utility of context-free derived features for thistask.
SPADE makes use of a particular kind of fea-ture from the parse trees, and does not train a gen-eral classifier making use of other features beyondthe parse-derived indicator features.
As we shallshow, its performance is not the highest that can beachieved via context-free parser derived features.In this paper, we train a classifier using a gen-eral machine learning approach and a range of finite-state and context-free derived features.
We investi-gate the impact on discourse segmentation perfor-mance when one feature set is used versus another,in such a way establishing the utility of features de-rived from context-free parses.
In the course of sodoing, we achieve the best reported performance onthis task, an absolute F-score improvement of 5.0%over SPADE, which represents a more than 34% rel-ative error rate reduction.By focusing on segmentation, we provide an ap-proach that is generally applicable to all of thevarious annotation approaches, given the similari-ties between the various sentence-level segmenta-tion guidelines.
Given that segmentation has beenshown to be a primary impediment to high accu-racy sentence-level discourse structure annotation,this represents a large step forward in our ability toautomatically parse the discourse structure of text,whatever annotation approach we choose.2 Methods2.1 DataFor our experiments we use the Rhetorical StructureTheory Discourse Treebank (Carlson et al, 2002),which we will denote RST-DT, a corpus annotatedwith discourse segmentation and relations accordingto Rhetorical Structure Theory (Mann and Thomp-son, 1988).
The RST-DT consists of 385 docu-ments from the Wall Street Journal, about 176,000words, which overlaps with the Penn Wall St. Jour-nal (WSJ) Treebank (Marcus et al, 1993).The segmentation of sentences in the RST-DTis into clause-like units, known as elementary dis-course units, or edus.
We will use the two terms?edu?
and ?segment?
interchangeably throughout therest of the paper.
Human agreement for this segmen-tation task is quite high, with agreement betweentwo annotators at an F-score of 98.3 for unlabeledsegmentation (Soricut and Marcu, 2003).The RST-DT corpus annotates edu breaks, whichtypically include sentence boundaries, but sentenceboundaries are not explicitly annotated in the corpus.To perform sentence-level processing and evalua-tion, we aligned the RST-DT documents to the samedocuments in the Penn WSJ Treebank, and used thesentence boundaries from that corpus.2 An addi-tional benefit of this alignment is that the Penn WSJTreebank tokenization is then available for parsingpurposes.
Simple minimum edit distance alignmenteffectively allowed for differences in punctuationrepresentation (e.g., double quotes) and tokenizationwhen deriving the optimal alignment.The RST-DT corpus is partitioned into a train-ing set of 347 documents and a test set of 38 doc-uments.
This test set consists of 991 sentences with2,346 segments.
For training purposes, we createda held-out development set by selecting every tenthsentence of the training set.
This development setwas used for feature development and for selectingthe number of iterations used when training models.2.2 EvaluationPrevious research into RST-DT segmentation andparsing has focused on subsets of the 991 sentencetest set during evaluation.
Soricut and Marcu (2003)2A small number of document final parentheticals are in theRST-DT and not in the Penn WSJ Treebank, which our align-ment approach takes into account.489omitted sentences that were not exactly spanned bya subtree of the treebank, so that they could fo-cus on sentence-level discourse parsing.
By ourcount, this eliminates 40 of the 991 sentences in thetest set from consideration.
Sporleder and Lapata(2005) went further and established a smaller sub-set of 608 sentences, which omitted sentences withonly one segment, i.e., sentences which themselvesare atomic edus.Since the primary focus of this paper is on seg-mentation, there is no strong reason to omit any sen-tences from the test set, hence our results will eval-uate on all 991 test sentences, with two exceptions.First, in Section 2.3, we compare SPADE results un-der our configuration with results from Sporlederand Lapata (2005) in order to establish compara-bility, and this is done on their 608 sentence sub-set.
Second, in Section 3.2, we investigate feed-ing our segmentation into the SPADE system, in or-der to evaluate the impact of segmentation improve-ments on their sentence-level discourse parsing per-formance.
For those trials, the 951 sentence subsetfrom Soricut and Marcu (2003) is used.
All othertrials use the full 991 sentence test set.Segmentation evaluation is done with precision,recall and F1-score of segmentation boundaries.Given a word string w1 .
.
.
wk, we can index wordboundaries from 0 to k, so that each word wi fallsbetween boundaries i?1 and i.
For sentence-basedsegmentation, indices 0 and k, representing the be-ginning and end of the string, are known to be seg-ment boundaries.
Hence Soricut and Marcu (2003)evaluate with respect to sentence internal segmenta-tion boundaries, i.e., with indices j such that 0<j<kfor a sentence of length k. Let g be the numberof sentence-internal segmentation boundaries in thegold standard, t the number of sentence-internal seg-mentation boundaries in the system output, and mthe number of correct sentence-internal segmenta-tion boundaries in the system output.
ThenP = mt R =mg and F1 =2PRP+R =2mg+tIn Sporleder and Lapata (2005), they were pri-marily interested in labeled segmentation, where thesegment initial boundary was labeled with the seg-ment type.
In such a scenario, the boundary at in-dex 0 is no longer known, hence their evaluation in-cluded those boundaries, even when reporting un-labeled results.
Thus, in section 2.3, for compar-ison with reported results in Sporleder and Lapata(2005), our F1-score is defined accordingly, i.e., seg-Segmentation system F1Sporleder and Lapata best (reported) 88.40SPADESporleder and Lapata configuration (reported): 87.06current configuration: 91.04Table 1: Segmentation results on the Sporleder and Lapata(2005) data set, with accuracy defined to include sentence initialsegmentation boundaries.mentation boundaries j such that 0 ?
j < k.In addition, we will report unlabeled bracketingprecision, recall and F1-score, as defined in thePARSEVAL metrics (Black et al, 1991) and eval-uated via the widely used evalb package.
We alsouse evalb when reporting labeled and unlabeled dis-course parsing results in Section 3.2.2.3 Baseline SPADE setupThe publicly available SPADE package, which en-codes the approach in Soricut and Marcu (2003),is taken as the baseline for this paper.
We madeseveral modifications to the script from the default,which account for better baseline performance thanis achieved with the default configuration.
First, wemodified the script to take given parse trees as input,rather than running the Charniak parser itself.
Thisallowed us to make two modifications that improvedperformance: turning off tokenization in the Char-niak parser, and reranking.
The default script thatcomes with SPADE does not turn off tokenizationinside of the parser, which leads to degraded perfor-mance when the input has already been tokenized inthe Penn Treebank style.
Secondly, Charniak andJohnson (2005) showed how reranking of the 50-best output of the Charniak (2000) parser gives sub-stantial improvements in parsing accuracy.
Thesetwo modifications to the Charniak parsing outputused by the SPADE system lead to improvementsin its performance compared to previously reportedresults.Table 1 compares segmentation results of threesystems on the Sporleder and Lapata (2005) 608sentence subset of the evaluation data: (1) their bestreported system; (2) the SPADE system results re-ported in that paper; and (3) the SPADE system re-sults with our current configuration.
The evaluationuses the unlabeled F1 measure as defined in that pa-per, which counts sentence initial boundaries in thescoring, as discussed in the previous section.
As canbe seen from these results, our improved configu-ration of SPADE gives us large improvements overthe previously reported SPADE performance on thissubset.
As a result, we feel that we can use SPADE490as a very strong baseline for evaluation on the entiretest set.Additionally, we modified the SPADE script to al-low us to provide our segmentations to the full dis-course parsing that it performs, in order to evalu-ate the improvements to discourse parsing yieldedby any improvements to segmentation.2.4 Segmentation classifierFor this paper, we trained a binary classifier, whichwas applied independently at each word wi in thestring w1 .
.
.
wk, to decide whether that word is thelast in a segment.
Note that wk is the last word inthe string, and is hence ignored.
We used a log-linear model with no Markov dependency betweenadjacent tags,3 and trained the parameters of themodel with the perceptron algorithm, with averag-ing to control for over-training (Collins, 2002).Let C={E, I} be the set of classes: seg-mentation boundary (E) or non-boundary (I).
Letf(c, i, w1 .
.
.
wk) be a function that takes as in-put a class value c, a word index i and the wordstring w1 .
.
.
wk and returns a d-dimensional vectorof feature values for that word index in that stringwith that class.
For example, one feature might be(c = E,wi = the), which returns the value 1 whenc = E and the current word is ?the?, and returns0 otherwise.
Given a d-dimensional parameter vec-tor ?, the output of the classifier is that class whichmaximizes the dot product between the feature andparameter vectors:c?
(i, w1 .
.
.
wk) = argmaxc?C?
?
f(c, i, w1 .
.
.
wk) (1)In training, the weights in ?
are initialized to 0.For m epochs (passes over the training data), foreach word in the training data (except sentence finalwords), the model is updated.
Let i be the currentword position in string w1 .
.
.
wk and suppose thatthere have been j?1 previous updates to the modelparameters.
Let c?i be the true class label, and let c?ibe shorthand for c?
(i, w1 .
.
.
wk) in equation 1.
Thenthe parameter vector ?j at step j is updated as fol-lows:?j = ?j?1 ?
f(c?, i, w1 .
.
.
wk) + f(c?, i, w1 .
.
.
wk) (2)As stated in Section 2.1, we reserved every tenthsentence as held-out data.
After each pass over thetraining data, we evaluated the system performance3Because of the sparsity of boundary tags, Markov depen-dencies between tags buy no additional system accuracy.on this held-out data, and chose the model that op-timized accuracy on that set.
The averaged percep-tron was used on held-out and evaluation sets.
SeeCollins (2002) for more details on this approach.2.5 FeaturesTo tease apart the utility of finite-state derived fea-tures and context-free derived features, we considerthree feature sets: (1) basic finite-state features; (2)context-free features; and (3) finite-state approxima-tion to context-free features.
Note that every featuremust include exactly one class label c in order todiscriminate between classes in equation 1.
Hencewhen presenting features, it can be assumed that theclass label is part of the feature, even if it is not ex-plicitly mentioned.The three feature sets are not completely disjoint.We include simple position-based features in everysystem, defined as follows.
Because edus are typi-cally multi-word strings, it is less likely for a wordnear the beginning or end of a sentence to be at anedu boundary.
Thus it is reasonable to expect theposition within a sentence of a token to be a helpfulfeature.
We created 101 indicator features, repre-senting percentages from 0 to 100.
For a string oflength k, at position i, we round i/k to two decimalplaces and provide a value of 1 for the correspondingquantized position feature and 0 for the other posi-tion features.2.5.1 Basic finite-state featuresOur baseline finite-state feature set includes simpletagger derived features, as well as features based onposition in the string and n-grams4.
We annotatetag sequences onto the word sequence via a compet-itive discriminatively trained tagger (Hollingsheadet al, 2005), trained for each of two kinds of tagsequences: part-of-speech (POS) tags and shallowparse tags.
The shallow parse tags define non-hierarchical base constituents (?chunks?
), as definedfor the CoNLL-2000 shared task (Tjong Kim Sangand Buchholz, 2000).
These can either be usedas tag or chunk sequences.
For example, the treein Figure 2 represents a shallow (non-hierarchical)parse tree, with four base constituents.
Each baseconstituent X begins with a word labeled with BX ,which signifies that this word begins the constituent.All other words within a constituent X are labeled4We tried using a list of 311 cue phrases from Knott (1996)to define features, but did not derive any system improvementthrough this list, presumably because our simple n-gram fea-tures already capture many such lexical cues.491ROOT   @@@PPPPPPPPPNP HHBNPDTtheINPNNbrokerVP HHBVPMDwillIVPVBDsellNP HHBNPDTtheINPNNSstocksNPBNPNNtomorrowFigure 2: Tree representation of shallow parses, with B(egin)and I(nside) tagsIX , and words outside of any base constituent are la-beled O.
In such a way, each word is labeled withboth a POS-tag and a B/I/O tag.For our three sequences (lexical, POS-tag andshallow tag), we define n-gram features surround-ing the potential discourse boundary.
If the currentword is wi, the hypothesized boundary will occurbetween wi and wi+1.
For this boundary position,the 6-gram including the three words before and thethree words after the boundary is included as a fea-ture; additionally, all n-grams for n < 6 such thateither wi or wi+1 (or both) is in the n-gram are in-cluded as features.
In other words, all n-grams in asix word window of boundary position i are includedas features, except those that include neither wi norwi+1 in the n-gram.
The identical feature templatesare used with POS-tag and shallow tag sequences aswell, to define tag n-gram features.This feature set is very close to that used inSporleder and Lapata (2005), but not identical.Their n-gram feature definitions were different(though similar), and they made use of cue phrasesfrom Knott (1996).
In addition, they used a rule-based clauser that we did not.
Despite such differ-ences, this feature set is quite close to what is de-scribed in that paper.2.5.2 Context-free featuresTo describe our context-free features, we firstpresent how SPADE made use of context-free parsetrees within their segmentation algorithm, since thisforms the basis of our features.
The SPADE featuresare based on productions extracted from full syntac-tic parses of the given sentence.
The primary featurefor a discourse boundary after word wi is based onthe lowest constituent in the tree that spans wordswm .
.
.
wn such that m ?
i < n. For example, inthe parse tree schematic in Figure 3, the constituentlabeled with A is the lowest constituent in the treewhose span crosses the potential discourse bound-ary after wi.
The primary feature is the productionA   @@@PPPPPPPPB1 .
.
.
Bj?1HHC1 .
.
.
CnH.
.
.
TiwiBj .
.
.
BmFigure 3: Parse tree schematic for describing context-free seg-mentation featuresthat expands this constituent in the tree, with theproposed segmentation boundary marked, which inthis case is: A ?
B1 .
.
.
Bj?1||Bj .
.
.
Bm, where|| denotes the segmentation boundary.
In SPADE,the production is lexicalized by the head words ofeach constituent, which are determined using stan-dard head-percolation techniques.
This feature isused to predict a boundary as follows: if the relativefrequency estimate of a boundary given the produc-tion feature in the corpus is greater than 0.5, then aboundary is predicted; otherwise not.
If the produc-tion has not been observed frequently enough, thelexicalization is removed and the relative frequencyof a boundary given the unlexicalized production isused for prediction.
If the observations of the unlex-icalized production are also too sparse, then only thechildren adjacent to the boundary are maintained inthe feature, e.g., A ?
?Bj?1||Bj?
where ?
repre-sents zero or more categories.
Further smoothing isused when even this is unobserved.We use these features as the starting point for ourcontext-free feature set: the lexicalized productionA ?
B1 .
.
.
Bj?1||Bj .
.
.
Bm, as defined above forSPADE, is a feature in our model, as is the unlexi-calized version of the production.
As with the otherfeatures that we have described, this feature is usedas an indicator feature in the classifier applied at theword wi preceding the hypothesized boundary.
Inaddition to these full production features, we use theproduction with only children adjacent to the bound-ary, denoted by A ?
?Bj?1||Bj?.
This productionis used in four ways: fully lexicalized; unlexicalized;only category Bj?1 lexicalized; and only categoryBj lexicalized.
We also use A ?
?Bj?2Bj?1||?and A ?
?||BjBj+1?
features, both unlexicalizedand with the boundary-adjacent category lexical-ized.
If there is no category Bj?2 or Bj+1, they arereplaced with ?N/A?.In addition to these features, we fire the same fea-tures for all productions on the path from A down492Segment Boundary accuracy Bracketing accuracySegmentation system Recall Precision F1 Recall Precision F1SPADE 85.4 85.5 85.5 77.7 77.9 77.8Classifier: Basic finite-state 81.5 83.3 82.4 73.6 74.5 74.0Classifier: Full finite-state 84.1 87.9 86.0 78.0 80.0 79.0Classifier: Context-free 84.7 91.1 87.8 80.3 83.7 82.0Classifier: All features 89.7 91.3 90.5 84.9 85.8 85.3Table 2: Segmentation results on all 991 sentences in the RST-DT test set.
Segment boundary accuracy is for sentence internalboundaries only, following Soricut and Marcu (2003).
Bracketing accuracy is for unlabeled flat bracketing of the same segments.While boundary accuracy correctly depicts segmentation results, the harsher flat bracketing metric better predicts discourse parsingperformance.to the word wi.
For these productions, the seg-mentation boundary || will occur after all childrenin the production, e.g., Bj?1 ?
C1 .
.
.
Cn||, whichis then used in both lexicalized and unlexicalizedforms.
For the feature with only categories adja-cent to the boundary, we again use ?N/A?
to denotethe fact that no category occurs to the right of theboundary: Bj?1 ?
?Cn||N/A.
Once again, theseare lexicalized as described above.2.5.3 Finite-state approximation featuresAn approximation to our context-free features canbe made by using the shallow parse tree, as shownin Figure 2, in lieu of the full hierarchical parsetree.
For example, if the current word was ?sell?in the tree in Figure 2, the primary feature wouldbe ROOT ?
NP VP||NP NP, and it would have anunlexicalized version and three lexicalized versions:the category immediately prior to the boundary lex-icalized; the category immediately after the bound-ary lexicalized; and both lexicalized.
For lexicaliza-tion, we choose the final word in the constituent asthe lexical head for the constituent.
This is a rea-sonable first approximation, because such typicallyleft-headed categories as PP and VP lose their argu-ments in the shallow parse.As a practical matter, we limit the number of cat-egories in the flat production to 8 to the left and 8 tothe right of the boundary.
In a manner similar to then-gram features that we defined in Section 2.5.1, weallow all combinations with less than 8 contiguouscategories on each side, provided that at least oneof the adjacent categories is included in the feature.Each feature has an unlexicalized and three lexical-ized versions, as described above.3 ExperimentsWe performed a number of experiments to deter-mine the relative utility of features derived fromfull context-free syntactic parses and those derivedsolely from shallow finite-state tagging.
Our pri-mary concern is with intra-sentential discourse seg-mentation, but we are also interested in how muchthe improved segmentation helps discourse parsing.The syntactic parser we use for all context-freesyntactic parses used in either SPADE or our clas-sifier is the Charniak parser with reranking, as de-scribed in Charniak and Johnson (2005).
The Char-niak parser and reranker were trained on the sectionsof the Penn Treebank not included in the RST-DTtest set.All statistical significance testing is done via thestratified shuffling test (Yeh, 2000).3.1 SegmentationTable 2 presents segmentation results for SPADEand four versions of our classifier.
The ?Basic finite-state?
system uses only finite-state sequence fea-tures as defined in Section 2.5.1, while the ?Fullfinite-state?
also includes the finite-state approxima-tion features from Section 2.5.3.
The ?Context-free?system uses the SPADE-inspired features detailed inSection 2.5.2, but none of the features from Sections2.5.1 or 2.5.3.
Finally, the ?All features?
section in-cludes features from all three sections.5Note that the full finite-state system is consider-ably better than the basic finite-state system, demon-strating the utility of these approximations of theSPADE-like context-free features.
The performanceof the resulting ?Full?
finite-state system is not sta-tistically significantly different from that of SPADE(p=0.193), despite no reliance on features derivedfrom context-free parses.The context-free features, however, even withoutany of the finite-state sequence features (even lex-ical n-grams) outperforms the best finite-state sys-tem by almost two percent absolute, and the sys-tem with all features improves on the best finite-statesystem by over four percent absolute.
The system5In the ?All features?
condition, the finite-state approxima-tion features defined in Section 2.5.3 only include a maximumof 3 children to the left and right of the boundary, versus a max-imum of 8 for the ?Full finite-state?
system.
This was found tobe optimal on the development set.493Segmentation Unlabeled Nuc/SatSPADE 76.9 70.2Classifier: Full finite state 78.1 71.1Classifier: All features 83.5 76.1Table 3: Discourse parsing results on the 951 sentence Sori-cut and Marcu (2003) evaluation set, using SPADE for parsing,and various methods for segmentation.
Scores are unlabeledand labeled (Nucleus/Satellite) bracketing accuracy (F1).
Thefirst line shows SPADE performing both segmentation and dis-course parsing.
The other two lines show SPADE performingdiscourse parsing with segmentations produced by our classi-fier using different combinations of features.with all features is statistically significantly betterthan both SPADE and the ?Full finite-state?
classi-fier system, at p < 0.001.
This large improvementdemonstrates that the context-free features can pro-vide a very large system improvement.3.2 Discourse parsingIt has been shown that accurate discourse segmen-tation within a sentence greatly improves the over-all parsing accuracy to near human levels (Sori-cut and Marcu, 2003).
Given our improved seg-mentation results presented in the previous section,improvements would be expected in full sentence-level discourse parsing.
To achieve this, we modi-fied the SPADE script to accept our segmentationswhen building the fully hierarchical discourse tree.The results for three systems are presented in Ta-ble 3: SPADE, our ?Full finite-state?
system, andour system with all features.
Results for unlabeledbracketing are presented, along with results for la-beled bracketing, where the label is either Nucleusor Satellite, depending upon whether or not the nodeis more central (Nucleus) to the coherence of the textthan its sibling(s) (Satellite).
This label set has beenshown to be of particular utility for indicating whichsegments are more important to include in an auto-matically created summary or compressed sentence(Sporleder and Lapata, 2005; Marcu, 1998; Marcu,1999; Cristea et al, 2005).Once again, the finite-state system does notperform statistically significantly different fromSPADE on either labeled or unlabeled discourseparsing.
Using all features, however, results ingreater than 5% absolute accuracy improvementover both of these systems, which is significant, inall cases, at p < 0.001.4 Discussion and future directionsOur results show that context-free parse derived fea-tures are critical for achieving the highest level ofaccuracy in sentence-level discourse segmentation.Given that edus are by definition clause-like units,it is not surprising that accurate full syntactic parsetrees provide highly relevant information unavail-able from finite-state approaches.
Adding context-free features to our best finite-state feature modelreduces error in segmentation by 32.1%, an in-crease in absolute F-score of 4.5%.
These increasesare against a finite-state segmentation model that ispowerful enough to be statistically indistinguishablefrom SPADE.Our experiments also confirm that increased seg-mentation accuracy yields significantly better dis-course parsing accuracy, as previously shown to bethe case when providing reference segmentations toa parser (Soricut and Marcu, 2003).
The segmen-tation reduction in error of 34.5% propagates to a28.6% reduction in error for unlabeled discourseparse trees, and a 19.8% reduction in error for treeslabeled with Nucleus and Satellite.We have several key directions in which to con-tinue this work.
First, given that a general ma-chine learning approach allowed us to improve uponSPADE?s segmentation performance, we also be-lieve that it will prove useful for improving fulldiscourse parsing, both at the sentence level andbeyond.
For efficient inter-sentential discourseparsing, we see the need for an additional levelof segmentation at the paragraph level.
Whereasmost sentences correspond to a well-formed subtree,Sporleder and Lascarides (2004) report that over20% of the paragraph boundaries in the RST-DT donot correspond to a well-formed subtree in the hu-man annotated discourse parse for that document.Therefore, to perform accurate and efficient pars-ing of the RST-DT at the paragraph level, the textshould be segmented into paragraph-like segmentsthat conform to the human-annotated subtree bound-aries, just as sentences are segmented into edus.We also intend to begin work on the other dis-course annotated corpora.
While most work on tex-tual discourse parsing has made use of the RST-DTcorpus, the Discourse GraphBank corpus provides acompeting annotation that is not constrained to treestructures (Wolf and Gibson, 2005).
Once accuratelevels of segmentation and parsing for both corporaare attained, it will be possible to perform extrinsicevaluations to determine their relative utility for dif-ferent NLP tasks.
Recent work has shown promis-ing preliminary results for recognizing and labelingrelations of GraphBank structures (Wellner et al,2006), in the case that the algorithm is provided with494manually segmented sentences.
Sentence-level seg-mentation in the GraphBank is very similar to that inthe RST-DT, so our segmentation approach shouldwork well for Discourse GraphBank style parsing.The Penn Discourse Treebank (Miltsakaki et al,2004), or PDTB, uses a relatively flat annotation ofdiscourse structure, in contrast to the hierarchicalstructures found in the other two corpora.
It containsannotations for discourse connectives and their argu-ments, where an argument can be as small as a nom-inalization or as large as several sentences.
This ap-proach obviates the need to create a set of discourserelations, but sentence internal segmentation is stilla necessary step.
Though segmentation in the PDTBtends to larger units than edus, our approach to seg-mentation should be straightforwardly applicable totheir segmentation style.AcknowledgmentsThanks to Caroline Sporleder and Mirella Lapata fortheir test data and helpful comments.
Thanks also toRadu Soricut for helpful input.
This research wassupported in part by NSF Grant #IIS-0447214.
Anyopinions, findings, conclusions or recommendationsexpressed in this publication are those of the authorsand do not necessarily reflect the views of the NSF.ReferencesE.
Black, S. Abney, D. Flickenger, C. Gdaniec, R. Grish-man, P. Harrison, D. Hindle, R. Ingria, F. Jelinek, J. Kla-vans, M. Liberman, M.P.
Marcus, S. Roukos, B. Santorini,and T. Strzalkowski.
1991.
A procedure for quantita-tively comparing the syntactic coverage of english gram-mars.
In DARPA Speech and Natural Language Workshop,pages 306?311.L.
Carlson, D. Marcu, and M.E.
Okurowski.
2002.
RST dis-course treebank.
Linguistic Data Consortium, Catalog #LDC2002T07.
ISBN LDC2002T07.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best pars-ing and MaxEnt discriminative reranking.
In Proceedings ofthe 43rd Annual Meeting of ACL, pages 173?180.E.
Charniak.
2000.
A maximum-entropy-inspired parser.
InProceedings of the 1st Conference of the North AmericanChapter of the Association for Computational Linguistics,pages 132?139.M.J.
Collins.
2002.
Discriminative training methods for hiddenMarkov models: Theory and experiments with perceptronalgorithms.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP), pages1?8.D.
Cristea, O. Postolache, and I.
Pistol.
2005.
Summarisationthrough discourse structure.
In 6th International Conferenceon Computational Linguistics and Intelligent Text Process-ing (CICLing).K.
Hollingshead, S. Fisher, and B. Roark.
2005.
Comparingand combining finite-state and context-free parsers.
In Pro-ceedings of HLT-EMNLP, pages 787?794.A.
Knott.
1996.
A Data-Driven Methodology for Motivatinga Set of Coherence Relations.
Ph.D. thesis, Department ofArtificial Intelligence, University of Edinburgh.W.C.
Mann and S.A. Thompson.
1988.
Rhetorical structuretheory: Toward a functional theory of text organization.
Text,8(3):243?281.D.
Marcu.
1998.
Improving summarization through rhetoricalparsing tuning.
In The 6th Workshop on Very Large Corpora.D.
Marcu.
1999.
Discourse trees are good indicators of im-portance in text.
In I. Mani and M. Maybury, editors, Ad-vances in Automatic Text Summarization, pages 123?136.MIT Press, Cambridge, MA.M.P.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.
1993.Building a large annotated corpus of English: The PennTreebank.
Computational Linguistics, 19(2):313?330.E.
Miltsakaki, R. Prasad, A. Joshi, and B. Webber.
2004.
ThePenn Discourse TreeBank.
In Proceedings of the LanguageResources and Evaluation Conference.R.
Prasad, A. Joshi, N. Dinesh, A. Lee, E. Miltsakaki, andB.
Webber.
2005.
The Penn Discourse TreeBank as a re-source for natural language generation.
In Proceedings ofthe Corpus Linguistics Workshop on Using Corpora for Nat-ural Language Generation.R.
Soricut and D. Marcu.
2003.
Sentence level discourse pars-ing using syntactic and lexical information.
In Human Lan-guage Technology Conference of the North American Asso-ciation for Computational Linguistics (HLT-NAACL).C.
Sporleder and M. Lapata.
2005.
Discourse chunking and itsapplication to sentence compression.
In Human LanguageTechnology Conference and the Conference on EmpiricalMethods in Natural Language Processing (HLT-EMNLP),pages 257?264.C.
Sporleder and A. Lascarides.
2004.
Combining hierarchi-cal clustering and machine learning to predict high-level dis-course structure.
In Proceedings of the International Confer-ence in Computational Linguistics (COLING), pages 43?49.E.F.
Tjong Kim Sang and S. Buchholz.
2000.
Introduction tothe CoNLL-2000 shared task: Chunking.
In Proceedings ofCoNLL, pages 127?132.S.
Verberne, L. Boves, N. Oostdijk, and P.A.
Coppen.
2006.Discourse-based answering of why-questions.
TraitementAutomatique des Langues (TAL).B.
Wellner, J. Pustejovsky, C. Havasi, A. Rumshisky, andR.
Sauri.
2006.
Classification of discourse coherence re-lations: An exploratory study using multiple knowledgesources.
In Proceedings of the 7th SIGdial Workshop on Dis-course and Dialogue.F.
Wolf and E. Gibson.
2005.
Representing discourse coher-ence: A corpus-based analysis.
Computational Linguistics,31(2):249?288.A.
Yeh.
2000.
More accurate tests for the statistical signifi-cance of result differences.
In Proceedings of the 18th Inter-national COLING, pages 947?953.495
