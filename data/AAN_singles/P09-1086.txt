Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 764?772,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPImproving Automatic Speech Recognition for Lectures throughTransformation-based Rules Learned from Minimal DataCosmin Munteanu??
?National Research Council Canada46 Dineen DriveFredericton E3B 9W4, CANADACosmin.Munteanu@nrc.gc.caGerald Penn?
?University of TorontoDept.
of Computer ScienceToronto M5S 3G4, CANADA{gpenn,xzhu}@cs.toronto.eduXiaodan Zhu?AbstractWe demonstrate that transformation-basedlearning can be used to correct noisyspeech recognition transcripts in the lec-ture domain with an average word errorrate reduction of 12.9%.
Our method isdistinguished from earlier related work byits robustness to small amounts of trainingdata, and its resulting efficiency, in spite ofits use of true word error rate computationsas a rule scoring function.1 IntroductionImproving access to archives of recorded lecturesis a task that, by its very nature, requires researchefforts common to both Automatic Speech Recog-nition (ASR) and Human-Computer Interaction(HCI).
One of the main challenges to integratingtext transcripts into archives of webcast lectures isthe poor performance of ASR systems on lecturetranscription.
This is in part caused by the mis-match between the language used in a lecture andthe predictive language models employed by mostASR systems.
Most ASR systems achieve WordError Rates (WERs) of about 40-45% in realis-tic and uncontrolled lecture conditions (Leeuwiset al, 2003; Hsu and Glass, 2006).Progress in ASR for this genre requires bothbetter acoustic modelling (Park et al, 2005;Fu?gen et al, 2006) and better language modelling(Leeuwis et al, 2003; Kato et al, 2000; Munteanuet al, 2007).
In contrast to some unsupervised ap-proaches to language modelling that require largeamounts of manual transcription, either from thesame instructor or on the same topic (Nanjo andKawahara, 2003; Niesler and Willett, 2002), thesolution proposed by Glass et al (2007) uses halfof the lectures in a semester course to train anASR system for the other half or for when thecourse is next offered, and still results in signifi-cant WER reductions.
And yet even in this sce-nario, the business case for manually transcrib-ing half of the lecture material in every recordedcourse is difficult to make, to say the least.
Manu-ally transcribing a one-hour recorded lecture re-quires at least 5 hours in the hands of qualifiedtranscribers (Hazen, 2006) and roughly 10 hoursby students enrolled in the course (Munteanu etal., 2008).
As argued by Hazen (2006), any ASRimprovements that rely on manual transcripts needto offer a balance between the cost of producingthose transcripts and the amount of improvement(i.e.
WER reductions).There is some work that specializes in adap-tive language modelling with extremely limitedamounts of manual transcripts.
Klakow (2000)filters the corpus on which language models aretrained in order to retain the parts that are moresimilar to the correct transcripts on a particulartopic.
This technique resulted in relative WERreductions of between 7% and 10%.
Munteanuet al (2007) use an information retrieval tech-nique that exploits lecture presentation slides, au-tomatically mining the World Wide Web for doc-uments related to the topic as attested by texton the slides, and using these to build a better-matching language model.
This yields about an11% relative WER reduction for lecture-specificlanguage models.
Following upon other applica-tions of computer-supported collaborative work toaddress shortcomings of other systems in artificialintelligence (von Ahn and Dabbish, 2004), a wiki-based technique for collaboratively editing lecturetranscripts has been shown to produce entirely cor-764rected transcripts, given the proper motivation forstudents to participate (Munteanu et al, 2008).Another approach is active learning, where thegoal is to select or generate a subset of the avail-able data that would be the best candidate for ASRadaptation or training (Riccardi and Hakkani-Tur,2005; Huo and Li, 2007).1 Even with all of these,however, there remains a significant gap betweenthis WER and the threshold of 25%, at which lec-ture transcripts have been shown with statisticalsignificance to improve student performance ona typical lecture browsing task (Munteanu et al,2006).People have also tried to correct ASR output ina second pass.
Ringger and Allen (1996) treatedASR errors as noise produced by an auxiliarynoisy channel, and tried to decode back to the per-fect transcript.
This reduced WER from 41% to35% on a corpus of train dispatch dialogues.
Oth-ers combine the transcripts or word lattices (fromwhich transcripts are extracted) of two comple-mentary ASR systems, a technique first proposedin the context of NIST?s ROVER system (Fiscus,1997) with a 12% relative error reduction (RER),and subsequently widely employed in many ASRsystems.This paper tries to correct ASR output usingtransformation-based learning (TBL).
This, too,has been attempted, although on a professionaldictation corpus with a 35% initial WER (Petersand Drexel, 2004).
They had access to a very largeamount of manually transcribed data ?
so large,in fact, that the computation of true WER in theTBL rule selection loop was computationally in-feasible, and so they used a set of faster heuristicsinstead.
Mangu and Padmanabhan (2001) usedTBL to improve the word lattices from which thetranscripts are decoded, but this method also hasefficiency problems (it begins with a reduction ofthe lattice to a confusion network), is poorly suitedto word lattices that have already been heavilydomain-adapted because of the language model?slow perplexity, and even with higher perplexitymodels (the SWITCHBOARD corpus using a lan-1This work generally measures progress by reduction inthe size of training data rather than relative WER reduction.Riccardi and Hakkani-Tur (2005) achieved a 30% WER with68% less training data than their baseline.
Huo and Li (2007)worked on a small-vocabulary name-selection task that com-bined active learning with acoustic model adaptation.
Theyreduced the WER from 15% to 3% with 70 syllables of acous-tic adaptation, relative to a baseline that reduced the WER to3% with 300 syllables of acoustic adaptation.guage model trained over a diverse range of broad-cast news and telephone conversation transcripts),was reported to produce only a 5% WER reduc-tion.What we show in this paper is that a true WERcalculation is so valuable that a manual transcrip-tion of only about 10 minutes of a one-hour lectureis necessary to learn the TBL rules, and that thissmaller amount of transcribed data in turn makesthe true WER calculation computationally feasi-ble.
With this combination, we achieve a greateraverage relative error reduction (12.9%) than thatreported by Peters and Drexel (2004) on their dic-tation corpus (9.6%), and an RER over three timesgreater than that of our reimplementation of theirheuristics on our lecture data (3.6%).
This is ontop of the average 11% RER from language modeladaptation on the same data.
We also achievethe RER from TBL without the obligatory roundof development-set parameter tuning required bytheir heuristics, and in a manner that is robust toperplexity.
Less is more.Section 2 briefly introduces Transformation-Based Learning (TBL), a method used in variousNatural Language Processing tasks to correct theoutput of a stochastic model, and then introducesa TBL-based solution for improving ASR tran-scripts for lectures.
Section 3 describes our exper-imental setup, and Section 4 analyses its results.2 Transformation-Based LearningBrill?s tagger introduced the concept ofTransformation-Based Learning (TBL) (Brill,1992).
The fundamental principle of TBL isto employ a set of rules to correct the outputof a stochastic model.
In contrast to traditionalrule-based approaches where rules are manuallydeveloped, TBL rules are automatically learnedfrom training data.
The training data consist ofsample output from the stochastic model, alignedwith the correct instances.
For example, in Brill?stagger, the system assigns POSs to words in a text,which are later corrected by TBL rules.
Theserules are learned from manually-tagged sentencesthat are aligned with the same sentences taggedby the system.
Typically, rules take the form ofcontext-dependent transformations, for example?change the tag from verb to noun if one of thetwo preceding words is tagged as a determiner.
?An important aspect of TBL is rule scor-ing/ranking.
While the training data may suggest765a certain transformation rule, there is no guaranteethat the rule will indeed improve the system?s ac-curacy.
So a scoring function is used to rank rules.From all the rules learned during training, onlythose scoring higher than a certain threshold areretained.
For a particular task, the scoring func-tion ideally reflects an objective quality function.Since Brill?s tagger was first introduced, TBLhas been used for other NLP applications, includ-ing ASR transcript correction (Peters and Drexel,2004).
A graphical illustration of this task is pre-sented in Figure 1.
Here, the rules consist ofFigure 1: General TBL algorithm.
Transformationrules are learned from the alignment of manually-transcribed text (T ) with automatically-generatedtranscripts (TASR) of training data, ranked accord-ing to a scoring function (S) and applied to theASR output (T ?ASR) of test data.word-level transformations that correct n-gram se-quences.
A typical challenge for TBL is the heavycomputational requirements of the rule scoringfunction (Roche and Schabes, 1995; Ngai andFlorian, 2001).
This is no less true in large-vocabulary ASR correction, where large trainingcorpora are often needed to learn good rules overa much larger space (larger than POS tagging, forexample).
The training and development sets aretypically up to five times larger than the evaluationtest set, and all three sets must be sampled from thesame cohesive corpus.While the objective function for improving theASR transcript is WER reduction, the use of thisfor scoring TBL rules can be computationally pro-hibitive over large data-sets.
Peters and Drexel(2004) address this problem by using an heuris-tic approximation to WER instead, and it appearsthat their approximation is indeed adequate whenlarge amounts of training data are available.
Ourapproach stands at the opposite side of this trade-off ?
restrict the amount of training data to a bareminimum so that true WER can be used in therule scoring function.
As it happens, the mini-mum amount of data is so small that we can au-tomatically develop highly domain-specific lan-guage models for single 1-hour lectures.
We showbelow that the rules selected by this function leadto a significant WER reduction for individual lec-tures even if a little less than the first ten minutes ofthe lecture are manually transcribed.
This combi-nation of domain-specificity with true WER leadsto the superior performance of the present method,at least in the lecture domain (we have not experi-mented with a dictation corpus).Another alternative would be to change thescope over which TBL rules are ranked and eval-uated, but it is well known that globally-scopedranking over the entire training set at once is souseful to TBL-based approaches that this is nota feasible option ?
one must either choose anheuristic approach, such as that of Peters andDrexel (2004) or reduce the amount of trainingdata to learn sufficiently robust rules.2.1 Algorithm and Rule DiscoveryAs our proposed TBL adaptation operates di-rectly on ASR transcripts, we employ an adapta-tion of the specific algorithm proposed by Petersand Drexel (2004), which is schematically repre-sented in Figure 1.
This in turn was adapted fromthe general-purpose algorithm introduced by Brill(1992).The transformation rules are contextual word-replacement rules to be applied to ASR tran-scripts, and are learned by performing a word-level alignment between corresponding utterancesin the manual and ASR transcripts of trainingdata, and then extracting the mismatched wordsequences, anchored by matching words.
Thematching words serve as contexts for the rules?application.
The rule discovery algorithm is out-lined in Figure 2; it is applied to every mismatch-ing word sequence between the utterance-alignedmanual and ASR transcripts.For every mismatching sequence of words, a set766?
for every sequence of words c0w1 .
.
.
wnc1 in theASR output that is deemed to be aligned with acorresponding sequence c0w?1 .
.
.
w?mc1 in themanual transcript:?
add the following contextual replacements to theset of discovered rules:/ c0w1 .
.
.
wnc1 / c0w?1 .
.
.
w?mc1 // c0w1 .
.
.
wn / c0w?1 .
.
.
w?m // w1 .
.
.
wnc1 / w?1 .
.
.
w?mc1 // w1 .
.
.
wn / w?1 .
.
.
w?m /?
for each i such that 1 ?
i < min(n, m), addthe following contextual replacements to the set ofdiscovered rules:/ c0w1 .
.
.
wi / c0w?1 .
.
.
w?a(i) // wi+1 .
.
.
wnc1 / w?a(i+1) .
.
.
w?mc1 // w1 .
.
.
wi / w?1 .
.
.
w?a(i) // wi+1 .
.
.
wn / w?a(i+1) .
.
.
w?m /Figure 2: The discovery of transformation rules.of contextual replacement rules is generated.
Theset contains the mismatched pair, by themselvesand together with three contexts formed from theleft, right, and both anchor context words.
Inaddition, all possible splices of the mismatchedpair and the surrounding context words are alsoconsidered.2 Rules are shown here as replace-ment expressions in a sed-like syntax.
Given therule r = /w1 .
.
.
wn/w?1 .
.
.
w?m/, every instanceof the n-gram w1 .
.
.
wn appearing in the currenttranscript is replaced with the n-gram w?1 .
.
.
w?m.Rules cannot apply to their own output.
Rules thatwould result in arbitrary insertions of single words(e.g.
/ /w1/) are discarded.
An example of a rulelearned from transcripts is presented in Figure 3.2.2 Scoring Function and Rule ApplicationThe scoring function that ranks rules is the maincomponent of any TBL algorithm.
Assuming arelatively small size for the available training data,a TBL scoring function that directly correlateswith WER can be conducted globally over the en-tire training set.
In keeping with TBL tradition,however, rule selection itself is still greedily ap-proximated.
Our scoring function is defined as:SWER(r, TASR, T ) = WER(TASR, T )?WER(?
(r, TASR), T ),2The splicing preserves the original order of the word-level utterance alignment, i.e., the output of a typical dynamicprogramming implementation of the edit distance algorithm(Gusfield, 1997).
For this, word insertion and deletion oper-ations are treated as insertions of blanks in either the manualor ASR transcript.Utterance-align ASR output and correct transcripts:ASR: the okay one and you come and get your seatsCorrect: ok why don?t you come and get your seats?Insert sentence delimiters (to serve as possibleanchors for the rules):ASR: <s> the okay one and you come and get your seats </s>Correct: <s> ok why don?t you come and get your seats </s>?Extract the mismatching sequence, enclosed bymatching anchors:ASR: <s> the okay one and youCorrect: <s> ok why don?t you?Output all rules for replacing the incorrect ASRsequence with the correct text, using the entiresequence (a) or splices (b), with or withoutsurrounding anchors:(a) the okay one and / ok why don?t(a) the okay one and you / ok why don?t you(a) <s> the okay one and / <s> ok why don?t(a) <s> the okay one and you / <s> ok why don?t you(b) the okay / ok(b) <s> the okay / <s> ok(b) one and / why don?t(b) one and you / why don?t you(b) the okay one / ok why(b) <s> the okay one / <s> ok why(b) and / don?t(b) and you / don?t youFigure 3: An example of rule discovery.where ?
(r, TASR) is the result of applying rule ron text TASR.As outlined in Figure 1, rules that occur in thetraining sample more often than an establishedthreshold are ranked according to the scoring func-tion.
The ranking process is iterative: in each iter-ation, the highest-scoring rule rbest is selected.
Insubsequent iterations, the training data TASR arereplaced with the result of applying the selectedrule on them (TASR ?
?
(rbest, TASR)) and the re-maining rules are scored on the transformed train-ing text.
This ensures that the scoring and rankingof remaining rules takes into account the changesbrought by the application of the previously se-lected rules.
The iterations stop when the scoringfunction reaches zero: none of the remaining rulesimproves the WER on the training data.On testing data, rules are applied to ASR tran-767scripts in the same order in which they were se-lected.3 Experimental DesignSeveral combinations of TBL parameters weretested with no tuning or modifications betweentests.
As the proposed method was not refined dur-ing the experiments, and since one of the goals ofour proposed approach is to eliminate the need fordevelopmental data sets, the available data werepartitioned only into training and test sets, withone additional hour set aside for code developmentand debugging.It can be assumed that a one-hour lecture givenby the same instructor will exhibit a strong cohe-sion, both in topic and in speaking style, betweenits parts.
Therefore, in contrast to typical TBLsolutions, we have evaluated our TBL-based ap-proach by partitioning each 50 minute lecture intoa training and a test set, where the training set issmaller than the test set.
As mentioned in the intro-duction, it is feasible to obtain manual transcriptsfor the first 10 to 15 minutes of a lecture.
As such,the evaluation was carried out with two values forthe training size: the first fifth (TS = 20%) andthe first third (TS = 33%) of the lecture beingmanually transcribed.Besides the training size parameter, during allexperimental tests a second parameter was alsoconsidered: the rule pruning threshold (RT ).
Asdescribed in Section 2.2, of all the rules learnedduring the rule discovery step, only those that oc-cur more often than the threshold are scored andranked.
This parameter can be set as low as 1 (con-sider all rules) or 2 (consider all rules that occurat least twice over the training set).
For larger-scale tasks, the threshold serves as a pruning al-ternative to the computational burden of scoringseveral thousand rules.
A large threshold couldpotentially lead to discrediting low-frequency buthigh-scoring rules.
Due to the intentionally smallsize of our training data for lecture TBL, the low-est threshold was set to RT = 2.
When a de-velopment set is available, several values for theRT parameter could be tested and the optimal onechosen for the evaluation task.
Since we used nodevelopment set, we tested two more values for therule pruning threshold: RT = 5 and RT = 10.Since our TBL solution is an extension of thesolution proposed in Peters and Drexel (2004),their heuristic is our baseline.
Their scoring func-tion is the expected error reduction:XER = ErrLen ?
(GoodCnt?BadCnt),a WER approximation computed over all instancesof rules applicable to the training set which reflectsthe difference between true positives (the numberof times a rule is correctly applied to errorful tran-scripts ?
GoodCnt) and false positives (the in-stances of correct text being unnecessarily ?cor-rected?
by a rule ?
BadCnt).
These are weightedby the length in words (ErrLen) of the text areathat matches the left-hand side of the replacement.3.1 Acoustic ModelThe experiments were conducted using theSONIC toolkit (Pellom, 2001).
We used theacoustic model distributed with the toolkit, whichwas trained on 30 hours of data from 283 speak-ers from the WSJ0 and WSJ1 subsets of the1992 development set of the Wall Street Jour-nal (WSJ) Dictation Corpus.
Our own lecturesconsist of eleven lectures of approximately 50minutes each, recorded in three separate courses,each taught by a different instructor.
For eachcourse, the recordings were performed in differentweeks of the same term.
They were collected ina large, amphitheatre-style, 200-seat lecture hallusing the AKG C420 head-mounted directionalmicrophone.
The recordings were not intrusive,and no alterations to the lecture environment orproceedings were made.
The 1-channel record-ings were digitized using a TASCAM US-122 au-dio interface as uncompressed audio files with a16KHz sampling rate and 16-bit samples.
The au-dio recordings were segmented at pauses longerthan 200ms, manually for one instructor and au-tomatically for the other two, using the silencedetection algorithm described in Placeway et al(1997).
Our implementation was manually fine-tuned for every instructor in order to detect allpauses longer than 200ms while allowing a maxi-mum of 20 seconds in between pauses.The evaluation data are described in Table 1.Four evaluations tasks were carried out; for in-structor R, two separate evaluation sessions, R-1and R-2, were conducted, using two different lan-guage models.The pronunciation dictionary was custom-builtto include all words appearing in the corpus onwhich the language model was trained.
Pronunci-ations were extracted from the 5K-word WSJ dic-tionary included with the SONIC toolkit and from768Evaluationtask name R-1 R-2 G-1 K-1Instructor R. G. K.Gender Male Male FemaleAge Early 60s Mid 40s Early 40sSegmentation manual automatic automatic# lectures 4 3 4Lecture topic Interactive Software Unix pro-media design design grammingLanguage model WSJ-5K WEB ICSISWB WSJ-5KTable 1: The evaluation data.the 100K-word CMU pronunciation dictionary.For all models, we allowed one non-dictionaryword per utterance, but only for lines longer thanfour words.
For allowable non-dictionary words,SONIC?s sspell lexicon access tool was used togenerate pronunciations using letter-to-sound pre-dictions.
The language models were trained us-ing the CMU-CAM Language Modelling Toolkit(Clarkson and R., 1997) with a training vocabu-lary size of 40K words.3.2 Language ModelsThe four evaluations were carried out using thelanguage models given in Table 1, either custom-built for a particular topic or the baseline modelsincluded in the SONIC toolkit, as follows:WSJ-5K is the baseline model of the SONICtoolkit.
It is a 5K-word model built using the samecorpus as the base acoustic model included in thetoolkit.ICSISWB is a 40K-word model createdthrough the interpolation of language models builton the entire transcripts of the ICSI Meeting cor-pus and the Switchboard corpus.
The ICSI Meet-ing corpus consists of recordings of university-based multi-speaker research meetings, totalingabout 72 hours from 75 meetings (Janin et al,2003).
The Switchboard (SWB) corpus (Godfreyet al, 1992) is a large collection of about 2500scripted telephone conversations between approx-imately 500 English-native speakers, suitable forthe conversational style of lectures, as also sug-gested in (Park et al, 2005).WEB is a language model built for each par-ticular lecture, using information retrieval tech-niques that exploit the lecture slides to automat-ically mine the World Wide Web for documentsrelated to the presented topic.
WEB adapts IC-SISWB using these documents to build a languagemodel that better matches the lecture topic.
It isalso a 40K-word model built on training corporawith an average file size of approximately 200 MBper lecture, and an average of 35 million word to-kens per lecture.It is appropriate to take the difference betweenICSISWB and WSJ-5K to be one of greater genrespecificity, whereas the difference between WEBand ICSISWB is one of greater topic-specificity.Our experiments on these three models (Munteanuet al, 2007) shows that the topic adaptation pro-vides nearly all of the benefit.4 ResultsTables 2, 3 and 43 present the evaluation resultsICSISWB Lecture 1 Lecture 2 Lecture 3TS = % 20 33 20 33 20 33Initial WER 50.93 50.75 54.10 53.93 48.79 49.35XER RT = 10 46.63 49.38 49.93 48.61 49.52 50.43RT = 5 48.34 49.75 49.32 48.81 49.58 49.26RT = 2 54.05 56.84 52.01 49.11 50.37 51.66XER-NoS RT = 10 49.54 49.38 54.10 53.93 48.79 48.24RT = 5 49.54 49.31 56.70 55.50 48.51 48.42RT = 2 59.00 59.28 57.61 55.03 50.41 52.67SWER RT = 10 46.63 46.53 49.80 48.44 45.83 45.42RT = 5 46.63 45.60 47.75 47.23 44.76 44.44RT = 2 44.48 44.30 47.46 47.02 43.60 44.13Table 4: Experimental evaluation: WER values forinstructor G using the ICSISWB language model.for instructors R and G. The transcripts were ob-tained through ASR runs using three different lan-guage models.
The TBL implementation with ourscoring function SWER brings relative WER re-ductions ranging from 10.5% to 14.9%, with anaverage of 12.9%.These WER reductions are greater than thoseproduced by the XER baseline approach.
It is notpossible to provide confidence intervals since theproposed method does not tune parameters fromsampled data (which we regard as a very positivequality for such a method to have).
Our specu-lative experimentation with several values for TSand RT , however, leads us to conclude that thismethod is significantly less sensitive to variationsin both the training size TS and the rule pruningthreshold RT than earlier work, making it suitablefor application to tasks with limited training data?
a result somewhat expected since rules are vali-dated through direct WER reductions over the en-tire training set.3Although WSJ-5K and ICSISWB exhibited nearly thesame WER in our earlier experiments on all lecturers, wedid find upon inspection of the transcripts in question thatICSISWB was better interpretable on speakers that had morecasual speaking styles, whereas WSJ-5K was better on speak-ers with more rehearsed styles.
We have used whichever ofthese baselines was the best interpretable in our experimentshere (WSJ-5K for R and K, ICSISWB for G).769WSJ-5K Lecture 1 Lecture 2 Lecture 3 Lecture 4TS = % 20 33 20 33 20 33 20 33Initial WER 50.48 50.93 51.31 51.90 50.28 49.23 54.39 54.04XER RT = 10 49.97 49.82 49.27 49.77 46.85 48.08 52.17 50.58RT = 5 50.01 50.07 49.99 51.13 48.39 47.37 50.91 49.62RT = 2 49.87 51.75 49.52 51.13 47.13 47.31 52.70 50.56XER-NoS RT = 10 47.25 46.82 49.98 48.72 48.44 45.21 51.37 49.73RT = 5 49.03 48.78 47.37 51.25 47.84 44.07 49.54 48.97RT = 2 52.21 53.47 49.31 52.29 50.85 49.41 50.63 51.81SWER RT = 10 45.18 44.58 49.06 45.97 46.49 45.30 49.60 47.95RT = 5 44.82 43.82 46.73 45.52 45.64 43.18 47.79 46.74RT = 2 44.04 43.99 45.81 45.16 44.35 41.49 46.89 44.28Table 2: Experimental evaluation: WER values for instructor R using the WSJ-5K language model.WEB Lecture 1 Lecture 2 Lecture 3 Lecture 4TS = % 20 33 20 33 20 33 20 33Initial WER 45.54 45.85 43.36 43.87 46.69 47.14 49.78 49.38XER RT = 10 42.91 43.90 42.44 43.81 46.78 45.35 46.92 49.65RT = 5 43.45 43.81 42.65 44.37 46.90 42.12 47.34 46.04RT = 2 43.26 45.46 44.19 44.66 43.77 45.12 61.54 60.40XER-NoS RT = 10 43.51 42.97 42.11 41.98 44.66 46.59 47.24 46.30RT = 5 44.96 42.98 40.01 40.52 44.66 41.74 47.23 44.35RT = 2 46.72 48.16 44.79 45.87 40.44 44.32 61.84 64.40SWER RT = 10 41.98 41.44 42.11 40.75 44.66 45.27 47.24 45.85RT = 5 40.97 40.56 38.85 39.08 44.66 40.84 45.27 42.39RT = 2 40.67 40.47 38.00 38.07 40.00 40.08 43.31 41.52Table 3: Experimental evaluation: WER values for instructor R using the WEB language models.As for how the transcripts improve, words withlower information content (e.g., a lower tf.idfscore) are corrected more often and with moreimprovement than words with higher informationcontent.
The topic-specific language model adap-tation that the TBL follows upon benefits wordswith higher information content more.
It is possi-ble that the favour observed in TBL with SWERtowards lower information content is a bias pro-duced by the preceding round of language modeladaptation, but regardless, it provides a much-needed complementary effect.
This can be ob-served in Tables 2 and 3, in which TBL producesnearly the same RER in either table for any lecture.We have also extensively experimented with theusability of lecture transcripts on human subjects(Munteanu et al, 2006), and have found that task-based usability varies in linear relation to WER.An analysis of the rules selected by both TBLimplementations revealed that using the XER ap-proximation leads to several single-word rules be-ing selected, such as rules removing all instancesof frequent stop-words such as ?the?
and ?for?
orpronouns such as ?he.?
Therefore, an empiricalimprovement (XER ?NoS) of the baseline wasimplemented that, beside pruning rules below theRT threshold, omits such single-word rules frombeing selected.
As shown in Tables 2, 3 and 4,this restriction slightly improves the performanceof the approximation-based TBL for some valuesof the RT and TS parameters, although it stilldoes not consistently match the WER reductionsof our scoring function.Although the experimental evaluation showspositive improvements in transcript qualitythrough TBL, in particular when using the SWERscoring function, an exception is illustrated inTable 5.
The recordings for this evaluation werecollected from a course on Unix programming,and lectures were highly interactive.
InstructorK used numerous examples of C or Shell code,many of them being developed and tested inclass.
While the keywords from a programminglanguage can be easily added to the ASR lexicon,the pronunciation of such abbreviated forms (es-pecially for Shell programming) and of mostly allvariable and custom function names proved to bea significant difficulty for the ASR system.
This,combined with a high speaking rate and ofteninconsistently truncated words, led to few TBLrules occurring even above the lowest RT = 2threshold (despite many TBL rules being initiallydiscovered).As previously mentioned, one of the drawbacksof global TBL rule scoring is the heavy compu-tational burden.
The experiments conducted here,however, showed an average learning time of onehour per one-hour lecture, reaching at most three770WSJ-5K Lecture 1 Lecture 2 Lecture 3 Lecture 4TS = % 20 33 20 33 20 33 20 33Initial WER 44.31 44.06 46.12 45.80 51.10 51.19 53.92 54.89XER RT = 10 44.31 44.06 46.12 46.55 51.10 51.19 53.92 54.89RT = 5 44.31 44.87 46.82 47.47 51.10 51.19 53.96 55.56RT = 2 47.46 55.21 50.54 51.01 52.60 54.93 57.48 60.46XER-NoS RT = 10 44.31 44.06 46.12 46.55 51.10 51.19 53.92 54.89RT = 5 44.31 44.87 46.82 47.47 51.10 51.19 53.96 55.56RT = 2 46.43 54.41 50.54 51.01 53.01 55.02 57.47 60.02SWER RT = 10 44.31 44.06 46.12 45.80 51.10 51.19 53.92 54.89RT = 5 44.31 44.05 46.11 45.88 51.10 51.19 53.92 54.89RT = 2 44.34 44.07 46.03 45.89 50.96 50.93 54.01 55.16Table 5: Experimental evaluation: WER values for instructor K using the WSJ-5K language model.hours4 for a threshold of 2 when training over tran-scripts for one third of a lecture.
Therefore, it canbe concluded that, despite being computationallymore intensive than a heuristic approximation (forwhich the learning time is on the order of just afew minutes), a TBL system using a global, WER-correlated scoring function not only produces bet-ter transcripts, but also produces them in a feasibleamount of time with only a small amount of man-ual transcription for each lecture.5 Summary and DiscussionOne of the challenges to reducing the WER ofASR transcriptions of lecture recordings is thelack of manual transcripts on which to train var-ious ASR improvements.
In particular, for one-hour lectures given by different lecturers (such as,for example, invited presentations), it is often im-practical to manually transcribe parts of the lecturethat would be useful as training or developmentdata.
However, transcripts for the first 10-15 min-utes of a particular lecture can be easily obtained.In this paper, we presented a solution that im-proves the quality of ASR transcripts for lectures.WER is reduced by 10% to 14%, with an averagereduction of 12.9%, relative to initial values.
Thisis achieved by making use of manual transcriptsfrom as little as the first 10 minutes of a one-hourlecture.
The proposed solution learns word-leveltransformation-based rules that attempt to replaceparts of the ASR transcript with possible correc-tions.
The experimental evaluation carried outover eleven lectures from three different coursesand instructors shows that this amount of manualtranscription can be sufficient to further improve alecture-specific ASR system.4It should be noted that, in order to preserve compatibil-ity with other software tools, the code developed for theseexperiments was not optimized for speed.
It is expected thata dedicated implementation would result in even lower run-times.In particular, we demonstrated that a true WER-based scoring function for the TBL algorithm isboth feasible and effective with a limited amountof training data and no development data.
The pro-posed function assigns scores to TBL rules that di-rectly correlate with reductions in the WER of theentire training set, leading to a better performancethan that of a heuristic approximation.
Further-more, a scoring function that directly optimizesfor WER reductions is more robust to variationsin training size as well as to the value of the rulepruning threshold.
As little as a value of 2 can beused for the threshold (scoring all rules that occurat least twice), with limited impact on the com-putational burden of learning the transformationrules.ReferencesE.
Brill.
1992.
A simple rule-based part of speechtagger.
In Proc.
3rd Conf.
on Applied NLP (ANLP),pages 152 ?
155.P.R.
Clarkson and Rosenfeld R. 1997.
Statistical lan-guage modeling using the CMU-Cambridge Toolkit.In Proc.
Eurospeech, volume 1, pages 2707?2710.J.G.
Fiscus.
1997.
A post-processing system to yieldreduced word error rates: Recognizer output votingerror reduction (ROVER).
In Proc.
IEEE Workshopon Automatic Speech Recognition and Understand-ing (ASRU), pages 347?354.C.
Fu?gen, M. Kolss, D. Bernreuther, M. Paulik,S.
Stu?ker, S. Vogel, and A. Waibel.
2006.
Opendomain speech recognition & translation: Lecturesand speeches.
In Proc.
IEEE Conf.
on Acoustics,Speech, and Signal Processing (ICASSP), volume 1,pages 569?572.J.
Glass, T.J. Hazen, S. Cyphers, I. Malioutov,D.
Huynh, and R. Barzilay.
2007.
Recent progressin the MIT spoken lecture processing project.
InProc.
10th EuroSpeech / 8th InterSpeech, pages2553?2556.771J.
J. Godfrey, E. C. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus for re-search and development.
In Proc.
IEEE Conf.Acoustics, Speech, and Signal Processing (ICASSP),pages 517?520.D.
Gusfield.
1997.
Algorithms on Strings, Trees, andSequences.
Cambridge University Press.T.J.
Hazen.
2006.
Automatic alignment and errorcorrection of human generated transcripts for longspeech recordings.
In Proc.
9th Intl.
Conf.
on Spo-ken Language Processing (ICSLP) / InterSpeech,pages 1606?1609.B-J.
Hsu and J.
Glass.
2006.
Style & topic lan-guage model adaptation using HMM-LDA.
In Proc.ACL Conf.
on Empirical Methods in NLP (EMNLP),pages 373?381.Q.
Huo and W. Li.
2007.
An active approachto speaker and task adaptation based on automaticanalysis of vocabulary confusability.
In Proc.
10thEuroSpeech / 8th InterSpeech, pages 1569?1572.A.
Janin, Baron D., J. Edwards, D. Ellis, D. Gelbart,N.
Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stol-cke, and C. Wooters.
2003.
The ICSI meeting cor-pus.
In Proc.
IEEE Conf.
on Acoustics, Speech, andSignal Processing (ICASSP), pages 364?367.K.
Kato, H. Nanjo, and T. Kawahara.
2000.
Au-tomatic transcription of lecture speech using topic-independent language modeling.
In Proc.
Intl.
Conf.on Spoken Language Processing (ICSLP), volume 1,pages 162?165.D.
Klakow.
2000.
Selecting articles from the languagemodel training corpus.
In Proc.
IEEE Conf.
onAcoustics, Speech, and Signal Processing (ICASSP),pages 1695?1698.E.
Leeuwis, M. Federico, and M. Cettolo.
2003.
Lan-guage modeling and transcription of the TED corpuslectures.
In Proc.
Intl.
Conf.
on Acoustics, Speech,and Signal Processing (ICASSP), volume 1, pages232?235.L.
Mangu and M. Padmanabhan.
2001.
Error correc-tive mechanisms for speech recognition.
In Proc.IEEE Conf.
on Acoustics, Speech, and Signal Pro-cessing (ICASSP), pages 29?32.C.
Munteanu, R. Baecker, and G. Penn.
2008.
Collab-orative editing for improved usefulness and usabil-ity of transcript-enhanced webcasts.
In Proc.
ACMSIGCHI Conf.
(CHI), pages 373?382.C.
Munteanu, R. Baecker, G. Penn, E. Toms, andD.
James.
2006.
The effect of speech recognitionaccuracy rates on the usefulness and usability of we-bcast archives.
In Proc.
ACM SIGCHI Conf.
(CHI),pages 493?502.C.
Munteanu, G. Penn, and R. Baecker.
2007.
Web-based language modelling for automatic lecture tran-scription.
In Proc.
10th EuroSpeech / 8th Inter-Speech, pages 2353?2356.H.
Nanjo and T. Kawahara.
2003.
Unsupervised lan-guage model adaptation for lecture speech recogni-tion.
In Proc.
ISCA / IEEE Workshop on Sponta-neous Speech Processing and Recognition (SSPR).G.
Ngai and R. Florian.
2001.
Transformation-basedlearning in the fast lane.
In Proc.
2nd NAACL, pages1?8.T.
Niesler and D. Willett.
2002.
Unsupervised lan-guage model adaptation for lecture speech transcrip-tion.
In Proc.
Intl.
Conf.
on Spoken Language Pro-cessing (ICSLP/Interspeech), pages 1413?1416.A.
Park, T. J. Hazen, and J. R. Glass.
2005.
Auto-matic processing of audio lectures for informationretrieval: Vocabulary selection and language model-ing.
In Proc.
IEEE Conf.
on Acoustics, Speech, andSignal Processing (ICASSP).B.
L. Pellom.
2001.
SONIC: The university of col-orado continuous speech recognizer.
Technical Re-port #TR-CSLR-2001-01, University of Colorado.J.
Peters and C. Drexel.
2004.
Transformation-basederror correction for speech-to-text systems.
In Proc.Intl.
Conf.
on Spoken Language Processing (IC-SLP/Interspeech), pages 1449?1452.P.
Placeway, S. Chen, M. Eskenazi, U. Jain, V. Parikh,B.
Raj, M. Ravishankar, R. Rosenfeld, K. Seymore,and M. Siegler.
1997.
The 1996 HUB-4 Sphinx-3system.
In Proc.
DARPA Speech Recognition Work-shop.G.
Riccardi and D. Hakkani-Tur.
2005.
Active learn-ing: Theory and applications to automatic speechrecognition.
IEEE Trans.
Speech and Audio Pro-cessing, 13(4):504?511.E.
K. Ringger and J. F. Allen.
1996.
Error correctionvia a post-processor for continuous speech recogni-tion.
In Proc.
IEEE Conf.
on Acoustics, Speech, andSignal Processing (ICASSP), pages 427?430.E.
Roche and Y. Schabes.
1995.
Deterministic part-of-speech tagging with finite-state transducers.
Com-putational Linguistics, 21(2):227?253.L.
von Ahn and L. Dabbish.
2004.
Labeling imageswith a computer game.
In Proc.
ACM SIGCHI Conf.
(CHI), pages 319?326.772
