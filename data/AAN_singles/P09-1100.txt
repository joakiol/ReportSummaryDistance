Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 888?896,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPSetting Up User Action Probabilities in User Simulations for DialogSystem DevelopmentHua AiUniversity of PittsburghPittsburgh PA, 15260, USAhua@cs.pitt.eduDiane LitmanUniversity of PittsburghPittsburgh PA, 15260, USAlitman@cs.pitt.eduAbstractUser simulations are shown to be useful inspoken dialog system development.
Sincemost current user simulations deploy prob-ability models to mimic human user be-haviors, how to set up user action proba-bilities in these models is a key problemto solve.
One generally used approach isto estimate these probabilities from humanuser data.
However, when building a newdialog system, usually no data or only asmall amount of data is available.
In thisstudy, we compare estimating user proba-bilities from a small user data set versushandcrafting the probabilities.
We discussthe pros and cons of both solutions for dif-ferent dialog system development tasks.1 IntroductionUser simulations are widely used in spoken di-alog system development.
Recent studies useuser simulations to generate training corpora tolearn dialog strategies automatically ((Williamsand Young, 2007), (Lemon and Liu, 2007)), or toevaluate dialog system performance (Lo?pez-Co?zaret al, 2003).
Most studies show that using usersimulations significantly improves dialog systemperformance as well as speeds up system devel-opment.
Since user simulation is such a usefultool, dialog system researchers have studied howto build user simulations from a variety of perspec-tives.
Some studies look into the impact of trainingdata on user simulations.
For example, (Georgilaet al, 2008) observe differences between simu-lated users trained from human users of differentage groups.
Other studies explore different simu-lation models, i.e.
the mechanism of deciding thenext user actions given the current dialog context.
(Schatzmann et al, 2006) give a thorough reviewof different types of simulation models.
Sincemost of these current user simulation techniquesuse probabilistic models to generate user actions,how to set up the probabilities in the simulationsis another important problem to solve.One general approach to set up user action prob-abilities is to learn the probabilities from a col-lected human user dialog corpus ((Schatzmann etal., 2007b), (Georgila et al, 2008)).
While thisapproach takes advantage of observed user behav-iors in predicting future user behaviors, it suffersfrom the problem of learning probabilities fromone group of users while potentially using themwith another group of users.
The accuracy of thelearned probabilities becomes more questionablewhen the collected human corpus is small.
How-ever, this is a common problem in building newdialog systems, when often no data1 or only asmall amount of data is available.
An alterna-tive approach is to handcraft user action proba-bilities ((Schatzmann et al, 2007a), (Janarthanamand Lemon, 2008)).
This approach is less data-intensive, but requires nontrivial work by domainexperts.
What is more, as the number of proba-bilities increases, it is hard even for the experts toset the probabilities.
Since both handcrafting andtraining user action probabilities have their ownpros and cons, it is an interesting research ques-tion to investigate which approach is better for acertain task given the amount of data that is avail-able.In this study, we investigate a manual and atrained approach in setting up user action proba-bilities, applied to building the same probabilis-tic simulation model.
For the manual user simula-tions, we look into two sets of handcrafted proba-bilities which use the same expert knowledge butdiffer in individual probability values.
This aimsto take into account small variations that can possi-1When no human user data is collected with the dialogsystem, Wizard-of-Oz experiments can be conducted to col-lect training data for building user simulations.888bly be introduced by different domain experts.
Forthe trained user simulations, we examine two setsof probabilities trained from user corpora of dif-ferent sizes, since the amount of training data willimpact the quality of the trained probability mod-els.
We compare the trained and the handcraftedsimulations on three tasks.
We observe that in ourtask settings, the two manual simulations do notdiffer significantly on any tasks.
In addition, thereis no significant difference among the trained andthe manual simulations in generating corpus leveldialog behaviors as well as in generating trainingcorpora for learning dialog strategies.
When com-paring on a dialog system evaluation task, the sim-ulation trained from more data significantly out-performs the two manual simulations, which againoutperforms the simulation trained from less data.Based on our observations, we answer the orig-inal question of how to design user action proba-bilities for simulations that are similar to ours interms of the complexity of the simulations2.
Wesuggest that handcrafted user simulations can per-form reasonably well in building a new dialog sys-tem, especially when we are not sure that there isenough data for training simulation models.
How-ever, once we have a dialog system, it is use-ful to collect human user data in order to train anew user simulation model since the trained sim-ulations perform better than the handcrafted usersimulations on more tasks.
Since how to decidewhether enough data is available for simulationtraining is another research question to answer, wewill further discuss the impact of our results laterin Section 6.2 Related WorkMost current simulation models are probabilisticmodels in which the models simulate user actionsbased on dialog context features (Schatzmann etal., 2006).
We represent these models as:P (user action|feature1, .
.
.,featuren) (1)The number of probabilities involved in thismodel is:(# of possible actions-1) ?n?k=1(# of feature values).
(2)Some studies handcraft these probabilities.
Forexample, (Schatzmann et al, 2007a) condition the2The number of user action probabilities and the simu-lated user behaviors will impact the design choice.user actions on user?s goals and the agenda toreach those goals.
They manually author the prob-abilities in the user?s agenda update model and thegoal update model, and then calculate the user ac-tion probabilities based on the two models.
(Ja-narthanam and Lemon, 2008) handcraft 15 proba-bilities in simulated users?
initial profiles and thenauthor rules to update these probabilities duringthe dialogs.Other studies use a human user corpus as thetraining corpus to learn user action probabilitiesin user simulations.
Since the human user cor-pus often does not include all possible actions thatusers may take during interactions with the dialogsystem, different strategies are used to account foruser actions that do not appear in the training cor-pus but may be present when testing the user sim-ulations.
For example, (Schatzmann et al, 2007b)introduce a summary space approach to map theactual dialog context space into a more tractablesummary space.
Then, they use forward and back-ward learning algorithms to learn the probabili-ties from a corpus generated by 40 human users(160 dialogs).
(Rieser and Lemon, 2006) use atwo step approach in computing the probabilitiesfrom a corpus consisting of dialogs from 24 hu-man users (70 dialogs).
They first cluster dialogcontexts based on selected features and then buildconditional probability models for each cluster.In our study, we build a conditional probabilitymodel which will be described in detail in Sec-tion 3.2.1.
There are 40 probabilities to set up inthis model3.
We will explain different approachesto assign these probabilities later in Section 3.2.2.3 System and User SimulationsIn this section, we describe the dialog system, thehuman user corpus we collected with the system,and the user simulation we used.3.1 System and CorpusThe ITSPOKE system (Litman and Silliman,2004) is an Intelligent Tutoring System whichteaches Newtonian physics.
It is a speech-enhanced version of the Why2-Atlas tutoring sys-tem (Vanlehn et al, 2002).
During the interac-tion with students, the system initiates a spokentutoring dialog to correct misconceptions and to3There are 2 possible actions in our model, 20 possiblevalues for the first feature qCluster and 2 possible values forthe second feature prevCorrectness as described later in Sec-tion 3.2.1.
Using Equation 2, 40=(2-1)*20*2.889SYSTEM1: Do you recall what Newton?sthird law says?
[3rdLaw]Student1: Force equals mass timesacceleration.
[ic, c%=0, ncert]SYSTEM2: Newton?s third law says ...If you hit the wall harder, is theforce of your fist acting on thewall greater or less?
[3rdLaw]Student2: Greater.
[c, c%=50%,cert]Dialog goes onTable 1: Sample coded dialog excerpt.elicit further explanation.
A pretest is given beforethe interaction and a posttest is given afterwards.We calculate a Normalized Learning Gain for eachstudent to evaluate the performance of the systemin terms of the student?s knowledge gain:NLG = posttest score - pretest score1-pretest score (3)The current tutoring dialog strategy was hand-crafted in a finite state paradigm by domain ex-perts, and the tutor?s response is based only on thecorrectness of the student?s answer4.
However, tu-toring research (Craig et al, 2004) suggests thatother underlying information in student utterances(e.g., student certainty) is also useful in improvinglearning.
Therefore, we are working on learninga dialog strategy to also take into account studentcertainty.In our prior work, a corpus of 100 dialogs (1388student turns) was collected between 20 humansubjects (5 dialogs per subject) and the ITSPOKEsystem.
Correctness (correct(c), incorrect(ic)) isautomatically judged by the system and is kept inthe system?s logs.
We also computed the student?scorrectness rate (c%) and labeled it after everystudent turn.
Each student utterance was manu-ally annotated for certainty (certain(cert), notcer-tain(ncert)) in a previous study based on both lex-ical and prosodic information5.
In addition, wemanually clustered tutor questions into 20 clustersbased on the knowledge that is required to answerthat question, e.g.
questions on Newton?s ThirdLaw are put into a cluster labeled as (3rdLaw).There are other clusters such as gravity, acceler-ation, etc.
An example of a coded dialog betweenthe system and a student is given in Table 1.4Despite the limitation of the current system, studentslearn significantly after interacting with the system.5Kappa of 0.68 is gained in the agreement study.3.2 User Simulation Model and ModelProbabilities Set-up3.2.1 User Simulation ModelWe build a Knowledge Consistency Model6 (KCModel) to simulate consistent student behaviorswhile interacting with a tutoring system.
Ac-cording to learning literature (Cen et al, 2006),once a student acquires certain knowledge, his/herperformance on similar problems that requirethe same knowledge (i.e.
questions from thesame cluster we introduced in Section 3.1) willbecome stable.
Therefore, in the KC Model,we condition the student action stuAction basedon the cluster of tutor question (qCluster) andthe student?s correctness when last encounteringa question from that cluster (prevCorrectness):P (stuAction|qCluster, prevCorrectness).
Forexample, in Table 1, when deciding the student?sanswer after the second tutor question, the simu-lation looks back into the dialog and finds out thatthe last time (in Student1) the student answereda question from the same cluster 3rdLaw incor-rectly.
Therefore, this time the simulation givesa correct student answer based on the probabilityP (c|3rdLaw, ic).Since different groups of students often havedifferent learning abilities, we examine such dif-ferences among our users by grouping the usersbased on Normalized Learning Gains (NLG),which is an important feature to describe user be-haviors in tutoring systems.
By dividing our hu-man users into high/low learners based on the me-dian of NLG, we find a significant difference in theNLG of the two groups based on 2-tailed t-tests(p < 0.05).
Therefore, we construct a simula-tion to represent low learners and another simula-tion to represent high learners to better character-ize the differences in high/low learners?
behaviors.Similar approaches are adopted in other studies inbuilding user simulations for dialog systems (e.g.,(Georgila et al, 2008) simulate old versus youngusers separately).Our simulation models work on the word level 7because generating student dialog acts alone doesnot provide sufficient information for our tutoringsystem to decide the next system action.
Since itis hard to generate a natural language utterance foreach tutor?s question, we use the student answers6This is the best model we built in our previous studies(Ai and Litman, 2007).7See (Ai and Litman, 2006) for more details.890in the human user corpus as the candidate answersfor the simulated students.3.2.2 Model Probabilities Set-upNow we discuss how to set up user action prob-abilities in the KC Model.
We compare learningprobabilities from human user data to handcraftingprobabilities based on expert knowledge.
Since werepresent high/low learners using different mod-els, we build simulation models with separate useraction probabilities to represent the two groups oflearners.When learning the probabilities in the TrainedKC Models, we calculate user action probabilitiesfor high/low learners in our human corpus sepa-rately.
We use add-one smoothing to account foruser actions that do not appear in the human usercorpus.
For the first time the student answers aquestion in a certain cluster, we back-off the useraction probability to P(stuAction | average cor-rectness rate of this question in human user cor-pus).
We first train a KC model using the datafrom all 20 human users to build the TrainedMore(Tmore) Model.
Then, in order to investigate theimpact of the amount of training data on the qual-ity of trained simulations, we randomly pick 5 outof the 10 high learners and 5 out of the 10 lowlearners to get an even smaller human user corpus.We train the TrainedLess (Tless) Model from thissmall corpus .When handcrafting the probabilities in the Man-ual KC Models8, the clusters of questions arefirst grouped into three difficulty groups (Easy,Medium, Hard).
Based on expert knowledge,we assume on average 70% of students can cor-rectly answer the tutor questions from the Easygroup, while for the Medium group only 60%and for the hard group 50%.
Then, we assigna correctness rate higher than the average forthe high learners and a corresponding correctnessrate lower than the average for the low learners.For the first Manual KC model (M1), within thesame difficulty group, the same two probabilitiesP1(stuAction|qClusteri, prevCorrectness = c) andP2(stuAction|qClusteri, prevCorrectness = ic) areassigned to each clusteri as the averages for thecorresponding high/low learners.
Since a differenthuman expert will possibly provide a slightly dif-ferent set of probabilities even based on the samemechanism, we also design another set of prob-8The first author of the paper acts as the domain expert.abilities to account for such variations.
For thesecond Manual KC model (M2), we allow dif-ferences among the clusters within the same dif-ficulty group.
For the clusters in each difficultygroup, we randomly assign a probability that dif-fers no more than 5% from the average.
For exam-ple, for the easy clusters, we assign average proba-bilities of high/low learners between [65%, 75%].Although human experts may differ to some ex-tent in assigning individual probability values, wehypothesize that in general a certain amount of ex-pertise is required in assigning these probabilities.To investigate this, we build a baseline simula-tion with no expert knowledge, which is a Ran-dom Model (Ran) that randomly assigns valuesfor these user action probabilities.4 Evaluation MeasuresIn this section, we introduce the evaluation mea-sures for comparing the simulated corpora gen-erated by different simulation models to the hu-man user corpus.
In Section 4.1, we use a set ofwidely used domain independent features to com-pare the simulated and the human user corporaon corpus-level dialog behaviors.
These compar-isons give us a direct impression of how similarthe simulated dialogs are to human user dialogs.Then, we compare the simulations in task-orientedcontexts.
Since simulated user corpora are oftenused as training corpora for using MDPs to learnnew dialog strategies, in Section 4.2 we estimatehow different the learned dialog strategies wouldbe when trained from different simulated corpora.Another way to use user simulation is to test dialogsystems.
Therefore, in Section 4.3, we comparethe user actions predicted by the various simula-tion models with actual human user actions.4.1 Measures on Corpus Level DialogBehaviorsWe compare the dialog corpora generated by usersimulations to our human user corpus using a com-prehensive set of corpus level measures proposedby (Schatzmann et al, 2005).
Here, we use a sub-set of the measures which describe high-level dia-log features that are applicable to our data.
Themeasures we use include the number of studentturns (Sturn), the number of tutor turns (Tturn), thenumber of words per student turn (Swordrate), thenumber of words per tutor turn (Twordrate), the ra-tio of system/user words per dialog (WordRatio),891and the percentage of correct answers (cRate).4.2 Measures on Dialog Strategy LearningIn this section, we introduce two measures to com-pare the simulations based on their performanceon a dialog strategy learning task.
In recent stud-ies (e.g., (Janarthanam and Lemon, 2008)), usersimulations are built to generate a large corpusto build MDPs in using Reinforcement Learning(RL) to learn new dialog strategies.
When buildingan MDP from a training corpus9, we compute thetransition probabilities P (st+1|st, a) (the proba-bility of getting from state st to the next state st+1after taking action a), and the reward of this transi-tion R(st, a, st+1).
Then, the expected cumulativevalue (V-value) of a state s can be calculated usingthis recursive function:V (s) =?st+1P (st+1|st, a)[R(st, a, st+1) + ?V (st+1)](4)?
is a discount factor which ranges between 0 and1.For our evaluation, we first compare the tran-sition probabilities calculated from all simulatedcorpora.
The transition probabilities are only de-termined by the states and user actions presentedby the training corpus, regardless of the rest of theMDP configuration.
Since the MDP configurationhas a big impact on the learned strategies, we wantto first factor this impact out and estimate the dif-ferences in learned strategies that are brought inby the training corpora alone.
As a second evalua-tion measure, we apply reinforcement learning tothe MDP representing each simulated corpus sep-arately to learn dialog strategies.
We compare theExpected Cumulative Rewards (ECRs)(Williamsand Young, 2007) of these dialog strategies, whichshow the expectation of the rewards we can obtainby applying the learned strategies.The MDP learning task in our study is to max-imize student certainty during tutoring dialogs.The dialog states are characterized using the cor-rectness of the current student answer and the stu-dent correctness rate so far.
We represent the cor-rectness rate as a binary feature: lc if it is belowthe training corpus average and hc if it is above theaverage.
The end of dialog reward is assigned tobe +100 if the dialog has a percent certainty higher9In this paper, we use off-line model-based RL (Paek,2006) rather than learning an optimal strategy online duringsystem-user interactions.than the median from the training corpus and -100otherwise.
The action choice of the tutoring sys-tem is to give a strong (s) or weak (w) feedback.A strong feedback clearly indicates the correctnessof the current student answer while the weak feed-back does not.
For example, the second systemturn in Table 1 contains a weak feedback.
If thesystem says ?Your answer is incorrect?
at the be-ginning of this turn, that would be a strong feed-back.
In order to simulate student certainty, wesimply output the student certainty originally asso-ciated in each student utterance.
Thus, the outputof the KC Models here is a student utterance alongwith the student certainty (cert, ncert).
In a pre-vious study (Ai et al, 2007), we investigated theimpact of different MDP configurations by com-paring the ECRs of the learned dialog strategies.Here, we use one of the best-performing MDPconfigurations, but vary the simulated corpora thatwe train the dialog strategies on.
Our goal is to seewhich user simulation performs better in generat-ing a training corpus for dialog strategy learning.4.3 Measures on Dialog System EvaluationIn this section, we introduce two ways to com-pare human user actions with the actions predictedby the simulations.
The aim of this comparisonis to assess how accurately the simulations canreplicate human user behaviors when encounter-ing the same dialog situation.
A simulated userthat can accurately predict human user behaviorsis needed to replace human users when evaluatingdialog systems.We randomly divide the human user dialog cor-pus into four parts: each part contains a balancedamount of high/low learner data.
Then we performfour fold cross validation by always using 3 partsof the data as our training corpus for user simula-tions, and the remaining one part of the data astesting data to compare with simulated user ac-tions.
We always compare high human learnersonly with simulation models that represent highlearners and low human learners only with simu-lation models that represent low learners.
Compar-isons are done on a turn by turn basis.
Every timethe human user takes an action in the dialogs in thetesting data, the user simulations are used to pre-dict an action based on related dialog informationfrom the human user dialog.
For a KC Model, therelated dialog information includes qCluster andprevCorrectness .
We first compare the simulation892predicted user actions directly with human user ac-tions.
We define simulation accuracy as:Accuracy = Correctly predicted human user actionsTotal number of human user actions (5)However, since our simulation model is a prob-abilistic model, the model will take an actionstochastically after the same tutor turn.
In otherwords, we need to take into account the probabil-ity for the simulation to predict the right humanuser action.
If the simulation outputs the right ac-tion with a small probability, it is less likely thatthis simulation can correctly predict human userbehaviors when generating a large dialog corpus.We consider a simulated action associated with ahigher probability to be ranked higher than an ac-tion with a lower probability.
Then, we use the re-ciprocal ranking from information retrieval tasks(Radev et al, 2002) to assess the simulation per-formance10.
Mean Reciprocal Ranking is definedas:MRR = 1AA?k=11ranki (6)In Equation 6, A stands for the total number ofhuman user actions, ranki stands for the rankingof the simulated action which matches the i-th hu-man user action.Table 2 shows an example of comparing simu-lated user actions with human user actions in thesample dialog in Table 1.
In the first turn Stu-dent1, a simulation model has a 60% chance tooutput an incorrect answer and a 40% chance tooutput a correct answer while it actually outputsan incorrect answer.
In this case, we consider thesimulation ranks the actions in the order of: ic, c.Since the human user gives an incorrect answer atthis time, the simulated action matches with thishuman user action and the reciprocal ranking is1.
However, in the turn Student2, the simulation?soutput does not match the human user action.
Thistime, the correct simulated user action is rankedsecond.
Therefore, the reciprocal ranking of thissimulation action is 1/2.We hypothesize that the measures introducedin this section have larger power in differentiat-ing different simulated user behaviors since every10(Georgila et al, 2008) use Precision and Recall to cap-ture similar information as our accuracy, and Expected Pre-cision and Expected Recall to capture similar information asour reciprocal ranking.simulated user action contributes to the compar-ison between different simulations.
In contrast,the measures introduced in Section 4.1 and Sec-tion 4.2 have less differentiating power since theycompare at the corpus level.5 ResultsWe let al user simulations interact with our dia-log system, where each simulates 250 low learnersand 250 high learners.
In this section, we reportthe results of applying the evaluation measures wediscuss in Section 4 on comparing simulated andhuman user corpora.
When we talk about signifi-cant results in the statistics tests below, we alwaysmean that the p-value of the test is ?
0.05.5.1 Comparing on Corpus Level DialogBehaviorFigure 1 shows the results of comparisons usingdomain independent high-level dialog features ofour corpora.
The x-axis shows the evaluation mea-sures; the y-axis shows the mean for each corpusnormalized to the mean of the human user cor-pus.
Error bars show the standard deviations ofthe mean values.
As we can see from the figure,the Random Model performs differently from thehuman and all the other simulated models.
Thereis no difference in dialog behaviors among the hu-man corpus, the trained and the manual simulatedcorpora.In sum, both the Trained KC Models andthe Manual KC Models can generate human-likehigh-level dialog behaviors while the RandomModel cannot.5.2 Comparing on Dialog Strategy LearningTaskNext, we compare the difference in dialog strategylearning when training on the simulated corporausing similar approaches in (Tetreault and Litman,2008).
Table 3 shows the transition probabilitiesstarting from the state (c, lc).
For example, thefirst cell shows in the Tmore corpus, the probabil-ity of starting from state (c, lc), getting a strongfeedback, and transitioning into the same state is24.82%.
We calculate the same table for the otherthree states (c, hc), (ic, lc), and (ic, hc).
Usingpaired-sample t-tests with bonferroni corrections,the only significant differences are observed be-tween the random simulated corpus and each ofthe other simulated corpora.893i-th Turn human Simulation Model Simulation Output CorrectlyPredictedActions ReciprocalRankingStudent1 ic 60% ic, 40% c ic 1 1Student2 c 70% ic, 30% c ic 0 1/2Average / / / (1+0)/2 (1+1/2)/2Table 2: An Example of Comparing Simulated Actions with Human User Actions.Figure 1: Comparison of human and simulated dialogs by high-level dialog features.Tmore Tless M1 M2 Rans?c lc 24.82 31.42 25.64 22.70 13.25w?c lc 17.64 12.35 16.62 18.85 9.74s?ic lc 2.11 7.07 1.70 1.63 19.31w?ic lc 1.80 2.17 2.05 3.25 21.06s?c hc 29.95 26.46 22.23 31.04 10.54w?c hc 13.93 9.50 22.73 15.10 11.29s?ic hc 5.52 2.51 4.29 0.54 7.13w?ic hc 4.24 9.08 4.74 6.89 7.68Table 3: Comparisons of MDP transition proba-bilities at state (c, lc) (Numbers in this table arepercentages).Tmore Tless M1 M2 RanECR 15.10 11.72 15.24 15.51 7.03CI ?2.21 ?1.95 ?2.07 ?3.46 ?2.11Table 4: Comparisons of ECR of learned dialogstrategies.We also use a MDP toolkit to learn dialog strate-gies from all the simulated corpora and then com-pute the Expected Cumulative Reward (ECR) forthe learned strategies.
In Table 4, the upper partof each cell shows the ECR of the learned dialogstrategy; the lower part of the cell shows the 95%Confidence Interval (CI) of the ECR.
We can seefrom the overlap of the confidence intervals thatthe only significant difference is observed betweenthe dialog strategy trained from the random simu-lated corpus and the strategies trained from eachof the other simulated corpora.
Also, it is inter-esting to see that the CI of the two manual simu-lations overlap more with the CI of Tmore modelthan with the CI of the Tless model.In sum, the manual user simulations work aswell as the trained user simulation when beingused to generate a training corpus to apply MDPsto learn new dialog strategies.Tmore Tless M1 M2 RanAccu- 0.78 0.60 0.70 0.72 0.41racy (?0.01) (?0.02) (?0.02) (?0.02) (?0.02)MRR 0.72 0.52 0.63 0.64 0.32(?0.02) (?0.02) (?0.02) (?0.01) (?0.02)Table 5: Comparisons of correctly predicted hu-man user actions.5.3 Comparisons in Dialog SystemEvaluationFinally, we compare how accurately the user sim-ulations can predict human user actions given thesame dialog context.
Table 5 shows the averagesand CIs (in parenthesis) from the four fold crossvalidations.
The second row shows the resultsbased on direct comparisons with human user ac-tions, and the third row shows the mean recipro-cal ranking of simulated actions.
We observe thatin terms of both the accuracy and the reciprocalranking, the performance ranking from the high-est to the lowest (with significant difference be-tween adjacent ranks) is: the Tmore Model, bothof the manual models (no significant differencesbetween these two models), the Tless Model, andthe Ran Model.
Therefore, we suggest that thehandcrafted user simulation is not sufficient to beused in evaluating dialog systems because it doesnot generate user actions that are as similar to hu-man user actions.
However, the handcrafted usersimulation is still better than a user simulationtrained with not enough training data.
This re-sult also indicates that this evaluation measure hasmore differentiating power than the previous mea-sures since it captures significant differences thatare not shown by the previous measures.In sum, the Tmore simulation performs the bestin predicting human user actions.8946 Conclusion and Future WorkSetting up user action probabilities in user sim-ulation is a non-trivial task, especially when notraining data or only a small amount of data isavailable.
In this study, we compare several ap-proaches in setting up user action probabilitiesfor the same simulation model: training from allavailable human user data, training from half ofthe available data, two handcrafting approacheswhich use the same expert knowledge but differslightly in individual probability assignments, anda baseline approach which randomly assigns alluser action probabilities.
We compare the builtsimulations from different aspects.
We find thatthe two trained simulations and the two hand-crafted simulations outperform the random simu-lation in all tasks.
No significant difference is ob-served among the trained and the handcrafted sim-ulations when comparing their generated corporaon corpus-level dialog features as well as whenserving as the training corpora for learning dialogstrategies.
However, the simulation trained fromall available human user data can predict humanuser actions more accurately than the handcraftedsimulations, which again perform better than themodel trained from half of the human user corpus.Nevertheless, no significant difference is observedbetween the two handcrafted simulations.Our study takes a first step in comparing thechoices of handcrafting versus training user simu-lations when only limited or even no training datais available, e.g., when constructing a new dialogsystem.
As shown for our task setting, both typesof user simulations can be used in generating train-ing data for learning new dialog strategies.
How-ever, we observe (as in a prior study by (Schatz-mann et al, 2007b)) that the simulation trainedfrom more user data has a better chance to outper-form the simulation trained from less training data.We also observe that a handcrafted user simulationwith expert knowledge can reach the performanceof the better trained simulation.
However, a cer-tain level of expert knowledge is needed in hand-crafting user simulations since a random simula-tion does not perform well in any tasks.
Therefore,our results suggest that if an expert is available fordesigning a user simulation when not enough userdata is collected, it may be better to handcraft theuser simulation than training the simulation fromthe small amount of human user data.
However,it is another open research question to answer howmuch data is enough for training a user simulation,which depends on many factors such as the com-plexity of the user simulation model.
When usingsimulations to test a dialog system, our results sug-gest that once we have enough human user data, itis better to use the data to train a new simulationto replace the handcrafted simulation.In the future, we will conduct follow up stud-ies to confirm our current findings since there areseveral factors that can impact our results.
Firstof all, our current system mainly distinguishes thestudent answers as correct and incorrect.
We arecurrently looking into dividing the incorrect stu-dent answers into more categories (such as par-tially correct answers, vague answers, or over-specific answers) which will increase the numberof simulated user actions.
Also, although the sizeof the human corpus which we build the traineduser simulations from is comparable to other stud-ies (e.g., (Rieser and Lemon, 2006), (Schatzmannet al, 2007b)), using a larger human corpus mayimprove the performance of the trained simula-tions.
We are in the process of collecting anothercorpus which will consist of 60 human users (300dialogs).
We plan to re-train a simulation whenthis new corpus is available.
Also, we would beable to train more complex models (e.g., a simula-tion model which takes into account a longer dia-log history) with the extra data.
Finally, althoughwe add some noise into the current manual simula-tion designed by our domain expert to account forvariations of expert knowledge, we would like torecruit another human expert to construct a newmanual simulation to compare with the existingsimulations.
It would also be interesting to repli-cate our experiments on other dialog systems tosee whether our observations will generalize.
Ourlong term goal is to provide guidance of how to ef-fectively build user simulations for different dialogsystem development tasks given limited resources.AcknowledgmentsThe first author is supported by Mellon Fellow-ship from the University of Pittsburgh.
This workis supported partially by NSF 0325054.
We thankK.
Forbes-Riley, P. Jordan and the anonymous re-viewers for their insightful suggestions.ReferencesH.
Ai and D. Litman.
2006.
Comparing Real-Real,Simulated-Simulated, and Simulated-Real Spoken895Dialogue Corpora.
In Proc.
of the AAAI Workshopon Statistical and Empirical Approaches for SpokenDialogue Systems.H.
Ai and D. Litman.
2007.
Knowledge ConsistentUser Simulations for Dialog Systems.
In Proc.
ofInterspeech 2007.H.
Ai, J. Tetreault, and D. Litman.
2007.
ComparingUser Simulation Models for Dialog Strategy Learn-ing.
In Proc.
of NAACL-HLT 2007.H.
Cen, K. Koedinger and B. Junker.
2006.
Learn-ing Factors Analysis-A General Method for Cogni-tive Model Evaluation and Improvement.
In Proc.
of8th International Conference on ITS.S.
Craig, A. Graesser, J. Sullins, and B. Gholson.
2004.Affect and learning: an exploratory look into therole of affect in learning with AutoTutor.
Journalof Educational Media 29(3), 241250.K.
Georgila, J. Henderson, and O.
Lemon.
2005.Learning User Simulations for Information StateUpdate Dialogue Systems.
In Proc.
of Interspeech2005.K.
Georgila, M. Wolters, and J. Moore.
2008.
Simu-lating the Behaviour of Older versus Younger Userswhen Interacting with Spoken Dialogue Systems.
InProc.
of 46th ACL.S.
Janarthanam and O.
Lemon.
2008.
User simulationsfor online adaptation and knowledge-alignment inTroubleshooting dialogue systems.
In Proc.
of the12th SEMdial Workshop on on the Semantics andPragmatics of Dialogues.O.
Lemon and X. Liu.
2007.
Dialogue Policy Learn-ing for combinations of Noise and User Simulation:transfer results.
In Proc.
of 8th SIGdial.D.
Litman and S. Silliman.
2004.
ITSPOKE: An Intel-ligent Tutoring Spoken Dialogue System.
In Com-panion Proc.
of the Human Language Technology:NAACL.R.
Lo?pez-Co?zar, A.
De la Torre, J. C. Segura and A.J.
Rubio.
2003.
Assessment of dialogue systems bymeans of a new simulation technique.
Speech Com-munication (40): 387-407.T.
Paek.
2006.
Reinforcement learning for spo-ken dialogue systems: Comparing strengths andweaknesses for practical deployment.
In Proc.of Interspeech-06 Workshop on ?Dialogue on Dia-logues - Multidisciplinary Evaluation of AdvancedSpeech-based Interacive Systems?.D.
Radev, H. Qi, H. Wu, and W. Fan.
2002.
Evaluatingweb-based question answering systems.
In Proc.
ofLREC 2002.V.
Rieser and O.
Lemon.
2006.
Cluster-based UserSimulations for Learning Dialogue Strategies.
InProc.
of Interspeech 2006.J.
Schatzmann, K. Georgila, and S. Young.
2005.Quantitative Evaluation of User Simulation Tech-niques for Spoken Dialogue Systems.
In Proc.
of 6thSIGDial.J.
Schatzmann, K. Weilhammer, M. Stuttle, and S.Young.
2006.
A Survey of Statistical User Simula-tion Techniques for Reinforcement-Learning of Di-alogue Management Strategies.
Knowledge Engi-neering Review 21(2): 97-126.J.
Schatzmann, B. Thomson, K. Weilhammer, H. Ye,and S. Young.
2007a.
Agenda-based User Simula-tion for Bootstrapping a POMDP Dialogue System.In Proc.
of HLT/NAACL 2007.J.
Schatzmann, B. Thomson and S. Young.
2007b.
Sta-tistical User Simulation with a Hidden Agenda.
InProc.
of 8th SIGdial.J.
Tetreault and D. Litman.
2008.
A ReinforcementLearning Approach to Evaluating State Representa-tions in Spoken Dialogue Systems.
Speech Commu-nication (Special Issue on Evaluating new methodsand models for advanced speech-based interactivesystems), 50(8-9): 683-696.K.
VanLehn, P. Jordan, C.
Rose?, D. Bhembe, M.Bo?ttner, A. Gaydos, M. Makatchev, U. Pap-puswamy, M. Ringenberg, A. Roque, S. Siler, R.Srivastava, and R. Wilson.
2002.
The architectureof Why2-Atlas: A coach for qualitative physics es-say writing.
In Proc.
Intelligent Tutoring SystemsConference..J. Williams and S. Young.
2007.
Partially ObservableMarkov Decision Processes for Spoken Dialog Sys-tems.
Computer Speech and Language 21(2): 231-422.896
