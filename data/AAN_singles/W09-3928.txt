Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 188?195,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsThe Role of Interactivity in Human-Machine Conversation for AutomaticWord AcquisitionShaolin Qu Joyce Y. ChaiDepartment of Computer Science and EngineeringMichigan State UniversityEast Lansing, MI 48824{qushaoli,jchai}@cse.msu.eduAbstractMotivated by the psycholinguistic findingthat human eye gaze is tightly linked tospeech production, previous work has ap-plied naturally occurring eye gaze for au-tomatic vocabulary acquisition.
However,unlike in the typical settings for psycholin-guistic studies, eye gaze can serve differ-ent functions in human-machine conver-sation.
Some gaze streams do not linkto the content of the spoken utterancesand thus can be potentially detrimental toword acquisition.
To address this prob-lem, this paper investigates the incorpo-ration of interactivity in identifying theclose coupling of speech and gaze streamsfor word acquisition.
Our empirical re-sults indicate that automatic identificationof closely coupled gaze-speech streamsleads to significantly better word acquisi-tion performance.1 IntroductionSpoken conversational interfaces have become in-creasingly important in many applications suchas remote interaction with robots (Lemon et al,2002), intelligent space station control (Aist etal., 2003), and automated training and educa-tion (Razzaq and Heffernan, 2004).
As in any con-versational system, one major bottleneck in con-versational interfaces is robust language interpre-tation.
To address this problem, previous multi-modal conversational systems have utilized pen-based or deictic gestures (Bangalore and John-ston, 2004; Qu and Chai, 2006) to improve in-terpretation.
Besides gestures, eye movementsthat naturally occur during interaction provide an-other important channel for language understand-ing, for example, reference resolution (Byron etal., 2005; Prasov and Chai, 2008).
Recent workhas also shown that what users look at on the inter-face (e.g., natural scenes or generated graphic dis-plays) during speech production provides uniqueopportunities for word acquisition, namely auto-matically acquiring semantic meanings of spokenwords by grounding them to visual entities (Liuet al, 2007) or domain concepts (Qu and Chai,2008).Psycholinguistic studies have shown that eyegaze indicates a person?s attention (Just and Car-penter, 1976), and eye movement can facilitatespoken language comprehension (Tanenhaus etal., 1995; Eberhard et al, 1995).
It has beenfound that users?
eyes move to the mentioned ob-ject directly before speaking a word (Meyer etal., 1998; Rayner, 1998; Griffin and Bock, 2000).This parallel behavior of eye gaze and speech pro-duction motivates our previous work on word ac-quisition (Liu et al, 2007; Qu and Chai, 2008).However, in interactive conversation, human gazebehavior is much more complex than in the typ-ical controlled settings used in psycholinguisticstudies.
There are different types of eye move-ments (Kahneman, 1973).
The naturally occur-ring eye gaze during speech production may servedifferent functions, for example, to engage in theconversation or to manage turn taking (Nakano etal., 2003).
Furthermore, while interacting with agraphic display, a user could be talking about ob-jects that were previously seen on the display orsomething completely unrelated to any object theuser is looking at.
Therefore using every speech-gaze pair for word acquisition can be detrimental.The type of gaze that is mostly useful for wordacquisition is the kind that reflects the underlyingattention and tightly links to the content of the co-occurring speech.
Thus, one important questionis how to identify the closely coupled speech andgaze streams to improve word acquisition.To address this question, we develop an ap-proach that incorporates interactivity (e.g., speech,188user activity, conversation context) with eye gazeto identify closely coupled speech and gazestreams.
We further use the identified speechand gaze streams to acquire words with a trans-lation model.
Our empirical evaluation demon-strates that automatic identification of closely cou-pled gaze-speech streams can lead to significantlybetter word acquisition performance.2 Related WorkPrevious work has explored word acquisition bygrounding words to visual entities.
In (Roy andPentland, 2002), given speech paired with videoimages of objects, mutual information betweenauditory and visual signals was used to acquirewords by associating acoustic phone sequenceswith the visual prototypes (e.g., color, size, shape)of objects.
Given parallel pictures and descrip-tion texts, generative models were used to acquirewords by associating words with image regions in(Barnard et al, 2003).
Different from this previouswork, in our work, the visual attention foci accom-panying speech are indicated by eye gaze.
As animplicit and subconscious input, eye gaze bringsadditional challenges in word acquisition.Eye gaze has been explored for word acqui-sition in previous work.
In (Yu and Ballard,2004), given speech paired with eye gaze andvideo images, a translation model was used toacquire words by associating acoustic phone se-quences with visual representations of objects andactions.
Word acquisition from transcribed speechand eye gaze during human-machine conversa-tion has been investigated recently.
In (Liu etal., 2007), a translation model was developed toassociate words with visual objects on a graphi-cal display.
In our previous work (Qu and Chai,2008), enhanced translation models incorporat-ing speech-gaze temporal information and domainknowledge were developed to improve word ac-quisition.
However, none of these previous workshas investigated the role of interactivity in wordacquisition, which is the focus of this paper.3 Data CollectionWe collected speech and eye gaze data throughuser studies.
This data set is different from the dataset used in our previous work (Qu and Chai, 2008).The difference lies in two aspects: 1) the data forthis investigation was collected during mixed ini-tiative human-machine conversation whereas thedata in (Qu and Chai, 2008) was based only onquestion and answering; 2) user studies were con-ducted in a more complex domain for this investi-gation, which resulted in a richer data set that con-tains a larger vocabulary.3.1 DomainFigure 1: Treasure hunting domainFigure 1 shows the 3D treasure hunting domainused in our work.
In this application, the userneeds to consult with a remote ?expert?
(i.e., an ar-tificial system) to find hidden treasures in a castlewith 115 3D objects.
The expert has some knowl-edge about the treasures but can not see the cas-tle.
The user has to talk to the expert for adviceregarding finding the treasures.
The application isdeveloped based on a game engine and provides animmersive environment for the user to navigate inthe 3D space.
During the experiment, each user?sspeech was recorded, and the user?s eye gaze wascaptured by a Tobii eye tracker.3.2 Data PreprocessingFrom 20 users?
experiments, we collected 3709 ut-terances with accompanying gaze fixations.
Wetranscribed the collected speech.
The vocabularysize of the speech transcript is 1082, among which227 are either nouns or adjectives.
The user?sspeech was also automatically recognized onlineby the Microsoft speech recognizer with a worderror rate (WER) of 48.1% for the 1-best recog-nition.
The vocabulary size of the 1-best speechrecognition is 3041, among which 1643 are eithernouns or adjectives.The collected speech and gaze streams were au-tomatically paired together by the system.
Eachtime the system detected a sentence boundary (in-dicated by a long pause of 500 milliseconds) of theuser?s speech, it paired the recognized speech withthe gaze fixations that the system had been ac-cumulating since the previously detected sentence189[table_vase]speech streamgaze stream[fixated entity]ts tegaze fixation[vase_purple] [vase_greek3][vase_greek3] [vase_greek3][vase_greek3]There?s orangevase in anpurplea faceFigure 2: Accompanying gaze fixations and the 1-best recognition of a user?s utterance ?There?s a purplevase and an orange vase.?
(There are two incorrectly recognized words ?in?
and ?face?
in the 1-bestrecognition)boundary.
Figure 2 shows a pair of user speechand accompanying stream of gaze fixations.
Inthe speech stream, each spoken word was times-tamped by the speech recognizer.
In the gazestream, each gaze fixation has a starting timestampts and an ending timestamp te provided by the eyetracker.
Each gaze fixation results in a fixated en-tity (3D object).
When multiple entities are fixatedby one gaze fixation due to the overlapping of en-tities, the one in the forefront is chosen.Given the paired speech and gaze streams, webuild a set of parallel word sequence and gaze fix-ated entity sequence {(w, e)} for the task of wordacquisition.
In section 6, we will evaluate wordacquisition in two settings: 1) word sequence wcontains all of the nouns/adjectives in the speechtranscript, and 2) w contains all of the recognizednouns/adjectives in the 1-best speech recognition.4 Word Acquisition With Eye GazeThe task of word acquisition in our application isto ground words to the visual entities.
Specifi-cally, given the parallel word and entity sequences{(w, e)}, we want to find the best match betweenthe words and the entities.
Following our previ-ous work (Qu and Chai, 2008), we formulate wordacquisition as a translation problem and use trans-lation models for word acquisition.
For each en-tity e, we first estimate the word-entity associationprobability p(w|e) with a translation model, thenchoose the words with the highest probabilities asacquired words for e.Inspired by the psycholinguistic findings thatusers?
eyes move to the mentioned object beforespeaking a word (Meyer et al, 1998; Rayner,1998; Griffin and Bock, 2000), in our previouswork (Qu and Chai, 2008), we have incorpo-rated the gaze-speech temporal information in thetranslation model as follows (referred as Model-2tthrough the rest of this paper):p(w|e) =m?j=1l?i=0pt(aj = i|j, e,w)p(wj |ei)where l and m are the lengths of entity and wordsequences respectively.
In this equation, pt(aj =i|j, e,w) is the temporal alignment probabilityrepresenting the probability thatwj is aligned withei, which is further defined by:pt(aj = i|j, e,w) ={0 d(ei, wj) > 0exp[?
?d(ei,wj)]?i exp[?
?d(ei,wj)]d(ei, wj) ?
0where ?
is a scaling factor, and d(ei, wj) is thetemporal distance between ei and wj .
Based onthe psycholinguistic finding that eye gaze happensbefore a spoken word, wj is not allowed to bealigned with ei when wj happens earlier than ei(i.e., d(ei, wj) > 0).
When wj happens no earlierthan ei (i.e., d(ei, wj) ?
0), the closer they are, themore likely they are aligned.
An EM algorithm isused to estimate p(w|e) and ?
in the model.Our evaluation in (Qu and Chai, 2008) hasshown that Model-2t that incorporates temporalalignment between speech and eye gaze achievessignificantly better word acquisition performancecompared to the model where no temporal align-ment is introduced.
Therefore, this model is usedfor the investigation in this paper.5 Identification of Closely CoupledGaze-Speech PairsSuccessful word acquisition with the translationmodels relies on the tight coupling between thegaze fixations and the speech content.
As men-tioned earlier, not all gaze-speech pairs have thistight coupling.
In a gaze-speech pair, if the speech190does not have any word that relates to any of thegaze fixated entities, this instance only adds noiseto word acquisition.
Therefore, we should identifythe closely coupled gaze-speech pairs and only usethem for word acquisition.In this section, we first describe the feature ex-traction, then evaluate the application of a logis-tic regression classifier to predict whether a gaze-speech pair is a closely coupled gaze-speech in-stance ?
an instance where at least one noun oradjective in the speech stream describes some en-tity fixated by the gaze stream.
For the training ofthe classifier, we manually labeled each instanceas either a coupled instance or not based on thespeech transcript and the gaze fixations.5.1 Feature ExtractionFor a gaze-speech instance, the following sets offeatures are automatically extracted.5.1.1 Speech Features (S)The following features are extracted fromspeech:?
cw ?
count of nouns and adjectives.More nouns and adjectives are expected inthe user?s utterance describing entities.?
cw/ls ?
normalized noun/adjective count.The effect of speech length ls on cw is con-sidered.5.1.2 Gaze Features (G)For each fixated entity ei, let lie be its temporalfixation length.
Note that several gaze fixationsmay have the same fixated entity, lie is the totallength of all the gaze fixations that fixate on entityei.
We extract the following features from gazestream:?
ce ?
count of different gaze fixated entities.Fewer fixated entities are expected when theuser is describing entities while looking atthem.?
ce/ls ?
normalized entity count.The effect of temporal spoken utterancelength ls on ce is considered.?
maxi(lie) ?
maximal fixation length.At least one fixated entity?s fixation is ex-pected to be long enough when the user isdescribing entities while looking at them.?
mean(lie) ?
average fixation length.The average gaze fixation length is expectedto be longer when the user is describing enti-ties while looking at them.?
var(lie) ?
variance of fixation lengths.The variance of the fixation lengths is ex-pected to be smaller when the user is describ-ing entities while looking at them.The number of gaze fixated entities is not onlydetermined by the user?s eye gaze, but also af-fected by the visual scene.
Let cse be the countof all the entities that have been visible during thetime period concurrent with the gaze stream.
Wealso extract the following scene related feature:?
ce/cse ?
scene-normalized fixated entitycount.The effect of the visual scene on ce is consid-ered.5.1.3 User Activity Features (UA)While interacting with the system, the user?s ac-tivity can also be helpful in determining whetherthe user?s eye gaze is tightly linked to the contentof the speech.
The following features are extractedfrom the user?s activities:?
maximal distance of the user?s movements ?the maximal change of user position (3D co-ordinates) during speech.The user is expected to move within a smallerrange while looking at entities and describingthem.?
variance of the user?s positionsThe user is expected to move less frequentlywhile looking at entities and describing them.5.1.4 Conversation Context Features (CC)While talking to the system (i.e., the ?expert?
),the user?s language and gaze behavior are influ-enced by the state of the conversation.
For eachgaze-speech instance, we use the previous sys-tem response type as a nominal feature to predictwhether this is a closely coupled gaze-speech in-stance.In our treasure hunting domain, there are 8 typesof system responses in 2 categories:System Initiative Responses:?
specific-see ?
the system asks whether theuser sees a certain entity, e.g., ?Do you seeanother couch??.?
nonspecific-see ?
the system asks whether theuser sees anything, e.g., ?Do you see any-thing else?
?, ?Tell me what you see?.191?
previous-see ?
the system asks whether theuser has previously seen something, e.g.,?Have you previously seen a similar object??.?
describe ?
the system asks the user to de-scribe in detail what the user sees, e.g., ?De-scribe it?, ?Tell me more about it?.?
compare ?
the system asks the user to com-pare what the user sees, e.g., ?Compare theseobjects?.?
repair-request ?
the system asks the user tomake clarification, e.g., ?I did not understandthat?, ?Please repeat that?.?
action-request ?
the system asks the user totake action, e.g., ?Go back?, ?Try moving it?.User Initiative Responses:?
misc ?
the system hands the initiative backto the user without specifying further require-ments, e.g., ?I don?t know?, ?Yes?.5.2 Evaluation of Gaze-Speech IdentificationGiven the extracted features and the ?closely cou-pled?
label of each instance in the training set, wetrain a logistic regression classifier (le Cessie andvan Houwelingen, 1992) to predict whether an in-stance is a closely coupled gaze-speech instance.Since the goal of identifying closely coupledgaze-speech instances is to improve word acqui-sition and we are only interested in acquiringnouns and adjectives, only the instances with rec-ognized nouns/adjectives are used for training thelogistic regression classifier.
Among the 2969 in-stances with recognized nouns/adjectives and gazefixations, 2002 (67.4%) instances are labeled as?closely coupled?.
The prediction is evaluated bya 10-fold cross validation.Feature sets Precision RecallNull (baseline) 0.674 1S 0.686 0.995G 0.707 0.958UA 0.704 0.942CC 0.688 0.936G + UA 0.719 0.948G + UA + S 0.741 0.908G + UA + CC 0.731 0.918G + UA + CC + S 0.748 0.899Table 1: Gaze-speech prediction performance forthe instances with 1-best speech recognitionTable 1 shows the prediction precision and re-call when different sets of features are used.
Asseen in the table, as more features are used, theprediction precision goes up and the recall goesdown.
It is important to note that prediction pre-cision is more critical than recall for word acqui-sition when sufficient amount data is available.Noisy instances where the gaze is not coupled withthe speech content will only hurt word acquisi-tion since they will guide the translation modelsto ground words to the wrong entities.
Althoughhigher recall can be helpful, its effect is expectedto be reduced when more data becomes available.The results show that speech features (S) andconversation context features (CC), when usedalone, do not improve prediction precision muchcompared to the baseline of predicting all in-stances as closely coupled (with a precision of67.4%).
When used alone, gaze features (G) anduser activity features (UA) are the two most use-ful feature sets for increasing prediction precision.When they are used together, the prediction pre-cision is further increased.
Adding either speechfeatures or conversation context features to gazeand user activity features (G + UA + S/CC) furtherincreases the prediction precision.
Using all fea-tures (G + UA + CC + S) achieves the highest pre-diction precision, which is significantly better thanthe baseline: z = 5.93, p < 0.001.
Therefore, wechoose to use all feature sets to identify the closelycoupled gaze-speech instances for word acquisi-tion.To compare the effects of the automatic gaze-speech identification on word acquisition fromvarious speech input (1-best speech recognition,speech transcript), we also use the logistic re-gression classifier with all feature sets to iden-tify the closely coupled gaze-speech instances forthe instances with speech transcript.
For the in-stances with speech transcript, there are 2948 in-stances with nouns/adjectives and gaze fixations,2128 (72.2%) of them being labeled as ?closelycoupled?.
The prediction precision is 77.9% andthe recall is 93.8%.
The prediction precision issignificantly better than the baseline of predictingall instances as coupled: z = 4.92, p < 0.001.6 Evaluation of Word AcquisitionEvery conversational system has an initial vocabu-lary where words are associated with domain con-cepts of entities.
In our evaluation, we assume that192the system?s vocabulary has one default word foreach entity that indicates the semantic type of theentity.
For example, the word ?barrel?
is the de-fault word for the entity barrel.
For each entity,we only evaluate those new words that are not inthe system?s vocabulary.The acquired words are evaluated against the?gold standard?
words that were manually com-piled for each entity and its properties based onall users?
speech transcripts.
For the 115 entitiesin our domain, each entity has 1 to 20 ?gold stan-dard?
words.
The average number of ?gold stan-dard?
words for an entity is 6.7.6.1 Evaluation MetricsWe evaluate the n-best acquired words (wordsgrounded to domain concepts of entities) usingprecision, recall, and F-measure.
When a differ-ent n is chosen, we will have different precision,recall, and F-measure.We also evaluate the whole ranked candidateword list on Mean Reciprocal Rank Rate (MRRR)as in (Qu and Chai, 2008):MRRR =?e?Nei=1 1/index(wie)?Nei=1 1/i#ewhere Ne is the number of all ?gold standard?words {wie} for entity e, index(wie) is the indexof word wie in the ranked list of candidate wordsfor entity e.MRRR measures how close the ranks of the?gold standard?
words in the candidate word listsare to the best-case scenario where the top Newords are the ?gold standard?
words for e. Thehigher the MRRR, the better is the acquisition per-formance.6.2 Evaluation ResultsWe evaluate the effect of the closely coupled gaze-speech instances on word acquisition from the 1-best speech recognition and speech transcript.
Thepredicted closely coupled gaze-speech instancesare generated by a 10-fold cross validation withthe logistic regression classifier.Figure 3 shows the precision, recall, and F-measure of the n-best words acquired from 1-bestspeech recognition by Model-2t using all instances(all), predicted coupled instances (predicted), andtrue (manually labeled) coupled instances (true).As shown in the figure, using predicted coupledinstances achieves consistently better performance1 2 3 4 5 6 7 8 9 100.20.250.30.350.40.45n-bestPrecisionallpredictedtrue(a) precision1 2 3 4 5 6 7 8 9 100.050.10.150.20.250.30.35n-bestRecallallpredictedtrue(b) recall1 2 3 4 5 6 7 8 9 100.10.150.20.250.3n-bestF-measureallpredictedtrue(c) F-measureFigure 3: Performance of word acquisition on 1-best speech recognitionthan using all instances.
These results show thatthe identification of coupled gaze-speech predic-tion helps word acquisition.
When the true cou-pled instances are used, the performance is furtherimproved.
This means that reliable identificationof coupled gaze-speech instances can lead to bet-ter word acquisition.Figure 4 shows the precision, recall, and F-measure of the n-best words acquired from speechtranscript by Model-2t using all instances, pre-dicted coupled instances, and true coupled in-stances.
Consistent with the performance basedon the 1-best speech recognition, we can observe1931 2 3 4 5 6 7 8 9 100.250.30.350.40.450.50.55n-bestPrecisionallpredictedtrue(a) precision1 2 3 4 5 6 7 8 9 100.050.10.150.20.250.30.350.40.450.50.55n-bestRecallallpredictedtrue(b) recall1 2 3 4 5 6 7 8 9 100.10.150.20.250.30.350.40.45n-bestF-measureallpredictedtrue(c) F-measureFigure 4: Performance of word acquisition onspeech transcriptthat automatic identification of coupled instancesresults in better word acquisition performance andusing the true coupled instances results in evenbetter performance.Table 2 presents the MRRRs achieved byModel-2t when words are acquired from differ-ent speech input (speech transcript, 1-best recog-nition) with different set of instances (all in-stances, predicted coupled instances, true coupledinstances).
These results also show the consis-tent behavior.
Using predicted coupled instancesachieves significantly better MRRR than using allinstances no matter the words are acquired from 1-best speech recognition (t = 2.59, p < 0.006) orspeech transcript(t = 3.15, p < 0.002).
When thetrue coupled instances are used, the performancesare further improved for both 1-best recognition(t = 2.29, p < 0.013) and speech transcript(t = 5.21, p < 0.001) compared to using pre-dicted coupled instances.Instances All Predicted TrueTranscript 0.462 0.480 0.5261-best reco 0.343 0.369 0.390Table 2: MRRRs based on different data setThe quality of speech recognition is critical toword acquisition performance.
Comparing wordacquisition based on speech transcript and 1-bestspeech recognition, as expected, word acquisitionperformance on speech transcript is much betterthan on recognized speech.
However, the acqui-sition performance based on speech transcript isstill comparably low.
For example, the recall ofacquired words is still below 55% even when the10 best word candidates are acquired for each en-tity.
This is mainly due to the scarcity of words.Many words appear less than three times in thedata, which makes them unlikely to be associatedwith any entity by the translation model.
Whenmore data is available, we expect to see better ac-quisition performance.Note that our current evaluation is based on atwo-stage approach, i.e., first identifying closely-coupled streams based on supervised classifica-tion and then automatically establishing mappingsbetween words and entities in an unsupervisedmanner.
There could be other approaches to ad-dress the word acquisition problem (e.g., super-vised learning to directly identify whether a wordis mapped to an object).
Our two-stage approachhas the advantage of requiring minimum super-vision since the models learned from the firststage is application-independent and is potentiallyportable to different domains.7 ConclusionsUnlike in the typical settings for psycholinguisticstudies, human eye gaze can serve different func-tions during human machine conversation.
Somegaze and speech streams may not be tightly cou-pled and thus can be detrimental to word acqui-sition.
Therefore, this paper describes an ap-proach that incorporates features from the interac-194tion context to identify closely coupled gaze andspeech streams.
Our empirical results indicatethat the word acquisition based on these automati-cally identified gaze-speech streams achieves sig-nificantly better performance than the word acqui-sition based on all gaze-speech streams.
Our fu-ture work will combine gaze-based word acquisi-tion with multiple speech recognition hypotheses(e.g., word lattices) to further improve word acqui-sition and language interpretation performance.AcknowledgmentsThis work was supported by grants IIS-0347548and IIS-0535112 from the National Science Foun-dation.
We thank anonymous reviewers for theirvaluable comments and suggestions.ReferencesG.
Aist, J. Dowding, B.
A. Hockey, M. Rayner,J.
Hieronymus, D. Bohus, B. Boven, N. Blaylock,E.
Campana, S. Early, G. Gorrell, and S. Phan.2003.
Talking through procedures: An intelligentspace station procedure assistant.
In Proceedings ofthe 10th Conference of the European Chapter of theAssociation for Computational Linguistics (EACL).S.
Bangalore and M. Johnston.
2004.
Robust multi-modal understanding.
In Proceedings of the Inter-national Conference on Acoustics, Speech, and Sig-nal Processing (ICASSP).K.
Barnard, P. Duygulu, N. de Freitas, D. Forsyth,D.
Blei, and M. Jordan.
2003.
Matching words andpictures.
Journal of Machine Learning Research,3:1107?1135.D.
Byron, T. Mampilly, V. Sharma, and T. Xu.
2005.Utilizing visual attention for cross-modal corefer-ence interpretation.
In Proceedings of the FifthInternational and Interdisciplinary Conference onModeling and Using Context (CONTEXT-05), pages83?96.K.
Eberhard, M. Spivey-Knowiton, J. Sedivy, andM.
Tanenhaus.
1995.
Eye movements as a win-dow into real-time spoken language comprehensionin natural contexts.
Journal of Psycholinguistic Re-search, 24:409?436.Z.
Griffin and K. Bock.
2000.
What the eyes say aboutspeaking.
Psychological Science, 11:274?279.M.
Just and P. Carpenter.
1976.
Eye fixations and cog-nitive processes.
Cognitive Psychology, 8:441?480.D.
Kahneman.
1973.
Attention and Effort.
Prentice-Hall, Inc., Englewood Cliffs.S.
le Cessie and J. van Houwelingen.
1992.
Ridgeestimators in logistic regression.
Applied Statistics,41(1):191?201.O.
Lemon, A. Gruenstein, and S. Peters.
2002.
Col-laborative activities and multitasking in dialoguesystems.
Traitement Automatique des Langues,43(2):131?154.Y.
Liu, J. Chai, and R. Jin.
2007.
Automated vocab-ulary acquisition and interpretation in multimodalconversational systems.
In Proceedings of the 45thAnnual Meeting of the Association of ComputationalLinguistics (ACL).A.
Meyer, A. Sleiderink, and W. Levelt.
1998.
View-ing and naming objects: eye movements duringnoun phrase production.
Cognition, 66(22):25?33.Y.
Nakano, G. Reinstein, T. Stocky, and J. Cassell.2003.
Towards a model of face-to-face grounding.In Proceedings of the Annual Meeting of the Associ-ation for Computational Linguistics (ACL).Z.
Prasov and J. Chai.
2008.
What?s in a gaze?
the roleof eye-gaze in reference resolution in multimodalconversational interfaces.
In Proceedings of ACM12th International Conference on Intelligent Userinterfaces (IUI).S.
Qu and J. Chai.
2006.
Salience modeling basedon non-verbal modalities for spoken language un-derstanding.
In Proceedings of the InternationalConference on Multimodal Interfaces (ICMI), pages193?200.S.
Qu and J. Chai.
2008.
Incorporating temporal andsemantic information with eye gaze for automaticword acquisition in multimodal conversational sys-tems.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 244?253.K.
Rayner.
1998.
Eye movements in reading and in-formation processing - 20 years of research.
Psy-chological Bulletin, 124(3):372?422.L.
Razzaq and N. Heffernan.
2004.
Tutorial dialog inan equation solving intelligent tutoring system.
InProceedings of the Workshop on Dialog-based In-telligent Tutoring Systems: State of the art and newresearch directions.D.
Roy and A. Pentland.
2002.
Learning words fromsights and sounds, a computational model.
Cogni-tive Science, 26(1):113?146.M.
Tanenhaus, M. Spivey-Knowiton, K. Eberhard, andJ.
Sedivy.
1995.
Integration of visual and linguis-tic information in spoken language comprehension.Science, 268:1632?1634.C.
Yu and D. Ballard.
2004.
A multimodal learninginterface for grounding spoken language in sensoryperceptions.
ACM Transactions on Applied Percep-tions, 1(1):57?80.195
