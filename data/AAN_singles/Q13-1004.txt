Transactions of the Association for Computational Linguistics, 1 (2013) 37?48.
Action Editor: Ryan McDonald.Submitted 11/2012; Revised 2/2013; Published 3/2013.
c?2013 Association for Computational Linguistics.Branch and Bound Algorithm for Dependency Parsingwith Non-local FeaturesXian Qian and Yang LiuComputer Science DepartmentThe University of Texas at Dallas{qx,yangl}@hlt.utdallas.eduAbstractGraph based dependency parsing is inefficientwhen handling non-local features due to highcomputational complexity of inference.
Inthis paper, we proposed an exact and effi-cient decoding algorithm based on the Branchand Bound (B&B) framework where non-local features are bounded by a linear combi-nation of local features.
Dynamic program-ming is used to search the upper bound.
Ex-periments are conducted on English PTB andChinese CTB datasets.
We achieved competi-tive Unlabeled Attachment Score (UAS) whenno additional resources are available: 93.17%for English and 87.25% for Chinese.
Parsingspeed is 177 words per second for English and97 words per second for Chinese.
Our algo-rithm is general and can be adapted to non-projective dependency parsing or other graph-ical models.1 IntroductionFor graph based projective dependency parsing, dy-namic programming (DP) is popular for decodingdue to its efficiency when handling local features.It performs cubic time parsing for arc-factored mod-els (Eisner, 1996; McDonald et al 2005a) and bi-quadratic time for higher order models with richersibling and grandchild features (Carreras, 2007; Kooand Collins, 2010).
However, for models with gen-eral non-local features, DP is inefficient.There have been numerous studies on global in-ference algorithms for general higher order parsing.One popular approach is reranking (Collins, 2000;Charniak and Johnson, 2005; Hall, 2007).
It typi-cally has two steps: the low level classifier gener-ates the top k hypotheses using local features, thenthe high level classifier reranks these candidates us-ing global features.
Since the reranking quality isbounded by the oracle performance of candidates,some work has combined candidate generation andreranking steps using cube pruning (Huang, 2008;Zhang and McDonald, 2012) to achieve higher or-acle performance.
They parse a sentence in bottomup order and keep the top k derivations for each s-pan using k best parsing (Huang and Chiang, 2005).After merging the two spans, non-local features areused to rerank top k combinations.
This approachis very efficient and flexible to handle various non-local features.
The disadvantage is that it tends tocompute non-local features as early as possible sothat the decoder can utilize that information at inter-nal spans, hence it may miss long historical featuressuch as long dependency chains.Smith and Eisner modeled dependency parsingusing Markov Random Fields (MRFs) with glob-al constraints and applied loopy belief propaga-tion (LBP) for approximate learning and inference(Smith and Eisner, 2008).
Similar work was donefor Combinatorial Categorial Grammar (CCG) pars-ing (Auli and Lopez, 2011).
They used posteriormarginal beliefs for inference to satisfy the tree con-straint: for each factor, only legal messages (satisfy-ing global constraints) are considered in the partitionfunction.A similar line of research investigated the useof integer linear programming (ILP) based parsing(Riedel and Clarke, 2006; Martins et al 2009).
This37method is very expressive.
It can handle arbitrarynon-local features determined or bounded by linearinequalities of local features.
For local models, LP isless efficient than DP.
The reason is that, DP workson a small number of dimensions in each recursion,while for LP, the popular revised simplex methodneeds to solve a m dimensional linear system ineach iteration (Nocedal and Wright, 2006), wherem is the number of constraints, which is quadraticin sentence length for projective dependency pars-ing (Martins et al 2009).Dual Decomposition (DD) (Rush et al 2010;Koo et al 2010) is a special case of Lagrangian re-laxation.
It relies on standard decoding algorithmsas oracle solvers for sub-problems, together with asimple method for forcing agreement between thedifferent oracles.
This method does not need to con-sider the tree constraint explicitly, as it resorts to dy-namic programming which guarantees its satisfac-tion.
It works well if the sub-problems can be welldefined, especially for joint learning tasks.
Howev-er, for the task of dependency parsing, using variousnon-local features may result in many overlappedsub-problems, hence it may take a long time to reacha consensus (Martins et al 2011).In this paper, we propose a novel Branch andBound (B&B) algorithm for efficient parsing withvarious non-local features.
B&B (Land and Doig,1960) is generally used for combinatorial optimiza-tion problems such as ILP.
The difference betweenour method and ILP is that the sub-problem in ILPis a relaxed LP, which requires a numerical solution,while ours bounds the non-local features by a lin-ear combination of local features and uses DP fordecoding as well as calculating the upper bound ofthe objective function.
An exact solution is achievedif the bound is tight.
Though in the worst case,time complexity is exponential in sentence length,it is practically efficient especially when adopting apruning strategy.Experiments are conducted on English PennTreeBank and Chinese Tree Bank 5 (CTB5) with stan-dard train/develop/test split.
We achieved 93.17%Unlabeled Attachment Score (UAS) for English at aspeed of 177 words per second and 87.25% for Chi-nese at a speed of 97 words per second.2 Graph Based Parsing2.1 Problem DefinitionGiven a sentence x = x1, x2, .
.
.
, xn where xi isthe ith word of the sentence, dependency parsing as-signs exactly one head word to each word, so thatdependencies from head words to modifiers form atree.
The root of the tree is a special symbol de-noted by x0 which has exactly one modifier.
In thispaper, we focus on unlabeled projective dependencyparsing but our algorithm can be adapted for labeledor non-projective dependency parsing (McDonald etal., 2005b).The inference problem is to search the optimalparse tree y?y?
= argmaxy?Y(x)?
(x, y)where Y(x) is the set of all candidate parse trees ofsentence x.
?
(x, y) is a given score function whichis usually decomposed into small parts?
(x, y) =?c?y?c(x) (1)where c is a subset of edges, and is called a factor.For example, in the all grandchild model (Koo andCollins, 2010), the score function can be representedas?
(x, y) =?ehm?y?ehm(x) +?egh,ehm?y?egh,ehm(x)where the first term is the sum of scores of all edgesxh ?
xm, and the second term is the sum of thescores of all edge chains xg ?
xh ?
xm.In discriminative models, the score of a parse treey is the weighted sum of the fired feature functions,which can be represented by the sum of the factors?
(x, y) = wT f(x, y) =?c?ywT f(x, c) =?c?y?c(x)where f(x, c) is the feature vector that depends onc.
For example, we could define a feature for grand-child c = {egh, ehm}f(x, c) =????
?1 if xg = would ?
xh = be?xm = happy ?
c is selected0 otherwise382.2 Dynamic Programming for Local ModelsIn first order models, all factors c in Eq(1) contain asingle edge.
The optimal parse tree can be derivedby DP with running time O(n3) (Eisner, 1996).
Thealgorithm has two types of structures: complete s-pan, which consists of a headword and its descen-dants on one side, and incomplete span, which con-sists of a dependency and the region between thehead and modifier.
It starts at single word spans, andmerges the spans in bottom up order.For second order models, the score function?
(x, y) adds the scores of siblings (adjacent edgeswith a common head) and grandchildren?
(x, y) =?ehm?y?ehm(x)+?egh,ehm?y?ehm,egh(x)+?ehm,ehs?y?ehm,ehs(x)There are two versions of second order models,used respectively by Carreras (2007) and Koo et al(2010).
The difference is that Carreras?
only con-siders the outermost grandchildren, while Koo andCollin?s allows all grandchild features.
Both modelspermit O(n4) running time.Third-order models score edge triples such asthree adjacent sibling modifiers, or grand-siblingsthat score a word, its modifier and its adjacent grand-children, and the inference complexity is O(n4)(Koo and Collins, 2010).In this paper, for all the factors/features that canbe handled by DP, we call them the local fac-tors/features.3 The Proposed Method3.1 Basic IdeaFor general high order models with non-local fea-tures, we propose to use Branch and Bound (B&B)algorithm to search the optimal parse tree.
A B&Balgorithm has two steps: branching and bounding.The branching step recursively splits the search s-pace Y(x) into two disjoint subspaces Y(x) =Y1?Y2 by fixing assignment of one edge.
For eachsubspace Yi, the bounding step calculates the upperbound of the optimal parse tree score in the sub-space: UBYi ?
maxy?Yi ?
(x, y).
If this bound isno more than any obtained parse tree score UBYi ??
(x, y?
), then all parse trees in subspace Yi are nomore optimal than y?, and Yi could be pruned safely.The efficiency of B&B depends on the branchingstrategy and upper bound computation.
For exam-ple, Sun et al(2012) used B&B for MRFs, wherethey proposed two branching strategies and a noveldata structure for efficient upper bound computation.Klenner and Ailloud (2009) proposed a variation ofBalas algorithm (Balas, 1965) for coreference reso-lution, where candidate branching variables are sort-ed by their weights.Our bounding strategy is to find an upper boundfor the score of each non-local factor c containingmultiple edges.
The bound is the sum of new scoresof edges in the factor plus a constant?c(x) ?
?e?c?e(x) + ?cBased on the new scores {?e(x)} and constants{?c}, we define the new score of parse tree y?
(x, y) =?c?y(?e?c?e(x) + ?c)Then we have?
(x, y) ?
?
(x, y), ?y ?
Y(x)The advantage of such a bound is that, it is thesum of new edge scores.
Hence, its optimum treemaxy?Y(x) ?
(x, y) can be found by DP, which isthe upper bound of maxy?Y(x) ?
(x, y), as for anyy ?
Y(x), ?
(x, y) ?
?
(x, y).3.2 The Upper Bound FunctionIn this section, we derive the upper bound function?
(x, y) described above.
To simplify notation, wedrop x throughout the rest of the paper.
Let zc bea binary variable indicating whether factor c is se-lected in the parse tree.
We reformulate the scorefunction in Eq(1) as?
(y) ?
?
(z) =?c?czc (2)39Correspondingly, the tree constraint is replaced byz ?
Z .
Then the parsing task isz?
= argmaxz?Z?czc (3)Notice that, for any zc, we havezc = mine?c zewhich means that factor c appears in parse tree if andonly if all its edges {e|e ?
c} are selected in the tree.Here ze is short for z{e} for simplicity.Our bounding method is based on the followingfact: for a set {a1, a2, .
.
.
ar} (aj denotes the jth el-ement) , its minimummin{aj} = minp??
?jpjaj (4)where ?
is probability simplex?
= {p|pj ?
0,?jpj = 1}We discuss the bound for ?czc in two cases: ?c ?0 and ?c < 0.If ?c ?
0, we have?czc = ?cmine?c ze= ?c minpc??
?e?cpecze= minpc??
?e?c?cpeczeThe second equation comes from Eq(4).
For sim-plicity, letgc(pc, z) =?e?c?cpeczewith domain domgc = {pc ?
?
; ze ?
{0, 1}, ?e ?c}.
Then we have?czc = minpc gc(pc, z) (5)If ?c < 0, we have two upper bounds.
One iscommonly used in ILP when all the variables are bi-narya?
= minj{aj}rj=1?a?
?
aja?
?
?jaj ?
(r ?
1)According to the last inequality, we have the upperbound for negative scored factors?czc ?
?c(?e?cze ?
(rc ?
1))(6)where rc is the number of edges in c. For simplicity,we use the notation?c(z) = ?c(?e?cze ?
(rc ?
1))The other upper bound when ?c < 0 is simple?czc ?
0 (7)Notice that, for any parse tree, one of the upperbounds must be tight.
Eq(6) is tight if c appearsin the parse tree: zc = 1, otherwise Eq(7) is tight.Therefore?czc = min {?c(z), 0}Lethc(pc, z) = p1c?c(z) + p2c ?
0with domhc = {pc ?
?
; ze ?
{0, 1}, ?e ?
c}.According to Eq(4), we have?czc = minpc hc(pc, z) (8)Let?
(p, z) =?c,?c?0gc(pc, z) +?c,?c<0hc(pc, z)Minimize ?
with respect to p, we haveminp?
(p, z)= minp??
?c,?c?0gc(pc, z) +?c,?c<0hc(pc, z)?
?=?c,?c?0minpcgc(pc, z) +?c,?c<0minpchc(pc, z)=?c,?c?0?czc +?c,?c<0?czc= ?
(z)The second equation holds since, for any two fac-tors, c and c?, gc (or hc) and gc?
(or hc?)
are separable.The third equation comes from Eq(5) and Eq(8).Based on this, we have the following proposition:40Proposition 1.
For any p, pc ?
?, and z ?
Z ,?
(p, z) ?
?
(z).Therefore, ?
(p, z) is an upper bound function of?(z).
Furthermore, fixing p, ?
(p, z) is a linear func-tion of ze , see Eq(5) and Eq(8), variables zc for largefactors are eliminated.
Hence z?
= argmaxz?
(p, z)can be solved efficiently by DP.Because?
(p, z?)
?
?
(p, z?)
?
?(z?)
?
?(z?
)after obtaining z?
, we get the upper bound and lowerbound of ?(z?
): ?
(p, z?)
and ?(z?
).The upper bound is expected to be as tight as pos-sible.
Using min-max inequality, we getmaxz?Z?
(z) = maxz?Zminp?
(p, z)?
minpmaxz?Z?
(p, z)which provides the tightest upper bound of ?(z?
).Since ?
is not differentiable w.r.t p, projectedsub-gradient (Calamai and More?, 1987; Rush et al2010) is used to search the saddle point.
Morespecifically, in each iteration, we first fix p andsearch z using DP, then we fix z and update p bypnew = P?
(p+ ??
?p ?
)where ?
> 0 is the step size in line search, functionP?
(q) denotes the projection of q onto the proba-bility simplex ?.
In this paper, we use Euclideanprojection, that isP?
(q) = minp??
?p?
q?2which can be solved efficiently by sorting (Duchi etal., 2008).3.3 Branch and Bound Based ParsingAs discussed in Section 3.1, the B&B recursive pro-cedure yields a binary tree structure called Branchand Bound tree.
Each node of the B&B tree hassome fixed ze, specifying some must-select edgesand must-remove edges.
The root of the B&B treehas no constraints, so it can produce all possibleparse trees including z?.
Each node has two chil-dren.
One adds a constraint ze = 1 for a free edgez =e1 0 10 1 0 1z =e2??=9=4?<LB?
?=8=5 ??=7=4?
?=7=4 ?
?=7=5 ?
?=4=3 ?
?=6=2minp maxz?Zze1=0ze2=1?
(p, z)6Figure 1: A part of B&B tree.
?, ?
are short for?(z?)
and ?
(p?, z?)
respectively.
For each node,some edges of the parse tree are fixed.
All parsetrees that satisfy the fixed edges compose the subsetof S ?
Z .
A min-max problem is solved to get theupper bound and lower bound of the optimal parsetree over S. Once the upper bound ?
is less thanLB, the node is removed safely.e and the other fixes ze = 0.
We can explore thesearch space {z|ze ?
{0, 1}} by traversing the B&Btree in breadth first order.Let S ?
Z be subspace of parse trees satisfyingthe constraint, i.e., in the branch of the node.
Foreach node in B&B tree, we solvep?, z?
= argminpmaxz?S?
(p, z)to get the upper bound and lower bound of the bestparse tree in S. A global lower bound LB is main-tained which is the maximum of all obtained lowerbounds.
If the upper bound of the current node islower than the global lower bound, the node can bepruned from the B&B tree safely.
An example isshown in Figure 1.When the upper bound is not tight: ?
> LB, weneed to choose a good branching variable to gener-ate the child nodes.
Let G(z?)
= ?
(p?, z?)
?
?(z?
)denote the gap between the upper bound and lowerbound.
This gap is actually the accumulated gaps ofall factors c. Let Gc be the gap of cGc ={gc(p?c, z?)?
?cz?c if ?c ?
0hc(p?c, z?)?
?cz?c if ?c < 041We choose the branching variable heuristically:for each edge e, we define its gap as the sum of thegaps of factors that contain itGe =?c,e?cGcThe edge with the maximum gap is selected as thebranching variable.Suppose there are N nodes on a level of B&Btree, and correspondingly, we get N branching vari-ables, among which, we choose the one with thehighest lower bound as it likely reaches the optimalvalue faster.3.4 Lower Bound InitializationA large lower bound is critical for efficient pruning.In this section, we discuss an alternative way to ini-tialize the lower bound LB.
We apply the similartrick to get the lower bound function of ?
(z).Similar to Eq(8), for ?c ?
0, we have?czc = max{?c(?e?cze ?
(rc ?
1)), 0}= max{?c(z), 0}Using the fact thatmax{aj} = maxp??
?jpjajwe have?czc = maxpc?
?p1c?c(z) + p2c ?
0= maxpchc(pc, z)For ?c < 0, we have?czc = maxe?c {?cze}= maxpc??
?e?cpec?cze= maxpcgc(pc, z)Put the two cases together, we get the lower boundfunction?
(p, z) =?c,?c?0hc(pc, z) +?c,?c<0gc(pc, z)Algorithm 1 Branch and Bound based parsingRequire: {?c}Ensure: Optimal parse tree z?Solve p?, z?
= argmaxp,z?
(p, z)Initialize S = {Z}, LB = ?
(p?, z?
)while S ?= ?
doSet S ?
= ?
{nodes that survive from pruning}foreach S ?
SSolve minp maxz ?
(p, z) to get LBS , UBSLB = max{LB,LBS?S}, update z?foreach S ?
S, add S to S ?, if UBS > LBSelect a branching variable ze.Clear S = ?foreach S ?
S ?Add S1 = {z|z ?
S, ze = 1} to SAdd S2 = {z|z ?
S, ze = 0} to S.end whileFor any p, pc ?
?, z ?
Z?
(p, z) ?
?(z)?
(p, z) is not concave, however, we could alterna-tively optimize z and p to get a good approximation,which provides a lower bound for ?(z?
).3.5 SummaryWe summarize our B&B algorithm in Algorithm 1.It is worth pointing out that so far in the abovedescription, we have used the assumption that thebackbone DP uses first order models, however, thebackbone DP can be the second or third order ver-sion.
The difference is that, for higher order DP,higher order factors such as adjacent siblings, grand-children are directly handled as local factors.In the worst case, all the edges are selected forbranching, and the complexity grows exponentiallyin sentence length.
However, in practice, it is quiteefficient, as we will show in the next section.4 Experiments4.1 Experimental SettingsThe datasets we used are the English Penn TreeBank (PTB) and Chinese Tree Bank 5.0 (CTB5).
Weuse the standard train/develop/test split as describedin Table 1.We extracted dependencies using Joakim Nivre?sPenn2Malt tool with standard head rules: Yamadaand Matsumoto?s (Yamada and Matsumoto, 2003)42Train Develop TestPTB sec.
2-21 sec.
22 sec.
23CTB5 sec.
001-815 sec.
886-931 sec.
816-8851001-1136 1148-1151 1137-1147Table 1: Data split in our experimentfor English, and Zhang and Clark?s (Zhang andClark, 2008) for Chinese.
Unlabeled attachment s-core (UAS) is used to evaluate parsing quality1.
TheB&B parser is implemented with C++.
All the ex-periments are conducted on the platform Intel Corei5-2500 CPU 3.30GHz.4.2 Baseline: DP Based Second Order ParserWe use the dynamic programming based second or-der parser (Carreras, 2007) as the baseline.
Aver-aged structured perceptron (Collins, 2002) is usedfor parameter estimation.
We determine the numberof iterations on the validation set, which is 6 for bothcorpora.For English, we train the POS tagger using linearchain perceptron on training set, and predict POStags for the development and test data.
The parser istrained using the automatic POS tags generated by10 fold cross validation.
For Chinese, we use thegold standard POS tags.We use five types of features: unigram features,bigram features, in-between features, adjacent sib-ling features and outermost grand-child features.The first three types of features are firstly introducedby McDonald et al(2005a) and the last two type-s of features are used by Carreras (2007).
All thefeatures are the concatenation of surrounding words,lower cased words (English only), word length (Chi-nese only), prefixes and suffixes of words (Chineseonly), POS tags, coarse POS tags which are derivedfrom POS tags using a simple mapping table, dis-tance between head and modifier, direction of edges.For English, we used 674 feature templates to gener-ate large amounts of features, and finally got 86.7Mnon-zero weighted features after training.
The base-line parser got 92.81% UAS on the testing set.
ForChinese, we used 858 feature templates, and finallygot 71.5M non-zero weighted features after train-1For English, we follow Koo and Collins (2010) and ignoreany word whose gold-standard POS tag is one of { ?
?
: , .}.
ForChinese, we ignore any word whose POS tag is PU.ing.
The baseline parser got 86.89% UAS on thetesting set.4.3 B&B Based Parser with Non-local FeaturesWe use the baseline parser as the backbone of ourB&B parser.
We tried different types of non-localfeatures as listed below:?
All grand-child features.
Notice that this fea-ture can be handled by Koo?s second ordermodel (Koo and Collins, 2010) directly.?
All great grand-child features.?
All sibling features: all the pairs of edges withcommon head.
An example is shown in Fig-ure 2.?
All tri-sibling features: all the 3-tuples of edgeswith common head.?
Comb features: for any word with more than 3consecutive modifiers, the set of all the edgesfrom the word to the modifiers form a comb.2?
Hand crafted features: We perform cross val-idation on the training data using the baselineparser, and designed features that may correc-t the most common errors.
We designed 13hand-craft features for English in total.
One ex-ample is shown in Figure 3.
For Chinese, wedid not add any hand-craft features, as the er-rors in the cross validation result vary a lot, andwe did not find general patterns to fix them.4.4 Implementation DetailsTo speed up the solution of the min-max subprob-lem, for each node in the B&B tree, we initialize pwith the optimal solution of its parent node, sincethe child node fixes only one additional edge, its op-timal point is likely to be closed to its parent?s.
Forthe root node of B&B tree, we initialize pec = 1rc forfactors with non-negative weights and p1c = 0 for2In fact, our algorithm can deal with non-consecutive mod-ifiers; however, in such cases, factor detection (detect regularexpressions like x1.
?
x2.
?
.
.
. )
requires the longest com-mon subsequence algorithm (LCS), which is time-consumingif many comb features are generated.
Similar problems arisefor sub-tree features, which may contain many non-consecutivewords.43c 0 c 1 c 2 c 3hc 0 c 1h c 0 c 2h c 0 c 3hc 1 c 2h c 2 c 3h c 1 c 3hsecondorder higher orderFigure 2: An example of all sibling features.
Top:a sub-tree; Bottom: extracted sibling features.
Ex-isting higher order DP systems can not handle thesiblings on both sides of head.regulation occurs through inaction , rather than through ...Figure 3: An example of hand-craft feature: for theword sequence A .
.
.
rather than A, where A is apreposition, the first A is the head of than, than isthe head of rather and the second A.negative weighted factors.
Step size ?
is initializedwith maxc,?c ?=0{ 1|?c|}, as the vector p is bounded ina unit box.
?
is updated using the same strategy asRush et al(2010).
Two stopping criteria are used.One is 0 ?
?old ?
?new ?
?, where ?
> 0 is a givenprecision3.
The other checks if the bound is tight:UB = LB.
Because all features are boolean (notethat they can be integer), their weights are integerduring each perceptron update, hence the scores ofparse trees are discrete.
The minimal gap betweendifferent scores is 1N?T after averaging, where N isthe number of training samples, and T is the itera-tion number for perceptron training.
Therefore theupper bound can be tightened as UB = ?NT?
?NT .During testing, we use the pre-pruning method asused in Martins et al(2009) for both datasets to bal-ance parsing quality and speed.
This method uses asimple classifier to select the top k candidate head-s for each word and exclude the other heads fromsearch space.
In our experiment, we set k = 10.3we use ?
= 10?8 in our implementationSystem PTB CTBOur baseline 92.81 86.89B&B +all grand-child 92.97 87.02+all great grand-child 92.78 86.77+all sibling 93.00 87.05+all tri-sibling 92.79 86.81+comb 92.86 86.91+hand craft 92.89 N/A+all grand-child + all sibling + com-b + hand craft93.17 87.253rd order re-impl.
93.03 87.07TurboParser (reported) 92.62 N/ATurboParser (our run) 92.82 86.05Koo and Collins (2010) 93.04 N/AZhang and McDonald (2012) 93.06 86.87Zhang and Nivre (2011) 92.90 86.00System integrationBohnet and Kuhn (2012) 93.39 87.5Systems using additional resourcesSuzuki et al(2009) 93.79 N/AKoo et al(2008) 93.5 N/AChen et al(2012) 92.76 N/ATable 2: Comparison between our system and the-state-of-art systems.4.5 Main ResultExperimental results are listed in Table 2.
For com-parison, we also include results of representativestate-of-the-art systems.
For the third order pars-er, we re-implemented Model 1 (Koo and Collins,2010), and removed the longest sentence in the CTBdataset, which contains 240 words, due to theO(n4)space complexity 4.
For ILP based parsing, we usedTurboParser5, a speed-optimized parser toolkit.
Wetrained full models (which use all grandchild fea-tures, all sibling features and head bigram features(Martins et al 2011)) for both datasets using its de-fault settings.
We also list the performance in itsdocumentation on English corpus.The observation is that, the all-sibling features aremost helpful for our parser, as some good siblingfeatures can not be encoded in DP based parser.
Forexample, a matched pair of parentheses are alwayssiblings, but their head may lie between them.
An-4In fact, Koo?s algorithm requires only O(n3) space.
Ourimplementation is O(n4) because we store the feature vectorsfor fast training.5http://www.ark.cs.cmu.edu/TurboParser/44other observation is that all great grandchild featuresand all tri-sibling features slightly hurt the perfor-mance and we excluded them from the final system.When no additional resource is available, ourparser achieved competitive performance: 93.17%Unlabeled Attachment Score (UAS) for English ata speed of 177 words per second and 87.25% forChinese at a speed of 97 words per second.
High-er UAS is reported by joint tagging and parsing(Bohnet and Nivre, 2012) or system integration(Bohnet and Kuhn, 2012) which benefits from bothtransition based parsing and graph based parsing.Previous work shows that combination of the twoparsing techniques can learn to overcome the short-comings of each non-integrated system (Nivre andMcDonald, 2008; Zhang and Clark, 2008).
Sys-tem combination will be an interesting topic for ourfuture research.
The highest reported performanceon English corpus is 93.79%, obtained by semi-supervised learning with a large amount of unla-beled data (Suzuki et al 2009).4.6 Tradeoff Between Accuracy and SpeedIn this section, we study the trade off between ac-curacy and speed using different pre-pruning setups.In Table 3, we show the parsing accuracy and in-ference time in testing stage with different numbersof candidate heads k in pruning step.
We can seethat, on English dataset, when k ?
10, our pars-er could gain 2 ?
3 times speedup without losingmuch parsing accuracy.
There is a further increaseof the speed with smaller k, at the cost of some ac-curacy.
Compared with TurboParser, our parser isless efficient but more accurate.
Zhang and McDon-ald (2012) is a state-of-the-art system which adoptscube pruning for efficient parsing.
Notice that, theydid not use pruning which seems to increase parsingspeed with little hit in accuracy.
Moreover, they didlabeled parsing, which also makes their speed notdirectly comparable.For each node of B&B tree, our parsing algorithmuses projected sub-gradient method to find the sad-dle point, which requires a number of calls to a DP,hence the efficiency of Algorithm 1 is mainly deter-mined by the number of DP calls.
Figure 4 and Fig-ure 5 show the averaged parsing time and number ofcalls to DP relative to the sentence length with differ-ent pruning settings.
Parsing time grows smoothlyPTB CTBSystem UAS w/s UAS w/sOurs (no prune) 93.18 52 87.28 73Ours (k = 20) 93.17 105 87.28 76Ours (k = 10) 93.17 177 87.25 97Ours (k = 5) 93.10 264 86.94 108Ours (k = 3) 92.68 493 85.76 128TurboParser(full) 92.82 402 86.05 192TurboParser(standard) 92.68 638 85.80 283TurboParser(basic) 90.97 4670 82.28 2736Zhang and McDon-ald (2012)?93.06 220 86.87 N/ATable 3: Trade off between parsing accuracy (UAS)and speed (words per second) with different pre-pruning settings.
k denotes the number of candi-date heads of each word preserved for B&B parsing.
?Their speed is not directly comparable as they per-forms labeled parsing without pruning.when sentence length ?
40.
There is some fluctua-tion for the long sentences.
This is because there arevery few sentences for a specific long length (usual-ly 1 or 2 sentences), and the statistics are not stableor meaningful for the small samples.Without pruning, there are in total 132, 161 callsto parse 2, 416 English sentences, that is, each sen-tence requires 54.7 calls on average.
For Chinese,there are 84, 645 calls for 1, 910 sentences, i.e., 44.3calls for each sentence on average.5 Discussion5.1 Polynomial Non-local FactorsOur bounding strategy can handle a family of non-local factors that can be expressed as a polynomialfunction of local factors.
To see this, supposezc =?i?i?e?EizeFor each i, we introduce new variable zEi =mine?Ei ze.
Because ze is binary, zEi =?e?Ei ze.In this way, we replace zc by several zEi that can behandled by our bounding strategy.We give two examples of these polynomial non-local factors.
First is the OR of local factors: zc =max{ze, z?e}, which can be expressed by zc = ze +z?e?zez?e.
The second is the factor of valency feature450 10 20 30 40 50 600510parsing time (sec.
)sentence lengthk=3k=5k=10k=20no prune(a) PTB corpus0 20 40 60 80 100 120 140020406080parsing time (sec.
)sentence lengthk=3k=5k=10k=20no prune(b) CTB corpusFigure 4 Averaged parsing time (seconds) relative to sentence length with different pruning settings, kdenotes the number of candidate heads of each word in pruning step.0 10 20 30 40 50 600100200Callsto DPsentence lengthk=3k=5k=10k=20no prune(a) PTB corpus0 20 40 60 80 100 120 14005001000Callsto DPsentence lengthk=3k=5k=10k=20no prune(b) CTB corpusFigure 5 Averaged number of Calls to DP relative to sentence length with different pruning settings, kdenotes the number of candidate heads of each word in pruning step.
(Martins et al 2009).
Let binary variable vik indi-cate whether word i has k modifiers.
Given {ze} forthe edges with head i, then {vik|k = 1, .
.
.
, n ?
1}can be solved by?kkjvik =(?eze)j0 ?
j ?
n?
1The left side of the equation is the linear function ofvik.
The right side of the equation is a polynomialfunction of ze.
Hence, vik could be expressed as apolynomial function of ze.5.2 k Best ParsingThough our B&B algorithm is able to capture a va-riety of non-local features, it is still difficult to han-dle many kinds of features, such as the depth of theparse tree.
Hence, a reranking approach may be use-ful in order to incorporate such information, wherek parse trees can be generated first and then a secondpass model is used to rerank these candidates basedon more global or non-local features.
In addition,k-best parsing may be needed in many applicationsto use parse information and especially utilize infor-mation from multiple candidates to optimize task-specific performance.
We have not conducted anyexperiment for k best parsing, hence we only dis-cuss the algorithm.According to proposition 1, we haveProposition 2.
Given p and subset S ?
Z , let zkdenote the kth best solution of maxz?S ?
(p, z).
If aparse tree z?
?
S satisfies ?(z?)
?
?
(p, zk), then z?is one of the k best parse trees in subset S.Proof.
Since zk is the kth best solution of ?
(p, z),for zj , j > k, we have ?
(p, zk) ?
?
(p, zj) ??(zj).
Since the size of the set {zj |j > k} is|S| ?
k, hence there are at least |S| ?
k parse treeswhose scores ?
(zj) are less than ?
(p, zk).
Because?(z?)
?
?
(p, zk), hence z?
is at least the kth bestparse tree in subset S.Therefore, we can search the k best parse treesin this way: for each sub-problem, we use DP toderive the k best parse trees.
For each parse treez, if ?
(z) ?
?
(p, zk), then z is selected into the kbest set.
Algorithm terminates until the kth bound istight.466 ConclusionIn this paper we proposed a new parsing algorithmbased on a Branch and Bound framework.
The mo-tivation is to use dynamic programming to searchfor the bound.
Experimental results on PTB andCTB5 datasets show that our method is competitivein terms of both performance and efficiency.
Ourmethod can be adapted to non-projective dependen-cy parsing, as well as the k best MST algorithm(Hall, 2007) to find the k best candidates.AcknowledgmentsWe?d like to thank Hao Zhang, Andre Martins andZhenghua Li for their helpful discussions.
We al-so thank Ryan McDonald and three anonymous re-viewers for their valuable comments.
This workis partly supported by DARPA under Contract No.HR0011-12-C-0016 and FA8750-13-2-0041.
Anyopinions expressed in this material are those of theauthors and do not necessarily reflect the views ofDARPA.ReferencesMichael Auli and Adam Lopez.
2011.
A comparison ofloopy belief propagation and dual decomposition forintegrated CCG supertagging and parsing.
In Proc.
ofACL-HLT.Egon Balas.
1965.
An additive algorithm for solvinglinear programs with zero-one variables.
OperationsResearch, 39(4).Bernd Bohnet and Jonas Kuhn.
2012.
The best ofbothworlds ?
a graph-based completion model fortransition-based parsers.
In Proc.
of EACL.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Proc.
ofEMNLP-CoNLL.Paul Calamai and Jorge More?.
1987.
Projected gradien-t methods for linearly constrained problems.
Mathe-matical Programming, 39(1).Xavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In Proc.
of EMNLP-CoNLL.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proc.
of ACL.Wenliang Chen, Min Zhang, and Haizhou Li.
2012.
U-tilizing dependency language models for graph-baseddependency parsing models.
In Proc.
of ACL.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proc.
of ICML.Michael Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In Proc.
of EMNLP.John Duchi, Shai Shalev-Shwartz, Yoram Singer, andTushar Chandra.
2008.
Efficient projections onto thel1-ball for learning in high dimensions.
In Proc.
ofICML.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: an exploration.
In Proc.
ofCOLING.Keith Hall.
2007.
K-best spanning tree parsing.
In Proc.of ACL.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
of IWPT.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proc.
of ACL-HLT.Manfred Klenner and E?tienne Ailloud.
2009.
Opti-mization in coreference resolution is not needed: Anearly-optimal algorithm with intensional constraints.In Proc.
of EACL.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proc.
of ACL.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Proc.of ACL-HLT.Terry Koo, Alexander M. Rush, Michael Collins, TommiJaakkola, and David Sontag.
2010.
Dual decomposi-tion for parsing with non-projective head automata.
InProc.
of EMNLP.Ailsa H. Land and Alison G. Doig.
1960.
An automat-ic method of solving discrete programming problems.Econometrica, 28(3):497?520.Andre Martins, Noah Smith, and Eric Xing.
2009.
Con-cise integer linear programming formulations for de-pendency parsing.
In Proc.
of ACL.Andre Martins, Noah Smith, Mario Figueiredo, and Pe-dro Aguiar.
2011.
Dual decomposition with manyoverlapping components.
In Proc.
of EMNLP.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In Proc.
of ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proc.
of HLT-EMNLP.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proc.
of ACL-HLT.Jorge Nocedal and Stephen J. Wright.
2006.
NumericalOptimization.
Springer, 2nd edition.47Sebastian Riedel and James Clarke.
2006.
Incrementalinteger linear programming for non-projective depen-dency parsing.
In Proc.
of EMNLP.Alexander M Rush, David Sontag, Michael Collins, andTommi Jaakkola.
2010.
On dual decomposition andlinear programming relaxations for natural languageprocessing.
In Proc.
of EMNLP.David Smith and Jason Eisner.
2008.
Dependency pars-ing by belief propagation.
In Proc.
of EMNLP.Min Sun, Murali Telaprolu, Honglak Lee, and SilvioSavarese.
2012.
Efficient and exact MAP-MRF in-ference using branch and bound.
In Proc.
of AISTATS.Jun Suzuki, Hideki Isozaki, Xavier Carreras, andMichaelCollins.
2009.
An empirical study of semi-supervisedstructured conditional models for dependency parsing.In Proc.
of EMNLP.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisticaldependency analysis with support vector machines.
InProc.
of IWPT.Yue Zhang and Stephen Clark.
2008.
A tale of t-wo parsers: Investigating and combining graph-basedand transition-based dependency parsing.
In Proc.
ofEMNLP.Hao Zhang and Ryan McDonald.
2012.
Generalizedhigher-order dependency parsing with cube pruning.In Proc.
of EMNLP.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProc.
of ACL-HLT.48
