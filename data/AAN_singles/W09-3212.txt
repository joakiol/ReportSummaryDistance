Proceedings of the 2009 Workshop on Graph-based Methods for Natural Language Processing, ACL-IJCNLP 2009, pages 84?92,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPQuantitative analysis of treebanks using frequent subtree mining methodsScott MartensCentrum voor Computerlingu?
?stiek, KU LeuvenBlijde-Inkomststraat 13, bus 33153000 Leuven Belgiumscott@ccl.kuleuven.beAbstractThe first task of statistical computationallinguistics, or any other type of data-driven processing of language, is the ex-traction of counts and distributions of phe-nomena.
This is much more difficult forthe type of complex structured data foundin treebanks and in corpora with sophisti-cated annotation than for tokenized texts.Recent developments in data mining, par-ticularly in the extraction of frequent sub-trees from treebanks, offer some solutions.We have applied a modified version of theTreeMiner algorithm to a small treebankand present some promising results.1 IntroductionStatistical corpus linguistics and many natural lan-guage processing applications rely on extractingthe frequencies and distributions of phenomenafrom natural language data sources.
This is rela-tively simple when language data is treated as bagsof tokens or as n-grams, but much more compli-cated for corpora annotated with complex featureschemes and for treebanks where syntactic depen-dencies are marked.
A great deal of useful infor-mation is encoded in these more complex struc-tured corpora, but access to it is very limited usingthe traditional algorithms and analytical tools ofcomputational linguistics.
Many of the most pow-erful techniques available to natural language pro-cessing have been built on the basis of n-gram andbag of words models, but we already know thatthese methods are inadequate to fully model theinformation in texts or we would have little usefor treebanks or annotation schemes.Suffix trees provide some improvement overn-grams and bag-of-words schemes by identify-ing all frequently occurring sequences regard-less of length (Weiner, 1973; McCreight, 1976;Ravichandran and Hovy, 2002).
While this hasvalue in identifying some multi-word phenomena,any algorithm that models languages on the basisof frequent contiguous string discovery will havetrouble modeling a number of pervasive phenom-ena in natural language.
In particular:?
Long distance dependencies ?
i.e., dependen-cies between words that are too far apart to beaccessible to n-gram models.?
Flexible word orders ?
languages usuallyhave contexts where word order can vary.?
Languages with very rich morphologies thatmust be taken into account or where too muchimportant information is lost through lemma-tization.?
Correlations between different levels of ab-straction in annotation, such as between thelemma of a verb and the semantic or syntac-tic class of its arguments.?
Extra-syntactic correlations that may involveany nearby word, such as semantic primingeffects.In treebanks and other annotated corpora thatcan be converted into rooted, directed graphs,many of these phenomena are accessible as fre-quently recurring subtrees.
For example, considerthe Dutch idiom ?naar huis gaan?, (to go home).The components of this phrase can appear in a va-riety of orders and with words inserted betweenthe constituents:1.
Ik zou naar huis kunnen gaan.
(I could gohome.)2.
We gaan naar huis.
(We?re going home.
)In a treebank, these two sentences would sharea common subtree that encompasses the phrase?naar huis gaan?, as in Figure 1.
Note that for thispurpose, two subtrees are treated as identical if the84Figure 1: The two Dutch sentences Ik zou naarhuis kunnen gaan and We gaan naar huis, parsed,and with the frequent section highlighted.
Notethat these two subtrees are identical except for theorder of the nodes.
(N.B.
: This tree does not takethe difference between the infinitive and conju-gated forms into account.
)only difference between them is the order of thechildren of some or all the nodes.Most theories of syntax use trees to represent in-terlexical dependencies, and generally theories ofmorphology and phonology use either hierarchicaltree structures to represent their formalisms, or useunstructured bags that can be trivially representedas trees.
Most types of linguistic feature systemsare at least in part hierarchical and representable intree form.
Because so many linguistic phenomenaare manifest as frequent subtrees within hierarchi-cal representations that are motivated by linguistictheories, efficient methods for extracting frequentsubtrees from treebanks are therefore potentiallyvery valuable to corpus and computational linguis-tics.2 Previous and Related WorkTree mining research is a subset of graph min-ing focused specifically on rooted, directed acyclicgraphs.
Although there is research into extractingfrequent subtrees from free (unrooted and undi-rected) trees, free tree mining usually proceeds bydeciding which node in any particular tree willbe treated as a root, and then treating it as if itwas a rooted and directed tree (Chi et al, 2003).
(a)(b) (c) (d)Figure 2: Tree (a) and different types of subtree:(b) a bottom-up subtree of (a), (c) an induced sub-tree of (a), and (d) an embedded subtree of (a).Research on frequent subtree discovery generallydraws heavily on early work by Zaki (2002) andAsai et al (2002) who roughly simultaneously be-gan applying the Apriori algorithm to frequent treediscovery (Agrawal et al, 1993).
For a summaryof Apriori, which is widely used in data mining,and a short review of its extensive literature, seeKotsiantis and Kanellopoulos (2006).
A broadsummary of algorithms for frequent subtree min-ing can be found in Chi et al (2004).Research into frequent substructures in compu-tational linguistics is quite limited.
The Data Ori-ented Processing model (Bod et al, 2003) alongwith its extension into machine translation - theData Oriented Translation model (Poutsma, 2000;Poutsma, 2003; Hearne and Way, 2003) - is themost developed approach to using frequent sub-tree statistics to do natural language processing.There is also growing work, largely stemming outof DOP research, into subtree alignment in bilin-gual parsed treebanks as an aid in the developmentof statistical and example-based machine transla-tions systems (Hassan et al, 2006; Tinsley et al,2007; Zhechev and Way, 2008).3 Key conceptsAmong the key concepts in tree mining is the dif-ference between bottom-up subtrees, induced sub-trees and embedded subtrees.
A bottom-up subtreeT?
of a tree T is a subtree where, for every nodein T ?, if its corresponding node in T has children,then all those children are also in T ?.
An induced85Figure 3: ?...between the European Commission and the government of the [German] Federal Repub-lic...?
This structure is a subtree of one of the sentences in the Alpino corpus of Dutch where a node hastwo children with the same labels - two NPs.
This often occurs with conjunctions and can prevent thealgorithm from discovering some frequent subtrees.subtree T ?
of T is a subtree where every node inT?
is either the root of T ?
or its parent in T is alsoits parent in T ?.
An embedded subtree T ?
of T isa subtree where every node in T ?
is either the rootof T ?
or its parent in T ?
is one of its ancestors inT .
See Figure 2 for an example of these differenttypes of subtrees.Linear time solutions exist for finding all fre-quent bottom-up subtrees in a treebank becausethis problem can be transformed into finding allfrequent substrings in a string, a problem forwhich fast solutions are well known (Luccio et al,2001; Luccio et al, 2004).Solutions for induced and embedded subtreesdraw heavily on Zaki (2002) (the TreeMiner al-gorithm) and Asai et al (2002) (the FREQT al-gorithm), both of whom propose Apriori-style ap-proaches.
This type of solution has the generalproperty that runtime is proportionate to the sizeof the output: the sum of the number of timeseach frequent subtree appears in the treebank.
Thisis not readily predictable, because the numberand frequencies of subtrees is not formally de-terminable from the size of the treebank and cangrow very rapidly.3.1 Ordered and unordered treesTreeMiner/FREQT approaches require all trees tobe ordered so that the nodes of any frequent sub-tree will always appear in the same order everytime it appears.
The children of each non-leafnode are sorted into a lexicographic order, butthis only guarantees that frequent subtrees will al-ways appear with the same ordering if no nodehas more than one non-leaf child node with thesame label.
This is not uniformly true of naturallanguage parse trees, as shown in Figure 3.
So-lutions exist that remove this limitation - notablyChi et al (2003) - but they come at a significantlyincreased processing cost.3.2 Closed treesGiven that this type of approach to subtree discov-ery has runtime bounds proportionate to the unpre-dictable size of the output, one way to keep subtreediscovery within manageable bounds is to restrictthe output.
Many of the frequent trees present intreebanks are redundant, since they are identicallydistributed with other, larger trees, as in Figure 4.If a corpus has a sequence of tokens ABCDEthat appears f times, then that corpus also con-tains at least f instances of the sequences A, B, C,D, E, AB, BC, CD, DE, ABC, BCD, CDE,ABCD, and BCDE.
If any of these sequencesappears only in the context of ABCDE, then theyare redundant, because they have the same countand distribution as the longer sequence ABCDE.If a set of sequences is identically distributed -appearing in all the same places - then the longestof those sequences is called a closed sequence.
Inmore formal terms, a sequence S that appears ftimes in a corpus is called closed if and only ifthere is no prefix or suffix a such that aS or Saalso appears f times in the corpus.
This definitionextends easily to trees: A subtree T in a treebankis closed if and only if there is no node that can beadded to it to produce a new subtree T ?
such thatthe frequency of T ?
is equal to the frequency of T .All subtrees in a corpus are either closed subtreesor are subtrees of closed subtrees that appear inexactly the same places in the treebank.
The set ofclosed subtrees in a treebank is the smallest set ofsubtrees that encompasses all the distributions ofsubtrees in the treebank.
Any subtree that is not inthe list of closed subtrees is either a subtree of oneof the closed subtrees that appears exactly as oftenand in all the same places, or does not appear inthe treebank at all.There are algorithms that extract onlyclosed subtrees from treebanks - notablyChi et al (2005a) - and thereby increasetheir speed dramatically without producing less86(a) The common subtreeof the two parse trees inFigure 1: ?naar huis gaan?
(b) Redundant subtrees of tree (a).
Thereare many more such structures.Figure 4: Closed and non-closed subtrees in justthe two sentences in Figure 1.
In a larger treebank,some of these might not be redundant.information, since any non-closed subtree presentin the treebank is a subtree of a closed one andshares its distribution.4 Algorithm and data structuresThe algorithm used in this research is an extensionof the TreeMiner algorithm (Zaki, 2002), modifiedto extract only closed subtrees.
It takes a minimumfrequency threshold as a parameter and extractsonly those subtrees which are closed and whosefrequency is at least equal to the threshold.
Thisalgorithm suffers from the same shortcoming ofZaki?s original algorithm in that it is only guar-anteed to find all frequent subtrees among orderedtrees where no node has two non-leaf children withthe same label.It has one novel property which it appears notto share with any other subtree extraction schemeto date: This algorithm outputs subtrees in orderfrom the most frequent to the least.
Given thedifficulty of predicting in advance how large theoutput will be, and the large size of many naturallanguage data sources, this can be a real boon.
Ifoutput size or memory usage grow too large, or toomuch time has passed, the program can be stoppedwhile still guaranteeing that it has not missed anymore frequent subtree than the last one outputted.This section can only very briefly describe thealgorithm.4.1 DefinitionsA treebank is any collection of trees where eachnode bears a label and each node is uniquely ad-dressable in such a way that the address anof anode n is always greater than the address apof itsparent p. This is accomplished by representing alltrees as ordered depth-first canonical strings.
(SeeChi et al (2005b).
)Each appearance of a subtree within a treebankis characterized by the address of its root in thetreebank and the address of its rightmost node.This data structure will be called a Hit.
The listof all Hits corresponding to all the appearancesof some subtree in the treebank will be called aHitList.
So, for each subtree there is a correspond-ing HitList and vice-versa.
HitLists are alwaysconstructed in sequential order, from first instancein the treebank to last, and can never contain du-plicates.We will define the function queueKey onHitLists to output an array of four numbers in aspecific order, given a HitList as input:1.
The number of Hits in the HitList.2.
The distance from the address of the root ofthe first Hit to the end of the treebank.3.
The distance from the address of the right-most node of the first Hit to the end of thetreebank.4.
The number of nodes in the subtree associ-ated with that HitList.These keys are sortable and designed to ensurethat HitLists from a single treebank can always besorted into a fixed order such that, for two HitListsA and B, if A > B then:1.
A has more Hits than B.2.
If A has the same number of Hits as B, thenthe root of the first Hit in A precedes the rootof the first Hit in B.3.
If A?s first root is identical to B?s, then the ad-dress of the rightmost node of the first Hit inA precedes the address of the rightmost nodeof the first Hit in B.4.
If the first Hit in A is exactly the same the firstHit in B, then the subtree associated with Ahas more nodes than the subtree associatedwith B.87A self-sorting queue is any data structure thatstores key-data pairs and stores the keys in orderfrom greatest to least.
The data structure used toimplement a self-sorting queue in this research isan AVL tree (Adelson-Velskii and Landis, 1962),however, other structures could equally well havebeen used.
The self-sorting queue will be used tomaintain a sorted list of HitLists, sorted in the or-der of their queueKeys as described above.4.2 InitializationFix a minimum frequency threshold t for the sub-trees you wish to extract from the treebank.
Startprocessing by initializing one HitList for eachunique label in the treebank with the set of Hitsthat corresponds to each occurrence of that label.We will treat each as a HitList with an associ-ated subtree containing only one node.
This setis constructed in linear time by iterating over allthe nodes in the treebank.Of the initial HitLists, throw away all those withfewer than threshold frequency t Hits in them.The remaining HitLists are inserted into the self-sorting queue.4.3 Extracting induced subtrees withoutchecking for closureExtracting all the subtrees above a fixed frequency- not just the closed subtrees - in order from themost frequent to the least, proceeds as follows:1.
Initialize as described in Section 4.2.2.
Pop the top HitList hl and its associated sub-tree s from the queue.3.
Extend hl:(a) Visit each Hit in hl and find all thenodes that can be added to the right sideof s to produce new induced subtrees.
(b) Generate new HitLists for all subtreesthat extend s by one node to the right.
(c) Test each new HitList to make sure itappears more than threshold frequencyt times, and if it does, insert it into thequeue.
(d) Output s and hl.4.
Repeat until the queue is empty.This is essentially identical to the TreeMinerand FREQT algorithms already published byZaki (2002) and by Asai et al (2002), except thatit outputs frequent subtrees in order from the mostfrequent to the least.4.4 Extracting only closed induced subtreesBy controlling the order in which HitLists reachthe top of the queue, it is possible to efficientlyprevent any subtree which is not a closed sub-tree or a prefix of a closed subtree from beingextended, and to prevent any subtree that is notclosed from being outputted.Every subtree with a frequency of f is eithera closed subtree, a prefix of a closed subtree thatalso has a frequency of f and can be constructedby adding more nodes to the right, or is a redun-dant non-closed subtree that need not be extendedor stored.
Consider a redundant, non-closed sub-tree x and a closed subtree or prefix of a closedsubtree y which has the same frequency, and hasthe same set of addresses for the rightmost node ofeach of its appearances in the treebank.
The sortorder of the self-sorting queue (see Section 4.1)ensures that if a prefix of a closed subtree y is inthe queue and some subtree of it x is also in thequeue, then y is closer to the top of the queue thanx is.
Furthermore, it can be proven that the pre-fix of a closed subtree with the same distributionas any non-closed, redundant subtree will be gen-erated, inserted into the queue, and removed fromthe top of the queue before x can reach the top.So, to prevent x from being extended or stored,all that is necessary is to check to see there is someclosed subtree or prefix of a closed subtree y suchthat:?
y has already been at the top of the queue.?
y has the same frequency as x.?
The set of rightmost nodes of every Hit iny?s HitList is identical to the set of rightmostnodes of every Hit in x?s HitList.?
x is a subtree of yThis can be checked by constructing a hashvalue for each HitList based on its frequency andsome subset of the set of rightmost nodes of ev-ery Hit.
In our experiments, we used only the firstnode of each HitList.
If x?s hash value matchessome previously processed y?s hash value, thencheck if x is a subtree of y and reject it if it is.The result is to only instantiate closed subtrees andtheir prefixes, and subtrees which are one node ex-tensions of closed subtrees and their prefixes.Like TreeMiner, worst case space and timebounds are proportionate to the number of sub-trees instantiated and the number of times eachappears in the corpus.
This is smaller than the88worst case bounds for TreeMiner because it doesnot instantiate all frequent subtrees.
There is addi-tional approximately constant time processing foreach instantiated subtree to check for closure andto store it in the self-sorting queue.
At the lowestfrequency thresholds, this can take up the major-ity of runtime, but is generally negligible at highfrequencies.5 ResultsWe applied this algorithm to a parsed and hand-corrected 7137 sentence subset of the Alpino Tree-bank of Dutch.1 The average sentence length inthis small treebank is roughly 20 words, and thecorresponding trees have an average of approx-imately 32 nodes for a total of 230,673 nodes.With the minimum frequency set to 2, this algo-rithm extracted 342,401 closed subtrees in about2000 seconds on a conventional workstation run-ning Linux2.
The same implementation but with-out testing for closure - which makes this algo-rithm equivalent to TreeMiner - extracted some 4.2million trees in roughly 11,000 seconds.
Closedtree extraction contrasts quite favorably to extrac-tion without closure, even over a small dataset.Min.
Freq.
Subtrees extracted RuntimeThreshold2 342401 1952.33s3 243484 1004.30s4 176885 588.58s5 134495 402.26s8 72732 209.51s10 53842 163.22s15 30681 112.39s20 20610 85.24s30 11516 66.05s40 7620 54.14s50 5549 47.98s60 4219 43.24s70 3365 39.97sTable 1: Runtime and closed trees extracted at dif-ferent minimum frequency thresholds, using the7137 sentence sample of the Alpino Treebank.Runtime and the number of trees produced fallvery dramatically as thresholds rise - so much so1http://www.let.rug.nl/vannoord/trees/2A Dell Precision 490 workstation with an Intel Dual-Core Xeon processor and 8GB of memory.
The algorithmwas not implemented to use two processors.Sentences Total Subtrees Runtimenodes extracted2500 94528 37607 61.08s5000 189170 98538 260.91s10000 379980 264616 1495.19s15000 573629 477750 3829.29s20000 763502 704018 7998.57sTable 2: Runtime and closed trees extracted fromautomatically parsed samples of the EuroparlDutch corpus, keeping the minimum frequencythreshold constant at 5 for all sizes of treebank.that setting the minimum frequency to 3 instead of2 halved the runtime.
This pattern is characteristicof a power law distribution like Zipf?s law.
(SeeTable 1 and Figure 5.)
Given the pervasivenessof power law distributions in word frequencies, itshould perhaps not be surprising to discover thatfrequent closed subtrees in treebanks are similarlydistributed.
This research may be the first effortto empirically support such a conclusion, althoughadmittedly only very tentatively.To test the impact of varying the size of the tree-bank, but keeping the minimum frequency thresh-old constant, we used a section of the Dutch por-tion of the Europarl corpus (Koehn, 2005) auto-matically parsed using the Alpino Dutch parser(van Noord, 2006) without any manual correction.Random samples of 2500, 5000, 10000, 15000and 20000 sentences were selected, and all sub-trees of frequency 5 or higher were extracted fromeach, as summarized in Table 2.
As treebank sizegrows, the number of subtrees extracted at thesame minimum frequency threshold, and the timeand memory used extracting them, grows expo-nentially.
This is in sharp contrast to algorithmsthat extract frequently recurring strings, which in-crease linearly in time and memory usage as thedata grows.However, if the minimum frequency thresholdis kept constant as a proportion of the size of thetreebank, then the number of trees extracted re-mains roughly constant and the time and memoryused to extract them grows roughly linearly withthe size of the treebank.
Table 3 shows the resultfor different sized random samples of the parsedEuroparl corpus.Lastly, since this algorithm has known difficul-ties when presented with trees where more thanone non-leaf child of a node can have the same89(a) Runtime by minimum fre-quency threshold.
(b) Subtrees extracted by mini-mum frequency threshold.
(c) Log-log plot of (b).Figure 5: Runtime (a) and subtrees extracted (b) from the Alpino sample using different minimum fre-quency thresholds.
Figure (c) is a log-log plot of (b).
Figure (c) looks close to a straight line, which ischaracteristic of a power law distribution.Sents Total Min.
Subtrees Runnodes Freq.
extracted timeThres.2500 99323 5 42905 72.95s5000 194736 10 42783 122.18s10000 382022 20 41988 216.23s15000 574632 30 43078 325.86s20000 770240 40 44416 435.19sTable 3: Runtime and closed trees extracted fromautomatically parsed samples of the EuroparlDutch corpus, with minimum frequency thresh-olds kept roughly constant as a proportion of thesample size.label (see sections 3.1 and 4), we attempted todetermine if this problem is marginal or perva-sive.
The 7137 sentence Alpino Treebank samplecontains 3833 nodes with more than one non-leafchild node with identical labels or roughly 1.7% ofall nodes.
Furthermore, these nodes are present in2666 sentences - some 37% of all sentences!
Thisis a very large minority.In order to estimate the effect this phenomenonhas on the extraction of closed trees, we lookedfor outputted trees that are not closed by compar-ing the HitLists of all outputted trees to all otheroutputted trees with the same frequency.
Table 4shows the number of trees with identical distribu-tions to other outputted trees - i.e.
trees that ap-peared to be closed to this algorithm, but in factare not.
The number was surprisingly large, butdistributed overwhelmingly at the very lowest fre-quencies.Min.
Freq.
Non-closed as a % ofThreshold trees all trees extracted2 2874 0.84%3 693 0.28%4 225 0.13%5 101 0.08%8 18 0.02%10 11 0.02%15 6 0.02%20 3 0.01%30 0 0.00%Table 4: Non-closed trees from the 7137 sentencesample of the Alpino Treebank, produced erro-neously as closed trees because of repeated labels.There were no non-closed trees extracted at fre-quencies over 30.6 ConclusionsThe algorithm presented above opens up tree-banks and annotated corpora to much more de-tailed quantitative analysis, and extends the toolsavailable for data-driven natural language process-ing.
This makes a number of new applicationspossible.
We are developing treebank indexing forfast retrieval by tree similarity, in order to makefull treebanks available for example-based parsingand machine translation in real time.
This algo-rithm also has applications in constructing concor-dances of syntactic, morphological and semanticstructures - types of information that are not tradi-tionally amenable to indexing.
Furthermore, sta-tistical models of natural language data can take90advantage of comprehensive subtree censuses tobecome fully syntax-aware, instead of relying onbag of words and n-gram models.However, there are a number of drawbacks andcaveats that must be highlighted.Runtime, memory usage and output size are dif-ficult to estimate in advance.
This is mitigated inpart by the order in which subtrees are outputted,making it possible to extract only the most fre-quent subset of subtrees given fixed time and spacebounds.
Empirically, it appears that resource re-quirements and output size can also be estimatedby sampling, if minimum frequency thresholdscan be kept constant as a proportion of total tree-bank size.The formalisms used in most theories of syn-tax allow nodes to have multiple non-leaf chil-dren with the same labels.
Although errors causedby non-unique labels are overwhelmingly presentonly among the lowest frequency subtrees, er-rors appear often enough to pose a non-negligibleproblem for this algorithm.We are investigating the degree to which thiscan be mitigated by making different choices oflinguistic formalism.
Syntax trees that containonly binary trees - for example, those constructedusing Chomsky Normal Form rules (Jurafsky andMartin, 2009) - cannot have identically labellednon-leaf children, but must suffer some loss ofgenerality in their frequent subtrees because if it.Other theories can reduce the size of this source oferror, notably dependency syntax which often usesfewer abstract labels (Tesnie`re, 1959; Mel?c?uk,1988; Sugayama and Hudson, 2006), but will mostlikely be poor sources of highly general rules as aconsequence.Furthermore, tree mining algorithms exist thateliminate this problem, but at some cost.
We areinvestigating a hybrid solution to the non-uniquelabel problem that identifies only those subtreeswhere more resource-intensive closure checking isnecessary.
This will guarantee the correct extrac-tion of closed subtrees in all cases while minimiz-ing the additional processing burden.Among the open problems suggested by this re-search is the degree to which the empirical resultsobtained above are dependent on the language ofthe underlying data and the linguistic formalismsused to produce treebanks.
Different linguistictheories use different abstractions and use their ab-stract categories differently.
This has an immedi-ate effect on the number of nodes in a treebankand on the topology of the trees.
Some theoriesproduce more compact trees than others.
Someproduce deep trees, others produce shallow trees.It is likely that the formalisms used in treebankshave a pervasive influence on the number and kindof frequent subtrees extracted.
By doing quantita-tive research on the structures found in treebanks,it may become possible to make reliable opera-tional choices about the linguistic formalisms usedin treebanks on the basis of the kinds of structuresone hopes to get out of them.AcknowledgmentsThis research is supported by the AMASS++Project3 directly funded by the Institute for thePromotion of Innovation by Science and Technol-ogy in Flanders (IWT) (SBO IWT 060051).ReferencesGeorgiy M. Adelson-Velskii and Yevgeniy M. Landis.1962.
An algorithm for the organization of informa-tion.
Proceedings of the USSR Academy of Sciences,146(2):263?266.Rakesh Agrawal, Tomasz Imielinski and Arun Swami.1993.
Mining association rules between sets ofitems in large databases.
Proceedings of the 1993ACM SIGMOD International Conference on Man-agement of Data, 207?216.Tatsuya Asai, Kenji Abe, Shinji Kawasoe, Hi-roki Arimura, Hiroshi Sakamoto and Set-suo Arikawa.
2002.
Efficient substructurediscovery from large semi-structured data.
Proceed-ings of the Second SIAM International Conferenceon Data Mining, 158?174.Rens Bod, Khalil Sima?an and Remko Scha, editors.2003.
Data-Oriented Parsing.
CLSI Publicatons,Stanford, CA.Yun Chi, Yirong Yang and Richard R. Muntz.
2003.Indexing and Mining Free Trees.
UCLA ComputerScience Department Technical Report No.
030041.Yun Chi, Richard R. Muntz, Siegfried Nijssen andJoost N. Kok.
2004.
Frequent Subtree Mining ?
AnOverview.
Fundamenta Informaticae, 66(1-2):161?198.Yun Chi, Yi Xia, Yirong Yang and Richard R. Muntz.2005a.
Mining Closed and Maximal FrequentSubtrees from Databases of Labeled Rooted Trees.IEEE Transactions on Knowledge and Data Engi-neering, 17(2):190?202.3http://www.cs.kuleuven.be/?liir/projects/amass/91Yun Chi, Yi Xia, Yirong Yang and Richard R. Muntz.2005b.
Canonical forms for labelled trees andtheir applications in frequent subtree mining.
IEEETransactions on Knowledge and Data Engineering,8(2):203?234.Hany Hassan, Mary Hearne, Khalil Sima?an andAndy Way.
2006.
Syntactic Phrase-Based Statisti-cal Machine Translation.
Proceedings of the IEEE2006 Workshop on Spoken Language Translation,238?241.Mary Hearne and Andy Way.
2003.
Seeing the Woodfor the Trees: Data-Oriented Translation.
Proceed-ings of the 9th Machine Translation Summit, 165?172.Daniel Jurafsky and James H. Martin.
2009.
Speechand Language Processing.
Pearson Prentice Hall,Upper Saddle River, NJ.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
Proceedings of the10th Machine Translation Summit, 79?86.Sotiris Kotsiantis and Dimitris Kanellopoulos.
2006.Association Rules Mining: A Recent Overview.GESTS International Transactions on ComputerScience and Engineering, 32(1):71?82.Fabrizio Luccio, Antonio Enriquez, Pablo Rieumontand Linda Pagli.
2001.
Exact Rooted SubtreeMatching in Sublinear Time.
Universita` Di PisaTechnical Report TR-01-14.Fabrizio Luccio, Antonio Enriquez, Pablo Rieumontand Linda Pagli.
2004.
Bottom-up subtree isomor-phism for unordered labeled trees.
Universita` DiPisa Technical Report TR-04-13.Edward M. McCreight.
1976.
A Space-EconomicalSuffix Tree Construction Algorithm.
Journal of theAssociation for Computing Machinery, 23(2):262?272.Igor A. Mel?c?uk.
1988.
Dependency Syntax: Theoryand Practice.
State University of New York Press,Albany, NY.Arjen Poutsma.
2000.
Data-Oriented Translation.Proc.
of the 18th International Conference on Com-putational Linguistics (COLING 2000), 635?641.Arjen Poutsma.
2003.
Machine Translation with Tree-DOP.
Data-Oriented Parsing, 63?81.Deepak Ravichandran and Eduard Hovy 2002.
Learn-ing surface text patterns for a question answeringsystem.
Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics(ACL-02), 41?47.Kensei Sugayama and Richard Hudson, editors.
2006.Word Grammar: New Perspectives on a Theory ofLanguage Structure.
Continuum International Pub-lishing, London.Lucien Tesnie`re.
1959.
Ele?ments de la syntaxe struc-turale.
Editions Klincksieck, Paris.John Tinsley, Mary Hearne and Andy Way.
2007.Exploiting Parallel Treebanks for use in Statisti-cal Machine Translation.
Proceedings of Treebanksand Linguistic Theories (TLT ?07), Bergen, Norway175?187.Gertjan van Noord.
2006.
At last parsing is nowoperational.
Verbum Ex Machina.
Actes de la13e confe?rence sur le traitement automatique deslangues naturelles (TALN6), 20?42.Peter Weiner.
1973 Linear pattern matching algorithm.14th Annual IEEE Symposium on Switching and Au-tomata Theory, 1?11.Mohammed J. Zaki.
2002.
Efficiently mining fre-quent trees in a forest.
Proceedings of the 8th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, 1021?1035.Ventsislav Zhechev and Andy Way.
2008.
Auto-matic Generation of Parallel Treebanks.
Proceed-ings of the 22nd International Conference on Com-putational Linguistics (COLING 2008), 1105?1112.92
