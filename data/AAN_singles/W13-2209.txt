Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 99?103,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsYandex School of Data Analysis machine translation systems for WMT13Alexey Borisov, Jacob Dlougach, Irina GalinskayaYandex School of Data Analysis16, Leo Tolstoy street, Moscow, Russia{alborisov,jacob,galinskaya}@yandex-team.ruAbstractThis paper describes the English-Russianand Russian-English statistical machinetranslation (SMT) systems developed atYandex School of Data Analysis for theshared translation task of the ACL 2013Eighth Workshop on Statistical MachineTranslation.
We adopted phrase-basedSMT approach and evaluated a numberof different techniques, including data fil-tering, spelling correction, alignment oflemmatized word forms and translitera-tion.
Altogether they yielded +2.0 and+1.5 BLEU improvement for ru-en and en-ru language pairs.
We also report on theexperiments that did not have any positiveeffect and provide an analysis of the prob-lems we encountered during the develop-ment of our systems.1 IntroductionWe participated in the shared translation task ofthe ACL 2013 Workshop on Statistical MachineTranslation (WMT13) for ru-en and en-ru lan-guage pairs.
We provide a detailed description ofthe experiments carried out for the development ofour systems.The rest of the paper is organized as follows.Section 2 describes the tools and data we used.Our Russian?English and English?Russian se-tups are discussed in Section 3.
In Section 4 wereport on the experiments that did not have anypositive effect despite our expectations.
We pro-vide a thorough analysis of erroneous outputs inSection 5 and draw conclusions in Section 6.2 Tools and data2.1 ToolsWe used an open source SMT system Moses(Koehn et al 2007) for all our experiments ex-cluding the one described in Section 4.1 due to itsperformance constraints.
To overcome the limita-tion we employed our in-house decoder.Language models (LM) were created with anopen source IRSTLM toolkit (Federico et al2008).
We computed 4-gram LMs with modifiedKneser-Ney smoothing (Kneser and Ney, 1995).We used an open source MGIZA++ tool (Gaoand Vogel, 2008) to compute word alignment.To obtain part of speech (POS) tags we usedan open source Stanford POS tagger for English(Toutanova et al 2003) and an open source suiteof language analyzers, FreeLing 3.0 (Carreras etal., 2004; Padr?
and Stanilovsky, 2012), for Rus-sian.We utilized a closed source free for non-commercial use morphological analyzer, Mystem(Segalovich, 2003), that used a limited dictionaryto obtain lemmas.We also made use of the in-house language rec-ognizer based on (Dunning, 1994) and a spellingcorrector designed on the basis of the work ofCucerzan and Brill (2004).We report all results in case-sensitive BLEU(Papineni et al 2002) using mt-eval13a scriptfrom Moses distribution.2.2 DataTraining dataWe used News Commentary and News Crawlmonolingual corpora provided by the organizersof the workshop.Bilingual training data comprised English-Russian parallel corpus release by Yandex1, NewsCommentary and Common Crawl corpora pro-vided by the organizers.We also exploited Wiki Headlines collection ofthree parallel corpora provided by CMU2 as a1https://translate.yandex.ru/corpus2http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz99source of reliable data.Development setThe newstest2012 test set (Callison-Burch et al2012) was divided in the ratio 2:1 into a tuningset and a test set.
The latter is referred to asnewstest2012-test in the rest of the paper.3 Primary setups3.1 BaselineWe built the baseline systems according to the in-structions available at the Moses website3.3.2 PreprocessingThe first thing we noticed was that some sentencesmarked as Russian appeared to be sentences inother languages (most commonly English).
Weapplied a language recognizer for both monolin-gual and bilingual corpora.
Results are given inTable 1.Corpus Filtered out (%)Bilingual 3.39Monolingual (English) 0.41Monolingual (Russian) 0.58Table 1: Results of the language recognizer: per-centage of filtered out sentences.The next thing we came across was the pres-ence of a lot of spelling errors in our training data,so we applied a spelling corrector.
Statistics arepresented in Table 2.Corpus Modified (%)Bilingual (English) 0.79Bilingual (Russian) 1.45Monolingual (English) 0.61Monolingual (Russian) 0.52Table 2: Results of the spelling corrector: percent-age of modified sentences.3.3 Alignment of lemmatized word formsRussian is a language with rich morphology.
Thediversity of word forms results in data sparse-ness that makes translation of rare words dif-ficult.
In some cases inflections do not con-tain any additional information and are used3http://www.statmt.org/moses/?n=moses.baselineonly to make an agreement between two words.E.g.
ADJ + NOUN: ??????
??
????
(beau-tiful harp), ??????
??
???????
(beautiful pi-ano), ??????
??
?????
(beautiful grand piano).These inflections reflect the gender of the nounwords, that has no equivalent in English.In this particular case we can drop the inflec-tions, but for other categories they can still be use-ful for translation, because the information theycontain appears in function words in English.
Onthe other hand, most of Russian morphology isuseless for word alignment.We applied a morphological analyzer Mystem(Segalovich, 2003) to the Russian text and con-verted each word to its dictionary form.
Nextwe computed word alignment between the origi-nal English text and the lemmatized Russian text.All the other steps were executed according to thestandard procedure with the original texts.3.4 Phrase score adjustmentSometimes phrases occur one or two times in thetraining corpus.
In this case the correspondingphrase translation probability would be overesti-mated.
We used Good-Turing technique describedin (Gale, 1994) to decrease it to some more realis-tic value.3.5 DecodingMinimum Bayes-Risk (MBR)MBR decoding (Kumar and Byrne, 2004) aimsto minimize the expected loss of translation er-rors.
As it is not possible to explore the space ofall possible translations, we approximated it withthe 1,000 most probable translations.
A minussmoothed BLEU score (Lin and Och, 2004) wasused for the loss function.Reordering constrainsWe forbade reordering over punctuation and trans-lated quoted phrases independently.3.6 Handling unknown wordsThe news texts contained a lot of proper namesthat did not appear in the training data.
E.g.
al-most 25% of our translations contained unknownwords.
Dropping the unknown words would leadto better BLEU scores, but it might had causedbad effect on human judgement.
To leave themin Cyrillic was not an option, so we exploited twoapproaches: incorporating reliable data from WikiHeadlines and transliteration.100newstest2012-test newstest2013Russian?EnglishBaseline 28.96 21.82+ Preprocessing 29.59 22.28+ Alignment of lemmatized word forms 29.97 22.61+ Good-Turing 30.31 22.87+ MBR 30.45 23.21+ Reordering constraints 30.54 23.33+ Wiki Headlines 30.68 23.46+ Transliteration 30.93 23.73English?RussianBaseline 21.96 16.24+ Preprocessing 22.48 16.76+ Good-Turing 22.84 17.13+ MBR and Reordering constraints 23.27 17.45+ Wiki Headlines and Transliteration 23.54 17.80Table 3: Experimental results in case-sensitive BLEU for Russian?English and English?Russian tasks.Wiki HeadlinesWe replaced the names occurring in the text withtheir translations, based on the information in"guessed-names" corpus from Wiki Headlines.As has been mentioned in Section 3.3, Russianis a morphologically rich language.
This oftenmakes it hard to find exactly the same phrases,so we applied lemmatization of Russian languageboth for the input text and the Russian side of thereference corpus.Russian?English transliterationWe gained considerable improvement from incor-porating Wiki Headlines, but still 17% of transla-tions contained Cyrillic symbols.We applied a transliteration algorithm based on(Knight and Graehl, 1998).
This technique yieldedus a significant improvement, but introduced a lotof errors.
E.g.
??????
????
(James Bond) wasconverted to Dzhejms Bond.English?Russian transliterationIn Russian, it is a common practice to leave someforeign words in Latin.
E.g.
the names of compa-nies: Apple, Google, Microsoft look inadmissiblewhen either translated directly or transliterated.Taking this into account, we applied thesame transliteration algorithm (Knight and Graehl,1998), but replaced an unknown word with itstransliteration only if we found a sufficient num-ber of occurrences of its transliterated form in themonolingual corpus.
We used five for such num-ber.3.7 Experimental resultsWe summarized the gains from the de-scribed techniques for Russian?English andEnglish?Russian tasks on Table 3.4 What did not work4.1 Translation in two stagesFrequently machine translations contain errorsthat can be easily corrected by human post-editors.Since human aided machine translation is cost-efficient, we decided to address this problem to thecomputer.We propose to translate sentences in two stages.At the first stage a SMT system is used to trans-late the input text into a preliminary form (in targetlanguage).
At the next stage the preliminary formis translated again with an auxiliary SMT systemtrained on the translated and the target sides of theparallel corpus.We encountered a technical challenge, when wehad to build a SMT system for the second stage.A training corpus with one side generated withthe first stage SMT system was not possible to beacquired with Moses due to its performance con-straints.
Thereupon we utilized our in-house SMTdecoder and managed to translate 2M sentences intime.We applied this technique both for ru-en and en-ru language pairs.
Approximately 20% of the sen-101tences had changed, but the BLEU score remainedthe same.4.2 Factored modelWe tried to build a factored model for ru-en lan-guage pair with POS tags produced by StanfordPOS tagger (Toutanova et al 2003).Unfortunately, we did not gain any improve-ments from it.5 AnalysisWe carefully examined the erroneous outputs ofour system and compared it with the outputs ofthe other systems participating in ru-en and en-rutasks, and with the commercial systems availableonline (Bing, Google, Yandex).5.1 TransliterationRussian?EnglishThe standard transliteration procedure is not in-vertible.
This means that a Latin word being trans-fered into Cyrillic and then transliterated backto Latin produces an artificial word form.
E.g.??????
??????????
/ Havard Halvarsen wascorrectly transliterated by only four out of 23systems, including ours.
Twelve systems eitherdropped one of the words or left it in Cyrillic.We provide a list of typical mistakes in order oftheir frequency: Khavard Khalvarsen, KhavardKhal?varsen, Xavard Xaljvarsen.
Another exam-ple: ????
??????
(Miss Wyatt) ?
Miss Uayett(all the systems failed).The next issue is the presence of non-null in-flections that most certainly would result in wrongtranslation by any straight-forward algorithm.
E.g.???????????
?
(Heidelberg)?
Heidelberga.English?RussianIn Russian, most words of foreign origin are writ-ten phonetically.
Thereby, in order to obtain thebest quality we should transliterate the transcrip-tion, not the word itself.
E.g.
the French derivedname Elsie Monereau [?elsi mon@?r@V] being trans-lated by letters would result in ????
???????
?while the transliteration of the transcription wouldresult in the correct form ????
????
?.5.2 GrammarsEnglish and Russian make use of different gram-mars.
When the difference in their sentence struc-ture becomes fundamental the phrase-based ap-proach might get inapplicable.Word orderBoth Russian and English are classified as subject-verb-object (SOV) languages, but Russian hasrather flexible word order compared to Englishand might frequently appear in other forms.
Thisoften results in wrong structure of the translatedsentence.
A common mistake made by our sys-tem and reproduced by the major online services:??
??????????
?
???????
(rules have not beenchanged either) ?
have not changed and therules.Constructions?
there is / there are is a non-local construc-tion that has no equivalent in Russian.
Inmost cases it can not be produced from theRussian text.
E.g.
??
?????
?????
??????-??
(there is a matryoshka doll on the table)?
on the table is a matryoshka.?
multiple negatives in Russian are grammati-cally correct ways to express negation (a sin-gle negative is sometimes incorrect) whilethey are undesirable in standard English.
E.g.???
?????
???????
??
???
(nobody hasever been there) being translated word byword would result in there nobody never notwas.5.3 IdiomsIdiomatic expressions are hard to discover anddangerous to translate literary.
E.g.
a Russianidiom ????
??
????
(let come what may) be-ing translated word by word would result in wasnot was.
Neither of the commercial systems wechecked managed to collect sufficient statistic totranslate this very popular expression.6 ConclusionWe have described the primary systems developedby the team of Yandex School of Data Analysis forWMT13 shared translation task.We have reported on the experiments anddemonstrated considerable improvements over therespective baseline.
Among the most notable tech-niques are data filtering, spelling correction, align-ment of lemmatized word forms and translitera-tion.
We have analyzed the drawbacks of our sys-tems and shared the ideas for further research.102ReferencesChris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical ma-chine translation.
In Proceedings of the Sev-enth Workshop on Statistical Machine Translation(WMT12), pages 10?51, Montr?al, Canada, June.Association for Computational Linguistics.Xavier Carreras, Isaac Chao, Llu?s Padr?, and MuntsaPadr?.
2004.
FreeLing: An open-source suite oflanguage analyzers.
In Proceedings of the 4th In-ternational Conference on Language Resources andEvaluation (LREC).Silviu Cucerzan and Eric Brill.
2004.
Spelling cor-rection as an iterative process that exploits the col-lective knowledge of web users.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 293?300.Ted Dunning.
1994.
Statistical identification of lan-guage.
Technical report, Computing Research Lab(CRL), New Mexico State University, Las Cruces,NM, USA.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkitfor handling large scale language models.
In Pro-ceedings of 9th Annual Conference of the Interna-tional Speech Communication Association (INTER-SPEECH), pages 1618?1621.William Gale.
1994.
Good-Turing smoothing with-out tears.
Journal of Quantitative Linguistics (JQL),2:217?237.Qin Gao and Stephan Vogel.
2008.
Parallel imple-mentations of word alignment tool.
In Proceedingsof the 46th Annual Meeting of the Association forComputational Linguistics (ACL), pages 49?57.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the International Conference on Acous-tics, Speech, and Signal Processing (ICASSP), vol-ume 1, pages 181?184.Kevin Knight and Jonathan Graehl.
1998.
Ma-chine transliteration.
Computational Linguistics,24(4):599?612.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-burch, Richard Zens, Rwth Aachen,Alexandra Constantin, Marcello Federico, NicolaBertoldi, Chris Dyer, Brooke Cowan, Wade Shen,Christine Moran, and Ondr?ej Bojar.
2007.
Moses:Open source toolkit for statistical machine trans-lation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL), pages 177?180.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of the Human Language Tech-nology Conference of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL), pages 163?171.Chin-Yew Lin and Franz Josef Och.
2004.
OR-ANGE: a method for evaluating automatic evalua-tion metrics for machine translation.
In Proceed-ings of the 20th international conference on Com-putational Linguistics (COLING), Stroudsburg, PA,USA.
Association for Computational Linguistics.Llu?s Padr?
and Evgeny Stanilovsky.
2012.
FreeLing3.0: Towards wider multilinguality.
In Proceedingsof the Language Resources and Evaluation Confer-ence (LREC), Istanbul, Turkey, May.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Processingsof the 41st Annual Meeting of the Association forComputational Linguistics (ACL), pages 311?318.Ilya Segalovich.
2003.
A fast morphological algorithmwith unknown word guessing induced by a dictio-nary for a web search engine.
In Hamid R. Arab-nia and Elena B. Kozerenko, editors, Proceedings ofthe International Conference on Machine Learning;Models, Technologies and Applications (MLMTA),pages 273?280, Las Vegas, NV, USA, June.
CSREAPress.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the Human Language Technol-ogy Conference of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL), pages 252?259.103
