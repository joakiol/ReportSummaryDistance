Proceedings of NAACL HLT 2007, Companion Volume, pages 109?112,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsEfficient Computation of Entropy Gradient forSemi-Supervised Conditional Random FieldsGideon S. Mann and Andrew McCallumDepartment of Computer ScienceUniversity of MassachusettsAmherst, MA 01003gideon.mann@gmail.com , mccallum@cs.umass.eduAbstractEntropy regularization is a straightforwardand successful method of semi-supervisedlearning that augments the traditional con-ditional likelihood objective function withan additional term that aims to minimizethe predicted label entropy on unlabeleddata.
It has previously been demonstratedto provide positive results in linear-chainCRFs, but the published method for cal-culating the entropy gradient requires sig-nificantly more computation than super-vised CRF training.
This paper presentsa new derivation and dynamic programfor calculating the entropy gradient thatis significantly more efficient?having thesame asymptotic time complexity as su-pervised CRF training.
We also presentefficient generalizations of this methodfor calculating the label entropy of allsub-sequences, which is useful for activelearning, among other applications.1 IntroductionSemi-supervised learning is of growing importancein machine learning and NLP (Zhu, 2005).
Condi-tional random fields (CRFs) (Lafferty et al, 2001)are an appealing target for semi-supervised learningbecause they achieve state-of-the-art performanceacross a broad spectrum of sequence labeling tasks,and yet, like many other machine learning methods,training them by supervised learning typically re-quires large annotated data sets.Entropy regularization (ER) is a method of semi-supervised learning first proposed for classificationtasks (Grandvalet and Bengio, 2004).
In addition tomaximizing conditional likelihood of the availablelabels, ER also aims to minimize the entropy of thepredicted label distribution on unlabeled data.
By in-sisting on peaked, confident predictions, ER guidesthe decision boundary away from dense regions ofinput space.
It is simple and compelling?no pre-clustering, no ?auxiliary functions,?
tuning of onlyone meta-parameter and it is discriminative.Jiao et al (2006) apply this method to linear-chain CRFs and demonstrate encouraging accuracyimprovements on a gene-name-tagging task.
How-ever, the method they present for calculating thegradient of the entropy takes substantially greatertime than the traditional supervised-only gradient.Whereas supervised training requires only classicforward/backward, taking time O(ns2) (sequencelength times the square of the number of labels),their training method takes O(n2s3)?a factor ofO(ns) more.
This greatly reduces the practicalityof using large amounts of unlabeled data, which isexactly the desired use-case.This paper presents a new, more efficient entropygradient derivation and dynamic program that hasthe same asymptotic time complexity as the gradientfor traditional CRF training, O(ns2).
In order to de-scribe this calculation, the paper introduces the con-cept of subsequence constrained entropy?the en-tropy of a CRF for an observed data sequence whenpart of the label sequence is fixed.
These meth-ods will allow training on larger unannotated dataset sizes than previously possible and support active109learning.2 Semi-Supervised CRF TrainingLafferty et al (2001) present linear-chain CRFs, adiscriminative probabilistic model over observationsequences x and label sequences Y = ?Y1..Yn?,where |x| = |Y | = n, and each label Yi has s differ-ent possible discrete values.
For a linear-chain CRFof Markov order one:p?
(Y |x) =1Z(x)exp(?k?kFk(x, Y )),where Fk(x, Y ) =?i fk(x, Yi, Yi+1, i),and the partition function Z(x) =?Y exp(?k ?kFk(x, Y )).
Given trainingdata D = ?d1..dn?, the model is trained bymaximizing the log-likelihood of the dataL(?
;D) =?d log p?
(Y(d)|x(d)) by gradientmethods (e.g.
Limited Memory BFGS), where thegradient of the likelihood is:???kL(?
;D) =?dFk(x(d), Y (d))??d?Yp?
(Y |x(d))Fk(x(d), Y ).The second term (the expected counts of the featuresgiven the model) can be computed in a tractableamount of time, since according to the Markov as-sumption, the feature expectations can be rewritten:?Yp?
(Y |x)Fk(x, Y ) =?i?Yi,Yi+1p?
(Yi, Yi+1|x)fk(x, Yi, Yi+1).A dynamic program (the forward/backward algo-rithm) then computes in time O(ns2) all the neededprobabilities p?
(Yi, Yi+1), where n is the sequencelength, and s is the number of labels.For semi-supervised training by entropy regular-ization, we change the objective function by addingthe negative entropy of the unannotated data U =?u1..un?.
(Here Gaussian prior is also shown.)L(?
;D,U) =?nlog p?
(Y(d)|x(d)) ?
?k?k2?2+ ??up?
(Y(u)|x(u)) log p?
(Y(u)|x(u)).This negative entropy term increases as the decisionboundary is moved into sparsely-populated regionsof input space.3 An Efficient Form of the EntropyGradientIn order to maximize the above objective function,the gradient for the entropy term must be computed.Jiao et al (2006) perform this computation by:????
H(Y |x) = covp?
(Y |x)[F (x, Y )]?,wherecovp?
(Y |x)[Fj(x, Y ), Fk(x, Y )] =Ep?
(Y |x)[Fj(x, Y ), Fk(x, Y )]?
Ep?
(Y |x)[Fj(x, Y )]Ep?
(Y |x)[Fk(x, Y )].While the second term of the covariance is easyto compute, the first term requires calculation ofquadratic feature expectations.
The algorithm theypropose to compute this term is O(n2s3) as it re-quires an extra nested loop in forward/backward.However, the above form of the gradient is notthe only possibility.
We present here an alternativederivation of the gradient:??
?k?H(Y |x) =???kXYp?
(Y |x) log p?
(Y |x)=XY????kp?
(Y |x)?log p?
(Y |x)+ p?
(Y |x)???
?klog p?
(Y |x)?=XYp?
(Y |x) log p?
(Y |x)?Fk(x, Y ) ?XY ?p?
(Y?|x)Fk(x, Y?)!+XYp?
(Y |x)Fk(x, Y ) ?XY ?p?
(Y?|x)Fk(x, Y?
)!.Since?Y p?
(Y |x)?Y ?
p?
(Y?|X)Fk(x, Y ?)
=?Y ?
p?
(Y?|X)Fk(x, Y ?
), the second summand can-cels, leaving:???
?H(Y |x) =XYp?
(Y |x) log p?
(Y |x)Fk(x, Y )?XYp?
(Y |x) log p?
(Y |x)!XY ?p?
(Y?|x)Fk(x, Y?
)!.Like the gradient obtained by Jiao et al (2006),there are two terms, and the second is easily com-putable given the feature expectations obtained by110forward/backward and the entropy for the sequence.However, unlike the previous method, here the firstterm can be efficiently calculated as well.
First,the term must be further factored into a form moreamenable to analysis:?Yp?
(Y |x) log p?
(Y |x)Fk(x, Y )=?Yp?
(Y |x) log p?
(Y |x)?ifk(x, Yi, Yi+1, i)=?i?Yi,Yi+1fk(x, Yi, Yi+1, i)?Y?(i..i+1)p?
(Y |x) log p?
(Y |x).Here, Y?
(i..i+1) = ?Y1..(i?1)Y(i+2)..n?.
In orderto efficiently calculate this term, it is sufficientto calculate?Y?(i..i+1)p?
(Y |x) log p?
(Y |x) for allpairs yi, yi+1.
The next section presents a dynamicprogram which can perform these computations inO(ns2).4 Subsequence Constrained EntropyWe define subsequence constrained entropy asH?(Y?
(a..b)|ya..b, x) =?Y?(a..b)p?
(Y |x) log p?
(Y |x).The key to the efficient calculation for all subsetsis to note that the entropy can be factored given alinear-chain CRF of Markov order 1, since Yi+2 isindependent of Yi given Yi+1.?Y?(a..b)p?(Y?
(a..b), ya..b|x) log p?(Y?
(a..b), ya..b|x)=?Y?(a..b)p?(ya..b|x)p?(Y?
(a..b)|ya..b, x)?
(log p?
(ya..b|x) + log p?(Y?
(a..b)|ya..b, x))=p?
(ya..b|x) log p?
(ya..b|x)+ p?(ya..b|x)H?(Y?
(a..b)|ya..b, x)=p?
(ya..b|x) log p?
(ya..b|x)+ p?(ya..b|x)H?(Y1..
(a?1)|ya, x)+ p?(ya..b|x)H?
(Y(b+1)..n|yb, x).Given the H?(?)
and H?(?)
lattices, any sequenceentropy can be computed in constant time.
Figure 1H (0|y6)H (Y6|y5)H (0|y1) H (Y1|y2)y4y3?
?
?
?Figure 1: Partial lattice shown for com-puting the subsequence constrained entropy:PY p(Y?
(3..4), y3, y4) log p(Y?
(3..4), y3, y4).
Once thecomplete H?
and H?
lattices are constructed (in the directionof the arrows), the entropy for each label sequence can becomputed in linear time.illustrates an example in which the constrained se-quence is of size two, but the method applies toarbitrary-length contiguous label sequences.Computing the H?(?)
and H?(?)
lattices is easilyperformed using the probabilities obtained by for-ward/backward.
First recall the decomposition for-mulas for entropy:H(X,Y ) = H(X) + H(Y |X)H(Y |X) =?xP (X = x)H(Y |X = x).Using this decomposition, we can define a dynamicprogram over the entropy lattices similar to for-ward/backward:H?
(Y1..i|yi+1, x)=H(Yi|yi+1, x) + H(Y1..(i?1)|Yi, yi+1, x)=?yip?
(yi|yi+1, x) log p?
(yi|yi+1, x)+?yip?
(yi|yi+1, x)H?(Y1..
(i?1)|yi).The base case for the dynamic program isH?
(?|y1) = p(y1) log p(y1).
The backward entropyis computed in a similar fashion.
The conditionalprobabilities p?
(yi|yi?1, x) in each of these dynamicprograms are available by marginalizing over theper-transition marginal probabilities obtained fromforward/backward.The computational complexity of this calcula-tion for one label sequence requires one run of for-ward/backward at O(ns2), and equivalent time to111calculate the lattices for H?
and H?
.
To calculatethe gradient requires one final iteration over all labelpairs at each position, which is again time O(ns2),but no greater, as forward/backward and the en-tropy calculations need only to be done once.
Thecomplete asymptotic computational cost of calcu-lating the entropy gradient is O(ns2), which is thesame time as supervised training, and a factor ofO(ns) faster than the method proposed by Jiao etal.
(2006).Wall clock timing experiments show that thismethod takes approximately 1.5 times as long astraditional supervised training?less than the con-stant factors would suggest.1 In practice, since thethree extra dynamic programs do not require re-calculation of the dot-product between parametersand input features (typically the most expensive partof inference), they are significantly faster than cal-culating the original forward/backward lattice.5 Confidence EstimationIn addition to its merits for computing the entropygradient, subsequence constrained entropy has otheruses, including confidence estimation.
Kim et al(2006) propose using entropy as a confidence esti-mator in active learning in CRFs, where exampleswith the most uncertainty are selected for presenta-tion to humans labelers.
In practice, they approxi-mate the entropy of the labels given the N-best la-bels.
Not only could our method quickly and ex-actly compute the true entropy, but it could also beused to find the subsequence that has the highest un-certainty, which could further reduce the additionalhuman tagging effort.6 Related WorkHernando et al (2005) present a dynamic programfor calculating the entropy of a HMM, which hassome loose similarities to the forward pass of thealgorithm proposed in this paper.
Notably, our algo-rithm allows for efficient calculation of entropy forany label subsequence.Semi-supervised learning has been used in manymodels, predominantly for classification, as opposedto structured output models like CRFs.
Zhu (2005)1Reporting experimental results with accuracy is unneces-sary since we duplicate the training method of Jiao et al (2006).provides a comprehensive survey of popular semi-supervised learning techniques.7 ConclusionThis paper presents two algorithmic advances.
First,it introduces an efficient method for calculatingsubsequence constrained entropies in linear-chainCRFs, (useful for active learning).
Second, itdemonstrates how these subsequence constrainedentropies can be used to efficiently calculate thegradient of the CRF entropy in time O(ns2)?the same asymptotic time complexity as the for-ward/backward algorithm, and a O(ns) improve-ment over previous algorithms?enabling the prac-tical application of CRF entropy regularization tolarge unlabeled data sets.AcknowledgementsThis work was supported in part by DoD contract #HM1582-06-1-2013, in part by The Central Intelligence Agency, the Na-tional Security Agency and National Science Foundation underNSF grant #IIS-0427594, and in part by the Defense AdvancedResearch Projects Agency (DARPA), through the Departmentof the Interior, NBC, Acquisition Services Division, under con-tract number NBCHD030010.
Any opinions, findings and con-clusions or recommendations expressed in this material belongto the author(s) and do not necessarily reflect those of the spon-sor.ReferencesY.
Grandvalet and Y. Bengio.
2004.
Semi-supervised learningby entropy minimization.
In NIPS.D.
Hernando, V. Crespi, and G. Cybenko.
2005.
Efficient com-putation of the hidden markov model entropy for a givenobservation sequence.
IEEE Trans.
on Information Theory,51:7:2681?2685.F.
Jiao, S. Wang, C.-H. Lee, R. Greiner, and D. Schuur-mans.
2006.
Semi-supervised conditional random fieldsfor improved sequence segmentation and labeling.
In COL-ING/ACL.S.
Kim, Y.
Song, K. Kim, J.-W. Cha, and G. G. Lee.
2006.Mmr-based active machine learning for bio named entityrecognition.
In HLT/NAACL.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting and la-beling sequence data.
In Proceedings of ICML, pages 282?289.X.
Zhu.
2005.
Semi-supervised learning literature survey.Technical Report 1530, Computer Sciences, University ofWisconsin-Madison.112
