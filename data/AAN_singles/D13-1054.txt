Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 567?577,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsRecursive Autoencoders for ITG-based TranslationPeng Li, Yang Liu and Maosong SunState Key Laboratory of Intelligent Technology and SystemsTsinghua National Laboratory for Information Science and TechnologyDepartment of Computer Science and Technology, Tsinghua University, Beijing 100084, Chinapengli09@gmail.com, {liuyang2011,sms}@tsinghua.edu.cnAbstractWhile inversion transduction grammar (ITG)is well suited for modeling ordering shiftsbetween languages, how to make applyingthe two reordering rules (i.e., straight andinverted) dependent on actual blocks beingmerged remains a challenge.
Unlike previouswork that only uses boundary words, we pro-pose to use recursive autoencoders to makefull use of the entire merging blocks alter-natively.
The recursive autoencoders are ca-pable of generating vector space representa-tions for variable-sized phrases, which enablepredicting orders to exploit syntactic and se-mantic information from a neural languagemodeling?s perspective.
Experiments on theNIST 2008 dataset show that our system sig-nificantly improves over the MaxEnt classifierby 1.07 BLEU points.1 IntroductionPhrase-based models (Koehn et al 2003; Och andNey, 2004) have been widely used in practical ma-chine translation (MT) systems due to their effec-tiveness, simplicity, and applicability.
First, as se-quences of consecutive words, phrases are capableof memorizing local word selection and reorder-ing, making them an effective mechanism for trans-lating idioms or translations with word insertionsor omissions.
Moreover, n-gram language modelscan be seamlessly integrated into phrase-based de-coders since partial translations grow left to rightin decoding.
Finally, phrase-based systems can beapplicable to most domains and languages, espe-cially for resource-scarce languages without high-accuracy parsers.However, as phrase-based decoding casts transla-tion as a string concatenation problem and permitsarbitrary permutations, it proves to be NP-complete(Knight, 1999).
Therefore, phrase reordering mod-eling has attracted intensive attention in the pastdecade (e.g., Och et al 2004; Tillman, 2004; Zenset al 2004; Al-Onaizan and Papineni, 2006; Xionget al 2006; Koehn et al 2007; Galley and Man-ning, 2008; Feng et al 2010; Green et al 2010;Bisazza and Federico, 2012; Cherry, 2013).Among them, reordering models based on inver-sion transduction grammar (ITG) (Wu, 1997) areone of the important ongoing research directions.As a formalism for bilingual modeling of sentencepairs, ITG is particularly well suited to predictingordering shifts between languages.
As a result, anumber of authors have incorporated ITG into left-to-right decoding to constrain the reordering spaceand reported significant improvements (e.g., Zens etal., 2004; Feng et al 2010).
Along another line,Xiong et al(2006) propose a maximum entropy(MaxEnt) reordering model based on ITG.
They usethe CKY algorithm to recursively merge two blocks(i.e., a pair of source and target strings) into largerblocks, either in a straight or an inverted order.
Un-like lexicalized reordering models (Tillman, 2004;Koehn et al 2007; Galley and Manning, 2008) thatare defined on individual bilingual phrases, the Max-Ent ITG reordering model is a two-category classi-fier (i.e., straight or inverted) for two arbitrary bilin-gual phrases of which the source phrases are adja-cent.
This potentially alleviates the data sparseness567problem since there are usually a large number ofreordering training examples available (Xiong et al2006).
As a result, the MaxEnt ITG model and itsextensions (Xiong et al 2008; Xiong et al 2010)have achieved competing performance as comparedwith state-of-the-art phrase-based systems.Despite these successful efforts, the ITG reorder-ing classifiers still face a major challenge: how toextract features from training examples (i.e., a pairof bilingual strings).
It is hard to decide which wordsare representative for predicting reordering, eithermanually or automatically, especially for long sen-tences.
As a result, Xiong et al(2006) only useboundary words (i.e., the first and the last words ina string) to predict the ordering.
What if we lookinside?
Is it possible to avoid manual feature engi-neering and learn semantic representations from thedata?Fortunately, the rapid development of intersect-ing deep learning with natural language processing(Bengio et al 2003; Collobert and Weston, 2008;Collobert et al 2011; Glorot et al 2011; Bordes etal., 2011; Socher et al 2011a; Socher et al 2011b;Socher et al 2011c; Socher et al 2012; Bordes etal., 2012; Huang et al 2012; Socher et al 2013;Hermann and Blunsom, 2013) brings hope for alle-viating this problem.
In these efforts, natural lan-guage words are represented as real-valued vectors,which can be naturally fed to neural networks as in-put.
More importantly, it is possible to learn vec-tor space representations for multi-word phrases us-ing recursive autoencoders (Socher et al 2011c),which opens the door to leveraging semantic repre-sentations of phrases in reordering models from aneural language modeling point of view.In this work, we propose an ITG reordering clas-sifier based on recursive autoencoders.
The neu-ral network consists of four autoencoders (i.e., thefirst source phrase, the first target phrase, the sec-ond source phrase, and the second target phrase)and a softmax layer.
The recursive autoencoders,which are trained on reordering examples extractedfrom word-aligned bilingual corpus, are capableof producing vector space representations for arbi-trary multi-word strings in decoding.
Therefore,our model takes the whole phrases rather than onlyboundary words into consideration when predict-ing phrase permutations.
Experiments on the NIST2008 dataset show that our system significantly im-proves over the MaxEnt classifier by 1.07 in termsof case-insensitive BLEU score.2 Recursive Autoencoders for ITG-basedTranslation2.1 Inversion Transduction GrammarInversion transduction grammar (ITG) (Wu, 1997)is a formalism for synchronous parsing of bilingualsentence pairs.
Xiong et al(2006) apply bracketingtransduction grammar (BTG), which is a simplifiedversion of ITG, to phrase-based translation using thefollowing production rules:X ?
[X1, X2] (1)X ?
?X1, X2?
(2)X ?
f/e (3)where X is a block that consists of a pair of sourceand target strings, f is a source phrase, and e is a tar-get phrase.
X1 and X2 are two neighboring blocksof which the two source phrases are adjacent.
Whilerule (1) merges two target phrases in a straight or-der, rule (2) merges in an inverted order.
Besidesthese two reordering rules, rule (3) is a lexical rulethat translates a source phrase f into a target phrasee.
This is exactly a bilingual phrase used in conven-tional phrase-based systems.An ITG derivation, which consists of a sequenceof production rules, explains how a sentence pair isgenerated simultaneously.
Figure 1 shows an ITGderivation for a Chinese sentence and its Englishtranslation.
We distinguish between two types ofblocks:1. atomic blocks: blocks generated by applyinglexical rules,2.
composed blocks: blocks generated by apply-ing reordering rules.In Figure 1, the sentence pair is segmented intofive atomic blocks:X0,3,0,3 : wo you yi ge?
I have aX3,5,5,6 : cong mei you?
neverX5,8,6,8 : jian guo de?
seen beforeX8,10,3,5 : nv xing peng you?
female friendX10,11,8,9 : .?
.568(1) X0,11,0,9 ?
[X0,10,0,8, X10,11,8,9](2) X0,10,0,8 ?
[X0,3,0,3, X3,10,3,8](3) X0,3,0,3 ?
wo you yi ge / I have a(4) X3,10,3,8 ?
?X3,8,5,8, X8,10,3,5?
(5) X3,8,5,8 ?
[X3,5,5,6, X5,8,6,8](6) X3,5,5,6 ?
cong mei you / never(7) X5,8,6,8 ?
juan guo de / seen before(8) X8,10,3,5 ?
nv xing peng you/ female friend(9) X10,11,8,9 ?
.
/ .Figure 1: An ITG derivation for a Chinese sentence and its translation.
We useXi,j,k,l = ?fji , elk?
to represent a block.Our neural ITG reordering model first assigns vector space representations to single words and then produces vectorsfor phrases using recursive autoencoders, which form atomic blocks.
The atomic blocks are recursively merged intocomposed blocks, the vector space representations of which are produced by recursive autoencoders simultaneously.The neural classifier makes decisions at each node using the vectors of all its descendants.569where X3,5,5,6 indicates that the block consists of asource phrase spanning from position 3 to position 5(i.e., ?cong mei you?)
and a target phrase spanningfrom position 5 to position 6 (i.e., ?never?).
Moreformally, a block Xi,j,k,l = ?fji , elk?
is a pair of asource phrase f ji = fi+1 .
.
.
fj and a target phraseelk = ek+1 .
.
.
el.
Obviously, these atomic blocksare generated by lexical rules.Two blocks of which the source phrases are adja-cent can be merged into a larger one in two ways:concatenating the target phrases in a straight orderusing rule (1) or in an inverted order using rule (2).For example, atomic blocks X3,5,5,6 and X5,8,6,8 aremerged into a composed block X3,8,5,8 in a straightorder, which is further merged with an atomic blockX8,10,3,5 into another composed block X3,10,3,8 inan inverted order.
This process recursively proceedsuntil the entire sentence pair is generated.The major challenge of applying ITG to machinetranslation is to decide when to merge two blocksin a straight order and when in an inverted order.Therefore, the ITG reordering model can be seen asa two-category classifier P (o|X1, X2), where o ?
{straight, inverted}.A naive way is to assign fixed probabilities to tworeordering rules, which is referred to as flat modelby Xiong et al(2006):P (o|X1, X2) ={p o = straight1?
p o = inverted(4)The drawback of the flat model is ignoring theactual blocks being merged.
Intuitively, differentblocks should have different preferences betweenthe two orders.To alleviate this problem, Xiong et al(2006) pro-pose a maximum entropy (MaxEnt) classifier:P (o|X1, X2) =exp(?
?
h(o,X1, X2))?o?
exp(?
?
h(o?, X1, X2))(5)where h(?)
is a vector of features defined on theblocks and the order, ?
is a vector of feature weights.While MaxEnt is a flexible and powerful frame-work for including arbitrary features, feature engi-neering becomes a major challenge for the MaxEntclassifier.
Xiong et al(2006) find that boundarywords (i.e., the first and the last words in a string)are informative for predicting reordering.
Actually,Figure 2: A recursive autoencoder for multi-word strings.The example is adapted from (Socher et al 2011c).
Blueand grey nodes are original and reconstructed ones, re-spectively.it is hard to decide which internal words in a longcomposed blocks are representative and informa-tive.
Therefore, they only use boundary words asthe main features.However, it seems not enough to just considerboundary words and ignore all internal words whenmaking order predictions, especially for long sen-tences.1 Indeed, Xiong et al(2008) find that theMaxEnt classifier with boundary words as featuresis prone to make wrong predictions for long com-posed blocks.
As a result, they have to impose a hardconstraint to always prefer merging long composedblocks in a monotonic way.Therefore, it is important to consider more thanboundary words to make more accurate reorderingpredictions.
We need a new mechanism to achievethis goal.2.2 Recursive Autoencoders2.2.1 Vector Space Representations for WordsIn neural networks, a natural language word isrepresented as a real-valued vector (Bengio et al2003; Collobert and Weston, 2008).
For example,we can use [0.1 0.8 0.4]T to represent ?female?
and1Strictly speaking, the ITG reordering model is not a phrasereordering model since phrase pairs are only the atomic blocks.Instead, it is defined to work on arbitrarily long strings becausecomposed blocks become larger and larger until the entire sen-tence pair is generated.570Figure 3: A neural ITG reordering model.
The binary classifier makes decisions based on the vector space representa-tions of the source and target sides of merging blocks.
[0.7 0.1 0.5]T to represent ?friend?.
Such vectorspace representations enable natural language wordsto be fed to neural networks as input.Formally, we denote each word as a vector x ?Rn.
These word vectors are then stacked into a wordembedding matrix L ?
Rn?|V |, where |V | is the vo-cabulary size.
Given a sentence that is an ordered listofmwords, each word has an associated vocabularyindex k into the word embedding matrix L that weuse to retrieve the word?s vector space representa-tion.
This look-up operation can be seen as a simpleprojection layer:xi = Lbk ?
Rn (6)where bk is a binary vector which is zero in all posi-tions except for the kth index.In Figure 1, we assume n = 3 for simplicity andcan retrieve vectors for Chinese and English wordsfrom two embedding matrices, respectively.2.2.2 Vector Space Representations forMulti-Word StringsTo apply neural networks to ITG-based transla-tion, it is important to generate vector space repre-sentations for atomic and composed blocks.For example, since the vector of ?female?
is[0.1 0.8 0.4]T and the vector of ?friend?
is[0.7 0.1 0.5]T , what is the vector of the phrase ?fe-male friend??
If we denote ?female friend?
as p(i.e., parent), ?female?
as c1 (i.e., the first child),and ?friend?
as c2 (i.e., the second child), this canbe done by applying a function f (1):p = f (1)(W (1)[c1; c2] + b(1)) (7)where [c1; c2] ?
R2n?1 is the concatenation of c1and c2, W (1) ?
Rn?2n is a parameter matrix, b(1) ?Rn?1 is a bias term, and f (1) is an element-wise ac-tivation function such as tanh(?
), which is used inour experiments.Note that the resulting vector for the parent is alsoan n-dimensional vector, e.g, [0.6 0.9 0.2]T .
Thesame neural network can be recursively applied totwo strings until the vector of the entire sentence isgenerated.
As ITG derivation builds a binary parsetree, the neural network can be naturally integratedinto CKY parsing.To assess how well the learned vector p representsits children, we can reconstruct the children in areconstruction layer:[c?1; c?2] = f(2)(W (2)p+ b(2)) (8)where c?1 and c?2 are the reconstructed children,W(2)is a parameter matrix for reconstruction, b(2) is a biasterm for reconstruction, and f (2) is an element-wiseactivation function, which is also set as tanh(?)
inour experiments.
Similarly, the same reconstructionneural network can be applied to each node in anITG parse.These neural networks are called recursive au-toencoders (Socher et al 2011c).
Figure 2 illus-trates an application of a recursive autoencoder to a571binary tree.
The blue and grey nodes are the originaland reconstructed nodes, respectively.
The autoen-coder is re-used at each node of the tree.
The bi-nary tree is composed of a set of triplets in the formof (p ?
c1 c2), where p is a parent vector and c1and c2 are children vectors of p. Each child can beeither an input word vector or a multi-word vector.Therefore, the tree in Figure 2 can be represented asthree triplets: (y1 ?
x1 x2), (y2 ?
y1 x3), and(y3 ?
y2 x4).In Figure 1, we use recursive autoencoders to gen-erate vector space representations for Chinese andEnglish phrases, which form the atomic blocks forfurther block merging.2.2.3 A Neural ITG Reordering ModelOnce the vectors for blocks are generated, it isstraightforward to introduce a neural ITG reorder-ing model.
As shown in Figure 3, the neural net-work consists of an input layer and a softmax layer.The input layer is composed of the vectors of thefirst source phrase, the first target phrase, the secondsource phrase, and the second target phrase.
Notethat all phrases in the same language use the thesame recursive autoencoder.
The softmax layer out-puts the probabilities of the two merging orders:P (o|X1, X2) =exp(g(o,X1, X2))?o?
exp(g(o?, X1, X2))(9)g(o,X1, X2) = f(W oc(X1, X2) + bo) (10)where o ?
{straight, inverted}, W o ?
R1?4nis a parameter matrix, bo ?
R is a bias term, andc(X1, X2) ?
R4n?1 is the concatenation of the vec-tors of the four phrases.3 TrainingThere are three sets of parameters in our recursiveautoencoders:1.
?L: word embedding matrix L for both sourceand target languages (Section 2.2.1);2.
?rec: recursive autoencoder parameter matricesW (1), W (2) and bias terms b(1), b(2) for bothsource and target languages (Section 2.2.2);3.
?reo: neural ITG reordering model parametermatrix W o and bias term bo (Section 2.2.3).All these parameters are learned automatically fromthe training data.
For clarity, we will use ?
to denoteall these parameters in the rest of the paper.For training word embedding matrix, there aretwo settings commonly used.
In the first setting,the word embedding matrix is initialized randomly.This works well in a supervised scenario, in whicha neural network updates the matrix in order to op-timize some task-specific objectives (Collobert etal., 2011; Socher et al 2011c).
In the second set-ting, the word embedding matrix is pre-trained us-ing an unsupervised neural language model (Bengioet al 2003; Collobert and Weston, 2008) with hugeamount of unlabeled data.
In this work, we prefer tothe first setting because the word embedding matri-ces can be trained to minimize errors with respect toreordering modeling.There are two kinds of errors involved1.
reconstruction error: how well the learnedvector space representations represent the cor-responding strings?2.
reordering error: how well the classifier pre-dicts the merging order?As described in Section 2.2.2, the input vectorc1 and c2 of a recursive autoencoder can be recon-structed using Eq.
8 as c?1 and c?2.
We use Euclideandistance between the input and the reconstructedvectors to measure the reconstruction error:Erec([c1; c2]; ?)
=12??
[c1; c2]?
[c?1; c?2]?
?2 .
(11)Given a sentence, there are exponentially manyways to obtain its vector space representation.
Notethat each way corresponds to a binary tree like Fig-ure 2.
To find a binary tree with minimal reconstruc-tion error, we follow Socher et al(2011c) to use agreedy algorithm.
Taking Figure 2 as an example,the greedy algorithm begins with computing the re-construction error Erec(?)
for each pair of consecu-tive vectors, i.e., Erec([x1;x2]; ?
), Erec([x2;x3]; ?
)and Erec([x3;x4]; ?).
Suppose Erec([x1;x2]; ?)
isthe smallest, the algorithm will replace x1 and x2with their vector representation y1 produced by therecursive autoencoder.
Then, the algorithm evalu-ates Erec([y1;x3]; ?)
and Erec([x3;x4]; ?)
and re-peats the above replacing steps until only one vector572remains.
Socher et al(2011c) find that the greedyalgorithm runs fast without significant loss in perfor-mance as compared with CKY-style algorithms.Given a training example set S = {ti =(oi, X1i , X2i )}, the average reconstruction error onthe source side on the training set is defined asErec,s(S; ?)
=1Ns?i?p?T ?R(ti,s)Erec([p.c1, p.c2]; ?
)(12)where T ?R(ti, s) denotes all the intermediate nodeson the source side in binary trees, Ns is the num-ber of these intermediate nodes, and p.ck is the kthchild vector of p. The average reconstruction erroron the target side, denoted by Erec,t(S; ?
), can becomputed in a similar way.Therefore, the reconstruction error is defined asErec(S; ?)
= Erec,s(S; ?)
+ Erec,t(S; ?).
(13)Given a training example ti = (oi, X1i , X2i ), weassume the probability distribution dti for its labelis [1, 0] when oi = straight, and [0, 1] when oi =inverted.
Then the cross-entropy error isEc(ti; ?)
= ?
?odti(o) log(P?
(o|X1, X2))(14)where o ?
{straight, inverted}.
As a result, thereordering error is defined asEreo(S; ?)
=1|S|?iEc(ti; ?).
(15)Therefore, the joint training objective function isJ = ?Erec(S; ?)+(1??
)Ereo(S; ?)+R(?)
(16)where ?
is a parameter used to balance the prefer-ence between reconstruction error and reordering er-ror, R(?)
is the regularizer and defined as 2R(?)
=?L2?
?L?2 +?rec2?
?rec?2 +?reo2?
?reo?2 .
(17)As Socher et al(2011c) stated, a naive way forlowering the reconstruction error is to make themagnitude of the hidden layer very small, which is2The bias terms b(1), b(2) and bo are not regularized.
We donot exclude them from the equation explicitly just for clarity.not desirable.
In order to prevent such behavior, wenormalize all the output vectors of the hidden layersto have length 1 in the same way as (Socher et al2011c).
Namely we set p = p||p|| after computing pas in Eq.
7, and c?1 =c?1||c?1||, c?2 =c?2||c?2||in Eq.
8.Following Socher et al(2011c), we use L-BFGSto estimate the parameters with respect to the jointtraining objective.
Given a set of parameters, weconstruct binary trees for all the phrases using thegreedy algorithm.
The derivatives for these fixedbinary trees can be computed via backpropagationthrough structures (Goller and Kuchler, 1996).4 Experiments4.1 Data PreparationWe evaluated our system on Chinese-English trans-lation.
The training corpus contains 1.23M sen-tence pairs with 32.1M Chinese words and 35.4MEnglish words.
We used SRILM (Stolcke, 2002)to train a 4-gram language model on the Xinhuaportion of the GIGAWORD corpus, which con-tains 398.6M words.
We used the NIST 2006 MTChinese-English dataset as the development set andNIST 2008 dataset as the test set.
The evaluationmetric is case-insensitive BLEU.
Because of the ex-pensive computational cost for training our neuralITG reordering model, only the reordering exam-ples extracted from about 1/5 of the entire paralleltraining corpus were used to train our neural ITG re-ordering model.For the neural ITG reordering model, we set thedimension of the word embedding vectors to 25 em-pirically, which is a trade-off between computationalcost and expressive power.
We use the early stop-ping principle to determine when to stop L-BFGS.The hyper-parameters ?, ?L, ?rec and ?reo are op-timized by random search (Bergstra and Bengio,2012).
As preliminary experiments show that classi-fication accuracy has a high correlation with BLEUscore, we optimize these hyper-parameters with re-spect to classification accuracy instead of BLEUto reduce computational cost.
We randomly select400,000 reordering examples as training set, 500 asdevelopment set, and another 500 as test set.
Thenumbers of straight and inverted reordering exam-ples in the development/test set are set to be equalto avoid biases.
We draw ?
uniformly from 0.05573System NIST 2006 (tune) NIST 2008maxent 30.40 23.75neural 31.61* 24.82*Table 1: BLEU scores on the NIST 2006 and 2008datasets.
*: significantly better (p < 0.01).
?maxent?denotes the baseline maximum entropy system and ?neu-ral?
denotes our recursive autoencoder system.length > = <[1, 10] 43 121 57[11, 20] 181 67 164[21, 30] 170 11 152[31, 40] 105 3 90[41, 50] 69 1 53[51, 119] 40 0 30Table 2: Number of sentences that our system has ahigher (>), equal (=) or lower (<) sentence-level BLEU-4 score on the NIST 2008 dataset.to 0.3, and ?L, ?rec, ?reo exponentially from 10?8to 10?2.
We use the following hyper-parameters inour experiments: ?
= 0.11764, ?L = 7.59 ?
10?5,?rec = 1.30?
10?5 and ?reo = 3.80?
10?4.
3The baseline system is a re-implementation of(Xiong et al 2006).
Our system is different from thebaseline by replacing the MaxEnt reordering modelwith a neural model.
Both the systems have the samepruning settings: the threshold pruning parameter isset to 0.5 and the histogram pruning parameter to40.
For minimum-error-rate training, both systemsgenerate 200-best lists.4.2 MT EvaluationTable 1 shows the case-insensitive BLEU-4 scoresof the baseline system and our system on the devel-opment and test sets.
Our system outperforms thebaseline system by 1.21 BLEU points on the de-velopment set and 1.07 on the test set.
Both thedifferences are statistically significant at p = 0.01level (Riezler and Maxwell, 2005).Table 2 shows the number of sentences that oursystem has a higher (>), equal (=) or lower (<)BLEU score on the NIST 2008 dataset.
We find thatour system is superior to the baseline system for long3The choice of ?
is very important for achieving high BLEUscores.
We tried a number of intervals and found that the clas-sification accuracy is most stable in the interval [0.100,0.125].5 10 15 20 25 30 351909510088 LengthAccuracy (%)neuralmaxentFigure 4: Comparison of reordering classification accu-racies between the MaxEnt and neural classifiers overvarying phrase lengths.
?Length?
denotes the sum of thelengths of two source phrases in a reordering example.Our classifier (neural) outperforms the MaxEnt classi-fier (maxent) consistently, especially for predicting long-distance reordering.# of examples NIST 2006 (tune) NIST 2008100,000 30.88 23.78200,000 30.75 23.89400,000 30.80 24.35800,000 31.01 24.456,004,441 31.61 24.82Table 3: The effect of reordering training data size onBLEU scores.
The BLEU scores rise with the increase oftraining data size.
Due to the computational cost, we onlyused 1/5 of the entire bilingual corpus to train our neuralreordering model.sentences.Figure 4 compares classification accuracies of theneural and MaxEnt classifiers.
?Length?
denotes thesum of the lengths of two source phrases in a re-ordering example.
For each length, we randomly se-lect 200 unseen reordering examples to calculate theclassification accuracy.
Our classifier outperformsthe baseline consistently, especially for long com-posed blocks.Xiong et al(2008) find that the performance ofthe baseline system can be improved by forbiddinginverted reordering if the phrase length exceeds apre-defined distortion limit.
This heuristic increasesthe BLEU score of the baseline system significantlyto 24.46 but is still significantly worse (p < 0.05)than our system without the heuristic.
We find thatimposing this heuristic fails to improve our system574cluster 1 cluster 2 cluster 3 cluster 4 cluster 51.18 works for alternative duties these people who of the threeaccessibility verify on one-day conference the reasons why on the fundamentalwheelchair tunnels from armed groups the story of how over the entirecandies transparency in chinese language works the system which through its owncough opinion at eating habits the trend towards with the bestTable 4: Words and phrases that are close in the Euclidean space.
The words and phrases in the same cluster havesimilar behaviors from a reordering point of view rather than relatedness, suggesting that the vector representationsproduced by the recursive autoencoders are helpful for capturing reordering regularities.significantly.
One possible reason is that there islimited room for improvement as our system makesfewer wrong predictions for long composed blocks.The above results suggest that our system does gobeyond using boundary words and make a better useof the merging blocks by using vector space repre-sentations.Table 3 shows the effect of training dataset sizeon BLEU scores.
We find that BLEU scores on boththe development and test sets rise with the increaseof the training dataset size.
As the training process isvery time-consuming, only the reordering examplesextracted from 1/5 of the entire parallel training cor-pus are used in our experiments to train our model.Obviously, with more efficient training algorithms,making full use of all the reordering examples ex-tracted from the entire corpus will result in betterresults.
We leave this for future work.4.3 Qualitative Analysis on VectorRepresentationsTable 4 shows a number of words and phrases thatare close (measured by Euclidean distance) in then-dimensional space.
We randomly select about370K target side phrases used in our experimentsand cluster them into 983 clusters using k-means al-gorithm (MacQueen, 1967).
The distance betweentwo phrases are measured by the Euclidean distancebetween their vector representations.
As shown inTable 4, cluster 1 mainly consists of nouns, clus-ter 2 mainly contains verb/noun+preposition struc-tures, cluster 3 contains compound phrases, cluster4 consists of phrases which should be followed bya clause, and cluster 5 mainly contains the begin-ning parts of prepositional phrases that tend to befollowed by a noun phrase or word.
We find thatthe words and phrases in the same cluster have sim-ilar behaviors from a reordering point of view ratherthan relatedness.
This indicates that the vector rep-resentations produced by the recursive autoencodersare helpful for capturing reordering regularities.5 ConclusionWe have presented an ITG reordering classifierbased on recursive autoencoders.
As recursive au-toencoders are capable of producing vector spacerepresentations for arbitrary multi-word strings indecoding, our neural ITG system achieves an ab-solute improvement of 1.07 BLEU points over thebaseline on the NIST 2008 Chinese-English dataset.There are a number of interesting directions wewould like to pursue in the near future.
First, re-placing the MaxEnt classifier with a neural one re-defines the conditions for risk-free hypothesis re-combination.
We find that the number of hypothe-ses that can be recombined reduces in our system.Therefore, we plan to use forest reranking (Huang,2008) to alleviate this problem.
Second, it is in-teresting to follow Socher et al(2013) to combinelinguistically-motivated labels with recursive neuralnetworks.
Another problem with our system is thatthe decoding speed is much slower than the baselinesystem because of the computational overhead intro-duced by RAEs.
It is necessary to investigate moreefficient decoding algorithms.
Finally, it is possibleto apply our method to other phrase-based and evensyntax-based systems.AcknowledgmentsThis research is supported by the 863 Program un-der the grant No.
2012AA011102, by the BoeingTsinghua Joint Research Project on Language Pro-cessing (Agreement TBRC-008-SDB-2011 Phase 3575(2013)), by the Singapore National Research Foun-dation under its International Research Centre @Singapore Funding Initiative and administered bythe IDM Programme Office, and by a Research FundNo.
20123000007 from Tsinghua MOE-MicrosoftJoint Laboratory.
Many thanks go to Chunyang Liuand Chong Kuang for their great help for setting upthe computing platform.
We also thank Min-YenKan, Meng Zhang and Yu Zhao for their insightfuldiscussions.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion models for statistical machine translation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages529?536, Sydney, Australia, July.Yoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155, March.James Bergstra and Yoshua Bengio.
2012.
Randomsearch for hyper-parameter optimization.
The Jour-nal of Machine Learning Research, 13(1):281?305,February.Arianna Bisazza and Marcello Federico.
2012.
Modi-fied distortion matrices for phrase-based statistical ma-chine translation.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 478?487, Jeju Is-land, Korea, July.Antoine Bordes, Jason Weston, Ronan Collobert, andYoshua Bengio.
2011.
Learning structured embed-dings of knowledge bases.
In Proceedings of theTwenty-Fifth AAAI Conference on Artificial Intelli-gence, pages 301?306.Antoine Bordes, Xavier Glorot, Jason Weston, andYoshua Bengio.
2012.
Joint learning of words andmeaning representations for open-text semantic pars-ing.
In International Conference on Artificial Intelli-gence and Statistics, pages 127?135.Colin Cherry.
2013.
Improved reordering for phrase-based translation using sparse features.
In Proceed-ings of the 2013 Conference of the North AmericanChapter of the Association for Computational Linguis-tics: Human Language Technologies, pages 22?31,Atlanta, Georgia, June.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceed-ings of the 25th International Conference on MachineLearning, pages 160?167.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.Yang Feng, Haitao Mi, Yang Liu, and Qun Liu.
2010.
Anefficient shift-reduce decoding algorithm for phrased-based machine translation.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics: Posters, pages 285?293, Beijing, China, August.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 848?856, Honolulu, Hawaii, October.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Proceed-ings of the 28th International Conference on MachineLearning (ICML-11), pages 513?520.Christoph Goller and Andreas Kuchler.
1996.
Learningtask-dependent distributed representations by back-propagation through structure.
In Proceedings of 1996IEEE International Conference on Neural Networks(Volume:1), volume 1, pages 347?352.Spence Green, Michel Galley, and Christopher D. Man-ning.
2010.
Improved models of distortion cost forstatistical machine translation.
In Proceedings of Hu-man Language Technologies: The 2010 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, pages 867?875,Los Angeles, California, June.Karl Moritz Hermann and Phil Blunsom.
2013.
The roleof syntax in vector space models of compositional se-mantics.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 894?904, Sofia, Bulgaria,August.Eric Huang, Richard Socher, Christopher Manning, andAndrew Ng.
2012.
Improving word representationsvia global context and multiple word prototypes.
InProceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics (Volume 1: LongPapers), pages 873?882, Jeju Island, Korea, July.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In Proceedings of the46th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 586?594, Columbus, Ohio, June.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25(4):607?615, December.576Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology-Volume 1, pages48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Companion Volume Pro-ceedings of the Demo and Poster Sessions, pages 177?180, Prague, Czech Republic, June.James MacQueen.
1967.
Some methods for classifica-tion and analysis of multivariate observations.
In Pro-ceedings of the Fifth Berkeley Symposium on Mathe-matical Statistics and Probability, volume 1.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine transla-tion.
Computational Linguistics, 30(4):417?449.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics: HLT-NAACL 2004: Main Proceedings, pages 161?168,Boston, Massachusetts, USA, May.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, pages 57?64, Ann Arbor, Michigan, June.Richard Socher, Eric H. Huang, Jeffrey Pennin, An-drew Y. Ng, and Christopher D. Manning.
2011a.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Proceedings of Advancesin Neural Information Processing Systems 24, pages801?809.Richard Socher, Cliff C. Lin, Andrew Y. Ng, and Christo-pher D. Manning.
2011b.
Parsing natural scenes andnatural language with recursive neural networks.
InProceedings of the 26th International Conference onMachine Learning (ICML), pages 129?136.Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011c.Semi-supervised recursive autoencoders for predictingsentiment distributions.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, pages 151?161, Edinburgh, Scot-land, UK., July.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic compositional-ity through recursive matrix-vector spaces.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 1201?1211, Jeju Island, Korea, July.Richard Socher, John Bauer, Christopher D. Manning,and Ng Andrew Y.
2013.
Parsing with compositionalvector grammars.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics (Volume 1: Long Papers), pages 455?465, Sofia,Bulgaria, August.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing,vol.
2, pages 901?904, September.Christoph Tillman.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofthe Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics: HLT-NAACL 2004: Short Pa-pers, pages 101?104, Boston, Massachusetts, USA,May.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Maxi-mum entropy based phrase reordering model for sta-tistical machine translation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 521?528, Sydney,Australia, July.Deyi Xiong, Min Zhang, Aiti Aw, Haitao Mi, Qun Liu,and Shouxun Lin.
2008.
Refinements in BTG-basedstatistical machine translation.
In Proceedings of theThird International Joint Conference on Natural Lan-guage Processing: Volume-I, pages 505?512.Deyi Xiong, Min Zhang, Aiti Aw, and Haizhou Li.
2010.Linguistically annotated reordering: Evaluation andanalysis.
Computational Linguistics, 36(3):535?568,September.Richard Zens, Hermann Ney, Taro Watanabe, and Ei-ichiro Sumita.
2004.
Reordering constraints forphrase-based statistical machine translation.
In Pro-ceedings of the 20th International Conference onComputational Linguistics, pages 205?211, Geneva,Switzerland, Aug 23?Aug 27.577
