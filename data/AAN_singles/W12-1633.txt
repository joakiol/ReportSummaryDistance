Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 237?246,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsA Unified Probabilistic Approach to Referring ExpressionsKotaro Funakoshi Mikio NakanoHonda Research Institute Japan Co., Ltd.8-1 Honcho, Wako,Saitama 351-0188, Japan{funakoshi,nakano}@jp.honda-ri.comTakenobu Tokunaga Ryu IidaTokyo Institute of Technology2-12-1 Oookayama, Meguro,Tokyo 152-8550, Japan{take,ryu-i}@cl.cs.titech.ac.jpAbstractThis paper proposes a probabilistic approachto the resolution of referring expressions fortask-oriented dialogue systems.
The approachresolves descriptions, anaphora, and deixis ina unified manner.
In this approach, the notionof reference domains serves an important roleto handle context-dependent attributes of enti-ties and references to sets.
The evaluation withthe REX-J corpus shows promising results.1 IntroductionReferring expressions (REs) are expressions in-tended by speakers to identify entities to hearers.REs can be classified into three categories: descrip-tions, anaphora, and deixis; and, in most cases,have been studied within each category and with anarrowly focused interest.
Descriptive expressions(such as ?the blue glass on the table?)
exploit at-tributes of entities and relations between them todistinguish an entity from the rest.
They are wellstudied in natural language generation, e.g., (Daleand Reiter, 1995; Krahmer et al, 2003; Dale and Vi-ethen, 2009).
Anaphoric expressions (such as ?it?
)refer to entities or concepts introduced in the pre-ceding discourse and are studied mostly on textualmonologues, e.g., (Kamp and Reyle, 1993; Mitkov,2002; Ng, 2010).
Deictic (exophoric) expressions(such as ?this one?)
refer to entities outside the pre-ceding discourse.
They are often studied focusingon pronouns accompanied with pointing gestures inphysical spaces, e.g., (Gieselmann, 2004).Dialogue systems (DSs) as natural human-machine (HM) interfaces are expected to han-dle all the three categories of referring expres-sions (Salmon-Alt and Romary, 2001).
In fact, thethree categories are not mutually exclusive.
To beconcrete, a descriptive expression in conversation iseither deictic or anaphoric.
It is, however, not easy totell whether a RE is deictic or anaphoric in advanceof a resolution (regardless of whether the RE is de-scriptive or not).
Therefore, we propose a generalunified approach to the above three kinds of REs.We employ a Bayesian network (BN) to model aRE.
Dealing with continuous information and vaguesituations is critical to handle real world problems.Probabilistic approaches enable this for reference re-solvers.
Each BN is dynamically constructed basedon the structural analysis result of a RE and contex-tual information available at that moment.
The BNis used to estimate the probability with which thecorresponding RE refers to an entity.One of the two major contributions of this paper isour probabilistic formulation that handles the abovethree kinds of REs in a unified manner.
PreviouslyIida et al (2010) proposed a quantitative approachthat handles anaphoric and deictic expressions in aunified manner.
However it lacks handling of de-scriptive expressions.
Our formulation subsumesand extends it to handle descriptive REs.
So far, nopreviously proposed method for reference resolutionhandles all three types of REs.The other contribution is bringing referencedomains into that formulation.
Reference do-mains (Salmon-Alt and Romary, 2000) are sets ofreferents implicitly presupposed at each use of REs.By considering them, our approach can appropri-ately interpret context-dependent attributes.
In ad-dition, by treating a reference domain as a referent,REs referring to sets of entities are handled, too.
Asfar as the authors know, this work is the first thattakes a probabilistic approach to reference domains.2371.1 Reference domainsFirst, we explain reference domains concretely.
Ref-erence domains (RDs) (Salmon-Alt and Romary,2000; Salmon-Alt and Romary, 2001; Denis, 2010)are theoretical constructs, which are basically setsof entities presupposed at each use of REs.
RDs inthe original literature are not mere sets of entitiesbut mental objects equipped with properties suchas type, focus, or saliency and internally structuredwith partitions.
In this paper, while we do not ex-plicitly handle partitions, reference domains can benested as an approximation of partitioning, that is,an entity included in a RD is either an individual en-tity or another RD.
Each RD d has its focus and de-gree of saliency (a non-negative real number).
Here-after, two of them are denoted as foc(d) and sal(d)respectively.
RDs are sorted in descending order ac-cording to saliency.We illustrate reference domains with figure 1.
Itshows a snapshot of solving a Tangram puzzle (thepuzzle and corpus are explained in section 3.1).
RDsare introduced into our mental spaces either linguis-tically (by hearing a RE) or visually (by observinga physical situation).
If one says ?the two big tri-angles?
in the situation shown in figure 1, we willrecognize a RD consisting of pieces 1 and 2.
If weobserve one moves piece 1 and attaches it to piece2, we will perceptually recognize a RD consistingof pieces 1, 2, and 6 due to proximity (Tho?risson,1994).
In a similar way, a RD consisting of pieces 5and 7 also can be recognized.
Hereafter, we indicatea RD with the mark @ with an index, and denoteits elements by enclosing them with [ ].
E.g., @1 =[1, 2], @2 = [1, 2, 6], @3 = [5, 7].
The focused en-tity is marked by ?*?.
Thus, foc([1?, 2]) = 1.The referent of a RE depends on which RD is pre-supposed.
That is, if one presupposes @1 or @2, thereferent of ?the right piece?
should be piece 1.
Ifone presupposes @3, the referent of the same REshould be piece 5.
This is the context-dependencymentioned above.Previous work on RDs (Salmon-Alt and Romary,2000; Salmon-Alt and Romary, 2001; Denis, 2010)employ not probabilistic but formal approaches.1.2 Probabilistic approaches to REsHere, previous probabilistic approaches to REs areexplained and differences between ours and theirsFigure 1: Tangram puzzle.
(The labels 1 to 7 are for il-lustration purposes and not visible to participants.
)are highlighted.
Bayesian networks (Pearl, 1988;Jensen and Nielsen, 2007) have been not often butoccasionally applied to problems in natural languageprocessing/computational linguistics since (Char-niak and Goldman, 1989).
With regard to REs,Burger and Connolly (1992) proposed a BN special-ized for anaphora resolution.
Weissenbacher (2005;2007) proposed a BN for the resolution of non-anaphoric ?it?
and also a BN for the resolution ofpronominal anaphora.
They used pre-defined fixedBNs for their tasks while our approach dynamicallytailors a BN for each RE.Cho and Maida (1992) and Roy (2002) adoptednot exactly BNs but similar probabilistic approachesfor reference resolution and generation respectively.However, their foci are only on descriptions.Lison et al (2010) proposed an approach usingMarkov logic networks (MLNs) (Richardson andDomingos, 2006) to reference resolution.
Theydealt with only deictic and descriptive REs.
Eventhough MLNs are also a probabilistic framework, itis difficult for DS developers to provide quantitativedomain knowledge needed to resolve REs becauseMLNs accept domain knowledge in the form of for-mal logic rules with weights, which must be deter-mined globally.
In contrast, BNs are more flexibleand easy in providing quantitative knowledge to DSsin the form of conditional probability tables, whichcan be determined locally.As just described, there are several probabilis-tic approaches to REs but none of them incorpo-rates reference domains.
In the next section, we in-troduce our REBNs (Referring Expression BayesianNetworks), a novel Bayesian network-based model-ing approach to REs that incorporates reference do-mains.238W C X DFigure 2: WCXD fundamental structure.2 Bayesian Network-based Modeling ofReferring ExpressionsEach REBN is dedicated for a RE in the context atthe moment.
Its structure is determined by the syn-tactic and semantic information in the RE and prob-ability tables are determined by the context.2.1 StructuresFigure 2 shows the fundamental network structureof REBNs.
We call this structure WCXD.
The fournodes (random variables)W ,C,X , andD representan observed word, the concept denoted by the word,the referent of the RE, and the presupposed RD, re-spectively.
Here, a word means a lexical entry inthe system dictionary defined by the DS developer(concept dictionary; section 3.2.1).Each REBN is constructed by modifying or mul-tiply connecting the WCXD structure as shown infigures 3 and 4.
Figure 3 shows the network for REsindicating one referent such as ?that table.?
EachWinode has a corresponding word wi.
Figure 4 showsthe network for REs indicating two referents such as?his table.?
We call the class of the former REs s-REX (simple Referring EXpression) and the class ofthe latter REs c-REX (compound Referring EXpres-sion).
Although REBNs have the potential to dealwith c-REX, hereafter we concentrate on s-REX be-cause the page space is limited and the corpus usedfor evaluation contains very few c-REX instances.Although, in section 1, we explained that (Iida etal., 2010) handles anaphoric and deictic expressionsin a unified manner, it handles anaphora to instancesonly and does not handle that to concepts.
There-fore, it cannot satisfactorily resolve such an expres-sion ?Bring me the red box, and the blue one, too.
?Here, ?one?
does not refer to the physical referentof ?the red box?
but refers to the concept of ?box?.TheC nodes will enable handling of such referencesto concepts.
This is one of the important features ofREBNs but will be investigated in future work.W1C1X DW2C2Figure 3: BN for two-word REs indicating one referent.W1C1X1D1W2C2X2D2Figure 4: BN for two-word REs indicating two referents.2.2 Domains of random variablesA REBN for an s-REX instance of N wordshas 2N + 2 discrete random variables:W1, .
.
.
,WN , C1, .
.
.
, CN , X , and D. The do-main of each variable depends on the correspondingRE and the context at the moment.
Here, D(V )denotes the domain of a random variable V .D(Wi) contains the corresponding observed wordwi and a special symbol ?
that represents other pos-sibilities, i.e., D(Wi) = {wi,?}.
Each Wi has acorresponding node Ci.D(Ci) containsM concepts that can be expressedby wi and a special concept ?
that represents otherpossibilities, i.e., D(Ci) = {c1i , .
.
.
, cMi ,?}.
cji(j = 1 .
.
.M ) are looked up from the concept dic-tionary (see section 3.2.1, table 2).D(D) contains L + 1 RDs recognized up to thatpoint in time, i.e., D(D) = {@0,@1, .
.
.
,@L}.
@0is the ground domain that contains all the individ-ual entities to be referred to in a dialogue.
At thebeginning of the dialogue, D(D) = {@0}.
OtherL RDs are incrementally added in the course of thedialogue.D(X) contains all the possible referents, i.e., Kindividual entities and L + 1 RDs.
Thus, D(X) ={x1, .
.
.
, xK ,@0, .
.
.
,@L}.
Including RDs enableshandling of references to sets.Then reference resolution is formalized as below:x?
= argmaxx?D(X)P (X = x|W1 = w1, .
.
.
,WN = wN ).
(1)P (X|W1, .
.
.
,WN ) is obtained by marginalizingthe joint probabilities that are computed with theprobability tables described in the next subsection.2392.3 Probability tablesProbability distributions are given as (conditional)probability tables since all the random variablesused in a REBN are discrete.
Here, four types ofprobability tables used by REBNs are described.2.3.1 P (Wi|Ci, X)P (Wi = w|Ci = c,X = x) is the probability thata hearer observes w from c and x which the speakerintends to indicate.In most cases, Wi does not depend on X , i.e.,P (Wi|Ci, X) ?
P (Wi|Ci).
X is, however, nec-essary to handle individualized terms (names).There are several conceivable ways of probabil-ity assignment.
One simple way is: for each cji ,P (W = wi|C = cji ) = 1/T, P (W = ?|C =cji ) = (T ?
1)/T , and for ?, P (W = wi|C =?)
= ", P (W = ?|C = ?)
= 1 ?
".
Here T is thenumber of possible words for cji . "
is a predefinedsmall number such as 10?8.
We use this assignmentin the evaluation.2.3.2 P (Ci|X,D)P (Ci = c|X = x,D = d) is the probability thatconcept c is chosen from D(Ci) to indicate x in d.The developers of DSs cannot provideP (Ci|X,D) in advance because D(Ci) is context-dependent.
Therefore, we take an approach ofcomposing P (Ci|X = x,D = d) from R(cji , x, d)(cji ?
D(Ci)\{?}).
Here R(cji , x, d) is the rele-vancy of concept cji to referent x with regard to d,and 0 ?
R(cji , x, d) ?
1.
1 means full relevancyand 0 means no relevancy.
0.5 means neutral.
Forexample, a concept BOX will have a high relevancyto a suitcase such as 0.8 but a concept BALL willhave a low relevancy to the suitcase such as 0.1.If x is not in d, R(cji , x, d) is 0.
Algorithm 1in appendix A shows an algorithm to composeP (Ci|X = x,D = d) from R(cji , x, d).
Concept?
will be assigned a high probability if none ofcji ?
D(Ci)\{?}
has a high relevancy to x.If cji is static,1 R(cji , x, d) is numerically given inadvance in the form of a table.
If not static, it is im-plemented as a function by the DS developer, that is,R(cji , x, d) = fcji (x, d, I).
Here I is all the informa-tion available from the DS.1Whether a concept is static or not depends on each DS.For example, given a situation such as shown infigure 1, the relevancy function of a positional con-cept LEFT (suppose a RE such as ?the left piece?
)can be implemented as below:fLEFT(x, d, I) = (ux ?
ur)/(ul ?
ur).
(2)Here, ux, ul and ur are respectively the horizontalcoordinates of x, the leftmost piece in d, and therightmost piece in d, which are obtained from I .
Ifx is a RD, the relevancy is given as the average ofentities included in the RD.2.3.3 P (X|D)P (X = x|D = d) is the probability that entity xin RD d is referred to, which is estimated accordingto the contextual information at the time the corre-sponding RE is uttered but irrespective of attributiveinformation in the RE.
The contextual informationincludes the history of referring so far (discourse)and physical statuses such as the gaze of the referrer(situation).
We call P (X = x|D = d) the predic-tion model.The prediction model can be constructed by us-ing a machine learning-based method.
We use aranking-based method (Iida et al, 2010).
The scoreoutput by the method is input into the standard sig-moid function and normalized to be a probability.
Ifx is not in d, P (X = x|D = d) is 0.2.3.4 P (D)P (D = d) is the probability that RD d is presup-posed at the time the RE is uttered.
We cannot col-lect data to estimate this probabilistic model becauseRDs are implicit.
Therefore, we examine three a pri-ori approximation functions based on the saliency ofd.
Saliency is proportional to recency.2Uniformmodel This model ignores saliency.
Thisis introduced to see the importance of saliency.P (D = d) = 1/|D(D)| (3)Linear model This model distributes probabilitiesin proportion to saliency.
This is an analogy of themethod used in (Denis, 2010).P (D = d) = sal(d)?d?
?D(D) sal(d?
)(4)2Assignment of saliency is described in section 3.2.3.240Exponential model This model puts emphasis onrecent RDs.
This function is so called soft-max.P (D = d) = exp(sal(d))?d?
?D(D) exp(sal(d?
))(5)3 Experimental EvaluationWe evaluated the potential of the proposed frame-work by using a situated human-human (HH) dia-logue corpus.3.1 CorpusWe used the REX-J Japanese referring expressioncorpus (Spanger et al, 2010).
The REX-J corpusconsists of 24 HH dialogues in each of which twoparticipants solve a Tangram puzzle of seven pieces(see figure 1).
The goal of the puzzle is combiningseven pieces to form a designated shape (such as aswan).
One of two subjects takes the role of opera-tor (OP) and the other takes the role of solver (SV).The OP can manipulate the virtual puzzle pieces dis-played on a PC monitor by using a computer mousebut does not know the goal shape.
The SV knowsthe goal shape but cannot manipulate the pieces.
Thestates of the pieces and the mouse cursor operated bythe OP are shared by the two subjects in real time.Thus, the two participants weave a collaborative dia-logue including many REs to the pieces.
In additionto REs, the positions and directions of the pieces, theposition of the mouse cursor, and the manipulationby the OP were recorded with timestamps and theIDs of relevant pieces.3.1.1 AnnotationEach RE is annotated with its referent(s) as shownin table 1.
The 1st RE okkiisankaku3 big triangle ?abig triangle?
in the table is ambiguous and refers toeither piece 1 or 2.
The 7th and 8th REs refer tothe set of pieces 1 and 2.
The other REs refer to anindividual piece.To skip the structural analysis of REs to avoidproblems due to errors in such analysis, we haveadditionally annotated the corpus with intermediatestructures, from which REBNs are constructed.
Be-cause we focus on s-REX only in this paper, the3Words are not separated by white spaces in Japanese.intermediate structures are straightforward:4 paren-thesized lists of separated words as shown in ta-ble 1.
The procedure to generate a REBN of s-REXfrom such an intermediate structure is also straight-forward and thus it is not explained due to the pagelimitation.3.2 ImplementationsWe use BNJ5 for probabilistic computation.
Herewe describe the implementations of resources andprocedures that are more or less specific to the taskdomain of REX-J.3.2.1 Concept dictionaryTable 2 shows an excerpt of the concept dictio-nary defined for REX-J.
We manually defined 40concepts by observing the dialogues.3.2.2 Static relevancy table and relevancyfunctionsFor 13 concepts out of 40, their relevancy valueswere manually determined by the authors.
Table 3shows an excerpt of the static relevancy table definedfor the seven pieces shown in figure 1.
TRI is rele-vant only to pieces 1 to 5, and SQR is relevant onlyto pieces 6 and 7 but is not totally relevant to piece 7because it is not a square in a precise sense.
FIG isequally but not very relevant to all the pieces,6For the remaining 27 concepts, we implementedrelevancy functions (see appendix B).3.2.3 Updating the list of RDsIn our experiment, REs are sequentially resolvedfrom the beginning of each dialogue in the corpus.In the course of resolution, RDs are added into a listand updated by the following procedure.
RDs aresorted in descending order according to saliency.At each time of resolution, we assume that all theprevious REs are correctly resolved.
Therefore, af-ter each time of resolution, if the correct referent ofthe last RE is a set, we add a new RD equivalentto the set into the list of RDs, unless the list con-tains another equivalent RD already.
In either case,the saliency of the RD equivalent to the set is set to?+1 unless the RD is at the head of the list already.4In the case of c-REX, graph-like structures are required.5http://bnj.sourceforge.net/6This is because concept FIG in REX-J is usually used torefer to not a single piece but a shaped form (combined pieces).241D-ID Role Start End Referring expression Referents Intermediate structure0801 SV 17.345 18.390 okkiisankaku big triangle 1 or 2 (okkii sankaku)0801 SV 20.758 21.368 sore it 1 (sore)0801 SV 23.394 24.720 migigawanookkiisankaku right big triangle 1 (migigawano okkii sankaku)0801 SV 25.084 25.277 kore this 1 (kore)0801 SV 26.512 26.671 sono that 1 (sono)0801 SV 28.871 29.747 konookkiisankaku this big triangle 2 (kono okkii sankaku)0801 OP 46.497 48.204 okkinasankakkei big triangle 1, 2 (okkina sankakkei)0801 OP 51.958 52.228 ryo?ho?
both 1, 2 (ryo?ho?)?D-ID?
means dialogue ID.
?Start?
and ?End?
mean the end points of a RE.Table 1: Excerpt of the corpus annotation (w/ English literal translations).Concept WordsTRI triangle, right triangleSQR quadrate, square, regular tetragonFIG figure, shapeTable 2: Dictionary (excerpted and translated in English).Concept Relevancy values by piece(1) (2) (3) (4) (5) (6) (7)TRI 1 1 1 1 1 0 0SQR 0 0 0 0 0 1 0.8FIG 0.3 0.3 0.3 0.3 0.3 0.3 0.3Table 3: Static relevancy table.Here, ?
is the largest saliency value in the list at themoment (the saliency value of the head RD).Before each time of resolution, we check whetherthe piece that is most recently manipulated after theprevious RE constitutes a perceptual group by usingthe method explained in section 3.2.4 at the onsettime of the target RE.
If such a group is recognized,we add a new RD equivalent to the recognized groupunless the list contains another equivalent RD.
In ei-ther case, the saliency of the RD equivalent is set to?+1 unless the RD is at the head of the list already,and the focus of the equivalent RD is set to the mostrecently manipulated piece.When a new RD@m is added to the list, a comple-mentary RD @n and a subsuming RD @l are also in-serted just after @m in the list.
Here, @n = @0\@mand @l = [@m?,@n].
This operation is required tohandle a concept REST, e.g., ?the remaining pieces.
?3.2.4 Perceptual groupingThere is a generally available method of simulatedperceptual grouping (Tho?risson, 1994).
It workswell in a spread situation such as shown in figure 1but tends to produce results that do not match ourintuition when pieces are tightly packed at the endof a dialogue.
Therefore, we adopt a simple methodthat recognizes a group when a piece is attached toanother.
This method is less general but works sat-isfactorily in the REX-J domain due to the nature ofthe Tangram puzzle.3.2.5 Ranking-based prediction modelAs mentioned in section 2.3.3, a ranking-basedmethod (Iida et al, 2010) using SVMrank (Joachims,2006) was adopted for constructing the predictionmodel P (X|D).
This model ranks entities accord-ing to 16 binary features such as whether the tar-get entity is previously referred to (a discourse fea-ture), whether the target is under the mouse cursor(a mouse cursor feature), etc.7When a target is a set (i.e., a RD), discourse fea-tures for it are computed as in the case of a piece;meanwhile, mouse cursor features are handled in adifferent manner.
That is, if one of the group mem-bers meets the criterion of a mouse cursor feature,the group is judged as meeting the criterion.In (Iida et al, 2010), preparing different modelsfor pronouns and non-pronouns achieved better per-formance.
Therefore we trained two linear kernelSVM models for pronouns and non-pronouns withthe 24 dialogues.3.3 ExperimentWe used the 24 dialogues for evaluation.8 As men-tioned in section 2.1, we focused on s-REX.
These24 dialogues contain 1,474 s-REX instances and 28c-REX instances.
In addition to c-REX, we ex-cluded REs mentioning complicated concepts, forwhich it is difficult to implement relevancy func-tions in a short time.9 After excluding those REs,7Following the results shown in (Iida et al, 2010), we didnot use the 6 manipulation-related features (CO1 .
.
.
CO6).8We used the same data to train the SVM-rank models.
Thisis equivalent to assuming that we have data large enough to sat-urate the performance of the prediction model.9Mostly, those are metaphors such as ?neck?
and conceptsrelated to operations such as ?put.?
For example, although242P (D) model Most-recent Mono-domain Uniform Linear ExponentialCategory Single Plural Total Single Plural Total Single Plural Total Single Plural Total Single Plural Totalw/o S/P info.
42.4 28.8 40.0 77.5 47.3 73.3 77.1 40.6 72.0 78.3 45.1 73.7 76.2 48.4 72.3w/ S/P info.
44.3 35.4 42.7 84.8 58.8 81.2 84.4 55.0 80.3 85.6 61.0 82.1 83.4 68.1 81.3Table 4: Results of reference resolution (Accuracy in %).1,310 REs were available.
Out of the 1,310 REs, 182REs (13.9%) refers to sets, and 612 REs (46.7%) aredemonstrative pronouns such as sore ?it.
?3.3.1 SettingsWe presupposed the following conditions.Speaker role independence: We assumed REsare independent of speaker roles, i.e., SV and OP.All REs were mixed and processed serially.Perfect preprocessing and past information:As mentioned in sections 3.1.1 and 3.2.3, we as-sumed that no error comes from preprocessing in-cluding speech recognition, morphological analysis,and syntactic analysis;10 and all the correct referentsof past REs are known.11No future information: In HH dialogue, some-times information helpful for resolving a RE is pro-vided after the RE is uttered.
We, however, do notconsider such future information.Numeral information: Many languages includ-ing English grammatically require indication of nu-meral distinctions by using such as articles, singu-lar/plural forms of nouns and copulas, etc.
AlthoughJapanese does not have such grammatical devices,12it would be possible to predict such distinctions byusing a machine learning technique with linguistic?putting a piece?
and ?getting a piece out?
are distinguisheddue to speakers?
intentions, they are (at least superficially) ho-mogeneous in the physical data available from the corpus anddifficult for machines to distinguish each other.10In general, the speech and expressions in human-machine(HM) dialogue are less complex and less difficult to processthan those in HH dialogue data.
This is typcially observed asfewer disfluencies (Shriberg, 2001) and simpler sentences withfewer omissions (Itoh et al, 2002).
Therefore, when we applyour framework to real DSs, we can expect clearer and simplerinput and thus better performance.
We supposed that the condi-tion of perfect preprocessing in HH dialogue approximates theresults to those obtained when HM dialogue data is used.11If a reference is misinterpreted (i.e., wrongly resolved) in adialogue, usually that misinterpretation will be repaired by theinterlocutors in the succeeding interaction once the misinterpre-tation becomes apparent.
Therefore, accumulating all past er-rors in resolution is rather irrational as an experimental setting.12Japanese has a plurality marker -ra (e.g., sore-ra), but useof it is not mandatory (except for personal pronouns).and gestural information.
Therefore we observed theeffect of providing such information.
In the follow-ing experiment we provide the singular/plural dis-tinction information to REBNs by looking at the an-notations of the correct referents in advance.
Thisis achieved by adding a special evidence node C0,where D(C0) = {S,P}.
P (C0 = S|X = x) = 1and P (P|x) = 0 if x is a piece.
On the contrary,P (S|x) = 0 and P (P|x) = 1 if x is a set.3.3.2 BaselinesTo our best knowledge, there is no directly com-parable method.
We set up two baselines.
The firstbaseline uses the most recent as the resolved refer-ent for each RE (Initial resolution of each dialoguealways fails).
This baseline is called Most-recent.As the second baseline, we prepared anotherP (D) model in addition to those explained in sec-tion 2.3.4, which is called Mono-domain.
In Mono-domain, D(D) consists of only a single RD @?0,which contains individual pieces and the RDs recog-nized up to that point in time.
That is, @?0 = D(X).Resolution using this model can be considered asa straightforward extension of (Iida et al, 2010),which enables handling of richer concepts in REs13and handling of REs to sets14.3.3.3 ResultsThe performance of reference resolution is pre-sented by category and by condition in terms of ac-curacy (# of correctly resolved REs/# of REs).We set up the three categories in evaluating res-olution, that is, Single, Plural, and Total.
CategorySingle is the collection of REs referring to a singlepiece.
Plural is the collection of REs referring to aset of pieces.
Total is the sum of them.
Ambigu-ous REs such as the first one in table 1 are countedas ?Single?
and the resolution of such a RE is con-sidered correct if the resolved result is one of thepossible referents.13(Iida et al, 2010) used only object types and sizes.
Otherconcepts such as LEFT were simply ignored.14(Iida et al, 2010) did not deal with REs to sets.243?w/o S/P info.?
indicates experimental resultswithout singular/plural distinction information.
?w/S/P info.?
indicates experimental results with it.Table 4 shows the results of reference resolutionper P (D) modeling method.15 Obviously S/P infor-mation has a significant impact.While the best performance for category Singlewas achieved with the Linear model, the best perfor-mance for Plural was achieved with the Exponen-tial model.
If it is possible to know whether a REis of Single or Plural, that is, if S/P information isavailable, we can choose a suitable P (D) model.Therefore, by switching models, the best perfor-mance of Total with S/P information reached 83.4%,and a gain of 2.0 points against Mono-domain wasachieved (sign test, p < 0.0001).Because the corpus did not include many in-stances to which the notion of reference domains iseffective, the impact of RDs may appear small on thewhole.
In fact, the impact was not small.
By intro-ducing RDs, resolution in category Plural achieveda significant advancement.
The highest gain fromMono-domain was 9.3 points (sign test, p < 0.005).Moreover, more REs containing positional conceptssuch as LEFT and RIGHT were correctly resolvedin the cases of Uniform, Linear, and Exponential.Table 5 summarizes the resolution results of fourpositional concepts (with S/P information).
WhileMono-domain resolved 65% of them, Linear cor-rectly resolved 75% (sign test, p < 0.05).As shown in table 4, the performance of the Uni-form model was worse than that of Mono-domain.This indicates that RDs introduced without an ap-propriate management of them would be harmfulnoise.
Conversely, it also suggests that there mightbe a room for improvement by looking deeply intothe management of RDs (e.g., forgetting old RDs).4 ConclusionThis paper proposed a probabilistic approach to ref-erence resolution, REBNs, which stands for Refer-ring Expression Bayesian Networks.
At each timeof resolution, a dedicated BN is constructed for the15According to the results of preliminary experiments, evenin the case of the Uniform/Linear/Exponential models, we re-solved the REs having demonstratives with the Mono-domainmodel.
This is in line with the finding of separating modelsbetween pronouns and non-pronouns in (Iida et al, 2010).Concept Count Mono Uni.
Lin.
Exp.LEFT 21 11 12 16 13RIGHT 33 23 23 25 27UPPER 9 6 6 6 4LOWER 6 5 4 5 4Total 69 45 45 52 48(Count means the numbers of occurrence of each concept.
Mono, Uni.,Lin., and Exp.
correspond to Mono-domain, Uniform, Linear and Ex-ponential.
)Table 5: Numbers of correctly resolved REs containingpositional concepts.RE in question.
The constructed BN deals with ei-ther descriptive, deictic or anaphoric REs in a uni-fied manner.
REBNs incorporate the notion of ref-erence domains (RDs), which enables the resolutionof REs with context-dependent attributes and han-dling of REs to sets.
REBNs are for task-orienteddialogue systems and presuppose a certain amountof domain-dependent manual implementation by de-velopers.
Therefore, REBNs would not be suitedto general text processing or non-task-oriented sys-tems.
However, REBNs have the potential to be astandard approach that can be used for any and alltask-oriented applications such as personal agents insmart phones, in-car systems, service robots, etc.
?The proposed approach was evaluated with theREX-J human-human dialogue corpus and promis-ing results were obtained.
The impact of incorpo-rating RDs in the domain of the REX-J corpus wasrecognizable but not so large on the whole.
How-ever, in other types of task domains where groupingand comparisons of objects occur frequently, the im-pact would be larger.
Note that REBNs are not lim-ited to Japanese, even though the evaluation used aJapanese corpus.
Evaluations with human-machinedialogue are important future work.Although this paper focused on the simple type ofREs without relations, REBNs are potentially ableto deal with complex REs with relations.
The eval-uation for complex REs is necessary to validate thispotential of REBN.
Currently REBN assumes REswhose referents are concrete entities.
An extensionfor handling abstract entities (Byron, 2002; Mu?ller,2007) is important future work.
Another directionwould be generating REs with REBNs.
A generate-and-test approach is a naive application of REBNfor generation.
More efficient method is, however,necessary.244ReferencesJohn D. Burger and Dennis Connoly.
1992.
Probabilisticresolution of anaphoric reference.
In Proceedings ofthe AAAI Fall Symposium on Intelligent ProbabilisticApproaches to Natural Language, pages 17?24.Donna Byron.
2002.
Resolving pronominal referenceto abstract entities.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 80?87.Eugene Charniak and Robert Goldman.
1989.
A se-mantics for probabilistic quantifier-free first-order lan-guages with particular application to story understand-ing.
In Proceedings of the Eleventh International JointConference on Artificial Intelligence (IJCAI), pages1074?1079, Menlo Park, CA, USA.Sehyeong Cho and Anthony Maida.
1992.
Using aBayesian framework to identify the referent of definitedescriptions.
In Proceedings of the AAAI Fall Sympo-sium on Intelligent Probabilistic Approaches to Natu-ral Language, pages 39?46.Robert Dale and Ehud Reiter.
1995.
Computational in-terpretations of the Gricean maxims in the generationof referring expressions.
Cognitive Science, 18:233?263.Robert Dale and Jette Viethen.
2009.
Referring expres-sion generation through attribute-based heuristics.
InProceedings of the the 12th European Workshop onNatural Language Generation (ENLG), pages 59?65,Athens, Greece, March.Alexandre Denis.
2010.
Generating referring expres-sions with reference domain theory.
In Proceedingsof the 6th International Natural Language GenerationConference (INLG), pages 27?35.Petra Gieselmann.
2004.
Reference resolution mech-anisms in dialogue management.
In Proceedings ofthe 8th workshop on the semantics and pragmatics ofdialogue (CATALOG), pages 28?34, Barcelona, Italy,July.Ryu Iida, Shumpei Kobayashi, and Takenobu Tokunaga.2010.
Incorporating extra-linguistic information intoreference resolution in collaborative task dialogue.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 1259?1267, Uppsala, Sweden, July.Toshihiko Itoh, Atsuhiko Kai, Tatsuhiro Konishi, andYukihiro Itoh.
2002.
Linguistic and acoustic changesof user?s utterances caused by different dialogue situa-tions.
In Proceedings of the 7th International Confer-ence on Spoken Language Processing (ICSLP), pages545?548.Finn V. Jensen and Thomas D. Nielsen.
2007.
BayesianNetworks and Decision Graphs.
Springer, second edi-tion.Thorsten Joachims.
2006.
Training linear SVMs in lin-ear time.
In Proceedings of the ACM Conference onKnowledge Discovery and Data Mining (KDD), pages217?226, Philadelphia, PA, USA, August.Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic.
Kluwer Academic Publishers.Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29:53?72.Pierre Lison, Carsten Ehrler, and Geert-Jan M. Kruijff.2010.
Belief modelling for situation awareness inhuman-robot interaction.
In Proceedings of the 19thInternational Symposium on Robot and Human In-teractive Communication (RO-MAN), pages 138?143,Viareggio, Italy, September.Ruslan Mitkov.
2002.
Anaphora Resolution.
Studies inLanguage and Linguistics.
Pearson Education.Christoph Mu?ller.
2007.
Resolving it, this, and that inunrestricted multi-party dialog.
In Proceedings of the45th Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 816?823.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 1396?1411, Uppsala, Swe-den, July.Judea Pearl.
1988.
Probabilistic Reasoning in Intelli-gent Systems: Networks of Plausible Inference.
Mor-gan Kaufmann, San Mateo, CA, USA.Matthew Richardson and Pedor Domingos.
2006.Markov logic networks.
Machine Learning, 62(1?2):107?136.Deb K. Roy.
2002.
Learning visually-grounded wordsand syntax for a scene description task.
ComputerSpeech and Language, 16(3):353?385.Susanne Salmon-Alt and Laurent Romary.
2000.
Gen-erating referring expressions in multimodal contexts.In Proceedings of the INLG 2000 workshop on Coher-ence in Generated Multimedia, Mitzpe Ramon, Israel,June.Susanne Salmon-Alt and Laurent Romary.
2001.
Ref-erence resolution within the framework of cognitivegrammar.
In Proceedings of the International Col-loqium on Cognitive Science, San Sebastian, Spain,May.Elizabeth Shriberg.
2001.
To ?errrr?
is human: ecologyand acoustics of speech disfluencies.
Journal of theInternational Phonetic Association, 31(1):153?169.Philipp Spanger, Masaaki Yasuhara, Ryu Iida, TakenobuTokunaga, Asuka Terai, and Naoko Kuriyama.
2010.REX-J: Japanese referring expression corpus of sit-uated dialogs.
Language Resources and Evaluation.Online First, DOI: 10.1007/s10579-010-9134-8.245Kristinn R. Tho?risson.
1994.
Simulated perceptualgrouping: An application to human-computer interac-tion.
In Proceedings of the 16th Annual Conferenceof the Cognitive Science Society, pages 876?881, At-lanta, GA, USA.Davy Weissenbacher and Adeline Nazarenko.
2007.
ABayesian approach combining surface clues and lin-guistic knowledge: Application to the anaphora reso-lution problem.
In Proceedings of the InternationalConference Recent Advances in Natural LanguageProcessing (RANLP), Borovets, Bulgaria.Davy Weissenbacher.
2005.
A Bayesian network for theresolution of non-anaphoric pronoun it.
In Proceed-ings of the NIPS 2005 Workshop on Bayesian Meth-ods for Natural Language Processing, Whistler, BC,Canada.A Algorithm to compose P (C|X,D)Algorithm 1 Composing P (C|X = x,D = d).Input: D(C); R(c, x, d) for all c ?
D(C)\{?
}Output: P (C|X = x,D = d)1: n ?
0, s ?
0, S = D(C)\{?
}2: for all c ?
S do3: r[c] ?
R(c, x, d) #{Relevancy of concept c}4: s ?
s+ r[c] #{Sum of relevancy r[c]}5: n ?
n+ (1?
r[c]) #{Sum of residual (1?
r[c])}6: end for7: r[?]
?
n/|S|8: s ?
s+ r[?
]9: for all c ?
D(C) do10: P (C = c|X = x,D = d) ?
r[c]/s11: end for(#{.
.
. }
is a comment.
)B Relevancy functionsAs explained in section 2.3.2, the relevancy func-tions for positional concepts such as LEFT andRIGHT were implemented as geometric calcula-tions.
Here several other relevancy functions areshown with corresponding example REs.
?this figure?
:R(FIG, x, d)=??
?0.3 : if single(x)1 : if not single(x) and shape(x)0 : otherwise(single(x) means x is a single piece.
shape(x)means x is a set of pieces that are concatenated andform a shape.
0.3 comes from the static relevancytable.
)?both the triangles?
:R(BOTH, x, d) ={1 : if |x| = 20 : otherwise?another one?
:R(ANOTHER, x, d) ={1 : if foc(d) 6= x0 : otherwise?the remaining ones?
:R(REST, x, d) ={1 : if d = [x, y?
]0 : otherwise(REST requires |d| = 2, and both x and y are sets.ANOTHER does not.)?all?
:R(ALL, x, d) ={1 : if x = d0 : otherwise(ALL does not always refer to @0.
)246
