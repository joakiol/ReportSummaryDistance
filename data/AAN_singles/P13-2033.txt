Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 183?189,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsAccurate Word Segmentation using Transliterationand Language Model ProjectionMasato Hagiwara Satoshi SekineRakuten Institute of Technology, New York215 Park Avenue South, New York, NY{masato.hagiwara, satoshi.b.sekine}@mail.rakuten.comAbstractTransliterated compound nouns notseparated by whitespaces pose diffi-culty on word segmentation (WS).
Of-fline approaches have been proposed tosplit them using word statistics, butthey rely on static lexicon, limitingtheir use.
We propose an online ap-proach, integrating source LM, and/or,back-transliteration and English LM.The experiments on Japanese and Chi-nese WS have shown that the pro-posed models achieve significant im-provement over state-of-the-art, reduc-ing 16% errors in Japanese.1 IntroductionAccurate word segmentation (WS) is thekey components in successful language pro-cessing.
The problem is pronounced in lan-guages such as Japanese and Chinese, wherewords are not separated by whitespaces.
Inparticular, compound nouns pose difficultiesto WS since they are productive, and oftenconsist of unknown words.In Japanese, transliterated foreign com-pound words written in Katakana are ex-tremely difficult to split up into componentswithout proper lexical knowledge.
For ex-ample, when splitting a compound noun ?????????
burakisshureddo, a traditionalword segmenter can easily segment this as ????/?????
?
*blacki shred?
since ?????
shureddo ?shred?
is a known, frequent word.It is only the knowledge that ????buraki(*?blacki?)
is not a valid word which preventsthis.
Knowing that the back-transliterated un-igram ?blacki?
and bigram ?blacki shred?
areunlikely in English can promote the correctWS, ??????/???
?blackish red?.
In Chi-nese, the problem can be more severe sincethe language does not have a separate scriptto represent transliterated words.Kaji and Kitsuregawa (2011) tackledKatakana compound splitting using back-transliteration and paraphrasing.
Their ap-proach falls into an offline approach, whichfocuses on creating dictionaries by extract-ing new words from large corpora separatelybefore WS.
However, offline approaches havelimitation unless the lexicon is constantlyupdated.
Moreover, they only deal withKatakana, but their method is not directly ap-plicable to Chinese since the language lacks aseparate script for transliterated words.Instead, we adopt an online approach, whichdeals with unknown words simultaneously asthe model analyzes the input.
Our ap-proach is based on semi-Markov discrimina-tive structure prediction, and it incorporatesEnglish back-transliteration and English lan-guage models (LMs) into WS in a seamlessway.
We refer to this process of transliterat-ing unknown words into another language andusing the target LM as LM projection.
Sincethe model employs a general transliterationmodel and a general English LM, it achievesrobust WS for unknown words.
To the bestof our knowledge, this paper is the first to usetransliteration and projected LMs in an online,seamlessly integrated fashion for WS.To show the effectiveness of our approach,we test our models on a Japanese balanced cor-pus and an electronic commerce domain cor-pus, and a balanced Chinese corpus.
The re-sults show that we achieved a significant im-provement in WS accuracy in both languages.2 Related WorkIn Japanese WS, unknown words are usu-ally dealt with in an online manner with theunknown word model, which uses heuristics183depending on character types (Kudo et al,2004).
Nagata (1999) proposed a Japanese un-known word model which considers PoS (partof speech), word length model and orthog-raphy.
Uchimoto et al (2001) proposed amaximum entropy morphological analyzer ro-bust to unknown words.
In Chinese, Peng etal.
(2004) used CRF confidence to detect newwords.For offline approaches, Mori and Nagao(1996) extracted unknown word and estimatedtheir PoS from a corpus through distributionalanalysis.
Asahara and Matsumoto (2004)built a character-based chunking model usingSVM for Japanese unknown word detection.Kaji and Kitsuregawa (2011)?s approach isthe closest to ours.
They built a modelto split Katakana compounds using back-transliteration and paraphrasing mined fromlarge corpora.
Nakazawa et al (2005) isa similar approach, using a Ja-En dictionaryto translate compound components and checktheir occurrence in an English corpus.
Sim-ilar approaches are proposed for other lan-guages, such as German (Koehn and Knight,2003) and Urdu-Hindi (Lehal, 2010).
Correctsplitting of compound nouns has a positive ef-fect on MT (Koehn and Knight, 2003) and IR(Braschler and Ripplinger, 2004).A similar problem can be seen in Korean,German etc.
where compounds may not beexplicitly split by whitespaces.
Koehn andKnight (2003) tackled the splitting problem inGerman, by using word statistics in a mono-lingual corpus.
They also used the informa-tion whether translations of compound partsappear in a German-English bilingual corpus.Lehal (2010) used Urdu-Devnagri translitera-tion and a Hindi corpus for handling the spaceomission problem in Urdu compound words.3 Word Segmentation ModelOut baseline model is a semi-Markov struc-ture prediction model which estimates WS andthe PoS sequence simultaneously (Kudo et al,2004; Zhang and Clark, 2008).
This modelfinds the best output y?
from the input sen-tence string x as: y?
= argmaxy?Y (x) w ??
(y).Here, Y (x) denotes all the possible sequencesof words derived from x.
The best analysis isdetermined by the feature function ?
(y) theID Feature ID Feature1 wi 13 w1i?1w1i2 t1i 14 t1i?1t1i3* t1i t2i 15* t1i?1t2i?1t1i t2i4* t1i t2i t3i 16* t1i?1t2i?1t3i?1t1i t2i t3i5* t1i t2i t5i t6i 17* t1i?1t2i?1t5i?1t6i?1t1i t2i t5i t6i6* t1i t2i t6i 18* t1i?1t2i?1t6i?1t1i t2i t6i7 wit1i 19 ?LMS1 (wi)8* wit1i t2i 20 ?LMS2 (wi?1, wi)9* wit1i t2i t3i 21 ?LMP1 (wi)10* wit1i t2i t5i t6i 22 ?LMP2 (wi?1, wi)11* wit1i t2i t6i12 c(wi)l(wi)Table 1: Features for WS & PoS taggingweight vector w. WS is conducted by stan-dard Viterbi search based on lattice, whichis illustrated in Figure 1.
We limit the fea-tures to word unigram and bigram features,i.e., ?
(y) = ?i[?1(wi) + ?2(wi?1, wi)] for y =w1...wn.
By factoring the feature function intothese two subsets, argmax can be efficientlysearched by the Viterbi algorithm, with itscomputational complexity proportional to theinput length.
We list all the baseline featuresin Table 11.
The asterisks (*) indicate the fea-ture is used for Japanese (JA) but not for Chi-nese (ZH) WS.
Here, wi and wi?1 denote thecurrent and previous word in question, and tjiand tji?1 are level-j PoS tags assigned to them.l(w) and c(w) are the length and the set ofcharacter types of word w.If there is a substring for which no dic-tionary entries are found, the unknown wordmodel is invoked.
In Japanese, our unknownword model relies on heuristics based on char-acter types and word length to generate wordnodes, similar to that of MeCab (Kudo etal., 2004).
In Chinese, we aggregated con-secutive 1 to 4 characters add them as ?n(common noun)?, ?ns (place name)?, ?nr (per-sonal name)?, and ?nz (other proper nouns),?since most of the unknown words in Chineseare proper nouns.
Also, we aggregated up to20 consecutive numerical characters, makingthem a single node, and assign ?m?
(number).For other character types, a single node withPoS ?w (others)?
is created.1The Japanese dictionary and the corpus we usedhave 6 levels of PoS tag hierarchy, while the Chineseones have only one level, which is why some of thePoS features are not included in Chinese.
As charactertype, Hiragana (JA), Katakana (JA), Latin alphabet,Number, Chinese characters, and Others, are distin-guished.
Word length is in Unicode.184Input: ?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
???
??
??
?
?
EOS?
?
?
??
?
?
?
??
?
?
??
?
?.
.
.. .
.?
?
??
?
??
?
?
?
?BOSvery  popular       color                     blackish                            redbu kibla bra kishbrakiblakibrakiblakibrackishblackishledreadredshreadshrednode (a)node (b)edge (c)TransliterationModelEnglishLMFigure 1: Example lattice with LM projection4 Use of Language ModelLanguage Model Augmentation Analo-gous to Koehn and Knight (2003), we can ex-ploit the fact that ???
reddo (red) in theexample ?????????
is such a commonword that one can expect it appears frequentlyin the training corpus.
To incorporate thisintuition, we used log probability of n-gramas features, which are included in Table 1(ID 19 and 20): ?LMS1 (wi) = log p(wi) and?LMS2 (wi?1, wi) = log p(wi?1, wi).
Here theempirical probability p(wi) and p(wi?1, wi) arecomputed from the source language corpus.
InJapanese, we applied this source language aug-mentation only to Katakana words.
In Chi-nese, we did not limit the target.4.1 Language Model ProjectionAs we mentioned in Section 2, EnglishLM knowledge helps split transliterated com-pounds.
We use (LM) projection, which isa combination of back-transliteration and anEnglish model, by extending the normal lat-tice building process as follows:Firstly, when the lattice is being built, eachnode is back-transliterated and the resultingnodes are associated with it, as shown inFigure 1 as the shaded nodes.
Then, edgesare spanned between these extended Englishnodes, instead of between the original nodes,by additionally taking into consideration En-glish LM features (ID 21 and 22 in Table 1):?LMP1 (wi) = log p(wi) and ?LMP2 (wi?1, wi) =log p(wi?1, wi).
Here the empirical probabil-ity p(wi) and p(wi?1, wi) are computed fromthe English corpus.
For example, Feature 21is set to ?LMP1 (?blackish?)
for node (a), to?LMP1 (?red?)
for node (b), and Feature 22 isset to ?LMP2 (?blackish?, ?red?)
for edge (c) inFigure 1.
If no transliterations were generated,or the n-grams do not appear in the Englishcorpus, a small frequency ?
is assumed.Finally, the created edges are traversed fromEOS, and associated original nodes are chosenas the WS result.
In Figure 1, the bold edgesare traversed at the final step, and the corre-sponding nodes ??
- ??
- ?
- ??????-????
are chosen as the final WS result.For Japanese, we only expand and projectKatakana noun nodes (whether they areknown or unknown words) since transliteratedwords are almost always written in Katakana.For Chinese, only ?ns (place name)?, ?nr (per-sonal name)?, and ?nz (other proper noun)?nodes whose surface form is more than 1-character long are transliterated.
As the En-glish LM, we used Google Web 1T 5-gram Ver-sion 1 (Brants and Franz, 2006), limiting it tounigrams occurring more than 2000 times andbigrams occurring more than 500 times.5 TransliterationFor transliterating Japanese/Chinese wordsback to English, we adopted the Joint SourceChannel (JSC) Model (Li et al, 2004), a gen-erative model widely used as a simple yet pow-erful baseline in previous research e.g., (Hagi-wara and Sekine, 2012; Finch and Sumita,2010).2 The JSC model, given an inputof source word s and target word t, de-fines the transliteration probability based ontransliteration units (TUs) ui = ?si, ti?
as:PJSC(?s, t?)
=?fi=1 P (ui|ui?n+1, ..., ui?1),where f is the number of TUs in a given source/ target word pair.
TUs are atomic pair unitsof source / target words, such as ?la/??
and?ish/????.
The TU n-gram probabilities arelearned from a training corpus by following it-erative updates similar to the EM algorithm3.In order to generate transliteration candidates,we used a stack decoder described in (Hagi-wara and Sekine, 2012).
We used the trainingdata of the NEWS 2009 workshop (Li et al,2009a; Li et al, 2009b).As reference, we measured the performanceon its own, using NEWS 2009 (Li et al, 2009b)data.
The percentage of correctly transliter-ated words are 37.9% for Japanese and 25.6%2Note that one could also adopt other generative /discriminative transliteration models, such as (Jiampo-jamarn et al, 2007; Jiampojamarn et al, 2008).3We only allow TUs whose length is shorter than orequal to 3, both in the source and target side.185for Chinese.
Although the numbers seem lowat a first glance, Chinese back-transliterationitself is a very hard task, mostly becauseChinese phonology is so different from En-glish that some sounds may be dropped whentransliterated.
Therefore, we can regard thisperformance as a lower bound of the translit-eration module performance we used for WS.6 Experiments6.1 Experimental SettingsCorpora For Japanese, we used (1) ECcorpus, consists of 1,230 product titles anddescriptions randomly sampled from Rakuten(Rakuten-Inc., 2012).
The corpus is manuallyannotated with the BCCWJ style WS (Oguraet al, 2011).
It consists of 118,355 tokens, andhas a relatively high percentage of Katakanawords (11.2%).
(2) BCCWJ (Maekawa, 2008)CORE (60,374 sentences, 1,286,899 tokens,out of which approx.
3.58% are Katakanawords).
As the dictionary, we used UniDic(Den et al, 2007).
For Chinese, we usedLCMC (McEnery and Xiao, 2004) (45,697 sen-tences and 1,001,549 tokens).
As the dictio-nary, we used CC-CEDICT (MDGB, 2011)4.Training and Evaluation We used Aver-aged Perceptron (Collins, 2002) (3 iterations)for training, with five-fold cross-validation.
Asfor the evaluation metrics, we used Precision(Prec.
), Recall (Rec.
), and F-measure (F).
Weadditionally evaluated the performance lim-ited to Katakana (JA) or proper nouns (ZH)in order to see the impact of compound split-ting.
We also used word error rate (WER) tosee the relative change of errors.6.2 Japanese WS ResultsWe compared the baseline model, theaugmented model with the source language(+LM-S) and the projected model (+LM-P).Table 3 shows the result of the proposed mod-els and major open-source Japanese WS sys-tems, namely, MeCab 0.98 (Kudo et al, 2004),JUMAN 7.0 (Kurohashi and Nagao, 1994),4Since the dictionary is not explicitly annotatedwith PoS tags, we firstly took the intersection of thetraining corpus and the dictionary words, and assignedall the possible PoS tags to the words which appearedin the corpus.
All the other words which do not appearin the training corpus are discarded.and KyTea 0.4.2 (Neubig et al, 2011) 5.
Weobserved slight improvement by incorporat-ing the source LM, and observed a 0.48 pointF-value increase over baseline, which trans-lates to 4.65 point Katakana F-value changeand 16.0% (3.56% to 2.99 %) WER reduc-tion, mainly due to its higher Katakana wordrate (11.2%).
Here, MeCab+UniDic achievedslightly better Katakana WS than the pro-posed models.
This may be because it istrained on a much larger training corpus (thewhole BCCWJ).
The same trend is observedfor BCCWJ corpus (Table 2), where we gainedstatistically significant 1 point F-measure in-crease on Katakana word.Many of the improvements of +LM-S overBaseline come from finer grained splitting,for example, * ??????
reinsuutsu ?rainsuits?
to ???/??
?, while there is wrongover-splitting, e.g., ???????terekyasutaa?Telecaster?
to * ??/?????.
This type oferror is reduced by +LM-P, e.g., * ???/???
purasu chikku ?
*plus tick?
to ?????
?purasuchikku ?plastic?
due to LM projection.+LM-P also improved compounds whose com-ponents do not appear in the training data,such as * ????????
ruukasufirumu to????/????
?Lucus Film.?
Indeed, werandomly extracted 30 Katakana differencesbetween +LM-S and +LM-P, and found outthat 25 out of 30 (83%) are true improvement.One of the proposed method?s advantages isthat it is very robust to variations, such as??????????
akutibeitiddo ?activated,?even though only the original form, ???????
akutibeito ?activate?
is in the dictionary.One type of errors can be attributedto non-English words such as ?????
?sunokobeddo, which is a compound of Japaneseword ???
sunoko ?duckboard?
and an En-glish word ???
beddo ?bed.
?6.3 Chinese WS ResultsWe compare the results on Chinese WS,with Stanford Segmenter (Tseng et al, 2005)(Table 4) 6.
Including +LM-S decreased the5Because MeCab+UniDic and KyTea models areactually trained on BCCWJ itself, this evaluation isnot meaningful but just for reference.
The WS granu-larity of IPADic, JUMAN, and KyTea is also differentfrom the BCCWJ style.6Note that the comparison might not be fair since(1) Stanford segmenter?s criteria are different from186Model Prec.
(O) Rec.
(O) F (O) Prec.
(K) Rec.
(K) F (K) WERMeCab+IPADic 91.28 89.87 90.57 88.74 82.32 85.41 12.87MeCab+UniDic* (98.84) (99.33) (99.08) (96.51) (97.34) (96.92) (1.31)JUMAN 85.66 78.15 81.73 91.68 88.41 90.01 23.49KyTea* (81.84) (90.12) (85.78) (99.57) (99.73) (99.65) (20.02)Baseline 96.36 96.57 96.47 84.83 84.36 84.59 4.54+LM-S 96.36 96.57 96.47 84.81 84.36 84.59 4.54+LM-S+LM-P 96.39 96.61 96.50 85.59 85.40 85.50 4.50Table 2: Japanese WS Performance (%) on BCCWJ ?
Overall (O) and Katakana (K)Model Prec.
(O) Rec.
(O) F (O) Prec.
(K) Rec.
(K) F (K) WERMeCab+IPADic 84.36 87.31 85.81 86.65 73.47 79.52 20.34MeCab+UniDic 95.14 97.55 96.33 93.88 93.22 93.55 5.46JUMAN 90.99 87.13 89.2 92.37 88.02 90.14 14.56KyTea 82.00 86.53 84.21 93.47 90.32 91.87 21.90Baseline 97.50 97.00 97.25 89.61 85.40 87.45 3.56+LM-S 97.79 97.37 97.58 92.58 88.99 90.75 3.17+LM-S+LM-P 97.90 97.55 97.73 93.62 90.64 92.10 2.99Table 3: Japanese WS Performance (%) on the EC domain corpusModel Prec.
(O) Rec.
(O) F (O) Prec.
(P) Rec.
(P) F (P) WERStanford Segmenter 87.06 86.38 86.72 ?
?
?
17.45Baseline 90.65 90.87 90.76 83.29 51.45 63.61 12.21+LM-S 90.54 90.78 90.66 72.69 43.28 54.25 12.32+LM-P 90.90 91.48 91.19 75.04 52.11 61.51 11.90Table 4: Chinese WS Performance (%) ?
Overall (O) and Proper Nouns (P)performance, which may be because one can-not limit where the source LM features areapplied.
This is why the result of +LM-S+LM-P is not shown for Chinese.
On theother hand, replacing LM-S with LM-P im-proved the performance significantly.
Wefound positive changes such as * ??/????
oumai/ersalihe to ???/??
?oumaier/salihe ?Umar Saleh?
and * ??/????
lingdao/renmandela to ???/??
?lingdaoren/mandela?Leader Mandela?.
How-ever, considering the overall F-measure in-crease and proper noun F-measure decreasesuggests that the effect of LM projection isnot limited to proper nouns but also promotedfiner granularity because we observed propernoun recall increase.One of the reasons which make Chinese LMprojection difficult is the corpus allows sin-gle tokens with a transliterated part and Chi-nese affices, e.g., ??????
makesizhuy-izhe ?Marxists?
(???
makesi ?Marx?
+ ???
zhuyizhe ?-ist (believers)?)
and ??
?niluohe ?Nile River?
( ??
niluo ?Nile?
+?
he ?-river?).
Another source of errors istransliteration accuracy.
For example, no ap-ours, and (2) our model only uses the intersection ofthe training set and the dictionary.
Proper noun per-formance for the Stanford segmenter is not shown sinceit does not assign PoS tags.propriate transliterations were generated for???
weinasi ?Venus,?
which is commonlyspelled ???
weinasi.
Improving the JSCmodel could improve the LM projection per-formance.7 Conclusion and Future WorksIn this paper, we proposed a novel, on-line WS model for the Japanese/Chinesecompound word splitting problem, by seam-lessly incorporating the knowledge that back-transliteration of properly segmented wordsalso appear in an English LM.
The experi-mental results show that the model achievesa significant improvement over the baselineand LM augmentation, achieving 16% WERreduction in the EC domain.The concept of LM projection is generalenough to be used for splitting other com-pound nouns.
For example, for Japanese per-sonal names such as ????
Naka Riisa, ifwe could successfully estimate the pronuncia-tion Nakariisa and look up possible splits inan English LM, one is expected to find a cor-rect WS Naka Riisa because the first and/orthe last name are mentioned in the LM.
Seek-ing broader application of LM projection is afuture work.187ReferencesMasayuki Asahara and Yuji Matsumoto.
2004.Japanese unknown word identification bycharacter-based chunking.
In Proceedings ofCOLING 2004, pages 459?465.Thorsten Brants and Alex Franz.
2006.
Web 1T5-gram Version 1.
Linguistic Data Consortium.Martin Braschler and B?rbel Ripplinger.
2004.How effective is stemming and decompoundingfor german text retrieval?
Information Re-trieval, pages 291?316.Michael Collins.
2002.
Discriminative trainingmethods for hidden markov models: theory andexperiments with perceptron algorithms.
InProceedings of EMNLP 2012, pages 1?8.Yasuharu Den, Toshinobu Ogiso, Hideki Ogura,Atsushi Yamada, Nobuaki Minematsu, Kiy-otaka Uchimoto, and Hanae Koiso.
2007.The development of an electronic dictionaryfor morphological analysis and its applicationto Japanese corpus linguistics (in Japanese).Japanese linguistics, 22:101?122.Andrew Finch and Eiichiro Sumita.
2010.
Abayesian model of bilingual segmentation fortransliteration.
In Proceedings of IWSLT 2010,pages 259?266.Masato Hagiwara and Satoshi Sekine.
2012.
La-tent class transliteration based on source lan-guage origin.
In Proceedings of NEWS 2012,pages 30?37.Sittichai Jiampojamarn, Grzegorz Kondrak, andTarek Sherif.
2007.
Applying many-to-manyalignments and hidden markov models to letter-to-phoneme conversion.
In Proceedings ofNAACL-HLT 2007, pages 372?379.Sittichai Jiampojamarn, Colin Cherry, and Grze-gorz Kondrak.
2008.
Joint processing and dis-criminative training for letter-to-phoneme con-version.
In Proceedings of ACL 2008, pages905?913.Nobuhiro Kaji and Masaru Kitsuregawa.
2011.Splitting noun compounds via monolingual andbilingual paraphrasing: A study on japanesekatakana words.
In Proceedings of the EMNLP2011, pages 959?969.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In Proceedingsof EACL 2003, pages 187?193.Taku Kudo, Kaoru Yamamoto, and Yuji Mat-sumoto.
2004.
Applying conditional randomfields to Japanese morphological analysis.
InProceedings of EMNLP 2004, pages 230?237.Sadao Kurohashi and Makoto Nagao.
1994.
Im-provements of Japanese morphological analyzerjuman.
In Proceedings of the InternationalWorkshop on Sharable Natural Language Re-sources, pages 22?38.Gurpreet Singh Lehal.
2010.
A word segmentationsystem for handling space omission problem inurdu script.
In Proceedings of the 1st Workshopon South and Southeast Asian Natural LanguageProcessing (WSSANLP), pages 43?50.Haizhou Li, Zhang Min, and Su Jian.
2004.
Ajoint source-channel model for machine translit-eration.
In Proceedings of ACL 2004, pages159?166.Haizhou Li, A Kumaran, Vladimir Pervouchine,and Min Zhang.
2009a.
Report of news 2009machine transliteration shared task.
In Proceed-ings of NEWS 2009, pages 1?18.Haizhou Li, A Kumaran, Min Zhang, and VladimirPervouchine.
2009b.
Whitepaper of news 2009machine transliteration shared task.
In Proceed-ings of NEWS 2009, pages 19?26.Kikuo Maekawa.
2008.
Compilation ofthe Kotonoha-BCCWJ corpus (in Japanese).Nihongo no kenkyu (Studies in Japanese),4(1):82?95.Anthony McEnery and Zhonghua Xiao.
2004.
Thelancaster corpus of mandarin chinese: A corpusfor monolingual and contrastive language study.In Proceedings of LREC 2004, pages 1175?1178.MDGB.
2011.
CC-CEDICT,Retreived August, 2012 fromhttp://www.mdbg.net/chindict/chindict.php?page=cedict.Shinsuke Mori and Makoto Nagao.
1996.
Wordextraction from corpora and its part-of-speechestimation using distributional analysis.
In Pro-ceedings of COLING 2006, pages 1119?1122.Masaaki Nagata.
1999.
A part of speech estima-tion method for Japanese unknown words usinga statistical model of morphology and context.In Proceedings of ACL 1999, pages 277?284.Toshiaki Nakazawa, Daisuke Kawahara, and SadaoKurohashi.
2005.
Automatic acquisition of ba-sic katakana lexicon from a given corpus.
InProceedings of IJCNLP 2005, pages 682?693.Graham Neubig, Yosuke Nakata, and ShinsukeMori.
2011.
Pointwise prediction for robust,adaptable Japanese morphological analysis.
InProceedings of ACL-HLT 2011, pages 529?533.Hideki Ogura, Hanae Koiso, Yumi Fujike, SayakaMiyauchi, and Yutaka Hara.
2011.
Mor-phological Information Guildeline for BCCWJ:Balanced Corpus of Contemporary Written188Japanese, 4th Edition.
National Institute forJapanese Language and Linguistics.Fuchun Peng, Fangfang Feng, and Andrew McCal-lum.
2004.
Chinese segmentation and new worddetection using conditional random fields.
InProceedings COLING 2004.Rakuten-Inc. 2012.
Rakuten Ichibahttp://www.rakuten.co.jp/.Huihsin Tseng, Pichuan Chang, Galen Andrew,Daniel Jurafsky, and Christopher Manning.2005.
A conditional random field word seg-menter.
In Fourth SIGHAN Workshop on Chi-nese Language Processing.Kiyotaka Uchimoto, Satoshi Sekine, and HitoshiIsahara.
2001.
Morphological analysis basedon a maximum entropy model ?
an ap-proach to the unknown word problem ?
(inJapanese).
Journal of Natural Language Pro-cessing, 8:127?141.Yue Zhang and Stephen Clark.
2008.
Joint wordsegmentation and pos tagging using a single per-ceptron.
In Proceedings of ACL 2008, pages888?896.189
