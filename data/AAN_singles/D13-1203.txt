Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1971?1982,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsEasy Victories and Uphill Battles in Coreference ResolutionGreg Durrett and Dan KleinComputer Science DivisionUniversity of California, Berkeley{gdurrett,klein}@cs.berkeley.eduAbstractClassical coreference systems encode varioussyntactic, discourse, and semantic phenomenaexplicitly, using heterogenous features com-puted from hand-crafted heuristics.
In con-trast, we present a state-of-the-art coreferencesystem that captures such phenomena implic-itly, with a small number of homogeneousfeature templates examining shallow proper-ties of mentions.
Surprisingly, our featuresare actually more effective than the corre-sponding hand-engineered ones at modelingthese key linguistic phenomena, allowing usto win ?easy victories?
without crafted heuris-tics.
These features are successful on syntaxand discourse; however, they do not modelsemantic compatibility well, nor do we seegains from experiments with shallow seman-tic features from the literature, suggesting thatthis approach to semantics is an ?uphill bat-tle.?
Nonetheless, our final system1 outper-forms the Stanford system (Lee et al(2011),the winner of the CoNLL 2011 shared task)by 3.5% absolute on the CoNLL metric andoutperforms the IMS system (Bjo?rkelund andFarkas (2012), the best publicly available En-glish coreference system) by 1.9% absolute.1 IntroductionCoreference resolution is a multi-faceted task: hu-mans resolve references by exploiting contextualand grammatical clues, as well as semantic infor-mation and world knowledge, so capturing each of1The Berkeley Coreference Resolution System is availableat http://nlp.cs.berkeley.edu.these will be necessary for an automatic system tofully solve the problem.
Acknowledging this com-plexity, coreference systems, either learning-based(Bengtson and Roth, 2008; Stoyanov et al 2010;Haghighi and Klein, 2010; Rahman and Ng, 2011b)or rule-based (Haghighi and Klein, 2009; Lee etal., 2011), draw on diverse information sources andcomplex heuristics to resolve pronouns, model dis-course, determine anaphoricity, and identify seman-tically compatible mentions.
However, this leads tosystems with many heterogenous parts that can bedifficult to interpret or modify.We build a learning-based, mention-synchronouscoreference system that aims to use the simplest pos-sible set of features to tackle the various aspectsof coreference resolution.
Though they arise froma small number of simple templates, our featuresare numerous, which works to our advantage: wecan both implicitly model important linguistic ef-fects and capture other patterns in the data that arenot easily teased out by hand.
As a result, our data-driven, homogeneous feature set is able to achievehigh performance despite only using surface-leveldocument characteristics and shallow syntactic in-formation.
We win ?easy victories?
without design-ing features and heuristics explicitly targeting par-ticular phenomena.Though our approach is successful at modelingsyntax, we find semantics to be a much more chal-lenging aspect of coreference.
Our base systemuses only two recall-oriented features on nominaland proper mentions: head match and exact stringmatch.
Building on these features, we critically eval-uate several classes of semantic features which intu-1971itively should prove useful but have had mixed re-sults in the literature, and we observe that they areineffective for our system.
However, these featuresare beneficial when gold mentions are provided toour system, leading us to conclude that the largenumber of system mentions extracted by most coref-erence systems (Lee et al 2011; Fernandes et al2012) means that weak indicators cannot overcomethe bias against making coreference links.
Capturingsemantic information in this shallow way is an ?up-hill battle?
due to this structural property of corefer-ence resolution.Nevertheless, using a simple architecture and fea-ture set, our final system outperforms the two bestpublicly available English coreference systems, theStanford system (Lee et al 2011) and the IMS sys-tem (Bjo?rkelund and Farkas, 2012), by wide mar-gins: 3.5% absolute and 1.9% absolute, respectively,on the CoNLL metric.2 Experimental SetupThroughout this work, we use the datasets from theCoNLL 2011 shared task2 (Pradhan et al 2011),which is derived from the OntoNotes corpus (Hovyet al 2006).
When applicable, we use the standardautomatic parses and NER tags for each document.All experiments use system mentions except whereotherwise indicated.
For each experiment, we reportMUC (Vilain et al 1995), B3 (Bagga and Baldwin,1998), and CEAFe (Luo, 2005), as well as their av-erage, the CoNLL metric.
All metrics are computedusing version 5 of the official CoNLL scorer.33 A Mention-Synchronous FrameworkWe first present the basic architecture of our corefer-ence system, independent of a feature set.
Unlike bi-nary classification-based coreference systems whereindependent binary decisions are made about eachpair (Soon et al 2001; Bengtson and Roth, 2008;Versley et al 2008; Stoyanov et al 2010), we use alog-linear model to select at most one antecedent for2This dataset is identical to the English portion of theCoNLL 2012 data, except for the absence of a small pivot text.3Note that this version of the scorer implements a modifiedversion ofB3, described in Cai and Strube (2010), that was usedfor the CoNLL shared tasks.
The implementation of CEAFeis also not exactly as described in Luo et al(2004), but forcompleteness we include this metric as well.each mention or determine that it begins a new clus-ter (Denis and Baldridge, 2008).
In this mention-ranking or mention-synchronous framework, fea-tures examine single mentions to evaluate whetheror not they are anaphoric and pairs of mentions toevaluate whether or not they corefer.
While otherwork has used this framework as a starting pointfor entity-level systems (Luo et al 2004; Rahmanand Ng, 2009; Haghighi and Klein, 2010; Durrett etal., 2013), we will show that a mention-synchronousapproach is sufficient to get state-of-the-art perfor-mance on its own.3.1 Mention DetectionOur system first identifies a set of predicted men-tions from text annotated with parses and named en-tity tags.
We extract three distinct types of mentions:proper mentions from all named entity chunks ex-cept for those labeled as QUANTITY, CARDINAL, orPERCENT, pronominal mentions from single wordstagged with PRP or PRP$, and nominal mentionsfrom all other maximal NP projections.
These basicrules are similar to those of Lee et al(2011), exceptthat their system uses an additional set of filteringrules designed to discard instances of pleonastic it,partitives, certain quantified noun phrases, and otherspurious mentions.
In contrast to this highly engi-neered approach and to systems which use a trainedclassifier to compute anaphoricity separately (Rah-man and Ng, 2009; Bjo?rkelund and Farkas, 2012),we aim for the highest possible recall of gold men-tions with a low-complexity method, leaving us witha large number of spurious system mentions that wewill have to reject later.3.2 Coreference ModelFigure 1 shows the mention-ranking architecturethat serves as the backbone of our coreference sys-tem.
Assume we have extracted n mentions froma document x, where x denotes the surface proper-ties of a document and any precomputed informa-tion.
The ith mention in a document has an asso-ciated random variable ai taking values in the set{1, .
.
.
, i?1, NEW}; this variable specifies mentioni?s selected antecedent or indicates that it begins anew coreference chain.
A setting of the ai, denotedby a = (a1, ..., an), implies a unique set of corefer-ence chains C that serve as our system output.1972[Voters]1agree when [they]1are given a [chance]2to decide if [they]1...NEWFalse NewCorrect1?NEW2?3?1?Correcta1NEW NEW1?2?False AnaphorFalse AnaphorCorrectWrong LinkFalse NewCorrectCorrecta4a3a2Figure 1: The basic structure of our coreference model.
The ith mention in a document has i possible antecedencechoices: link to one of the i?
1 preceding mentions or begin a new cluster.
We place a distribution over these choiceswith a log-linear model.
Structurally different kinds of errors are weighted differently to optimize for final coreferenceloss functions; error types are shown corresponding to the decisions for each mention.We use a log linear model of the conditional dis-tribution P (a|x) as follows:P (a|x) ?
exp(n?i=1w>f(i, ai, x))where f(i, ai, x) is a feature function that examinesthe coreference decision ai for mention i with doc-ument context x.
When ai = NEW, the featuresfired indicate the suitability of the given mention tobe anaphoric or not; when ai = j for some j, thefeatures express aspects of the pairwise linkage, andcan examine any relevant attributes of the anaphori or the antecedent j, since information about eachmention is contained in x.Inference in this model is efficient: becauselogP (a|x) decomposes linearly over mentions, wecan compute ai = argmaxai P (ai|x) separatelyfor each mention and return the set of coreferencechains implied by these decisions.3.3 LearningDuring learning, we optimize for conditional log-likelihood augmented with a parameterized lossfunction (Durrett et al 2013).
The main compli-cating factor in this process is that the supervisionin coreference consists of a gold clustering C?
de-fined over gold mentions.
This is problematic fortwo reasons: first, because the clustering is definedover gold mentions rather than our system mentions,and second, because a clustering does not specify afull antecedent structure of the sort our model pro-duces.
We can address the first of these problemsby imputing singleton clusters for mentions that donot appear in the gold standard; our system will thensimply learn to put spurious mentions in their ownclusters.
Singletons are always removed before eval-uation because the OntoNotes corpus does not anno-tate them, so in this way we can neatly dispose ofspurious mentions.
To address the lack of explicitantecedents in C?, we simply sum over all possibleantecedent structures licensed by the gold clusters.Formally, we will maximize the conditional log-likelihood of the set A(C?)
of antecedent vectorsa for a document that are consistent with the goldannotation.4 Consistency for an antecedent choiceai under gold clusters C?
is defined as follows:1.
If ai = j, ai is consistent iff mentions i and jare present in C?
and are in the same cluster.2.
If ai = NEW, ai is consistent off mention i isnot present in C?, or it is present in C?
and hasno gold antecedents, or it is present in C?
andnone of its gold antecedents are among the setof system predicted mentions.Given t training examples of the form (xk, C?k),we write the following likelihood function:`(w) =t?k=1log??
?a?A(C?k)P ?(a|xk)?
?+ ?
?w?1where P ?
(a|xk) ?
P (a|xk) exp(l(a,C?k)) withl(a,C?)
being a real-valued loss function.
The loss4Because of this marginalization over latent antecedentchoices, our objective is non-convex.1973here plays an analogous role to the loss in struc-tured max-margin objectives; incorporating it into aconditional likelihood objective is a technique calledsoftmax-margin (Gimpel and Smith, 2010).Our loss function l(a,C?)
is a weighted linearcombination of three error types, examples of whichare shown in Figure 1.
A false anaphor (FA) erroroccurs when ai is chosen to be anaphoric when itshould start a new cluster.
A false new (FN) error oc-curs in the opposite case, when ai wrongly indicatesa new cluster when it should be anaphoric.
Finally,a wrong link (WL) error occurs when the antecedentchosen for ai is the wrong antecedent (but ai is in-deed anaphoric).
Our final parameterized loss func-tion is a weighted sum of the counts of these threeerror types:l(a,C?)
= ?FAFA(a,C?)
+ ?FNFN(a,C?)
+ ?WLWL(a,C?
)where FA(a,C?)
gives the number of false anaphorerrors in prediction a with gold chains C?
(FN andWL are analogous).
By setting ?FA low and ?FNhigh relative to ?WL, we can counterbalance thehigh number of singleton mentions and bias the sys-tem towards making more coreference linkages.
Weset (?FA, ?FN, ?WL) = (0.1, 3.0, 1.0) and ?
=0.001 and optimize the objective using AdaGrad(Duchi et al 2011).4 Easy Victories from Surface FeaturesOur primary goal with this work is to show that ahigh-performance coreference system is attainablewith a small number of feature templates that useonly surface-level information sources.
These fea-tures will be general-purpose and capture linguisticeffects to the point where standard heuristic-drivenfeatures are no longer needed in our system.4.1 SURFACE Features and ConjunctionsOur SURFACE feature set only considers the follow-ing properties of mentions and mention pairs:?
Mention type (nominal, proper, or pronominal)?
The complete string of a mention?
The semantic head of a mention?
The first word and last word of each mentionFeature name CountFeatures on the current mention[ANAPHORIC] + [HEAD WORD] 41371[ANAPHORIC] + [FIRST WORD] 18991[ANAPHORIC] + [LAST WORD] 19184[ANAPHORIC] + [PRECEDING WORD] 54605[ANAPHORIC] + [FOLLOWING WORD] 57239[ANAPHORIC] + [LENGTH] 4304Features on the antecedent[ANTECEDENT HEAD WORD] 57383[ANTECEDENT FIRST WORD] 24239[ANTECEDENT LAST WORD] 23819[ANTECEDENT PRECEDING WORD] 53421[ANTECEDENT FOLLOWING WORD] 55718[ANTECEDENT LENGTH] 4620Features on the pair[EXACT STRING MATCH (T/F)] 47[HEAD MATCH (T/F)] 46[SENTENCE DISTANCE, CAPPED AT 10] 2037[MENTION DISTANCE, CAPPED AT 10] 1680Table 1: Our SURFACE feature set, which exploits asmall number of surface-level mention properties.
Fea-ture counts for each template are computed over the train-ing set, and include features generated by our conjunctionscheme (not explicitly shown in the table; see Figure 2),which yields large numbers of features at varying levelsof expressivity.?
The word immediately preceding and the wordimmediately following a mention?
Mention length, in words?
Two distance measures between mentions(number of sentences and number of mentions)Table 1 shows the SURFACE feature set.
Featuresthat look only at the current mention fire on all de-cisions (ai = j or ai = NEW), whereas featuresthat look at the antecedent in any way (the lattertwo groups of features) only fire on pairwise link-ages (ai 6= NEW).Two conjunctions of each feature are also in-cluded: first with the ?type?
of the mention be-ing resolved (either NOMINAL, PROPER, or, if it ispronominal, the citation form of the pronoun), andthen additionally with the antecedent type (only ifthe feature is over a pairwise link).
This conjunc-tion process is shown in Figure 2.
Note that featuresthat just examine the antecedent will end up with1974[Voters]1generally agree when [they]1...NEW1?a2NEW ?
LEN = 1NEW ?
LEN = 1 ?
[they]ANT.
HEAD = VotersANT.
HEAD = Voters ?
[they]ANT.
HEAD = Voters ?
[they] ?
NOMMENT DIST = 1MENT DIST = 1 ?
[they]MENT DIST = 1 ?
[they] ?
NOMFigure 2: Demonstration of the conjunction scheme weuse.
Each feature on anaphoricity is conjoined with thetype (NOMINAL, PROPER, or the citation form if it is apronoun) of the mention being resolved.
Each feature ona mention pair is additionally conjoined with the types ofthe current and antecedent mentions.conjunctions that examine properties of the currentmention as well, as shown with the ANT.
HEAD fea-ture in the figure.Finally, we found it beneficial for our lexical indi-cator features to only fire on words occurring at least20 times in the training set; for rare words, we usethe part of speech of the word instead.The performance of our system is shown in Ta-ble 2.
We contrast our performance with that ofthe Stanford system (Lee et al(2011), the winnerof the CoNLL 2011 shared task) and the IMS sys-tem (Bjo?rkelund and Farkas (2012), the best publiclyavailable English coreference system).
Despite itssimplicity, our SURFACE system is sufficient to out-perform these sophisticated systems: the Stanfordsystem uses a cascade of ten rule-based sieves eachof which has customized heuristics, and the IMSsystem uses a similarly long pipeline consisting ofa learned referentiality classifier followed by multi-ple resolvers, which are run in sequence and rely onthe outputs of the previous resolvers as features.4.2 Data-Driven versus Heuristic-DrivenFeaturesWhy are the SURFACE features sufficient to givehigh coreference performance, when they do notmake apparent reference to important linguistic phe-nomena?
The main reason is that they actually docapture the same phenomena as standard corefer-MUC B3 CEAFe Avg.STANFORD 60.46 65.48 47.07 57.67IMS 62.15 65.57 46.66 58.13SURFACE 64.39 66.78 49.00 60.06Table 2: Results for our SURFACE system, the STAN-FORD system, and the IMS system on the CoNLL 2011development set.
Complete results are shown in Ta-ble 7.
Despite using limited information sources, our sys-tem is able to substantially outperform the other two, thetwo best publicly-available English coreference systems.Bolded values are significant with p < 0.05 according toa bootstrap resampling test.ence features, just implicitly.
For example, ratherthan having rules targeting person, number, gender,or animacy of mentions, we use conjunctions withpronoun identity, which contains this information.Rather than explicitly writing a feature targeting def-initeness, our indicators on the first word of a men-tion will capture this and other effects.
And finally,rather than targeting centering theory (Grosz et al1995) with rule-based features identifying syntac-tic positions (Stoyanov et al 2010; Haghighi andKlein, 2010), our features on word context can iden-tify configurational clues like whether a mentionis preceded or followed by a verb, and thereforewhether it is likely in subject or object position.5Not only are data-driven features able to capturethe same phenomena as heuristic-driven features,but they do so at a finer level of granularity, and cantherefore model more patterns in the data.
To con-trast these two types of features, we experiment withthree ablated versions of our system, where we re-place data-driven features with their heuristic-drivencounterparts:1.
Instead of using an indicator on the first wordof a mention (1STWORD), we instead firea feature based on that mention?s manually-computed definiteness (DEF).2.
Instead of conjoining features on pronominal-pronominal linkages with the citation form of5Heuristic-driven approaches were historically more appro-priate, since past coreference corpora such as MUC and ACEwere smaller and therefore more prone to overfitting feature-rich models.
However, the OntoNotes dataset contains thou-sands of documents, so having support for features is less of aconcern.1975MUC B3 CEAFe Avg.SURFACE 64.39 66.78 49.00 60.06?1STWORD 63.32 66.22 47.89 59.14+DEF?1STWORD 63.79 66.46 48.35 59.53?PRONCONJ 59.97 63.46 47.94 57.12+AGR?PRONCONJ 63.54 66.10 48.72 59.45?CONTEXT 60.88 64.66 47.60 57.71+POSN?CONTEXT 62.45 65.44 48.08 58.65+DEF+AGR+POSN 64.55 66.93 48.94 60.14Table 3: CoNLL metric scores on the development set,for the three different ablations and replacement featuresdescribed in Section 4.2.
Feature types are described inthe text; + indicates inclusion of that feature class, ?
in-dicates exclusion.
Each individual shallow indicator ap-pears to do as well at capturing its target phenomenon asthe hand-engineered features, while capturing other infor-mation as well.
Moreover, the hand-engineered featuresgive no benefit over the SURFACE system.each pronoun (PRONCONJ), we only conjoinwith a PRONOUN indicator and add featurestargeting the person, number, gender, and an-imacy of the two pronouns (AGR).3.
Instead of using our context features on thepreceding and following word (CONTEXT), weuse manual determinations of when mentionsare in subject, direct object, indirect objection,or oblique position (POSN).All rules for computing person, number, gender, an-imacy, definiteness, and syntactic position are takenfrom the system of Lee et al(2011).Table 3 shows each of the target ablations, as wellas the SURFACE system with the DEF, AGR, andPOSN features added.
While the heuristic-drivenfeature always help over the corresponding ablatedsystem, they cannot do the work of the fine-graineddata-driven features.
Most tellingly, though, none ofthe heuristic-driven features give statistically signifi-cant improvements on top of the data-driven featureswe have already included, indicating that we are atthe point of diminishing returns on modeling thosespecific phenomena.
While this does not precludefurther engineering to take better advantage of othersyntactic constraints, our simple features representan ?easy victory?
on this subtask.5 Uphill Battles on SemanticsIn Section 4, we gave a simple set of features thatyielded a high-performance coreference system; thishigh performance is possible because features tar-geting only superficial properties in a fine-grainedway can actually model complex linguistic con-straints.
However, while our existing features cap-ture syntactic and discourse-level phenomena sur-prisingly well, they are not effective at capturing se-mantic phenomena like type compatibility.
We willshow that due to structural aspects of the coreferenceresolution problem, even a combination of severalshallow semantic features from the literature fails toadequately model semantics.5.1 Analysis of the SURFACE SystemWhat can the SURFACE system resolve correctly,and what errors does it still make?
To answer thisquestion, we will split mentions into several cate-gories based on their observable properties and thegold standard coreference information, and exam-ine our system?s accuracy on each mention subclassin order to more thoroughly characterize its perfor-mance.6 These categories represent important dis-tinctions in terms of the difficulty of mention reso-lution for our system.We first split mentions into three categories bytheir status in the gold standard: singleton (unanno-tated in the OntoNotes corpus), starting a new entitywith at least two mentions, or anaphoric.
It is impor-tant to note that while singletons and mentions start-ing new entities are outwardly similar in that theyhave no antecedents, and the prediction should bethe same in either case (NEW), we treat them as dis-tinct because the factors that impact the coreferencedecision differ in the two cases.
Mentions that startnew clusters are semantically similar to anaphoricmentions, but may be marked by heaviness or by atendency to be named entities, whereas singletonsmay be generic or temporal NPs which might bethought of as coreferent in a loose sense, but are not6This method of analysis is similar to that undertaken inStoyanov et al(2009) and Rahman and Ng (2011b), thoughwe split our mentions along different axes, and can simply eval-uate on accuracy because our decisions do not directly implymultiple links, as they do in binary classification-based systems(Stoyanov et al 2009) or in entity-mention models (Rahmanand Ng, 2011b).1976Nominal/ProperPronominal1st w/head 2nd+ w/headSingleton 99.7% 18.1K 85.5% 7.3K 66.5% 1.7KStarts Entity 98.7% 2.1K 78.9% 0.7K 48.5% 0.3KAnaphoric 7.9% 0.9K 75.5% 3.9K 72.0% 4.4KTable 4: Analysis of our SURFACE system on the de-velopment set.
We characterize each predicted mentionby its status in the gold standard (singleton, starting anew entity, or anaphoric), its type (pronominal or nom-inal/proper), and by whether its head has appeared as thehead of a previous mention.
Each cell shows our sys-tem?s accuracy on that mention class as well as the sizeof the class.
The biggest weakness of our system appearsto be its inability to resolve anaphoric mentions with newheads (bottom-left cell).included in the OntoNotes dataset due to choices inthe annotation standard.Second, we divide mentions by their type,pronominal versus nominal/proper; we then furthersubdivide nominals and propers based on whether ornot the head word of the mention has appeared as thehead of a previous mention in the document.Table 4 shows the results of our analysis.
Ineach cell, we show the fraction of mentions thatwe correctly resolve (i.e., for which we make anantecedence decision consistent with the gold stan-dard), as well as the total number of mentions fallinginto that cell.
First, we observe that there are a sur-prisingly large number of singleton mentions withmisleading head matches to previous mentions (of-ten recurring temporal nouns phrases, like July).The features in our system targeting anaphoricity areuseful for exactly this reason: the more bad headmatches we can rule out based on other criteria, themore strongly we can rely on head match to makecorrect linkages.Our system is most noticeably poor at resolvinganaphoric mentions whose heads have not appearedbefore.
The fact that exact match and head matchare our only recall-oriented features on nominalsand propers is starkly apparent here: when we can-not rely on head match, as is true for this mentionclass, we only resolve 7.9% of anaphoric mentionscorrectly.7 Many of the mentions in this category7There are an additional 346 anaphoric nominal/proper men-tions in the 2nd+ category whose heads only appeared previ-ously as part of a different cluster; we only resolve 1.7% ofcan only be correctly resolved by exploiting worldknowledge, so we will need to include features thatcapture this knowledge in some fashion.5.2 Incorporating Shallow SemanticsAs we were able to incorporate syntax with shal-low features, so too might we hope to incorporatesemantics.
However, the semantic information con-tained even in a coreference corpus of thousandsof documents is insufficient to generalize to unseendata,8 so system designers have turned to exter-nal resources such as semantic classes derived fromWordNet (Soon et al 2001), WordNet hypernymyor synonymy (Stoyanov et al 2010), semantic simi-larity computed from online resources (Ponzetto andStrube, 2006), named entity type features, genderand number match using the dataset of Bergsma andLin (2006), and features from unsupervised clus-ters (Hendrickx and Daelemans, 2007; Durrett et al2013).
In this section, we consider the followingsubset of these information sources:?
WordNet hypernymy and synonymy?
Number and gender data for nominals andpropers from Bergsma and Lin (2006)?
Named entity types?
Latent clusters computed from English Giga-word (Graff et al 2007), where a latent clusterlabel generates each nominal head (excludingpronouns) and a conjunction of its verbal gov-ernor and semantic role, if any (Durrett et al2013).
We use twenty clusters, which includeclusters like president and leader (things whichannounce).Together, we call these the SEM features.
Weshow results from this expansion of the feature set inTable 5.
When using system mentions, the improve-ments are not statistically significant on every met-ric, and are quite marginal given that these featuresadd information that is intuitively central to corefer-ence and otherwise unavailable to the system.
Weexplore the reasons behind this in the next section.these extremely tricky cases correctly.8We experimented with bilexical features on head pairs, butthey did not give statistically significant improvements over theSURFACE features.1977MUC B3 CEAFe Avg.SURFACE 64.39 66.78 49.00 60.06SURFACE+SEM 64.70 67.27 49.28 60.42SURFACE (G) 82.80 74.10 68.33 75.08SURFACE+SEM (G) 84.49 75.65 69.89 76.68Table 5: CoNLL metric scores on the development setfor our SEM features when added on top of our SURFACEfeatures.
We experiment on both system mentions andgold mentions.
Surprisingly, despite the fact that absoluteperformance numbers are much higher on gold mentionsand there is less room for improvement, the semantic fea-tures help much more than they do on system mentions.5.3 Analysis of Semantic FeaturesThe main reason that weak semantic cues are notmore effective is the small fraction of positive coref-erence links present in the training data.
From Ta-ble 4, the number of annotated coreferent spans inthe OntoNotes data is about a factor of five smallerthan the number of system mentions.9 This bothmeans that most NPs are not coreferent, and forthose that are, choosing the correct links is muchmore difficult because of the large number of pos-sible antecedents.
Even head match, which is gen-erally considered a high-precision indicator (Lee etal., 2011), would introduce many spurious corefer-ence arcs if applied too liberally (see Table 4).In light of this fact, a system needs very strongevidence to overcome the default hypothesis that amention is not coreferent, and a weak indicator willhave such a high ?false positive?
rate that it cannotbe relied on (given high weight, this feature woulddo more harm than good, by introducing many falselinkages).To confirm this intuition, we show in the bot-tom part of Table 5 results when we apply these se-mantic features on top of our SURFACE system ongold mentions, where there are no singletons.
In thegold mention setting, we see that the semantic fea-tures give a consistent improvement on every metric.Moreover, if we look at a breakdown of errors, themain improvement the semantic features give us ison resolution of anaphoric nominals with no head9This observation is more general than just our system: themajority of coreference systems, including the winners of theCoNLL shared tasks (Lee et al 2011; Fernandes et al 2012),opt for high mention recall and resolve a relatively large numberof system mentions.match: accuracy on the 1601 mentions that fall intothis category improves from 28.0% to 37.9%.
Onpredicted mentions, by contrast, this category onlyimproves from 7.9% to 12.2%, a much smaller ab-solute improvement and one that comes at the ex-pense of performance on most other resolution class.The one class that does not get worse, singleton pro-nouns, actually improves by a similar 4% margin,indicating that roughly half of the gains we observeare not even necessarily a result of our features do-ing what they were designed to do.Our weak cues do yield some small gains, so thereis hope that better weak indicators of semantic com-patibility could prove more useful.
However, whileextremely high-precision approaches with carefullyengineered features have been shown to be suc-cessful (Rahman and Ng, 2011a; Bansal and Klein,2012; Recasens et al 2013a), we conclude that cap-turing semantics in a data-driven, shallow mannerremains an uphill battle.6 FINAL System and ResultsWhile semantic features ended up giving onlymarginal benefit, we have demonstrated that nev-ertheless our SURFACE system is a state-of-the-artEnglish coreference system.
However, there remaina few natural features that we omitted in order tokeep the system as simple as possible, since theywere orthogonal to the discussion of data-drivenversus heuristic-driven features and do not targetworld knowledge.
Before giving final results, wewill present a small set of additional features thatconsider four additional mention properties beyondthose in Section 4.1:?
Whether two mentions are nested?
Ancestry of each mention head: the depen-dency parent and grandparent POS tags and arcdirections (shown in Figure 3)?
The speaker of each mention?
Number and gender of each mention as deter-mined by Bergsma and Lin (2006)The specific additional features we use are shownin Table 6.
Note that unlike in Section 5, we usethe number and gender information only on the an-tecedent.
Due to our conjunction scheme, both this1978ROOT... sent    it    to  the [president] ... [President Obama] signed ...VBD PRP TODET NN NNP NNP VBDpresidentRTOVBDRObamaLVBDROOTFigure 3: Demonstration of the ancestry extraction pro-cess.
These features capture more sophisticated configu-rational information than our context word features do: inthis example, president is in a characteristic indirect ob-ject position based on its dependency parents, and Obamais the subject of the main verb of the sentence.semantic information and the speaker informationcan apply in a fine-grained way to different pro-nouns, and can therefore improve pronoun resolu-tion substantially; however, these features generallyonly improve pronoun resolution.Full results for our SURFACE and FINAL featuresets are shown in Table 7.
Again, we compare to Leeet al(2011) and Bjo?rkelund and Farkas (2012).10Despite our system?s emphasis on one-pass resolu-tion with as simple a feature set as possible, we areable to outperform even these sophisticated systemsby a wide margin.7 Related WorkMany of the individual features we employ in the FI-NAL feature set have appeared in other coreferencesystems (Bjo?rkelund and Nugues, 2011; Rahmanand Ng, 2011b; Fernandes et al 2012).
However,other authors have often emphasized bilexical fea-tures on head pairs, whereas our features are heavilymonolexical.
For feature conjunctions, other authorshave exploited three classes (Lee et al 2011) or au-tomatically learned conjunction schemes (Fernandeset al 2012; Lassalle and Denis, 2013), but to ourknowledge we are the first to do fine-grained mod-eling of every pronoun.
Inclusion of a hierarchy of10Discrepancies between scores here and those printed inPradhan et al(2012) arise from two sources: improvementsto the system of Lee et al(2011) since the first CoNLL sharedtask, and a fix to the scoring of B3 in the official scorer sinceresults of the two CoNLL shared tasks were released.
Unfor-tunately, because of this bug in the scoring program, directcomparison to the printed results of the other highest-scoringEnglish systems, Fernandes et al(2012) and Martschat et al(2012), is impossible.Feature name CountFeatures of the SURFACE system 418704Features on the current mention[ANAPHORIC] + [CURRENT ANCESTRY] 46047Features on the antecedent[ANTECEDENT ANCESTRY] 53874[ANTECEDENT GENDER] 338[ANTECEDENT NUMBER] 290Features on the pair[HEAD CONTAINED (T/F)] 136[EXACT STRING CONTAINED (T/F)] 133[NESTED (T/F)] 355[DOC TYPE] + [SAME SPEAKER (T/F)] 437[CURRENT ANCESTRY] + [ANT.
ANCESTRY] 2555359Table 6: FINAL feature set; note that this includes theSURFACE feature set.
As with the features of the SUR-FACE system, two conjoined variants of each featureare included: first with the type of the current mention(NOMINAL, PROPER, or the citation form of the pro-noun), then with the types of both mentions in the pair.These conjunctions allow antecedent features on genderand number to impact pronoun resolution, and they al-low speaker match to capture effects like I and you beingcoreferent when the speakers differ.features with regularization also means that we or-ganically get distinctions among different mentiontypes without having to choose a level of granularitya priori, unlike the distinct classifiers employed byDenis and Baldridge (2008).In terms of architecture, many coreference sys-tems operate in a pipelined fashion, making par-tial decisions about coreference or pruning arcsbefore full resolution.
Some systems use sepa-rate rule-based and learning-based passes (Chenand Ng, 2012; Fernandes et al 2012), a seriesof learning-based passes (Bjo?rkelund and Farkas,2012), or referentiality classifiers that prune the setof mentions before resolution (Rahman and Ng,2009; Bjo?rkelund and Farkas, 2012; Recasens etal., 2013b).
By contrast, our system resolves allmentions in one pass and does not need pruning:the SURFACE system can train in less than twohours without any subsampling of coreference arcs,and rule-based pruning of coreference arcs actuallycauses our system to perform less well, since ourfeatures can learn valuable information from thesenegative examples.1979MUC B3 CEAFe Avg.Prec.
Rec.
F1 Prec.
Rec.
F1 Prec.
Rec.
F1 F1CoNLL 2011 Development SetSTANFORD 61.62 59.34 60.46 74.05 58.70 65.48 45.98 48.22 47.07 57.67IMS 66.67 58.20 62.15 77.60 56.77 65.57 42.92 51.11 46.66 58.13SURFACE* 68.42 60.80 64.39 76.57 59.21 66.78 45.30 53.36 49.00 60.06FINAL* 68.97 63.47 66.10 76.58 62.06 68.56 47.32 53.19 50.09 61.58CoNLL 2011 Test SetSTANFORD 60.91 62.13 61.51 70.61 57.31 63.27 45.79 44.56 45.17 56.65IMS 68.15 61.60 64.71 75.97 56.39 64.73 42.30 48.88 45.35 58.26FINAL* 66.81 66.04 66.43 71.07 61.89 66.16 47.37 48.22 47.79 60.13Table 7: CoNLL metric scores for our systems on the CoNLL development and blind test sets, compared to the resultsof Lee et al(2011) (STANFORD) and Bjo?rkelund and Farkas (2012) (IMS).
Starred systems are contributions of thiswork.
Bolded F1 values represent statistically significant improvements over other systems with p < 0.05 using abootstrap resampling test.
Metric values reflect version 5 of the CoNLL scorer.8 ConclusionWe have presented a coreference system that uses asimple, homogeneous set of features in a discrim-inative learning framework to achieve high perfor-mance.
Large numbers of lexicalized, data-drivenfeatures implicitly model linguistic phenomena suchas definiteness and centering, obviating the need forheuristic-driven rules explicitly targeting these samephenomena.
Additional semantic features give onlyslight benefit beyond head match because they donot provide strong enough signals of coreference toimprove performance in the system mention setting;modeling semantic similarity still requires complexoutside information and deep heuristics.Our system, the Berkeley CoreferenceResolution System, is publicly available athttp://nlp.cs.berkeley.edu.AcknowledgmentsThis work was partially supported by BBN un-der DARPA contract HR0011-12-C-0014 and byan NSF fellowship for the first author.
Thanksto Sameer Pradhan for helpful discussions regard-ing the CoNLL scoring program, and thanks to theanonymous reviewers for their insightful comments.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forScoring Coreference Chains.
In Proceedings of theConference on Language Resources and EvaluationWorkshop on Linguistics Coreference.Mohit Bansal and Dan Klein.
2012.
Coreference Seman-tics from Web Features.
In Proceedings of the Associ-ation for Computational Linguistics.Eric Bengtson and Dan Roth.
2008.
Understanding theValue of Features for Coreference Resolution.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing.Shane Bergsma and Dekang Lin.
2006.
BootstrappingPath-Based Pronoun Resolution.
In Proceedings of theConference on Computational Linguistics and the As-sociation for Computational Linguistics.Anders Bjo?rkelund and Richa?rd Farkas.
2012.
Data-driven Multilingual Coreference Resolution using Re-solver Stacking.
In Proceedings of the Joint Confer-ence on Empirical Methods in Natural Language Pro-ceedings and Conference on Computational NaturalLanguage Learning - Shared Task.Anders Bjo?rkelund and Pierre Nugues.
2011.
ExploringLexicalized Features for Coreference Resolution.
InProceedings of the Conference on Computational Nat-ural Language Learning: Shared Task.Jie Cai and Michael Strube.
2010.
Evaluation Metrics forEnd-to-End Coreference Resolution Systems.
In Pro-ceedings of the Special Interest Group on Discourseand Dialogue.Chen Chen and Vincent Ng.
2012.
Combining the Bestof Two Worlds: A Hybrid Approach to MultilingualCoreference Resolution.
In Proceedings of the JointConference on Empirical Methods in Natural Lan-guage Proceedings and Conference on ComputationalNatural Language Learning - Shared Task.Pascal Denis and Jason Baldridge.
2008.
SpecializedModels and Ranking for Coreference Resolution.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.1980John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
Journal of MachineLearning Research, 12:2121?2159, July.Greg Durrett, David Hall, and Dan Klein.
2013.
Decen-tralized Entity-Level Modeling for Coreference Reso-lution.
In Proceedings of the Association for Compu-tational Linguistics.Eraldo Rezende Fernandes, C?
?cero Nogueira dos Santos,and Ruy Luiz Milidiu?.
2012.
Latent Structure Per-ceptron with Feature Induction for Unrestricted Coref-erence Resolution.
In Proceedings of the Joint Con-ference on Empirical Methods in Natural LanguageProceedings and Conference on Computational Nat-ural Language Learning - Shared Task.Kevin Gimpel and Noah A. Smith.
2010.
Softmax-Margin CRFs: Training Log-Linear Models with CostFunctions.
In Proceedings of the North AmericanChapter for the Association for Computational Lin-guistics.David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.2007.
English Gigaword Third Edition.
LinguisticData Consortium, Catalog Number LDC2007T07.Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.1995.
Centering: A Framework for Modeling the Lo-cal Coherence of Discourse.
Computational Linguis-tics, 21(2):203?225, June.Aria Haghighi and Dan Klein.
2009.
Simple CoreferenceResolution with Rich Syntactic and Semantic Features.In Proceedings of Empirical Methods in Natural Lan-guage Processing.Aria Haghighi and Dan Klein.
2010.
Coreference Res-olution in a Modular, Entity-Centered Model.
In Pro-ceedings of the North American Chapter of the Asso-ciation for Computational Linguistics.Iris Hendrickx and Walter Daelemans, 2007.
Adding Se-mantic Information: Unsupervised Clusters for Coref-erence Resolution.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:The 90% Solution.
In Proceedings of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Short Papers.Emmanuel Lassalle and Pascal Denis.
2013.
ImprovingPairwise Coreference Models Through Feature SpaceHierarchy Learning.
In Proceedings of the Associationfor Computational Linguistics.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s Multi-Pass Sieve Coreference ResolutionSystem at the CoNLL-2011 Shared Task.
In Proceed-ings of the Conference on Computational Natural Lan-guage Learning: Shared Task.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
AMention-Synchronous Coreference Resolution Algo-rithm Based on the Bell Tree.
In Proceedings of theAssociation for Computational Linguistics.Xiaoqiang Luo.
2005.
On Coreference Resolution Per-formance Metrics.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing.Sebastian Martschat, Jie Cai, Samuel Broscheit, E?vaMu?jdricza-Maydt, and Michael Strube.
2012.
AMultigraph Model for Coreference Resolution.
InProceedings of the Joint Conference on EmpiricalMethods in Natural Language Proceedings and Con-ference on Computational Natural Language Learning- Shared Task.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting Semantic Role Labeling, WordNet andWikipedia for Coreference Resolution.
In Proceed-ings of the North American Chapter of the Associationof Computational Linguistics.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling Unre-stricted Coreference in OntoNotes.
In Proceedings ofthe Conference on Computational Natural LanguageLearning: Shared Task.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
CoNLL-2012 Shared Task: Modeling Multilingual Unre-stricted Coreference in OntoNotes.
In Joint Confer-ence on EMNLP and CoNLL - Shared Task.Altaf Rahman and Vincent Ng.
2009.
Supervised Mod-els for Coreference Resolution.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing.Altaf Rahman and Vincent Ng.
2011a.
CoreferenceResolution with World Knowledge.
In Proceedingsof the Association for Computational Linguistics: Hu-man Language Technologies.Altaf Rahman and Vincent Ng.
2011b.
Narrowingthe Modeling Gap: A Cluster-Ranking Approach toCoreference Resolution.
Journal of Artificial Intelli-gence Research, 40(1):469?521, January.Marta Recasens, Matthew Can, and Daniel Jurafsky.2013a.
Same Referent, Different Words: Unsuper-vised Mining of Opaque Coreferent Mentions.
In Pro-ceedings of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013b.
The Life and Death of Dis-course Entities: Identifying Singleton Mentions.
In1981Proceedings of the North American Chapter of the As-sociation for Computational Linguistics: Human Lan-guage Technologies.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A Machine Learning Approach to Coref-erence Resolution of Noun Phrases.
ComputationalLinguistics, 27(4):521?544, December.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in Noun PhraseCoreference Resolution: Making Sense of the State-of-the-Art.
In Proceedings of the Association for Com-putational Linguistics.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
Coref-erence Resolution with Reconcile.
In Proceedings ofthe Association for Computational Linguistics: ShortPapers.Yannick Versley, Simone Paolo Ponzetto, Massimo Poe-sio, Vladimir Eidelman, Alan Jern, Jason Smith, Xi-aofeng Yang, and Alessandro Moschitti.
2008.
BART:A Modular Toolkit for Coreference Resolution.
InProceedings of the Association for Computational Lin-guistics: Demo Session.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A Model-Theoretic Coreference Scoring Scheme.
In Proceed-ings of the Conference on Message Understanding.1982
