Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 595?602,Sydney, July 2006. c?2006 Association for Computational LinguisticsReinforcing English Countability Prediction with One Countability perDiscourse PropertyRyo NagataHyogo University of Teacher Education6731494, Japanrnagata@hyogo-u.ac.jpAtsuo KawaiMie University5148507, Japankawai@ai.info.mie-u.ac.jpKoichiro MorihiroHyogo University of Teacher Education6731494, Japanmori@hyogo-u.ac.jpNaoki IsuMie University5148507, Japanisu@ai.info.mie-u.ac.jpAbstractCountability of English nouns is impor-tant in various natural language process-ing tasks.
It especially plays an importantrole in machine translation since it deter-mines the range of possible determiners.This paper proposes a method for reinforc-ing countability prediction by introducinga novel concept called one countability perdiscourse.
It claims that when a nounappears more than once in a discourse,they will all share the same countability inthe discourse.
The basic idea of the pro-posed method is that mispredictions canbe correctly overridden using efficientlythe one countability per discourse prop-erty.
Experiments show that the proposedmethod successfully reinforces countabil-ity prediction and outperforms other meth-ods used for comparison.1 IntroductionCountability of English nouns is important in var-ious natural language processing tasks.
It is par-ticularly important in machine translation froma source language that does not have an articlesystem similar to that of English, such as Chi-nese and Japanese, into English since it determinesthe range of possible determiners including arti-cles.
It also plays an important role in determiningwhether a noun can take singular and plural forms.Another useful application is to detect errors in ar-ticle usage and singular/plural usage in the writingof second language learners.
Given countability,these errors can be detected in many cases.
Forexample, an error can be detected from ?We havea furniture.?
given that the noun furniture is un-countable since uncountable nouns do not toleratethe indefinite article.Because of the wide range of applications, re-searchers have done a lot of work related tocountability.
Baldwin and Bond (2003a; 2003b)have proposed a method for automatically learn-ing countability from corpus data.
Lapata andKeller (2005) and Peng and Araki (2005) haveproposed web-based models for learning count-ability.
Others including Bond and Vatikiotis-Bateson (2002) and O?Hara et al (2003) use on-tology to determine countability.In the application to error detection, re-searchers have explored alternative approachessince sources of evidence for determining count-ability are limited compared to other applications.Articles and the singular/plural distinction, whichare informative for countability, cannot be used incountability prediction aiming at detecting errorsin article usage and singular/plural usage.
Return-ing to the previous example, the countability of thenoun furniture cannot be determined as uncount-able by the indefinite article; first, its countabil-ity has to be predicted without the indefinite arti-cle, and only then whether or not it tolerates theindefinite article is examined using the predictedcountability.
Also, unlike in machine translation,the source language is not given in the writing ofsecond language learners such as essays, whichmeans that information available is limited.To overcome these limitations, Nagataet al (2005a) have proposed a method forpredicting countability that relies solely on words(except articles and other determiners) surround-ing the target noun.
Nagata et al (2005b) haveshown that the method is effective to detectingerrors in article usage and singular/plural usage inthe writing of Japanese learners of English.
They595also have shown that it is likely that performanceof the error detection will improve as accuracy ofthe countability prediction increases since most offalse positives are due to mispredictions.In this paper, we propose a method for reinforc-ing countability prediction by introducing a novelconcept called one countability per discourse thatis an extension of one sense per discourse pro-posed by Gale et al (1992).
It claims that whena noun appears more than once in a discourse,they will all share the same countability in the dis-course.
The basic idea of the proposed methodis that initially mispredicted countability can becorrected using efficiently the one countability perdiscourse property.The next section introduces the one countabilityper discourse concept and shows that it can be agood source of evidence for predicting countabil-ity.
Section 3 discusses how it can be efficientlyexploited to predict countability.
Section 4 de-scribes the proposed method.
Section 5 describesexperiments conducted to evaluate the proposedmethod and discusses the results.2 One Countability per DiscourseOne countability per discourse is an extensionof one sense per discourse proposed by Galeet al (1992).
One sense per discourse claims thatwhen a polysemous word appears more than oncein a discourse it is likely that they will all sharethe same sense.
Yarowsky (1995) tested the claimon about 37,000 examples and found that when apolysemous word appeared more than once in adiscourse, they took on the majority sense for thediscourse 99.8% of the time on average.Based on one sense per discourse, we hypothe-size that when a noun appears more than once in adiscourse, they will all share the same countabilityin the discourse, that is, one countability per dis-course.
The motivation for this hypothesis is thatif one sense per discourse is satisfied, so is onecountability per discourse because countability isoften determined by word sense.
For example, ifthe noun paper appears in a discourse and it hasthe sense of newspaper, which is countable, therest of papers in the discourse also have the samesense according to one sense per discourse, andthus they are also countable.We tested this hypothesis on a set of nouns11The conditions of this test are shown in Section 5.
Notethat although the source of the data is the same as in Section 5,as Yarowsky (1995) did.
We calculated how ac-curately the majority countability for each dis-course predicted countability of the nouns in thediscourse when they appeared more than once.
Ifthe one countability per discourse property is al-ways satisfied, the majority countability for eachdiscourse should predict countability with the ac-curacy of 100%.
In other others, the obtained ac-curacy represents how often the one countabilityper discourse property is satisfied.Table 1 shows the results.
?MCD?
in Table 1stands for Majority Countability for Discourse andits corresponding column denotes accuracy wherecountability of individual nouns was predictedby the majority countability for the discourse inwhich they appeared.
Also, ?Baseline?
denotesaccuracy where it was predicted by the majoritycountability for the whole corpus used in this test.Table 1: Accuracy obtained by Majority Count-ability for DiscourseTarget noun MCD Baselineadvantage 0.772 0.618aid 0.943 0.671authority 0.864 0.771building 0.850 0.811cover 0.926 0.537detail 0.829 0.763discipline 0.877 0.652duty 0.839 0.714football 0.938 0.930gold 0.929 0.929hair 0.914 0.902improvement 0.735 0.685necessity 0.769 0.590paper 0.807 0.647reason 0.858 0.822sausage 0.821 0.750sleep 0.901 0.765stomach 0.778 0.778study 0.824 0.781truth 0.783 0.724use 0.877 0.871work 0.861 0.777worry 0.871 0.843Average 0.851 0.754Table 1 reveals that the one countability per dis-discourses in which the target noun appears only once areexcluded from this test unlike in Section 5.596course property is a good source of evidence forpredicting countability compared to the baselinewhile it is not as strong as the one sense per dis-course property is.
It also reveals that the tendencyof one countability per discourse varies from nounto noun.
For instance, nouns such as aid andcover show a strong tendency while others suchas advantage and improvement do not.
On aver-age, ?MCD?
achieves an improvement of approx-imately 10% in accuracy over the baseline.Having observed the results, it is reasonable toexploit the one countability per discourse prop-erty for predicting countability.
In order to doit, however, the following two questions shouldbe addressed.
First, how can the majority count-ability be obtained from a novel discourse?
Sinceour intention is to predict values of countability ofinstances in a novel discourse, none of them areknown.
Second, even if the majority countabilityis known, how can it be efficiently exploited forpredicting countability?
Although we could sim-ply predict countability of individual instances ofa target noun in a discourse by the majority count-ability for the discourse, it is highly possible thatthis simple method will cause side effects consid-ering the results in Table 1.
These two questionsare addressed in the next section.3 Basic Idea3.1 How Can the Majority Countability beObtained from a Novel Discourse?Although we do not know the true value of the ma-jority countability for a novel discourse, we canat least estimate it because we have a method forpredicting countability to be reinforced by the pro-posed method.
That is, we can predict countabilityof the target noun in a novel discourse using themethod.
Simply counting the results would givethe majority countability for it.Here, we should note that countability of eachinstance is not the true value but a predicted one.Considering this fact, it is sensible to set a cer-tain criterion in order to filter out spurious predic-tions.
Fortunately, most methods based on ma-chine learning algorithms give predictions withtheir confidences.
We use the confidences as thecriterion.
Namely, we only take account of predic-tions whose confidences are greater than a certainthreshold when we estimate the majority count-ability for a novel discourse.3.2 How Can the Majority Countability beEfficiently Exploited?In order to efficiently exploit the one countabil-ity per discourse property, we treat the majoritycountability for each discourse as a feature in ad-dition to other features extracted from instances ofthe target noun.
Doing so, we let a machine learn-ing algorithm decide which features are relevant tothe prediction.
If the majority countability featureis relevant, the machine learning algorithm shouldgive a high weight to it compared to others.To see this, let us suppose that we have a setof discourses in which instances of the target nounare tagged with their countability (either countableor uncountable2) for the moment; we will describehow to obtain it in Subsection 4.1.
For each dis-course, we can know its majority countability bycounting the numbers of countables and uncount-ables.
We can also generate a model for predictingcountability from the set of discourses using a ma-chine learning algorithm.
All we have to do is toextract a set of training data from the tagged in-stances and to apply a machine learning algorithmto it.
This is where the majority countability fea-ture comes in.
The majority countability for eachinstance is added to its corresponding training dataas a feature to create a new set of training data be-fore applying a machine learning algorithm; thena machine learning algorithm is applied to the newset.
The resulting model takes the majority count-ability feature into account as well as the other fea-tures when making predictions.It is important to exercise some care in count-ing the majority countability for each discourse.Note that one countability per discourse is alwayssatisfied in discourses where the target noun ap-pears only once.
This suggests that it is highlypossible that the resulting model too strongly fa-vors the majority countability feature.
To avoidthis, we could split the discourses into two sets,one for where the target noun appears only onceand one for where it appears more than once, andtrain a model on each set.
However, we do nottake this strategy because we want to use as muchdata as possible for training.
As a compromise,we approximate the majority countability for dis-courses where the target noun appears only onceto the value unknown.2This paper concentrates solely on countable and un-countable nouns, since they account for the vast majority ofnouns (Lapata and Keller, 2005).597    yesyesyesyesnononono yes noCOUNTABLEmodified by a little??COUNTABLEUNCOUNTABLE?
UNCOUNTABLEplural?modified by one of the wordsin Table 2(a)?modified by one of the wordsin Table 2(b)?modified by one of the wordsin Table 2(c)?Figure 1: Framework of the tagging rulesTable 2: Words used in the tagging rules(a) (b) (c)the indenite article much the denite articleanother less demonstrative adjectivesone enough possessive adjectiveseach sufficient interrogative adjectives?
?
quantiers?
?
?s genitives4 Proposed Method4.1 Generating Training DataAs discussed in Subsection 3.2, training data areneeded to exploit the one countability per dis-course property.
In other words, the proposedmethod requires a set of discourses in which in-stances of the target noun are tagged with theircountability.
Fortunately, Nagata et al (2005b)have proposed a method for tagging nouns withtheir countability.
This paper follows it to gener-ate training data.To generate training data, first, instances of thetarget noun used as a head noun are collected froma corpus with their surrounding words.
This can besimply done by an existing chunker or parser.Second, the collected instances are tagged witheither countable or uncountable by tagging rules.For example, the underlined paper:... read a paper in the morning ...is tagged as... read a paper/countable in the morning ...because it is modified by the indefinite article.Figure 1 and Table 2 represent the tagging rulesbased on Nagata et al (2005b)?s method.
Fig-ure 1 shows the framework of the tagging rules.Each node in Figure 1 represents a question ap-plied to the instance in question.
For instance, theroot node reads ?Is the instance in question plu-ral??.
Each leaf represents a result of the classifi-cation.
For instance, if the answer is ?yes?
at theroot node, the instance in question is tagged withcountable.
Otherwise, the question at the lowernode is applied and so on.
The tagging rules donot classify instances in some cases.
These unclas-sified instances are tagged with the symbol ??
?.Unfortunately, they cannot readily be included intraining data.
For simplicity of implementation,they are excluded from training data (we will dis-cuss the use of these excluded data in Section 6).Note that the tagging rules cannot be used forcountability prediction aiming at detecting errorsin article usage and singular/plural usage.
Thereason is that they are useless in error detectionwhere whether determiners and the singular/pluraldistinction are correct or not is unknown.
Obvi-ously, the tagging rules assume that the target textcontains no error.Third, features are extracted from each instance.As the features, the following three types of con-textual cues are used: (i) words in the noun phrasethat the instance heads, (ii) three words to the leftof the noun phrase, and (iii) three words to itsright.
Here, the words in Table 2 are excluded.Also, function words (except prepositions) suchas pronouns, cardinal and quasi-cardinal numer-598als, and the target noun are excluded.
All wordsare reduced to their morphological stem and con-verted entirely to lower case when collected.
Inaddition to the features, the majority countabilityis used as a feature.
For each discourse, the num-bers of countables and uncountables are countedto obtain its majority countability.
In case of ties,it is set to unknown.
Also, it is set to unknownwhen only one instance appears in the discourseas explained in Subsection 3.2.To illustrate feature extraction, let us considerthe following discourse (target noun: paper):... writing a new paper/countable in his room ...... read papers/countable with ...The discourse would give a set of features:-3=write, NP=new, +3=in, +3=room, MC=c-3=read, +3=with, MC=cwhere ?MC=c?
denotes that the majority count-ability for the discourse is countable.
In this exam-ple (and in the following examples), the featuresare represented in a somewhat simplified mannerfor the purpose of illustration.
In practice, featuresare represented as a vector.Finally, the features are stored in a file with theircorresponding countability as training data.
Eachpiece of training data would be as follows:-3=read, +3=with, MC=c, LABEL=cwhere ?LABEL=c?
denotes that the countabilityfor the instance is countable.4.2 Model GenerationThe model used in the proposed method can be re-garded as a function.
It takes as its input a featurevector extracted from the instance in question andpredicts countability (either countable or uncount-able).
Formally, 	where  ,  , anddenote the model, the feature vector, and ,respectively; here, 0 and 1 correspond to count-able and uncountable, respectively.Given the specification, almost any kind of ma-chine learning algorithm cab be used to generatethe model used in the proposed method.
In thispaper, the Maximum Entropy (ME) algorithm isused which has been shown to be effective in awide variety of natural language processing tasks.Model generation is done by applying the MEalgorithm to the training data.
The resulting modeltakes account of the features including the major-ity countability feature and is used for reinforcingcountability prediction.4.3 Reinforcing Countability PredictionBefore explaining the reinforcement procedure, letus introduce the following discourse for illustra-tion (target noun: paper):... writing paper in room ... wrote paper in ...... submitted paper to ...Note that articles and the singular/plural distinc-tion are deliberately removed from the discourse.This kind of situation can happen in machinetranslation from a source language that does nothave articles and the singular/plural distinction3.The situation is similar in the writing of secondlanguage learners of English since they often omitarticles and the singular/plural distinction or useimproper ones.
Here, suppose that the true valuesof the countability for all instances are countable.A method to be reinforced by the proposedmethod would predict countability as follows:... writing paper/countable (0.97) in room ...... wrote paper/countable (0.98) in ...... submitted paper/uncountable (0.57) to ...where the numbers in brackets denote the confi-dences given by the method.
The third instance ismistakenly predicted as uncountable4.Now let us move on to the reinforcement pro-cedure.
It is divided into three steps.
First, themajority countability for the discourse in questionis estimated by counting the numbers of the pre-dicted countables and uncountables whose confi-dences are greater than a certain threshold.
In caseof ties, the values of the majority countability isset to unknown.
In the above example, the major-ity countability for the discourse is estimated to becountable when the threshold is set to (twocountables).
Second, features explained in Sub-section 4.1 are extracted from each instance.
Asfor the majority countability feature, the estimatedone is used.
Returning to the above example, thethree instances would give a set of features:-3=write, +3=in, +3=room, MC=c,-3=write, +3=in, MC=c,-3=submit, +3=to, MC=c.Finally, the model generated in Subsection 4.2is applied to the features to predict countability.Because of the majority countability feature, it3For instance, the Japanese language does not have an ar-ticle system similar to that of English, neither does it markthe singular/plural distinction.4The reason would be that the contextual cues did not ap-pear in the training data used in the method.599is likely that previous mispredictions are overrid-den by correct ones.
In the above example, thethird one would be correctly overridden by count-able because of the majority countability feature(MC=c) that is informative for the instance beingcountable.5 Experiments5.1 Experimental ConditionsIn the experiments, we chose Nagataet al (2005a)?s method as the one to be re-inforced by the proposed method.
In thismethod, the decision list (DL) learning algo-rithm (Yarowsky, 1995) is used.
However, weused the ME algorithm because we found that themethod with the ME algorithm instead of the DLlearning algorithm performed better when trainedon the same training data.As the target noun, we selected 23 nouns thatwere also used in Nagata et al (2005a)?s experi-ments.
They are exemplified as nouns that are usedas both countable and uncountable by Huddlestonand Pullum (2002).Training data were generated from the writ-ten part of the British National Corpus (Burnard,1995).
A text tagged with the text tags was usedas a discourse unit.
From the corpus, 314 texts,which amounted to about 10% of all texts, wererandomly taken to obtain test data.
The rest oftexts were used to generate training data.We evaluated performance of prediction by ac-curacy.
We defined accuracy by the ratio of thenumber of correct predictions to that of instancesof the target noun in the test data.5.2 Experimental ProceduresFirst, we generated training data for each targetnoun from the texts using the tagging rules ex-plained in Subsection 4.1.
We used the OAK sys-tem5 to extract noun phrases and their heads.
Ofthe extracted instances, we excluded those that hadno contextual cues from the training data (and alsothe test data).
We also generated another set oftraining data by removing the majority countabil-ity features from them.
This set of training datawas used for comparison.Second, we obtained test data by applying thetagging rules described in Subsection 4.1 to eachinstance of the target noun in the 314 texts.
Na-gata et al (2005b) showed that the tagging rules5http://www.cs.nyu.edu/ sekine/PROJECT/OAK/achieved an accuracy of 0.997 in the texts thatcontained no errors.
Considering these results, weused the tagging rules to obtain test data.
Instancestagged with ???
were excluded in the experiments.Third, we applied the ME algorithm6 to thetraining data without the majority countability fea-ture.
Using the resulting model, countability ofthe target nouns in the test data was predicted.Then, the predictions were reinforced by the pro-posed method.
The threshold to filter out spu-rious predictions was set to .
For compar-ison, the predictions obtained by the ME modelwere simply replaced with the estimated majoritycountability for each discourse.
In this method, theoriginal predictions were used when the estimatedmajority countability was unknown.
Also, Nagataet al (2005a)?s method that was based on the DLlearning algorithm was implemented for compari-son.Finally, we calculated accuracy of each method.In addition to the results, we evaluated the baselineon the same test data where all predictions weredone by the majority countability for the wholecorpus (training data).5.3 Experimental Results and DiscussionTable 3 shows the accuracies7.
?ME?
and ?Pro-posed?
in Table 3 refer to accuracies of the MEmodel and the ME model reinforced by the pro-posed method, respectively.
?ME+MCD?
refersto accuracy obtained by replacing predictions ofthe ME model with the estimated majority count-ability for each discourse.
Also, ?DL?
refers toaccuracy of the DL-based method.Table 3 shows that the three ME-based meth-ods (?Proposed?, ?ME?, and ?ME+MCD?)
per-form better than ?DL?
and the baseline.
Espe-cially, ?Proposed?
outperforms the other methodsin most of the target nouns.Figure 2 summarizes the comparison betweenthe three ME-based methods.
Each plot in Fig-ure 2 represents each target noun.
The horizon-tal and vertical axises correspond to accuracy of?ME?
and that of ?Proposed?
(or ?ME+MCD?),respectively.
The diagonal line corresponds to theline ff .
So if ?Proposed?
(or ?ME+MCD?
)achieved no improvement at all over ?ME?, all the6All ME models were generated using theopennlp.maxent package (http://maxent.sourceforge.net/).7The baseline in Table 3 is different from that in Table 1because discourses where the target noun appears only onceare not taken into account in Table 1.600Table 3: Experimental resultsTarget noun Freq.
Baseline Proposed ME ME+MCD DLadvantage 570 0.604 0.933 0.921 0.811 0.882aid 385 0.665 0.909 0.873 0.896 0.722authority 1162 0.760 0.857 0.851 0.840 0.804building 1114 0.803 0.848 0.842 0.829 0.807cover 210 0.567 0.790 0.757 0.800 0.714detail 1157 0.760 0.906 0.904 0.821 0.869discipline 204 0.593 0.804 0.745 0.750 0.696duty 570 0.700 0.879 0.877 0.828 0.847football 281 0.907 0.925 0.907 0.925 0.911gold 140 0.929 0.929 0.929 0.921 0.929hair 448 0.902 0.908 0.902 0.904 0.904improvement 362 0.696 0.735 0.715 0.685 0.738necessity 83 0.566 0.831 0.843 0.831 0.783paper 1266 0.642 0.859 0.836 0.808 0.839reason 1163 0.824 0.885 0.893 0.834 0.843sausage 45 0.778 0.778 0.733 0.756 0.778sleep 107 0.776 0.925 0.897 0.897 0.813stomach 30 0.633 0.800 0.800 0.800 0.733study 1162 0.779 0.832 0.819 0.782 0.808truth 264 0.720 0.761 0.777 0.765 0.731use 1390 0.869 0.879 0.863 0.871 0.873work 3002 0.778 0.858 0.842 0.837 0.806worry 119 0.798 0.874 0.840 0.849 0.849Average 662 0.741 0.857 0.842 0.828 0.8120.70.80.910.7  0.8  0.9  1Accuracy(Proposed/ME+MCD)Accuracy (ME)ME vs ProposedME vs ME+MCDFigure 2: Comparison between ?ME?
and ?Pro-posed/ME+MCD?
in each target nounplots would be on the line.
Plots above the linemean improvement over ?ME?
and the distancefrom the line expresses the amount of improve-ment.
Plots below the line mean the opposite.Figure 2 clearly shows that most of the plots ( fi )corresponding to the comparison between ?ME?and ?Proposed?
are above the line.
This meansthat the proposed method successfully reinforced?ME?
in most of the target nouns.
Indeed, the av-erage accuracy of ?Proposed?
is significantly su-perior to that of ?ME?
at the 99% confidence level(paired t-test).
This improvement is close to thatof one sense per discourse (Yarowsky, 1995) (im-provement ranging from 1.3% to 1.7%), whichseems to be a sensible upper bound of the pro-posed method.
By contrast, about half of theplots ( fl ) corresponding to the comparison between?ME?
and ?ME+MCD?
are below the line.From these results, it follows that the one count-ability per discourse property is a good source ofevidence for predicting countability, but it is cru-cial to devise a way of exploiting the property aswe did in this paper.
Namely, simply replacingoriginal predictions with the majority countabil-ity for the discourse causes side effects, whichhas been already suggested in Table 1.
This is601also exemplified as follows.
Suppose that sev-eral instances of the target noun advantage ap-pear in a discourse and that its majority countablyis countable.
Further suppose that an idiomaticphrase ?take advantage of?
of which countabilityis uncountable happens to appear in it.
On onehand, simply replacing all the predictions with itsmajority countability (countable) would lead to amisprediction for the idiomatic phrase even if theoriginal prediction is correct.
On the other hand,the proposed method would correctly predict thecountability because the contextual cues stronglyindicate that it is uncountable.6 ConclusionsThis paper has proposed a method for reinforc-ing English countability prediction by introducingone countability per discourse.
The experimentshave shown that the proposed method successfullyoverrode original mispredictions using efficientlythe one countability per discourse property.
Theyalso have shown that it outperformed other meth-ods used for comparison.
From these results, weconclude that the proposed method is effective inreinforcing English countability prediction.In addition, the proposed method has two ad-vantages.
The first is its applicability.
It can re-inforce almost any earlier method.
Even to hand-coded rules, it can be applied as long as they givepredictions with their confidences.
This furthergives an additional advantage.
Recall that theinstances tagged with ???
by the tagging rulesare discarded when training data are generatedas described in Subsection 4.1.
These instancescan be retagged with their countability by usingthe proposed method and some kind of bootstrap-ping (Yarowsky, 1995).
This means increase intraining data, which might eventually result in fur-ther improvement.
The second is that the proposedmethod is unsupervised.
It requires no human in-tervention to reinforce countability prediction.For future work, we will investigate what mod-els are most appropriate for exploiting the onecountability per discourse property.
We will alsoexplore a method for including instances taggedwith ???
in training data by using the proposedmethod and bootstrapping.AcknowledgmentsThe authors would like to thank Satoshi Sekinewho has developed the OAK System.
The authorsalso would like to thank three anonymous review-ers for their useful comments on this paper.ReferencesT.
Baldwin and F. Bond.
2003a.
Learning the count-ability of English nouns from corpus data.
In Proc.of 41st Annual Meeting of ACL, pages 463?470.T.
Baldwin and F. Bond.
2003b.
A plethora of meth-ods for learning English countability.
In Proc.
of2003 Conference on Empirical Methods in NaturalLanguage Processing, pages 73?80.F.
Bond and C. Vatikiotis-Bateson.
2002.
Using anontology to determine English countability.
In Proc.of 19th International Conference on ComputationalLinguistics, pages 99?105.L.
Burnard.
1995.
Users Reference Guide for theBritish National Corpus.
version 1.0.
Oxford Uni-versity Computing Services, Oxford.W.A.
Gale, K.W.
Church, and D. Yarowsky.
1992.
Onesense per discourse.
In Proc.
of 4th DARPA Speechand Natural Language Workshop, pages 233?237.R.
Huddleston and G.K. Pullum.
2002.
The Cam-bridge Grammar of the English Language.
Cam-bridge University Press, Cambridge.M.
Lapata and F. Keller.
2005.
Web-based models fornatural language processing.
ACM Transactions onSpeech and Language Processing, 2(1):1?31.R.
Nagata, F. Masui, A. Kawai, and N. Isu.
2005a.
Anunsupervised method for distinguishing mass andcount noun in context.
In Proc.
of 6th InternationalWorkshop on Computational Semantics, pages 213?224.R.
Nagata, T. Wakana, F. Masui, A. Kawai, and N. Isu.2005b.
Detecting article errors based on the masscount distinction.
In Proc.
of 2nd International JointConference on Natural Language Processing, pages815?826.T.
O?Hara, N. Salay, M. Witbrock, D. Schneider,B.
Aldag, S. Bertolo, K. Panton, F. Lehmann, J. Cur-tis, M. Smith, D. Baxter, and P. Wagner.
2003.
In-ducing criteria for mass noun lexical mappings usingthe Cyc KB, and its extension to WordNet.
In Proc.of 5th International Workshop on Computational Se-mantics, pages 425?441.J.
Peng and K. Araki.
2005.
Detecting the countabilityof English compound nouns using web-based mod-els.
In Companion Volume to Proc.
of 2nd Interna-tional Joint Conference on Natural Language Pro-cessing, pages 105?109.D.
Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proc.
of33rd Annual Meeting of ACL, pages 189?196.602
