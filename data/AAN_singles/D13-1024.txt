Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 233?243,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsStructured Penalties for Log-linear Language ModelsAnil Nelakanti,*?
Ce?dric Archambeau,* Julien Mairal,?
Francis Bach,?
Guillaume Bouchard**Xerox Research Centre Europe, Grenoble, France?INRIA-LEAR Project-Team, Grenoble, France?INRIA-SIERRA Project-Team, Paris, Francefirstname.lastname@xrce.xerox.com firstname.lastname@inria.frAbstractLanguage models can be formalized as log-linear regression models where the input fea-tures represent previously observed contextsup to a certain length m. The complexityof existing algorithms to learn the parametersby maximum likelihood scale linearly in nd,where n is the length of the training corpusand d is the number of observed features.
Wepresent a model that grows logarithmicallyin d, making it possible to efficiently leveragelonger contexts.
We account for the sequen-tial structure of natural language using tree-structured penalized objectives to avoid over-fitting and achieve better generalization.1 IntroductionLanguage models are crucial parts of advanced nat-ural language processing pipelines, such as speechrecognition (Burget et al 2007), machine trans-lation (Chang and Collins, 2011), or informationretrieval (Vargas et al 2012).
When a sequenceof symbols is observed, a language model pre-dicts the probability of occurrence of the next sym-bol in the sequence.
Models based on so-calledback-off smoothing have shown good predictivepower (Goodman, 2001).
In particular, Kneser-Ney(KN) and its variants (Kneser and Ney, 1995) arestill achieving state-of-the-art results for more than adecade after they were originally proposed.
Smooth-ing methods are in fact clever heuristics that requiretuning parameters in an ad-hoc fashion.
Hence,more principled ways of learning language mod-els have been proposed based on maximum en-tropy (Chen and Rosenfeld, 2000) or conditionalrandom fields (Roark et al 2004), or by adoptinga Bayesian approach (Wood et al 2009).In this paper, we focus on penalized maxi-mum likelihood estimation in log-linear models.In contrast to language models based on unstruc-tured norms such as `2 (quadratic penalties) or`1 (absolute discounting), we use tree-structurednorms (Zhao et al 2009; Jenatton et al 2011).Structured penalties have been successfully appliedto various NLP tasks, including chunking and namedentity recognition (Martins et al 2011), but not lan-guage modelling.
Such penalties are particularlywell-suited to this problem as they mimic the nestednature of word contexts.
However, existing optimiz-ing techniques are not scalable for large contexts m.In this work, we show that structured tree normsprovide an efficient framework for language mod-elling.
For a special case of these tree norms, weobtain an memory-efficient learning algorithm forlog-linear language models.
Furthermore, we aslogive the first efficient learning algorithm for struc-tured `?
tree norms with a complexity nearly lin-ear in the number of training samples.
This leads toa memory-efficient and time-efficient learning algo-rithm for generalized linear language models.The paper is organized as follows.
The modeland other preliminary material is introduced in Sec-tion 2.
In Section 3, we review unstructured penal-ties that were proposed earlier.
Next, we proposestructured penalties and compare their memory andtime requirements.
We summarize the characteris-tics of the proposed algorithms in Section 5 and ex-perimentally validate our findings in Section 6.23334665 777(a) Trie-structured vector.w = [ 3 4 6 6 4 5 7 7 ]>.346 [2]4 57 [2](b) Tree-structured vector.w = [ 3 4 6 6 4 5 7 7 ]>.2.83.54.84.32.3 35.64.9(c) `T2 -proximal ?`T2(w, 0.8) =[ 2.8 3.5 4.8 4.3 2.3 3 5.6 4.9 ]>.345.2 [2]3.2 4.25.4 [2](d) `T?-proximal ?`T?
(w, 0.8) =[ 3 4 5.2 5.2 3.2 4.2 5.4 5.4 ]>.Figure 1: Example of uncollapsed (trie) and corresponding collapsed (tree) structured vectors and proximaloperators applied to them.
Weight values are written inside the node.
Subfigure (a) shows the completetrie S and Subfigure (b) shows the corresponding collapsed tree T .
The number in the brackets shows thenumber of nodes collapsed.
Subfigure (c) shows vector after proximal projection for `T2 -norm (which cannotbe collapsed), and Subfigure (d) that of `T?-norm proximal projection which can be collapsed.2 Log-linear language modelsMultinomial logistic regression and Poisson regres-sion are examples of log-linear models (McCullaghand Nelder, 1989), where the likelihood belongsto an exponential family and the predictor is lin-ear.
The application of log-linear models to lan-guage modelling was proposed more than a decadeago (Della Pietra et al 1997) and it was shown tobe competitive with state-of-the-art language mod-elling such as Knesser-Ney smoothing (Chen andRosenfeld, 2000).2.1 Model definitionLet V be a set of words or more generally a set ofsymbols, which we call vocabulary.
Further, let xybe a sequence of n+1 symbols of V , where x ?
V nand y ?
V .
We model the probability that symbol ysucceeds x asP (y = v|x) =ew>v ?m(x)?u?V ew>u ?m(x), (1)where W = {wv}v?V is the set of parameters, and?m(x) is the vector of features extracted from x, thesequence preceding y.
We will describe the featuresshortly.Let x1:i denote the subsequence of x starting atthe first position up to the ith position and yi the nextsymbol in the sequence.
Parameters are estimated byminimizing the penalized log-loss:W ?
?
argminW?Kf(W ) + ??
(W ), (2)where f(W ) := ?
?ni=1 ln p(yi|x1:i;W ) and K isa convex set representing the constraints applied onthe parameters.
Overfitting is avoided by adjust-ing the regularization parameter ?, e.g., by cross-validation.2.2 Suffix tree encodingSuffix trees provide an efficient way to store andmanipulate discrete sequences and can be con-structed in linear time when the vocabulary isfixed (Giegerich and Kurtz, 1997).
Recent examplesinclude language models based on a variable-lengthMarkovian assumption (Kennington et al 2012)and the sequence memoizer (Wood et al 2011).
Thesuffix tree data structure encodes all the unique suf-fixes observed in a sequence up to a maximum givenlength.
It exploits the fact that the set of observedcontexts is a small subset of all possible contexts.When a series of suffixes of increasing lengths are234Algorithm 1 W ?
:= argmin {f(X,Y ;W )+??
(W )} Stochastic optimization algorithm (Hu etal., 2009)1 Input: ?
regularization parameter , L Lipschitz constant of?f , ?
coefficient of strong-convexity of f + ?
?, X designmatrix, Y label set2 Initialize: W = Z = 0, ?
= ?
= 1, ?
= L+ ?3 repeat until maximum iterations4 #estimate point for gradient updateW = (1?
?
)W + ?Z5 #use mini-batch {X?, Y?}
for updateW = ParamUpdate(X?, Y?, W , ?, ?
)6 #weighted combination of estimatesZ = 1??+?((1?
?
)Z + (??
?
)W + ?W)7 #update constants?
= L+ ?/?, ?
=?4?+?2?
?2 , ?
= (1?
?
)?Procedure: W := ParamUpdate(X?, Y?, W , ?, ?
)1 W ?
= W ?
1?
?f(X?, Y?,W ) #gradient step2 W = [W ]+ #projection to non-negative orthant3 W = ??
(w, ?)
#proximal stepalways observed in the same context, the successivesuffixes are collapsed into a single node.
The un-collapsed version of the suffix tree T is called a suf-fix trie, which we denote S. A suffix trie also hasa tree structure, but it potentially has much largernumber of nodes.
An example of a suffix trie S andthe associated suffix tree T are shown in Figures 1(a)and 1(b) respectively.
We use |S| to denote the num-ber of nodes in the trie S and |T | for the number ofnodes in the tree T .Suffix tree encoding is particularly helpful in ap-plications where the resulting hierarchical structuresare thin and tall with numerous non-branching paths.In the case of text, it has been observed that the num-ber of nodes in the tree grows slower than that ofthe trie with the length of the sequence (Wood etal., 2011; Kennington et al 2012).
This is a signif-icant gain in the memory requirements and, as wewill show in Section 4, can also lead to importantcomputational gains when this structure is exploited.The feature vector ?m(x) encodes suffixes (orcontexts) of increasing length up to a maximumlength m. Hence, the model defined in (1) is simi-lar tom-gram language models.
Naively, the featurevector ?m(x) corresponds to one path of length mstarting at the root of the suffix trie S. The entriesin W correspond to weights for each suffix.
We thushave a trie structure S on W (see Figure 1(a)) con-straining the number of free parameters.
In otherwords, there is one weight parameter per node in thetrie S and the matrix of parameters W is of size |S|.In this work, however, we consider models wherethe number of parameters is equal to the size of thesuffix tree T , which has much fewer nodes than S.This is achieved by ensuring that all parameters cor-responding to suffixes at a node share the same pa-rameter value (see Figure 1(b)).
These parameterscorrespond to paths in the suffix trie that do notbranch i.e.
sequence of words that always appear to-gether in the same order.2.3 Proximal gradient algorithmThe objective function (2) involves a smooth convexloss f and a possibly non-smooth penalty ?.
Sub-gradient descent methods for non-smooth ?
couldbe used, but they are unfortunately very slow to con-verge.
Instead, we choose proximal methods (Nes-terov, 2007), which have fast convergence ratesand can deal with a large number of penalties ?,see (Bach et al 2012).Proximal methods iteratively update the currentestimate by making a generalized gradient update ateach iteration.
Formally, they are based on a lin-earization of the smooth function f around a param-eter estimate W , adding a quadratic penalty termto keep the updated estimate in the neighborhoodof W .
At iteration t, the update of the parameter Wis given byW t+1 = argminW?K{f(W ) + (W ?W )>?f(W )+?
(W ) +L2?W ?W?22}, (3)where L > 0 is an upper-bound on the Lipschitzconstant of the gradient ?f .
The matrix W couldeither be the current estimate W t or its weightedcombination with the previous estimate for accel-erated convergence depending on the specific algo-rithm used (Beck and Teboulle, 2009).
Equation (3)can be rewritten to be solved in two independentsteps: a gradient update from the smooth part fol-lowed by a projection depending only on the non-smooth penalty:W ?
= W ?1L?f(W ), (4)235W t+1 = argminW?K12?
?W ?W ??
?22 +??
(W )L. (5)Update (5) is called the proximal operator of W ?with parameter ?L that we denote ??
(W ?, ?L).
Ef-ficiently computing the proximal step is crucial tomaintain the fast convergence rate of these methods.2.4 Stochastic proximal gradient algorithmIn language modelling applications, the number oftraining samples n is typically in the range of 105or larger.
Stochastic version of the proximal meth-ods (Hu et al 2009) have been known to be welladapted when n is large.
At every update, thestochastic algorithm estimates the gradient on amini-batch, that is, a subset of the samples.
The sizeof the mini-batches controls the trade-off betweenthe variance in the estimate of gradient and the timerequired for compute it.
In our experiments we usemini-batches of size 400.
The training algorithm issummarized in Algorithm 1.
The acceleration is ob-tained by making the gradient update at a specificweighted combination of the current and the previ-ous estimates of the parameters.
The weighting isshown in step 6 of the Algorithm 1.2.5 Positivity constraintsWithout constraining the parameters, the memoryrequired by a model scales linearly with the vocabu-lary size |V |.
Any symbol in V observed in a givencontext is a positive example, while any symbolsin V that does not appear in this context is a neg-ative example.
When adopting a log-linear languagemodel, the negative examples are associated with asmall negative gradient step in (4), so that the solu-tion is not sparse accross multiple categories in gen-eral.
By constraining the parameters to be positive(i.e., the set of feasible solutions K is the positiveorthant), the projection step 2 in Algorithm 1 can bedone with the same complexity, while maintainingsparse parameters accross multiple categories.
Moreprecisely, the weights for the category k associatedto a given context x, is always zeros if the category knever occured after context x.
A significant gain inmemory (nearly |V |-fold for large context lengths)was obtained without loss of accuracy in our exper-iments.3 Unstructured penaltiesStandard choices for the penalty function ?
(W ) in-clude the `1-norm and the squared `2-norm.
Theformer typically leads to a solution that is sparseand easily interpretable, while the latter leads to anon-sparse, generally more stable one.
In partic-ular, the squared `2 and `1 penalties were used inthe context of log-linear language models (Chen andRosenfeld, 2000; Goodman, 2004), reporting perfor-mances competitive with bi-gram and tri-gram inter-polated Kneser-Ney smoothing.3.1 Proximal step on the suffix trieFor squared `2 penalties, the proximal step?`22(wt, ?2 ) is the element-wise rescaling operation:w(t+1)i ?
w(t)i (1 + ?
)?1 (6)For `1 penalties, the proximal step ?`1(wt, ?)]
is thesoft-thresholding operator:w(t+1)i ?
max(0, w(t)i ?
?).
(7)These projections have linear complexity in thenumber of features.3.2 Proximal step on the suffix treeWhen feature values are identical, the correspondingproximal (and gradient) steps are identical.
This canbe seen from the proximal steps (7) and (6), whichapply to single weight entries.
This property can beused to group together parameters for which the fea-ture values are equal.
Hence, we can collapse suc-cessive nodes that always have the same values in asuffix trie (as in Figure 1(b)), that is to say we candirectly work on the suffix tree.
This leads to a prox-imal step with complexity that scales linearly withthe number of symbols seen in the corpus (Ukkonen,1995) and logarithmically with context length.4 Structured penaltiesThe `1 and squared `2 penalties do not account forthe sequential dependencies in the data, treating suf-fixes of different lengths equally.
This is inappro-priate considering that longer suffixes are typicallyobserved less frequently than shorter ones.
More-over, the fact that suffixes might be nested is disre-garded.
Hence, we propose to use the tree-structured236Algorithm 2 w := ?`T2 (w, ?)
Proximal projectionstep for `T2 on grouping G.1 Input: T suffix tree, w trie-structured vector, ?
threshold2 Initialize: {?i} = 0, {?i} = 13 ?
= UpwardPass(?, ?, ?, w)4 w = DownwardPass(?, w)Procedure: ?
:= UpwardPass(?, ?, ?, w)1 for x ?
DepthFirstSuffixTraversal(T, PostOrder)2 ?x = w2x +?h?children(x) ?h3 ?x = [1?
?/?
?x]+4 ?x = ?2x?xProcedure: w := DownwardPass(?, w)1 for x ?
DepthFirstSuffixTraversal(T, PreOrder)2 wx = ?xwx3 for h ?
children(x)4 ?h = ?x?ha DepthFirstSuffixTraversal(T,Order) returns observed suf-fixes from the suffix tree T by depth-first traversal in the orderprescribed by Order.b wx is the weights corresponding to the suffix x from theweight vector w and children(x) returns all the immediatechildren to suffix x in the tree.norms (Zhao et al 2009; Jenatton et al 2011),which are based on the suffix trie or tree, where sub-trees correspond to contexts of increasing lengths.As will be shown in the experiments, this preventsthe model to overfit unlike the `1- or squared `2-norm.4.1 Definition of tree-structured `Tp normsDefinition 1.
Let x be a training sequence.
Groupg(w, j) is the subvector of w associated with thesubtree rooted at the node j of the suffix trie S(x).Definition 2.
Let G denote the ordered set of nodesof the tree T (x) such that for r < s, g(w, r) ?g(w, s) = ?
or g(w, r) ?
g(w, s).
The tree-structured `p-norm is defined as follows:`Tp (w) =?j?G?g(w, j)?p .
(8)We specifically consider the cases p = 2,?
forwhich efficient optimization algorithms are avail-able.
The `Tp -norms can be viewed as a groupsparsity-inducing norms, where the groups are or-ganized in a tree.
This means that when the weightassociated with a parent in the tree is driven to zero,the weights associated to all its descendants shouldalso be driven to zero.Algorithm 3 w := ?`T?
(w, ?)
Proximal projectionstep for `T?
on grouping G.Input: T suffix tree, w=[v c] tree-structured vector v withcorresponding number of suffixes collapsed at each node inc, ?
threshold1 for x ?
DepthFirstNodeTraversal(T, PostOrder)2 g(v, x) := pi`T?
( g(v, x), cx?
)Procedure: q := pi`?
(q, ?
)Input: q = [v c], qi = [vi ci], i = 1, ?
?
?
, |q|Initialize: U = {}, L = {}, I = {1, ?
?
?
, |q|}1 while I 6= ?2 pick random ?
?
I #choose pivot3 U = {j|vj ?
v?}
#larger than v?4 L = {j|vj < v?}
#smaller than v?5 ?S =?i?U vi ?
ci, ?C =?i?U ci6 if (S + ?S)?
(C + ?C)?
< ?7 S := (S + ?S), C := (C + ?C), I := L8 else I := U\{?
}9 r = S?
?C , vi := vi ?max(0, vi ?
r) #take residualsa DepthFirstNodeTraversal(T,Order) returns nodes x from thesuffix tree T by depth-first traversal in the order prescribedby Order.For structured `Tp -norm, the proximal stepamounts to residuals of recursive projections on the`q-ball in the order defined by G (Jenatton et al2011), where `q-norm is the dual norm of `p-norm1.In the case `T2 -norm this comes to a series of pro-jections on the `2-ball.
For `T?-norm it is insteadprojections on the `1-ball.
The order of projectionsdefined by G is generated by an upward pass of thesuffix trie.
At each node through the upward pass,the subtree below is projected on the dual norm ballof size ?, the parameter of proximal step.
We detailthe projections on the norm ball below.4.2 Projections on `q-ball for q = 1, 2Each of the above projections on the dual norm balltakes one of the following forms depending on thechoice of the norm.
Projection of vector w on the`2-ball is equivalent to thresholding the magnitudeof w by ?
units while retaining its direction:w ?
[||w||2 ?
?]+w||w||2.
(9)This can be performed in time linear in size of w,O(|w|).
Projection of a non-negative vectorw on the`1-ball is more involved and requires thresholding1`p-norm and `q-norm are dual to each other if 1p +1q = 1.`2-norm is self-dual while the dual of `?-norm is the `1-norm.237by a value such that the entries in the resulting vectoradd up to ?, otherwise w remains the same:w ?
[w ?
?
]+ s.t.
||w||1 = ?
or ?
= 0.
(10)?
= 0 is the case where w lies inside the `1-ballof size ?
with ||w||1 < ?, leaving w intact.
In theother case, the threshold ?
is to be computed suchthat after thresholding, the resulting vector has an`1-norm of ?.
The simplest way to achieve this isto sort by descending order the entries w = sort(w)and pick the k largest values such that the (k + 1)thlargest entry is smaller than ?
:k?i=1wi ?
?
= ?
and ?
> wk+1.
(11)We refer to wk as the pivot and are only interested inentries larger than the pivot.
Given a sorted vector,it requires looking up to exactly k entries, however,sorting itself take O(|w| log |w|).4.3 Proximal stepNaively employing the projection on the `2-ball de-scribed above leads to an O(d2) algorithm for `T2proximal step.
This could be improved to a linear al-gorithm by aggregating all necessary scaling factorswhile making an upward pass of the trie S and ap-plying them in a single downward pass as describedin (Jenatton et al 2011).
In Algorithm 2, we detailthis procedure for trie-structured vectors.The complexity of `T?-norm proximal step de-pends directly on that of the pivot finding algorithmused within its `1-projection method.
Naively sort-ing vectors to find the pivot leads to an O(d2 log d)algorithm.
Pivot finding can be improved by ran-domly choosing candidates for the pivot and thebest known algorithm due to (Bruckner, 1984) hasamortized linear time complexity in the size of thevector.
This leaves us with O(d2) complexity for`T?-norm proximal step.
(Duchi et al 2008) pro-poses a method that scales linearly with the num-ber of non-zero entries in the gradient update (s)but logarithmically in d. But recursive calls to`1-projection over subtrees will fail the sparsityassumption (with s ?
d) making proximal stepquadratic.
Procedure for ?`T?
on trie-structured vec-tors using randomized pivoting method is describedin Algorithm 3.We next explain how the number of `1-projectionscan be reduced by switching to the tree T instead oftrie S which is possible due to the good properties of`T?-norm.
Then we present a pivot finding methodthat is logarithmic in the feature size for our appli-cation.4.4 `T?-norm with suffix treesWe consider the case where all parameters are ini-tialized with the same value for the optimization pro-cedure, typically with zeros.
The condition that theparameters at any given node continue to share thesame value requires that both the gradient update (4)and proximal step (5) have this property.
We mod-ify the tree structure to ensure that after gradient up-dates parameters at a given node continue to share asingle value.
Nodes that do not share a value aftergradient update are split into multiple nodes whereeach node has a single value.
We formally definethis property as follows:Definition 3.
A constant value non-branching pathis a set of nodes P ?
P(T,w) of a tree structure Tw.r.t.
vector w if P has |P | nodes with |P |?1 edgesbetween them and each node has at most one childand all nodes i, j ?
P have the same value in vectorw as wi = wj .The nodes of Figure 1(b) correspond to constantvalue non-branching paths when the values for allparameters at each of the nodes are the same.
Nextwe show that this tree structure is retained afterproximal steps of `T?-norm.Proposition 1.
Constant value non-branching pathsP(T,w) of T structured vector w are preserved un-der the proximal projection step ?`T?
(w, ?
).Figure 1(d) illustrates this idea showing `T?
pro-jection applied on the collapsed tree.
This makes itmemory efficient but the time required for the prox-imal step remains the same since we must projecteach subtree of S on the `1-ball.
The sequence ofprojections at nodes of S in a non-branching pathcan be rewritten into a single projection step usingthe following technique bringing the number of pro-jections from |S| to |T |.Proposition 2.
Successive projection steps for sub-trees with root in a constant value non-branchingpath P = {g1, ?
?
?
, g|P |} ?
P(T,w) for ?`T?
(w, ?
)238is pig|P | ??
?
?
?pig1(w, ?)
applied in bottom-up orderdefined by G. The composition of projections can berewritten into a single projection step with ?
scaledby the number of projections |P | as,pig|P |(w, ?|P |) ?
pig|P | ?
?
?
?
?
pig1(w, ?
).The above propositions show that `T?-norm can beused with the suffix tree with fewer projection steps.We now propose a method to further improve eachof these projection steps.4.5 Fast proximal step for `T?-normLet k be the cardinality of the set of values largerthan the pivot in a vector to compute the thresh-old for `1-projection as referred in (11).
This valuevaries from one application to another, but for lan-guage applications, our experiments on 100K en-glish words (APNews dataset) showed that k is gen-erally small: its value is on average 2.5, and itsmaximum is around 10 and 20, depending on theregularization level.
We propose using a max-heapdata structure (Cormen et al 1990) to fetch the k-largest values necessary to compute the threshold.Given the heap of the entries the cost of finding thepivot is O(k log(d)) if the pivot is the kth largest en-try and there are d features.
This operation is per-formed d times for `T?-norm as we traverse the treebottom-up.
The heap itself is built on the fly dur-ing this upward pass.
At each subtree, the heap isbuilt by merging those of their children in constanttime by using Fibonacci heaps.
This leaves us with aO(dk log(d)) complexity for the proximal step.
Thisprocedure is detailed in Algorithm 4.5 Summary of the algorithmsTable 1 summarizes the characteristics of the algo-rithms associated to the different penalties:1.
The unstructured norms `p do not take intoaccount the varying sparsity level with con-text length.
For p=1, this leads to a sparsesolution and for p=2, we obtain the classicalquadratic penalty.
The suffix tree representa-tion leads to an efficient memory usage.
Fur-thermore, to make the training algorithm timeefficient, the parameters corresponding to con-texts which always occur in the same largerAlgorithm 4 w := ?`T?
(w, ?)
Proximal projectionstep for `T?
on grouping G using heap data structure.Input: T suffix tree, w=[v c] tree-structured vector v withcorresponding number of suffixes collapsed at each node inc, ?
thresholdInitializeH = {}# empty set of heaps1 for x ?
DepthFirstNodeTraversal(T, PostOrder)g(v, x) := pi`T?
(w, x, cx?,H )Procedure: q := pi`?
(w, x, ?,H )1 Hx = NewHeap(vx, cx, vx)2 for j ?
children(x) # merge with child heaps?x = ?x + ?j # update `1-normHx = Merge(Hx,Hj),H = H\Hj3 H = H ?Hx, S = 0, C = 0, J = {}4 ifHx(?)
< ?, setHx = 0 return5 for j ?
OrderedIterator(Hx) # get max valuesif vj >S+(vj?cj)?
?C+cjS = S + (vj ?cj), C = C + cj , J = J ?
{j}else break6 r = S?
?C , ?
= 0 # compute threshold7 for j ?
J # apply threshold?
= min(vj , r), ?
= ?
+ (vj ?
?
)Hj(v) = ?8 Hx(?)
= Hj(?)?
?
# update `1-norma.
Heap structure on vector w holds three values (v, c, ?)
ateach node.
v, c being value and its count, ?
is the `1-norm ofthe sub-vector below.
Tuples are ordered by decreasing valueof v and Hj refers to heap with values in sub-tree rooted atj.
Merge operation merges the heaps passed.
OrderedIteratorreturns values from the heap in decreasing order of v.context are grouped.
We will illustrate in theexperiments that these penalties do not lead togood predictive performances.2.
The `T2 -norm nicely groups features by subtreeswhich concurs with the sequential structure ofsequences.
This leads to a powerful algorithmin terms of generalization.
But it can only beapplied on the uncollapsed tree since there isno closure property of the constant value non-branching path for its proximal step making itless amenable for larger tree depths.3.
The `T?-norm groups features like the `T2 -normwhile additionally encouraging numerous fea-ture groups to share a single value, leading toa substantial reduction in memory usage.
Thegeneralization properties of this algorithm is asgood as the generalization obtained with the `T2penalty, if not better.
However, it has the con-stant value non-branching path property, which239Penalty good generalization memory efficient time efficientunstructured `1 and `22 no yes O(|T |) yes O(|T |)struct.`T2 yes no O(|S|) no O(|S|)`T?
rand.
pivot yes yes O(|T |) no O(|T |2)`T?
heap yes yes O(|T |) yes O(|T | log |T |)Table 1: Properties of the algorithms proposed in this paper.
Generalization properties are as compared bytheir performance with increasing context length.
Memory efficiency is measured by the number of freeparameters of W in the optimization.
Note that the suffix tree is much smaller than the trie (uncollapsedtree): |T | << |S|.
Time complexities reported are that of one proximal projection step.2 4 6 8 10 12210220230240250260order of language modelperplexityKN`22`1`T2`T?
(a) Unweighted penalties.2 4 6 8 10 12210220230240250260order of language modelperplexityKNw`22w`1w`T2w`T?
(b) Weighted penalties.2 4 6 8 10 1202468x 105order of language model#ofparametersKNw`T2w`T?
(c) Model complexity for structuredpenalties.Figure 2: (a) compares average perplexity (lower is better) of different methods from 2-gram through 12-gram on four different 100K-20K train-test splits.
(b) plot compares the same with appropriate featureweighting.
(c) compares model complexity for weighted structured penalties w`T2 and w`T?
measure bythen number of parameters.means that the proximal step can be applied di-rectly to the suffix tree.
There is thus also asignificant gain of performances.6 ExperimentsIn this section, we demonstrate empirically the prop-erties of the algorithms summarized in Table 1.
Weconsider four distinct subsets of the Associated PressNews (AP-news) text corpus with train-test sizes of100K-20K for our experiments.
The corpus waspreprocessed as described in (Bengio et al 2003)by replacing proper nouns, numbers and rare wordswith special symbols ?
?proper noun?
?, ?#n?
and??unknown??
respectively.
Punctuation marks areretained which are treated like other normal words.Vocabulary size for each of the training subsets wasaround 8,500 words.
The model was reset at the startof each sentence, meaning that a word in any givensentence does not depend on any word in the previ-ous sentence.
The regularization parameter ?
is cho-sen for each model by cross-validation on a smallersubset of data.
Models are fitted to training sequenceof 30K words for different values of ?
and validatedagainst a sequence of 10K words to choose ?.We quantitatively evaluate the proposed modelusing perplexity, which is computed as follows:P ({xi, yi},W ) = 10{?1nV?ni=1 I(yi?V ) log p(yi|x1:i;W )},where nV =?i I(yi ?
V ).
Performance is mea-sured for varying depth of the suffix trie with dif-ferent penalties.
Interpolated Kneser-Ney resultswere computed using the openly available SRILMtoolkit (Stolcke, 2002).Figure 2(a) shows perplexity values averaged overfour data subsets as a function of the language modelorder.
It can be observed that performance of un-structured `1 and squared `2 penalties improve untila relatively low order and then degrade, while `T2penalty does not show such degradation, indicating2402 4 6 8 10 12204060tree depthtime(sec)rand-pivotrand-pivot-col(a) Iteration time of random-pivoting onthe collapsed and uncollapsed trees.1 2 3 4 5x 106204060train sizetime(sec)k-best heaprand-pivot-col(b) Iteration time of random-pivoting andk-best heap on the collapsed tree.Figure 3: Comparison of different methods for performing `T?
proximal projection.
The rand-pivotis the random pivoting method of (Bruckner, 1984) and rand-pivot-col is the same applied with thenodes collapsed.
The k-best heap is the method described in Algorithm 4.that taking the tree-structure into account is benefi-cial.
Moreover, the log-linear language model with`T2 penalty performs similar to interpolated Kneser-Ney.
The `T?-norm outperforms all other modelsat order 5, but taking the structure into accountdoes not prevent a degradation of the performanceat higher orders, unlike `T2 .
This means that a singleregularization for all model orders is still inappro-priate.To investigate this further, we adjust the penal-ties by choosing an exponential decrease of weightsvarying as ?m for a feature at depth m in the suffixtree.
Parameter ?
was tuned on a smaller validationset.
The best performing values for these weightedmodels w`22, w`1, w`T2 and w`T?
are 0.5, 0.7, 1.1and 0.85 respectively.
The weighting scheme fur-ther appropriates the regularization at various levelsto suit the problem?s structure.
Perplexity plots forweighted models are shown in Figure 2(b).
Whilew`1 improves at larger depths, it fails to compareto others showing that the problem does not admitsparse solutions.
Weighted `22 improves consider-ably and performs comparably to the unweightedtree-structured norms.
However, the introduction ofweighted features prevents us from using the suf-fix tree representation, making these models inef-ficient in terms of memory.
Weighted `T?
is cor-rected for overfitting at larger depths and w`T2 gainsmore than others.
Optimal values for ?
are frac-tional for all norms except w`T2 -norm showing thatthe unweighted model `T2 -norm was over-penalizingfeatures at larger depths, while that of others wereunder-penalizing them.
Interestingly, perplexity im-proves up to about 9-grams with w`T2 penalty forthe data set we considered, indicating that there ismore to gain from longer dependencies in naturallanguage sentences than what is currently believed.Figure 2(c) compares model complexity mea-sured by the number of parameters for weightedmodels using structured penalties.
The `T2 penaltyis applied on trie-structured vectors, which growsroughly at a linear rate with increasing model order.This is similar to Kneser-Ney.
However, the numberof parameters for the w`T?
penalty grows logarith-mically with the model order.
This is due to the factthat it operates on the suffix tree-structured vectorsinstead of the suffix trie-structured vectors.
Theseresults are valid for, both, weighted and unweightedpenalties.Next, we compare the average time taken per iter-ation for different implementations of the `T?
prox-imal step.
Figure 3(a) shows this time against in-creasing depth of the language model order for ran-dom pivoting method with and without the collaps-ing of parameters at different constant value non-branching paths.
The trend in this plot resemblesthat of the number of parameters in Figure 2(c).
Thisshows that the complexity of the full proximal stepis sublinear when accounting for the suffix tree datastructure.
Figure 3(b) plots time per iteration ran-dom pivoting and k-best heap against the varyingsize of training sequence.
The two algorithms areoperating directly on the suffix tree.
It can be ob-served that the heap-based method are superior with241increasing size of training data.7 ConclusionIn this paper, we proposed several log-linear lan-guage models.
We showed that with an efficientdata structure and structurally appropriate convexregularization schemes, they were able to outper-form standard Kneser-Ney smoothing.
We also de-veloped a proximal projection algorithm for the tree-structured `T?-norm suitable for large trees.Further, we showed that these models can betrained online, that they accurately learn the m-gramweights and that they are able to better take advan-tage of long contexts.
The time required to run theoptimization is still a concern.
It takes 7583 min-utes on a standard desktop computer for one pass ofthe of the complete AP-news dataset with 13 mil-lion words which is little more than time reportedfor (Mnih and Hinton, 2007).
The most time con-suming part is computing the normalization factorfor the log-loss.
A hierarchical model in the flavourof (Mnih and Hinton, 2008) should lead to signifi-cant improvements to this end.
Currently, the com-putational bottleneck is due to the normalization fac-tor in (1) as it appears in every gradient step com-putation.
Significant savings would be obtained bycomputing it as described in (Wu and Khundanpur,2000).AcknowledgementsThe authors would like to thank anonymous review-ers for their comments.
This work was partiallysupported by the CIFRE grant 1178/2010 from theFrench ANRT.ReferencesF.
Bach, R. Jenatton, J. Mairal, and G. Obozinski.
2012.Optimization with sparsity-inducing penalties.
Foun-dations and Trends in Machine Learning, pages 1?106.A.
Beck and M. Teboulle.
2009.
A fast itera-tive shrinkage-thresholding algorithm for linear in-verse problems.
SIAM Journal of Imaging Sciences,2(1):183?202.Y.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003.A neural probabilistic language model.
Journal of Ma-chine Learning Research, 3:1137?1155.P.
Bruckner.
1984.
An o(n) algorithm for quadraticknapsack problems.
Operations Research Letters,3:163?166.L.
Burget, P. Matejka, P. Schwarz, O. Glembek, and J.H.Cernocky.
2007.
Analysis of feature extraction andchannel compensation in a GMM speaker recognitionsystem.
IEEE Transactions on Audio, Speech andLanguage Processing, 15(7):1979?1986, September.Y-W. Chang and M. Collins.
2011.
Exact decodingof phrase-based translation models through lagrangianrelaxation.
In Proc.
Conf.
Empirical Methods for Nat-ural Language Processing, pages 26?37.S.
F. Chen and R. Rosenfeld.
2000.
A survey ofsmoothing techniques for maximum entropy models.IEEE Transactions on Speech and Audio Processing,8(1):37?50.T.
H. Cormen, C. E. Leiserson, and R. L. Rivest.
1990.An Introduction to Algorithms.
MIT Press.S.
Della Pietra, V. Della Pietra, and J. Lafferty.
1997.Inducing features of random fields.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,19(4):380?393.J.
Duchi, S. Shalev-Shwartz, Y.
Singer, and T. Chandra.2008.
Efficient projections onto the `1-ball for learn-ing in high dimensions.
Proc.
25th Int.
Conf.
MachineLearning.R.
Giegerich and S. Kurtz.
1997.
From ukkonen to Mc-Creight and weiner: A unifying view of linear-timesuffix tree construction.
Algorithmica.J.
Goodman.
2001.
A bit of progress in language mod-elling.
Computer Speech and Language, pages 403?434, October.J.
Goodman.
2004.
Exponential priors for maximum en-tropy models.
In Proc.
North American Chapter of theAssociation of Computational Linguistics.C.
Hu, J.T.
Kwok, and W. Pan.
2009.
Accelerated gra-dient methods for stochastic optimization and onlinelearning.
Advances in Neural Information ProcessingSystems.R.
Jenatton, J. Mairal, G. Obozinski, and F. Bach.2011.
Proximal methods for hierarchical sparse cod-ing.
Journal of Machine Learning Research, 12:2297?2334.C.
R. Kennington, M. Kay, and A. Friedrich.
2012.
Sufxtrees as language models.
Language Resources andEvaluation Conference.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In Proc.
IEEE Int.
Conf.Acoustics, Speech and Signal Processing, volume 1.A.
F. T. Martins, N. A. Smith, P. M. Q. Aguiar, andM.
A. T. Figueiredo.
2011.
Structured sparsity instructured prediction.
In Proc.
Conf.
Empirical Meth-ods for Natural Language Processing, pages 1500?1511.242P.
McCullagh and J. Nelder.
1989.
Generalized linearmodels.
Chapman and Hall.
2nd edition.A.
Mnih and G. Hinton.
2007.
Three new graphical mod-els for statistical language modelling.
Proc.
24th Int.Conference on Machine Learning.A.
Mnih and G. Hinton.
2008.
A scalable hierarchicaldistributed language model.
Advances in Neural In-formation Processing Systems.Y.
Nesterov.
2007.
Gradient methods for minimizingcomposite objective function.
CORE Discussion Pa-per.B.
Roark, M. Saraclar, M. Collins, and M. Johnson.2004.
Discriminative language modeling with con-ditional random fields and the perceptron algorithm.Proc.
Association for Computation Linguistics.A.
Stolcke.
2002.
Srilm- an extensible language mod-eling toolkit.
Proc.
Int.
Conf.
Spoken Language Pro-cessing, 2:901?904.E.
Ukkonen.
1995.
Online construction of suffix trees.Algorithmica.S.
Vargas, P. Castells, and D. Vallet.
2012.
Explicit rel-evance models in intent-oriented information retrievaldiversification.
In Proc.
35th Int.
ACM SIGIR Conf.Research and development in information retrieval,SIGIR ?12, pages 75?84.
ACM.F.
Wood, C. Archambeau, J. Gasthaus, J. Lancelot, andY.-W. Teh.
2009.
A stochastic memoizer for sequencedata.
In Proc.
26th Intl.
Conf.
on Machine Learning.F.
Wood, J. Gasthaus, C. Archambeau, L. James, andY.
W. Teh.
2011.
The sequence memoizer.
In Com-munications of the ACM, volume 54, pages 91?98.J.
Wu and S. Khundanpur.
2000.
Efficient training meth-ods for maximum entropy language modeling.
Proc.6th Inter.
Conf.
Spoken Language Technologies, pages114?117.P.
Zhao, G. Rocha, and B. Yu.
2009.
The compos-ite absolute penalties family for grouped and hierar-chical variable selection.
The Annals of Statistics,37(6A):3468?3497.243
