Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 814?824,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsCoreference Resolution with World KnowledgeAltaf Rahman and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{altaf,vince}@hlt.utdallas.eduAbstractWhile world knowledge has been shown toimprove learning-based coreference resolvers,the improvements were typically obtained byincorporating world knowledge into a fairlyweak baseline resolver.
Hence, it is not clearwhether these benefits can carry over to astronger baseline.
Moreover, since there hasbeen no attempt to apply different sources ofworld knowledge in combination to corefer-ence resolution, it is not clear whether they of-fer complementary benefits to a resolver.
Wesystematically compare commonly-used andunder-investigated sources of world knowl-edge for coreference resolution by applyingthem to two learning-based coreference mod-els and evaluating them on documents anno-tated with two different annotation schemes.1 IntroductionNoun phrase (NP) coreference resolution is the taskof determining which NPs in a text or dialogue referto the same real-world entity.
The difficulty of thetask stems in part from its reliance on world knowl-edge (Charniak, 1972).
To exemplify, consider thefollowing text fragment.Martha Stewart is hoping people don?t run out on her.The celebrity indicted on charges stemming from .
.
.Having the (world) knowledge that Martha Stewartis a celebrity would be helpful for establishing thecoreference relation between the two NPs.
One mayargue that employing heuristics such as subject pref-erence or syntactic parallelism (which prefers re-solving an NP to a candidate antecedent that has thesame grammatical role) in this example would alsoallow us to correctly resolve the celebrity (Mitkov,2002), thereby obviating the need for world knowl-edge.
However, since these heuristics are not per-fect, complementing them with world knowledgewould be an important step towards bringing coref-erence systems to the next level of performance.Despite the usefulness of world knowledge forcoreference resolution, early learning-based coref-erence resolvers have relied mostly on morpho-syntactic features (e.g., Soon et al (2001), Ng andCardie (2002), Yang et al (2003)).
With recent ad-vances in lexical semantics research and the devel-opment of large-scale knowledge bases, researchershave begun to employ world knowledge for corefer-ence resolution.
World knowledge is extracted pri-marily from three data sources, web-based encyclo-pedia (e.g., Ponzetto and Strube (2006), Uryupinaet al (2011)), unannotated data (e.g., Daume?
IIIand Marcu (2005), Ng (2007)), and coreference-annotated data (e.g., Bengtson and Roth (2008)).While each of these three sources of world knowl-edge has been shown to improve coreference resolu-tion, the improvements were typically obtained byincorporating world knowledge (as features) into abaseline resolver composed of a rather weak coref-erence model (i.e., the mention-pair model) and asmall set of features (i.e., the 12 features adoptedby Soon et al?s (2001) knowledge-lean approach).As a result, some questions naturally arise.
First,can world knowledge still offer benefits when usedin combination with a richer set of features?
Sec-ond, since automatically extracted world knowledgeis typically noisy (Ponzetto and Poesio, 2009), arerecently-developed coreference models more noise-tolerant than the mention-pair model, and if so, canthey profit more from the noisily extracted worldknowledge?
Finally, while different world knowl-814edge sources have been shown to be useful when ap-plied in isolation to a coreference system, do they of-fer complementary benefits and therefore can furtherimprove a resolver when applied in combination?We seek answers to these questions by conduct-ing a systematic evaluation of different world knowl-edge sources for learning-based coreference reso-lution.
Specifically, we (1) derive world knowl-edge from encyclopedic sources that are under-investigated for coreference resolution, includingFrameNet (Baker et al, 1998) and YAGO (Suchaneket al, 2007), in addition to coreference-annotateddata and unannotated data; (2) incorporate suchknowledge as features into a richer baseline featureset that we previously employed (Rahman and Ng,2009); and (3) evaluate their utility using two coref-erence models, the traditional mention-pair model(Soon et al, 2001) and the recently developedcluster-ranking model (Rahman and Ng, 2009).Our evaluation corpus contains 410 documents,which are coreference-annotated using the ACE an-notation scheme as well as the OntoNotes annota-tion scheme (Hovy et al, 2006).
By evaluating ontwo sets of coreference annotations for the same setof documents, we can determine whether the use-fulness of world knowledge sources for coreferenceresolution is dependent on the underlying annotationscheme used to annotate the documents.2 PreliminariesIn this section, we describe the corpus, the NP ex-traction methods, the coreference models, and theevaluation measures we will use in our evaluation.2.1 Data SetWe evaluate on documents that are coreference-annotated using both the ACE annotation schemeand the OntoNotes annotation scheme, so that wecan examine whether the usefulness of our worldknowledge sources is dependent on the underlyingcoreference annotation scheme.
Specifically, ourdata set is composed of the 410 English newswirearticles that appear in both OntoNotes-2 and ACE2004/2005.
We partition the documents into a train-ing set and a test set following a 80/20 ratio.ACE and OntoNotes employ different guide-lines to annotate coreference chains.
A majordifference between the two annotation schemes isthat ACE only concerns establishing coreferencechains among NPs that belong to the ACE entitytypes, whereas OntoNotes does not have this re-striction.
Hence, the OntoNotes annotation schemeshould produce more coreference chains (i.e., non-singleton coreference clusters) than the ACE anno-tation scheme for a given set of documents.
For ourdata set, the OntoNotes scheme yielded 4500 chains,whereas the ACE scheme yielded only 3637 chains.Another difference between the two annotationschemes is that singleton clusters are annotated inACE but not OntoNotes.
As discussed below, thepresence of singleton clusters may have an impacton NP extraction and coreference evaluation.2.2 NP ExtractionFollowing common practice, we employ differentmethods to extract NPs from the documents anno-tated with the two annotation schemes.To extract NPs from the ACE-annotated docu-ments, we train a mention extractor on the train-ing texts (see Section 5.1 of Rahman and Ng (2009)for details), which recalls 83.6% of the NPs in thetest set.
On the other hand, to extract NPs from theOntoNotes-annotated documents, the same methodshould not be applied.
To see the reason, recall thatonly the NPs in non-singleton clusters are annotatedin these documents.
Training a mention extractoron these NPs implies that we are learning to ex-tract non-singleton NPs, which are typically muchsmaller in number than the entire set of NPs.
Inother words, doing so could substantially simplifythe coreference task.
Consequently, we follow theapproach adopted by traditional learning-based re-solvers and employ an NP chunker to extract NPs.Specifically, we use the markable identification sys-tem in the Reconcile resolver (Stoyanov et al, 2010)to extract NPs from the training and test texts.
Thisidentifier recalls 77.4% of the NPs in the test set.2.3 Coreference ModelsWe evaluate the utility of world knowledge using themention-pair model and the cluster-ranking model.2.3.1 Mention-Pair ModelThe mention-pair (MP) model is a classifier thatdetermines whether two NPs are coreferent or not.815Each instance i(NPj , NPk) corresponds to NPj andNPk, and is represented by a Baseline feature set con-sisting of 39 features.
Linguistically, these featurescan be divided into four categories: string-matching,grammatical, semantic, and positional.
These fea-tures can also be categorized based on whether theyare relational or not.
Relational features capturethe relationship between NPj and NPk, whereas non-relational features capture the linguistic property ofone of these two NPs.
Since space limitations pre-clude a description of these features, we refer thereader to Rahman and Ng (2009) for details.We follow Soon et al?s (2001) method for cre-ating training instances: we create (1) a positiveinstance for each anaphoric NP, NPk, and its clos-est antecedent, NPj ; and (2) a negative instance forNPk paired with each of the intervening NPs, NPj+1,NPj+2, .
.
., NPk?1.
The classification of a traininginstance is either positive or negative, depending onwhether the two NPs are coreferent in the associatedtext.
To train the MP model, we use the SVM learn-ing algorithm from SVMlight (Joachims, 2002).1After training, the classifier is used to identify anantecedent for an NP in a test text.
Specifically, eachNP, NPk , is compared in turn to each preceding NP,NPj , from right to left, and NPj is selected as its an-tecedent if the pair is classified as coreferent.
Theprocess terminates as soon as an antecedent is foundfor NPk or the beginning of the text is reached.Despite its popularity, the MP model has twomajor weaknesses.
First, since each candidate an-tecedent for an NP to be resolved (henceforth an ac-tive NP) is considered independently of the others,this model only determines how good a candidateantecedent is relative to the active NP, but not howgood a candidate antecedent is relative to other can-didates.
So, it fails to answer the critical question ofwhich candidate antecedent is most probable.
Sec-ond, it has limitations in its expressiveness: the in-formation extracted from the two NPs alone may notbe sufficient for making a coreference decision.2.3.2 Cluster-Ranking ModelThe cluster-ranking (CR) model addresses the twoweaknesses of the MP model by combining thestrengths of the entity-mention model (e.g., Luo et1For this and subsequent uses of the SVM learner in ourexperiments, we set al parameters to their default values.al.
(2004), Yang et al (2008)) and the mention-ranking model (e.g., Denis and Baldridge (2008)).Specifically, the CR model ranks the preceding clus-ters for an active NP so that the highest-ranked clus-ter is the one to which the active NP should belinked.
Employing a ranker addresses the first weak-ness, as a ranker allows all candidates to be com-pared simultaneously.
Considering preceding clus-ters rather than antecedents as candidates addressesthe second weakness, as cluster-level features (i.e.,features that are defined over any subset of NPs in apreceding cluster) can be employed.
Details of theCR model can be found in Rahman and Ng (2009).Since the CR model ranks preceding clusters, atraining instance i(cj , NPk) represents a precedingcluster, cj , and an anaphoric NP, NPk.
Each instanceconsists of features that are computed based solelyon NPk as well as cluster-level features, which de-scribe the relationship between cj and NPk .
Mo-tivated in part by Culotta et al (2007), we createcluster-level features from the relational features inour feature set using four predicates: NONE, MOST-FALSE, MOST-TRUE, and ALL.
Specifically, for eachrelational feature X, we first convert X into an equiv-alent set of binary-valued features if it is multi-valued.
Then, for each resulting binary-valued fea-ture Xb, we create four binary-valued cluster-levelfeatures: (1) NONE-Xb is true when Xb is false be-tween NPk and each NP in cj ; (2) MOST-FALSE-Xbis true when Xb is true between NPk and less than half(but at least one) of the NPs in cj ; (3) MOST-TRUE-Xb is true when Xb is true between NPk and at leasthalf (but not all) of the NPs in cj ; and (4) ALL-Xb istrue when Xb is true between NPk and each NP in cj .We train a cluster ranker to jointly learnanaphoricity determination and coreference reso-lution using SVMlight?s ranker-learning algorithm.Specifically, for each NP, NPk, we create a traininginstance between NPk and each preceding cluster cjusing the features described above.
Since we arelearning a joint model, we need to provide the rankerwith the option to start a new cluster by creating anadditional training instance that contains the non-relational features describing NPk .
The rank valueof a training instance i(cj , NPk) created for NPk is therank of cj among the competing clusters.
If NPk isanaphoric, its rank is HIGH if NPk belongs to cj , andLOW otherwise.
If NPk is non-anaphoric, its rank is816LOW unless it is the additional training instance de-scribed above, which has rank HIGH.After training, the cluster ranker processes theNPs in a test text in a left-to-right manner.
For eachactive NP, NPk , we create test instances for it by pair-ing it with each of its preceding clusters.
To allowfor the possibility that NPk is non-anaphoric, we cre-ate an additional test instance as during training.
Allthese test instances are then presented to the ranker.If the additional test instance is assigned the highestrank value, then we create a new cluster containingNPk.
Otherwise, NPk is linked to the cluster that hasthe highest rank.
Note that the partial clusters pre-ceding NPk are formed incrementally based on thepredictions of the ranker for the first k ?
1 NPs.2.4 Evaluation MeasuresWe employ two commonly-used scoring programs,B3 (Bagga and Baldwin, 1998) and CEAF (Luo,2005), both of which report results in terms of recall(R), precision (P), and F-measure (F) by comparingthe gold-standard (i.e., key) partition, KP , againstthe system-generated (i.e., response) partition, RP .Briefly, B3 computes the R and P values of eachNP and averages these values at the end.
Specifi-cally, for each NP, NPj , B3 first computes the numberof common NPs in KPj and RPj , the clusters con-taining NPj in KP and RP , respectively, and thendivides this number by |KPj | and |RPj | to obtainthe R and P values of NPj , respectively.
On the otherhand, CEAF finds the best one-to-one alignment be-tween the key clusters and the response clusters.A complication arises when B3 is used to scorea response partition containing automatically ex-tracted NPs.
Recall that B3 constructs a mappingbetween the NPs in the response and those in thekey.
Hence, if the response is generated using gold-standard NPs, then every NP in the response ismapped to some NP in the key and vice versa.
Inother words, there are no twinless (i.e., unmapped)NPs (Stoyanov et al, 2009).
This is not the casewhen automatically extracted NPs are used, but theoriginal description of B3 does not specify howtwinless NPs should be scored (Bagga and Baldwin,1998).
To address this problem, we set the recalland precision of a twinless NP to zero, regardless ofwhether the NP appears in the key or the response.Note that CEAF can compare partitions with twin-less NPs without any modification, since it operatesby finding the best alignment between the clusters inthe two partitions.Additionally, in order not to over-penalize a re-sponse partition, we remove all the twinless NPs inthe response that are singletons.
The rationale issimple: since the resolver has successfully identifiedthese NPs as singletons, it should not be penalized,and removing them avoids such penalty.Since B3 and CEAF align NPs/clusters, the lackof singleton clusters in the OntoNotes annotationsimplies that the resulting scores reflect solely howwell a resolver identifies coreference links and donot take into account how well it identifies singletonclusters.3 Extracting World KnowledgeIn this section, we describe how we extract worldknowledge for coreference resolution from threedifferent sources: large-scale knowledge bases,coreference-annotated data and unannotated data.3.1 World Knowledge from Knowledge BasesWe extract world knowledge from two large-scaleknowledge bases, YAGO and FrameNet.3.1.1 Extracting Knowledge from YAGOWe choose to employ YAGO rather than the morepopularly-used Wikipedia due to its potentiallyricher knowledge, which comprises 5 million factsextracted from Wikipedia and WordNet.
Each factis represented as a triple (NPj , rel, NPk), where relis one of the 90 YAGO relation types defined ontwo NPs, NPj and NPk .
Motivated in part by previ-ous work (Bryl et al, 2010; Uryupina et al, 2011),we employ the two relation types that we believeare most useful for coreference resolution, TYPEand MEANS.
TYPE is essentially an IS-A relation.For instance, the triple (AlbertEinstein, TYPE,physicist) denotes the fact that Albert Einsteinis a physicist.
MEANS provides different ways ofexpressing an entity, and therefore allows us to dealwith synonymy and ambiguity.
For instance, the twotriples (Einstein, MEANS, AlbertEinstein)and (Einstein, MEANS, AlfredEinstein)denote the facts that Einstein may refer to the physi-cist Albert Einstein and the musicologist Alfred Ein-stein, respectively.
Hence, the presence of one or817both of these relations between two NPs providesstrong evidence that the two NPs are coreferent.YAGO?s unification of the information inWikipedia and WordNet enables it to extractfacts that cannot be extracted with Wikipediaor WordNet alne, such as (MarthaStewart,TYPE, celebrity).
To better appreciate YAGO?sstrengths, let us see how this fact was extracted.YAGO first heuristically maps each of the Wikicategories in the Wiki page for Martha Stewartto its semantically closest WordNet synset.
Forinstance, the Wiki category AMERICAN TELE-VISION PERSONALITIES is mapped to the synsetcorresponding to sense #2 of the word personality.Then, given that personality is a direct hyponym ofcelebrity in WordNet, YAGO extracts the desiredfact.
This enables YAGO to extract facts that cannotbe extracted with Wikipedia or WordNet alne.We incorporate the world knowledge from YAGOinto our coreference models as a binary-valued fea-ture.
If the MP model is used, the YAGO featurefor an instance will have the value 1 if and only ifthe two NPs involved are in a TYPE or MEANS re-lation.
On the other hand, if the CR model is used,the YAGO feature for an instance involving NPk andpreceding cluster c will have the value 1 if and onlyif NPk has a TYPE or MEANS relation with any ofthe NPs in c. Since knowledge extraction from web-based encyclopedia is typically noisy (Ponzetto andPoesio, 2009), we use YAGO to determine whethertwo NPs have a relation only if one NP is a namedentity (NE) of type person, organization, or locationaccording to the Stanford NE recognizer (Finkel etal., 2005) and the other NP is a common noun.3.1.2 Extracting Knowledge from FrameNetFrameNet is a lexico-semantic resource focused onsemantic frames (Baker et al, 1998).
As a schematicrepresentation of a situation, a frame contains thelexical predicates that can invoke it as well as theframe elements (i.e., semantic roles).
For example,the JUDGMENT COMMUNICATION frame describessituations in which a COMMUNICATOR communi-cates a judgment of an EVALUEE to an ADDRESSEE.This frame has COMMUNICATOR and EVALUEE asits core frame elements and ADDRESSEE as its non-core frame elements, and can be invoked by morethan 40 predicates, such as acclaim, accuse, com-mend, decry, denounce, praise, and slam.To better understand why FrameNet contains po-tentially useful knowledge for coreference resolu-tion, consider the following text segment:Peter Anthony decries program trading as ?limiting thegame to a few,?
but he is not sure whether he wants todenounce it because ...To establish the coreference relation between it andprogram trading, it may be helpful to know that de-cry and denounce appear in the same frame and thetwo NPs have the same semantic role.This example suggests that features encoding boththe semantic roles of the two NPs under considera-tion and whether the associated predicates are ?re-lated?
to each other in FrameNet (i.e., whether theyappear in the same frame) could be useful for iden-tifying coreference relations.
Two points regardingour implementation of these features deserve men-tion.
First, since we do not employ verb sense dis-ambiguation, we consider two predicates related aslong as there is at least one semantic frame in whichthey both appear.
Second, since FrameNet-style se-mantic role labelers are not publicly available, weuse ASSERT (Pradhan et al, 2004), a semantic rolelabeler that provides PropBank-style semantic rolessuch as ARG0 (the PROTOAGENT, which is typi-cally the subject of a transitive verb) and ARG1 (thePROTOPATIENT, which is typically its direct object).Now, assuming that NPj and NPk are the argu-ments of two stemmed predicates, predj and predk ,we create 15 features using the knowledge extractedfrom FrameNet and ASSERT as follows.
First, weencode the knowledge extracted from FrameNet asone of three possible values: (1) predj and predkare in the same frame; (2) they are both predicatesin FrameNet but never appear in the same frame;and (3) one or both predicates do not appear inFrameNet.
Second, we encode the semantic roles ofNPj and NPk as one of five possible values: ARG0-ARG0, ARG1-ARG1, ARG0-ARG1, ARG1-ARG0,and OTHERS (the default case).2 Finally, we create15 binary-valued features by pairing the 3 possiblevalues extracted from FrameNet and the 5 possiblevalues provided by ASSERT.
Since these features2We focus primarily on ARG0 and ARG1 because they arethe most important core arguments of a predicate and may pro-vide more useful information than other semantic roles.818are computed over two NPs, we can employ them di-rectly for the MP model.
Note that by construction,exactly one of these features will have a non-zerovalue.
For the CR model, we extend their definitionsso that they can be computed between an NP, NPk,and a preceding cluster, c. Specifically, the value ofa feature is 1 if and only if its value between NPk andone of the NPs in c is 1 under its original definition.The above discussion assumes that the two NPsunder consideration serve as predicate arguments.
Ifthis assumption fails, we will not create any featuresbased on FrameNet for these two NPs.To our knowledge, FrameNet has not been ex-ploited for coreference resolution.
However, theuse of related verbs is similar in spirit to Bean andRiloff?s (2004) use of patterns for inducing contex-tual role knowledge, and the use of semantic roles isalso discussed in Ponzetto and Strube (2006).3.2 World Knowledge from Annotated DataSince world knowledge is needed for coreferenceresolution, a human annotator must have employedworld knowledge when coreference-annotating adocument.
We aim to design features that can ?re-cover?
such world knowledge from annotated data.3.2.1 Features Based on Noun PairsA natural question is: what kind of world knowl-edge can we extract from annotated data?
We maygather the knowledge that Barack Obama is a U.S.president if we see these two NPs appearing in thesame coreference chain.
Equally importantly, wemay gather the commonsense knowledge needed fordetermining non-coreference.
For instance, we maydiscover that a lion and a tiger are unlikely to referto the same real-world entity after realizing that theynever appear in the same chain in a large number ofannotated documents.
Note that any features com-puted based on WordNet distance or distributionalsimilarity are likely to incorrectly suggest that lionand tiger are coreferent, since the two nouns are sim-ilar distributionally and according to WordNet.Given these observations, one may collect thenoun pairs from the (coreference-annotated) train-ing data and use them as features to train a resolver.However, for these features to be effective, we needto address data sparseness, as many noun pairs inthe training data may not appear in the test data.To improve generalization, we instead create dif-ferent kinds of noun-pair-based features given anannotated text.
To begin with, we preprocess eachdocument.
A training text is preprocessed by ran-domly replacing 10% of its common nouns with thelabel UNSEEN.
If an NP, NPk , is replaced with UN-SEEN, all NPs that have the same string as NPk willalso be replaced with UNSEEN.
A test text is prepro-cessed differently: we simply replace all NPs whosestrings are not seen in the training data with UN-SEEN.
Hence, artificially creating UNSEEN labelsfrom a training text will allow a learner to learn howto handle unseen words in a test text.Next, we create noun-pair-based features for theMP model, which will be used to augment the Base-line feature set.
Here, each instance corresponds totwo NPs, NPj and NPk , and is represented by threegroups of binary-valued features.Unseen features are applicable when both NPjand NPk are UNSEEN.
Either an UNSEEN-SAME fea-ture or an UNSEEN-DIFF feature is created, depend-ing on whether the two NPs are the same string be-fore being replaced with the UNSEEN token.Lexical features are applicable when neither NPjnor NPk is UNSEEN.
A lexical feature is an orderedpair consisting of the heads of the NPs.
For a pro-noun or a common noun, the head is the last word ofthe NP; for a proper name, the head is the entire NP.Semi-lexical features aim to improve generaliza-tion, and are applicable when neither NPj nor NPk isUNSEEN.
If exactly one of NPj and NPk is taggedas a NE by the Stanford NE recognizer, we createa semi-lexical feature that is identical to the lexicalfeature described above, except that the NE is re-placed with its NE label.
On the other hand, if bothNPs are NEs, we check whether they are the samestring.
If so, we create a *NE*-SAME feature, where*NE* is replaced with the corresponding NE label.Otherwise, we check whether they have the same NEtag and a word-subset match (i.e., whether the wordtokens in one NP appears in the other?s list of wordtokens).
If so, we create a *NE*-SUBSAME feature,where *NE* is replaced with their NE label.
Other-wise, we create a feature that is the concatenation ofthe NE labels of the two NPs.The noun-pair-based features for the CR modelcan be generated using essentially the same method.Specifically, since each instance now corresponds to819an NP, NPk, and a preceding cluster, c, we can gener-ate a noun-pair-based feature by applying the abovemethod to NPk and each of the NPs in c, and its valueis the number of times it is applicable to NPk and c.3.2.2 Features Based on Verb PairsAs discussed above, features encoding the seman-tic roles of two NPs and the relatedness of the asso-ciated verbs could be useful for coreference resolu-tion.
Rather than encoding verb relatedness, we mayreplace verb relatedness with the verbs themselvesin these features, and have the learner learn directlyfrom coreference-annotated data whether two NPsserving as the objects of decry and denounce arelikely to be coreferent or not, for instance.Specifically, assuming that NPj and NPk are thearguments of two stemmed predicates, predj andpredk , in the training data, we create five featuresas follows.
First, we encode the semantic roles ofNPj and NPk as one of five possible values: ARG0-ARG0, ARG1-ARG1, ARG0-ARG1, ARG1-ARG0,and OTHERS (the default case).
Second, we createfive binary-valued features by pairing each of thesefive values with the two stemmed predicates.
Sincethese features are computed over two NPs, we canemploy them directly for the MP model.
Note thatby construction, exactly one of these features willhave a non-zero value.
For the CR model, we extendtheir definitions so that they can be computed be-tween an NP, NPk , and a preceding cluster, c. Specif-ically, the value of a feature is 1 if and only if itsvalue between NPk and one of the NPs in c is 1 un-der its original definition.The above discussion assumes that the two NPsunder consideration serve as predicate arguments.
Ifthis assumption fails, we will not create any featuresbased on verb pairs for these two NPs.3.3 World Knowledge from Unannotated DataPrevious work has shown that syntactic apposi-tions, which can be extracted using heuristics fromunannotated documents or parse trees, are a usefulsource of world knowledge for coreference resolu-tion (e.g., Daume?
III and Marcu (2005), Ng (2007),Haghighi and Klein (2009)).
Each extraction is anNP pair such as <Barack Obama, the president>and <Eastern Airlines, the carrier>, where the firstNP in the pair is a proper name and the second NP isa common NP.
Low-frequency extractions are typi-cally assumed to be noisy and discarded.We combine the extractions produced by Fleis-chman et al (2003) and Ng (2007) to form adatabase consisting of 1.057 million NP pairs, andcreate a binary-valued feature for our coreferencemodels using this database.
If the MP model is used,this feature will have the value 1 if and only if thetwo NPs appear as a pair in the database.
On theother hand, if the CR model is used, the feature foran instance involving NPk and preceding cluster cwill have the value 1 if and only if NPk and at leastone of the NPs in c appears as a pair in the database.4 Evaluation4.1 Experimental SetupAs described in Section 2, we use as our evalua-tion corpus the 411 documents that are coreference-annotated using the ACE and OntoNotes annota-tion schemes.
Specifically, we divide these docu-ments into five (disjoint) folds of roughly the samesize, training the MP model and the CR model us-ing SVMlight on four folds and evaluate their per-formance on the remaining fold.
The linguistic fea-tures, as well as the NPs used to create the trainingand test instances, are computed automatically.
Weemploy B3 and CEAF as described in Section 2.3 toscore the output of a coreference system.4.2 Results and Discussion4.2.1 Baseline ModelsSince our goal is to evaluate the effectiveness ofthe features encoding world knowledge for learning-based coreference resolution, we employ as ourbaselines the MR model and the CR model trainedon the Baseline feature set, which does not con-tain any features encoding world knowledge.
Forthe MP model, the Baseline feature set consists ofthe 39 features described in Section 2.3.1; for theCR model, the Baseline feature set consists of thecluster-level features derived from the 39 featuresused in the Baseline MP model (see Section 2.3.2).Results of the MP model and the CR model em-ploying the Baseline feature set are shown in rows 1and 8 of Table 1, respectively.
Each row contains theB3 and CEAF results of the corresponding corefer-ence model when it is evaluated using the ACE and820ACE OntoNotesB3 CEAF B3 CEAFFeature Set R P F R P F R P F R P FResults for the Mention-Pair Model1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.52 Base+YAGO Types (YT) 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.83 Base+YAGO Means (YM) 56.7 70.0 62.7 55.3 66.5 60.4 50.6 57.0 53.6 49.3 54.9 51.94 Base+Noun Pairs (WP) 57.5 70.6 63.4 55.8 67.4 61.1 51.6 57.6 54.4 49.7 55.4 52.45 Base+FrameNet (FN) 56.4 70.9 62.8 54.9 67.5 60.5 50.5 57.5 53.8 48.8 55.1 51.86 Base+Verb Pairs (VP) 56.9 71.3 63.3 55.2 67.6 60.8 50.7 57.9 54.0 49.0 55.4 52.07 Base+Appositives (AP) 56.9 70.0 62.7 55.6 66.9 60.7 50.3 57.1 53.5 49.1 55.1 51.9Results for the Cluster-Ranking Model8 Base 61.7 71.2 66.1 59.6 68.8 63.8 53.4 59.2 56.2 51.1 57.3 54.09 Base+YAGO Types (YT) 63.5 72.4 67.6 61.7 70.0 65.5 54.8 60.6 57.6 52.4 58.9 55.410 Base+YAGO Means (YM) 62.0 71.4 66.4 59.9 69.1 64.1 53.9 59.5 56.6 51.4 57.5 54.311 Base+Noun Pairs (WP) 64.1 73.4 68.4 61.3 70.1 65.4 55.9 62.1 58.8 53.5 59.1 56.212 Base+FrameNet (FN) 61.8 71.9 66.5 59.8 69.3 64.2 53.5 60.0 56.6 51.1 57.9 54.313 Base+Verb Pairs (VP) 62.1 72.2 66.8 60.1 69.3 64.4 54.4 60.1 57.1 51.9 58.2 54.914 Base+Appositives (AP) 63.1 71.7 67.1 60.5 69.4 64.6 54.1 60.1 56.9 51.9 57.8 54.7Table 1: Results obtained by applying different types of features in isolation to the Baseline system.ACE OntoNotesB3 CEAF B3 CEAFFeature Set R P F R P F R P F R P FResults for the Mention-Pair Model1 Base 56.5 69.7 62.4 54.9 66.3 60.0 50.4 56.7 53.3 48.9 54.5 51.52 Base+YT 57.3 70.3 63.1 58.7 67.5 62.8 51.7 57.9 54.6 50.3 55.6 52.83 Base+YT+YM 57.8 70.9 63.6 59.1 67.9 63.2 52.1 58.3 55.0 50.8 56.0 53.34 Base+YT+YM+WP 59.5 71.9 65.1 57.5 69.4 62.9 53.1 59.2 56.0 51.5 57.1 54.15 Base+YT+YM+WP+FN 59.6 72.1 65.3 57.2 69.7 62.8 53.1 59.5 56.2 51.3 57.4 54.26 Base+YT+YM+WP+FN+VP 59.9 72.5 65.6 57.8 70.0 63.3 53.4 59.8 56.4 51.8 57.7 54.67 Base+YT+YM+WP+FN+VP+AP 59.7 72.4 65.4 57.6 69.8 63.1 53.2 59.8 56.3 51.5 57.6 54.4Results for the Cluster-Ranking Model8 Base 61.7 71.2 66.1 59.6 68.8 63.8 53.4 59.2 56.2 51.1 57.3 54.09 Base+YT 63.5 72.4 67.6 61.7 70.0 65.5 54.8 60.6 57.6 52.4 58.9 55.410 Base+YT+YM 63.9 72.6 68.0 62.1 70.4 66.0 55.2 61.0 57.9 52.8 59.1 55.811 Base+YT+YM+WP 66.1 75.4 70.4 62.9 72.4 67.3 57.7 64.4 60.8 55.1 61.6 58.212 Base+YT+YM+WP+FN 66.3 75.1 70.4 63.1 72.3 67.4 57.3 64.1 60.5 54.7 61.2 57.813 Base+YT+YM+WP+FN+VP 66.6 75.9 70.9 63.5 72.9 67.9 57.7 64.4 60.8 55.1 61.6 58.214 Base+YT+YM+WP+FN+VP+AP 66.4 75.7 70.7 63.3 72.9 67.8 57.6 64.3 60.8 55.0 61.5 58.1Table 2: Results obtained by adding different types of features incrementally to the Baseline system.OntoNotes annotations as the gold standard.
As wecan see, the MP model achieves F-measure scores of62.4 (B3) and 60.0 (CEAF) on ACE and 53.3 (B3)and 51.5 (CEAF) on OntoNotes, and the CR modelachieves F-measure scores of 66.1 (B3) and 63.8(CEAF) on ACE and 56.2 (B3) and 54.0 (CEAF)on OntoNotes.
Also, the results show that the CRmodel is stronger than the MP model, corroboratingprevious empirical findings (Rahman and Ng, 2009).4.2.2 Incorporating World KnowledgeNext, we examine the usefulness of world knowl-edge for coreference resolution.
The remaining rowsin Table 1 show the results obtained when differenttypes of features encoding world knowledge are ap-plied to the Baseline system in isolation.
The bestresult for each combination of data set, evaluationmeasure, and coreference model is boldfaced.Two points deserve mention.
First, each typeof features improves the Baseline, regardless of thecoreference model, the evaluation measure, and theannotation scheme used.
This suggests that all thesefeature types are indeed useful for coreference reso-lution.
It is worth noting that in all but a few casesinvolving the FrameNet-based and appositive-basedfeatures, the rise in F-measure is accompanied by a8211.
The Bush White House is breeding non-duck ducks the same way the Nixon White House did: It hops on anissue that is unopposable ?
cleaner air, better treatment of the disabled, better child care.
The President cameup with a good bill, but now may end up signing the awful bureaucratic creature hatched on Capitol Hill.2.
The tumor, he suggested, developed when the second, normal copy also was damaged.
He believed coloncancer might also arise from multiple ?hits?
on cancer suppressor genes, as it often seems to develop in stages.Table 3: Examples errors introduced by YAGO and FrameNet.simultaneous rise in recall and precision.
This is per-haps not surprising: as the use of world knowledgehelps discover coreference links, recall increases;and as more (relevant) knowledge is available tomake coreference decisions, precision increases.Second, the feature types that yield the best im-provement over the Baseline are YAGO TYPE andNoun Pairs.
When the MP model is used, the bestcoreference system improves the Baseline by 1?1.3% (B3) and 1.3?2.8% (CEAF) in F-measure.
Onthe other hand, when the CR model is used, the bestsystem improves the Baseline by 2.3?2.6% (B3) and1.7?2.2% (CEAF) in F-measure.Table 2 shows the results obtained when the dif-ferent types of features are added to the Baseline oneafter the other.
Specifically, we add the feature typesin this order: YAGO TYPE, YAGO MEANS, NounPairs, FrameNet, Verb Pairs, and Appositives.
Incomparison to the results in Table 1, we can see thatbetter results are obtained when the different typesof features are applied to the Baseline in combina-tion than in isolation, regardless of the coreferencemodel, the evaluation measure, and the annotationscheme used.
The best-performing system, whichemploys all but the Appositive features, outperformsthe Baseline by 3.1?3.3% in F-measure when theMR model is used and by 4.1?4.8% in F-measurewhen the CR model is used.
In both cases, thegains in F-measure are accompanied by a simulta-neous rise in recall and precision.
Overall, theseresults seem to suggest that the CR model is mak-ing more effective use of the available knowledgethan the MR model, and that the different featuretypes are providing complementary information forthe two coreference models.4.3 Example ErrorsWhile the different types of features we consideredimprove the performance of the Baseline primarilyvia the establishment of coreference links, some ofthese links are spurious.
Sentences 1 and 2 of Table3 show the spurious coreference links introduced bythe CR model when YAGO and FrameNet are used,respectively.
In sentence 1, while The President andBush are coreferent, YAGO caused the CR modelto establish the spurious link between The Presidentand Nixon owing to the proximity of the two NPsand the presence of this NP pair in the YAGO TYPErelation.
In sentence 2, FrameNet caused the CRmodel to establish the spurious link between The tu-mor and colon cancer because these two NPs are theARG0 arguments of develop and arise, which appearin the same semantic frame in FrameNet.5 ConclusionsWe have examined the utility of three majorsources of world knowledge for coreference resolu-tion, namely, large-scale knowledge bases (YAGO,FrameNet), coreference-annotated data (Noun Pairs,Verb Pairs), and unannotated data (Appositives), byapplying them to two learning-based coreferencemodels, the mention-pair model and the cluster-ranking model, and evaluating them on documentsannotated with the ACE and OntoNotes annotationschemes.
When applying the different types of fea-tures in isolation to a Baseline system that does notemploy world knowledge, we found that all of themimproved the Baseline regardless of the underlyingcoreference model, the evaluation measure, and theannotation scheme, with YAGO TYPE and NounPairs yielding the largest performance gains.
Nev-ertheless, the best results were obtained when theywere applied in combination to the Baseline system.We conclude from these results that the different fea-ture types we considered are providing complemen-tary world knowledge to the coreference resolvers,and while each of them provides fairly small gains,their cumulative benefits can be substantial.822AcknowledgmentsWe thank the three reviewers for their invaluablecomments on an earlier draft of the paper.
This workwas supported in part by NSF Grant IIS-0812261.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In Proceedings of the Lin-guistic Coreference Workshop at The First Interna-tional Conference on Language Resources and Eval-uation, pages 563?566.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the 36th Annual Meeting of the Association forComputational Linguistics and the 17th InternationalConference on Computational Linguistics, Volume 1,pages 86?90.David Bean and Ellen Riloff.
2004.
Unsupervised learn-ing of contextual role knowledge for coreference reso-lution.
In Proceedings of the Human Language Tech-nology Conference of the North American Chapter ofthe Association for Computational Linguistics, pages297?304.Eric Bengtson and Dan Roth.
2008.
Understanding thevalues of features for coreference resolution.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 294?303.Volha Bryl, Claudio Giuliano, Luciano Serafini, andKateryna Tymoshenko.
2010.
Using backgroundknowledge to support coreference resolution.
In Pro-ceedings of the 19th European Conference on ArtificialIntelligence, pages 759?764.Eugene Charniak.
1972.
Towards a Model of Children?sStory Comphrension.
AI-TR 266, Artificial Intelli-gence Laboratory, Massachusetts Institute of Technol-ogy.Aron Culotta, Michael Wick, and Andrew McCallum.2007.
First-order probabilistic models for coreferenceresolution.
In Human Language Technologies 2007:The Conference of the North American Chapter of theAssociation for Computational Linguistics; Proceed-ings of the Main Conference, pages 81?88.Hal Daume?
III and Daniel Marcu.
2005.
A large-scaleexploration of effective global features for a joint en-tity detection and tracking model.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 97?104.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 660?669.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages363?370.Michael Fleischman, Eduard Hovy, and AbdessamadEchihabi.
2003.
Offline strategies for online ques-tion answering: Answering questions before they areasked.
In Proceedings of the 41st Annual Meeting ofthe Association for Computational Linguistics, pages1?7.Aria Haghighi and Dan Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1152?1161.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:The 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, Com-panion Volume: Short Papers, pages 57?60.Thorsten Joachims.
2002.
Optimizing search engines us-ing clickthrough data.
In Proceedings of the EighthACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 133?142.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the Bell tree.
In Proceedings of the 42nd AnnualMeeting of the Association for Computational Linguis-tics, pages 135?142.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language Processing, pages 25?32.Ruslan Mitkov.
2002.
Anaphora Resolution.
Longman.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, pages 104?111.Vincent Ng.
2007.
Shallow semantics for coreferenceresolution.
In Proceedings of the Twentieth Inter-national Joint Conference on Artificial Intelligence,pages 1689?1694.Simone Paolo Ponzetto and Massimo Poesio.
2009.State-of-the-art NLP approaches to coreference reso-lution: Theory and practical recipes.
In Tutorial Ab-stracts of ACL-IJCNLP 2009, page 6.Simone Paolo Ponzetto and Michael Strube.
2006.Exploiting semantic role labeling, WordNet andWikipedia for coreference resolution.
In Proceedings823of the Human Language Technology Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 192?199.Sameer S. Pradhan, Wayne H. Ward, Kadri Hacioglu,James H. Martin, and Dan Jurafsky.
2004.
Shallowsemantic parsing using support vector machines.
InProceedings of the Human Language Technology Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, pages 233?240.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968?977.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrase coref-erence resolution: Making sense of the state-of-the-art.
In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 656?664.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
Coref-erence resolution with Reconcile.
In Proceedings ofthe ACL 2010 Conference Short Papers, pages 156?161.Fabian Suchanek, Gjergji Kasneci, and Gerhard Weikum.2007.
YAGO: A core of semantic knowledge unifyingwordnet and wikipedia.
In Proceedings of the WorldWide Web Conference, pages 697?706.Olga Uryupina, Massimo Poesio, Claudio Giuliano, andKateryna Tymoshenko.
2011.
Disambiguation andfiltering methods in using web knowledge for coref-erence resolution.
In Proceedings of the 24th Interna-tional Florida Artificial Intelligence Research SocietyConference.Xiaofeng Yang, Guodong Zhou, Jian Su, and Chew LimTan.
2003.
Coreference resolution using competitivelearning approach.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 176?183.Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, andSheng Li.
2008.
An entity-mention model for coref-erence resolution with inductive logic programming.In Proceedings of the 46th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 843?851.824
