The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 12?21,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsIdentifying science concepts and student misconceptionsin an interactive essay writing tutorSteven BethardUniversity of ColoradoBoulder, Colorado, USAsteven.bethard@colorado.eduIfeyinwa OkoyeUniversity of ColoradoBoulder, Colorado, USAifeyinwa.okoye@colorado.eduMd.
Arafat SultanUniversity of ColoradoBoulder, Colorado, USAarafat.sultan@colorado.eduHaojie HangUniversity of ColoradoBoulder, Colorado, USAhaojie.hang@colorado.eduJames H. MartinUniversity of ColoradoBoulder, Colorado, USAjames.martin@colorado.eduTamara SumnerUniversity of ColoradoBoulder, Colorado, USAtamara.sumner@colorado.eduAbstractWe present initial steps towards an interac-tive essay writing tutor that improves scienceknowledge by analyzing student essays for mis-conceptions and recommending science web-pages that help correct those misconceptions.We describe the five components in this sys-tem: identifying core science concepts, deter-mining appropriate pedagogical sequences forthe science concepts, identifying student mis-conceptions in essays, aligning student miscon-ceptions to science concepts, and recommend-ing webpages to address misconceptions.
Weprovide initial models and evaluations of themodels for each component.1 IntroductionStudents come to class with a variety of misconcep-tions present in their science knowledge.
For ex-ample, science assessments developed by the Amer-ican Association for the Advancement of Science(AAAS)1 showed that 49% of American 6th-8thgraders believe that the Earth?s tectonic plates areonly feet thick (while in fact they are miles thick)and that 48% of American 6th-8th graders believethat atoms of a solid are not moving (while in factall atoms are in constant motion).
A key challengefor interactive tutoring systems is thus to identify andcorrect such student misconceptions.In this article, we develop an interactive essay writ-ing tutor that tries to address these challenges.
Thetutor first examines a set of science webpages to iden-tify key concepts (Section 4) and attempts to order1http://assessment.aaas.org/the science concepts in a pedagogically appropriatelearning path (Section 5).
Then the tutor examines astudent essay and identifies misconception sentences(Section 6) and aligns these misconceptions to thetrue science concepts (Section 7).
Finally, the tutorsuggests science webpages that can help the studentaddress each of the misconceptions (Section 8).The key contributions of this work are:?
Demonstrating that a summarization approachcan identify core science concepts?
Showing how a learning path model can be boot-strapped from webpages with grade metadata?
Developing models for misconception identifi-cation based on textual entailment techniques?
Presenting an information retrieval approach toaligning misconceptions to science concepts?
Designing a system that recommends webpagesto address student misconceptions2 Related workInteractive tutoring systems have been designed fora variety of domains and applications.
Dialog-basedtutoring systems, such as Why2-Atlas (VanLehn etal., 2002), AutoTutor (Graesser et al, 2004) andMetaTutor (Azevedo et al, 2008), interact with stu-dents via questions and answers.
Student knowledgeis judged by comparing student responses to knowl-edge bases of domain concepts and misconceptions.These knowledge bases are typically manually cu-rated, and a new knowledge base must be constructedfor each new domain where the tutor is to be used.12Essay-based tutoring systems, such as SummaryStreet (Wade-Stein and Kintsch, 2004) or CLICK(de la Chica et al, 2008b), interact with students whoare writing a summary or essay.
They compare whatthe student has written to domain knowledge in theform of textbooks or webpages.
They typically do notrequire a knowledge base to be manually constructed,instead using natural language processing techniquesto compare the student?s essay to the information inthe textbooks or webpages.The current work is inspired by these essay-basedtutoring systems, where interaction revolves aroundessay writing.
However, where Summary Street re-lies primarily upon measuring how much of a text-book a student essay has ?covered?, we aim to givemore detailed assessments that pinpoint specific stu-dent misconceptions.
CLICK targets a similar goalto ours, but assumes that accurate knowledge mapscan be generated for both the domain knowledge andfor each student essay.
Our approach does not re-quire the automatic generation of knowledge maps,instead working directly with the sentences in thestudent essays and the webpages of science domainknowledge.3 System overviewOur system is composed of five key components.First, a core concept identifier examines domainknowledge (webpages) and identifies key concepts(sentences) that describe the most important piecesof knowledge in the domain.
Second, a concept se-quencer assigns a pedagogically appropriate order inwhich a student should learn the identified core con-cepts.
Third, a misconception identifier examines thestudent essay and identifies sentences that describemisconceptions the student has about the domain.Fourth, a misconception-concept aligner finds a coreconcept that can be used to correct each misconcep-tion.
Finally, a recommender takes all the informa-tion about core concepts and student misconceptions,decides what order to address the misconceptions in,and identifies a set of resources (webpages) for thestudent to read.To assemble this system, we draw on a variety ofexisting datasets (and some data collection of ourown).
For example, we use data from an annotationstudy of concept coreness to evaluate our model foridentifying domain concepts, and we use data fromscience assessments of the American Association forthe Advancement of Science to train and evaluate ourmodel for identifying misconceptions.
We use thisdisparate data to establish baseline models for each ofthe tutor?s components.
In the near future, this base-line tutoring system will be used to collect studentessays and other data that will allow us to developmore sophisticated model for each component.4 Identifying core conceptsThis first module aims at automatically identifying aset of core concepts in a given set of digital libraryresources or webpages.
Core concepts in a subjectdomain are critical ideas necessary to support deepscience learning and transfer in that domain.
Froma digital learning perspective, availability of suchconcepts helps in providing pedagogical feedbackto learners to support robust learning and also inprioritizing instructional intervention (e.g., decidingthe order in which to treat student misconceptions).A concept can be materialized using different levelsof linguistic expressions (e.g.
phrases, sentences orparagraphs), but for this work, we focus only onindividual sentences as expressions of concepts.We used COGENT (de la Chica et al, 2008a), amulti-document summarization system to extract con-cepts (i.e.
sentences) from a given set of resources.In the following two subsections, we describe theCOGENT system, discuss how we used it for coreconcept extraction and report the results of its evalu-ation of effectiveness.4.1 ModelCOGENT is a text summarizer that builds on MEAD(Radev et al, 2004), a multidocument summarizationand evaluation platform .
MEAD was originally de-veloped to summarize news articles.
COGENT aimsto generate pedagogically useful summaries fromeducational resources.COGENT extends MEAD by incorporating newfeatures in the summarization process.
MEAD usesa set of generic (i.e.
domain-independent) features toevaluate each sentence in the given set of documents.These features include the length of the sentence, thedistance from the sentence to the beginning of thedocument, etc.
Individual scores of a sentence along13these dimensions are combined to assign a total scoreto the sentence.
After removing redundant sentences,MEAD then generates a summary using the sentencesthat had the highest scores.
A user-specified parame-ter determines the number of sentences included inthe summary.COGENT extends this framework by incorporat-ing new domain-general and domain-specific featuresin the sentence scoring process.
The domain-generalfeatures include a document structure feature, whichtakes into account a sentence?s level in terms ofHTML headings, and a content word density fea-ture, which computes the ratio of content words tofunction words.
The domain-specific features includean educational standards feature, which uses a TF-IDF based textual similarity score between a sentenceand nationally recognized educational goals from theAmerican Association for the Advancement of Sci-ence (AAAS) Benchmarks (Project2061., 1993) andthe associated National Science Education Standards(NRC, 1996), and a gazetteer feature, which scoressentences highly that mention many unique namesfrom a gazetteer of named entities.While in the past, COGENT was used primarilyas a summarization system, in the current work, weevaluate its utility as a means of identifying coreconcepts.
That is, are the top sentences selectedby COGENT also the sentences describing the keyscience concepts in the domain?4.2 EvaluationWe evaluate the core concept extraction module byassessing the extracted concepts against human ex-pert annotations.
We ran an annotation study wheretwo human experts assigned ?coreness?
ratings toa selected set of sentences collected from digitalresources in three science domains: Plate Tecton-ics, Weather and Climate, and Biological Evolution.These experts had been recruited based on their train-ing and expertise in the selected subject domains.First, a set of digital resources was selected fromthe Digital Library for Earth System Education(DLESE) 2 across the three subject domains.
ThenCOGENT was used to extract the top 5% sentencesfor each domain.
The experts then annotated eachextracted sentence with its coreness rating on a scale2http://www.dlese.orgExtraction %0.5% 1.0% 2.5% 5.0%Plate Tectonics 3.33 3.27 3.00 2.81Weather and Climate 3.13 2.97 3.07 2.99Biological Evolution 2.00 2.13 2.46 2.25Table 1: Average coreness of sentences extracted at differ-ent percentages in each domainof 1 to 4, 4 being the highest.
Human annotation isa time-consuming process and this is why we hadto limit the number of extracted sentences to a mod-erate 5% (which is still more than 400 sentences).17% of the sentences were double annotated and theinter-rater reliability, measured by Spearman?s rho,was 0.38.
These expert ratings of sentences form thebasis of our evaluation.Table 1 shows the average coreness assigned by theexperts to sentences extracted by COGENT in eachdomain, for different extraction percentages.
For ex-ample, if COGENT is used to extract the top 1% ofsentences from all the Plate Tectonics resources, thenthe average of their coreness ratings (as assigned bythe experts) is 3.27, representing a high level of core-ness.
This is essentially a measure of the precisionof COGENT at 1% extraction.
Note that we cannotcalculate a measure of recall without asking expertsto annotate all of the domain sentences, a time con-suming task which was outside of the scope of thisstudy.The performance of COGENT was the best in thePlate Tectonics domain since the domain-aware fea-tures (e.g.
the gazetteer features) used to train CO-GENT were selected from this domain.
In the ?neardomain?
of Weather and Climate, the performance isstill good, but performance falls in the ?far domain?of Biological Evolution, because of the significantdifferences between the training domain and the testdomain.
In the two latter domains, the performanceof COGENT was also inconsistent in that with anincrease in the extraction percentage, the averagecoreness increased in some cases and decreased inothers.
This inconsistency and overall degradationin performance in the two latter domains are indica-tive of the importance of introducing domain-awarefeatures into COGENT.It is evident from the values in Table 1 that thecore concepts extraction module does a decent job,14especially when trained with appropriate domain-aware features.5 Sequencing core conceptsThe goal of this next component is to take a set ofcore science concepts (sentences), as produced bythe preceding module, and predict an appropriate se-quence in which those concepts should be learned bythe student.
Some concepts serve as building blocksfor other concepts, and thus it is essential to learn thebasic concepts first (and address any misconceptionsassociated with them) before moving on to other con-cepts that depend on the basic concepts.
For example,a student must first understand the concept of tectonicplates before they can understand the concept of aconvergent plate boundary.
The sequence of coreconcepts that results from this module will serve asinput for the later module that prioritizes a student?smisconceptions.There may exist several different but reasonableconcept sequences (also known as learning paths) ?the goal of this component is to recommend at leastone of these.
As a first step, we focus on generatinga single concept sequence that represents a generalpath through the learning goals, much like textbooksand curriculums do.5.1 ModelsOur model for concept sequencing is a pair-wiseordering model, that takes two concepts c1 and c2,and predicts whether c1 should come before or afterc2 in the recommended learning path.
Formally,SEQUENCE(c1, c2) ={0 if c1 < c21 if c1 ?
c2To generate a complete ordering of concepts, weconstruct a precedence table from these pair-wisejudgments and generate a path that is consistent withthese judgments.We learn the SEQUENCE model as a supervisedclassifier, where a feature vector is extracted for eachof the two concepts and the two feature vectors, con-catenated, serve as the input to the classifier.
For eachword in each concept, we include the following twofeatures:?
local word count - the number of times theword appeared in this concept?
global word count - the log of the ratio betweenthe number of times the word occurred in theconcept and the number of times it occurred ina background corpus, Gigaword (Graff, 2002)These features are motivated by the work of Tanaka-ishii et al(2010) that showed that local and globalword count features were sufficient to build a pair-wise readability classifier that achieved 90% accu-racy.For the supervised classifier, we consider naiveBayes, decision trees, and support vector machines.5.2 EvaluationTo evaluate our concept sequencing model, we gath-ered learning paths from experts in high school earthscience.
Using the model from Section 4, we selected30 core concepts for the domain of plate tectonics.We asked two earth science experts to each come upwith two learning paths for these core concepts, withthe first path following an evidence or research basedand second path following a traditional learning path.An evidence or research based learning path, isa pedagogy where students are encouraged to usethe scientific method to learn about a phenomena, i.ethey gather information by observing the phenomena,form a hypothesis, perform experiment, collect andanalyze data and then interpret the data and drawconclusions that hopefully align with the current un-derstanding about the phenomena.
A teacher thatuses this learning path acts as a guide on the side.
Atraditional learning path on the other hand, is the ped-agogy where teachers are simply trying to pass on thecorrect information to students rather than letting thestudents discover the information themselves.
In aclassroom environment, a teacher using this learningpath would be seen as the classical sage on stage.We used the learning paths collected from the ex-perts to form two test sets, one for the evidence-basedpedagogy, and one for the traditional pedagogy.
Foreach pedagogy, we asked which of all the possiblepair-wise orderings our experts agreed upon.
For ex-ample, if the first expert said that A < B < C andthe second expert said that A < C < B, then bothexperts agreed that A < B and A < C, while theydisagreed on whether B < C or C < B.
Note thatwe evaluate pair-wise orderings here, not a completeranking of the concepts, because the experts did not15Pedagogy Pairs (%) c1 < c2 c1 ?
c2Evidence 637 (68%) 48.5% 51.5%Traditional 613 (70%) 48.5% 51.5%Table 2: Test sets for sequencing concepts.
The Pairscolumn shows how many pairs the experts agreed upon(out of a total of 30 ?
29 = 870 pairs).produce a total ordering of the concepts, only a par-tial tree-like ordering.
The experts put the conceptsin levels, with concepts in the same level having noprecedence relationship, while a concept in a lowerlevel preceded a concept in a higher level.For our test sets, we selected only the pairs onwhich both experts agreed.
Table 2 shows that expertsagreed on 68-70% of the pair-wise orderings.
Table2 also shows the percentage of each type of pair-wiseordering (c1 < c2 vs. c1 ?
c2) present in the data.Note that even though all concepts are paired with allother concepts, because the experts do not producecomplete orderings, the number of agreements foreach type of ordering may not be the same.
Considerthe case where expert E1 says that concepts A andB are on the same level (i.e., A = B) and expert E2says that concept A is in a lower level than conceptB (i.e., A < B).
Then for the pair (A,B), theydisagree on the relation (E1 says A ?
B while E2says A < B) but for the pair (B,A) they agree onthe relation (they both say B ?
A).
As a result, thec1 ?
c2 class is slightly larger than the c1 < c2 class.Since these data sets were small, we reserved themfor testing, and trained our pair-wise classificationmodel using a proxy task: ordering sentences bygrade.
In this task, the model is given two sentencess1 and s2, one written for middle school and writtenfor high school, and asked to decide whether s1 < s2(i.e.
s1 is the middle school sentence) or s2 < s1(i.e.
s2 is the middle school sentence).
We expectthat a model for ordering sentences by grade shouldalso be a reasonable model for ordering conceptsfor a pedagogical learning path.
And importantly,getting grade ordering data automatically is easy: theDigital Library for Earth System Education (DLESE)contains a variety of earth science resources withmetadata about the grade level they were written for.To construct the training data, we searched theDLESE website for text resources that containedthe words earthquake or plate tectonics.
We col-Baseline NaiveBayes SVMEvidence 51.5% 60.8% 53.3%Traditional 51.5% 56.6% 49.7%Table 3: Accuracy result from Naive Bayes and SVM forclassifying the core conceptslected 10 such resources for each of the two gradecohorts, middle school (we allowed anything K-8)and high school (we allowed anything 9+).
We down-loaded the webpage for each resource, and used CO-GENT to extract the 20 most important sentencesfrom each.
This resulted in 200 sentences for eachof the two grade cohorts.
To create pairs of grade-ordered sentences, we paired up middle and highschool concepts both ways: middle school first (i.e.SEQUENCE(cm, ch) = 0) and high school first (i.e.SEQUENCE(ch, cm) = 1).
This resulted in 40,000grade-ordered sentence pairs for training.We then used this proxy-task training data totrain our models.
We extracted 1702 unique non-stopwords from the training data, resulting in 3404features per concept, and 6808 features per con-cept pair (i.e.
per classification instance).
On thegrade-ordering task, we evaluated three models usingWEKA3, a naive Bayes model, a decision tree (J48)model, and a support vector machine (SVM) model.Using a stratified 50/50 split of the training data, wefound that the naive Bayes and SVM models bothachieved an accuracy of 80.2%, while the decisiontree achieved only 62%.
So, we selected the naiveBayes and SVM models for our real task, conceptsequencing.Table 3 shows the performance of the two modelson the expert judgments of concept sequencing.
Wefind that the naive Bayes model produces more expert-like concept sequences than would be generated bychance and also outperforms the SVM model on theconcept sequencing task.
For the final output of themodule, we combine the pair-wise judgments into acomplete concept sequence, breaking any ties in thepair-wise judgments by preferring the order of theconcepts in the output of the core concept identifier.3http://www.cs.waikato.ac.nz/ml/weka/166 Identifying student misconceptionsThe previous components have focused on analyzingthe background knowledge ?
finding core conceptsin the domain and selecting an appropriate learningsequence for these concepts.
The current componentfocuses on the student essay, using the collected back-ground knowledge to help analyze the essay and givefeedback.Given a student essay, the goal of this componentis to identify which sentences in the essay are mostlikely to be misconceptions.
The task of misconcep-tion identification is closely related to the task oftextual entailment (Dagan et al, 2006), in which thegoal is to predict if a hypothesis sentence, H, can bereasonably concluded given another sentence, T. Inmisconception identification, the goal is to predict ifa student sentence can be concluded from any com-bination of the sentences in the domain knowledge,similar to a textual entailment task with a single Hbut many Ts.
A student sentence that can not beconcluded from the domain knowledge is likely amisconception.6.1 ModelsWe developed two models for identifying studentmisconceptions, inspired by work in textual entail-ment that showed that a model that simply counts thewords in H that appeared in T, after expanding thewords in T using WordNet, achieves state-of-the-artperformance (Shnarch et al, 2011)4.The Coverage model scores a student sentenceby counting the number of its words that are also insome domain sentence.
Low-scoring sentences arelikely misconceptions.
Formally:SCORE(s) =|s ?
d||s|d =?s??DEXPAND(s?
)where s is a student sentence (a list of words), D isthe set of domain sentences, and EXPAND performslexical expansion on the words of a sentence.The Retrieval model indexes the domain sen-tences with an information retrieval system (we use4The paper also proposes a more elaborate probabilisticmodel, but shows that the ?lexical coverage?
model we adopthere is quite competitive both with their probabilistic model andwith the top-performing systems of RTE5 and RTE6.Lucene5), and scores a student sentence by queryingthe index and summing the scores.
Formally:SCORE(s) =?s?
?DSCORElucene(s, EXPAND(s?
))where s, D and EXPAND are defined as before, andSCORElucene is a cosine over TF-IDF vectors6.For both the Coverage and Retrieval models, weconsider the following lexical expansion techniquesfor defining the EXPAND function:?
tokens ?
words in the sentence (no expansion)?
tokens, synsets ?
words in the sentence, plusall lemmas of all WordNet synsets of each word?
tokens, synsetsexpanded ?
words in the sentence,plus all lemmas of all WordNet synsets of eachword, plus all lemmas of derived forms, hy-ponyms or meroynms of the WordNet synsets?
tokens, synsetsexpanded?4 ?
words in the sen-tence, plus all lemmas of all WordNet synsets ofeach word, plus all lemmas of WordNet synsetsreachable by a path of no more than 4 linksthrough derived forms, hyponyms or meroynms6.2 EvaluationWe evaluate the quality of our misconception identi-fication models using data collected from the Amer-ican Association for the Advancement of Science?sProject 2061 Science Assessment Website7.
Thiswebsite identifies the main ideas in various topicsunder Life Science, Physical Science and Earth Sci-ence, and for each idea provides several sentencesof description along with its individual concepts andcommon student misconceptions.We used 3 topics (17 ideas, averaging 6.2 descrip-tion sentences, 7.1 concept sentences and 9.9 miscon-ception sentences each) as a development set:CE CellsAM Atoms, Molecules, and States of MatterPT Plate TectonicsWe used 11 topics (64 ideas, averaging 5.9 descrip-tion sentences, 9.4 concept sentences and 8.6 miscon-ception sentences each) as the test set:5http://lucene.apache.org6See org.apache.lucene.search.Similarity javadoc for details.7http://assessment.aaas.org/17Model MAP P@1Randomly ordered 0.607 0.607Coverage - tokens 0.647 0.471Coverage - tokens, synsets 0.633 0.529Coverage - tokens, synsetsexpanded 0.650 0.471Coverage - tokens, synsetsexpanded?4 0.690 0.706Retrieval - tokens 0.665 0.529Retrieval - tokens, synsets 0.641 0.471Retrieval - tokens, synsetsexpanded 0.650 0.529Retrieval - tokens, synsetsexpanded?4 0.684 0.647Table 4: Development set results for identifying miscon-ceptions.EN Evolution and Natural SelectionBF Human Body SystemsIE Interdependence in EcosystemsME Matter and Energy in Living SystemsRH Reproduction, Genes, and HeredityEG Energy: Forms, Transformation, Transfer.
.
.FM Force and MotionSC Substances, Chemical Reactions.
.
.WC Weather and Climate: Basic ElementsCL Weather and Climate: Seasonal DifferencesWE Weathering, Erosion, and DepositionFor the evaluation, we provide all of the idea?s de-scription sentences as the domain knowledge, andcombine all of an idea?s concepts and misconcep-tions into a ?student essay?8.
We then ask the systemto rank the sentences in the essay, placing miscon-ceptions above true concepts.
Accuracy at placingmisconceptions at the top of the ranked list is thenmeasured using mean average precision (MAP) andprecision at the first item (P@1).The models were compared to a chance baseline:the expected MAP and P@1 if the concept and mis-conception sentences were ordered randomly.
Table 4shows that on the development set, while all modelsoutperformed the random ordering baseline?s MAP(0.607), only models with lexical expansion from4-link WordNet chains outperformed the baseline?sP@1 (0.607).
The Coverage and Retrieval models us-ing this expansion technique had comparable MAPs8These ?student essays?
are a naive approximation of realessays, but the sentences are at least drawn from real student er-rors.
In the future, we hope to create an evaluation corpus wherereal student essays have been annotated for misconceptions.Model MAP P@1Randomly ordered 0.487 0.487Coverage - tokens, synsetsexpanded?4 0.603 0.578Retrieval - tokens, synsetsexpanded?4 0.644 0.625Table 5: Test set results for identifying misconceptions.
(0.690 vs. 0.684), but the Coverage model had ahigher P@1 (0.706 vs. 0.647).
These top two mis-conception identification models were evaluated onthe test set.
Table 5 shows that both models againoutperformed the random ordering baseline, and theRetrieval model outperformed the Coverage model(0.644 vs. 0.603 MAP, 0.625 vs. 0.578 P@1).7 Aligning misconceptions to conceptsThe goal of this component is to take the miscon-ception sentences identified in a student essay andalign them to the core science concepts identified forthe domain.
For example, a student misconceptionlike Earth?s plates cannot bend would be aligned toa science concept like Mountains form when platematerial slowly bends over time.7.1 ModelsThe model for misconception-concept alignmenttakes a similar approach to that of the Retrievalmodel for misconception identification.
The align-ment model applies lexical expansion to each wordin a core science concept, indexes the expanded con-cepts with an information retrieval system, and scoreseach concept for its relevance to a student misconcep-tion by querying the index with the misconceptionand returning the index?s score for that concept.
For-mally:SCORE(c) = SCORElucene(m, EXPAND(c))where m is the query misconception, c is the scienceconcept, and EXPAND and SCORElucene are definedas in the Retrieval model for misconception identi-fication.
The concept with the highest score is theconcept that best aligns to the student misconceptionaccording to the model.For lexical expansion, we consider the same defini-tions of EXPAND as for misconception identification:tokens; tokens, synsets; tokens, synsetsexpanded;and tokens, synsetsexpanded?4.18Model MAP P@1Randomly ordered 0.276 0.276Alignment - Tokens 0.731 0.639Alignment - Tokens, synsets 0.813 0.734Alignment - tokens, synsetsexpanded 0.790 0.698Alignment - Tokens, synsetsexpanded?4 0.762 0.639Table 6: Development set results for aligning concepts tomisconceptions.7.2 EvaluationWe again leverage the AAAS Science Assessments toevaluate the misconception-concept alignment mod-els.
In addition to identifying key science ideas, andthe concepts and common misconceptions withineach idea, the AAAS Science Assessments providelinks between the misconceptions and the concepts.Usually there is a single concept to which each mis-conception is aligned, but the AAAS data aligns asmany as 16 concepts to a misconception in somecases.For the evaluation, we give the system one miscon-ception from an idea, and the list of all concepts fromthat idea, and ask the system to rank the concepts9.If the system performs well, the concepts that arealigned to the misconception should be ranked abovethe other concepts.
Accuracy at placing the alignedconcepts at the top of the ranked list is then measuredusing mean average precision (MAP) and precisionat the first item (P@1).The models were compared to a chance baseline:the expected MAP and P@1 if the concept and mis-conception sentences were ordered randomly.
Ta-ble 6 shows that on the development set, all modelsoutperformed the random ordering baseline.
Lexi-cal expansion with tokens and synsets achieved thehighest performance, 0.813 MAP and 0.734 P@1.This model was evaluated on the test set, and Table 7shows that the model again outperformed the randomordering baseline, achieving 0.704 MAP and 0.611P@1.
Overall, these are promising results ?
given astudent misconception, the model?s first choice for aconcept to address the misconception is helpful morethan 60% of the time.9As discussed in Section 6.2, there are on average 9.4 con-cepts per item.
This is not too far off from the 10-20 core con-cepts we typically expect the tutor to extract for each domain.Model MAP P@1Randomly ordered 0.259 0.259Alignment - Tokens, synsets 0.704 0.611Table 7: Test set results for aligning concepts to miscon-ceptions.8 Recommending resourcesThe goal of this component is to take a set of studentmisconceptions, the core science concepts to whicheach misconception is aligned, and the pedagogicalordering of the core science concepts, and recom-mend digital resources (webpages) to address themost important of the misconceptions.
For example,a student that believes that water evaporates into theair only when the air is very warm might be directedto websites about evaporation and condensation.
Therecommended resources are intended to help the stu-dent quickly locate the concept knowledge necessaryto correct each of their misconceptions.8.1 ModelsThe intuition behind our model is simple: sentencesfrom recommended resources should contain thesame or lexically related terminology as both themisconception sentences and their aligned concepts.As a first approach to this problem, we focus on theoverlap between recommended sentences and themisconception sentences, and use an information re-trieval approach to build a resource recommender.First, the user gives the model a set of domainknowledge webpages, and we use an information re-trieval system (Lucene) to index each sentence fromeach of the webpages.
(Note that we index all sen-tences, not just core concept sentences.)
Given astudent misconception, we query the index and iden-tify the source URL for each sentence that is returned.We then return the list of the recommended URLs,keeping only the first instance of each URL if dupli-cates exist.
Formally:SCORE(url) = maxs?urlSCORElucene(m, s)where url is a domain resource, s is a sentence from adomain resource and m is the student misconception.URLs are ranked by score and the top k URLs arereturned as recommendations.198.2 EvaluationAs a preliminary evaluation of the resource recom-mendation model, we obtained student misconcep-tion sentences that had been aligned to concepts ina knowledge map of plate tectonics (Ahmad, 2009).The concepts in the knowledge map were originallydrawn from 37 domain webpages, thus each conceptcould serve as a link between a student misconcep-tion and a recommended webpage.
For evaluation,we took all 11 misconceptions for a single student,where each misconception had been aligned throughthe concepts to on average 3.4 URLs.
For each mis-conception, we asked the recommender model torank the 37 domain URLs in order of their relevanceto the student misconception.We expect the final interactive essay writing sys-tem to return up to k = 5 resources for each mis-conception, so we evaluated the performance of therecommender model in terms of precision at five(P@5).
That is, of the top five URLs recommendedby the system, how many were also recommendedby the experts?
Averaging over the 11 student mis-conception queries, the current model achieves P@5of 32%, an acceptable initial baseline as randomlyrecommending resources would achieve only P@5of 9%.9 DiscussionIn this article, we have presented our initial stepstowards an interactive essay writing system that canhelp students identify and remedy misconceptions intheir science knowledge.
The system relies on tech-niques drawn from a variety of areas of natural lan-guage processing research, including multi-documentsummarization, textual entailment and informationretrieval.
Each component has been evaluated inde-pendently and demonstrated promising initial perfor-mance.A variety of challenges remain for this effort.
Thecore concept identification system performs well onthe plate tectonics domain that it was originally de-veloped for, but poorer on more distant domains,suggesting the need for more domain-independentfeatures.
The model for sequencing science conceptspedagogically uses only the most basic of word-basedfeatures, and could potentially benefit from featuresdrawn from other research areas such as text readabil-ity.
The misconception identification and alignmentmodels perform well on the AAAS science assess-ments but have not yet been evaluated on real studentessays, which may require moving from lexical cover-age models to more sophisticated entailment models.Finally, the recommender model considers only in-formation about the misconception sentence (not thealigned core concept nor the pedagogical ordering ofconcepts) and recommends entire resources insteadof directing students to specifically relevant sentencesor paragraphs.Perhaps the most important challenge for this workwill be moving from evaluating the components in-dependently to a whole-system evaluation in the con-text of a real essay writing task.
We are currentlydesigning a study to gather data on students using thesystem, from which we hope to derive informationabout which components are most reliable or usefulto the students.
This information will help guide ourresearch to focus on improving the components thatyield the greatest benefits to the students.References[Ahmad2009] Faisal Ahmad.
2009.
Generating conceptu-ally personalized interactions for educational digitallibraries using concept maps.
Ph.D. thesis, Universityof Colorado at Boulder.
[Azevedo et al2008] Roger Azevedo, Amy Witherspoon,Arthur Graesser, Danielle McNamara, Vasile Rus,Zhiqiang Cai, Mihai Lintean, and Emily Siler.
2008.MetaTutor: An adaptive hypermedia system for train-ing and fostering self-regulated learning about complexscience topics.
In Meeting of Society for Computers inPsychology, November.
[Dagan et al2006] Ido Dagan, Oren Glickman, and Ber-nardo Magnini.
2006.
The PASCAL recognisingtextual entailment challenge.
In Joaquin Quin?oneroCandela, Ido Dagan, Bernardo Magnini, and Florenced?Alche?
Buc, editors, Machine Learning Challenges.Evaluating Predictive Uncertainty, Visual Object Clas-sification, and Recognising Tectual Entailment, volume3944 of Lecture Notes in Computer Science, pages 177?190.
Springer Berlin / Heidelberg.
[de la Chica et al2008a] Sebastian de la Chica, Faisal Ah-mad, James H. Martin, and Tamara Sumner.
2008a.Pedagogically useful extractive summaries for scienceeducation.
In Proceedings of the 22nd InternationalConference on Computational Linguistics - Volume 1,COLING ?08, pages 177?184, Stroudsburg, PA, USA.Association for Computational Linguistics.20[de la Chica et al2008b] Sebastian de la Chica, Faisal Ah-mad, Tamara Sumner, James H. Martin, and KirstenButcher.
2008b.
Computational foundations for person-alizing instruction with digital libraries.
InternationalJournal on Digital Libraries, 9(1):3?18, July.
[Graesser et al2004] Arthur Graesser, Shulan Lu, GeorgeJackson, Heather Mitchell, Mathew Ventura, AndrewOlney, and Max Louwerse.
2004.
AutoTutor: A tutorwith dialogue in natural language.
Behavior ResearchMethods, 36:180?192.
[Graff2002] David Graff.
2002.
English Gigaword.
Lin-guistic Data Consortium.
[NRC1996] National Research Council NRC.
1996.National Science Education Standards.
NationalAcademy Press, Washington DC.
[Project2061.1993] Project2061.
1993.
Benchmarks forScience Literacy.
Oxford University Press, New York,United States.
[Radev et al2004] Dragomir R. Radev, Hongyan Jing,Ma?gorzata Stys?, and Daniel Tam.
2004.
Centroid-based summarization of multiple documents.
Inf.
Pro-cess.
Manage., 40(6):919?938, November.
[Shnarch et al2011] Eyal Shnarch, Jacob Goldberger, andIdo Dagan.
2011.
A probabilistic modeling frame-work for lexical entailment.
In Proceedings of the49th Annual Meeting of the Association for Compu-tational Linguistics: Human Language Technologies,pages 558?563, Portland, Oregon, USA, June.
Associa-tion for Computational Linguistics.
[Tanaka-Ishii et al2010] K. Tanaka-Ishii, S. Tezuka, andH.
Terada.
2010.
Sorting texts by readability.
Compu-tational Linguistics, 36(2):203?227.
[VanLehn et al2002] Kurt VanLehn, Pamela Jordan, Car-olyn Rose?, Dumisizwe Bhembe, Michael Bo?ttner, AndyGaydos, Maxim Makatchev, Umarani Pappuswamy,Michael Ringenberg, Antonio Roque, Stephanie Siler,and Ramesh Srivastava.
2002.
The architecture ofWhy2-Atlas: A coach for qualitative physics essay writ-ing.
In Stefano Cerri, Guy Gouarde`res, and Fa?bioParaguac?u, editors, Intelligent Tutoring Systems, vol-ume 2363 of Lecture Notes in Computer Science, pages158?167.
Springer Berlin / Heidelberg.
[Wade-Stein and Kintsch2004] David Wade-Stein andEileen Kintsch.
2004.
Summary Street: Interactivecomputer support for writing.
Cognition and Instruc-tion, 22(3):333?362.21
