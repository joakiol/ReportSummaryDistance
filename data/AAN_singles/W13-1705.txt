Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 42?47,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsAutomated Essay Scoring for SwedishRobert ?stlingDepartment of LinguisticsStockholm UniversitySE-106 91 Stockholmrobert@ling.su.seAndre SmolentzovDepartment of LinguisticsStockholm UniversitySE-106 91 Stockholmasmolentzov@gmail.comBj?rn Tyrefors HinnerichDepartment of EconomicsStockholm UniversitySE-106 91 Stockholmbjorn.hinnerich@ne.su.seErik H?glinNational Institute of Economic ResearchKungsgatan 12-14103 62 Stockholmerik.hoglin@konj.seAbstractWe present the first system developed for auto-mated grading of high school essays written inSwedish.
The system uses standard text qual-ity indicators and is able to compare vocabu-lary and grammar to large reference corpora ofblog posts and newspaper articles.
The systemis evaluated on a corpus of 1 702 essays, eachgraded independently by the student?s ownteacher and also in a blind re-grading processby another teacher.
We show that our system?sperformance is fair, given the low agreementbetween the two human graders, and further-more show how it could improve efficiency ina practical setting where one seeks to identifyincorrectly graded essays.1 IntroductionAutomated Essay Scoring (AES) is the field of auto-matically assigning grades to student essays (Sher-mis and Burstein, 2003; Dikli, 2006).Previous work on AES has primarily focused onEnglish texts, and to the best of our knowledge noAES system for Swedish essays has been published.We exploit some peculiarities of the Swedish lan-guage, such as its compounding nature, to designnew features for classification.
We also use con-structions in the shape of hybrid n-grams (Tsao andWible, 2009) extracted from large corpora in theclassification.Earlier results from this work have been presentedin the B.A.
thesis of Smolentzov (2013), where fur-ther details can be found.
Source code, a trainedmodel as well as an on-line version of our tool areavailable from the website of the Department of Lin-guistics.1 Due to legal restrictions, we are currentlyunable to publish the corpus of essays used for train-ing the model and in our evaluation.
While this isvery regrettable, there are so far no suitable trainingcorpora available for Swedish that are publicly avail-able.
We hope in the future to be able to produce ananonymized version of the corpus, to be shared withother researchers.2 DataWe use a corpus of essays from the essay writingpart of the Swedish high school national exams inSwedish.2 These were collected using random sam-pling by Hinnerich et al(2011), who had them dig-itized, anonymized, and re-graded by high schoolteachers experienced with grading the national ex-ams.
The essays were originally graded by the stu-dent?s own teacher.
In total, 1 702 essays have all theinformation we require: digitized text and the twogrades.
The size of the corpus is 1 116 819 tokens,or an average of 656 per essay.
The essays havebeen automatically annotated with lemma and partof speech (PoS) information using Stagger (?stling,2012).There are four grades: IG (fail), G (pass), VG(pass with distinction) and MVG (excellent).
Hin-nerich et al(2011) found that the agreement be-tween the two human graders is rather low, and inthe set of essays used in this study only 780 (45.8%)of the 1 702 essays received the same grade by both1http://www.ling.su.se/aes2Course Svenska B, fall 2005/spring 2006.42TeacherIG G VG MVG SumBlindgrader IG 74 147 50 5 276G 68 437 293 55 853VG 12 136 223 75 446MVG 1 25 55 46 127Sum 155 745 621 181 1 702Table 1: Confusion matrix for the grades assigned by thestudents?
own teachers, and during the blind re-gradingprocess.
In total, 780 essays (45.8%) are assigned thesame grade.
Linear weighted ?
= 0.276graders.
In 148 cases (8.7%), the grade differencewas more than one step.In Table 1, we can clearly see that the blindgraders?
grades are generally lower.
The disagree-ment is also more severe for the grades at the ex-tremes of the scale.It is important to note that the grading guide-lines for the national exams do not focus exclu-sively on the quality of the language used, but ratheron the ability of the student to produce a coher-ent and convincing argument, understanding and re-lating to other texts, or describing personal experi-ences.
Some work has been carried out using high-level features in automated essay scoring.
Milt-sakaki and Kukich (2004) use some manual anno-tation to explore the role of coherence, and Attaliand Burstein (2005) automatically analyze the over-all structure of essays.
Others take the contents ofessays into account (Landauer et al 2003), whichis suitable for essay questions in non-language sub-jects.We will, however, focus on form rather than con-tent.
One important reason for this is that our cor-pus of essays is spread out over 19 different topics(in several cases with as few as 20?30 essays each),where the type of text expected can vary consider-ably between topics.3 MethodsWe use a supervised machine learning approach,based on a Linear Discriminant Analysis classifier inthe implementation of Pedregosa et al(2011).
Eachessay is represented by a feature vector, whose con-tents we will describe in some detail in the followingsections.It is important to note that we are using corre-lations between grade and different features of thetext, but the relationship between these features andthe qualities of the essay on which the grade shouldbe based may be complex.
As a cautionary tale, wecould mention that vocabulary related to cell phoneswas found to correlate strongly with essay grade.
Itturned out that poor students showed a strong pref-erence for one of the given essay topics, which hap-pened to center around cell phones.
In the field ofAES, it is particularly important to keep in mind thatcorrelation does not imply causation.3.1 Simple featuresWe use a number of features that may be directlymeasured from the text.
These are presented be-low, roughly in decreasing order of correlation withessay grade.
Most of the features have been dis-cussed in previous literature on AES (Attali andBurstein, 2005), and specifically in the context ofSwedish high school essays by Hultman and West-man (1977).
Some further features that did not con-tribute much to grading accuracy were tried, but willbe omitted from this discussion.Text length Since the essays are composed in aclassroom setting with a fixed amount of time allot-ted (five hours), a student?s fluency in writing is di-rectly mirrored in the length of an essay, which be-comes the feature that most strongly correlates withgrade.
While one might want to exclude the lengthfrom consideration in the grading process, it is im-portant to keep this correlation in mind since othermeasures may correlate with length, and thereforeindirectly correlate with essay grade without con-tributing any new information.Average word length The average number of let-ters per word also correlates with grade but onlyweakly with the length (in words).
It does howevercorrelate strongly with the distribution of parts ofspeech, primarily pronouns (which tend to be short)and nouns (which tend to be long, particularly sinceSwedish is a compounding language).OVIX lexical diversity measure OVIX (Hult-man, 1994) was in fact developed for the verypurpose of analyzing lexical diversity in Swedishhigh school essays, and has been found to correlate43strongly with grade in this setting.
At the same time,the measure is mostly independent of text length.OVIX = log ntokens/(2?log ntypeslog ntokens)Part of speech distribution The relative frequen-cies of different parts of speech also correlate withessay grade, although more weakly so than the re-lated measure of average word length.3.2 Corpus-induced featuresWhile the size our corpus of graded student essaysis in the order of one million words, much largeramounts of Swedish text are available from differ-ent sources, such as opinion pieces, news articles,and blog posts.
Due to the large amounts of textavailable, from tens of millions to several billions ofwords depending on the source, we can extract re-liable statistics even about relatively rare languagephenomena.By comparing student essays to statistics gatheredfrom different text types, we obtain new variablesthat often correlate strongly with essay grades.PoS tag cross-entropy The average cross-entropyper token from a PoS trigram model (with simpleadditive smoothing) is used to model the similarityon a syntactic level.
This includes both elements ofstyle (e.g.
frequent use of passive constructions) andmechanics (e.g.
agreement errors).
We use a corpusof news texts3 to train the model.Vocabulary cross-entropy With word frequencystatistics from two different text sources, we com-pute the average cross-entropy per token given a un-igram model, and use the difference between thesevalues for the two models to indicate which type oftext the present essay is most similar to.
In our ex-periments, the two text sources are of equal size andconsist of the news texts mentioned above, and a cor-pus of blog posts.Hybrid n-gram cross-entropy We can general-ize the vocabulary cross-entropy measure describedabove by using hybrid n-grams (Tsao and Wible,2009) rather than single words.
This allows for some3The corpus consists of ca 200 million words, crawled fromthe WWW editions of Dagens Nyheter and Svenska Dagbladet.patterns that are neither entirely grammatical nor en-tirely lexical to be used, complementing the two pre-vious approaches.
The same news and blog corporaas above are used.3.3 Language error featuresSpelling errors We implemented a simple spellchecker, using the SALDO lexicon (Borin and Fors-berg, 2009) and statistics from a corpus of news text.On average, a misspelling was detected in 0.63% ofall word tokens, or about four misspellings per essay.Manual inspection showed that the spell checkermade some errors, so it is reasonable to assume thatresults could be improved somewhat using a moreaccurate tool.Split compound errors Swedish is a compound-ing language, with noun compounding particularlyfrequent.
It is a fairly common error among inexpe-rienced writers to separate the segments of a com-pound word.
We use word uni- and bigram statisticsfrom a corpus of news texts to find instances of theseerrors in the essays.
Only 0.10% of word tokensare found to be incorrectly split, or less than oneinstance per essay on average.
As expected, thereis a (weak) negative correlation between split com-pound frequency and grade, which seems to be dueto a small number of poor essays with many sucherrors.3.4 Evaluation measuresThe simplest measure of overlap between twograders (either among humans, or between human(s)and machine) is the percentage of essays on whichthey agree about the grade.
However, in our set-ting this is not so informative because there is ahigh chance of graders assigning the same grade bychance, and this probability varies between differentpairs of graders.This makes comparisons difficult, so we insteaduse Cohen?s kappa value (Cohen, 1968), linearlyweighted according to the numeric values of gradesused by the Swedish school system: IG correspondsto 0 points, G to 10, VG to 15, and MVG to 20.A kappa value of 1 would indicate perfect agree-ment, while 0 would mean random agreement.
The44Feature Correlationntokens0.25 0.535ntokens 0.502hybrid n-gram cross-entropy 0.363vocabulary cross-entropy 0.361average word length 0.307OVIX 0.304nlong/ntokens 0.284spelling errors -0.257PoS cross-entropy 0.216split compound errors -0.208Table 2: Correlation between grade (average of twograders) and features.
Interactions between features arenot taken into account.
Only features with Pearson coef-ficient ?
> 0.2 are included, all are highly significant.weighted kappa value is computed as:?
= 1?
?i,j wijOij?i,j wijEijwhere Oij is the number of times annotator 1 as-signed grade i and annotator 2 assigned grade j,while Eij is the expected number of times for thesame event, given that both annotators randomly as-sign grades according to a multinomial distribution.wij is the difference in score between grades i andj, according to the above.4 Results4.1 Feature-grade correlationsFirst, we look at the correlations between thehuman-assigned grades and individual features.Since a linear machine learning algorithm is used,we use the Pearson coefficient to measure linear de-pendence.
Spearman?s rank correlation coefficientgives similar results.From Table 2 we can see that only ten of thefeatures show a correlation above 0.2.
There werestatistically significant (but weak) correlations be-low this threshold, e.g.
the ratios of different partsof speech, where the strongest correlations were?
= ?0.192 (pronouns) and ?
= 0.177 (preposi-tions).4.2 Automated gradingTable 3 shows the performance of our system, usingthe leave-one-out evaluation method on all 1 702 es-ComputerIG G VG MVG SumHumanavg.
IG 107 176 6 0 289G 61 752 110 11 934VG 2 225 189 17 433MVG 0 9 27 10 46Sum 170 1 162 332 38 1 702Table 3: Confusion matrix for the grades assigned by thesystem, and the average (rounded down) of the two hu-man graders.
In total, 1 058 essays (62.2%) are assignedthe same grade, ?
= 0.399.says, i.e.
evaluating each essay using a model trainedon all the other 1 701 essays.
We see that the com-puter?s grades are biased towards the most com-mon grade (G, pass), but that overall accuracy isquite high (62.2%, ?
= 0.399) compared to 58.4%(?
= 0.249) when using only the strongest feature(4th root of essay length), 54.9% when assigningthe most common grade to all essays, or the 45.8%(?
= 0.276) agreement between the two humangraders.It is also encouraging to see that only 28 essays(1.6%) receive a grade by the computer that differsmore than one step from the human-assigned grade.The corresponding figure is 148 essays (8.7%) be-tween the two humans.When training and evaluating using only thegrades of the blind grader, the agreement betweencomputer and human was 57.6% (?
= 0.369), andonly 53.6% (?
= 0.345) using the grades of thestudent?s teacher.
Both these figures are below the62.2% (?
= 0.399) obtained when using the aver-age grade, and the explanation closest at hand is thatthe features we model (partially) represent or corre-late with the actual grading criteria of the exam.Since the teachers are affected by various sourcesof bias (Hinnerich et al 2011), a weaker correla-tion (mirrored by a lower ?)
to any kind of ?objec-tive?
measure would be expected.
Similarly, usingthe average of two graders should decrease the largeindividual variance due to the difficult and partiallysubjective nature of the task, leading to a strongercorrelation with relevant features of the text.454.3 Re-gradingIn 148 cases (8.7%) of our 1 702 essays, the gradeassigned in the blind re-grading process differs bymore than one step from the original grade, and weperformed an experiment to see how efficiently thesehighly deviant grades could be identified.
This sce-nario could arise within an organization responsi-ble for evaluating the consistency in grading a na-tional exam, where resources are insufficient for re-grading all essays manually.
Given a training corpusof graded essays, our system could then be used toselect candidates among the larger set of essays forfurther manual re-grading.In other to evaluate the usefulness of this method,we let the system re-grade all essays based on theblind grades of all other essays (leave-one-out).
Inthe cases where the system?s grade differs by morethan one step from the teacher?s grade, we checkwhether the difference between the system?s gradeand that of the blind grader is less than between thetwo human graders.
It turns out that we can correctlyidentify 43 (29.1%) of the 148 cases in this way, withonly 91 essays (5.3% of the total) considered.In a scenario where we have a large amount ofessays but only the resources to manually re-gradea fraction of them, we can thus increase the ratio ofhighly deviant grades found from 8.7% (148/1702,by randomly choosing essays to re-grade) to 47%(43/91, by only re-grading those identified by oursystem).5 Conclusions and future workWe have presented a system for automatic gradingof Swedish high school essays.
While its accu-racy is not high enough to be used in grading high-stakes exams, we have demonstrated its usefulnessin a practical setting of finding instances of incorrectgrading (as identified by humans).
Novel aspects in-clude features based on constructions induced usingunsupervised methods, and on (language-specific)compounding errors.It would be interesting to apply some of our meth-ods to other languages and other data sets, for in-stance of second language learners.
Since our sys-tem is quite general, all that would be needed toadapt it to another domain is a training corpus ofgraded essays.
Adapting to another language wouldin addition require a PoS tagger and suitable unla-beled text corpora.AcknowledgmentsWe would like to thank the anonymous reviewers fortheir useful comments.ReferencesYigal Attali and Jill Burstein.
2005.
Automated essayscoring with e-rater R?
v.2.0.
Technical report, Educa-tional Testing Services.Lars Borin and Markus Forsberg.
2009.
All in the fam-ily: A comparison of SALDO and WordNet.
In Pro-ceedings of the Nodalida 2009 Workshop on WordNetsand other Lexical Semantic Resources ?
between Lexi-cal Semantics, Lexicography, Terminology and FormalOntologies, Odense.Jacob Cohen.
1968.
Weighted kappa: Nominal scaleagreement provision for scaled disagreement or partialcredit.
Psychological Bulletin, 70:213?220.Semire Dikli.
2006.
An overview of automated scoringof essays.
Journal of Technology, Learning, and As-sessment, 5.Bj?rn Tyrefors Hinnerich, Erik H?glin, and Magnus Jo-hannesson.
2011.
Are boys discriminated in swedishhigh schools?
Economics of Education Review,30:682?690.Tor G. Hultman and Margareta Westman.
1977.
Gymna-sistsvenska.
LiberL?romedel.Tor G. Hultman.
1994.
Hur gick det med ovix?
InSpr?kbruk, grammatik och spr?kf?r?ndring.
En fest-skrift till Ulf Teleman, pages 55?64.
Lund University.Thomas K. Landauer, Darrell Laham, and Peter Foltz.2003.
Automatic essay assessment.
Assessment in Ed-ucation, 10:295?308.E.
Miltsakaki and K. Kukich.
2004.
Evaluation of textcoherence for electronic essay scoring systems.
Natu-ral Language Engineering, 10:25?55.Robert ?stling.
2012.
Stagger: A modern POS taggerfor Swedish.
In Proceedings of the Swedish LanguageTechnology Conference (SLTC).F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Prettenhofer,R.
Weiss, V. Dubourg, J. Vanderplas, A. Passos,D.
Cournapeau, M. Brucher, M. Perrot, and E. Duches-nay.
2011.
Scikit-learn: Machine learning in Python.Journal of Machine Learning Research, 12:2825?2830.M.D.
Shermis and J. Burstein, editors.
2003.
AutomatedEssay Scoring: A Cross Disciplinary Perspective.
L.Erlbaum Associates.46Andr?
Smolentzov.
2013.
Automated Essay Scoring:Scoring Essays in Swedish.
Bachelor?s thesis, Depart-ment of Linguistics, Stockholm University.Nai-Lung Tsao and David Wible.
2009.
A method forunsupervised broad-coverage lexical error detectionand correction.
In Proceedings of the Fourth Work-shop on Innovative Use of NLP for Building Educa-tional Applications, EdAppsNLP ?09, pages 51?54,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.47
