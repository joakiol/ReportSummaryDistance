Noun-phrase co-occurrence statistics for semi-automatic semanticlexicon constructionBr ian  RoarkCognit ive and Linguistic SciencesBox 1978Brown UniversityProvidence, RI 02912, USABrian_Roark?Brown.
eduEugene CharniakComputer  ScienceBox 1910Brown UniversityProvidence, RI 02912, USAec@cs, brown, eduAbstractGenerating semantic lexicons semi-automatically could be a great time saver,relative to creating them by hand.
In thispaper, we present an algorithm for extractingpotential entries for a category from an on-linecorpus, based upon a small set of exemplars.Our algorithm finds more correct terms andfewer incorrect ones than previous work inthis area.
Additionally, the entries that aregenerated potentially provide broader coverageof the category than would occur to an indi-vidual coding them by hand.
Our algorithmfinds many terms not included within Wordnet(many more than previous algorithms), andcould be viewed as an "enhancer" of existingbroad-coverage r sources.1 In t roduct ionSemantic lexicons play an important role inmany natural anguage processing tasks.
Effec-tive lexicons must often include many domain-specific terms, so that available broad coverageresources, such as Wordnet (Miller, 1990), areinadequate.
For example, both Escort and Chi-nook are (among other things) types of vehi-cles (a car and a helicopter, respectively), butneither are cited as so in Wordnet.
Manu-ally building domain-specific lexicons can be acostly, time-consuming affair.
Utilizing exist-ing resources, such as on-line corpora, to aidin this task could improve performance both bydecreasing the time to construct the lexicon andby improving its quality.Extracting semantic information from wordco-occurrence statistics has been effective, par-ticularly for sense disambiguation (Schiitze,1992; Gale et al, 1992; Yarowsky, 1995).
InRiloff and Shepherd (1997), noun co-occurrencestatistics were used to indicate nominal cate-gory membership, for the purpose of aiding inthe construction of semantic lexicons.
Generi-cally, their algorithm can be outlined as follows:1.
For a given category, choose a small set ofexemplars (or 'seed words')2.
Count co-occurrence of words and seedwords within a corpus3.
Use a figure of merit based upon thesecounts to select new seed words4.
Return to step 2 and iterate n times5.
Use a figure of merit to rank words for cat-egory membership and output a ranked listOur algorithm uses roughly this same genericstructure, but achieves notably superior esults,by changing the specifics of: what counts asco-occurrence; which figures of merit to use fornew seed word selection and final ranking; themethod of initial seed word selection; and howto manage compound nouns.
In sections 2-5we will cover each of these topics in turn.
Wewill also present some experimental results fromtwo corpora, and discuss criteria for judging thequality of the output.2 Noun Co-Occur renceThe first question that must be answered in in-vestigating this task is why one would expectit to work at all.
Why would one expect thatmembers of the same semantic ategory wouldco-occur in discourse?
In the word sense disam-biguation task, no such claim is made: wordscan serve their disambiguating purpose regard-less of part-of-speech or semantic haracteris-tics.
In motivating their investigations, Riloffand Shepherd (henceforth R~S) cited severalvery specific noun constructions in which co-occurrence between ouns of the same semantic1110class would be expected, including conjunctions(cars and trucks), lists (planes, trains, and auto-mobiles), appositives (the plane, a twin-enginedCessna.)
and noun compounds (pickup truck).Our algorithm focuses exclusively on theseconstructions.
Because the relationship be-tween nouns in a compound is quite differentthan that between ouns in the other construc-tions, the algorithm consists of two separatecomponents: one to deal with conjunctions,lists, and appositives; and the other to dealwith noun compounds.
All compound nounsin the former constructions are represented bythe head of the compound.
We made the sim-plifying assumptions that a compound noun is astring of consecutive nouns (or, in certain cases,adjectives - see discussion below), and that thehead of the compound is the rightmost noun.To identify conjunctions, lists, and apposi-tives, we first parsed the corpus, using an ef-ficient statistical parser (Charniak et al, 1998),trMned on the Penn Wall Street Journal Tree-bank (Marcus et al, 1993).
We defined co-occurrence in these constructions using thestandard definitions of dominance and prece-dence.
The relation is stipulated to be transi-tive, so that all head nouns in a list co-occurwith each other (e.g.
in the phrase planes,trains, and automobiles all three nouns arecounted as co-occuring with each other).
Twohead nouns co-occur in this algorithm if theymeet the following four conditions:1. they are both dominated by a common NPnode2.
no dominating S or VP nodes are domi-nated by that same NP node3.
all head nouns that precede one, precedethe other4.
there is a comma or conjunction that pre-cedes one and not the otherIn contrast, R&S counted the closest nounto the left and the closest noun to the right ofa head noun as co-occuring with it.
Considerthe following sentence from the MUC-4 (1992)corpus: "A cargo aircraft may drop bombs anda truck may be equipped with artillery for war.
"In their algorithm, both cargo and bombs wouldbe counted as co-occuring with aircraft.
In ouralgorithm, co-occurrence is only counted withina noun phrase, between head nouns that areseparated by a comma or conjunction.
If thesentence had read: "A cargo aircraft, fighterplane, or combat helicopter ...", then aircraft,plane, and helicopter would all have counted asco-occuring with each other in our algorithm.3 S ta t i s t i cs  for  se lec t ing  and  rank ingR&S used the same figure of merit both for se-lecting new seed words and for ranking wordsin the final output.
Their figure of merit wassimply the ratio of the times the noun coocurswith a noun in the seed list to the total fre-quency of the noun in the corpus.
This statis-tic favors low frequency nouns, and thus neces-sitates the inclusion of a minimum occurrencecutoff.
They stipulated that no word occur-ing fewer than six times in the corpus wouldbe considered by the algorithm.
This cutoff hastwo effects: it reduces the noise associated withthe multitude of low frequency words, and itremoves from consideration a fairly large num-ber of certainly valid category members.
Ide-ally, one would like to reduce the noise withoutreducing the number of valid nouns.
Our statis-tics allow for the inclusion of rare occcurances.Note that this is particularly important givenour algorithm, since we have restricted the rele-vant occurrences to a specific type of structure;even relatively common nouns m~v not occur inthe corpus more than a handful of times in sucha context.The two figures of merit that we employ, oneto select and one to produce a final rank, usethe following two counts for each noun:1. a noun's co-occurrences with seed words2.
a noun's co-occurrences with any wordTo select new seed words, we take the ratioof count 1 to count 2 for the noun in question.This is similar to the figure of merit used inR&:S, and also tends to promote low frequencynouns.
For the final ranking, we chose the loglikelihood statistic outlined in Dunning (1993),which is based upon the co-occurrence ounts ofall nouns (see Dunning for details).
This statis-tic essentially measures how surprising the givenpattern of co-occurrence would be if the distri-butions were completely random.
For instance,suppose that two words occur forty times each,iiiiand they co-occur twenty times in a million-word corpus.
This would be more surprisingfor two completely random distributions thanif they had each occurred twice and had alwaysco-occurred.
A simple probability does not cap-ture this fact.The rationale for using two different statisticsfor this task is that each is well suited for its par-ticular role, and not particularly well suited tothe other.
We have already mentioned that thesimple ratio is ill suited to dealing with infre-quent occurrences.
It is thus a poor candidatefor ranking the final output,  if that list includeswords of as few as one occurrence in the corpus.The log likelihood statistic, we found, is poorlysuited to selecting new seed words in an iterativealgorithm of this sort, because it promotes highfrequency nouns, which can then overly influ-ence selections in future iterations, if they areselected as seed words.
We termed this phe-nomenon infection, and found that it can be sostrong as to kill the further progress of a cate-gory.
For example, if we are processing the cat-egory vehicle and the word artillery is selectedas a seed word, a whole set of weapons that co-occur with artillery can now be selected in fu-ture iterations.
If one of those weapons occursfrequently enough, the scores for the words thatit co-occurs with may exceed those of any vehi-cles, and this effect may be strong enough thatno vehicles are selected in any future iteration.In addition, because it promotes high frequencyterms, such a statistic tends to have the sameeffect as a minimum occurrence cutoff, i.e.
fewif any low frequency words get added.
A simpleprobability is a much more conservative statis-tic, insofar as it selects far fewer words withthe potential for infection, it limits the extentof any infection that does occur, and it includesrare words.
Our motto in using this statistic forselection is, "First do no harm.
"4 Seed word selectionThe simple ratio used to select new seed wordswill tend not to select higher frequency wordsin the category.
The solution to this problemis to make the initial seed word selection fromamong the most frequent head nouns in the cor-pus.
This is a sensible approach in any case,since it provides the broadest coverage of cat-egory occurrences, from which to select addi-tional likely category members.
In a task thatcan suffer from sparse data, this is quite impor-tant.
We printed a list of the most commonnouns in the corpus (the top 200 to 500), andselected category members by scanning throughthis list.
Another option would be to use headnouns identified in Wordnet, which, as a set,should include the most common members ofthe category in question.
In general, however,the strength of an algorithm of this sort is inidentifying infrequent or specialized terms.
Ta-ble 1 shows the seed words that were used forsome of the categories tested.5 Compound NounsThe relationship between the nouns in a com-pound noun is very different from that in theother constructions we are considering.
Thenon-head nouns in a compound noun may ormay not be legitimate members of the category.For instance, either pickup truck or pickup isa legitimate vehicle, whereas cargo plane is le-gitimate, but cargo is not.
For this reason,co-occurrence within noun compounds is notconsidered in the iterative portions of our al-gorithm.
Instead, all noun compounds with ahead that is included in our final ranked list,are evaluated for inclusion in a second list.The method for evaluating whether or not toinclude a noun compound in the second list isintended to exclude constructions uch as gov-ernment plane and include constructions uchas fighter plane.
Simply put, the former doesnot correspond to a type of vehicle in the sameway that the latter does.
We made the simplify-ing assumption that the higher the probabilityof the head given the non-head noun, the betterthe construction for our purposes.
For instance,if the noun government is found in a noun com-pound, how likely is the head of that compoundto be plane?
How does this compare to the nounfighter?For this purpose, we take two counts for eachnoun in the compound:1.
The number of times the noun occurs in anoun compound with each of the nouns toits right in the compound2.
The number of times the noun occurs in anoun compoundFor each non-head noun in the compound, we1112Crimes (MUC): murder(s), crime(s), killing(s), trafficking, kidnapping(s)Crimes (WSJ): murder(s), crime(s), theft(s), fraud(s), embezzlementVehicle: plane(s), helicopter(s), car(s), bus(es), aircraft(s), airplane(s), vehicle(s)Weapon: bomb(s), weapon(s), rifle(s), missile(s), grenade(s), machinegun(s), dynamiteMachines: computer(s), machine(s), equipment, chip(s), machineryTable 1: Seed Words Usedevaluate whether or not to omit it in the output.If all of them are omitted, or if the resultingcompound has already been output, the entryis skipped.
Each noun is evaluated as follows:First, the head of that noun is determined.To get a sense of what is meant here, considerthe following compound: nuclear-powered air-craft carrier.
In evaluating the word nuclear-powered, it is unclear if this word is attachedto aircraft or to carrier.
While we know thatthe head of the entire compound is carrier, inorder to properly evaluate the word in question,we must determine which of the words follow-ing it is its head.
This is done, in the spirit ofthe Dependency Model of Lauer (1995), by se-lecting the noun to its right in the compoundwith the highest probability of occuring withthe word in question when occurring in a nouncompound.
(In the case that two nouns have thesame probability, the rightmost noun is chosen.
)Once the head of the word is determined, the ra-tio of count 1 (with the head noun chosen) tocount 2 is compared to an empirically set cut-off.
If it falls below that cutoff, it is omitted.
Ifit does not fall below the cutoff, then it is kept(provided its head noun is not later omitted).6 Out l ine  o f  the  a lgor i thmThe input to the algorithm is a parsed corpusand a set of initial seed words for the desiredcategory.
Nouns are matched with their pluralsin the corpus, and a single representation is set-tled upon for both, e.g.
car(s).
Co-Occurrencebigrams are collected for head nouns accordingto the notion of co-occurrence outlined above.The algorithm then proceeds as follows:1.
Each noun is scored with the selectingstatistic discussed above.2.
The highest score of all non-seed words isdetermined, and all nouns with that scoreare added to the seed word list.
Then re-turn to step one and repeat.
This iterationcontinues many times, in our case fifty.3.
After the number of iterations in (2) arecompleted, any nouns that were not se-lected as seed words are discarded.
Theseed word set is then returned to its origi-nal members.4.
Each remaining noun is given a score basedupon the log likelihood statistic discussedabove.5.
The highest score of all non-seed words isdetermined, and all nouns with that scoreare added to the seed word list.
We then re-turn to step (5) and repeat he same num-ber of times as the iteration in step (2).6.
Two lists are output, one with head nouns,ranked by when they were added to theseed word list in step (6), the other consist-ing of noun compounds meeting the out-lined criterion, ordered by when their headswere added to the list.7 Empi r i ca l  Resu l ts  and  D iscuss ionWe ran our algorithm against both the MUC-4corpus and the Wall Street Journal (WSJ) cor-pus for a variety of categories, beginning withthe categories of vehicle and weapon, both in-cluded in the five categories that R~S inves-tigated in their paper.
Other categories thatwe investigated were crimes, people, comm.ercialsites, states (as in static states of affairs), andmachines.
This last category was run becauseof the sparse data for the category weapon in theWall Street Journal.
It represents roughly thesame kind of category as weapon, namely tech-nological artifacts.
It, in turn, produced sparseresults with the MUC-4 corpus.
Tables 3 and4 show the top results on both the head nounand the compound noun lists generated for thecategories we tested.R~S evaluated terms for the degree to whichthey are related to the category.
In contrast, wecounted valid only those entries that are clearmembers of the category.
Related words (e.g.1113crash for the category vehicle) did not count.A valid instance was: (1) novel (i.e.
not in theoriginal seed set); (2) unique (i.e.
not a spellingvariation or pluralization of a previously en-countered entry); and (3) a proper class withinthe category (i.e.
not an individual instance ora class based upon an incidental feature).
As anillustration of this last condition, neither GalileoProbe nor gray plane is a valid entry, the formerbecause it denotes an individual and the latterbecause it is a class of planes based upon anincidental feature (color).In the interests of generating as many validentries as possible, we allowed for the inclusionin noun compounds of words tagged as adjec-tives or cardinality words.
In certain occasions(e.g.
four-wheel drive truck or nuclear bomb)this is necessary to avoid losing key parts ofthe compound.
Most common adjectives aredropped in our compound noun analysis, sincethey occur with a wide variety of heads.We determined three ways to evaluate theoutput of the algorithm for usefulness.
The firstis the ratio of valid entries to total entries pro-duced.
R&S reported a ratio of .17 valid tototal entries for both the vehicle and weaponcategories (see table 2).
Oil the same corpus,our algorithm yielded a ratio of .329 valid to to-tal entries for the category vehicle, and .36 forthe category weapon.
This can be seen in theslope of the graphs in figure 1.
Tables 2 and5 give the relevant data for the categories thatwe investigated.
In general, the ratio of valid tototal entries fell between .2 and .4, even in thecases that the output was relatively small.A second way to evaluate the algorithm is bythe total number of valid entries produced.
Ascan be seen from the numbers reported in table2, our algorithm generated from 2.4 to nearly 3times as many valid terms for the two contrast-ing categories from the MUC corpus than thealgorithm of R?:S.
Even more valid terms weregenerated for appropriate categories using theWall Street Journal.Another way to evaluate the algorithm is withthe number of valid entries produced that arenot in Wordnet.
Table 2 presents these numbersfor the categories vehicle and weapon.
Whereasthe R&S algorithm produced just 11 terms notalready present in Wordnet for the two cate-gories combined, our algorithm produced 106,R & C (MUC)R & C (wsJ) ,R & S (MUC) 1120100 Vehicle f, , t  .
.
.
.604o200 r50 100 150 200 250Terms Generated100Weapon8O6O402O0 ~ I J I I50 100 i 50 200Terms GeneratedI250F igure  1: Results for the Categories Vehicle andWeaponor over 3 for every 5 valid terms produced.
It isfor this reason that we are billing our algorithmas something that could enhance xisting broad-coverage resources with domain-specific lexicalinformation.8 Conc lus ionWe have outlined an algorithm in this paperthat, as it stands, could significantly speed up1114MUC=4 corpus WSJ  corpusCategory Algorithm Total Valid Valid Total Valid ValidTerms Terms Terms not Terms Terms Terms notGenerated Generated in Wordnet Generated Generated in WordnetVeh ic le  1% & C 249 82 52 339 123 81Veh ic le  R & S 200 34 4 NA NA NAWeapon R & C 257 93 54 150 17Weapon R&S 200 34 NA NATable 2: Valid category terms found that are not in Wordnet12NACrimes (a): terrorism, extortion, robbery(es), assassination(s), arrest(s), disappearance(s), violation(s), as-sault(s), battery(es), tortures, raid(s), seizure(s), search(es), persecution(s), iege(s), curfew, capture(s), subver-sion, good(s), humiliation, evictions, addiction, demonstration(s), outrage(s), parade(s)Crimes (b): action-the murder(s), Justines crime(s), drug trafficking, body search(es), dictator Noriega, gunrunning, witness account(s)Sites (a): office(s), enterprise(s), company(es), dealership(s), drugstore(s), pharmacies, upermarket(s), termi-nal(s), aqueduct(s), shoeshops, marinas, theater(s), exchange(s), residence(s), business(es), employment, farm-land, range(s), industry(es), commerce, tc., transportation-have, market(s), sea, factory(es)Sites (b): grocery store(s), hardware store(s), appliance store(s), book store(s), shoe store(s), liquor store(s), A1-batros store(s), mortgage bank(s), savings bank(s), creditor bank(s), Deutsch-Suedamerikanische bank(s), reservebank(s), Democracia building(s), apartment building(s), hospital-the building(s)Vehicle (a): gunship(s), truck(s), taxi(s), artillery, Hughes-500, tires, jitneys, tens, Huey-500, combat(s), am-bulance(s), motorcycle(s), Vides, wagon(s), Huancora, individual(s), KFIR, M-bS, T-33, Mirage(s), carrier(s),passenger(s), luggage, firemen, tank(s)Vehicle (b): A-37 plane(s), A-37 Dragonfly plane(s), passenger plane(s), Cessna plane(s), twin-engined Cessnaplane(s), C-47 plane(s), grayplane(s), KFIR plane(s), Avianca-HK1803 plane(s), LATN plane(s), Aeronicaplane(s), 0-2 plane(s), push-and-pull 0-2 plane(s), push-and-pull p ane(s), fighter-bomber plane(s)Weapon (a)-" launcher(s), submachinegun(s), mortar(s), explosive(s), cartridge(s), pistol(s), ammunition(s), car-bine(s), radio(s), amount(s), shotguns, revolver(s), gun(s), materiel, round(s), stick(s) clips, caliber(s), rocket(s),quantity(es), type(s), AK-47, backpacks, plugs, light(s)Weapon (b): car bomb(s), night-two bomb(s), nuclear bomb(s), homemade bomb(s), incendiary bomb(s), atomicbomb(s), medium-sized bomb(s), highpower bomb(s), cluster bomb(s), WASP cluster bomb(s), truck bomb(s),WASP bomb(s), high-powered bomb(s), 20-kg bomb(s), medium-intensity bomb(s)Table 3: Top results from (a) the head noun listthe task of building a semantic lexicon.
Wehave also examined in detail the reasons whyit works, and have shown it to work well formultiple corpora and multiple categories.
Thealgorithm generates many words not included inbroad coverage resources, uch as Wordnet, andcould be thought of as a Wordnet "enhancer"for domain-specific applications.More generally, the relative success of the al-gorithm demonstrates the potential benefit ofnarrowing corpus input to specific kinds of con-structions, despite the danger of compoundingsparse data problems.
To this end, parsing isinvaluable.and (b) the compound noun list using MUC-4 corpus9 AcknowledgementsThanks to Mark Johnson for insightful discus-sion and to Julie Sedivy for helpful comments.Re ferencesE.
Charniak, S. Goldwater, and M. Johnson.1998.
Edge-based best-first chart parsing.forthcoming.T.
Dunning.
1993.
Accurate methods for thestatistics of surprise and coincidence.
Com-putational Linguistics, 19(1):61-74.W.A.
Gale, K.W.
Church, and D. Yarowsky.1992.
A method for disambiguating word1115Crimes (a): conspiracy(es), perjury, abuse(s), influence-peddling, sleaze, waste(s), forgery(es), inefficiency(es),racketeering, obstruction, bribery, sabotage, mail, planner(s), bttrglary(es), robbery(es), auto(s), purse-snatchings,premise(s), fake, sin(s), extortion, homicide(s), kilting(s), statute(s)Crimes (b): bribery conspiracy(es), substance abuse(s), dual-trading abuse(s), monitoring abuse(s), dessert-menu planner(s), gun robbery(es), chance accident(s), carbon dioxide, sulfur dioxide, boiler-room scare(s), identityscam(s), 19th-century drama(s), fee seizure(s)Machines (a): workstation(s), tool(s), robot(s), installation(s), dish(es), lathes, grinders, subscription(s), trac-tor(s), recorder(s), gadget(s), bakeware, RISC, printer(s), fertilizer(s), computing, pesticide(s), feed, set(s), am-plifier(s), receiver(s), substance(s), tape(s), DAT, circumstancesMachines (b): hand-held computer(s), Apple computer(s), upstart Apple computer(s), Apple Macintosh com-puter(s), mainframe computer(s), Adam computer(s), Gray computer(s), desktop computer(s), portable com-puter(s), laptop computer(s), MIPS computer(s), notebook computer(s), mainframe-class computer(s), Compaqcomputer(s), accessible computer(s)Sites (a): apartment(s), condominium(s), tract(s), drugstore(s), setting(s), supermarket(s), outlet(s), cinema,club(s), sport(s), lobby(es), lounge(s), boutique(s), stand(s), landmark, bodegas, thoroughfare, bowling, steak(s),arcades, food-production, pizzerias, frontier, foreground, martSites (b): department store(s), flagship store(s), warehouse-type store(s), chain store(s), five-and-dime store(s),shoe store(s), furniture store(s), sporting-goods store(s), gift shop(s), barber shop(s), film-processing shop(s), shoeshop(s), butcher shop(s), one-person shop(s), wig shop(s)Vehicle (a): truck(s), van(s), minivans, launch(es), nightclub(s), troop(s), october, tank(s), missile(s), ship(s),fantasy(es), artillery, fondness, convertible(s), Escort(s), VII, Cherokee, Continental(s), Taurus, jeep(s), Wag-oneer, crew(s), pickup(s), Corsica, BerettaVehicle (b): gun-carrying plane(s), commuter plane(s), fighter plane(s), DC-10 series-10 plane(s), high-speedplane(s), fuel-efficient plane(s), UH-60A Blackhawk helicopter(s), passenger car(s), Mercedes car(s), American-made car(s), battery-powered car(s), battery-powered racing car(s), medium-sized car(s), side car(s), excitingcar(s)Table 4: Top results from (a) the head noun list and (b) the compound noun list using WSJ corpusMUC-4 corpus WSJ  corpusCategory Total Valid Total ValidiTerms Terms Terms TermsCrimes' 115 24 90 24Machines 0 0 335 117People 338 85 243 103Sites 155 33 140 33States 90 35 96 17Table 5: Valid category terms found by our algorithmfor other categories testedsenses in a large corpus.
Computers and theHumanities, 26:415-439.M.
Lauer.
1995.
Corpus statistics meet thenoun compound: Some empirical results.
InProceedings of the 33rd Annual Meeting ofthe Association for Computational Linguis-tics, pages 47-55.M.P.
Marcus, B. Santorini, and M.A.Marcinkiewicz.
1993.
Building a largeannotated corpus of English: The PennTreebank.
Computational Linguistics,19(2):313-330.G.
Miller.
1990.
Wordnet: An on-line lexicaldatabase.
International Journal of Lexicog-raphy, 3(4).MUC-4 Proceedings.
1992.
Proceedings of theFourth Message Understanding Conference.Morgan Kaufmann, San Mateo, CA.E.
Riloff and J. Shepherd.
1997.
A corpus-based approach for building semantic lexi-cons.
In Proceedings of the Second Confer-ence on Empirical Methods in Natural Lan-guage Processing, pages 127-132.H.
Schiitze.
1992.
Word sense disambiguationwith sublexical representation.
In WorkshopNotes, Statistically-Based NLP Techniques,pages 109-113.
AAAI.D.
Yarowsky.
1995.
Unsupervised word sensedisambiguation rivaling supervised methods.In Proceedings of the 33rd Annual Meeting ofthe Association for Computational Linguis-tics, pages 189-196.1116
