Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1787?1796,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsRelational Inference for WikificationXiao Cheng Dan RothDepartment of Computer ScienceUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801{cheng88,danr}@illinois.eduAbstractWikification, commonly referred to as Disam-biguation to Wikipedia (D2W), is the task ofidentifying concepts and entities in text anddisambiguating them into the most specificcorresponding Wikipedia pages.
Previous ap-proaches to D2W focused on the use of lo-cal and global statistics over the given text,Wikipedia articles and its link structures, toevaluate context compatibility among a list ofprobable candidates.
However, these meth-ods fail (often, embarrassingly), when somelevel of text understanding is needed to sup-port Wikification.
In this paper we introducea novel approach to Wikification by incorpo-rating, along with statistical methods, richerrelational analysis of the text.
We provide anextensible, efficient and modular Integer Lin-ear Programming (ILP) formulation of Wik-ification that incorporates the entity-relationinference problem, and show that the abilityto identify relations in text helps both candi-date generation and ranking Wikipedia titlesconsiderably.
Our results show significant im-provements in both Wikification and the TACEntity Linking task.1 IntroductionWikification (D2W), the task of identifying conceptsand entities in text and disambiguating them intotheir corresponding Wikipedia page, is an importantstep toward supporting deeper textual understand-ing, by augmenting the ability to ground text in ex-isting knowledge and facilitating knowledge expan-sion.D2W has been studied extensively recently(Cucerzan, 2007; Mihalcea and Csomai, 2007;Milne and Witten, 2008; Ferragina and Scaiella,2010; Ratinov et al 2011) and has already foundbroad applications in NLP, Information Extraction,and Knowledge Acquisition from text, from coref-erence resolution (Ratinov and Roth, 2012) to entitylinking and knowledge population (Ellis et al 2011;Ji et al 2010; Cucerzan, 2011).Given a document D containing a set of conceptand entity mentionsM ( referred to later as surface),the goal of Wikification is to find the most accuratemapping from mentions to Wikipedia titles T ; thismapping needs to take into account our understand-ing of the text as well as background knowledge thatis often needed to determine the most appropriate ti-tle.
We also allow a special NIL title that capturesall mentions that are outside Wikipedia.Earlier approaches treated this task as a word-sense disambiguation (WSD) problem, which waslater enhanced with a certain level of global rea-soning, but essentially all approaches focused ongeneric statistical features in order to achieve robustdisambiguation.
It was shown that by disambiguat-ing to the most likely title for every surface, in-dependently maximizing the conditional probabilityPr(title|surface), we already achieve a very com-petitive baseline on several Wikification datasets(Ratinov et al 2011).
This strong statistical baselinemakes use of the relatively comprehensive coverageof the existing Wikipedia links from surface stringsto Wikipedia titles.
Although more involved statis-tical features are required in order to make substan-tial improvements, global features such as contextTF-IDF, better string similarity, etc., statistics-basedWikification systems give a fairly coherent set ofdisambiguation when sufficient context is available.Consider the following example: Earth?s biosphere1787then significantly altered the atmospheric and other ba-sic physical conditions, which enabled the proliferationof organisms.
The atmosphere is composed of 78.09%nitrogen, 20.95% oxygen, 0.93% argon, 0.039% carbondioxide, and small amounts of...The baseline system we adopted (Ratinov et al2011), one of the best Wikification systems, al-ready disambiguates atmosphere correctly to the ti-tle Earth?s atmosphere instead of the more generaltitle Atmosphere, making use of the concept Earth inits local context to resolve the mention to the morespecific title that better coheres with the topic.
How-ever, consider the following example:Ex.
1 ?As Mubarak, the wife of deposed EgyptianPresident Hosni Mubarak got older, her influence...?The bold faced name should be mapped to SuzanneMubarak, but all existing Wikification systems mapboth names in this sentence to the dominant page(the most linked page) of Hosni Mubarak, failing tounderstand the relation between them, which shouldprevent them from being mapped to the same page.A certain level of text understanding is required evento be able to generate a good list of title candidates.For example, in:Ex.
2 ?...ousted long time Yugoslav President Slo-bodan Milos?evic?
in October.
Mr.
Milos?evic?
?s So-cialist Party...?the bold-faced concept should be mapped to thepage of the Socialist Party of Serbia, which is fardown the list of titles that could be related to ?So-cialist Party?
; making this title a likely candidaterequires understanding the possessive relation withMilos?evic?
and then making the knowledge-informeddecision that he is more related to Socialist Party ofSerbia than any other possible titles.
Finally, inEx.
3 ?James Senn, director of Robinson College?sCenter for Global Business Leadership at GeorgiaState University...?we must link Robinson College to J. Mack Robin-son College of Business which is located at Geor-gia State University instead of Robinson College,Cambridge, which is the only probable title linkedby the surface Robinson College in the version ofthe Wikipedia dump we used.These examples further illustrate that, along withunderstanding the relation expressed in the text, weneed to access background knowledge sources andto deal with variability in surface representationacross the text, Wikipedia, and knowledge, in orderto reliably address the Wikification problem.In this paper we focus on understanding those nat-ural language constructs that will allow eliminat-ing these ?obvious?
(to a human reader) mistakesfrom Wikification.
In particular, we focus on resolv-ing coreference and a collection of local syntactico-semantic relations (Chan and Roth, 2011); better un-derstanding the relational structure of the text allowsus to generate title candidates more accurately giventhe text, rank these candidates better and determinewhen a mention in text has no corresponding titlein Wikipedia and should be mapped to NIL, a keyproblem in Wikification.
Moreover, it allows us toaccess external knowledge based resources more ef-fectively in order to support these decisions.We incorporate the outcome of our relationalanalysis, along with the associated features extractedfrom external sources and the ?standard?
wikifica-tion statistical features, into an ILP-based inferenceframework that globally determines the best assign-ment of mentions to titles in a given document.
Weshow that by leveraging a better understanding ofthe textual relations, we can substantially improvethe Wikification performance.
Our system signifi-cantly outperforms all the top Wikification systemson the widely adopted standard datasets and showsstate-of-the-art results when evaluated (without be-ing trained directly) on the TAC 2011 Entity Linkingtask.2 The Wikification ApproachA general Wikification decision consists of threecomputational components: (1) generating a rankedlist of title candidates for each mention, (2) rank-ing candidates globally, and (3) dealing with NILmentions.
For (1), the ?standard?
way of usingPr(title|surface) is often not sufficient; considerthe case where the mention is the single word ?Presi-dent?
; disambiguating such mentions depends heav-ily on the context, i.e.
to determine the relevantcountry or organization.
However, it is intractable tosearch the entire surface-to-title space, and using anarbitrary top-K list will inevitably leave out a largenumber of potential solutions.
For (2), even though1788the anchor texts cover many possible ways of para-phrasing the Wikipedia article titles and thus usingthe top Pr(title|surface) is proven to be a fairlystrong baseline, it is never comprehensive.
There isa need to disambiguate titles that were never linkedby any anchor text, and to disambiguate mentionsthat have never been observed as the linked text.
For(3) the Wikifier needs to determine when a mentioncorresponds to no title, and map it to a NIL entity.Simply training a classifier using coherency featuresor topical models turns out to be insufficient, since ithas a predetermined granularity at which it can dis-tinguish entities.Next we provide a high-level description (Alg.
1)of our approach to improve Wikification by leverag-ing textual relations in these three stages.Algorithm 1 Relational Inference for WikificationNote: ?
: M ?
T is the sought after mapping fromall mentions in the document to all candidate titlesin Wikipedia.Require: Document D, Knowledge Base K con-sisting of relation triples ?
= (ta, p, tb), wherep is the relation predicate.1: Generate initial mentions M = {mi} from D.2: Generate candidates ti = {tki } for mention miand initialize candidate priors Pr(tki |mi) withexisting Wikification system, for all mi ?M .3: Instantiate non-coreference relational con-straints and add relational candidates.4: Instantiate coreference relational constraintsand add relational candidates.5: Construct an ILP objective function and solvefor the arg max?
Pr(?
).6: return ?.Most of our discussion addresses the relationalanalysis and its impact on stage (2) and (3) above.We will only briefly discuss improvements to thestandard candidate generation stage in Sec.
4.43 Problem FormulationWe now describe how we formulate our global deci-sion problem as an Integer Linear Program (ILP).We use two types of boolean variables: eki isused to denote whether we disambiguate mi to tki(?
(mi) = tki ) or not.
r(k,l)ij is used to denote iftitles tki and tlj are chosen simultaneously, that is,r(k,l)ij = eki ?
elj .Our models determine two types of score forthe boolean variables above: ski = Pr(eki ) =Pr(?
(mi) = tki ), represents the initial score for thekth candidate title being chosen for mentionmi.
Fora pair of titles (tki , tlj), we denote the confidence offinding a relation between them by w(k,l)ij .
Its valuedepends on the textual relation type and on how co-herent it is with our existing knowledge.Our goal is to find the best assignment to vari-ables eki , such that it satisfies some legitimacy (hard)constraints and the soft constraints dictated by therelational constraints (via scores w(k,l)ij ).
To accom-plish that we define our objective function as a Con-strained Conditional Model (CCM) (Roth and Yih,2004; Chang et al 2012) that is used to reward orpenalize a pair of candidates tki , tlj by w(k,l)ij whenthey are chosen in the same document.
Specifically,we choose the assignment ?D that optimizes:?D = arg max?
?i?kski eki +?i,j?k,lw(k,l)ij r(k,l)ijs.t.
r(k,l)ij ?
{0, 1} Integral constraintseki ?
{0, 1} Integral constraints?i?k eki = 1 Unique solution2r(k,l)ij ?
eki + elj Relation definitionNote that as in most NLP problems, the prob-lem is very sparse, resulting in a tractable ILPthat is solved quickly by off-the-shelf ILP packages(Gurobi Optimization, 2013).
In our case the keyreason for the sparseness is that w(k,l)ij = 0 for mostpairs considered, which does not require explicit in-stantiation of r(k,l)ij .4 Relational AnalysisThe key challenge in incorporating relational anal-ysis into the Wikification decision is to systemati-cally construct the relational constraints (the solidedges between candidates in Figure 1) and incorpo-rate them into our inference framework.
Two maincomponents are needed: first, we need to extracthigh precision textual relations from the text; then,1789Slobodan Milo?evi?......Savo Milo?evi?Slobodan Milo?evi?
Socialist Party (France)Socialist Party...Socialist Party of SerbiaYugoslavia President...President of the Federal Republic of Yugoslavia...ousted long time [ Yugoslav President ] [Slobodan Milo?evi?]
in October.
Mr.
[Milo?evi?
]'s [Socialist Party] ...CoreferenceAppositionsearch in lexical/relational spacesearch in lexical and P(title|surface) spacePossessivem1m2m3m4t1t2...founder_ofholds_office=Figure 1: Textual relation inference framework: The goal is to maximize the objective function assigning mentionsto titles while enforcing coherency with relations extracted from both text and an external knowledge base.
Here,searching the external KB reveals that Slobodan Milos?evic?
is the founder of the Socialist Party of Serbia, which can bereferred to by the surface Socialist Party; we therefore reward the output containing this pair of candidates.
The sameidea applies for the relation ?Slobodan Milos?evic?
holds office as President of the Federal Republic of Yugoslavia?
aswell as to the coreference relation between two mentions of Slobodan Milos?evic?.we need to assign weights to these semantic rela-tions.
We determine the weights by combining typeand confidence of the relation extracted from textwith the confidence in relations retrieved from an ex-ternal Knowledge Base (KB) by using the mentionpairs as a query.
It is noteworthy that although con-text window based coherency objective functionscapture many proximity relations, using these unfil-tered relations as constraints in our experiments in-troduced excessive amount of false-positives for theintrinsically sparse textual relations and resulted insevere performance hit.In Sec.
4.1 we describe how we extract relationsfrom text; our goal is to reliably identify argumentsthat we hypothesize to be in a relation; we showthat this is essential both to our candidate genera-tion, our ranking and the mapping to NIL.
Sec.
4.2describes how we use an external KB to verify thatthese arguments are indeed in a relation.
Finally,Sec.
4.3 shows how we generate scores for the men-tions and relations, as coefficients in the objectivefunction of Sec.
3.
The process is illustrated in Fig-ure 1.
Overall, our approach is an ambiguity-awareapproach that identifies, filters and scores the rele-vant relations; this is essential due to the ambiguity,variability and noise inherent in directly matchingsurface forms to titles.4.1 Relation ExtractionEven though relation extraction is an open prob-lem, analysis on the ACE2004 Relation Detectionand Characterization (RDC) dataset shows that ap-proximately 80% of the relations are expressedthrough syntactico-semantic structures (Chan andRoth, 2011) that are easy to extract with high pre-cision.
Unlike the general ACE RDC task, we canrestrict relation arguments to be named entities andthus leverage the large number of known relations inexisting databases (e.g.
Wikipedia infoboxes).
Wealso consider conference relations that potentiallyaid mapping different mentions to the same title.4.1.1 Syntactico-semantic RelationsWe introduce our approach using the followingexample.
Consider a news article discussing Israelipolitics while briefly mentioning:Ex.
4 An official at the [Iranian]1 [Ministry ofDefense]2 told Tehran Radio that...A purely statistical approach would very likely mapthe entity [Ministry of Defense]2 to Ministry of De-fense (Israel) instead of Ministry of Defense andArmed Forces Logistics (Iran) because the context ismore coherent with concepts related to Israel ratherthan to Iran.
Nevertheless, the pre-modifier relationbetween [Iranian]1 and [Ministry of Defense]2 de-mands the answer to be tightly related to Iran.
Eventhough human readers may not know the correct ti-tle needed here, understanding the pre-modifier re-lation allows them to easily filter through a list ofcandidates and enforce constraints that are derivedjointly from the relation expressed in the text andtheir background knowledge.In our attempt to mimic this general approach, weemploy several high precision classifiers to resolve1790a range of local relations that are used to retrieverelevant background knowledge, and consequentlyintegrated into our inference framework.
Our in-put for relation extraction is any segment matchedby the regular expression to be mentioned in sec-tion 4.4 in the candidate generation stage; we ana-lyze its constituents by decomposing it into the twolargest sub-entities that have (in Wikipedia) corre-sponding candidates.
In the above example, Ira-nian Ministry of Defense would be decomposed intoIranian and Ministry of Defense and our relationextraction process hypothesizes a relation betweenthese arguments.Note that we do not use any full parsing since itdoes not address our needs directly nor does it scalewell with the typical amount of data used in Wikifi-cation.4.1.2 Coreference RelationsIn addition to syntactico-semantic relations, wecould also encounter other textual relations.
The fol-lowing example illustrates the importance of under-standing co-reference relations in Wikification:Ex.
5 [Al Goldman]1, chief market strategist atA.G.
Edwards, said ... [Goldman]2 told us that...There is no Wikipedia entry (or redirection) thatmatches the name Al Goldman.
Clearly [Goldman]2refers to the same person and should be mapped tothe same entity (or to NIL) rather than popular en-tities frequently referred to as Goldman, coherentwith context or not, such as Goldman Sachs.
To ac-complish that, we cluster named entities that sharetokens or are acronyms of each other when thereis no ambiguity (e.g.
no other longer named en-tity mentions containing Goldman in the document)and use a voting algorithm (Algorithm 2) to generatecandidates locally from within the clusters.
We alsoexperimented with using full-fledged coference sys-tems, but found it to be time consuming while pro-viding no significant end-to-end performance differ-ence.4.1.3 Coreferent Nominal MentionsDocument level coreference also provides impor-tant relations between named entities and nominalmentions.
Extracting these relations proved to bevery useful for classifying NIL entities, as unfamil-iar concepts tend to be introduced with these suc-cinct appositional nominal mentions.
These descrip-tions provide a clean ?definition?
of the entity, al-lowing us to abstract the inference to a limited ?nounphrase entailment problem?.
That is, it allows us todetermine whether the target mention corresponds toa candidate title.
Consider, for example, wikifyingDorothy Byrne in: Dorothy Byrne, a state coordi-nator for the Florida Green Party, .
.
.Identifying the apposition relation allows us to de-termine that this Dorothy Byrne is not the baselineWikipedia title.
We use the TF-IDF cosine similar-ity between the nominal description and the lexicalcontext (Ratinov et al 2011) of the candidate page,head word attributes and entity relation (i.e.
betweenDorothy Byrne and Florida Green Party) to deter-mine whether any candidates of Dorothy Byrne canentail the nominal mention.4.2 Relational QueriesStatistics based candidate generation algorithms al-ways generate the same list of candidates given thesame surface string; even though this approach hasa competitive coverage rate, it will not work wellin some ?obvious?
(to human) cases; for example,it offers very little information on highly ambigu-ous surface strings such as ?President?
for which itis even intractable to rank all the candidates.
Top-K lists which were used in previous literature suf-fer from the same problem.
Instead, we make useof relational queries to generate a more likely set ofcandidates.Once mention pairs are generated from text us-ing the syntactico-semantic structures and corefer-ence, we use these to query our KB of relationaltriples.
We first indexed all Wikipedia links and DB-pedia relations as unordered triples ?
= (ti, p, tj),where the arguments ti, tj are tokenized, stemmedand lowercased for best recall.
p is either a relationpredicate from the DBpedia ontology or the predi-cate LINK indicating a hyperlink relation.
Sinceour baseline system has approximately 80% accu-racy at this stage, it is reasonable to assume that atleast one of the argument mentions is correctly dis-ambiguated.
Therefore we prune the search spaceby making only two queries for each mention pair(mi,mj): q0 = (t?i ,mj) and q1 = (mi, t?j ) wheret?i , t?j are the strings representing the top titles cho-sen by the current model for mentions mi,mj re-1791spectively.We also aggressively prune the search results ina way similar to the process in Sec.
4.4, only keep-ing the arguments that are known to be possible orvery likely candidates of the mention, based on theambiguity that exists in the query result.4.3 Relation ScoringFor the final assignment made using our objectivefunction (Sec.
3) we need to normalize and rescalethe output of individual components of our system asthey come from different scoring functions.
We con-sider adding new title candidates from two sources,through the coreference module and through thecombined DBpedia and Wikipedia inter-page linkstructures.
Next we describe how to compute andcombine these scores.4.3.1 Scoring Knowledge Base RelationsOur model uses both explicit relations p 6=LINK from DBpedia and Wikipedia hyperlinksp = LINK (implicit relation).
We want to favorrelations with explicit predicate, each weighted as ?implicit relation (we use ?
= 5 in our experiments,noting the results are insensitive to slight changes ofthis parameter).For each query, we denote the score returned byour KB search engine1 given query q and triple ?as Sim?,q.
The relational weight wk,li,j between twocandidates (see Sec.
3) is determined as:wk,li,j =1Z???
?Sim?,qwhere the sum is over the top 20 KB triples, ??
isthe relation type scaling constant (?
or 1), and Z isa normalization factor that normalizes all wk,li,j to therange [0, 1].Note that we do not check the type of the relationagainst the textual relation.
The key reason is thatexplicit relations are not as robust, especially con-sidering that we restrict one of the arguments in therelation and constraining the other argument?s lexi-cal form.
Moreover, we back off to restricting the re-lations to be between known candidates when mul-tiple lexically matched arguments are retrieved withhigh ambiguity.
Additionally, most of our relations1http://lucene.apache.org/Algorithm 2 Coreferent Candidates VotingRequire: Coreference cluster C1: Vote collector vt denotes the score for a candi-date t, which by default is 0.2: ti = {t1i .
.
.
tni } is the set of candidates of men-tion mi.3: li is the token count of mi4: for all mi ?
C, li ?
2 do5: for all tki ?
ti do6: vtki = vtki + ski7: end for8: end for9: Let AllSingle denote whether ?i, li = 110: for all mi ?
C where li = 1 do11: for all tki ?
ti do12: if AllSingle or vtki > 0 then13: vtki = vtki + ski14: end if15: end for16: end for17: return vdo not have explicit predicates in the text anyhow,and extracting a type would add noise to our deci-sion.4.3.2 Scoring Coreference RelationsFor coreference relations, we simply use hardconstraints by assigning candidates in the samecoreference cluster a high relational weight, whichis a cheap approximation to penalizing the outputwhere the coreferent mentions disambiguate to dif-ferent titles.
In practice, using a weight of 10 is suf-ficient.
Another important issue here is that the cor-rect coreferent candidate might not exist in the can-didate list of the shorter mentions in the cluster.
Forexample, if a mention has the surface Richard, thenumber of potential candidates is so large that anytop K list of titles will not be informative.
We there-fore ignore candidates generated from short surfacestrings and give it the same candidate list as the headmentions in its cluster.
Figure 2 shows the voting al-gorithm we use to elect the potential candidates forthe cluster.The reason for separating the votes of longer andshorter mentions is that shorter mentions are inher-ently more ambiguous.
Once a coreferent relation1792is determined, longer mentions in the cluster shoulddictate what this cluster should collectively refer to.4.4 Candidate GenerationBeyond the algorithmic improvements, the mentionand candidate generation stage is aided by a fewsystematic preprocessing improvement briefly de-scribed below.4.4.1 Mention SegmentationSince named entities may sometimes overlap witheach other, we use regular expressions to matchlonger surface forms that are often incorrectly seg-mented or ignored by NER 2 due to different an-notation standards.
For example, this will capture:Prime Minister of the United Kingdom.
The regu-lar expression pattern we used for Step 1 in Algo-rithm 1 simply adds mentions formed by any twoconsecutive capitalized word chunks connected byup to 2 punctuation marks, prepositions, and the to-kens ?the?, ??s?
& ?and?.
These segments are alsoused as arguments for relation extraction.4.4.2 Lexical SearchWe link certain mentions directly to their exactmatching titles in Step 3 when there is very low am-biguity.
Specifically, when no title is known for amention that is relatively long and fuzzily matchesthe lexically retrieved title, we perform this aggres-sive linking.
The lexical similarity metrics are com-puted using the publicly available NESim 3 package(Do et al 2009) with a threshold tuned on a subsetof Wikipedia redirects, and by insisting that ORGtype entities must have the same head word as thecandidate titles.
We only accept the link if there ex-ists exactly one title in the lexical searching resultafter pruning.5 Experiments and EvaluationThis section describes our experimental evaluation.We compare our system against the top D2W sys-tems and perform several experiments to analyzeand better understand the power of our approach.We based our work on the GLOW system from2We used the IllinoisNER package http://cogcomp.cs.illinois.edu/page/software_view/43http://cogcomp.cs.illinois.edu/page/software_view/22(Ratinov et al 2011) to initialize the candidates andcorresponding priors ski in our objective function.Both the baseline system and our new system arepublicly available 4.5.1 Comparison with other WikificationsystemsWe first evaluate on the same 4 datasets5 used in(Ratinov et al 2011).
The AQUAINT dataset, orig-inally introduced in (Milne and Witten, 2008), re-sembles the Wikipedia annotation structure in thatonly the first mention of a title is linked, and isthus less sensitive to coreference capabilities.
TheMSNBC dataset is from (Cucerzan, 2007) and in-cludes many mentions that do not easily map toWikipedia titles due to rare surface or other idiosyn-cratic lexicalization (Cucerzan, 2007; Ratinov et al2011).
Both of these datasets came from the newsdomain and do not contain any annotated NIL enti-ties.
The ACE and Wikipedia datasets are both takenfrom (Ratinov et al 2011) where ACE is a subsetof ACE2004 Coreference documents annotated byAmazon Mechanical Turkers in a similar standard asin AQUAINT but with NIL entities.
The Wikipediadataset is a sample of Wikipedia pages with its orig-inal hyperlink annotation.The evaluation methodology Bag of Titles (BOT)F1 was used in both (Milne and Witten, 2008; Rati-nov et al 2011).
For each document, the gold bagof titles is evaluated against our bag of system out-put titles requiring exact segmentation match.DatasetSystem ACE MSNBC AQUAINT WikiM&W 72.76 68.49 83.61 80.32R&R 77.25 74.88 83.94 90.54RI 85.30 81.20 88.88 93.09Table 1: Performance on Wikification datasets, BOT F1Performance.
Our system, Relational Inference (RI) ex-hibits significant improvements over M&W (Milne andWitten, 2008) and R&R (Ratinov et al 2011).4http://cogcomp.cs.illinois.edu/page/download_view/Wikifier5http://cogcomp.cs.illinois.edu/page/resource_view/417935.2 Ablation studyWe incrementally add various components to thesystem and study their impact on the end perfor-mance.
Due to the changes in Wikipedia since thedatasets were generated, some of the pages no longerexist; in order to minimize the interference causedby these inconsistencies to an accurate evaluationof various componenents, we consider all non-NILgold annotations that do not exist in the currentWikipedia index as NIL entities.
Additionally in theMSNBC dataset, 127 out of 756 surface forms areknown to be non-recallable.
This explains the per-formance difference between the final rows in Tab.1 and 2.DatasetComponents ACE MSNBC AQUAINT WikiBaseline 80.68 83.00 83.93 91.93+Lexical Match 83.47 84.13 88.88 93.41+Coreference 83.40 87.88 88.88 93.09RI 85.83 88.16 88.88 93.09Table 2: Ablation study on Wikification datasets, BOT F1PerformanceThe Baseline refers to the best performing configu-ration that was used in (Ratinov et al 2011) exceptfor using the current Wikipedia redirects.
The Lexi-cal Match refers to the applying solely the method-ology introduced in Sec.
4.4.
The Coreference per-formance includes all the inference performed with-out the KB triples, while the Relational Inference(RI) line represents all aspects of the proposed re-lational inference.
It is clear that different datasetsshow somewhat different characteristics and conse-quently different gains from the various aspects ofour approach but that, overall, all aspects contributeto improved performance.5.3 TAC Entity Linking 2011Next we evaluate our approach on the TAC EnglishEntity Linking Task, which provides standardizedevaluation metrics, allowing us to compare to a largenumber of other systems.
We did not evaluate on the2012 English Entity Linking due to the significantamount of ambiguous NIL entities included (Ellis etal., 2011) in the queries and the need to cluster them,which our D2W task definition does not address indepth.
We compare our system with the Top 3 TAC2011 systems (LCC, MS-MLI and NUSchime) aswell as our baseline system GLOW that participatedin TAC 2011 English Entity Linking (Ratinov andRoth, 2011) in table 3.
The evaluation metric is theofficial modified B3 and Micro-Average explainedin (Ji et al 2011).Given the TAC Knowledge Base (TKB), which isa subset of the 2009 Wikipedia Dump, the TAC En-tity Linking objective is to answer a named entityquery string with either a TKB entry ID or a NILentity ID, where the NIL entity IDs should be clus-tered across documents.It is important to note that we did not retrain oursystem on the TAC data as the top three systems did,even though the objective function is slightly differ-ent.
Instead, we ran our system on the TAC doc-uments directly without any query expansion.
Forthe final output of each query, we simply use themost confident candidate among all matched men-tions.
Due to the clustering requirement, we alsotrivially cluster NIL entities that either are mappedto the same out-of-KB Wikipedia URL or have thesame surface form.PerformanceSystem MA B3 P B3 R B3 F1LCC 86.1 84.4 84.7 84.6MS-MLI 86.8 84.8 83.4 84.1RI 86.1 82.9 84.5 83.7NUSchime 86.3 81.5 84.9 83.1RI-0 81.4 78.6 79.1 78.8Cogcomp 78.7 75.7 76.5 76.1Table 3: TAC2011 Entity Linking performance.
MA isMicro-Average.
LLC (Monahan et al 2011) is the bestperforming system in terms of B3 F1 while MS-MLI(Cucerzan, 2011) is the best in terms of Micro-Average.Cogcomp (Ratinov and Roth, 2011) is the GLOW basedsystem that participated in TAC 2011.RI is the completerelational inference system described in this paper; as de-scribed in the text, RI was not trained on the TAC data,unlike the other top systems.We performed two runs on the TAC2011 data tostudy the effects of relational inference.
The firstrun, RI-0, uses the current Wikipedia index and17940.650.70.750.80.850.9LCCMS_MLI RINUSchimeCUNY_UIUC_SRICOGCOMPCMCRCStanford_UBCCUNY_BLENDERHLTCOETHUNLP HITDMIR_INESIDMSRAWBSGMicro?averaged AccuracyB?cubed+ F?MeasureB?cubed+ PrecisionB?cubed+ RecallFigure 2: The RI compared with the other top 14TAC2011 English Entity Linking systems ranked bymodified B3 F1 measure.
Original figure from (Ji et al2011).redirects for lexical matching without any inference,which scored 2.7% higher than the original GLOWsystem (Cogcomp).
We can regard this performanceas the new baseline that benefited from the fuzzylexical matching capabilities that we have added, aswell as the broader set of surface forms and redirectsfrom the current Wikipedia dump.
In the second run,RI, the complete relational inference described inthis paper, scored 4.9% higher than the new base-line and sits on par with the top tier systems despitenot being trained on the given data.
The LCC sys-tem used sophisticated clustering algorithms trainedon the TAC development set (Monahan et al 2011).The second-ranked MS-MLI system relied on topicmodeling, external web search engine logs as well astraining on the development data (Cucerzan, 2011).This shows the robustness of our methods as wellas the general importance of understanding textualrelations in the task of Entity Linking and Wikifica-tion.6 Related Work and DiscussionEarlier works on Wikification formulated the taskas a WSD problem (Bunescu and Pasca, 2006; Mi-halcea and Csomai, 2007) and focused primarily ontraining a model using local context.
Later, variousglobal statistical approaches were proposed to em-phasize different coherence measures between the ti-tles of the disambiguated mentions in the same doc-ument (Cucerzan, 2007; Milne and Witten, 2008;Ratinov et al 2011).
Built on top of the statisti-cal models, our work focuses on leveraging deeperunderstanding of the text to more effectively and ac-curately utilize existing knowledge.We have demonstrated that, by incorporating tex-tual relations and semantic knowledge as linguisticconstraints in an inference framework, it is possibleto significantly improve Wikification performance.In particular, we have shown that our system is ca-pable of making ?intelligent?
inferences that makesuse of basic text understanding and has the ability toreason with it and verify it against relevant informa-tion sources.
This allows our Relational Inferenceapproach to resolve a variety of difficult examplesillustrated in the Introduction.Our system features high modularity since the re-lations are considered only at inference time; con-sequently, we can use any underlying Wikificationsystem as long as it outputs a distribution of titlecandidates for each mention.One possibility for future work is to supply thisframework with a richer set of relations from thetext, such as verbal relations.
It will also be inter-esting to incorporate high-level typed relations andrelax the relation arguments to be general conceptsrather than only named entities.AcknowledgmentsWe sincerely thank the three anonymous review-ers for their suggestions on the paper.
This ma-terial is based on research sponsored by DARPAunder agreement number FA8750-13-2-0008, andpartly supported by the Intelligence Advanced Re-search Projects Activity (IARPA) via Departmentof Interior National Business Center contract num-ber D11PC20155, by the Army Research Labora-tory (ARL) under agreement W911NF-09-2-0053,and by the Multimodal Information Access & Syn-thesis Center at UIUC, part of CCICADA, a DHSScience and Technology Center of Excellence.
TheU.S.
Government is authorized to reproduce and dis-tribute reprints for Governmental purposes notwith-standing any copyright annotation thereon.
Dis-claimer: The views and conclusions containedherein are those of the authors and should not beinterpreted as necessarily representing the official1795policies or endorsements, either expressed or im-plied, of DARPA, IARPA, DoI/NBC, ARL, or theU.S.
Government.ReferencesR.
Bunescu and M. Pasca.
2006.
Using encyclopedicknowledge for named entity disambiguation.
In Pro-ceedings of the European Chapter of the ACL (EACL).Y.
Chan and D. Roth.
2011.
Exploiting syntactico-semantic structures for relation extraction.
In Pro-ceedings of the Annual Meeting of the Association forComputational Linguistics (ACL), Portland, Oregon.M.
Chang, L. Ratinov, and D. Roth.
2012.
Structuredlearning with constrained conditional models.
Ma-chine Learning, 88(3):399?431, 6.Silviu Cucerzan.
2007.
Large-scale named entity disam-biguation based on Wikipedia data.
In Proceedings ofthe 2007 Joint Conference of EMNLP-CoNLL, pages708?716.Silviu Cucerzan.
2011.
Tac entity linking by perform-ing full-document entity extraction and disambigua-tion.
In Proceedings of the Text Analysis Conference.Q.
Do, D. Roth, M. Sammons, Y. Tu, and V. Vydiswaran.2009.
Robust, light-weight approaches to computelexical similarity.
Technical report, Computer ScienceDepartment, University of Illinois.Joe Ellis, Xuansong Li, Kira Griffitt, Stephanie MStrassel, and Jonathan Wright.
2011.
Linguistic re-sources for 2012 knowledge base population evalua-tions.Paolo Ferragina and Ugo Scaiella.
2010.
Tagme: on-the-fly annotation of short text fragments (by wikipediaentities).
In Proceedings of the 19th ACM interna-tional conference on Information and knowledge man-agement, CIKM ?10, pages 1625?1628, New York,NY, USA.
ACM.Inc.
Gurobi Optimization.
2013.
Gurobi optimizer refer-ence manual.Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-fitt, and Joe Ellis.
2010.
Overview of the tac 2010knowledge base population track.
In Third Text Anal-ysis Conference (TAC 2010).Heng Ji, Ralph Grishman, and Hoa Trang Dang.
2011.Overview of the tac 2011 knowledge base populationtrack.
In Fourth Text Analysis Conference (TAC 2011).R.
Mihalcea and A. Csomai.
2007.
Wikify!
: linking doc-uments to encyclopedic knowledge.
In Proceedingsof ACM Conference on Information and KnowledgeManagement (CIKM), pages 233?242.D.
Milne and I. H. Witten.
2008.
Learning to linkwith wikipedia.
In Proceedings of ACM Conferenceon Information and Knowledge Management (CIKM),pages 509?518.Sean Monahan, John Lehmann, Timothy Nyberg, JessePlymale, and Arnold Jung.
2011.
Cross-lingual cross-document coreference with entity linking.
In Proceed-ings of the Text Analysis Conference.L.
Ratinov and D. Roth.
2011.
Glow tac-kbp 2011 entitylinking system.
In TAC.
Text Analysis Conference, 11.L.
Ratinov and D. Roth.
2012.
Learning-based multi-sieve co-reference resolution with knowledge.
InEMNLP.L.
Ratinov, D. Roth, D. Downey, and M. Anderson.2011.
Local and global algorithms for disambiguationto wikipedia.
In ACL.D.
Roth and W. Yih.
2004.
A linear programming formu-lation for global inference in natural language tasks.
InHwee Tou Ng and Ellen Riloff, editors, Proceedingsof the Annual Conference on Computational NaturalLanguage Learning (CoNLL), pages 1?8.
Associationfor Computational Linguistics.1796
