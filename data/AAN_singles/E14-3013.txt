Proceedings of the Student Research Workshop at the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 116?126,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsGenerating artificial errors for grammatical error correctionMariano FeliceComputer LaboratoryUniversity of CambridgeUnited Kingdommf501@cam.ac.ukZheng YuanComputer LaboratoryUniversity of CambridgeUnited Kingdomzy249@cam.ac.ukAbstractThis paper explores the generation of ar-tificial errors for correcting grammaticalmistakes made by learners of English asa second language.
Artificial errors are in-jected into a set of error-free sentences in aprobabilistic manner using statistics froma corpus.
Unlike previous approaches, weuse linguistic information to derive errorgeneration probabilities and build corporato correct several error types, includingopen-class errors.
In addition, we alsoanalyse the variables involved in the selec-tion of candidate sentences.
Experimentsusing the NUCLE corpus from the CoNLL2013 shared task reveal that: 1) trainingon artificially created errors improves pre-cision at the expense of recall and 2) dif-ferent types of linguistic information arebetter suited for correcting different errortypes.1 IntroductionBuilding error correction systems using machinelearning techniques can require a considerableamount of annotated data which is difficult to ob-tain.
Available error-annotated corpora are oftenfocused on particular groups of people (e.g.
non-native students), error types (e.g.
spelling, syn-tax), genres (e.g.
university essays, letters) or top-ics so it is not clear how representative they areor how well systems based on them will gener-alise.
On the other hand, building new corpora isnot always a viable solution since error annotationis expensive.
As a result, researchers have triedto overcome these limitations either by compilingcorpora automatically from the web (Mizumoto etal., 2011; Tajiri et al., 2012; Cahill et al., 2013) orusing artificial corpora which are cheaper to pro-duce and can be tailored to their needs.Artificial error generation allows researchers tocreate very large error-annotated corpora with lit-tle effort and control variables such as topic anderror types.
Errors can be injected into candidatetexts using a deterministic approach (e.g.
fixedrules) or probabilities derived from manually an-notated samples in order to mimic real data.Although artificial errors have been used in pre-vious work, we present a new approach based onlinguistic information and evaluate it using the testdata provided for the CoNLL 2013 shared task ongrammatical error correction (Ng et al., 2013).Our work makes the following contributions.First, we are the first to use linguistic informa-tion (such as part-of-speech (PoS) information orsemantic classes) to characterise contexts of natu-rally occurring errors and replicate them in error-free text.
Second, we apply our technique to alarger number of error types than any other pre-vious approach, including open-class errors.
Theresulting datasets are used to train error correctionsystems aimed at learners of English as a secondlanguage (ESL).
Finally, we provide a detailed de-scription of the variables that affect artificial errorgeneration.2 Related workThe use of artificial data to train error correctionsystems has been explored by other researchers us-ing a variety of techniques.Izumi et al.
(2003), for example, use artificialerrors to target article mistakes made by Japaneselearners of English.
A corpus is created by replac-ing a, an, the or the zero article by a different ar-ticle chosen at random in more than 7,500 correctsentences and used to train a maximum entropymodel.
Results show an improvement for omis-sion errors but no change for replacement errors.Brockett et al.
(2006) describe the use of a sta-tistical machine translation (SMT) system for cor-recting a set of 14 countable/uncountable nouns116which are often confusing for ESL learners.
Theirtraining corpus consists of a large number of sen-tences extracted from news articles which were de-liberately modified to include typical countabilityerrors based on evidence from a Chinese learnercorpus.
Their approach to artificial error injec-tion is deterministic, using hand-coded rules tochange quantifiers (much?
many), generate plu-rals (advice?
advices) or insert unnecessary de-terminers.
Experiments show their system wasgenerally able to beat the standard Microsoft Word2003 grammar checker, although it produced a rel-atively higher rate of erroneous corrections.SMT systems are also used by Ehsan and Faili(2013) to correct grammatical errors and context-sensitive spelling mistakes in English and Farsi.Training corpora are obtained by injecting arti-ficial errors into well-formed treebank sentencesusing predefined error templates.
Whenever anoriginal sentence from the corpus matches one ofthese templates, a pair of correct and incorrect sen-tences is generated.
This process is repeated mul-tiple times if a single sentence matches more thanone error template, thereby generating many pairsfor the same original sentence.
A comparison be-tween the proposed systems and rule-based gram-mar checkers show they are complementary, witha hybrid system achieving the best performance.2.1 Probabilistic approachesA few researchers have explored probabilisticmethods in an attempt to mimic real data more ac-curately.
Foster and Andersen (2009), for exam-ple, describe a tool for generating artificial errorsbased on statistics from other corpora, such as theCambridge Learner Corpus (CLC).1Their experi-ments show a drop in accuracy when artificial sen-tences are used as a replacement for real incorrectsentences, suggesting that they may not be as use-ful as genuine text.
Their report also includes anextensive summary of previous work in the area.Rozovskaya and Roth propose more sophis-ticated probabilistic methods to generate artifi-cial errors for articles (2010a) and prepositions(2010b; 2011), also based on statistics from anESL corpus.
In particular, they compile a set ofsentences from the English Wikipedia and applythe following generation methods:1http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/Cambridge-International-Corpus-Cambridge-Learner-Corpus/GeneralTarget words (e.g.
articles) are replaced with oth-ers of the same class with probability x (varyingfrom 0.05 to 0.18).
Each new word is chosen uni-formly at random.Distribution before correction (in ESL data)Target words in the error-free text are changedto match the distribution observed in ESL error-annotated data before any correction is made.Distribution after correction (in ESL data)Target words in the error-free text are changedto match the distribution observed in ESL error-annotated data after corrections are made.Native language-specific distributionsIt has been observed that second language produc-tion is affected by a learner?s native language (L1)(Lee and Seneff, 2008; Leacock et al., 2010).
Acommon example is the difficulty in using Englisharticles appropriately by learners whose L1 hasno article system, such as Russian or Japanese.Because word choice errors follow systematic pat-terns (i.e.
they do not occur randomly), this infor-mation is extremely valuable for generating errorsmore accurately.L1-specific errors can be imitated by computingword confusions in an error-annotated ESL cor-pora and using these distributions to change tar-get words accordingly in error-free text.
Morespecifically, if we estimate P(source|target) in anerror-tagged corpus (i.e.
the probability of anincorrect source word being used when the cor-rect target is expected), we can generate more ac-curate confusion sets where each candidate hasan associated probability depending on the ob-served word.
For example, supposing that agroup of learners use the preposition to in 10%of cases where the preposition for should be used(that is, P(source=to|target=for)=0.10), we canreplicate this error pattern by replacing the oc-currences of the preposition for with to with aprobability of 0.10 in a corpus of error-free sen-tences.
When the source and target words are thesame, P(source=x|target=x) expresses the proba-bility that a learner produces the correct/expectedword.Because errors are generally sparse (and there-fore error rates are low), replicating mistakesbased on observed probabilities can easily lead to117low recall.
In order to address this issue during ar-tificial error generation, Rozovskaya et al.
(2012)propose an inflation method that boosts confusionprobabilities in order to generate a larger propor-tion of artificial instances.
This reformulation isshown to improve F-scores when correcting deter-miners and prepositions.Experiments reveal that these approaches yieldbetter results than assuming uniform probabilis-tic distributions where all errors and correc-tions are equally likely.
In particular, classifierstrained on artificially generated data outperformedthose trained on native error-free text (Rozovskayaand Roth, 2010a; Rozovskaya and Roth, 2011).However, it has also been shown that using arti-ficially generated data as a replacement for non-native error-corrected data can lead to poorer per-formance (Sj?obergh and Knutsson, 2005; Fosterand Andersen, 2009).
This would suggest that ar-tificial errors are more useful than native data butless useful than corrected non-native data.Rozovskaya and Roth also control other vari-ables in their experiments.
On the one hand, theyonly evaluate their systems on sentences that haveno spelling mistakes so as to avoid degrading per-formance.
This is particularly important whentraining classifiers on features extracted with lin-guistic tools (such as parsers or taggers) as theycould provide inaccurate results for malformed in-put.
On the other hand, the authors work on a lim-ited set of error types (mainly articles and preposi-tions) which are closed word classes and thereforehave reduced confusion sets.
Thus, it becomes in-teresting to investigate how their ideas extrapolateto open-class error types, like verb form or contentword errors.Their probabilistic error generation approachhas also been used by other researchers.
Imamuraet al.
(2012), for example, applied this method togenerate artificial incorrect sentences for Japaneseparticle correction with an inflation factor rangingfrom 0.0 (no errors) to 2.0 (double error rates).Their results show that the performance of artifi-cial corpora depends largely on the inflation ratebut can achieve good results when domain adapta-tion is applied.In a more exhaustive study, Cahill et al.
(2013) investigate the usefulness of automatically-compiled sentences from Wikipedia revisionsfor correcting preposition errors.
A numberof classifiers are trained using error-free text,automatically-compiled annotated corpora and ar-tificial sentences generated using error probabili-ties derived from Wikipedia revisions and Lang-8.2Their results reveal a number of interestingpoints, namely that artificial errors provide com-petitive results and perform robustly across differ-ent test sets.
A learning curve analysis also showssystem performance increases as more trainingdata is used, both real and artificial.More recently, some teams have also reportedimprovements by using artificial data in theirsubmissions to the CoNLL 2013 shared task.Rozovskaya et al.
(2013) apply their inflationmethod to train a classifier for determiner errorsthat achieves state-of-the-art performance whileYuan and Felice (2013) use naively-generated arti-ficial errors within an SMT framework that placesthem third in terms of precision.3 Advanced generation of artificialerrorsOur work is based on the hypothesis that usingcarefully generated artificial errors improves theperformance of error correction systems.
This im-plies generating errors in a way that resemblesavailable error-annotated data, using similar textsand accurate injection methods.
Like other proba-bilistic approaches, our method assumes we haveaccess to an error-corrected reference corpus fromwhich we can compute error generation probabili-ties.3.1 Base text selectionWe analyse a set of variables that we consider im-portant for collecting suitable texts for error injec-tion, namely:TopicReplicating errors on texts about the same topicas the training/test data is more likely to producebetter results than out-of-domain data, as vocab-ulary and word senses are more likely to be sim-ilar.
In addition, similar texts are more likely toexhibit suitable contexts for error injection andconsequently help the system focus on particularlyuseful information.GenreIn cases where no a priori information about topicis available (for example, because the test set is2http://lang-8.com/118unknown or the system will be used in differentscenarios), knowing the genre or type of text thesystem will process can also be useful.
Examplegenres include expository (descriptions, essays,reports, etc.
), narrative (stories), persuasive (re-views, advertisements, etc.
), procedural (instruc-tions, recipes, experiments, etc.)
and transactionaltexts (letters, interviews, etc.
).Style/registerAs with the previous aspects, style (colloquial,academic, etc.)
and register (from formal writ-ten to informal spoken) also affect production andshould therefore be modelled accurately in thetraining data.Text complexity/language proficiencyCandidate texts should exhibit the same readingcomplexity as training/test texts and be written byor targeted at learners with similar English profi-ciency.
Otherwise, the overlap in vocabulary andgrammatical structures is more likely to be smalland thus hinder error injection.Native languageBecause second language production is known tobe affected by a learner?s L1, using candidate textsproduced by groups of the same L1 as the train-ing/test data should provide more suitable contextsfor error injection.
When such texts are not avail-able, using data by speakers of other L1s that ex-hibit similar phenomena (e.g.
no article system,agglutinative languages, etc.)
might also be use-ful.
However, finding error-free texts written inEnglish by a specific population can be difficult,which is why most approaches resort to nativeEnglish text.In our experiments, the aforementioned vari-ables are manually controlled although we believemany of them could be assessed automatically.For example, topics could be estimated using textsimilarity measures, genres could be predicted us-ing structural information and L1s could be in-ferred using a native language identifier.3For an analysis of other variables such as do-main and error distributions, the reader should re-fer to Cahill et al.
(2013).3See the First Edition of the Shared Task on NativeLanguage Identification (Tetreault et al., 2013) at https://sites.google.com/site/nlisharedtask2013/3.2 Error replicationOur approach to artificial error generation is sim-ilar to the one proposed by Rozovskaya and Roth(2010a) in that we also estimate probabilities ina corpus of ESL learners which are then used todistort error-free text.
However, unlike them, werefine our probabilities by imposing restrictionson the linguistic functions of the words and thecontexts where they occur.
Because we extendgeneration to open-class error types (such as verbform errors), this refinement becomes necessary toovercome disambiguation issues and lead to moreaccurate replication.Our work is the first to exploit linguistic infor-mation for error generation, as described below.Error type distributionsWe compute the probability of each error type p(t)occurring over the total number of relevant in-stances (e.g.
noun phrases are relevant instancesfor article errors).
During generation, p(t) is uni-formly distributed over all the possible choices forthe error type (e.g.
for articles, choices are a, an,the or the zero article).
Relevant instances are de-tected in the base text and changed for an alter-native at random using the estimated probabilities.The probability of leaving relevant instances un-changed is 1?
p(t).MorphologyWe believe morphological information such asperson or number is particularly useful for identi-fying and correcting specific error types, such asarticles, noun number or subject-verb agreement.Thus, we compute the conditional probability ofwords in specific classes for different morpholog-ical contexts (such as noun number or PoS).
Thefollowing example shows confusion probabilitiesfor singular head nouns requiring an:P(source-det=an|target-det=anhead-noun=NN) = 0.942P(source-det=the|target-det=anhead-noun=NN) = 0.034P(source-det=a|target-det=anhead-noun=NN) = 0.015P(source-det=other|target-det=anhead-noun=NN) = 0.005P(source-det=?|target-det=anhead-noun=NN) = 0.004PoS disambiguationMost approaches to artificial error generation areaimed at correcting closed-class words such asarticles or prepositions, which rarely occur with119a different part of speech in the text.
However,when we consider open-class error types, weshould perform PoS disambiguation since thesame surface form could play different roles ina sentence.
For example, consider generatingartificial verb form errors for the verb to play afterobserving its distribution in an error-annotatedcorpus.
By using PoS tags, we can easily deter-mine if an occurrence of the word play is a verb ora noun and thus compute or apply the appropriateprobabilities:P(source=play|target=playV) = 0.98P(source=plays|target=playV) = 0.02P(source=play|target=playN) = 0.84P(source=plays|target=playN) = 0.16Semantic classesWe hypothesise that semantic information aboutconcepts in the sentences can shed light onspecific usage patterns that may otherwise behidden.
For example, we could refine confusionsets for prepositions according to the type ofobject they are applied to (a location, a recipient,an instrument, etc.
):P(prep=in|noun class=location) = 0.39P(prep=to|noun class=location) = 0.31P(prep=at|noun class=location) = 0.16P(prep=from|noun class=location) = 0.07P(prep=?|noun class=location) = 0.05P(prep=other|noun class=location) = 0.03By abstracting from surface forms, we can alsogenerate faithful errors for words that have notbeen previously observed, e.g.
we may have notseen hospital but we may have seen school, mysister?s house or church.Word sensesPolysemous words with the same PoS can exhibitdifferent patterns of usage for each of their mean-ings (e.g.
one meaning may co-occur with a spe-cific preposition more often than the others).
Forthis reason, we introduce probabilities for eachword sense in an attempt to capture more accurateusage.
As an example, consider a hypothetical sit-uation in which a group of learners confuse prepo-sitions used with the word bank as a financial insti-tution but they produce the right preposition whenit refers to a river bed:P(prep=in|noun=bank1) = 0.76P(prep=at|noun=bank1) = 0.18P(prep=on|noun=bank1) = 0.06P(prep=on|noun=bank2) = 1.00Although it is rare that occurrences of the sameword will refer to different meanings within adocument (the so-called ?one sense per discourse?assumption (Gale et al., 1992)), this is not thecase when large corpora containing different doc-uments are used for characterising and generatingerrors.
In such scenarios, word sense disambigua-tion should produce more accurate results.Table 1 lists the actual probabilities computedfrom each type of information and the errors theyare able to generate.4 Experimental setup4.1 Corpora and toolsWe use the NUCLE v2.3 corpus (Dahlmeier etal., 2013) released for the CoNLL 2013 sharedtask on error correction, which comprises error-annotated essays written in English by studentsat the National University of Singapore.
Theseessays cover topics such as environmental pollu-tion, health care, welfare, technology, etc.
All thesentences were manually annotated by human ex-perts using a set of 27 error types, but we used thefiltered version containing only the five types se-lected for the shared task: ArtOrDet (article or de-terminer), Nn (noun number), Prep (preposition),SVA (subject-verb agreements) and Vform (verbform) errors.
The training set of the NUCLE cor-pus contains 57,151 sentences and 1,161,567 to-kens while the test set comprises 1,381 sentencesand 29,207 tokens.
The training portion of the cor-pus was used to estimate the required conditionalprobabilities and train a few variations of our sys-tems while the test set was reserved to evaluateperformance.Candidate native texts for error injection wereextracted from the English Wikipedia, controllingthe variables described Section 3.1 as follows:Topic: We chose an initial set of 50 Wikipediaarticles based on keywords in the NUCLEtraining data and proceeded to collect relatedarticles by following hyperlinks in their ?Seealso?
section.
We retrieved a total of 494 arti-cles which were later preprocessed to remove120Information Probability Generated error typesError type distribution P(error type) ArtOrDet, Nn, Prep, SVA, VformMorphology P(source=determiner|target=determiner, head noun tag) ArtOrDet, SVAP(source=verb tag|target=verb tag, subj head noun tag)PoS disambiguation P(source=word|target=word, PoS) Nn, VformSemantic classes P(source=determiner|target=determiner, head noun class) ArtOrDet, PrepP(source=preposition|target=preposition, head noun class)Word senses P(source=preposition|verb sense + obj head noun sense) ArtOrDet, Prep, SVAP(source=preposition|target=preposition, head noun sense)P(source=preposition|target=preposition, dep adj sense)P(source=determiner|target=determiner, head noun sense)P(source=verb tag|target=verb tag, subj head noun sense)Table 1: Probabilities computed for each type of linguistic information.
Error codes correspond to thefive error types in the CoNLL 2013 shared task: ArtOrDet (article or determiner), Nn (noun number),Prep (prepositions), SVA (subject-verb agreement) and Vform (verb form).wikicode tags, yielding 54,945 sentences andapproximately 1,123,739 tokens.Genre: Both NUCLE and Wikipedia contain ex-pository texts, although they are not necessar-ily similar.Style/register: Written, academic and formal.Text complexity/language proficiency: Essaysin the NUCLE corpus are written by ad-vanced university students and are thereforecomparable to standard English Wikipediaarticles.
For less sophisticated language,the Simple English Wikipedia could be analternative.Native language: English Wikipedia articles aremostly written by native speakers whereasNUCLE essays are not.
This is the only dis-cordant variable.PoS tagging was performed using RASP(Briscoe et al., 2006).
Word sense dis-ambiguation was carried out using theWordNet::SenseRelate:AllWords Perl module(Pedersen and Kolhatkar, 2009) which assignsa sense from WordNet (Miller, 1995) to eachcontent word in a text.
As for semantic in-formation, we use WordNet classes which arereadily available in NLTK (Bird et al., 2009).WordNet classes respond to a classification inlexicographers?
files4and are defined for contentwords as shown in Table 2, depending on theirlocation in the hierarchy.4http://wordnet.princeton.edu/man/lexnames.5WN.htmlPart of speech WordNet classificationAdjective all, pertainyms, participialAdverb allNoun act, animal, artifact, attribute, body,cognition, communication, event,feeling, food, group, location, motive,object, person, phenomenon, plant,possession, process, quantity, relation,shape, state, substance, timeVerb body, change, cognition,communication, competition,consumption, contact, creation,emotion, motion, perception,possession, social, stative, weatherTable 2: WordNet classes for content words.Name CompositionED errors based on error type distributionsMORPH errors based on morphologyPOS errors based on PoS disambiguationSC errors based on semantic classesWSD errors based on word sensesTable 3: Generated artificial corpora based on dif-ferent types of linguistic information.4.2 Error generationFor each type of information in Table 1, we com-pute the corresponding conditional probabilitiesusing the NUCLE training set.
These probabili-ties are then used to generate six different artificialcorpora using the inflation method (Rozovskaya etal., 2012), as listed in Table 3.4.3 System trainingWe approach the error correction task as a transla-tion problem from incorrect into correct English.Systems are built using an SMT framework anddifferent combinations of NUCLE and our artifi-cial corpora, where the source side contains in-121Original RevisedC M U P R F1C M U P R F1NUCLE (baseline) 181 1462 513 0.2608 0.1102 0.1549 200 1483 495 0.2878 0.1188 0.1682ED 53 1590 150 0.2611 0.0323 0.0574 62 1621 141 0.3054 0.0368 0.0657MORPH 74 1569 333 0.1818 0.0450 0.0722 83 1600 324 0.2039 0.0493 0.0794POS 42 1601 99 0.2979 0.0256 0.0471 42 1641 99 0.2979 0.0250 0.0461SC 80 1563 543 0.1284 0.0487 0.0706 87 1596 536 0.1396 0.0517 0.0755WSD 82 1561 305 0.2119 0.0499 0.0808 91 1592 296 0.2351 0.0541 0.0879NUCLE+ED 173 1470 411 0.2962 0.1053 0.1554 194 1489 390 0.3322 0.1153 0.1712NUCLE+MORPH 163 1480 427 0.2763 0.0992 0.1460 182 1501 408 0.3085 0.1081 0.1601NUCLE+POS 164 1479 365 0.3100 0.0998 0.1510 182 1501 347 0.3440 0.1081 0.1646NUCLE+SC 162 1481 488 0.2492 0.0986 0.1413 181 1502 469 0.2785 0.1075 0.1552NUCLE+WSD 163 1480 413 0.2830 0.0992 0.1469 181 1502 395 0.3142 0.1075 0.1602Table 4: Evaluation of our correction systems over the original and revised NUCLE test set using the M2Scorer.
Columns C, M and U show the number of correct, missed and unnecessary corrections suggestedby each system.
Results in bold show improvements over the baseline.correct sentences and the target side contains theircorrected versions.
Our setup is similar to the onedescribed by Yuan and Felice (2013) in that wetrain a PoS-factored phrase-based model (Koehn,2010) using Moses (Koehn et al., 2007), Giza++(Och and Ney, 2003) for word alignment and theIRSTLM Toolkit (Federico et al., 2008) for lan-guage modelling.
However, unlike them, we donot optimise decoding parameters but use defaultvalues instead.We build 11 different systems in total: a base-line system using only the NUCLE training set,one system per artificial corpus and other addi-tional systems using combinations of the NUCLEtraining data and our artificial corpora.
Each ofthese systems uses a single translation model thattackles all error types at the same time.5 ResultsEach system was evaluated in terms of precision,recall and F1on the NUCLE test data using theM2Scorer (Dahlmeier and Ng, 2012), the officialevaluation script for the CoNLL 2013 shared task.Table 4 shows results of evaluation on the originaltest set (containing only one gold standard correc-tion per error) and a revised version (which allowsfor alternative corrections submitted by participat-ing teams).Results reveal our ED and POS corpora are ableto improve precision for both test sets.
It is surpris-ing, however, that the least informed dataset (ED)is one of the best performers although this seemsreasonable if we consider it is the only dataset thatincludes artificial instances for all error types (seeTable 1).
Hybrid datasets containing the NUCLEtraining set plus an artificial corpus also gener-ally improve precision, except for NUCLE+SC.
Itcould be argued that the reason for this improve-ment is corpus size, since our hybrid datasets aredouble the size of each individual set, but the smalldifferences in precision between the ED and POSdatasets and their corresponding hybrid versionsseem to contradict that hypothesis.
In fact, re-sults would suggest artificial and naturally occur-ring errors are not interchangeable but rather com-plementary.The observed improvement in precision, how-ever, comes at the expense of recall, for whichnone of the systems is able to beat the baseline.This contradicts results by Rozovskaya and Roth(2010a), who show their error inflation method in-creases recall, although this could be due to differ-ences in the training paradigm and data.
Still, re-sults are encouraging since precision is generallypreferred over recall in error correction scenarios(Yuan and Felice, 2013).We also evaluated performance by error type onthe original (Table 5) and revised (Table 6) testdata using an estimation approach similar to theone in CoNLL 2013.
Results show that the per-formance of each dataset varies by error type, sug-gesting that certain types of information are bet-ter suited for specific error types.
In particular,we find that on the original test set, ED achievesthe highest precision for article and determiners,WSD maximises precision for prepositions andSC achieves the highest recall and F1.
When us-ing hybrid sets, results improve overall, with thehighest precision being as follows: NUCLE+POS(ArtOrDet), NUCLE+ED (Nn), NUCLE+WSD122ArtOrDet Nn Prep SVA/Vform OtherP R F1P R F1P R F1P R F1C M UNUCLE (b) 0.2716 0.1551 0.1974 0.4625 0.0934 0.1555 0.1333 0.0386 0.0599 0.2604 0.1016 0.1462 0 0 34ED 0.2813 0.0391 0.0687 0.6579 0.0631 0.1152 0.0233 0.0032 0.0056 0.0000 0.0000 ?
0 0 5MORPH 0.1862 0.1058 0.1349 ?
0.0000 ?
0.0000 0.0000 ?
0.1429 0.0041 0.0079 0 0 7POS 0.0000 0.0000 ?
0.4405 0.0934 0.1542 0.0000 0.0000 ?
0.1515 0.0203 0.0358 0 0 10SC 0.1683 0.0739 0.1027 ?
0.0000 ?
0.0986 0.0932 0.0959 0.0000 0.0000 ?
0 0 21WSD 0.2219 0.1029 0.1406 0.0000 0.0000 ?
0.1905 0.0257 0.0453 0.1875 0.0122 0.0229 0 0 8NUCLE+ED 0.3185 0.1348 0.1894 0.5465 0.1187 0.1950 0.1304 0.0386 0.0596 0.2658 0.0854 0.1292 0 0 35NUCLE+MORPH 0.2857 0.1507 0.1973 0.4590 0.0707 0.1225 0.1719 0.0354 0.0587 0.2817 0.0813 0.1262 0 0 30NUCLE+POS 0.3384 0.1290 0.1868 0.4659 0.1035 0.1694 0.1884 0.0418 0.0684 0.2625 0.0854 0.1288 0 0 29NUCLE+SC 0.2890 0.1290 0.1784 0.4500 0.0682 0.1184 0.1492 0.0868 0.1098 0.2836 0.0772 0.1214 0 0 34NUCLE+WSD 0.3003 0.1449 0.1955 0.4667 0.0707 0.1228 0.1948 0.0482 0.0773 0.2632 0.0813 0.1242 0 0 30Table 5: Error type analysis of our correction systems over the original NUCLE test set using the M2Scorer.
Columns C, M and U show the number of correct, missed and unnecessary corrections outsidethe main categories suggested by each system.
Results in bold show improvements over the baseline.ArtOrDet Nn Prep SVA/Vform OtherP R F1P R F1P R F1P R F1C M UNUCLE (b) 0.3519 0.2026 0.2572 0.6163 0.1302 0.2150 0.2069 0.0682 0.1026 0.4105 0.1718 0.2422 0 0 34ED 0.4063 0.0579 0.1014 0.7297 0.0684 0.1250 0.0465 0.0077 0.0132 0.1818 0.0183 0.0332 0 0 5MORPH 0.2270 0.1311 0.1662 ?
0.0000 ?
0.0000 0.0000 ?
0.2857 0.0092 0.0179 0 0 7POS 0.0000 0.0000 ?
0.5465 0.1169 0.1926 0.0000 0.0000 ?
0.4242 0.0631 0.1098 0 0 10SC 0.2112 0.0944 0.1305 ?
0.0000 ?
0.1088 0.1221 0.1151 0.0000 0.0000 ?
0 0 21WSD 0.2781 0.1313 0.1784 0.0000 0.0000 ?
0.2143 0.0347 0.0598 0.2000 0.0138 0.0259 0 0 8NUCLE+ED 0.4334 0.1849 0.2592 0.7000 0.1552 0.2540 0.1685 0.0575 0.0857 0.4744 0.1630 0.2426 0 0 35NUCLE+MORPH 0.3791 0.2006 0.2624 0.6308 0.1017 0.1752 0.2295 0.0536 0.0870 0.4714 0.1454 0.2222 0 0 30NUCLE+POS 0.4601 0.1761 0.2547 0.6087 0.1383 0.2254 0.2424 0.0613 0.0979 0.4430 0.1549 0.2295 0 0 29NUCLE+SC 0.3961 0.1773 0.2450 0.6154 0.0993 0.1709 0.1844 0.1250 0.1490 0.4848 0.1410 0.2184 0 0 34NUCLE+WSD 0.3994 0.1933 0.2605 0.6308 0.1017 0.1752 0.2432 0.0690 0.1075 0.4667 0.1535 0.2310 0 0 30Table 6: Error type analysis of our correction systems over the revised NUCLE test set using the M2Scorer.
Columns C, M and U show the number of correct, missed and unnecessary corrections outsidethe main categories suggested by each system.
Results in bold show improvements over the baseline.
(Prep) and NUCLE+SC (SVA/Vform).
As ex-pected, the use of alternative annotations in the re-vised test set improves results but it does not revealany qualitative difference between datasets.Finally, when compared to other systems in theCoNLL 2013 shared task in terms of F1, our bestsystems would rank 9th on both test sets.
Thiswould suggest that using an off-the-shelf SMTsystem trained on a combination of real and ar-tificial data can yield better results than other ma-chine learning techniques (Yi et al., 2013; van denBosch and Berck, 2013; Berend et al., 2013) orrule-based approaches (Kunchukuttan et al., 2013;Putra and Szabo, 2013; Flickinger and Yu, 2013;Sidorov et al., 2013).6 ConclusionsThis paper presents early results on the genera-tion and use of artificial errors for grammaticalerror correction.
Our approach uses conditionalprobabilities derived from an ESL error-annotatedcorpus to replicate errors in native error-free data.Unlike previous work, we propose using linguisticinformation such as PoS or sense disambiguationto refine the contexts where errors occur and thusreplicate them more accurately.
We use five differ-ent types of information to generate our artificialcorpora, which are later evaluated in isolation aswell as coupled to the original ESL training data.General results show error distributions and PoSinformation produce the best results, although thisvaries when we analyse each error type separately.These results should allow us to generate errorsmore efficiently in the future by using the best ap-proach for each error type.We have also observed that precision improvesat the expense of recall and this is more pro-nounced when using purely artificial sets.
Finally,artificially generated errors seem to be a comple-ment rather than an alternative to genuine data.7 Future workThere are a number of issues we plan to address infuture research, as described below.Scaling up artificial dataThe experiments presented here use a small andmanually selected collection of Wikipedia articles.123However, we plan to study the performance of oursystems as corpus size is increased.
We are cur-rently using a larger selection of Wikipedia ar-ticles to produce new artificial datasets rangingfrom 50K to 5M sentences.
The resulting corporawill be used to train new error correction systemsand study how precision and recall vary as moredata is added during the training process, similarto Cahill et al.
(2013).Reducing differences between datasetsAs shown in Table 1, we are unable to producethe same set of errors for each different type of in-formation.
This is a limitation of our conditionalprobabilities which encode different informationin each case.
In consequence, comparing overallresults between datasets seems unfair as they donot target the same error types.
In order to over-come this problem, we will define new probabili-ties so that we can generate the same types of errorin all cases.Exploring larger contextsOur current probabilities model error contextsin a limited way, mostly by considering rela-tions between two or three words (e.g.
arti-cle+noun, verb+preposition+noun, etc.).
In or-der to improve error injection, we will definenew probabilities using larger contexts, such asP(source=verb|target=verb, subject class, auxil-iary verbs, object class) for verb form errors.Using more specific contexts can also be useful forcorrecting complex error types, such as the use ofpronouns, which often requires analysing corefer-ence chains.Using new linguistic informationIn this work we have used five types of linguis-tic information.
However, we believe other typesof information and their associated probabilitiescould also be useful, especially if we aim to cor-rect more error types.
Examples include spelling,grammatical relations (dependencies) and wordorder (syntax).
Additionally, we believe the useof semantic role labels can be explored as an al-ternative to semantic classes, as they have proveduseful for error correction (Liu et al., 2010).Mixed error generationIn our current experiments, each artificial corpus isgenerated using only one type of information at atime.
However, having found that certain types ofinformation are more suitable than others for cor-recting specific error types (see Tables 5 and 6), webelieve better artificial corpora could be created bygenerating instances of each error type using onlythe most appropriate linguistic information.AcknowledgmentsWe would like to thank Prof Ted Briscoe for hisvaluable comments and suggestions as well asCambridge English Language Assessment, a divi-sion of Cambridge Assessment, for supporting thisresearch.ReferencesGabor Berend, Veronika Vincze, Sina Zarrie?, andRich?ard Farkas.
2013.
Lfg-based featuresfor noun number and article grammatical errors.In Proceedings of the Seventeenth Conferenceon Computational Natural Language Learning:Shared Task, pages 62?67, Sofia, Bulgaria, August.Association for Computational Linguistics.Steven Bird, Edward Loper, and Ewan Klein.2009.
Natural Language Processing with Python.O?Reilly Media Inc.Ted Briscoe, John Carroll, and Rebecca Watson.2006.
The second release of the RASP sys-tem.
In Proceedings of the COLING/ACL onInteractive presentation sessions, COLING-ACL?06, pages 77?80, Sydney, Australia.
Association forComputational Linguistics.Chris Brockett, William B. Dolan, and MichaelGamon.
2006.
Correcting ESL Errors UsingPhrasal SMT Techniques.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, pages249?256, Sydney, Australia, July.
Association forComputational Linguistics.Aoife Cahill, Nitin Madnani, Joel Tetreault, andDiane Napolitano.
2013.
Robust systems forpreposition error correction using wikipedia revi-sions.
In Proceedings of the 2013 Conferenceof the North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, pages 507?517, Atlanta, Georgia,June.
Association for Computational Linguistics.Daniel Dahlmeier and Hwee Tou Ng.
2012.Better evaluation for grammatical error correc-tion.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Associationfor Computational Linguistics: Human LanguageTechnologies, NAACL 2012, pages 568 ?
572,Montreal, Canada.124Daniel Dahlmeier, Hwee Tou Ng, and Siew MeiWu.
2013.
Building a Large Annotated Corpusof Learner English: The NUS Corpus of LearnerEnglish.
In Proceedings of the 8th Workshop onInnovative Use of NLP for Building EducationalApplications, BEA 2013, pages 22?31, Atlanta,Georgia, USA, June.
Association for ComputationalLinguistics.Nava Ehsan and Heshaam Faili.
2013.
Grammaticaland context-sensitive error correction using a sta-tistical machine translation framework.
Software:Practice and Experience, 43(2):187?206.Marcello Federico, Nicola Bertoldi, and MauroCettolo.
2008.
IRSTLM: an open source toolkitfor handling large scale language models.
InProceedings of the 9th Annual Conference of theInternational Speech Communication Association,INTERSPEECH 2008, pages 1618?1621, Brisbane,Australia, September.
ISCA.Dan Flickinger and Jiye Yu.
2013.
Towardmore precision in correction of grammatical er-rors.
In Proceedings of the Seventeenth Conferenceon Computational Natural Language Learning:Shared Task, pages 68?73, Sofia, Bulgaria, August.Association for Computational Linguistics.Jennifer Foster and ?istein Andersen.
2009.Generrate: Generating errors for use in grammati-cal error detection.
In Proceedings of the FourthWorkshop on Innovative Use of NLP for BuildingEducational Applications, pages 82?90, Boulder,Colorado, June.
Association for ComputationalLinguistics.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1992.
One sense per discourse.In Proceedings of the workshop on Speechand Natural Language, HLT ?91, pages 233?237, Harriman, New York.
Association forComputational Linguistics.Kenji Imamura, Kuniko Saito, Kugatsu Sadamitsu, andHitoshi Nishikawa.
2012.
Grammar error correc-tion using pseudo-error sentences and domain adap-tation.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics(Volume 2: Short Papers), pages 388?392, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga,Thepchai Supnithi, and Hitoshi Isahara.
2003.Automatic Error Detection in the Japanese Learners?English Spoken Data.
In Proceedings of the 41stAnnual Meeting on Association for ComputationalLinguistics - Volume 2, ACL ?03, pages 145?148, Sapporo, Japan.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Prague, Czech Republic.Association for Computational Linguistics.Philipp Koehn.
2010.
Statistical Machine Translation.Cambridge University Press, New York, NY, USA,1st edition.Anoop Kunchukuttan, Ritesh Shah, and PushpakBhattacharyya.
2013.
Iitb system for conll 2013shared task: A hybrid approach to grammatical er-ror correction.
In Proceedings of the SeventeenthConference on Computational Natural LanguageLearning: Shared Task, pages 82?87, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morganand Claypool Publishers.John Lee and Stephanie Seneff.
2008.
An analysis ofgrammatical errors in non-native speech in English.In Amitava Das and Srinivas Bangalore, editors,Proceedings of the 2008 IEEE Spoken LanguageTechnology Workshop, SLT 2008, pages 89?92,Goa, India, December.
IEEE.Xiaohua Liu, Bo Han, Kuan Li, Stephan HyeonjunStiller, and Ming Zhou.
2010.
SRL-basedverb selection for ESL.
In Proceedings ofthe 2010 Conference on Empirical Methodsin Natural Language Processing, pages 1068?1076, Cambridge, MA, October.
Association forComputational Linguistics.George A. Miller.
1995.
WordNet: a lexicaldatabase for English.
Communications of the ACM,38(11):39?41, November.Tomoya Mizumoto, Mamoru Komachi, MasaakiNagata, and Yuji Matsumoto.
2011.
MiningRevision Log of Language Learning SNSfor Automated Japanese Error Correction ofSecond Language Learners.
In Proceedings of5th International Joint Conference on NaturalLanguage Processing, pages 147?155, Chiang Mai,Thailand, November.
Asian Federation of NaturalLanguage Processing.Hwee Tou Ng, Siew Mei Wu, Yuanbin Wu, ChristianHadiwinoto, and Joel Tetreault.
2013.
TheCoNLL-2013 Shared Task on Grammatical ErrorCorrection.
In Proceedings of the SeventeenthConference on Computational Natural LanguageLearning: Shared Task, pages 1?12, Sofia, Bulgaria,August.
Association for Computational Linguistics.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.125Ted Pedersen and Varada Kolhatkar.
2009.WordNet::SenseRelate::AllWords: a broad cover-age word sense tagger that maximizes semanticrelatedness.
In Proceedings of Human LanguageTechnologies: The 2009 Annual Conference of theNorth American Chapter of the Association forComputational Linguistics, Companion Volume:Demonstration Session, NAACL-Demonstrations?09, pages 17?20, Boulder, Colorado.
Associationfor Computational Linguistics.Desmond Darma Putra and Lili Szabo.
2013.Uds at conll 2013 shared task.
In Proceedingsof the Seventeenth Conference on ComputationalNatural Language Learning: Shared Task, pages88?95, Sofia, Bulgaria, August.
Association forComputational Linguistics.Alla Rozovskaya and Dan Roth.
2010a.
Trainingparadigms for correcting errors in grammar and us-age.
In Human Language Technologies: The 2010Annual Conference of the North American Chapterof the Association for Computational Linguistics,HLT ?10, pages 154?162, Los Angeles, California.Association for Computational Linguistics.Alla Rozovskaya and Dan Roth.
2010b.
Generatingconfusion sets for context-sensitive error correction.In Proceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 961?970, Cambridge, Massachusetts.Association for Computational Linguistics.Alla Rozovskaya and Dan Roth.
2011.
Algorithmselection and model adaptation for ESL correctiontasks.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics:Human Language Technologies - Volume 1, HLT?11, pages 924?933, Portland, Oregon.
Associationfor Computational Linguistics.Alla Rozovskaya, Mark Sammons, and Dan Roth.2012.
The UI system in the HOO 2012 shared taskon error correction.
In Proceedings of the SeventhWorkshop on Building Educational ApplicationsUsing NLP, pages 272?280, Montreal, Canada.Association for Computational Linguistics.Alla Rozovskaya, Kai-Wei Chang, Mark Sammons,and Dan Roth.
2013.
The University ofIllinois System in the CoNLL-2013 Shared Task.In Proceedings of the Seventeenth Conferenceon Computational Natural Language Learning:Shared Task, pages 13?19, Sofia, Bulgaria, August.Association for Computational Linguistics.Grigori Sidorov, Anubhav Gupta, Martin Tozer, DolorsCatala, Angels Catena, and Sandrine Fuentes.
2013.Rule-based system for automatic grammar correc-tion using syntactic n-grams for english languagelearning (l2).
In Proceedings of the SeventeenthConference on Computational Natural LanguageLearning: Shared Task, pages 96?101, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.Jonas Sj?obergh and Ola Knutsson.
2005.
Fakingerrors to avoid making errors: Very weakly su-pervised learning for error detection in writing.In Proceedings of RANLP 2005, pages 506?512,Borovets, Bulgaria, September.Toshikazu Tajiri, Mamoru Komachi, and YujiMatsumoto.
2012.
Tense and aspect error cor-rection for esl learners using global context.
InProceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics (Volume2: Short Papers), pages 198?202, Jeju Island, Korea,July.
Association for Computational Linguistics.Joel Tetreault, Daniel Blanchard, and Aoife Cahill.2013.
A Report on the First Native LanguageIdentification Shared Task.
In Proceedingsof the Eighth Workshop on Innovative Use ofNLP for Building Educational Applications, pages48?57, Atlanta, Georgia, June.
Association forComputational Linguistics.Antal van den Bosch and Peter Berck.
2013.
Memory-based grammatical error correction.
In Proceedingsof the Seventeenth Conference on ComputationalNatural Language Learning: Shared Task, pages102?108, Sofia, Bulgaria, August.
Association forComputational Linguistics.Bong-Jun Yi, Ho-Chang Lee, and Hae-Chang Rim.2013.
Kunlp grammatical error correction sys-tem for conll-2013 shared task.
In Proceedingsof the Seventeenth Conference on ComputationalNatural Language Learning: Shared Task, pages123?127, Sofia, Bulgaria, August.
Association forComputational Linguistics.Zheng Yuan and Mariano Felice.
2013.
Constrainedgrammatical error correction using statistical ma-chine translation.
In Proceedings of the SeventeenthConference on Computational Natural LanguageLearning: Shared Task, pages 52?61, Sofia,Bulgaria, August.
Association for ComputationalLinguistics.126
