The Necessity of Parsing for Predicate Argument RecognitionDaniel Gildea and Martha PalmerUniversity of Pennsylvaniadgildea,mpalmer@cis.upenn.eduAbstractBroad-coverage corpora annotated withsemantic role, or argument structure, in-formation are becoming available for therst time.
Statistical systems have beentrained to automatically label seman-tic roles from the output of statisticalparsers on unannotated text.
In this pa-per, we quantify the eect of parser accu-racy on these systems' performance, andexamine the question of whether aat-ter \chunked" representation of the in-put can be as eective for the purposesof semantic role identication.1 IntroductionOver the past decade, most work in the eld ofinformation extraction has shifted from complexrule-based, systems designed to handle a widevariety of semantic phenomena including quan-tication, anaphora, aspect and modality (e.g.Alshawi (1992)), to simpler nite-state or sta-tistical systems such as Hobbs et al (1997) andMiller et al (1998).
Much of the evaluation ofthese systems has been conducted on extractingrelations for specic semantic domains such ascorporate acquisitions or terrorist events in theframework of the DARPA Message Understand-ing Conferences.Recently, attention has turned to creating cor-pora annotated for argument structure for abroader range of predicates.
The Propbankproject at the University of Pennsylvania (Kings-bury and Palmer, 2002) and the FrameNet projectat the International Computer Science Institute(Baker et al, 1998) share the goal of document-ing the syntactic realization of arguments of thepredicates of the general English lexicon by an-notating a corpus with semantic roles.
Even fora single predicate, semantic arguments often havemultiple syntactic realizations, as shown by thefollowing paraphrases:(1) John will meet with Mary.John will meet Mary.John and Mary will meet.
(2) The door opened.Mary opened the door.Correctly identifying the semantic roles of thesentence constituents is a crucial part of interpret-ing text, and in addition to forming an importantpart of the information extraction problem, canserve as an intermediate step in machine trans-lation or automatic summarization.
In this pa-per, we examine how the information provided bymodern statistical parsers such as Collins (1997)and Charniak (1997) contributes to solving thisproblem.
We measure the eect of parser accu-racy on semantic role prediction from parse trees,and determine whether a complete tree is indeednecessary for accurate role prediction.Gildea and Jurafsky (2002) describe a statisti-cal system trained on the data from the FrameNetproject to automatically assign semantic roles.The system rst passed sentences through an au-tomatic parser, extracted syntactic features fromthe parses, and estimated probabilities for seman-tic roles from the syntactic and lexical features.Both training and test sentences were automat-ically parsed, as no hand-annotated parse treeswere available for the corpus.
While the errorsintroduced by the parser no doubt negatively af-fected the results obtained, there was no directway of quantifying this eect.
Of the systemsevaluated for the Message Understanding Confer-ence task, Miller et al (1998) made use of an inte-grated syntactic and semantic model producing afull parse tree, and achieved results comparable toother systems that did not make use of a completeparse.
As in the FrameNet case, the parser wasnot trained on the corpus for which semantic an-notations were available, and the eect of better,or even perfect, parses could not be measured.One of the dierences between the two semanticannotation projects is that the sentences chosenComputational Linguistics (ACL), Philadelphia, July 2002, pp.
239-246.Proceedings of the 40th Annual Meeting of the Association forfor annotation for Propbank are from the sameWall Street Journal corpus chosen for annotationfor the original Penn Treebank project, and thushand-checked syntactic parse trees are availablefor the entire dataset.
In this paper, we com-pare the performance of a system based on gold-standard parses with one using automatically gen-erated parser output.
We also examine whether itis possible that the additional information con-tained in a full parse tree is negated by the errorspresent in automatic parser output, by testing arole-labeling system based on aat or \chunked"representation of the input.2 The DataThe results in this paper are primarily derivedfrom the Propbank corpus, and will be comparedto earlier results from the FrameNet corpus.
Be-fore proceeding to the experiments, this sectionwill briey describe the similarities and dierencesbetween the two sets of data.While the goals of the two projects are similar inmany respects, their methodologies are quite dif-ferent.
FrameNet is focused on semantic frames,which are dened as schematic representation ofsituations involving various participants, props,and other conceptual roles (Fillmore, 1976).
Theproject methodology has proceeded on a frame-by-frame basis, that is by rst choosing a semanticframe, dening the frame and its participants orframe elements, and listing the various lexicalpredicates which invoke the frame, and then nd-ing example sentences of each predicate in the cor-pus (the British National Corpus was used) andannotating each frame element.
The example sen-tences were chosen primarily for coverage of allthe syntactic realizations of the frame elements,and simple examples of these realizations werepreferred over those involving complex syntacticstructure not immediate relevant to the lexicalpredicate itself.
From the perspective of an auto-matic classication system, the overrepresentationof rare syntactic realizations may cause the systemto perform more poorly than it might on more sta-tistically representative data.
On the other hand,the exclusion of complex examples may make thetask articially easy.
Only sentences where thelexical predicate was used \in frame" were anno-tated.
A word with multiple distinct senses wouldgenerally be analyzed as belonging to dierentframes in each sense, but may only be found in theFrameNet corpus in the sense for which a framehas been dened.
It is interesting to note that thesemantic frames are a helpful way of generalizingbetween predicates; words in the same frame havebeen found frequently to share the same syntacticargument structure.
A more complete descriptionof the FrameNet project can be found in (Bakeret al, 1998; Johnson et al, 2001), and the rami-cations for automatic classication are discussedmore thoroughly in (Gildea and Jurafsky, 2002).The philosophy of the Propbank project can belikened to FrameNet without frames.
While thesemantic roles of FrameNet are dened at the levelof the frame, in Propbank, roles are dened on aper-predicate basis.
The core arguments of eachpredicate are simply numbered, while remainingarguments are given labels such as \temporal" or\locative".
While the two types of label names arereminiscent of the traditional argument/adjunctdistinction, this is primarily as a convenience indening roles, and no claims are intended as tooptionality or other traditional argument/adjuncttests.
To date, Propbank has addressed onlyverbs, where FrameNet includes nouns and ad-jectives.
Propbank's annotation process has pro-ceeded from the most to least common verbs, andall examples of each verb from the corpus are an-notated.
Thus, the data for each predicate arestatistically representative of the corpus, as arethe frequencies of the predicates themselves.
An-notation takes place with reference to the PennTreebank trees | not only are annotators shownthe trees when analyzing a sentence, they are con-strained to assign the semantic labels to portionsof the sentence corresponding to nodes in the tree.Propbank annotators tag all examples of a givenverb, regardless of word sense.
The tagging guide-lines for a verb may contain many \rolesets", cor-responding to word sense at a relatively coarse-grained level.
The need for multiple rolesets isdetermined by the roles themselves, that is, usesof the verb with dierent arguments are given sep-arate rolesets.
However, the preliminary versionof the data used in the experiments below arenot tagged for word sense, or for the roleset used.Sense tagging is planned for a second pass throughthe data.
In many cases the roleset can be deter-mined from the argument annotations themselves.However, we did not make any attempt to distin-guish sense in our experiments, and simply at-tempted to predict argument labels based on theidentity of the lexical predicate.3 The ExperimentsIn previous work using the FrameNet corpus,Gildea and Jurafsky (2002) developed a system topredict semantic roles from sentences and theirparse trees as determined by the statistical parserof Collins (1997).
We will briey review theirprobability model before adapting the system tohandle unparsed data.Probabilities of a parse constituent belongingto a given semantic role were calculated from thefollowing features:Phrase Type: This feature indicates the syntac-tic type of the phrase expressing the semanticroles: examples include noun phrase (NP),verb phrase (VP), and clause (S).
Phrasetypes were derived automatically from parsetrees generated by the parser, as shown inFigure 1.
The parse constituent spanningeach set of words annotated as an argumentwas found, and the constituent's nonterminallabel was taken as the phrase type.
As anexample of how this feature is useful, in com-munication frames, the Speaker is likely toappear as a noun phrase, Topic as a prepo-sitional phrase or noun phrase, and Mediumas a prepositional phrase, as in: \We talkedabout the proposal over the phone."
Whenno parse constituent was found with bound-aries matching those of an argument duringtesting, the largest constituent beginning atthe argument's left boundary and lying en-tirely within the element was used to calcu-late the features.Parse Tree Path: This feature is designed tocapture the syntactic relation of a constituentto the predicate.
It is dened as the pathfrom the predicate through the parse treeto the constituent in question, representedas a string of parse tree nonterminals linkedby symbols indicating upward or downwardmovement through the tree, as shown in Fig-ure 2.
Although the path is composed as astring of symbols, our systems will treat thestring as an atomic value.
The path includes,as the rst element of the string, the part ofspeech of the predicate, and, as the last ele-ment, the phrase type or syntactic categoryof the sentence constituent marked as an ar-gument.Position: This feature simply indicates whetherthe constituent to be labeled occurs beforeor after the predicate dening the semanticframe.
This feature is highly correlated withgrammatical function, since subjects will gen-erally appear before a verb, and objects after.This feature may overcome the shortcom-ings of reading grammatical function from theparse tree, as well as errors in the parser out-put.SNP VPNPHe ate some pancakesPRPDT NNVBFigure 2: In this example, the path from the pred-icate ate to the argument He can be representedas VB"VP"S#NP, with " indicating upward move-ment in the parse tree and # downward movement.Voice: The distinction between active and pas-sive verbs plays an important role in the con-nection between semantic role and grammat-ical function, since direct objects of activeverbs correspond to subjects of passive verbs.From the parser output, verbs were classiedas active or passive by building a set of 10passive-identifying patterns.
Each of the pat-terns requires both a passive auxiliary (someform of \to be" or \to get") and a past par-ticiple.Head Word: Lexical dependencies provide im-portant information in labeling semanticroles, as one might expect from their usein statistical models for parsing.
Since theparser used assigns each constituent a headword as an integral part of the parsing model,the head words of the constituents can beread from the parser output.
For example, ina communication frame, noun phrases headedby \Bill", \brother", or \he" are more likelyto be the Speaker, while those headed by\proposal", \story", or \question" are morelikely to be the Topic.To predict argument roles in new data, wewish to estimate the probability of each rolegiven these ve features and the predicate p:P (rjpt; path; position; voice; hw; p).
Due to thesparsity of the data, it is not possible to estimatethis probability from the counts in the training.Instead, we estimate probabilities from varioussubsets of the features, and interpolate a linearcombination of the resulting distributions.
Theinterpolation is performed over the most specicdistributions for which data are available, whichcan be thought of as choosing the topmost distri-butions available from a backo lattice, shown inFigure 3.HePRPNPheardVBDthe sound of liquid slurping in a metal containerNPasINFarrellNNPNPapproachedVBDhimPRPNPfromINbehindNNNPPPVPSSBARVPSpredicate SourceGoalThemeFigure 1: A sample sentence with parser output (above) and argument structure annotation (below).Parse constituents corresponding to frame elements are highlighted.P(r | h)P(r | pt, voice)P(r | h, pt, p) P(r | pt, voice, p)P(r | pt, p)P(r | p)P(r | pt, path, p)P(r | h, p)Figure 3: Backo lattice with more specic distri-butions towards the top.We applied the same system, using the samefeatures to a preliminary release of the Propbankdata.
The dataset used contained annotations for26,138 predicate-argument structures containing65,364 individual arguments and containing exam-ples from 1,527 lexical predicates (types).
In orderto provide results comparable with the statisticalparsing literature, annotations from Section 23 ofthe Treebank were used as the test set; all othersections were included in the training set.The system was tested under two conditions,one in which it is given the constituents whichare arguments to the predicate and merely has topredict the correct role, and one in which it has toboth nd the arguments in the sentence and labelthem correctly.
Results are shown in Tables 1 and2.Although results for Propbank are lower thanfor FrameNet, this appears to be primarily due tothe smaller number of training examples for eachpredicate, rather than the dierence in annotationstyle between the two corpora.
The FrameNetdata contained at least ten examples from eachpredicate, while 17% of the Propbank data hadfewer than ten training examples.
Removing theseexamples from the test set gives 84.1% accuracywith gold-standard parses and 80.5% accuracywith automatic parses.As our path feature is a somewhat unusual wayof looking at parse trees, its behavior in the sys-tem warrants a closer look.
The path feature ismost useful as a way of nding arguments in theunknown boundary condition.
Removing the pathfeature from the known-boundary system resultsin only a small degradation in performance, from82.3% to 81.7%.
One reason for the relativelysmall impact may be sparseness of the feature |7% of paths in the test set are unseen in trainingdata.
The most common values of the feature areshown in Table 3, where the rst two rows cor-respond to standard subject and object positions.One reason for sparsity is seen in the third row:in the Treebank, the adjunction of an adverbialphrase or modal verb can cause an additional VPnode to appear in our path feature.
We tried twovariations of the path feature to address this prob-lem.
The rst collapses sequences of nodes withAccuracyFrameNet Propbank Propbank> 10 ex.Gold-standard parses 82.8 84.1Automatic parses 82.0 79.2 80.5Table 1: Accuracy of semantic role prediction for known boundaries | the system is given the con-stituents to classify.FrameNet Propbank Propbank > 10Precision Recall Precision Recall Precision RecallGold-standard parses 71.1 64.4 73.5 71.7Automatic parses 64.6 61.2 57.7 50.0 59.0 55.4Table 2: Accuracy of semantic role prediction for unknown boundaries | the system must identify thecorrect constituents as arguments and give them the correct roles.Path FrequencyVB"VP#NP 17:6%VB"VP"S#NP 16:4VB"VP"VP"S#NP 7:8VB"VP#PP 7:6VB"VP#PP#NP 7:3VB"VP#SBAR#S 4:3VB"VP#S 4:3VB"VP#ADVP 2:41031 others 76:0Table 3: Common values for parse tree path inPropbank data, using gold-standard parses.the same label, for example combining rows 2 and3 of Table 3.
The second variation uses only twovalues for the feature: NP under S (subject posi-tion), and NP under VP (object position).
Nei-ther variation improved performance in the knownboundary condition.
As a gauge of how closely thePropbank argument labels correspond to the pathfeature overall, we note that by always assigningthe most common role for each path, for examplealways assigning ARG0 to the subject position,and using no other features, we obtain the correctrole 69.4% of the time, vs. 82.3% for the completesystem.4 Is Parsing Necessary?Many recent information extraction systems forlimited domains have relied on nite-state systemsthat do not build a full parse tree for the sentencebeing analyzed.
Among such systems, (Hobbs etal., 1997) built nite-state recognizers for vari-ous entities, which were then cascaded to formrecognizers for higher-level relations, while (Rayand Craven, 2001) used low-level \chunks" froma general-purpose syntactic analyzer as observa-tions in a trained Hidden Markov Model.
Suchan approach has a large advantage in speed, asthe extensive search of modern statistical parsersis avoided.
It is also possible that this approachmay be more robust to error than parsers.
Al-though we expect the attachment decisions madeby a parser to be relevant to determining whethera constituent of a sentence is an argument of aparticular predicate, and what its relation to thepredicate is, those decisions may be so frequentlyincorrect that a much simpler system can do justas well.
In this section we test this hypothesisby comparing a system which is given only aat,\chunked" representation of the input sentence tothe parse-tree-based systems described above.
Inthis representation, base-level constituent bound-aries and labels are present, but there are no de-pendencies between constituents, as shown by thefollowing sample sentence:(3) [NPBig investment banks] [V Prefused tostep] [ADV Pup] [PPto] [NPthe plate][V Pto support] [NPthe beleagueredoortraders] [PPby] [V Pbuying] [NPbig blocks][PPof] [NPstock] , [NPtraders] [V Psay] .Our chunks were derived from the Tree-bank trees using the conversion described byTjong Kim Sang and Buchholz (2000).
Thus,the experiments were carried out using \gold-standard" rather than automatically derivedchunk boundaries, which we believe will providean upper bound on the performance of a chunk-based system.The information provided by the parse tree canbe decomposed into three pieces: the constituentboundaries, the grammatical relationship betweenpredicate and argument, expressed by our pathfeature, and the head word of each candidate con-stituent.
We will examine the contribution of eachof these information sources, beginning with theproblem of assigning the correct role in the casewhere the boundaries of the arguments in the sen-tence are known, and then turning to the problemof nding arguments in the sentence.When the argument boundaries are known, thegrammatical relationship of the the constituentto the predicate turns out to be of little value.Removing the path feature from the system de-scribed above results in only a small degradationin performance, from 82.3% to 81.7%.
While thepath feature serves to distinguish subjects fromobjects, the combination of the constituent po-sition before or after the predicate and the ac-tive/passive voice feature serves the same purpose.However, this result still makes use of the parseroutput for nding the constituent's head word.We implemented a simple algorithm to guess theargument's head word from the chunked output: ifthe argument begins at a chunk boundary, takingthe last word of the chunk, and in all other cases,taking the rst word of the argument.
This heuris-tic matches the head word read from the parse tree77% of the the time, as it correctly identies thenal word of simple noun phrases as the head, thepreposition as the head of prepositional phrases,and the complementizer as the head of sententialcomplements.
Using this process for determininghead words, the system drops to 77.0% accuracy,indicating that identifying the relevant head wordfrom semantic role prediction is in itself an impor-tant function of the parser.
This chunker-based re-sult is only slightly lower than the 79.2% obtainedusing automatic parses in the known boundarycondition.
These results for the known boundarycondition are summarized in Table 4.Path Head Accuracygold parse gold parse 82.3auto parse auto parse 79.2not used gold parse 81.7not used chunks 77.0Table 4: Summary of results for known boundaryconditionWe might expect the information provided bythe parser to be more important in identifying thearguments in the sentence than in assigning themthe correct role.
While it is easy to guess whethera noun phrase is a subject or object given onlyits position relative to the predicate, identifyingcomplex noun phrases and determining whetherthey are arguments of a verb may be more di?cultwithout the attachment information provided bythe parser.To test this, we implemented a system in whichthe argument labels were assigned to chunks, withthe path feature used by the parse-tree-based sys-tem replaced by a number expressing the distancein chunks to the left or right of the predicate.Of the 3990 arguments in our test set, only39.8% correspond to a single chunk in theat-tened sentence representation, giving an upperbound to the performance of this system.
In par-ticular, sentential complements (which comprise11% of the data) and prepositional phrases (whichcomprise 10%) always correspond to more thanone chunk, and therefore cannot be correctly la-beled by our system which assigns roles to singlechunks.
In fact, this system achieves 27.6% preci-sion and 22.0% recall.In order to see how much of the performancedegradation is caused by the di?culty of ndingexact argument boundaries in the chunked rep-resentation, we can relax the scoring criteria tocount as correct all cases where the system cor-rectly identies the rst chunk belonging to anargument.
For example, if the system assigns thecorrect label to the preposition beginning a prepo-sitional phrase, the argument will be counted ascorrect, even though the system does not ndthe argument's righthand boundary.
With thisscoring regime, the chunk-based system performsat 49.5% precision and 35.1% recall, still signi-cantly lower than the 57.7% precision/50.0% recallfor exact matches using automatically generatedparses.
Results for the unknown boundary condi-tion are summarized in Table 5.Precision Recallgold parse 71.1 64.4auto parse 57.7 50.0chunk 27.6 22.0chunk, relaxed scoring 49.5 35.1Table 5: Summary of results for unknown bound-ary conditionAs an example for comparing the behavior ofthe tree-based and chunk-based systems, considerthe following sentence, with human annotationsshowing the arguments of the predicate support:(4) [ARG0Big investment banks] refused to stepup to the plate to support [ARG1thebeleagueredoor traders] [MNRby buyingbig blocks of stock] , traders say .Our tree-based system assigned the following anal-ysis:(5) Big investment banks refused to step up tothe plate to support [ARG1the beleagueredoor traders] [MNRby buying big blocks ofstock] , traders say .In this case, the system failed to nd the predi-cate's ARG0 relation, because it is syntacticallydistant from the verb support.
The original Tree-bank syntactic tree contains a trace which wouldallow one to recover this relation, co-indexing theempty subject position of support with the nounphrase \Big investment banks".
However, ourautomatic parser output does not include suchtraces, nor does our system make use of them.The chunk-based system assigns the following ar-gument labels:(6) Big investment banks refused to step up to[ARG0the plate] to support [ARG1thebeleagueredoor traders] by buying bigblocks of stock , traders say .Here, as before, the true ARG0 relation is notfound, and it would be di?cult to imagine iden-tifying it without building a complete syntacticparse of the sentence.
But now, unlike in thetree-based output, the ARG0 label is mistakenlyattached to a noun phrase immediately before thepredicate.
The ARG1 relation in direct object po-sition is fairly easily identiable in the chunkedrepresentation as a noun phrase directly follow-ing the verb.
The prepositional phrase expressingthe Manner relation, however, is not identied bythe chunk-based system.
The tree-based system'spath feature for this constituent is VB"VP#PP,which identies the prepositional phrase as at-taching to the verb, and increases its probabilityof being assigned an argument label.
The chunk-based system sees this as a prepositional phraseappearing as the second chunk after the predi-cate.
Although this may be a typical position forthe Manner relation, the fact that the prepositionattaches to the predicate rather than to its directobject is not represented.In interpreting these results, it is important tokeep in mind the dierences between this taskand other information extraction datasets.
Incomparison to the domain-specic relations eval-uated by the Message Understanding Conference(MUC) tasks, we have a wider variety of relationsbut fewer training instances for each.
The rela-tions may themselves be less susceptible to nitestate methods.
For example, a named-entity sys-tem which indenties corporation names can goa long way towards nding the \employment" re-lation of MUC, and similarly systems for tagginggenes and proteins help a great deal for relationsin the biomedical domain.
Both Propbank andFrameNet tend to include longer arguments withinternal syntactic structure, making parsing deci-sions more important in nding argument bound-aries.
They also involve abstract relations, with awide variety of possible llers for each role.5 ConclusionOur chunk-based system takes the last word ofthe chunk as its head word for the purposes ofpredicting roles, but does not make use of theidentities of the chunk's other words or the inter-vening words between a chunk and the predicate,unlike Hidden Markov Model-like systems such asBikel et al (1997), McCallum et al (2000) andLaerty et al (2001).
While a more elaboratenite-state system might do better, it is possiblethat additional features would not be helpful giventhe small amount of data for each predicate.
Byusing a gold-standard chunking representation, wehave obtained higher performance over what couldbe expected from an entirely automatic systembased on aat representation of the data.We feel that our results show that statisticalparsers, although computationally expensive, doa good job of providing relevant information forsemantic interpretation.
Not only the constituentstructure but also head word information, pro-duced as a side product, are important features.Parsers, however, still have a long way to go.Our results using hand-annotated parse trees showthat improvements in parsing should translate di-rectly into better semantic interpretations.Acknowledgments This work was undertakenwith funding from the Institute for Research inCognitive Science at the University of Pennsylva-nia and from the Propbank project, DoD GrantMDA904-00C-2136.ReferencesHiyan Alshawi, editor.
1992.
The Core LanguageEngine.
MIT Press, Cambridge, MA.Collin F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The Berkeley FrameNet project.In Proceedings of COLING/ACL, pages 86{90,Montreal, Canada.D.
M. Bikel, S. Miller, R. Schwartz, andR.
Weischedel.
1997.
Nymble: a high-performance learning name-nder.
In Proceed-ings of the Fifth Conference on Applied NaturalLanguage Processing.Eugene Charniak.
1997.
Statistical parsing witha context-free grammar and word statistics.
InAAAI-97, pages 598{603, Menlo Park, August.AAAI Press.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of the 35th Annual Meeting of the ACL,pages 16{23, Madrid, Spain.Charles J. Fillmore.
1976.
Frame semanticsand the nature of language.
In Annals of theNew York Academy of Sciences: Conference onthe Origin and Development of Language andSpeech, volume 280, pages 20{32.Daniel Gildea and Daniel Jurafsky.
2002.
Auto-matic labeling of semantic roles.
ComputationalLinguistics, in press.Jerry R. Hobbs, Douglas Appelt, John Bear,David Israel, Megumi Kameyama, Mark E.Stickel, and Mabry Tyson.
1997.
FASTUS:A cascaded nite-state transducer for extract-ing information from natural-language text.
InEmmanuel Roche and Yves Schabes, editors,Finite-State Language Processing, pages 383{406.
MIT Press, Cambridge, MA.Christopher R. Johnson, Charles J. Fillmore,Esther J.
Wood, Josef Ruppenhofer, Mar-garet Urban, Miriam R. L. Petruk, andCollin F. Baker.
2001.
The FrameNetproject: Tools for lexicon building.
Version 0.7,http://www.icsi.berkeley.edu/~framenet/book.html.Paul Kingsbury and Martha Palmer.
2002.
FromTreebank to PropBank.
In Proceedings of the3rd International Conference on Language Re-sources and Evaluation (LREC-2002), Las Pal-mas, Canary Islands, Spain.John Laerty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random elds:Probabilistic models for segmenting and label-ing sequence data.
In Machine Learning: Pro-ceedings of the Eighteenth International Confer-ence (ICML 2001), Stanford, California.Andrew McCallum, Dayne Freitag, and FernandoPereira.
2000.
Maximum entropy Markov mod-els for information extraction and segmenta-tion.
In Machine Learning: Proceedings of theSeventeenth International Conference (ICML2000), pages 591{598, Stanford, California.Scott Miller, Michael Crystal, Heidi Fox, LanceRamshaw, Richard Schwartz, Rebecca Stone,Ralph Weischedel, and the Annotation Group.1998.
Algorithms that learn to extract informa-tion { BBN: Description of the SIFT system asused for MUC-7.
In Proceedings of the SeventhMessage Understanding Conference (MUC-7),April.Soumya Ray and Mark Craven.
2001.
Represent-ing sentence structure in hidden markov modelfor information extraction.
In Seventeenth In-ternational Joint Conference on Articial Intel-ligence (IJCAI-01), Seattle, Washington.Erik F. Tjong Kim Sang and Sabine Buchholz.2000.
Introduction to the conll-2000 sharedtask: Chunking.
In Proceedings of CoNLL-2000and LLL-2000, Lisbon, Portugal.
