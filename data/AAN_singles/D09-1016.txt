Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 151?160,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPA Unified Model of Phrasal and Sentential Evidencefor Information ExtractionSiddharth Patwardhan and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{sidd,riloff}@cs.utah.eduAbstractInformation Extraction (IE) systems thatextract role fillers for events typically lookat the local context surrounding a phrasewhen deciding whether to extract it.
Of-ten, however, role fillers occur in clausesthat are not directly linked to an eventword.
We present a new model for eventextraction that jointly considers both thelocal context around a phrase along withthe wider sentential context in a proba-bilistic framework.
Our approach uses asentential event recognizer and a plausiblerole-filler recognizer that is conditioned onevent sentences.
We evaluate our systemon two IE data sets and show that ourmodel performs well in comparison to ex-isting IE systems that rely on local phrasalcontext.1 IntroductionInformation Extraction (IE) systems typically useextraction patterns (e.g., Soderland et al (1995),Riloff (1996), Yangarber et al (2000), Califfand Mooney (2003)) or classifiers (e.g., Freitag(1998), Freitag and McCallum (2000), Chieu et al(2003), Bunescu and Mooney (2004)) to extractrole fillers for events.
Most IE systems consideronly the immediate context surrounding a phrasewhen deciding whether to extract it.
For tasks suchas named entity recognition, immediate context isusually sufficient.
But for more complex tasks,such as event extraction, a larger field of view isoften needed to understand how facts tie together.Most IE systems are designed to identify rolefillers that appear as arguments to event verbsor nouns, either explicitly via syntactic relationsor implicitly via proximity (e.g., John murderedTom or the murder of Tom by John).
But manyfacts are presented in clauses that do not containevent words, requiring discourse relations or deepstructural analysis to associate the facts with eventroles.
For example, consider the sentences below:Seven people have died.
.
.
and 30 were injured in India after terror-ists launched an attack on the Taj Hotel.. .
.
in Mexico City and its surrounding sub-urbs in a Swine Flu outbreak.. .
.
after a tractor-trailer collided with a busin Arkansas.Two bridges were destroyed.
.
.
in Baghdad last night in a resurgence ofbomb attacks in the capital city.. .
.
and $50 million in damage was caused bya hurricane that hit Miami on Friday.. .
.
to make way for modern, safer bridgesthat will be constructed early next year.These examples illustrate a common phenomenonin text where information is not explicitly statedas filling an event role, but readers have no trou-ble making this inference.
The role fillers above(seven people, two bridges) occur as arguments toverbs that reveal state information (death, destruc-tion) but are not event-specific (i.e., death and de-struction can result from a wide variety of incidenttypes).
IE systems often fail to extract these rolefillers because these systems do not recognize theimmediate context as being relevant to the specifictype of event that they are looking for.We propose a new model for information ex-traction that incorporates both phrasal and senten-tial evidence in a unified framework.
Our uni-fied probabilistic model, called GLACIER, consistsof two components: a model for sentential eventrecognition and a model for recognizing plausi-ble role fillers.
The Sentential Event Recognizeroffers a probabilistic assessment of whether a sen-tence is discussing a domain-relevant event.
The151Plausible Role-Filler Recognizer is then condi-tioned to identify phrases as role fillers based uponthe assumption that the surrounding context is dis-cussing a relevant event.
This unified probabilisticmodel allows the two components to jointly makedecisions based upon both the local evidence sur-rounding each phrase and the ?peripheral vision?afforded by the sentential event recognizer.This paper is organized as follows.
Section2 positions our research with respect to relatedwork.
Section 3 presents our unified probabilisticmodel for information extraction.
Section 4 showsexperimental results on two IE data sets, and Sec-tion 5 discusses directions for future work.2 Related WorkMany event extraction systems rely heavily on thelocal context around words or phrases that are can-didates for extraction.
Some systems use extrac-tion patterns (Soderland et al, 1995; Riloff, 1996;Yangarber et al, 2000; Califf and Mooney, 2003),which represent the immediate contexts surround-ing candidate extractions.
Similarly, classifier-based approaches (Freitag, 1998; Freitag and Mc-Callum, 2000; Chieu et al, 2003; Bunescu andMooney, 2004) rely on features in the immedi-ate context of the candidate extractions.
Our workseeks to incorporate additional context into IE.Indeed, several recent approaches have shownthe need for global information to improve IE per-formance.
Maslennikov and Chua (2007) use dis-course trees and local syntactic dependencies ina pattern-based framework to incorporate widercontext.
Finkel et al (2005) and Ji and Grish-man (2008) incorporate global information by en-forcing event role or label consistency over a doc-ument or across related documents.
In contrast,our approach simply creates a richer IE model forindividual extractions by expanding the ?field ofview?
to include the surrounding sentence.The two components of the unified model pre-sented in this paper are somewhat similar to ourprevious work (Patwardhan and Riloff, 2007),where we employ a relevant region identificationphase prior to pattern-based extraction.
In thatwork we adopted a pipeline paradigm, where aclassifier identifies relevant sentences and onlythose sentences are fed to the extraction module.Our unified probabilistic model described in thispaper does not draw a hard line between rele-vant and irrelevant sentences, but gently balancesthe influence of both local and sentential contextsthrough probability estimates.3 A Unified IE Model that CombinesPhrasal and Sentential EvidenceWe introduce a probabilistic model for event-based IE that balances the influence of two kindsof contextual information.
Our goal is to createa model that has the flexibility to make extractiondecisions based upon strong evidence from the lo-cal context, or strong evidence from the wider con-text coupled with a more general local context.
Forexample, some phrases explicitly refer to an event,so they almost certainly warrant extraction regard-less of the wider context (e.g., terrorists launchedan attack).1 In contrast, some phrases are poten-tially relevant but too general to warrant extrac-tion on their own (e.g., people died could be theresult of different incident types).
If we are confi-dent that the sentence discusses an event of inter-est, however, then such phrases could be reliablyextracted.Our unified model for IE (GLACIER) combinestwo types of contextual information by incorpo-rating it into a probabilistic framework.
To deter-mine whether a noun phrase instance NPishouldbe extracted as a filler for an event role, GLACIERcomputes the joint probability that NPi:(1) appears in an event sentence, and(2) is a legitimate filler for the event role.Thus, GLACIER is designed for noun phrase ex-traction and, mathematically, its decisions arebased on the following joint probability:P (EvSent(SNPi),PlausFillr(NPi))where SNPiis the sentence containing noun phraseNPi.
This probability estimate is based on con-textual features F appearing within SNPiand inthe local context of NPi.
Including F in the jointprobability, and applying the product rule, we cansplit our probability into two components:P (EvSent(SNPi),PlausFillr(NPi)|F ) =P (EvSent(SNPi)|F )?
P (PlausFillr(NPi)|EvSent(SNPi), F )These two probability components, in the expres-sion above, form the basis of the two modules in1There are always exceptions of course, such as hypothet-ical statements, but they are relatively uncommon.152our IE system ?
the sentential event recognizer andthe plausible role-filler recognizer.
In arriving ata decision to extract a noun phrase, our unifiedmodel for IE uses these modules to estimate thetwo probabilities based on the set of contextualfeatures F .
Note that having these two probabilitycomponents allows the system to gently balancethe influence from the sentential and phrasal con-texts, without having to make hard decisions aboutsentence relevance or phrases in isolation.In this system, the sentential event recog-nizer is embodied in the probability compo-nent P (EvSent(SNPi)|F ).
This is essentiallythe probability of a sentence describing a rel-evant event.
Similarly, the plausible role-filler recognizer is embodied by the probabil-ity P (PlausFillr(NPi)|EvSent(SNPi), F ).
Thiscomponent, therefore, estimates the probabilitythat a noun phrase fills a specific event role, as-suming that the noun phrase occurs in an eventsentence.
Many different techniques could be usedto produce these probability estimates.
In the restof this section, we present the specific models thatwe used for each of these components.3.1 Plausible Role-Filler RecognizerThe plausible role-filler recognizer is similar tomost traditional IE systems, where the goal is todetermine whether a noun phrase can be a legiti-mate filler for a specific type of event role based onits local context.
Pattern-based approaches matchthe context surrounding a phrase using lexico-syntactic patterns or rules.
However, most of theseapproaches do not produce probability estimatesfor the extractions.
Classifier-based approachesuse machine learning classifiers to make extrac-tion decisions, based on features associated withthe local context.
Any classifier that can generateprobability estimates, or similar confidence val-ues, could be plugged into our model.In our work, we use a Na?
?ve Bayes classifier asour plausible role-filler recognizer.
The probabili-ties are computed using a generative Na?
?ve Bayesframework, based on local contextual features sur-rounding a noun phrase.
These clues include lexi-cal matches, semantic features, and syntactic rela-tions, and will be described in more detail in Sec-tion 3.3.
The Na?
?ve Bayes (NB) plausible role-filler recognizer is defined as follows:P (PlausFillr(NPi)|EvSent(SNPi), F ) =1ZP (PlausFillr(NPi)|EvSent(SNPi)) ?
?fi?FP (fi|PlausFillr(NPi),EvSent(SNPi))where F is the set of local contextual featuresand Z is the normalizing constant.
The priorP (PlausFillr(NPi)|EvSent(SNPi)) is estimatedfrom the fraction of role fillers in the training data.The product term in the equation is the likelihood,which makes the simplifying assumption that allof the features in F are independent of one an-other.
It is important to note that these probabil-ities are conditioned on the noun phrase NPiap-pearing in an event sentence.Most IE systems need to extract several differ-ent types of role fillers for each event.
For in-stance, to extract information about terrorist inci-dents a system may extract the names of perpetra-tors, victims, targets, and weapons.
We create aseparate IE model for each type of event role.
Toconstruct a unified IE model for an event role, wemust specifically create a plausible role-filler rec-ognizer for that event role, but we can use a singlesentential event recognizer for all of the role fillertypes.3.2 Sentential Event RecognizerThe task at hand for the sentential event recognizeris to analyze features in a sentence and estimatethe probability that the sentence is discussing arelevant event.
This is very similar to the task per-formed by text classification systems, with someminor differences.
Firstly, we are dealing withthe classification of sentences, as opposed to en-tire documents.
Secondly, we need to generate aprobability estimate of the ?class?, and not justa class label.
Like the plausible role-filler recog-nizer, here too we employ machine learning clas-sifiers to estimate the desired probabilities.3.2.1 Na?
?ve Bayes Event RecognizerSince Na?
?ve Bayes classifiers estimate class prob-abilities, we employ such a classifier to create asentential event recognizer:P (EvSent(SNPi)|F ) =1ZP (EvSent(SNPi))?
?fi?FP (fi|EvSent(SNPi))where Z is the normalizing constant and F is theset of contextual features in the sentence.
The153prior P (EvSentS(NPi)) is obtained from the ra-tio of event and non-event sentences in the train-ing data.
The product term in the equation is thelikelihood, which makes the simplifying assump-tion that the features in F are independent of oneanother.
The features used by the model will bedescribed in Section 3.3.A known issue with Na?
?ve Bayes classifiers isthat, even though their classification accuracy isoften quite reasonable, their probability estimatesare often poor (Domingos and Pazzani, 1996;Zadrozny and Elkan, 2001; Manning et al, 2008).The problem is that these classifiers tend to overes-timate the probability of the predicted class, result-ing in a situation where most probability estimatesfrom the classifier tend to be either extremely closeto 0.0 or extremely close to 1.0.
We observed thisproblem in our classifier too, so we decided to ex-plore an additional model to estimate probabilitiesfor the sentential event recognizer.
This secondmodel, based on SVMs, is described next.3.2.2 SVM Event RecognizerGiven the all-or-nothing nature of the probabilityestimates that we observed from the Na?
?ve Bayesmodel, we decided to try using a Support VectorMachine (SVM) (Vapnik, 1995; Joachims, 1998)classifier as an alternative to Na?
?ve Bayes.
Oneof the issues with doing this is that SVMs are notprobabilistic classifiers.
SVMsmake classificationdecisions using on a decision boundary defined bysupport vectors identified during training.
A deci-sion function is applied to unseen test examplesto determine which side of the decision bound-ary those examples lie.
While the values obtainedfrom the decision function only indicate class as-signments for the examples, we used these val-ues to produce confidence scores for our sententialevent recognizer.To produce a confidence score from the SVMclassifier, we take the values generated by the deci-sion function for each test instance and normalizethem based on the minimum and maximum valuesproduced across all of the test instances.
This nor-malization process produces values between 0 and1 that we use as a rough indicator of the confidencein the SVM?s classification.
We observed that wecould effect a consistent recall/precision trade-offby using these values as thresholds for classifica-tion decisions, which suggests that this approachworked reasonably well for our task.3.3 Contextual FeaturesWe used a variety of contextual features in bothcomponents of our system.
The plausible role-filler recognizer uses the following types of fea-tures for each candidate noun phrase NPi: lexicalhead ofNPi, semantic class ofNPi?s lexical head,named entity tags associated with NPiand lexico-syntactic patterns that represent the local contextsurrounding NPi.
The feature set is automaticallygenerated from the texts.
Each feature is assigneda binary value for each instance, indicating eitherthe presence or absence of the feature.The named-entity features are generated by thefreely available Stanford NER tagger (Finkel etal., 2005).
We use the pre-trained NER modelthat comes with the software to identify person,organization and location names.
The syntac-tic and semantic features are generated by theSundance/AutoSlog system (Riloff and Phillips,2004).
We use the Sundance shallow parser toidentify lexical heads, and use its semantic dictio-naries to assign semantic features to words.
TheAutoSlog pattern generator (Riloff, 1996) is usedto create the lexico-syntactic pattern features thatcapture local context around each noun phrase.Our training sets produce a very large numberof features, which initially bogged down our clas-sifiers.
Consequently, we reduced the size of thefeature set by discarding all features that appearedfour times or less in the training set.Our sentential event recognizer uses the samecontextual features as the plausible role-filler rec-ognizer, except that features are generated forevery NP in the sentence.
In addition, it usesthree types of sentence-level features: sentencelength, bag of words, and verb tense, which arealso binary features.
We have two binary sentencelength features indicating that the sentence is long(greater than 35 words) or is short (shorter than 5words).
Additionally, all of the words in each sen-tence in the training data are generated as bag ofwords features for the sentential model.
Finally,we generate verb tense features from all verbs ap-pearing in each sentence.
Here too we apply a fre-quency cutoff and eliminate all features that ap-pear four times or less in the training data.4 IE Evaluation4.1 Data SetsWe evaluated the performance of our IE system ontwo data sets: the MUC-4 terrorism corpus (Sund-154heim, 1992), and a ProMed disease outbreaks cor-pus (Phillips and Riloff, 2007; Patwardhan andRiloff, 2007).
The MUC-4 data set is a standardIE benchmark collection of news stories about ter-rorist events.
It contains 1700 documents dividedinto 1300 development (DEV) texts, and four testsets of 100 texts each (TST1, TST2, TST3, andTST4).
Unless otherwise stated, our experimentsadopted the same training/test split used in pre-vious research: the 1300 DEV texts for training,200 texts (TST1+TST2) for tuning, and 200 texts(TST3+TST4) as the blind test set.
We evaluatedour system on five MUC-4 string roles: perpetra-tor individuals, perpetrator organizations, physi-cal targets, victims, and weapons.The ProMed corpus consists of 120 documentsobtained from ProMed-mail2, a freely accessibleglobal electronic reporting system for outbreaksof diseases.
These 120 documents are paired withcorresponding answer key templates.
Unless oth-erwise noted, all of our experiments on this dataset used 5-fold cross validation.
We extracted twotypes of event roles: diseases and victims3.Unlike some other IE data sets, many of thetexts in these collections do not describe a rele-vant event.
Only about half of the MUC-4 arti-cles describe a specific terrorist incident4, and onlyabout 80% of the ProMed articles describe a dis-ease outbreak.
The answer keys for the irrelevantdocuments are therefore empty.
IE systems are es-pecially susceptible to false hits when they can begiven texts that contain no relevant events.The complete IE task involves the creation ofanswer key templates, one template per incident(many documents in our data sets describe multi-ple events).
Our work focuses on accurately ex-tracting the facts from the text and not on tem-plate generation per se (e.g., we are not concernedwith coreference resolution or which extractionbelongs in which template).
Consequently, our ex-periments evaluate the accuracy of the extractionsindividually.
We used head noun scoring, wherean extraction is considered to be correct if its headnoun matches the head noun in the answer key.52http://www.promedmail.org3The ?victims?
can be people, animals, or plants.4With respect to the definition of terrorist incidents in theMUC-4 guidelines (Sundheim, 1992).5Pronouns were discarded from both the system responsesand the answer keys since we do not perform coreference res-olution.
Duplicate extractions (e.g., the same string extractedmultiple times from the same document) were conflated be-fore being scored, so they count as just one hit or one miss.4.2 BaselinesWe generated three baselines to use as compar-isons with our IE system.
As our first baseline,we used AutoSlog-TS (Riloff, 1996), which is aweakly-supervised, pattern-based IE system avail-able as part of the Sundance/AutoSlog softwarepackage (Riloff and Phillips, 2004).
Our previouswork in event-based IE (Patwardhan and Riloff,2007) also used a pattern-based approach that ap-plied semantic affinity patterns to relevant regionsin text.
We use this system as our second base-line.
As a third baseline, we trained a Na?
?ve BayesIE classifier that is analogous to the plausible role-filler recognizer in our unified IE model, exceptthat this baseline system is not conditioned on theassumption of having an event sentence.
Conse-quently, this baseline NB classifier is akin to a tra-ditional supervised learning-based IE system thatuses only local contextual features to make extrac-tion decisions.
Formally, the baseline NB classi-fier uses the formula:P (PlausFillr(NPi)|F ) =1ZP (PlausFillr(NPi))?
?fi?FP (fi|PlausFillr(NPi))where F is the set of local features,P (PlausFillr(NPi)) is the prior probability,and Z is the normalizing constant.
We used theWeka (Witten and Frank, 2005) implementationof Na?
?ve Bayes for this baseline NB system.New Jersey, February, 26.
An outbreak of Ebola hasbeen confirmed in Mercer County, New Jersey.
Five teenageboys appear to have contracted the deadly virus from anunknown source.
The CDC is investigating the cases and istaking measures to prevent the spread.
.
.Disease: EbolaVictims: Five teenage boysLocation: Mercer County, New JerseyDate: February 26Figure 1: A Disease Outbreak Event TemplateBoth the MUC-4 and ProMed data sets haveseparate answer keys rather than annotated sourcedocuments.
Figure 1 shows an example of a doc-ument and its corresponding answer key template.To train the baseline NB system, we identify allinstances of each answer key string in the sourcedocument and consider every instance a positivetraining example.
This produces noisy trainingdata, however, because some instances occur in155PerpInd PerpOrg Target Victim WeaponP R F P R F P R F P R F P R FAutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50NB .50 .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10NB .70 .41 .25 .31 .43 .31 .36 .58 .42 .48 .58 .37 .45 1.00 .04 .07NB .90 .51 .17 .25 .56 .15 .24 .67 .30 .41 .75 .23 .36 1.00 .02 .04Table 1: Baseline Results on MUC-4Disease VictimP R F P R FAutoSlog-TS .33 .60 .43 .36 .49 .41Sem Affinity .31 .49 .38 .41 .47 .44NB .50 .20 .73 .31 .29 .56 .39NB .70 .23 .67 .34 .37 .52 .44NB .90 .34 .59 .43 .47 .39 .43Table 2: Baseline Results on ProMedundesirable contexts.
For example, if the string?man?
appears in an answer key as a victim, oneinstance of ?man?
may refer to the actual vic-tim in an event sentence, while another instanceof ?man?
may occur in a non-event context (e.g.,background information) or may refer to a com-pletely different person.We report three evaluation metrics in our exper-iments: precision (P), recall (R), and F-score (F),where recall and precision are equally weighted.For the Na?
?ve Bayes classifier, the natural thresh-old for distinguishing between positive and nega-tive classes is 0.5, but we also evaluated this clas-sifier with thresholds of 0.7 and 0.9 to see if wecould effect a recall/precision trade-off.
Tables 1and 2 present the results of our three baseline sys-tems.
The NB classifier performs comparably toAutoSlog-TS and Semantic Affinity on most eventroles, although a threshold of 0.90 is needed toreach comparable performance on ProMed.
Therelatively low numbers across the board indicatethat these corpora are challenging, but these re-sults suggest that our plausible role-filler recog-nizer is competitive with other existing IE sys-tems.
In Section 4.4 we will show how our unifiedIE model compares to these baselines.
But beforethat (in the next section) we evaluate the quality ofthe second component of our IE system: the sen-tential event recognizer.4.3 Sentential Event Recognizer ModelsThe sentential event recognizer is one of the corecontributions of this research, so in this section weevaluate it by itself, before we employ it within theunified framework.
The purpose of the sententialevent recognizer is to determine whether a sen-tence is discussing a domain-relevant event.
Forour data sets, the classifier must decide whether asentence is discussing a terrorist incident (MUC-4) or a disease outbreak (ProMed).
Ideally, wewant such a classifier to operate independentlyfrom the answer keys and the extraction task perse.
For example, a terrorism IE system could bedesigned to extract only perpetrators and victimsof terrorist events, or it could be designed to ex-tract only targets and locations.
The job of the sen-tential event recognizer remains the same: to iden-tify sentences that discuss a terrorist event.
How totrain and evaluate such a system is a difficult ques-tion.
In this section, we present two approachesthat we explored to generate the training data: (a)using the IE answer keys, and (b) using humanjudgements.4.3.1 Sentence Annotation via Answer KeysWe have argued that the event relevance of a sen-tence should not be tied to a specific set of eventroles.
However, the IE answer keys can be usedto identify some sentences that describe an event,because they contain an answer string.
So we canmap the answer strings back to sentences in thesource documents to automatically generate eventsentence annotations.6 These annotations will benoisy, though, because an answer string can appearin a non-event sentence, and some event sentencesmay not contain any answer strings.
The alterna-tive, however, is sentence annotations by humans,which (as we will discuss in Section 4.3.2) is chal-lenging.4.3.2 Sentence Annotation via HumanJudgementsFor many sentences there is a clear consensusamong people that an event is being discussed.
Forexample, most readers would agree that sentence(1) below is describing a terrorist event, while sen-6A similar strategy was used in previous work (Patward-han and Riloff, 2007) to generate a test set for the evaluationof a relevant region classifier.156Evaluation on Answer Keys Evaluation on Human AnnotationsEvent Non-Event Event Non-EventAcc Pr Rec F Pr Rec F Acc Pr Rec F Pr Rec FMUC-4 (Terrorism)Ans NB .80 .57 .55 .56 .86 .87 .87 .81 .46 .60 .52 .91 .85 .88SVM .80 .68 .42 .52 .84 .93 .88 .83 .55 .44 .49 .88 .91 .90Hum NB .82 .64 .48 .55 .85 .92 .88 .85 .56 .57 .57 .91 .91 .91SVM .79 .64 .41 .50 .83 .91 .87 .84 .62 .51 .56 .90 .91 .91ProMed (Disease Outbreaks)Ans NB .75 .62 .61 .61 .81 .82 .82 .72 .43 .58 .50 .86 .77 .81SVM .74 .78 .31 .44 .74 .95 .83 .76 .51 .26 .35 .80 .92 .86Hum NB .73 .61 .46 .52 .77 .86 .81 .79 .56 .57 .56 .87 .86 .86SVM .70 .62 .32 .42 .73 .89 .81 .79 .62 .42 .50 .84 .90 .87Table 3: Sentential Event Recognizers Results (5-fold Cross-Validation)Evaluation on Human AnnotationsEvent Non-EventAcc Pr Rec F Pr Rec FNB .83 .50 .70 .58 .94 .86 .90SVM .89 .83 .39 .53 .89 .98 .94Table 4: Sentential Event Recognizer Results forMUC-4 using 1300 Documents for Trainingtence (2) is not.
However it is difficult to draw aclear line.
Sentence (3), for example, describes anaction taken in response to a terrorist event.
Is thisa terrorist event sentence?
Precisely how to definean event sentence is not obvious.
(1) Al Qaeda operatives launched an at-tack on the Madrid subway system.
(2) Madrid has a population of about3.2 million people.
(3) City officials stepped up security inresponse to the attacks.We tackled this issue by creating detailed an-notation guidelines to define the notion of anevent sentence, and conducting a human annota-tion study.
The guidelines delineated a generaltime frame for the beginning and end of an event,and constrained the task to focus on specific inci-dents that were reported in the IE answer key.
Wegave the annotators a brief description (e.g., mur-der in Peru) of each event that had a filled answerkey in the data set.
They only labeled sentencesthat discussed those particular events.We employed two human judges, who anno-tated 120 documents from the ProMed test set,and 100 documents from the MUC-4 test set.
Weasked both judges to label 30 of the same docu-ments from each data set so that we could computeinter-annotator agreement.
The annotators had anagreement of 0.72 Cohen?s ?
on the ProMed data,and 0.77 Cohen?s ?
on the MUC-4 data.
Giventhe difficulty of this task, we were satisfied thatthis task is reasonably well-defined and the anno-tations are of good quality.4.3.3 Event Recognizer ResultsWe evaluated the two sentential event recognizermodels described in Section 3.2 in two ways:(1) using the answer key sentence annotations fortraining/testing, and (2) using the human annota-tions for training/testing.
Table 3 shows the re-sults for all combinations of training/testing data.Since we only have human annotations for 100MUC-4 texts and 120 ProMed texts, we performed5-fold cross-validation on these documents.
Forour classifiers, we used the Weka (Witten andFrank, 2005) implementation of Na?
?ve Bayes andthe SVMLight (Joachims, 1998) implementationof the SVM.
For each classifier we report overallaccuracy, and precision, recall and F-scores withrespect to both the positive and negative classes(event vs. non-event sentences).The rows labeled Ans show the results for mod-els trained via answer keys, and the rows labeledHum show the results for the models trained withhuman annotations.
The left side of the tableshows the results using the answer key annotationsfor evaluation, and the right side of the table showsthe results using the human annotations for evalua-tion.
One expects classifiers to perform best whenthey are trained and tested on the same type ofdata, and our results bear this out ?
the classifiersthat were trained and tested on the same kind ofannotations do best.
The boldfaced numbers rep-resent the best accuracies achieved for each do-main.
As we would expect, the classifiers that areboth trained and tested with human annotations(Hum) show the best performance, with the Na?
?veBayes achieving the best accuracy of 85% on the157PerpInd PerpOrg Target Victim WeaponP R F P R F P R F P R F P R FAutoSlog-TS .33 .49 .40 .52 .33 .41 .54 .59 .56 .49 .54 .51 .38 .44 .41Sem Affinity .48 .39 .43 .36 .58 .45 .56 .46 .50 .46 .44 .45 .53 .46 .50NB (baseline) .36 .34 .35 .35 .46 .40 .53 .49 .51 .50 .50 .50 1.00 .05 .10GLACIERNB/NB .90 .39 .59 .47 .33 .51 .40 .39 .72 .51 .52 .54 .53 .47 .55 .51NB/SVM .40 .51 .58 .54 .34 .45 .38 .42 .72 .53 .55 .58 .56 .57 .53 .55NB/SVM .50 .66 .47 .55 .41 .26 .32 .50 .62 .55 .62 .36 .45 .64 .43 .52Table 5: Unified IE Model on MUC-4MUC-4 texts, and the SVM achieving the best ac-curacy of 79% on the ProMed texts.The recall and precision for non-event sentencesis much higher than for event sentences.
This clas-sifier is forced to draw a hard line between theevent and non-event sentences, which is a difficulttask even for people.
One of the advantages of ourunified IE model, which will be described in thenext section, is that it does not require hard deci-sions but instead uses a probabilistic estimate ofhow ?event-ish?
a sentence is.Table 3 showed that models trained on humanannotations outperform models trained on answerkey annotations.
But with the MUC-4 data, wehave the luxury of 1300 training documents withanswer keys, while we only have 100 documentswith human annotations.
Even though the answerkey annotations are noisier, we have 13 times asmuch training data.So we trained another sentential event recog-nizer using the entire MUC-4 training set.
Theseresults are shown in Table 4.
Observe that usingthis larger (albeit noisy) training data does not ap-pear to affect the Na?
?ve Bayes model very much.Compared with the model trained on 100 manu-ally annotated documents, its accuracy decreasesby 2% from 85% to 83%.
The SVM model, onthe other hand, achieves an 89% accuracy whentrained with the larger MUC-4 training data, com-pared to 84% accuracy for the model trained fromthe 100 manually labeled documents.
Conse-quently, the sentential event recognizer modelsused in our unified IE framework (described inSection 4.4) are trained with this 1300 documenttraining set.4.4 Evaluation of the Unified IE ModelWe now evaluate the performance of our unified IEmodel, GLACIER, which allows a plausible role-filler recognizer and a sentential event recognizerto make joint decisions about phrase extractions.Tables 5 and 6 present the results of the unifiedDisease VictimP R F P R FAutoSlog-TS .33 .60 .43 .36 .49 .41Sem Affinity .31 .49 .38 .41 .47 .44NB (baseline) .34 .59 .43 .47 .39 .43GLACIERNB/NB .90 .41 .61 .49 .38 .52 .44NB/SVM .40 .31 .66 .42 .32 .55 .41NB/SVM .50 .38 .54 .44 .42 .47 .44Table 6: Unified IE Model on ProMedIE model on the MUC-4 and ProMed data sets.The NB/NB systems use Na?
?ve Bayes models forboth components, while the NB/SVM systems usea Na?
?ve Bayes model for the plausible role-fillerrecognizer and an SVM for the sentential eventrecognizer.
As with our baseline system, we ob-tain good results using a threshold of 0.90 for ourNB/NB model (i.e., only NPs with probability ?0.90 are extracted).
For our NB/SVM models, weevaluated using the default threshold (0.50) but ob-served that recall was sometimes low.
So we alsouse a threshold of 0.40, which produces superiorresults.
Here too, we used the Weka (Witten andFrank, 2005) implementation of the Na?
?ve Bayesmodel and the SVMLight (Joachims, 1998) imple-mentation of the SVM.For the MUC-4 data, our unified IE model us-ing the SVM (0.40) outperforms all 3 baselineson three roles (PerpInd, Victim, Weapon) andoutperforms 2 of the 3 baselines on the Targetrole.
When GLACIER outperforms the other sys-tems it is often by a wide margin: the F-scorefor PerpInd jumped from 0.43 for the best base-line (Sem Affinity) to 0.54 for GLACIER, and theF-scores for Victim and Weapon each improvedby 5% over the best baseline.
These gains camefrom both increased recall and increased precision,demonstrating that GLACIER extracts some infor-mation that was missed by the other systems andis also less prone to false hits.Only the PerpOrg role shows inferior per-formance.
Organizations perpetrating a terrorist158event are often discussed later in a document, farremoved from the main event description.
For ex-ample, a statement that Al Qaeda is believed tobe responsible for an attack would typically ap-pear after the event description.
As a result, thesentential event recognizer tends to generate lowprobabilities for such sentences.
We believe thataddressing this issue would require the use of dis-course relations or the use of even larger contextsizes.
We intend to explore these avenues of re-search in future work.On the ProMed data, GLACIER produces resultsthat are similar to the baselines for theVictim role,but it outperforms the baselines for the Diseaserole.
We find that for this domain, the unified IEmodel with the Na?
?ve Bayes sentential event rec-ognizer is superior to the unified IE model withthe SVM classifier.
For the Disease role, the F-score jumped 6%, from 0.43 for the best base-line systems (AutoSlog-TS and the NB baseline)to 0.49 for GLACIERNB/NB.
In contrast to theMUC-4 data, this improvement was mostly dueto an increase in precision (up to 0.41), indicatingthat our unified IE model was effective at elimi-nating many false hits.
For the Victim role, theperformance of the unified model is comparableto the baselines.
On this event role, the F-scoreof GLACIERNB/NB(0.44) matches that of the bestbaseline system (Sem Affinity, with 0.44).
How-ever, note that GLACIERNB/NBcan achieve a 5%gain in recall over this baseline, at the cost of a 3%precision loss.4.5 Specific ExamplesFigure 2 presents some specific examples of ex-tractions that are failed to be extracted by thebaseline models, but are correctly identified byGLACIER because of its use of sentential evidence.Observe that in each of these examples, GLACIERcorrectly extracts the underlined phrases, in spiteof the inconclusive evidence in the local contextsaround them.
In the last sentence in Figure 2, forexample, GLACIER correctly makes the inferencethat the policemen in the bus (which was travelingon the bridge) are likely the victims of the terroristevent.
Thus, we see that our system manages tobalance the influence of the two probability com-ponents to make extraction decisions that wouldbe impossible to make by relying only on the localphrasal context.
In addition, the sentential eventrecognizer can also help improve precision by pre-THE MNR REPORTED ON 12 JANUARY THAT HEAVILYARMED MEN IN CIVILIAN CLOTHES HAD INTERCEPTEDA VEHICLE WITH OQUELI AND FLORES ENROUTE FORLA AURORA AIRPORT AND THAT THE TWO POLITICALLEADERS HAD BEEN KIDNAPPED AND WERE REPORTEDMISSING.PerpInd: HEAVILY ARMED MENTHE SCANT POLICE INFORMATION SAID THAT THEDEVICES WERE APPARENTLY LEFT IN FRONT OF THE TWOBANK BRANCHES MINUTES BEFORE THE CURFEW BEGANFOR THE 6TH CONSECUTIVE DAY ?
PRECISELY TOCOUNTER THE WAVE OF TERRORISM CAUSED BY DRUGTRAFFICKERS.Weapon: THE DEVICESTHOSE WOUNDED INCLUDE THREE EMPLOYEES OF THEGAS STATION WHERE THE CAR BOMB WENT OFF ANDTWO PEOPLE WHO WERE WALKING BY THE GAS STATIONAT THE MOMENT OF THE EXPLOSION.Victim: THREE EMPLOYEES OF THE GAS STATIONVictim: TWO PEOPLEMEMBERS OF THE BOMB SQUAD HAVE DEACTIVATEDA POWERFUL BOMB PLANTED AT THE ANDRES AVELINOCACERES PARK, WHERE PRESIDENT ALAN GARCIA WASDUE TO PARTICIPATE IN THE COMMEMORATION OF THEBATTLE OF TARAPACA.Victim: PRESIDENT ALAN GARCIAEPL [POPULAR LIBERATION ARMY] GUERRILLAS BLEWUP A BRIDGE AS A PUBLIC BUS, IN WHICH SEVERALPOLICEMEN WERE TRAVELING, WAS CROSSING IT.Victim: SEVERAL POLICEMENFigure 2: Examples of GLACIER Extractionsventing extractions from non-event sentences.5 ConclusionsWe presented a unified model for IE that balancesthe influence of sentential context with local con-textual evidence to improve the performance ofevent-based IE.
Our experimental results showedthat using sentential contexts indeed produced bet-ter results on two IE data sets.
Our current modeluses supervised learning, so one direction for fu-ture work is to adapt the model for weakly super-vised learning.
We also plan to incorporate dis-course features and investigate even wider con-texts to capture broader discourse effects.AcknowledgmentsThis work has been supported in part by the De-partment of Homeland Security Grant N0014-07-1-0152.
We are grateful to Nathan Gilbert andAdam Teichert for their help with the annotationof event sentences.159ReferencesR.
Bunescu and R. Mooney.
2004.
Collective In-formation Extraction with Relational Markov Net-works.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguis-tics, pages 438?445, Barcelona, Spain, July.M.
Califf and R. Mooney.
2003.
Bottom-Up Rela-tional Learning of Pattern Matching Rules for In-formation Extraction.
Journal of Machine LearningResearch, 4:177?210, December.H.
Chieu, H. Ng, and Y. Lee.
2003.
Closing theGap: Learning-Based Information Extraction Rival-ing Knowledge-Engineering Methods.
In Proceed-ings of the 41st Annual Meeting of the Associationfor Computational Linguistics, pages 216?223, Sap-poro, Japan, July.P.
Domingos and M. Pazzani.
1996.
Beyond Inde-pendence: Conditions for the Optimality of the Sim-ple Bayesian Classifier.
In Proceedings of the Thir-teenth International Conference on Machine Learn-ing, pages 105?112, Bari, Italy, July.J.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating Non-local Information into InformationExtraction Systems by Gibbs Sampling.
In Proceed-ings of the 43rd Annual Meeting of the Associationfor Computational Linguistics, pages 363?370, AnnArbor, MI, June.D.
Freitag and A. McCallum.
2000.
Informa-tion Extraction with HMM Structures Learned byStochastic Optimization.
In Proceedings of the Sev-enteenth National Conference on Artificial Intelli-gence, pages 584?589, Austin, TX, August.D.
Freitag.
1998.
Toward General-Purpose Learningfor Information Extraction.
In Proceedings of the36th Annual Meeting of the Association for Compu-tational Linguistics and 17th International Confer-ence on Computational Linguistics, pages 404?408,Montreal, Quebec, August.H.
Ji and R. Grishman.
2008.
Refining Event Ex-traction through Cross-Document Inference.
In Pro-ceedings of ACL-08: HLT, pages 254?262, Colum-bus, OH, June.T.
Joachims.
1998.
Text Categorization with Sup-port Vector Machines: Learning with Many Rele-vant Features.
In Proceedings of the Tenth EuropeanConference on Machine Learning, pages 137?142,April.C.
Manning, P. Raghavan, and H Schu?tze.
2008.
Intro-duction to Information Retrieval.
Cambridge Uni-versity Press, New York, NY.M.
Maslennikov and T. Chua.
2007.
A Multi-resolution Framework for Information Extractionfrom Free Text.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Lin-guistics, pages 592?599, Prague, Czech Republic,June.S.
Patwardhan and E. Riloff.
2007.
Effective Informa-tion Extraction with Semantic Affinity Patterns andRelevant Regions.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 717?727, Prague, Czech Re-public, June.W.
Phillips and E. Riloff.
2007.
Exploiting Role-Identifying Nouns and Expressions for Informa-tion Extraction.
In Proceedings of InternationalConference on Recent Advances in Natural Lan-guage Processing, pages 165?172, Borovets, Bul-garia, September.E.
Riloff and W. Phillips.
2004.
An Introduction to theSundance and AutoSlog Systems.
Technical ReportUUCS-04-015, School of Computing, University ofUtah.E.
Riloff.
1996.
Automatically Generating ExtractionPatterns from Untagged Text.
In Proceedings of theThirteenth National Conference on Articial Intelli-gence, pages 1044?1049, Portland, OR, August.S.
Soderland, D. Fisher, J. Aseltine, and W. Lehnert.1995.
CRYSTAL: Inducing a Conceptual Dictio-nary.
In Proceedings of the Fourteenth InternationalJoint Conference on Artificial Intelligence, pages1314?1319, Montreal, Canada, August.B.
Sundheim.
1992.
Overview of the Fourth MessageUnderstanding Evaluation and Conference.
In Pro-ceedings of the Fourth Message Understanding Con-ference (MUC-4), pages 3?21, McLean, VA, June.V.
Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer, New York, NY.I.
Witten and E. Frank.
2005.
Data Mining - PracticalMachine Learning Tools and Techniques.
Morgan?Kaufmann, San Francisco, CA.R.
Yangarber, R. Grishman, P. Tapanainen, and S. Hut-tunen.
2000.
Automatic Acquisition of DomainKnowledge for Information Extraction.
In Proceed-ings of the 18th International Conference on Com-putational Linguistics, pages 940?946, Saarbru?cken,Germany, August.B.
Zadrozny and C. Elkan.
2001.
Obtaining Cal-ibrated Probability Estimates from Decision Treesand Na?
?ve Bayesian Classiers.
In Proceedings of theEighteenth International Conference on MachineLearning, pages 609?616, Williamstown, MA, June.160
