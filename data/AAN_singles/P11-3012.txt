Proceedings of the ACL-HLT 2011 Student Session, pages 64?68,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsAn Error Analysis of Relation Extraction in Social Media DocumentsGregory Ichneumon BrownUniversity of Colorado at BoulderBoulder, Coloradobrowngp@colorado.eduAbstractRelation extraction in documents allows thedetection of how entities being discussed in adocument are related to one another (e.g.
part-of).
This paper presents an analysis of a re-lation extraction system based on prior workbut applied to the J.D.
Power and AssociatesSentiment Corpus to examine how the systemworks on documents from a range of socialmedia.
The results are examined on three dif-ferent subsets of the JDPA Corpus, showingthat the system performs much worse on doc-uments from certain sources.
The proposedexplanation is that the features used are moreappropriate to text with strong editorial stan-dards than the informal writing style of blogs.1 IntroductionTo summarize accurately, determine the sentiment,or answer questions about a document it is often nec-essary to be able to determine the relationships be-tween entities being discussed in the document (suchas part-of or member-of).
In the simple sentimentexampleExample 1.1: I bought a new car yesterday.
I lovethe powerful engine.determining the sentiment the author is expressingabout the car requires knowing that the engine is apart of the car so that the positive sentiment beingexpressed about the engine can also be attributed tothe car.In this paper we examine our preliminary resultsfrom applying a relation extraction system to theJ.D.
Power and Associates (JDPA) Sentiment Cor-pus (Kessler et al, 2010).
Our system uses lex-ical features from prior work to classify relations,and we examine how the system works on differentsubsets from the JDPA Sentiment Corpus, breakingthe source documents down into professionally writ-ten reviews, blog reviews, and social networking re-views.
These three document types represent quitedifferent writing styles, and we see significant differ-ence in how the relation extraction system performson the documents from different sources.2 Relation Corpora2.1 ACE-2004 CorpusThe Automatic Content Extraction (ACE) Corpus(Mitchell, et al, 2005) is one of the most commoncorpora for performing relation extraction.
In addi-tion to the co-reference annotations, the Corpus isannotated to indicate 23 different relations betweenreal-world entities that are mentioned in the samesentence.
The documents consist of broadcast newstranscripts and newswire articles from a variety ofnews organizations.2.2 JDPA Sentiment CorpusThe JDPA Corpus consists of 457 documents con-taining discussions about cars, and 180 documentsdiscussing cameras (Kessler et al, 2010).
In thiswork we only use the automotive documents.
Thedocuments are drawn from a variety of sources,and we particularly focus on the 24% of the doc-uments from the JDPA Power Steering blog, 18%from Blogspot, and 18% from LiveJournal.64The annotated mentions in the Corpus are singleor multi-word expressions which refer to a particu-lar real world or abstract entity.
The mentions areannotated to indicate sets of mentions which con-stitute co-reference groups referring to the same en-tity.
Five relationships are annotated between theseentities: PartOf, FeatureOf, Produces, InstanceOf,and MemberOf.
One significant difference betweenthese relation annotations and those in the ACE Cor-pus is that the former are relations between sets ofmentions (the co-reference groups) rather than be-tween individual mentions.
This means that theserelations are not limited to being between mentionsin the same sentence.
So in Example 1.1, ?engine?would be marked as a part of ?car?
in the JDPA Cor-pus annotations, but there would be no relation an-notated in the ACE Corpus.
For a more direct com-parison to the ACE Corpus results, we restrict our-selves only to mentions within the same sentence(we discuss this decision further in section 5.4).3 Relation Extraction System3.1 OverviewThe system extracts all pairs of mentions in a sen-tence, and then classifies each pair of mentions aseither having a relationship, having an inverse rela-tionship, or having no relationship.
So for the PartOfrelation in the JDPA Sentiment Corpus we considerboth the relation ?X is part of Y?
and ?Y is part ofX?.
The classification of each mention pair is per-formed using a support vector machine implementedusing libLinear (Fan et al, 2008).To generate the features for each of the mentionpairs a proprietary JDPA Tokenizer is used for pars-ing the document and the Stanford Parser (Klein andManning, 2003) is used to generate parse trees andpart of speech tags for the sentences in the docu-ments.3.2 FeaturesWe used Zhou et al?s lexical features (Zhou et al,2005) as the basis for the features of our system sim-ilar to what other researchers have done (Chan andRoth, 2010).
Additional work has extended thesefeatures (Jiang and Zhai, 2007) or incorporated otherdata sources (e.g.
WordNet), but in this paper we fo-cus solely on the initial step of applying these samelexical features to the JDPA Corpus.The Mention Level, Overlap, Base Phrase Chunk-ing, Dependency Tree, and Parse Tree features arethe same as Zhou et al (except for using the Stan-ford Parser rather than the Collins Parser).
The mi-nor changes we have made are summarized below:?
Word Features: Identical, except rather thanusing a heuristic to determine the head word ofthe phrase it is chosen to be the noun (or anyother word if there are no nouns in the men-tion) that is the least deep in the parse tree.
Thischange has minimal impact.?
Entity Types: Some of the entity types in theJDPA Corpus indicate the type of the relation(e.g.
CarFeature, CarPart) and so we replacethose entity types with ?Unknown?.?
Token Class: We added an additional feature(TC12+ET12) indicating the Token Class ofthe head words (e.g.
Abbreviation, DollarAm-mount, Honorific) combined with the entitytypes.?
Semantic Information: These features arespecific to the ACE relations and so are notused.
In Zhou et al?s work, this set of featuresincreases the overall F-Measure by 1.5.4 Results4.1 ACE Corpus ResultsWe ran our system on the ACE-2004 Corpus as abaseline to prove that the system worked properlyand could approximately duplicate Zhou et al?s re-sults.
Using 5-fold cross validation on the newswireand broadcast news documents in the dataset weachieved an average overall F-Measure of 50.6 onthe fine-grained relations.
Although a bit lower thanZhou et al?s result of 55.5 (Zhou et al, 2005), weattribute the difference to our use of a different tok-enizer, different parser, and having not used the se-mantic information features.4.2 JDPA Sentiment Corpus ResultsWe randomly divided the JDPA Corpus into train-ing (70%), development (10%), and test (20%)datasets.
Table 1 shows relation extraction resultsof the system on the test portion of the corpus.The results are further broken out by three differ-ent source types to highlight the differences caused65RelationAll Documents LiveJournal Blogspot JDPAP R F P R F P R F P R FFEATURE OF 44.8 42.3 43.5 26.8 35.8 30.6 44.1 40.0 42.0 59.0 55.0 56.9MEMBER OF 34.1 10.7 16.3 0.0 0.0 0.0 36.0 13.2 19.4 36.4 13.7 19.9PART OF 46.5 34.7 39.8 41.4 17.5 24.6 48.1 35.6 40.9 48.8 43.9 46.2PRODUCES 51.7 49.2 50.4 05.0 36.4 08.8 43.7 36.0 39.5 66.5 64.6 65.6INSTANCE OF 37.1 16.7 23.0 44.8 14.9 22.4 42.1 13.0 19.9 30.9 29.6 30.2Overall 46.0 36.2 40.5 27.1 22.6 24.6 45.2 33.3 38.3 53.7 46.5 49.9Table 1: Relation extraction results on the JDPA Corpus test set, broken down by document source.LiveJournal Blogspot JDPA ACETokens Per Sentence 19.2 18.6 16.5 19.7Relations Per Sentence 1.08 1.71 2.56 0.56Relations Not In Same Sentence 33% 30% 27% 0%Training Mention Pairs in One Sentence 58,452 54,480 95,630 77,572Mentions Per Sentence 4.26 4.32 4.03 3.16Mentions Per Entity 1.73 1.63 1.33 2.36Mentions With Only One Token 77.3% 73.2% 61.2% 56.2%Table 2: Selected document statistics for three JDPA Corpus document sources.by the writing styles from different types of media:LiveJournal (livejournal.com), a social media sitewhere users comment and discuss stories with eachother; Blogspot (blospot.com), Google?s bloggingplatform; and JDPA (jdpower.com?s Power Steeringblog), consisting of reviews of cars written by JDPAprofessional writers/analysts.
These subsets wereselected because they provide the extreme (JDPAand LiveJournal) and average (Blogspot) results forthe overall dataset.5 AnalysisOverall the system is not performing as well as itdoes on the ACE-2004 dataset.
However, there isa 25 point F-Measure difference between the Live-Journal and JDPA authored documents.
This sug-gests that the informal style of the LiveJournal doc-uments may be reducing the effectiveness of thefeatures developed by Zhou et al, which were de-veloped on newswire and broadcast news transcriptdocuments.In the remainder of this section we look at a sta-tistical analysis of the training portion of the JDPACorpus, separated by document source, and suggestareas where improved features may be able to aidrelation extraction on the JDPA Corpus.5.1 Document Statistic Effects on ClassifierTable 2 summarizes some important statistical dif-ferences between the documents from differentsources.
These differences suggest two reasons whythe instances being used to train the classifier couldbe skewed disproportionately towards the JDPA au-thored documents.First, the JDPA written documents express a muchlarger number of relations between entities.
Whentraining the classifier, these differences will cause alarge share of the instances that have a relation to befrom a JDPA written document, skewing the clas-sifier towards any language clues specific to thesedocuments.Second, the number of mention pairs occurringwithin one sentence is significantly higher in theJDPA authored documents than the other docu-ments.
This disparity is even true on a per sentenceor per document basis.
This provides the classifierwith significantly more negative examples written ina JDPA written style.66LiveJournal Blogspot JDPAMention%Mention%Mention%Phrase Phrase Phrasecar 6.2 it 8.1 features 2.4Maybach 5.6 car 2.1 vehicles 1.6it 3.7 its 2.0 its 1.4it?s 1.7 cars 2.0 Journey 1.3Maybach1.5 Hyundai 2.0 car 1.257 SIt 1.2 vehicle 1.52 T1.2Sportmileage 1.1 one 1.5 G37 1.2its 1.1 engine 1.5 models 1.1engine 0.9 power 1.1 engine 1.157 S 0.9 interior 1.1 It 1.1Total: 23.9% Total: 22.9% Total: 13.6%Table 3: Top 10 phrases in mention pairs whose relationwas incorrectly classified, and the total percentage of er-rors from the top ten.5.2 Common ErrorsTable 3 shows the mention phrases that occurmost commonly in the incorrectly classified men-tion pairs.
For the LiveJournal and Blogspot data,many more of the errors are due to a few specificphrases being classified incorrectly such as ?car?,?Maybach?, and various forms of ?it?.
The top fourphrases constitute 17% of the errors for LiveJour-nal and 14% for Blogspot.
Whereas the JDPA doc-uments have the errors spread more evenly acrossmention phrases, with the top 10 phrases constitut-ing 13.6% of the total errors.Furthermore, the phrases causing many of theproblems for the LiveJournal and Blogspot relationdetection are generic nouns and pronouns such as?car?
and ?it?.
This suggests that the classifieris having difficulty determining relationships whenthese less descriptive words are involved.5.3 VocabularyTo investigate where these variations in phrase errorrates comes from, we performed two analyses of theword frequencies in the documents: Table 4 showsthe frequency of some common words in the docu-ments; Table 5 shows the frequency of a select set ofparts-of-speech per sentence in the document.WordPercent of All Tokens in DocumentsLiveJournal Blogspot JDPA ACEcar 0.86 0.71 0.20 0.01I 1.91 1.28 0.24 0.21it 1.42 0.97 0.23 0.63It 0.33 0.27 0.35 0.09its 0.25 0.18 0.22 0.19the 4.43 4.60 3.54 4.81Table 4: Frequency of some common words per token.POSPOS Occurrence Per SentenceLiveJournal Blogspot JDPA ACENN 2.68 3.01 3.21 2.90NNS 0.68 0.73 0.85 1.08NNP 0.93 1.41 1.89 1.48NNPS 0.03 0.03 0.03 0.06PRP 0.98 0.70 0.20 0.57PRP$ 0.21 0.18 0.07 0.20Table 5: Frequency of select part-of-speech tags.We find that despite all the documents discussingcars, the JDPA reviews use the word ?car?
much lessoften, and use proper nouns significantly more often.Although ?car?
also appears in the top ten errors onthe JDPA documents, the total percentage of the er-rors is one fifth of the error rate on the LiveJour-nal documents.
The JDPA authored documents alsotend to have more multi-word mention phrases (Ta-ble 2) suggesting that the authors use more descrip-tive language when referring to an entity.
77.3%of the mentions in LiveJournal documents use onlya single word while 61.2% of mentions JDPA au-thored documents are a single word.Rather than descriptive noun phrases, the Live-Journal and Blogspot documents make more use ofpronouns.
LiveJournal especially uses pronouns of-ten, to the point of averaging one per sentence, whileJDPA uses only one every five sentences.5.4 Extra-Sentential RelationsMany relations in the JDPA Corpus occur betweenentities which are not mentioned in the same sen-tence.
Our system only detects relations betweenmentions in the same sentence, causing about 29%of entity relations to never be detected (Table 2).67The LiveJournal documents are more likely to con-tain relationships between entities that are not men-tioned in the same sentence.
In the semantic rolelabeling (SRL) domain, extra-sentential argumentshave been shown to significantly improve SRL per-formance (Gerber and Chai, 2010).
Improvementsin entity relation extraction could likely be made byextending Zhou et al?s features across sentences.6 ConclusionThe above analysis shows that at least some of thereason for the system performing worse on the JDPACorpus than on the ACE-2004 Corpus is that manyof the documents in the JDPA Corpus have a dif-ferent writing style from the news articles in theACE Corpus.
Both the ACE news documents, andthe JDPA authored documents are written by profes-sional writers with stronger editorial standards thanthe other JDPA Corpus documents, and the relationextraction system performs much better on profes-sionally edited documents.
The heavy use of pro-nouns and less descriptive mention phrases in theother documents seems to be one cause of the re-duction in relation extraction performance.
There isalso some evidence that because of the greater num-ber of relations in the JPDA authored documents thatthe classifier training data could be skewed more to-wards those documents.Future work needs to explore features that can ad-dress the difference in language usage that the dif-ferent authors use.
This work also does not ad-dress whether the relation extraction task is beingnegatively impacted by poor tokenization or pars-ing of the documents rather than the problems beingcaused by the relation classification itself.
Furtherwork is also needed to classify extra-sentential rela-tions, as the current methods look only at relationsoccurring within a single sentence thus ignoring alarge percentage of relations between entities.AcknowledgmentsThis work was partially funded and supported byJ.
D. Power and Associates.
I would like to thankNicholas Nicolov, Jason Kessler, and Will Headdenfor their help in formulating this work, and my the-sis advisers: Jim Martin, Rodney Nielsen, and MikeMozer.ReferencesChan, Y. S. and Roth D. Exploiting Background Knowl-edge for Relation Extraction.
Proceedings of the 23rdInternational Conference on Computational Linguis-tics (Coling 2010).R.-E.
Fan, K.-W. Chang, C.-J.
Hsieh, X.-R. Wang, andC.-J.
Lin.
LIBLINEAR: A library for large linearclassification.
Journal of Machine Learning Research9(2008), 1871-1874.
2008.Gerber, M. and Chai, J.
Beyond NomBank: A Study ofImplicit Arguments for Nominal Predicates.
Proceed-ings of the 48th Annual Meeting of the Association forComputational Linguistics, pages 1583-1592.
2010.Jiang, J. and Zhai, C.X.
A systematic exploration of thefeature space for relation extraction.
In The Proceed-ings of NAACL/HLT.
2007.Kessler J., Eckert M., Clark L., and Nicolov N.. TheICWSM 2010 JDPA Sentiment Corpus for the Auto-motive Domain International AAAI Conference onWeblogs and Social Media Data Challenge Workshop.2010.Klein D. and Manning C. Accurate Unlexicalized Pars-ing.
Proceedings of the 41st Meeting of the Asso-ciation for Computational Linguistics, pp.
423-430.2003.Mitchell A., et al ACE 2004 Multilingual Training Cor-pus.
Linguistic Data Consortium, Philadelphia.
2005.Zhou G., Su J., Zhang J., and Zhang M. Exploring var-ious knowledge in relation extraction.
Proceedings ofthe 43rd Annual Meeting of the ACL.
2005.68
