DISAMBIGUATING PREPOSITIONAL PHRASE ATTACHMENTSON-LINE DICTIONARY DEFINITIONSBY USINGKaren Jensen and Jean-Louis Binot*Computer Sciences DepartmentIBM Thomas J. Watson Research CenterP.O.
Box 218Yorktown Heights, New York 10598Standard on-line dictionaries offer a wealth of knowledge xpressed in natural language form.
We claimthat such knowledge can and should be accessed by natural language processing systems to solve difficultambiguity problems.
This paper sustains that claim by describing a set of computational tools andtechniques used to disambiguate prepositional phrase attachments in English sentences, by accessingon-line dictionary definitions.
Such techniques offer hope for eliminating the time-consuming, and oftenincomplete, hand coding of semantic information that has been conventional in natural languageunderstanding systems.1.
INTRODUCTIONA customary pattern for papers on natural languageprocessing runs roughly as follows:1.
Here's a difficult language situation.2.
Here's the semantic (pragmatic / discourse / what-ever) information ecessary to interpret he situ-ation as we humans interpret it.3.
Here's how I put this information into my system.4.
Therefore, my system can handle the situation.Although the amount of effort which has been ex-pended in doing this kind of thing is admirable, there isstill something less than totally satisfactory about theapproach.
First, it tends to trivialize the notion of"solution to a problem."
Second, and more important,the approach relies on hand-coding pieces of informa-tion (commonly called "knowledge").
The more wewant our programs to know, the more we have tohand-feed them.
We can keep on doing this, and maybe,after several generations, someone will put all thepieces together and discover that AI researchers havesucceeded in rewriting the history of the universe as weunderstand it.Without question, this will be a form of success.
Butthere might be a faster way to the goal ---one thatdoesn't involve reinventing the wheel of knowledge.
We* The second author currently works for B.I.M., Belgium.already have volumes of hand-coded natural languageinformation in our reference books ---dictionaries andencyclopedias, for instance.
If computers could accessthat information, and use it to help get out of difficultsituations, they (and we) would be much further ahead.Problems should be expected, of course: referenceworks are arbitrary and inconsistent; just having theinformation does not guarantee being able to use it.It is nevertheless possible to think of on-line refer-ence books as knowledge bases.
In this paper wepropose techniques for processing the definitions of anon-line standard dictionary, and for extracting fromthem the semantic information ecessary to resolve theambiguities inherent in the attachment of English prep-ositional phrases.
We consult he on-line dictionary as ifit were a semantic expert, and we find in it the kind ofinformation that has previously been supplied by meansof scripts, frames, templates, and similar hand-crafteddevices.We start by presenting PEG, a broad-coverage com-putational grammar of English which is our tool foranalyzing on-line definitions, and by discussing the toolsnecessary for extracting from these definitions theknowledge relevant for disambiguation.
Subsequentsections describe these tools in greater detail, focussingon specific sentences and on the heuristic machineryused to discover their most likely interpretations.
Afinal section sums up the work and suggests someCopyright 1987 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material isgranted providedthat the copies are not made for direct commercial dvantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, or to republish, requires afee and/or specific permission.0362-613X/87/030251-260503.00Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 251Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase Attachmentimportant areas for future research.
Two appendicesprovide traces of the processing of some examples, andtechnical information about the "approximate r ason-ing" techniques used in our system.2.
PEG AND SYNTACTIC PARSINGThe computational grammar called PEG (PLNLP En-glish Grammar) analyzes English sentences by usingsyntactic information in augmented phrase structurerules with a bottom-up, parallel processor.
The systemthat provides these facilities is PLNLP, or the Program-ming Language for Natural Language Processing (Hei-dorn 1975).
PEG produces useful sentence parses for alarge amount, and a wide variety, of English text.
Theseparses contain syntactic and functional (subject, object,etc.)
information, but, as yet, no semantic information,or other information beyond the functional level.We do not argue against he general notion of inte-grating syntax and semantics.
When natural anguageprocessing techniques are further developed, we willprobably be able to see more clearly how this integra-tion should best be accomplished.
But there are alsoindependent reasons for postponing the use of seman-tics as long as possible.
And the development of PEGhas shown that a syntax-only stage of parsing is notcomputationally expensive, and is both efficient anduseful (Jensen 1986).There is an enduring notion that allowing ambiguitiesto persist in a syntactic parser must cause a disastrouscombinatorial explosion in the number of parses pro-duced.
This is not true.
In the first place, syntax is morehelpful than has generally been supposed, and can beused to filter out the grossly unlikely parses.
In thesecond place, there are economical ways of dealing withambiguities.
PEG's method, during the syntactic phaseof parsing, is to attach pre- and post-modifiers in a singlearbitrary pattern (usually to the closest possible head,which is always labeled with "*"), but to mark otherpossible attachment sites so that they can be referred tofor later semantic processing.
A question mark indicatesthese possible attachment sites:DECL h'P PROI~VERB* "age"HP DETNOUN*?
PPPUNC " "ttitIADJ* "a"ttf ishttPREP "w i th"DET ADJ*NOUN* "fork"flaw,Figure 1.
PEG parse tree for a syntactically ambiguoussentence, "I ate a fish with a fork.
"This is a compact shorthand for expressing attachmentambiguities.
The attachment strategy eliminates anycombinatorial explosion; it does not significantly affectparsing time; and it allows for the full range of latersemantic processing.One of the jobs that needs doing in order to movebeyond syntax is the elimination of syntactically egal,but semantically anomalous, ambiguities like the ambi-guity in Figure 1.
All sorts of modifiers can potentiallybe multiply attached: nouns, adjectives, adverbs,phrases, and clauses.
In this paper we will concentrateon the disambiguation f prepositional phrase attach-ments, which certainly qualifies as a difficult problem innatural anguage processing, and also one which is ofconsiderable current interest (Lytinen 1986, Dahlgrenand McDowell 1986, Schubert 1986).3.
DICTIONARY DEFINITIONSOne of the strengths of PEG is that it is a genuinelybroad-coverage grammar; it can attempt a syntacticparse of any sentence inordinary English.
This is a gain,and should not be lost as we move into semanticprocessing.
Having a largely domain-independent syn-tax, we do not want to create a domain-dependentsemantics.There is, of course, one repository of semanticinformation which already exists, which has been de-veloped over many years, and which applies in a broadfashion to almost all English words, and to large subsetsof the vocabulary of all task domains.
This is thedefining of words and word senses in a good standarddictionary.
We have been using syntactic informationfrom two on-line dictionaries: Webster's Seventh NewCollegiate (W7) and the Longman Dictionary of Con-temporary English (LDOCE).
The next step is to ex-plore on-line definitions with the goal of extractingsemantic information.
For this initial stage of research,we have used only W7.Dictionary definitions contain the following kinds ofinformation that are of interest o the present ask:?
the definition(s);?
example sentences and phrases;?
synonyms;?
usage notes and other comments.In order to make use of this information for naturallanguage processing, we need at least:?
a tool to parse dictionary definitions and examplesentences;?
a tool to extract he semantically relevant informationfrom the definitions and examples;?
a tool for looking up the definitions of related words:superordinate words or ancestors (hypernyms), sub-ordinate words or descendents (hyponyms), andsame-level words (synonyms and antonyms);?
the robustness necessary to cope with the manypotential errors or format variations which can ap-pear in a man-made dictionary.PEG provides the necessary parsing tool.
Robust toolsfor extracting information from dictionary entries, and252 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase Attachmentfor making use of that information to disambiguateattachment problems, are described in the followingsections.4.
HEURISTICSOne of the key factors for achieving robustness is therecognition that every single rule that we might want towrite for processing dictionary entries is bound to fail atleast once in a while.
Therefore rules should be de-signed as heuristics, which can only provide approxi-mate results.
The cumulative ffect of many heuristics,and not the perfection of each one taken separately, hasto do the job.As in a typical "approximate reasoning" paradigm,all heuristics hould only yield weighted preferences, orcertainty factors, and no categorical acceptance or re-jection of an interpretation.
Thus different heuristicscan interact in an additive way, and unforeseen exam-ples will be more easily processed.
In that way also, thewhole system will be incremental: we can start mod-estly and progressively augment its capabilities.
Theprocess should be fail-proof in the sense that it shouldalways give an answer: at one extreme, this answershould be to state that no better ordering can be made,and that things should be left as they are.As a first step, we have started to study the heuristicsthat will be necessary to disambiguate the attachmentsin sentences using the preposition with, and the rela-tionships signaled by this preposition.
According to astandard handbook on prepositions, with can signal:place, time, agency or instrument, cooperation, oppo-sition, possession or characteristic, separation or alien-ation, association, and more (Funk and Wagnalls 1953,pp.
34-35).
Heuristics will eventually be necessary forall of these relationships.
For present purposes how-ever, we are particularly interested in the relationshipsof INSTRUMENT and PARTOF.
PARTOF involveswhat has been called "inalienable possession" - -  pos-session by nature, not by accident.
In these terms, ahand would be PARTOF a body, but a label would notbe PARTOF a dress.
(Other heuristics will be developedto handle other types of possession.
)The relationships in which we are interested can beillustrated by the following sentences (Binot 1985):(1) I ate a fish with a fork.
(2) I ate a fish with bones.
(3) I ate a fish with my fingers.In all cases, the ambiguity resides in the placement ofthe with prepositional phrase, which can modify fish orate.
And in all cases, PEG has the PP attached to theclosest possible head, fish, with a question mark show-ing where the PP would be placed if it modified the mainverb ate (see Figure 1).The rest of this section presents, in an informal way,our strategy for solving such ambiguities, and the mo-tivations behind some particular heuristics that we areusing.
The remaining sections provide more detailsabout the implementation of the system, and aboutdifficulties that still have to be resolved.Focussing on (1) and following the "approximate"approach stated above, another way to phrase the keyquestion is " Is it more likely that a fork is associatedwith a fish or with an act of eating?"
To answer thatquestion, the system evaluates eparately the plausibil-ity of the two proposed constructs:(4a) eat with fork.
(4b) fish with forkthen orders the solutions, and picks the one with thehighest rating.In the heuristics we are currently using, one con-struct is rated higher than another if the words that itcontains, and their relationship to each other, can bemore easily validated in dictionary definitions.
Forexample, we look up the noun fork in W7.
If, by chance,its definition were to include the phrase eat with a fork,then (4a) would be immediately validated.
Most likely,however, these exact same words will not occur.
But,as we will see, the definition of fork contains the phraseused for taking up, and eating is defined as a kind oftaking in the dictionary.
If we can provide a way toestablish these relationships, then (4a) is validated,although somewhat less easily.Ease (or certainty) of validation becomes progres-sively less, the harder it is to establish these inter-wordrelationships.
And the relationships are established:(a) by identifying equivalent function-word patternsin the definitions, such as the equivalence of usedfor and the instrumental with;(b) by linking important definition words (i.e., cen-tral terms in definitional phrases, such as headsof phrases, or else synonyms).
This can be doneby parsing the definitions, identifying the centralword(s), and then following hierarchical chains ofdefinitions through the dictionary.The two main heuristics that will be used to evaluatethe plausability of (4a) against (4b) can be described innatural language as follows (for more details on theformalization and implementation of the heuristics, seesection 6):HI- for checking for an INSTRUMENT relation be-tween a head and a with complement:1. if the head is not a verb, the relation doesn't hold(certainty factor = -1);2. if some "instrument pattern" (see below) exists inthe dictionary definition of the complement, and ifthis pattern points to a defining term that can belinked with the head, then the relation probablyholds (certainty factor = 0.7);3. else assume that there is more chance that therelation doesn't hold (certainty factor = -0.3).H2- for checking for a PARTOF relation between a headand a with complement:1. if the head is not a noun, the relation doesn't hold(certainty factor = -1);2. if some "part-of pattern" (see below) exists in theComputational Linguistics Volume 13, Numbers 3-4, July-December 1987 253Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase Attachmentdictionary definition of the complement, and ifthis pattern points to a defining term that can belinked with the head, then the PARTOF relationprobably holds (certainty factor = 0.7);3. else assume that there is more chance that therelation doesn't hold (certainty factor -- -0.3).Heuristic answers are expressed in terms of certaintyfactors which, as in the MYCIN system (Shortliffe1976), take their values in the range (-I,+ 1):-1 expresses absolute disbelief;0 expresses complete uncertainty;1 expresses absolute belief.Intermediate values express varying degrees of belief ordisbelief.
It must be understood that each certaintyfactor refers to the specific proposition (or goal) towhich the heuristic is applied.
Thus, if clause 3 ofheuristic H2 is used when applied to the proposition(4b), the resulting certainty factor -0.3 will indicate arelatively moderate disbelief in this proposition, stem-ming from the fact that the system has not been able tofind any positive vidence in the dictionary to sustain it.The above heuristics are possible because there arespecific words and/or phrases in dictionary definitions,forming what we shall call here patterns, which arealmost systematically used to express pecific semanticrelations (Markowitz et al 1986).
For the two relationsconsidered here, some of these patterns are:INSTRUMENT: for, used for, used to, a means for,etc.PARTOF: part of, arises from, end of, member of,etc.These prepositional patterns generally take, as theirobjects, some central term (or terms) in the definition ofthe complement word.
An attempt can then be made tolink that term with the head of the construct that is beingstudied.Focussing again on example sentence (1), the systemstarts by examining the first construct, (4a).
It parsesthe definition of the complement fork, and discovers atleast one INSTRUMENT pattern, used for:fork: 1.
An implement with two or more prongs usedesp for taking up (as in eating), pitching ordigging.Taking the conjunction i to account, the system findsthree possible terms: taking up, pitching, and digging,which it tries to link with eat.
(For the present, wedeliberately avoid the phrase as in eating which offers adirect match - -  in order to show that our approach doesnot rely on such lucky coincidences.)
Following hierar-chical chains of definitions by parsing one definition andthen extracting, for example, its head word, the systemis able to establish that eat is a direct hyponym of takeaccording to W7; the link is thus probably established.The system then moves on to consider (4b).
Since noPARTOF pattern can be found in the definitions of fork,this second possible construct will be ranked as muchless likely--(4a) receives a certainty factor of +0.7, but(4b) gets a certainty factor of only -0.3.
Therefore thesystem recommends attaching the prepositional phraseto the main verb in sentence (1).For sentence (2), the constructs to be compared areeat with bones and fish with bones.
In the definition ofbone, no useful INSTRUMENT pattern is found; so eatwith bones cannot be easily validated.
On the otherhand, the first definition of bone gives the followingPARTOF pattern:bone: 1.
One of the hard parts of the skeleton of avertebrate.This yields two possible links for fish: skeleton andvertebrate.
Fish can be identified as a direct hyponym ofvertebrate according to W7.
Therefore, fish with bonesreceives a higher certainty factor than eat with bones,and the system recommends attaching the prepositionalphrase to the direct object NP in sentence (2).The word-linking operation, which is used in heuris-tics H1 and H2, will presumably be used in many moreheuristics as we extend the system.
Such often-usedoperations can be made into subgoals, for which we candesign specific heuristics.
The heuristic urrently usedfor establishing links between words is:H3- for checking if one word (typically the head of aconstruct to be tested) is linked to some other word(typically a term pointed to by a pattern in adictionary definition):1. if head and term have the same base, the link iscertainly established (certainty factor = 1);2. if the term appears as an hyponym or hypernym ofthe head by following hierarchical patterns in thedictionary, the link is probably established (cer-tainty factor -~- 0.7);3. if term and head appear as common hyponyms ofthe same genus word by following hierarchicalpatterns in the dictionary, the link is (less) prob-ably established (certainty factor = 0.5);4. else, the link is probably not established (certaintyfactor = -0.7).The link-checking operation embodied in H3 can belikened to the preference-checking operation proposedby Wilks (1975).
An established link will validate a kindof semantic restriction which can fail, but which, ifsatisfied, will increase our confidence in the likelihoodof the proposed relation.
However, in our case, theinformation used to check the satisfaction of the pref-erence is not obtained from any hand-made semanticclassification, frame, script, or other such structure, butfrom the W7 dictionary itself.
Moreover, it should bepointed out here that we want to take the notion of"preference satisfaction" in a very loose sense.
Al-though the sentences discussed above exemplified onlycases of simple inclusion, what we propose to do in thelong term is to check if some connection can be estab-lished between the two terms - -  the word being definedand the preference indication - -  by using hyponym,hypernym and synonym information extracted from thedictionary.Many useful disambiguations can be performed by254 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase Attachmentusing the kind of link checking outlined above.
Theheuristic approach, however, does not limit us to thismechanism; and it would indeed be a mistake to acceptsuch a limit.
Sentence (3) is a case in point.
Here, nouseful INSTRUMENT or PARTOF pattern can befound in the definitions of finger.
However our strongintuition is that the presence of a possessive my, whichdisagrees in person with the possible head fish, pre-cludes any PARTOF relation.
This intuition can beformulated in the following heuristic:H4- for checking for a PARTOF relationship between ahead and a with complement:1. ifa.
the head is a noun, andb.
the complement is qualified by a possessive, andc.
this possessive doesn't agree in person with thehead, then the PARTOF relation certainly doesn'thold (certainty factor = -1),2. else this heuristic doesn't give any indication(certainty factor = 0).The effect of H4 on sentence (3) will be to give toPARTOF such a bad score that the interpretation eatwith myfingers will be preferred espite the absence ofany positive evidence in its favor.
When a possessivedoes not appear, H4 does not alter, in any way, theresults provided by previous heuristics.
This isachieved, in the second clause of H4, through the use ofthe zero certainty factor, which expresses total uncer-tainty and will not modify any other certainty factorproposed by another heuristic for the same problem.
(See Appendix 2 for details on the combining of cer-tainty factors.)
Another interesting point illustrated byH4 is the ease with which syntactic and semantic luescan be integrated in this kind of approach.
Differentheuristics can cover different ypes of clues, and theresulting certainty factors will be combined to yield aunique, weighted answer to the stated problem.5.
PATTERNSOne of the most basic capabilities needed to implementthe strategy outlined above is the capability to matchspecific patterns against dictionary definitions.
Twoproblems must be mentioned here:1.
A string level pattern matching mechanism willnot do.
Consider for example the PARTOF pat-tern part of.
What we really need to find is thehead of the prepositional phrase introduced by ofand attached to the noun part.
This clearly re-quires us to be able to handle the syntactic struc-ture of the definition.2.
We are - -  and must be - -  unsure of the patternswe use.
On the one hand, further experience withour system, or other parallel studies, might revealnew useful patterns; on the other hand,it mightprove necessary to add new constraints o existingpatterns in order to improve their selectivity.
Weobviously need a flexible and declarative way tospecify patterns, so that changes can always beeasily incorporated into the system.The PEG grammar itself provides an adequate solu-tion to the first problem.
Each definition is parsed byPEG, and the pattern-matching is performed on theresulting parse trees.
An additional difficulty occurshere.
Dictionary definitions rarely form complete sen-tences.
In W7, for example, a definition for a noun istypically formulated as a noun phrase, and a definitionfor a verb as a verb phrase.
PEG has the capability tohandle this by setting a switch indicating that the inputshould parsed as an NP, or as a VP, rather than as acomplete sentence.
Figure 2 below shows the parse forthe first definition of the noun bone:NP* NOUN* "one"PP PREP "of t'DET ADJ*AJP ADJ*NOUN ?4' "parts"?
PP PREPDETNOUN*?
?
PP"the""hard"t'of"ADJ* **the**"ske le ton"PREP 'tofttDET ADJ* ttaUNOUN* "vnr1:ebra~e"Figure 2.
Parse tree for the first definition of the noun bone:one of the hard parts of the skeleton of a vertebrate.To solve the second problem, we have developed adeclarative language for specifying patterns to be ap-plied to PEG parse trees.
Each node in a parse tree is infact a PLNLP record with a set of attribute-value pairs.An expression in the pattern language will describe atarget node by enumerating constraints that this nodemust satisfy.
For example, the pattern:(take head ((segtype pp)(has prp ((base of)))(in ((has head ((base part)))))))directs the pattern-matcher to find in the parse tree thehead of a node of type PP having as value for theattribute PRP (PRePosition) a record representing thepreposition of, and being a successor in the tree of anode having as head the word part.
Such patterns areapplied by a special function SEARCHPARSE, whichtakes as arguments a pattern and a parse tree.
If calledwith the pattern shown above and the parse tree ofFigure 2, SEARCHPARSE will return as result thefollowing PLNLP record:SEb-WYPE ' NOUN *STR ,t ske l  etonttSUP 'UNI~C'BASE ' SKELETON'POS 'ADJ ~POS t NOUN tINDIC SING PERS3Figure 3.
PLNLP record for the head of the PP node in theparse tree of Figure 2.Computational Linguistics Volume 13, Numbers 3-4, July-December 1987 255Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase AttachmentThe value of the BASE attribute of that record canthen be taken as argument for the link checking mech-anism outlined in the last section.
The pattern languagehas a simple, highly recursive structure.
It currentlyoffers operations to test the value of an attribute field, ofan indicator, of the predecessors and successors of atarget node in a tree, and of the position of the stringcovered by the target node in the input text.6.
IMPLEMENTATION OF HEURISTICS.Heuristics are best stated in some formal declarativelanguage that allows us easily to specify new heuristics.An option would have been to use a classical expertsystem shell with a built-in inference engine.
But itseems unlikely, at least at this stage of the research, thatwe will need the full power of such an engine, becausewe will probably not use long reasoning chains; whenwe need to solve a subgoal, heuristics for that subgoalcan be invoked directly by an explicit call in the mainheuristic.
Thus we decided to dispense with backtrack-ing and with logical unification, and to design our own,simpler but faster, control structures.The disambiguation system is called via the top-levelfunction CHOOSE, which receives as unique argumenta list of goals; each goal states one possible interpreta-tion whose plausibility is to be evaluated.
As an exam-ple, the call corresponding to sentence (1) of section 4is:(CHOOSE ( (WITH <FISH> <FORK>)(WITH <EAT> <FORK>) ))Figure 4.
A call to the disambiguation system.where a name between angled brackets denotes thePLNLP record representing the node having that nameas base in the parse tree of the sentence.
Thus (EAT)denotes the node labeled "ate" in the parse tree shownin Figure 1.CHOOSE will return its argument goal list, butsorted in decreasing order of plausibility of each goal.When two or more goals have the same plausibility, theoriginal order will be kept, thereby instructing thesystem to accept he parse tree created by PEG.CHOOSE works by applying to each member of itsgoal list the function SOLVE, which finds all heuristicsapplicable to a given goal and uses them to determinethe plausibility of that goal.
As can be seen from Figure4, the general structure of a goal is:::= ( (goal type) (argument)*)a list starting with a keyword enoting the type of thegoal, and then enumerating the specific arguments ofthegoal.
The number and kind of the arguments dependupon the type of goal being considered.Each heuristic is coded separately as a PLNLPprocedure.
The following figure gives the formal speci-fication of the heuristic H2, which was informallydescribed in section 4:112 (CONTROLE~PTR, CONP~PTR,<SEGTYPE (CONTR01,ER) .lqg.
'NOON',<'LF< ' PAR'I3~ t , I"-1.0">>,<SU3GOAL2< ' P I~F ' .
.
.
CORI'2OLE2... E I~ I "R IES<COIGI~,' P~FI~IF ' > .
.
.
SEG'I"~E (CONTROLE2)... 2>,<'LF<'  P~OF t , l"O.
7">>,<'LF< IP&EroF ' , ! "
-0 .3">)Figure 5.
Formal specification ofa heuristic.While we do not want to discuss this formalism inany detail, a few points are worth noting.
First, eachheuristic has indexing information i dicating what kindof goal it can solve.
This information allows the systemto retrieve, easily and efficiently, all the heuristicsapplying to a given goal.
Secondly, each heuristic hasformal arguments (HEAD and COMPLEMENT in thecase of H2) which will be unified to the effectivearguments of the goal it is applied to.
All the heuristicsdesigned for the same type of goal have the samearguments.The body of a heuristic onsists of a set of clauses,each of which includes one condition and one action.The clauses are evaluated in sequence until a conditionis satisfied; the evaluation of the corresponding actionthen yields the solution proposed, by the heuristic, forthe goal to which it is applied.Conditions can make use of any function returning atruth value, but can also make explicit calls to "solvingoperators" for solving subgoals.
Thus the call toSUBGOAL2 in Figure 5 starts the process of solving thepreference checking subgoal.
Such conditions are con-sidered to be satisfied if the resulting certainty factor ofthe subgoal is positive.
This factor is then combinedwith the certainty factor specified by the action of theclause according to rules of "conditional evidence"explained in Appendix 2.Up to now, and for the sake of simplicity, we haveassumed that a solution to a goal consists imply of onecertainty factor, summarizing the final amount of beliefor disbelief in the proposition stated by that goal.However, for the kinds of problems we are concernedwith, we can also distinguish subsolutions.
Knowingwhich subsolution has provided the resulting certaintyfactor is an important part of the answer.
Thus, for thegoal (WITH (EAT)(FORK)), we want to know, if pos-sible, not only if fork is a credible with complement ofeat, but also what kind of semantic relation is expressedby this complement (in that case, INSTRUMENT).
Asanother example, when checking if a word satisfies asemantic preference, we might be interested in knowingfor which sense of the word the preference is bestsatisfied.256 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase AttachmentA solution will then be defined as a list of subsolu-tions, each subsolution having its own certainty factor:(solution)::= ((subsolution)*)(subsolution)::= ((answer) .
factor).For the call to CHOOSE given (WITH (EAT)(FORK)),the heuristics of section 4 will yield respectively thefollowing solutions:HI: ((INSTRUMENT.
0.49))H2: ((PARTOF.-1))H4: ((ALL.
0))The answer provided by H4 makes use of a specialsubsolution called ALL, which specifies that the corre-sponding certainty factor (0) applies to all possiblesubsolutions of the problem.
The effect here is to assertthat this heuristic has nothing useful to say about thisspecific goal.The separate solutions produced by the three heuris-tics provide "cumulative evidence" which will have tobe combined to yield the overall solution to our goal,which will be, in this case:((INSTRUMENT .
0.49)(OTHERS .
0)(PARTOF .-1))This solution asserts that the most credible way inwhich (FORK) can be a with complement for (EAT) isby playing an INSTRUMENT role, with a certaintyfactor of 0.49.
The combination of the special ALLsubsolution with the other answers has led to theintroduction of another special subsolution, calledOTHERS, meaning "all subsolutions not mentioned inthe current solution."
The precise rules used to com-bine cumulative vidence are stated in Appendix 2.For the second goal of Figure 4, (WITH(FISH)(FORK)), the final solution, obtained by follow-ing the same steps, will be:((OTHERS .
0)(PARTOF .-0.3)(INSTRUMENT .-I))Since the plausibility of a solution is defined as thehighest certainty factor of its subsolutions, this answeris clearly less favorable then the previous one, and thesystem concludes that (FORK) should be attachedpreferably to.might soon be possible to build a separate structure ofsemantic hierarchies, by automatically processing dic-tionary definitions; but it is too early to impose any rigidform on this sort of knowledge base.This line of research should benefit all areas ofnatural anguage processing - -  from text analysis tomachine translation to data base query - -  by opening upthe prospect of a genuinely broad-coverage approach tomeaning and understanding.FUTURE RESEARCHImportant areas for future development include thefollowing, some of which constitute interesting diffi-culties for the disambiguation process:?
Development of new heuristics, and adjustment andimprovement of the certainty factors, in order to testthe system on a larger scale.?
Analysis of relationships that are signaled by prepo-sitions, conjunctions, and other function words, andof the relational patterns that can be found in dictio-nary entries;?
Incorporation of other types of dictionary informa-tion, from as many dictionaries as possible.?
Addition of information on phrasal verbs (verb-par-ticle pairs, verb preposition pairs), and investigationof how these will help in the parsing process.APPENDIX 1" TRACES OF PROCESSING OF EXAMPLESWe give here traces of the execution of the threeexamples discussed in the text.
Lines preceded by"***" are comments introduced manually to explainthe operations.
For reasons of space, we give only theparse trees of the definitions contributing directly to thesolution.EXAMPLE l.I ate a fish with a fork.
*** Parse tree for the sentence:7.
SUMMING UP AND LOOKING FORWARDCONCLUSIONSWe have shown that it is possible to consult, automat-ically, a good machine-readable dictionary as a seman-tic expert or knowledge base, in order to resolveambiguities in computational parsing of natural lan-guage text.
Only prepositional phrase attachment ambi-guities have been discussed, but the techniques de-scribed here should be extendible to ambiguities thatinvolve all other classes of phrases and clauses.
InAppendix 1, we present races of the disambiguationprocedure for the prepositional phrase attachments insentences (1-3).All types of information present and available indictionary entries should eventually be used to help inthe disambiguation process: examples, synonyms, andusage notes, along with the definitions themselves.
ItDECL lqP PRON*VERB* "ate"NP DETNOUN*?
PPnit,ADJ w "a"" f i sh"PREP "with"DET AD3*NOUI~ "fork""8"*** Start of disambiguation process:*** Evaluation of the first construct:solving problem: (WITH (FISH) (FORK))*** H4 has nothing useful to contribute in this case:solution for heuristic H4: ((ALL 0 ))*** Applying H2, the system parses all definitions of*** "fork," looking for PARTOF patterns.
None areComputational Linguistics Volume 13, Numbers 3-4, July-December 1987 257Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase Attachment*** found:solution for heuristic H2: ((PARTOF -0.3 ))solution for heuristic HI: ((INSTRUMENT -1 ))problem solution: ((OTHERS 0 )(PARTOF -0.3 )( INSTRUMENT-1 ))*** Evaluation of the second construct:solving problem: (WITH EAT FORK )solution for heuristic H4: ((ALL 0 ))solution for heuristic H2: ((PARTOF -1 ))*** Applying HI, the system looks again in the*** definitions of "fork", this time looking for*** INSTRUMENT patterns.
The following definition*** yields three possible preference terms: "take",*** "pitch," and "dig":I Q ~ I ~ Q m ~  ~ Q I ~ Q I ~ Q Q I ~ Q Q I Q Q Q I g  Q ~ i I ~ Q ~ I O g W I I Q I Q Q ~ I ~ O m Q  QNP* DET ADJ* "an""' ' "~ ?mp lesent"PP PREP "wXth"qUiNT Lj'P ADJ*COMJa' "or"LTP"pro::Ss"VERSe "usNt"?
7 PP &VPvP?
?
TwaoEewADW"fOZ wVERB*PREPPP~U N mCONu'~' ", or""~Sp"" t&klUS"PREP "u  4" nVEU*  "ut?~"P~E ")""pitch?~"VP VEU* "d~tuS"- .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.... .
.
.
.
.
.
.
.
*** Start of the disambiguation process:e IDDmIQi DmD  I ODI I IWI I D I  I I IQ IQDImDD IQg IQ DECL NP PROM* " i "VERB* "ate"NP DET AN J* "a"NOUN* "fish"?
PP PREP "with"NOUN* "bones" D D i e l i O D I N   I t  i g m m   l m   g l l i I Q  D I D D  l Q Q  Dm  !
*** Evaluation of the first construct:solving problem: (WITH (FISH) (BONE))solution for heuristic H4: ((ALL 0 ))*** Applying H2, the system parses all definitions of*** "bone," looking for PARTOF patterns.
The follow-*** ing definition is productive:NP* NOUN* "one"PP PREP "of"D~T ~J*EIP AD3*NOUN* "parts"?
PP PREPDETNOUN*T ?
PP"the""hard"Wtof"AD,?
* "the""skeleton"PREP "of"DET ADJ* "a"NOUN* "vertebrate" Q Q l a Q  D I D   l  l J D   l l l m  l l m  Q m l  g D l l l O O D m l m l  Q g l i l  g l l*** H3 is applied to each of the terms, attempting to link*** them to "eat";  a match is found for "take":solving problem: (PREF (EAT) (TAKE) VERB ) solu-tion for heuristic H3: ((TAKE 0.7 ))problem solution: ((TAKE 0.7 ))solving problem: (PREF (EAT) (PITCH) VERB )solution for heuristic H3: (((PITCH) -0.7 ))problem solution: (((PITCH) -0.7 ))solving problem: (PREF (EAT) (DIG) VERB )solution for heuristic H3: (((DIG) -0.7 ))problem solution: (((DIG) -0.7 ))solution for heuristic H 1: ((INSTRUMENT 0.49 ))problem solution: ((INSTRUMENT 0.49 )(OTHERS0 )(PARTOF -1 ))*** The final result lists all possible constructs,*** ordered according to decreasing certainty factors.
*** The answer says that "fork" goes more likely with*** "eat," the underlying relation being*** INSTRUMENT, with a certainty factor of 0.49:((WITH (EAT) (FORK))( INSTRUMENT 0.49)(OTHERS 0 )(PARTOF-1 ))((WITH (FISH) (FORK))(OTHERS 0 )(PARTOF -0.3)(INSTRUMENT-1 ))EXAMPLE 2.I ate a fish with bones.
*** Parse tree for the sentence:*** The terms found are "skeleton" and "vertebrate";*** they are checked in turn:solving problem: (PREF (FISH) (SKELETON)NOUN ) solution for heuristic H3: (((SKELETON) -0.7))problem solution: (((SKELETON) -0.7 ))solving problem: (PREF (FISH) (VERTEBRATE)NOUN ) solution for heuristic H3: (((VERTEBRATE)0.7 ))problem solution: (((VERTEBRATE) 0.7 ))solution for heuristic H2: ((PARTOF 0.49 ))solution for heuristic HI: ((INSTRUMENT -1 ))problem solution: ((PARTOF 0.49 )(OTHERS 0 )(IN-STRUMENT-1 ))*** Evaluation of the second construct:solving problem: (WITH (EAT) (BONE))solution for heuristic H4: ((ALL 0 ))solution for heuristic H2: ((PARTOF -1 ))*** Looking for INSTRUMENT patterns in the defini-*** tion of "bone",  H1 will find one productive defini-*** tion:*** The term found is "stiffen".
However, this will not*** provide any link with "eat":solving problem: (PREF (EAT) (STIFFEN) VERB )solution for heuristic H3: ((STIFFEN -0.7 ))problem solution: (((STIFFEN) -0.7 ))solution for heuristic HI: ((INSTRUMENT -0.3 ))258 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase AttachmentA t t t t l t l t t t l l l i l l l l l t l l t l l t l l l t i l t t t t t l l l t i l l l l l l l l l l l~* DET ~ ms"~ nltNlWVlDUI* %sed"T INFCLNIPn,r.o '~" | t / f  f in"I~ DI~ ~ nanCOt~k~ "or"NP NOON* "ctress"problem solution: ((OTHERS 0 )(INSTRUMENT-0.3 )(PARTOF -1 )) *** Final result:((WITH (FISH) (BONE))(PARTOF 0.49 )(OTHERS0 )(INSTRUMENT -1 ))((WITH (EAT) (BONE))(OTHERS 0 )(INSTRU-MENT -0.3 )(PARTOF -1 ))EXAMPLE 3.I ate a fish with my fingers.
*** Parse tree for thesentence:*** Start of the disambiguation process:DECL NP PROI~VERB* "ate"DETNOUN*?
PPt f l t lADjUre ttattt t f j .
sh t tPREP ttTttli~h ~tD~'f ADJ* "my"NOUN* "f:b~ers"*** Evaluation of the first construct:solving problem: (WITH (FISH) (FINGER))*** H4 this time catches the "my"  and answers:solution for heuristic H4: ((PARTOF -1 ))*** Applying H2, the system tries to find PARTOF*** patterns, but there are none:solution for heuristic H2: ((PARTOF -0.3 ))solution for heuristic H 1: ((INSTRUMENT - 1 ))problem solution: ((INSTRUMENT -1 )(PARTOF-\]))*** Evaluation of the second construct:solving problem: (WITH (EAT) (FINGER))solution for heuristic H4: ((ALL 0 ))solution for heuristic H2: ((PARTOF -1 ))*** There are no INSTRUMENT patterns either:solution for heuristic HI: ((INSTRUMENT -0.3 ))problem solution: ((OTHERS 0 )(INSTRUMENT-0.3 )(PARTOF -1 ))*** Final result:((WITH (EAT) (F INGER)) (OTHERS 0 )(IN-STRUMENT -0.3 )(PARTOF -1 ))((WITH (FISH) (FINGER))( INSTRUMENT -1)(PARTOF-1 ))APPENDIX 2: OPERATIONS ON HEURISTIC SOLUTIONSCumulative vidence is handled by the following oper-ation, which computes a new certainty factor out of twogiven ones N1 and N2 ("<-"indicates the value re-turned by the function):CUMULFarguments: two certainty factors N1 and N2result: a new certainty factorI.
if N1.EQUAL.-1 or N2.EQUAL.<-I, -1,2. if N1.EQUAL.I or N2.EQUAL.1 , < 1,3. if (NI+N2).EQUAL.0, <-0,4.
else <- NI+N2-(NI*N2)Before using this operation, however, we need toknow the certainty factor resulting from each subsolu-tion, which is obtained by combining the certaintyfactor directly assigned to that subsolution and thecertainty factor of the special subsolution ALL.
This isperformed by the operation of normalizing a solution,which can be described as follows:NORMALIZEargument: a solutionresult: this solution, normalized.1.
For each subsolution other than ALL, change itscertainty factor to the result obtained by com-bining it (using CUMULF) with the certaintyfactor of the ALL subsolution;2. replace the ALL subsolution, if present, byanother special subsolution called OTHERS,with the same certainty factor.The certainty factor of ALL cannot just be discardedafter normalization, because it applies also to all possi-ble subsolutions not mentioned in the current solution;this is taken into account by the special subsolutionOTHERS, whose certainty factor applies to all subso-lutions not currently mentioned in the solution.We can now define the operation of combining twosolutions:CUMULarguments: two solutionsresult: a new solution combining the two given onesas incremental evidence.I.
normalize the two solutions given as arguments;2.
For each subsolution (including OTHERS) oc-curing in both solutions, define a new certaintyfactor as the result of CUMULF applied to thecertainty factors given in the two solutions;3.
For each subsolution occuring in only one solu-tion but such that OTHERS appears in the othersolution, define a new certainty factor as theresult of CUMULF applied to the certainty fac-tors of that subsolution and of OTHERS;4.
For each subsolution ot processed in the twopreceding steps, keep its certainty factor.According to these rules, having a subsolution otmentioned in a a solution is equivalent to having itmentioned with a certainty factor of zero.The processing of conditional evidence (how theComputational Linguistics Volume 13, Numbers 3-4, July.December 1987 259Karen Jensen and Jean-Louis Binot Disambiguating Prepositional Phrase Attachmentsolution given by a heuristic is affected by solutions tosubgoals used to execute this heuristic) follows thefollowing rules:?
the certainty factor of a solution is defined as thehighest certainty factor (after normalization) of thesubsolutions of that solution.?
when the solution of a heuristic depends on somesubgoal solved by another heuristic, the resultingsolution for the heuristic will be obtained by theoperation COMBINE defined below:COMBINEarguments: two solutions, one for a main problemor goal, and one for a subgoal;result: a new solution for the goal.1.
Normalize the two argument solutions;2.
Find the highest certainty factor of the solutionfor the subgoal;3.
Assign to each subsolution in the goal a newcertainty factor obtained as the result of COM-BINEF applied to the old certainty factor and tothe value found in the previous tep.COMBINEFarguments: two certainty factors N1 and N2,result: a new certainty factor.1.
If N1.EQUAL.-1 or N2.EQUAL.-1, <-1,2.
<- else NI*N2REFERENCESBinot, Jean-Louis.
1985.
SABA: vers un systeme portable d'analysedu francais ecrit.
Ph.D. dissertation, University of Liege, Liege,Belgium.Dahlgren, K. and J. McDowell.
1986.
Using Commonsense Knowl-edge to Disambiguate Prepositional Phrase Modifiers.
Proceedingsof AAAI-86, Philadelphia, PA, August 1986.Funk & Wagnalls editorial staff.
1953.
Standard Handbook of Prep-ositions, Conjunctions, Relative Pronouns and Adverbs.
Funk &Wagnalls Co., New York.Heidorn, George E. 1975.
Augmented Phrase Structure Grammars.In: Nash-Webber and Schank, eds., Theoretical Issues in NaturalLanguage Processing.
Association for Computational Linguistics.Jensen, Karen.
1986.
Parsing Strategies in a Broad-coverage Gram-mar of English.
Research Report RC 12147, IBM Thomas J.Watson Research, Yorktown Heights, NY.Lytinen, Steven L. 1986.
Dynamically Combining Syntax and Seman-tics in Natural Language Processing.
Proceedings of AAA1-86,Philadelphia, PA, August 1986.Markowitz, Judith, Thomas Ahlswede, and Martha Evens.
1986.Semantically Significant Patterns in Dictionary Definitions.
Pro-ceedings of the 24th Annual Meeting of the ACL, ColumbiaUniversity\] June 1986.Schubert, Lenhart K. 1986.
Are There Preference Trade-offs inAttachment Decisions?
Proceedings of AAAI-86, Philadelphia,PA, August 1986.Shortliffe, E.H. 1976.
Computer-based Medical Consultation: MY-CIN.
Artificial Intelligence Series.
Elsevier.Wilks, Yorick.
1975.
An Intelligent Analyser and Understander ofEnglish.
Communications ofthe ACM 18(5).260 Computational Linguistics Volume 13, Numbers 3-4, July-December 1987
