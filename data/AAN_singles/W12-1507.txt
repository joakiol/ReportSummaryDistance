INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 31?39,Utica, May 2012. c?2012 Association for Computational LinguisticsGeneration for Grammar EngineeringClaire GardentCNRS, LORIA, UMR 7503Vandoeuvre-le`s-Nancy, F-54000, Franceclaire.gardent@loria.frGerman KruszewskiInria, LORIA, UMR 7503Villers-le`s-Nancy, F-54600, Francegerman.kruszewski@inria.frAbstractWhile in Computer Science, grammar engi-neering has led to the development of varioustools for checking grammar coherence, com-pletion, under- and over-generation, in Natu-ral Langage Processing, most approaches de-veloped to improve a grammar have focusedon detecting under-generation and to a muchlesser extent, over-generation.
We argue thatgeneration can be exploited to address otherissues that are relevant to grammar engineer-ing such as in particular, detecting grammarincompleteness, identifying sources of over-generation and analysing the linguistic cover-age of the grammar.
We present an algorithmthat implements these functionalities and wereport on experiments using this algorithm toanalyse a Feature-Based Lexicalised Tree Ad-joining Grammar consisting of roughly 1500elementary trees.1 IntroductionGrammar engineering, the task of developing largescale computational grammars, is known to be er-ror prone.
As the grammar grows, the interactionsbetween the rules and the lexicon become increas-ingly complex and the generative power of the gram-mar becomes increasingly difficult for the grammarwriter to predict.While in Computer Science, grammar engineer-ing has led to the development of various tools forchecking grammar coherence, completion, under-and over-generation (Klint et al, 2005), in Natu-ral Langage Processing, most approaches developedto improve a grammar have focused on detectingunder-generation (that is cases where the grammarand/or the lexicon fails to provide an analysis fora given, grammatical, input) and to a lesser degreeover-generation.In this paper, we argue that generation can be ex-ploited to address other issues that are relevant togrammar engineering.
In particular, we claim that itcan be used to:?
Check grammar completeness: for each gram-mar rule, is it possible to derive a syntacticallycomplete tree ?
That is, can each grammar rulebe used to derive a constituent.?
Analyse generation and over-generation: givensome time/recursion upper bounds, what doesthe grammar generate?
How much of the out-put is over-generation?
Which linguistic con-structions present in a language are covered bythe grammar?We present a generation algorithm called GRADE(GRAmmar DEbugger) that permits addressingthese issues.
In essence, this algorithm implementsa top-down grammar traversal guided with semanticconstraints and controlled by various parameteris-able constraints designed to ensure termination andlinguistic control.The GRADE algorithm can be applied to any gen-erative grammar i.e., any grammar which uses astart symbol and a set of production rules to gen-erate the sentences of the language described bythat grammar.
We present both an abstract descrip-tion of this algorithm and a concrete implementationwhich takes advantage of Definite Clause Grammars31to implement grammar traversal.
We then presentthe results of several experiments where we use theGRADE algorithm to examine the output of SEM-TAG, a Feature-Based Lexicalised Tree AdjoiningGrammar (FB-LTAG) for French.The paper is structured as follows.
Section 2summarises related work.
Section 3 presents theGRADE algorithm.
Section 4 introduces the gram-mar used for testing and describes an implementa-tion of GRADE for FB-LTAG.
Section 5 presentsthe results obtained by applying the GRADE algo-rithm to SEMTAG.
We show that it helps (i) to detectsources of grammar incompleteness (i.e., rules thatdo not lead to a complete derivation) and (ii) to iden-tify overgeneration and analyse linguistic coverage.Section 6 concludes.2 Related WorkTwo main approaches have so far been used to im-prove grammars: treebank-based evaluation and er-ror mining techniques.
We briefly review this workfocusing first, on approaches that are based on pars-ing and second, on those that exploit generation.Debugging Grammars using Parsing Over thelast two decades, Treebank-Based evaluation has be-come the standard way of evaluating parsers andgrammars.
In this framework (Black et al, 1991),the output of a parser is evaluated on a set of sen-tences that have been manually annotated with theirsyntactic parses.
Whenever the parse tree producedby the parser differs from the manual annotation, thedifference can be traced back to the parser (timeout,disambiguation component), the grammar and/or tothe lexicon.
Conversely, if the parser fails to re-turn an output, undergeneration can be traced backto missing or erroneous information in the grammaror/and in the lexicon.While it has supported the development of ro-bust, large coverage parsers, treebank based evalu-ation is limited to the set of syntactic constructionsand lexical items present in the treebank.
It alsofails to directly identify the most likely source ofparsing failures.
To bypass these limitations, errormining techniques have been proposed which per-mit detecting grammar and lexicon errors by pars-ing large quantities of data (van Noord, 2004; Sagotand de la Clergerie, 2006; de Kok et al, 2009).
Theoutput of this parsing process is then divided intotwo sets of parsed and unparsed sentences which areused to compute the ?suspicion rate?
of n-grams ofword forms, lemmas or part of speech tags wherebythe suspicion rate of an item indicates how likely agiven item is to cause parsing to fail.
Error miningwas shown to successfully help detect errors in thelexicon and to a lesser degree in the grammar.Debugging Grammars using Generation Mostof the work on treebank-based evaluation and errormining target undergeneration using parsing.
Re-cently however, some work has been done which ex-ploits generation and more specifically, surface real-isation to detect both under- and over-generation.Both (Callaway, 2003) and the Surface Realisa-tion (SR) task organised by the Generation Chal-lenge (Belz et al, 2011) evaluate the output of sur-face realisers on a set of inputs derived from thePenn Treebank.
As with parsing, these approachespermit detecting under-generation in that an inputfor which the surface realiser fails to produce asentence points to shortcomings either in the sur-face realisation algorithm or in the grammar/lexicon.The approach also permits detecting overgenerationin that a low BLEU score points to these inputsfor which the realiser produced a sentence that ismarkedly different from the expected answer.Error mining approaches have also been devel-oped using generation.
(Gardent and Kow, 2007) issimilar in spirit to the error mining approaches de-veloped for parsing.
Starting from a set of manu-ally defined semantic representations, the approachconsists in running a surface realiser on these repre-sentations; manually sorting the generated sentencesas correct or incorrect; and using the resulting twodatasets to detect grammatical structures that sys-tematically occur in the incorrect dataset.
The ap-proach however is only partially automatised sinceboth the input and the output need to be manuallyproduced/annotated.
More recently, (Gardent andNarayan, 2012) has shown how the fully automaticerror mining techniques used for parsing could beadapted to mine for errors in the output of a surfacerealiser tested on the SR input data.
In essence, theypresent an algorithm which enumerate the subtreesin the input data that frequently occur in surface re-alisation failure (the surface realiser fails to gener-32ate a sentence) and rarely occur in surface realisa-tion success.
In this way, they can identify subtreesin the input that are predominantly associated withgeneration failure.In sum, tree-bank based evaluation permits de-tecting over- and under-generation while error min-ing techniques permits identifying sources of er-rors; Treebank-based evaluation requires a refer-ence corpus while error mining techniques requirea way to sort good from bad ouput; and in all cases,generation-based grammar debugging requires inputto be provided (while for parsing, textual input isfreely available).Discussion The main difference between theGRADE approach and both error mining and tree-bank based evaluation is that GRADE is grammarbased.
No other input is required for the GRADEalgorithm to work than the grammar1.
Whereas ex-isting approaches identify errors by processing largeamounts of data, GRADE identifies errors by travers-ing the grammar.
In other words, while other ap-proaches assess the coverage of a parser or a genera-tor on a given set of input data, GRADE permits sys-tematically assessing the linguistic coverage and theprecision of the constructs described by the grammarindependently of any input data.Currently, the output of GRADE needs to be man-ually examined and the sources of error manuallyidentified.
Providing an automatic means of sortingGRADE ?s output into good and bad sentences is de-veloped however, it could be combined with errormining techniques so as to facilitate interpretation.3 The GraDE AlgorithmHow can we explore the quirks and corners of agrammar to detect inconsistencies and incorrect out-put?In essence, the GRADE algorithm performs a top-down grammar traversal and outputs the sentencesgenerated by this traversal.
It is grammar neutral inthat it can be applied to any generative grammar i.e.,any grammar which includes a start symbol and aset of production rules.
Starting from the string con-sisting of the start symbol, the GRADE algorithmrecursively applies grammar rules replacing one oc-1Although some semantic input is possible.currence of its left-hand side in the string by its right-hand side until a string that contains neither the startsymbol nor designated nonterminal symbols is pro-duced.Since NL grammars describe infinite sets of sen-tences however, some means must be provided tocontrol the search and output sets of sentences thatare linguistically interesting.
Therefore, the GRADEalgorithm is controlled by several user-defined pa-rameters designed to address termination (Given thatNL grammars usually describe an infinite set of sen-tences, how can we limit sentence generation toavoid non termination?
), linguistic control (How canwe control sentence generation so that the sentencesproduced cover linguistic variations that the linguistis interested in ?)
and readibility (How can we con-strain sentence generation in such a way that the out-put sentences are meaningful sentences rather thanjust grammatical ones?
).3.1 Ensuring terminationTo ensure termination, GRADE supports three user-defined control parameters which can be used simul-taneously or in isolation namely: a time out parame-ter; a restriction on the number and type of recursiverules allowed in any derivation; and a restriction onthe depth of the derivation tree.Each of these restrictions is implemented as a re-striction on the grammar traversal process as fol-lows.Time out.
The process halts when the time boundis reached.Recursive Rules.
For each type of recursive rule,a counter is created which is initialised to the valuesset by the user and decremented each time a recur-sive rule of the corresponding type is used.
Whenall counters are null, recursive rules can no longerbe used.
The type of a recursive rule is simply themain category expanded by that rule namely, N, NP,V, VP and S. In addition, whenever a rule is applied,the GRADE algorithm arbitrarily divides up the re-cursion quotas of a symbol among the symbol?s chil-dren.
If it happens to divide them a way that can-not be fulfilled, then it fails, backtracks, and dividesthem some other way.33Derivation Depth.
A counter is used to keep trackof the depth of the derivation tree and either halts (ifno other rule applies) or backtracks whenever the setdepth is reached.3.2 Linguistic Coverage and OutputReadibilityGRADE provides several ways of controlling the lin-guistic coverage and the readibility of the outputsentences.Modifiers.
As we shall show in Section 5, the re-cursivity constraints mentioned in the previous sec-tion can be used to constrain the type and the numberof modifiers present in the output.Root Rule.
Second, the ?root rule?
i.e., the rulethat is used to expand the start symbol can be con-strained in several ways.
The user can specify whichrule should be used; which features should labelthe lhs of that rule; which subcategorisation type itshould model; and whether or not it is a recursiverule.
For instance, given the FB-LTAG we are using,by specifying the root rule to be used, we can con-strain the generated sentences to be sentences con-taining an intransitive verb in the active voice com-bining with a canonical nominal subject.
If we onlyspecify the subcategorisation type of the root rulee.g., transitive, we can ensure that the main verb ofthe generated sentences is a transitive verb; And ifwe only constrain the features of the root rule to in-dicative mode and active voice, then we allow forthe generation of any sentence whose main verb isin the indicative mode and active voice.Input Semantics.
Third, in those cases where thegrammar is a reversible grammar associating sen-tences with both a syntactic structure and a seman-tic representation, the content of the generated sen-tences can be controlled by providing GRADE withan input semantics.
Whenever a core semanticsis specified, only rules whose semantics includesone or more literal(s) in the core semantics can beused.
Determiner rules however are selected inde-pendent of their semantics.
In this way, it is possi-ble to constrain the output sentences to verbalise agiven meaning without having to specify their fullsemantics (the semantic representations used in re-versible grammars are often intricate representationswhich are difficult to specify manually) and whileallowing for morphological variations (tense, num-ber, mode and aspect can be left unspecified and willbe informed by the calls to the lexicon embedded inthe DCG rules) as well as variations on determin-ers2.
For instance, the core semantics {run(E M),man(M)} is contained in, and therefore will gen-erate, the flat semantics for the sentences The manruns, The man ran, A man runs, A man ran, Thisman runs, My man runs, etc..4 ImplementationIn the previous section, we provided an abstract de-scription of the GRADE algorithm.
We now describean implementation of that algorithm tailored for FB-LTAGs equipped with a unification-based composi-tional semantics.
We start by describing the gram-mar used (SEMTAG), we then summarise the im-plementation of GRADE for FB-LTAG.4.1 SemTAGFor our experiments, we use the FB-LTAG describedin (Crabbe?, 2005; Gardent, 2008).
This grammar,called SEMTAG, integrates a unification-based se-mantics and can be used both for parsing and forgeneration.
It covers the core constructs for nonverbal constituents and most of the verbal construc-tions for French.
The semantic representations builtare MRSs (Minimal Recursion Semantic representa-tions, (Copestake et al, 2001)).More specifically, a tree adjoining grammar(TAG) is a tuple ?
?, N, I, A, S?
with ?
a set of ter-minals, N a set of non-terminals, I a finite set ofinitial trees, A a finite set of auxiliary trees, and Sa distinguished non-terminal (S ?
N ).
Initial treesare trees whose leaves are labeled with substitutionnodes (marked with a downarrow) or terminal cate-gories3.
Auxiliary trees are distinguished by a footnode (marked with a star) whose category must bethe same as that of the root node.2The rules whose semantics is not checked during derivationare specified as a parameter of the system and can be modifiedat will e.g., to include adverbs or auxiliaries.
Here we choosedto restrict underspecification to determiners.3Due to space limitation we here give a very sketchy defini-tion of FB-LTAG.
For a more detailed presentation, see (Vijay-Shanker and Joshi, 1988).34Two tree-composition operations are used to com-bine trees: substitution and adjunction.
Substitu-tion inserts a tree onto a substitution node of someother tree while adjunction inserts an auxiliary treeinto a tree.
In a Feature-Based Lexicalised TAG(FB-LTAG), tree nodes are furthermore decoratedwith two feature structures (called top and bottom)which are unified during derivation; and each treeis anchored with a lexical item.
Figure 1 shows anexample toy FB-LTAG with unification semantics.NPjJohnl0:proper q(c hr hs)l1:named(j john)qeq(hr l1)SbNP?c VPbaVarunslv:run(a,j)VPxoften VP*xlo:often(x)?
l0:proper q(c hr hs) l1:named(j john), qeq(hr, l1),lv:run(a,j), lv:often(a)Figure 1: MRS for ?John often runs?4.2 GraDe for FB-LTAGThe basic FB-LTAG implementation of GRADE isdescribed in detail in (Gardent et al, 2011; Gar-dent et al, 2010).
In brief, this implementationtakes advantage of the top-down, left-to-right, gram-mar traversal implemented in Definite Clause Gram-mars by translating the FB-LTAG to a DCG.
In theDCG formalism, a grammar is represented as a set ofProlog clauses and Prolog?s query mechanism pro-vides a built-in top-down, depth-first, traversal of thegrammar.
In addition, the DCG formalism allowsarbitrary Prolog goals to be inserted into a rule.
Toimplement a controlled, top-down grammar traver-sal of SEMTAG, we simply convert SEMTAGto aDefinite Clause Grammar (DCG) wherein arbitraryProlog calls are used both to ground derivations withlexical items and to control Prolog?s grammar traver-sal so as to respect the user defined constraints onrecursion and on linguistic coverage.
In addition,we extended the approach to handle semantic con-straints (i.e., to allow for an input semantic to con-strain the traversal) as discussed in Section 3.
Thatis, for a subset of the grammar rules, a rule will onlybe applied if its semantics subsumes a literal in theinput semantics.For more details, on the FB-LTAG implementa-tion of the GRADE algorithm and of the conversionfrom FB-LTAG to DCG, we refer the reader to (Gar-dent et al, 2011; Gardent et al, 2010).5 Grammar AnalysisDepending on which control parameters are used,the GRADE algorithm can be used to explore thegrammar from different viewpoints.
In what fol-lows, we show that it can be used to check grammarcompleteness (Can all rules in the grammar be usedso as to derive a constituent?
); to inspect the vari-ous possible realisations of syntactic functors and oftheir arguments (e.g., Are all possible syntactic real-isations of the verb and of its arguments generatedand correct?
); to explore the interactions betweenbasic clauses and modifiers; and to zoom in on themorphological and syntactic variants of a given coresemantics (e.g., Does the grammar correctly accountfor all such variants ?
).5.1 Checking for Grammar CompletenessWe first use GRADE to check, for each grammarrule, whether it can be used to derive a completeconstituent i.e., whether a derivation can be foundsuch that all leaves of the derivation tree are ter-minals (words).
Can all trees anchored by a verbfor instance, be completed to build a syntacticallycomplete clause?
Trees that cannot yield a completeconstituent points to gaps or inconsistencies in thegrammar.To perform this check, we run the GRADE algo-rithm on verb rules, allowing for up to 1 adjunc-tion on either a noun, a verb or a verb phrase andhalting when either a derivation has been found orall possible rule combinations have been tried.
Ta-ble 1 shows the results per verb family4.
As can beseen, there are strong differences between the fam-ilies with e.g., 80% of the trees failing to yield aderivation in the n0Vs1int (Verbs with interrogativesentential complement) family against 0% in the ilV4The notational convention for verb types is from XTAG andreads as follows.
Subscripts indicate the thematic role of theverb argument.
n indicates a nominal, Pn a PP and s a sententialargument.
pl is a verbal particle.
Upper case letters describethe syntactic functor type: V is a verb, A an adjective and BEthe copula.
For instance, n0Vn1 indicates a verb taking twonominal arguments (e.g., like) .35Tree Family Trees Fails Fails/TreesCopulaBe 60 1 1%ilV 2 0 0%n0V 10 0 0%n0ClV 9 0 0%n0ClVn1 45 2 4%n0ClVden1 36 3 8%n0ClVpn1 29 3 10%n0Vn1 84 3 3%n0Vn1Adj2 24 6 25%n0Van1 87 3 3%n0Vden1 38 3 7%n0Vpn1 30 3 10%ilVcs1 2 0 0%n0Vcs1 30 23 74%n0Vas1 15 10 66%n0Vn1Adj2 24 0 0%s0Vn1 72 9 12%n0Vs1int 15 12 80%n0Vn1n2 24 0 0%n0Vn1an2 681 54 7%Table 1: Checking for Gaps in the Grammar(impersonal with expletive subject, ?it rains?)
andthe n0V (intransitive, ?Tammy sings?).
In total, ap-proximatively 10% (135/1317) of the grammar rulescannot yield a derivation.5.2 Functor/Argument DependenciesTo check grammar completeness, we need only findone derivation for any given tree.
To assess the de-gree to which the grammar correctly generates allpossible realisations associated with a given syn-tactic functor however, all realisations generated bythe grammar need to be produced.
To restrict theoutput to sentences illustrating functor/argument de-pendencies (no modifiers), we constrain adjunctionto the minimum required by each functor.
In mostcases, this boils down to setting the adjunction coun-ters to null for all categories.
One exception areverbs taking a sentential argument which require oneS adjunction.
We also allow for one N-adjunctionand one V-adjunction to allow for determiners andthe inverted subject clitic (t?il).
In addition, the lex-icon is restricted to avoid lexical or morphologicalvariants.We show below some of the well-formed sen-tences output by GRADE for the n0V (intransitiveverbs) family.Elle chante (She sings), La tatou chante-t?elle?
(Does the armadillo sing?
),La tatou chante (The armadillo sings ),La tatou qui chante (The armadillo whichsings ), Chacun chante -t?il (Does every-one sing?
), Chacun chante (Everyonesings ), Quand chante chacun?
(Whendoes everyone sing?
), Quand chante latatou?
(When does the armadillo sing?)
Quand chante quel tatou?
(When doeswhich armadillo sing?
), Quand chanteTammy?
(When does Tammy sing?
),Chante-t?elle?
(Does she sing? )
Chante-t?il?
(Does he sing?
), Chante!
(Sing!
), Quel tatou chante ?
(Which armadillosing?
), Quel tatou qui chante ..?
(Whicharmadillo who sings ..? )
Tammy chante-t?elle?
(Does Tammy sing?
), Tammychante (Tammy sings ), une tatou quichante chante (An armadillo which singssings ), C?est une tatou qui chante (It is anarmadillo which sings ), ...The call on this family returned 55 distinct MRSsand 65 distinct sentences of which only 28 were cor-rect.
Some of the incorrect cases are shown below.They illustrate the four main sources of overgener-ation.
The agreement between the inverted subjectclitic and the subject fails to be enforced (a); the in-verted nominal subject fails to require a verb in theindicative mode (b); the inverted subject clitic failsto be disallowed in embedded clauses (c); the inter-rogative determiner quel fails to constrain its nomi-nal head to be a noun (d,e).
(a) Chacun chante-t?elle?
(Everyonesings?)
(b) Chante?e chacun?
(Sung every-one?)
(c) La tatou qui chante-t?elle?
(Thearmadillo which does she sing?)
(d) Quelchacun chante ?
(Which everyone sings?
)(e) quel tammy chante ?
(Which Tammysings?
)5.3 Interactions with ModifiersOnce basic functor/argument dependencies havebeen verified, adjunction constraints can be used to36explore the interactions between e.g., basic clausesand modification5.
Allowing for N-adjunctions forinstance, will produce sentences including determin-ers and adjectives.
Similarly, allowing for V ad-junction will permit for auxiliaries and adverbs tobe used; and allowing for VP or S adjunctions willlicence the use of raising verbs and verbs subcate-gorising for sentential argument.We queried GRADE for derivations rooted in n0V(intransitive verbs) and with alternatively, 1N, 2N,1V and 1VP adjunction.
Again a restricted lexiconwas used to avoid structurally equivalent but lexi-cally distinct variants.
The following table showsthe number of sentences output for each query.0 1S 1VP 1V 1N 2N36 170 111 65 132 638As the examples below show, the generated sen-tences unveil two further shortcomings in the gram-mar: the inverted subject clitic fails to be constrainedto occur directly after the verb (1) and the order andcompatibility of determiners are unrestricted (2).
(1) a. Semble-t?il chanter?
/ * Semble chantert?il?
(Does he seems to sing?)b.
Chante-t?il dans Paris?
/ * Chante dansParis-t?il?
(Does he sing in Paris?)c.
Chante-t?il beaucoup?
/ * Chantebeaucoup-t?il?
(Does he sing a lot?)d.
Veut-t?il que Tammy chante?
/ * Veut queTammy chante-t?il?
(Does he want thatTammy sings?
(2) * Un quel tatou, *Quel cette tatou, Ma quelletatou (Un which armadillo, Which this ar-madillo, My which armadillo)5.4 Inspecting Coverage and CorrectnessIn the previous sections, GRADE was used to gen-erate MRSs and sentences ex nihilo.
As mentionedabove however, a core semantics can be used to re-strict the set of output sentences to sentences whoseMRS include this core semantics.
This is useful for5Recall that in FB-LTAG, adjunction is the operation whichpermits applying recursive rules (i.e., auxiliary trees).
Henceallowing for adjunctions amounts to allowing for modificationwith the exception already noted above of certain verbs subcat-egorising for sentential arguments.Tree Family MRS Sent.
S/MRSilV 7 52 7.4n0V 65 161 2.4n0ClV 30 62 2.0n0ClVn1 20 25 1.25n0ClVden1 10 15 1.5n0ClVpn1 40 63 1.57n0Vn1 20 110 5.5n0Van1 30 100 3.33n0Vden1 5 15 3.00n0Vpn1 25 76 3.04ilVcs1 1 1 1.00n0Vcs1 200 660 3.3n0Vas1 35 120 3.42n0Vn1Adj2 10 15 1.5s0Vn1 4 24 6.00n0Vn1n2 10 48 4.80n0Vn1an2 5 45 9.00Table 2: Producing Variantsinstance, to systematically inspect all variations out-put by the grammar on a given input.
These varia-tions include all morphological variations supportedby the lexicon (number, tense, mode variations) andthe syntactic variations supported by the grammarfor the same MRSs (e.g., active/passive).
It also in-cludes the variations supported by GRADE in thatsome rules are not checked for semantic compati-bility thereby allowing for additional materials to beadded.
In effect, GRADE allows for the inclusion ofarbitrary determiners and auxiliaries.Table 2 shows the number of MRSs and sen-tences output for each verb family given a match-ing core semantics and a morphological lexicon in-cluding verbs in all simple tenses (3rd person only)and nouns in singular and plural6.
The ratio S/M ofsentences on MRSs produced by one GRADE callshows how the underspecified core semantics per-mits exploring a larger number of sentences gener-ated by the grammar than could be done by gener-ating from fully specified MRSs.
For the n0Vn1an2class, for instance, the GRADE call permits generat-ing 9 times more sentences in average than generat-ing from a single MRS.6The lexicon used in this experiment includes more mor-phological variants than in the experiment of Section 5.2 wherethe focus was on syntactic rather than morphological variants.Hence the different number of generated sentences.376 ConclusionWhen using a grammar for generation, it is essen-tial, not only that it has coverage (that it does notundergenerate) but also that it be precise (that itdoes not overgenerate).
Nonetheless, relatively lit-tle work has been done on how to detect overgener-ation.
In this paper, we presented an algorithm anda methodology to explore the sentences generatedby a grammar; we described an implementation ofthis algorithm based on DCGs (GRADE ); and weillustrated its impact by applying it to an existinggrammar.
We showed that GRADE could be usedto explore a grammar from different viewpoints: tofind gaps or inconsistencies in the rule system; tosystematically analyse the grammar account of func-tor/argument dependencies; to explore the interac-tion between base constructions and modifiers; andto verify the completeness and correctness of syn-tactic and morphological variants.There are many directions in which to pursuethis research.
One issue is efficiency.
Unsurpris-ingly, the computational complexity of GRADE isformidable.
For the experiments reported here, run-times are fair (a few seconds to a few minutes de-pending on how much output is required and on thesize of the grammar and of the lexicon).
As the com-plexity of the generated sentences and the size of thelexicons grow, however, it is clear that runtimes willbecome unpractical.
We are currently using YAPProlog tabling mechanism for storing intermediateresults.
It would be interesting however to comparethis with the standard tabulating algorithms used forparsing and surface realisation.Another interesting issue is that of the interac-tion between GRADE and error mining.
As men-tioned in Section 2, GRADE could be usefully com-plemented by error mining techniques as a meansto automatically identify the most probable causesof errors highlighted by GRADE and thereby of im-proving the grammar.
To support such an integrationhowever, some means must be provided of sortingGRADE ?s output into ?good?
and ?bad?
output i.e.,into sentences that are grammatical and sentencesthat are over-generated by the grammar.
We plan toinvestigate whether language models could be usedto identify those sentences that are most probablyincorrect.
In a first step, simple and highly con-strained input would be used to generate from thegrammar and the lexicon a set of correct sentencesusing GRADE .
Next these sentences would be usedto train a language model which could be used todetect incorrect sentences produced by GRADE onmore complex, less constrained input.Other issues we are currently pursueing are theuse of GRADE (i) for automating the creation ofgrammar exercises for learners of french and (ii) forcreating a bank of MRSs to be used for the evalua-tion and comparison of data-to-text generators.
Thevarious degrees of under-specification supported byGRADE permit producing either many sentences outof few input (e.g., generate all basic clauses whoseverb is of a given subcategorisation type as illus-trated in Section 5.2); or fewer sentences out a moreconstrained input (e.g., producing all syntactic andmorphological variants verbalising a given input se-mantics).
We are currently exploring how seman-tically constrained GRADE calls permit producingvariants of a given meaning; and how these vari-ants can be used to automatically construct gram-mar exercises which illustrate the distinct syntac-tic and morphological configurations to be acquiredby second language learners.
In contrast, more un-derspecified GRADE calls can be used to automat-ically build a bank of semantic representations andtheir associated sentences which could form the ba-sis for an evaluation of data-to-text surface realis-ers.
The semantics input to GRADE are simplifiedrepresentations of MRSs.
During grammar traver-sal, GRADE reconstructs not only a sentence andits associated syntactic tree but also its full MRS.As a result, it is possible to produce a generationbank which, like the Redwook Bank, groups to-gether MRSs and the sentences verbalising theseMRSs.
This bank however would reflect the linguis-tic coverage of the grammar rather than the linguis-tic constructions present in the corpus parsed to pro-duce the MRS.
It would thus provide an alternativeway to test the linguistic coverage of existing surfacerealisers.AcknowledgmentsThe research presented in this paper was partiallysupported by the European Fund for Regional De-velopment within the framework of the INTERREGIVA Allegro Project.38ReferencesAnja Belz, Michael White, Dominic Espinosa, Eric Kow,Deirdre Hogan, and Amanda Stent.
2011.
The firstsurface realisation shared task: Overview and evalua-tion results.
In Proc.
of the 13th European Workshopon Natural Language Generation.E.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, Ingria R., F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitatively comparing the syntactic cov-erage of english grammars.
In Proceedings of theDARPA Speech and Natural Language Workshop,page 306311.Charles B. Callaway.
2003.
Evaluating coverage forlarge symbolic NLG grammars.
In 18th IJCAI, pages811?817, Aug.Ann Copestake, Alex Lascarides, and Dan Flickinger.2001.
An algebra for semantic construction inconstraint-based grammars.
In Proceedings of the39th Annual Meeting of the Association for Compu-tational Linguistics, Toulouse, France.Benoit Crabbe?.
2005.
Reprsentation informatique degrammaires d?arbres fortement lexicalise?es : le cas dela grammaire d?arbres adjoints.
Ph.D. thesis, NancyUniversity.Danie?l de Kok, Jianqiang Ma, and Gertjan van Noord.2009.
A generalized method for iterative error miningin parsing results.
In ACL2009 Workshop GrammarEngineering Across Frameworks (GEAF), Singapore.Claire Gardent and Eric Kow.
2007.
Spotting overgener-ation suspect.
In 11th European Workshop on NaturalLanguage Generation (ENLG).Claire Gardent and Shashi Narayan.
2012.
Error miningon dependency trees.
In Proceedings of ACL.Claire Gardent, Benjamin Gottesman, and Laura Perez-Beltrachini.
2010.
Benchmarking surface realisers.
InCOLING 2010 (Poster Session), Beijing, China.Claire Gardent, Benjamin Gottesman, and Laura Perez-Beltrachini.
2011.
Using regular tree grammar toenhance surface realisation.
Natural Language En-gineerin, 17:185?201.
Special Issue on Finite StateMethods and Models in Natural Language Processing.Claire Gardent.
2008.
Integrating a unification-basedsemantics in a large scale lexicalised tree adjoininggrammar for french.
In COLING?08, Manchester, UK.Paul Klint, Ralf La?mmel, and Chris Verhoef.
2005.Toward an engineering discipline for grammarware.ACM Transactions on Software Engineering Method-ology, 14(3):331?380.Benoit Sagot and Eric de la Clergerie.
2006.
Error min-ing in parsing results.
In ACL, editor, Proceedings ofthe ACL 2006, pages 329?336, Morristown, NJ, USA.Gertjan van Noord.
2004.
Error mining for wide-coverage grammar engineering.
In ACL, editor, Pro-ceedings of the ACL 2004, pages 446?454, Morris-town, NJ, USA.K.
Vijay-Shanker and Aravind Joshi.
1988.
FeatureStructures Based Tree Adjoining Grammars.
Proceed-ings of the 12th conference on Computational linguis-tics, 55:v2.39
