Proceedings of the ACL 2010 Conference Short Papers, pages 194?199,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsSparsity in Dependency Grammar InductionJennifer Gillenwater and Kuzman GanchevUniversity of PennsylvaniaPhiladelphia, PA, USA{jengi,kuzman}@cis.upenn.eduJo?o Gra?aL2F INESC-IDLisboa, Portugaljoao.graca@l2f.inesc-id.ptFernando PereiraGoogle Inc.Mountain View, CA, USApereira@google.comBen TaskarUniversity of PennsylvaniaPhiladelphia, PA, USAtaskar@cis.upenn.eduAbstractA strong inductive bias is essential in un-supervised grammar induction.
We ex-plore a particular sparsity bias in de-pendency grammars that encourages asmall number of unique dependencytypes.
Specifically, we investigatesparsity-inducing penalties on the poste-rior distributions of parent-child POS tagpairs in the posterior regularization (PR)framework of Gra?a et al (2007).
In ex-periments with 12 languages, we achievesubstantial gains over the standard expec-tation maximization (EM) baseline, withaverage improvement in attachment ac-curacy of 6.3%.
Further, our methodoutperforms models based on a standardBayesian sparsity-inducing prior by an av-erage of 4.9%.
On English in particular,we show that our approach improves onseveral other state-of-the-art techniques.1 IntroductionWe investigate an unsupervised learning methodfor dependency parsing models that imposes spar-sity biases on the dependency types.
We assumea corpus annotated with POS tags, where the taskis to induce a dependency model from the tags forcorpus sentences.
In this setting, the type of a de-pendency is defined as a pair: tag of the dependent(also known as the child), and tag of the head (alsoknown as the parent).
Given that POS tags are de-signed to convey information about grammaticalrelations, it is reasonable to assume that only someof the possible dependency types will be realizedfor a given language.
For instance, in English itis ungrammatical for nouns to dominate verbs, ad-jectives to dominate adverbs, and determiners todominate almost any part of speech.
Thus, the re-alized dependency types should be a sparse subsetof all possible types.Previous work in unsupervised grammar induc-tion has tried to achieve sparsity through priors.Liang et al (2007), Finkel et al (2007) and John-son et al (2007) proposed hierarchical Dirichletprocess priors.
Cohen et al (2008) experimentedwith a discounting Dirichlet prior, which encour-ages a standard dependency parsing model (seeSection 2) to limit the number of dependent typesfor each head type.Our experiments show a more effective sparsitypattern is one that limits the total number of uniquehead-dependent tag pairs.
This kind of sparsitybias avoids inducing competition between depen-dent types for each head type.
We can achieve thedesired bias with a constraint on model posteri-ors during learning, using the posterior regulariza-tion (PR) framework (Gra?a et al, 2007).
Specifi-cally, to implement PR we augment the maximummarginal likelihood objective of the dependencymodel with a term that penalizes head-dependenttag distributions that are too permissive.Although not focused on sparsity, several otherstudies use soft parameter sharing to couple dif-ferent types of dependencies.
To this end, Cohenet al (2008) and Cohen and Smith (2009) inves-tigated logistic normal priors, and Headden III etal.
(2009) used a backoff scheme.
We compare totheir results in Section 5.The remainder of this paper is organized as fol-194lows.
Section 2 and 3 review the models and sev-eral previous approaches for learning them.
Sec-tion 4 describes learning with PR.
Section 5 de-scribes experiments across 12 languages and Sec-tion 6 analyzes the results.
For additional detailson this work see Gillenwater et al (2010).2 Parsing ModelThe models we use are based on the generative de-pendency model with valence (DMV) (Klein andManning, 2004).
For a sentence with tags x, theroot POS r(x) is generated first.
Then the modeldecides whether to generate a right dependent con-ditioned on the POS of the root and whether otherright dependents have already been generated forthis head.
Upon deciding to generate a right de-pendent, the POS of the dependent is selected byconditioning on the head POS and the direction-ality.
After stopping on the right, the root gener-ates left dependents using the mirror reversal ofthis process.
Once the root has generated all itsdependents, the dependents generate their own de-pendents in the same manner.2.1 Model ExtensionsFor better comparison with previous work weimplemented three model extensions, borrowedfrom Headden III et al (2009).
The first exten-sion alters the stopping probability by condition-ing it not only on whether there are any depen-dents in a particular direction already, but also onhow many such dependents there are.
When wetalk about models with maximum stop valency Vs= S, this means it distinguishes S different cases:0, 1, .
.
.
, S?2, and?
S?1 dependents in a givendirection.
The basic DMV has Vs = 2.The second model extension we implement isanalogous to the first, but applies to dependent tagprobabilities instead of stop probabilities.
Again,we expand the conditioning such that the modelconsiders how many other dependents were al-ready generated in the same direction.
When wetalk about a model with maximum child valencyVc = C, this means we distinguish C differentcases.
The basic DMV has Vc = 1.
Since thisextension to the dependent probabilities dramati-cally increases model complexity, the third modelextension we implement is to add a backoff for thedependent probabilities that does not condition onthe identity of the parent POS (see Equation 2).More formally, under the extended DMV theprobability of a sentence with POS tags x and de-pendency tree y is given by:p?
(x,y) = proot(r(x))?Yy?ypstop(false | yp, yd, yvs)pchild(yc | yp, yd, yvc)?Yx?xpstop(true | x, left, xvl) pstop(true | x, right, xvr )(1)where y is the dependency of yc on head yp in di-rection yd, and yvc , yvs , xvr , and xvl indicate va-lence.
For the third model extension, the backoffto a probability not dependent on parent POS canbe formally expressed as:?pchild(yc | yp, yd, yvc) + (1?
?
)pchild(yc | yd, yvc) (2)for ?
?
[0, 1].
We fix ?
= 1/3, which is a crudeapproximation to the value learned by Headden IIIet al (2009).3 Previous Learning ApproachesIn our experiments, we compare PR learningto standard expectation maximization (EM) andto Bayesian learning with a sparsity-inducingprior.
The EM algorithm optimizes marginal like-lihood L(?)
= log?Y p?
(X,Y), where X ={x1, .
.
.
,xn} denotes the entire unlabeled corpusand Y = {y1, .
.
.
,yn} denotes a set of corre-sponding parses for each sentence.
Neal and Hin-ton (1998) view EM as block coordinate ascent ona function that lower-bounds L(?).
Starting froman initial parameter estimate ?0, the algorithm it-erates two steps:E : qt+1 = argminqKL(q(Y) ?
p?t(Y | X)) (3)M : ?t+1 = argmax?Eqt+1 [log p?
(X,Y)] (4)Note that the E-step just sets qt+1(Y) =p?t(Y|X), since it is an unconstrained minimiza-tion of a KL-divergence.
The PR method wepresent modifies the E-step by adding constraints.Besides EM, we also compare to learning withseveral Bayesian priors that have been applied tothe DMV.
One such prior is the Dirichlet, whosehyperparameter we will denote by ?.
For ?
< 0.5,this prior encourages parameter sparsity.
Cohenet al (2008) use this method with ?
= 0.25 fortraining the DMV and achieve improvements overbasic EM.
In this paper we will refer to our ownimplementation of the Dirichlet prior as the ?dis-counting Dirichlet?
(DD) method.
In addition to195the Dirichlet, other types of priors have been ap-plied, in particular logistic normal priors (LN) andshared logistic normal priors (SLN) (Cohen et al,2008; Cohen and Smith, 2009).
LN and SLN aimto tie parameters together.
Essentially, this has asimilar goal to sparsity-inducing methods in that itposits a more concise explanation for the grammarof a language.
Headden III et al (2009) also im-plement a sort of parameter tying for the E-DMVthrough a learning a backoff distribution on childprobabilities.
We compare against results from allthese methods.4 Learning with Sparse PosteriorsWe would like to penalize models that predict alarge number of distinct dependency types.
To en-force this penalty, we use the posterior regular-ization (PR) framework (Gra?a et al, 2007).
PRis closely related to generalized expectation con-straints (Mann and McCallum, 2007; Mann andMcCallum, 2008; Bellare et al, 2009), and is alsoindirectly related to a Bayesian view of learningwith constraints on posteriors (Liang et al, 2009).The PR framework uses constraints on posteriorexpectations to guide parameter estimation.
Here,PR allows a natural and tractable representation ofsparsity constraints based on edge type counts thatcannot easily be encoded in model parameters.
Weuse a version of PR where the desired bias is apenalty on the log likelihood (see Ganchev et al(2010) for more details).
For a distribution p?, wedefine a penalty as the (generic) ?-norm of expec-tations of some features ?:||Ep?
[?(X,Y)]||?
(5)For computational tractability, rather than penaliz-ing the model?s posteriors directly, we use an aux-iliary distribution q, and penalize the marginal log-likelihood of a model by the KL-divergence of p?from q, plus the penalty term with respect to q.For a fixed set of model parameters ?
the full PRpenalty term is:minqKL(q(Y) ?
p?
(Y|X)) + ?
||Eq[?(X,Y)]||?
(6)where ?
is the strength of the regularization.
PRseeks to maximize L(?)
minus this penalty term.The resulting objective can be optimized by a vari-ant of the EM (Dempster et al, 1977) algorithmused to optimize L(?
).4.1 `1/`?
RegularizationWe now define precisely how to count dependencytypes.
For each child tag c, let i range over an enu-meration of all occurrences of c in the corpus, andlet p be another tag.
Let the indicator ?cpi(X,Y)have value 1 if p is the parent tag of the ith occur-rence of c, and value 0 otherwise.
The number ofunique dependency types is then:Xcpmaxi?cpi(X,Y) (7)Note there is an asymmetry in this count: occur-rences of child type c are enumerated with i, butall occurrences of parent type p are or-ed in ?cpi.That is, ?cpi = 1 if any occurrence of p is the par-ent of the ith occurrence of c. We will refer to PRtraining with this constraint as PR-AS.
Instead ofcounting pairs of a child token and a parent type,we can alternatively count pairs of a child tokenand a parent token by letting p range over all to-kens rather than types.
Then each potential depen-dency corresponds to a different indicator ?cpij ,and the penalty is symmetric with respect to par-ents and children.
We will refer to PR trainingwith this constraint as PR-S.
Both approaches per-form very well, so we report results for both.Equation 7 can be viewed as a mixed-normpenalty on the features ?cpi or ?cpij : the sum cor-responds to an `1 norm and the max to an `?norm.
Thus, the quantity we want to minimizefits precisely into the PR penalty framework.
For-mally, to optimize the PR objective, we completethe following E-step:argminqKL(q(Y)||p?
(Y|X)) + ?XcpmaxiEq[?
(X,Y)],(8)which can equivalently be written as:minq(Y),?cpKL(q(Y) ?
p?
(Y|X)) + ?Xcp?cps.
t. ?cp ?
Eq[?
(X,Y)](9)where ?cp corresponds to the maximum expecta-tion of ?
over all instances of c and p. Note thatthe projection problem can be solved efficiently inthe dual (Ganchev et al, 2010).5 ExperimentsWe evaluate on 12 languages.
Following the ex-ample of Smith and Eisner (2006), we strip punc-tuation from the sentences and keep only sen-tences of length ?
10.
For simplicity, for all mod-els we use the ?harmonic?
initializer from Klein196Model EM PR Type ?DMV 45.8 62.1 PR-S 1402-1 45.1 62.7 PR-S 1002-2 54.4 62.9 PR-S 803-3 55.3 64.3 PR-S 1404-4 55.1 64.4 PR-AS 140Table 1: Attachment accuracy results.
Column 1: Vc-Vs used for the E-DMV models.
Column 3: Best PR re-sult for each model, which is chosen by applying each ofthe two types of constraints (PR-S and PR-AS) and trying?
?
{80, 100, 120, 140, 160, 180}.
Columns 4 & 5: Con-straint type and ?
that produced the values in column 3.and Manning (2004), which we refer to as K&M.We always train for 100 iterations and evaluateon the test set using Viterbi parses.
Before eval-uating, we smooth the resulting models by addinge?10 to each learned parameter, merely to removethe chance of zero probabilities for unseen events.
(We did not tune this as it should make very littledifference for final parses.)
We score models bytheir attachment accuracy ?
the fraction of wordsassigned the correct parent.5.1 Results on EnglishWe start by comparing English performance forEM, PR, and DD.
To find ?
for DD we searchedover five values: {0.01, 0.1, 0.25, 1}.
We found0.25 to be the best setting for the DMV, the sameas found by Cohen et al (2008).
DD achieves ac-curacy 46.4% with this ?.
For the E-DMV wetested four model complexities with valencies Vc-Vs of 2-1, 2-2, 3-3, and 4-4.
DD?s best accuracywas 53.6% with the 4-4 model at ?
= 0.1.
Acomparison between EM and PR is shown in Ta-ble 1.
PR-S generally performs better than the PR-AS for English.
Comparing PR-S to EM, we alsofound PR-S is always better, independent of theparticular ?, with improvements ranging from 2%to 17%.
Note that in this work we do not performthe PR projection at test time; we found it detri-mental, probably due to a need to set the (corpus-size-dependent) ?
differently for the test set.
Wealso note that development likelihood and the bestsetting for ?
are not well-correlated, which un-fortunately makes it hard to pick these parameterswithout some supervision.5.2 Comparison with Previous WorkIn this section we compare to previously publishedunsupervised dependency parsing results for En-glish.
It might be argued that the comparison isunfair since we do supervised selection of modelLearning Method Accuracy?
10 ?
20 allPR-S (?
= 140) 62.1 53.8 49.1LN families 59.3 45.1 39.0SLN TieV & N 61.3 47.4 41.4PR-AS (?
= 140) 64.4 55.2 50.5DD (?
= 1, ?
learned) 65.0 (?5.7)Table 2: Comparison with previous published results.
Rows2 and 3 are taken from Cohen et al (2008) and Cohen andSmith (2009), and row 5 from Headden III et al (2009).complexity and regularization strength.
However,we feel the comparison is not so unfair as we per-form only a very limited search of the model-?space.
Specifically, the only values of ?
we searchover are {80, 100, 120, 140, 160, 180}.First, we consider the top three entries in Ta-ble 2, which are for the basic DMV.
The first en-try was generated using our implementation ofPR-S.
The second two entries are logistic nor-mal and shared logistic normal parameter tying re-sults (Cohen et al, 2008; Cohen and Smith, 2009).The PR-S result is the clear winner, especially aslength of test sentences increases.
For the bot-tom two entries in the table, which are for the E-DMV, the last entry is best, corresponding to us-ing a DD prior with ?
= 1 (non-sparsifying), butwith a special ?random pools?
initialization and alearned weight ?
for the child backoff probabil-ity.
The result for PR-AS is well within the vari-ance range of this last entry, and thus we conjec-ture that combining PR-AS with random pools ini-tialization and learned ?
would likely produce thebest-performing model of all.5.3 Results on Other LanguagesHere we describe experiments on 11 additionallanguages.
For each we set ?
and model complex-ity (DMV versus one of the four E-DMV exper-imented with previously) based on the best con-figuration found for English.
This likely will notresult in the ideal parameters for all languages, butprovides a realistic test setting: a user has avail-able a labeled corpus in one language, and wouldlike to induce grammars for many other languages.Table 3 shows the performance for all models andtraining procedures.
We see that the sparsifyingmethods tend to improve over EM most of thetime.
For the basic DMV, average improvementsare 1.6% for DD, 6.0% for PR-S, and 7.5% forPR-AS.
PR-AS beats PR-S in 8 out of 12 cases,197Bg Cz De Dk En Es Jp Nl Pt Se Si TrDMV ModelEM 37.8 29.6 35.7 47.2 45.8 40.3 52.8 37.1 35.7 39.4 42.3 46.8DD 0.25 39.3 30.0 38.6 43.1 46.4 47.5 57.8 35.1 38.7 40.2 48.8 43.8PR-S 140 53.7 31.5 39.6 44.0 62.1 61.1 58.8 31.0 47.0 42.2 39.9 51.4PR-AS 140 54.0 32.0 39.6 42.4 61.9 62.4 60.2 37.9 47.8 38.7 50.3 53.4Extended ModelEM (3,3) 41.7 48.9 40.1 46.4 55.3 44.3 48.5 47.5 35.9 48.6 47.5 46.2DD 0.1 (4,4) 47.6 48.5 42.0 44.4 53.6 48.9 57.6 45.2 48.3 47.6 35.6 48.9PR-S 140 (3,3) 59.0 54.7 47.4 45.8 64.3 57.9 60.8 33.9 54.3 45.6 49.1 56.3PR-AS 140 (4,4) 59.8 54.6 45.7 46.6 64.4 57.9 59.4 38.8 49.5 41.4 51.2 56.9Table 3: Attachment accuracy results.
The parameters used are the best settings found for English.
Values for hyperparameters(?
or ?)
are given after the method name.
For the extended model (Vc, Vs) are indicated in parentheses.
En is the English PennTreebank (Marcus et al, 1993) and the other 11 languages are from the CoNLL X shared task: Bulgarian [Bg] (Simov et al,2002), Czech [Cz] (Bohomov?
et al, 2001), German [De] (Brants et al, 2002), Danish [Dk] (Kromann et al, 2003), Spanish[Es] (Civit and Mart?, 2004), Japanese [Jp] (Kawata and Bartels, 2000), Dutch [Nl] (Van der Beek et al, 2002), Portuguese[Pt] (Afonso et al, 2002), Swedish [Se] (Nilsson et al, 2005), Slovene [Sl] (D?eroski et al, 2006), and Turkish [Tr] (Oflazer etal., 2003).Unadpapeleranc esvs undobjetonc civilizadoaqUnadpapeleranc esvs undobjetonc civilizadoaq1.001.00 1.000.490.511.000.570.43Unadpapeleranc esvs undobjetonc civilizadoaq1.00 0.83 0.75 0.990.920.350.48Figure 1: Posterior edge probabilities for an example sen-tence from the Spanish test corpus.
At the top are the golddependencies, the middle are EM posteriors, and bottom arePR posteriors.
Green indicates correct dependencies and redindicates incorrect dependencies.
The numbers on the edgesare the values of the posterior probabilities.though the average increase is only 1.5%.
PR-Sis also better than DD for 10 out of 12 languages.If we instead consider these methods for the E-DMV, DD performs worse, just 1.4% better thanthe E-DMV EM, while both PR-S and PR-AS con-tinue to show substantial average improvementsover EM, 6.5% and 6.3%, respectively.6 AnalysisOne common EM error that PR fixes in many lan-guages is the directionality of the noun-determinerrelation.
Figure 1 shows an example of a Span-ish sentence where PR significantly outperformsEM because of this.
Sentences such as ?Llevatiempo entenderlos?
which has tags ?main-verbcommon-noun main-verb?
(no determiner tag)provide an explanation for PR?s improvement?when PR sees that sometimes nouns can appearwithout determiners but that the opposite situationdoes not occur, it shifts the model parameters tomake nouns the parent of determiners instead ofthe reverse.
Then it does not have to pay the costof assigning a parent with a new tag to cover eachnoun that doesn?t come with a determiner.7 ConclusionIn this paper we presented a new method for unsu-pervised learning of dependency parsers.
In con-trast to previous approaches that constrain modelparameters, we constrain model posteriors.
Ourapproach consistently outperforms the standardEM algorithm and a discounting Dirichlet prior.We have several ideas for further improving ourconstraints, such as: taking into account the direc-tionality of the edges, using different regulariza-tion strengths for the root probabilities than for thechild probabilities, and working directly on wordtypes rather than on POS tags.
In the future, wewould also like to try applying similar constraintsto the more complex task of joint induction of POStags and dependency parses.AcknowledgmentsJ.
Gillenwater was supported by NSF-IGERT0504487.
K. Ganchev was supported byARO MURI SUBTLE W911NF-07-1-0216.J.
Gra?a was supported by FCT fellowshipSFRH/BD/27528/2006 and by FCT project CMU-PT/HuMach/0039/2008.
B. Taskar was partlysupported by DARPA CSSG and ONR YoungInvestigator Award N000141010746.198ReferencesS.
Afonso, E. Bick, R. Haber, and D. Santos.
2002.Floresta Sinta(c)tica: a treebank for Portuguese.
InProc.
LREC.K.
Bellare, G. Druck, and A. McCallum.
2009.
Al-ternating projections for learning with expectationconstraints.
In Proc.
UAI.A.
Bohomov?, J. Hajic, E. Hajicova, and B. Hladka.2001.
The prague dependency treebank: Three-levelannotation scenario.
In Anne Abeill?, editor, Tree-banks: Building and Using Syntactically AnnotatedCorpora.S.
Brants, S. Dipper, S. Hansen, W. Lezius, andG.
Smith.
2002.
The TIGER treebank.
In Proc.Workshop on Treebanks and Linguistic Theories.M.
Civit and M.A.
Mart?.
2004.
Building cast3lb: ASpanish Treebank.
Research on Language & Com-putation.S.B.
Cohen and N.A.
Smith.
2009.
The shared logisticnormal distribution for grammar induction.
In Proc.NAACL.S.B.
Cohen, K. Gimpel, and N.A.
Smith.
2008.
Lo-gistic normal priors for unsupervised probabilisticgrammar induction.
In Proc.
NIPS.A.P.
Dempster, N.M. Laird, and D.B.
Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal of the Royal Statistical So-ciety, 39(1):1?38.S.
D?eroski, T. Erjavec, N. Ledinek, P. Pajas,Z.
?abokrtsky, and A.
?ele.
2006.
Towards aSlovene dependency treebank.
In Proc.
LREC.J.
Finkel, T. Grenager, and C. Manning.
2007.
Theinfinite tree.
In Proc.
ACL.K.
Ganchev, J. Gra?a, J. Gillenwater, and B. Taskar.2010.
Posterior regularization for structured latentvariable models.
Journal of Machine Learning Re-search.J.
Gillenwater, K. Ganchev, J. Gra?a, F. Pereira, andB.
Taskar.
2010.
Posterior sparsity in unsuperviseddependency parsing.
Technical report, MS-CIS-10-19, University of Pennsylvania.J.
Gra?a, K. Ganchev, and B. Taskar.
2007.
Expec-tation maximization and posterior constraints.
InProc.
NIPS.W.P.
Headden III, M. Johnson, and D. McClosky.2009.
Improving unsupervised dependency pars-ing with richer contexts and smoothing.
In Proc.NAACL.M.
Johnson, T.L.
Griffiths, and S. Goldwater.
2007.Adaptor grammars: A framework for specifyingcompositional nonparametric Bayesian models.
InProc.
NIPS.Y.
Kawata and J. Bartels.
2000.
Stylebook for theJapanese Treebank in VERBMOBIL.
Technical re-port, Eberhard-Karls-Universitat Tubingen.D.
Klein and C. Manning.
2004.
Corpus-based induc-tion of syntactic structure: Models of dependencyand constituency.
In Proc.
ACL.M.T.
Kromann, L. Mikkelsen, and S.K.
Lynge.
2003.Danish Dependency Treebank.
In Proc.
TLT.P.
Liang, S. Petrov, M.I.
Jordan, and D. Klein.
2007.The infinite PCFG using hierarchical Dirichlet pro-cesses.
In Proc.
EMNLP.P.
Liang, M.I.
Jordan, and D. Klein.
2009.
Learn-ing from measurements in exponential families.
InProc.
ICML.G.
Mann and A. McCallum.
2007.
Simple, robust,scalable semi-supervised learning via expectationregularization.
In Proc.
ICML.G.
Mann and A. McCallum.
2008.
Generalized expec-tation criteria for semi-supervised learning of condi-tional random fields.
In Proc.
ACL.M.
Marcus, M. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguis-tics, 19(2):313?330.R.
Neal and G. Hinton.
1998.
A new view of the EMalgorithm that justifies incremental, sparse and othervariants.
In M. I. Jordan, editor, Learning in Graph-ical Models, pages 355?368.
MIT Press.J.
Nilsson, J.
Hall, and J. Nivre.
2005.
MAMBA meetsTIGER: Reconstructing a Swedish treebank fromantiquity.
NODALIDA Special Session on Tree-banks.K.
Oflazer, B.
Say, D.Z.
Hakkani-T?r, and G. T?r.2003.
Building a Turkish treebank.
Treebanks:Building and Using Parsed Corpora.K.
Simov, P. Osenova, M. Slavcheva, S. Kolkovska,E.
Balabanova, D. Doikoff, K. Ivanova, A. Simov,E.
Simov, and M. Kouylekov.
2002.
Building a lin-guistically interpreted corpus of bulgarian: the bul-treebank.
In Proc.
LREC.N.
Smith and J. Eisner.
2006.
Annealing structuralbias in multilingual weighted grammar induction.
InProc.
ACL.L.
Van der Beek, G. Bouma, R. Malouf, and G. Van No-ord.
2002.
The Alpino dependency treebank.
Lan-guage and Computers.199
