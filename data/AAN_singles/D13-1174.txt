Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1677?1687,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsTranslating into Morphologically Rich Languages with Synthetic PhrasesVictor Chahuneau Eva Schlinger Noah A. Smith Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{vchahune,eschling,nasmith,cdyer}@cs.cmu.eduAbstractTranslation into morphologically rich lan-guages is an important but recalcitrant prob-lem in MT.
We present a simple and effec-tive approach that deals with the problem intwo phases.
First, a discriminative model islearned to predict inflections of target wordsfrom rich source-side annotations.
Then, thismodel is used to create additional sentence-specific word- and phrase-level translationsthat are added to a standard translation modelas ?synthetic?
phrases.
Our approach re-lies on morphological analysis of the targetlanguage, but we show that an unsupervisedBayesian model of morphology can success-fully be used in place of a supervised analyzer.We report significant improvements in transla-tion quality when translating from English toRussian, Hebrew and Swahili.1 IntroductionMachine translation into morphologically rich lan-guages is challenging, due to lexical sparsity and thelarge variety of grammatical features expressed withmorphology.
In this paper, we introduce a methodthat uses target language morphological grammars(either hand-crafted or learned unsupervisedly) toaddress this challenge and demonstrate its effective-ness at improving translation from English into sev-eral morphologically rich target languages.Our approach decomposes the process of produc-ing a translation for a word (or phrase) into twosteps.
First, a meaning-bearing stem is chosen andthen an appropriate inflection is selected using afeature-rich discriminative model that conditions onthe source context of the word being translated.Rather than attempting to directly produce full-sentence translations using such an elementary pro-cess, we use our model to generate translations ofindividual words and short phrases that augment?on a sentence-by-sentence basis?the inventory oftranslation rules obtained using standard translationrule extraction techniques (Chiang, 2007).
We callthese synthetic phrases.The major advantages of our approach are: (i)synthesized forms are targeted to a specific transla-tion context; (ii) multiple, alternative phrases maybe generated with the final choice among rules leftto the global translation model; (iii) virtually nolanguage-specific engineering is necessary; (iv) anyphrase- or syntax-based decoder can be used with-out modification; and (v) we can generate forms thatwere not attested in the bilingual training data.The paper is structured as follows.
We firstpresent our ?translate-and-inflect?
model for pre-dicting lexical translations into morphologically richlanguages given a source word and its context (?2).Our approach requires a morphological grammar torelate surface forms to underlying ?stem, inflection?pairs; we discuss how either a standard morpholog-ical analyzer or a simple Bayesian unsupervised an-alyzer can be used (?3).
After describing an ef-ficient parameter estimation procedure for the in-flection model (?4), we employ the translate-and-inflect model in an MT system.
We describehow we use our model to synthesize translationoptions (?5) and then evaluate translation qualityon English?Russian, English?Hebrew, and English?1677Swahili translation tasks, finding significant im-provements in all language pairs (?6).
We finallyreview related work (?7) and conclude (?8).2 Translate-and-Inflect ModelThe task of the translate-and-inflect model is illus-trated in Fig.
1 for an English?Russian sentence pair.The input will be a sentence e in the source language(in this paper, always English) and any available lin-guistic analysis of e. The output f will be composedof (i) a sequence of stems, each denoted ?
and (ii)one morphological inflection pattern for each stem,denoted ?.
When the information is available, astem ?
is composed of a lemma and an inflectionalclass.
Throughout, we use ??
to denote the setof possible morphological inflection patterns for agiven stem ?.
??
might be defined by a grammar;our models restrict ??
to be the set of inflectionsobserved anywhere in our monolingual or bilingualtraining data as a realization of ?.1We assume the availability of a deterministicfunction that maps a stem ?
and morphological in-flection ?
to a target language surface form f .
Insome cases, such as our unsupervised approach in?3.2, this will be a concatenation operation, thoughfinite-state transducers are traditionally used to de-fine such relations (?3.1).
We abstractly denote thisoperation by ?
: f = ?
?
?.Our approach consists in defining a probabilisticmodel over target words f .
The model assumes in-dependence between each target word f conditionedon the source sentence e and its aligned position i inthis sentence.2 This assumption is further relaxedin ?5 when the model is integrated in the translationsystem.We decompose the probability of generating eachtarget word f in the following way:p(f | e, i) =????=fp(?
| ei)?
??
?gen.
stem?
p(?
| ?, e, i)?
??
?gen.
inflectionHere, each stem is generated independently from asingle aligned source word ei, but in practice we1This prevents the model from generating words that wouldbe difficult for the language model to reliably score.2This is the same assumption that Brown et al(1993) makein, for example, IBM Model 1.use a standard phrase-based model to generate se-quences of stems and only the inflection model op-erates word-by-word.
We turn next to the inflectionmodel.2.1 Modeling InflectionIn morphologically rich languages, each stem maybe combined with one or more inflectional mor-phemes to express many different grammatical fea-tures (e.g., case, definiteness, mood, tense, etc.
).Since the inflectional morphology of a word gen-erally expresses multiple grammatical features, wewould like a model that naturally incorporates rich,possibly overlapping features in its representation ofboth the input (i.e., conditioning context) and out-put (i.e., the inflection pattern).
We therefore usethe following parametric form to model inflectionalprobabilities:u(?, e, i) = exp[?
(e, i)>W?(?)+?(?)>V?(?)],p(?
| ?, e, i) =u(?, e, i)??????
u(?
?, e, i).
(1)Here, ?
is an m-dimensional source context fea-ture vector function, ?
is an n-dimensional mor-phology feature vector function, W ?
Rm?n andV ?
Rn?n are parameter matrices.
As with themore familiar log-linear parametrization that is writ-ten with a single feature vector, single weight vec-tor and single bias vector, this model is linear in itsparameters (it can be understood as working witha feature space that is the outer product of the twofeature spaces).
However, using two feature vectorsallows to define overlapping features of both the in-put and the output, which is important for modelingmorphology in which output variables are naturallyexpressed as bundles of features.
The second termin the sum in u enables correlations among outputfeatures to be modeled independently of input, andas such can be understood as a generalization of thebias terms in multi-class logistic regression (on thediagonal Vii) and interaction terms between outputvariables in a conditional random field (off the diag-onalVij).1678???
????????
????????
????
??
??
????????
?she had attempted to cross the road on her bikePRP   VBD         VBN          TO    VB       DT     NN    IN  PRP$   NNnsubjauxxcomp?:????????_V,+,?
:mis2sfm2eC50   C473        C28          C8    C275   C37   C43  C82 C94   C331root-1 +1???
???
????
?
?
????
????
??
??
????????
?she had attempted to cross the road on her bikePRP   VBD         VBN         TO   VB      DT     NN  IN  PRP$   NNnsubjauxxcomp?:????????_V,+,?
:mis sfm2eC50   C473        C28         C8    C275   C37   C43  C82 C94   C331root-1 +1Figure 1: The inflection model predicts a form for the target verb lemma ?
=????????
(pytat?sya) based on itssource attempted and the linear and syntactic source context.
The correct inflection string for the observed Russianform in this particular training instance is ?
= mis-sfm-e (equivalent to the more traditional morphological string:+MAIN+IND+PAST+SING+FEM+MEDIAL+PERF).??????
?source aligned word eiparent word epii with its dependency pii ?
iall children ej | pij = i with their dependency i?
jsource words ei?1 and ei+1?????????
?tokenpart-of-speech tagword cluster????
are ei, epii at the root of the dependency tree??
number of children, siblings of eiFigure 2: Source features ?
(e, i) extracted from e and its linguistic analysis.
pii denotes the parent of the token inposition i in the dependency tree and pii ?
i the typed dependency link.2.2 Source Context Features: ?
(e, i)In order to select the best inflection of a target-language word, given the source word it translatesand the context of that source word, we seek to ex-ploit as many features of the context as are avail-able.
Consider the example shown in Fig.
1, wheremost of the inflection features of the Russian word(past tense, singular number, and feminine gender)can be inferred from the context of the English wordit is aligned to.
Indeed, many grammatical functionsexpressed morphologically in Russian are expressedsyntactically in English.
Fortunately, high-qualityparsers and other linguistic analyzers are availablefor English.On the source side, we apply the following pro-cessing steps:?
Part-of-speech tagging with a CRF taggertrained on sections 02?21 of the Penn Tree-bank.?
Dependency parsing with TurboParser (Mar-tins et al 2010), a non-projective dependencyparser trained on the Penn Treebank to producebasic Stanford dependencies.?
Assignment of tokens to one of 600 Brownclusters, trained on 8G words of English text.3We then extract binary features from e using thisinformation, by considering the aligned source wordei, its preceding and following words, and its syn-tactic neighbors.
These are detailed in Figure 2.3 Morphological Grammars and FeaturesWe now describe how to obtain morphological anal-yses and convert them into feature vectors (?)
forour target languages, Russian, Hebrew, and Swahili,using supervised and unsupervised methods.3.1 Supervised MorphologyThe state-of-the-art in morphological analysis usesunweighted morphological transduction rules (usu-3The entire monolingual data available for the translationtask of the 8th ACL Workshop on Statistical Machine Transla-tion was used.1679ally in the form of an FST) to produce candidateanalyses for each word in a sentence and then sta-tistical models to disambiguate among the analy-ses in context (Hakkani-Tu?r et al 2000; Hajic?
etal., 2001; Smith et al 2005; Habash and Rambow,2005, inter alia).
While this technique is capableof producing high quality linguistic analyses, it isexpensive to develop, requiring hand-crafted rule-based analyzers and annotated corpora to train thedisambiguation models.
As a result, such analyzersare only available for a small number of languages,and, as a practical matter, each analyzer (which re-sulted from different development efforts) operatesdifferently from the others.We therefore focus on using supervised analysisfor a single target language, Russian.
We use theanalysis tool of Sharoff et al(2008) which producesfor each word in context a lemma and a fixed-lengthmorphological tag encoding the grammatical fea-tures.
We process the target side of the parallel datawith this tool to obtain the information necessaryto extract ?lemma, inflection?
pairs, from which wecompute ?
and morphological feature vectors ?(?
).Supervised morphology features: ?(?).
Sincea positional tag set is used, it is straightforward toconvert each fixed-length tag ?
into a feature vectorby defining a binary feature for each key-value pair(e.g., Tense=past) composing the tag.3.2 Unsupervised MorphologySince many languages into which we might want totranslate do not have supervised morphological an-alyzers, we now turn to the question of how to gen-erate morphological analyses and features using anunsupervised analyzer.
We hypothesize that perfectdecomposition into rich linguistic structures may notbe required for accurate generation of new inflectedforms.
We will test this hypothesis by experimentingwith a simple, unsupervised model of morphologythat segments words into sequences of morphemes,assuming a (na?
?ve) concatenative generation processand a single analysis per type.Unsupervised morphological segmentation.
Weassume that each word can be decomposed into anynumber of prefixes, a stem, and any number of suf-fixes.
Formally, we let M represent the set of allpossible morphemes and define a regular grammarM?MM?
(i.e., zero or more prefixes, a stem, andzero or more suffixes).
To infer the decompositionstructure for the words in the target language, we as-sume that the vocabulary was generated by the fol-lowing process:1.
Sample morpheme distributions from symmet-ric Dirichlet distributions: ?p ?
Dir|M |(?p)for prefixes, ??
?
Dir|M |(??)
for stems, and?s ?
Dir|M |(?s) for suffixes.2.
Sample length distribution parameters?p ?
Beta(?p, ?p) for prefix sequencesand ?s ?
Beta(?s, ?s) for suffix sequences.3.
Sample a vocabulary by creating each wordtype w using the following steps:(a) Sample affix sequence lengths:lp ?
Geometric(?p);ls ?
Geometric(?s).
(b) Sample lp prefixes p1, .
.
.
, plp indepen-dently from ?p; ls suffixes s1, .
.
.
, sls in-dependently from ?s; and a stem ?
?
??.
(c) Concatenate prefixes, the stem, and suf-fixes: w = p1+?
?
?+plp+?+s1+?
?
?+sls .We use blocked Gibbs sampling to sample seg-mentations for each word in the training vocabulary.Because of our particular choice of priors, it possibleto approximately decompose the posterior over thearcs of a compact finite-state machine.
Sampling asegmentation or obtaining the most likely segmenta-tion a posteriori then reduces to familiar FST opera-tions.
This model is reminiscent of work on learningmorphology using adaptor grammars (Johnson et al2006; Johnson, 2008).The inferred morphological grammar is very sen-sitive to the Dirichlet hyperparameters (?p, ?s, ??
)and these are, in turn, sensitive to the number oftypes in the vocabulary.
Using ?p, ?s  ??
1tended to recover useful segmentations, but we havenot yet been able to find reliable generic priors forthese values.
Therefore, we selected them empiri-cally to obtain a stem vocabulary size on the paralleldata that is one-to-one with English.4 Future work4Our default starting point was to use ?p = ?s =10?6, ??
= 10?4 and then to adjust all parameters by factorsof 10.1680Table 1: Corpus statistics.Parallel Parallel+MonolingualSentences EN-tokens TRG-tokens EN-types TRG-types Sentences TRG-tokens TRG-typesRussian 150k 3.5M 3.3M 131k 254k 20M 360M 1,971kHebrew 134k 2.7M 2.0M 48k 120k 806k 15M 316kSwahili 15k 0.3M 0.3M 23k 35k 596k 13M 334kwill involve a more direct method for specifying orinferring these values.Unsupervised morphology features: ?(?).
Forthe unsupervised analyzer, we do not have a map-ping from morphemes to structured morphologicalattributes; however, we can create features from theaffix sequences obtained after morphological seg-mentation.
We produce binary features correspond-ing to the content of each potential affixation posi-tion relative to the stem:prefix      suffix...-3 -2 -1 STEM +1 +2 +3...For example, the unsupervised analysis ?
=wa+ki+wa+STEM of the Swahili word wakiwapigawill produce the following features:?prefix[?3][wa](?)
= 1,?prefix[?2][ki](?)
= 1,?prefix[?1][wa](?)
= 1.4 Inflection Model Parameter EstimationTo set the parametersW andV of the inflection pre-diction model (Eq.
1), we use stochastic gradient de-scent to maximize the conditional log-likelihood ofa training set consisting of pairs of source (English)sentence contextual features (?)
and target word in-flectional features (?).
The training instances areextracted from the word-aligned parallel corpus withthe English side preprocessed as discussed in ?2.2and the target side disambiguated as discussed in ?3.When morphological category information is avail-able, we train an independent model for each open-class category (in Russian, nouns, verbs, adjectives,numerals, adverbs); otherwise a single model is usedfor all words (excluding words less than four char-acters long, which are ignored).Statistics of the parallel corpora used to train theinflection model are summarized in Table 1.
It isimportant to note here that our richly parameterizedmodel is trained on the full parallel training cor-pus, not just on a handful of development sentences(which are typically used to tune MT system param-eters).
Despite this scale, training is simple: the in-flection model is trained to discriminate among dif-ferent inflectional paradigms, not over all possibletarget language sentences (Blunsom et al 2008) orlearning from all observable rules (Subotin, 2011).This makes the training problem relatively tractable:all experiments in this paper were trained on a sin-gle processor using a Cython implementation of theSGD optimizer.
For our largest model, trained on3.3M Russian words, n = 231K ?
m = 336 fea-tures were produced, and 10 SGD iterations wereperformed in less than 16 hours.4.1 Intrinsic EvaluationBefore considering the broader problem of integrat-ing the inflection model in a machine translationsystem, we perform an artificial evaluation to ver-ify that the model learns sensible source sentence-target inflection patterns.
To do so, we create aninflection test set as follows.
We preprocess thesource (English) sentences exactly as during train-ing (?2.2), and using the target language morpholog-ical analyzer, we convert each aligned target word to?stem, inflection?
pairs.
We perform word alignmenton the held-out MT development data for each lan-guage pair (cf.
Table 1), exactly as if it were going toproduce training instances, but instead we use themfor testing.Although the resulting dataset is noisy (e.g., dueto alignment errors), this becomes our intrinsic eval-uation test set.
Using this data, we measure inflec-tion quality using two measurements:55Note that we are not evaluating the stem translation model,1681acc.
ppl.
|?
?|SupervisedRussianN 64.1% 3.46 9.16V 63.7% 3.41 20.12A 51.5% 6.24 19.56M 73.0% 2.81 9.14average 63.1% 3.98 14.49Unsup.
Russian all 71.2% 2.15 4.73Hebrew all 85.5% 1.49 2.55Swahili all 78.2% 2.09 11.46Table 2: Intrinsic evaluation of inflection model (N:nouns, V: verbs, A: adjectives, M: numerals).?
the accuracy of predicting the inflection giventhe source, source context and target stem, and?
the inflection model perplexity on the same setof test instances.Additionally, we report the average number of pos-sible inflections for each stem, an upper bound to theperplexity that indicates the inherent difficulty of thetask.
The results of this evaluation are presented inTable 2 for the three language pairs considered.
Weremark on two patterns in these results.
First, per-plexity is substantially lower than the perplexity of auniform model, indicating our model is overall quiteeffective at predicting inflections using source con-text only.
Second, in the supervised Russian results,we see that predicting the inflections of adjectivesis relatively more difficult than for other parts-of-speech.
Since adjectives agree with the nouns theymodify in gender and case, and gender is an idiosyn-cratic feature of Russian nouns (and therefore notdirectly predictable from the English source), thisdifficulty is unsurprising.We can also inspect the weights learned by themodel to assess the effectiveness of the featuresin relating source-context structure with target-sidemorphology.
Such an analysis is presented in Fig.
3.4.2 Feature AblationOur inflection model makes use of numerous fea-ture types.
Table 3 explores the effect of removingdifferent kinds of (source) features from the model,evaluated on predicting Russian inflections usingsupervised morphological grammars.6 Rows 2?3just the inflection prediction model.6The models used in the feature ablation experiment weretrained on fewer examples, resulting in overall lower accuraciesshow the effect of removing either linear or depen-dency context.
We see that both are necessary forgood performance; however removing dependencycontext substantially degrades performance of themodel (we interpret this result as evidence that Rus-sian morphological inflection captures grammaticalrelationships that would be expressed structurally inEnglish).
The bottom four rows explore the effectof source language word representation.
The resultsindicate that lexical features are important for accu-rate prediction of inflection, and that POS tags andBrown clusters are likewise important, but they seemto capture similar information (removing one has lit-tle impact, but removing both substantially degradesperformance).Table 3: Feature ablation experiments using supervisedRussian classification experiments.Features (?
(e, i)) acc.all 54.7%?linear context 52.7%?dependency context 44.4%?POS tags 54.5%?Brown clusters 54.5%?POS tags, ?Brown cl.
50.9%?lexical items 51.2%5 Synthetic PhrasesWe turn now to translation; recall that our translate-and-inflect model is used to augment the set of rulesavailable to a conventional statistical machine trans-lation decoder.
We refer to the phrases it producesas synthetic phrases.Our baseline system is a standard hierarchicalphrase-based translation model (Chiang, 2007).
Fol-lowing Lopez (2007), the training data is compiledinto an efficient binary representation which allowsextraction of sentence-specific grammars just beforedecoding.
In our case, this also allows the creationof synthetic inflected phrases that are produced con-ditioning on the sentence to translate.To generate these synthetic phrases with new in-flections possibly unseen in the parallel trainingthan seen in Table 2, but the pattern of results is the relevantdatapoint here.1682Russian supervisedVerb: 1st Personchild(nsubj)=I child(nsubj)=weVerb: Future tensechild(aux)=MD child(aux)=willNoun: Animatesource=animals/victims/...Noun: Feminine gendersource=obama/economy/...Noun: Dative caseparent(iobj)Adjective: Genitive casegrandparent(poss)HebrewSuffix ??
(masculine plural)parent=NNS after=NNSPrefix ?
(first person sing.
+ future)child(nsubj)=I child(aux)='llPrefix ?
(preposition like/as)child(prep)=IN parent=asSuffix ?
(possesive mark)before=my child(poss)=mySuffix ?
(feminine mark)child(nsubj)=she before=shePrefix ??
(when)before=when before=WRBSwahiliPrefix li (past)source=VBD source=VBNPrefix nita (1st person sing.
+ future)child(aux) child(nsubj)=IPrefix ana (3rd person sing.
+ present)source=VBZPrefix wa (3rd person plural)before=they child(nsubj)=NNSSuffix tu (1st person plural)child(nsubj)=she before=shePrefix ha (negative tense)source=no after=notFigure 3: Examples of highly weighted features learned by the inflection model.
We selected a few frequent morpho-logical features and show their top corresponding source context features.data, we first construct an additional phrase-basedtranslation model on the parallel corpus prepro-cessed to replace inflected surface words with theirstems.
We then extract a set of non-gappy phrasesfor each sentence (e.g., X ?
<attempted,????????
V>).
The target side of each such phraseis re-inflected, conditioned on the source sentence,using the inflection model from ?2.
Each stem isgiven its most likely inflection.7The original features extracted for the stemmedphrase are conserved, and the following featuresare added to help the decoder select good syntheticphrases:?
a binary feature indicating that the phrase issynthetic,?
the log-probability of the inflected forms ac-cording to our model,?
the count of words that have been inflected,with a separate feature for each morphologicalcategory in the supervised case.Finally, these synthetic phrases are combined withthe original translation rules obtained for the base-line system to produce an extended sentence-specificgrammar which is used as input to the decoder.
If a7Several reviewers asked about what happens when k-bestinflections are added.
The results for k ?
{2, 4, 8} range fromno effect to an improvement over k = 1 of about 0.2 BLEU(absolute).
We hypothesize that larger values of k could have agreater impact, perhaps in a more ?global?
model of the targetstring; however, exploration of this question is beyond the scopeof this paper.phrase already existing in the standard phrase tablehappens to be recreated, both phrases are kept andwill compete with each other with different featuresin the decoder.For example, for the large EN?RU system, 6%of all the rules used for translation are syntheticphrases, with 65% of these phrases being entirelynew rules.6 Translation ExperimentsWe evaluate our approach in the standard discrim-inative MT framework.
We use cdec (Dyer et al2010) as our decoder and perform MIRA trainingto learn feature weights of the sentence translationmodel (Chiang, 2012).
We compare the followingconfigurations:?
A baseline system, using a 4-gram languagemodel trained on the entire monolingual andbilingual data available.?
An enriched system with a class-based n-gramlanguage model8 trained on the monolingualdata mapped to 600 Brown clusters.
Class-based language modeling is a strong baselinefor scenarios with high out-of-vocabulary ratesbut in which large amounts of monolingualtarget-language data are available.?
The enriched system further augmented withour inflected synthetic phrases.
We expect theclass-based language model to be especially8For Swahili and Hebrew, n = 6; for Russian, n = 7.1683helpful here and capture some basic agreementpatterns that can be learned more easily ondense clusters than from plain word sequences.Detailed corpus statistics are given in Table 1:?
The Russian data consist of the News Com-mentary parallel corpus and additional mono-lingual data crawled from news websites.9?
The Hebrew parallel corpus is composed oftranscribed TED talks (Cettolo et al 2012).Additional monolingual news data is also used.?
The Swahili parallel corpus was obtained bycrawling the Global Voices project website10for parallel articles.
Additional monolingualdata was taken from the Helsinki Corpus ofSwahili.11We evaluate translation quality by translating andmeasuring the BLEU score of a 2000?3000 sentence-long evaluation corpus, averaging the results over 3MIRA runs to control for optimizer instability (Clarket al 2011).
Table 4 reports the results.
For all lan-guages, using class language models improves overthe baseline.
When synthetic phrases are added, sig-nificant additional improvements are obtained.
Forthe English?Russian language pair, where both su-pervised and unsupervised analyses can be obtained,we notice that expert-crafted morphological analyz-ers are more efficient at improving translation qual-ity.
Globally, the amount of improvement observedvaries depending on the language; this is most likelyindicative of the quality of unsupervised morpholog-ical segmentations produced and the kinds of gram-matical relations expressed morphologically.Finally, to confirm the effectiveness of our ap-proach as corpus size increases, we use our tech-nique on top of a state-of-the art English?Russiansystem trained on data from the 8th ACL Work-shop on Machine Translation (30M words of bilin-gual text and 410M words of monolingual text).
Thesetup is identical except for the addition of sparse9http://www.statmt.org/wmt13/translation-task.html10http://sw.globalvoicesonline.org11http://www.aakkl.helsinki.fi/cameel/corpus/intro.htmTable 4: Translation quality (measured by BLEU) aver-aged over 3 MIRA runs.EN?RU EN?HE EN?SWBaseline 14.7?0.1 15.8?0.3 18.3?0.1+Class LM 15.7?0.1 16.8?0.4 18.7?0.2+Syntheticunsupervised 16.2?0.1 17.6?0.1 19.0?0.1supervised 16.7?0.1 ?
?rule shape indicator features and bigram cluster fea-tures.
In these large scale conditions, the BLEU scoreimproves from 18.8 to 19.6 with the addition of wordclusters and reaches 20.0 with synthetic phrases.Details regarding this system are reported in Ammaret al(2013).7 Related WorkTranslation into morphologically rich languages isa widely studied problem and there is a tremen-dous amount of related work.
Our technique of syn-thesizing translation options to improve generationof inflected forms is closely related to the factoredtranslation approach proposed by Koehn and Hoang(2007); however, an important difference to thatwork is that we use a discriminative model that con-ditions on source context to make ?local?
decisionsabout what inflections may be used before combin-ing the phrases into a complete sentence translation.Combination pre-/post-processing solutions arealso frequently proposed.
In these, the tar-get language is generally transformed from multi-morphemic surface words into smaller units moreamenable to direct translation, and then a post-processing step is applied independent of the trans-lation model.
For example, Oflazer and El-Kahlout(2007) experiment with partial morpheme groupingsto produce novel inflected forms when translatinginto Turkish; Al-Haj and Lavie (2010) compare dif-ferent processing schemes for Arabic.
A related butdifferent approach is to enrich the source languageitems with grammatical features (e.g., a source sen-tence like John saw Mary is preprocessed into, e.g.,John+subj saw+msubj+fobj Mary+obj) so asto make the source and target lexicons have simi-lar morphological contrasts (Avramidis and Koehn,2008; Yeniterzi and Oflazer, 2010; Chang et al16842009).
In general, this work suffers from the prob-lem that it is extremely difficult to know a prioriwhat the right preprocessing is for a given languagepair, data size, and domain.Several post-processing approaches have reliedon supervised classifiers to predict the optimal com-plete inflection for an incomplete or lemmatizedtranslation.
Minkov et al(2007) present a methodfor predicting the inflection of Russian and Arabicsentences aligned to English sentences.
They train asequence model to predict target morphological fea-tures from the lemmas and the syntactic structuresof both aligned sentences and demonstrate its abilityto recover accurately inflections on reference trans-lations.
Toutanova et al(2008) apply this methodto generate inflections after translation in two differ-ent ways: by rescoring inflected n-best outputs or bytranslating lemmas and re-inflecting them a posteri-ori.
El Kholy and Habash (2012) follow a similarmethod and compare different approaches for gen-erating rich morphology in Arabic after a transla-tion step.
Fraser et al(2012) observe improvementsfor translation into German with a similar method.As in that work, we model morphological featuresrather than directly inflected forms.
However, thatwork may be criticized for providing no mechanismto translate surface forms directly, even when evi-dence for a direct translation is available in the par-allel data.Unsupervised morphology has begun to play arole in translation between morphologically com-plex languages.
Stallard et al(2012) show that anunsupervised approach to Arabic segmentation per-forms as well as a supervised segmenter for source-side preprocessing (in terms of English translationquality).
For translation into morphological rich lan-guages, Clifton and Sarkar (2011) use an unsuper-vised morphological analyzer to produce morpho-logical affixes in Finnish, injecting some linguisticknowledge in the generation process.Several authors have proposed using conditionalmodels to predict the probability of phrase transla-tion in context (Gimpel and Smith, 2008; Chan etal., 2007; Carpuat and Wu, 2007; Jeong et al 2010).Of particular note is the work of Subotin (2011),who use a conditional model to predict morpholog-ical features conditioned on rich linguistic features;however, this latter work also conditions on targetcontext, which substantially complicates decoding.Finally, synthetic phrases have been used fordifferent purposes than generating morphology.Callison-Burch et al(2006) expanded the cov-erage of a phrase table by adding synthesizedphrases by paraphrasing source language phrases,Chen et al(2011) produced ?fabricated?
phrasesby paraphrasing both source and target phrases, andHabash (2009) created new rules to handle out-of-vocabulary words.
In related work, Tsvetkov et al(2013) used synthetic phrases to improve generationof (in)definite articles when translating into Englishfrom Russian and Czech, two languages which donot lexically mark definiteness.8 ConclusionWe have presented an efficient technique that ex-ploits morphologically analyzed corpora to producenew inflections possibly unseen in the bilingualtraining data.
Our method decomposes into twosimple independent steps involving well-understooddiscriminative models.By relying on source-side context to generate ad-ditional local translation options and by leaving thechoice of the full sentence translation to the decoder,we sidestep the difficulty of computing features ontarget translations hypotheses.
However, many mor-phological processes (most notably, agreement) aremost best modeled using target language context.
Tocapture target context effects, we depend on strongtarget language models.
Therefore, an importantextension of our work is to explore the interactionof our approach with more sophisticated languagemodels that more directly model morphology, e.g.,the models of Bilmes and Kirchhoff (2003), or, alter-natively, ways to incorporate target language contextin the inflection model.We also achieve language independence byexploiting unsupervised morphological segmen-tations in the absence of linguistically informedmorphological analyses.Code for replicating the experiments is available fromhttps://github.com/eschling/morphogen;further details are available in (Schlinger et al 2013).1685AcknowledgmentsThis work was supported by the U. S. Army ResearchLaboratory and the U. S. Army Research Office undercontract/grant number W911NF-10-1-0533.
We wouldlike to thank Kim Spasaro for curating the Swahili devel-opment and test sets, Yulia Tsvetkov for assistance withRussian, and the anonymous reviewers for their helpfulcomments.ReferencesHassan Al-Haj and Alon Lavie.
2010.
The im-pact of Arabic morphological segmentation on broad-coverage English-to-Arabic statistical machine trans-lation.
In Proc.
of AMTA.Waleed Ammar, Victor Chahuneau, Michael Denkowski,Greg Hanneman, Wang Ling, Austin Matthews, Ken-ton Murray, Nicola Segall, Yulia Tsvetkov, AlonLavie, and Chris Dyer.
2013.
The CMU machinetranslation systems at WMT 2013: Syntax, synthetictranslation options, and pseudo-references.
In Proc.
ofWMT.Eleftherios Avramidis and Philipp Koehn.
2008.
Enrich-ing morphologically poor languages for statistical ma-chine translation.
In Proc.
of ACL.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.
InProc.
of NAACL.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In Proc.
of ACL.Peter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, and Robert L. Mercer.
1993.
The mathe-matics of statistical machine translation: parameter es-timation.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Improved statistical machine transla-tion using paraphrases.
In Proc.
of NAACL.Marine Carpuat and Dekai Wu.
2007.
Improving statisti-cal machine translation using word sense disambigua-tion.
In Proc.
of EMNLP.Mauro Cettolo, Christian Girardi, and Marcello Federico.2012.
WIT3: Web inventory of transcribed and trans-lated talks.
In Proc.
of EAMT.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proc.
of ACL.Pi-Chuan Chang, Dan Jurafsky, and Christopher D. Man-ning.
2009.
Disambiguating ?DE?
for Chinese?English machine translation.
In Proc.
of WMT.Boxing Chen, Roland Kuhn, and George Foster.
2011.Semantic smoothing and fabrication of phrase pairs forSMT.
In Proc.
of IWSLT.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.David Chiang.
2012.
Hope and fear for discrimina-tive training of statistical translation models.
JMLR,13:1159?1187.Jonathan H. Clark, Chris Dyer, Alon Lavie, and Noah A.Smith.
2011.
Better hypothesis testing for statisticalmachine translation: Controlling for optimizer insta-bility.
In Proc.
of ACL.Ann Clifton and Anoop Sarkar.
2011.
Combin-ing morpheme-based machine translation with post-processing morpheme prediction.
In Proc.
of ACL.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proc.
ofACL.Ahmed El Kholy and Nizar Habash.
2012.
Translate,predict or generate: Modeling rich morphology in sta-tistical machine translation.
In Proc.
of EAMT.Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-bienne Cap.
2012.
Modeling inflection and word-formation in SMT.
In Proc.
of EACL.Kevin Gimpel and Noah A. Smith.
2008.
Rich source-side context for statistical machine translation.
InProc.
of WMT.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proc.
of ACL.Nizar Habash.
2009.
REMOOV: A tool for online han-dling of out-of-vocabulary words in machine transla-tion.
In Proceedings of the 2nd International Confer-ence on Arabic Language Resources and Tools.Jan Hajic?, Pavel Krbec, Pavel Kve?ton?, Karel Oliva, andVladim?
?r Petkevic?.
2001.
Serial combination of rulesand statistics: A case study in Czech tagging.
In Proc.of ACL.Dilek Z. Hakkani-Tu?r, Kemal Oflazer, and Go?khan Tu?r.2000.
Statistical morphological disambiguation foragglutinative languages.
In Proc.
of COLING.Minwoo Jeong, Kristina Toutanova, Hisami Suzuki, andChris Quirk.
2010.
A discriminative lexicon modelfor complex morphology.
In Proc.
of AMTA.Mark Johnson, Thomas L. Griffiths, and Sharon Goldwa-ter.
2006.
Adaptor grammars: A framework for spec-ifying compositional nonparametric Bayesian models.NIPS, pages 641?648.Mark Johnson.
2008.
Unsupervised word segmentationfor Sesotho using adaptor grammars.
In Proc.
SIG-MORPHON.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proc.
of EMNLP.1686Adam Lopez.
2007.
Hierarchical phrase-based transla-tion with suffix arrays.
In Proc.
of EMNLP.Andre?
F.T.
Martins, Noah A. Smith, Eric P. Xing, Pe-dro M.Q.
Aguiar, and Ma?rio A.T. Figueiredo.
2010.Turbo parsers: Dependency parsing by approximatevariational inference.
In Proc.
of EMNLP.Einat Minkov, Kristina Toutanova, and Hisami Suzuki.2007.
Generating complex morphology for machinetranslation.
In Proc.
of ACL.Kemal Oflazer and I?lknur Durgar El-Kahlout.
2007.
Ex-ploring different representational units in English-to-Turkish statistical machine translation.
In Proc.
ofWMT.Eva Schlinger, Victor Chahuneau, and Chris Dyer.
2013.morphogen: Translation into morphologically rich lan-guages with synthetic phrases.
Prague Bulletin ofMathematical Linguistics, (100).Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, AnnaFeldman, and Dagmar Divjak.
2008.
Designing andevaluating a Russian tagset.
In Proc.
of LREC.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-based morphological disambiguationwith random fields.
In Proc.
of EMNLP.David Stallard, Jacob Devlin, Michael Kayser,Yoong Keok Lee, and Regina Barzilay.
2012.Unsupervised morphology rivals supervised morphol-ogy for Arabic MT.
In Proc.
of ACL.Michael Subotin.
2011.
An exponential translationmodel for target language morphology.
In Proc.
ACL.Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.2008.
Applying morphology generation models tomachine translation.
In Proc.
of ACL.Yulia Tsvetkov, Chris Dyer, Lori Levin, and Archna Bha-tia.
2013.
Generating English determiners in phrase-based translation with synthetic translation options.
InProc.
of WMT.Reyyan Yeniterzi and Kemal Oflazer.
2010.
Syntax-to-morphology mapping in factored phrase-based statis-tical machine translation from English to Turkish.
InProc.
of ACL.1687
