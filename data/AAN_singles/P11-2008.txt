Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 42?47,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsPart-of-Speech Tagging for Twitter: Annotation, Features, and ExperimentsKevin Gimpel, Nathan Schneider, Brendan O?Connor, Dipanjan Das, Daniel Mills,Jacob Eisenstein, Michael Heilman, Dani Yogatama, Jeffrey Flanigan, and Noah A. SmithSchool of Computer Science, Carnegie Mellon Univeristy, Pittsburgh, PA 15213, USA{kgimpel,nschneid,brenocon,dipanjan,dpmills,jacobeis,mheilman,dyogatama,jflanigan,nasmith}@cs.cmu.eduAbstractWe address the problem of part-of-speech tag-ging for English data from the popular micro-blogging service Twitter.
We develop a tagset,annotate data, develop features, and reporttagging results nearing 90% accuracy.
Thedata and tools have been made available to theresearch community with the goal of enablingricher text analysis of Twitter and related so-cial media data sets.1 IntroductionThe growing popularity of social media and user-created web content is producing enormous quanti-ties of text in electronic form.
The popular micro-blogging service Twitter (twitter.com) is oneparticularly fruitful source of user-created content,and a flurry of recent research has aimed to under-stand and exploit these data (Ritter et al, 2010; Shar-ifi et al, 2010; Barbosa and Feng, 2010; Asur andHuberman, 2010; O?Connor et al, 2010a; Thelwallet al, 2011).
However, the bulk of this work eschewsthe standard pipeline of tools which might enablea richer linguistic analysis; such tools are typicallytrained on newstext and have been shown to performpoorly on Twitter (Finin et al, 2010).One of the most fundamental parts of the linguis-tic pipeline is part-of-speech (POS) tagging, a basicform of syntactic analysis which has countless appli-cations in NLP.
Most POS taggers are trained fromtreebanks in the newswire domain, such as the WallStreet Journal corpus of the Penn Treebank (PTB;Marcus et al, 1993).
Tagging performance degradeson out-of-domain data, and Twitter poses additionalchallenges due to the conversational nature of thetext, the lack of conventional orthography, and 140-character limit of each message (?tweet?).
Figure 1shows three tweets which illustrate these challenges.
(a) @Gunservatively@ obozo?
willV goV nutsAwhenR PA?
electsV aD RepublicanA GovernorNnextP Tue?
., CanV youO sayV redistrictingV ?,(b) SpendingV theD dayN withhhP mommmaN !,(c) lmao!
..., s/oV toP theD coolA assN asianAofficerN 4P #1$ notR runninV myD licenseN and&#2$ notR takinV druN booN toP jailN ., ThankVuO God?
., #amen#Figure 1: Example tweets with gold annotations.
Under-lined tokens show tagger improvements due to featuresdetailed in Section 3 (respectively: TAGDICT, METAPH,and DISTSIM).In this paper, we produce an English POS taggerthat is designed especially for Twitter data.
Our con-tributions are as follows:?
we developed a POS tagset for Twitter,?
we manually tagged 1,827 tweets,?
we developed features for Twitter POS taggingand conducted experiments to evaluate them, and?
we provide our annotated corpus and trained POStagger to the research community.Beyond these specific contributions, we see thiswork as a case study in how to rapidly engi-neer a core NLP system for a new and idiosyn-cratic dataset.
This project was accomplished in200 person-hours spread across 17 people and twomonths.
This was made possible by two things:(1) an annotation scheme that fits the unique char-acteristics of our data and provides an appropriatelevel of linguistic detail, and (2) a feature set thatcaptures Twitter-specific properties and exploits ex-isting resources such as tag dictionaries and phoneticnormalization.
The success of this approach demon-strates that with careful design, supervised machinelearning can be applied to rapidly produce effectivelanguage technology in new domains.42Tag Description Examples %Nominal, Nominal + VerbalN common noun (NN, NNS) books someone 13.7O pronoun (personal/WH; notpossessive; PRP, WP)it you u meeee 6.8S nominal + possessive books?
someone?s 0.1?
proper noun (NNP, NNPS) lebron usa iPad 6.4Z proper noun + possessive America?s 0.2L nominal + verbal he?s book?ll iono(= I don?t know)1.6M proper noun + verbal Mark?ll 0.0Other open-class wordsV verb incl.
copula,auxiliaries (V*, MD)might gonnaought couldn?t iseats15.1A adjective (J*) good fav lil 5.1R adverb (R*, WRB) 2 (i.e., too) 4.6!
interjection (UH) lol haha FTW yearight2.6Other closed-class wordsD determiner (WDT, DT,WP$, PRP$)the teh its it?s 6.5P pre- or postposition, orsubordinating conjunction(IN, TO)while to for 2 (i.e.,to) 4 (i.e., for)8.7& coordinating conjunction(CC)and n & + BUT 1.7T verb particle (RP) out off Up UP 0.6X existential there,predeterminers (EX, PDT)both 0.1Y X + verbal there?s all?s 0.0Twitter/online-specific# hashtag (indicatestopic/category for tweet)#acl 1.0@ at-mention (indicatesanother user as a recipientof a tweet)@BarackObama 4.9~ discourse marker,indications of continuationof a message acrossmultiple tweetsRT and : in retweetconstruction RT@user : hello3.4U URL or email address http://bit.ly/xyz 1.6E emoticon :-) :b (: <3 o O 1.0Miscellaneous$ numeral (CD) 2010 four 9:30 1.5, punctuation (#, $, '', (,), ,, ., :, ``)!!!
....
?!?
11.6G other abbreviations, foreignwords, possessive endings,symbols, garbage (FW,POS, SYM, LS)ily (I love you) wby(what about you) ?s -->awesome...I?m1.1Table 1: The set of tags used to annotate tweets.
Thelast column indicates each tag?s relative frequency in thefull annotated data (26,435 tokens).
(The rates for M andY are both < 0.0005.
)2 AnnotationAnnotation proceeded in three stages.
For Stage 0,we developed a set of 20 coarse-grained tags basedon several treebanks but with some additional cate-gories specific to Twitter, including URLs and hash-tags.
Next, we obtained a random sample of mostlyAmerican English1 tweets from October 27, 2010,automatically tokenized them using a Twitter tok-enizer (O?Connor et al, 2010b),2 and pre-taggedthem using the WSJ-trained Stanford POS Tagger(Toutanova et al, 2003) in order to speed up man-ual annotation.
Heuristics were used to mark tokensbelonging to special Twitter categories, which tookprecedence over the Stanford tags.Stage 1 was a round of manual annotation: 17 re-searchers corrected the automatic predictions fromStage 0 via a custom Web interface.
A total of2,217 tweets were distributed to the annotators inthis stage; 390 were identified as non-English andremoved, leaving 1,827 annotated tweets (26,436 to-kens).The annotation process uncovered several situa-tions for which our tagset, annotation guidelines,and tokenization rules were deficient or ambiguous.Based on these considerations we revised the tok-enization and tagging guidelines, and for Stage 2,two annotators reviewed and corrected all of theEnglish tweets tagged in Stage 1.
A third anno-tator read the annotation guidelines and annotated72 tweets from scratch, for purposes of estimatinginter-annotator agreement.
The 72 tweets comprised1,021 tagged tokens, of which 80 differed from theStage 2 annotations, resulting in an agreement rateof 92.2% and Cohen?s ?
value of 0.914.
A finalsweep was made by a single annotator to correct er-rors and improve consistency of tagging decisionsacross the corpus.
The released data and tools usethe output of this final stage.2.1 TagsetWe set out to develop a POS inventory for Twitterthat would be intuitive and informative?while atthe same time simple to learn and apply?so as tomaximize tagging consistency within and across an-1We filtered to tweets sent via an English-localized user in-terface set to a United States timezone.2http://github.com/brendano/tweetmotif43notators.
Thus, we sought to design a coarse tagsetthat would capture standard parts of speech3 (noun,verb, etc.)
as well as categories for token varietiesseen mainly in social media: URLs and email ad-dresses; emoticons; Twitter hashtags, of the form#tagname, which the author may supply to catego-rize a tweet; and Twitter at-mentions, of the form@user, which link to other Twitter users from withina tweet.Hashtags and at-mentions can also serve as wordsor phrases within a tweet; e.g.
Is #qadaffi going down?.When used in this way, we tag hashtags with theirappropriate part of speech, i.e., as if they did not startwith #.
Of the 418 hashtags in our data, 148 (35%)were given a tag other than #: 14% are proper nouns,9% are common nouns, 5% are multi-word express-sions (tagged as G), 3% are verbs, and 4% are some-thing else.
We do not apply this procedure to at-mentions, as they are nearly always proper nouns.Another tag, ~, is used for tokens marking spe-cific Twitter discourse functions.
The most popularof these is the RT (?retweet?)
construction to publisha message with attribution.
For example,RT @USER1 : LMBO !
This man filed anEMERGENCY Motion for Continuance onaccount of the Rangers game tonight !Wow lmaoindicates that the user @USER1 was originally thesource of the message following the colon.
We ap-ply ~ to the RT and : (which are standard), andalso, which separates the author?s comment fromthe retweeted material.4 Another common discoursemarker is ellipsis dots (.
.
. )
at the end of a tweet,indicating a message has been truncated to fit the140-character limit, and will be continued in a sub-sequent tweet or at a specified URL.Our first round of annotation revealed that, due tononstandard spelling conventions, tokenizing undera traditional scheme would be much more difficult3Our starting point was the cross-lingual tagset presented byPetrov et al (2011).
Most of our tags are refinements of thosecategories, which in turn are groupings of PTB WSJ tags (seecolumn 2 of Table 1).
When faced with difficult tagging deci-sions, we consulted the PTB and tried to emulate its conventionsas much as possible.4These ?iconic deictics?
have been studied in other onlinecommunities as well (Collister, 2010).than for Standard English text.
For example, apos-trophes are often omitted, and there are frequentlywords like ima (short for I?m gonna) that cut acrosstraditional POS categories.
Therefore, we opted notto split contractions or possessives, as is commonin English corpus preprocessing; rather, we intro-duced four new tags for combined forms: {nominal,proper noun} ?
{verb, possessive}.5The final tagging scheme (Table 1) encompasses25 tags.
For simplicity, each tag is denoted with asingle ASCII character.
The miscellaneous categoryG includes multiword abbreviations that do not fitin any of the other categories, like ily (I love you), aswell as partial words, artifacts of tokenization errors,miscellaneous symbols, possessive endings,6 and ar-rows that are not used as discourse markers.Figure 2 shows where tags in our data tend to oc-cur relative to the middle word of the tweet.
Wesee that Twitter-specific tags have strong positionalpreferences: at-mentions (@) and Twitter discoursemarkers (~) tend to occur towards the beginning ofmessages, whereas URLs (U), emoticons (E), andcategorizing hashtags (#) tend to occur near the end.3 SystemOur tagger is a conditional random field (CRF; Laf-ferty et al, 2001), enabling the incorporation of ar-bitrary local features in a log-linear model.
Ourbase features include: a feature for each word type,a set of features that check whether the word con-tains digits or hyphens, suffix features up to length 3,and features looking at capitalization patterns in theword.
We then added features that leverage domain-specific properties of our data, unlabeled in-domaindata, and external linguistic resources.TWORTH: Twitter orthography.
We have featuresfor several regular expression-style rules that detectat-mentions, hashtags, and URLs.NAMES: Frequently-capitalized tokens.
Micro-bloggers are inconsistent in their use of capitaliza-tion, so we compiled gazetteers of tokens which arefrequently capitalized.
The likelihood of capital-ization for a token is computed as Ncap+?CN+C , where5The modified tokenizer is packaged with our tagger.6Possessive endings only appear when a user or the tok-enizer has separated the possessive ending from a possessor; thetokenizer only does this when the possessor is an at-mention.44Figure 2: Average position, relative to the middle word in the tweet, of tokens labeled with each tag.
Most tags fallbetween ?1 and 1 on this scale; these are not shown.N is the token count, Ncap is the capitalized to-ken count, and ?
and C are the prior probabilityand its prior weight.7 We compute features formembership in the top N items by this metric, forN ?
{1000, 2000, 3000, 5000, 10000, 20000}.TAGDICT: Traditional tag dictionary.
We addfeatures for all coarse-grained tags that each wordoccurs with in the PTB8 (conjoined with their fre-quency rank).
Unlike previous work that uses tagdictionaries as hard constraints, we use them as softconstraints since we expect lexical coverage to bepoor and the Twitter dialect of English to vary sig-nificantly from the PTB domains.
This feature maybe seen as a form of type-level domain adaptation.DISTSIM: Distributional similarity.
When train-ing data is limited, distributional features from un-labeled text can improve performance (Schu?tze andPedersen, 1993).
We used 1.9 million tokens from134,000 unlabeled tweets to construct distributionalfeatures from the successor and predecessor proba-bilities for the 10,000 most common terms.
The suc-cessor and predecessor transition matrices are hori-zontally concatenated into a sparse matrixM, whichwe approximate using a truncated singular value de-composition: M ?
USVT, where U is limited to50 columns.
Each term?s feature vector is its rowin U; following Turian et al (2010), we standardizeand scale the standard deviation to 0.1.METAPH: Phonetic normalization.
Since Twitterincludes many alternate spellings of words, we usedthe Metaphone algorithm (Philips, 1990)9 to createa coarse phonetic normalization of words to simplerkeys.
Metaphone consists of 19 rules that rewriteconsonants and delete vowels.
For example, in our7?
= 1100 , C = 10; this score is equivalent to the posteriorprobability of capitalization with a Beta(0.1, 9.9) prior.8Both WSJ and Brown corpora, no case normalization.
Wealso tried adding the WordNet (Fellbaum, 1998) and Moby(Ward, 1996) lexicons, which increased lexical coverage but didnot seem to help performance.9Via the Apache Commons implementation: http://commons.apache.org/codec/data, {thangs thanks thanksss thanx thinks thnx}are mapped to 0NKS, and {lmao lmaoo lmaooooo}map to LM.
But it is often too coarse; e.g.
{war we?rewear were where worry} map to WR.We include two types of features.
First, we usethe Metaphone key for the current token, comple-menting the base model?s word features.
Second,we use a feature indicating whether a tag is the mostfrequent tag for PTB words having the same Meta-phone key as the current token.
(The second featurewas disabled in both ?TAGDICT and ?METAPH ab-lation experiments.
)4 ExperimentsOur evaluation was designed to test the efficacy ofthis feature set for part-of-speech tagging given lim-ited training data.
We randomly divided the set of1,827 annotated tweets into a training set of 1,000(14,542 tokens), a development set of 327 (4,770 to-kens), and a test set of 500 (7,124 tokens).
We com-pare our system against the Stanford tagger.
Dueto the different tagsets, we could not apply the pre-trained Stanford tagger to our data.
Instead, we re-trained it on our labeled data, using a standard setof features: words within a 5-word window, wordshapes in a 3-word window, and up to length-3prefixes, length-3 suffixes, and prefix/suffix pairs.10The Stanford system was regularized using a Gaus-sian prior of ?2 = 0.5 and our system with a Gaus-sian prior of ?2 = 5.0, tuned on development data.The results are shown in Table 2.
Our tagger withthe full feature set achieves a relative error reductionof 25% compared to the Stanford tagger.
We alsoshow feature ablation experiments, each of whichcorresponds to removing one category of featuresfrom the full set.
In Figure 1, we show examplesthat certain features help solve.
Underlined tokens10We used the following feature modules in the Stanford tag-ger: bidirectional5words, naacl2003unknowns,wordshapes(-3,3), prefix(3), suffix(3),prefixsuffix(3).45Dev.
TestOur tagger, all features 88.67 89.37independent ablations:?DISTSIM 87.88 88.31 (?1.06)?TAGDICT 88.28 88.31 (?1.06)?TWORTH 87.51 88.37 (?1.00)?METAPH 88.18 88.95 (?0.42)?NAMES 88.66 89.39 (+0.02)Our tagger, base features 82.72 83.38Stanford tagger 85.56 85.85Annotator agreement 92.2Table 2: Tagging accuracies on development and testdata, including ablation experiments.
Features are or-dered by importance: test accuracy decrease due to ab-lation (final column).Tag Acc.
Confused Tag Acc.
ConfusedV 91 N !
82 NN 85 ?
L 93 V, 98 ~ & 98 ?P 95 R U 97 ,?
71 N $ 89 PD 95 ?
# 89 ?O 97 ?
G 26 ,A 79 N E 88 ,R 83 A T 72 P@ 99 V Z 45 ?~ 91 ,Table 3: Accuracy (recall) rates per class, in the test setwith the full model.
(Omitting tags that occur less than10 times in the test set.)
For each gold category, the mostcommon confusion is shown.are incorrect in a specific ablation, but are correctedin the full system (i.e.
when the feature is added).The ?TAGDICT ablation gets elects, Governor,and next wrong in tweet (a).
These words appearin the PTB tag dictionary with the correct tags, andthus are fixed by that feature.
In (b), withhh is ini-tially misclassified an interjection (likely caused byinterjections with the same suffix, like ohhh), but iscorrected by METAPH, because it is normalized to thesame equivalence class as with.
Finally, s/o in tweet(c) means ?shoutout?, which appears only once inthe training data; adding DISTSIM causes it to be cor-rectly identified as a verb.Substantial challenges remain; for example, de-spite the NAMES feature, the system struggles toidentify proper nouns with nonstandard capitaliza-tion.
This can be observed from Table 3, whichshows the recall of each tag type: the recall of propernouns (?)
is only 71%.
The system also struggleswith the miscellaneous category (G), which coversmany rare tokens, including obscure symbols and ar-tifacts of tokenization errors.
Nonetheless, we areencouraged by the success of our system on thewhole, leveraging out-of-domain lexical resources(TAGDICT), in-domain lexical resources (DISTSIM),and sublexical analysis (METAPH).Finally, we note that, even though 1,000 train-ing examples may seem small, the test set accuracywhen training on only 500 tweets drops to 87.66%,a decrease of only 1.7% absolute.5 ConclusionWe have developed a part-of-speech tagger for Twit-ter and have made our data and tools available to theresearch community at http://www.ark.cs.cmu.edu/TweetNLP.
More generally, we be-lieve that our approach can be applied to addressother linguistic analysis needs as they continue toarise in the era of social media and its rapidly chang-ing linguistic conventions.
We also believe that theannotated data can be useful for research into do-main adaptation and semi-supervised learning.AcknowledgmentsWe thank Desai Chen, Chris Dyer, Lori Levin, BehrangMohit, Bryan Routledge, Naomi Saphra, and Tae Yanofor assistance in annotating data.
This research was sup-ported in part by: the NSF through CAREER grant IIS-1054319, the U. S. Army Research Laboratory and theU.
S. Army Research Office under contract/grant num-ber W911NF-10-1-0533, Sandia National Laboratories(fellowship to K. Gimpel), and the U. S. Department ofEducation under IES grant R305B040063 (fellowship toM.
Heilman).ReferencesSitaram Asur and Bernardo A. Huberman.
2010.
Pre-dicting the future with social media.
In Proc.
of WI-IAT.Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on Twitter from biased and noisy data.In Proc.
of COLING.Lauren Collister.
2010.
Meaning variation of the iconicdeictics ?
and <?
in an online community.
In NewWays of Analyzing Variation.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.46Tim Finin, Will Murnane, Anand Karandikar, NicholasKeller, Justin Martineau, and Mark Dredze.
2010.
An-notating named entities in Twitter data with crowd-sourcing.
In Proceedings of the NAACL HLT 2010Workshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk.John Lafferty, Andrew McCallum, and Fernando Pereira.2001.
Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
In Proc.
ofICML.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: The Penn Treebank.
ComputationalLinguistics, 19:313?330.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010a.From tweets to polls: Linking text sentiment to publicopinion time series.
In Proc.
of ICWSM.Brendan O?Connor, Michel Krieger, and David Ahn.2010b.
TweetMotif: Exploratory search and topicsummarization for Twitter.
In Proc.
of ICWSM (demotrack).Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
ArXiv:1104.2086.Lawrence Philips.
1990.
Hanging on the Metaphone.Computer Language, 7(12).Alan Ritter, Colin Cherry, and Bill Dolan.
2010.
Unsu-pervised modeling of Twitter conversations.
In Proc.of NAACL.Hinrich Schu?tze and Jan Pedersen.
1993.
A vector modelfor syntagmatic and paradigmatic relatedness.
In Pro-ceedings of the 9th Annual Conference of the UW Cen-tre for the New OED and Text Research.Beaux Sharifi, Mark-Anthony Hutton, and Jugal Kalita.2010.
Summarizing microblogs automatically.
InProc.
of NAACL.Mike Thelwall, Kevan Buckley, and Georgios Paltoglou.2011.
Sentiment in Twitter events.
Journal of theAmerican Society for Information Science and Tech-nology, 62(2):406?418.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proc.
ofHLT-NAACL.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In Proc.
of ACL.Grady Ward.
1996.
Moby lexicon.
http://icon.shef.ac.uk/Moby.47
