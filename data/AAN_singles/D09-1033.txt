Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 315?323,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPClassifier Combination for Contextual Idiom DetectionWithout Labelled DataLinlin Li and Caroline SporlederSaarland UniversityPostfach 15 11 5066041 Saarbr?uckenGermany{linlin,csporled}@coli.uni-saarland.deAbstractWe propose a novel unsupervised approachfor distinguishing literal and non-literal useof idiomatic expressions.
Our model com-bines an unsupervised and a supervisedclassifier.
The former bases its decisionon the cohesive structure of the context andlabels training data for the latter, which canthen take a larger feature space into account.We show that a combination of both classi-fiers leads to significant improvements overusing the unsupervised classifier alone.1 IntroductionIdiomatic expressions are abundant in natural lan-guage.
They also often behave idiosyncraticallyand are therefore a significant challenge for naturallanguage processing systems.
For example, idiomscan violate selectional restrictions (as in push one?sluck), disobey typical subcategorisation constraints(e.g., in line without a determiner before line), orchange the default assignments of semantic rolesto syntactic categories (e.g., in break sth with X theargument X would typically be an instrument butfor the idiom break the ice it is more likely to fill apatient role, as in break the ice with Russia).In order to deal with such idiosyncracies and as-sign the correct analyses, NLP systems need to beable to recognise idiomatic expressions.
Much pre-vious research on idioms has been concerned withtype-based classification, i.e., dividing expressionsinto ?idiom?
or ?not idiom?
irrespective of their ac-tual use in a given context.
However, while someexpressions, such as by and large, always have anidiomatic meaning, several other expressions, suchas break the ice or spill the beans, can be used liter-ally as well as idiomatically (see examples (1) and(2), respectively).
Sometimes the literal usage caneven dominate in a domain, as for drop the ball,which occurs fairly frequently in a literal sense inthe sports section of news texts.
(1) Dad had to break the ice on the chicken troughs sothat they could get water.
(2) Somehow I always end up spilling the beans allover the floor and looking foolish when the clerkcomes to sweep them up.Hence, whether a particular occurrence of a po-tentially ambiguous expression has literal or non-literal meaning has to be inferred from the context(token-based idiom classification).
Recently, therehas been increasing interest in this classificationtask and both supervised and unsupervised tech-niques have been proposed.
The work we presenthere builds on previous research by Sporleder andLi (2009), who describe an unsupervised methodthat exploits the presence or absence of cohesiveties between the component words of a potentialidiom and its context to distinguish between literaland non-literal use.
If strong ties can be foundthe expression is classified as literal otherwise asnon-literal.
While this approach often works fairlywell, it has the disadvantage that it focuses exclu-sively on lexical cohesion, other linguistic cuesthat might influence the classification decision aredisregarded.We show that it is possible to improve onSporleder and Li?s (2009) results by employinga two-level strategy, in which a cohesion-basedunsupervised classifier is combined with a super-vised classifier.
We use the unsupervised classifierto label a sub-set of the test data with high confi-dence.
This sub-set is then passed on as trainingdata to the supervised classifier, which then labelsthe remainder of the data set.
Compared to a fullyunsupervised approach, this two-stage method hasthe advantage that a larger feature set can be ex-ploited.
This is beneficial for examples, in whichthe cohesive ties are relatively weak but which con-tain other linguistic cues for literal or non-literaluse.3152 Related WorkMost studies on idiom classification focus on type-based classification; few researchers have workedon token-based approaches (i.e., classification of anexpression in a given context).
Type-based meth-ods frequently exploit the fact that idioms have anumber of properties which differentiate them fromother expressions.
For example, they often exhibita degree of syntactic and lexical fixedness.
Someidioms, for instance, do not allow internal modi-fiers (*shoot the long breeze) or passivisation (*thebucket was kicked).
They also typically only al-low very limited lexical variation (*kick the vessel,*strike the bucket).Many approaches for identifying idioms focuson one of these two aspects.
For instance, measuresthat compute the association strength between theelements of an expression have been employedto determine its degree of compositionality (Lin,1999; Fazly and Stevenson, 2006) (see also Villav-icencio et al (2007) for an overview and a com-parison of different measures).
Other approachesuse Latent Semantic Analysis (LSA) to determinethe similarity between a potential idiom and itscomponents (Baldwin et al, 2003).
Low similar-ity is supposed to indicate low compositionality.Bannard (2007) looks at the syntactic fixednessof idiomatic expressions, i.e., how likely they areto take modifiers or be passivised, and comparesthis to what would be expected based on the ob-served behaviour of the component words.
Fazlyand Stevenson (2006) combine information aboutsyntactic and lexical fixedness (i.e., estimated de-gree of compositionality) into one measure.The few token-based approaches include a studyby Katz and Giesbrecht (2006), who devise a super-vised method in which they compute the meaningvectors for the literal and non-literal usages of agiven expression in the training data.
An unseentest instance of the same expression is then labelledby performing a nearest neighbour classification.Birke and Sarkar (2006) model literal vs. non-literal classification as a word sense disambiguationtask and use a clustering algorithm which comparestest instances to two automatically constructed seedsets (one with literal and one with non-literal ex-pressions), assigning the label of the closest set.While the seed sets are created without immediatehuman intervention they do rely on manually cre-ated resources such as databases of known idioms.Cook et al (2007) and Fazly et al (2009) pro-pose an alternative method which crucially relieson the concept of canonical form, which is a fixedform (or a small set of those) corresponding to thesyntactic pattern(s) in which the idiom normallyoccurs (Riehemann, 2001).1 The canonical formallows for inflectional variation of the head verb butnot for other variations (such as nominal inflection,choice of determiner etc.).
It has been observed thatif an expression is used idiomatically, it typicallyoccurs in its canonical form.
For example, Riehe-mann (2001, p. 34) found that for decomposableidioms 75% of the occurrences are in canonicalform, rising to 97% for non-decomposable idioms.2Cook et al exploit this behaviour and propose anunsupervised method which classifies an expres-sion as idiomatic if it occurs in canonical form andliteral otherwise.Finally, in earlier work, we proposed an unsu-pervised method which detects the presence or ab-sence of cohesive links between the componentwords of the idiom and the surrounding discourse(Sporleder and Li, 2009).
If such links can be foundthe expression is classified as ?literal?
otherwise as?non-literal?.
In this paper we show that the per-formance of such a classifier can be significantlyimproved by complementing it with a second-stagesupervised classifier.3 First Stage: Unsupervised ClassifierAs our first-stage classifier, we use the unsuper-vised model proposed by Sporleder and Li (2009).This model exploits the fact that words in a co-herent discourse exhibit lexical cohesion (Hallidayand Hasan, 1976), i.e.
concepts referred to in sen-tences are typically related to other concepts men-tioned elsewhere in the discourse.
Given a suitablemeasure of semantic relatedness, it is possible tocompute the strength of such cohesive ties betweenpairs of words.
While the component words ofliterally used expressions tend to exhibit lexical co-hesion with their context, the words of non-literallyused expressions do not.
For example, in (3) the ex-pression play with fire is used literally and the wordfire is related to surrounding words like grilling,dry-heat, cooking, and coals.
In (4), however playwith fire is used non-literally and cohesive ties be-1This is also the form in which an idiom is usually listedin a dictionary.2Decomposable idioms are expressions such as spill thebeans which have a composite meaning whose parts can bemapped to the words of the expression (e.g., spill??reveal?,beans??secret?
).316tween play or fire and the context are absent.
(3) Grilling outdoors is much more than just anotherdry-heat cooking method.
It?s the chance toplay with fire, satisfying a primal urge to stir aroundin coals .
(4) And PLO chairman Yasser Arafat has accused Israelof playing with fire by supporting HAMAS in itsinfancy.To determine the strength of cohesive links, theunsupervised model builds a graph structure (calledcohesion graph) in which all pairs of content wordsin the context are connected by an edge which isweighted by the pair?s semantic relatedness.
Thenthe connectivity of the graph is computed, definedas the average edge weight.
If the connectivityincreases when the component words of the idiomare removed, then there are no strong cohesive tiesbetween the expression and the context and theexample is labelled as ?non-literal?, otherwise it islabelled as ?literal?.To model semantic distance, we use the Nor-malized Google Distance (NGD, see Cilibrasi andVitanyi (2007)), which computes relatedness on thebasis of page counts returned by a search engine.3It is defined as follows:NGD(x, y) =max{log f(x), log f(y)} ?
log f(x, y)log M ?min{log f(x), log f(y)}(5)where x and y are the two words whose associationstrength is computed (e.g., fire and coal), f(x) isthe page count returned by the search engine for x(and likewise for f(y) and y), f(x, y) is the pagecount returned when querying for ?x AND y?, andM is the number of web pages indexed by thesearch engine.
The basic idea is that the more oftentwo terms occur together, relative to their overalloccurrence, the more closely they are related.We hypothesise that the unsupervised classifierwill give us relatively good results for some exam-ples.
For instance, in (3) there are several strongcues which suggest that play with fire is used liter-ally.
However, because the unsupervised classifieronly looks at lexical cohesion, it misses many otherclues which could help distinguish literal and non-literal usages.
For example, if break the ice isfollowed by the prepositions between or over as inexample (6), it is more likely to be used idiomati-cally (at least in the news domain).
(6) ?Gujral will meet Sharif on Monday and discussbilateral relations,?
the Press Trust of India added.3We employ Yahoo!
rather than Google since we foundthat it returns more stable counts.The minister said Sharif and Gujral would be ableto break the ice over Kashmir.Furthermore, idiomatic usages also exhibit co-hesion with their context but the cohesive ties arewith the non-literal meaning of the expression.
Forexample, in news texts, break the ice in its figu-rative meaning often co-occurs with discuss, rela-tions, talks or diplomacy (see (6)).
At the momentwe do not have any way to model these cohesivelinks, as we do not know the non-literal meaningof the idiom.4 However if we had labelled data wecould train a supervised classifier to learn these andother contextual clues.
The trained classifier mightthen be able to correctly classify examples whichwere misclassified by the unsupervised classifier,i.e., examples in which the cohesive ties are weakbut where other clues exist which indicate how theexpression is used.For example, in (7) there is weak cohesive evi-dence for a literal use of break the ice, due to thesemantic relatedness between ice and water.
How-ever, there are stronger cues for non-literal usage,such as the preposition between and the presenceof words like diplomats and talks, which are in-dicative of idiomatic usage.
Examples like thisare likely to be misclassified by the unsupervisedmodel; a supervised classifier, on the other hand,has a better chance to pick up on such additionalcues and predict the correct label.
(7) Next week the two diplomats will meet in an attemptto break the ice between the two nations.
A crucialissue in the talks will be the long-running waterdispute.4 Second Stage: Supervised ClassifierFor the supervised classifier, we used Support Vec-tor Machines as implemented by the LIBSVMpackage.5 We implemented four types of features,which encode both cohesive information and wordco-occurrence more generally.64It might be possible to compute the Normalized GoogleDistance between the whole expression and the words in thecontext, assuming that whenever the whole expression occursit is much more likely to be used figuratively than literally.For expressions in canonical form this is indeed often thecase (Riehemann, 2001), however there are exceptions (seeSection 6.1) for which such an approach would not work.5Available from: http://www.csie.ntu.edu.tw/?cjlin/libsvm/ We used the default parameters.6We also experimented with linguistically more informedfeatures, such as the presence of named entities in the localcontext of the expression, and properties of the subject orco-ordinated verbs, but we found that these features did notlead to a better performance of the supervised classifier.
Thisis probably partly due to data sparseness.317Salient Words (salW) This feature aims to iden-tify words which are particularly salient for literalusage.
We used a frequency-based definition ofsalience and computed the literal saliency score foreach word in a five-paragraph context around thetarget expression:sallit(w) =log flit(w)?
ilit(w)log fnonlit(w)?
inonlit(w)(8)where sallit(w) is the saliency score of the word wfor the class lit; flit(w) is the token frequency ofthe word w for literally used expressions; ilit(w) isthe number of instances of the target expressionsclassified as lit which co-occur with word w (andmutatis mutandis nonlit for target expressions la-belled as non-literal).7Words with a high sallitoccur much more fre-quently with literal usages than with non-literalones.
Conversely, words with a low sallitshouldbe more indicative of the non-literal class.
How-ever, we found that, in practice, the measure isbetter at picking out indicative words for the literalclass; non-literal usages tend to co-occur with awide range of words.
For example, among the high-est scoring words for break the ice we find thick,bucket, cold, water, reservoir etc.
While we do findwords like relations, diplomacy, discussions amongthe lowest scoring terms (i.e., terms indicative ofthe non-literal class), we also find a lot of noise(ask, month).
The effect is even more pronouncedfor other expressions (like drop the ball) whichtend to be used idiomatically in a wider varietyof situations (drop the ball on a ban of chemicalweapons, drop the ball on debt reduction etc.
).We implement the saliency score in our model byencoding for the 300 highest scoring words whetherthe word is present in the context of a given exam-ple and how frequently it occurs.8 Note that thisfeature (as well as the next one) can be computed ina per-idiom or a generic fashion.
In the former case,we would encode the top 300 words separately foreach idiom in the training set, in the latter across allidioms (with the consequence that more frequent7Our definition of sallitbears similarities with the wellknown tf.idf score.
We include both the term frequencies(flit) and the instance frequencies (ilit) in the formula becausewe believe both are important.
However, the instance fre-quency is more informative and less sensitive to noise becauseit indicates that expression classified as ?literal?
consistentlyco-occurs with the word in question.
Therefore we weightdown the effect of the term frequency by taking its log.8We also experimented with different feature dimensionsbesides 300 but did not find a big difference in performance.idioms in the training set contribute to more po-sitions in the feature vector).
We found that, inpractice, it does not make a big difference whichvariant is used.
Moreover, in our bootstrappingscenario, we cannot ensure that we have sufficientexamples of each idiom in the training set to trainseparate classifiers, so we opted for generic modelsthroughout all experiments.Related Words (relW) This feature set is a vari-ant of the previous one.
Here we score the wordsnot based on their saliency but we determine thesemantic relatedness between the noun in the id-iomatic expression and each word in the globalcontext, using the Normalized Google Distancementioned in Section 3.
Again we encode the 300top-scoring words.While the related words feature is less prone tooverestimation of accidental co-occurrence than thesaliency feature, it has the disadvantage of conflat-ing different word senses.
For example, among thehighest scoring words for ice are cold, melt, snow,skate, hockey but also cream, vanilla, dessert.Relatedness Score (relS) The fourth feature setimplements the relatedness score which encodesthe scores for the 100 most highly weighted edgesin the cohesion graph of an instance.9 If thesescores are high, there are many cohesive ties withthe surrounding discourse and the target expressionis likely to be used literally.Discourse Connectivity (connect.)
Finally, weimplemented two features which look at the cohe-sion graph of an instance.
We encode the connec-tivity of the graph (i) when the target expressionis included and (ii) when it is excluded.
The un-supervised classifier uses the difference betweenthese two values to make its prediction.
By encod-ing the absolute connectivity values as features weenable the supervised classifier to make use of thisinformation as well.5 Combining the ClassifiersAs mentioned before, we use the unsupervised clas-sifier to label an initial training set for the super-vised one.
To ensure that the training set doesnot contain too much noise, we only add those ex-amples about which the unsupervised classifier is9We only used the 100 highest ranked edges because weare looking at a specific context here rather than the contextsof the literal or non-literal class overall.
Since the contexts weuse are only five paragraphs long, recording the 100 strongestedges seems sufficient.318most confident.
We thus need to address two ques-tions: (i) how to define a confidence function forthe unsupervised classifier, and (ii) how to set theconfidence threshold governing what proportion ofthe data set is used for training the second classifier.The first question is relatively easy to answer:as the unsupervised classifier bases its decision onthe difference in connectivity between including orexcluding the component words of the idiom in thecohesion graph, an obvious choice for a confidencefunction is the difference in connectivity; i.e., thehigher the difference, the higher the confidence ofthe classifier in the predicted label.The confidence threshold could be selected onthe basis of the unsupervised classifier?s perfor-mance on a development set.
Note that when choos-ing such a threshold there is usually a trade-off be-tween the size of the training set and the amount ofnoise in it: the lower the threshold, the larger andthe noisier the training set.
Ideally we would likea reasonably-sized training set which is also rela-tively noise-free, i.e., does not contain too manywrongly labelled examples.
One way to achievethis is to start with a relatively small training setand then expand it gradually.A potential problem for the supervised classifieris that our data set is relatively imbalanced, withthe non-literal class being four times as frequentas the literal class.
Supervised classifiers oftenhave problems with imbalanced data and tend to beoverly biased towards the majority class (see, e.g.,Japkowicz and Stephen (2002)).
To overcome thisproblem, we experimented with boosting the literalclass with additional examples.10We describe ourmethods for training set enlargement and boostingthe literal class in the remainder of this section.Iteratively Enlarging the Training Set A typi-cal method for increasing the training set is to gothrough several iterations of enlargement and re-training.11 We adopt a conservative enlargementstrategy: we only consider instances on whose la-bels both classifiers agree and we use the confi-dence function of the unsupervised classifier todetermine which of these examples to add to thetraining set.
The motivation for this is that we hy-pothesise that the supervised classifier will not have10Throughout this paper, we use the term ?boosting?
in anon-technical sense.11In our case re-training also involves re-computing theranked lists of salient and related words.
As the process goeson the classifier will be able to discover more and more usefulcue words and encode them in the feature vector.a very good performance initially, as it is trainedon a very small data set.
As a consequence its con-fidence function may also not be very accurate.
Onthe other hand, we know from Sporleder and Li(2009) that the unsupervised classifier has a rea-sonably good performance.
So while we give thesupervised classifier a veto-right, we do not allowit to select new training data by itself or overturnclassifications made by the unsupervised classifier.A similar strategy was employed by Ng andCardie (2003) in a self-training set-up.
However,while they use an ensemble of supervised classi-fiers, which they re-train after each iteration, wecan only re-train the second classifier; the first one,being unsupervised, will never change its predic-tion.
Hence it does not make sense to go througha large number of iterations; the more iterationswe go through, the closer the performance of thecombined classifier will be to that of the unsuper-vised one because that classifier will label a largerand larger proportion of the data.
However, goingthrough one or two iterations allows us to slowlyenlarge the training set and thereby gradually im-prove the performance of the supervised classifier.In each iteration, we select 10% of the remain-ing examples to be added to the training set.12We could simply add those 10% of the data aboutwhich the unsupervised classifier is most confident,but if the classifier was more confident about oneclass than about the other, we would risk obtain-ing a severely imbalanced training set.
Hence, wedecided to separate examples classified as ?literal?from those classified as ?non-literal?
and add thetop 10% from each set.
Provided the automaticclassification is reasonably accurate, this will en-sure that the distribution of classes in the trainingset is roughly similar to that in the overall data setat least at the early stages of the bootstrapping.Boosting the Literal Class As the process goeson, we are still likely to introduce more and moreimbalance in the training set.
This is due to thefact that the supervised classifier is likely to havesome bias towards the majority class (and our ex-periments in Section 6.2 suggest that this is indeedthe case).
Hence, as the bootstrapping process goeson, potentially more and more examples will belabelled as ?non-literal?
and if we always select thetop 10% of these, our training set will gradually12Since we do not have a separate development set, wechose the value of 10% intuitively as it seemed a reasonablygood threshold.319become more imbalanced.
This is a well-knownproblem for bootstrapping approaches (Blum andMitchell, 1998; Le et al, 2006).
We could coun-teract this by selecting a higher proportion of ex-amples labelled as ?literal?.
However given thatthe number of literal examples in our data set isrelatively small, we would soon deplete our literalinstance pool and moreover, because we would beforced to add less confidently labelled examplesfor the literal class, we are likely to introduce morenoise in the training set.A better option is to boost the literal class withexternal examples.
To do this we exploit thefact that non-canonical forms of idioms are highlylikely to be used literally.
Given that our data setonly contains canonical forms (see Section 6.1), weautomatically extract non-canonical form variantsand label them as ?literal?.
To generate possiblevariants, we either (i) change the number of thenoun (e.g., rock the boat becomes rock the boats),(ii) change the determiner (e.g., rock a boat), or (iii)replace the verb or noun by one of its synonyms,hypernyms, or siblings from WordNet (e.g., rockthe ship).
While this strategy does not give us addi-tional literal examples for all idioms, for examplewe were not able to find non-canonical form occur-rences of sweep under the carpet in the Gigawordcorpus, for most idioms we were able to gener-ate additional examples.
Note that this data set ispotentially noisy as not all non-canonical form ex-amples are used literally.
However, when checkinga small sample manually, we found that only verysmall percentage (<< 1%) was mis-labelled.To reduce the classifier bias when enlarging thetraining set, we add additional literal examples dur-ing each iteration to ensure that the class distri-bution does not deviate too much from the dis-tribution originally predicted by the unsupervisedclassifier.13 The examples to be added are selectedrandomly but we try to ensure that each idiom isrepresented.
When reporting the results, we disre-gard these additional external examples.6 Experiments and ResultsWe carried out a number of different experiments.In Section 6.2 we investigate the performance ofthe different features of the supervised classifierand in Section 6.3 we look more closely at the13We are assuming that the true distribution is not knownand use the predictions of the unsupervised classifier to ap-proximate the true distribution.behaviour of the combined classifier.
We start bydescribing the data set.6.1 DataWe used the data from Sporleder and Li (2009),which consist of 17 idioms that can be used bothliterally and non-literally (see Table 1).
For eachexpression, all canonical form occurrences wereextracted from the Gigaword corpus together withfive paragraphs of context and labelled as ?literal?or ?non-literal?.14 The inter-annotator agreementon a small sample of doubly annotated exampleswas 97% and the kappa score 0.7 (Cohen, 1960).non-expression literal literal allback the wrong horse 0 25 25bite off more than one can chew 2 142 144bite one?s tongue 16 150 166blow one?s own trumpet 0 9 9bounce off the wall* 39 7 46break the ice 20 521 541drop the ball* 688 215 903get one?s feet wet 17 140 157pass the buck 7 255 262play with fire 34 532 566pull the trigger* 11 4 15rock the boat 8 470 478set in stone 9 272 281spill the beans 3 172 175sweep under the carpet 0 9 9swim against the tide 1 125 126tear one?s hair out 7 54 61all 862 3102 3964Table 1: Idiom statistics (* indicates expressionsfor which the literal usage is more common thanthe non-literal one)6.2 Feature Analysis for the SupervisedClassifierIn a first experiment, we tested the contribution ofthe different features (Table 2).
For each set, wetrained a separate classifier and tested it in 10-foldcross-validation mode.
We also tested the perfor-mance of the first three features combined (salientand related words and relatedness score) as wewanted to know whether their combination leadsto performance gains over the individual classifiers.Moreover, testing these three features in combi-nation allows us to assess the contribution of theconnectivity feature, which is most closely relatedto the unsupervised classifier.
We report the accu-racy, and because our data are fairly imbalanced,14The restriction to canonical forms was motivated by thefact that for the mostly non-decomposable idioms in the set,the vast majority (97%) of non-canonical form occurrenceswill be used literally (see Section 2).320also the F-Score for the minority class (?literal?).Avg.
literal (%) Avg.
(%)Feature Prec.
Rec.
F-Score Acc.salW 77.10 56.10 65.00 86.83relW 78.00 43.20 55.60 84.99relS 74.90 37.50 50.00 83.68connectivity 78.30 2.10 4.10 78.58salW+relW+relS 82.90 63.50 71.90 89.20all 85.80 66.60 75.00 90.34Table 2: Performance of different feature sets, 10-fold cross-validationIt can be seen that the salient words (salW) fea-ture has the highest performance of the individualfeatures, both in terms of accuracy and in terms ofliteral F-Score, followed by related words (relW),and relatedness score (relS).
Intuitively, it is plausi-ble that the saliency feature performs quite well asit can also pick up on linguistic indicators of idiomusage that do not have anything to do with lexicalcohesion.
However, a combination of the first threefeatures leads to an even better performance, sug-gesting that the features do indeed model somewhatdifferent aspects of the data.The performance of the connectivity feature isalso interesting: while it does not perform very wellon its own, as it over-predicts the non-literal class, itnoticeably increases the performance of the modelwhen combined with the other features, suggestingthat it picks up on complementary information.6.3 Testing the Combined ClassifierWe experimented with different variants of thecombined classifier.
The results are shown in Ta-ble 3.
In particular, we looked at: (i) combining thetwo classifiers without training set enlargement orboosting of the literal class (combined), (ii) boost-ing the literal class with 200 automatically labellednon-canonical form examples (combined+boost),(iii) enlarging the training set by iteration (com-bined+it), and (iv) enlarging the training set byiteration and boosting the literal class after eachiteration (combined+boost+it).
The table showsthe literal precision, recall and F-Score of the com-bined model (both classifiers) on the complete dataset (excluding the extra literal examples).
Note thatthe results for the set-ups involving iterative train-ing set enlargement are optimistic: since we do nothave a separate development set, we report the op-timal performance achieved during the first seveniterations.
In a real set-up, when the optimal num-ber of iterations is chosen on the basis of a separatedata set, the results may be lower.
The table alsoshows the majority class baseline (Basemaj), andthe overall performance of the unsupervised model(unsup) and the supervised model when trained in10-fold cross-validation mode (super 10CV).Model PreclReclF-ScorelAcc.Basemaj- - - 78.25unsup.
50.04 69.72 58.26 78.38combined 83.86 45.82 59.26 86.30combined+boost 70.26 62.76 66.30 86.13combined+it?85.68 46.52 60.30 86.68combined+boost+it?71.86 66.36 69.00 87.03super.
10CV 85.80 66.60 75.00 90.34Table 3: Results for different classifiers; ?
indicatesbest performance (optimistic)It can be seen that the combined classifier is 8%more accurate than both the majority baseline andthe unsupervised classifier.
This amounts to anerror reduction of over 35% (the difference is sta-tistically significant, ?2 test, p << 0.01).
Whilethe F-Score of the unboosted combined classifier iscomparable to that of the unsupervised one, boost-ing the literal class leads to a 7% increase, dueto a significantly increased recall, with no signif-icant drop in accuracy.
These results show thatcomplementing the unsupervised classifier with asupervised one, can lead to tangible performancegains.
Note that the accuracy of the combined clas-sifier, which uses no manually labelled trainingdata, is only 4% below that of a fully supervisedclassifier; in other words, we do not lose much bystarting with an automatically labelled data set.
It-erative enlargement of the training set can lead tofurther improvements, especially when combinedwith boosting to reduce the classifier bias.To get a better idea of the effect of training setenlargement, we plotted the accuracy and F-Scoreof the combined classifier for a given number ofiterations with boosting (Figure 1) and without (Fig-ure 2).
It can be seen that enlargement has a notice-able positive effect if combined with boosting.
Ifthe literal class is not boosted, the increasing biasof the classifier seems to outweigh most of the pos-itive effects from the enlarged training set.
Figure 1also shows that the best performance is obtained af-ter a relatively small number of iterations (namelytwo), as expected.15 With more iterations the per-formance decreases again.
However, it decays rel-15Note that this also depends on the confidence threshold.For example, if a threshold of 5% is chosen, more iterationsmay be required for optimal performance.321atively gracefully and even after seven iterations,when more than 40% of the data are classified bythe unsupervised classifier, the combined classifierstill achieves an overall performance that is sig-nificantly above that of the unsupervised classifier(84.28% accuracy compared to 78.38%, significantat p << 0.01).
Hence, the combined classifierseems not to be very sensitive to the exact numberof iterations and performs reasonably well even ifthe number of iterations is sub-optimal.1234567Iterations5050555560606565707075758080858590909595PerformanceAcc.combinedAcc.unsupervisedF-Score combinedF-Score unsupervisedFigure 1: Accuracy and literal F-Score on completedata set after different iterations with boosting ofthe literal class1234567Iteration5050555560606565707075758080858590909595Performance246Acc.combinedAcc.unsupervisedF-Score combinedF-Score unsupervisedFigure 2: Accuracy and literal F-Score on completedata set after different iterations without boostingof the literal classFigure 3 shows how the training set increasesas the process goes on16 and how the number ofmis-classifications in the training set develops.
In-terestingly, when going from the first to the seconditeration the training set nearly doubles (from 396to 669 instances), while the proportion of errors isalso reduced by a third (from 7% to 5%).
Hence,the training set does not only grow but the pro-portion of noise in it decreases, too.
This shows16Again, we disregard the extra literal examples here.that our conservative enlargement strategy is fairlysuccessful in selecting correctly labelled examples.Only at later stages, when the classifier bias takesover, does the proportion of noise increase again.4006008001000120014001600Itemsin Training Set44.555.566.577.58Classification Errortraining set error rateFigure 3: Training set size and error in training setat different iterations7 ConclusionWe presented a two-stage classification approachfor distinguishing literal and non-literal use of id-iomatic expressions.
Our approach complementsan unsupervised classifier, which exploits informa-tion about the cohesive structure of the discourse,with a supervised classifier.
The latter can makeuse of a range of features and therefore base itsclassification decision on additional properties ofthe discourse, besides lexical cohesion.
We showedthat such a combined classifier can lead to a sig-nificant reduction of classification errors.
Its per-formance can be improved further by iterativelyincreasing the training set in a bootstrapping loopand by adding additional examples of the literalclass, which is typically the minority class.
Wefound that such examples can be obtained automat-ically by extracting non-canonical variants of thetarget idioms from an unlabelled corpus.Future work should look at improving the su-pervised classifier, which so far has an accuracyof 90%.
While this is already pretty good, a moresophisticated model might lead to further improve-ments.
For example, one could experiment withlinguistically more informed features.
While ourinitial studies in this direction were negative, care-ful feature engineering might lead to better results.AcknowledgementsThis work was funded by the Cluster of Excellence ?Multi-modal Computing and Interaction?.322ReferencesTimothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model ofmultiword expression decomposability.
In Proceed-ings of the ACL 2003 Workshop on Multiword Ex-pressions: Analysis, Acquisition and Treatment.Colin Bannard.
2007.
A measure of syntactic flex-ibility for automatically identifying multiword ex-pressions in corpora.
In Proceedings of the ACL-07Workshop on A Broader Perspective on MultiwordExpressions.Julia Birke and Anoop Sarkar.
2006.
A clusteringapproach for the nearly unsupervised recognition ofnonliteral language.
In Proceedings of EACL-06.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Pro-ceedings of COLT-98.Rudi L. Cilibrasi and Paul M.B.
Vitanyi.
2007.
TheGoogle similarity distance.
IEEE Trans.
Knowledgeand Data Engineering, 19(3):370?383.Jacob Cohen.
1960.
A coefficient of agreementfor nominal scales.
Educational and PsychologicalMeasurements, 20:37?46.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2007.
Pulling their weight: Exploiting syntacticforms for the automatic identification of idiomaticexpressions in context.
In Proceedings of the ACL-07 Workshop on A Broader Perspective on Multi-word Expressions.Afsaneh Fazly and Suzanne Stevenson.
2006.
Auto-matically constructing a lexicon of verb phrase id-iomatic combinations.
In Proceedings of EACL-06.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identification ofidiomatic expressions.
Computational Linguistics,35(1):61?103.M.A.K.
Halliday and R. Hasan.
1976.
Cohesion inEnglish.
Longman House, New York.Nathalie Japkowicz and Shaju Stephen.
2002.
Theclass imbalance problem: A systematic study.
In-telligent Data Analysis Journal, 6(5):429?450.Graham Katz and Eugenie Giesbrecht.
2006.
Au-tomatic identification of non-compositional multi-word expressions using latent semantic analysis.
InProceedings of the ACL/COLING-06 Workshop onMultiword Expressions: Identifying and ExploitingUnderlying Properties.Anh-Cuong Le, Akira Shimazu, and Le-Minh Nguyen.2006.
Investigating problems of semi-supervisedlearning for word sense disambiguation.
In Proc.ICCPOL-06.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL-99,pages 317?324.Vincent Ng and Claire Cardie.
2003.
Weakly super-vised natural language learning without redundantviews.
In Proc.
of HLT-NAACL-03.Susanne Riehemann.
2001.
A Constructional Ap-proach to Idioms and Word Formation.
Ph.D. thesis,Stanford University.Caroline Sporleder and Linlin Li.
2009.
Unsupervisedrecognition of literal and non-literal use of idiomaticexpressions.
In Proceedings of EACL-09.Aline Villavicencio, Valia Kordoni, Yi Zhang, MarcoIdiart, and Carlos Ramisch.
2007.
Validation andevaluation of automatically acquired multiword ex-pressions for grammar engineering.
In Proceedingsof EMNLP-07.323
