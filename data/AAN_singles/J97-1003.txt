TextTiling: Segmenting Text intoMulti-paragraph Subtopic PassagesMarti A. Hearst*Xerox PARCTextTiling is a technique for subdividing texts into multi-paragraph units that represent passages,or subtopics.
The discourse cues for identifying major subtopic shifts are patterns of lexicalco-occurrence and distribution.
The algorithm is fully implemented and is shown to producesegmentation that corresponds well to human judgments of the subtopic boundaries of 12 texts.Multi-paragraph subtopic segmentation should be useful for many text analysis tasks, includinginformation retrieval and summarization.1.
IntroductionMost work in discourse processing, both theoretical nd computational, has focusedon analysis of interclausal or intersentential phenomena.
This level of analysis is im-portant for many discourse-processing tasks, such as anaphor esolution and dialoguegeneration.
However, important and interesting discourse phenomena also occur atthe level of the paragraph.
This article describes a paragraph-level model of discoursestructure based on the notion of subtopic shift, and an algorithm for subdividingexpository texts into multi-paragraph "passages" or subtopic segments.In this work, the structure of an expository text is characterized asa sequence ofsubtopical discussions that occur in the context of one or more main topic discussions.Consider a21-paragraph science news article, called Stargazers, whose main topic is theexistence of life on earth and other planets.
Its contents can be described as consistingof the following subtopic discussions (numbers indicate paragraphs):lm3 Intro - the search for life in space4--5 The moon's chemical composition6m8 How early earth-moon proximity shaped the moon9--12 How the moon helped life evolve on earth13 Improbability of the earth-moon system14- -16  Binary/trinary star systems make life unlikely17--18 The low probability of nonbinary/trinary systems19--20 Properties of earth's sun that facilitate life21 SummarySubtopic structure is sometimes marked in technical texts by headings and sub-headings.
Brown and Yule (1983, 140) state that this kind of division is one of the mostbasic in discourse.
However, many expository texts consist of long sequences of para-graphs with very little structural demarcation, and for these a subtopical segmentationcan be useful.
* 3333 Coyote Hill Rd, Palo Alto, CA.
94304.
E-mail: hearst@parc.xerox.com(~) 1997 Association for Computational LinguisticsComputational Linguistics Volume 23, Number 1This article describes fully implemented techniques for the automatic detectionof multi-paragraph subtopical structure.
Because the goal is to partition texts intocontiguous, nonoverlapping subtopic segments, I call the general approach TextTiling(Hearst, 1993, 1994a, 1994b).
1Subtopic discussions are assumed to occur within thescope of one or more overarching main topics, which span the length of the text.This two-level structure is chosen for reasons of computational feasibility and for thepurposes of the application types described below.TextTiling makes use of patterns of lexical co-occurrence and distribution.
Thealgorithm has three parts: tokenization into terms and sentence-sized units, determi-nation of a score for each sentence-sized unit, and detection of the subtopic bound-aries, which are assumed to occur at the largest valleys in the graph that results fromplotting sentence-units against scores.
Three methods for score assignment have beenexplored: blocks, vocabulary introductions, and chains, although only the first twoare evaluated in this article (the third is discussed in Hearst \[1994b\]).
All three scoringmethods make use only of patterns of lexical co-occurrence and distribution withintexts, eschewing other kinds of discourse cues.The ultimate goal of passage-level structuring is not just to identify the subtopicunits, but also to identify and label their subject matter.
This article focuses onlyon the discovery of the segment boundaries, but there is extensive ongoing researchon automated topic classification (Lewis and Hayes 1994).
Most classification workfocuses on identifying main topic(s), as opposed to TextTiling's method of findingboth globally distributed main topics and locally occurring subtopics; nevertheless,variations on some existing algorithms hould be applicable to subtopic lassification.The next section argues for the need for algorithms that can detect multi-paragraphsubtopic structure (referred to here interchangeably as passages and subtopic seg-ments), and discusses application areas that should benefit from such structure.
Sec-tion 3 describes in more detail what is meant in this article by "subtopic" and presentsa description of the discourse model that underlies this work.
Section 4 introduces thegeneral framework of using lexical co-occurrence information for detecting subtopicshift, and describes other related work in empirical discourse analysis.
The TextTil-ing algorithms are described in more detail in Section 5 and their performance isassessed in Section 6.
Finally, Section 7 summarizes the work and describes futuredirections.2.
Why Multi-paragraph Units?In school we are taught that paragraphs are to be written as coherent, self-containedunits, complete with topic sentence and summary sentence.
In real-world text, theseexpectations are often not met.
Paragraph markings are not always used to indicatea change in discussion, but instead can sometimes be invoked just to break up thephysical appearance of the text in order to aid reading (Stark 1988).
A conspicuousexample of this practice can be found in the layout of the columns of text in manynewspapers (Longacre 1979).
Brown and Yule (1983, 95-96) note that text genre hasa strong influence on the role of paragraph markings, and that markings differ fordifferent languages.
Hinds (1979, 137) also suggests that different discourse types havedifferent organizing principles.Although most discourse segmentation work is done at a finer granularity than1 A free version of the code, written in C, is available for research purposes.
Contact the author for moreinformation.34Hearst TextTilingthat suggested here, multi-paragraph segmentation has many potential applications.TextTiling is geared towards expository text; that is, text that explicitly explains orteaches, as opposed to, say, literary texts, since expository text is better suited to themain target applications of information retrieval and summarization.
More specifi-cally, TextTiling is meant to apply to expository text that is not heavily stylized orstructured, and for simplicity does not make use of headings or other kinds of or-thographic information.
A typical example is a 5-page science magazine article or a20-page nvironmental impact report.This section concentrates on two application areas for which the need for multi-paragraph units has been recognized: hypertext display and information retrieval.There are also potential applications in some other areas, such as text summarization.Some summarization algorithms extract sentences directly from the text.
These meth-ods make use of information about the relative positions of the sentences in the text(Kupiec, Pedersen, and Chen 1995; Chen and Withgott 1992).
However, these methodsdo not use subtopic structure to guide their choices, focusing more on the beginningand ending of the document and on position within paragraphs.
Paice (1990) recog-nizes the need for taking topical structure into account but does not suggest a methodfor determining such structure.Another area that models the multi-paragraph unit is automated text generation.Mooney, Carberry, and McCoy (1990) present a method centered around the notion ofBasic Blocks: multi-paragraph units of text, each of which consists of (1) an organiza-tional focus such as a person or a location, and (2) a set of concepts related to thatfocus.
Their scheme mphasizes the importance of organizing the high-level structureof a text according to its topical content, and afterwards incorporating the necessaryrelated information, as reflected in discourse cues, in a finer-grained pass.2.1 Online Text Display and HypertextResearch in hypertext and text display has produced hypotheses about how textualinformation should be displayed to users.
One study of an on-line documentationsystem (Girill 1991) compares display of fine-grained portions of text (i.e., sentences),full texts, and intermediate-sized units.
Girill finds that divisions at the fine-grainedlevel are less efficient o manage and less effective in delivering useful answers thanintermediate-sized units of text.Girill does not make a commitment about exactly how large the desired text unitshould be, but talks about "passages" and describes passages in terms of the com-municative goals they accomplish (e.g., a problem statement, an illustrative xample,an enumerated list).
The implication is that the proper unit is the one that groupstogether the information that performs ome communicative function; in most cases,this unit will range from one to several paragraphs.
(Girill also finds that using doc-ument boundaries is more useful than ignoring document boundaries, as is done insome hypertext systems, and that premarked sectional information, if available andnot too long, is an appropriate unit for display.
)Tombaugh, Lickorish, and Wright (1987) explore issues relating to ease of read-ability of long texts on CRT screens.
Their study explores the usefulness of multiplewindows for organizing the contents of long texts, hypothesizing that providing read-ers with spatial cues about the location of portions of previously read texts will aidin their recall of the information and their ability to quickly locate information thathas already been read once.
In the experiment, he text is divided using premarkedsectional information, and one section is placed in each window.
They conclude thatsegmenting the text by means of multiple windows can be very helpful if readers arefamiliar with the mechanisms supplied for manipulating the display.35Computational Linguistics Volume 23, Number 1Converting text to hypertext, in what is called post hoc authoring (Marchionini,Liebscher, and Lin 1991), requires division of the original text into meaningful nits (atask noted by these authors to be a challenging one) as well as meaningful intercon-nection of the units.
Automated multi-paragraph segmentation should help with thefirst step of this process, and is more important than ever now that pre-existing docu-ments are being put up for display on the World Wide Web.
Salton et al (1996) haverecognized the need for multi-paragraph units in the automatic reation of hypertextlinks as well as theme generation (this work is discussed in Section 5).2.2 Information RetrievalIn the field of information retrieval, there has recently been a surge of interest inthe role of passages in full text.
Until very recently, most information retrieval ex-periments made use only of titles and abstracts, bibliographic entries, or very shortnewswire articles, as opposed to full text.
When long texts are available, there arisesthe question: can retrieval results be improved if the query is compared against only apassage or subpart of the text, as opposed to the text as a whole?
And if so, what sizeunit should be used?
In this context, "passage" refers to any segment of text isolatedfrom the full text.
This includes author-determined segments, marked orthographi-cally (paragraphs, ections, and chapters) (Hearst and Plaunt 1993; Salton, Allan, andBuckley 1993; Moffat et al 1994) and/or automatically derived units of text, includ-ing fixed-length blocks (Hearst and Plaunt 1993; Callan 1994), segments motivatedby subtopic structure (TextTiles) (Hearst and Plaunt 1993), or segments motivated byproperties of the query (Mittendorf and Sch~iuble 1994).Hearst and Plaunt (1993), in some early passage-based retrieval experiments, re-port improved results using passages over full-text documents, but do not find asignificant difference between using motivated subtopic segments and arbitrarily cho-sen block lengths that approximated the average subtopic segment length.
Salton, Al-lan, and Buckley (1993), working with encyclopedia text, find that comparing a queryagainst orthographically marked sections and then paragraphs i  more successful thancomparing against full documents alone.Moffat et al (1994) find, somewhat surprisingly, that manually supplied section-ing information may lead to poorer etrieval results than techniques that automaticallysubdivide the text.
They compare two methods of subdividing long texts.
The first con-sists of using author-supplied sectioning information.
The second uses a heuristic inwhich small numbers of paragraphs are grouped together until they exceed a sizethreshold.
The results are that the small, artificial multi-paragraph groupings eemedto perform better than the author-supplied sectioning information (which usually con-sisted of many more paragraphs than Moffet et al's subdivision algorithm or Text-Tiling would create).
More experiments in this vein are necessary to firmly establishthis result, but it does lend support o the conjecture that multi-paragraph subtopic-sized segments, uch as those produced by TextTiling, are useful for similarity-basedcomparisons in information retrieval.It will not be surprising if motivated subtopic segments are not found to per-form significantly better than appropriately sized, but arbitrarily segmented, units ina coarse-grained information retrieval evaluation.
At TREC, the most prominent in-formation retrieval evaluation platform (Harman 1993), the top 1,000 documents areevaluated for each query, and the best-performing systems tend to use very simplestatistical methods for ranking documents.
In this kind of evaluation methodology,subtle distinctions in analysis techniques tend to be lost, whether those distinctions behow accurately words are reduced to their roots (Hull and Grefenstette 1995; Harman1991), or exactly how passages are subdivided.
The results of Hearst and Plaunt (1993),36Hearst TextTilingSalton, Allan, and Buckley (1993) and Moffat et al (1994) suggest that it is the natureof the intermediate size of the passages that matters.Perhaps amore appropriate use of motivated segment information is in the displayof information to the user.
One obvious way to use segmentation information is to havethe system display the passages with the closest similarity to the query, and to displaya passage-based summary of the documents' contents.As a more elaborate xample of using segmentation in full-text information ac-cess, I have used the results of TextTiling in a new paradigm for display of retrievalresults (Hearst 1995).
This approach, called TileBars, allows the user to make informeddecisions about which documents and which passages of those documents to view,based on the distributional behavior of the query terms in the documents.
TileBarsallows users to specify different sets of query terms, as discussed later.
The goal is tosimultaneously and compactly indicate:1. the relative length of the document,2.
the frequency of the term sets in the document, and3.
the distribution of the term sets with respect o the document and toeach other.TextTiling is used to partition each document, in advance, into a set of multi-paragraphsubtopical segments.Figure 1 shows an example query about automated systems for medical diagno-sis, run over the ZIFF portion of the TIPSTER collection (Harman 1993).
Each largerectangle next to a title indicates a document, and each square within the rectanglerepresents a TextTile in the document.
The darker the tile, the more frequent the term(white indicates 0, black indicates 8or more hits; the frequencies ofall the terms withina term set are added together).
The top row of each rectangle corresponds to the hitsfor Term Set 1, the middle row to hits for Term Set 2, and the bottom row to hits forTerm Set 3.
The first Column of each rectangle corresponds to the first TextTile of thedocument, he second column to the second TextTile, and so on.
The patterns of gray-level are meant o provide a compact summary of which passages of the documentmatched which topics of the query.Users' queries are written as lists of words, where each list, or term set, is meantto correspond to a different component of the query.
2This list of words is then trans-lated into conjunctive normal form.
For example, the query in the Figure is translatedby the system as: (patient OR medicine OR medical) AND (test OR scan OR cure ORdiagnosis) AND (software OR program).
This formulation allows the interface to reflecteach conceptual part of the query: the medical terms, the diagnosis terms, and thesoftware terms.
The document whose title begins "VA automation means faster ad-missions" is quite likely to be relevant to the query, and has hits on all three term setsthroughout the document.
By contrast, the document whose title begins "It's hard toghostbust a network .
.
. "
is about computer-aided diagnosis, but has only a passingreference to medical diagnosis, as can be seen by the graphical representation.This version of the TileBars interface allows the user to filter the retrieved oc-uments according to which aspects of the query are most important.
For example, ifthe user decides that medical terms should be better epresented, the Min Hits or Min2 This query format was found to be unproblematic for users in a separate study (Hearst et al 1996), andis also used in the Grateful Med medical information system (Hersh et al 1995).37Computational Linguistics Volume 23, Number 1~t =?== wz= dLi~gumsi,,Iii,i ii ,~ r ~"TextPert (So{b~mre R view) (eva{u~on)"~ I "PC pdn~ers gain 3;;87 power with pro~col converters~ (Har~var~"VA au~matJon means faster admissions, (US Department ol Veterans A~Iai~"Better ADP could cut VA delays 4D%, of Bcia{s ay, (autgma~c 0~a process"Card smarts, (smart car~s)*"It's hard to ghostbust a ne~k ~ current diagnostic tools managers s~"Army tests prototype battlefield information ~m,&O *?
Lack of \]rnagtnat~lon stalls op*dcai-disk appiications.&O""The electrlc cadaver, \[computerized anatomy {assorts and digits!
dissection\[ "interesting r~,  things, (monitoring and testing equipment) (buyers g"MegaDdve 20 is fellah!e, has exceltent sol~ra~e: 'mice adjunct to standard hFigure 1The TileBars Display on a query about automated systems for medical diagnosis (Hearst 1995(~) ACM).Distribution constraint on this term set can be adjusted accordingly.
Min Hits indicatesthe min imum number  of times words from a term set must  appear  in the documentin order for it to be displayed.
Similarly, Min Distribution indicates the min imum per-centage of tiles that must  have a representative from the term set.
The setting MinOverlap Span refers to the min imum number  of tiles that must  have at least one hitfrom each of the three term sets.
In Figure 1, the user has indicated that the diagnosisaspect of the query must  be strongly present in the retrieved documents,  by settingthe Min Distribution to 30% for the second term set.
3When the user mouse-clicks on a square in a TileBar, the corresponding documentis displayed beginning at the selected TextTile.
Thus the user can also view the subtopicstructure within the document itself.3 Most likely this setting information is too complicated for a typical user; I have performed someexperiments todetermine how to set these constraints automatically (Hearst 1996) to be used in futureversions of the interface.38Hearst TextTilingThis section has discussed why multi-paragraph segmentation is important andhow it might be used.
The next section elaborates on what is meant by multi-paragraphsubtopic structure, casting the problem in terms of detection of topic or subtopic shift.3.
Coarse-Grained Subtopic Structure3.1 What is Subtopic Structure?In order to describe the detection of subtopic structure, it is important to define thephenomenon of interest.
The use of the term subtopic here is meant o signify piecesof text "about" something and is not to be confused with the topic/comment distinc-tion (Grimes 1975), also known as the given/new contrast (Kuno 1972), found withinindividual sentences.The difficulty of defining the notion of topic is discussed at length in Brown andYule (1983, Section 3).
They note:The notion of 'topic' is clearly an intuitively satisfactory way of de-scribing the unifying principle which makes one stretch of discourse'about' something and the next stretch 'about' something else, for itis appealed to very frequently in the discourse analysis literature .
.
.
.Yet the basis for the identification of 'topic' is rarely made explicit.(pp.
69-70)After many pages of attempting to pin the concept down, they suggest, as onealternative, investigating topic-shift markers instead:It has been suggested.., that instead of undertaking the difficult askof attempting to define 'what a topic is', we should concentrate ondescribing what we recognize as topic shift.
That is, between two con-tiguous pieces of discourse which are intuitively considered to havetwo different 'topics', there should be a point at which the shift fromone topic to the next is marked.
If we can characterize this markingof topic-shift, hen we shall have found a structural basis for dividingup stretches of discourse into a series of smaller units, each on a sep-arate topic .
.
.
.
The burden of analysis is consequently transferred toidentifying the formal markers of topic-shift in discourse.
(pp.
94-95)This notion of looking for a shift in content bears a close resemblance to Chafe's notionof The Flow Model of discourse in narrative texts (Chafe 1979), in description of whichhe writes:Our data ... suggest that as a speaker moves from focus to focus (orfrom thought o thought) there are certain points at which there maybe a more or less radical change in space, time, character configuration,event structure, or, even, world .
.
.
.
At points where all of these changein a maximal way, an episode boundary is strongly present.
But oftenone or another will change considerably while others will change lessradically, and all kinds of varied interactions between these severalfactors are possible.
4 (pp.
179-80)4 Interestingly, Chafe arrived at the Flow Model after working extensively with, and then becomingdissatisfied with, a hierarchical model of paragraph structure like that of Longacre (1979).39Computational Linguistics Volume 23, Number 1Thus, rather than identifying topics (or subtopics) per se, several theoretical dis-course analysts have suggested that changes or shifts in topic can be more readilyidentified and discussed.
TextTiling adopts this stance.
The problem remains, then, ofhow to detect subtopic shift.
Brown and Yule (1983) consider in detail two markers:adverbial clauses and certain kinds of prosodic markers.
By contrast, the next sub-section will show that lexical co-occurrence patterns can be used to identify subtopicshift.3.2 Relationship to Segmentation in Hierarchical Discourse ModelsMuch of the current work in empirical discourse processing makes use of hierarchicaldiscourse models, and several prominent heories of discourse assume a hierarchicalsegmentation model.
Foremost among these are the attentional/intentional structure ofGrosz and Sidner (1986) and the Rhetorical Structure Theory of Mann and Thompson(1987).
The building blocks for these theories are phrasal or clausal units, and thetargets of the analyses are usually very short texts, typically one to three paragraphs inlength.
5Many problems in discourse analysis, such as dialogue generation and turn-taking (Moore and Pollack 1992; Walker and Whittaker 1990), require fine-grained,hierarchical models that are concerned with utterance-level segmentation.
Progress isbeing made in the automatic detection of boundaries at this level of granularity usingmachine learning techniques combined with a variety of well-chosen discourse cues(Litman and Passonneau 1995).In contrast, TextTiling has the goal of identifying major subtopic boundaries, at-tempting only a linear segmentation.
We should expect o see, in grouping togetherparagraph-sized units instead of utterances, a decrease in the complexity of the fea-ture set and algorithm needed.
The work described here makes use only of lexicaldistribution information, in lieu of prosodic cues such as intonational pitch, pause,and duration (Hirschberg and Nakatani 1996), discourse markers such as oh, well,ok, however (Schiffrin 1987; Litman and Passonneau 1995), pronoun reference resolu-tion (Passonneau and Litman 1993; Webber 1988) and tense and aspect (Webber 1987;Hwang and Schubert 1992).
From a computational viewpoint, deducing textual topicstructure from lexical occurrence information alone is appealing, both because it iseasy to compute, and because discourse cues are sometimes misleading with respectto the topic structure (Brown and Yule 1983, Section 3).4.
Detecting Subtopic Change via Lexical Co-occurrence PatternsTextTiling assumes that a set of lexical items is in use during the course of a givensubtopic discussion, and when that subtopic changes, a significant proportion of thevocabulary changes as well.
The algorithm is designed to recognize pisode boundariesby determining where thematic omponents like those listed by Chafe (1979) changein a maximal way.
However, unlike other researchers who have studied setting, time,characters, and the other thematic factors that Chafe mentions, I attempt to determinewhere a relatively large set of active themes changes simultaneously, regardless ofthe type of thematic factor.
This is especially important in expository text in whichthe subject matter tends to structure the discourse more so than characters, setting,and so on.
For example, in the Stargazers text introduced in Section 1, a discussion of5 Discourse work at the multi-paragraph level has been mainly in the theoretical, sopposed tocomputational, realm, notably the work on macrostructures (van Dijk 1980, 1981) and story grammars(Lakoff 1972; Rumelhart 1975).40Hearst TextTilingcontinental movement, shoreline acreage, and habitability gives way to a discussionof binary and unary star systems.
This is not so much a change in setting or characteras a change in subject matter.The flow of subtopic structure as determined by lexical co-occurrence is illustratedgraphically in Figure 2.
This figure shows the distribution, by sentence number, of se-lected terms from the Stargazers text.
The number of times a given word occurs in agiven sentence is shown, with blank spaces indicating zero occurrences.
Words thatoccur frequently throughout the text (e.g., life, moon) are often indicative of the maintopic(s) of the text.
Words that are less frequent but more uniform in distribution, suchas form and scientist, tend to be neutral and do not provide much information aboutthe divisions within the discussions.
The remaining words are what are of interesthere.
They are "clumped" together, and it is these clumps or groups that TextTilingassumes are indicative of the subtopic structure.
The problem of segmentation there-fore becomes the problem of detecting where these clumps begin and end.For example, words binary through planet have considerable overlap in sentences58 to 78, and correspond to the subtopic discussion Binary/trinary star systems makelife unlikely shown in the (manually produced) outline in Section 1.
There is also awell-demarcated cluster of terms between sentences 35 and 50, corresponding to thegrouping together of paragraphs 10, 11, and 12 by human judges who have read thetext, and to the subtopic discussion in Section 1 of How the moon helped life evolve onearth.These observations suggest hat a very simple take on lexical cohesion relations(Halliday and Hasan 1976) can be used to determine subtopic boundaries.
However,from the diagram it is evident hat simply looking for chains of repeated terms (assuggested by Morris and Hirst \[1991\]) is not sufficient for determining subtopic breaks.Even combining terms that are closely related semantically into single chains is insuf-ficient, since often several different hemes are active within the same segment.
Forexample, sentences 37 to 51 contain dense interactions among the terms move, conti-nent, shoreline, time, species, and life, and all but the latter occur only in this region.
(Itis, however, the case that the interlinked terms of sentences 57 to 71, space, star, binary,trinary, astronomer, orbit, are closely related semantically, assuming the appropriatesenses of the words.
)Because groups of words that are not necessarily closely related conceptually seemto work together to indicate subtopic structure, I adopt a technique that can take intoaccount he occurrences of multiple simultaneous themes rather than use chains oflexical cohesion relations alone.
This viewpoint is also advocated by Skorochod'ko(1972), who suggests discovering a text's structure by dividing it up into sentencesand seeing how much word-overlap appears among the sentences.
The overlap formsa kind of intrastructure; fully connected graphs might indicate dense discussions of atopic, while long spindly chains of connectivity might indicate a sequential account.The central idea is that of defining the structure of a text as a function of the con-nectivity patterns of the terms that comprise it, in contrast with segmentation guidedprimarily by fine-grained iscourse cues such as register change and cue words.Many researchers, (e.g., Halliday and Hasan \[1976\], Tarmen \[1989\], and Walker\[1992\]), have noted that term repetition is a strong cohesion indicator.
Phillips (1985)suggests performing "an analysis of the distribution of the selected text elements rela-tive to each other in some suitable text interval ... for whatever patterns of associationthey may contract with each other as a function of repeated co-occurrence" (p. 59).Perhaps urprisingly, however, the results in Section 6 show that term repetition alone,independent of other discourse cues, can be a very useful indicator of subtopic struc-ture.
This may be less true in the case of narrative texts, which tend to use more41Computational Linguistics Volume 23, Number 1Ob0O~LOO00~0LO0 l"-I t )0L.O0 ",~LOLO00m00Figure 2,?-I,r.t,r-t,?--I(',1T-t,r-t.wt,?-tP.P.,r--tC4CO.i-I?'1,?-t,?-tv-I?
,--I ~ 0 ?g r--I -~ ?
I~ ?-,-I ~'~ ~ I~l 0O ~ 0,:::1~ O0 u~ LO LO ~1~ O0 b.- ?,0 ?.0 1~ ,~ Ob 1,~.
O0 b.. O0 ?
.000  ?,0 ~tO0~OO~tO~0O00u3t ' -oO?.OLOOI.O,q4COO03O,IOv-IOOODistribution of selected terms from the Stargazer text, with a single digit frequency persentence number (blanks indicate a frequency of zero).42Hearst TextTilingvariation in the way concepts are expressed, and so may require that thesaural rela-tions be used as well, as in (Kozima 1993).It should be noted that other researchers have experimented with the display ofpatterns of cohesion cues other than lexical cohesion as tools for analyzing discoursestructure.
Grimes (1975, Chapter 6) introduces pan charts to show the interactionof various thematic devices uch as character identification, setting, and tense.
Stod-dard (1991) creates cohesion maps by assigning to each word a location on a two-dimensional grid corresponding to the word's position in the text.To summarize, many discourse analysis tasks require a fine-grained, hierarchicalmodel, and consequently require many kinds of discourse cues for segmentation inpractice.
TextTiling attempts a coarser-grained analysis and so gets away with usinga simpler feature set.
Additionally, if we think of subtopic segmentation in terms ofdetection of shift from one discussion to the next, we can simplify the task to oneof detecting where the use of one set of terms ends and another set begins.
Figure 2illustrates that lexical distribution information can be used to discover such subtopicshifts.The next subsections describe three different strategies for detecting subtopic shift.The first is based on the observations of this subsection, that subtopics can be viewedas "clumps" of vocabulary, and the problem of segmentation is one of detecting theseclumps.
The following two subsections describe alternative techniques, derived byrecasting other researchers' algorithms into a more appropriate framework for theTextTiling task.4.1 Comparing Adjacent Blocks of TextIn the block comparison algorithm, adjacent pairs of text blocks are compared foroverall lexical similarity.
The TextTiling algorithm requires that a score, called thelexical score, be computed for every sentence, or more precisely, for the gap betweenevery pair of sentences ( ince this is where paragraph breaks take place).The sketch in Figure 3(a) illustrates the scores computed for the block comparisonalgorithm.
In this figure is shown a sequence of eight hypothetical sentences, theircontents represented as columns of letters, where each letter represents a term orword.
The sentences are grouped into blocks of size k, where in this illustration k = 2.The more words the blocks have in common, the higher the lexical score at the gapbetween them.
If a low lexical score is preceded by and followed by high lexical scores,this is assumed to indicate a shift in vocabulary corresponding to a subtopic hange.The blocks act as moving windows over the text.
Several sentences can be con-tained within a block, but the blocks shift by only one sentence at a time.
Thus if thereare k sentences within a block, each sentence occurs in k, 2 score computations (exceptfor sentences at the extreme nds of the text).The current version of the block algorithm computes scores in a very simple man-ner, as the inner product of two vectors, where a vector contains the number of timeseach lexical item occurs in its corresponding block.
The inner product is normalizedto make the score fall between 0 and 1, inclusive.Figure 3(a) shows the computation of the scores at the gaps between sentences 2and 3, between 4 and 5, and between 6 and 7.
The scores shown are simple, unnor-malized inner products of the frequencies of the terms in the blocks.
For example thegap between sentences 2 and 3 gets assigned a score of 8 computed as 2 ?
1 (for A)+1 ?
1 (for B) +2 * 1 (for C) +1 * 1 (for D) +1 * 2 (for E).
Results for this approach arereported in Section 6.After these scores are computed, the blocks are shifted by one sentence (sentences1 and 8 need to be handled as boundary conditions).
So, for example, in addition43Computational Linguistics Volume 23, Number 11 2 3 4 5 6 7 8A Ac o  c BE EI, , ,o/ , , , .
/ - , , .
/g 3 9 (a)1 2 3 4 5 6 7 8c B B B) H 05 + 0 + 3 + 1 \ /N / ' , , ,  /(b) 5 3 41 2 3 4 5 6 7 8A -A IBC-  C.4--  CDEA I II B I B - I -  BI ID I IE -  E.--I.- E II F~F- I -  FI G I G -GI H~H- I -  H -HI I II Is 1 4 (c)F igure  3Illustration of three ways to compute the lexical score at gaps between sentences.
Numbersindicate a numbered sequence of sentences, columns of letters ignify the terms in the givensentence.
(a) Blocks - dot product of vectors of word counts in the block on the left and theblock on the right.
(b) Vocabulary introduction - the number of words that occur for the firsttime within the interval centered at the sentence gap.
(c) Chains - the number of active chains,or terms that repeat within threshold sentences and span the sentence gap.to comparing sentences 3 and 4 against sentences 5 and 6, the algorithm comparessentences 4 and 5 against sentences 6 and 7.An earlier version of the algorithm (Hearst 1993; Hearst and Plaunt 1993) weightedterms according to tf.idf weights from Information Retrieval (Salton 1989).
This weight-ing function computes, for each word, the number  of times it occurs in the documenttf, times the inverse of the number  of documents that the term occurs in, in a large col-lection idf, or as in this case, with some normalizing constants.
The idea is that termsthat commonly occur throughout a collection are not necessarily good indicators ofrelevance to a query because they are so common, and so their importance is down-weighted.
Hearst (1993) posited that this argument should also apply to determiningwhich words best distinguish one subtopic from another.
However,  the estimates ofimportance that tf.idf makes seem not to be accurate nough within the scope of com-paring adjacent pieces of text to justify using this measure, and the results seem morerobust weighting the words according to their frequency within the block alone.44Hearst TextTiling4.2 Vocabulary IntroductionsAnother ecent analytic technique that makes use of lexical information is described inYoumans (1991), which introduces a variant on type/token curves, called the Vocabu-lary-Management Profile.
Type/token curves are simply plots of the number of uniquewords against he number of words in a text, starting with the first word and pro-ceeding through the last.
Youmans modifies this algorithm to keep track of how manyfirst-time uses of words occur at the midpoint of every 35-word window in a text.Youmans' goal is to study the distribution of vocabulary in discourse rather than tosegment i along topical ines, but upon examining many English narratives, essays,and transcripts he notices that sharp upturns after deep valleys in the curve "correlateclosely to constituent boundaries and information flow" (p. 788).Youmans' analysis of the graphs is descriptive in nature, mainly attempting toidentify the cause of each peak or valley in terms of a principle of narrative structure,and is done at a very fined-grained level.
He discusses one text in detail, describ-ing changes at the single-word level, and focusing on within-paragraph and within-sentence events.
Examples of events are changes in characters, occurrences ofdialogue,and descriptions ofplaces, each of which ranges in length from one clause to a few sen-tences.
He also finds that paragraph boundaries are not always predicted--sometimesthe onset of a new paragraph is signaled by the occurrence of a valley in the graph,but often paragraph onset is not signaled until one or two sentences beyond onset.
6One of Youmans' main foci is an attempt to cast the resulting peaks in terms ofco-ordination and subordination relations.
However, in the discussion he notes thatthis does not seem like an appropriate use of the graphs.
No systematic evaluation ofthe algorithm is presented, nor is there any discussion of how one might automaticallydetermine the significance of the peaks and valleys.Nomoto and Nitta (1994) attempt to use Youmans' algorithm for distinguishingentire articles from one another when they are concatenated into a single file.
They findthat it "fails to detect any significant pattern in the corpus" (p. 1148).
I recast Youmans'algorithm into the TextTiling framework, renaming it the vocabulary introductionmethod.
Figure 3(b) illustrates.
The text is analyzed, and the positions at which termsare first introduced are recorded (shown in black circles in the figure).
A movingwindow is used again, as in the blocks algorithm, and this window corresponds toYoumans' interval.
The number of new terms that occur on either side of the midpoint,or the sentence gap of interest, are added together and plotted against sentence gapnumber.This approach differs from that of Youmans (1991) and Nomoto and Nitta (1994) intwo main ways.
First, Nomoto and Nitta (1994) use too large an interval--300 words--because this is approximately the average size needed for their implementation f theblocks version of TextTiling.
Large paragraph-sized intervals for measuring introduc-tion of new words seem unlikely to be useful since every paragraph of a given lengthshould have approximately the same number of new words, although those at the be-ginning of a subtopic segment will probably have slightly more.
Instead, I use intervallengths of size 40, closer to Youmans' suggestion of 35.Second, the granularity at which Youmans takes measurements is too fine, sincehe plots the score at every word.
Sampling this frequently ields a very spiky plotfrom which it is quite difficult o draw conclusions at a paragraph-sized granularity.
I6 This might be explained inpart by Stark (1988) who shows that readers disagree measurably aboutwhere to place paragraph boundaries when presented with texts with those boundaries removed.45Computational Linguistics Volume 23, Number 1plot the score at every sentence gap, thus eliminating the wide variation that is seenwhen measuring after each word.
Results for this approach are reported in Section 6.4.3 Lexical ChainsMorris and Hirst's pioneering work on computing discourse structure from lexicalrelations (Morris and Hirst 1991; Morris 1988) is a precursor to the work reported onhere.
Influenced by Halliday and Hasan's (1976) theory of lexical coherence, Morrisdeveloped an algorithm that finds chains of related terms via a comprehensive the-saurus (Roget's Fourth Edition).
7For example, the words residential nd apartment bothindex the same thesaural category and can thus be considered to be in a coherencerelation with one another.
The chains are used to structure texts according to the at-tentional/intentional theory of discourse structure (Grosz and Sidner 1986) discussedabove.
The extent of the lexical chains is assumed to correspond to the extent of asegment.
The algorithm also incorporates the notion of chain returns--repetition fterms after a long hiatus--to complete an intention that spans over a digression.
Theboundaries of the segments correspond to the sentences that contain the first and lastwords of the chain.Since the Morris and Hirst (1991) algorithm attempts to discover attentional/inten-tional structure, its goals are different han those of TextTiling.
Specifically, the dis-course structure it attempts to discover is hierarchical and more fine-grained thanthat discussed here.
Morris (1988) provides five short example texts for which she hasdetermined the intentional structure, and states that the lexical chains generated byher algorithm provide a good indication of the segment boundaries that Grosz andSidner's theory assumes.
In Morris (1988) and Morris and Hirst (1991), tables are pre-sented showing the sentences spanned by the lexical chains and by the correspondingsegments of the attentional/intentional structure (derived by hand), but no formalevaluation is performed.This algorithm is not directly applicable for TextTiling for several reasons.
First,many words are ambiguous and fall into more than one thesaurus class.
This is notstated as a concern in Morris's work, perhaps because the texts were short, and pre-sumably, if a word were ambiguous, the correct hesaurus class would neverthelessbe chosen because the chained-to words would share only the correct hesaurus class.However, my experimentation with an implemented version of Morris' algorithm thatmade use of Roget's 1911 thesaurus (which is admittedly less structured than thethesaurus used by Morris), when run on longer texts, found ambiguous links to be acommon occv',:ence and detrimental tothe algorithm.
A thesaurus-based disambigua-tion algorithm (Yarowsky 1992) may help alleviate this problem (this option is revisitedin Section 7), but another solution is to move away from thesaurus classes and usesimple word co-occurrence instead, since within a given text a word is usually usedwith only one sense (Gale, Church, and Yarowsky 1992b).
The potential downside ofthis approach is that many useful inks may be missed.Another limitation of the Morris algorithm is that it does not take advantage of,or discuss how to account for, the tendency for multiple simultaneous chains to occurover the same intention (each chain corresponds toone intention).
Related to this is thefact that chains tend to overlap one another in long texts, as can be seen in Figure 2.These two types of difficulties can be circumvented by recasting the Morris al-gorithm to take advantage of the observations atthe beginning of this section.
Three7 The algorithm was executed by hand since the thesaurus is not generally available online.
Currentextensions to this work make use of WordNet (Miller et al 1990).46Hearst TextTilingchanges are made to the algorithm: First, no thesaurus classes are used (only termrepetition of morphological variants of the same word); second, multiple chains areallowed to span an intention; and third, chains at all levels of intentions are analyzedsimultaneously.
Instead of deciding which chain is the applicable one for a given in-tention, it measures how many chains at all levels are active at each sentence gap.This approach is illustrated in Figure 3(c).
A lexical chain for term t is consideredactive across a sentence gap if instances of t occur within some distance threshold ofone another.
In the figure, all three instances of the word A occur within the distancethreshold.
The third B, however, follows too far after the second B to continue thechain.
The score for the gap between 2 and 3 is simply the number of active chainsthat span this gap.
Boundaries are determined as specified in Section 5.
This variationof the TextTiling algorithm is explored and evaluated in Hearst (1994b).4.4 Vector Space Similarity ComparisonsAs mentioned in Section 2, Salton and Allan (1993) report work in the automatic de-tection of hypertext links and theme generation from large documents, focusing pri-marily on encyclopedia text.
They describe the application of similarity comparisonsbetween articles, sections, and paragraphs within an encyclopedia, both for creatinglinks among related passages, and for better facilitating retrieval of articles in responseto user queries.
Their approach finds similarities among the paragraphs of large doc-uments using normalized tfidf term weighting, scoring text segments according toa normalized inner product of vectors of these weights (this algorithm is called thevector space model \[Salton 1989\]).Salton and Allan (1993) do not try to determine the extents of passages withinarticles or sections.
Instead, all paragraphs, sections, and articles are assigned pair-wise similarity scores, and links are drawn between those with the highest scores,independent of their position within the text.
This distinction is important becausethe difficulty in subtopic segmentation lies in detecting the subtle differences betweenadjacent ext blocks.
A method that finds blocks with the topmost similarity to oneanother can succeed at finding the equivalent of the center of a subtopic extent, butdoes not distinguish where one subtopic ends and the next begins.If the algorithm of Salton and Allan (1993) were transformed so that adjacent textunits were compared, and a method for determining where the similarity scores arelow were used, then it would resemble the blocks algorithm with tfidf weighting,but without the use of overlapping text windows.
However, a consequence of thefact that the vector space method is better at distinguishing similarities than differ-ences, is that similarity scores alone are probably less effective at finding the transitionpoints between subtopic discussions than sequences of similarity scores, using movingwindows of text, in the manner described above.Salton et al (1996) attempt to address a version of the subtopic segmentation prob-lem by extending the algorithm to finding "text pieces exhibiting internal consistencythat can be distinguished from the remainder of the surrounding text" (p. 55).
As onepart of this goal, they seek what is called the text segment, which is defined as "acontiguous piece of text that is linked internally, but largely disconnected from theadjacent text.
Typically, a segment might consist of introductory material, or cover theexposition and development of the text, or contain conclusions and results" (p. 55).Thus, they do not address the subtopic detection task because they attempt only tofind those segments of text that are strongly different han the surrounding text.
Theydo this by comparing similarity between a paragraph and its four closest paragraphneighbors to the left and the right.
If a similarity score between a pair of paragraphsdoes not exceed a threshold, then the link between that pair is removed.
If a discon-47Computat iona l  Linguistics Volume 23, Number  1nected sequence of paragraphs i found, that sequence is considered a text segment.This algorithm is not evaluated.4.5 Other Related ApproachesKozima (1993) describes an algorithm for the detection of text segments, which aredefined as "a sequence of clauses or sentences that display local coherence" (p. 286) innarrative text.
Kozima (1993) presents a very elaborate algorithm for computing thelexical cohesiveness of a window of words, using spreading activation in a seman-tic network created from an English dictionary.
The cohesion score is plotted againstwords and smoothed, and boundaries are considered to fall at the lowest-scoringwords.
This complex computation, asopposed to simple term repetition, may be nec-essary when working with narrative texts, but no comparison of methods is done.
Thealgorithm's results are shown on one text, but are not evaluated formally.Reynar (1994) describes an algorithm similar to that of Hearst (1993) and Hearstand Plaunt (1993) with a difference in the way in which the size of the blocks ofadjacent regions are chosen.
A greedy algorithm is used: the algorithm begins with noboundaries, then a boundary b (between two sentences) is chosen which maximizesthe lexical score resulting from comparing the block on the left whose extent rangesfrom b to the closest existing boundary on the left, and similarly for the right.
Thisprocess is repeated until a prespecified number of boundaries have been chosen.
Thisseems problematic, since the initial comparisons are between very large text segments:the first boundary is chosen by comparing the entire text to the right and left of theinitial position.
The algorithm is evaluated only in terms of how well it distinguishesentire articles from one another when concatenated into one file.
The precision/recalltradeoffs varied widely: on 660 Wall Street Journal articles, if the algorithm is allowedto be off by up to three sentences, it achieves precision of .80 with recall of .30, andprecision of .30 with recall of .92.5.
The TextTiling AlgorithmThe TextTiling algorithm for discovering subtopic structure using term repetition hasthree main parts:1.2.3.TokenizationLexical Score DeterminationBoundary IdentificationEach is discussed in turn below.
The methods for lexical score determination wereoutlined in Section 4, but more detail is presented here.5.1 TokenizationTokenization refers to the division of the input text into individual lexical units, andis sensitive to the format of the input text.
For example, if the document has markupinformation, the header and other auxiliary information is skipped until the body ofthe text is located.
Tokens that appear in' the body of the text are converted to alllower-case characters and checked against a stop list of closed-classed and other high-frequency words, s If the token is a stop word then it is not passed on to the next8 "Stop list" is a term commonly used in Information Retrieval (Salton 1989).
In this case, the list consistsof 898 words, developed in a somewhat ad hoc manner.48Hearst TextTilingstep.
Otherwise, the token is reduced to its root by a morphological nalysis functionbased on that of Kartunen, Koskenniemi, and Kaplan (1987), converting regularly andirregularly inflected nouns and verbs to their roots.The text is subdivided into pseudosentences of apredefined size w (a parameter ofthe algorithm) rather than using "real" syntactically-determined sentences.
This is doneto allow for comparison between equal-sized units, since the number of shared termsbetween two long sentences and between a long and a short sentence would probablyyield incomparable scores (and sentences are too short to expect normalization toreally accommodate for the differences).
For the purposes of the rest of the discussionthese groupings of tokens will be referred to as token-sequences.
The morphologicallyanalyzed token is stored in a table along with a record of the token-sequence numberit occurred in, and the number of times it appeared in the token-sequence.
A record isalso kept of the locations of the paragraph breaks within the text.
Stop words contributeto the computation of the size of the token-sequence, but not to the computation ofthe similarity between blocks of text.5.2 Determining ScoresAs mentioned above, two methods for determining the score to be assigned at eachtoken-sequence gap are explored here.
The first, block comparison, compares adjacentblocks of text to see how similar they are according to how many words the adjacentblocks have in common.
The second, the vocabulary introduction method, assignsa score to a token-sequence gap based on how many new words were seen in theinterval in which it is the midpoint.5.2.1 Blocks.
In the block comparison algorithm, adjacent pairs of blocks of token-sequences are compared for overall exical similarity.
The block size, labeled k, is thenumber of token-sequences that are grouped together into a block to be comparedagainst an adjacent group of token-sequences.
This value is meant o approximate heaverage paragraph length.
Actual paragraphs are not used because their lengths canbe highly irregular, leading to unbalanced comparisons, but perhaps with a clevernormalizing scheme, "real" paragraphs could be used (analogous to the substitutionof token-sequences for real sentences).Similarity values are computed for every token-sequence gap number; that is, ascore is assigned to token-sequence gap i corresponding to how similar the token-sequences from token-sequence i - k to i are to the token-sequences from i + 1 toi + k + 1.
Note that this moving window approach means that each token-sequenceappears in k ?
2 similarity computations.The lexical score for the similarity between blocks is calculated by a nor-malized inner product: given two text blocks bl and b2, each with k token-se-quences, where bl = {token-sequencei_k,.
.
.
,  token-sequencei} and b2 ~- {token-sequencei+l,.
.
.
.
token-sequencei+k  l },score(i) = Y~t Wt,bl Wt,b2V/G  w2 G w2 t,bl t,b2where t ranges over all the terms that have been registered uring the tokenizationstep (thus excluding stop words), and Wt,b is the weight assigned to term t in block b.As mentioned in Section 4, in this version of the algorithm, the weights on the termsare simply their frequency within the block, This formula yields a score between 0and 1, inclusive.These scores can be plotted, token-sequence number against similarity score.
How-ever, since similarity is measured between blocks bl and b2, the score's x-axis coordi-49Computational Linguistics Volume 23, Number 1nate falls between token-sequences i and i + 1.
Rather than plotting a token-sequencenumber on the x-axis, the token-sequence gap number i is plotted instead.5.2.2 Vocabulary Introduction.
The lexical score assigned in the vocabulary introduc-tion version of scoring is the ratio of new words in an interval divided by the lengthof that interval.
Tokenization is as described above, eliminating stop words and per-forming morphological nalysis.
A score is then assigned to a token-sequence gap asfollows: the number of never-yet-seen words in the token-sequence to the left of thegap is added to the number of never-yet-seen words in the token-sequence to the right,and this number is divided by the total number of tokens in the two token-sequences,or w ?
2.
Since in these experiments w is set to 20, this yields an interval length of 40,which is close to the parameter 35 suggested as most useful in (Youmans 1991).
As inthe block version of the algorithm, the score is plotted at the token-sequence gap, andscores can range from 0 to 1, inclusive.The lexical score is computed as follows.
For each token-sequence gap i, create atext interval b of length w ?
2 (where w is the length of the token-sequences) centeredaround i, and let b be subdivided into two equal-length parts, bl and b2, where bl ={ tokensi_w .
.
.
.
, tokensi } and b2 : { tokensi+ l, .
.
.
, tokensi+w+ l }.
Then,score(i) = NumNewTerms(b l  ) + NumNewTerms(b2)w,2where NumNewTerms(b)  returns the number of terms in interval b seen for the firsttime in the text.5.3 Boundary IdentificationBoundary identification is done identically for all lexical scoring methods, and assignsa depth score, the depth of the valley (if one occurs), to each token-sequence gap.
Thedepth score corresponds tohow strongly the cues for a subtopic hanged on both sidesof a given token-sequence gap and is based on the distance from the peaks on bothsides of the valley to that valley.
Figure 4 illustrates.
In Figure 4(a), the depth score atgap a2 is (Yax -Ya2) q- (Ya3 --Ya2)" Relatively "deeper" valleys receive higher scores thanshallower ones.
More formally, for a given token-sequence gap i, the program recordsthe lexical score of the token-sequence gap I to the left of i until the score for I - 1 issmaller than the score for l (meaning the top of the peak was found at 1).
Similarly, fortoken sequences to the right of i, the program monitors the score of token-sequencer until the score for r + 1 is less than that of r. Finally, score(r) - score(i) is added toscore(l) - score(i), and the result is the depth score at i.A potential problem with this scoring method is illustrated in Figure 4(b).
Herewe see a small valley at gap b4 that can be said to "interrupt" the score for b2.
As onesafeguard, the algorithm uses smoothing (described below) to help eliminate smallperturbations of the kind seen at b4.
Additionally, because the distance between Yb3and Yb4 is small in these kinds of cases, this gap is less likely to be marked as aboundary than gaps like b2, which have large peak distances both to the left and theright.
This example illustrates the need to take into account he length of both sidesof the valley, since a valley that has high peaks on both sides indicates that not onlyhas the vocabulary on the left decreased in score, but the vocabulary on the right hasincreasing score, thus signaling a strong subtopic hange.Figure 4(c) shows another potentially problematic case, in which two strong peaksflank a long, flat valley.
The question becomes which of gaps c2, c3, or both, shouldbe assigned a boundary.
Such "plateaus" occur when vocabulary changes very grad-ually and reflect a poor fit of the corresponding portion of the document to the model50Hearst TextTilingYb5Ya3 Ybl YN~Ya2 Yb2 Yc2 Yc3I I t Ia2 b2 c2 c3(a) (b) (c)Figure 4A sketch illustrating the computation of depth scores in three different situations.
The x-axisindicates token sequence gap number and the y-axis indicates lexical score.assumed by TextTiling.
When the plateau occurs over a longer stretch, usually it is rea-sonable to choose both bordering aps as boundaries.
However, when such a plateauoccurs over a very short stretch of text, the algorithm is forced to make a somewhatarbitrary choice.
Choices like these are cases in which the algorithm should proba-bly make use of additional information, such as more localized lexical distributioninformation, or perhaps more conventional discourse cues.Note that the depth scores are based only on relative score information, ignoringabsolute values.
The justification for this is twofold.
First, it helps make decisions in thecases in which a gap's lexical score falls into the middle of the lexical score range, butis flanked by tall peaks on either side, and this situation happens commonly enough tobe important.
Second, using relative rather than absolute scores helps avoid problemsassociated with situations like that of Figure 4(c), in which all gaps between c2 and c3would be considered boundaries if only absolute scores were taken into account.The depth scores are sorted and used to determine segment boundaries.
The largerthe score, the more likely the boundary occurs at that location, modulo adjustmentsas necessary to place the boundaries at orthographically marked paragraphs (if avail-able).
A proviso check is made to prevent assignment of very close adjacent segmentboundaries.
Currently, at least hree intervening token-sequences arerequired betweenboundaries.
This helps control for the fact that many texts have spurious header in-formation and single-sentence paragraphs.An alternative to this method of computing depth scores is to use the slope ofthe valley's sides, or the "sharpness" of the vocabulary change.
However, becausedeeper valleys with smaller slopes indicate larger, although more gradual, shifts invocabulary usage than shallower valleys with larger slopes, they are preferable fordetecting subtopic boundaries.
Furthermore, steep slopes can sometimes indicate aspurious change associated with a very short digression.
The depth score is morerobust for the purposes of subtopic boundary detection.51Computational Linguistics Volume 23, Number 15.4 Smoothing the PlotAs mentioned above, the plot is smoothed to remove small dips, using average smooth-ing with a width of size s, as follows:for each token-sequence gap g and a small even number sfind the scores of the s/2 gaps to the left of gfind the scores of the s/2 gaps to the right of gfind the score at gtake the average of these scores and assign it to grepeat his procedure n timesThe choice of smoothing function is somewhat arbitrary; other low-pass filters couldbe used instead.5.5 Determining the Number of BoundariesThe algorithm must determine how many segments to assign to a document, sinceevery paragraph is a potential segment boundary.
Any attempt o make an absolutecutoff, even one normalized for the length of the document, is problematic since thereshould be some relationship between the structure and style of the text and the numberof segments assigned to it.
As discussed above, a cutoff based on a particular valleydepth is similarly problematic.Instead, I suggest making the cutoff a function of the characteristics of the depthscores for a given document, using the average ~ and standard deviation ~ of theirscores (thus assuming that the scores are normally distributed).
One version of thisfunction entails drawing a boundary only if the depth score exceeds ~ - cr (the liberalmeasure, LC).
This function can be varied to achieve correspondingly varying preci-sion/recall trade-offs.
A higher precision but lower recall can be found by setting thelimit to be depth scores exceeding ~- or/2 (the conservative measure, HC) instead of3-  o-.6.
EvaluationThere are several ways to evaluate a segmentation algorithm, including comparingits segmentation against that of human judges, comparing its segmentation againstauthor-specified orthographic information, and comparing its segmentation againstother automated segmentation strategies in terms of how they effect the outcome ofsome computational task.
This section presents comparisons of the results of the algo-rithm against human judgments and against article boundaries.
It is possible to com-pare against author-specified markups, but unfortunately, as discussed above, authorsusually do not specify the kind of subtopic information desired.
As mentioned above,Hearst (1995) and Hearst and Plaunt (1993) show how to use TextTiles in informationretrieval tasks, although this work does not show whether or not the results of thesealgorithms produce better performance than the results of some other segmentationstrategy would.6.1 Reader JudgmentsThere is a growing concern surrounding issues of intercoder eliability when usinghuman judgments to evaluate discourse-processing al orithms (Carletta 1996; Condonand Cech 1995).
Proposals have recently been made for protocols for the collection ofhuman discourse segmentation data (Nakatani et al 1995) and for how to evaluate thevalidity of judgments o obtained (Carletta 1996; Isard and Carletta 1995; Ros6 1995;Passonneau and Litman 1993; Litman and Passonneau 1995).
Recently, Hirschberg52Hearst TextTilingand Nakatani (1996) have reported promising results for obtaining higher interjudgeagreement using their collection protocols.For the evaluation of the TextTiling algorithms, judgments were obtained fromseven readers for each of 12 magazine articles that satisfied the length criteria (between1,800 and 2,500 words) 9and that contained little structural demarcation.
The judgeswere asked simply to mark the paragraph boundaries at which the topic changed; theywere not given more explicit instructions about the granularity of the segmentation.
1?Figure 5 shows the boundaries marked by seven judges on the Stargazers text.
Thisformat helps illustrate the general trends in the judges' assessments, and also helpsshow where and how often they disagree.
For instance, all but one judge marked aboundary between paragraphs 2 and 3.
The dissenting judge did mark a boundaryafter 3, as did two of the concurring judges.
The next three major boundaries occurafter paragraphs 5, 9, 12, and 13.
There is some contention in the later paragraphs;three readers marked both 16 and 18, two marked 18 alone, and two marked 17 alone.The outline in the Introduction gives an idea of what each segment is about.Passonneau and Litman (1993) discuss at length considerations about evaluat-ing segmentation algorithms according to reader judgment information.
As Figure 5shows, agreement among judges is imperfect, but trends can be discerned.
In thedata of Passonneau and Litman (1993), if four or more out of seven judges mark aboundary, the segmentation is found to be significant using a variation of the Q-test(Cochran 1950).
However, in later work (Litman and Passonneau 1995), three out ofseven judges marking a boundary was considered sufficient o classify, that point as a"major" boundary.Carletta (1996) and Ros6 (1995) point out the importance of taking into accountthe expected chance agreement among judges when computing whether or not judgesagree significantly.
They suggest using the kappa coefficient (K) for this purpose.
Ac-cording to Carletta (1996), K measures pairwise agreement among a set of codersmaking category judgments, correcting for expected chance agreement as follows:K-  P(A) -P(E)1 -P(E)where P(A) is the proportion of times that the coders agree and P(E) is the proportionof times that they would be expected to agree by chance.
The coefficient can be com-puted by making pairwise comparisons against an expert or by comparing to a groupdecision.
Carletta (1996) also states that in the behavioral sciences, K > .8 signals goodreplicability, and .67 < K < .8 allows tentative conclusions to be drawn.
The kappacoefficients found in Isard and Carletta (1995) ranged from .43 to .68 for four codersplacing transaction boundaries, and those found in (Ros~ 1995) ranged from .65 to.90 for four coders egmenting sentences.
Carletta cautions, however, that "... codingdiscourse and dialogue phenomena, nd especially coding segment boundaries, may9 One longer text of 2,932 words was used since reader judgments  had been obtained for it from anearlier experiment.
Judges were technical researchers.
Two texts had three or four short headers, whichwere removed for consistency.
One text that was used in Hearst (1994b) is not used here becauseinconsistencies were found in the paragraph break locations.10 Specifically, the instructions were in written form and ran as follows: "You will receive three texts.Mark where the topics seem to change- -draw a line between the paragraphs, where any blank line canbe considered a paragraph boundary.
It's recommended that you read quickly; no need to understandall the nuances.
However, you are allowed to go back and look over parts that you've already lookedat and change your markings if desired.
If on occasion you can't decide between two places, definitelypick one but indicate that you thought  he other one was just as appropriate."
On the rare occasions inwhich the subject picked a secondary boundary, only the pr imary one was retained for evaluation.53Computational Linguistics Volume 23, Number 1be inherently more difficult han many previous types of content analysis (for instance,dividing newspaper articles based on subject matter)" and so implies that the levels ofagreement eeded to indicate good reliability for TextTiling may be justified in beinglower.For my test texts, the judges placed boundaries on average 39.1% of the time,and nonboundaries 60.9%.
Thus the expected chance agreement P(E) is .524 (sinceP(Boundary) = .391 and P(Nonboundary) ~- .609, (.3912 + .6092) = .524).
To compute K,each judge's decision was compared to the group decision, where a paragraph gap wasconsidered a "true" boundary if at least three out of seven judges placed a boundarymark there, as in Litman and Passonneau (1995).
11 The remaining aps are considerednonboundaries.
The average K for these texts was .647.
This score is at the low endof the stated acceptability range but is comparable with those of other interreliabilityresults (with fewer judges) found in discourse segmentation experiments.6.2 Parameter SettingsAn unfortunate aspect of the algorithm in its current form is that it requires the set-ting of several interdependent parameters, the most important of which are the size ofthe text unit that is compared, and the number of words in a token-sequence (whichcontrols the number of times a term appears in a window as well as the number ofdata points that are sampled).
The method, width, and number of rounds of smoothingmust also be chosen.
Usually only modest amounts of smoothing can be allowed, sincemore dramatic smoothing tends to obscure the point at which the subtopic transitiontakes place.
Finally, the method for determining how many boundaries to assign mustbe specified.
The three are interrelated: for example, using a larger text window re-quires less smoothing and fewer boundaries will be found, yielding a coarser-grainedsegmentation.Initial testing was done on the texts evaluated with several different sets of pa-rameter settings and a default configuration that seems to cover many different exttypes was chosen.
The defaults et w = 20, k = 10, n = 1, s = 2, for token-sequencesize, block size, number of rounds of smoothing, and smoothing width, respectively.The evaluation presented here shows the results for different setting types to give afeeling for the space of results.
Because the evaluation collection is very small, theseresults can be seen only as a suggestion; different settings may work better in differentsituations.6.3 Results: Qualitative AnalysisFigure 6 shows a plot of the results of applying the block comparison algorithm to theStargazer text with k set to 10.
When the lowermost portion of a valley is not located ata paragraph gap, the judgment is moved to the nearest paragraph gap.
12 For the mostpart, the regions of strong similarity correspond to the regions of strong agreementamong the readers.
(The results for this text are among the stronger ones and appearin the last line of Table 2.)
Note however, that the similarity information around para-graph 12 is weak.
This paragraph briefly summarizes the contents of the previousthree paragraphs; much of the terminology that occurred in all of them reappears in11 Paragraphs of three or fewer sentences were combined with their neighbor if that neighbor wasdeemed to follow at a "major" boundary, as in paragraphs 2 and 3 of the Stargazers text.12 More specifically, if the closest paragraph location (first left, then right) has not been marked as aboundary, then mark it.
Otherwise, look to the paragraph to the left.
If that paragraph as not beenmarked and if it is at least gap_limit = 3 token-sequences away, then mark the paragraph to the left.
Ifthis fails, try the paragraph to the right in a similar way.
If both fail, mark nothing.54Hearst TextTilingF igure 5I==23  13 14 15 16 17 18 19 20 4 5 6 7 8 9 10 11 12I f 1; 2; 3; ,; 3; 6; 7; ~o ~ .
.
.
.Judgments of seven readers on the Stargazer text.
Internal numbers indicate location of gapsbetween paragraphs; x-axis indicates token-sequence gap number, y-axis indicates judgenumber, a break in a horizontal line indicates a judge-specified segment break.0.70.6 I, , I4 $ 67  8151 1010 10 20 30 40 50 60 70 80 90 100Figure 6Results of the block similarity algorithm on the Stargazer text with k set to 10 and the looseboundary cutoff limit.
Both the smoothed and unsmoothed plot are shown.
Internal numbersindicate paragraph numbers, x-axis indicates token-sequence gap number, y-axis indicatessimilarity between blocks centered at the corresponding token-sequence gap.
Vertical linesindicate boundaries chosen by the algorithm; for example, the leftmost vertical line representsa boundary after paragraph 3.Note how these align with the boundary gaps of Figure 5 above.this one location (in the spirit of a Grosz and Sidner \[1986\] "pop" operation).
Thusit displays low similarity both to itself and to its neighbors.
This is an example of abreakdown caused by the assumptions about the subtopic structure.Because of the depth score cutoff, not all valleys are chosen as boundaries.
Al-though there is a dip around paragraph gaps 5 and 6, no boundary is marked there.From the summary of the text's contents in Section 1, we know that paragraphs 4 and5 discuss the moon's chemical composition while 6 to 8 discuss how it got its shape;these two subtopic discussions are more similar to one another in content han theyare to the subtopics on either side of them, thus accounting for the small change insimilarity.55Computational Linguistics Volume 23, Number 1Table 1Average K, precision, and recall scores for 12 test texts.
Baseline shows the scores for analgorithm that assigns aboundary 39% of the time (the average overall), Tiling (V) indicatesthe vocabulary introduction version of computing lexical scores with token-sequence sizew = 20, and Tiling (B) indicates the blocks version with token-sequence size w = 20 and blocksize k = 10.
Both versions' results are shown at both the low cutoff (LC) and the high cutoff(HC) for terminating boundary assignment.
Judges shows the average kappa, precision, andrecall for all judges averaged over all texts.Baseline Tiling (V) Tiling (B) JudgesLC HC LC HCP R K P K P K75 47K P K50 51 23 52 78 32 58 64 46 66 71 59 65 83 71Five out of seven readers indicated a break between paragraphs 18 and 19.
Thealgorithm registers a slight, but not significant valley at this point.
Upon inspection itturns out that paragraph 19 really is a continuation of the discussion in 18, answeringa question that is posed at the end of 18.
However, paragraph 19 begins with anintroductory phrase type that strongly signals a change in subtopic: For the last twocenturies, astronomers have studied .
.
.
.The final paragraph is a summary of the entire text; the algorithm recognizes thechange in terminology from the preceding paragraphs and marks a boundary, but onlytwo of the readers chose to differentiate he summary; for this reason the algorithm isjudged to have made an error even though this sectioning decision is reasonable.
Thisillustrates the inherent fallibility of testing against reader judgments, although in partthis is because the judges were given loose constraints.6.4 Results: Quantitative AnalysisTo assess the results of the algorithm quantitatively, I follow the advice of Gale, Church,and Yarowsky (1992a), and compare the algorithm against both upper and lowerbounds.
The upper bound in this case is the reader judgment data.
The lower boundis a baseline algorithm that is a simple, reasonable approach to the problem, whichcan be automated.
A simple way to segment the texts is to place boundaries randomlyin the document, constraining the number of boundaries to equal that of the averagenumber of paragraph gaps per document assigned as boundaries by judges.
In thetest data, boundaries are placed in about 39% of the paragraph gaps.
A program waswritten that places a boundary at each potential gap 39% of the time (using a randomnumber generator), and run 10,000 times for each text, and the average of the scoresof these runs was found.
These scores appear in Table 1.The algorithms are evaluated according to the proportion of "true" or majorityboundaries they select out of the total selected (precision) and the proportion of "true"boundaries found out of the total possible (recall) (Salton 1989).
Precision also impliesthe number of extraneous boundaries (or false positives, or insertion errors), and recallimplies the number of missed boundaries (or false negatives, or deletion errors).Table 1 shows that both the blocks algorithm for lexical score assignment and thevocabulary introduction algorithm fall between the upper and lower bounds.
The re-sults are shown for making both a liberal (LC) and a conservative (HC) number ofboundary assignments ( ee Section 5.5).
As is to be expected, when more boundariescan be assigned, recall becomes higher at the expense of precision, and conversely,56Hearst TextTilingTab le  2Precision for various parameter settings at the recall level obtained on average by the judgesfor 12 texts.
NP: number of paragraphs; NB: number of boundaries according to judges'consensus; JP: judges' average precision; JR: judges' average recall; K: kappa for the judges foreach text; Bk: precision for the blocks algorithm with block size k and w = 20; Vw: precision forthe vocabulary introduction algorithm with token sequence size w. Dashes occur in cases inwhich the algorithm does not produce a recall level equivalent to that of the judges' average.NP NB JP JR B9 B10 B12 V10 V16 V20 V241 18 82 30 103 21 94 41 145 30 96 25 167 39 88 28 109 27 1110 24 811 17 812 21 9.809 .696.897 .714.907 .778.892 .684.716 .619.932 .688.736 .732.793 .657.917 .649.743 .857.812 .768.839 .651K B7.56 .580.74 .877.72 .875.68 .593.72 .480.52.75.63 1.0.65 .682.67 .695.61 - -.58 .673.580 .611 .524- -  .781.875 .875 .788.577 .614 .790.687 .687 - -1.0 .766 .766.854 .781 .683.707 .707.605 .544 - -.673 .745  .745.480 .500 .442 .442.505 .617 .633 - -.583 .500  .778  .636.528 .558 .633 - -.478 .649  .500  .581.785 .785 - -  - -.422 .634  .402  .467.522 .464 .541 .450.460 .416  .704  .588.478 - -  .471.380 .458 .591 .662.500 .604  .539  .455when boundary assignment is conservative, better precision is obtained at the ex-pense of recall.
This table also shows the average K scores for the agreement betweenthe algorithm and the judges.
The scores for the blocks version of the algorithm arestronger than those for the vocabulary introduction version.Table 2 shows results in more detail, varying some of the parameter settings.
Toallow for a more direct comparison, the precision for each version of the algorithmis shown at the recall level obtained by the judges, on average.
This is computed asfollows for each version of the algorithm: The depth scores are examined in order oftheir strength.
For each depth score, if it corresponds to a true boundary, the countof correct boundaries is incremented, otherwise the count of incorrect boundaries isincremented.
Precision and recall are computed after each correct boundary encoun-tered.
When the recall equals that of the judges' average recall, the correspondingprecision of the algorithm is returned.
If the recall level exceeds that of the judges',then the value of the precision is estimated as a linear interpolation between the twoprecision scores whose recall scores most closely surround that of the judges' averagerecall.
(This assumption of a linear interpolation is justified because in most cases,although not all, precision changes monotonically.)
In some cases the algorithm doesnot produce a recall level as high as that found by the judges, since paragraphs witha nonpositive depth score are not eligible for boundary assignment, and these casesare marked with a dash.
Note that this evaluation does away with the need for LCand HC cutoff levels.From Table 2 we can see that varying the parameter settings improves the scores forsome texts while detracting from others.
We can also see that the blocks algorithm forlexical score determination produces tronger esults in most cases than the vocabularyintroduction method, although the latter seems to do better on the cases where theblocks algorithm finds few boundaries (e.g., texts 6, 7, and 11).
In almost all cases thealgorithms are not as accurate as the judges, but the scores for the blocks version ofthe algorithm are very strong in many cases.In looking at the results in more detail, one might wonder  why the algorithmperforms better on some texts than on others.
Text 7, for example, scores especially57Computational Linguistics Volume 23, Number 1poorly.
This may be caused by the fact that this text has a markedly different stylefrom the others.
It is a chatty article (about how to survive office politics), and con-sists of a series of anecdotes about particular individuals.
The article is interspersedthroughout with spoken quotations, and these tend to throw the algorithm off becausespoken statements usually contain different vocabulary than the surrounding prose.This phenomenon occurs in some of the other texts as well, but to a much lesserextent.
It suggests a need for recognizing and accommodating very short digressionsmore effectively.
Another interesting property of this text is that most of the subtopicswitches occur when switching from one anecdote to another, and by inspection itappears that the best cues for these switches are pronouns that appear on the stop listand are discarded (for example, the anecdotes alternate between men and women'sexperiences, and correspondingly alternate between using she and her and using heand him).
However, in most cases, use of the stop list improves results.It should also be noted that the texts used in this study were not chosen to havewell-defined boundaries, and so pose a difficult est for the algorithm.
Perhaps ometests against exts with more obvious subtopic boundaries (for which the kappa coef-ficient for interjudge agreement is larger) would be illuminating.6.5 Detecting Breaks between Consecutive DocumentsOne way to evaluate the algorithm is in terms of how well it distinguishes entirearticles from one another when they are concatenated into one file.
Nomoto and Nitta(1994) implement the tf.idf version of TextTiling from Hearst (1993) and Hearst andPlaunt (1993) and evaluate it this way on Japanese newswire text.
13 Also, as discussedin Section 4, Reynar (1994) uses this form of evaluation on a greedy version of theblocks algorithm.This task violates a major assumption of the TextTiling algorithm.
TextTiling as-sumes that the similarity comparisons are done within the vocabulary patterns ofone text, and so a relatively large shift in vocabulary indicates a change in subtopic.Because this evaluation method assumes that article boundary changes are more im-portant han subtopic boundary changes, it penalizes the algorithm for marking verystrong subtopic hanges that occur within a very cohesive document before relativelyweaker changes in vocabulary between similar articles.
For example, for hypotheticalarticles dl, d2, and d3, assume dl has very strong internal coherence indicators, d2 hasrelatively weak ones, and d3 is in the midrange.
The interidr subtopic transition scoresfor dl can swamp out the score for the transition between d2 and d3.Nevertheless, because others have used this evaluation method, one such evalu-ation is shown here as well.
The evaluation set consisted of 44 articles from the WallStreet Journal from 1989.
Consecutive articles were used, except any article fewer than10 sentences was removed.
The data consisted of 691 paragraphs, most of which con-tained between 1 and 3 sentences, ome of which were very short, e.g., article bylines(thus making exact assignment ofboundary locations more difficult).
The text was not"clean": several articles consisted of a sequence of stories, several had tabular data,and one article was just a listing of interest rates.The blocks version of TextTiling was run over this data using the default param-eter settings.
The depth scores were sorted and the number of assignments o articleboundaries that were within three sentences of the correct location were recorded atseveral cutoff levels and are shown in Table 3.
B corresponds tothe number of bound-13 Instead of using fixed-sized blocks, Nomoto and Nitta (1994) take advantage of the fact hat Japaneseprovides discourse markers indicating multi-sentence units that participate in a topic/commentrelationship, and find these motivated units can work slightly better.58Hearst TextTilingTable 3Performance for blocks algorithmwith default settings distinguishingbetween article boundaries innewspaper text consisting of 44articles.
B: number of boundarieschosen; C: number of correctboundaries; P: precision; R: recall.B C P R10 8 .80 .1920 16 .80 .3730 22 .73 .5140 27 .68 .6343* 29 .67 .6750 31 .62 .7260 36 .60 .8370 41 .59 .95aries assigned, in sorted order (i.e., the first row shows the precision and recall afterthe first 10 boundaries are assigned), C corresponds to the number of correctly placedboundaries, P the precision, R the recall, and the asterisk shows the precision/recallbreak-even point.The higher-scoring boundaries are almost always exact hits, but those farther downare more likely to be off by one to three sentences.
Only one transition is missedentirely, and it occurs after a sequence of five isolated sentences and a byline (a weakboundary is marked preceding these isolated sentences).
The high-scoring boundariesthat do not correspond to shifts between articles almost always correspond to strongsubtopic shifts.
One exception occurs in the article consisting only of interest ratelistings.
Another occurs in an article associating numerical information with names.Overall the scores are much stronger than those reported in Reynar (1994), and arecomparable to those of Nomoto and Nitta (1994) whose best precision/recall trade-offon a collection of approximately 80articles is approximately .50 precision and .81 recall.However, all three studies are done on different est collections and so comparisonsare at best suggestive.7.
Summary and Future WorkThis article has described an algorithm that uses changes in patterns of lexical repeti-tion as the cue for the segmentation f expository texts into multi-paragraph subtopicstructure.
It has also advocated the investigation and use of the multi-paragraph dis-course unit, something that had not been explored .in the computational literatureuntil this work was introduced.
The algorithms described here are fully implemented,and use term repetition alone, without requiring thesaural relations, knowledge bases,or inference mechanisms.
Evaluation reveals acceptable performance when comparedagainst human judgments of segmentation, although there is room for improvement.TextTiles have already been integrated into a user interface in an information re-trieval system (Hearst 1995) and have been used successfully for segmenting Arabicnewspaper texts, which have no paragraph breaks, for information retrieval (Has-nah 1996).
With the increase in importance of multimedia information, especially inthe context of Digital Library projects, the need for segmentation a d summarizationof alternative media types is becoming increasingly important.
For example, the al-59Computational Linguistics Volume 23, Number 1gorithms described here should prove useful for topic-based segmentation of videotranscripts (Christel et al 1995).
In a line of work we call Mixed-Media access (Chenet al 1994), textual subtopic structure is being integrated with other media types, suchas images and speech.TextTiling has been used in innovative ways by other researchers.
Karlgren (1996),in a study of the effects of stylistic variation in texts on information retrieval results,uses TextTiling as one of several ways of characterizing newspaper texts.
Overall, hefinds that relevant documents tend to be more complex than nonrelevant ones in termsof length, sentence structure, and other metrics.
When examining documents of alllengths, he finds that relevant documents tend to have more TextTiles than nonrelevantones (95% significant by a Mann Whitney test).
As another example of an innovativeapplication, van der Eijk (1994) suggests using TextTiles to align parallel multilingualtext corpora according to the overlap in their subtopic structure for English, German,and French text.
This work, along with that of Nomoto and Nitta (1994), on Japanese,and Hasnah (1996), on Arabic, also provides evidence that TextTiling is applicable toa wide range of natural anguages.There are several ways that the algorithms could be modified to attempt o im-prove the results.
One way is to use thesaural relations in addition to term repetition tomake better estimates about the cohesiveness of the discussion.
Earlier work (Hearst1993) incorporated thesaural information into the ' algorithms, but later experimentsfound that this information degrades the performance.
This could very well be due toproblems with the thesaurus and assignment algorithm used.
A simpler algorithm thatjust posits relations among terms that are a small distance apart according to Word-Net (Miller et al 1990), modeled after Morris and Hirst's heuristics, might work better.Therefore, the issue should not be considered closed, but rather as an area for futureexploration, with this work as a baseline for comparison.
The approach to similaritycomparison suggested by Kozima (1993), while very expensive to compute, might alsoprove able to improve results.
Other ways of computing semantic similarity, such asthose of Sch~itze (1993) or Resnik (1995), may also prove useful.
As a related point, ex-perimentation should be done with variations in tokenization strategies, and it may beespecially interesting to incorporate phrase or bigram information into the similaritycomputation.The methods for computing lexical score also have the potential to be improved.Some possibilities are weighting terms according to their prior probabilities, weight-ing terms according to the distance from the location under scrutiny according to aGaussian distribution, or treating the plot as a probabilistic time series and detectingthe boundaries based on the likelihood of a transition from nontopic to topic.
Anotheralternative is to devise a good normalization strategy that would allow for meaningfulcomparisons of "real" paragraphs, rather than regular-sized windows of text.The question arises as to how to extend the algorithm to capture hierarchical struc-ture.
One solution is to use the coarse subtopic structure to guide the more fine-grainedmethods.
Another is to make several passes through the text, using the results of oneround as the input, in terms of which blocks of text are compared, in the next round.Finally, it may prove fruitful to use localized discourse cue information or otherspecialized processing around potential boundary locations to help better determineexactly where segmentation should take place.
The use of discourse cues for detectionof segment boundaries and other discourse purposes has been extensively researched,although predominantly on spoken text (see Hirschberg and Litman \[1993\] for a sum-mary of six research groups' treatments of 64 cue words).
It is possible that incorpo-ration of such information may improve the cases where the algorithm is off by oneparagraph.60Hearst TextTilingAcknowledgmentsThis article was enormously improved as aresult of the careful comments of fouranonymous reviewers, the editors of thisspecial issue, and Christine Nakatani andAndreas StOlcke.
Earlier writeups of thiswork benefited from the comments of JanPedersen, Per-Kristian Halvorsen, KenChurch, Bill Gale, David Yarowsky, GraemeHirst, Jeff Siskind, Michael Braverman,Narciso Jaramillo, Dan Jurafsky, Mike Schiff,Dekai Wu, Penni Sibun, John Maxwell,Hinrich Sch~itze, and Christine Nakatani.
Iwould like to thank Anne Fontaine for herinterest and help in the early stages of thiswork, and Robert Wilensky for supportingthis line of research as my thesis advisor.This work was sponsored in part by theAdvanced Research Projects Agency underGrant No.
MDA972-92-J-1029 with theCorporation for National ResearchInitiatives (CNRI), the University ofCalifornia and Digital EquipmentCorporation under Digital's flagshipresearch project Sequoia 2000: LargeCapacity Object Servers to Support GlobalChange Research, and by the Xerox PaloAlto Research Center.ReferencesBrown, Gillian and George Yule.
1983.Discourse Analysis.
Cambridge Textbooksin Linguistics Series.
CambridgeUniversity Press.Callan, James P. 1994.
Passage-levelevidence in document retrieval.
InProceedings ofthe 17th Annual InternationalACM/SIGIR Conference, pages 302-310,Dublin, Ireland.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22(2):249-254.Chafe, Wallace L. 1979.
The flow of thoughtand the flow of language.
In Talmy Giv6n,editor, Syntax and Semantics: Discourse andSyntax, volume 12.
Academic Press, NewYork, pages 159-182.Chen, Francine, Marti A. Hearst, JulianKupiec, Jan O. Pedersen, and LynnWilcox.
1994.
Metadata in mixed-mediaaccess.
SIGMOD Record, 23(4):64-71.Chen, Francine R. and Margaret Withgott.1992.
The use of emphasis toautomatically summarize a spokendiscourse.
In ICASSP-92:1992 IEEEInternational Conference on Acoustics, Speechand Signal Processing, volume 1, pages229-232.Christel, M., T. Kanade, M. Mauldin,R.
Reddy, M. Sirbu, S. Stevens, andH.
Wactlar.
1995.
Informedia digital videolibrary.
Communications of the ACM,38(4):57-58, April.Cochran, W, G. 1950.
The comparison ofpercentages in matched samples.Biometrika, 37:256-266.Condon, Sherri L. and Claude G. Cech.1995.
Problems for reliable discoursecoding systems.
In Johanna Moore andMarilyn Walker, editors, Empirical Methodsin Discourse: Interpretation & Generation,AAAI Technical Report SS-95-06, MenloPark, CA.
AAA!
Press.Gale, William A., Kenneth W. Church, andDavid Yarowsky.
1992a.
Estimating upperand lower bounds on the performance ofword-sense disambiguation programs.
InProceedings ofthe 30th Meeting, pages249-256.
Association for ComputationalLinguistics.Gale, William A., Kenneth W. Church, andDavid Yarowsky.
1992b.
One sense perdiscourse.
In Proceedings ofthe DARPASpeech and Natural Language Workshop.Girill, T. R. 1991.
Information chunking asan interface design issue for full-textdatabases.
In Martin Dillon, editor,Interfaces for Information Retrieval ndOnline Systems.
Greenwood Press, NewYork, NY, pages 149-158.Grimes, J.
1975.
The Thread of Discourse.Mouton, The Hague.Grosz, Barbara J. and Candace L. Sidner.1986.
Attention, intention, and thestructure of discourse.
ComputationalLinguistics, 12(3):172-204.Halliday, M. A. K. and R. Hasan.
1976.Cohesion in English.
Longman, London.Harman, Donna.
1991.
How effective issuffixing?
Journal of the American Society forInformation Science (JASIS), 42(1):7-15.Harman, Donna.
1993.
Overview of the firstText REtrieval Conference.
In Proceedingsof the 16th Annual International ACM/SIG1RConference, pages 36-48, Pittsburgh, PA.Hasnah, Ahmad.
1996.
Full Text Processingand Retrieval: Weight Ranking, TextStructuring, and Passage Retrieval for ArabicDocuments.
Ph.D. thesis, Illinois Instituteof Technology.Hearst, Marti A.
1993.
TextTiling: Aquantitative approach to discoursesegmentation.
Technical Report Sequoia93/24, Computer Science Division,University of California, Berkeley.Hearst, Marti A.
1994a.
Context and Structurein Automated Full-Text Information Access.Ph.D.
thesis, University of California at61Computational Linguistics Volume 23, Number 1Berkeley.
(Computer Science DivisionTechnical Report UCB/CSD-94/836).Hearst, Marti A.
1994b.
Multi-paragraphsegmentation f expository text.
InProceedings ofthe 32nd Meeting, pages 9-16,Las Cruces, NM, June.
Association forComputational Linguistics.Hearst, Marti A.
1995.
TileBars:Visualization of term distributioninformation i  full text informationaccess.
In Proceedings ofthe ACM SIGCHIConference on Human Factors in ComputingSystems, Denver, CO, May.Hearst, Marti A.
1996.
Improving full-textprecision using simple query constraints.In Proceedings ofthe Fifth Annual Symposiumon Document Analysis and InformationRetrieval (SDAIR), Las Vegas, NV.Hearst, Marti, Jan Pedersen, Peter Pirolli,Hinrich Sch(ietze, Gregory Grefenstette,and David Hull.
1996.
Four TREC-4Tracks: The Xerox site report.
In DonnaHarman, editor, Proceedings ofthe FourthText Retrieval Conference TREC-4.
NationalInstitute of Standards and TechnologySpecial Publication.
(To appear).Hearst, Marti A. and Christian Plaunt.
1993.Subtopic structuring for full-lengthdocument access.
In Proceedings ofthe 16thAnnual International ACM/SIGIRConference, pages 59-68, Pittsburgh, PA.Hersh, William R., Diane L. Elliot, David H.Hickam, Stephanie L. Wolf, and AnnaMolnar.
1995.
Towards new measures ofinformation retrieval evaluation.
InProceedings ofthe 18th Annual InternationalACM/SIGIR Conference, pages 164-170,Seattle, WA.Hinds, John.
1979.
Organizational patternsin discourse.
In Talmy Giv6n, editor,Syntax and Semantics: Discourse and Syntax,volume 12.
Academic Press, New York,pages 135-158.Hirschberg, Julia and Diane Litman.
1993.Empirical studies on the disambiguationof cue phrases.
Computational Linguistics,19(3):501-530.Hirschberg, Julia and Christine H. Nakatani.1996.
A prosodic analysis of discoursesegments in direction-giving monologues.In Proceedings ofthe 34th Annual Meeting,pages 286-293, Santa Cruz, CA.Association for ComputationalLinguistics.Hull, David and Gregory Grefenstette.
1995.Stemming algorithms--A case study fordetailed evaluation.
Journal of the AmericanSociety for Information Science (JASIS), 46(9).Hwang, Chung Hee and Lenhart K.Schubert.
1992.
Tense trees as the 'finestructure' of discourse.
In Proceedings ofthe30th Meeting, pages 232-240.
Associationfor Computational Linguistics.Isard, Amy and Jean Carletta.
1995.Replicability of transaction and actioncoding in the map task corpus.
InJohanna Moore and Marilyn Walker,editors, Empirical Methods in Discourse:Interpretation & Generation, AAAI TechnicalReport SS-95~06.
AAAI Press, Menlo Park,CA.Karlgren, Jussi.
1996.
Stylistic variation inan information retrieval experiment.
InProceedings ofthe NeMLaP-2 Conference,Ankara, Turkey, September.Karttunen, Lauri, Kimmo Koskenniemi, andRonald M. Kaplan.
1987.
A compiler fortwo-level phonological rules.
In MaryDalrymple, editor, Tools for MorphologicalAnalysis.
Center for the Study ofLanguage and Information, Stanford, CA.Kozima, Hideki.
1993.
Text segmentationbased on similarity between words.
InProceedings ofthe 31th Annual Meeting(Student Session), pages 286-288,Columbus, OH.
Association forComputational Linguistics.Kuno, Susumo.
1972.
Functional sentenceperspective: A case study from Japaneseand English.
Linguistic Inquiry,3(3):269-320.Kupiec, Julian, Jan Pedersen, and FrancineChen.
1995.
A trainable documentsummarizer.
In Proceedings ofthe 18thAnnual International ACM/SIGIRConference, pages 68-73, Seattle, WA.Lakoff, George P. 1972.
Structuralcomplexity in fairy tales.
The Study of Man,1:128-150.Lewis, David D. and Philip J. Hayes.
1994.Special issue on text categorization.Transactions ofOffice Information Systems,12(3).Litman, Diane J. and Rebecca J. Passonneau.1995.
Combining multiple knowledgesources for discourse segmentation.
IProceedings ofthe 33rd Meeting, pages108-115, June.
Association forComputational Linguistics.Longacre, R. E. 1979.
The paragraph as agrammatical unit.
In Talmy Giv6n, editor,Syntax and Semantics: Discourse and Syntax,volume 12.
Academic Press, New York,pages 115-134.Mann, William C. and Sandra A.Thompson.
1987.
Rhetorical structuretheory: A theory of text organization.Technical Report ISI/RS 87-190, ISI.Marchionini, Gary, Peter Liebscher, and XiaLin.
1991.
Authoring hyperdocuments:Designing for interaction.
In MartinDillon, editor, Interfaces for Information62Hearst TextTilingRetrieval and Online Systems.
GreenwoodPress, New York, NY, pages 119-131.Miller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross, andKatherine J. Miller.
1990.
Introduction toWordNet: An on-line lexical database.Journal of Lexicography, 3(4):235-244.Mittendorf, Elke and Peter Scha'uble.
1994.Document and passage retrieval based onHidden Markov Models.
In Proceedings ofthe 17th Annual International ACM/SIGIRConference, pages 318-327, Dublin, Ireland.Moffat, Alistair, Ron Sacks-Davis, RossWilkinson, and Justin Zobel.
1994.Retrieval of partial documents.
In DonnaHarman, editor, Proceedings ofthe SecondText Retrieval Conference TREC-2.
NationalInstitute of Standards and TechnologySpecial Publication 500-215, pages181-190.Mooney, David J., M. Sandra Carberry, andKathleen E McCoy.
1990.
The generationof high-level structure for extendedexplanations.
In Proceedings oftheThirteenth International Conference onComputational Linguistics, volume 2, pages276-281, Helsinki.Moore, Johanna D. and Martha E. Pollack.1992.
A problem for RST: The need formulti-level discourse analysis.Computational Linguistics, 18(4):537-544.Morris, Jane.
1988.
Lexical cohesion, thethesaurus, and the structure of text.Technical Report CSRI-219, ComputerSystems Research Institute, University ofToronto.Morris, Jane and Graeme Hirst.
1991.Lexical cohesion computed by thesauralrelations as an indicator of the structure oftext.
Computational Linguistics, 17(1):21-48.Nakatani, Christine H., Barbara J. Grosz,David D. Ahn, and Julia Hirschberg.
1995.Instructions for annotating discourses.Technical Report TR-25-95, HarvardUniversity Center for Research inComputing Technolog~ Cambridge, MA.Nomoto, Tadashi and Yoshihiko Nitta.
1994.A grammatico-statistical approach todiscourse partitioning.
In Proceedings oftheFifteenth International Conference onComputational Linguistics (COLING), pages1145-1150, Kyoto, Japan, August.Paice, Chris D. 1990.
Constructing literatureabstracts by computer: Techniques andprospects.
Information Processing andManagement, 26(1):171-186.Passonneau, Rebecca J. and Diane J. Litman.1993.
Intention-based segmentation:Human reliability and correlation withlinguistic cues.
In Proceedings ofthe 31stAnnual Meeting, pages 148-155.Association for ComputationalLinguistics.Phillips, Martin.
1985.
Aspects of TextStructure: An Investigation of the LexicalOrganisation of Text.
North-Holland,Amsterdam.Resnik, Philip.
1995.
Using informationcontent o evaluate semantic similarity ina taxonomy.
In Proceedings oftheInternational Joint Conference on ArtificialIntelligence (IJCAI-95), volume 1, pages448-453, Montreal, Canada.Reynar, Jeffrey C. 1994.
An automaticmethod of finding topic boundaries.
InProceedings ofthe 32nd Annual Meeting(Student Session), pages 331-333, LasCruces, NM.
Association forComputational Linguistics.RosG Carolyn Penstein.
1995.
Conversationacts, interactional structure, andconversational outcomes.
In JohannaMoore and Marilyn Walker, editors,Empirical Methods in Discourse:Interpretation & Generation, AAAI TechnicalReport SS-95-06.
AAAI Press, Menlo Park,CA.Rumelhart, David.
1975.
Notes on a schemafor stories.
In Daniel G. Bobrow andAllan Collins, editors, Representation a dUnderstanding.
Academic Press, NewYork, pages 211-236.Salton, Gerard.
1989.
Automatic TextProcessing: The Transformation, Analysis, andRetrieval of Information by Computer.Addison-Wesley, Reading, MA.Salton, Gerard and James Allan.
1993.Selective text utilization and texttraversal.
In Proceedings ofACM Hypertext"93.Salton, Gerard, James Allan, and ChrisBuckley.
1993.
Approaches to passageretrieval in full text information systems.In Proceedings ofthe 16th AnnualInternational ACM/SIGIR Conference, pages49-58, Pittsburgh, PA.Salton, Gerard, Amit Singhal, Chris Buckley,and Mandar Mitra.
1996.
Automatic textdecomposition using text segmentationand text themes.
In Proceedings ofHypertext '96, Seventh ACM Conference onHypertext, pages 53-65, Washington, D.C.Schiffrin, Deborah.
1987.
Discourse Markers.Cambridge University Press, Cambridge.SchCitze, Hinrich.
1993.
Word space.
InStephen J. Hanson, Jack D. Cowan, andC.
Lee Giles, editors, Advances in NeuralInformation Processing Systems 5.
MorganKaufmann, San Mateo, CA.Skorochod'ko, E.E 1972.
Adaptive methodof automatic abstracting and indexing.
InC.V.
Freiman, editor, Information Processing63Computational Linguistics Volume 23, Number 171: Proceedings ofthe IFIP Congress 71,pages 1179-1182.
North-HollandPublishing Company.Stark, Heather.
1988.
What do paragraphmarkers do?
Discourse Processes,11(3):275-304.Stoddard, Sally.
1991.
Text and Texture:Patterns of Cohesion.
Advances inDiscourse Processes, volume XL.
AblexPublishing Corporation, Norwood, NJ.Tannen, Deborah.
1989.
Talking Voices:Repetition, Dialogue, and Imagery inConversational Discourse.
Studies inInteractional Sociolinguistics 6.Cambridge University Press.Tombaugh, J., A. Lickorish, and P. Wright.1987.
Multi-window displays for readersof lengthy texts.
International Journal ofMan \[sic\] -Machine Studies, 26:597-615.van der Eijk, Pim.
1994.
Comparativediscourse analysis of parallel texts.Technical Report cmp-lg/9407022, DigitalEquipment Corporation.van Dijk, Teun A.
1980.
Macrostructures.Lawrence Erlbaum Associates, Hillsdale,N.J.van Dijk, Teun A.
1981.
Studies in thePragmatics of Discourse.
Mouton, TheHague.Walker, Marilyn A.
1992.
Redundancy incollaborative dialogue.
In Proceedings oftheFourteenth International Conference onComputational Linguistics (COLING), pages345-351, Nantes, France, July.Walker, Marilyn and Steve Whittaker.
1990.Mixed initiative dialogue: Aninvestigation i to discourse segmentation.In Proceedings ofthe 28th Annual Meeting,pages 70-78.
Association forComputational Linguistics.Webber, Bonnie Lynn.
1987.
Theinterpretation f tense in discourse.
InProceedings ofthe 25th Annual Meeting,pages 147-154, Stanford, CA.
Associationfor Computational Linguistics.Webber, Bonnie Lynn.
1988.
Discoursedeixis: Reference to discourse segments.In Proceedings ofthe 26th Annual Meeting,pages 113-122, Buffalo, NY.
Associationfor Computational Linguistics.Yarowsky, David.
1992.
Word sensedisambiguation using statistical models ofRoget's categories trained on largecorpora.
In Proceedings ofthe FourteenthInternational Conference on ComputationalLinguistics, pages 454-460, Nantes, France,July.Youmans, Gilbert.
1991.
A new tool fordiscourse analysis: Thevocabulary-management profile.Language, 67(4):763-789.64
