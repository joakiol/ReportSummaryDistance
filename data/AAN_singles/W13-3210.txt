Proceedings of the Workshop on Continuous Vector Space Models and their Compositionality, pages 83?90,Sofia, Bulgaria, August 9 2013. c?2013 Association for Computational LinguisticsTowards Dynamic Word Sense Discrimination with Random IndexingHans Moen, Erwin Marsi, Bjo?rn Gamba?ckNorwegian University of Science and TechnologyDepartment of Computer and Information and ScienceSem S?lands vei 7-9NO-7491 Trondheim, Norway{hansmoe,emarsi,gamback}@idi.ntnu.noAbstractMost distributional models of word sim-ilarity represent a word type by a singlevector of contextual features, even though,words commonly have more than onesense.
The multiple senses can be capturedby employing several vectors per word in amulti-prototype distributional model, pro-totypes that can be obtained by first con-structing all the context vectors for theword and then clustering similar vectorsto create sense vectors.
Storing and clus-tering context vectors can be expensivethough.
As an alternative, we introduceMulti-Sense Random Indexing, which per-forms on-the-fly (incremental) clustering.To evaluate the method, a number of mea-sures for word similarity are proposed,both contextual and non-contextual, in-cluding new measures based on optimalalignment of word senses.
Experimentalresults on the task of predicting semantictextual similarity do, however, not showa systematic difference between single-prototype and multi-prototype models.1 IntroductionMany terms have more than one meaning, orsense.
Some of these senses are static and canbe listed in dictionaries and thesauri, while othersenses are dynamic and determined by the con-texts the terms occur in.
Work in Word Sense Dis-ambiguation often concentrate on the static wordsenses, making the task of distinguishing betweenthem one of classification into a predefined set ofclasses (i.e., the given word senses); see, e.g., Erket al(2013; Navigli (2009) for overviews of cur-rent work in the area.
The idea of fixed genericword senses has received a fair amount of criti-cism in the literature (Kilgarriff, 2000).This paper instead primarily investigates dy-namically appearing word senses, word senses thatdepend on the actual usage of a term in a cor-pus or a domain.
This task is often referred to asWord Sense Induction or Word Sense Discrimina-tion (Schu?tze, 1998).
This is, in contrast, essen-tially a categorisation problem, distinguished bydifferent senses being more or less similar to eachother at a given time, given some input data.
Thedividing line between Word Sense Disambigua-tion and Discrimination is not necessarily razorsharp though: also different senses of a term listedin a dictionary tend to have some level of overlap.In recent years, distributional models have beenwidely used to infer word similarity.
Most suchmodels represent a word type by a single vector ofcontextual features obtained from co-occurrencecounts in large textual corpora.
By assigning asingle vector to each term in the corpus, the re-sulting model assumes that each term has a fixedsemantic meaning (relative to all the other terms).However, due to homonomy and polysemy, wordsemantics cannot be adequately represented by asingle-prototype vector.Multi-prototype distributional models in con-trast employ different vectors to represent differentsenses of a word (Reisinger and Mooney, 2010).Multiple prototypes can be obtained by first con-structing context vectors for all words and thenclustering similar context vectors to create a sensevector.
This may be expensive, as vectors need tostored and clustered.
As an alternative, we proposea new method called Multi-Sense Random Index-ing (MSRI), which is based on Random Indexing(Kanerva et al 2000) and performs an on-the-fly(incremental) clustering.MSRI is a method for building a multi-prototype / multi-sense vector space model, whichattempts to capture one or more senses per uniqueterm in an unsupervised manner, where each senseis represented as a separate vector in the model.83This differs from the classical Random Indexing(RI) method which assumes a static sense inven-tory by restricting each term to have only one vec-tor (sense) per term, as described in Section 2.
TheMSRI method is introduced in Section 3.Since the induced dynamic senses do not neces-sarily correspond to the traditional senses distin-guished by humans, we perform an extrinsic eval-uation by applying the resulting models to datafrom the Semantic Textual Similarity shared task(Agirre et al 2013), in order to compare MSRIto the classical RI method.
The experimental set-up is the topic of Section 4, while the results ofthe experiments are given in Section 5.
Section 6then sums up the discussion and points to ways inwhich the present work could be continued.2 Vector Space ModelsWith the introduction of LSA, Latent SemanticAnalysis (Deerwester et al 1990), distributedmodels of lexical semantics, built from unla-belled free text data, became a popular sub-fieldwithin the language processing research commu-nity.
Methods for building such semantic mod-els rely primarily on term co-occurrence infor-mation, and attempt to capture latent relationsfrom analysing large amounts of text.
Most ofthese methods represent semantic models as multi-dimensional vectors in a vector space model.After LSA, other methods for building seman-tic models have been proposed, one of them beingRandom Indexing (Kanerva et al 2000).
Com-mon to these methods is that they generate a con-text vector for each unique term in the training datawhich represents the term?s ?contextual?
meaningin the vector space.
By assigning a single con-text vector to each term in the corpus, the resultingmodel assumes that each term has a fixed semanticmeaning (relative to all other terms).Random Indexing incrementally builds a co-occurrence matrix of reduced dimensionality, byfirst assigning index vectors to each unique term.The vectors are of a predefined size (typicallyaround 1000), and consist of a few randomlyplaced 1s and -1s.
Context vectors of the same sizeare also assigned to each term, initially consistingof only zeros.
When traversing a document corpususing a sliding window of a fixed size, the contextvectors are continuously updated: the term in thecentre of the window (the target term), has the in-dex vectors of its neighbouring terms (the ones inthe window) added to its context vector using vec-tor summation.
Then the cosine similarity mea-sure can be used on term pairs to calculate theirsimilarity (or ?contextual similarity?
).Random Indexing has achieved promising re-sults in various experiments, for example, on theTOEFL test (?Test of English as a Foreign Lan-guage?)
(Kanerva et al 2000).
However, it is ev-ident that many terms have more than one mean-ing or sense, some being static and some dynamic,that is, determined by the contexts the terms occurin.
Schu?tze (1998) proposed a method for clus-tering the contextual occurrences of terms into in-dividual ?prototype?
vectors, where one term canhave multiple prototype vectors representing sep-arate senses of the term.
Others have adoptedthe same underlying idea, using alternative meth-ods and techniques (Reisinger and Mooney, 2010;Huang et al 2012; Van de Cruys et al 2011; Dinuand Lapata, 2010).3 Multi-Sense Random Indexing, MSRIInspired by the work of Schu?tze (1998) andReisinger and Mooney (2010), this paper intro-duces a novel variant of Random Indexing, whichwe have called ?Multi-Sense Random Indexing?.MSRI attempts to capture one or more senses perunique term in an unsupervised and incrementalmanner, each sense represented as an separate vec-tor in the model.
The method is similar to classicalsliding window RI, but each term can have mul-tiple context vectors (referred to as sense vectorshere) which are updated separately.When updating a term vector, instead of directlyadding the index vectors of the neighbouring termsin the window to its context vector, the system firstcomputes a separate window vector consisting ofthe sum of the index vectors.
The similarity be-tween the window vector and each of the term?ssense vectors is calculated.
Each similarity scoreis then compared to a pre-set similarity threshold:?
if no score exceeds the threshold, the windowvector becomes a new separate sense vectorfor the term,?
if exactly one score is above the threshold,the window vector is added to that sense vec-tor, and?
if multiple scores are above the threshold, allthe involved senses are merged into one sensevector, together with the window vector.84Algorithm 1 MSRI trainingfor all terms t in a document D dogenerate window vector ~win from the neigh-bouring words?
index vectorsfor all sense vectors ~si of t dosim(si) = CosSim( ~win,~si)end forif sim(si..k) ?
?
thenMerge ~si..k and ~win through summingelseif sim(si) ?
?
then~si+ = ~winend ifelseif sim(si..n) < ?
thenAssign ~win as new sense vector of tend ifend ifend forSee Algorithm 1 for a pseudo code version.
Here?
represents the similarity threshold.This accomplishes an incremental (on-line)clustering of senses in an unsupervised manner,while retaining the other properties of classical RI.Even though the algorithm has a slightly highercomplexity than classical RI, this is mainly a mat-ter of optimisation, which is not the focus of thispaper.
The incremental clustering that we applyis somewhat similar to what is used by Lughofer(2008), although we are storing in memory onlyone element (i.e., vector) for each ?cluster?
(i.e.,sense) at any given time.When looking up a term in the vector space, apre-set sense-frequency threshold is applied to fil-ter out ?noisy?
senses.
Hence, senses that haveoccurred less than the threshold are not includedwhen looking up a term and its senses for, for ex-ample, similarity calculations.As an example of what the resulting modelscontain in terms of senses, Table 1 shows four dif-ferent senses of the term ?round?
produced by theMSRI model.
Note that these senses do not nec-essarily correspond to human-determined senses.The idea is only that using multiple prototypevectors facilitates better modelling of a term?smeaning than a single prototype (Reisinger andMooney, 2010).round1 round2 round3 round4finish camping inch launcherfinal restricted bundt grenadematch budget dough propelhalf fare thick antitankthird adventure cake antiaircraftTable 1: Top-5 most similar terms for four dif-ferent senses of ?round?
using the Max similaritymeasure to the other terms in the model.3.1 Term Similarity MeasuresUnlike classical RI, which only has a single con-text vector per term and thus calculates similaritybetween two terms directly using cosine similarity,there are multiple ways of calculating the similar-ity between two terms in MSRI.
Some alternativesare described in Reisinger and Mooney (2010).
Inthe experiment in this paper, we test four ways ofcalculating similarity between two terms t and t?in isolation, with the Average and Max methodsstemming from Reisinger and Mooney (2010).Let ~si..n and ~s?j..m be the sets of sense vectorscorresponding to the terms t and t?
respectively.Term similarity measures are then defined as:CentroidFor term t, compute its centroid vector bysumming its sense vectors ~si..n. The same isdone for t?
with its sense vectors ~s?j..m. Thesecentroids are in turn used to calculate the co-sine similarity between t and t?.AverageFor all ~si..n in t, find the pair ~si, ~s?j with high-est cosine similarity:1nn?i=1CosSimmax(~si, ~s?j)Then do the same for all ~s?j..m in t?
:1mm?j=1CosSimmax(~s?j , ~si)The similarity between t and t?
is computedas the average of these two similarity scores.MaxThe similarity between ti and t?i equals thesimilarity of their most similar sense:Sim(t, t?)
= CosSimmaxij (~si, ~s?i)85Hungarian AlgorithmFirst cosine similarity is computed for eachpossible pair of sense vectors ~si..n and ~s?j..m,resulting in a matrix of similarity scores.Finding the optimal matching from senses ~sito ~s?j that maximises the sum of similaritiesis known as the assignment problem.
Thiscombinatorial optimisation problem can besolved in polynomial time through the Hun-garian Algorithm (Kuhn, 1955).
The over-all similarity between terms t and t?
is thendefined as the average of the similarities be-tween their aligned senses.All measures defined so far calculate similarity be-tween terms in isolation.
In many applications,however, terms occur in a particular context thatcan be exploited to determine their most likelysense.
Narrowing down their possible meaning toa subset of senses, or a single sense, can be ex-pected to yield a more adequate estimation of theirsimilarity.
Hence a context-sensitive measure ofterm similarity is defined as:Contextual similarityLet ~C and ~C ?
be vectors representing the con-texts of terms t and t?
respectively.
Thesecontext vectors are constructed by summingthe index vectors of the neighbouring termswithin a window, following the same proce-dure as used when training the MSRI model.We then find s?
and s?
?
as the sense vectorsbest matching the context vectors:s?
= argmaxi CosSim(~si, ~C)s?
?
= argmaxj CosSim(~sj , ~C ?
)Finally, contextual similarity is defined as thesimilarity between these sense vectors:Simcontext(t, t?)
= CosSim(s?, s?
?
)3.2 Sentence Similarity FeaturesIn the experiments reported on below, a range ofdifferent ways to represent sentences were tested.Sentence similarity was generally calculated bythe average of the maximum similarity betweenpairs of terms from both sentences, respectively.The different ways of representing the data incombination with some sentence similarity mea-sure will here be referred to as similarity features.1.
MSRI-TermCentroid:In each sentence, each term is represented asthe sum of its sense vectors.
This is similarto having one context vector, as in classicalRI, but due to the sense-frequency filtering,potentially ?noisy?
senses are not included.2.
MSRI-TermMaxSense:For each bipartite term pair in the two sen-tences, their sense-pairs with maximum co-sine similarity are used, one sense per term.3.
MSRI-TermInContext:A 5 + 5 window around each (target) termis used as context for selecting one sense ofthe term.
A window vector is calculated bysumming the index vectors of the other termsin the window (i.e., except for the target termitself).
The sense of the target term which ismost similar to the window vector is used asthe representation of the term.4.
MSRI-TermHASenses:Calculating similarity between two terms isdone by applying the Hungarian Algorithmto all their bipartite sense pairs.5.
RI-TermAvg:Classical Random Indexing ?
each term isrepresented as a single context vector.6.
RI-TermHA:Similarity between two sentences is calcu-lated by applying the Hungarian Algorithm tothe context vectors of each constituent term.The parameters were selected based on a com-bination of surveying previous work on RI (e.g.,Sokolov (2012)), and by analysing how sensecounts evolved during training.
For MSRI, weused a similarity threshold of 0.2, a vector dimen-sionality of 800, a non-zero count of 6, and a win-dow size of 5 + 5.
Sense vectors resulting fromless than 50 observations were removed.
For clas-sical RI, we used the same parameters as for MSRI(except for a similarity threshold).4 Experimental SetupIn order to explore the potential of the MSRImodel and the textual similarity measures pro-posed here, experiments were carried out on datafrom the Semantic Textual Similarity (STS) sharedtask (Agirre et al 2012; Agirre et al 2013).86Given a pair of sentences, systems participatingin this task shall compute how semantically sim-ilar the two sentences are, returning a similar-ity score between zero (completely unrelated) andfive (completely semantically equivalent).
Goldstandard scores are obtained by averaging multi-ple scores obtained from human annotators.
Sys-tem performance is then evaluated using the Pear-son product-moment correlation coefficient (?)
be-tween the system scores and the human scores.The goal of the experiments reported here wasnot to build a competitive STS system, but ratherto investigate whether MSRI can outperform clas-sical Random Indexing on a concrete task such ascomputing textual similarity, as well as to identifywhich similarity measures and meaning represen-tations appear to be most suitable for such a task.The system is therefore quite rudimentary: a sim-ple linear regression model is fitted on the trainingdata, using a single sentence similarity measureas input and the similarity score as the dependentvariable.
The implementations of RI and MSRIare based on JavaSDM (Hassel, 2004).As data for training random indexing models,we used the CLEF 2004?2008 English corpus,consisting of approximately 130M words of news-paper articles (Peters et al 2004).
All text wastokenized and lemmatized using the TreeTaggerfor English (Schmid, 1994).
Stopwords were re-moved using a customized version of the stoplistprovided by the Lucene project (Apache, 2005).Data for fitting and evaluating the linear re-gression models came from the STS developmentand test data, consisting of sentence pairs witha gold standard similarity score.
The STS 2012development data stems from the Microsoft Re-search Paraphrase corpus (MSRpar, 750 pairs),the Microsoft Research Video Description cor-pus (MSvid, 750 pairs), and statistical machinetranslation output based on the Europarl corpus(SMTeuroparl, 734 pairs).
Test data for STS2012 consists of more data from the same sources:MSRpar (750 pairs), MSRvid (750 pairs) andSMTeuroparl (459 pairs).
In addition, differenttest data comes from translation data in the newsdomain (SMTnews, 399 pairs) and ontology map-pings between OntoNotes and WordNet (OnWN,750 pairs).
When testing on the STS 2012 data, weused the corresponding development data from thesame domain for training, except for OnWN wherewe used all development data combined.The development data for STS 2013 consistedof all development and test data from STS 2012combined, whereas test data comprised machinetranslation output (SMT, 750 pairs), ontologymappings both between WordNet and OntoNotes(OnWN, 561 pairs) and between WordNet andFrameNet (FNWN, 189 pairs), as well as news ar-ticle headlines (HeadLine, 750 pairs).
For sim-plicity, all development data combined were usedfor fitting the linear regression model, even thoughcareful matching of development and test data setsmay improve performance.5 Results and DiscussionTable 2 shows Pearson correlation scores per fea-ture on the STS 2012 test data using simple linearregression.
The most useful features for each dataset are marked in bold.
For reference, the scores ofthe best performing STS systems for each data setare also shown, as well as baseline scores obtainedwith a simple normalized token overlap measure.There is large variation in correlation scores,ranging from 0.77 down to 0.27.
Part of this vari-ation is due to the different nature of the data sets.For example, sentence similarity in the SMT do-main seems harder to predict than in the videodomain.
Yet there is no single measure that ob-tains the highest score on all data sets.
There isalso no consistent difference in performance be-tween the RI and MSRI measures, which seemto yield about equal scores on average.
TheMSRI-TermInContext measure has the low-est score on average, suggesting that word sensedisambiguation in context is not beneficial in itscurrent implementation.The corresponding results on the STS 2013 testdata are shown in Table 3.
The same observationsas for the STS 2012 data set can be made: againthere was no consistent difference between the RIand MSRI features, and no single best measure.All in all, these results do not provide any ev-idence that MSRI improves on standard RI forthis particular task (sentence semantic similarity).Multi-sense distributional models have, however,been found to outperform single-sense models onother tasks.
For example, Reisinger and Mooney(2010) report that multi-sense models significantlyincrease the correlation with human similarityjudgements.
Other multi-prototype distributionalmodels may yield better results than their single-prototype counterparts on the STS task.87Features: MSRpar MSRvid SMTeuroparl SMTnews OnWN MeanBest systems 0.73 0.88 0.57 0.61 0.71 0.70Baseline 0.43 0.30 0.45 0.39 0.59 0.43RI-TermAvg 0.44 0.71 0.50 0.42 0.65 0.54RI-TermHA 0.41 0.72 0.44 0.35 0.56 0.49MSRI-TermCentroid 0.45 0.73 0.50 0.33 0.64 0.53MSRI-TermHASenses 0.40 0.77 0.47 0.39 0.68 0.54MSRI-TermInContext 0.33 0.55 0.36 0.27 0.42 0.38MSRI-TermMaxSense 0.44 0.71 0.50 0.32 0.64 0.52Table 2: Pearson correlation scores per feature on STS 2012 test data using simple linear regressionFeature Headlines SMT FNWN OnWN MeanBest systems 0.78 0.40 0.58 0.84 0.65Baseline 0.54 0.29 0.21 0.28 0.33RI-TermAvg 0.60 0.37 0.21 0.52 0.42RI-TermHA 0.65 0.36 0.27 0.52 0.45MSRI-TermCentroid 0.60 0.35 0.37 0.45 0.44MSRI-TermHASenses 0.63 0.35 0.33 0.54 0.46MSRI-TermInContext 0.20 0.29 0.19 0.36 0.26MSRI-TermMaxSense 0.58 0.35 0.31 0.45 0.42Table 3: Pearson correlation scores per feature on STS 2013 test data using simple linear regressionNotably, the more advanced features used in ourexperiment, such as MSRI-TermInContext,gave very clearly inferior results when comparedto MSRI-TermHASenses.
This suggests thatmore research on MSRI is needed to understandhow both training and retrieval can be fully uti-lized and optimized.6 Conclusion and Future WorkThe paper introduced a new method called Multi-Sense Random Indexing (MSRI), which is basedon Random Indexing and performs on-the-flyclustering, as an efficient way to construct multi-prototype distributional models for word similar-ity.
A number of alternative measures for wordsimilarity were proposed, both context-dependentand context-independent, including new measuresbased on optimal alignment of word senses us-ing the Hungarian algorithm.
An extrinsic eval-uation was carried out by applying the resultingmodels to the Semantic Textual Similarity task.Initial experimental results did not show a sys-tematic difference between single-prototype andmulti-prototype models in this task.There are many questions left for future work.One of them is how the number of senses per wordevolves during training and how the distributionof senses in the final model looks like.
So far weonly know that on average the number of senseskeeps growing with more training material, cur-rently resulting in about 5 senses per word at theend of training (after removing senses with fre-quency below the sense-frequency threshold).
Itis worth noting that this depends heavily on thesimilarity threshold for merging senses, as well ason the weighting schema used.In addition there are a number of model para-meters that have so far only been manually tunedon the development data, such as window size,number of non-zeros, vector dimensionality, andthe sense frequency filtering threshold.
A system-atic exploration of the parameter space is clearlydesirable.
Another thing that would be worthlooking into, is how to compose sentence vectorsand document vectors from the multi-sense vectorspace in a proper way, focusing on how to pickthe right senses and how to weight these.
It wouldalso be interesting to explore the possibilities forcombining the MSRI method with the ReflectiveRandom Indexing method by Cohen et al(2010)in an attempt to model higher order co-occurrencerelations on sense level.The fact that the induced dynamic word sensesdo not necessarily correspond to human-createdsenses makes evaluation in traditional word sensedisambiguation tasks difficult.
However, correla-88tion to human word similarity judgement may pro-vide a way of intrinsic evaluation of the models(Reisinger and Mooney, 2010).
The Usim benchmark data look promising for evaluation of wordsimilarity in context (Erk et al 2013).It is also worth exploring ways to optimise thealgorithm, as this has not been the focus of ourwork so far.
This would also allow faster trainingand experimentation on larger text corpora, suchas Wikipedia.
In addition to the JavaSDM pack-age (Hassel, 2004), Lucene (Apache, 2005) withthe Semantic Vectors package (Widdows and Fer-raro, 2008) would be an alternative framework forimplementing the proposed MSRI algorithm.AcknowledgementsThis work was partly supported by the Re-search Council of Norway through the EviCareproject (NFR project no.
193022) and by theEuropean Community?s Seventh Framework Pro-gramme (FP7/20072013) under grant agreementnr.
248307 (PRESEMT).
Part of this work hasbeen briefly described in our contribution to theSTS shared task (Marsi et al 2013).ReferencesEneko Agirre, Daniel Cer, Mona Diab, and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6: Apilot on semantic textual similarity.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics (*SEM), volume 2: Proceed-ings of the Sixth International Workshop on Seman-tic Evaluation, pages 385?393, Montreal, Canada,June.
Association for Computational Linguistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic textual similarity.
In Second JointConference on Lexical and Computational Seman-tics (*SEM), volume 1: Proceedings of the MainConference and the Shared Task: Semantic TextualSimilarity, pages 32?43, Atlanta, Georgia, June.
As-sociation for Computational Linguistics.Apache.
2005.
Apache Lucene open source package.http://lucene.apache.org/.Trevor Cohen, Roger Schvaneveldt, and Dominic Wid-dows.
2010.
Reflective random indexing and indi-rect inference: A scalable method for discovery ofimplicit connections.
Journal of Biomedical Infor-matics, 43(2):240?256, April.Scott Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard Harshman.1990.
Indexing by latent semantic analysis.
Jour-nal of the American Society for Information Science,41(6):391?407.Georgiana Dinu and Mirella Lapata.
2010.
Measur-ing distributional similarity in context.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 1162?1172,Cambridge, Massachusetts, October.
Association forComputational Linguistics.Katrin Erk, Diana McCarthy, and Nicholas Gaylord.2013.
Measuring word meaning in context.
Com-putational Linguistics, 39(3):501?544.Martin Hassel.
2004.
JavaSDM package.
http://www.nada.kth.se/?xmartin/java/.School of Computer Science and Communication;Royal Institute of Technology (KTH); Stockholm,Sweden.Eric H. Huang, Richard Socher, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Improving wordrepresentations via global context and multiple wordprototypes.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers - Volume 1, ACL ?12, pages 873?882, Jeju Island, Korea.
Association for Computa-tional Linguistics.Pentti Kanerva, Jan Kristoferson, and Anders Holst.2000.
Random indexing of text samples for latentsemantic analysis.
In Proceedings of the 22nd An-nual Conference of the Cognitive Science Society,page 1036, Philadelphia, Pennsylvania.
Erlbaum.Adam Kilgarriff.
2000.
I don?t believe in word senses.Computers and the Humanities, 31(2):91?113.Harold W. Kuhn.
1955.
The Hungarian method forthe assignment problem.
Naval Research LogisticsQuarterly, 2:83?97.Edwin Lughofer.
2008.
Extensions of vector quantiza-tion for incremental clustering.
Pattern Recognition,41(3):995?1011, March.Erwin Marsi, Hans Moen, Lars Bungum, Gleb Sizov,Bjo?rn Gamba?ck, and Andre?
Lynum.
2013.
NTNU-CORE: Combining strong features for semantic sim-ilarity.
In Second Joint Conference on Lexical andComputational Semantics (*SEM), volume 1: Pro-ceedings of the Main Conference and the SharedTask: Semantic Textual Similarity, pages 66?73, At-lanta, Georgia, June.
Association for ComputationalLinguistics.Roberto Navigli.
2009.
Word Sense Disambiguation:a survey.
ACM Computing Surveys, 41(2):1?69.Carol Peters, Paul Clough, Julio Gonzalo, Gareth J.F.Jones, Michael Kluck, and Bernardo Magnini, ed-itors.
2004.
Multilingual Information Accessfor Text, Speech and Images, 5th Workshop of theCross-Language Evaluation Forum, CLEF 2004,volume 3491 of Lecture Notes in Computer Science.Springer-Verlag, Bath, England.89Joseph Reisinger and Raymond J. Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In Human Language Technologies: The 2010Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 109?117, Los Angeles, California, June.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of the1st International Conference on New Methods inNatural Language Processing, pages 44?49, Univer-sity of Manchester Institute of Science and Technol-ogy, Manchester, England, September.Hinrich Schu?tze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?123, March.Artem Sokolov.
2012.
LIMSI: learning semanticsimilarity by selecting random word subsets.
InProceedings of the First Joint Conference on Lexi-cal and Computational Semantics (*SEM), volume2: Proceedings of the Sixth International Workshopon Semantic Evaluation, pages 543?546, Montreal,Canada, June.
Association for Computational Lin-guistics.Tim Van de Cruys, Thierry Poibeau, and Anna Korho-nen.
2011.
Latent vector weighting for word mean-ing in context.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1012?1022, Edinburgh, Scotland,July.
Association for Computational Linguistics.Dominic Widdows and Kathleen Ferraro.
2008.
Se-mantic vectors: a scalable open source package andonline technology management application.
In Pro-ceedings of the Sixth International Language Re-sources and Evaluation (LREC?08), pages 1183?1190, Marrakech, Morocco.90
