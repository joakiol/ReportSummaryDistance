Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 303?308,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsOpen IE as an Intermediate Structure for Semantic TasksGabriel Stanovsky?Ido Dagan?Mausam?
?,?Department of Computer Science, Bar-Ilan University?Department of Computer Science & Engg, Indian Institute of Technology, Delhi?gabriel.satanovsky@gmail.com?dagan@cs.biu.ac.il?mausam@cse.iitd.ac.inAbstractSemantic applications typically extract in-formation from intermediate structures de-rived from sentences, such as dependencyparse or semantic role labeling.
In this pa-per, we study Open Information Extrac-tion?s (Open IE) output as an additional in-termediate structure and find that for taskssuch as text comprehension, word similar-ity and word analogy it can be very effec-tive.
Specifically, for word analogy, OpenIE-based embeddings surpass the state ofthe art.
We suggest that semantic applica-tions will likely benefit from adding OpenIE format to their set of potential sentence-level structures.1 IntroductionSemantic applications, such as QA or summa-rization, typically extract sentence features froma derived intermediate structure.
Common in-termediate structures include: (1) Lexical repre-sentations, in which features are extracted fromthe original word sequence or the bag of words,(2) Stanford dependency parse trees (De Marneffeand Manning, 2008), which draw syntactic rela-tions between words, and (3) Semantic role label-ing (SRL), which extracts frames linking predi-cates with their semantic arguments (Carreras andM`arquez, 2005).
For instance, a QA applicationcan evaluate a question and a candidate answerby examining their lexical overlap (P?erez-Couti?noet al, 2006), by using short dependency paths asfeatures to compare their syntactic relationships(Liang et al, 2013), or by using SRL to comparetheir predicate-argument structures (Shen and La-pata, 2007).In a seemingly independent research direction,Open Information Extraction (Open IE) extractscoherent propositions from a sentence, each com-prising a relation phrase and two or more argumentphrases (Etzioni et al, 2008; Fader et al, 2011;Mausam et al, 2012).
We observe that while OpenIE is primarily used as an end goal in itself (e.g.,(Fader et al, 2014)), it also makes certain struc-tural design choices which differ from those madeby dependency or SRL.
For example, Open IEchooses different predicate and argument bound-aries and assigns different relations between them.Given the differences between Open IE andother intermediate structures (see Section 2), a re-search question arises: Can certain downstreamapplications gain additional benefits from utiliz-ing Open IE structures?
To answer this questionwe quantitatively evaluate the use of Open IE out-put against other dominant structures (Sections 3and 4).
For each of text comprehension, wordsimilarity and word analogy tasks, we choose astate-of-the-art algorithm in which we can easilyswap the intermediate structure while preservingthe algorithmic computations over the features ex-tracted from it.
We find that in several tasks OpenIE substantially outperforms other structures, sug-gesting that it can provide an additional set of use-ful sentence-level features.2 Intermediate StructuresIn this section we review how intermediate struc-tures differ from each other, in terms of their im-posed structure, predicate and argument bound-aries, and the type of relations that they introduce.We include Open IE in this analysis, along withlexical, dependency and SRL representations, andhighlight its unique properties.
As we show inSection 4, these differences have an impact on theoverall performance of certain downstream appli-cations.Lexical representations introduce little or nostructure over the input text.
Features for follow-ing computations are extracted directly from theoriginal word sequence, e.g., word count statisticsor lexical overlap (see Figure 1a).303Syntactic dependencies impose a tree structure(see Figure 1b), and use words as atomic elements.This structure implies that predicates are generallycomposed of a single word and that arguments arecomputed either as single words or as entire spansof subtrees subordinate to the predicate word.In SRL (see Figure 1c), several non-connectedframes are extracted from the sentence.
Theatomic elements of each frame consist of a single-word predicate (e.g., the different frames for visitand refused), and a list of its semantic arguments,without marking their internal structure.
Each ar-gument is listed along with its semantic relation(e.g., agent, instrument, etc.)
and usually spansseveral words.Open IE (see Figure 1d) also extracts non-connected propositions, consisting of a predicateand its arguments.
In contrast to SRL, argumentrelations are not analyzed, and predicates (as wellas arguments) may consist of several consecu-tive words.
Since Open IE focuses on human-readability, infinitive constructions (e.g., refusedto visit), and multi-word predicates (e.g., took ad-vantage) are grouped in a single predicate slot.Additionally, arguments are truncated in casessuch as prepositional phrases and reduced rela-tive clauses.
The resulting structure can be under-stood as an extension of shallow syntactic chunk-ing (Abney, 1992), where chunks are labeled aseither predicates or arguments, and are then inter-linked to form a complete proposition.It is not clear apriory whether the differencesmanifested in Open IE?s structure could be ben-eficial as intermediate structures for downstreamapplications.
Although a few end tasks have madeuse of Open IE?s output (Christensen et al, 2013;Balasubramanian et al, 2013), there has been nosystematic comparison against other structures.
Inthe following sections, we quantitatively study andanalyze the value of Open IE structures againstthe more common intermediate structures ?
lexi-cal, dependency and SRL, for three downstreamNLP tasks.3 Tasks and AlgorithmsComparing the effectiveness of intermediate struc-tures in semantic applications is hard for severalreasons: (1) extracting the underlying structure de-pends on the accuracy of the specific system used,(2) the overall performance in the task dependsheavily on the computations carried on top of theseS: John refused to visit a Vegas casinoCA: John visited a Vegas casino(a) Lexical matching of a 5 words window (marked with a box).Current window yields a score of 4 - words contributing to thescore are marked in bold.
(b) Dependency matching yields a score of 3.
Contributingtriplets are marked in bold.S: refused0.1: A0: John A1: to visit a Vegas casinovisit0.1: A0: John A1: a Vegas casinoCA: visit0.1: A0: John A1: a Vegas casino(c) SRL frames matching yields a score of 4, frame elementscontributing to the score marked in bold.S: (John, refused to visit, a Vegas casino)CA: (John, visited, a Vegas casino)(d) Open IE matching yields a score of 2, contributing entriesmarked in bold.Figure 1: Different intermediate structures used tocompute the modified text comprehension match-ing score (Section 3), when answering a question?Where did John visit?
?, given an input sentenceS: ?John refused to visit a Vegas casino?, and awrong candidate answer CA: ?John visited a Ve-gas casino?.structures, and (3) different structures may be suit-able for different tasks.
To mitigate these com-plications, and comparatively evaluate the effec-tiveness of different types of structures, we choosethree semantic tasks along with state-of-the-art al-gorithms that make a clear separation between fea-ture extraction and subsequent computation.
Wethen compare performance by using features fromfour intermediate structures ?
lexical, dependency,SRL and Open IE.
Each of these is extracted usingstate-of-the-art systems.
Thus, while our compar-isons are valid only for the tested tasks and sys-tems, they do provide valuable evidence for thegeneral question of effective intermediate struc-tures.3.1 Text Comprehension TaskText comprehension tasks extrinsically test naturallanguage understanding through question answer-304Target Lexical Dependency SRL Open IErefusedJohn nsubj John A0 John 0 Johnto xcomp visit A1 to 1 tovisit A1 visit 1 visitVegas A1 Vegas 2 VegasTable 1: Some of the different contexts for the tar-get word ?refused?
in the sentence ?John refusedto visit Vegas?.
SRL and Open IE contexts are pre-ceded by their element (predicate or argument) in-dex.
See figure 1 for the different representationsof this sentence.ing.
We use the MCTest corpus (Richardson etal., 2013), which is composed of short stories fol-lowed by multiple choice questions.
The MCTesttask does not require extensive world knowledge,which makes it ideal for testing underlying sen-tence representations, as performance will mostlydepend on accuracy and informativeness of the ex-tracted structures.We adapt the unsupervised lexical matchingalgorithm from the original MCTest paper.
Itcounts lexical matches between an assertion ob-tained from a candidate answer (CA) and a slidingwindow over the story.
The selected answer is theone for which the maximum number of matchesare found.
Our adaptation changes the algorithmto compute a modified matching score by countingmatches between structure units.
The correspond-ing units are either dependency edges, SRL frameelements or Open IE tuple elements.
Figure 1 il-lustrates computations for a sentence - candidateanswer pair.3.2 Similarity and Analogy TasksWord similarity tasks deal with assessing the de-gree of ?similarity?
between two input words.
Tur-ney (2012) classifies two types of similarity: (1)domain similarity, e.g., carpenter is similar towood, hammer, and nail, (2) functional similarity,in which carpenter will be similar to other profes-sions, e.g., shoemaker, brewer, miner etc.
Severalevaluation test sets exist for this task, each target-ing a slightly different aspect of similarity.
WhileBruni (2012), Luong (2013), Radinsky (2011),and ws353 (Finkelstein et al, 2001) can be largelycategorized as targeting domain similarity, sim-lex999 (Hill et al, 2014) specifically targets func-tional aspects of similarity (e.g., coast will be sim-ilar to shore, while closet will not be similar toclothes).
A related task is word analogy, in whichsystems take three input words (A:A?, B:?)
andoutput a word B?, such that the relation betweenB and B?is closest to the relation between A andA?.
For instance, queen is the desired answer forthe triple (man:king, woman:?
).Some recent state-of-the-art approaches to thesetwo tasks derive a similarity score via arithmeticcomputations on word embeddings (Mikolov etal., 2013b).
While original training of word em-beddings used lexical contexts (n-grams), recentlyLevy and Goldberg (2014) generalized this to ar-bitrary contexts, such as dependency paths.
Weuse their software1and recompute the word em-beddings using contexts from our four structures:lexical context, dependency paths, SRL?s seman-tic relations, and Open IE?s surrounding tuple ele-ments.
Table 1 shows the different contexts for asample word.4 EvaluationIn our experiments we use MaltParser (Nivre etal., 2007) for dependency parsing, and ClearNLP(Choi and Palmer, 2011) for SRL.To obtain Open-IE structures, we use the re-cent Open IE-4 system2which produces n-ary ex-tractions of both verb-based relation phrases usingSRLIE (an improvement over (Christensen et al,2011)) and nominal relations using regular expres-sions.
SRLIE first processes sentences using SRLand then uses hand-coded rules to convert SRLframes and associated dependency parses to openextractions.We choose these tools as they are on par withstate-of-the-art in their respective fields, and there-fore represent the current available off-the-shelfintermediate structures for semantic applications.Furthermore, Open IE-4 is based on ClearNLP?sSRL, allowing for a direct comparison.
For SRLsystems, we take argument boundaries as theircomplete parse subtrees.3Results on Text Comprehension Task We re-port results (in percentage of correct answers) onthe whole of MC500 dataset (ignoring train-dev-test split) since all our methods are unsupervised.Figure 2 shows the accuracies obtained on themultiple-choice questions, categorized by single(the question can be answered based on a sin-1https://bitbucket.org/yoavgo/word2vecf2http://knowitall.github.io/openie/3We tried an alternative approach which takes only theheads as arguments, but that performed much worse.305Open IE Lexical Deps SRLbruni .757 .735 .618 .491luong .288 .229 .197 .171radinsky .681 .674 .592 .433simlex .39 .365 .447 .306ws353-rel .647 .64 .492 .551ws353-sym .77 .763 .759 .439ws353-full .711 .703 .629 .693Table 2: Performance in word similarity tasks(Spearman?s ?
)Google MSRAdd Mul Add MulOpen IE .714 .719 .529 .55Lexical .651 .656 .438 .455Deps .34 .367 .4 .434SRL .352 .362 .389 .406Table 3: Performance in word analogy tasks (per-centage of correct answers)gle story sentence) , multiple (multiple sentencesneeded) and all (single + multiple).4In this task, we find that Open IE and depen-dency edges substantially outperform lexical andSRL.
We conjecture that SRL?s weak performanceis due to its treatment of infinitives and multi-wordpredicates as different propositions (see Section2).
This adds noise by wrongly counting partialmatching between predications, as exemplified inFigure 1c.
The gain over the lexical approachcan be explained by the ability to capture longerrange relations than the fixed size window.5Inour results Open IE slightly improves over depen-dency.
This can be traced back to the differentstructural choices depicted in Section 2 ?
OpenIE counts matches at the proposition level whilethe dependency variant may count path matchesover unrelated sentence parts.
The differences be-tween the performance of Open IE and all othersystems were found to be statistically significant(p < 0.01).Results on Similarity and Analogy Tasks Forthese tasks, we train the various word embeddings4As expected, all sentence-level intermediate structuresperform best on the single partition, yet results show thatsome of the questions from the multiple partition may also beanswered correctly using information from a single sentence.5We experimented with various window sizes and foundthat window size of the length of the current candidate-answer performed best.on a Wikipedia dump (August 2013 dump), con-taining 77.5M sentences and 1.5B tokens.
Weused the default hyperparameters from Levy andGoldberg (2014): 300 dimensions, skip gram withnegative sampling of size 5.
Lexical embeddingswere trained with 5-gram contexts.
Performanceis measured using Spearman?s ?, in order to assessthe correlation of the predictions to the gold anno-tations, rather than comparing their values directly.Table 2 compares the results on the word similar-ity task using cosine similarity between embed-dings as the similarity predictor.
For the ws353test set we report results on the whole corpus (full)as well as on the partition suggested by (Agirreet al, 2009) into relatedness (mainly meronym-holonym) and similarity (synonyms, antonyms, orhyponym-hypernym).We find that Open IE-based embeddings consis-tently do well; performing best across all test sets,except for simlex999.
Analysis reveals that OpenIE?s ability to represent multi-word predicates andarguments allows it to naturally incorporate bothnotions of similarity.
Context words originatingfrom the same Open IE slot (either predicate or ar-gument) are lexically close and indicate domain-similarity, whereas context words from other ele-ments in the tuple express semantic relationships,and target functional similarity.Thus, Open IE performs better on word-pairswhich exhibit both topical and functional similar-ity, such as (latinist, classicist), or (provincialism,narrow-mindedness), which were taken from theLuong test set.
Table 4 further illustrates this dualcapturing of both types of similarity in Open IEspace.Our results also reiterate previous findings ?lexical contexts do well on domain-similarity testsets (Mikolov et al, 2013b).
The results on thesimlex999 test set can be explained by its focuson functional similarity, previously identified asbetter captured by dependency contexts (Levy andGoldberg, 2014).For the Word analogy task we use the Google(Mikolov et al, 2013a) and the Microsoft cor-pora (Mikolov et al, 2013b), which are composedof ?
195K and 8K instances respectively.
Weobtain the analogy vectors using both the addi-tive and multiplicative measures (Mikolov et al,2013b; Levy and Goldberg, 2014).
Table 3 showsthe results ?
Open IE obtains the best accuraciesby vast margins (p < 0.01), for reasons simi-306Figure 2: Performance in MCTest (percentage ofcorrect answers).lar to the word similarity tasks.
To our knowl-edge, Open IE results on both analogy datasetssurpass the state of the art.
An example (fromthe Microsoft test set) which supports the observa-tion regarding Open IE embeddings space is (gen-tlest:gentler, loudest:?
), for which only Open IEanswers correctly as louder, while lexical respondwith higher-pitched (domain similar to loudest),and dependency with thinnest (functionally sim-ilar to loudest).
Our Open-IE embeddings arefreely available6and we note that these can serveas plug-in features for other NLP applications, asdemonstrated in (Turian et al, 2010).5 ConclusionsWe studied Open IE?s output compared with otherdominant structures, highlighting their main dif-ferences.
We then conduct experiments and anal-ysis suggesting that these structural differencesprove beneficial for certain downstream semanticapplications.
A key strength is Open IE?s ability tobalance lexical proximity with long range depen-dencies in a single representation.
Specifically, forthe word analogy task, Open IE-based embeddings6http://www.cs.bgu.ac.il/?gabrielsTarget Word Lexical Dependency Open IEcaninedog feline dogincisor bovine carnassialdentition equine felineparvovirus porcine fang-likedysplasia murine bovineTable 4: Closest words to canine in various wordembeddings.
Illustrating domain similarity (Lex-ical), functional similarity (Dependency), and amixture of both (Open IE).surpass all prior results.
We conclude that an NLPpractitioner will likely benefit from adding OpenIE to their toolkit of potential sentence representa-tions.AcknowledgmentsThis work was partially supported by the IsraelScience Foundation grant 880/12, the GermanResearch Foundation through the German-IsraeliProject Cooperation (DIP, grant DA 1600/1-1),a Google Research Award to Ido Dagan, andGoogle?s Language Understanding and Knowl-edge Discovery Focused Research Award toMausam.ReferencesSteven P Abney.
1992.
Parsing by chunks.
Principle-based parsing, pages 257?278.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 19?27.
Association for Computational Lin-guistics.Niranjan Balasubramanian, Stephen Soderland,Mausam, and Oren Etzioni.
2013.
Generatingcoherent event schemas at scale.
In Conferenceon Empirical Methods in Natural LanguageProcessing, pages 1721?1731.Elia Bruni, Gemma Boleda, Marco Baroni, and Nam-Khanh Tran.
2012.
Distributional semantics in tech-nicolor.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics: Long Papers-Volume 1, pages 136?145.
Asso-ciation for Computational Linguistics.Xavier Carreras and Llu?
?s M`arquez.
2005.
Introduc-tion to the conll-2005 shared task: Semantic rolelabeling.
In Proceedings of The SIGNLL Confer-ence on Computational Natural Language Learning(CoNLL), pages 152?164.Jinho D Choi and Martha Palmer.
2011.
Transition-based semantic role labeling using predicate argu-ment clustering.
In Proceedings of the ACL 2011Workshop on Relational Models of Semantics, pages37?45.
Association for Computational Linguistics.Janara Christensen, Mausam, Stephen Soderland, andOren Etzioni.
2011.
An analysis of open informa-tion extraction based on semantic role labeling.
InProceedings of the 6th International Conference onKnowledge Capture (K-CAP ?11).307Janara Christensen, Mausam, Stephen Soderland, andOren Etzioni.
2013.
Towards coherent multi-document summarization.
In Conference of theNorth American Chapter of the Association forComputational Linguistics Human Language Tech-nologies, pages 1163?1173.Marie-Catherine De Marneffe and Christopher D Man-ning.
2008.
The stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation, pages 1?8.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S Weld.
2008.
Open information extrac-tion from the Web.
Communications of the ACM,51(12):68?74.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 1535?1545.
Association for ComputationalLinguistics.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated andextracted knowledge bases.
In The 20th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, KDD ?14, New York,NY, USA - August 24 - 27, 2014, pages 1156?1165.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2001.
Placing search in context: Theconcept revisited.
In Proceedings of the 10th inter-national conference on World Wide Web, pages 406?414.
ACM.Felix Hill, Roi Reichart, and Anna Korhonen.
2014.Simlex-999: Evaluating semantic models with(genuine) similarity estimation.
arXiv preprintarXiv:1408.3456.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for Computa-tional Linguistics, volume 2.Percy Liang, Michael I. Jordan, and Dan Klein.
2013.Learning dependency-based compositional seman-tics.
Computational Linguistics, 39(2):389?446.Minh-Thang Luong, Richard Socher, and Christo-pher D. Manning.
2013.
Better word representa-tions with recursive neural networks for morphol-ogy.
The SIGNLL Conference on ComputationalNatural Language Learning (CoNLL), 104.Mausam, Michael Schmitz, Stephen Soderland, RobertBart, and Oren Etzioni.
2012.
Open language learn-ing for information extraction.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 523?534, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word rep-resentations in vector space.
In Workshop at TheInternational Conference on Learning Representa-tions (ICLR).Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics Human Language Technologies,pages 746?751.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(02):95?135.Manuel P?erez-Couti?no, Manuel Montes-y G?omez, Au-relio L?opez-L?opez, and Luis Villase?nor-Pineda.2006.
The role of lexical features in Question An-swering for Spanish.
Springer.Kira Radinsky, Eugene Agichtein, EvgeniyGabrilovich, and Shaul Markovitch.
2011.
Aword at a time: computing word relatedness usingtemporal semantic analysis.
In Proceedings of the20th international conference on World wide web,pages 337?346.
ACM.Matthew Richardson, Christopher JC Burges, and ErinRenshaw.
2013.
Mctest: A challenge dataset forthe open-domain machine comprehension of text.
InConference on Empirical Methods in Natural Lan-guage Processing, pages 193?203.Dan Shen and Mirella Lapata.
2007.
Using semanticroles to improve question answering.
In EMNLP-CoNLL 2007, Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning, June 28-30, 2007, Prague, Czech Repub-lic, pages 12?21.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general methodfor semi-supervised learning.
In Proceedings of the48th annual meeting of the association for compu-tational linguistics, pages 384?394.
Association forComputational Linguistics.Peter D Turney.
2012.
Domain and function: A dual-space model of semantic relations and compositions.Journal of Artificial Intelligence Research, 44:533?585.308
