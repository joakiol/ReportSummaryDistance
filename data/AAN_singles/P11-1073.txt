Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 722?731,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsComputing and Evaluating Syntactic Complexity Features forAutomated Scoring of Spontaneous Non-Native SpeechMiao Chen Klaus ZechnerSchool of Information Studies NLP & Speech GroupSyracuse University Educational Testing ServiceSyracuse, NY, USA Princeton, NJ, USAmchen14@syr.edu kzechner@ets.orgAbstractThis paper focuses on identifying, extractingand evaluating features related to syntacticcomplexity of spontaneous spoken responses aspart of an effort to expand the current featureset of an automated speech scoring system inorder to cover additional aspects consideredimportant in the construct of communicativecompetence.Our goal is to find effective features, se-lected from a large set of features proposedpreviously and some new features designed inanalogous ways from a syntactic complexityperspective that correlate well with human rat-ings of the same spoken responses, and to buildautomatic scoring models based on the mostpromising features by using machine learningmethods.On human transcriptions with manuallyannotated clause and sentence boundaries, ourbest scoring model achieves an overall Pearsoncorrelation with human rater scores of r=0.49on an unseen test set, whereas correlations ofmodels using sentence or clause boundariesfrom automated classifiers are around r=0.2.1 IntroductionPast efforts directed at automated scoring ofspeech have used mainly features related to fluency (e.g., speaking rate, length and distribution ofpauses), pronunciation (e.g., using log-likelihoodscores from the acoustic model of an AutomaticSpeech Recognition (ASR) system), or prosody(e.g., information related to pitch  contours or syl-lable stress)  (e.g., Bernstein, 1999; Bernstein etal., 2000; Bernstein et al, 2010; Cucchiarini et al,1997; Cucchiarini et al, 2000; Franco et al, 2000a;Franco et al, 2000b; Zechner et al, 2007, Zechneret al, 2009).While this approach is a good match to most ofthe important properties related to low entropyspeech (i.e., speech which is highly predictable),such as reading a passage aloud, it lacks many im-portant aspects of spontaneous speech which arerelevant to be evaluated both by a human rater andan automated scoring system.
Examples of suchaspects of speech, which are considered part of theconstruct1 of ?communicative competence (Bach-man, 1990), include grammatical accuracy, syntac-tic complexity, vocabulary diversity, and aspects ofspoken discourse structure, e.g., coherence andcohesion.
These different aspects of speaking pro-ficiency are often highly correlated in a non-nativespeaker (Xi and Mollaun, 2006; Bernstein et al,2010), and so scoring models built solely on fea-tures of fluency and pronunciation may achievereasonably high correlations with holistic humanrater scores.
However, it is important to point outthat such systems would still be unable to assessmany important aspects of the speaking constructand therefore cannot be seen as ideal from a validi-ty point of view.2The purpose of this paper is to address one ofthese important aspects of spoken language inmore detail, namely syntactic complexity.
Thispaper can be seen as a first step toward including1  A construct is a set of knowledge, skills, and abilitiesmeasured by a test.2 ?Construct validity?
refers to the extent that a test measureswhat it is designed to measure, in this case, communicativecompetence via speaking.722features related to this part of the speaking con-struct into an already existing automated speechscoring system for spontaneous speech which sofar mostly uses features related to fluency and pro-nunciation (Zechner et al, 2009).We use data from the speaking section of theTOEFL?
Practice Online (TPO) test, which is alow stakes practice test for non-native speakerswhere they are asked to provide six spontaneousspeech samples of about one minute in length eachin response to a variety of prompts.
Some promptsmay be simple questions, and others may involvereading or listening to passages first and then ans-wering related questions.
All responses werescored holistically by human raters according topre-defined scoring rubrics (i.e., specific scoringguidelines) on a scale of 1 to 4, 4 being the highestproficiency level.In our automated scoring system, the first com-ponent is an ASR system that decodes the digitizedspeech sample, generating a time-annotated hypo-thesis for every response.
Next, fluency and pro-nunciation features are computed based on theASR output hypotheses, and finally a multiple re-gression scoring model, trained on human raterscores, computes the score for a given spoken re-sponse (see Zechner et al (2009) for more details).We conducted the study in three steps: (1) findingimportant measures of syntactic complexity fromsecond language acquisition (SLA) and Englishlanguage learning (ELL) literature, and extendingthis feature set based on our observations of theTPO data in analogous ways; (2) computing fea-tures based on transcribed speech responses andselecting features with highest correlations to hu-man rater scores, also considering their compara-tive values for native speakers taking the same test;and (3) building scoring models for the selectedsub-set of the features to generate a proficiencyscore for each speaker, using all six responses ofthat speaker.In the remainder of the paper, we will addressrelated work in syntactic complexity (Section 2),introduce the speech data sets of our study (Section3), describe the methods we used for feature ex-traction (Section 4), provide the experiment designand results (Section 5), analyze and discuss theresults in Section 6, before concluding the paper(Section 7).2 Related Work2.1 Literature on Syntactic ComplexitySyntactic complexity is defined as ?the range offorms that surface in language production and thedegree of sophistication of such forms?
(Ortega,2003).
It is an important factor in the second lan-guage assessment construct as described  in Bach-man?s (1990) conceptual model of languageability, and therefore is often used as an index oflanguage proficiency and development status of L2learners.
Various studies have proposed and inves-tigated measures of syntactic complexity as well asexamined its predictiveness for language profi-ciency, in both L2 writing and speaking settings,which will be reviewed respectively.WritingWolfe-Quintero et al (1998) reviewed a number ofgrammatical complexity measures in L2 writingfrom thirty-nine studies, and their usage for pre-dicting language proficiency was discussed.
Someexamples of syntactic complexity measures are:mean number of clauses per T-unit33 T-units are defined as ?shortest grammatically allowablesentences into which (writing can be split) or minimallyterminable units?
(Hunt, 1965:20)., mean lengthof clauses, mean number of verbs per sentence, etc.The various measures can be grouped into two cat-egories: (1) clauses, sentences, and T-units interms of each other; and (2) specific grammaticalstructures (e.g., passives, nominals) in relation toclauses, sentences, or T-units (Wolfe-Quintero etal., 1998).
Three primary methods of calculatingsyntactic complexity measures are frequency, ratio,and index, where frequency is the count of occur-rences of a specific grammatical structure, ratio isthe number of one type of unit divided by the totalnumber of another unit, and index is computingnumeric scores by specific formulae (Wolfe-Quintero et al, 1998).
For example, the measure?mean number of clauses per T-unit?
is obtainedby using the ratio calculation method and theclause and T-unit grammatical structures.
Somestructures such as clauses and T-units only needshallow linguistic processing to acquire, whilesome require parsing.
There are numerous combi-nations for measures and we need empirical evi-723dence to select measures with the highest perfor-mance.There have been a series of empirical studiesexamining the relationship of syntactic complexitymeasures to L2 proficiency using real-world data(Cooper, 1976; Larsen-Freeman, 1978; Perkins,1980; Ho-Peng, 1983; Henry, 1996; Ortega, 2003;Lu, 2010).
The studies investigate measures thathighly correlate with proficiency levels or distin-guish between different proficiency levels.
ManyT-unit related measures were identified as statisti-cally significant indicators to L2 proficiency, suchas mean length of T-unit (Henry, 1996; Lu, 2010),mean number of clauses per T-unit (Cooper, 1976;Lu, 2010), mean number of complex nominals perT-unit (Lu, 2010), or the mean number of error-free T-units per sentence (Ho-Peng, 1983).
Othersignificant measures are mean length of clause (Lu,2010), or frequency of passives in composition(Kameen, 1979).SpeakingSyntactic complexity analysis in speech mainlyinherits measures from the writing domain, and theabovementioned measures can be employed in thesame way on speech transcripts for complexitycomputation.
A series of studies have examinedrelations between the syntactic complexity ofspeech and the speakers?
holistic speaking profi-ciency levels (Halleck, 1995; Bernstein et al,2010; Iwashita, 2006).
Three objective measures ofsyntactic complexity, including mean T-unitlength, mean error-free T-unit length, and percentof error-free T-units were found to correlate withholistic evaluations of speakers in Halleck (1995).Iwashita?s (2006) study on Japanese L2 speakersfound that length-based complexity features (i.e.,number of T-units and number of clauses per T-unit) are good predictors for oral proficiency.
Instudies directly employing syntactic complexitymeasures in other contexts, ratio-based measuresare frequently used.
Examples are mean length ofutterance (Condouris et al, 2003), word count ortree depth (Roll et al, 2007), or mean length of T-units and mean number of clauses per T-unit(Bernstein et al, 2010).
Frequency-based measureswere used less, such as number of full phrases inRoll et al (2007).The speaking output is usually less clean thanwriting data (e.g., considering disfluencies such asfalse starts, repetitions, filled pauses etc.).
There-fore we may need to remove these disfluencies firstbefore computing syntactic complexity features.Also, importantly, ASR output does not containinterpunctuation but both for sentential-based fea-tures as well as for parser-based features, theboundaries of clauses and sentences need to beknown.
For this purpose, we will use automatedclassifiers that are trained to predict clause andsentence boundaries, as described in Chen et al(2010).
With previous studies providing us a richpool of complexity features, additionally we alsodevelop features analogous to the ones from theliterature, mostly by using different calculationmethods.
For instance, the frequency of Preposi-tional Phrases (PPs) is a feature from the literature,and we add some variants such as number of PPsper clause as a new feature to our extended featureset.2.2 Devising the Initial Feature SetThrough this literature review, we identified someimportant features that were frequently used inprevious studies in both L2 speaking and writing,such as length of sentences and number of clausesper sentence.
In addition, we also collected candi-date features that were less frequently mentionedin the literature, in order to start with a larger fieldof potential candidate features.
We further ex-tended the feature set by inspecting our data, de-scribed in the following section, and createdsuitable additional features by means of analogy.This process resulted in a set of 91 features, 11 ofwhich are related to clausal and sentential unitmeasurements (frequency-based) and 80 to mea-surements within such units (ratio-based).
Fromthe perspective of extracting measures, in our study,some measures can be computed using only clauseand sentence boundary information, and some canbe derived only if the spoken responses are syntac-tically parsed.
In our feature set, there are twotypes of features: clause and sentence boundarybased (26 in total) and parsing based (65).
The fea-tures will be described in detail in Section 4.3 DataOur data set contains (1) 1,060 non-native speechresponses of 189 speakers from the TPO test (NNset), and (2) 100 responses from 48 native speakersthat took the same test (Nat set).
All responseswere verbatim transcribed manually and scored724holistically by human raters.
(We only made use ofthe scores for the non-native data set in this study,since we purposefully selected speakers with per-fect or near perfect scores for the Nat set from alarger native speech data set.)
As mentioned above,there are four proficiency levels for human scoring,levels 1 to 4, with higher levels indicating betterspeaking proficiency.The NN set was randomly partitioned into atraining (NN-train) and a test set with 760 and 300responses, respectively, and no speaker overlap.DataSetRes-ponsesSpeakers Responses perSpeaker(average)NN-train760 137 5.55Description: used to train sentence andclause boundary detectors, evaluate fea-tures and train scoring models1:NN-test-1-Hum300 52 5.77Description: human transcriptions andannotations of sentence and clause boun-daries2:NN-test-2-CB300 52 5.77Description: human transcriptions, au-tomatically predicted clause boundaries3:NN-test-3-SB300 52 5.77Description: human transcriptions, au-tomatically predicted sentence bounda-ries4:NN-test-4-ASR-CB300 52 5.77Description: ASR hypotheses, automati-cally predicted clause boundaries5:NN-test-5-ASR-SB300 52 5.77Description: ASR hypotheses, automati-cally predicted sentence boundariesTable 1.
Overview of non-native data sets.A second version of the test set contains ASRhypotheses instead of human transcriptions.
Theword error rate (WER44 Word error rate (WER) is the ratio of errors from a stringbetween the ASR hypothesis and the reference transcript,where the sum of substitutions, insertions, and deletions is) on this data set is 50.5%.We used a total of five variants of the test sets, asdescribed in Table 1.
Sets 1-3 are based on humantranscriptions, whereas sets 4 and 5 are based onASR output.
Further, set 1 contains human anno-tated clause and sentence boundaries, whereas theother 4 sets have clause or sentence boundariespredicted by a classifier.All human transcribed files from the NN dataset were annotated for clause boundaries, clausetypes, and disfluencies by human annotators (seeChen et al (2010)).For the Nat data set, all of the 100 transcribedresponses were annotated in the same manner by ahuman annotator.
They are not used for any train-ing purposes but serve as a comparative referencefor syntactic complexity features derived from thenon-native corpus.The NN-train set was used both for trainingclause and sentence boundary classifiers, as well asfor feature selection and training of the scoringmodels.
The two boundary detectors were machinelearning based Hidden Markov Models, trained byusing a language model derived from the 760 train-ing files which had sentence and clause boundarylabels (NN-train; see also Chen et al (2010)).Since a speaker?s response to a single test itemcan be quite short (fewer than 100 words in manycases), it may contain only very few syntacticcomplexity features we are looking for.
(Note thatmuch of the previous work focused on written lan-guage with much longer texts to be considered.
)However, if we aggregate responses of a singlespeaker, we have a better chance of finding a largernumber of syntactic complexity features in the ag-gregated file.
Therefore we joined files from thesame speaker to one file for the training set and thefive test sets, resulting in 52 aggregated files ineach test set.
Accordingly, we averaged the re-sponse scores of a single speaker to obtain the totalspeaker score to be used later in scoring modeltraining and evaluation (Section 5).5While disfluencies were used for the training ofthe boundary detectors, they were removed after-wards from the annotated data sets to obtain a tran-divided by the length of the reference.
To obtain WER inpercent, this ratio is multiplied by 100.0.5 Although in most operational settings, features are derivedfrom single responses, this may not be true in all cases.Furthermore, scores of multiple responses are often combinedfor score reporting, which would make such an approacheasier to implement and argue for operationally.725scription which is ?cleaner?
and lends itself betterto most of the feature extraction methods we use.4 Feature Extraction4.1 Feature SetAs mentioned in Section 2, we gathered 91 candi-date syntactic complexity features based on ourliterature review as initial feature set, which isgrouped into two categories: (1) Clause and sen-tence Boundary based features (CB features); and(2) Parse Tree based features (PT features).
Clausebased features are based on both clause boundariesand clause types and can be generated from humanclause annotations, e.g., ?frequency of adjectiveclauses6We first selected features showing high correla-tion to human assigned scores.
In this process theCB features were computed from human labeledclause boundaries in transcripts for best accuracy,and PT features were calculated from using parsingand other tools because we did not have humanparse tree annotations for our data.per one thousand words?, ?mean numberof dependent clauses per clause?, etc.
Parse treebased features refer to features that are generatedfrom parse trees and cannot be extracted from hu-man annotated clauses directly.We used the Stanford Parser (Klein and Man-ning, 2003) in conjunction with the Stanford Tre-gex package (Levy and Andrew, 2006) whichsupports using rules to extract specific configura-tions from parse trees, in a package put together byLu (Lu, 2011).
When given a sentence, the Stan-ford Parser outputs its grammatical structure bygrouping words (and phrases) in a tree structureand identifies grammatical roles of words andphrases.Tregex is a tree query tool that takes Stanfordparser trees as input and queries the trees to findsubtrees that meet specific rules written in Tregexsyntax (Levy and Andrew, 2006).
It uses relationaloperators regulated by Tregex, for example, ?A <<B?
stands for ?subtree A dominates subtree B?.The operators primarily function in subtree prece-dence, dominance, negation, regular expression,tree node identity, headship, or variable groups,among others (Levy and Andrew, 2006).6 An adjective clause is a clause that functions as an adjectivein modifying a noun.
E.g., ?This cat is a cat that is difficult todeal with.
?Lu?s tool (Lu, 2011), built upon the StanfordParser and Tregex, does syntactic complexity anal-ysis given textual data.
Lu?s tool contributed 8 ofthe initial CB features and 6 of the initial PT fea-tures, and we computed the remaining CB and PTfeatures using Perl scripts, the Stanford Parser, andTregex.Table 2 lists the sub-set of 17 features (out of 91features total) that were used for building the scor-ing models described later (Section 5).4.2 Feature SelectionWe determined the importance of the features bycomputing each feature?s correlation with humanraters?
proficiency scores based on the training setNN-train.
We also used criteria related to thespeaking construct, comparisons with nativespeaker data, and feature inter-correlations.
Whileapproaches coming from a pure machine learningperspective would likely use the entire feature poolas input for a classifier, our goal here is to obtainan initial feature set by judicious and careful fea-ture selection that can withstand the scrutiny ofconstruct validity in assessment development.As noted earlier, the disfluencies in the training sethad been removed to obtain a ?cleaner?
text thatlooks somewhat more akin to a written passage andis easier to process by NLP modules such as pars-ers and part-of-speech (POS) taggers.
77 We are aware that disfluencies can provide valuable cluesabout spoken proficiency in and of themselves; however, thisstudy is focused exclusively on syntactic complexity analysis,and in this context, disfluencies would distort the pictureconsiderably due to the introduction of parsing errors, e.g.The ex-tracted features partly were taken directly fromproposals in the literature and partly were slightlymodified to fit our clause annotation scheme.
Inorder to have a unified framework for computingsyntactic complexity features, we used a combina-tion of the Stanford Parser and Tregex for compu-ting both clause- and sentence-based features aswell as parse-tree-based features, i.e., we did notmake use of the human clause boundary label an-notations here.
The only exception to this726is that we are using human clause and sentencelabels to create a candidate set for the clause boun-dary features evaluated by the Stanford Parser andTregex, as explained in the following subsection.8  Feature type: CB=Clause boundary based feature type,PT=Parse tree based feature type9A ?linguistically meaningful PP?
(PP_ling) is defined as a PPimmediately dominated by another PP in cases where apreposition contains a noun such as ?in spite of?
or ?in frontof?.
An example would be ?she stood in front of a house?where ?in front of a house?
would be parsed as two embeddedPPs but only the top PP would be counted in this case.10 A ?linguistically meaningful VP?
(VP_ling) is defined  as averb phrase immediately dominated by a clausal phrase, inorder to avoid VPs embedded in another VP, e.g., "should goto work" is identified as one VP instead of two embeddedVPs.11 The ?P-based Sampson?
is a raw production-based measure(Sampson, 1997), defined as "proportion of the daughters of anonterminal node which are themselves nonterminal andnonrightmost, averaged over the nonterminals of a sentence".Clause and Sentence based Features (CB fea-tures)Firstly, we extracted all 26 initial CB features di-rectly from human annotated data of NN-train, us-ing information from the clause and sentence typelabels.
The reasoning behind this was to create aninitial pool of clause-based features that reflectsthe distribution of clauses and sentences as accu-rately as possible, even though we did not plan touse this extraction method operationally, where theparser decides on clause and sentence types.
Aftercomputing the values of each CB feature, we cal-culated correlations between each feature and hu-man-rated scores.
Then we created an initial CBfeature pool by selecting features that met two cri-teria: (1) the absolute Pearson correlation coeffi-cient with human scores was larger than 0.2; and(2) the mean value of the feature on non-nativespeakers was at least 20% lower than that for na-Name Type8 Meaning  Correlation RegressionMLS CB Mean length of sentences 0.329 0.101MLT CB Mean length of T-units 0.300 -0.059DC/C CB Mean number of dependent clauses per clause 0.291 2.873SSfreq CB Frequency of simple sentences per 1000 words -.0242 0.001MLSS CB Mean length of simple sentences 0.255 0.040ADJCfreq CB Frequency of adjective clauses per 1000 words 0.253 0.004Ffreq CB Frequency of fragments per 1000 words -0.386 -0.057MLCC CB Mean length of coordinate clauses 0.224 0.017CT/T PT Mean number of complex T-units per T-unit 0.248 0.908PP_ling/S PT Mean number of linguistically meaningful prepositional phrases (PP) per sentence9 0.310  0.423NP/S PT Mean number of noun phrases (NP) per sentence 0.244 -0.411CN/S PT Mean number of complex nominal per sentence 0.325 0.653VB _ling/T PT Mean number of linguistically meaningful10 0.273   verb phrases per T-unit -0.780PAS/S PT Mean number of passives per sentence 0.260 1.520DI/T PT Mean number of dependent infinitives per T-unit 0.325 1.550MLev PT Mean number of parsing tree levels per sentence 0.306 -0.134MPSam PT Mean P-based Sampson11 0.254  per sentence 0.234Table 2.
List of syntactic complexity features selected to be included in building the scoring models.727tive speakers in case of positive correlation and atleast by 20% higher than for native speakers incase of negative correlation, using the Nat data setfor the latter criterion.
Note that all of these fea-tures were computed without using a parser.
Thisresulted in 13 important features.Secondly, Tregex rules were developed basedon Lu?s tool to extract these 13 CB features fromparsing results where the parser is provided withone sentence at a time.
By applying the same selec-tion criteria as before, except for allowing for cor-relations above 0.1 and giving preference tolinguistically more meaningful features, we found8 features that matched our criteria:MLS, MLT, DC/C, SSfreq, MLSS, ADJCfreq,Ffreq, MLCCAll 28 pairwise inter-correlations between these8 features were computed and inspected to avoidincluding features with high inter-correlations inthe scoring model.
Since we did not find any inter-correlations larger than 0.9, the features were con-sidered moderately independent and none of themwere removed from this set so it also maintainslinguistic richness for the feature set.Due to the importance of T-units in complexityanalysis, we briefly introduce how we obtain themfrom annotations.
Three types of clauses labeled inour transcript can serve as T-units, including sim-ple sentences, independent clauses, and conjunct(coordination) clauses.
These clauses were identi-fied in the human-annotated text and extracted asT-units in this phase.
T-units in parse trees areidentified using rules in Lu?s tool.Parse Tree based Features (PT features)We evaluated 65 features in total and selected fea-tures with highest importance using the followingtwo criteria (which are very similar as before): (1)the absolute Pearson correlation coefficient withhuman scores is larger than 0.2; and (2) the featuremean value on native speakers (Nat) is higher thanon score 4 for non-native speakers in case of posi-tive correlation, or lower for negative correlation.20 of 65 features were found to meet the require-ments.Next, we examined inter-correlations betweenthese features and found some correlations largerthan 0.85.12CT/T, PP_ling/S, NP/S, CN/S, VP_ling/T, PAS/S,DI/T, MLev, MPSamFor each feature pair exhibiting highinter-correlation, we removed one feature accord-ing to the criterion that the removed feature shouldbe linguistically less meaningful than the remain-ing one.
After this filtering, the 9 remaining PTfeatures are:In summary, as a result of the feature selectionprocess, a total of 17 features were identified asimportant features to be used in scoring models forpredicting speakers?
proficiency scores.
Amongthem 8 are clause boundary based and the other 9are parse tree based.5 Experiments and ResultsIn the previous section, we identified 17 syntacticfeatures that show promising correlations with hu-man rater speaking proficiency scores.
These fea-tures as well as the human-rated scores will beused to build scoring models by using machinelearning methods.
As introduced in Section 3, wehave one training set (N=137 speakers with all oftheir responses combined) for model building andfive testing sets (N=52 for each of them) for evalu-ation.The publicly available machine learning pack-age Weka was used in our experiments (Hall et al2009).
We experimented with two algorithms inWeka: multiple regression (called ?LinearRegres-sion?
in Weka) and decision tree (called ?M5P?inWeka).
The score values to be predicted are realnumbers (i.e., non-integer), because we have tocompute the average score of one speaker?s res-ponses.
Our initial runs showed that decision treemodels were consistently outperformed by mul-tiple regression (MR) models and thus decided toonly focus on MR models henceforth.We set the ?AttributeSelectionMethod?
parame-ter in Weka?s LinearRegression algorithm to all 3of its possible values in turn: (Model-1) M5 me-thod; (Model-2) no attribute selection; and (Model-3) greedy method.
The resulting three multiple re-gression models were then tested against the fivetesting sets.
Overall, correlations for all models forthe NN-test-1-Hum set were between 0.45 and0.49, correlations for sets NN-test-2-CB and NN-12 The reason for using a lower threshold than above was toobtain a roughly equal number of CB and PT features in theend.728test-3-SB (human transcript based, and using au-tomated boundaries) around 0.2, and for sets NN-test-4-ASR-CB  and NN-test-5-ASR-SB (ASR hy-potheses, and using automated boundaries), thecorrelations were not significant.
Model-2 (usingall 17 features) had the highest correlation on NN-test-1-Hum and we provide correlation results ofthis model in Table 3.Test setCorrelationcoefficientCorrelation significance(p < 0.05)NN-test-1-Hum 0.488 SignificantNN-test-2-CB 0.220 SignificantNN-test-3-SB 0.170 SignificantNN-test-4-ASR-CB -0.025 Not significantNN-test-5-ASR-SB -0.013 Not significantTable 3.
Multiple regression model testing results forModel-2.6 DiscussionAs we can see from the result table (Table 3) in theprevious section, using only syntactic complexityfeatures, based on clausal or parse tree informationderived from human transcriptions of spoken testresponses, can predict holistic human rater scoresfor combined speaker responses over a whole testwith an overall correlation of r=0.49.
While this isa promising result for this study with a focus on abroad spectrum of syntactic complexity features,the results also show significant limitations for animmediate operational use of such features.
First,the imperfect prediction of clause and sentenceboundaries by the two automatic classifiers causesa substantial degradation of scoring model perfor-mance to about r=0.2, and secondly, the rather higherror rate of the ASR system (50.5%) does not al-low for the computation of features that would re-sult in any significant correlation with humanscores.
We want to note here that while ASR sys-tems can be found that exhibit WERs below 10%for certain tasks, such as restricted dictation inlow-noise environments by native speakers, ourASR task is significantly harder in several ways:(1) we have to recognize non-native speak-ers?rresponses where speakers have a number ofdifferent native language backgrounds; (2) the pro-ficiency level of the test takers varies widely; and(3) the responses are spontaneous and uncon-strained in terms of vocabulary.As for the automatic clause and sentence boun-dary classifiers, we can observe (in Table 4) thatalthough the sentence boundary classifier has aslightly higher F-score than the clause boundaryclassifier, errors in sentence boundary detectionhave more negative effects on the accuracy ofscore prediction than those made by the clauseboundary classifier.
In fact, the lower F-score ofthe latter is mainly due to its lower precision whichindicates that there are more spurious clause boun-daries in its output which apparently cause littleharm to the feature extraction processes.Among the 17 final features, 3 of them are fre-quency-based and the remaining 14 are ratio-based, which mirrors our findings from previouswork that frequency features have been used lesssuccessfully than ratio features.
As for ratio fea-tures, 5 of them are grammatical structure countsagainst sentence units, 4 are counts against T-units,and only 1 is based on counts against clause units.The feature set covers a wide range of grammaticalstructures, such as T-units, verb phrases, nounphrases, complex nominals, adjective clauses,coordinate clauses, prepositional phrases, etc.While this wide coverage provides for richness ofthe construct of syntactic complexity, some of thefeatures exhibit relatively high correlation witheach other which reduces their overall contribu-tions to the scoring model?s performance.Going through the workflow of our system, wefind at least five major stages that can generateerrors which in turn can adversely affect featurecomputation and scoring model building.
Errorsmay appear in each stage of our workflow, passingor even enlarging their effects from previous stagesto later stages:1) grammatical errors by the speakers (test takers);2) errors by the ASR system;3) sentence/clause boundary detection errors;4) parser errors; and5) rule extraction errors.In future work we will need to address each er-ror source to obtain a higher overall system per-formance.729Table 4.
Performance of clause and sentence boundarydetectors.7 Conclusion and Future WorkIn this paper, we investigated associations betweenspeakers?
syntactic complexity features and theirspeaking proficiency scores provided by humanraters.
By exploring empirical evidence from non-native and native speakers?
data sets of spontane-ous speech test responses, we identified 17 featuresrelated to clause types and parse trees as effectivepredictors of human speaking scores.
The featureswere implemented based on Lu?s L2 SyntacticComplexity Analyzer toolkit (Lu, 2011) to be au-tomatically extracted from human or ASR tran-scripts.
Three multiple regression models werebuilt from non-native speech training data withdifferent parameter setup and were tested againstfive testing sets with different preprocessing steps.The best model used the complete set of 17 fea-tures and exhibited a correlation with humanscores of r=0.49 on human transcripts with boun-dary annotations.When using automated classifiers to predictclause or sentence boundaries, correlations withhuman scores are around r=0.2.
Our experimentsindicate that by enhancing the accuracy of the twomain automated preprocessing components, name-ly ASR and automatic sentence and clause boun-dary detectors, scoring model performance willincrease substantially, as well.
Furthermore, thisresult demonstrates clearly that syntactic complexi-ty features can be devised that are able to predicthuman speaking proficiency scores.Since this is a preliminary study, there is amplespace to improve all major stages in the featureextraction process.
The errors listed in the previoussection are potential working directions for prepro-cessing enhancements prior to machine learning.Among the five types of errors, we can work onimproving the accuracy of the speech recognizer,sentence and clause boundary detectors, parser,and feature extraction rules; as for the grammaticalerrors produced by test takers, we are envisioningto automatically identify and correct such errors.We will further experiment with syntactic com-plexity measures to balance construct richness andmodel simplicity.
Furthermore, we can also expe-riment with additional types of machine learningmodels and tune parameters to derive scoringmodels with better performance.AcknowledgementsThe authors wish to thank Lei Chen and Su-YounYoon for their help with the sentence and clauseboundary classifiers.
We also would like to thankour colleagues Jill Burstein, Keelan Evanini, YokoFutagi, Derrick Higgins, Nitin Madnani, and JoelTetreault, as well as the four anonymous ACL re-viewers for their valuable and helpful feedback andcomments on our paper.ReferencesBachman, L.F. (1990).
Fundamental considerations inlanguage testing.
Oxford: Oxford University Press.Bernstein, J.
(1999).
PhonePass testing: Structure andconstruct.
Menlo Park, CA: Ordinate Corporation.Bernstein, J., DeJong, J., Pisoni, D. & Townshend, B.(2000).
Two experiments in automatic scoring ofspoken language proficiency.
Proceedings of In-STILL 2000, Dundee, Scotland.Bernstein, J., Cheng, J., & Suzuki, M. (2010).
Fluencyand structural complexity as predictors of L2 oralproficiency.
Proceedings of Interspeech 2010, Tokyo,Japan, September.Chen, L., Tetreault, J.
& Xi, X.
(2010).
Towards usingstructural events to assess non-native speech.NAACL-HLT 2010.
5th Workshop on InnovativeUse of NLP for Building Educational Applications,Los Angeles, CA, June.Condouris, K., Meyer, E. & Tagger-Flusberg, H.(2003).
The relationship between standardized meas-ures of language and measures of spontaneous speechin children with autism.
American Journal of Speech-Language Pathology, 12(3), 349-358.Cooper, T.C.
(1976).
Measuring written syntactic pat-terns of second language learners of German.
TheJournal of Educational Research, 69(5), 176-183.Cucchiarini, C., Strik, H. & Boves, L. (1997).
Automat-ic evaluation of Dutch pronunciation by using speechrecognition technology.
IEEE Automatic SpeechRecognition and Understanding Workshop, SantaBarbara, CA.Classifier Accu-racyPreci-sionRe-callF scoreClause boundary 0.954 0.721 0.748 0.734Sentence boundary 0.975    0.811 0.755 0.782730Cucchiarini, C., Strik, H. & Boves, L. (2000).
Quantita-tive assessment of second language learners' fluencyby means of automatic speech recognition technolo-gy.
Journal of the Acoustical Society of America,107, 989-999.Franco, H., Abrash, V., Precoda, K., Bratt, H., Rao, R.& Butzberger, J.
(2000a).
The SRI EduSpeak system:Recognition and pronunciation scoring for languagelearning.
Proceedings of InSTiLL-2000 (IntelligentSpeech Technology in Language Learning), Dundee,Scotland.Franco, H., Neumeyer, L., Digalakis, V. & Ronen, O.(2000b).
Combination of machine scores for auto-matic grading of pronunciation quality.
SpeechCommunication, 30, 121-130.Hall, M., Frank, E., Holmes, G., Pfahringer, B.,Reutemann, P. &  Witten, I.H.
(2009).
The WEKAData Mining Software: An Update.
SIGKDD Explo-rations, 11(1).Halleck, G.B.
(1995).
Assessing oral proficiency: Acomparison of holistic and objective measures.
TheModern Language Journal, 79(2), 223-234.Henry, K. (1996).
Early L2 writing development: Astudy of autobiographical essays by university-levelstudents on Russian.
The Modern Language Journal,80(3), 309-326.Ho-Peng, L. (1983).
Using T-unit measures to assesswriting proficiency of university ESL students.RELC Journal, 14(2), 35-43.Hunt, K. (1965).
Grammatical structures written at threegrade levels.
NCTE Research report No.3.
Cham-paign, IL: NCTE.Iwashita, N. (2006).
Syntactic complexity measures andtheir relations to oral proficiency in Japanese as aforeign language.
Language Assessment Quarterly,3(20), 151-169.Kameen, P.T.
(1979).
Syntactic skill and ESL writingquality.
In C. Yorio, K. Perkins, & J.
Schachter(Eds.
), On TESOL ?79: The learner in focus (pp.343-364).
Washington, D.C.: TESOL.Klein, D. & Manning, C.D.
(2003).
Fast exact inferencewith a factored model for a natural language parsing.In S.Becker, S. Thrun & K. Obermayer (Eds.
), Ad-vances in Neural Information Processing Systems 15(pp.3-10).
Cambridge, MA: MIT Press.Larsen-Freeman, D. (1978).
An ESL index of develop-ment.
Teachers of English to Speakers of Other Lan-guages Quarterly, 12(4), 439-448.Levy, R. & Andrew, G. (2006).
Tregex and Tsurgeon:Tools for querying and manipulating tree data struc-tures.
Proceedings of the Fifth International Confe-rence on Language Resources and Evaluation.Lu, X.
(2010).
Automatic analysis of syntactic complex-ity in second language writing.
International Journalof Corpus Linguistics, 15(4), 474-496.Lu, X.
(2011).
L2 Syntactic Complexity Analyzer.
Re-trieved fromhttp://www.personal.psu.edu/xxl13/downloads/l2sca.htmlOrtega, L. (2003).
Syntactic complexity measures andtheir relationship to L2 proficiency: A research syn-thesis of college-level L2 writing.
Applied Linguis-tics, 24(4), 492-518.Perkins, K. (1980).
Using objective methods of attainedwriting proficiency to discriminate among holisticevaluations.
Teachers of English to Speakers of Oth-er Languages Quarterly, 14(1), 61-69.Roll, M., Frid, J.
& Horne, M. (2007).
Measuring syn-tactic complexity in spontaneous spoken Swedish.Language and Speech, 50(2), 227-245.Sampson, G. (1997).
Depth in English grammar.
Journalof Linguistics, 33, 131-151.Wolfe-Quintero, K., Inagaki, S. & Kim, H. Y.
(1998).Second language development in writing: Measuresof fluency, accuracy, & complexity.
Honolulu, HI:University of Hawaii Press.Xi, X., & Mollaun, P. (2006).
Investigating the utilityof analytic scoring for the TOEFL?
AcademicSpeaking Test (TAST).
TOEFL iBT Research Re-port No.
TOEFLiBT-01.Zechner, K., Higgins, D. & Xi, X.
(2007).
SpeechRa-ter(SM): A construct-driven approach to score spon-taneous non-native speech.
Proceedings of the 2007Workshop of the International Speech Communica-tion Association (ISCA) Special Interest Group onSpeech and Language Technology in Education(SLaTE), Farmington, PA, October.Zechner, K., Higgins, D., Xi, X, & Williamson, D.M.(2009).
Automatic scoring of non-native spontaneousspeech in tests of spoken English.
Speech Communi-cation, 51 (10), October.731
