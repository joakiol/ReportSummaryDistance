Chinese Verb Sense Discrimination Using an EM Clustering Model with RichLinguistic FeaturesJinying Chen, Martha PalmerDepartment of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA, 19104{jinying,mpalmer}@linc.cis.upenn.eduAbstractThis paper discusses the application of theExpectation-Maximization (EM) clusteringalgorithm to the task of Chinese verb sensediscrimination.
The model utilized richlinguistic features that capture predicate-argument structure information of the targetverbs.
A semantic taxonomy for Chinesenouns, which was built semi-automaticallybased on two electronic Chinese semanticdictionaries, was used to provide semanticfeatures for the model.
Purity and normalizedmutual information were used to evaluate theclustering performance on 12 Chinese verbs.The experimental results show that the EMclustering model can learn sense or sensegroup distinctions for most of the verbssuccessfully.
We further enhanced the modelwith certain fine-grained semantic categoriescalled lexical sets.
Our results indicate thatthese lexical sets improve the model?sperformance for the three most challengingverbs chosen from the first set of experiments.1 IntroductionHighly ambiguous words may lead to irrelevantdocument retrieval and inaccurate lexical choice inmachine translation (Palmer et al, 2000), whichsuggests that word sense disambiguation (WSD) isbeneficial and sometimes even necessary in suchNLP tasks.
This paper addresses WSD in Chinesethrough developing an Expectation-Maximization(EM) clustering model to learn Chinese verb sensedistinctions.
The major goal is to do sensediscrimination rather than sense labeling, similar to(Sch?tze, 1998).
The basic idea is to divideinstances of a word into several clusters that haveno sense labels.
The instances in the same clusterare regarded as having the same meaning.
Wordsense discrimination can be applied to documentretrieval and similar tasks in information access,and to facilitating the building of large annotatedcorpora.
In addition, since the clustering model canbe trained on large unannotated corpora andevaluated on a relatively small sense-taggedcorpus, it can be used to find indicative features forsense distinctions through exploring huge amountof available unannotated text data.The EM clustering algorithm (Hofmann andPuzicha, 1998) used here is an unsupervisedmachine learning algorithm that has been appliedin many NLP tasks, such as inducing asemantically labeled lexicon and determininglexical choice in machine translation (Rooth et al,1998), automatic acquisition of verb semanticclasses (Schulte im Walde, 2000) and automaticsemantic labeling (Gildea and Jurafsky, 2002).
Inour task, we equipped the EM clustering modelwith rich linguistic features that capture thepredicate-argument structure information of verbsand restricted the feature set for each verb usingknowledge from dictionaries.
We also semi-automatically built a semantic taxonomy forChinese nouns based on two Chinese electronicsemantic dictionaries, the Hownet dictionary1 andthe Rocling dictionary.2 The 7 top-level categoriesof this taxonomy were used as semantic featuresfor the model.
Since external knowledge is used toobtain the semantic features and guide featureselection, the model is not completelyunsupervised from this perspective; however, itdoes not make use of any annotated training data.Two external quality measures, purity andnormalized mutual information (NMI) (Strehl.2002), were used to evaluate the model?sperformance on 12 Chinese verbs.
Theexperimental results show that rich linguisticfeatures and the semantic taxonomy are both veryuseful in sense discrimination.
The modelgenerally performs well in learning sense groupdistinctions for difficult, highly polysemous verbsand sense distinctions for other verbs.
Enhanced bycertain fine-grained semantic categories calledlexical sets (Hanks, 1996), the model?s1 http://www.keenage.com/.2 A Chinese electronic dictionary liscenced from TheAssociation for Computational Linguistics and ChineseLanguage Processing (ACLCLP), Nankang, Taipei,Taiwan.performance improved in a preliminary experimentfor the three most difficult verbs chosen from thefirst set of experiments.The paper is organized as follows: we brieflyintroduce the EM clustering model in Section 2and describe the features used by the model inSection 3.
In Section 4, we introduce a semantictaxonomy for Chinese nouns, which is built semi-automatically for our task but can also be used inother NLP tasks such as co-reference resolutionand relation detection in information extraction.We report our experimental results in Section 5and conclude our discussion in Section 6.2 EM Clustering ModelThe basic idea of our EM clustering approach issimilar to the probabilistic model of co-occurrencedescribed in detail in (Hofmann and Puzicha1998).
In our model, we treat a set of features { }mfff ,...,, 21 , which are extracted from theparsed sentences that contain a target verb, asobserved variables.
These variables are assumed tobe independent given a hidden variable c, the senseof the target verb.
Therefore the joint probability ofthe observed variables (features) for each verbinstance, i.e., each parsed sentence containing thetarget verb, is defined in equation (1),?
?==cmiim cfpcpfffp121 )|()(),...,,(           (1)The if ?s are discrete-valued features that cantake multiple values.
A typical feature used in ourmodel is shown in (2),=if???????
(2)At the beginning of training (i.e., clustering), themodel?s parameters )(cp  and )|( cfp i  arerandomly initialized.3 Then, the probability of cconditioned on the observed features is computedin the expectation step (E-step), using equation (3),?
?
?===cmiimiimcfpcpcfpcpfffcp1121)|()()|()(),...,,|(~   (3)3 In our experiments, for verbs with more than 3senses, syntactic and semantic restrictions derived fromdictionary entries are used to constrain the randominitialization.In the maximization step (M-step), )(cp  and)|( cfp i  are re-computed by maximizing the log-likelihood of all the observed data which iscalculated by using ),...,,|(~ 21 mfffcp  estimatedin the E-step.
The E-step and M-step are repeatedfor a fixed number of rounds, which is set to 20 inour experiments,4 or till the amount of change of)(cp  and )|( cfp i  is under the threshold 0.001.When doing classification, for each verbinstance, the model calculates the same conditionalprobability as in equation (3) and assigns theinstance to the cluster with the maximal),...,,|( 21 mfffcp .3 Features Used in the ModelThe EM clustering model uses a set of linguisticfeatures to capture the predicate-argumentstructure information of the target verbs.
Thesefeatures are usually more indicative of verb sensedistinctions than simple features such as wordsnext to the target verb or their POS tags.
Forexample, the Chinese verb ?
?| chu1?
has a senseof produce, the distinction between this sense andthe verb?s other senses, such as happen and go out,largely depends on the semantic category of theverb?s direct object.
Typical examples are shownin (1),(1 ??
?
?
??)
a.
/their  /county  /produce  /banana?Their county produces bananas.???
?
?
?
b.
/their  /county  /happen  /big?
?
/event /ASP?A big event happened in their county.???
?
?
?
c.  /their  /county  /go out  ?/door?
?
/right away  /be  /mountain?In their county, you can see mountains as soonas you step out of the doors.
?The verb has the sense produce in (1a) and itsobject should be something producible, such as???
/banana?.
While in (1b), with the sensehappen, the verb typically takes an event or event-like ??
object, such as ?
/big event?,???
/accident?
or ???/problem?
etc.
In (1c),?the verb?s object ?
/door?
is closely related tolocation, consistent with the sense go out.
Incontrast, simple lexical or POS tag featuressometimes fail to capture such information, whichcan be seen clearly in (2),4 In our experiments, we set 20 as the maximalnumber of rounds after trying different numbers ofrounds (20, 40, 60, 80, 100) in a preliminaryexperiment.0  iff  the target verb has no sententialcomplement1  iff  the target verb has a nonfinitesentential complement2  iff  the target verb has a finitesentential complement??
?
(2) a.
/last year  /produce  ?
?/banana  3000??
/ kilogram?3000 kilograms of bananas were produced lastyear.??
?
b.
/in order to /produce   ??/Hainan??
?
??
/best  /DE   /banana?In order to produce the best bananas inHainan, ??
?The verb?s object ??
?/banana?, which is nextto the verb in (2a), is far away from the verb in(2b).
For (2b), a classifier only looking at theadjacent positions of the target verb tends to bemisled by the NP right after the verb, i.e.,??
?/Hainan?, which is a Province in China and atypical object of the verb with the sense go out.Five types of features are used in our model:1.
Semantic category of the subject of the targetverb2.
Semantic category of the object of the targetverb3.
Transitivity of the target verb4.
Whether the target verb takes a sententialcomplement and which type of sententialcomplement (finite or nonfinite) it takes5.
Whether the target verb occurs in a verbcompoundWe obtain the values for the first two types offeatures (1) and (2) from a semantic taxonomy forChinese nouns, which we will introduce in detail inthe next section.In our implementation, the model uses differentfeatures for different verbs.
The criteria for featureselection are from the electronic CETA dictionaryfile 5 and a hard copy English-Chinese dictionary,The Warmth Modern Chinese-English Dictionary.6For example, the verb ??|chu1?
never takessentential complements, thus the fourth type offeature is not used for it.
It could be supposed thatwe can still have a uniform model, i.e., a modelusing the same set of features for all the targetverbs, and just let the EM clustering algorithm finduseful features for different verbs automatically.The problem here is that unsupervised learningmodels (i.e., models trained on unlabeled data) aremore likely to be affected by noisy data thansupervised ones.
Since all the features used in ourmodel are extracted from automatically parsedsentences that inevitably have preprocessing errorssuch as segmentation, POS tagging and parsingerrors, using verb-specific sets of features canalleviate the problem caused by noisy data to someextent.
For example, if the model already knows5 Licensed from the Department of Defense6 The Warmth Modern Chinese-English Dictionary,Wang-Wen Books Ltd, 1997.that a verb like ??|chu1?
can never take sententialcomplements (i.e., it does not use the fourth type offeature for that verb), it will not be misled byerroneous parsing information saying that the verbtakes sentential complements in certain sentences.Since the corresponding feature is not included, thenoisy data is filtered out.
In our EM clusteringmodel, all the features selected for a target verb aretreated in the same way, as described in Section 2.4 A Semantic Taxonomy Built Semi-automaticallyExamples in (1) have shown that the semanticcategory of the object of a verb sometimes iscrucial in distinguishing certain Chinese verbsenses.
And our previous work on informationextraction in Chinese (Chen et al, 2004) hasshown that semantic features, which are moregeneral than lexical features but still contain richinformation about words, can be used to improve amodel?s capability of handling unknown words,thus alleviating potential sparse data problems.We have two Chinese electronic semanticdictionaries: the Hownet dictionary, which assigns26,106 nouns to 346 semantic categories, and theRocling dictionary, which assigns 4,474 nouns to110 semantic categories.7 A preliminaryexperimental result suggests that these semanticcategories might be too fine-grained for the EMclustering model (see Section 5.2 for greaterdetails).
An analysis of the sense distinctions ofseveral Chinese verbs also suggests that moregeneral categories on top of the Hownet andRocling categories could still be informative andmost importantly, could enable the model togenerate meaningful clusters more easily.
Wetherefore built a three-level semantic taxonomybased on the two semantic dictionaries using bothautomatic methods and manual effort.The taxonomy was built in three steps.
First, asimple mapping algorithm was used to mapsemantic categories defined in Hownet andRocling into 27 top-level WordNet categories.8The Hownet or Rocling semantic categories haveEnglish glosses.
For each category gloss, thealgorithm looks through the hypernyms of its firstsense in WordNet and chooses the first WordNettop-level category it finds.7 Hownet assigns multiple entries (could be differentsemantic categories) to polysemous words.
The Roclingdictionary we used only assigns one entry (i.e., onesemantic category) to each noun.8 The 27 categories contain 25 unique beginners fornoun source files in WordNet, as defined in (Fellbaum,1998) and two higher level categories Entity andAbstraction.The mapping obtained from step 1 needs furthermodification for two reasons.
First, the glosses ofHownet or Rocling semantic categories usuallyhave multiple senses in WordNet.
Sometimes, thefirst sense in WordNet for a category gloss is notits intended meaning in Hownet or Rocling.
In thiscase, the simple algorithm cannot get the correctmapping.
Second, Hownet and Rocling sometimesuse adjectives or non-words as category glosses,such as animate and LandVehicle etc., which haveno WordNet nominal hypernyms at all.
However,those adjectives or non-words usually havestraightforward meanings and can be easilyreassigned to an appropriate WordNet category.Although not accurate, the automatic mapping instep 1 provides a basic framework or skeleton forthe semantic taxonomy we want to build andmakes subsequent work easier.In step 2, hand correction, we found that wecould make judgments and necessary adjustmentson about 80% of the mappings by only looking atthe category glosses used by Hownet or Rocling,such as livestock, money, building and so on.
Forthe other 20%, we could make quick decisions bylooking them up in an electronic table we created.For each Hownet or Rocling category, our tablelists all the nouns assigned to it by the twodictionaries.
We merged two WordNet categoriesinto others and subdivided three categories thatseemed more coarse-grained than others into 2~5subcategories.
Step 2 took three days and 35intermediate-level categories were generated.In step 3, we manually clustered the 35intermediate-level categories into 7 top-levelsemantic categories.
Figure 1 shows part of thetaxonomy.The EM clustering model uses the 7 top-levelcategories to define the first two types of featuresthat were introduced in Section 3.
For example, thevalue of a feature kf  is 1 if and only if the objectNP of the target verb belongs to the semanticcategory Event and is otherwise 0.5 Clustering ExperimentsSince we need labeled data to evaluate theclustering performance but have limited sense-tagged corpora, we applied the clustering model to12 Chinese verbs in our experiments.
The verbs arechosen from 28 annotated verbs in Penn ChineseTreebank so that they have at least two verbmeanings in the corpus and for each of them, thenumber of instances for a single verb sense doesnot exceed 90% of the total number of instances.In our task, we generally do not include sensesfor other parts of speech of the selected words,such as noun, preposition, conjunction and particleetc., since the parser we used has a very highaccuracy in distinguishing different parts of speechof these words (>98% for most of them).
However,we do include senses for conjunctional and/orprepositional usage of two words, ??|dao4?
and?
?|wei4?, since our parser cannot distinguish theverb usage from the conjunctional or prepositionalusage for the two words very well.Five verbs, the first five listed in Table 1, areboth highly polysemous and difficult for asupervised word sense classifier (Dang et al,2002).
9 In our experiments, we manually groupedthe verb senses for the five verbs.
The criteria forthe grouping are similar to Palmer et al?s (toappear) work on English verbs, which considersboth sense coherence and predicate-argumentstructure distinctions.
Figure 2 gives an example of9 In the supervised task, their accuracies are lowerthan 85%, and four of them are even lower than thebaselines.EntityPlant     ArtifactDocumentFood    ?
?Moneydrinks, edible, meals, vegetable, ?LocationLocation_PartLocationGroup  ?
?institution, army, corporation, ?EventNatural PhenomenaHappeningActivity    ?
?Processchase, cut, pass, split, cheat, ?process, BecomeLess, StateChange, disappear, ?.Top levelIntermediate levelHownet/RoclingcategoriesFigure 1.
Part of the 3-level Semantic Taxonomy for Chinese Nouns (other top-level nodesare Time, Human, Animal and State)the definition of sense groups.
The manuallydefined sense groups are used to evaluate themodel?s performance on the five verbs.The model was trained on an unannotatedcorpus, People?s Daily News (PDN), and tested onthe manually sense-tagged Chinese Treebank (withsome additional sense-tagged PDN data).10 Weparsed the training and test data using a MaximumEntropy parser and extracted the features from theparsed data automatically.
The number of clustersused by the model is set to the number of thedefined senses or sense groups of each target verb.For each verb, we ran the EM clustering algorithmten times.
Table 2 shows the average performanceand the standard deviation for each verb.
Table 1summarizes the data used in the experiments,where we also give the normalized senseperplexity11 of each verb in the test data.5.1 Evaluation MethodsWe use two external quality measures, purityand normalized mutual information (NMI) (Strehl.2002) to evaluate the clustering performance.Assuming a verb has l senses, the clustering modelassigns n instances of the verb into k clusters, in isthe size of the ith cluster, jn  is the number ofinstances hand-tagged with the jth sense, and jin isthe number of instances with the jth sense in the ithcluster, purity is defined in equation (4):?==kijijnnpurity1max1             (4)10 The sense-tagged PDN data we used here are thesame as in (Dang et al, 2002).11 It is calculated as the entropy of the sensedistribution of a verb in the test data divided by thelargest possible entropy, i.e., log2 (the number of sensesof the verb in the test data).It can be interpreted as classification accuracywhen for each cluster we treat the majority ofinstances that have the same sense as correctlyclassified.
The baseline purity is calculated bytreating all instances for a target verb in a singlecluster.
The purity measure is very intuitive.
In ourcase, since the number of clusters is preset to thenumber of senses, purity for verbs with two sensesis equal to classification accuracy defined insupervised WSD.
However, for verbs with morethan 2 senses, purity is less informative in that aclustering model could achieve high purity bymaking the instances of 2 or 3 dominant senses themajority instances of all the clusters.Mutual information (MI) is more theoreticallywell-founded than purity.
Treating the verb senseand the cluster as random variables S and C, theMI between them is defined in equation (5):??
?= ===ljkijijijicsnnnnnncpspcspcspCSMI1 1,log)()(),(log),(),((5)MI(S,C) characterizes the reduction inuncertainty of one random variable S (or C) due toknowing the other variable C (or S).
A singlecluster with all instances for a target verb has azero MI.
Random clustering also has a zero MI inthe limit.
In our experiments, we used [0,1]-normalized mutual information (NMI) (Strehl.2002).
A shortcoming of this measure, however, isthat the best possible clustering (upper bound)evaluates to less than 1, unless classes arebalanced.
Unfortunately, unbalanced sensedistribution is the usual case in WSD tasks, whichmakes NMI itself hard to interpret.
Therefore, inaddition to NMI, we also give its upper bound(upper-NMI) and the ratio of NMI and its upperbound (NMI-ratio) for each verb, as shown incolumns 6 to 8 in Table 2.Senses for ??|dao4?
Sense groups for ??|dao4?1.
to go to, leave for2.
to come3.
to arrive4.
to reach a particular stage, condition, or level5.
marker for completion of activities (after a verb)6. marker for direction of activities (after a verb)7. to reach a time point8.
up to, until (prepositional usage)9. up to, until, (from ?)
to ?
(conjunctional usage)1, 24,7,8,9536Figure 2.
Sense groups for the Chinese verb ?
?|dao4?Verb| Pinyin Sample senses ofthe verb# Senses intest data# Sensegroups intest dataSenseperplexity#Clusters# Traininginstances# Testinstances?
|chu1 go out /produce 16 7 0.68 8 399 157?
|dao4 come /reach 9 5 0.72 6 1838 186?
|jian4 see /show 8 5 0.68 6 117 82?
|xiang3 think/suppose 6 4 0.64 6 94 228?
|yao4 Should/intend to 8 4 0.65 7 2781 185?
?|biao3shi4 Indicate /express 2  0.93 2 666 97?
?|fa1xian4 discover /realize 2  0.76 2 319 27?
?|fa1zhan3 develop /grow 3  0.69 3 458 130?
?|hui1fu4 resume /restore 4  0.83 4 107 125?
|shuo1 say /express bywritten words7  0.40 7 2692 307?
?|tou2ru4 to input /plunge into 2  1.00 2 136 23?
|wei2_4 to be /in order to 6  0.82 6 547 463Verb SenseperplexityBaselinePurity (%)Purity(%)Std.
Dev.
ofpurity (%)NMI Upper-NMINMI-ratio (%)Std.
Dev.
ofNMI ratio (%)?
0.68 52.87 63.31 1.59 0.2954 0.6831 43.24 1.76?
0.72 40.32 90.48 1.08 0.4802 0.7200 75.65 0.00?
0.68 58.54 72.20 1.61 0.1526 0.6806 22.41 0.66?
0.64 68.42 79.39 3.74 0.2366 0.6354 37.24 8.22?
0.65 69.19 69.62 0.34 0.0108 0.6550 1.65 0.78??
0.93 64.95 98.04 1.49 0.8670 0.9345 92.77 0.00??
0.76 77.78 97.04 3.87 0.7161 0.7642 93.71 13.26??
0.69 53.13 90.77 0.24 0.4482 0.6918 64.79 2.26??
0.83 45.97 65.32 0.00 0.1288 0.8234 15.64 0.00?
0.40 80.13 93.00 0.58 0.3013 0.3958 76.13 4.07??
1.00 52.17 95.65 0.00 0.7827 0.9986 78.38 0.00?
0.82 32.61 75.12 0.43 0.4213 0.8213 51.30 2.07Average 0.73 58.01 82.50 1.12 0.4088 0.7336 54.41 3.315.2 Experimental ResultsTable 2 summarizes the experimental results forthe 12 Chinese verbs.
As we see, the EM clusteringmodel performs well on most of them, except theverb ?
?|yao4?.12 The NMI measure NMI-ratioturns out to be more stringent than purity.
A highpurity does not necessarily mean a high NMI-ratio.Although intuitively, NMI-ratio should be relatedto sense perplexity and purity, it is hard toformalize the relationships between them from theresults.
In fact, the NMI-ratio for a particular verbis eventually determined by its concrete sensedistribution in the test data and the model?sclustering behavior for that verb.
For example, theverbs ??|chu1?
and ??|jian4?
have the samesense perplexity and ??|jian4?
has a higher puritythan ??|chu1?
(72.20% vs. 63.31%), but the NMI-ratio for ??|jian4?
is much lower than ??|chu1?
(22.41% vs. 43.24%).
An analysis of the12 For all the verbs except ?
?|yao4?, the model?spurities outperformed the baseline purities significantly(p<0.05, and p<0.001 for 8 of them).classification results for ??|jian4?
shows that theclustering model made the instances of the verb?smost dominant sense the majority instances ofthree clusters (of total 5 clusters), which ispenalized heavily by the NMI measure.Rich linguistic features turn out to be veryeffective in learning Chinese verb sensedistinctions.
Except for the two verbs,???|fa1xian4?
and ??
?|biao3shi4?, the sensedistinctions of which can usually be made only bysyntactic alternations,13 features such as semanticfeatures or combinations of semantic features andsyntactic alternations are very beneficial andsometimes even necessary for learning sensedistinctions of other verbs.
For example, the verb??|jian4?
has one sense see, in which the verbtypically takes a Human subject and a sententialcomplement, while in another sense show, the verbtypically takes an Entity subject and a State object.An inspection of the classification results shows13 For example, the verb ???|fa1xian4?
takes anobject in one sense discover and a sententialcomplement in the other sense realize.Table 1.
A summary of the training and test data used in the experimentsTable 2.
The performance of the EM clustering model on 12 Chinese verbs measuredby purity and normalized mutual information (NMI)that the EM clustering model has indeed learnedsuch combinatory patterns from the training data.The experimental results also indicate that thesemantic taxonomy we built is beneficial for thetask.
For example, the verb ???|tou1ru4?
hastwo senses, input and plunge into.
It typically takesan Event object for the second sense but not for thefirst one.
A single feature obtained from oursemantic taxonomy, which tests whether the verbtakes an Event object, captures this property neatly(achieves purity 95.65% and NMI-ratio 78.38%when using 2 clusters).
Without the taxonomy, thetop-level category Event is split into many fine-grained Hownet or Rocling categories, whichmakes it very difficult for the EM clustering modelto learn sense distinctions for this verb.
In fact, in apreliminary experiment only using the Hownet andRocling categories, the model had the same purityas the baseline (52.17%) and a low NMI-ratio(4.22%) when using 2 clusters.
The purityimproved when using more clusters (70.43% with4 clusters and 76.09% with 6), but it was still muchlower than the purity achieved by using thesemantic taxonomy and the NMI-ratio droppedfurther (1.19% and 1.20% for the two cases).By looking at the classification results, weidentified three major types of errors.
First,preprocessing errors create noisy data for themodel.
Second, certain sense distinctions dependheavily on global contextual information (cross-sentence information) that is not captured by ourmodel.
This problem is especially serious for theverb ??|yao4?.
For example, without globalcontextual information, the verb can have at leastthree meanings want, need or should in the sameclause, as shown in (3).
(3) ?
?
?
?/he    /want/need/should    /at once??
??
?
/finish reading  /this /book.
?He wants to/needs to/should finish reading thisbook at once.
?Third, a target verb sometimes has specific typesof NP arguments or co-occurs with specific typesof verbs in verb compounds in certain senses.
Suchinformation is crucial for distinguishing thesesenses from others, but is not captured by thegeneral semantic taxonomy used here.
We didfurther experiments to investigate how muchimprovement the model could gain by capturingsuch information, as discussed in Section 5.3.5.3 Experiments with Lexical SetsAs discussed by Patrick Hanks (1996), certainsenses of a verb are often distinguished by verynarrowly defined semantic classes (called lexicalsets) that are specific to the meaning of that verbsense.
For example, in our case, the verb???|hui1fu4?
has a sense recover in which itsdirect object should be something that can berecovered naturally.
A typical set of object NPs ofthe verb for this particular sense is partially listedin (4),(4) Lexical set for naturally recoverable things??
??
??
{ /physical strength, /body, /health,??
?
?/mental energy, /hearing ?
?, /feeling,??
?/memory, ??
}Most words in this lexical set belong to theHownet category attribute and the top-levelcategory State in our taxonomy.
However, even thelower-level category attribute still contains manyother words irrelevant to the lexical set, some ofwhich are even typical objects of the verb for twoother senses, resume and regain, such as??
?/diplomatic relations?
in ???/resume?
?/diplomatic relations?
and ??
?/reputation?in ???/regain??/reputation?.
Therefore, alexical set like (4) is necessary for distinguishingthe recover sense from other senses of the verb.It has been argued that the extensional definitionof lexical sets can only be done using corpusevidence and it cannot be done fully automatically(Hanks, 1997).
In our experiments, we use abootstrapping approach to obtain five lexical setssemi-automatically for three verbs ??|chu1?,??|jian4?
and ???|hui1fu4?
that have both lowpurity and low NMI-ratio in the first set ofexperiments.
14 We first extracted candidates forthe lexical sets from the training data.
For example,we extracted all the direct objects of the verb???|hui1fu4?
and all the verbs that combinedwith the verb ??|chu1?
to form verb compoundsfrom the automatically parsed training data.
Fromthe candidates, we manually selected words toform five initial seed sets, each of which containsno more than ten words.
A simple algorithm wasused to search for all the words that have the samedetailed Hownet semantic definitions (semanticcategory plus certain supplementary information)as the seed words.
We did not use Rocling becauseits semantic definitions are so general that a seedword tends to extend to a huge set of irrelevantwords.
Highly relevant words were manuallyselected from all the words found by the searchingalgorithm and added to the initial seed sets.
Theenlarged sets were used as lexical sets.The enhanced model first uses the lexical sets toobtain the semantic category of the NP arguments14 We did not include ?
?|yao4?, since its meaningrarely depends on local predicate-argument structureinformation.of the three verbs.
Only when the search fails doesthe model resort to the general semantic taxonomy.The model also uses the lexical sets to determinethe types of the compound verbs that contain thetarget verb ??|chu1?
and uses them as newfeatures.Table 3 shows the model?s performance on thethree verbs with or without using lexical sets.
Aswe see, lexical sets improves the model?sperformance on all of them, especially on the verb??|chu1?.
Although the results are stillpreliminary, they nevertheless provide us hints ofhow much a WSD model for Chinese verbs couldgain from lexical sets.w/o  lexical sets (%) with lexical sets (%) Verb Purity NMI-ratio Purity NMI-ratio?
63.61 43.24 76.50 52.81?
72.20 22.41 77.56 34.63??
65.32 15.64 69.03 19.716 ConclusionWe have shown that an EM clustering modelthat uses rich linguistic features and a generalsemantic taxonomy for Chinese nouns generallyperforms well in learning sense distinctions for 12Chinese verbs.
In addition, using lexical setsimproves the model?s performance on three of themost challenging verbs.Future work is to extend our coverage and toapply the semantic taxonomy and the same typesof features to supervised WSD in Chinese.
Sincethe experimental results suggest that a generalsemantic taxonomy and more constrained lexicalsets are both beneficial for WSD tasks, we willdevelop automatic methods to build large-scalesemantic taxonomies and lexical sets for Chinese,which reduce human effort as much as possible butstill ensure high quality of the obtained taxonomiesor lexical sets.7 AcknowledgementsThis work has been supported by an ITICsupplement to a National Science FoundationGrant, NSF-ITR-EIA-0205448.
Any opinions,findings, and conclusions or recommendationsexpressed in this material are those of the author(s)and do not necessarily reflect the views of theNational Science Foundation.ReferencesJinying Chen, Nianwen Xue and Martha Palmer.2004.
Using a Smoothing Maximum EntropyModel for Chinese Nominal Entity Tagging.
InProceedings of the 1st Int.
Joint Conference onNatural Language Processing.
Hainan Island,China.Hoa Trang Dang, Ching-yi Chia, Martha Palmer,and Fu-Dong Chiou.
2002.
Simple Features forChinese Word Sense Disambiguation.
InProceedings of COLING-2002 Nineteenth Int.Conference on Computational Linguistics,Taipei, Aug.24?Sept.1.Christiane Fellbaum.
1998.
WordNet ?
anElectronic Lexical Database.
The MIT Press,Cambridge, Massachusetts, London.Daniel Gildea and Daniel Jurafsky.
2002.Automatic Labeling of Semantic Roles.Computational Linguistics, 28(3): 245-288,2002.Patrick Hanks.
1996.
Contextual dependencies andlexical sets.
The Int.
Journal of CorpusLinguistics, 1:1.Patrick Hanks.
1997.
Lexical sets: relevance andprobability.
in B. Lewandowska-Tomaszczykand M. Thelen (eds.)
Translation and Meaning,Part 4, School of Translation and Interpreting,Maastricht, The Netherlands.Thomas Hofmann and Puzicha Jan. 1998.Statistical models for co-occurrence data, MITArtificial Intelligence Lab., Technical ReportAIM-1625.Adam Kilgarriff and Martha Palmer.
2000.Introduction to the sepcial issue on SENSEVAL.Computers and the Humanities, 34(1-2): 15-48.Martha Palmer, Hoa Trang Dang, and ChristianeFellbaum.
To appear.
Making fine-grained andcoarse-grained sense distinctions, both manuallyand automatically.
Natural LanguageEngineering.Mats Rooth, Stefan Riezler, Detlef Prescher, GlennCarroll, and Franz Beil.
1998.
EM-basedclustering for NLP applications.
AIMS Report4(3).Institut f?r Maschinelle Sprachverarbeitung.Sabine Schulte im Walde.
2000.
Clustering verbssemantically according to their alternationbehaviour.
In Proceedings of the 18th Int.Conference on Computational Linguistics, 747-753.Hinrich Sch?tze.
1998.
Automatic Word SenseDiscrimination.
Computational Linguistics, 24(1): 97-124.Alexander Strehl.
2002.
Relationship-basedClustering and Cluster Ensembles for High-dimensional Data Mining.
Dissertation.
TheUniversity of Texas at Austin.
http://www.lans.ece.utexas.edu/~strehl/diss/.Table 3.
Clustering performance with andwithout lexical sets for three Chinese verbs
