Proceedings of the Workshop on Innovative Hybrid Approaches to the Processing of Textual Data (Hybrid2012), EACL 2012, pages 87?96,Avignon, France, April 23 2012. c?2012 Association for Computational LinguisticsMethods Combination and ML-based Re-ranking of MultipleHypothesis for Question-Answering SystemsArnaud GrappyLIMSI-CNRSarnaud.grappy@limsi.frBrigitte GrauLIMSI-CNRSENSIIEbrigitte.grau@limsi.frSophie RossetLIMSI-CNRSsophie.rosset@limsi.frAbstractQuestion answering systems answer cor-rectly to different questions because theyare based on different strategies.
In orderto increase the number of questions whichcan be answered by a single process, wepropose solutions to combine two questionanswering systems, QAVAL and RITEL.QAVAL proceeds by selecting short pas-sages, annotates them by question terms,and then extracts from them answers whichare ordered by a machine learning valida-tion process.
RITEL develops a multi-levelanalysis of questions and documents.
An-swers are extracted and ordered accordingto two strategies: by exploiting the redun-dancy of candidates and a Bayesian model.In order to merge the system results, we de-veloped different methods either by merg-ing passages before answer ordering, or bymerging end-results.
The fusion of end-results is realized by voting, merging, andby a machine learning process on answercharacteristics, which lead to an improve-ment of the best system results of 19 %.1 IntroductionQuestion-answering systems aim at giving shortand precise answers to natural language ques-tions.
These systems are quite complex, andinclude many different components.
Question-Answering systems are generally organizedwithin a pipeline which includes at a high levelat least three components: questions processing,snippets selection and answers extraction.
Buteach module of these systems is quite different.They are based on different knowledge sourcesand processing.
Even if the global performance ofthese systems are similar, they show great dispar-ity when examining local results.
Moreover thereis no question-answering system able to answercorrectly to all possible questions.
Considering allQA evaluation campaigns in French like CLEF,EQUER or Qu?ro, or for other languages likeTREC, no system obtained 100% correct answersat first rank.
A new direction of research was builtupon these observations: how can we combinecorrect answers provided by different systems?This work deals with this issue1 .
In this paperwe describe different experiments concerning thecombination of QA systems.
We used two differ-ent available systems, QAVAL and RITEL, whileRITEL includes two different answer extractionstrategies.
We propose to merge the results ofthese systems at different levels.
First, at an in-termediary step (for example, between snippet se-lection and answer extraction).
This approach al-lows to evaluate a fusion process based on the in-tegration of different strategies.
Another way toproceed is to execute the fusion at the end of eachsystem.
The aim is then to choose between all thecandidate answers the best one for each question.Such an approach has been successfully appliedin the information retrieval field, with the defini-tion of different functions for combining resultsof search engines (Shaw and Fox, 1994).
How-ever, in QA, the problem is different as answers toquestions are not made of a list of answers, but aremade of excerpts of texts, which may be differentin their writing, but which correspond to a uniqueand same answer.
Thus, we propose fusion meth-ods that rely on the information generally com-puted by QA systems, such as score, rank, an-1This work was partially financed by OSEO under theQuro program87swer redundancy, etc.
We defined new voting andscoring functions, and a machine learning systemto combine these features.
Most of the strategiespresented here allow a clear improvement (up to19 %) on the first ranked correct answers.In the following, related work is presented inthe section 2.
We then describe the different sys-tems used in this work (Section 3.1 and 3.2).
Theproposed approach are presented (Section 4 and5).
The methods and the different systems arethen evaluated on the same corpus.2 Related workQA system hybridization often consists in merg-ing end-results.
The first studies presented hereaim at merging the results of different strate-gies for finding answers in the same set of doc-uments.
(Jijkoun and Rijke, 2004) developed sev-eral strategies for answering questions, based ondifferent paradigms for extracting answers.
Theysearch for answers in a knowledge base or by ap-plying extraction patterns or by selecting the n-grams the closest to the question words.
They de-fined different methods for recognizing the simi-larity of two answers: equality, inclusion and anedit distance.
The merging of answers is realizedby summing the confidence scores of similar an-swers and leads to improve the number of rightanswers at first rank of 31 %.
(Tellez-Valero et al, 2010) combine the out-put of QA systems, whose strategy is not known.They only dispose of the provided answers asso-ciated with a supporting snippet.
Merging is doneby a machine learning approach, which combinesdifferent criteria such as the question category, theexpected answer type, the compatibility betweenthe provided answer and the question, the systemwhich was applied and the rate of question termsin the snippet.
When applying this module on theCLEF QA systems which were run on the Span-ish data, they obtain a better MRR2 value than thebest system from 0.62 up to 0.73.In place of diversifying the answering strate-gies, another possibility is to apply a same strat-egy on different collections.
(Aceves-Pe?rez et al,2008) apply classical merging strategies to mul-tilingual QA systems, by merging answers ac-cording to their rank or by combining their con-fidence scores, normalized or not.
They show that2Mean Reciprocal Rankthe combination of normalized scores obtains re-sults which are better than a monolingual system(MRR from 0.64 up to 0.75).
They also testedhybridization at the passage level by extractinganswers from the overall set of passages whichproved to be less relevant than answer merging.
(Chalendar et al, ) combine results obtained bysearching the Web in parallel to a given collec-tion.
The combination which consists in boostinganswers if they are found by the two systems isvery effective, as it is less probable to find sameincorrect answers on different documents.The hybridization we are interested in concernsthe merging of different strategies and differentsystem capabilities in order to improve the finalresult.
We tested different hybridization levels,and different merging methods.
One is closedto (Tellez-Valero et al, 2010) as it is based ona validation module.
Other are voting and scor-ing methods which have been defined accordingto our task, and are compared to classical merg-ing scheme which have been proposed in infor-mation retrieval (Shaw and Fox, 1994), ComSumand CombMNZ.3 The Question-Answering systems3.1 The QAVAL system3.1.1 General overviewQAVAL(Grappy et al, 2011) is made of se-quential modules, corresponding to five mainsteps (see Fig.
1).
The question analysis providesmain characteristics for retrieving passages andfor guiding the validation process.
Short passagesof about 300-character long are obtained directlyfrom the search engine Lucene and are annotatedwith question terms and their weighted variants.They are then parsed by a syntactic parser and en-riched with the question characteristics, which al-lows QAVAL to compute the different features forvalidating or discarding candidate answers.A specificity of QAVAL relies on its validationmodule.
Candidate answers are extracted accord-ing to the expected answer type, i.e.
a named en-tity or not.
In case of a named entity, all the namedentities corresponding to the expected type areextracted while, in the second case, QAVAL ex-tracts all the noun phrases which are not questionphrases.
As many candidate answers can be ex-tracted, a first step consists in recognizing obviousfalse answers.
Answers from a passage that does88QuestionanalysisPassageselectionAnswervalidationandrankingCandidateanswerextractionAnnotationandsyntacticanalysisof passagesQuestionsanswersanswersanswersDocumentsAnswerrankingAnswerfusionQAVALRITELStandardRITELProbabilistic5 answers QuestionanalysisAnnotationandsyntacticanalysisof passagesPassageselectionCandidateanswerextractionAnswerrankingHybridizationpointFigure 1: The QAVAL and RITEL systems and theirpossible hybridizationsnot contain all the named entities of the questionare discarded.
The remaining answers are thenranked based on a learning method which com-bines features characterizing the passage and thecandidate answer it provides.
The QAVAL sys-tem has been evaluated on factual questions andobtains good results.3.1.2 Answer ranking by validationA machine based learning validation moduleprovides scores to each candidate answer.
Fea-tures relative to passages aim at evaluating inwhich part a passage conveys the same meaningas the question.
They are based on lexical fea-tures, as the rate of question words in the passage,their POS tag, the main terms of the question, etc.Features relative to the answer represent theproperty that an answer has to be of an expectedtype, if explicitly required, and to be related tothe question terms.
Another kind of criterion con-cerns the answer redundancy: the most frequentan answer is, the most relevant it is.
Answer typeverification is applied for questions which give anexplicit type for the answer, as in ?Which presi-dent succeeded Georges W.
Bush??
that expectsas answer the name of a president, more specificthan the named entity type PERSON.
This mod-ule (Grappy and Grau, 2010) combines resultsgiven by different kinds of verifications, basedon named entity recognizers and searches in cor-pora.
To evaluate the relation degree of an answerwith the question terms, QAVAL computes i) thelongest chain of consecutive common words be-tween the question plus the answer and the pas-sage; ii) the average distance between the answerand each of the question words in the passage.Other criteria are the passage rank given by us-ing results of the passage analysis, the questioncategory, i.e.
definition, characterization of an en-tity, verb modifier or verb complement, etc.3.2 The RITEL systems3.3 General overviewThe RITEL system (see Figure 1) which we usedin these experiments is fully described in (Bernardet al, 2009).
This system has been devel-oped within the framework of the Ritel projectwhich aimed at building a human-machine dia-logue system for question-answering in open do-main (Toney et al, 2008).The same multilevel analysis is carried out onboth queries and documents.
The objective of thisanalysis is to find the bits of information that maybe of use for search and extraction, called perti-nent information chunks.
These can be of dif-ferent categories: named entities, linguistic enti-ties (e.g., verbs, prepositions), or specific entities(e.g., scores).
All words that do not fall into suchchunks are automatically grouped into chunks viaa longest-match strategy.
The analysis is hierar-chical, resulting in a set of trees.
Both answersand important elements of the questions are sup-posed to be annotated as one of these entities.The first step of the QA system itself is to builda search descriptor (SD) that contains the impor-tant elements of the question, and the possibleanswer types with associated weights.
Answertypes are predicted through rules based on com-binations of elements of the question.
On all sec-ondary and mandatory chunks, the possible trans-formations (synonym, morphological derivation,etc.)
are indicated and weighted in the SD.
Docu-ments are selected using this SD.
Each element ofthe document is scored with the geometric meanof the number of occurrences of all the SD ele-ments that appear in it, and sorted by score, keep-ing the n-best.
Snippets are extracted from thedocument using fixed-size windows and scoredusing the geometrical mean of the number of oc-89currences of all the SD elements that appear in thesnippet, smoothed by the document score.3.3.1 Answer selection and rankingTwo different strategies are implemented in RI-TEL.
The first one is based on distance betweenquestion words and candidate answer, named RI-TEL Standard.
The second one is based on aBayesian model, named RITEL Probabilistic.Distance-based answer scoring The snippetsare sorted by score and examined one by one in-dependently.
Every element in a snippet with atype found in the list of expected answer types ofthe SD is considered an answer candidate.
RITELassociates to each candidate answer a score whichis the sum of the distances between itself and theelements of the SD.
That score is smoothed withthe snippet score through a ?-ponderated geomet-ric mean.
All the scores for the different instancesof the same element are added together.
The enti-ties with the best scores then win.
The scores foridentical (type,value) pairs are added together andgive the final scoring to the candidate answers.Answer scoring through Bayesian modelingThis method of answer scoring is built upon aBayesian modeling of the process of estimatingthe quality of an answer candidate.
This approachrelies on multiple elementary models includingelement co-occurrence probabilities, question el-ement appearance probability in the context of acorrect answer and out of context answer proba-bility.
The model parameters are either estimatedon the documents or are set empirically.
This sys-tem has not better result than the distance-basedone but is interesting because it allows to obtaindifferent correct answers.3.4 Systems combinationThe systems we used in these experiments arevery different especially with respect to the pas-sage selection and the answer extraction and scor-ing methods.
The QAVAL system proceeds tothe passage selection before any analysis whilethe two RITEL systems do a complete and multi-level analysis on the documents before the pas-sage selection.
Concerning the answer extractionand scoring, the QAVAL system uses an answervalidation process based on machine learning ap-proach while the answer extraction of the RITEL-S system uses a distance-based scoring and theRITEL-P Bayesian models.
It seems then inter-esting to combine these various approaches in ain-system way (see Section 4): (1) the passagesselected by the QAVAL system are provided asdocument collection to the RITEL systems; (2)the candidate answers provided by the RITELsystems are given to the answer validation mod-ule of the QAVAL system.We also worked, in a more classical way, oninterleaving results of answer selection methods(see Section 5 and 6).
These methods make use ofthe various information provided by the differentsystems along with all candidate answers.4 Internal combination4.1 QAVAL snippets used by RITELThe RITEL system proceeds to a complete analy-sis of the document which is used during the doc-ument and selection extraction procedure and ob-tains 80.3% of the questions having a correct an-swer in at least one passage.
The QAVAL systemextracts short passages (150) using Lucene andobtains a score of 88%.
We hypothesized that theRITEL?s fine-grained analysis could better workon small collection than on the overall documentcollection (combination 1 Fig.
1).
We considerthe passages extracted by the QAVAL system be-ing a new collection for the RITEL system.
First,the analysis is done on this new collection andthe analysis result is indexed.
Then the gen-eral question-answering procedures are applied:question analysis, SD construction, document andsnippet extraction and then answer selection andranking.
The two answer extraction methods havebeen applied and the results are presented in theTable 1.
This simple approach does not allow anyAll documents QAVAL?
snippetsRitel-S Ritel-P Ritel-S Ritel-Ptop-1 34.0% 22.4% 29.9% 22.4%MRR 0.41 0.29 0.38 0.32top-20 61.2% 48.7% 54.4% 49.7%Table 1: Results of Ritel systems (Ritel-S usedthe distance-based answer scoring, Ritel-P used theBayesian modeling) working on the QAVAL?
snippets.improvement.
Actually all the results are worsen-ing, except maybe for the Ritel-P systems (whichis actually not the best one).
One of our hypoth-esis is that the QAVAL snippets are too short and90do not fit the criteria used by the RITEL system.4.2 Answer validationIn QAVAL, answer ranking is done by an an-swer validation module (fully described in sec-tion 3.1).
The candidate answers ranked by thismodule are associated to a confidence score.
Theobjective of this answer validation module is todecide whether the candidate answer is correct ornot given an associated snippet.
The objective isto use this answer validation module on the candi-date answers and the snippets provided by all thesystems (combination 2 Fig.
1).
Unfortunately,this method did not obtain better results than thebest system.
We assume that this module beinglearnt on the QAVAL data only is not robust todifferent data and more specifically to the passagelength which is larger in RITEL than in QAVAL.A possible improvement could be to add answersfound by the RITEL system in the training base.5 Voting methods and scorescombinationThese methods are based on a comparison be-tween the candidate answers: are they identical ?An observation that can be made concerning theuse of a strict equality between answers is that insome cases, 2 different answers can be more orless identical.
For example if one system returns?Sarkozy?
and another one ?Nicolas Sarkozy?
wemay want to consider these two answers as iden-tical.
We based the comparison of answers on thenotion of extended equality.
For that, we usedmorpho-syntactic information such as the lemmasand the part of speech of each words of the an-swers.
The TreeTagger tool3 has been used.
Ananswer R1 is then considered as included in ananswer R2 if all non-empty words of R1 are in-cluded in R2.
Two words having the same lemmaare considered as identical.
For example ?chanta?and ?chanterons?
are identical because they sharethe same lemma ?chanter?.
Adjectives, propernames and substantives are considered as non-empty words.
Following this definition, two an-swers R1 and R2 are considered identical if R1 isincluded in R2 and R2 in R1.3www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger5.1 Merge based on candidate answer rankThe first information we used takes into accountthe rank of the candidate answers.
The hypothesisbeyond this is that the systems often provide thecorrect answer at first position, if they found it.5.1.1 Simple interleavingThe first method, and probably the simplest, isto merge the candidate answers provided by allthe systems: the first candidate answer of the firstsystem is ranked in the first position; the first an-swer of the second system is ranked in the sec-ond position; the second answer of the first sys-tem is ranked in the third position, and so on.
Ifone answer was already merged (because rankedat a higher rank by another system), it is not used.We choose to base the systems order given theirindividual score.
The first system is QAVAL, thesecond RITEL-S and the third RITEL-P. Follow-ing that method, the accuracy (percentage of cor-rect answers at first rank) is the one obtained bythe best system.
But we assume that the MRR atthe top-n (with n > 1) would be improved.5.1.2 Sum of the inverse of the rankThe simple interleaving method does not takeinto account the answer rank provided by the dif-ferent systems.
However, this information maybe relevant and was used in order to merge can-didate answer extracted from different documentcollection, Web articles and news paper (Chalen-dar et al, ).
In our case, answers are extractedfrom the same document collection by the dif-ferent systems.
Then it is possible that the samewrong answers will be extracted by the differentsystems.A first possible method to take into accountthe rank provided by the systems is to weight thecandidate answer using this information.
For asame answer provided by the different systems,the weight is the sum of the inverse of the rankgiven by the systems.
To compare the answers thestrict equality is applied.
If a system ranks an an-swer at the first position and another system ranksthe same answer at the second position, the weightis 1.5 (1 + 12 ).
The following equation express ina more formalized way this method.weight =?
1rankComparing to the previous method, that oneshould allow to place more correct answers at thefirst rank.915.2 Using confidence scoresIn order to rank all their candidate answers, thesystems used a confidence score associated toeach candidate answer.
We then wanted to usethese confidence scores in order to re-rank all thecandidate answers provided by all the systems.But this is only possible if all systems producecomparable scores.
This is not the case.
QAVALproduces scores ranging from -1 to +1.
RITEL-P, being probabilistic, produces a score between 0and +1.
And RITEL-S does not use strict intervaland the scores are potentially ranged from ??
to+?.
The following normalization (a linear re-gression) has been applied to the RITEL-S andRITEL-P scores in order to place it in the range-1 to 1.valuenormalized =2 ?
valueoriginvalMin ?
valMax?
15.2.1 Sum of confidence scoresIn order to compare our methods with classi-cal approaches, we used two methods presentedin (Shaw and Fox, 1994):?
CombSum which adds the different confi-dence scores of an answer given by the dif-ferent systems;?
CombMNZ which adds the confidencescores of the different systems and multiplythe obtained value by the number of systemshaving found the considered answer.5.2.2 Hybrid methodAn hybrid method combining the rank and theconfidence score has been defined.
The weight isthe sum of two elements: the higher confidencescore and a value taking into account the rankgiven by the different systems.
This value is de-pendent on the number of answers, the type of theequality (the answers are included or equal) whichresults in the form of a bonus, and the rank of thedifferent considered answers.
The weight of ananswer a to a question q is then:w(a) = s(a) +?be ?
(|a(q)| ?
?r(a)) (1)with be the equality bonus, w the weight, s, thescore and r the rank.The equality bonus, found empirically, is givenfor each systems pair.
The value is 3 if the twoanswers are equal, 2 if an answer is included inthe other and 1 otherwise.
When an answer isfound by two or more systems, the higher con-fidence score is kept.
The result of this method isthat the answers extracted by more than one sys-tem are favored.
An answer found by only onesystem, even with a very high confidence score,may be downgraded.6 Machine-learning-based method foranswer re-rankingTo solve a re-ranking problem, machine learn-ing approaches can be used (for example (Mos-chitti et al, 2007)).
But in most of the cases,the objective is to re-rank answers provided byone system, that means to re-rank multiple hy-potheses from one system.
In our case, we wantto re-rank multiple answers from different sys-tems.
We decided to use an SVM-based approach,namely SVMrank (Joachims, 2006), which is welladapted to our problem.
An important aspect isthen to choose the pertinent features for such atask.
Our objective is to consider robust enoughfeatures to deal with different systems?
answerswithout introducing biases.
Two classes of char-acteristic should be able to give a useful represen-tation of the answers: those related to the answeritself and those related to the question.6.1 Answer characteristicsFirst of all, we should use the rank and the scoreas we did in the preceding merging methods.
Theproblem may appear here because not all candi-date answers are found by the different systems.In that case, the score and the rank given to thesesystems is then -2.
It guarantees us that the fea-tures are out of the considered range [?1,+1].Considering that, it may be useful to know whichsystem provided the considered answer.
For eachanswer all systems having found that answer areindicated.
Moreover this information may helpto distinguish answers coming from for exampleQAVAL and RITEL-S or RITEL-P from answerscoming from RITEL-S and RITEL-P.
The two RI-TEL systems share most of the modules and theiranswers may have the same problems.
Concern-ing the answer, another aspect may be of interest:how many time this answer has been found?
Thequestion is not, how many times the answer ap-pears in the documents but how many times theanswer appears in a context allowing this answer92to be considered as a candidate answer.
We usedthe number of different snippets selected by thesystems in which that answer was found.6.2 Question characteristicsWhen observing the results obtained by the sys-tems on different questions, we observed that the?kind?
of the question has an impact on the sys-tems?
performance.
More specifically, it is largelyaccepted in the community that at least two crite-ria are of importance: the length of the question,and the type of the expected answer (EAT).Question length We may consider that the lengthof the questions is more or less a good indicatorfor the complexity level of the question.
The num-ber of non-empty words of the question can thenbe a interesting feature.Expected answer type One of the task of thequestion processing, in a classical Question-Answering system, is to decide of which type willbe the answer.
For example, for a question likeWho is the president of France?
the type of theexpected answer will be a named entity of theclass person and for a question like what wine todrink with seafood?
that the EAT is not a namedentity.
(Grappy, 2009) observed that the QAVALsystem is better when the EAT is of a named entityclass.
It is possible that adding this informationwill, during the learning phase, positively weightan answer coming from RITEL when the EAT isnot a named entity.The value of this feature indicates the compat-ibility of the answer and the EAT.
We used themethod presented in (Grappy and Grau, 2010) andalready used for the answer validation module ofthe QAVAL system.
This method is based on aML-based combination of different methods us-ing named entity dictionaries, wikipedia knowl-edge, etc.
This system gives a confidence score,ranging from -1 to +1 which indicates the con-fidence the system has in compatibility betweenthe answer and the EAT.
In some cases, the ques-tion processing module may indicate if the EATis of a more fine-grained entity.
For example, thequestion Who is the president of France?
is notonly waiting for a person but more precisely for aperson having the function of a president.
A newfeature is then added.
If the EAT is a fine-grainednamed entity, then the value is 1 and -1 otherwise.7 Experiments and results7.1 Data and observationsFor the training of the SVM model, we usedthe answers to 104 questions provided by the2009 Quaero evaluation campaign (Quintard etal., 2010).
Only 104 questions have been used be-cause we need to have at least one correct answerprovided by at least one system in the trainingbase for each question.
Models have been trainedusing 5, 10, 15 and 20 answers for each system.For the evaluation, we used 147 factoid ques-tions used in the 2010 Quaero4 evaluation cam-paign.
The document collection is made of500,000 Web pages5.
We used the Mean Re-ciprocal Rank (MRR) as it is a usual metric inQuestion-Answering on the first five candidateanswers.
The MRR is the average of the recip-rocal ranks of all considered answers.
We alsoused the top-1 metric which indicates the numberof correct answers ranked at the first position.The baseline results, provided by each of thethree systems, are presented in Table 2.
QAVALand RITEL-S have quite similar results which arehigher than those obtained by the RITEL-P sys-tem.
We can observe that, within the 20 top ranks,38% of the questions have an answer given byall the systems, 76 % by at least 2 systems and21% receive no correct answers.
The best possi-ble result that could be obtained by a perfect fu-sion method is also indicated in this table (0.79 ofMRR and 79% for top-1).
Such a method wouldlead to rank first each correct answer found by atleast a system.
Figure 2 presents the answer repar-System MRR % top-1 (#)QAVAL 0.45 36 (53)RITEL-S 0.41 32 (47)RITEL-P 0.26 18 (27)Perfect fusion 0.79 79 (115)Table 2: Baseline resultstition between ranks 2 and 20 (the numbers of cor-rect answers in first rank are given in Table 2).This figure shows that the systems ranked the cor-rect answer mostly in the first positions.
Thatmeans that these systems are relatively effectivefor re-ranking their own candidate answers.
Very4http://www.quaero.org5crawled by Exalead http://www.exalead.com/93few correct answers are ranked after the tenth po-sition.
Following these observations, the evalua-tions are done on the first 10 candidate answers.2 3 4 5 6 7 8 9 10 200246810121416182022QAVALRITEL-S RITTEL-PSVM3 4 5 6 7 8 9 1Figure 2: Answer repartition7.2 Results and analysisTable 3 presents the results obtained with the dif-ferent merging methods: simple interleaving (In-ter.
), Sum of the inverse of the rank, CombSum,CombMNZ, hybrid method (Hyb.
Meth.)
andSVM model.
In order to evaluate the impact of theRITEL-P (which achieved less good results), theresults are given using two (QAVAL and RITEL-S) or three systems.Method MRR % Top-1 (#)(2 sys.
/ 3 sys.)
(2 sys.
/ 3 sys.)Inter.
0.47 / 0.45 36 (53) / 36 (53)?
1rang 0.48 / 0.46 38 (56)/ 36 (53)CombSum 0.46 / 0.44 38 (56) / 34 (50)CombMNZ 0.46/ 0.44 38 (56) / 35 (51)Hyb.
meth.
0.49 /0.44 40 (58) / 34 (50)SVM 0.48 / 0.51 39 (57) / 42 (62)QAVAL 0.44 36 (53)Table 3: General results.As shown in Table 3, the different methodsimprove the results and the best method is theSVM-based model which allows an improvementof 19% of correct answer at first rank.
This re-sult is significantly better than the baseline resultand this method can be considered as very effec-tive.
Figure 2 shows the results of this model.
Inorder to validate our choice of using the SVM-Rank model, we also tested the use of a com-bination of decision trees, as QAVAL obtained# candidate answers % Top-1 (#)20 39 (58)15 39 (58)10 43 (63)5 37 (55)Table 4: Impact of the number of candidate answersnormalization MRR # Top-1without 0.49 58 (39%)with 0.51 63 (43%)Table 5: Impact of the normalizationgood results with this classifier in the validationmodule.
We obtained a MRR of 0.44 which isobviously lower than the result obtained by theSVM method.
Generally speaking, the methodstaking into account the answer rank allow betterresults than the methods using the answer confi-dence score.
Another interesting observation isthat the interleaving methods obtained better re-sults when not using the RITEL-P system whilethe SVM one obtained better results when usingthe three systems.
We assume that these two sys-tems, RITEL-S and RITEL-P are too similar toprovide strict useful information, but that a ML-based approach is able to generalize such infor-mation.In order to validate our choice of using onlythe first ten candidate answers, we did some moretests using 5, 10, 15 and 20 candidate answers.Table 4 shows the results obtained with the SVMmodel.
We can see that is is better to consider10 candidate answers.
Beyond the first 10 can-didate answers it is difficult to re-rank the cor-rect answer without adding unsustainable noise.Moreover most of the correct answers are in thefirst ten candidates.In order to validate the confidence score nor-malization, we did experiments with and withoutthis normalization.
Table 5 presents results whichvalidate our choice.To better understand how the fusion is made,we observed the repartition of the correct answersat the first rank and at the top five ranks accordingto the number of systems which extracted them(figure 3 and figure 4).
We do this for the threebest fusion approaches: the ML method with 3systems, the hybrid method and the sum of the in-verse of the ranks with two systems.
As we can94Feuille1Page 1SVM Hybrid sum 1/rank%5%10%15%20%25%30%35%40%45%50%27%12%36% 33%4%3% 4%1 system2 systems3 systemsFigure 3: First rank Feuille1Page 1SVM Hybrid sum 1/rank%15%05%25%35%45%75%65%26%14%40% 43%10% 15% 10%1 system0 systems2 systemsFigure 4: Top five rankssee, in most of the cases, the three approaches of-ten rank the correct answers found by all the sys-tems.
The best approach is the SVM-based one.It ranks 98 % of the correct answers given by the3 systems in top 5 ranks.
It also ranks better cor-rect answers given by 2 systems (60% are rankedin the top 5 ranks versus about 48 % with the twoother methods).The rank-based method is globally reliable forselecting correct answers in the top 5 ranks.
Thisbehavior is consistent with the fact that our QAsystems, when they found a correct answer, gen-erally rank it in first positions.Some correct answers given by only one sys-tem remain in the first position, and about 10%of them remain in the top 5 ranks and are not su-perseded by common wrong answers.
Howeverthe major part of these correct single-system an-swers are discarded after the 5 first ranks (39% ofthem by the SVM method, 45% by the rank-basedmethod and 53% by the hybrid method).
In thatcase, a ML method is a better solution for decid-ing, however an improvement would be possibleonly if other features could be found for a bettercharacterization of a correct answer, or maybe byenlarging the training base.According to these results, we also can expectthat with more QA systems, a fusion approachwould be more effective.8 ConclusionImproving QA systems is a very difficult task,given the variability of the pairs (question / an-swering passages), the complexity of the pro-cesses and the variability of they performances.Thus, an improvement can be searched by the hy-bridization of different QA systems.
We studiedhybridization at different levels, internal combi-nation of processes and merging of end-results.The first combination type did not proved to beuseful, maybe because each system has its globalcoherence leading their modules to be more in-terdependent than expected.
Thus it appearsthat combining different strategies is better re-alized with the combination of their end-results,specially when these strategies obtain good re-sults.
We proposed different combination meth-ods, based on the confidence scores, the answerrank, that are adapted to the QA context, anda ML-method which considers more features forcharacterizing the answers.
This last method ob-tains the better results, even if the simpler onesalso show good results.
The proposed methodscan be applied to other QA systems, as the fea-tures used are generally provided by the systems.ReferencesR.M.
Aceves-Pe?rez, M. Montes-y Go?mez, L. Vil-lasen?or-Pineda, and L.A. Uren?a-Lo?pez.
2008.
Twoapproaches for multilingual question answering:Merging passages vs. merging answers.
Interna-tional Journal of Computational Linguistics & Chi-nese Language Processing, 13(1):27?40.G.
Bernard, S. Rosset, O. Galibert, E. Bilinski, andG.
Adda.
2009.
The LIMSI participation to theQAst 2009 track.
In Working Notes of CLEF 2009Workshop, Corfu, Greece, October.G.
De Chalendar, T. Dalmas, F. Elkateb-gara, O. Fer-ret, B. Grau, M. Hurault-plantet, G. Illouz, L. Mon-ceaux, I. Robba, and A. Vilnat.
The question an-swering system QALC at LIMSI: experiments inusing Web and WordNet.Arnaud Grappy and Brigitte Grau.
2010.
Answer typevalidation in question answering systems.
In Adap-95tivity, Personalization and Fusion of HeterogeneousInformation, RIAO ?10, pages 9?15.Arnaud Grappy, Brigitte Grau, Mathieu-Henri Falco,Anne-Laure Ligozat, Isabelle Robba, and Anne Vil-nat.
2011.
Selecting answers to questions from webdocuments by a robust validation process.
In The2011 IEEE/WIC/ACM International Conference onWeb Intelligence.Arnaud Grappy.
2009.
Validation de rponses dans unsystme de questions rponses.
Ph.D. thesis, UniversitParis Sud, Orsay.Valentin Jijkoun and Maarten De Rijke.
2004.
AnswerSelection in a Multi-Stream Open Domain QuestionAnswering System.
In Proceedings 26th EuropeanConference on Information Retrieval (ECIR?04),volume 2997 of LNCS, pages 99?111.
Springer.Thorsten Joachims.
2006.
Training linear SVMsin linear time.
In Proceedings of the 12th ACMSIGKDD international conference on Knowledgediscovery and data mining, KDD ?06, pages 217?226, New York, NY, USA.
ACM.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
ExploitingSyntactic and Shallow Semantic Kernels for Ques-tion Answer Classification.
In Proceedings of the45th Annual Meeting of the Association of Compu-tational Linguistics, pages 776?783, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Ludovic Quintard, Olivier Galibert, Gilles Adda,Brigitte Grau, Dominique Laurent, VeroniqueMoriceau, Sophie Rosset, Xavier Tannier, and AnneVilnat.
2010.
Question Answering on Web Data:The QA Evaluation in Quaero.
In LREC?10, Val-letta, Malta, May.Joseph A. Shaw and Edward A.
Fox.
1994.
Combina-tion of multiple searches.
In TREC-2.
NIST SPE-CIAL PUBLICATION SP.Alberto Tellez-Valero, Manuel Montes Gomez,Luis Villasenor Pineda, and Anselmo Penas.
2010.Towards multi-stream question answering usinganswer validation.
Informatica, 34(1):45?54.Dave Toney, Sophie Rosset, Aurlien Max, Olivier Gal-ibert, and Eric Bilinski.
2008.
An Evaluation ofSpoken and Textual Interaction in the RITEL Inter-active Question Answering System.
In EuropeanLanguage Resources Association (ELRA), editor,Proceedings of the Sixth International LanguageResources and Evaluation (LREC?08), Marrakech,Morocco, May.96
