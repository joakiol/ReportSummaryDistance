Proceedings of the Ninth International Workshop on Parsing Technologies (IWPT), pages 179?185,Vancouver, October 2005. c?2005 Association for Computational LinguisticsExploring Features for Identifying Edited Regions in Disfluent SentencesQi Zhang Fuliang WengDepartment of Computer Science Research and Technology CenterFudan University Robert Bosch Corp.Shanghai, P.R.China 200433 Palo Alto, CA 94304qi_zhang@fudan.edu.cn fuliang.weng@rtc.bosch.comAbstractThis paper describes our effort on the taskof edited region identification for parsingdisfluent sentences in the Switchboardcorpus.
We focus our attention onexploring feature spaces and selectinggood features and start with analyzing thedistributions of the edited regions andtheir components in the targeted corpus.We explore new feature spaces of a part-of-speech (POS) hierarchy and relaxed forrough copy in the experiments.
Thesesteps result in an improvement of 43.98%percent relative error reduction in F-scoreover an earlier best result in editeddetection when punctuation is included inboth training and testing data [Charniakand Johnson 2001], and 20.44% percentrelative error reduction in F-score over thelatest best result where punctuation isexcluded from the training and testingdata [Johnson and Charniak 2004].1 IntroductionRepairs, hesitations, and restarts are common inspoken language, and understanding spokenlanguage requires accurate methods for identifyingsuch disfluent phenomena.
Processing speechrepairs properly poses a challenge to spoken dialogsystems.
Early work in this field is primarily basedon small and proprietary corpora, which makes thecomparison of the proposed methods difficult[Young and Matessa 1991, Bear et al 1992,Heeman & Allen 1994].
Because of the availabilityof the Switchboard corpus [Godfrey et al 1992]and other conversational telephone speech (CTS)corpora, there has been an increasing interest inimproving the performance of identifying theedited regions for parsing disfluent sentences[Charniak and Johnson 2001, Johnson andCharniak 2004, Ostendorf et al 2004, Liu et al2005].In this paper we describe our effort towards thetask of edited region identification with theintention of parsing disfluent sentences in theSwitchboard corpus.
A clear benefit of havingaccurate edited regions for parsing has beendemonstrated by a concurrent effort on parsingconversational speech [Kahn et al2005].
Sincedifferent machine learning methods provide similarperformances on many NLP tasks, in this paper,we focus our attention on exploring feature spacesand selecting good features for identifying editedregions.
We start by analyzing the distributions ofthe edited regions and their components in thetargeted corpus.
We then design several featurespaces to cover the disfluent regions in the trainingdata.
In addition, we also explore new featurespaces of a part-of-speech hierarchy and extendcandidate pools in the experiments.
These stepsresult in a significant improvement in F-score overthe earlier best result reported in [Charniak andJohnson 2001], where punctuation is included inboth the training and testing data of theSwitchboard corpus, and a significant errorreduction in F-score over the latest best result[Johnson and Charniak 2004], where punctuationis ignored in both the training and testing data ofthe Switchboard corpus.179In this paper, we follow the definition of [Shriberg1994] and others for speech repairs: A speechrepair is divided into three parts: the reparandum,the part that is repaired; the interregnum, the partthat can be either empty or fillers; and therepair/repeat, the part that replaces or repeats thereparandum.
The definition can also beexemplified via the following utterance:Nrepeatreparanda int erregnum,  , this is  a big problem.This is you knowThis paper is organized as follows.
In section 2, weexamine the distributions of the editing regions inSwitchboard data.
Section 3, then, presents theBoosting method, the baseline system and thefeature spaces we want to explore.
Section 4describes, step by step, a set of experiments thatlead to a large performance improvement.
Section5 concludes with discussion and future work.2 Repair Distributions in SwitchboardWe start by analyzing the speech repairs in theSwitchboard corpus.
Switchboard has over onemillion words, with telephone conversations onprescribed topics [Godfrey et al 1992].
It is full ofdisfluent utterances, and [Shriberg 1994, Shriberg1996] gives a thorough analysis and categorizationof them.
[Engel et al 2002] also showed detaileddistributions of the interregnum, includinginterjections and parentheticals.
Since the majorityof the disfluencies involve all the three parts(reparandum, interregnum, and repair/repeat), thedistributions of all three parts will be very helpfulin constructing patterns that are used to identifyedited regions.For the reparandum and repair types, we includetheir distributions with and without punctuation.We include the distributions with punctuation is tomatch with the baseline system reported in[Charniak and Johnson 2001], where punctuationis included to identify the edited regions.
Resentresearch showed that certain punctuation/prosodymarks can be produced when speech signals areavailable [Liu et al 2003].
The interregnum type,by definition, does not include punctuation.The length distributions of the reparanda in thetraining part of the Switchboard data with andwithout punctuation are given in Fig.
1.
Thereparanda with lengths of less than 7 words makeup 95.98% of such edited regions in the trainingdata.
When we remove the punctuation marks,those with lengths of less than 6 words reachroughly 96%.
Thus, the patterns that consider onlyreparanda of length 6 or less will have very goodcoverage.Length distribution of reparanda0%10%20%30%40%50%1 2 3 4 5 6 7 8 9 10With punctation Without punctationFigure 1.
Length distribution of reparanda inSwitchboard training data.Length distribution ofrepairs/repeats/restarts0%10%20%30%40%50%0 1 2 3 4 5 6 7 8 9With punctation Without punctationFigure 2.
Length distribution ofrepairs/repeats/restarts in Switchboard training data.Length distribution of interregna0%20%40%60%80%100%0 1 2 3 4 5 6 7 8 9 10Figure 3.
Length distribution of interregna inSwitchboard training data.The two repair/repeat part distributions in thetraining part of the Switchboard are given in Fig.
2.The repairs/repeats with lengths less than 7 words180make 98.86% of such instances in the training data.This gives us an excellent coverage if we use 7 asthe threshold for constructing repair/repeat patterns.The length distribution of the interregna of thetraining part of the Switchboard corpus is shown inFig.
3.
We see that the overwhelming majority hasthe length of one, which are mostly words such as?uh?, ?yeah?, or ?uh-huh?.In examining the Switchboard data, we noticed thata large number of reparanda and repair/repeat pairsdiffer on less than two words, i.e.
?as to, you know,when to?1, and the amount of the pairs differing onless than two POS tags is even bigger.
There arealso cases where some of the pairs have differentlengths.
These findings provide a good base for ourfeature space.3 Feature Space Selection for BoostingWe take as our baseline system the work by[Charniak and Johnson 2001].
In their approach,rough copy is defined to produce candidates forany potential pairs of reparanda and repairs.
Aboosting algorithm [Schapire and Singer 1999] isused to detect whether a word is edited.
A total of18 variables are used in the algorithm.
In the restof the section, we first briefly introduce theboosting algorithm, then describe the method usedin [Charniak and Johnson 2001], and finally wecontrast our improvements with the baselinesystem.3.1 Boosting AlgorithmIntuitively, the boosting algorithm is to combine aset of simple learners iteratively based on theirclassification results on a set of training data.Different parts of the training data are scaled ateach iteration so that the parts of the data previousclassifiers performed poorly on are weightedhigher.
The weighting factors of the learners areadjusted accordingly.We re-implement the boosting algorithm reportedby [Charniak and Johnson 2001] as our baselinesystem in order to clearly identify contributing1  ?as to?
is the edited region.
Italicized words in theexamples are edited wordsfactors in performance.
Each word token ischaracterized by a finite tuple of random variables(Y, X1,..., Xm ).Y is  the conditioned variables and ranges from{-1,+1}, with Y = +1 indicating that the word isedited.
X1,..., Xm  are the conditioning variables;each variable jX  ranges over a finite set j?
.
Thegoal of the classifer is to predict the value of Ygiven a value for X1,..., Xm .A boosting classifier is a linear combination of nfeatures to define the prediction variable Z.?==niiiFZ1?
(1)where ?i is the weight to be estimated for feature ?i.
?i is a set of variable-value pairs, and each Fi hasthe form of:Fi = (X j = x j )<X j ,x j >??
i?
(2)with X?s being conditioning variables and x?s beingvalues.Each component in the production for Fi  isdefined as:(X j = x j ) =1  < X j = x j >?
?i0   otherwise?
?
?
(3)In other words, Fi is 1 if and only if all thevariable-value pairs for the current position belongto ?i.The prediction made by the classifier is|Z| Z/ sign(Z)= .
Intuitively, our goal is to adjustthe vector of feature weights 1( ,...., )n?
?
?=K  tominimize the expected misclassification rate]E[sign(Z) Y?
.
This function is difficult tominimize, so our boosting classifier minimizes theexpected boost loss )][(exp(-YZE?t  as in [Collins2000], where ][E?t ?
is the expectation on theempirical training corpus distribution.
In ourimplementation, each learner contains only onevariable.
The feature weights are adjustediteratively, one weight per iteration.
At eachiteration, it reduces the boost loss on the trainingcorpus.
In our experiments, ?K is obtained after1811500 iterations, and contains around 1350 non-zerofeature weights.3.2 Charniak-Johnson approachIn [Charniak and Johnson 2001], identifying editedregions is considered as a classification problem,where each word is classified either as edited ornormal.
The approach takes two steps.
The firststep is to find rough copy.
Then, a number ofvariables are extracted for the boosting algorithm.In particular, a total of 18 different conditioningvariables are used to predict whether the currentword is an edited word or a non-edited word.
The18 different variables listed in Table 1 correspondto the 18 different dimensions/factors for thecurrent word position.
Among the 18 variables, sixof them, Nm, Nu, Ni, Nl, Nr and Tf , depend on theidentification of a rough copy.For convenience, their definition of a rough copy isrepeated here.
A rough copy in a string of taggedwords has the form of 21 ??
??
, where:1.
1?
(the source) and 2?
(the copy) bothbegin   with    non-punctuation,2.
the strings of non-punctuation POS tagof   1?
and 2?
are identical,3.
?
(the free final) consists of zero ormore sequences of a free final word  (seebelow) followed by optional punctuation,4.
?
(the interregnum) consists ofsequences of an interregnum string (seebelow) followed by optional punctuation.The set of free final words includes all partialwords and a small set of conjunctions, adverbs andmiscellanea.
The set of interregnum stringsconsists of a small set of expressions such as uh,you know, I guess, I mean, etc.3.3 New ImprovementsOur improvements to the Charniak-Johnsonmethod can be classified into three categories withthe first two corresponding to the twp steps in theirmethod.
The three categories of improvements aredescribed in details in the following subsections.3.3.1 Relaxing Rough CopyWe relax the definition for rough copy, becausemore than 94% of all edits have both reparandumand repair, while the rough copy defined in[Charniak and Johnson 2001] only covers 77.66%of such instances.Two methods are used to relax the rough copydefinition.
The first one is to adopt a hierarchicalPOS tag set: all the Switchboard POS tags arefurther classified into four major categories: N(noun related), V (verb related), Adj (nounmodifiers), Adv (verb modifiers).
Instead ofrequiring the exact match of two POS tagsequences, we also consider two sequences as aVariables Name Short descriptionX1 W0 The current orthographic word.X2 ?
X5 P0,P1,P2,Pf Partial word flags for the current position, the next two to the right, and the first onein a sequence of free-final words (partial, conjunctions, etc.)
to the right of thecurrent position.X6 ?
X10 T-1,T0,T1,T2,Tf Part of speech tags for the left position, the current position, the next two positionsto the right, and the first free-final word position to the right of the current position.X11 Nm Number of words in common in reparandum and repairX12 Nn Number of words in reparandum but not repairX13 Ni Number of words in interregnumX14 Nl Number of words to the left edge of reparandumX15 Nr Number of words to the right edge of reparandumX16 Ct The first non-punctuation tag to the right of the current positionX17 Cw The first non-punctuation word to the right of the current positionX18 Ti The tag of the first word right after the interregnum that is right after the currentword.Table 1.
Descriptions of the 18 conditioning variables from [Charniak and Johnson 2001]182rough copy if their corresponding major categoriesmatch.
This relaxation increases the rough copycoverage, (the percent of words in edited regionsfound through the definition of rough copy), from77.66% to 79.68%.The second is to allow one mismatch in the twoPOS sequences.
The mismatches can be anaddition, deletion, or substitution.
This relaxationimproves the coverage from 77.66% to 85.45%.Subsequently, the combination of the tworelaxations leads to a significantly higher coverageof 87.70%.
Additional relaxation leads to excessivecandidates and worse performance in thedevelopment set.3.3.2 Adding New FeaturesWe also include new features in the feature set:one is the shortest distance (the number of words)between the current word and a word of the sameorthographic form to the right, if that repeatedword exists; another is the words around thecurrent position.
Based on the distributionalanalysis in section 2, we also increase the windowsizes for POS tags ( 5 5,...,T T? )
and words( 5 5,...,W W? )
to ?5 and partial words ( 3 3,...,P P?
)to ?3, extending Ti and Pj.3.3.3 Post Processing StepIn addition to the two categories, we try to usecontextual patterns to address the independency ofvariables in the features.
The patterns have beenextracted from development and training data, todeal with certain sequence-related errors, e.g.,E N E ?
E E E,which means that if the neighbors on both sides ofa word are classified into EDITED, it should beclassified into EDITED as well.4 Experimental ResultsWe conducted a number of experiments to test theeffectiveness of our feature space exploration.Since the original code from [Charniak andJohnson 2001] is not available, we conducted ourfirst experiment to replicate the result of theirbaseline system described in section 3.
We usedthe exactly same training and testing data from theSwitchboard corpus as in [Charniak and Johnson2001].
The training subset consists of all files inthe sections 2 and 3 of the Switchboard corpus.Section 4 is split into three approximately equalsize subsets.
The first of the three, i.e., filessw4004.mrg to sw4153.mrg, is the testing corpus.The files sw4519.mrg to sw4936.mrg are thedevelopment corpus.
The rest files are reserved forother purposes.
When punctuation is included inboth training and testing, the re-establishedbaseline has the precision, recall, and F-score of94.73%, 68.71% and 79.65%, respectively.
Theseresults are comparable with the results from[Charniak & Johnson 2001], i.e., 95.2%, 67.8%,and 79.2% for precision, recall, and f-score,correspondingly.In the subsequent experiments, the set of additionalfeature spaces described in section 3 are added,step-by-step.
The first addition includes theshortest distance to the same word and windowsize increases.
This step gives a 2.27%improvement on F-score over the baseline.
Thenext addition is the introduction of the POShierarchy in finding rough copies.
This also givesmore than 3% absolute improvement over thebaseline and 1.19% over the expanded feature setmodel.
The addition of the feature spaces ofrelaxed matches for words, POS tags, and POShierarchy tags all give additive improvements,which leads to an overall of 8.95% absoluteimprovement over the re-implemented baseline, or43.98% relative error reduction on F-score.When compared with the latest results from[Johnson and Charniak 2004], where nopunctuations are used for either training or testingdata, we also observe the same trend of theimproved results.
Our best result gives 4.15%absolute improvement over their best result, or20.44% relative error reduction in f-scores.
As asanity check, when evaluated on the training dataas a cheating experiment, we show a remarkableconsistency with the results for testing data.For error analysis, we randomly selected 100sentences with 1673 words total from the testsentences that have at least one mistake.
Errors canbe divided into two types, miss (should be edited)and false alarm (should be noraml).
Among the207 misses, about 70% of them require somephrase level analysis or acoustic cues for phrases.183For example, one miss is ?because of the friendsbecause of many other things?, an error we wouldhave a much better chance of correct identification,if we were able to identify prepositional phrasesreliably.
Another example is ?most of all myfamily?.
Since it is grammatical by itself, certainprosodic information in between ?most of?
and ?allmy family?
may help the identification.
[Ostendorfet al 2004] reported that interruption point couldhelp parsers to improve results.
[Kahn et al 2005]also showed that prosody information could helpparse disfluent sentences.
The second major classof the misses is certain short words that are notlabeled consistently in the corpus.
For example,?so?, ?and?, and ?or?, when they occur in thebeginning of a sentence, are sometimes labeled asedited, and sometimes just as normal.
The lastcategory of the misses, about 5.3%, contains theones where the distances between reparanda andrepairs are often more than 10 words.Among the 95 false alarms, more than threequarters of misclassified ones are related to certaingrammatical constructions.
Examples include caseslike, ?the more ?
the more?
and ?I think Ishould ??.
These cases may be fixable if moreelaborated grammar-based features are used.5 ConclusionsThis paper reports our work on identifying editedregions in the Switchboard corpus.
In addition to aResults on testing data Results on training datawith punctuation Punctuation on both  No punctuation on bothMethod codesPrecision Recall f-score Precision Recall f-score Precision Recall f-scoreCJ?01    95.2 67.8 79.2JC?04 p       82.0 77.8 79.7R CJ?01 94.9 71.9 81.81 94.73 68.71 79.65 91.46 64.42 75.59+d 94.56 78.37 85.71 94.47 72.31 81.92 91.79 68.13 78.21+d+h 94.23 81.32 87.30 94.58 74.12 83.11 91.56 71.33 80.19+d+rh 94.12 82.61 87.99 92.61 77.15 84.18 89.92 72.68 80.39+d+rw 96.13 82.45 88.77 94.79 75.43 84.01 92.17 70.79 80.08+d+rw+rh 94.42 84.67 89.28 94.57 77.93 85.45 92.61 73.46 81.93+d+rw+rt+wt 94.43 84.79 89.35 94.65 76.61 84.68 92.08 72.61 81.19+d+rw+rh+wt 94.58 85.21 89.65 94.72 79.22 86.28 92.69 75.30 83.09+d+rw+rh+wt+ps 93.69 88.62 91.08 93.81 83.94 88.60 89.70 78.71 83.85Table 2.
Result summary for various feature spaces.Method codes Method descriptionCJ?01 Charniak and Johnson 2001JC?04 p Johnson and Charniak 2004, parser resultsR CJ?01 Duplicated results for Charniak and Johnson 2001+d Distance + window sizes+d+h Distance + window sizes + POS hierarchy in rough copy+d+rh Distance + window sizes + relaxed POS hierarchy in rough copy+d+rw Distance + window sizes + relaxed word in rough copy+d+rw+rh Distance + window sizes + relaxed word and POS hierarchy in rough copy+d+rw+rt+wt Distance + window sizes + word & tag pairs + relaxed word and POS in rough copy+d+rw+rh+wt Distance + window sizes + word & tag pairs + relaxed word and POS hierarchy inrough copy+d+rw+rh+wt+ps Distance + window sizes + word & tag pairs + relaxed word and POS hierarchy inrough copy + pattern substitutionTable 3.
Description of method codes used in the result table.184distributional analysis for the edited regions, anumber of feature spaces have been explored andtested to show their effectiveness.
We observed a43.98% relative error reduction on F-scores for thebaseline with punctuation in both training andtesting [Charniak and Johnson 2001].
Comparedwith the reported best result, the same approachproduced a 20.44% of relative error reduction onF-scores when punctuation is ignored in trainingand testing data [Johnson and Charniak 2004].
Theinclusion of both hierarchical POS tags and therelaxation for rough copy definition gives largeadditive improvements, and their combination hascontributed to nearly half of the gain for the testset with punctuation and about 60% of the gain forthe data without punctuation.Future research would include the use of otherfeatures, such as prosody, and the integration ofthe edited region identification with parsing.6 AcknowledgementThis work has been done while the first author isworking at the Research and Technology Center ofRobert Bosch Corp.
The research is partlysupported by the NIST ATP program.
The authorswould also like to express their thanks to TessHand-Bender for her proof-reading and Jeremy G.Kahn for many useful comments.
Nevertheless, allthe remaining errors are ours.ReferencesJohn Bear, John Dowding and Elizabeth Shriberg.
1992.Integrating Multiple Knowledge Sources for Detectionand Correction of Repairs in Human-Computer Dialog.Proc.
Annual Meeting of the Association forComputational Linguistics.
1992.Charniak, Eugene and Mark Johnson.
2001.
EditDetection and Parsing for Transcribed Speech.
Proc.
ofthe 2nd Meeting of the North American Chapter of theAssociation for Computational Linguistics, pp 118-126.Collins, M. 2000.
Discriminative reranking for naturallanguage parsing.
Proc.
ICML 2000.Engel, Donald, Eugene Charniak, and Mark Johnson.2002.
Parsing and Disfluency Placement.
Proc.EMNLP, pp 49-54, 2002.Godfrey, J.J., Holliman, E.C.
and McDaniel, J.SWITCHBOARD: Telephone speech corpus forresearch and development, Proc.
ICASSP, pp 517-520,1992.Heeman, Peter, and James Allen.
1994.
Detecting andCorrecting Speech Repairs.
Proc.
of the annual meetingof the Association for Computational Linguistics.
LasCruces, New Mexico,  pp 295-302, 1994.Johnson, Mark, and Eugene Charniak.
2004.
A TAG-based noisy-channel model of speech repairs.
Proc.
ofthe 42nd Annual Meeting of the Association forComputational Linguistics.Kahn, Jeremy G., Mari Ostendorf, and Ciprian Chelba.2004.
Parsing Conversational Speech Using EnhancedSegmentation.
Proc.
of HLT-NAACL, pp 125-138, 2004.Kahn, Jeremy G., Matthew Lease, Eugene Charniak,Mark Johnson and Mari Ostendorf 2005.
Effective Useof Prosody in Parsing Conversational Speech.
Proc.EMNLP, 2005.Liu, Yang, Elizabeth Shriberg, Andreas Stolcke,Barbara Peskin, Jeremy Ang, Dustin Hillard, MariOstendorf, Marcus Tomalin, Phil Woodland, MaryHarper.
2005.
Structural Metadata Research in theEARS Program.
Proc.
ICASSP, 2005.Liu, Yang, Elizabeth Shriberg, Andreas Stolcke.
2003.Automatic disfluency identification in conversationalspeech using multiple knowledge sources Proc.Eurospeech, 2003Ostendorf, Mari, Jeremy G. Kahn, Darby Wong, DustinHillard, and William McNeill.
Leveraging StructuralMDE in Language Processing.
EARS RT04 Workshop,2004.Robert E. Schapire and Yoram Singer, 1999.
ImprovedBoosting Algorithms Using Confidence-ratedPredictions.
Machine Learning 37(3): 297-336, 1999.Shriberg, Elizabeth.
1994.
Preliminaries to a Theory ofSpeech Disfluencies.
Ph.D. Thesis.
UC Berkeley,1994.Shriberg, Elizabeth.
1996.
Disfluencies in Switchboard.Proc.
of ICSLP.
1996.Young, S. R. and Matessa, M. (1991).
Using pragmaticand semantic knowledge to correct parsing of spokenlanguage utterances.
Proc.
Eurospeech 91, Genova,Italy.185
