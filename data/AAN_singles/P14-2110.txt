Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674?679,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsLearning Polylingual Topic Models fromCode-Switched Social Media DocumentsNanyun Peng Yiming Wang Mark DredzeHuman Language Technology Center of ExcellenceCenter for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD USA{npeng1,freewym,mdredze}@jhu.eduAbstractCode-switched documents are commonin social media, providing evidence forpolylingual topic models to infer alignedtopics across languages.
We presentCode-Switched LDA (csLDA), which in-fers language specific topic distributionsbased on code-switched documents to fa-cilitate multi-lingual corpus analysis.
Weexperiment on two code-switching cor-pora (English-Spanish Twitter data andEnglish-Chinese Weibo data) and showthat csLDA improves perplexity overLDA, and learns semantically coherentaligned topics as judged by human anno-tators.1 IntroductionTopic models (Blei et al, 2003) have become stan-dard tools for analyzing document collections, andtopic analyses are quite common for social media(Paul and Dredze, 2011; Zhao et al, 2011; Hongand Davison, 2010; Ramage et al, 2010; Eisen-stein et al, 2010).
Their popularity owes in part totheir data driven nature, allowing them to adapt tonew corpora and languages.
In social media espe-cially, there is a large diversity in terms of both thetopic and language, necessitating the modeling ofmultiple languages simultaneously.
A good candi-date for multi-lingual topic analyses are polylin-gual topic models (Mimno et al, 2009), whichlearn topics for multiple languages, creating tuplesof language specific distributions over monolin-gual vocabularies for each topic.
Polylingual topicmodels enable cross language analysis by group-ing documents by topic regardless of language.Training of polylingual topic models requiresparallel or comparable corpora: document tuplesfrom multiple languages that discuss the sametopic.
While additional non-aligned documentsUser 1: ?Don Samuel es un crack!
#VamosM?exico #DaleTriRT @User4: Arriba!
Viva Mexico!
Advanced to GOLD.medal match in ?Football?
!User 2: @user1 rodo que tal el nuevo Mountain ?User 3: @User1 @User4 wow this is something !!
Ja ja jaFootball well saidFigure 1: Three users discuss Mexico?s footballteam advancing to the Gold medal game in the2012 Olympics in code-switched Spanish and En-glish.can be folded in during training, the ?glue?
doc-uments are required to aid in the alignment acrosslanguages.
However, the ever changing vocabu-lary and topics of social media (Eisenstein, 2013)make finding suitable comparable corpora diffi-cult.
Standard techniques ?
such as relying on ma-chine translation parallel corpora or comparabledocuments extracted from Wikipedia in differentlanguages ?
fail to capture the specific terminol-ogy of social media.
Alternate methods that relyon bilingual lexicons (Jagarlamudi and Daum?e,2010) similarly fail to adapt to shifting vocabular-ies.
The result: an inability to train polylingualmodels on social media.In this paper, we offer a solution: utilize code-switched social media to discover correlationsacross languages.
Social media is filled with ex-amples of code-switching, where users switch be-tween two or more languages, both in a conversa-tion and even a single message (Ling et al, 2013).This mixture of languages in the same context sug-gests alignments between words across languagesthrough the common topics discussed in the con-text.We learn from code-switched social media byextending the polylingual topic model frameworkto infer the language of each token and then auto-matically processing the learned topics to identifyaligned topics.
Our model improves both in termsof perplexity and a human evaluation, and we pro-vide some example analyses of social media thatrely on our learned topics.6742 Code-SwitchingCode-switched documents has received consider-able attention in the NLP community.
Severaltasks have focused on identification and analysis,including mining translations in code-switcheddocuments (Ling et al, 2013), predicting code-switched points (Solorio and Liu, 2008a), identi-fying code-switched tokens (Lignos and Marcus,2013; Yu et al, 2012; Elfardy and Diab, 2012),adding code-switched support to language mod-els (Li and Fung, 2012), linguistic processing ofcode switched data (Solorio and Liu, 2008b), cor-pus creation (Li et al, 2012; Diab and Kamboj,2011), and computational linguistic analyses andtheories of code-switching (Sankofl, 1998; Joshi,1982).Code-switching specifically in social media hasalso received some recent attention.
Lignos andMarcus (2013) trained a supervised token levellanguage identification system for Spanish andEnglish code-switched social media to study code-switching behaviors.
Ling et al (2013) minedtranslation spans for Chinese and English in code-switched documents to improve a translation sys-tem, relying on an existing translation model to aidin the identification and extraction task.
In contrastto this work, we take an unsupervised approach,relying only on readily available document levellanguage ID systems to utilize code-switched data.Additionally, our focus is not on individual mes-sages, rather we aim to train a model that can beused to analyze entire corpora.In this work we consider two types of code-switched documents: single messages and conver-sations, and two language pairs: Chinese-Englishand Spanish-English.
Figure 1 shows an exam-ple of a code-switched Spanish-English conversa-tion, in which three users discuss Mexico?s foot-ball team advancing to the Gold medal game inthe 2012 Summer Olympics.
In this conversation,some tweets are code-switched and some are in asingle language.
By collecting the entire conver-sation into a single document we provide the topicmodel with additional content.
An example of aChinese-English code-switched messages is givenby Ling et al (2013):watup Kenny Mayne!!
- Kenny Mayne??????!
!Here a user switches between languages in a singlemessage.
We empirically evaluate our model onboth conversations and messages.
In the modelpresentation we will refer to both as ?documents.
?3 csLDATo train a polylingual topic model on social me-dia, we make two modifications to the model ofMimno et al (2009): add a token specific languagevariable, and a process for identifying aligned top-ics.First, polylingual topic models require paral-lel or comparable corpora in which each docu-ment has an assigned language.
In the case ofcode-switched social media data, we require a per-token language variable.
However, while docu-ment level language identification (LID) systemsare common place, very few languages have per-token LID systems (King and Abney, 2013; Lig-nos and Marcus, 2013).To address the lack of available LID systems,we add a per-token latent language variable to thepolylingual topic model.
For documents that arenot code-switched, we observe these variables tobe the output of a document level LID system.
Inthe case of code-switched documents, these vari-ables are inferred during model inference.Second, polylingual topic models assume thealigned topics are from parallel or comparable cor-pora, which implicitly assumes that a topics pop-ularity is balanced across languages.
Topics thatshow up in one language necessarily show up inanother.
However, in the case of social media,we can make no such assumption.
The topicsdiscussed are influenced by users, time, and lo-cation, all factors intertwined with choice of lan-guage.
For example, English speakers will morelikely discuss Olympic basketball while Spanishspeakers football.
There may be little or no docu-ments on a given topic in one language, while theyare plentiful in another.
In this case, a polylin-gual topic model, which necessarily infers a topic-specific word distribution for each topic in eachlanguage, would learn two unrelated word dis-tributions in two languages for a single topic.Therefore, naively using the produced topics as?aligned?
across languages is ill-advised.Our solution is to automatically identify alignedpolylingual topics after learning by examininga topic?s distribution across code-switched docu-ments.
Our metric relies on distributional proper-ties of an inferred topic across the entire collec-tion.675To summarize, based on the model of Mimno etal.
(2009) we will learn:?
For each topic, a language specific word distri-bution.?
For each (code-switched) token, a language.?
For each topic, an identification as to whetherthe topic captures an alignment across lan-guages.The first two goals are achieved by incorporat-ing new hidden variables in the traditional polylin-gual topic model.
The third goal requires an auto-mated post-processing step.
We call the resultingmodel Code-Switched LDA (csLDA).
The gener-ative process is as follows:?
For each topic z ?
T?
For each language l ?
L?
Draw word distribution?lz?Dir(?l)?
For each document d ?
D:?
Draw a topic distribution ?d?
Dir(?)?
Draw a language distribution?d?Dir(?)?
For each token i ?
d:?
Draw a topic zi?
?d?
Draw a language li?
?d?
Draw a word wi?
?lzFor monolingual documents, we fix lito the LIDtag for all tokens.
Additionally, we use a singlebackground distribution for each language to cap-ture stopwords; a control variable pi, which fol-lows a Dirichlet distribution with prior parameter-ized by ?, is introduced to decide the choice be-tween background words and topic words follow-ing (Chemudugunta et al, 2006)1.
We use asym-metric Dirichlet priors (Wallach et al, 2009), andlet the optimization process learn the hyperparam-eters.
The graphical model is shown in Figure 2.3.1 InferenceInference for csLDA follows directly from LDA.A Gibbs sampler learns the word distributions ?lzfor each language and topic.
We use a block Gibbssampler to jointly sample topic and language vari-ables for each token.
As is customary, we collapseout ?, ?
and ?.
The sampling posterior is:P (zi, li|w, z?i, l?i, ?, ?, ?)
?
(nl,zwi)?i+ ?nl,z?i+W?
?mz,d?i+ ?md?i+ T ?
?ol,d?i+ ?od?i+ L?
(1)where (nl,zwi)?iis the number of times the type forword wiassigned to topic z and language l (ex-1Omitted from the generative process but shown in Fig.
2.?
?li?d?d?lz?lb?B?zibiwiDNLTFigure 2: The graphical model for csLDA.cluding current word wi), mz,d?iis the number oftokens assigned to topic z in document d (exclud-ing current word wi), ol,d?iis the number of tokensassigned to language l in document d (excludingcurrent word wi), and these variables with super-scripts or subscripts omitted are totals across allvalues for the variable.
W is the number of wordsin the corpus.
All counts omit words assignedto the background.
During sampling, words arefirst assigned to the background/topic distributionand then topic and language are sampled for non-background words.We optimize the hyperparameters ?, ?, ?
and ?by interleaving sampling iterations with a Newton-Raphson update to obtain the MLE estimate forthe hyperparameters.
Taking ?
as an example, onestep of the Newton-Raphson update is:?new= ?old?H?1?L??
(2)where H is the Hessian matrix and?L?
?is the gra-dient of the likelihood function with respect tothe optimizing hyperparameter.
We interleave 200sampling iterations with one Newton-Raphson up-date.3.2 Selecting Aligned TopicsWe next identify learned topics (a set of relatedword-distributions) that truly represent an alignedtopic across languages, as opposed to an unrelatedset of distributions for which there is no support-ing alignment evidence in the corpus.
We begin bymeasuring how often each topic occurs in code-switched documents.
If a topic never occurs ina code-switched document, then there can be noevidence to support alignment across languages.For the topics that appear at least once in a code-switched document, we estimate their probability676in the code-switched documents by a MAP esti-mate of ?.
Topics appearing in at least one code-switched document with probability greater thana threshold p are selected as candidates for truecross-language topics.4 DataWe used two datasets: a Sina Weibo Chinese-English corpus (Ling et al, 2013) and a Spanish-English Twitter corpus.Weibo Ling et al (2013) extracted over 1mChinese-English parallel segments from SinaWeibo, which are code-switched messages.
Werandomly sampled 29,705 code-switched mes-sages along with 42,116 Chinese and 42,116 En-glish messages from the the same time frame.
Weused these data for training.
We then sampledan additional 2475 code-switched messages, 4221English and 4211 Chinese messages as test data.Olympics We collected tweets from July 27,2012 to August 12, 2012, and identified 302,775tweets about the Olympics based on related hash-tags and keywords (e.g.
olympics, #london2012,etc.)
We identified code-switched tweets usingthe Chromium Language Detector2.
This systemprovides the top three possible languages for agiven document with confidence scores; we iden-tify a tweet as code-switched if two predicted lan-guages each have confidence greater than 33%.We then used the tagger of Lignos and Marcus(2013) to obtain token level LID tags, and onlytweets with tokens in both Spanish and English areused as code-switched tweets.
In total we iden-tified 822 Spanish-English code-switched tweets.We further expanded the mined tweets to full con-versations, yielding 1055 Spanish-English code-switched documents (including both tweets andconversations), along with 4007 English and 4421Spanish tweets composes our data set.
We reserve10% of the data for testing.5 ExperimentsWe evaluated csLDA on the two datasets and eval-uated each model using perplexity on held out dataand human judgements.
While our goal is to learnpolylingual topics, we cannot compare to previouspolylingual models since they require comparabledata, which we lack.
Instead, we constructed abaseline from LDA run on the entire dataset (no2https://code.google.com/p/chromium-compact-language-detector/language information.)
For each model, we mea-sured the document completion perplexity (Rosen-Zvi et al, 2004) on the held out data.
We ex-perimented with different numbers of topics (T ).Since csLDA duplicates topic distributions (T ?L)we used twice as many topics for LDA.Figure 3 shows test perplexity for varying T andperplexity for the best setting of csLDA (T =60)and LDA (T =120).
The table lists both mono-lingual and code-switched test data; csLDA im-proves over LDA in almost every case, and acrossall values of T .
The background distribution (-bg)has mixed results for LDA, whereas for csLDAit shows consistent improvement.
Table 4 showssome csLDA topics.
While there are some mis-takes, overall the topics are coherent and aligned.We use the available per-token LID system(Lignos and Marcus, 2013) for Spanish/Englishto justify csLDA?s ability to infer the hidden lan-guage variables.
We ran csLDA-bg with liset tothe value provided by the LID system for code-switched documents (csLDA-bg with LID), whichgives csLDA high quality LID labels.
While wesee gains for the code-switched data, overall theresults for csLDA-bg and csLDA-bg with LID aresimilar, suggesting that the model can operate ef-fectively even without a supervised per-token LIDsystem.5.1 Human EvaluationWe evaluate topic alignment quality through a hu-man judgements (Chang et al, 2009).
For eachaligned topic, we show an annotator the 20 mostfrequent words from the foreign language topic(Chinese or Spanish) with the 20 most frequentwords from the aligned English topic and two ran-dom English topics.
The annotators are asked toselect the most related English topic among thethree; the one with the most votes is consideredthe aligned topic.
We count how often the model?salignments agree.LDA may learn comparable topics in differentlanguages but gives no explicit alignments.
Wecreate alignments by classifying each LDA topicby language using the KL-divergence between thetopic?s words distribution and a word distributionfor the English/foreign language inferred from themonolingual documents.
Language is assigned toa topic by taking the minimum KL.
For Weibodata, this was not effective since the vocabulariesof each language are highly unbalanced.
Instead,67720/40 30/60 40/80 50/100 60/120 70/140# Topics800085009000950010000PerplexityLDALDA-bgcsLDA-bg with LIDcsLDA-bgcsLDA20/40 30/60 40/80 50/100 60/120 70/140# Topics18000200002200024000260002800030000PerplexityLDALDA-bgcsLDA-bgcsLDAT =60/120 Olympics WeiboEn Es CS En Cn CSLDA 11.32 9.44 6.97 29.19 23.06 11.69LDA-bg 11.35 9.51 6.79 40.87 27.56 10.91csLDA 8.72 7.94 6.17 18.20 17.31 12.72csLDA-bg 8.72 7.73 6.04 18.25 17.74 12.46csLDA-bg 8.73 7.93 4.91 - - -with LIDFigure 3: Plots show perplexity for different T (Olympics left, Weibo right).
Perplexity in the table arein magnitude of 1?
103.Football BasketballEnglish Spanish English Spanishmexico mucho game espa?nabrazil argentina basketball baloncestosoccer m?exico year basketballvs brasil finals broncewomens ganar?a gonna chinafootball tri nba finalmens yahel castillo obama rusiafinal delpo lebron espa?nolaSocial Media TransportationEnglish Chinese English Chinesetwitter ???
car ?
?bitly ??
drive ?
?facebook ??
road ?
?check ??
line ?
?use ??
train ??
?blog ??
harry ?
?free pm ??
?
?post ??
bus ?
?Figure 4: Examples of aligned topics from Olympics (left) and Weibo (right).we manually labeled the topics by language.
Wethen pair topics across languages using the cosinesimilarity of their co-occurrence statistics in code-switched documents.
Topic pairs with similarityabove t are considered aligned topics.
We alsoused a threshold p (?3.2) to select aligned topicsin csLDA.
To ensure a fair comparison, we selectthe same number of aligned topics for LDA andcsLDA.3.
We used the best performing setting:csLDA T =60, LDA T =120, which produced 12alignments from Olympics and 28 from Weibo.Using Mechanical Turk we collected multiplejudgements per alignment.
For Spanish, we re-moved workers who disagreed with the majoritymore than 50% of the time (83 deletions), leav-ing 6.5 annotations for each alignment (85.47%inter-annotator agreement.)
For Chinese, sincequality of general Chinese turkers is low (Pavlicket al, 2014) we invited specific workers andobtained 9.3 annotations per alignment (78.72%inter-annotator agreement.)
For Olympics, LDAalignments matched the judgements 25% of thetime, while csLDA matched 50% of the time.While csLDA found 12 alignments and LDA 29,the 12 topics evaluated from both models showthat csLDA?s alignments are higher quality.
Forthe Weibo data, LDA matched judgements 71.4%,while csLDA matched 75%.
Both obtained high3We used thresholds p = 0.2 and t = 0.0001.
We limitedthe model with more alignments to match the one with less.quality alignments ?
likely due both to the factthat the code-switched data is curated to find trans-lations and we hand labeled topic language ?
butcsLDA found many more alignments: 60 as com-pared to 28.
These results confirm our automatedresults: csLDA finds higher quality topics thatspan both languages.ReferencesDavid M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
Journal of Ma-chine Learning Research (JMLR), 3:993?1022.Jonathan Chang, Sean Gerrish, Chong Wang, Jordan LBoyd-graber, and David M Blei.
2009.
Readingtea leaves: How humans interpret topic models.
InAdvances in neural information processing systems,pages 288?296.Chaitanya Chemudugunta, Padhraic Smyth, and MarkSteyvers.
2006.
Modeling general and specific as-pects of documents with a probabilistic topic model.In NIPS.Mona Diab and Ankit Kamboj.
2011.
Feasibility ofleveraging crowd sourcing for the creation of a largescale annotated resource for Hindi English codeswitched data: A pilot annotation.
In Proceedingsof the 9th Workshop on Asian Language Resources,pages 36?40, Chiang Mai, Thailand, November.Asian Federation of Natural Language Processing.Jacob Eisenstein, Brendan O?Connor, Noah A Smith,and Eric P Xing.
2010.
A latent variable model678for geographic lexical variation.
In Proceedings ofthe 2010 Conference on Empirical Methods in Nat-ural Language Processing, pages 1277?1287.
Asso-ciation for Computational Linguistics.Jacob Eisenstein.
2013.
What to do about bad lan-guage on the internet.
In NAACL.Heba Elfardy and Mona Diab.
2012.
Token levelidentification of linguistic code switching.
In Pro-ceedings of COLING 2012: Posters, pages 287?296,Mumbai, India, December.
The COLING 2012 Or-ganizing Committee.Liangjie Hong and Brian D Davison.
2010.
Empiricalstudy of topic modeling in twitter.
In Proceedings ofthe First Workshop on Social Media Analytics, pages80?88.
ACM.Jagadeesh Jagarlamudi and Hal Daum?e.
2010.
Ex-tracting multilingual topics from unaligned compa-rable corpora.
Advances in Information Retrieval,pages 444?456.Aravind K Joshi.
1982.
Processing of sentenceswith intra-sentential code-switching.
In Proceed-ings of the 9th Conference on Computational lin-guistics (COLING), pages 145?150.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents us-ing weakly supervised methods.
In NAACL.Ying Li and Pascale Fung.
2012.
Code-switch lan-guage model with inversion constraints for mixedlanguage speech recognition.
In Proceedings ofCOLING 2012, pages 1671?1680, Mumbai, India,December.
The COLING 2012 Organizing Commit-tee.Ying Li, Yue Yu, and Pascale Fung.
2012.
Amandarin-english code-switching corpus.
In Nico-letta Calzolari, Khalid Choukri, Thierry Declerck,Mehmet U?gur Do?gan, Bente Maegaard, Joseph Mar-iani, Jan Odijk, and Stelios Piperidis, editors, Pro-ceedings of the Eighth International Conference onLanguage Resources and Evaluation (LREC-2012),pages 2515?2519, Istanbul, Turkey, May.
EuropeanLanguage Resources Association (ELRA).
ACLAnthology Identifier: L12-1573.Constantine Lignos and Mitch Marcus.
2013.
To-ward web-scale analysis of codeswitching.
In An-nual Meeting of the Linguistic Society of America.Wang Ling, Guang Xiang, Chris Dyer, Alan Black, andIsabel Trancoso.
2013.
Microblogs as parallel cor-pora.
In Proceedings of the 51st Annual Meetingon Association for Computational Linguistics, ACL?13.
Association for Computational Linguistics.David Mimno, Hanna M Wallach, Jason Naradowsky,David A Smith, and Andrew McCallum.
2009.Polylingual topic models.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing: Volume 2-Volume 2, pages880?889.
Association for Computational Linguis-tics.Michael J Paul and Mark Dredze.
2011.
You are whatyou tweet: Analyzing twitter for public health.
InICWSM.Ellie Pavlick, Matt Post, Ann Irvine, Dmitry Kachaev,and Chris Callison-Burch.
2014.
The language de-mographics of Amazon Mechanical Turk.
Transac-tions of the Association for Computational Linguis-tics, 2(Feb):79?92.Daniel Ramage, Susan T Dumais, and Daniel JLiebling.
2010.
Characterizing microblogs withtopic models.
In ICWSM.Michal Rosen-Zvi, Thomas Griffiths, Mark Steyvers,and Padhraic Smyth.
2004.
The author-topic modelfor authors and documents.
In Proceedings of the20th conference on Uncertainty in artificial intelli-gence, pages 487?494.
AUAI Press.David Sankofl.
1998.
The production of code-mixeddiscourse.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics, Volume 1, pages 8?21, Montreal,Quebec, Canada, August.
Association for Computa-tional Linguistics.Thamar Solorio and Yang Liu.
2008a.
Learning topredict code-switching points.
In Proceedings ofthe 2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 973?981, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.Thamar Solorio and Yang Liu.
2008b.
Part-of-Speechtagging for English-Spanish code-switched text.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing, pages1051?1060, Honolulu, Hawaii, October.
Associa-tion for Computational Linguistics.Hanna M Wallach, David M Mimno, and Andrew Mc-Callum.
2009.
Rethinking lda: Why priors matter.In NIPS, volume 22, pages 1973?1981.Liang-Chih Yu, Wei-Cheng He, and Wei-Nan Chien.2012.
A language modeling approach to identify-ing code-switched sentences and words.
In Pro-ceedings of the Second CIPS-SIGHAN Joint Confer-ence on Chinese Language Processing, pages 3?8,Tianjin, China, December.
Association for Compu-tational Linguistics.Wayne Xin Zhao, Jing Jiang, Jianshu Weng, JingHe, Ee-Peng Lim, Hongfei Yan, and Xiaoming Li.2011.
Comparing twitter and traditional media us-ing topic models.
In Advances in Information Re-trieval, pages 338?349.
Springer.679
