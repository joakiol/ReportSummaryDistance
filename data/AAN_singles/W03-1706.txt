The Effect of Rhythm on Structural Disambiguation in ChineseHonglin Sun                     Dan JurafskyCenter for Spoken Language ResearchUniversity of Colorado at Boulder{honglin.sun, jurafsky}@colorado.eduAbstractThe length of a constituent (number ofsyllables in a word or number of words in aphrase), or rhythm, plays an important rolein Chinese syntax.
This paper systematicallysurveys the distribution of rhythm inconstructions in Chinese from the statisticaldata acquired from a shallow tree bank.Based on our survey, we then used therhythm feature in a practical shallow parsingtask by using rhythm as a statistical featureto augment a PCFG model.
Our results showthat using the probabilistic rhythm featuresignificantly improves the performance ofour shallow parser.1 IntroductionSyntactic research indicates that prosodic features,including stress, rhythm, intonation, and others,have an impact on syntactic structure.
For example,normally in a coordination construction like ?Aand B?, A and B are interchangeable, that is to say,you can say ?B and A?
and the change of wordorder does not change the meaning.
However,sometimes A and B are not interchangeable.
Quirket al(1985) gives the following examples:man and woman          * woman and manladies and gentleman   *gentleman and ladiesObviously, the examples above cannot beexplained by gender preference.
A reasonableexplanation is that the length of the words (perhapsin syllables) is playing a role; the first constituenttends to be shorter than the second constituent.This feature of the length in syllables of aconstituent plays an even more important role inChinese syntax than in English (Feng, 2000).
Forexample, in the verb-object construction inChinese, there is a preference for the object to beequal to or longer than the verb.
Thus while both???
(plant)  and  ????
(plant) are verbs and havethe same meaning,  ?
?
/plant ?
/tree?
isgrammatical while ?
?
?
/plant ?
/tree?
isungrammatical.
However, both verbs allow bi-syllabic nouns as objects (e.g., ????
(fruit tree),????
(cotton) etc.).
The noun phrases formed by?noun + verb?
give us another example in whichrhythm feature places constraints on syntax, asindicated in the following examples(ungrammatical with *):?
?/cotton   ??/planting*?
?/cotton   ?/planting*?/flower       ?
?/planting*?/flower       ?/planting??
?/cotton   ??/planting?
is grammatical but??
?/cotton   ?/planting?
, ?
?/flower   ??/planting?
and ?
?/flower   ?/planting?
are allungrammatical, although ???
/cotton?
and ??/flour?
, ???/planting?
and ??/planting?
havethe same POS and the same or similar meaning.The only difference lies in that they have differentnumber of syllables or different length.This paper systematically surveys the effect ofrhythm on Chinese syntax from the statistical datafrom a shallow tree bank.
Based on the observationthat rhythm places constraints on syntax in Chinese,we try to deploy a feature based on rhythm toimprove disambiguation in a probabilistic parserby mixing the rhythm feature into a statisticalparsing model.The rest of the paper is organized as follows: wepresent specific statistical analyses of rhythmfeature in Chinese syntax in Section 2.
Section 3introduces the content chunk parsing which is thetask in our experiment.
Section 4 presents thestatistical model used in our experiment in which aprobabilistic rhythm feature is integrated.
Section 5gives the experimental results and finally Section 6draws some conclusions.2 Analysis of Rhythmic ConstraintsWe divide our analysis of the use of rhythm inChinese phrases into two categories, based on twotypes of phrases in Chinese: (1) simple phrases,containing only words, i.e.
all the child nodes arePOS tag in the derivation tree; and (2) complexphrases in which at least one constituent is a phraseitself, i.e.
it has at least one child node with phrasetype symbol (like NP, VP) in its derivation tree.Below we will give the statistical analysis of thedistribution of rhythm feature in differentconstructions from both simple and complexphrases.
The corpus from which the statistical datais drawn contains 200K words of newspaper textfrom the People?s Daily.
The texts are word-segmented, POS tagged and labeled with contentchunks.
The content chunk is a phrase containingonly content words, akin to a generalization of aBaseNP.
These content chunks are parsed intobinary shallow trees.
More details about contentchunks can be found in Section 3.2.1 Rhythm feature in simple phrasesSimple phrases contain two lexical words (since, asdiscussed above, our parse trees are binary).
Therhythm feature of each word is defined to be thenumber of syllables in it.
Thus the rhythm featurefor a word can take on one of the following threevalues: (1) monosyllabic; (2) bi-syllabic; and (3)multi-syllabic, meaning with three syllables ormore.Since each binary phrase contains two words,the set of rhythm features for a simple phrase is:F = { (0?0), (0,1), (0,2), (1,0), (1,1), (1,2), (2,0),(2,1), (2,2) }where 0, 1, 2 represent monosyllabic, bi-syllabicand multi-syllabic respectively.In the following sections, we will present threecase studies on the distributions of rhythm featurein different constructions: (1) verbs as modifier orhead in NP; (2) the contrast between NPs and VPsformed by ?
verb + noun?
sequences; (3) ?noun +verb?
sequences.2.1.1  Case 1: Verb as modifier/head  in NPIn Chinese, verbs can function as modifier or headin a noun phrase without any change of forms.
Forexample, in ??
?/fruit tree ?
?/growing?, ????
is the head while in ???
/growing ?
?/technique?, ????
is a modifier.
However, insuch constructions, there are strong constraints onthe length of both verbs and nouns.
Table 1 givesthe distributions of the rhythm feature in the rule?NP -> N V?(?N?
and ?V?
represent noun and verbrespectively)  in which the verb is the head and?NP -> V N?
in which the verb is a modifier.Table 1  Distribution of rhythm feature in NP with verb as modifier or head[0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] TotalNP -> V N 0 4 0 0 1275 4 0 88 0 1371NP -> N V 13 10 0 401 2328 91 0 44 2 2889Table 2  Distribution of rhythm feature in NP and VP formed by ?V N?
[0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] TotalVP -> V N 826 640 49 80 1221 121 0 11 1 2777NP -> V N 13 10 0 401 2328 91 0 44 2 2889Table 3  Distribution of rhythm feature in phrases  formed by ?N V?
sequence[0,0] [0,1] [0,2] [1,0] [1,1] [1,2] [2,0] [2,1] [2,2] TotalNP -> N V 0 4 0 0 1275 4 0 88 0 1371NC -> N V 384 578 42 1131 3718 143 90 435 15 6536S    -> N V 28 1 2 17 347 22 2 43 8 470Table 1 indicates that in both rules, the rhythmpattern [1,1], ie.
?bi-syllabic + bi-syllabic?,prevails.
In the rule ?NP -> V N?, this patternaccounts for 93% among the nine possible patternswhile in the rule ?NP -> N V?, this patternaccounts for 81%.
We can also find that in bothcases, [0,2] and [2,0]  are prohibited, that is to say,both verbs and nouns cannot be longer than twosyllables.2.1.2  Case 2: Contrast between NP and VPformed by ?V N?
sequenceThe sequence ?V N?
(?verb + noun?)
can constitutean NP or a VP.
The rhythm patterns in the twotypes of phrases are significantly different,however, as shown in Table 2.
We see that in theNP case, verbs are mainly bi-syllabic.
The totalnumber of examples with bi-syllabic verbs in NP is2820, accounting for 98% of all the cases.
On theother hand, mono-syllabic verbs are less likely toappear in this position.
The total number ofexamples with mono-syllabic verbs in NP is 23,accounting for only 0.8% of all the cases.
That is tosay, the likelihood of bi-syllabic verbs appearing inthis syntactic position is 122 times the likelihoodof mono-syllabic verbs.
On the other hand, there isno big difference between bi-syllabic verbs andmono-syllabic verbs in the VP formed by ?V + N?.The ratios of bi-syllabic and mono-syllabic verbsin VP are 48 % and 55% respectively.
Thestatistical facts tell us that for a ?verb + noun?sequence, if the verb is not bi-syllabic then it isvery unlikely to be an NP.
Figure 1 depicts moreclearly the difference between NP and VP formedby ?V N?
sequence in the distribution of rhythmfeature.050010001500200025000,0 0,2 1,1 2,0 2,2VP-> v nNP-> v nFigure 1   Distributions of rhythm feature in NPand VP formed by ?verb + noun?.2.1 3  Case 3: ?N  V?
sequenceAn ?N V?
(?noun + verb?)
sequence can be mainlydivided into three types by the dominating phrasalcategory:(1) NP(noun phrase), e.g.
??
?/fruit tree ??/growth?
;(2) S(subject-verb construction), e.g.
?
?
?/colored flag ??/flutter?
;(3)NC(non-constituent), eg.
??
?/economy ??/develop?
in ??
?/China ?/DE ??/economy?
?/develop ?/DE ?/very ?/fast?.
(?China?seconomy develops very fast?
)Table 3 gives the distribution of rhythm feature inthe three types of cases.We see in Table 3, in rule ?NP -> N V?, that theverb cannot be mono-syllabic since the first row is0 in all the patterns in which verb is mono-syllabic([0,0], [1,0],[2,0]).
The ?bi-syllabic + bi-syllabic?
([1,1]) pattern accounts for 93%(1275/1371) of the total number.
Let?s look at thecases with mono-syllabic verbs in all the threetypes.
The total number of such examples is 1652in the corpus (adding all the numbers in columns[0,0], [1,0] and [2,0] on the three rows).
Amongthese 1652 cases, there is not one example inwhich the ?N V?
is an NP.
The sequence has aprobability of 3%(47/1652) to be an S and 97%(1605/1652) of being an NC(non-constituent).2.2 Rhythm feature in complex phrasesJust as we saw with two word simple phrases, therhythm feature also has an effect on complexphrases where at least one component is a phrase,i.e.
spanning over two words or more.
For example,for the following fragment of a sentence:?/stride   ?/into   ?
?/the Three Gorges?
?/project    ?
?/gate?enter into the gate of the Three Gorges Project?according to PCFG, the parse as indicated inFigure 2 (a) is incorrectly assigned the greatestVPNPVPNP?
?
??
??
??
(a)VPNPVP                       NP?
?
??
??
??
(b)Figure 2  (a) incorrect parse and(b) correct parseprobability but the correct parse is that given inFigure 2 (b).
One major error in (a) is that itapplies the rule ?NP-> VP N?
(i.e.
??
??
??
?
modifying ?
?
?
?).
This rule has 216occurrences in the corpus, of which 168 times itcontains a VP of 2 words, 30 times a VP of 3words and 18 times a VP of more than 3 words.These statistics indicate that this rule prefers tochoose a short VP acting as the modifier of a noun,as in ?NP(VP( ?
/grow  ?
/grain) ??
/largefamily)?
and ?NP(VP(?/learn ?
?/Lei Feng) ??/model)?.
But in the example in Figure 2(a), theVP contains 3 words, so it is less likely to be amodifier in an NP.When a phrase works as a constituent in a largerphrase, its rhythm feature is defined as the numberof words in it.
Thus a phrase may take on one ofthe three values for the rhythm feature: (1) twowords; (3) three words; and (3) more than threewords.
Similar to that in the simple phrases, wemay use 0, 1, 2 to represent the three valuesrespectively.
Therefore, for every constructioncontaining two constituents, its rhythm feature canbe described by a 3?3 matrix uniformly.
Forexample, in the examples for rule ?NP -> VP N?above, the feature value for  ?NP(VP(?/grow  ?/grain) ?
?/large family)?
is [0, 1] in which 0indicates the VP contains 2 words and 1 representsthat the noun is bi-syllabic.
The rule helps tointerpret the meaning of the feature value, i.e.
thevalue is for a word or a phrase.
For example, forrule ?VP -> V N?, feature value [0, 1] means thatthe verb is mono-syllabic and the noun is bi-syllabic, while for rule ?NP-> VP N?, feature [0,1]means that the VP contains two words and thenoun is bi-syllabic.3   Content Chunk ParsingWe have chosen the task of content chunk parsingto test the usefulness of our rhythm feature toChinese text.
In this section we address twoquestions: (1) What is a content chunk?
(2) Whyare we interested in content chunk parsing?A content chunk is a phrase formed by a sequenceof content words, including nouns, verbs,adjectives and content adverbs.
There are threekinds of cases for the mapping between contentword sequences and content chunks:(1) A content word sequence is a content chunk.
Aspecial case of this is that a whole sentence is acontent chunk when all the words in it are contentwords, eg.
[[?
?/Prospect  ?
?/company]NP [?
?/release [?
?/advanced [?
?/computer [?
?/typesetting ?
?
/system]NP]NP]NP]VP(?Prospect Company released an advancedcomputer typesetting system.?).
(2) A content word sequence is not a contentchunk.
For example, in ??
?/China ?/AUX ?
?/economy ?
?
/develop ?
/AUX ?
/very ?/fast?
(?China?s economy develops very fast.?
), ???
/economy ??
/develop?
is a content wordsequence, but it?s not a phrase in the sentence.
(3) A part of a content word sequence is a contentchunk.
For example, in ?
?
?
/private ?
?/economy ??
/develop ?
/AUX ??
/trend ?/very ?
/good?
(?The developmental trend ofprivate economy is very good.?
), ??
?/private ??
/economy ??
/develop?
is a content wordsequence, but it?s not a phrase; only ???/private??/economy?
in it is a phrase.The purpose of content chunk parsing is torecognize phrases in a sequence of content words.Specifically speaking, the content chunkingcontains two subtasks: (1) to recognize themaximum phrase in a sequence of content words;(2) to analyze the hierarchical structure within thephrase down to words.
Like baseNPchunking(Church, 1988; Ramshaw & Marcus1995), content chunk parsing is also a kind ofshallow parsing.
Content chunk parsing is deeperthan baseNP chunking in two aspects: (1) a contentchunk may contain verb phrases and other phraseseven a full sentence as long as the all thecomponents are content words; (2) it may containrecursive NPs.
Thus the content chunk can supplymore structural information than a baseNP.The motives for content chunk parsing are two-fold: (1) Like other shallow parsing tasks, it cansimplify the parsing task.
This can be explained intwo aspects.
First, it can avoid the ambiguitiesbrought up by functional words.
In Chinese, themost salient syntactic ambiguities are prepositionalphrases and the ?DE?
construction.
Forprepositional phrases, the difficulty lies in how todetermine the right boundary, because almost anyconstituent can be the object of a preposition.
For?DE?
constructions, the problem is how todetermine its left boundary, since almost anyconstituent can be followed by ?DE?
to form a?DE?
construction.
Second, content chunk parsingcan simplify the structure of a sentence.
When acontent chunk is acquired, it can be replaced by itshead word, thus reducing the length of the originalsentence.
If we get a parse from the reducedsentence with a full parser, then we can get a parsefor the original sentence by replacing the head-word nodes with the content chunks from whichthe head-words are extracted.
(2) The contentchunk parsing may be useful for applications likeinformation extraction and question answering.When using template matching, a content chunkmay be just the correct level of shallow structurefor matching with an element in a template.4   PCFG + PF ModelIn the experiment we propose a statistical modelintegrating probabilistic context-free grammar(PCFG) model with a simple probabilistic features(PF) model.
In this section we first give thedefinition for the statistical model and then we willgive the method for parameter estimation.4.1   DefinitionAccording to PCFG, each rule r used to expand anode n in a parse is assigned a probability, i.e.
:)|())(( APnrP ?=                                            (1)where A -> ?
is a CFG rule.
The probability of aparse T is the product of each rule used to expandeach node n in T:?
?=TnnrPSTP ))(()|(                                 (2)We expand PCFG by the way that when a lefthand side category A is expanded into a string ?, afeature set FS related to ?
is also generated.
Thus, aprobability is assigned for expansion of each noden when a rule r is applied:)|,())(( AFSPnrP ?=                         (3)where A -> ?
is a CFG rule and FS is a feature setrelated to ?.
From Equation (3) we get:)|(*),|())(( APAFSPnrP ?
?=     (4)where P(FS| ?, A) is probabilistic feature(PF)model and P(?
| A) is PCFG model.
PF modeldescribes the probability of each feature in featureset FS taking on specific values when a CFG ruleA -> ?
is given.
To make the model more practicalin parameter estimation, we assume the features infeature set FS are independent from each other,thus:?
?=FSFiAFiPAFSP ),|(),|( ??
(5)Under this PCFG+PF model, the goal of a parseris to choose a parse that maximizes the followingscore:)|,(maxarg)|(1AFS iiiniTPSTScore ?
?==      (6)Our model is thus a simplification of moresophisticated models which integrate PCFGs withfeatures, such as those in Magerman(1995),Collins(1997) and Goodman(1997).
Comparedwith these models, our model is more practicalwhen only small training data is available, sincewe assume the independence between features.
Forexample, in Goodman?s probabilistic featuregrammar (PFG), each symbol in a PCFG isreplaced by a set of features, so it can describespecific constraints on the rule.
In the PFG modelthe generation of each feature is dependent on allthe previously generated features, thus likelyleading to severe sparse data problem in parameterestimation.
Our simplified model assumesindependence between the features, thus datasparseness problem can be significantly alleviated.4.2  Parameter EstimationLet F be a feature associated with a string ?, wherethe possible values for F are f1,f2,?,fn, E is the setof observations of rule A ?
?
in the trainingcorpus, and thus E can be divided into n disjointsubsets: E1,E2,?,En, corresponding to f1,f2,?,fnrespectively.
The probability of F taking on a valueof fi given A ?
?
can be estimated as follows,according to MLE:EEAfFP ii == ),|( ?
(7)This indicates that feature F adds constraints onCFG rule A ?
?
by dividing ?, the state space ofA ?
?, into n disjoint subspaces ?1, ?2,?, ?n, andeach case of F taking a value of fi given A ?
?
isviewed as a random event.5  Experimental Results5.1 Training and Test DataA Chinese corpus of 200K words extracted fromthe People?s Daily are segmented, POS-tagged andhand-labeled with content chunks in which all thetrees are binary.
The corpus is divided into twoparts: (1) 180K for training set and (2) 20K for testset.5.2 Metrics and resultsWe take two kinds of criteria to measure thesystem?s performance: labeled and unlabeled.According to the labeled criterion, a recognizedphrase is correct only if a phrase with the samestarting position, ending position and the samelabel is found in the gold standard.
According tothe unlabeled criterion, a recognized phrase iscorrect as long as a phrase with the same startingposition and ending position is found in the goldstandard.Table 4   Experimental  ResultsLabeled UnlabeledP R F P R FPCFG 49.91 64.96 56.45 53.33 80.73 65.66PCFG+RF in simple phrases 53.25 68.46 59.90 57.46    81.21 67.30PCFG +RF in all the phrases 56.47 72.08 63.33 60.07 83.57 69.90Table 5    Effect of rhythm feature on structural disambiguationWord sequence Rule P(?|A) RF P(RF=[0,1]|A,?)P(RF=(0,1),?)|A)?
?
?country  sacrificeNC?
N V  0.120273 [0,1] 0.08843   0.010636?
??
S   ?
N V 0.161679 [0,1] 0.00292   0.000344?
??
NP?
N V 0.063159 [0,1] 0.00213   0.000184?
??
V  ?
N V 0.011573 [0,1] 0.0   0.0Within each criterion, precision, recall and F-measure are given as metrics for the system?sperformance.
Precision represents how manyphrases are correct among the phrases recognized,recall represents how many phrases in the goldstandard are correctly recognized, and F-measureis defined as follows:callecisioncallecisionmeasureFRePr2RePr+?
?=?Table 4 gives the experimental results in threedifferent conditions: the first row gives the resultof PCFG model; the second row gives the result ofPCFG model integrated with rhythm feature model(RF) where only the features of simple phrases areconsidered; the last row gives the result of PCFGmodel plus RF where the rhythm features in all thephrases are considered.
The results indicate thatthe rhythm features in both simple and complexphrases contribute to the improvement ofperformance over PCFG model.
We see that therhythm feature improves the labeled F-measure6.88 percent and the unlabeled F-measure 4.24percent over the unaugmented PCFG model.5.3 Effect of rhythm feature on parsingThe experiment shows that the rhythm feature canhelp the performance of a parser in Chinese.Specifically, the effects of rhythm feature onparsing are shown in two ways:(1) Help for the disambiguation of phrasal type.Table 5 shows the difference of the resultsbetween PCFG model and PCFG + RF model forthe sequence ?
?/country ??/sacrifice?
in thesentence ??
/the ?
/school ?
/have 900 ?
?/students ?/for ?/country ??/sacrifice?
(`900students from this school gave their lives for theircountry?
).In the sentence above, ??/country?
is the objectof preposition ?
?
/for?, ?
?
/country ?
?/sacrifice?
is not a constituent.
But theunaugmented PCFG model incorrectly parses it asa S(subject-predicate construction).
Contrarily,according to PCFG+RF model, the type withgreatest probability is the (correct) NC(non-constituent) parse.
(2) Help for pruning.Let?s give an example to explain it.
For thesentence ???
/solve ??
/resident ?
/eat ?/vegetable ?
?
/problem ?
?
/very ?
?/difficult?
(?It?s very difficult to solve the vegetableproblem for the residents.?
), the number of edgesgenerated by the PCFG is 1236, but the numberdecreases to 348 after the rhythm feature is applied,thus pruning 73% of the edges.
As indicated inTable 1, in the rule ?NP -> N V?, P(RF = [1,0] ) =0, so ?[?
?/N ?/V]NP?
is pruned after addingRF.
Similarly, in rule ?NP -> V N?, P(RF = [0, 1] )= 0.003, so ?
[?/V ?/N]NP?
is pruned since it hasvery low probability.
With these two edges pruned,more potential edges containing them will not begenerated.6 ConclusionIn this paper, we systematically survey thedistribution of rhythm (number of syllables perword or numbers of words per phrase for aconstituent) in different constructions in Chinese.Our analysis suggests that rhythm places strongconstraints on Chinese syntax.
Based on thisobservation, we used the rhythm feature in apractical shallow parsing task in which a PCFGmodel is augmented with a probabilisticrepresentation of the rhythm feature.
Theexperimental results show that the probabilisticrhythm feature aids in disambiguation in Chineseand thus helps to improve the performance of aChinese parser.
We can expect that theperformance of the parser may further improvewhen more features are considered under theprobabilistic feature (PF) model.AcknowledgmentsThis research was partially supported by the NSF,via a KDD extension to NSF IIS-9978025.ReferencesChurch,,K.,1988.A stochastic parts program andnoun phrase parser for unrestricted text.
InProceedings of the Second Conference onApplied Natural Language Processing, pp.136-143.Collins, M. 1997.
Three generative lexicalizedmodels for statistical parsing, in Proceedings ofthe 35th Annual Meeting of the ACL, pp.
16-23.Feng, Shengli.
2000.
The Rhythmic syntax ofChinese(in Chinese), Shanghai Education Press.Goodman, J.
1997.
Probabilistic FeatureGrammars, In Proceedings of the InternationalWorkshop on Parsing Technologies, September1997Magerman, D. 1995.
Statistical decision-treemodels for parsing, in Proceedings of the 33rdAnnual Meeting of the Association forComputational Linguistics, pp.276-283.Quirk et al 1985.
A Comprehensive Grammar ofEnglish Languge, Longman.Ramshaw L., and Marcus M. 1995.
Text chunkingusing transformation-based learning.
In Proc-eedings of the Third Workshop on Very LargeCorpora.pp.86-95.
