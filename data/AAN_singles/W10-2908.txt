Proceedings of the Fourteenth Conference on Computational Natural Language Learning, page 56,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsBayesian Hidden Markov Models and ExtensionsInvited TalkZoubin GhahramaniEngineering Department, University of Cambridge, Cambridge, UKzoubin@eng.cam.ac.ukHidden Markov models (HMMs) are one of the cornerstones of time-series modelling.
I will reviewHMMs, motivations for Bayesian approaches to inference in them, and our work on variational Bayesianlearning.
I will then focus on recent nonparametric extensions to HMMs.
Traditionally, HMMs havea known structure with a fixed number of states and are trained using maximum likelihood techniques.The infinite HMM (iHMM) allows a potentially unbounded number of hidden states, letting the modeluse as many states as it needs for the data.
The recent development of ?Beam Sampling?
?
an efficientinference algorithm for iHMMs based on dynamic programming ?
makes it possible to apply iHMMs tolarge problems.
I will show some applications of iHMMs to unsupervised POS tagging and experimentswith parallel and distributed implementations.
I will also describe a factorial generalisation of the iHMMwhich makes it possible to have an unbounded number of binary state variables, and can be thought ofas a time-series generalisation of the Indian buffet process.
I will conclude with thoughts on futuredirections in Bayesian modelling of sequential data.56
