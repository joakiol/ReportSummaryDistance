Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 352?359,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsConditional Modality Fusion for Coreference ResolutionJacob Eisenstein and Randall DavisComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of TechnologyCambridge, MA 02139 USA{jacobe,davis}@csail.mit.eduAbstractNon-verbal modalities such as gesture canimprove processing of spontaneous spokenlanguage.
For example, similar hand ges-tures tend to predict semantic similarity, sofeatures that quantify gestural similarity canimprove semantic tasks such as coreferenceresolution.
However, not all hand move-ments are informative gestures; psycholog-ical research has shown that speakers aremore likely to gesture meaningfully whentheir speech is ambiguous.
Ideally, onewould attend to gesture only in such cir-cumstances, and ignore other hand move-ments.
We present conditional modalityfusion, which formalizes this intuition bytreating the informativeness of gesture as ahidden variable to be learned jointly withthe class label.
Applied to coreferenceresolution, conditional modality fusion sig-nificantly outperforms both early and latemodality fusion, which are current tech-niques for modality combination.1 IntroductionNon-verbal modalities such as gesture and prosodycan increase the robustness of NLP systems to theinevitable disfluency of spontaneous speech.
For ex-ample, consider the following excerpt from a dia-logue in which the speaker describes a mechanicaldevice:?So this moves up, and it ?
everything moves up.And this top one clears this area here, and goes allthe way up to the top.
?The references in this passage are difficult todisambiguate, but the gestures shown in Figure 1make the meaning more clear.
However, non-verbalmodalities are often noisy, and their interactionswith speech are complex (McNeill, 1992).
Ges-ture, for example, is sometimes communicative, butother times merely distracting.
While people havelittle difficulty distinguishing between meaningfulgestures and irrelevant hand motions (e.g., self-touching, adjusting glasses) (Goodwin and Good-win, 1986), NLP systems may be confused by suchseemingly random movements.
Our goal is to in-clude non-verbal features only in the specific caseswhen they are helpful and necessary.We present a model that learns in an unsupervisedfashion when non-verbal features are useful, allow-ing it to gate the contribution of those features.
Therelevance of the non-verbal features is treated as ahidden variable, which is learned jointly with theclass label in a conditional model.
We demonstratethat this improves performance on binary corefer-ence resolution, the task of determining whether anoun phrases refers to a single semantic entity.
Con-ditional modality fusion yields a relative increase of73% in the contribution of hand-gesture features.The model is not specifically tailored to gesture-speech integration, and may also be applicable toother non-verbal modalities.2 Related workMost of the existing work on integrating non-verbalfeatures relates to prosody.
For example, Shriberget al (2000) explore the use of prosodic features forsentence and topic segmentation.
The first modal-352And this top one clears this area here, and goesall the way up to the top...2So this moves up.
And it ?
everything moves up.1Figure 1: An example where gesture helps to disambiguate meaning.ity combination technique that they consider trains asingle classifier with all modalities combined into asingle feature vector; this is sometimes called ?earlyfusion.?
Shriberg et al also consider training sepa-rate classifiers and combining their posteriors, eitherthrough weighted addition or multiplication; this issometimes called ?late fusion.?
Late fusion is alsoemployed for gesture-speech combination in (Chenet al, 2004).
Experiments in both (Shriberg et al,2000) and (Kim et al, 2004) find no conclusive win-ner among early fusion, additive late fusion, andmultiplicative late fusion.Toyama and Horvitz (2000) introduce a Bayesiannetwork approach to modality combination forspeaker identification.
As in late fusion, modality-specific classifiers are trained independently.
How-ever, the Bayesian approach also learns to predictthe reliability of each modality on a given instance,and incorporates this information into the Bayesnet.
While more flexible than the interpolation tech-niques described in (Shriberg et al, 2000), trainingmodality-specific classifiers separately is still sub-optimal compared to training them jointly, becauseindependent training of the modality-specific classi-fiers forces them to account for data that they can-not possibly explain.
For example, if the speaker isnot gesturing meaningfully, it is counterproductiveto train a gesture-modality classifier on the featuresat this instant; doing so can lead to overfitting andpoor generalization.Our approach combines aspects of both early andlate fusion.
As in early fusion, classifiers for allmodalities are trained jointly.
But as in Toyama andHorvitz?s Bayesian late fusion model, modalities canbe weighted based on their predictive power for spe-cific instances.
In addition, our model is trained tomaximize conditional likelihood, rather than jointlikelihood.3 Conditional modality fusionThe goal of our approach is to learn to weight thenon-verbal features xnv only when they are rele-vant.
To do this, we introduce a hidden variablem ?
{?1, 1}, which governs whether the non-verbal features are included.
p(m) is conditioned ona subset of features xm, which may belong to anymodality; p(m|xm) is learned jointly with the classlabel p(y|x), with y ?
{?1, 1}.
For our coreferenceresolution model, y corresponds to whether a givenpair of noun phrases refers to the same entity.In a log-linear model, parameterized by weightsw, we have:p(y|x;w) =?mp(y,m|x;w)=?m exp(?
(y,m,x;w))?y?,m exp(?
(y?,m,x;w)).Here, ?
is a potential function representing thecompatibility between the label y, the hidden vari-able m, and the observations x; this potential is pa-rameterized by a vector of weights, w. The numera-tor expresses the compatibility of the label y and ob-servations x, summed over all possible values of thehidden variablem.
The denominator sums over bothm and all possible labels y?, yielding the conditionalprobability p(y|x;w).
The use of hidden variables353in a conditionally-trained model follows (Quattoniet al, 2004).This model can be trained by a gradient-basedoptimization to maximize the conditional log-likelihood of the observations.
The unregularizedlog-likelihood and gradient are given by:l(w) =Xiln(p(yi|xi;w)) (1)=XilnPm exp(?
(yi,m,xi;w))Py?,m exp(?(y?,m,xi;w))(2)?li?wj=Xmp(m|yi,xi;w)??wj?
(yi,m,xi;w)?Xy?,mp(m, y?|xi;w)??wj?
(y?,m,xi;w)The form of the potential function ?
is where ourintuitions about the role of the hidden variable areformalized.
Our goal is to include the non-verbalfeatures xnv only when they are relevant; conse-quently, the weight for these features should go tozero for some settings of the hidden variable m. Inaddition, verbal language is different when used incombination with meaningful non-verbal commu-nication than when it is used unimodally (Kehler,2000; Melinger and Levelt, 2004).
Thus, we learna different set of feature weights for each case: wv,1when the non-verbal features are included, and wv,2otherwise.
The formal definition of the potentialfunction for conditional modality fusion is:?
(y,m,x;w) ?
{y(wTv,1xv +wTnvxnv) +wTmxm m = 1ywTv,2xv ?wTmxm m = ?1.
(3)4 Application to coreference resolutionWe apply conditional modality fusion to corefer-ence resolution ?
the problem of partitioning thenoun phrases in a document into clusters, where allmembers of a cluster refer to the same semantic en-tity.
Coreference resolution on text datasets is well-studied (e.g., (Cardie and Wagstaff, 1999)).
Thisprior work provides the departure point for our in-vestigation of coreference resolution on spontaneousand unconstrained speech and gesture.4.1 Form of the modelThe form of the model used in this application isslightly different from that shown in Equation 3.When determining whether two noun phrases core-fer, the features at each utterance must be consid-ered.
For example, if we are to compare the simi-larity of the gestures that accompany the two nounphrases, it should be the case that gesture is relevantduring both time periods.For this reason, we create two hidden variables,m1 and m2; they indicate the relevance of ges-ture over the first (antecedent) and second (anaphor)noun phrases, respectively.
Since gesture similarityis only meaningful if the gesture is relevant duringboth NPs, the gesture features are included only ifm1 = m2 = 1.
Similarly, the linguistic featureweights wv,1 are used when m1 = m2 = 1; oth-erwise the weights wv,2 are used.
This yields themodel shown in Equation 4.The vector of meta features xm1 includes allsingle-phrase verbal and gesture features from Ta-ble 1, computed at the antecedent noun phrase;xm2 includes the single-phrase verbal and gesturefeatures, computed at the anaphoric noun phrase.The label-dependent verbal features xv include bothpairwise and single phrase verbal features from thetable, while the label-dependent non-verbal featuresxnv include only the pairwise gesture features.
Thesingle-phrase non-verbal features were not includedbecause they were not thought to be informative asto whether the associated noun-phrase would partic-ipate in coreference relations.4.2 Verbal featuresWe employ a set of verbal features that is similarto the features used by state-of-the-art coreferenceresolution systems that operate on text (e.g., (Cardieand Wagstaff, 1999)).
Pairwise verbal features in-clude: several string-match variants; distance fea-tures, measured in terms of the number of interven-ing noun phrases and sentences between the candi-date NPs; and some syntactic features that can becomputed from part of speech tags.
Single-phraseverbal features describe the type of the noun phrase(definite, indefinite, demonstrative (e.g., this ball),or pronoun), the number of times it appeared inthe document, and whether there were any adjecti-354?
(y,m1,m2,x;w) ?
{y(wTv,1xv +wTnvxnv) +m1wTmxm1 +m2wTmxm2 , m1 = m2 = 1ywTv,2xv +m1wTmxm1 +m2wTmxm2 , otherwise.
(4)val modifiers.
The continuous-valued features werebinned using a supervised technique (Fayyad andIrani, 1993).Note that some features commonly used for coref-erence on the MUC and ACE corpora are not appli-cable here.
For example, gazetteers listing names ofnations or corporations are not relevant to our cor-pus, which focuses on discussions of mechanical de-vices (see section 5).
Because we are working fromtranscripts rather than text, features dependent onpunctuation and capitalization, such as apposition,are also not applicable.4.3 Non-verbal featuresOur non-verbal features attempt to capture similar-ity between the speaker?s hand gestures; similar ges-tures are thought to suggest semantic similarity (Mc-Neill, 1992).
For example, two noun phrases maybe more likely to corefer if they are accompanied byidentically-located pointing gestures.
In this section,we describe features that quantify various aspects ofgestural similarity.The most straightforward measure of similarity isthe Euclidean distance between the average hand po-sition during each noun phrase ?
we call this theFOCUS-DISTANCE feature.
Euclidean distance cap-tures cases in which the speaker is performing a ges-tural ?hold?
in roughly the same location (McNeill,1992).However, Euclidean distance may not correlatedirectly with semantic similarity.
For example,when gesturing at a detailed part of a diagram,very small changes in hand position may be se-mantically meaningful, while in other regions posi-tional similarity may be defined more loosely.
Ide-ally, we would compute a semantic feature cap-turing the object of the speaker?s reference (e.g.,?the red block?
), but this is not possible in gen-eral, since a complete taxonomy of all possible ob-jects of reference is usually unknown.
Instead, weuse a hidden Markov model (HMM) to perform aspatio-temporal clustering on hand position and ve-locity.
The SAME-CLUSTER feature reports whetherthe hand positions during two noun phrases wereusually grouped in the same cluster by the HMM.JS-DIV reports the Jensen-Shannon divergence, acontinuous-valued feature used to measure the simi-larity in cluster assignment probabilities between thetwo gestures (Lin, 1991).The gesture features described thus far capture thesimilarity between static gestures; that is, gesturesin which the hand position is nearly constant.
How-ever, these features do not capture the similarity be-tween gesture trajectories, which may also be usedto communicate meaning.
For example, a descrip-tion of two identical motions might be expressedby very similar gesture trajectories.
To measure thesimilarity between gesture trajectories, we use dy-namic time warping (Huang et al, 2001), whichgives a similarity metric for temporal data that isinvariant to speed.
This is reported in the DTW-DISTANCE feature.All features are computed from hand and bodypixel coordinates, which are obtained via computervision; our vision system is similar to (Deutscher etal., 2000).
The feature set currently supports onlysingle-hand gestures, using the hand that is farthestfrom the body center.
As with the verbal feature set,supervised binning was applied to the continuous-valued features.4.4 Meta featuresThe role of the meta features is to determine whetherthe gesture features are relevant at a given point intime.
To make this determination, both verbal andnon-verbal features are applied; the only require-ment is that they be computable at a single instantin time (unlike features that measure the similaritybetween two NPs or gestures).Verbal meta features Meaningful gesture hasbeen shown to be more frequent when the associatedspeech is ambiguous (Melinger and Levelt, 2004).Kehler finds that fully-specified noun phrases areless likely to receive multimodal support (Kehler,2000).
These findings lead us to expect that pro-355Pairwise verbal featuresedit-distance a numerical measure of the string simi-larity between the two NPsexact-match true if the two NPs have identical sur-face formsstr-match true if the NPs are identical after re-moving articlesnonpro-str true if i and j are not pronouns, and str-match is truepro-str true if i and j are pronouns, and str-match is truej-substring-i true if the anaphor j is a substring ofthe antecedent ii-substring-j true if i is a substring of joverlap true if there are any shared words be-tween i and jnp-dist the number of noun phrases between iand j in the documentsent-dist the number of sentences between i andj in the documentboth-subj true if both i and j precede the first verbof their sentencessame-verb true if the first verb in the sentences fori and j is identicalnumber-match true if i and j have the same numberSingle-phrase verbal featurespronoun true if the NP is a pronouncount number of times the NP appears in thedocumenthas-modifiers true if the NP has adjective modifiersindef-np true if the NP is an indefinite NP (e.g.,a fish)def-np true if the NP is a definite NP (e.g., thescooter)dem-np true if the NP begins with this, that,these, or thoselexical features lexical features are defined for the mostcommon pronouns: it, that, this, andtheyPairwise gesture featuresfocus-distance the Euclidean distance in pixels be-tween the average hand position duringthe two NPsDTW-agreement a measure of the agreement of the hand-trajectories during the two NPs, com-puted using dynamic time warpingsame-cluster true if the hand positions during the twoNPs fall in the same clusterJS-div the Jensen-Shannon divergence be-tween the cluster assignment likeli-hoodsSingle-phrase gesture featuresdist-to-rest distance of the hand from rest positionjitter sum of instantaneous motion across NPspeed total displacement over NP, divided bydurationrest-cluster true if the hand is usually in the clusterassociated with rest positionmovement-cluster true if the hand is usually in the clusterassociated with movementTable 1: The feature setnouns should be likely to co-occur with meaningfulgestures, while definite NPs and noun phrases thatinclude adjectival modifiers should be unlikely to doso.
To capture these intuitions, all single-phrase ver-bal features are included as meta features.Non-verbal meta features Research on gesturehas shown that semantically meaningful hand mo-tions usually take place away from ?rest position,?which is located at the speaker?s lap or sides (Mc-Neill, 1992).
Effortful movements away from thesedefault positions can thus be expected to predict thatgesture is being used to communicate.
We iden-tify rest position as the center of the body on thex-axis, and at a fixed, predefined location on the y-axis.
The DIST-TO-REST feature computes the av-erage Euclidean distance of the hands from the restposition, over the duration of the NP.As noted in the previous section, a spatio-temporal clustering was performed on the hand po-sitions and velocities, using an HMM.
The REST-CLUSTER feature takes the value ?true?
iff the mostfrequently occupied cluster during the NP is thecluster closest to rest position.
In addition, pa-rameter tying in the HMM forces all clusters butone to represent static hold, with the remainingcluster accounting for the transition movements be-tween holds.
Only this last cluster is permitted tohave an expected non-zero speed; if the hand ismost frequently in this cluster during the NP, thenthe MOVEMENT-CLUSTER feature takes the value?true.
?4.5 ImplementationThe objective function (Equation 1) is optimizedusing a Java implementation of L-BFGS, a quasi-Newton numerical optimization technique (Liu andNocedal, 1989).
Standard L2-norm regulariza-tion is employed to prevent overfitting, with cross-validation to select the regularization constant.
Al-though standard logistic regression optimizes a con-vex objective, the inclusion of the hidden variablerenders our objective non-convex.
Thus, conver-gence to a global minimum is not guaranteed.5 Evaluation setupDataset Our dataset consists of sixteen short dia-logues, in which participants explained the behavior356of mechanical devices to a friend.
There are ninedifferent pairs of participants; each contributed twodialogues, with two thrown out due to recording er-rors.
One participant, the ?speaker,?
saw a shortvideo describing the function of the device priorto the dialogue; the other participant was tested oncomprehension of the device?s behavior after the di-alogue.
The speaker was given a pre-printed dia-gram to aid in the discussion.
For simplicity, onlythe speaker?s utterances were included in these ex-periments.The dialogues were limited to three minutes in du-ration, and most of the participants used the entireallotted time.
?Markable?
noun phrases ?
those thatare permitted to participate in coreference relations?
were annotated by the first author, in accordancewith the MUC task definition (Hirschman and Chin-chor, 1997).
A total of 1141 ?markable?
NPs weretranscribed, roughly half the size of the MUC6 de-velopment set, which includes 2072 markable NPsover 30 documents.Evaluation metric Coreference resolution is of-ten performed in two phases: a binary classifi-cation phase, in which the likelihood of corefer-ence for each pair of noun phrases is assessed;and a partitioning phase, in which the clusters ofmutually-coreferring NPs are formed, maximizingsome global criterion (Cardie and Wagstaff, 1999).Our model does not address the formation of noun-phrase clusters, but only the question of whethereach pair of noun phrases in the document corefer.Consequently, we evaluate only the binary classifi-cation phase, and report results in terms of the areaunder the ROC curve (AUC).
As the small size ofthe corpus did not permit dedicated test and devel-opment sets, results are computed using leave-one-out cross-validation, with one fold for each of thesixteen documents in the corpus.Baselines Three types of baselines were comparedto our conditional modality fusion (CMF) technique:?
Early fusion.
The early fusion baseline in-cludes all features in a single vector, ignor-ing modality.
This is equivalent to standardmaximum-entropy classification.
Early fusionis implemented with a conditionally-trainedlinear classifier; it uses the same code as theCMF model, but always includes all features.?
Late fusion.
The late fusion baselines trainseparate classifiers for gesture and speech, andthen combine their posteriors.
The modality-specific classifiers are conditionally-trained lin-ear models, and again use the same code as theCMF model.
For simplicity, a parameter sweepidentifies the interpolation weights that maxi-mize performance on the test set.
Thus, it islikely that these results somewhat overestimatethe performance of these baseline models.
Wereport results for both additive and multiplica-tive combination of posteriors.?
No fusion.
These baselines include the fea-tures from only a single modality, and againbuild a conditionally-trained linear classifier.Implementation uses the same code as the CMFmodel, but weights on features outside the tar-get modality are forced to zero.Although a comparison with existing state-of-the-art coreference systems would be ideal, all suchavailable systems use verbal features that are inap-plicable to our dataset, such as punctuation, capital-ization, and gazetteers.
The verbal features that wehave included are a representative sample from theliterature (e.g., (Cardie and Wagstaff, 1999)).
The?no fusion, verbal features only?
baseline thus pro-vides a reasonable representation of prior work oncoreference, by applying a maximum-entropy clas-sifier to this set of typical verbal features.Parameter tuning Continuous features arebinned separately for each cross-validation fold,using only the training data.
The regularizationconstant is selected by cross-validation within eachtraining subset.6 ResultsConditional modality fusion outperforms all otherapproaches by a statistically significant margin (Ta-ble 2).
Compared with early fusion, CMF offers anabsolute improvement of 1.20% in area under theROC curve (AUC).1 A paired t-test shows that this1AUC quantifies the ranking accuracy of a classifier.
If theAUC is 1, all positively-labeled examples are ranked higher thanall negative-labeled ones.357model AUCConditional modality fusion .8226Early fusion .8109Late fusion, multiplicative .8103Late fusion, additive .8068No fusion (verbal features only) .7945No fusion (gesture features only) .6732Table 2: Results, in terms of areas under the ROCcurve2 3 4 5 6 7 80.790.7950.80.8050.810.8150.820.8250.83log of regularization constantAUCCMFEarly FusionSpeech OnlyFigure 2: Conditional modality fusion is robust tovariations in the regularization constant.result is statistically significant (p < .002, t(15) =3.73).
CMF obtains higher performance on fourteenof the sixteen test folds.
Both additive and multi-plicative late fusion perform on par with early fu-sion.Early fusion with gesture features is superior tounimodal verbal classification by an absolute im-provement of 1.64% AUC (p < 4 ?
10?4, t(15) =4.45).
Thus, while gesture features improve coref-erence resolution on this dataset, their effectivenessis increased by a relative 73% when conditionalmodality fusion is applied.
Figure 2 shows how per-formance varies with the regularization constant.7 DiscussionThe feature weights learned by the system to deter-mine coreference largely confirm our linguistic in-tuitions.
Among the textual features, a large pos-itive weight was assigned to the string match fea-tures, while a large negative weight was assigned tofeatures such as number incompatibility (i.e., sin-pronoun def dem indef "this" "it" "that" "they"modifiers?0.6?0.5?0.4?0.3?0.2?0.100.10.20.30.4 Weights learned with verbal meta featuresFigure 3: Weights for verbal meta featuresgular versus plural).
The system also learned thatgestures with similar hand positions and trajectorieswere likely to indicate coreferring noun phrases; allof our similarity metrics were correlated positivelywith coreference.
A chi-squared analysis found thatthe EDIT DISTANCE was the most informative ver-bal feature.
The most informative gesture featurewas DTW-AGREEMENT feature, which measuresthe similarity between gesture trajectories.As described in section 4, both textual and gestu-ral features are used to determine whether the ges-ture is relevant.
Among textual features, definiteand indefinite noun phrases were assigned nega-tive weights, suggesting gesture would not be use-ful to disambiguate coreference for such NPs.
Pro-nouns were assigned positive weights, with ?this?and the much less frequently used ?they?
receivingthe strongest weights.
?It?
and ?that?
received lowerweights; we observed that these pronouns were fre-quently used to refer to the immediately precedingnoun phrase, so multimodal support was often un-necessary.
Last, we note that NPs with adjectivalmodifiers were assigned negative weights, support-ing the finding of (Kehler, 2000) that fully-specifiedNPs are less likely to receive multimodal support.
Asummary of the weights assigned to the verbal metafeatures is shown in Figure 3.
Among gesture metafeatures, the weights learned by the system indicatethat non-moving hand gestures away from the bodyare most likely to be informative in this dataset.3588 Future workWe have assumed that the relevance of gesture tosemantics is dependent only on the currently avail-able features, and not conditioned on prior history.In reality, meaningful gestures occur over contigu-ous blocks of time, rather than at randomly dis-tributed instances.
Indeed, the psychology literaturedescribes a finite-state model of gesture, proceed-ing from ?preparation,?
to ?stroke,?
?hold,?
and then?retraction?
(McNeill, 1992).
These units are calledmovement phases.
The relevance of various gesturefeatures may be expected to depend on the move-ment phase.
During strokes, the trajectory of thegesture may be the most relevant feature, while dur-ing holds, static features such as hand position andhand shape may dominate; during preparation andretraction, gesture features are likely to be irrelevant.The identification of these movement phasesshould be independent of the specific problem ofcoreference resolution.
Thus, additional labels forother linguistic phenomena (e.g., topic segmenta-tion, disfluency) could be combined into the model.Ideally, each additional set of labels would transferperformance gains to the other labeling problems.9 ConclusionsWe have presented a new method for combiningmultiple modalities, which we feel is especially rel-evant to non-verbal modalities that are used to com-municate only intermittently.
Our model treats therelevance of the non-verbal modality as a hiddenvariable, learned jointly with the class labels.
Ap-plied to coreference resolution, this model yields arelative increase of 73% in the contribution of thegesture features.
This gain is attained by identify-ing instances in which gesture features are especiallyrelevant, and weighing their contribution more heav-ily.
We next plan to investigate models with a tem-poral component, so that the behavior of the hiddenvariable is governed by a finite-state transducer.Acknowledgments We thank Aaron Adler, ReginaBarzilay, S. R. K. Branavan, Sonya Cates, Erdong Chen,Michael Collins, Lisa Guttentag, Michael Oltmans, and TomOuyang.
This research is supported in part by MIT Project Oxy-gen.ReferencesClaire Cardie and Kiri Wagstaff.
1999.
Noun phrase corefer-ence as clustering.
In Proceedings of EMNLP, pages 82?89.Lei Chen, Yang Liu, Mary P. Harper, and Elizabeth Shriberg.2004.
Multimodal model integration for sentence unit de-tection.
In Proceedings of ICMI, pages 121?128.Jonathan Deutscher, Andrew Blake, and Ian Reid.
2000.
Artic-ulated body motion capture by annealed particle filtering.
InProceedings of CVPR, volume 2, pages 126?133.Usama M. Fayyad and Keki B. Irani.
1993.
Multi-intervaldiscretization of continuousvalued attributes for classifica-tion learning.
In Proceedings of IJCAI-93, volume 2, pages1022?1027.
Morgan Kaufmann.M.H.
Goodwin and C. Goodwin.
1986.
Gesture and co-participation in the activity of searching for a word.
Semiot-ica, 62:51?75.Lynette Hirschman and Nancy Chinchor.
1997.
MUC-7 coref-erence task definition.
In Proceedings of the Message Un-derstanding Conference.Xuedong Huang, Alex Acero, and Hsiao-Wuen Hon.
2001.Spoken Language Processing.
Prentice Hall.Andrew Kehler.
2000.
Cognitive status and form of referencein multimodal human-computer interaction.
In Proceedingsof AAAI, pages 685?690.Joungbum Kim, Sarah E. Schwarm, and Mari Osterdorf.2004.
Detecting structural metadata with decision treesand transformation-based learning.
In Proceedings of HLT-NAACL?04.
ACL Press.Jianhua Lin.
1991.
Divergence measures based on the shannonentropy.
IEEE transactions on information theory, 37:145?151.Dong C. Liu and Jorge Nocedal.
1989.
On the limited memoryBFGS method for large scale optimization.
MathematicalProgramming, 45:503?528.David McNeill.
1992.
Hand and Mind.
The University ofChicago Press.Alissa Melinger and Willem J. M. Levelt.
2004.
Gesture andcommunicative intention of the speaker.
Gesture, 4(2):119?141.Ariadna Quattoni, Michael Collins, and Trevor Darrell.
2004.Conditional random fields for object recognition.
In NeuralInformation Processing Systems, pages 1097?1104.Elizabeth Shriberg, Andreas Stolcke, Dilek Hakkani-Tur, andGokhan Tur.
2000.
Prosody-based automatic segmentationof speech into sentences and topics.
Speech Communication,32.Kentaro Toyama and Eric Horvitz.
2000.
Bayesian modal-ity fusion: Probabilistic integration of multiple vision al-gorithms for head tracking.
In Proceedings of ACCV ?00,Fourth Asian Conference on Computer Vision.359
