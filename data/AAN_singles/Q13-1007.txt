Transactions of the Association for Computational Linguistics, 1 (2013) 75?88.
Action Editor: Sharon Goldwater.Submitted 11/2012; Revised 1/2013; Published 3/2013.
c?2013 Association for Computational Linguistics.An HDP Model for Inducing Combinatory Categorial GrammarsYonatan Bisk and Julia HockenmaierDepartment of Computer ScienceThe University of Illinois at Urbana-Champaign201 N Goodwin Ave Urbana, IL 61801{bisk1,juliahmr}@illinois.eduAbstractWe introduce a novel nonparametric Bayesianmodel for the induction of Combinatory Cat-egorial Grammars from POS-tagged text.
Itachieves state of the art performance on anumber of languages, and induces linguisti-cally plausible lexicons.1 IntroductionWhat grammatical representation is appropriate forunsupervised grammar induction?
Initial attemptswith context-free grammars (CFGs) were not verysuccessful (Carroll and Charniak, 1992; Charniak,1993).
One reason may be that CFGs require thespecification of a finite inventory of nonterminal cat-egories and rewrite rules, but unless one adopts lin-guistic principles such as X-bar theory (Jackendoff,1977), these nonterminals are essentially arbitrarylabels that can be combined in arbitrary ways.
Whilefurther CFG-based approaches have been proposed(Clark, 2001; Kurihara and Sato, 2004), most re-cent work has followed Klein and Manning (2004)in developing models for the induction of projec-tive dependency grammars.
It has been shown thatmore sophisticated probability models (Headden IIIet al 2009; Gillenwater et al 2011; Cohen andSmith, 2010) and learning regimes (Spitkovsky etal., 2010), as well as the incorporation of prior lin-guistic knowledge (Cohen and Smith, 2009; Berg-Kirkpatrick and Klein, 2010; Naseem et al 2010)can lead to significant improvement over Klein andManning?s baseline model.
The use of dependencygrammars circumvents the question of how to obtainan appropriate inventory of categories, since depen-dency parses are simply defined by unlabeled edgesbetween the lexical items in the sentence.
But de-pendency grammars make it also difficult to cap-ture non-local structures, and Blunsom and Cohn(2010) show that it may be advantageous to refor-mulate the underlying dependency grammar in termsof a tree-substitution grammar (TSG) which pairswords with treelets that specify the number of leftand right dependents they have.
In this paper, weexplore yet another option: instead of dependencygrammars, we use Combinatory Categorial Gram-mar (CCG, Steedman (1996; 2000)), a linguisticallyexpressive formalism that pairs lexical items withrich categories that capture all language-specific in-formation.
This may seem a puzzling choice, sinceCCG requires a significantly larger inventory of cat-egories than is commonly assumed for CFGs.
How-ever, unlike CFG nonterminals, CCG categories arenot arbitrary symbols: they encode, and are deter-mined by, the basic word order of the language andthe number of arguments each word takes.
CCG isvery similar to TSG in that it also pairs lexical itemswith rich items that capture all language-specific in-formation.
Like TSG and projective dependencygrammars, we restrict ourselves to a weakly context-free fragment of CCG.
But while TSG does not dis-tinguish between argument and modifier dependen-cies, CCG makes an explicit distinction between thetwo.
And while the elementary trees of Blunsomand Cohn (2010)?s TSG and their internal nodel la-bels have no obvious linguistic interpretation, thesyntactic behavior of any CCG constituent can bedirectly inferred from its category.
To see whether75the algorithm has identified the basic syntactic prop-erties of the language, it is hence sufficient to in-spect the induced lexicon.
Conversely, Boonkwanand Steedman (2011) show that knowledge of thesebasic syntactic properties makes it very easy to cre-ate a language-specific lexicon for accurate unsu-pervised CCG parsing.
We have recently proposedan algorithm for inducing CCGs (Bisk and Hocken-maier, 2012b) that has been shown to be competitivewith other approaches even when paired with a verysimple probability model (Gelling et al 2012).
Inthis paper, we pair this induction algorithm with anovel nonparametric Bayesian model that is basedon a different factorization of CCG derivations, andshow that it outperforms our original model andmany other approaches on a large number of lan-guages.
Our results indicate that the use of CCGyields grammars that are significantly more robustwhen dealing with longer sentences than most de-pendency grammar-based approaches.2 Combinatory Categorial GrammarCombinatory Categorial Grammar (Steedman,2000) is a linguistically expressive, lexicalizedgrammar formalism that associates rich syntactictypes with words and constituents.
For simplicity,we restrict ourselves to the standard two atomictypes S (sentences) and N (encompassing bothnouns and noun phrases) from which we recursivelybuild categories.
Complex categories are of theform X/Y or X\Y, and represent functions whichreturn a result of type X when combined with anargument of type Y.
The directionality of the slashindicates whether the argument precedes or followsthe functor.
We write X|Y when the direction of theslash does not matter.The CCG lexicon encodes all language-specificinformation.
It pairs every word with a set of cate-gories that define both its specific syntactic behavioras well as the overall word order of the language:N : {he, girl , lunch, ...} N/N : {good , the, eating , ...}S\N : {sleeps, ate, eating , ...} (S\N)/N : {sees, ate, ...}S\S : {quickly , today ...} (S\N)/(S\N) : {good , the, ...}To draw a simple contrast, in Spanish we wouldexpect adjectives to take the category N\N becauseSpanish word ordering dictates that the adjective fol-low the noun.
The lexical categories also captureword-word dependencies: head-argument relationsare captured by the lexical category of the head (e.g.
(S\N)/N), whereas head-modifier relations are cap-tured by the lexical category of the modifier, whichis of the form X\X or X/X, and may take furtherarguments of its own.
Our goal will be to automati-cally learn these types of lexicons for a language.
InFigure 3, we juxtapose several such lexicons whichwere automatically discovered by our system.The rules of CCG are defined by a small set ofof combinatory rules, which are traditionally writ-ten as schemas that define how constituents can becombined in a bottom-up fashion (although genera-tive probability models for CCG view them in a top-down manner, akin to CFG rules).
The first, andmost obvious, of these rules is function application:X/Y Y ?
X (B0>)Y X\Y ?
X (B0<)Here the functor X/Y or X\Y is applied to anargument Y resulting in X.
While standard CCGhas a number of additional combinatory rules (type-raising, generalized variants of composition andsubstitution) that increase its generative capacity be-yond context-free grammars and allow an eleganttreatment of non-local dependencies arising in ex-traction, coordination and scrambling, we followBisk and Hockenmaier (2012b) and use a restrictedfragment, without type-raising, that allows only ba-sic composition and is context-free:X/Y Y/Z ?
X/Z (B1> )X/Y Y\Z ?
X\Z (B1X>)Y\Z X\Y ?
X\Z (B1< )Y/Z X\Y ?
X/Z (B1X<)The superscript 1 denotes the arity of the compo-sition which is too low to recover non-projective de-pendencies, and our grammar is thus weakly equiva-lent to the dependency grammar representations thatare commonly used for grammar induction.
Themain role of composition in our fragment is that itallows sentential and verb modifiers to both take cat-egories of the form S\S and S/S.
Composition in-76troduces spurious ambiguities, which we eliminateby using Eisner (1996)?s normal form.1Coordinating conjunctions have a special cate-gory conj, and we binarize coordination as follows(Hockenmaier and Steedman, 2007):X X[conj] ?&1 X (&1)conj X ?&2 X[conj] (&2)3 Category inductionUnlike dependency grammars, CCG requires an in-ventory of lexical categories.
Given a set of lexicalcategories, the combinatory rules define the set ofparses for each sentence.
We follow the algorithmproposed by Bisk and Hockenmaier (2012b) to au-tomatically induce these categories.
The lexicon isinitialized by pairing all nominal tags (nouns, pro-nouns and determiners) with the category N, all verbtags with the category S, and coordinating conjunc-tions with the category conj:CONJ ?
conjDET, NOUN, NUM, PRON ?
NVERB ?
SAlthough our lexicons are defined over corpus-specific POS tags, we use a slightly modified versionof Petrov et al(2012)?s Universal POS tagset to cat-egorize them into these broad classes.
The primarychanges we make to their mappings are the additionof a distinction (where possible) between subordi-nating and coordinating conjunctions and betweenmain and auxiliary verbs2.Since the initial lexicon consists only of atomiccategories, it cannot parse any complex sentences:The man ate quicklyDT NNS VBD RB- N S -Complex lexical categories are induced by con-sidering the local context in which tokens appear.Given an input sentence, and a current lexicon whichassigns categories to at least some of the tokens inthe sentence, we apply the following two rules toadd new categories to the lexicon: The argumentrule allows any lexical tokens that have categoriesother than N and conj to take immediately adjacent1The normal-form of Hockenmaier and Bisk (2010) is notrequired for this fragment of CCG.2This distinction was suggested by the authors (p.c.
)Ns as arguments.
The modifier rule allows any to-ken (other than coordinating conjunctions that ap-pear in the middle of sentences) to modify an imme-diate neighbor that has the category S or N or is amodifier (S|S or N|N) itself.The man ate quicklyDT NNS VBD RBN/N N, S/S S, N\N S\SS\NThese rules can be applied iteratively to formmore complex categories.
We restrict lexical cate-gories to a maximal arity of 2, and disallow the cat-egory (S/N)\N, since it is equivalent to (S\N)/N.The man ate quicklyDT NNS VBD RBN/N, N, S/S S, N\N, S\S,(S/S)/(S/S)(N\N)/(N\N) S\N (N\N)\(N\N)(N/N)\(N/N) (S/S)\(S/S)(S\S)/(S\S)The resultant, overly general, lexicon is then usedto parse the training data.
Each complete parse hasto be of category S or N, with the constraint thatsentences that contain a main verb can only formparses of category S.4 A new probability model for CCGGenerative models define the probability of a parsetree ?
as the product of individual rule probabili-ties.
Our previous work (Bisk and Hockenmaier,2012b) uses the most basic model of Hockenmaierand Steedman (2002), which first generates the headdirection (left, right, unary, or lexical), followed bythe head category, and finally the sister category.
3This factorization does not take advantage of theunique functional nature of CCG.
We therefore in-troduce a new factorization we call the ArgumentModel.
It exploits the fact that CCG imposes strongconstraints on a category?s left and right children,since these must combine to create the parent typevia one of the combinators.
In practice this meansthat given the parent X/Z, the choice of combinator4c and an argument Y we can uniquely determine thecategories of the left and right children:3Huang et al(2012) present a (deficient) variant andBayesian extension of the Bisk and Hockenmaier (2012b)model without k-best smoothing that both underperform ourpublished results.4If X is an atomic category, only application is possible.77Parent c ?
Left RightX/Z B0> (X/Z)/Y YB0< Y (X/Z)\YB1> X/Y Y/ZB1< Y/Z X\Yand correspondingly for X\Z:Parent c ?
Left RightX\Z B0> (X\Z)/Y YB0< Y (X\Z)\YB1> X/Y Y\ZB1< Y\Z X\YWhile type-changing and raising are not used inthis work the model?s treatment of root productionsextends easily to handle these other unary cases.
Wesimply treat the argument Y as the unary outcome sothat the parent, combinator and argument uniquelyspecify every detail of the unary rule:Parent c ?
YTOP TOP ?
{S,N}S/(S\N) T< NS\(S/N) T> NWe still distinguish the same rule types as before(lexical, unary, binary with head left/right), leadingus to the following model definition:Given: P := X/Zwhere type(t) ?
{Left,Right,Unary,Lex}p(t|P)?
{p(w|P, t) Lexp(Y|P, t)?
p(c|P, t,Y) o.w.Argument CombinatorNote that this model generates only one CCG cat-egory but uniquely defines the two children of a par-ent node.
We will see below that this greatly simpli-fies the development of non-parametric extensions.5 HDP-CCG: a nonparametric modelSimple generative models such as PCFGs or Biskand Hockenmaier (2012b)?s CCG model are notrobust in the face of sparsity, since they assignzero probability to any unseen event.
Sparsity isa particular problem for formalisms like CCG thathave a rich inventory of object types.
Nonpara-metric Bayesian models, e.g.
Dirichlet Processes(Teh, 2010) or their hierarchical variants (Teh etal., 2006) and generalizations (Teh, 2006) overcomethis problem in a very elegant manner, and are usedby many state-of-the-art grammar induction systems(Naseem et al 2010; Blunsom and Cohn, 2010;Boonkwan and Steedman, 2011).
They also im-pose a rich-getting-richer behavior that seems to beadvantageous in many modeling applications.
Bycontrast, Bisk and Hockenmaier (2012b) propose aweighted top-k scheme to address these issues in anad-hoc manner.The argument model introduced above lends it-self particularly well to nonparametric extensionssuch as the standard Hierarchical Dirichlet Pro-cesses (HDP).
In this work the size of the grammarand the number of productions are fixed and small,but we present the formulation as infinite to allow foreasy extension in the future.
Specifically, this frame-work allows for extensions which grow the grammarduring parsing/training or fully lexicalize the pro-ductions.
Additionally, while our current work usesonly a restricted fragment of CCG that has only afinite set of categories, the literature?s generalizedvariants of composition make it possible to gener-ate categories of unbounded arity.
We therefore be-lieve that this is a very natural probabilistic frame-work for CCG, since HDPs make it possible to con-sider a potentially infinite set of categories that caninstantiate the Y slot, while allowing the model tocapture language-specific preferences for the set ofcategories that can appear in this position.The HDP-CCG model In Bayesian models,multinomials are drawn from a corresponding n-dimensional Dirichlet distribution.
The DirichletProcess (DP) generalizes the Dirichlet distributionto an infinite number of possible outcomes, allowingus to deal with a potentially infinite set of categoriesor words.
DPs are defined in terms of a base dis-tribution H that corresponds to the mean of the DP,and a concentration or shape parameter ?.
In a Hi-erarchical Dirichlet Process (Teh et al 2006), thereis a hierarchy of DPs, such that the base distributionof a DP at level n is a DP at level n?
1.The HDP-CCG (Figure 1) is a reformulation ofthe Argument Model introduced above in terms ofHierarchical Dirichlet Processes.5 At the heart ofthe model is a distribution over CCG categories.
Bycombining a stick breaking process with a multino-mial over categories we can define a DP over CCG5An alternative HDP model for semantic parsing with CCGis proposed by Kwiatkowski et al(2012).78HDP-CCG1) Draw global parametersDefine MLE root parameter ?TOPDraw top-level symbol weights ?Y ?
GEM(?Y )Draw top-level lexical weights ?L ?
GEM(?L)For each grammar symbol z ?
{1, 2, ...}:Define MLE rule type parameters ?TzDraw argument parameters ?Yz ?
DP(?Y, ?Y )Draw lexical emission parameters ?Lz ?
DP(?L, ?L)For each grammar symbol y ?
{1, 2, ...}:Define MLE combinator parameters ?Cz,y2) For each parse tree:Generate root node zTOP ?
Binomial(?TOP )For each node i in the parse tree:Choose rule type ti ?Multinomial(?Tzi )If ti == Lex:Emit terminal symbol xi ?Multinomial(?Lzi )If ti == Left/Right/Unary:Generate argument category yi ?Multinomial(?Yzi )Generate combinator ci ?Multinomial(?Czi,yi )Deterministically create zL(i) (and zR(i) if binary)ziyi cizL(i) zR(i)xL(i) xR(i)z ?
?y?Y?T?C?L?Y?LBecause we are working with CCG, theparent zi, argument yi and combinator ciuniquely define the two children categories(zL(i), zR(i)).
The dashed arrows here rep-resent the deterministic process used togenerate these two categories.Figure 1: The HDP-CCG has two base distributions, one over the space of categories and the other overwords (or tags).
For every grammar symbol, an argument distribution and emission distribution is drawnfrom the corresponding Dirichlet Processes.
In addition, there are several MLE distributions tied to a givensymbol for generating rule types, combinators and lexical tokens.categories whose stick weights (?Y ) correspond tothe frequency of the category in the corpus.
Next webuild the hierarchical component of our model bychoosing an argument distribution (?Y ), again overthe space of categories, for every parent X/Z.
Thisargument distribution is drawn from the previouslydefined base DP, allowing for an important level ofparameter tying across all argument distributions.While the base DP does define the mean aroundwhich all argument distributions are drawn, we alsorequire a notion of variance or precision which de-termines how similar individual draws will be.
Thisprecision is determined by the magnitude of the hy-perparameter ?Y .
This hierarchy is paralleled forlexical productions which are drawn from a unigrambase DP over terminal symbols controlled by ?L.For simplicity we use the same scheme for settingthe values for ?L as ?Y .
We present experimen-tal results in which we vary the value of ?Y as afunction of the number of outcomes allowed by thegrammar for argument categories or the corpus inthe case of terminal symbols.
Specifically, we set?Y = np for conditioning contexts with n out-comes.
Since Liang et al(2009) found that the idealvalue for alpha appears to be superlinear but sub-quadratic in n, we present results where p takes thevalues 0, 1.0, 1.5, and 2.0 to explore the range fromuniform to quadratic.
This setting for ?
is the onlyfree parameter in the model.
By controlling preci-sion we can tell the model to what extent global cor-pus statistics should be trusted.
We believe this hasa similar effect to Bisk and Hockenmaier (2012b)?stop-k upweighting and smoothing scheme.One advantage of the argument model is that itonly requires a single distribution over categories foreach binary tree.
In contrast to similar proposals forCFGs (Liang et al 2007), which impose no formalrestrictions on the nonterminals X, Y, Z that can ap-pear in a rewrite rule X?
Y Z, this greatly sim-plifies the modeling problem (yielding effectively amodel that is more akin to nonparametric HMMs),since it avoids the need to capture correlations be-79tween different base distributions for Y and Z.Variational Inference HDPs need to be estimatedwith approximate techniques.
As an alternative toGibbs sampling (Teh et al 2006), which is exact,but typically very slow and has no clear convergencecriteria, variational inference algorithms (Bishop,2006; Blei and Jordan, 2004) estimate the parame-ters of a truncated model to maximize a lower boundof the likelihood of the actual model.
This allows forfactorization of the model and a training procedureanalogous to the Inside-Outside algorithm (Lari andYoung, 1991), allowing training to run very quicklyand in a trivially parallelizable manner.To initialize the base DP?s stick weights, we fol-low the example of Kurihara et al(2007) and usean MLE model initialized with uniform distributionsto compute global counts for the categories in ourgrammar.
When normalized these provide a betterinitialization than a uniform set of weights.
Updatesto the distributions are then performed in a coordi-nate descent manner which includes re-estimation ofthe base DPs.In variational inference, multinomial weights Wtake the place of probabilities.
The weights for anoutcome Y with conditioning variable P are com-puted by summing pseudocounts with a scaled meanvector from the base DP.
The computation involvesmoving in the direction of the gradient of the Dirich-let distribution which results in the following differ-ence of Digammas (?
):WP (Y ) = ?
(C(P, Y ) + ?P?Y )??
(C(P, ?)
+ ?P )Importantly, the Digamma and multinomialweights comprise a righ-get-richer scheme, biasingthe model against rare outcomes.
In addition, sincevariational inference is done by coordinate descent,it is trivially parallelizeable.
In practice, training andtesting our models on the corpora containing sen-tences up to length 15 used in this paper takes be-tween one minute to at most three hours on a single12-core machine depending on their size.6 EvaluationAs is standard for this task, we evaluate our systemsagainst a number of different dependency treebanks,and measure performance in terms of the accuracy ofdirected dependencies (i.e.
the percentage of wordsin the test corpus that are correctly attached).
We usethe data from the PASCAL challenge for grammarinduction (Gelling et al 2012), the data from theCoNLL-X shared task (Buchholz and Marsi, 2006)and Goldberg (2011)?s Hebrew corpus.Converting CCG derivations into dependencies ismostly straightforward, since the CCG derivationidentifies the root word of each sentence, and head-argument and head-modifier dependencies are easilyread off of CCG derivations, since the lexicon de-fines them explicitly.
Unlike dependency grammar,CCG is designed to recover non-local dependenciesthat arise in control and binding constructions aswell as in wh-extraction and non-standard coordi-nation, but since this requires re-entrancies, or co-indexation of arguments (Hockenmaier and Steed-man, 2007), within the lexical categories that triggerthese constructions, our current system returns onlylocal dependencies.
But since dependency gram-mars also captures only local dependencies, this hasno negative influence on our current evaluation.However, a direct comparison between depen-dency treebanks and dependencies produced byCCG is more difficult (Clark and Curran, 2007),since dependency grammars allow considerablefreedom in how to analyze specific constructionssuch as verb clusters (which verb is the head?
)prepositional phrases and particles (is the head thenoun or the preposition/particle?
), subordinatingconjunctions (is the conjunction a dependent of thehead of the main clause and the head of the embed-ded clause a dependent of the conjunction, or viceversa?)
and this is reflected in the fact that the tree-banks we consider often apply different conventionsfor these cases.
Although remedying this issue isbeyond the scope of this work, these discrepanciesvery much hint at the need for a better mechanism toevaluate linguistically equivalent structures or tree-bank standardization.The most problematic construction is coordina-tion.
In standard CCG-to-dependency schemes, bothconjuncts are independent, and the conjunction itselfis not attached to the dependency graph, whereas de-pendency grammars have to stipulate that either oneof the conjuncts or the conjunction itself is the head,with multiple possibilities of where the remaining80constituents attach.
In addition to the standard CCGscheme, we have identified five main styles of con-junction in our data (Figure 2), although several cor-pora distinguish multiple types of coordinating con-junctions which use different styles (not all shownhere).
Since our system has explicit rules for coordi-nation, we transform its output into the desired targetrepresentation that is specific to each language.7 ExperimentsWe evaluate our system on 13 different languages.In each case, we follow the test and training regimesthat were used to obtain previously published resultsin order to allow a direct comparison.
We com-pare our system to the results presented at the PAS-CAL Challenge on Grammar Induction (Gelling etal., 2012)6, as well as to Gillenwater et al(2011)and Naseem et al(2012).
We use Nivre (2006)?sPenn2Malt implementation of Collins (2003)?s headrules to translate the WSJ Penn Treebank (Marcuset al 1993) into dependencies.
Finally, when train-ing the MLE version of our model we use a simplesmoothing scheme which defines a small rule proba-bility (e?15) to prevent any rule used during trainingfrom going to zero.7.1 PASCAL Challenge on GrammarInductionIn Table 1, we compare the performance of the ba-sic Argument model (MLE), of our HDP model withfour different settings of the hyperparameters (as ex-plained above) and of the systems presented in thePASCAL Challenge on Grammar Induction (Gellinget al 2012).
The systems in this competition wereinstructed to train over the full dataset, including theunlabelled test data, and include Bisk and Hocken-maier (2012a)?s CCG-based system (BH) to Cohn etal.
(2010)?s reimplementation of Klein and Manning(2004)?s DMV model in a tree-substitution gram-mar framework (BC), as well as three other de-pendency based systems which either incorporateNaseem et al(2010)?s rules in a deterministic fash-ion (S?gaard, 2012), rely on extensive tuning on6Numbers are from personal correspondence with the orga-nizers.
The previously published numbers are not comparableto literature due to an error in the evaluation.
http://wiki.cs.ox.ac.uk/InducingLinguisticStructure/ResultsDepComparablethe development set (Tu, 2012) or incorporate mil-lions of additional tokens from Wikipedia to esti-mate model parameters (Marecek and Zabokrtsky,2012).
We ignore punctuation for all experimentsreported in this paper, but since the training data(but not the evaluation) includes punctuation marks,participants were free to choose whether to includepunctuation or ignore it.While BH is the only other system with directlyinterpretable linguistic output, we also include a di-rect comparison with BC, whose TSG representa-tion is equally expressive to ours.
Finally we presenta row with the maximum performance among theother three models.
As we have no knowledge ofhow much data was used in the training of other sys-tems we simply present results for systems trainedon length 15 (not including punctuation) sentencesand then evaluated at lengths 10 and 15.The MLE version of our model shows rather vari-able performance: although its results are particu-larly bad on Basque (Eu), it outperforms both BHand BC on some other settings.
By contrast, theHDP system is always better than the MLE model.It outperforms all other systems on half of the cor-pora.
On average, it outperforms BH and BC by10.3% and 9.3% on length 10, or 9.7% and 7.8 %on length 15 respectively.
The main reason why oursystem does not outperform BC by an even highermargin is the very obvious 11.4%/11.5% deficit onSlovene.
However, the Slovene dependency tree-bank seems to follow a substantially different anno-tation scheme.
In particular, the gold standard an-notation of the 1,000 sentences in the Slovene de-velopment set treats many of them as consisting ofindependent sentences (often separated by punctua-tion marks that our system has no access to), so thatthe average number of roots per sentence is 2.7:>>?verjetibelievetiI,,??
?jeismehkosoftreklasaidWhen our system is presented with these shortcomponents in isolation, it oftentimes analyzes themcorrectly, but since it has to return a tree with a sin-gle root, its performance degrades substantially.We believe the HDP performs so well as com-pared to the MLE model because of the influenceof the shared base distribution, which allows the81Ar, Eu, Cs, Nl,WSJ, Ch, He Da, He Es, Bg, De, Pt Sv, Sl Janoun conj noun noun conj noun noun conj noun noun conj noun noun conj nounFigure 2: In the treebanks used for evaluation different standards exist for annotating coordination.
Whilenot exhaustive, this table demonstrates five of the most common schemes used in the literature.
Syntacticallythese are identical and traditionally CCG draws arcs only to the arguments without attaching the conjunction.For the purposes of comparison with the literature we have implemented these five translation schemes.Arabic Danish Slovene Swedish Dutch Basque Portuguese WSJ Childes Czech# Tokens 5,470 25,341 54,032 61,877 78,737 81,345 158,648 163,727 290,604 436,126# Tags 20 24 36 30 304 14 23 36 69 62PASCAL BC 60.8/58.4 44.7/39.4 62.6/57.9 63.2/56.6 51.8/52.0 53.0/48.9 52.4/50.2 68.6/63.3 47.4/46.1 47.9/43.1Max 67.2/66.8 60.1/56.0 65.6/61.8 72.8/63.4 51.1/47.6 53.7/47.8 67.0/61.8 71.2/64.8 56.0/54.5 58.3/54.4BH 41.6/43.7 46.4/43.8 49.6/43.9 63.7/57.0 49.7/43.6 45.1/39.6 70.8/67.2 68.2/59.6 61.4/59.8 45.0/38.9ThisworkMLE 41.6/42.9 43.4/39.2 46.1/41.1 70.1/59.7 52.2/47.2 29.6/26.5 62.2/59.7 59.5/52.4 53.3/51.9 50.5/45.8HDP0.0 48.0/50.0 63.9/58.5 44.8/39.8 67.6/62.1 45.0/33.9 41.6/39.1 71.0/66.0 59.8/52.9 56.3/55.2 54.0/49.0HDP1.0 45.6/47.1 45.7/42.3 53.9/46.9 74.5/66.9 58.5/54.4 50.1/44.6 65.1/60.6 64.3/56.5 71.5/70.3 55.8/50.7HDP1.5 49.6/50.4 58.7/54.4 53.2/48.2 74.3/67.1 57.4/54.5 50.6/45.0 70.0/64.7 65.5/57.2 69.6/68.6 55.6/50.3HDP2.0 66.4/65.1 56.5/49.5 54.2/46.4 71.6/64.1 51.7/48.3 49.4/43.3 76.3/70.5 70.7/62.9 74.1/73.3 54.4/48.5+/?
-0.8/-1.7 +3.8/+2.5 -11.4/-15.4 +1.7/+3.5 +6.7/+2.4 -3.1/-3.9 +5.5/+3.3 -0.5/-1.9 +12.7/+13.5 -2.5/-3.7Table 1: A comparison of the basic Argument model (MLE) and four hyper-parameter settings of the HDP-CCG against two syntactic formalisms that participated in the PASCAL Challenge (Gelling et al 2012),BH (Bisk and Hockenmaier, 2012a) and BC (Blunsom and Cohn, 2010), in addition to a max over all otherparticipants.
We trained on length 15 data (punctuation removed), including the test data as recommendedby the organizers.
The last row indicates the difference between our best system and the competition.global category distribution to influence each of themore specific distributions.
Further, it provides avery simple knob in the choice of hyperparame-ters, which has a substantial effect on performance.A side effect of the hyperparameters is that theirstrength also determines the rate of convergence.This may be one of the reasons for the high vari-ance seen in the four settings tested, although wenote that since our initialization is always uniform,and not random, consecutive runs do not introducevariance in the model?s performance.7.2 Comparison with systems that capturelinguistic constraintsSince our induction algorithm is based on the knowl-edge of which POS tags are nouns and verbs, wecompare in Table 2 our system to Naseem et al(2010), who present a nonparametric dependencymodel that incorporates thirteen universal linguisticconstraints.
Three of these constraints correspondto our rules that verbs are the roots of sentences andmay take nouns as dependents, but the other ten con-straints (e.g.
that adjectives modify nouns, adverbsmodify adjectives or verbs, etc.)
have no equivalentin our system.
Although our system has less priorknowledge, it still performs competitively.On the WSJ, Naseem et aldemonstrate the im-portance and effect of the specific choice of syntacticrules by comparing the performance of their systemwith hand crafted universal rules (71.9), with En-glish specific rules (73.8), and with rules proposedby Druck et al(2009) (64.9).
The performance ofNaseem et als system drops very significantly assentence length (and presumable parse complexity)82Sl Es Da Pt Sv?#Tokens 3.8K 4.2K 9.5K 15K 24KN10 50.9 67.2 51.9 71.5 63.3HDP 56.6 62.1 51.5 74.7 69.8Table 2: A comparison of our system with Naseemet al(2010), both trained and tested on the length 10training data from the CoNLL-X Shared Task.increases, whereas our system shows significantlyless decline, and outperforms their universal systemby a significant margin.7?
10 ?
20Naseem Universal Rules 71.9 50.4Naseem English Rules 73.8 66.1HDP-CCG 68.2 64.2HDP-CCG (train ?
20) 71.9In contrast to Spitkovsky et al(2010), who reportedthat performance of their dependency based systemdegrades when trained on longer sentences, our per-formance on length 10 sentences increases to 71.9when we train on sentences up to length 20.Another system that is also based on CCG, butcaptures significantly more linguistic knowledgethan ours, was presented by Boonkwan and Steed-man (2011), who achieve an accuracy of 74.5 onWSJ10 section 23 (trained on sections 02-22).
Us-ing the same settings, our system achieves an accu-racy of 68.4.
Unlike our approach, Boonkwan andSteedman do not automatically induce an appropri-ate inventory of lexical category, but use an exten-sive questionnaire that defines prototype categoriesfor various syntactic constructions, and requires sig-nificant manual engineering of which POS tags aremapped to what categories to generate a language-specific lexicon.
However, their performance de-grades significantly when only a subset of the ques-tions are considered.
Using only the first 14 ques-tions, covering facts about the ordering of subjects,verbs and objects, adjectives, adverbs, auxiliaries,adpositions, possessives and relative markers, theyachieve an accuracy of 68.2, which is almost iden-7Our earlier generative model showed similar behavior, al-though the results in Bisk and Hockenmaier (2012b) are notdirectly comparable due to differences in the data.Sl Es Da Pt Sv#Tokens 3,857 4,230 9,549 15,015 24,021G10 51.2 62.4 47.2 54.3 48.6HDP 57.9 65.4 49.3 73.5 73.2Bg WSJ Nl Ja De#Tokens 38,220 42,442 43,405 43,501 77,705G10 59.8 64.4 47.5 60.2 47.4HDP 66.1 70.3 56.2 64.1 68.4Table 3: A comparison of our system with Gillenwa-ter et al(2010), both trained on the length 10 train-ing data, and tested on the length 10 test data, fromthe CoNLL-X Shared task.tical to ours, even though we use significantly lessinitial knowledge.
However, the lexicons we presentbelow indicate that we are in fact learning many ofthe very exact details that in their system are con-structed by hand.
The remaining 14 questions inBoonkwan and Steedman?s questionnaire cover lessfrequent phenomena such as the order of negativemarkers, dative shift, and pro-drop.
The obvious ad-vantage of this approach is that this allows them todefine a much more fine-grained inventory of lexicalcategories than our system can automatically induce.We also stipulate that for certain languages knowl-edge of pro-drop could play a significant role in thesuccess of their approach: if complete sentences areallowed to be of the form S\N or S/N, the same lex-ical category can be used for the verb regardless ofwhether the subject is present or has been dropped.7.3 Additional LanguagesIn order to provide results on additional languages,we present in Table 3 a comparison to the work ofGillenwater et al(2010) (G10), using the ConLL-XShared Task data (Buchholz and Marsi, 2006).
Fol-lowing Gillenwater et al we train only on sentencesof length 10 from the training set and evaluate on thetest set.
Since this is a different training regime, andthese corpora differ for many languages from that ofthe PASCAL challenge, numbers from Table 1 can-not be compared directly with those in Table 3.
Wehave also applied our model to Goldberg (2011)?sHebrew corpus, where it achieves an accuracy of62.1 (trained and tested on all sentences length 10;7,253) and 59.6 (length 15; 21,422 tokens).83Arabic % Swedish % WSJ % Childes % Japanese % Czech %VERB (S\N)/N 56 S 45 S\N 52 S/N 44 S 84 S 26(S/N)/N 29 S\N 20 (S\N)/N 19 S 37 S\N 25ADP N\N 68 (S\S)/N 49 (S\S)/N 46 (S\S)/N 45 (S/S)\N 44 (S\S)/N 42N/N 21 (N\N)/N 25 (N\N)/N 20 N/N 25 N\N 23 (S/S)/N 26NOUN N\N 50 N 91 N 79 N 89 N 73 N 74N 35 N/N 12ADJ N\N 82 N/N 50 N/N 70 N/N 46 S/S 64 N/N 55Figure 3: Partial lexicons demonstrating language specific knowledge learned automatically for five lan-guages.
For ease of comparison between languages, we use the universal tag label (Verb, Adposition, Nounand Adjective).
Shown are the most common categories and the fraction of occurrences of the tag that areassigned this category (according to the Viterbi parses).8 The Induced LexiconsSince our approach is based on a lexicalized for-malism such as CCG, our system automatically in-duces lexicons that pair words (or, in our case, POS-tags) with language-specific categories that capturetheir syntactic behavior.
If our approach is success-ful, it should learn the basic syntactic properties ofeach language, which will be reflected in the corre-sponding lexicon.
In Figure 3 one sees how verbssubcategorize differently, how word ordering differsby language, and how the attachment structures ofprepositions are automatically discovered and differacross languages.
In Arabic, for example, the sys-tem learns that word order is variable and thereforethe verb must allow for both SVO and VOS styleconstructions.
We generally learn that adpositions(prepositions or postpositions) take nouns as argu-ments.
In Czech, PPs can appear before and after theverb, leading to two different categories ((S\S)/Nand (S/S)/N).
Japanese has postpositions that ap-pear in preverbal position ((S/S)\N), but when thiscategory is assigned to nominal particles that cor-respond to case markers, it effectively absorbs thenoun, leading to a preference for verbs that do nottake any arguments (S), and to a misanalysis of ad-jectives as verb modifiers (S/S).
Our lexicons alsoreflect differences in style: while Childes and theWSJ are both English, they represent very differentregisters.
We learn that subjects are mostly absent inthe informal speech and child-directed instructionscontained in Childes, while effectively mandatory inthe Wall Street Journal.9 ConclusionsThis paper has introduced a novel factorization forCCG models and showed how when combined withnon-parametric Bayesian statistics it can competewith every other grammar induction system cur-rently available, including those that capture a sig-nificant amount of prior linguistic knowledge.
Theuse of a powerful syntactic formalism proves ben-eficial both in terms of requiring very limited uni-versal knowledge and robustness at longer sentencelengths.
Unlike standard grammar induction sys-tems that are based on dependency grammar, oursystem returns linguistically interpretable lexiconsfor each language that demonstrate it has discov-ered their basic word order.
Of particular note is thesimplicity of the model both algorithmically and interms of implementation.
By not faltering on longersentences or requiring extensive tuning, the systemcan be easily and quickly deployed on a new lan-guage and return state of the art performance andeasily interpretable lexicons.
In this paper, we haveapplied this model only to a restricted fragment ofCCG, but future work will address the impact of lex-icalization and the inclusion of richer combinators.10 AcknowledgementsThis work is supported by NSF CAREER award1053856 (Bayesian Models for Lexicalized Gram-mars).84ReferencesTaylor Berg-Kirkpatrick and Dan Klein.
2010.
Phyloge-netic Grammar Induction.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1288?1297, Uppsala, Sweden, July.Christopher Bishop.
2006.
Pattern Recognition and Ma-chine Learning.
Springer-Verlag, August.Yonatan Bisk and Julia Hockenmaier.
2012a.
Inductionof Linguistic Structure with Combinatory CategorialGrammars.
In NAACL HLT Workshop on Induction ofLinguistic Structure, pages 90?95, Montre?al, Canada,June.Yonatan Bisk and Julia Hockenmaier.
2012b.
SimpleRobust Grammar Induction with Combinatory Cate-gorial Grammars.
In Proceedings of the Twenty-SixthConference on Artificial Intelligence (AAAI-12), pages1643?1649, Toronto, Canada, July.David M Blei and Michael I Jordan.
2004.
VariationalMethods for the Dirichlet Process.
In Proceedings ofthe Twenty-First International Conference on MachineLearning (ICML 2004), Banff, Alberta, Canada, July.Phil Blunsom and Trevor Cohn.
2010.
UnsupervisedInduction of Tree Substitution Grammars for Depen-dency Parsing.
Proceedings of the 2010 Conferenceon Empirical Methods of Natural Language Process-ing, pages 1204?1213, October.Prachya Boonkwan and Mark Steedman.
2011.
Gram-mar Induction from Text Using Small Syntactic Proto-types.
In Proceedings of 5th International Joint Con-ference on Natural Language Processing, pages 438?446, Chiang Mai, Thailand, November.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.
InProceedings of the 10th Conference on ComputationalNatural Language Learning (CoNLL-X), pages 149?164, New York City, June.Glenn Carroll and Eugene Charniak.
1992.
Two Exper-iments on Learning Probabilistic Dependency Gram-mars from Corpora.
Working Notes of the WorkshopStatistically-Based NLP Techniques, pages 1?13.Eugene Charniak.
1993.
Statistical Language Learning.The MIT Press, Cambridge, Massachusetts.Stephen Clark and James R Curran.
2007.
Formalism-Independent Parser Evaluation with CCG and Dep-Bank.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages248?255, Prague, Czech Republic, June.Alex Clark.
2001.
Unsupervised Language Acquisition:Theory and Practice.
Ph.D. thesis, University of Sus-sex, September.Shay B Cohen and Noah A Smith.
2009.
VariationalInference for Grammar Induction with Prior Knowl-edge.
Proceedings of the ACL-IJCNLP 2009 Confer-ence Short Papers, pages 1?4.Shay B Cohen and Noah A Smith.
2010.
Covariancein Unsupervised Learning of Probabilistic Grammars.The Journal of Machine Learning Research, pages3117?3151, November.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing Tree-Substitution Grammars.
TheJournal of Machine Learning Research, 11:3053?3096, November.Michael Collins.
2003.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Computational Lin-guistics, 29(4):589?637, December.Gregory Druck, Gideon Mann, and Andrew McCal-lum.
2009.
Semi-supervised Learning of DependencyParsers using Generalized Expectation Criteria.
InProceedings of the Joint Conference of the 47th An-nual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP, pages 360?368, Suntec, Singapore, Au-gust.Jason Eisner.
1996.
Efficient Normal-Form Parsing forCombinatory Categorial Grammar.
In Proceedings ofthe 34th Annual Meeting of the Association for Com-putational Linguistics, pages 79?86, Santa Cruz, Cali-fornia, USA, June.Douwe Gelling, Trevor Cohn, Phil Blunsom, and Joa?o VGraca.
2012.
The PASCAL Challenge on GrammarInduction.
In NAACL HLT Workshop on Induction ofLinguistic Structure, pages 64?80, Montre?al, Canada,June.Jennifer Gillenwater, Kuzman Ganchev, Joa?o V Graca,Fernando Pereira, and Ben Taskar.
2010.
Sparsity inDependency Grammar Induction.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics, pages 194?199, Uppsala, Swe-den, July.Jennifer Gillenwater, Kuzman Ganchev, Joa?o V Graca,Fernando Pereira, and Ben Taskar.
2011.
PosteriorSparsity in Unsupervised Dependency Parsing.
TheJournal of Machine Learning Research, 12:455?490,February.Yoav Goldberg.
2011.
Automatic Syntactic Processing ofModern Hebrew.
Ph.D. thesis, Ben-Gurion Universityof the Negev, November.William P Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving Unsupervised DependencyParsing with Richer Contexts and Smoothing.
In Pro-ceedings of Human Language Technologies: The 2009Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages101?109, Boulder, Colorado, June.Julia Hockenmaier and Yonatan Bisk.
2010.
Normal-form parsing for Combinatory Categorial Grammarswith generalized composition and type-raising.
In85Proceedings of the 23rd International Conference onComputational Linguistics (Coling 2010), pages 465?473, Beijing, China, August.
Coling 2010 OrganizingCommittee.Julia Hockenmaier and Mark Steedman.
2002.
Gener-ative Models for Statistical Parsing with CombinatoryCategorial Grammar.
In Proceedings of 40th AnnualMeeting of the Association for Computational Lin-guistics, pages 335?342, Philadelphia, Pennsylvania,USA, July.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and DependencyStructures Extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396, September.Yun Huang, Min Zhang, and Chew Lim Tan.
2012.Improved Combinatory Categorial Grammar Induc-tion with Boundary Words and Bayesian Inference.In Proceedings of the 24rd International Conferenceon Computational Linguistics (Coling 2012), Mumbai,India, December.Ray Jackendoff.
1977.
X-Bar Syntax: A Study of PhraseStructure.
The MIT Press.Dan Klein and Christopher D Manning.
2004.
Corpus-Based Induction of Syntactic Structure: Models of De-pendency and Constituency.
In Proceedings of the42nd Meeting of the Association for ComputationalLinguistics (ACL?04), Main Volume, pages 478?485,Barcelona, Spain, July.Kenichi Kurihara and Taisuke Sato.
2004.
An Appli-cation of the Variational Bayesian Approach to Prob-abilistic Context-Free Grammars.
International JointConference on Natural Language Language Process-ing Workshop Beyond Shallow Analyses, March.Kenichi Kurihara, Max Welling, and Yee-Whye Teh.2007.
Collapsed Variational Dirichlet Process Mix-ture Models.
In Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence (IJCAI07),pages 2796?2801, Hyderabad, India, January.Tom Kwiatkowski, Sharon Goldwater, Luke Zettlemoyer,and Mark Steedman.
2012.
A probabilistic model ofsyntactic and semantic acquisition from child-directedutterances and their meanings.
In Proceedings of the13th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 234?244, Avignon, France, April.
Association for Compu-tational Linguistics.Karim Lari and Steve J Young.
1991.
Applicationsof stochastic context-free grammars using the Inside-Outside algorithm.
Computer speech & language,5(3):237?257, January.Percy Liang, Slav Petrov, Michael I Jordan, and DanKlein.
2007.
The Infinite PCFG Using HierarchicalDirichlet Processes.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 688?697,Prague, Czech Republic.Percy Liang, Michael I Jordan, and Dan Klein.
2009.Probabilistic Grammars and Hierarchical DirichletProcesses.
In The Oxford Handbook of AppliedBayesian Analysis.
Oxford University Press.Mitchell P Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a Large AnnotatedCorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330, June.David Marecek and Zdenek Zabokrtsky.
2012.
Unsu-pervised Dependency Parsing using Reducibility andFertility features.
In NAACL HLT Workshop on Induc-tion of Linguistic Structure, pages 84?89, Montre?al,Canada, June.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using Universal Linguistic Knowl-edge to Guide Grammar Induction.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 1234?1244, Cambridge,MA, October.Tahira Naseem, Regina Barzilay, and Amir Globerson.2012.
Selective Sharing for Multilingual DependencyParsing.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 629?637, Jeju, Republicof Korea, July.Joakim Nivre.
2006.
Inductive Dependency Parsing.Springer.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A Universal Part-of-Speech Tagset.
In Proceedingsof the 8th International Conference on Language Re-sources and Evaluation (LREC-2012), pages 2089?2096, Istanbul, Turkey, May.Anders S?gaard.
2012.
Two baselines for unsuper-vised dependency parsing.
In NAACL HLT Work-shop on Induction of Linguistic Structure, pages 81?83, Montre?al, Canada, June.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Juraf-sky.
2010.
From Baby Steps to Leapfrog: How ?Lessis More?
in Unsupervised Dependency Parsing.
In Hu-man Language Technologies: The 2010 Annual Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, pages 751?759,Los Angeles, California, June.Mark Steedman.
1996.
Surface Structure and Interpre-tation.
The MIT Press, January.Mark Steedman.
2000.
The Syntactic Process.
The MITPress, September.Yee-Whye Teh, Michael I Jordan, Matthew J Beal, andDavid M Blei.
2006.
Hierarchical Dirichlet Pro-86cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Yee-Whye Teh.
2006.
A Hierarchical Bayesian Lan-guage Model based on Pitman-Yor Processes.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting ofthe Association for Computational Linguistics, pages985?992, Sydney, Australia, July.Yee-Whye Teh.
2010.
Dirichlet Process.
In Encyclope-dia of Machine Learning, pages 280?287.
Springer.Kewei Tu.
2012.
Combining the Sparsity and Unambi-guity Biases for Grammar Induction.
In NAACL HLTWorkshop on Induction of Linguistic Structure, pages105?110, Montre?al, Canada, June.8788
