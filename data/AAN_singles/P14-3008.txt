Proceedings of the ACL 2014 Student Research Workshop, pages 56?63,Baltimore, Maryland USA, June 22-27 2014. c?2014 Association for Computational LinguisticsGoing beyond sentences when applying tree kernelsDmitry IlvovskySchool of Applied Mathematics and Information ScienceNational Research University Higher School of EconomicsMoscow, Russiadilvovsky@hse.ruAbstractWe go beyond the level of individualsentences applying parse tree kernels toparagraphs.
We build a set of extendedtrees for a paragraph of text from the in-dividual parse trees for sentences andlearn short texts such as search resultsand social profile postings to take ad-vantage of additional discourse-relatedinformation.
Extension is based on coref-erences and rhetoric structure relationsbetween the phrases in different sentenc-es.
We evaluate our approach, trackingrelevance classification improvement formulti-sentence search task.
The searchproblem is formulated as classification ofsearch results into the classes of relevantand irrelevant, learning from the Bingsearch results.
We compare performancesof individual sentence kernels with theones for extended parse trees and showthat adding discourse information tolearning data helps to improve classifica-tion results.1 IntroductionIn spite of substantial efforts to formulate a com-plete linking theory between syntax and seman-tics, it is not available yet.
Hence the design ofsyntactic features for automated learning of syn-tactic structures is still an art.
One of the solu-tions to systematically treat these syntactic fea-tures ?
tree kernels built over syntactic parsetrees.
Convolution tree kernel (Collins andDuffy, 2002) defines a feature space consistingof all subtree types of parse trees and counts thenumber of common subtrees as the syntactic sim-ilarity between two parse trees.
They have founda number of applications in several natural lan-guage tasks, e.g.
syntactic parsing re-ranking,relation extraction (Zelenko et al, 2003; Zhanget al2006), named entity recognition (Cumbyand Roth, 2003) and Semantic Role Labeling(Moschitti, 2004), pronoun resolution (Yang etal., 2006), question classification (Zhang andLee, 2003) and machine translation (Zhang andLi, 2009).The kernel ability to generate large feature setsis useful to quickly model new and not well un-derstood linguistic phenomena in learning ma-chines.
However, it is often possible to manuallydesign features for linear kernels that producehigh accuracy and fast computation time whereasthe complexity of tree kernels may prevent theirapplication in real scenarios.Many learning algorithms, such as SVM(Vapnik, 1998) can work directly with kernels byreplacing the dot product with a particular kernelfunction.
This useful property of kernel methods,that implicitly calculates the dot product in ahigh-dimensional space over the original repre-sentations of objects such as sentences, has madekernel methods an effective solution to modelingstructured objects in NLP.
A number of NL tasksrequire computing of semantic features over par-agraphs of text containing multiple sentences.Doing it in a sentence pair-wise manner is notalways accurate, since it is strongly dependent onhow information (phrases) is distributed throughsentences.An approach to build a kernel based on morethan a single parse tree has been proposed(Severyn et.al., 2012), however without any rela-tions between parse trees or for a different pur-pose than treating multi-sentence portions oftext.
To compensate for parsing errors (Zhang etal., 2008), a convolution kernel over packedparse forest (Severyn and Moschitti, 2012; Aioliet.al, 2007) is used to mine syntactic featuresfrom it directly.
A packed forest compactly en-codes exponential number of n-best parse trees,and thus containing much more rich structuredfeatures than a single parse tree.
This advantageenables the forest kernel not only to be more ro-bust against parsing errors, but also to be able tolearn more reliable feature values and help to56solve the data sparseness issue that exists in thetraditional tree kernel.On the contrary, in this study we form a tree for-est of sequence of sentences in a paragraph oftext.
Currently, kernel methods tackle individualsentences.
However, in learning settings wheretexts include multiple sentences, structures whichinclude paragraph-level information need to beemployed.
We demonstrate that in certain do-mains and certain cases discourse structure isessential for proper classification of texts.2 Necessity to extend parse treesWe introduce a domain where a pair-wise com-parison of sentences is insufficient to properlylearn certain semantic features of texts.
This isdue to the variability of ways information can becommunicated in multiple sentences, and varia-tions in possible discourse structures of textwhich needs to be taken into account.We consider an example of text classificationproblem, where short portions of text belong totwo classes:?
Tax liability of a landlord renting officeto a business.?
Tax liability of a business owner rentingan office from landlord.I rent an office space.
This office is for my busi-ness.
I can deduct office rental expense from mybusiness profit to calculate net income.To run my business, I have to rent an office.
Thenet business profit is calculated as follows.
Rentalexpense needs to be subtracted from revenue.To store goods for my retail business I rent somespace.
When I calculate the net income, I take revenueand subtract business expenses such as office rent.I rent out a first floor unit of my house to a travelbusiness.
I need to add the rental income to my profit.However, when I repair my house, I can deduct therepair expense from my rental income.I receive rental income from my office.
I have toclaim it as a profit in my tax forms.
I need to add myrental income to my profits, but subtract rental ex-penses such as repair from it.I advertised my property as a business rental.
Ad-vertisement and repair expenses can be subtractedfrom the rental income.
Remaining rental incomeneeds to be added to my profit and be reported as tax-able profit.Firstly, note that keyword-based analysis doesnot help to separate the first three paragraphs andthe second three paragraphs.
They all share thesame keywords rent-al/office/income/profit/add/subtract.
Phrase-based analysis does not help, since both sets ofparagraphs share similar phrases.
Secondly, pair-wise sentence comparison does not solve theproblem either.
Anaphora resolution is helpfulbut insufficient.
All these sentences include ?I?and its mention, but other links between words orphrases in different sentences need to be used.Rhetoric structures need to come into play toprovide additional links between sentences.
Thestructure to distinguish betweenrenting for yourself and deducting from total in-comeandrenting to someone and adding to incomeembraces multiple sentences.
The second clauseabout adding/subtracting incomes is linked bymeans of the rhetoric relation of elaboration withthe first clause for landlord/tenant.
This rhetoricrelation may link discourse units within a sen-tence, between consecutive sentences and evenbetween first and third sentence in a paragraph.Other rhetoric relations can play similar role forforming essential links for text classification.Which representations for these paragraphs oftext would produce such common sub-structurebetween the structures of these paragraphs?
Webelieve that extended trees, which include thefirst, second, and third sentence for each para-graph together can serve as a structure to differ-entiate the two above classes.The dependency parse trees for the first text inour set and its coreferences are shown in Fig.
1.There are multiple ways the nodes from parsetrees of different sentences can be connected: wechoose the rhetoric relation of elaboration whichlinks the same entity office and helps us to formthe structure rent-office-space ?
for-my-business?
deduct-rental-expense which is the base for ourclassification.
We used Stanford Core NLP, co-references resolution (Lee et al, 2012) and itsvisualization to form Figs.
1 and 2.Fig.
2 shows the resultant extended tree withthe root ?I?
from the first sentence.
It includes thewhole first sentence, a verb phrase from the sec-ond sentence and a verb phrase from the thirdsentence according to rhetoric relation of elabo-ration.
Notice that this extended tree can be intui-tively viewed as representing the ?main idea?
ofthis text compared to other texts in our set.
Allextended trees need to be formed for a text and57then compared with that of the other texts, sincewe don?t know in advance which extended tree isessential.
From the standpoint of tree kernellearning, extended trees are learned the sameway as regular parse trees.Fig.1: Coreferences and the set of dependency treesfor the first text.Fig.
2: Extended tree which includes 3 sentences3 Building extended treesFor every arc which connects two parse trees, wederive the extension of these trees, extendingbranches according to the arc (Fig.
3).In this approach, for a given parse tree, wewill obtain a set of its extension, so the elementsof kernel will be computed for many extensions,instead of just a single tree.
The problem here isthat we need to find common sub-trees for amuch higher number of trees than the number ofsentences in text, however by subsumption (sub-tree relation) the number of common sub-treeswill be substantially reduced.If we have two parse trees P1 and P2 for twosentences in a paragraph, and a relationR12: P1i ?P2j between the nodes P1i and P2j, weform the pair of extended trees P1*P2:?, P1i-2, P1i-1, P1i, P2j, P2j+1, P2j+2,?
?, P2j-2, P2j-1, P2j, P1i, P1i+1, P2i+2,?,which would form the feature set for tree kernellearning in addition to the original trees P1 andP2.
Notice that the original order of nodes ofparse trees is retained under operation ?*?
(Fig.3).Fig.
3: An arc which connects two parse trees for twosentences in a text (on the top) and the derived set ofextended trees (on the bottom).The algorithm for building an extended tree for aset of parse trees T is presented below:Input:1) Set of parse trees T.2) Set of relations R, which includes relations Rijk be-tween the nodes of Ti and Tj: Ti ?T, Tj ?T, Rijk ?R.We use index k to range over multiple relations be-tween the nodes of parse tree for a pair of sentences.Output: the exhaustive set of extended trees E.Set E = ?
;For each tree i=1:|T|For each relation Rijk,  k= 1: |R|Obtain TjForm the pair of extended trees Ti * Tj;Verify that each of the extended trees do not havea super-tree in EIf verified, add to E;Return E.Notice that the resultant trees are not the prop-er parse trees for a sentence, but neverthelessform an adequate feature space for tree kernellearning.P11P1i P2jP21P2j+158To obtain the inter-sentence links, we em-ployed the following sources:?
Coreferences from Stanford NLP (Re-casens et al, 2013, Lee et al, 2013).?
Rhetoric relation extractor based on therule-based approach to finding relationsbetween elementary discourse units(Galitsky et al, 2013).
We combinedmanual rules with automatically learnedderived from the available discourse cor-pus by means of syntactic generalization.4 Assessment of classification improve-mentTo confirm that using a set of extended parsetrees for paragraphs leverages additional seman-tic information compared to a set of parse treesfor all sentences in a paragraph, we perform anevaluation of relevance in search domain:?
As a baseline, we take all trees for sen-tences in paragraphs?
As an expected improvement, we take allextended trees in a paragraph.Since a benchmarking database for answeringcomplex multi-sentence questions is not availa-ble, we form our own dataset for product-relatedopinions.
The question answering problem isformulated as finding information on the web,relevant to a user posting / opinion expression ina blog, forum or social network.For the purpose of this evaluation it is not es-sential to provide the best possible set of an-swers.
Instead, we are concerned with the com-parison of relevance improvement by using ex-tended parse tree, as long as the evaluation set-tings of question answering are identical.
Thedetails of the evaluation are given in Section 7.5 Implementation of kernel learning forextended treesThe evaluation framework described here is im-plemented as an OpenNLP contribution.
It relieson the following systems:?
OpenNLP/Stanford NLP parser;?
Stanford NLP Coreference;?
Bing search;?
Wrapper of TK-Light kernel learner(Moschitti, 2006).Framework includes the following compo-nents of Apache OpenNLP.similarity project:?
Rhetoric parser?
Parse thicket builder and generalizer(Galitsky et al, 2012).
Not used in thisevaluation.?
A number of applications based on theabove component, including search (re-quest handler for SOLR), speech recog-nition, content generation and others.One of the use cases of thisOpenNLP.similarity component is a Java wrap-per for tree kernel algorithms implemented inC++.
It allows seamless integration of tree kernelalgorithms into other open source systems avail-able in Java for search, information retrieval andmachine learning.
Moreover, tree kernel algo-rithms can be embedded into Hadoop frameworkin the domains where offline performance is es-sential.
Libraries and evaluation results describedin this paper are also available athttp://code.google.com/p/relevance-based-on-parse-trees andhttp://svn.apache.org/repos/asf/opennlp/sandbox/opennlp-similarity/.6 Complexity estimationTo estimate the complexity of building extendedtrees, let us consider an average case with 5 sen-tences in each paragraph and 15 words in eachsentence.
We have on average 10 inter-sentencearcs, which give us up to 20 extended treesformed from two sentences, and 60 extendedtrees formed from 3 sentences.
Hence we have toapply tree learning to up to 100 trees (of a biggersize) instead of just 5 original trees.
We observethat kernel learning of extended trees has to han-dle at least 20 times bigger input set.However, most of the smaller subtrees are re-petitive and will be reduced in the course of di-mensionality reduction.7 EvaluationTo estimate whether additional high-level se-mantic and discourse information contributes toclassical kernel based approach, we compare twosources for trees:?
Regular parse trees?
Extended parse trees59To perform this estimation, we need a corpusincluding a high number of short texts similar toour example in Introduction.
These texts shouldhave high similarity (otherwise keyword ap-proach would do well), certain discourse struc-ture, and describe some objects (products) in ameaningful application domain.
Unfortunately,to the best of our knowledge such corpus is notavailable.
Therefore, for comparison of tree ker-nel performances we decided to use search re-sults, given the query which is a short text.
Werely on search engine APIs following the evalua-tion settings in the studies on answering complexquestions (Galitsky et al, 2013).Search results typically include texts of fairlyhigh similarity, which is leveraged in our evalua-tion.
To formulate classification problem on theset of texts obtained as search results, we need toform positive and negative sets.
To do that, weselect the first n search results as relevant (posi-tive) and also n results towards to tail of searchresults lists as irrelevant (negative).
In this caseeach search session yields an individual training(and evaluation) dataset.
The same nature of suchdata allows averaging of precision and recall,having individual training dataset of a limitedsize.
Hence reliability of our results is achievednot via the size of individual dataset, but insteadby the increased number of search sessions.
Toassure an abrupt change in relevance proceedingfrom the head to the tail of search results lists,we use complicated queries including multiplesentences, which are not handled by modernsearch engines well.The preparation of search queries (which in-clude multiple sentences) is based on the follow-ing steps:1.
Forming the names of products and theirshort descriptions2.
Given (1), find a text including an ex-tended review or opinion about thisproduct.3.
Texts (2) cannot be used as queries asthey are.
To form the queries from (2),we need to extract most significantphrases from them; otherwise, searchengines are confused which keywords tochoose and give either duplicate, or irrel-evant results.
These were the longestnoun and selected verb phrases from (2).The analogous steps were conducted for Ya-hoo Answers data.
We manually select a 100most interesting search queries for each domain.The training/evaluation datasets is formedfrom search results in the following way.
Weobtain a first hundred search results (or less ifhundred is not available).
We select 1..20 (orfirst 20%) of search results as a positive set, and81..100 as a negative set.
Search results 21..80form the basis of evaluation dataset, from whichwe randomly select 10 texts to be classified intothe classes of positive or negative.
Hence wehave the ratio 4:1 between the training and eval-uation datasets.To motivate our evaluation setting, we rely onthe following observations.
In case of searchingfor complex multi-sentence queries, relevanceindeed drops abruptly with proceeding from thefirst 10-20 search results, as search evaluationresults demonstrated (Galitsky et al, 2013).
Theorder of search results in first 20% and last 20%does not affect our evaluation.
Although the last20% of search results is not really a ?gold stand-ard?, it is nevertheless a set that can be reasona-bly separated from the positive set.
If such sepa-ration is too easy or too difficult, it would behard to adequately evaluate the difference be-tween regular parse trees and extended trees fortext classification.
Search-based approach to col-lect texts for evaluation of classification allowsreaching maximum degree of experiment auto-mation.It turned out that the use of tail search resultsas negative set helps to leverage the high levelsemantic and discourse information.
Negativeexamples, as well as positive ones, include mostkeywords from the queries.
However, the maindifference between the positive and negativesearch results is that the former include muchmore coreferences and rhetoric structures similarto the query, than the latter set.
The use of theextended trees was beneficial in the cases wherephrases from queries are distributed through mul-tiple sentences in search results.We conducted two independent experimentsfor each search session, classifying search resultsnippets and also original texts, extracted fromwebpages.
For the snippets, we split them intosentence fragments and built extended trees forthese fragments of sentences.
For original texts,we extracted all sentences related to the snippetfragments and built extended trees for these sen-tences.Training and classification occurs in the auto-mated mode, and the classification assessment is60conducted by the members of research groupguided by the authors.
The assessors only con-sulted the query and answer snippets.We used the standard parameters of tree se-quence kernels fromhttp://disi.unitn.it/moschitti/Tree-Kernel.htm(Moschitti, 2006).
Tree kernel is applied to alltree pairs from two forests.
The latest version oftree kernel learner was obtained from the author.Products  BasickernelsExtended ker-nels (co-refs+RST)Textsfrom thepagesPrecision 0,5679 0,5868Recall 0,7516 0,8458F-measure 0,6485 0,6752SnippetsPrecision 0,5625 0,6319Recall 0,7840 0,8313F-measure 0,6169 0,6695Table 1: Evaluation results for products domainAnswers  BasickernelsExtendedkernels(corefs)Extendedkernels(corefs+RST)Textsfrom thepagesP 0,5167 0,5083 0,5437R 0,7361 0,7917 0,8333F 0,6008 0,5458 0,6278SnippetsP 0,5950 0,6264 0,6794R 0,7329 0,7492 0,7900F 0,6249 0,6429 0,7067Table 2: Evaluation results for popular answers do-mainEvaluation results show visible improvement ofclassification accuracy achieved by extendedtrees.
For Yahoo Answers one can observe thatcoreferences only provide a slight improvementof accuracy, whereas RST added to coreferencesgives a stronger improvement.
Stronger increaseof recall in comparison to precision can be ex-plained by the following.
It is due to the acquiredcapability of extended trees to match phrasesfrom the search results distributed through multi-ple sentences, with questions.8 Conclusions and future workIn this study we focused on how discourse in-formation can help with text relevance tasks irre-spectively of learning mechanisms.
We com-pared two sets of linguistic features:?
The baseline, parse trees for individualsentences,?
Parse trees and discourse information,and demonstrated that the enriched set of fea-tures indeed improves the classification accura-cy, having the learning framework fixed.
Thisimprovement varies from 2 to 8 % in differentdomains with different structure of texts.
Totackle such enriched set of linguistic features, anadjustment of tree kernel algorithm itself was notnecessary.The approach developed in this paper can alsobe applied to parse tree querying and manipula-tion problem (Levy and Galen, 2006).
A systemsuch as Tregex is an expressive and flexible wayfor single sentence parse tree querying and ma-nipulation.
Extending parse trees of individualsentences towards paragraph of text, the recall ofa tree querying system would dramatically in-crease, and dependence on how phrases are dis-tributed through sentences would decrease.There are a few possible directions of futuredevelopment.
One interesting continuation of thisstudy is to applying standard ranking mecha-nisms such as NDCG.
We can draw the compari-son between the standard and extended kernels interms of standard Bing ranking, as well as spe-cial ranking based on syntactic similarity be-tween the query and search results (Galitsky etal., 2013).We also plan to generalize extended tree ker-nels towards graphs (DAGs) (Suzuki et al,2003).
In this case we can perform learning onParse thickets (Galitsky et al, 2013) ?
the struc-tures which are the sets of parse trees for a para-graph.
It will be fruitful to compare performanc-es of various ways of kernel computation andestimate the contribution of a particular way ofparagraph representation to the quality of classi-fication.It is possible to apply the outlined approach toperform question answering in the case wherethe latter are extensive portions of paragraph-sized text and the former include multiple sen-tences.Another obvious direction is applying treekernels to classify short texts based on standardcorpus data.
However, a corpus of short texts,where advantages of kernel methods over alter-natives would become visible, does not exist.One of our next tasks is to form such a corpus.AcknowledgmentsWe would like to thank Baidu for travel and con-ference support for this paper.61ReferencesCumby, C., Roth, D. 2003.
Kernel methods for rela-tional learning.
In: ICML.Kim, Jung-Jae, Pezik, P. and Rebholz-Schuhmann, D.2008.
MedEvi: Retrieving textual evidence of rela-tions between biomedical concepts from Medline.Bioinformatics.
Volume 24, Issue 11 pp.
1410-1412.Zelenko, D., Aone, C., Richardella, A.
2003.
Kernelmethods for relation extraction.
JMLR (2003).Suzuki, J., Hirao, H., Sasaki, Y and Maeda, E., Hier-archical Directed Acyclic Graph Kernel: Methodsfor Structured Natural Language Data.
In Proceed-ings of the 41th Annual Meeting of Association forComputational Linguistics (ACL).
2003.Galitsky, B.
Natural Language Question AnsweringSystem: Technique of Semantic Headers.
AdvancedKnowledge International, Australia (2003).Galitsky, B., de la Rosa, J., Dobrocsi, G. 2012.
Infer-ring the semantic properties of sentences by miningsyntactic parse trees.
Data & Knowledge Engineer-ing.
Volume 81-82, November (2012) 21-45.Galitsky, B., Usikov, D. Kuznetsov, S. 2013.
ParseThicket Representations for Answering Multi-sentence questions.
20th International Conferenceon Conceptual Structures, ICCS 2013.Galitsky, B., Kuznetsov, S. 2008.
Learning communi-cative actions of conflicting human agents.
J. Exp.Theor.
Artif.
Intell.
20(4): 277-317.Galitsky, B.
2012.
Machine Learning of SyntacticParse Trees for Search and Classification of Text.Engineering Application of AI,http://dx.doi.org/10.1016/j.engappai.2012.09.017.Galitsky, B., Ilvovsky, D., Kuznetsov, S., Strok, F.2013.
Improving Text Retrieval Efficiency withPattern Structures on Parse Thickets, Workshop"Formal Concept Analysis meets Information Re-trieval" at ECIR 2013, Moscow, Russia.Ehrlich H.-C., Rarey M. 2011.
Maximum commonsubgraph isomorphism algorithms and their appli-cations in molecular science: review.
Wiley Inter-disciplinary Reviews: Computational MolecularScience, 2011, vol.
1 (1), pp.
68-79.Yan, X., Han, J.
2002. gSpan: Graph-Based Substruc-ture Pattern Mining.
In: Proc.
IEEE Int.
Conf.
onData Mining, ICDM?02, IEEE Computer Society(2002), pp 721?724.Jiangning Wu, Zhaoguo Xuan and Donghua Pan, En-hancing text representation for classification taskswith semantic graph structures, International Jour-nal of Innovative Computing, Information andControl (ICIC), Volume 7, Number 5(B).Haussler, D. 1999.
Convolution kernels on discretestructures.Moschitti, A.
2006.
Efficient Convolution Kernels forDependency and Constituent Syntactic Trees.
InProceedings of the 17th European Conference onMachine Learning, Berlin, Germany.Severyn, A., Moschitti, A.
2012.
Structural relation-ships for large-scale learning of answer re-ranking.
SIGIR 2012: 741-750.Severyn, A., Moschitti, A.
2012.
Fast Support VectorMachines for Convolution Tree Kernels.
Data Min-ing Knowledge Discovery 25: 325-357.Aiolli, F., Da San Martino, G., Sperduti, A. and Mos-chitti, A.
2007.
Efficient Kernel-based Learning forTrees, Proceeding of the IEEE Symposium onComputational Intelligence and Data Mining(CIDM), Honolulu, Hawaii.Punyakanok, V., Roth, D., & Yih, W. 2004.
Mappingdependencies trees: an application to question an-swering.
In: Proceedings of AI & Math, Florida,USA.Mann, William C., Christian M. I. M. Matthiessenand Sandra A. Thompson.
1992.
Rhetorical Struc-ture Theory and Text Analysis.
Discourse Descrip-tion: Diverse linguistic analyses of a fund-raisingtext.
ed.
by W. C. Mann and S. A. Thompson.
Am-sterdam, John Benjamins: 39-78.Sun, J., Min Zhang, Chew Lim Tan.
2011.
Tree Se-quence Kernel for Natural Language.
AAAI-25,2011.Zhang, M.; Che, W.; Zhou, G.; Aw, A.; Tan, C.; Liu,T.
; and Li, S. 2008.
Semantic role labeling using agrammar-driven convolution tree kernel.
IEEEtransactions on audio, speech, and language pro-cessing 16(7):1315?1329.Montaner, M.; Lopez, B.; de la Rosa, J. L. (June2003).
A Taxonomy of Recommender Agents on theInternet.
Artificial Intelligence Review 19 (4):285?330.Collins, M., and Duffy, N. 2002.
Convolution kernelsfor natural language.
In Proceedings of NIPS,625?632, 2002.Heeyoung Lee, Angel Chang, Yves Peirsman, Na-thanael Chambers, Mihai Surdeanu and Dan Juraf-sky.
2013.
Deterministic coreference resolutionbased on entity-centric, precision-ranked rules.Computational Linguistics 39(4).Daniel Jurafsky, James H. Martin.
2008.
Speech andLanguage Processing.
An Introduction to NaturalLanguage Processing, Computational Linguistics,and Speech Recognition.Robinson J.A.
1965.
A machine-oriented logic basedon the resolution principle.
Journal of the Associa-tion for Computing Machinery, 12:23-41.62Mill, J.S.
1843.
A system of logic, ratiocinative andinductive.
London.Fukunaga, K. Introduction to statistical patternrecognition (2nd ed.
), Academic Press Profession-al, Inc., San Diego, CA, 1990.Mitchell, T. 1997.
Machine Learning.
McGraw Hill.Furukawa, K. 1998.
From Deduction to Induction:Logical Perspective.
The Logic Programming Par-adigm.
In Apt, K.R., Marek V.W., Truszczynski,M., Warren, D.S., Eds.
Springer.Bharat Bhasker; K. Srikumar.
2010.
RecommenderSystems in E-Commerce.
CUP.
ISBN 978-0-07-068067-8.Trias i Mansilla, A., JL de la Rosa i Esteva.
2012.Asknext: An Agent Protocol for Social Search.
In-formation Sciences 190, 144?161.Punyakanok, V.,Roth, D. and Yih, W. 2005.
The Ne-cessity of Syntactic Parsing for Semantic Role La-beling.
IJCAI-05.Domingos P. and Poon, H. 2009.
Unsupervised Se-mantic Parsing, In: Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing, Singapore: ACL.Marcu, D. 1997.
From Discourse Structures to TextSummaries, in I. Mani and M.Maybury (eds) Pro-ceedings of ACL Workshop on Intelligent ScalableText Summarization, pp.
82?8, Madrid, Spain.Abney, S. 1991.
Parsing by Chunks, Principle-BasedParsing, Kluwer Academic Publishers, 1991, pp.257-278.Hyeran Byun, Seong-Whan Lee.
2002.
Applicationsof Support Vector Machines for Pattern Recogni-tion: A Survey.
In Proceedings of the First Interna-tional Workshop on Pattern Recognition with Sup-port Vector Machines (SVM '02), Seong-WhanLee and Alessandro Verri (Eds.).
Springer-Verlag,London, UK, UK, 213-236.Chris Manning and Hinrich Sch?tze, Foundations ofStatistical Natural Language Processing, MITPress.
Cambridge, MA: May 1999.Sun, J.; Zhang, M.; and Tan, C. 2010.
Exploring syn-tactic structural features for sub-tree alignment us-ing bilingual tree kernels.
In Proceedings of ACL,306?315.Kohavi, Ron.
1995.
A Study of Cross-Validation andBootstrap for Accuracy Estimation and Model Se-lection.
International Joint Conference on ArtificialIntelligence IJCAI 1995.Kalervo Jarvelin, Jaana Kekalainen.
2002.
Cumulatedgain-based evaluation of IR techniques.
ACMTransactions on Information Systems 20(4), 422?446.Roger Levy and Galen Andrew, Tregex and Tsur-geon: tools for querying and manipulating tree datastructures.
5th International Conference on Lan-guage Resources and Evaluation (LREC 2006),2006.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint Entity andEvent Coreference Resolution across Documents.In EMNLP-CoNLL 2012.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013.
The Life and Death ofDiscourse Entities: Identifying Singleton Mentions.In Proceedings of NAACL 2013.63
