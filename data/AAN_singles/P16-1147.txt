Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1557?1566,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsStack-propagation: Improved Representation Learning for SyntaxYuan Zhang?CSAIL, MITCambridge, MA 02139, USAyuanzh@csail.mit.eduDavid WeissGoogle IncNew York, NY 10027, USAdjweiss@google.comAbstractTraditional syntax models typically lever-age part-of-speech (POS) information byconstructing features from hand-tunedtemplates.
We demonstrate that a betterapproach is to utilize POS tags as a reg-ularizer of learned representations.
Wepropose a simple method for learning astacked pipeline of models which we call?stack-propagation?.
We apply this to de-pendency parsing and tagging, where weuse the hidden layer of the tagger networkas a representation of the input tokens forthe parser.
At test time, our parser doesnot require predicted POS tags.
On 19 lan-guages from the Universal Dependencies,our method is 1.3% (absolute) more accu-rate than a state-of-the-art graph-based ap-proach and 2.7% more accurate than themost comparable greedy model.1 IntroductionIn recent years, transition-based dependencyparsers powered by neural network scoring func-tions have dramatically increased the state-of-the-art in terms of both speed and accuracy (Chen andManning, 2014; Alberti et al, 2015; Weiss et al,2015).
Similar approaches also achieve state-of-the-art in other NLP tasks, such as constituencyparsing (Durrett and Klein, 2015) or semanticrole labeling (FitzGerald et al, 2015).
Theseapproaches all share a common principle: re-place hand-tuned conjunctions of traditional NLPfeature templates with continuous approximationslearned by the hidden layer of a feed-forward net-work.
?Research conducted at Google.However, state-of-the-art dependency parsersdepend crucially on the use of predicted part-of-speech (POS) tags.
In the pipeline or stacking(Wolpert, 1992) method, these are predicted froman independently trained tagger and used as fea-tures in the parser.
However, there are two maindisadvantages of a pipeline: (1) errors from thePOS tagger cascade into parsing errors, and (2)POS taggers often make mistakes precisely be-cause they cannot take into account the syntacticcontext of a parse tree.
The POS tags may alsocontain only coarse information, such as when us-ing the universal tagset of Petrov et al (2011).One approach to solve these issues has been toavoid using POS tags during parsing, e.g.
eitherusing semi-supervised clustering instead of POStags (Koo et al, 2008) or building recurrent repre-sentations of words using neural networks (Dyer etal., 2015; Ballesteros et al, 2015).
However, thebest accuracy for these approaches is still achievedby running a POS tagger over the data first andcombining the predicted POS tags with additionalrepresentations.
As an alternative, a wide range ofprior work has investigated jointly modeling bothPOS and parse trees (Li et al, 2011; Hatori etal., 2011; Bohnet and Nivre, 2012; Qian and Liu,2012; Wang and Xue, 2014; Li et al, 2014; Zhanget al, 2015; Alberti et al, 2015).
However, theseapproaches typically require sacrificing either ef-ficiency or accuracy compared to the best pipelinemodel, and often they simply re-rank the predic-tions of a pipelined POS tagger.In this work, we show how to improve accuracyfor both POS tagging and parsing by incorporat-ing stacking into the architecture of a feed-forwardnetwork.
We propose a continuous form of stack-ing that allows for easy backpropagation down thepipeline across multiple tasks, a process we call1557Backpropagation Task ATask BTraditional Stacking Stack-propagationTask ATask BFigure 1: Traditional stacking (left) vs. Stack-propagation(right).
Stacking uses the output of Task A as features inTask B, and does not allow backpropagation between tasks.Stack-propagation uses a continuous and differentiable linkbetween Task A and Task B, allowing for backpropagationfrom Task B into Task A?s model.
Updates to Task A act asregularization on the model for Task B, ensuring the sharedcomponent is useful for both tasks.?stack-propagation?
(Figure 1).
At the core of thisidea is that we use POS tags as regularization in-stead of features.Our model design for parsing is very simple:we use the hidden layer of a window-based POStagging network as the representation of tokens ina greedy, transition-based neural network parser.Both networks are implemented with a refined ver-sion of the feed-forward network (Figure 3) fromChen and Manning (2014), as described in Weisset al (2015).
We link the tagger network to theparser by translating traditional feature templatesfor parsing into feed-forward connections from thetagger to the parser (Figure 2).
At training time,we unroll the parser decisions and apply stack-propagation by alternating between stochastic up-dates to the parsing or tagging objectives (Figure4).
The parser?s representations of tokens are thusregularized to be individually predictive of POStags, even as they are trained to be useful for pars-ing when concatenated and fed into the parser net-work.
This model is similar to the multi-task net-work structure of Collobert et al (2011), whereCollobert et al (2011) shares a hidden layer be-tween multiple tagging tasks.
The primary differ-ence here is that we show how to unroll parsertransitions to apply the same principle to taskswith fundamentally different structure.The key advantage of our approach is that attest time, we do not require predicted POS tagsfor parsing.
Instead, we run the tagger network upto the hidden layer over the entire sentence, andthen dynamically connect the parser network tothe tagger network based upon the discrete parserconfigurations as parsing unfolds.
In this way, weavoid cascading POS tagging errors to the parser.As we show in Section 5, our approach can beused in conjunction with joint transition systemsin the parser to improve both POS tagging as wellas parsing.
In addition, because the parser re-usesthe representation from the tagger, we can drop alllexicalized features from the parser network, lead-ing to a compact, faster model.The rest of the paper is organized as follows.
InSection 2, we describe the layout of our combinedarchitecture.
In Section 3, we introduce stack-propagation and show how we train our model.We evaluate our approach on 19 languages fromthe Universal Dependencies treebank in Section 4.We observe a >2% absolute gain in labeled ac-curacy compared to state-of-the-art, LSTM-basedgreedy parsers (Ballesteros et al, 2015) and a>1% gain compared to a state-of-the-art, graph-based method (Lei et al, 2014).
We also evaluateour method on the Wall Street Journal, where wefind that our architecture outperforms other greedymodels, especially when only coarse POS tagsfrom the universal tagset are provided during train-ing.
In Section 5, we systematically evaluate thedifferent components of our approach to demon-strate the effectiveness of stack-propagation com-pared to traditional types of joint modeling.
Wealso show that our approach leads to large reduc-tions in cascaded errors from the POS tagger.We hope that this work will motivate fur-ther research in combining traditional pipelinedstructured prediction models with deep neuralarchitectures that learn intermediate representa-tions in a task-driven manner.
One importantfinding of this work is that, even without POStags, our architecture outperforms recurrent ap-proaches that build custom word representationsusing character-based LSTMs (Ballesteros et al,2015).
These results suggest that learning richembeddings of words may not be as important asbuilding an intermediate representation that takesmultiple features of the surrounding context intoaccount.
Our results also suggest that deep mod-els for dependency parsing may not discover POSclasses when trained solely for parsing, even whenit is fully within the capacity of the model.
De-signing architectures to apply stack-propagation inother coupled NLP tasks might yield significantaccuracy improvements for deep learning.1558I ate a snack today?
?I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{Window-basedclassifierPRON VERB DET NOUN NOUNI ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{{Transition-based parserIate asnacktodayI ate ateaatesnackateTransition-basedparser{I ate a snack today?
?I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{Window-basedclassifierPRON VERB DET NOUN NOUNI ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN A P NOUNParsing loss{{I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{{Transition-based parserIate asnacktodayI ate ateaatesnackateTransition-basedparser{I ate a snack today?
?I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{Window-basedclassifierPRON VERB DET NOUN NOUNI ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{I ate a sandwich with cheeseI ate asandwich with cheesePRON VERB DET NOUN ADP NOUNParsing loss{{{Transition-based parserIate asnacktodayI ate ateaatesnackateTransition-basedparser{I ate a snack today?
?Tagger network(window-based)PRON VERB DET NOUN NOUNParser network(state-based)I ate asnackBufferI ate ate aate snackateParser?stateSHIFT LEFT(nsubj) SHIFT SHIFT LEFT(det)ate a a snack today todayRIGHT(dobj)atetodaySHIFT RIGHT(tmod)todayate ate?
?StackDiscrete state updatesConnections depend on state (e.g.
stack, parse)Discrete ?linkNN linkParser networkTagger networkstack:1stack:0 input:0Figure 2: Detailed example of the stacked parsing model.
Top: The discrete parser state, consisting of the stack and the buffer,is updated by the output of the parser network.
In turn, the feature templates used by the parser are a function of the state.In this example, the parser has three templates, stack:0, stack:1, and input:0.
Bottom: The feature templates createmany-to-many connections from the hidden layer of the tagger to the input layer of the parser.
For example, the predictedroot of the sentence (?ate?)
is connected to the input of most parse decisions.
At test time, the above structure is constructeddynamically as a function of the parser output.
Note also that the predicted POS tags are not directly used by the parser.2 Continuous Stacking ModelIn this section, we introduce a novel neural net-work model for parsing and tagging that incorpo-rates POS tags as a regularization of learned im-plicit representations.
The basic unit of our model(Figure 3) is a simple, feed-forward network thathas been shown to work very well for parsing tasks(Chen and Manning, 2014; Weiss et al, 2015).The inputs to this unit are feature matrices whichare embedded and passed as input to a hiddenlayer.
The final layer is a softmax prediction.We use two such networks in this work:a window-based version for tagging and atransition-based version for dependency parsing.In a traditional stacking (pipeline) approach, wewould use the discrete predicted POS tags fromthe tagger as features in the parser (Chen andManning, 2014).
In our model, we instead feedthe continuous hidden layer activations of the tag-ger network as input to the parser.
The primarystrength of our approach is that the parser has ac-cess to all of the features and information used bythe POS tagger during training time, but it is al-lowed to make its own decisions at test time.To implement this, we show how we can re-use feature templates from Chen and Manning(2014) to specify the feed-forward connectionsfrom the tagger network to the parser network.An interesting consequence is that because thisstructure is a function of the derivation producedby the parser, the final feed-forward structure ofthe stacked model is not known until run-time.PrefixesWordsFeature ?extractionSuffixesClustersEmbeddingHidden (Relu)SoftmaxWords SuffixesX>0 E0 X>1 E1 X>GEG?P (y) / exp{ >y h0 + by}h0 = max{0,W>hX>g Egi + b0}Embedding LayerHidden LayerSoftmax LayerFeature templatesD=64D=16Figure 3: Elementary NN unit used in our model.
Featurematrices from multiple channels are embedded, concatenatedtogether, and fed into a rectified linear hidden layer.
In theparser network, the feature inputs are continuous representa-tions from the tagger network?s hidden layer.However, because the connections for any specificparsing decision are fixed given the derivation, wecan still extract examples for training off-line byunrolling the network structure from gold deriva-tions.
In other words, we can utilize our approachwith the same simple stochastic optimization tech-niques used in prior works.
Figure 2 shows a fullyunrolled architecture on a simple example.2.1 The Tagger NetworkAs described above, our POS tagger follows thebasic structure from prior work with embedding,hidden, and softmax layers.
Like the ?window-approach?
network of Collobert et al (2011), thetagger is evaluated per-token, with features ex-tracted from a window of tokens surrounding thetarget.
The input consists of a rich set of fea-1559The        fox jumps     .stack bufferdet(a) Parser configuration c.Figure 4: A schematic for the PARSERstack-propagation update.
a: Exampleparser configuration c with correspond-ing stack and buffer.
b: Forward andbackward stages for the given single ex-ample.
During the forward phase, thetagger networks compute hidden acti-vations for each feature template (e.g.stack0and buffer0), and activations arefed as features into the parser network.For the backward update, we backpropa-gate training signals from the parser net-work into each linked tagging example.TaggerForwardFeature ExtractorEmbedding LayerHidden Layerw0=fox, w-1=the, ?
w0=jumps, w-1=fox, ?Parser ?
? implicitbu?er0 implicitstack0Backwardstack0.lab=det,?Embedding Layer ?
?Hidden LayerSoftmax LayerXlabel ximplicitstack0(b) Forward and backward schema for (a).ximplicitbu?er0ximplicitstack0ximplicitbu?er0 implicitstack0  implicitbu?er0tures for POS tagging that are deterministically ex-tracted from the training data.
As in prior work,the features are divided into groups of differentsizes that share an embedding matrix E. Featuresfor each group g are represented as a sparse ma-trix Xgwith dimension Fg?
Vg, where Fgisthe number of feature templates in the group, andVgis the vocabulary size of the feature templates.Each row of Xgis a one-hot vector indicating theappearance of each feature.The network first looks up the learned embed-ding vectors for each feature and then concate-nates them to form the embedding layer.
This em-bedding layer can be written as:h0= [XgEg| ?g] (1)where Egis a learned Vg?
Dgembedding ma-trix for feature group.
Thus, the final size |h0| =?gFgDgis the sum of all embedded featuresizes.
The specific features and their dimensionsused in the tagger are listed in Table 1.
Note thatfor all features, we create additional null valuethat triggers when features are extracted outsidethe scope of the sentence.
We use a single hiddenlayer in our model and apply rectified linear unit(ReLU) activation function over the hidden layeroutputs.
A final softmax layer reads in the acti-vations and outputs probabilities for each possiblePOS tag.2.2 The Parser NetworkThe parser component follows the same designas the POS tagger with the exception of the fea-tures and the output space.
Instead of a window-based classifier, features are extracted from an arc-Features (g) Window DSymbols 1 8Capitalization +/- 1 4Prefixes/Suffixes (n = 2, 3) +/- 1 16Words +/-3 64Table 1: Window-based tagger feature spaces.
?Symbols?indicates whether the word contains a hyphen, a digit or apunctuation.standard parser configuration1c consisting of thestack s, the buffer b and the so far constructed de-pendencies (Nivre, 2004).
Prior implementationsof this model used up to four groups of discretefeatures: words, labels (from previous decisions),POS tags, and morphological attributes (Chen andManning, 2014; Weiss et al, 2015; Alberti et al,2015).In this work, we apply the same design princi-ple but we use an implicitly learned intermediaterepresentation in the parser to replace traditionaldiscrete features.
We only retain discrete featuresover the labels in the incrementally constructedtree (Figure 4).
Specifically, for any token of inter-est, we feed the hidden layer of the tagger networkevaluated for that token as input to the parser.
Weimplement this idea by re-using the feature tem-plates from prior work as indexing functions.We define this process formally as follows.
Letfi(c) be a function mapping from parser config-urations c to indices in the sentence, where i de-notes each of our feature templates.
For example,in Figure 4(a), when i =stack0, fi(c) is the in-1Note that the ?stack?
in the parse configuration is sepa-rate from the ?stacking?
of the POS tagging network and theparser network (Figure 1).1560dex of ?fox?
in the sentence.
Let htagger1(j) bethe hidden layer activation of the tagger networkevaluated at token j.
We define the input Ximplicitby concatenating these tagger activations accord-ing to our feature templates:ximpliciti, htagger1(fi(c)).
(2)Thus, the feature group Ximplicitis the row-concatenation of the hidden layer activations ofthe tagger, as indexed by the feature templates.We have that Fimplicitis the number of featuretemplates, and Vimplicit= Htagger, the num-ber of possible values is the number of hiddenunits in the tagger.
Just as for other features,we learn an embedding matrix Eimplicitof sizeHimplicit?Fimplicit.
Note that as in the POS tag-ger network, we reserve an additional null valuefor out of scope feature templates.
A full exam-ple of this lookup process, and the resulting feed-forward network connections created, is shown fora simple three-feature template consisting of thetop two tokens on the stack and the first on thebuffer in Figure 2.
See Table 1 for the full list of20 tokens that we extract for each state.3 Learning with Stack-propagationIn this section we describe how we train our stack-ing architecture.
At a high level, we simply applybackpropagation to our proposed continuous formof stacking (hence ?stack-propagation.?)
There aretwo major issues to address: (1) how to handlethe dynamic many-to-many connections betweenthe tagger network and the parser network, and (2)how to incorporate the POS tag labels during train-ing.Addressing the first point turns out to be fairlyeasy in practice: we simply unroll the gold treesinto a derivation of (state, action) pairs that pro-duce the tree.
The key property of our pars-ing model is that the connections of the feed-forward network are constructed incrementally asthe parser state is updated.
This is different than ageneric recurrent model such as an LSTM, whichpasses activation vectors from one step to the next.The important implication at training time is that,unlike a recurrent network, the parser decisionsare conditionally independent given a fixed his-tory.
In other words, if we unroll the networkstructure ahead of time given the gold derivation,we do not need to perform inference when trainingwith respect to these examples.
Thus, the overalltraining procedure is similar to that introduced inChen and Manning (2014).To incorporate the POS tags as a regularizationduring learning, we take a fairly standard approachfrom multi-task learning.
The objective of learn-ing is to find parameters ?
that maximize the datalog-likelihood with a regularization on ?
for bothparsing and tagging:max???x,y?Tlog(P?
(y | x))+?c,a?Plog (P?
(a | c)) , (3)where {x, y} are POS tagging examples extractedfrom individual tokens and {c, a} are parser (con-figuration, action) pairs extracted from the un-rolled gold parse tree derivations, and ?
is a trade-off parameter.We optimize this objective stochastically by al-ternating between two updates:?
TAGGER: Pick a POS tagging example andupdate the tagger network with backpropaga-tion.?
PARSER: (Figure 4) Given a parser con-figuration c from the set of gold contexts,compute both tagger and parser activations.Backpropagate the parsing loss through thestacked architecture to update both parser andtagger, ignoring the tagger?s softmax layerparameters.While the learning procedure is inspired frommulti-task learning?we only update each stepwith regards one of the two likelihoods?there aresubtle differences that are important.
While a tra-ditional multi-task learning approach would usethe final layer of the parser network to predict bothPOS tags and parse trees, we predict POS tagsfrom the first hidden layer of our model (the ?tag-ger?
network) only.
We treat the POS labels asregularization of our parser and simply discard thesoftmax layer of the tagger network at test time.As we will show in Section 4, this regularizationleads to dramatic gains in parsing accuracy.
Notethat in Section 5, we also show experimentallythat stack-propagation is more powerful than thetraditional multi-task approach, and by combiningthem together, we can achieve better accuracy onboth POS and parsing tasks.1561Method ar bg da de en es eu fa fi fr hi id it iw nl no pl pt sl AVGNO TAGSB?15 LSTM 75.6 83.1 69.6 72.4 77.9 78.5 67.5 74.7 73.2 77.4 85.9 72.3 84.1 73.1 69.5 82.4 78.0 79.9 80.1 76.6Ours (window) 76.1 82.9 70.9 71.7 79.2 79.3 69.1 77.5 72.5 78.2 87.1 71.8 83.6 76.2 72.3 83.2 77.8 79.0 79.8 77.3UNIVERSAL TAGSETB?15 LSTM 74.6 82.4 68.1 73.0 77.9 77.8 66.0 75.0 73.6 78.0 86.8 72.2 84.2 74.5 68.4 83.3 74.5 80.4 78.1 76.2Pipeline Ptag73.7 83.6 72.0 73.0 79.3 79.5 63.0 78.0 66.9 78.5 87.8 73.5 84.2 75.4 70.3 83.6 73.4 79.5 79.4 76.6RBGParser 75.8 83.6 73.9 73.5 79.9 79.6 68.0 78.5 65.4 78.9 87.7 74.2 84.7 77.6 72.4 83.9 75.4 81.3 80.7 77.6Stackprop 77.0 84.3 73.8 74.2 80.7 80.7 70.1 78.5 74.5 80.0 88.9 74.1 85.8 77.5 73.6 84.7 79.2 80.4 81.8 78.9Table 2: Labeled Attachment Score (LAS) on Universal Dependencies Treebank.
Top: Results without any POS tag observa-tions.
?B?15 LSTM?
is the character-based LSTM model (Ballesteros et al, 2015), while ?Ours (window)?
is our window-basedarchitecture variant without stackprop.
Bottom: Comparison against state-of-the-art baselines utilizing the POS tags.
Pairedt-tests show that the gain of Stackprop over all other approaches is significant (p < 10?5for all but RBGParser, which isp < 0.02).3.1 Implementation detailsFollowing Weiss et al (2015), we use mini-batched averaged stochastic gradient descent(ASGD) (Bottou, 2010) with momentum (Hinton,2012) to learn the parameters ?
of the network.We use a separate learning rate, moving average,and velocity for the tagger network and the parser;the PARSER updates all averages, velocities, andlearning rates, while the TAGGER updates only thetagging factors.
We tuned the hyperparameters ofmomentum rate ?, the initial learning rate ?0andthe learning rate decay step ?
using held-out data.The training data for parsing and tagging can beextracted from either the same corpus or differentcorpora; in our experiments they were always thesame.To trade-off the two objectives, we used a ran-dom sampling scheme to perform 10 epochs ofPARSER updates and 5 epochs of TAGGER up-dates.
In our experiments, we found that pre-training with TAGGER updates for one epoch be-fore interleaving PARSER updates yielded fastertraining with better results.
We also experimentedusing the TAGGER updates solely for initializingthe parser and found that interleaving updates wascrucial to obtain improvements over the baseline.4 ExperimentsIn this section, we evaluate our approach on sev-eral dependency parsing tasks across a wide vari-ety of languages.4.1 Experimental SetupWe first investigated our model on 19 lan-guages from the Universal Dependencies Tree-banks v1.2.2We selected the 19 largest cur-2http://universaldependencies.orgrently spoken languages for which the full datawas freely available.
We used the coarse universaltagset in our experiments with no explicit morpho-logical annotations.
To measure parsing accuracy,we report unlabeled attachment score (UAS) andlabeled attachment score (LAS) computed on alltokens (including punctuation), as is standard fornon-English datasets.For simplicity, we use the arc-standard (Nivre,2004) transition system with greedy decoding.
Be-cause this transition system only produces projec-tive trees, we first apply a projectivization step toall treebanks before unrolling the gold derivationsduring training.
We make an exception for Dutch,where we observed a significant gain on develop-ment data by introducing the SWAP action (Nivre,2009) and allowing non-projective trees.For models that required predicted POS tags,we trained a window-based tagger using the samefeatures as the tagger component of our stackingmodel.
We used 5-fold jackknifing to produce pre-dicted tags on the training set.
We found that thewindow-based tagger was comparable to a state-of-the-art CRF tagger for most languages.
For ev-ery network we trained, we used the developmentdata to evaluate a small range of hyperparameters,stopping training early when UAS no longer im-proved on the held-out data.
We use H = 1024hidden units in the parser, and H = 128 hiddenunits in the tagger.
The parser embeds the tag-ger activations with D = 64.
Note that followingBallesteros et al (2015), we did not use any aux-iliary data beyond that in the treebanks, such aspre-trained word embeddings.For a final set of experiments, we evaluated onthe standard Wall Street Journal (WSJ) part of thePenn Treebank (Marcus et al, 1993)), dependen-cies generated from version 3.3.0 of the Stanford1562Method UAS LASNO TAGSDyer et al (2015) 92.70 90.30Ours (window-based) 92.85 90.77UNIVERSAL TAGSETPipeline (Ptag) 92.52 90.50Stackprop 93.23 91.30FINE TAGSETChen & Manning (2014) 91.80 89.60Dyer et al (2015) 93.10 90.90Pipeline (Ptag) 93.10 91.16Stackprop 93.43 91.41Weiss et al (2015) 93.99 92.05Alberti et al (2015) 94.23 92.36Table 3: WSJ Test set results for greedy and state-of-the-artmethods.
For reference, we show the most accurate modelsfrom Alberti et al (2015) and Weiss et al (2015), which usea deeper model and beam search for inference.converter (De Marneffe et al, 2006).
We followedstandard practice and used sections 2-21 for train-ing, section 22 for development, and section 23for testing.
Following Weiss et al (2015), weused section 24 to tune any hyperparameters of themodel to avoid overfitting to the development set.As is common practice, we use pretrained wordembeddings from the word2vec package whentraining on this dataset.4.2 ResultsWe present our main results on the Universal Tree-banks in Table 2.
We directly compare our ap-proach to other baselines in two primary ways.First, we compare the effectiveness of our learnedcontinuous representations with those of Alberti etal.
(2015), who use the predicted distribution overPOS tags concatenated with word embeddings asinput to the parser.
Because they also incorpo-rate beam search into training, we re-implement agreedy version of their method to allow for directcomparisons of token representations.
We refer tothis as the ?Pipeline (Ptag)?
baseline.
Second, wealso compare our architecture trained without POStags as regularization, which we refer to as ?Ours(window-based)?.
This model has the same archi-tecture as our full model but with no POS supervi-sion and updates.
Since this model never observesPOS tags in any way, we compare against a re-current character-based parser (Ballesteros et al,Model Variant UAS LAS POSArc-standard transition systemPipeline (Ptag) 81.56 76.55 95.14Ours (window-based) 82.08 77.08 -Ours (Stackprop) 83.38 78.78 -Joint parsing & tagging transition systemPipeline (Ptag) 81.61 76.57 95.30Ours (window-based) 82.58 77.76 94.92Ours (Stackprop) 83.21 78.64 95.43Table 4: Averaged parsing and POS tagging results on the UDtreebanks for joint variants of stackprop.
Given the window-based architecture, stackprop leads to higher parsing accura-cies than joint modeling (83.38% vs. 82.58%).2015) which is state-of-the-art when no POS tagsare provided.3Finally, we compare to RGBParser(Lei et al, 2014), a state-of-the art graph-based(non-greedy) approach.Our greedy stackprop model outperforms allother methods, including the graph-based RBG-Parser, by a significant margin on the test set(78.9% vs 77.6%).
This is despite the limitationsof greedy parsing.
Stackprop also yields a 2.3%absolute improvement in accuracy compared tousing POS tag confidences as features (PipelinePtag).
Finally, we also note that adding stack-prop to our window-based model improves accu-racy in every language, while incorporating pre-dicted POS tags into the LSTM baseline leads tooccasional drops in accuracy (most likely due tocascaded errors.
)5 DiscussionStackprop vs. other representations.
One un-expected result was that, even without the POStag labels at training time, our stackprop archi-tecture achieves better accuracy than either thecharacter-based LSTM or the pipelined baselines(Table 2).
This suggests that adding window-based representations?which aggregate over manyfeatures of the word and surrounding context?is more effective than increasing the expressive-ness of individual word representations by usingcharacter-based recurrent models.
In future workwe will explore combining these two complemen-tary approaches.We hypothesized that stackprop might providelarger gains over the pipelined model when the3We thank Ballesteros et al (2015) for their assistancerunning their code on the treebanks.1563Token married by a judge.
Don?t judge a book by and walked away satisfied when I walk in the doorNeighbors mesmerizing as a rat.
doesn?t change the company?s tried, and tried hard upset when I went toA staple!
won?t charge your phone and incorporated into I mean besides meday at a bar, then go don?t waste your money and belonged to the I felt as if IPattern a [noun] ?nt [verb] and [verb]ed I [verb]Table 5: Four of examples of tokens in context, along with the three most similar tokens according to the tagger network?sactivations, and the simple pattern exhibited.
Note that this model was trained with the Universal tagset which does notdistinguish verb tense.POS tags are very coarse.
We tested this latter hy-pothesis on the WSJ corpus by training our modelusing the coarse universal tagsets instead of thefine tagset (Table 3).
We found that stackpropachieves similar accuracy using coarse tagsets asthe fine tagset, while the pipelined baseline?s per-formance drops dramatically.
And while stack-prop doesn?t achieve the highest reported accura-cies on the WSJ, it does achieve competitive ac-curacies and outperforms prior state-of-the-art forgreedy methods (Dyer et al, 2015).Stackprop vs. joint modeling.
An alternativeto stackprop would be to train the final layer ofour architecture to predict both POS tags anddependency arcs.
To evaluate this, we trainedour window-based architecture with the integratedtransition system of Bohnet and Nivre (2012),which augments the SHIFT transition to predictPOS tags.
Note that if we also apply stackprop, thenetwork learns from POS annotations twice: oncein the TAGGER updates, and again the PARSER up-dates.
We therefore evaluated our window-basedmodel both with and without stack-propagation,and with and without the joint transition system.We compare these variants along with our re-implementation of the pipelined model of Albertiet al (2015) in Table 4.
We find that stackprop isalways better, even when it leads to ?double count-ing?
the POS annotations; in this case, the result isa model that is significantly better at POS taggingwhile marginally worse at parsing than stackpropalone.Reducing cascaded errors.
As expected, weobserve a significant reduction in cascaded POStagging errors.
An example from the English UDtreebank is given in Figure 5.
Across the 19 lan-guages in our test set, we observed a 10.9% gain(34.1% vs. 45.0%) in LAS on tokens where thepipelined POS tagger makes a mistake, comparedto a 1.8% gain on the rest of the corpora.Heterosexuals   increasingly     back        gay         marriageroot                 advmod          advmod     amod           dobjNOUN                ADV               ADV          ADJ           NOUN(a) Tree by a pipeline model.Heterosexuals   increasingly     back        gay         marriagensubj               advmod            root          amod           dobjNOUN                ADV               ADV          ADJ           NOUN(b) Tree by Stackprop model.Figure 5: Example comparison between predictions by apipeline model and a joint model.
While both models pre-dict a wrong POS tag for the word ?back?
(ADV rather thanVERB), the joint model is robust to this POS error and predictthe correct parse tree.Decreased model size.
Previous neural parsersthat use POS tags require learning embeddings forwords and other features on top of the parametersused in the POS tagger (Chen and Manning, 2014;Weiss et al, 2015).
In contrast, the number of to-tal parameters for the combined parser and tag-ger in the Stackprop model is reduced almost byhalf compared to the Pipeline model, because theparser and tagger share parameters.
Furthermore,compared to our implementation of the pipelinemodel, we observed that this more compact parsermodel was also roughly twice as fast.Contextual embeddings.
Finally, we also ex-plored the significance of the representationslearned by the tagger.
Unlike word embeddingmodels, the representations used in our parser areconstructed for each token based on its surround-ing context.
We demonstrate a few interestingtrends we observed in Table 5, where we show thenearest neighbors to sample tokens in this contex-tual embedding space.
These representations tendto represent syntactic patterns rather than individ-ual words, distinguishing between the form (e.g.?judge?
as a noun vs. a verb?)
and context of to-kens (e.g.
preceded by a personal pronoun).15646 ConclusionsWe present a stacking neural network model fordependency parsing and tagging.
Through a sim-ple learning method we call ?stack-propagation,?our model learns effective intermediate represen-tations for parsing by using POS tags as regular-ization of implicit representations.
Our model out-performs all state-of-the-art parsers when evalu-ated on 19 languages of the Universal Dependen-cies treebank and outperforms other greedy mod-els on the Wall Street Journal.We observe that the ideas presented in this workcan also be as a principled way to optimize up-stream NLP components for down-stream appli-cations.
In future work, we will extend this ideabeyond sequence modeling to improve models inNLP that utilize parse trees as features.
The basictenet of stack-propagation is that the hidden lay-ers of neural models used to generate annotationscan be used instead of the annotations themselves.This suggests a new methodology to building deepneural models for NLP: we can design them fromthe ground up to incorporate multiple sources ofannotation and learn far more effective intermedi-ate representations.AcknowledgmentsWe would like to thank Ryan McDonald, EmilyPitler, Chris Alberti, Michael Collins, and SlavPetrov for their repeated discussions, suggestions,and feedback, as well all members of the GoogleNLP Parsing Team.
We would also like tothank Miguel Ballesteros for assistance runningthe character-based LSTM.ReferencesChris Alberti, David Weiss, Greg Coppola, and SlavPetrov.
2015.
Improved transition-based parsingand tagging with neural networks.
In Proceedingsof EMNLP 2015.Miguel Ballesteros, Chris Dyer, and Noah A. Smith.2015.
Improved transition-based parsing by model-ing characters instead of words with lstms.
Proced-dings of EMNLP.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1455?1465.
Association for Computational Linguis-tics.L?eon Bottou.
2010.
Large-scale machine learningwith stochastic gradient descent.
In Proceedings ofCOMPSTAT?2010, pages 177?186.
Springer.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), volume 1, pages 740?750.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel P. Kuksa.2011.
Natural language processing (almost) fromscratch.
JMLR.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of Fifth International Conference onLanguage Resources and Evaluation, pages 449?454.Greg Durrett and Dan Klein.
2015.
Neural crf parsing.In Proceedings of the Association for ComputationalLinguistics.
Association for Computational Linguis-tics.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
ACL.Nicholas FitzGerald, Oscar Tckstrm, KuzmanGanchev, and Dipanjan Das.
2015.
Semantic rolelabeling with neural network factors.
In Proceed-ings of the 2015 Conference on Empirical Methodsin Natural Language Processing (EMNLP ?15).Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2011.
Incremental joint pos tag-ging and dependency parsing in chinese.
In IJC-NLP, pages 1216?1224.
Citeseer.Geoffrey E Hinton.
2012.
A practical guide to trainingrestricted boltzmann machines.
In Neural Networks:Tricks of the Trade, pages 599?619.
Springer.Terry Koo, Xavier Carreras P?erez, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In 46th Annual Meeting of the Association for Com-putational Linguistics, pages 595?603.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the52nd Annual Meeting of the Association for Com-putational Linguistics, volume 1, pages 1381?1391.Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,Wenliang Chen, and Haizhou Li.
2011.
Joint mod-els for chinese pos tagging and dependency pars-ing.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1180?1191.
Association for Computational Linguis-tics.1565Zhenghua Li, Min Zhang, Wanxiang Che, Ting Liu,and Wenliang Chen.
2014.
Joint optimizationfor chinese POS tagging and dependency parsing.IEEE/ACM Transactions on Audio, Speech & Lan-guage Processing, pages 274?286.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Joakim Nivre.
2004.
Incrementality in deterministicdependency parsing.
In Proceedings of the Work-shop on Incremental Parsing: Bringing Engineer-ing and Cognition Together, IncrementParsing ?04,pages 50?57, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Joakim Nivre.
2009.
Non-projective dependency pars-ing in expected linear time.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP, pages351?359.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A universal part-of-speech tagset.
arXiv preprintarXiv:1104.2086.Xian Qian and Yang Liu.
2012.
Joint chinese wordsegmentation, pos tagging and parsing.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 501?511.
Association for Computational Linguistics.Zhiguo Wang and Nianwen Xue.
2014.
Joint pos tag-ging and transition-based constituent parsing in chi-nese with non-local features.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics, pages 733?742.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural net-work transition-based parsing.
In Proceedings ofACL 2015, pages 323?333.David H Wolpert.
1992.
Stacked generalization.
Neu-ral networks.Yuan Zhang, Chengtao Li, Regina Barzilay, and Ka-reem Darwish.
2015.
Randomized greedy inferencefor joint segmentation, POS tagging and dependencyparsing.
In Proceedings of the 2015 Conference ofthe North American Chapter of the Association forComputational Linguistics, pages 42?52.1566
