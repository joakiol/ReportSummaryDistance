Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 89?92,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Novel Feature-based Approach to Chinese Entity Relation ExtractionWenjie Li1, Peng Zhang1,2, Furu Wei1, Yuexian Hou2 and Qin Lu11Department of Computing 2School of Computer Science and TechnologyThe Hong Kong Polytechnic University, Hong Kong Tianjin University, China{cswjli,csfwei,csluqin}@comp.polyu.edu.hk {pzhang,yxhou}@tju.edu.cnAbstractRelation extraction is the task of findingsemantic relations between two entities fromtext.
In this paper, we propose a novelfeature-based Chinese relation extractionapproach that explicitly defines and exploresnine positional structures between two entities.We also suggest some correction and inferencemechanisms based on relation hierarchy andco-reference information etc.
The approach iseffective when evaluated on the ACE 2005Chinese data set.1 IntroductionRelation extraction is promoted by the ACE program.It is the task of finding predefined semantic relationsbetween two entities from text.
For example, thesentence ?Bill Gates is the chairman and chiefsoftware architect of Microsoft Corporation?
conveysthe ACE-style relation ?ORG-AFFILIATION?between the two entities ?Bill Gates (PER)?
and?Microsoft Corporation (ORG)?.The task of relation extraction has been extensivelystudied in English over the past years.
It is typicallycast as a classification problem.
Existing approachesinclude feature-based and kernel-based classification.Feature-based approaches transform the context oftwo entities into a liner vector of carefully selectedlinguistic features, varying from entity semanticinformation to lexical and syntactic features of thecontext.
Kernel-based approaches, on the other hand,explore structured representation such as parse treeand dependency tree and directly compute thesimilarity between trees.
Comparably, feature-basedapproaches are easier to implement and achieve muchsuccess.In contrast to the significant achievementsconcerning English and other Western languages,research progress in Chinese relation extraction isquite limited.
This may be attributed to the differentcharacteristic of Chinese language, e.g.
no wordboundaries and lack of morphologic variations, etc.
Inthis paper, we propose a character-based Chineseentity relation extraction approach that complementsentity context (both internal and external) characterN-grams with four word lists extracted from apublished Chinese dictionary.
In addition to entitysemantic information, we define and examine ninepositional structures between two entities.
To copewith the data sparseness problem, we also suggestsome correction and inference mechanisms accordingto the given ACE relation hierarchy and co-referenceinformation.
Experiments on the ACE 2005 data setshow that the positional structure feature can providestronger support for Chinese relation extraction.Meanwhile, it can be captured with less effort thanapplying deep natural language processing.
Butunfortunately, entity co-reference does not help asmuch as we have expected.
The lack of necessaryco-referenced mentions might be the main reason.2 Related WorkMany approaches have been proposed in the literatureof relation extraction.
Among them, feature-based andkernel-based approaches are most popular.Kernel-based approaches exploit the structure ofthe tree that connects two entities.
Zelenko et al(2003)proposed a kernel over two parse trees, whichrecursively matched nodes from roots to leaves in atop-down manner.
Culotta and Sorensen (2004)extended this work to estimate similarity betweenaugmented dependency trees.
The above two workwas further advanced by Bunescu and Mooney (2005)who argued that the information to extract a relationbetween two entities can be typically captured by theshortest path between them in the dependency graph.Later, Zhang et al(2006) developed a compositekernel that combined parse tree kernel with entitykernel and Zhou et al(2007) experimented with acontext-sensitive kernel by automatically determiningcontext-sensitive tree spans.In the feature-based framework, Kambhatla (2004)employed ME models to combine diverse lexical,syntactic and semantic features derived from word,entity type, mention level, overlap, dependency andparse tree.
Based on his work, Zhou et al(2005)89further incorporated the base phrase chunkinginformation and semi-automatically collected countryname list and personal relative trigger word list.
Jiangand Zhai (2007) then systematically explored a largespace of features and evaluated the effectiveness ofdifferent feature subspaces corresponding to sequence,syntactic parse tree and dependency parse tree.
Theirexperiments showed that using only the basic unitfeatures within each feature subspace can alreadyachieve state-of-art performance, while over-inclusionof complex features might hurt the performance.Previous approaches mainly focused on Englishrelations.
Most of them were evaluated on the ACE2004 data set (or a sub set of it) which defined 7relation types and 23 subtypes.
Although Chineseprocessing is of the same importance as English andother Western language processing, unfortunately fewwork has been published on Chinese relationextraction.
Che et al(2005) defined an improved editdistance kernel over the original Chinese stringrepresentation around particular entities.
The onlyrelation they studied is PERSON-AFFLIATION.
Theinsufficient study in Chinese relation extraction drivesus to investigate how to find an approach that isparticularly appropriate for Chinese.3 A Chinese Relation Extraction ModelDue to the aforementioned reasons, entity relationextraction in Chinese is more challenging than inEnglish.
The system segmented words are already noterror free, saying nothing of the quality of thegenerated parse trees.
All these errors willundoubtedly propagate to the subsequent processing,such as relation extraction.
It is therefore reasonable toconclude that kernel-based especially tree-kernelapproaches are not suitable for Chinese, at least atcurrent stage.
In this paper, we study a feature-basedapproach that basically integrates entity relatedinformation with context information.3.1 Classification FeaturesThe classification is based on the following four typesof features.z Entity Positional Structure FeaturesWe define and examine nine finer positionalstructures between two entities (see Appendix).
Theycan be merged into three coarser structures.z Entity FeaturesEntity types and subtypes are concerned.z Entity Context FeaturesThese are character-based features.
We considerboth internal and external context.
Internal contextincludes the characters inside two entities and thecharacters inside the heads of two entities.
Externalcontext involves the characters around two entitieswithin a given window size (it is set to 4 in this study).All the internal and external context characters aretransformed to Uni-grams and Bi-grams.z Word List FeaturesAlthough Uni-grams and Bi-grams should be ableto cover most of Chinese words given sufficienttraining data, many discriminative words might not bediscovered by classifiers due to the severe sparsenessproblem of Bi-grams.
We complement character-based context features with four word lists which areextracted from a published Chinese dictionary.
Theword lists include 165 prepositions, 105 orientations,20 auxiliaries and 25 conjunctions.3.2 Correction with Relation/ArgumentConstraints and Type/Subtype Consistency CheckAn identified relation is said to be correct only whenits type/subtype (R) is correct and at the same time itstwo arguments (ARG-1 and ARG-2) must be of thecorrect entity types/subtypes and of the correct order.One way to improve the previous feature-basedclassification approach is to make use of the priorknowledge of the task to find and rectify the incorrectresults.
Table 1 illustrates the examples of possiblerelations between PER and ORG.
We regard possiblerelations between two particular types of entityarguments as constraints.
Some relations aresymmetrical for two arguments, such as PER_SOCIAL.FAMILY, but others not, such as ORG_AFF.EMPLOYMENT.
Argument orders are important forasymmetrical relations.PER ORGPER PER_SOCIAL.BUS, PER_SOCIAL.FAMILY, ?ORG_AFF.EMPLOYMENT,ORG_AFF.OWNERSHIP, ?ORG  PART_WHOLE.SUBSIDIARY, ORG_AFF.INVESTOR/SHARE, ?Table 1 Possible Relations between ARG-1 and ARG-2Since our classifiers are trained on relations insteadof arguments, we simply select the first (as in adjacentand separate structures) and outer (as in nestedstructures) as the first argument.
This setting works atmost of cases, but still fails sometimes.
The correctionworks in this way.
Given two entities, if the identifiedtype/subtype is an impossible one, it is revised toNONE (it means no relation at all).
If the identifiedtype/subtype is possible, but the order of argumentsdoes not consist with the given relation definition, theorder of arguments is adjusted.Another source of incorrect results is theinconsistency between the identified types andsubtypes, since they are typically classified separately.90This type of errors can be checked against theprovided hierarchy of relations, such as the subtypesOWNERSHIP and EMPLOYMENT must belong tothe ORG_AFF type.
There are existing strategies todeal with this problem, such as strictly bottom-up (i.e.use the identified subtype to choose the type it belongsto), guiding top-down (i.e.
to classify types first andthen subtypes under a certain type).
However, thesetwo strategies lack of interaction between the twoclassification levels.
To insure consistency in aninteractive manner, we rank the first n numbers of themost likely classified types and then check themagainst the classified subtype one by one until thesubtype conforms to a type.
The matched type isselected as the result.
If the last type still fails, bothtype and subtype are revised to NONE.
We call thisstrategy type selection.
Alternatively, we can choosethe most likely classified subtypes, and check themwith the classified type (i.e.
subtype selectionstrategy).
Currently, n is 2.3.2 Inference with Co-reference Information andLinguistic PatternsEach entity can be mentioned in different places intext.
Two mentions are said to be co-referenced to oneentity if they refers to the same entity in the worldthough they may have different surface expressions.For example, both ?he?
and ?Gates?
may refer to ?BillGates of Microsoft?.
If a relation ?ORG-AFFILIATION?
is held between ?Bill Gates?
and?Microsoft?, it must be also held between ?he?
and?Microsoft?.
Formally, given two entities E1={EM11,EM12, ?, EM1n} and E2={EM21, EM22, ?, EM2m} (Eiis an entity, EMij is a mention of Ei), it is true thatR(EM11, EM21)?
R(EM1l, EM2k).
This nature allowsus to infer more relations which may not be identifiedby classifiers.Our previous experiments show that theperformance of the nested and the adjacent relations ismuch better than the performance of other structuredrelations which suffer from unbearable low recall dueto insufficient training data.
Intuitively we can followthe path of ?Nested ?
Adjacent ?
Separated ?Others?
(Nested, Adjacent and Separated structuresare majority in the corpus) to perform the inference.But soon we have an interesting finding.
If two relatedentities are nested, almost all the mentions of them arenested.
So basically inference works on ?Adjacent ?Separated?
?.When considering the co-reference information, wemay find another type of inconsistency, i.e.
the oneraised from co-referenced entity mentions.
It ispossible that R(EM11, EM21) ?
R(EM12, EM22) when Ris identified based on the context of EM.
Co-referencenot only helps for inference but also provides thesecond chance to check the consistency among entitymention pairs so that we can revise accordingly.
As theclassification results of SVM can be transformed toprobabilities with a sigmoid function, the relations oflower probability mention pairs are revised accordingto the relation of highest probability mention pairs.The above inference strategy is called coreference-based inference.
Besides, we find that pattern-basedinference is also necessary.
The relations of adjacentstructure can infer the relations of separated structureif there are certain linguistic indicators in the localcontext.
For example, given a local context ?EM1 andEM2 located EM3?, if the relation of EM2 and EM3 hasbeen identified, EM1 and EM3 will take the relationtype/subtype that EM2 and EM3 holds.
Currently, theonly indicators under consideration are ?and?
and ?or?.However, more patterns can be included in the future.4 Experimental ResultsThe experiments are conducted on the ACE 2005Chinese RDC training data (with true entities) where 6types and 18 subtypes of relations are annotated.
Weuse 75% of it to train SVM classifiers and theremaining to evaluate results.The aim of the first set of experiments is to examinethe role of structure features.
In these experiments, a?NONE?
class is added to indicate a null type/subtype.With entity features and entity context features andword list features, we consider three differentclassification contexts: (1), only three coarserstructures 1 , i.e.
nested, adjacent and separated, areused as feature, and a classifier is trained for eachrelation type and subtype; (2) similar to (1) but all ninestructures are concerned; and (3) similar to (2) but thetraining data is divided into 9 parts according tostructure, i.e.
type and subtype classifiers are trainedon the data with the same structures.
The resultspresented in Table 2 show that 9-structure is muchmore discriminative than 3-structure.
Also, theperformance can be improved significantly bydividing training data based on nine structures.Type / Subtype Precision Recall F-measure3-Structure 0.7918/0.7356 0.3123/0.2923 0.4479/0.41839-Structure 0.7533/0.7502 0.4389/0.3773 0.5546/0.50219-Structure_Divide 0.7733/0.7485 0.5506/0.5301 0.6432/0.6209Table 2 Evaluation on Structure FeaturesStructure Positive Class Negative Class RatioNested 6332 4612 1 : 0.7283Adjacent 2028 27100 1 : 13.36291 Nine structures are combined to three by merging (b) and (c) to (a), (e)and (f) to (d), (h) and (i) to (g).91Separated 939 79989 1 : 85.1853Total 9299 111701 1 : 12.01Table 3 Imbalance Training Class ProblemIn the experiments, we find that the training classimbalance problem is quite serious, especially for theseparated structure (see Table 3 above where?Positive?
and ?Negative?
mean there exists a relationbetween two entities and otherwise).
A possiblesolution to alleviate this problem is to detect whetherthe given two entities have some relation first and ifthey do then to classify the relation types and subtypesinstead of combining detection and classification inone process.
The second set of experiment is toexamine the difference between these twoimplementations.
Against our expectation, thesequence implementation does better than thecombination implementation, but not significantly, asshown in Table 4 below.Type / Subtype Precision Recall F-measureCombination 0.7733/0.7485 0.5506/0.5301 0.6432/0.6206Sequence 0.7374/0.7151 0.5860/0.5683 0.6530/0.6333Table 4 Evaluation of Two Detection and Classification ModesBased on the sequence implementation, we set upthe third set of experiments to examine the correctionand inference mechanisms.
The results are illustratedin Table 5.
The correction with constraints andconsistency check is clearly contributing.
It improvesF-measure 7.40% and 6.47% in type and subtypeclassification respectively.
We further compare fourpossible consistency check strategies in Table 6 andfind that the strategies using subtypes to determine orselect types perform better than top down strategies.This can be attributed to the fact that correction withrelation/argument constraints in subtype is tighter thanthe ones in type.Type / Subtype Precision Recall F-measureSeq.
+ Cor.
0.8198/0.7872 0.6127/0.5883 0.7013/0.6734Seq.
+ Cor.
+ Inf.
0.8167/0.7832 0.6170/0.5917 0.7029/0.6741Table 5 Evaluation of Correction and Inference MechanismsType / Subtype Precision Recall F-measureGuiding Top-Down 0.7644/0.7853 0.6074/0.5783 0.6770/0.6661Subtype Selection 0.8069/0.7738 0.6065/0.5817 0.6925/0.6641Strictly Bottom-Up 0.8120/0.7798 0.6146/0.5903 0.6996/0.6719Type Selection 0.8198/0.7872 0.6127/0.5883 0.7013/0.6734Table 6 Comparison of Different Consistency Check StrategiesFinally, we provide our findings from the fourth setof experiments which looks at the detailedcontributions from four feature types.
Entity typefeatures themselves do not work.
We incrementallyadd the structures, the external contexts and internalcontexts, Uni-grams and Bi-grams, and at last theword lists on them.
The observations are: Uni-gramsprovide more discriminative information thanBi-grams; external context seems more useful thaninternal context; positional structure provides strongersupport than other individual recognized features suchas entity type and context; but word list feature can notfurther boost the performance.Type / Subtype Precision Recall F-measureEntity Type + Structure 0.7288/0.6902 0.4876/0.4618 0.5843/0.5534+ External (Uni-) 0.7935/0.7492 0.5817/0.5478 0.6713/0.6321+ Internal (Uni-) 0.8137/0.7769 0.6113/0.5836 0.6981/0.6665+ Bi- (Internal & External) 0.8144/0.7828 0.6141/0.5902 0.7002/0.6730+ Wordlist 0.8167/0.7832 0.6170/0.5917 0.7029/0.6741Table 6 Evaluation of Feature and Their Combinations5 ConclusionIn this paper, we study feature-based Chinese relationextraction.
The proposed approach is effective on theACE 2005 data set.
Unfortunately, there is no resultreported on the same data so that we can compare.6 Appendix: Nine Positional StructuresAcknowledgmentsThis work was supported by HK RGC (CERG PolyU5211/05E)and China NSF (60603027).ReferencesRazvan Bunescu and Raymond Mooney.
2005.
A Shortest PathDependency Tree Kernel for Relation Extraction, In Proceedings ofHLT/EMNLP, pages 724-731.Aron Culotta and Jeffrey Sorensen.
2004.
Dependency Tree Kernels forRelation Extraction, in Proceedings of ACL, pages 423-429.Jing Jiang, Chengxiang Zhai.
2007.
A Systematic Exploration of theFeature Space for Relation Extraction.
In proceedings ofNAACL/HLT, pages 113-120.Nanda Kambhatla.
2004.
Combining Lexical, Syntactic, and SemanticFeatures with Maximum Entropy Models for Extracting Relations.In Proceedings of ACL, pages 178-181.Dmitry Zelenko, Chinatsu Aone and Anthony Richardella.
2003.Kernel Methods for Relation Extraction.
Journal of MachineLearning Research 3:1083-1106Min Zhang, Jie Zhang, Jian Su and Guodong Zhou.
2006.
A CompositeKernel to Extract Relations between Entities with both Flat andStructured Features, in Proceedings of COLING/ACL, pages825-832.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.
2005.
ExploringVarious Knowledge in Relation Extraction.
In Proceedings of ACL,pages 427-434.GuoDong Zhou, Min Zhang, Donghong Ji and Qiaoming Zhu.
2007.Tree Kernel-based Relation Extraction with Context-SensitiveStructured Parse Tree Information.
In Proceedings of EMNLP,pages 728-736.Wanxiang Che et al 2005.
Improved-Edit-Distance Kernel for ChineseRelation Extraction.
In Proceedings of IJCNLP, pages 132-137.92
