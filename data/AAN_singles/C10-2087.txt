Coling 2010: Poster Volume, pages 757?765,Beijing, August 2010Dependency-Driven Feature-based Learning for ExtractingProtein-Protein Interactions from Biomedical TextBing Liu   Longhua QianHongling Wang   Guodong ZhouJiangsu Provincial Key Lab for Computer Information Processing TechnologySchool of Computer Science and TechnologySoochow UniversityEmail: liubingnlp@gmail.com{qianlonghua,redleaf,gdzhou}@suda.edu.cnCorresponding authorAbstractRecent kernel-based PPI extractionsystems achieve promising perform-ance because of their capability tocapture structural syntactic informa-tion, but at the expense of computa-tional complexity.
This paper incorpo-rates dependency information as wellas other lexical and syntactic knowl-edge in a feature-based framework.Our motivation is that, considering thelarge amount of biomedical literaturebeing archived daily, feature-basedmethods with comparable performanceare more suitable for practical applica-tions.
Additionally, we explore thedifference of lexical characteristics be-tween biomedical and newswire do-mains.
Experimental evaluation on theAIMed corpus shows that our systemachieves comparable performance of54.7 in F1-Score with otherstate-of-the-art PPI extraction systems,yet the best performance among all thefeature-based ones.1 IntroductionIn recent years, automatically extractingbiomedical information has been the subject ofsignificant research efforts due to the rapidgrowth in biomedical development anddiscovery.
A wide concern is how tocharacterize protein interaction partners sinceit is crucial to understand not only thefunctional role of individual proteins but alsothe organization of the entire biologicalprocess.
However, manual collection ofrelevant Protein-Protein Interaction (PPI)information from thousands of research paperspublished every day is so time-consuming thatautomatic extraction approaches with the helpof Natural Language Processing (NLP)techniques become necessary.Various machine learning approaches forrelation extraction have been applied to thebiomedical domain, which can be classifiedinto two categories: feature-based methods(Mitsumori et al, 2006; Giuliano et al, 2006;S?tre et al, 2007) and kernel-based methods(Bunescu et al, 2005; Erkan et al, 2007;Airola et al, 2008; Kim et al, 2010).Provided a large-scale manually annotatedcorpus, the task of PPI extraction can beformulated as a classification problem.Typically, for featured-based learning eachprotein pair is represented as a vector whosefeatures are extracted from the sentenceinvolving two protein names.
Early studiesidentify the existence of protein interactionsby using ?bag-of-words?
features (usuallyuni-gram or bi-gram) around the proteinnames as well as various kinds of shallowlinguistic information, such as POS tag,lemma and orthographical features.
However,these systems do not achieve promising resultssince they disregard any syntactic or semanticinformation altogether, which are very usefulfor the task of relation extraction in thenewswire domain (Zhao and Grishman, 2005;Zhou et al, 2005).
Furthermore, feature-basedmethods fail to effectively capture thestructural information, which is essential to757identify the relationship between two proteinsin a syntactic representation.With the wide application of kernel-basedmethods to many NLP tasks, various kernelssuch as subsequence kernels (Bunescu andMooney, 2005) and tree kernels (Li et al,2008), are also applied to PPI detection..Particularly, dependency-based kernels suchas edit distance kernels (Erkan et al, 2007)and graph kernels (Airola et al, 2008; Kim etal., 2010) show some promising results for PPIextraction.
This suggests that dependencyinformation play a critical role in PPIextraction as well as in relation extractionfrom newswire stories (Culotta and Sorensen,2004).
In order to appreciate the advantages ofboth feature-based methods and kernel-basedmethods, composite kernels (Miyao et al,2008; Miwa et al, 2009a; Miwa et al, 2009b)are further employed to combine structuralsyntactic information with flat word featuresand significantly improve the performance ofPPI extraction.
However, one criticalchallenge for kernel-based methods is theircomputation complexity, which prevents themfrom being widely deployed in real-worldapplications regarding the large amount ofbiomedical literature being archived everyday.Considering the potential of dependency in-formation for PPI extraction and the challengeof computation complexity of kernel-basedmethods, one may naturally ask the question:?Can the essential dependency information bemaximally exploited in featured-based PPIextraction so as to enhance the performancewithout loss of efficiency??
?If the answer isYes, then How?
?This paper addresses these problems, focus-ing on the application of dependency informa-tion to feature-based PPI extraction.
Startingfrom a baseline system in which commonlexical and syntactic features are incorporatedusing Support Vector Machines (SVM), wefurther augment the baseline with various fea-tures related to dependency information,including predicates in the dependency tree.Moreover, in order to reveal the linguisticdifference between distinct domains we alsocompare the effects of various features on PPIextraction from biomedical texts with those onrelation extraction from newswire narratives.Evaluation on the AIMed and other PPI cor-pora shows that our method achieves the bestperformance among all feature-based systems.The rest of the paper is organized as follows.A feature-based PPI extraction baseline systemis given in Section 2 while Section 3 describesour dependency-driven method.
We report ourexperiments in Section 4, and compare ourwork with the related ones in Section 5.Section 6 concludes this paper and gives somefuture directions.2 Feature-based PPI extraction:BaselineFor feature-based methods, PPI extraction taskis re-cast as a classification problem by firsttransforming PPI instances intomulti-dimensional vectors with various fea-tures, and then applying machine learning ap-proaches to detect whether the potentialrelationship exists for a particular protein pair.In training, a feature-based classifier learningalgorithm, such as SVM or MaxEnt, uses theannotated PPI instances to learn a classifierwhile, in testing, the learnt classifier is in turnapplied to new instances to determine their PPIbinary classes and thus candidate PPI instancesare extracted.As a baseline, various linguistic features,such as words, overlap, chunks, parse tree fea-tures as well as their combined ones are ex-tracted from a sentence and formed as a vectorinto the feature-based learner.1) WordsFour sets of word features are used in our sys-tem: 1) the words of both the proteins; 2) thewords between the two proteins; 3) the wordsbefore M1 (the 1st protein); and 4) the wordsafter M2 (the 2nd protein).
Both the words be-fore M1 and after M2 are classified into twobins: the first word next to the proteins and thesecond word next to the proteins.
This meansthat we only consider the two words before M1and after M2.
Words features include:x MW1: bag-of-words in M1x MW2: bag-of-words in M2x BWNULL: when no word in betweenx BWO: other words in between exceptfirst and last words when at least threewords in betweenx BWM1FL: the only word before M1758x BWM1F: first word before M1x BWM1L: second word before M1x BWM1: first and second word beforeM1x BWM2FL: the only word after M2x BWM2F: first word after M2x BWM2L: second word after M2x BWM2: first and second word after M22) OverlapThe numbers of other protein names as well asthe words that appear between two proteinnames are included in the overlap features.This category of features includes:x #MB: number of other proteins in be-tweenx #WB: number of words in betweenx E-Flag: flag indicating whether the twoproteins are embedded or not3) ChunksIt is well known that chunking plays animportant role in the task of relation extractionin the ACE program (Zhou et al, 2005).
How-ever, its significance in PPI extraction has notfully investigated.
Here, the Stanford Parser1is first employed for full parsing, and thenbase phrase chunks are derived from full parsetrees using the Perl script2.
The chunking fea-tures usually concern about the head words ofthe phrases between the two proteins, whichare further classified into three bins: the firstphrase head in between, the last phrase head inbetween and other phrase heads in between.
Inaddition, the path of phrasal labels connectingtwo proteins is also a common syntacticindicator of the polarity of the PPI instance,just as the path NP_VP_PP_NP in the sen-tence ?The ability of PROT1 to interact withthe PROT2 was investigated.?
is likely to sug-gest the positive interaction between two pro-teins.
These base phrase chunking featurescontain:x CPHBNULL: when no phrase in be-tween.x CPHBFL: the only phrase head whenonly one phrase in betweenx CPHBF: the first phrase head in betweenwhen at least two phrases in between.1 http://nlp.stanford.edu/software/lex-parser.shtml2 http://ilk.kub.nl/~sabine/chunklink/x CPHBL: the last phrase head in betweenwhen at least two phrase heads in be-tween.x CPHBO: other phrase heads in betweenexcept first and last phrase heads whenat least three phrases in between.x CPP: path of phrase labels connectingthe two entities in the chunkingFurthermore, we also generate a set ofbi-gram features which combine the abovechunk features except CPP with their corre-sponding chunk types.4) Parse TreeIt is obvious that full pares trees encompassrich structural information of a sentence.Nevertheless, it is much harder to exploresuch information in featured-based methodsthan in kernel-based ones.
Thus so far onlythe path connecting two protein names in thefull-parse tree is considered as a parse treefeature.x PTP: the path connecting two proteinnames in the full-parse tree.Again, take the sentence ?The ability ofPROT1 to interact with the PROT2 wasinvestigated.?
as an example, the parse pathbetween PROT1 and PROT2 isNP_S_VP_PP_NP, which is slightly differentfrom the CPP feature in the chunking featureset.3 Dependency-Driven PPI ExtractionThe potential of dependency information forPPI extraction lies in the fact that the depend-ency tree may well reveal non-local orlong-range dependencies between the wordswithin a sentence.
In order to capture thenecessary information inherent in thedepedency tree for identifying theirrelationship, various kernels, such as editdistance kernel based on dependency path(Erkan et al, 2007), all-dependency-pathgraph kernel (Airola et al, 2008), andwalk-weighted subsequence kernels (Kim etal., 2010) as well as other composite kernels(Miyao et al, 2008; Miwa et al, 2009a; Miwaet al, 2009b), have been proposed to addressthis problem.
It?s true that these methodsachieve encouraging results, neverthless, theysuffer from prohibitive computation burden.759Thus, our solution is to fold the structuraldependency information back into flatfeatures in a feature-based framework so as tospeed up the learning process while retainingcomparable performance.
This is what werefer to as dependency-driven PPI extraction.First, we construct dependency trees fromgrammatical relations generated by the Stan-ford Parser.
Every grammatical relation has theform of dependent-type (word1, word2),Where word1 is the head word, word2 is de-pendent on word1, and dependent-type denotesthe pre-defined type of dependency.
Then,from these grammatical relations the followingfeatures called DependenecySet1 are takeninto consideration as illustrated in Figure 1:x DP1TR: a list of words connectingPROT1 and the dependency tree root.x DP2TR: a list of words connectingPROT2 and the dependency tree root.x DP12DT: a list of dependency typesconnecting the two proteins in thedependency tree.x DP12: a list of dependent words com-bined with their dependency types con-necting the two proteins in the depend-ency tree.x DP12S: the tuple of every word com-bined with its dependent type in DP12.x DPFLAG: a boolean value indicatingwhether the two proteins are directlydependent on each other.The typed dependencies produced by theStanford Parser for the sentence ?PROT1contains a sequence motif binds to PROT2.
?are listed as follows:nsubj(contains-2,PROT1-1)det(motif-5, a-3)nn(motif-5, sequence-4)nsubj(binds-6, motif-5)ccomp(contains-2, binds-6)prep_to(binds-6, PROT2-8)Each word in a dependency tuple is fol-lowed by its index in the original sentence,ensuring accurate positioning of the headword and dependent word.
Figure 1 shows thedependency tree we construct from the abovegrammatical relations.containsPROT1motifbindsPROT2a sequencensubj ccompprep_tonsubjdet nnFigure 1: Dependency tree for the sentence?PROT1 contains a sequence motif binds toPROT2.
?Erkan et al (2007) extract the pathinformation between PROT1 and PROT2 inthe dependency tree for kernel-based PPIextraction and report promising results,neverthless, such path is so specific forfeature-based methods that it may incurehigher precision but lower recall.
Thus wealleviate this problem by collapsing the featureinto multiple ones with finer granularity,leading to the features such as DP12S.It is widely acknowledged that predicatesplay an important role in PPI extraction.
Forexample, the change of a pivot predicatebetween two proteins may easily lead to thepolarity reversal of a PPI instance.
Therefore,we extract the predicates and their positions inthe dependency tree as predicate featurescalled DependencySet2:x FVW: the predicates in the DP12 featureoccurring prior to the first protein.x LVW: the predicates in the DP12 featureoccurring next to the second entity.x MVW: other predicates in the DP12features.x #FVW: the number of FVWx #LVW: the number of LVWx #MVW: the number of MVW4 ExperimentationThis section systematically evaluates our fea-ture-based method on the AIMed corpus aswell as other commonly used corpus and re-ports our experimental results.7604.1 Data SetsWe use five corpora3 with the AIMed corpusas the main experimental data, which contains177 Medline abstracts with interactions be-tween two interactions, and 48 abstracts with-out any PPI within single sentences.
There are4,084 protein references and around 1,000annotated interactions in this data set.For corpus pre-procession, we first renametwo proteins of a pair as PROT1 and PROT2respectively in order to blind the learner forfair comparison with other work.
Then, allthe instances are generated from the sentenceswhich contain at least two proteins,  that is, ifa sentence contains n different proteins, thereare n2 different pairs of proteins and thesepairs are considered untyped and undirected.For the purpose of comparison with previouswork, all the self-interactions (59 instances)are removed, while all the PPI instances withnested protein names are retained (154 in-stances).
Finally, 1002 positive instances and4794 negative instances are generated andtheir corresponding features are extracted.We select Support Vector Machines (SVM)as the classifier since SVM represents thestate-of-the-art in the machine learning re-search community.
In particular, we use thebinary-class SVMLigh 4 developed byJoachims (1998) since it satisfies our require-ment of detecting potential PPI instances.Evaluation is done using 10-fold docu-ment-level cross-validation.
Particularly, weapply the extract same 10-fold split that wasused by Bunescu et al (2005) and Giuliano etal.
(2006).
Furthermore, OAOD (One Answerper Occurrence in the Document) strategy isadopted, which means that the correct interac-tion must be extracted for each occurrence.This guarantees the maximal use of the avail-able data, and more important, allows faircomparison with earlier relevant work.The evaluation metrics are commonly usedPrecision (P), Recall (R) and harmonicF1-score (F1).
As an alternative to F1-score,the AUC (area under the receiver operatingcharacteristics curve) measure is proved to beinvariant to the class distribution of the train-ing dataset.
Thus we also provide AUC scores3 http://mars.cs.utu.fi/PPICorpora/GraphKernel.html4 http://svmlight.joachims.org/for our system as Airola et al (2008) andMiwa et al (2009a).4.2 Results and DiscussionFeatures P(%) R(%) F1Baseline featuresWords 59.4 40.6 47.6+Overlap 60.4 39.9 47.4+Chunk 59.2 44.5 50.6+Parse 60.9 44.8 51.4Dependency-driven features+Dependency Set1 62.9 48.0 53.9+Dependency Set2 63.4 48.8 54.7Table 1: Performance of PPI extraction with vari-ous features in the AIMed corpusWe present in Table 1 the performance of oursystem using document-wise evaluationstrategies and 10-fold cross-validation withdifferent features in the AIMed corpus, wherethe plus sign before a feature means it isincrementally added to the feature set.
Table 1reports that our system achieves the best per-formance of 63.4/48.8/54.7 in P/R/F scores.
Italso shows that:x Words features alone achieve a relativelylow performance of 59.4/40.9/47.6 inP/R/F, particularly with fairly low recallscore.
This suggests the difficulty of PPIextraction and words features alone can?teffectively capture the nature of proteininteractions.x Overlap features slightly decrease the per-formance.
Statistics show that both thedistributions of #MB and #WB betweenpositives and negatives are so similar thatthey are by no means the discriminators forPPI extraction.
Hence, we exclude theoverlap features in the succeeding experi-ments.x Chunk features significantly improves theF-measure by 3 units largely due to the in-crease of recall by 3.9%, though at theslight expense of precision.
This suggeststhe effectiveness of shallow parsing infor-mation in the form of headwords capturedby chunking on PPI extraction.x The usefulness of the parse tree features isquite limited.
It only improves theF-measure by 0.8 units.
The main reasonmay be that these paths are usually long761and specific, thus they suffer from theproblem of data sparsity.
Furthermore,some of the parse tree features are alreadyinvolved in the chunk features.x The DependencySet1 features are veryeffective in that it can increase the preci-sion and recall by 2.0 and 3.2 unitsrespectively, leading to the increase of F1score by 2.5 units.
This means that the de-pendency-related features can effectivelyretrieve more PPI instances without intro-ducing noise that will severely harm theprecision.
According to our statistics, thereare over 60% sentences with more than 5words between their protein entities in theAIMed corpus.
Therefore, dependency in-formation exhibit great potential to PPIextraction since they can capturelong-range dependencies within sentences.Take the aforementioned sentence?PROT1 contains a sequence motif bindsto PROT2.?
as an example, although thetwo proteins step over a relatively longdistance, the dependency path betweenthem is concise and accurate, reflecting theessence of the interaction.x The predicate features also contribute tothe F1-score gain of 0.8 units.
It is notsurprising since some predicates, such as?interact?, ?activate?
and ?inhibit?
etc, arestrongly suggestive of the interactionpolarity between two proteins.We compare in Table 2 the performance ofour system with other systems in the AIMedcorpus using the same 10-fold cross validationstrategy.
These systems are grouped into threedistinct classes: feature-based, kernel-basedand composite kernels.
Except for Airola et al(2008) Miwa et al (2009a) and Kim et al(2010), which adopt graph kernels, our systemperforms comparably with other systems.
Inparticular, our dependency-driven systemachieves the best F1-score of 54.7 among allfeature-based systems.In order to measure the generalization abil-ity of our dependency-driven PPI extractionsystem across different corpora, we furtherapply our method to other four publicly avail-able PPI corpora: BioInfer, HPRD50, IEPAand LLL.Table 2: Comparison with other PPI extractionsystems in the AIMed corpusThe corresponding performance ofF1-score and AUC metrics as well as theirstandard deviations is present in Table 3.Comparative available results from Airola etal.
(2008) and Miwa et al (2009a) are alsoincluded in Table 3 for comparison.
This tableshows that our system performs almostconsistently with the other two systems, that is,the LLL corpus gets the best performance yetwith the greatest variation, while the AIMedcorpus achieves the lowest performance withreasonable variation.It is well known that biomedical texts ex-hibit distinct linguistic characteristics fromnewswire narratives, leading to dramatic per-formance gap between PPI extraction andrelation detection in the ACE corpora.
How-ever, no previous work has ever addressed thisproblem and empirically characterized thisdifference.
In this paper, we devise a series ofexperiments over the ACE RDC corpora usingour dependency-driven feature-based methodas a touchstone task.
In order to do that, a sub-5 Airola et al (2008) repeat the method published byGiuliano et al (2006) with a correctly preprocessedAIMed and reported an F1-score of 52.4%.6 The results from Table 1 (Miyao et al, 2009) with themost similar settings to ours (Stanford Parser with SDrepresentation) are reported.Systems P(%) R(%) F1Feature-based methodsOur system 63.4 48.8 54.7Giuliano et al, 20065 60.9 57.2 59.0S?tre et al, 2007 64.3 44.1 52.0Mitsumori et al, 2006 54.2 42.6 47.7Yakushiji et al, 2005 33.7 33.1 33.4Kernel-based methodsKim et al, 2010 61.4 53.3 56.7Airola et al, 2008 52.9 61.8 56.4Bunescu et al, 2006  65.0 46.4 54.2Composite kernelsMiwa et al, 2009a - - 62.0Miyao et al, 20086 51.8 58.1 54.5762set of 5796 relation instances is randomlysampled from the ACE 2003 and 2004 cor-pora respectively.
The same cross-validationand evaluation metrics are applied to thesetwo sets as PPI extraction in the AIMed cor-pus.Our system Airola et al (2008) 7 Miwa et al (2009a)Corpus F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUC F1 ?F1 AUC ?AUCAIMed 54.7 4.5 82.4 3.5 56.4 5.0 84.8 2.3 60.8 6.6 86.8 3.3BioInfer 59.8 3.5 80.9 3.3 61.3 5.3 81.9 6.5 68.1 3.2 85.9 4.4HPRD50 64.9 13.4 79.8 8.5 63.4 11.4 79.7 6.3 70.9 10.3 82.2 6.3IEPA 62.1 6.2 74.8 6.6 75.1 7.0 85.1 5.1 71.7 7.8 84.4 4.2LLL 78.1 15.8 85.1 8.3 76.8 17.8 83.4 12.2 80.1 14.1 86.3 10.8Table 3: Comparison of performance across the five PPI corporaAIMed ACE2003 ACE2004FeaturesP(%) R(%) F1 P(%) R(%) F1 P(%) R(%) F1Words 59.4 40.6 47.6 66.5 51.6 57.9 68.1 59.6 63.4+Overlap +1.0 -0.7 -0.2 +5.4 +1.8 +3.2 +4.6 +1.2 +2.7+Chunk -1.7 +4.6 +3.2 +2.3 +5.1 +4.0 +1.5 +1.9 +1.7+Parse +1.7 +0.3 +0.8 +0.3 +0.6 +0.5 +0.6 +0.4 +0.5+Dependency Set1 +2.0 +3.2 +2.5 +0.8 +0.7 +0.7 +0.5 +0.9 +0.7+Dependency Set2 +0.5 +0.8 +0.8 +0.3 +0.2 +0.3 +0.2 +0.4 +0.3Table 4: Comparison of contributions of different features to relation detection across multiple domainsTable 4 compares the performance of ourmethod over different domains.
The table re-ports that the words features alone achieve thebest F1-score of 63.4 in ACE2004 but the low-est F1-score of 47.6 in AIMed.
This suggeststhe wide difference of lexical distribution be-tween these domains.
We extract the wordsappearing before the 1st mention, between thetwo mentions and after the 2nd mention fromthe training sets of these corpora respectively,and summarize the statistics (the number oftokens, the number of occurrences) in Table 5,where the KL divergence between positivesand negatives is summed over the distributionof the 500 most frequently occurring words.7 The performance results of F1 and AUC on the BioInfer corpus are slightly adjusted according to Table 3 in Miwa etal.
(2009b)Table 5: Lexical statistics on three corporaThe table shows that AIMed uses the mostkinds of words and the most words around thetwo mentions than the other two.
More impor-tant, AIMed has the least distribution differ-ence between the words appearing in positivesand negatives, as indicated by its least KLdivergence.
Therefore, the lexical words inAIMed are less discriminative for relationdetection than they do in the other two.
Thisnaturally explains the reason why the perform-ance by words feature alone isAIMed<ACE2003<ACE2004.
In addition,Table 4 also shows that:x The overlap features significantly improvethe performance in ACE while slightlydeteriorating that in AIMed.
The reason isthat, as indicated in Zhou et al (2005), mostof the positive relation instances in ACEexist in local contexts, while the positiveinteractions in AIMed occur in relativelong-range just as the negatives, thereforethese features are not discriminative forAIMed.Statistics AIMed ACE2003 ACE2004# of tokens 2,340 2,064 2,099# of occurrences 69,976 53,744 49,570KL divergence  0.22 0.28 0.33x The chunk features consistently greatlyboost the performance across multiple cor-pora.
This implies that the headwords inchunk phrases can well capture the partialnature of relation instances regardless oftheir genre.x It?s not surprising that the parse featureattain moderate performance gain in all do-mains since these parse paths are usually763long and specificity, leading to datasparseness problem.x It is interesting to note that the depend-ency-related features exhibit more signifi-cant improvement in AIMed than that inACE.
The reason may be that, thesedependency features can effectively cap-ture long-range relationships prevailing inAIMed, while in ACE a large number oflocal relationships dominate the corpora.5 Related WorkAmong feature-based methods, the PreBINDsystem (Donaldson et al, 2003) uses words andword bi-grams features to identify the existenceof protein interactions in abstracts and suchinformation is used to enhance manual expertreviewing for the BIND database.
Mitsumori etal.
(2006) use SVM to extract protein-proteininteractions, where bag-of-words features, spe-cifically the words around the protein names,are employed.
Sugiyama et al (2003) extractvarious features from the sentences based onthe verbs and nouns in the sentences such as theverbal forms, and the part-of-speech tags of the20 words surrounding the verb.
In addition toword features, Giuliano et al (2006) extractshallow linguistic information such as POS tag,lemma, and orthographic features of tokens forPPI extraction.
Unlike our dependency-drivenmethod, these systems do not consider anysyntactic information.For kernel-based methods, there are severalsystems which utilize dependency information.Erkan et al (2007) defines similarity functionsbased on cosine similarity and edit distancebetween dependency paths, and then incorpo-rate them in SVM and KNN learning for PPIextraction.
Airola et al (2008) introduceall-dependency-paths graph kernel to capturethe complex dependency relationships betweenlexical words and attain significant perform-ance boost at the expense of computationalcomplexity.
Kim et al (2010) adoptwalk-weighted subsequence kernel based ondependency paths to explore various substruc-tures such as e-walks, partial match, andnon-contiguous paths.
Essentially, their kernelis also a graph-based one.For composite kernel methods, S?tre et al(2007) combine a ?bag-of-words?
kernel withdependency and PAS (Predicate ArgumentStructure) tree kernels to exploit both the wordsfeatures and the structural syntactic information.Hereafter, Miyao et al (2008) investigate thecontribution of various syntactic features usingdifferent representations from dependencyparsing, phrase structure parsing and deepparsing by different parsers.
Miwa et al(2009a) integrate ?bag-of-words?
kernel, PAStree kernel and all-dependency-paths graphkernel to achieve the higher performance.
They(Miwa et al, 2009b) also use similar compos-ite kernels for corpus weighting learningacross multiple PPI corpora.6 Conclusion and Future WorkIn this paper, we have combined various lexicaland syntactic features, particularly dependencyinformation, into a feature-based PPI extractionsystem.
We find that the dependency informa-tion as well as the chunk features contributesmost to the performance improvement.
Thepredicate features involved in the dependencytree can also moderately enhance the perform-ance.
Furthermore, comparative study betweenbiomedical domain and the ACE newswiredomain shows that these domains exhibitdifferent lexical characteristics, rendering thetask of PPI extraction much more difficult thanthat of relation detection from the ACE cor-pora.In future work, we will explore more syntac-tic features such as PAS information for fea-ture-based PPI extraction to further boost theperformance.AcknowledgmentThis research is supported by Projects60873150 and 60970056 under the NationalNatural Science Foundation of China and Pro-ject BK2008160 under the Natural ScienceFoundation of Jiangsu, China.
We are also verygrateful to Dr. Antti Airola from TrukuUniversity for providing partial experimentalmaterials.ReferencesA.
Airola, S. Pyysalo, J. Bj?rne, T. Pahikkala, F.Ginter, and T. Salakoski.
2008.
All-paths graphkernel for protein-protein interaction extraction764with evaluation of cross corpus learning.
BMCBioinformatics.R.
Bunescu, R. Ge, R. Kate, E. Marcotte, R. Mooney,A.
Ramani, and Y. Wong.
2005.
ComparativeExperiments on learning information extractorsfor Proteins and their interactions.
Journal ofArtificial Intelligence In Medicine, 33(2).R.
Bunescu and R. Mooney.
2005.
Subsequencekernels for relation extraction.
In Proceedings ofNIPS?05, pages 171?178.A.
Culotta and J. Sorensen.
2004.
DependencyTree Kernels for Relation Extraction.
InProceedings of ACL?04.I.
Donaldson, J. Martin, B. de Bruijn, C. Wolting, V.Lay, B. Tuekam, S. Zhang, B. Baskin, G. D.Bader, K. Michalockova, T. Pawson, and C. W. V.Hogue.
2003.
Prebind and textomy - mining thebiomedical literature for protein-proteininteractions using a support vector machine.Journal of BMC Bioinformatics, 4(11).G.
Erkan, A.
?zg?r, and D.R.
Radev.
2007.Semi-Supervised Classification for ExtractingProtein Interaction Sentences using DependencyParsing, In Proceedings of EMNLP-CoNLL?07,pages 228?237.C.
Giuliano, A. Lavelli, and L. Romano.
2006.Exploiting Shallow Linguistic Information forRelation Extraction from Biomedical Literature.In Proceedings of EACL?06, pages 401?408.S.
Kim, J. Yoon, J. Yang, and S. Park.
2010.Walk-weighted subsequence kernels forprotein-protein interaction extraction.
Journal ofBMC Bioinformatics, 11(107).J.
Li, Z. Zhang, X. Li, and H. Chen.
2008.Kernel-Based Learning for Biomedical Relationextraction.
Journal of the American Society forInformation Science and Technology, 59(5).T.
Mitsumori, M. Murata, Y. Fukuda, K. Doi, and H.Doi.
2006.
Extracting protein-protein interactioninformation from biomedical text with SVM.IEICE Transactions on Information and System,E89-D (8).M.
Miwa, R. S?tre, Y. Miyao, and J. Tsujii.
2009a.Protein-Protein Interaction Extraction byLeveraging Multiple Kernels and Parsers.Journal of Medical Informatics, 78(2009).M.
Miwa, R. S?tre, Y. Miyao, and J. Tsujii.
2009b.A Rich Feature Vector for Protein-ProteinInteraction Extraction from Multiple Corpora.
InProceedings of EMNLP?09, pages 121?130.Y.
Miyao, R. S?tre, K. Sagae, T. Matsuzaki, andJ.Tsujii.
2008.
Task-oriented evaluation ofsyntactic parsers and their representations.
InProceedings of ACL?08, pages 46?54.T.
Ono, H. Hishigaki, A. Tanigami, and T. Takagi.2001.
Automated extraction of information onprotein-protein interactions from the biologicalliterature.
Journal of Bioinformatics, 17(2).K.
Sugiyama, K. Hatano, M. Yoshikawa, and S.Uemura.
2003.
Extracting information onprotein-protein interactions from biologicalliterature based on machine learning approaches.Journal of Genome Informatics, (14): 699?700.R.
S?tre, K. Sagae, and J. Tsujii.
2007.
Syntacticfeatures for protein-protein interaction extraction.In Proceedings of LBM?07, pages 6.1?6.14.A.
Yakushiji, M. Yusuke, T. Ohta, Y. Tateishi, J.Tsujii.
2006.
Automatic construction ofpredicate-argument structure patterns forbiomedical information extraction.
InProceedings of EMNLP?06, pages 284?292.S.B.
Zhao and R. Grishman.
2005.
ExtractingRelations with Integrated Information UsingKernel Methods.
In Proceedings of ACL?05,pages 419-426.G.D.
Zhou, J. Su, J. Zhang, and M. Zhang.
2005.Exploring various knowledge in relationextraction.
In Proceedings of ACL?05, pages427-434.765
