Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1?11, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsSyntactic Transfer Using a Bilingual LexiconGreg Durrett, Adam Pauls, and Dan KleinComputer Science DivisionUniversity of California, Berkeley{gdurrett,adpauls,klein}@cs.berkeley.eduAbstractWe consider the problem of using a bilingualdictionary to transfer lexico-syntactic infor-mation from a resource-rich source languageto a resource-poor target language.
In con-trast to past work that used bitexts to trans-fer analyses of specific sentences at the tokenlevel, we instead use features to transfer thebehavior of words at a type level.
In a dis-criminative dependency parsing framework,our approach produces gains across a rangeof target languages, using two different low-resource training methodologies (one weaklysupervised and one indirectly supervised) andtwo different dictionary sources (one manu-ally constructed and one automatically con-structed).1 IntroductionBuilding a high-performing parser for a languagewith no existing treebank is still an open problem.Methods that use no supervision at all (Klein andManning, 2004) or small amounts of manual su-pervision (Haghighi and Klein, 2006; Cohen andSmith, 2009; Naseem et al2010; Berg-Kirkpatrickand Klein, 2010) have been extensively studied, butstill do not perform well enough to be deployedin practice.
Projection of dependency links acrossaligned bitexts (Hwa et al2005; Ganchev et al2009; Smith and Eisner, 2009) gives better perfor-mance, but crucially depends on the existence oflarge, in-domain bitexts.
A more generally appli-cable class of methods exploits the notion of univer-sal part of speech tags (Petrov et al2011; Das and...   the    senators    demand    strict   new    ethics    rules   ...DT      NNS          VBP          JJ       JJ       NNS     NNSGewerkschaften     verlangen       Verzicht         auf       die     ReformNN                  VVFIN             NN          APPR    ART       NNUnions               demand     abandonment     on       the      reformFigure 1: Sentences in English and German both contain-ing words that mean ?demand.?
The fact that the Englishdemand takes nouns on its left and right indicates that theGerman verlangen should do the same, correctly suggest-ing attachments to Verzicht and Gewerkschaften.Petrov, 2011) to train parsers that can run on any lan-guage with no adaptation (McDonald et al2011)or unsupervised adaptation (Cohen et al2011).While these universal parsers currently constitutethe highest-performing methods for languages with-out treebanks, they are inherently limited by operat-ing at the coarse POS level, as lexical features arevital to supervised parsing models.In this work, we consider augmenting delexical-ized parsers by transferring syntactic informationthrough a bilingual lexicon at the word type level.These parsers are delexicalized in the sense that, al-though they receive target language words as input,their feature sets do not include indicators on thosewords.
This setting is appropriate when there is toolittle target language data to learn lexical features di-rectly.
Our main approach is to add features whichare lexical in the sense that they compute a functionof specific target language words, but are still un-1lexical in the sense that all lexical knowledge comesfrom the bilingual lexicon and training data in thesource language.Consider the example English and German sen-tences shown in Figure 1, and suppose that we wishto parse the German side without access to a Ger-man treebank.
A delexicalized parser operating atthe part of speech level does not have sufficient in-formation to make the correct decision about, for ex-ample, the choice of subcategorization frame for theverb verlangen.
However, demand, a possible En-glish translation of verlangen, takes a noun on itsleft and a noun on its right, an observation that in thiscase gives us the information we need.
We can firefeatures in our German parser on the attachmentsof Gewerkschaften and Verzicht to verlangen indi-cating that similar-looking attachments are attestedin English for an English translation of verlangen.This allows us to exploit fine-grained lexical cues tomake German parsing decisions even when we havelittle or no supervised German data; moreover, thissyntactic transfer is possible even in spite of the factthat demand and verlangen are not observed in par-allel context.Using type-level transfer through a dictionary inthis way allows us to decouple the lexico-syntacticprojection from the data conditions under which weare learning the parser.
After computing feature val-ues using source language resources and a bilinguallexicon, our model can be trained very simply us-ing any appropriate training method for a supervisedparser.
Furthermore, because the transfer mecha-nism is just a set of features over word types, we arefree to derive our bilingual lexicon either from bitextor from a manually-constructed dictionary, makingour method strictly more general than those of Mc-Donald et al2011) or Ta?ckstro?m et al2012), whorely centrally on bitext.
This flexibility is potentiallyuseful for resource-poor languages, where a human-curated bilingual lexicon may be broader in cover-age or more robust to noise than a small, domain-limited bitext.
Of course, it is an empirical questionwhether transferring type level information aboutword behavior is effective; we show that, indeed,this method compares favorably with other transfermechanisms used in past work.The actual syntactic information that we transferconsists of purely monolingual lexical attachmentstatistics computed on an annotated source languageresource.1 While the idea of using large-scale sum-mary statistics as parser features has been consid-ered previously (Koo et al2008; Bansal and Klein,2011; Zhou et al2011), doing so in a projection set-ting is novel and forces us to design features suitablefor projection through a bilingual lexicon.
Our fea-tures must also be flexible enough to provide benefiteven in the presence of cross-lingual syntactic dif-ferences and noise introduced by the bilingual dic-tionary.Under two different training conditions and withtwo different varieties of bilingual lexicons, weshow that our method of lexico-syntactic projectiondoes indeed improve the performance of parsers thatwould otherwise be agnostic to lexical information.In all settings, we see statistically significant gainsfor a range of languages, with our method providingup to 3% absolute improvement in unlabeled attach-ment score (UAS) and 11% relative error reduction.2 ModelThe projected lexical features that we propose in thiswork are based on lexicalized versions of featuresfound in MSTParser (McDonald et al2005), anedge-factored discriminative parser.
We take MST-Parser to be our underlying parsing model and use itas a testbed on which to evaluate the effectiveness ofour method for various data conditions.2 By instanti-ating the basic MSTParser features over coarse partsof speech, we construct a state-of-the-art delexical-ized parser in the style of McDonald et al2011),where feature weights can be directly transferredfrom a source language or languages to a desiredtarget language.
When we add projected lexical fea-tures on top of this baseline parser, we do so in away that does not sacrifice this generality: whileour new features take on values that are language-specific, they interact with the model at a language-independent level.
We therefore have the best of1Throughout this work, we will use English as the sourcelanguage, but it is possible to use any language for which theappropriate bilingual lexicons and treebanks exist.
One mightexpect to find the best performance from using a source lan-guage closely related to the target.2We train MSTParser using the included implementation ofMIRA (Crammer and Singer, 2001) and use projective decodingfor all experiments described in this paper.2DELEXFeature ValueVERB?NOUN 1VERB?NOUN, L 1???
??
?PROJQuery Feature (signature) Valueverlangen?NOUN [VERB]?CHILD 0.723verlangen?NOUN, L [VERB]?CHILD, DIR 0.711VERB?Gewerkschaften PARENT?
[NOUN] 0.822???
???
??
?Gewerkschaften     verlangen       Verzicht         auf       die     ReformNOUN               VERB           NOUN        ADP    DET     NOUNUnions              demand     abandonment     on       the      reformDELEXFeature ValueVERB?NOUN 1VERB?NOUN, R 1???
??
?PROJQuery Feature (signature) Valueverlangen?NOUN [VERB]?CHILD 0.723verlangen?NOUN, R [VERB]?CHILD, DIR 0.521VERB?Verzicht PARENT?
[NOUN] 0.623???
???
??
?Figure 2: Computation of features on a dependency arc.
DELEX features are indicators over characteristics of depen-dency links that do not involve the words in the sentence.
PROJ features are real-valued analogues of DELEX featuresthat do contain words.
We form a query from each stipulated set of characteristics, compute the values of these queriesheuristically, and then fire a feature based on each query?s signature.
Signatures indicate which attachment propertieswere considered, which part of the query was lexicalized (shown by brackets here), and the POS of the query word.This procedure yields a small number of real-valued features that still capture rich lexico-syntactic information.two worlds in that our features can be learned onany treebank or treebanks that are available to us,but still exploit highly specific lexical informationto achieve performance gains over using coarse POSfeatures alone.2.1 DELEX FeaturesOur DELEX feature set consists of all of the unlexi-calized features in MSTParser, only lightly modifiedto improve performance for our setting.
McDonaldet al2005) present three basic types of such fea-tures, ATTACH, INBETWEEN, and SURROUNDING,which we apply at the coarse POS level.
The AT-TACH features for a given dependency link consist ofindicators of the tags of the head and modifier, sep-arately as well as together.
The INBETWEEN andSURROUNDING features are indicators on the tagsof the head and modifier in addition to each inter-vening tag in turn (INBETWEEN) or various com-binations of tags adjacent to the head or modifier(SURROUNDING).3MSTParser by default also includes a copy ofeach of these indicator features conjoined withthe direction and distance of the attachment it de-notes.
These extra features are important to getting3As in Koo et al2008), our feature set contains morebacked-off versions of the SURROUNDING features than are de-scribed in McDonald et al2005).good performance out of the baseline model.
Weslightly modify the conjunction scheme and expandit with additional backed-off conjunctions, sincethese changes lead to features that empirically trans-fer better than the MSTParser defaults.
Specifically,we use conjunctions with attachment direction (leftor right), coarsened distance,4 and attachment direc-tion and coarsened distance combined.We emphasize again that these baseline featuresare entirely standard, and all the DELEX feature setdoes is recreate an MSTParser-based analogue of thedirect transfer parser described by McDonald et al(2011).2.2 PROJ FeaturesWe will now describe how to compute our projectedlexical features, the PROJ feature set, which con-stitutes the main contribution of this work.
Recallthat we wish our method to be as general as possibleand work under many different training conditions;in particular, we wish to be able to train our modelon only existing treebanks in other languages whenno target language trees are available (discussed inSection 3.3), or on only a very small target languagetreebank (Section 3.4).
It would greatly increasethe power of our model if we were able to includetarget-language-lexicalized versions of the ATTACH4Our five distance buckets are {1, 2, 3?5, 6?10, 11+}.3features, but these are not learnable without a largetarget language treebank.
We instead must augmentour baseline model with a relatively small number offeatures that are nonetheless rich enough to transferthe necessary lexical information.Our overall approach is sketched in Figure 2,where we show the features that fire on two pro-posed edges in a German dependency parse.
Fea-tures on an edge in MSTParser incorporate a sub-set of observable properties about that edge?s head,modifier, and context in the sentence.
For sets ofproperties that do not include a lexical item, suchas VERB?NOUN, we fire an indicator feature fromthe DELEX feature set.
For those that do include alexical item, such as verlangen?NOUN, we form aquery, which resembles a lexicalized indicator fea-ture.
Rather than firing the query as an indicatorfeature directly, which would result in a model pa-rameter for each target word, we fire a broad featurecalled an signature whose value reflects the specificsof the query (computation of these values is dis-cussed in Section 2.2.2).
For example, we abstractverlangen?NOUN to [VERB]?CHILD, with squarebrackets indicating the element that was lexicalized.Section 2.2.1 discusses this coarsening in more de-tail.
The signatures are agnostic to individual wordsand even the language being parsed, so they can belearned on small amounts of data or data from otherlanguages.Our signatures allow us to instantiate features atdifferent levels of granularity corresponding to thelevels of granularity in the DELEX feature set.
Whena small amount of target language data is present,the variety of signatures available to us means thatwe can learn language-specific transfer characteris-tics: for example, nouns tend to follow prepositionsin both French and English, but the ordering of ad-jectives with respect to nouns is different.
We alsohave the capability to train on languages other thanour target language, and while this is expected to beless effective, it can still teach us to exploit somesyntactic properties, such as similar verb attachmentconfigurations if we train on a group of SVO lan-guages distinct from a target SVO language.
There-fore, our feature set manages to provide the trainingprocedure with choices about how much syntacticinformation to transfer at the same time as it preventsoverfitting and provides language independence.2.2.1 Query and Signature TypesA query is a subset of the following pieces of in-formation about an edge: parent word, parent POS,child word, child POS, attachment direction, andbinned attachment distance.
It must contain exactlyone word.5 We experimented with properties fromINBETWEEN and SURROUNDING features as well,but found that these only helped under some circum-stances and could lead to overfitting.6A signature contains the following three pieces ofinformation:1.
The non-empty subset of attachment propertiesincluded in the query2.
Whether we have lexicalized on the parent orchild of the attachment, indicated by brackets3.
The part of speech of the included wordBecause either the parent or child POS is includedin the signature, there are three meaningful proper-ties to potentially condition on, of which we must se-lect a nonempty subset.
Some multiplication showsthat we have 7?
2?
13 = 182 total PROJ features.As an example, the queriesverlangen?
NOUNverlangen?
ADPsprechen?
NOUNall share the signature [VERB]?CHILD, butverlangen?
NOUN,RIGHTVerzicht?
ADPVERB ?
Verzichthave [VERB]?CHILD,DIR, [ADP]?CHILD, andPARENT?
[NOUN] as their signatures, respectively.The level of granularity for signatures is a param-eter that simply must be engineered.
We found somebenefit in actually instantiating two signatures forevery query, one as described above and one that5Bilexical features are possible in our framework, but we donot use them here, so for clarity we assume that each query hasone associated word.6One hypothesis is that features looking at the sentence con-text are more highly specialized to a given language, since theyexamine the parent, the child, and one or more other parts ofspeech or words.4?demand, DIRPARENT?demanddemandWord POS Dir DistthatADP R3saidVERB L7<root>ROOT L6senatorsNOUN L1rulesNOUN R4WeNOUN L1thatADP R1TheyNOUN L1concessionsNOUN R1fromADP R2ParentsChildrenDIRValueL0.66R0.33PARENTValueADP0.33VERB0.33ROOT0.33He   reports that   the   senators demand strict new ethics rules [...]PRON     VERB     ADP     DET       NOUN          VERB        ADJ     ADJ      NOUN  NOUN?
We   demand that these hostilities cease    ,        ?
said [...]PUNC   PRON       VERB      ADP     DET        NOUN        VERB   PUNC  PUNC   VERBThey  demand concessions  from   the  Israeli authorities    <root>PRON        VERB              NOUN            ADP      DET      ADJ           NOUN               ROOT??
?Figure 3: Computation of query values.
For each occurrence of a given source word, we tabulate the attachments ittakes part in (parents and children) and record their properties.
We then compute relative frequency counts for eachpossible query type to get source language scores, which will later be projected through the dictionary to obtain targetlanguage feature values.
Only two query types are shown here, but values are computed for many others as well.does not condition on the part of speech of the wordin the signature.
One can also imagine using morerefined signatures, but we found that this led to over-fitting in the small training scenarios under consid-eration.2.2.2 Query Value EstimationEach query is given a value according to a gener-ative heuristic that involves the source training dataand the probabilistic bilingual lexicon.7 For a par-ticular signature, a query can be written as a tu-ple (x1, x2, .
.
.
, wt) where wt is the target languagequery word and the xi are the values of the includedlanguage-independent attachment properties.
Thevalue this feature takes is given by a simple gener-ative model: we imagine generating the attachmentproperties xi given wt by first generating a source7Lexicons such as those produced by automatic aligners in-clude probabilities natively, but obviously human-created lexi-cons do not.
For these dictionaries, we simply assume that eachword translates with uniform probability into each of its pos-sible translations.
Tweaking this method did not substantiallychange performance.word ws from wt based on the bilingual lexicon,then jointly generating the xi conditioned on ws.Treating the choice of source translation as a latentvariable to be marginalized out, we havevalue = p(x1, x2, .
.
.
|wt)=?wsp(ws|wt)p(x1, x2, .
.
.
|ws)The first term of the sum comes directly from ourprobabilistic lexicon, and the second we can esti-mate using the maximum likelihood estimator overour source language training data:p(x1, x2, .
.
.
|ws) =c(x1, x2, .
.
.
, ws)c(ws)(1)where c(?)
denotes the count of an event in thesource language data.The final feature value is actually the logarithmof this computed value, with a small constant addedbefore the logarithm is taken to avoid zeroes.53 Experiments3.1 Data ConditionsBefore we describe the details of our experiments,we sketch the data conditions under which we eval-uate our method.
As described in Section 1, there isa continuum of lightly supervised parsing methodsfrom those that make no assumptions (beyond whatis directly encoded in the model), to those that usea small set of syntactic universals, to those that usetreebanks from resource-rich languages, and finallyto those that use both existing treebanks and bitexts.Our focus is on parsing when one does not haveaccess to a full-scale target language treebank, butone does have access to realistic auxiliary resources.The first variable we consider is whether we haveaccess to a small number of target language trees oronly pre-existing treebanks in a number of other lan-guages; while not our actual target language, theseother treebanks can still serve as a kind of proxy forlearning which features generally transfer useful in-formation (McDonald et al2011).
We notate theseconditions with the following shorthand:BANKS: Large treebanks in other target languagesSEED: Small treebank in the right target languagePrevious work on essentially unsupervised meth-ods has investigated using a small number of targetlanguage trees (Smith and Eisner, 2009), but the be-havior of supervised models under these conditionshas not been extensively studied.
We will see inSection 3.4 that with only 100 labeled trees, evenour baseline model can achieve performance equalto or better than that of the model of McDonald etal.
(2011).
A single linguist could plausibly anno-tate such a number of trees in a short amount of timefor a language of interest, so we believe that this isan important setting in which to show improvement,even for a method primarily intended to augment un-supervised parsing.In addition, we consider two different sources forour bilingual lexicon:AUTOMATIC: Extracted from bitextMANUAL: Constructed from human annotationsBoth bitexts and human-curated bilingual dictionar-ies are more widely available than complete tree-banks.
Bitexts can provide rich information aboutlexical correspondences in terms of how words areused in practice, but for resource-poor languages,parallel text may only be available in small quan-tities, or be domain-limited.
We show results of ourmethod on bilingual dictionaries derived from bothsources, in order to show that it is applicable under avariety of data conditions and can successfully takeadvantage of such resources as are available.3.2 DatasetsWe evaluate our method on a range of languagestaken from the CoNLL shared tasks on multilingualdependency parsing (Buchholz and Marsi, 2006;Nivre et al2007).
We make use of dependencytreebanks for Danish, German, Greek, Spanish, Ital-ian, Dutch, Portuguese, and Swedish, all from the2006 shared task.For our English resource, we use 500,000 En-glish newswire sentences from English Gigawordversion 3 (Graff et al2007), parsed with the Berke-ley Parser (Petrov et al2006) and converted to adependency treebank using the head rules of Collins(1999).8 Our English test set (used in Section 3.4)consists of the first 300 sentences of section 23 of thePenn treebank (Marcus et al1993), preprocessedin the same way.
Our model does not use gold fine-grained POS tags, but we do use coarse POS tagsdeterministically generated from the provided goldfine-grained tags in the style of Berg-Kirkpatrickand Klein (2010) using the mappings of Petrov etal.
(2011).9 Following McDonald et al2011), westrip punctuation from all treebanks for the results ofSection 3.3.
All results are given in terms of unla-beled attachment score (UAS), ignoring punctuationeven when it is present.We use the Europarl parallel corpus (Koehn,2005) as the bitext from which to extract the AUTO-MATIC bilingual lexicons.
For each target language,we produce one-to-one alignments on the English-target bitext by running the Berkeley Aligner (Lianget al2006) with five iterations of IBM Model 1 and8Results do not degrade much if one simply uses Sections 2-21 of the Penn treebank instead.
Coverage of rare words in thetreebank is less important when a given word must also appearin the bilingual lexicon as the translation of an observed Germanword in order to be useful.9Note that even in the absence of gold annotation, such tagscould be produced from bitext using the method of (Das andPetrov, 2011) or could be read off from a bilingual lexicon.6This work Past workMANUAL AUTOMATIC MPH11* TMU12**DELEX DELEX+PROJ ?
DELEX+PROJ ?
Multi-dir Multi-proj ?
No clusters X-lingual ?DA 41.3 43.0 1.67 ?
43.6 2.30 ?
48.9* 0.6* 36.7** 2.0**DE 58.5 58.7 0.20 59.5 0.94 ?
56.7* -0.1* 48.9** 1.8**EL 57.9 59.9 1.99 ?
60.5 2.55 ?
60.1* 5.0* 59.5** 3.5**ES 64.2 65.4 1.20 ?
65.7 1.52 ?
64.2* 0.3* 60.2** 2.7**IT 65.9 66.5 0.58 67.4 1.54 ?
64.1* 0.9* 64.6** 4.2**NL 57.0 57.5 0.52 58.8 1.88 ?
55.8* 9.9* 52.8** 1.5**PT 75.4 77.2 1.83 ?
78.7 3.29 ?
74.0* 1.6* 66.8** 4.2**SV 64.5 66.1 1.61 ?
66.9 2.34 ?
65.3* 2.7* 55.4** 1.5**AVG 60.6 61.8 1.20 62.6 2.05 61.1* 2.7* 55.6** 2.7**Table 1: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on a con-catenation of non-target-language treebanks (the BANKS setting).
Values reported are UAS for sentences of all lengthsin the standard CoNLL test sets, with punctuation removed from training and test sets.
Daggers indicate statisticalsignificance computed using bootstrap resampling; a single dagger indicates p < 0.1 and a double dagger indicatesp < 0.05.
We also include the baseline results of McDonald et al2011) and Ta?ckstro?m et al2012) and improve-ments from their best methods of using bitext and lexical information.
These results are not directly comparable toours, as indicated by * and **.
However, we still see that the performance of our type-level transfer method approachesthat of bitext-based methods, which require complex bilingual training for each new language.five iterations of the HMM aligner with agreementtraining.
Our lexicon is then read off based on rel-ative frequency counts of aligned instances of eachword in the bitext.We also use our method on bilingual dictionar-ies constructed in a more conventional way.
Forthis purpose, we scrape our MANUAL bilingual lex-icons from English Wiktionary (Wikimedia Founda-tion, 2012).
We mine entries for English words thatexplicitly have foreign translations listed as well aswords in each target language that have English def-initions.
We discard all translation entries wherethe English side is longer than one word, exceptfor constructions of the form ?to VERB?, where wemanually remove the ?to?
and allow the word to bedefined as the English infinitive.
Finally, becauseour method requires a dictionary with probabilityweights, we assume that each target language wordtranslates with uniform probability into any of thecandidates that we scrape.3.3 BANKSWe first evaluate our model under the BANKS datacondition.
Following the procedure from McDonaldet al2011), for each language, we train both ourDELEX and DELEX+PROJ features on a concate-nation of 2000 sentences from each other CoNLLtraining set, plus 2000 sentences from the PennTreebank.
Again, despite the values of our PROJqueries being sensitive to which language we arecurrently parsing, the signatures are language in-dependent, so discriminative training still makessense over such a combined treebank.
Training ourPROJ features on the non-English treebanks in thisconcatenation can be understood as trying to learnwhich lexico-syntactic properties transfer ?univer-sally,?
or at least transfer broadly within the familiesof languages we are considering.Table 1 shows the performance of the DELEX fea-ture set and the DELEX+PROJ feature set using bothAUTOMATIC and MANUAL bilingual lexicons.
Bothmethods provide positive gains across the board thatare statistically significant in the vast majority ofcases, though MANUAL is slightly less effective;we postpone until Section 4.1 the discussion of theshortcomings of the MANUAL lexicon.We include for reference the baseline results ofMcDonald et al2011) and Ta?ckstro?m et al2012)(multi-direct transfer and no clusters) and the im-provements from their best methods using lexi-cal information (multi-projected transfer and cross-lingual clusters).
We emphasize that these resultsare not directly comparable to our own, as wehave different training data (and even different train-ing languages) and use a different underlying pars-ing model (MSTParser instead of a transition-based7AUTOMATIC100 train trees 200 train trees 400 train treesDELEX DELEX+PROJ ?
DELEX DELEX+PROJ ?
DELEX DELEX+PROJ ?DA 67.2 69.5 2.32 ?
69.5 72.3 2.77 ?
71.4 74.6 3.16 ?DE 72.9 73.9 0.97 75.4 76.5 1.09 ?
77.3 78.5 1.25 ?EL 70.8 72.9 2.07 ?
72.6 74.9 2.30 ?
74.3 76.7 2.41 ?ES 72.5 73.0 0.46 74.1 75.4 1.29 ?
75.3 77.2 1.81 ?IT 73.3 75.4 2.13 ?
74.7 77.3 2.54 ?
76.0 78.7 2.74 ?NL 63.0 65.8 2.82 ?
64.7 67.6 2.86 ?
66.1 69.2 3.06 ?PT 78.1 79.5 1.45 ?
79.5 81.1 1.66 ?
80.7 82.4 1.63 ?SV 76.4 78.1 1.69 ?
78.1 80.2 2.02 ?
79.6 81.7 2.07 ?AVG 71.8 73.5 1.74 73.6 75.7 2.07 75.1 77.4 2.27EN 74.4 81.5 7.06 ?
76.6 83.0 6.35 ?
78.3 84.1 5.80 ?MANUALDA 67.2 68.1 0.88 69.5 70.9 1.44 ?
71.4 73.3 1.92 ?DE 72.9 73.4 0.44 75.4 76.2 0.77 77.3 78.4 1.12 ?EL 70.8 71.9 1.06 ?
72.6 74.1 1.48 ?
74.3 75.8 1.56 ?ES 72.5 71.9 -0.64 74.1 74.3 0.23 75.3 76.4 1.04 ?IT 73.3 74.3 1.01 ?
74.7 76.4 1.66 ?
76.0 78.0 2.01 ?NL 63.0 65.4 2.43 ?
64.7 67.5 2.76 ?
66.1 69.0 2.91 ?PT 78.1 78.2 0.13 79.5 80.1 0.62 80.7 81.5 0.82 ?SV 76.4 76.6 0.25 78.1 79.1 1.01 ?
79.6 81.0 1.40 ?AVG 71.8 72.5 0.70 73.6 74.8 1.25 75.1 76.7 1.60EN 74.4 81.5 7.06 ?
76.6 83.0 6.35 ?
78.3 84.1 5.80 ?Table 2: Evaluation of features derived from AUTOMATIC and MANUAL bilingual lexicons when trained on varioussmall numbers of target language trees (the SEED setting).
Values reported are UAS for sentences of all lengths onour enlarged CoNLL test sets (see text); each value is based on 50 sampled training sets of the given size.
Daggersindicate statistical significance as described in the text.
Statistical significance is not reported for averages.parser (Nivre, 2008)).
However, our baseline is com-petitive with theirs,10 demonstrating that we haveconstructed a state-of-the-art delexicalized parser.Furthermore, our method appears to approach theperformance of previous bitext-based methods, andbecause of its flexibility and the freedom from com-plex cross-lingual training for each new language, itcan be applied in the MANUAL case as well, a capa-bility which neither of the other methods has.3.4 SEEDWe now turn our attention to the SEED scenario,where a small number of target language trees areavailable for each language we consider.
While itis imaginable to continue to exploit the other tree-banks in the presence of target language trees, wefound that training our DELEX features on the seedtreebank alone gave higher performance than any10The baseline of Ta?ckstro?m et al2012) is lower because itis trained only on English rather than on many languages.attempt to also use the concatenation of treebanksfrom the previous section.
This is not too surpris-ing because, with this number of sentences, there isalready good monolingual coverage of coarse POSfeatures, and attempting to train features on otherlanguages can be expected to introduce noise intootherwise accurate monolingual feature weights.We train our DELEX+PROJ model with both AU-TOMATIC and MANUAL lexicons on target languagetraining sets of size 100, 200, and 400, and give re-sults for each language in Table 2.
The performanceof parsers trained on small numbers of trees canbe highly variable, so we create multiple treebanksof each size by repeatedly sampling from each lan-guage?s train treebank, and report averaged results.Furthermore, this evaluation is not on the standardCoNLL test sets, but is instead on those test sets witha few hundred unused training sentences added, thereason being that some of the CoNLL test sets arevery small (fewer than 200 sentences) and appeared8to give highly variable results.
To compute statisticalsignificance, we draw a large number of bootstrapsamples for each training set used, then aggregate allof their sufficient statistics in order to compute the fi-nal p-value.
We see that our DELEX+PROJ methodgives statistically significant gains at the 95% levelover DELEX for nearly all language and training setsize pairs, giving on average a 9% relative error re-duction in the 400-tree case.Because our features are relatively few in numberand capture heuristic information, one question wemight ask is how well they can perform in a non-projection context.
In the last line of the table, wereport gains that are achieved when PROJ featurescomputed from parsed Gigaword are used directlyon English, with no intermediate dictionary.
Theseare not comparable to the other values in the tablebecause we are using our projection strategy mono-lingually, which removes the barriers of imperfectlexical correspondence (from using the lexicon) andimperfect syntactic correspondence (from project-ing).
As one might expect, the gains on English arefar higher than the gains on other languages.
Thisindicates that performance is chiefly limited by theneed to do cross-lingual feature adaptation, not in-herently low feature capacity.
We delay further dis-cussion to Section 4.2.One surprising thing to note is that the gains givenby our PROJ features are in some cases larger herethan in the BANKS setting.
This result is slightlycounterintuitive, as our baseline parsers are muchbetter in this case and so we would expect dimin-ished returns from our method.
We conclude that ac-curately learning which signatures transfer betweenlanguages is important, and it is easier to learn goodfeature weights when some target language data isavailable.
Further evidence supporting this hypothe-sis is the fact that the gains are larger and more sig-nificant on larger training set sizes.4 Discussion4.1 AUTOMATIC versus MANUALOverall, we see that gains from using our MANUALlexicons are slightly lower than those from our AU-TOMATIC lexicons.
One might expect higher per-formance because scraped bilingual lexicons are notprone to some of the same noise that exists in auto-AUTOMATIC MANUALVoc OCC Voc OCCDA 324K 0.91 22K 0.64DE 320K 0.89 58K 0.55EL 196K 0.94 23K 0.43ES 165K 0.89 206K 0.74IT 158K 0.91 78K 0.65NL 251K 0.87 50K 0.72PT 165K 0.85 46K 0.53SV 307K 0.93 28K 0.60Table 3: Lexicon statistics for all languages for bothsources of bilingual lexicons.
?Voc?
indicates vocabularysize and ?OCC?
indicates open-class coverage, the frac-tion of open-class tokens in the test treebanks with entriesin our bilingual lexicon.matic aligners, but this is empirically not the case.Rather, as we see in Table 3, the low recall of ourMANUAL lexicons on open-class words appears tobe a possible culprit.
The coverage gap betweenthese and the AUTOMATIC lexicons is partially dueto the inconsistent structure of Wiktionary: inflectedGerman and Greek words often do not have theirown pages, so we miss even common morphologi-cal variants of verb forms in those languages.
Theinflected forms that we do scrape are also mappedto the English base form rather than the correspond-ing inflected form in English, which introduces fur-ther noise.
Coverage is substantially higher if wetranslate using stems only, but this did not empir-ically lead to performance improvements, possiblydue to conflating different parts of speech with thesame base form.One might hypothesize that our uniform weight-ing scheme in the MANUAL lexicon is anothersource of problems, and that bitext-derived weightsare necessary to get high performance.
This is notthe case here.
Truncating the AUTOMATIC dictio-nary to at most 20 translations per word and settingthe weights uniformly causes a slight performancedrop, but is still better than our MANUAL lexicon.This further demonstrates that these problems aremore a limitation of our dictionary than our method.English Wiktionary is not designed to be a bilingualdictionary, and while it conveniently provided aneasy way for us to produce lexicons for a wide array9Frauen    wollen    weiter     f?r       die     Quote  k?mpfenNN     VMFIN    ADV    APPR   ART      NN    VVINFWomen     want     further     for       the     quota     fightWomen    want    to   continue   to    fight   for   the   quotaNNP      VBP   TO      VB      TO    VB    IN   DT    NNFigure 4: Example of a German tree and a parallel En-glish sentence with high levels of syntactic divergence.The English verb want takes fundamentally different chil-dren than wollen does, so properties of the sort we presentin Section 2.2 will not transfer effectively.of languages, it is not the resource that one wouldchoose if designing a parser for a specific target lan-guage.
Bitext is not necessary for our approach towork, and results on the AUTOMATIC lexicon sug-gest that our type-level transfer method can in factdo much better given a higher quality resource.4.2 LimitationsWhile our method does provide consistent gainsacross a range of languages, the injection of lexicalinformation is clearly not sufficient to bridge the gapbetween unsupervised and supervised parsers.
Weargued in Section 3.4 that the cross-lingual transferstep of our method imposes a fundamental limitationon how useful any such approach can be, which wenow investigate further.In particular, any syntactic divergence, especiallyinconsistent divergences like head switching, willlimit the utility of transferred structure.
Considerthe German example in Figure 4, with a parallel En-glish sentence provided.
The English tree suggeststhat want should attach to an infinitival to, which hasno correlate in German.
Even disregarding this, itsgrandchild is the verb continue, which is realized inthe German sentence as the adverb weiter.
Whileit is still broadly true that want and wollen bothhave verbal elements located to their right, it is lessclear how to design features that can still take advan-tage of this while working around the differences wehave described.
Therefore, a gap between the per-formance of our features on English and the perfor-mance of our projected features, as is observed inTable 2, is to be expected in the absence of a morecomplete model of syntactic divergence.5 ConclusionIn this work, we showed that lexical attachment pref-erences can be projected to a target language at thetype level using only a bilingual lexicon, improvingover a delexicalized baseline parser.
This methodis broadly applicable in the presence or absenceof target language training trees and with bilinguallexicons derived from either manually-annotated re-sources or bitexts.
The greatest improvements arisewhen the bilingual lexicon has high coverage and anumber of target language trees are available in or-der to learn exactly what lexico-syntactic propertiestransfer from the source language.In addition, we showed that a well-tuned discrim-inative model with the correct features can achievegood performance even on very small training sets.While unsupervised and existing projection meth-ods do feature great versatility and may yet pro-duce state-of-the-art parsers on resource-poor lan-guages, spending time constructing small supervisedresources appears to be the fastest method to achievehigh performance in these settings.AcknowledgmentsThis work was partially supported by an NSF Grad-uate Research Fellowship to the first author, by aGoogle Fellowship to the second author, and by theNSF under grant 0643742.
Thanks to the anony-mous reviewers for their insightful comments.ReferencesMohit Bansal and Dan Klein.
2011.
Web-scale Featuresfor Full-scale Parsing.
In Proceedings of ACL, pages693?702, Portland, Oregon, USA.Taylor Berg-Kirkpatrick and Dan Klein.
2010.
Phylo-genetic Grammar Induction.
In Proceedings of ACL,pages 1288?1297, Uppsala, Sweden.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.
InProceedings of CoNLL, pages 149?164.Shay B. Cohen and Noah A. Smith.
2009.
Shared Logis-tic Normal Distributions for Soft Parameter Tying in10Unsupervised Grammar Induction.
In Proceedings ofNAACL, pages 74?82, Boulder, Colorado.Shay B. Cohen, Dipanjan Das, and Noah A. Smith.
2011.Unsupervised Structure Prediction with Non-ParallelMultilingual Guidance.
In Proceedings of EMNLP,pages 50?61, Edinburgh, UK.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Koby Crammer and Yoram Singer.
2001.
Ultraconserva-tive Online Algorithms for Multiclass Problems.
Jour-nal of Machine Learning Research, 3:2003.Dipanjan Das and Slav Petrov.
2011.
Unsupervised Part-of-Speech Tagging with Bilingual Graph-Based Pro-jections.
In Proceedings of ACL, pages 600?609, Port-land, Oregon, USA.Kuzman Ganchev, Jennifer Gillenwater, and Ben Taskar.2009.
Dependency Grammar Induction via Bitext Pro-jection Constraints.
In Proceedings of ACL, pages369?377, Suntec, Singapore.David Graff, Junbo Kong, Ke Chen, and Kazuaki Maeda.2007.
English Gigaword Third Edition.
LinguisticData Consortium, Catalog Number LDC2007T07.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenGrammar Induction.
In Proceedings of CoLING-ACL,pages 881?888, Sydney, Australia.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
BootstrappingParsers via Syntactic Projection Across Parallel Texts.Natural Language Engineering, 11:311?325, Septem-ber.Dan Klein and Christopher D. Manning.
2004.
Corpus-Based Induction of Syntactic Structure: Models of De-pendency and Constituency.
In Proceedings of ACL,pages 479?486.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In MT Summit X,pages 79?86, Phuket, Thailand.
AAMT.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple Semi-Supervised Dependency Parsing.
In Pro-ceedings of ACL.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by Agreement.
In Proceedings of NAACL, NewYork, New York.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a Large Annotated Cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19:313?330, June.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005.
Online Large-Margin Training of DependencyParsers.
In Proceedings of ACL, pages 91?98, AnnArbor, Michigan.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.Multi-Source Transfer of Delexicalized DependencyParsers.
In Proceedings of EMNLP, pages 62?72, Ed-inburgh, Scotland, UK.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using Universal Linguistic Knowl-edge to Guide Grammar Induction.
In Proceed-ings of EMNLP, pages 1234?1244, Cambridge, Mas-sachusetts.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mcdon-ald, Jens Nilsson, Sebastian Riedel, and Deniz Yuret.2007.
The CoNLL 2007 Shared Task on DependencyParsing.
In Proceedings of EMNLP-CoNLL, pages915?932, Prague, Czech Republic.Joakim Nivre.
2008.
Algorithms for Deterministic Incre-mental Dependency Parsing.
Computational Linguis-tics, 34:513?553, December.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and In-terpretable Tree Annotation.
In Proceedings of ACL,pages 433?440, Sydney, Australia.Slav Petrov, Dipanjan Das, and Ryan McDonald.
2011.A Universal Part-of-Speech Tagset.
In ArXiv, April.David A. Smith and Jason Eisner.
2009.
Parser Adapta-tion and Projection with Quasi-Synchronous GrammarFeatures.
In Proceedings of EMNLP, pages 822?831,Suntec, Singapore.Oscar Ta?ckstro?m, Ryan McDonald, and Jakob Uszkoreit.2012.
Cross-lingual Word Clusters for Direct Trans-fer of Linguistic Structure.
In Proceedings of NAACL,Montreal, Canada.Wikimedia Foundation.
2012.
Wiktionary.
Online athttp://www.wiktionary.org/.Guangyou Zhou, Jun Zhao, Kang Liu, and Li Cai.
2011.Exploiting Web-Derived Selectional Preference to Im-prove Statistical Dependency Parsing.
In Proceedingsof ACL, pages 1556?1565, Portland, Oregon, USA.11
