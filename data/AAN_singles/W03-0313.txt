Translation Spotting for Translation MemoriesMichel SimardLaboratoire de recherche applique?e en linguistique informatique (RALI)De?partement d?informatique et de recherche ope?rationnelleUniversite?
de Montre?alC.P.
6128, succursale Centre-ville, Local 2241Montre?al (Que?bec), Canada H3C 3J7simardm@iro.umontreal.caAbstractThe term translation spotting (TS) refers tothe task of identifying the target-language (TL)words that correspond to a given set of source-language (SL) words in a pair of text segmentsknown to be mutual translations.
This arti-cle examines this task within the context of asub-sentential translation-memory system, i.e.a translation support tool capable of proposingtranslations for portions of a SL sentence, ex-tracted from an archive of existing translations.Different methods are proposed, based on a sta-tistical translation model.
These methods takeadvantage of certain characteristics of the ap-plication, to produce TL segments submittedto constraints of contiguity and composition-ality.
Experiments show that imposing theseconstraints allows important gains in accuracy,with regard to the most probable alignmentspredicted by the model.1 IntroductionTranslation spotting is the term coined by Ve?ronis andLanglais (2000) for the task of identifying the word-tokens in a target-language (TL) translation that corre-spond to some given word-tokens in a source-language(SL) text.
Translation spotting (TS) takes as input a cou-ple, i.e.
a pair of SL and TL text segments, which areknown to be translations of one another, and a SL query,i.e.
a subset of the tokens of the SL segment, on which theTS will focus its attention.
The result of the TS processconsists of two sets of tokens, i.e.
one for each language.We call these sets the SL and TL answers to the query.In more formal terms:?
The input to the TS process is a pair of SL and TLtext segments ?S, T ?, and a contiguous, non-emptysequence of word-tokens in S, q = si1 ...si2 (thequery).?
The output is a pair of sets of tokens ?rq(S), rq(T )?,the SL answer and TL answer respectively.Figure 1 shows some examples of TS, where the wordsin italics represent the SL query, and the words in bold arethe SL and TL answers.As can be seen in these examples, the tokens in thequery q and answers rq(S) and rq(T ) may or may not becontiguous (examples 2 and 3), and the TL answer maypossibly be empty (example 4) when there is no satisfyingway of linking TL tokens to the query.Translation spotting finds different applications, forexample in bilingual concordancers, such as theTransSearch system (Macklovitch et al, 2000), andexample-based machine translation (Brown, 1996).
Inthis article, we focus on a different application: a sub-sentential translation memory.
We describe this applica-tion context in section 2, and discuss how TS fits in to thistype of system.
We then propose in section 3 a series ofTS methods, specifically adapted to this application con-text.
In section 4, we present an empirical evaluation ofthe proposed methods.2 Sub-sentential Translation MemorySystemsA translation memory system is a type of translation sup-port tool whose purpose is to avoid the re-translation ofsegments of text for which a translation has previouslybeen produced.
Typically, these systems are integratedto a word-processing environment.
Every sentence thatthe user translates within this environment is stored in adatabase (the translation memory ?
or TM).
Wheneverthe system encounters some new text that matches a sen-tence in the TM, its translation is retrieved and proposedto the translator for reuse.Sentence PairQuery SL (English) TL (French)1. and a growing gap Is this our model of the future, regionaldisparity and a growing gap betweenrich and poor?Est ce la` le mode`le que nous visons,soit la disparite?
re?gionale et un fosse?
deplus en plus large entre les riches et lespauvres?2.
the government?s com-mitmentThe government?s commitment waslaid out in the 1994 white paper.Le gouvernement a expose?
ses en-gagements dans le livre blanc de 1994.3. close to [...] years I have been fortunate to have been trav-elling for close to 40 years.J?ai eu la chance de voyager pendantpre`s de 40 ans .4. to the extent that To the extent that the Canadian govern-ment could be open, it has been so.Le gouvernement canadien a e?te?
aussiouvert qu?il le pouvait.Figure 1: Translation spotting examplesAs suggested in the above paragraph, existing systemsessentially operate at the level of sentences: the TM istypically made up of pairs of sentences, and the system?sproposals consist in translations of complete sentences.Because the repetition of complete sentences is an ex-tremely rare phenomenon in general language, this levelof resolution limits the usability of TM?s to very spe-cific application domains ?
most notably the translationof revised or intrinsically repetitive documents.
In lightof these limitations, some proposals have recently beenmade regarding the possibility of building TM systemsthat operate ?below?
the sentence level, or sub-sententialtranslation memories (SSTM) ?
see for example (Lange?et al, 1997; McTait et al, 1999).Putting together this type of system raises the prob-lem of automatically establishing correspondences be-tween arbitrary sequences of words in the TM, or, in otherwords, of ?spotting translations?.
This process (transla-tion spotting) can be viewed as a by-product of word-alignment, i.e.
the problem of establishing correspon-dences between the words of a text and those of its trans-lation: obviously, given a complete alignment betweenthe words of the SL and TL texts, we can extract onlythat part of the alignment that concerns the TS query;conversely, TS may be seen as a sub-task of the word-alignment problem: a complete word-alignment can beobtained by combining the results of a series of TS oper-ations, covering the entirety of the SL text.From the point of view of an SSTM application, theTS mechanism should find the TL segments that are themost likely to be useful to the translator in producing thetranslation of a given SL sentence.
In the end, the finalcriterion by which a SSTM will be judged is profitability:to what extent do the system?s proposals enable the userto save time and/or effort in producing a new translation.From that perspective, the two most important charac-teristics of the TL answers are relevance, i.e.
whetheror not the system?s TL proposals constitute valid trans-lations for some part of the source sentence; and co-herence, i.e.
whether the proposed segments are well-formed, at least from a syntactic point of view.
As sug-gested by McTait et al (1999), ?linguistically motivated?sub-sentential entities are more likely than arbitrary se-quences of words to lead to useful proposals for the user.Planas (2000) proposes a fairly simple approach for anSSTM: his system would operate on sequences of syntac-tic chunks, as defined by Abney (1991).
Both the contentsof the TM and the new text under consideration wouldbe segmented into chunks; sequences of chunks from thenew text would then be looked up verbatim in the TM;the translation of the matched sequences would be pro-posed to the user as partial translations of the current in-put.
Planas?s case for using sequences of chunks as theunit of translation for SSTM?s is supported by the coher-ence criterion above: chunks constitute ?natural?
textualunits, which users should find easier to grasp and reusethan arbitrary sequences.The coherence criterion also supports the case for con-tiguous TL proposals, i.e.
proposals that take the formof contiguous sequences of tokens from the TM, as op-posed to discontiguous sets such as those of examples 2and 3, in figure 1.
This also makes intuitive sense fromthe more general point of view of profitability: manually?filling holes?
within a discontiguous proposal is likely tobe time-consuming and counter-productive.
On the otherhand, filling those holes automatically, as proposed forexample by Lange?
et al and McTait et al, raises numer-ous problems with regard to syntactic and semantic well-formedness of the TL proposals.
In theory, contiguoussequences of token from the TM should not suffer fromsuch ills.Finally, and perhaps more importantly, in a SSTM ap-plication such as that proposed by Planas, there appearsto be statistical argument in favor of contiguous TL pro-posals: the more frequent a contiguous SL sequences, themore likely it is that its TL equivalent is also contiguous.In other words, there appears to be a natural tendencyfor frequently-occurring phrases and formulations to cor-respond to like-structured sequences in other languages.This will be discussed further in section 4.
But clearly,a TS mechanism intended for such a SSTM should takeadvantage of this tendency.3 TS MethodsIn this section, we propose various TS methods, specif-ically adapted to a SSTM application such as that pro-posed by Planas (2000), i.e.
one which takes as transla-tion unit contiguous sequences of syntactic chunks.3.1 Viterbi TSAs mentioned earlier, TS can be seen as a bi-product ofword-level alignments.
Such alignments have been thefocus of much attention in recent years, especially in thefield of statistical translation modeling, where they playan important role in the learning process.For the purpose of statistical translation modeling,Brown et al (1993) define an alignment as a vector a =a1...am that connects each word of a source-languagetext S = s1...sm to a target-language word in its transla-tion T = t1...tn, with the interpretation that word taj isthe translation of word sj in S (aj = 0 is used to denotewords of s that do not produce anything in T ).Brown et al also define the Viterbi alignment betweensource and target sentences S and T as the alignmenta?
whose probability is maximal under some translationmodel:a?
= argmaxa?APrM(a|S, T )where A is the set of all possible alignments between Sand T , and PrM(a|S, T ) is the estimate of a?s probabil-ity under model M, which we denote Pr(a|S, T ) fromhereon.
In general, the size of A grows exponentiallywith the sizes of S and T , and so there is no efficient wayof computing a?
efficiently.
However, under Model 2, theprobability of an alignment a is given by:Pr(a|S, T ) =m?i=1Pr(ai|i,m, n) (1)wherePr(j|i,m, n) =?
(j, i,m, n)?nJ=0 ?
(J, i,m, n), (2)and?
(j, i,m, n) = t(si|tj)a(j, i,m, n)In this last equation, t(si|tj) is the model?s estimate ofthe ?lexical?
distribution p(si|tj), while a(j, i,m, n) es-timates the ?alignment?
distribution p(j|i,m, n).
There-fore, with this model, the Viterbi alignment can be ob-tained by simply picking for each position i in S, thealignment that maximizes t(si|tj)a(j, i,m, n).
This pro-cedure can trivially be carried out in O(mn) operations.Because of this convenient property, we base the rest ofthis work on this model.Adapting this procedure to the TS task is straightfor-ward: given the TS query q, produce as TL answer thecorresponding set of TL tokens in the Viterbi alignment:rq(T ) = {ta?i1 , ..., ta?i2} (the SL answer is simply q it-self).
We call this method Viterbi TS: it corresponds tothe most likely alignment between the query q and TLtext T , given the probability estimates of the translationmodel.
If q contains I tokens, the Model 2 Viterbi TScan be computed in O(In) operations.
Figure 2 showsan example of the result of this process.query : the government ?s commitmentcouple:S = Let us see wherethe government?s commit-ment is really at in terms ofthe farm community.T = Voyons quel est leve?ritable engagement dugouvernement envers lacommunaute?
agricole.Viterbi alignment on query tokens:the ?
legovernment ?
gouvernement?s ?
ducommitment ?
engagementTL answer:T = Voyons quel est le ve?ritable engagement du gou-vernement envers la communaute?
agricole.Figure 2: Viterbi TS example3.2 Post-processingsThe tokens of the TL answer produced by Viterbi TS arenot necessarily contiguous in T which, as remarked ear-lier, is problematic in a TM application.
Various a poste-riori processings on rq(T ) are possible to fix this; we listhere only the most obvious:expansion : Take the minimum and maximum val-ues in {a?i1 , ..., a?i2}, and produce the sequencetmin ai ...tmax ai ; in other words, produce as TL an-swer the smallest contiguous sequence in T that con-tains all the tokens of rq(T ).longest-sequence : Produce the subset of rq(T ) thatconstitutes the longest contiguous sequence in T .zero-tolerance : If the tokens in rq(T ) cannot be ar-ranged in a contiguous sequence of T , then simplydiscard the whole TL answer.Figure 3 illustrates how these three strategies affect theViterbi TS of figure 2.3.3 Contiguous TSThe various independence assumptions underpinningIBM Model 2 often have negative effects on the result-ing Viterbi alignments.
In particular, this model assumesrq(T ) = {le, engagement, du, gouvernement}post-processing:expansion : X(rq(T )) = le ve?ritable engagement du gouvernementlongest-sequence : L(rq(T )) = engagement du gouvernementzero-tolerance : Z(rq(T )) = ?Figure 3: Post-processings on Viterbi TSthat all connections within an alignment are indepen-dent of each other, which leads to numerous aberrationsin the alignments.
Typically, each SL token gets con-nected to the TL token with which it has the most ?lex-ical affinities?, regardless of other existing connectionsin the alignment and, more importantly, of the relation-ships this token holds with other SL tokens in its vicinity.Conversely, some TL tokens end up being connected toseveral SL tokens, while other TL tokens are left uncon-nected.As mentioned in section 2, in a sub-sentential TM ap-plication, contiguous sequences of tokens in the SL tendto translate into contiguous sequences in the TL.
Thissuggests that it might be a good idea to integrate a ?con-tiguity constraint?
right into the alignment search proce-dure.For example, we can formulate a variant of the ViterbiTS method above, which looks for the alignment thatmaximizes Pr(a|S, T ), under the constraint that the TLtokens aligned with the SL query must be contiguous.Consider a procedure that seeks the (possibly null) se-quence tj1 ...tj2 of T , that maximizes:Pr(aq|si2i1 , tj2j1)Pr(aq?|si1?11 smi2+1, tj1?11 tnj2+1)Such a procedure actually produces two distinct align-ments over S and T : an alignment aq, which connects thequery tokens (the sequence si2i1) with a sequence of con-tiguous tokens in T (the sequence tj2j1), and an alignmentaq?, which connects the rest of sentence S (i.e.
all the to-kens outside the query) with the rest of T .
Together, thesetwo alignments constitute the alignment a = aq ?
aq?,whose probability is maximal, under a double constraint:1. the query tokens si2i1 can only be connected to tokenswithin a contiguous region of T (the sequences tj2j1);2. the tokens outside the query (in either one of the twosequences si1?11 and smi2+1) can only get connectedto tokens outside tj2j1 .With such an alignment procedure, we can trivially de-vise a TS method, which will return the optimal tj2j1 as TLanswer.
We call this method Contiguous TS.
Alignmentssatisfying the above constraints can be obtained directly,by computing Viterbi alignments aq and aq?
for each pairof target positions ?j1, j2?.
The TS procedure then re-tains the pair of TL language positions that maximizesthe joint probability of alignments aq and aq?.
This oper-ation requires the computation of two Viterbi alignmentsfor each pair ?j1, j2?, i.e.
n(n ?
1) Viterbi alignments,plus a ?null?
alignment, corresponding to the situationwhere tj2j1 = ?.
Overall, using IBM Model 2, the oper-ation requires O(mn3) operations.
Figure 4 illustrates acontiguous TS obtained on the example of figure 2.Alignment: Let us see ?
Voyonswhere ?
quelaq =the ?
engagementgovernment ?
gouvernement?s ?
ducommitment ?
engagementis ?
estreally ?
ve?ritableat ?
lain terms of ?
enversthe ?
lafarm ?
agricolecommunity ?
communaute?.
?
.TL answer:T = Voyons quel est le ve?ritable engagement du gou-vernement envers la communaute?
agricole.Figure 4: Contiguous TS Example3.4 Compositional TSAs pointed out in section 3.3, In IBM-style alignments,a single TL token can be connected to several SL to-kens, which sometimes leads to aberrations.
This con-trasts with alternative alignment models such as thoseof Melamed (1998) and Wu (1997), which impose a?one-to-one?
constraint on alignments.
Such a constraintevokes the notion of compositionality in translation: itsuggests that each SL token operates independently inthe SL sentence to produce a single TL token in theTL sentence, which then depends on no other SL token.This view is, of course, extreme, and real-life translationsare full of examples (idiomatic expressions, terminology,paraphrasing, etc.)
that show how this compositionalityprinciple breaks down as we approach the level of wordcorrespondences.However, in a TM application, TS usually needs not godown to the level of individual words.
Therefore, compo-sitionality can often be assumed to apply, at least to thelevel of the TS query.
The contiguous TS method pro-posed in the previous section implicitly made such an as-sumption.
Here, we push it a little further.Consider a procedure that splits each the source andtarget sentences S and T into two independent parts, insuch a way as to maximise the probability of the two re-sulting Viterbi alignments:argmax?i,j,d???????
?d = 1 : Pr(a1|si1, tj1)?Pr(a2|smi+1, tnj+1)d = ?1 : Pr(a1|si1, tnj+1)?Pr(a2|smi+1, tj1)In the triple ?i, j, d?
above, i represents a ?split point?in the SL sentence S, j is the analog for TL sentence T ,and d is the ?direction of correspondence?
: d = 1 denotesa ?parallel correspondence?, i.e.
s1...si corresponds tot1...tj and si+1...sm corresponds to tj+1...tn; d = ?1denotes a ?crossing correspondence?, i.e.
s1...si corre-sponds to tj+1...tn and si+1...sm corresponds to t1...tj .The triple ?I, J,D?
produced by this procedure refersto the most probable alignment between S and T , un-der the hypothesis that both sentences are made up oftwo independent parts (s1...sI and sI+1...sm on the onehand, t1...tJ and tJ+1...tn on the other), that correspondto each other two-by-two, following direction D. Suchan alignment suggests that translation T was obtainedby ?composing?
the translation of s1...sI with that ofsI+1...sm.This ?splitting?
process can be repeated recursively oneach pair of matching segments, down to the point whereeach SL segment contains a single token.
(TL segmentscan always be split, even when empty, because IBM-stylealignments make it possible to connect SL tokens to the?null?
TL token, which is always available.)
This givesrise to a word-alignment procedure that we call Compo-sitional word alignment.This procedure actually produces two different out-puts: first, a parallel partition of S and T into m pairs ofsegments ?si, tkj ?, where each tkj is a (possibly null) con-tiguous sub-sequence of T ; second, an IBM-style align-ment, such that each SL and TL token is linked to at mostone token in the other language: this alignment is actuallythe concatenation of individual Viterbi alignments on the?si, tkj ?
pairs, which connects each si to (at most) one ofthe tokens in the corresponding tkj .Of course, such alignments face even worst problemsthan ordinary IBM-style alignments when confrontedwith non-compositional translations.
However, whenadapting this procedure to the TS task, we can hypoth-esize that compositionality applies, at least to the level ofthe SL query.
This adaptation proceeds along the follow-ing modifications to the alignment procedure describedabove:1. forbid splittings within the SL query: i1 ?
i ?
i2;2. at each level of recursion, only consider that pair ofsegments which contains the SL query;3. stop the procedure as soon as it is no longer possibleto split the SL segment, i.e.
it consists of si1 ...si2 .The TL segment matched with si1 ...si2 when the proce-dure terminates is the TL answer.
We call this proce-dure Compositional TS.
It can be shown that it can becarried out in O(m3n2) operations in the worst case, andO(m2n2 logm) on average.
Furthermore, by limiting thesearch to split points yielding matching segments of com-parable sizes, the number of required operations can becut by one order of magnitude (Simard, 2003).Figure 5 shows how this procedure splits the examplepair of figure 2 (the query is shown in italics).4 EvaluationWe describe here a series of experiments that were car-ried out to evaluate the performance of the TS methodsdescribed in section 3.
We essentially identified a num-ber of SL queries, looked up these segments in a TM toextract matching pairs of SL-TL sentences, and manuallyidentified the TL tokens corresponding to the SL queriesin each of these pairs, hence producing manual TS?s.
Wethen submitted the same sentence-pairs and SL queriesto each of the proposed TS methods, and measured howthe TL answers produced automatically compared withthose produced manually.
We describe this process andthe results we obtained in more details below.4.1 Test MaterialThe test material for our experiments was gathered from atranslation memory, made up of approximately 14 yearsof Hansard (English-French transcripts of the Canadianparliamentary debates), i.e.
all debates published be-tween April 1986 and January 2002, totalling over 100million words in each language.
These documents weremostly collected over the Internet, had the HTML markupremoved, were then segmented into paragraphs and sen-tences, aligned at the sentence level using an implementa-tion of the method described in (Simard et al, 1992), andfinally dumped into a document-retrieval system (MG(Witten et al, 1999)).
We call this the Hansard TM.To identify SL queries, a distinct document from theHansard was used, the transcript from a session heldin March 2002.
The English version of this documentwas segmented into syntactic chunks, using an imple-mentation of Osborne?s chunker (Osborne, 2000).
Allsequences of chunks from this text that contained threeor more word tokens were then looked up in the HansardTM.
Among the sequences that did match sentences inthe TM, 100 were selected at random.
These made up thetest SL queries.RecursionLevel SL segment TL segment direction (d)1 [Let us see] [where the government ?scommitment is really at in terms of thefarm community]??
[Voyons] [quel est le ve?ritable engage-ment du gouvernement envers la com-munaute?
agricole]d = 12 [where the government ?s commitmentis really at] [in terms of the farm com-munity]??
[quel est le ve?ritable engagement dugouvernement] [envers la communaute?agricole]d = 13 [where] [the government ?s commitmentis really at]??
[quel] [est le ve?ritable engagement dugouvernement]d = 14 [the government ?s commitment] [is re-ally at]??
[est le ve?ritable] [engagement du gou-vernement]d = ?1Answers: rq(S) =the government ?s commitment ??
rq(T ) =engagement du gouvernementFigure 5: Compositional TS ExampleWhile some SL queries yielded only a handful ofmatches in the TM, others turned out to be very produc-tive, producing hundreds (and sometimes thousands) ofcouples.
For each test segment, we retained only the 100first matching pair of sentences from the TM.
This pro-cess yielded 4100 pairs of sentences from the TM, an av-erage of 41 per SL query; we call this our test corpus.Within each sentence pair, we spotted translations manu-ally, i.e.
we identified by hand the TL word-tokens cor-responding to the SL query for which the pair had beenextracted.
These annotations were done following the TSguidelines proposed by Ve?ronis (1998); we call this thereference TS.4.2 Evaluation MetricsThe results of our TS methods on the test corpus werecompared to the reference TS, and performance was mea-sured under different metrics.
Given each pair ?S, T ?from the test corpus, and the corresponding reference andevaluated TL answers r?
and r, represented as sets of to-kens, we computed:exactness : equal to 1 if r?
= r, 0 otherwise;recall : |r?
?
r|/|r?|precision : |r?
?
r|/|r|F-measure : 2 |r?r?||r|+|r?|In all the above computations, we considered that?empty?
TL answers (r = ?)
actually contained a single?null?
word.
These metrics were then averaged over allpairs of the test corpus (and not over SL queries, whichmeans that more ?productive?
queries weight more heav-ily in the reported results).4.3 ExperimentsWe tested all three methods presented in section 3, aswell as the three ?post-processings?
on Viterbi TS pro-posed in section 3.2.
All of these methods are based onIBM Model 2.
The same model parameters were used forall the experiments reported here, which were computedwith the GIZA program of the Egypt toolkit (Al-Onaizanet al, 1999).
Training was performed on a subset of about20% of the Hansard TM.
The results of our experimentsare presented in table 1.Metricmethod exact precision recall FViterbi 0.17 0.60 0.57 0.57+ Expansion 0.26 0.51 0.71 0.55+ Longest-sequence 0.03 0.63 0.20 0.29+ Zero-tolerance 0.20 0.28 0.28 0.28Contiguous 0.36 0.75 0.66 0.68Compositional 0.40 0.72 0.70 0.69Table 1: Results of experimentsThe Zero-tolerance post-processing produces emptyTL answers whenever the TL tokens are not contigu-ous.
On our test corpus, over 70% of all Viterbi align-ments turned out to be non-contiguous.
These emptyTL answers were counted in the statistics above (Viterbi+ Zero-tolerance row), which explains the low perfor-mance obtained with this method.
In practice, the in-tention of Zero-tolerance post-processing is to filter outnon-contiguous answers, under the hypotheses that theyprobably would not be usable in a TM application.
Table2 presents the performance of this method, taking intoaccount only non-empty answers.Metricmethod exact precision recall FViterbi+ Zero-tolerance 0.56 0.83 0.82 0.81Table 2: Performance of zero-tolerance filter on non-empty TL answers4.4 DiscussionGlobally, in terms of exactness, compositional TS pro-duces the best TL answers, with 40% correct answers, animprovement of 135% over plain Viterbi TS.
This gainis impressive, particularily considering the fact that allmethods use exactly the same data.
In more realisticterms, the gain in F -measure is over 20%, which is stillconsiderable.The best results in terms of precision are obtained withcontiguous TS, which in fact is not far behind composi-tional TS in terms of recall either.
This clearly demon-strates the impact of a simple contiguity constraint in thistype of TS application.
Overall, the best recall figuresare obtained with the simple Extension post-processingon Viterbi TS, but at the cost of a sharp decrease in preci-sion.
Considering that precision is possibly more impor-tant than recall in a TM application, the contiguous TSwould probably be a good choice.The Zero-tolerance strategy, used as a filter on Viterbialignments, turns out to be particularily effective.
It is in-teresting to note that this method is equivalent to the oneproposed by Marcu (Marcu, 2001) to automatically con-struct a sub-sentential translation memory.
Taking onlynon-null TS?s into consideration, it outclasses all othermethods, regardless of the metric.
But this is at the costof eliminating numerous potentially useful TL answers(more than 70%).
This is particularily frustrating, con-sidering that over 90% of all TL answers in the referenceare indeed contiguous.To understand how this happens, one must go back tothe definition of IBM-style alignments, which specifiesthat each SL token is linked to at most one TL token.This has a direct consequence on Viterbi TS?s: if the SLqueries contains K word-tokens, then the TL answer willitself contain at most that number of tokens.
As a re-sult, this method has systematic problems when the ac-tual TL answer is longer than the SL query.
It turns outthat this occurs very frequently, especially when aligningfrom English to French, as is the case here.
For exam-ple, consider the English sequence airport security, mostoften translated in French as se?curite?
dans les ae?roports.The Viterbi alignment normally produces links airport ?ae?roport and security ?
se?curite?, and the sequence dansles is then left behind (or accidentally picked up by er-roneous links from other parts of the SL sentence), thusleaving a non-contiguous TL answer.The Expansion post-processing, which finds the short-est possible sequence that covers all the tokens of theViterbi TL answer, solves the problem in simple sit-uations such as the one in the above example.
Butin general, integrating contiguity constraints directly inthe search procedure (contiguous and compositional TS)turns out to be much more effective, without solving theproblem entirely.
This is explained in part by the fact thatthese techniques are also based on IBM-style alignments.When ?surplus?
words appear at the boundaries of theTL answer, these words are not counted in the alignmentprobability, and so there is no particular reason to includethem in the TL answer.
Consider the following example:?
These companies indicated their support for thegovernment ?s decision.?
Ces compagnies ont de?clare?
qu?
elles appuyaient lade?cision du gouvernement .When looking for the French equivalent to the Englishindicated their support, we will probably end up with analignment that links indicated ?
de?clare?
and support ?appuyaient.
As a result of contiguity constraints, the TLsequence qu?
elle will naturally be included in the TL an-swer, possibly forcing a link their ?
elles in the process.However, the only SL that could be linked to ont is theverb indicated, which is already linked to de?clare?.
As aresult, ont will likely be left behind in the final alignment,and will not be counted when computing the alignment?sprobability.5 ConclusionWe have presented different translation spottings meth-ods, specifically adapted to a sub-sentential translationmemory system that proposes TL translations for SLsequences of syntactic chunks, as proposed by Planas(2000).
These methods are based on IBM statistical trans-lation Model 2 (Brown et al, 1993), but take advantageof certain characteristics of the segments of text that cantypically be extracted from translation memories.
By im-posing contiguity and compositionality constraints on thesearch procedure, we have shown that it is possible to per-form translation spotting more accurately than by simplyrelying on the most likely word alignment.Yet, the accuracy of our methods still leave a lot to bedesired; on closer examination most of our problems canbe attributed to the underlying translation model.
Com-puting word alignments with IBM Model 2 is straightfor-ward and efficient, which made it a good choice for ex-perimenting; however, this model is certainly not the stateof the art in statistical translation modeling.
Thenagain,the methods proposed here were all based on the ideaof finding the most likely word-alignment under variousconstraints.
This approach is not dependent on the under-lying translation model, and similar methods could cer-tainly be devised based on more elaborate models, suchas IBM Models 3?5, or the HMM-based models proposedby Och et al (1999) for example.Alternatively, there are other ways to compensate forModel 2?s weaknesses.
Each IBM-style alignment be-tween two segments of text denotes one particular expla-nation of how the TL words emerged from the SL words,but it doesn?t tell the whole story.
Basing our TS meth-ods on a set of likely alignments rather than on the singlemost-likely alignment, as is normally done to estimate theparameters of higher-level models, could possibly lead tomore accurate TS results.
Similarly, TS applications arenot bound to translation directionality as statistical trans-lation systems are; this means that we could also makeuse of a ?reverse?
model to obtain a better estimate of thelikelihood of two segments of text being mutual transla-tion.These are all research directions that we are currentlypursuing.References[Abney1991] Steven Abney.
1991.
Parsing by Chunks.In R.C.
Berwick, editor, Principle-Based Parsing:Computation and Psycholinguistics, pages 257?278.Kluwer Academic Publishers, Dordrecht, The Nether-lands.
[Al-Onaizan et al1999] Yaser Al-Onaizan, Jan Curin,Michael Jahr, Kevin Knight, John Lafferty, DanMelamed, Franz-Josef Och, David Purdy, Noah H.Smith, and David Yarowsky.
1999.
Statistical Ma-chine Translation - Final Report, JHU Workshop 1999.Technical report, Johns Hopkins University.
[Brown et al1993] Peter F. Brown, Stephen A. DellaPietra, Vincent J. Della Pietra, and Robert L. Mer-cer.
1993.
The Mathematics of Machine Transla-tion: Parameter Estimation.
Computational Linguis-tics, 19(2):263?311.
[Brown1996] Ralf D. Brown.
1996.
Example-Based Ma-chine Translation in the Pangloss System.
In Proceed-ings of the International Conference on ComputationalLinguistics (COLING) 1996, pages 169?174, Copen-hagen, Denmark, August.[Lange?
et al1997] Jean-Marc Lange?, ?Eric Gaussier, andBe?atrice Daille.
1997.
Bricks and Skeletons: SomeIdeas for the Near Future of MAHT.
Machine Trans-lation, 12(1?2):39?51.
[Macklovitch et al2000] Elliott Macklovitch, MichelSimard, and Philippe Langlais.
2000.
TransSearch:A Free Translation Memory on the World WideWeb.
In Proceedings of the Second InternationalConference on Language Resources & Evaluation(LREC), Athens, Greece.
[Marcu2001] Daniel Marcu.
2001.
Towards a UnifiedApproach to Memory- and Statistical-Based MachineTranslation.
In Proceedings of the 39th Annual Meet-ing of the Association for Computational Linguistics(ACL), Toulouse, France, July.
[McTait et al1999] Kevin McTait, Maeve Olohan, andArturo Trujillo.
1999.
A Building Blocks Approach toTranslation Memory.
In Proceedings of the 21st ASLIBInternational Conference on Translating and the Com-puter, London, UK.
[Melamed1998] I. Dan Melamed.
1998.
Word-to-WordModels of Translational Equivalence.
Technical Re-port 98-08, Dept.
of Computer and Information Sci-ence, University of Pennsylvania, Philadelphia, USA.
[Och et al1999] Franz Josef Och, Christoph Tillmann,and Hermann Ney.
1999.
Improved Alignment Mod-els for Statistical Machine Translation.
In Proceedingsof the 4th Conference on Empirical Methods in Natu-ral Language Processing (EMNLP)and 7th ACL Work-shop on Very Large Corpora (WVLC), pages 20?28,College Park, USA.
[Osborne2000] Miles Osborne.
2000.
Shallow Parsingas Part-of-Speech Tagging.
In Claire Cardie, Wal-ter Daelemans, Claire Ne?dellec, and Erik Tjong KimSang, editors, Proceedings of the Fourth Conferenceon Computational Natural Language Learning, Lis-bon, Portugal, September.
[Planas2000] Emmanuel Planas.
2000.
Extending Trans-lation Memories.
In EAMT Machine TranslationWorkshop, Ljubljana, Slovenia, May.
[Simard et al1992] Michel Simard, George Foster, andPierre Isabelle.
1992.
Using Cognates to Align Sen-tences in Bilingual Corpora.
In Proceedings of the4th Conference on Theoretical and MethodologicalIssues in Machine Translation (TMI), pages 67?82,Montre?al, Canada.
[Simard2003] Michel Simard.
2003.
Me?moires de tra-duction sous-phrastiques.
Ph.D. thesis, Universite?
deMontre?al.
to appear.
[Ve?ronis and Langlais2000] Jean Ve?ronis and PhilippeLanglais.
2000.
Evaluation of Parallel Text AlignmentSystems ?
The ARCADE Project.
In Jean Ve?ronis, ed-itor, Parallel Text Processing, Text, Speech and Lan-guage Technology.
Kluwer Academic Publishers, Dor-drecht, The Netherlands.
[Ve?ronis1998] Jean Ve?ronis.
1998.
Tagging guidelinesfor word alignment.
http://www.up.univ-mrs.fr/ vero-nis/arcade/2nd/word/guide/index.html, April.
[Witten et al1999] Ian H. Witten, Alistair Moffat, andTimothy C. Bell.
1999.
Managing Gigabytes: Com-pressing and Indexing Documents and Images.
Mor-gan Kaufmann Publishing, San Francisco, USA, 2ndedition edition.
[Wu1997] Dekai Wu.
1997.
Stochastic Inversion Trans-duction Grammars and Bilingual Parsing of ParallelCorpora.
Computational Linguistics, 23(3):377?404,September.
