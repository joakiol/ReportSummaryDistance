Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 532?537,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsExtracting Definitions and Hypernym Relations relying on SyntacticDependencies and Support Vector MachinesGuido BoellaUniversity of TurinDepartment of Computer Scienceboella@di.unito.itLuigi Di CaroUniversity of TurinDepartment of Computer Sciencedicaro@di.unito.itAbstractIn this paper we present a technique toreveal definitions and hypernym relationsfrom text.
Instead of using pattern match-ing methods that rely on lexico-syntacticpatterns, we propose a technique whichonly uses syntactic dependencies betweenterms extracted with a syntactic parser.The assumption is that syntactic informa-tion are more robust than patterns whencoping with length and complexity of thesentences.
Afterwards, we transform suchsyntactic contexts in abstract representa-tions, that are then fed into a SupportVector Machine classifier.
The results onan annotated dataset of definitional sen-tences demonstrate the validity of our ap-proach overtaking current state-of-the-arttechniques.1 IntroductionNowadays, there is a huge amount of textualdata coming from different sources of informa-tion.
Wikipedia1 , for example, is a free encyclo-pedia that currently contains 4,208,409 English ar-ticles2.
Even Social Networks play a role in theconstruction of data that can be useful for Infor-mation Extraction tasks like Sentiment Analysis,Question Answering, and so forth.From another point of view, there is the needof having more structured data in the forms ofontologies, in order to allow semantics-based re-trieval and reasoning.
Ontology Learning isa task that permits to automatically (or semi-automatically) extract structured knowledge fromplain text.
Manual construction of ontologies usu-ally requires strong efforts from domain experts,and it thus needs an automatization in such sense.1http://www.wikipedia.org/2April 12, 2013.In this paper, we focus on the extraction of hy-pernym relations.
The first step of such task relieson the identification of what (Navigli and Velardi,2010) called definitional sentences, i.e., sentencesthat contain at least one hypernym relation.
Thissubtask is important by itself for many tasks likeQuestion Answering (Cui et al, 2007), construc-tion of glossaries (Klavans and Muresan, 2001),extraction of taxonomic and non-taxonomic rela-tions (Navigli, 2009; Snow et al, 2004), enrich-ment of concepts (Gangemi et al, 2003; Cataldi etal., 2009), and so forth.Hypernym relation extraction involves two as-pects: linguistic knowlege, and model learning.Patterns collapse both of them, preventing to facethem separately with the most suitable techniques.First, patterns have limited expressivity; then, lin-guistic knowledge inside patterns is learned fromsmall corpora, so it is likely to have low coverage.Classification strictly depends on the learned pat-terns, so performance decreases, and the availableclassification techniques are restricted to thosecompatible with the pattern approach.
Instead, weuse a syntactic parser for the first aspect (with allits native and domain-independent knowledge onlanguage expressivity), and a state-of-the-art ap-proach to learn models with the use of SupportVector Machine classifiers.Our assumption is that syntax is less dependentthan learned patterns from the length and the com-plexity of textual expressions.
In some way, pat-terns grasp syntactic relationships, but they actu-ally do not use them as input knowledge.2 Related WorkIn this section we present the current state of theart concerning the automatic extraction of defini-tions and hypernym relations from plain text.
Wewill use the term definitional sentence referring tothe more general meaning given by (Navigli andVelardi, 2010): A sentence that provides a for-532mal explanation for the term of interest, and morespecifically as a sentence containing at least onehypernym relation.So far, most of the proposed techniques rely onlexico-syntactic patterns, either manually or semi-automatically produced (Hovy et al, 2003; Zhangand Jiang, 2009; Westerhout, 2009).
Such pat-terns are sequences of words like ?is a?
or ?refersto?, rather than more complex sequences includ-ing part-of-speech tags.In the work of (Westerhout, 2009), after a man-ual identification of types of definitions and relatedpatterns contained in a corpus, he successively ap-plied Machine Learning techniques on syntacticand location features to improve the results.A fully-automatic approach has been proposedby (Borg et al, 2009), where the authors appliedgenetic algorithms to the extraction of English def-initions containing the keyword ?is?.
In detail,they assign weights to a set of features for the clas-sification of definitional sentences, reaching a pre-cision of 62% and a recall of 52%.Then, (Cui et al, 2007) proposed an approachbased on soft patterns, i.e., probabilistic lexico-semantic patterns that are able to generalize overrigid patterns enabling partial matching by cal-culating a generative degree-of-match probabilitybetween a test instance and the set of training in-stances.Similarly to our approach, (Fahmi and Bouma,2006) used three different Machine Learning algo-rithms to distinguish actual definitions from othersentences also relying on syntactic features, reach-ing high accuracy levels.The work of (Klavans and Muresan, 2001) re-lies on a rule-based system that makes use of ?cuephrases?
and structural indicators that frequentlyintroduce definitions, reaching 87% of precisionand 75% of recall on a small and domain-specificcorpus.As for the task of definition extraction, mostof the existing approaches use symbolic methodsthat are based on lexico-syntactic patterns, whichare manually crafted or deduced automatically.The seminal work of (Hearst, 1992) represents themain approach based on fixed patterns like ?NPxis a/an NPy?
and ?NPx such as NPy?, that usu-ally imply < x IS-A y >.The main drawback of such technique is that itdoes not face the high variability of how a relationcan be expressed in natural language.
Still, it gen-erally extracts single-word terms rather than well-formed and compound concepts.
(Berland andCharniak, 1999) proposed similar lexico-syntacticpatterns to extract part-whole relationships.
(Del Gaudio and Branco, 2007) proposed a rule-based approach to the extraction of hypernymsthat, however, leads to very low accuracy valuesin terms of Precision.
(Ponzetto and Strube, 2007) proposed atechnique to extract hypernym relations fromWikipedia by means of methods based on theconnectivity of the network and classical lexico-syntactic patterns.
(Yamada et al, 2009) extendedtheir work by combining extracted Wikipedia en-tries with new terms contained in additional webdocuments, using a distributional similarity-basedapproach.Finally, pure statistical approaches presenttechniques for the extraction of hierarchies ofterms based on words frequency as well as co-occurrence values, relying on clustering proce-dures (Candan et al, 2008; Fortuna et al, 2006;Yang and Callan, 2008).
The central hypothesis isthat similar words tend to occur together in similarcontexts (Harris, 1954).
Despite this, they are de-fined by (Biemann, 2005) as prototype-based on-tologies rather than formal terminological ontolo-gies, and they usually suffer from the problem ofdata sparsity in case of small corpora.3 ApproachIn this section we present our approach to identifyhypernym relations within plain text.
Our method-ology consists in relaxing the problem into twoeasier subtasks.
Given a relation rel(x, y) con-tained in a sentence, the task becomes to find 1)a possible x, and 2) a possible y.
In case of morethan one possible x or y, a further step is neededto associate the correct x to the right y.By seeing the problem as two different classi-fication problems, there is no need to create ab-stract patterns between the target terms.
In ad-dition to this, the general problem of identifyingdefinitional sentences can be seen as to find at leastone x and one y in a sentence.3.1 Local Syntactic InformationDependency parsing is a procedure that extractssyntactic dependencies among the terms containedin a sentence.
The idea is that, given a hyper-nym relation, hyponyms and hypernyms may be533characterized by specific sets of syntactic contexts.According to this assumption, the task can be seenas a classification problem where each term in asentence has to be classified as hyponym, hyper-nym, or neither of the two.For each noun, we construct a textual represen-tation containing its syntactic dependencies (i.e.,its syntactic context).
In particular, for each syn-tactic dependency dep(a, b) (or dep(b, a)) of a tar-get noun a, we create an abstract token3 dep-target-b?
(or dep-b?-target), where b?
becomes thegeneric string ?noun?
in case it is another noun;otherwise it is equal to b.
This way, the nouns aretransformed into abstract strings; on the contrary,no abstraction is done for verbs.For instance, let us consider the sentence ?TheAlbedo of an object is the extent to which it dif-fusely reflects light from the sun?.
After the Part-Of-Speech annotation, the parser will extract a se-ries of syntactic dependencies like ?det(Albedo,The)?, ?nsubj(extent, Albedo)?, ?prepof(Albedo,object)?, where det identifies a determiner, nsubjrepresents a noun phrase which is the syntac-tic subject of a clause, and so forth4.
Then,such dependencies will be transformed in abstractterms like ?det-target-the?, ?nsubj-noun-target?,and ?prepof -target-noun?.
These triples representthe feature space on which the Support Vector Ma-chine classifiers will construct the models.3.2 Learning phaseOur model assumes a transformation of the localsyntactic information into labelled numeric vec-tors.
More in detail, given a sentence S annotatedwith the terms linked by the hypernym relation,the system produces as many input instances asthe number of nouns contained in S. For eachnoun n in S, the method produces two instancesSnx and Sny , associated to the label positive or neg-ative depending on their presence in the target re-lation (i.e., as x or y respectively).
If a noun isnot involved in a hypernym relation, both the twoinstances will have the label negative.
At the endof this process, two training sets are built, i.e., onefor each relation argument, namely the x-set andthe y-set.
All the instances of both the datasets arethen transformed into numeric vectors according3We make use of the term ?abstract?
to indicate that somewords are replaced with more general entity identifiers.4A complete overview of the Stan-ford dependencies is available athttp://nlp.stanford.edu/software/dependencies manual.pdf.to the Vector Space Model (Salton et al, 1975),and are finally fed into a Support Vector Machineclassifier5 (Cortes and Vapnik, 1995).
We refer tothe two resulting models as the x-model and they-model.
These models are binary classifiers that,given the local syntactic information of a noun, es-timate if it can be respectively an x or a y in a hy-pernym relation.Once the x-model and the y-model are built, wecan both classify definitional sentences and extracthypernym relations.
In the next section we deepenour proposed strategy in that sense.The whole set of instances of all the sentencesare fed into two Support Vector Machine classi-fiers, one for each target label (i.e., x and y).At this point, it is possible to classify each termas possible x or y by querying the respective clas-sifiers with its local syntactic information.4 Setting of the TasksIn this section we present how our proposed tech-nique is able to classify definitional sentences un-raveling hypernym relations.4.1 Classification of definitional sentencesAs already mentioned in previous sections, we la-bel as definitional all the sentences that contain atleast one noun n classified as x, and one noun mclassified as y (where n 6= m).
In this phase, itis not further treated the case of having more thanone x or y in one single sentence.
Thus, given aninput sentence:1. we extract all the nouns (POS-tagging),2. we extract all the syntactic dependencies ofthe nouns (dependency parsing),3. we fed each noun (i.e., its instance) to the x-model and to the y model,4.
we check if there exist at least one noun clas-sified as x and one noun classified as y: inthis case, we classify the sentences as defini-tional.4.2 Extraction of hypernym relationsOur method for extracting hypernym relationsmakes use of both the x-model and the y-modelas for the the task of classifying definitional sen-tences.
If exactly one x and one y are identified5We used the Sequential Minimal Optimization imple-mentation of the Weka framework (Hall et al, 2009).534in the same sentence, they are directly connectedand the relation is extracted.
The only constraintis that x and y must be connected within the sameparse tree.Now, considering our target relation hyp(x, y),in case the sentence contains more than one nounthat is classified as x (or y), there are two possiblescenarios:1. there are actually more than one x (or y), or2.
the classifiers returned some false positive.Up to now, we decided to keep all the possi-ble combinations, without further filtering opera-tions6.
Finally, in case of multiple classificationsof both x and y, i.e., if there are multiple x andmultiple y at the same time, the problem becomesto select which x is linked to which y7.
To do this,we simply calculate the distance between theseterms in the parse tree (the closer the terms, thebetter the connection between the two).
Neverthe-less, in the used corpus, only around 1.4% of thesentences are classified with multiple x and y.Finally, since our method is able to extractsingle nouns that can be involved in a hyper-nym relation, we included modifiers preceded bypreposition ?of?, while the other modifiers are re-moved.
For example, considering the sentence?An Archipelago is a chain of islands?, the wholechunk ?chain of islands?
is extracted from the sin-gle triggered noun chain.5 EvaluationIn this section we present the evaluation of ourapproach, that we carried out on an annotateddataset of definitional sentences (Navigli et al,2010).
The corpus contains 4,619 sentences ex-tracted from Wikipedia, and only 1,908 are anno-tated as definitional.
On a first instance, we test theclassifiers on the extraction of hyponyms (x) andhypernyms (y) from the definitional sentences, in-dependently.
Then, we evaluate the classificationof definitional sentences.
Finally, we evaluate theability of our technique when extracting whole hy-pernym relations.
With the used dataset, the con-structed training sets for the two classifiers (x-setand y-set) resulted to have approximately 1,500features.6We only used the constraint that x has to be differentfrom y.7Notice that this is different from the case in which a sin-gle noun is labeled as both x and y.Alg.
P R F AccWCL-3 98.8% 60.7% 75.2 % 83.4 %Star P. 86.7% 66.1% 75.0 % 81.8 %Bigrams 66.7% 82.7% 73.8 % 75.8 %Our sys.
88.0% 76.0% 81.6% 89.6%Table 1: Evaluation results for the classification ofdefinitional sentences, in terms of Precision (P ),Recall (R), F-Measure (F ), and Accuracy (Acc),using 10-folds cross validation.
For the WCL-3approach and the Star Patterns see (Navigli andVelardi, 2010), and (Cui et al, 2007) for Bigrams.Algorithm P R FWCL-3 78.58% 60.74% * 68.56%Our system 83.05% 68.64% 75.16%Table 2: Evaluation results for the hypernym re-lation extraction, in terms of Precision (P ), Re-call (R), and F-Measure (F ).
For the WCL-3 ap-proach, see (Navigli and Velardi, 2010).
These re-sults are obtained using 10-folds cross validation(* Recall has been inherited from the definitionclassification task, since no indication has been re-ported in their contribution).5.1 ResultsIn this section we present the evaluation of ourtechnique on both the tasks of classifying def-initional sentences and extracting hypernym re-lations.
Notice that our approach is susceptiblefrom the errors given by the POS-tagger8 and thesyntactic parser9 .
In spite of this, our approachdemonstrates how syntax can be more robust foridentifying semantic relations.
Our approach doesnot make use of the full parse tree, and we are notdependent on a complete and correct result of theparser.The goal of our evaluation is twofold: first, weevaluate the ability of classifying definitional sen-tences; finally, we measure the accuracy of the hy-pernym relation extraction.A definitional sentences is extracted only if atleast one x and one y are found in the same sen-tence.
Table 1 shows the accuracy of the ap-proach for this task.
As can be seen, our pro-posed approach has a high Precision, with a highRecall.
Although Precision is lower than the pat-8http://nlp.stanford.edu/software/tagger.shtml9http://www-nlp.stanford.edu/software/lex-parser.shtml535tern matching approach proposed by (Navigli andVelardi, 2010), our Recall is higher, leading to anhigher overall F-Measure.Table 2 shows the results of the extraction ofthe whole hypernym relations.
Note that our ap-proach has high levels of accuracy.
In particular,even in this task, our system outperforms the pat-tern matching algorithm proposed by (Navigli andVelardi, 2010) in terms of Precision and Recall.6 Conclusion and Future WorkWe presented an approach to reveal definitions andextract underlying hypernym relations from plaintext, making use of local syntactic information fedinto a Support Vector Machine classifier.
The aimof this work was to revisit these tasks as classicalsupervised learning problems that usually carry tohigh accuracy levels with high performance whenfaced with standard Machine Learning techniques.Our first results on this method highlight the va-lidity of the approach by significantly improvingcurrent state-of-the-art techniques in the classifi-cation of definitional sentences as well as in theextraction of hypernym relations from text.
In fu-ture works, we aim at using larger syntactic con-texts.
In fact, currently, the detection does notsurpass the sentence level, while taxonomical in-formation can be even contained in different sen-tences or paragraphs.
We also aim at evaluatingour approach on the construction of entire tax-onomies starting from domain-specific text cor-pora, as in (Navigli et al, 2011; Velardi et al,2012).
Finally, the desired result of the task of ex-tracting hypernym relations from text (as for anysemantic relationships in general) depends on thedomain and the specific later application.
Thus,we think that a precise evaluation and comparisonof any systems strictly depends on these factors.For instance, given a sentence like ?In mathemat-ics, computing, linguistics and related disciplines,an algorithm is a sequence of instructions?
onecould want to extract only ?instructions?
as hyper-nym (as done in the annotation), rather than the en-tire chunk ?sequence of instructions?
(as extractedby our technique).
Both results can be valid, anda further discrimination can only be done if a spe-cific application or use of this knowlege is takeninto consideration.ReferencesM.
Berland and E. Charniak.
1999.
Finding parts invery large corpora.
In Annual Meeting Associationfor Computational Linguistics, volume 37, pages57?64.
Association for Computational Linguistics.C.
Biemann.
2005.
Ontology learning from text: Asurvey of methods.
In LDV forum, volume 20, pages75?93.C.
Borg, M. Rosner, and G. Pace.
2009.
Evolutionaryalgorithms for definition extraction.
In Proceedingsof the 1st Workshop on Definition Extraction, pages26?32.
Association for Computational Linguistics.K.S.
Candan, L. Di Caro, and M.L.
Sapino.
2008.
Cre-ating tag hierarchies for effective navigation in so-cial media.
In Proceedings of the 2008 ACM work-shop on Search in social media, pages 75?82.
ACM.Mario Cataldi, Claudio Schifanella, K Selc?uk Can-dan, Maria Luisa Sapino, and Luigi Di Caro.
2009.Cosena: a context-based search and navigation sys-tem.
In Proceedings of the International Confer-ence on Management of Emergent Digital EcoSys-tems, page 33.
ACM.C.
Cortes and V. Vapnik.
1995.
Support-vector net-works.
Machine learning, 20(3):273?297.Hang Cui, Min-Yen Kan, and Tat-Seng Chua.
2007.Soft pattern matching models for definitional ques-tion answering.
ACM Trans.
Inf.
Syst., 25(2), April.R.
Del Gaudio and A. Branco.
2007.
Automatic ex-traction of definitions in portuguese: A rule-basedapproach.
Progress in Artificial Intelligence, pages659?670.I.
Fahmi and G. Bouma.
2006.
Learning to iden-tify definitions using syntactic features.
In Pro-ceedings of the EACL 2006 workshop on LearningStructured Information in Natural Language Appli-cations, pages 64?71.B.
Fortuna, D.
Mladenic?, and M. Grobelnik.
2006.Semi-automatic construction of topic ontologies.Semantics, Web and Mining, pages 121?131.Aldo Gangemi, Roberto Navigli, and Paola Velardi.2003.
The ontowordnet project: Extension andaxiomatization of conceptual relations in wordnet.In Robert Meersman, Zahir Tari, and DouglasC.Schmidt, editors, On The Move to Meaningful In-ternet Systems 2003: CoopIS, DOA, and ODBASE,volume 2888 of Lecture Notes in Computer Science,pages 820?838.
Springer Berlin Heidelberg.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The weka data mining software: an update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.536M.A.
Hearst.
1992.
Automatic acquisition of hy-ponyms from large text corpora.
In Proceedings ofthe 14th conference on Computational linguistics-Volume 2, pages 539?545.
Association for Compu-tational Linguistics.E.
Hovy, A. Philpot, J. Klavans, U. Germann, P. Davis,and S. Popper.
2003.
Extending metadata defi-nitions by automatically extracting and organizingglossary definitions.
In Proceedings of the 2003 an-nual national conference on Digital government re-search, pages 1?6.
Digital Government Society ofNorth America.J.L.
Klavans and S. Muresan.
2001.
Evaluation ofthe definder system for fully automatic glossary con-struction.
In Proceedings of the AMIA Symposium,page 324.
American Medical Informatics Associa-tion.Roberto Navigli and Paola Velardi.
2010.
Learningword-class lattices for definition and hypernym ex-traction.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, pages 1318?1327, Uppsala, Sweden, July.
As-sociation for Computational Linguistics.Roberto Navigli, Paola Velardi, and Juana Mara Ruiz-Martnez.
2010.
An annotated dataset for extractingdefinitions and hypernyms from the web.
In Pro-ceedings of the Seventh International Conferenceon Language Resources and Evaluation (LREC?10),Valletta, Malta.
European Language Resources As-sociation (ELRA).R.
Navigli, P. Velardi, and S. Faralli.
2011.
A graph-based algorithm for inducing lexical taxonomiesfrom scratch.
In Proceedings of the Twenty-Second international joint conference on ArtificialIntelligence-Volume Volume Three, pages 1872?1877.
AAAI Press.R.
Navigli.
2009.
Using cycles and quasi-cycles to dis-ambiguate dictionary glosses.
In Proceedings of the12th Conference of the European Chapter of the As-sociation for Computational Linguistics, pages 594?602.
Association for Computational Linguistics.S.P.
Ponzetto and M. Strube.
2007.
Deriving a largescale taxonomy from wikipedia.
In Proceedingsof the national conference on artificial intelligence,volume 22, page 1440.Menlo Park, CA; Cambridge,MA; London; AAAI Press; MIT Press; 1999.G.
Salton, A. Wong, and C. S. Yang.
1975.
A vec-tor space model for automatic indexing.
Commun.ACM, 18(11):613?620, November.R.
Snow, D. Jurafsky, and A.Y.
Ng.
2004.
Learn-ing syntactic patterns for automatic hypernym dis-covery.
Advances in Neural Information ProcessingSystems 17.Paola Velardi, Stefano Faralli, and Roberto Navigli.2012.
Ontolearn reloaded: A graph-based algorithmfor taxonomy induction.
Computational Linguistics,pages 1?72.Eline Westerhout.
2009.
Definition extraction usinglinguistic and structural features.
In Proceedingsof the 1st Workshop on Definition Extraction, WDE?09, pages 61?67, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.I.
Yamada, K. Torisawa, J. Kazama, K. Kuroda,M.Mu-rata, S. De Saeger, F. Bond, and A. Sumida.
2009.Hypernym discovery based on distributional simi-larity and hierarchical structures.
In Proceedingsof the 2009 Conference on Empirical Methods inNatural Language Processing: Volume 2-Volume 2,pages 929?937.
Association for Computational Lin-guistics.H.
Yang and J. Callan.
2008.
Ontology generation forlarge email collections.
In Proceedings of the 2008international conference on Digital government re-search, pages 254?261.
Digital Government Societyof North America.Chunxia Zhang and Peng Jiang.
2009.
Automatic ex-traction of definitions.
In Computer Science andInformation Technology, 2009.
ICCSIT 2009.
2ndIEEE International Conference on, pages 364 ?368,aug.537
