Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 98?103,Jeju, Republic of Korea, 10 July 2012. c?2012 Association for Computational LinguisticsTowards High-Quality Text Stream Extraction from PDFTechnical Background to the ACL 2012 Contributed Task?yvind Raddum Berg, Stephan Oepen, and Jonathon ReadDepartment of Informatics, Universitetet i Oslo{oyvinrb |oe |jread}@ifi.uio.noAbstractExtracting textual content and documentstructure from PDF presents a surprisingly(depressingly, to some, in fact) difficult chal-lenge, owing to the purely display-oriented de-sign of the PDF document standard.
While avariety of lower-level PDF extraction toolk-its exist, none fully support the recovery oforiginal text (in reading order) and relevantstructural elements, even for so-called born-digital PDFs, i.e.
those prepared electronicallyusing typesetting systems like LATEX, OpenOf-fice, and the like.
This short paper summarizesa new tool for high-quality extraction of textand structure from PDFs, combining state-of-the-art PDF parsing, font interpretation, layoutanalysis, and TEI-compliant output of text andlogical document markup.
?1 Introduction?MotivationTo view a collection of scholarly articles like theACL Anthology as a structured knowledge base sub-stantially transcends a na?ve notion of a corpus asa mere collection of running text.
Research litera-ture is the result of careful editing and typesettingand, thus, is organized around its complex internalstructure.
Relevant structural elements can compriseboth geometric (e.g.
pages, columns, blocks, or ta-bles) and logical units (e.g.
titles, abstracts, head-ings, paragraphs, or citations)?where (ideally) ge-ometric and logical document structure play handin hand to a degree that can make it hard to drawclear dividing lines in some cases (e.g.
in itemizedor numbered lists).To date, the dominant standard for electronic doc-ument archival is Portable Document Format (PDF),?We are indebted to Rebecca Dridan, Ulrich Sch?fer, and theACL workshop reviewers for helpful feedback on this work.originally created as a proprietary format by AdobeSystems Incorporated in the early 1990s and sub-sequently made an open ISO standard (which wasofficially adopted in 2008 and embraced by Adobethrough a public license that grants royalty-free us-age).
PDF is something of a composite standard,unifying at least three basic technologies:1.
A subset of the PostScript page ?programming?language, dropping constructs like loops andbranches, but including all graphical operationsto draw layout elements, text, and images.2.
A font embedding system which allows a doc-ument to ?carry along?
a broad variety of fonts(in various formats), as may be needed to en-sure display just as the document was designed.3.
A structured storage system, which organizesvarious data objects?for example images andfonts?inside a PDF document.All data objects in a PDF file are represented ina visually-oriented way, as a sequence of operatorswhich?when interpreted by a PDF renderer?willdraw the document on a page canvas.
This is a nat-ural approach considering the design roots of PDFas a PostScript successor and its original central rolein desktop publishing applications; but the implica-tions of such visually-centered design are unfortu-nate for the task of recovering textual content andlogical document structure.Interpretation of PDF operators will provide onewith all the individual characters, as well as theirformatting and position on the page.
However, theygenerally do not convey information about higherlevel text units such as tokens, lines, or columns?information about boundaries between such units isonly available implicitly through whitespace, i.e.
the98mere absence of textual or graphical objects.
Fur-thermore, data fragments comprising content text ona page may consist of individual characters, parts ofa word, whole lines, or any combination thereof?as dictated by font properties and kerning require-ments.
Complicating text extraction from PDF fur-ther, there are no rules governing the order in whichcontent is encoded in the document.
For example, toproduce a page with a two-column layout, the pagecould be drawn by first drawing the first lines of theleft and right columns, then the second lines, etc.Obtaining text in logical reading order, however, ob-viously requires that the text in the left column beprocessed before the one on the right, so a na?ve ap-proach to text extraction based on the sequencing ofobjects in the PDF file might produce undesirableresults.Since the standard is now open and free for any-one to use, we are fortunate to have several ma-ture, open-source libraries to handle low-level pars-ing and manipulation of objects in PDF documents.For this project, we build on Apache PDFBox1, forits maturity, relatively active support, and interfaceflexibility.
Originally as an MSc project in Com-puter Science (Berg, 2011), we have developed a pa-rameterizable toolkit for high-quality text and struc-ture extraction from born-digital PDFs, which wedub PDFExtract.2 In this application, we seek to ap-proximate this structure by using all the visual cluesand information we have available.The data presented in a PDF file consists ofstreams of objects; by placing hardly any signifi-cance on the order of elements within these streams,and more on the visual result obtained by (virtu-ally) ?rendering?
PDF operations, the task of text andstructure extraction is shifted slightly?from whattraditionally amounts to stream-processing, and to-wards a point of view related to computer vision.This view, in fact, essentially corresponds to thesame problem tackled by OCR software, thoughwithout the need to perform actual character recog-nition.
Some of the key elements of PDFExtract,thus, build on related OCR techniques and adaptand extend these to the PDF processing task.
Theprocess of ?understanding?
a PDF document in this1See http://pdfbox.apache.org/ for details.2See http://github.com/elacin/PDFExtract/.context is called document layout analysis, a taskwhich is commonly treated as two sequential sub-processes.
First, a page image is subjected to geo-metric layout analysis; the result of this first stagethen serves as input for a subsequent step of logi-cal layout analysis and content extraction.
The fol-lowing sections briefly review core aspects of thedesign and implementation of PDFExtract, rangingfrom low-level whitespace detection (?2), over ge-ometric and logical layout analysis (?3 and ?5, re-spectively), to aspects of font handling (?4).2 Whitespace DetectionAs a prerequisite to all subsequent analysis, seg-ment boundaries between tokens, lines, columns,and other blocks of content need to be made ex-plicit.
Such boundaries are predominantly repre-sented through whitespace, which is not overtly rep-resented among the data objects in PDF files.
Theapproach to whitespace detection and page segmen-tation in PDFExtract is an extension of the frame-work proposed by Breuel (2002) (originally in thecontext of OCR).The first step here is to find a cover of the back-ground whitespace of a document in terms of maxi-mal empty rectangles.
This is accomplished in a top-down procedure, using a whole page as its startingpoint, and working in a way abstractly analogous toquicksort or branch and bound algorithms.
Whites-pace rectangles are identified in order of decreasing?quality?
(as determined by size, shape, position, andrelations to actual page content), which means thatthe result will in general be globally optimal?in thesense that no other (equal-sized) sequence of cover-ing rectangles would yield a larger total quality sum.Figure 1 illustrates the main idea of the algorithm,which starts from a bound (initially the page at large)and a set of non-empty rectangles, called obstacles.If the set is empty, it means that the bound is a max-imal rectangle with respect to other obstacles (sur-rounding the bound).
If, as in Figure 1, there areobstacles, the bound needs to be further subdivided.To this end, we choose one obstacle as a pivot, whichideally is centered somewhere around the middle ofthe bound.
As no maximal rectangle can contain ob-stacles, in particular not the pivot, there are four pos-sibilities for the solution of the maximal whitespace99(a) (b) (c) (d)Figure 1: Schematic example of one iteration of the whitespace covering algorithm.
In (a) we see some obstacles (inblue) contained within a bounding rectangle; in (b) one of them is chosen as as pivot (in red); and (c) and (d) showhow the original bound is divided into four smaller rectangles (in grey) around the pivot.rectangle problem?one for each side of the pivot.The areas of these four sub-bounds are computed, alist of intersecting obstacles is computed for each ofthem, and they are processed in turn.As originally proposed by Breuel (2002), thebasic procedure proved applicable to born-digitalPDFs, though leaving room for improvements bothin terms of the quality of results and run-time perfor-mance.
Some deficiencies that were observed in pro-cessing documents from the ACL Anthology (andother samples of scholarly literature) are exempli-fied in Figure 2, relating to smallish, ?stray?
whites-pace rectangles in the middle of otherwise contigu-ous segments (top row in Figure 2), challenges re-lated to relative differences in line spacing (middle),and spurious vertical boundaries introduced by so-called rivers, i.e.
accidental alignment of horizon-tal spacing across lines (bottom).
Besides adjust-ments to the rectangle ?quality?
function, the prob-lems were addressed by (a) allowing a small degreeFigure 2: Select challenges to whitespace covering ap-proach: stray whitespace inbetween groups of text (top);inter- vs. intra-paragraph spacing (middle); and ?rivers?leading to spurious vertical boundaries (bottom).of overlap between whitespace rectangles and obsta-cles, (b) a strong preference for contiguous areas ofwhitespace (thus making the procedure work fromthe page borders inwards), (c) variable lower boundson the height and width of whitespace rectangles,computed dynamically from font properties of sur-rounding text, and (d) a small number of special-ized heuristic rules, to block unwanted whitespacerectangles in select configurations.
Berg (2011) pro-vides full details for these adaptations, as well as foralgorithmic optimizations and parameterization thatenable run-time throughputs of tens of pages per cpusecond.3 Determining Page LayoutThe high-level goal in analyzing page layout is toproduce a hierarchical representation of a page interms of blocks of homogenous content, thus mak-ing explicit relevant spatial relationship betweenthem.
In the realm of OCR, this task is often re-ferred to as geometric layout analysis (see, for ex-ample, (Cattoni et al, 1998)), whereas the term(de)boxing has at times been used in the context oftext stream extraction from PDFs.
In the followingparagraphs, we will focus on column boundary de-tection, but PDFExtract essentially applies the samegeneral techniques to the identification of other rel-evant inter-segment boundaries.While whitespace rectangles are essential to col-umn boundary identification, there is of course noguarantee for the existence of one rectangle whichwere equivalent to a whole column boundary.
First,as a natural consequence of the whitespace detectionprocedure, horizontal rectangles can ?interrupt?
can-didate colum boundaries.
Second, there may well betypographic imperfections causing gaps in the iden-tified whitespace (as exemplified in the top of Fig-100Figure 3: Select challenges to column identification: textelements protruding into the margin (top) and gaps inwhitespace rectangle coverage (often owed to processingbounds imposed for premium performance).ure 3), or it can be the case that geometric constraintsor computational limits imposed on the whitespacecover algorithm result in ?missing?
whitespace rect-angles (in the bottom of Figure 3).
Whereas the orig-inal design of Breuel (2002) makes no provisions forthese cases, PDFExtract adapts a revised, three-stepapproach to column detection, viz.
(a) extracting aninitial set of candidate boundaries; (b) heuristicallyexpanding column boundary candidates vertically;and (c) combining logically equivalent boundariesand filtering unwarranted ones.
Here, both steps (a)and (b) assume geometric constraints on the aspectratio of candidate column boundaries, as well as onthe existence and relative proportions of surround-ing non-whitespace content.
Again, please see Berg(2011) for further background on these steps.With column boundaries in place, PDFExtractproceeds to the identification of blocks of content(which may correspond to, for example, logicalparagraphs, headings, displayed equations, tables, orgraphical elements).
This step, essentially, is real-ized through a recursive ?flooding?
function, form-ing connected blocks from adjacent, non-whitespacePDF data objects where there are no interveningwhitespace rectangles.
Regions that (by contentor font properties) can be identified as (parts of)mathematical equations receive special attention atthis stage, allowing limited amounts of horizon-tally separating whitespace to be ignored for blockformation.
In a similar spirit, line segmentation(i.e.
grouping of vertically aligned data objects) isperformed block-wise?sorting content within eachblock by Y-coordinates and determining baselinesand inter-line spacing in a single downwards pass.The final key component in geometric layoutanalysis is the recovery of reading order (recallingthat PDFs do not provide reliable sequencing infor-mation for data objects).
PDFExtract adapts oneof the two techniques suggested by Breuel (2003),viz.
topological sorting of lines (which can includesingle-line blocks, where no block-internal line seg-mentation was detected) based on (a) relations of hi-erarchical nesting and (b) relative geometric posi-tions.
PDFExtract was tested against a set of some100 diverse PDF documents (from different sourcesof scholarly literature, a range of distinct PDF gener-ators, quite variable layout, and multiple languages),and its topological content sorting (detailed furtherin Berg, 2011) was found to give very satisfactoryresults in terms of reading order recovery.4 Font Handling and Word SegmentationMany of the steps of geometric layout analysis out-lined above depend on accurate coordinate informa-tion for glyphs, which turned out an unforeseen low-level challenge in our approach of building PDFEx-tract on top of Apache PDFBox.
Figure 4 (on theleft) shows a problematic example of ?raw?
glyphplacement information.
Several factors contribute toincorrect glyph positioning, including the sheer va-riety of font types supported in PDFs, missing in-formation about non-standard, embedded fonts, anddesign limitations and bugs in PDFBox.
To workaround common issues, PDFExtract includes a cou-ple of patches to PDFBox internals as well as spe-cialized code for different types of font embeddingin PDF to perform boundary box computation, po-sition offsetting, and and mapping to Unicode codepoints.
The (much improved though not quite per-fect) result of these adjustments, when applied toour running example, is depicted in the middle ofFigure 4.With the ultimate goal of creating a high-quality101(a) (b) (c)Figure 4: Examples of font-related challenges (before and after correction) and word segmentation.
(structured) text corpus from ACL Anthology doc-uments, word segmentation naturally is a mission-critical component of PDFExtract.
Seeing that inter-word whitespace is more often than not omittedfrom PDF data objects, word segmentation?muchlike other sub-tasks in geometric layout analysis?operates in terms of display positions.
Deter-mining whether the distance between two adjacentglyphs represents a word-separating whitespace ornot, might sound simple?but in practice it proveddifficult to devise a generic solution that performswell across differences in fonts and sizes (and corre-sponding variation in kerning, i.e.
intra-word spac-ing), deals with both high-quality and poor typog-raphy, and is somewhat robust to remaining inac-curacies in glyph positions .
PDFExtract arrived ata novel algorithm that approximates character textspacing (as could be set by the PDF Tc operator) byaveraging a selection of the smaller character dis-tances within a line.
The resulting average charac-ter spacing is subsequently used to normalize hori-zontal distances, i.e.
subtract line-specific characterspacing from every distance on that line?to ideallycenter character distances around zero, while leav-ing word distances larger (they will also be relativelymuch larger than before in comparison).
The iden-tification of word boundaries itself, accordingly, be-comes straightforward, comparing normalized dis-tances to a percentage of the local font size.
Theresults of this process are shown for our example inthe right of Figure 4.5 (Preliminary) Logical Layout AnalysisIn our view, thorough geometric layout analysis isan important prequisite of logical layout analysis.Hence, the emphasis of Berg (2011) was with re-spect to the geometric analysis.
However, what fol-lows is an overview of the preliminary procedure inPDFExtract to determine logical document structurefrom geometric layout and typographic information.The process begins by collating a set of text styles(i.e.
unique combinations of font type and size).Then, various heuristics govern the assignment ofstyles to logical roles:Body text Choose whichever style occurs most fre-quently (in terms of the number of characters).Title Choose the header-like block on the first pagethat has the largest font size.Abstract If one of the first pages has a single-lineblock with a style which is bigger or bolderthan body text, and contains the word abstract,it is chosen as an abstract header.
All body textuntil the next heading is the abstract text.Footnote Search for blocks on the lower part ofthe page that are smaller than body text; checkthat they start with a number or other footnote-indicating symbol.Sections Identify section header styles by compil-ing a list of styles that are either larger than orhave some emphasis on the body text style, andhave instances with evidence of section num-bering (e.g.
1.1, (1a)).
Infer the nesting levelof each section header style from its order ofoccurrence in the document; a section head-ing will always appear earlier than a subsectionheading, for instance.Having identified the different components in thedocument, these are used to create a logical hierar-chical representation following the TEI P5 Guide-lines (TEI Consortium, 2012) as introduced bySch?fer et al (2012).
Title, abstract, floaters, andfigures are separated from the main text.
The bodyof the document is then collated into a tree of sectionelements, with headers and body text.
Body text iscollected by combining consecutive text blocks that102have identical styles, before inferring paragraphs onthe basis of indented initial lines.
Dehyphenation istackled using a combination of a lexicon and a set oforthographic rules.6 Discussion?OutlookPDFExtract provides a fresh and open-source takeon the problem of high-quality content and struc-ture extraction from born-digital PDFs.
Unlike ex-isting initiatives (e.g.
the basic TextExtractionclass of PDFBox or the pdftotext command line util-ity from the Poppler library3), PDFExtract discardssequencing information available in the so-calledPDF text stream, but instead applies and adapts tech-niques from OCR?notably a whitespace coveringalgorithm, column, block, and line detection, recov-ery of reading order based on line-oriented topolog-ical sort, and improved word segmentation takingadvantage of specialized PDF font interpretation.While very comprehensive in terms of its geometriclayout analysis, PDFExtract to date only make avail-able a limited range of logical layout analysis func-tionality (and output into TEI-compliant markup),albeit also in this respect more so than pre-existingPDF text stream extraction approaches.For the ACL 2012 Contributed Task on Redis-covering 50 Years of Discoveries (Sch?fer et al,2012), PDFExtract outputs for the born-digital sub-set of the ACL Anthology are a component of the?starter package?
offered to participants, in the hopethat content and structure derived from OCR tech-niques (Sch?fer & Weitz, 2012) and those extracteddirectly from embedded content in the PDFs willcomplement each other.
As discussed in more detailby Sch?fer et al (2012), the two approaches havein part non-overlapping strengths and weaknesses,such that aligning content elements that correspondto each other across the two universes could yield amulti-dimensional, ideally both more complete andmore accurate perspective.
PDFExtract is a recentdevelopment and remains subject to refinement andextension.
Beyond a limited quantitative and qual-itative evaluation review by Berg (2011), the exactquality levels of text and document structure that itmakes available (as well as relevant factors of varia-tion, across different types of documents in the ACL3See http://poppler.freedesktop.org/.Anthology) remains to be determined empirically.We make available the full package, accompaniedby some technical documentation (Berg, 2011), aswell as a sample of gold-standard TEI-complianttarget outputs) in the hope that it may serve as thebasis for future work towards the ACL AnthologyCorpus?both at our own sites (i.e.
the Universityof Oslo and DFKI Saarbr?cken) and collaboratingpartners.
We would enthusiastically welcome addi-tional collaborators in this enterprise and will seekto provide any reasonable assistance required for thedeployment and extension of PDFExtract.ReferencesBerg, ?.
R. (2011).
High precision text extractionfrom PDF documents.
MSc Thesis, University ofOslo, Department of Informatics, Oslo, Norway.Breuel, T. (2002).
Two geometric algorithms forlayout analysis.
In Proceedings of the 5th work-shop on Document Analysis Systems (pp.
687?692).
Princeton, USA.Breuel, T. (2003).
Layout analysis based on text linesegment hypotheses.
In Third international work-shop on Document Layout Interpretation and itsApplications.
Edinburgh, Scotland.Cattoni, R., Coianiz, T., & Messelodi, S. (1998).
Ge-ometric layout analysis techniques for documentimage understanding.
A review (ITC-irst Techni-cal Report TR#9703-09).
Trento, Italy.Sch?fer, U., Read, J., & Oepen, S. (2012).
Towardsan ACL Anthology corpus with logical documentstructure.
An overview of the ACL 2012 con-tributed task.
In Proceedings of the ACL-2012main conference workshop on Rediscovering 50Years of Discoveries.
Jeju, Republic of Korea.Sch?fer, U., & Weitz, B.
(2012).
Combining OCRoutputs for logical document structure markup.Technical background to the ACL 2012 Con-tributed Task.
In Proceedings of the ACL-2012main conference workshop on Rediscovering 50Years of Discoveries.
Jeju, Republic of Korea.TEI Consortium.
(2012, February).
TEI P5: Guide-lines for electronic text encoding and interchange.
(http://www.tei-c.org/Guidelines/P5)103
