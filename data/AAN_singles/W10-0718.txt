Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon?s Mechanical Turk, pages 114?121,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsOpinion Mining of Spanish Customer Comments with Non-ExpertAnnotations on Mechanical TurkBart Mellebeek, Francesc Benavent, Jens Grivolla,Joan Codina, Marta R. Costa-jussa` and Rafael BanchsBarcelona Media Innovation CenterAv.
Diagonal, 177, planta 908018 Barcelona, Spain{bart.mellebeek|francesc.benavent|jens.grivolla|joan.codina|marta.ruiz|rafael.banchs}@barcelonamedia.orgAbstractOne of the major bottlenecks in the develop-ment of data-driven AI Systems is the cost ofreliable human annotations.
The recent ad-vent of several crowdsourcing platforms suchas Amazon?s Mechanical Turk, allowing re-questers the access to affordable and rapid re-sults of a global workforce, greatly facilitatesthe creation of massive training data.
Mostof the available studies on the effectiveness ofcrowdsourcing report on English data.
We useMechanical Turk annotations to train an Opin-ion Mining System to classify Spanish con-sumer comments.
We design three differentHuman Intelligence Task (HIT) strategies andreport high inter-annotator agreement betweennon-experts and expert annotators.
We evalu-ate the advantages/drawbacks of each HIT de-sign and show that, in our case, the use ofnon-expert annotations is a viable and cost-effective alternative to expert annotations.1 IntroductionObtaining reliable human annotations to train data-driven AI systems is often an arduous and expensiveprocess.
For this reason, crowdsourcing platformssuch as Amazon?s Mechanical Turk1, Crowdflower2and others have recently attracted a lot of attentionfrom both companies and academia.
Crowdsourc-ing enables requesters to tap from a global pool ofnon-experts to obtain rapid and affordable answersto simple Human Intelligence Tasks (HITs), which1https://www.mturk.com2http://crowdflower.com/can be subsequently used to train data-driven appli-cations.A number of recent papers on this subject pointout that non-expert annotations, if produced in a suf-ficient quantity, can rival and even surpass the qual-ity of expert annotations, often at a much lower cost(Snow et al, 2008), (Su et al, 2007).
However, thispossible increase in quality depends on the task athand and on an adequate HIT design (Kittur et al,2008).In this paper, we evaluate the usefulness of MTurkannotations to train an Opinion Mining System todetect opinionated contents (Polarity Detection) inSpanish customer comments on car brands.
Cur-rently, a large majority of MTurk tasks is designedfor English speakers.
One of our reasons for partic-ipating in this shared task was to find out how easyit is to obtain annotated data for Spanish.
In addi-tion, we want to find out how useful these data areby comparing them to expert annotations and usingthem as training data of an Opinion Mining Systemfor polarity detection.This paper is structured as follows.
Section 2 con-tains an explanation of the task outline and our goals.Section 3 contains a description of three differentHIT designs that we used in this task.
In Section4, we provide a detailed analysis of the retrievedHITs and focus on geographical information of theworkers, the correlation between the different HITdesigns, the quality of the retrieved answers and onthe cost-effectiveness of the experiment.
In Section5, we evaluate the incidence of MTurk-generated an-notations on a polarity classification task using twodifferent experimental settings.
Finally, we conclude114in Section 6.2 Task Outline and GoalsWe compare different HIT design strategies by eval-uating the usefulness of resulting Mechanical Turk(MTurk) annotations to train an Opinion MiningSystem on Spanish consumer data.
More specifi-cally, we address the following research questions:(i) Annotation quality: how do the differentMTurk annotations compare to expert annotations?
(ii) Annotation applicability: how does the per-formance of an Opinion Mining classifier vary aftertraining on different (sub)sets of MTurk and expertannotations?
(iii) Return on Investment: how does the use ofMTurk annotations compare economically againstthe use of expert annotations?
(iv) Language barriers: currently, most MTurktasks are designed for English speakers.
How easyis it to obtain reliable MTurk results for Spanish?3 HIT DesignWe selected a dataset of 1000 sentences contain-ing user opinions on cars from the automotive sec-tion of www.ciao.es (Spanish).
This website waschosen because it contains a large and varied poolof Spanish customer comments suitable to train anOpinion Mining System and because opinions in-clude simultaneously global numeric and specificratings over particular attributes of the subject mat-ter.
Section 5.1 contains more detailed informationabout the selection of the dataset.
An example of asentence from the data set can be found in (1):(1) ?No te lo pienses ma?s, co?mpratelo!?
(= ?Don?t think twice, buy it!?
)The sentences in the dataset were presented tothe MTurk workers in three different HIT designs.Each HIT design contains a single sentence to beevaluated.
HIT1 is a simple categorization schemein which workers are asked to classify the sentenceas being either positive, negative or neutral, as isshown in Figure 1b.
HIT2 is a graded categorizationtemplate in which workers had to assign a score be-tween -5 (negative) and +5 (positive) to the examplesentence, as is shown in Figure 1c.
Finally, HIT3 isa continuous triangular scoring template that allowsFigure 1: An example sentence (a) and the three HITdesigns used in the experiments: (b) HIT1: a simplecategorization scheme, (c) HIT2: a graded categoriza-tion scheme, and (d) HIT3: a continuous triangular scor-ing scheme containing both a horizontal positive-negativeaxis and a vertical subjective-objective axis.workers to use both a horizontal positive-negativeaxis and a vertical subjective-objective axis by plac-ing the example sentence anywhere inside the trian-gle.
The subjective-objective axis expresses the de-gree to which the sentence contains opinionated con-tent and was earlier used by (Esuli and Sebastiani,2006).
For example, the sentence ?I think this is awonderful car?
clearly marks an opinion and shouldbe positioned towards the subjective end, while thesentence ?The car has six cilinders?
should be lo-cated towards the objective end.
Figure 1d containsan example of HIT3.
In order not to burden theworkers with overly complex instructions, we didnot mention this subjective-objective axis but askedthem instead to place ambiguous sentences towardsthe center of the horizontal positive-negative axisand more objective, non-opinionated sentences to-wards the lower neutral tip of the triangle.115For each of the three HIT designs, we speci-fied the requirement of three different unique as-signments per HIT, which led to a total amount of3 ?
3 ?
1000 = 9000 HIT assignments being up-loaded on MTurk.
Mind that setting the requirementof unique assigments ensures a number of uniqueworkers per individual HIT, but does not ensure aconsistency of workers over a single batch of 1000HITs.
This is in the line with the philosophy ofcrowdsourcing, which allows many different peopleto participate in the same task.4 Annotation Task Results and AnalysisAfter designing the HITs, we uploaded 30 randomsamples for testing purposes.
These HITs were com-pleted in a matter of seconds, mostly by workers inIndia.
After a brief inspection of the results, it wasobvious that most answers corresponded to randomclicks.
Therefore, we decided to include a smallcompetence test to ensure that future workers wouldpossess the necessary linguistic skills to perform thetask.
The test consists of six simple categorisationquestions of the type of HIT1 that a skilled workerwould be able to perform in under a minute.
In orderto discourage the use of automatic translation tools,a time limit of two minutes was imposed and mosttest sentences contain idiomatic constructions thatare known to pose problems to Machine TranslationSystems.4.1 HIT StatisticsTable 1 contains statistics on the workers who com-pleted our HITs.
A total of 19 workers passed thecompetence test and submitted at least one HIT.
Ofthose, four workers completed HITs belonging totwo different designs and six submitted HITs in allthree designs.
Twelve workers are located in the US(64%), three in Spain (16%), one in Mexico (5%),Equador (5%), The Netherlands (5%) and an un-known location (5%).As to a comparison of completion times, it tooka worker on average 11 seconds to complete an in-stance of HIT1, and 9 seconds to complete an in-stance of HIT2 and HIT3.
At first sight, this resultmight seem surprising, since conceptually there is anincrease in complexity when moving from HIT1 toHIT2 and from HIT2 to HIT3.
These results mightOverall HIT1 HIT2 HIT3ID C % # sec.
# sec.
# sec.1 mx 29.9 794 11.0 967 8.6 930 11.62 us 27.6 980 8.3 507 7.8 994 7.43 nl 11.0 85 8.3 573 10.9 333 11.44 us 9.5 853 16.8 - - - -5 es 9.4 - - 579 9.1 265 8.06 ec 4.1 151 9.4 14 16.7 200 13.07 us 3.6 3 15.7 139 8.5 133 11.68 us 2.2 77 8.2 106 7.3 11 10.59 us 0.6 - - - - 50 11.210 us 0.5 43 5.3 1 5 - -11 us 0.4 - - 38 25.2 - -12 us 0.4 - - 10 9.5 27 10.813 es 0.4 - - - - 35 15.114 es 0.3 - - 30 13.5 - -15 us 0.3 8 24.7 18 21.5 - -16 us 0.2 - - - - 22 8.917 us 0.2 - - 17 16.5 - -18 ?
0.1 6 20 - - - -19 us 0.1 - - 1 33 - -Table 1: Statistics on MTurk workers for all three HITdesigns: (fictional) worker ID, country code, % of totalnumber of HITs completed, number of HITs completedper design and average completion time.suggest that users find it easier to classify itemson a graded or continuous scale such as HIT2 andHIT3, which allows for a certain degree of flexibil-ity, than on a stricter categorical template such asHIT1, where there is no room for error.4.2 Annotation DistributionsIn order to get an overview of distribution of the re-sults of each HIT, a histogram was plotted for eachdifferent task.
Figure 2a shows a uniform distribu-tion of the three categories used in the simple cat-egorization scheme of HIT1, as could be expectedfrom a balanced dataset.Figure 2b shows the distribution of the graded cat-egorization template of HIT2.
Compared to the dis-tribution in 2a, two observations can be made: (i)the proportion of the zero values is almost identicalto the proportion of the neutral category in Figure2a, and (ii) the proportion of the sum of the positivevalues [+1,+5] and the proportion of the sum of thenegative values [-5,-1] are equally similar to the pro-portion of the positive and negative categories in 2a.This suggests that in order to map the graded annota-tions of HIT2 to the categories of HIT1, an intuitivepartitioning of the graded scale into three equal partsshould be avoided.
Instead, a more adequate alterna-tive would consist of mapping [-5,-1] to negative, 0116Figure 2: Overview of HIT results: a) distribution of the three categories used in HIT1, b) distribution of results in thescaled format of HIT2, c) heat map of the distribution of results in the HIT3 triangle, d) distribution of projection oftriangle data points onto the X-axis (positive/negative).to neutral and [+1,+5] to positive.
This means thateven slightly positive/negative grades correspond topositive/negative categories.Figure 2c shows a heat map that plots the distri-bution of the annotations in the triangle of HIT3.
Itappears that worker annotations show a spontaneoustendency of clustering, despite the continuous natureof the design.
This suggests that this HIT design,originally conceived as continuous, was transformedby the workers as a simpler categorization task usingfive labels: negative, ambiguous and positive at thetop, neutral at the bottom, and other in the center.Figure 2d shows the distribution of all data-points in the triangle of Figure 2c, projected ontothe X-axis (positive/negative).
Although similar tothe graded scale in HIT2, the distribution shows aslightly higher polarization.These results suggest that, out of all three HIT de-signs, HIT2 is the one that contains the best balancebetween the amount of information that can be ob-tained and the simplicity of a one-dimensional an-notation.4.3 Annotation QualityThe annotation quality of MTurk workers can bemeasured by comparing them to expert annotations.This is usually done by calculating inter-annotatoragreement (ITA) scores.
Note that, since a singleHIT can contain more than one assignment and eachassignment is typically performed by more than oneannotator, we can only calculate ITA scores betweenbatches of assignments, rather than between individ-ual workers.
Therefore, we describe the ITA scoresin terms of batches.
In Table 4.4, we present a com-parison of standard kappa3 calculations (Eugenioand Glass, 2004) between batches of assignments inHIT1 and expert annotations.We found an inter-batch ITA score of 0.598,which indicates a moderate agreement due to fairlyconsistent annotations between workers.
Whencomparing individual batches with expert annota-tions, we found similar ITA scores, in the range be-tween 0.628 and 0.649.
This increase with respectto the inter-batch score suggests a higher variabilityamong MTurk workers than between workers andexperts.
In order to filter out noise in worker annota-tions, we applied a simple majority voting procedurein which we selected, for each sentence in HIT1, themost voted category.
This results in an additional3In reality, we found that fixed and free margin Kappa valueswere almost identical, which reflects the balanced distributionof the dataset.117batch of annotations.
This batch, refered in Table4.4 as Majority, produced a considerably higher ITAscore of 0.716, which confirms the validity of themajority voting scheme to obtain better annotations.In addition, we calculated ITA scores betweenthree expert annotators on a separate, 500-sentencedataset, randomly selected from the same corpus asdescribed at the start of Section 3.
This collectionwas later used as test set in the experiments de-scribed in Section 5.
The inter-expert ITA scoreson this separate dataset contains values of 0.725 for?1 and 0.729 for ?2, only marginally higher than theMajority ITA scores.
Although we are comparingresults on different data sets, these results seem toindicate that multiple MTurk annotations are able toproduce a similar quality to expert annotations.
Thismight suggest that a further increase in the numberof HIT assignments would outperform expert ITAscores, as was previously reported in (Snow et al,2008).4.4 Annotation CostsAs explained in Section 3, a total amount of 9000assignments were uploaded on MTurk.
At a rewardof .02$ per assignment, a total sum of 225$ (180$+ 45$ Amazon fees) was spent on the task.
Work-ers perceived an average hourly rate of 6.5$/hour forHIT1 and 8$/hour for HIT2 and HIT3.
These fig-ures suggest that, at least for assignments of typeHIT2 and HIT3, a lower reward/assignment mighthave been considered.
This would also be consis-tent with the recommendations of (Mason and Watts,2009), who claim that lower rewards might have aneffect on the speed at which the task will be com-pleted - more workers will be competing for the taskat any given moment - but not on the quality.
Sincewe were not certain whether a large enough crowdexisted with the necessary skills to perform our task,we explicitly decided not to try to offer the lowestpossible price.An in-house expert annotator (working at approx-imately 70$/hour, including overhead) finished abatch of 1000 HIT assignments in approximatelythree hours, which leads to a total expert annotatorcost of 210$.
By comparing this figure to the costof uploading 3 ?
1000 HIT assignments (75$), wesaved 210 ?
75 = 135$, which constitutes almost65% of the cost of an expert annotator.
These figuresdo not take into account the costs of preparing thedata and HIT templates, but it can be assumed thatthese costs will be marginal when large data sets areused.
Moreover, most of this effort is equally neededfor preparing data for in-house annotation.
?1 ?2Inter-batch 0.598 0.598Batch 1 vs.
Expert 0.628 0.628Batch 2 vs.
Expert 0.649 0.649Batch 3 vs.
Expert 0.626 0.626Majority vs.
Expert 0.716 0.716Experts4 0.725 0.729Table 2: Interannotation Agreement as a measure of qual-ity of the annotations in HIT1.
?1 = Fixed MarginKappa.
?2 = Free Margin Kappa.5 Incidence of annotations on supervisedpolarity classificationThis section intends to evaluate the incidence ofMTurk-generated annotations on a polarity classifi-cation task.
We present two different evaluations.In section 5.2, we compare the results of traininga polarity classification system with noisy availablemetadata and with MTurk generated annotations ofHIT1.
In section 5.3, we compare the results oftraining several polarity classifiers using differenttraining sets, comparing expert annotations to thoseobtained with MTurk.5.1 Description of datasetsAs was mentioned in Section 3, all sentences wereextracted from a corpus of user opinions on carsfrom the automotive section of www.ciao.es(Spanish).
For conducting the experimental evalu-ation, the following datasets were used:1.
Baseline: constitutes the dataset used for train-ing the baseline or reference classifiers in Ex-periment 1.
Automatic annotation for thisdataset was obtained by using the followingnaive approach: those sentences extracted fromcomments with ratings5 equal to 5 were as-signed to category ?positive?, those extracted5The corpus at www.ciao.es contains consumer opinionsmarked with a score between 1 (negative) and 5 (positive).118from comments with ratings equal to 3 wereassigned to ?neutral?, and those extracted fromcomments with ratings equal to 1 were assignedto ?negative?.
This dataset contains a total of5570 sentences, with a vocabulary coverage of11797 words.2.
MTurk Annotated: constitutes the dataset thatwas manually annotated by MTurk workers inHIT1.
This dataset is used for training the con-trastive classifiers which are to be comparedwith the baseline system in Experiment 1.
Itis also used in various ways in Experiment 2.The three independent annotations generatedby MTurk workers for each sentence within thisdataset were consolidated into one unique an-notation by majority voting: if the three pro-vided annotations happened to be different6,the sentence was assigned to category ?neutral?
;otherwise, the sentence was assigned to the cat-egory with at least two annotation agreements.This dataset contains a total of 1000 sentences,with a vocabulary coverage of 3022 words.3.
Expert Annotated: this dataset contains thesame sentences as the MTurk Annotated one,but with annotations produced internally byknown reliable annotators7.
Each sentence re-ceived one annotation, while the dataset wassplit between a total of five annotators.4.
Evaluation: constitutes the gold standard usedfor evaluating the performance of classifiers.This dataset was manually annotated by threeexperts in an independent manner.
The goldstandard annotation was consolidated by usingthe same criterion used in the case of the pre-vious dataset8.
This dataset contains a total of500 sentences, with a vocabulary coverage of2004 words.6This kind of total disagreement among annotators occurredonly in 13 sentences out of 1000.7While annotations of this kind are necessarily somewhatsubjective, these annotations are guaranteed to have been pro-duced in good faith by competent annotators with an excellentunderstanding of the Spanish language (native or near-nativespeakers)8In this case, annotator inter-agreement was above 80%, andtotal disagreement among annotators occurred only in 1 sen-tence out of 500Baseline Annotated EvaluationPositive 1882 341 200Negative 1876 323 137Neutral 1812 336 161Totals 5570 1000 500Table 3: Sentence-per-category distributions for baseline,annotated and evaluation datasets.These three datasets were constructed by ran-domly extracting sample sentences from an origi-nal corpus of over 25000 user comments contain-ing more than 1000000 sentences in total.
The sam-pling was conducted with the following constraintsin mind: (i) the three resulting datasets should notoverlap, (ii) only sentences containing more than3 tokens are considered, and (iii) each resultingdataset must be balanced, as much as possible, interms of the amount of sentences per category.
Table3 presents the distribution of sentences per categoryfor each of the three considered datasets.5.2 Experiment one: MTurk annotations vs.original Ciao annotationsA simple SVM-based supervised classification ap-proach was considered for the polarity detection taskunder consideration.
According to this, two dif-ferent groups of classifiers were used: a baselineor reference group, and a contrastive group.
Clas-sifiers within these two groups were trained withdata samples extracted from the baseline and anno-tated datasets, respectively.
Within each group ofclassifiers, three different binary classification sub-tasks were considered: positive/not positive, nega-tive/not negative and neutral/not neutral.
All trainedbinary classifiers were evaluated by computing pre-cision and recall for each considered category, aswell as overall classification accuracy, over the eval-uation dataset.A feature space model representation of the datawas constructed by considering the standard bag-of-words approach.
In this way, a sparse vector was ob-tained for each sentence in the datasets.
Stop-wordremoval was not conducted before computing vec-tor models, and standard normalization and TF-IDFweighting schemes were used.Multiple-fold cross-validation was used in allconducted experiments to tackle with statistical vari-119classifier baseline annotatedpositive/not positive 59.63 (3.04) 69.53 (1.70)negative/not negative 60.09 (2.90) 63.73 (1.60)neutral/not neutral 51.27 (2.49) 62.57 (2.08)Table 4: Mean accuracy over 20 independent simula-tions (with standard deviations provided in parenthesis)for each classification subtasks trained with either thebaseline or the annotated dataset.ability of the data.
In this sense, twenty independentrealizations were actually conducted for each exper-iment presented and, instead of individual output re-sults, mean values and standard deviations of evalu-ation metrics are reported.Each binary classifier realization was trained witha random subsample set of 600 sentences extractedfrom the training dataset corresponding to the clas-sifier group, i.e.
baseline dataset for reference sys-tems, and annotated dataset for contrastive systems.Training subsample sets were always balanced withrespect to the original three categories: ?positive?,?negative?
and ?neutral?.Table 4 presents the resulting mean values ofaccuracy for each considered subtask in classifierstrained with either the baseline or the annotateddataset.
As observed in the table, all subtasks ben-efit from using the annotated dataset for trainingthe classifiers; however, it is important to mentionthat while similar absolute gains are observed forthe ?positive/not positive?
and ?neutral/not neutral?subtasks, this is not the case for the subtask ?neg-ative/not negative?, which actually gains much lessthan the other two subtasks.After considering all evaluation metrics, the bene-fit provided by human-annotated data availability forcategories ?neutral?
and ?positive?
is evident.
How-ever, in the case of category ?negative?, althoughsome gain is also observed, the benefit of human-annotated data does not seem to be as much as forthe two other categories.
This, along with the factthat the ?negative/not negative?
subtask is actuallythe best performing one (in terms of accuracy) whenbaseline training data is used, might suggest thatlow rating comments contains a better representa-tion of sentences belonging to category ?negative?than medium and high rating comments do with re-spect to classes ?neutral?
and ?positive?.In any case, this experimental work only verifiesthe feasibility of constructing training datasets foropinionated content analysis, as well as it providesan approximated idea of costs involved in the gener-ation of this type of resources, by using MTurk.5.3 Experiment two: MTurk annotations vs.expert annotationsIn this section, we compare the results of trainingseveral polarity classifiers on six different trainingsets, each of them generated from the MTurk anno-tations of HIT1.
The different training sets are: (i)the original dataset of 1000 sentences annotated byexperts (Experts), (ii) the first set of 1000 MTurk re-sults (Batch1), (iii) the second set of 1000 MTurkresults (Batch2), (iv) the third set of 1000 MTurkresults (Batch3), (v) the batch obtained by major-ity voting between Batch1, Batch2 and Batch3 (Ma-jority), and (vi) a batch of 3000 training instancesobtained by aggregating Batch1, Batch2 and Batch3(All).
We used classifiers as implemented in Mal-let (McCallum, 2002) and Weka (Hall et al, 2009),based on a simple bag-of-words representation ofthe sentences.
As the objective was not to obtainoptimum performance but only to evaluate the dif-ferences between different sets of annotations, allclassifiers were used with their default settings.Table 5 contains results of four different clas-sifiers (Maxent, C45, Winnow and SVM), trainedon these six different datasets and evaluated on thesame 500-sentence test set as explained in Section5.1.
Classification using expert annotations usu-ally outperforms classification using a single batch(one annotation per sentence) of annotations pro-duced using MTurk.
Using the tree annotations persentence available from MTurk, all classifiers reachsimilar or better performance compared to the sin-gle set of expert annotations, at a much lower cost(as explained in section 4.4).It is interesting to note that most classifiers bene-fit from using the full 3000 training examples (1000sentences with 3 annotations each), which intu-itively makes sense as the unanimously labeled ex-amples will have more weight in defining the modelof the corresponding class, whereas ambiguous orunclear cases will have their impact reduced as theircharacteristics are attributed to various classes.On the contrary, Support Vector Machines show120SystemExpertsBatch1Batch2Batch3MajorityAllWinnow 44.2 43.6 40.4 47.6 46.2 50.6SVM 57.6 53.0 55.4 54.0 57.2 52.8C45 42.2 33.6 42.0 41.2 41.6 45.0Maxent 59.2 55.8 57.6 54.0 57.6 58.6Table 5: Accuracy figures of four different classifiers(Winnow, SVM, C45 and Maxent) trained on six differentdatasets (see text for details).an important drop in performance when using mul-tiple annotations, but perform well when using themajority vote.
As a first intuition, this may be due tothe fact that SVMs focus on detecting class bound-aries (and optimizing the margin between classes)rather than developing a model of each class.
Assuch, having the same data point appear severaltimes with the same label will not aid in finding ap-propriate support vectors, whereas having the samedata point with conflicting labels may have a nega-tive impact on the margin maximization.Having only evaluated each classifier (and train-ing set) once on a static test set it is unfortunately notpossible to reliably infer the significance of the per-formance differences (or determine confidence in-tervals, etc.).
For a more in-depth analysis it mightbe interesting to use bootstrapping or similar tech-niques to evaluate the robustness of the results.6 ConclusionsIn this paper we have examined the usefulness ofnon-expert annotations on Amazon?s MechanicalTurk to annotate the polarity of Spanish consumercomments.
We discussed the advantages/drawbacksof three different HIT designs, ranging from a sim-ple categorization scheme to a continous scoringtemplate.
We report high inter-annotator agree-ment scores between non-experts and expert anno-tators and show that training an Opinion MiningSystem with non-expert MTurk annotations outper-forms original noisy annotations and obtains com-petitive results when compared to expert annotationsusing a variety of classifiers.
In conclusion, wefound that, in our case, the use of non-expert anno-tations through crowdsourcing is a viable and cost-effective alternative to the use of expert annotations.In the classification experiments reported in thispaper, we have relied exclusively on MTurk anno-tations from HIT1.
Further work is needed to fullyanalyze the impact of each of the HIT designs forOpinion Mining tasks.
We hope that the added rich-ness of annotation of HIT2 and HIT3 will enable usto use more sophisticated classification methods.ReferencesA.
Esuli and F. Sebastiani.
2006.
SentiWordNet: a pub-licly available lexical resource for opinion mining.
InProceedings of LREC, volume 6.B.
D Eugenio and M. Glass.
2004.
The kappa statistic: Asecond look.
Computational linguistics, 30(1):95101.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18.A.
Kittur, E. H Chi, and B.
Suh.
2008.
Crowdsourcinguser studies with mechanical turk.W.
Mason and D. J Watts.
2009.
Financial incentivesand the performance of crowds.
In Proceedings ofthe ACM SIGKDD Workshop on Human Computation,pages 77?85.A.
K. McCallum.
2002.
Mallet: A machine learning forlanguage toolkit.
http://mallet.cs.umass.edu.R.
Snow, B. O?Connor, D. Jurafsky, and A. Y Ng.
2008.Cheap and fastbut is it good?
: evaluating non-expertannotations for natural language tasks.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 254?263.Q.
Su, D. Pavlov, J. H Chow, and W. C Baker.
2007.Internet-scale collection of human-reviewed data.
InProceedings of the 16th international conference onWorld Wide Web, pages 231?240.121
