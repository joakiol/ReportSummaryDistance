Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 599?608,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPGenerating High-Coverage Semantic Orientation LexiconsFrom Overtly Marked Words and a ThesaurusSaif Mohammad??
?, Cody Dunne?, and Bonnie Dorr???
?Laboratory for Computational Linguistics and Information Processing?Human-Computer Interaction LabInstitute for Advanced Computer Studies?Department of Computer Science?, University of Maryland.Human Language Technology Center of Excellence?
{saif,bonnie}@umiacs.umd.edu and {cdunne}@cs.umd.eduAbstractSentiment analysis often relies on a se-mantic orientation lexicon of positive andnegative words.
A number of approacheshave been proposed for creating such lex-icons, but they tend to be computation-ally expensive, and usually rely on signifi-cant manual annotation and large corpora.Most of these methods use WordNet.
Incontrast, we propose a simple approach togenerate a high-coverage semantic orien-tation lexicon, which includes both indi-vidual words and multi-word expressions,using only a Roget-like thesaurus and ahandful of affixes.
Further, the lexiconhas properties that support the PolyannaHypothesis.
Using the General Inquireras gold standard, we show that our lexi-con has 14 percentage points more correctentries than the leading WordNet-basedhigh-coverage lexicon (SentiWordNet).
Inan extrinsic evaluation, we obtain signifi-cantly higher performance in determiningphrase polarity using our thesaurus-basedlexicon than with any other.
Additionally,we explore the use of visualization tech-niques to gain insight into the our algo-rithm beyond the evaluations mentionedabove.1 IntroductionSentiment analysis involves determining the opin-ions and private states (beliefs, emotions, specu-lations, and so on) of the speaker (Wiebe, 1994).It has received significant attention in recent yearsdue to increasing online opinion content and ap-plications in tasks such as automatic product rec-ommendation systems (Tatemura, 2000; Terveenet al, 1997), question answering (Somasundaranet al, 2007; Lita et al, 2005), and summarizingmultiple view points (Seki et al, 2004) and opin-ions (Mohammad et al, 2008a).A crucial sub-problem is to determine whetherpositive or negative sentiment is expressed.
Auto-matic methods for this often make use of lexiconsof words tagged with positive and negative seman-tic orientation (Turney, 2002; Wilson et al, 2005;Pang and Lee, 2008).
A word is said to have apositive semantic orientation (SO) (or polarity)if it is often used to convey favorable sentimentor evaluation of the topic under discussion.
Someexample words that have positive semantic orien-tation are excellent, happy, honest, and so on.
Sim-ilarly, a word is said to have negative semantic ori-entation if it is often used to convey unfavorablesentiment or evaluation of the target.
Examplesinclude poor, sad, and dishonest.Certain semantic orientation lexicons have beenmanually compiled for English?the most notablebeing the General Inquirer (GI) (Stone et al,1966).1However, the GI lexicon has orientationlabels for only about 3,600 entries.
The Pitts-burgh subjectivity lexicon (PSL) (Wilson et al,2005), which draws from the General Inquirer andother sources, also has semantic orientation labels,but only for about 8,000 words.Automatic approaches to creating a seman-tic orientation lexicon and, more generally, ap-proaches for word-level sentiment annotation canbe grouped into two kinds: (1) those that relyon manually created lexical resources?most ofwhich use WordNet (Strapparava and Valitutti,2004; Hu and Liu, 2004; Kamps et al, 2004; Taka-mura et al, 2005; Esuli and Sebastiani, 2006; An-1http://www.wjh.harvard.edu/ inquirer599dreevskaia and Bergler, 2006; Kanayama and Na-sukawa, 2006); and (2) those that rely on text cor-pora (Hatzivassiloglou and McKeown, 1997; Tur-ney and Littman, 2003; Yu and Hatzivassiloglou,2003; Grefenstette et al, 2004).
Many of theselexicons, such as SentiWordNet (SWN) (Esuliand Sebastiani, 2006) were created using super-vised classifiers and significant manual annota-tion, whereas others such as the Turney andLittman lexicon (TLL) (2003) were created fromvery large corpora (more than 100 billion words).In contrast, we propose a computationallyinexpensive method to compile a high-coveragesemantic orientation lexicon without the use ofany text corpora or manually annotated semanticorientation labels.
Both of these resources maybe used, if available, to further improve results.The lexicon has about twenty times the numberof entries in the GI lexicon, and it includes en-tries for both individual words and common multi-word expressions.
The method makes use of aRoget-like thesaurus and a handful of antonym-generating affix patterns.
Whereas thesauri havelong been used to estimate semantic distance (Jar-masz and Szpakowicz, 2003; Mohammad andHirst, 2006), the closest thesaurus-based work onsentiment analysis is by Aman and Szpakowicz(2007) on detecting emotions such as happiness,sadness, and anger.
We evaluated our thesaurus-based algorithm both intrinsically and extrinsi-cally and show that the semantic orientation lex-icon it generates has significantly more correct en-tries than the state-of-the-art high-coverage lexi-con SentiWordNet, and that it has a significantlyhigher coverage than the General Inquirer andTurney?Littman lexicons.In Section 2 we examine related work.
Section 3presents our algorithm for creating semantic orien-tation lexicons.
We describe intrinsic and extrin-sic evaluation experiments in Section 4, followedby a discussion of the results in Section 5.
Ad-ditionally, in Section 6 we show preliminary vi-sualizations of how our algorithm forms chains ofpositive and negative thesaurus categories.
Goodvisualizations are not only effective in presentinginformation to the user, but also help us better un-derstand our algorithm.
Section 7 has our conclu-sions.2 Related WorkPang and Lee (2008) provide an excellent surveyof the literature on sentiment analysis.
Here webriefly describe the work closest to ours.Hatzivassiloglou and McKeown (1997) pro-posed a supervised algorithm to determine the se-mantic orientation of adjectives.
They first gen-erate a graph that has adjectives as nodes.
Anedge between two nodes indicates either that thetwo adjectives have the same or opposite seman-tic orientation.
A clustering algorithm partitionsthe graph into two subgraphs such that the nodesin a subgraph have the same semantic orientation.The subgraph with adjectives that occur more of-ten in text is marked positive and the other neg-ative.
They used a 21 million word corpus andevaluated their algorithm on a labeled set of 1336adjectives (657 positive and 679 negative).
Ourapproach does not require manually annotated se-mantic orientation entries to train on and is muchsimpler.Esuli and Sebastiani (2006) used a supervisedalgorithm to attach semantic orientation scores toWordNet glosses.
They train a set of ternary clas-sifiers using different training data and learningmethods.
The set of semantic orientation scoresof all WordNet synsets is released by the nameSentiWordNet.2An evaluation of SentiWordNetby comparing orientation scores of about 1,000WordNet glosses to scores assigned by human an-notators is presented in Esuli (2008).
Our ap-proach uses a Roget-like thesaurus, and it does notuse any supervised classifiers.Turney and Littman (2003) proposed a mini-mally supervised algorithm to calculate the se-mantic orientation of a word by determining ifits tendency to co-occur with a small set of pos-itive words is greater than its tendency to co-occurwith a small set of negative words.
They showthat their approach performs better when it has alarge amount of text at its disposal.
They use textfrom 350 million web-pages (more than 100 bil-lion words).
Our approach does not make use ofany text corpora, although co-occurrence statisticscould be used to further improve the lexicon.
Fur-thermore, our lexicon has entries for commonlyused multi-word expressions as well.Mohammad et al (2008b) developed a methodto determine the degree of antonymy (contrast)between two words using the Macquarie The-2http://sentiwordnet.isti.cnr.it/600saurus (Bernard, 1986), co-occurrence statistics,and a small set of antonym-generating affix pat-terns such as X?disX.
Often, one member of a pairof contrasting terms is positive and one member isnegative.
In this paper, we describe how a subsetof those affix patterns can be used in combinationwith a thesaurus and the edicts of marking the-ory to create a large lexicon of words and phrasesmarked with their semantic orientation.3 Generating the Semantic OrientationLexiconOur algorithm to generate a semantic orientationlexicon has two steps: (1) identify a seed set ofpositive and negative words; (2) use a Roget-likethesaurus to mark the words synonymous with thepositive seeds ?positive?
and words synonymouswith the negative seeds ?negative?.
The two stepsare described in the subsections below.
Our im-plementation of the algorithm used the MacquarieThesaurus (Bernard, 1986).
It has about 100,000unique words and phrases.3.1 Seed words3.1.1 Automatically identifying seed wordsIt is known from marking theory that overtlymarked words, such as dishonest, unhappy, andimpure, tend to have negative semantic orienta-tion, whereas their unmarked counterparts, hon-est, happy, and pure, tend to have positive seman-tic orientation (Lehrer, 1974; Battistella, 1990).Exceptions such as biased?unbiased and partial?impartial do exist, and in some contexts even apredominantly negative marked word may be pos-itive.
For example irreverent is negative in mostcontexts, but positive in the sentence below:Millions of fans follow Moulder?s irrev-erent quest for truth.However, as we will show through experiments,the exceptions are far outnumbered by those thatabide by the predictions of marking theory.We used a set of 11 antonym-generating af-fix patterns to generate overtly marked words andtheir unmarked counterparts (Table 1).
Similarantonyms-generating affix patterns exist for manylanguages (Lyons, 1977).
The 11 chosen af-fix patterns generated 2,692 pairs of marked andunmarked valid English words that are listed inthe Macquarie Thesaurus.
The marked wordsAffix pattern # wordw1w2pairs example word pairX disX 382 honest?dishonestX imX 196 possible?impossibleX inX 691 consistent?inconsistentX malX 28 adroit?maladroitX misX 146 fortune?misfortuneX nonX 73 sense?nonsenseX unX 844 happy?unhappyX Xless 208 gut?gutlesslX illX 25 legal?illegalrX irX 48 responsible?irresponsibleXless Xful 51 harmless?harmfulTotal 2692Table 1: Eleven affix patterns used to generate theseed set of marked and unmarked words.
Here ?X?stands for any sequence of letters common to bothwords w1and w2.are deemed negative and the unmarked ones pos-itive, and these form our seed set of positiveand negative words.
We will refer to this setof orientation-marked words as the affix seedslexicon (ASL).
Note that some words may havemultiple marked counterparts, for example, trust?trustless and trust?mistrust.
Thus, ASL has morenegative words (2,652) than positive ones (2,379).Also, the Xless?Xful pattern generates word pairsthat are both overtly marked; words generatedfrom Xless are deemed negative and words gen-erated from Xful are deemed positive.It should be noted that the affix patterns usedhere are a subset of those used in Mohammad etal.
(2008b) to generate antonym pairs.
The affixpatterns ignored are those that are not expectedto generate pairs of words with opposite seman-tic orientation.
For instance, the pattern imX-exXgenerates word pairs such as import?export andimplicit?explicit that are antonymous, but do nothave opposite semantic orientations.3.1.2 Using manually annotated seed wordsSince manual semantic orientation labels exist forsome English words (the GI lexicon), we inves-tigated their usefulness in further improving thecoverage and correctness of the entries in our lex-icon.
We used the GI words as seeds in the sameway as the words generated from the affix patternswere used (Section 3.1.1).3.2 Generalizing from the seedsA published thesaurus such as the Roget?s or Mac-quarie has about 1,000 categories, each consist-ing of on average 120 words and commonly used601Mode ofSO lexicon creation Resources used # entries # positives # negativesASL automatic 11 affix rules 5,031 2,379 (47.3%) 2,652 (52.7%)GI manual human SO annotation 3,605 1,617 (44.9%) 1,988 (55.1%)GI-subset manual human SO annotation 2,761 1,262 (45.7%) 1,499 (54.3%)MSOL(ASL) automatic thesaurus, 11 affix rules 51,157 34,152 (66.8%) 17,005 (33.2%)MSOL(GI) automatic GI, thesaurus 69,971 25,995 (37.2%) 43,976 (62.8%)MSOL(ASL and GI) automatic GI, thesaurus, 11 affix rules 76,400 30,458 (39.9%) 45,942 (60.1%)PSL mostly manual GI, other sources 6,450 2,298 (35.6%) 4,485 (64.4%)SWN automatic human SO annotation, 56,200 47,806 (85.1%) 8,394 (14.9%)WordNet, ternary classifiersTLL automatic 100 billion word corpus, 3,596 1,625 (45.2%) 1,971 (54.8%)minimal human SO annotationTable 2: Key details of semantic orientation (SO) lexicons.
ASL = affix seeds lexicon, GI = GeneralInquirer, MSOL = Macquarie semantic orientation lexicon, PSL = Pitt subjectivity lexicon, SWN =SentiWordNet, TLL = Turney?Littman lexicon.multi-word expressions.
Terms within a cate-gory tend to be closely related, and they are fur-ther grouped into sets of near-synonymous wordsand phrases called paragraphs.
There are about10,000 paragraphs in most Roget-like thesauri.Every thesaurus paragraph is examined to deter-mine if it has a seed word (by looking up the seedlexicon described in Section 3.1).
If a thesaurusparagraph has more positive seed words than neg-ative seed words, then all the words (and multi-word expressions) in that paragraph are marked aspositive.
Otherwise, all its words are marked neg-ative.Note that this method assigns semantic orienta-tion labels to word?thesaurus paragraph pairs.Thesaurus paragraphs can be thought of as wordsenses.
A word with multiple meanings is listedin multiple thesaurus paragraphs, and so will beassigned semantic orientation labels for each ofthese paragraphs.
Thus, the method assigns a se-mantic orientation to a word?sense combinationsimilar to the SentiWordNet approach and differ-ing from the General Inquirer and Turney?Littmanlexicons.However, in most natural language tasks, the in-tended sense of the target word is not explicitlymarked.
So we generate a word-based lexicon byasking the different senses of a word to vote.
Ifa word is listed in multiple thesaurus paragraphs,then the semantic orientation label most commonto them is chosen as the word?s label.
We will re-fer to this set of word?semantic orientation pairsas the Macquarie Semantic Orientation Lexicon(MSOL).
A set created from only the affix seedswill be called MSOL(ASL), a set created fromonly the GI seeds will be called MSOL(GI), andthe set created using both affix seeds and GI seedswill be called MSOL(ASL and GI).3We gener-ated a similar word-based lexicon for SentiWord-Net (SWN) by choosing the semantic orientationlabel most common to the synsets pertaining to atarget word.Table 2 summarizes the details of all the lex-icons.
MSOL(ASL and GI) has a much largerpercentage of negatives than MSOL(ASL) be-cause GI has a much larger percentage of negativewords.
These negative seeds generate many morenegative entries in MSOL(ASL and GI).
Of the51,157 entries in MSOL(ASL), 47,514 are single-word entries and 3,643 are entries for multi-wordexpressions.
Of the 69,971 entries in MSOL(GI),45,197 are single-word entries and 24,774 are en-tries for common multi-word expressions.
Of the76,400 entries in MSOL(ASL and GI), 51,208are single-word entries and 25,192 are entries forcommon multi-word expressions.
In our evalua-tion, we used only the single-word entries to main-tain a level playing field with other lexicons.4 EvaluationWe evaluated the semantic orientation lexiconsboth intrinsically (by comparing their entries to theGeneral Inquirer) and extrinsically (by using themin a phrase polarity annotation task).4.1 Intrinsic: Comparison with GISimilar to how Turney and Littman (2003) evalu-ated their lexicon (TLL), we determine if the se-mantic orientation labels in the automatically gen-erated lexicons match the semantic orientation la-3MSOL is publicly available at: www.umiacs.umd.edu/?saif/WebPages/ResearchInterests.html.602Lexicon All Positives NegativesMSOL(ASL) 74.3 84.2 65.9SWN 60.1 86.5 37.9TLL 83.3 83.8 82.8Table 3: The percentage of GI-subset entries (all,only the positives, only the negatives) that matchthose of the automatically generated lexicons.bels of words in GI.
GI, MSOL(ASL), SWN, andTLL all have 2,761 words in common.
We willcall the corresponding 2,761 GI entries the GI-subset.Table 3 shows the percentage of GI-subset en-tries that match those of the three automatically-generated lexicons (MSOL(ASL), SWN, andTLL).
(The differences in percentages shown inthe table are all statistically significant?p <0.001.)
We do not show results for MSOL(GI),MSOL(ASL and GI), and the Pittsburgh subjectiv-ity lexicon (PSL) because these lexicons were cre-ated using GI entries.
TLL most closely matchesthe GI-subset, and MSOL matches the GI-subsetmore closely than SWN with the GI-subset.
How-ever, the goal of this work is to produce a high-coverage semantic orientation lexicon and so weadditionally evaluate the lexicons on the extrinsictask described below.4.2 Extrinsic: Identifying phrase polarityThe MPQA corpus contains news articles man-ually annotated for opinions and private states.4Notably, it also has polarity annotations (posi-tive/negative) at the phrase-level.
We conductedan extrinsic evaluation of the manually-generatedand automatically-generated lexicons by usingthem to determine the polarity of phrases in theMPQA version 1.1 collection of positive and neg-ative phrases (1,726 positive and 4,485 negative).We used a simple algorithm to determine thepolarity of a phrase: (1) If any of the words inthe target phrase is listed in the lexicon as havingnegative semantic orientation, then the phrase ismarked negative.
(2) If none of the words in thephrase is negative and if there is at least one posi-tive word in the phrase, then it is marked positive.
(3) In all other instances, the classifier refrainsfrom assigning a tag.
Indeed better accuracies inphrase semantic orientation annotation can be ob-tained by using supervised classifiers and moresophisticated context features (Choi and Cardie,4http://www.cs.pitt.edu/mpqa2008).
However, our goal here is only to use thistask as a testbed for evaluating different seman-tic orientation lexicons, and so we use the methoddescribed above to avoid other factors from influ-encing the results.Table 4 shows the performance of the algorithmwhen using different lexicons.
The performancewhen using lexicons that additionally make useof GI entries?MSOL(GI), MSOL(ASL and GI),PSL, and a combined GI-SWN lexicon?is shownlower down in the table.
GI?SWN has entriesfrom both GI and SWN.
(For entries with oppos-ing labels, the GI label was chosen since GI en-tries were created manually.)
Observe that the bestF-scores are obtained when using MSOL (in bothcategories?individual lexicons and combinationswith GI).
The values are significantly better thanthose attained by others (p < 0.001).5 DiscussionThe extrinsic evaluation shows that our thesaurus-and affix-based lexicon is significantly more accu-rate than SentiWordNet.
Moreover, it has a muchlarger coverage than the GI and Pitt lexicons.
Ob-serve also that the affix seeds set, by itself, attainsonly a modest precision and a low recall.
This isexpected because it is generated by largely auto-matic means.
However, the significantly higherMSOL performance suggests that the generaliza-tion step (described in Section 3.2) helps improveboth precision and recall.
Precision is improvedbecause multiple seed words vote to decide the se-mantic orientation of a thesaurus paragraph.
Re-call improves simply because non-seed words ina paragraph are assigned the semantic orientationthat is most prevalent among the seeds in the para-graph.5.1 Support for the Polyanna HypothesisBoucher and Osgood?s (1969) Polyanna Hypoth-esis states that people have a preference for usingpositive words and expressions as opposed to us-ing negative words and expressions.
Studies haveshown that indeed speakers across languages usepositive words much more frequently than nega-tive words (Kelly, 2000).
The distribution of pos-itive and negative words in MSOL(ASL) furthersupports the Polyanna Hypothesis as it shows thateven if we start with an equal number of positiveand negative seed words, the expansion of the pos-itive set through the thesaurus is much more pro-603All phrases Only positives Only negativesSO lexicon P R F P R F P R FIndividual lexiconsASL 0.451 0.165 0.242 0.451 0.165 0.242 0.192 0.063 0.095GI 0.797 0.323 0.459 0.871 0.417 0.564 0.763 0.288 0.419MSOL(ASL) 0.623 0.474 0.539 0.631 0.525 0.573 0.623 0.458 0.528SWN 0.541 0.408 0.465 0.745 0.624 0.679 0.452 0.328 0.380TLL 0.769 0.298 0.430 0.761 0.352 0.482 0.775 0.279 0.411Automatic lexicons + GI informationMSOL(GI) 0.713 0.540 0.615 0.572 0.470 0.516 0.777 0.571 0.658MSOL(ASL and GI) 0.710 0.546 0.617 0.577 0.481 0.525 0.771 0.574 0.658PSL 0.823 0.422 0.558 0.860 0.487 0.622 0.810 0.399 0.535GI-SWN 0.650 0.494 0.561 0.740 0.623 0.677 0.612 0.448 0.517Table 4: Performance in phrase polarity tagging.
P = precision, R = recall, F = balanced F-score.
Thebest F-scores in each category are marked in bold.nounced than the expansion of the negative set.
(About 66.8% of MSOL(ASL) words are positive,whereas only 33.2% are negative.)
This suggeststhat there are many more near-synonyms of pos-itive words than near-synonyms of negative ones,and so there are many more forms for expressingpositive sentiments than forms for expressing neg-ative sentiment.5.2 LimitationsSome of the errors in MSOL were due to non-antonymous instantiations of the affix patterns.For example, immigrate is not antonymous to mi-grate.
Other errors occur because occasionally thewords in the same thesaurus paragraph have dif-fering semantic orientations.
For example, oneparagraph has the words slender and slim (which,many will agree, are positive) as well as the wordswiry and lanky (which many will deem negative).Both these kinds of errors can be mitigated using acomplementary source of information, such as co-occurrence with other known positive and negativewords (the Turney?Littman method).5.3 Future workTheoretically, a much larger Turney?Littman lex-icon can be created even though it may be com-putationally intensive when working with 100 bil-lion words.
However, MSOL and TLL are createdfrom different sources of information?MSOLfrom overtly marked words and a thesaurus, andTLL from co-occurrence information.
Therefore,a combination of the two approaches is expectedto produce an even more accurate semantic orien-tation lexicon, even with a modest-sized corpus atits disposal.
This is especially attractive for lowresource languages.
We are also developing meth-ods to leverage the information in an English the-saurus to create semantic orientation lexicons for alow-resource language through the use of a bilin-gual lexicon and a translation disambiguation al-gorithm.6 Visualizing the semantic orientation ofthesaurus categoriesIn recent years, there have been substantial de-velopments in the field of information visualiza-tion, and it is becoming increasingly clear thatgood visualizations can not only convey informa-tion quickly, but are also an important tool forgaining insight into an algorithm, detecting sys-tematic errors, and understanding the task.
In thissection, we present some preliminary visualiza-tions that are helping us understand our approachbeyond the evaluations described above.As discussed in Section 3.1.1, the affix seedsset connects the thesaurus words with opposite se-mantic orientation.
Usually these pairs of wordsoccur in different thesaurus categories, but this isnot necessary.
We can think of these connectionsas relationships of contrast in meaning and seman-tic orientation, not just between the two wordsbut also between the two categories.
To betteraid our understanding of the automatically deter-mined category relationships we visualized thisnetwork using the Fruchterman-Reingold force-directed graph layout algorithm (Fruchterman andReingold, 1991) and the NodeXL network analy-sis tool (Smith et al, 2009)5.Our dataset consists of 812 categories from theMacquarie Thesaurus and 27,155 antonym edgesbetween them.
There can be an edge from a cat-5Available from http://www.codeplex.com/NodeXL604Figure 1: After removing edges with low weight we can see the structure the network backbone.
Isolatecategory pairs are drawn in a ring around the main connected component and singletons are staggeredin the corners.
Each node is colored by its semantic orientation (red for negative, blue for positive)and edges are colored by their weight, from red to blue.
Node shape also codes semantic orientation,with triangles positive and circles negative.
Size codes the magnitude the semantic orientation, with thelargest nodes representing the extremes.
Node labels are shown for nodes in isolates and those in the top20 for betweenness centrality.egory to itself called a self-edge, indicating thata word and its antonym (with opposite seman-tic orientation) both exist in the same category.There can be multiple edges between two cate-gories indicating that one or more words in onecategory have one or more antonyms in the othercategory.
These multiple edges between categorypairs were merged together resulting in 14,597weighted meta-edges.
For example, if there are nedges between a category pair they were replacedby a single meta-edge of weight n.The network is too dense and interconnectedfor force-directed placement to generate a usefulpublication-size drawing of the entire network.
Byremoving edges that had a weight less than 6, wecan visualize a smaller and more understandable540 edge network of the core categories and anynew isolates created.
Additionally, we show onlyedges between categories with opposite semanticorientations (Figure 1).
Observe that there arethree groups of nodes: those in the core connectedcomponent, the small isolates in the ring surround-ing it, and the connectionless singletons arrangedin the corners.Each node c (thesaurus category) is colored ona red to blue continuous scale according to its se-mantic orientation SO, which is computed purelyfrom its graph structure (in-degree ID and out-degree OD):SO(c) =OD(c)?
ID(c)OD(c) + ID(c)(1)Blue nodes represent categories with many pos-itive words; we will call them positive cate-605gories (p).
Red nodes are categories with manynegative words; we will call them negative cate-gories (n).
Shades of purple in between are cat-egories that have words with both positive andnegative semantic orientation (mixed categories).Similarly, edges are colored according to theirweight from red (small weight) to blue (largeweight).
We also use shape coding for seman-tic orientation, with triangles being positive andcircles negative, and the size of the node depictsthe magnitude of the semantic orientation.
Forexample, the pair HEARING(p)?DEAFNESS(n) inthe top left of Figure 1 represent the two size ex-tremes: HEARING has a semantic orientation of 1and DEAFNESS has a score of -1.
The mixed cat-egories with near 0 semantic orientation such asLIKELIHOOD with a score of .07 are the smallest.Nodes are labeled by the thesaurus-providedhead words?a word or phrase that best representsthe coarse meaning of the category.
For read-ability, we have restricted the labels to nodes inthe isolates and the top 20 nodes in the core con-nected component that have the highest between-ness centrality, which means they occur on moreshortest paths between other nodes in the network(i.e., they are the bridges or gatekeepers).From the ring of isolates we can see howmany antonymous categories, and their se-mantic orientations, are correctly recognized.For example, ASSERTION(p)?DENIAL(n),HEARING(p)?DEAFNESS(n), GRATEFULNESS(p)?UNGRATEFULNESS(n), and so on.
Some codingsmay seem less intuitive, such as those in the core,but much of this is the effect of abstracting awaythe low weight edges, which may have moreclearly identified the relationships.An alternative approach to removing edges withlow weight is to filter categories in the networkbased on graph-theoretic metrics like betweennesscentrality, closeness centrality, and eigenvectorcentrality.
We discussed betweenness central-ity before.
The closeness centrality of a node isthe average distance along the shortest path be-tween that node and all other nodes reachable fromit.
Eigenvector centrality is another measure ofnode importance, assigning node score based onthe idea that connections to high-scoring nodes aremore important than those to low-scoring ones.We removed nodes with less than 0.1 between-ness centrality, less than 0.04 eigenvector central-ity, and above 2.1 closeness centrality, leavingthe key 56 nodes.
They have 497 edges betweenthem, of which we show only those between cat-egories with opposite semantic orientations (Fig-ure 2).
Node and edge color, size, and shape cod-ing is as before.Observe that most of these categories have astrongly evaluative nature.
Also, as our algorithmmakes connections using overt negative markers,it makes sense that the central categories in ournetwork have negative orientation (negative cat-egories have many words with overt markings).It is interesting, though, how some positive andmixed categories reside in the core too.
Further in-spection revealed that these categories have a largenumber of words within them.
For example, itmay be less intuitive as to why the category of MU-SIC is listed in the core, but this is because it hasabout 1,200 words in it (on average, each categoryhas about 120 words), and because many of thesewords, such as harmonious(p), melodious(n), andlament(n) are evaluative in nature.7 ConclusionWe created a high-coverage semantic orientationlexicon using only affix rules and a Roget-likethesaurus.
The method does not require termswith manually annotated semantic orientation la-bels, though we show that if available they can beused to further improve both the correctness of itsentries and its coverage.
The lexicon has abouttwenty times as many entries as in the General In-quirer and the Turney?Littman lexicons, and in-cludes entries for both individual words and com-mon multi-word expressions.
Experiments showthat it has significantly more correct entries thanSentiWordNet.
The approach is complementary tothat of Turney and Littman (2003) and a combina-tion of this approach with co-occurrence statistics(even if drawn from a modest sized corpus) is ex-pected to yield an even better lexicon.Visualization of the thesaurus categories as perthe semantic orientations assigned to them by ouralgorithm reveals that affix patterns produce astrongly connected graph and that indeed there aremany long chains of positive and negative cate-gories.
Furthermore, the key categories of thisgraph (the ones with high centrality and closeness)are strongly evaluative in nature, and most of themtend to have negative semantic orientation.606Figure 2: After filtering out nodes based on graph-theoretic metrics, the core of the network becomesvisible.
The visualization is colored as in Figure 1, and we can see how the core is dominated bycategories with negative semantic orientation (red).
Shape, size, and color coding is as before.AcknowledgmentsWe thank Douglas W. Oard, Ben Schneiderman,Judith Klavans and the anonymous reviewers fortheir valuable feedback.
This work was supported,in part, by the National Science Foundation un-der Grant No.
IIS-0705832, in part, by the HumanLanguage Technology Center of Excellence, andin part, by Microsoft Research for the NodeXLproject.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the sponsor.ReferencesSaima Aman and Stan Szpakowicz.
2007.
Identify-ing expressions of emotion in text.
Text, Speech andDialogue, 4629:196?205.Alina Andreevskaia and Sabine Bergler.
2006.
MiningWordNet for fuzzy sentiment: Sentiment tag extrac-tion from WordNet glosses.
In Proceedings of theEACL, Trento, Italy.Edwin Battistella.
1990.
Markedness: The Evalua-tive Superstructure of Language.
State Universityof New York Press, Albany, New York.John R. L. Bernard, editor.
1986.
The Macquarie The-saurus.
Macquarie Library, Sydney, Australia.Jerry D. Boucher and Charles E. Osgood.
1969.
Thepollyanna hypothesis.
Journal of Verbal Learningand Verbal Behaviour, 8:1?8.Yejin Choi and Claire Cardie.
2008.
Learning withcompositional semantics as structural inference forsubsentential sentiment analysis.
In Proceedings ofEmpirical Methods in Natural Language Processing(EMNLP-2008), Waikiki, Hawaii.Andrea Esuli and Fabrizio Sebastiani.
2006.
Senti-WordNet: A publicly available lexical resource foropinion mining.
In Proceedings of LREC, pages417?422, Genoa, Italy.Andrea Esuli.
2008.
Automatic Generation of LexicalResources for Opinion Mining: Models, Algorithmsand Applications.
Ph.D. thesis, Department of Infor-mation Engineering, University of Pisa, Pisa, Italy.Thomas M. J. Fruchterman and Edward M. Reingold.1991.
Graph drawing by force-directed placement.Software: Practice and Experience, 21(11):1129?1164.607Gregory Grefenstette, Yan Qu, David Evans, and JamesShanahan.
2004.
Validating the coverage of lex-ical resources for affect analysis and automaticallyclassifying new words along semantic axes.
InJames Shanahan Yan Qu and Janyce Wiebe, editors,Exploring Attitude and Affect in Text: Theories andApplications, AAAI-2004 Spring Symposium Series,pages 71?78, San Jose, California.Vasileios Hatzivassiloglou and Kathleen McKeown.1997.
Predicting the semantic orientation of ad-jectives.
In Proceedings of EACL, pages 174?181,Madrid, Spain.Minqing Hu and Bing Liu.
2004.
Mining andsummarizing customer reviews.
In Proceedings ofACM SIGKDD International ConferenceDiscoveryand Data Mining (KDD-04), Seattle, WA.Mario Jarmasz and Stan Szpakowicz.
2003.
Ro-get?s Thesaurus and semantic similarity.
In Pro-ceedings of the International Conference on RecentAdvances in Natural Language Processing (RANLP-2003), pages 212?219, Borovets, Bulgaria.Jaap Kamps, Maarten Marx, Robert J. Mokken, andMaarten de Rijke.
2004.
Using WordNet to mea-sure semantic orientation of adjectives.
In LREC.Hiroshi Kanayama and Tetsuya Nasukawa.
2006.Fully automatic lexicon expansion for domain-oriented sentiment analysis.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 355?363, Syd-ney, Australia, July.
Association for ComputationalLinguistics.Michael H. Kelly.
2000.
Naming on the bright side oflife.
volume 48, pages 3?26.Adrienne Lehrer.
1974.
Semantic fields and lexicalstructure.
North-Holland; American Elsevier, Ams-terdam and New York.Lucian Vlad Lita, Andrew Hazen Schlaikjer, We-iChang Hong, and Eric Nyberg.
2005.
Qualita-tive dimensions in question answering: Extendingthe definitional QA task.
In Proceedings of AAAI,pages 1616?1617.
Student abstract.John Lyons.
1977.
Semantics, volume 1.
CambridgeUniversity Press.Saif Mohammad and Graeme Hirst.
2006.
Distribu-tional measures of concept-distance: A task-orientedevaluation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP-2006), pages 35?43, Sydney, Australia.Saif Mohammad, Bonnie Dorr, Melissa Egan, JimmyLin, and David Zajic.
2008a.
Multiple alternativesentence compressions and word-pair antonymy forautomatic text summarization and recognizing tex-tual entailment.
In Proceedings of the Text AnalysisConference (TAC-2008), Gaithersburg, MD.Saif Mohammad, Bonnie Dorr, and Graeme Hirst.2008b.
Computing word-pair antonymy.
In Pro-ceedings of the Conference on Empirical Methodsin Natural Language Processing, Waikiki, Hawaii.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1?2):1?135.Yohei Seki, Koji Eguchi, and Noriko Kando.
2004.Analysis of multi-document viewpoint summariza-tion using multi-dimensional genres.
In Proceed-ings of the AAAI Spring Symposium on ExploringAttitude and Affect in Text: Theories and Applica-tions, pages 142?145.Marc Smith, Ben Shneiderman, Natasa Milic-Frayling,Eduarda Mendes Rodrigues, Vladimir Barash, CodyDunne, Tony Capone, Adam Perer, and Eric Gleave.2009.
Analyzing (social media) networks withNodeXL.
In C&T ?09: Proc.
Fourth InternationalConference on Communities and Technologies, Lec-ture Notes in Computer Science.
Springer.Swapna Somasundaran, Theresa Wilson, JanyceWiebe, and Veselin Stoyanov.
2007.
QA with atti-tude: Exploiting opinion type analysis for improvingquestion answering in on-line discussions and thenews.
In Proceedings of the International Confer-ence on Weblogs and Social Media (ICWSM).Philip Stone, Dexter Dunphy, Marshall Smith, andDaniel Ogilvie.
1966.
The General Inquirer: AComputer Approach to Content Analysis.
MIT.Carlo Strapparava and Alessandro Valitutti.
2004.WordNet-affect: and affective extension of Word-Net.
In Proceedings of LREC, Lisbon, Portugal.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientation of words us-ing spin model.
In Proceedings of the Associationfor Computational Linguistics (ACL), pages 133?140.Junichi Tatemura.
2000.
Virtual reviewers for collabo-rative exploration of movie reviews.
In Proceedingsof Intelligent User Interfaces (IUI), pages 272?275.Loren Terveen, Will Hill, Brian Amento, David Mc-Donald, and Josh Creter.
1997.
PHOAKS: A systemfor sharing recommendations.
Communications ofthe Association for Computing Machinery (CACM),40(3):59?62.Peter Turney and Michael Littman.
2003.
Measuringpraise and criticism: Inference of semantic orienta-tion from association.
ACM Transactions on Infor-mation Systems (TOIS), 21(4):315?346.Peter Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of ACL, pages417?424, Philadelphia, Pennsylvania.Janyce M. Wiebe.
1994.
Tracking point of view in nar-rative.
Computational Linguistics, 20(2):233?287.Theresa Wilson, Janyce Wiebe, and Paul Hoffman.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of HLT-EMNLP, pages 347?354, Vancouver, Canada.Hong Yu and Vassileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separating factsfrom opinions and identifying the polarity of opinionsentences.
In Proceedings of EMNLP, pages 129?136, Morristown, NJ.608
