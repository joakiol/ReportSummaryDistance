Bio-Medical Entity Extraction using Support Vector MachinesKoichi Takeuchi and Nigel CollierNational Institute of Informatics2-1-2 Hitotsubashi, Chiyoda-kuTokyo 101-8430, Japan{koichi,collier}@nii.ac.jpAbstractSupport Vector Machines have achievedstate of the art performance in several clas-sification tasks.
In this article we applythem to the identification and semantic an-notation of scientific and technical termi-nology in the domain of molecular biol-ogy.
This illustrates the extensibility ofthe traditional named entity task to spe-cial domains with extensive terminologiessuch as those in medicine and related dis-ciplines.
We illustrate SVM?s capabilitiesusing a sample of 100 journal abstractstexts taken from the {human, blood cell,transcription factor} domain of MED-LINE.
Approximately 3400 terms are an-notated and the model performs at about74% F-score on cross-validation tests.
Adetailed analysis based on empirical ev-idence shows the contribution of variousfeature sets to performance.1 IntroductionWith the rapid growth in the number of publishedpapers in the scientific fields such as medicine therehas been growing interest in the application of In-formation Extraction (IE), (Thomas et al, 1999)(Craven and Kumlien, 1999), to help solve someof the problems that are associated with informa-tion overload.
IE can benefit the medical sciencesby enabling the automatic extraction of facts relatedto prototypical events such as those contained in pa-tient records or research articles regarding molecularprocesses and their affect on human health.
Thesefacts can then be used to populate databases, aid insearching or document summarization and a varietyof tasks which require the computer to have an in-telligent understanding of the contents inside a doc-ument.Our aim here is to show a state of the art methodfor identifying and classifying technical terminol-ogy.
This task is an extension of the named entitytask defined by the DARPA-sponsored Message Un-derstanding Conferences (MUCs) (MUC, 1995) andis aimed at acquiring the shallow semantic buildingblocks that contribute to a high level understandingof the text.
Although our study here looks at shallowsemantics that can be captured using IE our basicgoal is to join this with deep semantic representa-tions so that computers can obtain a full understand-ing of the facts in a text using logical inference andreasoning.
The scenario is that human experts willcreate taxonomies and axioms (ontologies) and byproviding a small set of annotated examples, ma-chine learning can take over the role of instance cap-turing though information extraction technology.Recent studies into the use of supervised learning-based models for the named entity task haveshown that models based on hidden Markov mod-els (HMMs) (Bikel et al, 1997), and decision trees(Sekine et al, 1998), and maximum entropy (Borth-wick et al, 1998) are much more generalisable andadaptable to new classes of words than systemsbased on hand-built patterns (including wrappers)and domain specific heuristic rules such as (Herzigand Johns, 1997).The method we use is based on support vec-tor machines (SVMs)(Vapnik, 1995), a state of theart model that has achieved new levels of perfor-mance in many classification tasks.
In previouswork we have shown SVMs to be superior to sev-eral other commonly used machine learning meth-ods for named entity in previous experiments suchas HMMs and C4.5 (citations omitted).
This pa-per explores the underlying SVM model and showsthrough detailed empirical analysis the key featuresand parameter settings.To show the application of SVMs to term ex-traction in unstructured texts related to the medi-cal sciences we are using a collection of abstractsfrom PubMed?s MEDLINE (MEDLINE, 1999).
TheMEDLINE database is an online collection of ab-stracts for published journal articles in biology andmedicine and contains more than nine million arti-cles.
The collection we use in our tests is a con-trolled subset of MEDLINE obtained using threesearch keywords in the domain of molecular biol-ogy.
From the retrieved abstracts 100 were ran-domly chosen for annotation by a human expert ac-cording to classes in a small top-level ontology.In the remainder of this paper in Section (2) weoutline the background to the task and the data setwe are using; in Section (3) we described the basicadvantages of SVMs and the formal model we areusing as well as implementation specific issues suchas the choice of feature set and report experimentalresults.
In Section (4) we provide extensive resultsand a discussion of four sets of experiments we con-ducted that show the best feature sets and parametersettings in our sample domain.2 BackgroundThe names that we are trying to extract fall intoa number of categories that are outside the defini-tions used for the traditional named-entity task usedin MUC.
For this reason we consider the task ofterm identification and classification to be an ex-tended named entity task (NE+) in which the goalis to find types as well as individuals and where theterm classes belong to an explicitly defined ontol-ogy.
The use of an ontology allows us to associatehuman-readable terms in the domain with a set ofcomputer-readable classes, relations, properties andaxioms (Gruber, 1993).The particular difficulties with identifying andclassifying terms in scientific and technical domainsare the size of the vocabulary (Lindberg et al,1993), an open growing vocabulary (Lovis et al,1995), irregular naming conventions as well as ex-tensive cross-over in vocabulary between named en-tity classes.
The irregular naming arises in part be-cause of the number of researchers and practitionersfrom different fields who are working on the sameknowledge discovery area as well as the large num-ber of entities that need to be named.
Despite thebest efforts of major journals to standardize the ter-minology, there is also a significant problem withsynonymy so that often an entity has more thanone name that is widely used.
In molecular bi-ology for example class cross-over of terms mayarise because many DNA and RNA are named af-ter the protein with which they transcribe.
This se-mantic ambiguity which is dependent on often com-plex contextual conditions is one of the main rea-sons why we need learnable models and why it isdifficult to re-use existing term lists and vocabular-ies such as MeSH(NLM, 1997), UMLS (Lindberg etal., 1993) or those found in databases such as Swis-sProt (Bairoch and Apweiler, 1997).
An additionalobstacle to re-use is that the classification schemeused within an existing thesaurus or database maynot be the same as the one in the users?
ontologywhich may change from time to time as the consen-sus view of the structure of knowledge is refined.Our work has focussed on identifying names be-longing to the classes shown in Table 1 which are alltaken from the domain of molecular biology .
Exam-ple sentences from a marked up abstract are given inFigure 1.
The ontology (Tateishi et al, 2000) thatunderlies this classification scheme describes a sim-ple top-level model which is almost flat except forthe source class which shows places where geneticactivity occurs and has a number of sub-types.
Fur-ther discussion of our use of deep semantic struc-tures in the ontology is given elsewhere1 and wewill now focus our attention on the machine learningmodel used to capture low level semantics.The training set we used in our experiments calledBio1 consists of 100 MEDLINE abstracts, markedup in XML by a doctoral-qualified domain expert1Now being submitted for publicationClass # DescriptionPROTEIN 2125 proteins, protein groups,families, complexes andsubstructures.DNA 358 DNAs, DNA groups,regions and genesRNA 30 RNAs, RNA groups,regions and genesSOURCE.cl 93 cell lineSOURCE.ct 417 cell typeSOURCE.mo 21 mono-organismSOURCE.mu 64 multiorganismSOURCE.vi 90 virusSOURCE.sl 77 sublocationSOURCE.ti 37 tissueTable 1: Markup classes used in Bio1 with the num-ber of word tokens for each class.TI - Differential interactions of <NAME cl=?PROTEIN?>Rel </NAME >- <NAME cl=?PROTEIN?
>NF-kappa B</NAME > complexes with <NAME cl=?PROTEIN?
>Ikappa B alpha </NAME > determine pools of constitutive andinducible <NAME cl=?PROTEIN?
>NF-kappa B </NAME >activity.AB - The <NAME cl=?PROTEIN?
>Rel </NAME >-<NAME cl=?PROTEIN?
>NF-kappa B </NAME > fam-ily of transcription factors plays a crucial role in the regula-tion of genes involved in inflammatory and immune responses.We demonstrate that in vivo, in contrast to the other mem-bers of the family, <NAME cl=?PROTEIN?
>RelB </NAME>associates efficiently only with <NAME cl=?PROTEIN?>NF-kappa B1 </NAME > ( <NAME cl=?PROTEIN?>p105-p50 </NAME >) and <NAME cl=?PROTEIN?
>NF-kappa B2 </NAME > ( <NAME cl=?PROTEIN?
>p100-p52</NAME >), but not with <NAME cl=?PROTEIN?
>cRel</NAME > or <NAME cl=?PROTEIN?
>p65 </NAME >.The <NAME cl=?PROTEIN?
>RelB </NAME >- <NAMEcl=?PROTEIN?
>p52 </NAME >heterodimers display amuch lower affinity for <NAME cl=?PROTEIN?
>I kappaB alpha </NAME > than <NAME cl=?PROTEIN?
>RelB</NAME >- <NAME cl=?PROTEIN?
>p50 </NAME >heterodimers or <NAME cl=?PROTEIN?
>p65 </NAME >complexes.Figure 1: Example MEDLINE sentence marked upin XML for molecular biology named-entities.for the name classes given in Table 1.
The numberof named entities that were marked up by class arealso given in Table 1 and the total number of wordsin the corpus is 29940.
The abstracts were chosenfrom a sub-domain of molecular biology that we for-mulated by searching under the terms human, bloodcell, transcription factor in the PubMed database.An example can be seen in Figure 13 Method3.1 Basic modelThe named entity task can be formulated as a type ofclassification task.
In the supervised machine learn-ing approach which we adopt here we aim to esti-mate a classification function f ,f : ?N ?
{?1} (1)so that error on unseen examples is minimized,using training examples that are N dimensional vec-tors xi with class labels yi.
The sample set S withm examples isS = (x1, y1), (x2, y2), .
.
.
, (xm, ym) ?
?N ?
{?1}(2)The classification function returns either +1 if thetest data is a member of the class, or ?1 if it is not.SVMs use linear models to discriminate betweentwo classes.
This raises the question of how can theybe used to capture non-linear classification func-tions?
The answer to this is by the use of a non-linear mapping function called a kernel,?
: ?N ?
?
(3)which maps the input space ?N into a featurespace ?.
The kernel function k requires the evalu-ation of a dot productk(xi, xj) = (?
(xi) ?
?
(xj)) (4)Clearly the complexity of data being classified de-termines which particular kernel should be used andof course more complex kernels require longer train-ing times.By substituting ?
(xi) for each training examplein S we derive the final form of the optimal decisionfunction f ,f(x) = sgn(m?iyi?ik(x, xi) + b) (5)where b ?
R is the bias and the Lagrange pa-rameters ?i (?i ?
0) are estimated using quadraticoptimization to maximize the following functionw(?)
=m?i=1?i ?
12m?i,j?i?jyiyjk(xi, xj) (6)under the constraints thatm?i=1?iyi = 0 (7)and0 ?
?i ?
C (8)for i = 1, .
.
.
,m. C is a constant that controls theratio between the complexity of the function and thenumber of misclassified training examples.The number of parameters to be estimated in ?therefore never exceeds the number of examples.The influence of ?i basically means that trainingexamples with ?i > 0 define the decision func-tion (the support vectors) and those examples with?i = 0 have no influence, making the final modelvery compact and testing (but not training) very fast.The point x is classified as positive (or negative) iff(x) > 0 (or f(x) < 0).The kernel function we explored in our exper-iments was the polynomial function k(xi, xj) =(xi ?
xj + 1)d for d = 2 which was found to be thebest by (Takeuchi and Collier, 2002).
Once inputvectors have been mapped to the feature space thelinear discrimination function which is found is theone which gives the maximum the geometric marginbetween the two classes in the feature space.Besides efficiency of representation, SVMs areknown to maximize their generalizability, makingthem an ideal model for the NE+ task.
Generaliz-ability in SVMs is based on statistical learning the-ory and the observation that it is useful sometimesto misclassify some of the training data so that themargin between other training points is maximized.This is particularly useful for real world data setsthat often contain inseparable data points.We implemented our method using the Tiny SVMpackage from NAIST2 which is an implementationof Vladimir Vapnik?s SVM combined with an op-timization algorithm (Joachims, 1999).
The multi-class model is built up from combining binary clas-sifiers and then applying majority voting.3.2 Generalising with featuresIn order for the model to be successful it must recog-nize regularities in the training data that relate pre-classified examples of terms with unseen terms thatwill be encountered in testing.Following on from previous studies in named en-tity we chose a set of linguistically motivated word-level features that include surface word forms, partof speech tags using the Brill tagger (Brill, 1992)and orthographic features.
Additionally we usedhead-noun features that were obtained from pre-analysis of the training data set using the FDG shal-low parser from Conexor (Tapanainen and Ja?rvinen,1997).
A significant proportion of the terms inour corpus undergo a local syntactic transforma-tions such as coordination which introduces ambi-guity that needs to be resolved by shallow parsing.For example the c- and v-rel (proto) oncogenes andNF-kappaB and I kappa B protein families.
In thesecases the head noun features oncogene and fam-ily would be added to each word in the constituentphrase.
Head information is also needed when de-ciding the semantic category of a long term such astumor necrosis factor-alpha which should be a PRO-TEIN, whereas tumor necrosis factor (TNF) geneand tumor necrosis factor promoter region shouldboth be types of DNA.Table 2 shows the orthographic features that weused.
We hypothesize that such features will help themodel to find similarities between known words thatwere found in the training set and unknown words(of zero frequency in the training set) and so over-come the unknown word problem.In the experiments we report below we use featurevectors consisting of differing amounts of ?context?by varying the window around the focus word whichis to be classified into one of the semantic classes.The full window of context considered in these ex-periments is ?3 about the focus word.2Tiny SVM is available from http:// http://cl.aist-nara.ac.jp/taku-ku/software/ TinySVM/Feature Example Feature ExampleDigitNumber 15 CloseSquare ]SingleCap M Colon :GreekLetter alpha SemiColon ;CapsAndDigits I2 Percent %TwoCaps RalGDS OpenParen (LettersAndDigits p52 CloseParen )InitCap Interleukin Comma ,LowCaps kappaB FullStop .Lowercase kinases Determiner theHyphon - Conjunction andBackslash / Other * + #OpenSquare [Table 2: Orthographic features with examples4 Experiment and DiscussionResults are given as F-scores (van Rijsbergen, 1979)using the CoNLL evaluation script and are definedas F = (2PR)/(P+R).
where P denotes Precisionand R Recall.
P is the ratio of the number of cor-rectly found NE chunks to the number of found NEchunks, and R is the ratio of the number of correctlyfound NE chunks to the number of true NE chunks.All results are calculated using 10-fold cross valida-tion.4.1 Experiment 1: Effect of Training Set SizeThe effect of context window size is shown alongthe top column of Tables 3 and 4.
It can be seenthat without exception more training data results inhigher overall F-scores except at 10 per cent.
wherethe result seems to be biased by the small sample,perhaps because one abstract is partly included inthe training and testing sets.
As we would expectlarger training sets reduce the effects of data sparse-ness and allow more accurate models to be induced.The rate of increase in improvement however isnot uniform according to the feature sets that areused.
For surface word features and head nounfeatures the improvement in performance is consis-tently increasing whereas the improvement for usingorthographic and part of speech features is quite er-ratic.
This may be an effect of the small sample oftraining data that we used and we could not find anyconsistent explanation why this occurred.As we observed before, the best overall resultcomes from using Or hd, i.e.
surface words, or-thographic and head features.
However the to-tal score hides the fact that three classes, i.e.SOURCE.mo, SOURCE.mu and SOURCE.ti actu-ally perform worse when using anything but sur-face word forms (shown in Table 5).
One possi-ble explanation for this is that all of these classeshave very small numbers of samples and the effectof adding features may be to blur the distinction be-tween these and other more numerous classes in themodel.
However it is interesting to note that thisdoes not happen with the RNA class which is alsovery small.4.2 Experiment 2: Effect of Feature SetsThe effects of feature sets is of major importance inmodelling named entity.
In general we would liketo identify only the necessary features that are re-quired and to remove those that do not contribute toan increase in performance.
This also saves time intraining and testing.The results from Tables 3 and 4 at 100 per cent.training data are summarized in Table 5 and clearlyillustrate the value of surface word level featurescombined with orthographic and head noun features.Orthographic features allow us to capture many gen-eralities that are not obvious at the surface wordlevel such as IkappaB alpha and IkappaB beta bothbeing PROTEINs and IL-10 and IL-2 both beingPROTEINs.The orthographic-head noun feature combination(Or hd) gives the best combined-class performanceof 74.23 at 100 per cent.
training data on a -2+2 win-dow.
Overall orthographic features combined withsurface word features gave an improvement of be-tween 4.9 and 22.0 per cent.
at 100 per cent.
datadepending on window size over surface words alone.This was the biggest contribution by any feature ex-cept the surface words.
Head information for exam-ple allowed us to correctly capture the fact that inthe phrase NF-kappaB consensus site the whole ofit is a DNA, whereas using orthographic informa-tion alone the SVM could only say that NF-kappaBwas a PROTEIN and ignoring consensus site.
Wesee a similar case in the phrase primary NK cellswhich is correctly classified as SOURCE.ct usinghead noun and orthographic features but only NKcells are found using orthographic features.
Thismistake is a natural consequence of a limited con-textual view which the head noun feature helped torectify.Part of speech (POS) when combined with sur-face word features gave an improvement of between7.9 and 11.7 per cent.
at 100 per cent.
data.
Theinfluence of POS though does not appear to be sus-tained when combined with other features and wefound that it actually degraded performance slightlyin many cases.
This may possibly be due to ei-ther overlapping knowledge or more likely subtleinconsistencies between POS features and say, or-thographic features.
This could have occurred dur-ing training when the POS tagger was trained on anout of domain (news) text collection.
It is possiblethat if the POS tagger was trained on in-domain textsit would make a greater and more consistent con-tribution.
An example where orthographic featuresallowed correct classification but adding POS fea-tures resulted in failure is p50 in the phrase consist-ing of 50 (p50) - and 65 (p65) -kDa proteins.
Alsoin the phrase c-Jun transactivation domain whereonly c-Jun should be tagged as a protein, by usingorthographic features and POS the model tags thewhole phrase as a PROTEIN.
This is probably be-cause POS tagging gives a NN feature value (com-mon noun) to each word.
This is very general anddoes not allow the model to discriminate betweenthem.The fourth feature we investigated is related tosyntactic rather than lexical knowledge.
We feltthough that there should exist a strong semantic re-lation between a word in a term and the head nounof that term.
The results in Table 5 show that whilethe overall contribution of the Head feature is quitesmall, it is consistent for almost all classes.5 ConclusionThe method we have shown for identifying and clas-sifying technical terms has the advantage of be-ing portable, not requiring large domain dependentdictionaries and no hand-made patterns were used.Additionally, since all the word level features arefound automatically there is no need for interven-tion to create domain specific features.
Indeed theonly thing that is required is a quite small corpus oftext containing entities tagged by a domain expert.For future work we are now looking at how to bal-ance the scores from SVM for each word-class overthe whole of a sentence using dynamic program-ming.
Theoretically the existing SVM model cannotconsider evidence from outside the context window,in particular evidence related to named entity classscores in the history and later in the sentence.ReferencesA.
Bairoch and R. Apweiler.
1997.
The SWISS-PROTprotein sequence data bank and its new supplementTrEMBL.
Nucleic Acids Research, 25:31?36.D.
Bikel, S. Miller, R. Schwartz, and R. Wesichedel.1997.
Nymble: a high-performance learning name-finder.
In Proceedings of the Fifth Conference on Ap-plied Natural Language Processing (ANLP?97), Wash-ington D.C., USA., pages 194?201, 31 March ?
3April.A.
Borthwick, J.
Sterling, E. Agichtein, and R. Grishman.1998.
Exploiting diverse knowledge sources via max-imum entropy in named entity recognition.
In Pro-ceedings of the Sixth Workshop on Very Large Corpora(WVLC?98), Montreal, Canada, pages 152?160.E.
Brill.
1992.
A simple rule-based part of speech tagger.In Third Conference on Applied Natural LanguageProcessing ?
Association for Computational Linguis-tics, Trento, Italy, pages 152?155, 31st March ?
3rdApril.M.
Craven and J. Kumlien.
1999.
Constructing biolog-ical knowledge bases by extracting information fromtext sources.
In Proceedings of the 7th InternationalConference on Intelligent Systemps for Molecular Bi-ology (ISMB-99), pages 77?86, Heidelburg, Germany,August 6?10.T.
R. Gruber.
1993.
A translation approach toportable ontology specifications.
Knowledge Acqui-sition, 6(2):199?221.T.
Herzig and M. Johns.
1997.
Extraction of medicalinformation from textual sources: a statistical vari-ant of the boundary word method.
In Proceedings ofthe American Medical Informatics Association (AMIA)1997 Annual Fall Symposium, Nashville, USA, 25?29October.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Scholkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support VectorLearning.
MIT Press.Donald A.B.
Lindberg, L. Humphreys, Betsy, and T. Mc-Cray, Alexa.
1993.
The unified medical languagesystem.
Methods of Information in Medicine, 32:281?291.Feature Set & Percentage of data used in experimentWindow Size 10 20 30 40 50 60 70 80 90 100Wd -10 58.52 47.30 51.44 52.40 52.37 52.30 51.29 53.24 55.57 56.06Wd -1+1 55.35 48.15 53.91 54.50 56.02 55.30 55.92 58.98 60.28 61.55Wd -2+2 46.87 40.73 47.92 49.64 53.31 53.20 55.01 56.95 59.40 62.04Wd -3+2 46.12 38.55 44.19 47.93 49.50 50.50 51.21 54.76 56.66 60.25Wd -3+3 44.83 35.37 42.67 45.24 46.78 49.10 49.66 54.01 55.59 58.83Or -10 60.33 55.08 63.49 63.41 64.09 63.04 62.97 62.64 64.59 65.63Or -1+1 65.35 58.69 66.63 68.18 69.20 68.74 69.55 69.32 71.02 72.13Or -2+2 60.84 58.90 66.44 67.17 69.88 68.81 69.68 69.62 71.41 72.12Or -3+2 62.48 59.21 65.64 66.69 67.56 67.25 68.37 68.94 69.92 71.69Or -3+3 59.61 58.65 64.95 65.68 67.11 66.65 67.85 68.84 69.54 71.78Head -10 58.51 47.10 51.99 52.74 52.44 52.01 53.09 53.79 55.97 57.01Head -1+1 57.50 50.00 55.81 57.88 58.03 57.84 58.81 61.08 62.64 63.93Head -2+2 49.43 45.92 53.40 53.75 57.52 56.94 59.33 61.29 63.36 64.67Head -3+2 46.51 39.42 49.39 49.75 54.54 54.81 56.95 58.13 59.25 61.96Head -3+3 45.79 40.81 47.52 48.11 53.58 53.50 55.95 57.02 59.06 61.52POS -10 61.62 52.89 61.14 62.04 62.62 61.51 61.05 60.78 62.71 62.63POS -1+1 61.24 57.25 63.83 62.94 65.35 64.82 67.40 66.47 67.43 68.37POS -2+2 57.52 53.11 59.39 59.98 62.86 62.16 63.72 64.17 64.56 66.92POS -3+2 56.81 54.55 56.53 56.26 59.60 59.40 61.42 61.86 63.41 64.90POS -3+3 54.76 53.28 56.79 55.02 57.46 57.66 59.60 59.89 62.39 63.50Table 3: F-scores on Bio1 showing the effects of training set size, feature sets, and context window sizes.Wd: surface word level features; Or: Orthographic features; Head: Head noun features; POS: part of speechfeatures.Feature Set & Percentage of data used in experimentWindow Size 10 20 30 40 50 60 70 80 90 100Or hd -10 62.16 57.80 64.31 65.70 65.20 63.84 64.90 64.73 66.46 67.31Or hd -1+1 64.84 60.52 68.42 68.25 68.82 69.34 71.31 71.88 72.60 73.38Or hd -2+2 61.16 61.10 68.06 67.42 69.32 69.62 70.91 71.31 72.31 74.23Or hd -3+2 61.54 60.06 65.87 66.33 67.43 68.36 70.28 70.15 70.81 72.95Or hd -3+3 59.68 57.03 64.58 65.76 66.84 67.16 69.07 69.22 70.73 72.12Or POS -10 61.48 54.04 63.20 63.92 64.11 64.74 63.23 63.62 64.87 66.28Or POS -1+1 64.57 58.89 66.52 66.77 67.83 67.90 69.32 69.07 70.84 71.70Or POS -2+2 61.48 58.56 63.37 65.44 67.01 66.74 68.21 68.55 70.09 71.87Or POS -3+2 61.08 57.14 64.23 63.39 65.53 65.11 67.31 67.78 68.64 71.54Or POS -3+3 57.92 57.12 62.86 62.36 65.48 64.41 66.10 66.64 68.22 70.46POS hd -10 64.90 55.39 61.14 61.65 61.91 61.29 61.88 60.51 63.27 63.82POS hd -1+1 62.25 57.25 63.66 64.81 64.64 65.57 67.78 67.63 68.69 69.68POS hd -2+2 58.08 53.23 58.91 60.28 62.55 62.06 64.19 64.51 66.18 67.66POS hd -3+2 57.09 53.20 56.58 57.75 59.34 59.14 62.19 62.93 64.23 65.41POS hd -3+3 54.69 51.09 55.67 55.46 58.31 58.28 60.88 61.17 62.94 64.31Or POS hd -10 63.70 56.63 63.29 65.11 64.72 64.14 64.40 64.04 66.01 67.41Or POS hd -1+1 66.20 59.65 66.49 67.91 68.44 68.14 70.01 70.61 71.80 72.95Or POS hd -2+2 61.62 58.03 64.76 65.16 66.45 67.26 69.00 69.86 70.83 72.56Or POS hd -3+2 62.06 57.28 63.74 64.50 66.10 66.25 68.01 69.05 69.44 71.59Or POS hd -3+3 59.12 56.51 62.43 62.61 65.37 65.09 66.89 67.80 69.36 71.25Table 4: F-scores on Bio1 showing the effects of training set size, feature sets, and context window sizes.Wd: surface word level features; Or: Orthographic features; Head: Head noun features; POS: part of speechfeatures.NE+ Class Feature SetWd Or Head POS Or hd Or POS POS hd Or POS hdDNA 44.53 56.49 50.88 47.33 62.78 58.12 47.30 59.19PROTEIN 65.07 77.50 67.96 72.10 78.99 77.03 72.89 77.58RNA 12.12 42.11 12.90 24.24 43.24 37.84 6.67 29.41SOURCE.cl 52.63 57.14 51.52 54.79 59.21 55.90 56.94 59.87SOURCE.ct 65.83 66.39 66.22 63.70 69.32 67.03 65.65 68.94SOURCE.mo 32.00 16.67 9.09 17.39 17.39 16.67 17.39 17.39SOURCE.mu 61.02 58.41 55.24 57.14 51.92 54.55 53.33 51.92SOURCE.sl 55.22 62.86 62.69 51.20 68.53 62.41 54.84 63.38SOURCE.ti 23.26 18.18 0.00 14.63 5.00 14.29 0.00 0.00SOURCE.vi 76.54 75.16 79.50 73.68 80.25 74.84 75.00 73.33Table 5: Class by class performance using a -2+2 window shown against feature sets.
Wd: surface wordlevel features; Or: Orthographic features; Head: Head noun features; POS: part of speech features.C.
Lovis, P. Michel, R. Baud, and J. Scherrer.
1995.Word segmentation processing: a way to exponentiallyextend medical dictionaries.
Medinfo, 8:28?32.MEDLINE.
1999.
The PubMed database can be foundat:.
http://www.ncbi.nlm.nih.gov/PubMed/.DARPA.
1995.
Proceedings of the Sixth Message Under-standing Conference(MUC-6), Columbia, MD, USA,November.
Morgan Kaufmann.NLM.
1997.
Medical subject headings, bethesda, MD.National Library of Medicine.Satoshi Sekine, Ralph Grishman, and Hiroyuki Shinnou.1998.
A Decision Tree Method for Finding and Clas-sifying Names in Japanese Texts.
In Proceedings ofthe Sixth Workshop on Very Large Corpora, Montreal,Canada, August.K.
Takeuchi and N. Collier.
2002.
Use of support vec-tor machines in extended named entity recognition.
InProceedings of the 6th Conference on Natural Lan-guage Learning 2002 (CoNLL-2002), Roth, D. andvan den Bosch, A.
(eds), pages 119?125, August 31st.P.
Tapanainen and T. Ja?rvinen.
1997.
A non-projectivedependency parser.
In Proceedings of the 5th Confer-ence on Applied Natural Language Processing, Wash-ington D.C., Association of Computational Linguis-tics, pages 64?71.Y.
Tateishi, T. Ohta, N. Collier, C. Nobata, K. Ibushi, andJ.
Tsujii.
2000.
Building an annotated corpus in themolecular-biology domain.
In COLING?2000 Work-shop on Semantic Annotation and Intelligent Content,Luxemburg, 5th?6th August.J.
Thomas, D. Milward, C. Ouzounis, S. Pulman, andM.
Carroll.
1999.
Automatic extraction of protein in-teractions from scientific abstracts.
In Proceedings ofthe Pacific Symposium on Biocomputing?99 (PSB?99),pages 1?12, Hawaii, USA, January 4?9.C.
J. van Rijsbergen.
1979.
Information Retrieval.
But-terworths, London.V.
N. Vapnik.
1995.
The Nature of Statistical LearningTheory.
Springer-Verlag, New York.
