Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 419?424,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsNothing like Good Old Frequency:Studying Context Filters for Distributional ThesauriMuntsa Padr?o,?Marco Idiart?, Carlos Ramisch?, Aline Villavicencio?
?Institute of Informatics, Federal University of Rio Grande do Sul (Brazil)?Institute of Physics, Federal University of Rio Grande do Sul (Brazil)?Aix Marseille Universit?e, CNRS, LIF UMR 7279, 13288, Marseille (France)muntsa.padro@inf.ufrgs.br, marco.idiart@gmail.com,carlos.ramisch@lif.univ-mrs.fr, avillavicencio@inf.ufrgs.brAbstractMuch attention has been given to theimpact of informativeness and similar-ity measures on distributional thesauri.We investigate the effects of context fil-ters on thesaurus quality and propose theuse of cooccurrence frequency as a sim-ple and inexpensive criterion.
For eval-uation, we measure thesaurus agreementwith WordNet and performance in answer-ing TOEFL-like questions.
Results illus-trate the sensitivity of distributional the-sauri to filters.1 IntroductionLarge-scale distributional thesauri created auto-matically from corpora (Grefenstette, 1994; Lin,1998; Weeds et al., 2004; Ferret, 2012) are aninexpensive and fast alternative for representingsemantic relatedness between words, when man-ually constructed resources like WordNet (Fell-baum, 1998) are unavailable or lack coverage.
Toconstruct a distributional thesaurus, the (colloca-tional or syntactic) contexts in which a target wordoccurs are used as the basis for calculating its sim-ilarity with other words.
That is, two words aresimilar if they share a large proportion of contexts.Much attention has been devoted to refin-ing thesaurus quality, improving informativenessand similarity measures (Lin, 1998; Curran andMoens, 2002; Ferret, 2010), identifying and de-moting bad neighbors (Ferret, 2013), or usingmore relevant contexts (Broda et al., 2009; Bie-mann and Riedl, 2013).
For the latter in particular,as words vary in their collocational tendencies, itis difficult to determine how informative a givencontext is.
To remove uninformative and noisycontexts, filters have often been applied like point-wise mutual information (PMI), lexicographer?smutual information (LMI) (Biemann and Riedl,2013), t-score (Piasecki et al., 2007) and z-score(Broda et al., 2009).
However, the selection of ameasure and of a threshold value for these filtersis generally empirically determined.
We argue thatthese filtering parameters have a great influence onthe quality of the generated thesauri.The goal of this paper is to quantify the im-pact of context filters on distributional thesauri.We experiment with different filter methods andmeasures to assess context significance.
We pro-pose the use of simple cooccurrence frequency asa filter and show that it leads to better results thanmore expensive measures such as LMI or PMI.Thus we propose a cheap and effective way of fil-tering contexts while maintaining quality.This paper is organized as follows: in ?2 wediscuss evaluation of distributional thesauri.
Themethodology adopted in the work and the resultsare discussed in ?3 and ?4.
We finish with someconclusions and discussion of future work.2 Related WorkIn a nutshell, the standard approach to build a dis-tributional thesaurus consists of: (i) the extractionof contexts for the target words from corpora, (ii)the application of an informativeness measure torepresent these contexts and (iii) the application ofa similarity measure to compare sets of contexts.The contexts in which a target word appears canbe extracted in terms of a window of cooccurring(content) words surrounding the target (Freitag etal., 2005; Ferret, 2012; Erk and Pado, 2010) or interms of the syntactic dependencies in which thetarget appears (Lin, 1998; McCarthy et al., 2003;Weeds et al., 2004).
The informativeness of eachcontext is calculated using measures like PMI, andt-test while the similarity between contexts is cal-culated using measures like Lin?s (1998), cosine,Jensen-Shannon divergence, Dice or Jaccard.Evaluation of the quality of distributional the-sauri is a well know problem in the area (Lin,4191998; Curran and Moens, 2002).
For instance, forintrinsic evaluation, the agreement between the-sauri has been examined, looking at the averagesimilarity of a word in the thesauri (Lin, 1998),and at the overlap and rank agreement between thethesauri for target words like nouns (Weeds et al.,2004).
Although much attention has been given tothe evaluation of various informativeness and sim-ilarity measures, a careful assessment of the ef-fects of filtering on the resulting thesauri is alsoneeded.
For instance, Biemann and Riedl (2013)found that filtering a subset of contexts based onLMI increased the similarity of a thesaurus withWordNet.
In this work, we compare the impact ofusing different types of filters in terms of thesaurusagreement with WordNet, focusing on a distribu-tional thesaurus of English verbs.
We also proposea frequency-based saliency measure to rank andfilter contexts and compare it with PMI and LMI.Extrinsic evaluation of distributional thesaurihas been carried out for tasks such as En-glish lexical substitution (McCarthy and Navigli,2009), phrasal verb compositionality detection(McCarthy et al., 2003) and the WordNet-basedsynonymy test (WBST) (Freitag et al., 2005).
Forcomparative purposes in this work we adopt thelatter.3 MethodologyWe focus on thesauri of English verbs constructedfrom the BNC (Burnard, 2007)1.
Contexts are ex-tracted from syntactic dependencies generated byRASP (Briscoe et al., 2006), using nouns (headsof NPs) which have subject and direct object rela-tions with the target verb.
Thus, each target verbis represented by a set of triples containing (i) theverb itself, (ii) a context noun and (iii) a syntac-tic relation (object, subject).
The thesauri wereconstructed using Lin?s (1998) method.
Lin?s ver-sion of the distributional hypothesis states that twowords (verbs v1and v2in our case) are similar ifthey share a large proportion of contexts weightedby their information content, assessed with PMI(Bansal et al., 2012; Turney, 2013).In the literature, little attention is paid to contextfilters.
To investigate their impact, we comparetwo kinds of filters, and before calculating similar-ity using Lin?s measure, we apply them to remove1Even though larger corpora are available, we use a tradi-tional carefully constructed corpus with representative sam-ples of written English to control the quality of the thesaurus.potentially noisy triples:?
Threshold (th): we remove triples that oc-cur less than a threshold th.
Threshold valuesvary from 1 to 50 counts per triple.?
Relevance (p): we keep only the top p mostrelevant contexts for each verb, were rele-vance is defined according to the followingmeasures: (a) frequency, (b) PMI, and (c)LMI (Biemann and Riedl, 2013).
Values ofp vary between 10 and 1000.In this work, we want to answer two ques-tions: (a) Do more selective filters improve intrin-sic evaluation of thesaurus?
and (b) Do they alsohelp in extrinsic evaluation?For intrinsic evaluation, we determine agree-ment between a distributional thesaurus and Word-Net as the path similarities for the first k distri-butional neighbors of a verb.
A single score isobtained by averaging the similarities of all verbswith their k first neighbors.
The higher this scoreis, the closer the neighbors are to the target inWordNet, and the better the thesaurus.
Severalvalues of k were tested and the results showed ex-actly the same curve shapes for all values, withWordNet similarity decreasing linearly with k. Forthe remainder of the paper we adopt k = 10, as itis widely used in the literature.For extrinsic evaluation, we use the WBST setfor verbs (Freitag et al., 2005) with 7,398 ques-tions and an average polysemy of 10.4.
The taskconsists of choosing the most suitable synonymfor a word among a set of four options.
The the-saurus is used to rank the candidate answers bysimilarity scores, and select the first one as thecorrect synonym.
As discussed by Freitag et al.
(2005), the upper bound reached by English na-tive speakers is 88.4% accuracy, and simple lowerbounds are 25% (random choice) and 34.5% (al-ways choosing the most frequent option).4 ResultsFigure 1 shows average WordNet similarities forthesauri built filtering by frequency threshold thand by p most frequent contexts.
Table 1 sum-marizes the parametrization leading to the bestWordNet similarity for each kind of filter.
In allcases we show the results obtained for differentfrequency ranges2as well as the results when av-eraging over all verbs.2In order to study the influence of verb frequency on theresults, we divide the verbs in three groups: high-frequency42000.050.10.150.20.251  10WNsimilaritythWordNet path Similarity for different frequency ranges, k=10Filtering triples with frequency under thall verbshigh frequent verbsmid frequent verbslow frequent verbs  00.050.10.150.20.2510  100  1000WNsimilaritypWordNet path Similarity for different frequency ranges, k=10Keeping p most frequent triples per verball verbshigh frequent verbsmid frequent verbslow frequent verbsFigure 1: WordNet scores for verb frequency ranges, filtering by frequency threshold th (left) and p mostfrequent contexts (right).Filter All verbs Frequency rangeLow Mid HighNo filter - 0.148 - 0.101 - 0.144 - 0.198Filter low freq.
contexts th = 50 0.164 th = 50 0.202 th = 50 0.154 th = 1 0.200Keep p contexts (freq.)
p = 200 0.158 p = 500 0.138 p = 200 0.149 p = 200 0.206Keep p contexts (PMI) p = 1000 0.139 p = 1000 0.101 p = 1000 0.136 p = 1000 0.181Keep p contexts (LMI) p = 200 0.155 p = 100 0.112 p = 200 0.147 p = 200 0.208Table 1: Best scores obtained for each filter for all verbs and frequency ranges.
Scores are given in termsof WordNet path.
Confidence interval is arround ?
0.002 in all cases.When using a threshold filter (Figure 1 left),high values lead to better performance for mid-and low-frequency verbs.
This is because, for highth values, there are few low and mid-frequencyverbs left, since a verb that occurs less has lesschances to be seen often in the same context.
Thesimilarity for verbs with no contexts over the fre-quency threshold cannot be assessed and as a con-sequence those verbs are not included in the fi-nal thesaurus.
As Figure 2 shows, the numberof verbs decreases much faster for low and midfrequency verbs when th increases.3For exam-ple, for th = 50, there are only 7 remaining low-frequency verbs in the thesaurus and these tendto be idiosyncratic multiword expressions.
Oneexample is wreak, and the only triple contain-ing this verb that appeared more than 50 times iswreak havoc (71 occurrences).
The neighbors ofthis verb are cause and play, which yield a goodsimilarity score in WordNet.
Therefore, althoughhigher thresholds result in higher similarities forlow and mid-frequency verbs, this comes at a cost,as the number of verbs included in the thesaurusdecreases considerably.
(||v|| ?
500), mid-frequency (150 ?
||v|| < 500) and low-frequency (||v|| < 150).3For p most salient contexts, the number of verbs does notvary and is the same shown in Figure 2 for th = 1 (no filter).05001000150020002500300035001  10NumberofverbsthNumber of verbs in WordNetFiltering triples with frequency under thall verbshigh frequent verbsmid frequent verbslow frequent verbsFigure 2: Number of verbs per frequency rangeswhen filtering by context frequency threshold thAs expected, the best performance is obtainedfor high-frequency verbs and no filter, since it re-sults in more context information per verb.
In-creasing th decreases similarity due to the removalof some of these contexts.
In average, higher thvalues lead to better overall similarity among thefrequency ranges (from 0.148 with th = 1 to0.164 with th = 50).
The higher the threshold,the more high-frequency verbs will prevail in thethesauri, for which the WordNet path similaritiesare higher.On the other hand, when adopting a relevance42100.20.40.60.811  10P,R,F1thWBST task: P, R and F1Filtering triples with frequency under thPrecisionRecallF1  00.20.40.60.8110  100  1000P,R,F1pWBST task: P, R and F1Keeping p most frequent triples per verbPrecisionRecallF1Figure 3: WBST task scores filtering by frequency threshold th (left) and p most frequent contexts(right).filter of keeping the p most relevant contexts foreach verb (Figure 1 right), we obtain similar re-sults, but more stable thesauri.
The number ofverbs remains constant, since we keep a fixednumber of contexts for each verb and verbs are notremoved when the threshold is modified.
Word-Net similarity increases as more contexts are takeninto account, for all frequency ranges.
There is amaximum around p = 200, though larger valuesdo not lead to a drastic drop in quality.
This sug-gests that the noise introduced by low-frequencycontexts is compensated by the increase of infor-mativeness for other contexts.
An ideal balanceis reached by the lowest possible p that maintainshigh WordNet similarity, since the lower the p thefaster the thesaurus construction.In terms of saliency measure, when keepingonly the p most relevant contexts, sorting themwith PMI leads to much worse results than LMIor frequency, as PMI gives too much weight toinfrequent combinations.
This is consistent withresults of Biemann and Riedl (2013).
RegardingLMI versus frequency, the results using the latterare slightly better (or with no significant differ-ence, depending on the frequency range).
The ad-vantage of using frequency instead of LMI is thatit makes the process simpler and faster while lead-ing to equal or better performance in all frequencyranges.
Therefore for the extrinsic evaluation us-ing WBST task, we use frequency to select thep most relevant contexts and then compute Lin?ssimilarity using only those contexts.Figure 3 shows the performance of the thesauriin the WBST task in terms of precision, recall andF1.4For precision, the best filter is to remove con-4Filters based on LMI and PMI were also tested with thetexts occurring less than th times, but, this alsoleads to poor recall, since many verbs are left outof the thesauri and their WSBT questions cannotbe answered.
On the other hand, keeping the mostrelevant p contexts leads to more stable results andwhen p is high (right plot), they are similar to thoseshown in the left plot of Figure 3.4.1 DiscussionThe answer to our questions in Section 3 is yes,more selective filters improve intrinsic and extrin-sic thesaurus quality.
The use of both filteringmethods results in thesauri in which the neighborsof target verbs are closer in WordNet and get betterscores in TOEFL-like tests.
However, the fact thatfiltering contexts with frequency under th removesverbs in the final thesaurus is a drawback, as high-lighted in the extrinsic evaluation on the WBSTtask.Furthermore, we demonstrated that competitiveresults can be obtained keeping only the p mostrelevant contexts per verb.
On the one hand, thismethod leads to much more stable thesauri, withthe same verbs for all values of p. On the otherhand, it is important to highlight that the best re-sults to assess the relevance of the contexts are ob-tained using frequency while more sophisticatedfilters such as LMI do not improve thesaurus qual-ity.
Although an LMI filter is relatively fast com-pared to dimensionality reduction techniques suchas singular value decomposition (Landauer andDumais, 1997), it is still considerably more expen-sive than a simple frequency filter.In short, our experiments indicate that a reason-same results as intrinsic evaluation: sorting contexts by fre-quency leads to better results.422able trade-off between noise, coverage and com-putational efficiency is obtained for p = 200 mostfrequent contexts, as confirmed by intrinsic andextrinsic evaluation.
Frequency threshold th isnot recommended: it degrades recall because thecontexts for many verbs are not frequent enough.This result is useful for extracting distributionalthesauri from very large corpora like the UKWaC(Ferraresi et al., 2008) by proposing an alterna-tive that minimizes the required computational re-sources while efficiently removing a significantamount of noise.5 Conclusions and Future WorkIn this paper we addressed the impact of filterson the quality of distributional thesauri, evaluat-ing a set of standard thesauri and different filteringmethods.
The results suggest that the use of fil-ters and their parameters greatly affect the thesaurigenerated.
We show that it is better to use a filterthat selects the most relevant contexts for a verbthan to simply remove rare contexts.
Furthermore,the best performance was obtained with the sim-plest method: frequency was found to be a simpleand inexpensive measure of context salience.
Thisis especially important when dealing with largeamounts of data, since computing LMI for all con-texts would be computationally costly.
With ourproposal to keep just the p most frequent contextsper verb, a great deal of contexts are cheaply re-moved and thus the computational power requiredfor assessing similarity is drastically reduced.As future work, we plan to use these filters tobuild thesauri from larger corpora.
We would liketo generalize our findings to other syntactic con-figurations (e.g.
noun-adjective) as well as to othersimilarity and informativeness measures.
For in-stance, ongoing experiments indicate that the sameparameters apply when Lin?s similarity is replacedby cosine.
Finally, we would like to compare theproposed heuristics with more sophisticated filter-ing strategies like singular value decomposition(Landauer and Dumais, 1997) and non-negativematrix factorization (Van de Cruys, 2009).AcknowledgmentsWe would like to thank the support of projectsCAPES/COFECUB 707/11, PNPD 2484/2009,FAPERGS-INRIA 1706-2551/13-7, CNPq312184/2012-3, 551964/2011-1, 482520/2012-4and 312077/2012-2.ReferencesMohit Bansal, John DeNero, and Dekang Lin.
2012.Unsupervised translation sense clustering.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages773?782, Montr?eal, Canada, June.
Association forComputational Linguistics.Chris Biemann and Martin Riedl.
2013.
Text: Nowin 2D!
a framework for lexical expansion with con-textual similarity.
Journal of Language Modelling,1(1).Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In JamesCurran, editor, Proc.
of the COLING/ACL 2006 In-teractive Presentation Sessions, pages 77?80, Sid-ney, Australia, Jul.
ACL.Bartosz Broda, Maciej Piasecki, and Stan Szpakow-icz.
2009.
Rank-based transformation in mea-suring semantic relatedness.
In Proceedings ofthe 22nd Canadian Conference on Artificial Intel-ligence: Advances in Artificial Intelligence, Cana-dian AI ?09, pages 187?190, Berlin, Heidelberg.Springer-Verlag.Lou Burnard.
2007.
User Reference Guide for theBritish National Corpus.
Technical report, OxfordUniversity Computing Services, Feb.James R. Curran and Marc Moens.
2002.
Improve-ments in automatic thesaurus extraction.
In Proc.ofthe ACL 2002 Workshop on Unsupervised LexicalAcquisition, pages 59?66, Philadelphia, Pennsylva-nia, USA.
ACL.Katrin Erk and Sebastian Pado.
2010.
Exemplar-basedmodels for word meaning in context.
In Proc.
of theACL 2010 Conference Short Papers, pages 92?97,Uppsala, Sweden, Jun.
ACL.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database (Language, Speech, andCommunication).
MIT Press, May.
423 p.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluat-ing UKWaC, a very large web-derived corpus of En-glish.
In In Proceedings of the 4th Web as CorpusWorkshop (WAC-4.Olivier Ferret.
2010.
Testing semantic similarity mea-sures for extracting synonyms from a corpus.
InProc.
of the Seventh LREC (LREC 2010), pages3338?3343, Valetta, Malta, May.
ELRA.Olivier Ferret.
2012.
Combining bootstrapping andfeature selection for improving a distributional the-saurus.
In ECAI, pages 336?341.Olivier Ferret.
2013.
Identifying bad semantic neigh-bors for improving distributional thesauri.
In Proc.of the 51st ACL (Volume 1: Long Papers), pages561?571, Sofia, Bulgaria, Aug. ACL.423Dayne Freitag, Matthias Blume, John Byrnes, Ed-mond Chow, Sadik Kapadia, Richard Rohwer, andZhiqiang Wang.
2005.
New experiments in distri-butional representations of synonymy.
In Ido Daganand Dan Gildea, editors, Proc.
of the Ninth CoNLL(CoNLL-2005), pages 25?32, University of Michi-gan, MI, USA, Jun.
ACL.Gregory Grefenstette.
1994.
Explorations in Au-tomatic Thesaurus Discovery.
Springer, Norwell,MA, USA.Thomas K Landauer and Susan T. Dumais.
1997.
Asolution to platos problem: The latent semantic anal-ysis theory of acquisition, induction, and represen-tation of knowledge.
Psychological review, pages211?240.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Proc.
of the 36th ACL and17th COLING, Volume 2, pages 768?774, Montreal,Quebec, Canada, Aug. ACL.Diana McCarthy and Roberto Navigli.
2009.
The en-glish lexical substitution task.
Language Resourcesand Evaluation, 43(2):139?159.Diana McCarthy, Bill Keller, and John Carroll.2003.
Detecting a continuum of compositionalityin phrasal verbs.
In Francis Bond, Anna Korhonen,Diana McCarthy, and Aline Villavicencio, editors,Proc.
of the ACL Workshop on MWEs: Analysis, Ac-quisition and Treatment (MWE 2003), pages 73?80,Sapporo, Japan, Jul.
ACL.Maciej Piasecki, Stanislaw Szpakowicz, and BartoszBroda.
2007.
Automatic selection of heterogeneoussyntactic features in semantic similarity of polishnouns.
In Proceedings of the 10th internationalconference on Text, speech and dialogue, TSD?07,pages 99?106, Berlin, Heidelberg.
Springer-Verlag.Peter D. Turney.
2013.
Distributional semantics be-yond words: Supervised learning of analogy andparaphrase.
1:353?366.Tim Van de Cruys.
2009.
A non-negative tensor factor-ization model for selectional preference induction.In Proceedings of the Workshop on GeometricalModels of Natural Language Semantics, pages 83?90, Athens, Greece, March.
Association for Compu-tational Linguistics.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proc.
of the 20th COLING (COL-ING 2004), pages 1015?1021, Geneva, Switzerland,Aug.
ICCL.424
