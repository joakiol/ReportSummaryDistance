Proceedings of NAACL-HLT 2013, pages 471?481,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsA Beam-Search Decoder for Normalization of Social Media Textwith Application to Machine TranslationPidong WangDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417wangpd@comp.nus.edu.sgHwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nght@comp.nus.edu.sgAbstractSocial media texts are written in an infor-mal style, which hinders other natural lan-guage processing (NLP) applications such asmachine translation.
Text normalization isthus important for processing of social mediatext.
Previous work mostly focused on nor-malizing words by replacing an informal wordwith its formal form.
In this paper, to fur-ther improve other downstream NLP applica-tions, we argue that other normalization oper-ations should also be performed, e.g., missingword recovery and punctuation correction.
Anovel beam-search decoder is proposed to ef-fectively integrate various normalization oper-ations.
Empirical results show that our systemobtains statistically significant improvementsover two strong baselines in both normaliza-tion and translation tasks, for both Chineseand English.1 IntroductionSocial media texts include SMS (Short MessageService) messages, Twitter messages, Facebook up-dates, etc.
They are different from formal texts dueto their significant informal characteristics, so theyalways pose difficulties for applications such as ma-chine translation (MT) (Aw et al 2005) and namedentity recognition (Liu et al 2011), because of alack of training data containing informal texts.
Thus,the applications always suffer from a substantial per-formance drop when evaluated on social media texts.For example, Ritter et al(2011) reported a dropfrom 90% to 76% on part-of-speech tagging, andFoster et al(2011) found a drop of 20% in depen-dency parsing.Creating training data of social media texts specif-ically for a text processing task is time-consuming.For example, to create parallel Chinese-Englishtraining texts for translation of social media texts,it takes three minutes on average to translate an in-formally written social media text of eleven wordsfrom Chinese into English.
On the other hand, ittakes thirty seconds to normalize the same message,a six-fold increase in speed.
After training a text nor-malization system to normalize social media texts,we can use an existing statistical machine translation(SMT) system trained on normal texts (non-socialmedia texts) to carry out translation.
So we arguethat normalization followed by regular translation isa more practical approach.
Thus, text normalizationis important for social media text processing.Most previous work on normalization of socialmedia text focused on word substitution (Beaufortet al 2010; Gouws et al 2011; Han and Baldwin,2011; Liu et al 2012).
However, we argue thatsome other normalization operations besides wordsubstitution are also critical for subsequent natu-ral language processing (NLP) applications, suchas missing word recovery (e.g., zero pronouns) andpunctuation correction.In this paper, we propose a novel beam-searchdecoder for normalization of social media text forMT.
Our decoder can effectively integrate differ-ent normalization operations together.
In contrastto previous work, some of our normalization opera-tions are specifically designed for MT, e.g., missingword recovery based on conditional random fields471(CRF) (Lafferty et al 2001) and punctuation cor-rection based on dynamic conditional random fields(DCRF) (Sutton et al 2004).To the best of our knowledge, our work is thefirst to perform missing word recovery and punc-tuation correction for normalization of social me-dia text, and also the first to perform message-levelnormalization of Chinese social media text.
We in-vestigate the effects on translating social media textafter addressing various characteristics of informalsocial media text through normalization.
To showthe applicability of our normalization approach fordifferent languages, we experiment with two lan-guages, Chinese and English.
We achieved statisti-cally significant improvements over two strong base-lines: an improvement of 9.98%/7.35% in BLEUscores for normalization of Chinese/English socialmedia text, and an improvement of 1.38%/1.35% inBLEU scores for translation of Chinese/English so-cial media text.
We created two corpora: a Chinesecorpus containing 1,000 Weibo1 messages with theirnormalizations and English translations; and anothersimilar English corpus containing 2,000 SMS mes-sages from the NUS SMS corpus (How and Kan,2005).
As far as we know, our corpora are the firstpublicly available Chinese/English corpora for nor-malization and translation of social media text2.2 Related WorkZhu et al(2007) performed text normalization ofinformally written email messages using CRF (Laf-ferty et al 2001).
Due to its importance, normaliza-tion of social media text has been extensively studiedrecently.
Aw et al(2005) proposed a noisy chan-nel model consisting of different operations: sub-stitution of non-standard acronyms, deletion of fla-vor words, and insertion of auxiliary verbs and sub-ject pronouns.
Choudhury et al(2007) used hid-den Markov model to perform word-level normal-ization.
Kobus et al(2008) combined MT and auto-matic speech recognition (ASR) to better normalizeFrench SMS message.
Cook and Stevenson (2009)used an unsupervised noisy channel model consid-ering different word formation processes.
Han andBaldwin (2011) normalized informal words using1A Chinese version of Twitter at www.weibo.com2Available at www.comp.nus.edu.sg/?nlp/corpora.htmlmorphophonemic similarity.
Pennell and Liu (2011)only dealt with SMS abbreviations.
Xue et al(2011)normalized social media texts incorporating ortho-graphic, phonetic, contextual, and acronym factors.Liu et al(2012) designed a system combining dif-ferent human perspectives to perform word-levelnormalization.
Oliva et al(2013) normalized Span-ish SMS messages using a normalization and a pho-netic dictionary.
For normalization of Chinese so-cial media text, Xia et al(2005) investigated infor-mal phrase detection, and Li and Yarowsky (2008)mined informal-formal phrase pairs from Web cor-pora.All the above work focused on normalizingwords.
In contrast, our work also performs othernormalization operations such as missing word re-covery and punctuation correction, to further im-prove machine translation.
Previously, Aw et al(2006) adopted phrase-based MT to perform SMSnormalization, and required a relatively large num-ber of manually normalized SMS messages.
In con-trast, our approach performs beam search at the sen-tence level, and does not require large training data.We evaluate the success of social media text nor-malization in the context of machine translation, soresearch on machine translation of social media textis relevant to our work.
However, there is not muchcomparative evaluation of social media text transla-tion other than the Haitian Creole to English SMStranslation task in the 2011 Workshop on StatisticalMachine Translation (WMT 2011) (Callison-Burchet al 2011).
However, the setup of the WMT 2011task is different from ours, in that the task providedparallel training data of SMS texts and their transla-tions.
As such, text normalization is not necessaryin that task.
For example, the best reported systemin that task (Costa-jussa` and Banchs, 2011) did notperform SMS message normalization.In speech to speech translation (Paul, 2009;Nakov et al 2009), the input texts contain wronglytranscribed words due to errors in automatic speechrecognition, whereas social media texts contain ab-breviations, new words, etc.
Although the inputtexts in both cases deviate from normal texts, the ex-act deviations are different.472Category Freq.
ExamplePunctuation 81 ??[hi]?(???
[hi .
]);Pronunciation 47 ?[watch](??[don?t]);??(???
[this]);New word 43 ?[bud](??
[cute]);Interjection 27 ??[ok]?[oh](??
[ok]);Pronoun 23 ??[want](?[i]??
[want]);Segmentation 14 ???(??[don?t]???
[this]);Pronunciation 288 4(for); oredi(already);Abbreviation 98 slp(sleep); whr(where);Prefix 74 lect(lecture); doin(doing);Punctuation 69 where r u(where r u ?
);Interjection 68 ok lor .
(ok .
);Quotation 24 im sure(i ?m sure); dont go(don ?t go);Be 24 i coming; you free?
;Tokenization 19 ok.why ?
(ok .
why ?
);Time 2 end at 730(end at 7:30); 1130 am(11:30 am);Table 1: Occurrence frequency of various informal char-acteristics in 200 Chinese/English social media texts.3 Challenges in Normalization of SocialMedia TextTo better understand the informal characteristics ofsocial media texts, we first analyzed a small sampleof such texts in Chinese and English.
We crawled200 Chinese messages from Weibo.
The informalcharacteristics of these messages are shown in thefirst half of Table 1.
The manually normalized formis shown in round brackets, and the English gloss isshown in square brackets.
Omitted, extraneous, andmisused punctuation symbols occur frequently.
Onaverage, each Chinese message contains only lessthan one informal word, and many informal wordsare either new words or existing words with newmeaning.
The messages also contain redundant in-terjections like ??[oh]?.
Pronouns are often omit-ted in Chinese messages, especially for ??[I]?.
Chi-nese informal words can be wrongly segmented dueto lack of word segmentation training data contain-ing informal words.Similarly, 200 English SMS messages were ran-domly selected from the NUS SMS corpus (Howand Kan, 2005).
The informal characteristics ofthese messages are shown in the second half of Ta-ble 1.
We found that our English messages containmore informal words than Chinese messages.
En-glish words are shortened in three ways: (1) usinga shorter word form with similar pronunciation; (2)abbreviating a formal word; and (3) using only a pre-fix of a formal word.
Other informal characteristicsinclude: (1) informal punctuation conventions in-cluding omitted and misused punctuation; (2) redun-dant interjections; (3) quotation-related problems,e.g., omitted quotation marks; (4) ?be?
omission;(5) tokenization problems; and (6) informally writ-ten time expressions.4 MethodsAs can be seen in Section 3, social media texts ofdifferent languages exhibit different informal char-acteristics.
For example, English messages havemore informal words than Chinese messages, whilepunctuation problems are more prevalent for Chi-nese messages.
Also, fixing different types of infor-mal characteristics often depends on each other.
Forexample, to be able to correct punctuation, it helpsthat the surrounding words are already correctly nor-malized.
On the other hand, with punctuation al-ready corrected, it will be easier to normalize thesurrounding words.In this section, we first present our punctuationcorrection method based on a DCRF model, andthen present missing word recovery based on a CRFmodel.
Next, we present a novel beam-search de-coder for normalization of social media text, whichcan effectively integrate different normalization op-erations, including statistical and rule-based normal-ization.
Finally, details of text normalization forChinese and English are presented.4.1 Punctuation CorrectionIn normalization of social media text, punctuationcorrection is also important besides word normal-ization, as the subsequent NLP applications are typ-ically trained on formal texts with correct punctua-tion.
We define punctuation correction as correctingpunctuation in sentences which may have no or un-reliable punctuation.
The task performs three punc-tuation operations: insertion, deletion, and substitu-tion.To our knowledge, no previous work has beendone on punctuation correction for normalization ofsocial media text.
In ASR, punctuation predictiononly inserts punctuation symbols into ASR outputthat has no punctuation (Kim and Woodland, 2001;Huang and Zweig, 2002), but without punctuationdeletion or substitution.
Lu and Ng (2010) arguedthat punctuation prediction should be jointly per-473formed with sentence boundary detection, so theymodeled punctuation prediction using a two-layerDCRF model (Sutton et al 2004).We also believe that punctuation correction isclosely related to sentence boundary detection.Thus, we propose a two-layer DCRF model forpunctuation correction.
Layer 1 gives the actualpunctuation tags None, Comma, Period, Question-Mark, and Exclamatory-Mark.
Layer 2 givesthe sentence boundary, including tags Declarative-Begin, Declarative-In, Question-Begin, Question-In,Exclamatory-Begin, and Exclamatory-In, indicatingwhether the current word is at the beginning of (orinside) a declarative, question, or exclamatory sen-tence.We use word n-grams (n = 1, 2, 3) and punctu-ation symbols within 5 words before and after thecurrent word as binary features in the DCRF model.As an example, Table 2 shows the tags and featuresfor the word ?where?
in the message ?where| .|?
i|can| not| see| you| !|!
?, where the punctuation sym-bols after the vertical bars are the corrected symbols.Tags ContentLayer 1 Question-MarkLayer 2 Question-BeginFeatures Contentunigram <s>@-1 where@0 i@1 can@2 not@3see@4 you@5bigram <s>+where@-1 where+i@0 i+can@1can+not@2 not+see@3 see+you@4you+</s>@5trigram <s>+where+i@-1 where+i+can@0i+can+not@1 can+not+see@2not+see+you@3 see+you+</s>@4punctuation .
@0 !
@5Table 2: An example of tags and features used in punctu-ation correction.Due to the lack of informal training texts with cor-rected punctuation, we train our punctuation correc-tion model on formal texts with synthetically cre-ated punctuation errors.
We randomly add, delete,and substitute punctuation symbols in formal textswith equal probabilities.
Specifically, for s ?
{, .?!
},P (none|s) = P (, |s) = P (.|s) = P (?|s) =P (!|s) = 0.2 denotes the probability of replacinga punctuation symbol s (replacing s by none de-notes deletion); and for a real word (not a punctua-tion symbol)w, P (none|w) = P (, |w) = P (.|w) =P (?|w) = P (!|w) = 0.2 denotes the probabilityof inserting a punctuation symbol after w (insertingnone after w denotes no insertion).4.2 Missing Word RecoveryAs shown in Section 3, some words are often omit-ted in social media texts, e.g., the pronoun ??
[I]?in Chinese and be in English.
To fix this problem,we propose a CRF model to recover such missingwords.
We explain the CRF model using be in En-glish.
The CRF model has five tags: None, BE, IS,ARE, and AM.
In an input sentence, every token (in-cluding words, punctuation symbols, and a specialstart-of-sentence placeholder) will be assigned a tag,denoting the insertion of the form of be after the to-ken.
We use the same n-gram features as our punc-tuation correction model, but exclude the punctua-tion features.
The model is trained on syntheticallycreated training texts in which be has been randomlydeleted with probability 0.5.4.3 A Decoder for Text NormalizationWhen designing our text normalization system, weaim for a general framework that can be applied totext normalization across different languages withminimal effort.
This is a challenging task, since so-cial media texts in different languages exhibit differ-ent informal characteristics, as illustrated in Section3.
Motivated by the beam-search decoders for SMT(Koehn et al 2007), ASR (Young et al 2002), andgrammatical error correction (Dahlmeier and Ng,2012), we propose a novel beam-search decoder fornormalization of social media text.Given an input message, the normalization de-coder searches for its best normalization, i.e., thebest hypothesis, by iteratively performing two sub-tasks: (1) producing new sentence-level hypothesesfrom hypotheses in the current stack, carried out byhypothesis producers; and (2) evaluating the new hy-potheses to retain good ones, carried out by featurefunctions.
Each hypothesis is the result of apply-ing successive normalization operations on the ini-tial input message, where each normalization oper-ation is carried out by one hypothesis producer thatdeals with one aspect of the informal characteristicsof social media text.
The hypotheses are groupedinto stacks, where stack i stores all hypotheses ob-tained by applying i hypothesis producers on the in-put message.
The beam-search algorithm is shown474where you .whr you arewhr uwhr youDictionary: u=>youbewhere youAbbreviation: whr=>wherePunctuationwhere are youBewhere are you ?PunctuationFigure 1: An example search tree when normalizing ?whru?.
The solid (dashed) boxes represent good (bad) hy-potheses.
The hypothesis producers are indicated on theedges.in Algorithm 1, and Figure 1 shows an examplesearch tree for an English message.Algorithm 1 The beam-search decoderINPUT: a raw message M whose length is NRETURN: the best normalization for M1: initialize hypothesisStacks[N+1] and hypothesisProducers;2: add the initial hypothesis M to stack hypothesisStacks[0];3: for i?
0 to N-1 do4: for each hypo in hypothesisStacks[i] do5: for each producer in hypothesisProducers do6: for each newHypo produced by producer from hypo do7: add newHypo to hypothesisStacks[i+1];8: prune hypothesisStacks[i+1];9: return the best hypothesis in hypothesisStacks[0...N];We give the details of the hypothesis producersfor Chinese and English social media texts in thenext two subsections.
A number of the hypothesisproducers detect and deal with informal words wpresent in a hypothesis by relying on bigram countsof w in a large corpus of formal texts.
Specifically,a word w in a hypothesis .
.
.
w?1ww1 .
.
.
is consid-ered an informal word if both bigrams w?1w andww1 occur infrequently (?
5) in the formal corpus.Given a hypothesis message h, the feature func-tions include a language model score (the normal-ized sentence probability of h), an informal wordcount penalty (the number of informal words de-tected in h), and count feature functions.
Each countfeature function gives the count of the modificationsmade by a hypothesis producer.
The feature func-tions are used by the decoder to distinguish goodhypotheses from bad ones.
All feature functions arecombined using a linear model to obtain the scorefor a hypothesis h:score(h) =?i?ifi(h), (1)where fi is the i-th feature function with weight ?i.The weights of the feature functions are tuned usingthe pairwise ranking optimization algorithm (Hop-kins and May, 2011) on the development set.4.4 Text Normalization for ChineseTaking into account the informal characteristics ofChinese social media text in Section 3, we design thefollowing hypothesis producers for Chinese textnormalization:Dictionary: We have manually assembled a dic-tionary of 703 informal-formal word pairs from theInternet.
The word pairs are used to produce newhypotheses.
For example, given a hypothesis ???
[magical horse] ??
[time]?, if the dictionarycontains the word pair ?(??,??
[what])?, theDictionary hypothesis producer generates a new hy-pothesis ???[what]??
[time]?.Punctuation: A punctuation correction model(Section 4.1) is adopted to correct punctuation inthe current hypothesis, e.g., it may normalize ???[what]??[time]?
into ??????
?.Pronunciation: We use Chinese Pinyin to modelthe pronunciation similarity of words.
To accom-plish this, we pair some Pinyin initials that soundsimilar into a group.
The groups of paired Pinyininitials are (c, ch), (s, sh), and (z, zh).
For exam-ple, given the hypothesis ???[Beijing]??[tube]??
[come]?, the Pinyin of the informal word ????
is ?t ong z i?.
The Pinyin of the formal word???[comrade]?
is ?t ong zh i?.
Since the sim-ilar sounding Pinyin initials z and zh are pairedin a group, a new hypothesis ???
[Beijing] ??[comrade]??[come]?
can be produced.In practice, this hypothesis producer can proposemany spurious candidates w?
for an informal wordw.
As such, after we replace w by w?
in the hypoth-esis, we require that some 4-gram containing w?
andits surrounding words in the hypothesis appears in aformal corpus.
We call this filtering process contex-tual filtering.475Pronoun: With the method of Section 4.2, a CRFmodel is trained to recover the missing pronoun??
[I]?.Interjection: If a word w in a pre-defined listof frequent redundant interjections appears at theend of a sentence, we produce a new hypothesis byremoving w, e.g., from ???
[ok] ?[oh]?
to ???
[ok]?.Resegmentation: This hypothesis producer fixesword segmentation problems.
If an informal word isa concatenation of two constituent informal wordsw1 and w2 in our normalization dictionary, the in-formal word will be segmented into two words w1and w2.
As a result, the Dictionary hypothesis pro-ducer can subsequently normalize w1 and w2.4.5 Text Normalization for EnglishSimilar to Chinese text normalization, we also cre-ate the Dictionary, Punctuation, and Interjection hy-pothesis producers for English text normalization.We also add the following English-specific hypoth-esis producers:Pronunciation: This hypothesis producer usespronunciation similarity to find formal candidatesfor a given informal word.
It considers a word asa sequence of letters and convert it into a sequenceof phones using phrase-based SMT trained on theCMU pronouncing dictionary (Weide, 1998).
Simi-lar sounding phones are paired together in a group:(ah, ao), (ow, uw), and (s, z).
To illustrate, in the hy-pothesis ?wat is it?, the informal word ?wat?
mapsto the phone sequence ?w ao t?.
Since the formalword ?what?
maps to the phone sequence ?w ah t?and the phones ah and ao are paired in a group, thenew hypothesis ?what is it?
is generated.Be: We train a CRF model to recover missingwords be, as described in Section 4.2.Retokenization: This hypothesis producer fixestokenization problems.
More precisely, given an in-formal word which is not a URL or email addressand contains a period, it splits the informal word atthe period.
For example, ?how r u.where r u?
is nor-malized to ?how r u .
where r u?.Prefix: This hypothesis producer generates a for-mal word w?
for an informal word w if w is a prefixof w?.
To avoid spurious candidates, we only gener-ate w?
if |w| ?
3 and |w?| ?
|w| ?
4.Quotation: If an informal word ends with a letterin (m, s, t) and if the word produced by inserting aquotation mark before the letter is a formal word, anew hypothesis with the quotation mark inserted isproduced.
This hypothesis producer thus generates?i?m?
from ?im?, ?she?s?
from ?shes?, ?isn?t?
from?isnt?, etc.Abbreviation: Letters denoting the vowels in aformal word are often deleted to form an infor-mal word.
This hypothesis producer generates aformal word w?
from an informal word w if w?can be obtained from w by adding missing vowels.To avoid spurious candidates, we only consider wwhere |w| ?
2.Time: If a number can be a potential time expres-sion and appears after ?at?
or before ?am?
or ?pm?, anew hypothesis is produced by changing the numberinto a time expression, e.g., ?1130 am?
is normal-ized to ?11 : 30 am?.Since the Pronunciation, Prefix, and Abbreviationhypothesis producers can propose spurious candi-dates for an informal word, we also use contextualfiltering to further filter the candidates for these hy-pothesis producers.5 Experiments5.1 Evaluation CorporaAs previous work (Choudhury et al 2007; Han andBaldwin, 2011; Liu et al 2012) mostly focusedon word normalization, no data is available withcorrected punctuation and recovered missing words.We thus create the following two corpora (Table 3):Chinese-English corpus We crawled 1,000 mes-sages from Weibo which were first normalized intoformal Chinese and then translated into formal En-glish.
The first half of the corpus serves as our de-velopment set to tune our text normalization decoderfor Chinese, while the second half serves as the testset to evaluate text normalization for Chinese andChinese-English MT.English-Chinese corpus From the NUS EnglishSMS corpus (How and Kan, 2005), we randomly se-lected 2,000 messages.
The messages were first nor-malized into formal English and then translated intoformal Chinese.
Similar to the Chinese-English cor-pus, the first half of the corpus serves as our devel-opment set while the second half serves as the testset.476Corpus # messages # tokens (EN/CN/NCN)CN2EN-dev 500 6.95K/5.45K/5.70KCN2EN-test 500 7.14K/5.64K/5.82KCorpus # messages # tokens (EN/CN/NEN)EN2CN-dev 1,000 16.63K/18.14K/18.21KEN2CN-test 1,000 16.14K/17.69K/17.76KTable 3: Statistics of the corpora.
CN2EN-dev/CN2EN-test is the development/test set in our Chinese-English experiments.
EN2CN-dev/EN2CN-test is thedevelopment/test set in our English-Chinese experi-ments.
NEN/NCN denotes manually normalized En-glish/Chinese texts.The formal corpus used (as described in Section4) is the concatenation of two Chinese-English spo-ken parallel corpora: the IWSLT 2009 corpus (Paul,2009) and another spoken text corpus collected atthe Harbin Institute of Technology3.
The languagemodel used for Chinese (English) text normalizationis the Chinese (English) side of the formal corpusand the LDC Chinese (English) Gigaword corpus.To evaluate the effect of text normalizationon MT, we build phrase-based MT systemsusing Moses (Koehn et al 2007), with wordalignments generated by GIZA++ (Och andNey, 2003).
The MT training data containsthe above formal corpus and some LDC4 par-allel corpora (LDC2000T46, LDC2002E18,LDC2003E14, LDC2004E12, LDC2005T06,LDC2005T10, LDC2007T23, LDC2008T06,LDC2008T08, LDC2008T18, LDC2009T02,LDC2009T06, LDC2009T15, LDC2010T03).
Intotal, 214M/192M English/Chinese tokens are usedto train our MT systems.
The language modelof the Chinese-English (English-Chinese) MTsystem is the English (Chinese) side of the FBIScorpus (LDC2003E14) and the English (Chinese)Gigaword corpus.
Our MT systems are tuned on themanually normalized messages of our developmentsets.Following (Aw et al 2006; Oliva et al 2013),we use BLEU scores (Papineni et al 2002) to eval-uate text normalization.
We also use BLEU scoresto evaluate MT quality.
We use the sign test to de-termine statistical significance, for both text normal-ization and translation.3http://mitlab.hit.edu.cn/4http://www.ldc.upenn.edu/Catalog/5.2 BaselinesWe compare our text normalization decoder againstthree baseline methods for performing text normal-ization.
We then send the respective normalizedtexts to the same MT system to evaluate the effectof text normalization on MT.The simplest baseline for text normalization isone that does no text normalization.
The raw text(un-normalized) is simply passed on to the MT sys-tem for translation.
We call this baseline ORIGINAL.The second baseline, LATTICE, is to use a latticeto normalize text.
For each input message, a latticeis generated in which each informal word is aug-mented with its formal candidates taken from thesame normalization dictionary (downloaded fromInternet) used in our text normalization decoder.
Thelattice is then decoded by the same language modelused in our text normalization decoder to generatethe normalized text (Stolcke, 2002).
Another pos-sible way of using lattice is to directly feed the lat-tice to the MT system (Eidelman et al 2011), butsince in this paper, we assume that the MT systemcan only translate plain text, we leave this as futurework.The third baseline, PBMT, is a competitive base-line that performs text normalization via phrase-based MT, as proposed in Aw et al(2006).
Moses(Koehn et al 2007) is used to perform text normal-ization, by ?translating?
un-normalized text to nor-malized text.
The training data used is the samedevelopment set used in our text normalization de-coder.
The normalized text is then sent to our MTsystem for translation.
This method was also used inthe SMS translation task of WMT 2011 by (Stymne,2011).In the tables showing experimental results, nor-malization and translation BLEU scores that are sig-nificantly higher than (p < 0.01) the LATTICE orPBMT baseline are in bold or underlined, respec-tively.5.3 Chinese-English Experimental ResultsThe Chinese-English normalization and translationresults are shown in Table 4.
The first group ofexperiments is the three baselines, and the secondgroup is an oracle experiment using manually nor-malized messages as the output of text normaliza-477BLEU scores (%)System Normalization MTORIGINAL baseline 61.01 9.06LATTICE baseline 74.52 11.50PBMT baseline 76.77 12.65ORACLE 100.00 15.04Dictionary 77.80 12.35Punctuation 65.95 9.63Pronunciation 61.30 9.13Pronoun 61.11 9.01Interjection 61.05 9.14Resegmentation 60.98 9.03Dictionary 77.80 12.35+Punctuation 84.69 13.37+Pronunciation 84.69 13.40+Pronoun 84.96 13.50+Interjection 85.33 13.68+Resegmentation 86.75 14.03Table 4: Chinese-English experimental results.tion which indicates the theoretical upper bounds ofperfect normalization.
In the normalization experi-ments, the ORIGINAL baseline gets a BLEU scoreof 61.01%, and the LATTICE baseline greatly im-proves the ORIGINAL baseline by 13.51%, whichshows that the dictionary collected from the Inter-net is highly effective in text normalization.
ThePBMT baseline further improves the BLEU score by2.25%.
In the corresponding MT experiments, as thenormalization BLEU scores increase, the MT BLEUscores also increase.The third group is the isolated experiments, i.e.,each experiment only uses one hypothesis producer.As expected, the individual hypothesis producersalone do not work well except the Dictionary hy-pothesis producer.
One interesting discovery is thatthe Dictionary hypothesis producer outperforms theLATTICE baseline, which shows that our normaliza-tion decoder can utilize the dictionary more effec-tively, probably because of the additional featuresused in our normalization decoder such as the infor-mal word penalty.
The Resegmentation hypothesisproducer alone worsens the BLEU scores, since itcan only split informal words, and is designed towork together with other hypothesis producers tonormalize words.The last group is the combined experiments.
Weadd each hypothesis producer in the order of its nor-malization effectiveness in the isolated experiments.Adding the Punctuation hypothesis producer greatlyimproves the BLEU scores of both normalizationand translation, which confirms the importance ofpunctuation correction.
The Pronoun and Inter-jection hypothesis producers also contribute someimprovements.
Finally, Resegmentation signifi-cantly improves the normalization/translation BLEUscores by 1.42%/0.35%.
Compared with the isolatedexperiments, the combined experiments show thatour normalization decoder can effectively integratedifferent hypothesis producers to achieve better per-formance for both text normalization and transla-tion.Overall, in the Chinese text normalization exper-iments, our normalization decoder outperforms thebest baseline PBMT by 9.98% in BLEU score.
Inthe Chinese-English MT experiments, the normal-ized texts output by our normalization decoder leadto improved translation quality compared to normal-ization by the PBMT baseline, by 1.38% in BLEUscore.5.4 English-Chinese Experimental ResultsThe English-Chinese normalization and translationresults are shown in Table 5, with the same experi-mental setup as in the Chinese-English experiments.The text normalization BLEU score of the ORIG-INAL baseline is much lower in English comparedto Chinese, since the English texts contain more in-formal words.
Again, the individual hypothesis pro-ducers alone do not work well, except the Dictio-nary hypothesis producer.
The Retokenization hy-pothesis producer greatly improves the normaliza-tion/translation BLEU scores by 2.37%/0.86%.
ThePunctuation hypothesis producer helps less for En-glish compared to Chinese, suggesting that our Chi-nese texts contain noisier punctuation.Overall, we achieved similar improvements in En-glish text normalization and English-Chinese trans-lation, and the improvements in BLEU scores are7.35% and 1.35% respectively.5.5 Further AnalysisThe effect of contextual filtering.
To measurethe effect of contextual filtering proposed in Sec-tion 4.4, we ran our normalization decoder with-out contextual filtering.
We obtained BLEU scoresof 65.05%/22.38% in the English-Chinese experi-ments, which were lower than 66.54%/22.81% ob-478BLEU scores (%)System Normalization MTORIGINAL baseline 37.38 13.63LATTICE baseline 56.98 20.56PBMT baseline 59.19 21.46ORACLE 100.00 28.48Dictionary 59.90 20.84Retokenization 38.79 14.06Prefix 38.68 13.90Interjection 38.37 13.92Quotation 38.04 13.65Abbreviation 37.94 13.74Time 37.65 13.66Pronunciation 37.62 13.80Punctuation 37.62 13.79Be 37.47 13.59Dictionary 59.90 20.84+Retokenization 62.27 21.70+Prefix 63.22 21.88+Interjection 64.85 22.30+Quotation 65.24 22.31+Abbreviation 65.35 22.34+Time 65.59 22.38+Pronunciation 65.64 22.38+Punctuation 66.38 22.74+Be 66.54 22.81Table 5: English-Chinese experimental results.tained with contextual filtering.
This shows the ben-eficial effect of contextual filtering.Decoding speed.
The decoding speed of our textnormalization decoder was 0.2 seconds per messageon our test sets, using a 2.27 GHz Intel Xeon CPUwith 32 GB memory.The effect of text normalization decoder onMT.
We manually analyzed the effect of our textnormalization decoder on MT.
For example, giventhe un-normalized English test message ?yeah mustsign up , im in lt25?, our English-Chinese MTsystem translated it into ??
[yeah] ??
[must] ??
[sign up] ?
im ?
[in] lt25?
On the other hand,our normalization decoder normalized it into ?yeahmust sign up , i ?m in lt25 .?
which was then trans-lated into ??????
,??
lt25??
by our MTsystem.
This example shows that our text normal-ization decoder uses word normalization and punc-tuation correction to improve translation.6 ConclusionThis paper presents a novel beam-search decoderfor normalization of social media text.
Our de-coder for text normalization effectively integratesmultiple normalization operations.
In our experi-ments, we achieved statistically significant improve-ments over two strong baselines: an improvement of9.98%/7.35% in BLEU scores for normalization ofChinese/English social media text, and an improve-ment of 1.38%/1.35% in BLEU scores for transla-tion of Chinese/English social media text.
Futurework can investigate how to more tightly integrateour beam-search decoder for text normalization witha standard MT decoder, e.g., by using a lattice or ann-best list.AcknowledgmentsWe thank all the anonymous reviewers for their com-ments which have helped us improve this paper.This research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesAiTi Aw, Min Zhang, PohKhim Yeo, ZhenZhen Fan, andJian Su.
2005.
Input normalization for an English-to-Chinese SMS translation system.
In Proceedings ofthe Tenth MT Summit.AiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.
Aphrase-based statistical model for SMS text normaliza-tion.
In Proceedings of ACL-COLING.Richard Beaufort, Sophie Roekhaut, Louise-Ame?lieCougnon, and Ce?drick Fairon.
2010.
A hybridrule/model-based finite-state framework for normaliz-ing SMS messages.
In Proceedings of ACL.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In Proceedingsof WMT.Monojit Choudhury, Rahul Saraf, Vijit Jain, AnimeshMukherjee, Sudeshna Sarkar, and Anupam Basu.2007.
Investigation and modeling of the structure oftexting language.
International Journal on DocumentAnalysis and Recognition, 10(3-4):157?174.Paul Cook and Suzanne Stevenson.
2009.
An unsu-pervised model for text message normalization.
InProceedings of the Workshop on Computational Ap-proaches to Linguistic Creativity.Marta R. Costa-jussa` and Rafael E. Banchs.
2011.
TheBM-I2R Haitian-Cre?ole-to-English translation systemdescription for the WMT 2011 evaluation campaign.In Proceedings of WMT.Daniel Dahlmeier and Hwee Tou Ng.
2012.
A beam-search decoder for grammatical error correction.
InProceedings of EMNLP-CoNLL.479Vladimir Eidelman, Kristy Hollingshead, and PhilipResnik.
2011.
Noisy SMS machine translation in low-density languages.
In Proceedings of WMT.Jennifer Foster, O?zlem C?etinoglu, Joachim Wagner,Joseph Le Roux, Stephen Hogan, Joakim Nivre,Deirdre Hogan, and Josef Van Genabith.
2011.
#hardtoparse: POS tagging and parsing the twitterverse.In Proceedings of the AAAI Workshop On AnalyzingMicrotext.Stephan Gouws, Dirk Hovy, and Donald Metzler.
2011.Unsupervised mining of lexical variants from noisytext.
In Proceedings of the First Workshop on Unsu-pervised Learning in NLP.Bo Han and Timothy Baldwin.
2011.
Lexical normalisa-tion of short text messages: Makn sens a #twitter.
InProceedings of ACL-HLT.Mark Hopkins and Jonathan May.
2011.
Tuning as rank-ing.
In Proceedings of EMNLP.Yijue How and Min-Yen Kan. 2005.
Optimizing pre-dictive text entry for short message service on mobilephones.
In Proceedings of Human Computer Inter-faces International.Jing Huang and Geoffrey Zweig.
2002.
Maximum en-tropy model for punctuation annotation from speech.In Proceedings of ICSLP.Ji Hwan Kim and P. C. Woodland.
2001.
The use ofprosody in a combined system for punctuation gener-ation and speech recognition.
In Proceedings of Eu-rospeech.Catherine Kobus, Franc?ois Yvon, and Ge?raldineDamnati.
2008.
Normalizing SMS: are two metaphorsbetter than one?
In Proceedings of COLING.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL Demo and Poster Sessions.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of the Eighteenth International Con-ference on Machine Learning.Zhifei Li and David Yarowsky.
2008.
Mining and mod-eling relations between formal and informal Chinesephrases from web corpora.
In Proceedings of EMNLP.Xiaohua Liu, Shaodian Zhang, Furu Wei, and MingZhou.
2011.
Recognizing named entities in tweets.In Proceedings of ACL-HLT.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
A broad-coverage normalization system for social media lan-guage.
In Proceedings of ACL.Wei Lu and Hwee Tou Ng.
2010.
Better punctuationprediction with dynamic conditional random fields.
InProceedings of EMNLP.Preslav Nakov, Chang Liu, Wei Lu, and Hwee Tou Ng.2009.
The NUS statistical machine translation systemfor IWSLT 2009.
In Proceedings of the InternationalWorkshop on Spoken Language Translation.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.J.
Oliva, J. I. Serrano, M. D. Del Castillo, and A?.
Igesias.2013.
A SMS normalization system integrating mul-tiple grammatical resources.
Natural Language Engi-neering, 19(1):121?141.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL.Michael Paul.
2009.
Overview of the IWSLT 2009 eval-uation campaign.
In Proceedings of the InternationalWorkshop on Spoken Language Translation.Deana L. Pennell and Yang Liu.
2011.
A character-level machine translation approach for normalizationof SMS abbreviations.
In Proceedings of IJCNLP.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: An experi-mental study.
In Proceedings of EMNLP.Andreas Stolcke.
2002.
SRILM ?
An extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing.Sara Stymne.
2011.
Spell checking techniques for re-placement of unknown words and data cleaning forHaitian Creole SMS translation.
In Proceedings ofWMT.Charles Sutton, Khashayar Rohanimanesh, and AndrewMcCallum.
2004.
Dynamic conditional randomfields: Factorized probabilistic models for labeling andsegmenting sequence data.
In Proceedings of the 21stInternational Conference on Machine Learning.Robert L. Weide.
1998.
The CMU pronouncingdictionary.
URL: http://www.speech.cs.cmu.edu/cgi-bin/cmudict.Yunqing Xia, Kam-Fai Wong, and Wei Gao.
2005.
NILis not nothing: Recognition of Chinese network infor-mal language expressions.
In 4th SIGHAN Workshopon Chinese Language Processing.Zhenzhen Xue, Dawei Yin, and Brian D. Davison.
2011.Normalizing microtext.
In Proceedings of the AAAIWorkshop on Analyzing Microtext.Steve Young, Gunnar Evermann, Dan Kershaw, GarethMoore, Julian Odell, Dave Ollason, Valtcho Valtchev,and Phil Woodland.
2002.
The HTK book.
Cam-bridge University Engineering Department.480Conghui Zhu, Jie Tang, Hang Li, Hwee Tou Ng, and Tie-Jun Zhao.
2007.
A unified tagging approach to textnormalization.
In Proceedings of ACL.481
