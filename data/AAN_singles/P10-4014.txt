Proceedings of the ACL 2010 System Demonstrations, pages 78?83,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsIt Makes Sense: A Wide-Coverage Word Sense Disambiguation Systemfor Free TextZhi Zhong and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417{zhongzhi, nght}@comp.nus.edu.sgAbstractWord sense disambiguation (WSD)systems based on supervised learningachieved the best performance in SensE-val and SemEval workshops.
However,there are few publicly available opensource WSD systems.
This limits the useof WSD in other applications, especiallyfor researchers whose research interestsare not in WSD.In this paper, we present IMS, a supervisedEnglish all-words WSD system.
The flex-ible framework of IMS allows users to in-tegrate different preprocessing tools, ad-ditional features, and different classifiers.By default, we use linear support vectormachines as the classifier with multipleknowledge-based features.
In our imple-mentation, IMS achieves state-of-the-artresults on several SensEval and SemEvaltasks.1 IntroductionWord sense disambiguation (WSD) refers to thetask of identifying the correct sense of an ambigu-ous word in a given context.
As a fundamentaltask in natural language processing (NLP), WSDcan benefit applications such as machine transla-tion (Chan et al, 2007a; Carpuat and Wu, 2007)and information retrieval (Stokoe et al, 2003).In previous SensEval workshops, the supervisedlearning approach has proven to be the most suc-cessful WSD approach (Palmer et al, 2001; Sny-der and Palmer, 2004; Pradhan et al, 2007).
Inthe most recent SemEval-2007 English all-wordstasks, most of the top systems were based on su-pervised learning methods.
These systems useda set of knowledge sources drawn from sense-annotated data, and achieved significant improve-ments over the baselines.However, developing such a system requiresmuch effort.
As a result, very few open sourceWSD systems are publicly available ?
the onlyother publicly available WSD system that we areaware of is SenseLearner (Mihalcea and Csomai,2005).
Therefore, for applications which employWSD as a component, researchers can only makeuse of some baselines or unsupervised methods.An open source supervised WSD system will pro-mote the use of WSD in other applications.In this paper, we present an English all-wordsWSD system, IMS (It Makes Sense), built using asupervised learning approach.
IMS is a Java im-plementation, which provides an extensible andflexible platform for researchers interested in us-ing a WSD component.
Users can choose differ-ent tools to perform preprocessing, such as tryingout various features in the feature extraction step,and applying different machine learning methodsor toolkits in the classification step.
FollowingLee and Ng (2002), we adopt support vector ma-chines (SVM) as the classifier and integrate mul-tiple knowledge sources including parts-of-speech(POS), surrounding words, and local collocationsas features.
We also provide classification mod-els trained with examples collected from paralleltexts, SEMCOR (Miller et al, 1994), and the DSOcorpus (Ng and Lee, 1996).A previous implementation of the IMS sys-tem, NUS-PT (Chan et al, 2007b), participated inSemEval-2007 English all-words tasks and rankedfirst and second in the coarse-grained and fine-grained task, respectively.
Our current IMS im-plementation achieves competitive accuracies onseveral SensEval/SemEval English lexical-sampleand all-words tasks.The remainder of this paper is organized asfollows.
Section 2 gives the system description,which introduces the system framework and thedetails of the implementation.
In Section 3, wepresent the evaluation results of IMS on SensE-78val/SemEval English tasks.
Finally, we concludein Section 4.2 System DescriptionIn this section, we first outline the IMS system,and introduce the default preprocessing tools, thefeature types, and the machine learning methodused in our implementation.
Then we briefly ex-plain the collection of training data for contentwords.2.1 System ArchitectureFigure 1 shows the system architecture of IMS.The system accepts any input text.
For each con-tent word w (noun, verb, adjective, or adverb) inthe input text, IMS disambiguates the sense of wand outputs a list of the senses of w, where eachsense si is assigned a probability according to thelikelihood of si appearing in that context.
Thesense inventory used is based on WordNet (Miller,1990) version 1.7.1.IMS consists of three independent modules:preprocessing, feature and instance extraction, andclassification.
Knowledge sources are generatedfrom input texts in the preprocessing step.
Withthese knowledge sources, instances together withtheir features are extracted in the instance and fea-ture extraction step.
Then we train one classifica-tion model for each word type.
The model will beused to classify test instances of the correspondingword type.2.1.1 PreprocessingPreprocessing is the step to convert input texts intoformatted information.
Users can integrate differ-ent tools in this step.
These tools are applied on theinput texts to extract knowledge sources such assentence boundaries, part-of-speech tags, etc.
Theextracted knowledge sources are stored for use inthe later steps.In IMS, preprocessing is carried out in foursteps:?
Detect the sentence boundaries in a raw inputtext with a sentence splitter.?
Tokenize the split sentences with a tokenizer.?
Assign POS tags to all tokens with a POS tag-ger.?
Find the lemma form of each token with alemmatizer.By default, the sentence splitter and POS tag-ger in the OpenNLP toolkit1 are used for sen-tence splitting and POS tagging.
A Java version ofPenn TreeBank tokenizer2 is applied in tokeniza-tion.
JWNL3, a Java API for accessing the Word-Net (Miller, 1990) thesaurus, is used to find thelemma form of each token.2.1.2 Feature and Instance ExtractionAfter gathering the formatted information in thepreprocessing step, we use an instance extractortogether with a list of feature extractors to extractthe instances and their associated features.Previous research has found that combiningmultiple knowledge sources achieves high WSDaccuracy (Ng and Lee, 1996; Lee and Ng, 2002;Decadt et al, 2004).
In IMS, we follow Lee andNg (2002) and combine three knowledge sourcesfor all content word types4:?
POS Tags of Surrounding Words We usethe POS tags of three words to the left andthree words to the right of the target ambigu-ous word, and the target word itself.
ThePOS tag feature cannot cross sentence bound-ary, which means all the associated surround-ing words should be in the same sentence asthe target word.
If a word crosses sentenceboundary, the corresponding POS tag valuewill be assigned as null.For example, suppose we want to disam-biguate the word interest in a POS-taggedsentence ?My/PRP$ brother/NN has/VBZalways/RB taken/VBN a/DT keen/JJ inter-est/NN in/IN my/PRP$ work/NN ./.?.
The 7POS tag features for this instance are <VBN,DT, JJ, NN, IN, PRP$, NN>.?
Surrounding Words Surrounding words fea-tures include all the individual words in thesurrounding context of an ambiguous wordw.
The surrounding words can be in the cur-rent sentence or immediately adjacent sen-tences.However, we remove the words that are ina list of stop words.
Words that containno alphabetic characters, such as punctuation1http://opennlp.sourceforge.net/2http://www.cis.upenn.edu/?treebank/tokenizer.sed3http://jwordnet.sourceforge.net/4Syntactic relations are omitted for efficiency reason.79I n p u t D o c u m e n t C l a s s i f icatio n O u t p u tM a c h i n e L e a r n i n gT o o l k i tP r e p r o c es s in gI nstan c e E x t rac tio nI nstan c e E x t rac t o rF eat u r e E x t rac tio nP O S F e a t u r eE x t r a c t o r?
?L o c a l C o l l o c a t i o nE x t r a c t o rS u r r o u n d i n g W o r dE x t r a c t o rS e n t e n c e S p l i t t e rT o k e n i z e rP O S T a g g e rL e m m a t i z e r?
?Figure 1: IMS system architecturesymbols and numbers, are also discarded.The remaining words are converted to theirlemma forms in lower case.
Each lemma isconsidered as one feature.
The feature valueis set to be 1 if the corresponding lemma oc-curs in the surrounding context of w, 0 other-wise.For example, suppose there is a set of sur-rounding words features {account, economy,rate, take} in the training data set of the wordinterest.
For a test instance of interest inthe sentence ?My brother has always taken akeen interest in my work .
?, the surroundingword feature vector will be <0, 0, 0, 1>.?
Local Collocations We use 11 local collo-cations features including: C?2,?2, C?1,?1,C1,1, C2,2, C?2,?1, C?1,1, C1,2, C?3,?1,C?2,1, C?1,2, and C1,3, where Ci,j refers toan ordered sequence of words in the samesentence of w. Offsets i and j denote thestarting and ending positions of the sequencerelative to w, where a negative (positive) off-set refers to a word to the left (right) of w.For example, suppose in the training data set,the word interest has a set of local colloca-tions {?account .
?, ?of all?, ?in my?, ?tobe?}
for C1,2.
For a test instance of inter-est in the sentence ?My brother has alwaystaken a keen interest in my work .
?, the valueof feature C1,2 will be ?in my?.As shown in Figure 1, we implement one fea-ture extractor for each feature type.
The IMS soft-ware package is organized in such a way that userscan easily specify their own feature set by im-plementing more feature extractors to exploit newfeatures.2.1.3 ClassificationIn IMS, the classifier trains a model for each wordtype which has training data during the trainingprocess.
The instances collected in the previousstep are converted to the format expected by themachine learning toolkit in use.
Thus, the classifi-cation step is separate from the feature extractionstep.
We use LIBLINEAR5 (Fan et al, 2008) asthe default classifier of IMS, with a linear kerneland all the parameters set to their default values.Accordingly, we implement an interface to convertthe instances into the LIBLINEAR feature vectorformat.The utilization of other machine learning soft-ware can be achieved by implementing the corre-sponding module interfaces to them.
For instance,IMS provides module interfaces to the WEKA ma-chine learning toolkit (Witten and Frank, 2005),LIBSVM6, and MaxEnt7.The trained classification models will be ap-plied to the test instances of the correspondingword types in the testing process.
If a test instanceword type is not seen during training, we will out-put its predefined default sense, i.e., the WordNetfirst sense, as the answer.
Furthermore, if a wordtype has neither training data nor predefined de-fault sense, we will output ?U?, which stands forthe missing sense, as the answer.5http://www.bwaldvogel.de/liblinear-java/6http://www.csie.ntu.edu.tw/?cjlin/libsvm/7http://maxent.sourceforge.net/802.2 The Training Data Set for All-WordsTasksOnce we have a supervised WSD system, for theusers who only need WSD as a component intheir applications, it is also important to providethem the classification models.
The performanceof a supervised WSD system greatly depends onthe size of the sense-annotated training data used.To overcome the lack of sense-annotated train-ing examples, besides the training instances fromthe widely used sense-annotated corpus SEMCOR(Miller et al, 1994) and DSO corpus (Ng and Lee,1996), we also follow the approach described inChan and Ng (2005) to extract more training ex-amples from parallel texts.The process of extracting training examplesfrom parallel texts is as follows:?
Collect a set of sentence-aligned paralleltexts.
In our case, we use six English-Chineseparallel corpora: Hong Kong Hansards, HongKong News, Hong Kong Laws, Sinorama,Xinhua News, and the English translation ofChinese Treebank.
They are all availablefrom the Linguistic Data Consortium (LDC).?
Perform tokenization on the English textswith the Penn TreeBank tokenizer.?
Perform Chinese word segmentation on theChinese texts with the Chinese word segmen-tation method proposed by Low et al (2005).?
Perform word alignment on the parallel textsusing the GIZA++ software (Och and Ney,2000).?
Assign Chinese translations to each sense ofan English word w.?
Pick the occurrences of w which are alignedto its chosen Chinese translations in the wordalignment output of GIZA++.?
Identify the senses of the selected occur-rences of w by referring to their aligned Chi-nese translations.Finally, the English side of these selected occur-rences together with their assigned senses are usedas training data.We only extract training examples from paral-lel texts for the top 60% most frequently occur-ring polysemous content words in Brown Corpus(BC), which includes 730 nouns, 190 verbs, and326 adjectives.
For each of the top 60% nouns andadjectives, we gather a maximum of 1,000 trainingexamples from parallel texts.
For each of the top60% verbs, we extract not more than 500 examplesfrom parallel texts, as well as up to 500 examplesfrom the DSO corpus.
We also make use of thesense-annotated examples from SEMCOR as partof our training data for all nouns, verbs, adjectives,and 28 most frequently occurring adverbs in BC.POS noun verb adj adv# of types 11,445 4,705 5,129 28Table 1: Statistics of the word types which havetraining data for WordNet 1.7.1 sense inventoryThe frequencies of word types which we havetraining instances for WordNet sense inventoryversion 1.7.1 are listed in Table 1.
We generatedclassification models with the IMS system for over21,000 word types which we have training data.On average, each word type has 38 training in-stances.
The total size of the models is about 200megabytes.3 EvaluationIn our experiments, we evaluate our IMS systemon SensEval and SemEval tasks, the benchmarkdata sets for WSD.
The evaluation on both lexical-sample and all-words tasks measures the accuracyof our IMS system as well as the quality of thetraining data we have collected.3.1 English Lexical-Sample TasksSensEval-2 SensEval-3IMS 65.3% 72.6%Rank 1 System 64.2% 72.9%Rank 2 System 63.8% 72.6%MFS 47.6% 55.2%Table 2: WSD accuracies on SensEval lexical-sample tasksIn SensEval English lexical-sample tasks, boththe training and test data sets are provided.
A com-mon baseline for lexical-sample task is to selectthe most frequent sense (MFS) in the training dataas the answer.We evaluate IMS on the SensEval-2 andSensEval-3 English lexical-sample tasks.
Table 2compares the performance of our system to the top81two systems that participated in the above tasks(Yarowsky et al, 2001; Mihalcea and Moldovan,2001; Mihalcea et al, 2004).
Evaluation resultsshow that IMS achieves significantly better accu-racies than the MFS baseline.
Comparing to thetop participating systems, IMS achieves compara-ble results.3.2 English All-Words TasksIn SensEval and SemEval English all-words tasks,no training data are provided.
Therefore, the MFSbaseline is no longer suitable for all-words tasks.Because the order of senses in WordNet is basedon the frequency of senses in SEMCOR, the Word-Net first sense (WNs1) baseline always assigns thefirst sense in WordNet as the answer.
We will useit as the baseline in all-words tasks.Using the training data collected with themethod described in Section 2.2, we apply our sys-tem on the SensEval-2, SensEval-3, and SemEval-2007 English all-words tasks.
Similarly, we alsocompare the performance of our system to the toptwo systems that participated in the above tasks(Palmer et al, 2001; Snyder and Palmer, 2004;Pradhan et al, 2007).
The evaluation results areshown in Table 3.
IMS easily beats the WNs1baseline.
It ranks first in SensEval-3 English fine-grained all-words task and SemEval-2007 Englishcoarse-grained all-words task, and is also compet-itive in the remaining tasks.
It is worth notingthat because of the small test data set in SemEval-2007 English fine-grained all-words task, the dif-ferences between IMS and the best participatingsystems are not statistically significant.Overall, IMS achieves good WSD accuracies onboth all-words and lexical-sample tasks.
The per-formance of IMS shows that it is a state-of-the-artWSD system.4 ConclusionThis paper presents IMS, an English all-wordsWSD system.
The goal of IMS is to provide aflexible platform for supervised WSD, as well asan all-words WSD component with good perfor-mance for other applications.The framework of IMS allows us to integratedifferent preprocessing tools to generate knowl-edge sources.
Users can implement various fea-ture types and different machine learning methodsor toolkits according to their requirements.
Bydefault, the IMS system implements three kindsof feature types and uses a linear kernel SVM asthe classifier.
Our evaluation on English lexical-sample tasks proves the strength of our system.With this system, we also provide a large num-ber of classification models trained with the sense-annotated training examples from SEMCOR, DSOcorpus, and 6 parallel corpora, for all contentwords.
Evaluation on English all-words tasksshows that IMS with these models achieves state-of-the-art WSD accuracies compared to the topparticipating systems.As a Java-based system, IMS is platformindependent.
The source code of IMS andthe classification models can be found on thehomepage: http://nlp.comp.nus.edu.sg/software and are available for research,non-commercial use.AcknowledgmentsThis research is done for CSIDM Project No.CSIDM-200804 partially funded by a grant fromthe National Research Foundation (NRF) ad-ministered by the Media Development Authority(MDA) of Singapore.ReferencesMarine Carpuat and Dekai Wu.
2007.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 61?72, Prague,Czech Republic.Yee Seng Chan and Hwee Tou Ng.
2005.
Scalingup word sense disambiguation via parallel texts.
InProceedings of the 20th National Conference on Ar-tificial Intelligence (AAAI), pages 1037?1042, Pitts-burgh, Pennsylvania, USA.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007a.
Word sense disambiguation improves sta-tistical machine translation.
In Proceedings of the45th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 33?40, Prague,Czech Republic.Yee Seng Chan, Hwee Tou Ng, and Zhi Zhong.
2007b.NUS-PT: Exploiting parallel texts for word sensedisambiguation in the English all-words tasks.
InProceedings of the Fourth International Workshopon Semantic Evaluations (SemEval-2007), pages253?256, Prague, Czech Republic.Bart Decadt, Veronique Hoste, and Walter Daelemans.2004.
GAMBL, genetic algorithm optimization ofmemory-based WSD.
In Proceedings of the Third82SensEval-2 SensEval-3 SemEval-2007Fine-grained Fine-grained Fine-grained Coarse-grainedIMS 68.2% 67.6% 58.3% 82.6%Rank 1 System 69.0% 65.2% 59.1% 82.5%Rank 2 System 63.6% 64.6% 58.7% 81.6%WNs1 61.9% 62.4% 51.4% 78.9%Table 3: WSD accuracies on SensEval/SemEval all-words tasksInternational Workshop on Evaluating Word SenseDisambiguation Systems (SensEval-3), pages 108?112, Barcelona, Spain.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empir-ical evaluation of knowledge sources and learningalgorithms for word sense disambiguation.
In Pro-ceedings of the 2002 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP),pages 41?48, Philadelphia, Pennsylvania, USA.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.A maximum entropy approach to Chinese word seg-mentation.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, pages161?164, Jeju Island, Korea.Rada Mihalcea and Andras Csomai.
2005.
Sense-Learner: Word sense disambiguation for all words inunrestricted text.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Lin-guistics (ACL) Interactive Poster and DemonstrationSessions, pages 53?56, Ann Arbor, Michigan, USA.Rada Mihalcea and Dan Moldovan.
2001.
Patternlearning and active feature selection for word sensedisambiguation.
In Proceedings of the Second Inter-national Workshop on Evaluating Word Sense Dis-ambiguation Systems (SensEval-2), pages 127?130,Toulouse, France.Rada Mihalcea, Timothy Chklovski, and Adam Kilgar-riff.
2004.
The SensEval-3 English lexical sam-ple task.
In Proceedings of the Third InternationalWorkshop on Evaluating Word Sense Disambigua-tion Systems (SensEval-3), pages 25?28, Barcelona,Spain.George Miller, Martin Chodorow, Shari Landes, Clau-dia Leacock, and Robert Thomas.
1994.
Using asemantic concordance for sense identification.
InProceedings of ARPA Human Language TechnologyWorkshop, pages 240?243, Morristown, New Jersey,USA.George Miller.
1990.
Wordnet: An on-line lexicaldatabase.
International Journal of Lexicography,3(4):235?312.Hwee Tou Ng and Hian Beng Lee.
1996.
Integratingmultiple knowledge sources to disambiguate wordsense: An exemplar-based approach.
In Proceed-ings of the 34th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 40?47,Santa Cruz, California, USA.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of the38th Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 440?447, HongKong.Martha Palmer, Christiane Fellbaum, Scott Cotton,Lauren Delfs, and Hoa Trang Dang.
2001.
En-glish tasks: All-words and verb lexical sample.
InProceedings of the Second International Workshopon Evaluating Word Sense Disambiguation Systems(SensEval-2), pages 21?24, Toulouse, France.Sameer Pradhan, Edward Loper, Dmitriy Dligach, andMartha Palmer.
2007.
SemEval-2007 task-17: En-glish lexical sample, SRL and all words.
In Proceed-ings of the Fourth International Workshop on Se-mantic Evaluations (SemEval-2007), pages 87?92,Prague, Czech Republic.Benjamin Snyder and Martha Palmer.
2004.
The En-glish all-words task.
In Proceedings of the ThirdInternational Workshop on Evaluating Word SenseDisambiguation Systems (SensEval-3), pages 41?43, Barcelona, Spain.Christopher Stokoe, Michael P. Oakes, and John Tait.2003.
Word sense disambiguation in informationretrieval revisited.
In Proceedings of the Twenty-Sixth Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval (SIGIR), pages 159?166, Toronto, Canada.Ian H. Witten and Eibe Frank.
2005.
Data Mining:Practical Machine Learning Tools and Techniques.Morgan Kaufmann, San Francisco, 2nd edition.David Yarowsky, Radu Florian, Siviu Cucerzan, andCharles Schafer.
2001.
The Johns HopkinsSensEval-2 system description.
In Proceedings ofthe Second International Workshop on EvaluatingWord Sense Disambiguation Systems (SensEval-2),pages 163?166, Toulouse, France.83
