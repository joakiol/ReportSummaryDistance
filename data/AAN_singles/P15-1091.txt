Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 939?949,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsA Unified Kernel Approach for Learning Typed Sentence RewritingsMartin GleizeLIMSI-CNRS, Orsay, FranceUniversit?e Paris-Sud, Orsay, Francegleize@limsi.frBrigitte GrauLIMSI-CNRS, Orsay, FranceENSIIE, Evry, Francebg@limsi.frAbstractMany high level natural language process-ing problems can be framed as determin-ing if two given sentences are a rewrit-ing of each other.
In this paper, we pro-pose a class of kernel functions, referredto as type-enriched string rewriting ker-nels, which, used in kernel-based machinelearning algorithms, allow to learn sen-tence rewritings.
Unlike previous work,this method can be fed external lexical se-mantic relations to capture a wider classof rewriting rules.
It also does not assumepreliminary syntactic parsing but is stillable to provide a unified framework to cap-ture syntactic structure and alignments be-tween the two sentences.
We experimenton three different natural sentence rewrit-ing tasks and obtain state-of-the-art resultsfor all of them.1 IntroductionDetecting implications of sense between state-ments stands as one of the most sought-after goalsin computational linguistics.
Several high leveltasks look for either one-way rewriting betweensingle sentences, like recognizing textual entail-ment (RTE) (Dagan et al, 2006), or two-wayrewritings like paraphrase identification (Dolan etal., 2004) and semantic textual similarity (Agirreet al, 2012).
In a similar fashion, selecting sen-tences containing the answer to a question can beseen as finding the best rewritings of the ques-tion among answer candidates.
These problemsare naturally framed as classification tasks, andas such most current solutions make use of super-vised machine learning.
They have to tackle sev-eral challenges: picking an adequate language rep-resentation, aligning semantically equivalent el-ements and extracting relevant features to learnthe final decision.
Bag-of-words and by extensionbag-of-ngrams are traditionally the most direct ap-proach and features rely mostly on lexical match-ing (Wan et al, 2006; Lintean and Rus, 2011;Jimenez et al, 2013).
Moreover, a good solvingmethod has to account for typically scarce labeledtraining data, by enriching its model with lexicalsemantic resources like WordNet (Miller, 1995)to bridge gaps between surface forms (Mihalceaet al, 2006; Islam and Inkpen, 2009; Yih et al,2013).
Models based on syntactic trees remain thetypical choice to account for the structure of thesentences (Heilman and Smith, 2010; Wang andManning, 2010; Socher et al, 2011; Calvo et al,2014).
Usually the best systems manage to com-bine effectively different methods, like Madnani etal.
?s meta-classifier with machine translation met-rics (Madnani et al, 2012).A few methods (Zanzotto et al, 2007; Zanzottoet al, 2010; Bu et al, 2012) use kernel func-tions to learn what makes two sentence pairs sim-ilar.
Building on this work, we present a type-enriched string rewriting kernel giving the oppor-tunity to specify in a fine-grained way how wordsmatch each other.
Unlike previous work, rewrit-ing rules learned using our framework account forsyntactic structure, term alignments and lexico-semantic typed variations in a unified approach.We detail how to efficiently compute our kerneland lastly experiment on three different high-levelNLP tasks, demonstrating the vast applicability ofour method.
Our system based on type-enrichedstring rewriting kernels obtains state-of-the-art re-sults on paraphrase identification and answer sen-tence selection and outperforms comparable meth-ods on RTE.2 Type-Enriched String RewritingKernelKernel functions measure the similarity betweentwo elements.
Used in machine learning methods939like SVM, they allow complex decision functionsto be learned in classification tasks (Vapnik, 2000).The goal of a well-designed kernel function is tohave a high value when computed on two instancesof same label, and a low value for two instances ofdifferent label.2.1 String rewriting kernelString rewriting kernels (Bu et al, 2012) countthe number of common rewritings between twopairs of sentences seen as sequences of words.The rewriting rule (A) in Figure 1 can be viewedas a kind of phrasal paraphrase with linked vari-ables (Madnani and Dorr, 2010).
Rule (A) rewrites(B)?s first sentence into its second but it does nothowever rewrite the sentences in (C), which iswhat we try to fix in this paper.Following the terminology of string kernels, weuse the term string and character instead of sen-tence and word.
We denote (s, t) ?
(???
??)
aninstance of string rewriting, with a source strings and a target string t, both finite sequences ofelements in ?
the finite set of characters.
Sup-pose that we are given training data of such in-stances labeled in {+1,?1}, for paraphrase/non-paraphrase or entailment/non-entailment in appli-cations.
We can use a kernel method to train onthis data and learn to automatically classify unla-beled instances.
A kernel on string rewriting in-stances is a map:K : (???
??)?
(???
??)?
Rsuch that for all (s1, t1), (s2, t2) ?
???
?
?,K((s1, t1), (s2, t2)) = ??
(s1, t1),?
(s2, t2)?
(1)where ?
maps each instance into a high dimen-sion feature space.
Kernels allow us to avoid thepotentially expensive explicit representation of ?through the inner product space they define.
Thepurpose of the string rewriting kernels is to mea-sure the similarity between two pairs of strings interm of the number of rewriting rules of a set Rthat they share.
?
is thus naturally defined by?
(s, t) = (?r(s, t))r?Rwith ?r(s, t) = n thenumber of contiguous substring pairs of (s, t) thatrewriting rule r matches.2.2 Typed rewriting rulesLet the wildcard domain D ?
?
?be the set ofstrings which can be replaced by wildcards.
Wenow present the formal framework of the type-enriched string rewriting kernels.Let ?pbe the set of pattern types and ?vthe set ofvariable types.To a type ?p?
?p, we associate the typing relation?p?
?
??
?.To a type ?v?
?v,we associate the typing relation?v; ?
D ?D.Together with the typing relations, we call the as-sociation of ?pand ?vthe typing scheme of thekernel.
Let ?pbe defined as?p=????
{[a|b] | ?a, b ?
?, a??
b} (2)We finally define typed rewriting rules.
A typedrewriting rule is a triple r = (?s, ?t, ?
), where?s, ?t?
(?p?
{?
})?denote source and targetstring typed patterns and ?
?
ind?(?s)?ind?
(?t)denotes the alignments between the wildcards inthe two string patterns.
Here ind?(?)
denotes theset of indices of wildcards in ?.We say that a rewriting rule (?s, ?t, ?)
matches apair of strings (s, t) if and only if the followingconditions are true:?
string patterns ?s, resp.
?t, can be turned intos, resp.
t, by:?
substituting each element [a|b] of ?pinthe string pattern with an a or b (?
?)?
substituting each wildcard in the stringpattern with an element of the wildcarddomain D?
?
(i, j) ?
?
, s, resp.
t, substitutes the wild-cards at index i, resp.
j, by s??
D, resp.
t?,such that there exists a variable type ?
?
?vwith s??
; t?.A type-enriched string rewriting kernel (TESRK)is simply a string rewriting kernel as defined inEquation 1 but with R a set of typed rewritingrules.
This class of kernels depends on wildcarddomain D and the typed rewriting rules R whichcan be tuned to allow for more flexibility in thematching of pairs of characters in a rewriting rule.Within this framework, the k-gram bijective stringrewriting kernel (kb-SRK) is defined by the wild-card domain D = ?
and the rulesetR = {(?s, ?t, ?)
| ?s, ?t?
(?p?{?
})k, ?
bijective}under ?p= ?v= {id} with aid?
b, resp.
aid; b,if and only if a = b.940heardwasI heard Mary shouting.Mary was shouting.I caught him snoring.He was sleeping.
(A) (B) (C)Figure 1: Rewriting rule (A) matches pair of strings (B) but does not match (C).We now present an example of how kb-SRK isapplied to real pairs of sentences, what its limita-tions are and how we can deal with them by re-working its typing scheme.
Let us consider againFigure 1, (A) is a rewriting rule with ?s= (heard,?, ?
), ?t= (?, was, ?
), ?
= {(2, 1); (3, 3)}.
Eachstring pattern has the same length, and pairs ofwildcards in the two patterns are aligned bijec-tively.
This is a valid rule for kb-SRK.
It matchesthe pair of strings (B): each aligned pair of wild-cards is substituted in source and target sentencesby the same word and string patterns of (A) can in-deed be turned into pairs of substrings of the sen-tences.
However, it cannot match the pair of sen-tences (C) in the original kb-SRK.
We change ?pto {hypernym, id} where ahypernym?
b if and onlyif a and b have a common hypernym in WordNet.And we change ?vto ?v= {same pronoun, en-tailment, id} where asame pronoun; b if and only ifa and b are a pronoun of the same person and samenumber, and aentailment; b if and only if verb a hasa relation of entailment with b in WordNet.By redefining the typing scheme, rule (A) can nowmatch (C).3 Computing TESRK3.1 FormulationThe k-gram bijective string rewriting kernel can becomputed efficiently (Bu et al, 2012).
We showthat we can compute its type-enriched equivalentat the price of a seemingly insurmountable loosen-ing of theoretical complexity boundaries.
Experi-ments however show that its computing time is ofthe same order as the original kernel.A type-enriched kb-SRK is parameterized by k thelength of k-grams, and its typing scheme the sets?pand ?vand their associated relations.
The an-notations of ?pand ?vtoKkand?Kkwill be omit-ted for clarity and because they typically will notchange while we test different values for k.We rewrite the inner product in Equation 1 to bet-ter fit the k-gram framework:Kk((s1, t1), (s2, t2))=??s1?k-grams(s1)?t1?k-grams(t1)?
?s2?k-grams(s2)?t2?k-grams(t2)?Kk((?s1, ?t1), (?s2, ?t2))(3)where?Kkis the number of different rewritingrules which match two pairs of k-grams (the samerule cannot trigger twice in k-gram substrings):?Kk((?s1, ?t1), (?s2, ?t2))=?r?R1r(?s1, ?t1)1r(?s2, ?t2)(4)with 1rthe indicator function of rule r: 1 if rmatches the pair of k-grams, 0 otherwise.Computing Kkas defined in Equation 3 is obvi-ously intractable.
There is O((n?
k + 1)4) termsin the sum, where n is the length of the longeststring, and each term involves enumerating everyrewriting rule in R.3.2 Computing?Kkin type-enriched kb-SRKEnumerating all rewriting rules in Equation 4 isitself intractable: there are more than |?|2kruleswithout wildcards, where |?| is conceivably thesize of a typical lexicon.
In fact, we just haveto constructively generate the rules which substi-tute their string patterns correctly to simultane-ously produce both pairs of k-grams (?s1, ?t1) and(?s2, ?t2).Let the operator ?
be such that ?1?
?2=((?1[1], ?2[1]), ..., (?1[k], ?2[k])).
This operationis generally known as zipping in functional pro-gramming.
We use the function CountPerfect-Matchings computed by Algorithm 1 to recur-sively count the number of rewriting rules match-ing both (?s1, ?t1) and (?s2, ?t2).
The workingsof the algorithm will make clearer why we cancompute?Kkwith the following formula:?Kk((?s1, ?t1), (?s2, ?t2))= CountPerfectMatchings(?s1?
?s2, ?t1?
?t2)(5)941Algorithm 1 takes as input remaining characterpairs in ?s1?
?s2and ?t1?
?t2, and outputs thenumber of ways they can substitute aligned wild-cards in a matching rule.First (lines 2 and 3) we have the base case whereboth remaining sets are empty.
There is exactly 1way the empty set?s wildcards can be aligned witheach other: nothing is aligned.
In lines 4 to 9, thereis no source pairs anymore, so the algorithm con-tinues to deplete target pairs as long as they havea common pattern type, i.e.
as long as they donot have to substitute a wildcard.
If a candidatewildcard is found, as the opposing set is empty,we cannot align it and we return 0.
In the generalcase (lines 11 to 19), consider the first characterpair (a1, a2) in the reminder of ?s1?
?s2in line12.
What follows in the computation depends onits types.
Every character pair in ?t1?
?t2thatcan be paired through variable types with (a1, a2)(lines 15 to 19) is a new potential wildcard align-ment, so we try all the possible alignment and re-cursively continue the computation after removingboth aligned pairs.
And if (a1, a2) does not need tosubstitute a wildcard because it has common pat-tern types (lines 13 and 14), we can choose to notcreate any wildcard pairing with it and ignore it inthe recursive call.This algorithm enumerates all configurations suchthat each character pair has a common pattern typeor is matched 1-for-1 with a character pair withcommon variable types, which is exactly the defi-nition of a rewriting rule in TESRK.This problem is actually equivalent to count-ing the perfect matchings of the bipartite graphof potential wildcards.
It has been shown in-tractable (Valiant, 1979) and Algorithm 1 is anaive recursive algorithm to solve it.
In our im-plementation we represent the graph with its bi-adjacency matrix, and if our typing relations areindependent of k, the function has a O(k) timecomplexity without including its recursive calls.The number of recursive calls can be greater thank!2which is the number of perfect matchings in acomplete bipartite graph of 2k vertices.
In our ex-periments on linguistic data however, we observeda linear number of recursive calls for low valuesof k, and up to a quadratic number for k > 10?which is way past the point where the kernel be-comes ineffective.As an example, Figure 2 shows the zipped k-grams for source and target as a bipartite graphAlgorithm 1: Counting perfect matchings1 CountPerfectMatchings(remS, remT)Data: remS: remaining char.
pairs in sourceremT: remaining char.
pairs in targetgraph: ?s1?
?s2and ?t1?
?t2as a bipartitegraph, not added in the arguments to avoidcluttering the recursive callsruleSet: ?pand ?vResult: Number of rewriting rules matching(?s1, ?t1) and (?s2, ?t2)2 if remS == ?
and remT == ?
then3 return 1;4 else if remS == ?
then5 (b1, b2) = remT.first();6 if ??
?
?p| b1??
b2then7 return CountPerfectMatchings(?,remT - {(b1, b2)});8 else9 return 0;10 else11 result = 0;12 (a1, a2) = remS.first();13 if ??
?
?p| a1??
a2then14 res += CountPerfectMatchings(remS -{(a1, a2)}, remT);15 for (b1, b2) ?
remT16 | ??
?
?v| a1?
; b1and a2?
; b2do17 res += CountPerfectMatchings(18 remS - {(a1, a2)},19 remT - {(b1, b2)}20 );(s[1], s[1]) (s[k], s[k])(t[1], t[1]) (t[k], t[k])(a, a)(b, b') (e1, e2) (f1, f2)(d1, d2)(c1, c2)... ... ...
...............Figure 2: Bipartite graph of character pairs, withedges between potential wildcardswith 2k vertices and potential wildcard edges.
As-suming that vertices (a, a) and (b, b?)
have com-mon pattern types, they can be ignored as in lines7 and 14.
(c1, c2) to (f1, f2) however must substi-tute wildcards in a matching rewriting rule.
If wealign (c1, c2) with (e1, e2) in line 16, the recur-sive call will return 0 because the other two pairscannot be aligned.
A valid rule is generated if c?sare paired with f ?s and d?s with e?s.
This kind ofchoices is the main source of computational cost.942This problem did not arise in the original kb-SRKbecause of the transitivity of its only type (iden-tity).
In type-enriched kb-SRK, wildcard pairingis less constrained.3.3 Computing KkEven with an efficient method for computing?Kk,implementing Kkdirectly by applying Equation 3remains impractical.
The main idea is to effi-ciently compute a reasonably sized set C of el-ements ((?s1, ?t1), (?s2, ?t2)) which has the es-sential property of including all elements such that?Kk((?s1, ?t1), (?s2, ?t2)) 6= 0.By definition of C, we can compute efficientlyKk((s1, t1), (s2, t2))=?
((?s1,?s2),(?t1,?t2))?C?Kk((?s1, ?t1), (?s2, ?t2))(6)There are a number of ways to do it, with atrade-off between computation time and num-ber of elements in the reduced domain C.The main idea of our own algorithm is that?Kk((?s1, ?t1), (?s2, ?t2)) = 0 if the characterpairs (a1, a2) ?
?s1?
?s2with no common pat-tern type are not all matched with pairs (b1, b2) ??t1?
?t2such that a1?
; b1and a2?
; b2for some?
?
?v.
This is conversely true for character pairsin ?t1?
?t2with no common pattern type.
Moresimply, character pairs with no common patterntype are mismatched and have to substitute a wild-card in a rewriting rule matching both (?s1, ?t1)and (?s2, ?t2).
But introducing a wildcard on oneside of the rule means that there is a matchingwildcard on the other side, so we can eliminatek-gram quadruples that do not fill this wildcardinclusion.
This filtering can be done efficientlyand yields a manageable number of quadruples onwhich to compute?Kk.Algorithm 2 computes a set C to be used inEquation 6 for computing the final value of kernelKk.
In our experiments, it efficiently produces areasonable number of inputs.
All maps in the algo-rithm are maps to multisets, and multisets are usedextensively throughout.
Multisets are an extensionof sets where elements can appear multiple times,the number of times being called the multiplicity.Typically implemented as hash tables from setelements to integers, they allow for constant-timeretrieval of the number of a given element.
Union(?)
and intersection (?)
have special definitionson multisets.
If 1A(x) is the multiplicity of x inA, we have 1A?B(x) = max(1A(x),1B(x)) and1A?B(x) = min(1A(x),1B(x)).Algorithm 2: Computing a set including allelements on which?Kk6= 0Data: s1, t1, s2, t2strings, and k an integerResult: Set C which include all inputs suchthat?Kk6= 01 Initialize maps eis?tand maps eit?s, fori ?
{1, 2};2 for i ?
{1, 2} do3 for a ?
si, b ?
ti| a?
; b, ?
?
?vdo4 eis?t[a] += (b, ?
); eit?s[b] += (a, ?
);5 ws?t, aPt=OneWayInclusion(s1, s2, t1, t2, e1s?t, e2s?t);6 wt?s, aPs=OneWayInclusion(t1, t2, s1, s2, e1t?s, e2t?s);7 Initialize multiset res;8 for (?s1, ?s2) ?
aPsdo9 for (?t1, ?t2) ?
aPtdo10 res += ((?s1, ?s2), (?t1, ?t2));11 res = res ?ws?t?
wt?s.map(swap);12 return res;1314 OneWayInclusion(s1, s2, t1, t2, e1, e2)Initialize map d multisets resWildcards,resAllPatterns;15 for (?s1, ?s2) ?
kgrams(s1)?
kgrams(s2) do16 for (b1, b2) | ??
?
?v, (a1, a2) ??s1?
?s2, (bi, ?)
?
ei[ai] ?i ?
{1, 2} do17 d[(b1, b2)] += (?s1, ?s2);18 for (?t1, ?t2) ?
kgrams(t1)?
kgrams(t2) do19 for (b1, b2) ?
?t1?
?t2| b1?6= b2??
?
?pdo20 if compatWkgrms not initialized then21 Initialize multiset compatWkgrms= d[(b1, b2)];22 compatWkgrms = compatWkgrms?
d[(b1, b2)];23 if compatWkgrms not initialized then24 resAllPatterns += (?t1, ?t2);25 for (?s1, ?s2) ?
compatWkgrms do26 resWildcards+=((?s1, ?s2), (?t1, ?t2));27 return (resWildcards, resAllPatterns);Let us now comment on how the algorithm un-folds.
In lines 1 to 4, we index characters in sourcestrings by characters in target strings which have943common variable types, and vice versa.
It allowsin lines 15 to 19 to quickly map a character pair tothe set of opposing k-gram pairs with a matching?in the sense of variable types?
character pair, i.e.potential aligned wildcards.
In lines 20 to 28 wekeep only the k-gram quadruples whose wildcardcandidates (character pairs with no common pat-tern) from one side all find matches on the otherside.
We do not check for the other inclusion,hence the name of the function OneWayInclusion.At line 26, we did not find any character pair withno common pattern, so we save the k-gram pair as?all-pattern?.
All-pattern k-grams will be pairedin lines 8 to 10 in the result.
Finally, in line 11,we add the union of one-way compatible k-gramquadruples; calling swap on all the pairs of oneset is necessary to consistently have sources on theleft side and targets on the right side in the result.4 Experiments4.1 SystemsWe experimented on three tasks: paraphrase iden-tification, recognizing textual entailment and an-swer sentence selection.
The setup we used for allexperiments was the same save for the few param-eters we explored such as: k, and typing scheme.We implemented 2 kernels, kb-SRK, henceforthsimply denoted SRK, and the type-enriched kb-SRK, denoted TESRK.
All sentences were tok-enized and POS-tagged using OpenNLP (Mor-ton et al, 2005).
Then they were stemmed us-ing the Porter stemmer (Porter, 2001) in the caseof SRK.
Various other pre-processing steps wereapplied in the case of TESRK: they are consid-ered as types in the model and are detailed in Ta-ble 1.
We used LIBSVM (Chang and Lin, 2011)to train a binary SVM classifier on the trainingdata with our two kernels.
The default SVM al-gorithm in LIBSVM uses a parameter C, roughlyakin to a regularization parameter.
We 10-foldcross-validated this parameter on the training data,optimizing with a grid search for f-score, or MRRfor question-answering.
All kernels were normal-ized using?K(x, y) =K(x,y)?K(x,x)?K(y,y).
We de-note by ?+?
a sum of kernels, with normalizationsapplied both before and after summing.
Follow-ing Bu et al (Bu et al, 2012) experimental setup,we introduced an auxiliary vector kernel denotedPR of features named unigram precision and re-call, defined in (Wan et al, 2006).
In our experi-ments a linear kernel seemed to yield the best re-sults.
Our Scala implementation of kb-SRKs hasan average throughput of about 1500 original kb-SRK computations per second, versus 500 type-enriched kb-SRK computations per second on a 8-core machine.
It typically takes a few hours ona 32-core machine to train, cross-validate and teston a full dataset.Finally, Table 1 presents an overview of our typeswith how they are defined and implemented.
Ev-ery type can be used both as a pattern type oras a variable type, but the two roles are differ-ent.
Pattern types are useful to unify different sur-face forms of rewriting rules that are semanticallyequivalent, i.e.
having semantically similar pat-terns.
Variable types are useful for when the se-mantic relation between 2 entities across the samerewriting is more important than the entities them-selves.
That is why some types in Table 1 are in-herently more fitted to be used for one role ratherthan the other.
For example, it is unlikely thatreplacing a word in a pattern of a rewriting ruleby one of its holonyms will yield a semanticallysimilar rewriting rule, so holonym would not be agood pattern type for most applications.
On thecontrary, it can be very useful in a rewriting ruleto type a wildcard link with the relation holonym,as this provides constrained semantic roles to thelinked wildcards in the rule, thus holonym wouldbe a good variable type.4.2 Paraphrase identificationParaphrase identification asks whether two sen-tences have the same meaning.
The dataset weused to evaluate our systems is the MSR Para-phrase Corpus (Dolan and Brockett, 2005), con-taining 4,076 training pairs of sentences and 1,725testing pairs.
For example, the sentences ?An in-jured woman co-worker also was hospitalized andwas listed in good condition.?
and ?A woman waslisted in good condition at Memorial?s HealthParkcampus, he said.?
are paraphrases in this corpus.On the other hand, ?
?There are a number of lo-cations in our community, which are essentiallyvulnerable,?
Mr Ruddock said.?
and ?
?There area range of risks which are being seriously exam-ined by competent authorities,?
Mr Ruddock said.
?are not paraphrases.We report in Table 2 our best results, the sys-tem TESRK + PR, defined by the sum of PR andtyped-enriched kb-SRKs with k from 1 to 4, withtypes ?p= ?v= {stem, synonym}.
We observe944Type Typing relation on words (a, b) Tool/resourcesid words have same surface form and tag OpenNLP taggeridMinusTag words have same surface form OpenNLP tokenizerlemma words have same lemma WordNetStemmerstem words have same stem Porter stemmersynonym, antonym words are [type] WordNethypernym, hyponym b is a [type] of a WordNetentailment, holonymne a and b are both tagged with the same Named Entity BBN Identifinderlvhsn words are at edit distance of 1 Levenshtein distanceTable 1: TypesParaphrase system Accuracy F-scoreAll paraphrase 66.5 79.9Wan et al (2006) 75.6 83.0Bu et al (2012) 76.3 N/ASocher et al (2011) 76.8 83.6Madnani et al (2012) 77.4 84.1PR 73.5 82.1SRK + PR 76.2 83.6TESRK 76.6 83.7TESRK + PR 77.2 84.0Table 2: Evaluation results on MSR Paraphrasethat our results are state-of-the-art and in particu-lar, they improve on the orignal kb-SRK by a goodmargin.
We tried other combinations of types butit did not yield good results, this is probably due tothe nature of the MSR corpus, which did not con-tain much more advanced variations from Word-Net.
The only statistically significant improve-ment we obtained was between TESRK + PR andour PR baseline (p < 0.05).
The performancesobtained by all the cited systems and ours are notsignificantly different in any statistical sense.
Wemade a special effort to try to reproduce as best aswe could the original kb-SRK performances (Bu etal., 2012), although our implementation and theirsshould theoretically be equivalent.Figure 3 plots the average number of recursivecalls to CountPerfectMatchings (algorithm 1) dur-ing a kernel computation, as a function of k. Com-posing with logk, we can observe whether the em-piric number of recursive calls is closer toO(k) orO(k2).
We conclude that this element of complex-ity is linear for low values of k, but tends to ex-plode past k = 7.
Thankfully, counting commonrewriting rules on pairs of 7-to-10-grams rarelyyields non-zero results, so in practice using high0 2 4 6 8 1011.21.41.61.822.2klogk(#recursivecalls)Figure 3: Evolution of the number of recursivecalls to CountPerfectMatchings with k2 4 6 8 1000.511.522.5k|C|?sentencelengthsFigure 4: Evolution of the size of C with kvalues of k is not interesting.Figure 4 plots the average size of set C computedby algorithm 2, as a function of k (divided bythe sum of lengths of the 4 sentences involved inthe kernel computation).
We can observe that this945RTE system AccuracyAll entailments 51.2Heilman and Smith (2010) 62.8Bu et al (2012) 65.1Zanzotto et al (2007) 65.8Hickl et al (2006) 80.0PR 61.8TESRK (All) 62.1SRK + PR 63.8TESRK (Syn) + PR 64.1TESRK (All) + PR 66.1Table 3: Evaluation results on RTE-3quantity is small, except for a peak at low values ofk, which is not an issue because the computationof?Kkis very fast for those values of k.4.3 Recognizing textual entailmentRecognizing Textual Entailment asks whether themeaning of a sentence hypothesis can be inferredby reading a sentence text.
The dataset we usedto evaluate our systems is RTE-3.
Following sim-ilar work (Heilman and Smith, 2010; Bu et al,2012), we took as training data (text, hypothe-sis) pairs from RTE-1 and RTE-2?s whole datasetsand from RTE-3?s training data, which amounts to3,767 sentence pairs.
We tested on RTE-3 test-ing data containing 800 sentence pairs.
For ex-ample, a valid textual entailment in this dataset isthe pair of sentences ?In a move widely viewedas surprising, the Bank of England raised UK in-terest rates from 5% to 5.25%, the highest in fiveyears.?
and ?UK interest rates went up from 5% to5.25%.?
: the first entails the second.
On the otherhand, the pair ?Former French president GeneralCharles de Gaulle died in November.
More than6,000 people attended a requiem mass for him atNotre Dame cathedral in Paris.?
and ?Charles deGaulle died in 1970.?
does not constitute a textualentailment.We report in Table 3 our best results, the sys-tem TESRK (All) + PR, defined by the sum ofPR, 1b-SRK and typed-enriched kb-SRKs with kfrom 2 to 4, with types ?p= {stem, synonym}and ?v= {stem, synonym, hypernym, hyponym,entailment, holonym}.
Our results are to be com-pared with systems using techniques and resourcesof similar nature, but as reference the top perfor-mance at RTE-3 is still reported.
This time we didnot manage to fully reproduce Bu et al 2012?sperformance, but we observe that type-enrichedkb-SRK greatly improves upon our original imple-mentation of kb-SRK and outperforms their sys-tem anyway.
Combining TESRK and the PR base-line yields significantly better results than eitherone alone (p < 0.05), and performs significantlybetter than the system of (Heilman and Smith,2010), the only one which was evaluated on thesame three tasks as us (p < 0.10).
We triedwith less types in our system TESRK (Syn) + PRby removing all WordNet types but synonyms butgot lower performance.
This seems to indicatethat rich types indeed help capturing more com-plex sentence rewritings.
Note that we needed fork = 1 to replace the type-enriched kb-SRK by theoriginal kernel in the sum, otherwise the perfor-mance dropped significantly.
Our conclusion isthat including richer types is only beneficial if theyare captured within a context of a couple of wordsand that including all those variations on unigramsonly add noise.4.4 Answer sentence selectionAnswer sentence selection is the problem of se-lecting among single candidate sentences the onescontaining the correct answer to an open-domainfactoid question.
The dataset we used to evalu-ate our system on this task was created by (Wanget al, 2007) based on the QA track of past TextREtrieval Conferences (TREC-QA)1.
The train-ing set contains 4718 question/answer pairs, for94 questions, originating from TREC 8 to 12.The testing set contains 1517 pairs for 89 ques-tions.
As an example, a correct answer to thequestion ?What do practitioners of Wicca wor-ship??
is ?An estimated 50,000 Americans prac-tice Wicca, a form of polytheistic nature worship.
?On the other hand, the answer candidate ?Whenpeople think of Wicca, they think of either Sa-tanism or silly mumbo jumbo.?
is incorrect.
Sen-tences with more than 40 words and questions withonly positive or only negative answers were fil-tered out (Yao et al, 2013).
The average frac-tion of correct answers per question is 7.4% fortraining and 18.7% for testing.
Performances areevaluated as for a re-ranking problem, in term ofMean Average Precision (MAP) and Mean Re-ciprocal Rank (MRR).
We report our results inTable 4.
We evaluated several combinations offeatures.
IDF word-count (IDF) is a baseline of1Available at http://nlp.stanford.edu/mengqiu/data/qg-emnlp07-data.tgz946System MAP MRRRandom baseline 0.397 0.493Wang et al (2007) 0.603 0.685Heilman and Smith (2010) 0.609 0.692Wang and Manning (2010) 0.595 0.695Yao et al (2013) 0.631 0.748Yih et al (2013) LCLR 0.709 0.770IDF word-count (IDF) 0.596 0.650SRK 0.609 0.669SRK + IDF 0.620 0.677TESRK (WN) 0.642 0.725TESRK (WN+NE) 0.656 0.744TESRK (WN) + IDF 0.678 0.759TESRK (WN+NE) + IDF 0.672 0.768Table 4: Evaluation results on QAIDF-weighted common word counting, integratedin a linear kernel.
Then we implemented SRKand TESRK (with k from 1 to 5) with two typingschemes: WN stands for ?p= {stem, synonym}and ?v= {stem, synonym, hypernym, hyponym,entailment, holonym}, and WN+NE adds type neto both sets of types.
We finally summed our ker-nels with the IDF baseline kernel.
We observe thattypes which make use of WordNet variations seemto increase the most our performance.
Our as-sumption was that named entities would be usefulfor question answering and that we could learn as-sociations between question type and answer typethrough variations: NE does seem to help a littlewhen combined with WN alone, but is less use-ful once TESRK is combined with our baseline ofIDF-weighted common words.
Overall, typing ca-pabilities allow TESRK to obtain way better per-formances than SRK in both MAP and MRR, andour best system combining all our features is com-parable to state-of-the-art systems in MRR, andsignificantly outperforms SRK + IDF, the systemwithout types (p < 0.05).5 Related workLodhi et al (Lodhi et al, 2002) were among thefirst in NLP to use kernels: they apply string ker-nels which count common subsequences to textclassification.
Sentence pair classification how-ever require the capture of 2 types of links: thelink between sentences within a pair, and the linkbetween pairs.
Zanzotto et al (Zanzotto et al,2007) used a kernel method on syntactic tree pairs.They expanded on graph kernels in (Zanzotto etal., 2010).
Their method first aligns tree nodesof a pair of sentences to form a single tree withplaceholders.
They then use tree kernel (Mos-chitti, 2006) to compute the number of commonsubtrees of those trees.
Bu et al (Bu et al,2012) introduced a string rewriting kernel whichcan capture at once lexical equivalents and com-mon syntactic dependencies on pair of sentences.All these kernel methods require an exact matchor assume prior partial matches between words,thus limiting the kind of learned rewriting rules.Our contribution addresses this issue with a type-enriched string rewriting kernel which can accountfor lexico-semantic variations of words.
Limita-tions of our rewriting rules include the impossibil-ity to skip a pattern word and to replace wildcardsby multiple words.Some recent contributions (Chang et al, 2010;Wang and Manning, 2010) also provide a uniformway to learn both intermediary representations anda decision function using potentially rich featuresets.
They use heuristics in the joint learning pro-cess to reduce the computational cost, while ourkernel approach with a simple sequential repre-sentation of sentences has the benefit of efficientlycomputing an exact number of common rewritingrules between rewriting pairs.
This in turn allowsto precisely fine-tune the shape of desired rewrit-ing rules through the design of the typing scheme.6 ConclusionWe developed a unified kernel-based frameworkfor solving sentence rewriting tasks.
Types al-low for an increased flexibility in counting com-mon rewriting rules, and can also add a semanticlayer to the rewritings.
We show that we can effi-ciently compute a kernel which takes types into ac-count, called type-enriched k-gram bijective stringrewriting kernel.
A SVM classifier with this kernelyields state-of-the-art results in paraphrase identi-fication and answer sentence selection and outper-forms comparable systems in recognizing textualentailment.ReferencesEneko Agirre, Mona Diab, Daniel Cer, and AitorGonzalez-Agirre.
2012.
Semeval-2012 task 6: Apilot on semantic textual similarity.
In Proceedingsof the First Joint Conference on Lexical and Com-putational Semantics-Volume 1: Proceedings of themain conference and the shared task, and Volume9472: Proceedings of the Sixth International Workshopon Semantic Evaluation, pages 385?393.
Associa-tion for Computational Linguistics.Fan Bu, Hang Li, and Xiaoyan Zhu.
2012.
Stringre-writing kernel.
In Proceedings of the 50th An-nual Meeting of the Association for ComputationalLinguistics: Long Papers-Volume 1, pages 449?458.Association for Computational Linguistics.Hiram Calvo, Andrea Segura-Olivares, and AlejandroGarc??a.
2014.
Dependency vs. constituent basedsyntactic n-grams in text similarity measures forparaphrase recognition.
Computaci?on y Sistemas,18(3):517?554.Chih-Chung Chang and Chih-Jen Lin.
2011.
Lib-svm: a library for support vector machines.
ACMTransactions on Intelligent Systems and Technology(TIST), 2(3):27.Ming-Wei Chang, Dan Goldwasser, Dan Roth, andVivek Srikumar.
2010.
Discriminative learning overconstrained latent representations.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 429?437.
As-sociation for Computational Linguistics.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The pascal recognising textual entailmentchallenge.
In Machine learning challenges.
evalu-ating predictive uncertainty, visual object classifica-tion, and recognising tectual entailment, pages 177?190.
Springer.William B Dolan and Chris Brockett.
2005.
Automati-cally constructing a corpus of sentential paraphrases.In Proc.
of IWP.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.Unsupervised construction of large paraphrase cor-pora: Exploiting massively parallel news sources.In Proceedings of the 20th international conferenceon Computational Linguistics, page 350.
Associa-tion for Computational Linguistics.Michael Heilman and Noah A Smith.
2010.
Tree editmodels for recognizing textual entailments, para-phrases, and answers to questions.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 1011?1019.Association for Computational Linguistics.Aminul Islam and Diana Inkpen.
2009.
Semantic sim-ilarity of short texts.
Recent Advances in NaturalLanguage Processing V, 309:227?236.Sergio Jimenez, Claudia Becerra, Alexander Gelbukh,Av Juan Dios B?atiz, and Av Mendiz?abal.
2013.Softcardinality: hierarchical text overlap for studentresponse analysis.
In Proceedings of the 2nd jointconference on lexical and computational semantics,volume 2, pages 280?284.Mihai C Lintean and Vasile Rus.
2011.
Dissimilar-ity kernels for paraphrase identification.
In FLAIRSConference.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
The Journal ofMachine Learning Research, 2:419?444.Nitin Madnani and Bonnie J Dorr.
2010.
Generat-ing phrasal and sentential paraphrases: A surveyof data-driven methods.
Computational Linguistics,36(3):341?387.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining machine translation metricsfor paraphrase identification.
In Proceedings of the2012 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 182?190.
Asso-ciation for Computational Linguistics.Rada Mihalcea, Courtney Corley, and Carlo Strappa-rava.
2006.
Corpus-based and knowledge-basedmeasures of text semantic similarity.
In AAAI, vol-ume 6, pages 775?780.George A Miller.
1995.
Wordnet: a lexicaldatabase for english.
Communications of the ACM,38(11):39?41.Thomas Morton, Joern Kottmann, Jason Baldridge, andGann Bierner.
2005.
Opennlp: A java-based nlptoolkit.
http://opennlp.sourceforge.net.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In Machine Learning: ECML 2006, pages 318?329.Springer.Martin F Porter.
2001.
Snowball: A language for stem-ming algorithms.Richard Socher, Eric H Huang, Jeffrey Pennin, Christo-pher D Manning, and Andrew Y Ng.
2011.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In Advances in Neural In-formation Processing Systems, pages 801?809.Leslie G Valiant.
1979.
The complexity of enumer-ation and reliability problems.
SIAM Journal onComputing, 8(3):410?421.Vladimir Vapnik.
2000.
The nature of statistical learn-ing theory.
Springer Science & Business Media.Stephen Wan, Mark Dras, Robert Dale, and C?ecileParis.
2006.
Using dependency-based featuresto take the para-farce out of paraphrase.
In Pro-ceedings of the Australasian Language TechnologyWorkshop, volume 2006.Mengqiu Wang and Christopher D Manning.
2010.Probabilistic tree-edit models with structured latentvariables for textual entailment and question answer-ing.
In Proceedings of the 23rd International Con-ference on Computational Linguistics, pages 1164?1172.
Association for Computational Linguistics.948Mengqiu Wang, Noah A Smith, and Teruko Mita-mura.
2007.
What is the jeopardy model?
a quasi-synchronous grammar for qa.
In EMNLP-CoNLL,volume 7, pages 22?32.Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch, and Peter Clark.
2013.
Answer extractionas sequence tagging with tree edit distance.
In HLT-NAACL, pages 858?867.
Citeseer.Wen-tau Yih, Ming-Wei Chang, Christopher Meek, andAndrzej Pastusiak.
2013.
Question answering usingenhanced lexical semantic models.
In Proceedingsof the 26rd International Conference on Compu-tational Linguistics.
Association for ComputationalLinguistics.Fabio Massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2007.
Shallow semantics infast textual entailment rule learners.
In Proceed-ings of the ACL-PASCAL workshop on textual en-tailment and paraphrasing, pages 72?77.
Associa-tion for Computational Linguistics.Fabio Massimo Zanzotto, Lorenzo DellArciprete, andAlessandro Moschitti.
2010.
Efficient graph kernelsfor textual entailment recognition.
Fundamenta In-formaticae.949
