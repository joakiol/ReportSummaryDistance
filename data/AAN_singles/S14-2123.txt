Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 688?693,Dublin, Ireland, August 23-24, 2014.UEdin: Translating L1 Phrases in L2 Contextusing Context-Sensitive SMTEva HaslerILCC, School of InformaticsUniversity of Edinburghe.hasler@ed.ac.ukAbstractWe describe our systems for the SemEval2014 Task 5: L2 writing assistant where asystem has to find appropriate translationsof L1 segments in a given L2 context.
Weparticipated in three out of four possiblelanguage pairs (English-Spanish, French-English and Dutch-English) and achievedthe best performance for all our submit-ted systems according to word-based ac-curacy.
Our models are based on phrase-based machine translation systems andcombine topical context information andlanguage model scoring.1 IntroductionIn the past years, the fields of statistical machinetranslation (SMT) and word sense disambigua-tion (WSD) have developed largely in parallel,with each field organising their own shared tasksaimed at improving translation quality (Bojar etal., 2013) and predicting word senses, e.g.
Agirreet al.
(2010).
Because sense disambiguation isa central problem in machine translation, therehas been work on integrating WSD classifiers intoMT systems (Carpuat and Wu, 2007a; Carpuatand Wu, 2007b; Chan et al., 2007).
However,one problem with the direct integration of WSDtechniques into MT has been the mismatch be-tween word predictions of WSD systems and thephrase segmentations of MT system.
This prob-lem was adressed in Carpuat and Wu (2007b) byextending word sense disambiguation to phrasesense disambiguation.
The relation between wordsense distinctions and translation has also beenexplored in past SemEval tasks on cross-lingualword sense disambiguation, where senses are notThis work is licensed under a Creative Commons Attribution4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/defined in terms of WordNet senses as in previ-ous tasks, but instead as translations to anotherlanguage (Lefever and Hoste, 2010; Lefever andHoste, 2013).This year?s L2 writing assistant task is simi-lar to the cross-lingual word sense disambigua-tion task but differs in the context provided fordisambiguation and the length of the fragments(source phrases instead of words).
While in othertranslation and disambiguation tasks the sourcelanguage context is given, the L2 writing assis-tant task assumes a given target language con-text that constrains the possible translations of L1fragments.
This is interesting from a machinetranslation point-of-view because it allows for adirect comparison with systems that exploit thetarget context using a language model.
As lan-guage models have become more and more power-ful over the years, mostly thanks to increased com-puting power, new machine translation techniquesare also judged by their ability to improve perfor-mance over a baseline system with a strong lan-guage model.
Another difference to previous Se-mEval tasks is the focus on both lexical and gram-matical forms, while previous tasks have mostlyfocused on lexical selection.2 Translation Model for L1 Fragments inL2 ContextOur model for translating L1 fragments in L2 con-text is a phrase-based machine translation systemwith an additional context similarity feature.
Weaim to resolve lexical ambiguities by taking theentire topical L2 context of an L1 fragment intoaccount rather than only relying on the phrasal L1context.
We do not explicitly model the grammat-icality of target word forms but rather use a stan-dard 5-gram language model to score target wordsequences.
We describe the context similarity fea-ture in the following section.6882.1 Context Similarity FeatureThe context similarity feature is derived from thephrase pair topic model (PPT) described in Hasleret al.
(2014).
At training time, this model learnstopic distributions for all phrase pairs in the phrasetable in an unsupervised fashion, using a variant ofLatent Dirichlet Allocation (LDA).
The underly-ing assumption is that all phrase pairs share a set ofglobal topics of predefined size, thus each phrasepair is assigned a distribution over the same set ofglobal topics.
This is in contrast to Word Sense In-duction (WSI) methods which typically learn a setof topics or senses for each word type, for examplein Lau et al.
(2010).The input to the model are distributional profilesof words occurring in the context of each phrasepair, thus, the model learns lower-dimensionalrepresentations of the likely context words of aphrase pair.
While in a normal machine transla-tion setup the source sentence context is given, it isstraightforward to replace source language wordswith target language words as given in the L2 con-text for each test example.
At test time, the topicmodel is applied to the given L2 context to infer atopic distribution of the test context.
The topic dis-tribution of an applicable phrase pair is comparedto the topic distribution of a given test context (de-fined as all L2 words in the same sentence as theL1 fragment, excluding stop words) using cosinesimilarity.To adapt the translation system to the contextof each test sentence, the phrase table is filteredper test sentence and each applicable phrase pairreceives one additional feature that expresses itstopical similarity with the test context.
Whilethe baseline system (the system without similar-ity feature) translates the entire test set with thesame translation model, the context-sensitive sys-tem loads an adapted phrase table for each test sen-tence.
While the phrase pair topic model can alsodeal with document-level context, here we con-sider only sentence-level context as no wider con-text was available.
We evaluate three variations ofthe context similarity feature on top of a standardphrase-based MT system:?
50-topics The cosine similarity according tothe PPT model trained with 50 topics (sub-mitted as run1)?
mixture:geoAvg The geometric average ofthe cosine similarities according to PPT mod-els trained with 20, 50 and 100 topics (sub-mitted as run2)?
mixture:max For each source phrase, the co-sine similarity according to the PPT modelthat yields the lowest entropy (out of themodels with 20, 50 and 100 topics) whenconverting the similarities into probabilities(submitted as run3)2.2 Language Model Scoring of L2 ContextOn top of using the words in the L2 context forcomputing the similarity feature described above,we introduce a simple method for using a languagemodel to score the target sequence that includesthe translated L1 segments and the words to theleft and right of the translated segments.
In orderto use the language model scoring implemented inthe Moses decoder, we present the decoder withan input sentence that contains the L1 fragment aswell as the L2 context with XML markup.
Whilethe L1 fragments are translated without specialtreatment, the L2 tokens are passed through un-translated by specifying the identity translation asmarkup.
The XML markup also includes reorder-ing walls to prevent the decoder from reorderingthe L2 context.
An example input sentence withmarkup (French-English) is shown below:<wall/>les manifesteurs<wall/><np translation=?want?>want</np><wall/><np translation=?to?>to</np><wall/><np translation=?replace?>replace</np><wall/><np translation=?the?>the</np><wall/><np translation=?government?>government</np><wall/><np translation=?.
?>.</np><wall/>3 Experimental SetupAlthough the task is defined as building a transla-tion assistance system rather than a full machinetranslation system, we use a standard machinetranslation setup to translate L1 phrases.
We usedthe Moses toolkit (Koehn et al., 2007) to buildphrase-based translation systems for the languagepairs English-Spanish, French-English and Dutch-English1.
For preprocessing, we applied punctua-tion normalisation, truecasing and tokenisation us-ing the scripts provided with the Moses toolkit.The model contains the following standard fea-tures: direct and inverse phrase translation prob-abilities, lexical weights, word and phrase penalty,lexicalised reordering and distortion features anda 5-gram language model with modified Kneser-Ney smoothing.
In addition, we add the contextsimilarity feature described in Section 2.1.1We left out the English-German language pair for timereasons.689Training data English-Spanish French-English Dutch-EnglishEuroparl 1.92M 1.96M 1.95MNews Commentary 192K 181K n/aTED 157K 159K 145KNews 2.1G 2.1G 2.1GCommoncrawl 50M 82M -Table 1: Overview of parallel and monolingual training data (top/bottom, in number of sentences/words).3.1 Training DataMost of the training data was taken from theWMT13 shared task (Bojar et al., 2013), ex-cept where specified otherwise.
For the English-Spanish and French-English systems, we used par-allel training data from the Europarl and NewsCommentary corpora, as well as the TED corpus(Cettolo et al., 2012).
For Dutch-English, we usedparallel data from the Europarl and TED corpus.The language models were trained on the targetsides of the parallel data and additional news datafrom the years 2007-2012.
For English-Spanishand French-English, we used additional languagemodel data from the Commoncrawl corpus2.
Sep-arate language models were trained for each cor-pus and interpolated on a development set.
Anoverview of the training data is shown in Table 1.3.2 Tuning Model ParametersThe parameters of the baseline MT excludingthe similarity feature were tuned with kbest-mira(Cherry and Foster, 2012) on mixed developmentsets containing the trial data (500 sentence pairswith XML markup) distributed for the task as wellas development data from the news and TED cor-pora for the English-Spanish and French-Englishsystems and development data from the TED cor-pus for the Dutch-English system.
Because the do-main(s) of the test examples was not known be-forehand, we aimed for learning model weightsthat would generalise across domains by usingrather diverse tuning sets.
In total, the develop-ment sets consisted of 3435, 3705 and 3516 sen-tence pairs, respectively.
We did not tune theweight of the similarity feature automatically, butset it to an empirically determined value instead.3.3 Simulating Ambiguous DevelopmentDataWhen developing our systems using the trial datasupplied by the task organisers, we noticed that2For the Dutch-English system, the Commoncrawl datadid not seem to improve performance.Source words Translationscha?
?ne chain, string, channel, stationmati`ere matter, material, subjectflux stream, flow, feedd?emon demon, daemon, devilr?egime regime, diet, ruleTable 2: Examples of ambiguous source wordsand their different translations in the simulated de-velopment set.System French-EnglishBaseline 0.314+ LM context 0.72620-topics 0.603+ LM context 0.84550-topics 0.674+ LM context 0.886100-topics 0.628+ LM context 0.872mixture:arithmAvg 0.650+ LM context 0.869mixture:geoAvg 0.670+ LM context 0.883mixture:max 0.690+ LM context 0.889Table 3: Word accuracy (best) on the simulated de-velopment set for the smaller baseline system andthe systems with added context similarity feature,with and without language model context.the context similarity feature did not add much tothe overall performance, which we attributed tothe small number of ambiguous examples in thetrial data.
Therefore, we extracted a set of 1076development instances containing 14 ambiguousFrench words and their English translations froma mixed corpus containing data from the NewsCommentary, TED and Commoncrawl corpora asused in Hasler et al.
(2014).
Examples of ambigu-ous source words and their translations in that de-690velopment set are shown in Table 2.Translating the L1 fragments in the simulateddevelopment set using a smaller baseline systemtrained on this mixed data set yields the results atthe top of Table 3.
Note that even though the in-stances were extracted from the training set, thisdoes not affect the translation model since theL1 fragments contain only the ambiguous sourcewords and no further source context that could bememorised.The bottom part of Table 3 shows the perfor-mance of the three context similarity features de-scribed in Section 2.1 plus some further variants(the models with 20 and 100 topics as well asthe arithmetic average of the cosine similaritiesof models trained with 20, 50 and 100 topics).First, we observe that each of the features clearlyoutperforms the baseline system without languagemodel context.
Second, each context similarityfeature together with the language model contextstill outperforms the Baseline + LM context.
Eventhough the gain of the context similarity featuresis smaller when the target context is scored witha language model, the topical context still pro-vides additional information that improves lexicalchoice.
We trained versions of the three best mod-els from Table 3 (in bold) for our submissions onthe official test sets.4 Results and DiscussionIn this section we report the experimental resultsof our systems on the official test sets.
The re-sults without scoring the L2 context with a lan-guage model are shown in Table 4 and includinglanguage model scoring of L2 context in Table 5.We limit the reported scores to word accuracy anddo not report recall because our systems produceoutput for every L1 phrase.In Table 4, we compare the performance of thebaseline MT system to systems including one ofthree variants of the similarity feature as describedin Section 2.1, according to the 1-best transla-tion (best) as well as the 5-best translations (out-of-five) in a distinct n-best list.
For five out ofthe six tasks, at least one of the systems includ-ing the similiary feature yields better performancethan the baseline system.
Only for French-Englishbest, the baseline system yields the best word ac-curacy.
Among the three variants, 50-topics andmixture:geoAvg perform slightly better than mix-ture:max in most cases.Table 5 shows the results of our submitted runsInput: There are many ways of cooking<f>des ?ufs</f> for breakfast.Reference: There are many ways of cooking<f>eggs</f> for breakfast.Input: I loved animals when I was <f>unenfant</f>.Reference: I loved animals when I was <f>akid<alt>a child</alt></f>.Figure 1: Examples of official test instances.
(run1-run3) as well as the baseline system, allwith language model scoring of L2 context viaXML markup.
The first thing to note in com-parison to Table 4 is that providing the L2 con-text for language model scoring yields quite sub-stantial improvements (0.165, 0.101 and 0.073, re-spectively).
Again, in five out of six cases at leastone of the systems with context similarity featureperforms better than the baseline system.
Only forSpanish-English best, the baseline system yieldshigher word accuracy than the three submittedruns.
As before, 50-topics and mixture:geoAvgperform slightly better than mixture:max, with apreference for 50-topics.
For comparison, we alsoshow the word accuracies of the 2nd-ranked sys-tem for both tasks and each language pair.
Wenote that the distance to the respective runner-upsystem is largest for French-English and on aver-age larger for the out-of-five task than for the besttask.As a general observation, we can state thatalthough the similarity feature improves perfor-mance in most cases, the improvements are smallcompared to the improvements achieved by scor-ing the L2 language model contexts.
We suspecttwo reasons for this effect: first, we do not explic-itly model grammaticality of word forms.
There-fore, our system relies on the language model tochoose the best word form for those test examplesthat do not contain any lexical ambiguity.
Second,we have noticed that for some of the test exam-ples, the correct translations do not depend partic-ularly on words in the L2 context, as shown in Fig-ure 1 where the most common translations of thesource phrases without context would match thereference translations.
These are cases where wedo not expect much of an improvement in transla-tion by taking the L2 context into account.Since in Section 3.3 we have provided evidencethat topical similarity features can improve lexicalchoice over simply using a target language model,we believe that the lower performance of the sim-ilarity features on the official test set is caused by691System English-Spanish French-English Dutch-Englishbest oof best oof best oofBaseline 0.674 0.854 0.722 0.884 0.613 0.75050-topics 0.682 0.860 0.719 0.896 0.616 0.759mixture:geoAvg 0.677 0.863 0.715 0.896 0.619 0.756mixture:max 0.679 0.860 0.712 0.887 0.618 0.753Table 4: Word accuracy (best and out-of-five) of the baseline system and the systems with added contextsimilarity feature.
All systems were run without scoring the language model context.System English-Spanish French-English Dutch-Englishbest oof best oof best oofBaseline + LM context 0.839 0.944 0.823 0.934 0.686 0.809run1:50-topics + LM context 0.827 0.946 0.824 0.938 0.692 0.811run2:mixture:geoAvg + LM context 0.827 0.944 0.821 0.939 0.688 0.808run3:mixture:max + LM context 0.820 0.949 0.816 0.937 0.688 0.8082nd-ranked systems 0.80910.88720.69420.83920.67930.7533Table 5: Word accuracy (best and out-of-five) of all submitted systems (runs 1-3) as well as the baselinesystem without the context similarity feature.
All systems were run with the language model contextprovided via XML input.
Systems on 2nd rank:1UNAL-run2,2CNRC-run1,3IUCL-run1different levels of ambiguity in the simulated de-velopment set and the official test set.
For thesimulated development set, we explicitly selectedambiguous source words in contexts which trig-ger multiple different translations, while the offi-cial test set also contains examples where the fo-cus is on correct verb forms.
It further contains ex-amples where the baseline system without contextinformation could easily provide the correct trans-lation, as shown above.
Thus, the performance ofour topical context models should ideally be eval-uated on test sets that contain a sufficient numberof ambiguous source phrases in order to measureits ability to improve lexical selection.Finally, in Figure 2 we show some exampleswhere the 50-topics system (with LM context)produced semantically better translations than thebaseline system and where words in the L2 con-text would have helped in promoting them over thechoice of the baseline system.5 ConclusionWe have described our systems for the SemEval2014 Task 5: L2 writing assistant which achievedthe best performance for all submitted languagepairs and both the best and out-of-five tasks.
AllInput: Why has Air France authorised <f>les ap-pareils ?electroniques</f> at take-off?Baseline: .. <f>the electronics</f> ..50-topics: .. <f>electronic devices</f> ..Reference: .. <f>electronic devices</f> ..Input: This project represents one of the rare ad-vances in strenghtening <f>les liens</f>between Brazil and the European Union.Baseline: .. <f>the links</f> ..50-topics: .. <f>the ties</f> ..Reference: .. <f>the ties<alt>relations</alt><alt>the bonds</alt></f> ..Figure 2: Examples of improved translation outputwith the context similarity feature.systems are based on phrase-based machine trans-lation systems with an added context similarityfeature derived from a topic model that learnstopic distributions for phrase pairs.
We show thatthe additional similarity feature improves perfor-mance over our baseline models and that furthergains can be achieved by passing the L2 contextthrough the decoder via XML markup, therebyproducing language model scores of the sequencesof L2 context words and translated L1 fragments.We also provide evidence that the relative perfor-mance of the context similarity features dependson the level of ambiguity in the L1 fragments.692AcknowledgementsThis work was supported by funding from theScottish Informatics and Computer Science Al-liance and funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement 287658 (EU BRIDGE).ReferencesEneko Agirre, Oier Lopez De Lacalle, Christiane Fell-baum, Maurizio Tesconi, Monica Monachini, PiekVossen, and Roxanne Segers.
2010.
SemEval-2010Task 17: All-words Word Sense Disambiguation ona Specific Domain.
In Proceedings of the 5th Inter-national Workshop on Semantic Evaluation.Ondrej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 workshopon statistical machine translation.
In Proceedings ofWMT 2013.Marine Carpuat and Dekai Wu.
2007a.
How PhraseSense Disambiguation outperforms Word Sense Dis-ambiguation for Statistical Machine Translation.In International Conference on Theoretical andMethodological Issues in MT.Marine Carpuat and Dekai Wu.
2007b.
Improving Sta-tistical Machine Translation using Word Sense Dis-ambiguation.
In Proceedings of EMNLP, pages 61?72.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
WIT3: Web Inventory of Transcribedand Translated Talks.
In Proceedings of EAMT.Yee Seng Chan, Hwee Tou Ng, and David Chiang.2007.
Word Sense Disambiguation Improves Statis-tical Machine Translation.
In Proceedings of ACL.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProceedings of NAACL.Eva Hasler, Barry Haddow, and Philipp Koehn.
2014.Dynamic Topic Adaptation for SMT using Distribu-tional Profiles.
In Proceedings of the 9th Workshopon Statistical Machine Translation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for SMT.
In Proceedings of ACL:Demo and poster sessions.Jey Han Lau, Paul Cook, Diana Mccarthy, David New-man, and Timothy Baldwin.
2010.
Word Sense In-duction for Novel Sense Detection.
In Proceedingsof EACL.Els Lefever and Veronique Hoste.
2010.
SemEval-2010 Task 3: Cross-lingual Word Sense Disam-biguation.
In Proceedings of the 5th InternationalWorkshop on Semantic Evaluation.Els Lefever and Veronique Hoste.
2013.
SemEval-2013 Task 10: Cross-lingual Word Sense Disam-biguation.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation, in Conjunctionwith the Second Joint Conference on Lexical andComputational Semantics.693
