Proceedings of the ACL 2010 System Demonstrations, pages 48?53,Uppsala, Sweden, 13 July 2010. c?2010 Association for Computational LinguisticsPersonalising speech-to-speech translation in the EMIME projectMikko Kurimo1?, William Byrne6, John Dines3, Philip N. Garner3, Matthew Gibson6,Yong Guan5, Teemu Hirsima?ki1, Reima Karhila1, Simon King2, Hui Liang3, KeiichiroOura4, Lakshmi Saheer3, Matt Shannon6, Sayaka Shiota4, Jilei Tian5, Keiichi Tokuda4,Mirjam Wester2, Yi-Jian Wu4, Junichi Yamagishi21 Aalto University, Finland, 2 University of Edinburgh, UK, 3 Idiap Research Institute,Switzerland, 4 Nagoya Institute of Technology, Japan, 5 Nokia Research Center Beijing, China,6 University of Cambridge, UK?Corresponding author: Mikko.Kurimo@tkk.fiAbstractIn the EMIME project we have studied un-supervised cross-lingual speaker adapta-tion.
We have employed an HMM statisti-cal framework for both speech recognitionand synthesis which provides transfor-mation mechanisms to adapt the synthe-sized voice in TTS (text-to-speech) usingthe recognized voice in ASR (automaticspeech recognition).
An important ap-plication for this research is personalisedspeech-to-speech translation that will usethe voice of the speaker in the input lan-guage to utter the translated sentences inthe output language.
In mobile environ-ments this enhances the users?
interactionacross language barriers by making theoutput speech sound more like the origi-nal speaker?s way of speaking, even if sheor he could not speak the output language.1 IntroductionA mobile real-time speech-to-speech translation(S2ST) device is one of the grand challenges innatural language processing (NLP).
It involvesseveral important NLP research areas: auto-matic speech recognition (ASR), statistical ma-chine translation (SMT) and speech synthesis, alsoknown as text-to-speech (TTS).
In recent yearssignificant advance have also been made in rele-vant technological devices: the size of powerfulcomputers has decreased to fit in a mobile phoneand fast WiFi and 3G networks have spread widelyto connect them to even more powerful computa-tion servers.
Several hand-held S2ST applicationsand devices have already become available, for ex-ample by IBM, Google or Jibbigo1, but there arestill serious limitations in vocabulary and languageselection and performance.When an S2ST device is used in practical hu-man interaction across a language barrier, one fea-ture that is often missed is the personalization ofthe output voice.
Whoever speaks to the device inwhat ever manner, the output voice always soundsthe same.
Producing high-quality synthesis voicesis expensive and even if the system had many out-put voices, it is hard to select one that would soundlike the input voice.
There are many features in theoutput voice that could raise the interaction expe-rience to a much more natural level, for example,emotions, speaking rate, loudness and the speakeridentity.After the recent development in hidden Markovmodel (HMM) based TTS, it has become possi-ble to adapt the output voice using model trans-formations that can be estimated from a smallnumber of speech samples.
These techniques, forinstance the maximum likelihood linear regres-sion (MLLR), are adopted from HMM-based ASRwhere they are very powerful in fast adaptation ofspeaker and recording environment characteristics(Gales, 1998).
Using hierarchical regression trees,the TTS and ASR models can further be coupledin a way that enables unsupervised TTS adaptation(King et al, 2008).
In unsupervised adaptationsamples are annotated by applying ASR.
By elimi-nating the need for human intervention it becomespossible to perform voice adaptation for TTS inalmost real-time.The target in the EMIME project2 is to studyunsupervised cross-lingual speaker adaptation forS2ST systems.
The first results of the project have1http://www.jibbigo.com2http://emime.org48been, for example, to bridge the gap between theASR and TTS (Dines et al, 2009), to improvethe baseline ASR (Hirsima?ki et al, 2009) andSMT (de Gispert et al, 2009) systems for mor-phologically rich languages, and to develop robustTTS (Yamagishi et al, 2010).
The next step hasbeen preliminary experiments in intra-lingual andcross-lingual speaker adaptation (Wu et al, 2008).For cross-lingual adaptation several new methodshave been proposed for mapping the HMM states,adaptation data and model transformations (Wu etal., 2009).In this presentation we can demonstrate the var-ious new results in ASR, SMT and TTS.
Eventhough the project is still ongoing, we have aninitial version of mobile S2ST system and cross-lingual speaker adaptation to show.2 Baseline ASR, TTS and SMT systemsThe baseline ASR systems in the project are devel-oped using the HTK toolkit (Young et al, 2001)for Finnish, English, Mandarin and Japanese.
Thesystems can also utilize various real-time decoderssuch as Julius (Kawahara et al, 2000), Juicer atIDIAP and the TKK decoder (Hirsima?ki et al,2006).
The main structure of the baseline sys-tems for each of the four languages is similar andfairly standard and in line with most other state-of-the-art large vocabulary ASR systems.
Some spe-cial flavors for have been added, such as the mor-phological analysis for Finnish (Hirsima?ki et al,2009).
For speaker adaptation, the MLLR trans-formation based on hierarchical regression classesis included for all languages.The baseline TTS systems in the project utilizethe HTS toolkit (Yamagishi et al, 2009) whichis built on top of the HTK framework.
TheHMM-based TTS systems have been developedfor Finnish, English, Mandarin and Japanese.
Thesystems include an average voice model for eachlanguage trained over hundreds of speakers takenfrom standard ASR corpora, such as Speecon(Iskra et al, 2002).
Using speaker adaptationtransforms, thousands of new voices have beencreated (Yamagishi et al, 2010) and new voicescan be added using a small number of either su-pervised or unsupervised speech samples.
Cross-lingual adaptation is possible by creating a map-ping between the HMM states in the input and theoutput language (Wu et al, 2009).Because the resources of the EMIME projecthave been focused on ASR, TTS and speakeradaptation, we aim at relying on existing solu-tions for SMT as far as possible.
New methodshave been studied concerning the morphologicallyrich languages (de Gispert et al, 2009), but for theS2ST system we are currently using Google trans-late3.3 Demonstrations to show3.1 Monolingual systemsIn robust speech synthesis, a computer can learnto speak in the desired way after processing only arelatively small amount of training speech.
Thetraining speech can even be a normal qualityrecording outside the studio environment, wherethe target speaker is speaking to a standard micro-phone and the speech is not annotated.
This differsdramatically from conventional TTS, where build-ing a new voice requires an hour or more carefulrepetition of specially selected prompts recordedin an anechoic chamber with high quality equip-ment.Robust TTS has recently become possible us-ing the statistical HMM framework for both ASRand TTS.
This framework enables the use of ef-ficient speaker adaptation transformations devel-oped for ASR to be used also for the TTS mod-els.
Using large corpora collected for ASR, we cantrain average voice models for both ASR and TTS.The training data may include a small amount ofspeech with poor coverage of phonetic contextsfrom each single speaker, but by summing the ma-terial over hundreds of speakers, we can obtainsufficient models for an average speaker.
Only asmall amount of adaptation data is then required tocreate transformations for tuning the average voicecloser to the target voice.In addition to the supervised adaptation us-ing annotated speech, it is also possible to em-ploy ASR to create annotations.
This unsu-pervised adaptation enables the system to use amuch broader selection of sources, for example,recorded samples from the internet, to learn a newvoice.The following systems will demonstrate the re-sults of monolingual adaptation:1.
In EMIME Voice cloning in Finnish and En-glish the goal is that the users can clone theirown voice.
The user will dictate for about3http://translate.google.com49Figure 1: Geographical representation of HTS voices trained on ASR corpora for EMIME projects.Blue markers show male speakers and red markers show female speakers.
Available online viahttp://www.emime.org/learn/speech-synthesis/listen/Examples-for-D2.110 minutes and then after half an hour ofprocessing time, the TTS system has trans-formed the average model towards the user?svoice and can speak with this voice.
Thecloned voices may become especially valu-able, for example, if a person?s voice is laterdamaged in an accident or by a disease.2.
In EMIME Thousand voices map the goal isto browse the world?s largest collection ofsynthetic voices by using a world map in-terface (Yamagishi et al, 2010).
The usercan zoom in the world map and select anyvoice, which are organized according to theplace of living of the adapted speaker, to ut-ter the given sentence.
This interactive ge-ographical representation is shown in Figure1.
Each marker corresponds to an individualspeaker.
Blue markers show male speakersand red markers show female speakers.
Somemarkers are in arbitrary locations (in the cor-rect country) because precise location infor-mation is not available for all speakers.
Thisgeographical representation, which includesan interactive TTS demonstration of many ofthe voices, is available from the URL pro-vided.
Clicking on a marker will play syn-thetic speech from that speaker4.
As well as4Currently the interactive mode supports English andSpanish only.
For other languages this only provides pre-being a convenient interface to compare themany voices, the interactive map is an attrac-tive and easy-to-understand demonstration ofthe technology being developed in EMIME.3.
The models developed in the HMM frame-work can be demonstrated also in adapta-tion of an ASR system for large-vocabularycontinuous speech recognition.
By utilizingmorpheme-based language models instead ofword-based models the Finnish ASR systemis able to cover practically an unlimited vo-cabulary (Hirsima?ki et al, 2006).
This isnecessary for morphologically rich languageswhere, due to inflection, derivation and com-position, there exists so many different wordforms that word based language modeling be-comes impractical.3.2 Cross-lingual systemsIn the EMIME project the goal is to learn cross-lingual speaker adaptation.
Here the output lan-guage ASR or TTS system is adapted from speechsamples in the input language.
The results so farare encouraging, especially for TTS: Even thoughthe cross-lingual adaptation may somewhat de-grade the synthesis quality, the adapted speechnow sounds more like the target speaker.
Sev-eral recent evaluations of the cross-lingual speakersynthesised examples, but we plan to add an interactive type-in text-to-speech feature in the near future.50Figure 2: All English HTS voices can be used as online TTS on the geographical map.adaptation methods can be found in (Gibson et al,2010; Oura et al, 2010; Liang et al, 2010; Ouraet al, 2009).The following systems have been created todemonstrate cross-lingual adaptation:1.
In EMIME Cross-lingual Finnish/Englishand Mandarin/English TTS adaptation theinput language sentences dictated by the userwill be used to learn the characteristics of heror his voice.
The adapted cross-lingual modelwill be used to speak output language (En-glish) sentences in the user?s voice.
The userdoes not need to be bilingual and only readssentences in their native language.2.
In EMIME Real-time speech-to-speech mo-bile translation demo two users will interactusing a pair of mobile N97 devices (see Fig-ure 3).
The system will recognize the phrasethe other user is speaking in his native lan-guage and translate and speak it in the nativelanguage of the other user.
After a few sen-tences the system will have the speaker adap-tation transformations ready and can applythem in the synthesized voices to make themsound more like the original speaker insteadof a standard voice.
The first real-time demoversion is available for the Mandarin/Englishlanguage pair.3.
The morpheme-based translation system forFinnish/English and English/Finnish can becompared to a word based translation forarbitrary sentences.
The morpheme-basedapproach is particularly useful for languagepairs where one or both languages are mor-phologically rich ones where the amount andcomplexity of different word forms severelylimits the performance for word-based trans-lation.
The morpheme-based systems canlearn translation models for phrases wheremorphemes are used instead of words (deGispert et al, 2009).
Recent evaluations (Ku-rimo et al, 2009) have shown that the perfor-mance of the unsupervised data-driven mor-pheme segmentation can rival the conven-tional rule-based ones.
This is very useful ifhand-crafted morphological analyzers are notavailable or their coverage is not sufficient forall languages.AcknowledgmentsThe research leading to these results was partlyfunded from the European Communitys Seventh51ASR SMT TTSCross-lingualSpeaker adaptationSpeakeradaptationinput outputspeechFigure 3: EMIME Real-time speech-to-speechmobile translation demoFramework Programme (FP7/2007-2013) undergrant agreement 213845 (the EMIME project).ReferencesA.
de Gispert, S. Virpioja, M. Kurimo, and W. Byrne.2009.
Minimum Bayes risk combination of transla-tion hypotheses from alternative morphological de-compositions.
In Proc.
NAACL-HLT.J.
Dines, J. Yamagishi, and S. King.
2009.
Measur-ing the gap between HMM-based ASR and TTS.
InProc.
Interspeech ?09, Brighton, UK.M.
Gales.
1998.
Maximum likelihood linear transfor-mations for HMM-based speech recognition.
Com-puter Speech and Language, 12(2):75?98.M.
Gibson, T. Hirsima?ki, R. Karhila, M. Kurimo,and W. Byrne.
2010.
Unsupervised cross-lingualspeaker adaptation for HMM-based speech synthe-sis using two-pass decision tree construction.
InProc.
of ICASSP, page to appear, March.T.
Hirsima?ki, M. Creutz, V. Siivola, M. Kurimo, S.Virpioja, and J. Pylkko?nen.
2006.
Unlimited vo-cabulary speech recognition with morph languagemodels applied to finnish.
Computer Speech & Lan-guage, 20(4):515?541, October.T.
Hirsima?ki, J. Pylkko?nen, and M Kurimo.
2009.Importance of high-order n-gram models in morph-based speech recognition.
IEEE Trans.
Audio,Speech, and Language Process., 17:724?732.D.
Iskra, B. Grosskopf, K. Marasek, H. van denHeuvel, F. Diehl, and A. Kiessling.
2002.SPEECON speech databases for consumer devices:Database specification and validation.
In Proc.LREC, pages 329?333.T.
Kawahara, A. Lee, T. Kobayashi, K. Takeda,N.
Minematsu, S. Sagayama, K. Itou, A. Ito, M. Ya-mamoto, A. Yamada, T. Utsuro, and K. Shikano.2000.
Free software toolkit for japanese large vo-cabulary continuous speech recognition.
In Proc.ICSLP-2000, volume 4, pages 476?479.S.
King, K. Tokuda, H. Zen, and J. Yamagishi.
2008.Unsupervised adaptation for HMM-based speechsynthesis.
In Proc.
Interspeech 2008, pages 1869?1872, September.Mikko Kurimo, Sami Virpioja, Ville T. Turunen,Graeme W. Blackwood, and William Byrne.
2009.Overview and results of Morpho Challenge 2009.
InWorking Notes for the CLEF 2009 Workshop, Corfu,Greece, September.H.
Liang, J. Dines, and L. Saheer.
2010.
Acomparison of supervised and unsupervised cross-lingual speaker adaptation approaches for HMM-based speech synthesis.
In Proc.
of ICASSP, pageto appear, March.Keiichiro Oura, Junichi Yamagishi, Simon King, Mir-jam Wester, and Keiichi Tokuda.
2009.
Unsuper-vised speaker adaptation for speech-to-speech trans-lation system.
In Proc.
SLP (Spoken Language Pro-cessing), number 356 in 109, pages 13?18.K.
Oura, K. Tokuda, J. Yamagishi, S. King, andM.
Wester.
2010.
Unsupervised cross-lingualspeaker adaptation for HMM-based speech synthe-sis.
In Proc.
of ICASSP, page to appear, March.Y.-J.
Wu, S. King, and K. Tokuda.
2008.
Cross-lingualspeaker adaptation for HMM-based speech synthe-sis.
In Proc.
of ISCSLP, pages 1?4, December.Y.-J.
Wu, Y. Nankaku, and K. Tokuda.
2009.
Statemapping based method for cross-lingual speakeradaptation in HMM-based speech synthesis.
InProc.
of Interspeech, pages 528?531, September.J.
Yamagishi, T. Nose, H. Zen, Z.-H. Ling, T. Toda,K.
Tokuda, S. King, and S. Renals.
2009.
Robustspeaker-adaptive HMM-based text-to-speech syn-thesis.
IEEE Trans.
Audio, Speech and LanguageProcess., 17(6):1208?1230.
(in press).J.
Yamagishi, B. Usabaev, S. King, O. Watts, J. Dines,J.
Tian, R. Hu, K. Oura, K. Tokuda, R. Karhila, andM.
Kurimo.
2010.
Thousands of voices for hmm-based speech synthesis.
IEEE Trans.
Speech, Audio& Language Process.
(in press).52S.
Young, G. Everman, D. Kershaw, G. Moore, J.Odell, D. Ollason, V. Valtchev, and P. Woodland,2001.
The HTK Book Version 3.1, December.53
