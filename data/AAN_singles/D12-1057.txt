Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 619?630, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsExcitatory or Inhibitory: A New Semantic Orientation ExtractsContradiction and Causality from the WebChikara Hashimoto?
Kentaro Torisawa?
Stijn De Saeger?Jong-Hoon Oh?
Jun?ichi Kazama?National Institute of Information and Communications TechnologyKyoto, 619-0289, JAPAN{?
ch, ?
torisawa, ?
stijn, ?
rovellia, ?kazama}@nict.go.jpAbstractWe propose a new semantic orientation, Ex-citation, and its automatic acquisition method.Excitation is a semantic property of predicatesthat classifies them into excitatory, inhibitoryand neutral.
We show that Excitation is usefulfor extracting contradiction pairs (e.g., destroycancer ?
develop cancer) and causality pairs(e.g., increase in crime ?
heighten anxiety).Our experiments show that with automaticallyacquired Excitation knowledge we can extractone million contradiction pairs and 500,000causality pairs with about 70% precision froma 600 million page Web corpus.
Furthermore,by combining these extracted causality andcontradiction pairs, we can generate one mil-lion plausible causality hypotheses that are notwritten in any single sentence in our corpuswith reasonable precision.1 IntroductionRecognizing semantic relations between events intexts is crucial for such NLP tasks as question an-swering (QA).
For example, to answer the question?What ruined the crops in Japan??
a QA systemmust recognize that the sentence ?the Fukushimanuclear power plant caused radioactive pollutionand contaminated the crops in Japan?
contains acausal relation and that contaminate crops entailsruin crops but contradicts preserve crops.To facilitate the acquisition of causality, contra-diction, paraphrase and entailment relations betweenevents we propose a new semantic orientation, Ex-citation, that classifies unary predicates (templates,hereafter) into excitatory, inhibitory and neutral.
Anexcitatory template entails that the main function oreffect of the referent of its argument is activated orenhanced (e.g., cause X, preserve X), while an in-hibitory template entails that it is deactivated or sup-pressed (e.g., ruin X, contaminate X, prevent X).Excitation is useful for extracting contradiction;if two templates with similar distributional profileshave opposite Excitation polarities, they tend to becontradictions (e.g., contaminate crops and preservecrops).
With extracted contradictions we can distin-guish paraphrases from contradictions among distri-butionally similar phrases.
Furthermore, contradic-tion in itself is important knowledge for Recogniz-ing Textual Entailment (RTE) (Voorhees, 2008).Excitation is also a powerful indicator of causal-ity.
In the physical world, the activation or de-activation of one thing often causes the activationor deactivation of another.
Two excitatory or in-hibitory templates that co-occur in some temporalor logical order in the same narrative often describea causal chain of events, like ?the Fukushima nu-clear power plant caused radioactive pollution andcontaminated crops in Japan?.In this paper we propose both the concept of Ex-citation and an automatic method for its acquisition.Our method acquires Excitation templates based oncertain natural, language independent constraints onnarrative structures found in text.
We also proposeacquisition methods for contradiction and causal-ity relations based on Excitation.
Our methods ex-tract one million contradiction pairs with over 70%precision, and 500,000 causality pairs with about70% precision from a 600 million page Web corpus.Moreover, by combining these extracted causalitypairs and contradiction pairs, we generated one mil-lion plausible causality hypotheses that were not619written in any single sentence in our corpus with rea-sonable precision.
For example, a causality hypoth-esis prevent radioactive pollution ?
preserve cropscan be generated from an extracted causality causeradioactive pollution ?
contaminate crops.We target the Japanese language in this paper.2 What is Excitation?Excitation classifies templates into excitatory, in-hibitory, and neutral, as explained below.excitatory templates entail that the function, ef-fect, purpose or role of their argument?s refer-ent is activated or enhanced.
(e.g., cause X, buyX, produce X, import X, increase X, enable X)inhibitory templates entail that the function, ef-fect, purpose or role of their argument?s refer-ent is deactivated or suppressed.
(e.g., preventX, discard X, remedy X, decrease X, disable X)neutral templates are neither excitatory nor in-hibitory.
(e.g., consider X, proportional to X,related to X, evaluate X, close to X)For example, when fire fills the X slot of cause X,it suggests that the effect of fire is activated.
If pre-vent X?s slot is filled with flu, the effect of flu is sup-pressed.
In this study, we aim to acquire excitatoryand inhibitory templates that are useful for extract-ing contradiction and causality, though neutral tem-plates are the most frequent in our data (See Section5.1).
Collectively we call excitatory and inhibitorytemplates Excitation templates, and excitatory andinhibitory two opposite polarities.Excitation is independent of the good/bad seman-tic orientation.
(Hatzivassiloglou and McKeown,1997; Turney, 2002; Rao and Ravichandran, 2009).For example, sophisticate X and complicate X areboth excitatory, but only the former has a positiveconnotation.
Similarly, remedy X and degrade X areboth inhibitory but only the latter is negative.General Inquirer (Stone et al1966) deals withsemantic factors some of which were proposed byOsgood et al1957).
Their ?activity?
factor involvesbinary opposition between ?active?
and ?passive.
?Notice that activity and Excitation are independent.In General Inquirer, both accelerate X and abolishX are active, but only the former is excitatory.
Bothaccept X and abate X are passive, but only the lat-ter is inhibitory.
Pustejovsky (1995) proposed telicand agentive roles, which inspired our excitatory no-tion, but they have no corresponding notion of in-hibitory.
Andreevskaia and Bergler (2006) acquiredthe increase/decrease semantic orientation, which isa subclass of Excitation.Excitation is inverted if a template?s predicate isnegated.
For example, preserve X is excitatory,while don?t preserve X is inhibitory.
We acknowl-edge that this may seem somewhat counter-intuitiveand will address this issue in future work.3 Excitation Template AcquisitionThis section presents our acquisition method of Ex-citation templates.
We introduce constraints in theco-occurrence of templates in text that seem both ro-bust and language independent in Section 3.1.
Ourmethod exploits these constraints for the acquisitionof Excitation templates.
First we construct a tem-plate network where nodes are templates and linksrepresent that two connected templates have eitherSAME or OPPOSITE polarities.
Given 46 manuallyprepared seed templates we calculate the Excitationvalue of each template, a value in range [?1, 1] thatis positive if the template is excitatory and negativeif it is inhibitory.
Technically, our method treats alltemplates as excitatory or inhibitory, and, upon com-pletion, regards templates with small absolute Exci-tation values as neutral.The whole method is a bootstrapping process.Each iteration expands the network and the Excita-tion value of each template is (re-)calculated.3.1 Characteristics of Excitation TemplatesOur method exploits natural discourse constraints onthe possible combinations of (a) the polarity of co-occurring templates, (b) the nouns that fill their ar-gument slots and (c) the connectives that link thetemplates in a given sentence.
Table 1 shows theconstraints and Figure 1 shows examples that willbe explained shortly.
Though our target is Japanesewe believe these constraints are universal discourseprinciples, and as such not language dependent.
Ex-amples are given in English for ease of explanation.We first identify two categories of connectivesin our target sentences: AND/THUS-type (e.g., and,thus and since) and BUT-type (e.g., but and though).Both types suggest a sort of consistency or inconsis-tency between predicates.
We manually classified169 frequently used connectives into AND/THUS-620(1) He smoked cigarettes, AND/THUS he suffered lungcancer.
(Both smoke X and suffer X are excitatory.
)(2) He quit cigarettes, AND/THUS was immune from lungcancer.
(quit X and immune from X are inhibitory.
)(3) He smoked cigarettes, BUT didn?t suffer lung cancer.
(smoke X is excitatory, not suffer X is inhibitory.
)(4) He quit cigarettes, BUT he suffered lung cancer.
(quitX is inhibitory, but suffer X is excitatory.
)(5) He underwent cancer treatment, AND/THUS he couldcure the cancer.
(undergo X is excitatory, cure X isinhibitory.
)(6) He underwent cancer treatment, BUT still had cancer.
(Both undergo X and have X are excitatory.
)(7) Unnatural: He smoked cigarettes, BUT he sufferedlung cancer.
(smoke X and suffer X are excitatory.
)Figure 1: Examples of constraints: (cigarettes, lung can-cer) is PNP and (cancer treatment, cancer) is NNP.PNPs NNPs othersAND/THUS SAME OPPOSITE N/ABUT OPPOSITE SAME N/ATable 1: Constraint matrix.and BUT-type (See supplementary materials).Next we extract sentences from the Web in whichtwo templates co-occur and are joined by one ofthese connectives, and then classify the noun pairsfilling the templates?
argument slots into ?positively-associated?
and ?negatively-associated?
noun pairs(PNPs and NNPs).
Mirroring our definition of Excita-tion, PNPs are noun pairs in which the referent of thefirst noun facilitates the emergence of the referentof the second noun.
PNPs can range from causallyrelated noun pairs like (cigarettes, lung cancer) to?material-product?
relation pairs like (semiconduc-tor, electronic circuit).
We found that PNPs onlyfill the argument slots of (a) same Excitation polar-ity templates connected by AND/THUS-type connec-tives (examples 1 and 2 in Figure 1), or (b) oppositeExcitation polarity templates connected by a BUT-type connectives (examples 3 and 4).
Violating suchconstraints (example 7) seems unnatural.
Similarly,NNPs are noun pairs in which the referent of onenoun suppresses the emergence of the referent of theother noun.
Examples include such ?inverse causal-ity?
pairs as (cancer treatment, cancer).
NNPs onlyfill the argument slots of (a) opposite Excitation po-larity templates connected by AND/THUS-type con-nectives (example 5), or (b) same polarity templatesconnected by a BUT-type connective (example 6).All these constraints are summarized in Table 1,which we will call the constraint matrix.
Accord-ing to the constraint matrix, we can know whethertwo templates?
polarities are the same or opposite ifwe know whether a noun pair filling the two tem-plates?
slots is PNP or NNP.
Conversely, we canknow whether a noun pair is PNP or NNP if we knowwhether two templates whose slots are filled withthe noun pair have the same or opposite polarities.We believe these constraints capture certain univer-sal principles of discourse, since it is difficult in anylanguage to produce natural sounding sentences thatviolate these constraints.
We empirically confirmtheir validity for Japanese in Section 5.1.3.2 Bootstrapping Approach to ExcitationTemplate AcquisitionTo calculate the Excitation values for the templates,we construct a template network where templatesare connected by links indicating polarity agreementbetween two connected templates (either SAME orOPPOSITE polarity), as determined by the constraintmatrix.
Excitation values are determined by spread-ing activation applied to the network, given a smallnumber of manually prepared seed templates.However, we cannot construct the network unlesswe know whether each noun pair is PNP or NNP, dueto the configuration of the constraint matrix, and cur-rently we have no feasible method to classify all ofthem into PNPs and NNPs in advance.
We thereforeadopt a bootstrapping method (Figure 2) that startsfrom manually prepared excitatory and inhibitoryseed templates (Step 1 in Figure 2).
Our methodbegins by extracting noun pairs from the Web thatco-occur with two seed templates connected by aAND/THUS- or BUT-type connective, and classifiesthese noun pairs into PNPs and NNPs based on theconstraint matrix (Steps 2 and 3).
Next, we automat-ically extract additional (non-seed) template pairsfrom the Web that co-occur with these PNPs andNNPs.
Links (either SAME or OPPOSITE) betweenall template pairs are determined by the constraintmatrix (Step 4), and we construct a template networkfrom both seed and non-seed template pairs (Step 5).Our method calculates the Excitation values forall the templates in the network by first assign-ing Excitation values +1 and ?1 to the excitatoryand inhibitory seed templates, and applies a spread-ing activation method proposed by Takamura et al(2005) (Step 6) to the network.
This method calcu-6211.
Prepare initial seed templates with fixed excitation values (either+1 or ?1).2.
Make seed template pairs that are combinations of two seed tem-plates and a connective (either AND/THUS-type or BUT-type).3.
Extract noun pairs that co-occur with one of the seed templatepairs from the Web.
Classify the noun pairs into PNPs and NNPsbased on the constraints matrix.
Filter out those noun pairs thatappear as both PNP and NNP on the Web or those whose occur-rence frequency is less than or equal to F, which is set to 5.4.
Extract additional (non-seed) template pairs that are filled by oneof the PNPs or NNPs from the Web.
Determine the link type(SAME or OPPOSITE) for each template pair based on the con-straint matrix.
If a template pair appears on the Web as havingboth link types, we determine its link type by majority vote.5.
Construct the template network from all the template pairs.
Re-move from the network those templates whose number of linkedtemplates is less than D, which is set to 5.6.
Apply Takamura et al method to the network and fix the Exci-tation value of each template.7.
Extract the top- and bottom-ranked N ?
i templates from theresult of Takamura et al method.
N is a constant, which isset to 30. i is the iteration number.
They are used as additionalseed templates for the next iteration.
The top-ranked templatesare given Excitation value +1 and the bottom-ranked templatesare assigned ?1.
Go to Step 2.Figure 2: Bootstrapping for template acquisition.lates all templates?
excitation values by solving thenetwork constraints imposed by the SAME and OP-POSITE links, and the Excitation values of the seedtemplates (This method is detailed in Section 3.3).In each iteration i, our method selects the N ?
i top-ranked and bottom-ranked templates as additionalseed templates for the next iteration (N is set to 30)(Step 7).
Our method then constructs a new tem-plate network using the augmented seed templatesand restarts the calculation process.
Figure 2 sum-marizes our bootstrapping process.Bootstrapping stops after M iterations, with Mset to 7 based on our preliminary experiments.To prepare the initial seed templates we con-structed a maximal template network that could intheory be created by our bootstrapping method.
Thismaximal network consists of any two templates thatco-occur in a sentence with any connective, regard-less of their arguments.
We manually selected 36excitatory and 10 inhibitory seed templates fromamong 114 templates with the most links in the net-work (See supplementary materials).3.3 Determining Excitation in the NetworkThis section details Step 6 of our bootstrappingmethod, i.e., how Takamura et al method calcu-lates the Excitation value of each template.
Theirmethod is based on the spin model in physics, whereeach electron has a spin of either up or down.
Wechose this method due to the straightforward parallelbetween the spin model and our Excitation templatemodel.
Both models capture the spreading of acti-vation (either spin direction or excitation polarity)between neighboring objects in a network.
Deter-mining the optimal algorithm for this task is beyondour current scope, but for the purpose of our experi-ments we found that Takamura et al method gavesatisfactory results.The spin model defines an energy function on aspin network, and each electron?s spin can be esti-mated by minimizing this function:E(x,W ) = ?1/2?
?ijwijxixjHere, xi, xj ?
x are spins of electrons i and j, andmatrix W = {wij} assigns weights to links betweenelectrons.
We regard templates as electrons and Ex-citation polarities as their spins (up and down corre-spond to excitatory and inhibitory).
We define theweight wij of the link between templates i and j as:wij ={1/?d(i)d(j) if SAME(i, j)?1/?d(i)d(j) if OPPOSITE(i, j)Here, d(i) denotes the number of templates linkedto i.
SAME(i, j) (OPPOSITE(i, j)) indicates a SAME(OPPOSITE) link exists between i and j.
We obtainexcitation values by minimizing the above energyfunction.
Note that after minimizing E, xi and xjtend to get the same polarity when wij is positive.When wij is negative, xi and xj tend to have op-posite polarities.
Initially seed templates are givenvalues +1 or ?1 depending on whether they are ex-citatory or inhibitory, and others are given 0.We used SUPPIN (http://www.lr.pi.titech.ac.jp/?takamura/pubs/SUPPIN-0.01.tar.gz),an implementation of Takamura et al method.
Itsparameter ?
is set to the default value (0.75).4 Knowledge Acquisition by ExcitationThis section shows how the concept of Excitationcan be used for automatic knowledge acquisition.4.1 Contradiction ExtractionOur first knowledge acquisition method extractscontradiction pairs like destroy cancer ?
developcancer, based on our assumption that they often con-sist of distributionally similar templates that have asharp contrast in Excitation value.
Concretely, we622extract two phrases as a contradiction pair if (a)their templates have opposite Excitation polarities,(b) they share the same argument noun, and (c) thepart-of-speech of their predicates is the same.
Thenthe contradiction pairs are ranked by Ct:Ct(p1, p2) = |s1| ?
|s2| ?
sim(t1, t2)Here p1 and p2 are two phrases that satisfy condi-tions (a), (b) and (c) above, t1 and t2 are their re-spective templates, and |s1| and |s2| are the absolutevalues of t1 and t2?s excitation values.
sim(t1, t2) isthe distributional similarity proposed by Lin (1998).Note that ?contradiction?
here includes what wecall ?quasi-contradiction.?
This consists of twophrases such that, if the tendencies of the events theydescribe get stronger, they eventually become con-tradictions.
For example, the pair emit smells ?
re-duce smells is not logically contradictory since thetwo events can happen at the same time.
However,they become almost contradictory when their ten-dencies get stronger (i.e., emit smells more strongly?
thoroughly reduce smells).
We believe quasi-contradictions are useful for NLP tasks.4.2 Causality ExtractionOur second knowledge acquisition method extractscausality pairs like increase in crime ?
heightenanxiety that co-occur with AND/THUS-type connec-tives in a sentence.
The assumption is that if twotemplates (t1 and t2) with a strong Excitation ten-dency are connected by an AND/THUS-type connec-tive in a sentence, the event described by t1 and itsargument n1 tends to be a cause of the event de-scribed by t2 and its argument n2.
Here, Excitationstrength is expressed by absolute Excitation values.The intuition is that, if the referent of n1 is stronglyactivated or suppressed, it tends to have some causaleffect on the referent of n2 in the same sentence.We focus on extracting causality pairs that co-occur with only ?non-causal connectives?
like and,which are AND/THUS-type connectives that do NOTexplicitly signal causality, since causal connectiveslike thus can mask the effectiveness of Excitation.We prepared 139 non-causal connectives (See sup-plementary materials).
We extract two templatessuch as increase in X and heighten Y co-occurringwith only non-causal connectives, as well as thenoun pair that fills the two templates?
slots (e.g.,(crime, anxiety)) to obtain causal phrase pairs.
InJapanese, the temporal order between events is usu-ally determined by precedence in the sentence.
Csranks the obtained causality pairs:Cs(p1, p2) = |s1| ?
|s2|Here p1 and p2 are the phrases of causality pair, and|s1| and |s2| are absolute Excitation values of p1?sand p2?s templates.
As is common in the literature,this notion of causality should be interpreted prob-abilistically rather than logically, i.e., we interpretcausality A ?
B as ?if A happens, the probability ofB increases?.
This interpretation is often more use-ful for NLP tasks than a strict logical interpretation.4.3 Causality Hypothesis GenerationOur third knowledge acquisition method generatesplausible causality hypotheses that are not written inany single sentence using the previously extractedcontradiction and causality pairs.
We assume that ifa causal relation (e.g., increase in crime ?
heightenanxiety ) is valid, its inverse (e.g., decrease in crime?
diminish anxiety ) is often valid as well.
Froma logical definition of causation, taking the inverseof an implication obviously does not preserve valid-ity.
However, at least under our probabilistic inter-pretation, taking the inverse of a given causality pairusing the extracted contradiction pairs proves to bea viable strategy for generating non-trivial causalityhypotheses, as our experiments in Section 5.4 show.For an extracted causality pair, we generate itsinverse as a causality hypothesis by replacing bothphrases in the original pair with their contradictioncounterparts.
For instance, a causality hypothesisdecrease in crime ?
diminish anxiety is generatedfrom a causality increase in crime ?
heighten anxi-ety by two contradictions, decrease in crime ?
in-crease in crime and diminish anxiety ?
heightenanxiety.
Since we are interested in finding newcausal hypotheses, we filter out hypotheses whosephrase pair co-occurs in a sentence in our corpus.Remaining causality hypotheses are ranked by Hp.Hp(q1, q2) = Ct(p1, q1)?
Ct(p2, q2)?
Cs?
(p1, p2)Here, q1 and q2 are two phrases of a causality hy-pothesis.
p1 and p2 are two phrases of a hypothesis?soriginal causality.
That is, p1 ?
q1 and p2 ?
q2 arecontradiction pairs, and Ct(p1, q1) and Ct(p2, q2)are their contradiction scores.
Cs?
(p1, p2) is theoriginal causality?s causality score.
Cs?
can be Cs623from Section 4.2, but based on preliminary experi-ments we found the following score works better:Cs?
(p1, p2) = |s1| ?
|s2| ?
npfreq(n1, n2)|s1| and |s2| are absolute Excitation values of p1?sand p2?s templates, whose slots are filled with n1 andn2.
npfreq(n1, n2) is the co-occurrence frequencyof (n1, n2) with polarity-identical template pairs (if(n1, n2) is PNP) or with polarity-opposite templatepairs (if (n1, n2) is NNP).
Thus, npfreq indicates asort of association strength between two nouns.5 ExperimentsThis section shows that our template acquisitionmethod acquired many Excitation templates.
More-over, using only the acquired templates we extractedone million contradiction pairs with more than 70%precision, and 500,000 causality pairs with about70% precision.
Further, using only these extractedcontradiction and causality pairs we generated onemillion causality hypotheses with 57% precision.In our experiments we removed evaluation sam-ples containing the initial seed templates and exam-ples used for annotation instruction from the evalua-tion data.
Three annotators (not the authors) markedall evaluation samples, which were randomly shuf-fled so that they could not identify which sample wasproduced by which method.
Information about thepredicted labels or ranks was also removed from theevaluation data.
Final judgments were made by ma-jority vote between the annotators.
They were non-experts without formal training in linguistics or se-mantics.
See supplementary materials for our anno-tation manuals (translated into English).We used 600 million Japanese Web pages(Akamine et al2010) parsed by KNP (Kawaharaand Kurohashi, 2006) as a corpus.
We restrictedthe argument positions of templates to ha (topic),ga (nominative), wo (accusative), ni (dative), and de(instrumental).
We discarded templates appearingfewer than 20 times in compound sentences (regard-less of connectives) in our corpus.5.1 Excitation Template AcquisitionWe show that our proposed method for template ex-traction (PROPtmp) successfully acquired many Ex-citation templates from which we obtained a hugenumber of contradiction and causality pairs, and thatExcitation is a reasonably comprehensible notioneven for non-experts.
We also show that PROPtmpoutperformed two baselines by a large margin.The template network constructed by PROPtmpcontained 10,825 templates.
Among these, the boot-strapping process classified 8,685 templates as exci-tatory and 2,140 as inhibitory.
Note that these can-didates in fact also contain neutral templates, as ex-plained at the beginning of Section 3.Baselines The baseline methods are ALLEXC andSIM.
ALLEXC regards all templates that are ran-domly extracted from the Web as excitatory, since inour data excitatory templates outnumber inhibitoryones.
Actually, in our data neutral templates rep-resent the most frequent class, but since our objec-tive is to acquire excitatory and inhibitory templates,a baseline marking all templates as neutral wouldmake little sense.
SIM is a distributional similaritybaseline that takes as input the same 10,825 tem-plates of PROPtmp above, constructs a network byconnecting two templates whose distributional simi-larity is greater than zero, and regards two connectedtemplates as having the same polarity.
The weightof the links between templates is set to their distri-butional similarity based on Lin (1998).
Then SIMis given the same initial seed templates as PROPtmp,by which it calculates the Excitation values of tem-plates using Takamura et al method.
As a result,SIM assigned positive Excitation values to all tem-plates, and except for the 10 inhibitory initial seedtemplates no templates were regarded inhibitory.Evaluation scheme We randomly sampled 100templates each from PROPtmp?s 8,685 excitatorycandidates, PROPtmp?s 2,140 inhibitory candidates,all the ALLEXC?s templates, and all the SIM?s tem-plates, i.e., 400 templates in total.
To make the an-notators?
judgements easier, we randomly filled theargument slot of each template with a noun filling itsargument slot in our Web corpus.
Three annotatorslabeled each sample (a combination of a templateand a noun) as ?excitatory,?
?inhibitory,?
?neutral,?
or?undecided?
if they were not sure about its label.Results for excitatory In the top graph in Fig-ure 3, ?Proposed?
shows PROPtmp?s precision curve.The curve is drawn from its 100 samples whose X-axis positions represent their ranks.
We plot a dot forevery 5 samples.
Among the 100 samples, 37 werejudged as excitatory, 6 as inhibitory, 45 as neutral,and 6 as ?undecided?.
For the remaining 6 samples,62400.20.40.60.810  2000  4000  6000  8000  10000Precision?Proposed??Sim?
?Allexc?0.40.60.810  500  1000  1500  2000  2500PrecisionTop-N?Proposed?Figure 3: Precision of template acquisition: excitatory(top) and inhibitory (bottom).the three annotators gave three different labels andthe label was not fixed (?split-votes?
hereafter).
Forcalculating precision, only the 37 samples labeledexcitatory were regarded as correct.
PROPtmp out-performed all baselines by a large margin, with anestimated 70% precision for the top 2,000 templates.?Allexc?
and ?Sim?
in Figure 3 denote ALLEXC andSIM.
Among ALLEXC?s 100 samples, 19 werejudged as excitatory, 5 as inhibitory, 74 as neutral,and 2 as ?undecided?.
SIM?s low performance re-flects the fact that templates with opposite polaritiesare sometimes distributionally similar, and as a re-sult get connected by SAME links.Results for inhibitory ?Proposed?
in the bottomgraph in Figure 3 shows the precision curve drawnfrom the 100 samples of PROPtmp?s inhibitory can-didates.
Among the 100 samples, 41 were judged asinhibitory, 15 as excitatory, 32 as neutral, 4 as ?unde-cided?, and 8 as ?split-votes?.
Only the 41 inhibitorysamples were regarded as correct.
From the curvewe estimate that PROPtmp achieved about 70% pre-cision for the top 500.
Note that SIM could not ac-quire any inhibitory templates, yet we can think ofno other reasonable baseline for this task.Inter-annotator agreement The Fleiss?
kappa(Fleiss, 1971) of annotator judgements was 0.48(moderate agreement (Landis and Koch, 1977)).
Fortraining, the annotators were given a one-page anno-tation manual (see supplementary materials), whichbasically described the same contents in Section 2,in addition to 14 examples of excitatory, 14 exam-ples of inhibitory, and 6 examples of neutral tem-plates that were manually prepared by the authors.Using the manual and the examples, we instructedall the annotators face-to-face for a few hours.
Wealso made sure the evaluation data did not containany examples used during instruction.Observations about argument positions Amongthe 200 evaluation samples of PROPtmp (for both ex-citatory and inhibitory evaluations), 52 were judgedas excitatory, 47 as inhibitory, and 77 as neutral.
Forthe excitatory templates, the numbers of nominative,topic, accusative, dative, and instrumental argumentpositions are 15, 11, 10, 8, and 8, respectively.
Forthe inhibitory templates, the numbers are 17, 11, 16,3, and 0.
For the neutral templates, the numbers are8, 23, 17, 21, and 8.
Accordingly, we found no no-ticeable bias with regard to their numbers.
Likewise,we found no noticeable bias regarding their useful-ness for contradiction and causality acquisition re-ported shortly, too.Summary PROPtmp works well, as it outperformsthe baselines.
Its performance demonstrates the va-lidity of our constraint matrix in Table 1.
Besides,since our annotators were non-experts but showedmoderate agreement, we conclude that Excitation isa reasonably comprehensible notion.5.2 Contradiction ExtractionThis section shows that our proposed method forcontradiction extraction (PROPcont) extracted onemillion contradiction pairs with more than 70% pre-cision, and that Excitation values are useful for con-tradiction ranking.
As input for PROPcont we tookthe top 2,000 excitatory and the top 500 inhibitorytemplates from the previous experiment (i.e., theother templates were regarded as neutral).Baselines Our baseline methods are RANDcontand PROPcont-NE.
RANDcont randomly combinestwo phrases, each consisting of a template and anoun that they share.
It does not rank its output.PROPcont-NE is the same as PROPcont except that itdoes not use Excitation values; ranking is based onlyon sim(t1, t2).
PROPcont-NE does combine phraseswith opposite template polarities, just like PROPcont.Evaluation scheme We randomly sampled 200phrase pairs from the top one million results of each62500.20.40.60.810  200000  400000  600000  800000  1e+06PrecisionTop-N?Proposed??Proposed-ne?
?Random?Figure 4: Precision of contradiction extraction.PROPcont and PROPcont-NE, and 100 samples fromthe output of RANDcont?s output, giving 500 sam-ples.
Three annotators labeled whether the samplesare contradictions.
Fleiss?
kappa was 0.78 (substan-tial agreement).Results ?Proposed?
in Figure 4 shows the preci-sion curve of PROPcont.
PROPcont achieved an esti-mated 70% precision for its top one million results.Readers might wonder whether PROPcont?s outputconsists of a small number of template pairs that arefilled with many different nouns.
If this were thecase, PROPcont?s performance would be somewhatmisleading.
However, we found that PROPcont?s 200samples contained 194 different template pairs, sug-gesting that our method can acquire a large varietyof contradiction phrases.
?Proposed-ne?
is the pre-cision curve for PROPcont-NE.
Its precision is morethan 10% lower than PROPcont at the top one millionresults.
?Random?
shows that RANDcont?s precisionis only 4%.
Table 2 shows examples of PROPcont?soutputs and their English translation.
The labels?Cont,?
?Quasi?
and ?6?
denote whether a pair is con-tradictory, quasi-contradictory, or not contradictory.Among PROPcont?s 145 samples judged by the an-notators as contradiction, 46 were judged as quasi-contradictory by one of the authors.
The first 6case in Table 2 was caused by the template, X?????
(improve X).
It is tricky since it is excitatorywhen taking arguments like function, while it is in-hibitory when taking arguments like disorder.
How-ever, PROPtmp currently cannot distinguish these us-ages and judged it as inhibitory in our experimentsin Section 5.1, though it must be interpreted as ex-citatory for the 6 case.
The second 6 case was dueto PROPtmp?s error; it incorrectly judged the neutraltemplate, X?????
(related to X), as inhibitory.Rank Contradiction Pairs Label8,767 ???????????
?????????????
Contrepair imbalance ?
become imbalanced103,581 ??????
???????
Contassist the driver ?
disturb the driver151,338 ?????????
??????
Quasicalm tension ?
feel tension184,014 ???????
???????
6improve function ?
boost function316,881 ??????
????????
Contyen depreciation stops ?
yen depreciation develops317,028 ????????
????????
Contnoise gets worse ?
noise abates334,642 ?????
???????
Conta sour taste is augmented ?
a sour taste is lost487,496 ???????
???????
Quasifeel pain ?
reduce pain529,173 ????????
??????????
Contaccess occurs ?
curb access555,049 ??????
???????
Contlose nuclear plants ?
augment nuclear plants608,895 ?????????
???????
Quasiradioactivity is released ?
radioactivity is reduced638,092 ????????
?????????
ContEuro falls ?
Euro gets strong757,423 ???????
?????????
Quasihave share (in market) ?
share decreases833,941 ??????????
??????????
6generate active oxygen ?
related to active oxygen848,331 ???????
?????????
Contdestroy cancer ?
develop cancer982,980 ?????????
???????????
Contvirus becomes extinct ?
virus is activatedTable 2: Examples of PROPcont?s outputs.Summary PROPcont is a low cost but high perfor-mance method, since it acquired one million con-tradiction pairs with over 70% precision from onlythe 46 initial seed templates.
Besides, Excitationcontributes to contradiction ranking since PROPcontoutperformed PROPcont-NE by a 10% margin for thetop one million results.
Thus we conclude that ourassumption on contradiction extraction is valid.5.3 Causality ExtractionWe show that our method for causality extraction(PROPcaus) extracted 500,000 causality pairs withabout 70% precision, and that Excitation values con-tribute to the ranking of causal pairs.
PROPcaus tookas input all 10,825 templates classified by PROPtmp.Baselines RANDcaus randomly extracts twophrases that co-occur in a sentence with one of theAND/THUS-type connectives, i.e., it uses not onlynon-causal connectives but also causal ones likethus.
FREQ is the same as PROPcaus except that itranks its output by the phrase pair co-occurrencefrequency rather than Excitation values.62600.20.40.60.810  200000  400000  600000  800000  1e+06PrecisionTop-N?Proposed??Freq?
?Random?Figure 5: Precision of causality extraction.Evaluation scheme We randomly sampled 100pairs each from the top one million results ofPROPcaus and FREQ, and all RANDcaus?s output.The annotators were shown the original sentencesfrom which the samples were extracted.
Fleiss?kappa was 0.68 (substantial agreement).Results ?Proposed?
in Figure 5 is the precisioncurve for PROPcaus.
From this curve the estimatedprecision of PROPcaus is about 70% around the top500,000.
Note that PROPcaus outperformed FREQby a large margin, and extracted a large variety ofcausal pairs since its 100 samples contained 91 dif-ferent template pairs.
Table 3 shows examples ofPROPcaus?s output along with English translations.The labels ?4?
and ?6?
denote whether a pair iscausality or not.
The 6 cases in Table 3 wereexceptions to our assumption described in Section4.2; even if two Excitation templates co-occur in asentence with an AND/THUS-type connective, theysometimes do not constitute causality.
Actually, thefirst 6 case consists of two phrases that co-occurredin a sentence with a (non-causal) AND/THUS-typeconnective but described two events that happen asthe effects of introducing the RAID storage system;both are caused by the third event.
In the second 6case, the two phrases co-occurred in a sentence witha (non-causal) AND/THUS-type connective but justdescribed two opposing events.Summary PROPcaus performs well since it ex-tracted 500,000 causality pairs with about 70%precision.
Moreover, Excitation values contributeto causality ranking since PROPcaus outperformedFREQ by a large margin.
Then we conclude that ourassumption on causality extraction is confirmed.Rank Causality Pairs Label1,036 ??????????????????
4increase basal metabolism ?
enhance fat-burning ability2,128 ??????????????????
4increase desire to learn ?
facilitate self-learning6,471 ??????????????
6improve reliability ?
increase capacity29,638 ????????????????????????
4circulating thyroid hormone level increases ?
improves metabolism56,868 ???????????????
4exports increase ?
GDP grows267,364 ????????????????
4promote blood circulation ?
improve metabolism268,670 ????????????????
4BSE outbreak occurs ?
import ban (on beef) is issued290,846 ?????????????????
4improve the view ?
improve the efficiency of work322,121 ???????????????????
4giant earthquake occurs ?
meltdown is triggered532,106 ???????????????
4good at thermal efficiency ?
enhance heating efficiency563,462 ????????????????
4promote inflation (in Japan) ?
yen depreciation develops591,175 ????????????????
6bring profit ?
bring detriment657,676 ??????????????
4physical strength declines ?
immune system weakens676,902 ??????????????????
4sharp fall in government bond futures occurs ?
interest rates increase914,101 ??????????????
4have a margin of error ?
cause troubleTable 3: Examples of PROPcaus?s outputs.5.4 Causality Hypothesis GenerationHere we show that our causality hypothesis genera-tion method in Section 4.3 (PROPhyp) extracted onemillion hypotheses with about 57% precision.This experiment took the top 100,000 results ofPROPcaus as input, generated hypotheses from them,and randomly selected 100 samples from the top onemillion hypotheses.
We evaluated only PROPcaus,since we could not think of any reasonable baselinefor this task.
Randomly coupling two phrases mightbe a baseline, but it would perform so poorly that itcould not be a reasonable baseline.The annotators judged each sample in the sameway as Section 5.3, except that we presented themwith source causality pairs from which hypotheseswere generated, as well as the original sentences ofthese source pairs.
Fleiss?
kappa was 0.51 (moderateagreement).As a result, PROPhyp generated one million hy-potheses with 57% precision.
It generated variouskinds of hypotheses, since these 100 samples con-tained 99 different template pairs.
Table 4 showssome causal hypotheses generated by PROPhyp.
Thesource causal pair is shown in parentheses.
The la-627bels ?4?
and ?6?
denote whether a pair is causalityor not.
The first 6 case was due to an error made byRank Causality Hypotheses (and their Origin) Label18,886 ??????????????????
4(???????????????)
4alleviate stress ?
remedy insomnia(increase stress ?
continue to have insomnia)93,781 ????????????????
4(????????????)
4halt deflation ?
tax revenue increases(deflation is promoted ?
tax revenes declines)121,163 ??????????????????
4(???????????????)
4enjoyment increases ?
stress decreases(enjoyment decreases ?
stress grows)205,486 ??????????????
4(??????????????)
4decrease in crime ?
diminish anxiety(increase in crime ?
heighten anxiety)253,531 ?????????????????
4(????????????????????)
4reduce chlorine ?
bacteria grow(generate chlorine ?
bacteria extinct)450,353 ????????????????
4(????????????)
4expand demand ?
decrease unemployment rate(decrease demand ?
increase unemployment rate)464,546 ???????????????????
6(??????????????????)
6(ability of) digestion deteriorates ?
cholesterol increases(aid digestion ?
decrease cholesterol)538,310 ???????????????
4(?????????????)
4relieve fatigue ?
improve immunity(feel fatigued ?
immunity is weakened)789,481 ???????????????
4(????????????????)
4conditions improve ?
prevent troubles(conditions become bad ?
cause troubles)837,850 ?????????????????
6(????????????????)
4control economic conditions ?
accompany problems(economic conditions improve ?
problems are solved)Table 4: Examples of causality hypotheses.our causality extraction method PROPcaus; the casewas erroneous since its original causality was erro-neous.
The second 6 case was due to the fact thatone of the contradiction phrase pairs used to gener-ate the hypothesis was in fact not contradictory (???????????
6?
???????
?con-trol economic conditions 6?
economic conditions im-prove?
).From these results, we conclude that our assump-tion on causality hypothesis generation is valid.6 Related WorkWhile the semantic orientation involving good/bad(or desirable/undesirable) has been extensively stud-ied (Hatzivassiloglou and McKeown, 1997; Turney,2002; Rao and Ravichandran, 2009; Velikovich etal., 2010), we believe Excitation represents a gen-uinely new semantic orientation.Most previous methods of contradiction extrac-tion require either thesauri like Roget?s or WordNet(Harabagiu et al2006; Mohammad et al2008; deMarneffe et al2008) or large training data for su-pervision (Turney, 2008).
In contrast, our methodrequires only a few seed templates.
Lin et al2003)used a few ?incompatibility?
patterns to acquireantonyms, but they did not report their method?s per-formance on the incompatibility identification task.Many methods for extracting causality or script-like knowledge between events exist (Girju, 2003;Torisawa, 2005; Torisawa, 2006; Abe et al2008;Chambers and Jurafsky, 2009; Do et al2011; Shi-bata and Kurohashi, 2011), but none uses a notionsimilar to Excitation.
As we have shown, we expectthat Excitation will improve their performance.Regarding the acquisition of semantic knowledgethat is not explicitly written in corpora, Tsuchida etal.
(2011) proposed a novel method to generate se-mantic relation instances as hypotheses using auto-matically discovered inference rules.
We think thatautomatically generating plausible semantic knowl-edge that is not written (explicitly) in corpora as hy-potheses and augmenting semantic knowledge baseis important for the discovery of so-called ?unknownunknowns?
(Torisawa et al2010), among others.7 ConclusionWe proposed a new semantic orientation, Excitation,and its acquisition method.
Our experiments showedthat Excitation allows to acquire one million con-tradiction pairs with over 70% precision, as well ascausality pairs and causality hypotheses of the samevolume with reasonable precision from the Web.
Weplan to make all our acquired knowledge resourcesavailable to the research community soon (Visithttp://www.alagin.jp/index-e.html).We will investigate additional applications of Ex-citation in future work.
For instance, we expect thatExcitation and its related semantic knowledge ac-quired in this study will improve the performanceof Why-QA system like the one proposed by Oh etal.
(2012).628ReferencesShuya Abe, Kentaro Inui, and Yuji Matsumoto.
2008.Two-phrased event relation acquisition: Coupling therelation-oriented and argument-oriented approaches.In Proceedings of the 22nd International Conferenceon Computational Linguistics (COLING 2008), pages1?8.Susumu Akamine, Daisuke Kawahara, Yoshikiyo Kato,Tetsuji Nakagawa, Yutaka I. Leon-Suematsu, TakuyaKawada, Kentaro Inui, Sadao Kurohashi, and YutakaKidawara.
2010.
Organizing information on the webto support user judgments on information credibil-ity.
In Proceedings of 2010 4th International Uni-versal Communication Symposium Proceedings (IUCS2010), pages 122?129.Alina Andreevskaia and Sabine Bergler.
2006.
Semantictag extraction from wordnet glosses.
In Proceedingsof the 5th International Conference on Language Re-sources and Evaluation (LREC 2006).Nathanael Chambers and Dan Jurafsky.
2009.
Unsu-pervised learning of narrative schemas and their par-ticipants.
In Proceedings of the 47th Annual Meetingof the ACL and the 4th IJCNLP of the AFNLP (ACL-IJCNLP 2009), pages 602?610.Marie-Catherine de Marneffe, Anna Rafferty, andChristopher D. Manning.
2008.
Finding contradictionin text.
In Proceedings of the 48th Annual Meeting ofthe Association of Computational Linguistics: HumanLanguage Technologies (ACL-08: HLT), pages 1039?1047.Quang Xuan Do, Yee Seng Chan, and Dan Roth.
2011.Minimally supervised event causality identification.In Proceedings of the 2011 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2011), pages 294?303.Joseph L. Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378?382.Roxana Girju.
2003.
Automatic detection of causal re-lations for question answering.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics (ACL 2003), Workshop on Multi-lingual Summarization and Question Answering - Ma-chine Learning and Beyond, pages 76?83.Sanda Harabagiu, Andrew Hickl, and Finley Lacatusu.2006.
Negation, contrast and contradiction in text pro-cessing.
In Proceedings of the 21st National Confer-ence on Artificial Intelligence (AAAI-06), pages 755?762.Vasileios Hatzivassiloglou and Kathleen R. McKeown.1997.
Predicting the semantic orientation of adjec-tives.
In Proceedings of the 35 Annual Meeting ofthe Association for Computational Linguistics and the8the Conference of the European Chapter of the Asso-ciation of Computational Linguistics, pages 174?181.Daisuke Kawahara and Sadao Kurohashi.
2006.
A fully-lexicalized probabilistic model for Japanese syntacticand case structure analysis.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the ACL (HLT-NAACL2006),pages 176?183.J.
Richard Landis and Gary G. Koch.
1977.
The mea-surement of observer agreement for categorical data.Biometrics, 33(1):159?174.Dekang Lin, Shaojun Zhao Lijuan Qin, and Ming Zhou.2003.
Identifying synonyms among distributionallysimilar words.
In Proceedings of the 18th Inter-national Joint Conference on Artificial Intelligence(IJCAI-03), pages 1492?1493.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar words.
In Proceedings of the 36th AnnualMeeting of the Association for Computational Linguis-tics and 17th International Conference on Computa-tional Linguistics (COLING-ACL1998), pages 768?774.Saif Mohammad, Bonnie Dorr, and Greame Hirst.
2008.Computing word-pair antonymy.
In Proceedings ofthe 2008 Conference on Empirical Methods in Natu-ral Language Processing, pages 982?991.Jong-Hoon Oh, Kentaro Torisawa, Chikara Hashimoto,Takuya Kawada, Stijn De Saeger, Junichi Kazama, andYiou Wang.
2012.
Why question answering usingsentiment analysis and word classes.
In Proceedingsof EMNLP-CoNLL 2012: Conference on EmpiricalMethods in Natural Language Processing and NaturalLanguage Learning.Charles E. Osgood, George J. Suci, and Percy H. Tannen-baum.
1957.
The measurement of meaning.
Univer-sity of Illinois Press.James Pustejovsky.
1995.
The Generative Lexicon.
MITPress.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedingsof the 12th Conference of the European Chapter of theACL, pages 675?682.Tomohide Shibata and Sadao Kurohashi.
2011.
Acquir-ing strongly-related events using predicate-argumentco-occurring statistics and case frames.
In Proceed-ings of the 5th International Joint Conference on Natu-ral Language Processing (IJCNLP 2011), pages 1028?1036.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General Inquirer:A Computer Approach to Content Analysis.
MITPress.Hiroya Takamura, Takashi Inui, and Manabu Okumura.2005.
Extracting semantic orientation of words using629spin model.
In Proceedings of the 43rd Annual Meet-ing of the ACL, pages 133?140.Kentaro Torisawa, Stijn De Saeger, Jun?ichi Kazama,Asuka Sumida, Daisuke Noguchi, Yasunari Kakizawa,Masaki Murata, Kow Kuroda, and Ichiro Yamada.2010.
Organizing the web?s information explosionto discover unknown unknowns.
New GenerationComputing (Special Issue on Information Explosion),28(3):217?236.Kentaro Torisawa.
2005.
Automatic acquisition of ex-pressions representing preparation and utilization ofan object.
In Proceedings of the Recent Advancesin Natural Language Processing (RANLP05), pages556?560.Kentaro Torisawa.
2006.
Acquiring inference ruleswith temporal constraints by using japanese coordi-nated sentences and noun-verb co-occurrences.
InProceedings of the Human Language Technology Con-ference of the North American Chapter of the ACL(HLT-NAACL2006), pages 57?64.Masaaki Tsuchida, Kentaro Torisawa, Stijn De Saeger,Jong-Hoon Oh, Jun?ichi Kazama, Chikara Hashimoto,and Hayato Ohwada.
2011.
Toward finding semanticrelations not written in a single sentence: An inferencemethod using auto-discovered rules.
In Proceedingsof the 5th International Joint Conference on NaturalLanguage Processing (IJCNLP 2011), pages 902?910.Peter D. Turney.
2002.
Thumbs up or thumbs down?semantic orientation applied to unsupervised classifi-cation of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Linguis-tics (ACL 2002), pages 417?424.Peter Turney.
2008.
A uniform approach to analogies,synonyms, antonyms, and associations.
In Proceed-ings of the 22nd International Conference on Compu-tational Linguistics (COLING 2008), pages 905?912.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Han-nan, and RyanMcDonald.
2010.
The viability of web-derived polarity lexicons.
In Human Language Tech-nologies: The 2010 Annual Conference of the NorthAmerican Chapter of the ACL, pages 777?785.Ellen M. Voorhees.
2008.
Contradictions and justifica-tions: Extensions to the textual entailment task.
InProceedings of the 48th Annual Meeting of the Associ-ation of Computational Linguistics: Human LanguageTechnologies (ACL-08: HLT), pages 63?71.630
