Automatic Phonetic Baseform DeterminationL.
R. Bahl, S. Das, P. V. deSouza, M. Epstein, R. L. Mercer,B.
Merialdo, D. Nahamoo, M. A. Picheny, J. PowellContinuous Speech Recognition Group,Computer Sciences DepartmentIBM Research Division,Thomas J. Watson Research CenterP.
O.
Box 704, Yorktown Heights, NY 10598ABSTRACTPhonetic baseforms are the basic recognition units in mostlarge vocabulary speech recognition systems.
These base-forms are usually determined by hand once a vocabulary ischosen and not modified thereafter.
However, many applica-tions of speech recognition, such as dictation transcription, arehampered by a fixed vocabulary and require the user be able toadd new words to the vocabulary.
At least one phonetic base-form must be assigned to each new word to properly integratethe word into the recognition system.
Dictionary lookup is of-ten unsuccessful in determining a phonetic baseform becausenew words are often names or task-specific jargon; also, talk-ers tend to have idiosyncratic pronunciations for a substantialfraction of words.
This paper describes a series of experimentsin which the phonetic baseform is deduced automatically fornew words by utilizing actual utterances of the new word inconjunction with a set of automatically derived spelling-to-sound rules.
We evaluated recognition performance on newwords spoken by two different talkers when the phonetic base-forms were extracted via the above approach.
The error ateson these new words were found to be comparable toor betterthan when the phonetic baseforms were derived by hand, thusvalidating the basic approach.1 IntroductionPhonetic baseforms are the basic recognition units in mostlarge vocabulary speech recognition systems.
These base-forms are usually determined by hand once a vocabulary ischosen and not modified thereafter.
However, many applica-tions of speech recognition, such as dictation transcription, arehampered by a fixed vocabulary and require the user be able toadd new words to the vocabulary.
At least one phonetic base-form must be assigned to each new word to properly integratethe word into the recognition system 1.
This paper describesa seres of experiments in which the phonetic baseform isdeduced automatically for a new word given its spelling byutilizing actual utterances ofthe new word in conjunction witha set of automatically derived spelling-to-sound rules.Most previous attempts o construct a phonetic baseform fora new word have been in the area of speech synthesis.
Klattt One does not necessarily require aphonetic baseform to add a new wordto a recognition system \[I, 2\], but if the system is odginaUy based on phoneticbaseforms, the system structure iscomplicated by having more than one typeof recognition model present.\[3\] reviews various approaches to this problem; most solu-tions involve some set of human-derived rules combined witha dictionary lookup procedure for exceptions.
The best sys-tems seem to produce correct baseforms for 95%-98% of thewords in running text.
We believe that such systems are inad-equate to handle the baseform construction problem in speechrecognition for several reasons.
First, many new words inspeech recognition systems are either names or task-specificjargon (see discussion in section 4).
Pronunciation of suchitems tend to be highly irregular and present substantial dif-ficulties for text-to-speech systems \[3\].
Second, talkers tendto develop idiosyncratic pronunciations of many words, espe-cially proper names.
For example, the name Picheny is oftenpronounced Pitch'-uh-nee rather than Pitch-eh'-nee even bypeople familiar with the individual, and the word asymptoticcan be pronounced with an initial EI rather than with an AEby people not terribly familiar with the word.
Thirdly, thespelling of a word sometimes has very little correlation withits pronunciation - the orthography AAA is often pronouncedTriple-A.
Finally, human-derived rule-based systems are verylabor intensive to create and often very hard to transfer fromthe original development si e to other institutions that wish toutilize it.It seems obvious from the above examples that proper de-duction of a phonetic baseform for a new word requires bothknowledge of the spelling of the word and at least one acous-tic example of its pronunciation.
While in theory it shouldbe possible to deduce a baseform from the utterance alone,current speech recognition systems do not yet produce pho-netic transcriptions with a high enough accuracy for this tobe feasible.
We follow the basic approach as described inLucassen \[4\].
In particular, our goal is to find the phone stringT' to maximizeP('P I ?,/'/) ~ argmaxP(Lt I 7~)p(7 ~I ?).
(1)where H represents he utterance(s) and ?
represents he wordspelling.
This is a standard speech recognition problem inwhich p(H I ~)  is computed with an acoustic model, usuallya Hidden Markov Model \[5\], and p(P I ?)
is computed witha language model.
In Lucassen's work, this language modelwas constructed automatically from pairs of word spellingsand pronunciations with the aid of a decision tree.We have advanced Lucassen's original work in the follow-ing directions.
First, the construction of decision trees has179been studied in much more detail \[6, 7, 8\] since the origi-nal work was performed; this allows us to construct bettertrees.
Second, Lucassen only had the data in Webster's 7thDictionary avaliable in incomplete form for training the de-cision trees; we now have access to other sources of data;e.g., the 20000 baseforms used in the Tangora \[9\].
We havealso substantially improved our techniques for aligning phonestrings agianst letter strings, a crucial component of decisiontree construction (Section 2.2).
Third, Lucassen used a singleutterance of a word to construct a baseform; we have sincefound that multiple utterances of a word can be utilized toconstruct a more consistent baseform.
Finally, in the originalstudy, the evaluation of all baseforms was done via visual in-spection and recognition performance was not measured; wewill describe a series of experiments in which we attempt toevaluate the performance of the baseforms for recognition.The structure of the paper is as follows.
Section 2 de-scribes the construction ofdecision trees for spelling to soundrules.
Section 3 describes how a phonetic baseform is de-coded using one or more utterances in conjunction with thespelling-to-soundrules.
Section 4 describes a series of exper-iments in which the recognition performance ofthe baseformsis measured.
Finally, Section 5 discusses the recognition per-formance and has suggestions for future work.2 Decision Trees for Spelling-to-SoundRulesOur goal is to construct a model forp(7 ~ I ?
), the probability ofthe phone string given the letter string.
More specifically, let?
= l l , .
.
.
,  In = l~ be a string of letters and 7 ~ = P l , .
.
.
,  Pm=p~ be the string of phones corresponding to ?.
We assumecorresponding to each letter li is a string of phones whichcan be interpreted as the pronunciation of the letter.
Thepronunciation of letter li, denoted as ri, may correspond to asingle phone, a string of phones, or the null phone (i.e., it hasno corresponding pronunciation i the original phone string7 ~.
For example, Table 1 indicates the pronunciations of theletters in the word humane.Ihl u Iml a Inl?l  h juu l  m eil n 0Table 1: Pronunciations of letters in the word humane.Note that he e is silent, so we assign it a null pronunciation.Note also that the j might have be assigned to the letter hjust as well as the letter u; such decisions are arbitrary butconsistency across words should be enforced.
Details of howa set of pronunciations may be determined for a set of wordscan be found in Section 2.2; for now, we will assume thatwe have an inventory of such pronunciations in which eachpronunciation consists of a string of phones.We now may write the probability of pronunciation string7~asnp(7~1? )
= I Ip( r i l r~ -a,l~) (2)i=lPl~-- 1 li+5 IXp(rl l i-5,,i-sJ (3)i=I(4)We assume that the probability of the pronunciation rz onlydepends on the current letter, the five previous letters, the fivefollowing letters, and the five previous pronunciations.
Wedefine this to be the context of the current pronunciation.Estimatingp(7~ \[ ?)
in a straightforward fashion by count-ing the number of times ri occurs in a particular context isan impossible task - there are just too many contexts.
In-stead, we will map contexts onto a relatively small numberof equivalence classes with the aid of a decision tree.
Thedecision tree will partition the contexts into classes by ask-ing binary questions about he context elements; e.g., "Is thenext letter a vowel?
", "Is the previous pronunciation a plo-sive?
", etc.
The leaves of the decision tree will represent theequivalence lasses.
At each leaf is a probability distributionon the allowable pronunciations.
Lucassen \[4\] describes insubstantial detail techniques for constructing decsision treesto model p(7~ \[ ?
); we will limit ourselves to briefly out-lining the technique and also discuss some modifications toLucassen's original procedure.2.1 Dec is ion  Tree  Const ruct ionAssume we have for each letter a collection of data.
Eachdata item consists of the pronunciation f the letter in a partic-ular context, r, and a set of context elements: the five letterspreceeding the current letter in this context the five letters fol-lowing the current letter, and the the five previous pronuncia-tions, We assume we have an inventory of B binary questions(Lucassen \[4\] describes how such a set of questions may bedeveloped).
Each binary question by partitions the letters orpronunciations into two subsets.
Each bj may be applied ateach context element of each data item.
Therefore, for eachcombination of binary question and context element, he datawill be partitioned into two sub-collections; corresponding toeach sub-collection is a probability distribution over the pro-nunciations r. We select he question by and context elementk that minimizes the average ntropy of the two distributionscorresponding to the two sub-collections.
We now repeat heabove procedure for each of the sub-collections individually.The data is thus successively split into smaller and smallercollections until some termination criterion is met; the resultis a tree of binary questions about different context elements.The leaves of the tree consist of probability distributionsp(r)which can be used in equation 2, above.There are various termination criteria that can be used toavoid overtraining trees by growing them too deeply.
Lu-cassen used a combination of seven different termination cri-teria at a node; Breiman \[7\] recommends growing the treequite deeply and then pruning it back to minimize the entropyon held-out data.
We use the following two techniques, whichare quite simple to implement.
First, a threshold is placed on180the product of the number of samples at a node and the entropyofp(r).
Second, the data is divided into two parts, A and B.The data in part .,4 is first used to construct the tree.
When aquestion bi and context position k is selected using data frompart -4 at a particular node in the tree, data from part B isalso propagated to this node and the average cross entropybetween the .,4 and B data is calculated.
If the reduction incross entropy is below a threshold, the node is not expanded,otherwise the B data is split as the ,4 data and used at the nextlevel of tree construction.
This way, spurious questions thatresult from a lack of training data may be eliminated.
After atree using -4 is constructed, the roles of-4 and B are reversedand a second tree is grown.
The predictions from both treesare then averaged to compute p(TZ I ?
).In order to be robust with respect o new data, it is usu-ally important to smooth the distributions at the leaves of thetree.
This is typically done via deleted estimation \[5\]: onetries to maximize the probability of some held-out data withthe model by adjusting a set of smoothing coefficients.
Themost powerful technique is to smooth the distribution at a leafwith a linear combination of the distributions leading up tothe root of the tree from the leaf.
The main disadvantage ofthis scheme is that it requires that each leaf be represented inthe held-out data.
There are various olutions to this problem\[4, 6\] but they require very careful manipulation of the train-ing and held-out data.
Instead, we have developed a simplerecursive smoothing scheme that operates in the followingfashion.
Held-out data is poured down the tree from the root.Each node is assigned a class that is a function of the numberof held-out data samples that appear.
Each node n in the treeis smoothed according to:p'~ = Al(cn)p. + A2(cn)p}cn~ + A3(C.)U (5)wherep' denotes the smoothed distribution,p, the unsmootheddistribution on held-out data, f (n), the father node of n, c(n),the count class of node n, and u, the uniform distribution.
TheAs are computed via deleted estimation \[5\]; the p's may becomputed recursively from the root down for each iterationof deleted estimation.
The main disadvantage of this schemeis that it strongly favors node distributions close to the leaf inquestion for smoothing; for trees with many levels, this maybe disadvantageous.2.2 Data PreparationThe main task in data preparation for decision tree construc-tion lies in determining a series of pronunciations for theletters and in extracting the data items for each letter.
It isassumed that there exists a source of words matched againstphone strings.
Our primary source of such material was anonline version of Webster's 7th Collegiate Dictionary.
Tothis we added the phonetic baseforms from our 20K wordrecognition system, yielding a total of roughly 80K word-phone string pairs.
A substantial mount of time was spent"completing" the entries in Webster's - inflections and variantpronunciations were indicated by a shorthand notation thatwas not trivial to unravel.
In addition, the notation the dictio-nary used to indicate pronunciation was not consistent withthe notation in our recognition system; notation in Webster'shad to be converted to match our 'style' of writing baseforms.For example, Webster's makes many more distinctions be-tween vowels than we do; certain endings are consistentlytranscribed differently; e.g., we typically represent -ment as'm eh0 n t' while in Webster's it is found as 'm uh0 n t',etc..
Converting pronunciation formats to be consistent withlocal conventions i a generic problem in trying to utilize on-line dictionaries and often can take more time to solve that toemploy the dam.Given such a set of data, the next step is to determine aset of pronunciations for each letter.
First, an initial guess ismade and a set of pronunciations i  selected for each letter.A hidden markov model is generated for each letter whoseoutputs represent the allowable pronunciations for the letter.An HMM for the letter d is shown in Figure 1.Figure 1: Hidden Markov Model for Pronunciations of theletter d.There are five permissible pronunciations of d: D as in'dog', T as in 'passed', D EEl  as in 'PhD', D ZH as in'graduate', and ~ as in the second 'd'  in 'ladder'.
The HMMfor a word may be constructed by concatenating the HMMsfor the letters; the models are now trained using the phonestrings as output data.
Once the models are trained, a Viterbialignment is made of all the phone strings against he letterstrings for each word.
A certain umber of the alignments willfail, either because the original pronunciation set was deficientor because there was an error in the original phone string forthe word.
Based on observing the misalignments, one mayaugment the original pronunciation set or correct an incorrectbaseform, and repeat he procedure as many times as mightbe necessary.
One may then simply obtain the data items foreach letter from a combination of the Viterbi alignment andthe pronunciation i ventory; however, we find that there isoften complete arbitrariness the way certain letter strings arealigned against phones; for example, sometimes the phoneT will be aligned against he first t in the letter string "tt"and other times, the second.
To enforce homogeneity, it issometimes useful to post-process the Viterbi alignment.
Thebaseforms are written using a set of 50 phones; we typicallydevelop an inventory of 130 pronunciations.
Typical lettersaverage 5pronunciations; some vowels, such as 'u' ,  have upto 13 different pronunciations.1813 Decoding New BaseformsTo determine the best phonetic baseform using the concept ofpronunciations, we can rewrite quation 1in terms of pronun-ciation strings to find 7~ such that:argmaxp( l .
l  17~)p('E 1?)
(6)In the above equation, p(/,/I 7~) may be identified with theacoustic model component of a standard Hidden MarkovModel based speech recognition system, and p(7~ I z) withthe language model component.
The task of determining aphonetic baseform by combining acoustic information withspelling-to-sound rules is substantially simpler than the typi-cal speech recognition problem.
First, the 'vocabulary' is notlarge, typically 130 pronunciations.
Second, the number of'words' in each 'sentence' is determined by the length of theletter string.
However, the number of language model states,determined by the number of tree leaves, can be quite large(on the order of several thousand) thus making a Viterbi-typedecoder somewhat unwieldy.
Instead, a simple stack decoder\[5\] was implemented to determine the phonetic baseforms.For the n 'h letter of the word in question, we define the scoreza tobez~(l~, r~) a logp(u I r.) +l ogp( r ,  \[ r~_-~, l"+S x ~z~(l~- i, r~ -I )where subscripts and superscripts refer to strings of letters orpronunciations.
The first term on the right just represents hethe acoustic model score in a typical HMM-based system forthe pronunciation of the n th letter, and the second term can becomputed from the speUing-to-soundrules.
Note that z~ maybe computed recursively by proceeding left to right throughthe letters of the word.
Since each letter in the word canhave many different pronunciations, we only examine for then th letter the pronunciation strings associated with the top mvalues of za from the (n - 1) 'h letter; we have observed thatm = 16 or m = 32 is usually satisfactory.
We have also founditnecessary to premultiply logp(u \[ r,) by a factor a between.
1and .5 to obtain better baseforms.
This crudely compensatesfor poor modelling of time correlation i  the Hidden MarkovModels.The above algorithm is only suitable for producing a base-form when there is a single utterance of each word.
Whenthere are multiple utterances, we must modify the algorithmslightly.
For each utterance of a particular word, we use theabove algorithm to produce the top m-scoring pronunciationsrather than a single pronunciation, and separately keep trackof the score due to the acoustic omponent and the spelling-to-sound rules.
We form the union of all pronunciations acrossall utterances for each word, and from those pronunciations,find the pronunciation 7~ to maximizep13 logp(b/i I "R.) + logp('R.
I ?
)i=1where Hi is the i th utterance, and/3, a weighting factor similarto a above, is set to approximately .3.4 Baseform Determination Perfor-manceOur goal was to use the above technique to determine phoneticbaseforms for words not in the current speech recognition sys-tem's vocabulary of 20000 words.
To examine performance,we found the most frequently occurring words not in our20000 vocabulary from a corpus of one million words of re-cent internal electronic mail.
We selected a random subset of500 words from this list, and had two talkers each read the listof words on two different days.
Each word was read four timeseach in a single sentence on both days; e.g., "hello hello hellohello".
Baseforms were produced using the above techniquesunder several conditions; in all cases, the baseforms producedwere used in conjunction with the 20000 baseforms already inthe recognition systems' vocabulary to recognize the secondfour repetitions recorded from each talker.
All recognitiontook place with a language model that predicted each worduniformly.
Talkers recorded our 100 standard training sen-tences \[10\] to train the parameters of the Hidden MarkovModels.
Endpoints of words were located automatically byobtaining a Viterbi alignment of the VQ output against hetop baseform produced by the spelling to sound rules.
Thisintroduced some errors into the results (see below).The conditions examined were(7) 1.
Spelling-to-sound rules alone used to create baseforms.Weight for acoustic omponent (o0 set to zero.2.
Baseforms made from a single utterance.3.
Baseforms made from all four utterances.4.
Baseforms made from a single utterance; spelling-tosound rules score contribution set to zero.5.
Baseforms made from all four utterances; pelling-to-sound rules score contribution set to zero.Note that the fourth and fifth conditions are not  identical tocompletely ignoring the spelling of the word; the number ofletters in the word will determine the length of the pronuncia-tion string (though letters can be given the null pronunciation).Only 31% of the 500 words were covered by the 80000words used to determine the spelling-to-sound rules.
For thatmatter, we found that even a much larger word list, gener-ated by combining a list of 40000 common acronyms withthe 20000 words in our vocabulary and 250000 words in theShoup dictionary \[11\], only covered 60% of these 500 words.Most of the missing words consist of names, acronyms, andspecialized jargon, once again illustrating that straightforwarddictionary lookup procedures are not adequate for addingwords to a vocabulary.Recognition results for each of the two talkers across allfive conditions are shown in Table 2, and compared to resultson handwritten baseforms.
It can be seen that neither the(8) spelling-to-soundrules northe acoustics alone seem to be ad-equate to generate baseforms for recognition, when compared182Word Baseformto handwritten baseforms.
Spelling-to-soundrules whencom-bined with four utterances of the word produce baseformsthat perform quite competitively with handwritten baseforms;spelling-to-sound rules with a single utterance, or acousticsalone with four utterances yield slightly worse recognitionresults.
Note that the level of recognition performance wouldbe substantially higher in actual recognition applications; nat-ural sentences would be dictated and a strong language modelcould be used.Condition T1 T2Handwritten baseforms 29% 32%Spelling-to-soundrules alone 53% 58%Single utterance 33% 36%Four utterances 28% 32%Single utterance, acoustics alone 42% 43%Four utterances, acoustics alone 33% 36%Table 2: Recognition performance as a function of baseformconstruction techniques.Visual inspection of the baseforms generated by spelling-to-sound rules alone indicated that approximately one-thirdof the baseforms had errors in them, where errors are definedas substantial discrepancies between the artificially generatedbaseforms and handwritten ones that had a fair chance ofresulting in recognition errors.
Visual inspection of the base-forms generated by combining spelling-to-sound rules withfour utterances of each word for Talker 1 indicated approxi-mately one-tenth of the baseforms contained some error (asdefined above).
Approximately one-half the errors occurredon names, one-quarter on acronyms, and one-quarter on ac-tual words.
For comparison, the list of 500 words contained60% words or jargon, 35 % names, and 5% acronyms.
Anacronym is considered to be not only strings of letters but com-binations of strings of letters and words; e.g.
'PCSTORE' ispronounced 'pee-see-store'.
One error ('AAA', pronounced'triple-A') resulted from the word's pronunciation having es-sentially nothing to do with its spelling.
Table 3 lists 10 of thesample rrors (chosen at random among the 50).Some of the above errors, as for "dingles", "Gonzales","megapels .... Stefanos" and "cepstrum", never resulted inrecognition errors, in spite of what subjectively seemed tobe substantial errors in the baseforms.
The acronym errorswere traced a flaw in the endpoint detection algorithm.
End-points were detected via Viterbi alignment against baseformsproduced by the spelling-to-sound rules alone; the baseformsare so terrible for acronyms that substantial misalignment oc-curred during endpoint detection, generating faulty input tothe baseform generator.
Very few acronyms were present inthe data used to generate the spelling-to-sound rule trees; in-clusion of such data might have generated better baseforms.Finally, it should be pointed out that both talkers were notalways consistent across recording sessions in their pronun-ciations of words.
For example, the name "Kanevsky", pro-nounced "kq uh0 n ehl v s k ee0", was mispronounced in thefirst recording session as "kq eil n v eel s k all", and the word"cations" was initially pronounced "kq ael ta i l  uh0 n z" anddinglesGonzalesIrwinmegapelsNBSODASStefanosYongBCcepstrumd i l  ngg lzguh0nzaa l l zuh0 er0 w il nm eh 1 g uh0 p pquh01 zCommentMissing uh0before 'T 'Missing eh0before "z"Inserted uh0;er0 should be erlLast uh0 should bebe ehlehl s AcronymAcronym; nobaseform produceds t tq eh I f uh0 n aul should be oulau 1 ug zj aal n n should be ngb k kq Acronymtqsh ehl ppqs  t tq sh should be kqtqr uhl mTable 3: Sample errors in automatic baseform generation.then "k eil sh uh0 n z".
Therefore, even "correct" baseformsnever guarantee success in speech recognition!5 Discussion and ConclusionsInsofar as the automatically generated baseforms produce re-suits at least as good as handwritten ones if the user is willingto say each word multiple times, we view this technique assuccessful.
However, the spelling-to-sound rules obviouslyhave more problems with names and acronyms than actualwords; we attribute this at least in part to the fact that therewere relatively few names and acronyms in the data used toconstruct the spelling-to-sound rule trees.
We feel that in-cluding such information would give the spelling-to-soundrules substantially more power, and are currently working onobtaining data from such sources to improve the quality ofbaseform construction.It is hard to compare the performance ofthis technique withtechniques that try to deduce the baseform from the spellingalone.
Typical figures quoted are between a 3% and 5% errorrate in running text.
Crudely speaking, we have observedthat a fixed 20000 word vocabulary covers about 93% ofcompletely new text across a variety of sources, implying 7%of the words encountered will be new and must be added tothe vocabulary.
From the previous ection, we calculated a10% error rate on such new words in constructing baseforms,which would imply we would generate incorrect baseformsfor less than 1% of words in running text.
It is clear that thishardly impacts the 5% recognition error we obtain in the thelaboratory.In conclusion, we believe we have a viable technique forgenerating good phonetic baseforms for new words in speechrecognition systems.
Work is currently progressing to incor-porate more data in the above algorithms, and to employ thistechnique in other components of the speech recognizer to183supply us with the ability to adapt he phonetic baseforms forwords already in the vocabulary of the recognizer.References[1] L. R. Bahl, P. F. Brown, E V. deSouza, R. L. Mercer,and M. A. Picheny, "Acoustic Markov models used inthe tangora speech recognition system,"[2] L. R. Bahl, R. Bakis, P. V. deSouza, and R. L. Mercer,"Obtaining candidate words by polling in a large vo-cabulary speech recognition system," in Proceedings ofthe IEEE International Conference on Acoustics, Speechand Signal Processing, (New York City), pp.
489--492,April 1988.
[3] D. H. Klatt, "Review of text-to-speech onversion forenglish," Journal of the Acoustical Society of America,vol.
82, pp.
737-793, September 1987.
[4] J. M. Lucassen and R. L. Mercer, "An information-theoretic approach to the automatic determination ofphonemic baseforms," in Proceedings of the IEEE In-ternational Conference on Acoustics, Speech and SignalProcessing, pp.
42.5.1--42.5.4, 1984.
[5] L. R. Bahl, F. Jelinek, and R. L. Mercer, "A maximumlikelihood approach to continuous speech recognition,"IEEE Transactions on Pattern Analysis and MachineIntelligence, vol.
PAMI-5, pp.
179-190, March 1983.
[6] L. R. Bahl, P. F. Brown, P. V. deSouza, and R. L. Mer-cer, "A tree-based statistical language model for natu-ral language speech recognition," IEEE Transactions onAcoustics, Speech and Signal Processing, vol.
AS SP-37,pp.
1001-1008, July 1989.
[7] L. Breiman, J. H. Freidman, R. A. Olshen, and C. J.Stone, Classification and Regression Trees.
California:Wadsworth International Group, 1984.
[8] P. A. Chou, Applications of Information Theory to Pat-tern Recognition and the Design of Decision Trees andTrellises.
PhD thesis, Stanford University, June 1988.
[9] A. Averbuch et al, "An IBM-PC based large-vocabularyisolated-utterance sp ech recgnizer," in Proceedings ofthe IEEE International Conference on Acoustics, Speechand Signal Processing, (Tokyo, Japan), pp.
53-56, April1986.
[10] A. Averbuch et al, "Experiments with the tangora20,000 word speech recognizer," in Proceedings of theIEEE International Conference on Acoustics, Speechand Signal Processing, (Dallas, Texas), pp.
701-704,April 1987.
[11] J.E.
Shoup, "American English orthographic-phonemicdictionary," Air Force Office of Sponsored ResearchReport AD-763 784, Speech Communications ReserachLaboratory, Inc., 1973.184
