Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 363?374,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLanguage Models for Machine Translation:Original vs.
Translated TextsGennadi Lembersky and Noam Ordan and Shuly WintnerDepartment of Computer Science, University of Haifa, 31905 Haifa, Israelglembers@campus.haifa.ac.il, noam.ordan@gmail.com, shuly@cs.haifa.ac.ilAbstractWe investigate the differences betweenlanguage models compiled from originaltarget-language texts and those compiledfrom texts manually translated to the tar-get language.
Corroborating establishedobservations of Translation Studies, wedemonstrate that the latter are signifi-cantly better predictors of translated sen-tences than the former, and hence fit thereference set better.
Furthermore, trans-lated texts yield better language mod-els for statistical machine translation thanoriginal texts.1 IntroductionStatistical machine translation (MT) uses largetarget language models (LMs) to improve thefluency of generated texts, and it is commonlyassumed that for constructing language mod-els, ?more data is better data?
(Brants and Xu,2009).
Not all data, however, are created thesame.
In this work we explore the differences be-tween LMs compiled from texts originally writ-ten in the target language and LMs compiledfrom translated texts.The motivation for our work stems from muchresearch in Translation Studies that suggeststhat original texts are significantly differentfrom translated ones in various aspects (Geller-stam, 1986).
Recently, corpus-based compu-tational analysis corroborated this observation,and Kurokawa et al (2009) apply it to sta-tistical machine translation, showing that foran English-to-French MT system, a transla-tion model trained on an English-translated-to-French parallel corpus is better than one trainedon French-translated-to-English texts.
Our re-search question is whether a language modelcompiled from translated texts may similarlyimprove the results of machine translation.We test this hypothesis on several translationtasks, where the target language is always En-glish.
For each language pair we build two En-glish language models from two types of corpora:texts originally written in English, and humantranslations from the source language into En-glish.
We show that for each language pair, thelatter language model better fits a set of refer-ence translations in terms of perplexity.
We alsodemonstrate that the differences between thetwo LMs are not biased by content but ratherreflect differences on abstract linguistic features.Research in Translation Studies suggests thatall translated texts, irrespective of source lan-guage, share some so-called translation univer-sals.
Consequently, translated texts from sev-eral languages to a single target language resem-ble each other along various axes.
To test thishypothesis, we compile additional English LMs,this time using texts translated to English fromlanguages other than the source.
Again, we useperplexity to assess the fit of these LMs to refer-ence sets of translated-to-English sentences.
Weshow that these LMs depend on the source lan-guage and differ from each other.
Whereas theyoutperform original-based LMs, LMs compiledfrom texts that were translated from the sourcelanguage still fit the reference set best.Finally, we train phrase-based MT systems(Koehn et al, 2003) for each language pair.
Weuse four types of LMs: original; translated from363the source language; translated from other lan-guages; and a mixture of translations from sev-eral languages.
We show that the translated-from-source-language LMs provide a significantimprovement in the quality of the translationoutput over all other LMs, and that the mix-ture LMs always outperform the original LMs.This improvement persists even when the orig-inal LMs are up to ten times larger than thetranslated ones.The main contributions of this work are there-fore a computational corroboration of the hy-potheses that1.
original and translated texts exhibit signif-icant, measurable differences;2.
LMs compiled from translated texts betterfit translated references than LMs compiledfrom original texts of the same (and muchlarger) size (and, to a lesser extent, LMscompiled from texts translated from lan-guages other than the source language); and3.
MT systems that use LMs based on man-ually translated texts significantly outper-form LMs based on originally written texts.It is important to emphasize that translatedtexts abound: Many languages, especially low-resource ones, are more likely to have translatedtexts (religious scripts, educational materials,etc.)
than original ones.
Some numeric dataare listed in Pym and Chrupa la (2005).
Fur-thermore, such data can be automatically identi-fied (see Section 2).
The practical impact of ourwork on MT is therefore potentially dramatic.This paper is organized as follows: Section 2provides background and describes related work.We explain our research methodology and re-sources in Section 3 and detail our experimentsand results in Section 4.
Section 5 discusses theresults and their implications.2 Background and Related WorkNumerous studies suggest that translated textsare different from original ones.
Gellerstam(1986) compares texts written originally inSwedish and texts translated from English intoSwedish.
He notes that the differences betweenthem do not indicate poor translation but rathera statistical phenomenon, which he terms trans-lationese.
He focuses mainly on lexical dif-ferences, for example less colloquialism in thetranslations, or foreign words used in the trans-lations ?with new shades of meaning taken fromthe English lexeme?
(p.91).
Only later studiesconsider grammatical differences (see, e.g., San-tos (1995)).
The features of translationese weretheoretically organized under the terms laws oftranslation and translation universals.Toury (1980, 1995) distinguishes between twolaws: the law of interference and the law ofgrowing standardization.
The former pertainsto the fingerprints of the source text that areleft in the translation product.
The latter per-tains to the effort to standardize the translationproduct according to existing norms in the tar-get language (and culture).
Interestingly, thesetwo laws are in fact reflected in the architectureof statistical machine translation: interferencecorresponds to the translation model and stan-dardization to the language model.The combined effect of these laws creates a hy-brid text that partly corresponds to the sourcetext and partly to texts written originally in thetarget language but in fact belongs to neither(Frawley, 1984).
Baker (1993, 1995, 1996) sug-gests several candidates for translation univer-sals, which are claimed to appear in any trans-lated text, regardless of the source language.These include simplification, the tendency oftranslated texts to simplify the language, themessage or both; and explicitation, their ten-dency to spell out implicit utterances that occurin the source text.Baroni and Bernardini (2006) use machinelearning techniques to distinguish between origi-nal and translated Italian texts, reporting 86.7%accuracy.
They manage to abstract from con-tent and perform the task using only morpho-syntactic cues.
Ilisei et al (2010) perform thesame task for Spanish but enhance it theoreti-cally in order to check the simplification hypoth-esis.
The most informative features are lexicalvariety, sentence length and lexical density.van Halteren (2008) focuses on six languagesfrom Europarl (Koehn, 2005): Dutch, English,French, German, Italian and Spanish.
For each364of these languages, a parallel six-lingual sub-corpus is extracted, including an original textand its translations into the other five languages.The task is to identify the source language oftranslated texts, and the reported results are ex-cellent.
This finding is crucial: as Baker (1996)states, translations do resemble each other; how-ever, in accordance with the law of interference,the study of van Halteren (2008) suggests thattranslation from different source languages con-stitute different sublanguages.
As we show inSection 4.2, LMs based on translations from thesource language outperform LMs compiled fromnon-source translations, in terms of both fitnessto the reference set and improving MT.Kurokawa et al (2009) show that the directionof translation affects the performance of statis-tical MT.
They train systems to translate be-tween French and English (and vice versa) us-ing a French-translated-to-English parallel cor-pus, and then an English-translated-to-Frenchone.
They find that in translating into Frenchit is better to use the latter parallel corpus, andwhen translating into English it is better to usethe former.
Whereas they focus on the trans-lation model, we focus on the language modelin this work.
We show that using a LM trainedon a text translated from the source language ofthe MT system does indeed improve the resultsof the translation.3 Methodology and Resources3.1 HypothesesWe investigate the following three hypotheses:1.
Translated texts differ from original texts;2.
Texts translated from one language differfrom texts translated from other languages;3.
LMs compiled from manually translatedtexts are better for MT as measured usingBLEU than LMs compiled from original texts.We test our hypotheses by considering trans-lations from several languages to English.
Foreach language pair we create a reference set com-prising several thousands of sentences writtenoriginally in the source language and manuallytranslated to English.
Section 3.4 provides de-tails on the reference sets.To investigate the first hypothesis, we traintwo LMs for each language pair, one createdfrom original English texts and the other fromtexts translated into English.
Then, we checkwhich LM better fits the reference set.Fitness of a LM to a set of sentences is mea-sured in terms of perplexity (Jelinek et al, 1977;Bahl et al, 1983).
Given a language model anda test (reference) set, perplexity measures thepredictive power of the language model over thetest set, by looking at the average probabilitythe model assigns to the test data.
Intuitively,a better model assigns higher probablity to thetest data, and consequently has a lower perplex-ity; it is less surprised by the test data.
For-mally, the perplexity PP of a language model Lon a test set W = w1 w2 .
.
.
wN is the probabil-ity of W normalized by the number of words NJurafsky and Martin (2008, page 96):PP(L,W ) = N???
?N?i=11PL(wi|w1 .
.
.
wi?1)(1)For the second hypothesis, we extend the ex-periment to LMs created from texts translatedfrom other languages to English.
For exam-ple, we test how well a LM trained on French-to-English-translated texts fits the German-to-English reference set; and how well a LM trainedon German-to-English-translated texts fits theFrench-to-English reference set.Finally, for the third hypothesis, we use theseLMs for statistical MT (SMT).
For each lan-guage pair we build several SMT systems.
Allsystems use a translation model extracted froma parallel corpus which is oblivious to the direc-tion of the translation; and one of the above-mentioned LMs.
Then, we compare the trans-lation quality of these systems in terms of theBLEU metric (Papineni et al, 2002).3.2 Language ModelsIn all the experiments, we use SRILM (Stolcke,2002) to train 4-gram language models (withthe default backoff model) from various corpora.Our main corpus is Europarl (Koehn, 2005),specifically portions collected over years 1996 to3651999 and 2001 to 2009.
This is a large multi-lingual corpus, containing sentences translatedfrom several European languages.
However, itis organized as a collection of bilingual corporarather than as a single multilingual one, and itis hard to identify sentences that are translatedto several languages.We therefore treat each bilingual sub-corpusin isolation; each such sub-corpus contains sen-tences translated from various languages.
Werely on the language attribute of the speakertag to identify the source language of sentencesin the English part of the corpus.
Since this tagis rarely used with English-language speakers,we also exploit the ID attribute of the speakertag, which we match against the list of Britishmembers of the European parliament.We focus on the following languages: Ger-man (DE), French (FR), Italian (IT), and Dutch(NL).
For each of these languages, L, we con-sider the L-English Europarl sub-corpus.
Ineach sub-corpus, we extract chunks of approx-imately 2.5 million English tokens translatedfrom each of these source languages (T-L), aswell as sentences written originally in English(O-EN).
The mixture corpus (MIX), which isdesigned to represent ?general?
translated lan-guage, is constructed by randomly selecting sen-tences translated from any language (excludingoriginal English sentences).
Table 1 lists thenumber of sentences, number of tokens and av-erage sentence length, for each sub-corpus andeach original language.In addition, we use the Hansard corpus, con-taining transcripts of the Canadian parliamentfrom 1996?20071.
This is a bilingual French?English corpus comprising about 80% originalEnglish texts (EO) and about 20% texts trans-lated from French (FO).
We first separate orig-inal English from the original French and then,for each original language, we randomly extractportions of texts of different sizes: 1M, 5M and10M tokens from the FO corpus and 1M, 5M,10M, 25M, 50M and 100M tokens from the EOcorpus; see Table 2.1We are grateful to Cyril Goutte, George Foster andPierre Isabelle for providing us with an annotated versionof this corpus.German?EnglishOrig.
Lang.
Sent?s Tokens LenMIX 82,700 2,325,261 28.1O-EN 91,100 2,324,745 25.5T-DE 87,900 2,322,973 26.4T-FR 77,550 2,325,183 30.0T-IT 65,199 2,325,996 35.7T-NL 94,000 2,323,646 24.7French?EnglishOrig.
Lang.
Sent?s Tokens LenMIX 90,700 2,546,274 28.1O-EN 99,300 2,545,891 25.6T-DE 94,900 2,546,124 26.8T-FR 85,750 2,546,085 29.7T-IT 72,008 2,546,984 35.4T-NL 103,350 2,545,645 24.6Italian?EnglishOrig.
Lang.
Sent?s Tokens LenMIX 87,040 2,534,793 29.1O-EN 93,520 2,534,892 27.1T-DE 90,550 2,534,867 28.0T-FR 82,930 2,534,930 30.6T-IT 69,270 2,535,225 36.6T-NL 96,850 2,535,053 26.2Dutch?EnglishOrig.
Lang.
Sent?s Tokens LenMIX 90,500 2,508,265 27.7O-EN 97,000 2,475,652 25.5T-DE 94,200 2,503,354 26.6T-FR 86,600 2,523,055 29.1T-IT 73,541 2,518,196 34.2T-NL 101,950 2,513,769 24.7Table 1: Europarl corpus statisticsTo experiment with a non-European language(and a different genre) we choose Hebrew (HE).We use two English corpora: The original (O-EN) corpus comprises articles from the Interna-tional Herald Tribune, downloaded over a pe-riod of seven months (from January to July2009).
The articles cover four topics: news(53.4%), business (20.9%), opinion (17.6%) andarts (8.1%).
The translated (T-HE) corpus con-sists of articles collected from the Israeli news-paper HaAretz over the same period of time.HaAretz is published in Hebrew, but portions of366Original FrenchSize Sent?s Tokens Len1M 54,851 1,000,076 18.235M 276,187 5,009,157 18.1410M 551,867 10,001,716 18.12Original EnglishSize Sent?s Tokens Len1M 54,216 1,006,275 18.565M 268,806 5,006,482 18.6210M 537,574 10,004,191 18.6125M 1,344,580 25,001,555 18.5950M 2,689,332 50,009,861 18.60100M 5,376,886 100,016,704 18.60Table 2: Hansard corpus statisticsit are translated to English.
The O-corpus wasdownsized, so both corpora had approximatelythe same number of tokens in each topic.
Ta-ble 3 lists basic statistics for these corpora.Hebrew?EnglishOrig.
Lang.
Sent?s Tokens LenO-EN 135,228 3,561,559 26.3T-HE 147,227 3,561,556 24.2Table 3: Hebrew-to-English corpus statistics3.3 SMT Training DataTo focus on the effect of the language modelon translation quality, we design SMT train-ing corpora to be oblivious to the direction oftranslation.
Again, we use Europarl (January2000 to September 2000) as the main source ofour parallel corpora.
We also use the Hansardcorpus: We randomy extract 50,000 sentencesfrom the original French sub-corpora and an-other 50,000 sentences from the original Englishsub-corpora.
For Hebrew we use the Hebrew?English parallel corpus (Tsvetkov and Wintner,2010) which contains sentences translated fromHebrew to English (54%) and from English toHebrew (46%).
The English-to-Hebrew partcomprises many short sentences (approximately6 tokens per sentence) taken from a movie sub-title database.
This explains the small token tosentence ratio of this particular corpus.
Table 4lists some details on those corpora.Lang?s Side Sent?s Tokens LenDE-EN DE 92,901 2,439,370 26.3EN 92,901 2,602,376 28.0FR-EN FR 93,162 2,610,551 28.0EN 93,162 2,869,328 30.8IT-EN IT 85,485 2,531,925 29.6EN 85,485 2,517,128 29.5NL-EN NL 84,811 2,327,601 27.4EN 84,811 2,303,846 27.2Hansard FR 100,000 2,167,546 21.7EN 100,000 1,844,415 18.4HE-EN HE 95,912 726,512 7.6EN 95,912 856,830 8.9Table 4: SMT training data details3.4 Reference SetsThe reference sets have two uses.
First, theyare used as the test sets in the experiments thatmeasure the perplexity of the language models.Second, in the MT experiments we use them torandomly extract 1000 sentences for tuning and1000 (different) sentences for evaluation.For each language L we use the L-English sub-corpus of Europarl (over the period of Octoberto December 2000), containing only sentencesoriginally produced in language L. The Hansardreference set is completely disjoint from the LMand SMT training sets and comprises only orig-inal French sentences.
The Hebrew-to-Englishreference set is an independent (disjoint) partof the Hebrew-to-English parallel corpus.
Thisset mostly comprises literary data (88.6%) and asmall portion of news (11.4%).
All sentences areoriginally written in Hebrew and are manuallytranslated to English.
See Table 5.4 Experiments and ResultsWe detail in this section the experiments per-formed to test the three hypotheses: that trans-lated texts can be distinguished from originalones, and provide better language models ofother translated texts; that texts translatedfrom other languages than the source are stillbetter predictors of translations than originaltexts (Section 4.1); and that these differencesare important for SMT (Section 4.2).367Lang?s Side Sent?s Tokens LenDE-EN DE 6,675 161,889 24.3EN 6,675 178,984 26.8FR-EN FR 8,494 260,198 30.6EN 8,494 271,536 32.0IT-EN IT 2,269 82,261 36.3EN 2,269 78,258 34.5NL-EN NL 4,593 114,272 24.9EN 4,593 105,083 22.9Hansard FR 8,926 193,840 21.72EN 8,926 163,448 18.3HE-EN HE 7,546 102,085 13.5EN 7,546 126,183 16.7Table 5: Reference sets4.1 Translated vs.
Original textsWe train several 4-gram LMs for each Europarlsub-corpus, based on the corpora described inSection 3.2.
For each language L, we train aLM based on texts translated from L, from lan-guages other than L as well as texts originallywritten in English.
The LMs are applied to thereference set of texts translated from L, and wecompute the perplexity: the fitness of the LMto the reference set.
Table 6 details the results,where for each sub-corpus and LM we list thenumber of unigrams in the test set, the num-ber of out-of-vocabulary items (OOV) and theperplexity (PP).
The lowest perplexity (reflect-ing the best fit) in each sub-corpus is typeset inboldface, and the highest (worst fit) is slanted.These results overwhelmingly support our hy-pothesis.
For each language L, the perplexityof the LM that was created from L transla-tions is lowest, followed immediately by the MIXLM.
Furthermore, the perplexity of the LM cre-ated from originally-English texts is highest inall experiments.
In addition, the perplexity ofLMs constructed from texts translated from lan-guages other than L always lies between thesetwo extremes: it is a better fit of the refer-ence set than original texts, but not as goodas texts translated from L (or mixture trans-lations).
This corroborates the hypothesis thattranslations form a language in itself, and trans-lations from L1 to L2, form a sub-language,related to yet different from translations fromGerman to English translationsOrig.
Lang.
Unigrams OOV PPMIX 32,238 961 83.45O-EN 31,204 1161 96.50T-DE 27,940 963 77.77T-FR 29,405 1141 92.71T-IT 28,586 1122 95.14T-NL 28,074 1143 89.17French to English translationsOrig.
Lang.
Unigrams OOV PPMIX 33,444 1510 87.13O-EN 32,576 1961 105.93T-DE 28,935 2191 96.83T-FR 30,609 1329 82.23T-IT 29,633 1776 91.15T-NL 29,221 2148 100.18Italian to English translationsOrig.
Lang.
Unigrams OOV PPMIX 33,353 462 90.71O-EN 32,546 633 107.45T-DE 28,835 628 100.46T-FR 30,460 524 92.18T-IT 29,466 470 80.57T-NL 29,130 675 105.07Dutch to English translationsOrig.
Lang.
Unigrams OOV PPMIX 33,050 651 87.37O-EN 32,064 771 100.75T-DE 28,766 778 90.35T-FR 30,502 775 96.38T-IT 29,386 916 99.26T-NL 29,178 560 78.25Table 6: Fitness of various LMs to the reference setother languages to L2.A possible explanation for the different per-plexity results between the LMs could be thespecific contents of the corpora used to com-pile the LMs.
To rule out this possibility andto further emphasize that the corpora are in-deed structurally different, we conduct more ex-periments, in which we gradually abstract awayfrom the domain- and content-specific featuresof the texts and emphasize their syntactic struc-ture.
We focus on German-to-English.First, we remove all punctuation to eliminate368possible bias due to differences in punctuationconventions.
Then, we use the Stanford NamedEntity Recognizer (Finkel et al, 2005) to iden-tify named entities, which we replace with aunique token (?NE?).
Next, we replace all nounswith their POS tag; we use the Stanford POSTagger (Toutanova and Manning, 2000).
Fi-nally, for full lexical abstraction, we replace allwords with their POS tags.At each step, we train six language models onO- and T-texts and apply them to the referenceset (adapted to the same level of abstraction,of course).
As the abstraction of the text in-creases, we also increase the order of the LMs:From 4-grams for text without punctuation andNE abstraction to 5-grams for noun abstractionto 8-grams for full POS abstraction.
The results,which are depicted in Table 7, consistently showthat the T-based LM is a better fit to the ref-erence set, albeit to a lesser extent.
While wedo not show the details here, the same patternis persistent in all the other Europarl languageswe experiment with.We repeat this experiment with the Hebrew-to-English reference set.
We train two 4-gramLMs on the O-EN and T-HE corpora.
We thenapply the two LMs to the reference set and com-pute the perplexity.
The results are presentedin Table 8.
Although the T-based LM has moreOOVs, it is a better fit to the translated textthan the O-based LM: Its perplexity is lowerby 20.1%.
Interestingly, the O-corpus LM hasmore unique unigrams than the T-corpus LM,supporting the claim of Al-Shabab (1996) thattranslated texts have lower type-to-token ratio.We also conduct the above-mentioned ab-straction experiments.
The results, which aredepicted in Table 9, consistently show that theT-based LM is a better fit to the reference set.Clearly, then, translated LMs better fit thereferences than original ones, and the differencescan be traced back not just to (trivial) specificlexical choice, but also to syntactic structure, asevidenced by the POS abstraction experiments.In fact, in order to retain the low perplexity levelof translated texts, a LM based on original textsmust be approximately ten times larger.
We es-tablish this by experimenting with the HansardNo PunctuationOrig.
Lang.
OOVs PP PP diff.MIX 770 109.36 7.58%O-EN 946 127.03 20.43%T-DE 795 101.07 0.00%T-FR 909 122.03 17.18%T-IT 991 125.36 19.38%T-NL 936 117.37 13.89%NE AbstractionOrig.
Lang.
OOVs PP PP diff.MIX 643 99.13 6.99%O-EN 772 114.19 19.26%T-DE 661 92.20 0.00%T-FR 752 110.22 16.35%T-IT 823 112.72 18.21%T-NL 771 105.81 12.86%Noun AbstractionOrig.
Lang.
OOVs PP PP diff.MIX 400 38.48 4.71%O-EN 459 42.06 12.80%T-DE 405 36.67 0.00%T-FR 472 40.96 10.47%T-IT 489 41.39 11.39%T-NL 440 39.54 7.26%POS AbstractionOrig.
Lang.
OOVs PP PP diff.MIX 0 8.02 1.22%O-EN 0 8.19 3.31%T-DE 0 7.92 0.00%T-FR 0 8.10 2.16%T-IT 0 8.12 2.50%T-NL 0 8.03 1.42%Table 7: Fitness of O- vs. T-based LMs to the refer-ence set (DE-EN), different abstraction levelscorpus.
The results are persistent, but are omit-ted for lack of space.4.2 Original vs.
Translated LMs for MTThe last hypothesis we test is whether a bet-ter fitting language model yields a better ma-chine translation system.
In other words, weexpect the T-based LMs to outperform the O-based LMs when used as part of an MT sys-tem.
We construct German-to-English, French-to-English, Italian-to-English and Dutch-to-369Hebrew to English translationsOrig.
Lang.
Unigrams OOV PPO-EN 74,305 2,955 282.75T-HE 61,729 3,253 226.02Table 8: Fitness of O- vs. T-based LMs to the refer-ence set (HE-EN)No PunctuationOrig.
Lang.
OOVs PP PP diff.O-EN 2,601 442.95 19.2%T-HE 2,922 358.11 0.0%NE AbstractionOrig.
Lang.
OOVs PP PP diff.O-EN 1,794 350.3 17.3%T-HE 2,038 289.71 0.0%Noun AbstractionOrig.
Lang.
OOVs PP PP diff.O-EN 679 93.31 12.4%T-HE 802 81.72 0.0%POS AbstractionOrig.
Lang.
OOVs PP PP diff.O-EN 0 11.47 6.2%T-HE 0 10.76 0.0%Table 9: Fitness of O- vs. T-based LMs to the refer-ence set (HE-EN), different abstraction levelsEnglish MT systems using the Moses phrase-based SMT toolkit (Koehn et al, 2007).
Thesystems are trained on the parallel corpora de-scribed in Section 3.3.
We use the reference sets(Section 3.4) as follows: 1,000 sentences are ran-domly extracted for minimum error-rate tuning(Och, 2003), and another set of 1,000 sentencesis randomly used for evaluation.
Each systemis built and tuned with six different LMs: MIX,O-based and four T-based (Section 3.2).
We useBLEU (Papineni et al, 2002) to evaluate trans-lation quality.
The results are listed in Table 10.These results are consistent: the translated-from-source systems outperform all other sys-tems; mixture models come second; and systemsthat use original English LMs always performworst.
We test the statistical significance of dif-ferences between various MT systems using thebootstrap resampling method (Koehn, 2004).
Inall experiments, the best system (translated-from-source LM) is significantly better than allDE to ENLM BLEUMIX 21.95O-EN 21.35T-DE 22.42T-FR 21.47T-IT 21.79T-NL 21.59FR to ENLM BLEUMIX 25.43O-EN 24.85T-DE 25.03T-FR 25.91T-IT 25.44T-NL 25.17IT to ENLM BLEUMIX 26.79O-EN 25.69T-DE 25.86T-FR 26.56T-IT 27.28T-NL 25.77NL to ENLM BLEUMIX 25.17O-EN 24.46T-DE 25.12T-FR 24.79T-IT 24.93T-NL 25.73Table 10: Machine translation with various LMsother systems (p < 0.05); (even more) signifi-cantly better than the O-EN system (p < 0.01);and the mixture systems are significantly betterthan the O-EN systems (p < 0.01).We also construct a Hebrew-to-English MTsystem using Moses?
factored translation model(Koehn and Hoang, 2007).
Every token in thetraining corpus is represented as two factors:surface form and lemma.
Moreover, the Hebrewinput is fully segmented.
The system is builtand tuned with O- and T-based LMs.
Table 11depicts the performance of the systems.
TheT-based LM yields a statistically better BLEUscore than the O-based system.LM BLEU p-valueO-based LM 11.98 0.012T-based LM 12.57Table 11: Hebrew-to-English MT resultsThe LMs used in the above experiments aresmall.
We now want to assess whether the ben-efits of using translated LMs carry over to sce-narios where large original corpora exist.
Webuild yet another set of French-to-English MTsystems.
We use the Hansard SMT transla-tion model and Hansard LMs to train nine MTsystems, three with varying sizes of translatedtexts and six with varying sizes of original texts.370We tune and evaluate on the Hansard referenceset.
In another set of experiments we use theEuroparl French-to-English scenario (using Eu-roparl corpora for the translation model andfor tuning and evaluation), but we use the nineHansard LMs to see whether our findings areconsistent also when LMs are trained on out-of-domain (but similar genre) material.Table 12 shows that the original English LMsshould be enlarged by a factor of ten to achievetranslation quality similar to that of translation-based LMs.
In other words, much smaller trans-lated LMs perform better than much larger orig-inal ones, and this is true for various LM sizes.In-DomainOriginal FrenchSize BLEU1M 34.055M 35.1210M 35.65Original EnglishSize BLEU1M 32.575M 33.3710M 33.9225M 34.7150M 34.85100M 35.36Out-of-DomainOriginal FrenchSize BLEU1M 18.875M 23.9010M 24.36Original EnglishSize BLEU1M 18.685M 23.0210M 23.4525M 23.8250M 23.95100M 24.16Table 12: The effect of LM size on MT performance5 DiscussionWe use language models computed from dif-ferent types of corpora to investigate whethertheir fitness to a reference set of translated-to-English sentences can differentiate betweenthem (and, hence, between the corpora on whichthey are based).
Our main findings are that LMscompiled from manually translated corpora aremuch better predictors of translated texts thanLMs compiled from original-language corpora ofthe same size.
The results are robust, and aresustainable even when the corpora and the refer-ence sentences are abstracted in ways that retaintheir syntactic structure but ignore specific wordmeanings.
Furthermore, we show that trans-lated LMs are better predictors of translatedsentences even when the LMs are compiled fromtexts translated from languages other than thesource language.
However, LMs based on textstranslated from the source language still outper-form LMs translated from other languages.We also show that MT systems based ontranslated-from-source-language LMs outper-form MT systems based on originals LMs orLMs translated from other languages.
Again,these results are robust and the improvementsare statistically significant.
This effect seemsto be amplified as translation quality improves.Furthermore, our results show that original LMsrequire ten times more data to exhibit the samefitness to the reference set and the same trans-lation quality as translated LMs.More generally, this study confirms that in-sights drawn from the field of theoretical trans-lation studies, namely the dual claim accordingto which (1) translations as such differ from orig-inals, and (2) translations from different sourcelanguages differ from each other, can be veri-fied experimentally and contribute to the per-formance of machine translation.Future research is needed in order to un-derstand why this is the case.
One plausi-ble hypothesis is that recurrent multiword ex-pressions in the source language are frequentlysolved by human translations and each of theseexpressions converges to a set of high-qualitytranslation equivalents which are representedin the LM.
Another hypothesis is that sincetranslation-based LMs represent a simplifiedmode of language use, the error potential issmaller.
We therefore expect translation-basedLMs to use more unmarked forms.This work also bears on language typology:we conjecture that LMs compiled from textstranslated not from the original language, butfrom a closely related one, can be better thantexts translated from a more distant language.Some of our results support this hypothesis, butmore research is needed in order to establish it.AcknowledgementsThis research was supported by the Israel Sci-ence Foundation (grant No.
137/06).
We aregrateful to Alon Lavie for his consistent help.371ReferencesOmar S. Al-Shabab.
Interpretation and the lan-guage of translation: creativity and conven-tions in translation.
Janus, Edinburgh, 1996.Lalit R. Bahl, Frederick Jelinek, and Robert L.Mercer.
A maximum likelihood approach tocontinuous speech recognition.
IEEE Trans-actions on Pattern Analysis and Machine In-telligence, 5(2):179?190, 1983.Mona Baker.
Corpus linguistics and transla-tion studies: Implications and applications.
InGill Francis Mona Baker and Elena Tognini-Bonelli, editors, Text and technology: in hon-our of John Sinclair, pages 233?252.
JohnBenjamins, Amsterdam, 1993.Mona Baker.
Corpora in translation studies: Anoverview and some suggestions for future re-search.
Target, 7(2):223?243, September 1995.Mona Baker.
Corpus-based translation studies:The challenges that lie ahead.
In Gill Fran-cis Mona Baker and Elena Tognini-Bonelli,editors, Terminology, LSP and Translation.Studies in language engineering in honour ofJuan C. Sager, pages 175?186.
John Ben-jamins, Amsterdam, 1996.Marco Baroni and Silvia Bernardini.
A newapproach to the study of Translationese:Machine-learning the difference between orig-inal and translated text.
Literary and Lin-guistic Computing, 21(3):259?274, September2006.
URL http://llc.oxfordjournals.org/cgi/content/short/21/3/259?rss=1.Thorsten Brants and Peng Xu.
Distributedlanguage models.
In Proceedings of HumanLanguage Technologies: The 2009 AnnualConference of the North American Chapterof the Association for Computational Lin-guistics, Companion Volume: Tutorial Ab-stracts, pages 3?4, Boulder, Colorado, May2009.
Association for Computational Lin-guistics.
URL http://www.aclweb.org/anthology/N/N09/N09-4002.Jenny Rose Finkel, Trond Grenager, andChristopher Manning.
Incorporating non-local information into information extractionsystems by gibbs sampling.
In ACL ?05: Pro-ceedings of the 43rd Annual Meeting on Asso-ciation for Computational Linguistics, pages363?370, Morristown, NJ, USA, 2005.
Asso-ciation for Computational Linguistics.
doi:http://dx.doi.org/10.3115/1219840.1219885.William Frawley.
Prolegomenon to a theoryof translation.
In William Frawley, editor,Translation.
Literary, Linguistic and Philo-sophical Perspectives, pages 159?175.
Univer-sity of Delaware Press, Newark, 1984.Martin Gellerstam.
Translationese in Swedishnovels translated from English.
In LarsWollin and Hans Lindquist, editors, Trans-lation Studies in Scandinavia, pages 88?95.CWK Gleerup, Lund, 1986.Iustina Ilisei, Diana Inkpen, Gloria CorpasPastor, and Ruslan Mitkov.
Identificationof translationese: A machine learning ap-proach.
In Alexander F. Gelbukh, editor,Proceedings of CICLing-2010: 11th Interna-tional Conference on Computational Linguis-tics and Intelligent Text Processing, volume6008 of Lecture Notes in Computer Science,pages 503?511.
Springer, 2010.
ISBN 978-3-642-12115-9.
URL http://dx.doi.org/10.1007/978-3-642-12116-6.Frederick Jelinek, Robert L. Mercer, Lalit R.Bahl, and J. K. Baker.
Perplexity?a measureof the difficulty of speech recognition tasks.Journal of the Acoustical Society of America,62:S63, November 1977.
Supplement 1.Daniel Jurafsky and James H. Martin.
Speechand Language Processing: An Introduction toNatural Language Processing, ComputationalLinguistics and Speech Recognition.
PrenticeHall, second edition, February 2008.
ISBN013122798X.
URL http://www.worldcat.org/isbn/013122798X.Philipp Koehn.
Statistical significance testsfor machine translation evaluation.
In Pro-ceedings of EMNLP 2004, pages 388?395,Barcelona, Spain, July 2004.
Association forComputational Linguistics.Philipp Koehn.
Europarl: A parallel corpus for372statistical machine translation.
MT Summit,2005.Philipp Koehn and Hieu Hoang.
Factored trans-lation models.
In Proceedings of the 2007Joint Conference on Empirical Methods inNatural Language Processing and Computa-tional Natural Language Learning (EMNLP-CoNLL), pages 868?876, Prague, Czech Re-public, June 2007.
Association for Computa-tional Linguistics.
URL http://www.aclweb.org/anthology/D/D07/D07-1091.Philipp Koehn, Franz Josef Och, and DanielMarcu.
Statistical phrase-based translation.In NAACL ?03: Proceedings of the 2003 Con-ference of the North American Chapter ofthe Association for Computational Linguisticson Human Language Technology, pages 48?54.
Association for Computational Linguis-tics, 2003.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen,Christine Moran, Richard Zens, Chris Dyer,Ondrej Bojar, Alexandra Constantin, andEvan Herbst.
Moses: Open source toolkit forstatistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics Com-panion Volume Proceedings of the Demoand Poster Sessions, pages 177?180, Prague,Czech Republic, June 2007.
Association forComputational Linguistics.
URL http://www.aclweb.org/anthology/P07-2045.David Kurokawa, Cyril Goutte, and Pierre Is-abelle.
Automatic detection of translated textand its impact on machine translation.
In Pro-ceedings of MT-Summit XII, 2009.Franz Josef Och.
Minimum error rate train-ing in statistical machine translation.
In ACL?03: Proceedings of the 41st Annual Meetingon Association for Computational Linguis-tics, pages 160?167, Morristown, NJ, USA,2003.
Association for Computational Linguis-tics.
doi: http://dx.doi.org/10.3115/1075096.1075117.Kishore Papineni, Salim Roukos, Todd Ward,and Wei-Jing Zhu.
BLEU: a method for auto-matic evaluation of machine translation.
InACL ?02: Proceedings of the 40th AnnualMeeting on Association for ComputationalLinguistics, pages 311?318, Morristown, NJ,USA, 2002.
Association for ComputationalLinguistics.
doi: http://dx.doi.org/10.3115/1073083.1073135.Anthony Pym and Grzegorz Chrupa la.
Thequantitative analysis of translation flows inthe age of an international language.
In Al-bert Branchadell and Lovell M. West, editors,Less Translated Languages, pages 27?38.
JohnBenjamins, Amsterdam, 2005.Diana Santos.
On grammatical translationese.In In Koskenniemi, Kimmo (comp.
), Shortpapers presented at the Tenth Scandina-vian Conference on Computational Linguis-tics (Helsinki, pages 29?30, 1995.Andreas Stolcke.
SRILM?an extensible lan-guage modeling toolkit.
In Procedings ofInternational Conference on Spoken Lan-guage Processing, pages 901?904, 2002.
URLciteseer.ist.psu.edu/stolcke02srilm.html.Gideon Toury.
In Search of a Theory of Trans-lation.
The Porter Institute for Poetics andSemiotics, Tel Aviv University, Tel Aviv,1980.Gideon Toury.
Descriptive Translation Studiesand beyond.
John Benjamins, Amsterdam /Philadelphia, 1995.Kristina Toutanova and Christopher D. Man-ning.
Enriching the knowledge sources usedin a maximum entropy part-of-speech tag-ger.
In Proceedings of the 2000 Joint SIGDATconference on Empirical methods in naturallanguage processing and very large corpora,pages 63?70, Morristown, NJ, USA, 2000.
As-sociation for Computational Linguistics.
doi:http://dx.doi.org/10.3115/1117794.1117802.Yulia Tsvetkov and Shuly Wintner.
Automaticacquisition of parallel corpora from websiteswith dynamic content.
In Proceedings ofthe Seventh conference on International Lan-guage Resources and Evaluation (LREC?10),373pages 3389?3392.
European Language Re-sources Association (ELRA), May 2010.
ISBN2-9517408-6-7.Hans van Halteren.
Source language markers inEUROPARL translations.
In COLING ?08:Proceedings of the 22nd International Con-ference on Computational Linguistics, pages937?944, Morristown, NJ, USA, 2008.
Asso-ciation for Computational Linguistics.
ISBN978-1-905593-44-6.374
