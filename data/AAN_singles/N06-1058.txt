Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 455?462,New York, June 2006. c?2006 Association for Computational LinguisticsParaphrasing for Automatic EvaluationDavid KauchakDepartment of Computer ScienceUniversity of California, San Diegodkauchak@cs.ucsd.eduRegina BarzilayCSAILMassachusetts Institute of Technologyregina@csail.mit.eduAbstractThis paper studies the impact of para-phrases on the accuracy of automatic eval-uation.
Given a reference sentence and amachine-generated sentence, we seek tofind a paraphrase of the reference sen-tence that is closer in wording to the ma-chine output than the original reference.We apply our paraphrasing method in thecontext of machine translation evaluation.Our experiments show that the use ofa paraphrased synthetic reference refinesthe accuracy of automatic evaluation.
Wealso found a strong connection betweenthe quality of automatic paraphrases asjudged by humans and their contributionto automatic evaluation.1 IntroductionThe use of automatic methods for evaluatingmachine-generated text is quickly becoming main-stream in natural language processing.
The mostnotable examples in this category include measuressuch as BLEU and ROUGE which drive researchin the machine translation and text summarizationcommunities.
These methods assess the quality ofa machine-generated output by considering its simi-larity to a reference text written by a human.
Ideally,the similarity would reflect the semantic proximitybetween the two.
In practice, this comparison breaksdown to n-gram overlap between the reference andthe machine output.1a.
However, Israel?s reply failed to completelyclear the U.S. suspicions.1b.
However, Israeli answer unable to fullyremove the doubts.Table 1: A reference sentence and correspondingmachine translation from the NIST 2004 MT eval-uation.Consider the human-written translation and themachine translation of the same Chinese sentenceshown in Table 1.
While the two translations con-vey the same meaning, they share only auxiliarywords.
Clearly, any measure based on word over-lap will penalize a system for generating such a sen-tence.
The question is whether such cases are com-mon phenomena or infrequent exceptions.
Empiri-cal evidence supports the former.
Analyzing 10,728reference translation pairs1 used in the NIST 2004machine translation evaluation, we found that only21 (less than 0.2%) of them are identical.
Moreover,60% of the pairs differ in at least 11 words.
Thesestatistics suggest that without accounting for para-phrases, automatic evaluation measures may neverreach the accuracy of human evaluation.As a solution to this problem, researchers usemultiple references to refine automatic evaluation.Papineni et al (2002) shows that expanding thenumber of references reduces the gap between au-tomatic and human evaluation.
However, very fewhuman annotated sets are augmented with multiplereferences and those that are available are relatively1Each pair included different translations of the same sen-tence, produced by two human translators.455small in size.
Moreover, access to several referencesdoes not guarantee that the references will includethe same words that appear in machine-generatedsentences.In this paper, we explore the use of paraphras-ing methods for refinement of automatic evalua-tion techniques.
Given a reference sentence and amachine-generated sentence, we seek to find a para-phrase of the reference sentence that is closer inwording to the machine output than the original ref-erence.
For instance, given the pair of sentences inTable 1, we automatically transform the referencesentence (1a.)
intoHowever, Israel?s answer failed to com-pletely remove the U.S. suspicions.Thus, among many possible paraphrases of thereference, we are interested only in those that usewords appearing in the system output.
Our para-phrasing algorithm is based on the substitute in con-text strategy.
First, the algorithm identifies pairs ofwords from the reference and the system output thatcould potentially form paraphrases.
We select thesecandidates using existing lexico-semantic resourcessuch as WordNet.
Next, the algorithm tests whetherthe candidate paraphrase is admissible in the con-text of the reference sentence.
Since even synonymscannot be substituted in any context (Edmonds andHirst, 2002), this filtering step is necessary.
We pre-dict whether a word is appropriate in a new contextby analyzing its distributional properties in a largebody of text.
Finally, paraphrases that pass the filter-ing stage are used to rewrite the reference sentence.We apply our paraphrasing method in the contextof machine translation evaluation.
Using this strat-egy, we generate a new sentence for every pair ofhuman and machine translated sentences.
This syn-thetic reference then replaces the original human ref-erence in automatic evaluation.The key findings of our work are as follows:(1) Automatically generated paraphrases im-prove the accuracy of the automatic evaluationmethods.
Our experiments show that evaluationbased on paraphrased references gives a better ap-proximation of human judgments than evaluationthat uses original references.
(2) The quality of automatic paraphrases de-termines their contribution to automatic evalua-tion.
By analyzing several paraphrasing resources,we found that the accuracy and coverage of a para-phrasing method correlate with its utility for auto-matic MT evaluation.Our results suggest that researchers may find ituseful to augment standard measures such as BLEUand ROUGE with paraphrasing information therebytaking more semantic knowledge into account.In the following section, we provide an overviewof existing work on automatic paraphrasing.
Wethen describe our paraphrasing algorithm and ex-plain how it can be used in an automatic evaluationsetting.
Next, we present our experimental frame-work and data and conclude by presenting and dis-cussing our results.2 Related WorkAutomatic Paraphrasing and Entailment Ourwork is closely related to research in automatic para-phrasing, in particular, to sentence level paraphras-ing (Barzilay and Lee, 2003; Pang et al, 2003; Quirket al, 2004).
Most of these approaches learn para-phrases from a parallel or comparable monolingualcorpora.
Instances of such corpora include multipleEnglish translations of the same source text writ-ten in a foreign language, and different news arti-cles about the same event.
For example, Pang etal.
(2003) expand a set of reference translations us-ing syntactic alignment, and generate new referencesentences that could be used in automatic evaluation.Our approach differs from traditional work on au-tomatic paraphrasing in goal and methodology.
Un-like previous approaches, we are not aiming to pro-duce any paraphrase of a given sentence since para-phrases induced from a parallel corpus do not nec-essarily produce a rewriting that makes a referencecloser to the system output.
Thus, we focus onwords that appear in the system output and aim todetermine whether they can be used to rewrite a ref-erence sentence.Our work also has interesting connections withresearch on automatic textual entailment (Dagan etal., 2005), where the goal is to determine whethera given sentence can be inferred from text.
Whilewe are not assessing an inference relation betweena reference and a system output, the two tasksface similar challenges.
Methods for entailment456recognition extensively rely on lexico-semantic re-sources (Haghighi et al, 2005; Harabagiu et al,2001), and we believe that our method for contex-tual substitution can be beneficial in that context.Automatic Evaluation Measures A variety of au-tomatic evaluation methods have been recently pro-posed in the machine translation community (NIST,2002; Melamed et al, 2003; Papineni et al, 2002).All these metrics compute n-gram overlap betweena reference and a system output, but measure theoverlap in different ways.
Our method for referenceparaphrasing can be combined with any of thesemetrics.
In this paper, we report experiments withBLEU due to its wide use in the machine translationcommunity.Recently, researchers have explored additionalknowledge sources that could enhance automaticevaluation.
Examples of such knowledge sources in-clude stemming and TF-IDF weighting (Babych andHartley, 2004; Banerjee and Lavie, 2005).
Our workcomplements these approaches: we focus on the im-pact of paraphrases, and study their contribution tothe accuracy of automatic evaluation.3 MethodsThe input to our method consists of a reference sen-tence R = r1 .
.
.
rm and a system-generated sen-tence W = w1 .
.
.
wp whose words form the sets Rand W respectively.
The output of the model is asynthetic reference sentence SRW that preserves themeaning of R and has maximal word overlap withW .
We generate such a sentence by substitutingwords from R with contextually equivalent wordsfrom W .Our algorithm first selects pairs of candidate wordparaphrases, and then checks the likelihood of theirsubstitution in the context of the reference sentence.Candidate Selection We assume that words fromthe reference sentence that already occur in the sys-tem generated sentence should not be consideredfor substitution.
Therefore, we focus on unmatchedpairs of the form {(r, w)|r ?
R?W, w ?
W?R}.From this pool, we select candidate pairs whosemembers exhibit high semantic proximity.
In ourexperiments we compute semantic similarity us-ing WordNet, a large-scale lexico-semantic resourceemployed in many NLP applications for similar pur-2a.
It is hard to believe that such tremendouschanges have taken place for those people andlands that I have never stopped missing whileliving abroad.2b.
For someone born here but has beensentimentally attached to a foreign countryfar from home, it is difficult to believethis kind of changes.Table 2: A reference sentence and a correspondingmachine translation.
Candidate paraphrases are inbold.poses.
We consider a pair as a substitution candidateif its members are synonyms in WordNet.Applying this step to the two sentences in Table 2,we obtain two candidate pairs (home, place) and(difficult, hard).Contextual Substitution The next step is to de-termine for each candidate pair (ri, wj) whetherwj is a valid substitution for ri in the context ofr1 .
.
.
ri?12ri+1 .
.
.
rm.
This filtering step is essen-tial because synonyms are not universally substi-tutable2 .
Consider the candidate pair (home, place)from our example (see Table 2).
Words home andplace are paraphrases in the sense of ?habitat?, butin the reference sentence ?place?
occurs in a differ-ent sense, being part of the collocation ?take place?.In this case, the pair (home, place) cannot be usedto rewrite the reference sentence.We formulate contextual substitution as abinary classification task: given a contextr1 .
.
.
ri?12ri+1 .
.
.
rm, we aim to predict whetherwj can occur in this context at position i. Foreach candidate word wj we train a classifier thatmodels contextual preferences of wj .
To train sucha classifier, we collect a large corpus of sentencesthat contain the word wj and an equal number ofrandomly extracted sentences that do not containthis word.
The former category forms positiveinstances, while the latter represents the negative.For the negative examples, a random position ina sentence is selected for extracting the context.This corpus is acquired automatically, and does notrequire any manual annotations.2This can explain why previous attempts to use WordNet forgenerating sentence-level paraphrases (Barzilay and Lee, 2003;Quirk et al, 2004) were unsuccessful.457We represent context by n-grams and local col-locations, features typically used in supervisedword sense disambiguation.
Both n-grams andcollocations exclude the word wj .
An n-gramis a sequence of n adjacent words appearing inr1 .
.
.
ri?12ri+1 .
.
.
rm.
A local collocation alsotakes into account the position of an n-gram withrespect to the target word.
To compute local colloca-tions for a word at position i, we extract all n-grams(n = 1 .
.
.
4) beginning at position i ?
2 and endingat position i + 2.
To make these position dependent,we prepend each of them with the length and startingposition.Once the classifier3 for wj is trained, we ap-ply it to the context r1 .
.
.
ri?12ri+1 .
.
.
rm.
Forpositive predictions, we rewrite the string asr1 .
.
.
ri?1wjri+1 .
.
.
rm.
In this formulation, allsubstitutions are tested independently.For the example from Table 2, only the pair(difficult, hard) passes this filter, and thus the sys-tem produces the following synthetic reference:For someone born here but has been senti-mentally attached to a foreign country farfrom home, it is hard to believe this kindof changes.The synthetic reference keeps the meaning of theoriginal reference, but has a higher word overlapwith the system output.One of the implications of this design is the needto develop a large number of classifiers to test con-textual substitutions.
For each word to be insertedinto a reference sentence, we need to train a sepa-rate classifier.
In practice, this requirement is not asignificant burden.
The training is done off-line andonly once, and testing for contextual substitution isinstantaneous.
Moreover, the first filtering step ef-fectively reduces the number of potential candidates.For example, to apply this approach to the 71,520sentence pairs from the MT evaluation set (describedin Section 4.1.2), we had to train 2,380 classifiers.We also discovered that the key to the success ofthis approach is the size of the corpus used for train-ing contextual classifiers.
We derived training cor-pora from the English Gigaword corpus, and the av-erage size of a corpus for one classifier is 255,0003In our experiments, we used the publicly available BoosT-exter classifier (Schapire and Singer, 2000) for this task.sentences.
We do not attempt to substitute any wordsthat have less that 10,000 appearances in the Giga-word corpus.4 ExperimentsOur primary goal is to investigate the impact ofmachine-generated paraphrases on the accuracy ofautomatic evaluation.
We focus on automatic evalu-ation of machine translation due to the availability ofhuman annotated data in that domain.
The hypoth-esis is that by using a synthetic reference transla-tion, automatic measures approximate better humanevaluation.
In section 4.2, we test this hypothesisby comparing the performance of BLEU scores withand without synthetic references.Our secondary goal is to study the relationshipbetween the quality of paraphrases and their con-tribution to the performance of automatic machinetranslation evaluation.
In section 4.3, we present amanual evaluation of several paraphrasing methodsand show a close connection between intrinsic andextrinsic assessments of these methods.4.1 Experimental Set-UpWe begin by describing relevant background infor-mation, including the BLEU evaluation method, thetest data set, and the alternative paraphrasing meth-ods considered in our experiments.4.1.1 BLEUBLEU is the basic evaluation measure that we usein our experiments.
It is the geometric average ofthe n-gram precisions of candidate sentences withrespect to the corresponding reference sentences,times a brevity penalty.
The BLEU score is com-puted as follows:BLEU = BP ?
4???
?4?n=1pnBP = min(1, e1?r/c),where pn is the n-gram precision, c is the cardinalityof the set of candidate sentences and r is the size ofthe smallest set of reference sentences.To augment BLEU evaluation with paraphrasinginformation, we substitute each reference with thecorresponding synthetic reference.4584.1.2 DataWe use the Chinese portion of the 2004 NISTMT dataset.
This portion contains 200 Chinese doc-uments, subdivided into a total of 1788 segments.Each segment is translated by ten machine transla-tion systems and by four human translators.
A quar-ter of the machine-translated segments are scored byhuman evaluators on a one-to-five scale along twodimensions: adequacy and fluency.
We use only ad-equacy scores, which measure how well content ispreserved in the translation.4.1.3 Alternative Paraphrasing TechniquesTo investigate the effect of paraphrase quality onautomatic evaluation, we consider two alternativeparaphrasing resources: Latent Semantic Analysis(LSA), and Brown clustering (Brown et al, 1992).These techniques are widely used in NLP applica-tions, including language modeling, information ex-traction, and dialogue processing (Haghighi et al,2005; Serafin and Eugenio, 2004; Miller et al,2004).
Both techniques are based on distributionalsimilarity.
The Brown clustering is computed byconsidering mutual information between adjacentwords.
LSA is a dimensionality reduction techniquethat projects a word co-occurrence matrix to lowerdimensions.
This lower dimensional representationis then used with standard similarity measures tocluster the data.
Two words are considered to be aparaphrase pair if they appear in the same cluster.We construct 1000 clusters employing the Brownmethod on 112 million words from the North Amer-ican New York Times corpus.
We keep the top 20most frequent words for each cluster as paraphrases.To generate LSA paraphrases, we used the Infomapsoftware4 on a 34 million word collection of arti-cles from the American News Text corpus.
We usedthe default parameter settings: a 20,000 word vocab-ulary, the 1000 most frequent words (minus a stop-list) for features, a 15 word context window on eitherside of a word, a 100 feature reduced representation,and the 20 most similar words as paraphrases.While we experimented with several parametersettings for LSA and Brown methods, we do notclaim that the selected settings are necessarily opti-mal.
However, these methods present sensible com-4http://infomap-nlp.sourceforge.netMethod 1 reference 2 referencesBLEU 0.9657 0.9743WordNet 0.9674 0.9763ContextWN 0.9677 0.9764LSA 0.9652 0.9736Brown 0.9662 0.9744Table 4: Pearson adequacy correlation scores forrewriting using one and two references, averagedover ten runs.Method vs. BLEU vs. ContextWNWordNet // 44ContextWN // -LSA X 44Brown // 4Table 5: Paired t-test significance for all methodscompared to BLEU as well as our method for onereference.
Two triangles indicates significant at the99% confidence level, one triangle at the 95% con-fidence level and X not significant.
Triangles pointtowards the better method.parison points for understanding the relationship be-tween paraphrase quality and its impact on auto-matic evaluation.Table 3 shows synthetic references produced bythe different paraphrasing methods.4.2 Impact of Paraphrases on MachineTranslation EvaluationThe standard way to analyze the performance of anevaluation metric in machine translation is to com-pute the Pearson correlation between the automaticmetric and human scores (Papineni et al, 2002;Koehn, 2004; Lin and Och, 2004; Stent et al, 2005).Pearson correlation estimates how linearly depen-dent two sets of values are.
The Pearson correlationvalues range from 1, when the scores are perfectlylinearly correlated, to -1, in the case of inversely cor-related scores.To calculate the Pearson correlation, we createa document by concatenating 300 segments.
Thisstrategy is commonly used in MT evaluation, be-cause of BLEU?s well-known problems with docu-ments of small size (Papineni et al, 2002; Koehn,2004).
For each of the ten MT system translations,459Reference: The monthly magazine ?Choices?
has won the deep trust of the residents.
The currentInternet edition of ?Choices?
will give full play to its functions and will helpconsumers get quick access to market information.System: The public has a lot of faith in the ?Choice?
monthly magazine and the Council is nowworking on a web version.
This will enhance the magazine?s function and help consumerto acquire more up-to-date market information.WordNet The monthly magazine ?Choices?
has won the deep faith of the residents.
The currentInternet version of ?Choices?
will give full play to its functions and will helpconsumers acquire quick access to market information.ContextWN The monthly magazine ?Choices?
has won the deep trust of the residents.
The currentInternet version of ?Choices?
will give full play to its functions and will helpconsumers acquire quick access to market information.LSA The monthly magazine ?Choice?
has won the deep trust of the residents.
The currentweb edition of ?Choice?
will give full play to its functions and will helpconsumer get quick access to market information.Brown The monthly magazine ?Choices?
has won the deep trust of the residents.
The currentInternet version of ?Choices?
will give full play to its functions and will helpconsumers get quick access to market information.Table 3: Sample of paraphrasings produced by each method based on the corresponding system translation.Paraphrased words are in bold and filtered words underlined.the evaluation metric score is calculated on the docu-ment and the corresponding human adequacy scoreis calculated as the average human score over thesegments.
The Pearson correlation is calculated overthese ten pairs (Papineni et al, 2002; Stent et al,2005).
This process is repeated for ten differentdocuments created by the same process.
Finally, apaired t-test is calculated over these ten different cor-relation scores to compute statistical significance.Table 4 shows Pearson correlation scores forBLEU and the four paraphrased augmentations,averaged over ten runs.5 In all ten tests, ourmethod based on contextual rewriting (ContextWN)improves the correlation with human scores overBLEU.
Moreover, in nine out of ten tests Contex-tWN outperforms the method based on WordNet.The results of statistical significance testing are sum-marized in Table 5.
All the paraphrasing methodsexcept LSA, exhibit higher correlation with humanscores than plain BLEU.
Our method significantlyoutperforms BLEU, and all the other paraphrase-based metrics.
This consistent improvement con-firms the importance of contextual filtering.5Depending on the experimental setup, correlation valuescan vary widely.
Our scores fall within the range of previousresearchers (Papineni et al, 2002; Lin and Och, 2004).The third column in Table 4 shows that auto-matic paraphrasing continues to improve correlationscores even when two human references are para-phrased using our method.4.3 Evaluation of Paraphrase QualityIn the last section, we saw significant variationsin MT evaluation performance when different para-phrasing methods were used to generate a syntheticreference.
In this section, we examine the correla-tion between the quality of automatically generatedparaphrases and their contribution to automatic eval-uation.
We analyze how the substitution frequencyand the accuracy of those substitutions contributesto a method?s performance.We compute the substitution frequency of an au-tomatic paraphrasing method by counting the num-ber of words it rewrites in a set of reference sen-tences.
Table 6 shows the substitution frequency andthe corresponding BLEU score.
The substitutionfrequency varies greatly across different methods ?LSA is by far the most prolific rewriter, while Brownproduces very few substitutions.
As expected, themore paraphrases identified, the higher the BLEUscore for the method.
However, this increase does460Method Score SubstitutionsBLEU 0.0913 -WordNet 0.0969 994ContextWN 0.0962 742LSA 0.992 2080Brown 0.921 117Table 6: Scores and the number of substitutionsmade for all 1788 segments, averaged over the dif-ferent MT system translationsMethod Judge 1 Judge 2 Kappaaccuracy accuracyWordNet 63.5% 62.5% 0.74ContextWN 75% 76.0% 0.69LSA 30% 31.5% 0.73Brown 56% 56% 0.72Table 7: Accuracy scores by two human judges aswell as the Kappa coefficient of agreement.not translate into better evaluation performance.
Forinstance, our contextual filtering method removesapproximately a quarter of the paraphrases sug-gested by WordNet and yields a better evaluationmeasure.
These results suggest that the substitu-tion frequency cannot predict the utility value of theparaphrasing method.Accuracy measures the correctness of the pro-posed substitutions in the context of a reference sen-tence.
To evaluate the accuracy of different para-phrasing methods, we randomly extracted 200 para-phrasing examples from each method.
A paraphraseexample consists of a reference sentence, a refer-ence word to be paraphrased and a proposed para-phrase of that reference (that actually occurred in acorresponding system translation).
The judge wasinstructed to mark a substitution as correct only ifthe substitution was both semantically and grammat-ically correct in the context of the original referencesentence.Paraphrases produced by the four methods werejudged by two native English speakers.
The pairswere presented in random order, and the judges werenot told which system produced a given pair.
Weemploy a commonly used measure, Kappa, to as-sess agreement between the judges.
We found thatnegative positivefiltered 40 27non-filtered 33 100Table 8: Confusion matrix for the context filteringmethod on a random sample of 200 examples la-beled by the first judge.on all the four sets the Kappa value was around 0.7,which corresponds to substantial agreement (Landisand Koch, 1977).As Table 7 shows, the ranking between the ac-curacy of the different paraphrasing methods mir-rors the ranking of the corresponding MT evalua-tion methods shown in Table 4.
The paraphrasingmethod with the highest accuracy, ContextWN, con-tributes most significantly to the evaluation perfor-mance of BLEU.
Interestingly, even methods withmoderate accuracy, i.e.
63% for WordNet, have apositive influence on the BLEU metric.
At the sametime, poor paraphrasing accuracy, such as LSA with30%, does hurt the performance of automatic evalu-ation.To further understand the contribution of contex-tual filtering, we compare the substitutions made byWordNet and ContextWN on the same set of sen-tences.
Among the 200 paraphrases proposed byWordNet, 73 (36.5%) were identified as incorrect byhuman judges.
As the confusion matrix in Table 8shows, 40 (54.5%) were eliminated during the filter-ing step.
At the same time, the filtering erroneouslyeliminates 27 positive examples (21%).
Even at thislevel of false negatives, the filtering has an overallpositive effect.5 Conclusion and Future WorkThis paper presents a comprehensive study of theimpact of paraphrases on the accuracy of automaticevaluation.
We found a strong connection betweenthe quality of automatic paraphrases as judged byhumans and their contribution to automatic evalua-tion.
These results have two important implications:(1) refining standard measures such as BLEU withparaphrase information moves the automatic evalu-ation closer to human evaluation and (2) applyingparaphrases to MT evaluation provides a task-basedassessment for paraphrasing accuracy.461We also introduce a novel paraphrasing methodbased on contextual substitution.
By posing theparaphrasing problem as a discriminative task, wecan incorporate a wide range of features that im-prove the paraphrasing accuracy.
Our experimentsshow improvement of the accuracy of WordNetparaphrasing and we believe that this method cansimilarly benefit other approaches that use lexico-semantic resources to obtain paraphrases.Our ultimate goal is to develop a contextual filter-ing method that does not require candidate selectionbased on a lexico-semantic resource.
One source ofpossible improvement lies in exploring more power-ful learning frameworks and more sophisticated lin-guistic representations.
Incorporating syntactic de-pendencies and class-based features into the contextrepresentation could also increase the accuracy andthe coverage of the method.
Our current methodonly implements rewriting at the word level.
In thefuture, we would like to incorporate substitutions atthe level of phrases and syntactic trees.AcknowledgmentsThe authors acknowledge the support of the Na-tional Science Foundation (Barzilay; CAREERgrant IIS-0448168) and DARPA (Kauchak; grantHR0011-06-C-0023).
Thanks to Michael Collins,Charles Elkan, Yoong Keok Lee, Philip Koehn, IgorMalioutov, Ben Snyder and the anonymous review-ers for helpful comments and suggestions.
Anyopinions, findings and conclusions expressed in thismaterial are those of the author(s) and do not neces-sarily reflect the views of DARPA or NSF.ReferencesB.
Babych, A. Hartley.
2004.
Extending the BLEUevaluation method with frequency weightings.
In Pro-ceedings of the ACL, 621?628.S.
Banerjee, A. Lavie.
2005.
METEOR: An automaticmetric for MT evaluation with improved correlationwith human judgments.
In Proceedings of the ACLWorkshop on Intrinsic and Extrinsic Evaluation Mea-sures for MT and/or Summarization, 65?72.R.
Barzilay, L. Lee.
2003.
Learning to paraphrase: Anunsupervised approach using multiple-sequence align-ment.
In Proceedings of NAACL-HLT, 16?23.P.
F. Brown, P. V. deSouza, R. L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18:467?479.I.
Dagan, O. Glickman, B. Magnini, eds.
2005.
The PAS-CAL recognizing textual entailment challenge, 2005.P.
Edmonds, G. Hirst.
2002.
Near synonymy and lexicalchoice.
Computational Linguistics, 28(2):105?144.A.
Haghighi, A. Ng, C. Manning.
2005.
Robust tex-tual inference via graph matching.
In Proceedings ofNAACL-HLT, 387?394.S.
Harabagiu, D. Moldovan, M. Pasca, R. Mihal-cea, M. Surdeanu, R. Bunescu, R. Girju, V. Rus,P.
Morarescu.
2001.
The role of lexico-semantic feed-back in open-domain textual question-answering.
InProceedings of ACL, 274?291.P.
Koehn.
2004.
Statistical significance tests for machinetranslation evaluation.
In Proceedings of EMNLP,388?395.J.
R. Landis, G. G. Koch.
1977.
The measurement ofobserver agreement for categorical data.
Biometrics,33:159?174.C.
Lin, F. Och.
2004.
ORANGE: a method for evaluatingautomatic evaluation metrics for machine translation.In Proceedings of COLING, 501?507.I.
D. Melamed, R. Green, J. P. Turian.
2003.
Precisionand recall of machine translation.
In Proceedings ofNAACL-HLT, 61?63.S.
Miller, J. Guinness, A. Zamanian.
2004.
Name tag-ging with word clusters and discriminative training.
InProceedings of HLT-NAACL, 337?342.NIST.
2002.
Automatic evaluation of machine trans-lation quality using n-gram co-occurrence statistics,2002.B.
Pang, K. Knight, D. Marcu.
2003.
Syntax-basedalignment of multiple translations: Extracting para-phrases and generating new sentences.
In Proceedingsof NAACL-HLT, 102?209.K.
Papineni, S. Roukos, T. Ward, W. Zhu.
2002.
BLEU:a method for automatic evaluation of machine transla-tion.
In Proceedings of the ACL, 311?318.C.
Quirk, C. Brockett, W. Dolan.
2004.
Monolingualmachine translation for paraphrase generation.
In Pro-ceedings of EMNLP, 142?149.R.
E. Schapire, Y.
Singer.
2000.
Boostexter: A boosting-based system for text categorization.
Machine Learn-ing, 39(2/3):135?168.R.
Serafin, B. D. Eugenio.
2004.
FLSA: Extending la-tent semantic analysis with features for dialogue actclassification.
In Proceedings of the ACL, 692?699.A.
Stent, M. Marge, M. Singhai.
2005.
Evaluating eval-uation methods for generation in the presense of vari-ation.
In Proceedings of CICLING, 341?351.462
