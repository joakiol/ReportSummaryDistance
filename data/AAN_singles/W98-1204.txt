///|/////L//A Lexica l ly - Intens ive Algor i thmfor Domain -Spec i f i c  Knowlegde Acquisit ionRend Schneider *Text Understanding SystemsDaimler-Benz Research and TechnologyUlm, Germanyrene .
s chne ider@dbag,  u lm.
Da imlerBenz .
COMAbstractThis paper is an outline of a statistical learn-ing algorithm for information extraction sys-tems.
It is based on a lexicaUy intensive anal-ysis of a small number of texts that belongto one domain and provides a robust lemma-tisation of the word forms and the collectionof the most important syntagmatic dependen-cies in weighted regular expressions.
The lexi-cal and syntactical knowledge is collected in avery compact knowledge base that enables theanalysis of correct and partly incorrect textsor messages, which due to transmission er-rors, spelling or grammatical mistakes other-wise would have been rejected by conventionalsystems.1 IntroductionThe major tasks of information extraction systems(IE-Systems) are the unsupervised selection, fastanalysis and efficient storage of relevant ext pat-terns a person or a group of persons is interestedin.
It accomplishes this through the use of learnedor handcrafted patterns.
In the ideal case the re-sults lead to an appropriate reaction, executed bythe computer itself (see Figure 1).
The extractedinformation is stored in a template that usually isbased on a slot-and-filler model.
Whenever the tex-tual information does not fit automatically into thefillers, it has to be changed adequately to the formand content requirements of the template, otherwisethe text is rejected.
Thus, the templates architecturedepends very much on the domain the IE--systemwas built for, i.e.
before processing a text or a mes-sage and starting the linguistic analysis, the cate-gory that the text or message belong to is alreadyThis study is part of the project READ.
Theproject READ is funded by the German Ministry for Ed-ucation and Research (BMBF) under grant 01IN503C.The author is responsible for the contents of the publi-cation.known or has been labeled automatically with theaid of a categorizer.
In our investigation the systemwas built to process requests for business reports, ex-tracting the number, years and language of businessreports a certain sender asked for.?
.
i~.~:._5:~:.~::..:,:~_..WIISWl Im...
I v~$kl  1~ ~ly  ha~/ .
il ycea cot jId .~a  n~ iI*,eannt~ re~s  ~ y~Jr com~iny ..~ :  wan sew too I !Tel : 1234~6"~0 d k| \[,nfon'nationExwacUon--~ I ~ ?
.
Unsm~ctumci Text -?
cammn~Action Y~.
~.. 93, 94tFigure 1: Overview of an Information ExtractionSystemAlthough a lot of sophisticate investigation hasbeen done in the area of information extrac-tion (Pazienza, 1997) (esp.
since the start of theMUC-Conferences in 1987), only few works are con-cerned with the automatic acquisition of the knowl-edge bases that are needed for IE-tasks (Riloff,1993), which makes the construction of a new sys-tem for a different extraction task still very expen-sive and says much about the brittleness of "tradi-tional" IE-systems.
The problem gets worse whenthe information that has to be extracted is paper-bound and has to be digitized by scanners to makethe information available to the computer, becauseOptical Character Recognition (= OCR) still garblesa consLderable amount of information reduction andnoise on texts, so that there is also a need for morerobust information extraction systems that handleSchneider 19 Lexically-Intensive AlgorithmRen~ Schneider (1998) A Lexically-lntensive Algorithm for Domain-Specific Knowledge Acquisition.
In D.M.W.
Powers(ed.)
NeMLaP3/CoNLL98: New Methods in Language Processing and Computational Natural Language Learning, ACL, pp 19-28.noisy information adequately.The work presented in this paper reflects a sta-tistical approach for the automatic acquisition of alinguistic knowledge base, that allows the essentialanalysis for texts of a certain domain, independentof their transmission quality or pre-processing.2 Cha l lenges  in  In fo rmat ionExt ract ion2.1 The Acquisition BottleneckGenerally, I_E-Systems are built for a rather re-stricted task and work on a more or less limiteddomain.
This keeps their knowledge bases and therules that are needed to process the texts, e.g.
thesyntactic rules, quite compact.
But nevertheless, thechanges that have to be done whenever a workingsystem is applied to another domain are remarkablyhigh, in some cases leading to the construction of aalmost completely new knowledge base.Both, the construction of a new knowledge baseand their maintenance n ed a certain time and lots ofefforts have to be done by highly-skilled staff know-ing the system and the domain it is built for.
On theother hand, texts or messages that are written for avery specific purpose show the phenomena of Sub-languages (Harris, 1982), with less ambiguities andvarieties than unrestricted language but still morefreedom in expression than Controlled Languages.This fact strengthens the need for the automatic ac-quisition of linguistic knowledge, esp.
the construc-tion of a lemmatisation and a shallow parsing com-ponent.Statistical learning algorithms are usually appliedto processing large corpora, but in real life, hugesamples are hard to find for commercial nd indus-trial applications.
In our case, the corpora usuallyconsist of small samples of fewer than 150 very shorttexts and the whole sample must be split into atraining and a test corpora.
This disadvantages arecompensated bythe use of a domain-specific sublan-guage.
Any sublanguage shows some use of typicalvocabulary, styles, and grammatical constructions,and it can be said that the more specific the domainis, the stronger are the restrictions of the sublan-guage.
But even in categories where these restric-tions are weak, the essential and relevant informa-tion is carried by some typical words and located ina few kernel phrases, so that even simple statisticslike frequency lists, distance measures and weightedcollocation patterns may overcome parts of the ac-quisition problem (see section 4).2.2 The Noisy-Channel-ProblemThe second major problem is concerned with thefact that still a remarkably high number of paper-bound texts have to be pre-processed byan OCR-System in order to convert them into machine-readable code.
This problem can be compared tothe well known problem of a noisy channel, as indi-cated in Figure 2.INPUT OUTPUT?
e1 1I IFigure 2: An Example of a Noisy Channel in OCRTherefore, the development of OCR systems andthe improvement of their efficiency is still a ma-jor task in the area of document processing.
Buteven with high quality scanners, the promised 99.9%recognition rate is difficult to achieve (Taghva et al,1994) and remains the ideal case due to e.g.
the useof different fonts, low quality print or paper, a lowresolution etc.Besides the mistakes caused by OCR, a consid-erable number of documents include typographicalor grammatical mistakes (misspellings, wrong inflec-tion or word order), unusal expressions etc., whichshows that natural language processing (NLP) needsmore than just a grammar for grammatical expres-sions but indeed has to be fault-tolerant to process~real-world" utterances.
Though natural anguageitsdf has a lot to do with exceptions and irregular-ities, all these nuisances amplify the problems NLPis occupied with, but - -  as a glance at text samplesshows - -  IE,-systems are faced with a considerablenumber of these additional irregularities that occur?
as a result of low grammatical competence, .g.whenever a non-native speaker is obliged towrite a document or message in a second lan-guage;?
as careless slips, e.g.
misspellings, missing punc-tuations etc.However, the most of all occurring errors are pro-duced by OCR 1 and can be classified as follows:ZA brief example of an OCR-text: I/e would bevBry pleasd ifyou could send two 1992 annuaireports and a product brochure to: -..Schneider 20 Lexically-lntensive AlgorithmII!1IIIIIIIIIIIiIIIIIIIIIII!IIIIIIIIII?
Incorrect character recognition:- Merging or Splitting: Two or more charac-ters are represented as one and vice versa.- Replacement: Characters are confused, e.g.1 and 1.- Deletion: Characters are dropped (e.g.
dueto low print quality).- Insert ion:  Non-existing characters areadded.?
Incorrect word boundary recognition:- Agglutination: Two or more word bound-aries are not recognized, and distinct wordsare linked with each other.- Separation: A single word is split into twoor more fragments.Therefore, in this study one of the principle goalswas to find a new methodology that enables the com-puter to learn automatically from a very small dataset with examples of both grammatically incorrectand orthographically ill-formed text.3 Machine Learning in InformationExtraction3.1 Statistical Language LearningMachine learning techniques have been developed toacquire factual and conceptual knowledge automat-ically and all of them have been applied to naturallanguage processing.
The different techniques werederived from the fields of symbolic, connectionist,statistical and evolutionary computing and their ap-plication depends on the specific problem.
Recentdevelopments show that the consecutive or simulta-neous combination of different learning approaches,i.e.
hybrid strategies, often leads to better resultsthan the single use of one.
The methodology mostfrequently used to support other learning strategiesare statistics, but in several occasion they are alsoused exclusively, esp.
when the a priori knowledgeabout the content and the structure of the data isvery low (Vapnik, 1995).In such cases, all that is needed to start with, is theknowledge about some functional properties of thedata to deduce their dependencies.
Simply speaking,an unordered or hidden structure is transformed intoa systematic structure revealing the properties, re-lations and processes of the data.
In the ideal casethe discovery of these dependencies leads to the for-mulation of general principles or laws.In NLP, statistics are used to describe theprocesse of language acquisition, language changeMIROMIR, an independant financial andeconomic research society, is making astudy about Leasing in Europe.
In orderto make a prvsentation of your company,we would like to recieve your commorcialdocuments and your last snnual roports(from 1988 to 1991) in er~9~?sh.
Ifyou have a mailing ~st would you kindlyinclude our name for future issues ofannual repords and information on yourcompany.
With our grateful thanks,yours faithfully.Figure 3: Domain-Speci f ic  and Tezt-Relevant In-formation (OCR-text)or variation (Abney, 1996) using the methodsof information- and probability theory (Charniak,1993).
Thus, the starting point of every investiga-tion discovering these processes in order to "learn" alanguage or acquire knowledge about some languagewith statistical techniques i the hkelihood of wordsand their derivable distributions and functions.3.2 Domain-Specific and Text-RelevantKnowledgeBesides that, the formulation of what has to belearned needs to be formulated and described pre-cisely, esp.
in IE where the different elements ofthe whole data set are not regarded with the samedegree-of-interest and only a very small part of thewhole information is extracted.
Hence, the systemhas to learn to divide between the important or in-teresting and the unimportant or less interesting in-formation.
In case of OCR-errors, it has to be ableto clean the text from noisy parts and restore thoseparts appropriately.The interesting parts of a text or a message,which have a high significance for IF_~-systems, canbe divided into domain-specific and text-relevantdata (or high level and low level patterns (Yan-garber and Grishman, 1997)) as illustrated in Fig-ure 3, where the domain-specific words are repre-sented in bold and the corresponding text-relevantinformation in cursive letters.
The domain-specificwords can be seen as distinctive from all other wordssince they describe the domain and general pur-pose the text has been written for, whereas thetext-relevant words stand in a close relation to thedomain--specific data because they usually do notappear alone but determine xactly the meaningof the domain-specific words.
In the case of ourexample in Figure 3 the domain-specific nforma-Schneider 21 Lexically-lntensive Algorithmtion is represented by the words rec ieve ,  annualropor ts  and include,  mail ing l i s t .
The text-relevant information MIROFIIR, we, from 1988 to1991, eng l i sh ,  our name specifies the numbers,years and language of the annual reports requestedand of course the sender (which in the case of we andour name has to be unriddled by anapher resolution)that should be included into the mailing list.To illustrate the relationship between domain-specific and text-relevant information, their func-tions may be compared to those of constants andvariables in a mathematical equation with thedomain-specific words (representin.g the unvariableand basic components ofthe equation) and the text-relevant information representing the variables (asunstable and characteristic elements of the equa-tion).
Thinking in categories of natural language,the domain-specific information represents he prag-matic meaning and uses verbs and specific nouns todescribe specific events while the text-relevant in-formation is represented through names, numbers,dates etc.
In any case it has to be considered thatthis distinction depends very much on the sharpnessof the domain the IF_c-System is built for.
Generallyspeaking, the more specific a domain is, the betterdoes this distinction work and thus facilitates boththe construction of the output structure (templates)and the extraction of the relevant text features.Unfortunately - -  as will be seen in the next sec-tion - -  text-relevant information is very difficultto learn automatically, particularly when the textsthat are analyzed have been damaged by OCR: e.g.the differences of repor t  and roport  can easily bedetected and resolved, whereas names of persons,streets etc.
themselves have several spelling variantsand the change of a single letter changes the wholemeaning, as it happens for numbers, too.Therefore the main focus is on the the detection ofdomain-specific nformation with statistical meth-ods that leads in a following step to the text-relevantinformation, i.e.
the major task of the algorithm isto build automatically a knowledge base for the cru-cial words and the kernel phrases that represent thesalient information of a given text.4 Lex ica l ly - In tens ive  KnowledgeAcqu is i t ion4.1 An Outline of the AlgorithmThe algorithm as illustrated in Figure 4 is dividedinto the following major steps: First, a frequency listis computed from the training data, i.e.
the raw textof a limited number of texts belonging to the samedomain.
Then, all word forms are compared witheach other and the word forms with a low distanceare grouped together.
The results from these twoprocedures are combined and lead to the construc-tion of a very compact core lexicon that consists ofa limited number of entries with lexical prototypesand automatically assigned variants of the corpus'word forms.
Afterwards the training data is trans-Figure 4: Building a Domain-Specific Lexiconformed so that it only consists of the automaticallyderived lexical prototypes.
Then the most frequentsyntagmatic patterns from a length of two to fivelemmata re collected and weighted.
In the last butone step similar patterns having at least one domain-specific lexeme in Common are collected to reveal theneighbourhoods of the most important words.
Thedegree-of-interest of a word is computed from itsfrequency and the number of variants the word has.Finally the entries of the core lexicon are connectedwith one another and compressed into weighted reg-ular expressions.
The result is a domain-specific lex-icon that is represented asa net of lexical entries cov-ering the correct word and their variants and someof the possible incorrect variants and the syntacti-cal relations that are commonly used in texts of acertain domain.4.2 Acquisition of Lexical KnowledgeThe construction of the core lexicon is based on thecombination of a frequency list and a comparision ofthe distances of all word forms given in a corpus ofSchneider 22 Lexically-lntensive AlgorithmIIIIIIIIIIIiIiiIIIIIIII/IIIIIIIIIIII!1IIIIca.
one hundred texts.
2A computation ofthe relative size of unknown andalready known word (see Figure 5) shows that aftera very low number of texts generally 80 % of theinformation is confirmed, i.e.
it appeared already inone of the former texts.
These 80 % cover generallythe functional words such as articles, conjunctionsetc.
and of course the domain-specific information.The residual 20 % consist of text-relevant informa-tion, unimportant and less interesting information,misspellings, and - -  in OCR-texts - -  noisy informa-tion.~' .
, .~:!Quota of incomi~ and already imovm worcts~,mple: ~$1s  for anra~al rep~ -A A A A?
^ A A 4 4 ~a ~A ~ ta 4 , :4. .
,4 , : : .
444:: 4)A A A 4 A Aa  A~e '4  T~e,,4  ?
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
1 .
.
.
.
.
.
.
.
.
.
.
.
.
1 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.TeXlS (1-100)Figure 5: Incoming and Known Words in a Sequen-tially Ordered Training CorpusA closer look at the frequency lists strengthensthis impression and allows the postulation of the fol-lowing hypothesis:Hypothes is  1 The more frequent a word appearsin a number of consecutively ordered texts or mes-sages of a limited domain, the more probable will itrepresent the "lezical prototype" for a wordform andin OCR-texts the correct form of a prototype (or alemma).To find out which possible variants exist for thewhole number of word forms, the similarities (or dis-tances) of the word forms are computed.
An effec-tive method for the measurement of word distancesis the Levenshtein distance in combination with anadequate threshold value (Nerbonne t al., 1996),2In this paper we focus on the results of a trainingcorpus of business letter equests in english with a totalnumber of only 7,078 word forms distributed over 100texts.
Notice that the average size cannot be regardedas statistical significant due to the standard eviationof 42.32 and the text sizes ranging between 15 and 256tokens.WordrbportI Variants Distance Frequency1reportreportsroportsofreportsreportingreporting**xportsportscorti~eportportfolioimportantimportanceportfoliosopportunitynorthopportunities0.3330.4280.4280.5550.5550.60.6660.6660.6660.7140.7770.7770.80.80.8180.8330.8468962112111111111312Table 1: Unordered Lexicon Entry: rbportsince the operations that are done to calculate thisdistance cover most of the phenomena (see 2.2) thatoccur through OCR.
Any two words are comparedwith each other in a distance matrix, which mea-sures the least effort of transforming one word intothe other.
Least effort means the lowest number ofinsertions, deletions, or replacements (as a combi-nation of deletion and insertion).
The effort is nor-realized to the length of the longest word in orderto obtain a ratio-scaled value.
Table 1 gives the ex-ample of an unordered lexicon entry for the wordform rbport with all similar words that were foundin the corpus, having a Levenshtein distance lowerthan 0.9 3.
As already postulated in Hypothesis 1,the number of correct and "deflected" forms is al-ways higher than those of typical OCR-mistakes.
Infact it must be asked, whether typical OCR mistakesexist at all due to the different ypes of reasons forthese mistakes and the multitude of effects they mayhave.For every word with one or more similar wordsas determined by a threshold value of 0.9, a pre-liminary entry was created as illustrated in Table 1,covering the most important morphological deriva-STo facilitate and shorten the work of the algorithm,the alphabet was divided into interpretable signs (a-Z,0-9, punctuation) and non-interpretable signs (like *, - ,^ etc.)
which were converted into a middle point (,) .
Aword is considered to be everything between two emptyspaces.
A text or text body is everything that remainedon the document after the elimination of head and footstructures (e.g.
sender, address, signature, etc.
)Schneider 23 Lexically-Intensive Algorithmtions and graphemic alternations, whereas in noneof the entries a distinction between lemma and vari-ants is made so that the unordered lexicon bears ahuge burden of redundant information.
To diminishthis redundancy, it is necessary?
to drop those words having a high distance andshowing no linguistic relation to the other wordsin the entries and?
to make a clear distinction between a lemmaand its variants.Therefore the multitude of preliminary lexical en-tries was reduced to a very compact core lexicon asexampled in Table 2 and described as follows.The algorithm processes successively through thefrequency list, starting with the most frequent wordand finishing with the last hapax legomenon.
Eachword that can be found in the frequency list is con-sidered as the top of a new lexicon entry or lemma.Afterwards, the algorithm looks for the word formsin the preliminary lexicon, that are similar to thisword (having a distance smaller than 0.4), assignsthem as variants in the new entry and recursivelylooks for all variants of the previously assigned vari-ants (having a distance smaller than 0.7).
Each oneof these variants can no longer be regarded as topof another entry and consequently is taken out ofthe frequency lists, that simultaneously shrinks moreand more.
The variants' frequency is added to thatof the lemma.The results of the algorithm depend a lot on ana priori specified treshold value for the Levenshteindistances.
In our tests, good results are achievedwith a value of 0.4 for direct similarity and 0.7 forindirect similarity, meaning the newly computed is-tance of variants of a variant o a given lemma.
Thethreshold value may depend on the language and thedomains that are used.
This aspect will be furtherinvestigated.The result of this process is a core lexicon thatconsists of?
high frequent synsemantica or function wordshaving no variants,?
high frequent, domain specific autosemantica orcontent words and most of their occuring vari-ants,?
middle and low frequency words and their vari-ants, and?
one single entry for all the remaining hapaxlegomena having no similarity to one of the pre-ceding words lower than 0.4,StemreportEVariantsreports 0.142reprt 0.166repo 0.333repOrt 0.333rbport 0.333ofreports 0.333reporting 0.333reporting?
0.4roports 0.428fjeport 0.428repoii 0.666sports 0.666repods 0.66613D.istanc.e \[ f req8861211112111111163Table 2: Core Lexicon Entry: reportin order of their summarized frequencies.
Hence, thenumber of entries in the core lexicon is at about onethird of the total number of types 4.
Table 2 showsthe entry for repor t  and the assigned variants.As follows, many of the wrongly analyzed combi-nations of e.g.
your annual report that formerly leadto a rejection of the text, now can be transformedinto their correct forms.
This increases the numberof documents that can be analyzed by the IE-systemconsiderably.
The wrong assignment of sports as avariant of repor t  shows the domain dependency ofthe algorithm, but it has to be considered that thefrequency of such wrong assignments generally is 1and can be compensated by the extraction of syn-tactical patterns.4.3 Acquis i t ion of  Syntactical KnowledgeThe core lexicon bears the basic lexical knowledgethat is needed for a morphological text analysis andfurthermore can be used to "clean" documents fromnoisy sequences but it does not store any informa-tion about the syntagmatic relations or dependenciesthat exist in texts of a given domain.
To reveal thesedependencies, the original corpus was transformedinto a lemmatized version, consisting only of the ear-lier derived prototypes and "Weighted Ranks" forwords with the frequency 1 having no similarity toother words.
Figure 6 shows the example text (seeFigure 3) after the transformation into lemmata nd"jokers".
As can be seen in Figure 6, the algorithm4In the case of the english requests for annual reportsthe core lexicon comprised 537 entries with a total num-ber of 1758 types and 7078 tokens in the training corpus.Schneider 24 Lexically-lntensive AlgorithmI|Immmmmmmmmmmmmmmmmmm|mm- , an independant financial andeconomic research - , is making a studyabout leasing in european .
in order tomake a - of your company , we would liketo receive your commorcial documentsand your latest ~ua l  report from - to1991 in english .
if you have a mail inglist would you kind include our namefor future issues of ~nnual report andinformation on your company .
with ourgrateful thank , your -  .Figure 6: A Lemmatized Text?
suppresses (in this format) the Hapax-Legomena like Miromir, society,prvsentation, 1988 and faithfully;?
corrects the OCR-errors of the most impor-tant words, like roports  --+ report ,  repods --+report, information -+ information;?
corrects misspelled words, like recieve -+receive;?
lemmatizes several less frequent words to theirmore frequent prototypes, like ropods andrepods as plural forms of report ,  l as t  -+la tes t ,  kindly -+ kind, tb~ks  -+ thank,yours --4 your 5.To enhance the importance of the lexical pro-toypes or lemmata, their frequencies were multipli-cared with the number of their assigned variants asa result of the following hypothesis:Hypothes is  2 The more often a word appears intexts of a restricted category and the more morpho-logical and graphemic variants it has, the more prob-able the word will represent some domain-specific in-formation.The multiplication of frequencies and the number ofvariants of a word (freqz varx) leads to a weightedfrequency list (see Table 3) whose first ranks com-prise the most relevant lemmata that are neededfor the extraction of the salient syntactic patterns.Therefore, the texts are transformed parallely into acorpus of indices implying the ranks that are givento the lemmata fter they have been weighted.Scommorcial is head of an entry includingcommercial as the single variant, both having thefrequency 1.
Thus, the distinction between stem andvariant can not be done clearly by the algorithm (thesame holds true for independant and independent).Nevertheless, the two forms and all newly occuringforms having a small distance value will be clusteredtogether.WordreportannualyourwouldthankcompanyothermailinginformationpleasefinancialgratefulthetostatementsinternationaloflatestlistyouandreceiveI Sreq, I varx ITable 3: Weighted Frequency List (first 20 Ranks)The concluding analysis follows the Firthiannotion of "knowing a word by the company itkeeps" (Firth, 1957), a postulate which emphasizesthe fact that certain words have a strong tendency tobe used together.
Thus, the algorithm retrieves allcollocation patterns of different length (2 - 5) andmatches them with one another.
Repetitively themost frequent patterns are matched with the colloca-tion patterns of a greater length (patterns of length2 with patterns of length 3; patterns of length 3 withpatterns of length 4 etc.)
looking both left and rightfor high frequent lemmata in the neighbourhood fthe already composed patterns.
That means thatthe words from the top of the weighted frequencylist are connected with the most common words thatprecede and succeed them.
The result is a two--wayfinite-state automaton that may be analyzed usinglight parsing strategies (Grefenstette, 1996) with thesalient words of the weighted frequency lists as start-ing points (see Figure 7).One attractive alternative to parse the text is abottom-up island parser for the kernel phrases of anew text.
Island Parsers are a useful tool especiallyin those cases where no sentence markers exist (ase.g.
in speech recognition) or whenever they are nottransmissed correctly or added (as in OCR-texts).Furthermore a full parse contradicts in a certain waySchneider 25 Lexically-lntensive Algorithmthe real ambitions of IE-Systems (Grishman, 1996)and flat finite-state analyses are getting more andmore popular and efficient (Bayer et al, 1997).
Yet,6)Figure 7: Generation of a bi-directional regular ex-pressionthe statistical information that is represented in theranks of the lexical stems should not be omitted,though they show evidence of the degree-of-interestthat is needed for the parsing strategy.
The neigh-bourhood of low indices, such as 3 +- 2 <-- 1 (repre-senting your annual report should be regarded asmore representative for the corpus' syntax than e.g.15 <--- 3 -r6 representing of your company or even95 e- 45 --+ 381 representing am currently doing.The weighted ranks represent the degree-of-interestthat the words have for the IE-System.
With thehelp of the weighted ranks, it is possible to computea probabilistic value similar to transition likelihoods.Looking at a pattern or window of several words wiof a given pattern length n, we add up the ranks ofthe weighted frequency lists fw~ to ~,  and computethe average rank.
This value is divided by the overallfrequency freq of the whole pattern (wl ..wn):i=1 re( 2 ~%t~in".'
= i=1C(~1..~.)
= freq(wl..wn) n \]req(wl..Wn)The resulting value represents the weighted like-lihood for the c(>-occurrence C(wa..w.) of two (ormore) words indicating how probable a word pre-cedes or succeeds another word.
To give an examplethe word pattern of two words like mail ing l i s tthe equation is solved a follows:rmail ing + rl istC(mail ing list) = 2 freq(mailin# list)8+19 = 0.3852- 35or for longer patterns of a lower degree-of-interest,such as as any interim:ras -Jr rany -t- rinterimC ( as any interim) = 3 f r eq( as any interim)53 + 87 + 28- = 56.0 3-1As already pointed out, the values for the co--occurrences ofthe different lemmata were only com-puted up to a length of 5 lemmata.
Comparedto other collocation measures, this value does notonly take account of the words frequencies and thecollocations frequencies (as e.g.
Mutual Informa-tion (Church and P., 1990)) or their transition like-lihood (as e.g.
Markov chains (Thomason, 1986))but combines these two properties with a third one:the word's different modalities as indicated by theirnumber of variants, i.e.
their weighted ranks.
Thislast value weakens the influence of both less frequentand fimctional words and supports the degree-of-interest of domain-specific and correct words as de-termined in Hypothesis 1 and 2.The c(>-occurrence values may be labeled to thearcs of the regular expressions that are generatedduring this acquisition process to make the parsingprocess more effective since a low transition valuereflects a high significance or degree--of-interest intexts of a certain domain.5 Using the Domain-Specific LexiconThe connections that exist between the different lex-ical entries are also used to link the entries of the corelexicon, providing it with the syntactical informationthat is typical for a certain domain.
The contents ofthe entries and their relations, i.e.
the arcs connect-ing them, cover the essential statistical properties oflexemes and their syntactical relationship, enablinga robust lexical and syntactical nalysis of new texts.First results show that word forms are deflected?
-.
in the past your companys report hasbeen among those we collect .
however, our records indicate we do not havea copy of your 1992 annual rbport .please help us complete our collectionby sendhig a copy of your 1992 annualreport to the followhig adess .
.-.Figure 8: Example of an Unknown Text (OCR)and corrected (as shown in Figure 6 and 9); kernelphrases are isolated by extracting the islands of thedomain-specific words and their surroundings (asSchneider 26 Lexically-lntensive AlgorithmmmmmmmmmmmmmmB|BBBB|BBBBm|?
-.
in  the  - your  company ~epo~ has  beenamong these  we co l lec t ion  .
however  ,our records indicating we do  not havea copy of your 1992 ~nua l  ~epor~p lease  help us complete our collectionby sending a copy of your 1992 annual~eport to the following address ..-Figure 9: Lemmatizing an Unknown Textshown in Figure 7 and 10).
Although some words arelemmatized in a quite strange way, as e.g.
co l lec t-+ collection or indicate -~ indicating due totheir low frequence, the relevant patterns are con-verted to analyzable and weU formed strings.Given a new text with several occurences of ahighly-ranked words, (see Figure 8), the text is lem-matized (see Figure 9 and browsed for the word withthe highest degree--of-interest a  indicated by thewords' weighted ranks (in our example report).Afterwards the transition values for the threeneighbourhoods of repor t  are compared and or-dered after the values of the weighted transitionlikelihood (see Figure 10 with immediate transitionvalues i.e.
a window length 2).
In our case, thesecond phrase has the lowest transition values andwould consequently extract and parse succesfullythe most relevant phrase a copy of your 1992annual report to the following address.$9.16.0.
,6  0 .7  -Figure 10: Parsing the Most Likely NeighbourhoodThe lexicon's dynamic structure nables the anal-ysis of unknown texts and consequently updates theentries and the relations among them, i.e.
wheneveran unknown word or a new syntactical pattern ap-pears, the Levenshtein-Distance to the already exist-ing heads of the lexical entries is computed and theword either stored as a new variant or a new entrycreated.
Similar to the lexical updating process theweights of the tokens that connect the lexical entriesare either affirmed and strengthened with the repeti-tion of every pattern that was already known to thesystem or the new pattern is added to the network.6 Conc lus ionIn this paper we discussed the construction of a sta-tistical learning algorithm based on restricted o-mains and their underlying sublanguages in orderto build automatically a linguistic knowledge basefor information extraction tasks with the aid of verysimple arithmetic procedures.
The method is basedon weighted frequency lists of word forms and syn-tactical patterns.
Although very small informationabout the texts and the domain is known a prioriand only two functional dependencies (see Hypothe-ses 1 and 2) have been postulated, the algorithmlearns automatically tobuild a very compact knowl-edge base from small and noisy text corpora.
Themethod was tested empirically on several english,german and spanish corpora nd shows the same re-sults for noisy as well as for correct domain-specificcorpora.A comparison of the core lexicon with commonfrequency analyses (Francis and Ku~era, 1982) forcorrect exts shows that even with a very small textsample the resulting information for linguistically al-lowed alterations of a lexical base form is acquiredautomatically.
Additional information is achievedwith the subsumption oflinguistically incorrect vari-ants.
The acquired knowledge is stored in a com-pact and dynamic knowledge base whose structureis modified with every significant change of the lex-eme's probabilistic properties and relations.
Theknowledge base is quite compact and allows a veryquick analysis of unknown texts.First tests with different corpora nd different lan-guages (German and Spanish) show that this algo-rithm can be applied to different domains and otherlanguages and thus is a useful tool for the expansionof IE-systems that work with OCR--data.
Althoughthe results of the algorithm depend very much onthe data, i.e.
the limits or sharpness of the domainwhich is used, the underlying ideas may be used forany information extraction purpose and other appli-cations such as lexicography, information retrievalor terminology extraction.7 AcknowledgementsI wish to thank Ingrid Renz and Uli Bohnacker forall the ideas, suggestions and comments that foundtheir way into this paper.Schneider 27 Lexically-Intensive AlgorithmReferencesSteven Abney.
1996.
Statistical methods and lin-guistics.
In Klavans, J.L.
and Resnik, P., editors,The Balancing Act: Combining Symbolic and Sta-tistical Approaches to Language, pages 1-26.
MITPress, Cambridge, MA.Thomas Bayer, Uli Bohnacker and Ingrid Renz.1997.
Information extraction from paper docu-ments.
In Bunke, H. and Wang, P.S.P., editors,Handbook on Optical Character Recognition andDocument Image Analysis, pages 653-677.
WorldScientific Publishing Company, Singapore.Eugene Charniak.
1993.
Statistical Language Learn-ing.
MIT Press, Cambridge, MA.Kenneth W. Church and Patrick Hanks.
1990.
Wordassociation orms, mutual information and lex-icography.
Computational Linguistics, 16(3):22-29.John R. Firth.
1957.
Modes of meaning.
In J.R.Firth: Papers in Linguistics, pages 190--215, Lon-don.
Oxford University Press.W.
Nelson Francis and Henry Ku~era.
1982.
Fre-quency Analysis of English Usage.
Houghton Mif-flin, Boston, MA.Gregory Grefenstette.
1996.
Light parsing as finite-state filtering.
In Proceedings of the Workshopon Extended Finite State Models of Language,ECAI'96, Budapest, Hungary.Ralph Grishman.
1996.
The NYU system for MUC-6 or where's the syntax?
In Proceedings of theSixth Message Understanding Conference (MUC-6), Columbia, MD.
Morgan Kaufraann.Zellig S. Harris.
1982.
Discourse and sublan-guage.
In Kittredge, R. and Lehrberger, J., ed-itors, Sublanguage: Studies of Language in Re-stricted Semantic Domains, pages 231-236. deGruyter, Berlin.John Nerbonne, Wilbert Heeringa, Erik van denHout, Peter van der Kooi, Simone Otten andWillem van de Vis.
1996.
Phonetic distance be-tween dutch dialects.
In Durieux, G., Daelemans,W., and GiUis, S.,-editors, Proceedings of Com-putational Linguistics in the Netherlands, pages185--202, Antwerp, Centre for Dutch Languageand Speech (UIA).Maria Teresa Pazienza.
1997.
Information Extrac-tion - A Multidisciplinary Approach to an Emerg-ing Information Technology.
Springer, Berlin.Ellen Riloff.
1993.
Automatically constructing adictionary for information extraction tasks.
InProceedings of the 11th National Conference onArtificial Intelligence, AAAI-93, pages 811-816.Kazem Taghva, Julie Borsack, Allen Condit andSrinivas Erva.
1994.
The effects of noisy dataon text retrieval.
Journal of the American Soci-ety for Information Science, 45(1):50--58.Michael G. Thomason.
1986.
Syntactic patternrecognition: Stochastic languages.
In Fu, K.S.and Young, T.Y., editors, Handbook of PatternRecognition and Image Processing, pages 119-142.Academic Press, Orlando, FL.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, New York.Roman Yangarber and Ralph Grishman.
1997.Customization of information extraction system.In Proceedings of the International Workshop onLexically Driven Information Extraction.
Univer-sit~ di Roma "La Sapienza'.Schneider 28 Lexically-lntensive AlgorithmIIIIIIIIIIIIIIII!1IIIIIIIIII
