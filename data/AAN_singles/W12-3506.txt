Proceedings of the 1st Workshop on Speech and Multimodal Interaction in Assistive Environments, pages 34?42,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsTowards a Self-Learning Assistive Vocal Interface:Vocabulary and Grammar LearningJanneke van de Loo1, Jort F. Gemmeke2, Guy De Pauw1Joris Driesen2, Hugo Van hamme2, Walter Daelemans11CLiPS - Computational Linguistics, University of Antwerp, Antwerp, Belgium2ESAT - PSI Speech Group, KU Leuven, Leuven, Belgiumjanneke.vandeloo@ua.ac.be, jort.gemmeke@esat.kuleuven.be, guy.depauw@ua.ac.be,joris.driesen@esat.kuleuven.be, hugo.vanhamme@esat.kuleuven.be, walter.daelemans@ua.ac.beAbstractThis paper introduces research within theALADIN project, which aims to develop an as-sistive vocal interface for people with a phys-ical impairment.
In contrast to existing ap-proaches, the vocal interface is self-learning,which means it can be used with any language,dialect, vocabulary and grammar.
This pa-per describes the overall learning framework,and the two components that will provide vo-cabulary learning and grammar induction.
Inaddition, the paper describes encouraging re-sults of early implementations of these voca-bulary and grammar learning components, ap-plied to recorded sessions of a vocally guidedcard game, Patience.1 IntroductionVoice control of devices we use in our daily lives isstill perceived as a luxury, since often cheaper andmore straightforward alternatives are available, suchas pushing a button or using remote controls.
Butwhat if pushing buttons is not trivial?
Physicallyimpaired people with restricted (upper) limb mo-tor control are permanently in the situation wherevoice control could significantly simplify some ofthe tasks they want to perform (Noyes and Frankish,1992).
By regaining the ability to control more de-vices in the living environment, voice control couldcontribute to their independence of living and theirquality of life.Unfortunately, the speech recognition technologyemployed for voice control still lacks robustness tospeaking style, regional accents and noise, so thatusers are typically forced to adhere to a restrictivegrammar and vocabulary in order to successfullycommand and control a device.In this paper we describe research in the ALADINproject1, which aims to develop an assistive vocalinterface for people with a physical impairment.
Incontrast to existing vocal interfaces, the vocal inter-face is self-learning: The interface should automa-tically learn what the user means with commands,which words are used and what the user?s vocal char-acteristics are.
Users should formulate commandsas they like, using the words and grammatical con-structs they like and only addressing the functional-ity they are interested in.We distinguish two separate modules that estab-lish self-learning: The word finding module workson the acoustic level and attempts to automaticallyinduce the vocabulary of the user during training, byassociating recurring acoustic patterns (commands)with observed changes in the user?s environment(control).
The grammar induction module worksalongside the word finding module to automaticallydetect the compositionality of the user?s utterances,further enabling the user to freely express com-mands in their own words.This paper presents a functional description of theALADIN learning framework and describes feasibi-lity experiments with the word finding and grammarinduction modules.
In Section 2 we outline the over-all learning framework, the knowledge representa-tion that is used and the rationale behind the wordfinding and grammar induction modules.
In Sec-tion 3 we briefly describe the Patience corpus used1Adaptation and Learning for Assistive Domestic VocalINterfaces.
Project page: http://www.esat.kuleuven.be/psi/spraak/projects/ALADIN34Word finding Input: AudioGrammar inductionSemantics Frame  DescriptionGrammar Frame  Description  Input: Controls Device State  Output: Controls Device StateFigure 1: Schematic overview of the ALADIN framework.in the feasibility experiments, as well as the experi-mental setup.
In Section 4 we show and discuss ourexperimental results and we present our conclusionsand thoughts on future work in Section 5.2 The ALADIN frameworkThe ALADIN learning framework consists of sev-eral modules, which are shown schematically inFig.
1.
On the left-hand side, the provided input isshown, which consists of a spoken utterance (com-mand) coupled with a control input, such as the but-ton press on a remote control or a mouse click, pos-sibly augmented with the internal state of a device(for example the current volume of a television).In order to provide a common framework for allpossible actions we wish to distinguish, we adoptthe use of frames, a data structure that encapsulatesthe control inputs and/or device states relevant to theexecution of each action.
Frames consist of one ormultiple slots, which each can take a single valuefrom a set of predefined values.
In Section 2.1 wediscuss the frame representation in detail.During training, the word finding module buildsacoustic representations of recurring acoustic pat-terns, given a (small) set of training commands, eachdescribed by a frame description and features ex-tracted from the audio signal.
Using the frame de-scription, the module maps such acoustic representa-tions to each slot-value pair in each frame.
When us-ing the framework for decoding spoken commands,the output of the module is a score for each slot-value pair in each frame, representing the probabil-ity that this slot-value pair was present in the spokencommand.During training, the grammar induction modulebuilds a model of the grammatical constructs em-ployed by the user, using the frame description andthe output of the word finding module.
The output ofthe word finding module consists of estimates of theslot-value pair scores described above, based on thepresence of automatically derived recurring acousticpatterns.The semantics module, operational during decod-ing, processes the output of the word finding mod-ule to create a single frame description most likelyto match the spoken command.
This can then beconverted to a control representation the target de-vice can work with.
The module can make use ofa grammar module that describes which slot-valuepair combinations (and sequences) are likely to oc-cur for each frame.
Such a grammar descriptionshould ideally be provided by the grammar induc-tion module, but could optionally be hand-crafted.2.1 Frame descriptionEach action that can be performed with a device isrepresented in the form of a frame.
A frame is adata structure that represents the semantic conceptsthat are relevant to the execution of the action andwhich users of the command and control (hence-forth C&C) application are likely to refer to in theircommands.
It usually contains one or multiple slots,each associated with a single value.
The slots inan action frame represent relevant properties of theaction.
Such frame-based semantic representationshave previously been successfully deployed in C&Capplications and spoken dialog systems (Wang et al,2005).For our research, we distinguish three types offrames.
The first, the action frame, is automaticallygenerated during training by the device that is con-trolled with a conventional control method, such asbutton presses.
Depending on the frame, more slotsmay be defined than are likely to be referred to in anysingle command.
The second frame type, the oracle35Frame Slot Value<from suit> c<from value> 11<from column> 3<from hand> -<to suit> h<to value> 12<to foundation> -<to column> 4Figure 2: An example of a Patience move and theautomatically generated movecard action frame.
Acard is defined as the combination of a suit - (h)earts,(d)iamonds, (c)lubs or (s)pades - and a value, from ace(1) to king (13).
We also distinguish slots for the ?hand?
atthe bottom, the seven columns in the center of the playingfield and the four foundation stacks at the top right.action frame, is a manually constructed subset of theaction frame based on a transcription of the spokencommand.
In this subset, only those slots that arereferred to in the spoken command, are filled in.
Fi-nally, we define the oracle command frame, which isa version of the oracle action frame that can assignmultiple values to each slot in order to deal with pos-sible ambiguities in the spoken command.We will illustrate these frame types with an exam-ple from one the target applications in the ALADINproject: a voice-controlled version of the card gamePatience.
In this game, one of the possible actionsis moving a card in the playing field.
This actionis described by an action frame dubbed movecard,which contains slots specifying which card is movedand to which position it is moved.
Fig.
2 shows anexample of such a move, and the automatically gen-erated action frame description of that move.For instance, if the move in Fig.
2 was asso-ciated with the spoken command ?put the jack ofclubs on the red queen?, the oracle action frameof that particular move would only have the fol-lowing slot values filled in: <from suit>=c,<from value>=11, <to suit>=h and<to value>=12, since the columns are notreferred to in the spoken command.
Also, since noslot was defined that is associated with the color ofthe card, the spoken command is ambiguous andduring decoding, such a command might also be as-sociated with a frame containing the slot-value pair<to suit>=d.
As a result, the oracle commandframe will be constructed with <to suit>=h,drather than <to suit>=h.2.2 Word findingThe word finding module is tasked with creatingacoustic representations of recurring acoustic pat-terns, guided by action frames.
As such, the learningtask is only weakly supervised: rather than havingknowledge of the sequence of words that were spo-ken, as common in Automatic Speech Recognition(ASR), we only have knowledge of the slot-valuepairs in the action frame, each of which may havebeen referred to in the utterance with one or multi-ple words, and in any order.
To meet these require-ments, we turn to a technique called non-negativematrix factorization (NMF).2.2.1 Supervised NMFNMF is an algorithm that factorizes a non-negative M?N matrix V into a non-negative M?Rmatrix W and a non-negative R ?
N matrix H:V ?
W ?
H. In our approach, we construct theNMF problem as follows:V =[V0V1]?
[W0W1]H = WH (1)with the matrix V1 composed of N spoken com-mands, each represented by a vectorial representa-tion of dimension M1.
The columns of V0 asso-ciate each spoken command with a label vector ofdimension M0 that represents the frequency with36which a particular label occurred in that spokencommand.
After factorization, the matrix W1 con-tains R acoustic patterns of dimension M1, and thematrix H indicates the weights with which these Racoustic patterns are linearly combined for each spo-ken command n, 1 ?
n ?
N , to form the observedspoken commands in V1.
The columns of the ma-trix W0 describe the mapping between the R acous-tic patterns in W1 and the M0 labels that can beassociated with each spoken command.
In additionto columns of W1 associated with labels, we usea number of so-called ?garbage columns?
to captureacoustic representations not associated with labels,for example to capture less meaningful words suchas ?please?.To decode a spoken command (the ?testing?phase), we find a vector h for which holds: v1tst =W1htst, with W1 the matrix found during train-ing.
vtst1 is the M1 dimensional acoustic represen-tation of the spoken command we wish to decode,and htst is the R-dimensional vector that indicateswhich acoustic patterns in W1 need to be linearlycombined to explain vtst1 .
Finally, we calculate thelabel association with the spoken command vtst1 us-ing: a = W0htst, where a is a M0 dimensionalvector giving a score for each label.For more details on how to carry out these fac-torizations, we refer the reader to Lee and Seung(1999).
For a discussion on representing spokencommands of varying length as a M1-dimensionalvector, and the constraints under which it holds thatthe spoken command is the linear combination ofR such vectors from W1, we refer the reader to(Van hamme, 2008; Driesen and Van hamme, 2012;Driesen et al, 2012) and the references therein.2.2.2 Frame decodingIn our framework, we consider each uniqueslot-value pair of each frame (for example<to suit>=h of the frame movecard) asa single label, making the total number of labelsM0 equal to the cumulative number of differentvalues in all slots in all frames.
This way, eachframe description is uniquely mapped to a binaryvector v1, and likewise, the decoded label vector ais uniquely mapped back to a frame description.Put the jack of clubs on the queen of heartsO O I FV O I FS O O I TV O I TSFigure 3: Example of a command transcription, annotatedwith concept tags.2.3 Grammar inductionThe task of the grammar module is to automaticallyinduce a grammar during the training phase, that de-tects the compositionality of the utterances and re-lates it to the associated meaning.
In this case, thegrammatical properties of the utterances are associ-ated with action frames, containing slots and values.This grammar induction is performed on the basisof the output of the word finding module (hypothe-sized ?word?
units, represented as acoustic patternsand possibly associated frame slot values) and thegenerated frame descriptions of the actions.
Further-more, the grammar may also serve as an additionalaid during the decoding process, by providing infor-mation regarding the probability of specific frameslot sequences in the data.There are different options with respect to the typeof grammar that can be induced.
It could for instancebe a traditional context-free grammar, meaning thatthe contents of the frame description of the actionare derived on the basis of a parse tree of the ut-terance.
Unfortunately, context-free grammars havebeen proven to be very hard to automatically induce(de Marcken, 1999; Klein, 2005), particularly on thebasis of limited training data.Encouraging results have been reported in the un-supervised induction of sequence tags (Collobert etal., 2011).
In the context of the ALADIN project,we therefore decided to adopt a concept tagging ap-proach as a shallow grammar interface between ut-terance and meaning.
In this vein, each command issegmented into chunks of words, which are taggedwith the semantic concepts (i.e.
frame slots) towhich they refer.We use a tagging framework which is based onso-called IOB tagging, commonly used in the con-text of phrase chunking tasks (Ramshaw and Mar-cus, 1995).
Words inside a chunk are labeled with atag starting with I and words outside the chunks arelabeled with an O tag, which means that they do notrefer to any concept in the action frame.
Fig.
3 illus-trates the concept tagging approach for an examplecommand.373 Experimental setupThe experiments described in this paper pertain toa vocal interface for the card game Patience.
Thispresents an appropriate case study, since a C&C in-terface for this game needs to learn a non-trivial,but fairly restrictive vocabulary and grammar.
Com-mands such as ?put the four of clubs on the five ofhearts?
or ?put the three of hearts in column four?are not replaceable by holistic commands, and iden-tifying the individual components of the utteranceand their interrelation is essential for the derivationof its meaning.
This makes the Patience game amore interesting test case than domotica applica-tions such as controlling lights, doors or a television,where the collection of unordered sets of keywordsis usually sufficient to understand the commands.In this section, we will describe the corpus col-lected to enable this case study, as well as the setupfor exploratory experiments with the techniques out-lined in Section 2.3.1 Patience corpusThe Patience corpus consists of more than two thou-sand spoken commands in (Belgian) Dutch2, tran-scribed and manually annotated with concept tags.Eight participants were asked to play Patience on acomputer using spoken commands, which were sub-sequently executed by the experimenter.
The partic-ipants were told to advance the game by using theirown commands freely, in terms of vocabulary andgrammatical constructs.
The audio signals of thecommands were recorded and the associated actionswere stored in the form of action frames.
There aretwo types of frames: a movecard frame, describ-ing the movement of a card on the playing field (e.g.Fig.
2), and a dealcard frame that contains noframe slots, but simply triggers a new hand.
Oracleaction and command frames were derived on the ba-sis of the automatically generated action frames andthe manually annotated concept tags.Each participant played in two separate sessions,with at least three weeks in between, so as to capturepotential variation in command use over time.
Theparticipants?
ages range between 22 and 73 and webalanced for gender and education level.
We col-lected between 223 and 278 commands (in four to2Note however that the ALADIN system is inherently lan-guage independent, which is why we present the examples inEnglish.six games) per participant.
The total number of col-lected commands is 2020, which means an averageof 253 commands per participant and the averagenumber of moves per game is 55.
The total num-ber of frame slot-value pairs is 63.The experimental setup tries to mimic theALADIN learning situation as much as possible.For each participant, a separate learning curve wasmade, since the learning process in the targetedALADIN application will be personalized as well.For each learning curve, the last fifty utterances ofa participant were used as a constant test set.
Theremaining utterances of the same participant wereused as training material.
The chronological orderof the commands, as they were uttered by the partic-ipant, was preserved, in order to account for the de-velopment of the users?
command structure and vo-cabulary use during the games.
In each experiment,the first k utterances were used as training data, k be-ing an increasing number of slices of ten utterancesfor the grammar induction experiments and 25 utter-ances for the word finding experiments.3.2 Word findingSpoken commands are represented by a His-togram of Acoustic Co-occurrence (HAC) features(Van hamme, 2008), constructed as follows: First,we extract mel-cepstral coefficients (MFCC) fromaudio signals sampled at 16kHz, framed using timewindows of 25ms and shifted in increments of 10ms.From each of these frames, 13 cepstral coefficients,along with their first and second order differencesare determined, yielding a 39 dimensional featurevector.
Mean and variance normalization are appliedon a per-utterance basis.
Second, k-means clus-tering of 50000 randomly selected frames is usedto create a Vector Quantization codebook with 200codewords for each speaker, using k-means cluster-ing.
Finally, three sets of HAC features are con-structed by counting the co-occurrences of the au-dio expressed as VQ codewords, with time lags of 2,5 and 9 frames.
The final feature dimension M1 isthus M1=3?
2002 = 120000.In these initial experiments, we use the oracle ac-tion frames to provide supervision.
In the NMFlearning framework, two acoustic representationswere assigned to each label, with an additional 15representations used as garbage columns.
The to-tal number of acoustic representations R is thus38R = 2 ?
63 + 15 = 141.
For training, W1 is ini-tialized randomly and W0 is initialized so that twocolumns are mainly associated with each label (i.e.,a one in the corresponding label position and a small([0, 1e?
5]) random value for the other labels).
Theremaining 15 garbage columns are randomly initial-ized.
Finally, the entries of V1 and V0 are scaledso their cumulative weight is equal.
During training,the rows of H pertaining to non-garbage columnsin W0 are initialized to be the same as V0, witha small ([0, 1e ?
5]) random value replacing valuesthat are zero.
The rows of H pertaining to garbagecolumns are initialized randomly.
For the NMF fac-torization, we minimized the Kullback-Leibler di-vergence using 100 iterations of the procedure de-scribed in Lee and Seung (1999).In these experiments, frame decoding is guidedby a hand-crafted grammar, rather than an auto-matically induced grammar.
We defined 38 gram-mar rules corresponding to various possible slot se-quences, under the assumption that from slots pre-cede to slots, and that suit slots precede valueslots.
These 38 rules also include various slot se-quences in which the command was underspecified.A pilot experiment showed that this grammar cov-ers 98% to 100% of the spoken commands, depend-ing on the speaker.
The hand-crafted grammar wasimplemented as a labelvector-to-labelvector bigramtransition matrix, and Viterbi decoding was used togenerate a possible frame description for each gram-mar rule.
For scoring, the most likely frame descrip-tion was selected based on the most likely Viterbipath across grammar rules.
Finally, we express re-sults in terms of slot-value accuracy, which is theratio of the number of slot-value pairs correctly se-lected, according to the oracle command frame, andthe total number of slot-value pairs in the oraclecommand frame (expressed as a percentage).3.3 Grammar inductionThe exploratory experiments for the grammar in-duction module serve as a proof-of-the-principle ex-periment that showcases the learnability of the taskin optimal conditions and focuses on the minimallyrequired amount of training data needed to boot-strap successful concept tagging.
In these super-vised learning experiments, the annotated corpus isused as training material for a data-driven tagger,which is subsequently used to tag previously unseendata.
As our tagger of choice, we opted for MBT,the memory-based tagger (Daelemans et al, 2010),although any type of data-driven tagger can be used.In the targeted ALADIN application, the numberof utterances used to train the system should be assmall as possible, i.e.
the training phase should beas brief as possible in order to limit the amount ofextraneous physical work or assistance needed fortraining by the physically impaired person.
In or-der to get an idea of the minimal number of train-ing utterances needed to enable successful concepttagging, we evaluated the supervised tagging perfor-mance with increasing amounts of training data, re-sulting in learning curves.The metric used for the evaluation of the con-cept tagger is the micro-averaged F-score of the pre-dicted I chunks: the harmonic mean of the pre-cision and recall of the chunks with I labels (i.e.referring to slots in de frame description).
Thismeans that the concept tags as well as the boundariesof the predicted chunks are included in the evalu-ation.
Feature selection was performed on the ba-sis of a development set (last 25% of the trainingdata) and establishes the best combination of disam-biguation features, such as the number of (disam-biguated) concept tags to the left, the tokens them-selves (left/right/focus) and ambiguous tags (focustoken and right context).
We compare our resultsagainst a baseline condition, in which only the focusword is used as a feature, in order to see the rela-tive effect of the use of context information by thetagger.4 Results and discussion4.1 Word findingIn Fig.
4a we can observe the results obtained witha learning framework that combines word findingwith hand-crafted grammars.
From these results, wecan observe that the slot-value accuracy obtained af-ter using all available training material, varies be-tween 40.4% for speaker 4 and 76.0% for speaker 1.We can also observe that overall, the results for allspeakers show a fairly linear increase in accuracy asmore training material becomes available.
The factthat we do not yet observe that the accuracy levelsoff with increasing training data, indicates that theresults are likely to further improve with more train-ing data.390 50 100 150 200 25020406080100Size of training set (#utterances)Slot?value accuracy(%)11 11 111112 22 222 2 2 233 33 3 33 3334444 4 4 445 5 55 5 556 666 66 667 777 77 7 788 88 8 8 88 8p participant p(a) Word finding results0 50 100 150 200 25020406080100Size of training set (#utterances)I?ChunkF?score(%)1 111 1 1 111 11 1 1 1 1 1 1 1 1 1 1 112 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 23 3 3 3 33 33 3 3 3 3 3 3 3 3 3 3 3 3 3 3 344444 4 4 4 4 4 44 4 4 4 4 4 4 4 45 55 5 5 5 5 5 5 5 5 5 5 5 5 5 5566 6 66 6 6 6 6 66 66 6 6 6 6 6 67 77 7 7 7 7 7 7 777 7 7 7 7 7 7 7888 8 88 88 8 8 8 8 8 8 8 8 8 8 8 8 88p participant paverage baseline(b) Grammar induction resultsFigure 4: Learning curves viz.
word finding accuracy (left) and grammar induction I chunk F-score (right).This is also likely given the complexity of thelearning task: In Patience, about half of the spokencommands pertains to dealcard frames, whichmeans it is very likely that some slot-value pairshave never even occurred in the training data, evenafter 200 spoken commands.
We expect, however,that we need at least a few repetitions of each slot-value pair to build a robust acoustic representation:the accuracy of correctly detecting the dealcardframe, which has many repetitions in the trainingdata, is close to 100% for all speakers.
Given suchdata scarcity, the fact that we obtain accuracies up to76% is encouraging.Another observation that can be made is that forsome speakers, such as speaker 1, there is a largervariation between consecutive training sizes - for ex-ample for speaker 1 the best accuracy is obtained fora training size of 175 spoken commands.
There areseveral possible reasons.
For one, even though theNMF learning problem is initialized using the con-straints imposed by the frame labeling, the factor-ization process may not achieve the global optimalsolution during training.
This could be addressed byperforming multiple experiments with different ran-dom initializations (Driesen et al, 2012).Another issue is that the number of dealcardframes varies between speakers, due to the rela-tively small test set size of fifty spoken utterances.With the dealcard typically recognized correctly,this may account both for some of the differencesbetween speakers, as well as for the variation be-tween training sizes observed for some speakers: Ifthe number of movecard frames in the test set issmall, this makes the average accuracy more sen-sitive to errors on these frames.
This issue couldbe addressed by an alternative evaluation scheme inwhich multiple occurrences of the same utteranceare only counted once.4.2 Grammar inductionFig.
4b displays the learning curves for the super-vised concept tagging experiments.
There is a largeamount of variation between the participants in ac-curacy using the first 100 training utterances.
Six outof eight curves reach 95% or more with 130 train-ing utterances, and level off after that.
For two par-ticipants, the accuracies reach 100%, with trainingset sizes of 40 and 100 utterances respectively.
Thebaseline accuracies, averaged across all participants,are also shown in Fig.
4b.
These are significantly su-perseded by the individual learning curves with op-timized features, showing that the use of context in-40formation is important to enable successful concepttagging on this dataset.The fact that the tag accuracy for participant 6remains relatively low (around 88%) is mainly dueto a rather high level of inconsistency and ambigu-ity in the command structures that were used.
Oneremarkable source of errors in this case is a struc-ture repeatedly occurring in the test set and occur-ring only twice in the largest training set.
It is a par-ticularly difficult one: a structure in which multiplecards are specified to be moved (in one pile), suchas in ?the black three, the red four and the black fiveto the red six?.
In such cases, only the highest cardof the moved pile (black five in the example) shouldbe labeled with I FS and I FV tags (since only thatcard is represented in the action frame) and the lowercards should be tagged with O tags.The commands given by participants 2 and 5 werestructurally very consistent throughout the games,resulting in very fast learning.
Participant 5?s learn-ing curve reaches a tag accuracy of 100% usingas little as forty training utterances, underlining thelearnability of this task in optimal conditions.
Par-ticipant 3?s curve reaches 100% accuracy, but hasa dip at the beginning of the curve.
This is due tothe fact that in the utterance numbers 20-50, the suitspecification was often dropped (e.g.
?the three onthe four?
), whereas in the utterances before and afterthat, the suit specification was often included.5 Conclusions and future workIn this paper, we introduced a self-learning frame-work for a vocal interface that can be used with anylanguage, dialect, vocabulary and grammar.
In ad-dition to a description of the overall learning frame-work and its internal knowledge representation, wedescribed the two building blocks that will providevocabulary learning and grammar induction.
Ourexperiments show encouraging results, both for vo-cabulary learning and grammar induction, when ap-plied to the very challenging task of a vocally guidedcard game, Patience, with only limited training data.Although the word finding experiments use the or-acle action frames rather than the automatically gen-erated frames as supervision information, the pre-liminary experiments shown in this work are promis-ing enough to have confidence that even with thisadditional source of uncertainty, the goal of a self-learning vocal interface is feasible.
The concept tag-ging experiments show that this type of representa-tion is learnable in a supervised way with a high de-gree of accuracy on the basis of a relatively limitedamount of data.Future experiments will investigate how unsuper-vised learning techniques can be used to bootstrapconcept tagging without using annotated and manu-ally transcribed data.
This will enable the output ofthe grammar module to replace the manually craftedgrammar currently used by the word finding mod-ule.
Since the learning curves for the word findingmodule still show significant room for improvement,more data will need to be collected to adequately in-vestigate the interaction between the two modules.We expect the word finding results to improveonce speaker-specific grammars, provided by thegrammar induction module, can be incorporated.The hand-crafted grammar employed in the wordfinding experiments include almost all variations,while a speaker-specific grammar will typically bemore restrictive.
Another practical approach to im-prove the user experience is to have the ALADIN sys-tem produce an ordered set of several possible framedescriptions, based on the knowledge of the playingfield and the rules of the game.
Preliminary experi-ments revealed that even with a small ordered set ofonly five frame candidates, the slot-value accuracyof the Patience word finding experiments increasedby 10% to 20% absolute.
Furthermore, we expectthe number of repetitions needed for each slot-valuepair to reduce substantially if we allow sharing ofthe acoustic representations between slots.
For ex-ample, it is very likely that the user will refer to thesuit of ?hearts?
the same way, regardless of whetherit occurs in a from slot or in a to slot.While the self-learning modules have not yet beenintegrated and while there is still ample room for im-provement within each module individually, the re-sults of the feasibility experiments described in thispaper are encouraging.
The insights gained fromthese experiments form a solid basis for further ex-perimentation and will serve to further streamlinethe development of a language independent, self-learning command & control vocal interface for peo-ple with a physical impairment.AcknowledgmentsThis research was funded by IWT-SBO grant100049 (ALADIN).41ReferencesR.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2461?2505.W.
Daelemans, J. Zavrel, A. van den Bosch, and K. Vander Sloot.
2010.
MBT: Memory-based tagger, version3.2, reference guide.
Technical Report 10-04, Univer-sity of Tilburg.C.
de Marcken.
1999.
On the unsupervised induc-tion of phrase-structure grammars.
In S. Armstrong,K.
Church, P. Isabelle, S. Manzi, E. Tzoukermann, andD.
Yarowsky, editors, Natural Language ProcessingUsing Very Large Corpora, volume 11 of Text, Speechand Language Technology, pages 191?208.
KluwerAcademic Publishers.J.
Driesen and H. Van hamme.
2012.
Fast wordacquisition in an NMF-based learning framework.In Proceedings of the 36th International Conferenceon Acoustics, Speech and Signal Processing, Kyoto,Japan.J.
Driesen, J.F.
Gemmeke, and H. Van hamme.
2012.Weakly supervised keyword learning using sparse rep-resentations of speech.
In Proceedings of the 36th In-ternational Conference on Acoustics, Speech and Sig-nal Processing, Kyoto, Japan.D.
Klein.
2005.
The Unsupervised Learning of NaturalLanguage Structure.
Ph.D. thesis, Stanford Univer-sity.D.D.
Lee and H.S.
Seung.
1999.
Learning the parts ofobjects by nonnegative matrix factorization.
Nature,401:788?791.J.
Noyes and C. Frankish.
1992.
Speech recognitiontechnology for individuals with disabilities.
Augmen-tative and Alternative Communication, 8(4):297?303.L.A.
Ramshaw and M.P.
Marcus.
1995.
Text chunkingusing transformation-based learning.
In Proceedingsof the Third ACL Workshop on Very Large Corpora,pages 82?94, Cambridge, USA.H.
Van hamme.
2008.
Hac-models: a novel approach tocontinuous speech recognition.
In Proceedings Inter-national Conference on Spoken Language Processing,pages 2554?2557, Brisbane, Australia.Y.
Wang, L. Deng, and A. Acero.
2005.
An introductionto statistical spoken language understanding.
IEEESignal Processing Magazine, 22(5):16?31.42
