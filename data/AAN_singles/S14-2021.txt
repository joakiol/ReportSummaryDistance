Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 145?148,Dublin, Ireland, August 23-24, 2014.BUAP: Evaluating Compositional Distributional Semantic Models on FullSentences through Semantic Relatedness and Textual EntailmentSau?l Leo?n, Darnes Vilarin?o, David Pinto, Mireya Tovar, Beatriz Beltra?nBeneme?rita Universidad Auto?noma de PueblaFaculty of Computer Science14 Sur y Av.
San Claudio, CUPuebla, Puebla, Me?xico{saul.leon,darnes,dpinto,mtovar,bbeltran}@cs.buap.mxAbstractThe results obtained by the BUAP team atTask 1 of SemEval 2014 are presented in thispaper.
The run submitted is a supervised ver-sion based on two classification models: 1)We used logistic regression for determiningthe semantic relatedness between a pair ofsentences, and 2) We employed support vec-tor machines for identifying textual entailmentdegree between the two sentences.
The be-haviour for the second subtask (textual entail-ment) obtained much better performance thanthe one evaluated at the first subtask (related-ness), ranking our approach in the 7th positionof 18 teams that participated at the competi-tion.1 IntroductionThe Compositional Distributional Semantic Models(CDSM) applied to sentences aim to approximatethe meaning of those sentences with vectors summa-rizing their patterns of co-occurrence in corpora.
Inthe Task 1 of SemEval 2014, the organizers aimedto evaluate the performance of this kind of modelsthrough the following two tasks: semantic related-ness and textual entailment.
Semantic relatednesscaptures the degree of semantic similarity, in thiscase, between a pair of sentences, whereas textualentailment allows to determine the entailment rela-tion holding between two sentences.This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/This document is a description paper, therefore,we focus the rest of it on the features and models weused for carrying out the experiments.
A completedescription of the task and the dataset used are givenin Marelli et al.
(2014a) and in Marelli et al.
(2014b),respectively.The remaining of this paper is structured as fol-lows.
In Section 2 we describe the general modelwe used for comparing two sentences and the set ofthe features used for constructing the vectorial rep-resentation for each sentence.
Section 3 shows howwe integrate the features calculated in a single vectorwhich fed a supervised classifier aiming to constructa classication model that solves the two aforemen-tioned problems: semantic relatedness and textualentailment.
In the same section we show the ob-tained results.
Finally, in Section 4 we present ourfindings.2 Description of the DistributionalSemantic Model UsedGiven a sentence S = w1w2?
?
?w|S|, with wia sen-tence word, we have calculated different correlatedterms (ti,j) or a numeric vector (Vi) for each wordwias follows:1.
{ti,j|relation(ti,j, wi)} such as ?relation?
isone the following dependency relations: ?ob-ject?, ?subject?
or ?property?.2.
{ti,j|ti,j= ck?
?
?
ck+n} with n = 2, ?
?
?
, 5, andck?
wi; these tokens are also known as n-grams of length n.3.
{ti,j|ti,j= ck?
?
?
ck+((n?1)?r)} with n =1452, ?
?
?
, 5, r = 2, ?
?
?
, 5, and ck?
wi; these to-kens are also known as skip-grams of lengthn.4.
Viis obtained by applying the Latent SemanticAnalysis (LSA) algorithm implemented in theR software environment for statistical comput-ing and graphics.
Viis basically a vector of val-ues that represent relation of the word wiwithit context, calculated by using a corpus con-structed by us, by integrating information fromEuroparl, Project-Gutenberg and Open OfficeThesaurus.3 A Classification Model for SemanticRelatedness and Textual Entailmentbased on DSMOnce each sentence has been represented by meansof a vectorial representation of patterns, we con-structed a single vector, ?
?u , for each pair of sen-tences with the aim of capturing the semantic relat-edness on the basis of a training corpus.The entries of this representation vector are calcu-lated by obtaining the semantic similarity betweeneach pair of sentences, using each of the DSMshown in the previous section.
In order to calcu-late each entry, we have found the maximum similar-ity between each word of the first sentence with re-spect to the second sentence and, thereafter, we haveadded all these values, thus, ?
?u = {f1, ?
?
?
, f9}.Given a pair of sentences S1=w1,1w2,1?
?
?w|S1|,1and S2= w1,2w2,2?
?
?w|S2|,2,such as each wi,kis represented according to thecorrelated terms or numeric vectors establishedat Section 2, the entry fiof ?
?u is calculatedas: fl=?|S1|i=1max{sim(wi,1, wj,2)}, withj = 1, ?
?
?
, |S2|.The specific similarity measure (sim()) and thecorrelated term or numeric vector used for each flisdescribed as follows:1. f1: wi,kis the ?object?
of wi(as definedin 2), and sim() is the maximum similar-ity obtained by using the following six Word-Net similarity metrics offered by NLTK: Lea-cock & Chodorow (Leacock and Chodorow,1998), Lesk (Lesk, 1986), Wu & Palmer (Wuand Palmer, 1994), Resnik (Resnik, 1995), Lin(Lin, 1998), and Jiang & Conrath1 (Jiang andConrath, 1997).2. f2: wi,kis the ?subject?
of wi, and sim() isthe maximum similarity obtained by using thesame six WordNet similarity metrics.3.
f3: wi,kis the ?property?
of wi, and sim() isthe maximum similarity obtained by using thesame six WordNet similarity metrics.4.
f4: wi,kis an n-gram containing wi, and sim()is the cosine similarity measure.5.
f5: wi,kis an skip-gram containing wi, andsim() is the cosine similarity measure.6.
f6: wi,kis numeric vector obtained with LSA,and sim() is the Rada Mihalcea semantic sim-ilarity measure (Mihalcea et al., 2006).7. f7: wi,kis numeric vector obtained with LSA,and sim() is the cosine similarity measure.8.
f8: wi,kis numeric vector obtained with LSA,and sim() is the euclidean distance.9.
f9: wi,kis numeric vector obtained with LSA,and sim() is the Chebyshev distance.All these 9 features were introduced to a logisticregression classifier in order to obtain a classifica-tion model which allows us to determine the value ofrelatedness between a new pair of sentences2.
Here,we use as supervised class, the value of relatednessgiven to each pair of sentences on the training cor-pus.The obtained results for the relatedness subtaskare given in Table 1.
In columns 2, 3 and 5, a largevalue signals a more efficient system, but a largeMSE (column 4) means a less efficient system.
Ascan be seen, our run obtained the rank 12 of 17, withvalues slightly below the overall average.3.1 Textual EntailmentIn order to calculate the textual entailment judgment,we have enriched the vectorial representation previ-ously mentioned with synonyms, antonyms and cue-1Natural Language Toolkit of Python; http://www.nltk.org/2We have employed the Weka tool with the default settingsfor this purpose146Table 1: Results obtained at the substask ?Relatedness?
of the Semeval 2014 Task 1TEAM ID PEARSON SPEARMAN MSE RankECNU run1 0.82795 0.76892 0.32504 1StanfordNLP run5 0.82723 0.75594 0.32300 2The Meaning Factory run1 0.82680 0.77219 0.32237 3UNAL-NLP run1 0.80432 0.74582 0.35933 4Illinois-LH run1 0.79925 0.75378 0.36915 5CECL ALL run1 0.78044 0.73166 0.39819 6SemantiKLUE run1 0.78019 0.73598 0.40347 7CNGL run1 0.76391 0.68769 0.42906 8UTexas run1 0.71455 0.67444 0.49900 9UoW run1 0.71116 0.67870 0.51137 10FBK-TR run3 0.70892 0.64430 0.59135 11BUAP run1 0.69698 0.64524 0.52774 12UANLPCourse run2 0.69327 0.60269 0.54225 13UQeResearch run1 0.64185 0.62565 0.82252 14ASAP run1 0.62780 0.59709 0.66208 15Yamraj run1 0.53471 0.53561 2.66520 16asjai run5 0.47952 0.46128 1.10372 17overall average 0.71876 0.67159 0.63852 8-9Our difference against the overall average -2% -3% 11% -words (?no?, ?not?, ?nobody?
and ?none?)
for de-tecting negation at the sentences3 .
Thus, if some ofthese new features exist on the training pair of sen-tences, we add a boolean value of 1, otherwise weset the feature to zero.This new set of vectors is introduced to a supportvector machine classifier4, using as class the textualentailment judgment given on the training corpus.The obtained results for the textual entailmentsubtask are given in Table 2.
Our run obtained therank 7 of 18, with values above the overall average.We consider that this improvement over the related-ness task was a result of using other features thatare quite important for semantic relatedness, suchas lexical relations (synonyms and antonyms), andthe consideration of the negation phenomenon in thesentences.4 ConclusionsThis paper describes the use of compositional distri-butional semantic models for solving the problems3Synonyms were extracted from WordNet, whereas theantonyms were collected from Wikipedia.4Again, we have employed the weka tool with the defaultsettings for this purpose.of semantic relatedness and textual entailment.
Weproposed different features and measures for thatpurpose.
The obtained results show a competitiveapproach that may be further improved by consider-ing more lexical relations or other type of semanticsimilarity measures.In general, we obtained the 7th place in the officialranking list from a total of 18 teams that participatedat the textual entailment subtask.
The result at thesemantic relatedness subtask could be improved ifwe were considered to add the new features takeninto consideration at the textual entailment subtask,an idea that we will implement in the future.ReferencesJay J. Jiang and David W. Conrath.
Semantic simi-larity based on corpus statistics and lexical taxon-omy.
In Proc of 10th International Conferenceon Research in Computational Linguistics, RO-CLING?97, pages 19?33, 1997.Claudia Leacock and Martin Chodorow.
Combin-ing local context and wordnet similarity for wordsense identification.
In Christiane Fellfaum, edi-tor, MIT Press, pages 265?283, 1998.147Table 2: Results obtained at the substask ?Textual Entailment?
of the Semeval 2014 Task 1TEAM ID ACCURACY RankIllinois-LH run1 84.575 1ECNU run1 83.641 2UNAL-NLP run1 83.053 3SemantiKLUE run1 82.322 4The Meaning Factory run1 81.591 5CECL ALL run1 79.988 6BUAP run1 79.663 7UoW run1 78.526 8CDT run1 77.106 9UIO-Lien run1 77.004 10FBK-TR run3 75.401 11StanfordNLP run5 74.488 12UTexas run1 73.229 13Yamraj run1 70.753 14asjai run5 69.758 15haLF run2 69.413 16CNGL run1 67.201 17UANLPCourse run2 48.731 18Overall average 75.358 11-12Our difference against the overall average 4.31% -Michael Lesk.
Automatic sense disambiguation us-ing machine readable dictionaries: How to tell apine cone from an ice cream cone.
In Proceed-ings of the 5th Annual International Conferenceon Systems Documentation, pages 24?26.
ACM,1986.Dekang Lin.
An information-theoretic definition ofsimilarity.
In Proceedings of the Fifteenth Inter-national Conference on Machine Learning, ICML?98, pages 296?304, San Francisco, CA, USA,1998.
Morgan Kaufmann Publishers Inc.Marco Marelli, Luisa Bentivogli, Marco Baroni,Raffaella Bernardi, Stefano Menini, and RobertoZamparelli.
Semeval-2014 task 1: Evaluation ofcompositional distributional semantic models onfull sentences through semantic relatedness andtextual entailment.
In Proceedings of the 8thInternational Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland, 2014a.Marco Marelli, Stefano Menini, Marco Baroni,Luisa Bentivogli, Raffaella Bernardi, and RobertoZamparelli.
A sick cure for the evaluation ofcompositional distributional semantic models.
InProceedings of LREC 2014, Reykjavik, Iceland,2014b.Rada Mihalcea, Courtney Corley, and Carlo Strap-parava.
Corpus-based and knowledge-based mea-sures of text semantic similarity.
In Proceedingsof the 21st National Conference on Artificial In-telligence, pages 775?780, 2006.Philip Resnik.
Using information content to evalu-ate semantic similarity in a taxonomy.
In Proceed-ings of the 14th International Joint Conference onArtificial Intelligence, IJCAI?95, pages 448?453,San Francisco, CA, USA, 1995.Zhibiao Wu and Martha Stone Palmer.
Verb seman-tics and lexical selection.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics, pages 133?138, 1994.148
