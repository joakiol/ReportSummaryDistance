A Two-Stage Approach to Retrieving Answers for How-ToQuestionsLing YinCMIS, University of Brighton,Brighton, BN2 4GJ, United KingdomY.Ling@brighton.ac.ukAbstractThis paper addresses the problem ofautomatically retrieving answers forhow-to questions, focusing on those thatinquire about the procedure forachieving a specific goal.
For suchquestions, typical information retrievalmethods, based on key word matching,are better suited to detecting the contentof the goal (e.g., ?installing a WindowsXP server?)
than the general nature of thedesired information (i.e., procedural, aseries of steps for achieving this goal).We suggest dividing the process ofretrieving answers for such questionsinto two stages, with each stage focusingon modeling one aspect of a how-toquestion.
We compare the two-stageapproach with two alternativeapproaches: a baseline approach thatonly uses the content of the goal toretrieve relevant documents and anotherapproach that explores the potential ofautomatic query expansion.
The result ofthe experiment shows that the two-stageapproach significantly outperforms thebaseline but achieves similar result withthe systems using automatic queryexpansion techniques.
We analyze thereason and also present some future work.1 IntroductionHow-To questions constitute a large proportionof questions on the Web.
Many how-to questionsinquire about the procedure for achieving aspecific goal.
For such questions, typicalinformation retrieval (IR) methods, based on keyword matching, are better suited to detecting thecontent of the goal (e.g., installing a WindowsXP server) than the general nature of the desiredinformation (i.e., procedural, a series of steps forachieving this goal).
The reasons are given asbelow.First, documents that describe a procedureoften do not contain the word ?procedure?
itself,but we are able to abstract the concept?procedure?
from cues such as ?first?, ?next?
and?then?, all of which indicate sequentialrelationships between actions.
Secondly, Weexpect that the word ?procedure?
or the phrase?how to?
will occur in a much broader contextthan the words in the goal.
In other words, adocument that contains the words in the goal ismore likely to be relevant than a document thatcontains the word ?procedure?
or the phrase ?howto?.
Without noticing this difference, treating thetwo parts equally in the retrieving process willget many noisy documents.Many information requests seem to show sucha structure, with one part identifying a specifictopic and another part constraining the kind ofinformation required about this topic (Yin andPower, 2005).
The second part is often omittedwhen selecting retrieval terms from the request toconstruct an effective query for an IR system,such as in Picard (1999).The first point given above suggests that usingcues such as ?first?
and ?next?
to expand theinitial query may help in retrieving more relevantdocuments.
Expansion terms can be generatedautomatically by query expansion techniques.The typical process is: (1) use the initial query toretrieve documents (referred to as the first roundof retrieval); (2) consider a few top rankeddocuments as relevant and the rest irrelevant; (3)compare the relevant set with the irrelevant set toextract a list of most distinctive terms; (4) use theextracted terms to retrieve documents (referred toas the second round of retrieval).However, query expansion may not constitutea good solution, because its effectiveness largely63depends on the quality of the few top rankeddocuments retrieved in the first round when theaforementioned two problems are not yettackled.Our solution is to divide the process ofretrieving answers for such questions into twostages: (1) use typical IR approaches forretrieving documents that are relevant to thespecific goal; (2) use a text categorizationapproach to re-rank the retrieved documentsaccording to the proportion of procedural textthey contain.
By ?procedural text?
we refer toordered lists of steps, which are very common insome instructional genres such as online manuals.In this report, we will briefly introduce thetext categorization approach (details arepresented in (Yin and Power, 2006) ) and willexplain in more concrete terms how it isintegrated into the two-stage architectureproposed above.
We will compare theperformance of our two-stage architecture with abaseline system that uses only the content of thegoal to retrieve relevant documents (equivalentto the first stage in the two-stage architecture).We will also compare the two-stage approachwith systems that applies automatic queryexpansion techniques.This paper is organized as follows.
Section 2introduces some relevant work in IR andquestion answering (QA).
Section 3 talks aboutthe text categorization approach for rankingprocedural documents, covering issues such asthe features used, the training corpus, the designof a classification model as well as someexperiments for evaluation.
Section 4 talks aboutintegrating the text categorizer into the two-stagearchitecture and presents some experiments onretrieving relevant documents for how-toquestions.
Section 5 provides a short summaryand presents some future work.2 Related WorkThe idea of applying text categorizationtechnology to help information retrieval is notnew.
In particular, text categorization techniquesare widely adopted to filter a document sourceaccording to specific information needs.
Forexample, Stricker et al (2000) experiment onseveral news resources to find news addressingspecific topics.
They present a method forautomatically generating ?discriminant terms?
(Stricker et al, 2000) for each topic that are thenused as features to train a neural networkclassifier.
Compared to these approaches, thenovelty of our study lies in the idea that aninformation request consists of two differentparts that should be retrieved in different waysand the whole retrieval process should adopt atwo-stage architecture.A research area that is closely related to IR isquestion answering (QA), the differences beinga) the input of a QA system is a question ratherthan a few key words; b) a QA system aims toextract answers to a question rather thanretrieving relevant documents only.
Most QAsystems do adopt a two-stage architecture (if notconsider the initial question analysis stage), i.e.,perform IR with a few content words extractedfrom the query to locate documents likely tocontain an answer and then use informationextraction (IE) to find the text snippets thatmatch the question type (Hovy et al, 2001;Elworthy, 2000).
However, most questionanswering systems target factoid questions ?
theresearch of non-factoid questions started only afew years ago but limited to several kinds, suchas definitional questions (Xu et al, 2003) andquestions asking for biographies (Tsur et al,2004).Only a few studies have addressed proceduralquestions.
Murdok and Croft (2002) distinguishbetween ?task-oriented questions?
(i.e., ask abouta process) and ?fact-oriented questions?
(i.e., askabout a fact) and present a method toautomatically classify questions into these twocategories.
Following this work, Kelly et al(2002) explore the difference between documentsthat contain relevant information to the twodifferent types of questions.
They conclude,?lists and FAQs occur in more documents judgedrelevant to task-oriented questions than thosejudged relevant to fact-oriented questions?
(Kellyet al, 2002: 645) and suggest, ?retrievaltechniques specific to each type of questionshould be considered?
(Kelly et al, 2002: 647).Schwitter et al (2004) present a method toextract answers from technical documentationsfor How-questions.
To identify answers, theymatch the logical form of a sentence against thatof the question and also explore thetypographical conventions in technical domains.The work that most resembles ours is Takechi etal.
(2003), which uses word n-grams to classify(as procedural or non-procedural) list passagesextracted using HTML tags.
Our approach,however, applies to whole documents, the aimbeing to measure the degree of procedurality ?i.e., the proportion of procedural text theycontain.643 Ranking Procedural TextsThree essential elements of a text categorizationapproach are the features used to represent thedocument, the training corpus and the machinelearning method, which will be described insection 3.1, 3.2 and 3.3 respectively.
Section 3.4presents experiments on applying the learnedmodel to rank documents in a small test set.3.1 Feature Selection and DocumentRepresentationLinguistic Features and Cue PhrasesWe targeted six procedural elements: actions,times, sequence, conditionals, preconditions, andpurposes.
These elements can be recognizedusing linguistic features or cue phrases.
Forexample, an action is often conveyed by animperative; a precondition can be expressed bythe cue phrase ?only if?.
We used all thesyntactic and morphological tags defined inConnexor?s syntax analyzer 1 .
There are someredundant tags in this set.
For example, both thesyntactic tag ?@INFMARK>?
and themorphological tag ?INFMARK>?
refer to theinfinitive marker ?to?
and therefore always occurtogether at the same time.
We calculated thePearson?s product-moment correlationcoefficient (r) (Weisstein, 1999) between anytwo tags based on their occurrences in sentencesof the whole training set.
We removed one ineach pair of strongly correlated tags and finallygot 34 syntactic tags and 34 morphological tags.We also handcrafted a list of relevant cuephrases (44), which were extracted fromdocuments by using the Flex tool 2  for patternmatching.
Some sample cue phrases and thematching patterns are shown in table 1.ProceduralElementCue Phrase PatternPrecondition ?only if?
[Oo]nly[[:space:]]if[[:space:]]Purpose ?so that?
[sS]o[[:space:]]that[[:space:]]Condition ?as long as?
([Aa]s) [[:space:]]long[[:space:]]as[[:space:]]Sequence ?first?
[fF]irst [[:space:][:punct:]]Time ?now?
[nN]ow[[:space:][:punct:]]Table 1.
Sample cue phrases and matchingpatterns.Modeling Inter-Sentential Feature Co-occurrenceSome cue phrases are ambiguous and thereforecannot reliably suggest a procedural element.For example, the cue phrase ?first?
can be used to1 Refer to http://www.connexor.com/2 Refer to http://www.gnu.org/software/flex/flex.htmlrepresent a ranking order or a spatial relationshipas well as a sequential order.
However, it is morelikely to represent a sequential order betweenactions if there is also an imperative in the samesentence.
Indeed, sentences that contain both anordinal number and an imperative are veryfrequent in procedural texts.
We comparedbetween the procedural training set and the non-procedural training set to extract distinctivefeature co-occurrence patterns, each of which hasonly 2 features.
The formulae used to rankpatterns with regard to their distinctiveness canbe found in (Yin and Power, 2006).Document RepresentationEach document was represented as a vector{ }Njjjj xxxd ,...,, 21= , where ijx  represents thenumber of sentences in the document thatcontains a particular feature normalized by thedocument length.
We compare the effectivenessof using individual features ( ijx  refers to either asingle linguistic feature or a cue phrases) and ofusing feature co-occurrence patterns ( ijx refers toa feature co-occurrence pattern).3.2 Corpus PreparationPagewise 3  provides a list of subject-matterdomains, ranging from household issues to artsand entertainment.
We downloaded 1536documents from this website (referred tohereafter as the Pagewise collection).
We thenused some simple heuristics to select documentsfrom this set to build the initial training corpus.Specifically, to build the procedural set we chosedocuments with titles containing key phrases?how to?
and ?how can I?
(209 web documents);to build the non-procedural set, we chosedocuments which did not include these phrasesin their titles, and which also had no phrases like?procedure?
and ?recipe?
within the body of thetext (208 web documents).Samples drawn randomly from the proceduralset (25) and non-procedural set (28) weresubmitted to two human judges, who assignedprocedurality scores from 1 (meaning noprocedural text at all) to 5 (meaning over 90%procedural text).
The Kendall tau-b agreement(Kendall, 1979) between the two rankings was0.821.
Overall, the average scores for theprocedural and non-procedural samples were3.15 and 1.38.
We used these 53 sampledocuments as part of the test set and the3 Refer to http://www.essortment.com65remaining documents as the initial training set(184 procedural and 180 non-procedural).This initial training corpus is far from ideal:first, it is small in size; a more serious problem isthat many positive training examples do notcontain a major proportion of procedural text.
Inour experiments, we used this initial training setto bootstrap a larger training set.3.3 Learning MethodAlthough shown to be not so effective in someprevious studies (Yang, 1999; Yang and Liu,1999), Naive Bayes classifier is one of the mostcommonly-used classifiers for textcategorization.
Here we introduce a modeladapted from the Naive Bayes classifier from theweka-3-4 package (Witten and Frank, 2000).The Naive Bayes classifier scores a documentjd  according to whether it is a typical memberof its set ?
i.e., the probability of randomlypicking up a document like it from theprocedural class ( ( )proceduralCdp j =| ).
Thisprobability is estimated from the training corpus.As mentioned before, the average proceduralscore of the procedural training set is low.Therefore, there is obviously a danger that a trueprocedural document will be ranked lower than adocument that contains less procedural textswhen using this training set to train a NaiveBayes classifier.
Although our proceduraltraining set is not representative of theprocedural class, by comparing it with the non-procedural training set, we are able to tell thedifference between procedural documents andnon-procedural documents.
We adapted theNaive Bayes classifier to focus on modeling thedifference between the two classes.
For example,if the procedural training set has a higheraverage value on feature Xi than the non-procedural training set, we inferred that adocument with a higher feature value on Xishould be scored higher.
To reflect this rule, wescored a document jd  by the probability ofpicking a document with a lower feature valuefrom the procedural class (i.e.,)|( proceduralCxXp iji =< ).
Again thisprobability is estimated from the training set.The new model will be referred to hereafter asthe Adapted Naive Bayes classifier.
The detailsof this new model can be found in (Yin andPower, 2006).3.4 Experiments on Ranking ProceduralTextsThere are two sources from which we compiledthe training and testing corpora: the Pagewisecollection and the SPIRIT collection.
TheSPIRIT collection contains a terabyte of HTMLthat are crawled from the web starting from aninitial seed set of a few thousands universitiesand other educational organizations (Clarke etal., 1998).Our test set contained 103 documents,including the 53 documents that were sampledpreviously and then separated from the initialtraining corpus, another 30 documents randomlychosen from the Pagewise collection and 20documents chosen from the SPIRIT collection.We asked two human subjects to score theprocedurality for these documents, following thesame instruction described in section 3.2.
Thecorrelation coefficient (Kendall tau-b) betweenthe two rankings was 0.725, which is the upperbound of the performance of the classifiers.We first used the initial training corpus tobootstrap a larger training set (378 proceduraldocuments and 608 non-procedural documents),which was then used to select distinctive featureco-occurrence patterns and to train differentclassifiers.
We compared the Adapted NaiveBayes classifier with the Naive Bayes classifierand three other classifiers, including MaximumEntropy (ME) 4 , Alternating Decision Tree(ADTree) (Freund and Mason, 1999) and LinearRegression (Witten and Frank, 2000).          6XE6XEFigure 1.
Ranking results using individualfeatures: 1 refers to Adapted Naive Bayes, 2refers to Naive Bayes, 3 refers to ME, 4 refers toADTree and 5 refers to Linear Regression.Ranking Method Agreementwith Subject 1Agreementwith Subject 2AverageAdapted Naive Bayes 0.270841 0.367515 0.319178Naive Bayes 0.381921 0.464577 0.423249ME 0.446283 0.510926 0.4786054 Refer tohttp://homepages.inf.ed.ac.uk/s0450736/maxent.html66ADTree 0.371988 0.463966 0.417977Linear Regression 0.497395 0.551597 0.524496Table 2.
Ranking results using individualfeatures.          6XE6XEFigure 2.
Ranking results using feature co-occurrence patterns: 1 refers to Adapted NaiveBayes, 2 refers to Naive Bayes, 3 refers to ME, 4refers to ADTree and 5 refers to LinearRegression.Ranking Method Agreementwith Subject 1Agreementwith Subject 2AverageAdapted Naive Bayes 0.420423 0.513336 0.466880Naive Bayes 0.420866 0.475514 0.44819ME 0.414184 0.455482 0.434833ADTree 0.358095 0.422987 0.390541Linear Regression 0.190609 0.279472 0.235041Table 3.
Ranking results using feature co-occurrence patterns.Figure 1 and table 2 show the Kendall tau-bcoefficient between human subjects?
rankingresults and the trained classifiers?
ranking resultsof the test set when using individual features(112); Figure 2 and table 3 show the Kendalltau-b coefficient when using feature co-occurrence patterns (813).As we can see from the above figures, whenusing individual features, Linear Regressionachieved the best result, Adapted Naive Bayesperformed the worst, Naive Bayes, ME andADTree were in the middle; however, whenusing feature co-occurrence patterns, the orderalmost reversed, i.e., Adapted Naive Bayesperformed the best and Linear Regression theworst.
Detailed analysis of the result is beyondthe scope of this paper.
The best model gainedby using feature co-ocurrence patterns (i.e.,Adapted Naive Bayes classifier) and by usingindividual features (i.e., Linear Regressionclassification model) will be used for furtherexperiments on the two-stage architecture.4 Retrieving Relevant Documents forHow-To QuestionsIn this section we will describe the experimentson retrieving relevant documents for how-toquestions by applying different approachesmentioned in the introduction section.4.1 Experiment SetupWe randomly chose 60 how-to questions fromthe query logs of the FA Q finder system (Burkeet al, 1997).
Three judges went through thesequestions and agreed on 10 proceduralquestions5.We searched Google and downloaded 40 topranked documents for each question, which werethen mixed with 1000 web documents from theSPIRIT collection to compile a test set.
The two-stage architecture is as shown in figure 3.
In thefirst stage, we sent only the content of the goal toa state-of-the-art IR model to retrieve 30documents from the test set, which werereranked in the second stage according to thedegree of procedurality by a trained documentclassifier.7HVWVHW ,5PRGHOWRSUDQNHGZHEGRFV'RFXPHQWFODVVLILHUUHUDQNHGZHEGRFV7KHFRQWHQWRIWKHJRDOFigure 3.
A two-stage architecture.We also tried to test how well query expansioncould help in retrieving procedural documents,following a process as shown in figure 4.
First,key words in the content of goal were used toquery an IR model to retrieve an initial set ofrelevant documents, those of which that do notcontain the phrase ?how to?
were then removed.The remaining top ten documents were used togenerate 40 searching terms, which were appliedin the second round to retrieve documents.Finally the 30 top ranked documents werereturned as relevant documents.5 We distinguish questions asking for a series of steps(i.e., procedural questions) from those of which theanswer could be a list of useful hints, e.g., ?how tomake money?.Stage OneStage Two67,5PRGHO 7HVWVHW'RFXPHQWUDQNLQJOLVW,5PRGHOWRSUDQNHGZHEGRFV([WUDFWWHUPVIRUTXHU\H[SDQVLRQ$QH[SDQGHGVHWRITXHU\WHUPV7KHFRQWHQWRIWKHJRDO5HPRYHGRFXPHQWVFRQWDLQLQJQR?KRZWR?WRSUDQNHGZHEGRFVFigure 4.
An alternative architecture using queryexpansion.4.2 IR ModelFor the above-mentioned IR model, we used theBM25 and PL2 algorithms from the Terrier IRplatform6.The BM25 algorithm is one variety of theprobabilistic schema presented in (Robertson etal.
1993).
It has gained much success in TRECcompetitions and has been adopted by manyother TREC participants.The PL2 algorithm, as most other IR modelsimplemented in the Terrier IR platform, is basedon the Divergence From Randomness (DFR)framework.
Amati and Rijsbergen (2004)provide a detailed explanation of this frameworkand a set of term-weighting formulae derived byapplying different models of randomness anddifferent ways to normalize the weight of a termaccording to the document length and accordingto a notion called information gain.
They testthese different formulae in the experiments onretrieving relevant documents for various sets ofTREC topics and show that they achievecomparable result with the BM25 algorithm.We also used the Bo1 algorithm from thesame package to select terms for queryexpansion.
Refer to (Plachouras et al, 2004) fordetails about this algorithm.4.3 ResultWe tested eight systems, which could beorganized into two sets.
The first set uses BM25algorithm as the basic IR model and the secondset uses PL2 as the basic IR model.
Each setincludes four systems: a baseline system thatreturns the result of the first stage in the two-stage architecture, one system that uses queryexpansion technique following the architecturein figure 4 and two systems that apply the two-6 http://ir.dcs.gla.ac.uk/terrier/index.htmlstage architecture (one uses the Adapted NaiveBayes classifier and another one uses the LinearRegression classification model).The mean average precision (MAP) 7  ofdifferent retrieval systems is shown in table 4and figure 5.00.
10.
20.
30.
40.
50.
61 2Basel i neQuer y Expansi onAdapt ed Nai ve BayesLi near  Regr essi onFigure 5.
MAPs of different systems: 1 refers tousing BM25 as the IR model, 2 refers to usingPL2 as the IR model.Model MAPBM25 (Baseline) 0.33692Set1 BM25 + Query Expansion 0.50162BM25 + Adapted Naive Bayes 0.45605BM25 + Linear Regression 0.41597PL2  (Baseline) 0.33265Set2 PL2 + Query Expansion 0.45821PL2 + Adapted Naive Bayes 0.44263PL2 + Linear Regression 0.40218Table 4.
Results of different systems.We can see that in both sets: (1) systems thatadopts the two-stage architecture performedbetter than the baseline system but worse thanthe system that applies query expansiontechnique; (2) the system that uses AdaptedNaive Bayes classifier in the second stage gainedbetter result than the one that uses LinearRegression classification model.
We performed apairwise t-test to test the significance of thedifference between the results of the two systemswith an integrated Adapted Naive Bayesclassifier and of the two baseline systems.
Eachdata set contained 20 figures, with each figurerepresenting the average precision of theretrieving result for one question.
The differenceis significant (p=0.02).
We also performed apairwise t-test to test the significance of thedifference between the two systems with anintegrated Adapted Naive Bayes classifier and of7 The average precision of a single question is themean of the precision scores after each relevantdocument is retrieved.
The mean average precision isthe mean of the average precisions of a collection ofquestions.Round OneRound Two68the two systems using query expansiontechniques.
The difference is not significant(p=0.66).4.4 DiscussionContrary to our expectation, the result of theexperiments showed that the two-stage approachdid not perform better than simply applying aquery expansion technique to generate anexpanded list of querying terms.
An explanationcan be sought from the following two aspects(each of which corresponds to one of the twoproblems mentioned in the first section).First, we expected that many documents thatcontain procedures do not contain the word?procedure?
or the phrase ?how to?.
Therefore, asystem based on key word matching would notbe able to identify such documents.
However,we found that such words or phrases, althoughnot included in the body of the text, often occurin the title of the document.Another problem we pointed out before is thatthe phrase ?how to?
occurs in a much broadercontext than keywords in the content of the goal,therefore, it would bring a lot of irrelevantdocuments when used together with the contentof goal for document retrieval.
However, in ourexperiment, we used the content of the goal toretrieve document first and then removed thosecontaining no phrase ?how to?
(refer to figure 4).This is actually also a two-stage approach initself.Despite the experiment result, a well-knowndefect of query expansion is that it is onlyeffective if relevant documents are similar toeach other while the two-stage approach doesnot have this limitation.
For example, forretrieving documents about ?how to cookherring?, query expansion is only able to retrievetypical recipes while our two-stage approach isable to detect an exotic method as long as it isdescribed as a sequence of steps.5 Summary and Future WorkIn this paper, we suggested that a how-toquestion could be seen as consisting of twoparts: the specific goal and the general nature ofthe desired information (i.e., procedural).
Weproposed a two-stage architecture to retrievedocuments that meet the requirement of bothparts.
We compared the two-stage architecturewith other approaches: one only uses the contentof the goal to retrieve documents (baselinesystem) and another one uses an expanded set ofquery terms obtained by automatic queryexpansion techniques.
The result has shown thatthe two-stage architecture performed better thanthe base line system but did not show superiorityover query expansion techniques.
We provide anexplanation in section 4.4.As suggested in section 1, many informationrequests are formulated as consisting of twoparts.
As a future work, we will test the two-stage architecture for retrieving answers for otherkind of questions.ReferencesAmati, Gianni and Cornelis J. van Rijsbergen.
2002.Probabilistic models of information retrieval basedon measuring the divergence from randomness.ACM Transactions on Information Systems, 20 (4):357-389.Burke, Robin D., Kristian J. Hammond, VladimirKulyukin, Steven L. Lytinen, Noriko Tomuro, andScott Schoenberg.
1997.
Question answering fromfrequently-asked question files: experiences withthe FAQ finder system.
AI Magazine, 18(2): 57-66.Clarke, Charles, Gordan Cormack, M. Laszlo,Thomas Lynam, and Egidio Terra.
1998.
Theimpact of corpus size on question answeringperformance.
In Proceedings of the 25th AnnualInternational ACM SIGIR Conference on Researchand Development in IR, Tampere, Finland.Elworthy, David.
2000.
Question answering using alarge NLP system.
In Proceedings of the Ninth TextRetrieval Conference (TREC-9), pages 355-360.Freund, Yoav and Llew Mason.
1999.
The alternatingdecision tree learning algorithm.
In Proceeding ofthe Sixteenth International Conference on MachineLearning, pages 124-133, Bled, Slovenia.Hovy, Eduard, Laurie Gerber, Ulf Hermjakob,Michael Junk, and Chin-Yew Lin.
2001.
QuestionAnswering in Webclopedia.
In Proceedings of theNinth Text Retrieval Conference (TREC-9), pages655-664.Kelly, Diane, Vanessa Murdock, Xiao-Jun Yuan, W.Bruce Croft, and Nicholas J. Belkin.
2002.
Featuresof documents relevant to task- and fact-orientedquestions.
In Proceedings of the EleventhInternational Conference on Information andKnowledge Management (CIKM '02), pages 645-647, McLean, VA.Kendall, Maurice.
1979.
The Advanced Theory ofStatistics.
Fourth Edition.
Griffin, London.Murdok, Vanessa and Bruce Croft.
2002.
Taskorientation in question answering.
In Proceedingsof SIGIR ?02, pages 355-356, Tampere, Finland.69Picard, Justin.
1999.
Finding content-bearing termsusing term similarities.
In Proceedings of the NinthConference of the European Chapter of theAssociation for Computational Linguistics (EACL1999), pages 241-244, Bergen, Norway.Plachouras, Vassilis, Ben He, and Iadh Ounis.
2004.University of Glasgow at TREC2004: Experimentsin web, robust and terabyte tracks with Terrier.
InProceedings of the Thirteenth Text REtrievalConference (TREC 2004).Robertson, Stephen, Steve Walker, Susan Jones,Micheline Hancock-Beaulieu, and Mike Gatford.1993.
Okapi at TREC-2.
In Proceedings of theSecond Text Retrieval Conference (TREC-2),pages 21-24.Schwitter, Rolf, Fabio Rinaldi, and Simon Clematide.2004.
The importance of how-questions intechnical domains.
In Proceedings of the Question-Answering workshop of TALN 04, Fez, Morocco.Stricker, Mathieu, Frantz Vichot, G?rard Dreyfus,and Francis Wolinski.
2000.
Two steps featureselection and neural network classification for theTREC-8 routing.
CoRR cs.
CL/0007016.Takechi, Mineki, Takenobu Tokunaga, YujiMatsumoto, and Hozumi Tanaka.
2003.
Featureselection in categorizing procedural expressions.
InProceedings of the Sixth International Workshopon Information Retrieval with Asian Languages(IRAL2003), pages 49-56, Sapporo, Japan.Tsur, Oren, Maarten de Rijke, and Khalil Sima'an.2004.
BioGrapher: biography questions as arestricted domain question answering task.
InProceedings ACL 2004 Workshop on QuestionAnswering in Restricted Domains, Barcelona.Weisstein, Eric.
1999.
Correlation Coefficient.MathWorld--A Wolfram Web Resource.
Availableat: <URL: http://mathworld.wolfram.com/CorrelationCoefficient.html> [Accessed 21 Oct2005]Witten, Ian and Eibe Frank.
2000.
Data Mining:Practical Machine Learning Tools with JavaImplementations, Morgan Kaufmann, San Mateo,CA.Xu, Jinxi, Ana Licuanan, and Ralph Weischedel.2003.
TREC2003 QA at BBN: answeringdefinitional questions.
In Proceedings of theTwelfth Text Retrieval Conference (TREC 2003),pages 98-106.Yang, Yi-Ming.
1999.
An evaluation of statisticalapproaches to text categorization.
Journal ofInformation Retrieval 1(1/2): 67-88.Yang, Yi-Ming and Xin Liu.
1999.
A re-examinationof text categorization methods.
In Proceedings ofACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR'99),pages 42-49, Berkeley, CA.Yin, Ling and Richard Power.
2005.
Investigating thestructure of topic expressions: a corpus-basedapproach.
In Proceedings from the CorpusLinguistics Conference Series, Vol.1, No.1,University of Birmingham, Birmingham.Yin, Ling and Richard Power.
2006.
Adapting theNaive Bayes classifier to rank procedural texts.
InProceedings of the 28th European Conference onIR Research (ECIR 2006).70
