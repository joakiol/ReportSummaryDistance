Proceedings of the Fifth Law Workshop (LAW V), pages 92?100,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsProposal for an Extension of Traditional Named Entities:From Guidelines to Evaluation, an OverviewCyril Grouin?, Sophie Rosset?, Pierre Zweigenbaum?Kar?n Fort?,?
, Olivier Galibert?, Ludovic Quintard?
?LIMSI?CNRS, France ?INIST?CNRS, France ?LIPN, France ?LNE, France{cyril.grouin,sophie.rosset,pierre.zweigenbaum}@limsi.frkaren.fort@inist.fr, {olivier.galibert,ludovic.quintard}@lne.frAbstractWithin the framework of the construction of afact database, we defined guidelines to extractnamed entities, using a taxonomy based on anextension of the usual named entities defini-tion.
We thus defined new types of entitieswith broader coverage including substantive-based expressions.
These extended named en-tities are hierarchical (with types and compo-nents) and compositional (with recursive typeinclusion and metonymy annotation).
Humanannotators used these guidelines to annotate a1.3M word broadcast news corpus in French.This article presents the definition and noveltyof extended named entity annotation guide-lines, the human annotation of a global corpusand of a mini reference corpus, and the evalu-ation of annotations through the computationof inter-annotator agreements.
Finally, we dis-cuss our approach and the computed results,and outline further work.1 IntroductionWithin the framework of the Quaero project?a mul-timedia indexing project?we organized an evalu-ation campaign on named entity extraction aimingat building a fact database in the news domain, thefirst step being to define what kind of entities areneeded.
This campaign focused on broadcast newscorpora in French.
While traditional named enti-ties include three major classes (persons, locationsand organizations), we decided to extend the cov-erage of our campaign to new types of entities andto broaden their main parts-of-speech from propernames to substantives, this extension being neces-sary for ever-increasing knowledge extraction fromdocuments.
We thus produced guidelines to specifythe way corpora had to be annotated, and launchedthe annotation process.In this paper, after covering related work (Sec-tion 2), we describe the taxonomy we created (Sec-tion 3) and the annotation process and results (Sec-tion 4), including the corpora we gathered and thetools we developed to facilitate annotation.
We thenpresent inter-annotator agreement measures (Sec-tion 5), outline limitations (Section 6) and concludeon perspectives for further work (Section 7).2 Related work2.1 Named entity definitionsNamed Entity recognition was first defined as recog-nizing proper names (Coates-Stephens, 1992).
SinceMUC-6 (Grishman and Sundheim, 1996; SAIC,1998), named entities have been proper namesfalling into three major classes: persons, locationsand organizations.Proposals were made to sub-divide these entitiesinto finer-grained classes.
The ?politicians?
sub-class was proposed for the ?person?
class by (Fleis-chman and Hovy, 2002) while the ?cities?
subclasswas added to the ?location?
class by (Fleischman,2001; Lee and Lee, 2005).The CONLL conference added a miscellaneoustype that includes proper names falling outside theprevious classes.
Some classes have thus sometimesbeen added, e.g.
the ?product?
class by (Bick, 2004;Galliano et al, 2009).92Specific entities are proposed and handled insome tasks: ?language?
or ?shape?
for question-answering systems in specific domains (Rosset etal., 2007), ?email address?
or ?phone number?
toprocess electronic messages (Maynard et al, 2001).Numeric types are also often described and used.They include ?date?, ?time?, and ?amount?
types(?amount?
generally covers money and percentage).In specific domains, entities such as gene, protein,are also handled (Ohta, 2002), and campaigns are or-ganized for gene detection (Yeh et al, 2005).
At thesame time, extensions of named entities have beenproposed: (Sekine, 2004) defined a complete hierar-chy of named entities containing about 200 types.2.2 Named Entities and AnnotationAs for any other kind of annotation, some aspects areknown to lead to difficulties in obtaining coherencein the manual annotation process (Ehrmann, 2008;Fort et al, 2009).
Three different classes of prob-lems are distinguished: (1) selecting the correct cat-egory in cases of ambiguity, where one entity canfall into several classes, depending on the context(?Paris?
can be a town or a person name); (2) detect-ing the boundaries (in a person designation, is onlythe proper name to be annotated or the trigger ?Mr?too?)
and (3) annotating metonymies (?France?
canbe a sports team, a country, etc.
).In the ACE Named Entity task (Doddington et al,2004), a complex task, the obtained inter-annotatoragreement was 0.86 in 2002 and 0.88 in 2003.
Sometasks obtain better agreement.
Desmet and Hoste(2010) described the Named Entity annotation real-ized within the Sonar project, where Named Entityare clearly simpler.
They follow the MUC NamedEntity definition with the subtypes as proposedby ACE.
The agreement computed over the SonarDutch corpus ranges from 0.91 to 0.97 (kappa val-ues) depending of the emphasized elements (span,main type, subtype, etc.
).3 Taxonomy3.1 Guidelines productionHaving in mind the objective of building a factdatabase through the extraction of named entitiesfrom texts, we defined a richer taxonomy than thoseused in other information extraction works.Following (Bonneau-Maynard et al, 2005; Alexet al, 2010), the annotation guidelines were firstwritten from December 2009 to May 2010 by threeresearchers managing the manual annotation cam-paign.
During guidelines production, we evaluatedthe feasibility of this specific annotation task and theusefulness of the guidelines by annotating a smallpart of the target corpus.
Then, these guidelineswere delivered to the annotators.
They consist of adescription of the objects to annotate, general anno-tation rules and principles, and more than 250 pro-totypical and real examples extracted from the cor-pus (Rosset et al, 2010).
Rules are important to setthe general way annotations must be produced.
Ad-ditionally, examples are essential for human annota-tors to grasp the annotation rationale more easily.Indeed, while producing the guidelines, we knewthat the given examples would never cover all possi-ble cases because of the specificity of language andof the ambiguity of formulations and situations de-scribed in corpora, as shown in (Fort et al, 2009).Nevertheless, guidelines examples must be consid-ered as a way to understand the final objective ofthe annotation work.
Thanks to numerous meetingsfrom May to November 2010, we gathered feedbackfrom the annotators (four annotators plus one anno-tation manager).
This feedback allowed us to clarifyand extend the guidelines in several directions.
Theguidelines are 72 pages long and consist of 3 majorparts: general description of the task and the prin-ciples (25% of the overall document), presentationof each type of named entity (57%), and a simpler?cheat sheet?
(18%).3.2 DefinitionWe decided to use the three general types ofnamed entities: name (person, location, organi-zation) as described in (Grishman and Sundheim,1996; SAIC, 1998), time (date and duration), andquantity (amount).
We then included named entitiesextensions proposed by (Sekine, 2004; Galliano etal., 2009) (respectively products and functions) andwe extended the definition of named entities to ex-pressions which are not composed of proper names(e.g., phrases built around substantives).
The ex-tended named entities we defined are both hierar-chical and compositional.
For example, type pers(person) is split into two subtypes, pers.ind (indi-93Person Functionpers.ind (individualperson)pers.coll (group ofpersons)func.ind (individualfunction)func.coll (collectivityof functions)Location Productadministrative(loc.adm.town,loc.adm.reg,loc.adm.nat,loc.adm.sup)physical(loc.phys.geo,loc.phys.hydro,loc.phys.astro)facilities(loc.fac),oronyms(loc.oro),address(loc.add.phys,loc.add.elec)prod.object(manufac-tured object)prod.serv(transporta-tion route)prod.fin(financialproducts)prod.doctr(doctrine)prod.rule(law)prod.soft(software)prod.art prod.media prod.awardOrganization Timeorg.adm (administra-tion)org.ent (services)Amountamount (with unit or general object), includ-ing durationtime.date.abs(absolute date),time.date.rel (relativedate)time.hour.abs(absolute hour),time.hour.rel (relativehour)Table 1: Types (in bold) and subtypes (in italic)vidual person) and pers.coll (collective person), andpers entities are composed of several components,among which are name.first and name.last.3.3 HierarchyWe used two kinds of elements: types and compo-nents.
The types with their subtypes categorize anamed entity.
While types and subtypes were usedbefore (ACE, 2000; Sekine, 2004; ACE, 2005; Gal-liano et al, 2009), we consider that structuring thecontents of an entity (its components) is importanttoo.
Components categorize the elements inside anamed entity.Our taxonomy is composed of 7 main types(person, function, location, product, organization,amount and time) and 32 subtypes (Table 1).
Typesand subtypes refer to the general category of anamed entity.
They give general information aboutthe annotated expression.
Almost each type is thenspecified using subtypes that either mark an opposi-tion between two major subtypes (individual personvs.
collective person), or add precisions (for exam-ple for locations: administrative location, physicallocation, etc.
).This two-level representation of named entities,with types and components, constitutes a novel ap-proach.Types and subtypes To deal with the intrinsic am-biguity of named entities, we defined two specifictransverse subtypes: 1. other for entities with a dif-ferent subtype than those proposed in the taxon-omy (for example, prod.other for games), and 2. un-known when the annotator does not know which sub-type to use.Types and subtypes constitute the first level of an-notation.
They refer to a general segmentation ofthe world into major categories.
Within these cate-gories, we defined a second level of annotation wecall components.Components Components can be considered asclues that help the annotator to produce an anno-tation: either to determine the named entity type(e.g.
a first name is a clue for the pers.ind namedentity subtype), or to set the named entity bound-aries (e.g.
a given token is a clue for the named en-tity, and is within its scope, while the next token isnot a clue and is outside its scope).
Components aresecond-level elements, and can never be used out-side the scope of a type or subtype element.
An en-tity is thus composed of components that are of twokinds: transverse components and specific compo-nents (Table 2).
Transverse components can be usedin several types of entities, whereas specific compo-nents can only be used in one type of entity.94Transverse componentsname (name of the entity), kind (hyperonym of the entity), qualifier (qualifying adjective), demonym(inhabitant or ethnic group name), demonym.nickname (inhabitant or ethnic group nickname), val(a number), unit (a unit), extractor (an element in a series), range-mark (range between two values),time-modifier (a time modifier).pers.ind loc.add.phys time.date.abs/rel amountname.last, name.first,name.middle, pseudonym,name.nickname, titleaddress-number, po-box,zip-code,other-address-componentweek, day, month, year,century, millennium,reference-eraobjectprod.awardaward-catTable 2: Transverse and specific components3.4 CompositionAnother original point in this work is the compo-sitional nature of the annotations.
Entities can becompositional for three reasons: (i) a type contains acomponent; (ii) a type includes another type, used asa component; and (iii) in cases of metonymy.
Dur-ing the Ester II evaluation campaign, there was anattempt to use compositionality in named entities fortwo categories: persons and functions, where a per-son entity could contain a function entity.<pers.hum> <func.pol> pr?sident </func.pol><pers.hum> Chirac </pers.hum> </pers.hum>Nevertheless, the Ester II evaluation did not takethis inclusion into account and only focused onthe encompassing annotation (<pers.hum> pr?sidentChirac </pers.hum>).
We drew our inspiration fromthis experience, and allowed the annotators and thesystems to use compositionality in the annotations.Cases of inclusion can be found in the functiontype (Figure 1), where type func.ind, which spansthe whole expression, includes type org.adm, whichspans the single word ?budget?.
In this case, we con-sider that the designation of this function (?ministredu budget?)
includes both the kind (?ministre?)
andnouveauqualifierministrekinddu Budgetnameorg.admfunc.ind, Fran?oisname.firstBaroinname.lastpers.indFigure 1: Multi-level annotation of entity types (red tags)and components (blue tags): new minister of budget ,Fran?ois Baroin.the name (?budget?)
of the ministry, which itself istyped as is relevant (org.adm).
Recursive cases ofembedding can be found when a subtype includesanother named entity annotated with the same sub-type (org.ent in Figure 2).le collectifkinddes associationskinddes droits de l' Hommenameprod.ruleau Saharanameloc.phys.geoloc.adm.suporg.entorg.entFigure 2: Recursive embedding of the same subtype:Collective of the Human Rights Organizations in Sahara.Cases of metonymy include strict metonymy (aterm is substituted with another one in a relationof contiguity) and antonomasia (a proper name isused as a substantive or vice versa).
In such cases,the entity must be annotated with both types, first(inside) with the intrinsic type of the entity, then(outside) with the type that corresponds to the re-sult of the metonymy.
Basically, country namescorrespond to ?national administrative?
locations(loc.adm.nat) but they can also designate the admin-istration (org.adm) of the country (Figure 3).depuistime-modifierplusieursvalmoisunitamounttime.date.rel, la Russienameloc.adm.natorg.admFigure 3: Annotation with a metonymic use of country?Russia?
as its government: for several months , Russia...953.5 BoundariesOur definition of the scope of entities excludes rel-ative clauses, subordinate clauses, and interpolatedclauses: the annotation of an entity must end beforethese clauses.
If an interpolated clause occurs insidean entity, its annotation must be split.
Moreover, twodistinct persons sharing the same last name must beannotated as two separate entities (Figure 4); we in-tend to use relations between entities to gather thesesegments in the next step of the project.depuisutmi-oefrlvifr-euail n.s,etuiutmi-oefrlRprveuutmi-strlvifr-euaFigure 4: Separate (coordinated) named entities.4 Annotation process4.1 CorpusWe managed the annotation of a corpus of about onehundred hours of transcribed speech from severalFrench-speaking radio stations in France and Mo-rocco.
Both news and entertainment shows weretranscribed, including dialogs, with speaker turns.1Once annotated, the corpus was split into a de-velopment corpus: one file from a French radio sta-tion;2 a training corpus: 188 files from five Frenchstations3 and one Moroccan station;4 and a test cor-pus: 18 files from two French stations already stud-ied in the training corpus5 and from unseen sources,both radio6 and television,7 in order to evaluate therobustness of systems.
These data have been used inthe 2011 Quaero named entity evaluation campaign.1Potential named entities may be split across several seg-ments or turns.2News from France Culture.3News from France Culture (refined language), France Info(news with short news headlines), France Inter (generalist radiostation), Radio Classique (classical music and economic news),RFI (international radio broadcast out of France).4News from RTM (generalist French speaking radio).5News from France Culture, news and entertainment fromFrance Inter.6A popular entertainment show from Europe 1.7News from Arte (public channel with art and culture),France 2 (public generalist channel), and TF1 (private gener-alist popular channel).This corpus allows us to perform different evalua-tions, depending of the knowledge the systems haveof the source (source seen in the training corpus vs.unseen source), the kind of show (news vs. enter-tainment), the language style (popular vs. refined),and the type of media (radio vs. television).4.2 Tools for annotatorsTo perform our test annotations (see Section 2.2),we developed a very simple annotation tool as an in-terface based on XEmacs.
We provided the humanannotators with this tool and they decided to use itfor the campaign, despite the fact that it is very sim-ple and that we told them about other, more generic,annotation tools such as GATE8 or Glozz.9 This isprobably due to the fact that apart from being verysimple to install and use, it has interesting features.The first feature is the insertion of annotationsusing combinations of keyboard shortcuts based onthe initial of each type, subtype and componentname.
For example, combination F2 key + initialkeys is used to annotate a subtype (pers.ind, etc.
),F3 + keys for a transverse component (name, kind,etc.
), F4 + keys for a specific component (name.first,etc.
), and F5 to delete the annotation selected withthe cursor (both opening and closing tags).The second feature is boundary management: ifthe annotator puts the cursor over the token to anno-tate, the annotation tool will handle the boundariesof this token; opening and closing tags will be in-serted around the token.However, it presents some limitations: tags areinserted in the text (which makes visualization morecomplex, especially for long sentences or in casesof multiple annotations on the same entity), no per-sonalization is offered (tags are of only one color),and there is no function to express annotator uncer-tainty (the user must choose among several possibletags the one that fits the best;10 while producing theguidelines, we did not consider it could be of inter-est: as a consequence, no uncertainty managementwas implemented).
Therefore, this tool allows usersto insert tags rapidly into a text, but it offers no exter-nal resources, as real annotation tools (e.g.
GATE)often do.8http://gate.ac.uk/9http://www.glozz.org/10Uncertainty can be found in cases of lack of context.96These simplistic characteristics combined with afast learning curve allow the annotators to rapidlyannotate the corpora.
Annotators were allowed notto annotate the transverse component name (only ifit was the only component in the annotated phrase,e.g.
?Russia?
in Figure 3, blue tag) and to annotateevents, even though we do not focus on this typeof entity as of yet.
We therefore also provided anormalization tool which adds the transverse com-ponent name in these instances, and which removesevent annotations.4.3 Corpus annotationGlobal annotation It took four human annotatorstwo months and a half to annotate the entire corpus(10 man-month).
These annotators were hired grad-uate students (MS in linguistics).
The overall corpuswas annotated in duplicate.
Regular comparisons ofannotations were performed and allowed the anno-tators to develop a methodology, which was subse-quently used to annotate the remaining documents.Mini reference corpus To evaluate the global an-notation, we built a mini reference corpus by ran-domly selecting 400 sentences from the training cor-pus and distributing them into four files.
These fileswere annotated by four graduate human annotatorsfrom two research institutes (Figure 5) with two hu-mans per institute, in about 10 hours per annotator.Figure 5: Creation of mini reference corpus and compu-tation of inter-annotator agreement.
Institute 1 = LIMSI?CNRS, Institute 2 = INIST?CNRSFirst, we merged the annotations of each filewithin a given institute (1.5h per pair of annotators),then merged the results across the two institutes(2h).
Finally, we merged the results with the anno-tations of the hired annotators (8h).
We thus spentabout 90 hours to annotate and merge annotations inthis mini reference corpus (0.75 man-month).4.4 Annotation resultsOur broadcast news corpus includes 1,291,225tokens, among which there are 954,049 non-punctuation tokens.
Its annotation contains 113,885named entities and 146,405 components (Table 3),i.e.
one entity per 8.4 non-punctuation tokens, andone component per 6.5 non-punctuation tokens.There is an average of 6 annotations per line.PPPPPPPPInf.DataTraining Test# shows 188 18# lines 43,289 5,637# words 1,291,225 108,010# entity types 113,885 5,523# distinct types 41 32# components 146,405 8,902# distinct comp.
29 22Table 3: Statistics on annotated corpora.5 Inter-Annotator Agreement5.1 ProcedureDuring the annotation campaign, we measured sev-eral criteria on a regular basis: inter-annotator agree-ment and disagreement.
We used them to correct er-roneous annotations, and mapped these correctionsto the original annotations.
We also used these mea-sures to give the annotators feedback on the en-countered problems, discrepancies, and residual er-rors.
Whereas we performed these measurements allalong the annotation campaign, this paper focuseson the final evaluation on the mini reference corpus.5.2 MetricsBecause human annotation is an interpretation pro-cess (Leech, 1997), there is no ?truth?
to rely on.
Itis therefore impossible to really evaluate the validityof an annotation.
All we can and should do is to eval-uate its reliability, i.e.
the consistency of the anno-tation across annotators, which is achieved throughcomputation of the inter-annotator agreement (IAA).97The best way to compute it is to use one ofthe Kappa family coefficients, namely Cohen?sKappa (Cohen, 1960) or Scott?s Pi (Scott, 1955),also known as Carletta?s Kappa (Carletta, 1996),11as they take chance into account (Artstein and Poe-sio, 2008).
However, these coefficients imply acomparison with a ?random baseline?
to establishwhether the correlation between annotations is sta-tistically significant.
This baseline depends on thenumber of ?markables?, i.e.
all the units that couldbe annotated.In the case of named entities, as in many others,this ?random baseline?
is known to be difficult?ifnot impossible?to identify (Alex et al, 2010).
Wewish to analyze this in more detail, to see how wecould actually compute these coefficients and whatinformation it would give us about the annotation.Markables Annotators Both institutesF = 0.84522 F = 0.91123U1: n-grams?
= 0.84522 ?
= 0.91123pi = 0.81687 pi = 0.90258U2: n-grams ?
6?
= 0.84519 ?
= 0.91121pi = 0.81685 pi = 0.90257U3: NPs?
= 0.84458 ?
= 0.91084pi = 0.81628 pi = 0.90219U4: Ester entities?
= 0.71300 ?
= 0.82607pi = 0.71210 pi = 0.82598U5: Pooling?
= 0.71300 ?
= 0.82607pi = 0.71210 pi = 0.82598Table 4: Inter-Annotator Agreements (?
stands for Co-hen?s Kappa, pi for Scott?s Pi, and F for F-measure).
IAAvalues were computed by taking as the reference the hiredannotators?
annotation or that obtained by merging fromboth institutes (see Figure 5).In the present case, we could consider that, poten-tially, all the noun phrases can be annotated (row U3in Table 4, based on the PASSAGE campaign (Vil-nat et al, 2010)).
Of course, this is a wrong approx-imation as named entities are not necessarily nounphrases (e.g., ??
partir de l?automne prochain?, fromnext autumn).We could also consider all n-grams of tokens inthe corpus (row U1).
However, it would be more11For more details on terminology issues, we refer to the in-troduction of (Artstein and Poesio, 2008).relevant to limit their size.
For a maximum size ofsix, we get the results shown in row U2.
All this, ofcourse, is artificial, as the named entity annotationprocess is not random.To obtain results that are closer to reality, wecould use numbers of named entities from previousnamed entity annotation campaigns (row U4 basedon the Ester II campaign (Galliano et al, 2009)), butas we consider here a largely extended version ofthose, the results would again be far from reality.Another solution is to consider as ?markables?
allthe units annotated by at least one of the annotators(row U5).
In this particular case, units not annotatedby any of the annotators (i.e.
silence) are overlooked.The lowest IAA will be the one computed withthis last solution, while the highest IAA will beequal to the F-measure (i.e.
the measure computedwith all the markables as shown in row U1 in Ta-ble 4).
We notice that the first two solutions (U1and U2 with n-grams) are not acceptable becausethey are far from reality; even extended named en-tities are sparse annotations, and just consideringall tokens as ?markables?
is not suitable.
The lastthree ones seem to be more relevant because theyare based on an observed segmentation on similardata.
Still, the U3 solution (NPs) overrates the num-ber of markables because not all noun phrases areextended named entities.
Although the U4 solution(Ester entities) is based on the same corpus used fora related task, it underrates the number of markablesbecause that task produced 16.3 times less annota-tions.
Finally the U5 solution (pooling) gives thelower bound for the ?
estimation which is an in-teresting information but may easily undervalue thequality of the annotation.As (Hripcsak and Rothschild, 2005) showed, inour case ?
tends towards the F-measure when thenumber of negative cases tends towards infinity.
Ourresults show that it is hard to build a justifiable hy-pothesis on the number of markables which is largerthan the number of actually annotated entities whilekeeping ?
significantly under the F-measure.
Butbuilding no hypothesis leads to underestimating the?
value.This reinforces the idea of using the F-measureas the main inter-annotator agreement measure fornamed entity annotation tasks.986 LimitationsWe used syntax to define some components (e.g.
aqualifier is an adjective) and to set the scope of en-tities (e.g.
stop at relative clauses).
Nevertheless,this syntactic definition cannot fit all named enti-ties, which are mainly defined according to seman-tics: the phrase ?dans les mois qui viennent?
(?inthe coming months?)
expresses an entity of typetime.date.rel where the relative clause ?qui vien-nent?
is part of the entity and contributes the time-modifier component.The distinction between some types of entitiesmay be fuzzy, especially for the organizations (isthe Social Security an administrative organization ora company?)
and for context-dependent annotations(is lemonde.fr a URL, a media, or a company?).
As aconsequence, some entity types might be convertedinto specific components in a future revision, e.g.
thefunc type could become a component of the perstype, where it would become a description of thefunction itself instead of the person who performsthis function (Figure 6).depuisptm-itsoftrlitsvaienrtn.pl,numRpeulitsdepuisptmoftrvaienrtn.pl,numRpeulitsFigure 6: Possible revision: current annotation (left),transformation of func from entity to component (right).7 Conclusion and perspectivesIn this paper, we presented an extension of the tra-ditional named entity categories to new types (func-tions, civilizations) and new coverage (expressionsbuilt over a substantive).
We created guidelinesthat were used by graduate annotators to annotatea broadcast news corpus.The organizers also annotated a small part of thecorpus to build a mini reference corpus.
We evalu-ated the human annotations with our mini-referencecorpus: the actual computed ?
is between 0.71 et0.85 which, given the complexity of the task, seemsto indicate a good annotation quality.
Our results areconsistent with other studies (Dandapat et al, 2009)in demonstrating that human annotators?
training isa key asset to produce quality annotations.We also saw that guidelines are never fixed, butevolve all along the annotation process due to feed-back between annotators and organizers; the rela-tionship between guidelines producers and humanannotators evolved from ?parent?
to ?peer?
(Akrichand Boullier, 1991).
This evolution was observedduring the annotation development, beyond our ex-pectations.
These data have been used for the 2011Quaero Named Entity evaluation campaign.Extensions and revisions are planned.
Our firstgoal is to add a new type of named entity for allkinds of events; guidelines are being written and hu-man annotation tests are ongoing.
We noticed thatsome subtypes are more difficult to disambiguatethan others, especially org.adm and org.ent (defi-nition and examples in the guidelines are not clearenough).
We shall make decisions about this kindof ambiguity, either by merging these subtypes or byreorganizing the distinctions within the organizationtype.
We also plan to link the annotated entities us-ing relations; further work is needed to define moreprecisely the way we will perform these annotations.Moreover, the taxonomy we defined was applied toa broadcast news corpus, but we intend to use it inother corpora.
The annotation of an old press corpuswas performed according to the same process.
Itsevaluation will start in the coming months.AcknowledgmentsWe thank all the annotators who did such a greatwork on this project, as well as Sabine Barreaux(INIST?CNRS) for her work on the reference cor-pus.This work was partly realized as part of theQuaero Programme, funded by Oseo, French Stateagency for innovation and by the French ANR Etapeproject.ReferencesACE.
2000.
Entity detection and tracking,phase 1, ACE pilot study.
Task definition.http://www.nist.gov/speech/tests/ace/phase1/doc/summary-v01.htm.ACE.
2005.
ACE (Automatic Con-tent Extraction) English annotation guide-lines for entities version 5.6.1 2005.05.23.http://www.ldc.upenn.edu/Projects/ACE/docs/English-Entities-Guidelines_v5.6.1.pdf.99Madeleine Akrich and Dominique Boullier.
1991.
Lemode d?emploi, gen?se, forme et usage.
In DenisChevallier, editor, Savoir faire et pouvoir transmettre,pages 113?131.
?d.
de la MSH (collection Ethnologiede la France, Cahier 6).Beatrice Alex, Claire Grover, Rongzhou Shen, and MijailKabadjov.
2010.
Agile Corpus Annotation in Prac-tice: An Overview of Manual and Automatic Annota-tion of CVs.
In Proc.
of the Fourth Linguistic Annota-tion Workshop, pages 29?37, Uppsala, Sweden.
ACL.Ron Artstein and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Computa-tional Linguistics, 34(4):555?596.Eckhard Bick.
2004.
A named entity recognizer for dan-ish.
In LREC?04.H?l?ne Bonneau-Maynard, Sophie Rosset, Christelle Ay-ache, Anne Kuhn, and Djamel Mostefa.
2005.
Seman-tic Annotation of the French Media Dialog Corpus.
InInterSpeech, Lisbon.Jean Carletta.
1996.
Assessing Agreement on Classifi-cation Tasks: the Kappa Statistic.
Computational Lin-guistics, 22:249?254.Sam Coates-Stephens.
1992.
The analysis and acquisi-tion of proper names for the understanding of free text.Computers and the Humanities, 26:441?456.Jacob Cohen.
1960.
A Coefficient of Agreement forNominal Scales.
Educational and Psychological Mea-surement, 20(1):37?46.Sandipan Dandapat, Priyanka Biswas, Monojit Choud-hury, and Kalika Bali.
2009.
Complex Linguistic An-notation - No Easy Way Out!
A Case from Banglaand Hindi POS Labeling Tasks.
In Proc.
of the ThirdLinguistic Annotation Workshop, Singapour.
ACL.Bart Desmet and V?ronique Hoste.
2010.
Towards abalanced named entity corpus for dutch.
In LREC.George Doddington, Alexis Mitchell, Mark Przybocki,Lance Ramshaw, Stephanie Strassel, and RalphWeischedel.
2004.
The automatic content extraction(ACE) program tasks, data, and evaluation.
In Proc.
ofLREC.Maud Ehrmann.
2008.
Les entit?s nomm?es, de la lin-guistique au TAL : statut th?orique et m?thodes ded?sambigu?sation.
Ph.D. thesis, Univ.
Paris 7 Diderot.Michael Fleischman and Eduard Hovy.
2002.
Finegrained classification of named entities.
In Proc.
ofCOLING, volume 1, pages 1?7.
ACL.Michael Fleischman.
2001.
Automated subcategoriza-tion of named entities.
In Proc.
of the ACL 2001 Stu-dent Research Workshop, pages 25?30.Kar?n Fort, Maud Ehrmann, and Adeline Nazarenko.2009.
Towards a Methodology for Named Entities An-notation.
In Proceeding of the 3rd ACL Linguistic An-notation Workshop (LAW III), Singapore.Sylvain Galliano, Guillaume Gravier, and LauraChaubard.
2009.
The ESTER 2 evaluation campaignfor the rich transcription of French radio broadcasts.In Proc of Interspeech 2009.Ralph Grishman and Beth Sundheim.
1996.
MessageUnderstanding Conference - 6: A brief history.
InProc.
of COLING, pages 466?471.George Hripcsak and Adam S. Rothschild.
2005.
Tech-nical brief: Agreement, the f-measure, and reliabilityin information retrieval.
JAMIA, 12(3):296?298.Seungwoo Lee and Gary Geunbae Lee.
2005.
Heuris-tic methods for reducing errors of geographic namedentities learned by bootstrapping.
In IJCNLP, pages658?669.Geoffrey Leech.
1997.
Introducing corpus annotation.In Geoffrey Leech Roger Garside and Tony McEnery,editors, Corpus annotation: Linguistic informationfrom computer text corpora, pages 1?18.
Longman,London.Diana Maynard, Valentin Tablan, Cristian Ursu, HamishCunningham, and Yorick Wilks.
2001.
Named en-tity recognition from diverse text types.
In Recent Ad-vances in NLP 2001 Conference, Tzigov Chark.Tomoko Ohta.
2002.
The genia corpus: An annotatedresearch abstract corpus in molecular biology domain.In Proc.
of HLTC, pages 73?77.Sophie Rosset, Olivier Galibert, Gilles Adda, and EricBilinski.
2007.
The LIMSI participation to the QAsttrack.
In Working Notes for the CLEF 2007 Workshop,Budapest, Hungary.Sophie Rosset, Cyril Grouin, and Pierre Zweigenbaum.2010.
Entit?s nomm?es : guide d?annotation Quaero,November.
T3.2, presse ?crite et orale.SAIC.
1998.
Proceedings of the seventh message under-standing conference (MUC-7).William A Scott.
1955.
Reliability of Content Analysis:The Case of Nominal Scale Coding.
Public OpinionQuaterly, 19(3):321?325.Satoshi Sekine.
2004.
Definition, dictionaries and taggerof extended named entity hierarchy.
In Proc.
of LREC.Anne Vilnat, Patrick Paroubek, Eric Villemonte de laClergerie, Gil Francopoulo, and Marie-Laure Gu?not.2010.
Passage syntactic representation: a minimalcommon ground for evaluation.
In Proc.
of LREC.Alex Yeh, Alex Morgan, Marc Colosimo, and LynetteHirschman.
2005.
BioCreAtIvE task 1A: gene men-tion finding evaluation.
BMC Bioinformatics, 6(1).100
