Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 333?338,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsDocument-level Automatic MT Evaluationbased on Discourse RepresentationsJesu?s Gime?nez andLlu?
?s Ma`rquezTALP UPCBarcelona, Spain{jgimenez, lluism}@lsi.upc.eduElisabet Comelles andIrene Castello?nUniversitat de BarcelonaBarcelona, Spain{elicomelles,icastellon} @ub.eduVictoria ArranzELDA/ELRAParis, Francearranz@elda.orgAbstractThis paper describes the joint submission ofUniversitat Polite`cnica de Catalunya and Uni-versitat de Barcelona to the Metrics MaTr2010 evaluation challenge, in collaboration withELDA/ELRA.
Our work is aimed at widening thescope of current automatic evaluation measuresfrom sentence to document level.
Preliminary ex-periments, based on an extension of the metrics byGime?nez and Ma`rquez (2009) operating over dis-course representations, are presented.1 IntroductionCurrent automatic similarity measures for Ma-chine Translation (MT) evaluation operate all,without exception, at the segment level.
Trans-lations are analyzed on a segment-by-segment1fashion, ignoring the text structure.
Documentand system scores are obtained using aggregatestatistics over individual segments.
This strategypresents the main disadvantage of ignoring cross-sentential/discursive phenomena.In this work we suggest widening the scopeof evaluation methods.
We have defined genuinedocument-level measures which are able to ex-ploit the structure of text to provide more informedevaluation scores.
For that purpose we take advan-tage of two coincidental facts.
First, test beds em-ployed in recent MT evaluation campaigns includea document structure grouping sentences relatedto the same event, story or topic (Przybocki et al,2008; Przybocki et al, 2009; Callison-Burch et al,2009).
Second, we count on automatic linguisticprocessors which provide very detailed discourse-level representations of text (Curran et al, 2007).Discourse representations allow us to focus onrelevant pieces of information, such as the agent1A segment typically consists of one or two sentences.
(who), location (where), time (when), and theme(what), which may be spread all over the text.Counting on a means of discerning the events, theindividuals taking part in each of them, and theirrole, is crucial to determine the semantic equiva-lence between a reference document and a candi-date translation.Moreover, the discourse analysis of a documentis not a mere concatenation of the analyses of itsindividual sentences.
There are some phenom-ena which may go beyond the scope of a sen-tence and can only be explained within the con-text of the whole document.
For instance, in anewspaper article, facts and entities are progres-sively added to the discourse and then referredto anaphorically later on.
The following extractfrom the development set illustrates the impor-tance of such a phenomenon in the discourse anal-ysis: ?Among the current or underlying crises inthe Middle East, Rod Larsen mentioned the Arab-Israeli conflict and the Iranian nuclear portfolio,as well as the crisis between Lebanon and Syria.He stated: ?All this leads us back to crucial val-ues and opinions, which render the situation proneat any moment to getting out of control, more sothan it was in past days.??.
The subject pronoun?he?
works as an anaphoric pronoun whose an-tecedent is the proper noun ?Rod Larson?.
Theanaphoric relation established between these twoelements can only be identified by analyzing thetext as a whole, thus considering the gender agree-ment between the third person singular masculinesubject pronoun ?he?
and the masculine propernoun ?Rod Larson?.
However, if the two sen-tences were analyzed separately, the identificationof this anaphoric relation would not be feasibledue to the lack of connection between the two ele-ments.
Discourse representations allow us to tracelinks across sentences between the different factsand entities appearing in them.
Therefore, provid-ing an approach to the text more similar to that of333a human, which implies taking into account thewhole text structure instead of considering eachsentence separately.The rest of the paper is organized as follows.Section 2 describes our evaluation methods andthe linguistic theory upon which they are based.Experimental results are reported and discussed inSection 3.
Section 4 presents the metric submittedto the evaluation challenge.
Future work is out-lined in Section 5.As an additional result, document-level metricsgenerated in this study have been incorporated tothe IQMT package for automatic MT evaluation2.2 Metric DescriptionThis section provides a brief description of our ap-proach.
First, in Section 2.1, we describe the un-derlying theory and give examples on its capabili-ties.
Then, in Section 2.2, we describe the associ-ated similarity measures.2.1 Discourse RepresentationsAs previously mentioned in Section 1, a documenthas some features which need to be analyzed con-sidering it as a whole instead of dividing it upinto sentences.
The anaphoric relation betweena subject pronoun and a proper noun has alreadybeen exemplified.
However, this is not the onlyanaphoric relation which can be found inside atext, there are some others which are worth men-tioning:?
the connection between a possessive adjec-tive and a proper noun or a subject pro-noun, as exemplified in the sentences ?Mariabought a new sweater.
Her new sweater isblue.
?, where the possessive feminine adjec-tive ?her?
refers to the proper noun ?Maria?.?
the link between a demonstrative pronounand its referent, which is exemplified in thesentences ?He developed a new theory ongrammar.
However, this is not the only the-ory he developed?.
In the second sentence,the demonstrative pronoun ?this?
refers backto the noun phrase ?new theory on grammar?which occurs in the previous sentence.?
the relation between a main verb and an aux-iliary verb in certain contexts, as illustrated inthe following pair of sentences ?Would you2http://www.lsi.upc.edu/?nlp/IQMTlike more sugar?
Yes, I would?.
In this ex-ample, the auxiliary verb ?would?
used inthe short answer substitutes the verb phrase?would like?.In addition to anaphoric relations, other featuresneed to be highlighted, such as the use of discoursemarkers which help to give cohesion to the text,link parts of a discourse and show the relations es-tablished between them.
Below, some examplesare given:?
?Moreover?, ?Furthermore?, ?In addition?indicate that the upcoming sentence addsmore information.?
?However?, ?Nonetheless?, ?Nevertheless?show contrast with previous ideas.?
?Therefore?, ?As a result?, ?Consequently?show a cause and effect relation.?
?For instance?, ?For example?
clarify or il-lustrate the previous idea.It is worth noticing that anaphora, as well as dis-course markers, are key features in the interfacebetween syntax, semantics and pragmatics.
Thus,when dealing with these phenomena at a text levelwe are not just looking separately at the differentlanguage levels, but we are trying to give a com-plete representation of both the surface and thedeep structures of a text.2.2 Definition of Similarity MeasuresIn this work, as a first proposal, instead of elabo-rating on novel similarity measures, we have bor-rowed and extended the Discourse Representation(DR) metrics defined by Gime?nez and Ma`rquez(2009).
These metrics analyze similarities be-tween automatic and reference translations bycomparing their respective discourse representa-tions over individual sentences.For the discursive analysis of texts, DR met-rics rely on the C&C Tools (Curran et al, 2007),specifically on the Boxer component (Bos, 2008).This software is based on the Discourse Represen-tation Theory (DRT) by Kamp and Reyle (1993).DRT is a theoretical framework offering a rep-resentation language for the examination of con-textually dependent meaning in discourse.
A dis-course is represented in a discourse representationstructure (DRS), which is essentially a variation offirst-order predicate calculus ?its forms are pairs334of first-order formulae and the free variables thatoccur in them.DRSs are viewed as semantic trees, builtthrough the application of two types of DRS con-ditions:basic conditions: one-place properties (pred-icates), two-place properties (relations),named entities, time-expressions, cardinalexpressions and equalities.complex conditions: disjunction, implication,negation, question, and propositional attitudeoperations.For instance, the DRS representation for thesentence ?Every man loves Mary.?
is as follows:?y named(y,mary, per) ?
(?x man(x) ?
?z love(z) ?
event(z) ?
agent(z, x) ?patient(z, y)).
DR integrates three differentkinds of metrics:DR-STM These metrics are similar to the Syntac-tic Tree Matching metric defined by Liu andGildea (2005), in this case applied to DRSsinstead of constituent trees.
All semantic sub-paths in the candidate and reference trees areretrieved.
The fraction of matching subpathsof a given length (l=4 in our experiments) iscomputed.DR-Or(?)
Average lexical overlap between dis-course representation structures of the sametype.
Overlap is measured according to theformulae and definitions by Gime?nez andMa`rquez (2007).DR-Orp(?)
Average morphosyntactic overlap,i.e., between grammatical categories ?parts-of-speech?
associated to lexical items, be-tween discourse representation structures ofthe same type.We have extended these metrics to operate atdocument level.
For that purpose, instead of run-ning the C&C Tools in a sentence-by-sentencefashion, we run them document by document.This is as simple as introducing a ?<META>?
tagat the beginning of each document to denote doc-ument boundaries3 .3Details on the advanced use of Boxer are avail-able at http://svn.ask.it.usyd.edu.au/trac/candc/wiki/BoxerComplex.3 Experimental WorkIn this section, we analyze the behavior of the newDR metrics operating at document level with re-spect to their sentence-level counterparts.3.1 SettingsWe have used the ?mt06?
part of the developmentset provided by the Metrics MaTr 2010 organiza-tion, which corresponds to a subset of 25 docu-ments from the NIST 2006 Open MT EvaluationCampaign Arabic-to-English translation.
The to-tal number of segments is 249.
The average num-ber of segments per document is, thus, 9.96.
Thenumber of segments per document varies between2 and 30.
For the purpose of automatic evaluation,4 human reference translations and automatic out-puts by 8 different MT systems are available.
Inaddition, we count on the results of a process ofmanual evaluation.
Each translation segment wasassessed by two judges.
After independently andcompletely assessing the entire set, the judges re-viewed their individual assessments together andsettled on a single final score.
Average system ad-equacy is 5.38.In our experiments, metrics are evaluated interms of their correlation with human assess-ments.
We have computed Pearson, Spearmanand Kendall correlation coefficients between met-ric scores and adequacy assessments.
Document-level and system-level assessments have been ob-tained by averaging over segment-level assess-ments.
We have computed correlation coefficientsand confidence intervals applying bootstrap re-sampling at a 99% statistical significance (Efronand Tibshirani, 1986; Koehn, 2004).
Since thecost of exhaustive resampling was prohibitive, wehave limited to 1,000 resamplings.
Confidence in-tervals, not shown in the tables, are in all caseslower than 10?3.3.2 Metric PerformanceTable 1 shows correlation coefficients at the docu-ment level for several DR metric representatives,and their document-level counterparts (DRdoc).For the sake of comparison, the performance ofthe METEOR metric is also reported4.Contrary to our expectations, DRdoc variantsobtain lower levels of correlation than their DR4We have used METEOR version 1.0 with default param-eters optimized by its developers over adequacy and fluencyassessments.
The METEOR metric is publicly available athttp://www.cs.cmu.edu/?alavie/METEOR/335Metric Pearson?
Spearman?
Kendall?METEOR 0.9182 0.8478 0.6728DR-Or(?)
0.8567 0.8061 0.6193DR-Orp(?)
0.8286 0.7790 0.5875DR-STM 0.7880 0.7468 0.5554DRdoc-Or(?)
0.7936 0.7784 0.5875DRdoc-Orp(?)
0.7219 0.6737 0.4929DRdoc-STM 0.7553 0.7421 0.5458Table 1: Meta-evaluation results at document levelMetric Pearson?
Spearman?
Kendall?METEOR 0.9669 0.9151 0.8533DR-Or(?)
0.9100 0.6549 0.5764DR-Orp(?)
0.9471 0.7918 0.7261DR-STM 0.9295 0.7676 0.7165DRdoc-Or(?)
0.9534 0.8434 0.7828DRdoc-Orp(?)
0.9595 0.9101 0.8518DRdoc-STM 0.9676 0.9655 0.9272DR-Or(?)?
0.9836 0.9594 0.9296DR-Orp(?)?
0.9959 1.0000 1.0000DR-STM?
0.9933 0.9634 0.9307Table 2: Meta-evaluation results at system levelcounterparts.
There are three different factorswhich could provide a possible explanation forthis negative result.
First, the C&C Tools, like anyother automatic linguistic processor are not per-fect.
Parsing errors could be causing the metricto confer less informed scores.
This is especiallyrelevant taking into account that candidate transla-tions are not always well-formed.
Secondly, weargue that the way in which we have obtaineddocument-level quality assessments, as an averageof segment-level assessments, may be biasing thecorrelation.
Thirdly, perhaps the similarity mea-sures employed are not able to take advantage ofthe document-level features provided by the dis-course analysis.
In the following subsection weshow some error analysis we have conducted byinspecting particular cases.Table 2 shows correlation coefficients at systemlevel.
In the case of DR and DRdoc metrics, sys-tem scores are computed by simple average overindividual documents.
Interestingly, in this caseDRdoc variants seem to obtain higher correlationthan their DR counterparts.
The improvement isespecially substantial in terms of Spearman andKendall coefficients, which do not consider ab-solute values but ranking positions.
However, itcould be the case that it was just an average ef-fect.
While DR metrics compute system scores asan average of segment scores, DRdoc metrics av-erage directly document scores.
In order to clarifythis result, we have modified DR metrics so as tocompute system scores as an average of documentscores (DR?
variants, the last three rows in the ta-ble).
It can be observed that DR?
variants out-perform their DRdoc counterparts, thus confirmingour suspicion about the averaging effect.3.3 AnalysisIt is worth noting that DRdoc metrics are able todetect and deal with several linguistic phenomenarelated to both syntax and semantics at sentenceand document level.
Below, several examples il-lustrating the potential of this metric are presented.Control structures.
Control structures (eithersubject or object control) are always adifficult issue as they mix both syntactic andsemantic knowledge.
In Example 1 a coupleof control structures must be identifiedand DRdoc metrics deal correctly with theargument structure of all the verbs involved.Thus, in the first part of the sentence, asubject control verb can be identified being?the minister?
the agent of both verb forms?go?
and ?say?.
On the other hand, in the336quoted question, the verb ?invite?
works asan object control verb because its patient?Chechen representatives?
is also the agentof the verb visit.Example 1: The minister went on to say,?What would Moscow say if we were to inviteChechen representatives to visit Jerusalem?
?Anaphora and pronoun resolution.
Wheneverthere is a pronoun whose antecedent is anamed entity (NE), the metric identifiescorrectly its antecedent.
This feature ishighly valuable because a relationship be-tween syntax and semantics is established.Moreover, when dealing with SemanticRoles the roles of Agent or Patient are givento the antecedents instead of the pronouns.Thus, in Example 2 the antecedent of therelative pronoun ?who?
is the NE ?Putin?and the patient of the verb ?classified?
isalso the NE ?Putin?
instead of the relativepronoun ?who?.Example 2: Putin, who was not classifiedas his country Hamas as ?terrorist organiza-tions?, recently said that the European Unionis ?a big mistake?
if it decided to suspend fi-nancial aid to the Palestinians.Nevertheless, although Boxer was expectedto deal with long-distance anaphoric relationsbeyond the sentence, after analyzing severalcases, results show that it did not succeed incapturing this type of relations as shown inExample 3.
In this example, the antecedentof the pronoun ?he?
in the second sentenceis the NE ?Roberto Calderoli?
which ap-pears in the first sentence.
DRdoc metricsshould be capable of showing this connec-tion.
However, although the proper noun?Roberto Calderoli?
is identified as a NE, itdoes not share the same reference as the thirdperson singular pronoun ?he?.Example 3: Roberto Calderoli does not in-tend to apologize.
The newspaper CorriereDella Sera reported today, Saturday, thathe said ?I don?t feel responsible for thosedeaths.
?4 Our SubmissionInstead of participating with individual metrics,we have combined them by averaging their scoresas described in (Gime?nez and Ma`rquez, 2008).This strategy has proven as an effective means ofcombining the scores conferred by different met-rics (Callison-Burch et al, 2008; Callison-Burchet al, 2009).
Metrics submitted are:DRdoc an arithmetic mean over a heuristically-defined set of DRdoc metric variants, respec-tively computing lexical overlap, morphosyn-tactic overlap, and semantic tree match-ing (M = {?DRdoc-Or(?
)?, ?DRdoc-Orp(?
)?, ?DRdoc-STM4?}).
Since DRdoc metrics do not operateover individual segments, we have assignedeach segment the score of the document inwhich it is contained.DR a measure analog to DRdoc but using the de-fault version of DR metrics operating at thesegment level (M = {?DR-Or(?
)?, ?DR-Orp(?)?,?DR-STM4?
}).ULCh an arithmetic mean over a heuristically-defined set of metrics operating at differ-ent linguistic levels, including lexical met-rics, and measures of overlap between con-stituent parses, dependency parses, seman-tic roles, and discourse representations (M ={?ROUGEW ?, ?METEOR?, ?DP-HWCr?, ?DP-Oc(?)?,?DP-Ol(?
)?, ?DP-Or(?
)?, ?CP-STM4?, ?SR-Or(?
)?,?SR-Orv?, ?DR-Orp(?)?}).
This metric corre-sponds exactly to the metric submitted in ourprevious participation.The performance of these metrics at the docu-ment and system levels is shown in Table 3.5 Conclusions and Future WorkWe have presented a modified version of the DRmetrics by Gime?nez and Ma`rquez (2009) which,instead of limiting their scope to the segment level,are able to capture and exploit document-level fea-tures.
However, results in terms of correlationwith human assessments have not reported any im-provement of these metrics over their sentence-level counterparts as document and system qualitypredictors.
It must be clarified whether the prob-lem is on the side of the linguistic tools, in thesimilarity measure, or in the way in which we havebuilt document-level human assessments.For future work, we plan to continue the er-ror analysis to clarify why DRdoc metrics do notoutperform their DR counterparts at the documentlevel, and how to improve their behavior.
This337Document level System levelMetric Pearson?
Spearman?
Kendall?
Pearson?
Spearman?
Kendall?ULCDR 0.8418 0.8066 0.6135 0.9349 0.7936 0.7145ULCDRdoc 0.7739 0.7358 0.5474 0.9655 0.9062 0.8435ULCh 0.8963 0.8614 0.6848 0.9842 0.9088 0.8638Table 3: Meta-evaluation results at document and system level for submitted metricsmay imply defining new metrics possibly usingalternative linguistic processors.
In addition, weplan to work on the identification and analysisof discourse markers.
Finally, we plan to repeatthis experiment over other test beds with docu-ment structure, such as those from the 2009 Work-shop on Statistical Machine Translation sharedtask (Callison-Burch et al, 2009) and the 2009NIST MT Evaluation Campaign (Przybocki et al,2009).
In the case that document-level assess-ments are not provided, we will also explore thepossibility of producing them ourselves.AcknowledgmentsThis work has been partially funded by theSpanish Government (projects OpenMT-2,TIN2009-14675-C03, and KNOW, TIN-2009-14715-C0403) and the European Community?sSeventh Framework Programme (FP7/2007-2013)under grant agreement numbers 247762 (FAUSTproject, FP7-ICT-2009-4-247762) and 247914(MOLTO project, FP7-ICT-2009-4-247914).
Weare also thankful to anonymous reviewers for theircomments and suggestions.ReferencesJohan Bos.
2008.
Wide-coverage semantic analy-sis with boxer.
In Johan Bos and Rodolfo Del-monte, editors, Semantics in Text Processing.
STEP2008 Conference Proceedings, Research in Compu-tational Semantics, pages 277?286.
College Publi-cations.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on StatisticalMachine Translation, pages 70?106.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation, pages 1?28.James Curran, Stephen Clark, and Johan Bos.
2007.Linguistically motivated large-scale nlp with c&cand boxer.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational LinguisticsCompanion Volume Proceedings of the Demo andPoster Sessions, pages 33?36.Bradley Efron and Robert Tibshirani.
1986.
BootstrapMethods for Standard Errors, Confidence Intervals,and Other Measures of Statistical Accuracy.
Statis-tical Science, 1(1):54?77.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2007.
Linguis-tic Features for Automatic Evaluation of Heteroge-neous MT Systems.
In Proceedings of the ACLWorkshop on Statistical Machine Translation, pages256?264.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008.
A Smor-gasbord of Features for Automatic MT Evaluation.In Proceedings of the Third Workshop on StatisticalMachine Translation, pages 195?198.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2009.
On the Ro-bustness of Syntactic and Semantic Features for Au-tomatic MT Evaluation.
In Proceedings of the 4thWorkshop on Statistical Machine Translation (EACL2009).Hans Kamp and Uwe Reyle.
1993.
From Discourse toLogic: An Introduction to Modeltheoretic Semanticsof Natural Language, Formal Logic and DiscourseRepresentation Theory.
Dordrecht: Kluwer.Philipp Koehn.
2004.
Statistical Significance Testsfor Machine Translation Evaluation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 388?395.Ding Liu and Daniel Gildea.
2005.
Syntactic Featuresfor Evaluation of Machine Translation.
In Proceed-ings of ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization,pages 25?32.Mark Przybocki, Kay Peterson, and Se?bastien Bron-sart.
2008.
NIST Metrics for Machine Translation2008 Evaluation (MetricsMATR08).
Technical re-port, National Institute of Standards and Technol-ogy.Mark Przybocki, Kay Peterson, and Se?bastien Bron-sart.
2009.
NIST Open Machine Translation 2009Evaluation (MT09).
Technical report, National In-stitute of Standards and Technology.338
