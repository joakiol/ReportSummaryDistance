First Joint Conference on Lexical and Computational Semantics (*SEM), pages 579?585,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational Linguisticssranjans : Semantic Textual Similarity using Maximal WeightedBipartite Graph MatchingSumit Bhagwani, Shrutiranjan Satapathy, Harish KarnickComputer Science and EngineeringIIT Kanpur, Kanpur - 208016, India{sumitb,sranjans,hk}@cse.iitk.ac.inAbstractThe paper aims to come up with a sys-tem that examines the degree of semanticequivalence between two sentences.
At thecore of the paper is the attempt to gradethe similarity of two sentences by find-ing the maximal weighted bipartite matchbetween the tokens of the two sentences.The tokens include single words, or multi-words in case of Named Entitites, adjec-tivally and numerically modified words.Two token similarity measures are used forthe task - WordNet based similarity, and astatistical word similarity measure whichovercomes the shortcomings of WordNetbased similarity.
As part of three systemscreated for the task, we explore a simplebag of words tokenization scheme, a morecareful tokenization scheme which cap-tures named entities, times, dates, mone-tary entities etc., and finally try to capturecontext around tokens using grammaticaldependencies.1 IntroductionSemantic Textual Similarity (STS) measuresthe degree of semantic equivalence betweentexts.
The goal of this task is to create a unifiedframework for the evaluation of semantic textualsimilarity modules and to characterize their im-pact on NLP applications.
The task is part of theSemantic Evaluation 2012 Workshop (Agirre etal., 2012).STS is related to both Textual Entailment andParaphrase, but differs in a number of ways andit is more directly applicable to a number of NLPtasks.
Also, STS is a graded similarity notion -this graded bidirectional nature of STS is usefulfor NLP tasks such as MT evaluation, informa-tion extraction, question answering, and summa-rization.We propose a lexical similarity approach tograde the similarity of two sentences, where amaximal weighted bipartite match is found be-tween the tokens of the two sentences.
The ap-proach is robust enough to apply across differentdatasets.
The results on the STS test datasets areencouraging to say the least.
The tokens are sin-gle word tokens in case of the first system, whilein the second system, named and monetary en-tities, percentages, dates and times are handledtoo.
A token-token similarity measure is integralto the approach and we use both a statistical sim-ilarity measure and a WordNet based word sim-ilarity measure for the same.
In the final runof the task, apart from capturing the aforemen-tioned entities, we heuristically extract adjecti-vally and numerically modified words.
Also, thelast run naively attempts to capture the contextaround the tokens using grammatical dependen-cies, which in turn is used to measure contextsimilarity.Section 2 discusses the previous work donein this area.
Section 3 describes the datasets,the baseline system and the evaluation measuresused by the task organizers.
Section 4, 5 and 6introduce the systems developed and discuss theresults of each system.
Finally, section 7 con-579cludes the work and section 8 offers suggestionsfor future work.2 Related WorkVarious systems exist in literature for tex-tual similarity measurement, be it bag ofwords based models or complex semantic sys-tems.
(Achananuparp et al, 2008) enumerates afew word overlap measures, like Jaccard Similar-ity Coefficient, IDF Overlap measures, Phrasaloverlap measures etc, that have been used forsentential similarity.
(Liu et al, 2008) proposed an approach to cal-culate sentence similarity, which takes into ac-count both semantic information and word order.They define semantic similarity of sentence 1 rel-ative to sentence 2 as the ratio of the sum of theword similarity weighted by information contentof words in sentence 1 to the overall informationcontent included in both sentences.
The syntacticsimilarity is calculated as the correlation coeffi-cient between word order vectors.A similar semantic similarity measure, pro-posed by (Li et al, 2006), uses a semantic-vectorapproach to measure sentence similarity.
Sen-tences are transformed into feature vectors hav-ing individual words from the sentence pair asa feature set.
Term weights are derived fromthe maximum semantic similarity score betweenwords in the feature vector and words in the cor-responding sentence.
To utilize word order in thesimilarity calculation, they define a word ordersimilarity measure as the normalized differenceof word order between the two sentences.
Theyhave empirically proved that a sentence simi-larity measure performs the best when semanticmeasure is weighted more than syntactic measure(ratio ?
4:1).
This follows the conclusion froma psychological experiment conducted by themwhich emphasizes the role of semantic informa-tion over syntactic information in passage under-standing.3 Task Evaluation3.1 DatasetsThe development datasets are drawn from thefollowing sources :?
MSR Paraphrase : This dataset consistsof pairs of sentences which have been ex-tracted from news sources on the web.?
MSR Video : This dataset consists ofpairs of sentences where each sentence of apair tries to summarize the action in a shortvideo snippet.?
SMT Europarl : This dataset consists ofpairs sentences drawn from the proceedingsof the European Parliament, where eachsentence of a pair is a translation from a Eu-ropean language to English.In addition to the above sources, the testdatasets also contained the following sources :?
SMT News : This dataset consists of ma-chine translated news conversation sentencepairs.?
On WN : This dataset consists of pairsof sentences where the first comes fromOntonotes(Hovy et al, 2006) and the sec-ond from a WordNet definition.
Hence, thesentences are rather phrases.3.2 BaselineThe task organizers have used the followingbaseline scoring scheme.
Scores are producedusing a simple word overlap baseline system.The input sentences are tokenised by splittingat white spaces, and then each sentence is rep-resented as a vector in the multidimensional to-ken space.
Each dimension has 1 if the token ispresent in the sentence, 0 otherwise.
Similarityof vectors is computed using the cosine similar-ity.3.3 Evaluation CriteriaThe scores obtained by the participating systemsare evaluated against the gold standard of thedatasets using a pearson correlation measure.
Inorder to evaluate the overall performance of thesystems on all the five datasets, the organizersuse three evaluation measures :?
ALL : This measure takes the union of allthe test datasets, and finds the Pearson cor-relation of the system scores with the goldstandard of the union.580?
ALL Normalized : In this measure, a lin-ear fit is found for the system scores oneach dataset using a least squared error cri-terion, and then the union of the linearly fit-ted scores is used to calculate the Pearsoncorrelation against the gold standard union.?
Weighted Mean : The average of the Pear-son correlation scores of the systems on theindividual datasets is taken, weighted by thenumber of test instances in each dataset.4 SYSTEM 14.1 Tokenization SchemeEach sentence is tokenized into words, filter-ing out punctuations and stop-words.
The stop-words are taken from the stop-word list providedby the NLTK Toolkit (Bird et al, 2009).
Allthe word tokens are reduced to their lemmatizedform using the Stanford CoreNLP Toolkit (Min-nen et al, 2001).
The tokenization is basic innature and doesn?t handle named entities, times,dates, monetary entities or multi-word expres-sions.
The challenge with handling multi-wordtokens is in calculating multi-word token simi-larity, which is not supported in a WordNet word-similarity scheme or a statistical word similaritymeasure.4.2 Maximal Weighted Bipartite MatchA weighted bipartite graph is constructedwhere the two sets of vertices are the word-tokens extracted in the earlier subsection.
Thebipartite graph is made complete by assigning anedge weight to every pair of tokens from the twosentences.
The edge weight is based on a suitableword similarity measure.
We had two resourcesat hand - WordNet based word similarity and astatistical word similarity measure.4.2.1 WordNet Based Word SimilarityThe is-a hierarchy of WordNet is used in cal-culating the word similarity of two words.
Nounsand verbs have separate is-a hierarchies.
We usethe Lin word-sense similarity measure (Lin ,1998a).
Adjectives and adverbs do not have anis-a hierarchy and hence do not figure in the Linsimilarity measure.
To disambiguate the Word-Net sense of a word in a sentence, a variant ofthe Simplified Lesk Algorithm (Kilgarriff andJ.
Rosenzweig , 2000) is used.
WordNet basedword similarity has the following drawbacks :?
sparse in named entity content : similarityof named entities with other words becomesinfeasible to calculate.?
doesn?t support cross-POS similarity.?
applicable only to nouns and verbs.4.2.2 Statistical Word SimilarityWe use DISCO (Kolb , 2008) as our statisti-cal word similarity measure.
DISCO is a tool forretrieving the distributional similarity betweentwo given words.
Pre-computed word spaces arefreely available for a number of languages.
Weuse the English Wikipedia word space.
One pri-mary reason for using a statistical word similaritymeasure is because of the shortcomings of calcu-lating cross-POS word similarity when using aknowledge base like WordNet.DISCO works as follows : a term-(term,relative position) matrix is constructed withweights being pointwise mutual informationscores.
From this, a surface level word similar-ity score is obtained by using Lin?s informationtheoretic measure (Lin , 1998b) for word vectorsimilarity.
This score is used as matrix weightsto get second order word vectors, which are usedto compute a second order word similarity mea-sure .
This measure tries to emulate an LSA likesimilarity giving better performance, and henceis used for the task.A point to note here is that the precomputedword spaces that DISCO uses are case sensitive,which we think is a drawback.
We preserve thecase of proper nouns, while all other words areconverted to lower case, prior to evaluating wordsimilarity scores.4.3 Edge Weighting SchemeSentences in the MSR video dataset are simplerand shorter than the remaining datasets, with ahigh degree of POS correspondence between the581Dataset DISCO WordNet DISCO + WordNetMSR Video 0.61 0.71 0.73MSR Paraphrase 0.62 0.43 0.57SMT Europarl 0.58 0.44 0.54Figure 1: Edge Weight Scheme Evaluation on Development DatasetsCategory NE Normalized NEDATE 26th November, November 26 XXXX-11-26PERCENT 13 percent, 13% %13.0MONEY 56 dollars, $56, 56$ $56.0TIME 3 pm, 15:00 T15:00Figure 2: Normalization performed by Stanford CoreNLPtokens of two sentences, as can be observed inthe following example :?
A man is riding a bicycle.
VS A man is rid-ing a bike.This allows for the use of a Knowledge-BaseWord Similarity measure like WordNet wordsimilarity.
All the other datasets have length-ier sentences, resulting in cross-POS correspon-dence.
Additionally, there is an abundance ofnamed entities in these datasets.
The followingexamples, which are drawn from the MSR Para-phrase dataset, highlight these points :?
If convicted of the spying charges, he couldface the death penalty.
VS The charges ofespionage and aiding the enemy can carrythe death penalty.?
Microsoft has identified the freely dis-tributed Linux software as one of the biggestthreats to its sales.
VS The companyhas publicly identified Linux as one of itsbiggest competitive threats.Keeping this in mind, we use DISCO for edge-weighting in all the datasets except MSR Video.For MSR Video, we use the following edgeweighting scheme : for same-POS words, Word-Net similarity is used, DISCO otherwise.
Thischoice is justified by the results obtained in fig-ure 1 on the development datasets.4.3.1 ScoringA maximal weighted bipartite match is foundfor the bipartite graph constructed, using theHungarian Algorithm (Kuhn , 1955) - theintuition behind this being that every keywordin a sentence matches injectively to a uniquekeyword in the other sentence.
The maximalbipartite score is normalized by the sentences?length for two reasons - normalization andpunishment for extra detailing in either sentence.So the final sentence similarity score betweensentences s1 and s2 is:sim(s1, s2) =MaximalBipartiteMatchSum(s1,s2)max(tokens(s1),tokens(s2))4.4 ResultsThe results are evaluated on the test datasetsprovided for the STS task.
Figure 3 comparesthe performance of our systems with the top 3systems for the task.
The scores in the figureare Pearson Correlation scores.
Figure 4 showsthe performance and ranks of all our systems.
Atotal of 89 systems were submitted, includingthe baseline.
The results are taken from theSemeval?12 Task 6 webpage1As can be seen, System 1 suffers slightlyon the MSR Paraphrase and Video datasets,while doing comparably well on the other threedatasets when compared with the top 3 submis-sions.
Our ALL score suffers because we use1http://www.cs.york.ac.uk/semeval-2012/task6/index.php?id=results-update582System ALL MSR Para-phraseMSR Video SMT Eu-roparlOnWN SMT NewsRank 1 0.8239 0.6830 0.8739 0.5280 0.6641 0.4937Rank 2 0.8138 0.6985 0.8620 0.3612 0.7049 0.4683Rank 3 0.8133 0.7343 0.8803 0.4771 0.6797 0.3989System 1 0.6529 0.6124 0.7240 0.5581 0.6703 0.4533System 2 0.6651 0.6254 0.7538 0.5328 0.6649 0.5036System 3 0.5045 0.6167 0.7061 0.5666 0.5664 0.3968Li et al 0.4981 0.6141 0.6084 0.5382 0.6055 0.3760Baseline 0.3110 0.4334 0.2996 0.4542 0.5864 0.3908Figure 3: Results of top 3 Systems and Our SystemsSystem ALL ALL Rank All Nor-malizedAll Nor-malizedRankWeightedMeanWeightedMeanRankSystem 1 0.6529 30 0.8018 39 0.6249 12System 2 0.6651 24 0.8128 22 0.6366 8System 3 0.5045 62 0.7846 52 0.5905 30Figure 4: Evaluation of our Systems on different criteriaa combination of WordNet and statistical wordsimilarity measure for the MSR Video dataset,which affects the Pearson Correlation of all thedatasets combined.
The correlation values forthe ALL Normalized criterion are high becauseof the linear fitting it performs.
We get the bestperformance on the Weighted Mean evaluationcriterion.5 SYSTEM 2In System 2, in addition to System 1, we cap-ture named entities, dates and times, percentagesand monetary entities and normalize them.
Thetokens resulting from this can be multi-word be-cause of named entities.
This tokenization strat-egy gives us the best results among all our threeruns.
For capturing and normalizing the abovementioned expressions, we make use of the Stan-ford NER Toolkit (Finkel et al, 2005).
Somenormalized samples are mentioned in figure 2.When grading the similarity of multi-wordtokens, we use a second level maximal bipartitematch, which is normalized by the smaller of thetwo multi-word token lengths.
Thus, similaritybetween two multi-word tokens t1 and t2 isdefined as:sim(t1, t2) =MaximalBipartiteMatchSum(t1,t2)min(words(t1),words(t2))This was done to ensure that a completenamed entity in the first sentence matches ex-actly with a partial named entity (indicating thesame entity as the first) in the second sentence.For eg.
John Doe vs John will be given a scoreof 1.
Such occurrences are frequent in the taskdatasets.
For the sentence similarity, the scoredefined in System 1 is used, where the tokenlength of a sentence is the number of multi-wordtokens in it.5.1 ResultsRefer to figures 3 and 4 for results.This system gives the best results among allour systems.
The credit for this improvement canbe attributed to recognition and normalization ofnamed entities, dates and times, percentages andmonetary entities, as the datasets provided con-tain these in fairly large numbers.5836 SYSTEM 3In System 3, in addition to System 2, we heuris-tically capture compound nouns, adjectivallyand numerically modified words like ?passengerplane?, ?easy job?, ?10 years?
etc.
using the POSbased regular expression[JJ |NN |CD]?NNPOS Tagging is done using the Stanford POSTagger Toolkit (Toutanova et al, 2003).To make matching more context dependent,rather than just a bag of words approach, wenaively attempt to capture the similarity of thecontexts of two tokens.
We define the contextof a word in a sentence as all the words in thesentence which are grammatically related toit.
The grammatical relations are all the col-lapsed dependencies produced by the StanfordDependency parser (Marneffe et al, 2006).
Thecontext of a multi-word token is defined as theunion of contexts of all the words in it.
Wefurther filter the context by removing stop-wordsand punctuations in it.
The contexts of twotokens are then used to obtain context/syntacticsimilarity between tokens, which is definedusing the Jaccard Similarity Measure:Jaccard(C1, C2) =|C1 ?
C2||C1 ?
C2|A linear combination of word similarity andcontext similarity is taken as an edge weight inthe token-token similarity bipartite graph.
Moti-vated by (Li et al, 2006), we chose a ratio of 4:1for lexical similarity to context similarity.As in System 2, for multi-word token simi-larity, we use a second level maximal bipartitematch, normalized by smaller of the two tokenlengths.
This helps in matching multi-word to-kens expressing the same meaning with score1, for e.g.
passenger plane VS Cuban plane,divided Supreme Court VS Supreme Court etc.The sentence similarity score is the same as theone defined in System 2.6.1 ResultsRefer to figures 3 and 4 for results.This system gives a reduced performancecompared to our other systems.
This could bedue to various factors.
Capturing adjectivally andnumerically modified words could be done usinggrammatical dependencies instead of a heuristicPOS-tag regular expression.
Also, token-tokensimilarity should be handled in a more preciseway than a generic second level maximal bipar-tite match.
A better context capturing methodcan further improve the system.7 ConclusionsAmong the three systems proposed for the task,System 2 performs best on the test datasets, pri-marily because it identifies named entities as sin-gle entities, normalizes dates, times, percentagesand monetary figures.
The results for System3 suffer because of naive context capturing.
Abetter job can be done using syntacto-semanticstructured representations for the sentences.
Theperformance of our systems are compared with(Li et al, 2006) on the test datasets in figure3.
This highlights the improvement of maximalweighted bipartite matching over greedy match-ing.8 Future WorkOur objective is to group words together whichshare a common meaning.
This includes group-ing adjectival, adverbial, numeric modifiers withthe modified word, group the words of a collo-quial phrase together, capture multi-word expres-sions, etc.
These word-clusters will form the ver-tices of the bipartite graph.
The other challengethen is to come up with a suitable cluster-clustersimilarity measure.
NLP modules such as Lex-ical Substitution can help when we are using aword-word similarity measure at the core.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their valuable comments and sug-gestions to improve the quality of the paper.584ReferencesDan Klein and Christopher D. Manning.
2003.
Ac-curate Unlexicalized Parsing.
Proceedings of the41st Meeting of the Association for ComputationalLinguistics, pp.
423-430.Eduard Hovy, Mitchell Marcus, Martha Palmer,Lance Ramshaw and Ralph Weischedel.
2006.OntoNotes: The 90% Solution.
Proceedings ofHLT/NAACL, New York, 2006.Eneko Agirre, Daniel Cer, Mona Diab and AitorGonzalez-Agirre.
2012.
SemEval-2012 Task 6:A Pilot on Semantic Textual Similarity.
In Pro-ceedings of the 6th International Workshop on Se-mantic Evaluation (SemEval 2012), in conjunc-tion with the First Joint Conference on Lexical andComputational Semantics (*SEM 2012).G.
Minnen, J. Carroll and D. Pearce.
2001.
Ap-plied morphological processing of English.
Nat-ural Language Engineering, 7(3).
207-223.Harold W. Kuhn.
1955.
The Hungarian Method forthe assignment problem.
Naval Research LogisticsQuarterly, 2:8397, 1955.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating Non-local In-formation into Information Extraction Systems byGibbs Sampling.
Proceedings of the 43nd AnnualMeeting of the Association for Computational Lin-guistics (ACL 2005), pp.
363-370Kilgarriff and J. Rosenzweig.
2000.
English SEN-SEVAL : Report and Results.
In Proceedings ofthe 2nd International Conference on Language Re-sources and Evaluation, LREC, Athens, Greece.Kristina Toutanova, Dan Klein, Christopher Man-ning, and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Net-work.
In Proceedings of HLT-NAACL 2003, pp.252-259.Lin, D. 1998a.
An information-theoretic definitionof similarity.
In Proceedings of the InternationalConference on Machine Learning.Lin, D. 1998b.
Automatic Retrieval and Clustering ofSimilar Words..
In Proceedings of COLING-ACL1998, Montreal.Marie-Catherine de Marneffe, Bill MacCartney andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.In LREC 2006.Palakorn Achananuparp, Xiaohua Hu and Shen Xi-ajiong.
2008.
The Evaluation of Sentence Simi-larity Measures.
Science And Technology, 5182,305-316.
Springer.Peter Kolb.
2008.
DISCO: A Multilingual Databaseof Distributionally Similar Words.
In Proceedingsof KONVENS-2008, Berlin.Steven Bird, Ewan Klein, and Edward Loper.
2009.Natural Language Processing with Python - An-alyzing Text with the Natural Language Toolkit.O?Reilly Media, 2009Xiao-Ying Liu, Yi-Ming Zhou, Ruo-Shi Zheng.2008.
Measuring Semantic Similarity Within Sen-tences.
Proceedings of the Seventh InternationalConference on Machine Learning and Cybernetics,Kunming.Yuhua Li, David McLean, Zuhair A. Bandar, JamesD.
OShea, and Keeley Crockett.
2006.
SentenceSimilarity Based on Semantic Nets and CorpusStatistics.
IEEE Transections on Knowledge andData Engineering, Vol.
18, No.
8585
