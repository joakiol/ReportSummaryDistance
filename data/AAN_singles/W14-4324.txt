Proceedings of the SIGDIAL 2014 Conference, pages 181?185,Philadelphia, U.S.A., 18-20 June 2014.c?2014 Association for Computational LinguisticsEvaluating a Spoken Dialogue System that Detects and Adapts to UserAffective StatesDiane LitmanComputer Science Dept.
& LRDCUniversity of PittsburghPittsburgh, PA 15260 USAdlitman@pitt.eduKate Forbes-RileyLearning Research & Development CenterUniversity of PittsburghPittsburgh, PA 15260 USAforbesk@pitt.eduAbstractWe present an evaluation of a spokendialogue system that detects and adaptsto user disengagement and uncertainty inreal-time.
We compare this version of oursystem to a version that adapts to only userdisengagement, and to a version that ig-nores user disengagement and uncertaintyentirely.
We find a significant increase intask success when comparing both affect-adaptive versions of our system to our non-adaptive baseline, but only for male users.1 IntroductionThere is increasing interest in building dialoguesystems that can detect and adapt to user affec-tive states.1However, while this line of research ispromising, there is still much work to be done.
Forexample, most research has focused on detectinguser affective states, rather than on developing di-alogue strategies that adapt to such states once de-tected.
In addition, when affect-adaptive dialoguesystems have been developed, most systems detectand adapt to only a single user state, and typicallyassume that the same affect-adaptive strategy willbe equally effective for all users.In this paper we take a step towards examin-ing these issues, by presenting an evaluation ofthree versions of an affect-adaptive spoken tuto-rial dialogue system: one that detects and adaptsto both user disengagement and uncertainty, onethat adapts to only disengagement, and one thatdoesn?t adapt to affect at all.
Our evaluation exam-ines the impact of adapting to differing numbers ofaffective states on task success, and also examinesinteractions with user gender.
We target disen-gagement and uncertainty because these were the1We use the term affect to describe emotions and attitudesthat impact how people communicate.
Other researchers alsocombine concepts of emotion, arousal, and attitudes whereemotion is not full-blown (Cowie and Cornelius, 2003).most frequent affective states in prior studies withour system and their presence was negatively cor-related with task success2(Forbes-Riley and Lit-man, 2011; Forbes-Riley and Litman, 2012).
Thedetection of these and similar states is also of in-terest to the larger speech and language processingcommunities, e.g.
(Wang and Hirschberg, 2011;Bohus and Horvitz, 2009; Pon-Barry and Shieber,2011).
Our results suggest that while adaptingto affect increases task success compared to notadapting at all, the utility of our current methodsvaries with user gender.
Also, we find no differ-ence between adapting to one or two states.2 Related Work2.1 Adapting to Multiple Affective StatesWhile prior research has shown that users displaya range of affective states during spoken dialogue(e.g.
(Schuller et al., 2009)), only a few dialoguesystems have been developed that can adapt tomore than one user affective state (e.g., (D?Melloet al., 2010; Acosta and Ward, 2011)).
Further-more, prior evaluations have compared adaptingto at least one affective state to not adapting to af-fect at all, but have not examined the benefits ofadapting to one versus multiple affective states.In a first evaluation comparing singly and mul-tiply affect-adaptive dialogue systems, we com-pared an existing system that adapted to uncer-tainty to a new version that also adapted to disen-gagement (Forbes-Riley and Litman, 2012).
Themultiply-adaptive system increased motivation forusers with high disengagement, and reduced bothuncertainty and the likelihood of continued dis-engagement.
However, this evaluation was onlyconducted in a ?Wizard-of-Oz?
scenario, where ahidden human replaced the speech recognition, se-mantic analysis, and affect detection componentsof our dialogue system.
We also conducted a post-2Our success measure is learning gain (Section 4).181hoc correlational (rather than causal) study, usingdata from an earlier fully-automated version of theuncertainty-adaptive system.
Regressions demon-strated that using both automatically labeled dis-engagement and uncertainty to predict task suc-cess significantly outperformed using only disen-gagement (Forbes-Riley et al., 2012).
However, ifmanual labels were instead used, only disengage-ment was predictive of learning, and adding un-certainty didn?t help.
This suggests that detectingmultiple affective states might compensate for thenoise that is introduced in a fully-automated sys-tem.
In this paper we further investigate this hy-pothesis, by evaluating the utility of adapting tozero, one, or two affective states in a controlledexperiment involving fully-automated systems.2.2 Gender Effects in DialogueDifferences in dialogue structure have been foundbetween male and female students talking to a hu-man tutor (Boyer et al., 2007).
Studies have alsoshown gender differences in conversational en-trainment patterns, for acoustic-prosodic featuresin human-human dialogues (Levitan et al., 2012)and articles in movie conversations (Danescu-Niculescu-Mizil and Lee, 2011).
For dialogue sys-tems involving embodied conversational agents,gender effects have been found for facial dis-plays, with females preferring more expressiveagents (Foster and Oberlander, 2006).
When usedfor tutoring, females report more positive affectwhen a learning companion is used, while malesare more negative (Woolf et al., 2010).In our own prior work, we compared twouncertainty-adaptive and one non-adaptive ver-sions of a wizarded dialogue system.
Our resultsdemonstrated that only one method of adapting touser uncertainty increased task success, and onlyfor female users (Forbes-Riley and Litman, 2009).In this paper we extend this line of research, byadding an affective dialogue system that adaptsto two rather than just one user state to our eval-uation, and by moving from wizarded to fully-automated systems.3 System, Experiment and CorpusOur corpus consists of dialogues betweenusers and three different versions of ITSPOKE(Intelligent Tutoring SPOKEn dialog sys-tem) (Forbes-Riley and Litman, 2011; Forbes-Riley and Litman, 2012).
ITSPOKE is aspeech-enhanced and otherwise modified versionof the Why2-Atlas text-based qualitative physicstutor (VanLehn et al., 2002) that interacts withusers using a system initiative dialogue strategy.User speech is first digitized from head-mountedmicrophone input and sent to the PocketSphinxrecognizer.3The recognition output is then clas-sified as (in)correct with respect to the anticipatedphysics content via semantic analysis (Jordanet al., 2007).
Simultaneously, user uncertainty(UNC) and disengagement (DISE) are classifiedfrom prosodic, lexical and contextual featuresusing two binary classification models (Forbes-Riley et al., 2012).
All statistical components ofthe speech recognizer, the semantic analyzer, andthe uncertainty and disengagement detectors weretrained using prior ITSPOKE corpora.4Finally,ITSPOKE?s response is determined based on theanswer?s automatically labeled (in)correctness,(un)certainty, and (dis)engagement and then sentto the Cepstral text-to-speech system,5as well asdisplayed on a web-based interface.Our corpus was collected in an experiment con-sisting of three conditions (CONTROL, DISE,DISE+UNC), where ITSPOKE used a differentmethod of affect-adaptation in each condition.The experiment was designed to compare the ef-fectiveness of not adapting to user affect in IT-SPOKE (CONTROL), adapting to user disengage-ment (DISE), and adapting to user disengagementas well as user uncertainty (DISE+UNC).6In CONTROL, ITSPOKE?s responses to userutterances were based on only the correctness ofuser answers.
This version of the system thus ig-nored any automatically detected user disengage-ment or uncertainty.
In particular, after each cor-rect answer, ITSPOKE provided positive feedbackthen moved on to the next topic.
After incor-rect answers, ITSPOKE instead provided negative3http://www.speech.cs.cmu.edu/pocketsphinx4We have not yet performed the manual annotationsneeded to evaluate our current versions of these componentsin isolation.
However, earlier versions of our affect detec-tors yielded FMeasures of 69% and 68% for disengagementand uncertainty, respectively, on par with the best perform-ing affect detectors in the wider literature (Forbes-Riley andLitman, 2011; Forbes-Riley et al., 2012).5http://www.cepstral.com6We did not include an uncertainty-only condition (UNC)because in previous work we compared UNC versus CON-TROL (Forbes-Riley and Litman, 2011) and DISE+UNCversus UNC (Forbes-Riley and Litman, 2012).
Further de-tails and motivation for all experimental conditions can befound in the description of our earlier Wizard-of-Oz experi-ment (Forbes-Riley and Litman, 2012).182feedback, then provided remediation tutoring be-fore moving on to the next topic.In DISE, two adaptive responses were devel-oped to allow ITSPOKE?s responses to consideruser disengagement as well as the correctness ofthe user?s answer;7however, this system versionstill ignored user uncertainty.
In particular, af-ter each disengaged+correct answer, ITSPOKEprovided correctness feedback, a progress chartshowing user correctness on prior problems andthe current problem, and a brief re-engagementtip.
After each disengaged+incorrect answer, IT-SPOKE provided incorrectness feedback, a briefre-engagement tip, and an easier supplemental ex-ercise, which consisted of an easy fill-in-the-blanktype question to reengage the user, followed by re-mediation targeting the material on which the userdisengaged and answered incorrectly.
Examplesof both types of adaptive responses are shown inA.1 and A.2 of Appendix A, respectively.In DISE+UNC, ITSPOKE responded to dis-engagement as just described, but also adaptedto uncertainty.
In particular, after each uncer-tain+correct answer, ITSPOKE provided positivecorrectness feedback, but then added the remedia-tion designed for incorrect answers with the goalof reducing the user?s uncertainty.
A dialogue ex-cerpt illustrating this strategy is shown in A.3 ofAppendix A.
Note that when a single utterance ispredicted to be both disengaged and uncertain, theDISE and UNC adaptations are combined.Finally, our experimental procedure was as fol-lows.
College students who were native Englishspeakers and who had no college-level physicsread a short physics text, took a pretest, worked5 physics problems (one problem per dialogue)with the version of ITSPOKE from their experi-mental condition, and took a posttest isomorphicto the pretest.
The pretest and posttest were takenfrom our Wizard-of-Oz experiment and each con-tained 26 multiple choice physics questions.
Ourexperiment yielded a corpus of 335 dialogues (5per user) from 67 users (39 female and 28 male).Average pretest8and posttest scores were 50.4%and 74.7% (out of 100%), respectively.4 Performance AnalysisBased on the prior research discussed in Section 2,we had two experimental hypotheses:7Engaged answers were treated as in CONTROL.8Pretest did not differ across conditions (p = .92).Condition Learning Gain NMean (%) Std ErrDISE+UNC 53.2 5.0 23DISE 51.4 4.8 22CONTROL 46.6 4.7 22Gender Learning Gain NMale 53.2 4.3 28Female 47.6 3.6 39Table 1: No effect of experimental condition(p=.62) or gender (p=.32) on learning gain.Gender Condition Learning Gain NMn (%) Std ErrMale DISE+UNC 58.8 8.4 7DISE 62.2 7.0 10CONTROL 38.7 6.7 11Female DISE+UNC 47.5 5.6 16DISE 40.6 6.4 12CONTROL 54.6 6.7 11Table 2: Significant interaction between the ef-fects of gender and condition on learning (p=.02).H1: Responding to multiple affective states willyield greater task success than responding to onlya single state (DISE+UNC > DISE), which inturn will outperform not responding to affect at all(DISE > CONTROL).H2: The effects of ITSPOKE?s affect-adaptation method and of gender will interact.A two-way analysis of variance (ANOVA) wasthus conducted to examine the effect of ex-perimental condition (DISE+UNC, DISE, CON-TROL) and user gender (Male, Female) on tasksuccess.
As is typical in the tutoring domain, tasksuccess was computed as (normalized) learninggain:posttest?pretest100?pretest.Table 1 shows that although our results pat-terned as hypothesized when considering all users,the differences in learning gains were not statisti-cally different across experimental conditions, F(2, 61) = .487, p = .617.
There were also no maineffects of gender, F (1, 61) = 1.014, p = .318.In contrast, as shown in Table 2, there was astatistically significant interaction between the ef-fects of user gender and experimental condition onlearning gains, F (2, 61) = 4.141, p = .021.
Wethus tested the simple effects of condition withineach level of gender to yield further insights.For males, simple main effects analysis showed183that there were statistically significant differencesin learning gains between experimental conditions(p = .042).
In particular, males in the DISE con-dition had significantly higher learning gains thanmales in the CONTROL condition (p = .019).Males in the DISE+UNC condition also showeda trend for higher learning gains than males in theCONTROL condition (p = .066).
However, malesin the DISE and DISE+UNC conditions showedno difference in learning gains (p= .760).For females, in contrast, simple main effectsanalysis showed no statistically significant differ-ences in learning gains between any experimentalconditions (p = .327).In sum, hypothesis H1 regarding the utility ofaffect adaptations was only partially supported byour results, where (DISE+UNC = DISE) > CON-TROL, and only for males.
That is, adapting toaffect was indeed better than not adapting at all,but only for males (supporting hypothesis H2).Contrary to H1, adapting to uncertainty over andabove disengagement did not provide any bene-fit compared to adapting to disengagement alone(DISE+UNC = DISE), for both genders.5 Discussion and Future DirectionsOur results contribute to the increasing bodyof literature demonstrating the utility of addingfully-automated affect-adaptation to existing spo-ken dialogue systems.
In particular, males inour two affect-adaptive conditions (DISE+UNCand DISE) learned more than males in thenon-adaptive CONTROL.
While our prior workdemonstrated the benefits of adapting to uncer-tainty, the current results demonstrate the impor-tance of adapting to disengagement either aloneor in conjunction with uncertainty.
However, wealso predicted that DISE+UNC should outperformDISE, which was not the case.
In future work wewill examine other performance measures besideslearning, and will manually annotate true disen-gagement and uncertainty in order to group stu-dents by amount of disengagement.
Furthermore,since the motivating prior studies discussed in Sec-tion 2 were based on older versions of our system,annotation could identify problematic differencesin training and testing data.
A final potential is-sue is that the re-engagement tips do not conveyexactly the same information.Second, our results contribute to the literaturesuggesting that gender effects should be consid-ered when designing dialogue systems.
We seesimilar results as in our prior work; namely ourcurrent results continue to suggest that males don?tbenefit from adapting to their uncertainty as com-pared to ignoring it, but our current results alsosuggest that males do benefit from adapting totheir disengagement.
On the other hand, our cur-rent results suggest that females do not benefitfrom our disengagement adaptation and moreover,combining it with our uncertainty adaptation re-duces the benefit of the uncertainty adaptation forthem.
This suggests the possibility of a differ-ing affective hierarchy, in terms of how affectivestates may impact the learning process of the twogenders differently.
Our results yield an empiricalbasis for future investigations into whether adap-tive system performance can improve by adaptingto affect differently based on gender.
However,further research is needed to determine more ef-fective combinations of disengagement and uncer-tainty adaptations for both males and females, andto investigate whether gender differences might berelated to other types of measurable user factors.AcknowledgmentsThis work is funded by NSF 0914615.
We thank S.Silliman for experimental support, and H. Nguyen,W.
Xiong, and the reviewers for feedback.ReferencesJ.
C. Acosta and N. G. Ward.
2011.
Achieving rapportwith turn-by-turn, user-responsive emotional color-ing.
Speech Communication, 53(9-10):1137?1148.D.
Bohus and E. Horvitz.
2009.
Models for multipartyengagement in open-world dialog.
In Proceedingsof SIGdial, pages 225?234, London, UK.K.
Boyer, M. Vouk, and J. Lester.
2007.
The influ-ence of learner characteristics on task-oriented tuto-rial dialogue.
Frontiers in Artificial Intelligence andApplications, 158:365.R.
Cowie and R. R. Cornelius.
2003.
Describingthe emotional states that are expressed in speech.Speech Communication, 40(1-2):5?32.C.
Danescu-Niculescu-Mizil and L. Lee.
2011.Chameleons in imagined conversations: A new ap-proach to understanding coordination of linguisticstyle in dialogs.
In Proceedings of the 2nd Workshopon Cognitive Modeling and Computational Linguis-tics, pages 76?87.S.
D?Mello, B. Lehman, J. Sullins, R. Daigle,R.
Combs, K. Vogt, L. Perkins, and A. Graesser.1842010.
A time for emoting: When affect-sensitivityis and isn?t effective at promoting deep learning.
InIntelligent Tutoring Systems Conference, pages 245?254, Pittsburgh, PA, USA.K.
Forbes-Riley and D. Litman.
2009.
A usermodeling-based performance analysis of a wizardeduncertainty-adaptive dialogue system corpus.
InProceedings Interspeech, pages 2467?2470.K.
Forbes-Riley and D. Litman.
2011.
Benefitsand challenges of real-time uncertainty detectionand adaptation in a spoken dialogue computer tutor.Speech Communication, 53(9?10):1115?1136.K.
Forbes-Riley and D. Litman.
2012.
Adapting tomultiple affective states in spoken dialogue.
In Pro-ceedings of the 13th Annual Meeting of the SpecialInterest Group on Discourse and Dialogue, SIG-DIAL ?12, pages 217?226.K.
Forbes-Riley, D. Litman, H. Friedberg, and J. Drum-mond.
2012.
Intrinsic and extrinsic evaluation ofan automatic user disengagement detector for anuncertainty-adaptive spoken dialogue system.
InProceedings of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 91?102.M.
Foster and J. Oberlander.
2006.
Data-driven gen-eration of emphatic facial displays.
Proceedings ofEACL, 2006:353?363.P.
Jordan, B.
Hall, M. Ringenberg, Y. Cui, and C.P.Rose.
2007.
Tools for authoring a dialogue agentthat participates in learning studies.
In Proc.
Artifi-cial Intelligence in Education (AIED), pages 43?50.R.
Levitan, A. Gravano, L. Willson, S. Benus,J.
Hirschberg, and A. Nenkova.
2012.
Acoustic-prosodic entrainment and social behavior.
In Pro-ceedings of the North American Chapter of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 11?19.H.
Pon-Barry and S. Shieber.
2011.
Recognizing un-certainty in speech.
EURASIP Journal on Advancesin Signal Processing.B.
Schuller, S. Steidl, and A. Batliner.
2009.
The In-terspeech 2009 emotion challenge.
In Proc.
Inter-speech, pages 312?315, ISCA, Brighton, UK.K.
VanLehn, P. W. Jordan, C. Ros?e, D. Bhembe,M.
B?ottner, A. Gaydos, M. Makatchev, U. Pap-puswamy, M. Ringenberg, A. Roque, S. Siler, R. Sri-vastava, and R. Wilson.
2002.
The architecture ofWhy2-Atlas: A coach for qualitative physics essaywriting.
In Proc.
Intl.
Conf.
on Intelligent TutoringSystems.W.
Wang and J. Hirschberg.
2011.
Detecting levels ofinterest from spoken dialog with multistream predic-tion feedback and similarity based hierarchical fu-sion learning.
In Proc.
SIGDIAL, pages 152?161,Portland, Oregon.B.
Woolf, I. Arroyo, D. Cooper, W. Burleson, andK.
Muldner.
2010.
Affective tutors: Automatic de-tection of and response to student emotion.
In Ad-vances in Intelligent Tutoring Systems, pages 207?227.
Springer.Appendix A. Affect-Adaptive Examples.A.1 After a Predicted Disengaged+CorrectTurnITSPOKE: How does his velocity compare tothat of his keys?USER: same same sameITSPOKE: Right.
[Feedback] Here?s a quickprogress update.
[See Screenshot] It might helpto remember we will build on the topics we?rediscussing now.
[Re-engagement Tip] Now let?ssee what happens after the man releases his keys.
[Next Topic]A.2 After Predicting Disengaged+IncorrectITSPOKE: How does his velocity compare tothat of his keys?USER: verticalITSPOKE: That doesn?t sound right.
[Feed-back] Let?s think about this a little more.
[Re-engagement Tip] Since the man is holding hiskeys, they aren?t moving relative to each other.So their velocities must be WHAT?
[Supplemen-tal Exercise].A.3 After a Predicted Uncertain+Correct TurnITSPOKE: What are the forces exerted on theman after he releases his keys?USER: gravity??
?ITSPOKE: Right.
[Feedback] Gravity is theonly force acting on the man.
This force is alsocalled the weight of the man.
It?s always presentwhen an object is in earth?s gravitational field.[Remediation].
So what?s the direction of theforce of gravity on the man ?
[Next Topic]185
