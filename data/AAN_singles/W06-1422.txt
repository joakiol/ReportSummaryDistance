Proceedings of the Fourth International Natural Language Generation Conference, pages 136?138,Sydney, July 2006. c?2006 Association for Computational LinguisticsGENEVAL: A Proposal for Shared-task Evaluation in NLGEhud ReiterUniversity of Aberdeen, UKereiter@csd.abdn.ac.ukAnja BelzUniversity of Brighton, UKa.s.belz@brighton.ac.ukAbstractWe propose to organise a series of shared-task NLG events, where participants areasked to build systems with similar in-put/output functionalities, and these sys-tems are evaluated with a range of differ-ent evaluation techniques.
The main pur-pose of these events is to allow us to com-pare different evaluation techniques, bycorrelating the results of different evalua-tions on the systems entered in the events.1 BackgroundEvaluation is becoming increasingly important inNatural Language Generation (NLG), as in mostother areas of Natural Language Processing (NLP).NLG systems can be evaluated in many differ-ent ways, with different associated resource re-quirements.
For example, a large-scale task-effectiveness study with human subjects could lastover a year and cost more than US$100,000 (Re-iter et al, 2003); on the other hand, a small-scalecomparison of generated texts to human-writtenreference texts can be done in a manner of days.However, while the latter kind of study is veryappealing in terms of cost and time, and cheapand reliable evaluation techniques would be veryuseful for people developing and testing new NLGtechniques, it is only worth doing if we have rea-son to believe that its results tell us somethingabout how useful the generated texts are to realhuman users.
It is not obvious that this is the case(Reiter and Sripada, 2002).Perhaps the best way to study the reliability ofdifferent evaluation techniques, and more gener-ally to develop a better empirical understanding ofthe strengths and problems of different evaluationtechniques, is to perform studies where a range ofdifferent evaluation techniques are used to evalu-ate a set of NLG systems with similar functional-ities.
Correlating the results of the different eval-uation techniques will give us empirical insight asto how well these techniques work in practice.Unfortunately, few such studies have been car-ried out, perhaps because (to date) few NLG sys-tems have been built with comparable functional-ity (our own work in this area is discussed below).We hope to surmount this problem, by organising?shared task?
events to which NLG researchers cansubmit systems based on a supplied data set of in-puts and (human-written) text outputs.
We willthen carry out our evaluation experiments on thesubmitted systems.
We hope that such shared-taskevents will also make it easier for new researchersto get involved in NLG, by providing data sets andan evaluation framework.2 Comparative Evaluations in NLGThere is a long history of shared task initiativesin NLP, of which the best known is perhaps MUC(Hirschman, 1998); others include TREC, PARSE-VAL, SENSEVAL, and the range of shared tasks or-ganised by CoNLL.
Such exercises are now com-mon in most areas of NLP, and have had a majorimpact on many areas, including machine transla-tion and information extraction (see discussion ofhistory of shared-task initiatives and their impactin Belz and Kilgarriff (2006)).One of the best-known comparative studiesof evaluation techniques was by Papineni et al(2002) who proposed the BLEU metric for machinetranslation and showed that BLEU correlated wellwith human judgements when comparing severalmachine translation systems.
Several other studiesof this type have been carried out in the MT andSummarisation communities.The first comparison of NLG evaluation tech-niques which we are aware of is by Bangalore et al(2000).
The authors manually created severalvariants of sentences from the Wall Street Jour-nal, and evaluated these sentences using both hu-man judgements and several corpus-based metrics.They used linear regression to suggest a combina-tion of the corpus-based metrics which they be-136lieve is a better predictor of human judgementsthan any of the individual metrics.In our work (Belz and Reiter, 2006), we usedseveral different evaluation techniques (humanand corpus-based) to evaluate the output of fiveNLG systems which generated wind descriptionsfor weather forecasts.
We then analysed how wellthe corpus-based evaluations correlated with thehuman-based evaluations.
Amongst other things,we concluded that BLEU-type metrics work rea-sonably well when comparing statistical NLG sys-tems, but less well when comparing statistical NLGsystems to knowledge-based NLG systems.We worked in this domain because of the avail-ability of the SumTime corpus (Sripada et al,2003), which contains both numerical weatherprediction data (i.e., inputs to NLG) and humanwritten forecast texts (i.e., target outputs fromNLG).
We are not aware of any other NLG-relatedcorpora which contain a large number of texts andcorresponding input data sets, and are freely avail-able to the research community.3 Our ProposalWe intend to apply for funding for a three-yearproject to create more shared input/output data sets(we are focusing on data-to-text tasks for the rea-sons discussed in Belz and Kilgarriff (2006)), or-ganise shared task workshops, and create and testa range of methods for evaluating submitted sys-tems.3.1 Step 1: Create data setsWe intend to create input/output data sets that con-tain the following types of representations:?
raw non-linguistic input data;?
structured content representations, roughlycorresponding to document plans (Reiter andDale, 2000);?
semantic-level representations, roughly cor-responding to text specifications (Reiter andDale, 2000);?
actual human-authored corpus texts.The presence of intermediate representations inour data sets means that researchers who are justinterested in document planning, microplanning,or surface realisation do not need to build com-plete NLG systems in order to participate.We will create the semantic-level representa-tions by parsing the corpus texts, probably us-ing a LinGO parser1.
We will create the contentrepresentations using application-specific analysistools, similar to a tool we have already created forSumTime wind statements.
The actual data setswe currently intend to create are as follows (seealso summary in Table 1).SumTime weather statements: These are briefstatements which describe predicted precipitationand cloud over a forecast period.
We will extractthe texts (and the corresponding input data) fromthe existing SumTime corpus.Statistics summaries: We will ask people (prob-ably students) to write paragraph-length textualsummaries of statistical data.
The actual data willcome from opinion polls or national statistics of-fices.
The corpus will also include data about theauthors (e.g., age, sex, domain expertise).Nurses?
reports: As part of a new project at Ab-erdeen, Babytalk2, we will be acquiring a corpusof texts written by nurses to summarise the statusof a baby in a neonatal intensive care unit, alongwith the raw data this is based on (sensor read-ings, records of actions taken such as giving med-ication).3.2 Step 2: Organise workshopsThe second step is to organise workshops.
Weintend to use a fairly standard organisation (Belzand Kilgarriff, 2006).
We will release the datasets (but not the reference texts), give people sixmonths to develop systems, and invite people whosubmit systems to a workshop.
Participants cansubmit either complete data-to-text NLG systems,or components which just do document planning,microplanning, or realisation.We are planning to increase the number andcomplexity of tasks from one round to the next,as this has been useful in other NLP evaluations(Belz and Kilgarriff, 2006); for example, we willadd surface realisation as a separate task in round2 and layout/structuring task in round 3.We will carry out all evaluation activities (seebelow) ourselves, workshop participants will notbe involved in this.3.3 Step 3: EvaluationThe final step is to evaluate the systems and com-ponents submitted to the workshop.
As the main1http://lingo.stanford.edu/2http://www.csd.abdn.ac.uk/research/babytalk/137Corpus num texts num ref (*) text size main NLG challengesWeather statements 3000 300 1-2 sentences content det, lex choice, aggregationStatistical summaries 1000 100 paragraph above plus surface realisationNurses?
reports 200 50 several paras above plus text structuring/layout(*) In addition to the main corpus, we will also gather texts which will be used as reference texts forcorpus-based evaluations; ?num ref?
is the number of such texts.
These texts will not be released.Table 1: Planned GENEVAL data sets.purpose of this whole exercise is to see how welldifferent evaluation techniques correlate with eachother, we plan to carry out a range of differentevaluations, including the following.Corpus-based evaluations: We will developnew, linguistically grounded evaluation metrics,and compare these to existing metrics includingBLEU, NIST, and string-edit distance.
We will alsoinvestigate how sensitive different metrics are tosize and make-up of the reference corpus.Human-based preference judgements: We willinvestigate different experimental designs andmethods for overcoming respondent bias (e.g.what is known as ?central tendency bias?, wheresome respondents avoid judgements at either endof a scale).
As we showed previously (Belz andReiter, 2006) that there are significant inter-subjectdifferences in ratings, one thing we want to deter-mine is how many subjects are needed to get reli-able and reproducible results.Task performance.
This depends on the do-main, but e.g.
in the nurse-report domain wecould use the methodology of (Law et al, 2005),who showed medical professionals the texts, askedthem to make a treatment decision, and then ratedthe correctness of the suggested treatments.As well as recommendations about the appro-priateness of existing evaluation techniques, wehope the above experiments will allow us to sug-gest new evaluation techniques for NLG.4 Next StepsAt this point, we encourage NLG researchers togive us their views regarding our plans for the or-ganisation of GENEVAL, the data and evaluationmethods we are planning to use, to suggest addi-tional data sets or evaluation techniques, and espe-cially to let us know whether they would be inter-ested in participating.If our proposal is successful, we hope that theproject will start in summer 2007, with the firstdata set released in late 2007 and the first work-shop in summer 2008.
ELRA/ELDA have also al-ready agreed to help us with this work, contribut-ing human and data resources.ReferencesSrinavas Bangalore, Owen Rambow, and Steve Whit-taker.
2000.
Evaluation metrics for generation.
InProceedings of INLG-2000, pages 1?8.Anja Belz and Adam Kilgarriff.
2006.
Shared-taskevaluations in HLT: Lessons for NLG.
In Proceed-ings of INLG-2006.Anja Belz and Ehud Reiter.
2006.
Comparing auto-matic and human evaluation of NLG systems.
InProceedings of EACL-2006, pages 313?320.Lynette Hirschman.
1998.
The evolution of evaluation:Lessons from the Message Understanding Confer-ences.
Computer Speech and Language, 12:283?285.Anna Law, Yvonne Freer, Jim Hunter, Robert Logie,Neil McIntosh, and John Quinn.
2005.
Generat-ing textual summaries of graphical time series datato support medical decision making in the neonatalintensive care unit.
Journal of Clinical Monitoringand Computing, 19:183?194.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedingsof ACL-2002, pages 311?318.Ehud Reiter and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
Cambridge Univer-sity Press.Ehud Reiter and Somayajulu Sripada.
2002.
Shouldcorpora texts be gold standards for NLG?
In Pro-ceedings of INLG-2002, pages 97?104.Ehud Reiter, Roma Robertson, and Liesl Osman.
2003.Lessons from a failure: Generating tailored smokingcessation letters.
Artificial Intelligence, 144:41?58.Somayajulu Sripada, Ehud Reiter, Jim Hunter, and JinYu.
2003.
Exploiting a parallel text-data corpus.
InProceedings of Corpus Linguistics 2003, pages 734?743.138
