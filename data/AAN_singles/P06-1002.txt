Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 9?16,Sydney, July 2006. c?2006 Association for Computational LinguisticsGoing Beyond AER: An Extensive Analysis of Word Alignments andTheir Impact on MTNecip Fazil Ayan and Bonnie J. DorrInstitute of Advanced Computer Studies (UMIACS)University of MarylandCollege Park, MD 20742{nfa,bonnie}@umiacs.umd.eduAbstractThis paper presents an extensive evalua-tion of five different alignments and in-vestigates their impact on the correspond-ing MT system output.
We introducenew measures for intrinsic evaluations andexamine the distribution of phrases anduntranslated words during decoding toidentify which characteristics of differentalignments affect translation.
We showthat precision-oriented alignments yieldbetter MT output (translating more wordsand using longer phrases) than recall-oriented alignments.1 IntroductionWord alignments are a by-product of statisticalmachine translation (MT) and play a crucial rolein MT performance.
In recent years, researchershave proposed several algorithms to generate wordalignments.
However, evaluating word alignmentsis difficult because even humans have difficultyperforming this task.The state-of-the art evaluation metric?alignment error rate (AER)?attempts to balancethe precision and recall scores at the level ofalignment links (Och and Ney, 2000).
Other met-rics assess the impact of alignments externally,e.g., different alignments are tested by comparingthe corresponding MT outputs using automatedevaluation metrics (e.g., BLEU (Papineni et al,2002) or METEOR (Banerjee and Lavie, 2005)).However, these studies showed that AER andBLEU do not correlate well (Callison-Burch et al,2004; Goutte et al, 2004; Ittycheriah and Roukos,2005).
Despite significant AER improvementsachieved by several researchers, the improvementsin BLEU scores are insignificant or, at best, small.This paper demonstrates the difficulty in assess-ing whether alignment quality makes a differencein MT performance.
We describe the impact ofcertain alignment characteristics on MT perfor-mance but also identify several alignment-relatedfactors that impact MT performance regardless ofthe quality of the initial alignments.
In so doing,we begin to answer long-standing questions aboutthe value of alignment in the context of MT.We first evaluate 5 different word alignmentsintrinsically, using: (1) community-standardmetrics?precision, recall and AER; and (2) anew measure called consistent phrase error rate(CPER).
Next, we observe the impact of differ-ent alignments on MT performance.
We presentBLEU scores on a phrase-based MT system,Pharaoh (Koehn, 2004), using five different align-ments to extract phrases.
We investigate the im-pact of different settings for phrase extraction, lex-ical weighting, maximum phrase length and train-ing data.
Finally, we present a quantitative analy-sis of which phrases are chosen during the actualdecoding process and show how the distribution ofthe phrases differ from one alignment into another.Our experiments show that precision-orientedalignments yield better phrases for MT than recall-oriented alignments.
Specifically, they cover ahigher percentage of our test sets and result infewer untranslated words and selection of longerphrases during decoding.The next section describes work related to ouralignment evaluation approach.
Following thiswe outline different intrinsic evaluation measuresof alignment and we propose a new measure toevaluate word alignments within phrase-basedMTframework.
We then present several experimentsto measure the impact of different word align-ments on a phrase-based MT system, and inves-tigate how different alignments change the phrase9selection in the same MT system.2 Related WorkStarting with the IBM models (Brown et al,1993), researchers have developed various statis-tical word alignment systems based on differentmodels, such as hidden Markov models (HMM)(Vogel et al, 1996), log-linear models (Och andNey, 2003), and similarity-based heuristic meth-ods (Melamed, 2000).
These methods are un-supervised, i.e., the only input is large paral-lel corpora.
In recent years, researchers haveshown that even using a limited amount of manu-ally aligned data improves word alignment signif-icantly (Callison-Burch et al, 2004).
Supervisedlearning techniques, such as perceptron learn-ing, maximum entropy modeling or maximumweighted bipartite matching, have been shown toprovide further improvements on word alignments(Ayan et al, 2005; Moore, 2005; Ittycheriah andRoukos, 2005; Taskar et al, 2005).The standard technique for evaluating wordalignments is to represent alignments as a set oflinks (i.e., pairs of words) and to compare the gen-erated alignment against manual alignment of thesame data at the level of links.
Manual align-ments are represented by two sets: Probable (P )alignments and Sure (S) alignments, where S ?P .
Given A,P and S, the most commonly usedmetrics?precision (Pr), recall (Rc) and alignmenterror rate (AER)?are defined as follows:Pr =|A ?
P ||A|Rc =|A ?
S||S|AER = 1?|A ?
S|+ |A ?
P ||A|+ |S|Another approach to evaluating alignments is tomeasure their impact on an external application,e.g., statistical MT.
In recent years, phrase-basedsystems (Koehn, 2004; Chiang, 2005) have beenshown to outperform word-based MT systems;therefore, in this paper, we use a publicly-availablephrase-based MT system, Pharaoh (Koehn, 2004),to investigate the impact of different alignments.Although it is possible to estimate phrases di-rectly from a training corpus (Marcu and Wong,2002), most phrase-based MT systems (Koehn,2004; Chiang, 2005) start with a word alignmentand extract phrases that are consistent with thegiven alignment.
Once the consistent phrases areextracted, they are assigned multiple scores (suchTestLang # of # Words SourcePair Sent?s (en/fl)en-ch 491 14K/12K NIST MTEval?2002en-ar 450 13K/11K NIST MTEval?2003Trainingen-ch 107K 4.1M/3.3M FBISen-ar 44K 1.4M/1.1M News + TreebankTable 1: Test and Training Data Used for Experimentsas translation probabilities and lexical weights),and the decoder?s job is to choose the correctphrases based on those scores using a log-linearmodel.3 Intrinsic Evaluation of AlignmentsOur goal is to compare different alignments andto investigate how their characteristics affect theMT systems.
We evaluate alignments in terms ofprecision, recall, alignment error rate (AER), anda new measure called consistent phrase error rate(CPER).We focus on 5 different alignments obtained bycombining two uni-directional alignments.
Eachuni-directional alignment is the result of runningGIZA++ (Och, 2000b) in one of two directions(source-to-target and vice versa) with default con-figurations.
The combined alignments that areused in this paper are as follows:1.
Union of both directions (SU),2.
Intersection of both directions (SI),3.
A heuristic based combination techniquecalled grow-diag-final (SG), which is thedefault alignment combination heuristicemployed in Pharaoh (Koehn, 2004),4-5.
Two supervised alignment combinationtechniques (SA and SB) using 2 and 4 in-put alignments as described in (Ayan etal., 2005).This paper examines the impact of alignmentsaccording to their orientation toward precision orrecall.
Among the five alignments above, SU andSG are recall-oriented while the other three areprecision-oriented.
SB is an improved version ofSA which attempts to increase recall without a sig-nificant sacrifice in precision.Manually aligned data from two language pairsare used in our intrinsic evaluations using the fivecombinations above.
A summary of the trainingand test data is presented in Table 1.Our gold standard for each language pair isa manually aligned corpus.
English-Chinese an-10notations distinguish between sure and probablealignment links, but English-Arabic annotationsdo not.
The details of how the annotations aredone can be found in (Ayan et al, 2005) and (Itty-cheriah and Roukos, 2005).3.1 Precision, Recall and AERTable 2 presents the precision, recall, and AER for5 different alignments on 2 language pairs.
Foreach of these metrics, a different system achievesthe best score ?
respectively, these are SI, SU, andSB.
SU and SG yield low precision, high recallalignments.
In contrast, SI yields very high pre-cision but very low recall.
SA and SB attempt tobalance these two measures but their precision isstill higher than their recall.
Both systems havenearly the same precision but SB yields signifi-cantly higher recall than SA.Align.
en-ch en-arSys.
Pr Rc AER Pr Rc AERSU 58.3 84.5 31.6 56.0 84.1 32.8SG 61.9 82.6 29.7 60.2 83.0 30.2SI 94.8 53.6 31.2 96.1 57.1 28.4SA 87.0 74.6 19.5 88.6 71.1 21.1SB 87.8 80.5 15.9 90.1 76.1 17.5Table 2: Comparison of 5 Different Alignments using AER(on English-Chinese and English-Arabic)3.2 Consistent Phrase Error RateIn this section, we present a new method, calledconsistent phrase error rate (CPER), for evalu-ating word alignments in the context of phrase-based MT.
The idea is to compare phrases con-sistent with a given alignment against phrases thatwould be consistent with human alignments.CPER is similar to AER but operates at thephrase level instead of at the word level.
To com-pute CPER, we define a link in terms of the posi-tion of its start and end words in the phrases.
Forinstance, the phrase link (i1, i2, j1, j2) indicatesthat the English phrase ei1 , .
.
.
, ei2 and the FLphrase fj1 , .
.
.
, fj2 are consistent with the givenalignment.
Once we generate the set of phrasesPA and PG that are consistent with a given align-ment A and a manual alignment G, respectively,we compute precision (Pr), recall (Rc), and CPERas follows:1Pr =|PA ?
PG||PA|Rc =|PA ?
PG||PG|CPER = 1?2?
Pr ?RcPr + Rc1Note that CPER is equal to 1 - F-score.Chinese ArabicAlign.
CPER-3 CPER-7 CPER-3 CPER-7SU 63.2 73.3 55.6 67.1SG 59.5 69.4 52.0 62.6SI 50.8 69.8 50.7 67.6SA 40.8 51.6 42.0 54.1SB 36.8 45.1 36.1 46.6Table 3: Consistent Phrase Error Rates with MaximumPhrase Lengths of 3 and 7CPER penalizes incorrect or missing alignmentlinks more severely than AER.
While comput-ing AER, an incorrect alignment link reduces thenumber of correct alignment links by 1, affectingprecision and recall slightly.
Similarly, if there isa missing link, only the recall is reduced slightly.However, when computing CPER, an incorrect ormissing alignment link might result in more thanone phrase pair being eliminated from or added tothe set of phrases.
Thus, the impact is more severeon both precision and recall.Figure 1: Sample phrases that are generated from a humanalignment and an automated alignment: Gray cells show thealignment links, and rectangles show the possible phrases.In Figure 1, the first box represents a manualalignment and the other two represent automatedalignments A.
In the case of a missing align-ment link (Figure 1b), PA includes 9 valid phrases.For this alignment, AER = 1 ?
(2 ?
2/2 ?2/3)/(2/2 + 2/3) = 0.2 and CPER = 1?
(2?5/9?
5/6)/(5/9+5/6) = 0.33.
In the case of anincorrect alignment link (Figure 1c), PA includesonly 2 valid phrases, which results in a higherCPER (1?
(2?
2/2?
2/6)/(2/2+2/6) = 0.49)but a lower AER (1 ?
(2 ?
3/4 ?
3/3)/(3/4 +3/3) = 0.14).Table 3 presents the CPER values on two dif-ferent language pairs, using 2 different maximumphrase lengths.
For both maximum phrase lengths,SA and SB yield the lowest CPER.
For all 5alignments?in both languages?CPER increasesas the length of the phrase increases.
For allalignments except SI, this amount of increase isnearly the same on both languages.
Since SI con-tains very few alignment points, the number ofgenerated phrases dramatically increases, yielding11poor precision and CPER as the maximum phraselength increases.4 Evaluating Alignments within MTWe now move from intrinsic measurement to ex-trinsic measurement using an off-the-shelf phrase-based MT system Pharaoh (Koehn, 2004).
Ourgoal is to identify the characteristics of alignmentsthat change MT behavior and the types of changesinduced by these characteristics.All MT system components were kept the samein our experiments except for the component thatgenerates a phrase table from a given alignment.We used the corpora presented in Table 1 to trainthe MT system.
The phrases were scored usingtranslation probabilities and lexical weights in twodirections and a phrase penalty score.
We also usea language model, a distortion model and a wordpenalty feature for MT.We measure the impact of different alignmentson Pharaoh using three different settings:1.
Different maximum phrase length,2.
Different sizes of training data, and3.
Different lexical weighting.For maximum phrase length, we used 3 (basedon what was suggested by (Koehn et al, 2003) and7 (the default maximum phrase length in Pharaoh).For lexical weighting, we used the originalweighting scheme employed in Pharaoh and amodified version.
We realized that the publicly-available implementation of Pharaoh computesthe lexical weights only for non-NULL alignmentlinks.
As a consequence, loose phrases contain-ing NULL-aligned words along their edges receivethe same lexical weighting as tight phrases with-out NULL-aligned words along the edges.
Wetherefore adopted a modified weighting schemefollowing (Koehn et al, 2003), which incorporatesNULL alignments.MT output was evaluated using the standardevaluation metric BLEU (Papineni et al, 2002).2The parameters of the MT System were opti-mized for BLEU metric on NIST MTEval?2002test sets using minimum error rate training (Och,2003), and the systems were tested on NISTMTEval?2003 test sets for both languages.2We used the NIST script (version 11a) for BLEU withits default settings: case-insensitive matching of n-grams upto n = 4, and the shortest reference sentence for the brevitypenalty.
The words that were not translated during decodingwere deleted from the MT output before running the BLEUscript.The SRI Language Modeling Toolkit was usedto train a trigrammodel with modified Kneser-Neysmoothing on 155M words of English newswiretext, mostly from the Xinhua portion of the Gi-gaword corpus.
During decoding, the number ofEnglish phrases per FL phrase was limited to 100and phrase distortion was limited to 4.4.1 BLEU Score ComparisonTable 4 presents the BLEU scores for Pharaoh runson Chinese with five different alignments usingdifferent settings for maximum phrase length (3vs.
7), size of training data (107K vs. 241K), andlexical weighting (original vs. modified).3The modified lexical weighting yields huge im-provements when the alignment leaves severalwords unaligned: the BLEU score for SA goesfrom 24.26 to 25.31 and the BLEU score for SBgoes from 23.91 to 25.38.
In contrast, when thealignments contain a high number of alignmentlinks (e.g., SU and SG), modifying lexical weight-ing does not bring significant improvements be-cause the number of phrases containing unalignedwords is relatively low.
Increasing the phraselength increases the BLEU scores for all systemsby nearly 0.7 points and increasing the size of thetraining data increases the BLEU scores by 1.5-2points for all systems.
For all settings, SU yieldsthe lowest BLEU scores while SB clearly outper-forms the others.Table 5 presents BLEU scores for Pharaoh runson 5 different alignments on English-Arabic, usingdifferent settings for lexical weighting and max-imum phrase lengths.4 Using the original lexi-cal weighting, SA and SB perform better than theothers while SU and SI yield the worst results.Modifying the lexical weighting leads to slight re-ductions in BLEU scores for SU and SG, but im-proves the scores for the other 3 alignments signif-icantly.
Finally, increasing the maximum phraselength to 7 leads to additional improvements inBLEU scores, where SG and SU benefit nearly 2BLEU points.
As in English-Chinese, the worstBLEU scores are obtained by SU while the bestscores are produced by SB.As we see from the tables, the relation betweenintrinsic alignment measures (AER and CPER)3We could not run SB on the larger corpus because of thelack of required inputs.4Due to lack of additional training data, we could not doexperiments using different sizes of training data on English-Arabic.12Original Modified Modified ModifiedAlignment Max Phr Len = 3 Max Phr Len=3 Max Phr Len=7 Max Phr Len=3|Corpus| = 107K |Corpus| = 107K |Corpus| = 107K |Corpus| = 241KSU 22.56 22.66 23.30 24.40SG 23.65 23.79 24.48 25.54SI 23.60 23.97 24.76 26.06SA 24.26 25.31 25.99 26.92SB 23.91 25.38 26.14 N/ATable 4: BLEU Scores on English-Chinese with Different Lexical Weightings, Maximum Phrase Lengths and Training DataLW=Org LW=Mod LW=ModAlignment MPL=3 MPL=3 MPL=7SU 41.97 41.72 43.50SG 44.06 43.82 45.78SI 42.29 42.76 43.88SA 44.49 45.23 46.06SB 44.92 45.39 46.66Table 5: BLEU Scores on English-Arabic with DifferentLexical Weightings and Maximum Phrase Lengthsand the corresponding BLEU scores varies, de-pending on the language, lexical weighting, maxi-mum phrase length, and training data size.
For ex-ample, using a modified lexical weighting, the sys-tems are ranked according to their BLEU scores asfollows: SB, SA, SG, SI, SU?an ordering that dif-fers from that of AER but is identical to that ofCPER (with a phrase length of 3) for Chinese.
Onthe other hand, in Arabic, both AER and CPERprovide a slightly different ranking from that ofBLEU, with SG and SI swapping places.4.2 Tight vs.
Loose PhrasesTo demonstrate how alignment-related compo-nents of the MT system might change the trans-lation quality significantly, we did an additionalexperiment to compare different techniques for ex-tracting phrases from a given alignment.
Specifi-cally, we are comparing two techniques for phraseextraction:1.
Loose phrases (the original ?consistentphrase extraction?
method)2.
Tight phrases (the set of phrases wherethe first/last words on each side are forcedto align to some word in the phrase pair)Using tight phrases penalizes alignments withmany unaligned words, whereas using loosephrases rewards them.
Our goal is to comparethe performance of precision-oriented vs. recall-oriented alignments when we allow only tightphrases in the phrase extraction step.
To sim-plify things, we used only 2 alignments: SG, thebest recall-oriented alignment, and SB, the bestprecision-oriented alignment.
For this experiment,we used modified lexical weighting and a maxi-mum phrase length of 7.Chinese ArabicAlignment Loose Tight Loose TightSG 24.48 23.19 45.78 43.67SB 26.14 22.68 46.66 40.10Table 6: BLEU Scores with Loose vs.
Tight PhrasesTable 6 presents the BLEU scores for SG and SBusing two different phrase extraction techniqueson English-Chinese and English-Arabic.
In bothlanguages, SB outperforms SG significantly whenloose phrases are used.
However, when we useonly tight phrases, the performance of SB gets sig-nificantly worse (3.5 to 6.5 BLEU-score reductionin comparison to loose phrases).
The performanceof SG also gets worse but the degree of BLEU-score reduction is less than that of SB.
OverallSG performs better than SB with tight phrases;for English-Arabic, the difference between the twosystems is more than 3 BLEU points.
Note that, asbefore, the relation between the alignment mea-sures and the BLEU scores varies, this time de-pending on whether loose phrases or tight phrasesare used: both CPER and AER track the BLEUrankings for loose (but not for tight) phrases.This suggests that changing alignment-relatedcomponents of the system (i.e., phrase extractionand phrase scoring) influences the overall trans-lation quality significantly for a particular align-ment.
Therefore, when comparing two align-ments in the context of a MT system, it is im-portant to take the alignment characteristics intoaccount.
For instance, alignments with many un-aligned words are severely penalized when usingtight phrases.4.3 Untranslated WordsWe analyzed the percentage of words left untrans-lated during decoding.
Figure 2 shows the per-centage of untranslated words in the FL using theChinese and Arabic NIST MTEval?2003 test sets.On English-Chinese data (using all four settingsgiven in Table 4) SU and SG yield the highest per-centage of untranslated words while SI producesthe lowest percentage of untranslated words.
SAand SB leave about 2% of the FL words phrases13Figure 2: Percentage of untranslated words out of the totalnumber of FL wordswithout translating them.
Increasing the trainingdata size reduces the percentage of untranslatedwords by nearly half with all five alignments.
Nosignificant impact on untranslated words is ob-served from modifying the lexical weights andchanging the phrase length.On English-Arabic data, all alignments resultin higher percentages of untranslated words thanEnglish-Chinese, most likely due to data spar-sity.
As in Chinese-to-English translation, SUis the worst and SB is the best.
SI behavesquite differently, leaving nearly 7% of the wordsuntranslated?an indicator of why it produces ahigher BLEU score on Chinese but a lower scoreon Arabic compared to other alignments.4.4 Analysis of Phrase TablesThis section presents several experiments to an-alyze how different alignments affect the size ofthe generated phrase tables, the distribution of thephrases that are used in decoding, and the cover-age of the test set with the generated phrase tables.Size of Phrase Tables The major impact ofusing different alignments in a phrase-based MTsystem is that each one results in a different phrasetable.
Table 7 presents the number of phrasesthat are extracted from five alignments using twodifferent maximum phrase lengths (3 vs. 7) intwo languages, after filtering the phrase table forMTEval?2003 test set.
The size of the phrase tableincreases dramatically as the number of links inthe initial alignment gets smaller.
As a result, forboth languages, SU and SG yield a much smallerChinese ArabicAlignment MPL=3 MPL=7 MPL=3 MPL=7SU 106 122 32 38SG 161 181 48 55SI 1331 3498 377 984SA 954 1856 297 594SB 876 1624 262 486Table 7: Number of Phrases in the Phrase Table Filtered forMTEval?2003 Test Sets (in thousands)phrase table than the other three alignments.
Asthe maximum phrase length increases, the size ofthe phrase table gets bigger for all alignments;however, the growth of the table is more signifi-cant for precision-oriented alignments due to thehigh number of unaligned words.Distribution of Phrases To investigate how thedecoder chooses phrases of different lengths, weanalyzed the distribution of the phrases in the fil-tered phrase table and the phrases that were usedto decode Chinese MTEval?2003 test set.5 For theremaining experiments in the paper, we use mod-ified lexical weighting, a maximum phrase lengthof 7, and 107K sentence pairs for training.The top row in Figure 3 shows the distributionof the phrases generated by the five alignments(using a maximum phrase length of 7) accordingto their length.
The ?j-i?
designators correspondto the phrase pairs with j FL words and i Englishwords.
For SU and SG, the majority of the phrasescontain only one FL word, and the percentage ofthe phrases with more than 2 FL words is less than18%.
For the other three alignments, however, thedistribution of the phrases is almost inverted.
ForSI, nearly 62% of the phrases contain more than 3words on either FL or English side; for SA and SB,this percentage is around 45-50%.Given the completely different phrase distribu-tion, the most obvious question is whether thelonger phrases generated by SI, SA and SB areactually used in decoding.
In order to investigatethis, we did an analysis of the phrases used to de-code the same test set.The bottom row of Figure 3 shows the per-centage of phrases used to decode the ChineseMTEval?2003 test set.
The distribution of the ac-tual phrases used in decoding is completely the re-verse of the distribution of the phrases in the en-tire filtered table.
For all five alignments, the ma-jority of the used phrases is one-to-one (between5Due to lack of space, we will present results on Chinese-English only in the rest of this paper but the Arabic-Englishresults show the same trends.14Figure 3: Distribution of the phrases in the phrase tablefiltered for Chinese MTEval?2003 test set (top row) and thephrases used in decoding the same test set (bottom row) ac-cording to their lengths50-65% of the total number of phrases used in de-coding).
SI, SA and SB use the other phrase pairs(particularly 1-to-2 phrases) more than SU and SG.Note that SI, SA and SB use only a small portionof the phrases with more than 3 words although themajority of the phrase table contains phrases withmore than 3 words on one side.
It is surprisingthat the inclusion of phrase pairs with more than3 words in the search space increases the BLEUscore although the majority of the phrases used indecoding is mostly one-to-one.Length of the Phrases used in Decoding Wealso investigated the number and length of phrasesthat are used to decode the given test set for dif-ferent alignments.
Table 8 presents the averagenumber of English and FL words in the phrasesused in decoding Chinese MTEval?2003 test set.The decoder uses fewer phrases with SI, SA andSB than for the other two, thus yielding a highernumber of FL words per phrase.
The number ofEnglish words per phrase is also higher for thesethree systems than the other two.Coverage of the Test Set Finally, we examinethe coverage of a test set using phrases of a spe-cific length in the phrase table.
Table 9 presentsAlignment |Eng| |FL|SU 1.39 1.28SG 1.45 1.33SI 1.51 1.55SA 1.54 1.55SB 1.56 1.52Table 8: The average length of the phrases that are used indecoding Chinese MTEval?2003 test setthe coverage of the Chinese MTEval?2003 test set(source side) using only phrases of a particularlength (from 1 to 7).
For this experiment, we as-sume that a word in the test set is covered if it ispart of a phrase pair that exists in the phrase table(if a word is part of multiple phrases, it is countedonly once).
Not surprisingly, using only phraseswith one FL word, more than 90% of the test setcan be covered for all 5 alignments.
As the lengthof the phrases increases, the coverage of the testset decreases.
For instance, using phrases with 5FL words results in less than 5% coverage of thetest set.Phrase Length (FL)A 1 2 3 4 5 6 7SU 92.2 59.5 21.4 6.7 1.3 0.4 0.1SG 95.5 64.4 24.9 7.4 1.6 0.5 0.3SI 97.8 75.8 38.0 13.8 4.6 1.9 1.2SA 97.3 75.3 36.1 12.5 3.8 1.5 0.8SB 97.5 74.8 35.7 12.4 4.2 1.8 0.9Table 9: Coverage of Chinese MTEval?2003 Test Set UsingPhrases with a Specific Length on FL side (in percentages)Table 9 reveals that the coverage of the test setis higher for precision-oriented alignments thanrecall-oriented alignments for all different lengthsof the phrases.
For instance, SI, SA, and SB covernearly 75% of the corpus using only phrases with2 FL words, and nearly 36% of the corpus usingphrases with 3 FL words.
This suggests that recall-oriented alignments fail to catch a significant num-ber of phrases that would be useful to decode thistest set, and precision-oriented alignments wouldyield potentially more useful phrases.Since precision-oriented alignments make ahigher number of longer phrases available to thedecoder (based on the coverage of phrases pre-sented in Table 9), they are used more duringdecoding.
Consequently, the major differencebetween the alignments is the coverage of thephrases extracted from different alignments.
Themore the phrase table covers the test set, the morethe longer phrases are used during decoding, andprecision-oriented alignments are better at gener-ating high-coverage phrases than recall-orientedalignments.155 Conclusions and Future WorkThis paper investigated how different alignmentschange the behavior of phrase-based MT.
Weshowed that AER is a poor indicator of MTperformance because it penalizes incorrect linksless than is reflected in the corresponding phrase-based MT.
During phrase-based MT, an incorrectalignment link might prevent extraction of severalphrases, but the number of phrases affected by thatlink depends on the context.We designed CPER, a new phrase-oriented met-ric that is more informative than AER when thealignments are used in a phrase-based MT systembecause it is an indicator of how the set of phrasesdiffer from one alignment to the next according toa pre-specified maximum phrase length.Even with refined evaluation metrics (includingCPER), we found it difficult to assess the impactof alignment on MT performance because wordalignment is not the only factor that affects thechoice of the correct words (or phrases) duringdecoding.
We empirically showed that differentphrase extraction techniques result in better MToutput for certain alignments but the MT perfor-mance gets worse for other alignments.
Simi-larly, adjusting the scores assigned to the phrasesmakes a significant difference for certain align-ments while it has no impact on some others.
Con-sequently, when comparing two BLEU scores, it isdifficult to determine whether the alignments arebad to start with or the set of extracted phrases isbad or the phrases extracted from the alignmentsare assigned bad scores.
This suggests that findinga direct correlation between AER (or even CPER)and the automated MT metrics is infeasible.We demonstrated that recall-oriented alignmentmethods yield smaller phrase tables and a highernumber of untranslated words when compared toprecision-oriented methods.
We also showed thatthe phrases extracted from recall-oriented align-ments cover a smaller portion of a given test setwhen compared to precision-oriented alignments.Finally, we showed that the decoder with recall-oriented alignments uses shorter phrases more fre-quently as a result of unavailability of longerphrases that are extracted.Future work will involve an investigation intohow the phrase extraction and scoring should beadjusted to take the nature of the alignment intoaccount and how the phrase-table size might be re-duced without sacrificing the MT output quality.Acknowledgments This work has been supported, inpart, under ONR MURI Contract FCPO.810548265 and theGALE program of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011-06-2-0001.
We also thankAdam Lopez for his very helpful comments on earlier draftsof this paper.ReferencesNecip F. Ayan, Bonnie J. Dorr, and Christof Monz.
2005.Neuralign: Combining word alignments using neural net-works.
In Proceedings of EMNLP?2005, pages 65?72.Stanjeev Banerjee and Alon Lavie.
2005.
Meteor: An au-tomatic metric for MT evaluation with improved corre-lation with human judgments.
In Proceedings of Work-shop on Intrinsic and Extrinsic Evaluation Measures forMT and/or Summarization at ACL-2005.Peter F. Brown, Stephan A. Della Pietra, and Robert L. Mer-cer.
1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguistics,19(2):263?311.Chris Callison-Burch, David Talbot, and Miles Osborne.2004.
Statistical machine translation with word- andsentence-aligned parallel corpora.
In Proceedings ofACL?2004.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL?2005.Cyril Goutte, Kenji Yamada, and Eric Gaussier.
2004.
Align-ing words using matrix factorisation.
In Proceedings ofACL?2004, pages 502?509.Abraham Ittycheriah and Salim Roukos.
2005.
A maximumentropy word aligner for arabic-english machine transla-tion.
In Proceedings of EMNLP?2005.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.
Sta-tistical phrase-based translation.
In Proceedings of HLT-NAACL?2003.Philipp Koehn.
2004.
Pharaoh: A beam search decoder forphrase-based statistical machine translation.
In Proceed-ings of AMTA?2004.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine translation.In Proceedings of EMNLP?2002.I.
Dan Melamed.
2000.
Models of translational equivalenceamong words.
Computational Linguistics, 26(2):221?249.Robert C. Moore.
2005.
A discriminative frame-work for bilingual word alignment.
In Proceedings ofEMNLP?2005.Franz J. Och and Hermann Ney.
2000.
A comparison ofalignment models for statistical machine translation.
InProceedings of COLING?2000.Franz J. Och.
2000b.
GIZA++: Training of statistical transla-tion models.
Technical report, RWTH Aachen, Universityof Technology.Franz J. Och and Hermann Ney.
2003.
A systematic com-parison of various statistical alignment models.
Computa-tional Linguistics, 29(1):9?51, March.Franz J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of ACL?2003.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-JingZhu.
2002.
BLEU: a method for automatic evaluation ofmachine translation.
In Proceedings of ACL?2002.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.
2005.
Adiscriminative matching approach to word alignment.
InProceedings of EMNLP?2005.Stefan Vogel, Hermann Ney, and Christoph Tillmann.
1996.HMM-based word alignment in statistical translation.
InProceedings of COLING?1996, pages 836?841.16
