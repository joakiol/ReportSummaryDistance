Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 9?16,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Rote Extractor with Edit Distance-based Generalisation andMulti-corpora Precision CalculationEnrique Alfonseca12 Pablo Castells1 Manabu Okumura2 Maria Ruiz-Casado121Computer Science Deptartment 2Precision and Intelligence LaboratoryUniv.
Auto?noma de Madrid Tokyo Institute of TechnologyEnrique.Alfonseca@uam.es enrique@lr.pi.titech.ac.jpPablo.Castells@uam.es oku@pi.titech.ac.jpMaria.Ruiz@uam.es maria@lr.pi.titech.ac.jpAbstractIn this paper, we describe a rote extrac-tor that learns patterns for finding seman-tic relationships in unrestricted text, withnew procedures for pattern generalizationand scoring.
These include the use of part-of-speech tags to guide the generalization,Named Entity categories inside the pat-terns, an edit-distance-based pattern gen-eralization algorithm, and a pattern accu-racy calculation procedure based on eval-uating the patterns on several test corpora.In an evaluation with 14 entities, the sys-tem attains a precision higher than 50% forhalf of the relationships considered.1 IntroductionRecently, there is an increasing interest in auto-matically extracting structured information fromlarge corpora and, in particular, from the Web(Craven et al, 1999).
Because of the difficulty ofcollecting annotated data, several procedures havebeen described that can be trained on unannotatedtextual corpora (Riloff and Schmelzenbach, 1998;Soderland, 1999; Mann and Yarowsky, 2005).An interesting approach is that of rote extrac-tors (Brin, 1998; Agichtein and Gravano, 2000;Ravichandran and Hovy, 2002), which look fortextual contexts that happen to convey a certain re-lationship between two concepts.In this paper, we describe some contributionsto the training of Rote extractors, including a pro-cedure for generalizing the patterns, and a morecomplex way of calculating their accuracy.
Wefirst introduce the general structure of a rote ex-tractor and its limitations.
Next, we describe theproposed modifications (Sections 2, 3 and 4) andthe evaluation performed (Section 5).1.1 Rote extractorsAccording to the traditional definition of rote ex-tractors (Mann and Yarowsky, 2005), they esti-mate the probability of a relationship r(p, q) giventhe surrounding contextA1pA2qA3.
This is calcu-lated, with a training corpus T , as the number oftimes that two related elements r(x, y) from T ap-pear with that same context A1xA2yA3, dividedby the total number of times that x appears in thatcontext together with any other word:P (r(p, q)|A1pA2qA3) =Px,yr c(A1xA2yA3)Px,z c(A1xA2zA3)(1)x is called the hook, and y the target.
In orderto train a Rote extractor from the web, this proce-dure is usually followed (Ravichandran and Hovy,2002):1.
Select a pair of related elements to be usedas seed.
For instance, (Dickens,1812) for therelationship birth year.2.
Submit the query Dickens AND 1812 to asearch engine, and download a number ofdocuments to build the training corpus.3.
Keep all the sentences containing both ele-ments.4.
Extract the set of contexts between them andidentify repeated patterns.
This may just bethe m characters to the left or to the right,(Brin, 1998), the longest common substringof several contexts (Agichtein and Gravano,2000), or all substrings obtained with a suf-fix tree constructor (Ravichandran and Hovy,2002).5.
Download a separate corpus, called hook cor-pus, containing just the hook (in the example,Dickens).6.
Apply the previous patterns to the hook cor-pus, calculate the precision of each pattern9in the following way: the number of times itidentifies a target related to the hook dividedby the total number of times the pattern ap-pears.7.
Repeat the procedure for other examples ofthe same relationship.To illustrate this process, let us suppose that wewant to learn patterns to identify birth years.
Wemay start with the pair (Dickens, 1812).
From thedownloaded corpus, we extract sentences such asDickens was born in 1812Dickens (1812 - 1870) was an English writerDickens (1812 - 1870) wrote Oliver TwistThe system identifies that the contexts of the lasttwo sentences are very similar and chooses theirlongest common substring to produce the follow-ing patterns:<hook> was born in <target><hook> ( <target> - 1870 )In order to measure the precision of the ex-tracted patterns, a new corpus is downloaded us-ing the hook Dickens as the only query word, andthe system looks for appearances of the patternsin the corpus.
For every occurrence in which thehook of the relationship is Dickens, if the targetis 1812 it will be deemed correct, and otherwiseit will be deemed incorrect (e.g.
in Dickens wasborn in Portsmouth).1.2 Limitations and new proposalWe have identified the following limitations in thisalgorithm: firstly, to our knowledge, no Rote ex-tractor allows for the insertion of wildcards (e.g.
*) in the extracted patterns.
Ravichandran andHovy (2002) have noted that this might be dan-gerous if the wildcard matches unrestrictedly in-correct sentences.
However, we believe that theprecision estimation that is performed at the laststep of the algorithm, using the hook corpus, maybe used to rule out the dangerous wildcards whilekeeping the useful ones.Secondly, we believe that the procedure for cal-culating the precision of the patterns may be some-what unreliable in a few cases.
For instance,Ravichandran and Hovy (2002) report the follow-ing patterns for the relationships Inventor, Discov-erer and Location:Relation Prec.
PatternInventor 1.0 <target> ?s <hook> andInventor 1.0 that <target> ?s <hook>Discoverer 0.91 of <target> ?s <hook>Location 1.0 <target> ?s <hook>In this case, it can be seen that the same pattern(the genitive construction) may be used to indi-cate several different relationships, apart from themost common use indicating possession.
How-ever, they all receive very high precision values.The reason is that the patterns are only evaluatedfor the same hook for which they were extracted.Let us suppose that we obtain the pattern for Loca-tion using the pairs (New York, Chrysler Building).The genitive construction can be extracted fromthe context New York?s Chrysler Building.
After-ward, when evaluating it, only sentences contain-ing <target>?s Chrysler Building are taken intoaccount, which makes it unlikely that the patternis expressing a relationship other than Location,so the pattern will receive a high precision value.For our purposes, however, we need to collectpatterns for several relations such as writer-book,painter-picture, director-film, actor-film, and wewant to make sure that the obtained patterns areonly applicable to the desired relationship.
Pat-terns like <target> ?s <hook> are very likely tobe applicable to all of these relationships at thesame time, so we would like to be able to discardthem automatically.Hence, we propose the following improvementsfor a Rote extractor:?
A new pattern generalization procedure thatallows the inclusion of wildcards in the pat-terns.?
The combination with Named Entity recogni-tion, so people, locations, organizations anddates are replaced by their entity type in thepatterns, in order to increase their degree ofgenerality.
This is in line with Mann andYarowsky (2003)?s modification, consistingin replacing all numbers in the patterns withthe symbol ####.?
A new precision calculation procedure, in away that the patterns obtained for a given re-lationship are evaluated on the corpus for dif-ferent relationships, in order to improve thedetection of over-general patterns.2 Proposed pattern generalizationprocedureTo begin with, for every appearance of a pair ofconcepts, we extract a context around them.
Next,those contexts are generalized to obtain the partsthat are shared by several of them.
The procedureis detailed in the following subsections.10Birth year:BOS/BOS <hook> (/( <target> -/- number/entity )/) EOS/EOSBOS/BOS <hook> (/( <target> -/- number/entity )/) British/JJ writer/NNBOS/BOS <hook> was/VBD born/VBN on/IN the/DT first/JJ of/IN time expr/entity ,/, <target> ,/, at/IN location/entity ,/, of/INBOS/BOS <hook> (/( <target> -/- )/) a/DT web/NN guide/NNBirth place:BOS/BOS <hook> was/VBD born/VBN in/IN <target> ,/, in/IN central/JJ location/entity ,/,BOS/BOS <hook> was/VBD born/VBN in/IN <target> date/entity and/CC moved/VBD to/TO location/entityBOS/BOS Artist/NN :/, <hook> -/- <target> ,/, location/entity (/( number/entity -/-BOS/BOS <hook> ,/, born/VBN in/IN <target> on/IN date/entity ,/, worked/VBN as/INAuthor-book:BOS/BOS <hook> author/NN of/IN <target> EOS/EOSBOS/BOS Odysseus/NNP :/, Based/VBN on/IN <target> ,/, <hook> ?s/POS epic/NN from/IN Greek/JJ mythology/NNBOS/BOS Background/NN on/IN <target> by/IN <hook> EOS/EOSdid/VBD the/DT circumstances/NNS in/IN which/WDT <hook> wrote/VBD "/??
<target> "/??
in/IN number/entity ,/, and/CCCapital-country:BOS/BOS <hook> is/VBZ the/DT capital/NN of/IN <target> location/entity ,/, location/entity correct/JJ time/NNBOS/BOS The/DT harbor/NN in/IN <hook> ,/, the/DT capital/NN of/IN <target> ,/, is/VBZ number/entity of/IN location/entityBOS/BOS <hook> ,/, <target> EOS/EOSBOS/BOS <hook> ,/, <target> -/- organization/entity EOS/EOSFigure 1: Example patterns extracted from the training corpus for each several kinds of relationships.2.1 Context extraction procedureAfter selecting the sentences for each pair of re-lated words in the training set, these are pro-cessed with a part-of-speech tagger and a modulefor Named Entity Recognition and Classification(NERC) that annotates people, organizations, lo-cations, dates, relative temporal expressions andnumbers.
Afterward, a context around the twowords in the pair is extracted, including (a) at mostfive words to the left of the first word; (b) all thewords in between the pair words; (c) at most fivewords to the right of the second word.
The contextnever jumps over sentence boundaries, which aremarked with the symbols BOS (Beginning of sen-tence) and EOS (End of sentence).
The two relatedconcepts are marked as <hook> and <target>.Figure 1 shows several example contexts extractedfor the relationships birth year, birth place, writer-book and capital-country.Furthermore, for each of the entities in the re-lationship, the system also stores in a separate filethe way in which they are annotated in the trainingcorpus: the sequences of part-of-speech tags of ev-ery appearance, and the entity type (if marked assuch).
So, for instance, typical PoS sequences fornames of authors are ?NNP?1 (surname) and ?NNPNNP?
(first name and surname).
A typical entitykind for an author is person.2.2 Generalization pseudocodeIn order to identify the portions in common be-tween the patterns, and to generalize them, we ap-ply the following pseudocode (Ruiz-Casado et al,in press):1All the PoS examples in this paper are done with PennTreebank labels (Marcus et al, 1993).1.
Store all the patterns in a set P .2.
Initialize a setR as an empty set.3.
While P is not empty,(a) For each possible pair of patterns, cal-culate the distance between them (de-scribed in Section 2.3).
(b) Take the two patterns with the smallestdistance, pi and pj .
(c) Remove them from P , and add them toR.
(d) Obtain the generalization of both, pg(Section 2.4).
(e) If pg does not have a wildcard adjacentto the hook or the target, add it to P .4.
ReturnRAt the end, R contains all the initial patternsand those obtained while generalizing the previousones.
The motivation for step (e) is that, if a pat-tern contains a wildcard adjacent to either the hookor the target, it will be impossible to know whereit starts or ends.
For instance, when applying thepattern <hook> wrote * <target> to a text, thewildcard prevents the system from guessing wherethe title of the book starts.2.3 Edit distance calculationSo as to calculate the similarity between two pat-terns, a slightly modified version of the dynamicprogramming algorithm for edit-distance calcula-tion (Wagner and Fischer, 1974) is used.
The dis-tance between two patterns A and B is defined asthe minimum number of changes (insertion, addi-tion or replacement) that have to be done to thefirst one in order to obtain the second one.The calculation is carried on by filling a ma-trix M, as shown in Figure 2 (left).
At the same11A: wrote the well known novelB: wrote the classic novelM 0 1 2 3 4 D 0 1 2 3 40 0 1 2 3 4 0 I I I I1 1 0 1 2 3 1 R E I I I2 2 1 0 1 2 2 R R E I I3 3 2 1 1 2 3 R R R U I4 4 3 2 2 2 4 R R R R U5 5 4 3 3 2 5 R R R R EFigure 2: Example of the edit distance algorithm.
A and B are two word patterns;M is the matrix in which the edit distanceis calculated, and D is the matrix indicating the choice that produced the minimal distance for each cell inM.time that we calculate the edit distance matrix, itis possible to fill in another matrix D, in which werecord which of the choices was selected at eachstep: insertion, deletion, replacement or no edi-tion.
This will be used later to obtain the gener-alized pattern.
We have used the following fourcharacters:?
I means that it is necessary to insert a tokenin the first pattern to obtain the second one.?
R means that it is necessary to remove a to-ken.?
E means that the corresponding tokens areequal, so no edition is required.?
U means that the corresponding tokens areunequal, so a replacement has to be done.Figure 2 shows an example for two patterns,A and B, containing respectively 5 and 4 to-kens.
M(5, 4) has the value 2, indicating the dis-tance between the two complete patterns.
For in-stance, the two editions would be replacing wellby classic and removing known.2.4 Obtaining the generalized patternAfter calculating the edit distance between twopatterns A and B, we can use matrix D to obtaina generalized pattern, which should maintain thecommon tokens shared by them.
The procedureused is the following:?
Every time there is an insertion or a deletion,the generalized pattern will contain a wild-card, indicating that there may be anything inbetween.?
Every time there is replacement, the general-ized pattern will contain a disjunction of bothtokens.?
Finally, in the positions where there is no editoperation, the token that is shared betweenthe two patterns is left unchanged.The patterns in the example will produced thegeneralized patternWrote the well known novelWrote the classic novel??????????
?Wrote the well|classic * novelThe generalization of these two patterns pro-duces one that can match a wide variety of sen-tences, so we should always take care in order notto over-generalize.2.5 Considering part-of-speech tags andNamed EntitiesIf we consider the result in the previous example,we can see that the disjunction has been made be-tween an adverb and an adjective, while the otheradjective has been deleted.
A more natural result,with the same number of editing operations as theprevious one, would have been to delete the adverbto obtain the following generalization:Wrote the well known novelWrote the classic novel??????????
?Wrote the * known|classic novelThis is done taking into account part-of-speechtags in the generalization process.
In this way, theedit distance has been modified so that a replace-ment operation can only be done between words ofthe same part-of-speech.2 Furthermore, replace-ments are given an edit distance of 0.
This favorsthe choice of replacements with respect to dele-tions and insertions.
To illustrate this point, thedistance between known|classic/JJ and old/JJ2Note that, although our tagger produces the very detailedPennTreebank labels, we consider that all nouns (NN, NNS,NNP and NNPS) belong to the same part-of-speech class, andthe same for adjectives, verbs and adverbs.12Hook Birth Death Birth place Author of Director of Capital ofCharles Dickens 1812 1870 Portsmouth{Oliver Twist,The Pickwick Papers,Nicholas Nickleby,David Copperfield...}None NoneWoody Allen 1935 None Brooklin None{Bananas,Annie Hall,Manhattan, ... }NoneLuanda None None None None None AngolaTable 1: Example rows in the input table for the system.will be set to 0, because both tokens are adjectives.In other words, the d function is redefined as:d(A[i], B[j]) =(0 if PoS(A[i]) = PoS(B[j])1 otherwise(2)Note that all the entities identified by the NERCmodule will appear with a PoS tag of entity,so it is possible to have a disjunction such aslocation|organization/entity in a general-ized pattern (See Figure 1).3 Proposed pattern scoring procedureAs indicated above, if we measure the precision ofthe patterns using a hook corpus-based approach,the score may be inadvertently increased becausethey are only evaluated using the same terms withwhich they were extracted.
The approach pro-posed herein is to take advantage of the fact thatwe are obtaining patterns for several relationships.Thus, the hook corpora for some of the patternscan be used also to identify errors done by otherpatterns.The input of the system now is not just a listof related pairs, but a table including several rela-tionships for the same entities.
We may considerit as mini-biographies as in Mann and Yarowsky(2005)?s system.
Table 1 shows a few rows in theinput table for the system.
The cells for whichno data is provided have a default value of None,which means that anything extracted for that cellwill be considered as incorrect.Although this table can be written by hand, inour experiments we have chosen to build it auto-matically from the lists of related pairs.
The sys-tem receives the seed pairs for the relationships,and mixes the information from all of them in asingle table.
In this way, if Dickens appears inthe seed list for the birth year, death year, birthplace and writer-book relationships, four of thecells in its row will be filled in with values, andall the rest will be set to None.
This is probably avery strict evaluation, because, for all the cells forwhich there was no value in any of the lists, any re-sult obtained will be judged as incorrect.
However,the advantage is that we can study the behavior ofthe system working with incomplete data.The new procedure for calculating the patterns?precisions is as follows:1.
For every relationship, and for every hook,collect a hook corpus from the Internet.2.
Apply the patterns to all the hook corporacollected.
Whenever a pattern extracts a re-lationship from a sentence,?
If the table does not contain a row forthe hook, ignore the result.?
If the extracted target appears in the cor-responding cell in the table, consider itcorrect.?
If that cell contained different values, orNone, consider it incorrect.For instance, the pattern <target> ?s <hook>extracted for director-film may find, in the Dick-ens corpus, book titles.
Because these titles do notappear in the table as films directed by Dickens,the pattern will be considered to have a low accu-racy.In this step, every pattern that did not apply atleast three times in the test corpora was discarded.4 Pattern applicationFinally, given a set of patterns for a particularrelation, the procedure for obtaining new pairs isstraightforward:1.
For any of the patterns,2.
For each sentence in the test corpus,(a) Look for the left-hand-side context inthe sentence.
(b) Look for the middle context.
(c) Look for the right-hand-side context.
(d) Take the words in between, and checkthat either the sequence of part-of-speech tags or the entity type had been13Applied Prec.
Pattern3 1.0 BOS/BOS On/IN time expr/entity TARGET HOOK was/VBD baptized|born/VBN EOS/EOS15 1.0 "/??
HOOK (/( TARGET -/-4 1.0 ,/, TARGET ,/, */* Eugne|philosopher|playwright|poet/NNP HOOK earned|was/VBD */* at|in/IN23 1.0 -|--/- HOOK (/( TARGET -/-12 1.0 AND|and|or/CC HOOK (/( TARGET -/-48 1.0 By|about|after|by|for|in|of|with/IN HOOK TARGET -/-4 1.0 On|of|on/IN TARGET ,/, HOOK emigrated|faced|graduated|grew|has|perjured|settled|was/VBD12 1.0 BOS/BOS HOOK TARGET -|--/-49 1.0 ABOUT|ALFRED|Amy|Audre|Authors|BY| (...) |teacher|writer/NNPS HOOK (/( TARGET -|--/-7 1.0 BOS/BOS HOOK (/( born/VBN TARGET )/)3 1.0 BOS/BOS HOOK ,/, */* ,/, TARGET ,/,13 1.0 BOS/BOS HOOK ,|:/, TARGET -/-132 0.98 BOS/BOS HOOK (/( TARGET -|--/-18 0.94 By|Of|about|as|between|by|for|from|of|on|with/IN HOOK (/( TARGET -/-33 0.91 BOS/BOS HOOK ,|:/, */* (/( TARGET -|--/-10 0.9 BOS/BOS HOOK ,|:/, */* ,|:/, TARGET -/-3 0.67 ,|:|;/, TARGET ,|:/, */* Birth|son/NN of/IN */* General|playwright/NNP HOOK ,|;/,210 0.63 ,|:|;/, HOOK (/( TARGET -|--/-7 0.29 (/( HOOK TARGET )/)Table 3: Patterns for the relationship birth year..Relation Seeds Extr.
Gener.
Filt.Actor-film 133 480 519 10Writer-book 836 3858 4847 171Birth-year 492 2520 3220 19Birth-place 68 681 762 5Country-capital 36 932 1075 161Country-president 56 1260 1463 119Death-year 492 2540 3219 16Director-film 1530 3126 3668 121Painter-picture 44 487 542 69Player-team 110 2903 3514 195Table 2: Number of seed pairs for each relation,and number of unique patterns after the extractionand the generalization step, and after calculatingtheir accuracy and filtering those that did not apply3 times on the test corpus.seen in the training corpus for that role(hook or target).
If so, output the rela-tionship.5 Evaluation and resultsThe procedure has been tested with 10 differentrelationships.
For each pair in each seed list, acorpus with 500 documents has been collected us-ing Google, from which the patterns are extracted.Table 2 shows the number of patterns obtained.
Itis interesting to see that for some relations, such asbirth-year or birth-place, more than one thousandpatterns have been reduced to a few.
Table 3 showsthe patterns obtained for the relationship birth-year.
It can also be seen that some of the patternswith good precision contain the wildcard *, whichhelped extract the correct birth year in roughly 50occasions.
Specially of interest is the last pattern,(/( HOOK TARGET )/)which resulted in an accuracy of 0.29 with the pro-Relation Precision Incl.
prec.
AppliedActor-film 0% 76.84% 95Writer-book 6.25% 28.13% 32Birth-year 79.67% 79.67% 477Birth-place 14.56% 14.56% 103Country-capital 72.43% 72.43% 599Country-president 81.40% 81.40% 43Death-year 96.71% 96.71% 152Director-film 43.40% 84.91% 53Painter-picture - - 0Player-team 52.50% 52.50% 120Table 4: Precision, inclusion precision and num-ber of times that a pattern extracted information,when applied to a test corpus.cedure here indicated, but which would have ob-tained an accuracy of 0.54 using the traditionalhook corpus approach.
This is because in othertest corpora (e.g.
in the one containing soccerplayers and clubs) it is more frequent to find thename of a person followed by a number that is nothis/her birth year, while that did not happen so of-ten in the birth year test corpus.For evaluating the patterns, a new test corpushas been collected for fourteen entities not presentin the training corpora, again using Google.
Thechosen entities are Robert de Niro and NatalieWood (actors), Isaac Asimov and Alfred Bester(writers), Montevideo and Yaounde (capitals),Gloria Macapagal Arroyo and Hosni Mubarak(country presidents), Bernardo Bertolucci andFederico Fellini (directors), Peter Paul Rubens andPaul Gauguin (painters), and Jens Lehmann andThierry Henry (soccer players).
Table 4 shows theresults obtained for each relationship.We have observed that, for those relationshipsin which the target does not belong to a Named14Entity type, it is common for the patterns to extractadditional words together with the right target.
Forexample, rather than extracting The Last Emperor,the patterns may extract this title together withits rating or its length, the title between quotes,or phrases such as The classic The Last Emperor.In the second column in the table, we measuredthe percentage of times that a correct answer ap-pears inside the extracted target, so these exampleswould be considered correct.
We call this metricinclusion precision.5.1 Comparison to related approachesAlthough the above results are not comparable toMann and Yarowsky (2005), as the corpora usedare different, in most cases the precision is equalor higher to that reported there.
On the other hand,we have rerun Ravichandran and Hovy (2002)?salgorithm on our corpus.
In order to assure afair comparison, their algorithm has been slightlymodified so it also takes into account the part-of-speech sequences and entity types while extract-ing the hooks and the targets during the rule ap-plication.
So, for instance, the relationship birthdate is only extracted between a hook tagged asa person and a target tagged as either a date ora number.
The results are shown in Table 5.
Ascan be seen, our procedure seems to perform bet-ter for all of the relations except birth place.
Itis interesting to note that, as could be expected,for those targets for which there is no entity typedefined (films, books and pictures), Ravichandranand Hovy (2002)?s extracts many errors, becauseit is not possible to apply the Named Entity Rec-ognizer to clean up the results, and the accuracyremains below 10%.
On the other hand, that trenddoes not seem to affect our system, which hadvery poor results for painter-picture, but reason-ably good for actor-film.Other interesting case is that of birth places.A manual observation of our generalized patternsshows that they often contain disjunctions of verbssuch as that in (1), that detects not just the birthplace but also places where the person lived.
Inthis case, Ravichandran and Hovy (2002)?s pat-terns resulted more precise as they do not containdisjunctions or wildcards.
(1) HOOK ,/, returned|travelled|born/VBNto|in/IN TARGETIt is interesting that, among the three relation-ships with the smaller number of extracted pat-terns, one of them did not produce any result, andRavichandranRelation Our approach and Hovy?sActor-film 76.84% 1.71%Writer-book 28.13% 8.55%Birth-year 79.67% 49.49%Birth-place 14.56% 88.66%Country-capital 72.43% 24.79%Country-president 81.40% 16.13%Death-year 96.71% 35.35%Director-film 84.91% 1.01%Painter-picture - 0.85%Player-team 52.50% 44.44%Table 5: Inclusion precision on the same test cor-pus for our approach and Ravichandran and Hovy(2002)?s.the two others attained a low precision.
Therefore,it should be possible to improve the performanceof the system if, while training, we augment thetraining corpora until the number of extracted pat-terns exceeds a given threshold.6 Related workExtracting information using Machine Learningalgorithms has received much attention since thenineties, mainly motivated by the Message Un-derstanding Conferences (MUC6, 1995; MUC7,1998).
From the mid-nineties, there are systemsthat learn extraction patterns from partially an-notated and unannotated data (Huffman, 1995;Riloff, 1996; Riloff and Schmelzenbach, 1998;Soderland, 1999).Generalizing textual patterns (both manuallyand automatically) for the identification of re-lationships has been proposed since the earlynineties (Hearst, 1992), and it has been appliedto extending ontologies with hyperonymy andholonymy relationships (Kietz et al, 2000; Cimi-ano et al, 2004; Berland and Charniak, 1999),with overall precision varying between 0.39 and0.68.
Finkelstein-Landau and Morin (1999) learnpatterns for company merging relationships withexceedingly good accuracies (between 0.72 and0.93).Rote extraction systems from the web havethe advantage that the training corpora can becollected easily and automatically.
Severalsimilar approaches have been proposed (Brin,1998; Agichtein and Gravano, 2000; Ravichan-dran and Hovy, 2002), with various applications:Question-Answering (Ravichandran and Hovy,2002), multi-document Named Entity Corefer-ence (Mann and Yarowsky, 2003), and generating15biographical information (Mann and Yarowsky,2005).7 Conclusions and future workWe have described here a new procedure for build-ing a rote extractor from the web.
Compared toother similar approaches, it addresses several is-sues: (a) it is able to generate generalized patternscontaining wildcards; (b) it makes use of PoS andNamed Entity tags during the generalization pro-cess; and (c) several relationships are learned andevaluated at the same time, in order to test eachone on the test corpora built for the others.
The re-sults, measured in terms of precision and inclusionprecision are very good in most of the cases.Our system needs an input table, which mayseem more complicated to compile that the list ofrelated pairs used by previous approaches, but wehave seen that the table can be built automaticallyfrom the lists, with no extra work.
In any case,the time to build the table is significantly smallerthan the time needed to write the extraction pat-terns manually.Concerning future work, we are currently tryingto improve the estimation of the patterns accuracyfor the pruning step.
We also plan to apply the ob-tained patterns in a system for automatically gen-erating biographical knowledge bases from vari-ous web corpora.ReferencesE.
Agichtein and L. Gravano.
2000.
Snowball: Ex-tracting relations from large plain-text collections.In Proceedings of ICDL, pages 85?94.M.
Berland and E. Charniak.
1999.
Finding parts invery large corpora.
In Proceedings of ACL-99.S.
Brin.
1998.
Extracting patterns and relations fromthe World Wide Web.
In Proceedings of the WebDBWorkshop at the 6th International Conference on Ex-tending Database Technology, EDBT?98.P.
Cimiano, S. Handschuh, and S. Staab.
2004.
To-wards the self-annotating web.
In Proceedings of the13th World Wide Web Conference, pages 462?471.M.
Craven, D. DiPasquo, D. Freitag, A. McCallum,T.
Mitchell, K. Nigam, and S. Slattery.
1999.
Learn-ing to construct knowledge bases from the worldwide web.
Artificial Intelligence, 118(1?2):69?113.M.
Finkelstein-Landau and E. Morin.
1999.
Extractingsemantic relationships between terms: supervisedvs.
unsupervised methods.
In Workshop on Ontolo-gial Engineering on the Global Info.
Infrastructure.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In COLING-92.S.
Huffman.
1995.
Learning information extractionpatterns from examples.
In IJCAI-95 Workshop onNew Approaches to Learning for NLP.J.
Kietz, A. Maedche, and R. Volz.
2000.
A methodfor semi-automatic ontology acquisition from a cor-porate intranet.
In Workshop ?Ontologies and text?.G.
S. Mann and D. Yarowsky.
2003.
Unsupervisedpersonal name disambiguation.
In CoNLL-2003.G.
S. Mann and D. Yarowsky.
2005.
Multi-field in-formation extraction and cross-document fusion.
InACL 2005.M.
Marcus, B. Santorini, and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: the Penn Treebank.
Computational Linguis-tics, 19(2):313?330.MUC6.
1995.
Proceedings of the 6th Message Under-standing Conference (MUC-6).
Morgan Kaufman.MUC7.
1998.
Proceedings of the 7th Message Under-standing Conference (MUC-7).
Morgan Kaufman.D.
Ravichandran and E. Hovy.
2002.
Learning surfacetext patterns for a question answering system.
InProceedings of ACL-2002, pages 41?47.E.
Riloff and M. Schmelzenbach.
1998.
An empiricalapproach to conceptual case frame acquisition.
InProceedings of WVLC, pages 49?56.E.
Riloff.
1996.
Automatically generating extractionpatterns from untagged text.
In AAAI.M.
Ruiz-Casado, E. Alfonseca, and P. Castells.
inpress.
Automatising the learning of lexical pat-terns: an application to the enrichment of wordnetby extracting semantic relationships from wikipedia.Data and Knowledge Engineering.S.
Soderland.
1999.
Learning information extractionrules for semi-structured and free text.
MachineLearning, 34(1?3):233?272.R.
Wagner and M. Fischer.
1974.
The string-to-string correction problem.
Journal of Associationfor Computing Machinery, 21.16
