Learning Greek Verb Complements: Addressing the Class ImbalanceKatia Kermanidis, Manolis Maragoudakis, Nikos Fakotakis, George KokkinakisWire Communications LaboratoryUniversity of Patras, Rio, 26500, Greece{kerman,mmarag,fakotaki,gkokkin}@wcl.ee.upatras.grAbstractImbalanced training sets, where one class isheavily underrepresented compared to the oth-ers, have a bad effect on the classification ofrare class instances.
We apply One-sided Sam-pling for the first time to a lexical acquisitiontask (learning verb complements from ModernGreek corpora) to remove redundant and mis-leading training examples of verb non-dependents and thereby balance our trainingset.
We experiment with well-known learningalgorithms to classify new examples.
Perform-ance improves up to 22% in recall and 15% inprecision after balancing the dataset1.1 IntroductionAmong the dependents of a verb, arguments arekey participants in the event described by the verb,while adjuncts comprise secondary informationconcerning the ?setting?
of the event (its context,location etc.
).In previous work in automatic complement-adjunct distinction, Buchholz (1998) uses memory-based learning on the part-of-speech tagged andphrase structured part of the Wall Street Journalwith a generalization accuracy of 91.6% and sheincludes verb subcategorization information in herdata.
Merlo and Leybold (2001) use decision treesto distinguish prepositional arguments from prepo-sitional modifiers.
They incorporate semantic verbclass, preposition and noun cluster information andreach an accuracy of 86.5% with a training set of3692 and a test set of 400 instances.
Aldezabal etal.
(2002) work on Basque.
They apply mutual in-formation and Fisher?s Exact Test to verb-casepairs (a case is any type of argument) which wereobtained from a partially parsed newspaper corpusof 1.3 million words.
Evaluation was performed byhuman tagging of the dependents of ten test verbsinside (55% f-measure) and outside (95% f-measure) the context of the sentence.
Many re-searchers have attempted to distinguish comple-ments from adjuncts as a prerequisite foridentifying verb subcategorization frames: Sarkar1This work was supported by the EU Project INSPIRE (IST-2001-32746).and Zeman (2000) use a treebank and iterativelyreduce the size of the candidate frame to filter outadjuncts.
Briscoe and Carroll (1997) and Korhonenet al (2000) use a grammar and a sophisticatedparsing tool for argument-adjunct distinction.In this paper we address the issue of comple-ment-adjunct distinction in Modern Greek (MG)texts using well-known machine learning tech-niques (instance based learning, Na?ve Bayes, anddecision trees) and minimal resources.
We makeuse of input that is automatically annotated only upto the phrase level, where the verb dependents arenot identified.
Therefore, a significant dispropor-tion between the number of complements and non-complements (adjuncts and non-dependents) arisesamong the candidates (complements being signifi-cantly fewer).
This disproportion causes a signifi-cant drop in the minority (or positive) class (i.e.complements) prediction accuracy.
Henceforth byadjuncts we will mean non-complements.
Theproblem of class imbalance has been dealt with inprevious work in different ways: oversampling ofthe minority class until it consists of as many ex-amples as the majority (or negative) class (Japko-wicz 2000), undersampling of the majority class(either random or focused), their combination(Ling and Li 1998), the implementation of cost-sensitive classifiers (Domingos 1999), and theROC convex hull method (Provost and Fawcett2001).In general, undersampling the majority classleads to better classifier performances thanoversampling the minority class (Chawla et al2002).
Therefore, we apply One-sided Samplingand Tomek links (Tomek 1976) to our training datato obtain a more balanced subset of the initialtraining set by pruning out noisy and redundantinstances of the majority class.
This approach hasbeen used in the past in several domains such asimage processing (Kubat and Matwin 1997),medicine (Laurikkala 2001), text categorization(Lewis and Gale 1994), and we apply it here forthe first time to lexical acquisition.A novel variation in detecting Tomek links inthis work is the metric used for calculating the dis-tance between instance vectors.
Features in ourtask take exclusively nominal values.
We thereforeexperiment with the value difference metric (Stan-fill and Waltz 1986) besides the broadly usedEuclidean distance.
The former is more suitable forthis type of features, a claim supported by Stanfilland Waltz and also by our experimental results.2 Modern GreekConcerning morphology, MG is highly inflec-tional.
The part-of-speech (pos), the grammaticalcase, and the verb voice are key morphologicalfeatures for complement detection.Concerning sentence structure, MG is a ?semi-free?
word-order language.
The arguments of averb do not have fixed positions with respect to theverb and are therefore determined primarily bytheir morphology rather than their position.Certain semantic verb attributes are also verysignificant: the verb?s copularity, its mode, andwhether it is (im)personal.
A verb is copular whenit assigns a quality to its subject.
Mode is the prop-erty that determines the semantic relation betweenthe verb and its subject (whether the latter affectsor is affected by the verb action.
Although all ofthese features are normally context-dependent,there are verbs with apriori known values for them.This apriori information is taken into account inour final dataset, as context-dependent semanticinformation could not be provided automatically,and we tried to keep manual intervention to aminimum.In MG, verbs can take zero, one or two com-plements.
A complement may be a noun phrase inthe accusative or the genitive case, a prepositionalphrase or a secondary clause (Klairis and Babini-otis 1999).
Often the complements appear withinthe verb phrase itself in the form of weak personalpronouns.
Copular verbs only can take as an argu-ment a noun or adjective in the nominative (predi-cative).
Each of the above features is important butnot definitive on its own for complement detection.When combined, however, and including contextinformation of the candidate complement, manycases of ambiguity are correctly resolved.
The big-gest sources of ambiguity are the accusative nounphrase, which is very often adverbial denoting usu-ally time, and the prepositional phrase introducedby ??
(to), also often adverbial, denoting usuallyplace.3 Data CollectionThe corpora used in our experiments were:1.
The ILSP/ELEFTHEROTYPIA (Hatzigeor-giu et al 2000) and ESPRIT 860 (Partners ofESPRIT-291/860 1986) Corpora (a total of300,000 words).
Both these corpora are balancedand manually annotated with complete morpho-logical information.
The former also provides ad-verb type information (temporal, of manner etc.
).Further (phrase structure) information is obtainedautomatically.2.
The DELOS Corpus (Kermanidis et al 2002)is a collection of economic domain texts of ap-proximately five million words and of varyinggenre.
It has been automatically annotated from theground up.
Morphological tagging on DELOS wasperformed by the analyzer of Sgarbas et al (2000).Accuracy in pos tagging reaches 98%.
Case andvoice tagging reach 94% and 84% accuracy re-spectively.
Further (phrase structure) informationis again obtained automatically.
DELOS also con-tains subject-verb-object information limited tonominal and prepositional objects and detectedautomatically by a shallow parser that reaches 70%precision and recall.All the corpora have been phrase-analyzed bythe chunker described in detail in Stamatatos et al(2000).
Noun (NP), verb (VP), prepositional (PP),adverbial phrases (ADP) and conjunctions (CON)are detected via multi-pass parsing.
Precision andrecall reach 94.5% and 89.5% respectively.Phrases are non-overlapping.
Concerning phrasestructure, complements (except for weak personalpronouns) are not included in the verb phrase,nominal modifiers in the genitive case are includedwithin the noun phrase they modify, coordinatedsimple noun and adverbial phrases are groupedinto one phrase.The next step is empirical headword identifica-tion.
NP headwords are determined based on thepos and case of the phrase constituents.
For VPs,the headword is the main verb or the conjunction ifthey are introduced by one.
For PPs it is the prepo-sition introducing them.3.1  Data FormationTo take into account the freedom of the languagestructure, context information of every verb in thecorpus focuses on the two phrases preceding andthe three phrases following it.
Only one out of 200complements in the corpus appears outside thiswindow.
Each of these phrases is in turn the focusphrase (the candidate complement or adjunct) andan instance of twenty nine features (28 featuresplus the class label) is formed for every focusphrase (fp).
So a maximum of five instances perverb occurrence are formed.
Forming of these in-stances from a corpus sentence is shown in Figure1.The first five features are the verb lemma(VERB), its mode (F1), whether it is (im)personal(F2), its copularity (F3), and its voice (F4).
Twofeatures encode the presence of a personal pronounin the accusative (F5) or genitive (F6) within theVP.
For every fp (fps are in bold), apart from theseven features described above, a context windowof three phrases preceding the fp and three phrasesfollowing it is taken into account.
Each of these sixphrases (as well as the fp itself) is encoded into aset of three features (a total of twenty one fea-tures).
These triples appear next in each instance,from the leftmost (-3) to the rightmost phrase (+3).For each feature triple, the first feature is the typeof the phrase.
The second is the pos of the head-word for NPs and ADPs.
The third feature for NPsis the case of the headword.
For ADPs it is the typeof the adverb, if available.
If VPs are introduced bya conjunction, the second feature is its type (coor-dinating/subordinating) and the third is the con-junction itself.
Otherwise the second feature is theverb?s pos and the third empty.
For PPs, the secondfeature is empty and the third is the preposition.VP[*?????]
NP1[????
*?????]
NP2[?
*???????]
CON[???]
VP[*????????]
PP[*???
???.
](VP[Is] NP1[good boy] NP2[the Labros] CON[and] VP[believes] PP[in God.])
(Labros is a good boy and believes in God.
)VERB   F1 F2 F3 F4 F5 F6 FP   -3    -2    -1   +1      +2      +3    LABEL????
?,   O, P, C,  P, F, F, NP,N,n,  -,-,-,  -,-,-,  VP,V,-,  NP,N,n,  VP,V,-,  PP,-,?
?,  C????
?,   O, P, C,  P, F, F, NP,N,n,  -,-,-,  VP,V,-, NP,N,n,  VP,V,-,  PP,-,?
?,  -,-,-,   A??????
?, E, P, NC, A, F, F, NP,N,n,  -,-,-,  -,-,-,  VP,V,-,  NP,N,n,  VP,V,-,  PP,-,?
?,  A??????
?, E, P, NC, A, F, F, NP,N,n,  -,-,-,  VP,V,-, NP,N,n,  VP,V,-,  PP,-,?
?,  -,-,-,   A??????
?, E, P, NC, A, F, F, PP,-,?
?,  NP,N,n, NP,N,n, VP,V,-,  -,-,-,   -,-,-,   -,-,-,   CFigure 1: A sentence is transformed into the 5 labeled instances shown.
Words starting with the asterisk (*) are head-words.The first instance is for the verb ?????
and thecandidate complement/adjunct is the fp NP1.
In thesecond instance, for the same verb, the candidatecomplement/adjunct is the fp NP2.
There are onlytwo instances for this verb because 1. there are nophrases preceding it, and 2. the third phrase follow-ing it (consisting only of the coordinating conjunc-tion) has not much to contribute and is disregardedaltogether forcing us to consider the next phrase inthe sentence.
As the next phrase is a verb phrasethat is not introduced by a subordinating conjunc-tion (and therefore cannot be a dependent of theverb ?????
), it is also disregarded and no furtherphrases are tested.
In the same way, for the verb???????
we have an instance with fp the NP1, aninstance with fp the NP2 and one with PP as the fp.We experimented with various window sizes re-garding the context of the fp, i.e.
[fp], [-1, fp], [-2,fp], [-2, +1], [-3, +3].The formatting described in the previous sectionwas applied to the ILSP and ESPRIT corpora andto part (approximately 500,000 words) of theDELOS corpus.
For the first two corpora, the classof each fp for every created instance was hand-labeled by two linguists by looking up the verb inits context, based on the detailed descriptions forcomplements and adjuncts by Klairis and Babini-otis (1999).
For DELOS, which already containedautomatically detected verb-object information toan extent, existing erroneous complement informa-tion was manually corrected, while clausal com-plements were manually detected.
The datasetconsisted of 63,000 instances.
The imbalance ratiois 1:6.3 (one complement instance for every 6.3adjunct instances).4 Addressing the ImbalanceFrom the ratio given above, the complement classis underrepresented compared to the adjunct classin the data.
As the number of examples of the ma-jority class increases, the more likely it becomesfor the nearest neighbor of a complement to be anadjunct.
Therefore, complements are prone to mis-classifications.
We address this problem with One-sided Sampling, i.e.
pruning out redundant adjunct(negative) examples while keeping all the com-plement (positive) examples.
Instances of the ma-jority class can be categorized into four groups(Figure 2): Noisy are instances that appear within acluster of examples of the opposite class, border-line are instances close to the boundary region be-tween two classes, redundant are instances that canbe already described by other examples of thesame class and safe are instances crucial for deter-mining the class.
Instances belonging to one of thethree first groups need to be eliminated as they donot contribute to class prediction.Noisy and borderline examples can be detectedusing Tomek links: Two examples, x and y, of op-posite classes have a distance of ?(x,y).
This pair ofinstances constitutes a Tomek link if no other ex-ample exists at a smaller distance to x or y than?
(x,y).Redundant instances may be removed by creat-ing a consistent subset of the initial training set.
Asubset C of training set T is consistent with T, if,when using the nearest neighbor (1-NN) algorithm,it correctly classifies all the instances in T. To thisend we start with a subset C consisting of all com-plement examples and one adjunct example.
Wetrain a learner with C and try to classify the rest ofthe instances of the initial training set.
All misclas-sified instances are added to C, which is the finalreduced dataset.The exact process of the proposed algorithm is:1.
Let T be the original training set, where thesize of the negative examples outnumbers thatof the positive examples.2.
Construct a dataset C, containing all positiveinstances plus one randomly selected negativeinstance.3.
Classify T with 1-NN using the training ex-amples of C and move all misclassified items toC.
C is consistent with T, only smaller.4.
Remove all negative examples participatingin Tomek links.
The resulting set Topt is usedfor classification instead of T.4.1 Distance func-tions4.1 Distance functionsThe distance functions used to determine the in-stances participating in Tomek links are describedin this section.The most commonly used distance function isthe Euclidean distance.
One drawback of theEuclidean distance is that it is not very flexibleregarding nominal attributes.
The value differencemetric (VDM) is more appropriate for this type ofattributes, as it considers two nominal values to becloser if they have more similar classifications, i.e.more similar correlations with the output class.
TheVDM of two values ax and ay of a nominal attributeA in two vectors x and y is estimated as:, ,, ,, ,( , ) yxx yA a cA a cA x yc C A a A aNNvdm a aN N?= ?
?,A aN is the number of times value a of attribute Awas found in the training set,, ,A a cN is the numberof times value a co-occurred with output class cand C is the set of class labels.4.2 The reduced datasetWe used the above distance metrics to detect ex-amples that are safe to remove, and then appliedthe methodology of the previous section to ourdata.
Figure 3 depicts the reduction in the numberof negative instances for both metrics and every fpcontext window.
The more phrases are considered(the higher the vector dimension), the noisier theinstances, and the more redundant examples areremoved.
For small windows, the positive effect ofVDM is clear (more redundant examples are de-tected and removed).
As the window size in-creases, the Euclidean distance becomes smoother(depending on more features) and leads to the re-moval of as many examples as VDM.Figure 2: The four groups of negative instances.0,00%2,00%4,00%6,00%8,00%10,00%12,00%14,00%[0] [-1,0] [-2,0] [-2,1] [-3,3]EuclideanVDMFigure 3: Reduction (%) in the number of negative in-stances after applying One-sided Sampling.It is interesting to observe the type of instanceswhich are removed from the initial dataset afterbalancing.
Redundant instances are usually thosewith as fp headword a punctuation mark, a symboletc.
Such fps could never constitute a complementand appear in the dataset due to errors in the auto-matic nature of pre-processing.
Borderline in-stances are usually formed by fps that have asyntactically ambiguous headword like a noun inthe accusative case, an adjective in the nominativecase if the verb is copular, certain prepositionalphrases.
The following negative instance of theinitial dataset (with window [fp]) shows the differ-ence between the two distances.??????????
?,  E,  P,  NC,  A,  F,  F,  PP,-,?
?,  ?This instance appears only as negative through-out the whole dataset.
If the verb ???????????
(toreplace) were omitted, the remaining instance ap-pears several times in the data as positive with avariety of other verbs.
The Euclidean distance be-tween these instances is small, while the VDM isgreater, because the verb is a feature with a highcorrelation to the output class.
So the above in-stance is removed with the Euclidean distance asbeing borderline, while it remains untouched withVDM.5 Classifying new instancesFor classification we experimented with a set ofalgorithms that have been broadly used in severaldomains and their performance is well-known: in-stance-based learning (IB1), decision trees (an im-plementation of C4.5 with reduced error pruning)and Na?ve Bayes were used to classify new, unseeninstances as complements or adjuncts.
Unlike pre-vious approaches that test their methodology ononly a few new verb examples, we performed 10-fold cross validation on all our data: the dataset(whether initial or reduced) was divided into tensets of equal size, making sure that the proportionof the examples of the two classes remained thesame.
For guiding the C4.5 pruning process, one ofthe ten subsets was used as the held-out validationset.6 Experimental resultsUnlike previous approaches that evaluate theirmethodology using the accuracy metric, we evalu-ated classification using precision and recall met-rics for every class.
a and d are the correctlyidentified adjuncts and complements respectively,b are the adjuncts which have been misclassified ascomplements and c are the misclassified comple-ments.Aapr =a+c, Aare =a+b, Cdpr =b+d, Cdre =c+dThe f-measure for each class combines the pre-vious two metrics into one:2 precision recallf-measure=precision+recall?
?Table 1 shows the results for each classificationalgorithm and various window sizes using the ini-tial dataset before any attempt is made to reduce itssize.
The drop in performance of the minority classcompared to the majority class is obvious.
Thescores corresponding to the best f-measure for thecomplement class are indicated in bold.By explicitly storing and taking into accountevery training example, IB1 presents a drop in per-formance as the window size increases due tosparse data.
The performance of C4.5 remains rela-tively stable, regardless of the size of the instancevector.
Na?ve Bayes leads to a significant numberof adjunct instances being labeled as complements.This is attributed to the fact that the Na?ve Bayeslearner does not take into account conditional de-pendencies among features.
Given that an instanceis a complement, for example, if the fp is an adjec-tive in the nominative case, there is a very highprobability in reality that the verb is copular.
Thisdependence is not captured by the Na?ve Bayeslearner.
[0] [-1,0] [-2,0] [-2,1] [-3,3]PrA 91.3 92.5 92.4 92.6 92.9ReA 86.4 83.2 82.1 83.1 82.6PrC 45.5 43.4 41.8 43.4 43.1Na?veBayesReA 57.8 65.6 65.7 66.1 67.8PrA 91.5 91.4 91.3 91.3 91.5ReA 94.9 95.1 95.2 95.1 95.2PrC 68.0 68.5 68.7 68.2 68.9C4.5ReC 54.9 54.4 53.9 53.7 54.7PrA 91.7 92.2 91.6 90.0 87.7ReA 93.7 93.8 92.8 91.6 90.0PrC 63.8 65.4 60.6 52.8 40.5IB1ReC 56.9 59.8 56.5 47.9 35.1Table 1: Results for each algorithm and various fp con-text window sizes using the initial dataset.Tables 2 and 3 show the classification results af-ter balancing the dataset using the Euclidean dis-tance and VDM respectively.
The increase in f-measure after reducing the dataset is very interest-ing to observe and depends on the size of the fpcontext window.When taking into account the fp only, the high-est increase is over 8% in complement class f-measure with the Euclidean distance.When regarding the context surrounding the fp,the positive impact of balancing the dataset is evenstronger.
As the fp window size increases, Na?veBayes performs better, reaching an f-measure ofover 60% with [-3,+3] (as opposed to 53.4% priorto balancing).
Recall with C4.5 increases by 14%in context [-3,+3] after balancing.
Instance-basedlearning, as mentioned earlier is not helped by a lotof context information and reaches its highestscore when considering only one phrase precedingthe fp.
The increase in complement class precisionwith IB1 exceeds 12% with VDM.
This is the ex-periment which achieved the highest f-measure(73.7%).
Regarding larger context windows andIB1, the removal of the noisy and redundant exam-ples seems to compensate for the noise introducedby the increased number of features in the vector.Increase in recall reaches 22%.
As a general re-mark, instance-based learning performs best whenthe context surrounding the candidate complementis very restricted (at most one phrase preceding thefp), while Bayesian learning improves its perform-ance as the window increases.In most of the experiments VDM leads to betterresults than the Euclidean distance because it ismore appropriate for nominal features, especiallywhen the instance vector is small.
When largerwindows are considered, the two metrics have thesame effect.
Minor occasional differences (~0.1%)mirrored in the results are attributed to the 10-foldexperimentation.
[0] [-1,0] [-2,0] [-2,1] [-3,3]PrA 91.1 92.4 92.8 93.0 93.0ReA 87.4 83.2 82.6 84.6 85.1PrC 49.0 45.7 46.7 50.3 51.8Na?veBayesReA 58.4 67.4 70.5 70.9 71.3PrA 92.3 92.0 91.7 93.2 92.9ReA 95.1 95.2 95.6 94.6 94.9PrC 72.4 72.4 74.8 73.6 73.3C4.5ReC 61.7 60.4 60.1 68.5 68.8PrA 93.0 93.8 93.1 92.1 90.2ReA 94.7 95.5 94.6 93.0 90.5PrC 71.7 76.5 73.0 66.7 55.3IB1ReC 65.4 69.7 67.5 68.6 56.7Table 2: Results for the reduced dataset and the Euclid-ean distance.
[0] [-1,0] [-2,0] [-2,1] [-3,3]PrA 91.0 92.5 92.8 93.0 93.2ReA 87.3 83.1 82.6 84.6 85.4PrC 49.0 46.5 46.7 50.3 51.6Na?veBayesReA 58.6 68.6 70.5 70.9 71.3PrA 92.0 92.6 91.7 93.2 93.0ReA 95.0 95.2 95.6 94.6 94.8PrC 71.5 74.3 74.8 73.6 73.2C4.5ReC 60.1 64.6 60.1 68.5 68.9PrA 92.7 93.8 93.1 93.6 90.2ReA 94.4 95.6 94.6 93.0 90.5PrC 70.4 77.5 73.0 66.7 55.3IB1ReC 64.5 70.3 67.5 68.6 56.7Table 3: Results for the reduced set and VDM.Apart from the positive impact of One-sidedSampling on predicting positive examples, the ta-bles show its positive (or at least non-negative)impact on predicting negative instances.
Non-complement accuracy either increases or remainsthe same after balancing.Concerning the resolution of the ambiguitiesdiscussed in section 2, three classified examples ofthe verb ????
(to exercise) with context environ-ment [-1,fp] follow.
The first class label is the trueand the second is the predicted class.
Example (a)has been classified correctly with and without One-sided Sampling.
Examples (b) and (c) are the sameinstance classified without (b) and with (c) One-sided Sampling.
Example (b) is erroneously taggedas an adjunct due to class imbalance.
The phrasepreceding the fp helps resolve the ambiguity in (a)and (c): usually a punctuation mark before the fp(indicated by the triple NP,F,-)  separates syntacti-cally the fp from the verb and the fp is unlikely tobe a complement.a.
???
?, E, P, NC, A, F, F, PP,-,?
?, NP,F,-,  A Ab.
???
?, E, P, NC, A, F, F, PP,-,?
?, NP,N,a, C Ac.
???
?, E, P, NC, A, F, F, PP,-,?
?, NP,N,a, C C7 ConclusionIn this paper we describe the positive effect ofOne-sided Sampling of an imbalanced dataset forthe first time on the linguistic task of automaticallylearning verb complements from Greek text cor-pora.
Unlike traditional One-sided Sampling, weemploy the VDM metric and show that it is moreappropriate for nominal features.
We experimentwith various learning algorithms to classify newexamples and reach a precision and a recall valueof 77.5% and 70.3% respectively, having used onlya chunker for preprocessing.ReferencesI.
Aldezabal, M. Aranzabe, A. Atutxa, K. Gojenola andK.
Sarasola.
2002.
Learning argument/adjunct dis-tinction for Basque.
SIGLEX Workshop of the ACL,pages 42-50.
Philadelphia.T.
Briscoe and J. Carroll.
1997.
Automatic extraction ofsubcategorization from corpora.
Proceedings ofANLP 1997, pages 356-363.
Washington D.C.S.
Buchholz.
1998.
Distinguishing complements fromadjuncts using memory-based learning.
Proceedingsof the Workshop on Automated Acquisition of Syntaxand Parsing, ESSLLI-98, pages 41-48.
Saarbruecken,Germany.N.
Chawla, K. Bowyer, L. Hall and W.P.
Kegelmeyer.2002.
SMOTE: Synthetic minority over-samplingtechnique.
Journal of Artificial Intelligence Research16:321-357.
Morgan Kaufmann.P.
Domingos.
1999.
Metacost: A general method formaking classifiers cost-sensitive.
Proceedings of theInternational Conference on Knowledge Discoveryand Data Mining, pages 155-164.
San Diego, CA.N.
Hatzigeorgiu et al 2000.
Design and Implementationof the online ILSP Greek Corpus.
Proceedings ofLREC 2000, pages 1737-1742.
Greece.N.
Japkowicz.
2000.
The class imbalance problem:significance and strategies.
Proceedings of the Inter-national Conference on Artificial Intelligence.
LasVegas, Nevada.K.
Kermanidis, N. Fakotakis and G. Kokkinakis.
2002.DELOS: An automatically tagged economic corpusfor Modern Greek.
Proceedings of LREC 2002, pages93-100.
Las Palmas de Gran Canaria.C.
Klairis and G. Babiniotis.
1999.
Grammar of ModernGreek.
II.
The Verb.
(in Greek).
Athens: Greek Let-ters Publications.A.
Korhonen, G. Gorrell and D. McCarthy.
2000.
Statis-tical filtering and subcategorization frame acquisi-tion.
Proceedings of the Joint SIGDAT EMNLPConference, pages 199-205.
Hong Kong.M.
Kubat and S. Matwin.
1997.
Addressing the curse ofimbalanced training sets.
Proceedings of ICML 97,pages 179- 186.J.
Laurikkala.
2001.
Improving identification of difficultsmall classes by balancing class distribution.
Pro-ceedings of the Conference on Artificial Intelligencein Medicine in Europe, pages 63-66.
Portugal.D.
Lewis and W. Gale.
1994.
Training text classifiers byuncertainty sampling.
Proceedings of the Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 3-12.Dublin.C.
Ling and C. Li.
1998.
Data mining for direct market-ing problems and solutions.
Proceedings of KDD 98Conference.
New York, NY.P.
Merlo and M. Leybold.
2001.
Automatic distinctionof arguments and modifiers: the case of prepositionalphrases.
Proceedings of the Workshop on Computa-tional Language Learning, Toulouse, France.Partners of ESPRIT-291/860.
1986.
Unification of theword classes of the ESPRIT Project 860.
Internal Re-port BU-WKL-0376.F.
Provost and T. Fawcett.
2001.
Robust classificationfor imprecise environments.
Machine Learning42(3): 203-231.A.
Sarkar and D. Zeman.
2000.
Automatic extraction ofsubcategorization frames for Czech.
Proceedings ofCOLING 2000, pages 691-697.
Saarbruecken, Ger-many.K.
Sgarbas, N. Fakotakis and G. Kokkinakis.
2000.
Astraightforward approach to morphological analysisand synthesis.
Proceedings of COMLEX 2000, pages31-34.
Kato Achaia, Greece.E.
Stamatatos, N. Fakotakis and G. Kokkinakis.
2000.
Apractical chunker for unrestricted text.
Proceedingsof NLP 2000, pages 139-150.
Patras, Greece.C.
Stanfill and D. Waltz.
1986.
Toward memory-basedreasoning.
Communications of the ACM 29:1213-1228.I.
Tomek.
1976.
Two modifications of CNN.
IEEETransactions on Systems, Man and Communications,SMC-6:769-772.
