Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 232?239,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsMunich-Edinburgh-Stuttgart Submissions at WMT13:Morphological and Syntactic Processing for SMTMarion Weller1, Max Kisselew1, Svetlana Smekalova1, Alexander Fraser2,Helmut Schmid2, Nadir Durrani3, Hassan Sajjad4, Richa?rd Farkas51University of Stuttgart ?
(wellermn|kisselmx|smekalsa)@ims.uni-stuttgart.de2Ludwig-Maximilian University of Munich ?
(schmid|fraser)@cis.uni-muenchen.de3University of Edinburgh ?
dnadir@inf.ed.ac.uk4Qatar Computing Research Institute ?
hsajjad@qf.org.qa5University of Szeged ?
rfarkas@inf.u-szeged.huAbstractWe present 5 systems of the Munich-Edinburgh-Stuttgart1 joint submissions tothe 2013 SMT Shared Task: FR-EN, EN-FR, RU-EN, DE-EN and EN-DE.
Thefirst three systems employ inflectional gen-eralization, while the latter two employparser-based reordering, and DE-EN per-forms compound splitting.
For our ex-periments, we use standard phrase-basedMoses systems and operation sequencemodels (OSM).1 IntroductionMorphologically complex languages often lead todata sparsity problems in statistical machine trans-lation.
For translation pairs with morphologicallyrich source languages and English as target lan-guage, we focus on simplifying the input languagein order to reduce the complexity of the translationmodel.
The pre-processing of the source-languageis language-specific, requiring morphological anal-ysis (FR, RU) as well as sentence reordering (DE)and dealing with compounds (DE).
Due to timeconstraints we did not deal with inflection for DE-EN and EN-DE.The morphological simplification process con-sists in lemmatizing inflected word forms and deal-ing with word formation (splitting portmanteauprepositions or compounds).
This needs to takeinto account translation-relevant features (e.g.
num-ber) which vary across the different language pairs:while French only has the features number andgender, a wider array of features needs to be con-sidered when modelling Russian (cf.
table 6).
Inaddition to morphological reduction, we also applytransliteration models learned from automatically1The language pairs DE-EN and RU-EN were developedin collaboration with the Qatar Computing Research Instituteand the University of Szeged.mined transliterations to handle out-of-vocabularywords (OOVs) when translating from Russian.Replacing inflected word forms with simplervariants (lemmas or the components of split com-pounds) aims not only at reducing the general com-plexity of the translation model, but also at decreas-ing the amount of out-of-vocabulary words in theinput data.
This is particularly the case with Ger-man compounds, which are very productive andthus often lack coverage in the parallel trainingdata, whereas the individual components can betranslated.
Similarly, inflected word forms (e.g.
ad-jectives) benefit from the reduction to lemmas ifthe full inflection paradigm does not occur in theparallel training data.For EN-FR, a translation pair with a morpho-logically complex target language, we describe atwo-step translation system built on non-inflectedword stems with a post-processing component forpredicting morphological features and the genera-tion of inflected forms.
In addition to the advantageof a more general translation model, this methodalso allows the generation of inflected word formswhich do not occur in the training data.2 Experimental setupThe translation experiments in this paper are car-ried out with either a standard phrase-based Mosessystem (DE-EN, EN-DE, EN-FR and FR-EN) orwith an operation sequence model (RU-EN, DE-EN), cf.
Durrani et al(2013b) for more details.An operation sequence model (OSM) is a state-of-the-art SMT-system that learns translation andreordering patterns by representing a sentence pairand its word alignment as a unique sequence ofoperations (see e.g.
Durrani et al(2011), Durraniet al(2013a) for more details).
For the Moses sys-tems we used the old train-model perl scripts ratherthan the EMS, so we did not perform Good-Turingsmoothing; parameter tuning was carried out withbatch-mira (Cherry and Foster, 2012).2321 Removal of empty lines2 Conversion of HTML special characters like&quot; to the corresponding characters3 Unification of words that were written bothwith an ?
or with an oe to only one spelling4 Punctuation normalization and tokenization5 Putting together clitics and apostrophes likel ?
or d ?
to l?
and d?Table 1: Text normalization for FR-EN.Definite determiners la / l?
/ les ?
leIndefinite determiners un / une ?
unAdjectives Infl.
form ?
lemmaPortmanteaus e. g. au ?
a` leVerb participles Reduced toinflected for gender non-inflectedand number verb participle formending in e?e/e?s/e?es ending in e?Clitics and apostroph- d?
?
de,ized words are converted qu?
?
que,to their lemmas n?
?
ne, ...Table 2: Rules for morphological simplification.The development data consists of the concate-nated news-data sets from the years 2008-2011.Unless otherwise stated, we use all constrained data(parallel and monolingual).
For the target-side lan-guage models, we follow the approach of Schwenkand Koehn (2008) and train a separate languagemodel for each corpus and then interpolate themusing weights optimized on development data.3 French to EnglishFrench has a much richer morphology than English;for example, adjectives in French are inflected withrespect to gender and number whereas adjectivesin English are not inflected at all.
This causes datasparsity in coverage of French inflected forms.
Wetry to overcome this problem by simplifying Frenchinflected forms in a pre-processing step in order toadapt the French input better to the English output.Processing of the training and test data Thepre-processing of the French input consists of twosteps: (1) normalizing not well-formed data (cf.table 1) and (2) morphological simplification.In the second step, the normalized training datais annotated with Part-of-Speech tags (PoS-tags)and word lemmas using RFTagger (Schmid andLaws, 2008) which was trained on the French tree-bank (Abeille?
et al 2003).
French forms are thensimplified according to the rules given in table 2.Data and experiments We trained a French toEnglish Moses system on the preprocessed andSystem BLEU (cs) BLEU (ci)Baseline 29.90 31.02Simplified French* 29.70 30.83Table 3: Results of the French to English system(WMT-2012).
The marked system (*) correspondsto the system submitted for manual evaluation.
(cs:case-sensitive, ci: case-insensitive)simplified constrained parallel data.Due to tractability problems with word align-ment, the 109 French-English corpus and the UNcorpus were filtered to a more manageable size.The filtering criteria are sentence length (between15 and 25 words), as well as strings indicating thata sentence is neither French nor English, or other-wise not well-formed, aiming to obtain a subset ofgood-quality sentences.
In total, we use 9M par-allel sentences.
For the English language modelwe use large training data with 287.3M true-casedsentences (including the LDC Giga-word data).We compare two systems: a baseline with reg-ular French text, and a system with the describedmorphological simplifications.
Results for theWMT-2012 test set are shown in table 3.
Eventhough the baseline is better than the simplifiedsystem in terms of BLEU, we assume that the trans-lation model of the simplified system benefits fromthe overall generalization ?
thus, human annotatorsmight prefer the output of the simplified system.For the WMT-2013 set, we obtain BLEU scoresof 29,97 (cs) and 31,05 (ci) with the system builton simplified French (mes-simplifiedfrench).4 English to FrenchTranslating into a morphologically rich languagefaces two problems: that of asymmetry of mor-phological information contained in the source andtarget language and that of data sparsity.In this section we describe a two-step system de-signed to overcome these types of problems: first,the French data is reduced to non-inflected forms(stems) with translation-relevant morphological fea-tures, which is used to built the translation model.The second step consists of predicting all neces-sary morphological features for the translation out-put, which are then used to generate fully inflectedforms.
This two-step setup decreases the complex-ity of the translation task by removing language-specific features from the translation model.
Fur-thermore, generating inflected forms based on wordstems and morphological features allows to gener-233ate forms which do not occur in the parallel trainingdata ?
this is not possible in a standard SMT setup.The idea of separating the translation into twosteps to deal with complex morphology was in-troduced by Toutanova et al(2008).
Fraser etal.
(2012) applied this method to the languagepair English-German with an additional specialfocus on word formation issues such as the split-ting and merging of portmanteau prepositions andcompounds.
The presented inflection predictionsystems focuses on nominal inflection; verbal in-flection is not addressed.Morphological analysis and resources Themorphological analysis of the French training datais obtained using RFTagger, which is designedfor annotating fine-grained morphological tags(Schmid and Laws, 2008).
For generating inflectedforms based on stems and morphological features,we use an extended version of the finite-state mor-phology FRMOR (Zhou, 2007).
Additionally, weuse a manually compiled list of abbreviations andnamed entities (names of countries) and their re-spective grammatical gender.Stemming For building the SMT system, theFrench data (parallel and monolingual) is trans-formed into a stemmed representation.
Nouns,i.e.
the heads of NPs or PPs, are marked withinflection-relevant features: gender is consideredas part of the stem, whereas number is determinedby the source-side input: for example, we expectsource-language words in plural to be translated bytranslated by stems with plural markup.
This stem-markup is necessary in order to guarantee that thenumber information is not lost during translation.For a better generalization, portmanteaus are splitinto separate parts: au?
a`+le (meaning, ?to the?
).Predicting morphological features For predict-ing the morphological features of the SMT output(number and gender), we use a linear chain CRF(Lavergne et al 2010) trained on data annotatedwith these features using n-grams of stems and part-of-speech tags within a window of 4 positions toeach side of the current word.
Through the CRF,the values specified in the stem-markup (numberand gender on nouns) are propagated over the restof the linguistic phrase, as shown in column 2 oftable 4.
Based on the stems and the morphologicalfeatures, inflected forms can be generated usingFRMOR (column 3).Post-processing As the French data has beennormalized, a post-processing step is needed in or-der to generate correct French surface forms: splitportmanteaus are merged into their regular formsbased on a simple rule set.
Furthermore, apostro-phes are reintroduced for words like le, la, ne, ... ifthey are followed by a vowel.
Column 4 in table 4shows post-processing including portmanteau for-mation.
Since we work on lowercased data, anadditional recasing step is required.Experiments and evaluation We use the sameset of reduced parallel data as the FR-EN system;the language model is built on 32M French sen-tences.
Results for the WMT-2012 test set are givenin table 5.
Variant 1 shows the results for a smallsystem trained only on a part of the training data(Europarl+News Commentary), whereas variant 2corresponds to the submitted system.
A small-scaleanalysis indicated that the inflection prediction sys-tem tends to have problems with subject-verb agree-ment.
We trained a factored system using addi-tional PoS-tags with number information whichlead to a small improvement on both variants.While the small model is significantly better thanthe baseline2 as it benefits more from the general-ization, the result for the full system is worse thanthe baseline3.
Here, given the large amount ofdata, the generalization effect has less influence.However, we assume that the more general modelfrom the inflection prediction system produces bet-ter translations than a regular model containing alarge amount of irrelevant inflectional information,particularly when considering that it can producewell-formed inflected sequences that are inaccessi-ble to the baseline.
Even though this is not reflectedin terms of BLEU, humans might prefer the inflec-tion prediction system.For the WMT-2013 set, we obtain BLEU scoresof 29.6 (ci) and 28.30 (cs) with the inflection pre-diction system mes-inflection (marked in table 5).5 Russian-EnglishThe preparation of the Russian data includes thefollowing stages: (1) tokenization and tagging and(2) morphological reduction.Tagging and tagging errors For tagging, we usea version of RFTagger (Schmid and Laws, 2008)2Pairwise bootstrap resampling with 1000 samples.3However, the large inflection-prediction system has aslightly better NIST score than the baseline (7.63 vs. 7.61).234SMT-output predicted generated after post- glosswith stem-markup in bold print features forms processingavertissement<Masc><Pl>[N] Masc.Pl avertissements avertissements warningssinistre[ADJ] Masc.Pl sinistres sinistres direde[P] ?
de du fromle[ART] Masc.Sg le thepentagone<Masc><Sg>[N] Masc.Sg pentagone pentagone pentagonsur[P] ?
sur sur overde[P] ?
de d?
ofe?ventuel[ADJ] Fem.Pl e?ventuelles e?ventuelles potentialre?duction<Fem><Pl>[N] Fem.Pl re?ductions re?ductions reductionsde[P] ?
de du ofle[ART] Masc.Sg le thebudget<Masc><Sg>[N] Masc.Sg budget budget budgetde[P] ?
de de ofle[ART] Fem.Sg la la thede?fense<Fem><Sg>[N] Fem.Sg de?fense de?fense de?fenseTable 4: Processing steps for the input sentence dire warnings from pentagon over potential defence cuts.that has been developed based on data tagged withTreeTagger (Schmid, 1994) using a model fromSharoff et al(2008).
The data processed by Tree-Tagger contained errors such as wrong definitionof PoS for adverbs, wrong selection of gender foradjectives in plural and missing features for pro-nouns and adverbs.
In order to train RFTagger, theoutput of TreeTagger was corrected with a set ofempirical rules.
In particular, the morphologicalfeatures of nominal phrases were made consistentto train RFTagger: in contrast to TreeTagger, wheremorphological features are regarded as part of thePoS-tag, RFTagger allows for a separate handlingof morphological features and POS tags.Despite a generally good tagging quality, someerrors seem to be unavoidable due to the ambiguityof certain grammatical forms in Russian.
A goodexample of this are neuter nouns that have the sameform in all cases, or feminine nouns, which haveidentical forms in singular genitive and plural nom-inative (Sharoff et al 2008).
Since Russian has nobinding word order, and the case of nouns cannotbe determined on that basis, such errors cannot becorrected with empirical rules implemented as post-System BLEU (ci) BLEU (cs)1 Baseline 24.91 23.40InflPred 25.31 23.81InflPred-factored 25.53 24.042 Baseline 29.32 27.65InflPred* 29.07 27.40InflPred-factored 29.17 27.46Table 5: Results for French inflection predictionon the WMT-2012 test set.
The marked system (*)corresponds to the system submitted for manualevaluation.processing.
Similar errors occur when specifyingthe case of adjectives, since the suffixes of adjec-tives are even less varied as compared to the nouns.In our application, we hope that this type of errordoes not affect the result due to the following sup-pression of a number of morphological attributesincluding the case of adjectives.Morphological reduction In comparison toSlavic languages, English is morphologically poor.For example, English has no morphological at-tributes for nouns and adjectives to express genderor case; verbs have no gender either.
In contrast,Russian is morphologically very rich ?
there aree.g.
6 cases and 3 grammatical genders, whichmanifest themselves in different suffixes for nouns,pronouns, adjectives and some verb forms.
Whentranslating from Russian into English, many ofthese attributes are (hopefully) redundant and aretherefore deleted from the training data.
The mor-phological reduction in our system was applied tonouns, pronouns, verbs, adjectives, prepositionsand conjunctions.
The rest of the POS (adverbs,particles, interjections and abbreviations) have nomorphological attributes.
The list of the originaland the reduced attributes is given in Table 6.Transliteration mining to handle OOVs Themachine translation system fails to translate out-of-vocabulary words (OOVs) as they are unknown tothe training data.
Most of the OOVs are named en-tities and transliterating them to the target languagescript could solve this problem.
The transliterationsystem requires a list of transliteration pairs fortraining.
As we do not have such a list, we usethe unsupervised transliteration mining system ofSajjad et al(2012) that takes a list of word pairs for235Part of Attributes ReducedSpeech RFTagger attributesNoun Type TypeGender GenderNumber NumberCase Casenom,gen,dat,acc,instr,prep gen,notgenAnimateCase 2Pronoun Person PersonGender GenderNumber NumberCase Casenom,gen,dat,acc,instr,prep nom,notnomSyntactic typeAnimatedVerb Type TypeVForm VFormTense TensePerson PersonNumber NumberGenderVoice VoiceDefinitenessAspect AspectCaseAdjec- Type Typetive Degree DegreeGenderNumberCaseDefinitenessPrep- Typeosition FormationCaseConjunc- Type Typetion Formation FormationTable 6: Rules for simplifying the morphologicalcomplexity for RU.training and extracts transliteration pairs that canbe used for the training of the transliteration system.The procedure of mining transliteration pairs andtransliterating OOVs is described as follows: Weword-align the parallel corpus using GIZA++ andsymmetrize the alignments using the grow-diag-final-and heuristic.
We extract all word pairs whichoccur as 1-to-1 alignments (Sajjad et al 2011) andlater refer to them as a list of word pairs.
We trainthe unsupervised transliteration mining system onthe list of word pairs and extract transliterationpairs.
We use these mined pairs to build a transliter-ation system using the Moses toolkit.
The translit-eration system is applied as a post-processing stepto transliterate OOVs.The morphological reduction of Russian (cf.
sec-tion 5) does not process most of the OOVs as theyare also unknown to the POS tagger.
So OOVs thatwe get are in their original form.
When translit-Original corpusSYS WMT-2012 WMT-2013GIZA++ 32.51 25.5TA-GIZA++ 33.40 25.9*Morph-reducedSYS WMT-2012 WMT-2013GIZA++ 31.22 24.3TA-GIZA++ 31.40 24.45Table 7: Russian to English machine translationsystem evaluated on WMT-2012 and WMT-2013.Human evaluation in WMT13 is performed on thesystem trained using the original corpus with TA-GIZA++ for alignment (marked with *).erating them, the inflected forms generate wrongEnglish transliterations as inflectional suffixes gettransliterated too, specially OOV named entities.We solved this problem by stemming the OOVsbased on a list of suffixes ( , , , , , ) andtransliterating the stemmed forms.Experiments and results We trained the sys-tems separately on GIZA++ and transliterationaugmented-GIZA++ (TA-GIZA++) to comparetheir results; for more details see Sajjad et al(2013).
All systems are tuned using PROv1 (Nakovet al 2012).
The translation output is post-processed to transliterate OOVs.Table 7 summarizes the results of RU-EN trans-lation systems trained on the original corpus andon the morph-reduced corpus.
Using TA-GIZA++alignment gives the best results for both WMT-2012 and WMT-2013, leading to an improvementof 0.4 BLEU points.The system built on the morph-reduced dataleads to decreased BLEU results.
However, the per-centage of OOVs is reduced for both test sets whenusing the morph-reduced data set compared to theoriginal data.
An analysis of the output showedthat the morph-reduced system makes mistakes inchoosing the right tense of the verb, which mightbe one reason for this outcome.
In the future, wewould like to investigate this issue in detail.6 German to English and English toGermanWe submitted systems for DE-EN and EN-DEwhich used constituent parses for pre-reordering.For DE-EN we also deal with word formation is-sues such as compound splitting.
We did not per-form inflectional normalization or generation forGerman due to time constraints, instead focusing236our efforts on these issues for French and Russianas previously described.German to English German has a wider diver-sity of clausal orderings than English, all of whichneed to be mapped to the English SVO order.
Thisis a difficult problem to solve during inference, asshown for hierarchical SMT by Fabienne Brauneand Fraser (2012) and for phrase-based SMT byBisazza and Federico (2012).We syntactically parsed all of the source sidesentences of the parallel German to English dataavailable, and the tuning, test and blindtest sets.We then applied reordering rules to these parses.We use the rules for reordering German constituentparses of Collins et al(2005) together with theadditional rules described by Fraser (2009).
Theseare applied as a preprocess to all German data.For parsing the German sentences, we used thegenerative phrase-structure parser BitPar with opti-mizations of the grammar, as described by Fraseret al(2013).
The parser was trained on the TigerTreebank (Brants et al 2002) along with utilizingthe Europarl corpus as unlabeled data.
At the train-ing of Bitpar, we followed the targeted self-trainingapproach (Katz-Brown et al 2011) as follows.
Weparsed the whole Europarl corpus using a grammartrained on the Tiger corpus and extracted the 100-best parse trees for each sentence.
We selected theparse tree among the 100 candidates which got thehighest usefulness scores for the reordering task.Then we trained a new grammar on the concatena-tion of the Tiger corpus and the automatic parsesfrom Europarl.The usefulness score estimates the value of aparse tree for the reordering task.
We calculatedthis score as the similarity between the word orderachieved by applying the parse tree-based reorder-ing rules of Fraser (2009) and the word order indi-cated by the automatic word alignment betweenthe German and English sentences in Europarl.We used the Kendall?s Tau Distance as the simi-larity metric of two word orderings (as suggestedby Birch and Osborne (2010)).Following this, we performed linguistically-informed compound splitting, using the system ofFritzinger and Fraser (2010), which disambiguatescompeting analyses from the high-recall StuttgartMorphological Analyzer SMOR (Schmid et al2004) using corpus statistics.
We also split Germanportmanteaus like zum?
zu dem (meaning to the).system BLEU BLEU system name(ci) (cs)DE-EN (OSM) 27.60 26.12 MESDE-EN (OSM) 27.48 25.99 not submittedBitPar not self-trainedDE-EN (Moses) 27.14 25.65 MES-Szeged-reorder-splitDE-EN (Moses) 26.82 25.36 not submittedBitPar not self-trainedEN-DE (Moses) 19.68 18.97 MES-reorderTable 8: Results on WMT-2013 (blindtest)English to German The task of mapping En-glish SVO order to the different clausal orders inGerman is difficult.
For our English to Germansystems, we solved this by parsing the English andapplying the system of Gojun and Fraser (2012) toreorder English into the correct German clausal or-der (depending on the clause type which is detectedusing the English parse, see (Gojun and Fraser,2012) for further details).We primarily used the Charniak-Johnson gener-ative parser (Charniak and Johnson, 2005) to parsethe English Europarl data and the test data.
How-ever, due to time constraints we additionally usedBerkeley parses of about 400K Europarl sentencesand the other English parallel training data.
Wealso left a small amount of the English paralleltraining data unparsed, which means that it wasnot reordered.
For tune, test and blindtest (WMT-2013), we used the Charniak-Johnson generativeparser.Experiments and results We used all availabletraining data for constrained systems; results forthe WMT-2013 set are given in table 8.
For thecontrastive BitPar results, we reparsed WMT-2013.7 ConclusionWe presented 5 systems dealing with complex mor-phology.
For two language pairs with a morpho-logically rich source language (FR and RU), theinput was reduced to a simplified representationcontaining only translation-relevant morphologi-cal information (e.g.
number on nouns).
We alsoused reordering techniques for DE-EN and EN-DE.For translating into a language with rich morphol-ogy (EN-FR), we applied a two-step method thatfirst translates into a stemmed representation ofthe target language and then generates inflectedforms based on morphological features predictedon monolingual data.237AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful feedback and suggestions, DanielQuernheim for providing Berkeley parses of someof the English data, Stefan Ru?d for help with themanual evalution, and Philipp Koehn and BarryHaddow for providing data and alignments.Nadir Durrani was funded by the EuropeanUnion Seventh Framework Programme (FP7/2007-2013) under grant agreement n. 287658.
Alexan-der Fraser was funded by Deutsche Forschungs-gemeinschaft grant Models of Morphosyntax forStatistical Machine Translation and from the Eu-ropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013) under Grant Agreementn.
248005.
Marion Weller was funded from theEuropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013) under Grant Agreementn.
248005.
Svetlana Smekalova was funded byDeutsche Forschungsgemeinschaft grant Modelsof Morphosyntax for Statistical Machine Trans-lation.
Helmut Schmid and Max Kisselew weresupported by Deutsche Forschungsgemeinschaftgrant SFB 732.
Richa?rd Farkas was supported bythe European Union and the European Social Fundthrough project FuturICT.hu (grant n. TA?MOP-4.2.2.C-11/1/KONV-2012-0013).
This publicationonly reflects the authors?
views.ReferencesA.
Abeille?, L. Cle?ment, and F. Toussenel.
2003.
Build-ing a treebank for french.
In A.
Abeille?, editor, Tree-banks.
Kluwer, Dordrecht.Alexandra Birch and Miles Osborne.
2010.
Lrscore forevaluating lexical and reordering quality in mt.
InProceedings of ACL WMT and MetricsMATR, Upp-sala, Sweden.Arianna Bisazza and Marcello Federico.
2012.
Mod-ified distortion matrices for phrase-based statisticalmachine translation.
In ACL, pages 478?487.Sabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In Proceedings of the Workshop on Tree-banks and Linguistic Theories.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In ACL, pages 173?180, Ann Arbor, MI,June.
Association for Computational Linguistics.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the North American Chapter of the Asso-ciation for Computational Linguistics (NAACL).Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Porceedings of ACL 2005.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A Joint Sequence Translation Model with In-tegrated Reordering.
In Proceedings of ACL-HLT2011, Portland, Oregon, USA.Nadir Durrani, Alexander Fraser, and Helmut Schmid.2013a.
Model With Minimal Translation Units, ButDecode With Phrases.
In Proceedings of NAACL2013, Atlanta, Georgia, USA.Nadir Durrani, Helmut Schmid, Alexander Fraser, Has-san Sajjad, and Richa?rd Farkas.
2013b.
Munich-Edinburgh-Stuttgart Submissions of OSM Systemsat WMT13.
In Proceedings of the Eighth Workshopon Statistical Machine Translation, Sofia, Bulgaria.Anita Gojun Fabienne Braune and Alexander Fraser.2012.
Long-distance reordering during search forhierarchical phrase-based SMT.
In Proceedings ofEAMT 2012.Alexander Fraser, Marion Weller, Aoife Cahill, and Fa-bienne Cap.
2012.
Modeling Inflection and Word-Formation in SMT.
In Proceedings of EACL 2012,Avignon, France.Alexander Fraser, Helmut Schmid, Richa?rd Farkas,Renjing Wang, and Hinrich Schu?tze.
2013.
Knowl-edge sources for constituent parsing of German, amorphologically rich and less-configurational lan-guage.
Computational Linguistics - to appear.Alexander Fraser.
2009.
Experiments in morphosyn-tactic processing for translating to and from German.In EACL WMT.Fabienne Fritzinger and Alexander Fraser.
2010.
Howto avoid burning ducks: Combining linguistic analy-sis and corpus statistics for German compound pro-cessing.
In ACL WMT and Metrics MATR.Anita Gojun and Alexander Fraser.
2012.
Determin-ing the placement of German verbs in English-to-German SMT.
In Proceedings of EACL 2012.Jason Katz-Brown, Slav Petrov, Ryan McDon-ald, Franz Och, David Talbot, Hiroshi Ichikawa,Masakazu Seno, and Hideto Kazawa.
2011.
Train-ing a parser for machine translation reordering.
InProceedings of EMNLP 2011, Edinburgh, Scotland.Thomas Lavergne, Olivier Cappe?, and Franc?ois Yvon.2010.
Practical very large scale CRFs.
In Proceed-ings of ACL 2010, pages 504?513.Preslav Nakov, Francisco Guzma?n, and Stephan Vo-gel.
2012.
Optimizing for sentence-level BLEU+1yields short translations.
Mumbai, India.238Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2011.
An algorithm for unsupervised transliterationmining with an application to word alignment.
InProceedings of ACL 2011, Portland, USA.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2012.
A statistical model for unsupervised and semi-supervised transliteration mining.
In Proceedings ofACL 2012, Jeju, Korea.Hassan Sajjad, Svetlana Smekalova, Nadir Durrani,Alexander Fraser, and Helmut Schmid.
2013.QCRI-MES Submission at WMT13: Using Translit-eration Mining to Improve Statistical MachineTranslation.
In Proceedings of the Eighth Workshopon Statistical Machine Translation, Sofia, Bulgaria.Helmut Schmid and Florian Laws.
2008.
Estimationof conditional probabilities with decision trees andan application to fine-grained pos tagging.
In Pro-ceedings of COLING 2008, Stroudsburg, PA, USA.Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004.SMOR: a German Computational Morphology Cov-ering Derivation, Composition, and Inflection.
InProceedings of LREC 2004.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In Proceedings of theInternational Conference on New Methods in Lan-guage Processing.Holger Schwenk and Philipp Koehn.
2008.
Largeand diverse language models for statistical machinetranslation.
In Proceedings of IJCNLP 2008.Serge Sharoff, Mikhail Kopotev, Tomaz Erjavec, AnnaFeldman, and Dagmar Divjak.
2008.
Designing andevaluating russian tagsets.
In Proceedings of LREC2008.Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.2008.
Applying Morphology Generation Models toMachine Translation.
In Proceedings of ACL-HLT2008.Zhenxia Zhou.
2007.
Entwicklung einer franzo?sischenFinite-State-Morphologie.
Diploma Thesis, Insti-tute for Natural Language Processing, University ofStuttgart.239
