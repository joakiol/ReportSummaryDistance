Proceedings of NAACL HLT 2007, pages 420?427,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsA Cascaded Machine Learning Approachto Interpreting Temporal ExpressionsDavid Ahn Joris van Rantwijk Maarten de RijkeISLA, University of AmsterdamKruislaan 403, 1098 SJ Amsterdam, The Netherlands{ahn, rantwijk, mdr}@science.uva.nlAbstractA new architecture for identifying and in-terpreting temporal expressions is intro-duced, in which the large set of com-plex hand-crafted rules standard in sys-tems for this task is replaced by a seriesof machine learned classifiers and a muchsmaller set of context-independent seman-tic composition rules.
Experiments withthe TERN 2004 data set demonstrate thatoverall system performance is comparableto the state-of-the-art, and that normaliza-tion performance is particularly good.1 IntroductionIn order to fully understand a piece of text, wemust understand its temporal structure.
The firststep toward such an understanding is identifying ex-plicit references to time.
We focus on the task ofautomatically annotating temporal expressions (ortimexes)?both identifying them in text and inter-preting them to determine what times they refer to.Timex annotation is more than normalizing date ex-pressions.
First, time consists of more than calen-dar dates and clock times?it also includes points offiner and coarser granularity, durations, and sets oftimes.
Second, the expressions that refer to time arenot just full date and time expressions?they may beunderspecified, ambiguous, and anaphoric.Building a system for the full timex identifica-tion and interpretation task can be tedious, requiringa great deal of manual effort.
The 2004 TemporalExpression Recognition and Normalization (TERN)evaluation1 evaluated systems on two tasks: timex1http://timex2.mitre.org/tern.htmlrecognition (identification) alone and recognitionand normalization (interpretation) together.
All thefull-task systems were rule-based systems; the topperforming full-task system uses in excess of onethousand hand-crafted rules, which probe words andtheir contexts in order to both identify timexes andto assemble information necessary to interpret them(Negri and Marseglia, 2004).
By contrast, machinelearned systems dominated the recognition-only taskand even achieved slightly better recognition scoresthan their rule-based counterparts.We seek to demonstrate that a timex annotationsystem that performs both recognition and normal-ization need not be a tangle of rules that serve dou-ble duty for identification and interpretation and thatmix up context-dependent and context-independentprocessing.
We propose a novel architecture thatclearly separates syntactic, semantic, and prag-matic processing and factors out context-dependentfrom context-independent processing.
Factoringout context-dependent disambiguation into separateclassification tasks introduces the opportunity forusing machine learning, which supports our maingoal: building a portable, trainable timex annota-tion system in which the role of hand-crafted rulesis minimized.
The system we present here (avail-able from http://ilps.science.uva.nl/Resources/timextag/) achieves the goal ofmaking use of only a small set of hand-crafted,context-independent rules to achieve state-of-the-artnormalization performance.In the following section, we define what a timexis.
We give an overview of our system architecturein ?3 and describe the components in ?4?7.
?8 pro-vides an evaluation of our system on the full timexannotation task, and we conclude in ?9.4202 What is a timex?Temporal semantics receives a great deal of attentionin the semantics literature (cf.
(Mani et al, 2005)),but the focus is generally on verbal semantics (i.e.,tense and aspect).
In determining what a timex isand how one should be normalized, we simply fol-low the TIDES TIMEX2 standard for timex annota-tion (Ferro et al, 2004).
According to this standard,timexes are phrases or words that refer to times,where times may be points or durations, or sets ofpoints or durations.
Points are more than just in-stanteous moments in time?a point may also be atime with some duration, as long as it spans a singleunit of some temporal granularity.
Whether a timexrefers to a point or a duration is a question of per-spective rather than of ontology.
A point-referringtimex such as October 18, 2006 refers to an intervalof one day as an atom at the granularity of a day.
Aduration-referring timex such as the whole day mayrefer to the same temporal interval, but it focuses onthe durative nature of this interval.In addition to specifying which phrases aretimexes, the TIMEX2 standard also provides a setof attributes for normalizing these timexes.
We fo-cus on the VAL attribute, which takes values that arean extension of the ISO-8601 standard for represent-ing time (ISO, 1997).
TIMEX2 VAL attributes cantake one of three basic types of values:Points are expressed as a string matching the pat-tern dddd-dd-ddTdd:dd:dd.d+, where d in-dicates a digit.
Such a string is to be interpreted asyear-month-dateThour:minute:seconds, and may betruncated from the right, indicating points of coarsergranularity.
Any place may be filled with a place-holder X, which indicates an unknown or vaguevalue, and there are also a handful of token values(character strings) for seasons and parts of the daywhich may substitute for months and times.
There isalso an alternate week-based format dddd-Wdd-d,interpreted as year-Wweek number-day of the week.Durations are expressed as a string matching thepattern Pd+u or PTd+u, where d+ indicates one ormore digits and u indicates a unit token (such as Yfor years).
A placeholder X may be used instead ofa number to indicate vagueness.Vague points: past ref, present ref,future ref.ParseddocumentPhraseclassifierSemanticclassifierphrasestimexesDirectionclassifierpre-normpointsTemporalanchoringFinalnormalizationpre-normpointsw/dir &anchorpre-normpoints w/dirTimex-annotateddocumentnormalizedpointsSemantic composition rules(class-specific)durationsgenpointsrecursnormalizednon-pointspointsA BCDEFgendursFigure 1: Timex annotation architecture (letters forease of reference).The other attribute which we address in this paperis the boolean-valued SET attribute; a SET timexis one that refers to a recurring time.
The remain-ing attributes are MOD, ANCHOR VAL, and AN-CHOR DIR; our system produces values for theseattributes, but we do not address them in this paper.The TIMEX2 annotation standard has been usedto create several manually annotated corpora.
Forthe experiments we present in this paper, we usethe corpora annotated for the TERN 2004 evalua-tion (Ferro, 2004).
These consist of a training setof 511 documents of newswire and broadcast newstranscripts, with 5326 TIMEX2s, and a test set of192 similar documents, with 1828 TIMEX2s.3 ArchitectureThe architecture of our timex annotation system isdepicted in Fig.
1.
Our system begins with parseddocuments as input.
Our recognition module is amachine learned classifier (A); it is described in ?4.Phrases that have been classified as timexes arethen sent to the semantic class classifier (B).
Seman-tic class disambiguation is the first point at whichcontext dependence enters into timex interpretation.While some timexes are unambiguous with respectto whether they refer to a point, a duration, or aset, many timexes are semantically ambiguous andcan only be disambiguated in context.
The machinelearned classifier for this task is described in ?5.Based on the class assigned by the semantic class421classifier, the semantic composition component (C)generates (underspecified) semantic representationsusing class-specific, context-independent rules.
Therules we use are simple pattern-matching rules thatmap lexical items or sequences of lexical itemswithin a timex to semantic representations.
We de-scribe the semantic composition component in ?6.For most classes of timexes, the semantic compo-sition component generates a semantic representa-tion that can be directly translated into a normalizedvalue.
Timexes that refer to specific points are theonly exception.
While some point timexes are fullyqualified, and thus also directly normalizable, manyneed to be anchored to another time in context inorder to be fully normalized.
Thus, context depen-dence again enters the timex interpretation process,and now in two ways.
One is obvious: these refer-ential timexes, which need a temporal anchor, haveto find it in context.
This task requires a referenceresolution process (E), which is described in ?7.1.The second ambiguity regards the relation be-tween a referential timex and its anchor.
Referen-tial timexes, like anaphoric definites, relate to theiranchors through a bridging relation, which is deter-mined primarily by the content of the timex?e.g.,two years later refers to a point two years after itsanchor.
For some referential timexes, though, thedirection of the relation (before or after the anchor)is not specified.
The machine learned classifier (D)resolves this ambiguity; see ?7.2.For referential timexes, final normalization (F) isa straightforward combination of semantic represen-tation, temporal anchor, and direction class.Not pictured in Fig.
1 is a module that recognizesand normalizes timexes in document metadata usinga set of simple regular expressions (REs; 14 in total).This module also determines the document time-stamp for referential timexes by using a few heuris-tics to choose from among multiple timestamps or adate from the document text, if necessary.While our architecture is novel, we are not the firstto modularize timex annotation systems.
Even thor-oughly rule-based systems (Negri and Marseglia,2004; Saquete et al, 2002), separate temporal an-chor tracking from the rest of the normalization pro-cess.
The system of Mani and Wilson (2000) goesfurther in using separate sets of hand-crafted rulesfor recognition and normalization and in separatingout several disambiguation tasks.
Ahn et al (2005b)decouple recognition from normalization?even us-ing machine learning for recognition?and handleseveral disambiguation tasks separately.
In none ofthese systems, though, are context-independent andcontext-dependent processing thoroughly separated,as here, and in all these systems, it is the rules thatdrive the processing?in both Mani et al and Ahnet al?s systems, sets of rules are used to determinewhich timexes need to be disambiguated.4 Component A: Recognizing timexesSystems that perform both recognition and nor-malization tend to take a rule-based approach torecognition (Mani and Wilson, 2000; Saquete etal., 2002; Schilder, 2004; Negri and Marseglia,2004).
Recognition-only systems are often based onmachine learned classifiers (Hacioglu et al, 2005;Bethard and Martin, 2006), although some do usefinite-state methods (Boguraev and Ando, 2005).Ahn et al (2005a) find a benefit to decoupling recog-nition from normalization, and since our goal isto build a modular, trainable system, we take amachine-learning approach to recognition that is in-dependent of our normalization components.Generally, machine learned timex recognitionsystems reduce the task of identifying a timexphrase to one of classifying individual words by us-ing (some variant of) B-I-O tagging, in which eachword is tagged as (B)eginning, (I)nside, or (O)utsidea timex phrase.
Such a tagging scheme is not in-herently sensitive to syntactic constituency and notwell-suited to identifying nested timexes (but cf.
(Hacioglu et al, 2005)).
Considering that syntacticparsers are readily available, we have explored sev-eral ways of leveraging parse information in recog-nition, although we describe here only the methodwe use for experiments later in this paper.We treat timex recognition as a binary phraseclassification task: syntactic constituents are clas-sified as timexes or non-timexes.
We restrict clas-sification to the following phrase types and lexicalcategories (based on (Ferro et al, 2004, ?5)): NP,ADVP, ADJP, NN, NNP, JJ, CD, RB, and PP.2 Inorder to identify candidate phrases and to extract2We include PPs despite the TIDES guidelines, which ex-plicitly exclude temporal PPs such as before Thursday becauseof prepositional modifiers such as around and about.422Identification Exact matchprec rec F prec rec FTEXT 0.912 0.786 0.844 0.850 0.732 0.787DOC 0.929 0.813 0.867 0.878 0.769 0.819BRO 0.973 0.891 0.930 0.905 0.829 0.865BFT 0.976 0.880 0.926 0.885 0.798 0.839Table 1: Recognition results: Identification.parse-based features, we parse the TEXT elementsof our documents with the Charniak parser (Char-niak, 2000).
Because of both parser and annotatorerrors, only 90.2% of the timexes in the training dataalign exactly with a parse, which gives an estimatedupper-bound on recall using this method.We use support vector machines for classification,in particular, the LIBSVM linear kernel implemen-tation (Chang and Lin, 2001).
The features we ex-tract include character type patterns, lexical featuressuch as weekday name and numeric year, a contextwindow of two words to the left, and several parse-based features: the phrase type, the phrase head andinitial word (and POS tag), and the dependency par-ent (and corresponding relation) of the head.As with all our experiments in this paper, wetrain on the TERN training corpus and test on thetest corpus.
Our scores (precision, recall and F-measure for both identification (i.e., overlap) andexact-match) are given in Table 1, along with thescores of the best recognition-only (BRO) and full-task (BFT) TERN 2004 systems.
Since our phraseclassification method is only applied within docu-ment TEXT elements, we also present results usingboth our RE-based document metadata tagger andour phrase classifier for full documents (DOC).
Onlythese scores can be compared with the TERN scores.Our scores using this method approach those ofthe best systems, but there is still a gap, which, aswe see in ?8, affects our overall task performance.5 Component B: Semantic classificationTimexes may refer to points, durations, or recur-rences.
While some timexes refer unambiguously toone of these, many timexes are ambiguous betweentwo or even three of these (see (Hitzeman, 1993) fora theoretical semantic perspective on this ambigu-ity).
Timexes may also refer generically or vaguely,which is another source of ambiguity.While the TIMEX2 standard does not explicitlyspecify semantic classes in its annotations, the se-mantic classes we distinguish for our normalizationsystem can be easily inferred from the form of thevalues of the attributes that are annotated, as follows:Recurrence (recur): SET attribute set to trueGeneric or vague duration (gendur): VAL beginswith PX or PTXDuration: VAL begins with P[0-9] or PT[0-9]Generic or vague point (genpoint): Three possi-bilities: time-of-day w/o associated date expression(VAL begins with T[0-9]); general reference to past,present, or future (VAL is one of the vague tokens);date expression with unspecified high-order position(i.e., millennium position is X)Point: Date expression with specified high-orderposition (may be precise or not?i.e., may include Xat other positions?also may be of any granularity,from millennium down to hundredths of a second).Resolving semantic class ambiguities is a context-dependent task that can be easily factored out of se-mantic interpretation, reducing the burden on the se-mantic interpretation rules.
The classification task isstraightforward: each timex must be classified intoone of the five classes described above or into thenull class (for timexes that have no VAL).
Since theTERN data is not explicitly annotated for semanticclass, we use the class definitions above to derive thesemantic class of a timex from its VAL attribute.We again use the LIBSVM linear kernel for clas-sification, with the same features as for recogni-tion.
Even though some timexes are unambiguouswith respect to semantic class, we train the classi-fier over all timexes, in the expectation that the con-texts of unambiguous timexes will be similar enoughto those of ambiguous timexes of the same class tohelp in classification.
We compare the performanceof our machine learned classifier to a heuristic base-line classifier that uses the head of the timex and thepresence of numbers, names, and certain modifierswithin the timex to decide how to classify it.Table 2 gives the error rates, per class and overall,for the baseline and learned classifiers over phrase-aligned gold-standard timexes.
The machine learnedclassifier halves the error rate of the baseline, mostlyas a result of better performance on the duration andpoint classes.
In ?8, we see how this improvementin classification affects end-to-end performance.Mani and Wilson (2000) and Ahn et al (2005b)423classifier overall null duration .
.
.BL 0.2085 1.0000 0.2534 .
.
.SVM 0.1078 0.4143 0.1507 .
.
.class dist 1290 70 146 .
.
.. .
.
gendur genpoint point recur.
.
.
0.0204 0.1462 0.1322 0.6087. .
.
0.1020 0.1462 0.0496 0.2174. .
.
49 253 726 46Table 2: Error rates: semantic class.also perform limited semantic class disambiguation.Both use machine learned classifiers to distinguishspecific and generic uses of today, and Ahn et alalso use a machine learned classifier to disambiguatetimexes between a point and a duration reading.Their error rate for this task is 27%, but since a setof heuristics is first used to select just ambiguoustimexes, this score cannot be compared to ours.6 Component C: Semantic compositionThe semantic composition module uses context-independent, class-specific rules to compute for eachtimex an underspecified representation?a typedfeature structure that depends on the timex?s seman-tic class (features include unit and value for dura-tions, year, month, date, and referential class forpoints; cf.
(Dale and Mazur, 2006)).
As the rules arenot responsible for identification or class or direc-tion disambiguation, they are fewer in number andsimpler than in other systems (cf.
1000+ in (Negriand Marseglia, 2004)).
Each rule consists of an RE-pattern, which may refer to a small lexicon of names,units, and numeric words, and is applied using a cus-tom transducer.
In total, there are 89 rules; Table 3gives the distribution of rules and an example rulefor each class.
Tokens in ALLCAPS indicate lexicalclasses; tokens in MixedCase indicate other rules;and tokens in lowercase indicate lexical items.7 Temporal anchorsSome point timexes are fully qualified, while othersrequire a reference time, or temporal anchor, to befully normalized.3 There are three ways in whicha temporal anchor is chosen for a timex.
Sometimexes, such as today, three years ago, and nextweek, are deictic and anchored to the time of speech3Our use of the term temporal anchor is distinct from theANCHOR VAL and ANCHOR DIR attributes.class rules exampledur 13 Numeric -?
(UNIT | UNITS)gendur 3 (UNIT | UNITS)genpt 21 (NUM24 | NUMWORD) o ?
clockpoint 31 ?
Approx?
DAYNAME?
MONTHNAME.?
Num31OrRank ,?
YearNumrecur 11 (every | per) Numeric UNITSmisc 10 NUMWORD ((and | -)?
NUMWORD)*Table 3: Distribution of semantic composition rules.
(for us, the document timestamp).
Others, such astwo months earlier and the next week, are anaphoricand anchored to a salient time in discourse, just likean anaphoric pronoun or definite.
The distinctionbetween deictic and anaphoric timexes is not alwaysclear-cut, since many anaphoric timexes, in the ab-sence of an appropriate antecedent, are anchored de-ictically.
A timex may also contain its own anchor:e.g., two days after May 3, whose anchor is the em-bedded anaphoric timex May 3.Once a referential timex?s temporal anchor hasbeen determined, the value of the anchor must becombined with the timex, which may be either anoffset or a name-like timex.
Offsets, such as twomonths earlier, provide a unit u, a magnitude m,and optionally, a direction (before or after); the valueof an offset is the point (of granularity u) that is mu units from its anchor in the indicated direction.Name-like timexes provide a position in a cycle,such as a day name within a week, and optionally,a direction.
The value of a name-like timex is thetime point bearing the name within the correspond-ing cycle of its anchor (or the immediately precedingor succeeding cycle, depending on the direction).For both offsets and name-like timexes, the direc-tion indication is optional.
When no direction in-dication is given, the appropriate direction must bedetermined from context, as in this initial sentencefrom an article from 1998-11-28:(1) A fundamentalist Muslim lawmaker has vowedto stop a shopping festival planned in February,a newspaper reported Saturday.The first timex, February, clearly refers to the Febru-ary following its anchor (the timestamp), while thesecond timex, Saturday, seems to refer to a pointpreceding its anchor (also the timestamp).The next two sections describe our methods fortemporal anchoring and direction classification.4247.1 Component E: Temporal anchor trackingSince temporal anchors are not annotated in theTIMEX2 standard, our system uses a simple heuris-tic method for temporal anchoring (cf.
(Wiebe et al,1997), who use a more complex rule-based systemfor timex anchoring in scheduling dialogues).
Sincewe distinguish deictic and anaphoric timexes duringsemantic composition, we use a combination of twomethods: for deictic timexes, the document time-stamp is used, and for (some) anaphoric timexes, themost recent point timex, if it is fine-grained enough,is used as the temporal anchor (otherwise, the docu-ment timestamp is used).
Because the documents inour corpora are short news texts, we actually treatanaphoric name-like points as deictic and use themost recent timex only for anaphoric offsets.7.2 Component D: Direction classificationThe idea of separating direction classification fromthe remainder of the normalization task is not new.
(Mani and Wilson, 2000) use a heuristic methodfor this task, while (Ahn et al, 2005b) use a ma-chine learned classifier.
In contrast to Ahn et al,who use a set of heuristics to identify ambiguoustimexes and train and test only on those, we trainour classifier on all point and genpoint timexes andapply it to all point timexes.
Genpoint timexes andmany point timexes are not ambiguous w.r.t.
direc-tion, but we expect that the contexts of unambiguoustimexes will be similar enough to those of ambigu-ous timexes of the same class to help classification.Direction class is not annotated as part of theTIMEX2 standard.
Given a temporal anchor track-ing method, though, it is possible to derive imperfectdirection class information from the VAL attribute.We use our anchor tracking method to associate eachpoint and genpoint timex with an anchor and thencompare the VAL of the timex with that of its an-chor to decide what its direction class should be.We again use the LIBSVM linear kernel for clas-sification.
We add two sets of features to those usedfor recognition and semantic classification.
The firstis inspired by Mani et al, who rely on the tense ofneighboring verbs to decide direction class.
Sinceverb tense alone is inherently deictic, it is not suffi-cient to decide the direction, but we do add both theclosest verb (w.r.t.
dependency paths) and its POSclassifier overall after before sameBL 0.1749 0.4587 0.0802 0.1934SVM 0.2245 0.4404 0.1578 0.2305SVM VERB 0.2094 0.3119 0.1631 0.2346SVM ALL 0.1185 0.2110 0.0989 0.1070class dist 726 109 374 243Table 4: Error rates: direction class.tag (as well as any verbs directly related to this verb)as features.
The second set of features compares daynames, month names, and years to the documenttimestamp.
The comparison determines whether,within a single cycle of the appropriate granularity(week for day-names and year for month-names),the point named by the timex would be before, after,or the same as the point referred to by the timestamp.We compare our learned classifier with a heuristicbaseline classifier which first checks for the presenceof a year or certain modifiers such as ago or next inthe timex; if that fails, it computes the date featuresdescribed above for each word in the timex and re-turns same if any word compares to the timestampas same; if that fails, it uses the tense of the nearestverb; and finally, it defaults to same.Table 4 shows the results of applying our clas-sifiers to all phrase-aligned gold-standard pointtimexes.
BL is the baseline; SVM, SVM VERB, andSVM ALL are the classifiers learned using our basicfeature set, the basic feature set plus the verb fea-tures, and all the features, respectively.
The learnedclassifier using all the features reduces the error rateof the baseline classifier by about a third.
Note,though, that the learned classifiers without the datecomparison features (SVM and SVM VERB) performsubstantially worse than even the baseline.
One rea-son for this becomes clear from Table 5, which givesthe error rates for the classifiers restricted to timexesconsisting solely of a month or a day name.
Unlikepoints in general, these timexes are all ambiguouswith respect to direction and are, in fact, the primarymotivation for both Mani et al and Ahn et al to con-sider direction classification as a separate task.These results demonstrate that the date compari-son feature is responsible for a substantial reductionin error rate (over 85% from SVM to SVM ALL) andthat for the same class, performance is perfect.
Thisis largely due to the writing style of the documents,in which the current day is often referred to by name425classifier overall after before sameBL 0.1000 0.4348 0.1061 0.0000SVM 0.3647 0.6087 0.3485 0.3086SVM VERB 0.3176 0.3478 0.3485 0.2840SVM ALL 0.0529 0.1304 0.0909 0.0000class dist 170 23 66 81Table 5: Error rates: direction month/day.instead of as today, as in example (1).Although both Mani et al and Ahn et al builddirection classifiers, neither provide comparable re-sults.
Mani et al do not evaluate their directionheuristics at all, and Ahn et al train and test theirmachine learned classifier only on timexes deter-mined to be ambiguous by their heuristics.
In anycase, their error rate is significantly higher, at 38%.8 End-to-end performanceWe now consider the performance of the entire sys-tem and the contributions of the components.
First,though, we discuss our evaluation metrics.8.1 ScoringThe official TERN scoring script computes precisionand recall for VAL only with respect to correctly rec-ognized TIMEX2s with a non-null VAL.
While thismay be useful in determining how far behind nor-malization is from recognition for a given system, itdoes not provide an accurate picture of end-to-endsystem performance, since the recall base does notinclude all possible timexes and the precision basedoes not include incorrectly recognized timexes.The scoring script provides several raw countsthat can be used to compute measures that are moreindicative of end-to-end performance: actTIMEX2(# of actually recognized TIMEX2s); corrTIMEX2(# of correctly recognized TIMEX2s); posVAL(# of correctly recognized TIMEX2s with a non-null gold VAL); corrVAL (# of correctly recog-nized TIMEX2s with a non-null gold VAL forwhich the system assigns the correct VAL); andspurVAL (# of correctly recognized TIMEX2s withnull gold VAL for which the system assigns aVAL).
With these counts, we can define corrNOVAL(# of correctly recognized TIMEX2s with a nullgold VAL for which the system assigns a nullVAL), as corrTIMEX2 ?
posVAL ?
spurVAL.
Wethen define end-to-end precision (absP) and recall(absR) as (corrVAL + corrNOVAL)/actTIMEX2and (corrVAL+corrNOVAL)/possTIMEX2, respec-tively.
Official precision and recall for VAL are com-puted as corrVAL/actVAL and corrVAL/possVAL.8.2 ResultsOur first set of results (Table 6(Top)), which arerestricted to timexes in document TEXT elements,compares our system (LLL) to a version of our sys-tem (BL) that uses the baseline classifiers for seman-tic and direction class.
It also presents a series of or-acle results that demonstrate the effect of swappingin perfect classification for each of the learned clas-sifiers.
The oracle runs are labeled with a three-lettercode in which the first letter ((P)erfect or (L)earned)refers to phrase classification; the second, to seman-tic classification; and the third, to direction classi-fication.
Note: perfect phrase classification is notthe same as perfect recognition, since it excludestimexes that fail to align with parsed phrases.Using the learned classifiers (LLL), which reduceerror rates by about one-half for semantic class andone-third for direction class over the baseline clas-sifiers, results in a five-point improvement in abso-lute F-measure over the baseline system (BL).
Wealso see from runs LLP, LPL, and LPP that furtherimprovement of these classifiers would substantiallyimprove end-to-end performance.
Finally, we seefrom runs PLL and PPP that recognition performanceis a major limiting factor in our end-to-end scores.In Table 6(Bottom), we present results over fulldocuments, including metadata and text.
LLL andPLL are the same as before; ITC-IRST is the sys-tem of (Negri and Marseglia, 2004), which achievedthe highest official F-measure in the TERN 2004evaluation.
The results of our system (LLL) arecomparable to those of ITC-irst: because we recog-nize fewer timexes, our official F-measure is higher(0.899 vs. 0.872) while our absolute F-measure islower (0.769 vs. 0.806).
We see from run PLL thatour recognition module is largely to blame?withperfect phrase classification for recognition, our nor-malization modules produce substantially better re-sults.
With a system such as ITC-irst?s, it is not pos-sible to separate recognition performance from nor-malization performance, since there is a single rulebase that jointly performs the two tasks?all normal-izable timexes are presumably already recognized.426System corrVAL corrNOVAL actTIMEX2 P R F absP absR absFBL 859 32 1245 0.813 0.787 0.800 0.716 0.624 0.667LLL 931 33 1245 0.882 0.853 0.867 0.774 0.676 0.722LLP 938 33 1245 0.912 0.859 0.885 0.780 0.680 0.727LPL 951 39 1245 0.916 0.871 0.893 0.795 0.694 0.741LPP 987 39 1245 0.951 0.904 0.927 0.824 0.719 0.768PLL 1008 63 1287 0.886 0.828 0.856 0.832 0.751 0.789PPP 1097 70 1287 0.966 0.901 0.932 0.907 0.818 0.860LLL 1285 33 1601 0.910 0.887 0.899 0.823 0.721 0.769PLL 1362 63 1643 0.912 0.866 0.888 0.867 0.780 0.821ITC-IRST 1365 35 1648 0.875 0.870 0.872 0.850 0.766 0.806Table 6: Performance on VAL.
(Top): TEXT-only.
(Bottom): full document.9 ConclusionWe have described a novel architecture for a timexannotation system that eschews the complex setof hand-crafted rules that is a hallmark of othersystems.
Instead, we decouple recognition fromnormalization and factor out context-dependent se-mantic and pragmatic processing from context-independent semantic composition.
Our architec-ture allows us to use machine learned classifiers tomake context-dependent disambiguation decisions,which in turn allows us to use a small set of sim-ple, context-independent rules for semantic compo-sition.
The normalization performance of this sys-tem is competitive with the state of the art and ouroverall performance is limited primarily by recog-nition performance.
Improvement in semantic anddirection classification will yield further improve-ments in overall performance.
Our other plans forthe future include experimenting with dependencyrelations for semantic composition instead of lexi-cal patterns, evaluating our temporal anchor trackingmethod, and training the full system on other cor-pora and adapting it for other languages.Acknowledgement This research was supportedby the Netherlands Organization for Scientific Re-search (NWO) under project numbers 017.001.190,220-80-001, 264-70-050, 354-20-005, 600.065.120,612-13- 001, 612.000.106, 612.066.302,612.069.006, 640.001.501, 640.002.501, andby the E.U.
IST programme of the 6th FP for RTDunder project MultiMATCH contract IST-033104.ReferencesD.
Ahn, S. Fissaha Adafre, and M. de Rijke.
2005a.
Extractingtemporal information from open domain text: A comparativeexploration.
In R. van Zwol, editor, Proc.
DIR?05.D.
Ahn, S. Fissaha Adafre, and M. de Rijke.
2005b.
Recog-nizing and interpreting temporal expressions in open domaintexts.
In S. Artemov et al, editors, We Will Show Them:Essays in Honour of Dov Gabbay, Vol 1, pages 31?50.S.
Bethard and J.H.
Martin.
2006.
Identification of event men-tions and their semantic class.
In Proc.
EMNLP 2006.B.
Boguraev and R. Kubota Ando.
2005.
TimeML-complianttext analysis for temporal reasoning.
In Proc.
IJCAI-05.C.-C. Chang and C.-J.
Lin, 2001.
LIBSVM: a library for sup-port vector machines.
Software available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.E.
Charniak.
2000.
A maximum-entropy-inspired parser.
InProc.
NAACL 2000, pages 132?139.R.
Dale and P. Mazur.
2006.
Local semantics in the interpre-tation of temporal expressions.
In Proc.
Workshop on Anno-tating and Reasoning about Time and Events, pages 9?16.L.
Ferro, L. Gerber, I. Mani, and G. Wilson, 2004.
TIDES 2003Std.
for the Annotation of Temporal Expressions.
MITRE.L.
Ferro.
2004.
Annotating the TERN corpus.http://timex2.mitre.org/tern 2004/ferro2 TERN2004 annotation sanitized.pdf.K.
Hacioglu, Y. Chen, and B. Douglas.
2005.
Automatic timeexpression labeling for English and Chinese text.
In A.F.Gelbukh, editor, CICLing, volume 3406 of Lecture Notes inComputer Science, pages 548?559.J.
Hitzeman.
1993.
Temporal Adverbials and the Syntax-Semantics Interface.
Ph.D. thesis, University of Rochester.ISO.
1997.
ISO 8601: Information interchange ?
representa-tion of dates and times.I.
Mani and G. Wilson.
2000.
Robust temporal processing ofnews.
In Proc.
ACL?2000, pages 69?76.I.
Mani, J. Pustejovsky, and R. Gaizauskas, editors.
2005.
TheLanguage of Time: A Reader.
Oxford University Press.M.
Negri and L. Marseglia.
2004.
Recognition and normaliza-tion of time expressions: ITC-irst at TERN 2004.
Technicalreport, ITC-irst, Trento.E.
Saquete, P.
Mart?
?nez-Barco, and R. Mun?oz.
2002.
Recogniz-ing and tagging temporal expressions in Spanish.
In Work-shop on Annotation Standards for Temporal Information inNatural Language, LREC 2002, pages 44?51.F.
Schilder.
2004.
Extracting meaning from temporal nounsand temporal prepositions.
ACM TALIP.J.
Wiebe, T. O?Hara, K. McKeever, and T. O?hrstro?m Sandgren.1997.
An empirical approach to temporal reference resolu-tion.
In Proceedings of EMNLP-97, pages 174?186.427
