Combining Linguistic with Statistical Methods inAutomatic Speech UnderstandingPatti PriceSRI International333 Ravenswood AvenueMenlo Park, California 94025pprice@speech.sri.comIntroductionThis paper presents an overview of automatic speechunderstanding techniques that combine knowledge-based approaches with statistical pattern matchingmethods.
Such an approach requires amultidisciplinaryoutlook, addressing both cultural and technical differ-antes among the various component technologies.As argued in Price and Ostendorf (1994), the repre-sentatives of knowledge-based approaches and of ap-proaches based on statistical pattern matching mayview each other with suspicion - -  if they are awareof each other's work.
Psychologists and linguists, rep-resenting the knowledge-based approaches, may viewautomatic algorithms as "uninteresting collections ofadhoc ungeneralizable methods for limited domains."
Theautomatic speech recognition community, on the otherhand, may argue that automatic speech recognitionshould not be modeled after human speech recognition;since the tasks and goals of machines are very differentfrom those of humans, the methods hould also be dif-ferent.
Thus, in this view, knowledge-based approachesare "uninteresting collections of ad hoc ungeneralizablemethods for limited domains."
The two sides may use~he same words, but mean different hings, as indicatedill the glossary in the table.Spoken language is a social mechanism evolved forcommunication among entities whose biological prop-erties constrain the possibilities.
Therefore the mech-anisms that are successful for machines are likely toshare many properties with those successful for people.Further, in automatic spoken language applications, atleast one human being is typically involved.
Thus, theImderstanding of human communication may be essen-tim for generalizable methods robust to the variabilitymaaifested by humans.
Just as engineers could gainfrom a better uuderstanding of human mechanisms,psychologists and linguists could gain from a betterunderstanding of automatic techniques.
For example,these techniques can be viewed as theories of humancommunication made explicit enough to test.
Studyingwhere the techniques work and where they fail couldsh~,d light on the human communication process.
AI-r.hough differences in training, techniques, approaches,aninterestingad hocungeneralizableprovides noexplanation ofcognitiveprocesses.withouttheoreticalmotivation.
"techniquesthat help youclimb a treemay not helpyou get tothe moon.
"ENGINEERSprovides no usefulapplications.must be providedby hand.expense ofknowledgeengineeringprohibitsassessing newor morecomplex domains.goals, and culture have inhibited multidisciplinary col-laboration, there is much to gain from the multidisci-plinary approach.
As this paper will argue, combiningknowledge and techniques from the two communitiescan yield results that neither community alone couldachieve.Automat ic  Speech  Unders tand ing ,Genera l  I ssuesActivity and results in automatic speech understandinghave increased in recent years, largely because of the"arranged marriage" by a DARPA (Defense AdvancedResearch Projects Agency; now ARPA) program man-ager of two previously independent programs: speechrecognition and natural language understanding.
Thespeech recognition program was focussed on the auto-matic transcription of speech, while the natural lan-guage understanding program was focussed on inter-preting the meanings of typed input.
While there arepsychological nd scientific reasons to integrate thesetwo areas, there are technical and cultural reasons fortheir past and present degree of separation.In the ARPA speech understanding program of the1970s (see, e.g., Klatt's 1977 survey), artificial intelli-gence (AI) was a relatively new field with much promise.76Expert systems based on speech and language knowl-edge were developed by separating knowledge sourcesalong traditional linguistic divisions: e.g., acoustic pho-netics, phonology, morphology, lexical access, syntax,semantics, discourse.
Each module had well-defined in-puts and evaluation criteria.
A key weakness of theapproach, however, turned out to be the number ofmodules and the decision-making process.
Each mod-ule may have done fairly well as assessed independently,but when each module was forced to make irrevocabledecisions without interaction with other modules, er-rors could only propagate; a seven-stage serial processin which each module is 90% accurate has an overall ac-curacy of less than 50%.
As statistical pattern matchingtechniques appeared to perform much better with muchless research investment than did the knowledge-basedapproaches, the funding focus shifted.Although knowledge-based approaches and statisti-cally based approaches were espoused by people in al-ready largely nonoverlapping communities, these his-torical events led to larger separation.
The "knowl-edge" proponents argued that the statistical methodsonly worked accidentally in the short term, on this lim-ited task.
The "statistics" proponents assumed thatthey had been misled by the promise of AI and thatthey had little to learn from the "knowledge" group.The two communities seemed to come to an agreementthat they were working on different problems and hadlittle to say to or ask of each other.The field of natural anguage understanding cameto be populated largely by "computational linguists"trained in AI techniques in computer science depart-ments for the most part, while speech recognition cameto be populated mostly by engineers.
The methods,goals, evaluation criteria, background assumptions, andcultures of these two communities are quite different.
Infact, a basic disagreement persists over what counts asscience.
The "knowledge" side values argumentationstyle, ideas, and long-term research.
In their view, the"statistical" side is not scientific because it representsmere engineering "tweaks."
The "statistical" side val-ues measurable results.
In their view, the "knowledge"side is not scientific because it does not measure results(insofar as the long term tends never to come).In fact, however, the cause of the differences in per-formance between the two approaches during the 70s islikely to be an insight of value to both sides: makinghard (irrevocable) decisions early, i.e., before consider-ing more knowledge sources, can degrade performanceseverely.
It happened that statistical models provided amechanism that enabled delayed decision making, andsubsequent hardware and algorithmic developments en-abled for the consideration of increasingly larger sets ofhypotheses.
The remainder of this paper will surveythe techniques used in combining linguistic with statis-tical analyses, the issues of interest, and recent resultsin speech recognition, natural language understanding,and their integration.Speech  Recogn i t ionFor several years, the best performing speech recog-nition systems have been based on statistical patternmatching techniques (Pallett et al 1990, Pallett 1991,Pallett et al 1992, Pallett et al 1993, Pallett etal.
1994).
These models are constrained to variousdegrees by "knowledge" about speech and language(e.g., the topology of the models, the units modeled,the pronunciations modeled, etc.).
The most com-monly used method is probably hidden Markov mod-els (HMM)  (see, e.g., Bahl et al 1983, Rabiner 1989,Picone 1990).
There is also much current work us-ing other pattern matching techniques (see, e.g., Os-tendorf and Roukos 1989, Zue et al 1992), includingneural network-based approaches (see e.g., Hampshireand Weibel 1990) and hybrid HMM/neural network ap-proaches (see e.g., Abrash et al 1994).
One can thinkof the models as representing the linguistic or otherknowledge (e.g., what are the units?
how are they de-termined?
what aspects need to be represented explic-itly?
what features will represent the data?).
The pa-rarneters can then be estimated automatically, giventhe data and the constraints embedded in the model,to model our "ignorance" ~ those aspects we can't ordon't want to model explicitly.A Markov model represents the probabilities of se-quences of units, e.g., words or sounds.
The "hidden"Markov model, in addition, models the uncertainty ofthe current "state".
By analogy with speech produc-tion, and using phones as states, the mechanism canbe thought of as modeling two probabilities associatedwith each phone: the probability of the acoustics giventhe phone (to model the variability in the realizationof phones), and the probability of transition to anotherphone given the current phone.
Though some HMMsare used this way, most systems use states that aresmaller than a phone (e.g., first, middle, and last partof a phone).
Such models have more parameters, andhence can provide greater detail, which can be used tobetter model duration and context effects.
Adding skipsand loops to the states can model the temporal vari-ability of the realization of phones.
Given the model,parameters are estimated automatically from a corpusof data.
Thus models can be "tuned" to a particular(representative) sample.In this example, the words, phones, and states cho-sen for the model are units that can be manipulatedsymbolically (for example, in a set of rules for gener-ating pronunciations based on base forms provided bya dictionary), and they may be theory-driven, data-driven, or some combination.
The limited use of lin-guistic theory in deriving these symbolic components isprobably largely a function of the cultural dii~cultiesdiscussed above.
Bridging the cultural gap will requirecloser collaboration of the two communities: linguistsformulating theory in ways that these methods can use,engineers formulating methods that can better capturethese knowledge sources.77Typically the HMMs for phones are conditioned onthe context (of the previous and/or following phone orphone-class or word, for example).
Knowledge of speechand linguistics can provide the choice of the phone set(e.g., Are flap and stop realizations o f / t /mode led  asone unit or two?
), the topology of the models (e.g., Howmany states per phone model?
Do longer phones havemore states, or are loops on certain states sufficient?
),and the number of units (e.g., Will phones be modeledas a function of their context?
If so, what is the natureof the conditioning context?
).Individual HMMs for phones can be concatenatedto model words.
Linguistic knowledge, perhaps in theform of a dictionary, determines the sequence of phonesthat make up a word.
Linguistic knowledge in the formof phonological rules can be used to model possible vari-ations in pronunciation, such as the flap or stop realiza-tion of/t/.
For computational efficiency (at the expenseof storage), additional pronunciations can be added tothe dictionary.
This solution is not ideal for the lin-guist, since different pronunciations of the same wordare treated as totally independent even though theymay share all but one or two phones.
It is also not anideal solution for the engineer, since there may be manymore parameters to estimate, and recognition accuracymay be lost depending on the implementation, sincewords with more pronunciations may be disfavored rel-ative to those with few pronunciations.
The work ofCohen (e.g., Cohen et al 1987, Cohen 1989) and others(sce, e.g., Withgott and Chen 1993) attempts to addresssome of these issues, but solutions are not simple andsignificant performance gains have been hard to comeby.
Perhaps as researchers are forced to deal with morespontaneous speech effects (as opposed to read speechand highly planned "push-to-talk" speech), these dif-ficult issues will force engineers and linguists to worktogcther to find better solutions.Linguistic knowledge may also be used to model theeffects of lexical stress on vowels.
The number of mod-els for vowels could be simply doubled: each vowel hasa model representing lexical stress and no lexical stress.Although this captures the linguistic knowledge thatlexical stress has an acoustic effect on a vowel, it isnot linguistically elegant in that it models the effectsof lexical stress on each vowel as independent.
The lin-guist dislikes the solution because it does not capturea generalization about the effect of lexical stress acrossvowels.
The engineer dislikes it because it doubles thenumber of vowels to model and may not be worth slightgains in performance.
This is another example in whichthe structure of the model constrains how people thinkabo,t a problem, and in which linguistic and engineer-i,g expertise are needed to arrive at a solution.Modeling utterances the way words were modeled,i.e., a dictionary of all "possibilities," would be evenmore impossible than it is for words.
A list of allpossible utterances and their component words would,prickly exhaust our resources.
However, for limited ap-plications, this solution can be used to simulate contin-uous speech (i.e., simply model "words" that are verylong - -  namely, long enough to be utterances).
A sim-ple (and generally much cheaper) approach is to modelall the words in parallel and add a loop from the end tothe beginning, where one of the "words" is the "end-of-sentence" word so that the sentences are not infinitelylong.
Of course, this simple model has the disadvan-tage of assuming that the ends of all words are equiva-lent (the same state).
This model assumes that at eachpoint in an utterance, all words are equally likely, whichwe know is not true of any language.
Sequences of wordscan be modeled by concatenating the word models andestimating the probability of different word sequences.The Markov model (minus the hidden part) estimatesthe likelihoods of words given the previous word (orN words), based on a training corpus of sentence tran-scriptions.
In this example, little linguistic knowledge isused except he intuition that some sequences are morelikely than others.
That intuition is difficult to call "lin-guistic" insofar as many linguists work exclusively withgrammars in which sentences are either grammatical ornot.
Though there may be some recognition of doubt-ful cases, grammaticality is typically a binary decision.Statistical modeling of linguistically relevant relation-ships is a growing area of interest, though there remainsignificant echnical and cultural challenges.A survey of recent speech recognition papers revealsthe engineering bias (and relative lack of linguistic mo-tivation) in much recent work.
Although the majorissues facing speech recognition research today includeboth symbolic and statistical aspects, effective use ofboth aspects will require increased bridging of the cul-tural gaps between linguists and engineers.
Examplesof current speech recognition issues include:?
Features.
The raw speech waveform needs to be digi-tized for analysis, and if it is simply sampled in timeand amplitude, there is far too much data to han-dle directly; some feature extraction is needed.
Theoutput of feature extraction (sometimes called the"front-end") is the input to the recognition search(which can be based on HMMs, neural nets, or onsome other technique).
The most common featuresextracted are cepstral coefficients (derived from aspectral analysis), and derivatives of these coeffi-cients.
Sporadically, and especially with reference tonoise robustness, there has been interest in improvingfront-ends and in auditory modeling.
Little work hasbeen done since the 70s, however, in modeling lin-guistically motivated features (e.g., high, low, front,back).
Explicit detection of these features has provedchallenging.
However, a representation of phones interms of a smaller set of features would have severaladvantages: fewer parameters could be better esti-mated, given a fixed corpus; phones that are rare orunseen in the corpus could be estimated on the basisof the more frequently occurring features that com-pose them; and since 5.;itures tend to change more78slowly than phones, it is possible that sampling intime could be less frequent.?
Distributions.
In the HMM formulation, the stateoutput distributions have been a topic of research in-tersest recently.
The issue under debate has been theuse of discrete state distributions, continuous distri-butions, or (currently the most popular) a mixtureof Gaussian distributions (see, e.g., Digalakis andMurveit 1994).
Little role for linguistic knowledgeis apparent in this work.?
Model Inventory.
An area of growing interest re-cently is the choice of units to model.
Many systemssimply model phones, or phones conditioned on thesurrounding phonetic context.
Other systems, how-ever, claim improved performance through the selec-tion of units or combination of units determined au-tomatically or semiautomatically (see, e.g., Bahl etal.
1991).?
Language Modeling.
Any method that can be usedto constrain the sequences of occurring words can bethought of as a language model.
Traditional gram-mars are, for example, a type of language model, butso are the Markov models (N-grams) that model onlylocal constraints.
As mentioned above, it is a ripearea of research.
The goal is to develop languagemodels that can be created easily and can improvespeech recognition performance by modeling linguis-tically motivated attributes (for example, numberagreement of subject and verb; or co-occurrences ofadjectives with nouns, which may be an arbitrarynumber of words away from each other) rather thanthe accidents of word sequences typically estimatedby Markov models.?
Adaptation.
The first speech recognition systemstended to be speaker dependent (before using a sys-tem, a person had first to read a list of words or sen-tences).
In recent years, the trend has been towardspeaker independence.
Speaker-independent systemscan work reasonably well for a variety of talkers, butthe broader coverage of talker types, dialect types,and so on, the more fuzzy the models.
The futureis likely to be in speaker adaptation: the system be-gins as a speaker-independent system and graduallyadapts to the characteristics of a new speaker.
Thisapproach is not unlike linguistic experience in whichnew dialects may be difficult to understand at first.In a foreign language it may be easier to observeadaptation to individual speakers.?
Search.
Given the acoustic models, the languagemodels, and the input speech, the role of the rec-ognizer is to search through all possible hypothe-ses and find the best (most likely) string of words.As the acoustic and language models become moredetailed they become larger, and this can be anenormous task, even with increasing computationalpower.
Significant effort has been spent on managingthis search: depth-first vs. breadth first, beam search(which prunes hypotheses if they are enough b~lowthe best-scoring hypothesis), and, recently, variousschemes for making multiple passes using coarsermodels at first to narrow the search and progressivelymore detailed models to further narrow tile prunedsearch space (see, e.g., Murveit et al 1993, Nguyellet al 1993).?
Robustness.
Robustness is a key area of research.Systems can be developed that function well in nar-row contexts, but to be useful in a wide range of appli-cations, they need to be robust o the variability thatoccurs in speech communication: variability due todifferences in talkers, speech styles, microphone andnoise conditions, dialect, and language.
This is anarea in which the forcing function of hard problemsmay help to bridge the cultural gaps, as engineersrealize that narrowing the solution decreases robust-ness and requires the more general solution sought bylinguists and speech scientists.?
Portability.
Portability is another key area of re-search.
Creating a demonstration i a limited do-main may give the feel of accomplishment, but sci-ence (and applications) demand generalizability andreproducibility.
We cannot imitate the range of hu-man capability with speech recognition systems, butwe can create useful applications in limited domains.The amount of work we have done for one task thatcan be reused in another task is a measure of howmuch we have learned about speech generally.
(Aswill be argued in a later section, linguists and speechscientists also need to assess the portability of theirknowledge.)?
Scalability.
In an environment in which computcrpower is rapidly changing (increasing power on plat-forms of the same size, and decreased power on eversmaller platforms), another key issue is scalability:the capability of using increased memory and com-putational power for faster, more accurate recogni-tion on the one hand, and the capability of grace-fully degrading on platforms with less memory andcomputational power.
Although linguistic and speechknowledge, as suggested in the examples above, canhelp form more efficient representations, scalability isnot really a linguistic issue.The gap between speech scientists and speech recog-nition engineers has meant hat some aspects of speechhave had to be discovered independently.
Many cog-nitive models appear to be more continuous than theyused to be, and are looking a bit more like the statisti-cal models than was previously true.
The gap betweenthe two areas has meant hat many speech researchershave not been able to take advantage of statistical toolsthat could help them advance their knowledge.
It hasalso meant hat advances in speech research and mod-els of cognition have not been able to affect automaticspeech recognition.
For example, the notion of a pro-totype and distance from a prototype (see, e.g., Mas-79saro 1987, Kuhl 1990) which seems to explain muchdata from speech perception (and other areas of percep-tion), is not well modeled in the current speech recog-nition frameworks.
A person who has not been wellunderstood tends to change the speech style so as tobe better understood.
This may involve speaking moreloudly or more clearly, changing the phrasing, or per-haps even leaving pauses between words.
These changesmay help in human-human communication, but in typ-ical human-machine interactions, they result in formsthat are even more difficult for the machine to inter-pret.
The concept of a prototype in machine recogni-tion could lead to more robust recognition technology.That is, the maximum-likelihood approaches com-mon in speech recognition miss a crucial aspect of lan-guage: the role of contrast.
A given linguistic entity(e.g., phone) is characterized not just by what it is, butalso by what it is not, i.e., the system of contrast inwhich it is involved.
Thus, hyperarticulation may aidcommunication over noisy phone lines for humans, butmay decrease the performance of recognizers trained ona corpus in which this style of speech is rare or miss-ing.
The results can be disastrous for applications, sincewhen a recognizer misrecognizes, a common reaction isto hyperarticulate (Shriberg et al1992).
Discrimina-tive systems, such as neural network formulations, havean advantage over maximum-likelihood approaches inthis respect, though it is an area in which linguists andspeech perception experts could play a larger role.Although things are changing rapidly, and many fac-tors will affect just how well a system will perform, ex-amining recent benchmark evaluations can give an ideaof the relative difficulty of various aspects of speech (seee.g., Pallett et al 1994).
These areas could be thosein which increased linguistic knowledge could improveperformance.
For example, the variance across the talk-ers used in the test set was greater than the variance-~cross the systems tested.
Further, the various systemstested had the highest error rates for the same threetalkers who were the fastest talkers in the set.
Theseobservations could be taken as evidence that variabil-ity in pronunciation, at least insofar as fast speech isconcerned, may not currently be well modeled.
Furtherevidence of the need for better modeling of the pronun-.~iation variation observed in spontaneous speech arisesfrom the degradation in recognition accuracy observedin moving from read speech or carefully planned speechto normal, conversational speech.Natura l  Language Unders tand ingThe field of natural language (NL) understanding hasbeen traditionally populated by computational lin-guists, trained in artificial intelligence, largely in com-puter science departments.
The approaches have tra-ditionally been based in symbolic logic, using expert-systems techniques typically involving large sets ofhaIid-crafted rules.
The arranged "marriage" withspeech recognition has resulted in a great increase inthe use of statistical methods for automatically creat-ing natural anguage components, or for automaticallytraining their parameters.
Since the first joint meet-ing of the speech and natural language communitiesin 1989, the number of papers and the range of topicsaddressed using statistical methods have steadily in-creased.
At the most recent meeting (March 1994), thecategory of statistical language modeling and methodsreceived the most abstracts and was one of the mostpopular sessions.The issues of concern in natural language researchare largely determined by the interests of those doingthe research, and at present hey tend to be compu-tational inguists.
However, as argued above, there isa growing tendency to combine knowledge-based withstatistical/engineering approaches.
Based on recent pa-pers, topics of major concern include:?
Lezicon.
Although speech recognition componentsusually use a lexicon, lexical tools in natural lan-guage are more complex than lists of words andpronunciations.
Different formalisms store differenttypes and formats of information, including, for ex-ample, morphological derivations, part-of-speech in-formation, and syntactic and semantic constraints oncombinations with other words.
There is little evi-dence, however, in most of these representations thatsome structures are more likely than others.
* Grammar .
A grammar is typically a set of rules de-vised by observation of occurring patterns in a lan-guage or sublanguage.
Typically, grammars eitheraccept a sentence or reject it, although grammarsthat degrade more gracefully in the face of sponta-neous speech and recognition errors are being de-veloped (see, e.g., Hindle 1992).
Another issue ofrelevance is the development of grammars that canbe used either for analysis (parsing) or for genera-tion.
This should become increasingly important asmachines play a more active role in human-machinecollaboration.?
Robustness.
Robustness has been a major issue inrecent years in natural language.
The traditionalcomputational linguistic approach of covering a setof linguistically interesting examples was put to a se-vere test in the attempt o cover, in a limited domain,the set of utterances produced by people engaged inproblem-solving tasks.
Several new sources of com-plexity were introduced: the move to an empiricallybased approach (covering a seemingly endless num-ber of "simple" things became more important hancovering the "interesting," but more rare, complexphenomena), the separation of test and training ma-terials (adding rules to cover phenomena observed inthe training corpus may or may not affect coverageon an independent test corpus), the nature of spon-taneous peech (which has a different, and perhapsmore creative, structure than written language, pre~viously the focus of much NL work), and recovery80from errors that can occur in recognition.?
Parsing.
The goal of parsing is to retrieve or as-sign a structure (based on the grammar used) to astring of words for use by a later stage of processing.Typically, parsers have worked deterministically ona single string of input.
When parsers were facedwith typed input, aside from the occasional typo,the intended words were not in doubt (though theirparts of speech or syntactic role might have been).When speech is the input, however, speaker disflu-encies, novel syntactic onstructions and recognitionerrors pose serious difficulties for traditional parsers.?
Interpretat ion.
Interpretation may or may not beseparated from parsing.
Typically, however, pars-ing is faster than interpretation and narrows the fieldconsiderably for the interpretation stage.
Interpreta-tion is the stage at which a representation f meaningis constructed.
Of course, this representation is notof much use without a "back-end" that can use therepresentation to perform an appropriate response,e.g., retrieve a set of data from a database, ask formore information, etc.
This stage is typically purelysymbolic, though likelihoods or scores of plausibilitymay be used.?
Portabi l i ty.
Portability has been less of a researcharea in NL than in speech recognition, largely be-cause many of the methods used are so costly (datacollection for speech recognition can be costly as well,but it may be argued that it can be done by non-experts).
The portability issue can be expected togrow in importance in NL work.
Automatic acquisi-tion and automatic tuning of parameters are alreadygrowing areas of research, representing the impactof cross-disciplinary fertilization (see, e.g., the recentARPA Human Language Technology Workshop pro-ceedings).?
Scalabil ity.
As for speech research, scalability be-comes increasingly an issue as the technology be-comes appropriate for technology transfer.
Even fordemonstrations of feasibility, it can be important odevelop algorithms that run quickly enough on a plat-form small enough to be widely available.The combining of traditionally linguistic or AI ap-proaches with statistical modeling techniques, as al-ready mentioned, is more or less involved in all theissues just outlined.
Although difficult, such cross-disciplinary work still holds much promise for futureadvances.
Recent trends in ARPA proceedings papersindicate that new uses of statistics in NL areas far out-number new uses of linguistics in speech recognition.Perhaps the difficulties posed by conversational sponta-neous speech will cause engineers to take another lookat linguistics.Results in natural anguage understanding have beenmore resistant o quantification than those in speechrecognition (where there is fairly good agreement onthe string of words produced).
What does it mean tohave understood properly?
Can there be more than oneway to understand properly?
In the ARPA community,these hard questions have been postponed somewhatby agreeing to evaluate on the answer returned froma database.
Trained annotators examine the string ofwords (NL input) and use a database extraction toolto extract the min imum and max imum accepted set oftuples from the evaluation database.
A "comparator"then automatically determines whether a given answeris within the min imum and max imum allowed.The community is not, however, content with the cur-rent expense and limitations of the evaluation methoddescribed above, and is investing significant resourcesin finding a better solution.
Key to much of the debateis the cultural gap: engineers are uncomfortable withevaluation measures that cannot be automated (forget-ting the role of the annotator in the current process);and linguists are uncomfortable with evaluations thatare not diagnostic; and, of course, neither side wantssignificant resources to go to evaluation that would oth-erwise go to research.Integration of Speech Recognition andNatural Language UnderstandingThe integration of the two technologies outlined illthe previous sections seems to be a natural connec-tion.
Nonetheless, the two communities were distinctenough that, except for the funding impetus, the cou-pling might not have happened.
Many researchers inboth communities would agree however, that the in-tegration effort has been good for both.
To naturallanguage understanding, speech recognition can brillgprosodic information, information important for syntaxand semantics but not well represented in text.
NL canbring to speech recognition several knowledge sources(e.g., syntax and semantics) not previously used (N-grams model only local constraints, and largely ignoresystematic constraints such as number agreement).
Forboth, the integration affords the possibility of manymore applications than could otherwise be envisioned,and the acquisition of new techniques and knowledgebases not previously represented.One of the main lessons of the ARPA speech under-standing project of the 1970s was that considering allknowledge sources before making hard decisions was abig win.
In speech recognition, tighter integration hasconsistently led to improved performance.
However,technical and cultural differences inhibit such tight in-tegration.Technically, as coverage increased, language modelstended to grow and required either increasingly largeamounts of storage (possibly infinite, if all rules wereto be compiled, and some rules had infinite loops), orincreasingly large amounts of computation (if an inter-pretive approach was envisioned).
Thus, the prospect ofNL guiding the recognition search became nearly hope-less; rather, it appeared that speech output was neededto guide the NL search if the task was to be done quickly81enough.
Further, increased coverage also meant thatN L grammars provided less constraint, and constraintc,f a type that made it difficult to prune recognitionhypotheses early in the speech stream.
Although Iong-distance constraints like this were desired, the solutionsresulted in too many active speech hypotheses to bemanageable.The cultural difficulties have already been outlined,and as technical difficulties ettled in, there was a ten-dency to take the easy way out and settle into the ar-ranged marriage with separate bedrooms: a strictly se-rial approach defined the turf, and each side focussed onimproving its own technologies, though with some ex-posure to the techniques and culture of the other side.In this climate of compromise, the N-best integrationapproach became popular.
In this approach, the con-nection between the two components i strictly serial,but the hard-decisions-early issue is softened by sendingnot.
just the best hypothesis from speech recognition,but the N-best (where N may be on the order of 10 to100).
Tile NL component can then score this set forgrammaticality (where in some cases the "score" is justa I for "grammatical" or a 0 for "not grammatical"),and combine the acoustic with the grammar score todetermine the best-scoring hypothesis.
This approachis computationally tractable, and accomodates greatmodularity of design (different speech and NL modulescan be swapped in and out).
Examples of N-best in-terfaces include: Veilleux and Ostendorf 1993, Rayner1994.The limitations in the N-best integration are relatedto the modules: (1) if the speech module is not veryaccurate, N may have to be very large to ensure thatthe correct string is included; this becomes more of aproblem as vocabulary size, noise, or other parametersthat degrade performance increase; (2) an NL compo-nent that provides only a score of 1 or 0 is limited in itsability to take advantage of the N-best outputs, par-ticularly for large N, since many of the N-best maybe grammatical, and some will be more grammaticalthan others.
A strategy to combat this problem is alattice-based interface (Murveit et al 1993).
A lat-tice of speech hypotheses can compactly include a verylarge N and greatly improve computational efficiency,especially for parsers that can parse lattices.The integration of speech recognition and NL  is con-cerned with many of the same issues that each of thecomponents face: robustness, portability, speed, andsize.
This section has so far outlined some issues thatarise in designing the architecture for combining thespeech and NL.
However, the integration gives rise tosome new areas as well: how can an NL  component dealwith spontaneous speech effects such as false starts andrepairs and how can a speech component send informa-tion l.o help the NL component (see e.g., Bear et al19.q2, Shriberg et al 1992), how can techniques fromthe two component areas be effectively combined, andhow can prosodic information be effectively communi-cated between the two components (see, e.g., Price andOstendorf 1994 for survey).Speech understanding in some sense works as well asits two components if they are serially connected.
How-ever, performance can be maximized if the two compo-nents take into account he strengths and weaknesses ofeach other.
In the ARPA benchmarks, if we compareon the same testset, we find that the best speech recog-nition results provide a completely correct ranscriptionof the utterance less often than the speech understand-ing results (speech recognition plus NL) provide a cor-rect answer.
This condition arises because many of thespeech errors do not affect the correctness of the an-swer (e.g., "flight" vs. "flights", "a" vs. "the"), and be-cause the understanding components have become morerobust to speech recognition and speaker errors, falsestarts, and neologisms.
This situation is not unlike hu-man speech understanding (particularly apparent overthe telephone or in a language that is not your nativelanguage), when you can better make out the sense ofwhat is meant han give an exact transcription of whatwas said.The results compiled in the ARPA benchmark pa-pers document the state of the art.
However, it is notat all clear that these are the right measures, or at leastthe only right measures.
In separate xperiments, wehave tried to correlate the "correctness" of the system'sanswer with subsequent user behavior.
We have foundmany factors that affect user behavior in predictableways, but the correctness or incorrectness of the an-swer seems to have little effect.
While it might be thatwe just have not yet found the right measure, there areseveral reasons that this correlation may be difficult toobtain.
For example, the user may or may not noticethat the answer is incorrect; the system's answer maybe incorrect but provide a superset of the informationrequested so that the user can continue without mea-surable interruption; or the system's answer could becorrect but look incorrect o the user, either because ofuser error, or because a mistake in understanding hap-pens to result in a correct answer (as frequently hap-pens when a day of the week is misrecognized, sinceinformation on different days is the same).
In partic-ular, we know that performance is affected by factorssuch as vocabulary size, task complexity, noise condi-tions, but we do not know how to generalize results froma particular benchmark condition to those in which allthese parameters may differ.
Complementary measuresto "correct answer" include: user satisfaction, time tocomplete standard tasks, user preference, and (perhapsthe bottom line) units sold.
The speech understandingcommunity is quite active in refining evaluation mea-sures and in developing new ones (see e.g., Price et al1992, Hirschman et al 1993, Dahl et al 1994).
becausethe evaluation measures guide the research directions,it is important o choose the right measures,82Discussion and SummaryThis audience should not require motivation of sym-bolic and traditionally linguistic knowledge sources, al-though some may have hesitation about statisticallybased engineering approaches.
This paper is an at-tempt o help bridge the gap between the largely sym-bolic and the largely statistical approaches.
Althoughstatistical models are far from the only tool for inves-tigating speech and language, as argued in Price andOstendorf (1994) they do provide several important fea-tures: they can be trained automatically, they can pro-vide a systematic way to combine multiple knowledgesources, they can express the more continuous proper-ties of speech and language, they make it easier to dealwith large corpora, they provide a means for assessingincomplete knowledge, and they provide a means foracquiring knowledge about speech and language.The ability to consider large corpora isa particularlyvaluable attribute.
Large corpora are the only placerare phenomena will be found in sufficient number to bestudied adequately.
Large corpora offer the possibilityof mediating two competing trends in speech and lan-guage research: "ecological validity" (which acknowl-edges that any change in conditions can affect he data,and therefore limits the data to speech and language oc-curring in conditions as natural as possible) and "speechscience" (which acknowledges that any change in con-ditions can affect the data, and therefore limits thedata to speech and language occurring in strictly con-trolled environments such as sound-proof booths, andread speech.).
Both sides start with the same premiseand choose opposite approaches.
Because language isso rich and variable, there will continue to be a need forboth approaches.
However, large corpora offer a datapoint somewhat in between: if the variable of inter-est recurs frequently enough, large corpora can provideenough naturally occurring instances to "wash out" theeffects of the various environments in which it occurs.Without automatic methods, many of them involvingstatistics, large corpora would be impossible to analyze.As argued in Price and Ostendorf (1994), the increas-ingly popular classification and regression trees, or de-cision trees (see, e.g., Breiman et at.
1984) appear to hea particularly useful tool in bridging the cultural andtechnical gap in question.
In this formalism, the speechresearcher or linguist can input the types of informa-tion that are known to affect variability (duration of aphone, for example), and based on a corpus of data inwhich these parameters are observed, the resulting treecan show how much of the variability is accounted forby each source of information (for example, voicing offollowing consonant, compared to existence of follow-ing silence).
Examples of the use of this tool are nu-merous: e.g., Hirschberg 1993, Ostendorf and Veilleux1993, Wang and Hirschberg 1991, and Withgott andChen 1993.Of course, the biggest disadvantage of many of theexisting statistical and other engineering models is cul-tural discomfort.
New techniques structure tile way ,m,,thinks about problems, and this can be uncomforl.ahh,and even threatening, ltowever, tile advantage.~ oll'erc'dby multidisciplinary approaches are large.
Obviou.~ly.the gap can be bridged by becoming fluent ill the newtechniques, but this is increasingly difficult as the chal-lenges of keeping up with existing fields increase.
Thegap can also be bridged by collaboration with otherswho are already fluent in the techniques, and by en-couraging students to learn more about the techniques.In sum, combining statistical with linguistic modelshas led to important gains in speech recognition andspeech understanding, and to more powerful tools foracquiring further knowledge.
Fuller understanding willrequire knowledge that spans all linguistic levels, fromacoustics through semantics and pragmatics/discourse.Few people are trained in all these areas.
Fewer stillhave training in statistical methods.
Therefore, in thenear term, multidisciplinary collaborations will be es-sential for rapid progress.ACKNOWLEDGMENT.
I thank Marl Ostendorf forher useful comments on the manuscript.
I gratefully ac-knowledge the support of ARPA/ONR Contract ONRN00014-90-C-0085, and ARPA/NSF funding throughNSF Grant IRI-8905249.
The opinions expressed arethe author's and not necessarily those of the fundingagencies.References\[1\] Abrash, V., M. Cohen, H. Franco, and I. Arima(1994) "Incorporating Linguistic Features in a IIy-brid HMM/MLP Speech Recognizer," Proceedingsof the International Conference on Acoustics, Speechand Signal Processing, 62.8.1-4.\[2\] Bahl, L., Jelinek, F. and Mercer, R. L.
(1983).
"A Maximum Likelihood Approach to ContinuousSpeech Recognition," 1EEE Trans.
Pattern Analysisand Machine Intelligence PAM\[-5, 2, 179-190.\[3\] Bahl, L., P. de Souza, P. Gopalaktishnan, D. Na-hamoo, and M. Picheny (1991) "Context DependentModeling of Phones in Continuous Speech using De-cision Trees," Proceedings of the DARPA Speech andNatural Language Workshop, pp.
264-269.\[4\] Bear, J., J. Dowding, and E. Shriberg (1992) "In-tegrating Multiple Knowledge Sources for Detectionand Correction of Repairs in Human-Computer Dia-log," Proc.
of the Annual Meeting of the Associationfor Computational Linguistics, pp.
56-63.
Also pub-lished as SRI Technical Note 518.\[5\] Breiman, L., ~I.
H. Friedman, R. A. Olshen, and C.J.
Stone (1984) Classification and Regression Trees,Wadsworth and Brooks/Cole Advanced Books andSoftware, Monterey, CA.\[6\] Cohen, M., G. Baldwin, J. Bernstein, H. Murveitand M. Weintraub (1987) "Studies for an AdaptiveRecognition Lexicon," Proceedings of the DARPASpeech and Natural Language Workshop, pp.
49-55.83\[7\] Cohen, M. (1989) 'Phonological Structures forSpeech Recognition, Department of Computer Sci-ence, University of California t Berkeley, Ph.D. Dis-sertation, University of Michigan Microfilms.\[8\] Dahl, D., M. Bates, M. Brown, W. Fisher, K.lhmicke-Smith, D. Pallett, C. Pao, A. Rudnieky, andE.
Shriberg (1994) Proceedings of the ARPA HumanLanguage Technology Workshop, to appear.\[9\] Digalakis, V., and H. Murveit (1994) "An Algorithmfor Optimizing the Degree of Tying in a Large Vocab-ulary tIidden Markov Model Based Speech Recog-nizer," Proceedings of the International Conferenceon Acoustics, Speech and Signal Processing, 54.2.1-4.\[10\] llampshire, J. and A. Weibel (1990) "Connection-ist Architectures for Multi-Speaker Phoneme Recog-nition," in D. Rouretzky (ed.)
Advances in Neural In-formation Processing Systems 2, Morgan Kaufman.\[11\] Iindle, D. (1992) "An Analogical Parser for Re-stricted Domains," Proceedings of the ARPA HumanLanguage Technology Workshop, pp.
150-154.\[12\] liirschberg, J.
(1993) "Pitch Accent in Context:Predicting Prominence from Text," Artificial Intelli-gence, Vol.
63, No.
1-2, pp.
305-340.\[13\] Hirschman, L., L. Bates, D. Dalai, W. Fisher, J.Garofolo, D. Pallett, K. Hunieke-Smith, P. Price, A.Rudnicky, and E. Tzoukermann (1993) "Multi-SiteData Collection and Evaluation in Spoken LanguageUnderstanding," Proceedings of the ARPA HumanLanguage Technology Workshop, pp.
19-24.\[14\] Klatt, D. (1977) "Review of the ARPA Speech Un-derstanding Project" J. Acoust.
Soc.
Amer.
62, no.6, 1)1 ).
1345-1366.\[15\] Kuhl, P. (1990) "Towards a New Theory of theDevelopment of Speech Perception," Proc.
Int.
Conf.on Spoken Language Processing 2, pp.
745-748.\[16\] Massaro', D. (1987) Speech Perception by Ear andEye: A Paradigm for Psychological Inquiry, Hills-dale, N J, Lawrence Erlbaum Associates.\[17\] Murveit, H., J. Butzberger, V. Digalakis, and M.Weintraub (1993) "Large Vocabulary Dictation us-ing SRI's DECIPHER Speech Recognition System:?
Progressive Search Techniques," Proceedings of theInternational Conference on Acoustics, Speech andSignal Processing, pp.
11-319-322.\[18\] Nguyen, L., R. Schwartz, F. Kubala, and P. Place-way (1993) "Search Algorithms for Software-OnlyReal-Time Recognition with Very Large Vocabular-ies," Proceedings of the ARPA Human LanguageTechnology Workshop, pp.
91-95.11~9\] ()stendorf, M. and S. Roukos (1989) "A Stochas-tic Segment Model for Phoneme-Based Continu-~)us Speech Recognition," IEEE Transactions onAcoustics, Speech and Signal Processing, December,pp.
1857-1869.\[20\] Ostendorf, M. and N. Veilleux (1993) "A Hierar-chical Stochastic Model for Automatic Prediction ofProsodic Boundary Location," Computational Lin-guistics.\[21\] PMlett, D. (1991) "DARPA Resource Manage-ment and ATIS Benchmark Test Poster Session,"Proc.
Speech and Natural Language Workshop, Mor-gan Kaufman, pp.
49-58.\[22\] Pallett, D., N. Dahlgren, J. Fiscus, W. Fisher, J.Garofolo and B. Tjaden (1992) "DARPA February1992 ATIS Benchmark Test Results," Proc.
Speechand Natural Language Workshop, Morgan Kaufman,pp.
15-27.\[23\] Pallett, D., J. Flatus, W. Fisher, J. Garofolo, B.Lund, and M. Prysbocki (1994) "1993 BenchmarkTests for the ARPA Spoken Language Program,"Proc.
Human Language Technology Workshop, Mor-gan Kanfman, to appear.\[24\] Pallett, D., J. Fiscus, W. Fisher, and J. Garofolo(1993) "Benchmark Tests for the DARPA SpokenLanguage Program," Proc.
Human Language Tech-nology Workshop, Morgan Kaufman, pp.
7-18.\[25\] Pallett, D., W. Fisher, J. Fiscus and J. Garofolo(1990) "DARPA ATIS Test Results," Proc.
Speechand Natural Language Workshop, Morgan Kaufman,pp.
114-121.\[26\] Picone, J.
(1990) "Continuous Speech RecognitionUsing Hidden Markov Models," IEEE ASSP Maga-zine, pp.
26-41.\[27\] Price, P., L. Hirschman, E. Shriberg, E. Wade(1992) "Subject-Based Evaluation Measures for In-teractive Spoken Language Systems, " Proceedingsof the DARPA Speech and Natural Language Work-shop, pp.
34-38.\[28\] Price, P. and M. Ostendorf (1994) "Combin-ing Linguistic with Statistical Methods in Model-ing Prosody," in J. L. Morgan and K.
Demuth(Eds.
), Signal to Syntax: Bootstrapping from Speechto Grammar in Early Acquisition.
Hillsdale, NJ:Lawrence Erlbaum Associates.\[29\] Rabiner, L. (1989).
"A tutorial on hidden Markovmodels and selected applications in speech recogni-tion," IEEE Proceedings 77, 2, 257-286.\[30\] Rayner, M., D. Carter, V. Digalakis, and P. Price(1994) "Combining Knowledge Sources to ReorderN-Best Speech Hypothesis Lists," Proceedings of theARPA Human Language Technology Workshop, (toappear).\[31\] Shriberg, E., J.
Bear, and J. Dowding (1992)"Automatic Detection and Correction of Repairsin Human-Computer Dialog," Proceedings of theDARPA Speech and Natural Language Workshop,pp.
419-424.84\[32\] Shriberg, E., E. Wade, P. Price (1992) "Human-Machine Problem Solving Using Spoken LanguageSystems (SLS): Factors Affecting Performance andUser Satisfaction," Proceedings off the DARPASpeech and Natural Language Workshop, pp.
49-54.\[33\] Veilleux, N. and M. Ostendoff (1993) "Probabilis-tic Parse Scoring with Prosodic Information," Pro-ceedings of the International Conference on Acous-tics, Speech and Signal Processing, pp.
II51-55.\[34\] Wang, M. and J. Hirschberg (1991) "Predicting In-tonational Boundaries Automaticslly from Text: TheATIS Domain," Proceedings of the DARPA Speechand Natural Language Workshop, pp.
378-383.\[35\] Withgott, M. and F. Chen (1993) ComputationalModels of American Speech, CSLI Lecture NotesNumber 32.\[36\] Zue, V., J.
Glass, J. Goddean, D. Goodine, L.Hirschman, H. Leung, M. Phillips, J. Polifroni, andS.
Seneff (1992) "The MIT ATIS System: February1992 Progress Report," Proceedings of the ARPAHuman Language Technology Workshop, pp.
84-88.85
