Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 34?42,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsChinese?Japanese Parallel Sentence Extractionfrom Quasi?Comparable CorporaChenhui Chu, Toshiaki Nakazawa, Sadao KurohashiGraduate School of Informatics, Kyoto UniversityYoshida-honmachi, Sakyo-kuKyoto, 606-8501, Japan{chu,nakazawa}@nlp.ist.i.kyoto-u.ac.jp kuro@i.kyoto-u.ac.jpAbstractParallel sentences are crucial for statisticalmachine translation (SMT).
However, theyare quite scarce for most language pairs,such as Chinese?Japanese.
Many studieshave been conducted on extracting parallelsentences from noisy parallel or compara-ble corpora.
We extract Chinese?Japaneseparallel sentences from quasi?comparablecorpora, which are available in far largerquantities.
The task is significantly moredifficult than the extraction from noisyparallel or comparable corpora.
We ex-tend a previous study that treats parallelsentence identification as a binary classifi-cation problem.
Previous method of clas-sifier training by the Cartesian product isnot practical, because it differs from thereal process of parallel sentence extrac-tion.
We propose a novel classifier train-ing method that simulates the real sentenceextraction process.
Furthermore, we uselinguistic knowledge of Chinese characterfeatures.
Experimental results on quasi?comparable corpora indicate that our pro-posed approach performs significantly bet-ter than the previous study.1 IntroductionIn statistical machine translation (SMT) (Brownet al 1993; Koehn et al 2007), the qualityand quantity of the parallel sentences are cru-cial, because translation knowledge is acquiredfrom a sentence?level aligned parallel corpus.However, except for a few language pairs, suchas English?French, English?Arabic and English?Chinese, parallel corpora remain a scarce re-source.
The cost of manual construction for paral-lel corpora is high.
As non?parallel corpora are farmore available, constructing parallel corpora fromnon?parallel corpora is an attractive research field.Non?parallel corpora include various levels ofcomparability: noisy parallel, comparable andquasi?comparable.
Noisy parallel corpora con-tain non?aligned sentences that are neverthelessmostly bilingual translations of the same docu-ment, comparable corpora contain non?sentence?aligned, non?translated bilingual documents thatare topic?aligned, while quasi?comparable cor-pora contain far more disparate very?non?parallelbilingual documents that could either be on thesame topic (in?topic) or not (out?topic) (Fung andCheung, 2004).
Most studies focus on extractingparallel sentences from noisy parallel corpora orcomparable corpora, such as bilingual news ar-ticles (Zhao and Vogel, 2002; Utiyama and Isa-hara, 2003; Munteanu andMarcu, 2005; Tillmann,2009; Abdul-Rauf and Schwenk, 2011), patentdata (Utiyama and Isahara, 2007; Lu et al 2010)and Wikipedia (Adafre and de Rijke, 2006; Smithet al 2010).
Few studies have been conductedon quasi?comparable corpora.
Quasi?comparablecorpora are available in far larger quantities thannoisy parallel or comparable corpora, while theparallel sentence extraction task is significantlymore difficult.While most studies are interested in languagepairs between English and other languages, wefocus on Chinese?Japanese, where parallel cor-pora are very scarce.
This study extractsChinese?Japanese parallel sentences from quasi?comparable corpora.
We adopt a system pro-posed by Munteanu and Marcu (2005), which isfor parallel sentence extraction from comparablecorpora.
We extend the system in several aspectsto make it even suitable for quasi?comparable cor-pora.
The core component of the system is a clas-sifier which can identify parallel sentences fromnon?parallel sentences.
Previous method of clas-sifier training by the Cartesian product is not prac-tical, because it differs from the real process ofparallel sentence extraction.
We propose a novel34Translated Jasentences asqueriesSMTIR: top N resultsCommonChinesecharactersCandidatesentencepairsParallelsentencesFilteringChinesecorporaJapanesecorporaClassifierProbabilisticdictionary(2)(1)(3)(4)Zh-Ja parallelcorpus5k sentencesFigure 1: Parallel sentence extraction system.method of classifier training and testing that sim-ulates the real sentence extraction process, whichguarantees the quality of the extracted sentences.Since Chinese characters are used both in Chi-nese and Japanese, they can be powerful linguisticclues to identify parallel sentences.
Therefore, weuse Chinese character features, which significantlyimprove the accuracy of the classifier.
We con-duct parallel sentence extraction experiments onquasi?comparable corpora, and evaluate the qual-ity of the extracted sentences from the perspectiveof MT performance.
Experimental results showthat our proposed system performs significantlybetter than the previous study.2 Parallel Sentence Extraction SystemThe overview of our parallel sentence extractionsystem is presented in Figure 1.
Source sentencesare translated to target language using a SMT sys-tem (1).
We retrieve the top N documents from tar-get language corpora with a information retrieval(IR) framework, using the translated sentences asqueries (2).
For each source sentence, we treatall target sentences in the retrieved documents ascandidates.
Then, we pass the candidate sentencepairs through a sentence ratio filter and a word?overlap?based filter based on a probabilistic dic-tionary, to reduce the candidates keeping more re-liable sentences (3).
Finally, a classifier trained ona small number of parallel sentences, is used toidentify the parallel sentences from the candidates(4).
A parallel corpus is needed to train the SMTsystem, generate the probabilistic dictionary andtrain the classifier.Our system is inspired by Munteanu and Marcu(2005), however, there are several differences.
Thefirst difference is query generation.
Munteanu andMarcu (2005) generate queries by taking the topN translations of each source word according tothe probabilistic dictionary.
This method is im-precise due to the noise in the dictionary.
In-stead, we adopt a method proposed by Abdul?Rauf and Schwenk (2011).
We translate the sourcesentences to target language with a SMT systemtrained on the parallel corpus.
Then use the trans-lated sentences as queries.
This method can gen-erate more precise queries, because phrase?basedMT is better than word?based translation.Another difference is that we do not conductdocument matching.
The reason is that docu-ments on the same topic may not exist in quasi?comparable corpora.
Instead, we retrieve the topN documents for each source sentence.
In com-parable corpora, it is reasonable to only use thebest target sentence in the retrieved documents ascandidates (Abdul-Rauf and Schwenk, 2011).
Inquasi?comparable corpora, it is important to fur-ther guarantee the recall.
Therefore, we keep alltarget sentences in the retrieved documents as can-didates.Our system also differs by the way of classi-fier training and testing, which is described in Sec-tion 3 in detail.3 Binary Classification of ParallelSentence IdentificationParallel sentence identification from non?parallelsentences can be seen as a binary classificationproblem (Munteanu and Marcu, 2005; Tillmann,2009; Smith et al 2010; S?tefa?nescu et al 2012).35Since the quality of the extracted sentences is de-termined by the accuracy of the classifier, the clas-sifier becomes the core component of the extrac-tion system.
In this section, we first describe thetraining and testing process, then introduce thefeatures we use for the classifier.3.1 Training and TestingMunteanu and Marcu (2005) propose a method ofcreating training and test instances for the classi-fier.
They use a small number of parallel sentencesas positive instances, and generate non?parallelsentences from the parallel sentences as negativeinstances.
They generate all the sentence pairsexcept the original parallel sentence pairs in theCartesian product, and discard the pairs that do notfulfill the condition of a sentence ratio filter and aword?overlap?based filter.
Furthermore, they ran-domly discard some of the non?parallel sentenceswhen necessary, to guarantee the ratio of negativeto positive instances smaller than five for the per-formance of the classifier.Creating instances by using the Cartesian prod-uct is not practical, because it differs from the realprocess of parallel sentence extraction.
Here, wepropose a novel method of classifier training andtesting that simulates the real parallel sentence ex-traction process.
For training, we first select 5kparallel sentences from a parallel corpus.
Thentranslate the source side of the selected sentencesto target language with a SMT system trained onthe parallel corpus excluding the selected parallelsentences.
We retrieve the top N documents fromthe target language side of the parallel corpus, us-ing the translated sentences as queries.
For eachsource sentence, we consider all target sentencesin the retrieved documents as candidates.
Finally,we pass the candidate sentence pairs through asentence ratio filter and a word?overlap?based fil-ter, and get the training instances.
We treat thesentence pairs that exist in the original 5k parallelsentences as positive instances, while the remain-der as negative instances.
Note that positive in-stances may be less than 5k, because some of theparallel sentences do not pass the IR frameworkand the filters.
For the negative instances, we alsorandomly discard some of them when necessary,to guarantee the ratio of negative to positive in-stances smaller than five.
Test instances are gen-erated by another 5k parallel sentences from theparallel corpus using the same method.There are several merits of the proposedmethod.
It can guarantee the quality of the ex-tracted sentences, because of the similarity be-tween the real sentence extraction process.
Also,features from the IR results can be used to furtherimprove the accuracy of the classifier.
The pro-posed method can be evaluated not only on thetest sentences that passed the IR framework andthe filters, but also on all the test sentences, whichis similar to the evaluation for the real extractionprocess.
However, there is a limitation of ourmethod that a both sentence?level and document?level aligned parallel corpus is needed.3.2 Features3.2.1 Basic FeaturesThe following features are the basic features weuse for the classifier, which are proposed byMunteanu and Marcu (2005):?
Sentence length, length difference and lengthratio.?
Percentage of words on each side that have atranslation on the other side (according to theprobabilistic dictionary).?
Alignment features:?
Percentage and number of words thathave no connection.?
The top three largest fertilities.?
Length of the longest contiguous con-nected span.?
Length of the longest unconnected sub-string.Alignment features are extracted from the align-ment results of the parallel and non?parallel sen-tences used as instances for the classifier.
Notethat alignment features may be unreliable whenthe quantity of non?parallel sentences is signifi-cantly larger than parallel sentences.3.2.2 Chinese Character FeaturesDifferent from other language pairs, Chinese andJapanese share Chinese characters.
In Chinesethe Chinese characters are called Hanzi, while inJapanese they are called Kanji.
Hanzi can be di-vided into two groups, Simplified Chinese (usedin mainland China and Singapore) and TraditionalChinese (used in Taiwan, Hong Kong and Macau).The number of strokes needed to write characters36???????????????????????????????????????????????????
?Wash ether phase with saturated saline,  and dry it with anhydrous magnesium.Zh:Ja:Ref:Figure 2: Example of common Chinese characters in a Chinese?Japanese parallel sentence pair.Meaning snow love beginTC ?
(U+96EA) ?
(U+611B) ?
(U+767C)SC ?
(U+96EA) ?
(U+7231) ?
(U+53D1)Kanji ?
(U+96EA) ?
(U+611B) ?
(U+767A)Table 1: Examples of common Chinese characters(TC denotes Traditional Chinese and SC denotesSimplified Chinese).has been largely reduced in Simplified Chinese,and the shapes may be different from those in Tra-ditional Chinese.
Because Kanji characters origi-nated from ancient China, many common Chinesecharacters exist between Hanzi and Kanji.
Table 1gives some examples of common Chinese char-acters in Traditional Chinese, Simplified Chineseand Japanese with their Unicode.Since Chinese characters contain significant se-mantic information, and common Chinese charac-ters share the same meaning, they can be valuablelinguistic clues for many Chinese?Japanese NLPtasks.
Many studies have exploited common Chi-nese characters.
Tan et al(1995) used the occur-rence of identical common Chinese characters inChinese and Japanese (e.g.
?snow?
in Table 1) inautomatic sentence alignment task for document?level aligned text.
Goh et al(2005) detected com-mon Chinese characters where Kanji are identicalto Traditional Chinese, but different from Simpli-fied Chinese (e.g.
?love?
in Table 1).
Using a Chi-nese encoding converter1 that can convert Tradi-tional Chinese into Simplified Chinese, they builta Japanese?Simplified Chinese dictionary partlyusing direct conversion of Japanese into Chinesefor Japanese Kanji words.
Chu et al(2011) madeuse of the Unihan database2 to detect commonChinese characters which are visual variants ofeach other (e.g.
?begin?
in Table 1), and provedthe effectiveness of common Chinese charactersin Chinese?Japanese phrase alignment.
Chu etal.
(2012a) exploited common Chinese charac-ters in Chinese word segmentation optimization,which improved the translation performance.In this study, we exploit common Chinese char-1http://www.mandarintools.com/zhcode.html2http://unicode.org/charts/unihan.htmlacters in parallel sentence extraction.
Chu etal.
(2011) investigated the coverage of commonChinese characters on a scientific paper abstractparallel corpus, and showed that over 45% Chi-nese Hanzi and 75% Japanese Kanji are commonChinese characters.
Therefore, common Chinesecharacters can be powerful linguistic clues to iden-tify parallel sentences.We make use of the Chinese character map-ping table created by Chu et al(2012b) to de-tect common Chinese characters.
Following fea-tures are used.
We use an example of Chinese?Japanese parallel sentence presented in Figure 2 toexplain the features in detail, where common Chi-nese characters are in bold and linked with dottedlines.?
Number of Chinese characters on each side(Zh: 18, Ja: 14).?
Percentage of Chinese characters out of allcharacters on each side (Zh: 18/20=90%, Ja:14/32=43%).?
Ratio of Chinese character numbers on bothsides (18/14=128%).?
Number of n?gram common Chinese charac-ters (1?gram: 12, 2?gram: 6, 3?gram: 2, 4?gram: 1).?
Percentage of n?gram common Chinese char-acters out of all n?gram Chinese characterson each side (Zh: 1?gram: 12/18=66%, 2?gram: 6/16=37%, 3?gram: 2/14=14%, 4?gram: 1/12=8%; Ja: 1?gram: 12/14=85%,2?gram: 6/9=66%, 3?gram=: 2/5=40%, 4?gram: 1/3=33%).Note that Chinese character features are onlyapplicable to Chinese?Japanese.
However, sinceChinese and Japanese character information is akind of cognates (words or languages which havethe same origin), the similar idea can be applied toother language pairs by using cognates.
Cognatesamong European languages have been shown ef-fective in word alignments (Kondrak et al 2003).We also can use cognates for parallel sentence ex-traction.373.3 Rank FeatureOne merit of our classifier training and testingmethod is that features from the IR results can beused.
Here, we use the ranks of the retrieved doc-uments returned by the IR framework as feature.4 ExperimentsWe conducted classification and translation exper-iments to evaluate the effectiveness of our pro-posed parallel sentence extraction system.4.1 Data4.1.1 Parallel CorpusThe parallel corpus we used is a scientificpaper abstract corpus provided by JST3 andNICT4.
This corpus was created by the Japaneseproject ?Development and Research of Chinese?Japanese Natural Language Processing Technol-ogy?, containing various domains such as chem-istry, physics, biology and agriculture etc.
Thiscorpus is aligned in both sentence?level anddocument?level, containing 680k sentences and100k articles.4.1.2 Quasi?Comparable CorporaThe quasi?comparable corpora we used are scien-tific paper abstracts collected from academic web-sites.
The Chinese corpora were collected fromCNKI5, containing 420k sentences and 90k arti-cles.
The Japanese corpora were collected fromCiNii6 web portal, containing 5M sentences and880k articles.
Note that since the paper abstractsin these two websites were written by Chinese andJapanese researchers respectively through differ-ent periods, documents on the same topic may notexist in the collected corpora.
We investigatedthe domains of the Chinese and Japanese corporain detail.
We found that most documents in theChinese corpora belong to the domain of chem-istry.
While the Japanese corpora contain variousdomains such as chemistry, physics, biology andcomputer science etc.
However, the domain infor-mation is unannotated in both corpora.4.2 Classification ExperimentsWe conducted experiments to evaluate the accu-racy of the proposed method of classification, us-3http://www.jst.go.jp4http://www.nict.go.jp5http://www.cnki.net6http://ci.nii.ac.jping different 5k parallel sentences from the paral-lel corpus as training and test data.4.2.1 Settings?
Probabilistic dictionary: We took the top5 translations with translation probabilitylarger than 0.1 created from the parallel cor-pus.?
IR tool: Indri7 with the top 10 results.?
Segmenter: For Chinese, we used asegmenter optimized for Chinese?JapaneseSMT (Chu et al 2012a).
For Japanese, weused JUMAN (Kurohashi et al 1994).?
Alignment: GIZA++8.?
SMT: We used the state?of?the?art phrase?based SMT toolkit Moses (Koehn et al2007) with default options, except for the dis-tortion limit (6?20).?
Classifier: LIBSVM9 with 5?fold cross?validation and radial basis function (RBF)kernel.?
Sentence ratio filter threshold: 2.?
Word?overlap?based filter threshold: 0.25.?
Classifier probability threshold: 0.5.4.2.2 EvaluationWe evaluate the performance of classification bycomputing precision, recall and F?value, definedas:precision = 100?
classified wellclassified parallel, (1)recall = 100?
classified welltrue parallel, (2)F ?
value = 2?
precision ?
recallprecision + recall.
(3)Where classified well is the number of pairsthat the classifier correctly identified as parallel,classified parallel is the number of pairs thatthe classifier identified as parallel, true parallelis the number of real parallel pairs in the test set.Note that we only use the top 1 result identified asparallel by the classifier for evaluation.7http://www.lemurproject.org/indri8http://code.google.com/p/giza-pp9http://www.csie.ntu.edu.tw/?cjlin/libsvm38Features Precision Recall F?valueMunteanu+ 2005 88.43 85.20/79.76 86.78/83.87+Chinese character 91.62 93.63/87.66 92.61/89.60+Rank 92.15 94.53/88.50 93.32/90.29Table 2: Classification results for the filtered testsentences (before ?/?)
and all the test sentences(after ?/?
).4.2.3 ResultsWe conducted classification experiments, compar-ing the following three experimental settings:?
Munteanu+ 2005: Only using the featuresproposed by Munteanu and Marcu (2005).?
+Chinese character: Add the Chinese charac-ter features.?
+Rank: Further add the rank feature.Results evaluated for the test sentences thatpassed the IR framework and the filters, and allthe test sentences are shown in Table 2.
We cansee that the Chinese character features can signifi-cantly improve the accuracy.
The accuracy can befurther improved by the rank feature.4.3 Translation ExperimentsWe extracted parallel sentences from the quasi?comparable corpora, and evaluated Chinese?to?Japanese MT performance by appending the ex-tracted sentences to two baseline settings.4.3.1 Settings?
Baseline: Using all the 680k parallel sen-tences in the parallel corpus as training data(containing 11k sentences of chemistry do-main).?
Tuning: Using another 368 sentences ofchemistry domain.?
Test: Using another 367 sentences of chem-istry domain.?
Language model: 5?gram LM trained on theJapanese side of the parallel corpus (680ksentences) using SRILM toolkit10.?
Classifier probability threshold: 0.6.10http://www.speech.sri.com/projects/srilmClassifier # sentencesMunteanu+ 2005 (Cartesian) 27,077Munteanu+ 2005 (Proposed) 5,994+Chinese character (Proposed) 3,936+Rank (Proposed) 3,516Table 3: Number of extracted sentences.The reason we evaluate on chemistry domain isthe one we described in Section 4.1.2 that mostdocuments in the Chinese corpora belong to thedomain of chemistry.
We keep all the sentencepairs rather than the top 1 result (used in the clas-sification evaluation) identified as parallel by theclassifier.
The other settings are the same as theones used in the classification experiments.4.3.2 ResultsNumbers of extracted sentences using differentclassifiers are shown in Table 3, where?
Munteanu+ 2005 (Cartesian): Classifiertrained using the Cartesian product, and onlyusing the features proposed by Munteanu andMarcu (2005).?
Munteanu+ 2005 (Proposed): Classifiertrained using the proposed method, and onlyusing the features proposed by Munteanu andMarcu (2005).?
+Chinese character (Proposed): Add the Chi-nese character features.?
+Rank (Proposed): Further add the rank fea-ture.We can see that the extracted number is signif-icantly decreased by the proposed method com-pared to the Cartesian product, which may indi-cate the quality improvement of the extracted sen-tences.
Adding more features further decreases thenumber.We conducted Chinese?to?Japanese translationexperiments by appending the extracted sentencesto the baseline.
BLEU?4 scores for experimentsare shown in Table 4.
We can see that our proposedmethod of classifier training performs better thanthe Cartesian product.
Adding the Chinese charac-ter features and rank feature further improves thetranslation performance significantly.39Example 1Zh: ??????????????????
(Finally, this article explains the physical meaning of the optical operator.
)Ja: ?????????????????????????????
(Finally, briefly explain the physical meaning of the chemical potential.
)Example 2Zh: ?????????????????????
(Discussion of detection limit and measurement methods of emission spectral  analysis method.
)Ja: ??????????????????????
(Detection limit of emission spectral analysis method by photoelectric photometry.
)Figure 3: Examples of extracted sentences (parallel subsentential fragments are in bold).System BLEUBaseline 38.64Munteanu+ 2005 (Cartesian) 38.10Munteanu+ 2005 (Proposed) 38.54+Chinese character (Proposed) 38.87?+Rank (Proposed) 39.47?
?Table 4: BLEU scores for Chinese?to?Japanesetranslation experiments (???
and ???
denotes theresult is better than ?Munteanu+ 2005 (Cartesian)?significantly at p < 0.05 and p < 0.01 respec-tively, ?*?
denotes the result is better than ?Base-line?
significantly at p < 0.01).4.3.3 DiscussionThe translation results indicate that compared tothe previous study, our proposed method can ex-tract sentences with better qualities.
However,when we investigated the extracted sentences, wefound that most of the extracted sentences arenot sentence?level parallel.
Instead, they containmany parallel subsentential fragments.
Figure 3presents two examples of sentence pairs extractedby ?+Rank (Proposed)?, where parallel subsenten-tial fragments are in bold.
We investigated thealignment results of the extracted sentences.
Wefound that most of the parallel subsentential frag-ments were correctly aligned with the help of theparallel sentences in the baseline system.
There-fore, translation performance was improved by ap-pending the extracted sentences.
However, it alsoled to many wrong alignments among the non?parallel fragments which are harmful to transla-tion.
In the future, we plan to further extractthese parallel subsentential fragments, which canbe more effective for SMT (Munteanu and Marcu,2006).5 Related WorkAs parallel sentences trend to appear in similardocument pairs, many studies first conduct doc-ument matching, then identify the parallel sen-tences from the matched document pairs (Utiyamaand Isahara, 2003; Fung and Cheung, 2004;Munteanu and Marcu, 2005).
Approaches with-out document matching also have been proposed(Tillmann, 2009; Abdul-Rauf and Schwenk, 2011;S?tefa?nescu et al 2012).
These studies directly re-trieve candidate sentence pairs, and select the par-allel sentences using some filtering methods.
Weadopt a moderate strategy, which retrieves candi-date documents for sentences.The way of parallel sentence identification canbe specified with two different approaches: bi-nary classification (Munteanu and Marcu, 2005;Tillmann, 2009; Smith et al 2010; S?tefa?nescuet al 2012) and translation similarity measures(Utiyama and Isahara, 2003; Fung and Cheung,2004; Abdul-Rauf and Schwenk, 2011).
We adoptthe binary classification approach with a novelclassifier training and testing method and Chinesecharacter features.Few studies have been conducted for extract-ing parallel sentences from quasi?comparable cor-pora.
We are aware of only two previous efforts.Fung and Cheung (2004) proposed a multi-levelbootstrapping approach.
Wu and Fung (2005) ex-ploited generic bracketing Inversion TransductionGrammars (ITG) for this task.
Our approach dif-fers from the previous studies that we extend theapproach for comparable corpora in several as-pects to make it work well for quasi?comparablecorpora.6 Conclusion and Future WorkIn this paper, we proposed a novel method of clas-sifier training and testing that simulates the realparallel sentence extraction process.
Furthermore,we used linguistic knowledge of Chinese charac-ter features.
Experimental results of parallel sen-tence extraction from quasi?comparable corporaindicated that our proposed system performs sig-nificantly better than the previous study.40Our approach can be improved in several as-pects.
One is bootstrapping, which has beenproven effective in some related works (Fung andCheung, 2004; Munteanu and Marcu, 2005).
Inour system, bootstrapping can be done not onlyfor extension of the probabilistic dictionary, butalso for improvement of the SMT system used totranslate the source language to target language forquery generation.
Moreover, as parallel sentencesrarely exist in quasi?comparable corpora, we planto extend our system to parallel subsentential frag-ment extraction.
Our study showed that Chi-nese character features are helpful for Chinese?Japanese parallel sentence extraction.
We plan toapply the similar idea to other language pairs byusing cognates.ReferencesSadaf Abdul-Rauf and Holger Schwenk.
2011.
Par-allel sentence generation from comparable corporafor improved smt.
Machine Translation, 25(4):341?375.Sisay Fissaha Adafre and Maarten de Rijke.
2006.Finding similar sentences across multiple languagesin wikipedia.
In Proceedings of EACL, pages 62?69.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
Themathematics of statistical machine translation: Pa-rameter estimation.
Association for ComputationalLinguistics, 19(2):263?312.Chenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-hashi.
2011.
Japanese-chinese phrase alignmentusing common chinese characters information.
InProceedings of MT Summit XIII, pages 475?482, Xi-amen, China, September.Chenhui Chu, Toshiaki Nakazawa, Daisuke Kawahara,and Sadao Kurohashi.
2012a.
Exploiting sharedChinese characters in Chinese word segmentationoptimization for Chinese-Japanese machine transla-tion.
In Proceedings of the 16th Annual Conferenceof the European Association for Machine Transla-tion (EAMT?12), Trento, Italy, May.Chenhui Chu, Toshiaki Nakazawa, and Sadao Kuro-hashi.
2012b.
Chinese characters mapping table ofJapanese, Traditional Chinese and Simplified Chi-nese.
In Proceedings of the Eighth Conference onInternational Language Resources and Evaluation(LREC?12), Istanbul, Turkey, May.Dan S?tefa?nescu, Radu Ion, and Sabine Hunsicker.2012.
Hybrid parallel sentence mining from com-parable corpora.
In Proceedings of the 16th AnnualConference of the European Association for Ma-chine Translation (EAMT?12), Trento, Italy, May.Pascale Fung and Percy Cheung.
2004.
Multi-levelbootstrapping for extracting parallel sentences froma quasi-comparable corpus.
In Proceedings of Col-ing 2004, pages 1051?1057, Geneva, Switzerland,Aug 23?Aug 27.
COLING.Chooi-Ling Goh, Masayuki Asahara, and Yuji Mat-sumoto.
2005.
Building a Japanese-Chinese dic-tionary using kanji/hanzi conversion.
In Proceed-ings of the International Joint Conference on Natu-ral Language Processing, pages 670?681.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic,June.
Association for Computational Linguistics.Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.2003.
Cognates can improve statistical transla-tion models.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 46?48.Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-sumoto, and Makoto Nagao.
1994.
Improve-ments of Japanese morphological analyzer JUMAN.In Proceedings of the International Workshop onSharable Natural Language, pages 22?28.Bin Lu, Tao Jiang, Kapo Chow, and Benjamin K. Tsou.2010.
Building a large english-chinese parallel cor-pus from comparable patents and its experimentalapplication to smt.
In Proceedings of the 3rd Work-shop on Building and Using Comparable Corpora,LREC 2010, pages 42?49, Valletta, Malta, May.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguis-tics, 31(4):477?504, December.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Com-putational Linguistics, pages 81?88, Sydney, Aus-tralia, July.
Association for Computational Linguis-tics.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from compa-rable corpora using document level alignment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, pages403?411, Los Angeles, California, June.
Associa-tion for Computational Linguistics.41Chew Lim Tan and Makoto Nagao.
1995.
Automaticalignment of Japanese-Chinese bilingual texts.
IE-ICE Transactions on Information and Systems, E78-D(1):68?76.Christoph Tillmann.
2009.
A beam-search extrac-tion algorithm for comparable data.
In Proceedingsof the ACL-IJCNLP 2009 Conference Short Papers,pages 225?228, Suntec, Singapore, August.
Associ-ation for Computational Linguistics.Masao Utiyama and Hitoshi Isahara.
2003.
Reliablemeasures for aligning japanese-english news articlesand sentences.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 72?79, Sapporo, Japan, July.
Associ-ation for Computational Linguistics.Masao Utiyama and Hitoshi Isahara.
2007.
Ajapanese-english patent parallel corpus.
In Proceed-ings of MT summit XI, pages 475?482.Dekai Wu and Pascale Fung.
2005.
Inversion trans-duction grammar constraints for mining parallel sen-tences from quasi-comparable corpora.
In IJCNLP,pages 257?268.Bing Zhao and Stephan Vogel.
2002.
Adaptive paral-lel sentences mining from web abilingual news col-lections.
In Proceedings of the 2002 IEEE Interna-tional Conference on Data Mining, pages 745?748,Maebashi City, Japan.
IEEE Computer Society.42
