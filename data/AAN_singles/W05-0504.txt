Proceedings of the Second Workshop on Psychocomputational Models of Human Language Acquisition, pages 28?35,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsThe SED heuristic for morpheme discovery:a  look at SwahiliYu Hu and Irina MatveevaDepartment ofComputer ScienceThe University of ChicagoChicago IL 60637yuhu@cs.uchicago.edumatveeva@uchicago.eduJohn GoldsmithDepartments of Linguistics andComputer ScienceThe University of ChicagoChicago IL 60637ja-goldsmith@uchicago.eduColin SpragueDepartment of LinguisticsThe University of ChicagoChicago IL 60637sprague@uchicago.eduAbstractThis paper describes a heuristic formorpheme- and morphology-learningbased on string edit distance.Experiments with a 7,000 word corpusof Swahili, a language with a richmorphology, support the effectivenessof this approach.1 IntroductionThis paper describes work on a technique for theunsupervised learning of the morphology ofnatural languages which employs the familiarstring edit distance (SED) algorithm (Wagnerand Fischer 1974 and elsewhere) in its firststage;  we refer to it here as the SED heuristic.The heuristic finds 3- and 4-state finite stateautomata (FSAs) from untagged corpora.
Wefocus on Swahili, a Bantu language of EastAfrica, because of the very high average numberof morphemes per word, especially in the verbalsystem, a system that presents a real challenge toother systems discussed in the literature.1In Section 2, we present the SED heuristic,with precision and recall figures for itsapplication to a corpus of Swahili.
In Section 3,we propose three elaborations and extensions of1 An earlier version of this paper, with a more detaileddiscussion of the material presented in Section 3, isavailable at Goldsmith et al(2005).this approach, and in Section 4, we describe andevaluate the results from applying theseextensions to the corpus of Swahili.22 SED-based heuristicMost systems designed to learn natural languagemorphology automatically can be viewed asbeing composed of an initial heuristiccomponent and a subsequent explicit model.
Theinitial or bootstrapping heuristic, as the namesuggests, is designed to rapidly come up with aset of candidate strings of morphemes, while themodel consists of an explicit formulation ofeither (1) what constitutes an adequatemorphology for a set of data, or (2) an objectivefunction that must be optimized, given a corpusof data, in order to find the correctmorphological analysis.The best known and most widely usedheuristic is due to Zellig Harris (1955) (see alsoHarris (1967) and Hafer and Weiss (1974) for anevaluation based on an English corpus), using anotion that Harris called successor frequency(henceforth, SF).
Harris' notion can besuccinctly described in contemporary terms: ifwe encode all of the data in the data structureknown as a trie, with each node in the triedominating all strings which share a common2 SED has been used in unsupervised language learning in anumber of studies; see, for example, van Zaanen (2000)and references there, where syntactic structure is studied ina similar context.
To our knowledge, it has not been used inthe context of morpheme detection.28string prefix,3 then each branching node in thetrie is associated with a morpheme break.
Forexample, a typical corpus of English maycontain the words governed, governing,government, governor, and governs.
If this datais encoded in the usual way in a trie, then asingle node will exist in the trie which representsthe string prefix govern and which dominatesfive leaves corresponding to these five words.Harris's SF-based heuristic algorithm wouldpropose a morpheme boundary after govern onthis basis.
In contemporary terms, we caninterpret Harris?s heuristic as providing sets ofsimple finite state automata, as in (1), whichgenerate a string prefix (PF1) followed by a setof string suffixes (SFi) based on themeasurement of a successor frequency greaterthan 1 (or some threshold value) at the stringposition following PF1.
(1)SF1SF3PF1 SF2A variant on the SF-based heuristic,predecessor frequency (henceforth, PF), calls forencoding words in a trie from right to left.
Insuch a PF-trie, each node dominates all stringsthat share a common string suffix.
In general, weexpect SF to work best in a suffixing language,and PF to work best in prefixing language;Swahili, like all the Bantu languages, isprimarily a prefixing language, but it has asignificant number of important suffixes in boththe verbal and the nominal systems.Goldsmith (2001) argues for using thediscovery of signatures as the bootstrappingheuristic, where a signature is a maximal set ofstems and suffixes with the property that allcombinations of stems and suffixes are found inthe corpus in question.
We interpret Goldsmith?ssignatures as extensions of FSAs as in (1) to3 We use the terms string prefix and string suffix in thecomputer science sense: a string S is a string prefix of astring X iff there exists a string T such that X = S.T, where?.?
is the string concatenation operator; under suchconditions, T is likewise a string suffix of X.
Otherwise, weuse the terms prefix and suffix in the linguistic sense, and astring prefix (e.g., jump) may be a linguistic stem, as injump-ing.FSAs as in (2); (2) characterizes Goldsmith?snotion of signature in term of FSAs.
Inparticular, a signature is a set of forms that canbe characterized by an FSA of 3 states.
(2)PF1 SF1PF3 SF2PF2We propose a simple alternative heuristicwhich utilizes the familiar dynamicprogramming algorithm for calculating string-edit distance, and finding the best alignmentbetween two arbitrary strings (Wagner andFischer 1974).
The algorithm finds subsets ofthe data that can be exactly-generated bysequential finite state automata of 3 and 4 states,as in (3), where the labels mi should beunderstood as cover terms for morphemes ingeneral.
An automaton exactly-generates a set ofstrings S if it generates all strings in S and noother strings; a sequential FSA is one of theform sketched graphically in (1)-(3), where thereis a unique successor to each state.
(3)M1 M4M3 M6M2M7M9M5 M82.1 First stage: alignments.If presented with the pair of strings anapendaand anamupenda from an unknown language, itis not difficult for a human being to come upwith the hypothesis that mu is a morphemeinside a larger word that is composed of at leasttwo morphemes, perhaps ana- and -penda.
TheSED heuristic makes this observation explicit bybuilding small FSAs of the form in (4), where atmost one of m1 or m4 may be null, and at mostone of m2 and m3 may be null: we refer to theseas elementary alignments.
The strings m2 and m3are called counterparts; the pairs of strings m1and m4 are called the context (of thecounterparts).
(Indeed, we consider this kind ofstring comparison to be a plausible candidate forhuman language learning; see Dahan and Brent1999).29(4)1 432m1 m4m3m2The first stage of the algorithm consists oflooking at all pairs of words S, T in the corpus,and passing through the following steps:We apply several initial heuristics toeliminate a large proportion of the pairs ofstrings before applying the familiar SEDalgorithm to them, in view of the relativeslowness of the SED algorithm; see Goldsmithet al(2005) for further details.We compute the optimal alignment of S andT using the SED algorithm, where alignmentbetween two identical letters (which we calltwins) is assigned a cost of 0, alignment betweentwo different letters (which we call siblings) isassigned a cost of 1.5, and a letter in one stringnot aligned with a segment on the other string(which we call an orphan) is assigned a cost of1.
An alignment as in (5) is thus assigned a costof 5, based on a cost of 1.5 assigned to eachbroken line, and 1  to each dotted line that endsin a square box.
(5)n i l i m u p e n d an i t a k a m u p e n d aThere is a natural map from every alignmentto a unique sequence of pairs, where every pairis either of the form (S[i], T[j]) (representingeither a twin or sibling case) or of the form (S[i],0) or (0, T[j]) (representing the orphan case).
Wethen divide the alignment up into perfect andimperfect spans: perfect spans are composed ofmaximal sequences of twin pairs, whileimperfect spans are composed of maximalsequences of sibling or orphan pairs.
This isillustrated in (6).
(6)There is a natural equivalence betweenalignments and sequential FSAs as in (4), whereperfect spans correspond to pairs of adjacentstates with unique transitions and imperfectspans correspond to pairs of adjacent states withtwo transitions, and we will henceforth use theFSA notation to describe our algorithm.2.2 Collapsing alignmentsAs we noted above (4), for any elementaryalignment, a context is defined: the pair ofstrings (one of them possibly null) whichsurround the pair of counterparts.
Our first goalis to collapse alignments that share their context.We do this in the following way.Let us define the set of strings associatedwith the paths leaving a state S as the productionof state S. A four-state sequential FSA, as in (4),has three states with non-null productions; if thisparticular FSA corresponds to an elementaryalignment, then two of the state-productionscontain exactly one string?and these state-productions define the context?
and one of thestate-productions contains exactly two strings(one possibly the null string)?this defines thecounterparts.
If we have two such four-stateFSAs whose context are identical, then wecollapse the two FSAs into a single conflatedFSA in which the context states and theirproductions are identical, and the states thatproduced the counterparts are collapsed bycreating a state that produces the union of theproductions of the original states.
This isillustrated in (7): the two FSAs in (7a) share acontext, generated by their states 1 and 3, andthey are collapsed to form the FSA in (7b), inwhich the context states remain unchanged, andthe counterpart states, labeled ?2?, are collapsedto form a new state ?2?
whose production is theunion of the productions of the original states.
(7)a.1 432m1 m41 432m1 m4m7m8m3m2n i   l i   m u p e n d an i   t a k a   m u p e n d a30b.1 432m1 m4m8m7m3m22.3 Collapsing the resulting sequentialFSAsWe now generalize the procedure described inthe preceding section to collapse any twosequential FSAs for which all but one of thecorresponding states have exactly the sameproduction.
For example, the two sequentialFSAs in (8a) are collapsed into (8b).Three and four-state sequential FSAs as in(8b), where at least two of the state-transitionsgenerate more than one morpheme, form the setof templates derived from our bootstrappingheuristic.
Each such template can be usefullyassigned a quantitative score based on thenumber of letters ?saved?
by the use of thetemplate to generate the words, in the followingsense.
The template in (8b) summarizes fourwords: aliyesema, alimfuata, anayesema, andanamfuata.
The total string length of thesewords is 36, while the total number of letters inthe strings associated with the transitions in theFSA is 1+4+12 = 17; we say that the FSA saves36-17 = 19 letters.
In actual practice, thesignificant templates discovered save on theorder of 200 to 5,000 letters, and ranking themby the number of letters saved is a good measureof how significant they are in the overallmorphology of the language.
We refer to thisscore as a template?s robustness; we employ thisquantity again in section 3.1 below.By this ranking, the top template found in ourSwahili corpus of 50,000 running words was onethat generated a and wa (class 1 and 2 subjectmarkers) and followed by 246 correct verbcontinuations (all of them polymorphemic); thefirst 6 templates are summarized informally inTable 1.
We note that the third and fourthtemplate can also be collapsed to form atemplate of the form in (3), a point we return tobelow.
Precision, recall, and F-score for theseexperiments are given in Table 2.
(8)a.1 432a yesemanali1 432a mfuatanalib.1 432anali yesemamfuataState 1 State 2 State 3a, wa (sg., pl.human subjectmarkers)246 stemsku, hu(infinitive,habitualmarkers)51 stemswa (pl.
subjectmarker)ka, li (tensemarkers)25 stemsa (sg.
subjectmarker)ka, li (tensemarkers)29 stemsa (sg.
subjectmarker)ka, na (tensemarkers28 stems37 strings w (passivemarker)aTable 1 Top templates in SwahiliPrecision Recall  F-scoreSED 0.77 0.57 0.65SF 0.54 0.14 0.22PF 0.68 0.20 0.31Table 2 Results313 Further developmentsIn this section, we describe three developmentsof the SED-based heuristic sketched in section 2.The first disambiguates which state it is thatstring material should be associated with incases of ambiguity; the second collapsestemplates associated with similar morphologicalstructure; the third uses the FSAs to predictwords that do not actually occur in the corpus byhypothesizing stems on the basis of theestablished FSAs and as yet unanalyzed wordsin the corpus.3.1 Disambiguating FSAsIn the case of a sequential FSA, when the finalletter of the production of a (non-final) state Sare identical, then that letter can be moved frombeing the string-suffix of all of the productionsof state S to being the string-prefixes of all ofthe productions of the following state.
Moregenerally, when the n final letters of theproductions of a state are identical, there is an n-way ambiguity in the analysis, and the sameholds symmetrically for the ambiguity that ariseswhen the n initial letters of the production of a(non-initial) state.Thus two successive states, S and T, must (soto speak) fight over which will be responsiblefor generating the ambiguous string.
We employtwo steps to disambiguate these cases.Step 1: The first step is applicable when thenumber of distinct strings associated with statesS and T are quite different in size (typicallycorresponding to the case where one generatesgrammatical morphemes and the other generatesstems); in this case, we assign the ambiguousmaterial to the state that generates the smallernumber of strings.
There is a natural motivationfor this choice from the perspective of our desireto minimize the size of the grammar, if weconsider the size of the grammar to be based, inpart, on the sum of the lengths of the morphemesproduced by each state.Step 2: It often happens that an ambiguityarises with regard to a string of one or moreletters that could potentially be produced byeither of a pair of successive states involvinggrammatical morphemes.
To deal with this case,we make a decision that is also (like thepreceding step) motivated by a desire tominimize the description length of the grammar.In this case, however, we think of the FSA ascontaining explicit strings (as we have assumedso far), but rather pointers to strings, and the?length?
of a pointer to a string is inverselyproportional to the logarithm of its frequency.Thus the overall use of a string in the grammarplays a crucial role in determining the length ofa grammar, and we wish to maximize theappearance in our grammar of morphemes thatare used frequently, and minimize the use ofmorphemes that are used rarely.We implement this idea by collecting a tableof all of the morphemes produced by our FSA,and assigning each a score which consists of thesum of the robustness scores of each templatethey occur in (see discussion just above (8)).Thus morphemes occurring in several highrobustness templates will have high scores;morphemes appearing in a small number oflowly ranked templates will have low scores.To disambiguate strings which could beproduced by either of two successive states, weconsider all possible parsings of the stringbetween the states, and score each parsing as thesum of the scores of the component morphemes;we chose the parsing for which the total score isa maximum.For example, Swahili has two common tensemarkers, ka and ki, and this step corrected atemplate from {ak}+{a,i}+{stems} to{a}+{ka,ki}+{stems}, and others of similarform.
It also did some useful splitting of joinedmorphemes, as when it modified a template{wali} + {NULL, po} + {stems} to {wa} + {li,lipo} + {stems}.
In this case, wali should indeedbe split into wa + li (subject and tense markers,resp.
), and while the change creates an error (inthe sense that lipo is, in fact, two morphemes; pois a subordinate clause marker), the resultingerror occurs considerably less often in the data,and the correct template will better be able to beintegrated with out templates.3.2 Template collapsingFrom a linguistic point of view, the SED-basedheuristic creates too many FSAs because it staystoo close to the data provided by the corpus.
Theonly way to get a more correct grammar is bycollapsing the FSAs, which will have as a32consequence the generation of new words notfound in the corpus.
We apply the followingrelatively conservative strategy for collapsingtwo templates.We compare templates of the same numberof states, and distinguish between states thatproduce grammatical morphemes (five or fewerin number) and those that produce stems (that is,lexical morphemes, identified as being six ormore in number).
We collapse two templates ifthe productions of the corresponding statessatisfy the following conditions: if the statesgenerate stems, then the intersection of theproductions must be at least two stems, while ifthe states are grammatical morphemes, then theproductions of one pair of corresponding statesmust be identical, while for the other pair, thesymmetric difference of the productions must beno greater than two in number (that is, thenumber of morphemes produced by the state ofone template but not the other must not exceed2).3.3 Reparsing words in the corpus andpredicting new wordsWhen we create robust FSAs?that is, FSAs thatgenerate a large number of words?we are in aposition to go back to the corpus and reanalyze alarge number of words that could not bepreviously analyzed.
That is, a 4-state FSA inwhich each state produced two strings generates8 words, and all 8 words must appear in thecorpus for the method described so far in orderfor this particular FSA to generate any of them.But that condition is unlikely to be satisfied forany but the most common of morphemes, so weneed to go back to the corpus and infer theexistence of new stems (as defined operationallyin the preceding paragraph) based on theiroccurrence in several, but not all possible,words.
If there exist 3 distinct words in thecorpus which would all be generated by atemplate if a given stem were added to thetemplate, we add that stem to the template.4 Experiments and ResultsIn this section, we present three sets ofevaluations of the refinements of the SEDheuristics described in the preceding section.
Weused a corpus of 7,180 distinct words occurringin 50,000 running words from a Swahilitranslation of the Bible obtained on the internet.4.1 Disambiguating FSAsIn order to evaluate the effects of thedisambiguating of FSAs described in section3.1, we compare precision and recall of theidentification of morpheme boundaries using theSED method with and without thedisambiguation procedure described above.
InFigures 1 and 2, we graph precision and recallfor the top 10% of the templates, displayed asthe leftmost point, for the top 20% of thetemplates, displayed as the second point fromthe left; and so on, because the higher rankedFSAs are more intrinsically more reliable thanthe lower ranked ones.
We see thatdisambiguation repairs almost 50% of theprevious errors, and increases recalls by about10%.
With these increases in precision andrecall, it is clear that the disambiguating stepprovides a considerably more accuratemorpheme boundary discovery procedure.Precision0.70.750.80.850.90.95110 20 30 40 50 60 70 80 90 100Deciles(%)PrecisionWithoutWithFigure 1 Comparison of precisionCompare Recalls0.30.340.380.420.460.510 20 30 40 50 60 70 80 90 100Deciles(%)RecallsWithoutWithFigure 2 Comparison of recall334.2 Template collapsingThe second refinement discussed aboveconsists of finding pairs of similar templates,collapsing them as appropriate, and thus creatingpatterns that generate new words that did notparticipate in the formation of the originaltemplates.
These new words may or may notthemselves appear in the corpus.
We are,however, able to judge their morphological well-formedness by inspection.
We list in Table 3 theentire list of eight templates that are collapsed inthis step.All of the templates which are collapsed inthis step are in fact of the same morphologicalstructure (with one very minor exception4): theyare of the form subject marker + tense marker +stem, and the collapsing induced in thisprocedure correctly creates larger templates ofprecisely the same structure, generating newwords not seen in the corpus that are in factcorrect from our (non-native speaker)inspection.
We submitted the new words toYahoo to test the words ?existence?
by theirexistence on the internet, and actually found anaverage of 87% of the predicted words in atemplate; see the last column in Table 3 fordetails.4.3 ReparsingAfter previous refinements, we obtain anumber of robust FSAs, for example, thosecollapsed templates in Table 3.
With them, wethen search the corpus for those words that canonly be partly fitted into these FSAs andgenerate associated stems.
Table 4 shows thereparsed words that had not been parsed byearlier templates and also newly added stems forsome robust FSAs (the four collapsed templatesin Table 3).
Stems such as anza ?begin?
andfanya ?do?
are thus added to the first template,and all words derived by prepending a tensemarker and a subject marker are indeed accuratewords.
As the words in Table 4 suggest, thereparsing process adds new, common stems tothe stem-column of the templates, thus making it4 The exception involves the distinct morpheme po, asubordinate clause marker which must ultimately beanalyzed as appearing in a distinct template column to theright of the tense markers.easier for the collapsing function to findsimilarities across related templates.In future work, we will take use the largertemplates, populated with more stems, and inputthem to the collapsing function described in 3.2.5 ConclusionsOn the basis of the experiments with Swahilidescribed in this paper, the SED heuristicappears to be a useful tool for the discovery ofmorphemes in languages with richmorphologies, and for the discovery of the FSAsthat constitute the morphologies of thoselanguages.Ultimately, the value of the heuristic is besttested against a range of languages with complexconcatenative morphologies.
While a thoroughdiscussion would take us well beyond the limitsof this paper, we have applied the SED heuristicto English, Hungarian, and Finnish as well asSwahili.
For English, unsurprisingly, the methodworks as well as the SF and PF methods, thougha bit more slowly, while for Hungarian andFinnish, the results appear promising, and acomparison with Creutz and Lagus (2004) forFinnish, for example, would be appealing.34One TemplateThe other template Collapsed Template% found onYahoo search1 {a}-{ka,na}-{stems} {a}-{ka,ki}-{stems} {a}-{ka,ki,na}-{stems} 86 (37/43)2 {wa}-{ka,na}-{stems} {wa}-{ka,ki}-{stems} {wa}-{ka,ki,na}-{stems} 95 (21/22)3 {a}-{ka,ki,na}-{stems} {wa}-{ka,ki,na}-{stems} {a,wa}-{ka,ki,na}-{stems} 84 (154/183)4 {a}-{liye,me}-{stems} {a}-{liye,li}-{stems} {a}-{liye,li,me}-{stems} 100 (21/21)5 {a}-{ki,li}-{stems} {wa}-{ki,li}-{stems} {a,wa}-{ki,li}-{stems} 90 (36/40)6 {a}-{lipo,li}-{stems} {wa}-{lipo,li}-{stems} {a,wa}-{lipo,li}-{stems} 90 (27/30)7 {a,wa}-{ki,li}-{stems} {a,wa}-{lipo,li}-{stems} {a,wa}-{ki,lipo,li}-{stems} 74 (52/70)8 {a}-{na,naye}-{stems} {a}-{na,ta}-{stems} {a}-{na,ta,naye}-{stems} 80 (12/15)Table 3  Collapsed Templates and Created Words Sample.Template Reparsed Words Not ParsedBeforeAdded Stems1 {a, wa}-{ka,ki,na}-{stems} akawakweza, akiwa, anacho,akibatiza,  ?toka, anza, waita, fanya, enda, ?2 {a}-{li,liye,me }-{stems} ameinuka, ameugua, alivyo,aliyoniagiza,  ?zaliwa, kuwa, fanya, sema3 {a, wa}-{ki,li,lipo}-{stems} alimtoboa,  alimtaka,waliamini,  ?pata, kuwa, kaa, fanya, chukua,fika, ?4 {a} ?
{na,naye,ta}-{stems} analazwa,  atanitukuza,  anaye,anakuita,   ?ingia, semaTable 4 Reparsed words and "discovered" stemsReferencesCreutz, Mathias, and Krista Lagus.
(2004).
Inductionof a simple morphology for highly inflectinglanguages.
Proceedings of the Workshop ofSIGPHON (Barcelona).Dahan, Delphine, and Michael Brent.
(1999).
On thediscovery of novel world-like units fromutterances.
Journal of Experimental Psychology:General  128: 165-185.Goldsmith, John.
(2001).
Unsupervised Learning ofthe Morphology of a Natural Language.Computational Linguistics 27(2): 153-198.Goldsmith, John, Yu Hu, Irina Matveeva, and ColinSprague.
2005.
A heuristic for morphemediscovery based on string edit distance.
TechnicalReport TR-2005-4.
Department of ComputerScience.
University of Chicago.Hafer, M. A., Weiss, S. F.  (1974).
Wordsegmentation by letter successor varieties.Information Storage and Retrieval 10: 371-385.Harris, Zellig.
(1955).
From Phoneme to Morpheme.Language 31: 190-222.Harris, Zellig.
(1967).
Morpheme Boundaries withinWords: Report on a Computer Test.Transformations and Discourse Analysis Papers73.Oliver, Antoni, Irene Bastell?n, and Llu?s M?rquez.(2003).
Uso de Internet para aumentar la coberturade un sistema de adquisici?n l?xica del ruso.SEPLN 2003.Wagner, R. A., Fischer, M. J.
(1974).
The string-to-string correction problem.
Journal of theAssociation for Computing Machinery 21(1): 168-73.van Zaanen,  Menno.
2000.
ABL: Alignment-BasedLearning.
Proceedings of the 17th Conference onComputational Linguistics, vol.
2. p. 961-67.35
