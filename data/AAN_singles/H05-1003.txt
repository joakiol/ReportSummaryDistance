Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 17?24, Vancouver, October 2005. c?2005 Association for Computational LinguisticsUsing Semantic Relations to Refine Coreference DecisionsHeng Ji David Westbrook Ralph GrishmanDepartment of Computer ScienceNew York UniversityNew York, NY, 10003, USAhengji@cs.nyu.edu westbroo@cs.nyu.edu grishman@cs.nyu.eduAbstractWe present a novel mechanism for im-proving reference resolution by using theoutput of a relation tagger to rescorecoreference hypotheses.
Experimentsshow that this new framework can im-prove performance on two quite differentlanguages -- English and Chinese.1 IntroductionReference resolution has proven to be a majorobstacle in building robust systems for informationextraction, question answering, text summarizationand a number of other natural language processingtasks.Most reference resolution systems use represen-tations built out of the lexical and syntactic attrib-utes of the noun phrases (or ?mentions?)
for whichreference is to be established.
These attributes mayinvolve string matching, agreement, syntactic dis-tance, and positional information, and they tend torely primarily on the immediate context of thenoun phrases (with the possible exception of sen-tence-spanning distance measures such as Hobbsdistance).
Though gains have been made with suchmethods (Tetreault 2001; Mitkov 2000; Soon et al2001; Ng and Cardie 2002), there are clearly caseswhere this sort of local information will not be suf-ficient to resolve coreference correctly.Coreference is by definition a semanticrelationship: two noun phrases corefer if they bothrefer to the same real-world entity.
We shouldtherefore expect a successful coreference system toexploit world knowledge, inference, and otherforms of semantic information in order to resolvehard cases.
If, for example, two nouns refer topeople who work for two different organizations,we want our system to infer that these nounphrases cannot corefer.
Further progress will likelybe aided by flexible frameworks for representingand using the information provided by this kind ofsemantic relation between noun phrases.This paper tries to make a small step in that di-rection.
It describes a robust reference resolver thatincorporates a broad range of semantic informationin a general news domain.
Using an ontology thatdescribes relations between entities (the Auto-mated Content Extraction program1 relation ontol-ogy) along with a training corpus annotated forrelations under this ontology, we first train a classi-fier for identifying relations.
We then apply theoutput of this relation tagger to the task of refer-ence resolution.The rest of this paper is structured as follows.Section 2 briefly describes the efforts made byprevious researchers to use semantic information inreference resolution.
Section 3 describes our ownmethod for incorporating document-level semanticcontext into coreference decisions.
We propose arepresentation of semantic context that isolates aparticularly informative structure of interactionbetween semantic relations and coreference.
Sec-tion 4 explains in detail our strategies for usingrelation information to modify coreference deci-sions, and the linguistic intuitions behind thesestrategies.
Section 5 then presents the system archi-tectures and algorithms we use to incorporate rela-tional information into reference resolution.1The ACE task description can be found athttp://www.itl.nist.gov/iad/894.01/tests/ace/  and the ACE guidelines athttp://www.ldc.upenn.edu/Projects/ACE/17Section 6 presents the results of experiments onboth English and Chinese test data.
Section 7 pre-sents our conclusions and directions for futurework.2 Prior WorkMuch of the earlier work in anaphora resolution(from the 1970?s and 1980?s, in particular) reliedheavily on deep semantic analysis and inferenceprocedures (Charniak 1972; Wilensky 1983;Carbonell and Brown 1988; Hobbs et al 1993).Using these methods, researchers were able to giveaccounts of some difficult examples, often byencoding quite elaborate world knowledge.Capturing sufficient knowledge to provideadequate coverage of even a limited but realisticdomain was very difficult.
Applying thesereference resolution methods to a broad domainwould require a large scale knowledge-engineeringeffort.The focus for the last decade has been primarilyon broad coverage systems using relatively shallowknowledge, and in particular on corpus-trained sta-tistical models.
Some of these systems attempt toapply shallow semantic information.
(Ge et al1998) incorporate gender, number, and animaticityinformation into a statistical model for anaphoraresolution by gathering coreference statistics onparticular nominal-pronoun pairs.
(Tetreault andAllen 2004) use a semantic parser to add semanticconstraints to the syntactic and agreement con-straints in their Left-Right Centering algorithm.
(Soon et al 2001) use WordNet to test the seman-tic compatibility of individual noun phrase pairs.
Ingeneral these approaches do not explore the possi-bility of exploiting the global semantic contextprovided by the document as a whole.Recently Bean and Riloff (2004) have sought toacquire automatically some semantic patterns thatcan be used as contextual information to improvereference resolution, using techniques adaptedfrom information extraction.
Their experimentswere conducted on collections of texts in two topicareas (terrorism and natural disasters).3 Relational Model of Semantic ContextOur central goal is to model semantic and corefer-ence structures in such a way that we can take ad-vantage of a semantic context larger than theindividual noun phrase when making coreferencedecisions.
Ideally, this model should make it possi-ble to pick out important features in the contextand to distinguish useful signals from backgroundnoise.
It should, for example, be able to representsuch basic relational facts as whether the (possiblyidentical) people referenced by two noun phraseswork in the same organization, whether they ownthe same car, etc.
And it should be able to use thisinformation to resolve references even when sur-face features such as lexical or grammatical attrib-utes are imperfect or fail altogether.In this paper we present a Relational Corefer-ence Model (abbreviated as RCM) that makes pro-gress toward these goals.
To represent semanticrelations, we use an ontology (the ACE 2004 rela-tion ontology) that describes 7 main types of rela-tions between entities and 23 subtypes (Table 1).2These relations prove to be more reliable guidesfor coreference than simple lexical context or eventests for the semantic compatibility of heads andmodifiers.
The process of tagging relations implic-itly selects relevant items of context and abstractsraw lists of modifiers into a representation that isdeeper, but still relatively lightweight.Relation Type ExampleAgent-Artifact(ART)Rubin Military Design, themakers of the KurskDiscourse (DISC) each of whomEmployment/Membership(EMP-ORG)Mr. Smith, a senior pro-grammer at MicrosoftPlace-Affiliation(GPE-AFF)Salzburg Red Cross offi-cialsPerson-Social(PER-SOC)relatives of the deadPhysical(PHYS)a town some 50 miles southof SalzburgOther-Affiliation(Other-AFF)Republican senatorsTable 1.
Examples of the ACE Relation TypesGiven these relations we can define a semanticcontext for a candidate mention coreference pair(Mention 1b and Mention 2b) using the structure2See http://www.ldc.upenn.edu/Projects/ACE/docs/Eng-lishRDCV4-3-2.PDF for a more complete description of ACE2004 relations.18depicted in Figure 1.
If both mentions participatein relations, we examine the types and directions oftheir respective relations as well as whether or nottheir relation partners (Mention 1a and Mention2a) corefer.
These values (which correspond to theedge labels in Figure 1) can then be factored into acoreference prediction.
This RCM structureassimilates relation information into a coherentmodel of semantic context.Figure 1.
The RCM structure4 Incorporating Relations into ReferenceResolutionGiven an instance of the RCM structure, we needto convert it into semantic knowledge that can beapplied to a coreference decision.
We approachthis problem by constructing a set of RCM patternsand evaluating the accuracy of each pattern aspositive or negative evidence for coreference.
Theresulting knowledge sources fall into two catego-ries: rules that improve precision by pruning incor-rect coreference links between mentions, and rulesthat improve recall by recovering missed links.To formalize these relation patterns, based onFigure 1, we define the following clauses:A: RelationType1 = RelationType2B: RelationSubType1 = RelationSubType2C: Two Relations have the same directionSame_Relation: CBA ?
?CorefA: Mention1a and Mention2a coreferCorefBMoreLikely: Mention1b and Mention2b aremore likely to coreferCorefBLessLikely: Mention1b and Mention2b areless likely to coreferFrom these clauses we can construct the follow-ing plausible inferences:Rule (1)LikelyCorefBLessCorefAlationSame ??
?Re_Rule (2)LikelyCorefBLessCorefAlationSame ???
Re_Rule (3)LikelyCorefBMoreCorefAlationSame ?
?Re_Rule (1) and (2) can be used to prune corefer-ence links that simple string matching might incor-rectly assert; and (3) can be used to recover missedmention pairs.The accuracy of Rules (1) and (3) varies depend-ing on the type and direction of the particular rela-tion shared by the two noun phrases.
For example,if Mention1a and Mention 2a both refer to thesame nation, and Mentions 1b and 2b participate incitizenship relations (GPE-AFF) with Mentions 1aand 2a respectively, we should not necessarilyconclude that 1b and 2b refer to the same person.If 1a and 2a refer to the same person, however, and1b and 2b are nations in citizenship relations with1a and 2a, then it would indeed be the rare case inwhich 1b and 2b refer to two different nations.
Inother words, the relation of a nation to its citizensis one-to-many.Our system learns broad restrictions like theseby evaluating the accuracy of Rules (1) and (3)when they are instantiated with each possible rela-tion type and direction and used as weak classifi-ers.
For each such instantiation we use cross-validation on our training data to calculate a reli-ability weight defined as:| Correct decisions by rule for given instance || Total applicable cases for given instance |We count the number of correct decisions for arule instance by taking the rule instance as the onlysource of information for coreference resolutionand making only those decisions suggested by therule?s implication (interpreting CorefBMoreLikelyas an assertion that mention 1b and mention 2b doin fact corefer, and interpreting CorefBLessLikelyas an assertion that they do not corefer).Every rule instance with a reliability weight of70% or greater is retained for inclusion in the finalsystem.
Rule (2) cannot be instantiated with asingle type because it requires that the two relationtypes be different, and so we do not perform thisfiltering for Rule (2) (Rule (2) has 97% accuracyacross all relation types).This procedure yields 58 reliable (reliabilityweight > 70%) type instantiations of Rule (1) and(3), in addition to the reliable Rule 2.
We canRelation?Type2/Subtype2Mention1aMention2aCandidateMention1bMention2bRelation?Type1/Subtype1Contexts: Corefer?19recover an additional 24 reliable rules byconjoining additional boolean tests to less reliablerules.
Tests include equality of mention heads,substring matching, absence of temporal key wordssuch as ?current?
and ?former,?
numberagreement, and high confidence for originalcoreference decisions (Mention1b and Mention2b).For each rule below the reliability threshold, wesearch for combinations of 3 or fewer of theserestrictions until we achieve reliability of 70% orwe have exhausted the search space.We give some examples of particular ruleinstances below.Example for Rule (1)Bush campaign officials ... decided to tone down apost-debate rally, and were even considering can-celing it.
?The Bush and Gore campaigns did not talk to eachother directly about the possibility of postpone-ment, but went through the debate commission's di-rector, Janet Brown...Eventually, Brownrecommended that the debate should go on, andneither side objected, according to campaign offi-cials.Two mentions that do not corefer share the samenominal head (?officials?).
We can prune thecoreference link by noting that both occurrences of?officials?
participate in an Employee-Organization (EMP-ORG) relation, while the Or-ganization arguments of these two relation in-stances do not corefer (because the secondoccurrence refers to officials from both cam-paigns).Example for Rule (2)Despite the increases, college remains affordableand a good investment, said College Board Presi-dent Gaston Caperton in a statement with the sur-veys.
?A majority of students need grants or loans -- orboth -- but their exact numbers are unknown, aCollege Board spokesman said.
?Gaston Caperton?
stands in relation EMP-ORG/Employ-Executive with ?College Board?,while "a College Board spokesman" is in relationEMP-ORG/Employ-Staff with the same organiza-tion.
We conclude that ?Gaston Caperton?
does notcorefer with "spokesman.
"Example for Rule (3)In his foreign policy debut for Syria, this SundayBashar Assad met Sunday with Egyptian PresidentHosni Mubarak in talks on Mideast peace and theescalating violence in the Palestinian territories.
?The Syrian leader's visit came on a fourth day ofclashes that have raged in the West Bank, GazaStrip and Jerusalem?
?If we have detected a coreference link between?Syria?
and ?Syrian,?
as well as EMP-ORG/Employ-Executive relations between this countryand two noun phrases ?Bashar Assad?
and?leader?, it is likely that the two mentions bothrefer to the same person.
Without this inference, aresolver might have difficulty detecting thiscoreference link.5 AlgorithmsFigure 2.
System Pipeline (Test Procedure)CoreferenceRulesBaseline MaxentCoref ClassifiersRelationTaggerFinal coreference decisionsEntitiesRelation FeaturesRescoring Coreference DecisionsMentions20In this section we will describe our algorithm forincorporating semantic relation information fromthe RCM into the reference resolver.
In a nutshell,the system applies a baseline statistical resolver togenerate multiple coreference hypotheses, applies arelation tagger to acquire relation information, anduses the relation information to rescore thecoreference hypotheses.
This general system archi-tecture is shown in Figure 2.In section 5.1 below we present our baselinecoreference system.
In Section 5.2 we describe asystem that combines the output of this baselinesystem with relation information to improve per-formance.5.1 Baseline SystemBaseline reference resolverAs the first stage in the resolution process weapply a baseline reference resolver that uses norelation information at all.
This baseline resolvergoes through two successive stages.First, high-precision heuristic rules make somepositive and negative reference decisions.
Rulesinclude simple string matching (e.g., names thatmatch exactly are resolved), agreement constraints(e.g., a nominal will never be resolved with an en-tity that doesn't agree in number), and reliable syn-tactic cues (e.g., mentions in apposition areresolved).
When such a rule applies, it assigns aconfidence value of 1 or 0 to a candidate mention-antecedent pair.The remaining pairs are assigned confidencevalues by a collection of maximum entropy mod-els.
Since different mention types have differentcoreference problems, we separate the system intodifferent models for names, nominals, and pro-nouns.
Each model uses a distinct feature set, andfor each instance only one of these three models isused to produce a probability that the instancerepresents a correct resolution of the mention.When the baseline is used as a standalone system,we apply a threshold to this probability: if someresolution has a confidence above the  threshold,the highest confidence resolution will be made.Otherwise the mention is assumed to be the firstmention of an entity.
When the baseline is used asa component of the system depicted in figure 2, theconfidence value is passed on to the rescoringstage described in 5.2 below.Both the English and the Chinese coreferencemodels incorporate features representing agree-ment of various kinds between noun phrases(number, gender, humanness), degree of stringsimilarity, synonymy between noun phrase heads,measures of distance between noun phrases (suchas the number of intervening sentences), the pres-ence or absence of determiners or quantifiers, anda wide variety of other properties.Relation taggerThe relation tagger uses a K-nearest-neighbor algo-rithm.
We consider a mention pair as a possibleinstance of a relation only when: (1) there is atmost one other mention between their heads, and(2) the coreference probability produced for thepair by the baseline resolver is lower than a thresh-old.
Each training / test example consists of thepair of mentions and the sequence of interveningwords.
We defined a distance metric between twoexamples based on: whether the heads of the mentions match whether the ACE types of the heads of thementions match (for example, both are peopleor both are organizations) whether the intervening words matchTo tag a test example, we find the k nearesttraining examples, use the distance to weight eachneighbor, and then select the most heavilyweighted class in the weighted neighbor set.Name tagger and noun phrase chunkerOur baseline name tagger consists of a HMMtagger augmented with a set of post-processingrules.
The HMM tagger generally follows theNymble model (Bikel et al 1997), but with a largernumber of states (12 for Chinese, 30 for English)to handle name prefixes and suffixes, and, forChinese, transliterated foreign names separately.For Chinese it operates on the output of a wordsegmenter from Tsinghua University.
Our nominalmention tagger (noun phrase chunker) is amaximum entropy tagger trained on treebanksfrom the University of Pennsylvania.5.2 Rescoring stageTo incorporate information from the relation taggerinto the final coreference decision, we split themaxent classification into two stages.
The first21stage simply applies the baseline maxent models,without any relation information, and produces aprobability of coreference.
This probabilitybecomes a feature in the second (rescoring) stageof maxent classification, together with featuresrepresenting the relation knowledge sources.
If ahigh reliability instantiation of one of the RCMrules (as defined in section 4 above) applies to agiven mention-antecedent pair, we include thefollowing features for that pair: the type of theRCM rule, the reliability of the rule instantiation,the relation type and subtype, the direction of therelation, and the tokens for the two mentions.The second stage helps to increase the marginbetween correct and incorrect links and so effectsbetter disambiguation.
See figure 3 below for amore detailed description of the training and test-ing processes.Training1.
Calculate reliability weights of relation knowl-edge sources using cross-validation (for each of kdivisions of training data, train relation tagger on k?
1 divisions, tag relations in remaining divisionand compute reliability of each relation knowledgesource using this division).2.
Use high reliability relation knowledge sourcesto generate relation features for 2nd stage Maxenttraining data.3.
Apply baseline coreference resolver to 2nd stagetraining data.4.
Using output of both 2 and 3 as features, train2nd stage Maxent resolver.Test1.
Tag relations.2.
Convert relation knowledge sources into fea-tures for second stage Maxent models.3.
Use baseline Maxent models to get coreferenceprobabilities for use as features in second stageMaxent models.4.
Using output of 2 and 3 as features for 2nd stageMaxent model, apply 2nd stage resolver to makefinal coreference decisions.Figure 3.
Training and Testing Processes6 Evaluation Results6.1 CorporaWe evaluated our system on two languages:English and Chinese.
The following are thetraining corpora used for the components in thesetwo languages.EnglishFor English, we trained the baseline maxentcoreference model on 311 newswire andnewspaper texts from the ACE 2002 and ACE2003 training corpora.
We trained the relationtagger on 328 ACE 2004 texts.
We used 126newswire texts from the ACE 2004 data to train theEnglish second-stage model, and 65 newswiretexts from the ACE 2004 evaluation set as a test setfor the English system.ChineseFor Chinese, the baseline reference resolver wastrained on 767 texts from ACE 2003 and ACE2004 training data.
Both the baseline relationtagger and the rescoring model were trained on 646texts from ACE 2004 training data.
We used 100ACE texts for a final blind test.6.2 ExperimentsWe used the MUC coreference scoring metric(Vilain et al1995) to evaluate3 our systems.To establish an upper limit for the possibleimprovement offered by our models, we first didexperiments using perfect (hand-tagged) mentionsand perfect relations as inputs.
The algorithms for3In our scoring, we use the ACE keys and only score mentions which appear inboth the key and system response.
This therefore includes only mentions identi-fied as being in the ACE semantic categories by both the key and the systemresponse.
Thus these scores cannot be directly compared against coreferencescores involving all noun phrases.
(Ng 2005) applies another variation on theMUC metric to several systems tested on the ACE data by scoring all responsementions against all key mentions.
For coreference systems that don?t restrictthemselves to mentions in the ACE categories (or that don?t succeed in so re-stricting themselves), this scoring method could lead to some odd effects.
Forexample, systems that recover more correct links could be penalized for thisgreater recall because all links involving non-ACE mentions will be incorrectaccording to the ACE key.
For the sake of comparison, however, we presenthere English system results measured according to this metric: On newswiredata, our baseline had an F of 62.8 and the rescoring method had an F of 64.2.Ng?s best F score (on newspaper data) is 69.3.
The best F score of  the (Ng andCardie 2002)  system (also on newspaper data) is 62.1.
On newswire data the(Ng 2005) system had an F score of 54.7 and the (Ng and Cardie 2002) systemhad an F score of 50.1.
Note that Ng trained and tested these systems on differ-ent ACE data sets than those we used for our experiments.22these experiments are identical to those describedabove except for the omission of the relation taggertraining.
Tables 2 and 3 show the performance ofthe system for English and Chinese.Performance Recall Precision F-measureBaseline 74.5 86.6 80.1Rescoring 78.3 87.0 82.4Table 2.
Performance of English systemwith perfect mentions and perfect relationsPerformance Recall Precision F-measureBaseline 87.5 83.2 85.3Rescoring 88.8 84.7 86.7Table 3.
Performance of Chinese systemwith perfect mentions and perfect relationsWe can see that the relation informationprovided some improvements for both languages.Relation information increased both recall andprecision in both cases.We then performed experiments to evaluate theimpact of coreference rescoring when used withmentions and relations produced by the system.Table 4 and Table 5 list the results.4Performance Recall Precision F-measureBaseline 77.2 87.3 81.9Rescoring 80.3 87.5 83.7Table 4.
Performance of English systemwith system mentions and system relationsPerformance Recall Precision F-measureBaseline 75.0 76.3 75.6Rescoring 76.1 76.5 76.3Table 5.
Chinese system performance withsystem mentions and system relations4Note that, while English shows slightly less relative gain from rescoring whenusing system relations and mentions, all of these scores are higher than theperfect mention/perfect relation scores.
This increase may be a byproduct of thefact that the system mention tagger output contains almost 8% fewer scoreablementions than the perfect mention set (see footnote 3).
With a difference of thismagnitude, the particular mention set selected can be expected to have a sizableimpact on the final scores.The improvement provided by rescoring in trialsusing mentions and relations detected by thesystem is considerably less than the improvementin trials using perfect mentions and relations,particularly for Chinese.
The performance of ourrelation tagger is the most likely cause for thisdifference.
We would expect further gain afterimproving the relation tagger.A sign test applied to a 5-way split of each of thetest corpora indicated that for both languages, forboth perfect and system mentions/relations, thesystem that exploited relation information signifi-cantly outperformed the baseline (at the 95% con-fidence level, judged by F measure).6.3 Error AnalysisErrors made by the RCM rules reveal both thedrawbacks of using a lightweight semanticrepresentation and the inherent difficulty ofsemantic analysis.
Consider the following instance:Card's interest in politics began when he becamepresident of the class of 1965 at Holbrook HighSchool?In 1993, he became president and chiefexecutive of the American Automobile Manufac-turers Association, where he oversaw the lobbyingagainst tighter fuel-economy and air pollution regu-lations for automobiles?The two occurrences of ?president?
should core-fer even though they have EMP-ORG/Employ-Executive relations with two different organiza-tions.
The relation rule (Rule 1) fails here becauseit doesn't take into account the fact that relationschange over time (in this case, the same personfilling different positions at different times).
Inthese and other cases, a little knowledge is a dan-gerous thing: a more complete schema might beable to deal more thoroughly with temporal andother essential semantic dimensions.Nevertheless, performance improvements indi-cate that the rewards of the RCM?s simple seman-tic representation outweigh the risks.7 Conclusion and Future WorkWe have outlined an approach to improving refer-ence resolution through the use of semantic rela-tions, and have described a system which canexploit these semantic relations effectively.
Ourexperiments on English and Chinese data showed23that these small inroads into semantic territory doindeed offer performance improvements.
Further-more, the method is low-cost and not domain-specific.These experiments also suggest that some gainscan be made through the exploration of new archi-tectures for information extraction applications.The ?resolve coreference, tag relations, resolvecoreference?
procedure described above could beseen as one and a half iterations of a ?resolvecoreference then tag relations?
loop.
Seen in thisway, the system poses the question of whether fur-ther gains could be made by pushing the iterativeapproach further.
Perhaps by substituting an itera-tive procedure for the pipeline architecture?s linearsequence of stages we can begin to address theknotty, mutually determining nature of the interac-tion between semantic relations and coreferencerelations.
This approach could be applied morebroadly, to different NLP tasks, and also moredeeply, going beyond the simple one-and-a-half-iteration procedure we present here.
Ultimately, wewould want this framework to boost the perform-ance of each component automatically and signifi-cantly.We also intend to extend our method both tocross-document relation detection and to event de-tection.AcknowledgementsThis research was supported by the Defense Ad-vanced Research Projects Agency under GrantN66001-04-1-8920 from SPAWAR San Diego,and by the National Science Foundation underGrant 03-25657.
This paper does not necessarilyreflect the position or the policy of the U.S. Gov-ernment.ReferencesDavid Bean, Ellen Riloff.
2004.
Unsupervised learningof contextual role knowledge for coreference resolu-tion.
Proc.
HLT-NAACL 2004, pp.
297-304.Daniel M. Bikel, Scott Miller, Richard Schwartz, andRalph Weischedel.
1997.
Nymble: A high-performance learning name-finder.
Proc.
Fifth Conf.on Applied Natural Language Processing, Washing-ton, D.C., pp.
194-201.Carbonell, Jaime and Ralf Brown.
1988.
Anaphora reso-lution: A multi-strategy approach.
Proc.
COLING1988, pp.96-101Eugene Charniak.
1972.
Toward a model of children'sstory comprehension.
Ph.D. thesis, Massachusetts Insti-tute of Technology, Cambridge, MA.Niyu Ge, John Hale and Eugene Charniak.
1998.
A sta-tistical approach to anaphora resolution.
Proc.
theSixth Workshop on Very Large Corpora.Jerry Hobbs, Mark Stickel, Douglas Appelt and PaulMartin.
1993.
Interpretation as abduction.
ArtificialIntelligence, 63, pp.
69-142.Ruslan Mitkov.
2000.
Towards a more consistent andcomprehensive evaluation of anaphora resolution al-gorithms and systems.
Proc.
2nd Discourse Anaph-ora and Anaphora Resolution Colloquium, pp.
96-107Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.Proc.
ACL 2002, pp.104-111Wee Meng Soon, Hwee Tou Ng, and Daniel ChungYong Lim.
2001.
A machine learning approach tocoreference resolution of noun phrases.
Computa-tional Linguistics, Volume 27, Number 4, pp.
521-544Joel R. Tetreault.
2001.
A corpus-based evaluation ofcentering and pronoun resolution.
ComputationalLinguistics, Volume 27, Number 4, pp.
507-520Joel  R. Tetreault and James Allen.
2004.
Semantics,Dialogue, and Pronoun Resolution.
Proc.
CATALOG'04 Barcelona, Spain.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
Proc.
the 6thMessage Understanding Conference (MUC-6).
SanMateo, Cal.
Morgan Kaufmann.Robert Wilensky.
1983.
Planning and Understanding.Addison-Wesley.24
