Proceedings of the Workshop on Distributional Semantics and Compositionality (DiSCo?2011), pages 54?60,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsExemplar-based Word-Space Model for Compositionality Detection: Sharedtask system descriptionSiva ReddyUniversity of York, UKsiva@cs.york.ac.ukSuresh ManandharUniversity of York, UKsuresh@cs.york.ac.ukDiana McCarthyLexical Computing Ltd, UKdiana@dianamccarthy.co.ukSpandana GellaUniversity of York, UKspandana@cs.york.ac.ukAbstractIn this paper, we highlight the problems ofpolysemy in word space models of compo-sitionality detection.
Most models representeach word as a single prototype-based vec-tor without addressing polysemy.
We proposean exemplar-based model which is designedto handle polysemy.
This model is tested forcompositionality detection and it is found tooutperform existing prototype-based models.We have participated in the shared task (Bie-mann and Giesbrecht, 2011) and our best per-forming exemplar-model is ranked first in twotypes of evaluations and second in two otherevaluations.1 IntroductionIn the field of computational semantics, to representthe meaning of a compound word, two mechanismsare commonly used.
One is based on the distribu-tional hypothesis (Harris, 1954) and the other is onthe principle of semantic compositionality (Partee,1995, p. 313).The distributional hypothesis (DH) states thatwords that occur in similar contexts tend to havesimilar meanings.
Using this hypothesis, distribu-tional models like the Word-space model (WSM,Sahlgren, 2006) represent a target word?s meaningas a context vector (location in space).
The simi-larity between two meanings is the closeness (prox-imity) between the vectors.
The context vector of atarget word is built from its distributional behaviourobserved in a corpus.
Similarly, the context vector ofa compound word can be built by treating the com-pound as a single word.
We refer to such a vector asa DH-based vector.The other mechanism is based on the principle ofsemantic compositionality (PSC) which states thatthe meaning of a compound word is a function of,and only of, the meaning of its parts and the wayin which the parts are combined.
If the meaning ofa part is represented in a WSM using the distribu-tional hypothesis, then the principle can be appliedto compose the distributional behaviour of a com-pound word from its parts without actually using thecorpus instances of the compound.
We refer to thisas a PSC-based vector.
So a PSC-based is composedof component DH-based vectors.Both of these two mechanisms are capable of de-termining the meaning vector of a compound word.For a given compound, if a DH-based vector anda PSC-based vector of the compound are projectedinto an identical space, one would expect the vec-tors to occupy the same location i.e.
both the vectorsshould be nearly the same.
However the principleof semantic compositionality does not hold for non-compositional compounds, which is actually whatthe existing WSMs of compositionality detection ex-ploit (Giesbrecht, 2009; Katz and Giesbrecht, 2006;Schone and Jurafsky, 2001).
The DH-based andPSC-based vectors are expected to have high simi-larity when a compound is compositional and lowsimilarity for non-compositional compounds.Most methods in WSM (Turney and Pantel, 2010)represent a word as a single context vector built frommerging all its corpus instances.
Such a representa-tion is called the prototype-based modelling (Mur-phy, 2002).
These prototype-based vectors do not54distinguish the instances according to the senses ofa target word.
Since most compounds are less am-biguous than single words, there is less need for dis-tinguishing instances in a DH-based prototype vec-tor of a compound and we do not address that herebut leave ambiguity of compounds for future work.However the constituent words of the compound aremore ambiguous.
When DH-based vectors of theconstituent words are used for composing the PSC-based vector of the compound, the resulting vec-tor may contain instances, and therefore contexts,that are not relevant for the given compound.
Thesenoisy contexts effect the similarity between the PSC-based vector and the DH-based vector of the com-pound.
Basing compositionality judgements on asuch a noisy similarity value is no longer reliable.In this paper, we address this problem of pol-ysemy of constituent words of a compound usingan exemplar-based modelling (Smith and Medin,1981).
In exemplar-based modelling of WSM (Erkand Pado?, 2010), each word is represented by all itscorpus instances (exemplars) without merging theminto a single vector.
Depending upon the purpose,only relevant exemplars of the target word are acti-vated and then these are merged to form a refinedprototype-vector which is less-noisy compared tothe original prototype-vector.
Exemplar-based mod-els are more powerful than prototype-based ones be-cause they retain specific instance information.We have evaluated our models on the validationdata released in the shared task (Biemann and Gies-brecht, 2011).
Based on the validation results, wehave chosen three systems for public evaluation andparticipated in the shared task (Biemann and Gies-brecht, 2011).2 Word Space ModelIn this section, construction of WSM for all our ex-periments is described.
We use Sketch Engine1 (Kil-garriff et al, 2004) to retrieve all the exemplars fora target word or a pattern using corpus query lan-guage.
Let w1 w2 be a compound word with con-stituent words w1 and w2.
Ew denotes the set ofexemplars of w. Vw is the prototype vector of theword w, which is built by merging all the exemplarsin Ew1Sketch Engine http://www.sketchengine.co.ukFor the purposes of producing a PSC-based vectorfor a compound, a vector of a constituent word isbuilt using only the exemplars which do not containthe compound.
Note that the vectors are sensitiveto a compound?s word-order since the exemplars ofw1 w2 are not the same as w2 w1.We use other WSM settings following Mitchelland Lapata (2008).
The dimensions of the WSMare the top 2000 content words in the given corpus(along with their coarse-grained part-of-speech in-formation).
Cosine similarity (sim) is used to mea-sure the similarity between two vectors.
Values atthe specific positions in the vector representing con-text words are set to the ratio of the probability ofthe context word given the target word to the overallprobability of the context word.
The context windowof a target word?s exemplar is the whole sentence ofthe target word excluding the target word.
Our lan-guage of interest is English.
We use the ukWaC cor-pus (Ferraresi et al, 2008) for producing out WSMs.3 Related WorkAs described in Section 1, most WSM models forcompositionality detection measure the similaritybetween the true distributional vector Vw1w2 of thecompound and the composed vector Vw1?w2 , where?
denotes a compositionality function.
If the simi-larity is high, the compound is treated as composi-tional or else non-compositional.Giesbrecht (2009); Katz and Giesbrecht (2006);Schone and Jurafsky (2001) obtained the compo-sitionality vector of w1 w2 using vector additionVw1?w2 = aVw1 + bVw2 .
In this approach, ifsim(Vw1?w2 , Vw1w2) > ?, the compound is clas-sified as compositional, where ?
is a threshold fordeciding compositionality.
Global values of a and bwere chosen by optimizing the performance on thedevelopment set.
It was found that no single thresh-old value ?
held for all compounds.
Changing thethreshold alters performance arbitrarily.
This mightbe due to the polysemous nature of the constituentwords which makes the composed vector Vw1?w2filled with noisy contexts and thus making the judge-ment unpredictable.In the above model, if a=0 and b=1, the result-ing model is similar to that of Baldwin et al (2003).They also observe similar behaviour of the thresh-55old ?.
We try to address this problem by addressingthe polysemy in WSMs using exemplar-based mod-elling.The above models use a simple addition basedcompositionality function.
Mitchell and Lapata(2008) observed that a simple multiplication func-tion modelled compositionality better than addi-tion.
Contrary to that, Guevara (2011) observedadditive models worked well for building composi-tional vectors.
In our work, we try using evidencefrom both compositionality functions, simple addi-tion and simple multiplication.Bannard et al (2003); McCarthy et al (2003) ob-served that methods based on distributional similar-ities between a phrase and its constituent words helpwhen determining the compositionality behaviour ofphrases.
We therefore also use evidence from thesimilarities between each constituent word and thecompound.4 Our Approach: Exemplar-based ModelOur approach works as follows.
Firstly, given acompound w1 w2, we build its DH-based proto-type vector Vw1w2 from all its exemplars Ew1w2 .Secondly, we remove irrelevant exemplars in Ew1and Ew2 of constituent words and build the refinedprototype vectors Vwr1 and Vwr2 of the constituentwords w1 and w2 respectively.
These refined vec-tors are used to compose the PSC-based vectors 2 ofthe compound.
Related work to ours is (Reisingerand Mooney, 2010) where exemplars of a word arefirst clustered and then prototype vectors are built.This work does not relate to compositionality but tomeasuring semantic similarity of single words.
Assuch, their clusters are not influenced by other wordswhereas in our approach for detecting composition-ality, the other constituent word plays a major role.We use the compositionality functions, sim-ple addition and simple multiplication to buildVwr1+wr2 and Vwr1?wr2 respectively.
Based onthe similarities sim(Vw1w2 , Vwr1), sim(Vw1w2 , Vwr2),sim(Vw1w2 , Vwr1+wr2) and sim(Vw1w2 , Vwr1?wr2), wedecide if the compound is compositional or non-compositional.
These steps are described in a littlemore detail below.2Note that we use two PSC-based vectors for representing acompound.4.1 Building Refined Prototype VectorsWe aim to remove irrelevant exemplars of one con-stituent word with the help of the other constituentword?s distributional behaviour.
For example, letus take the compound traffic light.
Light occursin many contexts such as quantum theory, optics,lamps and spiritual theory.
In ukWaC, light has316,126 instances.
Not all these exemplars are rel-evant to compose the PSC-based vector of trafficlight.
These irrelevant exemplars increases the se-mantic differences between traffic light and light andthus increase the differences between Vtraffic?lightand Vtraffic light.
sim(Vlight, Vtraffic light) is found to be0.27.Our intuition and motivation for exemplar re-moval is that it is beneficiary to choose only theexemplars of light which share similar contexts oftraffic since traffic light should have contexts sim-ilar to both traffic and light if it is compositional.We rank each exemplar of light based on commonco-occurrences of traffic and also words which aredistributionally similar to traffic.
Co-occurrences oftraffic are the context words which frequently occurwith traffic, e.g.
car, road etc.
Using these, theexemplar from a sentence such as ?Cameras capturecars running red lights .
.
.?
will be ranked higherthan one which does not have contexts related totraffic.
The distributionally similar words to trafficare the words (like synonyms, antonyms) which aresimilar to traffic in that they occur in similar con-texts, e.g.
transport, flow etc.
Using these distri-butionally similar words helps reduce the impact ofdata sparseness and helps prioritise contexts of traf-fic which are semantically related.
We use SketchEngine to compute the scores of a word observedin a given corpus.
Sketch Engine scores the co-occurrences (collocations) using logDice motivatedby (Curran, 2003) and distributionally related wordsusing (Rychly?
and Kilgarriff, 2007; Lexical Com-puting Ltd., 2007).
For a given word, both of thesescores are normalised in the range (0,1)All the exemplars of light are ranked based onthe co-occurrences of these collocations and distri-butionally related words of traffic usingstrafficE ?
Elight =?c ?
ExEc ?
ytrafficc (1)where strafficE ?
Elight stands for the relevance score of the56exemplar E w.r.t.
traffic, c for context word in theexemplar E, xEc is the coordinate value (contextualscore) of the context word c in the exemplar E andytrafficc is the score of the context word c w.r.t.
traffic.A refined prototype vector of light is then built bymerging the top n exemplars of lightVlightr =n?ei?Etrafficlight ;i=0ei (2)where Etrafficlight are the set of exemplars of lightranked using co-occurrence information from theother constituent word traffic.
n is chosen such thatsim(Vlightr , Vtraffic light) is maximised.
This similar-ity is observed to be greatest using just 2286 (lessthan 1%) of the total exemplars of light.
After ex-emplar removal, sim(Vlightr , Vtraffic light) increased to0.47 from the initial value of 0.27.
Though n is cho-sen by maximising similarity, which is not desirablefor non-compositional compounds, the lack of simi-larity will give the strongest possible indication thata compound is not compositional.4.2 Building Compositional VectorsWe use the compositionality functions, simple ad-dition and simple multiplication to build composi-tional vectors Vwr1+wr2 and Vwr1?wr2 .
These are as de-scribed in (Mitchell and Lapata, 2008).
In model ad-dition, Vw1?w2 = aVw1 + bVw2 , all the previous ap-proaches use static values of a and b.
Instead, we usedynamic weights computed from the participatingvectors using a =sim(Vw1w2 ,Vw1 )sim(Vw1w2 ,Vw1 )+sim(Vw1w2 ,Vw2 )and b = 1?a.
These weights differ from compoundto compound.4.3 Compositionality JudgementTo judge if a compound is compositional or non-compositional, previous approaches (see Section 3)base their judgement on a single similarity value.
Asdiscussed, we base our judgement based on the col-lective evidences from all the similarity values usinga linear equation of the form?
(Vwr1 , Vwr2) = a0 + a1.sim(Vw1w2 , Vwr1)+ a2.sim(Vw1w2 , Vwr2) (3)+ a3.sim(Vw1w2 , Vwr1+wr2)+ a4.sim(Vw1w2 , Vwr1?wr2)Model APD Acc.Exm-Best 13.09 88.0Pro-Addn 15.42 76.0Pro-Mult 17.52 80.0Pro-Best 15.12 80.0Table 1: Average Point Difference (APD) and Av-erage Accuracy (Acc.)
of Compositionality Judge-mentswhere the value of ?
denotes the compositionalityscore.
The range of ?
is in between 0-100.
If ?
?34, the compound is treated as non-compositional,34 < ?
< 67 as medium compositional and ?
?67 as highly compositional.
The parameters ai?sare estimated using ordinary least square regressionby training over the training data released in theshared task (Biemann and Giesbrecht, 2011).
Forthe three categories ?
adjective-noun, verb-objectand subject-verb ?
the parameters are estimated sep-arately.Note that if a1 = a2 = a4 = 0, the model basesits judgement only on addition.
Similarly if a1 =a2 = a3 = 0, the model bases its judgement only onmultiplication.We also experimented with combinations such as?
(Vwr1 , Vw2) and ?
(Vw1 , Vwr2) i.e.
using refined vec-tor for one of the constituent word and the unrefinedprototype vector for the other constituent word.4.4 Selecting the best modelTo participate in the shared task, we have selectedthe best performing model by evaluating the mod-els on the validation data released in the shared task(Biemann and Giesbrecht, 2011).
Table 1 displaysthe results on the validation data.
The average pointdifference is calculated by taking the average of thedifference in a model?s score ?
and the gold scoreannotated by humans, over all compounds.
Table 1also displays the overall accuracy of coarse grainedlabels ?
low, medium and high.Best performance for verb(v)-object(o) com-pounds is found for the combination ?
(Vvr , Vor) ofEquation 3.
For subject(s)-verb(v) compounds, it isfor ?
(Vsr , Vvr) and a3 = a4 = 0.
For adjective(j)-noun(n) compounds, it is ?
(Vjr , Vn).
We are notcertain of the reason for this difference, perhapsthere may be less ambiguity of words within specificgrammatical relationships or it may be simply due to57TotPrd Spearman ?
Kendalls ?Rand-Base 174 0.02 0.02Exm-Best 169 0.35 0.24Pro-Best 169 0.33 0.23Exm 169 0.26 0.18SharedTaskNextBest 174 0.33 0.23Table 2: Correlation Scoresthe actual compounds in those categories.
We leaveanalysis of this for future work.
We combined theoutputs of these category-specific models to buildthe best model Exm-Best.For comparison, results of standard mod-els prototype addition (Pro-Addn) and prototype-multiplication (Pro-Mult) are also displayed in Table1.
Pro-Addn can be represented as ?
(Vw1 , Vw2) witha1 = a2 = a4 = 0.
Pro-Mult can be represented as?
(Vw1 , Vw2) with a1 = a2 = a3 = 0.
Pro-Best isthe best performing model in prototype-based mod-elling.
It is found to be ?
(Vw1 , Vw2).
(Note: De-pending upon the compound type, some of the ai?sin Pro-Best may be 0).Overall, exemplar-based modelling excelled inboth the evaluations, average point difference andcoarse-grained label accuracies.
The systems Exm-Best, Pro-Best and Exm ?
(Vwr1 , Vwr2) were submit-ted for the public evaluation in the shared task.
Allthe model parameters were estimated by regressionon the task?s training data separately for the 3 com-pound types as described in Section 4.3.
We performthe regression separately for these classes to max-imise performance.
In the future, we will investigatewhether these settings gave us better results on thetest data compared to setting the values the same re-gardless of the category of compound.5 Shared Task ResultsTable 2 displays Spearman ?
and Kendalls ?
corre-lation scores of all the models.
TotPrd stands forthe total number of predictions.
Rand-Base is thebaseline system which randomly assigns a compo-sitionality score for a compound.
Our model Exm-Best was the best performing system compared toall other systems in this evaluation criteria.
Shared-TaskNextBest is the next best performing systemapart from our models.
Due to lemmatization er-rors in the test data, our models could only predictjudgements for 169 out of 174 compounds.All ADJ-NN V-SUBJ V-OBJRand-Base 32.82 34.57 29.83 32.34Zero-Base 23.42 24.67 17.03 25.47Exm-Best 16.51 15.19 15.72 18.6Pro-Best 16.79 14.62 18.89 18.31Exm 17.28 15.82 18.18 18.6SharedTaskBest 16.19 14.93 21.64 14.66Table 3: Average Point Difference ScoresAll ADJ-NN V-SUBJ V-OBJRand-Base 0.297 0.288 0.308 0.30Zero-Base 0.356 0.288 0.654 0.25Most-Freq-Base 0.593 0.673 0.346 0.65Exm-Best 0.576 0.692 0.5 0.475Pro-Best 0.567 0.731 0.346 0.5Exm 0.542 0.692 0.346 0.475SharedTaskBest 0.585 0.654 0.385 0.625Table 4: Coarse Grained AccuracyTable 3 displays average point difference scores.Zero-Base is a baseline system which assigns a scoreof 50 to all compounds.
SharedTaskBest is the over-all best performing system.
Exm-Best was rankedsecond best among all the systems.
For ADJ-NNand V-SUBJ compounds, the best performing sys-tems in the shared task are Pro-Best and Exm-Bestrespectively.
Our models did less well on V-OBJcompounds and we will explore the reasons for thisin future work.Table 4 displays coarse grained scores.
As above,similar behaviour is observed for coarse grained ac-curacies.
Most-Freq-Base is the baseline systemwhich assigns the most frequent coarse-grained la-bel for a compound based on its type (ADJ-NN, V-SUBJ, V-OBJ) as observed in training data.
Most-Freq-Base outperforms all other systems.6 ConclusionsIn this paper, we examined the effect of polysemyin word space models for compositionality detec-tion.
We showed exemplar-based WSM is effectivein dealing with polysemy.
Also, we use multipleevidences for compositionality detection rather thanbasing our judgement on a single evidence.
Over-all, performance of the Exemplar-based models ofcompositionality detection is found to be superior toprototype-based models.58ReferencesBaldwin, T., Bannard, C., Tanaka, T., and Widdows,D.
(2003).
An empirical model of multiword ex-pression decomposability.
In Proceedings of theACL 2003 workshop on Multiword expressions:analysis, acquisition and treatment - Volume 18,MWE ?03, pages 89?96, Stroudsburg, PA, USA.Association for Computational Linguistics.Bannard, C., Baldwin, T., and Lascarides, A.
(2003).A statistical approach to the semantics of verb-particles.
In Proceedings of the ACL 2003 work-shop on Multiword expressions: analysis, ac-quisition and treatment - Volume 18, MWE ?03,pages 65?72, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Biemann, C. and Giesbrecht, E. (2011).
Distri-butional semantics and compositionality 2011:Shared task description and results.
In Pro-ceedings of DISCo-2011 in conjunction with ACL2011.Curran, J. R. (2003).
From distributional to semanticsimilarity.
Technical report, PhD Thesis, Univer-sity of Edinburgh.Erk, K. and Pado?, S. (2010).
Exemplar-based mod-els for word meaning in context.
In Proceed-ings of the ACL 2010 Conference Short Papers,ACLShort ?10, pages 92?97, Stroudsburg, PA,USA.
Association for Computational Linguistics.Ferraresi, A., Zanchetta, E., Baroni, M., and Bernar-dini, S. (2008).
Introducing and evaluatingukwac, a very large web-derived corpus of en-glish.
In Proceedings of the WAC4 Workshop atLREC 2008, Marrakesh, Morocco.Giesbrecht, E. (2009).
In search of semantic com-positionality in vector spaces.
In Proceedingsof the 17th International Conference on Concep-tual Structures: Conceptual Structures: Leverag-ing Semantic Technologies, ICCS ?09, pages 173?184, Berlin, Heidelberg.
Springer-Verlag.Guevara, E. R. (2011).
Computing semantic com-positionality in distributional semantics.
In Pro-ceedings of the Ninth International Conference onComputational Semantics, IWCS ?2011.Harris, Z. S. (1954).
Distributional structure.
Word,10:146?162.Katz, G. and Giesbrecht, E. (2006).
Automaticidentification of non-compositional multi-wordexpressions using latent semantic analysis.
InProceedings of the Workshop on Multiword Ex-pressions: Identifying and Exploiting Underly-ing Properties, MWE ?06, pages 12?19, Strouds-burg, PA, USA.
Association for ComputationalLinguistics.Kilgarriff, A., Rychly, P., Smrz, P., and Tugwell, D.(2004).
The sketch engine.
In Proceedings of EU-RALEX.Lexical Computing Ltd. (2007).
Statistics used inthe sketch engine.McCarthy, D., Keller, B., and Carroll, J.
(2003).Detecting a continuum of compositionality inphrasal verbs.
In Proceedings of the ACL 2003workshop on Multiword expressions: analysis,acquisition and treatment - Volume 18, MWE ?03,pages 73?80, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Mitchell, J. and Lapata, M. (2008).
Vector-basedModels of Semantic Composition.
In Proceed-ings of ACL-08: HLT, pages 236?244, Columbus,Ohio.
Association for Computational Linguistics.Murphy, G. L. (2002).
The Big Book of Concepts.The MIT Press.Partee, B.
(1995).
Lexical semantics and compo-sitionality.
L. Gleitman and M. Liberman (eds.
)Language, which is Volume 1 of D. Osherson (ed.
)An Invitation to Cognitive Science (2nd Edition),pages 311?360.Reisinger, J. and Mooney, R. J.
(2010).
Multi-prototype vector-space models of word mean-ing.
In Proceedings of the 11th Annual Confer-ence of the North American Chapter of the As-sociation for Computational Linguistics (NAACL-2010), pages 109?117.Rychly?, P. and Kilgarriff, A.
(2007).
An efficientalgorithm for building a distributional thesaurus(and other sketch engine developments).
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Ses-sions, ACL ?07, pages 41?44, Stroudsburg, PA,USA.
Association for Computational Linguistics.Sahlgren, M. (2006).
The Word-Space Model: Us-ing distributional analysis to represent syntag-59matic and paradigmatic relations between wordsin high-dimensional vector spaces.
PhD thesis,Stockholm University.Schone, P. and Jurafsky, D. (2001).
Is knowledge-free induction of multiword unit dictionary head-words a solved problem?
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?01.Smith, E. E. and Medin, D. L. (1981).
Categoriesand concepts / Edward E. Smith and Douglas L.Medin.
Harvard University Press, Cambridge,Mass.
:.Turney, P. D. and Pantel, P. (2010).
From frequencyto meaning: vector space models of semantics.
J.Artif.
Int.
Res., 37:141?188.60
