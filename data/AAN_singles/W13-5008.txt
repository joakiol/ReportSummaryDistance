Proceedings of the TextGraphs-8 Workshop, pages 53?60,Seattle, Washington, USA, 18 October 2013. c?2013 Association for Computational LinguisticsGraph-Structures Matching for Review Relevance IdentificationLakshmi Ramachandran and Edward F. GehringerNorth Carolina State University{lramach, efg}@ncsu.eduAbstractReview quality is determined by identifyingthe relevance of a review to a submission(the article or paper the review was writtenfor).
We identify relevance in terms of the se-mantic and syntactic similarities between twotexts.
We use a word order graph, whose ver-tices, edges and double edges help determinestructure-based match across texts.
We useWordNet to determine semantic relatedness.Ours is a lexico-semantic approach, which pre-dicts relevance with an accuracy of 66% andf -measure of 0.67.1 IntroductionReviews play a critical role in making decisions, e.g.,for grading students, accepting manuscripts for publi-cation, or funding grants.
Therefore, we must ensurethat the decision-making party finds the review?s con-tent useful.
Kuhne et al(2010) found that authors werecontented with reviewers who made an effort to under-stand their work.
Nelson and Schunn (2009) found thatreviews locating problems in the author?s work, or pro-viding suggestions for improvement help authors un-derstand and use feedback effectively.We investigated peer reviews from Expertiza, a web-based collaborative learning application (Gehringer,2010).
We found that reviewers provide commentssuch as, ?Yes, it is good!
It is very well organized.
?Such a review does not contain any unique information,or reference a specific concept or object in the author?ssubmission.
Such a generic review could work for anysubmission.
Consider the comment, ?I felt that some ofthe examples were cliche?d.?
The reviewer criticizes the?examples?
in the author?s work but does not explainwhy they find the example ?cliche?d?.A review?s quality may be assessed with the help ofseveral metrics such as relevance of a review to the sub-mission, its content type, coverage, tone, quantity offeedback provided (Ramachandran, 2011).
In this pa-per we focus on the study of one review quality metric- review relevance.A relevant review paraphrases the concepts de-scribed in a submission, with possible descriptions ofproblems identified in the author?s work.
Our aim is toidentify whether a review is relevant to the work it waswritten for.While paraphrasing, an idea may be restated by thereviewer with possible lexical and syntactic changes tothe text.
According to Liu et al(2009), a good para-phrase, while preserving the original meaning of thetext should contain some syntactic changes.
Accord-ing to Boonthum (2004) patterns followed commonlywhile paraphrasing include lexical synonymy, changein voice and change in sentence structure.
Therefore,conventional text matching approaches, which look forexact matches, may not be good at identifying rele-vance.2 Definition of RelevanceDefinition Let S be the set of sentences in the text un-der review (the submission) and R be the set of reviewsentences.
Let s and r represent a sentence in the sub-mission and review respectively.relevance(S,R) =1|R|?
?r?R{argmax?s?S( lexicoSemSim(s, r))} (1)lexicoSemSim(s,r) represents the lexico-semanticmatch between s and r. Relevance is the average of thebest lexico-semantic matches of a review?s sentenceswith corresponding submission sentences.
The mean-ing and usage of lexicoSemSim has been explained indetail in Section 6.
We acknowledge that all reviewsentences may not have corresponding matches in thesubmission.
Our aim is only to identify the proportionof review text that is lexico-semantically relevant to asubmission.Since our aim is to identify the lexico-semanticmatch between texts, we need a representation that cap-tures the syntax or order of tokens in a text.
Hence weuse a word order graph.
Word order graphs are suitedfor identifying lexical and voice changes, which arecommon among paraphrased text.
Similarity shouldcapture the degree of relatedness between texts.
Hencewe use a WordNet-based metric (Fellbaum, 1998).Figure 1 contains a sample submission and threesample reviews.
The first review has some instances ofexact match with the submission and is therefore rele-vant to the submission.
However, the relevance of thesecond review may not be determined by a text overlaps53Figure 1: The figure contains a sample submission, tworelevant reviews ?
one with overt text matches and an-other that is lexico-semantically similar to the submis-sion, and a non-relevant review.match.
The third review is lexico-semantically distinctfrom the submission.3 Related WorkThere is little previous work in the area of identifyingrelevance between a review and a submission.
Xiongand Litman (2011) use shallow metrics such as noun,verb count to identify review helpfulness.
Their ap-proach does not check for presence of paraphrases orsummaries in a review.
Ours is a pioneering effort inthe application of relevance identification to the studyof review helpfulness.In this section we list some related work in the areaof text matching, with a focus on approaches that usegraphs such as lexical chains or dependency trees torepresent text.
Haghighi et al(2005) use dependencytrees to determine text entailment.
They use node andpath substitutions to compare text graphs.Vertices in a dependency tree represent words, andedges capture the asymmetric dependency relation-ships between a head word and its modifier.
Figure 2(a)contains a dependency tree representation (Bohnet,2010) for the text ?The paper presented the importantconcepts.?
We see that every token in the text is a ver-tex in the tree and edges depict governance relations(head ?
modifier).
For example, ?presented?
is theroot of this sentence and the edge between ?presented?and ?paper?
signifies a subject relationship (SBJ).
De-pendency trees may not capture ordering information.For instance when we read the edges of the dependencytree in Figure 2(a) we get presented?
paper, presented?
concepts.
The order of words in the edges is re-versed, as in the case of presented ?
paper, althoughthe actual order in the text is paper?
presented.The corresponding word order graph representationin Figure 2(b) captures the order of the words.
The(a) Dependency tree(b) Word order graphFigure 2: Displaying the ordering difference between adependency tree representation and a word order repre-sentation for the text ?The paper presented the impor-tant concepts.
?word order graph captures SBJ?OBJ ordering as inpaper?presented?concepts, which the directed edgesin a dependency tree do not capture.
Thus dependencytree representations may not be a useful representationin studying lexical or word order changes across docu-ments.Mani and Bloedorn (1997) suggest a graph searchand matching approach for multi-document summa-rization.
The graph matching approach used byMani and Bloedorn focuses on concept or topics-basedmatching (noun entities).
The graph captures adja-cency relations between concepts or topics.
Theirgraph representation does not capture ordering infor-mation, which would be suited for tasks involving com-parison of lexical-order changes.
As noted earlier, textmatching with possible changes in word order is es-sential for a task like relevance identification.
Existingrepresentations and matching techniques do not capturethis information.
Van et al(2009) construct phrasenets using regular expressions.
Phrase nets are con-structed for specific relations between tokens e.g.
?X atY?
may indicate location of object X.
Phrase nets areused as a tool for visualizing relations between objectsin literary texts.The document index graph (DIG) used by Ham-mouda and Kamel (2002), capture phrases of a doc-ument.
Although the DIG captures order of wordswithin a phrase, it does not capture the order of phraseswithin a document.
As a result this representation doesnot capture complete sentence structure information,which may be necessary to identify whether a reviewcontains sentence structure changes.Mihalcea (2004) uses a graph to perform sentenceextraction and summarization.
Vertices in the graphrepresent sentences in a document.
Weighted graphedges represent the degree of overlap across content ofthe sentences.Kauchak and Barzilay (2006) suggest an auto-mated technique to create paraphrases for human and54machine-translated text pairs, by substituting words inmachine translated texts with their corresponding syn-onyms.
They define paraphrases primarily in terms ofsynonyms of individual tokens.Although there do exist independent research worksthat discuss graph-based summarization and paraphras-ing techniques, they use content overlap or synonymmatches to determine paraphrases.
They do not con-sider context during text comparison.
Our work is anamalgamation of existing research in the areas of textmatching and paraphrase recognition.4 Graph RepresentationIn a word order graph, edges represent relations be-tween contiguous vertices.
The graph captures word orphrase order of the text.
Figure 2(b) contains the graphrepresentation for a review.A word order graph is suitable for applications thatidentify relevance or paraphrases across texts.
Para-phrases may contain lexical changes and word orphrase shuffling across a text?s length.
Graph matchesidentify the presence or absence of lexical changes us-ing the ordering and context that the word order graphscapture.
A detailed description of the graph gener-ation algorithm can be found in Ramachandran andGehringer (2012).1.
The graph generator takes a piece of text as inputand generates a graph as its output.
We use period(.
), semicolons (;) or exclamations (!)
to break thetext into multiple segments1.
A text segment is acomplete grammatical unit that can stand indepen-dent of the other clauses in the sentence in termsof its meaning.2.
The text is then tagged with parts-of-speech (POS)(NN, DT, VB, RB2 etc.).
We use the Stan-ford NLP POS tagger to generate the tagged text(Toutanova et al 2003).
POS tags are usefulin determining how to group words into phraseswhile still maintaining the order.3.
We use a heuristic phrase chunking technique3to group consecutive subject components (nouns,prepositions etc.)
into a subject vertex, consecu-tive verbs (or modals) into a verb vertex, and sim-ilarly for adverb and adjective vertices.
A graphvertex may contain a phrase or a token.4.
When a verb vertex is created the algorithm looksfor the last created subject vertex to form an edgebetween the two.
Ordering is maintained when anedge is created, i.e., if a subject vertex was formed1Approach used is similar to that of the determinis-tic sentence splitter used by the Stanford NLP sentencesplitter.
http://nlp.stanford.edu/software/tokenizer.shtml2NN - noun, DT - determiner, VB(Z) - verb, RB - adverb3Our chunker groups words based on the POS tags with-out the overhead of training a model to perform chunking.before a verb vertex a subject?verb edge is cre-ated, else a verb?object edge is created.
An ad-jective or an adverb is attached to the subject orverb vertex found in the sentence (i.e., subject?adjective or verb?adverb edge).5.
We tag graph edges with dependencies (Bohnet,2010).
We use the anna library available as partof the mate tools package to identify dependen-cies.
Labels indicate the relation between wordsand their modifiers (e.g.
SBJ ?
subject?verb re-lationship, OBJ ?
verb?object relationship).
Postedge creation, we iterate through all edges todetermine whether a dependency exists betweenthe tokens representing the edge?s vertices.
Weadd an edge label if a dependency exists, e.g.,?concepts?important?
in Figure 2(b) captures thenoun-modifier (NMOD) relation.
Labels capturethe grammatical role played by tokens in a text.5 Semantic RelatednessMatch between two tokens could be one of: (1) ex-act, (2) synonym, (3) hypernym or hyponym (moregeneric or specific), (4) meronym or holonym (sub-part or whole) (5) presence of common parents (exclud-ing generic parents such as ?object?, ?entity?
), and (6)overlaps across definitions or examples of compared to-kens4, or (7) distinct or non-match.
Each match is givena weight value, which represents its degree of impor-tance, e.g., exact matches are more important than syn-onym matches, which are in turn more important thanhypernyms or hyponyms and so on.
Weight values arein the [0-6] range, 0 being the lowest match (distinct)and 6 the best match (exact).
Unlike other approaches,which capture just exact or synonymy matches, our ap-proach captures semantic relatedness between tokensusing a few types of matches (Ramachandran, 2013).Each match is identified using WordNet.
WordNethas been used successfully to measure relatedness byAgirre et al(2009).
We use WordNet because itis faster than querying a knowledge source such asWikipedia, which contains more than a million articles,not all of which may be relevant.6 Lexico-Semantic MatchingThe degree of match between two graphs depends onthe degree of match between their vertices and edges.In this section we describe three types of matchesacross graphs - (1) phrase or token matching, (2) con-text matching, and (3) sentence structure matching.Figure 3 contains an overview of our relevance iden-tification approach.6.1 Phrase or token matchingIn phrase or token matching, vertices containingphrases or tokens are compared across graphs.
This4Using context to match tokens was an approach used byLesk (1986) for word-sense disambiguation.55Figure 3: Overview of our approach for relevance iden-tification task.matching succeeds in capturing semantic relatednessbetween single or compound words.
When vertices?concepts?
and ?points?
are compared, a common par-ents match is found.
This match would have beenmissed when using only an exact or synonym match.Phrase(S, R) =1|Vr|?
?r(v)?Vrargmax?s(v)?Vs{match(s(v), r(v))}(2)An overall phrase match is determined by taking theaverage of the best match that every review phrase haswith a corresponding submission phrase.
Similaritybetween two vertices is calculated as the average ofmatches between their constituent words or phrases.Match could be one of those listed in Section 5.
InEquation 2, r(v) and s(v) refer to review and submis-sion vertices respectively, and Vr and Vs is the set ofvertices in a review and a submission.6.2 Context matchingContext matching compares edges with same and dif-ferent syntax, and edges of different types across twotext graphs.
We refer to the match as context matchingsince contiguous phrases (vertices) are chosen from agraph for comparison with another, i.e., more context.Relatedness between edges is the average of the vertexmatches.
We compare edge labels for matches retain-ing word order.
Edge labels capture grammatical rela-tions, and play an important role in matching.
Henceif edges have the same labels then the average match isretained, else the match is halved.
Some of the context-based matches include:?
Ordered match - Ordered match preserves theorder of phrases in a text.
We compare sametype edges5 with the same vertex order.
Figure4(a) shows the comparison of single edges fromtwo review graphs.
A match is identified betweenedges ?important?concepts?
and ?necessary?points?, because they capture the noun-modifierrelationship (NMOD), and because a relation ex-ists between tokens ?concepts?
and ?points?.5Same type edges are edges with same types of vertices.?
Lexical change - Lexical match flips the order ofcomparison, e.g., we compare subject?verb withverb?object edges or vice versa.
The match iden-tifies paraphrases, which involve lexical changes.Figure 4(b) depicts lexical change match.
Whencomparing edge ?paper?presented?
with edge?included?points?, we compare vertex ?paper?with ?points?
and ?presented?
with ?included?.A match is found between tokens ?paper?
and?points?, resulting in the edge pair getting a relat-edness value greater than a non-match.
Had it notbeen for the lexical change match, such a relationmay have been missed.?
Nominalization match - The match identifiesnoun nominalizations - nouns formed from verbsor adjectives (e.g.
abstract?
abstraction, ambigu-ous?
ambiguity).In an ordered and lexical change match we com-pare same types of vertices (of the comparededges).
We compare vertices of different types,e.g., the subject and verb vertices or the subjectand adjective vertices.
This match also capturesrelations between nouns and their adjective forms(e.g.
ethics ?
ethical), and nouns and their verbforms (e.g.
confusion?
to confuse).In Figure 4(b) when we compare the edge?paper?presented?
with edge ?presentation?included?, we compare ?paper?
(NN) with ?in-cluded?
(VB) and ?presented?
(VB) with ?presen-tation?
(NN) .
Token ?presentation?
is the nom-inalization of token ?presented?, as a result ofwhich a match is identified between the two edges.Context(S, R) =13|Er|(?r(e)?Erargmax?s(e)?Es{matchord(s(e), r(e))}+?r(e)?Erargmax?s(e)?Es{matchlex(s(e), r(e))}+?r(e)?Erargmax?s(e)?Es{matchnom(s(e), r(e))})(3)In Equation 2, r(e) and s(e) refer to review andsubmission edges.
The formula calculates the averagebest matches that review edges have with correspond-ing submission edges, for each of the above three typesof matches matchord, matchlex and matchnom.
Er andEs represent the sets of review and submission edgesrespectively.6.3 Sentence structure matchingSentence structure matching compares double edges(two contiguous edges6), which constitute a completesegment7 (e.g.
subject?verb?object), across graphs.6Two consecutive edges sharing a common vertex.7In this work we only consider single and double edges,and not more contiguous edges (triple edges etc.
), for textmatching.56(a) Ordered match - similar edges are comparedacross the two reviews, i.e., SBJ with SBJ, OBJ withOBJ etc.
(b) Lexical change - edges of different types are com-pared, i.e., SBJ with OBJ and OBJ with SBJ respec-tively.
Only the compared edges are shown in thegraph representation.Figure 4: Context matching across two text graphs.The matching captures similarity across segments, andit captures voice changes.
Relatedness between doubleedges is the average of the vertex matches.
Edge la-bels are compared in ordered matching, and the averagevertex match is halved if the edge labels are different.Some sentence structure matches are:?
Ordered match - Double edges capture moreword order than single edges, hence this match-ing captures more context.
In Figure 5(a)double edges ?paper?presented?concepts?
and?presentation?included?points?
are compared.Vertices ?paper?, ?presented?
and ?concepts?are compared with vertices ?presentation?, ?in-cluded?
and ?points?
respectively.?
Voice change - Voice match captures word orphrase shuffling.
Change of voice from activeto passive, or vice versa is common with para-phrased text.
Vertices of the same type are com-pared across double edges.
However, the order ofcomparison is flipped.
Consider the comparisonbetween active and passive texts ?The author pre-sented the important concepts.?
and ?Necessarypoints were explained by the author.?
in Figure5(b).
We compare ?author?
and ?author?
(exactmatch), ?presented?
and ?were explained?
(syn-onym match), and ?concepts?
and ?points?
(com-mon parents match).
This results in a cumulativevoice match value of 48.
Only a voice changematch succeeds in capturing such a relationshipacross the length of a sentence segment.8Average of the vertex match values - 6 for exact match,5 for synonym match, 2 for common parents match.
Edgelabels are not compared since the order of comparison of thevertices is flipped.
(a) Ordered sentence structure match.
(b) Voice change match - Order of comparison ofthe vertices is flipped, i.e., ?author?
is comparedwith ?author?, ?presented?
with ?were explained?and ?concepts?
with ?points?.Figure 5: Matching sentence segments across two textgraphs.
Compared vertices are denoted by similar bor-ders.SentStruct(S, R) =12|Tr|(?r(t)?Trargmax?s(t)?Ts{matchord(s(t), r(t))}+?r(t)?Trargmax?s(t)?Ts{matchvoice(s(t), r(t))})(4)The cumulative sentence structure match in Equation3 calculates the average of the best ordered (matchord)and voice change (matchvoice) matches that a review?sdouble edges have with corresponding submission dou-ble edges.
r(t) and s(t) refer to double edges, and Trand Ts are the number of double edges in the reviewand submission respectively.Relevance in Equation 1 can be re-written as the av-erage of the lexico-semantic relatedness values cal-culated from phrase, context and sentence structurematches.relevance(S, R) = 13 (Phrase(S, R) + Context(S, R)+SentStruct(S, R))(5)7 ExperimentsWe evaluate the performance of our graph matching ap-proach in identifying the relevance of a review.
We alsostudy the performance of each match - Phrase, Contextand SentStruct to determine whether the matches addvalue, and help improve the overall performance of ourapproach.7.1 Data and methodWe select review-submission pairs from assignmentscompleted using Expertiza (Gehringer, 2010).
Each re-view is compared with its respective submission, and57in order to include some explicit non-relevant cases re-views are compared with other submission texts.
Forthe sake of evaluation we identify whether a review isrelevant or not relevant to a submission.
We chose 986review-submission pairs containing an equal number ofrelevant and non-relevant reviews for our study.
Rel-evance thresholds for the different matches are deter-mined based on the averages.
Two annotators labeled19% of randomly selected data as relevant or non-relevant.
We found an 80% agreement, and a Spear-man correlation of 0.44 (significance p < .0001) be-tween the two annotators?
ratings.
We use labels fromthe first annotator for testing due to the high percentagreement.7.2 ResultsTable 1 contains the accuracy and f -measure values ofour approach in identifying relevance.
A phrase or to-ken matching contains no context.
Consider the sam-ple review ?I would retitle ?Teaching, Using and Im-plementing Ethics?
to ?Teaching and Using Codes ofEthics?.?
This review gets a good phrase match valueof 3.3 with a submission (in Figure 1) discussing dif-ferent codes of ethics.
However, this review is not fullyrelevant to the content of the submission, since it is sug-gesting a change in title, and does not discuss the sub-mission?s content.
Thus a simple non context-basedphrase match tends to magnify the degree of related-ness between two texts.
Thus although a phrase matchis important, the lack of context may inflate relevance.In the case of context matching, we found that lex-ical and nominalization matches produce lower matchvalues than an ordered match.
This happens becausenot all reviews contain word order changes or nomi-nalizations, and flipping the order of matching resultsin a lower match when compared to that from an or-dered match.
The lower values decrease the averagecontext matching, thus rendering a review non-relevantto a submission.
This phenomenon explains the dip incontext matching?s accuracy and f -measure.We observed a similar trend with sentence structurematches, where voice match produced a lower valuethan the ordered match in some of the cases.
How-ever the average SentStruct match in Equation 3, withan accuracy of 65%, shows an improvement over bothphrase and context matches (Table 1).Relevance is identified with an accuracy of 66% andf -measure of 0.67 (Table 1).
Our approach has a highrecall of 0.71, indicating a good degree of agreementwith human relevance ratings.
Thus the average of thephrase, context and sentence structure matches showsan improvement over each of the individual matches.This indicates that the addition of context (ordering)from edges and double edges contributes to an im-provement in performance.Dependency trees perform best for phrase matching(Table 1).
Accuracy and f -measure of identifying rel-evance decreases for context, sentence structure andTable 1: Comparing accuracy, precision, recall and f -measure values of our word order graph with those ofa dependency-tree representation.Metric Phrase Context Sentence Structure RelevanceWord order graphaccuracy 64% 62% 65% 66%precision 0.64 0.63 0.65 0.64recall 0.67 0.60 0.63 0.71f -measure 0.65 0.62 0.64 0.67Dependency treeaccuracy 64% 50% 52% 61%precision 0.63 0.50 0.52 0.6recall 0.7 0.40 0.41 0.65f -measure 0.66 0.44 0.46 0.62Figure 6: Identifying relevance with dependency treestakes more time (in milliseconds) than with word ordergraphs.overall relevance matches.
This is likely because edgesin dependency trees capture only governance (head?modifier relation), and not word order.Dependency trees contain more vertices and edgesthan our graph, which results in an increase in the timetaken to carry out pairwise comparison between the re-view and submission texts.
We randomly selected 4%of the data to study the time taken to identify relevanceby dependency trees, and by our graph.
We found thatin most cases dependency trees take more time than ourgraph (Figure 6).
Thus our graph has a better perfor-mance, and is also faster than a dependency tree repre-sentation.7.2.1 Comparison with a text overlap-basedapproachWe compare our approach with an overlap-based rel-evance identification approach.
For this measure weconsider the average of 1 to 4-gram overlaps between areview and a submission?s texts to determine relevance.This is a precision-based metric, similar to the one usedby Papineni et al(2002).relevanceoverlap =overlap(R,S)|R| , where overlap cal-culates the number of tokens in the review (R) thatoverlap with tokens in submission (S), and |R| indi-cates the number of tokens in the review.
Stopwordsand frequent words are excluded from the numeratorand denominator during overlap calculation.This approach classifies a majority 62% of therecords as non-relevant, and has an f -measure valueof 0.59.
The overlap approach has a high false negative58Figure 7: Example of phrase or token matching and sentence structure match between a review and a submission.Figure 8: Output from our review assessment systemdisplaying relevance value of reviews.
Review?s con-tents are relevant to article on ?software extensibility?.rate i.e., several relevant reviews were wrongly clas-sified as non-relevant (recall of 0.52).
A simple textoverlap, which does not capture the relations our ap-proach succeeds in capturing, does not outperform ourapproach.Figure 7 contains two sample reviews displayingphrase and sentence structure matching with sentencesfrom a sample submission.
The first review has someinstances of exact match with the submission and itsrelevance may be easy to identify.
However, relevanceof the second review may not be determined by a textoverlaps match.
Our order-based matching and seman-tic relatedness metric help capture the relevance be-tween the second review and the submission.8 Feedback to ReviewersA screenshot of the output from our review assessmentsystem can be seen in Figure 8.
In this example wehave a review written for an article on software extensi-bility9.
The sample review in Figure 8 has a relevanceof 0.1309 (on a scale of 0?1).
As can be seen fromthe screenshot, our automated assessment system pro-vides feedback on not just relevance but on other met-rics such as quantity, content and tone types too.
How-ever, a discussion of the approach involved in calculat-ing each of these metrics is beyond the scope of thispaper.Our aim with this review assessment system is tomotivate reviewers to update their review and make itmore relevant to the text under review.
This would helpauthors to better understand details of the review, anduse the review to fix and improve their work.In the future we are planning to improve the formatof this output by providing textual feedback in additionto the numeric feedback.
The feedback will point tospecific instances of the review that need improvement.This may make it easy for reviewers to interpret thenumeric score, and maybe further motivate reviewersto use the information to improve their reviews.9 ConclusionAssessment of reviews is an important problem in edu-cation, science and human resources, and so it is wor-thy of serious attention.
In this paper we use a graph-based approach to determine whether a review is rele-vant to a piece of submission.
Some important findingsfrom our experiments are:1.
Additional context from graph edges and sen-tence structures helps improve the accuracy andf -measure of predicting relevance.2.
Our approach produces higher f -measure than atext overlap-based approach, that takes the aver-age of 1 to 4-gram overlaps between review andsubmission texts to determine relevance.9Software Extensibility https://en.wikipedia.org/wiki/Extensibility593.
Our approach produces higher accuracy and f -measure than dependency trees, which captureword-modifier information and not word order in-formation.ReferencesAria D. Haghighi, Andrew Y. Ng and Christopher D.Manning.
2005.
Robust textual inference via graphmatching.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing.
Vancouver, BritishColumbia, Canada 387?394.Bernd Bohnet.
2010.
Very high accuracy and fast de-pendency parsing is not a contradiction.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics (COLING).
Beijing, China.89?97.Bing Quan Liu and Shuai Xu and Bao Xun Wang.2009.
A combination of rule and supervised learningapproach to recognize paraphrases.
In Proceedingsof the International Conference on Machine Learn-ing and Cybernetics.
July.
110?115.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
MIT Press, Cambridge, MA.Chutima Boonthum.
2004. iSTART: paraphrase recog-nition.
In Proceedings of the ACL 2004 workshop onStudent research (ACLstudent).
Barcelona, Spain.Conny Kuhne and Klemens Bohm and Jing Zhi Yue.2010.
Reviewing the reviewers: A study of au-thor perception on peer reviews in computer science.CollaborateCom.
1?8.David Kauchak and Regina Barzilay.
2006.
Para-phrasing for automatic evaluation.
In Proceedingsof the main conference on Human Language Tech-nology Conference of the North American Chapter ofthe Association of Computational Linguistics (HLT-NAACL ?06).
New York, New York.
455?462.Edward F. Gehringer.
2010.
Expertiza: ManagingFeedback in Collaborative Learning.
Monitoringand Assessment in Online Collaborative Environ-ments: Emergent Computational Technologies for E-Learning Support.
75?96.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics.Boulder, Colorado.
19?27.Frank Van Ham, Martin Wattenberg and Fernanda BVie?gas.
2009.
Mapping text with phrase nets.
IEEETransactions on Visualization and Computer Graph-ics 15(6):1169?1176.Inderjeet Mani and Eric Bloedorn.
1997.
Multi-document summarization by graph search andmatching.
In Proceedings of the fourteenth nationalconference on artificial intelligence and ninth con-ference on Innovative applications of artificial intel-ligence (AAAI ?97).
Providence, Rhode Island.
622?628.Khaled M. Hammouda and Mohamed S. Kamel.
2002.Phrase-based Document Similarity Based on an In-dex Graph Model.
In Proceedings of the 2002 IEEEInternational Conference on Data Mining (ICDM).Kishore Papineni, Salim Roukos, Todd Ward and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics (ACL).
Philadelphia, Pennsylva-nia.
311?318.Kristina Toutanova, Dan Klein, Christopher D. Man-ning and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.In Proceedings of HLT-NAACL 2003, 252?259.Lakshmi Ramachandran and Edward F. Gehringer.2011.
Automated assessment of review quality us-ing latent semantic analysis.
11th IEEE Interna-tional Conference on Advanced Learning Technolo-gies.
July.
136?138.Lakshmi Ramachandran and Edward F. Gehringer.2012.
A Word-Order Based Graph RepresentationFor Relevance Identification [poster].
CIKM 2012,21st ACM Conference on Information and Knowl-edge Management.
Maui, Hawaii.
October.Lakshmi Ramachandran and Edward F. Gehringer.2013.
An Ordered Relatedness Metric for Rele-vance Identification.
In proceedings of the SeventhIEEE International Conference on Semantic Com-puting (ICSC) 2013.Melissa M. Nelson and Christian D. Schunn.
2009.The nature of feedback: How different types of peerfeedback affect writing performance.
InstructionalScience.
27(4):375?401.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries: how to tell apine cone from an ice cream cone.
In Proceedingsof the 5th annual international conference on Sys-tem documentation (SIGDOC).
Toronto, Ontario,Canada.
24?26.Rada Mihalcea.
2004.
Graph-based ranking algo-rithms for sentence extraction, applied to text sum-marization.
In Proceedings of the ACL 2004 onInteractive poster and demonstration sessions (ACLdemo).
Stroudsburg, PA, USA.Wenting Xiong and Diane Litman.
2011.
Automat-ically predicting peer-review helpfulness.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies: short papers (HLT) - Volume 2.Portland, Oregon.
502?507.60
