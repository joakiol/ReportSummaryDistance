TWO S IMPLE  PREDICT ION ALGORITHMSTO FACIL ITATE TEXT PRODUCTIONLois BoggessP.O.
Drawer  CSMississ ippi  State UniversityMississ ippi  State, MS 39762ABSTRACTSeveral s imple pred ict ion schemes arepresented for systems intended to fac i l i -tate text p roduct ion  for handicappedindividuals.
The schemes are based ons ing le-subject  language models, where thesystem is se l f -adapt ing  to the pastlanguage use of the subject.
Sentencepos i t ion,  the immediate ly  preceding oneor two words,  and init ial  letters of thedesired word are cues which may be usedby the systems.INTRODUCTIONFor  some years we have been invest i -gating the use of a sizeable sample of apar t icu lar  indiv idual 's  language habitsin predict ing future language use forthat individual .
The research has takentwo direct ions.One of these, the HWYE (Hear WhatYou Expect)  system, bui lds a large lan -guage model  of the past language h istoryof the individual ,  with special  emphasison the most frequent words of thatperson, and the result is used in speechrecognit ion.
In studying the languagemodel  developed by the HWYE system,several s imple predict ive schemes werenoted which are capable of ant ic ipat ing,dur ing the generat ion of a sentence, asmall  set of words  f rom which the nextdesired word can be selected.
The twoschemes described here are used for textgenerat ion (not speech recognit ion)  in aformat  that could be of use to a phys i -cal ly hand icapped person; hence theschemes have no r ight context  avai lable.One of the schemes does use left context ,and the other uses only  sentence pos i t ionas "context ' .
Both are implemented onIBM-PC systems with minimal  memoryrequirements.MOTIVAT IONOne hundred Engl ish words account for47 per cent of the Brown corpus (aboutone mi l l ion words of American Engl ishtext taken from a wide range of sources).It seems reasonable to suppose that asingle indiv idual  might in fact requirefewer words to account for a largepropor t ion  of generated text.
F rom ourwork  on the HWYE system it was knownthat 75 words accounted for half of allthe text of Vanity Fair,  a 300,000 wordVictor ian Engl ish novel by Thackeray(which incorporated  a fair ly involvedsyntax,  much embedded quotat ion,  andpassages in dialect and in French)\[Engl ish and Boggess, 1986\].
We furtherfound that 50 words accounted for half ofall the verbiage in a 20,000 word set ofsentences prov ided by an indiv idual  whoco l laborated with us.
This lattercorpus,  called the Sherr i  data, is a setof texts prov ided by a speech-hand icappedindiv idual  who uses a typewr i ter  tocommunicate,  even with her family; it isconversat iona l  in nature, as can be seenin F igure 1.
Most of the work  reportedin this paper  gives special  at tent ion tothe set of words required to account forhalf of all the verbiage of a givenindividual.
We refer to this set as theset of h igh- f requency  words.33You said something about  a magazine that <namel> hadabout  computers  that I might l ike to borrow.I would some time.I th ink we have to pick up the chi ldren whi le <name2>is in the hospital .I want to visit her in the hospital .But you have to lift me up to the w indow for me to seethe baby.Well,  it's May first now.
Help!I thought  it would not be so busy but it looks  l ike itmight be now.F igure 1.
Sample set of cont iguous sentences in Sherr i  dataIt seems reasonable  to suppose thatfor conversat iona l  Engl ish, approx imate ly50 words  may account for half of theverbiage of most Engl ish users.
F romthe s tandpo int  of human factors,  anargument could be made that one shoulds imply  put the 50 words  up on the screenwith the a lphabet  and thus be assuredthat half of al l  the words  desired by theuser were instant ly  avai lable,  in knownlocat ions  that the user would  qu ick lybecome accustomed to.
Constant lychanging menus introduce an element ofuser fat igue \ [Gib ler  and Chi ldress,1982\].
That  argument may especial ly  makesense as larger screens with more linesper screen and more characters per linebecome more common.If we l imit ourselves to the top 20most f requent words  as a constant  menu,on ly  about  30 per cent of the user'sverbiage is accounted for.
However,  itwas observed, whi le work ing  with theHWYE system, that if one looked at thetop 20 words  for any given sentencepos i t ion,  one did not see the same set ofwords occurr ing.
C lear ly  the highf requency words  (the set that compr isehalf  of word  use) are mi ld ly sensit ive to"context" even when "context" is sobroad ly  def ined as sentence posit ion.Di f ferent subsets of the 50 member set ofhigh f requency words  appear  in the set of20 most f requent words for a givensentence posi t ion.
Moreover ,  afterprocessing approx imate ly  2000 sentencesf rom the user, it was sti l l  the case thatsome of the top 20 words  for a givenpos i t ion  were not members of the highf requency set at all.
For  example,  theword "they' ,  a member of the menu for thefirst sentence pos i t ion  Isee F igure 2)and hence one of the 20 most f requentwords to start a sentence, is not amember of the g lobal  high f requency set.A pre l iminary  analys is  by Engl ishsuggested that, whereas a constant"predict ion" of the top 20 most f requentwords would  yield a success rate of 30per cent, predict ing the top 20 mostf requent words  per pos i t ion  in sentencewould yield a success rate of 40 percent.~CONTEX7"  AS SENTENCE POSIT IONThe simplest scheme, which has beenbui lt  as a pro to type  on an IBM PC withtwo f loppy  disk drives, presents the userwith the top 20 most f requent words  thatthe user has employed at whateverpos i t ion  in a sentence is current.
Forexample,  F igure 2 shows the screenpresented to the user at the beginning ofp roduct ion  of a sentence.
On the left isa list of the 20 words which thatpart icu lar  user is known to have usedmost often to begin sentences.
On ther ight is the a lphabet,  which is normal lyavai lable to the user; and in otherplaces on the screen are specialfunctions.
(Selection of words,  letters341 but2 oan3 oou ld4 doS he6 hot*?
I8 I ?N9 i f10  i tI I  i t )  s12 Lo is13 she14 that15 the16 they17 we18 what19 when2e uouSPELLCAP ITAL?
b oPUNCTUATIONHELP-NENU S h IENDINGm n oNUNDERSPECIAL  ?
t uREUlENU zHARD-COPYSAUE-SENTj k 1p q rv w xHEN ERASE qUITHEN SENTENCE:Figure 2.
Init ial  Screenand funct ions  is made by mouse,  thoughthe actual se lect ion mechanism isseparated from the bulk of the code sothat replacement with another  se lect ionmechanism shou ld  be relat ively easy toimplement.)
The sentence is bui lt  at thebot tom of the screen.
If the userselects a word  from the menu at the left,it is placed in first pos i t ion  in thesentence, and a second menu, cons ist ingof the 20 most  frequent words  that theuser has used in second place in asentence, appears in the left por t ion  ofthe screen.
After a second word  has beenproduced and added to the sentence, athird menu, cons is t ing of the 20 mostfrequent words  for that user in thirdplace in a sentence, is offered, and soon.At any time the user may reject thelefthand menu by select ing a letter of thealphabet.
F igure 3 shows  the screen afterthe user has produced two words  of asentence and has begun to spel l  a thirdword  by select ing the letter "a +.
At thispoint ,  the top 20 most f requent ly  usedwords  beginning with +a" have been offeredat the left.
If the desired word  is notin the list, the user cont inues  by se lect -ing the second letter of the desired word(in this case, "n').
The le f t -hand menubecomes the 20 most f requent ly  used wordsbeginning with the pair of  letters givenso far.
As is shown in F igure 4, thereare times when fewer than 20 words  of agiven two- le t te r  start ing combinat ion  havebeen encountered from the user's pasth istory,  in which case this a lgor i thmoffers a shortened list.In the case i l lustrated, the desiredword  was on the list.
If it were not,  theuser wou ld  have had to spel l  out  the en-tire word,  and it wou ld  have been enteredinto the sentence.
In either case, thesystem subsequent ly  returns to of fer ingthe menu of most - f requent ly -used  words  forthe fourth pos i t ion ,  and cont inues  ins imilar fash ion to the end of thesentence.L ?2 ab le3 ?
bout4 ?
?
te r5 af ternoon6 apa ln7 a l l8 am9 anl e  and12 app le13 Apr i l14 are15 a~ound16 as17 ask18 asked19 at2e ?
un t?
b o d ?
Fg h i J k 1?
t u v w )<NEW SENTENCE:I haveFigure 3:1 an imal2 an imals3 Ani ta4 ann iversaryS AnnM6 another?
&nsuer8 answer ?9 anyIO  ?
nvoneI I  ?
np th |n ?User has selected "a"?
b o d ?
??
k I J k I?
t u v ~ xNEN SENTENCE:I haveFigure 4: User has selected "a-n"35The system keeps up with how often aword has been used and with how manytimes it has occurred in each pos i t ion  ina sentence, so that f rom time to time aword is p romoted  to one of the top 20a lphabet ic  or top 20 pos i t ion - re la ted  setsof words.
For  detai ls on the file o rgan i -zat ion scheme that a l lows this to be donein real time, see Wei \[1987\].
Detai ls  onthe mouse-based  implementat ion  for IBMPC's are avai lab le  in Chow \[1986\].A SECOND ALGORITHMAn a l ternat ive predict ive a lgor i thmhas been implemented which replaces thesentence-pos i t ion -based  first menu.
Itpays special  at tent ion to the 50 mostf requent ly  used words  in the indiv idual 'svocabu lary  (the h igh- f requency  words)  andto the words most l ike ly  to fo l low them.By virtue of their  frequency, these areprecisely the words  about  which the mostis known,  with the greatest confidence,after a re lat ive ly  small  body of inputsuch as a few thousand sentences.For  each of the 50 h igh- f requencywords,  a list is kept  of the top 20 mostf requent words  to fo l low that word.
Letus call  these the f irst order  fo l lowers.For  each of the first order  fo l lowers ,there is a list of second-order  fo l lowers:words known to have fo l lowed the twoword sequence consist ing of the h igh-f requency word and its f irst orderfo l lower .For  example,  the word "I" is a h igh-f requency word.
The first order  fo l lowersfor "I" include the word "wol)ld'.
Thesecond-order  fo l lowers  for "I would"include the word  "l ike' .
(See F igure 5.
)The second-order  fo l lowers  for "I would"also include many one- t ime-on ly  fo l lowers ,as well, so the system maintains athreshold  for the number  of oceurrancesbe low which a word  is not included in thelist of second-order  fo l lowers.
Thereasoning is that a word 's  having occurredonly  once in an env i ronment  hat bydef in i t ion occurs f requent ly  may be takenas counter -ev idence  that the word shouldbe predicted.Rather  than predict  a word with lowre l iabi l i ty ,  one of two al ternat ives aretaken.
If the f i r s t -o rder  fo l lower  isitself a h igh- f requency  word,  then low-re l iab i l i ty  second-order  fo l lowers  may bereplaced with the f i r s t -o rder  fo l lower 'sown fo l lowers.
( 'Would"  is a f i r s t -o rderI oFigure 5...~-!
thi,k ,-~--'"don ' t  *,-~, 1hope ~.
!
'iwaswishl i kewi l lhavewantwondergotr - ,~zI ' l lthewei tI t 'SoFVourea l lywan thave,,.
.
.
.
.
.
.
.
.
.
QF i rs t -  and second-  fo l lowersfor "I"fo l lower  of "I" and is itself a h igh-f requency word.
There are re lat ive ly  fewrel iable second-order  fo l lowers  to "would"in the left context  of "I', so the list isaugmented with f i r s t -o rder  fo l lowers  of"would" to round out a list of 20 words.
)The other  a l ternat ive,  taken when thef i r s t -o rder  fo l lower  is not a h igh-f requency word,  is to fi l l  out any shortlist of second-order  words  with the h igh-f requency words  themselves.This a lgor i thm is related to, buttakes less memory  and is less power fu lthan a fu l l -b lown second order  Markovmodel.
Each state in a second-order(tr igram) Marker  model  is un iquelydetermined by the prev ious two inputs.For  an input vocabu lary  of 2000 words,  thenumber  of mathemat ica l ly  poss ib le statesin a t r igram Marker  model  is 4,000,000,with more than 8 b i l l ion  arcs in tercon-necting the states.
For tunate ly ,  in thereal wor ld  most of these mathemat ica l lyposs ib le states and arcs do not actual lyoccur, but a t r igram model  for the realwor ld  poss ib i l i t ies  is sti l l  quite large.We exper imented with abstract ing theinput vocabu lary  by restr ict ing it to the50 h ighest - f requency  words plus thepseudo- input  OTHER onto  which all otherwords were mapped.
When we did so, thenumber  of states and arcs in the var iousorder  Markov  models was sti l l  fa i r ly  largefor the real wor ld  data \[Engl ish andBoggess, 1986\].
As F igure 6 shows, forexample,  the rate of growth  for a four th -order  abstract  Markov  model  (just the 50h ighest - f requency  words plus OTHER plusend-o f - sentence)  is in the ne ighborhood of250 new states and 450 new arcs per 100036Sher r i  data  Thackeray  datawords  new s ta tes  new arcs  new s ta tes  new arcs1000 527 677 639 8302000 469 620 624 8183000 471 636 476 7054000 399 562 467 7165000 397 566 463 7146000 391 579 437 6687000 337 507 389 6428000 311 476 370 6289000 323 500 361 61210000 285 486 384 62911000 329 518 348 60112000 278 448 331 58813000 276 445 310 54314000 240 408 291 53015000 248 425 287 52916000 244 420 290 53317000 243 414 269 49718000 259 446 234 468F igure 6.
Growth  of abstracted four th -order  Marker  modelsnew words of text,  after 17000 words ofinput.
This was true for both the Sherridata (conversat ional  English) and the moreformal  Thackeray  data.
Moreover,  thefour th -order  Marker  model for theabstracted Thackeray  data cont inued togrow.
After 100,000 words of input,  witha model  of approx imate ly  22,000 states andapprox imate ly  45,000 arcs, the rate ofgrowth was sti l l  more than 1,000 statesand 3,000 ares per 10,000 words of input.For  this par t icu lar  implementat ion,however,  neither r. fu l l -b lown Markovmodel  using tota l  vocabu lary  nor  anabstract  model  using the 50-word  vocabu-lary seemed appropr ia te .
On the one hand,models of the entire vocabu lary  conf i rmedthat many mult ip le  word sequences didoccur regular ly.
Nevertheless, for anybut the simplest order  Marker  models(orders zero and one), the vast bu lk  ofthe networks  were taken by word combina-t ions that occurred only  once.
On theother hand, restr ict ing the predict ivemechanism to on ly  the h igh- f requency  wordsobv ious ly  left out some of the regular lyoccurr ing word combinat ions.
Our f i r s t -and second- fo l lower  a lgor i thm described onthe previous pages a l lows lower f requencywords to be predicted when they occurregular ly  in combinat ion  with h igh-f requency words.PREDICT IVE  CAPABIL IT IESThe data used to test the predictivecapabilities of the system were type-scripts provided by the user, who  wasutilizing a manual typewriter; it followsthat the results were not biased by theuser's favoring sentence patterns that thesystem itself provided.
The system hadbccn given 1750 prior scntcnces producedby the user and the data collected werefor the performance of the system over thenext 97 sentences.
The 1750 sentenceswere 14,669 words in length with a vocabu-lary of 1512 words.
Twelve sentences ofthe 1750 were a single word in length{e.g.
"yeah", "no" and "gesundheit") and51 were of length 20 or greater.
Averagelength of sentence for the init ial  bodywas 8.4 words per sentence.
The first 200sentences included t ranscr ipt ions  of ora lsentences, which were much shorter  onaverage, since the user is speech hand i -capped.
If the first 200 sentences areomitted, the average sentence length is8.6 for the fo l lowing 1550 sentences.Of the next 97 sentences generated,the shortest sentence was "Thanks again.
"The longest was "You said something abouta magazine that Jenni had about  computersthat I might l ike to borrow."
The 97sentences consisted of 884 words (six ofwhich were numbers in digital  form), foran average length of 9.1 words persentence.37Of the 884 words,  350 were presentedon the first menu, 373 were presented onthe second menu (after one letter had beenspelled), 109 were presented on the thirdmenu (after two letters had been spelled),.2 were presented on the fourth  menu (afterthree letters had been spelled, 43 werespel led out in their entirety,  and 7 werenumbers in digital  form, produced usingthe number  screen of the system.F rom the above, it is obv ious  thatthe device of predict ing the 20 mostf requent words  by sentence pos i t ion  issuccessful 39.6 per cent of the time;42.2 per cent of the time, the desiredword is among the 20 most f requent wordsof a given init ial  letter but not in the20 most f requent words  by posit ion;combin ing these two facts, we see that81.8 per cent of the time, this s implepred ict ion scheme presents the desiredword on a first or second selection.
Thedesired word is offered in the first,second, or third menu 94.1 per cent of thetime, and most of the rest of the time(5.7 per cent of total),  the desired wordis unknown to the system and is "spelledout ' ,  where "spel l ing" includes produc ingnumbers.A l though the fourth  menu, consist ingof words with a th ree- le t te r  init ialsequence, present ly  has a low successrate, it is precisely this category thatwe expect to see improve as more of theuser's words  become known to the systemthrough spell ing.
That  is, as timepasses, we expect the user to have toresort  to complete spel l ing less and lessbecause the known vocabu lary  wil l  includemore and more of the actual  vocabu lary  ofthe user.
Many of the new words wil l  below frequency words  that we would expectto find on the menu for three- le t ter  com-b inat ions after they are known.The second a lgor i thm, using f i r s t -  andsecond- fo l lowers  of the h igh- f requencywords,  was run on i00 sentences, theshortest of which was "Help!"
(94 of the97 test sentences for the first a lgor i thmwere represented in the test set for thesecond.)
There were 895 words in thesample, of which 448 were presented on thefirst menu, 280 were presented on thesecond (after one letter had been spel ledout, 83 on the third (after two letterswere spelled), 1 on the fourth,  and 83were spel led out in their ent i rety (thiscategory included numbers).Running the second test gave us avery quick apprec ia t ion  for the value ofadding new words to the system as theyare encountered,  since this imp lementat ionof the second a lgor i thm did not.
Oneespecial ly  str ik ing example  was a wordbeginning with "w-o"  which had never beenused before, but which occurred five timesin the 100 test sentences and had to bespel led out each time.
This was espec ia l -ly i r r i tat ing since the "w-o"  menu (thirdmenu) had fewer than 20 entries and wouldhave accommodated the new word.
A com-par ison of the two columns of F igure 7suggests that for the text held in commonby the two tests, approx imate ly  30 wordshad to be spel led out by the second a lgo -r ithm, which were selected by menu in thefirst a lgor i thm because it added new wordsto its data sets as they were encountered.PROPOSED EXTENSIONSWe have several plans for the future,most of them involv ing the second a lgo -r ithm.
Our first task is to increase thenumber  of sentences in the Sherr i  data to3000 and determine how much (if at all)an enlarged base of exper ience improvesthe abi l i ty  of the a lgor i thm to predictSentence pos i t ion  a lgor i thmnumber sentences :  97number of words:  884frequent word/ lef t  context  a lgor i thmnumber sentences :  100number of words:  895words % to ta lf i r s t  menu: 350 39.6% 39.6%second menu: 373 42.2% 81.8%th i rd  menu: 109 12.3% 94.1%four th  menu: 2 0.2% 94.3%spe l led :  43 4.8% 99.2%numbers:  7 0.8% 100%words % to ta lf i r s t  menu: 448 50% 50%second menu: 280 31.3% 81.3%th i rd  menu: 83 9.3% 90.6%four th  menu: 1 0.1% 90.7%"spe l led ' :  83 9.3% 100%Figure 7.
Compar i son  of the predict ive capabi l i t ies.38the desired word on the first try.In its present form, the system isrel iable in its predict ions after severalhundred sentences by the user have beenprocessed.
We intend to take somethingl ike the Brown corpus for AmericanEngl ish and f rom it create a van i l l a -f lavored pred ictor  as a s ta r t -up  versionfor a new user, with faci l it ies bui lt  into have the user's own language patternsgradual ly  outweigh the Brown corpusin i t ia l izat ion as they are input.Eventual ly  the Brown corpus would haveessential ly no effect, or at least noeffect overr id ing the user's indiv idualuse of language (it might serve as abasic d ic t ionary  for text vocabu lary  notyet seen f rom the user).We intend to investigate what effectgenerat ing sentences whi le using thesystem has on our co l laborator .
To date,she has ob l ig ing ly  been wi l l ing tocont inue to use a typewr i ter  to generatetext, but she does own a personal  computerand is able to use a mouse.
Our ownexper ience in enter ing her sentences onthe system has made it clear that in manyinstances she would have expressed thesame ideas more rap id ly  on the system witha sl ight change in wording.
Since theprefer red words and patterns are derivedby the system from her own languagehistory,  they should feel normal  andnatural  to her and could inf luence her tomodi fy  her intent ions in generat ing asentence.
On the other hand, a dif ferenthandicapped indiv idual  (a quadr ip legic)has informed us that ease of mechanicalp roduct ion  of a sentence has l itt le or noeffect on his choice of words,  and thatwould appear  to be the case for ourco l laborator  whi le she uses thetypewriter .F inal ly ,  we wish to make use of themuch larger amounts  of memory avai lableon personal  computers  by taking account ofthe fo l lowers  for many of the moderate -f requency words.
For  example,  in thesentence "would you be able..." the word"able" is not high frequency.
Never the-less, the system could easi ly deduce whatfo l lowing word to expect,  since everyknown occurrence of "able" is fo l lowed by"to'.
As it happens,  "to" is one of thetop 20 most frequent words and hencefor tu i tous ly  is on the default  menu afterthe non-h igh- f requency  word "able' ,  butthere are many other examples where thesystem is not so lucky.
For  instance,"pick" is usual ly fo l lowed by "up" in theSherri  data, but "pick" is low frequencyand "up" is not on the default  first menu.S imi lar ly ,  "think" is a h igh- f requencyword and has a well  developed set offo l lowers.
"Thinks" and "thought" are noth igh- f requency  and hence are fo l lowed bythe default  first menu.
Yet v i r tua l lyevery fo l lower  for "thinks" and "thought"in the Sherri  data happens to belong tothe set of fo l lowers  for "think' .
Webelieve that by stor ing in format ion  onmoderate f requency words with s t rong lyassociated fo l lowers  and on clusters ofverb forms we may s igni f icant ly improvethe success of the first menu.RELATED WORKThat a small  number  of words accountfor a large propor t ion  of the tota l  ver -biage in conversat ion has been known forsome time \[Kucera and Francis,  1967\].The idea of using the first severalletters typed by a handicapped indiv idualto ant ic ipate the next desired word hasbeen used in numerous systems (e.g.,\ [Gib let  and Chi ldress,  1982\], \ [P icket inget al, 1984\]).
The Gib ler  and Chi ldresssystem is typical  in that it uses a few-thousand-word  vocabu lary  drawn from thegeneral publ ic,  plus a few hundred wordsspecific to the user of the system.
Theuser must type the first two lettersbefore the system provides a menu ofwords beginning with the letter pair.
Ifthe desired word was not on the menu, theuser had to spell  the word out.
It wasfelt that one letter was not in format iveenough to warrant  a menu.
Fur thermore ,Gi lb ler  and Chi ldress showed that increas -ing the system vocabu lary  degraded theper formance of their system and theyrecommended l imitat ion of the vocabu laryfor human factors reasons.By contrast,  our  system costs theuser no more effort  in terms of selectingthe first two letters - if indeed theyhave needed to go that far; 80 per centof the time, they haven't  needed to pro -vide two letters.
Further,  there is noquest ion that for our  system, a l lowing thevocabu lary  to grow is of benefit  both tosystem per formance and to user sat i s -faction.Gal l iers  \[1987\] describes a dif ferentapproach  for phys ica l ly  handicapped39persons conversant in the Bliss communi -cations system.
Communicat ion with Blissinvolves a high degree of interpretat ionby the "l istener' ,  and Gal l iers reports  animpressive 75 per cent success rate inautomating such interpretat ion.
TheGal l iers system is s ingle-subject ,  as oursis, and it does use past h istory tofaci l i tate interpretat ion.
It was, how-ever, l imited to a very small domain forthe exper iment described.One statistic cited by this last paperwas that the same text produced from theBliss communicat ion,  had it been producedby typing into a word processing system,would have required three times as manykey-press  operat ions.
Our own rat io ofkey-press  operat ions  to charactersproduced was 45 per cent for the sentencepos i t ion algorithm.
That is, on averageit took  45 presses of a mouse button toproduce 100 characters.
Part of thereason for such a high rat io has to dowith punctuat ion,  capita l izat ion,  andspecial screens such as the number screen,which requires not only  the same number ofpresses of the button as there are digits,for example,  but addi t ional  presses of thebutton to summon the screen and quit themenu.
But pr imar i ly  the rat io seems toderive from the fact that many of thewords in any text are short - "a', "to',"the', "of',  "in', and "on" being examplesfrom this very paragraph.
If the firstmenu does not contain a desired two- le t te rword,  one has to spell the first letterand then make a selection from the secondmenu - requir ing two presses of a button.By contrast,  Bliss users commonly usea telegraphic style of communicat ion andomit funct ion words altogether.CONCLUSIONIn summary, evidence exists that fora system bui lt  around a single user'slanguage, a predict ion scheme that simplyant ic ipated f i fty or so words would onaverage be correct  about half the time.Limit ing such a system to on ly  the top 20most frequent words would give a successrate of about 30 per cent.
However,  notall of the high frequency words are d is -tr ibuted evenly by sentence posit ion.
Asystem that offers the top 20 most f re -quent ly  occurr ing words for each posi t ionof a sentence was successful about 40 percent of the time on the next 97 sentences.Al lowing a user to reject the first set ofwords by giving the first letter of thedesired word and offer ing the 20 mostfrequent words beginning with that letterresulted in success for the combined firstand second menus 82 per cent of the time.After a training body of 1750 sen-tences (14,669 words), with a vocabularyof 1512 words, it was stil l the case thatabout six per cent of the desired wordswere unknown to the system.An alternat ive a lgor i thm for the firstoffer ing of 20 words, based pr imar i ly  onthe right hand contexts of the high f re -quency words, is successful on the firstguess 50 per cent of the time.REFERENCESBoggess, Lois and Thomas M. English, TheHWYE speech recogni t ion system: a user -specific model for expectat ion -basedrecognit ion,  in Proceedings of the 25thSoutheast Regional  Conference of theACM, Birmingham, 1987.Chow, C. L. A mouse-dr iven menu-basedtext prosthesis for the speechhandicapped, M.C.S.
project  report ,Mississippi State University, 1986.English, T. M. and Lois Boggess, A gram-matical approach to reducing the stat is -tical sparsity of language models in nat -ural domains, Proceedings of the In ter -nat ional  Conference on Acoustics, Speech,and Signal Processing, Tokyo ,  1986.Gall iers, Jul ia, AI for special needs -an "intel l igent" communicat ion aid forBliss users, Appl ied Ar t i f i c ia lIntel l igence, 1(1):77-86, 1987.Gibler,  D. C. and D. S. Childress, Lan-guage ant ic ipat ion with a computer  basedscanning aid, Proceedings of the IEEEComputer  Workshop on Computersto Aid the Handicavoed,  1982.Kucera, H. and W. N. Francis, Computa-t ional  analysis of p resent -day  AmericanEnglish.
Brown University Press, 1967.Pickering, J., J. L. Arnott,  J. G. Wolff ,and A. L. Swiff in, Predict ion and adap-tat ion in a communicat ion aid for thedisabled, Proceedings of the IFIPConference on Human-ComputerInteract ion,  London,  1984.Wei, Jan -Soong,  Fi le organizat ion  ofSherr i  System, M.C.S.
project  report ,Mississippi State University, 1987.40
