Attention Shifting for Parsing Speech ?Keith HallDepartment of Computer ScienceBrown UniversityProvidence, RI 02912kh@cs.brown.eduMark JohnsonDepartment of Cognitive and Linguistic ScienceBrown UniversityProvidence, RI 02912Mark Johnson@Brown.eduAbstractWe present a technique that improves the efficiencyof word-lattice parsing as used in speech recogni-tion language modeling.
Our technique applies aprobabilistic parser iteratively where on each iter-ation it focuses on a different subset of the word-lattice.
The parser?s attention is shifted towardsword-lattice subsets for which there are few or nosyntactic analyses posited.
This attention-shiftingtechnique provides a six-times increase in speed(measured as the number of parser analyses evalu-ated) while performing equivalently when used asthe first-stage of a multi-stage parsing-based lan-guage model.1 IntroductionSuccess in language modeling has been dominatedby the linear n-gram for the past few decades.
Anumber of syntactic language models have provento be competitive with the n-gram and better thanthe most popular n-gram, the trigram (Roark, 2001;Xu et al, 2002; Charniak, 2001; Hall and Johnson,2003).
Language modeling for speech could well bethe first real problem for which syntactic techniquesare useful.John ate the pizza on a plate with a fork .NP:plate NP:forkPP:withPP:onIN INVB NPVP:ateFigure 1: An incomplete parse tree with head-word an-notations.One reason that we expect syntactic models toperform well is that they are capable of model-ing long-distance dependencies that simple n-gram?
This research was supported in part by NSF grants 9870676and 0085940.models cannot.
For example, the model presentedby Chelba and Jelinek (Chelba and Jelinek, 1998;Xu et al, 2002) uses syntactic structure to identifylexical items in the left-context which are then mod-eled as an n-gram process.
The model presentedby Charniak (Charniak, 2001) identifies both syn-tactic structural and lexical dependencies that aid inlanguage modeling.
While there are n-gram mod-els that attempt to extend the left-context windowthrough the use of caching and skip models (Good-man, 2001), we believe that linguistically motivatedmodels, such as these lexical-syntactic models, aremore robust.Figure 1 presents a simple example to illustratethe nature of long-distance dependencies.
Usinga syntactic model such as the the Structured Lan-guage Model (Chelba and Jelinek, 1998), we pre-dict the word fork given the context {ate, with}where a trigram model uses the context {with, a}.Consider the problem of disambiguating between.
.
.
plate with a fork and .
.
.
plate with effort.
Thesyntactic model captures the semantic relationshipbetween the words ate and fork.
The syntactic struc-ture allows us to find lexical contexts for whichthere is some semantic relationship (e.g., predicate-argument).Unfortunately, syntactic language modeling tech-niques have proven to be extremely expensive interms of computational effort.
Many employ theuse of string parsers; in order to utilize such tech-niques for language modeling one must preselect aset of strings from the word-lattice and parse each ofthem separately, an inherently inefficient procedure.Of the techniques that can process word-lattices di-rectly, it takes significant computation to achievethe same levels of accuracy as the n?best rerank-ing method.
This computational cost is the result ofincreasing the search space evaluated with the syn-tactic model (parser); the larger space resulting fromcombining the search for syntactic structure with thesearch for paths in the word-lattice.In this paper we propose a variation of a proba-bilistic word-lattice parsing technique that increases0 1yesterday/02and/4.0043in/14.734tuesday/014tuesday/0 5to/0.0006two/8.7697it/51.59to/08outlaw/83.579outline/2.57310outlined/12.58outlines/10.71outline/0outlined/8.027outlines/7.14013to/0in/0of/115.4a/71.30the/115.3 11strategy/0strategy/0outline/012/0</s>/0Figure 2: A partial word-lattice from the NIST HUB-1 dataset.efficiency while incurring no loss of language mod-eling performance (measured as Word Error Rate ?WER).
In (Hall and Johnson, 2003) we presenteda modular lattice parsing process that operates intwo stages.
The first stage is a PCFG word-latticeparser that generates a set of candidate parses overstrings in a word-lattice, while the second stagerescores these candidate edges using a lexicalizedsyntactic language model (Charniak, 2001).
Underthis paradigm, the first stage is not only responsiblefor selecting candidate parses, but also for selectingpaths in the word-lattice.
Due to computational andmemory requirements of the lexicalized model, thesecond stage parser is capable of rescoring only asmall subset of all parser analyses.
For this reason,the PCFG prunes the set of parser analyses, therebyindirectly pruning paths in the word lattice.We propose adding a meta-process to the first-stage that effectively shifts the selection of word-lattice paths to the second stage (where lexical in-formation is available).
We achieve this by ensuringthat for each path in the word-lattice the first-stageparser posits at least one parse.2 Parsing speech word-latticesP (A,W ) = P (A|W )P (W ) (1)The noisy channel model for speech is presented inEquation 1, where A represents the acoustic data ex-tracted from a speech signal, and W represents aword string.
The acoustic model P (A|W ) assignsprobability mass to the acoustic data given a wordstring and the language model P (W ) defines a dis-tribution over word strings.
Typically the acousticmodel is broken into a series of distributions condi-tioned on individual words (though these are basedon false independence assumptions).P (A|w1.
.
.
wi.
.
.
wn) =n?i=1P (A|wi) (2)The result of the acoustic modeling process is a setof string hypotheses; each word of each hypothesisis assigned a probability by the acoustic model.Word-lattices are a compact representation ofoutput of the acoustic recognizer; an example is pre-sented in Figure 2.
The word-lattice is a weighteddirected acyclic graph where a path in the graph cor-responds to a string predicted by the acoustic recog-nizer.
The (sum) product of the (log) weights on thegraph (the acoustic probabilities) is the probabilityof the acoustic data given the string.
Typically wewant to know the most likely string given the acous-tic data.arg maxP (W |A) (3)= arg max P (A,W )= arg max P (A|W )P (W )In Equation 3 we use Bayes?
rule to find the opti-mal string given P (A|W ), the acoustic model, andP (W ), the language model.
Although the languagemodel can be used to rescore1 the word-lattice, it istypically used to select a single hypothesis.We focus our attention in this paper to syntacticlanguage modeling techniques that perform com-plete parsing, meaning that parse trees are builtupon the strings in the word-lattice.2.1 n?best list rerankingMuch effort has been put forth in developing effi-cient probabilistic models for parsing strings (Cara-ballo and Charniak, 1998; Goldwater et al, 1998;Blaheta and Charniak, 1999; Charniak, 2000; Char-niak, 2001); an obvious solution to parsing word-lattices is to use n?best list reranking.
The n?bestlist reranking procedure, depicted in Figure 3, uti-lizes an external language model that selects a setof strings from the word-lattice.
These strings areanalyzed by the parser which computes a languagemodel probability.
This probability is combined1To rescore a word-lattice, each arch is assigned a new score(probability) defined by a new model (in combination with theacoustic model).w1, ..., wi, ..., wn1...LanguageModelw1, ..., wi, ..., wn2w1, ..., wi, ..., wn3w1, ..., wi, ..., wn4w1, ..., wi, ..., wnmo1, ..., oi, ..., on82351 647 109the/0man/0is/0duh/1.385man/0 is/0surely/0early/0mans/1.385man's/1.385surly/0surly/0.692early/0early/0 n-bestlistextractorFigure 3: n?best list rerankingwith the acoustic model probability to reranked thestrings according to the joint probability P (A,W ).There are two significant disadvantages to this ap-proach.
First, we are limited by the performanceof the language model used to select the n?bestlists.
Usually, the trigram model is used to se-lect n paths through the lattice generating at mostn unique strings.
The maximum performance thatcan be achieved is limited by the performance ofthis extractor model.
Second, of the strings thatare analyzed by the parser, many will share com-mon substrings.
Much of the work performed bythe parser is duplicated for these substrings.
Thissecond point is the primary motivation behind pars-ing word-lattices (Hall and Johnson, 2003).2.2 Multi-stage parsing?PCFG Parser???
?LexicalizedParserFigure 4: Coarse-to-fine lattice parsing.In Figure 4 we present the general overview ofa multi-stage parsing technique (Goodman, 1997;Charniak, 2000; Charniak, 2001).
This process1.
Parse word-lattice with PCFG parser2.
Overparse, generating additional candidates3.
Compute inside-outside probabilities4.
Prune candidates with probability thresholdTable 1: First stage word-lattice parseris know as coarse-to-fine modeling, where coarsemodels are more efficient but less accurate thanfine models, which are robust but computation-ally expensive.
In this particular parsing model aPCFG best-first parser (Bobrow, 1990; Caraballoand Charniak, 1998) is used to search the uncon-strained space of parses ?
over a string.
This firststage performs overparsing which effectively al-lows it to generate a set of high probability candi-date parses ??.
These parses are then rescored us-ing a lexicalized syntactic model (Charniak, 2001).Although the coarse-to-fine model may include anynumber of intermediary stages, in this paper we con-sider this two-stage model.There is no guarantee that parses favored by thesecond stage will be generated by the first stage.
Inother words, because the first stage model prunesthe space of parses from which the second stagerescores, the first stage model may remove solutionsthat the second stage would have assigned a highprobability.In (Hall and Johnson, 2003), we extended themulti-stage parsing model to work on word-lattices.The first-stage parser, Table 1, is responsible forpositing a set of candidate parses over the word-lattice.
Were we to run the parser to completion itwould generate all parses for all strings describedby the word-lattice.
As with string parsing, we stopthe first stage parser early, generating a subset ofall parses.
Only the strings covered by completeparses are passed on to the second stage parser.
Thisindirectly prunes the word-lattice of all word-arcsthat were not covered by complete parses in the firststage.We use a first stage PCFG parser that performsa best-first search over the space of parses, whichmeans that it depends on a heuristic ?figure-of-merit?
(FOM) (Caraballo and Charniak, 1998).
Agood FOM attempts to model the true probabilityof a chart edge2 P (N ij,k).
Generally, this proba-bility is impossible to compute during the parsingprocess as it requires knowing both the inside andoutside probabilities (Charniak, 1993; Manning andSchu?tze, 1999).
The FOM we describe is an ap-proximation to the edge probability and is computedusing an estimate of the inside probability times anapproximation to the outside probability 3.The inside probability ?
(Nij,k) can be computedincrementally during bottom-up parsing.
The nor-malized acoustic probabilities from the acoustic rec-ognizer are included in this calculation.??
(N ij,k) (4)=?i,l,q,rfwd(T qi,j)p(N i|T q)p(Tr|N i)bkwd(T rk,l)The outside probability is approximated with abitag model and the standard tag/category bound-ary model (Caraballo and Charniak, 1998; Hall andJohnson, 2003).
Equation 4 presents the approx-imation to the outside probability.
Part-of-speechtags T q and T r are the candidate tags to the leftand right of the constituent Nij,k.
The fwd() andbkwd() functions are the HMM forward and back-ward probabilities calculated over a lattice con-taining the part-of-speech tag, the word, and theacoustic scores from the word-lattice to the left andright of the constituent, respectively.
p(Ni|T q) andp(Tr|N i) are the boundary statistics which are esti-mated from training data (details of this model canbe found in (Hall and Johnson, 2003)).FOM(N ij,k) = ??
(N ij,k)?
(N ij,k)?C(j, k) (5)The best-first search employed by the first stageparser uses the FOM defined in Equation 5, where?
is a normalization factor based on path lengthC(j, k).
The normalization factor prevents smallconstituents from consistently being assigned a2A chart edge Nij,kindicates a grammar category Ni canbe constructed from nodes j to k.3An alternative to the inside and outside probabilities arethe Viterbi inside and outside probabilities (Goldwater et al,1998; Hall and Johnson, 2003).higher probability than larger constituents (Goldwa-ter et al, 1998).Although this heuristic works well for directingthe parser towards likely parses over a string, itis not an ideal model for pruning the word-lattice.First, the outside approximation of this FOM isbased on a linear part-of-speech tag model (thebitag).
Such a simple syntactic model is unlikelyto provide realistic information when choosing aword-lattice path to consider.
Second, the model isprone to favoring subsets of the word-lattice caus-ing it to posit additional parse trees for the favoredsublattice rather than exploring the remainder of theword-lattice.
This second point is the primary moti-vation for the attention shifting technique presentedin the next section.3 Attention shifting4We explore a modification to the multi-stage parsingalgorithm that ensures the first stage parser positsat least one parse for each path in the word-lattice.The idea behind this is to intermittently shift the at-tention of the parser to unexplored parts of the wordlattice.IdentifyUsed EdgesClear Agenda/Add Edges forUnused WordsIs AgendaEmpty?
noContinueMulti-stageParsingyesPCFGWord-latticeParserFigure 5: Attention shifting parser.Figure 5 depicts the attention shifting first stageparsing procedure.
A used edge is a parse edge thathas non-zero outside probability.
By definition of4The notion of attention shifting is motivated by the work onparser FOM compensation presented in (Blaheta and Charniak,1999).the outside probability, used edges are constituentsthat are part of a complete parse; a parse is com-plete if there is a root category label (e.g., S for sen-tence) that spans the entire word-lattice.
In order toidentify used edges, we compute the outside prob-abilities for each parse edge (efficiently computingthe outside probability of an edge requires that theinside probabilities have already been computed).In the third step of this algorithm we clear theagenda, removing all partial analyses evaluated bythe parser.
This forces the parser to abandon analy-ses of parts of the word-lattice for which completeparses exist.
Following this, the agenda is popu-lated with edges corresponding to the unused words,priming the parser to consider these words.
To en-sure the parser builds upon at least one of theseunused edges, we further modify the parsing algo-rithm:?
Only unused edges are added to the agenda.?
When building parses from the bottom up, aparse is considered complete if it connects to aused edge.These modifications ensure that the parser focuseson edges built upon the unused words.
The sec-ond modification ensures the parser is able to de-termine when it has connected an unused word witha previously completed parse.
The application ofthese constraints directs the attention of the parsertowards new edges that contribute to parse anal-yses covering unused words.
We are guaranteedthat each iteration of the attention shifting algorithmadds a parse for at least one unused word, meaningthat it will take at most |A| iterations to cover the en-tire lattice, where A is the set of word-lattice arcs.This guarantee is trivially provided through the con-straints just described.
The attention-shifting parsercontinues until there are no unused words remain-ing and each parsing iteration runs until it has founda complete parse using at least one of the unusedwords.As with multi-stage parsing, an adjustable param-eter determines how much overparsing to performon the initial parse.
In the attention shifting algo-rithm an additional parameter specifies the amountof overparsing for each iteration after the first.
Thenew parameter allows for independent control of theattention shifting iterations.After the attention shifting parser populates aparse chart with parses covering all paths in thelattice, the multi-stage parsing algorithm performsadditional pruning based on the probability of theparse edges (the product of the inside and outsideprobabilities).
This is necessary in order to con-strain the size of the hypothesis set passed on to thesecond stage parsing model.The Charniak lexicalized syntactic languagemodel effectively splits the number of parse states(an edges in a PCFG parser) by the number ofunique contexts in which the state is found.
Thesecontexts include syntactic structure such as parentand grandparent category labels as well as lexicalitems such as the head of the parent or the head of asibling constituent (Charniak, 2001).
State splittingon this level causes the memory requirement of thelexicalized parser to grow rapidly.Ideally, we would pass all edges on to the sec-ond stage, but due to memory limitations, pruningis necessary.
It is likely that edges recently discov-ered by the attention shifting procedure are pruned.However, the true PCFG probability model is usedto prune these edges rather than the approximationused in the FOM.
We believe that by consideringparses which have a relatively high probability ac-cording to the combined PCFG and acoustic modelsthat we will include most of the analyses for whichthe lexicalized parser assigns a high probability.4 ExperimentsThe purpose of attention shifting is to reduce theamount of work exerted by the first stage PCFGparser while maintaining the same quality of lan-guage modeling (in the multi-stage system).
Wehave performed a set of experiments on the NIST?93 HUB?1 word-lattices.
The HUB?1 is a collec-tion of 213 word-lattices resulting from an acousticrecognizer?s analysis of speech utterances.
Profes-sional readers reading Wall Street Journal articlesgenerated the utterances.The first stage parser is a best-first PCFG parsertrained on sections 2 through 22, and 24 of the PennWSJ treebank (Marcus et al, 1993).
Prior to train-ing, the treebank is transformed into speech-liketext, removing punctuation and expanding numer-als, etc.5 Overparsing is performed using an edgepop6 multiplicative factor.
The parser records thenumber of edge pops required to reach the first com-plete parse.
The parser continues to parse a untilmultiple of the number of edge pops required forthe first parse are popped off the agenda.The second stage parser used is a modified ver-sion of the Charniak language modeling parser de-scribed in (Charniak, 2001).
We trained this parser5Brian Roark of AT&T provided a tool to perform thespeech normalization.6An edge pop is the process of the parser removing an edgefrom the agenda and placing it in the parse chart.on the BLLIP99 corpus (Charniak et al, 1999); acorpus of 30million words automatically parsed us-ing the Charniak parser (Charniak, 2000).In order to compare the work done by the n?bestreranking technique to the word-lattice parser, wegenerated a set of n?best lattices.
50?best lists wereextracted using the Chelba A* decoder7.
A 50?best lattice is a sublattice of the acoustic lattice thatgenerates only the strings found in the 50?best list.Additionally, we provide the results for parsing thefull acoustic lattices (although these work measure-ments should not be compared to those of n?bestreranking).We report the amount of work, shown as thecumulative # edge pops, the oracle WER for theword-lattices after first stage pruning, and the WERof the complete multi-stage parser.
In all of theword-lattice parsing experiments, we pruned the setof posited hypothesis so that no more than 30,000local-trees are generated8.
We chose this thresh-old due to the memory requirements of the sec-ond stage parser.
Performing pruning at the end ofthe first stage prevents the attention shifting parserfrom reaching the minimum oracle WER (most no-table in the full acoustic word-lattice experiments).While the attention-shifting algorithm ensures allword-lattice arcs are included in complete parses,forward-backward pruning, as used here, will elim-inate some of these parses, indirectly eliminatingsome of the word-lattice arcs.To illustrate the need for pruning, we computedthe number of states used by the Charniak lexi-calized syntactic language model for 30,000 localtrees.
An average of 215 lexicalized states weregenerated for each of the 30,000 local trees.
Thismeans that the lexicalized language model, on av-erage, computes probabilities for over 6.5 millionstates when provided with 30,000 local trees.Model # edge pops O-WER WERn?best (Charniak) 2.5 million 7.75 11.8100x LatParse 3.4 million 8.18 12.010x AttShift 564,895 7.78 11.9Table 2: Results for n?best lists and n?best lattices.Table 2 shows the results for n?best list rerank-ing and word-lattice parsing of n?best lattices.We recreated the results of the Charniak languagemodel parser used for reranking in order to measurethe amount of work required.
We ran the first stageparser with 4-times overparsing for each string in7The n?best lists were provided by Brian Roark (Roark,2001)8A local-tree is an explicit expansion of an edge and its chil-dren.
An example local tree is NP3,8?
DT3,4NN4,8.the n?best list.
The LatParse result represents run-ning the word-lattice parser on the n?best latticesperforming 100?times overparsing in the first stage.The AttShift model is the attention shifting parserdescribed in this paper.
We used 10?times overpars-ing for both the initial parse and each of the attentionshifting iterations.
When run on the n?best lattice,this model achieves a comparable WER, while re-ducing the amount of parser work sixfold (as com-pared to the regular word-lattice parser).Model # edge pops O-WER WERacoustic lats N/A 3.26 N/A100x LatParse 3.4 million 5.45 13.110x AttShift 1.6 million 4.17 13.1Table 3: Results for acoustic lattices.In Table 3 we present the results of the word-lattice parser and the attention shifting parser whenrun on full acoustic lattices.
While the oracle WERis reduced, we are considering almost half as manyedges as the standard word-lattice parser.
The in-creased size of the acoustic lattices suggests that itmay not be computationally efficient to consider theentire lattice and that an additional pruning phase isnecessary.The most significant constraint of this multi-stagelattice parsing technique is that the second stageprocess has a large memory requirement.
While theattention shifting technique does allow the parser topropose constituents for every path in the lattice, weprune some of these constituents prior to performinganalysis by the second stage parser.
Currently, prun-ing is accomplished using the PCFG model.
Onesolution is to incorporate an intermediate pruningstage (e.g., lexicalized PCFG) between the PCFGparser and the full lexicalized model.
Doing so willrelax the requirement for aggressive PCFG pruningand allows for a lexicalized model to influence theselection of word-lattice paths.5 ConclusionWe presented a parsing technique that shifts the at-tention of a word-lattice parser in order to ensuresyntactic analyses for all lattice paths.
Attentionshifting can be thought of as a meta-process aroundthe first stage of a multi-stage word-lattice parser.We show that this technique reduces the amount ofwork exerted by the first stage PCFG parser whilemaintaining comparable language modeling perfor-mance.Attention shifting is a simple technique that at-tempts to make word-lattice parsing more efficient.As suggested by the results for the acoustic latticeexperiments, this technique alone is not sufficient.Solutions to improve these results include modify-ing the first-stage grammar by annotating the cat-egory labels with local syntactic features as sug-gested in (Johnson, 1998) and (Klein and Manning,2003) as well as incorporating some level of lexical-ization.
Improving the quality of the parses selectedby the first stage should reduce the need for gen-erating such a large number of candidates prior topruning, improving efficiency as well as overall ac-curacy.
We believe that attention shifting, or somevariety of this technique, will be an integral part ofefficient solutions for word-lattice parsing.ReferencesDon Blaheta and Eugene Charniak.
1999.
Au-tomatic compensation for parser figure-of-meritflaws.
In Proceedings of the 37th annual meetingof the Association for Computational Linguistics,pages 513?518.Robert J. Bobrow.
1990.
Statistical agenda pars-ing.
In DARPA Speech and Language Workshop,pages 222?224.Sharon Caraballo and Eugene Charniak.
1998.New figures of merit for best-first probabilis-tic chart parsing.
Computational Linguistics,24(2):275?298, June.Eugene Charniak, Don Blaheta, Niyu Ge, KeithHall, John Hale, and Mark Johnson.
1999.BLLIP 1987?89 wsj corpus release 1.
LDC cor-pus LDC2000T43.Eugene Charniak.
1993.
Statistical LanguageLearning.
MIT Press.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 2000 Con-ference of the North American Chapter of the As-sociation for Computational Linguistics., ACL,New Brunswick, NJ.Eugene Charniak.
2001.
Immediate-head parsingfor language models.
In Proceedings of the 39thAnnual Meeting of the Association for Computa-tional Linguistics.Ciprian Chelba and Frederick Jelinek.
1998.
Astudy on richer syntactic dependencies for struc-tured language modeling.
In Proceedings of the36th Annual Meeting of the Association for Com-putational Linguistics and 17th InternationalConference on Computational Linguistics, pages225?231.Sharon Goldwater, Eugene Charniak, and MarkJohnson.
1998.
Best-first edge-based chart pars-ing.
In 6th Annual Workshop for Very Large Cor-pora, pages 127?133.Joshua Goodman.
1997.
Global thresholding andmultiple-pass parsing.
In Proceedings of the Sec-ond Conference on Empirical Methods in NaturalLanguage Processing, pages 11?25.Joshua Goodman.
2001.
A bit of progress in lan-guage modeling, extendend version.
In MicrosoftResearch Technical Report MSR-TR-2001-72.Keith Hall and Mark Johnson.
2003.
Languagemodeling using efficient best-first bottom-upparsing.
In Proceedings of IEEE AutomatedSpeech Recognition and Understanding Work-shop.Mark Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24:617?636.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings ofthe 41st Meeting of the Association for Computa-tional Linguistics (ACL-03).Christopher D. Manning and Hinrich Schu?tze.1999.
Foundations of statistical natural lan-guage processing.
MIT Press.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of english: The penn treebank.
Computa-tional Linguistics, 19:313?330.Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguis-tics, 27(3):249?276.Peng Xu, Ciprian Chelba, and Frederick Jelinek.2002.
A study on richer syntactic dependenciesfor structured language modeling.
In Proceed-ings of the 40th Annual Meeting of the Associ-ation for Computational Linguistics, pages 191?198.
