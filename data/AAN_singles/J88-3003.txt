MODEL ING THE USER'S  PLANS AND GOALSSandra CarberryDepartment of Computer and Information SciencesUniversity of DelawareNewark, DE 19716This work is an ongoing research effort aimed both at developing techniques for inferring andconstructing a user model from an information-seeking dialog and at identifying strategies for applyingthis model to enhance robust communication.
One of the most important components of a user modelis a representation of the system's beliefs about the underlying task-related plan motivating aninformation-seeker's queries.
These beliefs can be used to interpret subsequent utterances and produceuseful responses.
This paper describes the IREPS system, emphasizing its dynamic construction of thetask-related plan motivating the information-seeker's queries and the application of this component ofa user model to handling utterances that violate the pragmatic rules of the system's world model.
Byreasoning on a model of the user's plans and goals, the system often can deduce the intended meaningof faulty utterances and allow the dialogue to continue without interruption.
Some limitations of currentplan inference systems are discussed.
It is suggested that the problem of detecting and recovering fromdiscrepancies between the system's model of the user's plan and the actual plan under construction bythe user requires an enriched model that differentiates among its components on the basis of the supportthe system accords each component as a correct and intended part of the user's plan.1 INTRODUCTIONIdeally, a natural anguage system's responses houldcontain exactly the information that will be most helpfulto the user.
But since not all users are alike, achievingsuch behavior requires that the system have a model ofthe particular user with whom it is currently interacting.One way of constructing this model is to query the userat the start of the interaction, as was done in theGRUNDY system (Rich 1979).
But querying the usermay fail to provide an accurate and adequate character-ization.
Or extensive questioning of the user may beinappropriate if one of the system's goals is that itsdialog with the user resemble naturally occurring infor-mation-seeking dialogs.
In such cases, the system maybe able to use the information exchanged uring thedialog and its knowledge of the domain to hypothesize amodel of the user, and dynamically adjust and expandthe model as the dialog progresses.One of the most important components of a usermodel is the representation f the system's beliefs aboutthe user's plans and goals.
As demonstrated byCohen,Perrault, and Allen (1981), users of question answeringsystems "expect more than just answers to isolatedquestions.
They expect to engage in a conversationwhose coherence is manifested in the interdependenceof their often unstated plans and goals with those of thesystem.
"We are interested in a class of information-seekingdialogs in which the information-seeker is attempting toconstruct a plan for a task.
The task is not beingperformed uring the system's interaction with the user,as is the case in apprentice-expert dialogs, but instead isbeing constructed for future execution.
In some cases,only a partial plan will be constructed, with furtherdetails filled in later.
For example, a freshman accessingan advisement system may only construct part of hisplan for earning a degree, leaving other aspects of theplan to be fleshed out in subsequent years.
Thesedialogs are typical of a large percentage of interactionswith database management systems, decision supportsystems, and expert systems.
Typical tasks includeexpanding a company's product line, purchasing ahome, or pursuing a degree at a university.
One aspectof our research has been to develop a strategy fordynamically constructing a model of an information-seeker's underlying task-related plan from an ongoingdialog and for tracking his focus of attention i  this plan.Since the system's beliefs about the user's plans andCopyright 1988 by the Association for Computational Linguistics.
Permission tocopy without fee all or part of this material is granted providedthat the copies are not made for direct commercial advantage and the CL reference and this copyright notice are included on the first page.
Tocopy otherwise, orto republish, requires a fee and/or specific permission.0362-613X/88/0100o-o$ 03.00Computational Linguistics, Volume 14, Number 3, September 1988 23Sandra Carberry Modeling the User's Plans and Goalsgoals provide a context for understanding subsequentdialog, we will refer to this part of a user model as acontext model.Our analysis of naturally occurring dialog indicatesthat humans understand many utterances that wouldappear imperfect or incomplete to current natural an-guage systems.
For example, a speaker may inadver-tently use incorrect or ambiguous terminology in con-structing the language representation f his intendedquery, or he may shortcut its complete specification.But if a system's communication is to be regarded asnatural, it must be able to handle the full spectrum ofutterances that humans appear to understand with rel-ative ease.Part of this research as been concerned with how asystem can reason on its context model to remedy manyof a user's faulty utterances and allow the dialog tocontinue without interruption.
We have developed amethod, based on Grice's theory of meaning and maximof relevance, for handling queries that do not conformto the system's model of the world and are thereforeregarded as ill-formed.
This method uses the task-related plan inferred for an information-seeker to sug-gest variants of an ill-formed utterance that mightrepresent the information-seeker's intentions or at leastsatisfy his perceived needs.
We have also developed amethod for understanding intersentential elliptical utter-ances that occur during the course of an information-seeking dialogue.
Our strategy uses discourse xpecta-tions and our model of the speaker' s plan to identify thediscourse goal that he is pursuing via an ellipticalfragment and to interpret his elliptical utterance relativeto his task-related plan.
Our work on understandingellipsis is presented in Carberry (1985).Section 2 of this paper briefly reviews related work inplan recognition, and Section 3 presents our strategy fordynamically inferring amodel of the user's plan from anongoing dialog.
Section 4 describes our mechanism forreasoning on this context model to understand a class ofutterances that is problematic for current natural an-guage systems.
These ideas have been implemented andtested in the IREPS system (Intelligent REsPonse Sys-tem).
This is an ongoing research effort whose objectiveis a robust natural anguage interface to informationsystems.
The component that infers the user's underly-ing task-related plan is called TRACK.
To move fromone domain to another, only the corpus of domain-dependent plans and goals must be reconstructed; theheuristics and processing strategies are completelytransportable.Section 5 describes our current research on relaxingrestrictive assumptions present in previous work onplan recognition and developing a more robust planinference framework.
We propose a four-phase ap-proach to handling the problem of possible disparitybetween the system's context model and the actual planunder construction by the user, and argue for an en-riched context model that differentiates among its com-ponents according to the support it accords each com-ponent as a correct and intended part of the user's plan.Th:roughout this paper, the information-seeker andinforraation-provider will be referred to as IS and IP,respectively.2 RELATED WORK ON PLAN RECOGNITIONIN DIALOGEarly work in dialog understanding concentrated onapprentice-expert dialogs, during which an expertguided an apprentice in performing a task.
Grosz (1977)formulated heuristics for recognizing shifts in focus ofattention in the task structure and presented a strategythat used the knowledge currently focused on by thedialog participants to identify the referents of definitenoun phrases appearing inan utterance.
Robinson (Rob-inson et al 1980, Robinson 1981) extended Grosz'swork and developed a process model for determiningthe referents of verb phrases, such as variants of do inthe utterance"I 've done it.
"However, in apprentice-expert dialogs, the overall taskthat the apprentice is attempting toperform is known atthe outset of the dialog, and the ordering of actions andsubtasks in the plan being executed strongly influencesthe dialog between expert and apprentice.
This differsfrom the kind of information-seeking dialogs that we areinvestigating, in which the information-seeker is at-tempting to construct a plan for a task that will beexecuted at a later time.
In such dialogs, the informa-tion-seeker's utterances are not tightly constrained bythe order in which actions in the task will eventually beexecuted.
For example, in an information-seeking dia-log between a client and a travel agent, the client mayfirst plan hotel accommodations and theater attendancein New York before inquiring about ways to reach NewYork, even though travel to New York will occur beforeattend a New York theater in a temporal ordering ofactions in the resultant plan.Allen (Allen et al 1980, Perrault et al 1980) inferredthe goal underlying a speaker's utterance in the contextof an information agent in a train setting.
This inferredgoal was used to account for extra helpful informationincluded in the agent's response, and the inference pathconnecting the speaker's utterance and the inferred goalwas used to interpret indirect speech acts.
However,Allen's domain was very restricted; the only domaingoals were meeting a train and boarding a train, each ofwhich could be accomplished by a few primitive steps,and his system was primarily concerned with utterancesthat might occur at the outset of a dialog.
In morecomplex domains, the information-seeker's completeplan will consist of a hierarchy of subplans and subgoalsthat accomplish is overall goal.
Such a complete plan isnot immediately evident from a single utterance, and24 Computational Linguistics, Volume 14, Number 3, September 1988Sandra Carberry Modeling the User's Plans and GoalsLearn-Material(_agent:&PERSON, -sect:&SECTIONS, _syI:&SYLLABI)Plan-Body:Learn-From-Person(_agent:&PERSON, .sect:&SECTIONS, _fac:&FACULTY)where Teaches(_fac:&FACULTY, _sect:&SECTIONS)Learn-From-Text (.agent:&PERSO N, _txt:&TEXTS)where Uses(_sect:&SECTIONS, _txt:&TEXTS)Effects:Learn-Material(_agent:&PERSON, -seet:&SECTIONS, -syl:&SYLLABI)Figure 1.
Plan For Learning Course Material.individual utterances must be related to one another tobuild the user's plan as the dialog progresses.Sidner (1983, 1985) and Litman (1986) developedenhanced models of plan inference.
However, bothwere concerned with dialogs that were initiated in orderto begin or continue execution of an underlying task(display of structures on a graphics terminal and meet-ing/boarding a train), and the dialogs were thereforeconstrained by the order in which individual actions inthe task had to be executed.
In addition, Sidner inves-tigated how discourse markers aid in recognizing thespeaker's intent, and Litman studied ameta-plan frame-work for task dialogs.Pollack (1986) has recently proposed that plans beviewed as mental phenomena.
She contends that, inorder to comprehend an utterance and relate it to theuser's plan, the system must reason about the config-uration of beliefs and intentions that it should ascribe tothe speaker.
This will be discussed further in Section 5.3 INFERRING AND MODELING THE TASK-RELATED PLANIn order to reason about what the user wants to accom-plish, the system must have knowledge about he goalsthat a user might pursue in a domain and plans foraccomplishing these goals.
We view a plan as the meansby which an agent can accomplish a non-primitivetask-related goal.
Using an extension of the STRIPSformalism (Fikes et al 1971), we represent a plan as astructure containing applicability conditions, precondi-tions, a plan body, and effects.Applicability conditions and preconditions both rep-resent conditions that must exist before a plan can beexecuted.
However, an agent can plan to satisfy pre-conditions, whereas it is generally anomalous to plan tosatisfy applicability conditions; the latter determinewhether it is reasonable to even consider a particularplan for achieving a desired goal.
For example, supposean agent wants to vacation on a particular island.
If theisland has an airport and the agent has money for aticket, then the agent can plan to fly there.
But therequirements that the island have an airport and that theagent have money for a ticket are intuitively different.
Ifthe agent does not have enough money for a ticket, hecan plan to try and satisfy this requirement; but if theisland does not have an airport, it is unreasonable forthe agent to arrange for an airport to be built on theisland so that he can fly there for a vacation.Of course, agents ometimes do unreasonable acts.
Ifthe agent in the above case is very wealthy, is adamantabout vacationing on this particular island, and abhorsboats, he may build an airport on the island and chartera plane to fly him there.
Our plans are intended torepresent normal plans that an agent might be expectedto pursue, and the distinction between preconditionsand applicability conditions i  useful in preventing con-sideration of plans that would occur only in exceptionalcircumstances.
How exceptional p ans should be incor-porated into a plan recognition system is an area forfuture work.Wilkins used preconditions similar to our applicabil-ity conditions in representing operators in the SIPEsystem (Wilkins 1984).
What we call a precondition,Wilkins incorporated into the set of actions and goalscomprising an operator.
His reasons for having un-plannable preconditions in his representation schemewere both to capture the appropriateness of applying anoperator in a given situation and to connect differentlevels of detail in a hierarchical planner.
A proposition,that at one level of abstraction was part of the specifi-cation of how an operator was to be performed, mightappear at a lower level of abstraction as a preconditionof an operator, indicating that further planning for thelower level operator is inappropriate unless the propo-sition is already satisfied.
But mixing standard precon-ditions (conditions that must exist before an operatorcan be performed, but which can be planned for) withthe set of goals and actions that constitute performingan operator fails to capture the intuitive differencebetween the two.
For this reason, our representationscheme distinguishes among applicability conditions,preconditions that can be planned for, and how one goesabout performing an action.The plan body contains a conjunction of subgoals,and the effects represent he results of successfullyexecuting the plan.
Arguments in plans are either con-stants, represented asuppercase strings, or typed vari-ables, represented aslowercase strings preceded by thecharacter "_"  and followed by the characters " :&" andComputational Linguistics, Volume 14, Number 3, September 1988 25Sandra Carberry Modeling the User's Plans and Goalsan uppercase string giving the variable's type.
Figure 1presents a sample plan used by TRACK.
Its plan bodystates that in order to learn the material of a section ofa course, an agent must both learn from the personteaching the section and study the text used in thesection.
TRACK's plans are hierarchical, since many ofthe subgoals in the bodies of plans and many precondi-tions are non-primitive and therefore have associatedplans which may be substituted for them.
Thus a plancan be expanded to any desired degree of detail byreplacing non-primitive preconditions and subgoals withtheir associated plans.At the outset of an information-seeking dialog, thesystem has little knowledge about the information-seeker's (IS) purpose in requesting information.
In mostcases, a complete plan for IS cannot be constructedduring the first part of a dialog.
Instead, potential goalsmust be inferred from individual utterances and inte-grated into the overall plan structure, thereby incremen-tally expanding and instantiating the system's model ofIS's plan as the dialog progresses.Oftentimes there will be many domain goals that asingle utterance might address.
For example, if a stu-dent asks what time Dr. Smith arrives in the morning,he may want to either visit Dr. Smith or call him in hisoffice.
Furthermore, even if we can identify a singledomain goal addressed by an utterance, there may bemany ways in which that goal could be incorporatedinto an overall plan.
For example, if a student askswhether Political Science 210 is offered in the spring, wemight infer that the student wants to take PoliticalScience 210.
But how should this goal be built into thestudent's overall plan?
Perhaps he is considering takingPolitical Science 210 in order to satisfy a breadthrequirement or perhaps he wants to major or minor inPolitical Science.
So the issue that must be addressed indynamically inferring IS's underlying task-related planfrom an ongoing dialog is the following: how can weidentify which of many candidate goals is the actualgoal which IS is addressing with a particular utterance,and how can we determine where this particular goalfits into IS' s overall plan?Two factors appear to provide a basis for a solution:1) the organized nature of naturally occurring informa-tion-seeking dialogues, as exhibited in dialog tran-scripts, and 2) the assumption that IS and IP areworking cooperatively to help IS achieve his planconstruction goals.
These two factors produce a struc-ture in information-seeking dialogs.
As a result, we canformulate focusing heuristics that specify how individ-ual utterances hould be related to the existing dialogcontext, as represented by the plan inferred for IS andhis current focus of attention in that plan.Thus our approach is the following:1. hypothesize from an individual utterance a set ofdomain-dependent candidate focused goals thatmay represent the information-seeker's focus ofattention in the task; and2.
use focusing heuristics to select the candidatefocused goal most apropos to the existing dialoguecontext and incorporate it into the model of theinformation-seeker's plan.In some cases, several candidate focused goals may beequally likely, and alternative versions of the contextmodel may need to be built.
In a cooperative, coherentdialog: in which the information-seeker successfullycommtlnicates how his questions relate to what hewants to accomplish, subsequent utterances houldenable the system to identify the particular contextmodel that represents the user's plan.
However, if theuser asks a sequence of questions that have no definiterelationship to one another, then we may have a com-putationally explosive situation.
But this behavior vio-lates our assumption of an overall cooperative, coherentdialog.We have given some preliminary consideration to amore robust process model containing a stack of disjointcontexts with potential relationships to one another.Further utterances could clarify these relationships andpermit merger of the disjoint contexts into a singleoverall context model.
Such a strategy would have theadvantage of handling disconnected portions of dialogswithout the computational explosion that can resultfrom modeling all possible expanded contexts.3.1 HYPOTHESIZING CANDIDATE FOCUSED GOALSAND PLANSThe first stage of processing analyzes an utterancewithout considering the preceding dialog.
Plan-identifi-cation heuristics are used to hypothesize a set ofdomain-dependent goals and associated plans that mightrepresent that aspect of the task on which IS's attentionis currently focused.
These heuristics are extensions ofinference rules proposed by Allen (Allen et al 1980).For example, if IS wants to know the values of anargument that cause a proposition to be true, then thatproposition or its negation may be relevant o the planthat IS is considering.
Therefore any goals whose plansmight prompt such a request become candidate focusedgoals and their associated plans become candidatefocused plans.
Thus if IS asks,"Who is teaching section 10 of French 112 in thespring of 1988?
"then IS wants to know the values of the argument _fac:&FACULTY such that the propositionTeaches(_fac:&FACULTY,FRENCH 112-10-SPRING88)is true.
The plan for learning the material of a section ofa course (Figure 1) contains the propositionTeaches(_fac:&FAC U 1 ,TY, _sect: &SECTIONS)as a constraint on a subgoal in its body.
SubstitutingFRENCHl12-10-SPRING88 for _sect:&SECTIONSproduces the proposition26 Computational Linguistics, Volume 14, Number 3, September 1988Sandra Carberry Modeling the User's Plans and Goals* Leara-Ma~.rlal(ISt FRENCHII\[-10-SPRINGS8, JyI:&SYLLABI)Figure 2.
An Example of a Context Model.ation by IS.
In Figure 2, the active path contains thegoalsEarn-Credit(IS,FRENCH112,SPRING88,_cr2:&CREDITS)Earn-Credit-Section(IS,FRENCH 112- l0-SPRING88)Learn-Material(IS, FRENCH112-10-SPRING88,_syI:&SYLLABI)with the current focus of IS's attention believed to beTeaches(_.fac: &FACULTY,FRENCH112-10-SPRING88)addressed by IS's utterance.
Making this substitutionthroughout the plan in Figure 1 produces a plan forlearning the material of section 10 of French 112 in thespring of 1988.
Therefore the goalLearn-Material(IS, FRENCH112-10-SPRING88,_syI:&SYLLABI)becomes a candidate focused goal, and the plan pro-duced by substituting FRENCHI12-10-SPRING88 for_sect:&SECTIONS in Figure 1 becomes a candidatefocused plan.
The goalLearn-From-Person(IS, FRENCH 112-10-SPRING88,_fac:&FACULTY)whereTeaches(__fac:&FACULTY,FRENCH 112-10-SPRING88)is the most recently considered subgoal in this candidatefocused plan; thus it provides the greatest expectationsfor future utterances.3.2 CONTEXT PROCESSINGThe second stage relates an utterance to the contextestablished by the preceding dialog.
We use a treestructure called a context model to represent the task-related plan inferred for IS from the preceding dialog.Each node in this tree represents a goal that the systembelieves IS is considering achieving and, except for theroot, is a descendant of a higher-level goal whoseassociated plan contains the subgoal represented by thechild node.
In Figure 2, for example, learning thematerial for section 10 of French 112 appears as adescendant of the higher-level goal of earning credit insection 10 of French 112, representing the belief that ISis considering how he would go about learning thematerial of the section as part of a plan for earning creditin the section.One node in the tree is marked as the current focus ofattention and indicates that aspect of the task on whichIS's attention is currently centered.
The path from theroot of the context model to the current focus ofattention is called the active path and represents theglobal context, or sequence of progressively lower-levelgoals that led to the subgoal currently under consider-Learn-Material(IS, FRENCH112-10-SPRING88,_syI:&SYLLABI)Initially, there is no existing context; each candidatefocused goal and its plan become the root of a contextmodel and are marked as current focused goals andplans.
If there is only one context model and its rootgoal appears as part of only one domain-dependentplan, then we have further knowledge about what ISwants to do and can add this higher-level plan as thenew root of the context model, with the old root as itschild.
We continue expanding the tree upward untilmore than one higher-level plan is possible.
For exam-ple, if IS's first utterance was"Who is teaching section 10 of French 112 in thespring of 19887"then, as described in the previous ection,Learn-Material(IS, FRENCH112-10-SPRING88,_syI:&SYLLABI)would become a candidate focused goal.
This goal andits associated plan would be entered as the root of acontext model and be marked as the current focus ofattention.
In addition, since only the plan for earningcredit in section 10 of French 112 contains this goal, andsince only the plan for earning credit in French 112contains the goal of earning credit in a section of French112, the context model is expanded upward to includethese higher-level goals, producing the context model inFigure 2.
Thus, only that part of the user's task-relatedplan that the system believes IS intended it to recognizeis built into the context model.
Section 5 discusses morerobust user modeling, in which default inference rulesmight be used to expand the system's model of theuser's plan, and addresses the problem of detecting andrecovering from errors that might be introduced into themodel.As each new utterance occurs, it must be related tothe established context.
A set of focusing heuristics areused to determine the most likely relationship betweenone of the hypothesized candidate focused plans and thecontext model, and to expand the context model toinclude it.
Grosz (1977) introduced the concept offocusing in her work on identifying the referents ofdefinite noun phrases in apprentice-expert dialogs.
Shenoted that the focus of the discourse followed the planfor performing the apprentice's task.
Our information-seeking dialogs differ from apprentice-expert dialogs inComputational Linguistics, Volume 14, Number 3, September 1988 27Sandra Carberry Modeling the User's Plans and Goalsthat our dialogs are not constrained by the order ofexecution of the actions in the overall plans.
However,we do find structure in the dialogs we are studying, andit is the basis for our focusing heuristics.
This structureappears to be caused by two factors.The first is the organized nature of naturally occur-ring information-seeking dialogues.
Dialogue transcriptsindicate that humans generally ask all their questionsthat are relevant to a plan for one subgoal before movingon to ask questions about a plan for another subgoal ofthe overall task.
One possible explanation for thisbehavior is that it may require less mental effort thanswitching back and forth among partially constructedplans for different subgoals.The second factor producing structure in our dialogsis their cooperative nature.
Since the dialogs are coop-erative and miscommunication can occur if both dialogparticipants are not focused on the same subset ofknowledge (Grosz 1981), we expect IS to shift topicslowly between consecutive utterances and to adhere tothe focusing constraints espoused by McKeown (1985).McKeown expanded on focus rules proposed by Sidner(1981) to explain how speakers hould organize theirutterances when faced with a choice of topic.
In partic-ular, McKeown claims that a speaker should move to arecently introduced topic if he has something further tosay about it; otherwise he will have to reintroduce thetopic at a later time.
Similarly, the speaker shouldchoose to finish discussion of the current topic beforeswitching back to a previous one.Our focusing heuristics rely on these expectationsabout possible shifts in focus of attention in IS' s under-lying task-related plan to identify which candidate fo-cused plan is most apropos to the established ialogcontext and to determine how it fits into the contextmodel.
The following ordered list gives the focusingheuristics' preferences on the relationship between acandidate focused plan and the context model.
Eachrelationship is illustrated under the assumption that thetree shown in Figure 3a is the context model immedi-ately preceding the utterance, with node C (marked byan asterisk) representing the current focused goal/planand node G representing the most recently consideredsubgoal in the current focused plan.1.
The candidate focused plan is part of the expan-sion of a plan for the most recently consideredsubgoal in the current focused plan; for example,if Figure 3b is an expansion of the context modelshown in Figure 3a, then node C1 might representsuch a candidate focused plan.2.
The candidate focused plan is part of an expansionof the current focused plan.
For example, if Fig-ure 3b is an expansion of the context model shownin Figure 3a, then node C2 might represent such acandidate focused plan (where C2 is part of a planfor the goal at node F, which is in turn part of aplan for the goal at node C).3.
The candidate focused plan is part of the expan-p8? '
pC.Figure 3a.
Initial Context Model.,~tC "~cS p6Figure 3b.
Figure 3c.pQp~Figure 3d.
Figure 3e.Figure 3.
Examples of Relationships Between CandidatePlans and Context Model.28 Computational Linguistics, Volume 14, Number 3, September 1988Sandra Carberry Modeling the User's Plans and Goals* Satisfy-Major(IS, BA, CS) * Satlsfy-Major(IS, BS, CS)Figure 4.
Two Context Models.sion of a plan for a goal along the active path, withpreference given to goals that are closest o thecurrent focused goal on the active path.
Forexample, if Figure 3b is an expansion of thecontext model shown in Figure 3a, then nodes C3and C4 would both be part of the expansion of aplan for a goal along the active path; but if nodesC3 and C4 both represent candidate focusedplans, then node C3 would be preferred, since itappears in an expansion of the plan associatedwith node B, which is closer to the current fo-cused goal (represented by node C) than is nodeA.4.
The candidate focused plan is a plan whose ex-pansion contains the goal associated with the rootof the context model; Figure 3c illustrates uch arelationship, where node C5 represents a candi-date focused plan.5.
The candidate focused plan is part of the expan-sion of a higher-level p an, and this expansion alsocontains the goal associated with the root of thecontext model; Figure 3d illustrates uch a rela-tionship, where node C6 represents a candidatefocused plan.In applying each rule, we use a breadth-first expansionof plans, so that the resulting shift in focus of attentionwill be as small as possible.
For example, if nodes C7and C8 in Figure 3e both represented candidate focusedgoals/plans, the second rule in the above list wouldprefer C7 to C8, since C7 is closer to the existing focusof attention in the dialog.3.3 AN EXAMPLETo illustrate this plan inference process, let us considera dialog segment containing four utterances by IS.Suppose IS begins with the statement" I  want to major in computer science.
"Since IS states that he wants to achieve agoal, majoringin CS, TRACK's plan identification heuristics hypoth-esizeSatisfy-Major(IS, BA, CS)andSatisfy-Major(IS, BS, CS)as candidate focused goals and their associated plans ascandidate focused plans.
Since there is no way ofchoosing between these, two context models would bebuilt, each with one of the candidate focused goal/planpairs as its root (Figure 4).
The resulting current focusedplan in each context model is preceded by an asterisk.Suppose that IS's next utterance is the querySatisfy-Majoi(IS , BA, CS)-1~ Earn-Credit(IS,CS 180,.a~:&SEMESTERS,_crl :&CREDITS)S~ti~ry-Majol(LS, BS, O )Earn-Oredit(IS,C$180,.~:&SEMESTERS,_erl :&CREDITS)Figure 5.
Context Models After Two Utterances.
"What are the prerequisites for taking CS180?
"Since IS wants to know the preconditions for the planassociated with the goal of taking CS180 (the introduc-tory course for majors and minors in computer science),TRACK hypothesizesEarn-Credit(IS, CS180, _ss:&SEMESTERS,_cr 1: &CREDITS)and its associated plan as the candidate focused goal/plan pair.
The focusing heuristics must determine howthis relates to the preceding dialog, as represented bythe context model.
The strongest expectation is that ISis continuing with some aspect of the current focusedplan; since the preceding utterance did not address anyparticular goal in this plan, there is no most recentlyconsidered subgoal.
Since taking CS180 appears in anexpansion of the plans for majoring in computer sci-ence, TRACK expands the context models as shown inFigure 5 and marks the plan for earning credit in CS180as the new current focus of attention.Suppose that IS's next query is,"What courses must I take in order to satisfy theforeign language requirement?
"Since IS is asking about the argument (courses) of asubgoal (taking courses) that is part of a plan forachieving a second goal, TRACK hypothesizes thesecond goal and its associated planSatisfy-Language-Req(IS)as the candidate focused goal/plan pair.
The focusingheuristics must now determine how this relates to thepreceding dialog, as represented in the context model.The strongest expectation is that IS will continue withsome aspect of the current focused plan.
However, thecandidate focused plan does not appear in an expansionof the current focused plan, indicating that IS hasshifted focus to another aspect of the overall task.
Infact, none of the first four focusing heuristics find arelationship between the candidate focused plan and theComputational Linguistics, Volume 14, Number 3, September 1988 29Sandra Carberry Modeling the User's Plans and Goals* ~:  !1.
k~.EDITS)Figure 6.
Context Model After Three Utterances.context model.
However, the last focusing heuristicfinds that there is a goal,Obtain-Degree(IS, BA)whose associated plan can be expanded to include boththe candidate focused plan and the context modelwhose root is Satisfy-Major(IS, BA, CS), indicating thatIS has shifted his attention to another subtask (satis-fying the foreign language requirement) ofa higher-levelplan (obtaining a bachelor of arts degree), of which theold current focused plan (obtaining a computer sciencemajor) is also a part.
Therefore this goal becomes theroot of a new context model, as shown in Figure 6;Satisfy-Language-Req(IS) is marked as the new currentfocus of attention, as indicated by the asterisk precedingit.
The other previous context model, whose root wasSatisfy-Major(IS, BS, CS), is discarded, indicating thatIS's third utterance has led us to deduce that he wantsto pursue a bachelor of arts degree.
Note that our planinference process makes what Pollack (1987) terms theappropriate query assumptionwnamely, that IS doesnot ask queries that are inappropriate to his intendedgoal.
This aspect of our plan inference process will bediscussed further in Section 5, where we discuss a morerobust plan recognition paradigm.Suppose that IS's next query is,"Who is teaching section 10 of French 112 in thespring of 1988?
"As described earlier, since IS is asking about theteacher of a particular section of a course, he may beconsidering the subgoal of learning from that teacher;this subgoal appears in aplan for learning the material ofa course, and therefore TRACK hypothesizes the planassociated with the goalLearn-Material(IS, FRENCH112-10-SPRING88,_syl:&SYLLABI)as one of the candidate focused plans.
The focusingheuristics find that this candidate focused plan appearsin an expansion of the most recently considered subgoal(taking courses) in the current focused plan, and there-fore it is selected as the new focus of attention and thecontext model is expanded to include it (Figure 7).In this manner, our plan inference process dynam-ically infers from an ongoing dialog the underlyingtask-related plan motivating an information-seeker'squeries and tracks his focus of attention in this planstructure.4 APPLICATION OF CONTEXT MODELSThe context model is one component of a comprehen-sive user model, representing the system's acquiredbeliefs about he plan an information-seeker is trying toconstruct.
The possible xpansions of this plan provideexpectations about information that IS might want, andthese expectations can often be used to repair anddisambiguate IS's subsequent u terances.
We have de-veloped strategies that use our context model to handletwo forms of problematic input: pragmatically ill-formed utterances and intersentential elfipsis.
This sec-tion describes our approach to the first of these; ourframework for handling ellipsis is described in Carberry(1985).4.1 PRAGMATIC ILL-FORMEDNESSAn utterance can be syntactically and semantically wellformed, yet violate the structural properties of thelistener's world model.
This is not to say that thespeaker necessarily holds an incorrect view of theworld, or even one that differs from the listener's view,but only that the semantic representation f the speak-er's utterance does not conform to the listener's worldmodel.
We shall say that such an utterance ispragmat-ically ill-formed.Consider, for example, the queryIS: "What is the area of the special weapons maga-zine of the Alamo?
"that appears in a dialog transcript of an information-seeker attempting to load cargo onto ships using theREL natural anguage interface (Thompson 1980).
Asemantic representation f this query will contain thepropositionArea(SPECIAL-WEAPONS-MAG, _areaval:&SQ-FT)The system was unable to understand this query, sinceits semantic representation erroneously presumed thatstorage locations had an area attribute in the associateddata base.
ff a human information-provider had a similarproblem in understanding the utterance, or consideredthe meaning of "area" ambiguous, he might be able touse the context established by the preceding dialog toidentify what the information-seeker r ally wanted toknow.
For example, if IS's goal was to load cargo of theappropriate type into the various cargo holds, then heprobably wanted to know the remaining capacity of theSpecial Weapons Magazine.
On the other hand, if hisgoal was to assign ships to routes in order to handle the30 Computational Linguistics, Volume 14, Number 3, September 1988Sandra Carberry Modeling the User's Plans and GoalsSatidy-Satisfy-LanlEarn-Credit (IS,PRENOH112Obtain-Degree(IS, BA)age-Req(|S);PRING88,.cr2:&CREDITS)Sat~'fy-ne~-~jor (XS, nx)Satidy-Majlr(IS , CS, BA)Earn-Credit (IS,CS 180,-m:&SEMESTERSrerl :&CREDITS )Eam-Credit-Sectlon(IS, FiENOHI l~.-10-SPRING88)" Learn-Material(IS, PRENOHII2-10-SPRING88, .syI:&SYLLABI)Figure 7.
Context Model After Four Utterances.expected cargo shipping requirements, then IS probablywanted to know the total capacity of the Special Weap-ons Magazine.
Similarly, if his goal was to assignworkers to fill the storage holds, with one workerassigned to handle all cargo holds located in the samesection of the ship, then IS probably wanted to knowthe location of the Special Weapons Magazine.Another example of a pragmatically ill-formed queryillustrates the missing joins problem.IS: "Who is teaching section 10 of French 112 in thespring of 1988?
"IP: "Dr.
Walker.
"IS: "When's Mitchel meet?
"A semantic representation f the last query contains thepropositionMeeting-Time(MITCHEL,_tme:&MEETING-TIMES)Suppose that in the system's world model, faculty teachsections of courses, chair committees, and presentcolloquia, and each of these has a scheduled meetingtime, but there is no direct relationship between facultyand times.
Then the above query will appear pragmati-cally ill-formed.
Although this utterance might be anabbreviated version of any of the queries"When does the section of French 112 taught by Dr.Mitchel meet?
""When does the committee chaired by Dr.
Mitchelmeet?
""When does the colloquium given by Dr.
Mitchelmeet?
"a human information-provider would be likely to recog-nize from the above dialog that IS wants the meetingtime of the section of French 112 taught by Dr. Mitchel,and respond accordingly.4.2 UNDERSTANDING PRAGMATICALLY ILL-FORMEDQUERIESIf a natural anguage system's communication is to beregarded as natural, the system must be able to handlethe full spectrum of utterances that humans understandwith relative ease.
But our analysis of naturally occur-ring dialog indicates that human listeners understandmany utterances that would appear pragmatically ill-formed to current natural anguage systems.
A numberof researchers have investigated the problem of han-dling pragmatically ill-formed queries (Sowa 1976,Chang 1978, Mays 1980, Kaplan 1982), but their strate-gies were deficient in that they considered the queries inisolation, without using a model of the preceding dialogto address the speaker's intentions.Grice's theory of meaning (Grice 1969, Grice 1957)and maxim of relation (Grice 1975) suggest hat thelistener's beliefs about what the speaker is trying to doshould be used to recognize the intent behind an ill-formed query.
According to Grice's theory, a listenershould believe that the speaker believes the listener caninfer the intended meaning of an utterance--otherwisethe speaker would not have made it.
So given a prag-matically ill-formed query, a cooperative listener shouldattempt o deduce these intentions.
Grice's maxim ofrelation suggests that the speaker's utterance is relevantto the existing dialog context, so the listener should usethis context and the focus of attention immediately priorto the problematic utterance to attempt o deduce thespeaker's intended meaning and enable the dialog tocontinue without interruption.Our strategy is based on this theory of meaning andintenticm.
It uses the context model to suggest substi-tutions for the erroneous proposition appearing in thesemantic representation f IS's pragmatically ill-formedquery, thereby producing semantic representations forone or more revised queries, all of which are apropos towhat IS is trying to accomplish.
If more than oneComputational Linguistics, Volume 14, Number 3, September 1988 31Sandra Carberry Modeling the User's Plans and Goalsrevised query is proposed, then it must be determinedwhether any of these is significantly more likely than theothers to represent the speaker's intentions or satisfyhis perceived needs.
Two criteria appear appropriate forcomparing suggested revised queries.
The first is therelevance of the revised query to the current focus ofattention in the dialog.
Since we have contended thatsome shifts in focus of attention in the plan structure aremore likely than others, it is reasonable to hypothesizethat the more expected the shift in focus of attentionthat would result from a revised query, the more likelyis that query to represent the speaker's intentions.
Thesecond criteria for comparing suggested revised queriesis the similarity of a revised query to the speaker'sactual utterance.
For example, color has less semanticsimilarity to area than does remaining capacity.
There-fore substituting "color" for "area" in the examplequery"What is the area of the special weapons magazine ofthe Alamo?
"is a more significant alteration of the query than issubstituting "remaining capacity" for "area."
As aresult, the revised query"What is the color of the special weapons magazineof the Alamo?
"is less similar to the speaker's actual query than is therevised query"What is the remaining capacity of the special weap-ons magazine of the Alamo?
"Therefore our pragmatic ill-formedness processor con-tains a suggestion mechanism and a selection mecha-nism.
The suggestion mechanism proposes revised que-ries, all of which are relevant to IS's underlyingtask-related plan, and the selection mechanism uses thecriteria of relevance and semantic similarity to select,from among multiple suggestions, the revised querydeemed most likely to represent he speaker's inten-tions or satisfy his perceived needs.4.3 REPAIR STRATEGY4.3.1 PROPOSING REVISIONSThe suggestion mechanism uses two sets of substitutionheuristics, one for making simple substitutions of aproperty, relation, function, or object class for that usedby the speaker, and a second set for expanding rela-tional paths to handle the missing joins problem.As an example of a simple substitution, suppose thedialogue preceding the query"What is the area of the special weapons magazine ofthe Alamo?
"indicates that IS's current focused goal within hisoverall plan is to load cargo of the appropriate type intothe various cargo holds.
A subgoal in the plan associ-ated with this goal would beLoad-Type-Cargo(IS,_item: &CARGO),_storearea: &STORAGE-AREAwhereIs-Type(_item: &CARGO, _cartype: &CARGO-TYPE)Cargo-Type(_storearea: &STORAGE-AREA,_cartype:&CARGO-TYPE)and a plan for this subgoal would contain the precondi-tionGreater(_remcap:&CUBIC-FT,_itemsize:&CUBIC-FT)whereVolume(_item:&CARGO, _itemsize:&CUBIC-FT)Remaining-Capacity(_storearea: &STORAGE-AREA,_remcap:&CUBIC-FT)specifying that the storage area must have room for thecargo :item.
The property substitution heuristic wouldexamine this plan and suggest substituting either of thepropositionsCargo-Type(SPECIAL-WEAPONS-MAG,_cartype:&CARGO-TYPE)andRemaining-Capacity(SPECIAL-WEAPONS-MAG,_remcap:&CUBIC-FT)for the erroneous propositionArea(SPECIAL-WEAPONS-MAG, _areaval:&SQ-FT)appearing in the semantic representation f IS's query,producing suggested semantic representations equiva-lent to the two revised queriesIS: "What is the cargo type of the Special WeaponsMagazine of the Alamo?
"IS: "What is the remaining capacity of the SpecialWeapons Magazine of the Alamo?
"More formally, this heuristic is represented by thefollowing rule:If IS's proposition erroneously presumes that a mem-ber Objl of CLASS1 has a property Attl, thenreplace property Attl with property Art2 if the fol-lowing conditions hold:1. a proposition specifying property Att2 on a mem-ber Obj2 of CLASS1 appears in an expansion ofIS's context model.2.
Objl and Obj2 unify (Either Objl in IS's utteranceor Obj2 in the plan proposition refers to a generalmember of CLASS1, or both refer to the samespecific member of CLASS1).In the context of our student advisement dialogs, sup-pose a student wants to pursue an independent studyproject; such projects can be directed by full-timefaculty but not by faculty who are extension or on32 Computational Linguistics, Volume 14, Number 3, September 1988Sandra Carberry Modeling the User's Plans and Goalssabbatical.
The student might erroneously follow theutterance"I want to take an independent s udy project.
"with the pragmatically ill-f0rmed query"What is the classification of Dr.
Smith?
"In a university world model, only students have aclassification attribute; this attribute can have valuessuch as Arts&Science-1988, Engineering-1989, andBusiness-1990.
Faculty have attributes uch as rank,status, age, and salary.
Pursuing an independent s udyproject under the direction of Dr. Smith has the precon-dition that Dr. Smith's status be full-time or part-time.Our substitution mechanism would analyze the plan fortaking an independent study course, and the propertysubstitution rule would suggest substituting the propo-sitionEarn-Credit-Section(IS, ..secl:&SECTIONS)whereIs-Seetion-Of(..secl :&SECTIONS, FRENCH 112)whereIs-Syllabus-Of(.secl :&SECTIONS, ..sylI:&SYLLAB I)Learn- From- Person ( IS,.lec I :& IE CTIO N S,.fac:& FACULTY)wherewhereIs-Meeting- P\]~ce{..sec I :& S E CTIONS, .ple:&MEETINGPLCS)Is-Meeting- D ay( ..see h& S E CTI ON S, ..day:&MEETING DAYS)Is- Id~ting-Time(.secl:&SECTlON S, -tme:,~MEETINGTIMES)Figure 8.Status(DR.SMITH, _statusval: &STATUSVALUES)for the erroneous propositionClassification(DR. SMITH,_classval:&CLASSVALUES)appearing in the semantic representation of the stu-dent's query, resulting in a suggested revised semanticrepresentation equivalent to the query"What is the status of Dr.
Smith?
"As an example of the second set of heuristics, the pathexpansion heuristics, consider again the query"When's Mitchel meet?
"following the dialog that produced the context modelshown in Figure 7.
As mentioned earlier, the semanticrepresentation of this query contains the erroneouspropositionMeeting-Time(MITCHEL,_tme:&MEETING-TIMES)indicating a direct relationship between faculty andtimes.
Our path expansion heuristics will analyze andexpand the context model shown in Figure 7 and notethat a plan for the goalEarn-Credit(IS, FRENCHl l2,  SPRING88,_cr2:&CREDITS)can include a path containing the sequence of goalsshown in Figure 8.
One path expansion heuristic notesthat the propositionsTeaches(_fac: &FACULTY, _sec 1 :&SECTIONS)Is-Meeting-Time(_sec 1 :&SECTIONS,_tme:&MEETINGTIMES)both appear on this path in the expanded plan, andsuggests substituting the conjunction of the propositionsTeaches(MITCHEL, _sec 1 :&SECTIONS)Is-Meeting-Time(_sec 1 :&SECTIONS,_tme: &MEETINGTIMES)for the erroneous proposition appearing in the semanticrepresentation of IS's query, resulting in a revisedsemantic representation equivalent to the English query"When do sections taught by Mitchel meet?
"The revised semantic representation no longer violatesthe system's world model.
But it represents an incom-plete query, in that it contains an ellipsis.
Presumablythe speaker wants to know only the sections of French112 taught by Dr. Mitchel in the spring of 1988, notsections of any course taught by Dr. Mitchel during anysemester.
How the context model can be used tointerpret elliptical utterances i discussed in Carberry(1985).
Although we have only illustrated substituting aconjunction of two propositions for the erroneous prop-osition in the user's query, the path expansion heuris-tics can propose xpansions of any length.Five other heuristics and other parts of the user'splan can suggest substitutions in addition to the onesshown in our examples.
The important point is that allof the revised semantic representations resulting fromthese suggestions represent queries that are apropos tothe plan that IS is constructing.4.3.2 SELECTING THE APPROPRIATE REVISIONAs mentioned earlier, relevance to the current focus ofattention and similarity to the speaker's actual utteranceare used to select from among multiple suggestions.
Weuse focusing heuristics, similar to those used for con-structing the context model, to measure relevance of arevised query to the current focus of attention in thedialog, and generalization hierarchies for properties,relations, functions, and object classes to measure thesemantic similarity of a substituted term and the termthat it replaces.
In the exampleComputational Linguistics, Volume 14, Number 3, September 1988 33Sandra Carberry Modeling the User's Plans and Goals"What is the area of the Special Weapons Magazineof the Alamo?
"both suggested revised queries have approximately thesame relevance to the current dialog but, of the twoproperties cargo type and remaining capacity, remain-ing capacity is much closer semantically tothe propertyarea used by the speaker.
Therefore our selectionmechanism chooses the semantic representation equiv-alent to the query"What is the remaining capacity of the SpecialWeapons Magazine of the Alamo?
"as the most appropriate interpretation representing IS'sneeds.Instead of computing semantic representations forallsuggested revised queries and then selecting the bestrevision, we analyze nodes of the context model inorder of decreasing relevance to the existing focus ofattention, until a revision meeting an arbitrary level ofacceptability is found.
This acceptability level initially isset so that only revisions with extremely good evalua-tions will meet it, and it is steadily relaxed as largerparts of the context model are analyzed.
Since onefactor used by the evaluation metric is relevance to theexisting focus of attention in the dialog, scores fornewly suggested revisions will, in most cases, be worsethan the scores for revisions suggested much earlier.Thus as more of the context model is analyzed, arevision that previously did not receive a good enoughevaluation to terminate processing may now appearmore likely to represent he user's intentions.
Therelaxed acceptability level allows such a revision to beselected as the appropriate interpretation.This processing mechanism is efficient, since only asmall part of the user's expanded plan will usually beanalyzed.
It also avoids the problem of computationalexplosion.
If processing time exceeds a preset maxi-mum or the acceptability level is relaxed to some presetminimum level of goodness, then the system can termi-nate its search for an interpretation and is justified inbelieving that its failure to understand the user's utter-ance is not unnatural behavior.4.3.3 COMPARISON TO OTHER STRATEGIESThis approach is superior to previous trategies becauseit uses a model of the speaker to identify and address hisperceived intentions and needs in making an utterance.As such, it not only reasons on the context model tosuggest possible interpretations relevant o the user'sgoals and plans, but it also limits consideration tothoseinterpretations that are reasonable given the establisheddialog context.5 IMPROVING PLAN RECOGNITIONOur research as shown how an information-seeker'sunderlying task-related plan can be dynamically in-ferred from an ongoing dialog, and how the resultingcontext model can be used to achieve better communi-cation.
However, the kinds of cooperative information-seeking dialogs handled by current models of planrecognition i dicate that four critical assumptions havebeen made:1.
IS has no misconceptions about the task domain.l2.
IS's queries always address aspects of the taskwithin the system's limited knowledge.
These sys-tems maintain the closed world assumption (Reiter1978).3.
IS's statements and queries are correct and notunintentionally misleading.4.
The system's inference mechanisms do not introduceerrors into the context model.These assumptions represent unrealistic onstraints onreal-world dialogs and must be removed.
The firstassumption, called the validplan assumption by Pollack(1987), limits the kinds of beliefs IS can already haveabout he domain--namely, it says that IS's knowledgemay be incomplete but not erroneous.
But IS is inter-acting with the system because IS does not knowenough about the domain to construct his task-relatedplan by himself.
Therefore, since IS is not an expert inthe area, it is to be expected that some of his beliefsabout the domain may be false, contradicting the firstassumption.
An implication of the valid plan assumptionis what Pollack terms the appropriate query assump-tion-namely, that IS knows enough about how to solvehis problem that he always asks relevant questions.The second assumption limits the questions IS canask to those which the system can answer.
But even anexpert system has limited domain knowledge.
Further-more, in a rapidly changing world, knowledgeable usersmay have more accurate information about some as-pects of the domain than does the system.
For example,a student advisement system may not be altered imme-diately upon changing the teacher of a course.
A coop-erative system should recognize its limited knowledgeand reason with it to provide whatever pertinent, help-ful information it can.The third assumption restricts IS to utterances thatare clear, precise, and accurate.
For example, it elimi-nates the possibility that IS might say he is a junior,when in fact he is three credits short of junior standing,thereby leading the system to erroneously infer that ISis eligible for certain programs or awards.
But humaninformation-seekers are often imprecise, especiallywhen they are not aware that small perturbations in thedata can be significant.The fourth assumption says that the system nevermakes an error in inferring IS's plan.
But even in thesimplest cases, the system must hypothesize how indi-vidual utterances relate to one another.
Such decisionsselect from among multiple possibilities and are a po-tential source of error.Pollack (1987) argues against plan inference systemsmaking the first two assumptions, because they prevent34 Computational Linguistics, Volume 14, Number 3, September 1988Sandra Carberry Modeling the User's Plans and Goalsthe system from inferring plans which the user believeshe can pursue but which are novel (to the system) orinvalid.
However, there is another implication of relax-ing the appropriate query assumption that is not consid-ered by Pollack: IS may ask an irrelevant question thatseems perfectly reasonable to the system, thereby lead-ing the system to develop incorrect beliefs about IS'sobjectives.
Consider, for example, a student advise-ment system.
If only B.A.
degrees have a foreignlanguage requirement, the query"What courses must I take to satisfy the foreignlanguage requirement in French?
"may lead the system to infer that IS is pursuing abachelor of arts degree.
If only B.S.
degrees require asenior project, then a subsequent query such as"How many credits of senior project are required?
"is problematic.
Either the second query is inappropriateto IS's goal of obtaining a bachelor of arts degree(Pollack 1986), or the system's context model does notaccurately reflect what IS wants to do.
Note that, ineither case, the user has a misconception; but in thelatter case, the misconception went undetected and wasallowed to introduce rrors into the system's contextmodel.Traditional natural anguage plan inference systemsalso make the third and fourth assumptions, which,together with the first two, guarantee that the underly-ing plan inferred by the system and the task-related planunder construction by IS are never at variance with oneanother.
If we want systems capable of understandingand appropriately responding to naturally occurringdialog, natural language interfaces must be able to dealwith situations where those assumptions are not true.Grosz (1981) claimed that miscommunication canoccur if both dialog participants are not focused on thesame subset of knowledge.
Joshi (1982) contended thatsuccessful communication requires that the mutual be-liefs of the dialog participants be consistent.
Extendingthis to inferred plans, we claim that a successful coop-erative dialog requires that the system's beliefs aboutIS's plan be consistent with what IS is actually consid-ering doing.
But clearly it is unrealistic to expect hatthe system's model will always be correct, given thedifferent knowledge bases of the two participants andthe imperfections of communication via dialog.Thus we need a repair mechanism that attempts todetect inconsistencies in the models and repair themwhenever possible.
This view is supported by the workof Pollack, Hirschberg, and Webber (1982).
They sug-gested that expert-novice dialogs could be viewed as anegotiation process, during which not only an accept-able solution is negotiated, but also understanding of theterminology and the beliefs of the participants.
Thecontext model is one component of the system's beliefs,as is its belief that this model accurately reflects the planunder construction by IS.5.1 RELATED WORKSeveral research efforts have addressed problems re-lated to plan disparity.
Kaplan (1982) and McCoy (1986)investigated misconceptions about domain knowledgeand proposed responses intended to remove the miscon-ceptions.
However, such misconceptions may not beexhibited when they first influence the information-seeker's plan construction; in such cases, disparateplans may result, and correction will entail both aresponse correcting the misconception a d further proc-essing to bring the system's context model and the planunder construction by the information-seeker back intoalignment.Allen's plan inference system (Allen et al 1980)could accommodate some user misconceptions.
It didnot expressly eliminate invalid plans but insteadweighted them less favorably than valid ones.
However,his model did not consider how potential user miscon-ceptions might affect the partial plan inferred by thesystem.Pollack (1986) studied removal of the appropriatequery assumption of previous planning systems.
Sheproposed a richer model of planning that regarded plansas mental phenomena and explicitly reasoned about heinformation-seeker's possible beliefs and intentions.She addressed the problem of queries that indicated theinformation-seeker's plan was inappropriate ohis over-all goal, and attempted to isolate the erroneous beliefsthat led to the inappropriate query.
However, queriesdeemed inappropriate by the system may signal phe-nomena other than that the query is inappropriate towhat the user really wants to do.
For example, theinformation-seeker may have shifted focus to anotheraspect of the overall task without successfully convey-ing this to the system, the system's context model mayhave been in error prior to the query, or, as noted byPollack (1987), the information-seeker may be address-ing aspects of the task outside the system's limitedknowledge.Pollack was concerned with issues that arise whenthe information-seeker's plan is incorrect due to amisconception exhibited by the current query.
Sheassumed that, immediately prior to the user making theproblematic query, the system's partial model of theuser's plan was correct.
We argue that since the sys-tem's inference mechanisms are not infallible and com-munication itself is imperfect, he system must contendwith the possibility that its inferred model does notaccurately reflect he user's plan.
Previous research asfailed to address this problem.Schmidt, Sridharan, and Goodson (1978) proposed ahypothesize-and-revise paradigm for inferring a user'sgoal by observing his non-communicative actions.
Theyformulated a set of revision critics for altering a planupon observing actions that conflict with expectations,but failed to provide any principles or mechanism forselecting the appropriate revision.
Although the modelComputational Linguistics, Volume 14, Number 3, September 1988 35Sandra Carberry Modeling the User's Plans and Goalsof plan recognition for an office environment fornmlatedby Carver, Lesser, and McCue (1984) attempted torepair the inferred plan when actions inconsistent with itwere observed, it did not reason about where ttle planmight be wrong, but merely backtracked to selectanother interpretation.5.2 AN APPROACH TO ROBUST PLAN RECOGNITIONOur analysis of naturally occurring dialog suggests thata plan recognition framework for handling disparateplans should include four phases:1.
Detecting clues to possible disparity between thesystem's context model and the user's actualgoals and plans for accomplishing them.
For ex-ample, expressions of surprise at the system'sresponse and what appear to be major unsignaledshifts in focus of attention should lead the systemto suspect that its context model might be in error.2.
Reasoning on the system's context model and thesystem's domain knowledge to hypothesize thesource of these disparities.3.
Negotiating with the user to isolate the errors.
Thenegotiation phase should be guided by the sys-tem's hypothesis about he source of errors in thecontext model.4.
Appropriately repairing the context model, asindicated by the negotiation dialog.We believe that the knowledge acquired from the dialogand how it was used to construct the context model areimportant factors in hypothesizing the cause of disparitybetween the system's context model and the actual planunder construction by the information-seeker.
Naturallanguage systems must employ various techniques suchas focusing heuristics and default rules for understand-ing and relating dialog in order to do the kind ofinferencing exhibited in dialogue transcripts and pro-vide the most helpful responses.
But confidence inindividual components of the resultant context modelappears to be important in hypothesizing errors.
Wecontend that the system's context model should beenriched, so that its representation f the plan inferredfor the user differentiates among its components ac-cording to the support that the system accords eachcomponent as a correct and intended part of that plan.The system can then reason on this enriched contextmodel to hypothesize the most likely sources of sus-pected disparities.For example, if the system believes that the informa-tion-seeker intends the system to recognize from hisutterance that G is a component of his plan, then thesystem can confidently add G to its context model.Components hat the system adds to the context modelbecause of the system's domain knowledge should beless strongly believed.
This distinction resembles in-tended recognition versus keyhole recognition (Cohenet al 1981).
Intended recognition is the inference ofthose goals and plans that an agent intends to convey.Keyhole recognition is the inference of an agent's goalsand pIans by unobtrusively observing the agent, as ifthrough a keyhole.
Intended recognition is essential incommunicative situations (Cohen et al 1981), since thelistener must identify the intended meaning of a speak-er's utterance.Our analysis of naturally occurring dialog suggestskeyhole recognition is often critical to expand beliefsabout what the information-seeker is trying to do andhow it should be done.
For example, if CS180 is anintroductory course restricted to majors in computerscience and electrical engineering, then the systemmight infer from the utterance"Can you tell me what time CS180 meets?
"not only that the user wants to know the meeting timefor CSI80, but also that the user is a computer scienceor electrical engineering major.
However, the user mayintend the system to recognize the first goal, but it isquestionable whether the user actually intends the sys-tem to recognize that the user is pursuing a major incomputer science or electrical engineering.
This latterinference isbased on the system's beliefs about who cantake CS180--knowledge that the user may not have.Therefore, since the user may not have intended tocommunicate hese components, they are more likelysources of error than components hat the user intendedthe system to recognize.The particular rules used to add a component to thecontext model should affect the system's faith in thatcomponent as part of the information-seeker's overallplan.
For example, since default inference rules andfocusing heuristics elect from among multiple possibil-ities, they add components that are likely sources ofsuspected errors.We believe that if a plan recognition system buildssuch an enriched context model, uses it to hypothesizethe source of suspected errors in the model, and at-tempts to negotiate with the user to isolate and repair itsmodel, the system will be able to handle a much largerset of dialogs than can current models of plan inference,and will be likely to produce responses resembling thosefound in transcripts of naturally occurring information-seeking dialogs.6 CONCLUSIONS AND CURRENT RESEARCHA cooperative natural language system must attempt toinfer the underlying task-related plan motivating theinformation-seeker's queries and use this plan to pro-vide cooperative, helpful responses.
The system'smodel of this plan, which we call a context model, is onecomponent of a user model.
We have presented astrategy for dynamically inferring the context modelfrom an ongoing dialog, and have shown how this modelcan be used to handle one class of problematic utter-ances--the set of utterances that violate the pragmaticrules of the system's world model.
Our strategy, moti-vated by Grice's theory of meaning and maxim of36 Computati~onal Linguistics, Volume 14, Number 3, September 1988Sandra Carberry Modeling the User's Plans and Goalsrelevance, often enables the system to deduce theinformation-seeker's intended meaning, thereby allow-ing the dialog to continue without interruption.However, the assumptions underlying current planinference systems are unrealistic and must be removed.We contend that a natural language system must be ableto detect and recover from discrepancies between thesystem's context model and the actual plan underconstruction by the user, and have suggested thathandling disparate plans requires an enriched contextmodel that differentiates among its components accord-ing to the support it accords each component as acorrect and intended part of the information-seeker'splan.7 ACKNOWLEDGMENTSI would like to thank Kathy Cebulka, Dan Chester, Kathy McCoy,Alan Pope, Lance Ramshaw, Ralph Weischedel, and the participantsof the User Modeling Workshop at Maria Laach, West Germany, forfruitful discussions on various aspects of this research.
I would alsolike to thank the anonymous reviewers for their many constructivecomments.Some of this work was partially supported by a grant from theNational Science Foundation, IST-8311400, and a subcontract fromBolt Beranek and Newman Inc. of a grant from the National ScienceFoundation, 1ST-8419162.REFERENCESAllen, James F. and Perrault, C. Raymond 1980 Analyzing Intentionin Utterances.
Artificial Intelligence 15: 143-178.Carberry, Sandra 1985 A Pragmatics Based Approach to Understand-ing Intersentential Ellipsis.
In Proceedings of the 23rd AnnualMeedng of the Association for Computation Linguistics, Chicago,IL: 188-197.Carberry, Sandra 1986 TRACK: Toward a Robust Natural LanguageInterface.
In Proceedings of the Sixth Canadian Conference onArtificial Intelligence, Montreal, Quebec, Canada: 84--88.Carberry, Sandra 1986 User Models: The Problem of Disparity.
InProceedings of the llth International Conference on Computa-tional Linguistics, Bonn, West Germany: 2%34.Carver, Norman F.; Lesser, Victor R.; and McCue, Daniel L. 1984Focusing in Plan Recognition.
Proceedings of the Fourth NationalConference on Artificial Intelligence, Austin, Texas: 42--48.Chang, C.L.
1978 Finding Missing Joins for Incomplete Queries inRelational Databases.
Technical Report RJ2145, IBM ResearchLaboratory, Yorktown Heights, NY.Cohen, Philip R.; Perrault, C. Raymond; and Allen, James F. 1981Beyond Question Answering.
In W. Lehnert and M. Ringle, (eds.
),Strategies for Natural Language Processing, Lawrence ErlbaumAssociates; 245-275.Fikes, R.E.
and Nilsson, N.J. 1971 STRIPS: A New Approach to theApplication of Theorem Proving to Problem Solving.
ArtificialIntelligence 2: 189-208.Grice, H. Paul.
1975 Meaning.
Philosophical Review 56: 377-388.Grice, H. Paul.
1969 Utterer's Meaning and Intentions.
PhilosophicalReview 68: 147-177.Grice, H. Paul.
1975 Logic and Conversation.
In P. Cole and J.L.Morgan (eds.
), Syntax and Semantics 111: Speech Acts, AcademicPress, New York, NY: 41-58.Grosz, Barbara J.
1977 The Representation and Use of Focus in aSystem for Understanding Dialogs.
In Proceedings of the Interna-tional Joint Conference on Artificial Intelligence, Cambridge,Massachusetts: 67-76.Grosz, Barbara J.
1981 Focusing and Description in Natural LanguageDialogues.
In Webber B. ; Joshi A.; and I.
Sag (eds.
), Elements ofDiscourse Understanding, Cambridge University Press, Cam-bridge, England: 85-105.Joshi, Aravind K. 1982 Mutual Beliefs in Question-Answer Systems.In N. Smith (ed.
), Mutual Beliefs, Academic Press, New York,NY: 181-197.Kaplan, S.J.
1982 Cooperative Responses from a Portable NaturalLanguage Query System.
Artificial Intelligence 19(2): 165-187.Litman, Diane 1986 Linguistic Coherence: A Plan-Based Alternative.In Proceedings of the 24th Annual Meeting of the Association forComputational Linguistics, New York, NY: 215-223.Mays, E. 1980 Failures in Natural Language Systems: Applications toData-Base Query Systems.
In Proceedings of the First NationalConference on Artificial Intelligence, Stanford, CA: 327-330.McCoy, Kathleen F. 1986 The ROMPER System: Responding toObject-Related Misconceptions Using Perspective.
In Proceed-ings of the 24th Annual Meeting of the Association for Computa-tional Linguistics, New York, NY: 9%105.McKeown, Kathleen R. 1985 Text Generation.
Cambridge UniversityPress, Cambridge, England.Perrault, R. and Allen, J.
1980 A Plan-Based Analysis of IndirectSpeech Acts.
American Journal of Computational Linguistics:167-182.Pollack, Martha 1986 A Model of Plan Inference that DistinguishesBetween the Beliefs of Actors and Observers.
In Proceedings ofthe 24th Annual Meeting of the Association for ComputationalLinguistics, New York, NY: 207-214.Pollack, M. 1987 Some Requirements for a Model of the Plan-Inference Process in Conversation.
In Ronan Reilly (ed.
), Com-munication Failure in Dialogue, North Holland: 245-256.Pollack, M.; Hirschberg, J.; and Webber, B.
1982 User Participationin the Reasoning Processes of Expert Systems.
In Proceedings ofthe Second National Conference on Artificial Intelligence, AAAI,Pittsburgh, PA: 358-361.Reiter, R. 1978 On Closed World Data Bases.
In Gallaire H. andMinker J.
(eds.
), Logic and Data Bases, Plenum Press, New York,NY: 55-76.Rich, E. 1979 User Modeling via Stereotypes.
Cognitive Science 3(4):329-354.Robinson, Ann 1981 Determining Verb Phrase Referents in Dialogs.American Journal of Computational Linguistics: 1-18.Robinson, Ann E.; Appelt, Douglas E. ; Grosz, Barbara, J.; Hendrix,Gary G.; and Robinson, Jane J.
1980 Interpreting Natural Lan-guage Utterances in Dialogs about Tasks.
Technical ReportTR210, SRI International, Menlo Park, CA.Schmidt, C.F.
; Sridharan, N.S.
; and Goodson, J.L.
1978 The PlanRecognition Problem: An Intersection of Psychology and ArtificialIntelligence.
Artificial Intelligence 11: 45-82.Sidner, Candace L. 1981 Focusing for Interpretation of Pronouns.American Journal of Computational Linguistics: 217-231.Sidner, Candace L. 1983 What the Speaker Means: The Recognitionof Speakers' Plans in Discourse.
Computers and MathematicsWith Applications 9(1): 71-82.Sidner, Candace L. 1985 Plan Parsing for Intended Response Recog-nition in Discourse.
Computational Intelligence: 1-10.Sowa, J.F.
1976 Conceptual Graphs for a Database Interface.
IBMJournal of Research and Development: 336-357.Thompson, Bozena H. 1980 Linguistic Analysis of Natural LanguageCommunication with Computers.
In Proceedings of the 8th Inter-national Conference on Computational Linguistics: 190-201.Wilensky, Robert 1983 Planning and Understanding, Addison Wes-ley, Reading, MA.Wilkins, D.E.
1984 Domain-Independent Planning: Representationand Plan Generation.
Artificial Intelligence 22: 26%301.NOTEAllen's and Pollack's models of plan inference are exceptions, asdescribed later.Computational Linguistics, Volume 14, Number 3, September 1988 37
