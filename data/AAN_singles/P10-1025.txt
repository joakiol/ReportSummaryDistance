Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 237?246,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsTowards Open-Domain Semantic Role LabelingDanilo Croce, Cristina Giannone, Paolo Annesi, Roberto Basili{croce,giannone,annesi,basili}@info.uniroma2.itDepartment of Computer Science, Systems and ProductionUniversity of Roma, Tor VergataAbstractCurrent Semantic Role Labeling technolo-gies are based on inductive algorithmstrained over large scale repositories ofannotated examples.
Frame-based sys-tems currently make use of the FrameNetdatabase but fail to show suitable general-ization capabilities in out-of-domain sce-narios.
In this paper, a state-of-art systemfor frame-based SRL is extended throughthe encapsulation of a distributional modelof semantic similarity.
The resulting argu-ment classification model promotes a sim-pler feature space that limits the potentialoverfitting effects.
The large scale em-pirical study here discussed confirms thatstate-of-art accuracy can be obtained forout-of-domain evaluations.1 IntroductionThe availability of large scale semantic lexicons,such as FrameNet (Baker et al, 1998), allowed theadoption of a wide family of learning paradigmsin the automation of semantic parsing.
Buildingupon the so called frame semantic model (Fill-more, 1985), the Berkeley FrameNet project hasdeveloped a semantic lexicon for the core vocab-ulary of English, since 1997.
A frame is evokedin texts through the occurrence of its lexical units(LU ), i.e.
predicate words such verbs, nouns, oradjectives, and specifies the participants and prop-erties of the situation it describes, the so calledframe elements (FEs).Semantic Role Labeling (SRL) is the task ofautomatic recognition of individual predicates to-gether with their major roles (e.g.
frame ele-ments) as they are grammatically realized in in-put sentences.
It has been a popular task sincethe availability of the PropBank and FrameNet an-notated corpora (Palmer et al, 2005), the seminalwork of (Gildea and Jurafsky, 2002) and the suc-cessful CoNLL evaluation campaigns (Carrerasand Ma`rquez, 2005).
Statistical machine learningmethods, ranging from joint probabilistic modelsto support vector machines, have been success-fully adopted to provide very accurate semanticlabeling, e.g.
(Carreras and Ma`rquez, 2005).SRL based on FrameNet is thus not a novel task,although very few systems are known capable ofcompleting a general frame-based annotation pro-cess over raw texts, noticeable exceptions beingdiscussed for example in (Erk and Pado, 2006),(Johansson and Nugues, 2008b) and (Coppola etal., 2009).
Some critical limitations have been out-lined in literature, some of them independent fromthe underlying semantic paradigm.Parsing Accuracy.
Most of the employedlearning algorithms are based on complex sets ofsyntagmatic features, as deeply investigated in (Jo-hansson and Nugues, 2008b).
The resulting recog-nition is thus highly dependent on the accuracy ofthe underlying parser, whereas wrong structuresreturned by the parser usually imply large misclas-sification errors.Annotation costs.
Statistical learning ap-proaches applied to SRL are very demanding withrespect to the amount and quality of the train-ing material.
The complex SRL architecturesproposed (usually combining local and global,i.e.
joint, models of argument classification, e.g.
(Toutanova et al, 2008)) require a large numberof annotated examples.
The amount and quality ofthe training data required to reach a significant ac-curacy is a serious limitation to the exploitation ofSRL in many NLP applications.Limited Linguistic Generalization.
Severalstudies showed that even when large trainingsets exist the corresponding learning exhibitspoor generalization power.
Most of the CoNLL2005 systems show a significant performance dropwhen the tested corpus, i.e.
Brown, differs from237the training one (i.e.
Wall Street Journal), e.g.
(Toutanova et al, 2008).
More recently, the state-of-art frame-based semantic role labeling systemdiscussed in (Johansson and Nugues, 2008b) re-ports a 19% drop in accuracy for the argumentclassification task when a different test domain istargeted (i.e.
NTI corpus).
Out-of-domain testsseem to suggest the models trained on BNC do notgeneralize well to novel grammatical and lexicalphenomena.
As also suggested in (Pradhan et al,2008), the major drawback is the poor generaliza-tion power affecting lexical features.
Notice howthis is also a general problem of statistical learningprocesses, as large fine grain feature sets are moreexposed to the risks of overfitting.The above problems are particularly criticalfor frame-based shallow semantic parsing where,as opposed to more syntactic-oriented semanticlabeling schemes (as Propbank (Palmer et al,2005)), a significant mismatch exists between thesemantic descriptors and the underlying syntac-tic annotation level.
In (Johansson and Nugues,2008b) an upper bound of about 83.9% for the ac-curacy of the argument identification task is re-ported, it is due to the complexity in projectingframe element boundaries out from the depen-dency graph: more than 16% of the roles in theannotated material lack of a clear grammatical sta-tus.The limited level of linguistic generalizationoutlined above is still an open research problem.Existing solutions have been proposed in litera-ture along different lines.
Learning from richerlinguistic descriptions of more complex structuresis proposed in (Toutanova et al, 2008).
Limit-ing the cost required for developing large domain-specific training data sets has been also studied,e.g., (Fu?rstenau and Lapata, 2009).
Finally, the ap-plication of semi-supervised learning is attemptedto increase the lexical expressiveness of the model,e.g.
(Goldberg and Elhadad, 2009).In this paper, this last direction is pursued.
Asemi-supervised statistical model exploiting use-ful lexical information from unlabeled corpora isproposed.
The model adopts a simple featurespace by relying on a limited set of grammati-cal properties, thus reducing its learning capac-ity.
Moreover, it generalizes lexical informationabout the annotated examples by applying a ge-ometrical model, in a Latent Semantic Analysisstyle, inspired by a distributional paradigm (Padoand Lapata, 2007).
As we will see, the accu-racy reachable through a restricted feature space isstill quite close to the state-of-art, but interestinglythe performance drops in out-of-domain tests areavoided.In the following, after discussing existing ap-proaches to SRL (Section 2), a distributional ap-proach is defined in Section 3.
Section 3.2 dis-cusses the proposed HMM-based treatment ofjoint inferences in argument classification.
Thelarge scale experiments described in Section 4 willallow to draw the conclusions of Section 5.2 Related WorkState-of-art approaches to frame-based SRL arebased on Support Vector Machines, trained overlinear models of syntactic features, e.g.
(Jo-hansson and Nugues, 2008b), or tree-kernels, e.g.
(Coppola et al, 2009).
SRL proceeds through twomain steps: the localization of arguments in a sen-tence, called boundary detection (BD), and the as-signment of the proper role to the detected con-stituents, that is the argument classification, (AC)step.
In (Toutanova et al, 2008) a SRL modelover Propbank that effectively exploits the seman-tic argument frame as a joint structure, is pre-sented.
It incorporates strong dependencies withina comprehensive statistical joint model with a richset of features over multiple argument phrases.This approach effectively introduces a new stepin SRL, also called Joint Re-ranking, (RR), e.g.
(Toutanova et al, 2008) or (Moschitti et al, 2008).First local models are applied to produce rolelabels over individual arguments, then the jointmodel is used to decide the entire argument se-quence among the set of the n-best competingsolutions.
While these approaches increase theexpressive power of the models to capture moregeneral linguistic properties, they rely on com-plex feature sets, are more demanding about theamount of training information and increase theoverall exposure to overfitting effects.In (Johansson and Nugues, 2008b) the impact ofdifferent grammatical representations on the taskof frame-based shallow semantic parsing is stud-ied and the poor lexical generalization problemis outlined.
An argument classification accuracyof 89.9% over the FrameNet (i.e.
BNC) datasetis shown to decrease to 71.1% when a differenttest domain is evaluated (i.e.
the Nuclear ThreatInitiative corpus).
The argument classification238component is thus shown to be heavily domain-dependent whereas the inclusion of grammaticalfunction features is just able to mitigate this sen-sitivity.
In line with (Pradhan et al, 2008), it issuggested that lexical features are domain specificand their suitable generalization is not achieved.The lack of suitable lexical information is alsodiscussed in (Fu?rstenau and Lapata, 2009) throughan approach aiming to support the creation ofnovel annotated resources.
Accordingly a semi-supervised approach for reducing the costs of themanual annotation effort is proposed.
Through agraph alignment algorithm triggered by annotatedresources, the method acquires training instancesfrom an unlabeled corpus also for verbs not listedas existing FrameNet predicates.2.1 The role of Lexical Semantic InformationIt is widely accepted that lexical information (asfeatures directly derived from word forms) is cru-cial for training accurate systems in a number ofNLP tasks.
Indeed, all the best systems in theCoNLL shared task competitions (e.g.
Chunk-ing (Tjong Kim Sang and Buchholz, 2000)) makeextensive use of lexical information.
Also lexi-cal features are beneficial in SRL usually eitherfor systems on Propbank as well as for FrameNet-based annotation.In (Goldberg and Elhadad, 2009), a differentstrategy to incorporate lexical features into clas-sification models is proposed.
A more expres-sive training algorithm (i.e.
anchored SVM) cou-pled with an aggressive feature pruning strategyis shown to achieve high accuracy over a chunk-ing and named entity recognition task.
The sug-gested perspective here is that effective semanticknowledge can be collected from sources exter-nal to the annotated corpora (very large unanno-tated corpora or on manually constructed lexicalresources) rather than learned from the raw lexi-cal counts of the annotated corpus.
Notice howthis is also the strategy pursued in recent work ondeep learning approaches to NLP tasks.
In (Col-lobert and Weston, 2008) a unified architecturefor Natural Language Processing that learns fea-tures relevant to the tasks at hand given very lim-ited prior knowledge is presented.
It embodies theidea that a multitask learning architecture coupledwith semi-supervised learning can be effectivelyapplied even to complex linguistic tasks such asSRL.
In particular, (Collobert and Weston, 2008)proposes an embedding of lexical information us-ing Wikipedia as source, and exploiting the result-ing language model within the multitask learningprocess.
The idea of (Collobert and Weston, 2008)to obtain an embedding of lexical information byacquiring a language model from unlabeled data isan interesting approach to the problem of perfor-mance degradation in out-of-domain tests, as al-ready pursued by (Deschacht and Moens, 2009).The extensive use of unlabeled texts allows toachieve a significant level of lexical generalizationthat seems better capitalize the smaller annotateddata sets.3 A Distributional Model for ArgumentClassificationHigh quality lexical information is crucial for ro-bust open-domain SRL, as semantic generaliza-tion highly depends on lexical information.
Forexample, the following two sentences evoke theSTATEMENT frame, through the LUs say andstate, where the FEs, SPEAKER and MEDIUM, areshown.
[President Kennedy] SPEAKER said to an astronaut, ?Manis still the most extraordinary computer of all.?
(1)[The report] MEDIUM stated, that some problems neededto be solved.
(2)In sentence (1), for example, President Kennedyis the grammatical subject of the verb say andthis justifies its role of SPEAKER.
However, syn-tax does not entirely characterize argument seman-tics.
In (1) and (2), the same syntactic relation isobserved.
It is the semantics of the grammaticalheads, i.e.
report and Kennedy, the main respon-sible for the difference between the two resultingproto-agentive roles, SPEAKER and MEDIUM.In this work we explore two different aspects.First, we propose a model that does not dependon complex syntactic information in order to min-imize the risk of overfitting.
Second, we improvethe lexical semantic information available to thelearning algorithm.
The proposed ?minimalistic?approach will consider only two independent fea-tures:?
the semantic head (h) of a role, as it canbe observed in the grammatical structure.
Insentence (2), for example, the MEDIUM FE isrealized as the logical subject, whose head isreport.239?
the dependency relation (r) connecting thesemantic head to the predicate words.
In (2),the semantic head report is connected to theLU stated through the subject (SBJ) relation.In the rest of the section the distributional modelfor the argument classification step is presented.A lexicalized model for individual semantic rolesis first defined in order to achieve robust seman-tic classification local to each argument.
Then aHidden Markov Model is introduced in order toexploit the local probability estimators, sensitiveto lexical similarity, as well as the global informa-tion on the entire argument sequence.3.1 Distributional Local ModelsAs the classification of semantic roles is strictlyrelated to the lexical meaning of argument heads,we adopt a distributional perspective, where themeaning is described by the set of textual con-texts in which words appear.
In distributionalmodels, words are thus represented through vec-tors built over these observable contexts: similarvectors suggest semantic relatedness as a func-tion of the distance between two words, capturingparadigmatic (e.g.
synonymy) or syntagmatic re-lations (Pado, 2007).
Vectors?
?h are described byan adjacency matrix M , whose rows describe tar-get words (h) and whose columns describe theircorpus contexts.
Latent Semantic Analysis (LSA)(Landauer and Dumais, 1997), is then applied toM to acquire meaningful representations?
?h .
LSAexploits the linear transformation called SingularValue Decomposition (SVD) and produces an ap-proximation of the original matrix M , capturing(semantic) dependencies between context vectors.M is replaced by a lower dimensional matrix Ml,capturing the same statistical information in a newl-dimensional space, where each dimension is alinear combination of some of the original fea-tures (i.e.
contexts).
These derived features maybe thought as artificial concepts, each one repre-senting an emerging meaning component, as thelinear combination of many different words.In the argument classification task, the similar-ity between two argument heads h1 and h2 ob-served in FrameNet can be computed over?
?h1 and??h2.
The model for a given frame element FEkis built around the semantic heads h observed inthe role FEk: they form a set denoted by HFEk.These LSA vectors?
?h express the individual an-notated examples as they are immerse in the LSARole, FEk Clusters of semantic headsMEDIUMc1: {article, report, statement}c2: {constitution, decree, rule}SPEAKERc3: {brother, father, mother, sister }c4: {biographer, philosopher, ....}c5: {he, she, we, you}c6: {friend}TOPICc7: {privilege, unresponsiveness}c8: {pattern}Table 1: Clusters of semantic heads in the Subjposition for the frame STATEMENT with ?
= 0.5space acquired from the unlabeled texts.
More-over, given FEk, a model for each individual syn-tactic relation r (i.e.
that links h labeled as FEkto their corresponding predicates) is a partition ofthe set HFEkcalled HFEkr , i.e.
the subset ofHFEkproduced by examples of the relation r (e.g.Subj).
Given the annotated sentence (2), we havethat report ?
HMEDIUMSBJ .As the LSA vectors?
?h are available for the se-mantic heads h, a vector representation??
?FEk forthe role FEk can be obtained from the annotateddata.
However, one single vector is a too simplis-tic representation given the rich nature of seman-tic roles FEk.
In order to better represent FEk,multiple regions in the semantic space are used.They are obtained by a clustering process appliedto the set HFEkr according to the Quality Thresh-old (QT) algorithm (Heyer et al, 1999).
QT is ageneralization of k-mean where a variable numberof clusters can be obtained.
This number dependson the minimal value of intra-cluster similarity ac-cepted by the algorithm and controlled by a pa-rameter, ?
: lower values of ?
correspond to moreheterogeneous (i.e.
larger grain) clusters, whilevalues close to 1 characterize stricter policies andmore fine-grained results.
Given a syntactic rela-tion r, CFEkr denotes the clusters derived by QTclustering over HFEkr .
Each cluster c ?
CFEkris represented by a vector ?
?c , computed as thegeometric centroid of its semantic heads h ?
c.For a frame F , clusters define a geometric modelof every frame elements FEk: it consists of cen-troids ?
?c with c ?
HFEkr .
Each c represents FEkthrough a set of similar heads, as role fillers ob-served in FrameNet.
Table 1 represents clustersfor the heads HFEkSubj of the STATEMENT frame.In argument classification we assume that theevoking predicate word for the frame F in aninput sentence s is known.
A sentence s canbe seen as a sequence of role-relation pairs:240s = {(r1, h1), ..., (rn, hn)} where the heads hiare in the syntactic relation ri with the underlyinglexical unit of F .For every head h in s, the vector?
?h can be thenused to estimate its similarity with the differentcandidate roles FEk.
Given the syntactic relationr, the clusters c ?
CFEkr whose centroid vector ~cis closer to ~h are selected.
Dr,h is the set of therepresentations semantically related to h:Dr,h =?k{ckj ?
CFEkr |sim(h, ckj) ?
?}
(3)where the similarity between the j-th cluster forthe FEk, i.e.
ckj ?
CFEkr , and h is the usualcosine similarity: simcos(h, ckj) =?
?h ??
?c kj??
?h ?
??
?c kj?Then, through a k-nearest neighbours (k-NN)strategy withinDr,h, them clusters ckj most simi-lar to h are retained in the set D(m)r,h .
A probabilis-tic preference for the role FEk is estimated for hthrough a cluster-based voting scheme,prob(FEk|r, h) =|CFEkr ?D(m)r,h ||D(m)r,h |(4)or, alternatively, an instance-based one overD(m)r,h :prob(FEk|r, h) =?c?CFEkr ?D(m)r,h|c|?c?D(m)r,h|c|(5)In Fig.
1 the preference estimation for theincoming head h = professor connected toa LU by the Subj relation is shown.
Clus-ters for the heads in Table 1 are also reported.First, in the set of clusters whose similaritywith professor is higher than a threshold ?
them = 5 most similar clusters are selected.
Ac-cordingly, the preferences given by Eq.
4 areprob(SPEAKER|SBJ, h) = 3/5, prob(MEDIUM|SBJ, h) =2/5 and prob(TOPIC|SBJ, h) = 0.
The strategy mod-eled by Eq.
5 amplifies the role of largerclusters, e.g.
prob(SPEAKER|SBJ, h) = 9/14 andprob(MEDIUM|SBJ, h) = 5/14.
We call Distribu-tional, the model that applies Eq.
5 to the source(r, h) arguments, by rejecting cases only when noinformation about the head h is available from theunlabeled corpus or no example of relation r forthe role FEk is available from the annotated cor-pus.
Eq.
4 and 5 in fact do not cover all possiblecases.
Often the incoming head h or the relation rmay be unavailable:1.
If the head h has never been met in the un-labeled corpus or the high grammatical am-biguity of the sentence does not allow tolocate it reliably, Eq.
4 (or 5) should bebacked off to a purely syntactic model, thatis prob(FEk|r)2.
If the relation r can not be properly locatedin s, h is also unknown: the prior probabilityof individual arguments, i.e.
prob(FEk), ishere employed.Both prob(FEk|r) and prob(FEk) can be esti-mated from the training set and smoothing can bealso applied1.
A more robust argument preferencefunction for all arguments (ri, hi) ?
s of the frameF is thus given by:prob(FEk|ri, hi) = ?1prob(FEk|ri, hi) +?2prob(FEk|ri) + ?3prob(FEk) (6)where weights ?1, ?2, ?3 can be heuristically as-signed or estimated from the training set2.
Theresulting model is hereafter called Backoff model:although simply based on a single feature (i.e.
thesyntactic relation r), it accounts for information atdifferent reliability degrees.3.2 A Joint Model for ArgumentClassificationEq.
6 defines roles preferences local to individualarguments (ri, hi).
However, an argument frameis a joint structure, with strong dependencies be-tween arguments.
We thus propose to model thereranking phase (RR) as a HMM sequence label-ing task.
It defines a stochastic inference overmultiple (locally justified) alternative sequencesthrough a Hidden Markov Model (HMM).
It in-fers the best sequence FE(k1,...,kn) over all thepossible hidden state sequences (i.e.
made by thetarget FEki) given the observable emissions, i.e.the arguments (ri, hi).
Viterbi inference is appliedto build the best (role) interpretation for the inputsentence.Once Eq.
6 is available, the best frame elementsequence FE(?(1),...,?
(n)) for the entire sentence scan be selected by defining the function ?(?)
thatmaps arguments (ri, hi) ?
s to frame elementsFEk:?
(i) = k s.t.
FEk ?
F (7)1Lindstone smoothing was applied with ?
= 1.2In each test discussed hereafter, ?1, ?2, ?3 were assignedto .9,.09 and .01, in order to impose a strict priority to themodel contributions.241reportstatementarticlesurveyreviewconstitutiondecreeruletranslatorarchaeologistphilosopherbiographerfriendpatternpresidentkingsistermotherbrotherfatherwesheheyouMEDIUMSPEAKERTOPICtarget headprofessormanifestoprivilegeunresponsivenessFigure 1: A k-NN approach to the role classification for hi = professorNotice that different transfer functions ?(?
)are usually possible.
By computing their prob-ability we can solve the SRL task by select-ing the most likely interpretation, ??(?
), viaargmax?
P(?(?)
| s), as follows:??(?)
= argmax?P(s|?(?))P(?(?
))(8)In Eq.
8, the emission probability P(s|?(?
))andthe transition probability P(?(?
))are explicit.
No-tice that the emission probability corresponds toan argument interpretation (e.g.
Eq.
5) and it isassigned independently from the rest of the sen-tence.
On the other hand, transition probabilitiesmodel role sequences and support the expectationsabout argument frames of a sentence.The emission probability is approximated as:P(s | ?
(1) .
.
.
?
(n))?n?i=1P (ri, hi | FE?
(i))(9)as it is made independent from previous states ina Viterbi path.
Again the emission probability canbe rewritten as:P (ri, hi|FE?
(i)) =P (FE?
(i)|ri, hi) P (ri, hi)P (FE?
(i))(10)Since P (ri, hi) does not depend on the role la-beling, maximizing Eq.
10 corresponds to maxi-mize:P (FE?
(i)|ri, hi)P (FE?
(i))(11)whereas P (FE?
(i)|ri, hi) is thus estimatedthrough Eq.
6.The transition probability, estimated throughP(?
(1) .
.
.
?(n))?n?i=1P(FE?(i)|FE?
(i?1), FE?
(i?2))(12)accounts FEs sequence via a 3-gram model3 .4 Empirical AnalysisThe aim of the evaluation is to measure the reach-able accuracy of the simple model proposed andto compare its impact over in-domain and out-of-domain semantic role labeling tasks.
In particular,we will evaluate the argument classification (AC)task in Section 4.2.Experimental Set-Up.
The in-domain test hasbeen run over the FrameNet annotated corpus, de-rived from the British National Corpus (BNC).The splitting between train and test set is 90%-10% according to the same data set of (Johans-son and Nugues, 2008b).
In all experiments,the FrameNet 1.3 version and the dependency-based system using the LTH parser (Johanssonand Nugues, 2008a) have been employed.
Out-of-domain tests are run over the two training cor-pora as made available by the Semeval 2007 Task194 (Baker et al, 2007): the Nuclear Threat Ini-tiative (NTI) and the American National Corpus3Two empty states are added at the beginning of any se-quence.
Moreover, Laplace smoothing was also applied toeach estimator.4The NTI and ANC annotated collections are download-able at:nlp.cs.swarthmore.edu/semeval/tasks/task19/data/train.tar.gz242Corpus Predicates Argumentstraining FN-BNC 134,697 271,560testin-domain FN-BNC 14,952 30,173out-of-domainNTI 8,208 14,422ANC 760 1,389Table 2: Training and Testing data sets(ANC)5.
Table 2 shows the predicates and argu-ments in each data set.
All null-instantiated ar-guments were removed from the training and testsets.Vectors ~h representing semantic heads havebeen computed according to the ?dependency-based?
vector space discussed in (Pado and La-pata, 2007).
The entire BNC corpus has beenparsed and the dependency graphs derived fromindividual sentences provided the basic observ-able contexts: every co-occurrence is thus syntac-tically justified by a dependency arc.
The mostfrequent 30,000 basic features, i.e.
(syntactic re-lation,lemma) pairs, have been used to build thematrix M , vector components corresponding topoint-wise mutual information scores.
Finally, thefinal space is obtained by applying the SVD reduc-tion overM , with a dimensionality cut of l = 250.In the evaluation of the AC task, accuracy iscomputed over the nodes of the dependency graph,in line with (Johansson and Nugues, 2008b) or(Coppola et al, 2009).
Accordingly, also recall,precision and F-measure are reported on a pernode basis, against the binary BD task or for thefull BD +AC chain.4.1 The Role of Lexical ClusteringThe first study aims at detecting the impact of dif-ferent clustering policies on the resulting AC ac-curacy.
Clustering, as discussed in Section 3.1,allows to generalize lexical information: similarheads within the latent semantic space are builtfrom the annotated examples and they allow topredict the behavior of new unseen words as foundin the test sentences.
The system performanceshave been here measured under different cluster-ing conditions, i.e.
grains at which the clusteringof annotated examples is applied.
This grain is de-termined by the parameter ?
of the applied QualityThreshold algorithm (Heyer et al, 1999).
Noticethat small values of ?
imply large clusters, while if5Sentences whose arguments were not represented in theFrameNet training material were removed from all tests.Frames with a number of annotated examplesEq.
- ?
>0 >100 >500 >1K >3K >5K(5) - .85 86.3 86.5 87.2 88.3 85.9 82.0(4) - .5 85.1 85.5 85.8 87.2 83.5 79.4(4) - .1 84.5 84.8 85.1 86.5 83.0 78.7Table 3: Accuracy on Arg classification tasks wrtdifferent clustering policies?
?
1 then many singleton clusters are promoted(i.e.
one cluster for each example).
By varying thethreshold ?
we thus account for prototype-basedas well exemplar-based strategies, as discussed in(Erk, 2009).We measured the performance on the argumentclassification tasks of different models obtained bycombing different choices of ?
with Eq.
(4) or (5).Results are reported in Table 3.
The leftmost col-umn reports the different clustering settings, whilein the remaining columns we see performancesover test sentences related to different frames: weselected frames for which an increasing number ofannotated examples are available: from all frames(for more than 0 examples) to the only frame (i.e.SELF MOTION) that has more than 5,000 exam-ples in our training data set.The reported accuracies suggest that Eq.
(5),promoting an example driven strategy, better cap-tures the role preference, as it always outperformsalternative settings (i.e.
more prototype orientedmethods).
It limits overgeneralization and pro-motes fine grained clusters.
An interesting result isthat a per-node accuracy of 86.3 (i.e.
only 3 pointsunder the state of-the art on the same data set,(Johansson and Nugues, 2008b)) is achieved.
Allthe remaining tests have been run with the clus-tering configuration characterized by Eq.
(5) and?
= 0.85.4.2 Argument Classification AccuracyIn these experiments we evaluate the quality ofthe argument classification step against the lexi-cal knowledge acquired from unlabeled texts andthe reranking step.
The accuracy reachable on thegold standard argument boundaries has been com-pared across several experimental settings.
Twobaseline systems have been obtained.
The LocalPrior model outputs the sequence that maximizesthe prior probability locally to individual argu-ments.
The Global Prior model is obtained by ap-plying re-ranking (Section 3.2) to the best n = 10candidates provided by the Local Prior model.
Fi-243Model FN-BNC NTI ANCLocal Prior 43.9 50.9 50.4Global Prior 67.7 (+54.2%) 75.9 (+49.0%) 68.8 (+36.4%)Distributional 81.1 (+19.8%) 82.3 (+8.4%) 69.7 (+1.3%)Backoff 84.6 (+4.3%) 87.2 (+6.0%) 76.2 (+9.3%)Backoff+HMMRR 86.3 (+2.0%) 90.5 (+3.8%) 79.9 (+5.0%)(Johansson&Nugues, 2008) 89.9 71.1 -Table 4: Accuracy of the Argument Classification task over the different corpora.
In parenthesis therelative increment with respect to the immediately simpler model, previous rownally, the application of the backoff strategies (asin Eq.
6) and the HMM-based reranking character-ize the final two configurations.
Table 4 reports theaccuracy results obtained over the three corpora(defined as in Table 2): the accuracy scores are av-eraged over different values ofm in Eq.
5, rangingfrom 3 to 30.
In the in-domain scenario, i.e.
theFN-BNC dataset reported in column 2, it is worthnoticing that the proposed model, with backoff andglobal reranking, is quite effective with respect tothe state-of-the-art.Although results on the FN-BNC do not outper-form the state-of-the-art for the FrameNet corpus,we still need to study the generalization capabil-ity of our SRL model in out-of-domain conditions.In a further experiment, we applied the same sys-tem, as trained over the FN-BNC data, to the othercorpora, i.e.
NTI and ANC, used entirely as testsets.
Results, reported in column 3 and 4 of Ta-ble 4 and shown in Figure 2, confirm that no ma-jor drop in performance is observed.
Notice howthe positive impact of the backoff models and theHMM reranking policy is similarly reflected by allthe collections.
Moreover, the results on the NTIcorpus are even better than those obtained on theBNC, with a resulting 90.5% accuracy on the ACtask.86,3%90,5%79,9%40,0%50,0%60,0%70,0%80,0%90,0%100,0%Local        Prior Global     Prior Distributional Backoff Backoff    +HMMRRFN-BNCNTIANCFigure 2: Accuracy of the AC task over differentcorpora4.3 DiscussionThe above empirical findings are relevant if com-pared with the outcome of a similar test on the NTIcollection, discussed in (Johansson and Nugues,2008b)6.
There, under the same training condi-tions, a performance drop of about -19% is re-ported (from 89.9 to 71.1%) over gold standardargument boundaries.
The model proposed in thispaper exhibits no such drop in any collection (NTIand ANC).
This seems to confirm the hypothesisthat the model is able to properly generalize therequired lexical information across different do-mains.It is interesting to outline that the individualstages of the proposed model play different rolesin the different domains, as Table 4 suggests.
Al-though the positive contributions of the individualprocessing stages are uniformly confirmed, somedifferences can be outlined:?
The beneficial impact of the lexical informa-tion (i.e.
the distributional model) applies dif-ferently across the different domains.
TheANC domain seems not to significantly ben-efit when the distributional model (Eq.
5) isapplied.
Notice how Eq.
5 depends both fromthe evidence gathered in the corpus about lex-ical heads h as well as about the relation r. InANC the percentage of times that the Eq.
5 isbacked off against test instances (as h or r arenot available from the training data) is twiceas high as in the BNC-FN or in the NTI do-main (i.e.
15.5 vs. 7.2 or 8.7, respectively).The different syntactic style of ANC seemsthus the main responsible of the poor impactof distributional information, as it is often un-applicable to ANC test cases.?
The complexity of the three test sets is dif-ferent, as the three plots show.
The NTI col-6Notice that in this paper only the training portion of theNTI data set is employed as reported in Table 2 and results arenot directly comparable to (Johansson and Nugues, 2008b).244lections seems characterized by a lower levelof complexity (see for example the accuracyof the Local prior model, that is about 51%as for the ANC).
It then gets benefits fromall the analysis stages, in particular the finalHMM reranking.
The BNC-FN test collec-tion seems the most complex one, and the im-pact of the lexical information brought by thedistributional model is here maximal.
Thisis mainly due to the coherence between thedistributions of lexical and grammatical phe-nomena in the test and training data.?
The role of HMM reranking is an effectiveway to compensate errors in the local argu-ment classifications for all the three domains.However, it is particularly effective for theoutside domain cases, while, in the BNC cor-pus, it produces just a small improvement in-stead (i.e.
+2%, as shown in Table 4 ).
It isworth noticing that the average length of thesentences in the BNC test collection is about23 words per sentence, while it is higher forthe NTI and ANC data sets (i.e.
34 and 31,respectively).
It seems that the HMM modelwell captures some information on the globalsemantic structure of a sentence: this is help-ful in cases where errors in the grammati-cal recognition (of individual arguments orat sentence level) are more frequent and af-flict the local distributional model.
The morecomplex is the syntax of a corpus (e.g.
in theNTI and ANC data sets), the higher seems theimpact of the reranking phase.The significant performance of the AC modelhere presented suggest to test it when integratedwithin a full SRL architecture.
Table 5 reports theresults of the processing cascade over three col-lections.
Results on the Boundary Detection BDtask are obtained by training an SVM model onthe same feature set presented in (Johansson andNugues, 2008b) and are slightly below the state-of-the art BD accuracy reported in (Coppola etal., 2009).
However, the accuracy of the completeBD + AC + RR chain (i.e.
68%) improves thecorresponding results of (Coppola et al, 2009).Given the relatively simple feature set adoptedhere, this result is very significant as for its result-ing efficiency.
The overall BD recognition pro-cess is, on a standard architecture, performed atabout 6.74 sentences per second, that is basicallyCorpus Eval.
Setting Recall Precision F1BNCBD 72.6 85.1 78.4BD+AC+RR 62.6 74.5 68.0NTIBD 63.9 80.0 71.0BD+AC+RR 56.7 72.1 63.5ANCBD 64.0 81.5 71.7BD+AC+RR 47.4 62.5 53.9Table 5: Accuracy of the full cascade of the SRLsystem over three domainthe same as the time needed for applying the en-tire BD+AC +RR chain, i.e.
6.21 sentence persecond.5 ConclusionsIn this paper, a distributional approach for acquir-ing a semi-supervised model of argument classi-fication (AC) preferences has been proposed.
Itaims at improving the generalization capability ofthe inductive SRL approach by reducing the com-plexity of the employed grammatical features andthrough a distributional representation of lexicalfeatures.
The obtained results are close to thestate-of-art in FrameNet semantic parsing.
Stateof the art accuracy is obtained instead in out-of-domain experiments.
The model seems to cap-italize from simple methods of lexical modeling(i.e.
the estimation of lexico-grammatical pref-erences through distributional analysis over unla-beled data), estimation (through syntactic or lexi-cal back-off where necessary) and reranking.
Theresult is an accurate and highly portable SRL cas-cade.
Experiments on the integrated SRL archi-tecture (i.e.
BD + AC + RR chain) show thatstate-of-art accuracy (i.e.
68%) can be obtainedon raw texts.
This result is also very significantas for the achieved efficiency.
The system is ableto apply the entire BD + AC + RR chain at aspeed of 6.21 sentences per second.
This signif-icant efficiency confirms the applicability of theSRL approach proposed here in large scale NLPapplications.
Future work will study the appli-cation of the flexible SRL method proposed toother languages, for which less resources are avail-able and worst training conditions are the norm.Moreover, dimensionality reduction methods al-ternative to LSA, as currently studied on semi-supervised spectral learning (Johnson and Zhang,2008), will be experimented.245ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proc.
ofCOLING-ACL, Montreal, Canada.Collin Baker, Michael Ellsworth, and Katrin Erk.2007.
Semeval-2007 task 19: Frame semantic struc-ture extraction.
In Proceedings of SemEval-2007,pages 99?104, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Intro-duction to the CoNLL-2005 Shared Task: Seman-tic Role Labeling.
In Proc.
of CoNLL-2005, pages152?164, Ann Arbor, Michigan, June.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: deepneural networks with multitask learning.
In In Pro-ceedings of ICML ?08, pages 160?167, New York,NY, USA.
ACM.Bonaventura Coppola, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Shallow semantic parsingfor spoken language understanding.
In Proceedingsof NAACL ?09, pages 85?88, Morristown, NJ, USA.Koen Deschacht and Marie-Francine Moens.
2009.Semi-supervised semantic role labeling using the la-tent words language model.
In EMNLP ?09: Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing, pages 21?29,Morristown, NJ, USA.
Association for Computa-tional Linguistics.Katrin Erk and Sebastian Pado.
2006.
Shalmaneser -a flexible toolbox for semantic role assignment.
InProceedings of LREC 2006, Genoa, Italy.Katrin Erk.
2009.
Representing words as regionsin vector space.
In In Proceedings of CoNLL ?09,pages 57?65, Morristown, NJ, USA.
Association forComputational Linguistics.Charles J. Fillmore.
1985.
Frames and the semantics ofunderstanding.
Quaderni di Semantica, 4(2):222?254.Hagen Fu?rstenau and Mirella Lapata.
2009.
Graphalignment for semi-supervised semantic role label-ing.
In In Proceedings of EMNLP ?09, pages 11?20,Morristown, NJ, USA.Daniel Gildea and Daniel Jurafsky.
2002.
AutomaticLabeling of Semantic Roles.
Computational Lin-guistics, 28(3):245?288.Yoav Goldberg and Michael Elhadad.
2009.
On therole of lexical features in sequence labeling.
In InProceedings of EMNLP ?09, pages 1142?1151, Sin-gapore, August.
Association for Computational Lin-guistics.L.J.
Heyer, S. Kruglyak, and S. Yooseph.
1999.
Ex-ploring expression data: Identification and analysisof coexpressed genes.
Genome Research, (9):1106?1115.Richard Johansson and Pierre Nugues.
2008a.Dependency-based syntactic-semantic analysis withpropbank and nombank.
In Proceedings of CoNLL-2008, Manchester, UK, August 16-17.Richard Johansson and Pierre Nugues.
2008b.
Theeffect of syntactic representation on semantic rolelabeling.
In Proceedings of COLING, Manchester,UK, August 18-22.Rie Johnson and Tong Zhang.
2008.
Graph-basedsemi-supervised learning and spectral kernel de-sign.
IEEE Transactions on Information Theory,54(1):275?288.Tom Landauer and Sue Dumais.
1997.
A solution toplato?s problem: The latent semantic analysis the-ory of acquisition, induction and representation ofknowledge.
Psychological Review, 104.A.
Moschitti, D. Pighin, and R. Basili.
2008.
Treekernels for semantic role labeling.
ComputationalLinguistics, 34.Sebastian Pado and Mirella Lapata.
2007.Dependency-based construction of semanticspace models.
Computational Linguistics, 33(2).Sebastian Pado.
2007.
Cross-Lingual AnnotationProjection Models for Role-Semantic Information.Ph.D.
thesis, Saarland University.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: A corpus annotated withsemantic roles.
Computational Linguistics, 31(1),March.Sameer S. Pradhan, Wayne Ward, and James H. Mar-tin.
2008.
Towards robust semantic role labeling.Comput.
Linguist., 34(2):289?310.Erik F. Tjong Kim Sang and Sabine Buchholz.
2000.Introduction to the conll-2000 shared task: chunk-ing.
In Proceedings of the 2nd workshop on Learn-ing language in logic and the 4th conference onComputational natural language learning, pages127?132, Morristown, NJ, USA.
Association forComputational Linguistics.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2008.
A global joint model for semanticrole labeling.
Comput.
Linguist., 34(2):161?191.246
