Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 337?340,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsThe Simple Truth about Dependency and Phrase Structure RepresentationsAn Opinion PieceOwen RambowCCLS, Columbia UniversityNew York, NY, USArambow@ccls.columbia.eduAbstractThere are many misconceptions about de-pendency representations and phrase structurerepresentations for syntax.
They are partly dueto terminological confusion, partly due to alack of meta-scientific clarity about the rolesof representations and linguistic theories.
Thisopinion piece argues for a simple but clearview of syntactic representation.1 IntroductionTo the machine learning community, treebanks arejust collections of data, like pixels with captions,structural and behavioral facts about genes, or ob-servations about wild boar populations.
In contrast,to us computational linguists, treebanks are not nat-urally occurring data at all: they are the result ofa very complex annotation process.
While the textthat is annotated (usually) is naturally occurring, theannotation itself is already the result of a scientificactivity.
This opinion piece argues that the level ofdiscourse about treebanks often found in our com-munity does not reflect this fact (presumably dueto the influence of the brute machine learning per-spective).
We, as a community of computational lin-guists, need to be very precise when talking abouttreebanks and syntactic representations in general.So let?s start with three very important conceptswhich we must always distinguish.
The representa-tion type: what type of mathematical object is usedto represent syntactic facts?
In this opinion piece,I only consider dependency trees (DTs) and phrasestructure trees (PSTs) (Section 2).
The representedsyntactic content: the morphological and syntacticfacts of the analyzed sentence (Section 3).
The syn-tactic theory: it explains how syntactic content isrepresented in the chosen representation type (Sec-tion 4).A crucial confusing factor is the fact that the termsdependency and phrase structure both have both amathematical and a linguistic meaning.
The math-ematical meaning refers representation types.
Thelinguistic meaning refers to syntactic content.
I dis-cuss this issue in Section 3.
I discuss the issue ofconverting between DTs and PSTs in Section 5, asan example of how my proposed conceptualizationof syntactic representation throws light on a compu-tational problem.This opinion piece will be a success if after read-ing it, the reader concludes that actually he or sheknew this all along.
In fact, this opinion piece doesnot advocate for a controversial position; its missionis to make its readers be more precise when talkingabout syntactic representations.
This opinion pieceis intentionally polemical for rhetorical reasons.2 DTs and PSTs as Representation TypesAssume we have two disjoint symbol sets: a set ofterminal symbols which contains the words of thelanguage we are describing; and a set of nontermi-nal symbols.
A Dependency Tree (DT) is a treein which all nodes are labeled with words (elementsof the set of terminal symbols) or empty strings.
APhrase Structure Tree (PST) is a tree in which alland only the leaf nodes are labeled with words orempty strings, and internal nodes are labeled withnonterminal symbols.
There is nothing more to the337definitions.
Trees of both types can have many otherproperties which are not part of the two definitions,and which do not follow from the definitions.
I men-tion some such properties.Unordered trees.
DTs and PSTs can be ordered orunordered.
For example, the Prague Theory (Sgallet al, 1986) uses unordered DTs at the deeper levelof representation and ordered DTs at a more surfacylevel.
GPSG (Gazdar et al, 1985) uses unorderedtrees (or at any rate context-free rules whose right-hand side is ordered by a separate component of thegrammar), as does current Chomskyan theory (thePST at spell-out may be unordered).Empty categories.
Empty categories can be emptypronouns, or traces, which are co-indexed with aword elsewhere in the tree.
Empty pronouns arewidely used in both DT- and PST-based represen-tations.
While most DT-based approaches do notuse traces, Lombardo and Lesmo (1998) do; andwhile traces are commonly found in PST-based ap-proaches, there are many that do not use them, suchas the c-structure of LFG.Discontinuous Constituents or Non-Projectivity.Both types of trees can be used with or without dis-continuous constituents; PSTs are more likely to usetraces to avoid discontinuous constituents, but lin-guistic proposals for PSTs with discontinuous con-stituents have been made (work by McCawley, or(Becker et al, 1991)).Labeled Arcs.
In DTs, arcs often have labels; arcsin PSTs usually do not, but we can of course labelPST arcs as well, as is done in the German TIGERcorpus.I note that in both DTs and PSTs we can rep-resent the arc label as a feature on the daughter node,or as a separate node.3 Syntactic ContentWhile there is lots of disagreement about the properrepresentation type for syntax, there is actually abroad consensus among theoretical and descriptivesyntacticians of all persuasions about the range ofsyntactic phenomena that exist.
What exactly is thiscontent, then?
It is not a theory-neutral representa-tion of syntax (Section 4).
Rather, it is the empiricalmatter which linguistic theory attempts to representor explain.
We cannot represent it without a theory,but we can refer to it without a theory, using namessuch as control constructions or transitive verb.
Inthe same manner, we use the word light and physi-cists will agree on what the phenomenon is, but wecannot represent light within a theory without choos-ing a representation as either particles or wave.Note that in linguistics, the terms dependency andphrase structure refer to syntactic content, i.e., syn-tactic facts we can represent.
Syntactic depen-dency is direct relation between words.
Usually,this relation is labeled (or typed), and is identicalto (or subsumes) the notion of grammatical func-tion, which covers relations such as SUBJECT, OB-JECT, TEMPORAL-ADJUNCT and so forth.
Syn-tactic phrase structure, also known as syntacticconstituency structure is recursive representationusing sets of one or more linguistic units (wordsand empty strings), such that at each level, eachset (constituent) acts as a unit syntactically.
Lin-guistic phrase structure is most conveniently ex-pressed in a phrase structure tree, while linguis-tic dependency is most conveniently expressed ina dependency tree.
However, we can express thesame content in either type of tree!
For exam-ple, the English Penn Treebank (PTB) encodes thepredicate-argument structure of English using struc-tural conventions and special nonterminal labels(?dashtags?
), such as NP-SBJ.
And a dependencytree represents constituency: each node can be in-terpreted both as a preterminal node (X0) and as anode heading a constituent containing all terminalsincluded in the subtree it heads (the XP).
Of course,what is more complex to encode in a DT are inter-mediate projections, such as VP.
I leave a fuller dis-cussion aside for lack of space, but I claim that thesyntactic content which is expressed in intermediateprojections can also be expressed in a DT, throughthe use of features and arc labels.4 Syntactic TheoryThe choice of representation type does not deter-mine the representation for a given sentence.
Thisis obvious, but it needs to be repeated; I have heard?What is the DT for this sentence??
one too manytimes.
There are many possible DTs and PSTs, pro-posed by serious syntacticians, for even simple sen-338tences, even when the syntacticians agree on whatthe syntactic content (a transitive verb with SVO or-der, for example) of the analysis should be!
What isgoing on?In order to make sense of this, we need a third playerin addition to the representation type and the con-tent.
This is the syntactic theory.
A linguistic the-ory chooses a representation type and then definesa coherent mapping for a well-defined set of con-tent to the chosen representation type.
Here, ?coher-ent representation?
means that the different choicesmade for conceptually independent content are alsorepresentationally independent, so that we can com-pose representational choices.
Note that a theorycan decide to omit some content; for example, wecan have a theory which does not distinguish raisingfrom control (the English PTB does not).There are different types of syntactic theories.
Adescriptive theory is an account of the syntax ofone language.
Examples of descriptive grammarsinclude works such as Quirk for English, or the an-notation manuals of monolingual treebanks, suchas (Marcus et al, 1994; Maamouri et al, 2003).The annotation manual serves two purposes: it tellsthe annotators how to represent a syntactic phe-nomenon, and it tells the users of the treebank (us!
)how to interpret the annotation.
A treebank withoutmanual is meaningless.
And an arborescent struc-ture does not mean the same thing in all treebanks(for example, a ?flat NP?
indicates an unannotatedconstituent in the English ATB but a fully annotatedconstruction in the Arabic Treebank is).An explanatory theory is a theory which attemptsto account for the syntax of all languages, for exam-ple by reducing their diversity to a set of principlesand finite-valued parameters.
Linguistic theories(and explanatory theories in particular) often takethe form of a one-to-many mapping from a simplerepresentation of syntactic dependency (predicate-argument structure) to a structural representationthat determines surface word order.
The linguistictheory itself is formulated as a (computational) de-vice that relates the deeper level to the more surfacylevel.
LFG has a very pure expression of this ap-proach, with the deeper level expressed using a DT(actually, dependency directed acyclic graphs, butthe distinction is not relevant here), and the surfacylevel expressed using a PST.
But the Chomskyan ap-proaches fit the same paradigm, as do many othertheories of syntax.Therefore, there is no theory-neutral representationof a sentence or a set of sentences, because everyrepresentation needs a theory for us to extract itsmeaning!
Often what is meant by ?theory-neutraltree?
is a tree which is interpreted using some no-tion of consensus theory, perhaps a stripped-downrepresentation which omits much content for whichthere is no consensus on how to represent it.5 Converting Between DTs and PSTsConverting a set of DS annotations to PS or viceversa means that we want to obtain a representa-tion which expresses exactly the same content.
Thisis frequently done these days as interest in depen-dency parsing grows but many languages only havePS treebanks.
However, this process is often not un-derstood.To start, I observe that uninterpreted structures (i.e.,structures without a syntactic theory, or trees froma treebank without a manual) cannot be convertedfrom or into, as we do not know what they meanand we cannot know if we are preserving the samecontent or not.Now, my central claim about the possibility of au-tomatically converting between PSTs and DTs is thefollowing.
If we have an interpretation for the sourcerepresentation and the goal representation (as wemust in order for this task to be meaningful), thenwe can convert any facts that are represented in thesource structure, and we cannot convert any factsthat are not represented in the source structure.
Itis that simple.
If we are converting from a sourcewhich contains less information than the target, thenwe cannot succeed.
For example, if we are convert-ing from a PS treebank that does not distinguish par-ticles from prepositions to a DS treebank that does,then we will fail.
General claims about the possi-bility of conversion (?it is easier to convert PS toDS than DS to PS?)
are therefore meaningless.
Itonly matters what is represented, not how it is rep-resented.There is, however, no guarantee that there is a sim-ple algorithm for conversion, such as a parametrized339head percolation algorithm passed down from re-searcher to researcher like a sorcerer?s incantation.In general, if the two representations are indepen-dently devised and both are linguistically motivated,then we have no reason to believe that the conversioncan be done using a specific simple approach, or us-ing conversion rules which have some fixed property(say, the depth of the trees in the rules templates).
Inthe general case, the only way to write an automaticconverter between two representations is to study thetwo annotation manuals and to create a case-by-caseconverter, covering all linguistic phenomena repre-sented in the target representation.Machine learning-based conversion (for example,(Xia and Palmer, 2001)) is an interesting exercise,but it does not give us any general insights into de-pendency or phrase structure.
Suppose the sourcecontains all the information that the target shouldcontain.
Then if machine learning-based conversionfails or does not perform completely correctly, theexercise merely shows that the machine learning isnot adequate.
Now suppose that the source doesnot contain all the information that the target shouldcontain.
Then no fancy machine learning can everprovide a completely correct conversion.
Also, notethat unlike, for example, parsers which are basedon machine learning and which learn about a natu-ral phenomenon (language use), machine learning ofconversion merely learns an artificial phenomenon:the relation between the two syntactic theories inquestion, which are created by researchers.
(Ofcourse, in practice, machine learning of automaticconversion between DT to PSTs is useful.
)6 ConclusionI have argued that when talking about dependencyand phrase structure representations, one should al-ways distinguish the type of representation (depen-dency or phrase structure) from the content of therepresentation, and one needs to understand (andmake explicit if it is implicit) the linguistic the-ory that relates content to representation.
Machinelearning researchers have the luxury of treating syn-tactic representations as mere fodder for their mills;we as computational linguists do not, since this isour area of expertise.AcknowledgmentsI would like to thank my colleagues on the Hindi-Urdu treebank project (Bhatt et al, 2009) (NSFgrant CNS-0751089) for spirited discussions aboutthe issues discussed here.
I would like to thank Syl-vain Kahane, Yoav Goldberg, and Joakim Nivre forcomments that have helped me improve this paper.The expressed opinions have been influenced by fartoo many people to thank individually here.ReferencesTilman Becker, Aravind Joshi, and Owen Rambow.1991.
Long distance scrambling and tree adjoining gram-mars.
In Fifth Conference of the European Chapter of theAssociation for Computational Linguistics (EACL?91),pages 21?26.
ACL.Rajesh Bhatt, Bhuvana Narasimhan, Martha Palmer,Owen Rambow, Dipti Sharma, and Fei Xia.
2009.A multi-representational and multi-layered treebank forhindi/urdu.
In Proceedings of the Third Linguistic Anno-tation Workshop, pages 186?189, Suntec, Singapore.Gerald Gazdar, Ewan Klein, Geoffrey Pullum, and IvanSag.
1985.
Generalized Phrase Structure Grammar.Harvard University Press, Cambridge, Mass.Vincenzo Lombardo and Leonardo Lesmo.
1998.
For-mal aspects and parsing issue of dependency theory.
In36th Meeting of the Association for Computational Lin-guistics and 17th International Conference on Compu-tational Linguistics (COLING-ACL?98), pages 787?793,Montre?al, Canada.Mohamed Maamouri, Ann Bies, Hubert Jin, and TimBuckwalter.
2003.
Arabic treebank: Part 1 v 2.0.
Dis-tributed by the Linguistic Data Consortium.
LDC Cata-log No.
: LDC2003T06.Mohamed Maamouri, Ann Bies, and Tim Buckwalter.2004.
The Penn Arabic Treebank: Building a large-scale annotated arabic corpus.
In NEMLAR Conferenceon Arabic Language Resources and Tools, Cairo, Egypt.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,A.
Bies, M. Ferguson, K. Katz, and B. Schasberger.1994.
The Penn Treebank: Annotating predicate argu-ment structure.
In Proceedings of the ARPA Human Lan-guage Technology Workshop.Igor A. Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press, New York.P.
Sgall, E.
Hajic?ova?, and J. Panevova?.
1986.
The mean-ing of the sentence and its semantic and pragmatic as-pects.
Reidel, Dordrecht.Fei Xia and Martha Palmer.
2001.
Converting depen-dency structure to phrase structures.
In hlt2001, pages61?65.340
