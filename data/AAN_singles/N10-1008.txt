Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 64?72,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsDialogue-Oriented Review Summary Generation for Spoken DialogueRecommendation SystemsJingjing Liu, Stephanie Seneff, Victor ZueMIT Computer Science & Artificial Intelligence Laboratory32 Vassar Street, Cambridge, MA 02139{jingl, seneff, zue}@csail.mit.eduAbstractIn this paper we present an opinion summari-zation technique in spoken dialogue systems.Opinion mining has been well studied foryears, but very few have considered its appli-cation in spoken dialogue systems.
Reviewsummarization, when applied to real dialoguesystems, is much more complicated than puretext-based summarization.
We conduct a sys-tematic study on dialogue-system-orientedreview analysis and propose a three-levelframework for a recommendation dialoguesystem.
In previous work we have explored alinguistic parsing approach to phrase extrac-tion from reviews.
In this paper we will de-scribe an approach using statistical modelssuch as decision trees and SVMs to select themost representative phrases from the ex-tracted phrase set.
We will also explain howto generate informative yet concise reviewsummaries for dialogue purposes.
Experimen-tal results in the restaurant domain show thatthe proposed approach using decision tree al-gorithms achieves an outperformance of 13%compared to SVM models and an improve-ment of 36% over a heuristic rule baseline.Experiments also show that the decision-tree-based phrase selection model can achieve ra-ther reliable predictions on the phrase label,comparable to human judgment.
The pro-posed statistical approach is based on do-main-independent learning features and canbe extended to other domains effectively.1 IntroductionSpoken dialogue systems are presently availablefor many purposes, such as weather inquiry (Zueet al, 2000), bus schedules and route guidance(Raux et al, 2003), customer service (Gorin et al,1997), and train timetable inquiry (Eckert et al,1993).
These systems have been well developedfor laboratory research, and some have becomecommercially viable.The next generation of intelligent dialogue sys-tems is expected to go beyond factoid questionanswering and straightforward task fulfillment, byproviding active assistance and subjective recom-mendations, thus behaving more like humanagents.
For example, an intelligent dialogue sys-tem may suggest which airline is a better choice,considering cost, flight duration, take-off time,available seats, etc.
; or suggest which digital cam-era is the most popular among teenagers or highestrated by professional photographers; or which res-taurant is a perfect spot for a semi-formal businessmeeting or a romantic date.Luckily, there are enormous amounts of reviewspublished by general users on the web every day.These are perfect resources for providing subjec-tive recommendations and collective opinions.
Ifthere exists a systematic framework that harveststhese reviews from general users, extracts the es-sence from the reviews and presents it appropriate-ly in human-computer conversations, then we canenable dialogue systems to behave like a humanshopping assistant, a travel agent, or a local friendwho tells you where to find the best restaurant.Summarization from online reviews, therefore,plays an important role for such dialogue systems.There have been previous studies on review analy-sis for text-based summarization systems (Mei etal., 2007; Titov and McDonald, 2008a; Branavanet al, 2008).
Mixture models and topic models areused to predict the underlying topics of each doc-ument and generate a phrase-level summary.
Anaspect rating on each facet is also automatically64learned with statistical models (Snyder and Barzi-lay, 2007; Titov and McDonald, 2008b; Baccia-nella et al, 2009).
These approaches are all veryeffective, and the review databases generated arewell presented.So the first thought for developing a recom-mendation dialogue system is to use such a cate-gorized summary in a table-lookup fashion.
Forexample, a dialogue system for restaurant recom-mendations can look up a summary table as exem-plified in Table 1, and generate a responseutterance from each row: ?Restaurant A has goodservice and bad food; restaurant B has good ser-vice and good food; restaurant C has great serviceand nice atmosphere; restaurant D has poor serviceand reasonable price.
?Restaurant SummaryA Good service, bad food,B Good service, good foodC Great service, nice atmosphereD Poor service, reasonable priceTable 1.
A partial table of categorization-based reviewsummaries.Such a dialogue system is, however, not veryinformative.
First of all, there is too much redun-dancy.
Long utterances repeated in the same pat-tern on the same topic are quite boring, and theinformation density is very low.
Second, such asummary is too coarse-grained to be helpful.
Auser querying a restaurant recommendation systemexpects more fine-grained information such ashouse specials, wine selections and choices ondesserts rather than just general ?good food.
?In contrast to a ?text?
summarization system, thetextual space in a dialogue turn is often very li-mited.
Speech is inherently serial, and it cannot beskipped and scanned easily.
A dialogue systemwhich speaks long diatribes in each single conver-sation turn would likely not be well received.
Webelieve that the best unit of review summary fordialogue purposes is short succinct phrases (e.g.,?amazing sauce?
and ?romantic ambiance?)
whichare catalogued into categories to answer high-levelquestions such as ?How is the food at this restau-rant??
or ?What about the atmosphere??
Also,domain-specific phrases are greatly needed togenerate correct answers to specific queries, suchas ?Which restaurants have fresh sushi??
and?Show me restaurants that serve fantastic cock-tails.
?Thus, the generally used correlated topic mod-els, although very effective in text-based systems,are not quite suitable for interactive dialogue sys-tems.
The missing piece is an HCI-oriented (hu-man computer interaction), fine-grained,informative yet concise review summarization.
Agood recommendation system should be able toprovide sufficient yet specific information to helpusers choose a restaurant, a movie or a consumerproduct.
Figure 1 shows an example of a conversa-tion with a good recommendation system, which 1)provides a generalized yet high-density reviewsummary in human-friendly dialogue; 2) providesboth a coarse-grained summary (e.g., ?authenticfood?)
and fine-grained information such as housespecialties.U: Are there any French restaurants in Cambridge?S: There are five French restaurants in Cambridge.Most of them are on Backbay Street.
And most ofthem have attentive service.U: Show me the ones that have nice ambiance.S: Two of them have nice ambiance, Pierrot BistrotFrancais and Coriander.U: How about the food?S: Pierrot Bistrot Francais has authentic food,fresh salad, and absolutely delicious spaghetticarbonara.
The food at Coriander is below aver-age.
It has very greasy chicken tikka masala anddry bread.Figure 1.
Example of a conversation with a good rec-ommendation dialogue system (?U?
denotes the userand ?S?
denotes the dialogue system.
The responses tothe user queries are produced by our system and theactual dialogue was an illustration of system capacities).Therefore, the task of developing recommenda-tion dialogue systems is decomposed into threeproblems: 1) how to extract context-related phras-es, both coarse-grained and fine-grained, fromonline reviews; 2) how to select a representativeset from the extracted phrases to create an infor-mative yet concise dialogue-oriented summarydatabase; 3) how to generate human-friendly di-alogue responses from the review summary data-base.To tackle these problems, we propose a three-level framework.
In previous work (Liu and Seneff,2009), we explored the first level by proposing alinguistic parse-and-paraphrase paradigm for re-65view phrase extraction.
In this paper, we addressthe second problem: dialogue-oriented reviewsummary generation.
We propose an automaticapproach to classifying high/low informativephrases using statistical models.
Experiments con-ducted on a restaurant-domain dataset indicate thatthe proposed approach can predict phrase labelsconsistently with human judgment and can gener-ate high-quality review summaries for dialoguepurposes.The rest of the paper is organized as follows:Section 2 gives an overview of the three-levelframework for recommendation dialogue systems.In Section 3, we explain the proposed approach todialogue-oriented review summary generation.Section 4 provides a systematic evaluation of theproposed approach, and Section 5 gives a furtherdiscussion on the experimental results.
Section 6summarizes the paper as well as pointing to futurework.2 System OverviewThe three-level framework of a review-summary-based recommendation dialogue system is shownin Figure 2.
The bottom level is linguistic phraseextraction.
In previous work (Liu and Seneff,2009), we employed a probabilistic lexicalizedgrammar to parse review sentences into a hierar-chical representation, which we call a linguisticframe.
From the linguistic frames, phrases are ex-tracted by capturing a set of adjective-noun rela-tionships.
Adverbs and negations conjoined withthe adjectives are also captured.
We also calcu-lated a numerical score for sentiment strength foreach adjective and adverb, and further applied acumulative offset model to assign a sentimentscore to each phrase.The approach relies on linguistic features thatare independent of frequency statistics; therefore itcan retrieve very rare phrases such as ?very greasychicken tikka masala?
and ?absolutely deliciousspaghetti carbonara?, which are very hard to derivefrom correlated topic models.
Experimental resultsshowed that the linguistic paradigm outperformsexisting methods of phrase extraction which em-ploy shallow parsing features (e.g., part-of-speech).The main contribution came from the linguisticframe, which preserves linguistic structure of asentence by encoding different layers of semanticdependencies.
This allows us to employ more so-phisticated high-level linguistic features (e.g., longdistance semantic dependencies) for phrase extrac-tion.However, the linguistic approach fails to distin-guish highly informative and relevant phrasesfrom uninformative ones (e.g., ?drunken husband?,?whole staff?).
To apply these extracted phraseswithin a recommendation dialogue system, wehave to filter out low quality or irrelevant phrasesand maintain a concise summary database.
This isthe second level: dialogue-oriented review sum-mary generation.Figure 2.
Three-level framework of review-based rec-ommendation dialogue systems.The standard of highly informative and relevantphrases is a very subjective problem.
To gain in-sights on human judgment on this, the first twoauthors separately labeled a set of review-relatedphrases in a restaurant domain as ?good?
and ?bad?summary phrases.
We surveyed several subjects,all of whom indicated that, when querying a dialo-gue system for information about a restaurant,they care much more about special dishes servedin this restaurant than generic descriptions such as?good food.?
This knowledge informed the annota-tion task: to judge whether a phrase delivered by adialogue recommendation system would be help-66ful for users to make a decision.
Surprisingly, al-though this is a difficult and subjective problem,the judgment from the two annotators is substan-tially consistent.
By examining the annotations weobserved that phrases such as ?great value?
and?good quality?
are often treated as ?uninformative?as they are too common to be representative for aparticular product, a restaurant or a movie.
Phraseswith neutral sentiment (e.g., ?green beans?
and?whole staff?)
are often considered as uninforma-tive too.
Phrases on specific topics such as housespecialties (e.g., ?absolutely delicious spaghetticarbonara?)
are what the annotators care aboutmost and are often considered as highly relevant,even though they may have only been seen once ina large database.Driven by these criteria, from each phrase weextract a set of statistical features such as uni-gram/bigram probabilities and sentiment featuressuch as sentiment orientation degree of the phrase,as well as underlying semantic features (e.g.,whether the topic of the phrase fits in a domain-specific ontology).
Classification models such asSVMs and decision tree algorithms are thentrained on these features to automatically classifyhigh/low informative phrases.
Phrases identifiedas ?good?
candidates are further pruned and cata-logued to create concise summaries for dialoguepurposes.After generating the review summary database,the third level is to modify the response generationcomponent in dialogue systems to create genera-lized and interactive conversations, as exemplifiedin Figure 1.
The utterance from users is pipedthrough speech recognition and language under-standing.
The meaning representation is then sentto the dialogue management component for re-view-summary database lookup.
A response isthen generated by the language generation compo-nent, and a speech utterance is generated by thesynthesizer and sent back to the user.
The dialoguesystem implementation is beyond the scope of thispaper and will be discussed later in a separate pa-per.3 Dialogue-oriented Review SummaryGenerationGiven an inquiry from users, the answer from arecommendation system should be helpful andrelevant.
So the first task is to identify a phrase as?helpful?
or not.
The task of identifying a phrase asinformative and relevant, therefore, is defined as aclassification problem: =  ?
?
#?
= ?
%#%&%=1          (1)where y is the label of a phrase, assigned as ?1?
ifthe phrase is highly informative and relevant, and?-1?
if the phrase is uninformative.
#?
is the featurevector extracted from the phrase, and  ?
is thecoefficient vector.We employ statistical models such as SVMs(Joachims, 1998) and decision trees (Quinlan,1986) to train the classification model.
For modellearning, we employ a feature set including statis-tical features, sentiment features and semanticfeatures.Generally speaking, phrases with neutral senti-ment are less informative than those with strongsentiment, either positive or negative.
For example,?fried seafood appetizer?, ?baked halibut?, ?elec-tronic bill?
and ?red drink?
do not indicate whethera restaurant is worth trying, as they did not expresswhether the fried seafood appetizer or the bakedhalibut are good or bad.
Therefore, we take thesentiment score of each phrase generated from acumulative offset model (Liu and Seneff, 2009) asa sentiment feature.
Sentiment scores of phrasesare exemplified in Table 2 (on a scale of 1 to 5).Phrase Sc.
Phrase Sc.really welcomingatmosphere4.8 truly amazing flavor 4.6perfect portions  4.4 very tasty meat 4.3busy place 3.1 typical Italian restaurant 3.1a little bit highprice2.2 pretty bad soup 1.8sloppy service 1.8 absolute worst service 1.4Table 2.
Examples of sentiment scores of phrases.We also employ a set of statistical features formodel training, such as the unigram probability ofthe adjective in a phrase, the unigram probabilityof the noun in a phrase, the unigram probability ofthe phrase and the bigram probability of the adjec-tive-noun pair in a phrase.Statistical features, however, fail to reveal theunderlying semantic meaning of phrases.
For ex-ample, phrases ?greasy chicken tikka masala?
and?drunken husband?
have the same n-gram proba-bilities in our corpus (a single observation), but67they should certainly not be treated as the same.To capture the semantic meanings of phrases, wefirst cluster the topics of phrases into generic se-mantic categories.
The language-model based al-gorithm is given by:'(() | (%) = ?
'(() |+) ?
'(+|(%)+?.=  ?
'(+ ,())'(+) ?
'(+ ,(%)'((%)+?.=  1'((%)?
1'(+) ?
'(+, ()) ?
'(+, (%)+?.
(2)where A represents the set of all the adjectives inthe corpus.
We select a small set of initial topicswith the highest frequency counts (e.g., ?food?,?service?
and ?atmosphere?).
For each of the othertopics tc  (e.g., ?chicken?, ?waitress?
and ?d?cor?
),we calculate its similarity with each initial topic (%based on the bigram probability statistics.
Forthose topics with conditional probability higherthan a threshold for an initial topic (%, we assignthem to the cluster of (%.
We use this as a semanticfeature, e.g., whether the topic of a phrase belongsto a generic semantic category.
Table 3 gives someclustering examples.Category Relevant Topicsfoodappetizer, beer, bread, fish, fries, icecream, margaritas, menu, pizza, pasta,rib, roll, sauce, seafood, sandwich,steak, sushi, dessert, cocktail, brunchservice waiter, staff, management, server, hostess, chef, bartender, waitstaffatmosphere d?cor, ambiance, music, vibe, setting, environment, crowdprice bill, pricing, pricesTable 3.
Topic to semantic category clustering.This language-model-based method relies onbigram probability statistics and can well clusterhighly frequent topics.
Categories such as ?service?and ?atmosphere?
contain very limited related top-ics, most of which have high frequencies (e.g.,?waiter?, ?staff?, ?ambiance?
and ?vibe?).
The cate-gory ?food?, however, is very domain-specific andcontains a very large vocabulary, from genericsub-categories such as ?sushi?, ?dessert?
and?sandwich?
as shown in the examples, to specificcourses such as ?bosc pear bread pudding?
and?herb roasted vermont pheasant wine cap mu-shrooms?.
These domain-specific topics have verylow frequencies, yet they are very relevant andvaluable.
But many of them are discarded by theclustering.
It would be a similar case in other do-mains.
For example, consumer products, moviesand books all have domain-independent semanticcategories (e.g., ?price?
and ?released date?)
anddomain-specific categories (e.g., technical featuresof consumer products, casts of movies and authorsof books).To recover these context-relevant topics, weemploy domain context relations such as a con-text-related ontology.
A context-related ontologycan be constructed from structured web resourcessuch as online menus of restaurants, names of ac-tors and actresses from movie databases, and spe-cifications of products from online shops.
Anexample of a partial online menu of a restaurant isshown in Figure 3.
From these structured web re-sources, we can build up a hierarchical ontology,based on which a set of semantic features can beextracted (e.g., whether a phrase contains a coursename, or an actress?s name, or a dimension oftechnical features of a consumer product).EntreeRoasted Pork Loin Wrapped In Bacon with watermelon andred onion salad spicy honey-mustard bbq sauceSpicy Halibut And Clam Roast with bacon braised greens,white beans and black trumpet mushroomsParmesan and Caramelized Shallot Wrapper Style Ravi-oli turnip greens and white truffle oilHerb Roasted Vermont Pheasant Wine Cap Mushrooms,Pearl Onions and Fava BeansDessertChocolate Tasting Plate of white chocolate bombe milk choc-olate creme brul?e and dark chocolate flourless cakeWhite Fruit Tasting Plate of warm apple strudel butterscotch,Bosc Pear bread pudding and toasted coconut panna cottaEntr?e Pork loin, bacon, watermelon, red onionsalad, honey, mustard, bbq sauceDessert  Chocolate, milk, cr?me brulee, cakeFigure 3.
Example of a partial online menu and an ex-emplary ontology derived.After the classification, phrases identified as?highly informative and relevant?
are clusteredinto different aspects according to the semanticcategory clustering and the hierarchical ontology.An average sentiment score for each aspect is thencalculated:+/0(1() =?
233?41|41|(3)68where 1(  represents the aspect s of entry t (e.g., arestaurant, a movie, or a consumer product), 41represents the set of phrases in the cluster of as-pect s, and 23  represents the sentiment score ofphrase j in the cluster.The set of phrases selected for one entry maycome from several reviews on this single entry,and many of them may include the same noun(e.g., ?good fish?, ?not bad fish?
and ?above-average fish?
for one restaurant).
Thus, the nextstep is multi-phrase redundancy resolution.
Weselect the phrase with a sentiment score closest tothe average score of its cluster as the most repre-sentative phrase on each topic:5 = +265%&3?4%(|23 ?
+/0(1()|)    (4)where +/0(1()  represents the average sentimentscore of aspect 1, 4%  represents the set of phraseson the same topic %  in the cluster 1 , and 23represents the sentiment score of phrase 3.This sequence of topic categorization, ontologyconstruction, phrase pruning and redundancy eli-mination leads to a summary database, which canbe utilized for dialogue generation in spoken rec-ommendation systems.
A review summary data-base entry generated by the proposed approachesis exemplified in Figure 4.
{ restaurant "dali restaurant and tapas bar":atmosphere ( "wonderful evening", "cozy atmos-phere", "fun decor", "romantic date" ):atmosphere_rating "4.1":food ( "very fresh ingredients",  "tasty fish","creative dishes",  "good sangria" ):food_rating "3.9":service ( "fast service" ):service_rating "3.9":general ("romantic restaurant","small space" ):general_rating "3.6"                 }Figure 4.
Example of a review summary database entrygenerated by the proposed approaches.4 ExperimentsIn this project, we substantiate the proposed ap-proach in a restaurant domain for our spoken di-alogue system (Gruenstein and Seneff, 2007),which is a web-based multimodal dialogue systemallowing users to inquire about information aboutrestaurants, museums, subways, etc.
We harvesteda data collection of 137,569 reviews on 24,043restaurants in 9 cities in the U.S. from an onlinerestaurant evaluation website1.
From the dataset,857,466 sentences were subjected to parse analysis;and a total of 434,372 phrases (114,369 uniqueones) were extracted from the parsable subset(78.6%) of the sentences.Most pros/cons consist of well-formatted phras-es; thus, we select 3,000 phrases extracted frompros/cons as training data.
To generate a humanjudgment-consistent training set, we manually la-bel the training samples with ?good?
and ?bad?
la-bels.
We then randomly select a subset of 3,000phrases extracted from review texts as the test setand label the phrases.
The kappa agreement be-tween two sets of annotations is 0.73, indicatingsubstantial consistency.
We use the two annotationsets as the ground truth.To extract context-related semantic features, wecollect a large pool of well-formatted menus froman online resource2, which contains 16,141 restau-rant menus.
Based on the hierarchical structure ofthese collected menus, we build up a context-related ontology and extract a set of semantic fea-tures from the ontology, such as whether the topicof a phrase is on category-level (e.g., ?entr?e?,?dessert?, ?appetizers?, ?salad?
), whether the topicis on course-level (e.g., ?Roasted Pork Loin?, ?Spi-cy Halibut and Clam Roast?
), and whether the top-ic is on ingredient-level (e.g., ?beans?, ?chicken?,?mushrooms?, ?scallop?
).We employ the three types of features as afore-mentioned to train the SVMs and the decision treemodels.
To select the most valuable features formodel training, we conducted a set of leave-one-feature-out experiments for both the SVMs and thedecision tree models.
We found that all the fea-tures except the adjective unigram probabilitycontribute positively to model learning.
From fur-ther data analysis we observed that many phraseswith popular adjectives have context-unrelatednouns, which makes the adjective unigram proba-bility fail to become a dominant factor for phraserelevance.
Using the adjective unigram probabilityas a learning feature will mislead the system intotrusting an adjective that is common but has a poorbigram affinity to the noun in the phrase.
Thus, weeliminate this feature for both the SVMs and thedecision tree learning.1 http://www.citysearch.com2 http://www.menupages.com69To evaluate the performance of the classifica-tion models, we take a set of intuitively motivatedheuristic rules as the baseline.
Figure 5 gives thepseudo-code of the heuristic rule algorithm, whichuses variations of all the features except the uni-gram probability of adjectives.If(sentiment score of the phrase exists)if(sentiment score is within neutral range) label=-1;elseif(phrase appeared in the training data)if((3<frequency of phrase < 100))   label = 1;elseif(frequency of phrase >= 100)   label = -1;else    if(topic belongs to ontology)  label = 1;else   label = -1;elseif(topic belongs to ontology)   label = 1;else   label = -1;elseif(phrase appeared in the training data)if((3<frequency of phrase < 100))if(topic belongs to ontology)  label = 1;else   label = -1;elseif(frequency of phrase >= 100)   label = -1;elseif(topic belongs to ontology)  label = 1;else   if(frequency of noun > 100) label = 1;else   label = -1;elseif(topic belongs to ontology)  label = 1;else     if(frequency of noun > 100)   label = 1;else   label = -1;Figure 5.
Pseudo-code of the heuristic rule algorithm.The performance of classification by differentmodels is shown in Table 4.
Although the heuris-tic rule algorithm is complicated and involves hu-man knowledge, the statistical models trained bySVMs and the decision tree algorithms both out-perform the baseline significantly.
The SVM mod-el outperforms the baseline by 10.5% and 11.9%on the two annotation sets respectively.
The deci-sion tree model outperforms the baseline by 16.4%and 23.2% (average relative improvement of 36%),and it also outperforms the SVM model by 5.9%and 11.3% (average relative improvement of 13%).The classification model using the decision treealgorithm can achieve a precision of 77.9% and74.5% compared with the ground truth, which isquite comparable to human judgment (the preci-sion of one annotation set based on the other is74%).
This shows that the decision tree model canpredict phrase labels as reliably as human judg-ment.Baseline SVM Decision treeAnnotation 1 61.5% 72.0% 77.9%Annotation 2 51.3% 63.2% 74.5%Table 4.
Precision of phrase classification using theheuristic rule baseline, the SVM model, and the deci-sion tree algorithm.To gain further insight on the contributions ofeach feature set to the decision tree learning, Table5 gives the experimental results on leaving eachfeature out of model training.
As shown, withoutsemantic features, the precision is 70.6% and 65.4%on the two annotation sets, lower by 7.3% and 9.1%than the case of training the model with all thefeatures (77.9% and 74.5%).
This shows that thesemantic features significantly contribute to thedecision tree learning.Feature set A1 A2all features  77.9% 74.5%without bigram probabilityof adjective-noun pair56.6%(-21.3%)63.9%(-10.6%)without unigram probabilityof the phrase57.6%(-20.3%)64.3%(-10.2%)without unigram probabilityof the noun59.8%(-18.1%)67.8%(-6.7%)without sentiment score ofthe phrase63.4%(-14.5%)66.6%(-7.9%)without underlying semanticfeatures70.6%(-7.3%)65.4%(-9.1%)Table 5.
Performance of the decision tree model byleaving each feature out of model training (?A1?
and?A2?
represent the annotation set 1 and 2 respectively).The experimental results also show that the fea-ture of bigram probability of the adjective-nounpair contributes the most to the model learning.Without this feature, the precision drops by 21.3%and 10.6%, reaching the lowest precision amongall the leave-one-out experiments.
This confirmsour observation that although a single adjective isnot dominant, the pair of the adjective and thenoun that co-occurs with it plays an important rolein the classification.The sentiment of phrases also plays an impor-tant role.
Without sentiment features, the precision70drops to 63.4% and 66.6% respectively on the twoannotations, decreasing by 14.5% and 7.9%.
Thisshows that the sentiment features contribute sig-nificantly to the classification.5 DiscussionsExperimental results show that the decision treealgorithm outperforms the SVMs on this particularclassification problem, and it outperforms the heu-ristic rule baseline significantly.
Thus, althoughthe identification of informativeness and relevanceof phrases is a rather subjective problem, which isdifficult to predict using only human knowledge, itcan be well defined by decision trees.
Part of thereason is that the decision tree algorithm can makebetter use of a combination of Boolean value fea-tures (e.g., whether a topic belongs to a context-related ontology) and continuous value features.Also, as the phrase classification task is very sub-jective, it is very similar to a ?hierarchical if-elsedecision problem?
in human cognition, where de-cision tree algorithms can fit well.
Figure 6 showsa partial simplified decision tree learned from ourmodel, which can give an intuitive idea of the de-cision tree models.6 Related WorkSentiment classification and opinion mining havebeen well studied for years.
Most studies have fo-cused on text-based systems, such as document-level sentiment classification and sentence-levelopinion aggregation (Turney, 2002; Pang et al,2002; Dave et al, 2003; Hu and Liu, 2004; Popes-cu and Etzioni, 2005; Wilson et al, 2005; Zhuanget al, 2006; Kim and Hovy, 2006).There was a study conducted by Carenini et alin 2006, which proposed a combination of a sen-tence extraction-based approach and a languagegeneration-based approach for summarizing eva-luative arguments.
In our work, we utilize a lower-level phrase-based extraction approach, which uti-lizes high level linguistic features and syntacticstructure to capture phrase patterns.There was also a study on using reviews to gen-erate a dictionary of mappings between semanticrepresentations and realizations of concepts fordialogue systems (Higashinaka et al, 2006; Higa-shinaka, 2007).
They also used the associationbetween user ratings and reviews to capture se-mantic-syntactic structure mappings.
A set of fil-tering rules was manually created to eliminatelow-quality mappings.
In our approach, we use anautomatic approach to classifying high/low infor-mative phrases.
The learning features are domain-independent with no hand-crafted rules, and canbe extended to other domains effortlessly.7 ConclusionsIn this paper we proposed a three-level frameworkfor review-based recommendation dialogue sys-tems, including linguistic phrase extraction, dialo-gue-oriented review summary generation, andhuman-friendly dialogue generation.
The contribu-tions of this paper are three-fold: 1) it identifiedand defined the research goal of utilizing opinionsummarization for real human-computer conversa-tion; 2) it formulated an evaluation methodologyfor high-density review summary for dialoguepurposes; 3) it proposed an approach to automaticclassification of high/low informative phrases us-ing a decision tree model.
Experimental resultsshowed that the decision tree model significantlyoutperforms a heuristic rule baseline and the SVMmodel, and can resolve the phrase classificationproblem comparably to humans consistently.Future work will focus on: 1) applying the sen-timent scoring model to noun/verb sentiment as-sessment; 2) application of the review summarygeneration approach in other domains and otherlanguages; 3) data collection on user engagementwith our dialogue systems involving review-summary evaluation.Figure 6.
A partial simplified decision tree learned fromour model.71ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2009.
Multi-facet Rating of Product Reviews.In Proceedings of European Conference on Informa-tion Retrieval.S.R.K.
Branavan, Harr Chen, Jacob Eisenstein, andRegina Barzilay.
2008.
Learning document-levelsemantic properties from free-text annotations.
InProc.
of ACL.Giuseppe Carenini, Raymond Ng, and Adam Pauls.2006.
Multi-Document Summarization of EvaluativeText.
In Proceedings of the Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: opinion extractionand semantic classification of product reviews.
InProceedings of the International Conference onWorld Wide Web.W.
Eckert, T. Kuhn, H. Niemann, S. Rieck, A. Scheuer,and E. G. Schukat-talamazzini.
1993.
A Spoken Di-alogue System for German Intercity Train TimetableInquiries.
In Proc.
European Conf.
on Speech Tech-nology.Alexander Gruenstein and Stephanie Seneff.
2007.
Re-leasing a Multimodal Dialogue System into theWild: User Support Mechanisms.
In Proceedings ofthe 8th SIGdial Workshop on Discourse and Dialo-gue, Antwerp, pages 111-119.A.
L. Gorin, G. Riccardi and J. H. Wright.
1997.
?Howmay I help you??
Speech Communication, vol.
23,pp.
113?127.Ryuichiro Higashinaka, Rashmi Prasad and MarilynWalker.
2006.
Learning to GenerateNaturalistic Utterances Using Reviews in SpokenDialogue Systems.
In Proceedings of COLING-ACL.Ryuichiro Higashinaka, Marilyn Walker and RashmiPrasad.
2007.
An Unsupervised Methodfor Learning Generation Dictionaries for Spoken Di-alogue Systems by Mining User Reviews.Journal of ACM Transactions on Speech and Lan-guage Processing.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 2004ACM SIGKDD international conference on Know-ledge Discovery and Data mining.S.M.
Kim and E.H. Hovy.
2006.
Identifying and Ana-lyzing Judgment Opinions.
In Proc.
of HLT/NAACL.Jingjing Liu and Stephanie Seneff.
2009. Review Sen-timent Scoring via a Parse-and-Paraphrase Para-digm.
In proceedings of EMNLP.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su,and ChengXiang Zhai.
2007.
Topic Sentiment Mix-ture: Modeling Facets and Opinions in Weblogs.
InProc.
of WWW.Bo Pang, Lillian Lee, and S. Vaithyanathan.
2002.Thumbs up?
Sentiment classification using machinelearning techniques.
In Proceedings of EMNLP.A.M.
Popescu and O. Etzioni.
2005.
Extracting productfeatures and opinions from reviews.
In Proceedingsof EMNLP.JR Quinlan, 1986.
Induction of decision trees.
Machinelearning, Springer-Netherlands.A.
Raux, B. Langner, A.
Black, and M. Eskenazi.
2003.LET'S GO: Improving Spoken Dialog Systems forthe Elderly and Non-natives.
In Proc.
Eurospeech.Benjamin Snyder and Regina Barzilay.
2007.
MultipleAspect Ranking using the Good Grief Algorithm.
InProceedings of NAACL-HLT.Ivan Titov and Ryan McDonald.
2008a.
Modeling On-line Reviews with Multi-Grain Topic Models.
InProc.
of WWW.Ivan Titov and Ryan McDonald.
2008b.
A Joint Modelof Text and Aspect Ratings for Sentiment Summari-zation.
In Proceedings of the Annual Conference ofthe Association for Computational Linguistics.Peter D. Turney.
2002.
Thumbs up or thumbs down?Sentiment orientation applied to unsupervised classi-fication of reviews.
In Proceedings of the AnnualConference of the Association for ComputationalLinguistics.T.
Joachims.
1998.
Text categorization with supportvector machines: Learning with many relevant fea-tures.
In Proc.
of ECML, p. 137?142.T.
Wilson, J. Wiebe, and P. Hoffmann.
2005.
Recog-nizing Contextual Polarity in Phrase-Level Senti-ment Analysis.
In Proc.
of HLT/EMNLP.Victor Zue, Stephanie Seneff, James Glass, Joseph Po-lifroni, Christine Pao, Timothy J. Hazen, and LeeHetherington.
2000.
JUPITER: A Telephone-BasedConversational Interface for Weather Information.
InIEEE Transactions on Speech and Audio Processing,Vol.
8 , No.
1.Li Zhuang, Feng Jing, and Xiao-Yan Zhu.
2006.
Moviereview mining and summarization.
In Proceedings ofthe 15th ACM international conference on Informa-tion and knowledge management.72
