Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 225?232, Vancouver, October 2005. c?2005 Association for Computational LinguisticsError Handling in the RavenClaw Dialog Management FrameworkDan BohusComputer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA, 15213dbohus@cs.cmu.eduAlexander I. RudnickyComputer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA, 15213air@cs.cmu.eduAbstractWe describe the error handling architect-ture underlying the RavenClaw dialogmanagement framework.
The architectureprovides a robust basis for current and fu-ture research in error detection and recov-ery.
Several objectives were pursued in itsdevelopment: task-independence, ease-of-use, adaptability and scalability.
We de-scribe the key aspects of architectural de-sign which confer these properties, anddiscuss the deployment of this architect-ture in a number of spoken dialog systemsspanning several domains and interactiontypes.
Finally, we outline current researchprojects supported by this architecture.1 IntroductionOver the last decade, improvements in speech rec-ognition and other component technologies havepaved the way for the emergence of complex task-oriented spoken dialog systems.
While traditionallythe research community has focused on buildinginformation-access and command-and-controlsystems, recent efforts aim towards building moresophisticated language-enabled agents, such aspersonal assistants, interactive tutors, open-domainquestion answering systems, etc.
At the other endof the complexity spectrum, simpler systems havealready transitioned into day-to-day use and arebecoming the norm in the phone-based customer-service industry.Nevertheless, a number of problems remain inneed of better solutions.
One of the most importantlimitations in today?s spoken language interfaces istheir lack of robustness when faced with under-standing errors.
This problem appears across alldomains and interaction types, and stems primarilyfrom the inherent unreliability of the speech recog-nition process.
The recognition difficulties arefurther exacerbated by the conditions under whichthese systems typically operate: spontaneous spe-ech, large vocabularies and user populations, andlarge variability in input line quality.
In these set-tings, average word-error-rates of 20-30% (and upto 50% for non-native speakers) are quite common.Left unchecked, speech recognition errors canlead to two types of problems in a spoken dialogsystem: misunderstandings and non-understand-ings.
In a misunderstanding, the system obtains anincorrect semantic interpretation of the user?s turn.In the absence of robust mechanisms for assessingthe reliability of the decoded inputs, the systemwill take the misunderstanding as fact and will actbased on invalid information.
In contrast, in a non-understanding the system fails to obtain an inter-pretation of the input altogether.
Although no falseinformation is incorporated in this case, the situa-tion is not much better: without an appropriate setof recovery strategies and a mechanism for diag-nosing the problem, the system?s follow-up optionsare limited and uninformed.
In general, unlessmitigated by accurate error awareness and robustrecovery mechanisms, speech recognition errorsexert a strong negative impact on the quality andultimately on the success of the interactions (Sand-ers et al 2002).Two pathways towards increased robustnesscan be easily envisioned.
One is to improve theaccuracy of the speech recognition process.
Thesecond is to create mechanisms for detecting andgracefully handling potential errors at the conver-sation level.
Clearly, these two approaches do not225stand in opposition and a combined effort wouldlead to the best results.
The error handling archi-tecture we describe in this paper embodies the sec-ond approach: it aims to provide the mechanismsfor robust error handling at the dialog managementlevel of a spoken dialog system.The idea of handling errors through conversa-tion has already received a large amount of atten-tion from the research community.
On the theore-tical side, several models of grounding in commu-nication have been proposed (Clark and Schaefer,1989; Traum, 1998).
While these models provideuseful insights into the grounding process as ithappens in human-human communication, theylack the decision-making aspects required to drivethe interaction in a real-life spoken dialog system.In the Conversational Architectures project, Paekand Horvitz (2000) address this challenge by de-veloping a computational implementation of thegrounding process using Bayesian belief networks.However, questions still remain: the structure andparameters of the belief networks are handcrafted,as are the utilities for the various grounding ac-tions; due to scalability and task-representationissues, it is not known yet how the proposed ap-proach would transfer and scale to other domains.Three ingredients are required for robust errorhandling: (1) the ability to detect the errors, (2) aset of error recovery strategies, and (3) amechanism for engaging these strategies at theappropriate time.
For some of these issues, varioussolutions have emerged in the community.
Forinstance, systems generally rely on recognitionconfidence scores to detect potential misunder-standings (e.g.
Krahmer et al, 1999; Walker et al,2000) and use explicit and implicit confirmationstrategies for recovery.
The decision to engagethese strategies is typically based on comparing theconfidence score against manually preset thresh-olds (e.g.
Kawahara and Komatani, 2000).
Fornon-understandings, detection is less of a problem(systems know by definition when non-understand-ings occur).
Strategies such as asking the user torepeat or rephrase, providing help, are usually en-gaged via simple heuristic rules.At the same time, a number of issues remainunsolved: can we endow systems with better errorawareness by integrating existing confidence an-notation schemes with correction detection mecha-nisms?
Can we diagnose the non-understandingerrors on-line?
What are the tradeoffs between thevarious non-understanding recovery strategies?Can we construct a richer set of such strategies?Can we build systems which automatically tunetheir error handling behaviors to the characteristicsof the domains in which they operate?We have recently engaged in a research pro-gram aimed at addressing such issues.
More gener-ally, our goal is to develop a task-independent,easy-to-use, adaptive and scalable approach forerror handling in task-oriented spoken dialog sys-tems.
As a first step in this program, we havedeveloped a modular error handling architecture,within the larger confines of the RavenClaw dialogmanagement framework (Bohus and Rudnicky,2003).
The proposed architecture provides the in-frastructure for our current and future research onerror handling.
In this paper we describe the pro-posed architecture and discuss the key aspects ofarchitectural design which confer the desired prop-erties.
Subsequently, we discuss the deployment ofthis architecture in a number of spoken dialog sys-tems which operate across different domains andinteraction types, and we outline current researchprojects supported by the proposed architecture.2 RavenClaw Dialog ManagementWe begin with a brief overview of the RavenClawdialog management framework, as it provides thelarger context for the error handling architecture.RavenClaw is a dialog management frameworkfor task-oriented spoken dialog systems.
To date, ithas been used to construct a large number of sys-tems spanning multiple domains and interactiontypes (Bohus and Rudnicky, 2003): informationaccess (RoomLine, the Let?s Go Bus InformationSystem), guidance through procedures (LARRI),command-and-control (TeamTalk), taskable agents(Vera).
Together with these systems, RavenClawprovides the larger context as well as a test-bed forevaluating the proposed error handling architec-ture.
More generally, RavenClaw provides a robustbasis for research in various other aspects of dialogmanagement, such as learning at the task and dis-course levels, multi-participant dialog, timing andturn-taking, etc.A key characteristic of the RavenClaw frame-work is the separation it enforces between the do-main-specific and domain-independent aspects ofdialog control.
The domain-specific dialog controllogic is described by a Dialog Task Specification,226essentially a hierarchical dialog plan provided bythe system author.
A fixed, domain-independentDialog Engine manages the conversation by exe-cuting the given Dialog Task Specification.
In theprocess, the Dialog Engine also contributes a set ofdomain-independent conversational skills, such aserror handling (discussed extensively in Section 4),timing and turn-taking, etc.
The system authoringeffort is therefore minimized and focused entirelyon the domain-specific aspects of dialog control.2.1 The Dialog Task SpecificationA Dialog Task Specification consists of a tree ofdialog agents, where each agent manages a sub-part of the interaction.
Figure 1 illustrates a portionof the dialog task specification from RoomLine, aspoken dialog system which can assist users inmaking conference room reservations.
The rootnode subsumes several children: Welcome, whichproduces an introductory prompt, GetQuery whichobtains the time and room constraints from theuser, DoQuery which performs the database query,and DiscussResults which handles the follow-upnegotiation dialog.
Going one level deeper in thetree, GetQuery contains GetDate which requests thedate for the reservation, GetStartTime and GetEnd-Time which request the times, and so on.
This typeof hierarchical task representation has a number ofadvantages: it scales up gracefully, it can bedynamically extended at runtime, and it implicitlycaptures a notion of context in dialog.The agents located at the leaves of the tree arecalled basic dialog agents, and each of them im-plements an atomic dialog action (dialog move).There are four types of basic dialog agents: Inform?
conveys information to the user (e.g.
Welcome),Request ?
asks a question and expects an answer(e.g.
GetDate), Expect ?
expects information with-out explicitly asking for it, and EXecute ?
imple-ments a domain specific operation (e.g.
DoQuery).The agents located at non-terminal positions in thetree are called dialog agencies (e.g.
RoomLine,GetQuery).
Their role is to plan for and control theexecution of their sub-agents.
For each agent in thetree, the system author may specify preconditions,completion criteria, effects and triggers; variousother functional aspects of the dialog agents (e.g.state-specific language models for request-agents,help-prompts) are controlled through parameters.The information the system acquires and ma-nipulates in conversation is captured in concepts,associated with various agents in the tree (e.g.
date,start_time).
Each concept maintains a history ofprevious values, information about current candi-date hypotheses and their associated confidencescores, information about when the concept waslast updated, as well as an extended set of flagswhich describe whether or not the concept hasbeen conveyed to the user, whether or not the con-cept has been grounded, etc.
This rich representa-tion provides the necessary support for concept-level error handling.Dialog StackDialog EngineDialog TaskSpecificationExpectation Agendastart_time: [start_time] [time]date: [date]start_time: [start_time] [time]end_time: [end_time] [time]date: [date]start_time: [start_time] [time]end_time: [end_time] [time]location: [location]network: [with_network]->true,[without_network]->false?
?
?System: For when do you need the room?User:  let?s try two to four p.m.Parse:  [time](two) [end_time](to four pm)User InputRoomLineGetQueryGetStartTimedateend_time start_timeRoomLineI: Welcome GetQueryR: GetDateStart-OverR: GetStartTime R: GetEndTimeDiscussResults X: DoQueryFigure 1: RavenClaw architecture2272.2 The Dialog EngineThe Dialog Engine is the core domain-independentcomponent which manages the interaction by exe-cuting a given Dialog Task Specification.
The con-trol algorithms are centered on two data-structures:a dialog stack, which captures the dialog structureat runtime, and an expectation agenda, which cap-tures the system?s expectations for the user input ateach turn in the dialog.
The dialog is controlled byinterleaving Execution Phases with Input Phases.During an Execution Phase, dialog agents fromthe tree are placed on, and executed from the dia-log stack.
At the beginning of the dialog, the rootagent is placed on the stack.
Subsequently, the en-gine repeatedly takes the agent on the top of thestack and executes it.
When dialog agencies areexecuted, they typically schedule one of their sub-agents for execution by placing it on the stack.
Thedialog stack will therefore track the nested struc-ture of the dialog at runtime.
Ultimately, the execu-tion of the basic dialog agents on the leaves of thetree generates the system?s responses and actions.During an Input Phase, the system assemblesthe expectation agenda, which captures what thesystem expects to hear from the user in a giventurn.
The agenda subsequently mediates the trans-fer of semantic information from the user?s inputinto the various concepts in the task tree.
For theinterested reader, these mechanisms are describedin more detail in (Bohus and Rudnicky, 2003)Additionally, the Dialog Engine automaticallyprovides a number of conversational strategies,such as the ability to handle various requests forhelp, repeating the last utterance, suspending andresuming the dialog, starting over, reestablishingthe context, etc.
These strategies are implementedas library dialog agencies.
Their correspondingsub-trees are automatically added to the DialogTask Specification provided by the system author(e.g.
the Start-Over agency in Figure 1.)
The auto-matic availability of these strategies lessens devel-opment efforts and ensures a certain uniformity ofbehavior both within and across tasks.3 The Error Handling ArchitectureThe error handling architecture in the RavenClawdialog management framework subsumes twomain components: (1) a set of error handlingstrategies (e.g.
explicit and implicit confirmation,asking the user to repeat, etc.)
and (2) an errorhandling process which engages these strategies.The error handling strategies are implementedas library dialog agents.
The decision processwhich engages these strategies is part of the DialogEngine.
This design, in which both the strategiesand the decision process are decoupled from thedialog task, as well as from each other, provides anumber of advantages.
First, it ensures that the er-ror handling mechanisms are reusable across dif-ferent dialog systems.
Second, the approachguarantees a certain uniformity and consistency inerror handling behaviors both within and acrosssystems.
Third, as new error handling strategies aredeveloped, they can be easily plugged into any ex-isting system.
Last, but not least, the approach sig-nificantly lessens the system authoring effort byallowing developers to focus exclusively on de-scribing the dialog control logic.The responsibility for handling potential under-standing errors1 is delegated to the Error HandlingProcess which runs in the Dialog Engine (see Fig-ure 2).
At each system turn, this process collectsevidence and makes a decision with respect to en-gaging any of the error handling strategies.
Whennecessary, it will insert an error handling strategyon the dialog stack (e.g.
the ExplicitConfirm(start_time) strategy in Figure 2), thus modifyingon-the-fly the task originally specified by the sys-tem author.
The strategy executes and, once com-pleted, it is removed from the stack and the dialogresumes from where it was left off.1Note that the proposed framework aims to handleunderstanding errors.
The corresponding strategies are genericand can be applied in any domain.
Treatment of domain ortask-specific errors (e.g.
database access error, etc) still needsto be implemented as part of the dialog task specification.Error HandlingStrategiesError HandlingProcessExplicitConfirmRoomLineGetQueryGetStartTimeExplicitConfirm(start_time)Dialog StackEvidenceFigure 2: Error Handling ?
Block DiagramDialog Task SpecificationDialog Engine2283.1 Error Handling StrategiesThe error handling strategies can be divided intotwo groups: strategies for handling potential mis-understandings and strategies for handling non-understandings.For handling potential misunderstandings, threestrategies are currently available: Explicit Confir-mation, Implicit Confirmation and Rejection.For non-understandings, a larger number of er-ror recovery strategies are currently available:AskRepeat ?
the system asks the user to repeat;AskRephrase ?
the system asks the user to re-phrase; Reprompt ?
the system repeats the previousprompt; DetailedReprompt ?
the system repeats amore verbose version of the previous prompt,Notify ?
the system simply notifies the user that anon-understanding has occurred; Yield ?
the sys-tem remains silent, and thus implicitly notifies theuser that a non-understanding has occurred;MoveOn ?
the system tries to advance the task bygiving up on the current question and moving onwith an alternative dialog plan (note that this strat-egy is only available at certain points in the dia-log); YouCanSay ?
the system gives an example ofwhat the user could say at this point in the dialog;FullHelp ?
the system provides a longer help mes-sage which includes an explanation of the currentstate of the system, as well as what the user couldsay at this point.
An in-depth analysis of thesestrategies and their relative tradeoffs is available in(Bohus and Rudnicky, 2005a).
Several sampledialogs illustrating these strategies are availableon-line (RoomLine, 2003).3.2 Error Handling ProcessThe error handling decision process is imple-mented in a distributed fashion, as a collection oflocal decision processes.
The Dialog Engine auto-matically associates a local error handling processwith each concept, and with each request agent inthe dialog task tree, as illustrated in Figure 3.
Theerror handling processes running on individualconcepts are in charge of recovering from misun-derstandings on those concepts.
The error handlingprocesses running on individual request agents arein charge or recovering from non-understandingson the corresponding requests.At every system turn, each concept- andrequest-agent error handling process computes andforwards its decision to a gating mechanism, whichqueues up the actions (if necessary) and executesthem one at a time.
For instance, in the example inFigure 3, the error handling decision process forthe start_time concept decides to engage an explicitconfirmation on that concept, while the other deci-sion processes do not take any action.
In this casethe gating mechanism creates a new instance of anexplicit confirmation agency, passes it the pointerto the concept to be confirmed (start_time), andplaces it on the dialog stack.
On completion, thestrategy updates the confidence score of the con-firmed hypothesis in light of the user response, andthe dialog resumes from where it was left off.The specific implementation of the local deci-sion processes constitutes an active research issue.Currently, they are modeled as Markov DecisionProcesses (MDP).
The error handling processesrunning on individual concepts (concept-MDPs inend_timedatestart_timeExplicit ConfirmNo ActionFigure 3: A Distributed Error Handling ProcessExplicitConfirm(start_time)GatingMechanismError HandlingDecision Proc.
[Concept-MDP]No ActionNo ActionGetQueryR: GetDateR: GetStartTimeR: GetEndTimeRoomLineError HandlingDecision Proc.
[Concept-MDP]Error HandlingDecision Proc.
[Request-MDP]Error HandlingDecision Proc.
[Concept-MDP]229Figure 3) are partially-observable MDPs, with 3underlying hidden states: correct, incorrect andempty.
The belief state is constructed at each timestep from the confidence score of the top-hypothe-sis for the concept.
For instance, if the tophypothesis for the start_time concept is 10 a.m. withconfidence 0.76, then the belief state for thePOMDP corresponding to this concept is:{P(correct)=0.76, P(incorrect)=0.24, P(empty)=0}.The action-space for these models contains thethree error recovery strategies for handling poten-tial misunderstandings, and no-action.
The thirdingredient in the model is the policy.
A policy de-fines which action the system should take in eachstate, and is indirectly described by specifying theutility of each strategy in each state.
Currently, anumber of predefined policies (e.g.
always-explicit-confirm, pessimistic, and optimistic) areavailable in the framework.
Alternatively, systemauthors can specify and use their own policies.The error handling processes running on re-quest agents (request-MDPs in Figure 3) are incharge of handling non-understandings on thoserequests.
Currently, two types of models are avail-able for this purpose.
The simplest model has threestates: non-understanding, understanding andinactive.
A second model also includes informationabout the number of consecutive non-understand-ings that have already happened.
In the future, weplan to identify more features which carry usefulinformation about the likelihood of success of in-dividual recovery strategies and use them to createmore complex models.
The action-space is definedby the set of non-understanding recovery strategiespresented in the previous subsection, and no-action.
Similar to the concept-MDPs, a number ofdefault policies are available; alternatively, systemauthors can specify their own policy for engagingthe strategies.While the MDP implementation allows us toencode various expert-designed policies, our ulti-mate goal is to learn such policies from collecteddata using reinforcement learning.
Reinforcementlearning has been previously used to derive dialogcontrol policies in systems operating with smalltasks (Scheffler and Young, 2002; Singh et al2000).
The approaches proposed to date sufferhowever from one important shortcoming, whichhas so far prevented their use in large, practicalspoken dialog systems.
The problem is lack ofscalability: the size of the state space grows veryfast with the size of the dialog task, and this ren-ders the approach unfeasible in complex domains.A second important limitation of reinforcementlearning techniques proposed to date is that thelearned policies cannot be reused across tasks.
Foreach new system, a new MDP has to be con-structed, new data has to be collected, and a newtraining phase is necessary.
This requires a signifi-cant amount of expertise and effort from the sys-tem author.We believe that the error handling architecturewe have described addresses these issues in severalways.
The central idea behind the distributed na-ture of the approach is to keep the learning prob-lem tractable by leveraging independence relation-ships between different parts of the dialog.
First,the state and action-spaces can be maintained rela-tively small since we are only focusing on makingerror handling decisions (as opposed to other dia-log control decisions).
A more complex tasktranslates into a larger number of MDP instantia-tions rather than a more complex model structure.Second, both the model structure and parameters(i.e.
the transition probabilities) can be tied acrossmodels: for instance the MDP for grounding thestart_time concept can be identical to the one forgrounding the end_time concept; all models forgrounding Yes/No concepts could be tied together,etc.
Model tying has the potential to greatly im-prove scalability since data is polled together andthe total number of model parameters to be learnedgrows sub-linearly with the size of the task.
Third,since the individual MDPs are decoupled from theactual system task, the policies learned in a par-ticular system can potentially be reused in othersystems (e.g.
we expect that grounding yes/no con-cepts functions similarly at different locations inthe dialog, and across domains).
Last but not least,the approach can easily accommodate dynamictask generation.
In traditional reinforcementlearning approaches the state and action-spaces ofthe underlying MDP are task-specific.
The tasktherefore has to be fixed, known in advance: forinstance the slots that the system queries the userabout (in a slot-filling system) are fixed.
In con-trast, in the RavenClaw architecture, the dialogtask tree (e.g.
the dialog plan) can be dynamicallyexpanded at runtime with new questions and con-cepts, and the corresponding request- and concept-MDPs are automatically created by the Dialog En-gine.2304 Deployment and Current ResearchWhile a quantitative evaluation of design charac-teristics such as task-independence, scalability, andease-of-use is hard to perform, a first-order empiri-cal evaluation of the proposed error handling ar-chitecture can be accomplished by using it indifferent systems and monitoring the system au-thoring process and the system?s operation.To date, the architecture has been successfullydeployed in three different spoken dialog systems.A first system, RoomLine (2003), is a phone-basedmixed-initiative system that assists users in makingconference room reservations on campus.
A sec-ond system, the Let?s Go!
Bus Information System(Raux et al 2005), provides information about busroutes and schedules in the greater Pittsburgh area(the system is available to the larger public).
Fi-nally, Vera is a phone-based taskable agent thatcan be instructed to deliver messages to a thirdparty, make wake-up calls, etc.
Vera actually con-sists of two dialog systems, one which handles in-coming requests (Vera In) and one which performsmessage delivery (Vera Out).
In each of these sys-tems, the authoring effort with respect to errorhandling consisted of: (1) specifying which modelsand policies should be used for the concepts andrequest-agents in the dialog task tree, and (2)writing the language generation prompts for ex-plicit and implicit confirmations for each concept.Even though the first two systems operate insimilar domains (information access), they havevery different user populations: students and fac-ulty on campus in the first case versus the entirePittsburgh community in the second case.
As aresult, the two systems were configured with dif-ferent error handling strategies and policies (seeTable 1).
RoomLine uses explicit and implicit con-firmations with an optimistic policy to handle po-tential misunderstandings.
In contrast, the Let?s GoPublic Bus Information System always uses ex-plicit confirmations, in an effort to increase robust-ness (at the expense of potentially longer dialogs).For non-understandings, RoomLine uses the fullset of non-understanding recovery strategies pre-sented in section 3.1.
The Let?s Go Bus Informa-tion system uses the YouCanSay and FullHelpstrategies.
Additionally a new GoToAQuieterPlacestrategy was developed for this system (and is nowavailable for use into any other RavenClaw-basedsystem).
This last strategy asks the user to move toa quieter place, and was prompted by the observa-tion that a large number of users were calling thesystem from noisy environments.While the first two systems were developed byauthors who had good knowledge of the Raven-Claw dialog management framework, the third sys-tem, Vera, was developed as part of a class project,by a team of six students who had no prior experi-ence with RavenClaw.
Modulo an initial lack ofdocumentation, no major problems were encoun-tered in configuring the system for automatic errorhandling.
Overall, the proposed error handling ar-chitecture adapted easily and provided the desiredfunctionality in each of these domains: while newstrategies and recovery policies were developed forsome of the systems, no structural changes wererequired in the error handling architecture.Table 1: Spoken dialog systems using the RavenClaw error handling architectureRoomLine Let?s Go Public Vera In / OutDomain room reservations bus route information task-able agentInitiative type mixed system mixed / mixedTask size: #agents ; #concepts 110 ; 25 57 ; 19 29 ; 4 / 31 ; 13Strategies for misunderstandings explicit and implicit explicit explicit and implicit /explicit onlyPolicy for misunderstandings optimistic always-explicit optimistic /always-explicitStrategies for non-understandings all strategies(see Section 3.1)go-to-quieter-place,you-can-say, helpall strategies /repeat promptPolicy for non-understandings choose-random author-specifiedheuristic policychoose-random /always-repeat-promptSessions collected so far 1393 2836 72 / 131Avg.
task success rate 75% 52% (unknown)% Misunderstandings 17% 28% (unknown)% Non-understandings 13% 27% (unknown)% turns when strategies engage 41% 53% 36% / 44%2315 Conclusion and Future WorkWe have described the error handling architectureunderlying the RavenClaw dialog managementframework.
Its design is modular: the error han-dling strategies as well as the mechanisms for en-gaging them are decoupled from the actual dialogtask specification.
This significantly lessens thedevelopment effort: system authors focus exclu-sively on the domain-specific dialog control logic,and the error handling behaviors are generatedtransparently by the error handling process runningin the core dialog engine.
Furthermore, we haveargued that the distributed nature of the error han-dling process leads to good scalability propertiesand facilitates the reuse of policies within andacross systems and domains.The proposed architecture represents only thefirst (but an essential step) in our larger researchprogram in error handling.
Together with the sys-tems described above, it sets the stage for a numberof current and future planned investigations in er-ror detection and recovery.
For instance, we haverecently conducted an extensive investigation ofnon-understanding errors and the ten recoverystrategies currently available in the RavenClawframework.
The results of that study fall beyondthe scope of this paper and are presented separatelyin (Bohus and Rudnicky, 2005a).
In another pro-ject supported by this architecture, we have devel-oped a model for updating system beliefs overconcept values in light of initial recognition confi-dence scores and subsequent user responses tosystem actions.
Initially, our confirmation strate-gies used simple heuristics to update the system?sconfidence score for a concept in light of the userresponse to the verification question.
We haveshowed that a machine learning based approachwhich integrates confidence information with cor-rection detection information can be used to con-struct significantly more accurate system beliefs(Bohus and Rudnicky, 2005b).
Our next effortswill focus on using reinforcement learning toautomatically derive the error recovery policies.ReferencesBohus, D., Rudnicky, A., 2003 ?
RavenClaw: DialogueManagement Using Hierarchical Task Decomposi-tion and an Expectation Agenda, in Proceedings ofEurospeech-2003, Geneva, SwitzerlandBohus, D., Rudnicky, A., 2005a ?
Sorry, I didn?t CatchThat!
An Investigation into Non-understandings andRecovery Strategies, to appear in SIGDial-2005, Lis-bon, PortugalBohus, D., Rudnicky, A., 2005b ?
Constructing Accu-rate Beliefs in Spoken Dialog Systems, submitted toASRU-2005, Cancun, MexicoClark, H.H., Schaefer, E.F., 1989 ?
Contributing to Dis-course, in Cognitive Science, vol 13, 1989.Kawahara, T., Komatani, K., 2000 ?
Flexible mixed-initiative dialogue management using concept-levelconfidence measures of speech recognizer output, inProc.
of COLING, Saarbrucken, Germany, 2000.Krahmer, E., Swerts, M., Theune, M., Weegels, M.,1999 - Error Detection in Human-Machine Interac-tion, Speaking.
From Intention to Articulation, MITPress, Cambridge, Massachusetts, 1999Paek, T., Horvitz, E., 2000 ?
Conversation as ActionUnder Uncertainty, in Proceedings of the SixteenthConference on Uncertainty and Artificial Intelli-gence, Stanford, CA, June 2000.Raux, A., Langner, B., Bohus, D., Black, A., Eskenazi,M., 2005 ?
Let?s Go Public!
Taking a Spoken DialogSystem to the Real World, submitted to Interspeech-2005, Lisbon, PortugalRoomLine web site, as of June 2005 ?www.cs.cmu.edu/~dbohus/RoomLineSanders, G., Le, A., Garofolo, J., 2002 ?
Effects of WordError Rate in the DARPA Communicator Data Dur-ing 2000 and 2001, in Proceedings of ICSLP?02,Denver, Colorado, 2002.Scheffler, K., Young, S., 2002 ?
Automatic learning ofdialogue strategy using dialogue simulation and re-inforcement learning, in Proceedings of HLT-2002.Singh, S., Litman, D., Kearns, M., Walker, M., 2000 ?Optimizing Dialogue Management with Reinforce-ment Learning: Experiments with the NJFun System,in Journal of Artificial Intelligence Research, vol.
16,pp 105-133, 2000.Traum, D., 1998 ?
On Clark and Schaefer?s Contribu-tion Model and its Applicability to Human-ComputerCollaboration, in Proceedings of the COOP?98, May1998.Walker, M., Wright, J., Langkilde, I., 2000 ?
UsingNatural Language Processing and Discourse Fea-tures to Identify Understanding Errors in a SpokenDialogue System, in Proc.
of the 17?th InternationalConference of Machine Learning, pp 1111-1118.232
