Proceedings of the 2014 Workshop on Biomedical Natural Language Processing (BioNLP 2014), pages 38?44,Baltimore, Maryland USA, June 26-27 2014.c?2014 Association for Computational LinguisticsDetecting Health Related Discussions in Everyday TelephoneConversations for Studying Medical Events in the Lives of Older AdultsGolnar Sheikhshab, Izhak Shafran, Jeffrey KayeOregon Health & Science Universitysheikhsh,shafrani,kaye@ohsu.eduAbstractWe apply semi-supervised topic modelingtechniques to detect health-related discus-sions in everyday telephone conversations,which has applications in large-scale epi-demiological studies and for clinical in-terventions for older adults.
The privacyrequirements associated with utilizing ev-eryday telephone conversations precludemanual annotations; hence, we exploresemi-supervised methods in this task.
Weadopt a semi-supervised version of LatentDirichlet Allocation (LDA) to guide thelearning process.
Within this framework,we investigate a strategy to discard irrel-evant words in the topic distribution anddemonstrate that this strategy improvesthe average F-score on the in-domain taskand an out-of-domain task (Fisher corpus).Our results show that the increase in dis-cussion of health related conversations isstatistically associated with actual medi-cal events obtained through weekly self-reports.1 IntroductionThere has been considerable interest in under-standing, promoting, and monitoring healthylifestyles among older adults while minimizing thefrequency of clinical visits.
Longitudinal studieson large cohorts are necessary, for example, to un-derstand the association between social networks,depression, dementia, and general health.
In thiscontext, detecting discussions of health are impor-tant as indicators of under-reported health eventsin daily lives as well as for studying healthy so-cial support networks.
The detection of medicalevents such as higher levels of pain or discom-fort may also be useful in providing timely clin-ical intervention for managing chronic illness andthus promoting healthy independent living amongolder adults.Motivated by this larger goal, we develop andinvestigate techniques for identifying conversa-tions containing any health related discussion.
Weare interested in detecting discussions about med-ication with doctors, as well as conversations withothers, where among all different topics being dis-cussed, subjects may also be complaining aboutpain or changes in health status.The privacy concerns of recording and analyz-ing everyday telephone conversation prevents usfrom manually transcribing and annotating con-versations.
So, we automatically transcribe theconversations using an automatic speech recog-nition system and look-up the telephone numbercorresponding to each conversation as a heuristicmeans of deriving labels.
This technique is suit-able for labeling a small subset of the conversa-tions that are only sufficient for developing semi-supervised algorithms and for evaluating the meth-ods for analysis.Before delving into our approach, we discussa few relevant and related studies in Section 2and describe our unique naturalistic corpus in Sec-tion 3.
Given the restrictive nature of our labeledin-domain data set, we are interested in a clas-sifier that generalizes to the unlabeled data.
Weevaluate the generalizability of the classifiers us-ing an out-of-domain corpus.
We adopt a semi-supervised topic modeling approach to addressour task, and develop an iterative feature selec-tion method to improve our classifier, as describedin Section 4.
We evaluate the efficacy of our ap-proach empirically, on the in-domain as well as anout-of-domain corpus, and report results in Sec-tion 5.2 Related WorkThe task of identifying conversations where healthis mentioned differs from many other tasks in topic38modeling because in this task we are interested inone particular topic.
A similar study is the work ofPrier and colleagues (Prier et al., 2011).
They usea set of predefined seed words as queries to gathertweets related to tobacco or marijuana usage, andthen use LDA to discover related subtopics.
Thus,their method is sensitive to the seed words chosen.One way to reduce the sensitivity to the manu-ally specified seed words is to expand the set us-ing WordNet.
Researchers have investigated thisapproach in sentiment analysis (Kim and Hovy,2004; Yu and Hatzivassiloglou, 2003).
However,when expanding the seed word set using WordNet,we need to be careful to avoid antonyms and wordsthat have high degree of linkage with many wordsin the vocabulary.
Furthermore, we can not ap-ply such an approach for languages with poor re-sources, where manually curated knowledge is un-available.
The other drawback of this approach isthat we can not use characteristics of the end task,in our case health-related conversation retrieval,to select the words.
As an alternative method,Han and colleagues developed an interactive sys-tem where users selected the most relevant wordsfrom a set, proposed by an automated system (Hanet al., 2009).Another idea for expanding the seed words isusing the statistical information.
Among statis-tical methods, the simplest approach is to com-pute pairwise co-occurrence with the seed words.Li and Yamanishi ranked the words co-occurringwith the seed words according to information the-oretic costs, and used the highest ranked words asthe expanded set (Li and Yamanishi, 2003).
Thisidea can be more effective when the co-occurrenceis performed over subsets instead, as in Hisamitsuand Niwa?s work (Hisamitsu and Niwa, 2001).However, it is computationally expensive to searchover subsets of words.
Depending on the languageand task, heuristics might be applicable.
An ex-ample of this kind of approach is Zagibalov andCarroll?s work on sentiment analysis in Chinese(Zagibalov and Carroll, 2008).Alternatively, we can treat the task of identify-ing words associated with seed words as a cluster-ing problem with the intuition that the seed wordsare in the same cluster.
An effective strategy tocluster words into topics, is Latent Dirichlet Allo-cation (LDA) (Blei et al., 2003) .
However, LDAis an unsupervised algorithm and the clustered top-ics are not guaranteed to include the topic of inter-est.
The Seeded LDA, a variant of LDA, attemptsto address this problem by incorporating the seedwords as priors over the topics (Jagarlamudi etal., 2012).
However, the estimation procedure ismore complicated.
Alternatively, in Topic LDA(TLDA), a clever extension to LDA, Andrzejewskiand Zhu address this problem by fixing the mem-bership of the words to valid topics (Andrzejewskiand Zhu, 2009).
When the focus is on detectingjust one topic, as in our task, we can expand theseed words more selectively using the small set oflabeled data and that is the approach adopted inthis paper.3 DataOne interesting aspect of our study is the unique-ness of our corpus, which is both naturalistic andexhaustive.
We recorded about 41,000 land-lineeveryday telephone conversations from 56 volun-teers, 65 years or older, over a period of approxi-mately 6 to 12 months.
Since these everyday tele-phone conversations are private conversations, andmight include private information such as names,telephone numbers, or banking information, weassured the subjects that no one would listen to therecorded conversations.
Thus, we couldn?t manu-ally transcribe the conversations; instead, we usedan Automatic Speech Recognition (ASR) systemthat we describe here.Automatic Speech Recognition System Con-versations in our corpus were automatically tran-scribed using an ASR system, which is structuredafter IBM?s conversation telephony system (Soltauet al., 2005).
The acoustic models were trainedon about 2000 hours of telephone speech fromSwitchboard and Fisher corpora (Godfrey et al.,1992).
The system has a vocabulary of 47Kand uses a trigram language model with about10M n-grams, estimated from a mix of transcriptsand web-harvested data.
Decoding is performedin three stages using speaker-independent mod-els, vocal-tract normalized models and speaker-adapted models.
The three sets of models are sim-ilar in complexity with 4000 clustered pentaphonestates and 150K Gaussians with diagonal covari-ances.
Our system does not include discriminativetraining and performs at a word error rate of about24% on NIST RT Dev04 which is comparable tostate of the art performance for such systems.
Weare unable to measure the performance of this rec-ognizer on our corpus due to the stringent privacy39requirements mentioned earlier.
Since both cor-pora are conversational telephone speech and thetraining data contains large number of conversa-tions (2000 hours), we expect the performance ofour recognizer to be relatively close to results onNIST benchmark.Heuristically labeling a small subset of conver-sations For training and evaluation purposes, weneed a labeled set of conversations; that is, a setof conversations where we know whether or notthey contain health-related discussions.
Since theprivacy concerns do not allow for manually label-ing the conversations, we used reverse look-up ser-vice in www.whitepages.com.
We sent thephone number corresponding to each conversation(when available) to this website to obtain informa-tion about the other end of the conversation.
Basedof the information we got back from this web-site, we labeled a small subset of the conversationswhich fell into unambiguous business categories.For example, we labeled the calls to ?hospital?
and?pharmacy?
as health-related, and those to ?car re-pair?
and ?real estate?
as non-health-related.The limitations of the labeled set The labeledset we obtained is small and restricted in type ofconversations.
Since phone numbers are not avail-able for many of the conversations we recorded,and also because www.whitepages.com doesnot return unambiguous information for many ofavailable phone numbers, we managed to labelonly 681 conversations ?
275 health-related and406 non-health-related.
This labeled set has an-other limitation: it contains conversations to busi-ness numbers only.
In reality however, we are in-terested in the much larger set of conversationsbetween friends, relatives, and other members ofsubjects?
social support network.
Thus, the gener-alizability of the classifier we train is very impor-tant.Fisher Corpus To explicitly test the generaliz-ability of our classifier, we use a second evaluationset from Fisher corpus (Cieri et al., 2004).
Fishercorpus contains telephone conversations with pre-assigned topics.
There are 40 topics and onlyone of them, illness, is health-related.
We identi-fied 338 conversations on illness, and sampled 702conversations from the other 39 non-health topics.Since we do not train on Fisher corpus, we callit the out-of-domain task to apply our method onFisher corpus; as opposed to the in-domain taskwhich is to apply our method on the everyday tele-phone conversations.Extra information on subjects?
health In theeveryday telephone conversations corpus, we alsohave access to the subjects?
weekly self-reportson their medical status during the week indicatingmedical events such as injury or going to emer-gency room.
We will use these pieces of infor-mation to relate the health-related conversations toactual medical events in the subjects?
lives.4 Method4.1 OverviewAs we explained in Section 3, we can label a smallset of conversations in the everyday telephone con-versations corpus as health-related vs. non-healthrelated.
Using this labeled set we can train a sup-port vector machine (SVM) to classify the con-versations.
In absence of feature selection, theconversations are represented by a vector of tf-idfscores for every word in the vocabulary where tf-idf is a score for measuring the importance of aword in one document of a corpus.
As we see inSection 5, such a classifier doesn?t generalize tothe out-of-domain Fisher task (i.e.
when we testthe classifier on Fisher data set, we do not get goodprecision and recalls).
Generalizability is impor-tant in our case, especially because the data we usefor training is limited in number and the nature ofconversations.One way to improve generalization is to per-form feature selection.
That is, instead of usingtf-idf scores for the whole vocabulary, we wouldlike to rely only on features relevant to detectingthe health topic.
We propose a new way for featureselection for retrieving documents containing in-formation about a specific topic when there is onlya limited set of labeled documents available.
Theidea is to pick a few words highly related to thetopic of interest as seed words and to use TLDA(Andrzejewski and Zhu, 2009) to force those seedwords into one (for example, the first) topic.
In ourtask, the topic of interest is health.
So, we choosedoctor, medicine, and pain ?
often used while dis-cussing health ?
as our seed words.
Topics in LDAbased methods such as TLDA are usually repre-sented using the n most probable words; where nis an arbitrary number.
So, the first candidate setsfor expanding our seed words are the sets of 50most probable words in the topic of health in dif-40ferent runs of TLDA.
As our experiments reveal,these candidate sets contain many words that areunrelated to health .
To solve this problem, we usethe small labeled set of conversations to filter outthe unrelated words.Figure 1 shows the proposed iterative algo-rithm.
The algorithm starts with initializing theseed words to doctor, medicine, and pain.
Then,in each iteration, TLDA performs semi-supervisedtopic modeling and returns the 50 most probablecandidate words in the health topic.
We select asubset of these candidate words which, if addedto the seed words, would maximize the average ofprecision and recall on the train set for a simpleclassifier.
This simple classifier marks a conver-sion as health related if, and only if, it contains atleast one of the seed words.
The algorithm termi-nates when the subset selection is unable to adda new word contributing to the average of preci-sion and recall.
The tf-idf vector for the expandedset represents the conversations in the classifica-tion process.It is worth mentioning that we train TLDA usingall 41000 unlabeled conversations, and chose thenumber of topics, K, to be 20.5 ExperimentsIn all of our experiments, we trained SVMclassifiers, with different features, to detect theconversations on health using the popular lib-SVM (Chang and Lin, 2011) implementation.
Wechose the parameters of the SVM using a 30-foldcross-validated (CV) grid search over the trainingdata.
We also used a 4-fold cross validation overthe labeled set of conversations to maximize theuse of the relatively small labeled set.
That is, wetrained the feature selection algorithm on 3-foldsand tested the resulting SVM tested on the fourth.In in-domain task we always report the averageperformance across the folds.Table 1 shows the results of our experimentsusing different input features.
We report on re-call, precision and F-measure in in-domain andout-of-domain (Fisher) task as well as on averageF-measure of the two.
The justification for consid-ering the average F-measure is that we want ouralgorithm to work well on both in-domain corpusand Fisher corpus since we need to make sure thatour classifier is generalizable (i.e.
it works well onFisher) and it works well on the private and natu-ral telephone conversations (i.e.
the ones similarFigure 1: Expanding the set of seed words: in eachiteration, the current seed words are forced intothe topic of health to guide TLDA towards findingmore health related words.
The candidate set con-sists of the 50 most probable words of the topic ofhealth in TLDA.
We investigate the gain of addingeach word of the candidate set to the seed words bytemporarily adding it to the seed words and look-ing at the average of precision and recall on thetraining set for a classifier that classifies a conver-sation as health-related if and only if it contains atleast one of the seed words.
We select the wordsthat maximize this objective and add them to theseed words until no other words contributes to theaverage precision and recall.to the in-domain corpus)When using the full vocabulary, the in-domainperformance (the performance on the everydaytelephone conversations data) is relatively goodwith 75.1% recall and 83.5% precision.
But theout-of-domain recall (recall on the Fisher data set)is considerably low at 2.8%.
Ideally, we wanta classifier that performs well in both domains.Rows 2 to 5 can be seen as steps to get to sucha classifier.The second row shows the performance of theother extreme end of feature selection: the fea-tures include the manually chosen words doctor,medicine, and pain only.
While this leads to verygood out-of-domain performance, the in-domainrecall has dropped considerably.
We trainedTLDA 30 times, and selected the 50 most probablewords in the health topic.
The third row in Table 1shows the average performance of SVM when us-ing the tf-idf of these sets of words as the featurevector on in-domain and out-of-domain tasks.
Us-ing the 50 most probable words in health topic sig-nificantly improves average F-score (71%) across41Recall Precision F-measureFeature Words In-Domain Fisher In-Domain Fisher In-Domain Fisher AverageFull vocabulary(no feature selection) 75.2 2.8 83.5 91.1 79.1 5.4 42.3Initial words(doctor, medicine, pain ) 45.1 69.2 94.8 94.5 61.1 79.9 70.550 most probable words in health(average over 30 runs) 58.4 57.4 86.3 97.5 69.7 72.3 71.0Words selected by our method(average over 30 runs) 56.1 66.5 91.0 95.5 69.4 78.4 73.9Union of all selected words(across 30 runs) 67.7 69.4 87.8 95.1 76.5 80.2 78.3Table 1: Performance of SVM classifiers using different feature selection methods.
The In-Domain taskinvolves the everyday telephone conversations corpus.
We call Fisher corpus out of domain, because noexample of this corpus was used in training.both tasks over using the full vocabulary (42.3%)but it is clear that this is only due to improvementin out-of-domain task.
Table 2 shows one set ofthe 50 most probable words in health topic,the re-sult of one run of TLDA.
Evidently, these wordscontain many irrelevant words.
This is the motiva-tion for our iterative algorithm.Next, we evaluate the performance of our iter-ative algorithm.
The fourth row in Table 1 showsthe average performance of SVM using expandedseed words that our algorithm suggested in 30runs.
Our algorithm improves the average F-scoreby 3% comparing to the standard TLDA.
This isdue to a 5% improvement in out-of-domain taskas opposed to a 0.3% performance decrease in in-domain task.Since our algorithm has a probabilistic topicmodeling component (i.e.
TLDA), different runslead to different sets of expanded seed words.
Weextract a union of all the words chosen over 30runs and evaluate the performance of SVM usingthis union set.
This improves the performance ofour method further to achieve the best average F-score of 78.3%, which is an 85% improvementover using the SVM with full vocabulary.
It is im-portant to notice that the in-domain performance isstill lower than the full-vocabulary baseline by lessthan 3% while the out-of-domain performance isthe best obtained.
Once again, we are more inter-ested in the average F-measure because we needour algorithm to generalize well (work well onout-of-domain corpus) and to work well on naturalprivate conversations (on the conversations similarto the on-domain corpus).Our last experiment tests statistical associa-tion between health-related discussions in every-day telephone conversations, and actual medicalpain, medicine, appointment, medical, doc-tors, emergency, prescription, contact, med-ication, dial, insurance, pharmacy, schedule,moment, reached, questions, services, surgery,telephone, record, appointments, options, ad-dress, patient, advice, quality, tuesday, posi-tion, answered, records, wednesday, therapy,healthy, correct, department, ensure, numbers,act, doctor, personal, test, senior, nurse, plan,kaiserTable 2: 50 most probable words in the topic ofhealth returned by one run of TLDA.
The boldwords are the ones are hand-picked.events in older adults.
As mentioned in Section 3,we have access to weekly self-reports on medicalevents for subjects?
in everyday telephone conver-sations corpus.
We used our best classifier, theSVM with union of expanded seed words, to clas-sify all the conversations in our corpus into health-containing and health-free conversations.
We thenmark each conversation as temporally near a med-ical event if a reported medical event occurredwithin a 3-week time window.
We chose a 3-weekwindow to allow for one report before and afterthe event.Table 3 shows the number of conversations indifferent categories.
At first glance it might seemlike the number of false positives or false nega-tives is quite large but we should notice that be-ing near a medical event is not the ground truthhere.
We just want to see if there is any associa-tion between occurrence of health-related conver-sations and occurrence of an actual medical eventin lives of our subjects.
We can see that 90.9%42of the conversations are classified as health-relatedbut this percentage is slightly different for con-versations near medical events(91.5%) vs. for theother conversations (89.1).
This slight differenceis significant according to ?2test of independence(?2(df = 1, N = 47288) = 61.17, p < 0.001).near a Classified asmedical event health-related non-health-relatedyes 1348 11067no 2964 31909Table 3: Number of telephone conversations indifferent categories.
Each conversation is consid-ered near a medical even if and only if there isat least one self-report in a window of 3 weeksaround its date.
Being near a medical event doesnot reveal the true nature of the conversation andthus is not the ground truth.
So, there are no falsepositive, true positive, etc.
in this table.6 ConclusionsIn this paper, we investigated the problem of iden-tifying conversations with any mention of health.The private nature of our everyday telephone con-versations corpus poses constraints on manualtranscription and annotation.
Looking up phonenumbers associated with business calls, we labeleda small set of conversations when the other endwas a business clearly related or unrelated to thehealth industry.
However, the labeled set is notlarge enough for training a robust classifier.
Wedeveloped a semi-supervised iterative method forselecting features, where we learn a distributionof words on health topic using TLDA, and sub-sequently filter irrelevant words iteratively.
Wedemonstrate that our method generalizes well andimproves the average F-score on in-domain andout-of-domain tasks over two baselines, using fullvocabulary without feature selection or feature se-lection using TLDA alone.
In our task, the gener-alization of the classifier is important since we areinterested in detecting not only conversations onhealth with business (the annotated examples) butalso with others in subjects?
social network.
Usingour classifier, we find a significant statistical as-sociation between the occurrence of conversationsabout health and the occurrence of self-reportedmedical events.AcknowledgmentsThis research was supported in part by NIH Grants1K25AG033723, and P30 AG008017, as well asby NSF Grants 1027834, and 0964102.
Any opin-ions, findings, conclusions or recommendationsexpressed in this publication are those of the au-thors and do not necessarily reflect the views of theNIH.
We thank Nicole Larimer for help in collect-ing the data, Maider Lehr for testing the data col-lection devices and Katherine Wild for early dis-cussions on this project.
We are grateful to BrianKingsbury and his colleagues for providing us ac-cess to IBM?s attila software tools.ReferencesDavid Andrzejewski and Xiaojin Zhu.
2009.
La-tent dirichlet allocation with topic-in-set knowledge.In Proceedings of the NAACL HLT 2009 Workshopon Semi-Supervised Learning for Natural LanguageProcessing, SemiSupLearn ?09, pages 43?48.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet allocation.
Journal of Ma-chine Learning Research, 3:993?1022.C.-C. Chang and C.-J.
Lin.
2011.
Libsvm : a libraryfor support vector machines.
ACM Transactions onIntelligent Systems and Technology, 2.Christopher Cieri, David Miller, and Kevin Walker.2004.
The fisher corpus: a resource for the nextgenerations of speech-to-text.
In LREC, volume 4,pages 69?71.John J Godfrey, Edward C Holliman, and Jane Mc-Daniel.
1992.
Switchboard: Telephone speech cor-pus for research and development.
In Acoustics,Speech, and Signal Processing, 1992.
ICASSP-92.,1992 IEEE International Conference on, volume 1,pages 517?520.
IEEE.Hong-qi Han, Dong-Hua Zhu, and Xue-feng Wang.2009.
Semi-supervised text classification from un-labeled documents using class associated words.
InComputers & Industrial Engineering, 2009.
CIE2009.
International Conference on, pages 1255?1260.
IEEE.Toru Hisamitsu and Yoshiki Niwa.
2001.
Topic-wordselection based on combinatorial probability.
In NL-PRS, volume 1, page 289.Jagadeesh Jagarlamudi, Hal Daum?e III, and Raghaven-dra Udupa.
2012.
Incorporating lexical priors intotopic models.
In Proceedings of the 13th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 204?213.
Associ-ation for Computational Linguistics.43Soo-Min Kim and Eduard Hovy.
2004.
Determin-ing the sentiment of opinions.
In Proceedings ofthe 20th international conference on ComputationalLinguistics, page 1367.
Association for Computa-tional Linguistics.Hang Li and Kenji Yamanishi.
2003.
Topic analysisusing a finite mixture model.
Information process-ing & management, 39(4):521?541.Kyle W. Prier, Matthew S. Smith, Christophe Giraud-Carrier, and Carl L. Hanson.
2011.
Identifyinghealth-related topics on twitter: an exploration oftobacco-related tweets as a test topic.
In Proceed-ings of the 4th international conference on Socialcomputing, behavioral-cultural modeling and pre-diction, pages 18?25.Hagen Soltau, Brian Kingsbury, Lidia Mangu, DanielPovey, George Saon, and Geoffrey Zweig.
2005.The ibm 2004 conversational telephony system forrich transcription.
In Proc.
ICASSP, volume 1,pages 205?208.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separating factsfrom opinions and identifying the polarity of opinionsentences.
In Proceedings of the 2003 conference onEmpirical methods in natural language processing,pages 129?136.
Association for Computational Lin-guistics.Taras Zagibalov and John Carroll.
2008.
Automaticseed word selection for unsupervised sentiment clas-sification of chinese text.
In Proceedings of the22nd International Conference on ComputationalLinguistics-Volume 1, pages 1073?1080.
Associa-tion for Computational Linguistics.44
