Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 895?904,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsUnsupervised Bilingual Morpheme Segmentation and Alignment withContext-rich Hidden Semi-Markov ModelsJason Naradowsky?Department of Computer ScienceUniversity of Massachusetts AmherstAmherst, MA 01003narad@cs.umass.eduKristina ToutanovaMicrosoft ResearchRedmond, WA 98502kristout@microsoft.comAbstractThis paper describes an unsupervised dynamicgraphical model for morphological segmen-tation and bilingual morpheme alignment forstatistical machine translation.
The model ex-tends Hidden Semi-Markov chain models byusing factored output nodes and special struc-tures for its conditional probability distribu-tions.
It relies on morpho-syntactic and lex-ical source-side information (part-of-speech,morphological segmentation) while learning amorpheme segmentation over the target lan-guage.
Our model outperforms a competi-tive word alignment system in alignment qual-ity.
Used in a monolingual morphological seg-mentation setting it substantially improves ac-curacy over previous state-of-the-art modelson three Arabic and Hebrew datasets.1 IntroductionAn enduring problem in statistical machine trans-lation is sparsity.
The word alignment models ofmodern MT systems attempt to capture p(ei|fj),the probability that token ei is a translation of fj .Underlying these models is the assumption that theword-based tokenization of each sentence is, if notoptimal, at least appropriate for specifying a concep-tual mapping between the two languages.However, when translating between unrelated lan-guages ?
a common task ?
disparate morphologicalsystems can place an asymmetric conceptual bur-den on words, making the lexicon of one languagemuch more coarse.
This intensifies the problem ofsparsity as the large number of word forms created?This research was conducted during the author?s internshipat Microsoft Researchthrough morphologically productive processes hin-ders attempts to find concise mappings between con-cepts.For instance, Bulgarian adjectives may containmarkings for gender, number, and definiteness.
Thefollowing tree illustrates nine realized forms of theBulgarian word for red, with each leaf listing thedefinite and indefinite markings.Feminine NeuterSingular PluralRootMasculinecherven(iq)(iqt) cherveni(te)chervena(ta) cherveno(to)Table 1: Bulgarian forms of redContrast this with English, in which this informa-tion is marked either on the modified word or by sep-arate function words.In comparison to a language which isn?t mor-phologically productive on adjectives, the alignmentmodel must observe nine times as much data (as-suming uniform distribution of the inflected forms)to yield a comparable statistic.
In an area of researchwhere the amount of data available plays a large rolein a system?s overall performance, this sparsity canbe extremely problematic.
Further complications arecreated when lexical sparsity is compounded withthe desire to build up alignments over increasinglylarger contiguous phrases.To address this issue we propose an alternativeto word alignment: morpheme alignment, an align-ment that operates over the smallest meaningful sub-sequences of words.
By striving to keep a direct 1-to-1 mapping between corresponding semantic unitsacross languages, we hope to find better estimates895?
?the red flowercherven tsvetDET ADJ NNiaisthey want toh nA^PRN VB INFdyrsr~d yteach himVB PRNnw y??????
????
??
???
??
???????
?
?teFigure 1: A depiction of morpheme-level alignment.
Here dark lines indicate the more stem-focused alignmentstrategy of a traditional word or phrasal alignment model, while thin lines indicate a more fine-grained alignmentacross morphemes.
In the alignment between English and Bulgarian (a) the morpheme-specific alignment reducessparsity in the adjective and noun (red flowers) by isolating the stems from their inflected forms.
Despite Arabicexhibiting templatic morphology, there are still phenomena which can be accounted for with a simpler segmentationalapproach.
The Arabic alignment (b) demonstrates how the plural marker on English they would normally createsparsity by being marked in three additional places, two of them inflections in larger wordforms.for the alignment statistics.
Our results show thatthis improves alignment quality.In the following sections we describe an un-supervised dynamic graphical model approach tomonolingual morphological segmentation and bilin-gual morpheme alignment using a linguistically mo-tivated statistical model.
In a bilingual setting,the model relies on morpho-syntactic and lexicalsource-side information (part-of-speech, morpho-logical segmentation, dependency analysis) whilelearning a morpheme segmentation over the tar-get language.
In a monolingual setting we intro-duce effective use of context by feature-rich mod-eling of the probabilities of morphemes, morpheme-transitions, and word boundaries.
These additionalsources of information provide powerful bias for un-supervised learning, without increasing the asymp-totic running time of the inference algorithm.Used as a monolingual model, our system sig-nificantly improves the state-of-the-art segmenta-tion performance on three Arabic and Hebrew data-sets.
Used as a bilingual model, our system out-performs the state-of-the-art WDHMM (He, 2007)word alignment model as measured by alignment er-ror rate (AER).In agreement with some previous work on to-kenization/morpheme segmentation for alignment(Chung and Gildea, 2009; Habash and Sadat, 2006),we find that the best segmentation for alignmentdoes not coincide with the gold-standard segmenta-tion and our bilingual model does not outperformour monolingual model in segmentation F-Measure.2 ModelOur model defines the probability of a target lan-guage sequence of words (each consisting of a se-quence of morphemes), and alignment from targetto source morphemes, given a source language se-quence of words (each consisting of a sequence ofmorphemes).An example morpheme segmentation and align-ment of phrases in English-Arabic and English-Bulgarian is shown in Figure 1.
In our task setting,the words of the source and target language as wellas the morpheme segmentation of the source (En-glish) language are given.
The morpheme segmen-tation of the target language and the alignments be-tween source and target morphemes are hidden.The source-side input, which we assume to beEnglish, is processed with a gold morphologicalsegmentation, part-of-speech, and dependency treeanalysis.
While these tools are unavailable inresource-poor languages, they are often available forat least one of the modeled languages in commontranslation tasks.
This additional information thenprovides a source of features and conditioning infor-mation for the translation model.Our model is derived from the hidden-markovmodel for word alignment (Vogel et al, 1996; Ochand Ney, 2000).
Based on it, we define a dynamic896cherven.i.teflowerthe red= 'cherven'?1= 2a1= OFFb1= OFFb2= ONb3= 'i'?2= 'te'?3= 4 = 1a2 a3s= stemt1= suffix = suffixt2 t3Figure 2: A graphical depiction of the model generatingthe transliteration of the first Bulgarian word from Figure1.
Trigram dependencies and some incoming/outgoingarcs have been omitted for clarity.graphical model which lets us encode more lin-guistic intuition about morpheme segmentation andalignment: (i) we extend it to a hidden semi-markovmodel to account for hidden target morpheme seg-mentation; (ii) we introduce an additional observa-tion layer to model observed word boundaries andthus truly represent target sentences as words com-posed of morphemes, instead of just a sequenceof tokens; (iii) we employ hierarchically smoothedmodels and log-linear models to capture broadercontext and to better represent the morpho-syntacticmapping between source and target languages.
(iv)we enrich the hidden state space of the model to en-code morpheme types {prefix,suffix,stem}, in ad-dition to morpheme alignment and segmentation in-formation.Before defining our model formally, we introducesome notation.
Each possible morphological seg-mentation and alignment for a given sentence paircan be described by the following random variables:Let ?1?2 .
.
.
?I denote I morphemes in the seg-mentation of the target sentence.
For the Examplein Figure 1 (a) I=5 and ?1=cherven, ?2=i .
.
.
, and?5=ia.
Let b1, b2, .
.
.
bI denote Bernoulli variablesindicating whether there is a word boundary aftermorpheme ?i.
For our example, b3 = 1, b5 = 1,and the other bi are 0.
Let c1, c2, .
.
.
, cT denotethe non-space characters in the target string, andwb1, .
.
.
, wbT denote Bernoulli variables indicatingwhether there is a word boundary after the corre-sponding target character.
For our example, T = 14(for the Cyrillic version) and the only wb variablesthat are on are wb9 and wb14.
The c and wb vari-ables are observed.
Let s1s2 .
.
.
sT denote Bernoullisegmentation variables indicating whether there is amorpheme boundary after the corresponding char-acter.
The values of the hidden segmentation vari-ables s together with the values of the observed cand wb variables uniquely define the values of themorpheme variables ?i and the word boundary vari-ables bi.
Naturally we enforce the constraint thata given word boundary wbt = 1 entails a segmen-tation boundary st = 1.
If we use bold lettersto indicate a vector of corresponding variables, wehave that c,wb, s=?,b.
We will define the assumedparametric form of the learned distribution using the?,b but the inference algorithms are implementedin terms of the s and wb variables.We denote the observed source language mor-phemes by e1 .
.
.
eJ .
Our model makes use of ad-ditional information from the source which we willmention when necessary.The last part of the hidden model state repre-sents the alignment between target and source mor-phemes and the type of target morphemes.
Lettai = [ai, ti], i = 1 .
.
.
I indicate a factored statewhere ai represents one of the J source words (orNULL) and ti represents one of the three morphemetypes {prefix,suffix,stem}.
ai is the source mor-pheme aligned to ?i and ti is the type of ?i.We are finally ready to define the desired proba-bility of target morphemes, morpheme types, align-ments, and word boundaries given source:P (?, ta,b|e) =I?i=1PT (?i|tai, bi?1, bi?2, ?i?1, e)?
PB(bi|?i, ?i?1, tai, bi?1, bi?2, e)?
PD(tai|tai?1, bi?1, e) ?
LP (|?i|)We now describe each of the factors used by ourmodel in more detail.
The formulation makes ex-plicit the full extent of dependencies we have ex-plored in this work.
By simplifying the factors897we can recover several previously used models formonolingual segmentation and bilingual joint seg-mentation and alignment.
We discuss the relation-ship of this model to prior work and study the impactof the novel components in our experiments.When the source sentence is assumed to be empty(and thus contains no morphemes to align to) ourmodel turns into a monolingual morpheme segmen-tation model, which we show exceeds the perfor-mance of previous state-of-the-art models.
When weremove the word boundary component, reduce theorder of the alignment transition, omit the morpho-logical type component of the state space, and retainonly minimal dependencies in the morpheme trans-lation model, we recover the joint tokenization andalignment model based on IBM Model-1 proposedby (Chung and Gildea, 2009).2.1 Morpheme Translation ModelIn the model equation, PT denotes the morphemetranslation probability.
The standard dependence onthe aligned source morpheme is represented as a de-pendence on the state tai and the whole annotatedsource sentence e. We experimented with multipleoptions for the amount of conditioning context to beincluded.
When most context is used, there is a bi-gram dependency of target language morphemes aswell as dependence on two previous boundary vari-ables and dependence on the aligned source mor-pheme eai as well as its POS tag.When multiple conditioning variables are used weassume a special linearly interpolated backoff formof the model, similar to models routinely used in lan-guage modeling.As an example, suppose we estimate the mor-pheme translation probability as PT (?i|eai , ti).
Weestimate this in the M-step, given expected jointcounts c(?i, eai , ti) and marginal counts derivedfrom these as follows:PT (?i|eai , ti) =c(?i,eai ,ti)+?2P2(?i|ti)c(eai ,ti)+?2The lower order distributions are estimated recur-sively in a similar way:P2(?i|ti) =c(?i,ti)+?1P1(?i)c(ti)+?1P1(?i) =c(?i)+?0P0(?i)c(.
)+?0For P0 we used a unigram character languagemodel.
This hierarchical smoothing can be seenas an approximation to hierarchical Dirichlet priorswith maximum aposteriori estimation.Note how our explicit treatment of word bound-ary variables bi allows us to use a higher order de-pendence on these variables.
If word boundaries aretreated as morphemes on their own, we would needto have a four-gram model on target morphemes torepresent this dependency which we are now repre-senting using only a bigram model on hidden mor-phemes.2.2 Word Boundary Generation ModelThe PB distribution denotes the probability of gen-erating word boundaries.
As a sequence model ofsentences the basic hidden semi-markov model com-pletely ignores word boundaries.
However, they canbe powerful predictors of morpheme segments (byfor example, indicating that common prefixes fol-low word boundaries, or that common suffixes pre-cede them).
The log-linear model of (Poon et al,2009) uses word boundaries as observed left andright context features, and Morfessor (Creutz andLagus, 2007) includes boundaries as special bound-ary symbols which can inform about the morphemestate of a morpheme (but not its identity).Our model includes a special generative processfor boundaries which is conditioned not only on theprevious morpheme state but also the previous twomorphemes and other boundaries.
Due to the factthat boundaries are observed their inclusion in themodel does not increase the complexity of inference.The inclusion of this distribution lets us estimatethe likelihood of a word consisting of one,two,three,or more morphemes.
It also allows the estimation oflikelihood that particular morphemes are in the be-ginning/middle/end of words.
Through the includedfactored state variable tai word boundaries can alsoinform about the likelihood of a morpheme alignedto a source word of a particular pos tag to end aword.
We discuss the particular conditioning con-text for this distribution we found most helpful inour experiments.Similarly to the PT distribution, we make use ofmultiple context vectors by hierarchical smoothingof distributions of different granularities.8982.3 Distortion ModelPD indicates the distortion modeling distributionwe use.
1 Traditional distortion models representP (aj |aj?1, e), the probability of an alignment giventhe previous alignment, to bias the model away fromplacing large distances between the aligned tokensof consecutively sequenced tokens.
In addition tomodeling a larger state space to also predict mor-pheme types, we extend this model by using a spe-cial log-linear model form which allows the integra-tion of rich morpho-syntactic context.
Log-linearmodels have been previously used in unsupervisedlearning for local multinomial distributions like thisone in e.g.
(Berg-Kirkpatrick et al, 2010), and forglobal distributions in (Poon et al, 2009).The special log-linear form allows the inclusionof features targeted at learning the transitions amongmorpheme types and the transitions between corre-sponding source morphemes.
The set of featureswith example values for this model is depicted inTable 3.
The example is focussed on the featuresfiring for the transition from the Bulgarian suffixte aligned to the first English morpheme ?i?1 =te, ti?1=suffix, ai?1=1, to the Bulgarian root tsvetaligned to the third English morpheme ?i = tsvet,ti=root, ai=3.
The first feature is the absolute dif-ference between ai and ai?1 + 1 and is similar toinformation used in other HMM word alignmentmodels (Och and Ney, 2000) as well as phrase-translation models (Koehn, 2004).
The alignmentpositions ai are defined as indices of the alignedsource morphemes.
We additionally compute distor-tion in terms of distance in number of source wordsthat are skipped.
This distance corresponds to thefeature name WORD DISTANCE.
Looking at bothkinds of distance is useful to capture the intuitionthat consecutive morphemes in the same target wordshould prefer to have a higher proximity of theiraligned source words, as compared to consecutivemorphemes which are not part of the same targetword.
The binned distances look at the sign of thedistortion and bin the jumps into 5 bins, pooling thedistances greater than 2 together.
The feature SAMETARGET WORD indicates whether the two consecu-1To reduce complexity of exposition we have omitted thefinal transition to a special state beyond the source sentence endafter the last target morpheme.Feature ValueMORPH DISTANCE 1WORD DISTANCE 1BINNED MORPH DISTANCE fore1BINNED WORD DISTANCE fore1MORPH STATE TRANSITION suffix-rootSAME TARGET WORD FalsePOS TAG TRANSITION DET-NNDEP RELATION DET?NNNULL ALIGNMENT Falseconjunctions ...Figure 3: Features in log-linear distortion model firingfor the transition from te:suffix:1 to tsvet:root:3 in theexample sentence pair in Figure 1a.tive morphemes are part of the same word.
In thiscase, they are not.
This feature is not useful on itsown because it does not distinguish between differ-ent alignment possibilities for tai, but is useful inconjunction with other features to differentiate thetransition behaviors within and across target words.The DEP RELATION feature indicates the direct de-pendency relation between the source words con-taining the aligned source morphemes, if such rela-tionship exists.
We also represent alignments to nulland have one null for each source word, similarly to(Och and Ney, 2000) and have a feature to indicatenull.
Additionally, we make use of several featureconjunctions involving the null, same target word,and distance features.2.4 Length PenaltyFollowing (Chung and Gildea, 2009) and (Liang andKlein, 2009) we use an exponential length penaltyon morpheme lengths to bias the model away fromthe maximum likelihood under-segmentation solu-tion.
The form of the penalty is:LP (|?i|) = 1e|?i|lpHere lp is a hyper-parameter indicating the powerthat the morpheme length is raised to.
We fit this pa-rameter using an annotated development set, to op-timize morpheme-segmentation F1.
The model isextremely sensitive to this value and performs quitepoorly if such penalty is not used.2.5 InferenceWe perform inference by EM training on the alignedsentence pairs.
In the E-step we compute expected899counts of all hidden variable configurations that arerelevant for our model.
In the M-step we re-estimatethe model parameters (using LBFGS in the M-stepfor the distortion model and using count interpola-tion for the translation and word-boundary models).The computation of expectations in the E-stepis of the same order as an order two semi-markovchain model using hidden state labels of cardinality(J ?
3 = number of source morphemes times num-ber of target morpheme types).
The running timeof the forward and backward dynamic programmingpasses is T ?
l2 ?
(3J)2, where T is the length ofthe target sentence in characters, J is the numberof source morphemes, and l is the maximum mor-pheme length.
Space does not permit the completelisting of the dynamic programming solution but itis not hard to derive by starting from the dynamicprogram for the IBM-1 like tokenization model of(Chung and Gildea, 2009) and extending it to ac-count for the higher order on morphemes and thefactored alignment state space.Even though the inference algorithm is low poly-nomial it is still much more expensive than the infer-ence for an HMM model for word-alignment with-out segmentation.
To reduce the running time of themodel we limit the space of considered morphemeboundaries as follows:Given the target side of the corpus, we derive alist of K most frequent prefixes and suffixes using asimple trie-based method proposed by (Schone andJurafsky, 2000).2 After we determine a list of al-lowed prefixes and suffixes we restrict our model toallow only segmentations of the form : ((p*)r(s*))+where p and s belong to the allowed prefixes andsuffixes and r can match any substring.We determine the number of prefixes and suffixesto consider using the maximum recall achievable bylimiting the segmentation points in this way.
Re-stricting the allowable segmentations in this way notonly improves the speed of inference but also leadsto improvements in segmentation accuracy.2Words are inserted into a trie with each complete branchnaturally identifying a potential suffix, inclusive of its sub-branches.
The list comprises of the K most frequent of thesecomplete branches.
Inserting the reversed words will then yieldpotential prefixes.3 EvaluationFor a majority of our testing we borrow the paral-lel phrases corpus used in previous work (Snyderand Barzilay, 2008), which we refer to as S&B.The corpus consists of 6,139 short phrases drawnfrom English, Hebrew, and Arabic translations ofthe Bible.
We use an unmodified version of thiscorpus for the purpose of comparing morphologicalsegmentation accuracy.
For evaluating morphemealignment accuracy, we have also augmented the En-glish/Arabic subset of the corpus with a gold stan-dard alignment between morphemes.
Here mor-phological segmentations were obtained using thepreviously-annotated gold standard Arabic morpho-logical segmentation, while the English was prepro-cessed with a morphological analyzer and then fur-ther hand annotated with corrections by two nativespeakers.
Morphological alignments were manuallyannotated.
Additionally, we evaluate monolingualsegmentation models on the full Arabic Treebank(ATB), also used for unsupervised morpheme seg-mentation in (Poon et al, 2009).4 Results4.1 Morpheme SegmentationWe begin by evaluating a series of models which aresimplifications of our complete model, to assess theimpact of individual modeling decisions.
We focusfirst on a monolingual setting, where the source sen-tence aligned to each target sentence is empty.Unigram Model with Length PenaltyThe first model we study is the unigram mono-lingual segmentation model using an exponentiallength penalty as proposed by (Liang and Klein,2009; Chung and Gildea, 2009), which has beenshown to be quite accurate.
We refer to this model asModel-UP (for unigram with penalty).
It defines theprobability of a target morpheme sequence as fol-lows: (?1 .
.
.
?I) = (1?
?
)?Ii=1 ?PT (?i)LP (|?i|)This model can be (almost) recovered as a spe-cial case of our full model, if we drop the transitionand word boundary probabilities, do not model mor-pheme types, and use no conditioning for the mor-pheme translation model.
The only parameter notpresent in our model is the probability ?
of gener-ating a morpheme as opposed to stopping to gener-900ate morphemes (with probability 1 ?
?).
We exper-imented with this additional parameter, but found ithad no significant impact on performance, and so wedo not report results including it.We select the value of the length penalty powerby a gird search in the range 1.1 to 2.0, using .1 in-crements and choosing the values resulting in bestperformance on a development set containing 500phrase pairs for each language.
We also select theoptimal number of prefixes/suffixes to consider bymeasuring performance on the development set.
3Morpheme Type ModelsThe next model we consider is similar to the un-igram model with penalty, but introduces the useof the hidden ta states which indicate only mor-pheme types in the monolingual setting.
We usethe ta states and test different configurations to de-rive the best set of features that can be used in thedistortion model utilizing these states, and the mor-pheme translation model.
We consider two vari-ants: (1) Model-HMMP-basic (for HMM modelwith length penalty), which includes the hiddenstates but uses them with a simple uniform transitionmatrix P (tai|tai?1, bi?1) (uniform over allowabletransitions but forbidding the prefixes from transi-tioning directly to suffixes, and preventing suffixesfrom immediately following a word boundary), and(2) a richer model Model-HMMP which is allowedto learn a log-linear distortion model and a featurerich translation model as detailed in the model defi-nition.
This model is allowed to use word boundaryinformation for conditioning (because word bound-aries are observed), but does not include the PB pre-dictive word boundary distribution.Full Model with Word BoundariesFinally we consider our full monolingual modelwhich also includes the distribution predicting wordboundary variables bi.
We term this model Model-FullMono.
We detail the best context features forthe conditional PD distribution for each language.We initialize this model with the morpheme trans-3For the S&B Arabic dataset, we selected to use seven pre-fixes and seven suffixes, which correspond to maximum achiev-able recall of 95.3.
For the S&B Hebrew dataset, we used sixprefixes and six suffixes, for a maximum recall of 94.3.
TheArabic treebank data required a larger number of affixes: weused seven prefixes and 20 suffixes, for a maximum recall of98.3.lation unigram distribution of ModelHMMP-basic,trained for 5 iterations.Table 4 details the test set results of the differentmodel configurations, as well as previously reportedresults on these datasets.
For our main results we usethe automatically derived list of prefixes and suffixesto limit segmentation points.
The names of modelsthat use such limited lists are prefixed by Dict in theTable.
For comparison, we also report the resultsachieved by models that do not limit the segmenta-tion points in this way.As we can see the unigram model with penalty,Dict-Model-UP, is already very strong, especiallyon the S&B Arabic dataset.
When the segmenta-tion points are not limited, its performance is muchworse.
The introduction of hidden morpheme statesin Dict-HMMP-basic gives substantial improvementon Arabic and does not change results much on theother datasets.
A small improvement is observedfor the unconstrained models.4 When our model in-cludes all components except word boundary pre-diction, Dict-Model-HMMP, the results are substan-tially improved on all languages.
Model-HMMP isalso the first unconstrained model in our sequenceto approach or surpass previous state-of-the-art seg-mentation performance.Finally, when the full model Dict-MonoFull isused, we achieve a substantial improvement overthe previous state-of-the-art results on all three cor-pora, a 6.5 point improvement on Arabic, 6.2 pointimprovement on Hebrew, and a 9.3 point improve-ment on ATB.
The best configuration of this modeluses the same distortion model for all languages: us-ing the morph state transition and boundary features.The translation models used only ti for Hebrew andATB and ti and ?i?1 for Arabic.
Word bound-ary was predicted using ti in Arabic and Hebrew,and additionally using bi?1 and bi?2 for ATB.
Theunconstrained models without affix dictionaries arealso very strong, outperforming previous state-of-the-art models.
For ATB, the unconstrained modelslightly outperforms the constrained one.The segmentation errors made by this system shedlight on how it might be improved.
We find the dis-4Note that the inclusion of states in HMMP-basic onlyserves to provide a different distribution over the number ofmorphemes in a word, so it is interesting it can have a positiveimpact.901Arabic Hebrew ATBP R F1 P R F1 P R F1UP 88.1 55.1 67.8 43.2 87.6 57.9 79.0 54.6 64.6Dict-UP 85.8 73.1 78.9 57.0 79.4 66.3 61.6 91.0 73.5HMMP-basic 83.3 58.0 68.4 43.5 87.8 58.2 79.0 54.9 64.8Dict-HMMP-basic 84.8 76.3 80.3 56.9 78.8 66.1 69.3 76.2 72.6HMMP 73.6 76.9 75.2 70.2 73.0 71.6 94.0 76.1 84.1Dict-HMMP 82.4 81.3 81.8 62.7 77.6 69.4 85.2 85.8 85.5MonoFull 80.5 87.3 83.8 72.2 71.7 72.0 86.2 88.5 87.4Dict-MonoFull 86.1 83.2 84.6 73.7 72.5 73.1 92.9 81.8 87.0Poon et.
al 76.0 80.2 78.1 67.6 66.1 66.9 88.5 69.2 77.7S&B-Best 67.8 77.3 72.2 64.9 62.9 63.9 ?
?
?Morfessor 71.1 60.5 65.4 65.4 57.7 61.3 77.4 72.6 74.9Figure 4: Results on morphological segmentation achieved by monolingual variants of our model (top) with resultsfrom prior work are included for comparison (bottom).
Results from models with a small, automatically-derived listof possible prefixes and suffixes are labeled as ?Dict-?
followed by the model name.tributions over the frequencies of particular errorsfollow a Zipfian skew across both S&B datasets,with the Arabic being more pronounced (the mostfrequent error being made 27 times, with 627 er-rors being made just once) in comparison with theHebrew (with the most frequent error being made19 times, and with 856 isolated errors).
However,in both the Arabic and Hebrew S&B tasks we findthat a tendency to over-segment certain charactersoff of their correct morphemes and on to other fre-quently occurring, yet incorrect, particles is actuallythe cause of many of these isolated errors.
In Ara-bic the system tends to over segment the characteraleph (totally about 300 errors combined).
In He-brew the source of error is not as overwhelminglydirected at a single character, but yod and he, thelatter functioning quite similarly to the problematicArabic character and frequently turn up in the corre-sponding places of cognate words in Biblical texts.We should note that our models select a largenumber of hyper-parameters on an annotated devel-opment set, including length penalty, hierarchicalsmoothing parameters ?, and the subset of variablesto use in each of three component sub-models.
Thismight in part explain their advantage over previous-state-of-the-art models, which might use fewer (e.g.
(Poon et al, 2009) and (Snyder and Barzilay, 2008))or no specifically tuned for these datasets hyper-parameters (Morfessor (Creutz and Lagus, 2007)).4.2 AlignmentNext we evaluate our full bilingual model and a sim-pler variant on the task of word alignment.
We usethe morpheme-level annotation of the S&B English-Arabic dataset and project the morpheme alignmentsto word alignments.
We can thus compare align-ment performance of the results of different segmen-tations.
Additionally, we evaluate against a state-of-the-art word alignment system WDHMM (He,2007), which performs comparably or better thanIBM-Model4.
The table in Figure 5 presents the re-sults.
In addition to reporting alignment error ratefor different segmentation models, we report theirmorphological segmentation F1.The word-alignment WDHMM model performsbest when aligning English words to Arabic words(using Arabic as source).
In this direction it isable to capture the many-to-one correspondence be-tween English words and arabic morphemes.
Whenwe combine alignments in both directions using thestandard grow-diag-final method, the error goes up.We compare the (Chung and Gildea, 2009) model(termed Model-1) to our full bilingual model.
Wecan recover Model-1 similarly to Model-UP, exceptnow every morpheme is conditioned on an alignedsource morpheme.
Our full bilingual model outper-forms Model-1 in both AER and segmentation F1.The specific form of the full model was selected asin the previous experiments, by choosing the modelwith best segmentations of the development set.For Arabic, the best model conditions target mor-902Arabic HebrewAlign P Align R AER P R F1 P R F1Model-1 (C&G 09) 91.6 81.2 13.9 72.4 76.2 74.3 61.0 71.8 65.9Bilingual full 91.0 88.3 10.3 90.0 72.0 80.0 63.3 71.2 67.0WDHMM E-to-A 82.4 96.7 11.1WDHMM GDF 82.1 94.6 12.1Figure 5: Alignment Error Rate (AER) and morphological segmentation F1 achieved by bilingual variants of ourmodel.
AER performance of WDHMM is also reported.
Gold standard alignments are not available for the Hebrewdata set.phemes on source morphemes only, uses the bound-ary model with conditioning on number of mor-phemes in the word, aligned source part-of-speech,and type of target morpheme.
The distortion modeluses both morpheme and word-based absolute dis-tortion, binned distortion, morpheme types of states,and aligned source-part-of-speech tags.
Our bestmodel for Arabic outperforms WDHMM in wordalignment error rate.
For Hebrew, the best modeluses a similar boundary model configuration but asimpler uniform transition distortion distribution.Note that the bilingual models perform worse thanthe monolingual ones in segmentation F1.
Thisfinding is in line with previous work showing thatthe best segmentation for MT does not necessarilyagree with a particular linguistic convention aboutwhat morphemes should contain (Chung and Gildea,2009; Habash and Sadat, 2006), but contradictsother results (Snyder and Barzilay, 2008).
Furtherexperimentation is required to make a general claim.We should note that the Arabic dataset usedfor word-alignment evaluation is unconventionallysmall and noisy (the sentences are very shortphrases, automatically extracted using GIZA++).Thus the phrases might not be really translations,and the sentence length is much smaller than in stan-dard parallel corpora.
This warrants further modelevaluation in a large-scale alignment setting.5 Related WorkThis work is most closely related to the unsupervisedtokenization and alignment models of Chung andGildea (2009), Xu et al (2008), Snyder and Barzilay(2008), and Nguyen et al (2010).Chung & Gildea (2009) introduce a unigrammodel of tokenization based on IBM Model-1,whichis a special case of our model.
Snyder and Barzi-lay (2008) proposes a hierarchical Bayesian modelthat combines the learning of monolingual segmen-tations and a cross-lingual alignment; their model isvery different from ours.Incorporating morphological information intoMT has received reasonable attention.
For exam-ple, Goldwater & McClosky (2005) show improve-ments when preprocessing Czech input to reflecta morphological decomposition using combinationsof lemmatization, pseudowords, and morphemes.Yeniterzi and Oflazer (2010) bridge the morpholog-ical disparity between languages in a unique wayby effectively aligning English syntactic elements(function words connected by dependency relations)to Turkish morphemes, using rule-based postpro-cessing of standard word alignment.
Our work ispartly inspired by that work and attempts to auto-mate both the morpho-syntactic alignment and mor-phological analysis tasks.6 ConclusionWe have described an unsupervised model for mor-pheme segmentation and alignment based on Hid-den Semi-Markov Models.
Our model makes useof linguistic information to improve alignment qual-ity.
On the task of monolingual morphological seg-mentation it produces a new state-of-the-art level onthree datasets.
The model shows quantitative im-provements in both word segmentation and wordalignment, but its true potential lies in its finer-grained interpretation of word alignment, which willhopefully yield improvements in translation quality.AcknowledgementsWe thank the ACL reviewers for their valuablecomments on earlier versions of this paper, andMichael J. Burling for his contributions as a corpusannotator and to the Arabic aspects of this paper.903ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,John DeNero, and Dan Klein.
2010.
Unsupervisedlearning with features.
In Proceedings of the NorthAmerican chapter of the Association for Computa-tional Linguistics (NAACL).Tagyoung Chung and Daniel Gildea.
2009.
Unsuper-vised tokenization for machine translation.
In Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP).Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Trans.
Speech Lang.
Process.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.
InNorth American Chapter of the Association for Com-putational Linguistics.Xiaodong He.
2007.
Using word-dependent transitionmodels in HMM based word alignment for statisticalmachine translation.
In ACL 2nd Statistical MT work-shop, pages 80?87.Philip Koehn.
2004.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In AMTA.P.
Liang and D. Klein.
2009.
Online EM for unsu-pervised models.
In North American Association forComputational Linguistics (NAACL).ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.2010.
Nonparametric word segmentation for machinetranslation.
In Proceedings of the International Con-ference on Computational Linguistics.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In In Proceedings of the38th Annual Meeting of the Association for Computa-tional Linguistics.Hoifung Poon, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentationwith log-linear models.
In North American Chap-ter of the Association for Computation Linguistics- Human Language Technologies 2009 conference(NAACL/HLT-09).Patrick Schone and Daniel Jurafsky.
2000.
Knowlege-free induction of morphology using latent semanticanalysis.
In Proceedings of the Conference on Compu-tational Natural Language Learning (CoNLL-2000).Benjamin Snyder and Regina Barzilay.
2008.
Unsuper-vised multilingual learning for morphological segmen-tation.
In ACL.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In In COLING 96: The 16th Int.
Conf.
on Com-putational Linguistics.Jia Xu, Jianfeng Gao, Kristina Toutanova, and HermannNey.
2008.
Bayesian semi-supervised chinese wordsegmentation for statistical machine translation.
InCOLING.Reyyan Yeniterzi and Kemal Oflazer.
2010.
Syntax-to-morphology mapping in factored phrase-based statis-tical machine translation from english to turkish.
InProceedings of Association of Computational Linguis-tics.904
