Inverse Document Frequency (IDF):A Measure of Deviations from PoissonKenneth W. ChurchWilliam A. GaleAT&T Bell LaboratoriesMurray Hill, NJ, USA 07974kwc @research.att.comAbstractLow frequency words tend to be rich in content, and vice versa.
But not all equally frequent words areequally mean!ngful.
We will use inverse document frequency (IDF), a quantity borrowed fromInformation Retrieval, to distinguish words like somewhat and boycott.
Both somewhat and boycottappeared approximately 1000 times in a corpus of 1989 Associated Press articles, but boycott is a betterkeyword because its IDF is farther from what would be expected by chance (Poisson).1.
Document frequency is similar to word frequency, but differentWord frequency is commonly used in all sorts of natural anguage applications.
The practice implicitlyassumes that words (and ngrams) are distributed by a single parameter distribution such as a Poisson ora Binomial.
But we find that these distributions do not fit the data very well.
Both the Poisson andBinomial assume that the variance over documents i no larger than the mean, and yet, we find that itcan be quite a bit larger, especially for interesting words such as boycott where there are hiddenvariables uch as topic that conspire to undermine the independence assumption behind the Poisson andthe Binomial.
Much better fits are obtained by introducing a second parameter such as inversedocument frequency (IDF).Inverse document frequency (IDF) is commonly used in Information Retrieval (Sparck Jones, 1972).IDF is defined as -log2dfw/D, where D is the number of documents in the collection and dfw is thedocument frequency, the number of documents that contain w. Obviously, there is a strong relationshipbetween document frequency, dfw, and word frequency, fw.
The relationship is shown in Figure 1, aplot of iog\]0fw and IDF for 193 words selected from a 50 million word corpus of 1989 AssociatedPress (AP) Newswire stories (D = 85,432 stories).Although log lofw is highly correlated with IDF (p =-0.994),  it would be a mistake to assume that thetwo variables are completely predictable from one another.
Indeed, the experience of the InformationRetrieval community has indicated that IDF is a very useful quantity.
Attempts to replace IDF with fw(or some simple transform offw) have not been very successful.Figure 2 shows one such attempt.
It compares the observed IDF with II~F, an estimate based on fAssume that a document is merely a "bag of words" with no interesting structure (content).
Words arerandomly generated by a Poisson process, n. The probability of k instances of a word w is n(0,k)fw where O= - - :D121e-O O kn(0,k) - for k = 0,1, ?
?
?
Poissonk!In particular, the probability that w will not be found in a document is n(0,0).
Conversely, theprobability of at least one w is 1 -n (0 ,0) .
And therefore, IDF ought o be:119F = - log 2 ( 1 - rt (0,0) ) = - log 2 ( 1 - e - 0 ) Predicted IDFFigure 2 compares IDF with Ii)F. Note that I1)F is systematically too low, indicating that thepredictions are missing crucial generalizations.
Documents are more than just a bag of words.The prediction errors are shown in more detail in Figure 3, which plots the residual IDF (differencebetween predicted and observed) as a function of log lofw for the same 193 words shown in Figure 2.The prediction errors are relatively large in the middle of the frequency range, and smaller at both ends.Unfortunately, we believe the words in the middle are often the most important words for InformationRetrieval purposes.U.E3tO0to0O0Ibo0 Oooi0 1 2 3 4 5IoglOfrequencyFigure 1" IDF is highly correlated with log frequency (p =-0.994).
Thecircles show lOgl0f and IDF for 193 words selected from a corpus of1989 Associated Press Newswire stones (D = 85,432).2.
A Good Keyword is far from PoissonTo get a better look at the crucial differences between IDF and f in  the middle frequency range ~= 103),we selected a set of 53 words for further investigation with 1000 <f< 1020 in the 1989 AP corpus.
The53 words are shown in Table 1, sorted by dr.
Note that the words near the top of the list tend to be moreappropriate for use in an information retrieval system than the words toward the bottom of the list.Stories that mention the word boycott, for example, are likely to be about boycotts.
In contrast, storiesthat mention the word somewhat could be about practically anything.\]122tO"O"O5 10 15observed IDFFigure 2: The observed IDF is systematically ower than what would beexpected under a Poisson, - log2 ( 1 - e -ff/D ).
All but 6 of the circles fallbelow the x =y line.
The data are the same as in Figure 1.Why is IDF such a useful quantity?
One might try to answer the question in terms of informationtheory (Shannon, 1948).
IDF can be thought of as the usefulness in bits of a keyword to a keywordretrieval system.
If we tell you that the document that we are looking for has the keyword boycott, thenwe have narrowed the search space down to just 676/D documents.But, this answer doesn't explain the fundamental difference between boycott and somewhat, boycotthas an IDF of - log2676/D =7.0  bits, only a little more than somewhat, which has an IDF of- log 2 979/D = 6.4.
And yet, boycott is a reasonable keyword and somewhat is not.A good keyword, like boycott, picks out a very specific set of documents.
The problem with somewhatis that it behaves almost like chance (Poisson).
Under a Poisson, the 1013 instances of somewhatshould be found in approximately D( 1 - n (0 ,0 ) )  =D( 1 - n(1013/85432,0))  = 1007 documents.
In fact,somewhat was found in 979 documents, only a little less than what would have been expected bychance.
Good keywords tend to bunch up into many fewer documents, boycott, for example, bunchesup into only 676 documents, much less than chance (D(1- r r (1009/85432,0) )=1003) .
Almost allwords are more "interesting" in this sense than Poisson, but good keywords like boycott are a lot moreinteresting than Poisson, and crummy ones like somewhat are only a little more interesting thanPoisson.There is a weak tendency for nouns to appear higher on the list than non-nouns, though tendency is too weak toexplain the pattern of the systematic deviations from Poisson.
In addition, there are plenty of exceptions in bothdirections: raPe, pool, grants, code and premier are not necessarily nouns, and sweeping, leads, bound and worryare not necessarily non-nouns.123u_C~O9.Q COo diu_C~d.caIDq P~Q.
0Germanso o o ooo 8o oo o^ ?
c9 o9~?~o0',~ oo_ ~" '~ oo , -m,_~ ~ _tg^oo o?
o o  %ic,titl~8~" m o m 8~9% l;l~dpr'llg'?0 1 2 3 4 5IoglOfrequencyFigure 3: The Prediction errors are systematically positive.
The errorstend to be larger in the middle of the frequency range (Germans), andsmaller at both ends (Fromm, which).
The data are the same as inFigures 1-2.On this account, a good keyword is one that behaves very differently from the null hypothesis(Poisson).
We conjecture that the best keywords tend to be found toward the middle of the frequencyrange, where there are relatively large deviations from Poisson, as illustrated in Figure 3.
Thishypothesis runs counter to the standard practice in Information Retrieval of weighting words by IDF,favoring extremely rare words, no matter how they are distributed.Of course, IDF is but one of many ways to show deviations from chance.
Figure 4 shows thedistributions for boycott and somewhat.
Note that somewhat is much "closer" to Poisson in almost anysense of closeness that one might consider.
Three measures of "closeness" are presented in Table 2:IDF, variance (o2), and entropy (H).
Table 2 compares the top 10 words in Table 1 (labeled "betterkeywords") with the bottom 10 words in Table 1 (labeled "worse keywords").
The better keywordshave more IDF, more variance and less entropy than what would be expected under a Poisson withO= f /D= 1000/85,432=0.012.3.
How robust are these deviations f rom chance?We were concerned that the crucial deviations from Poisson behavior might not hold up if we looked atanother corpus of similar material.
Figure 5 shows the word boycott in five different years of the APnews.
The "fat tails" show up in each of the five years.
Clearly, the non-Poisson phenomenon isrobust.Figures 6 and 7 compare IDF and log 10 o 2 for the 53 words in Table 1, and find that IDF and log lo (I2are reasonably stable across years.
The correlations of IDF and log 10 (y2 across years are presented inTables 3-4.
All of the correlations are quite large.
The correlations for IDF are perhaps somewhatlarger than those for log\]0 O2, suggesting that IDF may be somewhat more robust, which is not124df435506551553563623639676687690695718722WgovernorsfestivalgangbullionattendantsrapepalaceboycottroutesincentivespovertydonationslawsuitsTable 1: More IDF (less df)df w df724 pool740 restaurants745 grants752 scheme754 code761 premier775 wire781 customer783 rooms786 engineering803 color811 possession815 projectedMore ContentW827 unity845 bed847 coastal851 educational853 lying853 neighbor863 tragedy867 acquire874 restored905 legitimate910 deliver914 type s929 rejectdf937940946951953955960960961966968969979986WworrycontainingexplainedboundleadshappensimprovingwelcomedtriggeredsweepingfairlyheadingsomewhatnotingIIE0o P0i~6n ...... ....... by?
ot t h~a l  "--.. .....0 2 4 6 8Figure 4: Most words have a fatter tail than Poisson (solid line).
Thedeviations from Poisson are more salient for good keywords likeboycott, than for crummy keywords like somewhat.surprising iven that empirical estimates of variance are notoriously subject o outliers.
None of thecorrelations in Tables 3 and 4 can be attributed to word frequency effects ince the 53 words were allchosen with almost he same 1989 frequency.In general, the correlations in Tables 3-4 are larger near the diagonal, suggesting that estimates degradeover time.
If you want to predict next year's IDF, it is better to use this year's estimate than a ten-year-old estimate.125Table 2: Good keywords have more IDF.
more var and less entropy than PoissonBetter Keywords Worse KeywordsIDF var entropy7.6 0.060 0.057 governors7.4 0.044 0.064 festival7.3 0.043 0.067 gang7.3 0.028 0.068 bullion7.2 0.042 0.068 attendants7.1 0.032 0.073 rape7.1 0.028 0.074 palace7.0 0.027 0.077 boycott7.0 0.026 0.078 routes7.0 0.025 0.078 incentives6.4 0.012 0.092 PoissonIDF var entropy6.5 0.013 0.092 leads6.5 0.013 0.092 happens6.5 0.013 0.092 improving6.5 0.013 0.092 welcomed6.5 0.013 0.092 triggered6.5 0.013 0.093 sweeping6.5 0.013 0.093 fairly6.5 0.013 0.093 heading6.4 0.013 0.093 somewhat6.4 0.012 0.092 noting6.4 0.012 0.092 Poissona.
8hII"Ee ?
:3tJ0 o ?
"54 I T I \[ I I0 1 2 3 4 5 6Figure 5: The strong deviations from Poisson for the word boycott showup very clearly in the AP in 1988, 1989, 1990, 1991 and 1992 (dottedlines).
Katz' K-mixture (Katz, personal communication), the solid linelabelled "K ,"  fits the data better than the Poisson.Another way to confirm that our measurements of IDF, variance and H have consequences across yearsin the AP data, is to note that measurements of IDF, variance and H in 1989 can be used to predict wordfrequency in some other year.
The correlations are shown in Table 5.
They may not not be large, butthey are too large to be due to chance and they all point in the same direction.
The correlations cannotbe attributed to variations in frequency in 1989, since all 53 words have almost the same 1989frequency.
Clearly, there are some interesting systematic relationships between IDF/variance/H and fthat hold up to replication across multiple years in the AP, measurement errors, and other sources ofnoise.126i1988o?~o~) ?6.5 7.56.4 6.8 7,2 7.6 6.0 7.5o ,~ g o %01989u % B ?
?~ ??
?
?1990 l~o~uo OOo6.5 7.519919.0?_,~?
~X o1992 ?7 8 9 10Figure 6: IDF in one year of the AP is very predictive of IDF in another(for the 53 words in Table 1).
Each scatter plot compares IDF in oneyear with IDF in another.
The fact that most of the points line up fairlywell indicates that IDF values are strongly correlated across years.
Thecorrelations are shown in Table 3.4.
Katz' K-mixtureClearly, the Poisson does not fit our data very well, especially for good keywords like boycott.however, a negative result.
Can we say something more constructive?This is,Katz (personal communication) proposed the following alternative to the Poisson.
Prg(k) is theprobability of k instances of w in a document.o~ 13 k Prg(k) = ( l - c0  ~k,0 + ~-  (~- )  K-mixture5k,0 is 1 when k=0, and 0 otherwise.
Katz' K-mixture distribution can be thought of as a mixture ofPoissons.
Suppose that, within documents, boycott is distributed by a Poisson process, but, acrossdocuments, the Poisson parameter 0 is allowed to vary from one document to another depending onhow much the document is about boycotts.
In other words, Prg(k) can be expressed as a convolutionof Poissons with a density function ~:ooPr(k) = f ~(0) n(O,k)dO for k = 0, 1, ?
?
?
Poisson Mixture0In this way, the 0s can depend on an infinite number of unknowable hidden variables, e.g., what thedocuments are about, who wrote them, when they were written, what was going on in the world whenthey were written, etc., but we don't need to know these dependencies for any particular document.
Allwe need to know is ~, the density of 0s, aggregated over all possible combinations of hidden variables.127l 1988o ou o?~ooo~o o. O .
0 .-2.0 -1.4-1.8 -1.4'u-2.2 -1.6 -1.0. .
.
.
t .
.
.
.
.
ol o o~ o ~ ~o I o oo ?
8_?~o ?80 o?-~?o ~ o o,d,l,,~ot o oo  o oo 7O t 1990 :o~ o o |o~:lYYt'~ I1991 ~ ?o?oo 19921 .
.
.
.
.
o,i-2.2 -1.6uoCoo ~-2.0 - 1.61989o oOo~0 vn(y2 Figure 7: log 10 is also predictable from one year to the next, thoughmaybe not as predictable as IDF (for the 53 words in Table 1).
Thecorrelations are shown in Table 4.Table 3: Correlations of IDF across years1988 1989 1990 1991 199219881989199019911992Tabh198819891990199119920.80 0.76 0.68 0.600.80 0.75 0.67 0.480.76 0.75 0.85 0.760.68 0.67 0.85 0.840.60 0.48 0.76 0.844: Correlations of log var across years1988 1989 1990 1991 19920.74 0.61 0.25 0.670.74 0.73 0.42 0.510.61 0.73 0.50 0.610.25 0.42 0.50 0.620.67 0.51 0.61 0.62Table 5: Correlations of IDF, log var and H in 1989 with log f in other years1988 log f 1990 log f 1991 log f 1992 log f1989 IDF1989 log var1989 H-0.18 -0.14 -0.20-0.13 -0.11 -0.140.17 0.15 0.20In the case of Katz' K-mixture, ~(0) is assumed to be (1 -~)  5(0)+-~- efunction, oo when k = 0, and otherwise, 0.-0.17-0.120.160~(k) is Dirac's delta128Katz' K-mixture has two parameters, ccand \[3.
The ~ parameter determines the fraction of relevant andirrelevant documents.
1 -  cx of the documents have no chance of mentioning boycott (0 = 0) becausethey are totally irrelevant o boycotts.
The \[3 parameter determines the average 0 among the relevantdocuments.The two parameters, txand \[3, can be fit from almost any pair of variables considered thus far, e.g., f,IDF, t~ 2, H. We have found thatfand IDF are particularly easy to work with, and are more robust hansome others such as ~2.~: f 21DF - 1f 1 (Z .~ h -D\[3It has been our experience that Katz' K-mixture fits the data much better than the Poisson, as can beseen in Figure 5.
Unlike the Poisson, the K-mixture has two parameters, tx and \[3, and can thereforeaccount for the fact that IDF and fare not completely predictable from one another.In related work (Church and Gale, submitted), we looked at a number of different Poisson mixtures, andfound that our data can also be fit by a negative binomial, which can be viewed as a Poisson mixturewhere Oun (0) is a Gamma distribution (Johnson and Kotz, 1969).
See Mosteller and Wallace (1964)for an example of how to use the negative binomial in a Bayesian discrimination task.
It isstraightforward to generalize the Mosteller and Wallace approach to use Katz' K-mixture or any othermixture of Poissons.5.
ConclusionsDocuments are much more than just a bag of words.
The Poisson distribution predicts that lightning isunlike to strike twice in a single document.
We shouldn't expect to see two or more instances ofboycott in the same document (unless there is some sort of hidden dependency that goes beyond thePoisson).
But when it rains, it pours.
If a document is about boycotts, we shouldn't be surprised to findtwo boycotts or even a half dozen in a single document.
The standard use of the Poisson in modelingthe distribution of words and ngrams fails to fit the data except where there are almost no interestinghidden dependencies a in the case of somewhat.Why are the deviations from Poisson more salient for "interesting" words like boycott than for"boring" words like somewhat?
Many applications uch as information retrieval, text categorization,author identification and word-sense disambiguation attempt to discriminate documents on the basis ofcertain hidden variables uch as topic, author, genre, style, etc.
The more that a keyword (or ngram)deviates from Poisson, the stronger the dependence on hidden variables, and the more useful thekeyword (or ngram) is for discriminating documents on the basis of these hidden dependences.
Similararguments apply in a host of other important applications uch as text compression and languagemodeling for speech recognition where it is desirable for word and ngram probabilities to adaptappropriately tofrequency changes due to various hidden dependencies.We have used document frequency, df, a concept borrowed from Information Retrieval, to finddeviations from Poisson behavior.
Document frequency is similar to word frequency, but different in asubtle but crucial way.
Although inverse document frequency (IDF) and log 10f are extremely highly129correlated (p =-  0.994), it would be a mistake to try to model one with a simple transform of the other.Figure 5 showed one such attempt, where f was transformed into a predicted IDF by introducing aPoisson assumption: I / )F=- log2( l -e - ? )
,  with 0=--.fw Unfortunately, the prediction errors wereDrelatively large for the most important keywords, words with moderate frequencies such as Germans.To get a better look at the subtle differences between document frequency and word frequency, wefocused our attention on a set of 53 words that all had approximately the same word frequency in acorpus of 1989 AP stories.
Table 1 showed that words with larger IDF tend to have more content.boycott, for example, is a better keyword than somewhat because it bunches up into a relatively smallset of documents.
Table 2 showed that variance and entropy can also be used as a measure of content(at least among a set of words with more or less the same word frequency).
A good keyword likeboycott is farther from Poisson (chance) than a crummy keyword like somewhat by almost any sense ofcloseness that one might consider, e.g., IDF, variance, entropy.
These crucial deviations from Poissonare robust.
We showed in section 4 that deviations from Poisson in one year of the AP can be used topredict deviations in another year of the AP.AcknowledgmentsThis work benefited considerably from extensive discussions with Slava Katz.ReferencesChurch, K., and Gale, W. (submitted) Poisson Mixtures.Johnson, N., and Kotz, S. (1969) Discrete Distributions, Houghton Mifflin, Boston.Katz, S. (in preparation).Mosteller, Fredrick, and David Wallace (1964) Inference and Disputed Authorship: The Federalist,Addison-Wesley, Reading, Massachusetts.Salton, G. (1989)Automatic Text Processing, Addison-Wesley.Shannon, C. (1948) "The Mathematical Theory of Communication," Bell System Technical Journal.Sparck Jones, K. (1972) "A Statistical Interpretation of Term Specificity and its Application inRetrieval," Journal of Documentation, 28:1, pp.
11-21.van Rijsbergen, C. (1979) Information Retrieval, Second Edition, Butterworths, London.130
