Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 649?656Manchester, August 2008Semantic Classification with Distributional KernelsDiarmuid?O S?eaghdhaComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FDUnited Kingdomdo242@cl.cam.ac.ukAnn CopestakeComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FDUnited Kingdomaac10@cl.cam.ac.ukAbstractDistributional measures of lexical similar-ity and kernel methods for classificationare well-known tools in Natural LanguageProcessing.
We bring these two meth-ods together by introducing distributionalkernels that compare co-occurrence prob-ability distributions.
We demonstrate theeffectiveness of these kernels by present-ing state-of-the-art results on datasets forthree semantic classification: compoundnoun interpretation, identification of se-mantic relations between nominals and se-mantic classification of verbs.
Finally, weconsider explanations for the impressiveperformance of distributional kernels andsketch some promising generalisations.1 IntroductionThis paper draws a connection between two well-known topics in statistical Natural Language Pro-cessing: distributional measures of lexical simi-larity and kernel methods for classification.
Dis-tributional similarity measures quantify the sim-ilarity between pairs of words through their ob-served co-occurrences with other words in corpusdata.
The kernel functions used in support vec-tor machine classifiers also allow an interpretationas similarity measures; however, not all similar-ity measures can be used as kernels.
In particu-lar, kernel functions must satisfy the mathemati-cal property of positive semi-definiteness.
In Sec-tion 2 we consider kernel functions suitable forcomparing co-occurrence probability distributionsand show that these kernels are closely related tomeasures known from the distributional similarityliterature.
We apply these distributional kernels?2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.to three semantic classification tasks: compoundnoun interpretation, identification of semantic re-lations between nominals and semantic classifica-tion of verbs.
In all cases, the distributional ker-nels outperform the linear and Gaussian kernelsstandardly used for SVM classification and fur-thermore achieve state-of-the-art results.
In Sec-tion 4 we provide a concrete explanation for thesuperior performance of distributional kernels, andin Section 5 we outline some promising directionsfor future research.2 Theory2.1 Distributional Similarity MeasuresDistributional approaches to lexical similarity as-sume that words appearing in similar contexts arelikely to have similar or related meanings.
Tomeasure distributional similarity, we use a repre-sentation of words based on observation of theirrelations with other words.
Specifically, a targetword w is represented in terms of a set C of ad-missible co-occurrence types c = (r, w?
), wherethe word w?belongs to a co-occurrence vocab-ulary Vcand r is a relation that holds betweenw and w?.
Co-occurrence relations may be syn-tactic (e.g., verb-argument, conjunct-conjunct) ormay simply be one of proximity in text.
Countsf(w, c) of a target word w?s co-occurrences canbe estimated from language corpora, and thesecounts can be weighted in a variety of ways to re-flect prior knowledge or to reduce statistical noise.A simple weighting method is to represent eachword w as a vector of co-occurrence probabilities(P (c1|w), .
.
.
, P (c|C||w)).
This vector defines theparameters of a categorical or multinomial proba-bility distribution, giving a useful probabilistic in-terpretation of the distributional model.
As thevector for each target word must sum to 1, themarginal distributions of target words have littleeffect on the resulting similarity estimates.
Many649similarity measures and weighting functions havebeen proposed for distributional vectors; compara-tive studies include Lee (1999), Curran (2003) andWeeds and Weir (2005).2.2 Kernel Methods for ComputingSimilarity and DistanceIn this section we describe two classes of func-tions, positive semi-definite and negative semi-definite kernels, and state some relationships be-tween these classes.
The mathematical treatmentfollows Berg et al (1984).
A good general intro-duction to kernels and support vector machines isthe book by Cristianini and Shawe-Taylor (2000).Let X be a set of items and let k : X ?
X ?R be a symmetric real-valued function on pairs ofitems in X .
Then k is a positive semi-definite (psd)kernel if for all finite n-element sets X ?
X , then?
n Gram matrix K defined by Kij= k(xi, xj)satisfies the propertyv?Kv ?
0, ?v ?
Rn(1)This is equivalent to requiring that k define an in-ner product in a Hilbert space F which may be thesame as X or may differ in dimensionality or intype (F is by definition a vector space, but X neednot be).
An intuitive interpretation of psd kernels isthat they provide a similarity measure on membersofX based on an embedding ?
from input spaceXinto feature space F .
It can be shown that a func-tion is psd if and only if all Gram matrices K haveno negative eigenvalues.Kernel functions have received significant atten-tion in recent years through their applications inmachine learning, most notably support vector ma-chines (SVMs, Cortes and Vapnik (1995)).
SVMclassifiers learn a decision boundary between twodata classes that maximises the minimum distanceor margin from the training points in each class tothe boundary.
The notion of distance used and thefeature space in which the boundary is set are de-termined by the choice of kernel function.
So longas the kernel satisfies (1), the SVM optimisationalgorithm is guaranteed to converge to a global op-timum that affords the geometric interpretation ofmargin maximisation.
Besides these desirable op-timisation properties, kernel methods have the ad-vantage that the choice of kernel can be based onprior knowledge about the problem and on the na-ture of the data.A negative semi-definite (nsd) kernel is a sym-metric function?k : X ?
X ?
R such that for allfinite n-element sets X ?
X and for all vectorsv = (v1, .
.
.
, vn) ?
Rnwith?ivi= 0v?
?Kv ?
0 (2)Whereas positive semi-definite kernels correspondto inner products in a Hilbert space F , negativesemi-definite kernels correspond to squared dis-tances.
In particular, if?k(x, y) = 0 only whenx = y then?
?k is a metric.
If a function k ispsd, then ?k is always nsd, but the converse doesnot hold.1However, Berg et al (1984) describetwo simple methods for inducing a positive semi-definite function k from negative semi-definite?k:k(x, y) =?k(x, x0) +?k(y, x0)?
?k(x, y)?
?k(x0, x0), ?x0?
X (3a)k(x, y) = exp(??
?k(x, y)), ??
> 0 (3b)The point x0in (3a) can be viewed as providingan origin in F that is the image of some point inthe input space X ; the choice of x0does not havean effect on SVM classification.
A familiar exam-ple of these transformations arises if we take?k tobe the squared Euclidean L2distance ?x ?
y?2=?i(xi?
yi)2.
Applying (3a) and setting x0tobe the zero vector, we obtain a quantity that istwice the linear kernel k(x, y) =?ixiyi.
Apply-ing (3b) we derive the Gaussian kernel k(x, y) =exp(???x?y?2).
In the next section we considerkernels obtained by plugging alternative squaredmetrics into equations (3a) and (3b).2.3 Distributional KernelsGiven the effectiveness of distributional similaritymeasures for numerous tasks in NLP and the in-terpretation of kernels as similarity functions, itseems natural to consider the use of kernels tai-lored for co-occurrence distributions when per-forming semantic classification.
As shown in Sec-tion 2.2 the standardly used linear and Gaussiankernels derive from theL2distance, yet Lee (1999)has shown that this distance measure is relativelypoor at comparing co-occurrence distributions.
In-formation theory provides a number of alterna-tive distance functions on probability measures, ofwhich the L1distance (also called variational dis-tance), Kullback-Leibler divergence and Jensen-Shannon divergence are well-known in NLP and1Negated nsd functions are sometimes called condition-ally psd; they constitute a superset of the psd functions.650Distance Definition Derived linear kernel(L2distance)2?c(P (c|w1)?
P (c|w2))2?cP (c|w1)P (c|w2)L1distance?c|P (c|w1)?
P (c|w2)|?cmin(P (c|w1), P (c|w2))Jensen-Shannon?cP (c|w1) log2(2P (c|w1)P (c|w1)+P (c|w2)) + ?
?cP (c|w1) log2(P (c|w1)P (c|w1)+P (c|w2)) +divergence P (c|w2) log2(2P (c|w2)P (c|w1)+P (c|w2)) P (c|w2) log2(P (c|w2)P (c|w1)+P (c|w2))Hellinger distance?c(?P (c|w1)?
?P (c|w2))2?c?P (c|w1)P (c|w2)Table 1: Squared metric distances on co-occurrence distributions and corresponding linear kernelswere shown by Lee to give better similarity esti-mates than the L2distance.In Section 2.2 we have seen how to derive psdkernels (similarities) from nsd kernels (distances).It seems likely that distance measures that areknown to work well for comparing co-occurrencedistributions will also give us suitable psd similar-ity measures.
Negative semi-definite kernels areby definition symmetric, which rules the Kullback-Leibler divergence and Lee?s (1999) ?-skew diver-gence out of consideration.
The nsd condition (2)is met if the distance function is a squared metric ina Hilbert space.
In this paper we use a parametricfamily of squared Hilbertian metrics on probabilitydistributions that has been discussed by Hein andBousquet (2005).
This family contains many fa-miliar distances including the L1distance, Jensen-Shannon divergence (JSD) and the Hellinger dis-tance used in statistics, though not the squared L2distance.
Positive semi-definite distributional ker-nels can be derived from these distances throughequations (3a) and (3b).
We interpret the distribu-tional kernels produced by (3a) and (3b) as ana-logues of the linear and Gaussian kernels respec-tively, given by a different norm or concept of dis-tance in the feature space F .
Hence the linear dis-tributional kernels produced by (3a) correspond toinner products in the input space X , and the rbfdistributional kernels produced by (3b) are radialbasis functions corresponding to inner productsin a high-dimensional Hilbert space of Gaussian-like functions.
In this paper we use the unmodi-fied term ?linear kernel?
in the standard sense ofthe linear kernel derived from the L2distance andmake explicit the related distance when referring toother linear kernels, e.g., the ?JSD linear kernel?.Likewise, we use the standard term ?Gaussian?
torefer to the L2rbf kernel, and denote other rbf ker-nels as, for example, the ?JSD rbf kernel?.Table 1 lists relevant squared metric distancesand their derived linear kernels.
The linear ker-nel derived from the L1distance is the same as thedifference-weighted token-based similarity mea-sure of Weeds and Weir (2005).
The JSD linearkernel can be rewritten as (2 - JSD), where JSDis the value of the Jensen-Shannon divergence.This formulation is used as a similarity measureby Lin (1999).
Dagan et al (1999) use a similar-ity measure 10?
?JSD, though they acknowledgethat this transformation is heuristically motivated.The rbf kernel exp(?
?JSD) provides a theoret-ically sound alternative when the psd property isrequired.
It follows from the above discussionthat these previously known distributional similar-ity measures are valid kernel functions and can beused directly for SVM classification.Finally, we consider the status of other populardistributional measures.
The familiar cosine sim-ilarity measure is provably a valid psd kernel, asit is the L2linear kernel calculated between L2-normalised vectors.
Distributional vectors are bydefinition L1-normalised (they sum to 1), but thereis evidence that L2normalisation is optimal whenusing L2kernels for tasks such as text categori-sation (Leopold and Kindermann, 2002).
Indeed,in the experiments described below L2-normalisedfeature vectors are used with the L2kernels, andthe L2linear kernel function then becomes identi-cal to the cosine similarity.
Other similarity mea-sures, such as that of Lin (1998), can be shown tobe non-psd by calculating similarity matrices fromreal or artificial data and showing that their non-zero eigenvalues are not all positive, as is requiredby psd functions.6513 Practice3.1 General MethodologyAll experiments were performed using the LIB-SVM Support Vector Machine library (Chang andLin, 2001), modified to implement one-against-all classification.
The members of the distribu-tional kernel family all performed similarly but theJensen-Shannon divergence kernels gave the mostconsistently impressive results, and we restrict dis-cussion to these kernels due to considerations ofspace and clarity.
In each experiment we com-pare the standard linear and Gaussian kernels withthe linear and JSD rbf kernels.
As a preprocess-ing step for the L2kernels, each feature vectorwas normalised to have unit L2norm.
For theJensen-Shannon kernels, the feature vectors werenormalised to have unit L1norm, i.e., to definea probability distribution.
For all datasets and alltraining-test splits the SVM cost parameter C wasoptimised in the range (2?6, 2?4, .
.
.
, 212) throughcross-validation on the training set.
In addition, thewidth parameter ?
was optimised in the same wayfor the rbf kernels.
The number of optimisationfolds differed according to the size of the datasetand the number of training-test splits to be eval-uated: we used 10 folds for the compound task,leave-one-out cross-validation for SemEval Task 4and 25 folds for the verb classification task.3.2 Compound Noun InterpretationThe task of interpreting the semantics of nouncompounds is one which has recently receivedconsiderable attention (Lauer, 1995; Girju et al,2005; Turney, 2006).
For a given noun-noun com-pound, the problem is to identify the semantic re-lation between the compound?s constituents ?
thata kitchen knife is a knife used in a kitchen but asteel knife is a knife made of steel.2The difficultyof the task is due to the fact that the knowledge re-quired to interpret compounds is not made explicitin the contexts where they appear, and hence stan-dard context-based methods for classifying seman-tic relations in text cannot be applied.
Most previ-ous work making use of lexical similarity has beenbased on WordNet measures (Kim and Baldwin,2005; Girju et al, 2005).
?O S?eaghdha and Copes-take (2007) were to our knowledge the first to ap-ply a distributional model.
Here we build on their2In the classification scheme considered here, kitchenknife would have the label IN and steel knife would be la-belled BE.methodology by introducing a probabilistic featureweighting scheme and applying the new distribu-tional kernels.For our experiments we used the dataset of?O S?eaghdha and Copestake (2007), which con-sists of 1443 noun compounds annotated withsix semantic relations: BE, HAVE, IN, AGENT,INSTRUMENT and ABOUT.3The classificationbaseline associated with always choosing themost frequent relation (IN) is 21.3%.
Foreach compound (N1, N2) in the dataset, weassociate the co-occurrence probability vector(P (c1|N1), .
.
.
, P (c|C||N1)) with N1and the vec-tor (P (c1|N2), .
.
.
, P (c|C||N2)) with N2.
Theprobability vector for the compound is createdby appending the two constituent vectors, eachscaled by 0.5 to weight both constituents equallyand ensure that the new vector sums to 1.These probability vectors are used to computethe Jensen-Shannon kernel values.
The pre-processing step for the L2kernels is analogous,except that the co-occurrence frequency vector(f(c1, Ni), .
.
.
, f(c|C|, Ni)) for each constituentNiis normalised to have unit L2norm (insteadof unit L1norm); the combined feature vector foreach data item is also L2-normalised.4The co-occurrence relation we counted to esti-mate the probability vectors was the conjunctionrelation.
This relation gives sparse but high-qualityinformation, and was shown to be effective by?OS?eaghdha and Copestake.
We extracted two fea-ture sets from two very different corpora.
Thefirst is the 90 million word written component ofthe British National Corpus (Burnard, 1995).
Thiscorpus was parsed with the RASP parser (Briscoeet al, 2006) and all instances of the conj gram-matical relation were counted.
The co-occurrencevocabulary Vcwas set to the 10,000 words mostfrequently entering into a conj relation acrossthe corpus.
The second corpus we used was theWeb 1T 5-Gram Corpus (Brants and Franz, 2006),which contains frequency counts for n-grams upto length 5 extracted from Google?s index of ap-proximately 1 trillion words of Web text.
As thenature of this corpus precludes parsing, we used asimple pattern-based technique to extract conjunc-tions.
An n-gram was judged to contain a conjunc-tion co-occurrence between Niand Njif it con-3This dataset is available from http://www.cl.cam.ac.uk/?do242/resources.html.4The importance of performing both normalisation stepswas suggested to us by an anonymous reviewer?s comments.652BNC 5-GramKernel Acc F Acc FLinear 57.9 55.8 55.0 52.5Gaussian 58.0 56.2 53.5 50.8JSD (linear) 59.9 57.8 60.2 58.1JSD (rbf) 59.8 57.9 61.0 58.8Table 2: Results for compound interpretationtained the patternNiand (?N)*Nj(?N)*.
A noundictionary automatically constructed from Word-Net and an electronic version of Webster?s 1913Unabridged Dictionary determined the sets of ad-missible nouns {N} and non-nouns {?N}.5Thevocabulary Vcwas again set to the 10,000 mostfrequent conjuncts, and the probability estimatesP (c|w) were based on the n-gram frequencies foreach n-gram matching the extraction pattern.
Athird feature set extracted from the 5-Gram Corpusby using a larger set of joining terms was also stud-ied but the results were not significantly differentfrom the sparser conjunction feature sets and arenot presented here.Performance was measured by splitting the datainto five folds and performing cross-validation.Results for the two feature sets and four kernelsare presented in Table 2.
The kernels derived fromthe Jensen-Shannon divergence clearly outperformthe L2distance-based linear and Gaussian kernelsin both accuracy and macro-averaged F-score.
Thebest performing kernel-feature combination is theJensen-Shannon rbf kernel with the 5-Gram fea-tures, which attains 61.0% accuracy and 58.8%F-score.
This surpasses the best previous resultof 57.1% accuracy, 55.3% F-score that was re-ported by?O S?eaghdha and Copestake (2007) forthis dataset.
That result was obtained by combin-ing a distributional model with a relational simi-larity model based on string kernels; incorporatingrelational similarity into the system described hereimproves performance even further (?O S?eaghdha,2008).3.3 SemEval Task 4Task 4 at the 2007 SemEval competition (Girju etal., 2007) focused on the identification of seman-tic relations among nominals in text.
Identifica-tion of each of seven relations was designed asa binary classification task with 140 training sen-5The electronic version of Webster?s is available fromhttp://msowww.anu.edu.au/?ralph/OPTED/.BNC 5-GramKernel Acc F Acc FLinear 67.6 57.1 65.4 63.3Gaussian 66.8 60.7 65.6 62.9JSD (linear) 71.4 68.8 69.6 65.8JSD (rbf) 69.9 66.7 70.7 67.5Table 3: Results for SemEval Task 4tences and around 70 test sentences.6To ensurethat the task be a challenging one, the negativetest examples were all ?near misses?
in that theywere plausible candidates for the relation to holdbut failed to meet one of the criteria for that rela-tion.
This was achieved by selecting both positiveand negative examples from the results of the sametargeted Google queries.
The majority-class base-line for this task gives Accuracy = 57.0%, F-score= 30.8%, while the all-true baseline (label everytest sentence positive) gives Accuracy = 48.5%, F-score = 64.8%.We used the same feature sets and kernels as inSection 3.2.
The results are presented in Table 3.Again, the JSD kernels outperform the standardL2kernels by a considerable margin.
The bestperforming feature-kernel combination achieves71.4% Accuracy and 68.8% F-score, higher thanthe best performance attained in the SemEval com-petition without using WordNet similarity mea-sures (Accuracy = 67.0%, F-score = 65.1%; Nakovand Hearst (2007)).
This is also higher than theperformance of all but three of the 14 SemEval en-tries which did use WordNet.
Davidov and Rap-poport (2008) have recently described a WordNet-free method that attains slightly lower accuracy(70.1%) and slightly higher F-score (70.6%) thanour method.
Taken together, Davidov and Rap-poport?s results and ours define the current stateof the art on this task.3.4 Verb ClassificationTo investigate the effectiveness of distributionalkernels on a different kind of semantic classifi-cation task, we tested our methods on the verbclass data of Sun et al (2008).
This dataset con-sists of 204 verbs assigned to 17 of Levin?s (1993)verb classes.
Each verb is represented by a setof features corresponding to the distribution of itsinstances across subcategorisation frames (SCFs).6The relations are Cause-Effect, Instrument-Agency,Product-Producer, Origin-Entity, Theme-Tool, Part-Wholeand Content-Container.653FS3 FS5Kernel Acc F Acc FLinear 67.1 65.5 67.6 65.9Gaussian 60.8 58.6 62.7 60.2JSD (linear) 70.6 67.3 69.6 66.4JSD (rbf) 68.6 65.1 70.1 67.2Sun et al (SVM) 57.8 58.2 57.3 57.4Sun et al (GS) 59.3 57.1 64.2 62.5Table 4: Results for leave-one-out verb classifica-tion and comparison with Sun et al?s (2008) SVMand Gaussian fitting methodsThese frames include information about syntac-tic constituents (NP, NP NP, NP SCOMP, .
.
.
)and some lexical information about subcategorisedprepositions (NP with, out, .
.
.
).
The feature val-ues are counts of SCFs extracted from a large cor-pus.
As the feature vector for each verb natu-rally defines a probability distribution over SCFs,it seems intuitive to apply distributional kernels tothe problem of predicting Levin classes for verbs.Sun et al use multiple feature sets of varyingsparsity and noisiness.
We report results on thetwo feature sets for which they reported best per-formance; for continuity we keep the names FS3and FS5 for these feature sets.
These were de-rived from the least filtered and hence least sparsesubcategorisation lexicon (which they call VALEX1) and differ in the granularity of prepositionalSCFs.
The SCF representation in FS5 is richerand hence potentially more discriminative, but it isalso sparser.
Using an SVM with a Gaussian ker-nel, Sun et al achieved their best results on FS3.Perhaps surprisingly, their best results overall wereattained with FS5 by a simple method based onfitting multivariate Gaussian distributions to eachclass in the training data and assigning the maxi-mum likelihood class to test points.Following Sun et al, we use a leave-one-outmeasure of verb classification performance.
Asthe examples are distributed equally across the 17classes, the random baseline accuracy is 5.9%.
Ta-ble 4 presents our results with L2and JSD kernels,as well as those of Sun et al The best overallperformance is attained by the JSD linear kernel,which scores higher than the L2-derived kernelson both feature sets.
The L2linear kernel also per-forms quite well and with consistency.
The JSDrbf kernel was less consistent over cross-validationruns, seemingly due to uncertainty in selecting theoptimal ?
parameter value; it clearly outperformsthe L2linear kernel on one feature set (FS5) buton the other (FS3) it attains a slightly lower F-score while maintaining a higher accuracy.
TheGaussian kernel seems particularly ill-suited to thisdataset, performing significantly worse than theother kernels.
The difference between Sun et al?sresults with the Gaussian kernel and ours with thesame kernel may be due to the use of one-against-all classification here instead of one-against-one,or it may be due to differences in preprocessing orparameter optimisation.4 The effect of marginal distributionsIt is natural to ask why distributional kernels per-form better than the standard linear and Gaussiankernels.
One answer might be that just as infor-mation theory provides the ?correct?
notion of in-formation for many purposes, it also provides the?correct?
notion of distance between probabilitydistributions.
Hein and Bousquet (2005) show thattheir family of distributional kernels are invariantto bijective transformations of the event space Cand suggest that this property is a valuable one forimage histogram classification where data may berepresented in a range of equivalent colour spaces.However, it is not clear that this confers an advan-tage when comparing lexical co-occurrence distri-butions; when transformations are performed onthe space of co-occurrence types, they are gener-ally not information-conserving, for example lem-matisation or stemming.A more practical explanation is that the distri-butional kernels and distances are less sensitivethan the (squared) L2distance and its derived ker-nels to the marginal frequencies of co-occurrencetypes.
When a type c has high frequency we expectthat it will have higher variance, i.e., the differ-ences |P (c|w1)?
P (c|w2)| will tend to be greatereven if c is not a more important signifier of simi-larity.7These differences contribute quadraticallyto the L2distance and hence also to the associ-ated rbf kernel, i.e., the Gaussian kernel.
It is alsoeasy to see that types c for which P (c|wi) tendsto be large will dominate the value of the linearkernel.
This explanation is also plausibly a fac-tor in the relatively poor performance of L2dis-tance as a lexical dissimilarity measure, as demon-7Chapelle et al (1999) give a similar explanation for theperformance of a related family of kernels on a histogramclassification task.654strated by Lee (1999).
In contrast, the differences|P (c|w1)?P (c|w2)| are not squared in the L1dis-tance formula, and the minimum function in theL1linear kernel dampens the effect of high-varianceco-occurrence types.
The Jensen-Shannon formulais more difficult to interpret, as the difference termsdo not directly appear.
While co-occurrence typeswith large P (c|w1) and P (c|w2) do contributemore to the distance and kernel values, it is theproportional size of the difference that appears inthe log term rather than its magnitude.
Finally, theHellinger distance and kernels squash the varianceassociated with c through the square root function.5 Discussion and Future DirectionsKernels on probability measures have been dis-cussed in the machine learning literature (Kondorand Jebara, 2003; Cuturi et al, 2005; Hein andBousquet, 2005), but they have previously beenapplied only to standard image and text classifi-cation benchmark tasks.
We seem to be the first touse distributional kernels for semantic classifica-tion and to note their connection with familiar lex-ical similarity measures.
Indeed, the only researchwe are aware of on kernels tailored for lexical sim-ilarity is the small body of work on WordNet ker-nels, e.g., Basili et al (2006).
In contrast, Sup-port Vector Machines have been widely adoptedfor computational semantic tasks, from word sensedisambiguation (Gliozzo et al, 2005) to semanticrole labelling (Pradhan et al, 2004).
The standardfeature sets for semantic role labelling and manyother tasks are collections of heterogeneous fea-tures that do not correspond to probability distri-butions.
So long as the features are restricted topositive values, distributional kernels can be ap-plied; it will be interesting (and informative) to seewhether they retain their superiority in this setting.One advantage of kernel methods is that kernelscan be defined for non-vectorial data structuressuch as strings, trees, graphs and sets.
A promis-ing topic of future research is the design of distri-butional kernels for comparing structured objects,based on the feature space embedding associatedwith convolution kernels (Haussler, 1999).
Thesekernels map structures in X into a space whose di-mensions correspond to substructures of the ele-ments of X .
Thus strings are mapped onto vec-tors of substring counts, and trees are mapped ontovectors of subtree counts.
We adopt the perspec-tive that this mapping represents structures xi?
Xas measures over substructures x?1, .
.
.
, x?d.
Prop-erly normalised, this gives a distributional proba-bility vector (P (x?1), .
.
.
, P (x?d)) similar to thoseused for computing lexical similarity.
This per-spective motivates the use of distributional innerproducts instead of the dot products implicitly usedin standard convolution kernels.
Several authorshave suggested applying distributional similaritymeasures to sentences and phrases for tasks such asquestion answering (Lin and Pantel, 2001; Weedset al, 2005).
Distributional kernels on strings andtrees should provide a flexible implementation ofthese suggestions that is compatible with SVMclassification and does not require manual featureengineering.
Furthermore, there is a ready gener-alisation to kernels on sets of structures; if a setis represented as the normalised sum of its mem-ber embeddings in feature space F , distributionalmethods can be applied directly.6 ConclusionIn this paper we have introduced distributional ker-nels for classification with co-occurrence proba-bility distributions.
The suitability of distribu-tional kernels for semantic classification is intu-itive, given their relation to proven distributionalmethods for computing semantic similarity, and inpractice they work very well.
As these kernelsgive state-of-the-art results on the three datasets wehave tested, we expect that they will prove usefulfor a wide range of semantic classification prob-lems in future.AcknowledgementsWe are grateful to Andreas Vlachos and threeanonymous reviewers for their useful comments,and to Anna Korhonen for providing the verb clas-sification dataset.
This work was supported in partby EPSRC Grant EP/C010035/1.ReferencesBasili, Roberto, Marco Cammisa, and Alessandro Mos-chitti.
2006.
A semantic kernel to classify texts withvery few training examples.
Informatica, 30(2):163?172.Berg, Christian, Jens P. R. Christensen, and Paul Ressel.1984.
Harmonic Analysis on Semigroups: Theory ofPositive Definite and Related Functions.
Springer,Berlin.Brants, Thorsten and Alex Franz, 2006.
Web 1T 5-gramCorpus Version 1.1.
Linguistic Data Consortium.655Briscoe, Ted, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Proceed-ings of the ACL Interactive Presentation Sessions.Burnard, Lou, 1995.
Users?
Guide for the British Na-tional Corpus.
British National Corpus Consortium,Oxford University Computing Service.Chang, Chih-Chung and Chih-Jen Lin, 2001.
LIBSVM:a library for support vector machines.
Softwareavailable at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Chapelle, Olivier, Patrick Haffner, and Vladimir N.Vapnik.
1999.
Support vector machines forhistogram-based image classification.
IEEE Trans-actions on Neural Networks, 10(5):1055?1064.Cortes, Corinna and Vladimir Vapnik.
1995.
Supportvector networks.
Machine Learning, 20(3):273?297.Cristianini, Nello and John Shawe-Taylor.
2000.
An In-troduction to Support Vector Machines.
CambridgeUniversity Press, Cambridge.Curran, James.
2003.
From Distributional to SemanticSimilarity.
Ph.D. thesis, University of Edinburgh.Cuturi, Marco, Kenji Fukumizu, and Jean-PhilippeVert.
2005.
Semigroup kernels on measures.
Jour-nal of Machine Learning Research, 6:1169?1198.Dagan, Ido, Lillian Lee, and Fernando Pereira.
1999.Similarity-based models of word cooccurrence prob-abilities.
Machine Learning, 34(1?4):43?69.Davidov, Dmitry and Ari Rappoport.
2008.
Classifica-tion of semantic relationships between nominals us-ing pattern clusters.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics.Girju, Roxana, Dan Moldovan, Marta Tatu, andDaniel Antohe.
2005.
On the semantics ofnoun compounds.
Computer Speech and Language,19(4):479?496.Girju, Roxana, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2007.SemEval-2007 Task 04: Classification of seman-tic relations between nominals.
In Proceedings ofthe 4th International Workshop on Semantic Evalua-tions.Gliozzo, Alfio, Claudio Giuliano, and Carlo Strappar-ava.
2005.
Domain kernels for word sense disam-biguation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics.Haussler, David.
1999.
Convolution kernels on dis-crete structures.
Technical Report UCSC-CRL-99-10, Computer Science Department, University ofCalifornia at Santa Cruz.Hein, Matthias and Olivier Bousquet.
2005.
Hilbertianmetrics and positive definite kernels on probabilitymeasures.
In Proceedings of the 10th InternationalWorkshop on Artificial Intelligence and Statistics.Kim, Su Nam and Timothy Baldwin.
2005.
Automaticinterpretation of noun compounds using WordNetsimilarity.
In Proceedings of the 2nd InternationalJoint Conference on Natural Language Processing.Kondor, Risi and Tony Jebara.
2003.
A kernel betweensets of vectors.
In Proceedings of the 20th Interna-tional Conference on Machine Learning.Lauer, Mark.
1995.
Designing Statistical LanguageLearners: Experiments on Compound Nouns.
Ph.D.thesis, Macquarie University.Lee, Lillian.
1999.
Measures of distributional similar-ity.
In Proceedings of the 37th Annual Meeting ofthe Association for Computational Linguistics.Leopold, Edda and J?org Kindermann.
2002.
Text cat-egorization with support vector machines.
how torepresent texts in input space?
Machine Learning,46(1?3):423?444.Levin, Beth.
1993.
English Verb Classes and Alter-nations: A Preliminary Investigation.
University ofChicago Press, Chicago.Lin, Dekang and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
Natural Lan-guage Engineering, 7(4):343?360.Lin, Dekang.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th Inter-national Conference on Machine Learning.Nakov, Preslav I. and Marti A. Hearst.
2007.
UCB:System description for SemEval Task #4.
In Pro-ceedings of the 4th International Workshop on Se-mantic Evaluations.
?O S?eaghdha, Diarmuid and Ann Copestake.
2007.
Co-occurrence contexts for noun compound interpreta-tion.
In Proceedings of the ACL Workshop on aBroader Perspective on Multiword Expressions.
?O S?eaghdha, Diarmuid.
2008.
Learning CompoundNoun Semantics.
Ph.D. thesis, Computer Labora-tory, University of Cambridge.
In preparation.Pradhan, Sameer, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Juraf-sky.
2004.
Support vector learning for semantic ar-gument classification.
Machine Learning, 60(1):11?39.Sun, Lin, Anna Korhonen, and Yuval Krymolowski.2008.
Verb class discovery from rich syntactic data.In Proceedings of the 9th International Conferenceon Intelligent Text Processing and ComputationalLinguistics.Turney, Peter D. 2006.
Similarity of semantic rela-tions.
Computational Linguistics, 32(3):379?416.Weeds, Julie and David Weir.
2005.
Co-occurrenceretrieval: A flexible framework for lexical dis-tributional similarity.
Computational Linguistics,31(4):439?476.Weeds, Julie, David Weir, and Bill Keller.
2005.
Thedistributional similarity of sub-parses.
In Proceed-ings of the ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment.656
