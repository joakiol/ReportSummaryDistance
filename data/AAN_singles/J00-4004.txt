Learning Methods to Combine LinguisticIndicators: Improving AspectualClassification and Revealing LinguisticInsightsEric V. Siegel*Columbia UniversityKathleen R. McKeown*Columbia UniversityAspectual classification maps verbs to a small set of primitive categories in order to reason abouttime.
This classification isnecessary for interpreting temporal modifiers and assessing temporalrelationships, and is therefore a required component for many natural anguage applications.A verb's aspectual category can be predicted by co-occurrence fr quencies between the verband certain linguistic modifiers.
These frequency measures, called linguistic indicators, are chosenby linguistic insights.
However, linguistic indicators used in isolation are predictively incomplete,and are therefore insufficient when used individually.In this article, we compare three supervised machine learning methods for combining multiplelinguistic indicators for aspectual c assification: decision trees, genetic programming, and logisticregression.
A set of 14 indicators are combined for classification according to two aspectual distinc-tions.
This approach improves the classification performance for both distinctions, as evaluatedover unrestricted sets of verbs occurring across two corpora.
This demonstrates the effectivenessof the linguistic indicators and provides amuch-needed full-scale method for automatic aspectualclassification.
Moreover, the models resulting from learning reveal several linguistic insights thatare relevant o aspectual classification.
We also compare supervised learning methods with anunsupervised method for this task.1.
IntroductionAspectual classification maps clauses (e.g., simple sentences) to a small set of cate-gories in order to reason about ime.
For example, events, such as, You called your father,are distinguished from states, such as, You resemble your father.
The ability to distinguishstative clauses from event clauses is a fundamental component of natural languageunderstanding.
These two high-level categories correspond to fundamental distinc-tions in many domains, including the distinctions between diagnosis and procedurein the medical domain, and between analysis and activity in the financial domain.Stativity is the first high-level distinction made when defining the aspectual classof a clause.
Events are further distinguished according to completedness ( ometimescalled telicity), which determines whether an event reaches a culmination or comple-tion point at which a new state is introduced.
For example, I made afire is culminated,since a new state is introduced--something is made, whereas I gazed at the sunset isnonculminated.
* Computer Science Dept., 1214 Amsterdam Ave., New York NY 10027.
E-mail: evs@cs.columbia.edut Computer Science Dept., 1214 Amsterdam Ave., New York, NY 10027.
E-mail: kathy@cs.columbia.edu@ 2001 Association for Computational LinguisticsComputational Linguistics Volume 26, Number 4Aspectual classification is necessary for interpreting temporal modifiers and as-sessing temporal entailments (Moens and Steedman 1988; Dorr 1992; Klavans 1994)and is therefore a required component for applications that perform certain naturallanguage interpretation, generation, summarization, information retrieval, and ma-chine translation tasks.
Each of these applications requires the ability to reason abouttime.A verb's aspectual category can be predicted by co-occurrence fr quencies betweenthe verb and linguistic phenomena such as the progressive tense and certain temporalmodifiers (Klavans and Chodorow 1992).
These frequency measures are called linguis-tic indicators.
The choice of indicators is guided by linguistic insights that describehow the aspectual category of a clause is constrained by the presence of these mod-ifiers.
For example, an event can be placed in the progressive, as in, She was jogging,but many stative clauses cannot, e.g., *She was resembling her father (Dowty 1979).
Oneadvantage of linguistic indicators is that they can be measured automatically.However, individual linguistic indicators are predictively incomplete, and aretherefore insufficient when used in isolation.
As demonstrated mpirically in this arti-cle, individual linguistic indicators uffer from limited classification performance dueto several linguistic and pragmatic factors.
For example, some indicators were not mo-tivated by specific linguistic insights.
However, linguistic indicators have the potentialto interact and supplement one another, so it would be beneficial to combine themsystematically.In this article, we compare three supervised machine learning methods for com-bining multiple linguistic indicators for aspectual classification: decision trees, geneticprogramming, and logistic regression.
A set of 14 indicators are combined, first forclassification according to stativity, and then for classification according to complet-edness.
This approach realizes the potential of linguistic indicators, improving classi-fication performance over a baseline method for both tasks with minimal overfitting,as evaluated over an unrestricted set of verbs occurring in two corpora.
This servesto demonstrate he effectiveness of these linguistic indicators and thus provides amuch-needed full-scale, expandable method for automatic aspectual classification.The results of learning are linguistically viable in two respects.
First, learning au-tomatically produces models that are specialized for different aspectual distinctions;the same set of 14 indicators are combined in different ways according to which clas-sification problem is targeted.
Second, inspecting the models resulting from learningrevealed linguistic insights that are relevant to aspectual classification.We also evaluate an unsupervised method for this task.
This method uses co-occurrence statistics to group verbs according to meaning.
Although this methodgroups verbs generically and is not designed to distinguish according to aspectualclass in particular, we show that the results do distinguish verbs according to stativity.The next two sections of this article describe aspectual classification and linguisticindicators.
Section 4 describes the three supervised learning methods employed tocombine linguistic indicators for aspectual classification.
Section 5 gives results interms of classification performance and resulting linguistic insights, comparing theseresults, across classification tasks, to baseline methods.
Section 6 describes experimentswith an unsupervised approach.
Finally, Sections 7, 8, and 9 survey related work,describe future work, and present conclusions.2.
Aspect in Natural LanguageBecause, in general, the sequential order of clauses is not enough to determine theunderlying chronological order, aspectual classification is required for interpreting596Siegel and McKeown Improving Aspectual ClassificationTable 1Aspectual classes.
This table is adapted from Moens and Steedman (1988, p. 17).CulminatedNonculminatedEVENTSpunctual extendedCULMINATION CULMINATEDPROCESSrecognize build a housePOINT PROCESShiccup run, swim, walkSTATESunderstandeven the simplest narratives in natural language.
For example, consider:(1) Sue mentioned Miami (event).
Jim cringed (event).In this case, the first sentence describes an event that takes place before the eventdescribed by the second sentence.
However, in(2) Sue mentioned Miami (event).
Jim already knew (state).the second sentence describes a state, which begins before the event described by thefirst sentence.Aspectual classification is also a necessary prerequisite for interpreting certainadverbial adjuncts, as well as identifying temporal constraints between sentences ina discourse (Moens and Steedman 1988; Dorr 1992; Klavans 1994).
In addition, itis crucial for lexical choice and tense selection in machine translation (Moens andSteedman 1988; Klavans and Chodorow 1992; Klavans 1994; Dorr 1992).Table 1 sun~narizes the three aspectual distinctions, which compose five aspec-tual categories.
In addition to the two distinctions described in the previous ection,atomicity distinguishes punctual events (e.g., She noticed the picture on the wall) fromextended events, which have a time duration (e.g., She ran to the store).
Therefore, fourclasses of events are derived: culmination, culminated process, process, and point.These aspectual distinctions are motivated by a series of syntactic and entailmentconstraints described in the first three subsections below.
Further cognitive and philo-sophical rationales behind these semantic distinctions are surveyed by Siegel (1998b).First we describe aspectual constraints that linguistically motivate the design of sev-eral of the linguistic indicators.
Next we describe an array of semantic entailments andtemporal constraints that can be put to use by an understanding system once inputclauses have been aspectually classified.
Then we describe how aspect influences theinterpretation of temporal connectives and modifiers.
Aspectual transformations aredescribed, and we introduce the concept of a clause's fundamental aspectual category.Finally, we describe several natural anguage applications that require an aspectualclassification component.2.1 Aspectual Markers and ConstraintsCertain features of a clause, such as the presence of adjuncts and tense, are constrainedby and contribute to the aspectual class of the clause (Vendler 1967; Dowty 1979;Pustejovsky 1991; Passonneau 1988; Klavans 1994; Resnik 1996; Olsen and Resnik1997).
Table 2 illustrates an array of linguistic constraints, as more comprehensively597Computational Linguistics Volume 26, Number 4Table 2Several aspectual markers and associated constraints on aspectual c ass.If a clause can occur: then it must be:with a temporal adverb (e.g., then)in progressiveas a complement offorce~persuadeafter "What happened was..."with a duration in-PP (e.g., in an hour)in the perfect enseEventExtended EventEventEventCulminated EventCulminated Event or Statesummarized by Klavans (1994) and Siegel (1998b).
Each entry in this table describesan aspectual marker and the constraints on the aspectual category of any clause thatappears with that marker.
For example, a clause must be extended to appear in theprogressive tense, e.g.,(3) He was prospering in India extended event),which contrasts with,(4) *You were noticing that I can hardly be blamed ... (atomic event))As a second example, since the perfect ense requires that the clause it occurs inmust entail a consequent s ate, an event must be culminated to appear in the perfecttense.
For example,(5) Thrasymachus has made an attempt to get the argument into his ownhands (culminated event),contrasts with,(6) *He has cowered own in a paralysis of fear (nonculminated vent).2.2 Aspectua l  Enta i lmentsTable 3 lists several aspectual entailments.
A more comprehensive list can be foundin Klavans (1994) or Siegel (1998b).
Each entry in this table describes a linguisticphenomenon, a resulting entailment, and the constraints on aspectual class that applyif the resulting entailment holds.
For example, the simple present reading of an event,e.g., He jogs, denotes the habi tua l  reading, i.e., every day, whereas the simple presentreading of a state, e.g., He appears healthy, entails at the moment.These entailments serve two purposes: They further validate the three aspectualdistinctions, and they illustrate an array of inferences that can be made by an under-standing system.
However, these inferences can only be made after identifying theaspectual category of input clauses.1 These example sentences are modifications of samples from the corpus of novels described below.598Siegel and McKeown Improving Aspectual ClassificationTable 3Several aspectual entailments.If a clause occurring: necessarily entails: then it must be:in past progressive t nseas argument ofstoppedin simple present tensepast tense readingpast tense readingthe habitual readingNonculminated EventNonculminated Event or StateEvent2.3 Interpret ing Tempora l  Connect ives  and Modi f iersSeveral researchers have developed models that incorporate aspectual class to assesstemporal constraints between connected clauses (Hwang and Schubert 1991; Schubertand Hwang 1990; Dorr 1992; Passonneau 1988; Moens and Steedman 1988; Hitzeman,Moens, and Grover 1994).
For example, stativity must be identified to detect emporalconstraints between clauses connected with when.
For example, in interpreting,(7) She had good strength when objectively tested.
2the have state began before or at the beginning of the test event, and ended after or atthe end of the test event:haveI ?
It2z2_However, in interpreting,(8) Phototherapy was d iscont inued when the bilirubin came down to 13.the discontinue vent began at the end of the come event:comeI Idiscont inueI ISuch models also predict temporal relations between clauses combined with otherconnectives such as before, after, and until.Certain temporal modifiers are disambiguated with aspectual class.
For example,for an hour can denote the duration of a nonculminated vent, as in, Igazed at the sunsetfor an hour.
In this case, an hour is the duration of the gazing event.
However, whenapplied to a culminated event, it denotes the duration of the resulting state, as in, Ileft the room for an hour.
In this case, an hour is not the duration of the leaving event,but, rather, the duration of what resulted from leaving, i.e., being gone.2 These xamples ofwhen come from the corpus of medical discharge summaries described below.599Computational Linguistics Volume 26, Number 42.4 Aspectual Transformations and CoercionSeveral aspectual markers uch as those shown in Table 2 transform the aspectual c assof the clause they modify.
For example, a durationfor-PP, e.g., for 10 minutes, denotesthe time duration of a (nonculminated) process, resulting in a culminated process, e.g.,(9) I stared at it (process).
(10) I stared at it for 10 minutes (culminated process).Some aspectual auxiliaries also perform an aspectual transformation of the clausethey modify, e.g.,(11) I finished staring at it (culminated process).Aspectual coercion, a second type of aspectual transformation, can take placewhen a clause is modified by an aspectual marker that violates an aspectual constraint(Moens and Steedman 1988; Pustejovsky 1991).
In this case, an alternative interpre-tation of the clause is inferred which satisfies the aspectual constraint.
For example,the progressive marker is constrained to appear with an extended event.
Therefore, ifit appears with an atomic event, e.g.,(12) He hiccupped (point),the event is transformed to an extended event, e.g.,(13) He was hiccupping (process).in this case with the iterated reading of the clause (Moens and Steedman 1988).2.5 The First Problem: Fundamental AspectWe define fundamental aspectual class as the aspectual class of a clause before anyaspectual transformations or coercions.
That is, the fundamental spectual categoryis the category the clause would have if it were stripped of any and all aspectualmarkers that induce an aspectual transformation, as well as all components of theclause's pragmatic context that induce a transformation.
Fundamental spectual c ass istherefore a function of the main verb and a select group of complements, asillustratedin the previous two subsections.
It is the task of detecting fundamental spect hat weaddress in this article.
As established by some previous work in linguistics, adjunctsare to be handled separately from other clausal constituents in assessing aspectualclass (Pustejovsky 1995).An understanding system can recognize the aspectual transformations that haveaffected a clause only after establishing the clause's fundamental spectual category.Linguistic models motivate the division between a module that first detects fundamen-tal aspect and a second that detects aspectual transformations (Hwang and Schubert1991; Schubert and Hwang 1990; Dorr 1992; Passonneau 1988; Moens and Steedman1988; Hitzeman, Moens, and Grover 1994).
In principle, it is possible for this secondmodule to detect aspectual transformations that apply to any input clause, indepen-dent of the manner in which the core constituents interact to produce its fundamentalaspectual class.600Siegel and McKeown Improving Aspectual Classification2.6 Applications of Aspectual ClassificationAspectual classification is a required component of applications that perform nat-ural language interpretation, atural language generation, summarization, informa-tion retrieval, and machine translation tasks (Moens and Steedman 1988; Klavans andChodorow 1992; Klavans 1994; Dorr 1992; Wiebe et al 1997).
These applications requirethe ability to reason about time, i.e., temporal reasoning.Assessing temporal relationships i a prerequisite for inferring sequences of med-ical procedures in medical domains.
Many applications that process medical reportsrequire aspectual classification because a patient's medical progress and history areestablished as a series of states and events that are temporally related.
One task isto automatically complete a database ntry for the patient by processing a medicaldischarge summary detailing a patient's visit to the hospital.
For example, considerthe temporal relationship between the clauses connected with when in,(14) The small bowel became completely free when dissection wascontinued.
3In this case, the become culmination takes place at the onset of the continue process.However, in(15) The small bowel became completely free when dissection wasperformed.the become culmination takes place at the completion of the perform culminated process.Aspect is also crucial for tense selection in machine translation between certainpairs of languages because some languages have explicit perfective markers and oth-ers do not.
The perfective marker is used in many languages, such as Bulgarian andRussian, to indicate completedness.
Therefore, a system translating from a languagewithout explicit perfective markers, such as English, to one with explicit perfectivemarkers must first detect he aspectual category of an input phrase in order to deter-mine the form of the output (Stys 1991; Dorr 1992).
Aspect is also required for lexicalselection in machine translation since, for example, some languages, e.g., German andFrench, have different words for the two uses of for discussed previously in Section 2.3.Applications that incorporate aspect rely on the ability to first automatically iden-tify the aspectual category of a clause.
For example, Passonneau (1998) describes analgorithm that depends on what is called lexical aspect, the aspectual informationstored in the lexicon for each verb, and Dorr (1992) augments Jackendoff's lexical en-tries with aspectual information.
Combining linguistic indicators with machine learn-ing automatically produces domain-specialized aspectual lexicons.3.
Linguistic IndicatorsAspectually categorizing verbs is the first step towards aspectually classifying clauses,since many clauses in certain domains can be categorized based on their main verbonly (Siegel 1997, 1998b, 1999).
However, the most frequent category of a verb isoften domain dependent, so it is necessary to perform a specialized analysis for eachdomain.3 These xample sentences are modifications of samples from the corpus of medical discharge summariesdescribed below.601Computational Linguistics Volume 26, Number 4Table 4Fourteen linguistic indicators evaluated for aspectual c assification.Linguistic Indicator Example Clausefrequencynot or nevertemporal adverbno subjectpast/pres participleduration in-PPperfectpresent enseprogressivemanner adverbevaluation adverbpast tenseduration for-PPcontinuous adverb(not applicable)She can not explain why.I saw to it then.He was admitted to the hospital.... blood pressure going up.She built it in an hour.They have landed.I am happy.I am behaving myself.She studied iligently.They performed horribly.I was happy.I sang for ten minutes.She will live indefinitely.Naturally occurring text contains vast amounts of information pertaining to as-pectual classification encoded by aspectual markers that have associated aspectualconstraints.
However, the best way to make use of these markers is not obvious.
Ingeneral, while the presence of a marker in a particular clause indicates a constraint onthe aspectual class of the clause, the absence thereof does not place any constraint.Therefore, as with most statistical methods for natural anguage, the linguisticconstraints associated with markers are best exploited by a system that measures co-occurrence frequencies.
In particular, we measure the frequencies ofaspectual markersacross verbs.
This way, the aspectual tendencies ofverbs are measured.
These tenden-cies are likely to correlate with aspectual class (Klavans and Chodorow 1992).
Forexample, a verb that appears more frequently in the progressive is more likely todescribe an event.
The co-occurrence frequency of a linguistic marker is a linguisticindicator.The first column of Table 4 lists the 14 linguistic indicators evaluated to classifyverbs.
Each indicator has a unique value for each verb.
The first indicator, frequency,is simply the frequency with which each verb occurs over the entire corpus.
Theremaining 13 indicators measure how frequently each verb occurs in a clause witha linguistic marker listed in Table 4.
For example, the next three indicators listedmeasure the frequency with which verbs (1) are modified by not or never, (2) aremodified by a temporal adverb such as then or frequently, and (3) have no deep subject(e.g., passivized phrases uch as, She was admitted to the hospital).Nine of these indicators measure the frequencies of aspectual markers, each ofwhich have linguistic onstraints: perfect, progressive, duration in-PP, durationfor-PP,no subject, and four adverb groups.
The remaining five indicators were discovered ur-ing the course of this research.
Further details regarding the measurement of hese indi-cators, and the linguistic onstraints hat motivate them, can be found in Siegel (1998b).Linguistic indicators are measured over corpora automatically.
To do this, theautomatic dentification of individual constituents within a clause is required to detectthe presence of aspectual markers and to identify the main verb of each clause.
Weemploy the English Slot Grammar (ESG) parser (McCord 1990), which has previouslybeen used on corpora to accumulate aspectual data (Klavans and Chodorow 1992).ESG is particularly attractive for this task since its output describes a clause's deeproles, detecting, for example, the deep subject and object of a passivized phrase.602Siegel and McKeown Improving Aspectual Classification4.
Combining Linguistic Indicators with Machine LearningThere are several reasons to expect superior classification performance when employ-ing multiple linguistic indicators in combination rather than using them individually.While individual indicators have predictive value, they are predictively incomplete.This incompleteness has been illustrated empirically by showing that some indicatorshelp for only a subset of verbs (Siegel 1998b).
Such incompleteness i  due in part tosparsity and noise of data when computing indicator values over a corpus with lim-ited size and some parsing errors.
However, this incompleteness i  also a consequenceof the linguistic haracteristics of various indicators.
For example:?
While the progressive indicator is linguistically linked to extendedness, itis only indirectly linked to completedness.
It may be useful forpredicting whether a verb is culminated or nonculminated due to thefact that nonextended (i.e., atomic) verbs are more likely to beculminated than extended, i.e., points are rare.?
Many location verbs can appear in the progressive, ven in their stativesense, e.g., The book was lying on the shelf.?
Some aspectual markers uch as the pseudocleft and many manneradverbs test for intentional events in particular (not all events in general),and therefore are not compatible with all events, e.g., *I died diligently.?
Aspectual coercion such as iteration can allow a punctual event oappear in the progressive, .g.
She was sneezing for a week(point ~ process ~ culminated process) 4 (Moens and Steedman 1988).?
The predictive power of some indicators is uncertain, since severalmeasure phenomena that are not linguistically constrained by anyaspectual category, e.g., the present ense, durative for-PPs, frequencyand not~never indicators.Therefore, the predictive power of individual linguistic indicators is incomplete;only the subset of verbs that adhere to the respective constraints or trends can becorrectly classified.
Such incomplete indicators may complement one another whenplaced in combination.
Our goal is to take full advantage of the complete range ofindicators by coordinating and combining them.Machine learning methods can be employed to automatically generate a modelthat will combine indicator values.
Figure 1 shows a system overview for this process(with additional details that are addressed below in Section 5.1.1).
This diagram out-lines a generic system that combines numerical indicators with machine learning for aclassification problem, in natural language understanding or otherwise.
Indicators arecomputed over an automatically parsed corpus.
Then, in the Combine Indicators tage,supervised training cases are used to automatically generate a model (ClassificationMethod) with supervised machine learning.
This method (the hypothesis) inputs in-dicator values and outputs the aspectual class.
The hypothesis then evaluated overunseen supervised test cases.4 In this example, for a week requires an extended vent, hus the first coercion.
However, this phrase alsomakes an event culminated, thus the second transformation.603Computational Linguistics Volume 26, Number 4States vs. Events~ , 1 5 9 , 8 9 1  wordsManual I.
1.863 clauses I Parser I ESGClassification I"( ,Test_)739 ( Training )739 t l Co.mp,t; i t\ \ /~)e(ISlOn I reeI IFigure 1System overview ith statistics of the medical discharge summary data for distinguishingaccording to stativity.There are five advantages to automating this process with machine learning:?
The cost of manually generating a model, which is often prohibitive, isavoided.?
Biases introduced by a human engineer are avoided.?
Automated approaches are extensible to multiple natural anguageclassification problems, across multiple domains and multiple languages.?
Once a system has been trained to distinguish verbs by indicator values,it can automatically classify all the verbs that appear in a corpus,including unseen verbs that were not included in the supervised trainingsample.?
Resulting models may reveal new linguistic insights.The remainder of this section describes the three supervised learning methodsevaluated for combining linguistic indicators: logistic regression, decision tree induc-tion, and a genetic algorithm.
At the end of this section, the designs of these threemethods are compared.
In the following section, the three are compared empirically:each method is evaluated for classification according to both stativity and completed-ness.604Siegel and McKeown Improving Aspectual Classification4.1 Logistic RegressionAs suggested by Klavans and Chodorow (1992), a weighted sum of multiple indica-tors that results in one "overall" indicator may provide an increase in classificationperformance.
This method follows the intuition that each indicator correlates with theprobability that a verb belongs in a certain class, but that each indicator has its ownunique scale, polarity, and predictive significance and so must be weighted accord-ingly.For example, consider the problem of using in combination (only) two indicators,not~never and progressive, to determine the stativity of verbs in a corpus of medicalreports.
The former indicator may show higher values for stative verbs since diag-noses (i.e., states) are often ruled out in medical discharge surrunaries, e.g., "The patientwas not hypertensive," but procedures (i.e., events) that were not done are not usuallymentioned, e.g., "?An examination was not performed."
The progressive indicator is lin-guistically predicted to show higher values for event verbs in general, so its polarityis the opposite of the not~never indicator.
Furthermore, a certain group of stative verbs(including, e.g., sit, lay, and rest) can also occur in the progressive, so this indicatormay be less powerful in its predictiveness.
Therefore, the best overall indicator mayresult from adding the value of the not or never indicator, as multiplied by a negativeweight, to the value of the progressive indicator, as multiplied by a positive weight oflesser magnitude.
A detailed examination of the weights resulting from learning andtheir linguistic interpretation is described below in Section 5.1.4.This set of weights can be determined by a standard gradient descent algorithm(see, for example, Mitchell \[1997\]).
However, the algorithm employed here is logisticregression (Sjantner and Duffy 1989), a popular technique for binary classification.This technique determines a set of weights for a linear model, which are applied incombination with a certain nonlinear model.
In particular, the iterative reweightedleast squares algorithm (Baker and Nelder 1989) is employed, and the inverse logic(nonlinear) function is applied.
The Splus statistical package was used for the inductionprocess.4.2 Decision Tree InductionAnother method capable of modeling nonlinear elationships between indicators is adecision tree.
An example tree is shown in Figure 2 (with additional details discussedin Section 5.1.4).
Each internal node of a decision tree is a choice point, dividing anindividual indicator into two ranges of possible values by way of a threshold.
Each leafnode is labeled with a classification (e.g., state or event, in the case of the tree shown).In effect, this is simply a set of nested if-then-else statements.
Given the set of indicatorvalues corresponding to a verb, that verb's class is predicted by traversing the treefrom the root node to a leaf as follows: at each node, the arc leading downward tothe left or right is traversed according to the question posed about an indicator valueat that node.
When a leaf node is reached, its label is then taken to be the verb'sclassification.
For example, if the frequency is less than 2,013, the arc to the left istraversed.
Then, if the not~never indicator is greater than or equal to 3.48%, the arc tothe right is traversed.
Finally, if the frequency is greater than or equal to 314, the arcto the right is traversed, arriving at a leaf labeled, State.This representation e ables complex interactions between indicator values.
In par-ticular, if one indicator can only help classify a proper subset of verbs, while anotherapplies only to a subset hat is distinct but intersects with the first, certain ranges ofindicator values may delimit verb groups for which the indicators complement oneanother.
An example of such delimitation within a learned decision tree is illustratedbelow in Section 5.1.4.605Computational Linguistics Volume 26, Number 4< 2,013"Izot" <28/29temporal adv < 0.68%) ~, j requency <<4~'7488 42/~-~'61 19/19106/132 9/10Figure 2Top portion of decision tree automatically created to distinguish events from states.
Leftwardarcs are traversed when comparisons test true, rightward arcs when they test false.
The valuesunder each leaf indicate the number of correctly classified examples in the correspondingpartition of training cases.
The full tree has 59 nodes and achieves 93.9% accuracy over unseentest cases.Table 5Default decision tree induction parameters implemented by Splus.Minimum partition size before first split:Minimum partition size for additional splits:Node selection criterion:Node purity threshold:510deviance = -2 times log-likelihooddeviance < .01The most popular method of decision tree induction, which we employ here, isrecursive partitioning (Quinlan 1986; Breiman et al 1984).
This method "grows" adecision tree by expanding it from top to bottom.
Initially, the tree has only one node,a leaf, corresponding to the entire set of training examples.
Then, the following processis repeated: At each leaf node of the tree, an indicator and threshold are selected suchthat the examples are best distinguished according to aspectual class.
This adds twoleaves beneath the node, and distributes the examples to the new leaves accordingly.Table 5 shows the parameters used to control decision tree growth.
The criterionoptimized for each split is deviance, implemented as minus twice the log likelihood.The Splus statistical package was used for the induction process.
We also comparedthese results to standard CART decision tree induction (Friedman 1977; Breiman et al1984).4.3 Genetic ProgrammingAn alternative method that enables arbitrary mathematical combinations of indicatorsis to generate ftmction trees that combine them.
A popular method for generating606Siegel and McKeown Improving Aspectual ClassificationTable 6Applying genetic programming to induce a function tree that combines linguistic indicators.Objective:Terminal set:Function set:Training cases:Raw fitness:Parameters:Identification of best of run:Function tree to combine linguistic indicators14, corresponding tothe set of linguistic indicators.ADD, MULTIPLY, and DIVIDE739 (stativity) or 307 (completedness) verb instances.Accuracy over training cases, when best threshold is selected.Number of generates =50,000, population size = 500,5-member tournament selection, steady state population(Syswerda 1989).Highest raw fitness.such function trees is a genetic algorithm (GA) (Holland 1975; Goldberg 1989), whichis modeled after population genetics and natural selection.
The use of GAs to generatefunction trees (Cramer 1985; Koza 1992) is often called genetic programming (GP).Inspired by Darwinian survival of the fittest, the GA works with a pool of initiallyrandom hypotheses (in this case, function trees), stochastically performing eneticrecombination a d mutation to propagate or create better individuals, which replaceold or less good individuals.
Recombination between function trees usually consistsof selecting a subtree from each individual, and swapping them, thereby creating twonew individuals (Cramer 1985; Koza 1992).
For random mutation, a randomly chosensubtree can be replaced by a new randomly created subtree (Koza 1992).
Becausethe genetic algorithm is stochastic, each run may produce a different function tree.Therefore, performance is evaluated over the models produced by multiple runs.The function trees are generated from a set of 17 primitives: the binary functionsADD, MULTIPLY, and DIVIDE, and 14 terminals corresponding to the 14 indicatorslisted in Table 4, which are occurrence frequencies.
The GA parameters are shown inTable 6.This representation e ables trategic ombinations of indicator values that aremathematical, s opposed to logical, manipulations.
For example, two indicators thatare high in predictiveness can be multiplied together, and perhaps added to an indi-cator with complementary but less reliable predictiveness.The set of primitives was established empirically; other primitives uch as con-ditional functions, subtraction, and random constants failed to improve performance.The polarities for several indicators were reversed according to the polarities of theweights established by logistic regression for stativity.
Runs of the GA maintain apopulation size of 500 and end after 50,000 new individuals have been evaluated.A threshold must be selected for both logistic and function tree combinations ofindicators o overall outputs can be discriminated to maximize classification perfor-mance.
For both methods, the threshold is established over the training set and frozenfor evaluation over the test set.4.4 Selecting and Comparing Learning MethodsThe use of machine learning techniques to combine numerical indicators for classi-fication problems in general is a well-established practice and includes work withdecision trees (Fayyad and Irani 1992), logistic regression (Sjantner and Duffy 1989),and GP (Koza 1992; Tackett and Carrel 1994).
Applications include doctrment classifi-cation (Masand 1994), image classification (Tackett 1993), and stock market prediction(Allen and Karjalainen 1995).607Computational Linguistics Volume 26, Number 4When combining linguistic indicators in particular, the choice of hypothesis repre-sentation determines the type of linguistic insights that can result.
Decision trees can beanalyzed by examining the subset of verbs that are sorted to a particular node and theconstraints on indicator values that put them there.
A path from the root to any nodeis a rule that puts constraints on indicator values; this rule can be examined to deter-mine if it has a linguistic interpretation.
The weights produced by logistic regressioncan be examined to see which indicators are most highly weighted for each classifi-cation task.
In addition to this, surprisingly, we discovered a decision tree-like ruleencoded by these weights, as described below in Section 5.1.4.
On the other hand, afunction tree, such as that produced by GP, is more difficult to analyze manually, sinceit is a mathematical combination.
However, GP's performance was tested due to thepotential improvement in classification performance of such a flexible representationfor numerical functions.The relative merit of various learning algorithms is often difficult to ascertain,even after results have been collected.
In general, each learning algorithm relies on aninductive bias, that may produce better esults for some data than for others (Mitchell1997).
When applied to linguistic indicators, there is little knowledge about how indica-tors interact, since initial analysis examines individual indicators in isolation; machinelearning is being used to automatically discover their interaction.
Intuition guides thechoice and design of algorithms, such as the rationale for each of the three techniquesdescribed above in this section.
Moreover, beyond the particular characteristics of anygiven classification task, the particular data sample to which a learning technique isapplied may have a large effect on performance, for example, due to the distributionand size of the training set, differences between the distributions of the training andtest sets, and even the particular order in which the training cases are listed.The three learning methods we examine in detail are diverse in their representa-tion abilities, as described in this section, and, arguably, are therefore representativeof the abilities of learning algorithms in general when applied to the same data.
Apilot study showed no further improvement in accuracy or in recall trade-off or ei-ther classification problem by another four standard learning algorithms: naive Bayes(Duda and Hart 1973), Ripper (Cohen 1995), ID3 (Quinlan 1986) and C4.5 (Quin-lan 1993).
Furthermore, using metalearning to combine multiple learning methodshierachically (sometimes called stacked generalization; Chan and Stolfo \[1993\], andWolpert \[1992\]), according to the JAM (Java Agents for Metalearning) model (Stolfo etal.
1997), produced equivalent results.
However, this may be due to the limited sizeof our supervised ata.5.
Supervised Learning: Method and ResultsIn this section, we evaluate the models produced by the three supervised learningmethods.
These methods are applied to combine the linguistic indicators computedover the medical discharge summaries in order to distinguish between states andevents.
Then, the methods are applied to indicators computed over novels in order todistinguish between culminated and nonculminated events.
At the end of this section,these results are compared to one another, and to an informed baseline classificationmethod.The two data sets are summarized in Table 7.
Table 8 illustrates the schema ofinputs for supervised learning.
There are a total of 14 continuous attributes for thetwo binary learning problems.
All attributes are proportions except frequency, whichis a positive integer.
This data is available at http://www.cs.columbia.edu/-evs/VerbData/.608Siegel and McKeown Improving Aspectual ClassificationTable 7Two classification problems on different data sets.Stativity CompletednessCorpus 3,224 medical discharge summaries 10 novelsCorpus size 1,159,891 words 846,913 wordsParsed clauses 97,973 75,289Training clauses 739 (634 events) 307 (196 culminated)Testing clauses 739 (619 events) 308 (195 culminated)Verbs in test set 222 204Clauses excluded be-, have-clauses stative clausesUnsupervised results N/A stativity (see Section 6)Table 8Schema of inputs for supervised learning.
Fourteen continuous attributes for two binarylearning problems: stativity and completedness.Linguistic Indicator stativity completednessClass yes..no yes..nomanner adverb 0.00..0.29 0.00..0.23duration in-PP 0.00..0.13 0.00..0.12continuous adverb 0.00..1.00 0.00..0.33temporal adverb 0.00..0.40 0.00..0.97not or never 0.47..1.00 0.58..1.00duration for-PP 0.00..0.15 0.00..1.00perfect 0.00..0.35 N/Aperfect (not progressive) N/A 0.00..0.50past/pres participle 0.00..1.00 0.33..1.00evaluation adverb 0.00..0.46 0.00..0.95no subject 0.00..1.00 0.00..1.00past tense 0.00..1.00 0.00..1.00present tense 0.00..1.00 0.00..1.00frequency 1..2,131 1..13,882not progressive 0.00..1.00 0.00..0.50For both classification problems, we show that individual indicators correlate withaspectual class, but attain limited classification accuracy when used alone.
Supervisedlearning is then used to combine indicators, improving classification performance andproviding linguistic insights.
The results of unsupervised learning are given in Sec-tion 6.Classification performance is evaluated according to a variety of performance mea-sures, since some applications weigh certain classes more heavily than others (Brodley1996; Cardie and Howe 1997).
An alternative to evaluation based on overall accu-racy is to measure the recall values for the dominant and nondominant (i.e., majorityand minority) categories eparately.
A favorable recall trade-off is achieved if the re-call of the nondominant category can be improved with no loss in overall accuracywhen compared against some baseline (Cardie and Howe 1997).
Achieving such atrade-off is nontrivial; it is not possible, for example, for an uninformed approach thatassigns everything to the dominant category.
A favorable recall trade-off presents anadvantage for applications that weigh the identification of nondominant instances, e.g.,nonculminated clauses, more heavily.
For example, correctly identifying the use of fordepends on identifying the aspectual class of the clause it modifies (see Section 2.3).
Asystem that surmnarizes the duration of events which incorrectly classifies She ran (for609Computational Linguistics Volume 26, Number 4a minute) as culminated will not detect hat for a minute describes the duration of therun event.
As another example, it is advantageous for a medical system that retrievespatient diagnoses to identify stative clauses, since there is a correspondence b tweenstates and medical diagnoses.Classification performance is evaluated over verb instances, that is, clauses inwhich the verb appears as the main verb.
5 Because of this, as discussed further inSection 5.4 below, the same verb may appear multiple times in the training and testingsets.
This measure is beneficial in several ways:?
Measured classification performance r flects the true distribution of theverbs--some are more frequent than others.?
Ambiguous verbs may appear with multiple aspectual categories,reflecting the true distribution of the data.?
In related work, clausal constituents other than the verb can beincorporated tohelp resolve ambiguity and alleviate verb sparsity (Siegel1998a, 1998b).5.1 States versus EventsOur experiments distinguishing states and events were performed across a corpusof 3,224 medical discharge summaries, with a total of 1,159,891 words.
A medicaldischarge summary describes the symptoms, history, diagnosis, treatment, and out-come of a patient's visit to the hospital.
Each summary consists of unstructured text,divided into several sections with titles such as: "History of Present Illness," and"Medical Summary."
The text under four of these titles was extracted and parsed withthe English Slot Grammar, resulting in 97,973 clauses that were parsed fully, with noself-diagnostic errors (ESG produced error messages on some of this corpus' com-plex sentences).
Other sections in the summaries were ignored since they report thenumerical results of certain medical tests, interspersed with incomplete sentences.Be and have, the two most popular verbs, covering 31.9% of the clauses in thiscorpus, are handled separately from all other verbs.
Clauses with be as their mainverb, composing 23.9% of the corpus, always denote a state.
Therefore, we can focusour efforts on the remaining clauses.
Clauses with have as their main verb, composing8.0% of the corpus, are highly ambiguous, and have been addressed separately byconsidering the direct object of such clauses (Siegel 1998a, 1998b).5.1.1 Manual Marking for Supervised Data.
As a basis for evaluating our approach,1,851 clauses from the parsed corpus were manually marked according to their fun-damental stativity.
In contrast to the entire parsed corpus (97,973 clauses), each clausein this set of supervised ata had to be judged by a linguist.
This subset was selecteduniformly from clauses in the corpus that had main verbs other than be and have.
Asa linguistic test for marking, each clause was tested for readability with What happenedwas .
.
.
.
Manual labeling followed a strict set of linguistically motivated guidelinesin order to ascertain fundamental spectual class.
Modifiers that result in aspectualtransformations, such as durativefor-PPs, and in exceptions, uch as not, were ignored.A comparison between human markers for this test was performed over a differentcorpus, and is reported below in Section 5.2.1.5 For evaluation over sets of unique verbs, see Siegel (1998b).610Siegel and McKeown Improving Aspectual ClassificationTable 9Indicators discriminate between states and events.Linguistic Indicator Stative Mean Event Mean T-test P-value:frequency 932.89 667.57 0.0000not or never 4.44% 1.56% 0.0000temporal adverb 1.00% 2.70% 0.0000no subject 36.05% 57.56% 0.0000past/pres participle 20.98% 15.37% 0.0005duration in-PP 0.16% 0.60% 0.0018perfect 2.27% 3.44% 0.0054present tense 11.19% 8.94% 0.0901progressive 1.79% 2.69% 0.0903manner adverb 0.00% 0.03% 0.1681evaluation adverb 0.69% 1.19% 0.1766past tense 62.85% 65.69% 0.2314duration for-PP 0.59% 0.61% 0.8402continuous adverb 0.04% 0.03% 0.8438Because of manually identified parsing problems (verb or direct object incorrectlyidentified), 373 clauses were rejected.
This left 1,478 clauses, which were dividedequally into training and testing sets of 739 clauses each.Figure 1 (see Section 4) shows the system overview with details regarding themedical discharge summary corpus used in this study.
As this shows, the values forlinguistic indicators are computed across the entire parsed corpus.
This is a fullyautomatic process.
Then, the 739 training examples are used to derive a mechanism,e.g., a decision tree, for combining multiple indicators.
The combination of indicatorsachieves an increase in classification performance.
This increase in performance is thenvalidated over the 739 unseen test cases.5.1.2 Upper and Lower Bounds in Accuracy.
Of clauses with main verbs other thanbe and have, 83.8% are events.
Therefore, simply classifying every verb as an eventachieves an accuracy of 83.8% over the 739 test cases.
However, this approach classifiesall state clauses incorrectly, achieving a stative recall of 0.0%.
This method serves asa baseline for comparison, since we are attempting to improve over an uninformedapproach.
6One limitation to our approach places an upper bound on classification accuracy.Our approach examines only the main verb, since linguistic indicators are computedfor verbs only.
For example, a verb occurring three times as an event and twice asa state will be misclassified at least two of the five times.
This limits classificationaccuracy to a maximum of 97.4% over the test cases due to the presence of verbs withmultiple classes.
The ramifications of this limitation are explored below in Section 5.4.5.1.3 Individual  Indicators.
The values of the 14 indicators listed in Table 9 werecomputed, for each verb, across the 97,973 parsed clauses from our corpus of medicaldischarge summaries.
As described in Section 3, each indicator has a unique value foreach verb, which corresponds to the frequency of the aspectual marker with the verb(except verb frequency,  which is an absolute measure over the corpus).6 Similar baselines for comparison have been used for many classification problems (Duda and Hart1973), e.g., part-of-speech tagging (Church 1988; Allen 1995).611Computational Linguistics Volume 26, Number 4The second and third columns of Table 9 show the average value for each indicatorover stative and event clauses, as measured over the training examples (which excludebe and have).
These values are computed solely over the 739 training cases in order toavoid biasing the classification experiments in the sections below, which are evaluatedover the unseen test cases.
For example, for the not~never indicator, stative clauseshave an average value of 4.44%, while event clauses have an average value of 1.56%.This makes sense, since diagnoses are often ruled out in medical discharge summaries,e.g., The patient was not hypertensive, but procedures that were not done are not usuallymentioned, e.g., ?An examination was not performed.The differences in stative and event means are statistically significant (p < .01)for the first seven of the 14 indicators listed in Table 9.
The fourth column showsthe results of t-tests that compare indicator values over stative verbs to those overevent verbs for each indicator.
For example, there is less than a 0.05% chance that thedifferences between stative and event means for the first seven indicators listed is dueto chance.
The differences in average value for the bottom seven indicators were notconfirmed to be significant with this small sample size (739 training examples).A positive correlation between indicator value and verb class does not necessarilymean an indicator can be used to increase classification accuracy over the baselineof 83.8%.
This is because of the dominance of events among the testing examples;a threshold to distinguish verbs that correctly classifies more than half of each classwill have an accuracy lower than the baseline if the number of states correctly clas-sified is less than the number of events misclassified.
To examine this, each indica-tor was tested individually for its ability to improve classification accuracy over thebaseline by establishing the best classification threshold over the training data.
Theperformance of each indicator was validated over the testing data using the samethreshold.Only the frequency indicator succeeded in significantly improving classificationaccuracy.
Both frequency and occurrences with not or never improved classificationaccuracy on the training data over the baseline obtained by classifying all clausesas events.
To validate this improved accuracy, the thresholds established over thetraining set were used over the test set, with resulting accuracies of 88.0% and 84.0%,respectively.
Binomial tests show the first of these, but not the second, to be a significantimprovement over the baseline of 83.8%.This improvement in accuracy was achieved simply by discriminating the pop-ular verb show as a state, but classifying all other verbs as events.
Although manydomains may primarily use show as an event, in its appearances in medical dischargesmnmaries, such as His lumbar puncture showed evidence of white cells, show primarilydenotes a state.
This observation illustrates the importance of empirical techniques forlexical knowledge acquisition.5.1.4 Indicators Combined with Learning.
All three machine learning methods uc-cessfully combined indicator values, improving classification accuracy over the base-line measure.
As shown in Table 10, the decision tree's accuracy was 93.9%, GP's func-tion trees had an average accuracy of 91.2% over seven runs, and logistic regressionachieved an 86.7% accuracy (Baseline 2 is discussed below in Section 5.4).
Binomialtests showed that both the decision tree and GP achieved a significant improvementover the 88.0% accuracy achieved by the frequency indicator alone.
These resultsshow that machine learning methods can successfully combine multiple numericalindicators to improve verb classification accuracy.The increase in the number of stative clauses correctly classified, i.e., stative recall,illustrates an even greater improvement over the baseline.
As shown in Table 10, stative612Siegel and McKeown Improving Aspectual ClassificationTable 10Comparison of three learning methods, optimizing for accuracy, and two performancebaselines, distinguishing states from events.Overall Accuracy States EventsRecall Precision Recall PrecisionDecision tree 93.9% 74.2% 86.4% 97.7% 95.1%GP (7 runs) 91.2% 47.4% 97.3% 99.7% 90.7%Logistic 86.7% 34.2% 68.3% 96.9% 88.4%Baseline 1 83.8% 0.0% 100.0% 100.0% 83.8%Baseline 2 94.5% 69.2% 95.4% 99.4% 94.3%Table 11Comparing training and test performance on three learning methods, distinguishing statesfrom events.Training Accuracy Testing AccuracyDecision tree 96.3% 93.9%GP 93.4% 91.2%Logistic 88.8% 86.7%Baseline 85.8% 83.8%recalls of 74.2%, 47.4%, and 34.2% were achieved by the three learning methods, ascompared to the 0.0% stative recall achieved by Baseline 1, while only a small lossin recall over event clauses was suffered.
The baseline does not classify any stativeclauses correctly because it classifies all clauses as events.
This difference in recall ismore dramatic than the accuracy improvement because of the dominance of eventclauses in the test set.Overfitting was moderate for each of the three supervised learning algorithms.As shown in Table 11, each learning method's performance over the training datawas about two points higher than that over the test data.
This corresponds to a two-point difference in baseline performance, which is due to a higher proportion of eventclauses in the training data.The thresholds established to discriminate the outputs of GP's function trees gener-alize well to unseen data.
When inducing these function trees with the GA, the trainingset is used to form the tree, and to select a threshold that best discriminates betweenverbs of the different classes.
There is the potential that a threshold determined overthe training cases will not generalize well when evaluated over the test cases.
To testthis, for each of the seven function trees generated by the GA to distinguish betweenstates and events, the best threshold was selected over the test cases.
For five of thefunction trees, there was no threshold that increased classification accuracy beyondthat attained by the threshold established over the training cases.
For the other two,a threshold was found that allowed for one more of the 739 test cases to be correctlyclassified.In the remainder of this section, we compare the resulting models of the threesupervised learning method and contrast he ways in which they combine indicators.Logistic Regression.
Logistic regression successfully combined the 14 linguistic indi-cators, achieving an accuracy of 86.7%, as shown in Table 10.
This is a significantimprovement over the baseline accuracy of 83.8%, as measured with a binomial test.Furthermore, a stative recall of 34.2% was achieved.613Computational Linguistics Volume 26, Number 4Table 12Weights produced by logistic regression to distinguish between stative and event verbs.Linguistic Indicator Logistic Weight T-test P-valuemanner adverb 11.04744 0.1681duration in-PP 0.06209624 0.0018continuous adverb - 0.04168417 0.8438temporal adverb 0.02127572 0.0000not or never 0.01714499 0.0000durationfor-PP - 0.01019155 0.8402perfect 0.009528091 0.0054past/pres participle 0.006981148 0.0005evaluation adverb 0.005695407 0.1766no subject 0.002742867 0.0000past tense 0.002586572 0.2314present tense 0.002409898 0.0901frequency - 0.001264895 0.0000not progressive - 0.0009369231 0.0903The particular weighting scheme resulting from logistic regression for this data ef-fectively integrates a decision tree type rule, along with the usual weighting of logisticregression.
This is illustrated in Table 12, which shows the weights automatically de-rived by logistic regression for each of the 14 linguistic indicators.
The value assignedto the manner adverb indicator, 11.04744, far outweighs the other 13 weights.
At firstglance, it may appear that this weighting scheme favors the manner adverb indicatorover all other indicators.
However, as shown in Table 13, manner adverb indicatorvalues are 0.0% for all verbs in the training set except he eight indicated, all of whichdenote vents.
Therefore, the large weight assigned to the manner adverb indicator isonly activated for those verbs, which are therefore ach classified as events, regardlessof the remaining 13 indicator values.
For all other verbs, the remaining 13 indicatorvalues determine the classification.This rule cannot increase accuracy over the baseline without the remaining 13indicators, since it does not positively identify any states--it only identifies events,which are all correctly classified by the baseline.
Therefore, it is only useful becausethe overall model also correctly identifies ome stative clauses.Genetic Programming.
GP successfully combined the 14 linguistic indicators, achievingan average accuracy of 91.2%, as shown in Table 10.
This is a significant improvementover the baseline accuracy of 83.8%, according to a binomial test.
Furthermore, a stativerecall of 47.4% was achieved.GP improved classification performance by emphasizing a different set of indica-tors than those emphasized by logistic modeling.
Figure 3 shows an example functiontree automatically generated by the GA, which achieved 92.7% accuracy.
Note that thisclassification performance was attained with a subset of only five linguistic indicators:duration in-PP, progressive, not or never, past tense, and frequency.
Two of these areranked lowest by logistic regression: frequency and progressive.
Furthermore, manneradverb, ranked highest by logistic regression, is not incorporated in this function treeat all.
This may be because this indicator only applies to a small number of verbs, asshown in Table 13, and because an/f-rule such as that captured by logistic regression isdifficult o encode with a function tree with no conditional functions.
Overall, we canconclude that multiple proper subsets of linguistic indicators are useful for aspectualclassification if combined with the correct model.614Siegel and McKeown Improving Aspectual Classification(/ (+ (+ (* (/ DurationInPP (+ Progressive (+ NotNever NotNever))) (+Progressive 75)) (/ (* (+ Progressive PastTense) Progressive)NegFrequency)) NotNever) DurationInPP)Figure 3Example function tree designed by a genetic algorithm to distinguish between stative andevent verbs, achieving 92.7% accuracy.Table 13Linguistic rule discovered by logistic regression.If frequency with verbs this Frequency in thenmanner adverbs is ... applies to: Training Set classify as:> 0.0% adjust 1 Eventcontinue 16decline 2decrease 4improve 5increase 4progress 2resolve 70.0% all other verbs 699 depending on other 13 indicatorsDecision Tree Induction.
Decision tree induction successfully combined the 14 linguisticindicators, achieving the highest accuracy of the three supervised learning algorithmstested, 93.9%, as shown in Table 10.
This is a significant improvement over the baselineaccuracy of 83.8%, as measured with a binomial test.
Furthermore, a stative recall of74.2% was achieved.
The top portion of the tree created with recursive partitioningis shown in Figure 2 (in Section 4.2, where it is explained).
Note that the root nodesimply distinguishes the stative verb show with the frequency indicator, as describedin Section 5.1.3.To achieve this increase in classification performance, the decision tree divided thetraining cases into relatively small partitions of verbs.
Table 14 shows the distributionof training case verbs examined by the highlighted tree node in Figure 2.
As seenby tracing the path from the root to the highlighted node, these are the verbs withfrequency less than 2,013 across the corpus, and modified by not or never at least 3.48%of the time.
From this subset, the highlighted node distinguishes the three verbs withfrequency at least 314, shown in capitals in Table 14, as states.
This is correct for all19 instances of these three verbs, and does not misclassify any event verbs.This example illustrates a benefit of distinguishing verbs based on indicator valuescomputed over large corpora.
Most of the verbs in Table 14 appear in the trainingset a small number of times, so it would be difficult for a classification system togenerate rules that apply to these individual verbs.
Rather, since our system drawsgeneralizations over the indicator values of verbs, it identifies tative verbs withoutmisclassifying any of the event verbs shown.Classification performance is equally competitive without he frequency indicator.Since frequency is the only indicator that can increase accuracy by itself, and since it isthe first discriminator f the decision tree, it may appear that :frequency highly dom-inates the set of indicators.
This could be problematic, since the relationship betweenverb frequency and verb category may be particularly domain dependent, in whichcase frequency could be less informative when applied to other domains.
However,when decision tree induction was employed to combine only the 13 indicators other615Computational Linguistics Volume 26, Number 4Table 14Verb distribution in the partition of training examples orted to the decision tree nodehighlighted in Figure 2.
The three stative verbs shown in capitals are distinguished at thisnode, with no event verbs misclassified.Events :get 3 talk 1 persue 1 drive 1transfuse 2 suggest 1 provide 1 drink 1respond 2 subside 1 pass 1 document 1lose 2 sleep 1 notice 1 detect 1live 2 retract 1 load 1 change 1interfere 2 repair 1 limit 1 achieve 1breathe 2 relieve 1 lift 1 regain 1visualize 1 recognize 1 help 1tell 1 radiate 1 explain 1States:FEEL 12 associate 3 want 1know 4 remember 2 support 1APPEAR 4 believe 2 indicate 1REQUIRE 3 accompany 2 desire 1concern 1account 1Table 15Indicators discriminate between culminated and nonculminated vents.Linguistic Indicator Culminated Mean Nonculminated Mean T-test P-valueperfect 7.87% 2.88% 0.0000temporal adverb 5.60% 3.41% 0.0000manner adverb 0.19% 0.61% 0.0008progressive 3.02% 5.03% 0.0031past/pres participle 14.03% 17.98% 0.0080no subject 30.77% 26.55% 0.0241duration in-PP 0.27% 0.06% 0.0626present tense 17.18% 14.29% 0.0757duration for-PP 0.34% 0.49% 0.1756continuous adverb 0.10% 0.49% 0.2563:frequency 345.86 286.55 0.5652not or never 3.41% 3.15% 0.6164evaluation adverb 0.46% 0.39% 0.7063past tense 53.62% 54.36% 0.7132than frequency, the resulting decision tree achieved 92.4% accuracy and 77.5% stativerecall.
Therefore, our results are not entirely dependent on the f requency indicator.5.2 Culminated versus Noncu lminated  EventsIn medical discharge summaries, nonculminated event clauses are rare.
Therefore,our experiments for classification according to completedness are performed across acorpus of 10 novels, comprising 846,913 words.
These novels were parsed with theEnglish Slot Grammar, resulting in 75,289 clauses that were parsed fully, with no self-diagnostic errors.
The values of the 14 indicators listed in Table 15 were computed,for each verb, across the parsed clauses.
Note that in this section, the perfect indicatordiffers in that we ignore occurrences of the perfect in clauses that are also in theprogressive, since any progressive clause can appear in the perfect, e.g., I have beenpainting.616Siegel and McKeown Improving Aspectual Classification5.2.1 Manual Marking for Supervised Data.
To evaluate the performance of our sys-tem, we manually marked 884 clauses from the parsed corpus according to their aspec-tual class.
These 884 were selected evenly across the corpus from parsed clauses thatdo not have be as the main verb, since we are testing a distinction between events.Of these, 109 were rejected because of manually identified parsing problems (verbor direct object incorrectly identified), and 160 were rejected because they describedstates.
This left 615 event clauses over which to evaluate classification performance.The division into training and test sets was derived such that the distribution of classeswas equal between the two sets.
This precaution was taken because preliminary ex-periments indicated ifficulty in demonstrating a significant increase in classificationaccuracy for completedness.
This process resulted in 307 training cases (196 culmi-nated) and 308 test cases (195 culminated).
Since 63.3% of test cases are culminatedevents, simply classifying every clause as culminated achieves an accuracy of 63.3%over the 308 test cases (Baseline 1A).
This method serves as a baseline for compari-son.We used linguistic tests that were selected for this task by Passonneau (1988) fromthe constraints and entailments listed in Tables 2 and 3.
First, the clause was tested forstativity with What happened was .
.
.
.
Then, as an additional check, we tested with thefollowing rule: if a clause can be read in a pseudocleft, i  is an event, e.g., What itsparentsdid was run off, versus *What we did was know what is on the other side.
Second, if a clausein the past progressive necessarily entails the past tense reading, the clause describesa nonculminated event.
For example, We were talking just like men (nonculminated)entails that We talked just like men, but The woman was building a house (culminated) doesnot necessarily entail that The woman built a house.
The guidelines described above inSection 5.1 were used in order to test for fundamental spectual class.Cross-checking between linguists shows high agreement.
In particular, in a pilotstudy manually annotating 89 clauses from the corpus of novels, two linguists agreed81 times (i.e., 91%).
Informal analysis suggests the remaining disagreement could befurther divided in half by a few simple refinements of the annotation protocol.
Fur-thermore, of 57 clauses agreed to be events, 46 were annotated in agreement withrespect o completedness.The verb say, which is a frequent point, i.e., nonculminated and nonextended,poses a challenge for manual marking.
Points are misclassified by the test for com-pletedness described above since they are nonextended and therefore cannot be placedin the progressive.
Therefore, say, which occurs nine times in the test set, was markedincorrectly as culminated.
After some initial experimentation, we switched the classof each occurrence of say in our supervised ata to nonculminated.
This change to saymade the class distribution slightly uneven between training and test data.5.2.2 Individual Indicators.
The second and third columns of Table 15 show the av-erage value for each indicator over culminated and nonculminated event clauses, asmeasured over the training examples.
For example, for the perfect ense indicator, cul-minated clauses have an average value of 7.87%, while nonculminated clauses havean average value of 2.88%.
These values were computed solely over the 307 trainingcases in order to avoid biasing the classification experiments in the sections below,which are evaluated over the unseen cases.The differences in culminated and nonculminated means are statistically significant(p < .05) for the first six of the 14 indicators listed in Table 15.
The fourth column showsthe results of t-tests that compare indicator values over culminated verbs to those overnonculminated verbs.
For example, there is less than a 0.05% chance that the differencesbetween culminated and nonculminated means for the first six indicators listed is due617Computational Linguistics Volume 26, Number 4Table 16Comparison of four learning methods, optimized for accuracy, and three performancebaselines distinguishing culminated from nonculminated vents.Overall Accuracy Culminated NonculminatedRecall Precision Recall PrecisionCART 74.0% 86.2% 76.0% 53.1% 69.0%Logistic 70.5% 83.1% 73.6% 48.7% 62.5%Logistic 2 67.2% 81.5% 71.0% 42.5% 57.1%GP (4 runs) 68.6% 77.3% 74.2% 53.6% 57.8%Decision tree 68.5% 86.2% 70.6% 38.1% 61.4%Baseline 1A 63.3% 100.0% 63.3% 0.0% 100.0%Baseline 1B 49.0% 46.4% 63.3% 53.6% 36.7%Baseline 2 70.8% 94.9% 69.8% 29.2% 76.7%to chance.
The differences in average value for the bottom eight indicators were notconfirmed to be significant with this small sample size (307 training examples).For completedness, no individual indicator used in isolation was shown to signif-icantly improve classification accuracy over the baseline.5.2.3 Indicators Combined  wi th  Learning.
When distinguishing according to com-pletedness, both CART and logistic regression successfully combined indicator values,improving classification accuracy over the baseline measure.
As shown in Table 16,classification accuracies were 74.0% and 70.5%, respectively.
A binomial test showedeach to be a significant improvement over the 63.3% accuracy achieved by Baseline 1A.Although the accuracies attained by GP and decision tree induction, 68.6% and 68.5%respectively, are also higher than that of Baseline 1A, based on a binomial test this isnot significant.
However, this may be due to our small test sample size.The increase in the number of nonculminated clauses correctly classified, i.e., non-culminated recall, illustrates a greater improvement over the baseline.
As detailed inTable 16, nonculminated recalls of 53.1%, 48.7%, 53.6%, and 38.1% were achieved by thelearning methods, as compared to the 0.0% nonculminated recall achieved by Baseline1A.
Baseline 1A does not classify any nonculminated clauses correctly because it clas-sifies all clauses as events.
This difference in recall is more dramatic than the accuracyimprovement because of the dominance of culminated clauses in the test set.
Note thatit is possible for an uninformed approach to achieve the same nonculminated recallas GP, 53.6%, by arbitrarily classifying 53.6% of all clauses as nonculminated, and therest as culminated.
However, as shown in Table 16, the average performance of sucha method (Baseline 1B) loses in comparison to GP, for example, in overall accuracy(49.0%) and nonculminated precision (36.7%).All three supervised learning methods highly prioritized the perfect indicator.The induced decision tree uses the perfect indicator as its first discriminator, logisticregression ranked the perfect indicator as fourth out of 14 (see Table 17), and onefunction tree created by GP includes the perfect indicator as one of five indicators usedtogether to increase classification performance (see Figure 4).
Furthermore, asshown inTable 15, the perfect indicator tied with the temporal adverb indicator as most highlycorrelated with completedness, according to t-test results.
This is consistent with thefact that, as discussed in Section 2.1, the perfect indicator is strongly connected tocompletedness on a linguistic basis.GP maintained classification performance while emphasizing a different set ofindicators than those emphasized by logistic regression.
Figure 4 shows an example618Siegel and McKeown Improving Aspectual ClassificationTable 17Weights produced by logistic regression to distinguish between culminated andnonculminated verbs.
Contrast with Table 12.Linguistic Indicator Logistic Weight T-test P-valueduration in-PP -0.1207664 0.0626manner adverb 0.03808262 0.0008evaluation adverb 0.03212381 0.7063perfect -0.02304221 0.0000temporal adverb -0.01643347 0.0000not or never -0.01212703 0.6164not progressive -0.01059269 0.0031no subject -0.006891114 0.0241past/pres participle -0.004127672 0.0080past tense 0.002484739 0.7132present tense 0.00218274 0.0757continuous adverb -0.001775534 0.2563duration for-PP 0.0001747421 0.1756frequency -0.0000916167 0.5652(+ (- (- (+ (/ NoSubject Frequency) TemporalAdv) (- 83 Perfect)) (/ (/NoSubject Frequency) Frequency)) (- (+ NotProgressive NotProgressive) (-60 Perfect)))Figure 4Example function tree designed by a genetic algorithm to distinguish between culminated andnonculminated verbs, achieving 69.2% accuracy and 62.8% nonculminated recall.function tree automatically generated by GP, which achieved 69.2% accuracy.
Notethat, as for stativity, this classification performance was attained with a subset ofonly five linguistic indicators: no subject, frequency, temporal adverb, perfect, andnot progressive.
(However, only two of these appeared in the example function treefor stativity shown in Figure 2: frequency and progressive.)
Since multiple propersubsets of indicators succeed in improving classification accuracy, this shows thatsome indicators are mutually correlated.5.3 Comparing Learning Results Across Classification TasksAs shown above, learning methods uccessfully produced models that were special-ized for the classification task.
In particular, the same set of 14 indicators were com-bined in different ways, successfully improving classification performance for bothstativity and completedness, and revealing linguistic insights for each.However, it is difficult to determine which learning method is the best for verbclassification in general, since their relative ranks differ across classification task andevaluation criteria.
The relative accuracies of the three supervised learning proceduresrank in opposite orders when comparing the results in classification according tostativity (Table 10) to results in classification according to completedness (Table 16).Furthermore, when measuring classification performance as the recall of the nondom-inant class (stative and nonculminated, respectively), the rankings are also conflictingwhen comparing results for the two classification tasks.
The difficulties in drawingconclusions about the relative performance of learning techniques are discussed inSection 4.4.The same two linguistic indicators were ranked in the top two positions for bothaspectual distinctions by logistic regression.
As shown in Tables 17 and 12, which givethe weights automatically derived by logistic regression for each of the 14 linguistic619Computational Linguistics Volume 26, Number 4indicators, the manner adverb and duration in-PP indicators are in the top two slots forboth weighting schemes, corresponding to the two aspectual distinctions.
This mayindicate that these two indicators are universally useful for aspectual classification,at least when modeling with logistic regression.
However, the remaining rankings oflinguistic indicators differ greatly between the two weighting schemes.5.4 Indicators versus Memorizing Verb AspectIn this work, clauses are classified by their main verb only.
Therefore, disambiguatingbetween multiple aspectual senses of the same verb is not possible, since other partsof the clause (e.g., verb arguments) are not available as a source of context with whichto disambiguate.
Thus, the improvement in accuracy attained reveals the extent owhich, across the corpora examined, most verbs are dominated by one sense.A competing baseline approach would be to simply memorize the most frequentaspectual category of each verb in the training set, and classify verbs in the test setaccordingly.
In this case, test verbs that did not appear in the training set would beclassified according to majority class.
However, classifying verbs and clauses accordingto numerical indicators has several important advantages over this baseline:?
Handles rare or unlabeled verbs.
The results we have shown serve toestimate classification performance over unseen verbs that were notincluded in the supervised training sample.
Once the system has beentrained to distinguish by indicator values, it can automatically classifyany verb that appears in unlabeled corpora, since measuring linguisticindicators for a verb is fully automatic.
This also applies to verbs that areunderrepresented in the training set.
For example, as discussed inSection 5.1.4, one node of the resulting decision tree trained todistinguish according to stativity identifies 19 stative test cases withoutmisclassifying any of 27 event est cases with verbs that occur only onetime each in the training set.?
Success when training doesn't include test verbs.
To test this, all testverbs were eliminated from the training set, and logistic regression wastrained over this smaller set to distinguish according to completedness.The result is shown in Table 16 (logistic 2).
Accuracy remained higherthan Baseline 1A (Baseline 2 is not applicable), and the recall trade-off isfelicitous.?
Improved performance.
Memorizing majority aspect does not achieve ashigh an accuracy as the linguistic indicators for completedness, nor doesit achieve as wide a recall trade-off for both stativity and completedness.These results are indicated as the second baselines (Baseline 2) inTables 10 and 16, respectively.?
Classifiers output scalar values.
This allows the trade-off between recalland precision to be selected for particular applications by selecting theclassification threshold.
For example, in a separate study, optimizing forF-measure resulted in a more dramatic trade-off in recall values ascompared to those attained when optimizing for accuracy (Siegel 1998b).Moreover, such scalar values can provide input to systems that performreasoning on fuzzy or uncertainty knowledge.?
Expandable framework.
One form of expansion is that additionalindicators can be integrated by measuring the frequencies of additional620Siegel and McKeown Improving Aspectual Classificationaspectual markers.
Furthermore, indicators measured over multipleclausal constituents (e.g., main verb-object pairs) alleviate verb ambiguityand sparsity and improve classification performance (Siegel 1998b).Manual analysis reveals linguistic insights.
As summarized below inSection 9, our analysis reveals linguistic insights that can be used toinform future work.6.
Unsupervised LearningUnsupervised methods for clustering words have been developed that do not requiremanually marked examples (Hatzivassiloglou and McKeown 1993; Schfitze 1992).These methods automatically determine the number of groups and the number ofverbs in each group.This section evaluates an approach to clustering verbs developed and implementedby Hatzivassiloglou, based on previous work for semantically clustering adjectives(Hatzivassiloglou and McKeown 1993; Hatzivassiloglou 1997).
This system automat-ically places verbs into semantically related groups based on the distribution of co-occurring direct objects.
Such a system avoids the need for a set of manually markedexamples for the training process.
Manual marking is time consuming and domain de-pendent, requires linguistic expertise, and must be repeated on a corpus representingeach new domain.The clustering approach differs from our approach combining linguistic indicatorsin two significant ways.
First, the method semantically groups words in a generalsense--qt is not designed or intended to group words according to any particularsemantic distinction such as stativity or completedness.
Second, this method measuresa co-occurrence r lation not embodied by any of the 14 linguistic indicators presentedin this article: the direct object.
Note, however, that there are several advantages tolinguistic indicators that measure the frequency of linguistic phenomena such as theprogressive over measuring the frequencies of open-class words (Siegel 1998b).The clustering algorithm was evaluated over the corpus of novels, which, as shownin Table 7, has 75,289 parsed clauses.
Clauses were eliminated from this set if they hadno direct object, or if the direct object was a clause, a proper noun, or a pronoun, orwas misspelled.
This left 14,038 distinct verb-object pairs of varying frequencies.Because the direct object is an open-class category (noun), occurrences of anyparticular verb-object pair are sparse as compared to the markers measured by thelinguistic indicators.
For example, make dinner occurs only once among the parsedclauses from the corpus of novels, but make occurs 34 times in the progressive.
Forthis reason, the clustering algorithm was evaluated over a set of frequent verbs only:56 verbs occurred more than 50 times each in the set of verb-object pairs.
Of these,the 19 shown in Figure 5 were selected as an evaluation set because of the naturalsemantic groups they fall into.
The groupings hown, which do not pay heed to as-pectual classification i  particular, were established manually, but are not used by theautomatic grouping algorithm.Figure 6 shows the output of the unsupervised system.
Seven groups were created,each with two to four verbs.
The grouping algorithm used by this system is designedfor data that is not as sparse with respect o the frequencies of verb-object pairs,e.g., data from a larger corpus.
Thus, this partition is not representative of the fullpower of the approach, and a larger amount of data could improve it significantly.
Formore detail on the clustering algorithm and further results see Hatzivassiloglou andMcKeown (1993) and Hatzivassiloglou (1997).621Computational Linguistics Volume 26, Number 41. sell(27) buy(20) acquire(8)2. push(28) pull(45)3. raise(68) lower(24)4. leave(160) enter(78)5. know(164) forget(50) learn(51)6. love(18) hate(16) like(ll2)7. want(60) need(69) require(57) demand(15)Figure 5The set of verbs manually selected for evaluating unsupervised clustering, with frequenciesshown.
The grouping shown here was established manually.I.
*hate *like pull2.
lower raise3.
demand *know *love *want4.
buy sell5.
enter forget learn6.
acquire *need *require7.
leave pushFigure 6Verb groupings created automatically by an unsupervised learning algorithm developed andimplemented by Hatzivassiloglou and McKeown (1993) and Hatzivassiloglou (1997) appliedover the corpus of 10 novels.
Stative verbs are shown with an asterisk, and event verbswithout.The algorithm clearly discriminated event verbs from stative verbs.
7 In Figure 6,stative verbs are shown with an asterisk; event verbs are shown without.
Three of thegroups are dominated by stative verbs, and the other four groups are composed en-tirely of event verbs.
Each stative verb is found in a group with 70.2% states, averagedacross the 7 stative verbs, and each event verb is found in a group with 82.6% events,averaged across the 12 event verbs.
This is an improvement  over an uninformed base-line system that randomly creates groups of two or more verbs each, which wouldachieve average precisions of 63.2% and 36.8%, respectively.We can draw two important conclusions from this result.
First, unsupervised learn-ing is a viable approach for classifying verbs according to particular semantic distinc-tions such as stativity.
Second, co-occurrence distributions between the verb and direct7 The algorithm also grouped verbs according to semantic relatedness in general, as can be seen bycomparing the manual and automatic groupings.
Further analysis of such results are given byHatzivassiloglou and McKeown (1993).622Siegel and McKeown Improving Aspectual Classificationobject inform the aspectual classification of verbs.
This is an additional source of in-formation beyond the 14 linguistic indicators we combine with supervised learning.7.
Related WorkThe aspectual classification of a clause has thus far been primarily approached from aknowledge-based perspective.
For example, Pustejovsky's generative l xicon describessemantic interactions between clausal constituents hat effect aspectual class (Puste-jovsky 1991).
Additionally, Resnik (1996) demonstrates the influence of implicit directobjects on aspectual classification.The application of automatic orpus-based techniques to aspectual classificationis in its infancy.
Klavans and Chodorow (1992) pioneered the application of statisticalcorpus analysis to aspectual classification by placing verbs on a scale according to thefrequency with which they occur with certain aspectual markers from Table 2.
Thisway, verbs are automatically ranked according to their "degree of stativity.
"Machine learning has become instrumental in the development of robust naturallanguage understanding systems in general (Cardie and Mooney 1999).
For example,decision tree induction has been applied to word sense disambiguation (Black 1988),determiner prediction (Knight et al 1995), coordination parsing (Resnik 1993), syn-tactic parsing (Magerman 1993), and disambiguating clue phrases (Siegel 1994; Siegeland McKeown 1994; Litman 1994).
An overview of psycholinguistic ssues behindlearning for natural language problems in particular is given by Powers and Turk(1989).
Models resulting from machine induction have beeen manually inspected todiscover linguistic insights for disambiguating clue words (Siegel and McKeown 1994).However, machine learning techniques have not previously been applied to aspectualdisambiguation.Previous efforts have applied machine induction methods to coordinate corpus-based linguistic indicators in particular, for example, to classify adjectives accordingto markedness (Hatzivassiloglou and McKeown 1995), to perform accent restoration(Yarowsky 1994), for sense disambiguation problems (Luk 1995), and for the automaticidentification of semantically related groups of words (Pereira, Tishby, and Lee 1993;Hatzivassiloglou and McKeown 1993; Schi.itze 1992).8.
Future WorkParallel bilingual corpora are potential sources of supervised examples for trainingand testing aspectual classification systems.
For example, since many languages haveexplicit markings corresponding to completedness (as described in Section 2.6), thecategory of a clause can be determined by its translation.Additional machine learning methods hould be evaluated for combining linguis-tic indicators.
For example, neural networks are especially suited for combining nu-merical inputs, and Naive Bayes models are especially suited for additive concepts.Also, iteratively refining the model (e.g., for logistic regression) may be an importantway to eliminate indicators that do not help for a particular classification problem andto eliminate redundancy between indicators that correlate highly with one another.Machine learning techniques may be able to automatically determine how best tomeasure linguistic indicators, if trained over a large supervised sample.
For example,previous work has measured indicators by applying a symbolic expression induced byGP to a subset of clauses in a corpus (Siegel and McKeown 1996).
This way, interactionsbetween markers in a clause can be automatically measured.
In principle, machinelearning techniques could further generalize these methods by automatically inducing623Computational Linguistics Volume 26, Number 4an algorithm that scans a corpus dynamically, depending on what it sees as it processesclauses.
This could automatically select relevant markers as well as relevant portionsof the corpus for a particular input clause.9.
ConclusionsWhile individual inguistic indicators have predictive value, they are predictively in-complete.
Such incompleteness i  due to sparsity and noise when computing indicatorvalues over a corpus of limited size, and is also a consequence of the linguistic behav-ior of certain indicators.
However, incomplete indicators can complement one anotherwhen placed in combination.Machine learning has served to illustrate the potential of 14 linguistic indicatorsby showing that they perform well in combination for two aspectual classificationproblems.
This potential was not clear when evaluating indicators individually.
Forstativity, decision tree induction achieved an accuracy of 93.9%, as compared to theuninformed baseline's accuracy of 83.8%.
Furthermore, GP and logistic regression alsoachieved improvements over the baseline.
For completedness, CART and logistic re-gression achieved accuracies of 74.0% and 70.5%, as compared to the uninformedbaseline's accuracy of 63.3%.
These improvements in classification performance aremore dramatically illustrated by favorable trade-offs between recall scores achievedfor both classification problems.
Such results are profitable for tasks that weigh theidentification of the less frequent class more heavily.This evaluation was performed over unrestricted sets of verbs occurring acrosstwo corpora.
The system can automatically classify all verbs appearing in a corpus,including those that have not been manually classified for supervised training data.Therefore, we have demonstrated a much-needed full-scale method for aspectual clas-sification that is readily expandable.
Since minimal overfitting was demonstrated withonly a small quantity of manually supervised ata required, this approach is easilyportable to other domains, languages, and semantic distinctions.The results of learning are linguistically viable in two respects.
First, learning au-tomatically produces models that are specialized for different aspectual distinctions;the same set of 14 indicators are combined in different ways according to which clas-sification problem is targeted.
Second, automatic learning often derives linguisticallyinformative insights.
We have shown several such insights revealed by inspecting themodels produced by learning, which are summarized here:?
Examining the logistic regression model for classification according tostativity revealed a decision tree type of rule incorporated with thenormal weighting scheme.?
Verb frequency distinguishes stative verbs within multiple subsets ofverbs.
When applied to all verbs in a medical corpus, it identifiesoccurrences of show.
Furthermore, examining an example node of thedecision tree that distinguishes according to stativity revealed that verbfrequency discriminates 19 stative clauses with 100.0% precision from thenode's partition of 60 training cases.?
Several proper subsets of the linguistic indicators prove independentlyuseful for aspectual classification when combined with an appropriatemodel.
This is illustrated by the fact that certain models revealcombinations of small sets of indicators that improved classificationperformance.
For example, GP results for both classification tasks624Siegel and McKeown Improving Aspectual Classificationincorporated a subset of only five indicators each.
In particular, manneradverb, which ranked highest by logistic regression, is not incorporatedin the example function tree induced by GP.
This may be because thisindicator only applies to a small number of verbs, as shown in Table 13,and because an/f-rule such as that captured by logistic regression isdifficult o encode with a function tree with no conditional primitives.Learning methods discovered that some indicators are particularly usefulfor both classification tasks.
For example, the same two indicators wereweighted most heavily by logistic regression for both tasks: durationin-PP and manner adverb.However, in general, learning methods emphasized different linguisticindicators for different classification tasks.
For example, decision treeinduction used frequency as the main discriminator to classify clausesaccording to stativity, while the perfect indicator was the maindiscriminator for classification according to completedness.Comparing the ability of learning methods to combine linguistic indicators is dif-ficult, since they rank differently depending on the classification task and evaluationcriteria.
For example, the relative accuracies of the three supervised learning proce-dures rank in opposite orders when comparing the results for stativity to the resultsfor completedness.The unsupervised grouping of verbs provides an additional method for aspectualclassification according to stativity.
Co-occurrence distributions between the verb anddirect object inform the aspectual classification of verbs.
This provides informationbeyond the 14 linguistic indicators that can also be derived automatically.
However,due to the sparsity intrinsic to pairs of open-class categories such as verb-object pairs,this approach was only evaluated over a small set of frequent verbs.AcknowledgmentsJudith Klavans was very helpful in ourformulation of the linguistic techniques inthis work, and Alexander Chaffee, VasileiosHatzivassiloglou, and Dekai Wu in theresults evaluation methods.
VasileiosHatzivassiloglou designed and implementedthe clustering of verbs described inSection 6.
For comments on earlier drafts ofthis work, we would like to thank thosepeople mentioned, as well as David Evans,Hongyan Jing, Min-Yen Kan, DragomirRadev, and Barry Schiffman.
Finally, wewould like to thank Andy Singleton for theuse of his GPQuick software.This research was supported in part bythe Columbia University Center forAdvanced Technology in High PerformanceComputing and Communications inHealthcare (funded by the New York StateScience and Technology Foundation), theOffice of Naval Research under contractN00014-95-1-0745, and the National ScienceFoundation under contract GER-90-24069.ReferencesAllen, Franklin and Risto Karjalainen.
1995.Using genetic algorithms to find technicaltrading rules.
Technical Report, Rodney L.White Center For Financial Research.Allen, James.
1995.
Natural LanguageUnderstanding.
Benjamin/Cummings,Redwood City, CA.Baker, R. J. and J.
A. Nelder.
1989.
The GLIMSystem, Release 3: Generalized LinearInteractive Modeling.
NumericalAlgorithms Group, Oxford.Black, Ezra.
1988.
An experiment incomputational discrimination ofEnglishword senses.
IBM Journal of Research andDevelopment, 2(32).Breiman, Leo, Jerome H. Friedman,Richard A. Olshen, and Charles J. Stone.1984.
Classification and Regression Trees.Wadsworth, Belmont.Brodley, Carla E. 1996.
Applyingclassification algorithms in practice.
InStatistics and Computing.Cardie, Claire and Nicholas Howe.
1997.Improving minority class prediction using625Computational Linguistics Volume 26, Number 4case-specific feature weights.
In D. Fisher,editor, Proceedings ofthe FourteenthInternational Conference on MachineLearning.
Morgan Kaufmann.Cardie, Claire and Raymond J. Mooney.1999.
Guest editors' introduction:Machine learning and natural language.Machine Learning, 1-3(34).Chan, Philip K. and Sal J. Stolfo.
1993.Toward multistrategy parallel anddistributed learning in sequence analysis.In Proceedings ofthe First InternationalConference on Intelligent Systems forMolecular Biology.Church, Ken.
1988.
A stochastic partsprogram and noun phrase parser forunrestricted text.
In Proceedings ofthe 2ndConference for Applied Natural LanguageProcessing, pages 136-143.Cohen, William W. 1995.
Fast effective ruleinduction.
In Proceedings ofthe 12thInternational Conference on MachineLearning, pages 115-123.Cramer, Nichael L. 1985.
A representationfor the adaptive generation of simplesequential programs.
In J. Grefenstette,editor, Proceedings ofthe \[First\] InternationalConference on Genetic Algorithms.
LawrenceErlbaum.Dorr, Bonnie J.
1992.
A two-level knowledgerepresentation formachine translation:lexical semantics and tense/aspect.
InJames Pustejovsky and Sabine Bergler,editors, Lexical Semantics and KnowledgeRepresentation.
Springer Verlag, Berlin.Dowty, David R. 1979.
Word Meaning andMontague Grammar.
D. Reidel, Dordrecht.Duda, Richard O. and Peter E. Hart.
1973.Pattern Class~cation and Scene Analysis.Wiley, New York.Fayyad, Usama M. and Keki B. Irani.
1992.On the handling of continuous-valuedattributes in decision tree generation.Machine Learning, 8.Friedman, Jerome H. 1977.
A recursivepartitioning decision rule fornon-parametric classification.
IEEETransactions on Computers.Goldberg, David.
1989.
Genetic Algorithms inSearch, Optimization, and Machine Learning.Addison-Wesley Publishing Company,Inc., Reading, MA.Hatzivassiloglou, Vasileios.
1997.
AutomaticAcquisition of Lexical Semantic Knowledgefrom Large Corpora: The Identi~cation fSemantically Related Words, Markedness,Polarity, and Antonymy.
Ph.D. thesis,Columbia University.Hatzivassiloglou, Vasileios and Kathleen R.McKeown.
1993.
Towards the automaticidentification of adjectival scales:clustering adjectives according tomeaning.
In Proceedings ofthe 31st AnnualMeeting, pages 172-182, Columbus, OH,June.
Association for ComputationalLinguistics.Hatzivassiloglou, Vasileios and Kathleen R.McKeown.
1995.
A quantitativeevaluation of linguistic tests for theautomatic prediction of semanticmarkedness.
In Proceedings ofthe 33rdAnnual Meeting, pages 197-204, Boston,MA, June.
Association for ComputationalLinguistics.Hitzeman, Janet, Marc Moens, and ClaireGrover.
1994.
Algorithms for analysingthe temporal structure of discourse.Technical Report, University ofEdinburgh.Holland, John.
1975.
Adaptation in Naturaland Art~cial Systems.
The University ofMichigan Press, Ann Arbor, MI.Hwang, Chung Hee and Lenhart K.Schubert.
1991.
Interpreting temporaladverbials.
Technical Report, Universityof Rochester.Klavans, Judith L. 1994.
Linguistic tests overlarge corpora: aspectual classes in thelexicon.
Technical Report, ColumbiaUniversity Dept.
of Computer Science.Klavans, Judith L. and Martin Chodorow.1992.
Degrees of stativity: the lexicalrepresentation f verb aspect.
InProceedings ofthe 14th InternationalConference on Computation Linguistics.Knight, K., I. Chander, M. Haines,V.
Hatzivassiloglou, E. Hovy, M. Iida,S.
K. Luk, R. Whitney, and K. Yamada.1995.
Filling knowledge gaps in abroad-coverage mtsystem.
In Proceedingsof the International Joint Conference onArtiJicial Intelligence.Koza, John R. 1992.
Genetic Programming: Onthe Programming of Computers by Means ofNatural Selection.
MIT Press, Cambridge,MA.Litman, Diane J.
1994.
Classifying cuephrases in text and speech using machinelearning.
In Proceedings of the TwelfthNational Conference on Artificial Intelligence,Menlo Park, CA, July.
AAAI Press.Luk, Alpha K. 1995.
Statistical sensedisambiguation with relatively smallcorpora using dictionary definitions.
InProceedings ofthe 33rd Annual Meeting,Columbus, OH, June.
Association forComputational Linguistics.Magerman, David H. 1993.
Parsing asstatistical pattern recognition.
TechnicalReport, IBM.Masand, Brij.
1994.
Optimizing confidenceof text classification by evolution of626Siegel and McKeown Improving Aspectual Classificationsymbolic expressions.
In K. Kinnear,editor, Advances in Genetic Programming.MIT Press, Cambridge, MA.McCord, Michael C. 1990.
SLOTGRAMMAR: A system for simplerconstruction of practical natural languagegrammars.
In R. Studer, editor,International Symposium on NaturalLanguage and Logic.
Springer Verlag.Mitchell, Tom M. 1997.
Machine Learning.
TheMcGraw-Hill Companies, Inc., New York.Moens, Marc and Mark Steedman.
1988.Temporal ontology and temporalreference.
Computational Linguistics, 14(2).Olsen, Mari B. and Philip Resnik.
1997.Implicit object constructions and the(in)transitivity continuum.
In Proceedingsof the 33rd Regional Meeting of the ChicagoLinguistics Society, April.Passonneau, Rebecca J.
1988.
Acomputational model of the semantics oftense and aspect.
ComputationalLinguistics, 14(2).Pereira, Fernando, Naftali Tishby, andLillian Lee.
1993.
Distributional clusteringof English words.
In Proceedings ofthe 31stAnnual Meeting, pages 183-190,Columbus, OH, June.
Association forComputational Linguistics.Powers, David M. W. and Christopher C. R.Turk.
1989.
Machine Learning of NaturalLanguage.
Springer-Verlag.Pustejovsky, James.
1991.
The syntax ofevent structure.
Cognition, 41(103):47-92.Pustejovsky, James.
1995.
The GenerativeLexicon.
MIT Press, Cambridge, MA.Quinlan, Jim R. 1986.
Induction of decisiontrees.
Machine Learning, 1(1):81-106.Quinlan, Jim R. 1993.
C4.5: Programs forMachine Learning.
Morgan Kaufmann, SanMateo, CA.Resnik, Philip.
1993.
Semantic lasses andsyntactic ambiguity.
In Proceedings oftheARPA Workshop on Human LanguageTechnology, March.Resnik, Philip.
1996.
Selectional constraints:An information-theoretic model and itscomputational realization.
Cognition, (61).Schfitze, Hinrich.
1992.
Dimensions ofmeaning.
In Proceedings ofSupercomputing.Schubert, Lenhart K. and Chung HeeHwang.
1990.
Picking reference eventsfrom tense trees: A formal, implementabletheory of English tense-aspect semantics.Technical Report, University of Rochester.Siegel, Eric V. 1994.
Competitively evolvingdecision trees against fixed training casesfor natural anguage processing.
InK.
Kinnear, editor, Advances in GeneticProgramming.
MIT Press, Cambridge, MA.Siegel, Eric V. 1997.
Learning methods forcombining linguistic indicators to classifyverbs.
In Proceedings ofthe SecondConference on Empirical Methods in NaturalLanguage Processing, Providence, RI,August.Siegel, Eric V. 1998a.
Disambiguating verbswith the WordNet category of the directobject.
In Proceedings ofthe Usage ofWordNet in Natural Language ProcessingSystems Workshop, Montreal, Canada.Siegel, Eric V. 1998b.
Linguistic Indicators forLanguage Understanding: Using MachineLearning Methods to Combine Corpus-basedIndicators for Aspectual Classification ofClauses.
Ph.D. thesis, Columbia University.Siegel, Eric V. 1999.
Corpus-based linguisticindicators for aspectual classification.
InProceedings ofthe 37th Annual Meeting.Association for ComputationalLinguistics.Siegel, Eric V. and Kathleen R. McKeown.1994.
Emergent linguistic rules frominducing decision trees: disambiguatingdiscourse clue words.
In Proceedings oftheTwelfth National Conference on ArtificialIntelligence, Menlo Park, CA, July.
AAAIPress.Siegel, Eric V. and Kathleen R. McKeown.1996.
Gathering statistics to aspectuallyclassify sentences with a geneticalgorithm.
In K. Oflazer and H. Somers,editors, Proceedings ofthe SecondInternational Conference on New Methods inLanguage Processing, Ankara, Turkey,September, Bilkent University.Sjantner, T. J. and D. E. Duffy.
1989.
TheStatistical Analysis of Discrete Data.Springer-Verlag, New York.Stolfo, Salvatore, Andreas L. Prodromidis,Shelley Tselepis, Wenke Lee, and Wei Fan.1997.
JAM: Java agents for meta-learningover distributed atabases.
TechnicalReport, Columbia University.Stys, Malgorzata E. 1991.
Parallel sciencetexts in English and Polish: Problems oflanguage and communication.
TechnicalReport, Warsaw University.
Master'sthesis.Syswerda, Gibert.
1989.
Uniform crossoverin genetic algorithms.
In J. D. Schaffer,editor, Proceedings ofthe Third InternationalConference on Genetic Algorithms.
MorganKaufmarm.Tackett, Walter A.
1993.
Geneticprogramming for feature discovery andimage discrimination.
In Proceedings oftheFifth International Conference on GeneticAlgorithms, San Mateo, CA.
MorganKaufmann.Tackett, Walter A. and Aviram Carmi.
1994.The donut problem: Scalability,627Computational Linguistics Volume 26, Number 4generalization a d breeding policies ingenetic programming.
In K. Kinnear,editor, Advances in Genetic Programming.MIT Press, Cambridge, MA.Vendler, Zeno.
1967.
Verbs and times.
InLinguistics in Philosophy.
CorneUUniversity Press, Ithaca, NY.Wiebe, Janyce M., Thomas P. O'Hara,Thorsten Ohrstr6m-Sandgren, a dKenneth J. McKeever.
1997.
An empiricalapproach to temporal reference resolution.In Proceedings ofthe Second Conference onEmpirical Methods in Natural LanguageProcessing, Providence, RI, August.Wolpert, David H. 1992.
Stackedgeneralization.
Neural Networks, 5.Yarowsky, David.
1994.
Decision lists forlexical ambiguity resolution: Applicationto accent restoration i Spanish andFrench.
In Proceedings ofthe 32nd AnnualMeeting, San Francisco, CA, June.
MorganKaufmann.
Association forComputational Linguistics.628
