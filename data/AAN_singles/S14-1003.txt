Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 22?29,Dublin, Ireland, August 23-24 2014.Improvement of a Naive Bayes Sentiment Classifier Using MRS-BasedFeaturesJared KramerUniversity of WashingtonSeattle, WAjaredkk@uw.eduClara GordonUniversity of WashingtonSeattle, WAcgordon1@uw.eduAbstractThis study explores the potential of us-ing deep semantic features to improve bi-nary sentiment classification of paragraph-length movie reviews from the IMBDwebsite.
Using a Naive Bayes classifier asa baseline, we show that features extractedfrom Minimal Recursion Semantics repre-sentations in conjunction with back-off re-placement of sentiment terms is effectivein obtaining moderate increases in accu-racy over the baseline?s n-gram features.Although our results are mixed, our mostsuccessful feature combination achievesan accuracy of 89.09%, which representsan increase of 0.76% over the baseline per-formance and a 6.48% reduction in error.1 IntroductionText-based sentiment analysis offers valuable in-sight into the opinions of large communities of re-viewers, commenters and customers.
In their sur-vey of the field, Pang and Lee (2008) highlight theimportance of sentiment analysis across a rangeof industries, including review aggregation web-sites, business intelligence, and reputation man-agement.
Detection and classification of sentimentcan improve downstream performance in applica-tions sensitive to user opinions, such as question-answering, automatic product recommendations,and social network analysis (ibid., p. 12).While previous research in sentiment analysishas investigated the extraction of features fromsyntactic dependency trees, semantic representa-tions appear to be underused as a resource formodeling opinion in text.
Indeed, to our knowl-edge, there has been no research using seman-tic dependencies created by a precision grammarfor sentiment analysis.
The goal of the presentresearch is to address this gap by augmenting abaseline classifier with features based on Min-imal Resursion Semantics (MRS; Copestake etal., 2005), a formal semantic representation pro-vided by the English Resource Grammar (ERG;Flickinger, 2000).
An MRS is a connected graphin which semantic entities may be linked directlythrough shared arguments or indirectly throughhandle or qeq constraints, which denote equal-ity modulo quantifier insertion (Copestake et al.,2005).
This schema allows for underspecificationof quantifier scope.Using Narayanan et al.
?s (2013) Naive Bayessentiment classifier as a baseline, we test the effec-tiveness of eight feature types derived from MRS.Our feature pipeline crawls various links in theMRS representations of sentences in our corpusof paragraph-length movie reviews and outputssimple, human-readable features based on varioustypes of semantic relationships.
This improvedsystem achieves modest increases in binary senti-ment classification accuracy for several of the fea-ture combinations tested.1In the following sections, we summarize previ-ous research in MRS feature extraction and senti-ment classification, describe the baseline systemand our modifications to it, and outline our ap-proach to parsing our data, constructing features,and integrating them into the existing system.
Fi-nally, we report our findings, examine in moredetail where our improved system succeeded andfailed in relation to the baseline, and suggest av-enues for further research in sentiment analysiswith MRS-based features.2 Context and Related WorkCurrent approaches to sentiment analysis taskstypically use supervised machine learning meth-1Because this task consists of binary classification on anevenly split dataset and every test document is assigned aclass, simple accuracy is the most appropriate measure of per-formance.22ods with bag-of-words features as a baseline, andfor classification of longer documents like the onesin our dataset, such features remain a powerfultool of analysis.
Wang and Manning (2012) com-pare the performance of several machine learningalgorithms using uni- and bigram features from avariety of common sentiment datasets, includingthe IMDB set used in this project.
They report thatthat SVM classifiers generally perform better sen-timent classification on paragraph-length reviews,while Native Bayes classifiers produce better re-sults for ?snippets,?
or short phrases (ibid., p. 91).For our dataset, they obtain the highest accuraciesusing a hybrid approach, SVM with Naive Bayesfeatures, which results in 91.22% accuracy (ibid.,p.
93).
This appears to be the best test result todate on this dataset.
Although we use a NaiveBayes classifier in our project, alternative machinelearning algorithms are a promising topic of fur-ther future investigation (see ?6).Two existing areas of research have direct rele-vance to this project: MRS feature extraction, andsentiment analysis using features based on deeplinguistic representations of data.
In their work onmachine translation, Oepen et al.
(2007) define atype of MRS triple based on elementary dependen-cies, a simplified ?variable-free?
representation ofpredicate-argument relations in MRS (p. 5).
Fu-jita et al.
(2007) and Pozen (2013) develop simi-lar features for HPSG parse selection, and Pozenexperiments with replacing segments of predicatevalues in triple features with WordNet sense, POS,and lemma information (2013, p. 32).While there has not yet been any research on us-ing MRS features in sentiment analysis, there hasbeen work on extracting features from deep repre-sentations of data for sentiment analysis.
In work-ing with deep representations such as MRSes ordependency parses, there are myriad sub-graphsthat can be used as features.
However these fea-tures are often quite sparse and do not general-ize well.
Joshi & Rose (2009) improve perfor-mance of a sentiment classifier by incorporatingtriples consisting of words and grammatical rela-tions extracted from dependency parses.
To in-crease the generalizability of these triples, theyperform back-off by replacing words with part-of-speech tags.
Similarly, Arora et al.
(2010) extractfeatures from dependency parses by using senti-ment back-off to identify potentially meaningfulportions of the dependency graph.
Given this suc-cess combining back-off with sub-graph features,we design several feature types following a similarmethodology.2.1 The IMBD DatasetWe use a dataset of 50,000 movie reviews crawledfrom the IMDB website, originally developed byMaas et al.
(2011).
The dataset is split equallybetween training and test sets.
Both training andtest sets contain equal numbers of positive andnegative reviews, which are defined according tothe number of stars assigned by the author onthe IMBD website: one to four stars for nega-tive reviews, and seven to ten stars for positivereivews.
The reviews vary in length but gener-ally contain between five and fifteen sentences.The Natural Language ToolKit?s (NLTK; Loperand Bird, 2002) sentence tokenizer distinguishes616,995 sentences in the dataset.Unlike previous research over this dataset, wedivide the 25,000 reviews of the test set into twodevelopment sets and a final test set.
As such,our results are not directly comparable to those ofWang & Manning (2012).2.2 The Baseline SystemThe system we use as a baseline, created byNarayanan et al.
(2013), implements several smallbut innovative improvements to a simple NaiveBayes classifier.
In the training phase, the base-line performs simple scope of negation annota-tion on the surface string tokens.
Any word con-taining the characters not, n?t or no triggers a?negated?
state, in which all following n-grams areprepended with not .
This continues until eithera punctuation delimiter (?.,!
:;) or another negationtrigger is encountered.During training, when an n-gram feature is readinto the classifier, it is counted toward P (f |c), andthe same feature with not prepended is countedtoward P (f |c?
), where c is the document classand c?
is the opposite class.
Singleton featuresare then pruned away.
Finally, the system runs aset of feature-filtering trials, in which the prunedfeatures are ranked by mutual information score.These trials start at a base threshold of 100,000features, and the number of features is increasedstepwise in increments of 50,000.
The feature setthat produces the highest accuracy in trials over adevelopment data set is then retained and used toclassify the test data.
Table 1 shows the ten mostinformative features, ranked by mutual informa-23Top N-Grams1.
worst 6. awful2.
bad 7. great3.
not the worst 8. waste4.
the worst 9. excellent5.
not worst 10. not not evenTable 1: Top MI-ranked baseline n-gram Features.tion score, out of the 12.1 million n-gram featuresgenerated by our baseline.Before modifying the baseline system?s code,we reproduced their reported accuracy figure of88.80% over the entire 25,000 review test set.However, it appears the baseline system used thetest data as development data.
In order to addressthis, we split the data as into development sets asdescribed above.
When we ran the baseline sys-tem over our final test set, we obtained accuraciesof 88.34% pre-feature filtering and 88.29% post-feature filtering; our division of the original testset into development and test sets accounts for thisdiscrepancy.3 MethodologyOur approach to this task consisted of three gen-eral stages: obtaining MRSes for the dataset,implementing a feature pipeline to process theMRSes, and integrating the new features into theclassifier.
In this section we will describe each ofthese processes in turn.3.1 Parsing with the ERGBecause most of the reviews in our data set ap-pear to be written in Standard English, we per-form minimal pre-processing before parsing thedataset with the ERG.
We use NLTK?s sentencetokenization function in our pipeline, along withtheir HTML-cleaning function to remove somestray HTML-style tags we encountered in the data.To obtain MRS parses of the data, we useACE version 0.9.17, an ?efficient processorfor DELPH-IN HPSG grammars.
?2ACE?s sim-ple command line interface allows the parsingpipeline to output MRS data in a single line toa separate directory of MRS data files.
We usedthe 1212 ERG grammar image3and specified root2Available at http://sweaglesw.org/linguistics/ace/.
Ac-cessed January 15, 2014.3Available at http://www.delph-in.net/erg/.
Accessed Jan-conditions that would allow for parses of the infor-mal and fragmented sentences sometimes foundin our dataset: namely, the root informal,root frag and root inffrag ERG rootnodes.Parsing with these conditions resulted in81.11% coverage over the entire dataset.
Aftermanual inspection of sentences that failed to parse,we found that irregularities in spelling and punc-tuation accounted for the majority of these failuresand further cleaning of the data would yield highercoverage.3.2 Feature DesignOur main focus in feature design is capturing rel-evant semantic relationships between sentimentterms that extend beyond the trigram boundary.Our entry point into the MRS is the elementarypredication (EP), and our pipeline algorithm ex-plores the three main EP components: argumentsand associated variables, label, and predicate sym-bol.
We also use the set of handle constraints incrawling the links between EPs.We use two main categories of crawled MRSfeatures: Predicate-Relation-Predicate (PRP)triples, a term borrowed from (Pozen, 2013), andShared-Label (SL) features.
Our feature templateconsists of eight feature subtypes, including plainEP symbols (type 1), five PRP features (types 2through 6) and two SL features (types 7 and 8).Table 2 gives examples of each type, along withthe unpruned counts of distinct features gatheredfrom our training data.
The examples for types1 through 6 are taken from the abridged MRSexample in Figure 1.
Note that an & characterseparates predicate and argument components inthe feature strings.
The type 7 and 8 examplesare taken from MRS of sentences featuring thephrases successfully explores and didn?t flow well,respectively.In our feature extraction pipeline, we use Good-man?s pyDelphin4tool, a Python module that al-lows for easy manipulation and querying of MRSconstituents.
This tool allows our pipeline toquickly process the ERG output files, obtain argu-ment and handle constraint information, and out-put the features for each MRS into a feature file tobe read by our classifier.
If the grammar has notreturned an analysis for a particular sentence, theuary 15, 2014.4Available at https://github.com/goodmami/pydelphin.Accessed January 20, 2014.24There is nothing redeeming about this trash.
[LTOP: h0INDEX:e2 [e SF:prop TENSE:pres MOOD:indicative PROG:- PERF:-]<[ be v there rel<6:8> LBL:h1 ARG0:e2 ARG1:x4] [thing rel<9:16> LBL:h5 ARG0:x4] [ no q rel<9:16>LBL:h6 ARG0:x4 RSTR:h7 BODY:h8] [" redeem v for rel"<17:26> LBL:h5 ARG0:e9 ARG1:x4 ARG2:x10][" about x deg rel"<27:32> LBL:h11 ARG0:e12 ARG1:u13] [ this q dem rel<33:37> LBL:h11 ARG0:x10 RSTR:h14BODY:h15] [" trash n 1 rel"<38:44> LBL:h16 ARG0:x10]>HCONS: <h0 qeq h1 h7 qeq h5 h14 qeq h16>]Figure 1: Sample abridged MRS, with mood, tense, and other morphosemantic features removed.
EachEP is enclosed in square brackets, bold type denotes predicate values.Type Description Example Count1 Pred value no q rel 4,505,3892 PRP: all no q rel&RSTR&" redeem v for rel" 10,255,0213 PRP: string preds only " redeem v for rel"&ARG2&" trash n 1 rel" 941,8314 PRP: first pred back-off " POS v rel"&ARG2&" trash n 1 rel" 635,0475 PRP: seond pred back-off " redeem v for rel"&ARG2&" NEG n rel" 621,9296 PRP: double back-off " POS v rel"&ARG2&" NEG n rel" 20,9627 SL: handle not a neg rel arg " successful a 1 rel"&" explore v 1 rel" 589,8878 SL: handle a neg rel arg neg rel&" flow v 1 rel"&" well a 1 rel" 43,427Table 2: Sample features (Note: Types 1 - 6 are taken from the MRS in Figure 1)pipeline simply does not output any features forthat sentence.3.2.1 MRS CrawlingIn their revisiting of the 2012 SEM scope of nega-tion shared task, Packard et al.
(2014) improve onthe previous best performance using a relativelysimple set of MRS crawling techniques.
We makeuse of two of these techniques, ?argument crawl-ing?
and ?label crawling?
in extracting our PRPand SL features (ibid., p. 3).
Both include select-ing an ?active EP?
and adding to its scope all EPsthat conform to certain specifications.
Argumentcrawling selects all EPs whose distinguished vari-able or label is an argument of the active EP, whilelabel crawling adds EPs that share a label with theactive EP (ibid., p. 3).Our features are constructed in a similar fash-ion; for every EP in an MRS, the pipeline se-lects all EPs linked to the current EP and con-structs features from this group of ?in-scope?EPs.
PRP and SL features are obtained throughone ?layer?
of argument and label crawling, re-spectively.
After observing a number of noisyand uninformative features in our preliminaryfeature vectors, we excluded a small numberof EPs from being considered as the ?activeEP?
in our pipeline algorithm: udef q rel,proper q rel, named rel, pron rel, andpronoun q rel.
More information about whatexactly these EPs represent can be found in Copes-take et al.
(2005).3.2.2 PRP FeaturesThese feature types are a version of the depen-dency triple features used in Oepen et al.
(2007)and Fujita et al.
(2007).
We define the linking re-lation as one in which the value of any argument ofthe first EP matches the distinguished variable orlabel of the second EP.
For handle variables, wecount any targets of a qeq constraint headed bythat variable as equivalent.
We use the same set ofEP arguments as Pozen (2013) to link predicatesin our PRP features: ARG, ARG1-N, L-INDEX,R-INDEX, L-HANDL, R-HANDL, and RESTR (p.31).We also use a set of negative and positive wordlists from the social media domain, developed byHu and Liu (2004), for back-off replacement inPRP features.
Our pipeline algorithm attemptsback-off replacement for all EPs in all PRP triples.If the surface string portion of the predicate value25Pre-Feature Post-FeatureFeature Types Filtering Filteringbaseline (n-grams only) 88.337 88.2891 88.289 88.5172 87.857 87.8093 88.589 88.7574 88.673 88.7575 88.709 88.8176 88.337 88.3017 88.193 88.2058 88.361 88.265Table 3: Individual MRS feature trial resultsmatches any of the entries in the lexicon, thepipeline produces a back-off predicate value by re-placing that portion with NEG or POS and strip-ping the sense category marker.
These replace-ments appear in various positions in feature types4, 5, and 6 (see Table 2).3.2.3 SL FeaturesTo further explore the relationships in the MRS,we include this second feature category in our fea-ture template, which links together EPs that sharea handle variable.
We limit SL features to groupsof EPs linked by a handle variable that is also anargument of another EP, or the target of a qeq con-straint of such a variable.
Our pipeline is thereforeable to extract both PRP and SL features in a sin-gle pass through the arguments of each EP.
Featuretype 7 consists of shared-label groupings of twoor more EPs, where the handle is not the ARG1of a neg rel EP.
Type 8 includes groups of oneor more EPs where the handle is a neg rel ar-gument, with neg rel prepended to the featurestring.Features of type 7 tend to capture relationshipsbetween modifiers, such as adverbs and adjectives,and modified entities.
Features of type 8 wereintended to provide some negation information,though our goals of more fully analyzing scopeof negation in our dataset remain unrealized atthis point.
We reasoned that the lemmatization ofstring predicate values might provide some usefulback-off for the semantic entities involved in nega-tion and modification.4 EvaluationTo test our MRS features, we adapted our base-line to treat them much like the n-gram features.Pre-Feature Post-FeatureFeature Types Filtering Filteringbaseline (n-grams only) 88.337 88.289n-grams with back-off 87.293 87.503MRS only (all types) 88.253 87.977n-grams, 4, 5 88.709 88.781n-grams, 3, 4, 5, 7, 8 88.961 88.853n-grams, 1, 4, 5 88.637 88.865n-grams, 3, 4, 5 8 88.853 88.961n-grams, 3, 4, 5, 7 88.889 88.973n-grams, 1, 3, 4, 5 88.793 89.021n-grams, 3, 4, 5 88.865 89.093Table 4: Combination feature resultsAs with n-grams, each MRS feature is counted to-ward the probability of the class of its source doc-ument, and a negated version of that feature, withnot prepended, is counted toward the oppositeclass.
We ran our feature filtering trials using thefirst development set, then obtained preliminaryaccuracy figures from our second development set.We began with each feature type in isolation andused these results to inform later experiments us-ing combinations of feature types.
The numbersreported here are the results over the final, held-out test set.Our final test accuracies indicate that three fea-ture types produce the best gains in accuracy:back-off PRPs with first- and second-predicate re-placement (types 4 and 5), and PRPs with stringpredicates only (type 3).
Table 3 displays isolatedfeature test results, while Table 4 ranks the topseven feature combinations in ascending order bypost-feature filtering accuracies.
The bolded fea-ture types show that all of the best combinationruns include one or more of the top three featuresmentioned above.
Notable also are the accura-cies for MRS-based features alone, which fall veryclose to the baseline.
The best accuracies for pre-and post-feature filtering tests appear in bold.The highest accuracy, achieved by running afeature-filtered combination of the baseline?s n-gram features and feature types 3, 4, and 5, re-sulted in a 0.80% increase over the baseline per-formance with feature filtering, and a 0.76% in-crease in the best baseline accuracy overall (ob-tained without feature filtering).
The experimentalbest run successfully categorizes 63 more of the8333 test documents than the baseline best run.Although these gains are small, they account for26a 6.48% reduction in error.Most Informative MRS Featuresnot " NEG a rel"&ARG1& the q rel" NEG a rel"&ARG1& the q rel" POS a rel"&ARG1& the q relnot " POS a rel"&ARG1& the q rel" POS a rel"&ARG1& a q relnot " POS a rel"&ARG1& a q relnot " NEG a rel"&ARG1&" movie n of rel"" NEG a rel"&ARG1&" movie n of rel"a q rel&RSTR&" POS a rel"not a q rel&RSTR&" POS a rel"not " NEG a rel"&ARG1&udef q rel" NEG a rel"&ARG1&udef q relsuperl rel&ARG1&" POS a rel"not superl rel&ARG1&" POS a rel"and c rel&LHNDL&" POS a rel"Table 5: Most informative MRS features5 Discussion5.1 The Most Successful ExperimentsThe test accuracies indicate that our back-off re-placement method, in combination with the simplepredicate-argument relationships captured in PRPtriples, is the most successful aspect of feature de-sign in this project.
However, as our error analysisindicates, back-off is the likely source of many ofour system?s errors (see ?5.2).
Table 5 lists the 15most informative MRS features from our best runbased on mutual information score, all of whichare of feature type 4 or 5.
Note that the notprepended to some features is a function of wayour classifier reads in binary features (as describedin ?2.2), not an indication of grammatical nega-tion.
The success of these partial back-off fea-tures confirms our intuition that the semantic rela-tionships between sentiment-laden terms and otherentities in the sentence offer a reliable indicatorof author sentiment.
When we performed back-off replacement directly on the surface stringsand ran our classifier with n-grams only, we ob-tained accuracies of 87.29% pre-feature filteringand 87.50% post-feature filtering, a small decreasefrom the baseline performance (see Table 4).
Thislends additional support to the idea that the com-bination of sentiment back-off and semantic de-pendencies is significant.
These results also fitwith the findings of of Joshi and Rose (2009), whodetermined that back-off triple features provide?more generalizable and useful patterns?
in sen-timent data than lexical dependency features alone(p. 316).Despite these promising results, we found thatthe separate EP values (type 1), PRP triples with-out replacement (type 2), PRPs with double re-placement (type 6) and SL features (types 7 and8) have very little effect on accuracy by them-selves.
For type 1, we suspect that EP values alonedon?t contribute enough information beyond ba-sic n-gram features.
We had hypothesized that thelemmatization in these values might provide somehelpful back-off.
However, this effect is likelydrowned out by the lack of any scope of negationhandling in the MRS features.We attribute the failure of the SL features to thefact that they often capture EPs originating in ad-jacent tokens in the surface string, which does notimprove on the n-gram features.
Lastly, we be-lieve the relative sparsity of double back-off fea-tures was the primary reason they did not producemeaningful results.These results also call into question the use-fulness of the feature filtering trials in our base-line.
By design, these trials produce performanceincreases on the dataset on which they are run.However, filtering produces small and inconsistentgains for the final held-out test set.Error TypesMisleading back-off 31Plot summary / Noise 20Obscure Words / Data Sparsity 7Data Error 3Nonsensical Review 3Reason Unsure 40Table 6: Error types from top MRS experiment5.2 Error AnalysisWe manually inspected the 104 reviews from thefinal test set that were correctly classified by thebest run of the baseline system but incorrectlyclassified by the best run of our improved sys-tem.
This set contains 50 false negatives, and 54false positives.
We classified them according tofive subjective categories: misleading back-off, inwhich many of the sentiment terms have a polarityopposite to the overall review; excess plot sum-27Incorrectly classified Correctly classifiedNegativedocs"_POS_a__rel"&ARG1&_the_q_rel "_NEG_n__rel"&ARG0&udef_q_rel"_POS_a__rel"&ARG1&_a_q_rel "_NEG_a__rel"&ARG1&_the_q_rel"_POS_a__rel"&ARG1&udef_q_rel "_NEG_a__rel"&ARG1&udef_q_rel"_NEG_n__rel"&ARG0&udef_q_rel "_POS_a__rel"&ARG1&_a_q_rel_a_q_rel&RSTR&"_POS_a__rel" _the_q_rel&RSTR&"_NEG_n__rel"Positivedocs"_NEG_n__rel"&ARG0&udef_q_rel "_POS_a__rel"&ARG1&_a_q_rel"_NEG_v__rel"&ARG1&pronoun_q_rel "_POS_a__rel"&ARG1&_the_q_rel"_NEG_v__rel"&ARG1&pron_rel _a_q_rel&RSTR&"_POS_a__rel""_NEG_a__rel"&ARG1&udef_q_rel "_NEG_n__rel"&ARG0&udef_q_rel"_NEG_a__rel"&ARG1&_the_q_rel "_POS_a__rel"&ARG1&udef_q_relTable 7: Most frequent features in test data by polarity and classification resultmary or off-topic language; use of obscure wordsnot likely to occur frequently in the data; miscat-egorization in the dataset; and confusing or non-sensical language.
The counts for these categoriesappear in Table 6.The prevalence of errors in the first categoryis revealing, and relates to certain subcategoriesof review that confound our sentiment back-offfeatures.
For horror films in particular, wordsthat would generally convey negative sentiment(creepy, horrible, gruesome) are instead used pos-itively.
This presents an obvious problem for senti-ment back-off, which relies on the assumption thatwords are generally used with the same intent.To explore this further, we collected counts ofthe most frequent features in these 104 reviews,and compared them to feature counts for correctlyclassified documents of the same class.
The starkcontrast between the back-off polarities of the fea-tures extracted and the polarity of the documentssuggests that these feature types are overgener-alizing and misleading the classifier (see Table7).
While the course-grained polarity of sentimentterms is often a good indicator of overall reviewpolarity, our system has difficulty with cases inwhich many sentiment terms do not align with thereview sentiment.
Our back-off PRP features donot include scope of negation handling, so even ifthese terms are negated, our classifier in its currentform is unable to take advantage of that informa-tion.Further manual observation of the feature vec-tors from these documents suggests that the senti-ment lexicon contains elements that are not suitedto the movie review domain; plot, for example isclassified as a negative term.
These results pointto the need for a more domain-specific sentimentlexicon, and perhaps additional features that lookat the combination of sentiment terms present in areview.
LDA models could provide some guidancein capturing and analyzing co-occurring groups ofsentiment terms.6 Conclusions and Future WorkOur attempt to improve binary sentiment classifi-cation with MRS-based features is motivated by adesire to move beyond shallow approaches and ex-plore the potential for features based on semanticdependencies.
Our preliminary results are promis-ing, if modest, and point to back-off replacementas a useful tool in combination with the relation-ships captured by predicate triples.There are a number of potential areas for im-provement and further development of our ap-proach.
In light of Wang and Manning?s (2012) re-sults using an SVM classifier on the same dataset,one obvious direction would be to experiment withthis and other machine learning algorithms.
Addi-tionally, the ability to account for negation in theMRS features types as in Packard et al.
(2014)would likely mitigate some of the errors causedby the back-off PRP featuresAnother possibility for expansion would be thedevelopment of features using larger feature sub-graphs.
Because of concerns about runtime anddata sparsity, we crawl only one level of the MRSand examine a limited set of relationships.
Thesuccess of Socher et al.
?s (2013) Recursive NeuralTensor Network suggest that with enough data, itis possible to capture the complex compositional28effects of various sub-components.
Given theirsuccess with syntactic dependencies, and the re-search presented here, we believe semantic de-pendencies will be a fruitful avenue for future re-search in sentiment analysis.
This project has beenan exciting first step into uncharted territory, andsuggests the potential to further exploit the MRSin sentiment analysis applications.
Nonetheless,the performance gains we were able to observedemonstrate the power of using semantic repre-sentations produced by a linguistically motivated,broad-coverage parser as an information sourcein a semantically sensitive task such as sentimentanalysis.AcknowledgmentsThanks to our professor, Emily Bender, for pro-viding her expertise and guidance at all stages ofthis project.We?re grateful to Michael Goodman for makinghis pyDelphin module freely available, guidingus in using it, and providing timely troubleshoot-ing and support via email for the duration of thisproject.
Thanks to Woodley Packard, who pro-vided helpful advice on getting the best use outof ACE.ReferencesS.
Arora, E. Mayfield, C Penstein-Ros?e, and E Nyberg.2010.
Sentiment Classification using AutomaticallyExtracted Subgraph Features.
In Proceedings of theNAACL HLT 2010 Workshop on Computational Ap-proaches to Analysis and Generation of Emotion inText, pp.
131 - 139.
Los Angeles, CA.A.
Copestake, D. Flickinger, C. Pollard, and I.
A. Sag.2005.
Minimal Recursion Semantics: An Introduc-tion.
Research on Natural Language and Computa-tion, 3(4), pp.
281 - 332.D.
Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering,, 6(1), pp.
15 - 28.S.
Fujita, F. Bond, S. Oepen, T. Tanaka.
2010.
Exploit-ing semantic information for HPSG parse selection.Research on Language and Computation.
8(1): 1-22M.
Hu and B. Liu.
2004.
Mining and Summariz-ing Customer Reviews.
In Proceedings of the ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD-2004).
Seattle,WA.M.
Joshi and C Penstein Ros?e.
2009.
GeneralizingDependency Features for Opinion Mining.
In Pro-ceedings of the ACL-IJCNLP 2009 Conference ShortPapers, pp.
313 - 316.
Suntec, Singapore.E.
Loper, and S., Bird.
2002.
NLTK: The Natu-ral Language Toolkit.
In Proceedings of the ACLWorkshop on Effective Tools and Methodologies forTeaching Natural Language Processing and Compu-tational Linguistics.
Philadelphia: Association forComputational LinguisticsL.
Jia, C. Yu, and W. Meng.
2009.
The effect of nega-tion on sentiment analysis and retrieval effective-ness.
In Proceedings of the 18th ACM conferenceon Information and knowledge management (CIKM?09), pp.
1827 - 1830.
Hong Kong, China.A.
L. Maas, R. E. Daly, Peter T. Pham, Dan Huang,Andrew Y. Ng, and C. Potts.
2011.
Learning wordvectors for sentiment analysis.
In Proceedings of the49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies, pp.
142 - 150.
Portland, Oregon.V.
Narayanan, I. Arora, and A. Bhatia.
2013.
Fast andaccurate sentiment classification using an enhancedNaive Bayes?model.
Intelligent Data Engineeringand Automated Learning IDEAL function LectureNotes in Computer Science, 8206:194 - 201.S.
Oepen, E. Velldal, J. Lonning, P. Meurer, V. Rosn,and D. Flickinger.
2007.
Towards Hybrid Qual-ity Oriented Machine Translation.
In Proceedingsof the 11th International Conference on Theoreticaland Methodological Issues in Machine Translation.W.
Packard, E. M. Bender, J.
Read, S. Oepen and R.Dridan.
2014.
Simple Negation Scope ResolutionThrough Deep Parsing: A Semantic Solution to aSemantic Problem.
In Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics.
Baltimore, MD.B.
Pang and L. Lee.
2008.
Opinion Mining and Senti-ment Analysis.
Foundations and Trends in Informa-tion Retrieval, 2(1-2):1 - 135.Z.
Pozen.
2013.
Using Lexical and Compositional Se-mantics to Improve HPSG Parse Selection.
Master?sThesis, University of Washington.R.
Socher, A. Perelygin, J. Wu, J. Chuang.
C. Man-ning, A. Ng, and C. Potts 2013.
Recursive DeepModels for Semantic Compositionality Over a Sen-timent Treebank In Proceedings of the 2013 Con-ference on Empirical Methods in Natural LanguageProcessing, pp.
1631-1642.
Seattle, WA.S.
Wang and C. D. Manning.
2012.
Baselines and Bi-grams: Simple, Good Sentiment and Topic Classica-tion.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics, pp.90 94.
Jeju, Republic of Korea.A Yeh.
2000.
More accurate tests for the statisticalsignificance of result differences.
In Proceedings ofthe 18th Conference on Computational Linguistics(COLING), pp.
147 - 153.29
