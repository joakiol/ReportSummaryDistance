Controlling Animated Agents in Natural LanguageKotaro FunakoshiDepartment of Computer Science,Tokyo Institute of Technology2-12-1 Oookayama, Meguro,Tokyo 152-8552, Japankoh@cl.cs.titech.ac.jpTakenobu TokuganaDepartment of Computer Science,Tokyo Institute of Technology2-12-1 Oookayama, Meguro,Tokyo 152-8552, Japantake@cl.cs.titech.ac.jpAbstractThis paper presents a prototype dia-logue system, K3 , in which a user caninstruct agents through speech input tomanipulate various objects in a 3-D vir-tual world.
In this paper, we focuson two distinctive features of the K3system: plan-based anaphora resolutionand handling vagueness in spatial ex-pressions.
After an overview of the sys-tem architecture, each of these featuresis described.
We also look at the futureresearch agenda of this system.1 IntroductionSHRDLU (?)
can be considered as the most im-portant natural language understanding system.Although SHRDLU was not ?embodied?, hav-ing had only a small stick to manipulate objects,it certainly had several features that a conversa-tional agent is supposed to have.
It had a greatpotential, and it was very promising for future re-search on natural language understanding.Recently better technologies have becomeavailable in speech recognition and natural lan-guage processing.
Major breakthroughs in thearea of computer graphics have enabled us to gen-erate complex, yet realistic 3-D animated agentsor embodied life-like agents in a virtual environ-ment.
Researchers are now in a good position togo beyond SHRDLU by combining these tech-nologies (?).
This paper presents a conversationalanimated agent system, K3 .Since all the actions carried out by an agent ofthe K3 system are visible, we can evaluate theperformance of the system by observing its an-imation.
Visualizing the agents?
actions yieldsmany interesting issues from a cognitive sciencepoint of view; more complex processes are in-volved than those found in most conventional nat-ural language understanding systems.After sketching out the overview of the K3 sys-tem in section 2, Two distinctive features of K3are discussed in section 3, and 4.
Finally, sec-tion 5 concludes the paper and looks at future re-search agenda.2 System OverviewA screen shot of K3 is shown in Fig.
1.
There aretwo agents and several objects in a virtual world.The current system accepts simple Japanese utter-ances with anaphoric and elliptical expressions,such as ?Walk to the desk.?
and ?Further?.
Thesize of the lexicon is about 100 words.Figure 1: A screenshot of K391The architecture of the K3 is illustrated inFig.
2. system.
The speech recognition modulereceives the user?s speech input and generates asequence of words.
The syntactic/semantic anal-ysis module analyzes the word sequence to ex-tract a case frame.
This module accepts ill-formedspeech input including postposition omission, in-version, and self-correction.
At this stage, notall case slots are necessarily filled, because of el-lipses in the utterance.
Even in cases where thereis no ellipsis, instances of objects are not identi-fied at this stage.Resolving ellipses and anaphora, and identify-ing instances in the world are performed by thediscourse analysis module.
Anaphora resolutionand instance identification are achieved by usingplan-knowledge, which will be described in sec-tion 3.The discourse analysis module extracts theuser?s goal as well and hands it over to the plan-ning modules, which build a plan to generate theappropriate animation.
In other words, the plan-ning modules translate the user?s goal into anima-tion data.
However, the properties of these twoends are very different and straightforward trans-lation is rather difficult.
The user?s goal is repre-sented in terms of symbols, while the animationdata is a sequence of numeric values.
To bridgethis gap, we take a two-stage approach ?
macro-and micro-planning.During the macro-planning, the planner needsto know the physical properties of objects, suchas their size, location and so on.
For example, topick up a ball, the agent first needs to move to thelocation at which he can reach the ball.
In thisplanning process, the distance between the balland the agent needs to be calculated.
This sortof information is represented in terms of coordi-nate values of the virtual space and handled by themicro-planner.To interface the macro- and micro-planning,we introduced the SPACE object to represent a lo-cation in the virtual space by its symbolic and nu-meric character.
The SPACE object is described insection 4.3 Plan-based Anaphora Resolution3.1 Surface-clue-based Resolution vs.Plan-based ResolutionConsider the following two dialogue examples.
(1-1) ?Agent X, push the red ball.?
(1-2) ?Move to the front of the blue ball.?
(1-3) ?Push it.?
(2-1) ?Agent X, pick up the red ball.?
(2-2) ?Move to the front of the blue ball.?
(2-3) ?Put it down.
?The second dialogue is different from the firstone only in terms of the verbs in the first and thirdutterances.
The syntactic structure of each sen-tence in the second dialogue (2-1)?
(2-3) is thesame as the corresponding sentence in the firstdialogue (1-1)?(1-3).
However, pronoun ?it?
in(1-3) refers to ?the blue ball?
in (1-2), and pro-noun ?it?
in (2-3) refers to ?the red ball?
in (2-1).The difference between these two examples is notexplained by the theories based on surface cluessuch as the centering theory (?
; ?
).In the setting of SHRDLU-like systems, theuser has a certain goal of arranging objects in theworld, and constructs a plan to achieve it throughinteraction with the system.
As Cohen pointedout, users tend to break up the referring and pred-icating functions in speech dialogue (?).
Thus,each user?s utterance suggests a part of plan ratherthan a whole plan that the user tries to perform.To avoid redundancy, users need to use anaphora.From these observations, we found that consid-ering a user?s plan is indispensable in resolvinganaphora in this type of dialogue system and de-veloped an anaphora resolution algorithm usingthe relation between utterances in terms of partialplans (plan operators) corresponding to them.The basic idea is to identify a chain of plan op-erators based on their effects and preconditions.Our method explained in the rest of this sectionfinds preceding utterances sharing the same goalas the current utterance with respect to their cor-responding plan operators as well as surface lin-guistic clues.92!"#$%&?()**"*+,-.)*/"#0"#/"%*)$12-*0-$"*+,1*/)#/"#3,-.
)*/"#&)*)(14"45()*("6$)$17"$/8)(9%$(0://-$)*#-;"4/%$1<*/%(%+1=%$00"#/"%*)$1>)*+8)+-.%0-(=%$0&4-?8-*#- @)4-&A$).- B%)(C)4"#.%D-.-*/!"#$%&?()*+@%%$0"*)/-&D)(8-,?--#;&"*?8/E*".)/"%*F"4#%8$4-)*)(14"4,?)#-$-#%+*"/"%*!)#$%?
()**"*+!%D-.-*/+-*-$)/"%*,?--#;$-#%+*"/"%*Figure 2: The system architecture of K33.2 Resolution AlgorithmRecognized speech input is transformed into acase frame.
At this stage, anaphora is not re-solved.
Based on this case frame, a plan opera-tor is retrieved in the plan library.
This processis generally called ?plan recognition.?
A planoperator used in our system is similar to that ofSTRIPS (?
), which consists of precondition, ef-fect and action description.Variables in the retrieved plan operator arefilled with case fillers in the utterance.
Theremight be missing case fillers when anaphora (zeropronoun) is used in the utterance.
The systemtries to resolve these missing elements in the planoperator.
To resolve the missing elements, thesystem again uses clue words and the plan library.An overview of the anaphora resolution algorithmis shown in Figure 3.When the utterance includes clue words, thesystem uses them to search the history databasefor the preceding utterance that shares the samegoal as the current utterance.
Then, it identifiesthe referent on the basis of case matching.There are cases in which the proper precedingutterance cannot be identified even with the cluewords.
These cases are sent to the left branch inFig.
3 where the plan library is used to resolveanaphora.When there is no clue word or the clue worddoes not help to resolve the anaphora, the processgoes through the left branch in Fig.
3.
First, thesystem enumerates the candidates of referents us-ing the surface information, then filters them outwith linguistic clues and the plan library.
For ex-ample, demonstratives such as ?this?, ?that?
areusually used for objects that are in the user?s view.Therefore, the referent of anaphora with demon-stratives is restricted to the objects in the currentuser?s view.If the effect of a plan operator satisfies the pre-condition of another plan operator, and the utter-ances corresponding to these plan operators areuttered in discourse, they can be considered tointend the same goal.
Thus, identifying a chainof effect-precondition relations gives importantinformation for grouping utterances sharing thesame goal.
We can assume an anaphor and itsreferent appear within the same utterance group.Once the utterance group is identified, the sys-tem finds the referent based on matching variablesbetween plan operators.After filtering out the candidates, there stillmight be more than one candidate left.
In such acase, each candidate is assigned a score that is cal-culated based on the following factors: saliency,agent?s view, and user?s view.4 Handling Spatial VaguenessTo interface the macro- and micro-planning, weintroduced the SPACE object which represents alocation in the virtual world.
Because of spacelimitations, we briefly explain the SPACE object.The macro planner uses plan operators de-scribed in terms of the logical forms.
Thus, the93Utteranceincludes clueword?Enumeratecandidates bysurface informationIdentify utteranceincluding referentby clue wordAnaphoraresolved?Uniquecandidate?FilteringcandidatesUniquecandidate?ScoringReferentidentifiednoyesnonoyesResolve anaphoraby case matchingnoyesyesFigure 3: Anaphora resolution algorithmSPACE object is designed to behave as a sym-bolic object in the macro-planning by referring toits unique identifier.
On the other hand, a loca-tion could be vague and the most plausible placechanges depending on the situation.
Therefore, itshould be treated as a certain region rather than asingle point.
To fulfill this requirement, we adoptthe idea of the potential model proposed by Ya-mada et al (?).
Vagueness of a location is nat-urally realized as a potential function embeddedin the SPACE object.
The most plausible point iscalculated by using the potential function with theSteepest Descent Method on request.Consider the following short conversation be-tween a human (H) and a virtual agent (A).H: Do you see a ball in front of the desk?A: Yes.H: Put it on the desk.When the first utterance is given in the situationshown in Fig.
1, the discourse analysis moduleidentifies an instance of ?a ball?
in the followingsteps.backleftrightDeskfrontViewpointBall(1)(2)(3)(4)Figure 4: Adjustment of axis(A) space#1 := new inFrontOf(desk#1, viewpoint#1,MIRROR)(B) list#1 := space#1.findObjects()(C) ball#1 := list#1.getFirstMatch(kindOf(BALL))In step (A), an instance of SPACE is created asan instance of the class inFrontOf.
The construc-tor of inFrontOf takes three arguments: the ref-erence object, the viewpoint, and the axis order.Although it is necessary to identify the referenceframe, we focus on the calculation of potentialfunctions given a reference frame.Suppose the parameters of inFrontOf have beenresolved in the preceding steps, and the discourseanalysis module chooses the axis mirror order andthe orientation of the axis based on the viewpointof the light-colored arrows in Fig.
4.
The closestarrow to the viewpoint-based ?front?
axis ((1) inFig.
4) is chosen as the ?front?
of the desk.
Then,the parameters of potential function correspond-ing to ?front?
are set.In step (B), the method matchObjects() returnsa list of objects located in the potential field ofspace#1 shown in Fig.
5.
The objects in the list aresorted in descending order of the potential valueof their location.In step (C), the most plausible object satisfy-ing the type constraint (BALL) is selected by themethod getFirstMatch().When receiving the next utterance, ?Put it onthe desk.
?, the discourse analysis module resolvesthe referent of the pronoun ?it?
and extracts theuser?s goal.walk(inFrontOf(ball#1, viewpoint#1, MIRROR)AND reachableByHand(ball#1)AND NOT(occupied(ball#1)))The movement walk takes a SPACE object rep-resenting its destination as an argument.
In thisexample, the conjunction of three SPACE objects94ViewpointFigure 5: Potential field of space#1is given as the argument.
The potential functionof the resultant SPACE is calculated by multiply-ing the values of the corresponding three potentialfunctions at each point.As this example illustrates, the SPACE objecteffectively plays a role as a mediator between themacro and micro planning.5 Conclusions and Future WorkWe have introduced our prototype systemK3 , twodistinctive features of which are described in thispaper.
Plan-based anaphora resolution enablesK3 to interpret the user?s intention more pre-cisely than the previous, surface-cue-based reso-lution algorithms.
The SPACE object is designedto bridge the gap between the symbolic system(language processing) and the continuous system(animation generation) .
In what follows, we de-scribe the research agenda of our project.One-to-many Conversation.
Conversationalagent systems should deal with one-to-many con-versations as well as one-to-one conversations.In a one-to-many conversation, it is not easy todecide who is the intended listener.
The situationgets worse when a speaker is concerned withonly performing an action without caring whodoes it.
In such cases, agents have to requestclarifications or negotiate among themselves.Agent Coordination.
In one-to-many conver-sations, agents must coordinate each other.
Somesorts of coordination are explicitly requested byuser, e.g., ?Agent A and B tidy up the table,please.?
But other kinds of coordination are im-plicitly requested, e.g., ?Agent A hands agent Bthe box, please.?
In this case, the speaker asksagent B nothing explicitly.
However, agent Bmust react to the request for agent A and coor-dinate with agent A to receive a box.Parallel Actions.
Most intelligent agent sys-tems perform only one action at a time.
Yet, if wewant to make systems become more flexible, wemust enable them to handle more than one actionat a time.
Hence, they must speak while walking,wave while nodding, and so on.Memory System.
A history database is notenough to serve realistic dialogue in the domainof K3 .
In such a domain, people often mention aprevious state, e.g., ?Put the ball back to the placewhere it was.?
To comply such a request, agentsmust have a human-like memory system.Interruption Handling.
Agents sometimesmisunderstand requests and perform not intendedactions.
In case of human conversations, aspeaker usually interrupts hearer and try to repairmisunderstanding.
Conversational agents alsoshould be able to accept such interruptions.Interruption handling is also essential to requesta next action before agents finish actions.AcknowledgmentThis work is partially supported by a Grant-in-Aid for Creative Scientific Research 13NP0301,the Ministry of Education, Culture, Sports, Sci-ence and Technology of Japan.
The URL ofthe project is http://www.cl.cs.titech.ac.jp/sinpro/en/index.html.ReferencesP.
R. Cohen.
1984.
The pragmatics of referring andthe modality of communication.
ComputationalLinguistics, 10(2):97?146.R.
E. Fikes.
1971.
STRIPS: A new approach to theapplication of theorem problem solving.
ArtificialIntelligence, 2:189?208.B.
J. Grosz, A. K. Joshi, and P. Weinstein.
1995.Centering: A framework for modeling the local co-herence of discourse.
Computational Linguistics,21(2):203?226.H.
Tanaka, T. Tokunaga, and Y. Shinyama.
2004.
An-imated agents capable of understanding natural lan-guage and performing actions.
In Life-Like Char-acters, pages 429?444.
Springer.95M.
A. Walker, A. K. Joshi, and E. F. Prince, editors.1998.
Centering Theory in Discourse.
ClarendonPress Oxford.T.
Winograd.
1972.
Understanding Natural Lan-guage.
Academic Press.A.
Yamada, T. Nishida, and S. Doshita.
1988.
Fig-uring out most plausible interpretation from spa-tial description.
In the 12th International Con-ference on Computational Linguistics (COLING),pages 764?769.96
