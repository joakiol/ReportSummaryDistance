Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1116?1127,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsA Fast Re-scoring Strategy to Capture Long-Distance DependenciesAnoop DeorasHLT-COE and CLSPJohns Hopkins UniversityBaltimore MD 21218, USAadeoras@jhu.edu,Toma?s?
MikolovBrno University of TechnologySpeech@FITCzech Republicimikolov@fit.vutbr.cz,Kenneth ChurchHLT-COE and CLSPJohns Hopkins UniversityBaltimore MD 21218, USAkenneth.church@jhu.eduAbstractA re-scoring strategy is proposed that makesit feasible to capture more long-distance de-pendencies in the natural language.
Two passstrategies have become popular in a num-ber of recognition tasks such as ASR (au-tomatic speech recognition), MT (machinetranslation) and OCR (optical character recog-nition).
The first pass typically applies aweak language model (n-grams) to a latticeand the second pass applies a stronger lan-guage model to N best lists.
The stronger lan-guage model is intended to capture more long-distance dependencies.
The proposed methoduses RNN-LM (recurrent neural network lan-guage model), which is a long span LM, to re-score word lattices in the second pass.
A hillclimbing method (iterative decoding) is pro-posed to search over islands of confusabilityin the word lattice.
An evaluation based onBroadcast News shows speedups of 20 overbasic N best re-scoring, and word error ratereduction of 8% (relative) on a highly compet-itive setup.1 IntroductionStatistical Language Models (LMs) have receivedconsiderable attention in the past few decades.
Theyhave proved to be an essential component in manystatistical recognition systems such as ASR (au-tomatic speech recognition), MT (machine trans-lation) and OCR (optical character recognition).The task of a language model is to assign prob-ability to any word sequence possible in the lan-guage.
The probability of the word sequence W ?w1, .
.
.
, wm ?
wm1 is typically factored using thechain rule:P (wm1 ) =m?i=1P (wi|wi?11 ) (1)In modern statistical recognition systems, an LMtends to be restricted to simple n-gram models,where the distribution of the predicted word dependson the previous (n ?
1) words i.e.
P (wi|wi?11 ) ?P (wi|wi?1i?n+1).Noam Chomsky argued that n-grams cannot learnlong-distance dependencies that span over more thann words (Chomsky, 1957, pp.13).
While that mightseem obvious in retrospect, there was a lot of ex-citement at the time over the Shannon-McMillan-Breiman Theorem (Shannon, 1948) which was inter-preted to say that, in the limit, under just a couple ofminor caveats and a little bit of not-very-importantfine print, n-gram statistics are sufficient to captureall the information in a string (such as an Englishsentence).
Chomsky realized that while that may betrue in the limit, n-grams are far from the most parsi-monious representation of many linguistic facts.
Ina practical system, we will have to truncate n-gramsat some (small) fixed n (such as trigrams or perhaps5-grams).
Truncated n-gram systems can capturemany agreement facts, but not all.1By long-distance dependencies, we mean factslike agreement and collocations that can span overmany words.
With increasing order of n-gram mod-els we can, in theory, capture more regularities in the1The discussion in this paragraph is taken as-is from an arti-cle (to appear) by Church (2012).1116language.
In addition, if we can move to more gen-eral models then we could hope to capture more, aswell.
However, due to data sparsity, it is hard to es-timate a robust n-gram distribution for large valuesof n ( say, n > 10) using the conventional Max-imum Likelihood techniques, unless a more robusttechnique is employed for modeling which gener-alizes well on unseen events.
Some of these wellknown long span / complex language models whichhave shown to perform very well on many speechtasks include: structured language model (Chelbaand Jelinek, 2000; Roark, 2001; Wang and Harper,2002; Filimonov and Harper, 2009), latent seman-tic analysis language model (Bellegarda, 2000),topic mixture language models (Iyer and Ostendorf,1999), whole sentence exponential language mod-els (Rosenfeld, 1997; Rosenfeld et al, 2001), feed-forward neural networks (Bengio et al, 2001), re-current neural network language models (Mikolovet al, 2010), among many others.Although better modeling techniques can nowcapture longer dependencies in a language, theirincorporation in decoders of speech recognition ormachine translation systems becomes computation-ally challenging.
Due to the prohibitive increase inthe search space of sentence hypotheses (or longerlength word sub sequences), it becomes challengingto use a long span language model in the first passdecoding.
A word graph (word lattices for speechrecognition systems and hypergraphs for machinetranslation systems), encoding exponential numberof hypotheses is hence outputted at the first pass out-put on which a sophisticated and complex languagemodel is deployed for re-scoring.
However, some-times even re-scoring of this refined search spacecan be computationally expensive due to explosionof state space.Previously, we showed in (Deoras et al, 2011)how to tackle the problem of incorporating long spaninformation during decoding in speech recogni-tion systems by variationaly approximating (Bishop,2006, pp.
462) the long span language model by atractable substitute such that this substitute modelcomes closest to the long span model (closest interms of Kullback Leibler Divergence (Cover andJ.A.Thomas, 1991, pp.
20)).
The tractable substi-tute was then used directly in the first pass speechrecognition systems.
In this paper we propose anapproach that keeps the model intact but approxi-mates the search space instead (which can becomeintractable to handle especially under a long spanmodel), thus enabling the use of full blown modelfor re-scoring.With this approach, we can achievefull lattice re-scoring with a complex model, at acost more than 20 times less than of a naive bruteforce approach that is commonly used today.The rest of the paper is organized as follows:We discuss a particular form of long span languagemodel in Sec.
2.
In Sec.
3 we discuss two standardre-scoring techniques and then describe and demon-strate our proposed technique in Sec.
4.
We presentexperimental results in Sec.
5 followed by conclu-sions and some remarks in Sec.
6.2 Recurrent Neural Networks (RNN)There is a long history of using neural networks tomodel sequences.
Elman (1990) used recurrent neu-ral network for modeling sentences of words gen-erated by an artificial grammar.
Work on statisticallanguage modeling of real natural language data, to-gether with an empirical comparison of performanceto standard techniques was done by Bengio et al(2001).
His work has been followed by Schwenk(2007), who has shown that neural network languagemodels actually work very well in the state-of-the-art speech recognition systems.
Recurrent Neu-ral Network based Language Models (RNN-LMs)(Mikolov et al, 2010) improved the ability of theoriginal model to capture patterns in the languagewithout using any additional features (such as partof speech, morphology etc) i.e.
other than lexicalones.
The RNN-LM was shown to have superiorperformance than the original feedforward neuralnetwork (Mikolov et al, 2011b).
Recently, we alsoshowed that this model outperforms many other ad-vanced language modeling techniques (Mikolov etal., 2011a).
We hence decided to work with thismodel.
This model uses whole history to make pre-dictions, thus it lies outside the family of n-grammodels.
Power of the model comes at a considerablecomputational cost.
Due to the requirement of un-limited history, many optimization tricks for rescor-ing with feedforward-based NNLMs as presented bySchwenk (2007) cannot be applied during rescoringwith RNN LM.
Thus, this model is a good candidate1117w( t )s( t )y( t )(delayed)U VFigure 1: Schematic Representation of Recurrent Neu-ral Network Language Model.
The network has an inputlayer w, a hidden layer s and an output layer y. MatricesU and V represent synapses.to show effectiveness and importance of our work.The basic RNNLM is shown in Fig.
1.
The modelhas an input layer w(t) that encodes previous wordusing 1 of N coding (thus, the size of the input layeris equal to the size of the vocabulary, and only theneuron that corresponds to the previous word in asequence is set to 1).
The hidden layer s(t) has addi-tional recurrent connections that are delayed by onetime step.
After the network is trained, the outputlayer y(t) represents probability distribution for thecurrent word, given the previous word and the stateof the hidden layer from the previous time step.The training is performed by ?backpropagation-through-time?
algorithm that is commonly used fortraining recurrent neural networks (Rumelhart et al,1986).
More details about training, setting initial pa-rameters, choosing size of the hidden layer etc.
arepresented in (Mikolov et al, 2010).
Additional ex-tensions that allow this model to be trained on largecorpora are presented in (Mikolov et al, 2011b).3 Standard Approaches for Rescoring3.1 Word Lattice RescoringA word lattice, L, obtained at the output of the firstpass decoding, encodes exponential number (expo-nential in the number of states (nodes) present inthe lattice) of hypotheses in a very compact datastructure.
It is a directed acyclic graph G =(V, E , ns, Ne), where V and E denote set of vertices(nodes / states) and edges (arcs / links), respectively.ns and Ne denote the unique start state and set ofend states.A path, pi, in a lattice is an element of E?
withconsecutive transitions.
We will denote the origin /previous state of this path by p[pi] and destination /next state of this path by n[pi].
A path, pi is calleda complete path if p[pi] = ns and n[pi] ?
Ne.
Apath, pi, is called a partial path if p[pi] = ns but n[pi]may or may not belong to Ne.
A path, pi, is calleda trailing path if p[pi] may or may not be equal tons and n[pi] ?
Ne.
We will also denote the timestamp at the start of the path by Ts[pi] and the timestamp at the end of the path by Te[pi].
Since thereare nodes attached to the start and end of any path,we will denote the time stamp at any node u ?
V byT [u].
Associated with every path, pi, is also a wordsequence W [pi] ?
W?, where W is the vocabularyused during speech recognition.
For the sake of sim-plicity, we will distinguish word sequence of length1 from the word sequences of length greater than 1by using lower and upper casing i.e.
w[?]
and W [?
]respectively.The acoustic likelihood of the path pi ?
E?
is thengiven as:A[pi] =|pi|?j=1P (aj |w[pij ])where ?j ?
{1, 2, .
.
.
, |pi|} pij ?
E , pi = |pi|j=1pijand P (aj |w[pij ]) is the acoustic likelihood of theacoustic substring aj , spanning between Ts[pij ] andTe[pij ], conditioned on the word w[pij ] associatedwith the edge pij .2 Similarly, the language modelscore of the path pi is given as:L[pi] =|pi|?j=1P (w[pij ]|w[pij?1], .
.
.
, w[pij?m+1])where P (w[pij ]|w[pij?1], .
.
.
, w[pij?m+1]) is them-th order Markov approximation for estimating theprobability of a word given the context upto thatpoint.
The speech recognizer, which uses m-th or-der Markov LM for first pass recognition, imposes aconstraint on the word lattice such that at each statethere exists an unambiguous context of consecutivem?
1 words.A first pass output is then a path pi?
having Max-imum a Posterior (MAP) probability.3 Thus pi?
is2We will use  symbol to denote concatenation of paths orword strings.3Note that asterisk symbol here connotes that the path is op-1118obtained as:pi?
= argmaxpi:p[pi]=nsn[pi]?NeA[pi]?L[pi],where ?
is the scaling parameter needed to balancethe dynamic variability between the distributions ofacoustic and language model (Ogawa et al, 1998).Efficient algorithms such as single source shortestpath (Mohri et al, 2000) can be used for finding outthe MAP path.Under a new n-gram Language Model, rescor-ing involves replacing the existing language modelscores of all paths pi.
If we denote the new languagemodel by Lnew and correspondingly the score of thepath pi by Lnew[pi], then it is simply obtained as:Lnew[pi] =|pi|?j=1P (w[pij ]|w[pij?1], .
.
.
, w[pij?n+1])where P (w[pij ]|w[pij?1], .
.
.
, w[pij?n+1]) is the n-th order Markov approximation for estimating theprobability of a word given the unambiguous con-text of n ?
1 words under the new rescoring LM.If the Markov rescoring n-gram LM needs a biggercontext for the task of prediction (i.e.
n > m, wherem?
1 is the size of the unambiguous context main-tained at every state of the word lattice), then eachstate of the lattice has to be split until an unambigu-ous context of length as large as that required by thenew re-scoring language model is not maintained.The best path, pi?
is then obtained as:pi?
= argmaxpi:p[pi]=nsn[pi]?NeA[pi]?Lnew[pi],where ?
acts as the new scaling parameter whichmay or may not be equal to the old scaling parameter?.It should be noted that if the rescoring LM needs acontext of the entire past in order to predict the nextword, then the lattice has to be expanded by splittingthe states many more times.
This usually blows upthe search space even for a reasonably small numbertimal under some model.
This should not be confused with theKleene stars appearing as superscripts for E andW , which servethe purpose of regular expressions implying 0 or many occu-rances of the element of E and V respectively.of state splitting iterations.
When the task is to dorescoring under a long span LM, such as RNN-LM,then exact lattice re-scoring option is not feasible.
Inorder to tackle this problem, a suboptimal approachvia N best list rescoring is utilized.
The details ofthis method are presented next.3.2 N best List RescoringN best list re-scoring is a popular way to cap-ture some long-distance dependencies, though themethod can be slow and it can be biased toward theweaker language model that was used in the firstpass.Given a word lattice, L, top N paths{pi1, .
.
.
, piN} are extracted such that their jointlikelihood under the baseline acoustic and languagemodels are in descending order i.e.
that:A[pi1]?L[pi1] ?
A[pi2]?L[pi2] ?
.
.
.
?
A[piN ]?L[piN ]Efficient algorithms exist for extractingN best pathsfrom word lattices (Chow and Schwartz, 1989;Mohri and Riley, 2002).
If a new language model,Lnew, is provided, which now need not be restrictedto finite state machine family, then that can be de-ployed to get the score of the entire path pi.
If wedenote the new LM scores by Lnew[?
], then under Nbest list paradigm, optimal path p?i is found out suchthat:p?i = argmaxpi?
{pi1,...,piN}A[pi]?Lnew[pi], (2)where ?
acts as the new scaling parameter whichmay or may not be equal to ?.
If N  |L| (where|L| is the total number of complete paths in word lat-tice, which are exponentially many), then the pathobtained using (2) is not guaranteed to be optimal(under the rescoring model).
The short list of hy-potheses so used for re-scoring would yield subop-timal output if the best path pi?
(according to thenew model) is not present among the top N candi-dates extracted from the lattice.
This search spaceis thus said to be biased towards a weaker modelmainly because the N best lists are representative ofthe model generating them.
To illustrate the idea,we demonstrate below a simple analysis on a rel-atively easy task of speech transcription on WSJdata.4 In this setup, the recognizer made use of a bi-4Full details about the setup can be found in (Deoras et al,2010)1119gram LM to produce lattices and hence N best lists.Each hypothesis in this set got a rank with the topmost and highest scoring hypothesis getting a rankof 1, while the bottom most hypothesis getting arank of N .
We then re-scored these hypotheses witha better language model (either with a higher orderMarkov LM i.e.
a trigram LM (tg) or the log linearcombination of n-gram models and syntactic mod-els (n-gram+syntactic) and re-ranked the hypothe-ses to obtain their new ranks.
We then used Spear-man?s rank correlation factor, ?, which takes valuesin [?1,+1], with ?1 meaning that the two rankedlists are negatively correlated (one list is in a reverseorder with respect to the other list) and +1 mean-ing that the two ranked lists are positively correlated(the two lists are exactly the same).
Spearman?s rankcorrelation factor is given as:?
= 1?
6?Nn=1 d2nN(N2 ?
1) , (3)where dn is the difference between the old and newrank of the nth entry (in our case, difference betweenn(?
{1, 2, .
.
.
, N}) and the new rank which the nthhypothesis got under the rescoring model).Table 1 shows how the correlation factor dropsdramatically when a better and a complementaryLM is used for re-scoring, suggesting that theN bestlists are heavily biased towards the starting models.Huge re-rankings suggests there is an opportunity toimprove and also a need to explore more hypotheses,i.e.
beyond N best lists.Model (?)
WER (%)bg 1.00 18.2%tg 0.41 17.4%n-gram+syntactic 0.33 15.8%Table 1: Spearman Rank Correlation on the N best listextracted from a bi-gram language model (bg) and re-scored with relatively better language models including,trigram LM (tg), and the log linear combination of n-gram models, and syntactic models (n-gram+syntactic).With a bigger and a better LM, the WER decreases atthe expense of huge re-rankings of N best lists, onlysuggesting the fact that N best lists generated under aweaker model, are not reflective enough of a relativelybetter model.In the next section, we propose an algorithmwhich keeps the representation of search space assimple as that of N best list, but does not restrict it-self to topN best paths alone and hence does not getbiased towards the starting weaker model.4 Proposed Approach for RescoringA high level idea of our proposed approach is toidentify islands of confusability in the word latticeand replace the problem of global search over wordlattice by series of local search problems over theseislands in an iterative manner.
The motivation be-hind this strategy is the observation that the recog-nizer produces bursts of errors such that they havea temporal scope.
The recognizer output (sentencehypotheses) when aligned together typically showsa pattern of confusions both at the word level andat the phrase level.
Regions where there are sin-gleton words competing with one another (reminis-cent of a confusion bin of a Confusion Network(CN) (Mangu, 2000)), choice of 1 word edit dis-tance works well for the formation of local neigh-borhood.
Regions where there are phrases com-peting with other phrases, choice of variable lengthneighborhood works well.
Previously, Richardsonet al (1995) demonstrated a hill climbing frame-work by exploring 1 word edit distance neighbor-hood, while in our own previous work (Deoras andJelinek, 2009), we demonstrated working of iterativedecoding algorithm, a hill climbing framework, forCNs, in which the neighborhood was formed by allwords competing with each other in any given timeslot, as defined by a confusion bin.In this work, we propose a technique which gen-eralizes very well on word lattices and overcomesthe limitations posed by a CN or by the limited na-ture of local neighborhood.
The size of the neigh-borhood in our approach is a variable factor whichdepends upon the confusability in any particular re-gion of the word lattice.
Thus the local neighbor-hood are in some sense a function of the confusabil-ity present in the lattice rather than some predeter-mined factor.
Below we describe the process, virtueof which, we can cut the lattice to form many selfcontained smaller sized sub lattices.
Once these sublattices are formed, we follow a similar hill climbingprocedure as proposed in our previous work (Deorasand Jelinek, 2009).11204.1 Islands of ConfusabilityWe will continue to follow the notation introducedin section 3.1.
Before we define the procedure forcutting the lattice into many small self containedlattices, we will define some more terms necessaryfor the ease of understandability of the algorithm.5For any node v ?
V , we define forward probability,?
(v), as the probability of any partial path pi ?
E?,s.t.
p[pi] = ns, n[pi] = v and it is given as:?
(v) =?pi?E?s.t.p[pi]=ns,n[pi]=vA[pi]?L[pi] (4)Similarly, for any node v ?
V , we define thebackward probability, ?
(v), as the probability of anytrailing path pi ?
E?, s.t.
p[pi] = v, n[pi] ?
Ne and itis given as:?
(v) =?pi?E?s.t.p[pi]=v,n[pi]?NeA[pi]?L[pi] (5)If we define the sum of joint likelihood under thebaseline acoustic and language models of all pathsin the lattice by Z, then it can simply be obtained as:Z =?u?Ne ?
(u) = ?
(ns)In order to cut the lattice, we want to identify setsof nodes, S1, S2, .
.
.
, S|S| such that for any set Si ?S following conditions are satisfied:1.
For any two nodes u, v ?
Si we have that:T [u] = T [v].
We will define this common timestamp of the nodes in the set by T [Si].2.
6 ?
pi ?
E such that Ts[pi] < T [Si] < Te[pi].The first property can be easily checked by firstpushing states into a linked list associated with eachtime marker (this can be done by iterating over allthe states of the graph) then iterating over the uniquetime markers and retrieving back the nodes asso-ciated with it.The second property can be checkedby first iterating over the unique time markers andfor each of the marker, iterating over the arcs andterminating the loop as soon as some arc is found5Goel and Byrne (2000) previously demonstrated the lat-tice segmentation procedure to solve the intractable problem ofMBR decoding.
The cutting procedure in our work is differentfrom theirs in the sense that we rely on time information forcollating competing phrases, while they do not.out violating property 2 for the specific time marker.Thus the time complexity for checking property 1 isO(|V|) and that for property 2 isO(|T |?|E|), where|T | is the total number of unique time markers.
Usu-ally |T |  |E| and hence the time complexity forchecking property 2 is almost linear in the numberof edges.
Thus effectively, the time complexity forcutting the lattice is O(|V|+ |E|).Having formed such sets, we can now cut thelattice at time stamps associated with these setsi.e.
that: T [S1], .
.
.
, T [S|S|].
It can be easily seenthat the number of sub lattices, C, will be equalto |S| ?
1.We will identify these sub lattices asL1,L1, .
.
.
,LC .
At this point, we have not formedself contained lattices yet by simply cutting the par-ent lattice at the cut points.Once we cut the lattice at these cut points, we im-plicitly introduce many new starting nodes and end-ing nodes for any sub lattice.
We will refer to thesenodes as exposed starting nodes and exposed end-ing nodes.
Thus for some jth sub lattice, Lj , therewill be as many new exposed starting nodes as thereare nodes in the set Sj and as many exposed endingnodes as there are nodes in the set Sj+1.
In orderto make these sub lattices consistent with the defini-tion of a word lattice (see Sec.
3.1), we unify all theexposed starting nodes and exposed ending nodes.To unify the exposed starting nodes, we introduceas many new edges as there are nodes in the set Sjsuch that they have a common starting node, ns[Lj ],(newly created) and distinct ending nodes presentin Sj .
To unify the exposed ending nodes of Lj ,we introduce as many new edges as there are nodesin the set Sj+1 such that they have distinct startingnodes present in Sj+1 and a common ending nodene[Lj ] (newly created).
From the totality of thesenew edges and nodes along with the ones alreadypresent in Lj forms an induced directed acyclic sub-graph G[Lj ] = (V[Lj ], E [Lj ], ns[Lj ], ne[Lj ]).For any path pi ?
E [Lj ] such that p[pi] = ns[Lj ]and n[pi] ?
Sj , we assign the value of ?
(n[pi])to denote the joint likelihood A[pi]?L[pi] and as-sign epsilon for word associated with these edgesi.e.
w[pi].
We assign T [Sj ] ?
?T to denote Ts[pi]and T [Sj ] to denote Te[pi].
Similarly, for any pathpi ?
E [Lj ] such that p[pi] ?
Sj+1 and n[pi] = ne[Lj ],1121we assign the value of ?
(p[pi])6 to denote the jointlikelihood A[pi]?L[pi] and assign epsilon for wordassociated with these edges i.e.
w[pi].
We assignT [Sj+1] to denote Ts[pi] and T [Sj+1] + ?T to de-note Te[pi].
This completes the process and we ob-tain self contained lattices, which if need be, can beindependently decoded and/or analyzed.4.2 Iterative Decoding on Word LatticesOnce we have formed the self contained lattices,L1,L1, .
.
.
,LC , where C is the total number of sublattices formed, then the idea is to divide the re-scoring problem into many small re-scoring prob-lems carried over the sub lattices one at a time byfixing single best paths from all the remaining sublattices.The inputs to the algorithm are the sub lattices(produced by cutting the parent lattice generated un-der some Markov n-gram LM) and a new rescor-ing LM, which now need not be restricted to fi-nite state machine family.
The output of the al-gorithm is a word string, W?, such that it is theconcatenation of final decoded word strings fromeach sub lattice.
Thus if we denote the final de-coded path (under some decoding scheme, whichwill become apparent next) in the jth sub latticeby pi?j and the concatenation symbol by ??
?, thenW?
= W [pi?1] ?W [pi?2] ?
.
.
.
?W [pi?C ] = Cj=1W [pi?j ].Algorithm 1 Iterative Decoding on word lattices.Require: {L1,L1, .
.
.
,LC}, LnewPrevHyp?
nullCurrentHyp?
Cj=1W [p?ij ]while PrevHyp 6= CurrentHyp dofor i?
1 .
.
.
C dop?ii ?
argmaxpii?E?i :p[pii]=ns[Li]n[pii]=ne[Li](Lnew[ 1?
?
.
.
.
?pii ?
.
.
.
?
k?]?A[pii]?
?kj=1j 6=iA[ j?
]?
)end forPrevHyp?
CurrentHypCurrentHyp?
Cj=1W [p?ij ]end while?j ?
{1, 2, .
.
.
, C} pi?j ?
p?ij6The values of ?(?)
and ?(?)
are computed under parent lat-tice structure.The algorithm is initialized by setting PrevHypoto null and CurrHypo to the concatenation of 1-bestoutput from each sub lattice.
During the initializa-tion step, each sub lattice is analyzed independent ofany other sub lattice and under the baseline acousticscores and baseline n-gram LM scores, 1-best pathis found out.
Thus if we define the best path underbaseline model in some jth sub-lattice by p?ij , Cur-rHypo is then initialized to: W [p?i1] ?
W [p?i2] ?
.
.
.
?W [p?iC ].
The algorithm then runs as long as Cur-rHypo is not equal to PrevHypo.
In each iteration,the algorithm sequentially re-scores each sub-latticeby keeping the surrounding context fixed.
Once allthe sub lattices are re-scored, that constitutes one it-eration.
At the end of each iteration, CurrHypo isset to the concatenation of 1 best paths from eachsub lattice while PrevHypo is set to the old valueof CurrHypo.
Thus if we are analyzing some ithsub-lattice in some iteration, then 1-best paths fromall but this sub-lattice is kept fixed and a new 1-bestpath under the re-scoring LM is found out.
It is nothard to see that the likelihood of the output underthe new re-scoring model is guaranteed to increasemonotonically after every decoding step.Since the cutting of parent lattices produce manysmall lattices with considerably lesser number ofnodes, in practice, an exhaustive search for the 1-best hypothesis can be carried out via N best list.Algorithm 1 outlines the steps for iterative decodingon word lattices.4.3 Entropy PruningIn this section, we will discuss a speed up techniquebased on entropy of the lattice.
Entropy of a latticereflects the confidence of the recognizer in recogniz-ing the acoustics.
Based on the observation that ifthe N best list / lattice generated under some modelhas a very low entropy, then the Spearman?s rankcorrelation factor, ?
(Eqn.
3), tends to be highereven when the N best lists / lattice is re-ranked witha bigger and a better model.
A low entropy underthe baseline model only reflects the confidence ofthe recognizer in recognizing the acoustic.
Table 2shows the rank correlation values between two setsof N best lists.
Both sets are produced by a bi-gram LM (bg).
The entropy of N best lists in thefirst set is 0.05 nats or less.
The N best lists in thesecond set have an entropy greater than 0.05 nats.1122Both these sets are re-ranked with bigger and bet-ter models (see Table 1 for model definitions).
Wecan see from Table 2 that the rank correlation valuestend to be higher (indicating little re-rankings) whenthe entropy of the N best list, under the baselinemodel, is lower.
Similarly, the rank-correlation val-ues tend to be lower (indicating more re-rankings)whenever the entropy of the N best list is higher.Note that these entropy values are computed with re-spect to the starting model (in this case, bigram LM).Of course, if the starting LM is much weaker thanthe rescoring model, then the entropy values neednot be reflective of the difficulty of the overall task.This observation then suggests that it is safe to re-score only those N best lists whose entropy underthe starting model is higher than some threshold.Rescoring Model ?
(H?0.05) ?
(H>0.05)bg 1.00 1.00tg 0.58 0.38n-gram+syntactic 0.54 0.31Table 2: Spearman Rank Correlation on the N best listextracted from a bi-gram language model (bg) and re-scored with relatively better language models (see Table 1for model definitions).
Entropy under the baseline modelcorrelates well with the rank correlation factor, suggest-ing that exhaustive search need not be necessary for ut-terances yielding lower entropy.While computation of entropy for N best list istractable, for a word lattice, the computation of en-tropy is intractable if one were to enumerate all thehypotheses.
Even if we were able to enumerate allhypotheses, this method tends to be slower.
Usingefficient semiring techniques introduced by Li andEisner (2009) or using posterior probabilities on theedges leading to end states, we can compute the en-tropy of a lattice in one single forward pass usingdynamic programming.
It should, however, be notedthat, for dynamic programming technique to work,only n-gram LMs can be used.
One has to resort toapproximate entropy computation via N best list, ifentropy under long span LM is desired.4.3.1 Speed Up for Iterative DecodingOur speed up technique is simple.
Once we haveformed self contained sub lattices, we want to pruneall but the top few best complete paths (obtained un-der baseline / starting model) of those sub latticeswhose entropy is below some threshold.
Thus, be-lieving in the original model?s confidence, we wantto focus only on those sub lattices which the recog-nizer found difficult to decode in the first pass.
Allother part of the parent lattice will be not be ana-lyzed.
The thresholds for pruning is very applicationand corpus specific and needs to be tuned on someheld out data.5 Experiments and ResultsWe performed recognition on the Broadcast News(BN) dev04f, rt03 and rt04 task using the state-of-the-art acoustic models trained on the EnglishBroadcast News (BN) corpus (430 hours of audio)provided to us by IBM (Chen et al, 2009).
IBM alsoprovided us its state-of-the-art speech recognizer,Attila (Soltau et al, 2010) and two Kneser-Neysmoothed backoff n-gram LMs containing 4.7M n-grams (n ?
4) and 54M n-grams (n ?
4), bothtrained on 400M word tokens.
We will refer to themas KN:BN-Small and KN:BN-Big respectively.
Werefer readers to (Chen et al, 2009) for more detailsabout the recognizer and corpora used for trainingthe models.We trained two RNN based language models -the first one, denoted further as RNN-limited, wastrained on a subset of the training data (58M tokens).It used 400 neurons in the hidden layer.
The secondmodel, denoted as RNN-all, was trained on all ofthe training data (400M tokens), but due to the com-putational complexity issues, we had to restrict itshidden layer size to 320 neurons.We followed IBM?s multi-pass decoding recipeusing KN:BN-Small in the first pass followed by ei-ther N best list re-scoring or word lattice re-scoringusing bigger and better models.7 For the purposeof re-scoring, we combined all the relevant statisti-cal models in one unified log linear framework rem-iniscent of work by Beyerlein (1998).
We, however,trained the model weights by optimizing expectedWER rather than 1-best loss as described in (De-oras et al, 2010).
Training was done on N bestlists of size 2K.
We will refer to the log linear com-7The choice of the order and size of LM to be used in thefirst pass decoding was determined by taking into considerationthe capabilities of the decoder.1123100 101 102 103 104 105 10610.81111.211.411.611.812Size of Search Space (Number of Hypotheses for evaluation)1 best WER(%)Plot of 1 best WER v/s Search Space SizeN BestIter.
Dec. (ID)ID with Ent.
PruningViterbi BaselineViterbi RescoringFigure 2: Plot of WER (y axis) on rt03+dev04f set versusthe size of the search space (x axis).
The baseline WERobtained using KN:BN-Small is 12% which then dropsto 11% when KN:BN-Big is used for re-scoring.
N bestlist search method obtains the same reduction in WERby evaluating as many as 228K sentence hypotheses onan average.
The proposed method obtains the same re-duction by evaluating 14 times smaller search space.
Thesearch effort reduces further to 40 times if entropy basedpruning is employed during re-scoring.bination of KN:BN-Big and RNN-limited by KN-RNN-lim; KN:BN-Big and RNN-all by KN-RNN-all and KN:BN-Big, RNN-limited and RNN-all byKN-RNN-lim-all.We used two sets for decoding: rt03+dev04f setwas used as a development set while rt04 was usedas a blind set for the purpose of evaluating the per-formance of long span RNN models using the pro-posed approach.
We made use of OpenFst C++ li-braries (Allauzen et al, 2007) for manipulating lat-tice graphs and generating N best lists.
Due to thepresence of hesitation tokens in reference transcriptsand the need to access the silence/pause tokens forpenalizing short sentences, we treated these tokensas regular words before extracting sentence hypothe-ses.
This, and poorly segmented nature of the testcorpora, led to huge enumeration of sentence hy-potheses.5.1 n-gram LM for re-scoringIn this setup, we used KN:BN-Small as the base-line starting LM which yielded the WER of 12%on rt03+dev04f set.
Using KN:BN-Big as the re-scoring LM, the WER dropped to 11%.
Since there-scoring LM belonged to the n-gram family, it waspossible to compute the optimal word string by re-scoring the whole lattice (see Sec.
3.1).
We nowcompare the performance of N best list approach(Sec.
3.2) with our proposed approach (Sec.
4).N best list achieved the best possible reduction byevaluating as many as 228K sentence hypotheseson an average.
As against that, our proposed ap-proach achieved the same performance by evaluat-ing 16.6K sentence hypotheses, thus reducing thesearch efforts by 13.75 times.
By carrying out en-tropy pruning (see Sec.
4.3 ) on sub lattices, our pro-posed approach required as little as 5.6K sentencehypotheses evaluations to obtain the same optimalperformance, reducing the search effort by as muchas 40.46 times.
For the purpose of this experiment,entropy based pruning was carried out when the en-tropy of the sub lattice was below 5 nats.
Table 3compares the two search methods for this setup andFig.
2 shows a plot of WER versus the size of thesearch space (in terms of number of sentence hy-potheses evaluated by an n-gram language model).On rt04, the KN:BN-Small LM gave a WER of14.1% which then dropped to 13.1% after re-scoringwith KN:BN-Big.
Since the re-scoring model wasan n-gram LM, it was possible to obtain the opti-mal performance via lattice update technique (seeSec.
3.1).
We then carried out the re-scoring of theword lattices under KN:BN-Big using our proposedtechnique and found it to give the same performanceyielding the WER of 13.1%.5.2 Long Span LM for re-scoringIn this setup, we used the strongest n-gram LMas our baseline.
We thus used KN:BN-Big as thebaseline LM which yielded the WER of 11% onrt03+dev04f.
We then used KN-RNN-lim-all for re-scoring.
Due to long span nature of the re-scoringLM, it was not possible to obtain the optimal WERperformance.
Hence we have compared the perfor-mance of our proposed method with N best list ap-proach.
N best list achieved the lowest possibleWER after evaluating as many as 33.8K sentencehypotheses on an average.
As against that, our pro-posed approach in conjunction with entropy pruningobtained the same performance by evaluating just1.6K sentence hypotheses, thus reducing the searchby a factor of 21.
Fig 3 shows a plot of WER versus1124100 101 102 103 10410.310.410.510.610.710.810.91111.1Size of Search Space (Number of Hypotheses for evaluation)1 best WER(%)Plot of 1 best WER v/s Search Space SizeN BestID with Ent.
PruningViterbi BaselineFigure 3: Plot of WER (y axis) on rt03+dev04f set versusthe size of the search space (x axis).
The baseline WERobtained using KN:BN-Big is 11% which then drops to10.4% when KN-RNN-lim-all is used for re-scoring.
Nbest list search method obtains this reduction in WER byevaluating as many as 33.8K sentence hypotheses on anaverage, while the proposed method (with entropy prun-ing) obtains the same reduction by evaluating 21 timessmaller search space.the size of the search space (in terms of number ofsentence hypotheses evaluated by a long span lan-guage model).In-spite of starting off with a very strong n-gramLM, theN best lists so extracted were still not repre-sentative enough of the long span rescoring models.Had we started off with KN:BN-Small, the N bestlist re-scoring method would have had no chance offinding the optimal hypothesis in reasonable size ofhypotheses search space.
Table 4 compares the twosearch methods for this setup when many other longspan LMs were also used for re-scoring.On rt04, the KN:BN-Big LM gave a WER of13.1% which then dropped to 12.15% after re-scoring with KN-RNN-lim-all using our proposedtechnique.8 Since the re-scoring model was not ann-gram LM, it was not possible to obtain the optimalperformance but we could enumerate huge N bestlist to approximate this value.
Our proposed methodis much faster than huge N best lists and no worsein terms of WER.
As far as we know, the result ob-tained on these sets is the best performance everreported on the Broadcast News corpus for speech8The WER obtained using KN-RNN-lim and KN-RNN-allwere 12.5% and 12.3% respectively.recognition.Models WER NBest ID SavingKN:BN-Small 12.0 - - -KN:BN-Big 11.0 228K 5.6K 40Table 3: The starting LM is a weak n-gram LM (KN:BN-Small) and the re-scoring LM is a much stronger but n-gram LM (KN:BN-Big).
The baseline WER in this caseis 12% and the optimal performance by the re-scoring LMis 11.0%.
The proposed method outperforms N best listapproach, in terms of search efforts, obtaining optimalWER.Models WER NBest ID SavingKN:BN-Big 11.0 - - -KN-RNN-lim 10.5 42K 1.1K 38KN-RNN-all 10.5 26K 1.3K 20KN-RNN-lim-all 10.4 34K 1.6K 21Table 4: The starting LM is a strong n-gram LM(KN:BN-Big) and the re-scoring model is a long spanLM (KN-RNN-*).
The baseline WER is 11.0%.
Dueto long span nature of the LM, optimal WER could notbe estimated.
The proposed method outperfoms N bestlist approach on every re-scoring task.6 ConclusionWe proposed and demonstrated a new re-scoringtechnique for general word graph structures such asword lattices.
We showed its efficacy by demonstrat-ing huge reductions in the search effort to obtain anew state-of-the-art performance on a very compet-itive speech task of Broadcast news.
As part of thefuture work, we plan to extend this technique for hy-pergraphs and lattices in re-scoring MT outputs withcomplex and long span language models.AcknowledgementThis work was partly funded by Human LanguageTechnology, Center of Excellence and by Tech-nology Agency of the Czech Republic grant No.TA01011328, and Grant Agency of Czech Repub-lic project No.
102/08/0707.
We would also like toacknowledge the contribution of Frederick Jelinektowards this work.
He would be a co-author if hewere available and willing to give his consent.1125ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst:A General and Efficient Weighted Finite-State Trans-ducer Library.
In Proceedings of the Ninth Interna-tional Conference on Implementation and Applicationof Automata, (CIAA 2007), volume 4783 of LectureNotes in Computer Science, pages 11?23.
Springer.J.
R. Bellegarda.
2000.
Exploiting latent semantic infor-mation in statistical language modeling.
Proceedingsof IEEE, 88(8):1279?1296.Yoshua Bengio, Re?jean Ducharme, and Pascal Vincent.2001.
A Neural Probabilistic Language Model.
InProceedings of Advances in Neural Information Pro-cessing Systems.Peter Beyerlein.
1998.
Discriminative Model Combina-tion.
In Proc.
of IEEE International Conference onAcoustics, Speech, and Signal Processing (ICASSP).Christopher M. Bishop.
2006.
Pattern Recognition andMachine Learning.
Springer.Ciprian Chelba and Frederick Jelinek.
2000.
Struc-tured Language Modeling.
Computer Speech and Lan-guage, 14(4):283?332.S.
F. Chen, L. Mangu, B. Ramabhadran, R. Sarikaya,and A. Sethy.
2009.
Scaling shrinkage-based lan-guage models.
In Proc.
of IEEE Workshop on Auto-matic Speech Recognition and Understanding (ASRU),pages 299?304.Noam Chomsky.
1957.
Syntactic Structures.
TheHague: Mouton.Yen-Lu Chow and Richard Schwartz.
1989.
The N-Bestalgorithm: an efficient procedure for finding top N sen-tence hypotheses.
In Proceedings of the workshop onSpeech and Natural Language, HLT ?89, pages 199?202, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Kenneth Church.
2012.
A Pendulum Swung Too Far.Linguistic Issues in Language Technology - LiLT.
toappear.T.M.
Cover and J.A.Thomas.
1991.
Elements of Infor-mation Theory.
John Wiley and Sons, Inc. N.Y.Anoop Deoras and Frederick Jelinek.
2009.
Iterative De-coding: A Novel Re-Scoring Framework for Confu-sion Networks.
In Proc.
of IEEE Workshop on Auto-matic Speech Recognition and Understanding (ASRU),pages 282 ?286.Anoop Deoras, Denis Filimonov, Mary Harper, and FredJelinek.
2010.
Model Combination for Speech Recog-nition using Empirical Bayes Risk Minimization.
InProc.
of IEEE Workshop on Spoken Language Tech-nology (SLT).Anoop Deoras, Toma?s?
Mikolov, Stefan Kombrink, Mar-tin Karafia?t, and Sanjeev Khudanpur.
2011.
Varia-tional Approximation of Long-Span Language Mod-els for LVCSR.
In Proc.
of IEEE International Con-ference on Acoustics, Speech, and Signal Processing(ICASSP).Jeffery Elman.
1990.
Finding Structure in Time.
In Cog-nitive Science, volume 14, pages 179?211.Denis Filimonov and Mary Harper.
2009.
A Joint Lan-guage Model with Fine-grain Syntactic Tags.
In Proc.of 2009 Conference on Empirical Methods in NaturalLanguage Processing.V.
Goel and W. Byrne.
2000.
Minimum Bayes Risk Au-tomatic Speech Recognition.
Computer, Speech andLanguage.Rukmini Iyer and Mari Ostendorf.
1999.
Modeling LongDistance Dependence in Language: Topic MixturesVersus Dynamic Cache Models.
IEEE Transactionson Speech and Audio Processing, 7(1):30?39.Zhifei Li and Jason Eisner.
2009.
First- and second-orderexpectation semirings with applications to minimum-risk training on translation forests.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 40?51, Singapore,August.Lidia Luminita Mangu.
2000.
Finding consensus inspeech recognition.
Ph.D. thesis, The Johns HopkinsUniversity.
Adviser-Brill, Eric.Toma?s?
Mikolov, Martin Karafia?t, Luka?s?
Burget,Jan ?Honza?
C?ernocky?, and Sanjeev Khudanpur.2010.
Recurrent Neural Network Based LanguageModel.
In Proc.
of the ICSLP-Interspeech.Toma?s?
Mikolov, Anoop Deoras, Stefan Kombrink, Luka?s?Burget, and Jan ?Honza?
C?ernocky?.
2011a.
EmpiricalEvaluation and Combination of Advanced LanguageModeling Techniques.
In Proc.
of Interspeech.Toma?s?
Mikolov, Stefan Kombrink, Luka?s?
Burget,Jan ?Honza?
C?ernocky?, and Sanjeev Khudanpur.2011b.
Extensions of Recurrent Neural Network Lan-guage Model.
In Proc.
of IEEE International Con-ference on Acoustics, Speech, and Signal Processing(ICASSP).Mehryar Mohri and Michael Riley.
2002.
An EfficientAlgorithm for the N-Best-Strings Problem.
In Pro-ceedings of the International Conference on SpokenLanguage Processing (ICSLP).M.
Mohri, F.C.N.
Pereira, and M. Riley.
2000.
The de-sign principles of a weighted finite-state transducer li-brary.
Theoretical Computer Science, 231:17-32.A.
Ogawa, K. Takeda, and F. Itakura.
1998.
Balanc-ing Acoustic and Linguistic Probabilities.
In Proc.
ofIEEE International Conference on Acoustics, Speech,and Signal Processing (ICASSP).1126F.
Richardson, M. Ostendorf, and J.R. Rohlicek.
1995.Lattice-based search strategies for large vocabularyspeech recognition.
In Proc.
of IEEE InternationalConference on Acoustics, Speech, and Signal Process-ing (ICASSP).Brian Roark.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.
2001.Whole-Sentence Exponential Language Models: a Ve-hicle for Linguistic-Statistical Integration.
ComputerSpeech and Language, 15(1).Roni Rosenfeld.
1997.
A Whole Sentence MaximumEntropy Language Model.
In Proc.
of IEEE workshopon Automatic Speech Recognition and Understanding(ASRU), Santa Barbara, California, December.D.E.
Rumelhart, G. E. Hinton, and R.J. Williams.
1986.Learning representations by back-propagating errors.Nature, 323:533?536.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21(3):492?518.C.
E. Shannon.
1948.
A Mathematical Theory ofCommunication.
The Bell System Technical Journal,27:379?423, 623?656.H.
Soltau, G. Saon, and B. Kingsbury.
2010.
The IBMAttila speech recognition toolkit.
In Proc.
of IEEEWorkshop on Spoken Language Technology (SLT).Wen Wang and Mary Harper.
2002.
The SuperARV lan-guage model: investigating the effectiveness of tightlyintegrating multiple knowledge sources.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP).1127
