Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 119?122,Dublin, Ireland, August 23-24, 2014.Bielefeld SC: Orthonormal Topic Modelling for Grammar InductionJohn P. McCraeCITEC, Bielefeld UniversityInspiration 1Bielefeld, Germanyjmccrae@cit-ec.uni-bielefeld.dePhilipp CimianoCITEC, Bielefeld UniversityInspiration 1Bielefeld, Germanycimiano@cit-ec.uni-bielefeld.deAbstractIn this paper, we consider the applicationof topic modelling to the task of induct-ing grammar rules.
In particular, we lookat the use of a recently developed methodcalled orthonormal explicit topic analysis,which combines explicit and latent modelsof semantics.
Although, it remains unclearhow topic model may be applied to thecase of grammar induction, we show thatit is not impossible and that this may allowthe capture of subtle semantic distinctionsthat are not captured by other methods.1 IntroductionGrammar induction is the task of inducing high-level rules for application of grammars in spokendialogue systems.
In practice, we can extract rel-evant rules and the task of grammar induction re-duces to finding similar rules between two strings.As these strings are not necessarily similar in sur-face form, what we really wish to calculate isthe semantic similarity between these strings.
Assuch, we could think of applying a semantic anal-ysis method.
As such we attempt to apply topicmodelling, that is methods such as Latent Dirich-let Allocation (Blei et al., 2003), Latent Seman-tic Analysis (Deerwester et al., 1990) or ExplicitSemantic Analysis (Gabrilovich and Markovitch,2007).
In particular we build on the recent workto unify latent and explicit methods by means oforthonormal explicit topics.In topic modelling the key choice is the docu-ment space that will act as the corpus and hencetopic space.
The standard choice is to regard allarticles from a background document collection?
Wikipedia articles are a typical choice ?
as theThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/topic space.
However, it is crucial to ensure thatthese topics cover the semantic space evenly andcompletely.
Following McCrae et al.
(McCrae etal., 2013) we remap the semantic space defined bythe topics in such a manner that it is orthonormal.In this way, each document is mapped to a topicthat is distinct from all other topics.The structure of the paper is as follows: we de-scribe our method in three parts, first the methodin section 2, followed by approximation method insection 3, the normalization methods in section 4and finally the application to grammar inductionin section 5, we finish with some conclusions insection 6.2 Orthonormal explicit topic analysisONETA (McCrae et al., 2013, Orthonormal ex-plicit topic analysis) follows Explicit SemanticAnalysis in the sense that it assumes the avail-ability of a background document collection B ={b1, b2, ..., bN} consisting of textual representa-tions.
The mapping into the explicit topic spaceis defined by a language-specific function ?
thatmaps documents into RNsuch that the jthvalue inthe vector is given by some association measure?j(d) for each background document bj.
Typicalchoices for this association measure ?
are the sumof the TF-IDF scores or an information retrievalrelevance scoring function such as BM-25 (Sorgand Cimiano, 2010).For the case of TF-IDF, the value of the j-thelement of the topic vector is given by:?j(d) =????tf-idf(bj)T???
?tf-idf(d)Thus, the mapping function can be representedas the product of a TF-IDF vector of document dmultiplied by a word-by-document (W ?N ) TF-IDF matrix, which we denote as a X:11Tdenotes the matrix transpose as usual119?
(d) =???????tf-idf(b1)T...????tf-idf(bN)T??????
?tf-idf(d) = XT????
?tf-idf(d)For simplicity, we shall assume from this pointon that all vectors are already converted to a TF-IDF or similar numeric vector form.In order to compute the similarity between twodocuments diand dj, typically the cosine-function(or the normalized dot product) between the vec-tors ?
(di) and ?
(dj) is computed as follows:sim(di, dj) = cos(?(di),?
(dj)) =?(di)T?(dj)||?(di)||||?
(dj)||sim(di, dj) = cos(XTdi,XTdj) =dTiXXTdj||XTdi||||XTdj||The key challenge with topic modelling ischoosing a good background document collectionB = {b1, ..., bN}.
A simple minimal criterionfor a good background document collection is thateach document in this collection should be maxi-mally similar to itself and less similar to any otherdocument:?i 6= j 1 = sim(bj, bj) > sim(bi, bj) ?
0As shown in McCrae et al.
(2013), this propertyis satisfied by the following projection:?ONETA(d) = (XTX)?1XTdAnd hence the similarity between two docu-ments can be calculated as:sim(di, dj) = cos(?ONETA(di),?ONETA(dj))3 ApproximationsONETA relies on the computation of a matrix in-verse, which has a complexity that, using currentpractical algorithms, is approximately cubic andas such the time spent calculating the inverse cangrow very quickly.We notice that X is typically very sparse andmoreover some rows ofX have significantly fewernon-zeroes than others (these rows are for termswith low frequency).
Thus, if we take the first N1columns (documents) in X, it is possible to re-arrange the rows of X with the result that thereis some W1such that rows with index greaterthan W1have only zeroes in the columns up toN1.
In other words, we take a subset of N1doc-uments and enumerate the words in such a waythat the terms occurring in the first N1documentsare enumerated 1, .
.
.
,W1.
Let N2= N ?
N1,W2= W ?W1.
The result of this row permuta-tion does not affect the value of XTX and we canwrite the matrix X as:X =(A B0 C)where A is a W1?
N1matrix representingterm frequencies in the first N1documents, B is aW1?N2matrix containing term frequencies in theremaining documents for terms that are also foundin the first N1documents, and C is a W2?
N2containing the frequency of all terms not found inthe first N1documents.Application of the well-known divide-and-conquer formula (Bernstein, 2005, p. 159) for ma-trix inversion yields the following easily verifiablematrix identity, given that we can find C?such thatC?C = I.((ATA)?1AT?
(ATA)?1ATBC?0 C?
)(A B0 C)= I(1)The inverse C?is approximated by the JacobiPreconditioner, J, of CTC:C?'
JCT(2)=??
?||c1||?20...0 ||cN2||?2??
?CT4 NormalizationA key factor in the effectiveness of topic-basedmethods is the appropriate normalization of the el-ements of the document matrix X.
This is evenmore relevant for orthonormal topics as the matrixinversion procedure can be very sensitive to smallchanges in the matrix.
In this context, we con-sider two forms of normalization, term and docu-ment normalization, which can also be consideredas row/column normalizations of X.A straightforward approach to normalization isto normalize each column of X to obtain a matrixas follows:120X?=(x1||x1||.
.
.xN||xN||)If we calculate X?TX?= Y then we get that the(i, j)-th element of Y is:yij=xTixj||xi||||xj||Thus, the diagonal of Y consists of ones only anddue to the Cauchy-Schwarz inequality we havethat |yij| ?
1, with the result that the matrix Yis already close to I.
Formally, we can use thisto state a bound on ||X?TX??
I||F, but in prac-tice it means that the orthonormalizing matrix hasmore small or zero values.
Previous experimentshave indicated that in general term normalizationsuch as TF-IDF is not as effective as using the di-rect term frequency in ONETA, so we do not applyterm normalization.5 Application to grammar inductionThe application to grammar induction is simplycarried out by taking the rules and creating a sin-gle ground instance.
That is if we have a rule ofthe formLEAVING FROM <CITY>We would replace the instance of <CITY> witha known terminal for this rule, e.g.,leaving from BerlinThis reduces the task to that of string simi-larity which can be processed by means of anystring similarity function, for example such as theONETA function described above.
As such theprocedure is as follows:1.
Ground the input grammar rule to an Englishstring d2.
Ground each candidate matching rule to anEnglish string di3.
Calculate for each di, the similaritysimONETA(d, di)4.
Add the rule to the grammar class with thehighest similarityThis approach has the obvious drawback that itremoves all information about the valence of therule, however the effect of this loss of informationremains unclear.For application, we used 20,000 Wikipedia ar-ticles, filtered to contain only those of over 100words, giving us a corpus of 15.6 million tokens.We applied ONETA using document normaliza-tion but no term normalization and the valueN1=5000.
These parameters were chosen based on thebest results in previous experiments.6 ConclusionsThe results show that such a naive approach isnot directly applicable to the case of grammar in-duction, however we believe that it is possiblethat the subtle semantic similarities captured bytopic modelling may yet prove useful for gram-mar induction.
However it is clear from the pre-sented results that the use of a topic model alonedoes not suffice to solve this task.
We notice thatfrom the data many of the distinctions rely onantonyms and stop words, especially distinctionssuch as ?to?/?from?, which are not captured by atopic model as topic models generally ignore stopwords, and generally consider antonyms to be inthe same topic, as they frequently occur togetherin text.
The question of when semantic similaritysuch as provided by topic modelling is applicableremains an open question.ReferencesDennis S Bernstein.
2005.
Matrix mathematics, 2ndEdition.
Princeton University Press Princeton.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.Scott C. Deerwester, Susan T Dumais, Thomas K. Lan-dauer, George W. Furnas, and Richard A. Harshman.1990.
Indexing by latent semantic analysis.
JASIS,41(6):391?407.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using Wikipedia-based explicit semantic analysis.
In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence, volume 6, page 12.John P. McCrae, Philipp Cimiano, and Roman Klinger.2013.
Orthonormal explicit topic analysis for cross-lingual document matching.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 1732?1740.121Philipp Sorg and Philipp Cimiano.
2010.
An experi-mental comparison of explicit semantic analysis im-plementations for cross-language retrieval.
In Natu-ral Language Processing and Information Systems,pages 36?48.
Springer.122
