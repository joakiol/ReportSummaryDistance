In: Proceedings of CoNLL-2000 and LLL-2000, pages 99-102, Lisbon, Portugal, 2000.Combining Text and Heuristics forCost-Sensitive Spam FilteringJ os4  M.  G6mez  H ida lgo  Manue l  Maf ia  L6pez*Universidad Europea-CEES,  Spain Universidad e Vigo, Spainjmgomez@dinar, es?.
uem.
es mj lopez@uvigo,  esEnrique Puertas SanzUniversidad Europea-CEES,  Spainepsilonmail@retemail, esAbst rac tSpam filtering is a text categorization task thatshows especial features that make it interest-ing and difficult.
First, the task has been per-formed traditionally using heuristics from thedomain.
Second, a cost model is required toavoid misclassification of legitimate messages.We present a comparative evaluation of severalmachine learning algorithms applied to spam fil-tering, considering the text of the messages anda set of heuristics for the task.
Cost-orientedbiasing and evaluation is performed.1 IntroductionSpam, or more properly Unsolicited Commer-cial E-mail (UCE), is an increasing threat tothe viability of Internet E-mail and a dangerto Internet commerce.
UCE senders take awayresources from users and service suppliers with-out compensation a d without authorization.
Avariety of counter-measures to UCE have beenproposed, from technical to regulatory (Cranorand LaMacchia, 1998).
Among the technicalones, the use of filtering methods is popular andeffective.UCE filtering is a text categorization task.Text categorization (TC) is the classification ofdocuments with respect o a set of one or morepre-existing categories.
In the case of UCE, thetask is to classify e-mail messages or newsgroupsarticles as UCE or not (that is, legitimate).
Thegeneral model of TC makes use of a set of pre-classified documents to classify new ones, ac-cording to the text content (i.e.
words) of thedocuments (Sebastiani, 1999).Although UCE filtering seems to be a simpleinstance of the more general TC task, it shows* Partially supported by the CICYT, project no.TEL99-0335-C04-03two special characteristics:* First, UCE filtering has been developed us-ing very simple heuristics for many years.For example, one individual could manu-ally build a filter that classifies as "spam"messages containing the phrase "win bigmoney", or with an unusual (big) numberof capital etters or non-alphanumeric char-acters.
These rules are examples of simplebut powerful heuristics that could be usedto complement a word-based automatic TCsystem for UCE filtering.?
Second, all UCE filtering errors are not ofequal importance.
Individuals usually pre-fer conservative filters that tend to classifyUCE as legitimate, because missing a le-gitimate message is more harmful than theopposite.
A cost model is imperative toavoid the risk of missing legitimate -mail.Many learning algorithms have been appliedto the problem of TC (Yang, 1999), but muchless with the problem of UCE filtering in mind.Sahami and others (1998) propose the utiliza-tion of a Naive Bayes classifier based on thewords and a set of manually derived heuris-tics for UCE filtering, showing that the heuris-tics improve the effectiveness of the classifier.Druker and others (1999) compare boosting,Support Vector Machines, Ripper and Rocchioclassifiers for UCE filtering.
Andruotsopoulosand others (2000) present a cost-oriented eval-uation of the Naive Bayes and k-nearest neigh-bor (kNN) algorithms for UCE filtering.
Fi-nally, Provost (1999) compares Naive Bayes andRIPPER for the task.
These three last worksdo not consider any set of heuristics for UCEfiltering.
So, an extensive valuation of learn-ing algorithms combining words and heuristics99remains to be done.
Also, although the  eval-uations performed in these works have takeninto account he importance of misclassifying le-gitimate e-mail, they have not considered thatmany learning algorithms (specially those thatare error-driven) can be biased to prefer somekind of errors to others.In this paper, we present a comparative eval-uation of a representative s lection of MachineLearning algorithms for UCE filtering.
The al-gorithms take advantage of two kinds of infor-mation: the words in the messages and a set ofheuristics.
Also, the algorithms are biased bya cost weighting schema to avoid, if possible,misclassifying legitimate -mail.
Finally, algo-rithms are evaluated according to cost-sensitivemeasures.2 Heur i s t i cs  for  UCE c lass i f i ca t ionSahami and others (Sahami et al, 1998) haveproposed a set of heuristic features to comple-ment the word Bayesian model in their work,including: a set of around 35 hand-crafted keyphrases (like "free money"); some non textfeatures (like the domain of the sender, orwhether the message comes from a distributionlist or not); and features concerning the non-alphanumeric characters in the messages.For this work, we have focused in this lastset of features.
The test collection used inour experiments, Spambase, already containeda set of nine heuristic features.
Spambase 1 isan e-mail messages collection containing 4601messages, being 1813 (39%) marked as UCE.The collection comes in preprocessed (not raw)form, and its instances have been representedas 58-dimensional vectors.
The first 48 featuresare words extracted from the original messages,without stop list nor stemming, and selected asthe most unbalanced words for the UCE class.The next 6 features are the percentage ofoccur-rences of the special characters ";', "(", "\[", "!
","$" and "#".
The following 3 features representdifferent measures of occurrences of capital et-ters in the text of the messages.
Finally, the lastfeature is the class label.
So, features 49 to 57represent heuristic attributes of the messages.In our experiments, we have tested severallearning algorithms on three feature sets: only1 This collection can be obtained fromhttp://www.ics.uci.edu/mlea~n/MLRepository.html.words, only heuristic attributes, and both.
Wehave divided the Spambase collection in twoparts: a 90% of the instances axe used for train-ing, and a 10% of the messages are retained fortesting.
This split has been performed preserv-ing the percentages of legitimate and UCE mes-sages in the whole collection.3 Cost -sens i t i ve  UCE c lass i f i ca t ionAccording to the problem of UCE filtering, acost-sensitive classification is required.
Eachlearning algorithm can be biased to prefer somekind of missclassification errors to others.
Apopular technique for doing this is resamplingthe training collection by multiplying the num-ber of instances of the preferred class by the costratio.
Also, the unpreferred class can be down-sampled by eliminating some instances.
Thesoftware package we use for our experiments ap-plies both methods depending on the algorithmtested.We have tested four learning algorithms:Naive Bayes (NB), C4.5, PART and k-nearestneighbor (kNN), all implemented in the Wekapackage (Witten and Frank, 1999).
The ver-sion of Weka used in this work is Weka 3.0.1.The algorithms used can be biased to prefer themistake of classify a UCE message as not UCEto the opposite, assigning a penalty to the sec-ond kind of errors.
Following (Androutsopou-los et al, 2000), we have assigned 9 and 999(9 and 999 times more important) penalties tothe missclassification f legitimate messages asUCE.
This means that every instance of a le-gitimate message has been replaced by 9 and999 instances of the same message respectivelyfor NB, C4.5 and PART.
However, for kNN thedata have been downsampled.4 Eva luat ion  and  resu l t sThe experiments results are summarized in theTable 1, 2 and 3.
The learning algorithms NaiveBayes (NB), 5-Nearest Neighbor (5NN), C4.5and PART were tested on words (-W), heuris-tic features (-H), and both (-WH).
The kNNalgorithm was tested with values of k equal to1, 2, 5 and 8, being 5 the optimal number ofneighbors.
We present he weighted accuracy(wacc), and also the recall (rec) and precision(pre) for the class UCE.
Weighted accuracy is ameasure that weights higher the hits and misses100for the preferred class.
Recall and precision forthe UCE class show how effective the filter isblocking UCE, and what is its effectiveness let-ting legitimate messages pass the filter, respec-tively (Androutsopoulos et al, 2000).In Table 1, no costs were used.
Tables 2 and3 show the results of our experiments for costratios of 9 and 999.
For these last cases, therewere not enough training instances for the kNNalgorithm to perform classification, due to thedownsampling method applied by Weka.5 D iscuss ion  and  conc lus ionsThe results of our experiments show that thebest performing algorithms are C4.5 and PART.However, for the cost value of 999, both algo-rithms degrade to the trivial rejector: they pre-fer to classify every message as legitimate in or-der to avoid highly penalized errors.
With theseresults, neither of these algorithms eems usefulfor autonomous classification of UCE as statedby Androutsopoulos, ince this cost ratio rep-resents a scenario in which UCE messages aredeleted without notifying the user of the UCEfilter.
Nevertheless, PART-WH shows competi-tive performance for a cost ratio of 9.
Its num-bers are comparable to those shown in a com-mercial study by the top performing Brightmailfiltering system (Mariano, 2000), which reachesa UCE recall of 0.73, and a precision close to1.0, and it is manually updated.Naive Bayes has not shown high variabilitywith respect o costs.
This is probably due tothe sampling method, which only slightly affectsto the estimation of probabilities (done by ap-proximation to a normal distribution).
In (Sa-hami et al, 1998; Androutsopoulos et al, 2000),the method followed is the variation of the prob-ability threshold, which leads to a high variationof results.
In future experiments, we plan to ap-ply the uniform method MetaCost (Domingos,1999) to the algorithms tested in this work, forgetting more comparable results.With respect o the use of heuristics, we cansee that this information alone is not competi-tive, but it can improve classification based onwords.
The improvement shown in our experi-ments is modest, due to the heuristics used.
Weare not able to add other heuristics in this casebecause the Spambase collection comes in a pre-processed fashion.
For future experiments, wewill use the collection from (Androutsopoulos etal., 2000), which is in raw form.
This fact willenable us to search for more powerful heuristics.Re ferencesI.
Androutsopoulos, G. Paliouras, V. Karkalet-sis, G. Sakkis, C.D.
Spyropoulos, andP.
Stamatopoulos.
2000.
Learning to fil-ter spam e-mail: A comparison of a naivebayesian and a memory-based approach.Technical Report DEMO 2000/5, Inst.
ofInformatics and Telecommunications, NCSRDemokritos, Athens, Greece.Lorrie F. Cranor and Brian A. LaMacchia.1998.
Spam!
Comm.
of the ACM, 41(8).Pedro Domingos.
1999.
Metacost: A generalmethod for making classifiers cost-sensitive.In Proc.
of the 5th International Conferenceon Knowledge Discovery and Data Mining.Harris Drucker, Donghui Wu, and Vladimir N.Vapnik.
1999.
Support vector machines forspam categorization.
IEEE Trans.
on NeuralNetworks, 10(5).Gwendolin Mariano.
2000.
Studyfinds filters catch only a fraction ofspam.
CNET News.com.
Available atht tp://news.cnet.com / news / 0-1005- 200-2086887.html.Jefferson Provost.
1999.
Naive-bayesvs.
rule-learning in classification ofemail.
Technical Report available athttp: / /www.cs.utexas.edu/users/jp / esearch / ,Dept.
of Computer Sciences at the U. ofTexas at Austin.Mehran Sahami, Susan Dumais, David Heck-erman, and Eric Horvitz.
1998.
A bayesianapproach to filtering junk e-mail.
In Learn-ing for Text Categorization: Papers from the1998 Workshop.
AAAI Tech.
Rep. WS-98-05.Fabrizio Sebastiani.
1999.
A tutorial on auto-mated text categorisation.
In Proc.
of theFirst Argentinian Symposium on ArtificialIntelligence (ASAI-99).Ian H. Witten and Eibe Frank.
1999.
DataMining: Practical Machine Learning Toolsand Techniques with Java Implementations.Morgan Kaufmann.Yiming Yang.
1999.
An evaluation of statisti-cal approaches to text categorization.
Infor-mation Retrieval, 1(1-2).101classif ierNB-WNB-HNB-WH5NN-W5NN-H5NN-WHrec pre wacc classifier rec pre0.97 0.74 0.85 C4.5-W 0.78 0.870.31 0.80 0.69 C4.5-H 0.81 0.900.97 0.73 0.84 C4.5-WH 0.85 0.890.79 0.85 0.86 Part-W ~ 0.81 0.870.72 0.83 0.83 Part-H 0.73 0.860.75 0.87 0.85 Part -WH 0.89 0.91Table 1: UCE recall, UCE precision and weighted accuracy forwacc0.860.880.890.870.840.92costs = 1.classif ierNB-WNB-HNB-WH5NN-W5NN-H5NN-WHrec pre wacc0.97 0.74 0.780.23 0.76 0.900.97 0.74 0.78classifier rec pre waccC4.5-W 0.55 0.96 0.95C4.5-H 0.41 0.96 0.95C4.5-WH 0.71 0.96 0.96Paxt-W 0.59 0.98 0.96Part-H 0.23 0.93 0.93Part -WH 0.71 0.98 0.97Table 2: UCE recall, UCE precision and weighted accuracy for costs = 9.Tableclassif ier rec pre wacc classif ier rec pre waccNB-W 0.18 0.79 0.96 C4.5-W 0.00 0.00 0.99NB-H 0.23 0.76 0.90 C4.5-H 0.00 0.00 0.99NB-WH 0.97 0.74 0.78 C4.5-WH 0.00 0.00 0.995NN-W - - - Part-W 0.00 0.00 0.995NN-H Part-H 0.00 0.00 0.995NN-WH Part -WH 0.00 0.00 0.993: UCE recall, UCE precision and weighted accuracy for costs = 999.102
