Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 919?924,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsQuestion Relevance in VQA:Identifying Non-Visual And False-Premise QuestionsArijit Ray1, Gordon Christie1, Mohit Bansal2, Dhruv Batra3,1, Devi Parikh3,11Virginia Tech 2UNC Chapel Hill 3Georgia Institute of Technology{ray93,gordonac,dbatra,parikh}@vt.edumbansal@cs.unc.eduAbstractVisual Question Answering (VQA) is thetask of answering natural-language questionsabout images.
We introduce the novel prob-lem of determining the relevance of questionsto images in VQA.
Current VQA models donot reason about whether a question is evenrelated to the given image (e.g., What is thecapital of Argentina?)
or if it requires infor-mation from external resources to answer cor-rectly.
This can break the continuity of a dia-logue in human-machine interaction.
Our ap-proaches for determining relevance are com-posed of two stages.
Given an image and aquestion, (1) we first determine whether thequestion is visual or not, (2) if visual, we de-termine whether the question is relevant to thegiven image or not.
Our approaches, based onLSTM-RNNs, VQA model uncertainty, andcaption-question similarity, are able to outper-form strong baselines on both relevance tasks.We also present human studies showing thatVQA models augmented with such questionrelevance reasoning are perceived as more in-telligent, reasonable, and human-like.1 IntroductionVisual Question Answering (VQA) is the task ofpredicting a suitable answer given an image and aquestion about it.
VQA models (e.g., (Antol et al,2015; Ren et al, 2015)) are typically discriminativemodels that take in image and question representa-tions and output one of a set of possible answers.Our work is motivated by the following key ob-servation ?
all current VQA systems always outputan answer regardless of whether the input questionmakes any sense for the given image or not.
Fig.
1Non-Visual Visual True-PremiseWho is the presidentof the USA?What is the girlwearing?What is the catwearing?Visual False-PremiseFigure 1: Example irrelevant (non-visual, false-premise)and relevant (visual true-premise) questions in VQA.shows examples of relevant and irrelevant questions.When VQA systems are fed irrelevant questions asinput, they understandably produce nonsensical an-swers (Q: ?What is the capital of Argentina??
A:?fire hydrant?).
Humans, on the other hand, areunlikely to provide such nonsensical answers andwill instead answer that this is irrelevant or use an-other knowledge source to answer correctly, whenpossible.
We argue that this implicit assumption byall VQA systems ?
that an input question is alwaysrelevant for the input image ?
is simply untenableas VQA systems move beyond standard academicdatasets to interacting with real users, who may beunfamiliar, or malicious.
The goal of this work is tomake VQA systems more human-like by providingthem the capability to identify relevant questions.While existing work has reasoned about cross-modal similarity, being able to identify whether aquestion is relevant to a given image is a novel prob-lem with real-world applications.
In human-robotinteraction, being able to identify questions that aredissociated from the perception data available is im-portant.
The robot must decide whether to processthe scene it perceives or query external world knowl-edge resources to provide a response.919As shown in Fig.
1, we study three types ofquestion-image pairs: Non-Visual.
These questionsare not questions about images at all ?
they do notrequire information from any image to be answered(e.g., ?What is the capital of Argentina??).
VisualFalse-Premise.
While visual, these questions do notapply to the given image.
For instance, the ques-tion ?What is the girl wearing??
makes sense onlyfor images that contain a girl in them.
Visual True-Premise.
These questions are relevant to (i.e., havea premise which is true) the image at hand.We introduce datasets and train models to rec-ognize both non-visual and false-premise question-image (QI) pairs in the context of VQA.
First, weidentify whether a question is visual or non-visual;if visual, we identify whether the question has a true-premise for the given image.
For visual vs. non-visual question detection, we use a Long Short-TermMemory (LSTM) recurrent neural network (RNN)trained on part of speech (POS) tags to capturevisual-specific linguistic structure.
For true vs. false-premise question detection, we present one set of ap-proaches that use the uncertainty of a VQA model,and another set that use pre-trained captioning mod-els to generate relevant captions (or questions) forthe given image and then compare them to the givenquestion to determine relevance.Our proposed models achieve accuracies of 92%for detecting non-visual, and 74% for detectingfalse-premise questions, which significantly outper-form strong baselines.
We also show through humanstudies that a VQA system that reasons about ques-tion relevance is picked significantly more often asbeing more intelligent, human-like and reasonablethan a baseline VQA system which does not.
Ourcode and datasets are publicly available on the au-thors?
webpages.2 Related WorkThere is a large body of existing work that reasonsabout cross-modal similarity: how well an imagematches a query tag (Liu et al, 2009) in text-basedimage retrieval, how well an image matches a cap-tion (Feng and Lapata, 2013; Xu et al, 2015; Or-donez et al, 2011; Karpathy and Fei-Fei, 2015; Fanget al, 2015), and how well a video matches a de-scription (Donahue et al, 2015; Lin et al, 2014a).In our work, if a question is deemed irrelevant,the VQA model says so, as opposed to answeringthe question anyway.
This is related to perceptionsystems that do not respond to an input where thesystem is likely to fail.
Such failure prediction sys-tems have been explored in vision (Zhang et al,2014; Devarakota et al, 2007) and speech (Zhaoet al, 2012; Sarma and Palmer, 2004; Choularton,2009; Voll et al, 2008).
Others attempt to providethe most meaningful answer instead of suppressingthe output of a model that is expected to fail for agiven input.
One idea is to avoid a highly specificprediction if there is a chance of being wrong, andinstead make a more generic prediction that is morelikely to be right (Deng et al, 2012).
Malinowskiand Fritz (2014) use semantic segmentations in theirapproach to question answering, where they reasonthat objects not present in the segmentations shouldnot be part of the answer.To the best of our knowledge, our work is the firstto study the relevance of questions in VQA.
Chen etal.
(2012) classify users?
intention of questions forcommunity question answering services.
Most re-lated to our work is Dodge et al (2012).
They extractvisual text from within Flickr photo captions to beused as supervisory signals for training image cap-tioning systems.
Our motivation is to endow VQAsystems the ability to detect non-visual questions torespond in a human-like fashion.
Moreover, we alsodetect a more fine-grained notion of question rele-vance via true- and false-premise.3 DatasetsFor the task of detecting visual vs. non-visualquestions, we assume all questions in the VQAdataset (Antol et al, 2015) are visual, since theAmazon Mechanical Turk (AMT) workers werespecifically instructed to ask questions about a dis-played image while creating it.
We also col-lected non-visual philosophical and general knowl-edge questions from the internet (see supplementarymaterial).
Combining the two, we have 121,512 vi-sual questions from the validation set of VQA and9,9521 generic non-visual questions collected fromthe internet.
We call this dataset Visual vs. Non-1High accuracies on this task in our experiments indicatethat this suffices to learn the corresponding linguistic structure.920Visual Questions (VNQ).We also collect a dataset of true- vs. false-premisequestions by showing AMT workers images pairedwith random questions from the VQA dataset andasking them to annotate whether they are applicableor not.
We had three workers annotate each QI pair.We take the majority vote as the final ground truthlabel.2 We have 10,793 QI pairs on 1,500 uniqueimages out of which 79% are non-applicable (false-premise).
We refer to this visual true- vs. false-premise questions dataset as VTFQ.Since there is a class imbalance in both of thesedatasets, we report the average per-class (i.e., nor-malized) accuracy for all approaches.
All datasetsare publicly available.4 ApproachHere we present our approaches for detecting (1) vi-sual vs. non-visual QI pairs, and (2) true- vs. false-premise QI pairs.4.1 Visual vs. Non-Visual DetectionRecall that the task here is to detect visual questionsfrom non-visual ones.
Non-visual questions, suchas ?Do dogs fly??
or ?Who is the president of theUSA?
?, often tend to have a difference in the lin-guistic structure from that of visual questions, suchas ?Does this bird fly??
or ?What is this man do-ing??.
We compare our approach (LSTM) with abaseline (RULE-BASED):1.
RULE-BASED.
A rule-based approach to detectnon-visual questions based on the part of speech(POS)3 tags and dependencies of the words in thequestion.
E.g., if a question has a plural noun withno determiner before it and is followed by a singularverb (?Do dogs fly??
), it is a non-visual question.42.
LSTM.
We train an LSTM with 100-dim hid-den vectors to embed the question into a vector andpredict visual vs. not.
Instead of feeding questionwords ([?what?, ?is?, ?the?, ?man?, ?doing?, ???
]), theinput to our LSTM is embeddings of POS tags ofthe words ([?pronoun?, ?verb?, ?determiner?, ?noun?,?verb?]).
Embeddings of the POS tags are learntend-to-end.
This captures the structure of image-278% of the time all three votes agree.3We use spaCy POS tagger (Honnibal and Johnson, 2015).4See supplement for examples of such hand-crafted rules.grounded questions, rather than visual vs. non-visual topics.
The latter are less likely to generalizeacross domains.4.2 True- vs. False-Premise DetectionOur second task is to detect whether a question Q en-tails a false-premise for an image I.
We present twofamilies of approaches to measure this QI ?compat-ibility?
: (i) using uncertainty in VQA models, and(ii) using pre-trained captioning models.Using VQA Uncertainty.
Here we work with thehypothesis that if a VQA model is uncertain aboutthe answer to a QI pair, the question may be irrele-vant for the given image since the uncertainty maymean it has not seen similar QI pairs in the trainingdata.
We test two approaches:1.
ENTROPY.
We compute the entropy of the soft-max output from a state-of-the art VQA model (An-tol et al, 2015; Lu et al, 2015) for a given QI pairand train a three-layer multilayer perceptron (MLP)on top with 3 nodes in the hidden layer.2.
VQA-MLP.
We feed in the softmax output to athree-layer MLP with 100 nodes in the hidden layer,and train it as a binary classifier to predict whethera question has a true- or false-premise for the givenimage.Using Pre-trained Captioning Models.
Here weutilize (a) an image captioning model, and (b) animage question-generation model ?
to measure QIcompatibility.
Note that both these models generatenatural language capturing the semantics of an im-age ?
one in the form of statement, the other in theform of a question.
Our hypothesis is that a givenquestion is relevant to the given image if it is similarto the language generated by these models for thatimage.
Specifically:1.
Question-Caption Similarity (Q-C SIM).
Weuse NeuralTalk2 (Karpathy and Fei-Fei, 2015) pre-trained on the MSCOCO dataset (Lin et al, 2014b)(images and associated captions) to generate a cap-tion C for the given image, and then compute alearned similarity between Q and C (details below).2.
Question-Question Similarity (Q-Q?
SIM).
Weuse NeuralTalk2 re-trained (from scratch) on thequestions in the VQA dataset to generate a questionQ?
for the image.
Then, we compute a learned simi-larity between Q and Q?.921Visual vs. Non-Visual True- vs. False-PremiseRULE-BASED LSTM ENTROPY VQA-MLP Q-GEN SCORE Q-C SIM Q-Q?
SIM75.68 92.27 59.66 64.19 57.41 74.48 74.58Table 1: Normalized accuracy results (averaged over 40 random train/test splits) for visual vs. non-visual detection andtrue- vs. false-premise detection.
RULE-BASED and Q-GEN SCORE were not averaged because they are deterministic.We now describe our learned Q-C similarity func-tion (the Q-Q?
similarity is analogous).
Our Q-Csimilarity model is a 2-channel LSTM+MLP (onechannel for Q, another for C).
Each channel se-quentially reads word2vec embeddings of the cor-responding language via an LSTM.
The last hid-den state vectors (40-dim) from the 2 LSTMs areconcatenated and fed as inputs to the MLP, whichoutputs a 2-class (relevant vs. not) softmax.
Theentire model is learned end-to-end on the VTFQdataset.
We also experimented with other represen-tations (e.g., bag of words) for Q, Q?, C, which areincluded in the supplement for completeness.Finally, we also compare our proposed modelsabove to a simpler baseline (Q-GEN SCORE), wherewe compute the probability of the input question Qunder the learned question-generation model.
Theintuition here is that since the question generationmodel has been trained only on relevant questions(from the VQA dataset), it will assign a high proba-bility to Q if it is relevant.5 Experiments and ResultsThe results for both experiments are presented in Ta-ble 1.
We present results averaged over 40 randomtrain/test splits.
RULE-BASED and Q-GEN SCOREwere not averaged because they are deterministic.Visual vs. Non-Visual Detection.
We use a ran-dom set of 100,000 questions from the VNQ datasetfor training, and the remaining 31,464 for testing.We see that LSTM performs 16.59% (21.92% rela-tive) better than RULE-BASED.True- vs. False-Premise Detection.
We use a ran-dom set of 7,195 (67%) QI pairs from the VTFQdataset to train and the remaining 3,597 (33%) totest.
While the VQA model uncertainty based ap-proaches (ENTROPY, VQA-MLP) perform reason-ably well (with the MLP helping over raw entropy),the learned similarity approaches perform much bet-ter (10.39% gain in normalized accuracy).
High un-certainty of the model may suggest that a similar QIpair was not seen during training; however, that doesnot seem to translate to detecting irrelevance.
Thelanguage generation models (Q-C SIM, Q-Q?
SIM)seem to work significantly better at modeling thesemantic interaction between the question and theimage.
The generative approach (Q-GEN SCORE)is outperformed by the discriminative approaches(VQA-MLP, Q-C SIM, Q-Q?
SIM) that are trainedexplicitly for the task at hand.
We show qualitativeexamples of Q-Q?
SIM for true- vs. false-premisedetection in Fig.
2.6 Human Qualitative EvaluationWe also perform human studies where we comparetwo agents: (1) AGENT-BASELINE?
always answersevery question.
(2) AGENT-OURS?
reasons aboutquestion relevance before responding.
If question isclassified as visual true-premise, AGENT-OURS an-swers the question using the same VQA model asAGENT-BASELINE (using (Lu et al, 2015)).
Other-wise, it responds with a prompt indicating that thequestion does not seem meaningful for the image.A total of 120 questions (18.33% relevant,81.67% irrelevant, mimicking the distribution of theVTFQ dataset) were used.
Of the relevant ques-tions, 54% were answered correctly by the VQAmodel.
Human subjects on AMT were shown theresponse of both agents and asked to pick the agentthat sounded more intelligent, more reasonable, andmore human-like after every observed QI pair.
EachQI pair was assessed by 5 different subjects.
Not allpairs were rated by the same 5 subjects.
In total, 28unique AMT workers participated in the study.AGENT-OURS was picked 65.8% of the time asthe winner, AGENT-BASELINE was picked only1.6% of the time, and both considered equally(un)reasonable in the remaining cases.
We also mea-sure the percentage of times each robot gets picked922Q":"Is"the"event"indoor"or"outdoor?Q'#:#What"is"the"elephant"doing?US GT(a)Q":"What"type"of"melon"is"that?Q' :"What"color"is"the"horse?US GT(b)Q:"Is"this"man"married?Q':"What"is"the"man"holding?US GT(c)Q:"Is"that"graffiti"on"the"wall?
"Q':"What"is"the"woman"holding?US GT(d)Figure 2: Qualitative examples for Q-Q?
SIM.
(a) and (b) show success cases, and (c) and (d) show failure cases.Our model predicts true-premise in (a) and (c), and false-premise in (b) and (d).
In all examples we show the originalquestion Q and the generated question Q?.by the workers for true-premise, false-premise, andnon-visual questions.
These percentages are shownin Table 2.True-PremiseFalse-PremiseNon-VisualAGENT-OURS 22.7 78.2 65.0AGENT-BASELINE 04.7 01.4 00.0Both 27.2 03.8 10.0None 45.4 16.6 25.0Table 2: Percentage of times each robot gets picked byAMT workers as being more intelligent, more reasonable,and more human-like for true-premise, false-premise, andnon-visual questions.Interestingly, humans often prefer AGENT-OURSover AGENT-BASELINE even when both models arewrong ?
AGENT-BASELINE answers the questionincorrectly and AGENT-OURS incorrectly predictsthat the question is irrelevant and refuses to answera legitimate question.
Users seem more tolerant tomistakes in relevance prediction than VQA.7 ConclusionWe introduced the novel problem of identifying ir-relevant (i.e., non-visual or visual false-premise)questions for VQA.
Our proposed models signifi-cantly outperform strong baselines on both tasks.
AVQA agent that utilizes our detector and refuses toanswer certain questions significantly outperforms abaseline (that answers all questions) in human stud-ies.
Such an agent is perceived as more intelligent,reasonable, and human-like.There are several directions for future work.
Onepossibility includes identifying the premise entailedin a question, as opposed to just stating true- orfalse-premise.
Another is determining what exter-nal knowledge is needed to answer non-visual ques-tions.Our system can be further augmented to com-municate to users what the assumed premise of thequestion is that is not satisfied by the image, e.g.,respond to ?What is the woman wearing??
for animage of a cat by saying ?There is no woman.
?AcknowledgementsWe thank Lucy Vanderwende for helpful sugges-tions and discussions.
We also thank the anony-mous reviewers for their helpful comments.
Thiswork was supported in part by the following: Na-tional Science Foundation CAREER awards to DBand DP, Alfred P. Sloan Fellowship, Army ResearchOffice YIP awards to DB and DP, ICTAS Junior Fac-ulty awards to DB and DP, Army Research Lab grantW911NF-15-2-0080 to DP and DB, Office of NavalResearch grant N00014-14-1- 0679 to DB, Paul G.Allen Family Foundation Allen Distinguished Inves-tigator award to DP, Google Faculty Research awardto DP and DB, AWS in Education Research grant toDB, and NVIDIA GPU donation to DB.923ReferencesStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-garet Mitchell, Dhruv Batra, C. Lawrence Zitnick, andDevi Parikh.
2015.
VQA: Visual Question Answer-ing.
In ICCV.Long Chen, Dell Zhang, and Levene Mark.
2012.
Un-derstanding User Intent in Community Question An-swering.
In WWW.Stephen Choularton.
2009.
Early Stage Detection ofSpeech Recognition Errors.
Ph.D. thesis, MacquarieUniversity.Jia Deng, Jonathan Krause, Alexander C Berg, andLi Fei-Fei.
2012.
Hedging Your Bets: OptimizingAccuracy-Specificity Trade-offs in Large Scale VisualRecognition.
In CVPR.Pandu R Devarakota, Bruno Mirbach, and Bjo?rn Otter-sten.
2007.
Confidence estimation in classificationdecision: A method for detecting unseen patterns.
InICAPR.Jesse Dodge, Amit Goyal, Xufeng Han, Alyssa Men-sch, Margaret Mitchell, Karl Stratos, Kota Yamaguchi,Yejin Choi, Hal Daume?, III, Alexander C. Berg, andTamara L. Berg.
2012.
Detecting Visual Text.
InNAACL HLT.Jeffrey Donahue, Lisa Anne Hendricks, Sergio Guadar-rama, Marcus Rohrbach, Subhashini Venugopalan,Kate Saenko, and Trevor Darrell.
2015.
Long-termRecurrent Convolutional Networks for Visual Recog-nition and Description.
In CVPR.Hao Fang, Saurabh Gupta, Forrest Iandola, Rupesh K.Srivastava, Li Deng, Piotr Dollar, Jianfeng Gao,Xiaodong He, Margaret Mitchell, John C. Platt,C.
Lawrence Zitnick, and Geoffrey Zweig.
2015.From Captions to Visual Concepts and Back.
InCVPR.Yansong Feng and Mirella Lapata.
2013.
AutomaticCaption Generation for News Images.
PAMI, 35(4).Matthew Honnibal and Mark Johnson.
2015.
An Im-proved Non-monotonic Transition System for Depen-dency Parsing.
In EMNLP.Andrej Karpathy and Li Fei-Fei.
2015.
Deep Visual-Semantic Alignments for Generating Image Descrip-tions.
In CVPR.Dahua Lin, Sanja Fidler, Chen Kong, and Raquel Ur-tasun.
2014a.
Visual Semantic Search: RetrievingVideos via Complex Textual Queries.
In CVPR.Tsung-Yi Lin, Michael Maire, Serge Belongie, JamesHays, Pietro Perona, Deva Ramanan, Piotr Dolla?r, andC Lawrence Zitnick.
2014b.
Microsoft COCO: Com-mon objects in context.
In ECCV.Dong Liu, Xian-Sheng Hua, Meng Wang, and HongJiangZhang.
2009.
Boost Search Relevance for Tag-basedSocial Image Retrieval.
In ICME.Jiasen Lu, Xiao Lin, Dhruv Batra, and Devi Parikh.2015.
Deeper LSTM and normalized CNN VisualQuestion Answering model.
https://github.com/VT-vision-lab/VQA_LSTM_CNN.Mateusz Malinowski and Mario Fritz.
2014.
A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input.
In NIPS.Vicente Ordonez, Girish Kulkarni, and Tamara L. Berg.2011.
Im2Text: Describing Images Using 1 MillionCaptioned Photographs.
In NIPS.Mengye Ren, Ryan Kiros, and Richard Zemel.
2015.
Ex-ploring models and data for image question answering.In NIPS.Arup Sarma and David D Palmer.
2004.
Context-basedSpeech Recognition Error Detection and Correction.In NAACL HLT.Kimberly Voll, Stella Atkins, and Bruce Forster.
2008.Improving the Utility of Speech Recognition ThroughError Detection.
Journal of Digital Imaging, 21(4).Kelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville,Ruslan Salakhutdinov, Richard Zemel, and YoshuaBengio.
2015.
Show, Attend and Tell: Neural ImageCaption Generation with Visual Attention.
In ICML.Peng Zhang, Jiuling Wang, Ali Farhadi, Martial Hebert,and Devi Parikh.
2014.
Predicting Failures of VisionSystems.
In CVPR.Tongmu Zhao, Akemi Hoshino, Masayuki Suzuki,Nobuaki Minematsu, and Keikichi Hirose.
2012.
Au-tomatic Chinese Pronunciation Error Detection UsingSVM Trained with Structural Features.
In SpokenLanguage Technology Workshop.924
