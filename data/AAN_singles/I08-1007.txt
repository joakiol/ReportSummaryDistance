Orthographic Disambiguation Incorporating Transliterated ProbabilityEiji ARAMAKI Takeshi IMAI Kengo Miyo Kazuhiko OheUniversity of Tokyo7-3-1 Hongo, Bunkyo-ku, Tokyo 113-8655, Japanaramaki@hcc.h.u-tokyo.ac.jpAbstractOrthographic variance is a fundamentalproblem for many natural language process-ing applications.
The Japanese language, inparticular, contains many orthographic vari-ants for two main reasons: (1) transliteratedwords allow many possible spelling varia-tions, and (2) many characters in Japanesenouns can be omitted or substituted.
Pre-vious studies have mainly focused on theformer problem; in contrast, this study hasaddressed both problems using the sameframework.
First, we automatically col-lected both positive examples (sets of equiv-alent term pairs) and negative examples (setsof inequivalent term pairs).
Then, by usingboth sets of examples, a support vector ma-chine based classifier determined whethertwo terms (t1 and t2) were equivalent.
Toboost accuracy, we added a transliteratedprobability P (t1|s)P (t2|s), which is theprobability that both terms (t1 and t2) weretransliterated from the same source term (s),to the machine learning features.
Exper-imental results yielded high levels of ac-curacy, demonstrating the feasibility of theproposed approach.1 IntroductionSpelling variations, such as ?center?
and ?centre?,which have different spellings but identical mean-ings, are problematic for many NLP applicationsincluding information extraction (IE), question an-swering (QA), and machine transliteration (MT).
InTable 1: Examples of Orthographic Variants.spaghetti Thompson operation* ??
indicates a pronunciation.
() indicates a translation.this paper, these variations can be termed ortho-graphic variants.The Japanese language, in particular, containsmany orthographic variants, for two main reasons:1.
It imports many words from other languagesusing transliteration, resulting in many possiblespelling variations.
For example, Masuyama etal.
(2004) found at least six different spellingsfor?
spaghetti?in newspaper articles (Table 1Left).2.
Many characters in Japanese nouns can beomitted or substituted, leading to tons of in-sertion variations (Daille et al, 1996) (Table 1Right).To address these problems, this study developed asupport vector machine (SVM) based classifier that48can determine whether two terms are equivalent.
Be-cause a SVM-based approach requires positive andnegative examples, we also developed a method toautomatically generate both examples.Our proposed method differs from previously de-veloped methods in two ways.1.
Previous studies have focused solely on the for-mer problem (transliteration); our target scopeis wider.
We addressed both transliterationand character omissions/substitutions using thesame framework.2.
Most previous studies have focused on back-transliteration (Knight and Graehl, 1998; Gotoet al, 2004), which has the goal of generating asource word (s) for a Japanese term (t).
In con-trast, we employed a discriminative approach,which has the goal of determining whether twoterms (t1 and t2) are equivalent.
These twogoals are related.
For example, if two terms (t1and t2) were transliterated from the same word(s), they should be orthographic variants.
Toincorporate this information, we incorporateda transliterated-probability (P (s|t1)?P (s|t2))into the SVM features.Although we investigated performance usingmedical terms, our proposed method does not de-pend on a target domain1.2 Orthographic Variance in DictionaryEntriesBefore developing our methodology, we examinedproblems related to orthographic variance.First, we investigated the amount of orthographicvariance between two dictionaries?
entries (DIC1(Ito et al, 2003), totaling 69,604 entries, and DIC2(Nanzando, 2001), totaling 27,971 entries).Exact matches between entries only occurred for10,577 terms (15.1% of DIC1, and 37.8% of DIC2).From other entries, we extracted orthographic vari-ance as follows.STEP 1: Extracting Term Pairs with SimilarSpelling1The domain could affect the performance, because most ofmedical terms are imported from other languages, leading tomany orthographic variants.    SIMRatio(%)Figure 1: Similarity Threshold and OrthographicVariants Ratio.We extracted term pairs with similar spelling(t1 and t2) using edit distance-based similarity(defined by Table 2).
We extracted term pairswith SIMed > 0.8, and found 5,064 term pairswith similar spelling.STEP 2: Judging Orthographic VarianceWe then manually judged whether each termpair was composed of orthographic variants(whether or not they had the same meaning).Our results indicated that 1,889 (37.3%) of theterms were orthographic variants.Figure 1 presents the relation between the ortho-graphic variation ratio and similarity threshold (0.8-1.0).
As shown in the figure, a higher similaritythreshold (SIM=0.96-97) does not always indicatethat terms are orthographic variants.The following term pair is a typical example:1.
(mutated hepatitis type B virus),2.
(mutated hepatitis type C virus).They have only one character difference (?B?
and?C?
), resulting in high levels of spelling similar-ity, but the meanings are not equivalent.
This typeof limitation, intrinsic to measurements of spellingsimilarity, motivated us to develop an SVM-basedclassifier.3 MethodWe developed an SVM-based classifier that deter-mines whether two terms are equivalent.
Section 3.149Table 2: Edit Distance-based Similarity (SIMed).The edit distance-based similarity (SIMed)between two terms (t1, t2) is defined as fol-lows:SIMed(t1, t2) = 1?EditDistance(t1, t2)?
2len(t1) + len(t2),where len(t1) is the number of characters oft1, len(t2) is the number of characters of t2,Edit Distance(t1, t2) is the minimum numberof point mutations required to change t1 intot2, where a point mutation is one of: (1) achange in a character, (2) the insertion of acharacter, and (3) the deletion of a character.For details, see (Levenshtein, 1965).will describe the method we used to build trainingdata, and Section 3.2 will introduce the classifier.3.1 Automatic Building of ExamplesPositive ExamplesOur method uses a straight forward approach toextract positive examples.
The basic idea is that or-thographic variants should have (1) similar spelling,and (2) the same English translation.The method consists of the following two steps:STEP 1: First, using two or more translation dictio-naries, extract a set of Japanese terms with thesame English translation.STEP 2: Then, for each extracted set, generate twopossible term pairs (t1 and t2) and calculate thespelling similarity between them.
Spelling sim-ilarity is measured by edit distance-based simi-larity (see Section 2).
Any term pair with morethan a threshold (SIMed(t1, t2) > 0.8) simi-larity is considered a positive example.Negative ExamplesWe based our method of extracting negative ex-amples using the dictionary-based method.
As withpositive examples, we collected term pairs with sim-ilar spellings (SIMed(t1, t2) > 0.8), but differingEnglish translations.However, the above heuristic is not sufficient toextract negative examples; different English termsmight have the same meaning, which could causeunsuitable negative examples.For example, t1 ?
(stomach cancer)?
andt2 ?
(stomach carcinoma)?
: although thesewords have differing English translations, unfortu-nately they are not a negative example (?cancer?
and?carcinoma?
are synonymous).To address this problem, we employed a corpus-based approach, hypothesizing that if two terms areorthographic variants, they should rarely both ap-pear in the same document.
Conversely, if bothterms appear together in many documents, they areunlikely to be orthographic variants (negative exam-ples).Based on this assumption, we defined the follow-ing scoring method:Score(t1, t2) =log(HIT (t1, t2))max(log(HIT (t1)), log(HIT (t2))),where HIT (t) is the number of Google hits for aquery t. We only used negative examples with thehighest K score, and discarded the others2.3.2 SVM-Based ClassifierThe next problem was how to convert training-datainto machine learning features.
We used two typesof features.Character-Based FeaturesWe expressed different characters between twoterms and their context (window size ?1) as fea-tures, shown in Table 3.
Thus, to represent an omis-sion, ??
(null)?
is considered a character.
Two ex-amples are provided in Figures 2.Note that if terms contain two or more differingparts, all the differing parts are converted into fea-tures.Similarity-based FeaturesAnother type of feature is the similarity betweentwo terms (t1 and t2).
We employed two similarities:1.
Edit distance-based similarity SIMed(t1, t2)(see Section 2).2.
Transliterated similarity, which is the probabil-ity that two terms (t1 and t2) were transliterated2In the experiments in Section 4, we set K is 41,120, whichis equal to the number of positive examples.50Table 3: Character-based Features.LEX-DIFFDiffering characters betweentwo terms, consisting of a pairof n : m characters (n > 0 andm > 0).
For example, we regard?
(t)?
??
as LEX-DIFF inFigure 2 TOP.LEX-PREPrevious character of DIFF.
Weregard ?
(ge)?
as LEX-PRE inFigure 2 TOP.LEX-POSTSubsequent character of DIFF.We regard ?
(te)?
as LEX-POST in Figure 2 TOP.TYPE-DIFFA script type of differingcharacters between two terms,classified into four cate-gories: (1) HIRAGANA-script,(2) KATAKANA-script, (3)Chinese-character script or(4) others (symbols, numer-ous expressions etc.))
Weregard ?KATAKANA?
??
asTYPE-DIFF in Figure 2 TOP.TYPE-PREA type previous character ofDIFF.
We regard ?KATAKANA?as TYPE-PRE in Figure 2 TOP.TYPE-POSTA type subsequent character ofDIFF.
We regard ?KATAKANA?as TYPE-POST in Figure 2 TOP.LEN-DIFF A length (the number of charac-ters) of differing parts.?
?
?
?
?
??
?
?
?
?g ePOST PREg eDIFFpasu itepasu itet??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?type virushepatitisbDIFF POST PREmutatedtype virushepatitiscmutatedFigure 2: A Positive Example (TOP) and A NegativeExample (BOTTOM).from the same source word (t) (defined in Table4).Note that the latter, transliterated similarity, isapplicable to a situation in which the input pair istransliterated.4 Experiments4.1 Test-SetTo evaluate the performance of our system, we usedjudged term pairs, as discussed in Section 2 (ALL-SET).
We also extracted a sub-set of these pairs inorder to focus on a transliteration problem (TRANS-SET).1.
ALL-SET: This set consisted of all examples(1,889 orthographic variants of 5,064 pairs)2.
TRANS-SET: This set contained only exam-ples of transliteration (543 orthographic vari-ants or 1,111 pairs).4.2 Training-SetUsing the proposed method set out in Section 3,we automatically constructed a training-set fromtwo translation dictionaries (Japan Medical Termi-nology English-Japanese(Nanzando, 2001) and 25-Thousand-Term Medical Dictionary(MEID, 2005)).51The resulting training-set consisted of 82,240 exam-ples (41,120 positive examples and 41,120 negativeexamples).4.3 Comparative MethodsWe compared the following methods:1.
SIM-ED: An edit distance-based method,which regards an input with a similaritySIMed(t1, t2) > TH as an orthographic vari-ant.2.
SIM-TR: A transliterated based method, whichregards an input with a spelling similaritySIMtr(t1, t2) > TH as an orthographic vari-ant (TRANS-SET only).3.
PROPOSED: Our proposed method withoutSIMtr features.4.
PROPOSED+TR: Our proposed method withSIMtr features.
(TRANS-SET only).For SVM learning, we used TinySVM3 with poly-nomial kernel (d=2).4.4 EvaluationWe used the three following measures to evaluateour method:Precision = # of pairs found and correcttotal # of pairs found,Recall = # of pairs found and correcttotal # of pairs correct,F?=1 = 2?Recall ?
PrecisionRecall + Precision.4.5 ResultsTable 5 presents the performance of all methods.The accuracy of similarity-based methods (SIM-EDand SIM-TR) varied depending on the threshold(TH).
Figure 3 is a precision-recall graph of allmethods in TRANS-SET.In ALL-SET, PROPOSED outperformed asimilarity-based method (SIM-ED) in F?=1,demonstrating the feasibility of the proposeddiscriminative approach.3http://chasen.org/ taku/software/TinySVM/    P re cision(%)Recall(%)SIM-TRSIM-EDPROPOSED+TRPROPOSEDFigure 3: SIM and orthographic variants ratio.In TRANS-SET, PROPOSED also outperformedtwo similarity-based methods (SIM-ED and SIM-TR).
In addition, PROPOSED+TR yielded higherlevels of accuracy than PROPOSED.
Based on thisresult, we can conclude that adding transliterated-probability improved accuracy.It was difficult to compare accuracy between theresults of our study and previous studies.
Previousstudies used different corpora, and also focused on(back-) transliteration.
However, our accuracy levelswere at least as good as those in previous studies(64% by (Knight and Graehl, 1998) and 87.7% by(Goto et al, 2004)).4.6 Error AnalysisWe investigated errors from PROPOSED and PRO-POSED+TR, and found two main types.1.
Different Script TypesThe Japanese language can be expressed usingthree types of script: KANJI (Chinese char-acters), KATAKANA, and HIRAGANA.
Al-though each of these scripts can be convertedto another, (such as ?
?
(?epilepsia?
inKANJI script) and ?
?
(?epilepsia?
inHIRAGANA script), our method cannot dealwith this phenomenon.
Future research willneed to add steps to solve this problem.2.
Transliteration from Non-English Lan-52Table 5: ResultsALL-SET TRANS-SETPrecision Recall F?=1 Precision Recall F?=1SIM-ED 65.2% 64.6% 0.65 91.2% 36.3% 0.51SIM-TR - - - 92.6% 43.9% 0.59PROPOSED 78.2% 70.2% 0.73 81.9% 75.6% 0.78PROPOSED+TR - - - 81.7% 82.7% 0.82* The performance in SIM-ED and SIM-TR showed the highest F?=1 values.guagesWhile our experimental set consisted of medi-cal terms, including a few transliterations fromLatin or German, transliteration-probabilitywas trained using transliterations from theEnglish language (using a general dictio-nary).
Therefore, PROPOSED+TR results areinferior when inputs are from non-Englishlanguages.
In a general domain, SIM-TR andPROPOSED+TR would probably yield higheraccuracy.5 Related WorksAs noted in Section 1, transliteration is the most rel-evant field to our work, because it results in manyorthographic variations.Most previous transliteration studies have focusedon finding the most suitable back-transliteration of aterm.
For example, Knight (1998) proposed a prob-abilistic model for transliteration.
Goto et al(2004)proposed a similar method, utilizing surroundingcharacters.Their method is not only applicable to Japanese;it has already been used for Korean(Oh and Choi,2002; Oh and Choi, 2005; Oh and Isahara, 2007),Arabic(Stalls and Knight, 1998; Sherif and Kon-drak, 2007), Chinese(Li et al, 2007), and Per-sian(Karimi et al, 2007).Our method uses a different kind of task-setting,compared to previous methods.
It is based on deter-mining whether two terms within the same languageare equivalent.
It provides high levels of accuracy,which should be practical for many applications.Another issue is that of how to represent translit-eration phenomena.
Methods can be classifiedinto three main types: grapheme-based (Li etal., 2004); phoneme-based (Knight and Graehl,1998); and combinations of both these meth-ods( hybrid-model(Bilac and Tanaka, 2004) andcorrespondence-based model(Oh and Choi, 2002;Oh and Choi, 2005)).
Our proposed method em-ployed a grapheme-based approach.
We selectedthis kind of approach because it allows us to han-dle not only transliteration but also character omis-sions/substitutions, which we would not be able toaddress using a phoneme-based approach (and acombination approach).Yoon et al (2007) also proposed a discriminativetransliteration method, but their system was basedon determining whether a target term was transliter-ated from a source term.Bergsma and Kondrak (2007) and Aramaki et al(2007) proposed on a discriminative method for sim-ilar spelling terms.
However, they did not deal witha transliterated probability.Masuyama et al (2004) collected 178,569Japanese transliteration variants (positive examples)from a large corpus.
In contrast, we collected bothpositive and negative examples in order to train theclassifier.6 ConclusionWe developed an SVM-based orthographic dis-ambiguation classifier, incorporating transliterationprobability.
We also developed a method for col-lecting both positive and negative examples.
Ex-perimental results yielded high levels of accuracy,demonstrating the feasibility of the proposed ap-proach.
Our proposed classifier could become a fun-damental technology for many NLP applications.AcknowledgmentsPart of this research is supported by Grant-in-Aid for Scientific Research of Japan So-53Table 4: Transliterated Similarity (SIMtr).The transliterated similarity (SIMtr) betweentwo terms (t1, t2) is defined as followsa:SIMtr(t1, t2) =?s?SP (t1|s)P (t2|s),where S is a set of back-transliterations that aregenerated from both t1 and t2, P (e|t) is a prob-ability of Japanese term (t) comes from a sourceterm s.P (t|s) =|K|?k=1P (tk|sk),P (tk|sk) =frequency of sk ?
tkfrequency of sk,where |K| is the number of characters in a termt, tk is the k-th character of a term t, sk is thek-th character sequence of a term s, ?frequencyof sk ?
tk?
is the occurrences of the alignments,?frequency of sk?
is the occurrences of a charac-ter sk.To get algnment, we extracted 100,128 translit-erated term pairs from a transliteration dictionary(EDP, 2005), and estimate its alignment by usingGIZA++b.
We aligned in Japanese-to-English di-rection, and got 1 : m alignments (one Japanesecharacter : m alphabetical characters) to cal-culate P (tk|sk).
These formulas are equal to(Karimi et al, 2007).aSIMtr(t1, t2) is a similarity (not a probability)bhttp://www.fjoch.com/GIZA++.htmlciety for the Promotion of Science (ProjectNumber:16200039, F.Y.2004-2007 and 18700133,F.Y.2006-2007) and the Research CollaborationProject (#047100001247) with Japan Anatomy Lab-oratory Co.Ltd.ReferencesEiji Aramaki, Takeshi Imai, Kengo Miyo, and KazuhikoOhe.
2007.
Support vector machine based ortho-graphic disambiguation.
In Proceedings of the Con-ference on Theoretical and Methodological Issues inMachine Translation (TMI2007), pages 21?30.Shane Bergsma and Grzegorz Kondrak.
2007.Alignment-based discriminative string similarity.
InProceedings of the Association for Computational Lin-guistics (ACL2007), pages 656?663.Slaven Bilac and Hozumi Tanaka.
2004.
A hybrid back-transliteration system for Japanese.
In Proceedings ofThe 20th International Conference on ComputationalLinguistics (COLING2004), pages 597?603.B.
Daille, B. Habert, C. Jacquemin, and J. Royaut.
1996.Empirical observation of term variations and princi-ples for their description.
Terminology, 3(2):197?258.EDP.
2005.
Eijiro Japanese-English dictionary, elec-tronic dictionary project.Isao Goto, Naoto Kato, Terumasa Ehara, and HidekiTanaka.
2004.
Back transliteration from Japaneseto English using target English context.
In Proceed-ings of The 20th International Conference on Compu-tational Linguistics (COLING2004), pages 827?833.M.
Ito, H. Imura, and H. Takahisa.
2003.
IGAKU-SHOIN?S MEDICAL DICTIONARY.
Igakusyoin.Sarvnaz Karimi, Falk Scholer, and Andrew Turpin.
2007.Collapsed consonant and vowel models: New ap-proaches for English-Persian transliteration and back-transliteration.
In Proceedings of the Annual Meet-ing of the Association of Computational Linguistics(ACL2007), pages 648?655.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4):599?612.V.
I. Levenshtein.
1965.
Binary codes capable of cor-recting deletions, insertions and reversals.
DokladyAkademii Nauk SSSR, 163(4):845?848.Haizhou Li, Min Zhang, and Jian Su.
2004.
A jointsource-channel model for machine transliteration.
InProceedings of the Meeting of the Association forComputational Linguistics (ACL2004), pages 159?166.Haizhou Li, Khe Chai Sim, Jin-Shea Kuo, and MinghuiDong.
2007.
Semantic transliteration of personalnames.
In Proceedings of the Annual Meeting of theAssociation of Computational Linguistics (ACL2007),pages 120?127.Takeshi Masuyama, Satoshi Sekine, and Hiroshi Nak-agawa.
2004.
Automatic construction of JapaneseKATAKANA variant list from large corpus.
InProceedings of The 20th International Conferenceon Computational Linguistics (COLING2004), pages1214?1219.MEID.
2005.
25-Mango Medical Dictionary.
NichigaiAssociates, Inc.54Nanzando.
2001.
Japan Medical Terminology English-Japanese 2nd Edition.
Committee of Medical Termi-nology, NANZANDO Co.,Ltd.Jong-Hoon Oh and Key-Sun Choi.
2002.
An English-Korean transliteration model using pronunciation andcontextual rules.
In Proceedings of The 19th In-ternational Conference on Computational Linguistics(COLING2002), pages 758?764.Jong-Hoon Oh and Key-Sun Choi.
2005.
An ensembleof grapheme and phoneme for machine transliteration.In Proceedings of Second International Joint Confer-ence on Natural Language Processing (IJCNLP2005),pages 450?461.Jong-Hoon Oh and Hitoshi Isahara.
2007.
Machinetransliteration using multiple transliteration enginesand hypothesis re-ranking.
In Proceedings of MT Sum-mit XI, pages 353?360.Tarek Sherif and Grzegorz Kondrak.
2007.
Substring-based transliteration.
In Proceedings of the 45th An-nual Meeting of the Association of Computational Lin-guistics (ACL2007), pages 944?951.Bonnie Glover Stalls and Kevin Knight.
1998.
Trans-lating names and technical terms in arabic text.
InProceedings of The International Conference on Com-putational Linguistics and the 36th Annual Meet-ing of the Association of Computational Linguistics(COLING-ACL1998) Workshop on Computational Ap-proaches to Semitic Languages.Su-Youn Yoon, Kyoung-Young Kim, and Richard Sproat.2007.
Multilingual transliteration using feature basedphonetic method.
In Proceedings of the Annual Meet-ing of the Association of Computational Linguistics(ACL2007), pages 112?119.55
