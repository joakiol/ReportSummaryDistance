Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 98?109,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsMultiparty Turn Taking in Situated Dialog:Study, Lessons, and DirectionsDan Bohus Eric HorvitzMicrosoft Research Microsoft ResearchOne Microsoft Way One Microsoft WayRedmond, WA, 98052 Redmond, WA, 98052dbohus@microsoft.com horvitz@microsoft.comAbstractWe report on an empirical study of a multipartyturn-taking model for physically situated spo-ken dialog systems.
We present subjective andobjective performance measures that show howthe model, supported with a basic set of sensorycompetencies and turn-taking policies, can en-able interactions with multiple participants in acollaborative task setting.
The analysis bringsto the fore several phenomena and frames chal-lenges for managing multiparty turn taking inphysically situated interaction.1.
IntroductionEffective dialog relies on the coordination of con-tributions by participants in a conversation via turntaking.
The complexity of understanding and man-aging turns grows significantly in moving fromdyadic to multiparty settings, including situationswhere groups of people converse as they collabo-rate on shared goals.
We are exploring computa-tional methods that can endow dialog systems withthe ability to participate in a natural, fluid mannerin conversations involving several people.In Bohus and Horvitz (2010a), we presented acomputational model for managing multiparty turntaking.
The model harnesses multisensory percep-tion and reasoning and includes a set of compo-nents and representations.
These include methodsfor tracking multiparty conversational dynamics,for making turn-taking decisions, and for renderingdecisions about turns into an appropriate set oflow-level, coordinated gaze, gesture and speechbehaviors.
We implemented the model and havebeen testing it in several domains.
The investiga-tions have been aimed at characterizing the sys-tem?s performance in complex multiparty settings.In Bohus and Horvitz (2010b), we examine datacollected during a user study to evaluate the abilityof the system to shape the flow of multiparty con-versational dynamics.
In this paper, we focus ourattention on the performance of the inference anddecision-making models.
We analyze the accuracyof current turn-taking inferences, the influence ofinference errors on decisions, and the overall effec-tiveness of the system?s decision making.
We re-port on subjective and objective measures of thesystem?s turn-taking performance.
We find that theturn-taking methodology enables our system tosuccessfully participate in multiparty interactions,even when relying on relatively coarse models forinference and decision making.
The analysis high-lights several general phenomena including stand-ing bottlenecks and difficulties, and opportunitiesfor enhancing multiparty turn taking in dialog sys-tems.
Based on the results, we discuss challengesand directions for research on turn taking in physi-cally situated dialog.2.
Related WorkWe begin by placing this work within the largercontext of research on multiparty interaction andturn taking.
In a seminal paper on turn taking innatural conversations, Sacks, Schegloff and Jeffer-son (1974) proposed a basic model for the organi-98zation of turns in conversation.
The model is cen-tered on the notion of turn-constructional-units,separated by transition relevance places that pro-vide opportunities for speaker changes.
In laterwork, Schegloff (2000) elaborates on several as-pects of this model, including interruptions andoverlap resolution devices.
Other researchers inconversational analysis and psycho-linguisticshave highlighted the important role played by gaze,gesture, and other non-verbal communicationchannels in regulating turn taking.
For instance,Duncan (1972) discusses the role of non-verbalsignals, and proposes that turn taking is mediatedvia a set of verbal and non-verbal cues.
Wiemannand Knapp (1975) survey prior investigations onturn-taking cues in several conversational settings,in an effort to elucidate differences.
Goodwin(1980) discusses various aspects of the relationshipbetween turn taking and attention.
More recently,Hjalmarsson (2011) investigates the additive effectturn-taking cues have on listeners in both humanand synthetic voices.Figure 1.
Components of turn-taking model.S(s) A(s)SensingCONTRIBDecisionsBehavioral ControlBehaviors andOutput ManagementSystemFloorActionDialogmanagementContribute Audio-visualevidenceDialogContextSpeechGazeGestureSemanticInputSemanticOutputFS(p) FI(p)FA(p)?TurnmanagementWithin the dialog systems community, effortshave been made on designing and implementingcomputational models for managing turn taking(e.g., Traum, 1994; Thorriss?n, 2002; Raux andEskenazi, 2009; Selfridge and Heeman, 2010).Moving beyond the dyadic setting, Traum andRickel (2002) describe a turn management compo-nent for supporting dialog between a trainee andmultiple virtual humans.
Kronlid (2006) describesa Harel state-chart implementation of the originalSSJ model.
Researchers studying human-robot in-teraction have developed prototype robots that caninteract with multiple human participants (e.g.
Ma-tsusaka et al, 2001; Bennewitz et al, 2005).
In ourprevious work Bohus and Horvitz (2009; 2010a;2010b), we describe a platform that leverages mul-timodal perception and reasoning to support multi-party dialog in open-world settings.3.
Multiparty Turn-Taking ModelWe engaged in a set of experiments to probe theinference and decision making competencies of acomputational model for multiparty turn taking(Bohus and Horvitz 2010a; 2010b).
To set thestage for the analysis to follow, we briefly reviewthe proposed approach.We model turn taking as an interactive, collabo-rative process by which participants in a conversa-tion monitor one another and take coordinated ac-tions to ensure that (generally) only one personspeaks at a given time.
The participant ratified tospeak via this process is said to have the floor.Each participant engaged in the interaction con-tinuously produces (i.e.
at every time tick) one offour floor management actions: a hold action indi-cates that a participant is maintaining the floor; arelease action indicates that the participant isyielding the floor to a set of other participants(which could be void, allowing for self-selectionnext turn allocation); a take action indicates thatthe participant is trying to acquire the floor; finally,a null action indicates that a participant is not mak-ing any floor claims.
The floor shifts from one par-ticipant to another as the result of the joint, coop-erative floor management actions taken by the par-ticipants.
Specifically, a release action must be metwith a take action for a floor shift to occur; in allother cases the floor stays with the participant thatcurrently holds it.Figure 1 illustrates the main components andkey abstractions in the model.
The sensing sub-component tracks the conversational dynamics,and includes models for detecting spoken signals s,inferring the source S(s) and the set of addresseesA(s) for each signal, as well as the floor stateFS(p), actions FA(p) and intentions FI(p) of eachparticipant p engaged in a conversation.
This in-formation is used in conjunction with higher-leveldialog context to decide when the system shouldgenerate new contributions and which floor actionshould be produced at each point in time.
Finally,floor actions are rendered by a behavioral compo-nent into a set of coordinated gaze, gesture andspeech behaviors.
By harnessing these differentcomponents, the proposed model can enable an99embodied conversational agent to handle a broadspectrum of turn-taking phenomena.Figure 2.
Questions game: screen and kiosk.4.
User StudyWe implemented an initial set of turn-taking infer-ence and decision making models in the context ofa multiparty dialog system, and we conducted alarge-scale multiparty interaction user study withthis system.
The study, described in more detailbelow, was designed to fulfill two goals: (1) to as-certain an initial performance baseline and identifycurrent bottlenecks and challenges to be addressedmoving forward, and (2)  to collect a large set ofmultiparty human-computer dialog data that can beused to study and improve multiparty turn taking indialog systems.4.1.
SystemThe platform used in these experiments, describedin detail in Bohus and Horvitz (2009), takes theform of a multimodal interactive kiosk that dis-plays an avatar head which plays a questions gamewith multiple participants.
The system leveragesaudiovisual information and employs componentsfor visually tracking multiple people in the scene,sound source localization, speech recognition,conversational scene analysis, behavioral controland dialog management.
Figure 2 shows a screengenerated by the system, with the rendered avatarand a sample challenge question.
Users can col-laborate on selecting an answer, and, after a con-firmation, the system provides an explanation if theanswer is incorrect, before moving on to the nextquestion.
Sample interactions are found in Appen-dix C and videos are available online (Situated In-teraction, 2011).4.2.
Turn-Taking Inference and DecisionsIn the current system, a voice activity detector isused to identify and segment spoken utterances.The source of each utterance is assumed to be theparticipant who is closest in the horizontal plane tothe sound direction identified by the microphonearray.
The set of addressees is identified by fusinginformation probabilistically about the focus ofattention of the source, as obtained through facedetection and head pose tracking, while the utter-ance is being detected.
In addition, the system as-sumes that non-understandings are addressed toother engaged participants, since initial tests indi-cated that in this domain about 80% of utterancesthat led to non-understandings were in fact ad-dressed to others.
Similarly, the system assumesthat utterances longer than three seconds are ad-dressed to others (responses addressed to the sys-tem tend to be short in this domain)Floor management actions are inferred as fol-lows.
If a participant has the floor, we assume theyare performing a hold action if speaking and a re-lease action otherwise.
The release is assumed tobe towards the addressees of the last spoken utter-ance.
Although the latter assumption on releasesmay not hold in the most general case, it is a rea-sonable one for the questions game domain.
If aparticipant does not have the floor, the system as-sumes they perform a take action if speaking or anull action otherwise.
The system also assumesthat the floor intentions are fully reflected by thefloor actions, i.e., a participant intends to have thefloor if and only if she performs a hold or take ac-tion.
Floor states are updated based on the joint,coordinated floor actions of all participants, as de-scribed earlier.Turn-taking decisions are based on a simpleheuristic policy.
The system takes the floor if (1)the floor is being released to it or (2) a participantreleases the floor to someone else, but no oneclaims the floor for a preset duration.
In most cas-es, this duration is set to 3.5 seconds.
However, ifthe floor is released to someone else after the sys-tem is interrupted during a question dialog act, thesystem will try to quickly reacquire the floorshould no one else be speaking, so as to finish orrestate its question.
The waiting duration is set inthe latter case to 500 milliseconds.
If after 500ms,when the system tries to take the floor another con-flict occurs (followed by a floor release to someoneelse), the waiting duration is increased again to 3.5seconds.
Finally, if a third consecutive conflict oc-100curs when the system tries to acquire the floor, thewaiting duration is set to a longer, 20 seconds.The system releases the floor at the end of itsown outputs.
In addition, it has to decide whether itshould release the floor when a user performs atake action (i.e.
barges in) while the system isspeaking.
The heuristic policy currently imple-mented by the system releases the floor only forbarge-ins occurring during question dialog acts.Finally, the behavioral models employ policiesinformed by the existing literature on the role ofgaze in regulating turn taking.
In particular, thesystem?s gaze is directed towards the speaking par-ticipant, or, if the system is speaking, towards theaddressees of the system?s utterance.
During si-lences, the system?s gaze is directed towards theparticipants that the floor is being released to.The models and policies described above repre-sent a starting point for inference and action, con-structed to enable data collection and an initialevaluation in this domain.
We are working to up-date the turn-taking architecture with more sophis-ticated evidential reasoning and utility-theoreticdecision making.
Nevertheless, when harnessed asan ensemble within the turn-taking approach thatwe have described, the current procedures providefor an array of complex, multiparty turn-takingbehaviors.
For instance, the system can addresseach participant individually or all participants as agroup via controlling the orientation of its headpose.
When participants talk amongst themselves,the system can monitor their exchanges and waituntil the floor is being released back to it.
If ananswer is heard during such a side conversation(e.g., one participant suggests an answer to an-other), the system highlights it on the screen (seeFigure 2).
If a significant pause is detected duringthis side conversation, the avatar takes the floorand the initiative, e.g., ?So, what do you think isthe correct answer??
Once a participant providesan answer, the system seeks confirmation fromanother participant before moving on.
In somecases, the avatar passes back the floor and seeksconfirmation non-verbally, by simply turning to-wards another participant and raising its eyebrows.The system can try to require the floor immediatelyafter being interrupted, but can also back off, giv-ing the participants a chance to finish a side con-versation, if successive floor conflicts occur.
Sam-ple interactions can be viewed in Appendix C andonline (Situated Interaction, 2011).4.3.
Study DesignThe user study was conducted in a usability laband involved a total of 60 participants recruited aspairs of people from the general population whopreviously knew one another (30 male and 30 fe-male, with ages between 18 and 61).
The studywas structured in 15 one-hour sessions, with eachsession involving four participants, i.e., two pairsof two previously acquainted participants.
In eachsession, we formed all possible subgroups of sizetwo (6 subgroups) and of size three (4 subgroups)with the four participants.
Each subgroup playedone game with the system.
This setup allowed us tocollect a large set of multiparty interactions underdiverse conditions (e.g., all-male, all-female,mixed-gender groups; groups where people werepreviously acquainted vs. not, etc.).
At the end ofeach session, participants filled in a subjective as-sessment survey.4.4.
Corpus, Annotations, and Cost AssessmentIn total, 150 multiparty interactions were collected:90 with two participants and the system, and 60with three participants and the system.
A profes-sional annotator transcribed the utterances detectedby the system at runtime, and labeled them withsource and addressee information.The system was noted to commit several typesof turn-taking errors.
To expand the error analysisbeyond occurrence statistics and to characterize theimpact of various types of errors, we conducted afollow-up study.
In this second study, a set of ad-ditional participants were recruited to review vid-eos of interactions from the first study and asked to(1) identify the turn-taking errors committed by thesystem and (2) to assess the costliness of the erroron a five-point scale.A total of 9 interactions (5 with two participantsand system; 4 with three participants and system)were randomly sampled from the collected corpus,while ensuring that each turn-taking outcome ofinterest (discussed in Section 5 and summarized inTable 1) was sufficiently represented.
Nine partici-pants were recruited via an email request to em-ployees at our organization.
Each participant re-viewed three interactions, and each interaction wasreviewed by three different participants.
Prior tothe experiment, each of the annotators received abrief review of the turn-taking process in human-human interaction.
Next, they used a multimodal101annotation tool that we created to review the inter-action videos.
As each video played, the annotatorpushed a button at each point they believed that thesystem had committed a turn-taking error.
In a se-cond pass, each annotator was asked to review theerrors that they had previously identified and toassess the relative cost of the error, on a scale from0 (?no error?)
to 5 (?worst error?).
In a final step,the authors manually aligned each identified turn-taking error with a turn-taking decision made bythe system and its corresponding outcome.5.
EvaluationWe now focus on the various types of turn-takingerrors, the outcomes that these errors lead to, andthe costs assessed for the outcomes.
We begin byfocusing on diarization challenges described inSection 5.1.
In Sections 5.2 and 5.3, we review theaccuracy of the system?s turn-taking inferences anddecisions, and their corresponding outcomes.
Fi-nally, in Section 5.4, we turn our attention to thesubjective assessment results obtained via the post-experiment user survey.Before diving into the details, we note that weeliminated 7 out of the total 150 interactions fromthe analysis due to significant problems withacoustic echo cancellation.
In the remaining 143interactions, we also identified and eliminated 24utterances in the transitional engagement stages,e.g., when the users were not ready or properlysetup in front of the system.
The analysis below isbased on the remaining 4379 utterances.5.1.
DiarizationThe system uses a voice activity detector whichleverages energy, acoustics and grammar to detectspoken utterances.
Our experiments indicate thatthis type of black-box solution can make diariza-tion errors, especially in multiparty settings wherepeople may speak simultaneously, at a fast pace,and address each other with language outside thesystem?s grammar.
Results show that only 72% ofthe detected segments contain speech from a singleparticipant.
Another 2% contain background noisesincorrectly identified as speech.
Most often theseare instances where the system heard itself due toacoustic echo-cancellation problems; the ratiogrows to about 6% among all utterances detectedwhile the system is speaking.
The remaining 26%contain overlapping or successive utterances frommultiple speakers.
Inspection of the data revealsthat some utterances spoken softly by participantswere not detected and that segmentation boundaryerrors are also sometimes present.
While such er-rors may be mitigated by inferences at higher lev-els in the turn-taking model, they can significantlyinfluence the system?s ability to track the conver-sational dynamics and make appropriate turn-taking decisions.
We plan to pursue more robustaudiovisual diarization methods that integratesound localization as detected by a microphonearray, along with higher-level interaction context.5.2.
Take versus NullWe now turn our attention to the system?s floorcontrol decisions.
The analysis below is based onthe utterances and segmentation detected by thesystem at runtime.
We note that a more preciseanalysis could be conducted with a ground truthsegmentation of utterances.
Utterances detected bythe system can be classified into three categories,based on their relationship to system outputs, asshown in Figure 3: overlaps, which start and endduring a system?s output, continuers, which beginduring but finish after a system output has ended,and responses, which do not overlap anywhere.With the current policy, the system chooseswhether it should take the floor following eachdetected continuer and response.
The dataset con-tains a total of 3265 such instances.
The system?sdecision at each of these points hinges on the re-sults of its inferences about the participants?
flooractions, and thus of inferences about the addresseesof each utterance.
Table 1 displays a tabulation ofthe release actions performed by the participantsversus the actions identified by the system.
Therelease actions are determined from labels assignedmanually by the professional annotator.
Recall thatwe make an assumption that the release is towardsthe set of addressees of an utterance.
For segmentsthat were labeled as containing multiple utterances,the release is made to the addressee of the last ut-terance.
The last row in Table 1 corresponds tobackground noises and system speech incorrectlyFigure 3.
Schematic of different classes of overlap.turn-initial overlap (TIO) response continuer turn-internal overlapSystem speechDetected utterancesActual utterances102Inferred Addressee / Release ActionTo System  Not to System2063 (64%)277 (9%)Take + Verbal Contribution1796 (87%)Delayed System Take59 (21%)ToSystemTurn-initialoverlap182 (10%)[17 Echo]0.25 No turn-initialoverlap1614 (90%)0.00Take+Non-verbalRelease267 (13%)0.42 Turn-initialoverlap22 (37%)[0 Echo]1.83No turn-initialoverlap37 (63%)2.58OtherTakes218 (79%)0.85305 (9%)588 (18%)Take + Verbal Contribution242 (79%)Delayed System Take131 (22%)LabeledAddressee/ ReleaseActionNot toSystemTurn-initialoverlap101 (42%)[0 Echo]1.76 No turn-initialoverlap141 (58%)0.42Take+Non-verbalRelease63 (21%)0.00Turn-initialoverlap38 (29%)[3 Echo]0.55No turn-initialoverlap93 (71%)0.00OtherTakes457 (78%)0.0310 (<1%)22 (<1%)Take + Verbal Contribution9 (90%)Delayed System Take13 (59%)BackgroundTurn-initialoverlap3 (33%)[0 Echo]No turn-initialoverlap6 (67%)Take+Non-verbalRelease1 (10%)Turn-initialoverlap7 (54%)[4 Echo]No turn-initialoverlap6 (46%)OtherTakes9 (41%)Table 1.
Decisions to take floor (vs. null), outcomes, and estimated costs (bar graph with confidence intervals).Echo denotes cases where the turn initial overlap is created by utterances where the system hears itself becauseof errors with echo cancellation.identified as utterances.On the task of detecting addressees, and thusfloor release actions, the results show an error rateof 18%, including 305 false-positives (erroneousdetections) and 277 false-negatives (missed detec-tions) of floor releases to the system.
These errorsinfluence the quality of turn taking in a variety ofways and underscore the need for more robust in-ferences about speech source and target, and floorrelease actions.
We believe that more sophisticatedmodels learned from audiovisual information (e.g.,prosody, head and body pose, etc.)
and attributesof the interaction context (e.g., who spoke last,where is the system looking, etc.)
can reduce errorssignificantly.Table 1 indicates that in 305 (9%) of the casesthe system incorrectly inferred that the floor wasbeing released to it.
In 79% of these cases, the sys-tem took the floor and produced a verbal contribu-tion.
Since the floor was not released to the system,such errors can lead to significant turn-taking prob-lems, which often manifest as floor conflictsmarked by turn-initial overlaps, where a partici-pant and the system start speaking around the sametime (see Figure 3).
Operationally, we define turn-initial overlaps as all detected overlaps with anactual onset of less than 300 milliseconds from thebeginning of the system?s utterance (see discussionin Appendix A); the other overlaps are dubbedturn-internal.
We note that the time at which anoverlap is detected by the system lags behind theactual onset of the utterance by an average of about700 milliseconds, due to core latencies in our audioand speech processing pipeline.
Accounting forthese computational lags, and others arising at dif-ferent places in processing pipelines, raise chal-lenges for turn taking in spoken dialog systems.42% of the verbal takes performed incorrectlyby the system led to turn-initial overlaps.
This isnot surprising, as the system starts speaking whenthe floor was not released to it.
In some of thesecases the same participant continues (e.g., diariza-tion errors incorrectly segmented the utterance), orsomeone else starts speaking.
The cost assessmentexperiment confirmed the impact of these errors ?the average estimated cost was 1.76.
If no turn-initial overlap occurred after the system incorrectlytook the floor, the average cost was 0.42.
Clearly103floor conflicts come with a cost.
The specific costassessments we obtained are perhaps influenced toa degree by the role of game mediator played bythe system.
With this role, taking the floor in caseswhen the system was not addressed is perhaps notas costly as it might be in other domains.Note that 182 turn-initial overlaps also occurwhen the system takes the floor after correctlyidentifying that the floor was released to it (upper-left quadrant in Table 1).
17 of them are created bythe system hearing itself as it starts speaking, dueto errors in acoustic echo cancellation; these in-stances are marked Echo in Table 1.
While the rel-ative percentage of turn-initial overlaps is smallerafter a floor release to the system (~10%), the ma-jority of all turn-initial overlaps (shaded cells inTable 1) occur in this context, because of the largerincidence of the situation.
Often, these utterancescontain an immediate answer or a short confirma-tion from another participant.
The cost of theseturn-initial overlaps is also much lower: 0.25 ver-sus 1.76 (again, the cost structure is probably sen-sitive to details of the domain).We believe the turn-initial overlaps that occurwhen the floor is released to the system can be ex-plained in part by the interpretation of the system?sshort delay in responding (per processing) as a sig-nal that the system is not taking the floor, leadingother participants to take initiative.
As another fac-tor, turn taking is a mixed-initiative process, andother participants might vie for the floor and issuetheir own contributions immediately after an an-swer directed to the system.
These observationsbring to the fore two questions: (1) how can weminimize the number of turn-initial overlaps, and(2) how can the system gracefully handle suchoverlaps once they occur?One approach to minimizing turn-initial over-laps is to reduce the system?s response delays viafaster processing or via the use of predictive mod-els to anticipate the end of turns (e.g.
Ferrer et al,2003; Schlangen, 2006; Raux and Eskenazi, 2008;Skantze and Schlangen, 2009).
Multiparty settingsrequire methods for forecasting not only when acurrent speaker will finish, but also whether anyparticipant will try to take (or release) the floorwithin a small window of time in the future, i.e.,accurately modeling all floor intentions.
Our turn-taking framework includes components for repre-senting and modeling floor intentions, but these arenot used in the current system.
We believe there ispromise in learning models to predict floor inten-tions and the timing of ends of utterances from in-teraction data.
The availability of such predictionscan fuel additional turn-taking strategies and alsopave the way to more graceful handling of turn-initial overlaps after they occur.
For instance, if thesystem can anticipate that someone else might startspeaking, it might still decide to take the floor butit might start with a filler, e.g., ?So [pause] Whatdo you think??
constructing a natural opportunityfor resolving a potential conflict after ?So?
Weplan to investigate the use of decision-theoreticmethods to anticipate and resolve such conflicts byintroducing and modulating an array of strategies,including the use of fillers, restarts, and acknowl-edgment gestures.In 21% of the 305 incorrectly detected floor re-leases to the system, our system immediately per-formed a non-verbal floor release to another par-ticipant by turning the avatar?s face towards themand raising its eyebrows (Take + Non-verbal Release inTable 1).
These situations are not costly, as thesystem?s action does not interrupt the flow of theconversation.
Indeed they were never penalized inthe cost assessment experiment that we conducted.However, the same action, performed when thefloor is actually released to the system (13% of2063 cases), has the potential to create problems ifnot properly recognized by the targeted participantas a floor release by the system; the average costassessed in this case was 0.42.The right-hand column in Table 1 shows caseswhere the system detected that the floor was notreleased to it.
In these cases, the system waits (per-forms null) for a specified duration.
The cost as-sessment indicates that waiting in this situation isoverall costly, and the cost depends on the ultimateoutcome.
If no one else takes the floor, the systemwill eventually do so (Delayed System Take cases inTable 1).
In some of these cases, turn-initial over-laps also occur.
The 277 cases in which the systemfails to detect that the floor was in fact released toit lead to no immediate response from the system.In these cases the system can be perceived as unre-sponsive and the participants eventually repeatthemselves.
We believe that performance can beimproved with the use of an ongoing decision-theoretic analysis that continuously reassesses thesituation while the system waits.
Such an analysiswould consider the delay, floor holder?s previousactions, inferences about participants?
floor inten-104tions, and cost-benefit tradeoffs of different flooractions.5.3.
Release versus HoldWe now turn our attention to the system?s deci-sions to release the floor.
Recall that, according tothe current policy, the system performs a floor holdwhile it is speaking and a floor release at the end ofits outputs.
In addition, if an overlap (i.e., barge-in)was detected during question dialog acts, the sys-tem performed a floor release immediately, inter-rupting its own output and allowing for the userbarge-in.Since such barge-ins were allowed only duringthe question dialog acts, as Table 2 shows, the cur-rent policy leads to an abundance of cases in whichthe system performs hold when an overlap is de-tected.
Some of these cases are continuers: theoverlap only happens at the very end of the sys-tem?s output.
These cases do not create significantturn-taking problems, as the floor still transitions tothe participant relatively quickly (the system re-leases at the end of its output).
However, in a sig-nificant number of cases the system appears to ig-nore the participants (shaded cells in Table 2).About three quarters of these overlaps occur whilethe system is providing an explanation after an in-correct answer.
Observations of the data indicatethat in these cases participants may discuss or givetheir opinion on the answer or some aspect of thesystem?s explanation, while ignoring the system asit blindly continues the explanation.We have separated in Table 2 turn-initial fromturn-internal overlaps.
The two types of overlapsreflect different phenomena.
As we have discussed,turn-initial overlaps mark floor conflicts, and vari-ous strategies could be used to negotiate such con-flicts (e.g., Yang and Heeman, 2010).
In contrast,turn-internal overlaps may reflect efforts by otherparticipants to take the floor, or might simply bebackchannels, laughter, exclamations or other lexi-cal or non-lexical events that do not mark a claimfor the floor.
Making appropriate floor control de-cisions in this case will require models for reliablydistinguishing between the two, i.e., between thetake or null floor actions of the participants.
This isan especially challenging inference problem asdecisions need to be made as early as possible afterthe onset of an utterance.We note the relatively large incidence of failuresin echo cancellation in our microphone array.
Onthe utterances marked Echo in Table 2, the systemheard itself and thought a user was speaking.
Webelieve these failures could be significantly re-duced with better acoustic echo cancellation.5.4.
Subjective AssessmentFinally, we present results from a subjective as-sessment of the system by participants, based on apost-experiment survey.
The survey included sev-eral 7-point Likert scale questions related to turntaking, which are displayed in Figure 4, togetherwith the mean user responses and the correspond-ing 95% confidence intervals.
Generally, partici-pants rated the system?s turn-taking abilities fa-vorably, with scores around 4.5-5.
No statisticallysignificant differences were detected in assess-ments across the participant?s gender or previousfamiliarity with speech recognition systems.
Wealso note that a parallel human?human interactionstudy would help us characterize better the sys-tem?s performance relative to human dialog.I knew when the avatarwas addressing meI knew when the avatarwas addressing othersI knew whom the avatarwas talking toI knew when it wasmy time to speakThe avatar knew whenI was speaking to itThe avatar knew whenI was speaking to othersThe avatar knew whenit was its time to speakThe avatar interruptedus at the wrong timeThe avatar waited toolong before taking its turnI felt left out or excludedduring the gamesThe interactionwas naturalI enjoyed playingthe gameFigure 4.
Results of subjective assessments.5.15.04.84.64.94.84.03.23.02.04.05.5[ lower is better ][ lower is better ][ lower is better ]1 - Never 7 - Always 2 3 4 5 6Action performed by system when overlap detectedHOLD RELEASE315 (23%)TurnInitialOverlap285 (90%)[14 Echo]Continuer30 (10%)[3 Echo]43 (3%)[7 Echo]968 (69%)OverlapTypeTurnInternalOverlap828 (86%)[44 Echo]Continuer140 (14%)[7 Echo]73 (5%)[13 Echo]Table 2.
Decisions to release floor (vs. hold).105In addition to the survey questions, participantswere invited to describe in their own words whatthey liked best and the first thing they wouldchange about the system.
21 of the 60 participantsmentioned aspects of multiparty interaction in the?what I liked best?
category, such as the system?sability to track the speaking participant and addresspeople individually.
Other frequent answers to thisquestion called out the overall experience with theintegrative intelligence of the system (15 answers),the fun/educational nature of the game (14), andaspects of speech recognition (11).
On the ?firstthing you would change,?
the majority of answers(32) included references to shortcomings in render-ing the avatar, while 13 answers included refer-ences to problematic aspects of the multiparty turntaking.
Other answers included task domain sug-gestions (6) and comments about improving thespeech recognition (5).
A sampling of answers ispresented in Appendix B.6.
Summary and Future WorkWe reported on a user study of a multiparty turn-taking model.
Objective measures of system per-formance and subjective assessments by partici-pants indicate that the approach can enable suc-cessful multiparty turn taking in the questionsgame domain.
When the correct turn-taking deci-sions are made, the multiparty interaction is seam-less and resembles human-human collaboration.The conversations exhibit fluid exchanges amongpeople and the system, including mixed-initiative,multiparty floor control, fluid back offs and re-starts, natural use of non-verbal cues, such as par-ticipants?
utterances being triggered by a turn ofthe avatar?s head or a lift of the eyebrows.
In con-trast, turn-taking failures lead to a striking loss offluidity and a qualitative jump out of an engagedprocess, where the system rapidly shifts from acollaborating participant into a distant and uncoor-dinated appliance.The results we have discussed are based on aninitial set of coarse perceptual and decision-makingmodels and thus reflect an initial baseline; there issignificant room for improvements.
A careful dis-section of the outcomes demonstrates the subtletiesof multiparty turn taking and highlights severaldirections we plan to address in future work.
First,our experiments have highlighted the importanceof accurate diarization in multiparty dialog set-tings.
Minimizing errors requires rich perceptualand inferential competencies, leveraging audiovis-ual evidence, general patterns of human discourse,and attributes of the task-specific goals and con-text.
We plan to explore the use of machine learn-ing procedures for constructing predictive modelsthat harness richer streams of evidence to identifyand segment utterances, and to make inferencesabout their sources and targets, and the floor state,actions and intentions of all participants.
Betterturn-taking decisions can also be supported by in-ferences about social norms, roles and dynamics,pace of interaction, and engagement.Although handcrafted turn-taking policies wenta long way in this domain, enabling more generalmultiparty turn taking will require continuous in-ference and decision making under uncertainty thatconsiders subtleties of intention and timing, andthat takes into consideration tradeoffs associatedwith different courses of actions.
We foresee thevalue of extending the current decision modelswith richer temporal reasoning for performing suchongoing analyses.
Challenges include a more in-depth understanding of the cost of different typesof turn-taking errors; the development of a widerarray of graded strategies and behaviors for taking,releasing, or holding the floor, and for gracefullynegotiating floor conflicts; and finally, the abilityto reason about uncertainty in the world as well asin the system?s own processing delays in order toresolve tradeoffs between taking timely action anddelaying for additional evidence that promises toenhance the accuracies of decisions.Much also remains to be done with the corre-sponding generation of subtle verbal and non-verbal cues for enhanced signaling and naturalnessof conversation, including the use of fillers, re-starts, backchannels, and envelope feedback.
Weare excited about tackling these and other chal-lenges on the path to fielding systems that can en-gage in fluid multiparty dialog.AcknowledgmentsWe thank Anne Loomis Thompson, Ece Kamar,Qin Cai, Cha Zhang, and Zicheng Liu for theircontributions.
We also thank our colleagues whoparticipated in pilot experiments for the user study.106ReferencesBennewitz, M., Faber, F., Joho, D., Schreiber, M., andBehnke, S., 2005.
Integrating vision and speech forConversations with Multiple Persons, in Proc.
ofIROS?05Bohus, D., and Horvitz, E., 2009.
Dialog in the Open-World: Platform and Applications, in Proc ICMI?09.Bohus, D., and Horvitz, E., 2010a.
ComputationalModels for Multiparty Turn Taking, MicrosoftResearch Technical Report MSR-TR 2010-115.Bohus, D., and Horvitz, E., 2010b.
FacilitatingMultiparty Dialog with Gaze, Gesture and Speech, inProc ICMI?10.Duncan, S. 1972.
Some Signals and Rules for TakingSpeaking Turns in Conversation, Journal ofPersonality and Social Psychology 23, 283-292.Ferrer, L., Shriberg, E., and Stolcke, A.
2003.
AProsody-Based Approach to End-Of-UtteranceDetection That Does Not Require SpeechRecognition, in Proc.
ICASSP?03.Goodwin, C. 1980.
Restarts, pauses and theachievement of mutual gaze at turn-beginning,Sociological Inquiry, 50(3-4).Hjalmarsson, A., 2011.
The additive effect of turn-taking cues in human and synthetic voice, in SpeechCommunication, vol.
53, issue 1.Kronlid, F., 2006.
Turn Taking for Artificial Conversa-tional Agents, in Cooperative Information Agents X,LNAI 4149, Springer-VerlagMatsusaka, Y., Fujie, S., and Kobayashi, T., 2001.Modeling of conversational strategy for the robotparticipating in the group conversation, in Proc ofEuroSpeech?01.Raux, A., and Eskenazi, M. 2008.
Optimizing endp-ointing thresholds using dialogue features in a spokendialogue system, in Proc of SIGdial-2008.Raux, A. and Eskenazi, M., 2009.
A Finite-State Turn-Taking Model for Spoken Dialog Systems, in Proc.HLT?09.Sacks, H., Schegloff.
E., and Jefferson, G. 1974.
Asimplest systematics for the organization of turn-taking in conversation, Language, 50, 696-735.Schegloff, E. 2000.
Overlapping talk and theorganization of turn-taking in conversation,Language in Society, 29, 1-63.Schlangen, D., 2006.
From reaction to prediction:Experiments with computational models of turn-taking, in Proc.
Interspeech?06, Panel on Prosody ofDialogue Acts and Turn-TakingSelfridge, E., and Heeman, P., 2010.
Importance-DrivenTurn-Bidding for Spoken Dialogue Systems, in Proc.of ACL-2010, Uppsala, SwedenSkantze, G., and Schlangen, D., 2009.
Incrementaldialogue processing in a micro-domain, in Proc.
ofEACL-2009.Situated Interaction, 2011.
Project web page:http://research.microsoft.com/~dbohus/si.htmlThorisson, K.R.
2002.
Natural Turn-Taking Needs NoManual: Computational Theory and Model, fromPerceptions to Action, Multimodality in Languageand Speech Systems, Kluwer Academic Publishers.Traum, D., 1994.
A Computational Theory of Ground-ing in Natural Language Conversation, TR-545, U.of Rochester.Traum, D., and Rickel, J., 2002.
Embodied Agents forMulti-party Dialogue in Immersive Virtual World, inProc.
AAMAS?02.Wiemann, J., and Knapp, M., 1975.
Turn-taking inconversation, Journal of Communication, 25, 75-92.Yang, F., and Heeman, P., 2010.
Initiative Conflicts inTask-Oriented Dialogue, in Computer, Speech andLanguage, vol.
24, issue 2.107Appendix A.
Details on derivation of operational definition of turn-initial overlaps.As described in Section 5.2, we operationally define turn-initial overlaps asdetected user utterances that have an actual onset of less than 0.3 seconds fromthe beginning of a system utterance.
Figure 5 shows the histogram of the onsettime for user speech with respect to system utterances (start of system utter-ance is at 0 seconds), for overlapping utterances, where this onset is between -2 and +5 seconds.
If multiple user utterances overlap with a single systemutterance, only the first user utterance, i.e.
the first overlap, is considered incomputing this histogram.
As Figure 5 shows, the onset distribution has a bi-modal character.
We believe that the two modes may reflect two differentphenomena in terms of the floor transition.
The early-onset mode correspondsto situations in which a user starts to speak right around (before or immedi-ately after) the time the system also started speaking; this indicates a situationwhere there is contention for the floor and the system cannot assume it hassuccessfully acquired the floor.
In contrast, user utterances starting at latertimes represent cases where the floor did first transition to the system and theuser is aware of this transition.
In producing an utterance the user is attempt-ing to barge-in and take the floor back from the system (unless the user utter-ance is a backchannel).
The threshold of 0.3 seconds on the onset for turn-initial overlaps was selected based on the shape of this distribution.-2 -1 0 1 2 3 4 50%2%4%6%8%10%12%onset (seconds)Figure 5.
Histogram of onsets for firstoverlaps.Appendix B.
Sample responses from surveyCategory # Example commentPlease describe what you liked best about interacting with the systemMultipartyinteractionprowess21- I enjoyed how it recognized who was speaking and actually looked at you- I liked how the avatar tracked the players; how it understood speech- It was great to play a game where you don?t have to use your hands, just your mind.
The way the avatar would recognizeposition of who spoke was nice.
The blinking action at the avatar made her more realistic but she needed more than her face.- That it would look right at you and ask a question- I liked how the avatar made eye contact with each person playing the game?Overallexperiencewith system15- It was very new and thus it was fun.
I don?t play computer games often and I did enjoy this one.
Which is rare for me.- It was different than any other trivia game I?ve played in the past- I think this is a great way for a human to interact with a computer?- It?s cool interacting with the avatar?Rewardingtask 14- I liked the challenge of the questions- It?s a great fun way to improve knowledge- New experience that I found enjoyable.
I enjoyed thinking about choices and having an interaction with the avatarSpeech andlanguage 11- Voice recognition was fairly accurate, no need to repeat- The ability of it to understand what I was saying.
Plus it?s pretty cool.- I liked it because it wasn?t really hard for the system to understand what we were saying.
Even though we have an accent.If there was one thing you could change about this system, what would it be?Avatarrendering 32- The avatar should be more friendly ?
she came off a bit austere ?
she didn?t smile even when we got 5 out of 6 questionsright, it was only ?pretty good?.- The way it moves its lips needs to be better- The avatar seemed a little to ?stiff?.
It needs to be more natural in movement and speech- The face was a ?warmer face?.
Smiling perhaps.Multipartyfailures 13- Extend the time limit when questions haven?t been fully answered.
It would sometimes say we were correct or false before wehad confirmed our answer- Sometimes it skips and pauses and making it difficult to understand- Consistency in waiting and asking player to confirm answer instead of overhearing conversations and choosing an answeritselfTask domain  6 - It would be cool if it could remember our names.
Also, 6 questions was a little short.
I think 8 or 10 questions would be better.
- I think the questions should be more pop culture relatedSpeech andlanguage 5- I enjoyed her.
I would like her to understand a little easier.
We had to repeat answers on occasion which wasn?t too bad.Overall I really liked it.
Perhaps it could ask our names and call us by name when speaking to us108Appendix C. Excerpts from interactions with the system.
We present and discuss two segments from an interactionwith the questions game system.
The segments illustrate challenges for diarization, tracking conversational dynamics (e.g.
infer-ring speech source, target, floor actions, etc.)
and decisions making for multiparty turn taking.
The video for this entire interac-tion, as well as an additional interaction are available online at (Situated Interaction, 2011)1 S?P1 Hi.
Would you like to play a questionsgame?1 S?P16 In the USA hospitals have a redcross, what sign do they use in theArab World?This segment starts with the system asking a question2 P16?P16 Hospitals have a red cross what signdo they user in the Arab World?3 P17?S Solid blue circle4 S?P16 Is that correct?5 P17?S No6 S?P16,P17 So what?s the correct answer?7 P17?P16 Red crescent moon8 P17?S Red crescent moon9 S?P16 [turns to P16, lifts eyebrows]10 P16?S Yes11 S?P16,P17 That?s right.
Okay, so far you?re twoout of three [?
]P16 echoes back the question to himself in a low voice.
The system detects this utterance, cor-rectly inferred that the floor is not being released to it (the utterance leads to a non-understanding), and the system turns towards the other participant, which produces ?Solid bluecircle?
(3).
The utterance and corresponding floor release to the system are correctly understood,and the system turns back to P16 for confirmation.
Next, the system detects an utterance from P17,who has changed her mind (5).
The system takes the floor and asks both participants (by directinggaze towards them) what the correct answer is (6), and releases the floor to both participants o.P17 speaks first towards P16 (7) ?
this utterance is low energy and not detected by the system, andthen towards the system (8).
The system recognizes the floor is released to it after (8), takes thefloor and directs it back towards P16 non-verbally, by turning towards him and raising eyebrows.P16 responds immediately, and the system takes the floor and continues to the next question.17 S?P16,P17 Next question [?]
The system then moves on to the next question2 P17?S Hi3 P16?S Hello4 P17?S Yes5 P18?S Yes6 S?P16 Here is the first question.
The study ofthe size and shape of the skull as anindicator of a person?s character isknown as: telepathy, graphology, orphrenology?7 P16?S I?ll go?
Graphology8 S?P17 What do you think?9 P17?P17 graph-10 P16?P17 That doesn?t sound right but11 P17?S Phrenology12 S?P16,P17 I?m sorry but that?s incorrect.
Popularduring the 19th century but not dis-credited, phrenology is a theory that aperson?s character and intelligencecan be determined by the size andshape of their skull.13 P16?P17 phrenology14 P16?P17 She did not wait for your answer15 P17?P16 I guess not16 P16?S You didn?t wait for my answerImmediately after the system?s greeting, the two participants also say ?Hi?
and ?Hello?.
Theirgreetings are detected as a single utterance by the system which partially overlaps with the be-ginning of the system?s follow-up question (overlaps are underlined in the examples to the left).According to the current policy, the system does not release the floor on this interruption andcontinues with its question.
The ?Yes?
responses from (4) and (5) are overlapping with each otherand are detected by the system as a single utterance which is correctly decoded.The system correctly infers the floor was addressed to it, and therefore takes the floor and pro-duces the first questionGiven the pause between ?I?ll go?
and ?Graphology?
the response in (7) is in fact detected as twoseparate utterances by the system.
The first part is non-understood, hence assumed addressed toother and the system does not take the floor.
The system correctly understands and takes thefloor after ?Graphology?, and moves to ask for confirmation (8).
Next, while the system asks theother participant for confirmation, due to imperfections in echo cancellation, the system hears anoise at the beginning of its utterance, but ignores the detected ?barge-in?.P17 softly says to herself ?graph-?.
This utterance is not actually detected by the system.Next, the system misunderstands the utterance in (10) as ?that sounds right?
and incorrectly infersthat the utterance was addressed to it.
It therefore takes the floor and continues.
This leads to aturn-initial overlap with the ?Phrenology?
utterance immediately produced by P17 (11)The follow-up utterances and discussion between participants (13-16) overlap with portions of thesystem?s explanation.
They indicate the high cost of the misunderstanding and of the system?sincorrect inference and decision to take the floor (admonished by the user in (16) n, as well asthe shortcomings of the current policy to not release the floor for barge-ins detected during expla-nations.
This example highlights the need for more robust inferences, but also better policies forreleasing back the floor and for machinery that would allow the system to gracefully backing fromdetected floor conflicts.P arrow showsdirection ofattentionP P has floorP P is speakingP P is anaddresseen In the first segment, while the system isspeaking to both participants (12), P17 leansin as she produces utterance (16)o In the second segment, the system re-leases the floor to both participants afterproducing (6)Illustrations of conversational sceneanalysis performed by the system in real-time, at runtime.109
