Predicting the Semantic Orientation of AdjectivesVas i le ios  Hatz ivass i log lou  and Kath leen  R .
McKeownDepar tment  of Computer  Science450 Computer  Science Bui ld ingCo lumbia  Univers i tyNew York, N.Y. 10027, USA{vh, kathy)?cs, columbia, eduAbstractWe identify and validate from a large cor-pus constraints from conjunctions on thepositive or negative semantic orientationof the conjoined adjectives.
A log-linearregression model uses these constraints topredict whether conjoined adjectives areof same or different orientations, achiev-ing 82% accuracy in this task when eachconjunction is considered independently.Combining the constraints across many ad-jectives, a clustering algorithm separatesthe adjectives into groups of different orien-tations, and finally, adjectives are labeledpositive or negative.
Evaluations on realdata and simulation experiments indicatehigh levels of performance: classificationprecision is more than 90% for adjectivesthat occur in a modest number of conjunc-tions in the corpus.1 In t roduct ionThe semantic orientation or polarity of a word indi-cates the direction the word deviates from the normfor its semantic group or lezical field (Lehrer, 1974).It also constrains the word's usage in the language(Lyons, 1977), due to its evaluative characteristics(Battistella, 1990).
For example, some nearly syn-onymous words differ in orientation because one im-plies desirability and the other does not (e.g., sim-ple versus simplisfic).
In linguistic constructs uchas conjunctions, which impose constraints on the se-mantic orientation of their arguments (Anscombreand Ducrot, 1983; Elhadad and McKeown, 1990),the choices of arguments and connective are mutu-ally constrained, as illustrated by:The tax proposal wassimple and well-received }simplistic but well-received*simplistic and well-receivedby the public.In addition, almost all antonyms have different se-mantic orientations3 If we know that two wordsrelate to the same property (for example, membersof the same scalar group such as hot and cold) buthave different orientations, we can usually infer thatthey are antonyms.
Given that semantically similarwords can be identified automatically on the basis ofdistributional properties and linguistic cues (Brownet al, 1992; Pereira et al, 1993; Hatzivassiloglou andMcKeown, 1993), identifying the semantic orienta-tion of words would allow a system to further refinethe retrieved semantic similarity relationships, ex-tracting antonyms.Unfortunately, dictionaries and similar sources(theusari, WordNet (Miller et al, 1990)) do not in-clude semantic orientation information.
2 Explicitlinks between antonyms and synonyms may also belacking, particularly when they depend on the do-main of discourse; for example, the opposition bear-bull appears only in stock market reports, where thetwo words take specialized meanings.In this paper, we present and evaluate a methodthat automatically retrieves emantic orientation i -formation using indirect information collected froma large corpus.
Because the method relies on the cor-pus, it extracts domain-dependent i formation andautomatically adapts to a new domain when the cor-pus is changed.
Our method achieves high preci-sion (more than 90%), and, while our focus to datehas been on adjectives, it can be directly applied toother word classes.
Ultimately, our goal is to use thismethod in a larger system to automatically identifyantonyms and distinguish near synonyms.2 Overv iew o f  Our  ApproachOur approach relies on an analysis of textual corporathat correlates linguistic features, or indicators, with1 Exceptions include a small number of terms that areboth negative from a pragmatic viewpoint and yet standin all antonymic relationship; such terms frequently lex-icalize two unwanted extremes, e.g., verbose-terse.2 Except implicitly, in the form of definitions and us-age examples.174semantic orientation.
While no direct indicators ofpositive or negative semantic orientation have beenproposed 3, we demonstrate that conjunctions be-tween adjectives provide indirect information aboutorientation.
For most connectives, the conjoined ad-jectives usually are of the same orientation: comparefair and legitimate and corrupt and brutal which ac-tually occur in our corpus, with ~fair and brutal and*corrupt and legitimate (or the other cross-productsof the above conjunctions) which are semanticallyanomalous.
The situation is reversed for but, whichusually connects two adjectives of different orienta-tions.The system identifies and uses this indirect infor-mation in the following stages:1.
All conjunctions of adjectives are extractedfrom the corpus along with relevant morpho-logical relations.2.
A log-linear egression model combines informa-tion from different conjunctions to determineif each two conjoined adjectives are of sameor different orientation.
The result is a graphwith hypothesized same- or different-orientationlinks between adjectives.3.
A clustering algorithm separates the adjectivesinto two subsets of different orientation.
Itplaces as many words of same orientation aspossible into the same subset.4.
The average frequencies in each group are com-pared and the group with the higher frequencyis labeled as positive.In the following sections, we first present he setof adjectives used for training and evaluation.
Wenext validate our hypothesis that conjunctions con-strain the orientation of conjoined adjectives andthen describe the remaining three steps of the algo-rithm.
After presenting our results and evaluation,we discuss simulation experiments that show howour method performs under different conditions ofsparseness of data.3 Data  Co l lec t ionFor our experiments, we use the 21 million word1987 Wall Street Journal corpus 4, automatically an-notated with part-of-speech tags using the PARTStagger (Church, 1988).In order to verify our hypothesis about the ori-entations of conjoined adjectives, and also to trainand evaluate our subsequent algorithms, we need a3Certain words inflected with negative affixes (suchas in- or un-) tend to be mostly negative, but this ruleapplies only to a fraction of the negative words.
Further-more, there are words so inflected which have positiveorientation, e.g., independent and unbiased.4Available form the ACL Data Collection Initiativeas CD ROM 1.Positive: adequate central clever famousintelligent remarkable reputedsensitive slender thrivingNegative: contagious drunken ignorant lankylistless primitive strident roublesomeunresolved unsuspectingFigure 1: Randomly selected adjectives with positiveand negative orientations.set of adjectives with predetermined orientation la-bels.
We constructed this set by taking all adjectivesappearing in our corpus 20 times or more, then re-moving adjectives that have no orientation.
Theseare typically members of groups of complementary,qualitative terms (Lyons, 1977), e.g., domestic ormedical.We then assigned an orientation label (either + or- )  to each adjective, using an evaluative approach.The criterion was whether the use of this adjectiveascribes in general a positive or negative quality tothe modified item, making it better or worse than asimilar unmodified item.
We were unable to reacha unique label out of context for several adjectiveswhich we removed from consideration; for example,cheap is positive if it is used as a synonym of in-expensive, but negative if it implies inferior quality.The operations of selecting adjectives and assigninglabels were performed before testing our conjunctionhypothesis or implementing any other algorithms, toavoid any influence on our labels.
The final set con-tained 1,336 adjectives (657 positive and 679 nega-tive terms).
Figure 1 shows randomly selected termsfrom this set.To further validate our set of labeled adjectives,we subsequently asked four people to independentlylabel a randomly drawn sample of 500 of theseadjectives.
They agreed with us that the posi-tive/negative concept applies to 89.15% of these ad-jectives on average.
For the adjectives where a pos-itive or negative label was assigned by both us andthe independent evaluators, the average agreementon the label was 97.38%.
The average inter-revieweragreement on labeled adjectives was 96.97%.
Theseresults are extremely significant statistically andcompare favorably with validation studies performedfor other tasks (e.g., sense disambiguation) in thepast.
They show that positive and negative orien-tation are objective properties that can be reliablydetermined by humans.To extract conjunctions between adjectives, weused a two-level finite-state grammar, which coverscomplex modification patterns and noun-adjectiveapposition.
Running this parser on the 21 mil-lion word corpus, we collected 13,426 conjunctionsof adjectives, expanding to a total of 15,431 con-joined adjective pairs.
After morphological trans-175Conjunction categoryConjunctiontypesanalyzedAll appositive and conjunctionsAll conjunctions 2,748All and conjunctions 2,294All or conjunctions 305All but conjunctions 214All attributive and conjunctions 1,077All predicative and conjunctions 86030% same-orientation(types)77.84%81.73%77.05%30.84%80.04%84.77%70.00%% same-orientation(tokens)72.39%78.07%60.97%25.94%76.82%84.54%63.64%P-Value(for types)< i ?
i0 -I~< 1 ?
10 -1~< 1 ?
10 -1~2.09.10 -:~< 1. i0 -16< 1. i0 -I~0.04277Table 1: Validation of our conjunction hypothesis.
The P-value is the probability that similarextreme results would have been obtained if same- and different-orientation conjunction types wereequally distributed.or moreactuallyformations, the remaining 15,048 conjunction tokensinvolve 9,296 distinct pairs of conjoined adjectives(types).
Each conjunction token is classified by theparser according to three variables: the conjunctionused (and, or, bu~, either-or, or neither-nor), thetype of modification (attributive, predicative, appos-itive, resultative), and the number of the modifiednoun (singular or plural).4 Validation of the ConjunctionHypothesisUsing the three attributes extracted by the parser,we constructed a cross-classification f the conjunc-tions in a three-way table.
We counted types and to-kens of each conjoined pair that had both membersin the set of pre-selected labeled adjectives discussedabove; 2,748 (29.56%) of all conjoined pairs (types)and 4,024 (26.74%) of all conjunction occurrences(tokens) met this criterion.
We augmented this ta-ble with marginal totals, arriving at 90 categories,each of which represents a triplet of attribute values,possibly with one or more "don't care" elements.We then measured the percentage of conjunctionsin each category with adjectives of same or differ-ent orientations.
Under the null hypothesis of sameproportions of adjective pairs (types) of same anddifferent orientation in a given category, the num-ber of same- or different-orientation pairs follows abinomial distribution with p = 0.5 (Conover, 1980).We show in Table 1 the results for several repre-sentative categories, and summarize all results be-low:?
Our conjunction hypothesis i validated overalland for almost all individual cases.
The resultsare extremely significant statistically, except fora few cases where the sample is small.?
Aside from the use of but with adjectives ofdifferent orientations, there are, rather surpris-ingly, small differences in the behavior of con-junctions between linguistic environments (asrepresented by the three attributes).
There area few exceptions, e.g., appositive and conjunc-tions modifying plural nouns are evenly splitbetween same and different orientation.
Butin these exceptional cases the sample is verysmall, and the observed behavior may be dueto chance.?
Further analysis of different-orientation pairs inconjunctions other than but shows that con-joined antonyms are far more frequent han ex-pected by chance, in agreement with (Justesonand Katz, 1991).5 P red ic t ion  o f  L ink  TypeThe analysis in the previous ection suggests a base-line method for classifying links between adjectives:since 77.84% of all links from conjunctions indicatesame orientation, we can achieve this level of perfor-mance by always guessing that a link is of the same-orientation type.
However, we can improve perfor-mance by noting that conjunctions using but exhibitthe opposite pattern, usually involving adjectives ofdifferent orientations.
Thus, a revised but still sim-ple rule predicts a different-orientation link if thetwo adjectives have been seen in a but conjunction,and a same-orientation li k otherwise, assuming thetwo adjectives were seen connected by at least oneconjunction.Morphological relationships between adjectives al-so play a role.
Adjectives related in form (e.g., ade-quate-inadequate or thoughtful-thoughtless) almostalways have different semantic orientations.
We im-plemented a morphological nalyzer which matchesadjectives related in this manner.
This process ishighly accurate, but unfortunately does not applyto many of the possible pairs: in our set of 1,336labeled adjectives (891,780 possible pairs), 102 pairsare morphologically related; among them, 99 are ofdifferent orientation, yielding 97.06% accuracy forthe morphology method.
This information isorthog-onal to that extracted from conjunctions: only 12of the 102 morphologically related pairs have beenobserved in conjunctions in our corpus.
Thus, we176PredictionmethodAlways predictsame orientationBut ruleLog-linear modelMorphologyused?NoYesNoYesAccuracy on reportedsame-orientation li ks77.84%78.18%81.81%82.20%NoYes81.53%82.00%Accuracy on reporteddifferent-orientation links97.06%69.16%78.16%73.70%82.44%Table 2: Accuracy of several ink prediction models.Overallaccuracy77.84%78.86%80.82%81.75%80.97%82.05%add to the predictions made from conjunctions thedifferent-orientation links suggested by morphologi-cal relationships.We improve the accuracy of classifying links de-rived from conjunctions as same or different orienta-tion with a log-linear egression model (Santner andDuffy, 1989), exploiting the differences between thevarious conjunction categories.
This is a generalizedlinear model (McCullagh and Nelder, 1989) with alinear predictor= wWxwhere x is the vector of the observed counts in thevarious conjunction categories for the particular ad-jective pair we try to classify and w is a vector ofweights to be learned during training.
The responsey is non-linearly related to r/ through the inverselogit function,e0Y= l q-e"Note that y E (0, 1), with each of these endpointsassociated with one of the possible outcomes.We have 90 possible predictor variables, 42 ofwhich are linearly independent.
Since using all the42 independent predictors invites overfitting (Dudaand Hart, 1973), we have investigated subsets of thefull log-linear model for our data using the methodof iterative stepwise refinement: starting with an ini-tial model, variables are added or dropped if theircontribution to the reduction or increase of the resid-ual deviance compares favorably to the resulting lossor gain of residual degrees of freedom.
This processled to the selection of nine predictor variables.We evaluated the three prediction models dis-cussed above with and without he secondary sourceof morphology relations.
For the log-linear model,we repeatedly partitioned our data into equally sizedtraining and testing sets, estimated the weights onthe training set, and scored the model's performanceon the testing set, averaging the resulting scores.
5Table 2 shows the results of these analyses.
Al-though the log-linear model offers only a small im-provement on pair classification than the simpler butprediction rule, it confers the important advantage5When morphology is to be used as a supplementarypredictor, we remove the morphologically related pairsfrom the training and testing sets.of rating each prediction between 0 and 1.
We makeextensive use of this in the next phase of our algo-rithm.6 Finding Groups of Same-OrientedAdjectivesThe third phase of our method assigns the adjectivesinto groups, placing adjectives of the same (but un-known) orientation in the same group.
Each pairof adjectives has an associated issimilarity valuebetween 0 and 1; adjectives connected by same-orientation links have low dissimilarities, and con-versely, different-orientation links result in high dis-similarities.
Adjective pairs with no connecting linksare assigned the neutral dissimilarity 0.5.The baseline and but methods make qualitativedistinctions only (i.e., same-orientation, different-orientation, or unknown); for them, we define dis-similarity for same-orientation li ks as one minusthe probability that such a classification link is cor-rect and dissimilarity for different-orientation linksas the probability that such a classification is cor-rect.
These probabilities are estimated from sep-arate training data.
Note that for these predictionmodels, dissimilarities are identical for similarly clas-sifted links.The log-linear model, on the other hand, offersan estimate of how good each prediction is, since itproduces a value y between 0 and 1.
We constructthe model so that 1 corresponds tosame-orientation,and define dissimilarity as one minus the producedvalue.Same and different-orientation links between ad-jectives form a graph.
To partition the graph nodesinto subsets of the same orientation, we employ aniterative optimization procedure on each connectedcomponent, based on the exchange method, a non-hierarchical clustering algorithm (Spgth, 1985).
Wedefine an objective/unction ~ scoring each possiblepartition 7 ) of the adjectives into two subgroups C1and C2 asi=1 x,y E Ci177Number ofadjectives intest set (\[An\[)2 7303 5164 3695 236Number oflinks intest set (\[L~\[)2,5682,1591,7421,238Average numberofl inksforeach adjective7.048.379.4410.49Accuracy78.08%82.56%87.26%92.37%Ratio of averagegroup frequencies1.86991.9235L34861.4040Table 3: Evaluation of the adjective classification and labeling methods.where \[Cil stands for the cardinality of cluster i, andd(z, y) is the dissimilarity between adjectives z andy.
We want to select the partition :Pmin that min-imizes ~, subject to the additional constraint thatfor each adjective z in a cluster C,1 1IC l -  1 d(=,y) < --IVl d(=, y) (1)where C is the complement of cluster C, i.e., theother member of the partition.
This constraint,based on Rousseeuw's (1987) s=lhoue~es, helps cor-rect wrong cluster assignments.To find Pmin, we first construct a random parti-tion of the adjectives, then locate the adjective thatwill most reduce the objective function if it is movedfrom its current cluster.
We move this adjective andproceed with the next iteration until no movementscan improve the objective function.
At the final it-eration, the cluster assignment of any adjective thatviolates constraint (1) is changed.
This is a steepest-descent hill-climbing method, and thus is guaran-teed to converge.
However, it will in general find alocal minimum rather than the global one; the prob-lem is NP-complete (Garey and $ohnson, 1979).
Wecan arbitrarily increase the probability of finding theglobally optimal solution by repeatedly running thealgorithm with different starting partitions.7 Labe l ing  the  C lus ters  as Pos i t iveor  Negat iveThe clustering algorithm separates each componentof the graph into two groups of adjectives, but doesnot actually label the adjectives as positive or neg-ative.
To accomplish that, we use a simple criterionthat applies only to pairs or groups of words of oppo-site orientation.
We have previously shown (Hatzi-vassiloglou and McKeown, 1995) that in oppositionsof gradable adjectives where one member is semanti-cally unmarked, the unmarked member is the mostfrequent one about 81% of the time.
This is relevantto our task because semantic markedness exhibitsa strong correlation with orientation, the unmarkedmember almost always having positive orientation(Lehrer, 1985; Battistella, 1990).We compute the average frequency of the wordsin each group, expecting the group with higher av-erage frequency to contain the positive terms.
Thisaggregation operation increases the precision of thelabeling dramatically since indicators for many pairsof words are combined, even when some of the wordsare incorrectly assigned to their group.8 Resu l ts  and  Eva luat ionSince graph connectivity affects performance, we de-vised a method of selecting test sets that makes thisdependence explicit.
Note that the graph density islargely a function of corpus size, and thus can beincreased by adding more data.
Nevertheless, wereport results on sparser test sets to show how ouralgorithm scales up.We separated our sets of adjectives A (containing1,336 adjectives) and conjunction- and morphology-based links L (containing 2,838 links) into trainingand testing groups by selecting, for several valuesof the parameter a, the maximal subset of A, An,which includes an adjective z if and only if thereexist at least a links from L between x and otherelements of An.
This operation in turn defines asubset of L, L~, which includes all links betweenmembers of An.
We train our log-linear model onL - La (excluding links between morphologically re-lated adjectives), compute predictions and dissimi-larities for the links in L~, and use these to classifyand label the adjectives in An.
c~ must be at least2, since we need to leave some links for training.Table 3 shows the results of these experiments fora = 2 to 5.
Our method produced the correct clas-sification between 78% of the time on the sparsesttest set up to more than 92% of the time when ahigher number of links was present.
Moreover, in allcases, the ratio of the two group frequencies correctlyidentified the positive subgroup.
These results areextremely significant statistically (P-value less than10 -16 ) when compared with the baseline method ofrandomly assigning orientations to adjectives, or thebaseline method of always predicting the most fre-quent (for types) category (50.82% of the adjectivesin our collection are classified as negative).
Figure 2shows some of the adjectives in set A4 and their clas-sifications.178Classified as positive:bo ld  decisive disturbing enerous goodhonest important large mature patientpeaceful positive proud soundstimulating s t ra ight forward  strangetalented vigorous wittyClassified as negative:ambiguous cautious cynical evasiveharmful hypocritical inefficient insecurei r ra t iona l  irresponsible minor outspokenpleasant reckless risky selfish tediousunsupported vulnerable wastefulFigure 2: Sample retrieved classifications of adjec-tives from set A4.
Correctly matched adjectives areshown in bold.9 Graph  Connect iv i ty  andPer formanceA strong point of our method is that decisions onindividual words are aggregated to provide decisionson how to group words into a class and whether tolabel the class as positive or negative.
Thus, theoverall result can be much more accurate than theindividual indicators.
To verify this, we ran a seriesof simulation experiments.
Each experiment mea-sures how our algorithm performs for a given levelof precision P for identifying links and a given av-erage number of links k for each word.
The goal isto show that even when P is low, given enough data(i.e., high k), we can achieve high performance forthe grouping.As we noted earlier, the corpus data is eventuallyrepresented in our system as a graph, with the nodescorresponding to adjectives and the links to predic-tions about whether the two connected adjectiveshave the same or different orientation.
Thus the pa-rameter P in the simulation experiments measureshow well we are able to predict each link indepen-dently of the others, and the parameter k measuresthe number of distinct adjectives each adjective ap-pears with in conjunctions.
P therefore directly rep-resents the precision of the link classification algo-rithm, while k indirectly represents the corpus size.To measure the effect of P and k (which are re-flected in the graph topology), we need to carry out aseries of experiments where we systematically varytheir values.
For example, as k (or the amount ofdata) increases for a given level of precision P for in-dividual links, we want to measure how this affectsoverall accuracy of the resulting groups of nodes.Thus, we need to construct a series of data sets,or graphs, which represent different scenarios cor-responding to a given combination of values of Pand k. To do this, we construct a random graphby randomly assigning 50 nodes to the two possibleorientations.
Because we don't have frequency andmorphology information on these abstract nodes, wecannot predict whether two nodes are of the sameor different orientation.
Rather, we randomly as-sign links between nodes so that, on average, eachnode participates in k links and 100 x P% of alllinks connect nodes of the same orientation.
Thenwe consider these links as identified by the link pre-diction algorithm as connecting two nodes with thesame orientation (so that 100 x P% of these pre-dictions will be correct).
This is equivalent o thebaseline link classification method, and provides alower bound on the performance of the algorithmactually used in our system (Section 5).Because of the lack of actual measurements suchas frequency on these abstract nodes, we also de-couple the partitioning and labeling components ofour system and score the partition found under thebest matching conditions for the actual labels.
Thusthe simulation measures only how well the systemseparates positive from negative adjectives, not howwell it determines which is which.
However, in allthe experiments performed on real corpus data (Sec-tion 8), the system correctly found the labels of thegroups; any misclassifications came from misplacingan adjective in the wrong group.
The whole proce-dure of constructing the random graph and findingand scoring the groups is repeated 200 times for anygiven combination of P and k, and the results areaveraged, thus avoiding accidentally evaluating oursystem on a graph that is not truly representative ofgraphs with the given P and k.We observe (Figure 3) that even for relatively lowt9, our ability to correctly classify the nodes ap-proaches very high levels with a modest number oflinks.
For P = 0.8, we need only about ?
linksper adjective for classification performance over 90%and only 12 links per adjective for performance over99%.
s The difference between low and high valuesof P is in the rate at which increasing data increasesoverall precision.
These results are somewhat moreoptimistic than those obtained with real data (Sec-tion 8), a difference which is probably due to the uni-form distributional assumptions in the simulation.Nevertheless, we expect the trends to be similar tothe ones shown in Figure 3 and the results of Table 3on real data support his expectation.10 Conc lus ion  and  Future  WorkWe have proposed and verified from corpus data con-straints on the semantic orientations of conjoined ad-jectives.
We used these constraints to automaticallyconstruct a log-linear egression model, which, com-bined with supplementary morphology rules, pre-dicts whether two conjoined adjectives are of same812 links per adjective for a set of n adjectives requires6n conjunctions between the n adjectives in the corpus.179~ 75'70.65.60"55-50 ~0 i2~4567891( )  1'2 14 16 18 20Avem0e neiohbo~ per node(a) P = 0.7525 30 32.7795.90.85.~75'Average neighbors per node(b) P = 0.8,~ 70656O5,550Average netghbo~ per node(c) P = 0.8525 28.64Figure 3: Simulation results obtained on 50 nodes.10(959O85P~ 7o55Average neighb0m per node(d) P = 0.9In each figure, the last z coordinate indicates the(average) maximum possible value of k for this P, and the dotted line shows the performance of a randomclassifier.or different orientation with 82% accuracy.
We thenclassified several sets of adjectives according to thelinks inferred in this way and labeled them as posi-tive or negative, obtaining 92% accuracy on the clas-sification task for reasonably dense graphs and 100%accuracy on the labeling task.
Simulation experi-ments establish that very high levels of performancecan be obtained with a modest number of links perword, even when the links themselves are not alwayscorrectly classified.As part of our clustering algorithm's output, a"goodness-of-fit" measure for each word is com-puted, based on Rousseeuw's (1987) silhouettes.This measure ranks the words according to how wellthey fit in their group, and can thus be used asa quantitative measure of orientation, refining thebinary positive-negative distinction.
By restrictingthe labeling decisions to words with high values ofthis measure we can also increase the precision ofour system, at the cost of sacrificing some coverage.We are currently combining the output of this sys-tem with a semantic group finding system so that wecan automatically identify antonyms from the cor-pus, without access to any semantic descriptions.The learned semantic ategorization of the adjec-tives can also be used in the reverse direction, tohelp in interpreting the conjunctions they partici-pate.
We will also extend our analyses to nouns andverbs.AcknowledgementsThis work was supported in part by the Officeof Naval Research under grant N00014-95-1-0745,jointly by the Office of Naval Research and theAdvanced Research Projects Agency under grantN00014-89-J-1782, by the National Science Founda-180tion under grant GER-90-24069, and by the NewYork State Center for Advanced Technology un-der contracts NYSSTF-CAT(95)-013 and NYSSTF-CAT(96)-013.
We thank Ken Church and theAT&T Bell Laboratories for making the PARTSpart-of-speech tagger available to us.
We also thankDragomir Radev, Eric Siegel, and Gregory SeanMcKinley who provided models for the categoriza-tion of the adjectives in our training and testing setsas positive and negative.ReferencesJean-Claude Anscombre and Oswald Ducrot.
1983.L ' Argumentation dans la Langue.
Philosophic etLangage.
Pierre Mardaga, Brussels, Belgium.Edwin L. Battistella.
1990.
Markedness: The Eval-uative Superstructure of Language.
State Univer-sity of New York Press, Albany, New York.Peter F. Brown, Vincent J. della Pietra, Peter V.de Souza, Jennifer C. Lai, and Robert L. Mercer.1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics, 18(4):487-479.Kenneth W. Church.
1988.
A stochastic partsprogram and noun phrase parser for unrestrictedtext.
In Proceedings of the Second Conference onApplied Natural Language Processing (ANLP-88),pages 136-143, Austin, Texas, February.
Associa-tion for Computational Linguistics.W.
J. Conover.
1980.
Practical NonparametricStatistics.
Wiley, New York, 2nd edition.Richard O. Duda and Peter E. Hart.
1973.
PatternClassification and Scene Analysis.
Wiley, NewYork.Michael Elhadad and Kathleen R. McKeown.
1990.A procedure for generating connectives.
In Pro-ceedings of COLING, Helsinki, Finland, July.Michael R. Garey and David S. Johnson.
1979.Computers and Intractability: A Guide to theTheory ofNP-Completeness.
W H. Freeman, SanFrancisco, California.Vasileios Hatzivassiloglou and Kathleen R. McKe-own.
1993.
Towards the automatic dentificationof adjectival scales: Clustering adjectives accord-ing to meaning.
In Proceedings of the 31st AnnualMeeting of the ACL, pages 172-182, Columbus,Ohio, June.
Association for Computational Lin-guistics.Vasileios I-Iatzivassiloglou and Kathleen R. MeKe-own.
1995.
A quantitative evaluation of linguis-tic tests for the automatic prediction of semanticmarkedness.
In Proceedings of the 83rd AnnualMeeting of the ACL, pages 197-204, Boston, Mas-sachusetts, June.
Association for ComputationalLinguistics.John S. Justeson and Slava M. Katz.
1991.
Co-occurrences of antonymous adjectives and theircontexts.
Computational Linguistics, 17(1):1-19.Adrienne Lehrer.
1974.
Semantic Fields and LezicalStructure.
North Holland, Amsterdam and NewYork.Adrienne Lehrer.
1985.
Markedness and antonymy.Journal of Linguistics, 31(3):397-429, September.John Lyons.
1977.
Semantics, volume 1.
CambridgeUniversity Press, Cambridge, England.Peter McCullagh and John A. Nelder.
1989.
Gen-eralized Linear Models.
Chapman and Hall, Lon-don, 2nd edition.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.1990.
Introduction to WordNet: An on-line lexi-cal database.
International Journal of Lexicogra-phy (special issue), 3(4):235-312.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional c ustering of English words.In Proceedings of the 3Ist Annual Meeting of theACL, pages 183-190, Columbus, Ohio, June.
As-sociation for Computational Linguistics.Peter J. Rousseeuw.
1987.
Silhouettes: A graphicalaid to the interpretation a d validation of clusteranalysis.
Journal of Computational and AppliedMathematics, 20:53-65.Thomas J. Santner and Diane E. Duffy.
1989.
TheStatistical Analysis of Discrete Data.
Springer-Verlag, New York.Helmuth Sp~ith.
1985.
Cluster Dissection and Anal-ysis: Theory, FORTRAN Programs, Examples.Ellis Horwo0d, Chiehester, West Sussex, England.181
