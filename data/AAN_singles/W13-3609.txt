Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 68?73,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsToward More Precision in Correction of Grammatical ErrorsDan FlickingerCenter for the Study ofLanguage and InformationStanford Universitydanf@stanford.eduJiye YuCenter for the Study ofLanguage and InformationStanford Universityjyu2009@stanford.eduAbstractWe describe a system for detecting andcorrecting instances of a small class of fre-quently occurring grammatical error typesin a corpus of essays which have beenmanually annotated for these errors.
Oursystem employs a precise broad-coveragegrammar of English which has been aug-mented with a set of mal-rules and mal-entries to explicitly license certain types oferroneous expressions.
The derivation treeproduced by a parser using this grammaridentifies the location and type of an errorin an ill-formed sentence, enabling a post-processing script to use the tree and the in-ventory of error types to delete and/or in-sert tokens in order to produce a correctedversion of the original sentence.1 OverviewAs a participating group in the 2013 CoNLLShared Task on Grammatical Error Correction,we adapted an existing system for error detec-tion in a simpler closed-vocabulary domain tomeet the additional demands of accommodatingan open vocabulary and producing corrections forthe errors identified.
The training and test datafor this shared task are from the NUCLE cor-pus (Dahlmeier et al 2013), which consists ofabout one million words of short essays writtenby relatively competent English language learn-ers.
Each sentence has been manually annotatedto identify and correct a wide range of grammat-ical and stylistic error types, though the sharedtask focused only on correcting instances of fiveof these types.
Following standard procedure forsuch shared tasks, the organizers supplied most ofthe annotated data as a development corpus, andheld out a 1381-sentence test corpus which wasused for the evaluation of system output.2 Resources and MethodThe system developed for this task is an extensionof an existing language-processing engine used toidentify grammatical errors in short sentences andparagraphs written by elementary school studentsas part of the automated Language Arts and Writ-ing course included in the EPGY (Education Pro-gram for Gifted Youth) course offerings (Suppes etal., 2012).
This error detection engine consists ofa grammar, a parser, and a post-processing scriptthat interprets the error codes in the derivationtree for each parsed sentence.
Both the grammarand the parser are open-source resources devel-oped and distributed as part of the DELPH-IN con-sortium (www.delph-in.net).
We use the EnglishResource Grammar, described below, which wehave augmented with both rules and lexical entriesthat license instances of certain error types, usingthe mal-rule approach of (Schneider and McCoy,1998), adapted and extended for the ERG as de-scribed in (Bender et al 2004).
For parsing eachsentence with this grammar, we use the relativelyefficient PET parser (Callmeier, 2002), along witha parse-ranking method based on a model trainedon a manually disambiguated treebank, so far con-sisting only of parses of well-formed sentences.
Inaddition to using the manually constructed 37,000-word lexicon included in the ERG, we accommo-date unknown words by mapping POS tags pro-duced by TnT (Brants, 2000) to generic lexical en-try types on the fly.
The bottom-up chart parserthen exhaustively applies the rules of the grammarto the lexical entries introduced by the tokens inthe input sentence, producing a packed forest ofanalyses (derivations) ranked by likelihood, andthen presents the most likely derivation for post-processing.
The post-processor is a script whichuses the derivation tree to identify the type and lo-cation of each error, and then takes appropriate ac-tion, which in the course is an instructional mes-68sage to the student, and in this shared task is a cor-rected version of the original sentence.2.1 English Resource GrammarThe English Resource Grammar used for this task(ERG: (Flickinger, 2000), (Flickinger, 2011)) isa broad-coverage grammar implementation whichhas been under continuous development since themid-1990s at Stanford.
As an implementationwithin the theoretical framework of Head-drivenPhrase Structure Grammar (HPSG: (Pollard andSag, 1994)), the ERG has since its inception en-coded both morphosyntactic and semantic prop-erties of English, in a declarative representationthat enables both parsing and generation.
Whiledevelopment has always taken place in the con-text of specific applications, primary emphasis inthe ERG has consistently been on the linguisticaccuracy of the resulting analyses, at some ex-pense to robustness.
Its initial use was for gener-ation within the German-English machine transla-tion prototype developed in the Verbmobil project(Wahlster, 2000), so constraining the grammarto avoid overgeneration was a necessary designrequirement that fit well with the broader aimsof its developers.
Applications using the gram-mar since then have included automatic processingof e-commerce customer support email messages,a second machine translation system (LOGON:(Lnning et al 2004)), and information extractionover the full English Wikipedia (Flickinger et al2010).At present, the ERG consists of a rich hier-archy of types encoding regularities both in thelexicon and in the syntactic constructions of En-glish.
The lexicon contains 40,000 manually con-structed lexeme entries, each assigned to one of975 lexical types at the leaves of this hierarchy,where the types encode idiosyncracies of subcat-egorization, modification targets, exceptional be-havior with respect to lexical rules, etc.
The gram-mar also includes 70 derivational and inflectionalrules which apply to these lexemes (or to eachother?s outputs) to produce the words as they ap-pear in text.
The grammar provides 225 syntacticrules which admit either unary or binary phrases;these include a relatively small number of highlyschematic rules which license ordinary combina-tions of heads with their arguments and their mod-ifiers, and a rather larger number of construction-specific rules both for frequently occurring phrasetypes such as coordinate structures or appositives,and for phrase types that occur with markedlydiffering frequencies in verious corpus domains,such as questions or vocatives.
Statistical modelstrained on manually annotated treebanks are usedboth in parsing (Toutanova et al 2005) and in gen-eration (Velldal, 2008) to rank the relative likeli-hoods of the outputs, in order to address the issueof disambiguation which is central to the use ofany broad-coverage grammar for almost any task.2.2 Mal-rule exampleEach of the hand-coded mal-rules added to thestandard ERG is a variant of a rule needed to anal-yse well-formed English input.
A simple exam-ple of a mal-rule is given below, expressed in theattribute-value representation for an HPSG rule;this unary rule licenses a noun phrase headed by asingular count noun but lacking its normally oblig-atory article, as for the NP black cet in That dogchased black cat.
Here the single daughter in thisnoun phrase (the HD-DTR) is a nominal phrase stillseeking an obligatory specifier (the article or de-terminer in a well formed noun phrase), where thehead noun is a singular count noun (non-divisible).The SYNSEM value in the rule discharges thatobligatory specifier requirement just as the normalunary rule for bare plural noun phrases does, andsupplies the necessary implicit quantifier in the se-mantics of the phrase.?????????????????SYNSEM?????LOCAL????CAT?
?HEAD1nounVAL[SPR <>COMPS <>]?
?CONT[RELS < quant rel >]?????????HD-DTR???????SYNSEM???????LOCAL??????CAT??
?HEAD1VAL[SPR <[OPT ?
]>COMPS <>]??
?AGR[PN 3singDIV ?]????????????????????????????????????
?Mal-rule for bare singular NP2.3 Error types in the taskOf the five error types used in the shared task,four were already included in the grammar as usedin the EPGY course, involving errors with arti-cles/determiners, number on nouns, subject-verbagreement, and verb form.
For the task, we addedmal-rules and mal-entries to analyze a subset of er-rors of the fifth type, which involve incorrect useof prepositions.
Within the ERG, each of the fiveerror types is associated with multiple mal-rules or69mal-entries, each licensing one specific error con-figuration, such as a mal-rule to accommodate theomission of an obligatory determiner for a nounphrase headed by a singular count noun, or a mal-entry for the unwanted use of the with a propername.Most of these grammar-internal error identifierscorrespond to a simple adjustment for correctionin the original sentence, such as the insertion ordeletion of a particular token, or a change to the in-flection of a particular noun or verb.
However, forsome errors, several candidate corrections are trig-gered by the error identifier, so the post-processingscript must select the most suitable of these correc-tion candidates.
The most frequent correction il-lustrating this ambiguity is for singular count nounphrases missing the determiner, such as black catin we admired black cat., where the correctionmight be the black cat, a black cat, or black cats.Lacking a rich discourse representation of the con-text surrounding the error, we employ an N-grambased ranking approach to choose among the threealternatives, where the post-processor currentlycalls the Microsoft N-gram online resource (Wanget al 2011).Since the development and test data is presentedas pre-tokenized input with one token per line infamiliar CoNLL format, we also employ an offlinescript which converts a file of this format into onewhich has a single sentence per line, preservingthe tokenization of the CoNLL file, and it is thisone-sentence-per-line file which is processed bythe correction script, which in turn calls the parserand applies the post-processing steps to its output.3 An exampleWe illustrate our method with a simple examplesentence, to show each step of the process.
Con-sider the analysis in Figure 1 of the following sen-tence taken from the test corpus:In supermarkets monitors is needed because wehave to track thieves .The parser is called with this sentence as in-put, constructs a packed forest of all candidateanalyses licensed by the grammar, and identifiesthe most likely analysis as determined by ageneral-purpose statistical model trained onlyon analyses of well-formed sentences.
A moredetailed view of the parse tree in Figure 1 is thebracketed derivation tree given in (2).
Each line ofthe derivation identifies the syntactic construction,lexical rule, or lexical entry used to build eachconstituent, and shows its token span, and for theleaf nodes, the lexical entry, its type (after theslash), and the surface form of that word in theinput sentence.
The boldface identifier on the firstline of the derivation tree shows that this analysiscontains at least one erroneous constituent, whicha perusal of the tree locates as the other boldfaceidentifier, be c is rbst, for the mal-entry for is thatlicenses a mismatch in subject-verb agreement.
(2) Derivation tree view of Fig.
1:hd-aj scp c 0 11 [ root robust s ]flr-hd nwh-nc-pp c 0 5hd-cmp u c 0 2in/p np i-reg 0 1 "in"hdn bnp c 1 2n pl olr 1 2supermarket n1/n - c 1 2"supermarkets"sb-hd nmc c 2 5hdn bnp c 2 3n pl olr 2 3monitor n1/n - c 2 3 "monitors"hd-cmp u c 3 5be c is rbst 3 4 "is"hd xaj-int-vp c 4 5hd optcmp c 4 5v pas odlr 4 5need v1/v np 4 5 "needed"hd-cmp u c 5 11because/p cp s 5 6 "because"sb-hd nmc c 6 11hdn bnp-qnt c 6 7we/n - pr-we 6 7 "we"hd-cmp u c 7 11v n3s-bse ilr 7 8have to1/v vp ssr 7 8 "have"hd-cmp u c 8 11to c prop/cm vp to 8 9 "to"hd-cmp u c 9 11v n3s-bse ilr 9 10track v1/v np* 9 10 "track"hdn bnp c 10 11period plr 10 11n pl olr 10 11thief n1/n - c 10 11 "thieves.
"The correction script finds this mal-entry identi-fier in the derivation tree, notes its token position,and determines from the identifier that the requiredcorrection consists of a simple token substitution,replacing the surface token is with are.
Since noother errors are present in the derivation tree, thescript then records in the corpus output file the cor-rected sentence with only the one alteration fromits original form.Of course, a derivation tree will often identifymultiple errors, and for some error types may re-quire that multiple tokens be modified for a sin-70SHHHHHHHHPP HHPinNPNNsupermarketsS/PPHHHHHHS/PPHHHNPNNmonitorsVP/PP HHV/PPisVP/PPVPVVneededPPHHHPbecauseSHHHNPNPweVPHHHVVhaveVPHHHCOMPtoVP HHVVtrackNPNNNthieves.Figure 1: Sample parse tree produced with ERGgle error, such as in the correction of the equip-ments have arrived to the equipment has arrived.Each mal-rule or mal-entry identifier is associatedwith a specific correction procedure defined in thecorrection script, and the script carries out thesechanges in a pre-determined order, for the rela-tively infrequent instances where the order of ap-plication matters.
For simple alterations such asa change of number on nouns or verbs, we couldhave used the grammar-internal inflectional rulemachinery, but found it more convenient to use ex-isting Perl and Python modules for English wordinflection.4 Results and DiscussionDuring the development phase of the shared task,we adapted and refined our method using the first5000 NUCLE sentences from the roughly 50,000-sentence development corpus.
Since our focus inthis task is on precision more than on recall, wecarried out repeated detailed examinations of thecorrection procedure?s outputs on the first 500sentences.
In comparing our system?s proposedcorrections with the ?gold?
human annotationsof errors for these 500, we found the followingfrequencies of mismatches between system andgold:(3) Comparison of System and Gold on Dev-500Alteration # of SentencesBoth match 34Missing gold 26Differing correction 25Wrong alteration 28Examples of the missing gold annotations include(a) ?ArtOrDet?
errors such as the missing articlefor habitable environment in sentence 829-4-0 andfor password in sentence 830-1-1; (b) ?SVA?
er-rors such as for the verb increase in sentence 831-3-8, and the verb are in sentence 840-4-2; and (c)?Nn?
errors such as for the noun equipments ap-pearing in sentence 836-1-0, or evidences in sen-tence 837-2-11.These varying sources of mismatches made theautomated scoring script used in the evaluationphase of the shared task (Dahlmeier and Ng, 2012)not so helpful during development, since it re-ported our system?s precision as 28%, whereas thesystem is actually correct in more than 50% of thealterations it makes for these first 500 sentences ofthe development corpus.This inconsistency in the gold annotations wasless of an issue, but still present, in our system?s71precision measure in the evaluation phase of theshared task, as we found in studying the goldannotations distributed for the test data after theevaluation phase ended.
The official scored resultsfor the system output that we submitted are givenin the table in (4).
(4) Official scoring of system output on test dataPrecision 29.93%Recall 5.86 %F1 9.81 %In examining the gold annotations for the 1381sentences comprising the test corpus, we found47 instances of genuine errors that were miss-ing gold annotation, but that our system correctlyidentified and repaired.
While this led to a some-what lower precision measure, we acknowledgethat compared with the total number of more than1600 annotated corrections, this level of imperfec-tion in the annotations was not seriously problem-atic for evaluation, and we view the official resultsin (4) as a reasonable measure of the system outputwe submitted for scoring.While comparing our system results with thegold test annotations after the evaluation phaseended, we have found and repaired several sourcesof undesirable behavior in the grammar and in ourcorrection script, with the most significant beingthe revision of lexical entries for two compoundnouns appearing with high frequency in the testcorpus: life expectancy (91 occurrences) and pop-ulation aging/ageing (40 occurrences).
Our lexi-con had erroneously identified life expectancy ascountable, and the parser had wrongly analyzedpopulation aging as a noun modified by a partici-ple, analogous to the person speaking.
A thirdfrequently occurring error in the corpus was notso simple to correct in our grammar, namely theword society (95 occurrences), which is used con-sistently in the test corpus as an abstract noun of-ten wrongly appearing with the.
Since this nouncan be used in a different sense (as an organiza-tion) where the article is appropriate, as in the so-ciety of wealthy patrons, we would need to findsome other knowledge source to determine thatin the domain of the test corpus, this sense is notused.
Hence our system still fails to identify andcorrect the frequent and spurious the in the society.With the small number of corrections made toour system?s lexicon, and some minor improve-ments to the post-processing script, our systemnow produces output on the test corpus with animproved precision measure of 47.5%, and a moremodest improvement in recall to 13.2%, for an F1of 20.7%.
Given the inconsistency of annotationin the development corpus, it is as yet difficult toevaluate whether these changes to our correctionscript will result in corresponding improvementsin precision on unseen data.5 Next stepsWe see prospects for significant improvement us-ing the method we are developing for the kind ofautomatic correction studied in this shared task.Many of the missteps that our correction proce-dure makes can be traced to imperfect parse selec-tion from among the candidate analyses producedby the parser, and this could well be improved bycreating a Redwoods-style treebank that includesboth well-formed and ill-formed sentences for an-notation, so the mal-rules and mal-entries get in-cluded in the ranking model trained on such a tree-bank.
While our primary focus will continue to beon increased precision in the corrections the sys-tem proposes, we welcome the attention to recallthat this task brings, and expect to work with hy-brid systems that do more with large-scale corporasuch as the English Wikipedia.ReferencesEmily M. Bender, Dan Flickinger, Stephan Oepen, An-nemarie Walsh, and Timothy Baldwin.
2004.
Ar-boretum.
Using a precision grammar for grammarchecking in CALL.
In Proceedings of the InSTILSymposium on NLP and Speech Technologies in Ad-vanced Language Learning Systems, Venice, Italy,June.Thorsten Brants.
2000.
TnT - A statistical part-of-speech tagger.
In Proceedings of the 6th ACL Con-ference on Applied Natural Language Processing,Seattle, WA.Ulrich Callmeier.
2002.
Preprocessing and encod-ing techniques in PET.
In Stephan Oepen, DanielFlickinger, J. Tsujii, and Hans Uszkoreit, editors,Collaborative Language Engineering.
A Case Studyin Efficient Grammar-based Processing.
CSLI Pub-lications, Stanford, CA.Daniel Dahlmeier and Hwee Tou Ng.
2012.
Betterevaluation for grammatical error correction.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages568 ?
572, Montreal, Canada.72Daniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu.2013.
Building a large annotated corpus of learnerenglish: The NUS Corpus of Learner English.
In Toappear in Proceedings of the 8th Workshop on Inno-vative Use of NLP for Building Educational Appli-cations, Atlanta, Georgia.Dan Flickinger, Stephan Oepen, and Gisle Ytrest?l.2010.
WikiWoods.
Syntacto-semantic annotationfor English Wikipedia.
In Proceedings of the 6th In-ternational Conference on Language Resources andEvaluation, Valletta, Malta.Dan Flickinger.
2000.
On building a more efficientgrammar by exploiting types.
Natural LanguageEngineering, 6 (1) (Special Issue on Efficient Pro-cessing with HPSG):15 ?
28.Dan Flickinger.
2011.
Accuracy vs. robustness ingrammar engineering.
In Emily M. Bender and Jen-nifer E. Arnold, editors, Language from a Cogni-tive Perspective: Grammar, Usage, and Processing,pages 31?50.
Stanford: CSLI Publications.Jan Tore Lnning, Stephan Oepen, Dorothee Beer-mann, Lars Hellan, John Carroll, Helge Dyvik, DanFlickinger, Janne Bondi Johannessen, Paul Meurer,Torbjrn Nordgrd, Victoria Rosn, and Erik Velldal.2004.
LOGON.
A Norwegian MT effort.
In Pro-ceedings of the Workshop in Recent Advances inScandinavian Machine Translation, Uppsala, Swe-den.Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar.
Studies in Contempo-rary Linguistics.
The University of Chicago Pressand CSLI Publications, Chicago, IL, and Stanford,CA.David Schneider and Kathleen McCoy.
1998.
Recog-nizing syntactic errors in the writing of second lan-guage learners.
In Proceedings of Coling-ACL 1998,pages 1198 ?
1204, Montreal.Patrick Suppes, Dan Flickinger, Elizabeth Macken,Jeanette Cook, and L. Liang.
2012.
Descriptionof the EPGY Stanford University online courses forMathematics and the Language Arts.
In Proceed-ings of the International Society for Technology inEducation, San Diego, California.Kristina Toutanova, Christoper D. Manning, DanFlickinger, and Stephan Oepen.
2005.
StochasticHPSG parse selection using the Redwoods corpus.Journal of Research on Language and Computation,3(1):83 ?
105.Erik Velldal.
2008.
Empirical Realization Ranking.Ph.D.
thesis, University of Oslo, Department of In-formatics.Wolfgang Wahlster, editor.
2000.
Verbmobil.
Foun-dations of Speech-to-Speech Translation.
Springer,Berlin, Germany.Kuansan Wang, Christopher Thrasher, Evelyne Viegas,Xiaolong Li, , and Paul Hsu.
2011.
An overviewof Microsoft Web N-gram corpus and applications.In Proceedings of the 2011 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,Portland, Oregon.73
