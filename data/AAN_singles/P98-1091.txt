An Empirical  Evaluation of Probabil ist ic Lexical ized TreeInsert ion Grammars *Rebecca  HwaHarvard UniversityCambridge, MA 02138 USArebecca~eecs.harvard.eduAbst rac tWe present an empirical study of the applica-bility of Probabilistic Lexicalized Tree Inser-tion Grammars (PLTIG), a lexicalized counter-part to Probabilistic Context-Free Grammars(PCFG), to problems in stochastic natural-language processing.
Comparing the perfor-mance of PLTIGs with non-hierarchical N-grammodels and PCFGs, we show that PLTIG com-bines the best aspects of both, with languagemodeling capability comparable to N-grams,and improved parsing performance over its non-lexicalized counterpart.
Furthermore, train-ing of PLTIGs displays faster convergence thanPCFGs.1 I n t roduct ionThere are many advantages to expressing agrammar in a lexicalized form, where an ob-servable word of the language is encoded ineach grammar ule.
First, the lexical wordshelp to clarify ambiguities that cannot be re-solved by the sentence structures alone.
Forexample, to correctly attach a prepositionalphrase, it is often necessary to consider the lex-ical relationships between the head word of theprepositional phrase and those of the phrasesit might modify.
Second, lexicalizing the gram-mar rules increases computational efficiency be-cause those rules that do not contain any ob-served words can be pruned away immediately.The Lexicalized Tree Insertion Grammar for-malism (LTIG) has been proposed as a wayto lexicalize context-free grammars (Schabes* This material is based upon work supported by the Na-tional Science Foundation under Grant No.
IR19712068.We thank Yves Schabes and Stuart Shieber for theirguidance; Joshua Goodman for his PCFG code; LillianLee and the three anonymous reviewers for their com-ments on the paper.and Waters, 1994).
We now apply a prob-abilistic variant of this formalism, Probabilis-tic Tree Insertion Grammars (PLTIGs), to nat-ural language processing problems of stochas-tic parsing and language modeling.
This pa-per presents two sets of experiments, compar-ing PLTIGs with non-lexicalized ProbabilisticContext-Free Grammars (PCFGs) (Pereira andSchabes, 1992) and non-hierarchical N-grammodels that use the right branching bracketingheuristics (period attaches high) as their pars-ing strategy.
We show that PLTIGs can be in-duced from partially bracketed ata, and thatthe resulting trained grammars can parse un-seen sentences and estimate the likelihood oftheir occurrences in the language.
The experi-ments are run on two corpora: the Air TravelInformation System (ATIS) corpus and a sub-set of the Wall Street Journal TreeBank cor-pus.
The results show that the lexicalized na-ture of the formalism helps our induced PLTIGsto converge faster and provide a better languagemodel than PCFGs while maintaining compara-ble parsing qualities.
Although N-gram modelsstill slightly out-perform PLTIGs on languagemodeling, they lack high level structures neededfor parsing.
Therefore, PLTIGs have combinedthe best of two worlds: the language modelingcapability of N-grams and the parse quality ofcontext-free grammars.The rest of the paper is organized as fol-lows: first, we present an overview of the PLTIGformalism; then we describe the experimentalsetup; next, we interpret and discuss the resultsof the experiments; finally, we outline future di-rections of the research.2 PLT IG  and  Re la ted  WorkThe inspiration for the PLTIG formalism stemsfrom the desire to lexicalize a context-free gram-557mar.
There are three ways in which one mightdo so.
First, one can modify the tree struc-tures so that all context-free productions con-tain lexical items.
Greibach normal form pro-vides a well-known example of such a lexical-ized context-free formalism.
This method isnot practical because altering the structures ofthe grammar damages the linguistic informa-tion stored in the original grammar (Schabesand Waters, 1994).
Second, one might prop-agate lexical information upward through theproductions.
Examples of formalisms using thisapproach include the work of Magerman (1995),Charniak (1997), Collins (1997), and Good-man (1997).
A more linguistically motivatedapproach is to expand the domain of produc-tions downward to incorporate more tree struc-tures.
The Lexicalized Tree-Adjoining Gram-mar (LTAG) formalism (Schabes et al, 1988),(Schabes, 1990) , although not context-free, isthe most well-known instance in this category.PLTIGs belong to this third category and gen-erate only context-free languages.LTAGs (and LTIGs) are tree-rewriting sys-tems, consisting of a set of elementary treescombined by tree operations.
We distinguishtwo types of trees in the set of elementary trees:the initial trees and the auxiliary trees.
Unlikefull parse trees but reminiscent of the produc-tions of a context-free grammar, both types oftrees may have nonterminal leaf nodes.
Aux-iliary trees have, in addition, a distinguishednonterminal leaf node, labeled with the samenonterminal s the root node of the tree, calledthe foot node.
Two types of operations are usedto construct derived trees, or parse trees: sub-stitution and adjunction.
An initial tree canbe substituted into the nonterminal leaf node ofanother tree in a way similar to the substitu-tion of nonterminals in the production rules ofCFGs.
An auxiliary tree is inserted into anothertree through the adjunction operation, whichsplices the auxiliary tree into the target tree ata node labeled with the same nonterminal asthe root and foot of the auxiliary tree.
By us-ing a tree representation, LTAGs extend the do-main of locality of a grammatical primitive, sothat they capture both lexical features and hi-erarchical structure.
Moreover, the adjunctionoperation elegantly models intuitive linguisticconcepts uch as long distance dependencies be-tween words.
Unlike the N-gram model, whichonly offers dependencies between neighboringwords, these trees can model the interaction ofstructurally related words that occur far apart.Like LTAGs, LTIGs are tree-rewriting sys-tems, but they differ from LTAGs in their gener-ative power.
LTAGs can generate some strictlycontext-sensitive languages.
They do so by us-ing wrapping auxiliary trees, which allow non-empty frontier nodes (i.e., leaf nodes whose la-bels are not the empty terminal symbol) on bothsides of the foot node.
A wrapping auxiliarytree makes the formalism context-sensitive b -cause it coordinates the string to the left of itsfoot with the string to the right of its foot whileallowing a third string to be inserted into thefoot.
Just as the ability to recursively center-embed moves the required parsing time fromO(n) for regular grammars to O(n 3) for context-free grammars, so the ability to wrap auxiliarytrees moves the required parsing time further,to O(n 8) for tree-adjoining grammars 1.
Thislevel of complexity is far too computationallyexpensive for current technologies.
The com-plexity of LTAGs can be moderated by elimi-nating just the wrapping auxiliary trees.
LTIGsprevent wrapping by restricting auxiliary treestructures to be in one of two forms: the leftauxiliary tree, whose non-empty frontier nodesare all to the left of the foot node; or the rightauxiliary tree, whose non-empty frontier nodesare all to the right of the foot node.
Auxil-iary trees of different ypes cannot adjoin intoeach other if the adjunction would result in awrapping auxiliary tree.
The resulting systemis strongly equivalent to CFGs, yet is fully lex-icalized and still O(n 3) parsable, as shown bySchabes and Waters (1994).Furthermore, LTIGs can be parameterized toform probabilistic models (Schabes and Waters,1993).
Informally speaking, a parameter is as-sociated with each possible adjunction or sub-stitution operation between a tree and a node.For instance, suppose there are V left auxiliarytrees that might adjoin into node r/.
Then thereare V q- 1 parameters associated with node r/1The best theoretical upper bound on time complex-ity for the recognition of Tree Adjoining Languages iO(M(n2)), where M(k) is the time needed to multiplytwo k x k boolean matrices.
(Rajasekaran andYooseph,1995)558Elem~ntwy ~ ~:t l~t t~ptl 1?
X, ~td It~rd 2 twordnX word 2 X word n * $Figure h A set of elementary LTIG trees thatrepresent a bigram grammar.
The arrows indi-cate adjunction sites.that describe the distribution of the likelihoodof any left auxiliary tree adjoining into node ~/.
(We need one extra parameter for the case ofno left adjunction.)
A similar set of parame-ters is constructed for the right adjunction andsubstitution distributions.3 Exper imentsIn the following experiments we show thatPLTIGs of varying sizes and configurations canbe induced by processing a large training cor-pus, and that the trained PLTIGs can provideparses on unseen test data of comparable qual-ity to the parses produced by PCFGs.
More-over, we show that PLTIGs have significantlylower entropy values than PCFGs, suggestingthat they make better language models.
Wedescribe the induction process of the PLTIGsin Section 3.1.
Two corpora of very differentnature are used for training and testing.
Thefirst set of experiments uses the Air Travel In-formation System (ATIS) corpus.
Section 3.2presents the complete results of this set of ex-periments.
To determine if PLTIGs can scaleup well, we have also begun another study thatuses a larger and more complex corpus, the WallStreet Journal TreeBank corpus.
The initial re-sults are discussed in Section 3.3.
To reduce theeffect of the data sparsity problem, we back offfrom lexical words to using the part of speechtags as the anchoring lexical items in all theexperiments.
Moreover, we use the deleted-interpolation smoothing technique for the N-gram models and PLTIGs.
PCFGs do not re-quire smoothing in these experiments.3.1 Grammar  InductionThe technique used to induce a grammar is asubtractive process.
Starting from a universalgrammar (i.e., one that can generate any stringmade up of the alphabet set), the parametersExample sentence:The cat chases the mouseCorresponding derivation tree:tinit .~d J .tthe .~dj.teat ~dj .tchase s ~dj .ttht ,,,1~t.
adj.tmouseFigure 2: An example sentence.
Because eachtree is right adjoined to the tree anchored withthe neighboring word in the sentence, the onlystructure is right branching.are iteratively refined until the grammar gen-erates, hopefully, all and only the sentences inthe target language, for which the training dataprovides an adequate sampling.
In the case ofa PCFG, the initial grammar production ruleset contains all possible rules in Chomsky Nor-mal Form constructed by the nonterminal andterminal symbols.
The initial parameters asso-ciated with each rule are randomly generatedsubject to an admissibility constraint.
As longas all the rules have a non-zero probability, anystring has a non-zero chance of being generated.To train the grammar, we follow the Inside-Outside re-estimation algorithm described byLari and Young (1990).
The Inside-Outside r -estimation algorithm can also be extended totrain PLTIGs.
The equations calculating theinside and outside probabilities for PLTIGs canbe found in Hwa (1998).As with PCFGs, the initial grammar must beable to generate any string.
A simple PLTIGthat fits the requirement is one that simulatesa bigram model.
It is represented by a tree setthat contains a right auxiliary tree for each lex-ical item as depicted in Figure 1.
Each tree hasone adjunction site into which other right auxil-iary trees can adjoin.
The tree set has only oneinitial tree, which is anchored by an empty lex-ical item.
The initial tree represents the startof the sentence.
Any string can be constructedby right adjoining the words together in order.Training the parameters of this grammar yieldsthe same result as a bigram model: the param-eters reflect close correlations between words559Ktemem~ Sits:t~t tl ~ 1 a word= ~rd l  uv'~?
m5i -_ / \  - / \  / \  - / \  ~X~ X. X X X, X X, X_sj _SIR_ " _51 __iSJR_word I word x wo~ 1 wo~ XFigure 3: An LTIG elementary tree set that al-low both left and right adjunctions.that are frequently seen together, but the modelcannot provide any high-level linguistic struc-ture.
(See example in Figure 2.
)Example  sentence :The cat chases the mouseCorresponding derivat ion t ree :tinit.~d j .re,chases~ l tca~ ~r,~rtottmel~l 'the ~' l , theFigure 4: With both left and right adjunctionspossible, the sentences can be parsed in a morelinguistically plausible wayTo generate non-linear structures, we need toallow adjunction in both left and right direc-tions.
The expanded LTIG tree set includes aleft auxiliary tree representation as well as rightfor each lexical item.
Moreover, we must mod-ify the topology of the auxiliary trees so thatadjunction in both directions can occur.
We in-sert an intermediary node between the root andthe lexical word.
At this internal node, at mostone adjunction of each direction may take place.The introduction of this node is necessary be-cause the definition of the formalism disallowsright adjunction into the root node of a left aux-iliary tree and vice versa.
For the sake of unifor-mity, we shall disallow adjunction into the rootnodes of the auxiliary trees from now on.
Figure3 shows an LTIG that allows at most one leftand one right adjunction for each elementarytree.
This enhanced LTIG can produce hierar-chical structures that the bigram model couldnot (See Figure 4.
)It is, however, still too limiting to allowonly one adjunction from each direction.
Many560words often require more than one modifier.
Forexample, a transitive verb such as "give" takesat least two adjunctions: a direct object nounphrase, an indirect object noun phrase, and pos-sibly other adverbial modifiers.
To create moreadjunct/on sites for each word, we introduce yetmore intermediary nodes between the root andthe lexical word.
Our empirical studies showthat each lexicalized auxiliary tree requires atleast 3 adjunction sites to parse all the sentencesin the corpora.
Figure 5(a) and (b) show twoexamples of auxiliary trees with 3 adjunctionsites.
The number of parameters in a PLTIGis dependent on the number of adjunction sitesjust as the size of a PCFG is dependent on thenumber of nonterminals.
For a language withV vocabulary items, the number of parametersfor the type of PLTIGs used in this paper is2(V+I)+2V(K)(V+I),  where K is the numberof adjunction sites per tree.
The first term ofthe equation is the number of parameters con-tributed by the initial tree, which always hastwo adjunction sites in our experiments.
Thesecond term is the contribution from the aux-iliary trees.
There are 2V auxiliary trees, eachtree has K adjunction sites; and V + 1 param-eters describe the distribution of adjunction ateach site.
The number of parameters ofa PCFGwith M nonterminals i  M 3 + MV.
For the ex-periments, we try to choose values of K and Mfor the PLTIGs and PCFGs such that2(Y + 1) + 2Y(g)(Y  + 1) ~ M 3 + MY3.2 ATISTo reproduce the results of PCFGs reported byPereira and Schabes, we use the ATIS corpusfor our first experiment.
This corpus contains577 sentences with 32 part-of-speech tags.
Toensure statistical significance, we generate tenrandom train-test splits on the corpus.
Eachset randomly partitions the corpus into threesections according to the following distribution:80% training, 10% held-out, and 10% testing.This gives us, on average, 406 training sen-tences, 83 testing sentences, and 88 sentencesfor held-out esting.
The results reported hereare the averages of ten runs.We have trained three types of PLTIGs, vary-ing the number of left and right adjunction sites.The L2R1 version has two left adjunction sitesand one right adjunction site; L1R2 has onetlw?rd nXx x.word  nre word nXx.
?
L\word n(a)tlwo;,d nXword nrrwordnX5xtword n(b)tlw?rd nXword n~'word nXx.
sx \word nl(c)\]t11 .
.
.
.
.No.
of  ~I I40 45  r~O?
, .
I F~-  m" t .2R l "  - - - -%2R2"  .
.
.
.
.
.
"PCFG1 S" - -"PCFG2~'IFigure 6: Average convergence rates of thetraining process for 3 PLTIGs and 2 PCFGs.Figure 5: Prototypical auxiliary trees for threePLTIGs: (a) L1R2, (b) L2R1, and (c) L2R2.left adjunction site and two right adjunctionsites; L2R2 has two of each.
The prototypi-cal auxiliary trees for these three grammars areshown in Figure 5.
At the end of every train-ing iteration, the updated grammars are usedto parse sentences in the held-out est sets D,and the new language modeling scores (by mea-suring the cross-entropy estimates f/(D, L2R1),f/(D, L1R2), and/ / (D ,  L2R2)) are calculated.The rate of improvement of the language model-ing scores determines convergence.
The PLTIGsare compared with two PCFGs: one with15-nonterminals, as Pereira and Schabes havedone, and one with 20-nonterminals, which hascomparable number of parameters to L2R2, thelarger PLTIG.In Figure 6 we plot the average iterativeimprovements of the training process for eachgrammar.
All training processes of the PLTIGsconverge much faster (both in numbers of itera-tions and in real time) than those of the PCFGs,even when the PCFG has fewer parameters toestimate, as shown in Table 1.
From Figure 6,we see that both PCFGs take many more iter-ations to converge and that the cross-entropyvalue they converge on is much higher than thePLTIGs.During the testing phase, the trained gram-mars are used to produce bracketed constituentson unmarked sentences from the testing setsT.
We use the crossing bracket metric toevaluate the parsing quality of each gram-mar.
We also measure the cross-entropy es-timates \[-I(T, L2R1), f-I(T, L1R2),H(T, L2R2),f-I(T, PCFG:5), and fI(T, PCFG2o) to deter-mine the quality of the language model.
Fora baseline comparison, we consider bigram andtrigram models with simple right branchingbracketing heuristics.
Our findings are summa-rized in Table 1.The three types of PLTIGs generate roughlythe same number of bracketed constituent errorsas that of the trained PCFGs, but they achievea much lower entropy score.
While the averageentropy value of the trigram model is the low-est, there is no statistical significance between itand any of the three PLTIGs.
The relative sta-tistical significance between the various types ofmodels is presented in Table 2.
In any case, theslight language modeling advantage of the tri-gram model is offset by its inability to handleparsing.Our ATIS results agree with the findings ofPereira and Schabes that concluded that theperformances of the PCFGs do not seem to de-pend heavily on the number of parameters oncea certain threshold is crossed.
Even thoughPCFG2o has about as many number of param-eters as the larger PLTIG (L2R2), its languagemodeling score is still significantly worse thanthat of any of the PLTIGs.561I\[ Bigram/Trigram PCFG 15Number of parameters 1088 / 34880 3855- 45 Iterations to convergenceReal-time convergence (min) - 62\[-I(T, Grammar) 2.88 / 2.71 3.81Crossing bracket (on T) 66.78 93.46PCFG201L1R21L2R1 I L2R28640 6402 6402 851445 19 17 24142 8 7 143.42 2.87 2.85 2.7893.41 93.07 93.28 94.51Table 1: Summary results for ATIS.
The machine used to measure real-time is an HP 9000/859.Number ofparametersBigram/Trigram2400 / 115296PCFG 154095PCFG 208960PCFG 23\[ LIR2 I L2R1 I L2R213271Iterations to - 80 60 70convergenceReal-time con- - 143 252 511vergence (hr).f-I(T, Grammar 3.39/3.20 4.31 4.27 4.13Crossing 49.44 56.41 78.82 79.30bracket (T)14210 14210 1891428 30 2838 41 603.58 3.56 3.5980.08 82.43 80.832Table 3: Summary results of the training phase for WSJPLTIGs II betterbigram better -trigram better - betterI\[ PCFGs PLTIGs bigramTable 2: Summary of pair-wise t-test for allgrammars.
If "better" appears at cell (i,j), thenthe model in row i has an entropy value lowerthan that of the model in column j in a statis-tically significant way.
The symbol "-" denotesthat the difference of scores between the modelsbears no statistical significance.3.3  WSJBecause the sentences in ATIS are short withsimple and similar structures, the difference inperformance between the formalisms may notbe as apparent.
For the second experiment,we use the Wall Street Journal (WSJ) corpus,whose sentences are longer and have more var-ied and complex structures.
We use sections02 to 09 of the WSJ corpus for training, sec-tion 00 for held-out data D, and section 23 fortest T. We consider sentences of length 40 orless.
There are 13242 training sentences, 1780sentences for the held-out data, and 2245 sen-tences in the test.
The vocabulary set con-sists of the 48 part-of-speech tags.
We comparethree variants of PCFGs (15 nonterminals, 20nonterminals, and 23 nonterminals) with threevariants of PLTIGs (L1R2, L2R1, L2R2).
APCFG with 23 nonterminals i  included becauseits size approximates that of the two smallerPLTIGs.
We did not generate random train-test splits for the WSJ corpus because it is largeenough to provide adequate sampling.
Table3 presents our findings.
From Table 3, we seeseveral similarities to the results from the ATIScorpus.
All three variants of the PLTIG formal-ism have converged at a faster rate and havefar better language modeling scores than any ofthe PCFGs.
Differing from the previous experi-ment, the PLTIGs produce slightly better cross-ing bracket rates than the PCFGs on the morecomplex WSJ corpus.
At least 20 nonterminalsare needed for a PCFG to perform in leaguewith the PLTIGs.
Although the PCFGs havefewer parameters, the rate seems to be indiffer-ent to the size of the grammars after a thresh-old has been reached.
While upping the numberof nonterminal symbols from 15 to 20 led to a22.4% gain, the improvement from PCFG2o toPCFG23 is only 0.5%.
Similarly for PLTIGs,L2R2 performs worse than L2R1 even though ithas more parameters.
The baseline comparisonfor this experiment results in more extreme out-comes.
The right branching heuristic receives a562crossing bracket rate of 49.44%, worse than eventhat of PCFG15.
However, the N-gram modelshave better cross-entropy measurements thanPCFGs and PLTIGs; bigram has a score of 3.39bits per word, and trigram has a score of 3.20bits per word.
Because the lexical relationshipmodeled by the PLTIGs presented in this pa-per is limited to those between two words, theirscores are close to that of the bigram model.4 Conc lus ion and Future  WorkIn this paper, we have presented the resultsof two empirical experiments u ing Probabilis-tic Lexicalized Tree Insertion Grammars.
Com-paring PLTIGs with PCFGs and N-grams, ourstudies show that a lexicalized tree represen-tation drastically improves the quality of lan-guage modeling of a context-free grammar tothe level of N-grams without degrading theparsing accuracy.
In the future, we hope tocontinue to improve on the quality of parsingand language modeling by making more useof the lexical information.
For example, cur-rently, the initial untrained PLTIGs consist ofelementary trees that have uniform configura-tions (i.e., every auxiliary tree has the samenumber of adjunction sites) to mirror the CNFrepresentation f PCFGs.
We hypothesize thata grammar consisting of a set of elementarytrees whose number of adjunction sites dependon their lexical anchors would make a closer ap-proximation to the "true" grammar.
We alsohope to apply PLTIGs to natural language tasksthat may benefit from a good language model,such as speech recognition, machine translation,message understanding, and keyword and topicspotting.ReferencesEugene Charniak.
1997.
Statistical parsingwith a context-free grammar and word statis-tics.
In Proceedings of the AAAI, pages 598-603, Providence, RI.
AAAI Press/MIT Press.Michael Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Pro-ceedings of the 35th Annual Meeting of theACL, pages 16-23, Madrid, Spain.Joshua Goodman.
1997.
Probabilistic fea-ture grammars.
In Proceedings of the Inter-national Workshop on Parsing Technologies1997.Rebecca Hwa.
1998.
An empirical evaluation ofprobabilistic lexicalized tree insertion gram-mars.
Technical Report 06-98, Harvard Uni-versity.
Full Version.K.
Lari and S.J.
Young.
1990.
The estima-tion of stochastic ontext-free grammars us-ing the inside-outside algorithm.
ComputerSpeech and Language, 4:35-56.David Magerman.
1995.
Statistical decision-models for parsing.
In Proceedings ofthe 33rdAnnual Meeting of the A CL, pages 276-283,Cambridge, MA.Fernando Pereira and Yves Schabes.
1992.Inside-Outside reestimation from partiallybracketed corpora.
In Proceedings ofthe 30thAnnual Meeting of the ACL, pages 128-135,Newark, Delaware.S.
Rajasekaran and S. Yooseph.
1995.
Talrecognition i O(M(n2)) time.
In Proceedingsof the 33rd Annual Meeting of the A CL, pages166-173, Cambridge, MA.Y.
Schabes and R. Waters.
1993.
Stochasticlexicalized context-free grammar.
In Proceed-ings of the Third International Workshop onParsing Technologies, pages 257-266.Y.
Schabes and R. Waters.
1994.
Tree insertiongrammar: A cubic-time parsable formalismthat lexicalizes context-free grammar withoutchanging the tree produced.
Technical Re-port TR-94-13, Mitsubishi Electric ResearchLaboratories.Y.
Schabes, A. Abeille, and A. K. Joshi.
1988.Parsing strategies with 'lexicalized' gram-mars: Application to tree adjoining gram-mars.
In Proceedings of the 1Pth Interna-tional Conference on Computational Linguis-tics (COLING '88), August.Yves Schabes.
1990.
Mathematical nd Com-putational Aspects of Lexicalized Grammars.Ph.D.
thesis, University of Pennsylvania, Au-gust.563
