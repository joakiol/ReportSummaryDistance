Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 45?55,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsHolistic Sentiment Analysis Across Languages:Multilingual Supervised Latent Dirichlet AllocationJordan Boyd-GraberUMD iSchooland UMIACSUniversity of MarylandCollege Park, MDjbg@umiacs.umd.eduPhilip ResnikDepartment of Linguisticsand UMIACSUniversity of MarylandCollege Park, MDresnik@umd.eduAbstractIn this paper, we develop multilingual super-vised latent Dirichlet alocation (MLSLDA),a probabilistic generative model that allowsinsights gleaned from one language?s data toinform how the model captures properties ofother languages.
MLSLDA accomplishes thisby jointly modeling two aspects of text: howmultilingual concepts are clustered into themat-ically coherent topics and how topics associ-ated with text connect to an observed regres-sion variable (such as ratings on a sentimentscale).
Concepts are represented in a generalhierarchical framework that is flexible enoughto express semantic ontologies, dictionaries,clustering constraints, and, as a special, degen-erate case, conventional topic models.
Boththe topics and the regression are discoveredvia posterior inference from corpora.
We showMLSLDA can build topics that are consistentacross languages, discover sensible bilinguallexical correspondences, and leverage multilin-gual corpora to better predict sentiment.Sentiment analysis (Pang and Lee, 2008) offersthe promise of automatically discerning how peoplefeel about a product, person, organization, or issuebased on what they write online, which is potentiallyof great value to businesses and other organizations.However, the vast majority of sentiment resourcesand algorithms are limited to a single language, usu-ally English (Wilson, 2008; Baccianella and Sebas-tiani, 2010).
Since no single language captures amajority of the content online, adopting such a lim-ited approach in an increasingly global communityrisks missing important details and trends that mightonly be available when text in multiple languages istaken into account.Up to this point, multiple languages have beenaddressed in sentiment analysis primarily by trans-ferring knowledge from a resource-rich language toa less rich language (Banea et al, 2008), or by ig-noring differences in languages via translation intoEnglish (Denecke, 2008).
These approaches are lim-ited to a view of sentiment that takes place throughan English-centric lens, and they ignore the poten-tial to share information between languages.
Ide-ally, learning sentiment cues holistically, across lan-guages, would result in a richer and more globallyconsistent picture.In this paper, we introduce Multilingual Super-vised Latent Dirichlet Allocation (MLSLDA), amodel for sentiment analysis on a multilingual cor-pus.
MLSLDA discovers a consistent, unified pictureof sentiment across multiple languages by learning?topics,?
probabilistic partitions of the vocabularythat are consistent in terms of both meaning and rel-evance to observed sentiment.
Our approach makesfew assumptions about available resources, requiringneither parallel corpora nor machine translation.The rest of the paper proceeds as follows.
In Sec-tion 1, we describe the probabilistic tools that we useto create consistent topics bridging across languagesand the MLSLDA model.
In Section 2, we presentthe inference process.
We discuss our set of seman-tic bridges between languages in Section 3, and ourexperiments in Section 4 demonstrate that this ap-proach functions as an effective multilingual topicmodel, discovers sentiment-biased topics, and usesmultilingual corpora to make better sentiment pre-dictions across languages.
Sections 5 and 6 discussrelated research and discusses future work, respec-tively.451 Predictions from Multilingual TopicsAs its name suggests, MLSLDA is an extension ofLatent Dirichlet alocation (LDA) (Blei et al, 2003),a modeling approach that takes a corpus of unan-notated documents as input and produces two out-puts, a set of ?topics?
and assignments of documentsto topics.
Both the topics and the assignments areprobabilistic: a topic is represented as a probabilitydistribution over words in the corpus, and each doc-ument is assigned a probability distribution over allthe topics.
Topic models built on the foundations ofLDA are appealing for sentiment analysis becausethe learned topics can cluster together sentiment-bearing words, and because topic distributions are aparsimonious way to represent a document.1LDA has been used to discover latent structurein text (e.g.
for discourse segmentation (Purver etal., 2006) and authorship (Rosen-Zvi et al, 2004)).MLSLDA extends the approach by ensuring that thislatent structure ?
the underlying topics ?
is consis-tent across languages.
We discuss multilingual topicmodeling in Section 1.1, and in Section 1.2 we showhow this enables supervised regression regardless ofa document?s language.1.1 Capturing Semantic CorrelationsTopic models posit a straightforward generative pro-cess that creates an observed corpus.
For each docu-ment d, some distribution ?d over unobserved topicsis chosen.
Then, for each word position in the doc-ument, a topic z is selected.
Finally, the word forthat position is generated by selecting from the topicindexed by z.
(Recall that in LDA, a ?topic?
is adistribution over words).In monolingual topic models, the topic distributionis usually drawn from a Dirichlet distribution.
Us-ing Dirichlet distributions makes it easy to specifysparse priors, and it also simplifies posterior infer-ence because Dirichlet distributions are conjugateto multinomial distributions.
However, drawing top-ics from Dirichlet distributions will not suffice ifour vocabulary includes multiple languages.
If weare working with English, German, and Chinese atthe same time, a Dirichlet prior has no way to fa-vor distributions z such that p(good|z), p(gut|z), and1The latter property has also made LDA popular for infor-mation retrieval (Wei and Croft, 2006)).p(ha?o|z) all tend to be high at the same time, or lowat the same time.
More generally, the structure of ourmodel must encourage topics to be consistent acrosslanguages, and Dirichlet distributions cannot encodecorrelations between elements.One possible solution to this problem is to use themultivariate normal distribution, which can producecorrelated multinomials (Blei and Lafferty, 2005),in place of the Dirichlet distribution.
This has beendone successfully in multilingual settings (Cohenand Smith, 2009).
However, such models complicateinference by not being conjugate.Instead, we appeal to tree-based extensions of theDirichlet distribution, which has been used to inducecorrelation in semantic ontologies (Boyd-Graber etal., 2007) and to encode clustering constraints (An-drzejewski et al, 2009).
The key idea in this ap-proach is to assume the vocabularies of all languagesare organized according to some shared semanticstructure that can be represented as a tree.
For con-creteness in this section, we will use WordNet (Miller,1990) as the representation of this multilingual se-mantic bridge, since it is well known, offers conve-nient and intuitive terminology, and demonstrates thefull flexibility of our approach.
However, the modelwe describe generalizes to any tree-structured rep-resentation of multilingual knowledge; we discusssome alternatives in Section 3.WordNet organizes a vocabulary into a rooted, di-rected acyclic graph of nodes called synsets, short for?synonym sets.?
A synset is a child of another synsetif it satisfies a hyponomy relationship; each child ?isa?
more specific instantiation of its parent concept(thus, hyponomy is often called an ?isa?
relationship).For example, a ?dog?
is a ?canine?
is an ?animal?
isa ?living thing,?
etc.
As an approximation, it is notunreasonable to assume that WordNet?s structure ofmeaning is language independent, i.e.
the conceptencoded by a synset can be realized using terms indifferent languages that share the same meaning.
Inpractice, this organization has been used to createmany alignments of international WordNets to theoriginal English WordNet (Ordan and Wintner, 2007;Sagot and Fis?er, 2008; Isahara et al, 2008).Using the structure of WordNet, we can now de-scribe a generative process that produces a distribu-tion over a multilingual vocabulary, which encour-ages correlations between words with similar mean-46ings regardless of what language each word is in.For each synset h, we create a multilingual worddistribution for that synset as follows:1.
Draw transition probabilities ?h ?
Dir (?h)2.
Draw stop probabilities ?h ?
Dir (?h)3.
For each language l, draw emission probabilities forthat synset ?h,l ?
Dir (pih,l).For conciseness in the rest of the paper, we will referto this generative process as multilingual Dirichlethierarchy, or MULTDIRHIER(?
,?,pi).2 Each ob-served token can be viewed as the end result of asequence of visited synsets ?.
At each node in thetree, the path can end at node i with probability ?i,1,or it can continue to a child synset with probability?i,0.
If the path continues to another child synset, itvisits child j with probability ?i,j .
If the path ends ata synset, it generates word k with probability ?i,l,k.3The probability of a word being emitted from a pathwith visited synsets r and final synset h in languagel is thereforep(w, ?
= r, h|l,?,?,?)
=???(i,j)?r?i,j?i,0??
(1?
?h,1)?h,l,w.
(1)Note that the stop probability ?h is independent oflanguage, but the emission ?h,l is dependent on thelanguage.
This is done to prevent the following sce-nario: while synset A is highly probable in a topicand words in language 1 attached to that synset havehigh probability, words in language 2 have low prob-ability.
If this could happen for many synsets ina topic, an entire language would be effectively si-lenced, which would lead to inconsistent topics (e.g.2Variables ?h, pih,l, and ?h are hyperparameters.
Their meanis fixed, but their magnitude is sampled during inference (i.e.
?h,iPk ?h,kis constant, but ?h,i is not).
For the bushier bridges,(e.g.
dictionary and flat), their mean is uniform.
For GermaNet,we took frequencies from two balanced corpora of German andEnglish: the British National Corpus (University of Oxford,2006) and the Kern Corpus of the Digitales Wo?rterbuch derDeutschen Sprache des 20.
Jahrhunderts project (Geyken, 2007).We took these frequencies and propagated them through themultilingual hierarchy, following LDAWN?s (Boyd-Graber etal., 2007) formulation of information content (Resnik, 1995) asa Bayesian prior.
The variance of the priors was initialized to be1.0, but could be sampled during inference.3Note that the language and word are taken as given, but thepath through the semantic hierarchy is a latent random variable.Topic 1 is about baseball in English and about travelin German).
Separating path from emission helpsensure that topics are consistent across languages.Having defined topic distributions in a way that canpreserve cross-language correspondences, we nowuse this distribution within a larger model that candiscover cross-language patterns of use that predictsentiment.1.2 The MLSLDA ModelWe will view sentiment analysis as a regression prob-lem: given an input document, we want to predicta real-valued observation y that represents the senti-ment of a document.
Specifically, we build on super-vised latent Dirichlet alocation (SLDA, (Blei andMcAuliffe, 2007)), which makes predictions basedon the topics expressed in a document; this can bethought of projecting the words in a document to lowdimensional space of dimension equal to the numberof topics.
Blei et al showed that using this latenttopic structure can offer improved predictions over re-gressions based on words alone, and the approach fitswell with our current goals, since word-level cues areunlikely to be identical across languages.
In additionto text, SLDA has been successfully applied to otherdomains such as social networks (Chang and Blei,2009) and image classification (Wang et al, 2009).The key innovation in this paper is to extend SLDAby creating topics that are globally consistent acrosslanguages, using the bridging approach above.We express our model in the form of a probabilis-tic generative latent-variable model that generatesdocuments in multiple languages and assigns a real-valued score to each document.
The score comesfrom a normal distribution whose sum is the dot prod-uct between a regression parameter ?
that encodesthe influence of each topic on the observation anda variance ?2.
With this model in hand, we use sta-tistical inference to determine the distribution overlatent variables that, given the model, best explainsobserved data.The generative model is as follows:1.
For each topic i = 1 .
.
.K, draw a topic distribution{?i,?i,?i} from MULTDIRHIER(?
,?,pi).2.
For each document d = 1 .
.
.M with language ld:(a) Choose a distribution over topics ?d ?Dir (?
).47(b) For each word in the document n = 1 .
.
.
Nd,choose a topic assignment zd,n ?
Mult (?d)and a path ?d,n ending at word wd,n accordingto Equation 1 using {?zd,n ,?zd,n ,?zd,n}.3.
Choose a response variable from y ?Norm(?>z?, ?2), where z?d ?
1N?Nn=1 zd,n.Crucially, note that the topics are not indepen-dent of the sentiment task; the regression encouragesterms with similar effects on the observation y tobe in the same topic.
The consistency of topics de-scribed above allows the same regression to be donefor the entire corpus regardless of the language of theunderlying document.2 InferenceFinding the model parameters most likely to explainthe data is a problem of statistical inference.
We em-ploy stochastic EM (Diebolt and Ip, 1996), using aGibbs sampler for the E-step to assign words to pathsand topics.
After randomly initializing the topics,we alternate between sampling the topic and pathof a word (zd,n, ?d,n) and finding the regression pa-rameters ?
that maximize the likelihood.
We jointlysample the topic and path conditioning on all of theother path and document assignments in the corpus,selecting a path and topic with probabilityp(zn = k, ?n = r|z?n,?
?n, wn, ?, ?,?)
=p(yd|z, ?, ?
)p(?n = r|zn = k,?
?n, wn, ?
,?,pi)p(zn = k|z?n, ?).
(2)Each of these three terms reflects a different influenceon the topics from the vocabulary structure, the doc-ument?s topics, and the response variable.
In the nextparagraphs, we will expand each of them to derivethe full conditional topic distribution.As discussed in Section 1.1, the structure of thetopic distribution encourages terms with the samemeaning to be in the same topic, even across lan-guages.
During inference, we marginalize over pos-sible multinomial distributions ?, ?, and ?, usingthe observed transitions from i to j in topic k; Tk,i,j ,stop counts in synset i in topic k, Ok,i,0; continuecounts in synsets i in topic k, Ok,i,1; and emissioncounts in synset i in language l in topic k, Fk,i,l.
TheHLMN?dzd,n?d,n?wd,n?
?ydK?i,h?h?i,h?h?i,h,l?h,lMultilingual Topics Text Documents Sentiment PredictionFigure 1: Graphical model representing MLSLDA.Shaded nodes represent observations, plates denote repli-cation, and lines show probabilistic dependencies.probability of taking a path r is thenp(?n = r|zn = k,?
?n) =?
(i,j)?r(Bk,i,j + ?i,j?j?
Bk,i,j?
+ ?i,jOk,i,1 + ?i?s?0,1Ok,i,s + ?i,s)?
??
?TransitionOk,rend,0 + ?rend?s?0,1Ok,rend,s + ?rend,sFk,rend,wn + pirend,l?w?
Frend,w?
+ pirend,w??
??
?Emission.
(3)Equation 3 reflects the multilingual aspect of thismodel.
The conditional topic distribution forSLDA (Blei and McAuliffe, 2007) replaces this termwith the standard Multinomial-Dirichlet.
However,we believe this is the first published SLDA-stylemodel using MCMC inference, as prior work hasused variational inference (Blei and McAuliffe, 2007;Chang and Blei, 2009; Wang et al, 2009).Because the observed response variable dependson the topic assignments of a document, the condi-tional topic distribution is shifted toward topics thatexplain the observed response.
Topics that move thepredicted response y?d toward the true yd will be fa-vored.
We drop terms that are constant across all48topics for the effect of the response variable,p(yd|z, ?, ?)
?exp[1?2(yd ??k?
Nd,k??k??k?
Nd,k?)?zk?k?
Nd,k?]?
??
?Other words?
influenceexp[??2zk2?2?k?
N2d,k?]?
??
?This word?s influence.
(4)The above equation represents the supervised aspectof the model, which is inherited from SLDA.Finally, there is the effect of the topics alreadyassigned to a document; the conditional distributionfavors topics already assigned in a document,p(zn = k|z?n, ?)
=Td,k + ?k?k?
Td,k?
+ ?k?.
(5)This term represents the document focus of thismodel; it is present in all Gibbs sampling inferenceschemes for LDA (Griffiths and Steyvers, 2004).Multiplying together Equations 3, 4, and 5 allowsus to sample a topic using the conditional distributionfrom Equation 2, based on the topic and path of theother words in all languages.
After sampling thepath and topic for each word in a document, we thenfind new regression parameters ?
that maximize thelikelihood conditioned on the current state of thesampler.
This is simply a least squares regressionusing the topic assignments z?d to predict yd.Prediction on documents for which we don?t havean observed yd is equivalent to marginalizing overyd and sampling topics for the document from Equa-tions 3 and 5.
The prediction for yd is then the dotproduct of ?
and the empirical topic distribution z?d.We initially optimized all hyperparameters usingslice sampling.
However, we found that the regres-sion variance ?2 was not stable.
Optimizing ?2 seemsto balance between modeling the language in the doc-uments and the prediction, and thus is sensitive todocuments?
length.
Given this sensitivity, we didnot optimize ?2 for our prediction experiments inSection 4, but instead kept it fixed at 0.25.
We leaveoptimizing this variable, either through cross valida-tion or adapting the model, to future work.3 Bridges Across LanguagesIn Section 1.1, we described connections across lan-guages as offered by semantic networks in a generalway, using WordNet as an example.
In this section,we provide more specifics, as well as alternative waysof building semantic connections across languages.Flat First, we can consider a degenerate mappingthat is nearly equivalent to running SLDA indepen-dently across multiple languages, relating topics onlybased on the impact on the response variable.
Con-sider a degenerate tree with only one node, with allwords in all languages associated with that node.
Thisis consistent with our model, but there is really noshared semantic space, as all emitted words mustcome from this degenerate ?synset?
and the modelonly represents the output distribution for this singlenode.WordNet We took the alignment of GermaNet toWordNet 1.6 (Kunze and Lemnitzer, 2002) and re-moved all synsets that were had no mapped Germanwords.
Any German synsets that did not have Englishtranslations had their words mapped to the lowestextant English hypernym (e.g.
?beinbruch,?
a bro-ken leg, was mapped to ?fracture?).
We stemmedall words to account for inflected forms not beingpresent (Porter and Boulton, 1970).
An exampleof the paths for the German word ?wunsch?
(wish,request) is shown in Figure 2(a).Dictionaries A dictionary can be viewed as a manyto many mapping, where each entry ei maps oneor more words in one language si to one or morewords ti in another language.
Entries were takenfrom an English-German dictionary (Richter, 2008)a Chinese-English dictionary (Denisowski, 1997),and a Chinese-German dictionary (Hefti, 2005).
Aswith WordNet, the words in entries for English andGerman were stemmed to improve coverage.
Anexample for German is shown in Figure 2(b).Algorithmic Connections In addition to hand-curated connections across languages, one could alsoconsider automatic means of mapping across lan-guages, such as using edit distance or local con-text (Haghighi et al, 2008; Rapp, 1995) or us-ing a lexical translation table obtained from paral-lel text (Melamed, 1998).
While we experimented49wish.n.04wish wunschentity.n.01entitiabstraction.n.06cognition.n.01 event.n.01event ereignis vorgang act.n.02deed act handlungspeech_act.n.01request.n.02option.n.02ask request anfrag wunschaltern option choic optionpreference.n.03objekt(a) GermaNetdict.1room gelassrootdict.2room raum platzroom zimm raumdict.3stub(b) DictionaryFigure 2: Two methods for constructing multilingual distributions over words.
On the left, paths to the German word?wunsch?
in GermaNet are shown.
On the right, paths to the English word ?room?
are shown.
Both English and Germanwords are shown; some internal nodes in GermaNet have been omitted for space (represented by dashed lines).
Notethat different senses are denoted by different internal paths, and that internal paths are distinct from the per-languageexpression.with these techniques, constructing appropriate hier-archies from these resources required many arbitrarydecisions about cutoffs and which words to include.Thus, we do not consider them in this paper.4 ExperimentsWe evaluate MLSLDA on three criteria: how wellit can discover consistent topics across languagesfor matching parallel documents, how well it candiscover sentiment-correlated word lists from non-aligned text, and how well it can predict sentiment.4.1 Matching on Multilingual TopicsWe took the 1996 documents from the Europarl cor-pus (Koehn, 2005) using three bridges: GermaNet,dictionary, and the uninformative flat matching.4 Themodel is unaware that the translations of documentsin one language are present in the other language.Note that this does not use the supervised framework4For English and German documents in all experiments,we removed stop words (Loper and Bird, 2002), stemmedwords (Porter and Boulton, 1970), and created a vocabularyof the most frequent 5000 words per language (this vocabularylimit was mostly done to ensure that the dictionary-based bridgewas of manageable size).
Documents shorter than fifty contentwords were excluded.
(as there is no associated response variable for Eu-roparl documents); this experiment is to demonstratethe effectiveness of the multilingual aspect of themodel.
To test whether the topics learned by themodel are consistent across languages, we representeach document using the probability distribution ?dover topic assignments.
Each ?d is a vector of lengthK and is a language-independent representation ofthe document.For each document in one language, we computedthe Hellinger distance between it and all of the docu-ments in the other language and sorted the documentsby decreasing distance.
The translation of the docu-ment is somewhere in that set; the higher the normal-ized rank (the percentage of documents with a ranklower than the translation of the document), the betterthe underlying topic model connects languages.We compare three bridges against what is to ourknowledge the only other topic model for unalignedtext, Multilingual Topics for Unaligned Text (Boyd-Graber and Blei, 2009).55The bipartite matching was initialized with the dictionaryweights as specified by the Multilingual Topics for UnalignedText algorithm.
The matching size was limited to 250 and thebipartite matching was only updated on the initial iteration thenheld fixed.
This yielded results comparable to when the matching50Average Parallel Document RankBridgeFlatGermaNetMuToDictionaryFlatGermaNetMuToDictionaryFlatGermaNetMuToDictionaryFlatGermaNetMuToDictionary0.0 0.2 0.4 0.6 0.85255075Figure 3: Average rank of paired translation documentrecovered from the multilingual topic model.
Randomguessing would yield 0.5; MLSLDA with a dictionarybased matching performed best.Figure 3 shows the results of this experiment.
Thedictionary-based bridge had the best performance onthe task, ranking a large proportion of documents(0.95) below the translated document once enoughtopics were available.
Although GermaNet is richer,its coverage is incomplete; the dictionary structurehad a much larger vocabulary and could build a morecomplete multilingual topics.
Using comparable in-put information, this more flexible model performedbetter on the matching task than the existing multi-lingual topic model available for unaligned text.
Thedegenerate flat bridge did no better than the baselineof random guessing, as expected.4.2 Qualitative Sentiment-Correlated TopicsOne of the key tasks in sentiment analysis has beenthe collection of lists of words that convey senti-ment (Wilson, 2008; Riloff et al, 2003).
Theseresources are often created using or in referenceto resources like WordNet (Whitelaw et al, 2005;Baccianella and Sebastiani, 2010).
MLSLDA pro-vides a method for extracting topical and sentiment-correlated word lists from multilingual corpora.
Ifwas updated more frequently.a WordNet-like resource is used as the bridge, theresulting topics are distributions over synsets, not justover words.As our demonstration corpus, we used the AmherstSentiment Corpus (Constant et al, 2009), as it hasdocuments in multiple languages (English, Chinese,and German) with numerical assessments of senti-ment (number of stars assigned to the review).
Wesegmented the Chinese text (Tseng et al, 2005) andused a classifier trained on character n-grams to re-move English-language documents that were mixedin among the Chinese and German language reviews.Figure 4 shows extracted topics from German-English and German-Chinese corpora.
MLSLDAis able to distinguish sentiment-bearing topics fromcontent bearing topics.
For example; in the German-English corpus, ?food?
and ?children?
topics arenot associated with a consistent sentiment signal,while ?religion?
is associated with a more negativesentiment.
In contrast, in the German-Chinese cor-pus, the ?religion/society?
topic is more neutral, andthe gender-oriented topic is viewed more negatively.Negative sentiment-bearing topics have reasonablewords such as ?pages,?
?ko?ng pa`?
(Chinese for ?I?mafraid that .
.
.
?)
and ?tuo?
(Chienese for ?discard?
),and positive sentiment-bearing topics have reason-able words such as ?great,?
?good,?
and ?juwel?
(Ger-man for ?jewel?
).The qualitative topics also betray some of theweaknesses of the model.
For example, in one ofthe negative sentiment topics, the German word ?gut?
(good) is present.
Because topics are distributionsover words, they can encode the presence of nega-tions like ?kein?
(no) and ?nicht?
(not), but not collo-cations like ?nicht gut.?
More elaborate topic modelsthat can model local syntax and collocations (John-son, 2010) provide options for addressing such prob-lems.We do not report the results for sentiment predic-tion for this corpus because the baseline of predictinga positive review is so strong; most algorithms do ex-tremely well by always predicting a positive review,ours included.4.3 Sentiment PredictionWe gathered 330 film reviews from a German filmreview site (Vetter et al, 2000) and combined themwith a much larger English film review corpus of over510.0-0.4-0.8-1.2 0.40.81.2himmelgedankenglaubeunserekirchewahrheitgodusreligionchurchhumanbuchromanleserseitengeschichtehandlungbooknovelreaderpagesbookstaleessendi?tverlierenbefindenk?rperrezeptedietfoodeatweighteatinghealthyfatbuchimmerlebenartlesenthemaautorbookbooksonelifepersonpeoplekindkinderelternbabynachtchildrenbabychildparentssleepfilmfilmeepisodestarstorygibtmoviefilmepisodemoviesscenesseparategivesgesellschaftgenau?berzeugtergebnismittelverlangengreatgoodbusinessallonecompaniesright(a) German / English-0.4-0.8-1.2-1.6 0.0 0.4 0.8??
(god)??
(lord)?
(both)??
(religion)??
(science)??
(community)(god) gott(lord) herr(religion) religion(universe) all(world) welt(science) wissenschaft(medicine) medizin(society) gesellschaft?
(good)?
(set)?
(treasure)?
(handsome)?
(both)??
(story)?
(small)(good) gut(sentence) satz(two) zwei(story) story(treasure) schatz(attractive) attraktiv(elegant) elegant(gem) juwel(book) buch(itself) sich(that) dass(much) viel(no) kein(good) gut(when) wenn?
(book) ??
([I'm afraid that...])?
(myself)?
(both)???
(mostly)?
(book)??
([really isn't])?
(discard)(woman) frau(point) punkt(man) mann(equal) gleich(fast) schnell(female) weiblich(soon) bald?
(quick)?
(a little)??
(woman)??
(man)?
(female)?
(male)??
(female)?
(both)??
(harry)?
(belt)?
(sky)?
(both)?
(section)???
(vampire)??
(strong)??
(last)(harry) harry(volume) band(sky) himmel (universe) all(vampire) vampir(last) letzt(part) teil(b) German / ChineseFigure 4: Topics, along with associated regression coefficient ?
from a learned 25-topic model on German-English (left)and German-Chinese (right) documents.
Notice that theme-related topics have regression parameter near zero, topicsdiscussing the number of pages have negative regression parameters, topics with ?good,?
?great,?
?ha?o?
(good) and?u?berzeugt?
(convinced) have positive regression parameters.
For the German-Chinese corpus, note the presence of ?gut?
(good) in one of the negative sentiment topics, showing the difficulty of learning collocations.Train Test GermaNet Dictionary FlatDE DE 73.8 24.8 92.2EN DE 7.44 2.68 18.3EN + DE DE 1.17 1.46 1.39Table 1: Mean squared error on a film review corpus.All results are on the same German test data, varying thetraining data.
Over-fitting prevents the model learning onthe German data alone; adding English data to the mixallows the model to make better predictions.5000 film reviews (Pang and Lee, 2005) to create amultilingual film review corpus.6The results for predicting sentiment in Germandocuments with 25 topics are presented in Table 1.On a small monolingual corpus, prediction is verypoor.
The model over-fits, especially when it hasthe entire vocabulary to select from.
The slightlybetter performance using GermaNet and a dictionaryas topic priors can be viewed as basic feature selec-tion, removing proper names from the vocabulary to6We followed Pang and Lee?s method for creating a nu-merical score between 0 and 1 from a star rating.
Wethen converted that to an integer by multiplying by 100;this was done because initial data preprocessing assumedinteger values (although downstream processing did not as-sume integer values).
The German movie review corpusis available at http://www.umiacs.umd.edu/?jbg/static/downloads_and_media.htmlprevent over-fitting.One would expect that prediction improves with alarger training set.
For this model, such an improve-ment is seen even when the training set includes nodocuments in the target language.
Note that even thedegenerate flat bridge across languages provides use-ful information.
After introducing English data, themodel learns to prefer smaller regression parameters(this can be seen as a form of regularization).Performance is best when a reasonably large cor-pus is available including some data in the targetlanguage.
For each bridge, performance improvesdramatically, showing that MLSLDA is successfullyable to incorporate information learned from bothlanguages to build a single, coherent picture of howsentiment is expressed in both languages.
With theGermaNet bridge, performance is better than boththe degenerate and dictionary based bridges, showingthat the model is sharing information both throughthe multilingual topics and the regression parameters.Performance on English prediction is comparableto previously published results on this dataset (Bleiand McAuliffe, 2007); with enough data, a monolin-gual model is no longer helped by adding additionalmultilingual data.525 Relationship to Previous ResearchThe advantages of MLSLDA reside largely in theassumptions that it makes and does not make: docu-ments need not be parallel, sentiment is a normallydistributed document-level property, words are ex-changeable, and sentiment can be predicted as a re-gression on a K-dimensional vector.By not assuming parallel text, this approach canbe applied to a broad class of corpora.
Other mul-tilingual topic models require parallel text, either atthe document (Ni et al, 2009; Mimno et al, 2009)or word-level (Kim and Khudanpur, 2004; Zhao andXing, 2006).
Similarly, other multilingual sentimentapproaches also require parallel text, often suppliedvia automatic translation; after the translated textis available, either monolingual analysis (Denecke,2008) or co-training is applied (Wan, 2009).
In con-trast, our approach requires fewer resources for a lan-guage: a dictionary (or similar knowledge structurerelating words to nodes in a graph) and comparabletext, instead of parallel text or a machine translationsystem.Rather than viewing one language through thelens of another language, MLSLDA views all lan-guages through the lens of the topics present in adocument.
This is a modeling decision with pros andcons.
It allows a language agnostic decision aboutsentiment to be made, but it restricts the expressive-ness of the model in terms of sentiment in two ways.First, it throws away information important to sen-timent analysis like syntactic constructions (Greeneand Resnik, 2009) and document structure (McDon-ald et al, 2007) that may impact the sentiment rating.Second, a single real number is not always sufficientto capture the nuances of sentiment.
Less critically,assuming that sentiment is normally distributed is nottrue of all real-world corpora; review corpora oftenhave a skew toward positive reviews.
We standardizeresponses by the mean and variance of the trainingdata to partially address this issue, but other responsedistributions are possible, such as generalized linearmodels (Blei and McAuliffe, 2007) and vector ma-chines (Zhu et al, 2009), which would allow moretraditional classification predictions.Other probabilistic models for sentiment classifi-cation view sentiment as a word level feature.
Somemodels use sentiment word lists, either given orlearned from a corpus, as a prior to seed topics sothat they attract other sentiment bearing words (Meiet al, 2007; Lin and He, 2009).
Other approachesview sentiment or perspective as a perturbation ofa log-linear topic model (Lin et al, 2008).
Suchtechniques could be combined with the multilingualapproach presented here by using distributions overwords that not only bridge different languages butalso encode additional information.
For example, thevocabulary hierarchies could be structured to encour-age topics that encourage correlation among similarsentiment-bearing words (e.g.
clustering words asso-ciated with price, size, etc.).
Future work could alsomore rigorously validate that the multilingual topicsdiscovered by MLSLDA are sentiment-bearing viahuman judgments.In contrast, MLSLDA draws on techniques thatview sentiment as a regression problem based on thetopics used in a document, as in supervised latentDirichlet alocation (SLDA) (Blei and McAuliffe,2007) or in finer-grained parts of a document (Titovand McDonald, 2008).
Extending these models tomultilingual data would be more straightforward.6 ConclusionsMLSLDA is a ?holistic?
statistical model for multi-lingual corpora that does not require parallel textor expensive multilingual resources.
It discoversconnections across languages that can recover la-tent structure in parallel corpora, discover sentiment-correlated word lists in multiple languages, and makeaccurate predictions across languages that improvewith more multilingual data, as demonstrated in thecontext of sentiment analysis.More generally, MLSLDA provides a formalismthat can be used to incorporate the many insights oftopic modeling-driven sentiment analysis to multi-lingual corpora by tying together word distributionsacross languages.
MLSLDA can also contribute tothe development of word list-based sentiment sys-tems: the topics discovered by MLSLDA can serveas a first-pass means of sentiment-based word listsfor languages that might lack annotated resources.MLSLDA also can be viewed as a sentiment-informed multilingual word sense disambiguation(WSD) algorithm.
When the multilingual bridge is anexplicit representation of sense such as WordNet, part53of the generative process is an explicit assignmentof every word to sense (the path latent variable ?
);this is discovered during inference.
The dictionary-based technique may be viewed as a disambiguationvia a transfer dictionary.
How sentiment predictionimpacts the implicit WSD is left to future work.Better capturing local syntax and meaningful col-locations would also improve the model?s ability topredict sentiment and model multilingual topics, aswould providing a better mechanism for represent-ing words not included in our bridges.
We intend todevelop such models as future work.7 AcknowledgmentsThis research was funded in part by the Army Re-search Laboratory through ARL Cooperative Agree-ment W911NF-09-2-0072 and by the Office of theDirector of National Intelligence (ODNI), Intelli-gence Advanced Research Projects Activity (IARPA),through the Army Research Laboratory.
All state-ments of fact, opinion or conclusions containedherein are those of the authors and should not beconstrued as representing the official views or poli-cies of ARL, IARPA, the ODNI, or the U.S. Govern-ment.
The authors thank the anonymous reviewers,Jonathan Chang, Christiane Fellbaum, and LawrenceWatts for helpful comments.
The authors especiallythank Chris Potts for providing help in obtaining andprocessing reviews.ReferencesDavid Andrzejewski, Xiaojin Zhu, and Mark Craven.2009.
Incorporating domain knowledge into topic mod-eling via Dirichlet forest priors.
In ICML.Andrea Esuli Stefano Baccianella and Fabrizio Sebastiani.2010.
Sentiwordnet 3.0: An enhanced lexical resourcefor sentiment analysis and opinion mining.
In LREC.Carmen Banea, Rada Mihalcea, Janyce Wiebe, and SamerHassan.
2008.
Multilingual subjectivity analysis usingmachine translation.
In EMNLP.David M. Blei and John D. Lafferty.
2005.
Correlatedtopic models.
In NIPS.David M. Blei and Jon D. McAuliffe.
2007.
Supervisedtopic models.
In NIPS.
MIT Press.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
JMLR, 3:993?1022.Jordan Boyd-Graber and David M. Blei.
2009.
Multilin-gual topic models for unaligned text.
In UAI.Jordan Boyd-Graber, David M. Blei, and Xiaojin Zhu.2007.
A topic model for word sense disambiguation.In EMNLP.Jonathan Chang and David M. Blei.
2009.
Relationaltopic models for document networks.
In AISTATS.Shay B. Cohen and Noah A. Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tying inunsupervised grammar induction.
In NAACL.Noah Constant, Christopher Davis, Christopher Potts, andFlorian Schwarz.
2009.
The pragmatics of expressivecontent: Evidence from large corpora.
Sprache undDatenverarbeitung, 33(1?2).Kerstin Denecke.
2008.
Using SentiWordNet for multilin-gual sentiment analysis.
In ICDEW 2008.Paul Denisowski.
1997.
CEDICT.http://www.mdbg.net/chindict/.Jean Diebolt and Eddie H.S.
Ip, 1996.
Markov ChainMonte Carlo in Practice, chapter Stochastic EM:method and application.
Chapman and Hall, London.Alexander Geyken.
2007.
The DWDS corpus: A ref-erence corpus for the German language of the 20thcentury.
In Idioms and Collocations: Corpus-basedLinguistic, Lexicographic Studies.
Continuum Press.Stephan Greene and Philip Resnik.
2009.
More thanwords: Syntactic packaging and implicit sentiment.
InNAACL.Thomas L. Griffiths and Mark Steyvers.
2004.
Findingscientific topics.
PNAS, 101(Suppl 1):5228?5235.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, andDan Klein.
2008.
Learning bilingual lexicons frommonolingual corpora.
In ACL, Columbus, Ohio.Jan Hefti.
2005.
HanDeDict.
http://chdw.de.Hitoshi Isahara, Fransis Bond, Kiyotaka Uchimoto, MasaoUtiyama, and Kyoko Kanzaki.
2008.
Development ofthe Japanese WordNet.
In LREC.Mark Johnson.
2010.
PCFGs, topic models, adaptorgrammars and learning topical collocations and thestructure of proper names.
In ACL.Woosung Kim and Sanjeev Khudanpur.
2004.
Lexicaltriggers and latent semantic analysis for cross-linguallanguage model adaptation.
TALIP, 3(2):94?112.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In MT Summit.http://www.statmt.org/europarl/.Claudia Kunze and Lothar Lemnitzer.
2002.
Standardiz-ing WordNets in a web-compliant format: The case ofGermaNet.
In Workshop on Wordnets Structures andStandardisation.Chenghua Lin and Yulan He.
2009.
Joint sentiment/topicmodel for sentiment analysis.
In CIKM.Wei-Hao Lin, Eric Xing, and Alexander Hauptmann.2008.
A joint topic and perspective model for ideo-logical discourse.
In ECML PKDD.54Edward Loper and Steven Bird.
2002.
NLTK: the natu-ral language toolkit.
In Tools and methodologies forteaching.
ACL.Ryan McDonald, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured models forfine-to-coarse sentiment analysis.
In ACL.Qiaozhu Mei, Xu Ling, Matthew Wondra, Hang Su, andChengXiang Zhai.
2007.
Topic sentiment mixture:modeling facets and opinions in weblogs.
In WWW.Ilya Dan Melamed.
1998.
Empirical methods for exploit-ing parallel texts.
Ph.D. thesis, University of Pennsyl-vania.George A. Miller.
1990.
Nouns in WordNet: A lexicalinheritance system.
International Journal of Lexicog-raphy, 3(4):245?264.David Mimno, Hanna Wallach, Jason Naradowsky, DavidSmith, and Andrew McCallum.
2009.
Polylingualtopic models.
In EMNLP.Xiaochuan Ni, Jian-Tao Sun, Jian Hu, and Zheng Chen.2009.
Mining multilingual topics from Wikipedia.
InWWW.Noam Ordan and Shuly Wintner.
2007.
Hebrew Word-Net: a test case of aligning lexical databases across lan-guages.
International Journal of Translation, 19(1):39?58.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
In ACL.Bo Pang and Lillian Lee.
2008.
Opinion Mining andSentiment Analysis.
Now Publishers Inc.Martin Porter and Richard Boulton.
1970.
Snowballstemmer.
http://snowball.tartarus.org/credits.php.Matthew Purver, Konrad Ko?rding, Thomas L. Griffiths,and Joshua Tenenbaum.
2006.
Unsupervised topicmodelling for multi-party spoken discourse.
In ACL.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In ACL, pages 320?322.Philip Resnik.
1995.
Using information content to evalu-ate semantic similarity in a taxonomy.
In IJCAI, pages448?453.Frank Richter.
2008.
Dictionary nice grep.
http://www-user.tu-chemnitz.de/ fri/ding/.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction pattern boot-strapping.
In NAACL.Michal Rosen-Zvi, Thomas L. Griffiths, Mark Steyvers,and Padhraic Smyth.
2004.
The author-topic model forauthors and documents.
In UAI.Beno?
?t Sagot and Darja Fis?er.
2008.
Building a FreeFrench WordNet from Multilingual Resources.
In On-toLex.Ivan Titov and Ryan McDonald.
2008.
A joint model oftext and aspect ratings for sentiment summarization.
InACL, pages 308?316.Huihsin Tseng, Pichuan Chang, Galen Andrew, Daniel Ju-rafsky, and Christopher Manning.
2005.
A conditionalrandom field word segmenter.
In SIGHAN Workshopon Chinese Language Processing.University of Oxford.
2006.
British Na-tional Corpus.
http://www.natcorp.ox.ac.uk/.http://www.natcorp.ox.ac.uk/.Tobias Vetter, Manfred Sauer, and Philipp Wallutat.2000.
Filmrezension.de: Online-magazin fu?r filmkritik.http://www.filmrezension.de.Xiaojun Wan.
2009.
Co-training for cross-lingual senti-ment classification.
In ACL.Chong Wang, David Blei, and Li Fei-Fei.
2009.
Simulta-neous image classification and annotation.
In CVPR.Xing Wei and Bruce Croft.
2006.
LDA-based documentmodels for ad-hoc retrieval.
In SIGIR.Casey Whitelaw, Navendu Garg, and Shlomo Argamon.2005.
Using appraisal groups for sentiment analysis.In CIKM.Theresa Ann Wilson.
2008.
Fine-grained Subjectivity andSentiment Analysis: Recognizing the Intensity, Polarity,and Attitudes of Private States.
Ph.D. thesis, Universityof Pittsburgh.Bing Zhao and Eric P. Xing.
2006.
BiTAM: Bilingualtopic admixture models for word alignment.
In ACL.Jun Zhu, Amr Ahmed, and Eric P. Xing.
2009.
Medlda:maximum margin supervised topic models for regres-sion and classification.
In ICML.55
