Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1597?1607,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsMachine Translation Detection from Monolingual Web-TextYuki AraseMicrosoft Research AsiaNo.
5 Danling St., Haidian Dist.Beijing, P.R.
Chinayukiar@microsoft.comMing ZhouMicrosoft Research AsiaNo.
5 Danling St., Haidian Dist.Beijing, P.R.
Chinamingzhou@microsoft.comAbstractWe propose a method for automaticallydetecting low-quality Web-text translatedby statistical machine translation (SMT)systems.
We focus on the phrase saladphenomenon that is observed in existingSMT results and propose a set of computa-tionally inexpensive features to effectivelydetect such machine-translated sentencesfrom a large-scale Web-mined text.
Un-like previous approaches that require bilin-gual data, our method uses only monolin-gual text as input; therefore it is applicablefor refining data produced by a variety ofWeb-mining activities.
Evaluation resultsshow that the proposed method achievesan accuracy of 95.8% for sentences and80.6% for text in noisy Web pages.1 IntroductionThe Web provides an extremely large volumeof textual content on diverse topics and areas.Such data is beneficial for constructing a largescale monolingual (Microsoft Web N-gram Ser-vices, 2010; Google N-gram Corpus, 2006) andbilingual (Nie et al, 1999; Shi et al, 2006;Ishisaka et al, 2009; Jiang et al, 2009) corpusthat can be used for training statistical models forNLP tools, as well as for building a large-scaleknowledge-base (Suchanek et al, 2007; Zhu et al,2009; Fader et al, 2011; Nakashole et al, 2012).With recent advances in statistical machine trans-lation (SMT) systems and their wide adoption inWeb services through APIs (Microsoft Translator,2009; Google Translate, 2006), a large amountof text in Web pages is translated by SMT sys-tems.
According to Rarrick et al (2011), theirWeb crawler finds that more than 15% of English-Japanese parallel documents are machine transla-tion.
Machine-translated sentences are useful ifthey are of sufficient quality and indistinguish-able from human-generated sentences; however,the quality of these machine-translated sentencesis generally much lower than sentences generatedby native speakers and professional translators.Therefore, a method to detect and filter such SMTresults is desired to best make use of Web-mineddata.To solve this problem, we propose a methodfor automatically detecting Web-text translated bySMT systems1.
We especially target machine-translated text produced through the Web APIsthat is rapidly increasing.
We focus on the phrasesalad phenomenon (Lopez, 2008), which char-acterizes translations by existing SMT systems,i.e., each phrase in a sentence is semanticallyand syntactically correct but becomes incorrectwhen combined with other phrases in the sentence.Based on this trait, we propose features for eval-uating the likelihood of machine-translated sen-tences and use a classifier to determine whetherthe sentence is generated by the SMT systems.The primary contributions of the proposedmethod are threefold.
First, unlike previous stud-ies that use parallel text and bilingual features,such as (Rarrick et al, 2011), our method onlyrequires monolingual text as input.
Therefore,our method can be used in monolingual Web datamining where bilingual information is unavailable.Second, the proposed features are designed to becomputationally light so that the method is suit-able for handling a large-scale Web-mined data.Our method determines if an input sentence con-tains phrase salads using a simple yet effective fea-tures, i.e., language models (LMs) and automati-cally obtained non-contiguous phrases that are fre-quently used by people but difficult for SMT sys-tems to generate.
Third, our method computes fea-tures using both human-generated text and SMT1In this paper, the term machine-translated is used for in-dicating translation by SMT systems.1597results to capture a phrase salad by contrastingthese features, which significantly improves detec-tion accuracy.We evaluate our method using Japanese and En-glish datasets, including a human evaluation to as-sess its performance.
The results show that ourmethod achieves an accuracy of 95.8% for sen-tences and 80.6% for noisy Web-text.2 Related WorkPrevious methods for detecting machine-translated text are mostly designed for bilingualcorpus construction.
Antonova and Misyurev(2011) design a phrase-based decoder fordetecting machine-translated documents inRussian-English Web data.
By evaluating theBLEU score (Papineni et al, 2002) of trans-lated documents (by their decoder) against thetarget-side documents, machine translation (MT)results are detected.
Rarrick et al (2011) extract avariety of features, such as the number of tokensand character types, from sentences in both thesource and target languages to capture words thatare mis-translated by MT systems.
With thesefeatures, the likelihood of a bilingual sentencepair being machine-translated can be determined.Confidence estimation of MT results is alsoa related area.
These studies aim to preciselyreplicate human judgment in terms of the qual-ity of machine-translated sentences based on fea-tures extracted using a syntactic parser (Corston-Oliver et al, 2001; Gamon et al, 2005; Avramidiset al, 2011) or essay scoring system (Partonet al, 2011), assuming that their input is al-ways machine-translated.
In contrast, our methodaims at making a binary judgment to distin-guish machine-translated sentences from a mix-ture of machine-translated and human-generatedsentences.
In addition, although methods forconfidence estimation can assume sentences of aknown source language and reference translationsas inputs, these are unavailable in our problem set-ting.Another related area is automatic grammaticalerror detection for English as a second language(ESL) learners (Leacock et al, 2010).
We usecommon features that are also used in this area.They target specific error types commonly madeby ESL learners, such as errors in prepositions andsubject-verb agreement.
In contrast, our methoddoes not specify error types and aims to de-tect machine-translated sentences focusing on thephrase salad phenomenon produced by SMT sys-tems.
In addition, errors generated by ESL learn-ers and SMT systems are different.
ESL learnersmake spelling and grammar mistakes at the wordlevel but their sentence are generally structuredwhile SMT results are unstructured due to phrasesalads.
Works on translationese detection (Baroniand Bernardini, 2005; Kurokawa et al, 2009; Iliseiet al, 2010) aim to automatically identify human-translated text by professionals using text gener-ated by native speakers.
These are related, but ourwork focuses on machine-translated text.The closest to our approach is the method pro-posed by Moore and Lewis (2010).
It automat-ically selects data for creating a domain-specificLM.
Specifically, the method constructs LMs us-ing corpora of target and non-target domains andcomputes a cross-entropy score of an input sen-tence for estimating the likelihood that the inputsentence belongs to the target or non-target do-mains.
While the context is different, our workuses a similar idea of data selection for the pur-pose of detecting low-quality sentences translatedby SMT systems.3 Proposed MethodWhen APIs of SMT services are used for machine-translating an Web page, they typically insertspecific tags into the HTML source.
Utilizingsuch tags makes MT detection trivial.
How-ever, the actual situation is more complicated inreal Web data.
When people manually copy andpaste machine-translated sentences, such tags arelost.
In addition, human-generated and machine-translated sentences are often mixed together evenin a single paragraph.
To observe the distribu-tion of machine-translated sentences in such diffi-cult cases, we examine 3K sentences collected byour in-house Web crawler.
Among them, exclud-ing the pages with the tags of MT APIs, 6.7% ofthem are found to be clearly machine translation.Our goal is to automatically identify these sen-tences that cannot be simply detected by the tags,except when the sentences are of sufficient qual-ity to be indistinguishable from human-generatedsentences.3.1 Phrase Salad PhenomenonFig.
1 illustrates the phrase salad phenomenon thatcharacterizes a sentence translated by an existing1598| Of surprise | was up | foreigners flocked | overseas | as well, | they publicized not only | Japan, | saw an article from the news.
|Natural English: The news was broadcasted not only in Japan but also overseas, and it surprised foreigners who read the article.Unnatural phrase sequenceNatural phrase|       |Missing combinational wordFigure 1: The phrase salad phenomenon in a sentence translated by an SMT system; each (segmented) phrase is correct andfluent, but dotted arcs show unnatural sequences of phrases and the boxed phrase shows an incomplete non-contiguous phrase.SMT system.
Each phrase, a sequence of con-secutive words, is fluent and grammatically cor-rect; however, the fluency and grammar correct-ness are both poor in inter-phrases.
In addition, aphrase salad becomes obvious by observing dis-tant phrases.
For example, the boxed phrase inFig.
1 is a part of the non-contiguous phrase ?notonly ?
but also2;?
however, it lacks the latter partof the phrase (?but also?)
that is also necessaryfor composing a meaning.
Such non-contiguousphrases are difficult for most SMT systems to gen-erate, since these phrases require insertion of sub-phrases in distant parts of the sentence.Based on the observation of these characteris-tics, we define features to capture a phrase saladby examining local and distant phrases.
Thesefeatures evaluate (1) fluency (Sec.
3.2), (2) gram-maticality (Sec.
3.3), and (3) completeness ofnon-contiguous phrases in a sentence (Sec.
3.4).Furthermore, humans can distinguish machine-translated text because they have prior knowledgeof how a human-generated sentence would looklike, which has been accumulated by observing alot of examples through their life.
This knowl-edge makes phrase-salads, e.g., missing objectsand influent sequence of words, obvious for hu-mans since they rarely appear on human-generatedsentences.
Based on this assumption, we ex-tract these features using both human-generatedand machine-translated text.
Features extractedfrom human-generated text represent the similar-ity to human-generated text.
Likewise, featuresextracted from machine-translated text depict thesimilarity to machine-translated text.
By contrast-ing these feature weights, we can effectively cap-ture phrase salads in the sentence.3.2 Fluency FeatureIn a machine-translated sentence, fluency becomespoor among phrases where a phrase salad occurs.We capture this influency using two independentLM scores; fw,H and fw,MT .
The former LM is2We use the symbol ?
to represent a gap in which anyword or phrase can be placed.trained with human-generated sentences and thelatter one is trained with machine-translated sen-tences.
We input a sentence into both of the LMsand use the scores as the fluency features.3.3 Grammaticality FeatureIn a sentence with phrase salads, its grammatical-ity is poor because tense and voice become in-consistent among phrases.
We capture this usingLMs trained with part-of-speech (POS) sequencesof human-generated and machine-translated sen-tences, and the features of fpos,H and fpos,MT arerespectively computed.
In a similar manner with aword-based LM, such grammatical inconsistencyamong phrases is detectable when computing aPOS LM score, since the score becomes worsewhen an N -gram covers inter-phrases where aphrase salad occurs.
This approach achieves com-putational efficiency since it only requires a POStagger.Since a phrase salad may occur among distantphrases of a sentence, it is also effective to evalu-ate combinations of phrases that cannot be cov-ered by the span of N -gram.
For this purpose,we make use of function words that sparsely ap-pear in a sentence where their combinations aresyntactically constrained.
For example, the samepreposition rarely appears many times in a human-generated sentence, while it does in a machine-translated sentence due to the phrase salad.
Simi-lar to the POS LM, we first analyze sentences gen-erated by human or SMT by a POS tagger, extractsequences of function words, and finally train LMswith the sequences.
We use these LMs to obtainscores that are used as features ffw,H and ffw,MT .3.4 Gappy-Phrase FeatureThere are a lot of common non-contiguous phrasesthat consist of sub-phrases (contiguous wordstring) and gaps, which we refer to as gappy-phrases (Bansal et al, 2011).
We specifically usegappy-phrases of 2-tuple, i.e., phrases consistingof two sub-phrases and one gap in the middle.Let us take an English example ?not only ?
but1599SequencesWorld population not only grows , but grows old .A press release not only informs but also teases .Hazelnuts are not only for food , but also fuel .The coalition must not only listen but also act .Table 1: Example of a sequence databasealso.?
When a sentence contains the phrase ?notonly,?
the phrase ?but also?
is likely to appear inhuman-generated setences.
Such a gappy-phraseis difficult for SMT systems to correctly generateand causes a phrase salad.
Therefore, we define afeature to evaluate how likely a sentence containsgappy-phrases in a complete form without missingsub-phrases.
This feature is effective to comple-ment LMs that capture characteristics inN -grams.Sequential Pattern Mining It is costly to man-ually collect a lot of such gappy-phrases.
There-fore, we regard the task as sequential pattern min-ing and apply PrefixSpan proposed by Pei et al(2001), which is a widely used sequential patternmining method3.Given a set of sequences and a user-specifiedmin support ?
N threshold, the sequential patternmining finds all frequent subsequences whose oc-currence frequency is no less than min support.For example, given a sequence database like Ta-ble 1, the sequential pattern mining finds all fre-quent subsequences, e.g., ?not only,?
?not only ?but also,?
?not ?
but ?,?
and etc.To capture a phrase salad by contrasting appear-ance of gappy-phrases in human-generated andmachine-translated text, we independently extractgappy-phrases from each of them using PrefixS-pan.
We then compute features fg,H and fg,MTusing the obtained phrases.Observation of Extracted Gappy-PhrasesBased on a preliminary experiment, we setthe parameter min support of PrefixSpan to100 for computational efficiency.
We extractgappy-phrases (of 2-tuple) from our develop-ment dataset described in Sec.
4.1 that includes254K human-generated and 134K machine-translated sentences in Japanese, and 210Khuman-generated and 159K machine-translatedsentences in English.Regarding the Japanese dataset, we obtainabout 104K and 64K gappy-phrases from human-3Due to the severe space limitation, readers are referred tothat paper.generated and machine-translated sentences, re-spectively.
According to our observation of theextracted phrases, 21K phrases commonly ap-pear in human-generated and machine-translatedsentences.
Many of these common phrases areincomplete forms of gappy-phrases that lack se-mantic meaning to humans, such as ?not only ?the?
and ?not only ?
and.?
On the other hand,complete forms of gappy-phrases that preserve se-mantic meaning exclusively appear in phrases ex-tracted from human-generated sentences.
We alsoobtain about 74K and 42K phrases from human-generated and machine-translated sentences in theEnglish dataset (21K of them are common).Phrase Selection As a result of sequentialpattern mining, we can gather a huge num-ber of gappy-phrases from human-generated andmachine-translated text, but as we describedabove, many of them are common.
In addition,it is computationally expensive to use all of them.Therefore, our method selects useful phrases fordetecting machine-translated sentences.Although there are several approaches for fea-ture selection, e.g., (Sebastiani, 2002), we use amethod that is suitable for handling a large num-ber of feature candidates.
Specifically, we evaluategappy-phrases based on the information gain thatmeasures the amount of information in bits ob-tained for class prediction when knowing the pres-ence or absence of a phrase and the correspondingclass distribution.
This corresponds to measuringan expected reduction in entropy, i.e., uncertaintyassociated with a random factor.
The informationgain G ?
R for a gappy-phrase g is defined asG(g) .= H(C)?
P (X1g )H(C|X1g )?P (X0g )H(C|X0g ),where H(C) represents the entropy of the classifi-cation, C is a stochastic variable taking a class,Xgis a stochastic variable representing the presence(X1g ) or absence (X0g ) of the phrase g, P (Xg) rep-resents the probability of presence or absence ofthe phrase g, and H(C|Xg) is the conditional en-tropy due to the phrase g. We use top-k phrasesbased on the information gain G. Specifically, weuse the top 40% of phrases to compute the featurevalues.
Table 2 shows examples of gappy-phrasesextracted from human-generated and machine-translated text in our development dataset and re-main after feature selection.1600in the early ?
period after ?
after theknown as ?
to and also ?
andHuman more ?
than MT and ?
but thenot only ?
but also no ?
notwith ?
as well as not ?
notTable 2: Example of gappy-phrases extracted from human-generated and machine-translated text; phrases preserving se-mantic meaning are extracted only from human-generatedtext.The gappy-phrases depend on each other, andthe more phrases extracted from human-generated(machine-translated) text are found in a sentence,the more likely the sentence is human-generated(machine-translated).
Therefore, we compute thefeature asfc(s) =?i?kwi?
(i, s),where wi is a weight of the i-th phrase, and ?
(i, s)is a Kronecker?s delta function that takes 1 if thesentence s includes the i-th phrase and takes 0 oth-erwise.
We may set the weight wi according to theimportance of the phrase, such as the informationgain.
In this work, we set wi to 1 for simplicity.3.5 ClassificationTable 3 summarizes the features employed inour method.
In addition to the discussed fea-tures, we use the length of a sentence as a fea-ture flen to avoid the bias of LM-based fea-tures that favor shorter sentences.
The proposedmethod takes a monolingual sentence from Webdata as input and computes a feature vector off = (fw,H , .
.
.
, flen) ?
R9.
Each feature is fi-nally normalized to have a zero-mean and unitvariance distribution.
In the feature space, asupport vector machine (SVM) classifier (Vap-nik, 1995) is used to determine the likelihoodsof machine-translated and human-generated sen-tences.4 ExperimentsWe evaluate our method using both Japanese andEnglish datasets from various aspects and investi-gate its characteristics.
In this section, we describeour experiment settings.4.1 Data PreparationFor the purpose of evaluation, we use human-generated and machine-translated sentences forFeature NotationFluency fw,H , fw,MTGrammaticality fpos,H , fpos,MTffw,H , ffw,MTGappy-phrase fg,H , fg,MTLength flenTable 3: List of proposed features and their notationsconstructing LMs, extracting gappy-phrases, andtraining a classifier.
These sentences shouldbe ensured to be human-generated or machine-translated, and the human-generated and machine-translated sentences express the same content forfairness of evaluation to avoid effects due to vo-cabulary difference.As a dataset that meets these requirements, weuse parallel text in public websites (this is for fairevaluation and our method can be trained usingnonparallel text on an actual deployment).
Eightpopular sites having Japanese and English paral-lel pages are crawled, whose text is manually veri-fied to be human-generated.
The main textual con-tent of these 131K parallel pages are extracted,and the sentences are aligned using (Ma, 2006).As illustrated in Fig.
2, the text in one languageis fed to the Bing translator, Google Translate,and an in-house SMT system4 implemented basedon (Chiang, 2005) by ourselves for obtaining sen-tences translated by SMT systems.
Due to a severelimitation on the number of requests to the APIs,we randomly subsample sentences before sendingthem to these SMT systems.
We use text in theother language as human-generated sentences5.In this manner, we prepare 508K human-generated and 268K machine-translated sentencesas a Japanese dataset, and 420K human-generatedand 318K machine-translated sentences as an En-glish dataset.
We split each of them into two evendatasets and use one for development and the otherfor evaluation.4.2 Experiment SettingFor the fluency and grammaticality features, wetrain 4-gram LMs using the development datasetwith the SRI toolkit (Stolcke, 2002).
To obtainthe POS information, we use Mecab (Kudo et al,2004) for Japanese and a POS tagger developed byToutanova et al (2003) for English.
We evaluate4A preliminary evaluation of the in-house SMT systemshows that it has comparable quality with Bing translator.5These are a mixture of sentences generated by nativespeakers and professional translators/editors.1601Parallel sentencesMT systems MachinetranslatedsentencesHuman -generatedsentencesHuman -generatedsentencesHuman -generatedsentences?EnglishJapaneseJapaneseJapanese??
?Figure 2: Experimental data preparation; text in one lan-guage is fed to SMT systems and the other is used as human-generated sentences.the effect of the sizes of N -grams and develop-ment dataset in the experiments.Using the proposed features, we train an SVMclassifier for detecting machine-translated sen-tences.
We use an implementation of LIB-SVM (Chang and Lin, 2011) with a radial basisfunction kernel due to the relatively small numberof features in the proposed method.
We set appro-priate parameters by grid search in a preliminaryexperiment.We evaluate the performance of MT detectionbased on accuracy6 that is a broadly used evalua-tion metric for classification problems:accuracy = nTP + nTNn ,where nTP and nTN are the numbers of true-positives and true-negatives, respectively, and nis the total number of exemplars.
The accuracyscores that we report in Sec.
5 are all based on 10-fold cross validation.4.3 Comparison MethodWe compare our method with the methodof (Moore and Lewis, 2010) (Cross-Entropy).
Al-though the Cross-Entropy method is designed forthe task of domain adaptation of an LM, our prob-lem is a variant of their original problem andthus their method is directly relevant.
In ourcontext, the method computes the cross-entropyscores IMT (s) and IH(s) of an input sentences against LMs trained on machine-translated andhuman-generated sentences.
Cross-entropy andperplexity are monotonically related, as perplex-ity of s according to an LM M is simply ob-6Although we also examine precision and recall of clas-sification results, they are similar to accuracy reported in thispaper.Method AccuracyCross-Entropy 90.7Lexical Feature 87.8Proposed feature Word LMs 94.1POS LMs 91.3FW LMs 82.7GPs 85.7Table 4: Accuracy (%) of individual features and compari-son methodstained by bIM (s) where IM (s) is cross-entropyscore and b is a base with regard to which thecross-entropy is measured.
The method scoresthe sentence according to the cross-entropy differ-ence, i.e., IMT (s)?
IH(s), and decides that thesentence is machine-translated when the score islower than a predefined threshold.
The classifica-tion is performed by 10-fold cross validation.
Wefind the best performing threshold on a training setand evaluate the accuracy with a test set using thedetermined threshold.Additionally, we compare our method to amethod that uses a feature indicating presence orabsence of unigrams, which we call Lexical Fea-ture.
This feature is commonly used for transla-tionese detection and shows the best performanceas a single feature in (Baroni and Bernardini,2005).
It is also used by Rarrick et al (2011) andshows the best performance by itself in detectingmachine-translated sentences in English-Japanesetranslation in the setting of bilingual input.
Weimplement the feature and use it against a mono-lingual input to fit our problem setting.5 Results and DiscussionsIn this section, we analyze and discuss the experi-ment results in detail.5.1 Accuracy on Japanese DatasetWe evaluate the sentence-level and document-level accuracy of our method using the Japanesedataset.
Specifically, we evaluate effects of indi-vidual features and their combinations, comparewith human annotations, and assess performancevariations across different sentence lengths andvarious settings on LM training.Effect of Individual Feature Table 4 shows theaccuracy scores of individual features and com-parison methods.
We refer to features for flu-ency (fw,H , fw,MT ) as Word LMs, grammatical-ity using POS LMs (fpos,H , fpos,MT ) as POS LMs1602Method AccuracyWord LMs + GPs 94.7Word LMs + POS LMs 95.1Word LMs + POS LMs + GPs 95.4Word LMs + POS LMs + FW LMs 95.5All 95.8Table 5: Accuracy (%) of feature combinations; there aresignificant differences (p  .01) against the accuracy scoreof Word LMs.and function word LMs (ffw,H , ffw,MT ) as FWLMs, respectively, and for completeness of gappy-phrases (fg,H , fg,MT ) as GPs.
The Word LMsshow the best accuracy that outperforms Cross-Entropy by 3.4% and Lexical Feature by 6.3%.This high accuracy is achieved by contrasting flu-ency in human-generated and machine-translatedtext to capture the phrase salad phenomenon.
Theaccuracy of Word LM trained only on human-generated sentences is limited to 65.5%.
On theother hand, the accuracy of Word LM trained onmachine-translated sentences shows a better per-formance (84.4%).
By combining these into asingle feature vector f = (fw,H , fw,MT , flen), theaccuracy is largely improved.It is interesting that Lexical Feature achievesa high accuracy of 87.8% despite its simplicity.Since Lexical Feature is a bag-of-words model,it can consider distant words in a sentence.
Thisis effective for capturing a phrase salad that oc-curs among distant phrases, which N -gram can-not cover.
As for Cross-Entropy, a simple sub-traction of cross-entropy scores cannot well con-trast the fluency in human-generated and machine-translated text and results in poorer accuracy thanWord LMs.The accuracy of POS LMs (91.3%) is slightlylower than that of Word LMs due to the limitedvocabulary, i.e., the number of POSs.
The accu-racy of FW LMs and GPs are even lower.
Thisis convincing since these features cannot have rea-sonable values when a sentence does not include afunction word and gappy-phrase.
However, thesefeatures are complementary to Word LMs as wewill see in the next paragraph.Effect of Feature Combination Table 5 showsthe accuracy when combining features.
Sign testsshow that the accuracy scores of these featurecombinations are significantly different (p .01)against the accuracy of Word LMs.
The resultsshow that the features complement each other.
TheError Ratio Accuracy(%) WordLMsAllHas wrong content words 37.8 93.1 95.0Misses content words 12.2 91.8 96.5Has wrong function words 19.7 92.7 97.1Misses function words 13.0 93.3 95.6Has wrong inflections 10.8 97.3 98.7Table 6: Distribution (%) of machine translation errors andaccuracy (%) of proposed method on the different errorscombination of all features reaches an accuracyof 95.8%, which improves the accuracy of WordLMs by 1.7%.
This result supports that FW LMsand GPs are effective to capture a phrase salad oc-curring in distant phrases and complement the ev-idence in N -grams that is captured by LMs.
Thiseffect becomes more obvious in the human evalu-ation.We also evaluate the accuracy of the proposedmethod at a document level.
Due to the high accu-racy at the sentence-level, we use a voting methodto judge a document, i.e., deciding if the docu-ment is machine-translated when ?% of its sen-tences are judged as machine-translated.
We useall features and find that our method achieves 99%precision and recall with ?
= 50.Human Evaluation To further investigate thecharacteristics of our method, we conduct a humanevaluation.
We sample Japanese sentences and askthree native speakers to 1) judge whether a sen-tence is human-generated or machine-translatedand 2) list errors that the sentence contains.
Re-garding the task 1), we allow the annotators to as-sign ?hard to determine?
for difficult cases.
We al-locate about 230 sentences for each annotator (intotal 700 sentences) without overlapping annota-tion sets.The accuracy of annotations is found to be88.2%, which shows that our method is even su-perior to native speakers.
Agreement between theannotators and our method (with all features) is85.1%.
As we interview the annotators, we findthat human annotations are strongly affected bythe annotators?
domain knowledge.
For example,technical sentences are more often misclassifiedby the annotators.Table 6 shows the distribution of errors onmachine-translated sentences found by the anno-tators (on sentences that they correctly classified)with the accuracy of Word LMs and all features on160302468107075808590951006 10 14 18 22 26 30 34 38 42 46 50 54 58 62 66 70 74 78Ratio(%)Accuracy (%)Num.
of words in a sentenceProposed MethodCross-EntropyLexical FeatureHumanLength distributionFigure 3: Accuracy (%) across different sentence lengths(the primary axis) and distribution (%) of sentence lengths inthe evaluation dataset (the secondly axis)these sentences (a sentence may contain multipleerrors).
It indicates that the accuracy of Word LMsis improved by feature combination; from 1.4% onsentences of ?Has wrong inflections?
to 4.7% onsentences of ?Misses content words?.Effect of Sentence Length The accuracy of theproposed method is significantly affected by sen-tence length (the number of words in a sentence).Fig.
3 shows the accuracy of the proposed method(with all features) and comparison methods w.r.t.sentence lengths (with the primary axis), as wellas the distribution of sentence lengths in the eval-uation dataset (with the secondly axis).
We ag-gregate the classification results on each cross-validation (test results).
It also shows the accu-racy of human annotations w.r.t.
sentence lengths,which we obtain for the 700 sentences in the hu-man evaluation.
The accuracy drops on all meth-ods when sentences are short; the accuracy of ourmethod is 91.6% when a sentence contains lessthan or equal to 10 words.
The proposed methodshows the similar trend with the human annota-tions, and even the accuracy of human annota-tions significantly drops on such short sentences.This result indicates that SMT results on shortsentences tend to be of sufficient quality and in-distinguishable from human-generated sentences.Since such high-quality machine-translations donot harm the quality of Web-mined data, we donot need to detect them.Effect of Setting on LM Training We evalu-ate the performance variation w.r.t.
the sizes ofN -grams and development dataset.
Fig.
4 showsthe accuracy of the LM based features and featurecombination when changing sizes of N -grams.The performance of Word LMs is stabilized after78838893981 2 3 4Accuracy (%)N- gramWord LMsPOS LMsFW LMsAL LFigure 4: Effect of the sizes of N -grams on MT detectionaccuracy (%)3-gram while that of POS LMs is still improvedat 4-gram.
This is because POS LMs need moreevidence to compensate for their limited vocabu-lary.
FW LMs become stable at 3-gram becausethe possible number of function words in a sen-tence should be small.When we change the size of the developmentdataset with 10% increments, the accuracy curve isstabilized when the size is 40% of all set.
Consid-ering the fact that the overall development datasetis small, it shows that our method is deployablewith a small dataset.5.2 Accuracy on English DatasetTo investigate the applicability of our method toother languages, we apply the same method tothe English dataset.
Because English is a config-urational language, function words are less flex-ible than case markers in Japanese.
Therefore,SMT systems may better handle English functionwords, which potentially decreases the effect ofFW LMs in our method.
In addition, because En-glish is a morphologically poor language, the ef-fect of POS LMs may be reduced.Nevertheless, in our experiment, all featuresare shown to be effective even with the Englishdataset.
The combination of all features achievesthe best performance, with an accuracy of 93.1%,which outperforms Cross-Entropy by 1.9%, andLexical Feature by 8.5%.
Even though improve-ments by POS LMs and FW LMs are smaller thanJapanese case, their effects are still positive.
Wealso find that GPs stably contribute to the accu-racy.
These results show the applicability of ourmethod to other languages.5.3 Accuracy on Raw Web PagesTo avoid unmodeled factors affecting the evalua-tion, we have carefully removed noise from ourexperiment datasets.
However, real Web pages are1604more complex; there are often instances of sen-tence fragments, such as captions and navigationallink text.
To evaluate the accuracy of our methodon real Web pages, we conduct experiments usingthe dataset generated by Rarrick et al (2011) thatcontains randomly crawled Web pages annotatedby two annotators to judge if a page is human-generated or machine-translated.
We use Japanesesentences extracted from 69 pages (43 human-generated and 26 machine-translated pages) wherethe annotators?
judgments agree; 3, 312 sentencesconsisting of 1, 399 machine-translated and 1, 913human-generated sentences.
To replicate the sit-uation in real Web pages, we conduct a minimalpreprocessing, i.e., simply removing HTML tags,and then feed all the remaining text to our method.An SVM classifier is trained with features ob-tained by the LMs and gappy-phrases computedfrom the data described in Sec.
4.1.
Our methodshows 80.6% accuracy at a sentence level and82.4% accuracy at a document level using the vot-ing method.
One factor for this performance dif-ference is again sentence lengths, as SMT resultsof short phrases in Web pages can be of high-quality.
Another factor is the noise in Web pages.We find that experimental pages contain lots ofnon-sentences, such as fragments of scripts andproduct codes.
The results show that we need apreprocessing to remove typical noise in Web textbefore SMT detection to handle noisy Web pages.5.4 Quality of Cleaned DataFinally, we briefly demonstrate the effect ofmachine-translation filtering in an end-to-end sce-nario, taking LM construction as an example.We construct LMs reusing the Japanese evalua-tion dataset described in Sec.
4.1 where machine-translated sentences are removed by the pro-posed method (LM-Proposed), Lexical Feature(LM-LF), and Cross-Entropy (LM-CE), as wellas an LM with all sentences, i.e., with machine-translated sentences (LM-All).
As a result of 5-fold cross-validation, LM-Proposed has 17.8%,17.1%, and 16.3% lower perplexities on averagecompared to LM-All, LM-LF, and LM-CE, re-spectively.
These results show that our methodis useful for improving the quality of Web-mineddata.6 ConclusionWe propose a method for detecting machine-translated sentences from monolingual Web-textfocusing on the phrase salad phenomenon pro-duced by existing SMT systems.
The experimen-tal results show that our method achieves an accu-racy of 95.8% for sentences and 80.6% for noisyWeb text.We plan to extend our method to detectmachine-translated sentences produced by differ-ent MT systems, e.g., a rule-based system, anddevelop a unified framework for cleaning varioustypes of noise in Web-mined data.
In addition, wewill investigate the effect of source and target lan-guages on translation in terms of MT detection.
AsLopez (2008) describes, a phrase-salad is a com-mon phenomenon that characterizes current SMTresults.
Therefore, we expect that our method isbasically effective on different language pairs.
Wewill conduct experiments to evaluate performancedifference using various language pairs.AcknowledgmentsWe sincerely appreciate Spencer Rarrick and WillLewis for active discussion and sharing the exper-imental data with us.
We thank Junichi Tsujii forhis valuable feedback to improve our work.ReferencesAlexandra Antonova and Alexey Misyurev.
2011.Building a web-based parallel corpus and filteringout machine translated text.
In Proceedings of theWorkshop on Building and Using Comparable Cor-pora, pages 136?144.Eleftherios Avramidis, Maja Popovic, David Vilar Tor-res, and Aljoscha Burchardt.
2011.
Evaluate withconfidence estimation: Machine ranking of trans-lation outputs using grammatical features.
In Pro-ceedings of the Workshop on Statistical MachineTranslation (WMT 2011), pages 65?70.Mohit Bansal, Chris Quirk, and Robert C. Moore.2011.
Gappy phrasal alignment by agreement.
InProceedings of the Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT 2011), pages 1308?1317.Marco Baroni and Silvia Bernardini.
2005.
A newapproach to the study of translationese: Machine-learning the difference between original and trans-lated text.
Literary and Linguistic Computing,21(3):259?274.1605Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM : a library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2(3):27:1?27:27.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL 2005), pages263?270.Simon Corston-Oliver, Michael Gamon, and ChrisBrockett.
2001.
A machine learning approach to theautomatic evaluation of machine translation.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL 2001), pages148?155.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open informationextraction.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2011), pages 1535?1545.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level MT evaluation without refer-ence translations: Beyond language modeling.
InProceedings of European Association for MachineTranslation (EAMT 2005).Google N-gram Corpus.
2006. http://www.ldc.upenn.edu/Catalog/CatalogEntry.jsp?catalogId=LDC2006T13.Google Translate.
2006. http://code.google.com/apis/language/.Iustina Ilisei, Diana Inkpen, Gloria Corpas Pastor, andRuslan Mitkov.
2010.
Identification of transla-tionese: A machine learning approach.
In Proceed-ings of the International Conference on IntelligentText Processing and Computational Linguistics (CI-CLing 2010), pages 503?511.Tatsuya Ishisaka, Masao Utiyama, Eiichiro Sumita, andKazuhide Yamamoto.
2009.
Development of aJapanese-English software manual parallel corpus.In Proceedings of the Machine Translation Summit(MT Summit XII).Long Jiang, Shiquan Yang, Ming Zhou, Xiaohua Liu,and Qingsheng Zhu.
2009.
Mining bilingual datafrom the web with adaptively learnt patterns.
InProceedings of the Joint Conference of the AnnualMeeting of the Association for Computational Lin-guistics and the International Joint Conference onNatural Language Processing of the Asian Federa-tion of Natural Language Processing (ACL-IJCNLP2009), pages 870?878.Taku Kudo, Kaoru Yamamoto, and Yuji Matsumoto.2004.
Applying conditional random fields toJapanese morphological analysis.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2004), pages 230?237.David Kurokawa, Cyril Goutte, and Pierre Isabelle.2009.
Automatic detection of translated text and itsimpact on machine translation.
In Proceedings ofthe Machine Translation Summit (MT-Summit XII).Claudia Leacock, Martin Chodorow, Michael Gamon,and Joel Tetreault.
2010.
Automated GrammaticalError Detection for Language Learners.
Morganand Claypool Publishers.Adam Lopez.
2008.
Statistical machine translation.ACM Computing Surveys, 40(3):1?49.Xiaoyi Ma.
2006.
Champollion: a robust parallel textsentence aligner.
In Proceedings of the InternationalConference on Language Resources and Evaluation(LREC 2006), pages 489?492.Microsoft Translator.
2009. http://www.microsofttranslator.com/dev/.Microsoft Web N-gram Services.
2010. http://research.microsoft.com/web-ngram.Robert Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In Pro-ceedings of the Annual Meeting of the Associationfor Computational Linguistics (ACL 2010), pages220?224.Ndapandula Nakashole, Gerhard Weikum, andFabian M. Suchanek.
2012.
PATTY: A taxonomyof relational patterns with semantic types.
InProceedings of the Joint Conference on Empir-ical Methods in Natural Language Processingand Computational Natural Language Learning(EMNLP-CoNLL 2012), pages 1135?1145.Jian-Yun Nie, Michel Simard, Pierre Isabelle, andRichard Durand.
1999.
Cross-language informationretrieval based on parallel texts and automatic min-ing of parallel texts from the web.
In Proceedingsof the Annual International ACM SIGIR Conference(SIGIR 1999), pages 74?81.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the Annual Meeting of the Association for Com-putational Linguistics (ACL 2002), pages 311?318.Kristen Parton, Joel Tetreault, Nitin Madnani, and Mar-tin Chodorow.
2011.
E-rating machine translation.In Proceedings of the Workshop on Statistical Ma-chine Translation (WMT 2011), pages 108?115.Jian Pei, Jiawei Han, Behzad Mortazavi-Asl, HelenPinto, Qiming Chen, Umeshwar Dayal, and Mei-Chun Hsu.
2001.
PrefixSpan: Mining sequen-tial patterns efficiently by prefix-projected patterngrowth.
In Proceedings of the International Con-ference on Data Engineering (ICDE 2001), pages215?224.1606Spencer Rarrick, Chris Quirk, and Will Lewis.
2011.MT detection in web-scraped parallel corpora.
InProceedings of the Machine Translation Summit(MT Summit XIII).Fabrizio Sebastiani.
2002.
Machine learning in au-tomated text categorization.
ACM Computing Sur-veys, 34(1):1?47.Lei Shi, Cheng Niu, Ming Zhou, and Jianfeng Gao.2006.
A DOM tree alignment model for mining par-allel data from the web.
In Proceedings of the Inter-national Conference on Computational Linguisticsand the Annual Meeting of the Association for Com-putational Linguistics (COLING-ACL 2006), pages489?496.Andreas Stolcke.
2002.
SRILM-an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Process-ing (ICSLP 2002), pages 901?904.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: a core of semantic knowl-edge.
In Proceedings of International Conferenceon World Wide Web (WWW 2007), pages 697?706.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology (HLT-NAACL 2003), pages 252?259.Vladimir N. Vapnik.
1995.
The nature of statisticallearning theory.
Springer.Jun Zhu, Zaiqing Nie, Xiaojiang Liu, Bo Zhang, andJi-Rong Wen.
2009.
StatSnowball: a statisticalapproach to extracting entity relationships.
In Pro-ceedings of International Conference on World WideWeb (WWW 2009), pages 101?110.1607
