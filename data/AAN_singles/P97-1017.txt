Machine TransliterationKev in  Kn ight  and  Jonathan  Graeh lIn fo rmat ion  Sciences Ins t i tu teUn ivers i ty  of Southern  Cal i forn iaMar ina  del Rey, CA  90292knight~isi, edu, graehl@isi, eduAbst rac tIt is challenging to translate names andtechnical terms across languages with differ-ent alphabets and sound inventories.
Theseitems are commonly transliterated, i.e., re-placed with approximate phonetic equivalents.For example, computer in English comes outas ~ i/l:::'=--~-- (konpyuutaa) in Japanese.Translating such items from Japanese back toEnglish is even more challenging, and of prac-tical interest, as transliterated items make upthe bulk of text phrases not found in bilin-gual dictionaries.
We describe and evaluate amethod for performing backwards translitera-tions by machine.
This method uses a gen-erative model, incorporating several distinctstages in the transliteration process.1 In t roduct ionTranslators must deal with many problems, andone of the most frequent is translating propernames and technical terms.
For language pairslike Spanish/English, this presents no great chal-lenge: a phrase like Antonio Gil usually gets trans-lated as Antonio Gil.
However, the situation ismore complicated for language pairs that employvery different alphabets and sound systems, suchas Japanese/English and Arabic/English.
Phonetictranslation across these pairs is called translitera-tion.
We will look at Japanese/English translitera-tion in this paper.Japanese frequently imports vocabulary fromother languages, primarily (but not exclusively)from English.
It has a special phonetic alphabetcalled katakana, which is used primarily (but notexclusively) to write down foreign names and loan-words.
To write a word like golf bag in katakana,some compromises must be made.
For example,Japanese has no distinct L and R sounds: the two En-glish sounds collapse onto the same Japanese sound.A similar compromise must be struck for EnglishH and F. Also, Japanese generally uses an alter-nating consonant-vowel structure, making it impos-sible to pronounce LFB without intervening vow-els.
Katakana writing is a syllabary rather than analphabet--there is one symbol for ga (~I), anotherfor g i  (4e), another for gu (P'), etc.
So the wayto write gol\]bag in katakana is =~'~ 7 ~ ~, ~,  roughlypronounced goruhubaggu.
Here are a few more ex-amples:Angela JohnsonTvz J~ ? "
J~ vY  v(anj ira jyonson)New York Times(nyuuyooku t aimuzu)ice creamT 4 x ~, ~) -- z,(aisukuriimu)Notice how the transliteration is more phonetic thanorthographic; the letter h in Johnson does not pro-duce any katakana.
Also, a dot-separator ( . )
isused to separate words, but not consistently.
Andtransliteration is clearly an information-losing oper-ation: a isukur i imu loses the distinction between icecream and I scream.Transliteration is not trivial to automate, butwe will be concerned with an even more challeng-ing problem--going from katakana back to En-glish, i.e., back-transliteration.
Automating back-transliteration has great practical importance inJapanese/English machine translation.
Katakanaphrases are the largest source of text phrases thatdo not appear in bilingual dictionaries or trainingcorpora (a.k.a.
"not-found words").
However, verylittle computational work has been done in this area;(Yamron et al, 1994) briefly mentions a pattern-matching approach, while (Arbabi et al, 1994) dis-cuss a hybrid neural-net/expert-system approach to(forward) transliteration.The information-losing aspect of transliterationmakes it hard to invert.
Here are some problem in-stances, taken from actual newspaper articles: 1ITexts used in ARPA Machine Translation evalua-tions, November 1994.128?T- -x~- -(aasudee)'9(robaato shyoon renaado)?
"~':~ --:~" l .
- - ) -~ y I-(masu~aazu~ oonamen~ o)English translations appear later in this paper.Here are a few observations about back-transliteration:?
Back-transliteration is less forgiving thantransliteration.
There are many ways to writean English word like switch in katakana, allequally valid, but we do not have this flexibilityin the reverse direction.
For example, we can-not drop the t in switch, nor can we write arturewhen we mean archer.?
Back-transliteration is harder than romaniza-tion, which is a (frequently invertible) trans-formation of a non-roman alphabet into ro-man letters.
There are several romanizationschemes for katakana writing--we have alreadybeen using one in our examples.
KatakanaWriting follows Japanese sound patterns closely,so katakana often doubles as a Japanese pro-nunciation guide.
However, as we shall see,there are many spelling variations that compli-cate the mapping between Japanese sounds andkatakana writing.?
Finally, not all katakana phrases can be"sounded out" by back-transliteration.
Somephrases are shorthand, e.g., r\] _ 7" ~ (uaapuro)should be translated as word processing.
Oth-ers are onomatopoetic and difficult to translate.These cases must be solved by techniques otherthan those described here.The most desirable feature of an automatic back-transliterator is accuracy.
If possible, our techniquesshould also be:?
portable to new language pairs like Ara-bic/English with minimal effort, possiblyreusing resources.?
robust against errors introduced by opticalcharacter recognition.?
relevant to speech recognition situations inwhich the speaker has a heavy foreign accent.?
able to take textual (topical/syntactic) ontextinto account, or at least be able to return aranked list of possible English translations.Like most problems in computational linguistics,this one requires full world knowledge for a 100%solution.
Choosing between Katarina and Catalina(both good guesses for ~' ~ ~ ")-) might even requiredetailed knowledge of geography and figure skating.At that level, human translators find the problemquite difficult as well.
so we only aim to match orpossibly exceed their performance.2 A Modular Learning ApproachBilingual glossaries contain many entries mappingkatakana phrases onto English phrases, e.g.
: (air-craft carrier --, ~ T ~ ~ 7 I.
~ ~ ~3 7" ).
It is possibleto automatically analyze such pairs to gain enoughknowledge to accurately map new katakana phrasesthat come along, and learning approach travels wellto other languages pairs.
However, a naive approachto finding direct correspondences between Englishletters and katakana symbols uffers from a numberof problems.
One can easily wind up with a sys-tem that proposes iskrym as a back-transliteration ofa isukur i imu.
Taking letter frequencies into accountimproves this to a more plausible-looking isclim.Moving to real words may give is crime: the i cor-responds to ai,  the s corresponds to su, etc.
Unfor-tunately, the correct answer here is ice cream.
Af-ter initial experiments along these lines, we decidedto step back and build a generative model of thetransliteration process, which goes like this:1.
An English phrase is written.2.
A translator pronounces it in English.3.
The pronunciation is modified to fit theJapanese sound inventory.4.
The sounds are converted into katakana.5.
Katakana is written.This divides our problem into five sub-problems.Fortunately, there are techniques for coordinatingsolutions to such sub-problems, and for using gen-erative models in the reverse direction.
These tech-niques rely on probabilities and Bayes' Rule.
Sup-pose we build an English phrase generator that pro-duces word sequences according to some probabilitydistribution P(w).
And suppose we build an Englishpronouncer that takes a word sequence and assignsit a set of pronunciations, again probabilistically, ac-cording to some P(plw).
Given a pronunciation p,we may want to search for the word sequence w thatmaximizes P(wtp ).
Bayes" Rule lets us equivalentlymaximize P(w) .
P(plw).
exactly the two distribu-tions we have modeled.Extending this notion, we settled down to buildfive probability distributions:1.
P(w) - -  generates written English word se-quences.2.
P(elw) - -  pronounces English word sequences.3.
P(jle) - -  converts English sounds into Japanesesounds.1294.
P(k\[j) ~ converts Japanese sounds to katakanawriting.5.
P(o{k) ~ introduces misspellings caused by op-tical character recognition (OCR).Given a katakana string o observed by OCR, wewant to find the English word sequence w that max-imizes the sum, over all e, j, and k, ofP(w) ?
P(e\[w).
P(j le)" P(kJ j) .
P(olk)Following (Pereira et al, 1994; Pereira and Riley,I996), we implement P(w) in a weighted finite-stateaceeptor (WFSA) and we implement the other dis-tributions in weighted finite-state transducers (WF-STs).
A WFSA is an state/transition diagram withweights and symbols on the transitions, makingsome output sequences more likely than others.
AWFST is a WFSA with a pair of symbols on eachtransition, one input, and one output.
Inputs andoutputs may include the empty symbol e. Also fol-lowing (Pereira and Riley, 1996), we have imple-mented a general composition algorithm for con-structing an integrated model P(zlz) from modelsP(~IY) and P(ylz), treating WFSAs as WFSTs withidentical inputs and outputs.
We use this to combinean observed katakana string with each of the mod-els in turn.
The result is a large WFSA containingall possible English translations.
We use Dijkstra'sshortest-path algorithm {Dijkstra, 1959) to extractthe most probable one.The approach is modular.
We can test each en-gine independently and be confident hat their re-sults are combined correctly.
We do no pruning,so the final WFSA contains every solution, howeverunlikely.
The only approximation is the Viterbi one,which searches for the best path through a WFSAinstead of the best sequence (i.e., the same sequencedoes not receive bonus points for appearing morethan once).3 Probabi l is t ic  ModelsThis section describes how we desigued and builteach of our five models.
For consistency, we continueto print written English word sequences in italics(golf ball), English sound sequences in all capitals(G AA L F B A0 L).
Japanese sound sequences inlower case (g o r u h u b o o r u)and katakanasequences naturally (=':t. 7 .~-  ~).3.1 Word  SequencesThe first model generates cored word sequences,the idea being that ice cream should score higherthan ice creme, which should score higher thannice kreem.
We adopted a simple unigram scor-ing method that multiplies the scores of the knownwords and phrases in a sequence.
Our 262,000-entryfrequency list draws its words and phrases from theWall Street Journal corpus, an online English namelist, and an online gazeteer of place names."
A por-tion of the WFSA looks like this:los / 0.000087federal / O .O013~ angele s~~ month 10.000992An ideal word sequence model would look a bitdifferent.
It would prefer exactly those stringswhich are actually grist for Japanese translitera-tots.
For example, people rarely transliterate aux-iliary verbs, but surnames are often transliterated.We have approximated such a model by removinghigh-frequency words like has, an, are, am, were,their, and does, plus unlikely words correspondingto Japanese sound bites, like coup and oh.We also built a separate word sequence model con-taining only English first and last names.
If we know(from context) that the transliterated phrase is apersonal name, this model is more precise.3.2 Words  to Engl ish SoundsThe next WFST converts English word sequencesinto English sound sequences.
We use the Englishphoneme inventory from the online CMU Pronuncia-tion Dictionary, 3 minus the stress marks.
This givesa total of 40 sounds, including 14 vowel sounds (e.g.,AA, AE, UW), 25 consonant sounds (e.g., K, 1tlt, It), plusour special symbol (PAUSE).
The dictionary has pro-nunciations for 110,000 words, and we organized aphoneme-tree based WFST from it:E:E:EE:IH?
;::KNote that we insert an optional PAUSE between wordpronunciations.
Due to memory limitations, we onlyused the 50,000 most frequent words.We originally thought to build a general letter-to-sound WFST, on the theory that while wrong(overgeneralized) pronunciations might occasionallybe generated, Japanese transliterators also mispro-nounce words.
However, our letter-to-sound WFSTdid not match the performance of Japanese translit-2Available from the ACL Dat~ Collection Initiative.3ht%p ://~ww.
speech, cs.
cmu.
edu/cgi-bin/cmudict.130erators, and it turns out that mispronunciations aremodeled adequately in the next stage of the cascade.3.3 Engl ish Sounds to Japanese  SoundsNext, we map English sound sequences ontoJapanese sound sequences.
This is an inherentlyinformation-losing process, as English R and Lsounds collapse onto Japanese r, the 14 Englishvowel sounds collapse onto the 5 Japanese vowelsounds, etc.
We face two immediate problems:1.
What is the target Japanese sound inventory?2.
How can we build a WFST to perform the se-quence mapping?An obvious target inventory is the Japanese syl-labary itself, written down in katakana (e.g., " )  ora roman equivalent (e.g., hi).
With this approach,the English sound K corresponds to one of 2 (ka),-'Y (ki), ~' (ku), ~ (ke), or = (ko), depending onits context.
Unfortunately, because katakana is asyllabary, we would be unable to express an obvi-ous and useful generalization, amely that Englishg usually corresponds to Japanese k, independent ofcontext.
Moreover, the correspondence of Japanesekatakana writing to Japanese sound sequences i notperfectly one-to-one (see next section), so an inde-pendent sound inventory is well-motivated in anycase.
Our Japanese sound inventory includes 39symbols: 5 vowel sounds, 33 consonant sounds (in-cluding doubled consonants like kk), and one spe-cial symbol (pause).
An English sound sequencelike (P R OW PAUSE S AA K ER) might map onto aJapanese sound sequence like (p u r o pause s akk a a).
Note that long Japanese vowel sounds arewritten with two symbols (a a) instead of just one(an).
This scheme is attractive because Japanesesequences are almost always longer than English se-quences.Our WFST is learned automatically from 8,000pairs of English/Japanese ound sequences, e.g., ( (sAA K ER) --* (s a kk a a)).
We were able to pro-duce'these pairs by manipulating a small English-katakana glossary.
For each glossary entry, weconverted English words into English sounds us-ing the previous section's model, and we convertedkatakana words into Japanese sounds using the nextsection's model.
We then applied the estimation-maximization (EM) algorithm (Baum, 1972) to gen-erate symbol-mapping probabilities, shown in Fig-ure 1.
Our EM training goes like this:1.
For each English/Japanese quence pair, com-pute all possible alignments between their ele-ments.
In our case.
an alignment is a drawing.
that connects each English sound with one ormore Japanese sounds, such that all Japanesesounds are covered and no lines cross.
For ex-ample, there are two ways to align the pair ((LOW) <-> (r o o)):L OW L OWl /\ /\ Ir o o r o o2.
For each pair, assign an equal weight to eachof its alignments, such that those weights sumto 1.
In the case above, each alignment gets aweight of 0.5.3.
For each of the 40 English sounds, count up in-stances of its different mappings, as observed inall alignments of all pairs.
Each alignment con-tributes counts in proportion to its own weight.4.
For each of the 40 English sounds, normalize thescores of the Japanese sequences it maps to, sothat the scores sum to 1.
These are the symbol-mapping probabilities shown in Figure 1.5.
Recompute the alignment scores.
Each align-ment is scored with the product of the scores ofthe symbol mappings it contains.6.
Normalize the alignment scores.
Scores for eachpair's alignments should sum to 1.7.
Repeat 3-6 until the symbol-mapping probabil-ities converge.We then build a WFST directly from the symbol-mapping probabilities:PAUSE:pauseAA:a / 0 024 ~ AA:o / 0,018o < --oOur WFST has 99 states and 283 arcs.We have also built models that allow individualEnglish sounds to be "swallowed" (i.e., produce zeroJapanese sounds).
However, these models are ex-pensive to compute (many more alignments) andlead to a vast number of hypotheses during WFSTcomposition.
Furthermore, in disallowing "swallow-ing," we were able to automatically remove hun-dreds of potentially harmful pairs from our train-ing set, e.g., ((B AA R B ER SH AA P) - -  (b a ab a a)) .
Because no alignments are possible, suchpairs are skipped by the learning algorithm; caseslike these must be solved by dictionary lookup any-way.
Only two pairs failed to align when we wishedthey had--both involved turning English Y UW intoJapanese u, as in ((Y UW K AH L EY L IY) ~ (ukurere) ) .Note also that our model translates each Englishsound without regard to context.
We have built alsocontext-based models, using decision trees recededas WFSTs.
For example, at the end of a word, En-glish T is likely to come out as (= o) rather than (1;).However, context-based models proved unnecessary131e J P(j l e)o 0.566a 0.382a a 0.024o o 0.018AE a 0.942y a 0.046AH a 0.486o 0.169e 0.134i 0.IIIu 0.076AO o 0.671o o 0.257a 0.047AW a u 0.830a w 0.095o o 0.027a o 0.020a 0.014AY a i 0.864i 0.073a 0.018a i y 0.018B b 0.802b u 0.185CH ch y 0.277ch 0.240tch  i 0.199ch i 0.159tch  0.038ch y u 0.021tch  y 0.020DHd 0.535d o 0.329dd o 0.053j 0.032z 0.670z u 0.125j 0.125a z 0.080EH e 0.901a 0.069ER a a 0.719a 0.081a r 0.063e r 0.042o r 0.029eE?J P( J  l e)e e 0.641a 0.122e 0.114e i 0.080a i 0.014F h 0.623h u 0.331hh  0.019a h u 0.010G g 0.598g u 0.304gg u 0.059gg 0.010HH h 0.959w 0.014IH i 0.908e 0.071IY i i 0.573i 0.317e 0.074e e 0.016JR j 0.329j y 0.328j i 0.129j j  i 0.066e j i 0.057z 0.032g 0.018j j  0.012e 0.012k 0.528k u 0.238kk u 0.150kk 0.043k i 0.015k y 0.012L r 0.621r u 0.362M m 0.653m u 0.207n 0.123n m 0.011N n 0.978NG n g u 0.743n 0.220n g 0.023e j P(j  I e)OW o 0.516o o 0.456o u 0.011OY o i 0.828o o i 0.057i 0.029o i y 0.029o 0.027o o y 0.014o o 0.014P p 0.649p u 0.218pp u 0.085pp 0.045PAUSE pause  1.000R r 0.661a 0.170o 0.076r u 0.042u r 0.016a r 0.012s u 0.539s 0.269sh 0.109u 0.028ss  0.0148H shy  0.475sh 0.175ssh  y u 0.166ssh  y 0.088sh i 0.029ssh  0.027shy  u 0.015t 0.463t o 0.305t t  o 0.103ch 0.043t t  0.021ts  0.020ts  u 0.011TH s u 0.418s 0.303sh 0.130ch 0.038t 0.029e j PU le )UH u 0.794u u 0.098dd 0.034a 0.030o 0.026UW u u 0.550u 0.302y u u 0.109y u 0.021V b 0.810b u 0.150w 0.015W w 0.693u 0.194o 0.039?
0.027a 0.015e 0.012y 0.652i 0.220y u 0.050u 0.048b 0.016z 0.296z u 0.283j 0.107s u 0.103u 0.073a 0.036o 0.018s 0.015n 0.013i 0.011sh  0.011ZH j y 0.324sh  i 0.270j i 0.173j 0.135a j y u 0.027shy  0.027s 0.027a j i 0.016F igure  1: Eng l i sh  sounds  ( in cap i ta l s )  w i th  probab i l i s t i c  mapp ings  to Japanese  sound sequences  ( in lowercase),  as learned by es t imat ion -max imizat ion .
On ly  mapp ings  w i th  cond i t iona l  p robab i l i t ies  g reater  than1% are shown,  so tile f igures may not  sum to 1.132for back-transliteration.
4 They are more useful forEnglish-to-Japanese forward transliteration.3.4 Japanese  sounds  to KatakanaTo map Japanese sound sequences like (m o o 1:a a) onto katakana sequences like (~- -$ t - - ) ,  wemanually constructed two WFSTs.
Composed to-gether, they yield an integrated WFST with 53states and 303 arcs.
The first WFST simply mergeslong Japanese vowel sounds into new symbols aa, i i ,uu, ee, and oo.
The second WFST maps Japanesesounds onto katakana symbols.
The basic idea isto consume a whole syllable worth of sounds beforeproducing any katakana, e.g.
::-:,0951This fragment shows one kind of spelling varia-tion in Japanese: long vowel sounds (oo) are usu-ally written with a long vowel mark (~- )  but aresometimes written with repeated katakana (~) .We combined corpus analysis with guidelines froma Japanese textbook (Jorden and Chaplin, 1976)to turn up many spelling variations and unusualkatakana symbols:?
the sound sequence (j ?)
is usually written ~,but occasionally ?:.?
(g u a) is usually ~'T,  but occasionally YT .?
(w o o) is variously ~z'---,  ~r - ,  or with aspecial, old-style katakana for wo.?
(y e) may be =I=, d ~,  or d ~.?
(w i ) i s  either #~" or ~ 4.?
(n y e) is a rare sound sequence, but is written-~* when it occurs.?
(1: y u) is rarer than (ch y u), but is written~-~- when it occurs.and so on.Spelling variation is clearest in cases where an En-glish word like swiIeh shows up transliterated vari-ously (:~ ~" :, ?-, :~4 ~, ?-, x ~, 4 ~, 4-) in differentdictionaries.
Treating these variations as an equiv-alence class enables us to learn general sound map-pings even if our bilingual glossary adheres to a sin-gle narrow spelling convention.
We do not, however,4And harmfully restrictive in their unsmoothedincarnations.generate all katakana sequences with this model;for example, we do not output strings that beginwith a subscripted vowel katakana.
So this modelalso serves to filter out some ill-formed katakanasequences, possibly proposed by optical characterrecognition.3.5 Katakana  to OCRPerhaps uncharitably, we can view optical characterrecognition (OCR) as a device that garbles perfectlygood katakana sequences.
Typical confusions madeby our commercial OCR system include ~ for ~-',?-for -)', T for 7,  and 7 for 7".
To generate pre-OCR text, we collected 19,500 characters worth ofkatakana words, stored them in a file, and printedthem out.
To generate post-OCR text, we OCR'dthe printouts.
We then ran the EM Mgorithm to de-termine symbol-mapping ("garbling") probabilities.Here is part of that table:k o P(o \[k)~:" ~:" 0.492~" O.4340.0427 0.011~" ~" 1.000.,~ z, 0.964\ ] ,  0.036This model outputs a superset of the 81 katakanasymbols, including spurious quote marks, alphabeticsymbols, and the numeral 7.4 ExampleWe can now use the models to do a sample back-transliteration.
We start with a katakana phraseas observed by OCR.
We then serially compose itwith the models, in reverse order.
Each intermedi-ate stage is a WFSA that encodes many possibilities.The final stage contains all back-transliterations sug-gested by the models, and we finally extract he bestone.We start with the masutaazutoonamento problemfrom Section 1.
Our OCR observes:~ x ~,--;~?
1.
- - - / -  j : /  1.This string has two recognition errors: ~' (ku)for $ (ta), and ?-(ch?)
for "3-(na).
We turn thestring into a chained 12-state/l l-arc WFSA andcompose it with the P(k\[o) model.
This yields a fat-ter 12-state/15-arc WFSA, which accepts the cor-rect spelling at a lower probability.
Next comesthe P(jlk) model, which produces a 28-state/31-arcWFSA whose highest-scoring sequence is:mas  ut  aazut  o o ch im ent  oNext comes P(elj ), yielding a 62-state/241-arcWFSA whose best sequence is:M AE S T AE AE DH UH T AO AO CH IH M EH N T AO133Next to last comes P(wle), which results in a 2982-state/4601-arc WFSA whose best sequence (out ofmyriads) is:masters tone am ent aweThis English string is closest phonetically to theJapanese, but we are willing to trade phonetic prox-imity for more sensical English; we restore thisWFSA by composing it with P(w) and extract thebest translation:masters tournament(Other Section 1 examples are translated correctlyas earth day and robert scan leonard.
)5 Exper imentsWe have performed two large-scale xperiments, oneusing a full-language P(w) model, and one using apersonal name language model.In the first experiment, we extracted 1449 uniquekatakana phrases from a corpus of 100 short newsarticles.
Of these, 222 were missing from an on-line 100,000-entry bilingual dictionary.
We back-transliterated these 222 phrases.
Many of the trans-lations are perfect: technical program, sez scandal,omaha beach, new york times, ramon diaz.
Oth-ers are close: tanya harding, nickel simpson, dangerwashington, world cap.
Some miss the mark: nancycare again, plus occur, patriot miss real.
While itis difficult to judge overall accuracy--some of thephases are onomatopoetic, and others are simply toohard even for good human translators--it s easierto identify system weaknesses, and most of these liein the P(w) model.
For example, nancy kerriganshould be preferred over nancy care again.In a second experiment, we took katakanaversions of the names of 100 U.S.
politicians,e.g.
: -Jm : / .
7' =--  ( jyon.buroo), T~/~ .~'0' I" (a.rhonsu.dama~;'?o), and "~'4 3' ?
~7 , f  :/(maiku.de~ain).
We back-transliterated these bymachine and asked four human subjects to do thesame.
These subjects were native English speakersand news-aware: we gave them brief instructions, ex-amples, and hints.
The results were as follows:correct(e.g., spencer abraham /spencer abraham)phonetically equivalent,but misspelled(e.g., richard brian /richard bryan)incorrect(e.g., olin hatch /omen hatch)human machine27% 64%7,% 12%66% 24%There is room for improvement on both sides.
Be-ing English speakers, the human subjects were goodat English name spelling and U.S. politics, but notat Japanese phonetics.
A native Japanese speakermight be expert at the latter but not the former.People who are expert in all of these areas, however,are rare.On the automatic side.
many errors can be cor-rected.
A first-name/last-name odel would rankrichard bryan more highly than richard brian.
A bi-gram model would prefer orren hatch over olin hatch.Other errors are due to unigram training problems,or more rarely, incorrect or brittle phonetic models.For example, "Long" occurs much more often than"R.on" in newspaper text, and our word selectiondoes not exclude phrases like "Long Island."
So weget long wyden instead of ton wyden.
Rare errorsare due to incorrect or brittle phonetic models.Still the machine's performance is impressive.When word separators ( , )  are removed from thekatakana phrases, rendering the task exceedingly dif-ficult for people, the machine's performance is un-changed.
When we use OCR.
7% of katakana tokensare mis-recognized, affecting 50% of test strings, butaccuracy only drops from 64% to 52%.6 D iscuss ionWe have presented a method for automatic back-transliteration which, while far from perfect, ishighly competitive.
It also achieves the objectivesoutlined in Section 1.
It ports easily to new lan-guage pairs; the P(w) and P(e\[w) models are entirelyreusable, while other models are learned automati-cally.
It is robust against OCR noise, in a rare ex-ample of high-level language processing being useful(necessary, even) in improving low-level OCK.We plan to replace our shortest-path extractionalgorithm with one of the recently developed k-shortest path algorithms (Eppstein, 1994).
We willthen return a ranked list of the k best translationsfor subsequent contextual disambiguation, either bymachine or as part of an interactive man-machinesystem.
We also plan to explore probabilistic modelsfor Arabic/English transliteration.
Simply identify-ing which Arabic words to transliterate is a difficulttask in itself; and while Japanese tends to insert ex-tra vowel sounds, Arabic is usually written withoutany (short) vowels.
Finally, it should also be pos-sible to embed our phonetic shift model P(jle) in-side a speech recognizer, to help adjust for a heavyJapanese accent, although we have not experimentedin this area.7 AcknowledgmentsWe would like to thank Alton Earl Ingram, YolandaGil, Bonnie Glover-Stalls, Richard Whitney, andKenji Yamada for their helpful comments.
We would134also like to thank our sponsors at the Department ofDefense.Re ferencesM.
Arbabi, S. M. Fischthal, and V. C. Cheng anddE.
Bart.
1994.
Algorithms for Arabic nametransliteration.
IBM J. Res.
Develop., 38(2).L.
E. Baum.
1972.
An inequality and associatedmaximization technique in statistical estimationofprobabilistic functions of a Markov process.
In-equalities, 3.E.
W. Dijkstra.
1959.
A note on two problems inconnexion with graphs.
Numerische Malhematik,1.David Eppstein.
1994.
Finding the k shortest paths.In Proc.
35th Syrup.
Foundations of ComputerScience.
IEEE.E.
H. Jorden and H. I. Chaplin.
1976.
ReadingJapanese.
Yale University Press, New Haven.F.
Pereira and M. Riley.
1996.
Speech recognitionby composition of weighted finite automata.
Inpreprint, cmp-lg/9603001.F.
Pereira, M. Riley, and R. Sproat.
1994.
Weightedrational transductions and their application to hu-man language processing.
In Proe.
ARPA HumanLanguage Technology Workshop.J.
Yamron, J. Cant, A. Demedts, T. Dietzel, andY.
Ito.
1994.
The automatic component ofthe LINGSTAT machine-aided translation sys-tem.
In Proc.
ARPA Workshop on Human Lan-guage Technology.135
