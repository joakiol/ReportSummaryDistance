1993 BENCHMARK TESTSFOR THE ARPA SPOKEN LANGUAGE PROGRAMDavid S. Pallett, Jonathan G. Fiscus, William M. Fisher,John S. Garofolo, Bruce A. Lund, and Mark A. PrzybockiNational Institute of Standards and Technology (NIST)Room A216, Building 225 (Technology)Gaithersburg, MD 20899ABSTRACTThis paper reports results obtained in benchmark testsconducted within the ARPA Spoken Language program inNovember and December of 1993.
In addition to ARPAcontractors, participants included a number of %olunteers",including foreign participants from Canada, France,Germany, and the United Kingdom.
The body of the paperis limited to an outline of the structure of the tests andpresents highlights and discussion of selected results.Detailed tabulations of reported "official" results, andadditional explanatory text appears in the Appendix.1.
INTRODUCTIONBenchmark tests were implemented within the ARPAHuman Language Technology research program during theperiod November 1993 - January 1994.
As in tests conductedlast year, the large-vocabulary continuous speech recognitiontechnology tests made use of Wall Street Journal-basedContinuous Speech Recognition (WSJ-CSR) corpus materialwhich was collected at SRI International (SRI) undercontract to the Linguistic Data Consortium (LDC).
Spokenlanguage understanding technology tests made use of ARPAAir Travel Information System (ATIS) material collected atseveral sites, processed at NIST, annotated at SRI, andprovided to participating members of the LDC.2.
WSJ-CSR TESTS2.1.
New ConditionsAll sites participating in the WSJ-CSR tests were required tosubmit results for (at least) one of two "Hub" tests.
The Hubtests were intended to measure basic speaker-independentperformance on either a 64K-word (Hub 1) or 5K-word (Hub2) read-speech test set, and included required use of eithera "standard" 20K trigram (Hub 1) or 5K bigram (Hub 2)grammar, and also required use of standard training sets.These requirements were intended to facilitate meaningfulcross-site comparisons.The "Spoke" tests were intended to support a number ofdifferent ehaUenges.Spokes 1, 3 and 4 supported problems in various types ofadaptation: incremental supervised language modeladaptation (Spoke 1), rapid enrollment speaker adaptationfor "recognition outliers" (i.e., non-native speakers) (Spoke3), incremental speaker adaptation (Spoke 4).
\[There wereno participants in what had been planned as Spoke 2.\]Spokes 5 through 8supported problems in noise and channelcompensation: unsupervised channel compensation (Spoke5), "known microphone" adaptation for two differentmicrophones (Spoke 6), unsupervised channel compensationfor 2 different environments (Spoke 7), and use of a noisecompensation algorithm with a known alternate microphonefor data collected in environments when there is competing"calibrated" noise (radio talk shows or music) (Spoke 8).Spoke 9 included spontaneous "dictation-style" speech.Additional details are found in Kubala, et al \[1\], on behalfof members of the ARPA Continuous peech recognitionCorpus Coordinating Committee (CCCC).2.2.
WSJ-CSR Summary HighlightsThe design of the "Hub and Spoke" test paradigm, was suchthat opportunities abounded for informative contrasts (e.g.,the use of bigram vs. trigram grammars, theenablement/disablement of supervised vs. unsupervisedadaptation strategies, ete).There were nine participating sites in the Hub I tests andfive sites participating in the Hub 2 tests, and some sitesreported results for more than one system or research team.The lowest word error rate in the Hub 1 baseline conditionwas achieved by the French CNRS-LIMSI group \[2,3\].Application of statistical significance tests indicated that theperformance differences between this system and a system49developed by Cambridge University Engineering Departmentusing the "HMM Toolkit" approach \[4-6\], were notsignificant.
The Cambridge University HMM Toolkitapproach also yielded excellent results for the smaller-vocabulary Hub 2 tests.
The lowest word error rate for anARPA contractor on the Hub 1 test data, for the C1condition permitting valid cross-site comparisons, wasreported by the group at CMU \[7-9\].
The CMU results werenot significantly different from the corresponding results forthe Cambridge University HMM Toolkit system.
The lowestword e:rror rate for an ARPA contractor for the (lessconstrained) P0 condition was reported by the group at BBN.R is difficult to summarize results of the spoke tests, exceptto note that there were results reported for 8 different "spokeconditions", with from 1 to 3 participants and systemstypically involved in each spoke.
Details are presented in theAppendix.2.3.
WSJ-CSR DiscussionIn NIST's analyses of the results, displays of the range ofreported word error rates for each speaker across all systemsare sometimes informative.
These displays tend to drawattention to particularly problematic speakers or systems.Figure 1 shows data for the 10 speakers and 11 systemsparticipating in the required Hub 1 C1 test.
The speakershave been ordered from low error rate at the top of thefigure to high error rate at the bottom.
The length of theplotted line indicates the range in word error rate reportedover all systems, and the one.standard-deviation p ints aboutthe mean are indicated with a "+" symbol.Note that three speakers (40h, 40j, and 40t) have unusuallyhigh error rates relative to the other seven in this test set.In previous tests involving the Resource ManagementCorpus, it was noted that high error rates seemed to becorrelated, at least indirectly, with unusually fast or slow rateof speech.
To see if this was the case for the present estdata, NIST obtained estimates of the average speaking rate(words/minute) for each of the test speakers.
These estimateswere based solely on the total number of words uttered andthe total duration of the waveform files, and moresophisticated measures would be desirable.
Figure 2 showsa plot of the word error rate vs. speaking rate for the 10speakers and 11 systems in the Hub 1 C1 test.This figure, like Figure 1, indicates that speakers 40h, 40j and40f not only have unusually high error rates relative to theother speakers in this test set, but it also indicates that forthese speakers, the speaking rate is markedly higher than forthe other seven.
Whereas the speaking rate for the sevenspeakers ranges from approximately 115 to 145 words/minute,for the three speakers with high error rate, the speaking rateranges from 165-175 words/minute.There are at least two factors that may contribute to highererror rates at these fast speaking rates: within-word andacross-word eoarticulatory effects (e.g., phone deletions)associated with fast (possibly better described as "careless" or"easuar') speech, and possible under-representation of theseeffects in the training material.Chase, et al \[9\], at CMU, noted that for the 4 speakers inSpoke 7 (40g, 40h, 40i, and 40j), two (40g and 40i) could besubjectively characterized as"careful speaker\[s\]", butthat 40hwas characterized as a "pretty fast speaker, \[with\] very lowgain", and 40j as a "very, very fast speaker".
These "fastspeakers" appear in a number of the test sets.NIST's analyses of the distributions of rate of speech for twosets of training material for the Hub 1 test (each consistingof approximately 30,000 utterances: "short-term" and "long-term" speakers) indicate that the distributions are ratherbroad, with the short-term speakers' distribution peaking at130 words/minute, with a standard deviation of 30words/minute, and the long-term speakers' distributionpeaking at 145 words/minute, with an associated standarddeviation of 30 words/minute.
Note that speaking rates forthe 3 "fast-talking" speakers fall just outside the "plus onestandard eviation region" range relative to the peak of thedistribution for the "short-term speaker" training set, and justinside the corresponding region relative to the "long-term"training set.Because anumber of the measured performance differencesbetween systems were small, and the results of the paired-comparison significance tests validated the relevant nullhypotheses, it has been observed that, in general, the use oflarger test sets, especially for the Hub tests, would have beenmore informative, especially with regard to the results ofsignificance tests requiring larger speaker populations (i.e.,the Sign and Wileoxon Signed-Rank tests).
With largerpopulations of test speakers, it would be less likely to havesuch disproportionately large representation f"fast speakers"in the test sets.Two spokes made use of microphones other than the"standard" Sennheiser close-talking microphone.
(See, forexample, the discussion in the Appendix of this paper forSpokes 5 and 6.)
Too other spokes dealt with the issue ofperformance degradations that were presumably due todegradations in the signal-to-noise ratio.
(See, for example,the discussion for Spokes 7 and 8.
)For the test data of Spokes 5-7, subsequent o thecompletion of the tests, NIST performed signal-to-noise ratio(SNR) analyses, using three different bandwidth (signal pre-processing) conditions: broadband, A-weighted, and 300 Hz-3000 kHz passband "telephone bandwidth".
The filteredSNR's are generally higher than the broadband values.Figure 3 shows the results of these SNR analyses.Figure 3 (a) indicates, the SNRs measured for the data ofSpoke 5, which includes 10 "unknown" microphones in50addition to the simultaneously collected reference Sennheiserdose-talking microphone data for each data subset, collectedin the normal data collection environment.
SRrs "normaloffices for recording" speech data have A-weighted soundlevel values in the 46.-48 dB range, There were 2 "tieelip" orlapel microphones, 5 stand-mounted microphones, a surface-effect microphone, a speakerphone, and a cordless telephonein this set of 10 test microphones.Note that the SNR values for the Sennheiser microphone aretypically about 45 dB for the both the broadband and A-weighted conditions, indicating that there is little low-frequency energy in the spectrum of the noise in theSennheiser microphone data.
Sennheiser microphone datatypically ield values of 50 dB for the telephone-bandwidthcondition.
For the alternate microphones, the broadbandSNR's range from about 23 dB (for the Audio-Teehnicastand-mounted microphone) to 45 dB (for the GE cordlesstelephone).
With filtration the SNR's are higher, asexpected.
Note that nearly all of the microphones provide atleast a 30 dB telephone-bandwidth SNR, and that the ATPro 7a lapel-mounted microphone provides approximately 40dB.Figure 3 (b) indicates the measured SNR's for the data ofSpoke 6, which includes 2 "known" alternate microphones inaddition to the reference Sennheiser dose-talkingmicrophone, collected in the normal data collectionenvironment.
For the Sennheiser dose-talking microphone,the broadband SNR's are, as for Spoke 5, 45-.46 dB.
Thereis a substantial difference between the broadband and A-weighted SNRs for the Audio-Teehniea stand-mountedmicrophone, corresponding tolow frequency noise picked upby this microphone, and for the telephone-bandwidthcondition the SNR is approximately 35 dB.
With thetelephone handset, SNRs are 38 to 40 dB, depending onbandwidth.The test set data for Spoke 7, shown in Figure 3 (c), involveduse of two different microphones (an Audio-Teehniea stand-mounted microphone and a telephone handset in addition tothe usual "reference" Sennheiser dose-talking microphone),in two different noise environments, with background A-weighted noise levels of 58-68 dB.In the quieter of the two "noisy" environments, a computerlaboratory with a reported A-weighted sound level in the 58-59 dB range, the broadband SNR was approximately 34-36dB for the Sennheiser microphone, and 35 dB for thetelephone handset data, but only 17 dB for the Audio-Techniea microphone.
Spectral analyses of the Audio-Teehniea background noise data demonstrate he presence ofsignificant low frequency energy as well as the presence ofharmonic components with an approximately 70 Hzfundamental.
These components may have originated in somerotating machinery (e.g., a cooling fan or disc drive).In the noisier environment, a room containing machinerywith conveyor belts for sorting packages, with a reported A-weighted sound level in the 62-68 dB range, the broadbandSNR ratio for the Sennheiser data degraded to 27-29 dB (adecrease of approximately 7 dB), and 27 dB for thetelephone handset data, and the Audio-Techniea to 16 dB (adecrease of only 1 dB).
With A-weighting, in the quieterenvironment, he SNR for the Sennheiser improved veryslightly (less than 1 dB, relative to the broad band values),and for the Audio-Techniea it was 25 dB, 8 dB higher thanthe broad band value.In the noisier environment, the A-weighted S/N ratio for theSennheiser data was approximately 29 dB, and the Audio-Techniea 20 dB.For the telephone handset data, both the telephone-bandwidth-filtered and the A-weighted SNRs were higherthan, but typically within one or two dBs, of the unweightedvalues, as might be expected.In summary, for the quieter of the two environments u ed incollecting the data of Spoke 7, none of the data subsets inSpoke 7 had an average filtered SNR worse than about 25dB, and in the noisier environment, the worst average filteredSNR for any data subset was approximately 20 dB.
TheseSNR values would not ordinarily be regarded as indicative ofsevere noise-degradation.Spoke 8 involved ata collected in the presence of competingnoise -- music and talk radio broadcasts.
For the case ofcompeting music, the broadband SNR for the referenceSennheiser microphone ranged from 44 DB for the so-called"20 dB" condition, to 36 dB for the "10 dB" condition, and 29dB for the "0 dB" condition.
For the Audio-Technicamicrophone, corresponding measured valueswere 25, 17, and11 dB.
NISTs measurements of SNR for the data containingcompeting speech were inconclusive because of the difficultyof distinguishing between the spoken test material and thecompeting talk radio.3.
ATIS TESTS3.1.
New ConditionsRecent ATIS tests were similar in many respects to previousATIS tests -- the primary difference consisting of expansionof the size of the relational air-travel-information database to46 cities, and use of a body of newly collected and annotateddata using this relational database \[I0\].
As in prior years,tests included spontaneous peech recognition (SPREC)tests, natural language understanding (NL) tests and spokenlanguage understanding (SIS) tests.
For the first time, datacollected at NIST was included in the test and training data.The NIST data was collected using systems provided to NISTby BBN and SRLIn previous years, results for NL and SLS tests werepresented and discussed in terms of a "weighted error"51percentage, which was computed as twice the percentage ofincorrect answers plus the percentage of "No Answer"responses.
The decision to weight 'kvrong answers" twice asheavily as "no answer" responses was reconsidered within thepast year by the ARPA Program Manager, and this year onlyunweighted NL and SLS errors are reported (i.e., incorrectanswers count the same as "No Answer n responses).
Formost system developers, this change of policy has appearedto result in changed strategies for system responses, o thatin this year's reported results, little use was made of the "NoAnswer" response.3.2.
Summary  H igh l ightsFor the recent ATIS tests, results were reported for systemsat seven sites.
Lowest error rates were reported by the groupat CMU \[11\].
The magnitude of the differences betweensystems is frequently small, and the significance of thesesmall differences i not known.As in previous years, error rates for "volunteers n are generallyhigher than for ARPA eontraetors, possibly reflecting alesserlevel-of-effort.Additional details about he test paradigm, and comments onsome aspeets by individual partieipants, are found in anotherpaper in this Proceedings, by Dahl, et al, on behalf ofmembers of the ARPA Multi-site ATIS Data COllectionWorking (MADCOW) Group \[10\].
Details about thetechnical approaches used by the partieipants, and their ownanalyses and comments, are to be found in references \[11,23-28\].3.3.
ATIS DiscussionThis year, 46% of the utterances were classified as Class Aand 34% in Class D, so that 80% of the test utterances were"answerable" (i.e., Class A or D).
Last year's test set hadabout the same percentage of Class A queries (43%), butsomewhat fewer classified as Class D (i.e., 25%), so that lastyear only 67% were answerable.
One possible reason for thischange (other than the test-set-to-test-set fluctuations) maybe that the Principles of Interpretation document iscontinually being extended to cover phenomena that wouldhave otherwise resulted in eategorization f some queries as"unanswerable", and therefore Class X.For text input (NL test), for last year's test material, thelowest unweighted NL error rate was 6.5% for the ClassA+D subset, 6.5% for Class A, and 6.4% for Class D, incontrast with this year's corresponding figures of 9.3%, 6.0%and 13.8%.
Note that this year's test set apparently had"more diffieult" Class D queries, and that there was a largerfraction of the queries that were classified as Class D thanlast year (34% vs. 25%).For speech input (SLS test), and for last year's unweightedtest material, the unweighted SLS error rate was 11.0% forthe Class A+D subset, 10.2% for Class A, and 12.5% forClass D, in contrast with this year's corresponding figures of13.2%, 8.9% and 17.5%.Note that while the lowest error rate for Class A queries issmaller this year (i.e., 8.9% vs. 10.2%), this year's best ClassD error rate was substantially higher than last year's.
It maybe the ease that this is related to the extended coverageprovided by the current Principles of Interpretationdocument, so that queries that in previous years would havebeen classified as unanswerable, are now judged to beanswerable, although context-dependent.4.
ACKNOWLEDGEMENTSThe "Hub and Spokes" Test paradigm could not have beendeveloped, specified, or implemented without he tireless andeffective fforts of Francis Kubala, as Chair of the ARPAcontinuous speech recognition Corpus CollectionCoordinating Committee (CCCC).
The tests would also nothave been possible without the dedicated efforts of DeniseDanielson and her colleagues at SRI in collecting anexceptionally large and varied amount of CSR data for CCSRsystem training and test purposes.
In the ATIS community,Debbie Dahl served as Chair of the MADCOW group, andit is to her credit that new data was collected at several siteswith the 46 eity relational database and that participatingsites reached agreement on the details of the current ests.Kate Hunicke-Smith and her colleagues at SRI Internationalwere again responsible for annotation of ATIS data and forassisting NIST in the adjudication process followingpreliminary scoring.
It is a pleasure to acknowledge Kate'sthoughtful and cheerful interactions with our group at NIST.As in previous years, the cooperation of many participants inthe ARPA data and test infrastructure -- typically severalindividuals at each site -- is gratefully acknowledged.52REFERENCES\[1\] Kubala, F., et al, "The Hub and Spoke Paradigm for CSREvaluation", in Proceedings of the Human LanguageTechnology Workshop, March 1994 (Weinstein, C.J., ed.
).\[2\] Gauvain, J.L, Lamel, LF., Adda, G. and Adda-Decker,M., "The LIMSI Continuous Speech Dictation System:Evaluation on the ARPA Wall Street Journal Task", inProceedings of ICASSP'94.\[3\] Oauvain.
J.L, Lamel.
LF.
Adda, G. and Adda-Decker,M., "The LIMSI Continuous Speech Dictation System" inProceedings of the Human Language Technology Workshop,March 1994 (Weinstein, C.J., ed.
).\[4\] Woodland, P.C., Odell, J.J., Valtehev, V. and Young, S.J.,"Large Vocabulary Continuous Speech Recognition UsingHTK", in Proceedings of ICASSP'94.\[5\] Odell, J.J., Woodland, P.C., and Young, SJ., "Tree-basedState Tying for High Accuracy Acoustic Modelling", inProceedings of the Human Language Technology Workshop,March 1994, (Weinstein, C.J., ed.
).\[6\] Odell, JJ., Valtchev, V., Woodland, P.C., and Young,SJ., "A One Pass Decoder Design for Large VocabularyRecognition," in Proceedings of the Human LanguageTechnology Workshop, March 1994 (Weinstein, CJ., ed.
).\[7\] Hwang, M., Thayer, F_,.
and Huang, X., "Semi-continuousHMMs with Phone-Dependent VQ Codebooks forContinuous Speech Recognition" in Proceedings ofICASSP'94.\[8\] Hwang, M., et al, "Improving Speech RecognitionPerformance via Phone-Dependent VQ codebooks andAdaptive Language Models in SPHINX-II" in Proceedings ofICASSP'94.\[9\] Hwang, M., Thayer, E., Mosur, R. and Chase, L, "Phone-Dependent Codebooks and Multiple Speaker Clusters inSPHINX-II", Oral Presentation at the Spoken LanguageTechnology Workshop, March 6-8, 1994, Princeton, NJ.\[10\] Dahl, D., et al, ~Expanding the Scope of the ATIS Task:The ATIS-3 Corpus ~, in Proceedings of the Human LanguageTechnology Workshop, March 1994 (Weinstein, C..I., ed.
).\[11\] (a) Ward, W. and Issar, S., "Recent Improvements in theCMU Spoken Language Understanding System M, inProceedings of the Human Language Technology Workshop,March 1994 (Weinstein, C.J., ed.
), and (b) Issar, s., and ward,W., "Flexible Parsing: CMU's Approach to Spoken LanguageUnderstanding", Oral Presentation at the Spoken LanguageTechnology Workshop, March 6-8, 1994, Princeton, NJ.\[12\] Garofolo, J., Robinson, T. and Fiscus, J., "IheDevelopment of F'fle Formats for Very Large SpeechCorpora: SPHERE and Shorten", in Proceedings ofICASSP'94.\[13\] (a) Zavaliagkos, G., et al, ~BBN Hub System andResults', (b) Lapre, C., et al, "Speaker Adaptation for Non-Native Speakers', (e) Anastasakos, A. et al, "FEnvironmentalRobustness: Adaptation to Known Alternate Microphones ~,and (d) Nguyen, Let  al., "Spoke 9: Spontaneous WSJDictation ~, Oral Presentations at the Spoken LanguageTechnology Workshop, March 6-8, 1994, Princeton, NJ.\[14\] Ostendorf, M. et al, "Stochastic segment Modelling forContinuous Speech Recognition: Wall Street JournalBenchmark Report ~, Oral Presentation at the SpokenLanguage Technology Workshop, March 6-8,1994, Princeton,NJ.\[15\] (a) Seattone, F., et al, *Dragon's Large VocabularySpeech Recoguition System", (b) Odoff, J., et al, "Spoke $4:Speaker Adaptation ~,and (e) Orloft, J., et al, "Spoke $6:Microphone Adaptation', Oral Presentation at the SpokenLanguage Technology Workshop, March 6-8,1994, Princeton,NJ.\[16\] Morgan, N., et al, "Scaling aHybrid HMM/MLP Systemfor Large Vocabulary CSR", Oral Presentation at the SpokenLanguage Technology Workshop, March 6-8,1994, Princeton,NJ.\[17\] (a) Paul, D.B., "The Lincoln Large Vocabulary Stack-Decoder Based HMM CSR", in Proceedings of the HumanLanguage Technology Workshop, March 1994 (Weinstein,CJ., ed.
), and (b) Paul, D.B., "The Lincoln Large VocabularyStack-Decoder Based HMM CSR: Spoke $4 IncrementalSpeaker Adaptation n, Oral Presentation at the SpokenLanguage Technology Workshop, March 6-8,1994, Princeton,NJ.\[18\] (a) Robinson, T., Hochberg, M. and Renals, S., "IPA:Improved Phone Modelling with Recurrent NeuralNetworks", in Proceedings of ICASSP'94, (b) Hochberg, M.,Robinson, T., and Renals, S. "ABBOT: The CUED HybridConneetionist-HMM Large-Vocabulary Recognition System",Oral Presentation at the Spoken Language TechnologyWorkshop, March 6-8, 1994, Princeton, NJ.\[19\] Aubert, X., et al, "The Philips Large Vocabulary CSRSystem", Oral Presentation at the Spoken LanguageTechnology Workshop, March 6.-8, 1994, Princeton, NJ.\[20\] Aubert, X., Dugast, C., Ney, H. and Steinbiss, V., "LargeVocabulary Continuous Speech Recognition of Wall StreetJournal Data n, in Proceedings of ICASSP'94.53\[21\] (a) Rosenfeld, R., "A Hybrid Approach to AdaptiveStatistica.l Language Modelling", in Proceedings of theHuman Language Technology Workshop, March 1994(Weinstein, C..I., ed.
), and (b) Chase, L, Mosur, R., andRosenfeld, R., "Language Model Adaptation in the CSREvaluaticm", oral Presentation at the Spoken LanguageTechnology Workshop, March 6-8, 1994, Princeton, NJ.\[22\] (a) IJu, EH., Moreno, P.J., Stem, R.M., and Aeero, A.,"Signal Processing for Robust Speech Recognition", inProeeedi:ngs ofthe Human Language Technology Workshop,March 1994 (Weinstein, C.J., ed.)
and (b) Stem, R.M., Liu,F,H., arid Moreno, P., "Robust Speech Recognition:Research at CMU", Oral Presentation at the SpokenLanguage Technology Workshop, March 6-8,1994, Princeton,NJ.\[23\] Boechieri, E., "The ATT ATIS System: March 94Report", Oral Presentation at the Spoken LanguageTechnology Workshop, March 6-8, 1994, Princeton, NJ.\[24\] (a) Stallard, D., et al, "Recent Work in SpokenLanguage Understanding in the BBN SLS Project", and (b)Miller, S., et al, "Statistical Language Processing UsingHidden Understanding Models", Oral Presentations at theSpoken Language Technology Workshop, March 6-8, 1994,Princeton, NJ.\[25\] Normandin, Y., "CRIM's December 1983 ATIS System",Oral Presentation at the Spoken Language TechnologyWorkshop, March 6-8, 1994, Princeton, NJ.\[26\] "The MIT ATIS System: March 1994 Progress Report",Oral Presentation at the Spoken language TechnologyWorkshop, March 6-8, 1994, Princeton, NJ.\[27\] Moore, R. and Cohen, M. et al, "SRI's Recent Progresson the ATIS Task", Oral Presentation at the SpokenLanguage Technology Workshop, March 6-8,1994, Princeton,NJ.\[28\] Dahl, D., Linebarger, M., Nguyen, N. and Norton, L,"Unisys Acth,Sties inSpoken Language Understanding", OralPresentation atthe Spoken Language Technology Workshop,March 6-8,\[29\] Digilakis, V., et al, "SRI November 1993 CSR Hubevaluation", Oral Presentation at the Spoken LanguageTechnology Workshop, March 6.-8, 1994, Princeton, NJ.\[30\] Weintraub, M., et.
al., "SRI November 1993 CSR SpokeEvaluation', Oral Presentation at the Spoken LanguageTechnology Workshop, March 6-8, 1994, Princeton, NJ.NOT ICEThroughout this paper, a number of references are providedin order to refer readers to relevant papers and oralpresentations by researchers at the indMdual sitesparticipating in the tests.
In some of these papers, results arecited that differ by small amounts from those tabulated inthis paper.
In some cases the authors cite unofficial orpreliminary, "pre-adjudieation" results.
In other eases, theauthors cite other unofficial test results conducted after the"official" test period dosed.The views expressed in this paper are those of the author(s).The results presented are for local, system-developer-implemented tests.
NIST's role in the tests is one ofselecting and distributing the test materials, implementingscoring software, and uniformly tabulating the results of thetests.
The views of the author(s) and these results are not tobe construed or represented asendorsements of any systemsor official findings on the part of NIST, ARPA or the U.S.Government.54APPENDIX:"BENCHMARK TEST RESULTS"A.1.
WSJ-CSR November 1993 Test MaterialThe 1993 WSJ-CSR tests make use of newly-collectedtraining material, a new compressed waveform file format,new test paradigms, and new test sets.The new training material for the WSJ-CSR task includes asubstantial mount of data (31 CD-ROMs containing trainingand developmental test data) collected at SRI Internationalunder contract o the Linguistic Data Consortium (LDC).In a collaborative ffort involving NIST, Tony Robinson atCambridge University's Engineering Department, and theLDC, the newly collected waveform data was processed withan "embedded" version (i.e., the file's SPHERE-formatheader is uncompressed, but the bulk of the file iscompressed) of a lossless waveform compression algorithm("shorten") using the NIST SPHERE file header convention,to reduce the storage requirements for this data by a factorof approximately 50% \[12\].
The CSR test material wasreleased in November.A.2.
WSJ-CSR Test Scoring and AdjudicationThe CSR tests were conducted in November and December.Test and scoring protocols were similar to last year.However, new to the CSR benchmark tests this year was theaddition of an official adjudication period.
Following apreliminary scoring of recognition results, sites participatingin the tests were permitted to submit requests foradjudication to NIST.
Adjudication requests in the CSRdomain contained requests for transcription modificationsdue to transcription errors, alternative transcriptions, etc.A total of 22 bug reports were received from 6 sites.
Thebug reports contained requests for changes to 199 (151unique) utterance transcriptions in all WSJ-CSR test sets.The NIST adjudicators carefully evaluated each request andultimately revised transcriptions of83 utterances (55% of theones in question.
)Of the transcriptions that were revised, most were the resultof judgements by the adjudicators that the transcriptionscontained words which could have multiple orthographicrepresentations (e.g., compound words, variant orthographicrepresentations, etc.)
or which were lexically ambiguous.
Inmany of these eases, both the original transcription and analternative transcription were permitted.
This wasimplemented by mapping alternate word forms to a singleform in both the transcriptions and the recognized strings.The remaining revisions were the correction of simpletranscription errors.A.3.
WSJ-CSR Test ParticipantsUnited States participants in the WSJ-CSR tests included:BBN Systems and Technologies (BBN) \[13\], BostonUniversity (BU) \[14\], Carnegie Mellon University (CMU) \[7-9\], Dragon Systems \[15\], the International Computer ScienceInstitute (ICSI) at Berkeley \[16\], Massachusetts Institute ofTeehnology's Lincoln Laboratory (MIT/LL) \[17\], and SRIInternational (SRI) \[29,30\].Foreign participants included two British groups atCambridge University's Engineering Department, onepursuing eonnectionist approaches (CU-CON) \[18\], andanother, developers of the HMM Toolkit (CU-HTK) \[4-6\],a French group at CNRS-LIMSI (LIMSI) \[2,3\], and aGerman group at the Philips GmbH Research Laboratoriesin Aachen \[20\].BU collaborated with BBN, making use of the N-best outputsof a BBN system, using an N-best reseoring formalism, astochastic segment modelling approach, and the use of bothBU and BBN knowledge sources.A.4.
WSJ-CSR Benchmark Test ResultsAA.1.
Hub 1: 64K Baseline.
The intention of the two "Hub"tests was "to improve basic \[speaker independent\]performance on clean \[read speech\] data".
For Hub 1, testdata consisted of 200 utterances -- 20 from each of 10speakers, using the primary (Sennheiser series HMD 410)microphone as used in prior tests.All sites were required to provide results for a static (i.e.,non-adaptive) Speaker-Independent (SI) baseline system thatwould permit cross-site comparisons, which would use thestandard 20K word trigram "open vocabulary" grammar anduse standardized training sets.The results of that baseline system are tabulated in thecolumn labelled "Contrast CI" in Table 1.Results for (optional) use of the same system training, butwith the 20K bigram grammar, are shown in the columnlabelled "Contrast C2".
These 'eontrastive' results wereintended for comparison with results for optional 'primary'systems.
The priraary systems could use "any grammar oracoustic training", and these results are shown in the columnlabelled "P0".In most cases, data from each site shows on a single line.The three BU "C1" systems each represent different N-bestrescofing formalisms using the BU stochastic segment modelrecognition system in combination with the BBN Byblossystem, using different knowledge sources to re-rank the N-best hypotheses.
The two different CMU systems aredifferent in many ways, so that comparisons are non-trivial.55For the baseline "el" systems, word error rates ranged from19.0% to 11.7%, with the lowest error rate reported for theLIMSI System.In this table, and others of this sort in this paper, the resultsof contrastive comparisons are shown in the boxes labelled"COMPARISONS AND SIGNIFICANCE TESTS".
Theresults of use of the NIST statistical significance tests thathave been used in previous tests are also shown.To illustrate interpretation of some of the tabulated results,note that BBN and MIT/LL achieved reductions in error rateof 13.9% and 9.8%, respectively, for their P0 systems whencompared to the C1 baseline systems.
In most cases, thesereductions were shown to be significant.
Refer to \[13\] and\[17\] for discussion of factors contributing to these reductionerror rate.When contrasting use of trigram and bigram grammars, anumber of sites achieved reductions in error rate of fromapproximately 12% to 23% for the ease of use of the trigramgrammar.Table 2 shows a matrix tabulation of the results of cross-siteand, in some eases, within-site, paired comparison statisticalsignificance tests for the baseline H1-C1 systems.A.4.2.
Hub 2: 5K Baseline.
Because run times for full 20Ksystems were in some cases regarded as prohibitive, asecondbaseline Hub test, requiring only a 5K lexicon, was permitted.For Hub 2, the required static SI baseline C1 system madeuse of a standard 5K bigram dosed vocabulary grammar andeither of two smaller training sets, consisting ofapproximately 7200 sentence utterances.As for Hub 1, the Hub 2 test data consisted of 200 utterances-- 20 from each of 10 speakers, using the primarymicrophone.Not surprisingly, error rates for the 5K systems were lowerthan for the 20K systems.Table 3 shows that for the baseline C1 systems, error ratesranged from 17.7% to 8.7%, with the lowest error ratereported by the Cambridge University's HTK research group\[4-6\].
For the P0 systems, for which "any grammar or acoustictraining" were permissible, lower error rates were to beexpected, and were achieved, typically with reductions inerror rate of from 25% to almost 50%.
In this case, also,one of the HTK configurations achieved the lowest worderror rate: 4.9%.Table 4 shows a matrix tabulation of the results of cross-siteand, in some eases, within-site, paired comparison statisticalsignificance tests for the baseline H2-C1 systems.A.4.3.
Spoke 1: Language Model Adaptation.
The statedgoal for this language model adaptation spoke was "toevaluate an incremental supervised language model (LM)adaptation algorithm on a problem of sublanguageadaptation".
The sole participant was Rosenfeld et al atCMU \[21\].
Test data consisted of read speech data fromfour speakers, each reading 1 to 5 articles consisting ofapproximately 20-25 sentence utterances, with the Sennheisermicrophone.
NIST's scoring was done on four successive 5-sentence utterance blocks throughout he articles (i.e.,utterances 1-5, 6-10, 11-15, and 16+).
Use of the statisticalsignificance tests was not thought o be appropriate sincethese tests assume independence of errors across sentences,and this assumption is probably not valid when using anadaptive language model.Table 5 presents the results for Spoke 1.
The column labelledP0 shows results with ineremental unsupervised adaptationenabled: word error rates vary from 16.5% on the first blockof 5 sentences to 18.2% on the last block.
In contrast, withlanguage model adaptation disabled, the word error ratescorrespondingly vary from 20.5% to 21.1%.
Comparisonsbetween P0 and C1, ir~olving enabling/disabling ofsupervised LM adaptation, indicate reductions in word errorrate of between 9.8% to 19.4%, with lesser eductions for theP0:C2 comparisons involving unsupervised LM adaptation.A.4A.
Spoke 3: SI Recognition Outliers.
'Hae stated goalfor this spoke was "to evaluate a rapid enrollment speakeradaptation algorithm on difficult speakers (e.g., non-nativespeakers of American English)".
The sole participant wasBBN \[13\].
Test data consisted of read speech from tenspeakers, each reading 40 sentence utterances, with theSennheiser microphone.
For each speaker, the 40 "rapidenrollment" utterances were available for use with the "rapidenrollment" speaker adaptation.Table 6 presents the results for Spoke 3.
The column labelledP0 shows results with rapid enrollment adaptation enabled:word error rate for the 400 utterance test set is 14.5%.
Incontrast, with adaptation disabled, the word error rate is32.0%.
Alternatively, the P0:C1 contrast indicates areductionin error rate 54.7%, which was shown to be significant usingall of the significance tests applied by NIST.A.4.5.
Spoke 4: Incremental SpoakerAdaptatlon.
The statedgoal for this spoke was "to evaluate an incremental speakeradaptation algorithm".
Two sites participated: Dragon \[15\]and MIT/LL \[17\].
In this spoke, there were only four testspeakers, with 100 sentence utterances for each.
NISTsscoring was done on four successive 25-sentence utteranceblocks (i.e., utterances 1-25, 26-50, 51-75, and 76+).Table 7 presents the results for Spoke 4.For the Dragon results, word error rates for the P0 condition(with incremental unsupervised adaptation enabled) rangefrom 15.5% to 14.3%.
For MIT/LL, the correspondingvariation is 10.9% to 11.1%.
There is evidence of significantreductions in error of the order of 20% to 30% for the P0:C1contrasts for the Dragon results (e.g., note the reduction offrom 19.4% to 15.5% for the first block of 25 utterances).56For the corresponding MIT/LL results, the magnitudes ofthereductions are not as large.
For both sites, the incrementalchanges in error rates between the P0 and C2 eases, involvingunsupervised/supervised adaptation, in most eases are notshown to be significant, and range from approximately 4% to16%.A.4.6.
Spoke 5: "Microphone Independence".
The statedgoal of this spoke was to "evaluate an unsupervised channelcompensation algorithm".
The different "channels" in thisease were different microphones -- each of the ten speakersin this test set used a different (unknown) microphone.Similar, but not identical, microphones had beenincorporated in training and development material.
For the200 utterances in each portion of this test set, both theunknown microphone data (in "wv2" data files) andcorresponding Sennheiser microphone data (in "wvl" files)were available.Both CMU \[22\] and SRI \[30\] participated in this spoke.Table 8 presents the results for Spoke 5.With unsupervised channel compensation e abled, the CMUsystem achieved an error rate of 15.1%, in contrast to 20.9%with compensation disabled -- a 27.8% reduction in worderror rate.
SRI achieved a comparable reduction of 24.2%,and with slightly lower error rates.
With compensationenabled, the CMU system achieved 9.7% word error for thecorresponding Sennheiser data, while the SRI systemachieved 6.6% word error.
Enabling/disabling the channelcompensation made essentially no difference for the case ofthe Sennheiser data subset, as might be suspected.A.4.7.
Spoke 6: Known Alternate Microphones.
The statedgoal of this spoke was to "evaluate a known microphoneadaptation algorithm".
There were two different microphones-- an Audio Techniea stand-mounted microphone, and atelephone handset which was to be connected to the datacollection apparatus "over external lines", in addition to theSennheiser (wvl) data.
Two-channel microphone adaptationdata -- for each of the two microphones and the (reference)Sennheiser microphone was provided fi'om "devtest data".There were ten speakers for the data for each of the twomicrophones, with 20 sentence utterances per speaker.
InNIST's analysis of the results, data are separately tabulatedfor the Audio-Teehniea (at) data, and for the telephonehandsets (th).Three sites participated: BBN \[13\], Dragon \[15\], and SRI\[301.Table 9 presents the results for Spoke 6.For the case of the microphone adaptation disabled (C1), forthe Audio-Technica microphone's data, word error rateswere 6.4% for the SRI system, 10.4% for the BBN system,and 18.5% for the Dragon results.
For telephone handsetdata, the SRI system had 19.1%, the BBN system had 29.3%and Dragon 65.4%.
These results for the telephone handsetdata were probably somewhat worse than might have beenexpected because of inadvertent channel differences betweendevelopment test and evaluation test sets.Considering the adaptation enabled/disabled P0:C1 contrast,BBN and Dragon achieved 9.4% and 11.7% reductions inword error rate for the Audio-Teehniea microphone, and57.4% (from 29.3% to 12.5% word error) and 11.7% forBBN and Dragon, respectively.
On corresponding Sennheiserdata, the BBN and SRI systems with adaptation disabledachieved word error rates ranging from 5.9% to 8.4%, whilethe Dragon results were 13.8% and 14.6%.A.4.8.
Spoke 7: "Noisy Envlronments".
The stated goal ofthis spoke was to "evaluate a noise compensation algorithmwith known alternate microphones" in two different data-collection environments with background A-weighted soundlevel of from 55 to 68 dB.
Two different microphones wereused, the same microphones as were used for Spoke 6, (theAudio-Teehniea and a telephone handset).
Utterances for themicrophone/channel adaptation (Sennheiser to knownalternate microphone) were available from development testdata, and there were files with background noise (but nospeech) for each microphone-noise-environment-speakercondition.
The two noise environments ("el" and "e2")consisted of computer laboratory (el), and a room withpackage sortation machinery in operation ("e2").The sole participant in this spoke was SRI \[30\].Table 10 presents the results for Spoke 7.As might be expected, the word error rate was smallest forthe lower of the two noise conditions with the alternate high-quality (but not close-talking) Audio-Technica microphone(8.5%) (for which the A-weighted S/N ratio wasapproximately 26dB), and markedly higher for both alternatemicrophones in the higher noise environment (17.4% and28.8%).
For corresponding data from the close-talkingSennheiser microphone, in the two different noiseenvironments, error rates of from 6.3% to 9.1% wereobtained.A.4.9.
Spoke 8: "Calibrated Noise Sources".
The stated goalof this spoke was to "evaluate a noise compensationalgorithm with a known alternate microphone on datacorrupted with calibrated noise sources".
Data was collectedusing the Audio-Technica microphone, which was also usedin Spokes $6 and $7, in the presence of competing noise(from a "boom box" radio-tape player situated nearby).
Thecompeting noise was either a variety of musical selections("mu") or talk radio ("tr").
The competing noise was"calibrated" in the sense that the level of the competing noisewas intended to be set so as to be 20 or 10 dB less than thespeech peak level, or equal to (or potentially greater than)the speech peak level, the "0 dB condition".
Note howeverthat NIST's measurements of SNR do not agree well withthese desiderata, as discussed in Section 2.3 of this paper57except ha some qualitative sense.CMU \[212\] was the sole participant in this spoke.Table 11 presents the results for Spoke 8.Data were submitted for the 3 competing noise conditions,both microphones (Sennheiser and Audio-Teehniea), andwith noise compensation e abled and disabled -- a total of 24conditions, permitting many cross-comparisons.With compensation disabled, there were reductions in errorrate with use of the close-talking, noise cancelling Sennheisermicrophone when comparing results for the two differentmicrophones (C3:C1).
With compensation enabled, andagain comparing the two different microphones (C3:P0), thedifferences in error rate are reduced, but are still significantin most cases.There is evidence of significant reductions in error rate whenconsidering compensation enabled/disabled (P0:C1) for bothmusic and talk radio at the 10 dB and 0 dB conditions.Further, enabling compensation appears to be beneficial formuch of the data obtained with the close talking Sennheisermicrophone (see, for example the C3:C2 comparisons).AA.10.
Spoke 9: Spontaneous WSJ Dictation.
The statedgoal of this spoke was to "improve basic performance onspontaneous dictation-style speech".
There were 10 speakers(all journalists, but with varying experience indictation), eachdictating 20 spontaneous Wall Street Journal-like sentenceutterances, and using the Sermheiser microphone.BBN \[13\] was the sole participant in this spoke.Table 12 presents the results for Spoke 9.Using the same system as used for the C1 condition in Hub1 (which achieved a word error rate of 14.2% on the Hub 1test data), a word error rate of 24.7% was achieved on the $9data, indicating that the spontaneous dictation $9 test set issubstantially more challenging.
BBN's $9 system achieved anerror rate of 19.1% on the $9 data, a significant reduction inword error rate of 22.8% over the H1-C1 system.test set for number of subjects or the difficulty of scenariosper collection site.
No "pre-filtering" of the test data wasperformed except to attempt o exclude subject-scenarioswith mostly repetitive queries.
The ATIS test material wasreleased in November, 1993.A.6.
ATIS Scoring and AdjudicationThe ATIS scoring and adjudication process took place inDecember and early January.
ATIS test and scoringprotocols were similar to those of previous benchmark tests.After the scored ATIS results were released in December1993, approximately 140 adjudication requests ("bug reports")were sent to NIST.
NISTworked in conjunction with SRI toresolve the requests, about 10 of which were duplicates.The majority of the bug reports dealt with transcriptionissues, in some cases pointing to limitations in ourcommunity's procedures for transcribing ATIS-domainspontaneous speech.
One utterance, inparticular, which wasclassified as Class X (and thus did not affect he NL or SLSscores), but was included in the ATIS SPREC scoring,included low-level remarks by the experimenter, asa resultof an inadvertent "open mike" condition.
Originally, thisblock of speech was transcribed as "unintelligible", but inadjudication, it was fully transcribed, partially because anumber of sites had objected to having been scored withsignificant numbers of insertion errors.
ARer adjudication,most sites continued to do very poorly on this one utterance,but were now penalized for substitutions and deletions aswell.
It alone accounts for an increment of approximately0.3% in the Class A+D+X word error for most sites, and asubstantially arger fraction of the Class X error rate.
Inretrospect, it is clear that this problematic utterance (and theentire subject-scenario) ught not to have been included inthe test set because of the "open mike" condition.Besides the recurrent complaints of bad transcriptions, aproblem involving fare IDs or flight IDs not appearing in themaximal reference answer fdes (the "rf2s") (which came to beknown as "Joe's Fare Bug") was brought o our attention.This bug was attributed to about 21 of the test utterancesbefore scoring.
The bug was fred by SRI and new .rf2s weregenerated prior to rescoring.A.5.
AT IS  November  1993 Test  Mater ia lThe final, adjudicated set of test material consisted of 965test utterances and was collected at 5 sites -- BBN, CMU,MIT, NIST and SRI.
As in previous years, it was selected byNIST staff rom set-aside material previously collected withinthe MADCOW community \[10\].
The test set was selected soas to balance the number of utterances per data collectionsite ( -200  utterances per site.)
Because of differences inthe scenarios and data collection systems used at thedifferent collection sites, it was not possible to balance theA.7.
AT IS  Test  Par t i c ipantsUnited States participants in the ATIS tests included: AT&TBell Laboratories (AT&T) \[23\], BBN Systems andTechnologies (BBN) \[24\], Carnegie Mellon University(CMU) \[11\], Massachusetts Institute of Technology'sLaboratory for Computer Science (MIT/LCS) \[26\], and SRIInternational (SRI) \[27\], and Unisys (UNISYS) \[28\].
Therewas one foreign participant: (CRIM) \[25\], from Canada.AT&T collaborated with CMU, using an AT&T-developed58ATIS..domain speech recognition system and the CMU ATISnatural language system, and Unisys collaborated with BBN,using a set of N-best outputs for a BBN ATIS-domain speechrecognition system as input for Unisys-developed naturallanguage technology.A.8.
ATIS Benchmark Test ResultsA.8.1.
SPontaneous peech RECognition (SPREC) Tests.Table 13 presents the results for the SPREC tests for allsystems and subsets of the ATIS test data, using theSennheiser close-talking microphone.
For the case of thesubset of all answerable queries, Class A+D, the word errorrates ranged from 3.3% to 9.0%.Table 14 presents a matrix tabulation of the ATIS SPRECresults for the Class A+D subset.
The overall word error rateacross all tested systems for the data from the severalcollecting sites ("Overall Totals" row along the bottom of theTable) ranges from 3.6% for the CMU-eolleeted ata to6.8% for the NIST-eollected ata, reflecting differences insubject populations and other factors.Table 15 presents the results, in matrix form, of theapplication of 4 paired-comparison significance tests for theSPREC systems for the Class A+D subset.
Among otherthings, note that the performance differences between theBBN and the CMU systems are not shown to be significant,and that the differences between the MIT, SRI and one ofthe Unisys systems are also not shown to be significant.
Notealso that significant differences are shown between the BBNresults and those for the two Unisys systems, which make useof BBN-provided N-best results.A.8.2.
Natural Language (NL) Understanding Tests.
Table16 presents atabulation of the results for the NL tests for allsystems and all sets of "answerable" ATIS queries, ClassA+D, Class A and Class D.For the set of all answerable queries, Class A+D, theunweighted error rate ("UW.
Err.")
ranges from 43.1% to9.3%.
For Class A queries, the range is 28.6% to 6.0%, andfor Class D, the range is 63.1% to 13.8%.
In each ease (andas in last year's results), the lowest error rates were reportedby the CMU system.As noted in Section A9 of this paper, the AT&T NL systemwas the results of a collaborative agreement with CMU, thusit is not surprising that the performance is nearly identical tothat of the CMU system.There are, in some cases, more than one set of resultssubmitted by individual sites, corresponding to differentsystems.
The differences between systems were specified inthe "Systems Descriptions" provided to NIST at the timeresults were submitted.
Space limitations prohibit discussionof these differences in this paper.After preliminary scoring had been completed, Moore at SRIadvised NIST that a bug had been found in the code thatproduced results submitted to NIST for the SRI NL and SLSsystems, with the effect of reporting results that were"essentially the output of \[the SRI\] system with the robustprocessing component turned off", because a "No_~Answer"response over-wrote the answer produced by the robustprocessing component (a "template mateher").
With thepermission of the ARPA Coordinating Committee, SRI laterresubmitted results for the debugged systems, and these SRIresults are shown as "late, debugged" results.Table 17 presents a matrix tabulation of the official NLresults for the several subsets of test material.
There is someindication of varying degrees of difficulty presented by thedifferent subsets of data from the different sites, subject-scenarios, and subject populations: note that the unweightederror rates reported in the "Overall Totals" row ranges from28.1% to 16.0%, but also note that both these values wereobtained with BBN systems -- one at BBN, and the other atNIST.
These differences probably are not significant sincethe numbers of speakers in the individual test sets is small.A.8.3.
Spoken Language System (SLS) Understanding Tests.Table 18 presents atabulation of the results for the SLS testsfor all systems and all sets of "answerable" NTIS queries,Class A+D, Class A and Class D.For the set of all answerable queries, Class A+D, theunweighted error rate ("UW.
Err.")
ranges from 46.8% to13.2%.
For Class A queries, the range is 33.5% to 8.9%, andfor Class D, the range is 65.2% to 17.5%.
For the ClassA+D and Class A results, the lowest error rates wereobtained by the CMU system, but for the Class D results, thelowest error rates were obtained by the MIT/LCS system.Table 19 presents a matrix tabulation of the official SISresults for the several subsets of Class A+D test materialfrom different sites.
Note that there is some evidence of"local adaptation" to locally collected ata (e.g., error ratesfor the CMU system are substantially lower for the CMU-collected ata).Note also that some sites (typically the "volunteers")continued to use the "No_~swer" option more frequentlythan others, which would be a beneficial strategy in a systemin which "wrong answers" were penalized more heavily than"no answer".
In some eases, use of this option was moreprevalent for data from some originating sites than others,perhaps reflecting differences between subject populations orsubject-scenario subsets.59RANGE ANALYSIS ACROSS SPEAKERS FOR THE TEST :November 1993 Hub I, Contrast 1by Speaker Word Error for Speakersi .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
lI .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
_ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I~0 5 10 15 20 25 30 35, 40 45 50 55 60 65 70 75 80 85 90 95 i001I sPKR I I I I I I I I I I I I I I I I I I I I I III 4oA I +- I - * - -  II '?z4c~ I - - * -  I - * - - - .
I -* II ,loc I * - - I - - * - - -  I4oE I +- I -+- - -  I4oB I * I - * -  I I sod I * - - I - - *  II 4OH-?- -  ~ - - -?
.
.
.
.
.I =oF I - -+ .
.
.
.
I - - -+- -  IJ -> shows the mean?
-> shows plus or minus one standard deviationF igure  1 Range of  Word  er ror  rates  for  the i0 speakers  of the Hub 1 C1tes t  set  for  i i systemsoo403530252015i0DARPA November  93 H i -C1  Test  Speakers0 I6O/1 !
1 ,.
~ .\]I i .<:.~ ',// ......... x.,,/i /~ ..'\[.
;~<'.>~q,,804OB \[40A 401 40C: 40D40G !!
4OEi iI00 120 140 160Speak ing  Rate (Words per  Minute)40JF igure  2 Word  er ror  rates for Hub 1 C1 speakers  vs.6O40H40F180speak ing  rate200F ig .
3A :AT PRO Ta  T lec l lpSonnShure  839VV T lec l lp  (VV I ro less )SonnSony  F -VX500 Un ld l rooUono lSennRad loShack  33-3007 Un ld | recUona lSonnLebteo  AM-22  Dynarn l?SonnAT ATM63 Un ld l recUona lSannAKG D 1200E Card lo ldSennCrown Sound Grabber  PZMSonnSony  IT -D I0O SpoakerphoneSonnGIE 2 -9510 Cord less  Te lephoneSenn0 10  20  30  40SNR Measurements  fo r  Spoke  6Te l .
Hand.SannAud.
Teen .8annSNR Measurements  fo r  Spoke  5~d~bandIghtedSndw.F ig .
3B  :80 600 1 0 20  30  40  80  80F ig .
3C  :SNR (d  B~.__._____Te l  HandSenn --E n ~ f i r o n m e:i;:Sann ~ ",,,: .............. : ............. :::::: .... ::::::: .... ::::::::: :::::::::.SNR Measurements  fo r  Spoke  7LegendOroadbendI A we ighted\ [~  Te l l ,  Bndw,nt2Te l .
Hand.SennAr id .
Teeth .Sann.
.
.
.
.
.
.
.
~ & ~ .
.
o  I Env i ronn~ent  1i;~i;i~i:i;i:i:i:i:i:i:i:i;i:iii~:~i:i:i:i:i:i:i:i:i:i:::i:i:i:i:!;i~i~i~i~i~i~i~i;i!i~i!i!i!i:i:i:i;i:i;i;i~i~i~i~i~i~i!i:i?i:i:i:i:!;!:i;!
:i:::::::::::::::::::::::::::::::::::::::::::::::::::::::::0 10  20  30  406150 60Nov 93 Hub andSpoke  CSR Eva luat ionHub I: 64K  Read WSJ  Base l ineGOAL:  Improve  bas ic  S I  per fo rmance  on  c lean  data.DATA:  10  speakers  * 20 utts  = 200 ut ts  64K-word  read  WSJdata ,  Sennhe iser  mic .P r imary  and  cont ras t  Cond i t lon8P0 (opt} any  g r ~ r  or  acoust i c  t ra ln lng ,  sess ionboundar ies  and  ut terance  order  g iven  as  s idein fo rmat ion .C1  (req) S ta t i c  S I  test  wlth  s tandard  20K t r i~ramopen-vocab  grammar  add  cho ice  of  e i ther  shor t - te rmor  iong- termspeakersC2  (opt)  S ta t i c  S I  tes t  w i th  s tandard  20K b lgramopen-vocab  g r ~ r  and  cho ice  of  e i ther  shor t - ta taRor  long- term BpeakersS IDE  INFO:  Sess ion  boundar les  and  ut terance  order  a re  knownfo r  H i -P0  on ly .
~ .
.
.
.
.
.
.. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.I P r imary  P0  I Cont ras t  C1  I C~tras t  c2System I Word  Er r .
(%} I Word  Err .
(%) I Word  Er r .
(%).
.
.
.
.
.
; .
.
.
.
.
.
.
; ; : ;  .
.
.
.
.
.
; .
.
.
.
.
.
.
; ; : ;  .
.
.
.
.
.
; .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.bu2 14 .3bu3  14 .5~u l  ' " I c~nu2 \[ 13.9  I 13.6 Idrag~.
l  19 .0l lms l l  11 .7  15.2mlt - l l l  I 16.8  I 18.6  Iph i l ips2  1 I 14 .8  l 17 .2s r l l  I I 14.4 I 16.5==================================================================COMPARISONS AND S IGNIF ICANCE TESTS.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Tes t  % Change S ign i f i cance  Tests :Comp.
W.E .
MAPSSWE S ign  wi lcoxon McNbbn l  P0 :C l  13 .9% P0 same P0 P0mi t - l l l  P0 :C I  9 .8% P0 P0 P0 P0.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Tes t  % Change S ign i f i cance  Tests :Comp.
W.E .
MAPS~E S ign  Wl lcoxon  McNcu-htk l  C I :C2  11.7%, C l  C1 C l  samel ims i l  C I :C2  22 .7% C1 C1 C l  C1ph i l ips2  C I :C2  14 .0% Cl  C1 C1 sames r l l  C I :C2  13 .0% C1 C1 C1 C1?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?Tab le  1 Hub 1 Resu l ts62i ||!!
II|| I||~ :"  : .... :~ "~:~ ""  , !
!
!
!
, !
!
!
!
i~!
!
!
i~o ,o ,~:~!
!~ io , !~ l l J i:--I I + +-- - - - -+, ~ ~ : I  t I : ~~: i ' ~ : ~ i  ~ ~ i~:i.i~33~i~33!3!33i3333i3!
!~i !!!
3~33 , ~ , ~ , ~ , ~  ~ , ~ , ~  ~i l l  .
.
.
.
~ .
.
.
.
i "  " ' i  .
.
.
.
i ", , ,~q ,q~ ,q~ ,qq~ ,~ ~.~qq~,: ;~  !~ .
.
.
.
:~ :~ .
.
.
.
.
.
, :~  : ~ : ~  ~ : ~ :~ i ~  i~:  , , ,~:~I  :~  .
.
.
.
.
:~ '~ ' :  .
.
.
.
.
.
.
.
.
.i~  : .
.
.
.
.
.
.
.
t - - - :  - t  .
.
.
.
.
.
.
.
.
.
.
.
.
: l ;  | !
t !
!~t~| i | | | | i  : ~;~ .
.
.
.
.
,~  " :~ '~ ' :  .
.
.
.
.
:~ i+ .
.
.
.
.
!~ " ' "  17"7" :i i  i l: t~; t !
t~; t  Iv-  + - - - - +  * - - - - - -  + - - - -  ~:~  ~ ~ ,i~  :i i~ ; t !
:: i17i l  :: ~, ~ ~, !~ '~  : :~ !~ :~ i~  ~, ~ ~ ~ ~ :~ !~:~ i~  ~ ~ ~ ~ :~ :~ : :.IJt-I( Jm4-)E~UU -Hu~4-Hb~E~63II Nov  93 Hub and Spoke CSR Eva luat ionI Hub 2: 5K Read WSJ  Base l ineIi ' GO.AL : IDATA:IIIIIf 1 S IDE INFO: 8eee lon  boundar ies  and ut te rance  order  are knownI .............. ~L_~_:~_~: .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Pr imary  P0 \[ Cont ras t  C1J .
.
.
.
~ .
.
.
.
.
.
.
; .
.
.
.
.
~-~; ;?
-~;  .. .
.
.
i .
.
.
.. ~ ; ;~-~; ; : -~  .
.
.
.
.bu2 | 5 .4  | 10.3cu-~on l  13.5I I "' 1 "" cu-htk3  12.5  I cml l  17o7ph i l lpe l  9.2  1 12.3I ph111ps2  ~ 6.4 i=================================================================I COMPARISONS AND S I~ IF ICANCE TESTS.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Tes t  % change S ign i f i cance  Tests:I Comp.
W.E.
MAPSSWE Sign Wi lcoxon  McNIbul  p0:CI  42.4% P0 P0 P0 P0I bu2 R0:CI  47.4% P0 P0 P0 P0bu3 P0:C1 46.6% P0 p0 P0 P0I cu -h tk2  p0:Cl  43.4% P0 P0 P0 P0l ims i2  P0:C1 43.7% P0 p0 P0 P0ph i l lps l  P0:Ci  25.5% P0 P0 P0 P0?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?Improve  bas ic  SI per fo rmance  on c lean  data.10 Qpeakere  * 20 ut ts  = 200 ut tu  SK-word  read  WSJdata, Sennhe lsermic .P r imary  and Cont ras t  Cond i t ions(opt\] any  gr~- r  or acoust ic  training, ~ess lonboundar les  and utterax*ce order  g iven  as  e ldein format ion.
(req) S ta t i c  SI test  w i th  s tandard  5K b ig ramc loeed-vocab  gr -~r  and cho ice  of e i thershor t - te rm or long- te rm speakers  f rom WSJ0  (7.2Kut ts ) .Tab le  3 Hub 2 Resu l ts64,~ '~o o ~, o~ ,~i, - -  + - - - - - -  + - - - -  +i~4~,~ mmi i+ - - - - ~ +?, 3 ?,~ mI 1 1 1 1 1 ~ 1 1, ~~ , ~  ~i , , 1 1mmmm~ N N- -+ .
.
.
.
+ _ _ _ _  + ______  +______  +______  ~i i i i  i i i i  i ~ .
.
.
.
.
.
.
.
.?
~ ' ~i.
- - - -  + - - - - - -  ?
- - - - - -  +!
:I - -  + - - - - - -  + - - - -i, - - ?
- - - - - - + - - - - - -~ ~,~,~~ i?o~ o, o,:,.
.
.
.
.I I I I I~ 1 1 1 11+-- - - - -  + - - - - - -I I I I i1?1 QI ~ QIi l l lI l t lo o ~O~o,.-I,.-fvrQr - IU~-,.-i-,-i-r-I650 ,-I ,-IO0~o ~ ~ ~ o .
~ ,,~'= .~ .~ ~ ~ 5 ~ ~ ~ ~"  ~~?
o ,  ~ o ~ ~ : .
.
."
?~ ~' ~ ~i  ~',~ ~ -+- t -~',~,I00-,-I4~OO-,-I4J--IOH U~~4000.t,--I0MO 0 ~~ .
g ~~ oo~~g0 ~.
.0 -o ~- -?
- -+  .
.
.
.0 ho ~!'
,~ ~I - - -~  - -+  .
.
.
./I I I +I ~ ~ rn rnen  , ~Ii ~' UUUUIH .
.
.
.
.I I I+UUUO?
~ o ~oODo~U O U Ul l l f00-,--t~00~00rd0~0LO66I / lo ~ .
~ ~?
~.
.~ ~ ~ _.~., ~.o  ~ o ?
~ ~ ~ ~~o ~ ~ ~,~oo~ ~ ~0,0 ,~ .~ , .~a  ~~ ~ ~ ~o~.
:~  ~ ~.
_ ~ o~o~?
I I- - @ - - + - - - - I Iog ~o 0N- - ?
- - + - -- - ?
- - ?
- --,-t- - + - - + - - t1.1 t ~o l~N~o oo  ~o ~. m ~ Cmmm oo ~ ~~ - -  ?__  - -?- - _ _ ?
_  _?ou ~ uu~o ~o ~~o ~ .~ ~o ~~10 ?00 O~ rOD0 0-.4..4 ~O - t  -,-4 DO~??
~?
?- -  ?
- - - - - -  t-~ oo  ~ o o  ~.
o~ ~ ~.oo~m~ rn~ :~ uu?
~ DOO~~ mm~ oo?
~m~oI ~  O0\ [ - - - - ?
.
.
.
.it1IIt- 'C~0O000mm uueO ~I l l0000r~00H0o0-MooO3CO..or - i??
'~" ~?
~ ~ ~ ~0'~  ~ ~ ~ ~ ~ ~.
.~o0o  ~ ~0 ~ ~?~ ~01o 0 ~ LI ~ ~ 0 ..,-t~ ~ ~ ~ ~,~ ~ ~ ~oum + _0 vIll0 ~- -+__t - - + - - +ttii, ~, ~, ~~ m o o ~ o ~~d~dgg~~om momI ?
I l l ?.
.
.
.
.
.
~t0q-.t b.l~ uu~ouuue g gg&;gS; ;~om momI ?
I l l *0 0 0 0 ~o o o o o o o ooo  ~UDU DUD~o~o~o~ o o o o ~0o00rut0?0H0067 E~~ ~0Ill I~ 'el ..-i ~ C i J  ,0 .,.iU~ ?
?
~ O~a~O~ill .,,-i ~.i?
!
l i  Ill fill r l  .l,a ill ,-I @~ IJ /,4 0 i l~O U i J  III i Jz ~,~o~ ~ , ~  ~ oIiii_ _ ~ i + +i i ui i IiI I IIi i IIi I I Ii ~ I Ii'lIII I ~1 I I IIU i~ i  ui i i i~J i ?
i m,d ,~m iiI,~ i ~ I i - lO l (M IIi J  i i iiI i u~,t~,  ?
.U # 0 I III ~ I III I #unI I J J I I~i ~ ,?l ,G iiill iii i J  oUU~Ui~1 ,o P. t~.o~ ,rh-~ v't v,,Io~ UUUU~,,,U UUUUu u ~cq cq ~q ~4 J=I n~JCEco~4-,~Co}.,q0=,-M0i noE~.
I J, - - Ii J  .l..i i{i o i..i !El .z:l ,,~ i..i i.CO "G'~ ~J ~ e 0 c~ ?=~ ~ o  Oo~o~ ~?
,~  c ~ o ?
o ~o~ ~o o~ ~ ?
~ , ~  o gg~0 ~o o~ ~ R l~J ?
~ ID~~o ,~ .
.
.
.
.
~o~ .~.
~ ~ i~ ?~ o~ ~ .o?~ ,~ "~ o~ o .
.
.
.
o ~ <~o ~??
~Oct  ~ 0 ~ -~-~~, .
,~  ~ ~,?
:>,,~:>,~'~ ,~o .
~ ~ ~ ~, ,oOi l  ~1 I ~a .u ,-I la +.-I ~.
bo~o~-  ~ o ~ o  ~ o o ~  ~ =~oUC 0- -+- -?
- - _ _ _i ii ii ii i I-i ir ,  i ,.0 io i i.~ iU i  o li - -+- -+- - t,~9  ~>~o 0~oooUUUUooUUUU~o ~~o ~ ~UUU~ ~.
.
.~u u o u uUUU~~?eO OUOU U~ ~ u ~u~~ m mO0C C ~CoJ~0(bajco4J<0U U U U U U~u uuuo~u Ii~ , 0ioo68..Qml~0~O~H- ,C:i !
oo0oEw ,,,"~ el!
o.o~mt~M~UUb~U~UU~o ~~7~mt~uuu~uu~o ~ ?o ouu~u~imi~uu~uu~0WU UUOUUUE,,-ID~oWO UUUOU~ie~ ~.
?
.
.
.
.
.
~o ~ .
.
.
.
.
.
~ ~9~.
.
.oo  oo  oo  oo~ o ~ o  ~ o ~ oo uou  o o u u u u~O UUOUUU ~O UUUUOOoo  oo  oo  oo  I~ o ~ o  ~ o ~ o lI I I I~  I I I I I I I~~.
.
.
.
.
.
~ .
.
.
.
.
.
~ .
.
.
.
.
.
~ 9 ~ 9O~~U UUOUUU ~0 UUUUUOIoo  oo  ~o~?
oo  ?too oo  uuo,  oui4..1,--4UIIll=i11000-H~o o ?
~ ~.
~ 0  .
.~D~Eo ~ ~~ ~?~?
~o 0 .oo~0 ?
~ ~~0 ~ ~ ~ o ~ ~oii~l , l Jo i ~.ii- -+- -?~E- -+- -+~ o ~ o~11111~ , ~uou04J-,-Ir--iuou},-I, - I,.Q69.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
~ ~  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i :  ~ .
.
.
.
.
.
.
oo  I g o .
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
g o .
.
.
.
.
.
.
.to4o,-44oowUu lMIDr-.toO0 ~ ~~ o ~ ~ ~ill@ O* l~ ./:I 0,~o 0 ??
~ ,~~o o~ ~D., 0 ..,d (0oo ~ ~ ~ .
.
.
.~.
~ o~ ' ,~  o~ ~ico iX~0 ~~0~ o ,E.., , - I  IZ ulH,~  ?o~o ~ oc2170CO4O,---4Wo-,-I.IJr~.IJo-,-Icoow.IJoC~coo04oq0).Dr~\[-~Dec93 ATIS SPREC Test Results  "C lass  A?D Subset  ~ lOr ig inat ing  Si te of Test  Data I Overa l l  I Fore ignBBN l C24U l M IT  I NIST -BBN ~ NIST-SRI  l SRI I TotA ls  I Coi l .
S i tel (146 Utt.)
I (163 Utt . )
I (132 Utt.)
1 (89 Utt.}
I (77 Utt.)
{ (166 Utt.)
~ 773 I Tota ls.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.art2 l 6.3 1.8 1.31 3.4 1.4 1.0 4.6 2.3 1.21 4.3 3.0 2-91 8.6 2.6 2.01 6.5 2.0 1.11 5.4 2.1 1.5 5.4 2.1 1.5I 9.4 49.3 I 5.0 25.2 8.1 47.0 I 10.2 51.7 I 13.2 49.4 i 9.6 40.4 I .
9.0 42.2 9.0 42.2. .
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.bbr3 I 0 .7  0 .1  0.51 1 .4  0 .4  0.61 2 .1  0 .3  1.01 3 .2  1.2 1.91 3 .6  0 .6  0.51 2 .2  0 .4  0.51 2 .0  0 .5  O.Sl  2 .3  0 .6  0 .9I 1.3 7 .5  I 2 .4  11.7 I 3 .5  2:~.5  I 6.3 34.S I 4 .9  23 .4  I 3 .1  17 .5  I 3 .3  10.0  I 3 .8  20 .4I 3 .5  23.3  I 2 .5  11,7 I 3.1 24.2  I 4.77 24.7 I 3 .8  22.1 I 3 .1  16.9 I I  3.3  19.7 I 3 .S  21 .0S cr lm3 I 3.0 0.4 1.31 2.1 0.7 2.1 I 3.7 0.5 2.51 4.4 1.5 3.7~ 5.2 0.8 2.4 I 4.1 0.0 2.311 3.6 0.7 2.3 I 3.6 0.7 2.3Y I 4 .8  24 .0  t 4.9 23 ,3  I 6.77 34.B I 9.6 46.1 I 8 .5  36.4  I 7 .2  32 .5  I I  6 .6  31.3  I 6 .6  31.3s .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I I .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
.
.
.T mi t  les2 i 2.2 0.8 0.41 1.7 1.5 0.5\] 3.2 1.4 1.0 I 1.9 1.2 0.81 4.4 1.1 0.51 3.0 1.2 0.711 2.6 1.2 0.71 2.5 1.2 0.6E I 3.4 23.3 l 3.7 16,6 l 5.6 31.0 I 3.9 24.7 l 6.0 28.6 l 5.0 19.9 II 4.5 23.3 l 4.2 21.5M .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l l ...............................S sri3 2.8 1.0 0.71 1.8 1.0 0.6 I 2.6 1.4 0.71 2.3 2.0 0.5~ 4.4 1.8 2.0j 1.9 1.3 0.51 2.5 1.4 0.7 2.6 1.4 0.8I 4.6  27 .4  I 3 .5  14 .3  I 4 .6  30 .3  I 4 .0  29 .2  I 8.2 31.2 I 3 .8  16.3 t 4 .6  23 .3  I 4.8 25.2. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I I  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.i r i4 i 2.2 1.0 0.51 1.8 1.0 0.61 2 .4  1.2 0.8 i 2.0 2.5 0.9 i 4.4 1.5 2.3 i 2.1 1.2 0.S I 2.3 1.3 0.0 i 2.4 1.4 0.9| 3.8 23.3 I 3.4 13.5 I 4.4 20.0  I 5.4 29.2 I 8.2 32.5 ~ 3.8 16.9 l 4 .5  22.3 l 4 .6  23.7. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.un i~ys2 1.3 0.5 0.7 1.4 0.3 1.3 .2.0 0.6 1.2 3.8 1.3 1.5 4.4 1.0 1.0 2.6.
0.6 0.6 2.3 0.7 1.0 2.3 0.7 1.0I 2.6  14 .4  I 3 .0  14 .7  I 3 .0  25.B  I 6 .6  34 .s  I 6.4 32 .s  I 3 .9  20 .s  I I  4 .0  21 .9  I 4 .0  21.91.3 6.8 3.0 15.3 3.6 24.2 7.0 32.6 I 6.2 28.6 4.3 20.5 3.9 19.7 I 3.9 19.7=========================================================== : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : :Overa l l  I 2.4 0.7 0.7 1.8 0.0 1.0 2.7 1.0 1.1 3.1 1.8 1.71 4.7 1.3 1.31 3.1 1.0 0.8 ITota ls  I 3 .9  22.1 3.6 16.2 4.8 30.0  6.$ 34.2 I 7.3 31.6 I 4.9 22.4i i ;:;I i .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
sy ,~ I 4.2 24.0 I 3.7 16.8 { 4.7 29.7 I ~.5 34.2 I 7.3 31.6 I 5.2 24.0 II %W.Er r  %Utt .E r r  I. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Matr ix  tabulat ion of resul ts  for the Dec93 AT IS  SPREC Test  Results ,  for the Class A?D Subset.Matr ix  co lumns present  results  for Test  Data  Subsets  co l lec ted  at several sites, and matr ix  rows present  results  for d i f fe rentsystems.Nt~mbers pr in ted  at the top of the matr ix  co lumns  ind icate  the number  of ut terances in the Test  Data (sub)set from the cor respond ingmite.
"Overal l  Totals"  (colttmn) present  resu l ts  for the ent i re  Class A+D Subset for the system cor respond ing to that  matr ix  row.
"Foreign Coll.
Site Totals" present  resu l ts  for  " fore ign  slte" data (i.e., excluding \]ccel ly co l lec ted  data) for the Class A?DSubset.
"Overal l  Totals"  (row) present  res~ Its accumulated  over  all systems corresponding to the Test Data (sub)set cor respond ing  to thati~atrix column.
"Foreign System Totals"  p resent  results  acc~Imulated over "foreign systens" (i.e., exc lud ing results  for thesystem(s) deve loped at the site respons ib le  for co l lec t ion  of that Test  Data s~ibset.
)Tab le  14  AT IS  SPREC Resu l ts :  C lass  (A+D) bv  Co l lec t ion  S i te71I ~ I I I  i.~ =='"  ??
?o o.~o~ ~ ~ o  o?~ ~o ~~ o0 ,'el ui- - +  - - ?
- - - - ?
.
- - - - ?
~  + .
~ - -f f~fII.
.
.
.
, ,~ i  .,.= i iII~,~,=,~ ~ ~=,I I i ,  I.,==II I I i I II i i i  I I i i  I I I I  I l l l  I i i i"!!!i"?
- - - - - - ?
- -~,~,  ~- -  ?
- - - - ?
- - - - ?
- - - - - -  ?
- - - - ?
.
.
.
.
.~ 1222oJ J J J  ' ' '~ '- -  ?
- - - - - - + - - - - +  .
.
.
.
.
.
.I- -  ?
- - - - - -  + - - - - - -  + - - - -I I~ ,  ~, I I I Iiiii1R ?
- - - - D  + - - - - - -i , i iiim + _ _ - - m ~  .
.
.
.i t4JU)H4JCOC~J~CO(UO~-,,-I-,-ICO,-4E~72Class A+D Class A Class D773 Utts.
448 Utts.
352 Utts.syste~ UW Err.
UW Err.
DW Err.attl 10.2 7.4 14.2bbnl 14.7 9.6 21.8bbn2 22.4 16.1 31.1cmul 9.3 6.0 13.8criml 36.4 21.7 56.6crim2 20.8 14.7 29.2mit_icsl 12.5 10.
O 16.0sril 21.9 14.3 32.3sri5 ** 18.2 10.5 28.9unlsysl 43.1 28.6 63.1Tab le  16  AT IS  NL  Test  Resu l tsClass (A+D} Set l IOrlgltx~tlng Site of Test Data ~ Overell I ForeignBEN \] C~U l MIT l NIST-SRI NIST-B~N SRI Totals Coll.
Sitel 146 l 163 l 132 l 77 l 89 l 166 \] 773 l Total,.... ~ ....... \[;;--;;---;-'-i;;--\[;---;T~;V-~V--;-i--~;--~;---; .... ;;---;---;-i-~;;--~;---;rl-~;J;;---;-?-;;i--;;---; -85 15 0 94 6 0 I 92 8 0 I 87 13 0 90 10 8 I 9o 10 0 I 90 10 0 90 10 0I 15 .1  I 6 .1  I 8 .3  I 13 .0  I 10 .1  I 10 .2  I ~0.2  I 10 .2......... + ............. + ............. + ............. ?
............. ?
............. + ..............
I ............. + .............bb~d I 124 21 I I 141 22 0 I 117 15 0 I 57 20 0 I 82 7 0 138 28 0 I 659 113 1 535 92 085 14 I I 87 13 0 I 89 11 0 74 26 0 92 8 0 83 17 0 85 15 0 85 15 0I 15 .1  I 13 .5  I 11 .4  I 26 .0  I 7 .9  I 16 .9  I 14 .7  I 14.7. .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.bbn2 104 41 I l 127 36 0 112 20 0 54 23 0 69 20 O 134 32 0 600 172 I I 496 131 017128 117822 018515 017030 01  78 22 018119 0 , 7822 0 \ [  79 21 028.8  22 .1  15 .2  29 .9  l 22 .5  19 .3  I I  22 .4  l 20 .9i~  .
.
.
.
.
i I ; ;~- ;~ i~I ; ;~- I ;~; - i~ ; ;~-~T~;~i~;~;~;~i~; ; -~;~- - ;~ i~I ; ;~;~; -  -7 ; I -7~- - - ; -T - ; J - ;~- - ; -I 88 12 0 I 94 8 o I 91 9 0 I 87 13 0 I 90 10 0 } 92 8 0 I I  91 9 0 I 90 10 oI 12 .3  I 6 l l  I 9 .1  I 13 .0  I 10 .1  I 7 .8  l 9 .3  I l o .2. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
t l  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.S crlml I 76 61 9 I 114 31 18 I 88 36 8 I 40 32 5 \] 79 10 0 I 95 57 14 l 492 227 54 I 492 227 54Y I 52 42 6 I 70 19 Ii I 67 27 6 I 52 42 6 I.
89 II 0 I 57 34 8 I 64 29 7 1 64 29 7s I 47 .9  I 3o .1  I 33 .3  I 48 .1  I 11 .2  I 42 .8  I 36.4 I 36.4T .
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
I I  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.E crim2 \] 112 33 1 I 133 27 3 I 109 23 0 I 57 17 3 I 76 12 I I 125 41 0 J 612 153 8 I 612 153 8M I 77 23 I J 82 17 2 1 83 17 0 I 74 22 4 1 85 13 i I 75 25 0 II 79 20 I I 79 20 1s I 23 .3  I 18 .4  I i7 .4  f 26 .0  I 14 .6  I 24 .7  I 20 .a  I 20 .8. .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
'1  .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.mit_ic~l 1 111 35 0 1 150 13 0 \[ 120 12 0 1 62 15 0 1 83 6 0 1 150 16 0 1 676 97 0 l 556 85 0I 76 24 0 I 92  8 0 f 91  9 0 I 81 19 o I 93  7 0 I 90 ao o It  87 13 o I ~7  13  ol 24.0 \[ 8.0 l 9.1 \] 19.5 I 6.7 I 9.6 I 12.5 I 13.3 ............................................................................................. II ...........................sril I 103 17 26 I 130 17 16 I III 8 13 I 54 21 2 I 75 14 0 I 131 30 5 I 604 107 62 I 473 77 57I 71 12 18 I 80 10 10 I 84 6 I0 I 70 27 3 I 84 16 0 l 79 18 3 1 78 14 8 I 78 13 9I 29 .5  t 20 .2  I 15 .9  I 29 .9  I 15 .7  I 21 .1  I t  21 .9  I 22 .1......... ?
............. + ............. ?
............. + ............. + ............. + ..... .
.
.
.
.
.
.
.
~I ............. ?
.............sri5 ** I I13 30 3 I 142 17 4 I 117 14 I I 54 21 2 I 75 14 0 I 131 30 5 I 632 126 15 I 501 96 10I 77 21 2 I 87 10 2 I 89 ii 1 I 70 27 3 I 84 16 0 \[ 79 18 3 II 82 16 2 I 83 16 2t 22 .6  I 12 .9  i 11 .4  I 29 .9  I 15 .7  I 2 l .
I  It 18.2 I 17.5. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I I  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.un isys l  I 76  31  39  I 88  33  42  I 91  26  15  I 40 24  13 I 49 27  13 I 96  20  50  I I  440 161 172  I 440 161  172  II 52 21  27 I 54 20  26  I 69 20  11  I 52 31  17 I 55 30  15  I 58 12  30  I I  57 21  22  I 57 21  22I 47 .9  I 46 .0  1 31 .1  I 48 .1  I 44 .9  I 42 ,2  I I  43 .1  I 43 .1. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.-~;; ; ; \ [~ .
.
.
.
.
i~ ;~; - -~- i z ; ; \ [~- - ; ; - i ;~ ; ; - - ;~-~- ; ;~-~;~-~- i - ; ; ; -~ ; - -~-~;~- -~- i~  .
.
.
.Totals I 73  21  5 I 82 13 5 I 84  13  3 i 72  25  3 I 84 14  2 I 78 17 4 I II 26 .6  I i8 .3  I 16 .2  I 28 .3  I 16 .0  \] 21 .6  I I  Legend:.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I I  .
.
.
.
.
.
.
.
.
.
.Foreign 1 843 247 78 11178 206 83 1986 165 37 I 552 193 25 I 748 128 14 11040 224 64 II i ;~ ;F ;~  iSystem I 72  21  7 I 80 14 6 I az 14 3 I 72  25  3 I a4 14 2 I 78 17 5 I I  I *T ~F ~NA ITotals J 27.8 I 19.7 l 17.0 I 28.3 I 16.0 I 21.7 II 1% Un-WelghtedErr l. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Matrix tabulation of results for the Dec 93 ATIS NL Test Results - Using Minimal/Maximal Scoring Criterion, for the Class (A+D)Subset.Matrlx columns present results for Test Data S~absets collected at several sites, and matr ix rows present results for dlfferemtsystems.Numbers pr inted at the top of the matrix columns indicate the number of ?valuable utterances in the Test Data (sub)set from thecorresponding site.
"Overall Totals" (column) present results for the entire Class (A?D) Subset for the system corresponding to that matr ix row.
"Foreign Coil.
Site Totals" present results for "foreign site" data (i.e., excluding locally col lected data) for the Class (A?D)Subset.
"Overall Totals" (row) present results accumulated over all systems corresponding to the Test Data (sub)set corresponding to thatraatrlx column.
"Foreign System Totals" present results acclLm~lated over "foreign systems" (i.e., excluding results for thesystem(s) developed at the site responsible for collection of that Test Data subset.
)** Late and for a debugged system.Tab le  17  AT IS  NL  Resu l ts :  C lass  (A+D)  by  Co l lec t ion  S i te73Class A+D Class A Class D773 Utts  448 Utts.
352 Utts.sys tem UW Err.
UWErr .
UW Err.att l  24.6 22.1 28.0bbnl  17.5 13.8 22.5cmul 13.2 8.9 19.1cr iml  43.3 28.6 63.7cr lm2 28.2 23.7 34.5ml t_ I cs l  14.2 11.8 17,5sr l l  24.8 16.5 36.3sr i2 25.d 18.5 34.8ur i5  ** 20.7 14.1  29.8sr i6 ** 21.2 13.8 31.4%Inlsysl 46.8 33.5 65.2Tab le  18  AT IS  SLS  Test  Resu l tsClass  (A?D) Set  IOr ig i .
t i~  S i te  o f  Tes t  ~ta  II Overa l l  I Fore lgnc~ I K*T \[ ~-sR I  I Nier -mm SRI I I  To~l~ I Co l l .
S i te\[ 146 I 163 \[ 132 I 77 I 89 I 166 I I  773 I Tota l ,.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I + .
.
.
.
.
.
.
.
.
.
.
.
.a t t l  106 40 0 I 138 25 0 100 32  0 58 19 0 72 17 0 I 109 57 0 i 583 190 0 583 190 073 27 0 85 15 0 76  24  0 75 25 0 81 19 0 66 34 0 75 25 0 75  25  0I 27 .4  I lS .3  I =4 .= I =4 .7  I 19 .1  I 34 .3  I I  =4 .6  I 24 .6. .
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.bbn l  I 121 24 1 I 128 38 0 I 117 15 0 I 01 16 0 I 75  14 0 I 136 30 0 I I  630 134 1 J 517 110 0I 03 16 1 I 79 21 0 I 89 11 0 I 79 21 0 I 84 16 o I 82 18 0 I I  83 17 o I 82  lO  oI 17 .1  I =1 .5  I 11 .4  I =0 .8  I lS .7  I 18.1  !!
17 .s  I 17 .5. .
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.~ul  I 127 19 0 I 152 11 0 I 114 18 0 J 64 13 0 76 13 0 138 28 0 I 671 102 0 I 519 91 087 13 0 93 7 0 86 14 0 83 17 0 J 85 15 0 83 17 0 ~1 87 13 0 I 85 15 0I 13 .0  I 6 .7  I 13 .6  I 16 .9  1 14.6 I 16.9 !
13.2 i 14.9. .
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.cr lml I 73 65 8 I 1O0 44 19 82 42 8 38 35 4 66 21 2 I 79 72 15 lJ 438 279 56 ~ 438 279 5650 45 s 61 27 1= I 62 32 6 I 49 45 5 74 24 2 I 48 43 9 I I  57 36 7 I 57 36 7I 5o.0  I 30 .7  I 37 .9  I 50 .6  I 25 .8  I 52 .4  I 43.3  43 .3. .
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.cr im2 I 104 40 2 I 121 40 2 I 99 33 0 ~ 55 20 2 I 69 18 2 I 107 58 I II 555 209 9 l 555 209 9I 71 27 1 I 74 25 1 l 75 25 0 J 71 26 3 I 78 20 2 I 64 35 1 II 72 27 1 I 72 27 1s I 28 .8  I 25 .0  I 25 .0  I 28 .6  I 22.5  I 35 .5  I 20.2  28.2y .
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
~ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.S mi t_ i cs l  l 110 36 0 \[ 148 15 0 I 116 16 O I 63 14 0 I 81 8 0 I 145 21 0 II 663 110 0 I 547 94 0T ~ 75 25 0 I 91 9 0 l 88 12 0 l 82 18 0 I 91 9 0 l 87 13 0 II 86 14 0 j 85 15 0E I 24 .7  I 9 .2  J 12 l l  I 18 .2  I 9 .0  I 12 .7  I I  1?
.2  I 14 .7M .
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I I  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.S sri l  I 94 19 33 \] 140 20 3 1 99 II 22 l 46 16 15 ~ 68 20 1 I 134 28 4 l\] 581 114 78 I 447 86 74l 64 13 23 I 86 12 2 l 75 8 17 I 60 21 19 t 76 22 1 I 81 17 2 II 78 15 10 l 74 14 12I 3s .6  I 14 .1  I 25 .0  I 40 .3  I 23 .6  I 19.3  I I  24 .8  I 26 .4. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I I  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.sri2 I I00 15 31 I 121 27 15 I 103 12 17 I 53 23 1 I 67 20 2 I 133 29 4 II 577 126 70 I 444 97 66I 68 10 a l l  74 17 9 t 78  9 131  69 30 1 I 75 22 21  80 17 2 I I 75 16 9 I 73  16 11I 31 .5  I 25 .8  I 22.0  j 31.2  I 24 .7  t 19.9  25 .4  I 26.9. .
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
Ii .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.sr i5 **  l 105 38 3 l 140 20 3 l 112 19 1 i 54, 21 2 1 68 20 1 ~ 134 28 4 613 146 14 l 479 118 I0I 72 26 2 l 86 12 2 1 85 14 1 1 70 27 3 I 76 22 1 1 81 17 2 79 19 2 l 79 19 228.1  I 14 .1  I 15.2  { 29 .9  I 23 .6  { 19 .3  20 .7  I 21 .1. .
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.sr i6 ** I 111 32 3 I 133 27 3 ~ 112 19 I I 53 23 1 \] 67 20 2 I 133 29 4 609 150 14 ~ 476 121 1OI 76 22 2 I 82 17 2 I 85 14 1 I 69 30 1 I 75 22 2 I 80 17 2 79 19 2 I 78 20 2I 24.0  18 .4  I 15 .2  I 31 .2  I 24 .7  I 19.9  21 .2  I 21.6. .
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
* .
.
.
.
.
.
.
.
.
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
.
.
.
I .
.
.
.
.
.
.
.
.
.
.
.
.
?
.
.
.
.
.
.
.
.
.
.
.
.
.un isys l  I 72 36 38 l 88 31 44 I 84 33 15 I 33 29 15 I 49 29 II I 85 30 51 411 188 174 I 411 188 174l 49 25 26 l 54 19 27 l 64 25 II l 43 38 19 I 55 33 12 ~ 51 18 31 53 24 23 1 53 24 23l 50.7 l 46.0 l 36.4 J 57.1 l 44.9 l 488 46.8 l 46.8===============================================================================================================================Overa l l  11123 364 119 ~1409 295 89 \]1138 250 64 ~ 578 229 40 I 758 200 21 I1333 410 83Tota ls  I 70 23 7 J 79 16 5 J 78 17 4 I 68 27 5 l 77 20 2 I 73 22 5I 3o .1  I 21 .4  1 21 .6  I 31 .8  I 22 .6  I 27 .0  Leg~_~:System l 69 23 8 I 77 17 5 \[ 77 18 5 \[ 68 27 5 I 77 20 2 I 69 25 6 I %T %F %NA \[Tota ls  I 31.4 l 22.9 l 22.6 l 31.8 I 22.6 ~ 31.2 1% Un-WelghtedEr r J. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Mat r ix  tabu la t ion  of resu l ts  for  the Dec  93 AT IS  SLS Test  Resul ts  - Us ing  Min imal /Max imal  Scor ing  Cr i ter ion ,  for the C lass  (A+D)Subset.Mat r ix  co lumns present  resul ts  for Tes t  Data  Subsets  co l lec ted  at several  sites, and  matr ix  rows present  resu l ts  for d i f fe rentsystems.Numbers  pr in ted  at  the top of the matr ix  co lumns  ind icate  the number  of eva luab le  ut terances  in the Test  Data  (sub)set  f rom thecor respond ing  site.
"Overal l  Tota ls"  (column) present  resu l ts  for the ent i re  Class (A?D) Subset  for the system cor respond ing  to that  mat r ix  row.
"Fore ign Col l .
S i te  Tota ls"  p resent  resu l t s  for  " fo re ign  site" data (i.e., exc lud ing  local ly  co l lec ted  data) for the C lass  (A?D)Subset.
"Overal l  Tota ls"  (row) present  resu l ts  accuxnulated over  al l  systems cor respond ing  to the Test  Data  (sub)set  cor respond ing  to thatmatr ix  column.
"Fore ign  System Tota ls"  p resent  resul ts  accumulated  over " fore ign systems" (i.e., exc lud ing  resu l ts  for thesystem(s) deve loped  at the s i te respons ib le  for  co l lec t ion  of thet Test  Data subset.
)** Late and for a debugged system.Tab le  19  AT IS  SLS  Resu l ts :  C lass  (A+D)  by  Co l lec t ion  S i te74
