Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 133?137, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUmelb: Cross-lingual Textual Entailment with Word Alignment and StringSimilarity FeaturesYvette Graham Bahar Salehi Timothy BaldwinDepartment of Computing and Information SystemsThe University of Melbourne{ygraham,bsalehi,tbaldwin}@unimelb.edu.auAbstractThis paper describes The University of Mel-bourne NLP group submission to the Cross-lingual Textual Entailment shared task, ourfirst tentative attempt at the task.
The ap-proach involves using parallel corpora and au-tomatic word alignment to align text fragmentpairs, and statistics based on unaligned wordsas features to classify items as forward andbackward before a compositional combinationinto the final four classes, as well as exper-iments with additional string similarity fea-tures.1 IntroductionCross-lingual Textual Entailment (CLTE) (Negri etal., 2012) proposes the task of automatically iden-tifying the kind of relation that exists between pairsof semantically-related text fragments written in twodistinct languages, a variant of the traditional Rec-ognizing Textual Entailment (RTE) task (Bentivogliet al 2009; Bentivogli et al 2010).
The task tar-gets the cross-lingual content synchronization sce-nario proposed in Mehdad et al(2010, 2011).
Com-positional classification can be used by training twodistinct binary classifiers for forward and backwardentailment classification, before combining labelsinto the four final entailment categories that now in-clude bidirectional and no entailment labels.
Themost similar previous work to this work is the cross-lingual approach of the FBK system (Mehdad etal., 2012) from Semeval 2012 (Negri et al 2012),in which the entailment classification is obtainedwithout translating T1 into T2 for the Spanish?English language pair.
We apply the cross-lingualapproach to German?English and instead of cross-lingual matching features, we use Giza++ (Och etal., 1999) and Moses (Koehn et al 2007) to auto-matically word align text fragment pairs to computestatistics of unaligned words.
In addition, we in-clude some additional experiments using string sim-ilarity features.2 Compositional ClassificationGiven a pair of topically related fragments, T1 (Ger-man) and T2 (English), we automatically annotate itwith one of the following entailment labels: bidi-rectional, forward, backward, no entailment.
Wetake the compositional approach and separately traina forward, as well as a backward binary classifier.Each classifier is run separately on the set of textfragment pairs to produce two binary labels for for-ward and backward entailment.
The two sets of la-bels are logically combined to produce a final clas-sification for each test pair of forward, backward,bidirectional or no entailment.3 Word Alignment FeaturesThe test set of topically-related text fragments, T1(German) and T2 (English) were added to EuroparlGerman?English parallel text (Koehn, 2005) andGiza++ was used for automatic word alignment inboth language directions.
Moses (Koehn et al2007) was then used for symmetrization with thegrow diag final and algorithm.
This produces amany-to-many alignment between the words of the133German, T1, and English, T2, with words also re-maining unaligned.The following features are computed for each testpair feature scores for the forward classifier:?
A1: count of unaligned words in T2?
A2: count of words comprised soley of digitsin T2 not in T1?
A3: count of unaligned words in T2 with lowprobability of appearing unaligned in Europarl(with threshold p=0.11)The number of words in T2 (English) that are notaligned with anything in T1 (German) should pro-vide an indication that, for example, the English textfragment contains information not present in the cor-responding German text fragment and subsequentlyevidence against the presence of forward entailment.We there include the feature, A1, that is simply acount of unaligned words in English T2.
In addi-tion, we hypothesize that the absence of a numberfrom T2 may be a more significant missing elementof T2 from T1.
We therefore include as a featurethe count of tokens comprised of digits in T2 thatare not also present in T1.
The final word align-ment feature attempts to refine A1, by distinguishingwords that are rarely unaligned in German?Englishtranslations.
Statistics are computed for every lexi-cal item from German?English Europarl translationsto produce a lexical unalignment probability, com-puted for each lexical item based on its relative fre-quency in the corpus when it is not aligned to anyother word.The backward classifier uses the same features butcomputed for each test pair on counts of unalignedT1 words.4 ResultsResults for several combinations of features areshown in Table 1 when the system is trained onthe 500-pair development set training corpus andtested on the 500-pair held-out development test set(DEV), in addition to results for feature combina-tions when trained on the entire 1000-pair develop-ment data and tested on the held-out 500-pair goldstandard (TEST) (Negri et al 2011), when the sys-tem is evaluated as two separate binary forward andbackward classifiers (2-CLASS) as well as the finalevaluation including all four entailment classes (4-CLASS).
The highest accuracy is achieved by theclassifier using the single feature of counts of un-aligned words, A1, of 34.6%.
As two separate bi-nary classifiers, the alignment features, A1+A2+A3,achieve a relatively high accuracy of 74.0% for for-ward with somewhat less accurate for backward(65.8%) classification (both over the DEV data).When combined to the final four CLTE classes, how-ever, accuracy drops significantly to an overall accu-racy of 50% (also over DEV).
A main cause is inac-curate labeling of no entailment gold standard testpairs, as the most severe decline is for recall of testpairs for this label (38.4%).Accuracy on the development set for the wordalignment features, A1+A2+A3, compared to thetest set shows a sever decline, from 50% to 32%.
Onthe test data, however, a main cause of inaccuracyis that backward gold standard test pairs, althoughachieving close accuracy to forward when evaluatedas binary classifiers, are inaccurately labeled in the4-class evaluation, as recall for backward drops toonly 18.4% for this label.Another insight revealed for the alignment fea-tures, A1+A2+A3, in the 4-class evaluation is thatwhen run on the development set, the classes for-ward and backward achieve significantly higherf-scores compared to no entailment.
However,the contrary is observed for the test data, asno entailment achieve higher results than both uni-directional classes.
This appears at first to be asomewhat counter-intuitive result, but in this case,the system is simply better at predicting forward andbackward when no entailment exists for a translationpair compared to when a unidirectional entailment ispresent.4.1 String Similarity FeaturesIn addition to the word alignment features, subse-quent to submitting results to the shared task, wehave carried out additional experiments using stringsimilarity features, based on our recent success inapply string similarity to both the estimation of com-positionality of MWEs (Salehi and Cook, to appear)and also the estimation of similarity between short1342-CLASS 4-CLASSAcc.
Prec Recall F1 Acc.
Prec Recall F1DEVA1 + A2 + A3bwrd 65.80 63.12 76.00 68.96 50.00 bwrd 54.80 59.20 56.90fwrd 74.00 72.22 78.00 75.00 fwrd 54.80 45.60 49.80none 50.50 38.40 43.60bidir 42.80 56.80 48.80S1 + S2 + S3bwrd 58.20 57.75 61.20 59.42 27.40 bwrd 14.30 0.80 1.50fwrd 47.00 47.17 50.00 59.42 fwrd 0.00 0.00 0.00none 30.70 39.70 39.70bidir 25.60 52.80 34.50TESTA1bwrd 57.00 58.54 48.00 52.75 34.60 bwrd 25.50 19.20 21.90fwrd 58.40 58.75 56.40 57.55 fwrd 34.90 36.00 35.40none 36.70 48.80 41.90bidir 38.70 34.40 36.40A2bwrd 50.00 0.00 0.00 0.00 33.60 bwrd 24.70 18.40 21.10fwrd 51.60 50.85 95.20 66.29 fwrd 34.70 34.40 34.50none 36.90 38.40 37.60bidir 35.30 43.20 38.80A3bwrd 54.80 55.61 47.60 51.29 34.20 bwrd 32.70 26.40 29.20fwrd 61.20 61.57 59.60 60.57 fwrd 33.30 34.40 33.90none 36.90 46.40 41.10bidir 32.70 29.60 31.10A1+A2bwrd 57.60 57.72 56.80 57.26 33.60 bwrd 24.70 18.40 21.10fwrd 59.80 58.84 65.20 61.86 fwrd 34.70 34.40 34.50none 36.90 38.40 37.60bidir 35.30 43.20 38.80A1+A3bwrd 57.20 57.96 52.40 55.04 33.00 bwrd 26.60 20.00 22.80fwrd 58.60 58.05 62.00 59.96 fwrd 31.90 34.40 33.10none 36.70 40.80 38.60bidir 34.80 36.80 35.80A2+A3bwrd 54.80 55.83 46.00 50.44 33.40 bwrd 32.30 25.60 28.60fwrd 61.00 61.70 58.00 59.79 fwrd 32.80 33.60 33.20none 34.90 46.40 39.90bidir 32.70 28.00 30.20A1 + A2 + A3bwrd 57.60 57.72 56.80 57.26 32.00 bwrd 24.00 18.40 20.80fwrd 59.20 58.39 64.00 61.07 fwrd 32.30 32.00 32.10none 36.20 37.60 36.90bidir 34.70 41.60 37.80S1 + S2 + S3bwrd 53.20 53.77 45.60 49.35 26.00 bwrd 20.00 1.50 29.50fwrd 48.60 48.36 41.20 44.49 fwrd 16.70 0.80 31.50none 28.00 63.20 38.80bidir 23.70 39.20 29.50A1 + A2 + A3 + S1bwrd 57.40 58.30 52.00 54.97 33.00 bwrd 27.60 19.20 22.60fwrd 59.80 58.84 65.20 61.86 fwrd 29.80 33.60 31.60none 38.20 41.60 39.80bidir 34.60 37.60 36.00A1 + A2 + A3 + S2bwrd 57.80 58.52 53.60 55.95 32.60 bwrd 26.70 19.20 22.30fwrd 59.60 58.70 64.80 61.60 fwrd 30.70 33.60 32.10none 37.30 40.00 38.60bidir 33.80 37.60 35.60A1 + A2 + A3 +S3bwrd 58.20 58.51 56.40 57.44 32.80 bwrd 24.70 19.20 21.60fwrd 59.60 58.82 64.00 61.30 fwrd 32.00 32.80 32.40none 37.40 39.20 38.30bidir 34.70 40.00 37.20Table 1: Cross-lingual Textual Entailment Results for Word alignment Features and String Similarity Measures, A1= count of unaligned words in T2, A2 = count of unaligned numbers in T2, A3 = count of unaligned words in T2with unaligned probability < 0.11, S1 = Number of matched words in the aligned sequence given by Smith-Watermanalgorithm, S2 = Penalty of aligning sentences using Smith-Waterman algorithm, S3 = Levenshtein distance betweenthe sentences135texts in the *SEM 2013 Shared Task (Gella et alto appear).
Using the alignments, we replace eachEnglish word with its corresponding word in Ger-man.
The resulting German sentence is comparedwith the actual one using string similarity measures.As the structure of both English and German sen-tences are usually SVO, we hypothesize that whenthere is no entailment between the two given sen-tences, the newly-made German sentence and theoriginal German sentence will differ a lot in wordorder.In order to compare the two German sentences,we use the Levenshtein (Levenshtein, 1966) and theSmith-Waterman (Smith and Waterman, 1981) al-gorithm.
The Levenshtein algorithm measures thenumber of world-level edits to change one sentenceinto another.
The edit operators consist of insertionand deletion.
We consider substitution as two edits(combination of insertion and deletion) based on thefindings of Baldwin (2009).We also use Smith-Waterman (SW) algorithm,which was originally developed to find the most sim-ilar region between two proteins.
The algorithmlooks for the longest common substring, except thatit permits small numbers of penalized editions con-sisting of insertion, deletion and substitution.
Wecall the best found substring the ?SW aligned se-quence?.
In this experiment, we consider the numberof matched words and the number of penalties in theSW aligned sequence as features.Results for the string similarity features are shownin Table 1.
Since the string similarity feature scoresdo not take the entailment direction into account,i.e.
there is a single set of feature scores for eachtext fragment pair as there is no distinction betweenforward and backward entailment, and they are notsuited for standalone use in compositional classifica-tion.
We do, however, include these scores in Table1 to illustrate how with the compositional approachusing the same set of features for forward and back-ward ultimately results in a classification of test pairsas either bidirectional or no entailment.When individual string similarity features areadded to the word alignment features, minor gains inaccuracy are achieved over the word alignment fea-tures alone, +1% for S1, +0.6% for S2 and +0.8%for S3 (= Levenstein).5 Possible Additions: Dictionary FeaturesWe hypothesize that when there is no entailment be-tween the two sentences, the aligner may not accu-rately align words.
An on-line dictionary contain-ing lemmatized words, such as Panlex (Baldwin andColowick, 2010), could be used to avoid errors insuch cases.
Dictionary-based feature scores basedon the presence or absence of alignments in the dic-tionary could then be applied.6 ConclusionsThis paper describes a compositional cross-lingualapproach to CLTE with experiments carried outfor the German-English language pair.
Our resultsshowed that in the first stages of binary classificationas forward and backward, the word alignment fea-tures alone achieved good accuracy but when com-bined suffer severely.
Accuracy of the approachusing word alignment features could benefit froma more directional multi-class classification as op-posed to the compositional approach we used.
Inaddition, results showed minor increases in accuracycan be achieved using string similarity measures.AcknowledgmentsThis work was supported by the Australian ResearchCouncil.ReferencesTimothy Baldwin and Jonathan Pool Susan M. Colowick.2010.
Panlex and lextract: Translating all words of alllanguages of the world.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics:Demonstrations, pages 37?40.Timothy Baldwin.
2009.
The hare and the tortoise:Speed and reliability in translation retrieval.
MachineTranslation, 23(4):195?240.L.
Bentivogli, I. Dagan, H. T. Dang, D. Giampiccolo, andB.
Magnini.
2009.
The fifth PASCAL recognizingtextual entailment challenge.
In TAC 2009 WorkshopProceedings, Gaithersburg, MD.L.
Bentivogli, P. Clark, I. Dagan, H. T. Dang, and D. Gi-ampiccolo.
2010.
The sixth PASCAL recognizingtextual entailment challenge.
In TAC 2010 WorkshopProceedings, Gaithersburg, MD.Spandana Gella, Bahar Salehi, Marco Lui, Karl Grieser,Paul Cook, and Timothy Baldwin.
to appear.
Integrat-ing predictions from multiple domains and feature sets136for estimating semantic textual similarity.
In Proceed-ings of *SEM 2013 Shared Task STS.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan HerbstHieu Hoang.
2007.
Moses:Open Source Toolkit for Statistical Machine Transla-tion.
In Annual Meeting of the Association for Com-putational Linguistics (ACL), demonstration session,Prague, Czech Republic, June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of the10th Machine Translation Summit, Phuket, Thailand.Vladimir I Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions and reversals.
In Sovietphysics doklady, volume 10, page 707.Y.
Mehdad, M. Negri, and M. Federico.
2010.
Towardscross-lingual textual entailment.
In Proceedings ofNAACL-HLT.Y.
Mehdad, M. Negri, and M. Federico.
2011.
Using par-allel corpora for cross-lingual textual entailment.
InProceedings of ACL-HLT 2011.Yashar Mehdad, Matteo Negri, and Jose G. C. de Souza.2012.
Fbk: Cross-lingual textual entailment with-outtranslation.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval2012).M.
Negri, L. Bentivogli, Y. Mehdad, D. Giampiccolo, andA.
Marchetti.
2011.
Divide and conquer: Crowd-sourcing the creation of cross-lingual textual entail-ment corpora.
In Proceedings of EMNLP 2011.Matteo Negri, Alessandro Marchetti, Yashar Mehdad,Luisa Bentivogli, and Danilo Giampiccolo.
2012.Semeval-2012 task 8: Cross-lingual textual entailmentfor content synchronization.
In First Joint Conferenceon Lexical and Computational Semantics, pages 399?407, Montreal, Canada.Franz Josef Och, Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Proceedings of the 1999 JointSIGDAT Conference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora, pages20?28, College Park, MD.Bahar Salehi and Paul Cook.
to appear.
Predictingthe compositionality of multiword expressions usingtranslations in multiple languages.
In Proceedings ofthe Second Joint Conference on Lexical and Computa-tional Semantics (*SEM 2013).Temple F Smith and Michael S Waterman.
1981.
Theidentification of common molecular subsequences.Journal of Molecular Biology, 147:195?197.137
