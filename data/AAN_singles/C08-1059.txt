Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 465?472Manchester, August 2008Stopping Criteria for Active Learning of Named Entity RecognitionFlorian LawsInstitute for NLPUniversit?at Stuttgartfl@ifnlp.orgHinrich Sch?utzeInstitute for NLPUniversit?at Stuttgarths999@ifnlp.orgAbstractActive learning is a proven method for re-ducing the cost of creating the training setsthat are necessary for statistical NLP.
How-ever, there has been little work on stoppingcriteria for active learning.
An operationalstopping criterion is necessary to be ableto use active learning in NLP applications.We investigate three different stopping cri-teria for active learning of named entityrecognition (NER) and show that one ofthem, gradient-based stopping, (i) reliablystops active learning, (ii) achieves near-optimal NER performance, (iii) and needsonly about 20% as much training data asexhaustive labeling.1 IntroductionSupervised statistical learning methods are impor-tant and widely successful tools for natural lan-guage processing.
These methods learn by esti-mating a statistical model on labeled training data.Often, these models require a large amount oftraining data that needs to be hand-annotated byhuman experts.
This is time-consuming and ex-pensive.
Active learning (AL) reduces this annota-tion effort by selecting unlabeled examples that aremaximally informative for the statistical learningmethod and handing them to a human annotatorfor labeling.
The statistical model is then updatedwith the newly gathered information.
In this pa-per, we adopt the uncertainty sampling approachto AL (Lewis and Gale, 1994).
Uncertainty sam-pling selects those examples in the pool as most in-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.formative for which the statistical classifier is leastcertain in its classification decision.While AL is an active area of research in NLP,the issue of determining when to stop the ALprocess has only recently come into focus (Zhuand Hovy, 2007; Vlachos, 2008).
This is some-what surprising because the main purpose of ac-tive learning is to save on annotation effort; decid-ing on the point when enough data is annotated iscrucial to fulfilling this goal.We investigate three different stopping criteriain this paper.
First, a user of a classification systemmay want to set a minimum absolute performancefor the system to be deployed.
The standard way ofassessing classifier performance uses a held-out la-beled test set.
However, labeling a test set of suffi-cient size is contrary to the goal of minimizing an-notation effort and impractical in most real-worldsettings.
We will show that the classifier can esti-mate its own performance using only an unlabeledreference set and propose to stop active learningif estimated performance reaches the threshold setby the user.
The estimation is somewhat inaccu-rate, however, and we investigate possible reasonsfor estimation error.An alternative criterion is based on maximumpossible performance.
We will show that our per-formance estimation method supports stopping ALat a point where performance is almost optimal.The third and last criterion is convergence.
Thebasic idea here is to stop active learning when moreexamples from the pool do not contribute moreinformation, indicated either by the fact that theclassifier has reached maximum performance or bythe fact that the ?uncertainty?
of the classifier can-not be decreased further.
We determine the pointwhere the pool has become uninformative by com-puting the gradient of either performance or uncer-465tainty.This paper is organized as follows.
Section 2shows that three uncertainty measures achievenear-optimal performance for NER at a fractionof the labeling cost of exhaustive labeling of thetraining set.
In Section 3, we introduce a newmethod for estimating the performance of an ac-tively learned classifier in support of stopping ac-tive learning when a certain level of performancehas been reached.
Section 4 shows that the stop-ping criterion of reaching peak confidence is notapplicable to NER with multiclass logistic regres-sion.
Section 5 presents stopping criteria basedon convergence.
Sections 6 and 7 discuss relatedwork and present our conclusions.2 Selection FunctionsFor measuring the uncertainty of a classificationdecision in uncertainty sampling there exist diversemeasures appropriate for different basic classifiers(e.g.
margin-based measures for SVMs, and mea-sures based on class probability for classification).Choosing such an uncertainty measure is relativelystraightforward for a binary classification problem,but for multiclass problems we need different mea-sures, and it is not obvious which will performbest.Following Schein (2005), but in the context ofNER, we compare several measures of uncertaintyfor multiclass logistic regression.
For a given mea-sureMi,X, we select in each iteration the unlabeledexample(s) in the pool that have the smallest valuefor Mi,X(corresponding to the maximum uncer-tainty).1-Entropy.Mi,1-Entropy= 1 ?H(p?
(.|xi))= 1 +?jp?
(cj|xi) log p?
(cj|xi)where p?
(cj|xi) is the current estimate of the proba-bility of class cjgiven the example xi.11-Entropyfavors examples where the classifier assigns simi-lar probabilities to all classes.Margin.
If c and c?are the two most likelyclasses, the margin is defined as follows:Mi,Margin= |p?
(c|xi) ?
p?
(c?|xi)|Margin picks examples where the distinction be-tween two likely classes is hard.1We use 1-Entropy instead of entropy, so all three mea-sures will have lower values for less certain instances.MinMax.Mi,MinMax= maxj(p?
(cj|xi))The rationale here is that a low probability of theselected class indicates uncertainty.
We proposeMinMax as a measure that is more directly basedon the classifier?s decision for a particular exam-ple.
The other two measures also take into accountthe classifier?s assessment of classes that were notchosen for the unlabeled example.2.1 ExperimentsWe used the newswire section of the ACE 2005Multilingual Training Corpus (128 documents,66,015 tokens) for our experiments.
A subset ofthe documents was randomly sampled into an eval-uation set that consists of 6301 tokens.
We used30.000 of the remaining tokens as the uncertaintysampling pool.
The rest was left aside for futureexperiments.
We use the BBR package (Genkin etal., 2007) for binary logistic regression as our baseclassifier, with default values for all of BBR?s pa-rameters.
As our main focus is on AL, we onlyuse basic features like capitalization, puctuation aswell as word identity, prefixes and suffixes, eachfor the classified word itself and for left and rightcontexts.We train separate classifiers for each named en-tity (NE) class and another one for the class ?not anNE?
(0).
For each token we normalize the outputprobability of the individual classifiers so they sumto 1 and then select for each token the class withthe highest probability.
Evaluation is performed bycomparing individual tokens to the gold standard.2Using all labeled training data as our fully super-vised baseline results in a performance of 78.7%F1(henceforth: F ) and 96.6% accuracy.
This iscomparable to the accuracy of 96.29% reportedby (Daume III, 2007) on the newswire domain.Daum?e?s work is the only study known to us thatuses the ACE dataset, but not the proprietary ACEvalue score.
In the rest of this paper, we report Fscores, because we believe that F is a more infor-mative measure for NER than accuracy.We use AL based on uncertainty sampling.
Westart with a seed set of ten consecutive tokensrandomly selected from the training pool and la-bel it.
In each round of AL we select the tentokens with the smallest value of Mi,X(where2Chunk-based NER results are not directly comparablewith this token-based evaluation.466Selection Baseline Peak perf.1-Entropy 78.7 2139 (7.1%) 80.8 3460 (11.5%)MinMax 78.7 2108 (7.0%) 80.8 3650 (12.1%)Margin 78.7 2019 (6.7%) 81.2 3694 (12.3%)Table 1: Percentage of data needed by AL to reachbaseline or peak performance.X ?
{1-Entropy,Margin,MinMax}) from the re-maining pool, including tokens with the label 0.We then label these tokens and add them to the la-beled training set.
The classifiers are retrained withthe new training set and the AL loop repeats.
Weperformed 20 runs of the experiments, each withthe same sampling pool, but a different seed set,randomly selected as described above.Table 1 shows that AL is quite successful forNER.
Only 7% of the training data is needed toachieve the same performance as the supervisedbaseline.Furthermore we find that after the baseline per-formance is reached the increase in performancequickly levels off to a point where using moretraining data does not yield performance improve-ments anymore.
In fact, our experiments showthat there is a peak in performance reached atabout 12% of the training data and performancedecreases again after this point (see Figure 1).The peak is more prominent if the pool is large.On a pool of 30,000 tokens, peak performance isabout 2.5% F -Score better than the baseline; on a6000 token pool, the difference is only about 1.7%.Therefore, once the peak is reached, the AL pro-cess should stop, even if the annotation budget isnot yet used up.0 2000 4000 6000 8000 100000.600.650.700.750.80Training examplesF?ScoreMargin1?EntropyMinMaxFigure 1: Performance as a function of number oflabeled training examples usedComparing the different selection functions, wefound little difference between their performance.Margin performs significantly better (Student?s t-test, ?
= 0.05), but the difference is small (< 1%F -Score).
If we compare two AL processes (sayMargin and 1-Entropy) that were started with thesame pool and seed set and stop both processeswhen they each reach their respective peak per-formances, Margin has a better peak performanceof 0.3% F -Score on average (significant at ?
=0.05).The differences between 1-Entropy andMinMaxare not statistically significant, except for a shortstart-up phase (see Figure 1).3 Performance EstimationIn practical applications, classifiers can only be re-liably deployed when they attain a predefined min-imum absolute performance level.
Thus, we wouldlike to determine if this level has been reached andthen stop the annotation process.
However, this isnot a simple task, because in these settings thereis no labeled test set available to evaluate perfor-mance.
Creating this test set would mean a sub-stantial annotation effort, which is what we wantto avoid by using AL in the first place.
Therefore,we will try to estimate the classifier?s performanceon unlabeled data.Following Lewis (1995), we estimate the F -Score based on the current estimates of the classprobabilities.
Based on the F measure?s definitionas the harmonic mean of precision (P) and recall(R), we can write F as a function of true positives(TP), false positives (TP) and false negatives (FN):F =2 ?
P ?RP +R=2TP2TP + FP + FNSimilar to Lewis, we estimate?TP,?FP,?FN, but weneed to extend their work from binary classifica-tion to 1-vs-all multiclass classification:?TP =n?iE?jp?
(cj|xi)di,j(1)?FP =n?iE?j(1 ?
p?
(cj|xi))di,j(2)?FN =n?iE?jp?
(cj|xi)(1 ?
di,j) (3)where n is the number of examples, E is the num-ber of named entity classes, excluding the ?not467an NE?
class.
p?
(cj|xi) is the estimated probabil-ity that example xihas class cj.
The flag di,jindicates ?is winning class?
: di,j= 1 if j =argmaxjp?
(cj|xi) and di,j= 0 else.Like standard NER evaluation schemes, e.g.
(Tjong Kim Sang and De Meulder, 2003), we con-sider only those decisions to be TPs where (i) thereference class matches the selected class and (ii)this class is not ?not an NE?.
When estimatingTP, we assume that the probability of a matchequals the probability of the selected class (whichis p?
(cj|xi) ?
di,j).
The probability of making anFP error is just the remaining probability mass.For FN, we can calculate the estimated probabil-ity by summing up the class probabilities of thenon-selected named entity classes.3.1 Evaluation of Performance EstimationTo evaluate the performance estimation method,we ran it on an unlabeled reference set.
The ref-erence set is a set of unlabeled data distinct fromthe sampling pool.
In our experiments, we use thetokens in the test set from 2.1, but with the labelsstripped off.We compare the true performance on the test set(reported as ?True?
in Table 2) with the estimate(reported as ?Lewis?).
The ?
columns report thedifference of the named method to ?True?.
We alsotested leave-one-out (LOO) estimation of F , P andR using the data of the selected training set.True Lewis ?
Lewis LOO ?
LOOF 79 92 +13 85 +6P 81 92 +11 86 +5R 77 92 +15 84 +7Table 2: Performance estimation.
LOO and Lewisoverestimate true F by 6% and 13%, respectively.We find that both methods overestimate preci-sion and recall by a large margin.
We also notethat the peak in performance at about 4200 train-ing examples that we found when evaluating onheld-out data (see Figure 1) does not occur whenevaluating performance using the Lewis method.Instead, the estimate of F grows monotonically.This means that we cannot use a peak of estimatedF as a criterion for stopping.
When setting an ab-solute threshold of F = 80% for stopping, activelearning stops at about 1000 iterations, yielding atrue performance of only F = 73% (selection byMargin, 20 trials).
This indicates that we cannotdirectly use Lewis estimates for stopping.3.2 Error AnalysisThe reason for the overestimation is that the logis-tic regression classifier is too confident in its owndecision.
For positive decisions, the class proba-bility very often is close to 1, for negative deci-sions, it is close to 0.
As a result, the estimatorgives very little score for FN (Equation 3) or FP(Equation 2) in most instances, which leads to thehigh overestimation of performance.To verify this, we grouped the empirical prob-ability of a selected class being the correct classin bins according to the estimated probability ofthe logistic classifier.
Table 3 shows this empiri-cal probability given a class and its estimate.
Thetable is split into two halves, such that the empir-ical probabilities for positive decisions (the classgot chosen as the best class) and negative deci-sions are shown separately.
The top value in eachcell (?emp?)
shows the empirical probability as op-posed to the estimated probability, which is thevalue below (?est?).
The product of the differ-ence of these two probabilities and the number ofinstances that were counted into this bin (?cnt?
),gives an estimate of how much the probability es-timates in the bin contribute to the error (absolutevalue) of the performance estimation.The table shows that class probabilities are infact estimated too optimistically.
For many of theentries in the positives table, the estimated prob-abilities are greater than the empirical probabili-ties.
In the negatives table, the estimated proba-bilities are smaller.
In both cases, the estimatesare closer to the respective extreme values 1 or 0,which means they are overconfident.
Note that forpositive decisions, the estimation error of the val-ues in a single bin contributes to the overall estima-tion error in two ways: overestimating TPs and un-derestimating FPs.
For example, the estimation er-ror for the cell in bold is 29.2, contributing ?29.2for FP (underestimation) and +29.2 for TP (over-estimation).
Also note that due to the high num-ber of non-NE tokens in the text, there is a largenumber of negative decisions for each entity-classclassifier; thus, small differences in the probabili-ties make large contributions to error.We ran a separate experiment in which wetrained a classifier on the entire labeled pool.
TheLewis estimator overestimated F by 12% in thiscase.
This indicates that the estimation error doesnot primarily come from the biased selection oftraining examples inherent in the selective sam-468negative decisions positive decisions0-.2 .2-.4 .4-.6 .2-.4 .4-.6 .6-.8 .8-1O emp 0.0643 0.269 0.25 0.0 0.25 0.233 0.991est 0.00825 0.295 0.438 0.394 0.537 0.714 0.999cnt 607 26 12 1 16 30 5609err 34 -0.67 -2.25 0.394(tn) 4.6 (tn) 14.4 (tn) 45.4 (tn)GPE emp 0.00384 0.391 0.5 0.0 0.333 0.571 0.875est 0.000812 0.296 0.435 0.357 0.535 0.687 0.989cnt 5985 23 6 1 9 21 256err 18.1 (fn) 2.19 (fn) 0.388 (fn) 0.357 (fp) 1.82 (fp) 2.42 (fp) 29.2 (fp)ORG emp 0.00853 0.393 0.667 0.5 0.615 0.828est 0.000847 0.283 0.441 0.545 0.71 0.968cnt 6093 28 12 14 26 128err 46.8 (fn) 3.06 (fn) 2.7 (fn) 0.631 (fp) 2.46 (fp) 17.9 (fp)PER emp 0.0041 0.455 0.5 0.273 0.5 0.93est 0.000748 0.283 0.48 0.563 0.718 0.98cnt 6102 22 6 11 18 142err 20.4 (fn) 3.78 (fn) 0.121 (fn) 3.2 (fp) 3.93 (fp) 7.19 (fp)Table 3: Empirical probabilities and contribution to estimation errors.
(We omit small classes and emptycolumns.)
Example (cell in bold): 256 tokens were estimated to be a GPE by the classifier with estimatedprobabilities between 0.8 and 1.0.
The average estimate was 0.989.
In reality, only 224 of these tokens(87.5%) were GPEs.
The contribution of this cell to the overall FP count is (0.989?0.875) ?256 ?
29.2.pling method, but from bias inherent in either thewhole pool of training data or the base classifier.3.3 Towards a Better EstimateOver-optimistic estimates for precision and recallstem from the classifier?s over-optimistic probabil-ity estimates.
We try to correct the estimates byreplacing the predicted class probabilities with theappropriate value in an empirical probability tablelike the one shown in Table 3.
However, sincein practice we do not have labels for the test set,we cannot compute the empirical probabilities di-rectly.
Instead, we use leave-one-out estimation tobootstrap the adjustment table from the selectedtraining data.
The adjusted estimation shows amarked increase in the estimates for FP and FN,leading to a quite accurate estimate for precision(+5 absolute error), but the now pessimistic esti-mate for recall (?16) leads to underestimation ofF -Score overall (?8) (see Table 4).True Lewis adj.
Lewis ?
adj.
LewisF 78 91 70 -8P 81 93 86 +5R 76 89 60 -16TP 520 596 555 +35FP 125 48 90 -35FN 163 70 379 +216Table 4: Lewis estimation with adjusted probabili-tiesAs we see, the adjustment overshoots for recall,indicating that the new estimated probabilities arestill off.
There could be several reasons for this.The first reason is that the bin width is quite coarse,as there are only five bins for the entire probabilityinterval, each bin covering a range of 0.2.
How-ever, using finer bin widths can lead to data spar-sity problems.Another reason might be the estimation errorswithin individual bins that compound to a quitelarge overall error especially in the negative case.Finally, differences in the distributions of trainingset and reference set could cause unreliable esti-mates.
The empirical probabilities for the adjust-ment table are estimated with leave-one-out on thetraining set.
However, since the training set is cre-ated by selective sampling, it will be biased.4 Confidence-based StoppingWe have found that performance estimation is notyet reliable enough to stop when a desired perfor-mance level is reached.
However, since there isa maximum performance that can be reached onany given sampling pool, the annotation processstill should stop at this point regardless of whethera target performance level has been reached ornot.
We therefore seek a stopping criterion thatfinds the maximum possible performance when theclassifier is iteratively trained on a given samplingpool.
Again, in practice we do not have a labeledtest set to evaluate against, so we have to try to findthe stopping point from either the remaining pool,or the separate unlabeled reference set.Vlachos (2008) proposes to calculate the confi-dence of the classifier by using the average uncer-469tainty on the unlabeled reference set.
For multi-class problems, he uses SVM classifiers with theSVM margin size as the uncertainty measure.
Us-ing this measure, Vlachos reports finding, albeitdistorted by fluctuations, a peak pattern in this con-fidence measure that coincides with reaching max-imal performance in his experiments.
He then sug-gests to use this peak confidence as the stoppingcriterion.However, in our experiments with multiclasslogistic regression, we could not find this peakpattern when calculating the confidence using thethree uncertainty measures introduced above: 1-Entropy, Margin and MinMax.0 2000 4000 6000 8000 100000.800.850.900.951.00IterationsConfidenceonunlabeledreferencedata1?EntropyMinmaxMarginFigure 2: Confidence on unlabeled reference set(selection: 1-Entropy).
The vertical lines indi-cate when baseline and optimal performance arereached.
There is no peak pattern in the curves,so reaching peak confidence cannot be used as astopping criterion.In Figure 2, we show the three measures, av-eraged over 20 trials as described in section 2.1.Due to instability of AL during start-up, there aresome fluctuations in the first 100 iterations.
Af-ter 500 iterations the confidence curves stabilizeand at about 4000 iterations approach asymptotes,without exhibiting peak patterns.
Thus, the pro-posed criterion of peak confidence based on aver-age reference uncertainty does not seem applicablefor controlling AL with multiclass logistic regres-sion.5 Gradient-based StoppingSince we cannot use peaks for stopping, we pro-pose to stop when a base measurement character-0 2000 4000 6000 8000 100000.00.20.40.60.81.0IterationsMarginFigure 3: Margin uncertainty of selected instance(single run).
The graph demonstrates that withoutsmoothing this criterion is too noisy.izing the progress of active learning has converged.We identify the point of convergence by computinggradients.
We find that the rise of the performanceestimation slows to an almost horizontal slope atabout the time when the true performance reachesits peak.
We therefore propose the following newstopping criterion: Estimate the gradient of thecurve and stop when it approaches 0.
Since wedo not need an accurate estimation of absolute per-formance here, we can use the unadjusted Lewisestimate for this method.
We call this stopping cri-terion (estimated) performance convergence.In a similar way, we can use the gradient of theuncertainty of the last selected instance.
The in-stance that was selected last is always the one withmaximum uncertainty, and thus the most informa-tive for training.
When the uncertainty measurecomes close to the extreme value of 1, we decidethat there are no informative examples left in thepool and we stop the AL process.
(Unfortunately,1 is minimum uncertainty and 0 is maximum un-certainty according to our definitions of the threemeasures.)
The gradient of the uncertainty mea-sure approaches 0 at this point (see Figure 3), sowe can again use a gradient criterion for imple-menting this idea.
We call this stopping criterionuncertainty convergence.In Figure 3, which shows a graph of the Mar-gin uncertainty of the selected instance, we canalso see that it is quite noisy.
The value dropssharply when some examples are encountered butquickly returns to the previous level after a few it-erations.
The performance estimation measure is470slightly noisy as well, so we need a robust way ofcomputing the gradient.
We achieve this with amoving median approach.
At each step, we com-pute the median of w2= {an?k, .
.
.
, an} (the lastn values) and of w1= {an?k?1, .
.
.
, an?1} (theprevious last n values).
Each value aiis the per-formance at iteration i (for the performance gradi-ent) or the uncertainty of the instance selected initeration i (for the uncertainty gradient).We then estimate the gradient using the mediansof the two windows:g = (median(w2) ?
median(w1))/1 (4)For the performance estimate, which is less noisy,we can also use the arithmetic mean instead of themedian.
In this case, we simply replace ?median?with ?mean?
in Equation 4.We found that a window of size k = 100 yieldsgood results in mitigating the noise while still re-acting fast enough to the changes in the gradient.We combine this criterion with a maximum crite-rion and only stop if the last value anis a newmax-imum.
We stop the AL process when (i) the currentcertainty or estimated performance is a new max-imum and (ii) the newly calculated gradient g ispositive and (iii) g falls below a predefined level .5.1 EvaluationWe show the results of gradient stopping appliedto each of the three uncertainty measures and theLewis estimate.
For comparison, we also includeresults with a threshold-based criterion, where ALstops when the uncertainty measure of the selectedinstance reaches a threshold of 1?.
This is similarto (Zhu and Hovy, 2007), but extended by us to allthree uncertainty measures.Table 5 shows results for each criterion.
The?Stop?
value indicates number of tokens at whichthe stopping criterion stopped AL.
??Bl?
indicatesthe difference between baseline performance andperformance at the stopping point, ??Pk?
the dif-ference to peak performance.
The ?sd?
columnsshow the respective standard deviations.We find that all stopping criteria stop before20% of the pool is used, providing a large reduc-tion in annotation effort.
While the point of peakperformance can not be precisely found by the cri-teria, all criteria reliably stop at a performancelevel that surpasses the fully supervised baseline.The threshold criteria seem to be a bit better infinding a stopping point closer to optimal perfor-mance.
Not unsurprisingly, the stopping functionthat matches the selection function performs best.The gradient methods, however, seem to be provid-ing better-than-baseline performance more consis-tently (less variation) and might require less tuningof the threshold parameter when other factors (e.g.,the batch size) change.
If lower noise allows it, asfor the Lewis estimate, moving averages should beused in place of moving medians.6 Related WorkSch?utze et al (2006) studied a Lewis-based per-formance estimation method in a binary text clas-sification setting.
They attribute difficulties in esti-mating recall to a ?missed cluster effect?, meaningthat the active sampling procedure is failing to se-lect some clusters of relevant training examples inthe pool that are too dissimilar to the relevant ex-amples already known.
Diversity measures as pro-posed by (Shen et al, 2004) might help in mitigat-ing this effect, but our experiments show that thereare fundamental differences between text classifi-cation and NER.
Since missed clusters of relevantexamples in the training data would eventually beused as we exhaustively label the entire pool, weshould see improvements in recall when the missedclusters get used.
Instead, we observed in section2.1, that there are no further performance gains af-ter a certain portion of the pool is labeled.
Thus, allexamples that the classifier can make use of musthave been taken into account, and there appear tobe no missed clusters.Tomanek et al (2007) present a stopping cri-terion for query-by-committee-based AL that isbased on the rate of disagreement of the classifiersin the committee.
While our uncertainty conver-gence criterion can only be applied to uncertaintysampling, the performance convergence criterioncan be used in a committee-based setting.Li and Sethi (2006) estimate the conditional er-ror as a measure of uncertainty in selection (insteadof using it for stopping as we do), using a variable-bin histogram for improving the error estimates.They do not evaluate the quality of the probabil-ity estimates.
As with our stopping criterion, weexpect this selection criterion to be the more ef-fective the more accurate the probability estimatesare.
We therefore believe that our method of im-proving probability estimates based on LOO binscould improve their selection criterion.471Stop crit.
 Peak Stop ?
Bl sd ?
Pk sd1-Entropy threshold 0.01 80.8 3645 12.0% 1.44 0.7 ?0.68 0.4MinMax threshold 0.01 80.8 3133 10.3% 0.11 1.0 ?2.0 0.8Margin threshold 0.01 80.8 3158 10.4% 1.1 0.8 ?1.0 0.81-Entropy gradient 0.00005 80.8 4572 15.0% 0.97 0.4 ?1.1 0.5MinMax gradient 0.00005 80.8 4397 14.5% 1.02 0.4 ?1.1 0.5Margin gradient 0.00005 80.8 5292 17.5% 0.81 0.3 ?1.32 0.4Lewis grd.
(Median) 0.00005 80.8 2791 9.2% 0.8 1.4 ?1.3 1.4Lewis grd.
(Mean) 0.00005 80.8 3999 13.1% 1.1 0.8 ?0.95 0.6Table 5: Performance at stopping points (baseline perf.
78.7, Selection: 1-Entropy)7 Conclusion and Future WorkIn this paper, we presented several criteria to stopthe AL process.
For stopping the training at auser-defined performance level, we proposed amethod for estimating classifier performance in amulticlass classification setting.
While we couldachieve acceptable accuracy in estimation of pre-cision, we find that recall estimation is hard.
Esti-mation is not accurate enough to assist in makinga reliable decision if the performance of the classi-fier is acceptable for practical use.
In the future, weplan to improve on performance estimation quality,e.g., by using the variable-bin approach suggestedby Li and Sethi (2006).Nevertheless, we showed that the gradient of theperformance estimate can successfully be used asa stopping criterion relative to the optimal perfor-mance that is attainable on a given pool.
We alsodescribe stopping criteria based on the gradient ofthe uncertainty measure of the instances selectedfor training.
The criteria reliably determine stop-ping points that result in a performance that is bet-ter than the supervised baseline and close to theoptimal performance.
We believe that these crite-ria can be applied to any AL setting based on un-certainty sampling, not just NER.If it turns out that the maximum possible per-formance does not meet a user?s expectations, theuser needs to acquire fresh data and refill the pool.This might lead to an approach to reduce the com-putational cost of AL we want to evaluate in fu-ture work: Subdivide a large sampling pool intosmaller sub-pools, run AL sequentially on the sub-pools.
When the stopping criterion is reached,switch to the next sub-pool.We also found that uncertainty curves of the se-lected examples are quite noisy.
We would like toinvestigate which properties of the training exam-ples cause these drops in the uncertainty curve.ReferencesDaume III, Hal.
2007.
Frustratingly easy domain adap-tation.
In ACL-07, pages 256?263.Genkin, A., D.D.
Lewis, and D. Madigan.
2007.Large-scale bayesian logistic regression for text cat-egorization.
Technometrics, 49(3):291?304.Lewis, D.D.
and W.A.
Gale.
1994.
A sequential algo-rithm for training text classifiers.
ACM SIGIR.Lewis, D.D.
1995.
Evaluating and optimizing au-tonomous text classification systems.
ACM SIGIR.Li, M. and I.K.
Sethi.
2006.
Confidence-Based Ac-tive Learning.
IEEE Transactions on Pattern Analy-sis and Machine Intelligence, 28(8):1251?1261.Schein, Andrew I.
2005.
Active Learning for LogisticRegression.
Ph.D. thesis, University of Pennsylva-nia.Sch?utze, H., E. Velipasaoglu, and J.O.
Pedersen.
2006.Performance thresholding in practical text classifica-tion.
In CIKM, pages 662?671.Shen, Dan, Jie Zhang, Jian Su, Guodong Zhou, andChew-Lim Tan.
2004.
Multi-criteria-based activelearning for named entity recognition.
In ACL ?04.Tjong Kim Sang, Erik F. and Fien De Meulder.2003.
Introduction to the conll-2003 shared task:Language-independent named entity recognition.
InProceedings of CoNLL-2003, pages 142?147.Tomanek, Katrin, Joachim Wermter, and Udo Hahn.2007.
An approach to text corpus constructionwhich cuts annotation costs and maintains reusabil-ity of annotated data.
In EMNLP-CoNLL.Vlachos, Andreas.
2008.
A stopping criterion foractive learning.
Computer Speech and Language,22(3):295?312.Zhu, J. and E. Hovy.
2007.
Active learning for wordsense disambiguation with methods for addressingthe class imbalance problem.
In EMNLP-CoNLL.472
