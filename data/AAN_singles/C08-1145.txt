Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1153?1160Manchester, August 2008Choosing the Right Translation:A Syntactically Informed Classification ApproachSimon ZwartsCentre for Language TechnologyMacquarie UniversitySydney, Australiaszwarts@ics.mq.edu.auMark DrasCentre for Language TechnologyMacquarie UniversitySydney, Australiamadras@ics.mq.edu.auAbstractOne style of Multi-Engine MachineTranslation architecture involves choos-ing the best of a set of outputs fromdifferent systems.
Choosing the besttranslation from an arbitrary set, evenin the presence of human references, isa difficult problem; it may prove betterto look at mechanisms for making suchchoices in more restricted contexts.In this paper we take a classification-based approach to choosing betweencandidates from syntactically informedtranslations.
The idea is that usingmultiple parsers as part of a classifiercould help detect syntactic problems inthis context that lead to bad transla-tions; these problems could be detectedon either the source side?perhaps sen-tences with difficult or incorrect parsescould lead to bad translations?or onthe target side?perhaps the outputquality could be measured in a moresyntactically informed way, looking forsyntactic abnormalities.We show that there is no evidence thatthe source side information is useful.However, a target-side classifier, whenused to identify particularly bad trans-lation candidates, can lead to signifi-cant improvements in Bleu score.
Im-provements are even greater when com-bined with existing language and align-ment model approaches.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.1 IntroductionIt is fairly safe to say that whenever thereare multiple approaches to solving a problemin Artificial Intelligence, the idea of trying tofind a better solution by combining those ap-proaches has been proposed: blackboard archi-tectures, ensemble methods for machine learn-ing, and so on.In Machine Translation (MT), there is along tradition of combining multiple machinetranslations, as through a Multi-Engine MT(MEMT) architecture; the origins of this aregenerally credited to Frederking and Nirenburg(1994).
One way of dividing up such systemsis into those that take the whole output frommultiple systems and judge between them toselect the best candidate, and those that com-bine elements of the outputs to construct abest candidate.Deciding between whole sentence level out-puts looks like a classical classification prob-lem.
Of course, deciding between MT out-puts in the general case is a problem that cur-rently has no good solution, and is unlikelyto in the near future: Bleu (and similar met-rics) require one or more reference texts to dis-tinguish between candidate outputs with thelevel of accuracy that they achieve, and eventhen they are open to substantial criticism(Callison-Burch et al, 2006).
However, thereare reasons to think that there is some promisein considering this as a classification problem.Corston-Oliver et al (2001) build a classifier todistinguish between human and machine trans-lations with an 80% accuracy.
Several otherlater systems have some success in distinguish-ing between MT outputs using language mod-els, alignment models, and voting schemes.
Inaddition, while the problem of deciding be-tween arbitrary MT outputs is difficult, it may1153be feasible in specific cases.
The classifier con-structed by Corston-Oliver et al (2001) takesadvantage of characteristic mistakes found inthe output of the particular MT system used.In general, we are interested in MT wheresyntax is involved.
The first part of the mainidea of this paper is that there are two waysin which problematic translations might be de-tected.
One is on the source side: perhaps sen-tences with difficult or incorrect parses couldlead to bad use of syntax and hence bad trans-lations, and this could be detected by a clas-sifier.
The other is that on the target side,perhaps the output quality could be measuredin a more syntactically informed way, lookingfor syntactic abnormalities.As to the particular system, in this paperwe look at a specific type of MT, the output ofsystems that use syntactic reordering as pre-processing (Collins et al, 2005; Wang et al,2007; Zwarts and Dras, 2007).
In these sys-tems, the source language is reordered to mir-ror the syntax of the target language in certainrespects, leading to an improvement in the ag-gregate quality of the output over the base-line, although it is not always the case thateach individual sentence in the reordered ver-sion is better.
This could then be framed as anMEMT, where the reordered candidate is con-sidered the default one, backing off to the base-line where the reordered one is worse, based onthe decision of a classifier.
Given the ?unnatu-ral?
order of the preprocessed source side, thereis reason to expect that bad or unsuccessful re-ordered translations might be detectable.The second part of the main idea of the pa-per is that a classifier could use a combina-tion of multiple parsers, as in Mutton et al(2007), to indicate problems.
In that work,designed to assess fluency of output of gen-eration systems, metrics were developed fromvarious parsers?log probability of most likelyparse, number of tree fragments, and so on?that correlated with human judgements, andthat could be combined in a classifier to pro-duce a better evaluation metric.
We take suchan approach as a starting point for developingclassifiers to indicate problematic source andtarget sides within a reordering MT system.In Section 2 we review some related work.
InSection 3 we investigate the potential gain incorrectly choosing the better translation can-didate in our context.
In Section 4 we builda classifier using an approximation to fairlystandard language and alignment model fea-tures, mostly for use as a comparator, whileSections 5 and 6 present our models based onsource and target language sides respectively.Section 7 concludes.2 Related workIn this section we briefly review some relevantwork on deciding between translation candi-dates in ?sentence-level?
MEMT.Most common is the use of language models,or voting which may be based on some kind ofalignment, or a combination.
Callison-Burchand Flournoy (2001) use a trigram languagemodel (LM) on MT outputs to decide the bestcandidate, looking at nine systems across fourlanguage directions and domains, and treatingthem as black boxes; evaluation is by humanjudges and on a fairly small data set.
Akibaet al (2002) score MT outputs by a combina-tion of a standard LM and an alignment model(here IBM 4), and then use statistical teststo determine rankings of MT system outputs.Eisele (2005) uses a heuristic voting schemebased on n-gram overlap of the different out-puts, and adds an LM to make decisions; theLM reportedly achieves further improvement.Rosti et al (2007) look at sentence-level com-binations (as well as word- and phrase-level),using reranking of n-best lists and confidencescores derived from generalised linear modelswith probabilistic features from n-best lists.Huang and Papineni (2007) propose a hier-archical model for word, phrase and sentencelevel combination; they use LMs and inter-estingly find that incorporating rudimentarylinguistic information like Part-of-Speech ishelpful.
Riezler and Maxwell (2006) combinetransfer-based and statistical MT; they backoff to the SMT translation when the grammaris inadequate, analysing the grammar to deter-mine this.Other work, like ours, uses a classifier.
Thegoal of Corston-Oliver et al (2001) is slightlydifferent, in that it aims to distinguish humantranslations from MT output.
The classifieruses syntactic features derived from a manualerror analysis, taking advantage of character-1154istics specific to their MT system and parser.Nomoto (2003) uses a LM and an IBM-basedalignment model, and then constructs sepa-rate SVMs for regression based on these, eachwith a single feature (i.e.
the LM value or thealignment model value); the SVM is thus notstrictly used as a classifier, but as a regressiontool.
Nomoto (2004) extends this by decid-ing on the best LM through a voting scheme.Other related work not in an MEMT contextthat uses parsers to distinguish better fromworse translations are on syntax-based lan-guage models (Charniak et al, 2003) and onsyntactically informed reranking (Och et al,2003).
Both use only single parsers and workonly with candidate translations generated in-side an SMT system (either all candidates orn-best).3 Potential GainThe type of system we focus on in this pa-per operates in two stages.
First, syntac-tically based reordering takes place to makethe source sentence more similar in struc-ture to the syntax of the target language.This is then passed to a Phrase-based SMT(PSMT) component (Pharaoh (Koehn, 2004)in the cited work).
For German to English(Collins et al, 2005) and Dutch to English(Zwarts and Dras, 2007) this reordering in-volves moving some long-distance dependen-cies closer together, such as clause-final par-ticiples and verb-second auxiliaries.
This im-proves translation quality by compensating forthe weakness in PSMT of long-distance wordreordering: Collins et al (2005) report a 1.6Bleu percentage point improvement, Zwartsand Dras (2007) a 1.0 Bleu percentage pointimprovement.However, individual sentences translatedfrom the original non-reordered source sen-tences are sometimes better than their re-ordered equivalent; examples are given in bothCollins et al (2005) and Zwarts and Dras(2007).
(We refer to these in the rest of thepaper as non-reordered translations and re-ordered translations respectively.)
For there tobe a point to constructing an MEMT-style sys-tem where the reordered translation is the de-fault translation and the non-reordered trans-lation the fallback, it is necessary for the non-reordered version to be better a reasonableproportion of the time, allowing scope for aBleu improvement across the system.To determine if this is the case, we constructan approximate oracle to choose the betterof each pair of reordered and non-reorderedtranslation sentences.
While Bleu is a rea-sonable choice for evaluating the quality of theoverall composite set of translation sentences,it is not suitable for sentence-level decisions.However, in line with Nomoto (2003)?s moti-vation for developing m-precision as an alter-native to Bleu, we make the following obser-vation.The Bleu score (ignoring brevity) is an har-monic mean between the different n-gram com-ponents:exp(?Nn=1log pn)Here pnis the precision for the different n-gramoverlap counts of a candidate sentence with agold standard sentence.
If we want to glob-ally optimise this score for an optimal Bleudocument score, we need to pick for each sen-tence the n-gram counts that contribute mostto the overall score.
For example, if we haveto pick between sentence A and sentence B,where A has 2 unigram counts and 1 bigramcount, and B has 2 unigram counts only, A isclearly preferred; however, for sentences C andD, where C has 4 unigram counts and D has 2unigram counts and 1 bigram count, we do notknow which eventually will lead to the globalmaximum Bleu.However we observe that because it is anharmonic mean, small values are weighted ex-ponentially heavier, due to the log operator.Our heuristic to achieve the highest score is tohave the most extreme possible small values.Since we know that an n-gram is always lessfrequent than an (n?
1)-gram we concentrateon the higher n-grams first.
The decision pro-cess between sentences is therefore to choosethe candidate with higher n-gram counts forthe maximum value of n, then n ?
1-gramcounts, and so on down to unigrams.Here we will work with the Dutch?Englishdata used by Zwarts and Dras (2007).
We usethe portions of the Europarl corpus (Koehn,2003) that were used for training in that work;and Bleu with 1 reference with n-grams upto length 4.
We then use our heuristic to se-lect between the reordered and non-reordered1155Not-Bleu comparableIdentical 179,327Undecidable 119,725Total 299,052Bleu comparableNon-Reordered better 128,585Reordered better 163,172Total 291,757Overall Total 590,809Table 1: Comparing translation qualityLearner Baseline AccuracyEnglish ?
DutchSVM - Polynomial 56.0% 56.6%SVM - Polynomial 50.0% 51.2%Maximum Entropy 50.0% 51.0%Dutch ?
EnglishSVM - Polynomial 50.0% 51.4%Table 2: Results for internal language decidertranslation candidates of Zwarts and Dras(2007) for the language direction Dutch to En-glish.
Selecting the reordered translation asdefault and backing off leads to a 1.1 Bleupercentage point improvement over the 1.0 al-ready mentioned.
Results for English to Dutchare similar.In Table 1 we see the breakdown of the en-tire corpus we work with.
Some sentences areidentical, and some are different but with noindication by our heuristic as to which of thetwo is better.
In the cases where we do havean indication we see a sizeable 44% of the non-reordered translations are better.4 Internal IndicatorsBefore looking at our syntax-related ap-proaches, it would be useful to have a com-parison based on the approaches of previouswork.
As noted in Section 2, these generallyuse language models and alignment models, asusual to estimate fluency and fidelity of candi-date translations.Because our two candidate solutions areboth ultimately produced by Pharaoh (Koehn,2004), our quick-and-dirty solution can usePharaoh?s own final translation probabili-ties, which capture language and alignmentmodel information.
We build a classifierLearner Baseline AccuracyEnglish ?
DutchSVM - Polynomial 50.0% 50.1%SVM - Radial 50.0% 49.7%Maximum Entropy 50.0% 50.2%Table 3: Results for Source language deciderthat attempts to distinguish the better of apair of reordered and non-reordered transla-tions.
Denoting the non-reordered transla-tion Tn, and the reordered Tr, we take as fea-tures log(P (Tn)), log(P (Tr)), and log(P (Tn))-log(P (Tr)).
In addition, because the sentencesdo not always have equal length and we donot want to penalise longer sentences, we alsohave three features describing the perplex-ity: elog(P (Tn))/length(Tn), elog(P (Tr))/length(Tr),and the difference between these two.
Herelength is the function returning the length ofa sentence in tokens.
Our training data weget by partitioning the sentences according towhether reordering is beneficial as measured byour heuristic from Section 3.
As machine learn-ers we used SVM-light1(Joachims, 1998) andthe MaxEnt decider from the Stanford Classi-fier2(Manning and Klein, 2003).Table 2 shows the results the classifier pro-duces on this data set.
While the accuracyrates for the classifiers are all statistically sig-nificantly different (at a 95% confidence level)from the baseline (using a standard test of pro-portions), the results are not promising.5 Source Language Indicators5.1 All DataThe finding that almost half of the reorderedtranslations degrade the actual translationquality raises the question of why.
Our ini-tial hypothesis is that because we use morelinguistic tools, this is likely to introduce newerrors.
We hypothesise that one of the prob-lems of reordering is either the parser gettingit wrong, or the rules getting it wrong becauseof parse complexity.
Our idea for estimatingthe wrongness of a parse, or the complexity ofa parse that might lead to incorrect reorderingrule application, is to use ?side-effect?
informa-1http://svmlight.joachims.org2http://nlp.stanford.edu/software/classifier.shtml1156Top Correct Accuracy10 5 50%50 23 46%100 48 48%200 100 50%500 240 48%1000 490 49%Table 4: Accuracy range for Source Side Ex-treme Predictionstion from multiple parsers, in a modification ofan idea taken from Mutton et al (2007).3Forexample, the parser of Collins (1999), in addi-tion to the actual parse, gives a probability forthe most likely parse; if this most likely parse isnot at all likely, this may be because the parseris having difficulty.
The Link Parser (Grinberget al, 1995) produces dependency-style parses,and gives an unlinked fragment count where acomplete parse cannot be made; this unlinkedfragment count may be indicative of parse dif-ficulty.
For this part, we therefore look onlyat translations with English as source side andDutch as target, in order to be able to use mul-tiple parsers on the source side sentences.Again, we construct a machine learner topredict which is the better of the reordered andnon-reordered translations.
Our training datais as in Section 4.As a feature set we use: character and to-ken length of the sentence, probability valuesas supplied by the Collins parser, and the un-linked fragment count as supplied by the LinkParser.
We used machine learners as in Sec-tion 4.
Both the SVM and the features aresimilar to Mutton et al (2007).The results are calculated on 39k examples,split 30k training, 9k testing.
Table 3 showsthe results for different learning techniqueswith different settings.
The accuracy scoresshow selection no different from random: noneof the differences are statistically significant.With such poor results, we do not bother tocalculate the Bleu effect of using the classifieras a decider here.3Similar work is that of Albrecht and Hwa (2007);however this requires human references unavailablehere.Learner Baseline AccuracyDutch ?
EnglishSVM - Polynomial 50.0% 52.3%Maximum Entropy 50.0% 52.9%Table 5: Results for target language decider5.2 ThresholdingBecause our MEMT uses the non-reorderedtranslations as a back-off, even if the classifieris not accurate over the whole set of sentences,it could still be useful to identify the poor-est reordered translations and back off only inthose cases.
SVM-light gives prediction scoresas part of its classification; data points that arefirmly within the positive (negative) classifica-tion spaces are higher positive (negative) val-ues, while border-line cases have a value veryclose to 0.
Here we interpret these as an es-timate of the magnitude of the difference inquality between reordered and non-reorderedtranslations.
We calculated the accuracy overthe n most extreme predictions for differentvalues of n. The results in Table 4 show thatthe ?extreme range?
does not have a higher ac-curacy either.6 Target Language Indicators6.1 All DataWe now consider our second approach, tryingto classify syntactic abnormality of the trans-lations.
Inspecting the sentences by hand,we found that there are some sentences withmarkedly poor grammaticality, even by thestandards of MT output.
Examples of of-ten reoccurring problems include verb posi-tioning (often still sentence-final), positioningof modals in the sentence, etc.
Most are in therealm of problems the reordering rules actuallytry to target.Here we use the multiple-parser approach ina way more like that of Mutton et al (2007),as an estimate of the fluency of the sentencewith a focus on syntactic characteristics.
As inSection 5, we construct a classifier using mul-tiple parser outputs to distinguish the betterof a pair of reordered and non-reordered trans-lations.
Similarly, we use as features the mostlikely parse probability of the Collins parser(Collins, 1999) and unlinked fragment count1157Learner Baseline AccuracySVM - Complete 50.0% 52.3%SVM - LargeDiff 50.0% 52.9%SVM - HugeDiff 50.0% 51.2%Table 6: Varying Bleu training datafrom the Link parser (Grinberg et al, 1995).We combine these with the sentences lengthsin both character count and token count of thetwo candidate sentences.Our translation direction in this section,Dutch to English, is the opposite of Section 5,for the same reason that we want to use multi-ple parsers on the target side.
The reorderingon the Dutch language is done on the results ofthe Alpino (Bouma et al, 2000) parser.
Therules for reordering are found in Zwarts andDras (2006).
Our training data is again as inSection 4.Table 5 shows the accuracy, calculated ona 38k examples, split 30k training, 8k testing.The accuracy again is close to baseline perfor-mance, although it is clearly better than ourLM and alignment classifier of Section 4.
Hereall the improvements are statistically signifi-cant on a 95% confidence level.
This is sur-prising as Mutton et al (2007) on a somewhatsimilar task was much more successful.
Theirperformance is expressed as a correlation withhuman judgement rather than accuracy, butcompared to our performance where the im-provement in accuracy is only a couple of timesthe standard error, their approach performedmuch better.
A possible explanation could bethat the data we work on has much subtler dif-ferences than their work.
We know both trans-lations are ultimately generated from the sameinput, which makes our both candidates veryclose.6.2 Varying Training DataIn particular in (Mutton et al, 2007) the train-ing data used human sentences as positive ex-emplars and very simple bigram-generated sen-tences as negative ones, so that there was a bigdifference in quality between them.
So per-haps there are too many borderline cases inthe training data here.Therefore we retrained the classifier of Sec-tion 6.1, selecting only those sentence pairsTop Correct Accuracy10 9 90%20 19 95%50 40 80%100 79 79%200 145 72.5%500 300 66.6%1000 538 53.8%Table 7: Accuracy of Prediction in the extremerangewhere the difference was more distinct.
Forthe LargeDiff set the difference was at least 4or more unigrams or 3 or more bigrams; forthe HugeDiff set the difference was at least 6or more unigrams or 5 or more bigrams.Table 6 shows the results; all accuracy scoresare better than the baseline with 95% confi-dence.
For LargeDiff, there is an improvementover using the complete data set.
Surprisingly,for the HugeDiff training data the gain is notonly gone, but this decider performs statis-tically significantly worse than using all thedata.We therefore conclude that the nature ofmistakes made when using reordering as apreprocessing step is of a very subtle kind.Very big mistakes are made as part of trans-lation process completely independent of re-ordering, while the improvement due to re-ordering is only where subtly a small set ofwords, compared to the reference, has beenchanged for the better.
The training size how-ever is only reduced to three quarters of thecomplete training size.
It is therefore very un-likely this sudden drop in performance is dueto data sparsity.6.3 ThresholdingAs in Section 5.2, we look at the cases whereour SVM gives a higher prediction score thatindicates a greater difference in quality of thenon-reordered translation over the reorderedone.
Here we use as training data the LargeDiffset from Section 6.2.Results are in Table 7, which unlike thethresholded results of Section 5.2 are quitepromising.
There is a clear pattern here, withvery high accuracy scores in the top range,slowly dropping to around overall performance1158System BleuBaseline 0.208Reordered 0.221SVM-pick 0.238Table 8: Bleu results for the different selec-tionsFeatures AccuracySVM - all 52.3%SVM - length only 49.8%SVM - length and Link 50.5%SVM - length and Collins 50.1%Table 9: Contribution of Parsersafter 1000 samples.
This 1000 mark is out of3461 negative samples in the test set range,roughly marking the first third mark beforeaccuracy scores have reached average perfor-mance.Predictions with an extreme score on theother side of the scale hardly show an improve-ment.
Because this subset of sentences showsa higher accuracy, it is worthwhile to calculateBleu scores over the sentences in the test setbelonging to the top 500 SVM-predictions pos-itive (reordered translation is better) and the500 SVM predictions negative (non-reorderedis better).
Table 8 shows the improvement ofBleu scores.4The first interesting thing which can be seenin the table is that this subset of sentencesalready has higher improvement than is seenin the whole data set simply by choosing thereordered only, because the SVM is alreadyused to pick the most discriminating sentences.We note that on this subset of sentences ourtechnique of picking the right sentence actu-ally scores an improvement equal to the use ofreordering by itself.6.4 Parser ContributionIn Table 9 we show the effects of individualparsers, taking as the starting point the SVMof Table 5.
Clearly, combining parsers leads toa much better decider.Learner Baseline AccuracyDutch ?
EnglishSVM Polynomial 50.0% 60.5%Table 10: Combining internal features withtarget side featuresTop Reordering Non-reordered10 9 90% 10 100%20 18 90% 18 90%50 33 66% 43 86%100 61 61% 77 77%200 114 57% 148 74%500 289 58% 383 76%1000 564 56% 748 75%Table 11: Accuracy of the Combined model6.5 Combining ModelsAs the results of classifying translation outputsusing features derived from multiple parsersare promising, we next look at whether itis useful to combine this information withthe language and alignment model informationfrom Section 4.
Remarkably, as can be seen inTable 10, the combination of these two fea-tures has a much greater effect that the twofeatures sets individually.
Comparing thesescores against 80% accuracy achieved in dis-tinguish MT output from human output in thework of Corston-Oliver et al (2001), this 60%on a dataset with much more subtle differencesis quite promising.Furthermore Table 11 shows the accuracyranking of the SVM for the combining modelfor the extreme SVM-predictions, similar toTables 4 and 7.
The last column of Table 11matches previous tables, but now we also showan improvement in correct prediction for thereordered cases.7 ConclusionIn this paper we have looked at a restrictedMEMT scenario, where we choose betweena syntactic-reordering-as-preprocessing trans-lation candidate, in the style of (Collins etal., 2005), and a baseline PSMT candidate.We have shown that using a classifier builtaround outputs of multiple parsers, to decide4Baseline here is the same baseline from Zwarts andDras (2007), which is the parser read-off of the tree.1159whether to back off to the baseline candidate,can be successful in selecting the right candi-date.
There is no indication that classifyinginformation on the source side?looking to seewhether sentences with difficult or incorrectparses could lead to bad reorderings and hencebad translations?is useful; however, applyingsuch a classifier to the target side?looking tosee whether the output quality could be mea-sured in a syntactically informed way, look-ing for syntactic abnormalities?is successfulin detecting particularly bad translation can-didates, and leads to an improvement in Bleuscore over the reordered translations equal tothe improvement gained by the reordering ap-proach over the baseline.
Multiple parsersclearly improve the results over single parsers.The target-side classifier can also be usefullycombined with language and alignment modelfeatures, improving its accuracy substantially;continuing with such an approach looks like apromising direction.
As a further step, the re-sults are sufficiently positive to extend to othersorts of syntactically informed SMT.ReferencesAkiba, Yasuhrio, Taro Watanabe, and Eiichiro Sumita.2002.
Using Language and Translation Models toSelect the Best among Outputs from Multiple MTsystems.
In Proc.
of Coling, pages 8?14.Albrecht, Joshua S. and Rebecca Hwa.
2007.
Regres-sion for Sentence-Level MT Evaluation.
In Proc.
ofACL, pages 296?303.Bouma, Gosse, Gertjan van Noord, and Robert Mal-ouf.
2000.
Alpino: Wide Coverage ComputationalAnalysis of Dutch.
In Computational Linguistics inthe Netherlands (CLIN).Callison-Burch, Chris and Raymond S. Flournoy.
2001.A Program for Automatically Selecting the BestOutput from Multiple Machine Translation Engines.In Proc.
MT Summit, pages 63?66.Callison-Burch, Chris, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the Role of Bleu inMachine Translation Research.
In Proc.
of EACL,pages 249?256.Charniak, Eugene, Kevin Knight, and Kenju Yamada.2003.
Syntax-based Language Models for StatisticalMachine Translation.
In Proc.
of MT Summit, pages40?46.Collins, Michael, Philipp Koehn, and Ivona Kucerova.2005.
Clause restructuring for statistical machinetranslation.
In Proc.
of ACL, pages 531?540, AnnArbor, Michigan, June.Collins, Michael.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Corston-Oliver, Simon, Michael Gamon, and ChrisBrockett.
2001.
A machine learning approach tothe automatic evaluation of machine translation.
InProc.
of ACL, pages 148?155.Eisele, Andreas.
2005.
First steps towards multi-enginemachine translation.
In Proc.
of the ACL Workshopon Building and Using Parallel Texts, pages 155?158.Frederking, Robert and Sergei Nirenburg.
1994.
ThreeHeads are Better than One.
In Proc.
of the ACLConference on Applied Natural Language Processing,pages 95 ?
100.Grinberg, Dennis, John Lafferty, and Daniel Sleator.1995.
A robust parsing algorithm for link grammars.In Proc.
of the International Workshop on ParsingTechnologies.Huang, Fei and Kishore Papineni.
2007.
HierarchicalSystem Combination for Machine Translation.
InProc.
of EMNLP, pages 277?286.Joachims, T. 1998.
Making large-scale support vec-tor machine learning practical.
In B. Scho?lkopf,C.
Burges, A. Smola, editor, Advances in Ker-nel Methods: Support Vector Machines.
MIT Press,Cambridge, MA.Koehn, Philipp.
2003.
Europarl: A Multilingual Cor-pus for Evaluation of Machine Translation PhilippKoehn, Draft, Unpublished.Koehn, Philipp.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
In Proc.
of AMTA, pages 115?124.Manning, Christopher and Dan Klein.
2003.
Optimiza-tion, Maxent Models, and Conditional Estimationwithout Magic.
Tutorial at HLT-NAACL 2003 andACL 2003.Mutton, Andrew, Mark Dras, Stephan Wan, andRobert Dale.
2007.
Gleu: Automatic evaluation ofsentence-level fluency.
In Proc of ACL, pages 344?351.Nomoto, Tadashi.
2003.
Predictive Models of Per-formance in Multi-Engine Machine Translation.
InProc.
of MT Summit, pages 269?276.Nomoto, Tadashi.
2004.
Multi-Engine Machine Trans-lation with Voted Language Model.
In Proc.
of ACL,pages 494?501.Och, Franz Josef, Daniel Gildea, Sanjeev Khundanpur,Anoop Sarkar, Kenju Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jainand Zhen Jin, and Dragomir Radev.
2003.Final report of Johns Hopkins 2003 summer work-shop on syntax for statistial machine translation.Riezler, Stefan and John Maxwell, III.
2006.
Gram-matical machine translation.
In Proc of NAACL,pages 248?255.Rosti, Antti-Veikko, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and B BonnieJ.
Dorr.
2007.
Combining Outputs from MultipleMachine Translation Systems?
in Human Language.In Proc.
of NAACL, pages 228?235.Wang, Chao, Michael Collins, and Philipp Koehn.2007.
Chinese Syntactic Reordering for StatisticalMachine Translation.
In Proc of EMNLP, pages737?745.Zwarts, Simon and Mark Dras.
2006.
This Phrase-Based SMT System is Out of Order: GeneralisedWord Reordering in Machine Translation.
In Proc.of the Australasian Language Technology Workshop,pages 149?156.Zwarts, Simon and Mark Dras.
2007.
Syntax-BasedWord Reordering in Phrase-Based Statistical Ma-chine Translation: Why Does it Work?
In Proc.of MT Summit, pages 559?566.1160
