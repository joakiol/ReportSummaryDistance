FLSA: Extending Latent Semantic Analysis with featuresfor dialogue act classificationRiccardo SerafinCEFRIELVia Fucini 220133 Milano, ItalyRiccardo.Serafin@students.cefriel.itBarbara Di EugenioComputer ScienceUniversity of IllinoisChicago, IL 60607 USAbdieugen@cs.uic.eduAbstractWe discuss Feature Latent Semantic Analysis(FLSA), an extension to Latent Semantic Analysis(LSA).
LSA is a statistical method that is ordinar-ily trained on words only; FLSA adds to LSA therichness of the many other linguistic features thata corpus may be labeled with.
We applied FLSAto dialogue act classification with excellent results.We report results on three corpora: CallHome Span-ish, MapTask, and our own corpus of tutoring dia-logues.1 IntroductionIn this paper, we propose Feature Latent SemanticAnalysis (FLSA) as an extension to Latent Seman-tic Analysis (LSA).
LSA can be thought as repre-senting the meaning of a word as a kind of averageof the meanings of all the passages in which it ap-pears, and the meaning of a passage as a kind ofaverage of the meaning of all the words it contains(Landauer and Dumais, 1997).
It builds a semanticspace where words and passages are represented asvectors.
LSA is based on Single Value Decompo-sition (SVD), a mathematical technique that causesthe semantic space to be arranged so as to reflectthe major associative patterns in the data.
LSA hasbeen successfully applied to many tasks, such as as-sessing the quality of student essays (Foltz et al,1999) or interpreting the student?s input in an Intel-ligent Tutoring system (Wiemer-Hastings, 2001).A common criticism of LSA is that it uses onlywords and ignores anything else, e.g.
syntactic in-formation: to LSA, man bites dog is identical to dogbites man.
We suggest that an LSA semantic spacecan be built from the co-occurrence of arbitrary tex-tual features, not just words.
We are calling LSAaugmented with features FLSA, for Feature LSA.Relevant prior work on LSA only includes Struc-tured Latent Semantic Analysis (Wiemer-Hastings,2001), and the predication algorithm of (Kintsch,2001).
We will show that for our task, dialogueact classification, syntactic features do not help, butmost dialogue related features do.
Surprisingly, onedialogue related feature that does not help is the di-alogue act history.We applied LSA / FLSA to dialogue act classi-fication.
Dialogue systems need to perform dia-logue act classification, in order to understand therole the user?s utterance plays in the dialogue (e.g.,a question for information or a request to performan action).
In recent years, a variety of empiri-cal techniques have been used to train the dialogueact classifier (Samuel et al, 1998; Stolcke et al,2000).
A second contribution of our work is toshow that FLSA is successful at dialogue act classi-fication, reaching comparable or better results thanother published methods.
With respect to a baselineof choosing the most frequent dialogue act (DA),LSA reduces error rates between 33% and 52%, andFLSA reduces error rates between 60% and 78%.LSA is an attractive method for this task becauseit is straightforward to train and use.
More impor-tantly, although it is a statistical theory, it has beenshown to mimic many aspects of human compe-tence / performance (Landauer and Dumais, 1997).Thus, it appears to capture important componentsof meaning.
Our results suggest that LSA / FLSAdo so also as concerns DA classification.
On Map-Task, our FLSA classifier agrees with human codersto a satisfactory degree, and makes most of the samemistakes.2 Feature Latent Semantic AnalysisWe will start by discussing LSA.
The input to LSAis a Word-Document matrix W with a row for eachword, and a column for each document (for us, adocument is a unit, e.g.
an utterance, tagged with aDA).
Cell c(i, j) contains the frequency with whichwordi appears in documentj .1 Clearly, this w ?
dmatrix W will be very sparse.
Next, LSA applies1Word frequencies are normally weighted according to spe-cific functions, but we used raw frequencies because we wantedto assess our extensions to LSA independently from any biasintroduced by the specific weighting technique.to W Singular Value Decomposition (SVD), to de-compose it into the product of three other matrices,W = T0S0DT0 , so that T0 and D0 have orthonormalcolumns and S0 is diagonal.
SVD then providesa simple strategy for optimal approximate fit usingsmaller matrices.
If the singular values in S0 are or-dered by size, the first k largest may be kept and theremaining smaller ones set to zero.
The product ofthe resulting matrices is a matrix W?
of rank k whichis approximately equal to W ; it is the matrix of rankk with the best possible least-squares-fit to W .The number of dimensions k retained by LSA isan empirical question.
However, crucially k is muchsmaller than the dimension of the original space.The results we will report later are for the best kwe experimented with.Figure 1 shows a hypothetical dialogue annotatedwith MapTask style DAs.
Table 1 shows the Word-Document matrix W that LSA starts with ?
note thatas usual stop words such as a, the, you have beeneliminated.
2 Table 2 shows the approximate repre-sentation of W in a much smaller space.To choose the best tag for a document in the testset, we first compute its vector representation in thesemantic space LSA computed, then we comparethe vector representing the new document with thevector of each document in the training set.
Thetag of the document which has the highest similaritywith our test vector is assigned to the new document?
it is customary to use the cosine between the twovectors as a measure of similarity.
In our case, thenew document is a unit (utterance) to be tagged witha DA, and we assign to it the DA of the document inthe training set to which the new document is mostsimilar.Feature LSA.
In general, in FLSA we add extrafeatures to LSA by adding a new ?word?
for eachvalue that the feature of interest can take (in somecases, e.g.
when adding POS tags, we extend thematrix in a different way ?
see Sec.
4).
The onlyassumption is that there are one or more non wordrelated features associated with each document thatcan take a finite number of values.
In the Word?Document matrix, the word index is increased to in-clude a new place holder for each possible value thefeature may take.
When creating the matrix, a countof one is placed in the rows related to the new in-dexes if a particular feature applies to the documentunder analysis.
For instance, if we wish to includethe speaker identity as a new feature for the dialogue2We use a very short list of stop words (< 50), as our experi-ments revealed that for dialogue act annotation LSA is sensitiveto the most common words too.
This is why to is included inTable 1.in Figure 1, the initial Word?Document matrix willbe modified as in Table 3 (its first 14 rows are as inTable 1).This process is easily extended if more than onenon-word feature is desired per document, if morethan one feature value applies to a single documentor if a single feature appears more than once in adocument (Serafin, 2003).3 CorporaWe report experiments on three corpora, SpanishCallHome, MapTask, and DIAG-NLP.The Spanish CallHome corpus (Levin et al,1998; Ries, 1999) comprises 120 unrestricted phonecalls in Spanish between family members andfriends, for a total of 12066 unique words and 44628DAs.
The Spanish CallHome corpus is annotated atthree levels: DAs, dialogue games and dialogue ac-tivities.
The DA annotation augments a basic tagsuch as statement along several dimensions, suchas whether the statement describes a psychologi-cal state of the speaker.
This results in 232 differ-ent DA tags, many with very low frequencies.
Inthis sort of situations, tag categories are often col-lapsed when running experiments so as to get mean-ingful frequencies (Stolcke et al, 2000).
In Call-Home37, we collapsed different types of statementsand backchannels, obtaining 37 different tags.
Call-Home37 maintains some subcategorizations, e.g.whether a question is yes/no or rhetorical.
In Call-Home10, we further collapse these categories.
Call-Home10 is reduced to 8 DAs proper (e.g., state-ment, question, answer) plus the two tags ??%?
?for abandoned sentences and ??x??
for noise.CallHome Spanish is further annotated for dialoguegames and activities.
Dialogue game annotation isbased on the MapTask notion of a dialogue game,a set of utterances starting with an initiation andencompassing all utterances up until the purpose ofthe game has been fulfilled (e.g., the requested infor-mation has been transferred) or abandoned (Car-letta et al, 1997).
Moves are the components ofgames, they correspond to a single or more DAs,and each is tagged as Initiative, Response or Feed-back.
Each game is also given a label, such asInfo(rmation) or Direct(ive).
Finally, activities per-tain to the main goal of a certain discourse stretch,such as gossip or argue.The HCRC MapTask corpus is a collection of di-alogues regarding a ?Map Task?
experiment.
Twoparticipants sit opposite one another and each ofthem receives a map, but the two maps differ.
Theinstruction giver (G)?s map has a route indicatedwhile instruction follower (F)?s map does not in-(Doc 1) G: Do you see the lake with the black swan?
Query?yn(Doc 2) F: Yes, I do Reply?y(Doc 3) G: Ok, Ready(Doc 4) G: draw a line straight to it Instruct(Doc 5) F: straight to the lake?
Check(Doc 6) G: yes, that?s right Reply?y(Doc 7) F: Ok, I?ll do it AcknowledgeFigure 1: A hypothetical dialogue annotated with MapTask tags(Doc 1) (Doc 2) (Doc 3) (Doc 4) (Doc 5) (Doc 6) (Doc 7)do 1 1 0 0 0 0 1see 1 0 0 0 0 0 0lake 1 0 0 0 1 0 0black 1 0 0 0 0 0 0swan 1 0 0 0 0 0 0yes 0 1 0 0 0 1 0ok 0 0 1 0 0 0 1draw 0 0 0 1 0 0 0line 0 0 0 1 0 0 0straight 0 0 0 1 1 0 0to 0 0 0 1 1 0 0it 0 0 0 1 0 0 1that 0 0 0 0 0 1 0right 0 0 0 0 0 1 0Table 1: The 14-dimensional word-document matrix Wclude the drawing of the route.
The task is for Gto give directions to F, so that, at the end, F is ableto reproduce G?s route on her map.
The MapTaskcorpus is composed of 128 dialogues, for a total of1,835 unique words and 27,084 DAs.
It has beentagged at various levels, from POS to disfluencies,from syntax to DAs.
The MapTask coding schemeuses 13 DAs (called moves), that include: Instruct(a request that the partner carry out an action), Ex-plain (one of the partners states some informationthat was not explicitly elicited by the other), Query-yn/-w, Acknowledge, Reply-y/-n/-w and others.
TheMapTask corpus is also tagged for games as definedabove, but differently from CallHome, 6 DAs areidentified as potential initiators of games (of coursenot every initiator DA initiates a game).
Finally,transactions provide the subdialogue structure of adialogue; each is built of several dialogue gamesand corresponds to one step of the task.DIAG-NLP is a corpus of computer mediated tu-toring dialogues between a tutor and a student whois diagnosing a fault in a mechanical system with atutoring system built with the DIAG authoring tool(Towne, 1997).
The student?s input is via menu, thetutor is in a different room and answers via a textwindow.
The DIAG-NLP corpus comprises 23 ?dia-logues?
for a total of 607 unique words and 660 DAs(it is thus much smaller than the other two).
It hasbeen annotated for a variety of features, includingfour DAs3 (Glass et al, 2002): problem solving, thetutor gives problem solving directions; judgment,the tutor evaluates the student?s actions or diagno-sis; domain knowledge, the tutor imparts domainknowledge; and other, when none of the previousthree applies.
Other features encode domain objectsand their properties, and Consult Type, the type ofstudent query.4 ResultsTable 4 reports the results we obtained for each cor-pus and method (to train and evaluate each method,we used 5-fold cross-validation).
We include thebaseline, computed as picking the most frequent DA3They should be more appropriately termed tutor moves.
(Doc 1) (Doc 2) (Doc 3) (Doc 4) (Doc 5) (Doc 6) (Doc 7)Dim.
1 1.3076 0.4717 0.1529 1.6668 1.1737 0.1193 0.9101Dim.
2 1.5991 0.6797 0.0958 -1.3697 -0.4771 0.2844 0.4205Table 2: The reduced 2-dimensional matrix W?
(Doc 1) (Doc 2) (Doc 3) (Doc 4) (Doc 5) (Doc 6) (Doc 7)do 1 1 0 0 0 0 1........................right 0 0 0 0 0 1 0<Giver> 1 0 1 1 0 1 0<Follower> 0 1 0 0 1 0 1Table 3: Word-document matrix W augmented with speaker identityin each corpus;4 the accuracy for LSA; the best ac-curacy for FLSA, and with what combination offeatures it was obtained; the best published result,taken from (Ries, 1999) and from (Lager and Zi-novjeva, 1999) respectively for CallHome and forMapTask.
Finally, for both LSA and FLSA, Table 4includes, in parenthesis, the dimension k of the re-duced semantic space.
For each LSA method andcorpus, we experimented with values of k between25 and 350.
The values of k that give us the best re-suls for each method were thus selected empirically.In all cases, we can see that LSA performsmuch better than baseline.
Moreover, we can seethat FLSA further improves performance, dramati-cally in the case of MapTask.
FLSA reduces errorrates between 60% and 78%, for all corpora otherthan DIAG-NLP (all differences in performance be-tween LSA and FLSA are significant, other than forDIAG-NLP).
DIAG-NLP may be too small a cor-pus to train FLSA; or Consult Type may not be ef-fective, but it was the only feature appropriate forFLSA (Sec.
5 discusses how we chose appropriatefeatures).
Another extension to LSA we developed,Clustered LSA, did give an improvement in perfor-mance for DIAG (79.24%) ?
please see (Serafin,2003).As regards comparable approaches, the perfor-mance of FLSA is as good or better.
For Span-ish CallHome, (Ries, 1999) reports 76.2% accu-racy with a hybrid approach that couples NeuralNetworks and ngram backoff modeling; the formeruses prosodic features and POS tags, and interest-ingly works best with unigram backoff modeling,i.e., without taking into account the DA history ?
seeour discussion of the ineffectiveness of the DA his-tory below.
However, (Ries, 1999) does not mention4The baselines for CallHome37 and CallHome10 are thesame because in both statement is the most frequent DA.his target classification, and the reported baseline ofpicking the most frequent DA appears compatiblewith both CallHome37 and CallHome10.5 Thus,our results with FLSA are slightly worse (- 1.33%)or better (+ 2.68%) than Ries?, depending on thetarget classification.
On MapTask, (Lager and Zi-novjeva, 1999) achieves 62.1% with TransformationBased Learning using single words, bigrams, wordposition within the utterance, previous DA, speakerand change of speaker.
We achieve much better per-formance on MapTask with a number of our FLSAmodels.As regards results on DA classification for othercorpora, the best performances obtained are up to75% for task-oriented dialogues such as Verbmobil(Samuel et al, 1998).
(Stolcke et al, 2000) reportsan impressive 71% accuracy on transcribed Switch-board dialogues, using a tag set of 42 DAs.
Theseare unrestricted English telephone conversations be-tween two strangers that discuss a general interesttopic.
The DA classification task appears more diffi-cult for corpora such as Switchboard and CallHomeSpanish, that cannot benefit from the regularitiesimposed on the dialogue by a specific task.
(Stolckeet al, 2000) employs a combination of HMM, neu-ral networks and decision trees trained on all avail-able features (words, prosody, sequence of DAs andspeaker identity).Table 5 reports a breakdown of the experimentalresults obtained with FLSA for the three tasks forwhich it was successful (Table 5 does not includek, which is always 25 for CallHome37 and Call-Home10, and varies between 25 and 75 for Map-Task).
For each corpus, under the line we find re-sults that are significantly better than those obtainedwith LSA.
For MapTask, the first 4 results that are5An inquiry to clarify this issue went unanswered.Corpus Baseline LSA FLSA Features Best known resultCallHome37 42.68% 65.36% (k = 50) 74.87% (k = 25) Game + Initiative 76.20%CallHome10 42.68% 68.91% (k = 25) 78.88% (k = 25) Game + Initiative 76.20%MapTask 20.69% 42.77% (k = 75) 73.91% (k = 25) Game + Speaker 62.10%DIAG-NLP 43.64% 75.73% (k = 50) 74.81% (k = 50) Consult Type n.a.Table 4: Accuracy for LSA and FLSACorpus accuracy FeaturesCallHome37 62.58% Previous DACallHome37 71.08% InitiativeCallHome37 72.69% GameCallHome37 74.87% Game+InitiativeCallHome10 68.32% Previous DACallHome10 73.97% InitiativeCallHome10 76.52% GameCallHome10 78.88% Game+InitiativeMapTask 41.84% SRuleMapTask 43.28% POSMapTask 43.59% DurationMapTask 46.91% SpeakerMapTask 47.09% Previous DAMapTask 66.00% GameMapTask 69.37% Game+Prev.
DAMapTask 73.25% Game+Speaker+Prev.
DAMapTask 73.91% Game+SpeakerTable 5: FLSA Accuracybetter than LSA (from POS to Previous DA) are stillpretty low; there is a difference of 19% in perfor-mance for FLSA when Previous DA is added andwhen Game is added.Analysis.
A few general conclusions can bedrawn from Table 5, as they apply in all three cases.First, using the previous DA does not help, eitherat all (CallHome37 and CallHome10), or very lit-tle (MapTask).
Increasing the length of the dialoguehistory does not improve performance.
In other ex-periments, we increased the length up to n = 4:we found that the higher n, the worse the perfor-mance.
As we will see in Section 5, introducingany new feature results in a larger and sparser initialmatrix, which makes the task harder for FLSA; tobe effective, the amount of information provided bythe new feature must be sufficient to overcome thishandicap.
It is clear that, the longer the dialogue his-tory, the sparser the initial matrix becomes, whichexplains why performance decreases.
However, thisdoes not explain why using even only the previousDA does not help.
This implies that the previousDA does not provide a lot of information, as in factis shown numerically in Section 5.
This is surpris-ing because the DA history is usually considered animportant determinant of the current DA (but (Ries,1999) observed the same).Second, the notion of Game appears to be reallypowerful, as it vastly improves performance on twovery different corpora such as CallHome and Map-Task.6 We will come back to discussing the usageof Game in a real dialogue system in Section 6.Third, the syntactic features we had access to donot seem to improve performance (they were avail-able only for MapTask).
In MapTask SRule indi-cates the main structure of the utterance, such asDeclarative or Wh-question.
It is not surprising thatSRule did not help, since it is well known that syn-tactic form is not predictive of DAs, especially thoseof indirect speech act flavor (Searle, 1975).
POStags don?t help LSA either, as has already been ob-served by (Wiemer-Hastings, 2001; Kanejiya et al,2003) for other tasks.
The likely reason is that it isnecessary to add a different ?word?
for each distinctpair word-POS, e.g., route becomes split as route-NN and route-VB.
This makes the Word-Documentmatrix much sparser: for MapTask, the number ofrows increases from 1,835 for plain LSA to 2,324for FLSA.These negative results on adding syntactic infor-mation to LSA may just reinforce one of the claimsof the LSA proponents, that structural informationis irrelevant for determining meaning (Landauer andDumais, 1997).
Alternatively, syntactic informationmay need to be added to LSA in different ways.
(Wiemer-Hastings, 2001) discusses applying LSAto each syntactic component of the sentence (sub-ject, verb, rest of sentence), and averaging out thosethree measures to obtain a final similarity measure.The results are better than with plain LSA.
(Kintsch,2001) proposes an algorithm that successfully dif-ferentiates the senses of predicates on the basis ontheir arguments, in which items of the semanticneighborhood of a predicate that are relevant to anargument are combined with the [LSA] predicatevector ... through a spreading activation process.6Using Game in MapTask does not introduce circularity,even if a game is identified by its initiating DA.
We checkedthe matching rates for initiating and non initiating DAs withthe FLSA model which employs Game + Speaker: they are78.12% and 71.67% respectively.
Hence, even if Game makesinitiating moves easier to classify, it is highly beneficial for theclassification of non initiating moves as well.5 How to select features for FLSAAn important issue is how to select features forFLSA.
One possible answer is to exhaustively trainevery FLSA model that corresponds to one possiblefeature combination.
The problem is that trainingLSA models is in general time consuming.
For ex-ample, training each FLSA model on CallHome37takes about 35 minutes of CPU time, and on Map-Task 17 minutes, on computers with one Pentium1.7Ghz processor and 1Gb of memory.
Thus, itwould be better to focus only on the most promis-ing models, especially when the number of featuresis high, because of the exponential number of com-binations.
In this work, we trained FLSA on eachindividual feature.
Then, we trained FLSA on eachfeature combinations that we expected to be effec-tive, either because of the good performances ofeach individual feature, or because they include fea-tures that are deemed predictive of DAs, such as theprevious DA(s), even if they did not perform wellindividually.After we ran our experiments, we performed apost hoc analysis based on the notion of Informa-tion Gain (IG) from decision tree learning (Quinlan,1993).
One approach to choosing the next featureto add to the tree at each iteration is to pick the onewith the highest IG.
Suppose the data set S is classi-fied using n categories v1...vn, each with probabil-ity pi.
S?s entropy H can be seen as an indicator ofhow uncertain the outcome of the classification is,and is given by:H(S) = ?n?i=1pilog2(pi) (1)If feature F divides S into k subsets S1...Sk, thenIG is the expected reduction in entropy caused bypartitioning the data according to the values of F :IG(S, A) = H(S)?k?i=1|Si||S|H(Si) (2)In our case, we first computed the entropy of thecorpora with respect to the classification inducedby the DA tags (see Table 6, which also includesthe LSA accuracy for convenience).
Then, we com-puted the IG of the features or feature combinationswe used in the FLSA experiments.Table 7 reports the IG for most of the featuresfrom Table 5; it is ordered by FLSA performance.On the whole, IG appears to be a reasonably accu-rate predictor of performance.
When a feature orfeature combination has a high IG, e.g.
over 1, thereCorpus Entropy LSACallHome37 3.004 65.36%CallHome10 2.51 68.91%MapTask 3.38 42.77%Table 6: Entropy measuresCorpus Features IG FLSACallHome37 Previous DA 0.21 62.58%CallHome37 Initiative 0.69 71.08%CallHome37 Game 0.59 72.69%CallHome37 Game+Initiative 1.09 74.87%CallHome10 Previous DA 0.13 68.32%CallHome10 Initiative 0.53 73.97%CallHome10 Game 0.53 76.52%CallHome10 Game+Initiative 1.01 78.88%MapTask Duration 0.54 43.59%MapTask Speaker 0.31 46.91%MapTask Prev.
DA 0.58 47.09%MapTask Game 1.21 66.00%MapTask Game+Speaker+Prev.
DA 2.04 73.25%MapTask Game+Speaker 1.62 73.91%Table 7: Information gain for FLSAis also a high performance improvement.
Occasion-ally, if the IG is small this does not hold.
For exam-ple, using the previous DA reduces the entropy by0.21 for CallHome37, but performance actually de-creases.
Most likely, the amount of new informationintroduced is rather low and it is overcome by hav-ing a larger and sparser initial matrix, which makesthe task harder for FLSA.
Also, when performanceimproves it does not necessarily increase linearlywith IG (see e.g.
Game + Speaker + Previous DAand Game + Speaker for MapTask).
Nevertheless,IG can be effectively used to weed out unpromisingfeatures, or to rank feature combinations so that themost promising FLSA models can be trained first.6 Discussion and future workIn this paper, we have presented a novel extensionto LSA, that we have called Feature LSA.
Our workis the first to show that FLSA is more effective thanLSA, at least for the specific task we worked on, DAclassification.
In parallel, we have shown that FLSAcan be effectively used to train a DA classifier.
Wehave reached performances comparable to or betterthan published results on DA classification, and wehave used an easily trainable method.FLSA also highlights the effectiveness of otherdialogue related features, such as Game, to classifyDAs.
The drawback of features such as Game is thatCorpus FLSACallHome37 0.676CallHome10 0.721MapTask 0.740Table 8: ?
measures of agreementa dialogue system may not have them at its disposalwhen doing DA classification in real time.
How-ever, this problem may be circumvented.
The num-ber of different games is in general rather low (8 inCallHome Spanish, 6 in MapTask), and the gamelabel is constant across DAs belonging to the samegame.
Each DA can be classified by augmenting itwith each possible game label, and by choosing themost accurate match among those returned by eachof these classification attempts.
Further, if the sys-tem can reliably recognize the end of a game, themethod just described needs to be used only for thefirst DA of each game.
Then, the game label thatgives the best result becomes the game label usedfor the next few DAs, until the end of the currentgame is detected.Another reason why we advocate FLSA overother approaches is that it appears to be close to hu-man performance for DA classification, in the sameway that LSA approximates well many aspects ofhuman competence / performance (Landauer andDumais, 1997).To support this claim, first, we used the ?
coef-ficient (Krippendorff, 1980; Carletta, 1996) to as-sess the agreement between the classification madeby FLSA and the classification from the corpora ?see Table 8.
A general rule of thumb on how tointerpret the values of ?
(Krippendorff, 1980) is torequire a value of ?
?
0.8, with 0.67 < ?
< 0.8allowing tentative conclusions to be drawn.
As awhole, Table 8 shows that FLSA achieves a satis-fying level of agreement with human coders.
Toput Table 8 in perspective, note that expert humancoders achieved ?
= 0.83 on DA classification forMapTask, but also had available the speech source(Carletta et al, 1997).We also compared the confusion matrix from(Carletta et al, 1997) with the confusion matrixwe obtained for our best result on MapTask (FLSAusing Game + Speaker).
For humans, the largestsources of confusion are between: check and query-yn; instruct and clarify; and acknowledge, reply-yand ready.
Likewise, our FLSA method makes themost mistakes when distinguishing between instructand clarify; and acknowledge, reply-y, and ready.Instead it performs better than humans on distin-guishing check and query-yn.
Thus, most of thesources of confusion for humans are the same as forFLSA.Future work includes further investigating how toselect promising feature combinations, e.g.
by usinglogical regression.We are also exploring whether FLSA can be usedas the basis for semi-automatic annotation of dia-logue acts, to be incorporated into MUP, an annota-tion tool we have developed (Glass and Di Eugenio,2002).
The problem is that large corpora are nec-essary to train methods based on LSA.
This wouldseem to defeat the purpose of using FLSA as the ba-sis for semi-automatic dialogue annotation, since, totrain FLSA in a new domain, we would need a largehand annotated corpus to start with.
Co-training(Blum and Mitchell, 1998) may offer a solution tothis problem.
In co-training, two different classi-fiers are initially trained on a small set of annotateddata, by using different features.
Afterwards, eachclassifier is allowed to label some unlabelled data,and picks its most confidently predicted positive andnegative examples; this data is added to the anno-tated data.
The process repeats until the desired per-fomance is achieved.
In our scenario, we will ex-periment with training two different FLSA models,or one FLSA model and a different classifier, suchas a naive Bayes classifier, on a small portion of an-notated data that includes features like DAs, Game,etc.
We will then proceed as described on the unla-belled data.Finally, we have started applying FLSA to a dif-ferent problem, that of judging the coherence oftexts.
Whereas LSA has been already successfullyapplied to this task (Foltz et al, 1998), the issue iswhether FLSA could perform better by also takinginto account those features of a text that enhanceits coherence for humans, such as appropriate cuewords.AcknowledgmentsThis work is supported by grant N00014-00-1-0640 fromthe Office of Naval Research, and in part, by award0133123 from the National Science Foundation.
Thanksto Michael Glass for initially suggesting extending LSAwith features and to HCRC (University of Edinburgh) forsharing their annotated MapTask corpus.
The work wasperformed while the first author was at the University ofIllinois in Chicago.ReferencesAvrim Blum and Tom Mitchell.
1998.
Combin-ing labeled and unlabeled data with co-training.In COLT98, Proceedings of the Conference onComputational Learning Theory.Jean Carletta, Amy Isard, Stephen Isard, Jacque-line C. Kowtko, Gwyneth Doherty-Sneddon, andAnne H. Anderson.
1997.
The reliability of a di-alogue structure coding scheme.
ComputationalLingustics, 23(1):13?31.Jean Carletta.
1996.
Assessing agreement on clas-sification tasks: the Kappa statistic.
Computa-tional Linguistics, 22(2):249?254.Peter W. Foltz, Walter Kintsch, and Thomas K. Lan-dauer.
1998.
The measurement of textual coher-ence with Latent Semantic Analysis.
DiscourseProcesses, 25:285?308.Peter W. Foltz, Darrell Laham, and Thomas K.Landauer.
1999.
The intelligent essay assessor:Applications to educational technology.
Interac-tive Multimedia Electronic Journal of Computer-Enhanced Learning, 1(2).Michael Glass and Barbara Di Eugenio.
2002.MUP: The UIC standoff markup tool.
In TheThird SigDIAL Workshop on Discourse and Di-alogue, Philadelphia, PA, July.Michael Glass, Heena Raval, Barbara Di Eugenio,and Maarika Traat.
2002.
The DIAG-NLP dia-logues: coding manual.
Technical Report UIC-CS 02-03, University of Illinois - Chicago.Dharmendra Kanejiya, Arun Kumar, and SurendraPrasad.
2003.
Automatic Evaluation of Students?Answers using Syntactically Enhanced LSA.
InHLT-NAACL Workshop on Building EducationalApplications using Natural Language Process-ing, pages 53?60, Edmonton, Canada.Walter Kintsch.
2001.
Predication.
Cognitive Sci-ence, 25:173?202.Klaus Krippendorff.
1980.
Content Analysis: anIntroduction to its Methodology.
Sage Publica-tions, Beverly Hills, CA.T.
Lager and N. Zinovjeva.
1999.
Training a dia-logue act tagger with the ?-TBL system.
In TheThird Swedish Symposium on Multimodal Com-munication, Linko?ping University Natural Lan-guage Processing Laboratory (NLPLAB).Thomas K. Landauer and S.T.
Dumais.
1997.
Asolution to Plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104:211?240.Lori Levin, Ann Thyme?-Gobbel, Alon Lavie, KlausRies, and Klaus Zechner.
1998.
A discourse cod-ing scheme for conversational Spanish.
In Pro-ceedings ICSLP.J.
Ross Quinlan.
1993.
C4.5: Programs for Ma-chine Learning.
Morgan Kaufmann.Klaus Ries.
1999.
HMM and Neural NetworkBased Speech Act Detection.
In Proceedings ofICASSP 99, Phoenix, Arizona, March.Ken Samuel, Sandra Carberry, and K. Vijay-Shanker.
1998.
Dialogue act tagging withtransformation-based learning.
In ACL/COLING98, Proceedings of the 36th Annual Meeting ofthe Association for Computational Linguistics(joint with the 17th International Conference onComputational Linguistics), pages 1150?1156.John R. Searle.
1975.
Indirect Speech Acts.In P. Cole and J.L.
Morgan, editors, Syntaxand Semantics 3.
Speech Acts.
Academic Press.Reprinted in Pragmatics.
A Reader, Steven Daviseditor, Oxford University Press, 1991.Riccardo Serafin.
2003.
Feature Latent SemanticAnalysis for dialogue act interpretation.
Master?sthesis, University of Illinois - Chicago.A.
Stolcke, K. Ries, N. Coccaro, E. Shriberg,R.
Bates, D. Jurafsky, P. Taylor, R. Martin, C. VanEss-Dykema, and M. Meteer.
2000.
Dialogueact modeling for automatic tagging and recog-nition of conversational speech.
ComputationalLinguistics, 26(3):339?373.Douglas M. Towne.
1997.
Approximate reasoningtechniques for intelligent diagnostic instruction.International Journal of Artificial Intelligence inEducation.Peter Wiemer-Hastings.
2001.
Rules for syntax,vectors for semantics.
In CogSci01, Proceedingsof the Twenty-Third Annual Meeting of the Cog-nitive Science Society, Edinburgh, Scotland.
