Augment ing a Hidden Markov Modelfor Phrase-DependentWord TaggingJulian KupiecXEROX PALO ALTO RESEARCH CENTER3333 Coyote Hill RoadPalo Alto, CA 94304AbstractThe paper describes refinements hat are currently being investigated in a model for part-of-speech assign-ment to words in unrestricted text.
The model has the advantage that a pre-tagged training corpus is notrequired.
Words are represented by equivalence classes to reduce the number of parameters equired andprovide an essentially vocabulary-independent model.
State chains are used to model selective higher-orderconditioning in the model, which obviates the proliferation of parameters attendant inuniformly higher-ordermodels.
The structure of the state chains is based on both an analysis of errors and linguistic knowledge.Examples how how word dependency across phrases can be modeled.IntroductionThe determination of part-of-speech ategories for words is an important problem in language modeling,because both the syntactic and semantic roles of words depend on their part-of-speech ategory (henceforthsimply termed "category").
Application areas include speech recognition/synthesis andinformation retrieval.Several workers have addressed the problem of tagging text.
Methods have ranged from locally-operatingrules (Greene and Rubin, 1971), to statistical methods (Church, 1989; DeRose, 1988; Garside, Leech andSampson, 1987; Jelinek, 1985) and back-propagation (Benello, Mackie and Anderson, 1989; Nakamura ndShikano, 1989).The statistical methods can be described in terms of Markov models.
States in a model represent categories{cl...c=} (n is the number of different categories used).
In a first order model, Ci and Ci_l are randomvariables denoting the categories of the words at position i and (i - 1) in a text.
The transition probabilityP(Ci = cz \] Ci_~ = %) linking two states cz and cy, represents he probability of category cx followingcategory %.
A word at position i is represented bythe random variable Wi, which ranges over the vocabulary{w~ ...wv} (v is the number of words in the vocabulary).
State-dependent probabilities of the form P(Wi = Wa\] Ci = cz) represent the probability that word Wa is seen, given category c~.
For instance, the word "dog" canbe seen in the states noun and verb, and only has a non-zero probability in those states.
A word sequenceis considered as being generated from an underlying sequence of categories.
Of all the possible categorysequences from which a given word sequence can be generated, the one which maximizes the probabilityof the words is used.
The Viterbi algorithm (Viterbi, 1967) will find this category sequence.
The systemspreviously mentioned require a pre-tagged training corpus in order to collect word counts or to performback-propagation.
The Brown Corpus (Francis and Kucera, 1982) is a notable example of such a corpus,and is used by many of the systems cited above.An alternative approach taken by Jelinek, (Jelinek, 1985) is to view the training problem in terms of a"hidden" Markov model: that is, only the words of the training text are available, their correspondingcategories are not known.
In this situation, the Baum-Welch algorithm (Baum, 1972) can be used to estimatethe model parameters.
This has the great advantage ofeliminating the pre-tagged corpus.
It minimizes theresources required, facilitates experimentation with different word categories, and is easily adapted for usewith other languages.The work described here also makes use of a hidden Markov model.
One aim of the work is to investigate hequality and performance of models with minimal parameter descriptions.
In this regard, word equivalence92classes were used (Kupiec, 1989).
There it is assumed that the distribution of the use of a word depends onthe set of categories it can assume, and words are partitioned accordingly.
Thus the words "play" and "touch"are considered to behave identically, as members of the class noun-or-verb, and "clay" and "zinc"are membersof the class noun.
This partitioning drastically reduces the number of parameters equired in the model, andaids reliable stimation using moderate amounts of training data.
Equivalence classes {Eqvl ...Eqvm} replacethe words {wl...Wv} (m << v) and P(Eqvi I Ci) replace the parameters P(Wi I Ci).
In the 21 category modelreported in Kupiec (1989) only 129 equivalence classes were required to cover a 30,000 word dictionary.
Infact, the number of equivalence classes is essentially independent of the size of the dictionary, enabling newwords to be added without any modification to the model.Obviously, a trade-off is involved.
For example, "dog" is more likely to be a noun than a verb and "see"is more likely to be a verb than a noun.
However they are both members of the equivalence class noun-or-verb, and so are considered to behave identically.
It is then local word context (embodied in the transitionprobabilities) which must aid disambiguation f the word.
In practice, word context provides ignificantconstraint, so the trade-off appears to be a remarkably favorable one.The Basic Mode lThe development of the model was guided by evaluation against a simple basic model (much of the develop-ment of the model was prompted by an analysis of the errors in its hehaviour).
The basic model containedstates representing the following categories:DeterminerNoun SingularNoun PluralProper NounPronounAdverbConjunctionPrepositionAdjectiveVerb UninflectedVerb 3rd Pers.
Sing.AuxiliaryPresent ParticiplePast ParticipleQuestion WordUnknownLispTo-inf.Sentence BoundaryIncluding mass nounsCo-ordinating and subordinatingIncluding comparative and superlativeAm, is, was, has, have, should, must, can, might, etc.Including erundIncluding past tenseWhen, what, why, etc.Words whose stems could not be found in dictionary.Used to tag common symbols in the the Lisp programming language (see below:)"To" acting as an infinitive markerThe above states were arranged in a first-order, fully connected network, each state having a transitionto every other state, allowing all possible sequences of categories.
The training corpus was a collection ofelectronic mail messages concerning the design of the Common-Lisp rogramming language - a somewhatless than ideal representation f English.
Many Lisp-specific words were not in the vocabulary, and thustagged as unknown, however the lisp category was nevertheless created for frequently occurring Lisp symbolsin an attempt to reduce bias in the estimation.
It is interesting to note that the model performs very well,despite such "noisy" training data.
The training was sentence-based, and the model was trained using 6,000sentences from the corpus.
Eight iterations of the Baum-Welch algorithm were used.The implementation f the hidden Markov model is based on that of Rabiner, Levinson and Sondhi (1983).By exploiting the fact that the matrix of probabilities P(Eqvi I Ci) is sparse, a considerable improvement canbe gained over the basic training algorithm in which iterations are made over all states.
The initial valuesof the model parameters are calculated from word occurrence probabilities, uch that words are initially9\]assumed to function equally probably as any of their possible categories.
Superlative and comparativeadjectives were collapsed into a single adjective category, to economize on the overall number of categories.
(If desired, after tagging the finer category can be replaced).
In the basic model all punctuation exceptsentence boundaries was ignored.
An interesting observation is worth noting with regard to words that canact both as auxiliary and main verbs.
Modal auxiliaries were consistently tagged as auxiliary whereas thetagging for other auxiliaries (e.g.
"is .... have" etc.)
was more variable.
This indicates that modal auxiliariescan be recognized as a natural class via their pattern of usage.Extending the Basic ModelThe basic model was used as a benchmark for successive improvements.
The first addition was the correcttreatment of all non-words in a text.
This includes hyphenation, punctuation, umbers and abbreviations.New categories were added for number, abbreviation, and comma.
All other punctuation was collapsed intothe single new punctuation category.Re f inement  o f  Bas ic  Categor iesThe verb states of the basic model were found to be too coarse.
For example, many noun/verb ambiguitiesin front of past participles were incorrectly tagged as verbs.
The replacement of the auxiliary category bythe following categories greatly improved this:Category Name Words included in CategoryBe beBeen beenBeing beingHave haveHave* has, have, had, havingbe* is, am, are, was, weredo* do, does, didmodal Modal auxiliariesUn ique  Equ iva lence  C lasses  for  Common WordsCommon words occur often enough to be estimated reliably.
In a ranked list of words in the corpus themost frequent 100 words account for approximately 50% of the total tokens in the corpus, and thus data isavailable to estimate them reliably.
The most frequent 100 words of the corpus were assigned individuallyin the model, thereby enabling them to have different distributions over their categories.
This leaves 50% ofthe corpus for training all the other equivalence classes.Ed i t ing  the  Trans i t ion  S t ruc tureA common error in the basic model was the assignment of the word "to" to the to-infcategory ("to" actingas an infinitive marker) instead of preposition before noun phrases.
This is not surprising, because "to" isthe only member of the to-inf category, P(Wi = "to" \[ Ci = to-in\]) = 1.0.
In contrast, P(Wi = "to" I Ci =preposition) = 0.086, because many other words share the preposition state.
Unless transition probabilitiesare highly constraining, the higher probability paths will tend to go through the to-infstate.
This situationmay be addressed in several ways, the simplest being to initially assign zero transition probabilities from theto-infstate to states other than verbs and the adverb state.94ADJECTIVEDETERMINERNOUNTo all statesin Basic Network"Transitions toall states inBasic Networkexcept NOUNand ADJECT IVE?
To all statesin Basic NetworkAUGMENTED NETWORKBASIC NETWORKFULLY-CONNECTED NETWORKCONTAIN ING ALL  STATESEXCEPT  DETERMINERFigure 1: Extending the Basic ModelAugment ing the Model  by Use of NetworksThe basic model consists of a first-order fully connected network.
The lexical context available for modelinga word's category is solely the category of the preceding word (expressed via the transition probabilitiesP(Ci \[ Ci-1).
Such limited context does not adequately model the constraint present in local word context.A straightforward method of extending the context is to use second-order conditioning which takes accountof the previous two word categories.
Transition probabilities are then of the form P(Ci \[ Ci-1, Ci-2).
For ann category model this requires n 3 transition probabilities.
Increasing the order of the conditioning requiresexponentially more parameters.
In practice, models have been limited to second-order, and smoothingmethods are normally required to deal with the problem of estimation with limited data.
The conditioningjust described is uniform- all possible two-category contexts are modeled.
Many of these neither contribute tothe performance of the model, nor occur frequently enough to be estimated properly: e.g.
P(Ci = determiner\[ e l -1  -~ determiner, Ci-2 = determiner).An alternative to uniformly increasing the order of the conditioning is to extend it selectively.
Mixed higher-order context can be modeled by introducing explicit state sequences.
In the arrangement the basic first-ordernetwork remains, permitting all possible category sequences, and modeling first-order dependency.
The basicnetwork is then augmented with the extra state sequences which model certain category sequences in moredetail.
The design of the augmented network has been based on linguistic considerations and also upon ananalysis of tagging errors made by the basic network.As an example, we may consider a systematic error made by the basic model.
It concerns the disambiguationof the equivalence class adjective-or-noun following a determiner.
The error is exemplified by the sentencefragment "The period of...", where "period" is tagged as an adjective.
To model the context necessary tocorrect he error, two extra states are used, as shown in Figure 1.
The "augmented network" uniquely modelsall second-order dependencies of the type determiner - noun - X, and determiner - adjective - X (X rangesover {cl...cn}).
Training a hidden Markov model having this topology corrected all nine instances of theerror in the test data.
An important point to note is that improving the model detail in this manner doesnot forcibly correct he error.
The actual patterns of category usage must be distinct in the language.95To complete the description of the augmented model it is necessary to mention tying of the model states(Jelinek and Mercer, 1980).
Whenever a transition is made to a state, the state-dependent probabilitydistribution P(Eqvi I Ci) is used to obtain the probability of the observed equivalence class.
A state isgenerally used in several places (E.g.
in Figure 1. there are two noun states, and two adjective states: oneof each in the augmented network, and in the basic network).
The distributions P(Eqvi I Ci) are consideredto be the same for every instance of the same state.
Their estimates are pooled and re-assigned i enticallyafter each iteration of the Baum-Welch algorithm.Modeling Dependencies across  PhrasesLinguistic onsiderations can be used to correct errors made by the model.
In this section two illustrationsare given, concerning simple subject/verb agreement across an intermediate prepositional phrase.
These areexemplified by the following sentence fragments:1.
"Temperatures in the upper mantle range apparently from....".2.
"The velocity of the seismic waves rises to...".The basic model tagged these sentences correctly, except for- "range" and "rises" which were tagged asnoun and plural-noun respectively 1.
The basic network cannot model the dependency of the number ofthe verb on its subject, which precedes it by a prepositional phrase.
To model such dependency across thephrase, the networks hown in Figure 2 can be used.
It can be seen that only simple forms of prepositionalphrase are modeled in the networks; a single noun may be optionally preceded by a single adjective and/ordeterminer.
The final transitions in the networks erve to discriminate between the correct and incorrectcategory assignment given the selected preceding context.
As in the previous ection, the corrections are notprogrammed into the model.
Only context has been supplied to aid the training procedure, and the latter isresponsible for deciding which alternative is more likely, based on the training data.
(Approximately 19,000sentences were used to train the networks used in this example).Discuss ion and Resu l tsIn Figure 2, the two copies of the prepositional phrase are trained in separate contexts (preceding singu-lax/plural nouns).
This has the disadvantage that they cannot share training data.
This problem couldbe resolved by tying corresponding transitions together.
Alternatively, investigation of a trainable gram-mar (Baker, 1979; Fujisaki et al, 1989) may be a fruitful way to further develop the model in terms ofgrammatical components.A model containing all of the refinements described, was tested using a magazine article containing 146sentences (3,822 words).
A 30,000 word dictionary was used, supplemented byinflectional analysis for wordsnot found directly in the dictionary.
In the document, 142 words were tagged as unknown (their possiblecategories were not known).
A total of 1,526 words had ambiguous categories (i.e.
40% of the document).Critical examination of the tagging provided by the augmented model showed 168 word tagging errors,whereas the basic model gave 215 erroneous word tags.
The former represents 95.6% correct word taggingon the text as a whole (ignoring unknown words), and 89% on the ambiguous words.
The performance of atagging program depends on the choice and number of categories used, and the correct ag assignment forwords is not always obvious.
In cases where the choice of tag was unclear (as often occurs in idioms), thetag was ruled as incorrect.
For example, 9 errors are from 3 instances of "... as well as ..." that arise inthe text.
It would be appropriate to deal with idioms separately, as done by Gaxside, Leech and Sampson(1987).
Typical errors beyond the scope of the model described here are exemplified by incorrect adverbialand prepositional ssignment.1 It is easy to construct counter-examples to the sentences presented here, where the tagging would be correct.
However, thetraining procedure affirms that counter-examples occur less frequently in the corpus than the cases shown here.96NOUNPREPOSITION ADJECTIVE NO U N ~PLURAL NOUNPLURAL NOUNPREPOSITION A E?TIVE NO2NJC)NOUN~ j VERBTRANSITIONS TO/FROM ~ 3RD.
SINGULARALL STATES INBASIC NETWORKNOT SHOWNFigure 2: Augmented Networks for Example of Subject/Verb AgreementFor example, consider the word "up" in the following sentences:"He ran up a big bill".
"He ran up a big hill".Extra information is required to assign the correct agging.
In these xamples it is worth noting that even if amodel was based on individual words, and trained on a pre-tagged corpus, the association of "up" (as adverb)with "bill" would not be captured by trigrams.
(Work on phrasal verbs, using mutual information estimates(Church et ai., 1989b) is directly relevant to this problem).
The tagger could be extended by further categoryrefinements (e.g.
inclusion of a gerund category), and the single pronoun category currently causes erroneoustags for adjacent words.
With respect o the problem of unknown words, alternative category assignmentsfor them could be made by using the context embodied in transition probabilities.ConclusionsA stochastic method for assigning part-of-speech ategories to unrestricted English text has been described.It minimizes the resources required for high performance automatic tagging.
A pre-tagged training corpus isnot required, and the tagger can cope with words not found in the training text.
It can be trained reliablyon moderate amounts of training text, and through the use of selectively augmented networks it can modelhigh-order dependencies without requiring an excessive number of parameters.AcknowledgementsI would like to thank Meg Withgott and Lanri Karttunen of Xerox PARC, for their helpful contributionsto this work.
I am also indebted to Sheldon Nicholl of the Univ.
of Illinois, for his comments and valuableinsight.
This work was sponsored in part by the Defense Advanced Research Projects Agency (DOD), underthe Information Science and Technology Office, contract #N00140-86-C-8996.97ReferencesJ.K.
Baker.
Trainable Grammars for Speech Recognition.
Speech Communications Paper.
97th.
Meeting ofAcoustical Soc.
of America, Cambridge, MA, 1979.L.E.
Banm.
An Inequality and Associated Maximization Technique in Statistical Estimation for ProbabilisticFunctions of a Markov Process.
Inequalities, 3, 1972. pp.
1-8.J.
Benello, A. Mackie, J. Anderson.
Syntactic Category Disambiguation with Neural Networks.
Computer Speechand Language, Vol.
3, No.
3, July 1989. pp.
203-217.K.
Church.
A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text.
Proc.
Int.
Conf.
onAcoustics, Speech and Signal Processing, 1989. pp.
695-698.
(1989b) K. Church, W. Gale, P. Hanks, D. Hindle.
Parsing, Word Associations and Typical Predicate-ArgumentRelations.
Proc.
Int.
Workshop on Parsing Technologies, Pittsburgh PA, Aug. 28-31 1989. pp.
389-398.S.
DeRose.
Grammatical Category Disambiguation by Statistical Optimization.
Computational Linguistics, Vol.14, No 1.
1988.W.N.
Francis, H. Kucera.
Frequency Analysis of English Usage.
Houghton Mifflin, 1982.T.
Fujisaki, F. Jelinek, J. Cocke, E. Black, T. Nishino.
A Probabilistic Method for Sentence Disambiguation.
Proc.Int.
Workshop on Parsing Technologies, Pittsburgh PA, Aug. 28-31 1989. pp.
85-94.R.
Garside, G. Leech, G. Sampson.
The Computational Analysis of English.
Longman, 1987.B.B.
Greene, G.M.
Rubin.
Automatic Grammatical Tagging of English.
Dept.
of Linguistics, Brown Univ.,Providence.
1971.F.
Jelinek.
Self-Organized Language Modeling for Speech Recognition.
Unpublished Technical Report, 1985.
IBMT.J.
Watson Research Center, Yorktown Heights, N.Y.F.
Jelinek, R.L.
Mercer.
Interpolated Estimation of Markov Source Parameters from Sparse Data.
Proc.
WorkshopPattern Recognition in Practice, May 21-23 1980.
Amsterdam, The Netherlands.
North-Holland.J.
Kupiec.
Probabilistic Models of Short and Long Distance Word Dependencies in Running Text.
Proc.
DARPASpeech and Natural Language Workshop, Philadelphia, Feb. 21-23 1989. pp.
290-295.M.
Nakamura, K. Shikano.
A Study of English Word Category Prediction Based on Neural Networks.
Proc.
Int.Conf.
on Acoustics, Speech and Signal Processing, 1989. pp.
731-734.L.R.
Rabiner, S.E.
Levinson, and M.M.
Sondhi.
An Introduction to the Application of the Theory of ProbabilisticFunctions of a Markov Process to Automatic Speech Recognition.
Bell System Technical Journal, Vol.
62, No.4, April 1983. pp 1035-1074.A.J.
Viterbi.
Error Bounds for Convolutional Codes and an Asymptotically Optimal Decoding Algorithm.
IEEETrans.
on Information Theory Vol.
IT-13, April 1967. pp.
260-269.98
