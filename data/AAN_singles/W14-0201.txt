Proceedings of the of the EACL 2014 Workshop on Dialogue in Motion (DM), pages 1?9,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsIn-Car Multi-Domain Spoken Dialogs: A Wizard of Oz StudySven Reichel, Ute Ehrlich, Andr?e BertonSpeech Dialogue SystemsDaimler AGUlm, Germany{sven.reichel, ute.ehrlich,andre.berton}@daimler.comMichael WeberInstitute of Media InformaticsUlm UniversityGermanymichael.weber@uni-ulm.deAbstractMobile Internet access via smartphonesputs demands on in-car infotainment sys-tems, as more and more drivers like to ac-cess the Internet while driving.
Spokendialog systems support the user by lessdistracting interaction than visual/haptic-based dialog systems.
To develop an intu-itive and usable spoken dialog system, anextensive analysis of the interaction con-cept is necessary.
We conducted a Wizardof Oz study to investigate how users willcarry out tasks which involve multiple ap-plications in a speech-only, user-initiativeinfotainment system while driving.
Re-sults show that users are not aware of dif-ferent applications and use anaphoric ex-pressions in task switches.
Speaking stylesvary and depend on type of task and di-alog state.
Users interact efficiently andprovide multiple semantic concepts in oneutterance.
This sets high demands for fu-ture spoken dialog systems.1 IntroductionThe acceptance of smartphones is a success story.These devices allow people to access the Internetnearly anywhere at anytime.
While driving, usinga smartphone is prohibited in many countries as itdistracts the driver.
Regardless of this prohibition,people use their smartphone and cause severe in-juries (National Highway Traffic Safety Adminis-tration (NHTSA), 2013).
In order to reduce driverdistraction, it is necessary to integrate the smart-phone?s functionality safely into in-car infotain-ment systems.
Since hands and eyes are involvedin driving, a natural and intuitive speech-based in-terface increases road safety (Maciej and Vollrath,2009).
There are already infotainment systemswith Internet applications like e.g.
weather, musicstreaming, gas prices, news, and restaurant search.However, not all of them can be controlled by nat-ural speech.In systems based on graphic and haptic modal-ity, the functionality is often grouped into variousapplications.
Among other things, this is due tothe limited screen size.
The user has to start anapplication and select the desired functionality.
Anatural speech interface does not require a frag-mentation of functionalities into applications, aspeople can express complex commands by speech.In single-application tasks, such as calling some-one, natural speech interfaces are established andproven.
However, users often encounter complextasks, which involve more than one application.For example, while hearing the news about a newmusic album, the driver might like to start listen-ing to this album via Internet radio.
Spoken lan-guage allows humans to express a request suchas ?Play this album?
easily, since the meaning isclear.
However, will drivers also use this kind ofinteraction while using an in-car spoken dialogsystem (SDS)?
Or is the mental model of ap-plication interaction schema dominant in human-computer interaction?
In a user experiment, weconfront drivers with multi-domain tasks, to ob-serve how they interact.While interacting with an SDS, one crucialproblem for users is to know which utterances thesystem is able to understand.
People use differentapproaches to solve this problem, for example byreading the manual, using on-screen help, or rely-ing on their experiences.
In multi-domain dialogsystems, utterances can be quite complex, thus re-membering all utterances from the manual or dis-playing them on screen would not be possible.
Asa result, users have to rely on their experience incommunications to know what to say.
Thus, anadvanced SDS needs to understand what a userwould naturally say in this situation to execute acertain task.1In this paper, we present results from a Wizardof Oz (WoZ) experiment on multi-domain in-teraction with an in-car SDS.
The goal of thisstudy is to build a corpus and analyze it ac-cording to application awareness, speaking styles,anaphoric references, and efficiency.
Our resultsprovide a detailed insight how drivers start multi-application tasks and switch between applicationsby speech.
This will answer the question whetherthey are primed to application-based-interactionor use a natural approach known from human-human-communication.
The results will be usedto design grammars or language models for work-ing prototypes, which establish a basis for realuser tests.
Furthermore, we provide guidelines formulti-domain SDSs.The remainder of this paper is structured as fol-lows: Section 2 provides an overview of otherstudies in this context.
Section 3 describes the do-main for the user experiment which is presentedin Section 4.
Data analysis methods are definedin Section 5.
We present the results in Section 6and discuss them in Section 7.
Finally, we con-clude and give guidelines for multi-domain SDSsin Section 8.2 Related WorkMany studies exist which evaluate SDSs concern-ing performance, usability, and driver distraction(a good overview provides Ei-Wen Lo and Green(2013)).
Usually, participants are asked to com-plete a task, while driving in a simulated envi-ronment or in real traffic.
Geutner et al.
(2002),for example, showed that a virtual co-driver con-tributes to ease of use with little distraction effects.In their WoZ experiment, natural language waspreferred to command-and-control input.
How-ever, no in-depth analysis of user utterances is pre-sented.
Cheng et al.
(2004) performed an analy-sis of natural user utterances.
They observed thatdrivers, occupied in a driving task, use disfluentand distracted speech and react differently than byconcentrating on the speech interaction task.
Noneof the studies provide in-depth analysis of multi-domain tasks, as our work does.Multi-domain SDS exist like e.g.
SmartKom(Reithinger et al., 2003) or CHAT (Weng et al.,2007).
They presented complex systems withmany functionalities, however, they do not eval-uate subtask switching from users?
point of view.In CHAT, the implicit application switch was evendisabled due to ?extra burden on the system?.
Do-main switches are analyzed in human-human com-munication as e.g.
in Villing et al.
(2008).
How-ever, people interact differently with a system thanwith a human.
Even in human-computer commu-nication, speaking styles differ depending on typeof task, as (Hofmann et al., 2012) showed in aweb-based user study.
In order to develop an intu-itive multi-application SDS, it is necessary to an-alyze how users interact in a driving situation bycompleting tasks across different domains.3 User TasksIn a user experiment it is crucial to set real tasksfor users, since artificial tasks will be hard to re-member and can reduce their attention.
We ana-lyzed current in-car infotainment systems with In-ternet access and derived eight multi-domain tasksfrom their functionality (see Table 1).
The sub-tasks were classified according to Kellar et al.
(2006)?s web information classification schema ininformation seeking (Inf), information exchange,and information maintenance.
Since informationmaintenance is not a strong automotive use case,these tasks were grouped together with informa-tion exchange.
We call them action subtasks (Act)as they initiate an action of the infotainment sys-tem (e.g.
?turn on the radio?
).No App 1 App 2 App31 POI Search Restaurant Call2 Knowledge Ski Weather Navigation3 Weather Hotel Search Address book4 Play Artist News Search Forward by eMail5 Navigation Restaurant Save as Favorite6 News Search Play Artist Share on Facebook7 News Search Knowledge Convert Currency8 Navigation Gas Prices Status Gas TankTable 1: Multi-application user tasks.Since only few use cases involve more thanthree applications, every user task is a story ofthree subtasks.
In task number 5 for example, auser has to start a subtask, which navigates himto Berlin.
Then he would like to search an Italianrestaurant at the destination.
Finally, he adds theselected restaurant to his favorites.
The focus is ontask entry and on subtask switch, thus the subtasksrequire only two to four semantic concepts (likeBerlin or Italian restaurant).
One of these con-cepts is a reference to the previous subtask (likeat the destination or the selected restaurant) to en-sure a natural cross-application dialog flow.
Afterthe system?s response for one subtask the user hasto initiate the next subtask to complete his task.24 User ExperimentDeveloping an SDS means specifying a grammaror training statistical language models for speechrecognition.
These steps precede any real user test.In system-initiated dialogs, with a few possible ut-terances, specifying a grammar is feasible.
How-ever, in strictly user-initiative dialogs with mul-tiple applications, this is rather complicated.
AWoZ study does not require to develop speechrecognition and understanding as this is performedby a human.
Analyzing the user utterances of aWoZ experiment provides a detailed view of howa user will interact with the SDS.
This helps in de-signing spoken dialogs and specifying grammarsand/or training language models for further eval-uations (Fraser and Gilbert, 1991; Glass et al.,2000).Interaction schemes of people vary among eachother and depend on age, personality, experience,context, and many more.
It is essential to con-duct a user study with people who might use theSDS later on.
A study by the NHTSA (NationalHighway Traffic Safety Administration (NHTSA),2013) showed that in 2011 73% of the drivers in-volved in fatal crashes due to cell phone use, wereless than 40 years old.
For this reason, our studyconsiders drivers between 18 and 40 years whoare technically affine and are likely to buy a carequipped with an infotainment system with Inter-net access.4.1 Experimental Set-UpWhen designing a user interaction experiment, itis important that it takes place in a real environ-ment.
As driving on a real road is dangerous, weused a fixed-base driving simulator in a laboratory.In front of the car, a screen covers the driver?s fieldof view (see Figure 1).
Steering and pedal signalsare picked from the car?s CAN bus.
It is impor-tant that the user assumes he is interacting witha computer as ?human-human interactions are notthe same as human-computer interactions?
(Fraserand Gilbert, 1991).
The wizard, a person in chargeof the experiment, was located behind the car andmouse clicks or any other interaction of the wizardwas not audible in the car.
To ensure a consistentbehavior of the wizard, we used SUEDE (Klem-mer et al., 2000) to define the dialog, which alsoprovides an interface for the wizard.
SUEDE de-fines a dialog in a state machine, in which the sys-tem prompts are states and user inputs are edgesbetween them.
The content of system prompts wassynthesized with NUANCE Vocalizer Expressive1version 1.2.1 (Voice: anna.full).
During the ex-periment, after each user input the wizards clicksthe corresponding edge and SUEDE plays the nextprompt.
All user utterances are recorded as audiofiles.Figure 1: Experimental Set-Up4.2 Experiment DesignInfotainment systems in cars are used while driv-ing.
This means the user cannot concentrate on theinfotainment system only, but also has to focus onthe road.
According to multiple resource theory,the human?s performance is reduced when humanresources overlap (Wickens, 2008).
In a dual-taskscenario, like using the infotainment system whiledriving, multiple resources are allocated and mayinterfere.
Considering this issue, we use a drivingtask to keep the participants occupied while theyinteract with the SDS.
This allows us to observeuser utterances in a stressful situation.Infotainment systems in cars are often equippedwith large displays providing visual and hapticinteraction.
These kinds of interaction competefor human resources which are needed for driv-ing.
This results in driver distraction, especiallyin demanding secondary tasks (Young and Regan,2007).
Furthermore, a visual interface can also in-fluence the communication of users (e.g.
they ut-ter visual terms).
As we intent to study how a userinteracts naturally with a multi-domain SDS, weavoid priming effects by not using any visual in-terface.4.2.1 Primary Task: Driving SimulatorOne major requirement for the driving task is tokeep the driver occupied at a constant level allthe time.
Otherwise, we would not be able toanalyze user utterances on a fine-grained level.1http://www.nuance.com/for-business/mobile-solutions/vocalizer-expressive/index.htm3Therefore, we used the Continuous Tracking andReaction (ConTRe) task (Mahr et al., 2012) whichallows controlled driving conditions.
It consists ofa steering and a reaction task, which require oper-ating the steering wheel and pedals.
In the steeringtask, a yellow cylinder moves unpredictable rightand left at a constant distance from the driver andthe driver must always steer towards it.
This issimilar to driving on a curved road.
Sometimes adriver needs to react to sudden events to prevent anaccident.
For this a traffic light shows randomlyred and green and requires the driver to push thethrottle or brake pedal.
The movement of the yel-low cylinder and the appearance of the stop lightcan be controlled by manipulating control vari-ables.
The ?hard driving setting?
from Mahr et al.
(2012) was used in this study.4.2.2 Secondary Task: cross application taskswith speech interactionAs described in Section 3, a task consists of threesubtasks and each subtask requires two to four se-mantic concepts.
For a user it is possible to insertmultiple concepts at once:U: ?Search an Italian restaurant at the destination?or as single utterances in a dialog:U: ?Search an Italian restaurant?S: ?Where do you search an Italian restaurant?
?U: ?At my destination?For all possible combinations prompts were speci-fied.
SUEDE provides a GUI for the wizard to se-lect which semantic concept a user input contains.Dependent on the selection, either another conceptis requested or the answer is provided.
Further-more, a user input can optionally contain a verbexpressing what the system should do.
For exam-ple, if users say ?Italian Restaurant?
the reactionis the same as they would say ?Search an Italianrestaurant?.The user has basically two options to select orswitch to an application.
Either an explicit selec-tion such as:U: ?Open restaurant application?S: ?Restaurant, what do you want?
?or an implicit selection such as:U: ?Search an Italian restaurant?By using an explicit selection, users assume theyhave to set the context to a specific application.After that, they can use the functionality of thisapplication.
This is a common interaction schemafor visual-based infotainment systems or smart-phones, as they cluster their functionality into var-ious applications.
An implicit selection is ratherlike current personal assistants interact, as they donot cluster their functionality.
Implicit selectionfacilitates the interaction for users since they canget an answer right away.
After the user providedthe necessary input for one subtask, the system re-sponds for example:S: ?There is one Italian restaurant: Pizzeria San Marco.
?Then the user needs to initiate an applicationswitch to proceed with his task.A system enabling user-initiated dialogs cannotalways understand the user correctly.
Especially inimplicit selection, the language models increase,and thus recognition as well as understanding iserror prone (Carstensen et al., 2010).
Further-more, the user could request a functionality whichis not supported by the system.
Therefore, errorhandling strategies need to be applied.
In termsof miscommunication, it can be distinguished be-tween misunderstanding and non-understanding(Skantze, 2007).
In the experiment, two of ourtasks do not support an implicit application switch,but require an explicit switch.
So if users try toswitch implicitly, the system will not understandtheir input in one task and will misinterpret it inthe other task.
A response to misunderstandingmight look like:U: ?Search an Italian restaurant?S: ?In an Italian restaurant you can eat pizza?A non-understanding informs the user and encour-ages him to try another request:S: ?Action unknown, please change your request?These two responses are used until the userchanges his strategy to explicit selection.
If thatdoes not happen, the task is aborted by the wizardif the user gets too frustrated.
This enables us toanalyze whether users will switch their strategy ornot and how many turns it will take.4.3 ProcedureThe experiment starts with an initial questionnaireto create a profile of the participant, concerningage, experience with smartphones, infotainmentsystems and SDSs.
Then participants are intro-duced to the driving task and they have time topractice till being experienced.
After complet-ing a baseline drive, they start to use the SDS.For each spoken dialog task users get a story de-scribing in prose what they like to achieve withthe system.
To minimize priming effects, theyhave to remember their task and are not allowed tokeep the description during the interaction.
There4is no explanation or example of the SDS, apartfrom a start command for activation.
After thestart command, the system plays a beep and theuser can say whatever he likes to achieve histask.
The exploration phase consists of four tasks,in which users can switch applications implic-itly and explicitly.
Then they rate the usabilityof the system with the questionnaire: SubjectiveAssessment of Speech System Interfaces (SASSI)(Hone and Graham, 2000).
In the second part ofthe experiment, four tasks with different interac-tion schemes for application switches are com-pleted randomly: implicit & explicit switch pos-sible, misunderstanding, non-understanding, anddialog-initiative change.5 Dialog Data AnalysisAll audio files of user utterances were transcribedand manually annotated by one person concerningthe application selection/switch, speaking style,anaphoric references, and semantic concepts.First of all, for each application entry and switchit was classified whether the participant used animplicit or explicit utterance.
Additionally, thenon-understanding and misunderstanding data setswere marked whether the dialog strategy waschanged and how many dialog turns this took.Since most of the user utterances were implicitones (see Section 6.1), we classified them fur-ther into different speaking styles.
In the dataset of implicit utterances, five different speakingstyles could be identified.
Table 2 shows themwith an example.
The illocutionary speech act tosearch a hotel is always the same, but how usersexpress their request varies.
Keyword style andexplicit demand is rather how we expect peopleto speak with machines, as these communicationforms are short commands and might be regardedas impolite between humans.
Kinder and gentlercommunications forms are implicit demands, Wh-questions, and Yes-No-Questions.
This is how wewould expect people to interact with each other.Keyword Style ?Restaurant search.
Berlin?Implicit Demand ?I?d like to search a restaurant inBerlin.
?Wh-Question ?Which restaurants are in Berlin?
?Yes-No-Question ?Are there any restaurants in Berlin?
?Explicit Demand ?Search restaurants in Berlin?Table 2: Speaking styles of user utterances.Two applications are always linked with a com-mon semantic concept.
The user has to refer tothis concept which he can do in various ways withanaphoric expressions.
The annotation of the dataset is based on Fromkin et al.
(2003) and shownin Table 3 (Examples are user utterances in re-sponse to the system prompt ?Navigation to Berlinstarted?).
In an elliptic anaphoric reference theconcept is not spoken, but still understood becauseof context - also called gapping.
Furthermore,pronominalization can be used as an anaphor.
Wedistinguish between a pronoun or adverb anaphorand an anaphor with a definite noun phrase, sincethe later contains the type of semantic concept.Another way is simply to rephrase the semanticconcept.Elliptic ?Search restaurants.
?Pronoun, Adverb ?Search restaurants there.
?Definite Noun Phrase ?Search restaurants in this city.
?Rephrase ?Search restaurants in Berlin.
?Table 3: Anaphoric reference types.6 ResultsIn the following, results on application awareness,speaking style, anaphoric expressions, efficiency,and usability are presented.
We analyzed datafrom 31 participants (16m/15f), with average ageof 26.65 (SD: 3.32).
26 people possess and use asmartphone on a regular basis and 25 of them areused to application-based interaction (18 peopleuse 1-5 apps and 7 people use 6-10 apps each day).Their experience with SDS is little (6-Likert Scale,avg: 3.06, SD: 1.48) as well as the usage of SDSs(5-Likert Scale, avg: 2.04, SD: 1.16).
We askedthem how they usually approach a new system orapp to learn its interaction schema and scope ofoperation.
On the smartphone, all 31 of them trya new app without informing themselves how it isused.
Concerning infotainment systems, trying isalso the most used learning approach, even whiledriving (26 people).
This means, people do notread a manual, but the system has to be naturallyusable.In total, we built a corpus of interactions with 5h25min with 3h 08min of user speech.
It contains243 task entries and 444 subtask switches.
Due todata loss 5 task entries could not be analyzed.
Sub-task switches were less than theoretically possible,because misunderstanding and non-understandingtasks were aborted by the wizard if the user didnot change his strategy.
Concerning the type ofsubtask, we analyzed 91 action and 152 informa-tion seeking subtasks for task entries, as well as5236 actions and 208 information seekings for taskswitches.6.1 Application AwarenessThe SDS was designed to be strictly user-initiative: after a beep users could say whateverthey liked.
We counted 4.9% of user utterances asexplicit entries to start a task, which means usersin general assume either the SDS is already in theright application context or it is not based on dif-ferent applications.
This is an interaction schemawhich would rather be used with a human com-munication partner.
1.1% explicit utterances insubtask switches reinforce this assumption.
Utter-ances addressing more than one application couldnot be observed.Furthermore, we analyzed whether userschange their strategy from implicit to explicitsubtask switch if the system does not react asexpected.
The implicit switch was preventedand the system answered as if a misunderstand-ing or a non-understanding has occurred.
Ta-ble 4 shows results for the number of subtaskswitches (subt.
sw.), number of successful strategychanges (succ.
), and average number of user utter-ances (avg.
UDT) till the strategy was changed.In total, only in 43.7% subtask switches userschanged their strategy.
The difference betweennon-understanding and misunderstanding was notsignificant (p=0.051), however, this might due tosmall sample size.subt.
sw. succ.
avg.
UDTnon-underst.
42 15 2.93 (SD=1.91)misunderst.
45 23 3.74 (SD=1.79)Table 4: Dialog repair changes to explicit strategy.In summary, only 6% of user utterances ad-dressed the application explicitly and only 43.7%of users changed their strategy from implicit to ex-plicit.
These results reveal that most users are notaware of different applications or do not addressapplications differently in a speech-only infotain-ment system.
They interact rather like with a hu-man being or with a personal assistant than with atypical in-car SDS.6.2 Speaking styles of implicit applicationselectionEven if people interact without being aware of dif-ferent applications, they might speak to a systemin another way than to a human.
We analyzedthe implicit user utterances according to differentspeaking styles (see Figure 2).
Overall, explicitdemand dominates with 37.07% for task entry and42.42% for subtask switching.
Keyword style isused in 16.16% for task entry and 9.29% for sub-task switches.
As mentioned, explicit demand andkeyword style are rather used in human-computerinteraction.
Here, slightly more than half of theparticipants (entry: 53.23%; switch: 51.71%) usethis kind of interaction.
The other half interactsin kinder and gentler forms known from human-human communication.Comparing task entry and subtask switch, dif-ferences could be found in keyword style, implicitdemand, and Yes-No-Question.
In the first contactwith the system, users might be unsure what it iscapable of, therefore, often keywords were usedto find out how the system reacts.
Additionally,the task description was formulated in implicit de-mand style, thus an unsure user might rememberthis sentence and use it.
Concerning the Yes-No-Questions, they might be a reaction to the naturallyformulated system prompts, thus the user adapts toa human-human-like communication style.Finally, we compare information seeking sub-tasks with action subtasks.
In action subtasks, im-plicit and explicit demand style dominate.
Thisis reasonable, as people give commands in eitherform and expect a system reaction.
Likewise, itwas anticipated that question styles are used for in-formation seeking.
One interesting finding is thatkeyword style is more often used in informationseeking.
This could be due to priming effects ofusing search engines like Google2, in which usersonly insert the terms they are interested in andGoogle provides the most likely answers.In summary, speaking styles vary.
Sometimesthe system is considered as a human-like commu-nication partner and sometimes users try to reachtheir goal as fast as possible by giving short com-mands.
However, speaking styles depend on thetype of subtask and dialog state.6.3 Anaphoric ExpressionsIn a cross-application task, it is of interest howusers refer to application-linking semantic con-cepts.
Figure 3 shows which kind of anaphoric ex-pressions were used in implicit utterances.
Nearlyhalf of the utterances (47.68%) contain a rephraseof the semantic concept and further 31.57% a def-2www.google.de65,95%1,08%17,26%9,27%0,00% 3,23% 0,60% 3,45%26,19%32,76%10,20%8,21%5,78%1,93% 22,11%22,71%1,02%7,00%10,88%9,66%0,0%5,0%10,0%15,0%20,0%25,0%30,0%35,0%40,0%45,0%Entry Switch Entry Switch Entry Switch Entry Switch Entry SwitchKeyword Style Implicit Demand Wh-Question Yes-No-Question Explicit DemandInfActActInfActInfFigure 2: Speaking styles of implicit task entry and subtask switch distinguished by action (Act) andinformation seeking (Inf)inite noun phrase.
A rephrase utterance can beinterpreted easily for an SDS, since there is noneed to determine the right antecedent from dia-log history.
A definite noun phrase contains thesemantic type of the antecedent and can be ref-ered easily in a semantic annotated dialog history.However, a pronoun or elliptic anaphoric expres-sion is harder to resolve, as the former only de-scribes the syntactic form of the antecedent andthe later does not contain any information of theantecedent.
Sometimes, also humans are not ableto resolve an anaphoric expression easily.
Com-paring information seeking and action subtasks,the only difference can be identified between def-inite noun phrases and rephrase.
In informationseeking subtasks, participants rephrased more of-ten than using definite noun phrases.4,98% 8,87%17,53% 18,61% 2,22%4,68%14,04%29,06%0,0%10,0%20,0%30,0%40,0%50,0%60,0%Elliptic Pronoun,Adverb Definite NP RephraseInfActFigure 3: Anaphoric expressions used in implicitapplication switches.6.4 EfficiencyEspecially in the car it is essential to support shortand efficient interactions.
In this study, partici-pants used on average 6.27 (SD: 2.62) words forone utterance.
However, the word length of a userutterance is only one part which influences dialoglength.
The number of semantic concepts utteredis more important, as the more semantic conceptsare spoken, the less system prompts are needed torequest missing information.
The semantic con-cepts of each user utterance were annotated andcounted (avg: 2.77; SD: 0.73; min: 1; max: 6).They are set in relation to the maximum requiredsemantic concepts (avg: 3.26; SD: 0.59; min: 2;max: 4) for the corresponding subtask.
We dividethe spoken concepts by the maximum concepts tocalculate an efficiency score (avg: 0.86; SD: 0.22).This means 86% of user utterances contain all nec-essary semantic concepts to answer the request.Therefore, in-car SDS need to understand multiplesemantic concepts in one utterance to keep a dia-log short, such as the city, street and street numberfor a destination entry.0,88 0,91 0,69 0,8700,20,40,60,81Entry SwitchActInfAct Inf  Act InfFigure 4: Efficiency scores of user utterances.Figure 4 shows efficiency scores split into taskentry and subtask switch as well as action andinformation seeking.
In total, there is no sig-nificant difference between task entry and sub-task switch concerning number of words, seman-tic concepts, or efficiency score.
Comparing typesof subtasks at task entry, the efficiency score foraction subtasks (avg: 0.69; SD: 0.2) is signifi-cantly (p=0.0018) less than for information seek-ing subtasks (avg.
0.88; SD 0.22).
Although, sig-nificantly (p=0.0003) more semantic concepts inactions were required (avg: 3.66; SD: 0.48) thanin information seekings (avg: 3.2; SD: 0.4), usersdo not utter more semantic concepts.
How manysemantic concepts users can utter in one sentencewhile driving, needs to be addressed in the future.6.5 UsabilityUsability is a necessary condition in order to eval-uate if people will use a system.
The SASSI scores7provide valid evidence of a system?s usability.
Fig-ure 5 shows results separated into the six dimen-sions System Response Accuracy (SRA), Like-ability (Like), Cognitive Demand (Cog Dem), An-noyance (Ann), Habitability (Hab), and Speed.
A7-Likert scale was used and recoded to values [-3, ..., 3].
If a system is less annoying, its usabil-ity will be better.
Thus, except of cognitive de-mand and habitability, the usability of our SDS israted good.
The low habitability score is due to thefact that we did not explain the SDS and after fourtasks users are not completely accustomed to thesystem.-3-2-10123SRA Like Cog Dem Ann Hab Speed SASSIFigure 5: SASSI Usability scores.7 Discussion and Further ResearchThe results show, that users are in general notaware of different applications in speech-only in-car SDSs and switch implicitly between differentdomains.
This interaction schema is similar tohuman-human communication, but may differ ifthe user is primed through a visual representation.Concerning speaking styles, more than half of theparticipants used keyword style and explicit de-mand, which might be regarded impolite betweenhumans.
They are aware to communicate witha system lacking emotions.
A user, who is notsure about the system?s functions, will rather startwith keywords and, after hearing natural formu-lated system prompts, is likely to adapt to natu-ral speaking styles.
A human-like prompt (insteadof our beep) may ensure the user from the begin-ning.
Obviously, speaking styles depend on typeof task, thus question and keyword style is usedfor information seeking and demand style to initi-ate an action.
More than 50% of the participantsused anaphoric expressions, which have to be re-solved within dialog context.
This is comprehen-sible, as for people it is usually easier and more ef-ficient to pronounce an anaphor than to pronouncethe antecedent.
For reaching their interaction goalfast and efficient, the participants used multiple se-mantic concepts in utterances.
In total, 86% ofuser utterances contain all necessary informationto answer the request.
This results in less dialogturns and thus is fundamental for in-car systems.In addition, the usability is rated good, thus thesystem might be accepted by drivers.Another crucial point for in-car systems is thatthey should distract the driver as little as possible.It can be assumed that without visual and hapticdistractions, the driver would keep his focus onthe road.
However, cognitive demand also causesdistraction.
The moderate SASSI score for cogni-tive demand requires an objective test.
Therefore,we will analyze multi-domain interactions with re-spect to mental pressure and driver performancefor further research.
So far, we have only consid-ered multi-domain dialogs with one common se-mantic concept.
By referring to multiple seman-tic concepts, drivers might use more anaphoric ex-pressions or aggregate them with a general term,which needs to be address in further experiments.8 ConclusionsThis paper presents results on how young andtechnically affine people interact with in-car SDSsin performing multi-domain tasks.
31 participantscompleted all together 243 tasks (each with twoapplication switches) while driving in a fixed-basedriving simulator.
In this experiment, a controlledWoZ setup was used instead of a real speechrecognition system.The results identify important guidelines formulti-domain SDSs.
Since users are in general notaware of applications in speech-only dialog sys-tems, implicit application switching is required.However, this should not replace explicit switch-ing commands.
Speaking styles vary and dependon type of task, and dialog state.
Thus languagemodels must therefore consider this issue.
Peo-ple rely on anaphora, which means an SDS mustmaintain a extensive dialog history across multi-ple applications to enable coreference resolution.It is further necessary that the SDS supports multi-ple semantic concepts in one utterance since it en-ables an efficient interaction and drivers use this.The SDS?s usability was rated good by the partici-pants.
For further research, we will analyze multi-domain interaction with respect to driver perfor-mance and multiple semantic concept anaphora.AcknowledgmentsThe work presented here was funded byGetHomeSafe (EU 7th Framework STREP288667).8ReferencesKai-Uwe Carstensen, Christian Ebert, Cornelia Ebert,Susanne Jekat, Ralf Klabunde, and Hagen Langer.2010.
Computerlinguistik und Sprachtechnologie.Spektrum, Akad.
Verl.Hua Cheng, Harry Bratt, Rohit Mishra, ElizabethShriberg, Sandra Upson, Joyce Chen, FuliangWeng, Stanley Peters, Lawrence Cavedon, and JohnNiekrasz.
2004.
A wizard of oz framework for col-lecting spoken human-computer dialogs.
In Proc.
ofICSLP-2000.Victor Ei-Wen Lo and Paul A.
Green.
2013.
Devel-opment and evaluation of automotive speech inter-faces: Useful information from the human factorsand the related literature.
Int.
Journal of VehicularTechnology, 2013:13.Norman M. Fraser and G.Nigel Gilbert.
1991.
Sim-ulating speech systems.
Computer Speech & Lan-guage, 5(1):81 ?
99.Victoria Fromkin, Robert Rodman, and Nina Hyams.2003.
An Introduction to Language.
Rosenberg,Michael, 7 edition.Petra Geutner, Frank Steffens, and Dietrich Manstetten.2002.
Design of the vico spoken dialogue system:Evaluation of user expectations by wizard-of-oz ex-periments.
In Proc.
of the Int.
Conf.
on LanguageResources and Evaluation, volume 2.James Glass, Joseph Polifroni, Stephanie Seneff, andVictor Zue.
2000.
Data collection and performanceevaluation of spoken dialogue systems: The mit ex-perience.
In Proc.
of 6th INT.Hansj?org Hofmann, Ute Ehrlich, Andr?e Berton, andWolfgang Minker.
2012.
Speech interaction withthe internet - a user study.
In Intelligent Environ-ments, Guanajuato, Mexico.Kate S Hone and Robert Graham.
2000.
Towards atool for the subjective assessment of speech systeminterfaces (sassi).
Natural Language Engineering,6(3&4):287?303.Melanie Kellar, Carolyn Watters, and Michael Shep-herd.
2006.
A goal-based classification of web in-formation tasks.
In In 69th Annual Meeting of theAmerican Society for Information Science and Tech-nology (ASIST).Scott R. Klemmer, Anoop K. Sinha, Jack Chen,James A. Landay, Nadeem Aboobaker, and AnnieWang.
2000.
Suede: a wizard of oz prototypingtool for speech user interfaces.
In Proc.
of the 13thannual ACM symposium on User interface softwareand technology, New York.
ACM.Jannette Maciej and Mark Vollrath.
2009.
Compari-son of manual vs. speech-based interaction with in-vehicle information systems.
Accident Analysis andPrevention, 41(5):924 ?
930.Angela Mahr, Michael Feld, Mohammad MehdiMoniri, and Rafael Math.
2012.
The contre (con-tinuous tracking and reaction) task: A flexible ap-proach for assessing driver cognitive workload withhigh sensitivity.
In Andrew L. Kun, Linda Ng Boyle,Bryan Reimer, and Andreas Riener, editors, Adj.Proc.
of the 4th International Conference on Au-tomotive User Interfaces and Interactive VehicularApplications, Portsmouth.
ACM.National Highway Traffic Safety Administra-tion (NHTSA).
2013.
Distracted driving 2011.Technical report.Norbert Reithinger, Jan Alexandersson, TilmanBecker, Anselm Blocher, Ralf Engel, MarkusL?ockelt, Jochen M?uller, Norbert Pfleger, PeterPoller, Michael Streit, and Valentin Tschernomas.2003.
Smartkom: Adaptive and flexible multimodalaccess to multiple applications.
In Multimodalinterfaces, New York.Gabriel Skantze.
2007.
Error Handling in Spoken Di-alogue Systems.
Ph.D. thesis, KTH Computer Sci-ence and Communication.Jessica Villing, Cecilia Holtelius, Staffan Larsson, An-ders Lindstrm, Alexander Seward, and Nina berg.2008.
Interruption, resumption and domain switch-ing in in-vehicle dialogue.
In Bengt Nordstrm andAarne Ranta, editors, Advances in Natural Lan-guage Processing, volume 5221 of Lecture Notes inComputer Science, pages 488?499.
Springer BerlinHeidelberg.Fuliang Weng, Baoshi Yan, Zhe Feng, Florin Ratiu,Madhuri Raya, Brian Lathrop, Annie Lien, Se-bastian Varges, Rohit Mishra, Feng Lin, MatthewPurver, Harry Bratt, Yao Meng, Stanley Peters, To-bias Scheideck, Badri Raghunathan, and ZhaoxiaZhang.
2007.
Chat to your destination.
In Proc.
of8th SIGdial Workshop on Discourse and Dialogue.Christopher D Wickens.
2008.
Multiple resources andmental workload.
In Human factors, volume 50,pages 449?55.
USA.Kristie Young and Michael Regan.
2007.
Driver dis-traction: A review of the literature.
Distracted Driv-ing, pages 379?405.9
