Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 227?234,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsPackPlay: Mining semantic data in collaborative gamesNathan GreenNC State University890 Oval DriveRaleigh, NC 27695Paul BreimyerNC State University890 Oval DriveRaleigh, NC 27695Vinay KumarNC State University890 Oval DriveRaleigh, NC 27695Nagiza F. SamatovaOak Ridge National Lab1 Bethel Valley RdOak Ridge, TN 37831AbstractBuilding training data is labor-intensiveand presents a major obstacle to advanc-ing machine learning technologies such asmachine translators, named entity recog-nizers (NER), part-of-speech taggers, etc.Training data are often specialized for aparticular language or Natural LanguageProcessing (NLP) task.
Knowledge cap-tured by a specific set of training data isnot easily transferable, even to the sameNLP task in another language.
Emergingtechnologies, such as social networks andserious games, offer a unique opportunityto change how we construct training data.While collaborative games have been usedin information retrieval, it is an open is-sue whether users can contribute accurateannotations in a collaborative game con-text for a problem that requires an exactanswer, such as games that would createnamed entity recognition training data.
Wepresent PackPlay, a collaborative gameframework that empirically shows players?ability to mimic annotation accuracy andthoroughness seen in gold standard anno-tated corpora.1 IntroductionAnnotated corpora are sets of structured textused in Natural Language Processing (NLP) thatcontain supplemental knowledge, such as taggedparts-of-speech, semantic concepts assigned tophrases, or semantic relationships between theseconcepts.
Machine Learning (ML) is a subfield ofArtificial Intelligence that studies how computerscan obtain knowledge and create predictive mod-els.
These models require annotated corpora tolearn rules and patterns.
However, these anno-tated corpora must be manually curated for eachdomain or task, which is labor intensive and te-dious (Scannell, 2007), thereby creating a bot-tleneck for advancing ML and NLP predictiontools.
Furthermore, knowledge captured by a spe-cific annotated corpus is often not transferable toanother task, even to the same NLP task in an-other language.
Domain and language specificcorpora are useful for many language technol-ogy applications, including named entity recogni-tion (NER), machine translation, spelling correc-tion, and machine-readable dictionaries.
The AnCru?bada?n Project, for example, has succeeded increating corpora for more than 400 of the world?s6000+ languages by Web crawling.
With a few ex-ceptions, most of the 400+ corpora, however, lackany linguistic annotations due to the limitations ofannotation tools (Rayson et al, 2006).Despite the many documented advantages ofannotated data over raw data (Granger andRayson, 1998; Mair, 2005), there is a dearth ofannotated corpora in many domains.
The ma-jority of previous corpus annotation efforts re-lied on manual annotation by domain experts,automated prediction tagging systems, and hy-brid semi-automatic systems that used both ap-proaches.
While yielding high quality and enor-mously valuable corpora, manually annotatingcorpora can be prohibitively costly and time con-suming.
For example, the GENIA corpus contains9,372 sentences, curated by five part-time annota-tors, one senior coordinator, and one junior coor-dinator over 1.5 years (Kim et al, 2008).
Semi-automatic approaches decrease human effort butoften introduce significant error, while still requir-ing human interaction.The Web can help facilitate semi-automatic ap-proaches by connecting distributed human usersat a previously unfathomable scale and presentsan opportunity to expand annotation efforts tocountless users using Human Computation, theconcept of outsourcing certain computational227processes to humans, generally to solve prob-lems that are intractable or difficult for comput-ers.
This concept is demonstrated in our previ-ous work, WebBANC (Green et al, 2009) andBioDEAL (Breimyer et al, 2009), which allowsusers to annotate Web documents through a Webbrowser plugin for the purposes of creating lin-guistically and biologically tagged annotated cor-pora and with micro-tasking via Mechanical Turk,which allows for a low cost option for manual la-bor tasks (Snow et al, 2008; Kittur et al, 2008).While the Web and Human Computation maybe a powerful tandem for generating data andsolving difficult problems, in order to succeed,users must be motivated to participate.
Humanshave been fascinated with games for centuriesand play them for many reasons, including forentertainment, honing skills, and gaining knowl-edge (FAS Summit, 2006).
Every year, a largeamount of hours are spent playing online computergames.
The games range form simple card andword games to more complex 3-D world games.One such site for word, puzzle, and card games isPogo.com1.
According to protrackr,2 Pogo has al-most 6 million unique visitors a day.
Alexa.com3shows that the average user is on the site for 11minutes at a time.
When the average time spent onthe site is propagated to each user, the combinedtime is equal to more than 45,000 days of humantime.
Arguably if, the games on Pogo were usedto harvest useful data, various fields of ComputerScience research could be advanced.There has been a recent trend to leverage hu-man?s fascination in game playing to solve diffi-cult problems through Human Computation.
Twosuch games include ESP and Google?s Image La-beler (Ahn and Dabbish, 2004), in which play-ers annotate images in a cooperative environmentto correctly match image tags with their partner.Semantic annotation has also been addressed inthe game Phrase Detectives (Chamberlain et al,2009), which has the goal of creating large scaletraining data for anaphora resolution.
These typesof games are part of a larger, serious games, initia-tive (Annetta, 2008).This paper introduces the Web-enabled collabo-rative game framework, PackPlay, and investigates1Pogo.
http://www.pogo.com/2Protrackr.com site information and statistis-tics.http://www.protrackr.com/3Alexa: The Web Information Company.http://www.alexa.com/how collaborative online gaming can affect anno-tation throughput and annotation accuracy.
Thereare two main questions for such systems: first,will overall throughput increase compared to tra-ditional methods of annotating, such as the man-ual construction of the Genia Corpus?
Second,how accurate are the collective annotations?
Asuccessful human computation environment, suchas PackPlay, would represent a paradigm shift inthe way annotated corpora are created.
However,adoption of such a framework cannot be expecteduntil these questions are answered.
We addressboth of these questions in multiple games in ourPackPlay system through evaluation of the collec-tive players?
annotations with precision and recallto judge accuracy of players?
annotations and thenumber of games played to judge throughput.
Weshow improvements in both areas over traditionalannotation methods and show accuracy compara-ble to expert prediction systems that could be usedfor semi-supervised annotation.2 MethodologyWe empirically show casual game players?
abil-ity to accurately and throughly annotate corporaby conducting experiments following the processdescribed in Section 2.1 with 8 players usingthe PackPlay System.
The testers annotate thedatasets described in Section 2.2 and results areanalyzed using the equations in Section 2.3.2.1 PackPlay Process FlowFigure 1 shows the average PackPlay process flowthat a player will follow for a multi-player game.Assuming the player is registered, the player willalways start by logging in and selecting the gamehe or she wants to play.
Once in the game screen,the system will try to pair the player with anotherplayer who is waiting.
After a set time limit,the game will automatically pair the user with aPlayerBot.
It is important to note that the playerwill not know that his or her partner is a Player-Bot.Once paired, a game can start.
In most games, aquestion will be sampled from our database.
Howthis sampling takes place is up to the individualgame.
Once sampled, the question will be dis-played to one player or all players, depending onwhether the game is synchronous or asynchronous(see definitions in Sections 3.1.2 and 3.2.2).
Oncethe question is displayed, two things can happen.228NoNoNoNoNoYesYesYesYesYesLoginSelect GameHas Partner?Wait for Partner for X secondsStart Screen w/ leader boardPair with BotDisplay QuestionAnnotateTimer expired?
Done?More Questions?
End GameHas Partner?Figure 1: User process flow for PackPlay games.First, the timer can run out; this timer is set by eachgame individually.
Second, the player may answerthe question and move on to the next question.
Af-ter either one of those two options, a new questionwill be sampled.
This cycle continues until thegame session is over.
This is usually determinedby the game, as each game can set the number ofquestions in a session, or by a player quiting thegame.2.2 Data SourcesTo compare named entity results, PackPlay usessentences and annotations from CoNLL 2003, a?gold?
standard corpus (Tjong et al, 2003).
Weuse the CoNLL 2003 corpus since it has been cu-rated by experts and the PackPlay system can com-pare our players?
annotations vs those of 16 sub-mitted predictive models, also refered to as theCoNLL average, in the 2003 conference on nat-ural language learning.
This paper will refer to thetraining corpus as the CoNLL corpus, and we se-lected it for our evaluation due to its widespreadadoption as a benchmark corpus.2.3 MetricsTo measure how thoroughly and accurately ourplayers annotate the data, we calculate both recall(Equation 1) and precision (Equation 2), in which?
is the set of words annotated in PackPlay and ?is the set of words in the base CoNLL corpus.Recall =|?
?
?
||?|(1)Precision =|?
?
?
||?|(2)Each game module in the PackPlay system hasits own scoring module, which is intended to im-prove the players?
precision.
For this reason, scor-ing is handled on a per game level.
Each game hasits own leader board as well.
The leader board isused to motivate the players to continue playingthe PackPlay games.
This is intended to improverecall for annotations in the system.3 Games3.1 Entity Discovery3.1.1 Game descriptionNamed entities are a foundational part of manyNLP systems from information extraction sys-tems to machine translation systems.
The abil-ity to detect an entity is an application area calledNamed Entity Recognition (NER).
The most com-mon named entity categories are Person (Per), Lo-cation (Loc), and Organization (Org).
The abilityto extract these entities may be used in everydaywork, such as extracting defendants, cities, andcompanies from court briefings, or it may be usedfor critical systems in national defense, such asmonitoring communications for people and loca-tions of interest.To help with the creation of more NER systems,Entity Discovery (see Figure 2), a game for an-notating sentences with supplied entities was cre-ated.
The goal of the game is to pair players witheach other and allow them to annotate sentencestogether.
While this annotation task could be doneby one person, it is a very time consuming activ-ity.
By creating a game, we hope that players willbe more likely to annotate for fun and will anno-tate correctly and completely in order to receive ahigher score in the PackPlay system.3.1.2 ImplementationEntity Discovery is implemented as a synchronoustwo-player game.
A synchronous game is one inwhich both players have the same task in the game,in this case, to annotate a sentence.
To have a basecomparison point, all players are asked to annotatea random set of 60 sentences to start, for which wehave the correct answers.
This way we will be ableto assess the trustworthiness score in future itera-tions.
After the pretest, the players will be shownsentences randomly sampled with replacement.229Figure 2: Screenshot of a player annotating the Person entity Jimi HendrixIn Entity Discovery, we made a design decisionto keep a player?s partner anonymous.
This shouldhelp reduce cheating, such as agreeing to selectthe same word over and over, and it should reducethe ability for a player to only play with his orher friends, which might enhance their ability tocheat by using other communication systems suchas instant messaging or a cell phone.
Since Pack-Play is still in the experimental stages, players maynot always be available.
For this reason, we haveimplemented a PlayerBot system.
The PlayerBotwill mimic another player by selecting previouslyannotated phrases for a given sentence from thedatabase.
From the human players?
point of view,nothing seems different.Players are asked to annotate, or tag, as manyentities as they can find in a sentence.
Players arealso told at the beginning of the game that they arepaired with another user.
Their goal is to annotatethe same things as their partner.
Our assumptionis that if the game is a single player game then theplayers may just annotate the most obvious enti-ties for gaining more points.
By having the playerto try to guess at what their partner may anno-tate we hope to get better overall coverage of enti-ties.
We try to minimize the errors, which guess-ing might produce, in a second game, Name ThatEntity (Section 3.2).To annotate a sentence, the player simply high-lights a word or phrase and clicks on a relevantentity.
For instance in Entity Discovery, a playercan annotate the phrase ?Jimi Hendrix?
as a Per-son entity.
From this point on, the player is freeto annotate more phrases in the sentence.
Whenthe player completes annotating a sentence, theplayer hits ?Next Problem.?
The system then waitsfor the player?s partner to hit ?Next Problem?
aswell.
When both players finish annotating, thegame points will be calculated and a new questionwill be sampled for the players.230Figure 3: Screenshot of what the player sees at theend of the Entity Discovery game3.1.3 ScoringScoring can be done in a variety of ways, each hav-ing an impact on players?
performance and enjoy-ment.
For Entity Discovery, we decided to giveeach user a flat score of 100 points for every an-swer that matched their partner.
At the end ofeach game session, the player will see what an-swers matched with their partner.
For instance, ifboth players tagged ?Jimi Hendrix?
as a Person,they will both receive 100 points.
We do not showthe players their matched scores after each sen-tence, since this might bias the user to tag moreor less depending on what their partner does.
Fig-ure 3 shows a typical scoring screen at the endof a game; in Figure 3, the players matched 4phrases, totaling 400 points.
It is important to notethat at this stage we do not distinguish betweencorrect and incorrect annotations, just whether thetwo players agree.3.1.4 User Case Study MethodologyTo examine Entity Discovery as a collaborativegame toward the creation of an annotated corpus,we conducted a user experiment to collect sam-ple data on a known data set.
Over a short time,8 players were asked to play both Entity Discov-ery and Name That Entity.
In PackPlay, through-put can be estimated, since each game has a de-fined time limit, defined as the average numberof entities annotated per question times the num-ber of users times the average number of ques-tions seen by a user.
Unlike other systems such asMechanical Turk (Snow et al, 2008; Kittur et al,2008), BioDeal (Breimyer et al, 2009), or Web-BANC (Green et al, 2009), in PackPlay we definethe speed at which a user annotates.Each game in Entity Discovery consists of 10sentences from the CoNLL corpus.
These sen-tences are not guaranteed to have a named en-tity within them.
The users in the study were notTable 1: Statistics returned from our user study forthe game Entity DiscoveryStatistic Total Mean# of games 29 3.62# of annotations 291 40.85informed of the entity content as to not bias theexperiment and falsely raise our precision scores.With only 8 players, we obtained 291 annotations,which averaged to about 40 annotations per user.This study was not done over a long period of time,so each user only played, on average, 3.6 games.Two players were asked to intentionally anno-tate poorly.
The goal of using poor annotatorswas to simulate real world players, who may justclick answers to ruin the game or who are clue-less to what a named entity is.
This informationcan be used in later research to help automaticallydetect ?bad?
annotators using anomaly detectiontechniques.PackPlay also stores information not used inthis study, such as time stamps for each questionanswered.
This information will be incorporatedinto future experiment analysis to see if we canfurther improve our annotated corpora based onthe order and time spent forming an annotation.For instance, the first annotation in a sentence mayhave a higher probability than the last annotation.It is possible that if a user answers too fast, theanswer is likely an error.3.1.5 Output QualityEvery player completes part of a 60 sentencepretest in which we know the answers.
For eachgame, the questions are sampled without replace-ment but this does not carry over after a game.For instance, if a player finishes game 1, he or shewill never see the same question twice.
For gametwo, no question within the game will be repeated,however, the player might see a question he or sheanswered in game 1.
Because of this, each userwill not see all 60 questions, but we will have agood sample to judge whether a user is accurateor not.
The ability to repeat a question in differentgames allows us, in future research, to test play-ers using intra-annotator agreement statistics.
Thistests how well a player agrees with himself or her-self.
From this set of 60 questions we have calcu-lated each player?s recall and precision scores.As Table 2 shows, the recall scores for Entity231Table 2: Recall and precision for Entity Discoveryannotations of CoNLL data.Per Loc Org Avg CoNLLAvgRecall(All Data) 0.94 0.95 0.85 0.9 0.82Precision(All Data) 0.47 0.70 0.53 0.62 0.83Discovery in this experiment were 0.94, 0.95, and0.85 for Person, Location, and Organization, re-spectively.
The overall average was 0.9, whichbeats out the CoNLL average, an average of 16expert systems, for recall.
Entity Discovery?s num-bers are similar to the pattern seen in the CoNLLpredictive systems for Person, Location and Or-ganization, in which Organization was the lowestand Person was the highest.
The precision num-bers were quite lower, with an average of 0.62.When examining the data, most of the precisionerrors occurred because of word phrase boundaryissues with the annotation and also players oftenare unsure whether to include titles such as Presi-dent, Mr., or Dr.
There were also quite a few errorswhere players annotated concepts as People suchas ?The Judge?
or ?The scorekeeper.?
While this isincorrect for named entity recognition, it might beof interest to a co-reference resolution corpus.
Theprecision numbers are likely low because of ouruntrained players and because some of the playerswere told to intentionally annotate entities incor-rectly.
To improve on these numbers, we applieda coverage requirement and majority voting.
Thecoverage requirement requires that more than oneplayer has annotated a given phrase for the an-notation to be included in the corpus.
Majorityvoting indicates that the phrase is only includedif 50% or more of the playerss who annotated aphrase, agreed on the specific entity assigned tothe phrase.As Table 3 shows, both majority voting andcoverage requirements improve precision by morethan 10%.
When combined, they improve theoverall precision to 0.88, a 26% improvement.This is an improvement to the expert CoNLL sys-tems score of 0.83.
The majority voting likelyremoved the annotations from our purposefully?bad?
annotators.For future work, as the number of players in-creases, we will have to increase our coverage re-Table 3: Precision for Entity Discovery annota-tions of CoNLL data with filteringPer Loc Org AvgPrecision(Majority Voting) 0.56 0.79 0.65 0.72Precision(Coverage Req.)
0.69 0.83 0.63 0.73Precision(Majority Voting +Coverage Req.)
0.90 0.95 0.81 0.88quirement to match.
This ratio has not been deter-mined and will need to be tested.
A more success-ful way to detect errors in our annotations may beto create a separate game to verify given answers.To initially test this concept we have made and setup an experiment with a game, called Name ThatEntity.3.2 Name That Entity3.2.1 Game DescriptionName That Entity is another game with a focuson named entities.
Name That Entity was createdto show that game mechanics and the creation offurther games would enhance the value of an an-notated corpus.
In the case of Name That Entity,we have created a multiple choice game in whichthe player will select the entity that best representsthe highlighted word or phrase.
Unlike Entity Dis-covery, this allows us to focus the annotation ef-fort on particular words or phrases.
Once again,this is modeled as a two-player game but the play-ers are not playing simultaneously.
The goal forthe player is to select the same entity type for thehighlighted word that their partner selects.
In thisgame, speed is of the essence since each questionwill ask for one entity as opposed to Entity Discov-ery, which was open ended to how many entitiesmight exist in a sentence.3.2.2 ImplementationAs described above, Name That Entity appears tobe a two-player synchronous game.
The playeris under the assumption that he or she must onceagain match his or her partner?s choice.
What theplayer does not know is that the multi-player issimulated in this case.
The player is replaced witha PlayerBot which chooses annotations from theEntity Discovery game.
This, in essence, creates232an asynchronous game, in which one player hasthe task of finding entities and the other player hasthe task of verifying entities.
This gives us a fur-ther mechanism to check the validity of entities an-notated by the Entity Discovery game.As with Entity Discovery, the player?s partner isanonymous.
This anonymity allows us to keep theasynchronous structure hidden, as well as judge anew metric, intra-annotator agreement, not testedin the previous game.
Since it is possible that aplayer in PackPlay may have a question sampledthat was previously annotated in the Entity Dis-covery game by the same player, we can use intra-annotator agreement.
While well-known inter-annotator statistics, such as Cohen?s Kappa, evalu-ate one annotator versus the other annotator, intra-annotator statistics allow us to judge an annota-tor versus himself or herself to test for consis-tency (Artstein and Poesio, 2008).
In the Pack-Play framework this allows us to detect playersswho are randomly guessing and are therefore notconsistent with themselves.3.2.3 ScoringSince entity coverage of a sentence is not an is-sue in the multiple choice game, we made use ofa different scoring system that would reward firstinstincts.
While the Entity Discovery game has aset score for every answer, Name That Entity has asliding scale.
For each question, the max score is100 points, as the time ticks away the user receivesfewer points.
The points remaining are indicatedto the user via a timing bar at the bottom of thescreen.When the player completes a game, he or sheis allowed to view the results for that game.
Un-like the Entity Discovery game, we display to theplayer what entity his or her partner chooses onthe question in which they both did not match.This gives us a quick and simple form of annotatortraining, since a player with no experience may notbe familiar with a particular entity.
This was seenwith the players?
ability to detect an Organizationentity.
We expect that when a player sees whathis or her partner annotates a phrase as, the player,is, in effect, being trained.
However, displayingthis at the end should not have any affects towardcheating since their partners are anonymous.3.2.4 User Case Study MethodologyOf the 8 players who participated in the Entity Dis-covery study, 7 also played Name That Entity dur-ing their game sessions.
We did not inform theplayers, but the questions asked in Name That En-tity were the same answers that the players gave inthe experiment in Section 3.1.4.
The basic anno-tation numbers from our small user study can beseen in Table 4.Table 4: Statistics returned from our user study forthe game Name That EntityStatistic Total Mean# of games 20 2.85# of annotations 195 27.853.2.5 Output QualityAs Name That Entity is not intended to be a solomechanism to generate annotations, but insteada way to verify existing annotations, we did notassess the recall and precision of the game.
In-stead we are looking at the number of annota-tions, unique annotations, and conflicting annota-tions generated by our players in this game.Table 5: Types of annotations generated by NameThat EntityError CountAnnotations 195Unique Annotations 141Conflicts 38Unique Conflicts 35In Table 5, unique annotations refer to annota-tions verified by only one user.
Of the 195 totalverified annotation, 38 had conflicting answers.
Inthe majority of the cases the players marked theseconflicts as ?None of the Above,?
indicating thatthe annotated phrase from Entity Discovery wasincorrect.
For instance, many players made themistake in Entity Discovery of marking phrasessuch as ?German,?
?English,?
and ?French?
as Lo-cation entities when they are, in fact, just adjec-tives.
In Name That Entity, the majority of playerscorrected each other and marked these as ?Noneof the Above.
?The main use of this game will be to incorporateit as an accuracy check for players based on theseconflicting annotation.
This accuracy check willbe used in future work to deal with user confidencescores and conflict resolution.2334 ConclusionAnnotated corpora generation presents a major ob-stacle to advancing modern Natural Language Pro-cessing technologies.
In this paper we introducedthe PackPlay framework, which aims to leveragea distributed web user community in a collabora-tive game to build semantically-rich annotated cor-pora from players annotations.
PackPlay is shownto have high precision and recall numbers whencompared to expert systems in the area of namedentity recognition.
These annotated corpora weregenerated from two collaborative games in Pack-Play, Entity Discovery and Name That Entity.
Thetwo games combined let us exploit the benefits ofboth synchronous and asynchronous gameplay asmechanisms to verify the quality of our annotatedcorpora.
Future work should combine the play-ers output with a player confidence score basedon conflict resolution algorithms, using both inter-and intra-annotator metrics.ReferencesLuis von Ahn and Laura Dabbish.
2004 Labeling im-ages with a computer game.
ACM, pages 319-326,Vienna, Austria.Leonard A. Annetta.
2008 Serious EducationalGames: From Theory to Practice.
Sense Publishers.Ron Artstein and Massimo Poesio.
2008 Inter-coder agreement for computational linguistics.Computational Linguistics, Vol.
34, Issue 4, pages555-596.Maged N. Kamel Boulos and Steve Wheeler.
2007.The emerging web 2.0 social software: an enablingsuite of sociable technologies in health and healthcare education.
Health information and librariesjournal, Vol.
24, pages 223.Paul Breimyer, Nathan Green, Vinay Kumar, and Na-giza F. Samatova.
2009.
BioDEAL: communitygeneration of biological annotations.
BMC MedicalInformatics and Decision Making, Vol.
9, pagesSuppl+1.Jon Chamberlain, Udo Kruschwitz, and Massimo Poe-sio.
2009.
Constructing an anaphorically anno-tated corpus with non-experts: assessing the qual-ity of collaborative annotations.
People?s Web ?09:Proceedings of the 2009 Workshop on The People?sWeb Meets NLP, pages 57-62.FAS Summit on educational games: Harnessing thepower of video games for learning (report), 2006.Sylviane Granger and Paul Rayson.
1998.
LearnerEnglish on Computer.
Longman, London, and NewYorks pp.
119-131.Nathan Green, Paul Breimyer, Vinay Kumar, and Na-giza F. Samatova.
2009.
WebBANC: Build-ing Semantically-Rich Annotated Corpora fromWeb User Annotations of Minority Languages.Proceedings of the 17th Nordic Conference ofComputational Linguistics (NODALIDA), Vol.
4,pages 48-56, Odense, Denmark.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.2008.
Corpus annotation for mining biomedicalevents from literature.
BMC Bioinformatics, 9:10.Aniket Kittur, Ed H. Chi, and Bongwon Suh.
2008.Crowdsourcing user studies with Mechanical TurkCHI ?08: Proceeding of the twenty-sixth annualSIGCHI conference on Human factors in computingsystems, pages 453-456, Florence, Italy.Ravi Kumar, Jasmine Novak, and Andrew Tomkins.2006 Structure and evolution of online social net-works.
KDD ?06: Proceedings of the 12th ACMSIGKDD international conference on Knowledgediscovery and data mining, pages 611-617, NewYork, NY.C.
Mair.
2005.
The corpus-based study of languagechange in progress: The extra value of tagged cor-pora.
The AAACL/ICAME Conference, Ann Arbor,MI.Paul Rayson, James Walkerdine,William H. Fletcher,and Adam Kilgarriff.
2006.
Annotated web as cor-pus The 2nd International Workshop on Web asCorpus (EACL06), Trento, Italy.Kevin P. Scannell.
2007.
The Crbadn Project:Corpus building for under-resourced languages.Proceedings of the 3rd Web as Corpus WorkshopLouvain-la-Neuve, Belgium.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast ?
but is itgood?
: evaluating non-expert annotations for nat-ural language tasks EMNLP ?08: Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 254?263, Honolulu,Hawaii.Erik F. Tjong, Kim Sang and Fien De Meul-der 2003 Introduction to the conll-2003 sharedtask: language-independent named entity recogni-tion.
Association for Computational Linguistics,pages 142-147, Edmonton, Canada.234
