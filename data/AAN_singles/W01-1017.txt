The Automatic Generation of Formal Annotations in a MultimediaIndexing and Searching EnvironmentThierry DeclerckDFKI GmbHStuhlsatzenhausweg 3D-66123 SaarbrueckenGermanydeclerck@dfki.dePeter WittenburgMPI for PsycholinguisticsWundtlaan 1, PB 310NL-6500 AH NijmegenThe NetherlandsPeter.Wittenburg@mpi.nlHamish CunninghamDept.
of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloGB-Sheffield S1 4DPGreat Britainhamish@dcs.shef.ac.ukAbstractWe describe in this paper the MU-MIS Project (Multimedia Indexing andSearching Environment)1 , which isconcerned with the development and in-tegration of base technologies, demon-strated within a laboratory prototype, tosupport automated multimedia index-ing and to facilitate search and retrievalfrom multimedia databases.
We stressthe role linguistically motivated annota-tions, coupled with domain-specific in-formation, can play within this environ-ment.
The project will demonstrate thatinnovative technology components canoperate on multilingual, multisource,and multimedia information and createa meaningful and queryable database.1 IntroductionMUMIS develops and integrates basic technolo-gies, which will be demonstrated within a labora-tory prototype, for the automatic indexing of mul-timedia programme material.
Various technologycomponents operating offline will generate for-mal annotations of events in the data material pro-cessed.
These formal annotations will form thebasis for the integral online part of the MUMISproject, consisting of a user interface allowing thequerying of videos.
The indexing of the video ma-terial with relevant events will be done along the1MUMIS is an on-going EU-funded project within theInformation Society Program (IST) of the European Union,section Human Language Technology (HLT).
See for moreinformation http://parlevink.cs.utwente.nl/projects/mumis/.line of time codes extracted from the various doc-uments.For this purpose the project makes use of datafrom different media sources (textual documents,radio and television broadcasts) in different lan-guages (Dutch, English and German) to build aspecialized set of lexicons and an ontology forthe selected domain (soccer).
It also digitizesnon-text data and applies speech recognition tech-niques to extract text for the purpose of annota-tion.The core linguistic processing for the anno-tation of the multimedia material consists ofadvanced information extraction techniques foridentifying, collecting and normalizing signifi-cant text elements (such as the names of playersin a team, goals scored, time points or sequencesetc.)
which are critical for the appropriate anno-tation of the multimedia material in the case ofsoccer.Due to the fact that the project is accessing andprocessing distinct media in distinct languages,there is a need for a novel type of merging tool inorder to combine the semantically related annota-tions generated from those different data sources,and to detect inconsistencies and/or redundancieswithin the combined annotations.
The merged an-notations will be stored in a database, where theywill be combined with relevant metadata.2Finally the project will develop a user interfaceto enable professional users to query the database,by selecting from menus based on structured an-2We see in this process of merging extracted informa-tions and their combination with metadata a fruitful base forthe identification and classification of content or knowledgefrom distinct types of documents.notations and metadata, and to view video frag-ments retrieved to satisfy the query, offering thusa tool to formulate queries about multimedia pro-grammes and directly get interactive access to themultimedia contents.
This tool constitutes the on-line component of the MUMIS environment.2 State of the ArtMUMIS differs in many significant ways from ex-isting technologies and already achieved or ad-vanced projects3 .
Most closely related to the the-matic focus of MUMIS are the HLT projects Pop-Eye [POP] and OLIVE [OLI].
Pop-Eye used sub-titles to index video streams and offered time-stamped texts to satisfy a user query, on requestdisplaying a storyboard or video fragment corre-sponding to the text hit.
OLIVE used automaticspeech recognition to generate transcriptions ofthe sound tracks of news reports, which were thenindexed and used in ways similar to the Pop-Eyeproject; both projects used fuzzy matching IR al-gorithms to search and retrieve text, offering lim-ited multilingual access to texts.
Instead of usingIR methods to index and search the transcriptions,MUMIS will create formal annotations to the in-formation, and will fuse information annotationsfrom different media sources.
The fusion resultis then used to direct retrieval, through interfacetechniques such as pop-up menus, keyword lists,and so on.
Search takes the user direct to the sto-ryboard and video clippings.The Informedia project at Carnegie-Mellon-University [INF] has a similar conceptual base-line to MUMIS.
The innovative contribution ofMUMIS is that it uses a variety of multilingualinformation sources and fuses them on the ba-sis of formal domain-specific annotations.
WhereInformedia primarily focuses on special applica-tions, MUMIS aims at the advancement and in-tegratibility of HLT-enhanced modules to enableinformation filtering beyond the textual domain.Therefore, MUMIS can be seen as complemen-tary to Informedia with extensions typical for Eu-rope.The THISL project [THI] is about spoken doc-ument retrieval, i.e., automatic speech recognition3We are aware of more related on-going projects, at leastwithin the IST program, but we can not compare those toMUMIS now, since we still lack first reports.is used to auto-transcribe news reports and theninformation retrieval is carried out on this infor-mation.
One main focus of THISL is to improvespeech recognition.
Compared to MUMIS it lacksthe strong language processing aspects, the fusionof multilingual sources, and the multimedia deliv-ery.Columbia university is running a project [COL]to use textual annotations of video streams to in-dicate moments of interest, in order to limit thescope of the video processing task which requiresextreme CPU capacities.
So the focus is on find-ing strategies to limit video processing.
The Uni-versity of Massachusetts (Amherst) is also run-ning projects about video indexing [UMA], butthese focus on the combination of text and im-ages.
Associated text is used to facilitate indexingof video content.
Both projects are funded underthe NSF Stimulate programme [NSF].Much work has been done on video and im-age processing (Virage [VIR], the EUROMEDIAproject [EUR], Surfimage [SUR], the ISIS project[ISI], IBM's Media Miner, projects funded underthe NSF Stimulate program [NSF], and many oth-ers).
Although this technology in general is in itsinfancy, there is reliable technology to indicate,for example, scene changes using very low-levelcues and to extract key frames at those instancesto form a storyboard for easy video access.
Someinstitutions are running projects to detect subtitlesin the video scene and create a textual annotation.This task is very difficult, given a sequence of realscenes with moving backgrounds and so on.
Evenmore ambitious tasks such as finding real patternsin real movies (tracing the course of the ball in asoccer match, for example) are still far from beingachieved.43 Formal Annotations for the SoccerDomainSoccer has been chosen as the domain to test andapply the algorithms to be developed.
There are anumber of reasons for this choice: availability ofpeople willing to help in analyzing user require-ments, existence of many information sources in4The URLs of the projects mentionned above are givenin the bibliography at the end of this paper.several languages5 , and great economic and pub-lic interest.
The prototype will also be tested byTV professionals and sport journalists, who willreport on its practicability for the creation andmanagement of their programme and informationmaterial.The principles and methods derived from thisdomain can be applied to other as well.
This hasbeen shown already in the context of text-basedInformation Extraction (IE), for which method-ologies for a fast adaptation to new domainshave been developed (see the MUC conferencesand (Neumann et al, 2000)).
And generallyspeaking the use of IE for automatic annotationof multimedia document has the advantage ofproviding, besides the results of the (shallow)syntactic processing, accurate semantic (or con-tent/conceptual) information (and thus potentialannotation) for specific predefined domains, sincea mapping from the linguistically analyzed rele-vant text parts can be mapped onto an unambigu-ous conceptual description6 .
Thus in a sense itcan be assumed that IE is supporting the wordsense disambiguation task.It is also commonly assumed (see among oth-ers (Cunningham, 1999)) that IE occupies an in-termediate place between Information Retrieval(with few linguistic knowledge involved) andText Understanding (involving the full deep lin-guistic analysis and being still not realized for thetime being.).
IE being robust but offering only apartial (but mostly accurate) syntactic and contentanalysis, it can be said that this language technol-ogy is actually filling the gap between availablelow-level annotated/indexed documents and cor-pora and the desirable full content annotation ofthose documents and corpora.
This is the reasonwhy MUMIS has chosen this technology for pro-viding automatic annotation (at distinct linguisticand domain-specific levels) of multimedia mate-rial, allowing thus to add queryable ?content in-formation?
to this material.75We would like to thank at this place the various institu-tions making available various textual, audio and video data.6This topic has already been object of a workshop dis-cussing the relations between IE and Corpus Linguistics(McNaught, 2000).7MUMIS was not explicitly designed for supportingknowledge management tasks, but we assume that the mean-ingful organization of domain-specific multimedia materialproposed by the project can be adapted to the organization of4 The Multimedia Material in MUMISThe MUMIS project is about automatic index-ing of videos of soccer matches with formal an-notations and querying that information to getimmediate access to interesting video fragments.For this purpose the project chose the EuropeanFootball Championships 2000 in Belgium and theNetherlands as its main database.
A major projectgoal is to merge the formal annotations extractedfrom textual and audio material (including the au-dio part of videos) on the EURO 2000 in threelanguages: English, German, Dutch.
The mate-rial MUMIS has to process can be classified inthe following way:1.
Reports from Newspapers (reports aboutspecific games, general reports) which isclassified as free texts (FrT)2.
Tickers, close captions, Action-Databaseswhich are classified as semi-formal texts(SFT)3.
Formal descriptions about specific gameswhich are classified as formal texts (FoT)4.
Audio material recorded from radio and TVbroadcasts5.
Video material recorded from TV broadcasts1-4 will be used for automatically generatingformal annotations in order to index 5.
MUMISis investigating the precise contribution of eachsource of information for the overall goal of theproject.Since the information contained in formal textscan be considered as a database of true facts, theyplay an important role within MUMIS.
But never-theless they contain only few information about agame: the goals, the substitutions and some otherfew events (penalties, yellow and red cards).
Sothere are only few time points available for in-dexing videos.
Semi-formal texts (SFT), like livetickers on the web, are offering much more timepoints sequences, related with a higher diversitythe distributed information of an enterprise and thus supportthe sharing and access to companies expertise and know-how.of events (goals scenes, fouls etc,) and seem to of-fer the best textual source for our purposes.
Nev-ertheless the quality of the texts of online tick-ers is often quite poor.
Free texts, like newspa-pers articles, have a high quality but the extrac-tion of time points and their associated events intext is more difficult.
Those texts also offer morebackground information which might be interest-ing for the users (age of the players, the clubs theyare normally playing for, etc.).
Figures 1 and 2 insection 8 show examples of (German) formal andsemi-formal texts on one and the same game.5 Processing Steps in MUMIS5.1 Media Pre-ProcessingMedia material has been delivered in variousformats (AudioDAT, AudioCassettes, Hi-8 videocassettes, DV video cassettes etc) and qualities.All audio signals (also those which are part ofthe video recordings) are digitized and stored inan audio archive.
Audio digitization is done with20 kHz sample frequency, the format generated isaccording to the de-facto wav standard.
For dig-itization any available tool can be used such asSoundForge.Video information (including the audio compo-nent) of selected games have been digitized intoMPEG1 streams first.
Later it will be encoded inMPEG2 streams.
While the quality of MPEG1 iscertainly not satisfying to the end-user, its band-width and CPU requirements are moderate forcurrent computer and network technology.
Themean bit rate for MPEG1 streams is about 1.5Mbps.
Current state-of-the-art computers can ren-der MPEG1 streams in real time and many net-work connections (Intranet and even Internet) cansupport MPEG1.
MPEG2 is specified for about 3to 5 Mbps.
Currently the top-end personal com-puters can render MPEG2, but MPEG2 is not yetsupported for the most relevant player APIs suchas JavaMediaFramework or Quicktime.
Whenthis support is given the MUMIS project will alsooffer MPEG2 quality.For all separate audio recordings as for ex-ample from radio stations it has to be checkedwhether the time base is synchronous to that oneof the corresponding video recordings.
In case oflarger deviations a time base correction factor hasto be estimated and stored for later use.
Given thatthe annotations cannot be created with too highaccuracy a certain time base deviation will be ac-cepted.
For part of the audio signals manual tran-scriptions have to be generated to train the speechrecognizers.
These transcripts will be delivered inXML-structured files.Since keyframes will be needed in the user in-terface, the MUMIS project will develop softwarethat easily can generate such keyframes around aset of pre-defined time marks.
Time marks willbe the result of information extraction processes,since the corresponding formal annotations is re-ferring to to specific moments in time.
The soft-ware to be written has to extract the set of timemarks from the XML-structured formal annota-tion file and extract a set of keyframes from theMPEG streams around those time marks.
A set ofkeyframes will be extracted around the indicatedmoments in time, since the estimated times willnot be exact and since the video scenes at suchdecisive moments are changing rapidly.
Thereis a chance to miss the interesting scene by us-ing keyframes and just see for example specta-tors.
Taking a number of keyframes increases thechance to grab meaningful frames.5.2 Multilingual Automatic SpeechRecognitionDomain specific language models will be trained.The training can be bootstrapped from written re-ports of soccer matches, but substantial amountsof transcribed recordings of commentaries onmatches are also required.
Novel techniqueswill be developed to interpolate the base-line lan-guage models of the Automatic Speech Recogni-tion (ASR) systems and the domain specific mod-els.
Moreover, techniques must be developed toadapt the vocabularies and the language modelsto reflect the specific conditions of a match (e.g.,the names players have to be added to the vocabu-lary, with the proper bias in the language model).In addition, the acoustic models must be adaptedto cope with the background noise present in mostrecordings.Automatic speech recognition of the soundtracks of television and (especially) radio pro-grammes will make use of closed caption subtitletexts and information extracted from formal textsto help in finding interesting sequences and auto-matically transcribing them.
Further, the domainlexicons will help with keyword and topic spot-ting.
Around such text islands ASR will be usedto transcribe the spoken soundtrack.
The ASRsystem will then be enriched with lexica contain-ing more keywords, to increase the number of se-quence types that can be identified and automati-cally transcribed.5.3 Multilingual Domain Lexicon BuildingAll the collected textual data for the soccer do-main are used for building the multilingual do-main lexicons.
This data can be in XML, HTML,plain text format, etc.
A number of automaticprocesses are used for the lexicon building, firston a monolingual and secondly on a multilin-gual level.
Manual browsing and editing is tak-ing place, mainly in order to provide the semanticlinks to the terms, but also for the fine-tuning ofthe lexicon according to the domain knowledge.Domain lexicons are built for four lan-guages, namely English, German, Dutch andSwedish.
The lexicons will be delivered in afully structured, XML-compliant, TMX-format(Translation Memory eXchange format).
Formore information about the TMX format seehttp://www.lisa.org/tmx/tmx.htm.We will also investigate how farEUROWORDNET resources (seehttp://www.hum.uva.nl/ ewn/) can be of usefor the organization of the domain-specificterminology.5.4 Building of Domain Ontology and EventTableThe project is currently building an ontology forthe soccer domain, taking into consideration therequirements of the information extraction andmerging components, as well as users require-ments.
The ontology will be delivered in an XMLformat8.8There are still on-going discussions within theproject consortium wrt the best possible encoding for-mat for the domain ontology, the alternative beingreduced probably to RDFS, OIL and IFF, see respec-tively, and among others, http://www.w3.org/TR/rdf-schema/, http://www.oasis-open.org/cover/oil.html andhttp://www.ontologos.org/IFF/The%20IFF%20Language.htmlIn parallel to building the ontology an event ta-ble is being described.
It contains the major eventtypes that can occur in soccer games and theirattributes.
This content of the table is matchingwith the content of the ontology.
The event ta-ble is a flat structure and guides the informationextraction processes to generate the formal eventannotations.
The formal event annotations buildthe basis for answering user queries.
The eventtable is specified as an XML schema to constrainthe possibilities of annotation to what has beenagreed within the project consortium.5.5 Generation of Formal AnnotationsThe formal annotations are generated by the IEtechnology and are reflecting the typical output ofIE systems, i.e.instantiated domain-specific tem-plates or event tables.
The slots to be filled bythe systems are basically entities (player, teamsetc.
), relations (player of, opponents etc.)
andevents (goal, substitution etc.
), which are all de-rived from the current version of the domain on-tology and can be queried for in the online com-ponent of the MUMIS prototype.
All the tem-plates associated with an event are including atime slot to be filled if the corresponding informa-tion is available in a least one of the sources con-sulted during the IE procedure.
This time infor-mation is necessary for the indexing of the videomaterial.The IE systems are applying to distinct sources(FoT, FrT etc.)
but they are not concerned withachieving consistency in the IE result on distinctsources about the same event (game): this is thetask of the merging tools, described below.Since the distinct textual sources are differ-ently structured, from ?formal?
to ?free?
texts, theIE systems involved have adopted a modular ap-proach: regular expressions for the detection ofNamed Entities in the case of formal texts, fullshallow parsing for the free texts.
On the base ofthe factual information extracted from the formaltexts, the IE systems are also building dynamicdatabases on certain entities (like name and ageof the players, the clubs they are normally playingfor, etc.)
or certain metadata (final score), whichcan be used at the next level of processing.5.6 The Merging ToolThe distinct formal annotations generated arepassed to a merging component, which is respon-sible for avoiding both inconsistencies and redun-dancies in the annotations generated on one event(in our case a soccer game).In a sense one can consider this mergingcomponent as an extension of the so-called co-reference task of IE systems to a cross-document(and cross-lingual) reference resolution task.
Thedatabase generated during the IE process will helphere for operating reference resolution for more?verbose?
types of texts, which in the contextof soccer are quite ?poetic?
with respect to thenaming of agents (the ?Kaiser?
for Beckenbauer,the ?Bomber?
for Mueller etc...), which wouldbe quite difficult to achieve within the sole refer-ential information available within the boundaryof one document.
The project will also investi-gate here the use of inferential mechanisms forsupporting reference resolution.
So for example,?knowing?
from the formal texts the final scoreof a game and the names of the scorers, follow-ing formulation can be resolved form this kindof formulation in a free text (in any language):?With his decisive goal, the ?Bomber?
gave thevictory to his team.
?, whereas the special nam-ing ?Bomber?
can be further added to the entry?Mueller?The merging tools used in MUMIS will alsotake into consideration some general representa-tion of the domain-knowledge in order to filter outsome annotations generated in the former phases.The use of general representations9 (like domainframes), combined with inference mechanisms,might also support a better sequential organiza-tion of some event templates in larger scenarios.It will also allow to induce some events whichare not explicitly mentioned in the sources underconsideration (or which the IE systems might nothave detected).5.7 User Interface BuildingThe user first will interact with a web-portal tostart a MUMIS query session.
An applet will be9Like for example the Type Description Language(TDL), a formalism supporting all kind of operations on(typed) features as well as multiple inheritance, see (Kriegerand Schaefer, 1994).down-line loaded in case of showing the MUMISdemonstration.
This applet mainly offers a queryinterface.
The user then will enter a query thateither refers to metadata, formal annotations, orboth.
The MUMIS on-line system will searchfor all formal annotations that meet the criteriaof the query.
In doing so it will find the appro-priate meta-information and/or moments in somemedia recording.
In case of meta-information itwill simply offer the information in scrollable textwidgets.
This will be done in a structured waysuch that different type of information can eas-ily be detected by the user.
In case that scenes ofgames are the result of queries about formal anno-tations the user interface will first present selectedvideo keyframes as thumbnails with a direct indi-cation of the corresponding metadata.The user can then ask for more metadataabout the corresponding game or for more mediadata.
It has still to be decided within the projectwhether several layers of media data zooming inand out are useful to satisfy the user or whetherthe step directly to the corresponding video frag-ment is offered.
All can be invoked by simpleuser interactions such as clicking on the presentedscreen object.
Playing the media means playingthe video and corresponding audio fragment instreaming mode requested from a media server.6 Standards for Multimedia ContentMUMIS is looking for a compliance with exist-ing standards in the context of the processing ofmultimedia content on the computer and so willadhere to emerging standards such as MPEG4,which defines how different media objects will bedecoded and integrated at the receiving station,and MPEG7, which is about defining standardsfor annotations which can be seen as multime-dia objects.
Further, MUMIS will also maintainawareness of international discussions and devel-opments in the aerea of multimedia streaming(RTP, RTSP, JMF...), and will follow the discus-sions within the W3C consortium and the EBUwhich are also about standardizing descriptions ofmedia content.7 Role of MUMIS for the Annotation ofMultimedia ContentTo conclude, we would like to list the pointswhere we think MUMIS will, directly or indi-rectly, contribute to extract and access multimediacontent:  uses multimedia (MM) and multilingual in-formation sources;  carries out multimedia indexing by applyinginformation extraction to a well-delineateddomain and using already existing informa-tion as constraints;  uses and extends advanced language tech-nology to automatically create formal anno-tations for MM content;  merges information from many sourcesto improve the quality of the annotationdatabase;  application of IE to the output of ASR andthe combination of this with already existingknowledge;  definition of a complex information annota-tion structure, which is stored in a standarddocument type definition (DTD);  integration of new methods into a query in-terface which is guided by domain knowl-edge (ontology and multilingual lexica).So in a sense MUMIS is contributing in defin-ing semantic structures of multimedia contents,at the level proposed by domain-specific IE anal-ysis.
The full machinery of IE, combined withASR (and in the future with Image Analysis)can be used for multimedia contents developmentand so efficiently support cross-media (and cross-lingual) information retrieval and effective navi-gation within multimedia information interfaces.There seems thus that this technolgy can play ahighly relevant role for the purposes of knowl-edge detection and management.
This is prob-ably specially valid for the merging component,which is eliminating redundancies in the annota-tions generated from sets of documents and estab-lishing complex reference resolutions, thus sim-plyfying the access to content (and knowledge)distributed over multiple documents and media.ReferencesDoug E. Appelt.
1999.
An introduction to informationextraction.
AI Communications, 12.Steven Bird and Mark Liberman.
2001.
A formalframework for linguistic annotation.
Speech Com-munication.K.
Bontcheva, H. Brugman, A. Russel, P. Wittenburg,and H. Cunningham.
2000.
An Experiment inUnifying Audio-Visual and Textual Infrastructuresfor Language Processing R&D.
In Proceedings ofthe Workshop on Using Toolsets and ArchitecturesTo Build NLP Systems at COLING-2000, Luxem-bourg.
http://gate.ac.uk/.Daan Broeder, Hamish Cunningham, Nancy Ide,David Roy, Henry Thompson, and Peter Witten-burg, editors.
2000.
Meta-Descriptions and An-notation Schemes for Multimodal/Multimedia Lan-gauge Resources LREC-2000.H.
Brugman, K. Bontcheva, P. Wittenburg, andH.
Cunningham.
1999.
Integrating Multimedia andTextual Software Architectures for Language Tech-nology.
Technical report mpi-tg-99-1, Max-PlanckInstitute for Psycholinguistics, Nijmegen, Nethed-lands.Hamish Cunningham.
1999.
An introduction to infor-mation extraction.
Research memo CS - 99 - 07.Thierry Declerck and G. Neumann.
2000.
Using a pa-rameterisable and domain-adaptive information ex-traction system for annotating large-scale corpora?In Proceedings of the Workshop Information Ex-traction meets Corpus Linguistics, LREC-2000.Kevin Humphreys, R. Gaizauskas, S. Azzam,C.
Huyck, B. Mitchell, H. Cunningham, andY.
Wilks.
1998.
University of sheffield:Description of the lasie-ii system as used formuc-7.
In SAIC, editor, Proceedings of the7th Message Understanding Conference, MUC-7,http://www.muc.saic.com/.
SAIC Information Ex-traction.Christopher Kennedy and B. Boguraev.
1996.Anaphora for everyone: Pronominal anaphora res-olution without a parser.
In Proceedings of the16th International Conference on ComputationalLinguistics, COLING-96, pages 113?118, Copen-hagen.Hans-Ulrich Krieger and U. Schaefer.
1994.?a type description language for constraint-basedgrammars.
In Proceedings of the 15th Interna-tional Conference on Computational Linguistics,COLING-94, pages 893?899.Shalom Lappin and H-H. Shih.
1996.
A generalizedalgorithm for ellipsis resolution.
In Proceedingsof the 16th International Conference on Compu-tational Linguistics, COLING-96, pages 687?692,Copenhagen.John McNaught, editor.
2000.
Information Extractionmeets Corpus Linguistics, LREC-2000.Ruslan Mitkov.
1998.
Robust pronoun resolution withlimited knowledge.
In Proceedings of the 17th In-ternational Conference on Computational Linguis-tics, COLING-98, pages 869?875, Montreal.MUC, editor.
1995.
Sixth Message UnderstandingConference (MUC-6).
Morgan Kaufmann.MUC, editor.
1998.
Seventh Message UnderstandingConference (MUC-7), http://www.muc.saic.com/.SAIC Information Extraction.Guenter Neumann, R. Backofen, J. Baur, M. Becker,and C. Braun.
1997.
An information extrac-tion core system for real world german text pro-cessing.
In Proceedings of the 5th Conference onApplied Natural Language Processing, ANLP-97,pages 209?216.Guenter Neumann, C. Braun, and J. Piskorski.
2000.A divide-and-conquer strategy for shallow parsingof german free texts.
In Proceedings of the 6th Con-ference on Applied Natural Language Processing,ANLP-00.Jakub Piskorski and G. Neumann.
2000.
An intel-ligent text extraction and navigation system.
InProceedings of the 6th Conference on Recherched'Information Assiste?e par Ordinateur, RIAO-2000.Project URLs:COL:  		fffiflffi ff! #"$%&'sumDemoEUR: http://www.foyer.de/euromedia/GDA: http://www.csl.sony.co.jp/person/nagao/gda/INF: http://www.informedia.cs.cmu.edu/ISI: http://www.wins.uva.nl/research/isis/isisNS.htmlISLE: http://www.ilc.pi.cnr.it/EAGLES/ISLE_Home_Page.htmNSF: http://www.nsf.gov./od/lpa/news/press/pr9714.htmOLI: http://twentyone.tpd.tno.nl/olivePOP: http://twentyone.tpd.tno.nl/popeyeSUR:  		'()*fl+%)fifl-,)./%fi.$fi).10023node1.htmlTHI: http://www.dcs.shef.ac.uk/research/groups/spandh/projects/thislUMA: http://ciir.cs.umass.edu/research/UNL: http://www.ias.unu.edu/research_prog/science_technology/universalnetwork_language.htmlVIR: http://www.virage.com/8 AnnexEngland - Deutschland 1:0 (0:0)England: Seaman (2,5) - G. Neville (3,5), Keown (3), Camp-bell (2), P. Neville (4,5) - Ince (3,5), Wise (5) - Beckham (4),Scholes (3) - Shearer (3), Owen (5) - Trainer: KeeganDeutschland: Kahn (2) - Matthaeus (3) - Babbel (3,5),Nowotny (2,5) - Deisler (3), Hamann (2,5), Jeremies (3,5),Ziege (3,5) - Scholl (5) - Jancker (4), Kirsten (5) - Trainer:RibbeckEingewechselt: 61.
Gerrard fuer Owen, 72.
Barmby fuerScholes - 70.
Rink fuer Kirsten, 72.
Ballack fuer Deisler,78.
Bode fuer JeremiesTore: 1:0 Shearer (53., Kopfball, Vorarbeit Beckham)Schiedsrichter: Collina, Pierluigi (Viareggio), Note 2 - bisauf eine falsche Abseits-Entscheidung souveraen und sicherZuschauer: 30000 (ausverkauft)Gelbe Karten: Beckham - Babbel, JeremiesFigure 1: Example of a so-called formal text,where one can see that only 5 distinct time pointscan be extracted, concerning the player subsitu-tions (?Eingewechselt?)
and one goal (?Tore?
).Gruppe A: England - Deutschland 1:0 (0:0)7.
Ein Freistoss von Christian Ziege aus 25 Metern geht ue-ber das Tor.12.
Ziege flankt per Freistoss in den Strafraum und Jeremiesversucht es per Kofball, verfehlt den Kasten jedoch deutlich.16.
Scholes flankt gefaehrlich von der Torauslinie inden Fuenfmeterraum, doch Ziege hat aufgepasst und kannklaeren.18.
Hamann versucht es mit einem Distanzschuss aus 20Metern, aber Seaman ist auf dem Posten.23.
Scholl mit einer Riesenchance: Nach Zuspiel vonHamann rennt er in den englischen Strafraum, wird jedochgleich von drei Seiten bedraengt und kommt nur zu einemunplazierten Schuss, den Seaman sicher abfangen kann.27.
Jancker spielt auf Ziege, dessen Schuss von derStrafraumgrenze kann von Seaman abgefangen werden.35.
Michael Owen kommt nach Flanke von Philip Nevillevoellig frei vor dem deutschen Tor zum Kopfball, doch Kahnkann zum ersten Mal sein Koennen unter Beweis stellen undrettet auf der Linie.43.
Kahn zum zweiten: Beckham flankt auf Scholes, derzieht ab in den rechten Winkel, aber der deutsche Keeperverhindert erneut die englische Fuehrung.47.
Christian Zieges Freistoss aus 20 Metern geht einen hal-ben Meter ueber das Tor.53.
Beckham flankt per Freistoss an der deutschen Abwehrvorbei auf den Kopf von Alan Shearer, der voellig freiste-hend zum 1:0 fuer die Englaender verwandelt.58.
Scholl wird von Matthaeus bedient, aber sein Schussgeht aus halbrechter Position um Zentimeter am Tor vorbei.65.
Seaman kann nach einem Eckball vor Kirsten klaeren,der Nachschuss von Jancker geht knapp am Tor vorbei.Riesenmoeglichkeit fuer die DFB-Elf.Figure 2: Example of a so-called semi-formaltext, where one can see that here more time pointsare available, and that those can be complemen-tary to the time points to be extracted from formaltexts.
So, already at this level, a unification ormerging of extracted time points is necessary.
