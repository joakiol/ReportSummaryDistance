SELECTIVE PLANNING OF INTERFACE EVALUATION~William C. MannUSC Information Sciences Institute1 The Scope of EvaluationsThe basic ides behind evaluation is 8 simple one: An object isproduced and then subjected to trials of its I~trformance.
Observing thetrials revesJs things about the character of the object, and reasoningabout those observations leads tO stJ=tements about the "value" of theobject, a collection of such statements bein.3 &n "evaluation."
Anevaluation thus dlffe~ from a description, a critique or an estimate.For our purl:)oses here, the object is a database system with a naturallanguage interface for users.
Ideally.
the trials are an instrumentedvariant of normal uSage.
The character of the users, their tasks, thedata, and so forth are reDreeentative of the intended use of the system.In thinking about evaluations we need to be clear about the intendedscope.
Is it the whole system that is to be evaluated, or just the naturallanguage interface portion, or pos~bly both?
The decision is crucial forplanning the evaluation and understanding the results.
As we will see.choice of the whole system as the scope of evaluation leads t O ver~different designs than the choice of the interface module.
It is unlikelythat an evaluation which is supposed to cover both scopes will coverboth well.2 Different Plans for Different ConsumersWe can't expect a single form or method of evaluation to be suitable forall uses.
In planning to evaluate (or not to evaluate) it heil~ a great dealto identify the potential usor of the evaluation.There are some obvious prlncipis?1.
If we can't identify the consumer of the evaluation, don'tevaluate.2.
If something other than sn evaluation meets theconsumer's needs better, plan tO use it instearl.Who are the potential consumers?
Clearly they ate not the same as thesDonsors, who have often lost interest by the time an evaluation istimely.
Instead, they are:1.
Organizations that Might Use the System ..- Theseconsumers need a good overview of what the system cando.
Their evaluation must be hotistic, not an evaluation of amodule or of particular techniqueS, They need informalinformation, and possibly a formal system evaluation aswell.However, they may do beet with no evaluation at all.Communication theorists point out that there has neverbeen s comprehensive effectivenees study of thetelephone.
Telephone service is sold without suchevaluations.2.
Public Observers of the Art ..." ScienOata and thegeneral public alike have shown a great intermit in AI, and alegitimate concern over its social effects- The interest isespecially great in natural language precepting.
However,neatly all of them are like obsorvem of the recent spaceshuttle: They can understand liftoff, landing and some ofthe discus=dons of the heat of re(retry, but the critical detailsare completely out of reach.
Rather than carefullycontrolled evaluations, the public needs competent andhonest interpretations of the action.3.
The Implementers' Egos --.
Human self-acceptance andenjoyment of life are worthwhile goals, even for systemdesigners and iml=lementers, We aJl have e~o needs.
Thetrouble with using evaluations to meet them is that they cangive only too little, too late.
Praise and encouragementaJong the way would be not only more timely, but moreefficient.
Implementers who plan an evaluation as theirvindication or grand demonstration will almost surely befrustrated.
The evaluation can serve them no better thanreceiving an academic degree serves a student.
If theprocess of getting it hasn't been enjoyable, the finalcertification won't helD.4.
The Cultural Imperative ...
There may be no potentialconsumers of the evaluation at all, but the scientificsubculture may require one anyway.
We seem to haveasCenDed this one far more successfully than some fields ofpsychology, but we should Still avoid evaluations performedout of social habit.
Otherwise We will have something like aschool graduation, a big.
eiaJoorete, exbenalve NO,OP.5.
The Fixers -?- These I:~ople, almost inevitably some ofthe implementers, are interested in tuning up the system tomeet the needs of real usem.
They must move from theimplementation environment, driven by expectation andintuition, to a more taoistic world in which thoseexpectations are at least vulnerable.Such Customers cannot be served by the sort of broadholistic performance test the" may serve the public or theorganization that is about to acquire the system.
Instead,they need detailed, specific exercises of the sort that willsupport a causal model of how the system really functions.The best sort of evaluation will function as a tutor, providinglots of ?oecifi?, well distributed, detailed information.6.
The Research and Developmeht Community ...These are the AI and system development Deople fromoutside of the project.
They are like the engineers for Fordwho test Dstsuns on the track.
Like the implementerso theyneed dch detail to support causal models.
Simple, ho(iSticevaluations are entirely inadequate.7.
The Inspector --- There is another model of howevaluations function.
Its premises differ grossly from thoseu~d adore.
In this model, the results of the evaluation,whatever they are, can be discarded because they havenothing tO do with the real effects.
The effects come fromthe threat of an evaluation, and they are like the threatof a military inspection.
All of the valuable effects arecomplete before the ins~oection takes piece.Of course, in s mature and stable culture, the insl:~ctedlearns to know what to expect, and the parties cartdevelop the game to a high state of irrelevance.
Perhaps inAI the ins~Cter could still do some good.33t "Both the imptemantere and the researchers need a special kind of test.and for the same reeson: to support deaign, l The value ofevaluations for them is in its influence on future design activity.There are two interesting psttems in the observations above.
The firstis on the differing needs of "insiders" and "outsiders."?
The "outsiders" (public observers, potentialorgani;r.ations) need evaluations of the entire system, inrelatively simple terms, well supplemented by informalinterpretation and demonstration.?
The "insiders," researcher~ in the same field, fixers andimplementera, need complex, detailed evaluations that leadto many separate insights about the system at hand?
Theyare much more ready to cope with such complexity, and thevalue of their evaluation de~enas on having it.These neede are so different, and their characteristics o contradictor./.that we should expect that to serve both neeOs would require bNOdifferent evaluations.The second pattsm concerns relative benefits?
The benefits ofevaluations for "insiders" are immediate, tangible and hard to obtain inany other way.
They are potentially of great value, especially indirecting design.In contrast, the benefits of evaluations to "outsiders" are tenuous andarguable.
The option of performing an evaluation is often dominated bybetter methods and the option of not evaluating is sometimes attractive.The significance of this contrast is this:SYSTEM EVALUATION BENEFITS PRINCIPALLYTHOSE WHO ARE WITHIN THE SYSTEM DEVELOPMENTFIELD: iMPt.EMENTERS, RESEARCHERS, SYSTEMDESIGNERS AND OTHER MEMBERS OF THETECHNICAL COMMUNITY.
2It seems oiovious that evaluationa should therefore be plannedDnncipally for this community?As a result, the outcomes of evalustione tend to be ex~'emelyconditional.
The most defensible con?luaione are the most conditional-?
they say "This is what happena with these u~4,  these questions, thismuch system load..." Since those conditions will never cooccur again,such results are rather useless.The key to doing better is in creating results which can be generalizsd.Evaluation plans are in tension between the possibility of creating highlycredible but insignificant results on one hand and the I=osalbiUty ofcreating broad, general results without a credible amount of Support onthe other.f know no general solution to the problem of making evaluation resultsganeraliza/Die and significant.
We can observe what others have done,even in this book, and proceed in a case  by case manner.
Focusing ourattention on results for design will halb.Design proceeds from causal models of its subieot matter.
Evaluationresults should therefora be interpreted in cesual mode.
There is atendency, particularly when statistical results are involved, to avoidcausal interpretations.
This comes in ~ from the view that it is part ofthe nature of statistical models to not supbort causal intor~retetions.Avoiding causal interpretation is formally defensible, but entirelyinappropriate.
If the evaluation is to have effects and value, causalinterpretationa will be made?
They are inevitable in the normal course ofsuccessful activity.
They must be made, and so these interpret,=tionsshould be made by those best qualified to do so.Who should make me first causal interpretation of an e~tmtion?
Notthe consumers of the evaluation, but the evaluetors themselves.
Theyare in the best position tO do so, and the act of stating the interDrets~onia a kind of che~ on its plal~libility.By identifying the consumer, focumn 0 on consequences for dui~n,  andproviding causal interpretabons of r~ i ts ,  we can crest,, v,,,usioleevaluations.3 The Kay Problem: General izationWe have already noticed that evaluations can become very complex,with both good and bad effects.
The complexity comes from the tssk:Useful systems are complex, the knowledge they contain is complex,users are complex and natural language is complex.
Beyond all thaLplanning ?
test from which reliable conclusions can be drawn is itself acomptex matter.l~n the face of so much complexity, it is hoDelees to try to soan the fullrange of the phenomena of interest.
One must sample in a many.dimensional sO=ace, hoping to focus attention where conclusions areboth ac, cesalble anG ,significant.II~mgn hire.
-,, m mo~ ~ ?ons~m almost entirety of recleB~n.2Th,q is no( to say that ~e  anl not le~timate, important neecls anmng"ou~ecl'.
Son~mn@ musZ select lmon O commmcmlly offered am~cs?
CXOCum new?o~or  sy.Jcems and so form.
U~or~k'un4mtecy.
me imvaiim ~mat ion  lec~mgy dolenm e~m mmoteht sa~-oach ?
meth~l~ogy  lot msetm 0such ~ For ezamQle,is nothing com~IrlOCe to c43m1~r i0ef~cnmlrkin 9 methods for intm~cl~wl natuttllanguag(l im~/lu:R. It is not thM "ou1~m~der~" don't hlve imoortant needs: rlm~r, vm anl~any ~Wi~e= to n~m m41~ nml~l.34
