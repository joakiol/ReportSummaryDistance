Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 622?630,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsBridging SMT and TM with Translation RecommendationYifan He Yanjun Ma Josef van Genabith Andy WayCentre for Next Generation LocalisationSchool of ComputingDublin City University{yhe,yma,josef,away}@computing.dcu.ieAbstractWe propose a translation recommendationframework to integrate Statistical MachineTranslation (SMT) output with Transla-tion Memory (TM) systems.
The frame-work recommends SMT outputs to a TMuser when it predicts that SMT outputs aremore suitable for post-editing than the hitsprovided by the TM.
We describe an im-plementation of this framework using anSVM binary classifier.
We exploit meth-ods to fine-tune the classifier and inves-tigate a variety of features of differenttypes.
We rely on automatic MT evalua-tion metrics to approximate human judge-ments in our experiments.
Experimentalresults show that our system can achieve0.85 precision at 0.89 recall, excluding ex-act matches.
Furthermore, it is possible forthe end-user to achieve a desired balancebetween precision and recall by adjustingconfidence levels.1 IntroductionRecent years have witnessed rapid developmentsin statistical machine translation (SMT), with con-siderable improvements in translation quality.
Forcertain language pairs and applications, automatedtranslations are now beginning to be consideredacceptable, especially in domains where abundantparallel corpora exist.However, these advances are being adoptedonly slowly and somewhat reluctantly in profes-sional localization and post-editing environments.Post-editors have long relied on translation memo-ries (TMs) as the main technology assisting trans-lation, and are understandably reluctant to givethem up.
There are several simple reasons forthis: 1) TMs are useful; 2) TMs represent con-siderable effort and investment by a company or(even more so) an individual translator; 3) thefuzzy match score used in TMs offers a good ap-proximation of post-editing effort, which is usefulboth for translators and translation cost estimationand, 4) current SMT translation confidence esti-mation measures are not as robust as TM fuzzymatch scores and professional translators are thusnot ready to replace fuzzy match scores with SMTinternal quality measures.There has been some research to address this is-sue, see e.g.
(Specia et al, 2009a) and (Specia etal., 2009b).
However, to date most of the researchhas focused on better confidence measures for MT,e.g.
based on training regression models to per-form confidence estimation on scores assigned bypost-editors (cf.
Section 2).In this paper, we try to address the problemfrom a different perspective.
Given that most post-editing work is (still) based on TM output, we pro-pose to recommend MT outputs which are betterthan TM hits to post-editors.
In this framework,post-editors still work with the TM while benefit-ing from (better) SMT outputs; the assets in TMsare not wasted and TM fuzzy match scores canstill be used to estimate (the upper bound of) post-editing labor.There are three specific goals we need toachieve within this framework.
Firstly, the rec-ommendation should have high precision, other-wise it would be confusing for post-editors andmay negatively affect the lower bound of the post-editing effort.
Secondly, although we have fullaccess to the SMT system used in this paper,our method should be able to generalize to caseswhere SMT is treated as a black-box, which is of-622ten the case in the translation industry.
Finally,post-editors should be able to easily adjust the rec-ommendation threshold to particular requirementswithout having to retrain the model.In our framework, we recast translation recom-mendation as a binary classification (rather thanregression) problem using SVMs, perform RBFkernel parameter optimization, employ posteriorprobability-based confidence estimation to sup-port user-based tuning for precision and recall, ex-periment with feature sets involvingMT-, TM- andsystem-independent features, and use automaticMT evaluation metrics to simulate post-editing ef-fort.The rest of the paper is organized as follows: wefirst briefly introduce related research in Section 2,and review the classification SVMs in Section 3.We formulate the classification model in Section 4and present experiments in Section 5.
In Section6, we analyze the post-editing effort approximatedby the TER metric (Snover et al, 2006).
Section7 concludes the paper and points out avenues forfuture research.2 Related WorkPrevious research relating to this work mainly fo-cuses on predicting the MT quality.The first strand is confidence estimation for MT,initiated by (Ueffing et al, 2003), in which pos-terior probabilities on the word graph or N-bestlist are used to estimate the quality of MT out-puts.
The idea is explored more comprehensivelyin (Blatz et al, 2004).
These estimations are oftenused to rerank the MT output and to optimize itdirectly.
Extensions of this strand are presentedin (Quirk, 2004) and (Ueffing and Ney, 2005).The former experimented with confidence esti-mation with several different learning algorithms;the latter uses word-level confidence measures todetermine whether a particular translation choiceshould be accepted or rejected in an interactivetranslation system.The second strand of research focuses on com-bining TM information with an SMT system, sothat the SMT system can produce better target lan-guage output when there is an exact or close matchin the TM (Simard and Isabelle, 2009).
This lineof research is shown to help the performance ofMT, but is less relevant to our task in this paper.A third strand of research tries to incorporateconfidence measures into a post-editing environ-ment.
To the best of our knowledge, the first paperin this area is (Specia et al, 2009a).
Instead ofmodeling on translation quality (often measuredby automatic evaluation scores), this research usesregression on both the automatic scores and scoresassigned by post-editors.
The method is improvedin (Specia et al, 2009b), which applies InductiveConfidence Machines and a larger set of featuresto model post-editors?
judgement of the translationquality between ?good?
and ?bad?, or among threelevels of post-editing effort.Our research is more similar in spirit to the thirdstrand.
However, we use outputs and features fromthe TM explicitly; therefore instead of having tosolve a regression problem, we only have to solvea much easier binary prediction problem whichcan be integrated into TMs in a straightforwardmanner.
Because of this, the precision and recallscores reported in this paper are not directly com-parable to those in (Specia et al, 2009b) as the lat-ter are computed on a pure SMT system without aTM in the background.3 Support Vector Machines forTranslation Quality EstimationSVMs (Cortes and Vapnik, 1995) are binary clas-sifiers that classify an input instance based on de-cision rules which minimize the regularized errorfunction in (1):minw,b,?12wTw + Cl?i=1?is.
t.
yi(wT?
(xi) + b) > 1?
?i?i > 0(1)where (xi, yi) ?
Rn ?
{+1,?1} are l traininginstances that are mapped by the function ?
to ahigher dimensional space.
w is the weight vec-tor, ?
is the relaxation variable and C > 0 is thepenalty parameter.Solving SVMs is viable using the ?kerneltrick?
: finding a kernel function K in (1) withK(xi, xj) = ?(xi)T?(xj).
We perform our ex-periments with the Radial Basis Function (RBF)kernel, as in (2):K(xi, xj) = exp(?
?||xi ?
xj ||2), ?
> 0 (2)When using SVMs with the RBF kernel, wehave two free parameters to tune on: the cost pa-rameter C in (1) and the radius parameter ?
in (2).In each of our experimental settings, the param-eters C and ?
are optimized by a brute-force grid623search.
The classification result of each set of pa-rameters is evaluated by cross validation on thetraining set.4 Translation Recommendation asBinary ClassificationWe use an SVM binary classifier to predict the rel-ative quality of the SMT output to make a recom-mendation.
The SVM classifier uses features fromthe SMT system, the TM and additional linguis-tic features to estimate whether the SMT output isbetter than the hit from the TM.4.1 Problem FormulationAs we treat translation recommendation as a bi-nary classification problem, we have a pair of out-puts from TM and MT for each sentence.
Ideallythe classifier will recommend the output that needsless post-editing effort.
As large-scale annotateddata is not yet available for this task, we use auto-matic TER scores (Snover et al, 2006) as the mea-sure for the required post-editing effort.
In the fu-ture, we hope to train our system on HTER (TERwith human targeted references) scores (Snover etal., 2006) once the necessary human annotationsare in place.
In the meantime we use TER, as TERis shown to have high correlation with HTER.We label the training examples as in (3):y ={+1 if TER(MT) < TER(TM)?1 if TER(MT) ?
TER(TM) (3)Each instance is associated with a set of featuresfrom both the MT and TM outputs, which are dis-cussed in more detail in Section 4.3.4.2 Recommendation Confidence EstimationIn classical settings involving SVMs, confidencelevels are represented as margins of binary predic-tions.
However, these margins provide little in-sight for our application because the numbers areonly meaningful when compared to each other.What is more preferable is a probabilistic confi-dence score (e.g.
90% confidence) which is betterunderstood by post-editors and translators.We use the techniques proposed by (Platt, 1999)and improved by (Lin et al, 2007) to obtain theposterior probability of a classification, which isused as the confidence score in our system.Platt?s method estimates the posterior probabil-ity with a sigmod function, as in (4):Pr(y = 1|x) ?
PA,B(f) ?11 + exp(Af + B) (4)where f = f(x) is the decision function of theestimated SVM.
A and B are parameters that min-imize the cross-entropy error function F on thetraining data, as in Eq.
(5):minz=(A,B)F (z) = ?l?i=1(tilog(pi) + (1?
ti)log(1?
pi)),where pi = PA,B(fi), and ti ={N++1N++2if yi = +11N?+2if yi = ?1(5)where z = (A,B) is a parameter setting, andN+ and N?
are the numbers of observed positiveand negative examples, respectively, for the labelyi.
These numbers are obtained using an internalcross-validation on the training set.4.3 The Feature SetWe use three types of features in classification: theMT system features, the TM feature and system-independent features.4.3.1 The MT System FeaturesThese features include those typically used inSMT, namely the phrase-translation model scores,the language model probability, the distance-basedreordering score, the lexicalized reordering modelscores, and the word penalty.4.3.2 The TM FeatureThe TM feature is the fuzzy match (Sikes, 2007)cost of the TM hit.
The calculation of fuzzy matchscore itself is one of the core technologies in TMsystems and varies among different vendors.
Wecompute fuzzy match cost as the minimum EditDistance (Levenshtein, 1966) between the sourceand TM entry, normalized by the length of thesource as in (6), as most of the current implemen-tations are based on edit distance while allowingsome additional flexible matching.hfm(t) = mineEditDistance(s, e)Len(s) (6)where s is the source side of t, the sentence totranslate, and e is the source side of an entry in theTM.
For fuzzy match scores F , this fuzzy matchcost hfm roughly corresponds to 1?F .
The differ-ence in calculation does not influence classifica-tion, and allows direct comparison between a pureTM system and a translation recommendation sys-tem in Section 5.4.2.6244.3.3 System-Independent FeaturesWe use several features that are independent ofthe translation system, which are useful when athird-party translation service is used or the MTsystem is simply treated as a black-box.
Thesefeatures are source and target side LM scores,pseudo source fuzzy match scores and IBM model1 scores.Source-Side Language Model Score and Per-plexity.
We compute the language model (LM)score and perplexity of the input source sentenceon a LM trained on the source-side training data ofthe SMT system.
The inputs that have lower per-plexity or higher LM score are more similar to thedataset on which the SMT system is built.Target-Side Language Model Perplexity.
Wecompute the LM probability and perplexity of thetarget side as a measure of fluency.
Languagemodel perplexity of the MT outputs are calculated,and LM probability is already part of the MT sys-tems scores.
LM scores on TM outputs are alsocomputed, though they are not as informative asscores on the MT side, since TM outputs shouldbe grammatically perfect.The Pseudo-Source Fuzzy Match Score.
Wetranslate the output back to obtain a pseudo sourcesentence.
We compute the fuzzy match scorebetween the original source sentence and thispseudo-source.
If the MT/TM system performswell enough, these two sentences should be thesame or very similar.
Therefore, the fuzzy matchscore here gives an estimation of the confidencelevel of the output.
We compute this score for boththe MT output and the TM hit.The IBM Model 1 Score.
The fuzzy matchscore does not measure whether the hit could bea correct translation, i.e.
it does not take into ac-count the correspondence between the source andtarget, but rather only the source-side information.For the TM hit, the IBM Model 1 score (Brownet al, 1993) serves as a rough estimation of howgood a translation it is on the word level; for theMT output, on the other hand, it is a black-boxfeature to estimate translation quality when the in-formation from the translation model is not avail-able.
We compute bidirectional (source-to-targetand target-to-source) model 1 scores on both TMand MT outputs.5 Experiments5.1 Experimental SettingsOur raw data set is an English?French translationmemory with technical translation from Syman-tec, consisting of 51K sentence pairs.
We ran-domly selected 43K to train an SMT system andtranslated the English side of the remaining 8Ksentence pairs.
The average sentence length ofthe training set is 13.5 words and the size of thetraining set is comparable to the (larger) TMs usedin the industry.
Note that we remove the exactmatches in the TM from our dataset, because ex-act matches will be reused and not presented to thepost-editor in a typical TM setting.As for the SMT system, we use a stan-dard log-linear PB-SMT model (Och and Ney,2002): GIZA++ implementation of IBM wordalignment model 4,1 the refinement and phrase-extraction heuristics described in (Koehn etal., 2003), minimum-error-rate training (Och,2003), a 5-gram language model with Kneser-Neysmoothing (Kneser and Ney, 1995) trained withSRILM (Stolcke, 2002) on the English side of thetraining data, and Moses (Koehn et al, 2007) todecode.
We train a system in the opposite direc-tion using the same data to produce the pseudo-source sentences.We train the SVM classifier using the lib-SVM (Chang and Lin, 2001) toolkit.
The SVM-training and testing is performed on the remaining8K sentences with 4-fold cross validation.
We alsoreport 95% confidence intervals.The SVM hyper-parameters are tuned using thetraining data of the first fold in the 4-fold cross val-idation via a brute force grid search.
More specifi-cally, for parameterC in (1) we search in the range[2?5, 215], and for parameter ?
(2) we search in therange [2?15, 23].
The step size is 2 on the expo-nent.5.2 The Evaluation MetricsWe measure the quality of the classification byprecision and recall.
Let A be the set of recom-mended MT outputs, and B be the set of MT out-puts that have lower TER than TM hits.
We stan-dardly define precision P , recall R and F-value asin (7):1More specifically, we performed 5 iterations of Model 1,5 iterations of HMM, 3 iterations of Model 3, and 3 iterationsof Model 4.625P = |A?B||A| , R =|A?B||B| and F =2PRP + R (7)5.3 Recommendation ResultsIn Table 1, we report recommendation perfor-mance using MT and TM system features (SYS),system features plus system-independent features(ALL:SYS+SI), and system-independent featuresonly (SI).Table 1: Recommendation ResultsPrecision Recall F-ScoreSYS 82.53?1.17 96.44?0.68 88.95?.56SI 82.56?1.46 95.83?0.52 88.70?.65ALL 83.45?1.33 95.56?1.33 89.09?.24From Table 1, we observe that MT and TMsystem-internal features are very useful for pro-ducing a stable (as indicated by the smaller con-fidence interval) recommendation system (SYS).Interestingly, only using some simple system-external features as described in Section 4.3.3 canalso yield a system with reasonably good per-formance (SI).
We expect that the performancecan be further boosted by adding more syntacticand semantic features.
Combining all the system-internal and -external features leads to limitedgains in Precision and F-score compared to usingonly system-internal features (SYS) only.
This in-dicates that at the default confidence level, currentsystem-external (resp.
system-internal) featurescan only play a limited role in informing the sys-tem when current system-internal (resp.
system-external) features are available.
We show in Sec-tion 5.4.2 that combing both system-internal and -external features can yield higher, more stable pre-cision when adjusting the confidence levels of theclassifier.
Additionally, the performance of systemSI is promising given the fact that we are usingonly a limited number of simple features, whichdemonstrates a good prospect of applying our rec-ommendation system to MT systems where we donot have access to their internal features.5.4 Further Improving RecommendationPrecisionTable 1 shows that classification recall is veryhigh, which suggests that precision can still be im-proved, even though the F-score is not low.
Con-sidering that TM is the dominant technology usedby post-editors, a recommendation to replace thehit from the TM would require more confidence,i.e.
higher precision.
Ideally our aim is to obtaina level of 0.9 precision at the cost of some recall,if necessary.
We propose two methods to achievethis goal.5.4.1 Classifier MarginsWe experiment with different margins on the train-ing data to tune precision and recall in order toobtain a desired balance.
In the basic case, thetraining example would be marked as in (3).
If welabel both the training and test sets with this rule,the accuracy of the prediction will be maximized.We try to achieve higher precision by enforc-ing a larger bias towards negative examples in thetraining set so that some borderline positive in-stances would actually be labeled as negative, andthe classifier would have higher precision in theprediction stage as in (8).y ={+1 if TER(SMT) + b < TER(TM)?1 if TER(SMT) + b > TER(TM)(8)We experiment with b in [0, 0.25] usingMT sys-tem features and TM features.
Results are reportedin Table 2.Table 2: Classifier marginsPrecision RecallTER+0 83.45?1.33 95.56?1.33TER+0.05 82.41?1.23 94.41?1.01TER+0.10 84.53?0.98 88.81?0.89TER+0.15 85.24?0.91 87.08?2.38TER+0.20 87.59?0.57 75.86?2.70TER+0.25 89.29?0.93 66.67?2.53The highest accuracy and F-value is achievedby TER + 0, as all other settings are trainedon biased margins.
Except for a small drop inTER+0.05, other configurations all obtain higherprecision than TER+ 0.
We note that we can ob-tain 0.85 precision without a big sacrifice in recallwith b=0.15, but for larger improvements on pre-cision, recall will drop more rapidly.When we use b beyond 0.25, the margin be-comes less reliable, as the number of positiveexamples becomes too small.
In particular, thiscauses the SVM parameters we tune on in the firstfold to become less applicable to the other folds.This is one limitation of using biased margins to626obtain high precision.
The method presented inSection 5.4.2 is less influenced by this limitation.5.4.2 Adjusting Confidence LevelsAn alternative to using a biased margin is to outputa confidence score during prediction and to thresh-old on the confidence score.
It is also possible toadd this method to the SVM model trained with abiased margin.We use the SVM confidence estimation tech-niques in Section 4.2 to obtain the confidencelevel of the recommendation, and change the con-fidence threshold for recommendation when nec-essary.
This also allows us to compare directlyagainst a simple baseline inspired by TM users.
Ina TM environment, some users simply ignore TMhits below a certain fuzzy match score F (usuallyfrom 0.7 to 0.8).
This fuzzy match score reflectsthe confidence of recommending the TM hits.
Toobtain the confidence of recommending an SMToutput, our baseline (FM) uses fuzzy match costshFM ?
1?F (cf.
Section 4.3.2) for the TM hits asthe level of confidence.
In other words, the higherthe fuzzy match cost of the TM hit is (lower fuzzymatch score), the higher the confidence of recom-mending the SMT output.
We compare this base-line with the three settings in Section 5.0.70.750.80.850.90.9510.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9PrecisionConfidenceSISysAllFMFigure 1: Precision Changes with ConfidenceLevelFigure 1 shows that the precision curve of FMis low and flat when the fuzzy match costs arelow (from 0 to 0.6), indicating that it is unwise torecommend an SMT output when the TM hit hasa low fuzzy match cost (corresponding to higherfuzzy match score, from 0.4 to 1).
We also observethat the precision of the recommendation receivesa boost when the fuzzy match costs for the TMhits are above 0.7 (fuzzy match score lower than0.3), indicating that SMT output should be recom-mended when the TM hit has a high fuzzy matchcost (low fuzzy match score).
With this boost, theprecision of the baseline system can reach 0.85,demonstrating that a proper thresholding of fuzzymatch scores can be used effectively to discrimi-nate the recommendation of the TM hit from therecommendation of the SMT output.However, using the TM information only doesnot always find the easiest-to-edit translation.
Forexample, an excellent SMT output should be rec-ommended even if there exists a good TM hit (e.g.fuzzy match score is 0.7 or more).
On the otherhand, a misleading SMT output should not be rec-ommended if there exists a poor but useful TMmatch (e.g.
fuzzy match score is 0.2).Our system is able to tackle these complica-tions as it incorporates features from the MT andthe TM systems simultaneously.
Figure 1 showsthat both the SYS and the ALL setting consistentlyoutperform FM, indicating that our classificationscheme can better integrate the MT output into theTM system than this naive baseline.The SI feature set does not perform well whenthe confidence level is set above 0.85 (cf.
the de-scending tail of the SI curve in Figure 1).
Thismight indicate that this feature set is not reliableenough to extract the best translations.
How-ever, when the requirement on precision is not thathigh, and the MT-internal features are not avail-able, it would still be desirable to obtain transla-tion recommendations with these black-box fea-tures.
The difference between SYS and ALL isgenerally small, but ALL performs steadily betterin [0.5, 0,8].Table 3: Recall at Fixed PrecisionRecallSYS @85PREC 88.12?1.32SYS @90PREC 52.73?2.31SI @85PREC 87.33?1.53ALL @85PREC 88.57?1.95ALL @90PREC 51.92?4.285.5 Precision ConstraintsIn Table 3 we also present the recall scores at 0.85and 0.9 precision for SYS, SI and ALL models todemonstrate our system?s performance when thereis a hard constraint on precision.
Note that oursystem will return the TM entry when there is anexact match, so the overall precision of the system627is above the precision score we set here in a ma-ture TM environment, as a significant portion ofthe material to be translated will have a completematch in the TM system.In Table 3 for MODEL@K, the recall scores areachieved when the prediction precision is betterthan K with 0.95 confidence.
For each model, pre-cision at 0.85 can be obtained without a very bigloss on recall.
However, if we want to demandfurther recommendation precision (more conser-vative in recommending SMT output), the recalllevel will begin to drop more quickly.
If we useonly system-independent features (SI), we cannotachieve as high precision as with other modelseven if we sacrifice more recall.Based on these results, the users of the TM sys-tem can choose between precision and recall ac-cording to their own needs.
As the threshold doesnot involve training of the SMT system or theSVM classifier, the user is able to determine thistrade-off at runtime.Table 4: Contribution of FeaturesPrecision Recall F ScoreSYS 82.53?1.17 96.44?0.68 88.95?.56+M1 82.87?1.26 96.23?0.53 89.05?.52+LM 82.82?1.16 96.20?1.14 89.01?.23+PS 83.21?1.33 96.61?0.44 89.41?.845.6 Contribution of FeaturesIn Section 4.3.3 we suggested three sets ofsystem-independent features: features based onthe source- and target-side language model (LM),the IBMModel 1 (M1) and the fuzzy match scoreson pseudo-source (PS).
We compare the contribu-tion of these features in Table 4.In sum, all the three sets of system-independentfeatures improve the precision and F-scores of theMT and TM system features.
The improvementis not significant, but improvement on every set ofsystem-independent features gives some credit tothe capability of SI features, as does the fact thatSI features perform close to SYS features in Table1.6 Analysis of Post-Editing EffortA natural question on the integration models iswhether the classification reduces the effort of thetranslators and post-editors: after reading theserecommendations, will they translate/edit less thanthey would otherwise have to?
Ideally this ques-tion would be answered by human post-editors ina large-scale experimental setting.
As we havenot yet conducted a manual post-editing experi-ment, we conduct two sets of analyses, trying toshow which type of edits will be required for dif-ferent recommendation confidence levels.
We alsopresent possible methods for human evaluation atthe end of this section.6.1 Edit StatisticsWe provide the statistics of the number of editsfor each sentence with 0.95 confidence intervals,sorted by TER edit types.
Statistics of positive in-stances in classification (i.e.
the instances in whichMT output is recommended over the TM hit) aregiven in Table 5.When an MT output is recommended, its TMcounterpart will require a larger average numberof total edits than the MT output, as we expect.
Ifwe drill down, however, we also observe that manyof the saved edits come from the Substitution cat-egory, which is the most costly operation from thepost-editing perspective.
In this case, the recom-mended MT output actually saves more effort forthe editors than what is shown by the TER score.It reflects the fact that TM outputs are not actualtranslations, and might need heavier editing.Table 6 shows the statistics of negative instancesin classification (i.e.
the instances in which MToutput is not recommended over the TM hit).
Inthis case, the MT output requires considerablymore edits than the TM hits in terms of all fourTER edit types, i.e.
insertion, substitution, dele-tion and shift.
This reflects the fact that some highquality TM matches can be very useful as a trans-lation.6.2 Edit Statistics on Recommendations ofHigher ConfidenceWe present the edit statistics of recommendationswith higher confidence in Table 7.
Comparing Ta-bles 5 and 7, we see that if recommended withhigher confidence, the MT output will need sub-stantially less edits than the TM output: e.g.
3.28fewer substitutions on average.From the characteristics of the high confidencerecommendations, we suspect that these mainlycomprise harder to translate (i.e.
different fromthe SMT training set/TM database) sentences, asindicated by the slightly increased edit operations628Table 5: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.5Insertion Substitution Deletion ShiftMT 0.9849 ?
0.0408 2.2881 ?
0.0672 0.8686 ?
0.0370 1.2500 ?
0.0598TM 0.7762 ?
0.0408 4.5841 ?
0.1036 3.1567 ?
0.1120 1.2096 ?
0.0554Table 6: Edit Statistics when NOT Recommending MT Outputs in Classification, confidence=0.5Insertion Substitution Deletion ShiftMT 1.0830 ?
0.1167 2.2885 ?
0.1376 1.0964 ?
0.1137 1.5381 ?
0.1962TM 0.7554 ?
0.0376 1.5527 ?
0.1584 1.0090 ?
0.1850 0.4731 ?
0.1083Table 7: Edit Statistics when Recommending MT Outputs in Classification, confidence=0.85Insertion Substitution Deletion ShiftMT 1.1665 ?
0.0615 2.7334 ?
0.0969 1.0277 ?
0.0544 1.5549 ?
0.0899TM 0.8894 ?
0.0594 6.0085 ?
0.1501 4.1770 ?
0.1719 1.6727 ?
0.0846on the MT side.
TM produces much worse edit-candidates for such sentences, as indicated bythe numbers in Table 7, since TM does not havethe ability to automatically reconstruct an outputthrough the combination of several segments.6.3 Plan for Human EvaluationEvaluation with human post-editors is crucial tovalidate and improve translation recommendation.There are two possible avenues to pursue:?
Test our system on professional post-editors.By providing them with the TM output, theMT output and the one recommended to edit,we can measure the true accuracy of ourrecommendation, as well as the post-editingtime we save for the post-editors;?
Apply the presented method on open do-main data and evaluate it using crowd-sourcing.
It has been shown that crowd-sourcing tools, such as the Amazon Me-chanical Turk (Callison-Burch, 2009), canhelp developers to obtain good human judge-ments on MT output quality both cheaply andquickly.
Given that our problem is related toMT quality estimation in nature, it can poten-tially benefit from such tools as well.7 Conclusions and Future WorkIn this paper we present a classification model tointegrate SMT into a TM system, in order to facili-tate the work of post-editors.
Insodoing we handlethe problem of MT quality estimation as binaryprediction instead of regression.
From the post-editors?
perspective, they can continue to work intheir familiar TM environment, use the same cost-estimation methods, and at the same time bene-fit from the power of state-of-the-art MT.
We useSVMs to make these predictions, and use gridsearch to find better RBF kernel parameters.We explore features from inside the MT sys-tem, from the TM, as well as features that makeno assumption on the translation model for the bi-nary classification.
With these features we makeglass-box and black-box predictions.
Experimentsshow that the models can achieve 0.85 precision ata level of 0.89 recall, and even higher precision ifwe sacrifice more recall.
With this guarantee onprecision, our method can be used in a TM envi-ronment without changing the upper-bound of therelated cost estimation.Finally, we analyze the characteristics of the in-tegrated outputs.
We present results to show that,if measured by number, type and content of ed-its in TER, the recommended sentences producedby the classification model would bring about lesspost-editing effort than the TM outputs.This work can be extended in the followingways.
Most importantly, it is useful to test themodel in user studies, as proposed in Section 6.3.A user study can serve two purposes: 1) it canvalidate the effectiveness of the method by mea-suring the amount of edit effort it saves; and 2)the byproduct of the user study ?
post-edited sen-tences ?
can be used to generate HTER scoresto train a better recommendation model.
Further-more, we want to experiment and improve on theadaptability of this method, as the current experi-ment is on a specific domain and language pair.629AcknowledgementsThis research is supported by the Science Foundation Ireland(Grant 07/CE/I1142) as part of the Centre for Next Gener-ation Localisation (www.cngl.ie) at Dublin City University.We thank Symantec for providing the TM database and theanonymous reviewers for their insightful comments.ReferencesJohn Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis, andNicola Ueffing.
2004.
Confidence estimation for ma-chine translation.
In The 20th International Conferenceon Computational Linguistics (Coling-2004), pages 315 ?321, Geneva, Switzerland.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: parameter estimation.Computational Linguistics, 19(2):263 ?
311.Chris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Mechani-cal Turk.
In The 2009 Conference on Empirical Methodsin Natural Language Processing (EMNLP-2009), pages286 ?
295, Singapore.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIB-SVM: a library for support vector machines.
Soft-ware available at http://www.csie.ntu.edu.tw/?cjlin/libsvm.Corinna Cortes and Vladimir Vapnik.
1995.
Support-vectornetworks.
Machine learning, 20(3):273 ?
297.R.
Kneser and H. Ney.
1995.
Improved backing-off form-gram language modeling.
In The 1995 InternationalConference on Acoustics, Speech, and Signal Processing(ICASSP-95), pages 181 ?
184, Detroit, MI.Philipp.
Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In The 2003 Confer-ence of the North American Chapter of the Association forComputational Linguistics on Human Language Technol-ogy (NAACL/HLT-2003), pages 48 ?
54, Edmonton, Al-berta, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In The 45th Annual Meet-ing of the Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and Poster Ses-sions (ACL-2007), pages 177 ?
180, Prague, Czech Re-public.Vladimir Iosifovich Levenshtein.
1966.
Binary codes capa-ble of correcting deletions, insertions, and reversals.
So-viet Physics Doklady, 10(8):707 ?
710.Hsuan-Tien Lin, Chih-Jen Lin, and Ruby C. Weng.
2007.A note on platt?s probabilistic outputs for support vectormachines.
Machine Learning, 68(3):267 ?
276.Franz Josef Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statistical ma-chine translation.
In Proceedings of 40th Annual Meetingof the Association for Computational Linguistics (ACL-2002), pages 295 ?
302, Philadelphia, PA.Franz Josef Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In The 41st Annual Meet-ing on Association for Computational Linguistics (ACL-2003), pages 160 ?
167.John C. Platt.
1999.
Probabilistic outputs for support vectormachines and comparisons to regularized likelihood meth-ods.
Advances in Large Margin Classifiers, pages 61 ?
74.Christopher B. Quirk.
2004.
Training a sentence-level ma-chine translation confidence measure.
In The Fourth In-ternational Conference on Language Resources and Eval-uation (LREC-2004), pages 825 ?
828, Lisbon, Portugal.Richard Sikes.
2007.
Fuzzy matching in theory and practice.Multilingual, 18(6):39 ?
43.Michel Simard and Pierre Isabelle.
2009.
Phrase-basedmachine translation in a computer-assisted translation en-vironment.
In The Twelfth Machine Translation Sum-mit (MT Summit XII), pages 120 ?
127, Ottawa, Ontario,Canada.Matthew Snover, Bonnie Dorr, Richard Schwartz, LinneaMicciulla, and John Makhoul.
2006.
A study of transla-tion edit rate with targeted human annotation.
In The 2006conference of the Association for Machine Translation inthe Americas (AMTA-2006), pages 223 ?
231, Cambridge,MA.Lucia Specia, Nicola Cancedda, Marc Dymetman, MarcoTurchi, and Nello Cristianini.
2009a.
Estimating thesentence-level quality of machine translation systems.
InThe 13th Annual Conference of the European Associationfor Machine Translation (EAMT-2009), pages 28 ?
35,Barcelona, Spain.Lucia Specia, Craig Saunders, Marco Turchi, Zhuoran Wang,and John Shawe-Taylor.
2009b.
Improving the confidenceof machine translation quality estimates.
In The TwelfthMachine Translation Summit (MT Summit XII), pages 136?
143, Ottawa, Ontario, Canada.Andreas Stolcke.
2002.
SRILM-an extensible languagemodeling toolkit.
In The Seventh International Confer-ence on Spoken Language Processing, volume 2, pages901 ?
904, Denver, CO.Nicola Ueffing and Hermann Ney.
2005.
Applicationof word-level confidence measures in interactive statisti-cal machine translation.
In The Ninth Annual Confer-ence of the European Association for Machine Translation(EAMT-2005), pages 262 ?
270, Budapest, Hungary.Nicola Ueffing, Klaus Macherey, and Hermann Ney.
2003.Confidence measures for statistical machine translation.In The Ninth Machine Translation Summit (MT SummitIX), pages 394 ?
401, New Orleans, LA.630
