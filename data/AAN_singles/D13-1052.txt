Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 545?555,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsFlexible and Efficient Hypergraph Interactions for Joint Hierarchical andForest-to-String Decoding?Martin C?mejrek??
?IBM Prague Research LabV Parku 2294/4Prague, Czech Republic, 148 00martin.cmejrek@us.ibm.comHaitao Mi?
and Bowen Zhou?
?IBM T. J. Watson Research Center1101 Kitchawan RdYorktown Heights, NY 10598{hmi,zhou}@us.ibm.comAbstractMachine translation benefits from systemcombination.
We propose flexible interactionof hypergraphs as a novel technique combin-ing different translation models within one de-coder.
We introduce features controlling theinteractions between the two systems and ex-plore three interaction schemes of hiero andforest-to-string models?specification, gener-alization, and interchange.
The experimentsare carried out on large training data withstrong baselines utilizing rich sets of denseand sparse features.
All three schemes signif-icantly improve results of any single systemon four testsets.
We find that specification?amore constrained scheme that almost entirelyuses forest-to-string rules, but optionally useshiero rules for shorter spans?comes out asthe strongest, yielding improvement up to 0.9(Ter-Bleu)/2 points.
We also provide a de-tailed experimental and qualitative analysis ofthe results.1 IntroductionRecent years have witnessed the success of var-ious statistical machine translation (SMT) mod-els using different levels of linguistic knowledge?phrase (Koehn et al 2003), hiero (Chiang, 2005),and syntax-based (Liu et al 2006; Galley et al2006).
System combination became a promisingway of building up synergy from different SMT sys-tems and their specific merits.Numerous efforts that have been proposed in thisfield recently can be broadly divided into two cat-?M.
C?
and H. M. contributed equally to this work.egories: Offline system combination (Rosti et al2007; He et al 2008; Watanabe and Sumita, 2011;Denero et al 2010) aims at producing consensustranslations from the outputs of multiple individ-ual systems.
Those outputs usually contain k-bestlists of translations, which only explore a small por-tion of the entire search space of each system.
Thisissue is well addressed in joint decoding (Liu etal., 2009), or online system combination, showingcomparable improvements to the offline combina-tion methods.
Rather than finding consensus trans-lations from the outputs of individual systems, jointdecoding works with different grammars at the de-coding time.
Although limited to individual systemssharing the same search paradigm (e.g.
left-to-rightor bottom-up), joint decoding offers many poten-tial advatages: search through a larger space, bet-ter efficiency, features designed once for all subsys-tems, potential cross-system features, online sharingof partial hypotheses, and many others.Different approaches have different strengths ingeneral?hiero rules are believed to provide reliablelexical coverage, while tree-to-string rules are goodat non-local reorderings.
Different contexts presentdifferent challenges?noun phrases usually followthe adjacency principle, while verb phrases requiremore challenging reorderings.
In this work, we studydifferent schemes of interaction between translationmodels, reflecting their specific strengths at differ-ent (syntactic) contexts.
We make five new contribu-tions:First, we propose a framework for joint decod-ing by means of flexible combination of trans-lation hypergraphs, allowing for detailed con-545trol of interactions between the different sys-tems using soft constraints (Section 3).Second, we study three interaction schemes?special cases of joint decoding: generalization,specification, and interchange (Section 3.3).Third, instead of using a tree-to-string system,we use a much stronger forest-to-string sys-temwith fuzzy match of nonterminal categories(Section 2.1).Fourth, we train strong systems on a large-scale data set, and test all methods on four testsets.
Experimental results (Section 6) show thatour new approach brings improvement of up to0.9 points in terms of (Ter ?
Bleu)/2 over thebest single system.Fifth, we conduct a comprehensive experimen-tal analysis, and find that joint decoding actu-ally prefers tree-to-string rules in both shorterand longer spans.
(Section 6.3).The paper is organized as follows: We briefly re-view the individual models in Section 2, describethe method of joint decoding using three alternativeinteraction schemes in Section 3, describe the fea-tures controlling the interactions and fuzzy match inSection 4, review the related work in Section 5, andfinally, describe our experiments and give detaileddiscussion of the results in Section 6.2 Individual ModelsOur individual models are two state-of-the-art sys-tems: a hiero model (Chiang, 2005), and a forest-to-string model (Mi et al 2008; Mi and Huang, 2008).We will use the following example from Chineseto English to explain both individual and joint de-coding algorithms throughout this paper.SS ta?olu`nSSSSSSSS hu`iSSSSS ze?nmeya`ngdiscussion/NN SSS will/VV how/VVS discuss/VV SS meeting/NNThere are several possible meanings based on thedifferent POS tagging sequences:1: NN VV VV: How is the discussion going?2: VV NN VV: Discuss about the meeting.3: NN NN VV: How was the discussion meeting?4: VV VV VV: Discuss what will happen.id ruler1 VV(ta?olu`n) ?
discussr2 NP(ta?olu`n) ?
the discussionr3 NP(hu`i) ?
the meetingr4 VP(ze?nmeya`ng) ?
howr?4 VP(ze?nmeya`ng) ?
aboutr5 IP(x1:NP x2:VP) ?
x2 x1r6 IP(x1:VV x2:IP) ?
x1 x2r7 IP(x1:NP VP(VV(hu`i) x2:VP)) ?
x2 is x1 goingr11 X(x1:X ze?nmeya`ng) ?
how was x1r12 X(ze?nmeya`ng) ?
whatr13 X(ta?olu`n hu`i) ?
the discussion meetingr14 X(hu`i x1:X) ?
x1 will happenr15 S(x1:S x2:X) ?
x1 x2Table 1: Translation rules.
Tree-to-string (r1?r7), hiero(r11?r14), vanilla glue (r15).IPx1:NP VPVVhu`ix2:VP?
x2 is x1 goingFigure 1: Tree-to-string rule r7.Table 1 shows translation rules that can generateall four translations.
We will use those rules in thefollowing sections.2.1 Forest-to-stringForest-to-string translation (Mi et al 2008) is a lin-guistic syntax-based system, which significantly im-proves the translation quality of the tree-to-stringmodel (Liu et al 2006; Huang et al 2006) by usinga packed parse forest as the input instead of a singleparse tree.Figure 1 shows a tree-to-string translationrule (Huang et al 2006), which is a tuple?lhs(r), rhs(r), ?
(r)?, where lhs(r) is the source-sidetree fragment, whose internal nodes are labeled bynonterminal symbols (like NP and VP), and whosefrontier nodes are labeled by source-language words(like ?hu`i?)
or variables from a set X = {x1, x2, .
.
.
};rhs(r) is the target-side string expressed in target-language words (like ?going?)
and variables; and?
(r) is a mapping from X to nonterminals.
Each546(a)IP0, 3VV0, 1ta?olu`nNP0, 1IP1, 3NP1, 2hu`iVV1, 2VP1, 3VP2, 3ze?nmeya`ngRt?
(b)IP0, 3X0, 2VV0, 1ta?olu`nNP0, 1IP1, 3NP1, 2hu`iVV1, 2X1, 3 VP1, 3VP2, 3ze?nmeya`ngX0, 3e5e6e7?
Rh ?(b?
)IP0, 3X0, 2VV0, 1ta?olu`nNP0, 1IP1, 3NP1, 2hu`iVV1, 2X1, 3 VP1, 3VP2, 3ze?nmeya`ngX2, 3X0, 3e11e14?
(c)IP0, 3X0, 2VV0, 1ta?olu`nNP0, 1IP1, 3NP1, 2hu`iVV1, 2X1, 3 VP1, 3VP2, 3ze?nmeya`ngX2, 3X0, 3Figure 2: Parse and translation hypergraphs.
(a) The parse forest of the example sentence.
Solid hyperedges denotethe 1-best parse.
(b) The corresponding translation forest F t after applying the tree-to-string translation rule set Rt.Target lexical content is not shown.
Each translation hyperedge (e.g.
e7) has the same index as the corresponding rule(r7).
Gray nodes (e.g.
VP1,3) became inaccessible due to the insufficient rule coverage.
(b?)
The translation forest Fhafter applying the hierarchical rule set Rh to the input sentence.
(c) The combined translation forest Hm obtained bysuperimposing b and b?.
The nodes within each solid box share the same span.
See Figure 3 for an example of theinternal structure of a box.
The forest-to-string system can produce the translation 1 (dashed derivation: r2, r4 and r7)and 2 (solid derivation: r1, r3, r?4, r5, and r6).
Hierarchical rules generate the translation 3 (r11 and r13).
The translation4 is available by using joint decoding at X1, 3 ?
IP1, 3 with the derivation: r1, r6, r12, and r14.variable xi ?
X occurs exactly once in lhs(r) andexactly once in rhs(r).
Take the rule r7 in Figure 1for example, we have:lhs(r7) = IP(x1:NP VP(VV(hu`i) x2:VP)),rhs(r7) = x2 is x1 going,?
(r7) = {x1 7?
NP, x2 7?
VP}.Typically, a forest-to-string system performstranslation in two steps (shown in Figure 2): pars-ing and decoding.
In the parsing step, we convert thesource language input into a parse forest (a).
In thedecoding step, we first convert the parse forest into atranslation forest Ft in (b) by using the fast pattern-matching technique (Zhang et al 2009).
For exam-ple, we pattern-match the rule r7 rooted at IP0, 3, insuch a way that x1 spans NP0, 1 and x2 spans VP2, 3,and add a translation hyperedge e7 in (b).
Then thedecoder searches for the best derivation on the trans-lation forest and outputs the target string.2.2 HieroHiero (hierarchical phrase-based) model (Chiang,2005) acquires rules of synchronous context-freegrammars (SCFGs) from word-aligned parallel data,and uses plain sequences of words as the input, with-out any syntactic information.547FNIP?1, 3IP1, 3BBBBSNX?1, 3X1, 3EEEEscheme interaction edges in supernodeGeneralizationIP?1, 3 X?1, 3IP1, 3 X1, 3SpecificationIP?1, 3 X?1, 3IP1, 3 X1, 3InterchangeIP?1, 3 X?1, 3IP1, 3 X1, 3Figure 3: Three interaction schemes for joint decoding.Details of the interaction supernode for span (1, 3) shownin Figure 2 (c).
Soft constraints control the transitions.SCFG can be formalized as a set of tuples?lhs(r), rhs(r), ?
(r)?, where lhs(r) is the source-sideone-level CFG, whose root is X or S, and whosefrontier nodes are labeled by source-language words(like ?hu`i?)
or variables from a set X = {x1, x2, .
.
.
};rhs(r) is the target-side string expressed in target-language words (like ?going?)
and variables; and?
(r) is a mapping from X to nonterminals.
Table 1shows examples of hiero rules r11?r15.Although different on source side, hiero decod-ing can be formalized equally as forest-to-string de-coding: First, pattern-match the input sentence intoa translation forest Fh.
For example, since the ruler11 matches ?ze?nmeya`ng?
such that x1 spans the firsttwo words, add a hyperedge e11 in Figure 2 (b?
).Then search for the best derivation over the trans-lation forest.3 Joint DecodingThe goal of joint decoding is to let different MTmodels collaborate within the framework of a singledecoder.
This can be done by combining translationhypergraphs of the different models at the decod-ing time, so that online sharing of partial hypothesesovercomes weaknesses and boosts strengths of thesystems combined.As both forest-to-string and hiero produce trans-lation forests that share the same hypergraph struc-ture, we first formalize the hypergraph, then we in-troduce an algorithm to combine different hyper-graphs, and finally we describe three joint decodingschemes over the merged hypergraph.3.1 HypergraphsMore formally, a hypergraph H is a pair ?V, E?,where V is the set of nodes, and E the set of hyper-edges.
For a given sentence w1:l = w1 .
.
.wl, eachnode v ?
V is in the form of Y i, j, where Y is anonterminal in the context-free grammar1 and i, j,0 ?
i < j ?
l, are string positions in the sentencew1:l, which denote the recognition of nonterminalY spanning the substring from positions i through j(that is, wi+1 .
.
.w j).
Each hyperedge e ?
E is a tuple?tails(e), head(e), target(e)?, where head(e) ?
V isthe consequent node in the deductive step, tails(e) ?V?
is the list of antecedent nodes, and target(e) isa list of rhs(r) for rules r such that each rule r hasthe same lhs(r) pattern-matched at the node head(e).For example, the hyperedge e7 in Figure 2 (b) ise7 = ?
(NP0, 1,VP2, 3), IP0, 3, (x2 is x1 going)?,where we can infer the mapping to be{x1 7?
NP0, 1, x2 7?
VP2, 3 }.We also denote BS(v) to be the set of incominghyperedges of node v, which represent the differentways of deriving v. For example, BS(IP0, 3) is a setof e7 and e6.There is also a distinguished root node TOP ineach hypergraph, denoting the goal item in transla-tion, which is simply TOP0, l.3.2 Combining HypergraphsWe enable interaction between translation hyper-graphs, such as hiero Fh = ?Vh, Eh?
and forest-to-string Ft = ?V t, Et?, on nodes covering the samespan (e.g.
IP1, 3 and X1, 3 in Figure 2 (c) grouped ina box).
We call such groups interaction supernodesand show a detailed example of a supernode for span(1, 3) in Figure 3.The combination runs in four steps:1In this paper, nonterminal labels X and S denote hieroderivations, other labels are tree-to-string labels.5481.
For each node v = Y i, j, v ?
Vh ?
V t, we createa new interaction node v?
= Y ?i, j with emptyBS (v?).
For example, we create two nodes,IP?1, 3 and X?1, 3, at the top of Figure 3.2.
For each hyperedge e ?
BS(v), v ?
V t ?
Vh,we replace each v in tails(e) with v?.
For exam-ple, e7 becomes ?
(NP?0, 1,VP?2, 3), IP0, 3, (x2 isx1 going)?.3.
All the nodes and hyperedges form the mergedhypergraph Fm, such as in Figure 2 (c).4.
Insert interaction hyperedges connecting nodeswithin each interaction supernode to make Fmconnected again.In the following subsection we present details of in-teractions and introduce three alternative schemes.3.3 Three Schemes of Joint DecodingInteraction hyperedges within each supernode allowthe decoder either to stay within the same system(e.g.
in hiero using X1, 3 ?
X?1, 3 in Figure 3), or toswitch to the other (e.g.
to forest-to-string using X1, 3?
IP?1, 3).For example, translation 4 can be produced asfollows: The source string ?ze?nmeya`ng?
is trans-lated by the phrase rule r12.
The hiero hyperedgee14 combines it with the translation of ?hu`i?, reach-ing the hiero node X1, 3.
Using the interaction edgeX1, 3 ?
IP?1, 3 will switch into the tree-to-stringmodel, so that the translation can be completed withthe tree-to-string edge e6 that connects it with a par-tial tree-to string translation of ?ta?olu`n?
done by r1.In order to achieve more precise control over theinteraction between tree-to-string and hiero deriva-tions, we propose the following three basic inter-action schemes: generalization, specification, in-terchange.
The schemes control the interaction be-tween hiero and tree-to-string models by means ofsoft constraints.
Some schemes may even restrictcertain types of transitions.
The schemes are de-picted in Figure 3 and their details are discussed inthe following three subsections.3.3.1 SpecificationThe specification decoding scheme reflects the in-tuition of using hiero rules to translate shorter spansand tree-to-string rules to reorder higher-level sen-tence structures.
In other words, the scheme allowsone-way switching from the hiero general nontermi-nal into the more specific nonterminal of a tree-to-string rule.
Transitions in reverse directions are notallowed.
This is achieved by inserting specificationinteraction hyperedges e leading from hiero nodesXi, j or Si, j into all tree-to-string interaction nodesY?i, j within the same supernode.3.3.2 GeneralizationIn some translation domains, hiero outperformstree-to-string systems, as was shown in experimentsin Section 6.
While local hiero or tree-to-string re-orderings perform well, long distance reorderingsproposed by tree-to-string may be too risky (e.g.
dueto parsing errors), so that monotone concatenationof long sequences2 is the more reliable strategy.
Thegeneralization decoding scheme, complementary tothe specification, is motivated by the idea of incorpo-rating reliable tree-to-string translations for some se-quences into a strong hiero translation system.
Thisis achieved by inserting generalization interactionhyperedges e leading from tree-to-string nodes Yi, jnodes into general hiero interaction nodes X?i, j andS?i, j within the same supernode.3.3.3 InterchangeThe interchange decoding scheme is a union ofthe two previous approaches.
Any derivation canfreely combine hiero and tree-to-string productions.Both specification and generalization interactionhyperedges are inserted leading from all hiero andtree-to-string nodes Xi, j, Si, j, and Yi, j into all inter-action nodes X?i, j, S?i, j, and Y?i, j.3.4 Fuzzy matchThe translation rule set cannot usually cover allhyperedges in the parse forest, thus some nodesbecome inaccessible in the translation forest (e.g.VP1, 3 in Figure 2).
However, in the parse forest, asopposed to a 1-best tree, we can find other nodesspanning the same sequence wi: j (e.g.
node IP1, 3).In order to re-enable inaccessible nodes and to in-crease the variability of the translation forest, weallow reaching them from the other tree-to-string2Monotone glue is the only possibility for very long spansexceeding the hiero maxParse treshold.549nodes within the same interaction node.
This canbe achieved by adding fuzzy hyperedges betweenevery tree-to-string state Y i, j and a differently la-beled tree-to-string interaction state Z?i, j.
For exam-ple, in the span (0,1), we have a fuzzy hyperedgeVV0, 1 ?
NP?0, 1.While interaction hyperedges combine differenttranslation models, fuzzy hyperedges combine dif-ferent derivations within the same (tree-to-string)model.4 Interaction FeaturesOur baseline systems use the log-linear frameworkto estimate the probability P(D) of a derivation Dfrom features ?i and their weights ?i as P(D) ?exp(?i ?i?i).
Similarly as Chiang et al(2009), oursystems use tens of dense (e.g.
language models,translation probabilities) and thousands of sparse(e.g.
lexical, fertility) features.The features related to the joint decoding experi-ments are the costs for specification, generalization,interchange, and the fuzzy match.
Let Lt be the setof the labels used by the source language parser andLh = {S,X} be the labels used by hiero.The generalization feature?Y?Z = |{e; e ?
D,?i, j tails(e) = {Yi, j} (1)?head(e) = Z?i, j}|is the total number of generalization hyperedges inD going from tree-to-string states Y ?
Lt to hierostates Z?
?
Lh.The specification feature?Z?Y = |{e; e ?
D,?i, j tails(e) = {Zi, j} (2)?head(e) = Y?i, j}|is the total number of specification hyperedges in Dgoing from hiero states Z ?
Lh to tree-to-string statesY ?
?
Lt.The interchange feature is implemented by en-abling the generalization and specification featuresat the same time for both tuning and testing.The fuzzy match feature?U?W = |{e; e ?
D,?i, j tails(e) = {Ui, j} (3)?head(e) = W?i, j}|is the total number of fuzzy match hyperedges in Dgoing from tree-to-tree statesU ?
Lt to tree-to-stringstates W?
?
Lt. 3We use MIRA to obtain weights for the new fea-tures by tuning on the development set.
The num-ber of new parameters to tune can be estimated as|Lh| ?
|Lt| for generalization and specification, and2 ?
|Lh| ?
|Lt| for interchange.
For the fuzzy matchof tree-to-string nonterminals we have |Lt| ?
|Lt| pa-rameters organized as a sparse matrix, since we onlyconsider combinations on nonterminal labels thatcooccur in the data.45 Related WorkFrom the previous explorations of online translationmodel combination, we see the work of Liu et al(2009) proposing an unconstrained combination ofhiero and tree-to-string models as a special configu-ration of our framework, and we also replicate it.Denero et al(2010) combine translation mod-els even with different search paradigms.
Their ap-proach is different, since their component systemsdo not interact at decoding time, instead, each ofthem provides its weighted translation forest first,the forests are then combined to infer a new com-bination model.6 ExperimentIn this section we describe the setup, present results,and analyze the experiments.
Finally, we propose fu-ture directions of research.3Here we allow U = W, which can be viewed in such a waythat exact match is a special case of fuzzy match.4We also carried out an alternative experiment with onlythree fuzzy match features estimated from the training dataparse forest by Na?
?ve Bayes by observing all spans in the train-ing data, accumulating counts Cs(U) and Cs(U,W) of nonter-minals (or pairs of nonterminals) heading the same span s. Thefirst two features (one for each direction) are based on condi-tional probabilities:?
(U |W) = ?
log(?s?spans Cs (U,W)?s?spans Cs(W)).
(4)The third feature is based on joint probability:?
(U,W) = ?
log(?s?spans Cs(U,W)?s?spans,A,B?Lt Cs(A, B)).
(5)The average performance drops by 0.1 (Ter-Bleu)/2 points,compared to the interchange eperiment.550SystemGALE-web P1R6-web MT08 news MT08 web Avg.Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 Bleu (T-B)/2 (T-B)/2SingleT2S 32.6 11.6 16.9 23.5 37.7 7.8 28.1 14.5 14.4Hiero 33.7 10.2 17.0 23.1 39.2 6.3 28.8 13.7 13.3F2S 34.0 10.3 17.3 23.2 39.6 6.3 29.2 13.6 13.4JointLiu:09 34.1 9.7 17.0 23.0 38.8 6.7 29.0 13.2 13.2Gen.
34.4 9.7 17.8 22.6 40.0 6.1 29.6 13.1 12.9Spe.
35.1 9.4 18.1 22.2 40.2 5.8 29.6 12.9 12.6Int.
34.9 9.4 17.9 22.3 40.0 6.2 29.6 12.9 12.7Table 2: All results of single and joint decoding systems.6.1 SetupThe training corpus consists of 16 million sen-tence pairs available within the DARPA BOLTChinese-English task.
The corpus includes a mixof newswire, broadcast news, webblog and comesfrom various sources such as LDC, HK Law, HKHansard and UN data.
The Chinese text is seg-mented with a segmenter trained on CTB data usingconditional random fields (CRF).
Language modelsare trained on the English side of the parallel cor-pus, and on monolingual corpora, such as Gigaword(LDC2011T07) and Google News, altogether com-prising around 10 billion words.We use a modified version of the Berkeley parser(Petrov and Klein, 2007) to obtain a parse forestfor each training sentence, then we prune it withthe marginal probability-based inside-outside algo-rithm to contain only 3n CFG nodes, where n is thesentence length.
Finally, we apply the forest-basedGHKM algorithm (Mi and Huang, 2008; Galley etal., 2004) to extract tree-to-string translation rulesfrom forest-string pairs.In the decoding step, we prune the input hyper-graphs to 10n nodes before we use fast pattern-matching (Zhang et al 2009) to convert the parseforest into the translation forest.We tune on 1275 sentences, each with 4 refer-ences, from the LDC2010E30 corpus, initially re-leased under the DARPA GALE program.All MT experiments are optimized withMIRA (Crammer et al 2006) to maximize(Ter-Bleu)/2.We test on four different test sets: GALE-web testset from LDC2010E30 corpus (1239 sentences, 4references), P1R6-web test set from LDC2012E124corpus (1124 sentences, 1 reference), NIST MT08newswire portion (691 sentences, 4 references), andNIST MT08 web portion (666 sentences, 4 refer-ences).6.2 ResultsTable 2 shows all results of single and joint decodingsystems.
The Bleu score of the single hiero baselineis 39.2 on MT08-news, showing that it is a strongsystem.
The single F2S baseline achieves compara-ble scores on all four test sets.Then, for reference, we present results of joint Hi-ero and T2S decoding, which is, to our knowledge, astrong and competitive reimplementaion of the workdescribed by Liu et al(2009).
Finally, we present re-sults of joint decoding of hiero and F2S in three in-teraction schemes: generalization, specification, andinterchange.All three combination schemes significantly im-prove results of any single system on all four test-sets.
On average and measured in (Ter-Bleu)/2,our systems improve the best single system by 0.4(generalization), 0.7 (specification), and 0.6 (inter-change).The specification comes out as the strongest inter-action scheme, beating the second interchange on 2testsets by 0.1 and 0.4 (Ter-Bleu)/2 points and on 3testsets by 0.2 Bleu points.6.3 Discussion of ResultsInterpretations of model behavior with thousands ofparameters that may possibly overlap and interfereshould be always attempted with caution.
In this sec-tion we highlight some interesting observations, ac-551Specification Generalization InterchangeX ?
?
?
?
X X ?
?
?
?
XVPIPVVNRADVPQPCCDVPNPP...CSCPADVRDPUADJPDNPPPPRNDP0.0690.0590.0530.0320.0250.0230.0170.0170.0170.012...-0.005-0.007-0.011-0.012-0.028-0.028-0.045-0.064-0.069-0.092QPPPNNDPNRDNPNPLCDECDEG...VVPRNPNBAVPVRDJJVCDFLPU0.0570.0540.0480.0440.0340.0320.0300.0250.0230.023...-0.010-0.011-0.013-0.015-0.015-0.028-0.035-0.037-0.054-0.073VVVPNNQPADVPLCPNPPIPNR...VSBPNPUMVRDDNPADJPPPDPPRN0.0620.0440.0340.0250.0220.0210.0180.0170.0160.016...-0.004-0.004-0.004-0.007-0.014-0.023-0.039-0.058-0.070-0.080NNPPCPLCPDEGDPDECQPLCNP...FLRDVPBAJJASVRDADVPPNDFLPU0.0480.0410.0350.0350.0310.0280.0270.0270.0210.019...-0.006-0.009-0.010-0.011-0.014-0.017-0.021-0.033-0.038-0.103Table 3: Examples of specification, generalization, and interchange weights.
POS tags in italics.0123456789101112131415161718ADVP CLPADJPFLRDFL DPVCPVSBVRDVCD QP NP DVPDNPLCP PP VP CPPRN IPFRAGAveragespanlengthFigure 4: Average span length for selected syntactic la-bels on GALE-web test set.companying them with our subjective judgementsand speculations.Table 3 shows the specification and generalizationfeatures tuned for the three combination schemes,then sorted by their weights ?X?Y or ?Y?X .
Featuresshown at the top of the table are very expensive (the#Interactions Generalization Inter.
gen.F2S ?
glue 5557 4202F2S ?
hiero 695 1178total gen. 6252 5380Specification Inter.
spec.phrase ?
F2S 2763 2235glue ?
F2S 946 841hiero ?
F2S 683 839total spec.
4392 3915Table 5: Rule interactions on GALE-web test set.system tries to avoid them), while inexpensive fea-tures are at the bottom (the system is encouraged touse them).The most expensive interactions for the specifi-cation belong to constituents (IP, VP) that usuallyoccur higher in a syntactic tree (see Figure 4 for av-erage span lengths of selected syntactic labels), andoften require non-local reorderings.
This indicatesthat the decoder is discouraged from switching fromhiero into F2S derivation at these higher-level spans.552rule type Generalization Specification InterchangeF2S 18,807 58% 19,399 70% 18,400 61%Hiero 3,730 12% 2,330 8% 3,133 10%Glue 7,367 23% 571 2% 4,714 16%Phrase 2,274 7% 5,484 20% 3,868 13%total 32,178 27,784 30,115Table 4: Rule counts on GALE-web test set.10^010^110^210^310^40  5  10  15  20  25  30  35  40  45  50  55  60  65  70Number of rulesSpan lengthGeneralizationF2SHieroGluePhrase10^010^110^210^310^40  5  10  15  20  25  30  35  40  45  50  55  60  65  70Number of rulesSpan lengthSpecificationF2SHieroGluePhrase10^010^110^210^310^40  5  10  15  20  25  30  35  40  45  50  55  60  65  70Number of rulesSpan lengthInterchangeF2SHieroGluePhraseFigure 5: Rule distributions on GALE-web test set.The third most expensive feature belongs to apart-of-speech tag?the preterminal VV.
We mayhypothesize that it shows the importance of lexicalinformation for the precision of reordering typicallycarried out within (parent) VP nodes, and/or the im-portance of POS information for succesful disam-biguation of word senses in translation.
Ideally, thesystem can use a VP rule with a lexicalized VV.
Lesspreferably, the VV part has to be translated by an-other T2S rule (losing the lexical constraint).
In theworst case, the system has to use a hiero hypothe-sis to translate the VV part (losing the syntactic con-straint), risking imprecise translation, since the hierorule is not constrained to senses corresponding to thesource POS VV.
Again, the high penalty discouragesfrom using the hiero derivation in this context.On the other hand, the bottom of the table showslabels that encourage using hiero?DP, PP, DNP,ADJP, etc.
?shorter phrases that tend to be monotoneand less ambiguous.Similar interpretations seem plausible when ex-amining the generalization experiment.
Expensivefeatures related to preterminals (NR, NN, CD) maysuggest two alternative principles: First, using F2Srules for thes POS categories and then switching tohiero is discouraged, since these contexts are morereliably handled by hiero due to better lexical cover-age and common adjacency in nominal categories.Second, since there is only one attempt to switchfrom F2S derivation to hiero, letting F2S completeeven larger spans (and maybe switching to hierolater) is favorable.The tail of generalization feature weights is moredifficult to interpret.
The discount on VP encouragesdecoder to use F2S for entire verb phrases beforeswitching to hiero, on the other hand, other verb-related preterminals occupy the tail as well, hurryinginto early switching from F2S to hiero.553Finally, the feature weights tuned for the in-terchange experiment are divided into two sub-columns.
Both generalization and specificationweights show similar trends as in the previous twointeraction schemes, although blurred (VP and IPdescending from the absolute top).
Since transitionsin both ways are allowed, the search space is big-ger and the system may behave differently.
It is evenpossible for a path in the hypergraph to zigzag be-tween F2S and hiero nodes to collect interaction dis-counts, ?diluting?
the syntactic homogeneity of thehypothesis.Figure 5 and Tables 4 and 5 show rule distribu-tions, total rule counts, and numbers of interactionsof different types for the three interaction schemeson the GALE-web test set.
The scope of phrase rulesis limited to 6 words.
The scope of hiero rules is lim-ited to 20 words by the commonly used maxParseparameter, leaving longer spans to the glue rule.The trends of F2S and glue rules show the mostobvious difference.
In the generalization, F2S rulestranslate spans of up to 50 words.
Glue rules pre-vail on spans longer then 7 words.
The specificationis reversed, pushing the longest scope of hiero andglue rules down to 40 words, completing the longestsentences entirely with F2S.
The interchange comesout as a mixture of the previous two trends.All three schemes prefer using F2S rules atshorter spans, to the contrary of our original assump-tion of phrasal and hiero rules being stronger on lo-cal contexts in general.
Here we may refer againto the specification feature weights for preterminalsVV, NR, CC and P in Table 3 and to our previouslystated hypothesis about the importance of preservinglexical and syntactic context.Hiero rules usage on longer spans drops fastestfor specification, slowest for generalization, and inbetween for interchange.It is also interesting to notice the trends on veryshort spans (2?4 words) shown by rule distributionsand reflected in numbers of interaction types.
Whilespecification often transitions from a single phraserule directly into F2S, the interchange has relativelyhigher counts of hiero rules, another sign of the hieroand F2S interaction.Synthesizing from several sources of indicationsis difficult, however, we arrive at the conclusion thatjoint decoding of hiero and F2S significantly im-proves the performance.
While the single systemsshow similar performance, their roles are not bal-anced in joint decoding.
It seems that the role of hi-ero consists in enabling F2S in most contexts.We have focused on three special cases of inter-action.
We see a great potential in further studiesof other schemes, allowing more flexible interactionthan simple specification, but still more constrainedthan the interchange.
It seems also promising to re-fine the interaction modeling with features takinginto account more information than a single syntac-tic label, and to explore additional ways of parame-ter estimation.7 ConclusionWe have proposed flexible interaction of hyper-graphs as a novel technique combining hieroand forest-to-string translation models within onedecoder.
We have explored three basic interac-tion schemes?specification, generalization, andinterchange?and described soft constraints control-ling the interactions.
We have carried out experi-ments on large training data and with strong base-lines.
Of the three schemes, the specification showsthe highest gains, achieving improvements from 0.5to 0.9 (Ter-Bleu)/2 points over the best single sys-tem.
We have conducted a detailed analysis of eachsystem output based on different indications of inter-actions, discussed possible interpretations of results,and finally offered our conclusion and proposed fu-ture lines of research.AcknowledgmentsWe thank Jir???
Havelka for proofreading and help-ful suggestions.
We would like to acknowledge thesupport of DARPA under Grant HR0011-12-C-0015for funding part of this work.
The views, opinions,and/or findings contained in this article/presentationare those of the author/presenter and should not beinterpreted as representing the official views or poli-cies, either expressed or implied, of the DARPA.ReferencesDavid Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine translation.In Proceedings of HLT-NAACL, pages 218?226.554David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL, pages 263?270, Ann Arbor, Michigan, June.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585.John Denero, Shankar Kumar, Ciprian Chelba, and FranzOch.
2010.
Model combination for machine transla-tion.
In In Proceedings NAACL-HLT, pages 975?983.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT-NAACL, pages 273?280.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING-ACL, pages 961?968, Sydney, Aus-tralia, July.Xiaodong He, Mei Yang, Jianfeng Gao, Patrick Nguyen,and Robert Moore.
2008.
Indirect-HMM-based hy-pothesis alignment for combining outputs from ma-chine translation systems.
In Proceedings of EMNLP,pages 98?107, October.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA, pages66?73.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL, pages 127?133.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING-ACL, pages 609?616.Yang Liu, Haitao Mi, Yang Feng, and Qun Liu.
2009.Joint decoding with multiple translation models.
InProceedings of ACL-IJCNLP, pages 576?584, August.Haitao Mi and Liang Huang.
2008.
Forest-based transla-tion rule extraction.
In Proceedings of EMNLP, pages206?214.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL: HLT, pages192?199.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL, pages 404?411.Antti-Veikko Rosti, Spyros Matsoukas, and RichardSchwartz.
2007.
Improved word-level system com-bination for machine translation.
In Proceedings ofACL, pages 312?319, Prague, Czech Republic, June.Taro Watanabe and Eiichiro Sumita.
2011.
Machinetranslation system combination by confusion forest.
InProceedings of ACL 2011, pages 1249?1257.Hui Zhang, Min Zhang, Haizhou Li, and Chew LimTan.
2009.
Fast translation rule matching for syntax-based statistical machine translation.
In Proceedingsof EMNLP, pages 1037?1045, Singapore, August.555
