Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 200?209,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPNon-Projective Parsing for Statistical Machine TranslationXavier Carreras Michael CollinsMIT CSAIL, Cambridge, MA 02139, USA{carreras,mcollins}@csail.mit.eduAbstractWe describe a novel approach for syntax-based statistical MT, which builds on avariant of tree adjoining grammar (TAG).Inspired by work in discriminative depen-dency parsing, the key idea in our ap-proach is to allow highly flexible reorder-ing operations during parsing, in combina-tion with a discriminative model that cancondition on rich features of the source-language string.
Experiments on trans-lation from German to English show im-provements over phrase-based systems,both in terms of BLEU scores and in hu-man evaluations.1 IntroductionSyntax-based models for statistical machine trans-lation (SMT) have recently shown impressive re-sults; many such approaches are based on ei-ther synchronous grammars (e.g., (Chiang, 2005)),or tree transducers (e.g., (Marcu et al, 2006)).This paper describes an alternative approach forsyntax-based SMT, which directly leverages meth-ods from non-projective dependency parsing.
Thekey idea in our approach is to allow highly flexiblereordering operations, in combination with a dis-criminative model that can condition on rich fea-tures of the source-language input string.Our approach builds on a variant of tree adjoin-ing grammar (TAG; (Joshi and Schabes, 1997))(specifically, the formalism of (Carreras et al,2008)).
The models we describe make use ofphrasal entries augmented with subtrees that pro-vide syntactic information in the target language.As one example, when translating the sentencewir mu?ssen auch diese kritik ernst nehmen fromGerman into English, the following sequence ofsyntactic phrasal entries might be used (we showeach English syntactic fragment above its associ-ated German sub-string):SNPweVPmust ADVPalsoNPthese criticismsADVPseriouslyVPtakewir mu?ssen auch diese kritik ernst nehmenTAG parsing operations are then used to combinethese fragments into a full parse tree, giving thefinal English translation we must also take thesecriticisms seriously.Some key aspects of our approach are as fol-lows:?
We impose no constraints on entries in thephrasal lexicon.
The method thereby retains thefull set of lexical entries of phrase-based systems(e.g., (Koehn et al, 2003)).1?
The model allows a straightforward integra-tion of lexicalized syntactic language models?forexample the models of (Charniak, 2001)?in addi-tion to a surface language model.?
The operations used to combine tree frag-ments into a complete parse tree are signifi-cant generalizations of standard parsing operationsfound in TAG; specifically, they are modified to behighly flexible, potentially allowing any possiblepermutation (reordering) of the initial fragments.As one example of the type of parsing opera-tions that we will consider, we might allow thetree fragments shown above for these criticismsand take to be combined to form a new structurewith the sub-string take these criticisms.
This stepin the derivation is necessary to achieve the correctEnglish word order, and is novel in a couple of re-spects: first, these criticisms is initially seen to theleft of take, but after the adjunction this order isreversed; second, and more unusually, the treeletfor seriously has been skipped over, with the re-sult that the German words translated at this point(diese, kritik, and nehmen) form a non-contiguoussequence.
More generally, we will allow any two1Note that in the above example each English phrase con-sists of a completely connected syntactic structure; this is not,however, a required constraint, see section 3.2 for discussion.200tree fragments to be combined during the transla-tion process, irrespective of the reorderings whichare introduced, or the non-projectivity of the pars-ing operations that are required.The use of flexible parsing operations raises twochallenges that will be a major focus of this paper.First, these operations will allow the model to cap-ture complex reordering phenomena, but will inaddition introduce many spurious possibilities.
In-spired by work in discriminative dependency pars-ing (e.g., (McDonald et al, 2005)), we add proba-bilistic constraints to the model through a discrim-inative model that links lexical dependencies in thetarget language to features of the source languagestring.
We also investigate hard constraints on thedependency structures that are created during pars-ing.
Second, there is a need to develop efficientdecoding algorithms for the models.
We describeapproximate search methods that involve a signif-icant extension of decoding algorithms originallydeveloped for phrase-based translation systems.Experiments on translation from German to En-glish show a 0.5% improvement in BLEU scoreover a phrase-based system.
Human evaluationsshow that the syntax-based system gives a sig-nificant improvement over the phrase-based sys-tem.
The discriminative dependency model givesa 1.5% BLEU point improvement over a basicmodel that does not condition on the source lan-guage string; the hard constraints on dependencystructures give a 0.8% BLEU improvement.2 Relationship to Previous WorkA number of syntax-based translation systemshave framed translation as a parsing problem,where search for the most probable translation isachieved using algorithms that are generalizationsof conventional parsing methods.
Early examplesof this work include (Alshawi, 1996; Wu, 1997);more recent models include (Yamada and Knight,2001; Eisner, 2003; Melamed, 2004; Zhang andGildea, 2005; Chiang, 2005; Quirk et al, 2005;Marcu et al, 2006; Zollmann and Venugopal,2006; Nesson et al, 2006; Cherry, 2008; Mi etal., 2008; Shen et al, 2008).
The majority ofthese methods make use of synchronous gram-mars, or tree transducers, which operate over parsetrees in the source and/or target languages.
Re-ordering rules are typically specified through rota-tions or transductions stated at the level of context-free rules, or larger fragments, within parse trees.These rules can be learned automatically from cor-pora.A critical difference in our work is to allowarbitrary reorderings of the source language sen-tence (as in phrase-based systems), through theuse of flexible parsing operations.
Rather thanstating reordering rules at the level of source ortarget language parse trees, we capture reorder-ing phenomena using a discriminative dependencymodel.
Other factors that distinguish us from pre-vious work are the use of all phrases proposed by aphrase-based system, and the use of a dependencylanguage model that also incorporates constituentinformation (although see (Charniak et al, 2003;Shen et al, 2008) for related approaches).3 A Syntactic Translation Model3.1 BackgroundOur work builds on the variant of tree adjoin-ing grammar (TAG) introduced by (Carreras etal., 2008).
In this formalism the basic unitsin the grammar are spines, which associate treefragments with lexical items.
These spines canbe combined using a sister-adjunction operation(Rambow et al, 1995), to form larger pieces ofstructure.2 For example, we might have the fol-lowing operation:NPthereSVPis?
SNPthereVPisIn this case the spine for there has sister-adjoinedinto the S node in the spine for is; we re-fer to the spine for there as being the modifierspine, and the spine for is being the head spine.There are close connections to dependency for-malisms: in particular in this operation we seea lexical dependency between the modifier wordthere and the head word is.
It is possible to de-fine syntactic language models, similar to (Char-niak, 2001), which associate probabilities withthese dependencies, roughly speaking of the formP (wm, sm|wh, sh, pos, ?
), where wmand smarethe identities of the modifier word and spine, whand share the identities of the head word andspine, pos is the position in the head spine that isbeing adjoined into, and ?
is some additional state(e.g., state that tracks previous modifiers that haveadjoined into the same spine).2We also make use of the r-adjunction operation defined in(Carreras et al, 2008), which, together with sister-adjunction,allows us to model the full range of structures found in thePenn treebank.201SNPthereVPis NPNPBno hierarchyPPof NPdiscriminationes gibt keine hierarchie der diskriminierungFigure 1: A training example consisting of an English (tar-get language) tree and a German (source language) sentence.In this paper we will also consider treelets,which are a generalization of spines, and whichallow lexical entries that include more than oneword.
These treelets can again be combined us-ing a sister-adjunction operation.
As an example,consider the following operation:VPbe ADJPableSGto VPrespond?
VPbe ADJPable SGto VPrespondIn this case the treelet for to respond sister-adjoinsinto the treelet for be able.
This operation intro-duces a bi-lexical dependency between the modi-fier word to and the head word able.3.2 S-phrasesThis section describes how phrase entries fromphrase-based translation systems can be modifiedto include associated English syntactic structures.These syntactic phrase-entries (from here on re-ferred to as ?s-phrases?)
will form the basis of thetranslation models that we describe.We extract s-phrases from training examplesconsisting of a source-language string paired witha target-language parse tree.
For example, con-sider the training example in figure 1.
We as-sume some method that enumerates a set of pos-sible phrase entries for each training example:each phrase entry is a pair ?
(i, j), (k, l)?
speci-fying that source-language words fi.
.
.
fjcorre-spond to target-language words ek.
.
.
elin the ex-ample.
For example, one phrase entry for the ex-ample might be ?
(1, 2), (1, 2)?, representing thepair ?es gibt ?
there is?.
In our experimentswe use standard methods in phrase-based systems(Koehn et al, 2003) to define the set of phrase en-tries for each sentence in training data.es gibt keine hierarchie derSNPthereVPisDTnoNPNPBhierarchyPPofFigure 2: Example syntactic phrase entries.
We show Ger-man sub-strings above their associated sequence of treelets.4For each phrase entry, we add syntactic infor-mation to the English string.
To continue our ex-ample, the resulting entry would be as follows:es gibt ?
SNPthereVPisTo give a more formal description of how syn-tactic structures are derived for phrases, first notethat each parse tree t is mapped to a TAG deriva-tion using the method described in (Carreras et al,2008).
This procedure uses the head finding rulesof (Collins, 1997).
The resulting derivation con-sists of a TAG spine for each word seen in the sen-tence, together with a set of adjunction operationswhich each involve a modifier spine and a headspine.
Given an English string e = e1.
.
.
en, withan associated parse tree t, the syntactic structureassociated with a substring ek.
.
.
el(e.g., there is)is then defined as follows:?
For each word in the English sub-string, in-clude its associated TAG spine in t.?
In addition, include any adjunction operationsin t where both the head and modifier word are inthe sub-string ej.
.
.
ek.In the above example, the resulting structure(i.e., the structure for there is) is a single treelet.In other cases, however, we may get a sequence oftreelets, which are disconnected from each other.For example, another likely phrase-entry for thistraining example is ?es gibt keine ?
there is no?resulting in the first lexical entry in figure 2, whichhas two treelets.
Allowing s-phrases with multipletreelets ensures that all phrases used by phrase-based systems can be used within our approach.As a final step, we add additional align-ment information to each s-phrase.
Con-sider an s-phrase which contains source-languagewords f1.
.
.
fnpaired with target-language wordse1.
.
.
em.
The alignment information is a vec-tor ?
(a1, b1) .
.
.
(am, bm)?
that specifies for eachword eiits alignment to words fai.
.
.
fbiin thesource language.
For example, for the phrase en-202try ?es gibt ?
there is?
a correct alignment wouldbe ?
(1, 1), (2, 2)?, specifying that there is alignedto es, and is is aligned to gibt (note that in many,but not all, cases ai= bi, i.e., a target languageword is aligned to a single source language word).The alignment information in s-phrases willbe useful in tying syntactic dependencies cre-ated in the target language to positions in thesource language string.
In particular, we will con-sider discriminative models (analogous to modelsfor dependency parsing, e.g., see (McDonald etal., 2005)) that estimate the probability of target-language dependencies conditioned on propertiesof the source-language string.
Alignments may bederived in a number of ways; in our method wedirectly use phrase entries proposed by a phrase-based system.
Specifically, for each target word eiin a phrase entry ?f1.
.
.
fn, e1.
.
.
em?
for a train-ing example, we find the smallest5 phrase entryin the same training example that includes eionthe target side, and is a subset of f1.
.
.
fnon thesource side; the word eiis then aligned to the sub-set of source language words in this ?minimal?phrase.In conclusion, s-phrases are defined as follows:Definition 1 An s-phrase is a 4-tuple ?f, e, t, a?where: f is a sequence of foreign words; e isa sequence of English words; t is a sequence oftreelets specifying a TAG spine for each Englishword, and potentially some adjunctions betweenthese spines; and a is an alignment.
For an s-phrase q we will sometimes refer to the 4 elementsof q as f(q), e(q), t(q) and a(q).3.3 The ModelWe now introduce a model that makes use of s-phrases, and which is flexible in the reorderingsthat it allows.
To provide some intuition, and somemotivation for the use of reordering operations,figure 3 gives several examples of German stringswhich have different word orders from English.The crucial idea will be to use TAG adjunctionoperations to combine treelets to form a completeparse tree, but with a complete relaxation on theorder in which the treelets are combined.
For ex-ample, consider again the example given in theintroduction to this paper.
In the first step of aderivation that builds on these treelets, the treelet5The ?size?
of a phrase entry is defined to be ns+ ntwhere nsis the number of source language words in thephrase, ntis the number of target language words.1(a) [die verwaltung] [muss] [ku?nftig] [schneller] [reagieren][ko?nnen] 1(b) the administration must be able to respondmore quickly in future1(c) NPtheadmin.
.
.SVPmustPPin futureADVPmorequicklySGto VPrespondVPbe ADJPable2(a) [meiner ansicht nach] [darf] [der erweiterungsprozess][nicht] [unno?tig] [verzo?gert] [werden] 2(b) in my opinion theexpansion process should not be delayed unnecessarily2(c) PPin myopinionSVPshouldNPthe .
.
.
processRBnotADVPunnecessarilyVPdelayedVPbeFigure 3: Examples of translations.
In each example (a)is the original German string, with a possible segmentationmarked with ?[?
and ?]?
; (b) is a translation for (a); and (c)is a sequence of phrase entries, including syntactic structures,for the segmentation given in (a).for these criticisms might adjoin into the treelet fortake, giving the following new sequence:SNPweVPmust ADVPalsoADVPseriouslyVPVtakeNPthese criticismsIn the next derivation step seriously is adjoined tothe right of take, giving the following treelets:SNPweVPmust ADVPalsoVPVtakeNPthese criticismsADVPseriouslyIn the final step the second treelet adjoins into theVP above must, giving a parse tree for the stringwe must also take these criticisms seriously, andcompleting the translation.Formally, given an input sentence f , a derivationd is a pair ?q, pi?
where:?
q = q1.
.
.
qnis a sequence of s-phrases suchthat f = f(q1)?f(q2)?
.
.
.
?f(qn) (where u?vdenotes the concatenation of strings u and v).?
pi is a set of adjunction operations thatconnects the sequence of treelets contained in?t(q1), t(q2), .
.
.
, t(qn)?
into a parse tree in thetarget language.
The operations allow a com-plete relaxation of word order, potentially allow-ing any of the n!
possible orderings of the n s-phrases.
We make use of both sister-adjunctionand r-adjunction operations, as defined in (Car-reras et al, 2008).66In principle we allow any treelet to adjoin into any othertreelet?for example there are no hard, grammar-based con-straints ruling out the combination of certain pairs of non-terminals.
Note however that in some cases operations willhave probability 0 under the syntactic language model intro-duced later in this section.203DTnoNPNPBhierarchyPPofNPdiscrimination?
NPNPBhierarchyPPofNPDTnodiscriminationFigure 4: A spurious derivation step.
The treelets arisefrom [keine] [hierarchie der] [diskriminierung].Given a derivation d = ?q, pi?, we define e(d)to be the target-language string defined by thederivation, and t(d) to be the complete target-language parse tree created by the derivation.
Themost likely derivation for a foreign sentence fis argmaxd?G(f)score(d), where G(f) is the setof possible derivations for f , and the score for aderivation is defined as7score(d) = scoreLM(e(d)) + scoreSY N(t(d))+ scoreR(d) +n?j=1scoreP(qj) (1)The components of the model are as follows:?
scoreLM(e(d)) is the log probability of theEnglish string under a trigram language model.?
scoreSY N(t(d)) is the log probability of theEnglish parse tree under a syntactic languagemodel, similar to (Charniak, 2001), that associatesprobabilities with lexical dependencies.?
scoreR(d) will be used to score the pars-ing operations in pi, based on the source-languagestring and the alignments in the s-phrases.
Thispart of the model is described extensively in sec-tion 4.1 of this paper.?
scoreP(q) is the score for an s-phrase q.This score is a log-linear combination of var-ious features, including features that are com-monly found in phrase-based systems: for exam-ple logP (f(q)|e(q)), log P (e(q)|f(q)), and lex-ical translation probabilities.
In addition, we in-clude a feature logP (t(q)|f(q), e(q)), which cap-tures the probability of the phrase in question hav-ing the syntactic structure t(q).Note that a model that includes the termsscoreLM(e(d)) and?nj=1scoreP(qj) alonewould essentially be a basic phrase-basedmodel (with no distortion terms).
The termsscoreSY N(t(d)) and scoreR(d) add syntacticinformation to this basic model.A key motivation for this model is the flexibilityof the reordering operations that it allows.
How-ever, the approach raises two major challenges:7In practice, MERT training (Och, 2003) will be used totrain relative weights for the different model components.Constraints on reorderings.
Relaxing the op-erations in the parsing model will allow complexreorderings to be captured, but will also introducemany spurious possibilities.
As one example, con-sider the derivation step shown in figure 4.
Thisstep may receive a high probability from a syntac-tic or surface language model?no discriminationis a quite plausible NP in English?but it shouldbe ruled out for other reasons, for example be-cause it does not respect the dependencies in theoriginal German (i.e., keine/no is not a modifierto diskriminierung/discrimination in the Germanstring).
The challenge will be to develop eitherhard constraints which rule out spurious derivationsteps such as these, or soft constraints, encapsu-lated in scoreR(d), which penalize them.Efficient search.
Exact search for the derivationwhich maximizes the score in Eq.
1 cannot beaccomplished efficiently using dynamic program-ming (as in phrase-based systems, it is easy toshow that the decoding problem is NP-complete).Approximate search methods will be needed.The next two sections of this paper describe so-lutions to these two challenges.4 Constraints on Reorderings4.1 A Discriminative Dependency ModelWe now describe the model scoreRintroduced inthe previous section.
Recall that pi specifies k ad-junction operations that are used to build a fullparse tree, where k ?
n is the number of treeletswithin the sequence of s-phrases q = ?q1.
.
.
qn?.Each of the k adjunction operations creates adependency between a modifier word wmwithina phrase qm, and a head word whwithin a phraseqh.
For example, in the example in section 3.3where these criticisms was combined with take,the modifier word is criticisms and the head wordis take.
The modifier and head words have TAGspines smand shrespectively.
In addition we candefine (am, bm) to be the start and end indices ofthe words in the foreign string to which the wordwmis aligned; this information can be recoveredbecause the s-phrase qmcontains alignment infor-mation for all target words in the phrase, includ-ing wm.
Similarly, we can define (ah, bh) to bealignment information for the head word wh.
Fi-nally, we can define ?
to be a binary flag speci-fying whether or not the adjunction operation in-volves reordering (in the take criticism example,this flag is set to true, because the order in En-204VPDT NNPNcriticismsthese takenehmenernstwir mu?ssen auch diese kritikFigure 5: An adjunction operation that involves the mod-ifier criticisms and the head take.
The phrases involved areunderlined; the dotted lines show alignments within s-phrasesbetween English words and positions in the German string.The ?-dependency in this case includes the head and modi-fier words, together with their spines, and their alignments topositions in the German string (kritik and nehmen).glish is reversed from that in German).
This leadsto the following definition:Definition 2 Given a derivation d = ?q, pi?, wedefine ?
(d) to be the set of ?-dependenciesin d. Each ?-dependency is a tuple?wm, sm, am, bm, wh, sh, ah, bh, ??
of elements asdescribed above.Figure 5 gives an illustration of how an adjunctioncreates one such ?-dependency.The model is then defined asscoreR(d) =????
(d)scorer(?, f)where scorer(?, f) is a score associated with the?-dependency ?.
This score can potentially besensitive to any information in ?
or the source-language string f ; in particular, note that the align-ment indices (am, bm) and (ah, bh) essentiallyanchor the target-language dependency to posi-tions in the source-language string, allowing thescore for the dependency to be based on featuresthat have been widely used in discriminative de-pendency parsing, for example features based onthe proximity of the two positions in the source-language string, the part-of-speech tags in the sur-rounding context, and so on.
These features havebeen shown to be powerful in the context of regu-lar dependency parsing, and our intent is to lever-age them in the translation problem.In our model, we define scoreras follows.
Weestimate a model P (y|?, f) where y ?
{?1,+1},and y = +1 indicates that a dependency does existbetween wmand wh, and y = ?1 indicates that adependency does not exist.
We then definescorer(?, f) = logP (+1|?, f)To estimate P (y|?, f), we first extract a set of la-beled training examples of the form ?yi, ?i, fi?
fori = 1 .
.
.
N from our training data as follows:for each pair of target-language words (wm, wh)seen in the training data, we can extract associ-ated spines (sm, sh) from the relevant parse tree,and also extract a label y indicating whether or nota head-modifier dependency is seen between thetwo words in the parse tree.
Given an s-phrase inthe training example that includes wm, we can ex-tract alignment information (am, bm) from the s-phrase; we can extract similar information (ah, bh)for wh.
The end result is a training example of theform ?y, ?, f?.8 We then estimate P (y|?, f) usinga simple backed-off model that takes into accountthe identity of the two spines, the value for the flagr, the distance between (am, bm) and (ah, bh), andpart-of-speech information in the source language.4.2 Contiguity of pi-ConstituentsWe now describe a second type of constraint,which limits the amount of non-projectivity inderivations.
Consider again the k adjunction op-erations in pi, which are used to connect treeletsinto a full parse tree.
Each adjunction operationinvolves a head treelet that dominates a modifiertreelet.
Thus for any treelet t, we can consider itsdescendants, that is, the entire set of treelets thatare directly or indirectly dominated by t. We de-fine a pi-constituent for treelet t to be the subsetof source-language words dominated by t and itsdescendants.
We then introduce the following con-straint on pi-constituents:Definition 3 (pi-constituent constraint.)
A pi-constituent is contiguous iff it consists of a con-tiguous sequence of words in the source language.A derivation pi satisfies the pi-constituent con-straint iff all pi-constituents that it contains arecontiguous.In this paper we constrain all derivations to sat-isfy the pi-constituent constraint (future work mayconsider probabilistic versions of the constraint).The intuition behind the constraint deservesmore discussion.
The constraint specifies that themodifiers to each treelet can appear in any or-der around the treelet, with arbitrary reorderingsor non-projective operations.
However, once atreelet has taken all its modifiers, the resulting pi-constituent must form a contiguous sub-sequence8To be precise, there may be multiple (or even zero) s-phrases which include wmor wh, and these s-phrases mayinclude conflicting alignment information.
Given nmdiffer-ent alignments seen for wm, and nhdifferent alignments seenfor wh, we create nm?nhtraining examples, which includeall possible combinations of alignments.205of the source-language string.
As one set of exam-ples, consider the translations in figure 3, and theexample given in the introduction.
These exam-ples involve reordering of arguments and adjunctswithin clauses, a very common case of reorderingin translation from German to English.
The re-orderings in these translations are quite flexible,but in all cases satisfy the pi-constituent constraint.As an illustration of a derivation that violatesthe constraint, consider again the derivation stepshown in figure 4.
This step has formed a par-tial hypothesis, no discrimination, which corre-sponds to the German words keine and diskrim-inierung, which do not form a contiguous sub-string in the German.
Consider now a completederivation, which derives the string there is hier-archy of no discrimination, and which includes thepi-constituent no discrimination shown in the fig-ure (i.e., where the treelet discrimination takes noas its only modifier).
This derivation will violatethe pi-constituent constraint.95 DecodingWe now describe decoding algorithms for the syn-tactic models: we first describe inference rulesthat are used to combine pieces of structure, andthen describe heuristic search algorithms that usethese inference rules.
Throughout this section,for brevity and simplicity, we describe algorithmsthat apply under the assumption that each s-phrasehas a single associated treelet.
The generalizationto the case where an s-phrase may have multipletreelets is discussed in section 5.3.5.1 Inference RulesParsing operations for the TAG grammars de-scribed in (Carreras et al, 2008) are based onthe dynamic programming algorithms in (Eisner,2000).
A critical idea in dynamic programming al-gorithms such as these is to associate constituentsin a chart with spans of the input sentence, andto introduce inference rules that combine con-stituents into larger pieces of structure.
The crucialstep in generalizing these algorithms to the non-projective case, and to translation, will be to makeuse of bit-strings that keep track of which words inthe German have already been translated in a chartentry.
To return to the example from the intro-duction, again assume that the selected s-phrases9Note, however, that the derivation step show in figure 4will be considered in the search, because if discriminationtakes additional modifiers, and thereby forms a pi-constituentthat dominates a contiguous sub-string in the German, thenthe resulting derivation will be valid.0.
Data structures: Qifor i = 1 .
.
.
n is a set of hypothesesfor each length i, S is a set of chart entries1.
S ?
?2.
Initialize Q1.
.
.Qnwith basic chart entries derivedfrom phrase entries3.
For i = 1 .
.
.
n4.
For any A ?
BEAM(Qi)5.
If S contains a chart entry with the same signatureas A, and which has a higher inside score,6.
continue7.
Else8.
Add A to S9.
For any chart entry C that can be derived fromA together with another chart entry B ?
S ,add C to the set Qjwhere j = length(C)10.
ReturnQn, a set of items of length nFigure 6: A beam search algorithm.
A dynamic-programming signature consists of the regular dynamic-programming state for the parsing algorithm, together withthe span (bit-string) associated with a constituent.segment the German input into [wir mu?ssen auch][diese kritik] [ernst] [nehmen], and the treelets areas shown in the introduction.
Each of these treeletswill form a basic entry in the chart, and will havean associated bit-string indicating which Germanwords have been translated by that entry.These basic chart entries can then be combinedto form larger pieces of structure.
For example,the following inferential step is possible:NP/0001100these criticismsVP/0000001Vtake?
VP/0001101VtakeNPthese criticismsWe have shown the bit-string representation foreach consituent: for example, the new constituenthas the bit-string 0001101 representing the factthat the non-contiguous sub-strings diese kritikand nehmen have been translated at this point.
Anytwo constituents can be combined, providing thatthe logical AND of their bit-strings is all 0?s.Inference steps such as that shown above willhave an associated score corresponding to theTAG adjunction that is involved: in our mod-els, both scoreSY Nand scoreRwill contribute tothis score.
In addition, we add state?specifically,word bigrams at the start and end of constituents?that allows trigram language model scores to becalculated as constituents are combined.5.2 Approximate SearchThere are 2n possible bit-strings for a sentence oflength n, hence the search space is of exponen-tial size; approximate algorithms are therefore re-quired in search for the highest scoring derivation.Figure 6 shows a beam search algorithm whichmakes use of the inference rules described in the206previous section.
The algorithm stores sets Qifor i = 1 .
.
.
n, where n is the source-languagesentence length; each set Qistores hypotheses oflength i (i.e., hypotheses with an associated bit-string with i ones).
These sets are initialized withbasic entries derived from s-phrases.The function BEAM(Qi) returns all itemswithin Qithat have a high enough score to fallwithin a beam (more details for BEAM are givenbelow).
At each iteration (step 4), each item inturn is taken from BEAM(Qi) and added to achart; the inference rules described in the previ-ous section are used to derive new items which areadded to the appropriate set Qj, where j > i.We have found the definition of BEAM(Qi) tobe critical to the success of the method.
As a firststep, each item in Qireceives a score that is a sumof an inside score (the cost of all derivation stepsused to create the item) and a future score (an esti-mate of the cost to complete the translation).
Thefuture score is based on the source-language wordsthat are still to be translated?this can be directlyinferred from the item?s bit-string?this is similarto the use of future scores in Pharoah (Koehn et al,2003), and in fact we use Pharoah?s future scoresin our model.
We then give the following defini-tion, where N is a parameter (the beam size):Definition 4 (BEAM) Given Qi, define Qi,jforj = 1 .
.
.
n to be the subset of items in Qiwhichhave their j?th bit equal to one (i.e., have the j?thsource language word translated).
Define Q?i,jtobe the N highest scoring elements in Qi,j.
ThenBEAM(Qi) = ?nj=1Q?i,j.To motivate this definition, note that a naivemethod would simply define BEAM(Qi) to bethe N highest scoring elements of Qi.
This def-inition, however, assumes that constituents whichform translations of different parts of a sentencehave scores that can be compared?an assumptionthat would be true if the future scores were highlyaccurate, but which quickly breaks down when fu-ture scores are inaccurate.
In contrast, the defi-nition above ensures that the top N analyses foreach of the n source language words are stored ateach stage, and hence that all parts of the sourcesentence are well represented.
In experiments, thenaive approach was essentially a failure, with pars-ing of some sentences either failing or being hope-lessly inefficient, depending on the choice of N .In contrast, definition 4 gives good results.System BLEU scoreSyntax-based 25.2Syntax (no ScoreR) 23.7 (-1.5)Syntax (no pi-c constraint) 24.4 (-0.8)Table 1: Development set results showing the effect of re-moving ScoreRor the pi-constituent constraint.5.3 Allowing Multiple Treelets per s-PhraseThe decoding algorithms that we have describedapply in the case where each s-phrase has a sin-gle treelet.
The extension of these algorithmsto the case where a phrase may have multipletreelets (e.g., see figure 2) is straightforward, butfor brevity the details are omitted.
The basic ideais to extend bit-string representations with a recordof ?pending?
treelets which have not yet been in-cluded in a derivation.
It is also possible to enforcethe pi-constituent constraint during decoding, aswell as a constraint that ensures that reordering op-erations do not ?break apart?
English sub-stringswithin s-phrases that have multiple treelets (for ex-ample, for the s-phrase in figure 2, we ensure thatthere is no remains as a contiguous sequence ofwords in any translation using this s-phrase).6 ExperimentsWe trained the syntax-based system on 751,088German-English translations from the Europarlcorpus (Koehn, 2005).
A syntactic languagemodel was also trained on the English sentencesin the training data.
We used Pharoah (Koehn etal., 2003) as a baseline system for comparison; thes-phrases used in our system include all phrases,with the same scores, as those used by Pharoah,allowing a direct comparison.
For efficiency rea-sons we report results on sentences of length 30words or less.10 The syntax-based method givesa BLEU (Papineni et al, 2002) score of 25.04,a 0.46 BLEU point gain over Pharoah.
This re-sult was found to be significant (p = 0.021) underthe paired bootstrap resampling method of Koehn(2004), and is close to significant (p = 0.058) un-der the sign test of Collins et al (2005).Table 1 shows results for the full syntax-basedsystem, and also results for the system with thediscriminative dependency scores (see section 4.1)and the pi-contituent constraint removed from thesystem.
In both cases we see a clear impact ofthese components of the model, with 1.5 and 0.8BLEU point decrements respectively.10Both Pharoah and our system have weights trained usingMERT (Och, 2003) on sentences of length 30 words or less,to ensure that training and test conditions are matched.207R: in our eyes , the opportunity created by this directive of introducing longer buses on international routes is efficient .S: the opportunity now presented by this directive is effective in our opinion , to use long buses on international routes .P: the need for this directive now possibility of longer buses on international routes to is in our opinion , efficiently .R: europe and asia must work together to intensify the battle against drug trafficking , money laundering , internationalcrime , terrorism and the sexual exploitation of minors .S: europe and asia must work together in order to strengthen the fight against drug trafficking , money laundering , againstinternational crime , terrorism and the sexual exploitation of minors .P: europe and asia must cooperate in the fight against drug trafficking , money laundering , against international crime ,terrorism and the sexual exploitation of minors strengthened .R: equally important for the future of europe - at biarritz and later at nice - will be the debate on the charter of fundamentalrights .S: it is equally important for the future of europe to speak on the charter of fundamental rights in biarritz , and then in nice .P: just as important for the future of europe , it will be in biarritz and then in nice on the charter of fundamental rights tospeak .R: the convention was thus a muddled system , generating irresponsibility , and not particularly favourable to well-ordereddemocracy .S: therefore , the convention has led to a system of a promoter of irresponsibility of the lack of clarity and hardly coincidedwith the rules of a proper democracy .P: the convention therefore led to a system of full of lack of clarity and hardly a promoter of the irresponsibility of the rulesof orderly was a democracy .Figure 7: Examples where both annotators judged the syntactic system to give an improved translation when compared tothe baseline system.
51 out of 200 translations fall into this category.
These examples were chosen at random from these 51examples.
R is the human (reference) translation; S is the translation from the syntax-based system; P is the output from thebaseline (phrase-based) system.Syntax PB = TotalSyntax 51 3 7 61PB 1 25 11 37= 21 14 67 102Total 73 42 85 200Table 2: Human annotator judgements.
Rows show re-sults for annotator 1, and columns for annotator 2.
Syntaxand PB show the number of cases where an annotator re-spectively preferred/dispreferred the syntax-based system.
=gives counts of translations judged to be equal in quality.In addition, we obtained human evaluations on200 sentences chosen at random from the test data,using two annotators.
For each example, the ref-erence translation was presented to the annota-tor, followed by translations from the syntax-basedand phrase-based systems (in a random order).
Foreach example, each annotator could either decidethat the two translations were of equal quality, orthat one translation was better than the other.
Ta-ble 2 shows results of this evaluation.
Both an-notators show a clear preference for the syntax-based system: for annotator 1, 73 translations arejudged to be better for the syntax-based system,with 42 translations being worse; for annotator 2,61 translations are improved with 37 being worse;both annotators?
results are statistically significantwith p < 0.05 under the sign test.
Figure 7 showssome translation examples where the syntax-basedsystem was judged to give an improvement.7 Conclusions and Future WorkWe have described a translation model that makesuse of flexible parsing operations, critical ideasbeing the definition of s-phrases, ?-dependencies,the pi-constituent constraint, and an approximatesearch algorithm.
A key area for future workwill be further development of the discriminativedependency model (section 4.1).
The model ofscorer(?, f) that we have described in this paper isrelatively simple; in general, however, there is thepotential for scorerto link target language depen-dencies to arbitrary properties of the source lan-guage string f (recall that ?
contains a head andmodifier spine in the target language, along withpositions in the source-language string to whichthese spines are aligned).
For example, we mightintroduce features that: a) condition dependenciescreated in the target language on dependency re-lations between their aligned words in the sourcelanguage; b) condition target-language dependen-cies on whether they are aligned to words thatare in the same clause or segment in the sourcelanguage string; or, c) condition the grammaticalroles of nouns in the target language on grammat-ical roles of aligned words in the source language.These features should improve translation qual-ity by giving a tighter link between syntax in thesource and target languages, and would be easilyincorporated in the approach we have described.Acknowledgments We would like to thank Ryan Mc-Donald for conversations that were influential in this work,and Meg Aycinena Lippow and Ben Snyder for translationjudgments.
This work was supported under the GALE pro-gram of the Defense Advanced Research Projects Agency,Contract No.
HR0011-06-C-0022.208ReferencesH.
Alshawi.
1996.
Head automata and bilingual tiling:Translation with minimal representations.
In Pro-ceedings of ACL, pages 167?176.X.
Carreras, M. Collins, and T. Koo.
2008.
TAG, dy-namic programming and the perceptron for efficient,feature-rich parsing.
In Proc.
of CoNLL.E.
Charniak, K. Knight, and K. Yamada.
2003.Syntax-based language models for machine transla-tion.
In Proceedings of MT Summit IX.E.
Charniak.
2001.
Immediate-head parsing for lan-guage models.
In Proceedings of ACL 2001.C.
Cherry.
2008.
Cohesive phrase-based decoding forstatistical machine translation.
In Proceedings ofACL-08: HLT, pages 72?80, Columbus, Ohio, June.Association for Computational Linguistics.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofACL.M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clauserestructuring for statistical machine translation.
InProceedings of ACL.M.
Collins.
1997.
Three generative, lexicalised mod-els for statistical parsing.
In Proceedings of the35th Annual Meeting of the Association for Com-putational Linguistics, pages 16?23, Madrid, Spain,July.
Association for Computational Linguistics.J.
Eisner.
2000.
Bilexical grammars and their cubic-time parsing algorithms.
In H. C. Bunt and A. Ni-jholt, editors, New Developments in Natural Lan-guage Parsing, pages 29?62.
Kluwer AcademicPublishers.J.
Eisner.
2003.
Learning non-isomorphic tree map-pings for machine translation.
In Proceedings ofACL.A.K.
Joshi and Y. Schabes.
1997.
Tree-adjoininggrammars.
In G. Rozenberg and K. Salomaa, ed-itors, Handbook of Formal Languages, volume 3,pages 169?124.
Springer.P.
Koehn, F.J. Och, and D. Marcu.
2003.
Statis-tical phrase-based translation.
In Proceedings ofHLT/NAACL.P.
Koehn.
2004.
Statistical significance tests for ma-chine translation evaluation.
In Dekang Lin andDekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.
Associationfor Computational Linguistics.P.
Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In Proceedings of MTSummit.D.
Marcu, W. Wang, A. Echihabi, and K. Knight.
2006.Spmt: Statistical machine translation with syntac-tified target language phrases.
In Proceedings ofEMNLP.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL.D.
Melamed.
2004.
Statistical machine translation byparsing.
In Proceedings of ACL.H.
Mi, L. Huang, and Q. Liu.
2008.
Forest-basedtranslation.
In Proceedings of ACL-08: HLT, pages192?199.
Association for Computational Linguis-tics.R.
Nesson, S.M.
Shieber, and A.
Rush.
2006.
In-duction of probabilistic synchronous tree-insertiongrammars for machine translation.
In Proceedingsof the 7th AMTA.F.J.
Och.
2003.
Minimum error rate training for statis-tical machine translation.
In Proceedings of ACL.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of ACL, pages 311?318.Association for Computational Linguistics.C.
Quirk, A. Menezes, and Colin Cherry.
2005.
De-pendency tree translation: Syntactically informedphrasal smt.
In Proceedings of ACL.O.
Rambow, K. Vijay-Shanker, and D. Weir.
1995.D-tree grammars.
In Proceedings of the 33rdAnnual Meeting of the Association for Computa-tional Linguistics, pages 151?158, Cambridge, Mas-sachusetts, USA, June.
Association for Computa-tional Linguistics.L.
Shen, J. Xu, and R. Weischedel.
2008.
A newstring-to-dependency machine translation algorithmwith a target dependency language model.
In Pro-ceedings of ACL.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Com-putational Linguistics, 23(3):377?404.K.
Yamada and K. Knight.
2001.
A syntax-based sta-tistical translation model.
In Proceedings of ACL.H.
Zhang and D. Gildea.
2005.
Stochastic lexicalizedinversion transduction grammar for alignment.
InProceedings of ACL, pages 473?482.A.
Zollmann and A. Venugopal.
2006.
Syntax aug-mented machine translation via chart parsing.
InProceedings of NAACL 2006 Workshop on Statisti-cal Machine Translation.209
