Extracting aspects of determiner meaningfrom dialogue in a virtual world environmentHilke Reckman, Jeff Orkin, and Deb RoyMIT Media Lab{reckman,jorkin,dkroy}@media.mit.eduAbstractWe use data from a virtual world game for automated learning of words and grammatical con-structions and their meanings.
The language data are an integral part of the social interaction in thegame and consist of chat dialogue, which is only constrained by the cultural context, as set by thenature of the provided virtual environment.
Building on previous work, where we extracted a vocab-ulary for concrete objects in the game by making use of the non-linguistic context, we now targetNP/DP grammar, in particular determiners.
We assume that we have captured the meanings of a setof determiners if we can predict which determiner will be used in a particular context.
To this end wetrain a classifier that predicts the choice of a determiner on the basis of features from the linguisticand non-linguistic context.1 IntroductionDeterminers are among those words whose meanings are hardest to define in a dictionary.
In NLP,determiners are often considered ?stop words?
that are not relevant for understanding the content of adocument and should be removed before any interesting processing is done.
On the other hand, it hasbeen shown that children are sensitive to determiner choice already at a very early age, using thesefunction words in figuring out what content nouns are intended to refer to.
Meanings of determiners havebeen argued to include important pragmatic and discourse-related functions.We have a corpus of dialogue that is grounded in a virtual environment.
This means that in our datathere is a relation between what people are saying and what they are doing, providing cues as to whatthey mean by the words and constructions they use.
We have chosen to use a virtual world environmentto collect data in, rather than a real world environment, because relatively rich virtual worlds are by nowavailable that are able to provide an interesting level of grounding, whereas making sense of real worldscenes using computer vision is still very challenging.
In addition, this choice allows us to convenientlycollect data online1.Although there exists a rich body of computational linguistics research on learning from corpus data,these corpora usually consist of text only.
Only recently corpora that include non-linguistic context havestarted to be collected and used for grounded learning of semantics (Chen et al, 2010; Frank et al,2009; Fleischman and Roy, 2005; Gorniak and Roy, 2005).
This kind of work offers new and insightfulperspectives on learning meanings of natural language words and constructions, based on the idea thatour own knowledge of natural language meanings is grounded in action and perception (Roy, 2005), andthat language is a complex adaptive system which evolves in a community through grounded interaction(e.g.
Steels, 2003).
So far the language in virtually grounded datasets has often been restricted to eitherdescriptions or directives, so utterances can be paired fairly directly with the actions they describe.
Theinteraction in our data is much freer.
That means that it is more representative for the data that humanlearners get, and that our methods can be applied to a wider variety of data, possibly also to datasets1Von Ahn and Dabbish (2004) were among the first to realize the potential of collecting human knowledge data online, in agame setup, collecting a large image-labeling corpus.245that have not been collected specifically for this purpose.
A related project is KomParse (Klu?wer et al,2010).
Piantadosi et al (2008) developed a Bayesian model that learns compositional semantic meaningsof different kinds of words, including quantifiers, but from completely artificial data.Our research focuses on learning from data, rather than through interaction, though the latter may bepossible in a later stage of the project.
An example of a virtual world project where language is learnedthrough interaction is ?Wubble World?
(Hewlett et al, 2007).
In the Give Challenge (Byron et al, 2009)a virtual world setup is used to evaluate natural language generation systems.In previous work we have extracted words and multi-word expressions that refer to a range of objectsthat are prominent in our virtual environment (Reckman et al, 2010).
Now we investigate if aspectsof determiner meaning can be learned from this dataset.
The extracted knowledge of nouns makesthe learning of determiners possible.
We study what factors contribute to the choice of the determinerand how they relate to each other, by training a decision tree classifier using these factors as features.The decision tree provides insight in which features are actually used, in which order, and to whicheffect.
The accuracy of the resulting classifier on a test set should give us an impression of how wellwe understand the use of the different determiners.
Although one may argue that this study is about userather than about meaning, we take it that meaning can only be learned through use, and it is meaningthat we are ultimately interested in.
One of the overarching questions we are concerned with is whatknowledge about language and how it works is needed to extract knowledge about constructions and theirmeanings from grounded data.
Practically, a computational understanding of determiners will contributeto determining the reference of referential expressions, particularly in situated dialogue, and to generatingfelicitous referential expressions (cf.
Belz et al, 2010).We first introduce our dataset.
Then we discuss the automated extraction of determiners.
Subse-quently, we motivate the features we use, present our classifier experiments, and discuss the results.2 Data: The Restaurant GameOrkin and Roy (2007) showed in The Restaurant Game project that current computer game technologyallows for simulating a restaurant at a high level-of-detail, and exploiting the game-play experiencesof thousands of players to capture a wider coverage of knowledge than what could be handcrafted bya team of researchers.
The restaurant theme was inspired by the idea of Schank and Abelson (1977),who argued that the understanding of language requires the representation of common ground for ev-eryday scenarios.
The goal is automating characters with learned behavior and dialogue.
The ongoingRestaurant Game project has provided a rich dataset for linguistic and AI research.
In an online two-player game humans are anonymously paired to play the roles of customers and waitresses in a virtualrestaurant (http://theRestaurantGame.net).
Players can chat with open-ended typed text, move aroundthe 3D environment, and manipulate 47 types of interactive objects through a point-and-click interface(see figure 1).
Every object provides the same interaction options: pick up, put down, give, inspect, siton, eat, and touch, but objects respond to these actions in different ways.
The chef and bartender arehard-coded to produce food items based on keywords in chat text.
A game takes about 10-15 minutes toplay.
Everything players say and do is logged in time-coded text files on our servers.
Although playerinteractions vary greatly, we have demonstrated that enough people do engage in common behavior thatit is possible for an automatic system to learn statistical models of typical behavior and language thatcorrelate highly with human judgment of typicality (Orkin and Roy, 2007).Over 10.000 games have been collected.
The dialogue is grounded in two (partially overlapping)ways.
Not only is there a simulated physical environment with objects that can be manipulated in variousways, but also social patterns of recurring events provide an anchor for making sense of the dialogue.Previous research results include a first implementation of a planner that drives AI characters playing thegame (Orkin and Roy, 2009).The intuition is that a human student of English starting from scratch (but with some common senseknowledge about restaurants), could learn quite a bit of English from studying the Restaurant Gameepisodes; possibly enough to play the game.
We try to computationally simulate such a learning process.246Figure 1: Screen-shots from The Restaurant Game, from left to right: third-person perspective, waitress?sperspective with dialogue, menu for interacting with objects.3 Extracting nounsPreviously, we extracted a vocabulary of referring expressions for a set of concrete objects, based onwhich words and phrases have the highest relative frequency in the contexts in which the objects areused (see figure 2).
We extracted words and phrases that can refer to the food and drink items on therestaurant?s menu, the menu, and the bill, and some other items.
These expressions represent the corenominal phrases in the game.
We will use these expressions as a starting point to extract determinersand nominal modifiers.
We restrict ourselves to the ordered food and drink items, the menu and the bill,expecting that these show a somewhat uniform and interesting behavior, as they are the objects that canappear and disappear during the course of a game.food type referring expressionsSOUP?soup?
?vegetable soup?
?soup du jour?
?soup de jour?SALAD ?salad?
?cobb salad?SPAGHETTI ?spaghetti?
?spaghetti marinara?FILET ?steak?
?filet?
?filet mignon?SALMON ?salmon?
?grilled salmon?LOBSTER ?lobster?
?lobster thermador?CHEESECAKE?cheesecake??cheese?
?cake?
?cherry cheesecake?
?cheese cake?PIE ?pie?
?berry pie?TART ?tart?
?nectarine tart?drink type referring expressionsWATER ?water?TEA ?tea?COFFEE ?coffee?BEER ?beer?REDWINE ?red?
?wine?
?red wine?WHITEWINE ?white?
?white wine?item type referring expressionsMENU ?menu?BILL ?bill?
?check?Figure 2: Extracted referring expressions for relevant items.The referring expressions for these object types have been extracted in an unsupervised mannermaking use of the relative frequency of words and phrases in the context of the objects being used.Words, bigrams and trigrams were validated against each other with the use of one threshold.
For moredetail see (Reckman et al, 2010).4 Extracting determinersExtracting determiners totally unsupervised is a non-trivial task.
Attempts to use the existing fully un-supervised grammar induction algorithm ADIOS (Solan et al, 2005) did not give us the results we werehoping for.
Instead, we decided to make use of the knowledge of nouns that we already have and target247determiners directly, rather than having to induce a full grammar.
In future work we will look into usingalternative grammar induction systems, for a wider range of learning tasks.We first narrowed down our search space by collecting words that are positively associated withthe position directly to the left of the nominal expression above a high recall, low precision threshold(phi=0.01)2.
This should favor determiners and other nominal modifiers over, for example, verbs.We expect determiners to appear with a wider range of different nouns than adjectival modifiers do.Especially in this restricted domain, adjectives are more likely to be restricted to specific object types.We consider pre-nominal terms that are general enough to appear with more than 5 different objects (outof 17) to be determiner candidates.
We also check that our candidates can be preceded by an utteranceboundary.The word the is most strongly associated with the relevant position, combines with most differentnouns, and can occur as only element between a boundary and a noun.
We therefore assume that atleast the is a determiner.
We order the other candidates according to their similarity to the, measuredas the cosine distance in a vector-space, with their two words to the left and to the right as dimensions.We accept words as determiners in order of similarity to the, starting with the most similar word, afterchecking that they are in complementary distribution with all of the already accepted words, i.e.
that theword does not occur adjacent to any of those.
This gives us the following determiners: the, my, your,some, a, another, our, one, ur, two, 2.3We can then identify adjectival modifiers by looking at what occurs between determiners and nouns.By checking what else these modifiers can be preceded by (that is also in complementary distributionwith known determiners), we can do another round of determiner search, and that lets us add any toour list.
As nouns can also be immediately preceded by an utterance boundary, we establish that thedeterminer position is not obligatorily filled.Of course this is not a complete set of determiners, but they appear to be the most prominent ones inthe game.
Real quantifiers are relatively rare and that is to be expected, given the setting.
Perhaps moresurprisingly, this and that are not associated with the left-of-noun position.
It turns out that they are notused very frequently as determiners in the game, and much more as pronouns.
In future work we willextract pronouns, by looking for single words that have a distribution that is similar to the distribution offull noun phrases with a determiner.In the process of extracting determiners, we also extract adjectives and modifiers such as glass of.With little extra effort we can build a vocabulary of these as well, including information as to whichnouns they are associated with.
Their meanings, however, are in most cases not sufficiently grounded inthe game to be understood.
We may in a more advanced stage of the project be able to figure out that theadjective free makes the item less likely to appear on the bill, but the meaning of hot will always remainunclear, as temperature is not modeled in the game.
Finding words associated with the position to the leftof specific nouns can also help us further improve our vocabulary of referring expressions, for exampleby identifying veg and veggie as alternatives for vegetable in vegetable soup4.We took a shortcut by directly targeting the position left of the noun.
This involves language-specificknowledge about English.
To make this method applicable to different languages and only use verygeneral knowledge at the start, we would first have to find out what the position of the determiner is.This may be to the right of the noun or affixed to it.
Not all languages have articles, but we can expectdeterminers like my, your, another etc.
to occur either adjacent to5, or morphologically expressed on thenoun6.
In previous work we have shown how a construction for coordination can be extracted (Reckman2The phi-score is a chi-square based association metric.
Manning and Schu?tze (2000) argue that such metrics are suitableto quantify collocational effects.
We also used it in extracting the referring expressions.3For the experiments we replace ur by your, and 2 by two.
We assume this could in principle be done automatically, althoughespecially in the latter case this is not trivial.4We do already have a list of spelling variants for all the terms, but veg and veggie were too different from the canonicalform to get through the edit-distance filter5Obviously we do not catch floating quantifiers this way.
We might catch their non-floating counterparts and then discoverthat they occur in other positions as well.6Several unsupervised morphological analyzers have been developed, which should in principle be run in an early stage oflearning.
For English however, the only interesting morphology at play here is plural formation.248et al, 2010).
Coordination, to our knowledge, occurs in all languages and this is probably a feature ofgeneral human cognition, so it makes sense to assume it exists in a language and look for it in the data.
Itcan then be used as a probe on structure.
Categories that are grammatically intimately connected to nounsare more likely to be repeated in a coordination involving two nouns.
If we look at our English data, forexample, we see that a lot more material tends to occur between and and the second noun-conjunct, thanbetween the first noun-conjunct and and, which suggests that things that are grammatically close to thenoun occur to the left of it.5 FeaturesIn this section we motivate the features we will use.
To capture the full meaning of determiners, wewould probably have to model the mental states of the players.
However, what we aim at here is apreliminary understanding of determiners as a step towards the understanding of full sentences, and theresolution of NP reference and co-reference, which would be prerequisites for any serious modeling ofmental states.
So we are interested in what can be learned from directly observable features.
The featuresare theoretically motivated, and reflect the nature of the referent, whether the referent has been mentionedbefore, whether the referent is present, and who the speaker and addressee are.The first feature is object type.
There are 17 different objects that we take into account: BEER,BILL, CHEESECAKE, COFFEE, FILET, LOBSTER, MENU, PIE, REDWINE, SALAD, SALMON, SOUP,SPAGHETTI, TART, TEA, WATER, and WHITEWINE.
We expect this feature to matter, because in arestaurant situation one usually orders ?the spaghetti?, but ?a beer?.
This may be to some extent dependenton what is on the menu, but not completely.
Regardless of what is on the menu, ordering ?the Heineken?seems to be more unusual than ordering ?the Merlot?.
This may mean that our data is not entirelyrepresentative of the general case, because of our restaurant setting.
However, it cannot be excluded thatsimilar effects play a role in other settings, too.
There is of course the effect of mass versus count nouns,too, but this may be a bit masked, because of unit expressions like glass of.
We chose to not includethese unit expressions as a feature, because the decision to use such modifiers can be considered part ofthe decision on which determiner to use.
So using the modifier as a feature, would be giving away partof the solution to the determiner-choice problem.The second feature captures the notion of discourse-old versus discourse-new.
We distinguish be-tween cases where an object of a particular type is mentioned for the first time, and where it has alreadybeen mentioned before.
In the latter case, we take it that the discourse referent has already been intro-duced.
The expected effect is that first mentions tend to be indefinite.7 This is only an approximation,because sometimes a second object of the same type is introduced and we do not resolve the referenceof our instances.The third and fourth features incorporate present versus future presence of the object, plus the posi-tion of the utterance with respect to the central action involving the object.
We keep track of the previousand following action in which the object is involved.
Actions of interest are restricted to the appearanceof the object and and its central action: ?eating?
for food and drink items, ?looking at?
for the menu, and?paying?
for the bill.
Being involved in such an action also implies presence.
Other intervening actionsare ignored.
The features are ?preceding action?
and ?following action?, and the values are ?appearance?,?main action?, and ?none?.
We expect indefinites before appearance, when the object is not yet present.Note that these features rely entirely on non-linguistic context.The fifth and sixth features identify speaker and addressee.
The speaker can be the customer or thewaitress.
For the addressee the relevant distinction is whether the staff (chef and bartender) are addressedor not.
We expect a tendency of the waitress using your when talking to the customer, and of the customerusing my more often.
We expect more indefinites or absence of a determiner when the staff is spoken to.These features are central to dialogue, and may reveal differences between the roles.7This is a typical feature for languages that have articles, and may be expressed through other means in other languages.2496 ExperimentsWe use the decision tree classifier from the Natural Language ToolKit for Python (Loper and Bird, 2002)and train and test it through 10-fold cross-validation on 74304 noun phrases from 5000 games, 23776 ofwhich actually have determiners.
The noun phrases used all contain nouns that can refer to the selectedobjects, though we cannot guarantee that they were intended to do so in all cases.
In fact, we have seenexamples where this is clearly not the case, and for example filet, which normally refers to the FILETobject, is used in the context of salmon.
This means that there is a level of noise in our data.The instances where the determiner is absent are very dominant, and this part of the data is necessarilynoisy, because of rare determiners that we?ve missed8, and possibly rather heterogeneous, as there aremany reasons why people may choose to not type a determiner in chat.
Therefore we focus on theexperiments where we have excluded these cases, as the results are more interesting.
We will refer tothe data that excludes instances with no determiner as the restricted dataset.
When instances with nodeterminer are included, we will talk about the full dataset.6.1 BaselinesIn the experiments we compare the results of using the features to two different baselines.
The simplestbaseline is to always choose the most frequent determiner.
For the instances that have overt determiners,the most frequent one is the.
Always choosing the gives us a mean accuracy of 0.364.
If we includethe instances with no overt determiners, that gives us a much higher baseline of 0.680, when the nodeterminer option is always chosen.
We call this the simple baseline.The second baseline is the result of using only the object feature, and forms the basis of our experi-ments.
We call this the object-only baseline.
On the restricted dataset the resulting classifier assigns thedeterminer a to the objects BEER, COFFEE, PIE, REDWINE, SALAD, TEA, WATER, and WHITEWINE,and the determiner the to BILL, CHEESECAKE, FILET, LOBSTER, MENU, SALMON, SOUP, SPAGHETTI,and TART.
This yields a mean accuracy of 0.520, which is a considerable improvement over the sim-ple baseline that is relevant for this part of the data.
If we look at the confusion matrix in figure 3 thatsummarizes the results of all 10 object-only runs we see that the objects?
preferences for definite versusindefinite determiners are also visible in the way instances with determiners other than the and a aremisclassified.
Instances with definite determiners are more often classified as the, and indefinites as a.a another any my one our some the two youra <4984> .
.
.
.
.
.
2912 .
.another 608 <.> .
.
.
.
.
76 .
.any 56 .
<.> .
.
.
.
24 .
.my 238 .
.
<.> .
.
.
742 .
.one 354 .
.
.
<.> .
.
241 .
.our 28 .
.
.
.
<.> .
178 .
.some 1109 .
.
.
.
.
<.> 438 .
.the 1270 .
.
.
.
.
.
<7383> .
.two 191 .
.
.
.
.
.
58 <.> .your 805 .
.
.
.
.
.
2075 .
<.>Figure 3: Confusion matrix for the object-only baseline.On the full dataset, the classifier assigns the to instances of BILL and MENU and no determinerto everything else, reflecting the count/mass distinction, and resulting in a mean accuracy of 0.707.This is also a statistically significant improvement over its baseline, but much less spectacular.
Thedefinite/indefinite distinction that we saw with the restricted dataset, does not really emerge here.8It is also hard to reliably recognize misspelled determiners as determiners tend to be very short words.2506.2 Adding the other featuresIn the core experiments of this paper we always use the object feature as a basis and measure the effect ofadding the other features, separately and in combination.
All differences reported are significant, unlessstated otherwise.
The table in figure 5 at the end of the section summarizes the results.If we add the feature of whether the item has been mentioned before or not, we get more indefi-nites, as was to be expected.
On the restricted dataset, the MENU, PIE, and TART objects get a if notmentioned previously, and the otherwise.
The mean accuracy is 0.527, which is a statistically significantimprovement over the object-only baseline (the improvement is consistent over all 10 runs), but it seemsrather small, nevertheless.
(Using the discourse feature without the object feature gives a score of 0.377.
)Adding information as to whether the customer has seen the menu does not make any difference.
On thefull dataset the discourse feature matters only for MENU, which gets a if not previously mentioned.
Themean accuracy is 0.709.If, instead, we add the action features we get a somewhat more substantial improvement for therestricted dataset; a mean accuracy of 0.561.
We also get a wider range of determiners: your tends to bechosen after appearing and before eating, another after eating, and a between no action and appearing.The order in which the following and preceding action features are applied by the classifier differs perobject.
(The action features without the object feature give a mean accuracy score of 0.427.)
For the fulldataset the mean accuracy is 0.714, again a consistent, but marginal improvement.
However, a, the andyour are the only determiners used, in addition to the no determiner option.Adding the speaker and addressee features to the object feature base gives the classifier a better gripon your.
More indefinites are used when the staff is addressed, your when the customer is spoken to.However, my is still not picked up.
The speaker and addressee features are used in both orders.
The meanaccuracy is 0.540, which is better than with the discourse feature, but worse than with the action features.
(The speaker and addressee features without the object feature give a mean accuracy score of 0.424.)
Inthe case of the full dataset, the new features are barely used, and there is no consistent improvement overthe different runs.
The mean accuracy is 0.711.If we combine the action features and speaker/addressee features on top of the object feature basis,we see a substantial improvement again for the restricted dataset.
The mean accuracy is 0.592.
Finally,we get some cases of my being correctly classified, and also your is correctly classified significantlymore often than in the previous experiments.
The object feature always comes first in the decision tree.For the other features, all relative orders are attested.
Adding the ?previously-mentioned?
feature tothis combination (see also figure 4) improves this result a little bit more, to a mean accuracy of 0.594,although we can expect the information contained in it to have a large overlap with the information inother features, for example, items mentioned for the first time will typically not have appeared yet.a another any my one our some the two youra <5732> 163 1 11 20 .
70 1773 1 125another 175 <350> .
2 .
.
48 70 .
39any 44 4 <.> .
.
.
2 29 .
1my 154 19 .
<9> .
.
20 765 .
13one 437 20 .
2 <16> .
4 70 .
46our 29 1 .
.
.
<.> 1 161 .
14some 881 48 .
6 3 .
<114> 421 .
74the 1332 74 .
33 8 .
34 <6131> .
1040two 191 10 .
2 .
.
1 45 <.> .your 218 88 .
.
.
.
20 781 .
<1773>(row = reference; col = test)Figure 4: Confusion matrix for the object, action, speaker/addressee and discourse features combined.2516.3 Linguistic context and dialogue actsIt will be part of future research to distinguish the different dialogue acts that the nominal phrases thatwe studied can be part of.
Identifying the ?task?
that an expression is part of may have a similar effect.Tasks of the type ?customer gets seated?, ?waitress serves food?, ?customer eats meal?, etc.
are annotatedfor supervised learning, and may consist of several actions and utterances (Orkin et al, 2010).To give an indication that the dialogue act that an expression is part of may be informative as to thecorrect choice of the determiner, we have done an extra experiment, where we have used the word beforeand the word after the DP as features.
This gives a tremendous amount of feature values, which are notvery insightful, due to the lack of generalization, and are a near guarantee for over-fitting.
However,it does yield an improvement over using the object-only baseline.
Moreover, the preceding word andfollowing word features are now applied before the object feature.
The mean accuracy in this experimentwas 0.562, which is comparable to the experiment with object and action features.
At the same time weget a wider range of determiners than we have had before, including some correctly classified instancesof our.
On the full dataset we even get a higher accuracy score than in any of the other experiments:0.769, also with a much wider range of determiners.
We suspect that this local linguistic context givesquite good cues as to whether the expression is part of a proper sentence or not, and that in the formercase an overt determiner is much more likely9.
The results of all experiments are summarized in figure 5.restricted fullsimple baseline 0.364 0.680object-only baseline 0.520 0.707object + discourse 0.527 0.709object + action 0.561 0.714object + speaker 0.540 0.711object + action + speaker 0.592 0.721object + action + speaker + discourse 0.594 0.721object + surrounding words 0.562 0.769Figure 5: Summary of the testing results.7 DiscussionMaybe the most surprising outcome is that the object type turns out to be the main factor in choosingthe determiner in this virtual restaurant setting.
It would beinteresting to see this reproduced on thedata of two new games that are currently being developed, with novel scenarios, locations and objects.At the same time, it is a strength of our approach, that we can simulate a specific setting and captureits ideosyncrasies, learning domain-specific aspects of language, and hopefully eventually learn whatgeneralizes across different scenarios.For the restricted dataset we see that, consistently, indefinites are mostly misclassified as a, anddefinites mostly as the.
If we evaluate only for definiteness, we get a mean accuracy of 0.800 for the casewith all features combined.
We could distinguish these two classes of determiners on the basis of thesimilarity of each determiner to the two dominant types.
It is, however, the object feature that seems tobe mainly responsible for the gain in definiteness accuracy with respect to the simple baseline.It is unsurprising that we haven?t learned much about one and two, except that they pattern withindefinites, as we haven?t included features that have to do with the number of objects.
There actuallyare more numerals that appear in the game, but did not make it into our list of determiners, becausethey did not occur with enough different objects.
In the general case, we are doubtful that numerals aresufficiently grounded in this game for their exact meanings to be learned.
It may however be possible tolearn a one-two-many kind of distinction.
This would also involve looking into plural morphology, andremains for future research.9We have observed that in several games people tend to just sum up food items, without embedding them in a sentence.252We also haven?t learned anything about our, except that it patterns with definites.
It is not quite clearwhat kind of features would be relevant to our in this setting.For the possessive pronouns your and my we have learned that one tends to be linked to the waitressas a speaker (and the customer as addressee) and the other to the customer.
It will be challenging to reachan understanding that goes deeper than this10.
The range of interactions in the game may be too limitedto learn the meanings of all determiners in their full generality.While we have treated a and another as different determiners, we have included cases of some moreunder some.
It may be worthwhile to include some more (and perhaps any more and one more as well) asa separate determiner.
However, our best classifier so far still cannot distinguish between a and anothervery well.The experiments with linguistic context suggest that dialogue act may make for an additional, pow-erful, albeit indirect, feature.
The fact that it helps to know when the main action involving the objecttook place, rather than just its appearance, may also be taken to point in the same direction, as peopletend to say different kinds of things about an object before and after the main action.Using a classifier seems to be a reasonable way of testing how well we understand determiners, aslong as our features provide insight.
Although there is still a lot of room for improvement, there is likelyto be a ceiling effect at some point, because sometimes more than one option is felicitous.
We also haveto keep in mind that chat is likely to be more variable than normal written or spoken language.8 ConclusionWe have carried out an exploratory series of experiments, to see if meanings of determiners, a veryabstract linguistic category, could be learned from virtually grounded dialogue data.
We have trained aclassifier on a set of theoretically motivated features, and used the testing phase to evaluate how wellthese features predict the choice of the determiner.Altogether, the results are encouraging.
If we exclude instances with no determiner we reach anaccuracy of 0.594 over a baseline of 0.364.
The features that identify the dialogue participants andsurrounding actions, including appearance, play an important role in this result, even though the objecttype remains the main factor.
A clear dichotomy between definite and indefinite determiners emerges.The results for the complete dataset are a bit messier, and need more work.In future work we will identify utterance types, or dialogue acts, that also rely on surrounding actionsand on the speaker and addressee.
We will also look into resolving reference and co-reference.AcknowledgmentsThis research was funded by a Rubicon grant from the Netherlands Organisation for Scientific Research(NWO), project nr.
446-09-011.ReferencesBelz, A., E. Kow, J. Viethen, and A. Gatt (2010).
Generating referring expressions in context: TheGREC task evaluation challenges.
In Empirical Methods in Natural Language Generation, pp.
294?327.
Springer.Byron, D., A. Koller, K. Striegnitz, J. Cassell, R. Dale, J. Moore, and J. Oberlander (2009).
Report onthe First NLG Challenge on Generating Instructions in Virtual Environments (GIVE).
In Proceedingsof the 12th European Workshop on Natural Language Generation, pp.
165?173.
ACL.Chen, D., J. Kim, and R. Mooney (2010).
Training aMultilingual Sportscaster: Using Perceptual Contextto Learn Language.
Journal of Artificial Intelligence Research 37, 397?435.10For their personal pronoun counterparts you and I we might stand a better chance.253Fleischman, M. and D. Roy (2005).
Why verbs are harder to learn than nouns: Initial insights from acomputational model of intention recognition in situated word learning.
In 27th Annual Meeting ofthe Cognitive Science Society, Stresa, Italy.Frank, M., N. Goodman, and J. Tenenbaum (2009).
Using speakers?
referential intentions to model earlycross-situational word learning.
Psychological Science 20(5), 578.Gorniak, P. and D. Roy (2005).
Probabilistic grounding of situated speech using plan recognition andreference resolution.
In Proceedings of the 7th international conference on Multimodal interfaces, pp.143.
ACM.Hewlett, D., S. Hoversten, W. Kerr, P. Cohen, and Y. Chang (2007).
Wubble world.
In Proceedings ofthe 3rd Conference on Artificial Intelligence and Interactive Entertainment.Klu?wer, T., P. Adolphs, F. Xu, H. Uszkoreit, and X. Cheng (2010).
Talking NPCs in a virtual gameworld.
In Proceedings of the ACL 2010 System Demonstrations, pp.
36?41.
ACL.Loper, E. and S. Bird (2002).
NLTK: The natural language toolkit.
In Proceedings of the ACL-02Workshop on Effective tools and methodologies for teaching natural language processing and compu-tational linguistics-Volume 1, pp.
70.
ACL.Manning, C. and H. Schu?tze (2000).
Foundations of statistical natural language processing.
MIT Press.Orkin, J. and D. Roy (2007).
The restaurant game: Learning social behavior and language from thousandsof players online.
Journal of Game Development 3(1), 39?60.Orkin, J. and D. Roy (2009).
Automatic learning and generation of social behavior from collectivehuman gameplay.
In Proceedings of The 8th International Conference on Autonomous Agents andMultiagent Systems-Volume 1, pp.
385?392.
International Foundation for Autonomous Agents andMultiagent Systems.Orkin, J., T. Smith, H. Reckman, and D. Roy (2010).
Semi-Automatic Task Recognition for InteractiveNarratives with EAT & RUN.
In Proceedings of the 3rd Intelligent Narrative Technologies Workshopat the 5th International Conference on Foundations of Digital Games (FDG).Piantadosi, S., N. Goodman, B. Ellis, and J. Tenenbaum (2008).
A Bayesian model of the acquisition ofcompositional semantics.
In Proceedings of the Thirtieth Annual Conference of the Cognitive ScienceSociety.
Citeseer.Reckman, H., J. Orkin, and D. Roy (2010).
Learning meanings of words and constructions, grounded ina virtual game.
In Proceedings of the 10th Conference on Natural Language Processing (KONVENS).Roy, D. (2005).
Semiotic schemas: A framework for grounding language in action and perception.Artificial Intelligence 167(1-2), 170?205.Schank, R. and R. Abelson (1977).
Scripts, plans, goals and understanding: An inquiry into humanknowledge structures.
Lawrence Erlbaum Associates Hillsdale, NJ.Solan, Z., D. Horn, E. Ruppin, and S. Edelman (2005).
Unsupervised learning of natural languages.Proceedings of the National Academy of Sciences of the United States of America 102(33), 11629.Steels, L. (2003).
Evolving grounded communication for robots.
Trends in cognitive sciences 7(7),308?312.Von Ahn, L. and L. Dabbish (2004).
Labeling images with a computer game.
In Proceedings of theSIGCHI conference on Human factors in computing systems, pp.
319?326.
ACM.254
