Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1395?1404,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsProbing the Linguistic Strengths and Limitationsof Unsupervised Grammar InductionYonatan Bisk and Julia HockenmaierDepartment of Computer ScienceThe University of Illinois at Urbana-Champaign201 N Goodwin Ave Urbana, IL 61801{bisk1,juliahmr@illinois.edu}AbstractWork in grammar induction should helpshed light on the amount of syntactic struc-ture that is discoverable from raw wordor tag sequences.
But since most cur-rent grammar induction algorithms pro-duce unlabeled dependencies, it is diffi-cult to analyze what types of constructionsthese algorithms can or cannot capture,and, therefore, to identify where additionalsupervision may be necessary.
This pa-per provides an in-depth analysis of theerrors made by unsupervised CCG parsersby evaluating them against the labeled de-pendencies in CCGbank, hinting at newresearch directions necessary for progressin grammar induction.1 IntroductionGrammar induction aims to develop algorithmsthat can automatically discover the latent syntacticstructure of language from raw or part-of-speechtagged text.
While such algorithms would havethe greatest utility for low-resource languages forwhich no treebank is available to train supervisedparsers, most work in this area has focused onlanguages where existing treebanks can be usedto measure and compare the performance of theresultant parsers.
Despite significant progress inthe last decade (Klein and Manning, 2004; Head-den III et al, 2009; Blunsom and Cohn, 2010;Spitkovsky et al, 2013; Mare?cek and Straka,2013), there has been little analysis performed onthe types of errors these induction systems make,and our understanding of what kinds of construc-tions these parsers can or cannot recover is stillrather limited.
One likely reason for this lack ofanalysis is the fact that most of the work in this do-main has focused on parsers that return unlabeleddependencies, which cannot easily be assigned alinguistic interpretation.This paper shows that approaches that arebased on categorial grammar (Steedman, 2000)are amenable to more stringent evaluation metrics,which enable detailed analyses of the construc-tions they capture, while the commonly usedunlabeled directed attachment scores hide linguis-tically important errors.
Any categorial grammarbased system, whether deriving its grammarfrom seed knowledge distinguishing nouns andverbs (Bisk and Hockenmaier, 2013), from alexicon constructed from a simple questionnairefor linguists (Boonkwan and Steedman, 2011), orfrom sections of a treebank (Garrette et al, 2015),will attach linguistically expressive categoriesto individual words, and can therefore producelabeled dependencies.
We provide a simple proofof concept for how these labeled dependenciescan be used to isolate problem areas in CCGinduction algorithms.
We illustrate how theymake the linguistic assumptions and mistakes ofthe model transparent, and are easily comparableto a treebank where available.
They also allow usto identify linguistic phenomena that require addi-tional supervision or training signal to master.
Ouranalysis will be based on extensions of our earliersystem (Bisk and Hockenmaier, 2013), since itrequires less supervision than the CCG-basedapproaches of Boonkwan and Steedman (2011)or Garrette et al (2015).
Our aim in presentingthis analysis is to initiate a broader conversationand classification of the impact of various types ofsupervision provided to these approaches.
We willsee that most of the constructions that our systemcannot capture, even when they are included inthe model?s search space, involve precisely thekinds of non-local dependencies that elude evensupervised dependency parsers (since they requiredependency graphs, instead of trees), and thathave motivated the use of categorial grammar-based approaches for supervised parsing.1395First, we provide a brief introduction to CCG.Next, we define a labeled evaluation metric that al-lows us to compare the labeled dependencies pro-duced by Bisk and Hockenmaier (2013)?s unsu-pervised parser with those in CCGbank (Hock-enmaier and Steedman, 2007).
Third, we ex-tend their induction algorithm to allow it to inducemore complex categories, and refine their proba-bility model to handle punctuation and lexicaliza-tion, which we show to be necessary when han-dling the larger grammars induced by our vari-ant of their algorithm.
While we also perform atraditional dependency evaluation for comparisonto the non-CCG based literature, we focus on ourCCG-based labeled evaluation metrics to performa comparative analysis of Bisk and Hockenmaier(2013)?s parser and our extensions.2 Combinatory Categorial GrammarCCG categories CCG (Steedman, 2000) is alexicalized grammar formalism which associateseach word with a set of lexical categories that fullyspecify its syntactic behavior.
Lexical categoriesindicate the expected number, type and relative lo-cation of arguments a word should take, or whatconstituents it may modify.
Even without explicitevaluation against a treebank, the CCG lexiconthat an unsupervised parser produces provides aneasily interpretable snapshot of the assumptionsthe model has made about a language (Bisk andHockenmaier, 2013).
The set of CCG categories isdefined recursively over a small set of atomic cat-egories (e.g.
S,N,NP,PP).
Complex categoriestake the form X\Y or X/Y and represent functionswhich create a result of category X when com-bined with an argument Y.
The slash indicateswhether the argument precedes (\) or follows (/)the functor (descriptions of CCG commonly usethe vertical slash | to range over both / and \).Modifiers are categories of the form X|X, and maytake arguments of their own.CCG rules CCG rules are defined schematicallyas function application (>,<), unary (>B1, <B1)and generalized composition (>Bn, <Bn), type-raising (>T, <T) and conjunction:X/Y Y ?>XX/Y Y|Z ?>B1X|ZX/Y Y|Z1|...|Zn?>BnX|Z1|...|ZnX ?>TT/(T\X)Y X\Y ?<XY|Z X\Y ?<B1X|ZY|Z1|...|ZnX\Y ?<BnX|Z1|...|ZnX ?<TT\(T/X)CCG derivations In the following derivation,forward application is used in line 1) as both theverb and the preposition take their NP arguments.In line 2), the prepositional phrase modifies theverb via backwards composition.
Finally, in line3), the derivation completes by producing a sen-tence (S) via backwards application:I saw her from afarN (S\N1)/N2N (S\S1)/N2N> >1) S\N1S\S1<B2) S\N1<3) SCCG dependencies CCG has two standardevaluation metrics.
Supertagging accuracy sim-ply computes how often a model chooses the cor-rect lexical category for a given word.
The cor-rect category is a prerequisite for recovering thecorrect labeled dependency.
By tracing throughwhich word fills which argument of which cate-gory, a set of dependency arcs, labeled by lexicalcategory and slot, can be extracted:lexical head of a lexical category ci is the corre-sponding word wi.
In general, the lexical head ofa derived category is determined by the (primary)functor, so that the lexical head of a category Xor X|Z1|...|Znth t resulted from combining X|Yand Y or Y|Z1|...|Znis identical to the lexical headof X.
However, when a modifier X|X with lexicalhead m is combined with an X|... whose lexicalhead is w, the lexical head of the resultant X|...is w, not m.2Otherwise, from would become thelexical head of the S\N saw her from afar, and thesentence You know I saw her from afar would havea dependency between know and from, rather thanbetween know and saw.In general, word wj is a dependent of word wiif the k-th argument of the lexical category ci ofword wi is instantiated with the lexical categoryof word wj .
In the above derivation:i j cik wiwj1 0 (S\N1)/N21 saw I1 2 (S\N1)/N22 saw her1 3 (S\S1)/N21 from saw4 3 (S\S1)/N22 from afarI saw her from afar(S\S)/N2(S\S)/N1(S\N)/N2(S\N)/N1The use of categories as dependency labelsmakes CCG labels more fine-grained than a stan-dard dependency grammar.
For example, the sub-ject role of intransitive, transitive and ditransitiveverbs are all SUB in dependency treebanks buttake at least three different labels in CCGbank.i j wjLabel2 1 I SUB0 2 saw ROOT2 3 her OBJ2 4 from VMOD4 5 afar PMODI saw her from afarPMODVMODOBJSUBROOTAn additional complexity in CCGbank are cer-tain types of lexical categories (e.g.
for relativepronouns or control verbs) which mediate non-local dependencies via a co-indexation mecha-nism.
Identifying such non-local dependencies,e.g.
to distinguish between subject and object con-trol (I promise her to come vs.
I persuade herto come), is most likely beyond the scope of anypurely syntactic grammar induction system butwill begin to emerge in a semi-supervised system.2That is, the argument X and result X of a modifier X|Xare not two distinct instances of the same category, but unify.Spurious ambiguity and normal-form parsingComposition and type-raising introduce an expo-nential number of derivations that are semanticallyequivalent, i.e.
yield the same set of dependen-cies.
In supervised CCG parsers (Hockenmaierand Steedman, 2002; Clark and Curran, 2007),this spurious ambiguity is largely eliminated be-cause the derivations in CCGbank are in a normalform that uses composition and type-raising onlywhen necessary, although it can be further allevi-ated via the use of a normal-form parsing algo-rithm (Eisner, 1996; Hockenmaier and Bisk, 2010)that minimizes the use of composition (and type-raising).
We will show below that this spuriousambiguity is particularly deleterious for unsuper-vised CCG parsers that do not impose any normal-form constraints.3 Unsupervised CCG parsingWe now review the unsupervised CCG parser ofBisk and Hockenmaier (2012b; 2013), which istrained over parse forests obtained from a CCGlexicon that was induced from POS-tagged text.Unsupervised CCG induction The inductionalgorithm needs to identify the set of lexicalcategories and to learn the mapping betweenwords and lexical categories, e.g.
:N:{he, girl, lunch,...} N/N:{good, the, eating, ...}S\N:{sleeps, ate, eating,...} (S\N)/N:{sees, ate, ...}S\S:{quickly, today...} S/S:{Today,...}Bisk and Hockenmaier (2012b) define an algo-rithm that automatically induces a CCG lexiconfrom part-of-speech tagged text in an iterative pro-cess.
This process starts with a small amount ofseed knowledge that defines which atomic cate-gories (S, N and conj) can be assigned to whichpart-of-speech tags (nominal POS tags may havethe category N, while verbs may have the cate-gory S).
Based on the assumption that, under mildrestrictions, words can either subcategorize for ormodify the words they are adjacent to, this processproduces lexical categories of increasing complex-ity.
Immediate neighbors of words with categoriesS or N may act as modifiers with categories S|Sor N|N.
The second round of induction can alsointroduce modifiers (X|X)|(X|X) of existing mod-ifiers X|X.
In the first iteration, words with cate-gory S can take adjacent N arguments.
In the sec-ond round, modifiers and words with category S|Nthat are adjacent to words with the category N orIn this exampl , I fills the fi st argument of saw.This is represented by an edge from saw to I, la-beled as a transitive verb ((S\N)/N).
This proce-dure is followed for every argument of every pred-icate, leading to a label d directed graph.Evaluation metrics for supervised CCG parsers(Clark et al, 2002) measure labeled f-score (LF1)precision of these dependencies (requiring thefunctor, argument, lexical category of the func-tor and slot of the argument to all match).
Asecond, looser, evaluation is often also performedwhich measures unlabeled, undirected depen-dency scores (UF1).Non-local dependencies and complex argu-ments One advantage of CCG is its ability torecover the non-local dependencies involved incontrol, raising, or wh-extraction.
Since theseconstructions introduce additional dep ndencies,CCG parsers return dependency graphs (DAGs),not trees.
To obtain these additional dependen-cies, relative pronouns and control verbs requirelexical categories that take complex arguments ofthe form S\NP or S/NP, and a mechanism for co-indexation of the NP inside this argument with an-other NP argument (e.g.
(NP\NPi)/(S|NPi) forrelative pronouns).
These co-indexed subjects canbe seen in Figure 1.1396INpromise(S\N)/(S\N)to(S\N)/(S\N)pay(S\N)/NyouNJohnN,,who(N\N)/(S\N)ran(S\N)/NhomeN,,ate(S\N)/NdinnerN(I, promise) (I, pay) (John, ran) (John, ate)Table 6: Unlabeled predicate argument structures for two sentences, both of whom result in DAGs, nottrees, as the subject is shared by multiple verbs.Additional Category p(cat | tag)((N\N)/(S\N))/N .93 WP$N/(S/N) .14 WPN/(S\N) .08 WP((N\N)/S)\((N\N)/N) .07 WDT((S\S)\(S\S))\N .04 RBRS/(S\N) .04 WPS/(S/N) .02 WPTable 8: Common categories that the algorithmcannot induce, and their corpus probability (giventheir most frequent tag in Sec.
02-21)Model Supervision LF1 UF1B1 POS tags 34.5 60.6B3P&L + Punc & Words 37.1 64.9BC1 + Complex Args 34.9 63.6Table 9: Overall performance of the final systemsdiscussed in this paper (Section 23)dicate missing information which only becomesavailable later in the discourse.7 Final Overall Model PerformanceFinally, we evaluate these models again on thestandard Section 23 against our simplified labelsetand on undirected unlabeled arcs.8 CoNLL vs CCGbank dependenciesFinally, we examine whether the performanceon standard unlabeled dependencies correlateswith performance on CCGbank dependencies (Ta-ble 10)2.
This also allows us to compare oursystems directly to an unsupervised dependencyparser (Naseem et al, 2010), who report directedattachment (unlabeled dependency) scores of adependency-based HDP model that incorporateseither ?universal?
knowledge (e.g.
that adjectivesmay modify nouns) or ?English-specific?
knowl-edge (e.g.
that adjectives tend to precede nouns)in the form of soft constraints.
Their universalknowledge is akin to, but more explicit and de-2BH13 use hyperparameter schemes and report 64.2@20.CCGbank 02-21 WSJ2-21 DAModel LF1 UF1 @10 @20 @1Naseem (Universal) 71.9 50.4Naseem (English) 73.8 66.1B1 33.8 60.3 70.7 63.1 58.4B3P&L 38.3 66.2 71.3 65.9 62.3BC1 34.4 62.0 70.5 65.4 61.9Table 10: Performance on CCGbank and CoNLL-style dependencies (Sections 02-21) for a compar-ison with Naseem et al (2010).tailed than the information given to the inductionalgorithm (see Bisk and Hockenmaier (2013) for adiscussion).
They evaluate on their training data,i.e.
sentences of up to length 20 (without punctu-ation marks) of Sections 02-21 of the Penn Tree-bank3.We see that performance increases on CCG-bank translate to similar gains on the CoNLL de-pendencies on long sentences.
We should notethat we expect this discrepancy to grow as sys-tems capture more fine-grained distinction.
In thisvein, we computed directed attachment recall be-tween CCGbank dependencies and Yamada andMatusumoto?s head finding rules and found onlya 72.5% overlap.
Many of the discrepancies ap-pear to be related to verb chains and analysis ofthe many DAG structures previously discussed.
Afull analsyis of the distinctions is beyond the scopeof this paper but there is an interesting empericalquestion for future work as to whether annotationstandards make learning even more burdensome.9 ConclusionsIn this paper, we have touched upon many linguis-tic phenomena that are common in language andwe feel are currently out of scope for grammar in-duction systems.
We focused our analysis on En-glish for simplicity but many of the same typesof problems exist in other languages and can beeasily identified as stemming from the same lack3With Yamada and Matsumoto?s (2003) head rulesro ise)/(S\N)to(S\N)/(S\N)pay(S\N)/NyouNJohnN,,who( \N)/(S\N)ran(S\ )/homeN,,ate(S\N)/NdinnerN(I, promise) (I, pay) (John, ran) (John, ate)able 6: nlabeled predicate argument structures for two sentences, both of whom result in DAGs, nottrees, as the subject is shared by multiple verbs.Additional Category p(cat | tag)((N\N)/(S\N))/N .93 WP$N/(S/N) .14 WPN/(S\N) .08 WP((N\N)/S)\((N\N)/N) .07 WDT((S\S)\(S\S))\N .04 RBRS/(S\N) .04 WPS/(S/N) .02 WPTable 8: Common categories that the algorithmcannot induce, and their corpus probability (giventheir most frequent tag in Sec.
02-21)Model Supervision LF1 UF11 POS tags 34.5 60.63P&L + Punc & Words 37.1 64.9BC1 + Complex Args 34.9 63.6Table 9: Overall performance of the final systemsdiscussed in this paper (Section 23)dicate missing information which only becomesavailable later in the discourse.7 Final Overall Model PerformanceFinally, we evaluate these models again on thestandard Section 23 against our simplified labelsetand on undirected unlabeled arcs.8 CoNLL vs CCGbank dependenciesFinally, we examine whether the performanceon standard unlabeled dependencies correlateswith performance on CCGbank dependencies (Ta-ble 10)2.
This also allows us to compare oursystems directly to an unsup rvised dependencyparser (Naseem et al, 2010), w o report directedat achment (unlabeled dependency) scores of adependenc -based HDP model that incorporatesith r ?universal?
knowledge (e.g.
that adjectivesmay modify nouns) r ?English-specific?
knowl-edge (e.g.
that adjectives tend to precede nouns)in the form of soft constraints.
Their universalknowledge is akin to, but more explicit and de-2BH13 use hyperparameter schemes and report 64.2@20.CCGbank 02-21 WSJ2-21 DAModel LF1 UF1 @10 @20 @1Naseem (Universal) 71.9 50.4Naseem (English) 73.8 66.1B1 33.8 60.3 70.7 63.1 58.4B3P&L 38.3 66.2 71.3 65.9 62.3BC1 34.4 62.0 70.5 65.4 61.9Table 10: Performance on CCGbank and CoNLL-style dependencies (Sections 02-21) for a compar-ison with Naseem et al (2010).tailed than the information given to the inductionalgorithm (se Bisk and Hock nm ier (2013) for adiscussion).
They evaluate on their raining data,i.e.
sentences of up to length 20 (without punctu-ation marks) of Sections 02-21 of the Penn Tree-bank3.We see that performance increases on CCG-bank translate to similar gains on the CoNLL de-pendencies on long sentences.
We should notethat we expect this discrepancy to grow as sys-tems capture more fine-grained distinction.
In thisvein, we computed directed attachment recall be-tween CCGbank dependencies and Yamada andMatusumoto?s head finding rules and found onlya 72.5% overlap.
Many of the discrepancies ap-pear to be related to verb chains and analysis ofthe many DAG structures previously discussed.
Afull analsyis of the distinctions is beyond the scopeof this paper but there is an interesting empericalquestion for future work as to whether annotationstandards make learning even more burdensome.9 ConclusionsIn this pape , we have tou hed upon any li guis-tic phenomena that are commo in language andwe feel are currently out of scop for grammar in-duction systems.
We focused our a alysis on En-glish for simplicity but many of the same typesof problems exist in other languages and can beeasily identified as stemming from the same lack3With Yamada and Matsumoto?s (2003) head rulesFigure 1: Unlabeled predicate-argument dependency graphs for two sentences with co-indexed subjects.Errors exposed by labeled evaluation We nowillustrate how the lexical categories and labeleddependencies produced by CCG parsers exposelinguistic mistakes.
First, we consider a wildly in-correct analysis of the first example sentence, inwhich the subject is treated as an adverb, and thePP as an NP object of the verb:I saw her from afarS/S (S/N1)/N2N N/N1N> >S/N1N>S>SNone of the labeled directed CCG dependenciesare correct.
But under the more lenient unlabeleddirected evaluation of Gar ette et al (2015), andthe even more lenient unlabeled undirected metricof Clark et al (2002), tw (or three) f the f urdependencies would be de med correct:Incorrect parse C rrect p rsensubj dobjprepS/S(S/N)/N(S/N)/N N/N (S\N)/N(S\S)/N(S\S)/N(S\N)/NpobjI saw h r fr m afar I saw her from afarI saw her from afar I saw her from afarnsubj dobjpr pS/(S/N)/N(S/N)/N N/ (S\N)/N(S\ )/N(S\ )/N(S\N)/NpobjI saw her from af r I saw her from afarI saw her from af r I saw her from afarWhen we translate the CCG analysis to an unla-beled dependency tr e ( nd henc flip the directionof modifier dependencies and add a root edge), asimilar picture emerges, and three out of five at-tachments are deemed correct:Incorrect parse Corre t parsnsubj dobjprepS/S(S/N)/N(S/N)/N N/N (S\N)/N(S\S)/N (S\S)/N(S\N)/NpobjI saw her from afar I saw her from afarI saw her from afar I saw her from afarnsubj dobjpreS/S(S/N)/(S/N)/ N/N (S\N)/(S\S)/N (S\S)/N(S\N)/pobjI saw her from af r I saw her from af rI saw her from af r I saw her from af rWe now turn t a subtle distinction that corre-sponds to a systematic mistake made by all mod-els we evaluate.
The cat gori s of noun-modifyingprepositions (at) and possessive markers (?)
differonly in the directionality of their slashes:X/Y Y )>XX/Y Y|Z )>B1X|ZX/Y Y|Z1|...|Zn)>BnX|Z1|...|ZnY X\Y )<XY|Z X\Y )<B1X|ZY|Z1|...|ZnX\Y )<BnX|Z1|...|ZnA full explanation of the calculus can be foundin (Steedman, 2000) including discussion of atype-raising and a ternary rule for conjunction.
Weassume no type-changing in this work.2.1 DependenciesBy tracing through which word fills which argu-ment of a category a set of dep ndency arcs, la-beled by lexical category and slot, can be extractedand are used for evaluation:lexical head of a lexical category ci is the corre-sponding word wi.
In genera , th lexi al head ofa deriv d category is determined by the (primary)funct r, so that the lexical head of a category Xor X|Z1|...|Znthat resulted from combining X|Yand Y or Y|Z1|...|Znis identical to the lexical headof X.
However, when a modifier X|X with lexicalhead m is combined with an X|... whose lexichead is w, the lexic l head of the resultant X|...is w, not m.2Otherwise, from would become thelexical head of the S\N saw her from afar, and thesentence You know I saw her from af r would havea depende cy between know nd from, rather thanbetw en know a d saw.In general, word wj is a d pendent of word wiif the k-th argument of the lexical category ci ofword wi is instantiated with the lexical categoryof word wj .
In the above derivation:i j cik wiwj1 0 (S\N1)/N21 saw I1 2 (S\N1)/N22 saw her1 3 (S\S1)/N21 from s w4 3 (S\S1)/N22 from afarI saw her from afar(S\S)/N2(S\S)/N1(S\N)/N2(S\N)/N1The use of categories as dependency labelsmakes CCG labels more fine-grained than a stan-dard d pendency grammar.
For exampl , the sub-ject role f intr nsitive, tran itiv and ditr nsitiveverbs are all SUB in dependency treebanks buttake at least th ee different l bels in CCGbank.i j wjLabel2 1 I SUB0 2 saw ROOT2 3 her OBJ2 4 from VMOD4 5 afar PMODI saw her from afarPMODVMODOBJSUBROOTAn additional complexity in CCGbank are cer-tain types of lexical categories (e.g.
for relativepronouns or control verbs) which mediate non-local dependencies via a co-indexation mecha-nism.
Identifying such non-local dependencies,e.g.
to distinguish between subject and object con-trol (I promise her to come vs.
I persuade herto come), is most likely beyond the scope of anypurely syntactic grammar induction system butwill begin to emerge in a semi-supervised system.2That is, the argument X and result X of a modifier X|Xare not two distinct instances of the same category, but unify.Spurious ambiguity and normal-form parsingComposition and type-raising introduce an expo-nential number of derivations that are semanticallyequivalent, i.e.
yield the same set of dependen-cies.
In supervised CCG parsers (Hockenmaierand Steedman, 2002; Clark and Curran, 2007),this spurious ambiguity is largely eliminated be-caus the der vations in CCGbank are in a normalform that uses composition and type-raising onlywhen necessary, although it can be further allevi-ated via the use of a normal-form parsing algo-rithm (Eisner, 1996; Hockenmaier and Bisk, 2010)that minimizes the use of composition (and type-raising).
We will show below that this spuriousambiguity is particularly deleterious for unsuper-vised CCG parsers that do not impose any normal-for constraints.3 Unsup rvis d CCG p rsingWe now review the unsupervised CCG parser ofBisk and H ckenmaier (2012b; 2013), which istrained over p rse forests obta ned from a CCGlexicon that was induced from POS-tagged text.Unsupervised CCG induction The inductionalgorithm needs to identify the set of lexicalcategories and to learn the mapping betweenwords and lexical categories, e.g.
:N:{he, girl, lunch,...} N/N:{good, the, eating, ...}S\N:{sleeps, ate, eating,...} (S\N)/N:{sees, ate, ...}S\S:{quickly, today...} S/S:{Today,...}Bisk and Hockenmaier (2012b) define an algo-rithm that automatically induces a CCG lexiconfrom part-of-speech tagged text in an iterative pro-cess.
This process starts with a small amount ofseed knowledge that defines which atomic cate-gories (S, N and conj) can be assigned to whichpart-of-speech tags (nominal POS tags may havethe category N, while verbs may have the cate-gory S).
Based on the assumption that, under mildrestrictions, words can either subcategorize for ormodify the words they are adjacent to, this processproduces lexical categories of increasing complex-ity.
Immediate neighbors of words with categoriesS or N may act as modifiers with categories S|Sor N|N.
The second round of induction can alsointroduce modifiers (X|X)|(X|X) of existing mod-ifiers X|X.
In the first iteration, words with cate-gory S can take adjacent N arguments.
In the sec-ond round, modifiers and words with category S|Nthat are adjacent to words with the category N orT es depend ncies are the complete predicate ar-gument structu e of the sentence and supervisedevaluation is performed by computing a parser?sprecision and recall on matching the head, depen-dant, category and slot of each arc.
A secondlooser evaluation is ofte also performed whichsimply checks that the undirected and unlabeledarcs match.
An example of this difference th t?sparticularly relevant to the discussion in this paperis the headedness of prepositional phrases versusposessives.Prepositional PhraseTheN/NwomanNat(N\N)/NtheN/NcompanyNlaughedS\N(N\N)/N2(N\N)/N1S\N1Posses iveTheN/NwomanN?s(N/N)\NITN/NcompanyNgrewS\N(N/N)\N1(N/N)\N2 S\N1The undirected edges for the inital noun phraseare identical, but the heads differ.
In CCG, we as-sume that categories of the form X|X where X isatomic are modifiers.
In this way, the first sentenceturns the prepositional phrase (at the company)into a modifier of the woman.
In contrast, in theposessive sentence woman ?s modifies the com-pany.
Because, the arcs are so similar, the undi-rected unlabeled score for confusing these analy-ses is 80% correct but the labeled score would be20%.
This example demonstrates how the head-edness of the resultant syntactic analysis requiressemantic knowledge about people and companies,as getting the wrong head leads to the companylaughi g r other s mantically n nse sical analy-ses.2.2 Using Labels to Diagnose ErrorsFinally, we quickly provide an incorrect analysisof the first xample sentence as a simple exercisein using labels to diagnos mistakes:I saw her fro afarS/S (S/N1)/N2N N/N1N> >S/N1N>S>SIn this example, the verb analy is is trying to an-alyze th language as VOS instead of SVO.
Oncefamiliar with reading CCG categories the model?soutput nd istake can b easily diagnos d. Amodel producing this analysis is not learning thecorrect word order of the language, nor the correctrole for repositions by taking afar as a subject.This typ f mis ake is obvious to a spe ker of thelanguage even without a treebank for evaluationIn this way we believe label prediction eases theanalysis burden when diagnosing a system?s out-put.3 A Simplified Labeled EvaluationIn languages with treebanks, labeled evaluationcan make this style of analysis even simpler.Fortunately, approaches using CCG can producelabeled output but unfortunately there are mis-matches between the basic set of categories andthose used in treebanks.
We will focus on the En-glish CCGbank but these details apply with onlyminor changes to German and Chinese as well.3.1 SimplificationBecause the lexical categories guide parsing, theset used in supervised parsing is extremely largeand augmented with features.
These features arenot strictly part of the CCG calculus but markproperties of the underlying words, for exampleindicating if a verb is declarative or infinitival or ifa noun phrase contains a number.
These featuresare written as brackets modifying the atomic sym-bols: (S[dcl]\NP, N/N[num], ... ).
Prior work onsupervised parsing with CCG found that many ofthese features can be recovered with proper mod-eling of latent state splitting (Fowler and Penn,2010).
In our proposed simplification we re-move these languge specific features.
Secondly,X/Y Y>XX/Y Y|Z>B1X|ZX/Y Y|Z1|...|Zn >BnX|Z1|...|ZnY X\Y<XY|Z X\Y<B1X|ZY|Z1|...|ZnX\Y<BnX|Z1|...|ZnA full xpla at o of the calcul s can be foundin (Steed an, 2000) inclu ing discussion of atype-raising nd a te nary rule or conjunction.
eassu e n type-changing in this work.2.1 DependenciesBy tracing hrough which word fills which argu-ent of a categ ry a set of depen ncy arcs, la-beled by lexical category and slot, can be extractedand are used for evaluation:lexical head of a lexical category ci is the corre-sponding word wi.
In general, the lexical head ofa derived category is deter ined by the (pri ary)funct r, so that the lexical head of a category Xor X|Z1|...|Znthat resulted fro co bining X|Yand Y or Y|Z1|...|Znis identical t the lexical headof X.
However, when a odifier X|X with lexicalhead m is co bined with an X|... whose lexicalhead is w, the lexical head of the resultant X|...is w, not m.2Otherwise, from would beco e thelexical head of the S\N saw her from afar, and thesentence You know I saw her from afar would havea dependency between know and from, rather thanbetween know and saw.In general, word wj is a d pendent of word wiif the k-th rgu ent of the lexi al category c ofword wi is instantiated with the lexical categoryof word wj .
In the above derivation:i j cik wiwj1 0 (S\N1)/N21 saw I1 2 (S\N1)/N22 saw her1 3 (S\S1)/N21 from s w4 3 (S\S1)/N22 from farI saw her from afar(S\S)/N2(S\S)/N1(S\N)/N2(S\N)/N1The use of at g ries as d pend ncy labelsak s CCG label ore fin -gr in d than a stan-dard d pendency gra ar.
For exa pl , the sub-ject rol of intransitive, transitive and ditransitivev rbs are all SUB in dependency treebanks buttake t least thre different labels in CCGbank.i j wjLabel2 1 I SUB0 2 saw ROOT2 3 her OBJ2 4 from VMOD4 5 afar PMODI saw her from afarPMODVMODOBJSUBROOTAn additional co plexity in CCGbank are cer-tain types of lexical categories (e.g.
for relativepronouns or control verbs) which ediate non-local dependencies via a co-indexation echa-nis .
Identifying such non-local dependencies,e.g.
to distinguish between subject and object con-trol (I promise her to come vs.
I persuade herto come), is ost likely beyond the scope of anypurely syntactic gra ar induction syste butwill begin to e erge in a se i-supervised syste .2That is, the argument X and result X of a modifier X|Xare not two distinct instanc s of the same categor , but unify.Spurious a biguity and nor al-for parsingCo position and type-raising introduce an expo-nential nu ber of derivations that are se anticallyequivalent, i.e.
yield the sa e set of dependen-cies.
In supervised CCG parsers (Hocken aierand Steed an, 2002; Clark nd Curran, 2007),this spurious a biguity is largely eli inated be-cause the derivations in CCGbank are in a nor alfor that uses co position and type-r ising onlywhen nece sary, although it can be further allevi-ted via the us of a nor al-for parsing algo-rith (Eisner, 1996; Hocken aier and Bisk, 2010)that ini izes the use of co position (and type-raising).
e will show below that this spuriousa biguity is particularly deleterious for unsuper-vis d CCG pa sers that do not i pose any nor al-form constrai ts.3 nsupervised parsinge now review the unsupervised CCG parser ofBi k and Ho ke ai r (2012b; 2013), which istrained over parse forests obtained fro a CCGlexicon that was induced fro POS-tagged text.Unsupervised CC induction The inductionalgorith needs to identify the set of lexicalcateg ries and to learn the apping betweenwords and lexical cate ories, e.g.
:N:{he, girl, lunch,...} N/N:{good, the, eating, ...}S\N:{sleeps, ate, eating,...} (S\N)/N:{sees, ate, ...}S\S:{quickly, today...} S/S:{Today,...}Bisk and H cken aier (2012b) define an algo-rith that auto atically induces a CCG lexiconfro part-of-speech tagged text in an iterative pro-cess.
This process starts with a s all a ount ofseed knowledg that defin s which o ic cate-gories (S, N and co j) can be assigned to whichpart-of-spee tags (no inal POS tags ay havethe category N, while verbs ay have the cate-gory S).
Bas d on the assu ption that, under ildr stric ions, words can either subcategorize for orodify the words th y are djacent o, this processproduces lexical categories of increasing co plex-ity.
I ediate neighbors of words with categoriesS or N ay act as odifiers with categories S|Sor N|N.
The second round of induction can alsointroduce odifiers (X|X)|(X|X) of existing od-ifi rs X|X.
In the fir t iteration, words with cate-gory S can take adjacent N argu ents.
In the sec-ond round, odifiers and words with category S|Nthat are adjacent to words with the category N orThese depend ncies are th co lete predi at ar-gu ent structure of the sentence and supervisedevaluation is perfor ed by co puting a parser?sprecision and recall on atching the head, depen-dant, category and slot of each arc.
A secondlooser evaluation is of e also perfor ed whichsi ply checks that t e undirected and unl beledarcs atch.
An ex ple of this difference that?sparticularly relevant to the discus ion in this paperis the headedness of prepositional phrases versusposessives.Prepositional PhraseTheN/NwomanNat(N\N)/NtheN/NcompanyNlaughedS\N(N\N)/N2(N\N)/N1S\N1Posses iveTheN/NwomanN?s(N/N)\NITN/NcompanyNgrewS\N(N/N)\N1(N/N)\N2 S\N1The undirected edges for the inital noun phraseare identical, but the heads differ.
In CCG, we as-su e that categories of the for X|X where X isato ic are odifiers.
In this way, the first sentenceturns the prepositional phrase (at the company)into a odifier of the woman.
In contrast, in theposessive sentence woman ?s odifies the com-pany.
Because, the arcs are so si ilar, the undi-rected unlabeled score for confusing these analy-ses is 80 correct but the labeled score would be20 .
This exa ple de onstrates how the head-edness of the resultant syntactic analysis requiresse antic knowledge about people and co panies,as getting the wrong head leads to the co panylaughi g or other se antically n nsensical analy-ses.2.2 Using Labels to Diagnose ErrorsFinally, we quickly provide an incorrec analysisof the first exa ple sent as a si ple exercisein using labels to diagnose istakes:I saw her fro afarS/S (S/N1)/N2N N/N1N> >S/N1N>S>SIn this exa ple, the verb analy is is trying to an-alyze th language as VOS instead of SVO.
Oncefa iliar with reading CCG categories the del?soutput nd istake can b easily diagnos d. Aodel producing this analysis is not learning thecorrect word order of the la guage, nor the correctrol for repositions by taking afar s a subject.This type of istake is obvious to a speaker of thelanguage even without a treebank for evaluationIn this way we believe label prediction eases theanalysis burden when diagnos ng a syste ?s out-put.3 Si plified Labeled EvaluationIn languages with treebanks, labeled evaluationcan ake this style of analysis even si pler.Fortunately, approaches using CCG can producelabeled output but unfortunately there are is-atches between the basic set of categories andthose used in treebanks.
e will focus on the En-glish CCGbank but these details apply with onlyinor changes to Ger an and Chinese as well.3.1 Si plificationBecause the lexical categories guide parsing, theset used in supervised parsing is extre ely largeand aug ented with features.
These features arenot strictly part of the CCG calculus but arkproperties of the underlying words, for exa pleindicating if a verb is declarative or infinitival or ifa noun phrase contains a nu ber.
These featuresare written as brackets odifying the ato ic sy -bols: (S[dcl]\NP, N/N[nu ], ... ).
Prior work onsupervised parsing with CCG found that any ofthese features can be recovered with proper od-eling of latent state splitting (Fowler and Penn,2010).
In our proposed si plification we re-ove these languge specific features.
Secondly,The unlabeled dependencies inside the nounphrases are identical, but the heads differ.
Thefirst s ntence turns the prepositional phrase (at thecompany) into a modifier of wo an.
In contrast,in the pos essive case, woman ?s modifies com-pany.
According to an unlabeled (directed) s ore,confusing these analyses would b 80% corr ct,wh reas LF1 would only be 20%.
But without asem tic bias for companies growing and womenlaughing, there is no signal for the learner.3 Lab led Evaluat on f r CCG InductionWe have just seen that labeled evaluation can ex-pose many linguistically important mistakes.
Inorder to enable a fair and informative comparisonof unsupervised CCG parsers against the lexicalcategories and labeled dependencies in CCGbank,we define a simplification of CCGbank?s lexicalcategories th t oes not alter the number or direc-tion of depend ncies, but makes the categories anddependency labels directly comparable to thoseproduced by an unsupervised parser.
We alsodo not alter he d rivat ns them lves, althoughthese may contain type-changing rules (which al-low e.g.
participial verb phrases S[ng]\NP to beused as NP modifiers NP\NP) that are beyond thesco e f our i duction algorithm.Although the CCG derivati ns and dependen-cies that CCG-based arsers retur hould in prin-ciple be amenable to a quantitative labeled evalu-ation when a gold-standard CCG corpus is avail-able, there may be min r sy tematic differencesbe ween the sets f categories assumed by the in-duced parser and those in the treebank.
In par-ticular, the lexic l categories in the English CCG-bank are augmented with morphosyntactic fea-tures that indicate e.g.
whether sentences aredeclarative (S[dcl]), or verb phrases are infiniti-val (S[to]\NP).
Prior work on supervis d parsingwith CCG foun that many of these features canbe r cover d with proper m deling of latent statesplitting (Fowler and Penn, 2010).
Since we wisht ev luate a system that does not aim to inducesuch features, we remove them.
We also removethe distinction between noun phrases (NP) andnouns (N), which is predicated on knowledge of1397Our simplification of CCGbank?s lexical categoriesCongress has n?t lifted the ceilingOriginal NP (S[dcl]\NP)/(S[pt]\NP) (S\NP)\(S\NP) (S[pt]\NP)/NP NP[nb]/N NSimplified N (S\N)/(S\N) S\S (S\N)/N N/N NFigure 2: We remove morphosyntactic features, simplify verb phrase modifiers, and change NP to N.CCGbank w/out Feats SimplifiedAll 1640 458 444Lexical 1286 393 384Table 1: Category types in CCGbank 02-21determiners and other structural elements of a lan-guage.
Finally, CCGbank distinguishes betweensentential modifiers (which have categories of theform S|S, without features) and verb phrase mod-ifiers ((S\NP)|(S\NP), again without features).But since the NP argument slot of a VP mod-ifier is never filled, we can maintain the samenumber of gold standard dependencies by remov-ing this distinction and changing all VP modifiersto be of the form S|S.
However, categories ofthe form (S[?]\NPi)/(S[?
]\NPi), which are usede.g.
for modals and auxiliaries, are changed to(S\Ni)/(S\Ni), allowing us to maintain the de-pendency on the subject.
With these three simplifi-cations we eliminate much of the detailed knowl-edge required to construct the precise CCGbank-style categories, and dramatically reduce the set ofcategories without losing expressive power.
Onedistinction that we do not conflate, even thoughit is currently beyond the scope of the induc-tion algorithm, is the distinction between PP argu-ments (requiring prepositions to have the categoryPP/NP) and adjuncts (requiring prepositions to be(NP\NP)/NP or ((S\NP)\(S\NP))/NP).This simplification is consistent with the mostbasic components of CCG and can therefore beeasily used for the evaluation and analysis of anyweakly or fully supervised CCG system, not justthat of Bisk and Hockenmaier (2012).
An examplesimplification is present in Figure 2, and the reduc-tion in the set of categories can be seen in Table 1.Similar simplifications should also be possible forCCGbanks in other languages.4 Our approachThere are two parts to our approach: 1) induc-ing a CCG grammar from seed knowledge and 2)learning a probability model over parses.
The in-duction algorithm (Bisk and Hockenmaier, 2012)uses the seed knowledge that nouns can take theCCG category N, that verbs can take the categoryS and may take N arguments, and that any wordmay modify a constituent it is adjacent to, to iter-atively induce a CCG lexicon to parse the train-ing data.
In Bisk and Hockenmaier (2013), weintroduced a model that is based on HierarchicalDirichlet Processes (Teh et al, 2006).
This HDP-CCG model gave state-of-the-art performance on anumber languages, and qualitative analysis of theresultant lexicons indicated that the system waslearning the word order and many of the correctattachments of the tested languages.
But this sys-tem also had a number of shortcomings: the in-duction algorithm was restricted to a small frag-ment of CCG, the model emitted only POS tagsrather than words, and punctuation was ignored.Here, we use our previous HDP-CCG system as abaseline, and introduce three novel extensions thatattempt to address these concerns.5 Experimental SetupFor our experiments we will follow the standardpractice in supervised parsing of using WSJ Sec-tions 02-21 for training, Section 22 for develop-ment and error analysis, and a final evaluation ofthe best models on Section 23.
Because the in-duced lexicons are overly general, the memoryfootprint grows rapidly as the complexity of thegrammar increases.
For this reason, we only trainon sentences that contain up to 20 words (as wellas an arbitrary number of punctuation marks).
Allanalyses and evaluation are performed with sen-tences of all lengths unless otherwise indicated.Finally, Bisk and Hockenmaier (2013) followedLiang et al (2007) in setting the values of the hy-perparameters ?
to powers (eg.
the square) of thenumber of observed outcomes in the distribution.But when the output consists of words rather thanPOS tags, the concentration parameter ?=V2istoo large to allow the model to learn.
For this rea-son, experiments will be reported with all hyper-parameters set to a constant of 2500.11We tested three values (1000, 2500, 5000) and found thatthe basic model at 2500 performed closest to the previously1398Base + Lexicalization + Punctuation + Punc & Lex + Allow (X|X)|XOnly Atomic Arguments B134.2 35.2 36.3 36.9 36.8(S, N) B334.4 35.1 33.8 38.9 38.8Allow Complex Arguments B133.0 34.9 33.2 35.7 35.8(S, N, S|N) B329.4 29.5 31.2 31.2 31.2Table 2: The impact of our changes to Bisk and Hockenmaier?s (2013) model (henceforth: B1, top left)on CCGbank dependencies (LF1, Section 22, all sentences).
The best overall model (B3P&L) uses B3,punctuation and lexicalization.
The best model with complex arguments (BC1) uses only B1.6 Extending the HDP-CCG systemWe now examine how extending the HDP-CCGbaseline model to capture lexicalization and punc-tuation, and how increasing the complexity of theinduced grammars affect performance (Table 2).6.1 Modeling LexicalizationIn keeping with most work in grammar inductionfrom part-of-speech tagged text, Bisk and Hocken-maier?s (2013) HDP-CCG treats POS tags t ratherthan words w as the terminals it generates basedon their lexical categories c. The advantage of thisapproach is that tag-based emissions p(t|c) are alot less sparse than word-based emissions p(w|c).It is therefore beneficial to first train a model thatemits tags rather than words (Carroll and Rooth,1998), and then to use this simpler model to ini-tialize a lexicalized model that generates words in-stead of tags.
To perform the switch we simply es-timate counts for the parse forests using the unlex-icalized model during the E-Step and then applythose counts to the lexicalized model during theM-Step.
Inside-Outside then continues as before.Many words, like prepositions, differ systemati-cally in their preferred syntactic role from that oftheir part-of-speech tags.
This change benefits allsettings of the model (Column 2 of Table 2).6.2 Modeling PunctuationSpitkovsky et al (2011) performed a detailed anal-ysis of punctuation for dependency-based gram-mar induction, and proposed a number of con-straints that aimed to capture the different waysin which dependencies might cross constituentboundaries implied by punctuation marks.A constituency-based formalism like CCG al-lows us instead to define a very simple, but effec-tive Dirichlet Process (DP) based Markov gram-reported dependency evaluation comparison with the work ofNaseem et al (2010).
We fixed this hyperparameter settingfor experimental simplicity but a more rigorous grid searchmight find better parameters for the complex models.mar that emits punctuation marks at the maximalprojections of constituents.
We note that CCGderivations are binary branching, and that virtuallyevery instance of a binary rule in a normal-formderivation combines a head X or X|Y with an ar-gument Y or modifier X|X.
Without reducing theset of strings generated by the grammar, we cantherefore assume that punctuation marks can onlybe attached to the argument Y or the adjunct X|X:Y, ,X/YXYX?X, ,XXX?XTo model this, for each maximal projection (i.e.whenever we generate a non-head child) with cate-gory C, we first decide whether punctuation marksshould be emitted (M = {true, false}) to the leftor right side (Dir) of C. Since there may be mul-tiple adjacent punctuation marks (...
.?
), we treatthis as a Markov process in which the history vari-able captures whether previous punctuation markshave been generated or not.
Finally, we generatean actual punctuation mark wm:p(M | Dir ,Hist ,C) ?
DP (?, p(M | dir))p(M | Dir) ?
DP (?, p(M))p(wm| Dir ,Hist ,C) ?
DP (?, p(wm| dir , hist))p(wm| Dir ,Hist) ?
DP (?, p(wm))We treat # and $ symbols as ordinary lexi-cal items for which CCG categories will be in-duced by the regular induction algorithm, but treatall other punctuation marks, including quotes andbrackets.
Commas and semicolons (,, ;) canact both as punctuation marks generated by thisMarkov grammar, and as conjunctions with lexicalcategory conj.
This model leads to further perfor-mance gains (Columns 3 and 4 of Table 2).6.3 Increasing Grammatical ComplexityThe existing grammar induction scheme is verysimplistic.
It assumes that adjacent words eithermodify one another or can be taken as arguments.Left unconstrained this space of grammatical cat-1399Model Supertagging LF1 UF1B159.2 34.5 60.6BC159.9 34.9 63.6BP&L362.3 37.1 64.9Table 3: Test set performance of the final systemsdiscussed in this paper (Section 23)egories introduced grows very rapidly, introduc-ing a tremendous number of incorrect categories(analyzed later in Table 9).
For this reason Biskand Hockenmaier (2013) applied the HDP-CCGmodel to a context-free fragment of CCG, limit-ing the arity of lexical categories (number of ar-guments they can take) to two and the arity ofcomposition (how many arguments can be passedthrough composition) to one.
We know the spaceof grammatical constructions is larger than this, sowe will allow the model to induce categories withthree arguments and use generalized composition(B3).
Bisk and Hockenmaier (2013) allow lexicalcategories to only take atomic arguments, but, asexplained above, non-local dependencies requirecomplex arguments of the form S|N.
We thereforeallow lexical categories to take up to one complexargument of the form S|N.
Atomic lexical cate-gories are not allowed to take complex arguments,eliminating S|(S|N) and N|(S|N).
Increasing thesearch space (Rows 3 and 4 of Table 2) shows cor-responding decreases in performance.Finally, Bisk and Hockenmaier (2013) elim-inated the possessive-preposition ambiguity ex-plained above by disallowing categories of theform (X\X)/X and (X/X)\X to be used simulta-neously.
Removing this restriction does not harmperformance (Column 5 of Table 2).6.4 Summary and test set performanceTable 2 shows the performance of 20 differentmodel settings on Section 22 under the simpli-fied labeled CCG-based dependency evaluationproposed above, starting with Bisk and Hocken-maier?s (2013) original model (henceforth: B1,top left).
We see that modeling punctuation andlexicalization both increase performance.
Wealso show that allowing categories of the form(X\X)/X and (X/X)\X on top of the lexicalizedmodels with punctuation does not lead to a notice-able decrease in performance.
We also see that anincrease in grammatical and lexical complexity isonly beneficial for the grammars that allow onlyatomic arguments, and only if both lexicalizationCCGbank 02-21 WSJ2-21 DAModel LF1 UF1 @10 @20 @?Naseem (Universal) 71.9 50.4Naseem (English) 73.8 66.1B133.8 60.3 70.7 63.1 58.4BC134.4 62.0 70.5 65.4 61.9BP&L338.3 66.2 71.3 65.9 62.3Table 4: Performance on CCGbank and CoNLL-style dependencies (Sections 02-21) for a compar-ison with Naseem et al (2010).and punctuation are modeled.
Allowing complexarguments is generally not beneficial, and perfor-mance drops further if the grammatical complex-ity is increased to B3.
Our further analysis willfocus on the three bolded models, B1, BC1(thebest model with complex arguments) and BP&L3(the best overall model), whose supertag accuracy,labeled (LF1) and unlabeled undirected CCG de-pendency recovery on Section 23 are shown in Ta-ble 3.
We see that BC1and BP&L3both outperformB1on all metrics, although the unlabeled met-ric (UF1) perhaps misleadingly suggests that BC1leads to a greater improvement than the supertag-ging and LF1 metrics indicate.6.5 CCGbank vs. dependency treesFinally, to compare our models directly to a com-parable unsupervised dependency parser (Naseemet al, 2010), we evaluate them against the un-labeled dependencies produced by Yamada andMatsumoto?s (2003) head rules for Sections 02-21 of the Penn Treebank (Table 4)2.
Naseem et al(2010) only report performance on sentences of upto length 20 (without punctuation marks).
Theirapproach incorporates prior linguistic knowledgeeither in the form of ?universal?
constraints (e.g.that adjectives may modify nouns) or ?English-specific?
constraints (e.g.
that adjectives tend tomodify and precede nouns).
These universal con-straints are akin to, but more explicit and detailedthan the information given to the induction algo-rithm (see Bisk and Hockenmaier (2013) for a dis-cussion).
Comparing these numbers to labeledand unlabeled CCG dependencies on the same cor-pus (all sentences, hence, @?
), we see that per-formance increases on CCGbank do not translateto similar gains on these unlabeled dependencies.While we have done our best to convert the predi-cate argument structure of CCG into dependencies2BH13 use hyperparameter schemes and report 64.2@20.1400Correct B1BP&L3BC1Category LR Used instead (%) LR Used instead (%) LR Used instead (%)N 82.6 N/N 7.5 74.5 N/N 8.3 77.4 N/N 9.8N/N 78.5 (S\S)\(S\S) 9.8 71.9 (S\S)\(S\S) 8.7 80.6 N 7.7S\N 17.3 S\S 43.5 22.1 S\S 27.6 18.3 S\S 39.5S\S 38.1 N 24.3 34.9 N 16.0 39.4 N 22.7S/S 37.8 N\N 20.8 41.1 N/N 16.3 57.2 (S\S)/S 13.8(N\N)/N 64.3 (S\S)/N 20.8 60.5 (S\S)/N 13.8 53.1 (S\S)/N 23.8(S\N)/N 25.6 S/N 27.0 26.0 (S/N)/N 23.5 29.4 S/N 22.3(S\S)/N 51.0 (N\N)/N 23.1 48.0 (N\N)/N 18.2 62.6 N/N 10.1(S\N)/S 60.7 S\N 12.1 55.7 S\N 12.4 57.9 S\N 11.0(S\S)/S 38.0 (N\N)/N 35.2 50.8 S/S 14.4 61.5 N 7.5Table 5: Detailed supertagging analysis: Recall scores of B1, BC1, and B3P&Lon the most commonrecoverable (simplified) lexical categories in Section 22 along with the most commonly produced error.Category Example usage Used instead by BC1(%)(N/N)\N The woman ?s company ... (N\N)/N 89.9 N/N 3.7 N 2.9(S/S)/N Before Monday, ... S/S 69.9 N/N 14.8 (N\N)/N 8.2(N/N)/(N/N) The very tall man ... N/N 38.0 (S\S)\(S\S) 33.9 (S\S)/N 10.1(N\N)/(S\N) John, who ran home, ... (S\S)/(S/N) 26.5 N\N 23.3 S/S 14.9(S\N)/(S\N) I promise to pay ... S\N 32.6 (S\S)/(S/N) 21.5 (S\N)/(S/N) 12.4((S\N)/N)/N I gave her a gift.
(S\N)/N 34.6 (S/N)/N 34.6 N/N 7.7((S\N)/(S\N))/N I persuaded her to pay ... (S\N)/N 24.8 (S/N)/N 22.0 N/N 11.0Table 6: Categories that are in the search space of the induction algorithm, but do not occur in any Viterbiparse, and what BC1uses instead.there are many constructions which have vastlydifferent analysis, making a proper conversion toodifficult for the scope of this paper.37 Error analysisSupertagging error analysis We first considerthe lexical categories that are induced by the mod-els.
Table 5 shows the accuracy with which theyrecover the most common gold lexical categories,together with the category that they most oftenproduced instead.
We see that the simplest model(B1) performs best on N, and perhaps over gen-erates (N\N)/N (noun-modifying prepositions),while the overall best model (BP&L3) outperformsboth other models only on intransitive verbs.The most interesting component of our analysisis the long tail of constructions that must be cap-tured in order to produce semantically appropriaterepresentations.
We can inspect the confusion ma-trix of the lexical categories that the model fails touse to obtain insight into how its predictions dis-agree with the ground truth, and why these con-structions may require special attention.
Table 6shows the most common CCGbank categories that3The overlap (F-score of unlabeled undirected attachmentscores) between CCGbank dependencies and those obtainedvia Matsumoto?s head finding rules is only 81.9%.were in the search space of some of the more com-plex models (e.g.
BC3), but were never used by anyof the parsers in a Viterbi parse.
These includepossessives, relative pronouns, modals/auxiliaries,control verbs and ditransitives.
We show the cat-egories that the BC1model uses instead.
The goldcategories shown correspond to the bold words inTable 6.
While the reason many of these casesare difficult is intuitive (e.g.
very modifying tallinstead of man), a more difficult type of errorthan previously discussed is that of recoveringnon-local dependencies.
The recovery of non-local dependencies is beyond the scope of bothstandard dependency-based approaches and Biskand Hockenmaier (2013)?s original induction al-gorithm.
But the parser does not learn to use lexi-cal categories with complex arguments correctlyeven when the algorithm is extended, to inducethem.
For example, BC1prefers to treat auxiliariesor equi verbs like promise as intransitives ratherthan as an auxiliary that shares its subject withpay.
The surface string supports this decision, asit can be parsed without having to capture the non-local dependencies (top row) present in the correct(bottom row) analysis:I promise to pay youN S\N (S\S)/S S/N NN (S\N)/(S\N) (S\N)/(S\N) (S\N)/N N14011st Argument 2nd ArgumentB1BC1BP&L3B1BC1BP&L3N/N 68.4 69.7 71.6S\N 12.2 24.9 14.6S\S 17.0 16.2 18.7S/S 24.0 27.1 33.8(N\N)/N 49.7 54.4 51.2 41.0 46.2 42.4(S\N)/N 26.6 32.9 34.4 30.6 33.2 33.8(S\S)/N 21.6 19.2 24.7 24.0 24.9 29.3(S\N)/S 23.9 50.3 32.5 25.2 59.1 35.0(S\S)/S 6.1 22.7 14.1 9.5 34.6 19.5Table 7: LF1 scores of B1, BC1and B3P&Lon themost common dependency types in Section 22.We also see that this model uses seemingly non-English verb categories of the form (S/N)/N, bothfor ditransitives, and object control verbs, perhapsbecause the possibly spurious /N argument couldbe swallowed by other categories that take argu-ments of the form S/N, like the (incorrect) treat-ment of subject relative pronouns.
One possiblelesson we can extract from this is that practicalapproaches for building parsers for new languagesmight need to focus on injecting semantic infor-mation that is outside the scope of the learner.Dependency error analysis Table 7 shows thelabeled recall of the most common dependencies.We see that both new models typically outper-form the baseline, although they yield differentimprovements on different dependency types.
BC1is better at recovering the subjects of intransitiveverbs (S\N) and verbs that take sentential com-plements ((S\N)/S), while B3is better for simpleadjuncts (N/N, S/S, S\S) and transitive verbs.Wh-words and the long tail To dig slightlydeeper into the set of missing constructions, wetried to identify the most common categories thatare beyond the search space of the current induc-tion algorithm.
We first computed the set of cat-egories used by each part of speech tag in CCG-bank, and thresholded the lexicon at 95% tokencoverage for each tag.
Removing the categoriesthat contain PP and those that can be induced bythe algorithm in its most general setting, we areleft with the categories shown in Table 8.
The tagsthat are missing categories are predominantly wh-words required for wh-questions, relative clausesor free relative clauses.
Some of these categoriesviolate the assumptions made by the induction al-gorithm: question words return a sentence (S) butare not themselves verbs.
Free relative pronounsreturn a noun, but take arguments.
However, this isAdditional Category p(cat | tag)((N\N)/(S\N))/N .93 WP$N/(S/N) .14 WPN/(S\N) .08 WP((N\N)/S)\((N\N)/N) .07 WDT((S\S)\(S\S))\N .04 RBRS/(S\N) .04 WPS/(S/N) .02 WPTable 8: Common categories that the algorithmcannot induceSize, ambiguity, coverage and precisionof the induced lexiconsArguments: Atomic Complex# Lexical Arity: 2 3 2 3# Lexical Categories 37 53 61 133Avg.
#Cats / Tag 26.4 29.5 42.3 56.3Token-based Coverage 84.3 84.4 89.8 90.2Type-based Coverage 20.3 21.6 27.0 32.4Type-based Precision 81.1 60.4 65.6 36.1Table 9: Size, ambiguity, coverage and precision(evaluated on Section 22) of the induced lexicons.a surprisingly small set of special function wordsand therefore perhaps a strategic place for super-vision.
Questions in particular pose an interestinglearning question ?
how does one learn that theseconstructions indicate missing information whichonly becomes available later in the discourse?Grammatical complexity and size of the searchspace As lexical categories are a good proxy forthe set of constructions the grammar will enter-tain, we can measure the size and ambiguity of thesearch space as a function of the number of lexicalcategory types it induces as compared to the per-centage that are actually valid categories for thelanguage.
In Table 9, we compare the lexicons in-duced by variants of the induction algorithm bytheir token-based coverage (the percent of tokensin Sections 22 for which the induced tag lexiconcontains the correct category), type-based cover-age (the percent of category types that the inducedlexicon contains), as well as type-based precision(the percent of induced category types that occurin Section 22).
This analysis is independent of thelearned models, as their probabilities are not takeninto account.
We see that as the number of lex-ical categories induced (subject to the constraintsof Bisk and Hockenmaier (2012)) increases, thepercent that are valid English categories decreasesrapidly (type-based precision falls from 81.1% to36.1%).
Despite this, and despite a high token1402coverage of up to 90%, we still miss almost 70%of the required category types.
This helps explainwhy performance degrades so much for BC3, thearity three lexicon with complex arguments.8 Dealing with Non-Local DependenciesWhile the methodology used here is restricted toCCG based algorithms, we believe the lessons tobe very general.
The aforementioned construc-tions involve optional arguments, non-local de-pendencies, and multiple potential heads.
Eventhough CCG is theoretically expressive enough tohandle these constructions, they present the un-supervised learner with additional ambiguity thatwill pose difficulties independently of the under-lying grammatical representation.For example, although our approach learns thatsubject NPs are taken as arguments by verbs, thetask of deciding which verb to attach the subjectto is frequently ambiguous.
This most commonlyoccurs in verb chains, and is compounded in thepresence of subject-modifying relative clauses (inCCGbank, both constructions are in fact treatedas several verbs sharing a single subject).
Toillustrate this, we ran the BC1and B3P&Lsystems onthe following three sentences:1.
The woman won an award2.
The woman has won an award3.
The woman being promoted has won an awardThe single-verb sentence is correctly parsed byboth models, but they flounder as distractors areadded.
Both treat has as an intransitive verb, wonas an adverb and an as a preposition:The woman won an awardB3P&L/BC1: N/N N (S\N)/N N/N NThe woman has won an awardB3P&L/BC1: N/N N S\N S\S (S\S)/N NTo accommodate the presence of two additionalverbs, both models analyze being as a noun modi-fier that takes promoted as an argument.
BC1(cor-rectly) stipulates a non-local dependency involv-ing promoted, but treats it (arguably incorrectly)as a case of object extraction:... being promoted has won an awardB3P&L(N\N)/S S S\N S\S (S\S)/N NBC1(N\N)/(S/N) S/N S\N S\S (S\S)/N NDiscovering these, and many of the other sys-tematic errors describe here, may be less obvi-ous when analyzing unlabeled dependency trees.But we would expect similar difficulties for anyunsupervised approach when sentence complexitygrows without a specific bias for a given analysis.9 ConclusionsIn this paper, we have introduced labeled evalu-ation metrics for unsupervised CCG parsers, andhave shown that these expose many common syn-tactic phenomena that are currently out of scopefor any unsupervised grammar induction systems.While we do not wish claim that CCGbank?s anal-yses are free of arbitrary decisions, we hope tohave demonstrated that these labeled metrics en-able linguistically informed error analyses, andhence allow us to at least in part address the ques-tion of where and why the performance of theseapproaches might plateau.
We focused our analy-sis on English for simplicity, but many of the sametypes of problems exist in other languages and canbe easily identified as stemming from the samelack of supervision.
For example, in Japanesewe would expect problems with post-positions, inGerman with verb clusters, in Chinese with mea-sure words, or in Arabic with morphology andvariable word order.We believe that one way to overcome the is-sues we have identified is to incorporate a seman-tic signal.
Lexical semantics, if sparsity can beavoided, might suffice; otherwise learning withgrounding or an extrinsic task could be used tobias the choice of predicates, their arity and in turnthe function words that connect them.
Alterna-tively, a simpler solution might be to follow thelead of Boonkwan and Steedman (2011) or Gar-rette et al (2015) where gold categories are as-signed by a linguist or treebank to tags and words.It is possible that more limited syntactic supervi-sion might be sufficient if focused on the semanti-cally ambiguous cases we have isolated.More generally, we hope to initiate a conver-sation about grammar induction which includes adiscussion of how these non-trivial constructionscan be discovered, learned, and modeled.
Relat-edly, in future extensions to semi-supervised orprojection based approaches, these types of con-structions are probably the most useful to get rightdespite comprising the tail, as analyses withoutthem may not be semantically appropriate.
Insummary, we hope to begin to pull back the veilon the types of information that a truly unsuper-vised system, if one should ever exist, would needto learn, and we pose a challenge to the commu-nity to find ways that a learner might discover thisknowledge without hand-engineering it.140310 AcknowledgmentsThis material is based upon work supported bythe National Science Foundation under Grants No.1053856, 1205627 and 1405883.
Any opinions,findings, and conclusions or recommendations ex-pressed in this material are those of the author(s)and do not necessarily reflect the views of the Na-tional Science Foundation.ReferencesYonatan Bisk and Julia Hockenmaier.
2012.
SimpleRobust Grammar Induction with Combinatory Cat-egorial Grammars.
In Proceedings of the Twenty-Sixth Conference on Artificial Intelligence (AAAI-12), pages 1643?1649, Toronto, Canada, July.Yonatan Bisk and Julia Hockenmaier.
2013.
An HDPModel for Inducing Combinatory Categorial Gram-mars.
Transactions of the Association for Computa-tional Linguistics, 1:75?88.Phil Blunsom and Trevor Cohn.
2010.
UnsupervisedInduction of Tree Substitution Grammars for Depen-dency Parsing.
In Proceedings of the 2010 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1204?1213, Cambridge, USA,October.Prachya Boonkwan and Mark Steedman.
2011.
Gram-mar Induction from Text Using Small Syntactic Pro-totypes.
In Proceedings of 5th International JointConference on Natural Language Processing, pages438?446, Chiang Mai, Thailand, November.Glenn Carroll and M Rooth.
1998.
Valence inductionwith a head-lexicalized PCFG.
In Proceedings ofthe 3rd Conference on Empirical Methods in NaturalLanguage Processing, page 36?45, Granada, Spain.Stephen Clark, Julia Hockenmaier, and Mark Steed-man.
2002.
Building Deep Dependency Structuresusing a Wide-Coverage CCG Parser.
In Proceedingsof 40th Annual Meeting of the Association for Com-putational Linguistics, pages 327?334, Philadelphia,USA, July.Timothy A D Fowler and Gerald Penn.
2010.
Accu-rate Context-Free Parsing with Combinatory Cate-gorial Grammar.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 335?344, Uppsala, Sweden, July.Dan Garrette, Chris Dyer, Jason Baldridge, and Noah ASmith.
2015.
Weakly-Supervised Grammar-Informed Bayesian CCG Parser Learning.
In Pro-ceedings of the Twenty-Ninth AAAI Conference onArtificial Intelligence (AAAI-15), Austin, USA.William P Headden III, Mark Johnson, and David Mc-Closky.
2009.
Improving Unsupervised Depen-dency Parsing with Richer Contexts and Smoothing.In Proceedings of Human Language Technologies:The 2009 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 101?109, Boulder, USA, June.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: A Corpus of CCG Derivations and Depen-dency Structures Extracted from the Penn Treebank.Computational Linguistics, 33:355?396, September.Dan Klein and Christopher D Manning.
2004.
Corpus-Based Induction of Syntactic Structure: Models ofDependency and Constituency.
In Proceedings ofthe 42nd Meeting of the Association for Compu-tational Linguistics (ACL?04), Main Volume, pages478?485, Barcelona, Spain, July.Percy Liang, Slav Petrov, Michael I Jordan, and DanKlein.
2007.
The Infinite PCFG Using Hierarchi-cal Dirichlet Processes.
In Proceedings of the 2007Joint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 688?697, Prague, Czech Republic, June.David Mare?cek and Milan Straka.
2013.
Stop-probability estimates computed on a large corpusimprove Unsupervised Dependency Parsing.
In Pro-ceedings of the 51st Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 281?290, Sofia, Bulgaria, August.Tahira Naseem, Harr Chen, Regina Barzilay, and MarkJohnson.
2010.
Using Universal Linguistic Knowl-edge to Guide Grammar Induction.
In Proceed-ings of the 2010 Conference on Empirical Methodsin Natural Language Processing, pages 1234?1244,Cambridge, USA, October.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2011.
Punctuation: Making a Point in Un-supervised Dependency Parsing.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning, pages 19?28, Portland, USA,June.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Ju-rafsky.
2013.
Breaking Out of Local Optima withCount Transforms and Model Recombination: AStudy in Grammar Induction.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1983?1995, Seattle,USA, October.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press.Yee-Whye Teh, Michael I Jordan, Matthew J Beal, andDavid M Blei.
2006.
Hierarchical Dirichlet Pro-cesses.
Journal of the American Statistical Associa-tion, 101(476):1566?1581.Hiroyasu Yamada and Yuji Matsumoto.
2003.
Statisti-cal Dependency Analysis With Support Vector Ma-chines.
In Proceedings of 8th International Work-shop on Parsing Technologies (IWPT), pages 195?206, Nancy, France.1404
