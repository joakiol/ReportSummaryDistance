Parsing with Principles and ProbabilitiesAndrew FordhamSCS Research GroupDepartment of SociologyUniversity of SurreyGuildfordSurrey, GU2 5XH, UKaj fesoc, surrey, ac.
ukAbst rac tThis paper is an attempt to bring together two ap-proaches to language analysis.
The possible use of prob-abilistic information in principle-based grammars andparsers is considered, including discussion on some the-oretical and computational problems that arise.
Finallya partial implementation of these ideas is presented,along with some preliminary results from testing on asmall set of sentences.I n t roduct ionBoth principle-based parsing and probabilistic methodsfor the analysis of natural language have become pop-ular in the last decade.
While the former borrows fromadvanced linguistic specifications of syntax, the latterhas been more concerned with extracting distributionalregularities from language to aid the implementation fNLP systems and the analysis of corpora.These symbolic and statistical pproaches axe begin-ning to draw together as it becomes clear that one can-not exist entirely without he other: the knowledge oflanguage posited over the years by theoretical linguistshas been useful in constraining and guiding statisticalapproaches, and the corpora now available to linguistshave resurrected the desire to account for real languagedata in a more principled way than had previously beenattempted.This paper falls directly between these approaches,using statistical information derived from corpora nal-ysis to weight syntactic analyses produced by a 'prin-ciples and parameters' parser.
The use of probabilisticinformation in principle-based grammars and parsersis considered, including discussion on some theoreticaland computational problems that arise.
Finally a pax-tial implementation f these ideas is presented, alongwith some preliminary results from testing on a smallset of sentences.Government .
-B ind ing  TheoryThe principles and paxameters paradigm inlinguistics ismost fully realised in the Government-Binding Theory(GB) of Chomsky \[Chomsky1981, Chomsky19861 andothers.
The grammar is divided into modules whichMat thew CrockerCentre for Cognitive ScienceUniversity of Edinburgh2 Buccleuch PlaceEdinburgh, EH8 9LWScotlandmwc@cogsc?, ed.
ac.
ukfilter out ungrammatical structures at the various levelsof representation; these levels are related by generaltransformations.
A sketch of the organisation of GB(the 'T-model') is shown in figure I.D-Structure ( X'-theory, lexicsI insertion,O-criterion)S-Struaure (Case Theory, Subjacency)Pf// k-movement ( ove.a)Phonetic Form Logical Form(Empty Category Principle,Bindino Theoq)Figure 1: The T-model of grammarLittle work has been done on the complexity of al-gorithms used to parse with a principle-based gram-mar, since such grammars do not exist as acceptedmathematically well-defined constructs.
It has beenestimated that in general, principle-based parsing canonly be accomplished in exponential time, i.e.
0(2")\[Berwick and Weinberg1984, Weinberg19881.A feature of principle-based grammars i their po-tential to assign some meaningful representation toa string which is strictly ungrammatical.
It is aninherent feature of phrase structure grammars thatthey classify the strings of words from a languageinto two (infinite) sets, one containing the grammat-ical strings and the other the ungrammatical strings.Although attempts have been made to modify PS gram-mars/parsers to cope with extragrammatical input,e.g.
\[Carbonell and Hayes1983, Douglas and Dale1992,Jensen et al1983, Mellish1989\], this is a feature whichhas to be 'added on' and tends to affect the statementof the grammar.Due to the lack of an accepted formalism for the37specification of prindple-based grammars, Crocker andLewi, \[Crocker and Lewin1992\] define the declarative'Proper Branch' formalism, which can be used with anumber of different parsing methods.A proper branch is a set of three nodes - -  a motherand two daughters - -  which are constructed by theparser, using a simple mechanism such as a shift-reduceinterpreter, and then 'licensed' by the principles ofgrammar.
A complete phrase marker of the input stringcan then be constructed by following the manner inwhich the mother node from one proper branch is usedas a daughter node in a dominating proper branch.Eadl proper branch is a binary branching struc-ture, and so all grammatical constraints will need tobe encoded locally.
Crocker \[Crocker19921 develops"a 'representational' reformulation of the transforma-tional model which decomposes syntactic analysis intosew,.ral representation types - -  including phrase struc-ture, chains, and coindexation - - allowing one to main-tain the strictly local characterisation of prindpleswith respect to their relevant representation types,"\[Crocker and Lewin1992, p. 511\].By using the proper branch method of axiomatis-ing the grammar, the structure building section of theparser is only constrained in that it must produceproper branches; it is therefore possible to experimentwith different interpreters (i.e.
structure proposing en-gines) while keeping the grammar constant.The  Grammar  and  ParserA small principle-based parser was built, follow-ing the proper branch formalism developed in\[Crocker and Lewin1992\].
Although the grammar isvery limited, the use of probabilities in ranking the\[mrscr's output can be seen as a first step towards im-plementing a principle-based parser using a more fullyspecified collection of grammar modules.The grammar is loosely based on three modulestaken from Government-Binding Theory - -  X-bar the-ory, Theta Theory and Case Theory.
Although theseembody the spirit of the constraints found in Chore-sky \[Chomsky1981\] they are not intended to be entirelyfaithful to this specification of syntactic theory.
Thereis also only a single level of representation (which isexplicitly constructed for output purposes but not con-sulted by the parser).
This representation is interpretedas S-structure.Explanations of the knowledge contained within eachgrammar principle is given in the following sections.TheoryX-bar Theory uses a set of schemata to license localsubtmes.
We use a parametrised version of the X-barschemata, similar to that of Muysken \[Muysken1983\],but employing features which relate to the state of thehead word's theta grid to give five schemata (figure 2) .A .ode includes the following features (among others):1.
X=2.
xS+3.
X_ s4.
X s _5.
X s _Figure~Y2 X + _~X~ Y+~XS+ y_----,X s Y=~y_- X s -2: The X-bar Schemata1.
Category: the standard category names are em-ployed.2.
Specifier (SPEC): this feature spedfies whether theword at the head of the phrase being built requires aspech%r.3.
Complement (COMP): the complement feature is re-dundant in that the information used to derive it'svalue is already present in a word's there grid, andwill therefore be checked for well-formedness by thetheta criterion.
Since this information is not refer-enced until later, the COMP feature is used to l imitthe number of superfluous proper-branches generatedby the parser.4.
The head (i.e.
lexical item) of a node is carried oneach projection of that node along with its theta grid.The probabilities for occurrences ofthe X-bar schemawere obtained from sentences from the preliminaryPenn Tmebank corpus of the Wall Street Journal, cho-sen because of their length and the head of their verbphrase (i.e.
the main verbs were all from the set forwhich theta role data was obtained); the examples weremanually parsed by the authors.The probabilities were calculated using the followingequation, where X~: --~ Y~## Z~s~ is a specific schema,X is the set of X-bar schemata nd A and B and Care variables over category, SPEC and COMP featurebundles:c, z&) z P(X~?
zS'Ixcb, ) = C(A  ~ B C) (I)This is different o manner in which probabilities arecollected for stochastic ontext-free grammars, wherethe identity of the mother node is taken into account,as in the equation below:c(x : r s' - c ,Z&)  P(xcS: ~ YcI' ZcS: Ix) = C(X~: --~ B (2) C)This would result in misleading probabilities for the X-bar schemata since the use of schemata (3), (4), and(5) would immediately bring down the probability ofa parse compared to a parse of the same string whichhappened to use only (1) and (2).
**The probabilities for (1) and (2) would be I as they haveunique mothers.38The overall (X-bar) likelihood of a parse can then becomputed by multiplying together all the probabilitiesobtaim:d from each application of the schemata, in amanner analogous to that used to obtain the probabil-ity of a phrase marker generated by an SCFG.
Usingthe schemata in this way suggests that the building ofstructure is category independent, i.e.
it is just as likelythat a verb will have a (filled) specifier position as it isfor a noun.
The work on stochastic ontext-free gram-mars suggests a different set of results, in that the spe-cific categories involved in expansions are all important.While SCFGs will tend to deny that all categories ex-pand in certain ways with the same probabilities, theymake this claim while using a homogeneous grammarformalism.
When a more modular theory is employed,the source of the supposedly category specific informa-tion is not as obvious.
The use of lexical probabilitieson specifier and complement co-occurrence with specificheads (i.e.
lexical items) could exihibit properties thatappear to be category specific, but are in fact causedby common properties which are shared by lexical itemsof the same category.
2 Since it can be argued that theprobabilistic information on lexical items will be neededindependently, there is no need to use category specificinformation in assigning probabilities to syntactic on-figurations.Theta  TheoryTheta theory is concerned with the assignment of anargument structure to a sentence.
A verb has a numberof the thematic (or 'theta') roles which must be assignedto its arguments, e.g.
a transitive verb has one thetarole to 'discharge' which must be assigned to an NP.If a binary branching formalism is employed, or in-deed any formalism where the arguments of an itemand the item itself are not necessarily all sisters, theproblem of when to access the probability of a thetaapplication is presented.
The easiest method of obtain-ing and applying theta probabilities will be with refer-ence to whole theta grids.
Each theta grid for a wordwill be assigned a probability which is not dependenton any particular items in the grid, but rather on theoccurrence of the theta grid as a whole.A preliminary version of the Penn Treebenk brack-eted corpus was analysed to extract information on thesisters of particular verbs.
Although the Penn Tree-bank data is unreliable since it does not always dis-tinguish complements from adjuncts, it was the onlysuitable parsed corpus to which the authors had access.Although the distinction between complements and ad-juncts is a theoretically interesting one, the process ofdetermining which constructions fill which functionalroles in the analysis of real text often creates a numberof problems (see \[Hindle and Rooth1993\] for discussion2It is of course possible to store these cross-item similar-ities as lexical rules \[Bresnan1978\], but this alone does notentail that the properties axe specific to a category, cff.
thetheta grids of verbs and their ~related' nouns.on this issue regarding output of the Fidditch parser\[Hindle1993\]).The probal)ilities for em'h of tim verbs' thcta t;l'hlswere calculated using the equati~ m bch Jw, w her(, I '(s, It,)is the probability of the theta grid st occurring with thcverb v, (v, si) is an occurrence of the items in si beinglicensed by v, and S ranges over all theta gr!ds for v:C(v,s,)PCsdv) = CCv,S) (3)Case  TheoryIn its simplest form, Case theory invokes the Case filterto ensure that all noun phrases in a parse are assigned(abstract) case.
Case theory differs from both X-barand Theta theory in that it is category specific: onlyNPs require, or indeed can be assigned, abstract case.If we are to implement a probabilistic version of a mod-ular grammar theory incorporating a Case component,a relevant question is: are there multiple ways of as-signing Case to noun phrases in a sentence?
i.e.
canambiguity arise due to the presence of two candidateCase assigners?Case theory suggests that the answer to this is neg-ative, since Case assignment is linked to theta theoryvia visibility, and it is not possible for an NP  to receivemore than one theta role.
As a result, the use of Caseprobabilities in a parser would be at best unimportant,since some form of ambiguity is needed in the module,i.e.
it is possible to satisfy the Case filter in more thanone way, for probabilities associated with the moduleto be of any use.
While having a provision for usingprobabilities deduced from Case information, the im-plemented parser does not in fact use Case in its parseranking operations.Local CalculationThe use of a heterogeneous grammar formalism andmultiple probabilities invokes the problem of their com-bination.
There are at least two ways in which eachmother's probabilities can be calculated; firstly, theprobability information of the same type can be used:the daughters' X-bar probabilities alone could be usedin calculating the mother's X-bar probability.
Alterna-tively, a combination of some or all of the daughters'probability features could be employed, thus making,e.g., the X-bar probability of the mother dependentupon all the stochastic information from the daughters,including theta and Case probabilities, etc.The need for a method of combining the daughterprobabilities into a useful figure for the calculation ofthe mother probabilities is likely to involve trial and er-ror, since theory thus far has had nothing to say on thesubject.
The former method, using only the relevantdaughter probabilities, therefore seems to be the mostfruitful path to follow at the outset, since it does notrequire a way of integrating probabilities from differ-ent modules while the parse is in progress, nor is it ~mcomputationally expensive.39Global Calcu la t ionThe manner in which the global probability is calct!-latcd will be partly dependent upon the information~'ontained in the local probability calculations.If the probabilities for partial analyses have been cal-culated using only probabilities of the same types fromthe subanalyses - - e.g.
X-bar, Theta - -  the probabil-ities at the top level will have been calculated usinginformationally distinct figures.
This has the advan-tage of making 'pure' probabilities available, in thatthe X-bar probability will reflect the likelihood of thestructure alone, and will be 'uncontaminated' by anyother information.
It should then be possible to exper-iment with different methods of combining these prob-abilities, other than the obvious 'multiplying them to-gether' techniques, which could result in one type ofpr~babililty emerging as the most important.On the other hand, if probabilities calculated ur-ing the parse take all the different types of probabilitiesinto account at each calculation - -  i.e.
the X-bar, theta,(~tc.
probabilities on daughters are all taken into accountwhen calculating the mother's X-bar probability - -  theprobabilities at the top level will not be pure, and a lotof the information contained in them will be redundantsince they will share a large subset of the probabilitiesused in their separate calculations.
It will not thereforebe c~asy to gain theoretical insight using these statis-tics, and their most profitable method of combinationis likely tt~ be more haphazard affair than when morepure probabilities are used.The parser used in testing employed the first methodand therefore produced separate module probabilitiesfor each node.
For the lack of a.better, theoretically mo-tivated method for combining these figures, the productof the probabilities was taken as the global probabilityfor each parse.Tes t ing  the  ParserThe parser was tested using sixteen sentences contain-ing verbs for which data had been collected from thePenn Treebank corpus.
The sentences were created bythe authors to exhibit at least a degree of ambiguitywhen it came to attaching a post-verbal phrase as anadjunct or a complement.
In order to force the choice ofthe 'l)est' parse on to the verb, the probabilities of thetagrids for nouns, prepositions, etc.
was kept constant.Of these 16 highest ranked parses, 7 are the expectedparse, with the other 9 exhibiting some form of mis-attachment.
The fact that each string received multi-pie parses (the mean number of analyses being 9.135,~md the median, 6) suggests that the probabilistic in-formation did favourably guide the selection of a singleamdysis.It is not really possible to say from these results howsuccessful the whole approach of probabilistic principle-based parsing would be if it were fully implemented.The inconclusive nature of the results obtained was dueto a number of limiting factors of the implementationincluding the simplicity of the grammar and the lack ofavailable data.D iscuss ionL imi ta t ions  o f  the  GrammarThe grammar employed is a partial characterisation fChomsky's Government-Binding theory \[Chomsky1981,Chomsky1986\] and only takes account of very local con-stralnts (i.e.
X-bar, Theta and Case); a way of encod-ing all constraints in the proper branch formalism (e.g.\[Crocker1992\]) will be needed before a grammar of suf-ficient coverage to be useful in corpora nalysis can beformulated.
The problem with using results obtainedfrom the implementation given here is that the gram-mar is sufficiently underspecified and so leaves too greata task for the probabilistic information.This approach could be viewed as putting the cart be-fore the horse; the usefulness of stochastic informationin parsers presumes that a certain level of accuracy canbe achieved by the grammar alone.
While GB is an el-egant heory of cognitive syntax, it has yet to be shownthat such a modular characteristion can be successfullyemployed in corpus analysis.S ta t i s t i ca l  Data  and  the i r  SourceThe use of the preliminary Penn Treebank corpus forthe extraction of probabilities used in the implementa-tion above was a choice forced by lack of suitable mate-rials.
There are still very few parsed corpora vailable,and none that contain information which is specified tothe level required by, e.g., a GB grammar.
While thisis not an absolute limitation, in that it is theoreticallypossible to extract his information manually or semi-automatically from a corpus, time constraints entailedthe rejection of this approach.It would be ultimately desirable if the use of probabil-ities in principle-based parsing could be used to mirrorthe way that a syntactic theory such as Government-Binding handles constructions - - various modules ofthe grammar conspire to rule out illegal structures orderivations.
It would be an elegant result if a construc-tion such as the passive were to use probabilities forchains, Case assignment etc.
to select a parse that re-flected the lexical changes that had been undergone, .g.the greater likelihood of an NP featuring in the verb!stheta grid.
It is this property of a number of modulesworking hand in hand that needs to be carried over intothe probabilistic domain.The objections that linguists once held against sta-tistical methods are disappearing slowly, partly due toresults in corpora nalysis that show the inadequacy oflinguistic theory when applied to naturally occurringdata.
It is also the case that the rise of the connection-ist phoenix has brought he idea of weighted (thoughnot strictly probabilistic) functions of cognition back tothe fore, freeing the hands of linguists who believe thatwhile an explanatorily adequate theory of grammar is40an elegant construct, its human implementation, and itsusage in computational linguists may not be straightforward.
This paper has hopefully shown that an in-tegration of statistical methods and current linguistictheory is a goal worth pursuing.Re ferences\[Berwick and Weinberg1984\] Robert Cregar Berwickand Amy S. Weinberg.
1984.
The Grammatical Basisof Linguistic Performance: Language Use and Ac-quistion.
MIT Press.\[Bresnan1978\] Joan.
W. Bresnan.
1978.
A realistictransformational grammar.
In M. Halle, J. Bresnan,and G. Miller, editors, Linguistic Theory and Psy-chological Reality.
MIT Press, Cambridge, MA.\[CarboneU and Hayes1983\] Jaime G. Carbonell andPhilip J. Hayes.
1983.
Recovery strategies for pars-ing extragrammatical l nguage.
American Journalof Computational Linguistics, 9(3-4):123-146, July-December.\[Chomsky1981\] Noam Chomsky.
1981.
Lectures onGovernment and Binding.
Studies in GenerativeGrammar No.
9.
Foris, Dordrecht.\[Chomsky1986\] Noam Chomsky.
1986.
Knowledge ofLanguage: Its Nature, Origin, and Use.
Convergence.Praeger, New York.\[Crocker and Lewin1992\] Matthew Walter Crocker andIan Lewin.
1992.
Parsing as deduction: Rules versusprinciples.
In B. Neumann, editor, ECAI 92. lOthEuropean Conference on Artificial Intelligence, pages508-512.
John Wiley and Sons, Ltd.\[Crocker1992\] Matthew Walter Crocker.
1992.
A Log-ical Model of Competence and Performance in theHuman Sentence Processor.
Ph.D. thesis, Dept.
Ar-tificial Intelligence, University of Edinburgh.\[Douglas and Dale1992\] Shona Douglas and RobertDale.
1992.
Towards robust PATR.
In Ch.
Boitet, ed-itor, COLING-9~, Proceedings of the fifteenth Inter-national Conference on Computational Linguistics,pages 468-474.\[Hindle and Rooth1993\] Donald Hindleand Mats Rooth.
1993.
Structural ambuiguity andlexical relations.
Computational Linguistics, 19(1).\[Hindle1993\] Donald Hindle.
1993.
A parser for textcorpora.
In 13.
T. S. Atkins and A. Zampolli, editors,Computational Approaches to the Lexicon.\[Jensen et al1983\] K. Jensen, G. E. Heidborn, L. A.Miller, and Y. R~vin.
1983.
Parse fitting and prosefixing: Getting a hold on ill-formedness.
AmericanJournal of Computational Linguistics, 9(3-4):147-160, July-December.\[Mellish1989\] Christopher S. Mellish.
1989.
Somechart-based techniques for parsing ill-formed input.In Proceedings of the $Tth Annual Meeting of the As-sociation for Computational Linguistics, pages 102-109.\[Muysken1983\] Pieter Muysken.
1983.
Paramctcrizingthe notion of head.
Journal of Linguistic Research,2:57-76.\[Weinberg1988\] Amy S. Weinberg.
1988.
Mathematicalproperties of grammars.
In Frederick J. Newtncycr,editor, Linguistics: the Cambridge Survey, Voi.
1,Linguistics Theory: Foundations, chapter 15, pages415-429.
Cambridge University Press.41
