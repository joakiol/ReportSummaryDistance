Back to Basics for Monolingual Alignment: Exploiting Word Similarity andContextual EvidenceMd Arafat Sultan?, Steven Bethard?
and Tamara Sumner?
?Institute of Cognitive Science and Department of Computer ScienceUniversity of Colorado Boulder?Department of Computer and Information SciencesUniversity of Alabama at Birminghamarafat.sultan@colorado.edu, bethard@cis.uab.edu, sumner@colorado.eduAbstractWe present a simple, easy-to-replicate monolin-gual aligner that demonstrates state-of-the-artperformance while relying on almost no su-pervision and a very small number of externalresources.
Based on the hypothesis that wordswith similar meanings represent potential pairsfor alignment if located in similar contexts, wepropose a system that operates by finding suchpairs.
In two intrinsic evaluations on alignmenttest data, our system achieves F1 scores of 88?92%, demonstrating 1?3% absolute improve-ment over the previous best system.
Moreover,in two extrinsic evaluations our aligner out-performs existing aligners, and even a naiveapplication of the aligner approaches state-of-the-art performance in each extrinsic task.1 IntroductionMonolingual alignment is the task of discovering andaligning similar semantic units in a pair of sentencesexpressed in a natural language.
Such alignments pro-vide valuable information regarding how and to whatextent the two sentences are related.
Consequently,alignment is a central component of a number ofimportant tasks involving text comparison: textualentailment recognition, textual similarity identifica-tion, paraphrase detection, question answering andtext summarization, to name a few.The high utility of monolingual alignment hasspawned significant research on the topic in the re-cent past.
Major efforts that have treated alignmentas a standalone problem (MacCartney et al., 2008;Thadani and McKeown, 2011; Yao et al., 2013a) areprimarily supervised, thanks to the manually alignedcorpus with training and test sets from Microsoft Re-search (Brockett, 2007).
Primary concerns of suchwork include both quality and speed, due to the factthat alignment is frequently a component of largerNLP tasks.Driven by similar motivations, we seek to devise alightweight, easy-to-construct aligner that produceshigh-quality output and is applicable to various endtasks.
Amid a variety of problem formulations andingenious approaches to alignment, we take a stepback and examine closely the effectiveness of twofrequently made assumptions: 1) Related semanticunits in two sentences must be similar or relatedin their meaning, and 2) Commonalities in their se-mantic contexts in the respective sentences provideadditional evidence of their relatedness (MacCartneyet al., 2008; Thadani and McKeown, 2011; Yao et al.,2013a; Yao et al., 2013b).
Alignment, based solelyon these two assumptions, reduces to finding the bestcombination of pairs of similar semantic units in sim-ilar contexts.Exploiting existing resources to identify similarityof semantic units, we search for robust techniquesto identify contextual commonalities.
Dependencytrees are a commonly used structure for this purpose.While they remain a central part of our aligner, weexpand the horizons of dependency-based alignmentbeyond exact matching by systematically exploitingthe notion of ?type equivalence?
with a small hand-crafted set of equivalent dependency types.
In addi-tion, we augment dependency-based alignment withsurface-level text analysis.While phrasal alignments are important and have219Transactions of the Association for Computational Linguistics, 2 (2014) 219?230.
Action Editor: Alexander Koller.Submitted 11/2013; Revised 1/2014; Published 5/2014.
c?2014 Association for Computational Linguistics.been investigated in multiple studies, we focus pri-marily on word alignments (which have been shownto form the vast majority of alignments (?
95%)in multiple human-annotated corpora (Yao et al.,2013b)), keeping the framework flexible enough toallow incorporation of phrasal alignments in future.Evaluation of our aligner on the benchmark datasetreported in (Brockett, 2007) shows an F1 score of91.7%: a 3.1% absolute improvement over the previ-ous best system (Yao et al., 2013a), correspondingto a 27.2% error reduction.
It shows superior perfor-mance also on the dataset reported in (Thadani etal., 2012).
Additionally, we present results of twoextrinsic evaluations, namely textual similarity iden-tification and paraphrase detection.
Our aligner notonly outperforms existing aligners in each task, butalso approaches top systems for the extrinsic tasks.2 BackgroundMonolingual alignment has been applied to variousNLP tasks including textual entailment recognition(Hickl et al., 2006; Hickl and Bensley, 2007), para-phrase identification (Das and Smith, 2009; Madnaniet al., 2012), and textual similarity assessment (Ba?ret al., 2012; Han et al., 2013) ?
in some cases ex-plicitly, i.e., as a separate module.
But many suchsystems resort to simplistic and/or ad-hoc strategiesfor alignment and in most such work, the alignmentmodules were not separately evaluated on alignmentbenchmarks, making their direct assessment difficult.With the introduction of the MSR alignment cor-pus (Brockett, 2007) developed from the secondRecognizing Textual Entailment challenge data (Bar-Haim et al., 2006), direct evaluation and comparisonof aligners became possible.
The first aligner trainedand evaluated on the corpus was a phrasal alignercalled MANLI (MacCartney et al., 2008).
It repre-sents alignments as sets of different edit operations(where a sequence of edits turns one input sentenceinto the other) and finds an optimal set of edits viaa simulated annealing search.
Weights of differentedit features are learned from the training set of theMSR alignment corpus using a perceptron learningalgorithm.
MANLI incorporates only shallow fea-tures characterizing contextual similarity: relativepositions of the two phrases being aligned (or not) inthe two sentences and boolean features representingwhether or not the preceding and following tokens ofthe two phrases are similar.Thadani and McKeown (2011) substitutedMANLI?s simulated annealing-based decoding withinteger linear programming, and achieved a consider-able speed-up.
More importantly for our discussion,they found contextual evidence in the form of syn-tactic constraints useful in better aligning stop words.Thadani et al.
(2012) further extended the model byadding features characterizing dependency arc edits,effectively bringing stronger influence of contextualsimilarity into alignment decisions.
Again the perfor-mance improved consequently.The most successful aligner to date both in termsof accuracy and speed, called JacanaAlign, was de-veloped by Yao et al.
(2013a).
In contrast to theearlier systems, JacanaAlign is a word aligner thatformulates alignment as a sequence labeling prob-lem.
Each word in the source sentence is labeledwith the corresponding target word index if an align-ment is found.
It employs a conditional random fieldto assign the labels and uses a feature set similar toMANLI?s in terms of the information they encode(with some extensions).
Contextual features includeonly semantic match of the left and the right neigh-bors of the two words and their POS tags.
Eventhough JacanaAlign outperformed the MANLI en-hancements despite having less contextual features,it is difficult to compare the role of context in thetwo models because of the large paradigmatic dispar-ity.
An extension of JacanaAlign was proposed forphrasal alignments in (Yao et al., 2013b), but thecontextual features remained largely unchanged.Noticeable in all the above systems is the use ofcontextual evidence as a feature for alignment, butin our opinion, not to an extent sufficient to harnessits full potential.
Even though deeper dependency-based modeling of contextual commonalities can befound in some other studies (Kouylekov and Magnini,2005; Chambers et al., 2007; Chang et al., 2010; Yaoet al., 2013c), we believe there is further scope forsystematic exploitation of contextual evidence foralignment, which we aim to do in this work.On the contrary, word semantic similarity has beena central component of most aligners; various mea-sures of word similarity have been utilized, includingstring similarity, resource-based similarity (derivedfrom one or more lexical resources like WordNet)220AlignidenticalwordsequencesAlignnamedentitiesAligncontentwordsAlignstopwordsFigure 1: System overviewand distributional similarity (computed from wordco-occurrence statistics in large corpora).
An impor-tant trade-off between precision, coverage and speedexists here and aligners commonly rely on only asubset of these measures (Thadani and McKeown,2011; Yao et al., 2013a).
We use the ParaphraseDatabase (PPDB) (Ganitkevitch et al., 2013), whichis a large resource of lexical and phrasal paraphrasesconstructed using bilingual pivoting (Bannard andCallison-Burch, 2005) over large parallel corpora.3 SystemOur system operates as a pipeline of alignment mod-ules that differ in the types of word pairs they align.Figure 1 is a simplistic representation of the pipeline.Each module makes use of contextual evidence tomake alignment decisions.
In addition, the last twomodules are informed by a word semantic similarityalgorithm.
Because of their phrasal nature, we treatnamed entities separately from other content words.The rationale behind the order in which the modulesare arranged is discussed later in this section (3.3.5).Before discussing each alignment module in de-tail, we describe the system components that identifyword and contextual similarity.3.1 Word SimilarityThe ability to correctly identify semantic similaritybetween words is crucial to our aligner, since con-textual evidence is important only for similar words.Instead of treating word similarity as a continuousvariable, we define three levels of similarity.The first level is an exact word or lemma matchwhich is represented by a similarity score of 1.
Thesecond level represents semantic similarity betweentwo terms which are not identical.
To identify suchword pairs, we employ the Paraphrase Database(PPDB)1.
We use the largest (XXXL) of the PPDB?slexical paraphrase packages and treat all pairs iden-tically by ignoring the accompanying statistics.
We1http://paraphrase.orgcustomize the resource by removing pairs of identi-cal words or lemmas and adding lemmatized formsof the remaining pairs.
For now, we use the termppdbSim to refer to the similarity of each word pairin this modified version of PPDB (which is a value in(0, 1)) and later explain how we determine it (Section3.3.5).
Finally, any pair of different words which isabsent in PPDB is assigned a zero similarity score.3.2 Extracting Contextual EvidenceOur alignment modules collect contextual evidencefrom two complementary sources: syntactic depen-dencies and words occurring within a small textualvicinity of the two words to be aligned.
The applica-tion of each kind assumes a common principle of min-imal evidence.
Formally, given two input sentencesS and T , we consider two words s ?
S and t ?
T toform a candidate pair for alignment if ?rs ?
S and?rt ?
T such that:1.
(s, t) ?
<Sim and (rs, rt) ?
<Sim, where<Sim is a binary relation indicating sufficientsemantic relatedness between the members ofeach pair (?
ppdbSim in our case).2.
(s, rs) ?
<C1 and (t, rt) ?
<C2 , such that<C1 ?
<C2 ; where <C1 and <C2 are binary re-lations representing specific types of contextualrelationships between two words in a sentence(e.g., an nsubj dependency between a verb anda noun).
The symbol ?
represents equivalencebetween two relationships, including identical-ity.Note that the minimal-evidence assumption holdsa single piece of contextual evidence as sufficientsupport for a potential alignment; but as we discusslater in this section, an evidence for word pair (s, t)(where s ?
S and t ?
S) may not lead to an align-ment if there exists a competing pair (s?, t) or (s, t?
)with more evidence (where s?
?
S and t?
?
T ).In the rest of this section, we elaborate the differentforms of contextual relationships we exploit alongwith the notion of equivalence between relationships.3.2.1 Syntactic DependenciesDependencies can be important sources of con-textual evidence.
Two nsubj children rs and rt oftwo verbs s ?
S and t ?
T , for example, pro-vide evidence for not only an (s, t) alignment, but221S: He wrote a book .nsubjdobjdetT : I read the book he wrote .nsubjdobjdetrcmodnsubjFigure 2: Equivalent dependency types: dobj and rcmodalso an (rs, rt) alignment if (s, t) ?
<Sim and(rs, rt) ?
<Sim.
(We adopt the Stanford typed de-pendencies (de Marneffe and Manning, 2008).
)Moreover, dependency types can exhibit equiva-lence; consider the two sentences in Figure 2.
Thedobj dependency in S is equivalent to the rcmoddependency in T (dobj ?
rcmod, following our ear-lier notation) since they represent the same semanticrelation between identical word pairs in the two sen-tences.
To be able to use such evidence for alignment,we need to go beyond exact matching of dependen-cies and develop a mapping among dependency typesthat encodes such equivalence.
Note also that theparent-child roles are opposite for the two depen-dency types in the above example, a scenario thatsuch a mapping must accommodate.The four possible such scenarios regarding parent-child orientations are shown in Figure 3.
If (s, t) ?<Sim and (rs, rt) ?
<Sim (represented by bidirec-tional arrows), then each orientation represents a setof possible ways in which the S and T dependen-cies (unidirectional arrows) can provide evidence ofsimilarity between the contexts of s in S and t in T .Each such set comprises equivalent dependency typepairs for that orientation.
In the example of Figure 2,(dobj, rcmod) is such a pair for orientation (c), givens = t = ?wrote?
and rs = rt = ?book?.We apply the notion of dependency type equiva-lence to intra-category alignment of content wordsin four major lexical categories: verbs, nouns,adjectives and adverbs (the Stanford POS tag-ger (Toutanova et al., 2003) is used to identify thecategories).
Table 1 shows dependency type equiva-lences for each lexical category of s and t.The ???
sign on column 5 of some rows repre-sents a duplication of the column 4 content of thesrstrtrssrttsrstrtsrstrt(a) (b) (c) (d)Figure 3: Parent-child orientations in dependenciessame row.
For each row, columns 4 and 5 show twosets of dependency types; each member of the firstis equivalent to each member of the second for thecurrent orientation (column 1) and lexical categoriesof the associated words (columns 2 and 3).
For exam-ple, row 2 represents the fact that an agent relation(between s and rs; s is the parent) is equivalent to annsubj relation (between t and rt; t is the parent).Note that the equivalences are fundamentally re-dundant across different orientations.
For example,row 2 (which is presented as an instance of ori-entation (a)) can also be presented as an instanceof orientation (b) with POS(s)=POS(t)=noun andPOS(rs)=POS(rt)=verb.
We avoid such redundancyfor compactness.
For the same reason, the equiva-lence of dobj and rcmod in Figure 2 is shown in thetable only as an instance of orientation (c) and not asan instance of orientation (d) (in general, this is whyorientations (b) and (d) are absent in the table).We present dependency-based contextual evidenceextraction in Algorithm 1.
(The Stanford dependencyparser (de Marneffe et al., 2006) is used to extract thedependencies.)
Given a word pair (si, tj) from the in-put sentences S and T , it collects contextual evidence(as indexes of rsi and rtj with a positive similarity)for each matching row in Table 1.
An exact matchof the two dependencies is also considered a pieceof evidence.
Note that Table 1 only considers con-tent word pairs (si, tj) such that POS(si)=POS(tj),but as 90% of all content word alignments in theMSR alignment dev set are within the same lexicalcategory, this is a reasonable set to start with.3.2.2 Textual NeighborhoodWhile equivalent dependencies can provide strongcontextual evidence, they can not ensure high recallbecause, a) the ability to accurately extract depen-222Orientation POS(s, t) POS(rs, rt) S Dependency Types T Dependency Typessrstrtverbverb {purpcl, xcomp} ?
?noun{agent, nsubj, xsubj} ??
{dobj, nsubjpass, rel} ??
{tmod, prep in, prep at, prep on} ??
{iobj, prep to} ?
?nounverb {infmod, partmod, rcmod} ??
(a) noun {pos, nn, prep of, prep in, prep at, prep for} ?
?adjective {amod, rcmod} ?
?srstrt verb verb{conj and} ??
{conj or} ??
{conj nor} ?
?noun {dobj, nsubjpass, rel} {infmod, partmod, rcmod}adjective {acomp} {cop, csubj}noun noun{conj and} ??
{conj or} ??
{conj nor} ?
?adjective {amod, rcmod} {nsubj}adjective adjective{conj and} ??
{conj or} ??
(c) {conj nor} ?
?adverb adverb{conj and} ??
{conj or} ??
{conj nor} ?
?Table 1: Equivalent dependency structuresAlgorithm 1: depContext(S, T, i, j, EQ)Input:1.
S, T : Sentences to be aligned2.
i: Index of a word in S3.
j: Index of a word in T4.
EQ: Dependency type equivalences (Table 1)Output: context = {(k, l)}: pairs of word indexescontext?
{(k, l) : wordSim(sk, tl) > 01?
(i, k, ?s) ?
dependencies(S)2?
(j, l, ?t) ?
dependencies(T )3?
POS(si) = POS(tj) ?
POS(sk) = POS(tl)4?
(?s = ?t5?
(POS(si), POS(sk), ?s, ?t) ?
EQ))}6dencies is limited by the accuracy of the parser, andb) we investigate equivalence types for only inter-lexical-category alignment in this study.
Thereforewe apply an additional model of word context: thetextual neighborhood of s in S and t in T .Extraction of contextual evidence for contentwords from textual neighborhood is described in Al-gorithm 2.
Like the dependency-based module, itaccumulates evidence for each (si, tj) pair by in-specting multiple pairs of neighboring words.
But in-stead of aligning only words within a lexical category,Algorithm 2: textContext(S, T, i, j, STOP)Input:1.
S, T : Sentences to be aligned2.
i: Index of a word in S3.
j: Index of a word in T4.
STOP: A set of stop wordsOutput: context = {(k, l)}: pairs of word indexesCi ?
{k : k ?
[i?
3, i+ 3] ?
k 6= i ?
sk 6?
STOP}1Cj ?
{l : l ?
[j ?
3, j + 3] ?
l 6= j ?
tl 6?
STOP}2context?
Ci ?
Cj3this module also performs inter-category alignment,considering content words within a (3, 3) windowof si and tj as neighbors.
We implement relationalequivalence (?)
here by holding any two positionswithin the window equally contributive and mutuallycomparable as sources of contextual evidence.3.3 The Alignment AlgorithmWe now describe each alignment module in thepipeline and their sequence of operation.3.3.1 Identical Word SequencesThe presence of a common word sequence in Sand T is indicative of an (a) identical, and (b) con-223textually similar word in the other sentence for eachword in the sequence.
We observe the results ofaligning identical words in such sequences of lengthn containing at least one content word.
This simpleheuristic demonstrates a high precision (?
97%) onthe MSR alignment dev set for n ?
2, and thereforewe consider membership in such sequences as thesimplest form of contextual evidence in our systemand align all identical word sequence pairs in S andT containing at least one content word.
From thispoint on, we will refer to this module as wsAlign.3.3.2 Named EntitiesWe align named entities separately to enable thealignment of full and partial mentions (and acronyms)of the same entity.
We use the Stanford Named EntityRecognizer (Finkel et al., 2005) to identify namedentities in S and T .
After aligning the exact termmatches, any unmatched term of a partial mentionis aligned to all terms in the full mention.
The mod-ule recognizes only first-letter acronyms and alignsan acronym to all terms in the full mention of thecorresponding name.Since named entities are instances of nouns, namedentity alignment is also informed by contextual ev-idence (which we discuss in the next section), buthappens before alignment of other generic contentwords.
Parents (or children) of a named entity aresimply the parents (or children) of its head word.
Wewill refer to this module as a method named neAlignfrom this point on.3.3.3 Content WordsExtraction of contextual evidence for promisingcontent word pairs has already been discussed inSection 3.2, covering both dependency-based contextand textual context.Algorithm 3 (cwDepAlign) describes thedependency-based alignment process.
For eachpotentially alignable pair (si, tj), the dependency-based context is extracted as described in Algorithm1, and context similarity is calculated as the sumof the word similarities of the (sk, tl) context wordpairs (lines 2-7).
(The wordSim method returns asimilarity score in {0, ppdbSim, 1}.)
The alignmentscore of the (si, tj) pair is then a weighted sumof word and contextual similarity (lines 8-12).
(We discuss how the weights are set in SectionAlgorithm 3: cwDepAlign(S, T,EQ,AE , w, STOP)Input:1.
S, T : Sentences to be aligned2.
EQ: Dependency type equivalences (Table 1)3.
AE : Already aligned word pair indexes4.
w: Weight of word similarity relative to contex-tual similarity5.
STOP: A set of stop wordsOutput: A = {(i, j)}: word index pairs of alignedwords {(si, tj)} where si ?
S and tj ?
T??
?
; ??
?
?
; ??
?1for si ?
S, tj ?
T do2if si 6?
STOP ?
?
?tl : (i, l) ?
AE3?
tj 6?
STOP ?
?
?sk : (k, j) ?
AE4?
wordSim(si, tj) > 0 then5context?
depContext(S, T, i, j, EQ)6contextSim??
(k,l)?contextwordSim(sk, tl)7if contextSim > 0 then8??
?
?
{(i, j)}9??
(i, j)?
context10?
(i, j)?
w ?
wordSim(si, tj)11+(1?
w) ?
contextSim12Linearize and sort ?
in decreasing order of ?
(i, j)13A?
?14for (i, j) ?
?
do15if ?
?l : (i, l) ?
A16??
?k : (k, j) ?
A then17A?
A ?
{(i, j)}18for (k, l) ?
??
(i, j) do19if ?
?q : (k, q) ?
A ?AE20??
?p : (p, l) ?
A ?AE then21A?
A ?
{(k, l)}223.3.5.)
The module then aligns (si, tj) pairs withnon-zero evidence in decreasing order of this score(lines 13-18).
In addition, it aligns all the pairsthat contributed contextual evidence for the (si, tj)alignment (lines 19-22).
Note that we implement aone-to-one alignment whereby a word gets alignedat most once within the module.Algorithm 4 (cwTextAlign) presents alignmentbased on similarities in the textual neighborhood.
Foreach potentially alignable pair (si, tj), Algorithm 2is used to extract the context, which is a set of neigh-boring content word pairs (lines 2-7).
The contextualsimilarity is the sum of the similarities of these pairs224Algorithm 4: cwTextAlign(S, T,AE , w, STOP)Input:1.
S, T : Sentences to be aligned2.
AE : Existing alignments by word indexes3.
w: Weight of word similarity relative to contex-tual similarity4.
STOP: A set of stop wordsOutput: A = {(i, j)}: word index pairs of alignedwords {(si, tj)} where si ?
S and tj ?
T??
?
; ??
?1for si ?
S, tj ?
T do2if si 6?
STOP ?
?
?tl : (i, l) ?
AE3?
tj 6?
STOP ?
?
?sk : (k, j) ?
AE4?
wordSim(si, tj) > 0 then5??
?
?
{(i, j)}6context?
textContext(S, T, i, j, STOP)7contextSim??
(k,l)?contextwordSim(sk, tl)8?
(i, j)?
w ?
wordSim(si, tj)9+ (1?
w) ?
contextSim10Linearize and sort ?
in decreasing order of ?
(i, j)11A?
?12for (i, j) ?
?
do13if ?
?l : (i, l) ?
A14??
?k : (k, j) ?
A then15A?
A ?
{(i, j)}16(line 8), and the alignment score is a weighted sum ofword similarity and contextual similarity (lines 9, 10).The alignment score is then used to make one-to-oneword alignment decisions (lines 11-16).
Consideringtextual neighbors as weaker sources of evidence, wedo not align the neighbors.Note that in cwTextAlign we also align semanti-cally similar content word pairs (si, tj) with no con-textual similarities if no pairs (sk, tj) or (si, tl) existwith a higher alignment score.
This is a consequenceof our observation of the MSR alignment dev data,where we find that more often than not content wordsare inherently sufficiently meaningful to be alignedeven in the absence of contextual evidence whenthere are no competing pairs.The content word alignment module is thus itselfa pipeline of cwDepAlign and cwTextAlign.3.3.4 Stop WordsWe follow the contextual evidence-based approachto align stop words as well, some of which get alignedAlgorithm 5: align(S, T,EQ,w, STOP)Input:1.
S, T : Sentences to be aligned2.
EQ: Dependency type equivalences (Table 1)3. w: Weight of word similarity relative to contex-tual similarity4.
STOP: A set of stop wordsOutput: A = {(i, j)}: word index pairs of alignedwords {(si, tj)} where si ?
S and tj ?
TA?
wsAlign(S, T )1A?
A ?
neAlign(S, T,EQ,A,w)2A?
A ?
cwDepAlign(S, T,EQ,A,w, STOP)3A?
A ?
cwTextAlign(S, T,A,w, STOP)4A?
A ?
swDepAlign(S, T,A,w, STOP)5A?
A ?
swTextAlign(S, T,A,w, STOP)6as part of word sequence alignment as discussed inSection 3.3.1, and neighbor alignment as discussedin Section 3.3.3.
For the rest we utilize dependen-cies and textual neighborhoods as before, with threeadjustments.Firstly, since stop word alignment is the last mod-ule in our pipeline, rather than considering all se-mantically similar word pairs for contextual similar-ity, we consider only aligned pairs.
Secondly, sincemany stop words (e.g.
determiners, modals) typi-cally demonstrate little variation in the dependenciesthey engage in, we ignore type equivalences for stopwords and implement only exact matching of depen-dencies.
(Stop words in general can participate inequivalent dependencies, but we leave constructinga corresponding mapping for future work.)
Finally,for textual neighborhood, we separately check align-ments of the left and the right neighbors and aggre-gate the evidences to determine alignment ?
againdue to the primarily syntactic nature of interaction ofstop words with their neighbors.Thus stop words are also aligned in a sequence ofdependency and textual neighborhood-based align-ments.
We assume two corresponding modulesnamed swDepAlign and swTextAlign, respectively.3.3.5 The AlgorithmOur full alignment pipeline is shown as the methodalign in Algorithm 5.
Note that the strict order of thealignment modules limits the scope of downstreammodules since each such module discards any wordthat has already been aligned by an earlier module225(this is accomplished via the variable A; the corre-sponding parameter in Algorithms 3 and 4 is AE).The rationales behind the specific order of the mod-ules can now be explained: (1) wsAlign is a modulewith relatively higher precision, (2) it is convenient toalign named entities before other content words to en-able alignment of entity mentions of different lengths,(3) dependency-based evidence was observed to bemore reliable (i.e.
of higher precision) than textualevidence in the MSR alignment dev set, and (4) stopword alignments are dependent on existing contentword alignments.The aligner assumes two free parameters:ppdbSim and w (in Algorithms 3 and 4).
Todetermine their values, we exhaustively searchthrough the two-dimensional space (ppdbSim,w)for ppdbSim,w ?
{0.1, ..., 0.9, 1} and the combina-tion (0.9, 0.9) yields the best F1 score for the MSRalignment dev set.
We use these values for the alignerin all of its subsequent applications.4 EvaluationWe evaluate the performance of our aligner both in-trinsically and extrinsically on multiple corpora.4.1 Intrinsic EvaluationThe MSR alignment dataset2 (Brockett, 2007) wasdesigned to train and directly evaluate automatedaligners.
Three annotators individually aligned wordsand phrases in 1600 pairs of premise and hypothe-sis sentences from the RTE2 challenge data (dividedinto dev and test sets, each consisting of 800 sen-tences).
The dataset has subsequently been used toassess several top performing aligners (MacCartneyet al., 2008; Thadani and McKeown, 2011; Yao etal., 2013a; Yao et al., 2013b).
We use the test set forevaluation in the same manner as these studies: (a)we apply majority rule to select from the three setsof annotations for each sentence and discard three-way disagreements, (b) we evaluate only on the surelinks (word pairs that annotators mentioned shouldcertainly be aligned, as opposed to possible links).We test the generalizability of the aligner by eval-uating it, unchanged (i.e.
with identical parametervalues), on a second alignment corpus: the Edin-2http://www.cs.biu.ac.il/ nlp/files/RTE 2006 Aligned.zipSystem P% R% F1% E%MSRMacCartney et al.
(2008) 85.4 85.3 85.3 21.3Thadani & McKeown (2011) 89.5 86.2 87.8 33.0Yao et al.
(2013a) 93.7 84.0 88.6 35.3Yao et al.
(2013b) 92.1 82.8 86.8 29.1This Work 93.7 89.8 91.7 43.8EDB++ Yao et al.
(2013a) 91.3 82.0 86.4 15.0Yao et al.
(2013b) 90.4 81.9 85.9 13.7This Work 93.5 82.5 87.6 18.3Table 2: Results of intrinsic evaluation on two datasetsburgh++3 (Thadani et al., 2012) corpus.
The test setconsists of 306 pairs; each pair is aligned by at mosttwo annotators and we adopt the random selectionpolicy described in (Thadani et al., 2012) to resolvedisagreements.Table 2 shows the results.
For each corpus, itshows precision (% alignments that matched withgold annotations), recall (% gold alignments discov-ered by the aligner), F1 score and the percentageof sentences that received the exact gold alignments(denoted by E) from the aligner.On the MSR test set, our aligner shows a 3.1%improvement in F1 score over the previous best sys-tem (Yao et al., 2013a) with a 27.2% error reduction.Importantly, it demonstrates a considerable increasein recall without a loss of precision.
TheE score alsoincreases as a consequence.On the Edinburgh++ test set, our system achieves a1.2% increase in F1 score (an error reduction of 8.8%)over the previous best system (Yao et al., 2013a),with improvements in both precision and recall.
Thisis a remarkable result that demonstrates the generalapplicability of the aligner, as no parameter tuningtook place.4.1.1 Ablation TestWe perform a set of ablation tests to assess theimportance of the aligner?s individual components.Each row of Table 3 beginning with (-) shows a fea-ture excluded from the aligner and two associatedsets of metrics, showing the performance of the re-sulting algorithm on the two alignment corpora.Without a word similarity module, recall dropsas would be expected.
Without contextual evidence(word sequences, dependencies and textual neigh-bors) precision drops considerably and recall alsofalls.
Without dependencies, the aligner still gives3http://www.ling.ohio-state.edu/?scott/#edinburgh-plusplus226MSR EDB++Feature P% R% F1% P% R% F1%Original 93.7 89.8 91.7 93.5 82.5 87.6(-) Word Similarity 95.2 86.3 90.5 95.1 77.3 85.3(-) Contextual Evidence 81.3 86.0 83.6 86.4 80.6 83.4(-) Dependencies 94.2 88.8 91.4 93.8 81.3 87.1(-) Text Neighborhood 85.5 90.4 87.9 90.4 84.3 87.2(-) Stop Words 94.2 88.3 91.2 92.2 80.0 85.7Table 3: Ablation test resultsstate-of-the-art results, which points to the possibilityof a very fast yet high-performance aligner.
Withoutevidence from textual neighbors, however, the preci-sion of the aligner suffers badly.
Textual neighborsfind alignments across different lexical categories,a type of alignment that is currently not supportedby our dependency equivalences.
Extending the setof dependency equivalences might alleviate this is-sue.
Finally, without stop words (i.e.
while aligningcontent words only) recall drops just a little for eachcorpus, which is expected as content words form thevast majority of non-identical word alignments.4.2 Extrinsic EvaluationWe extrinsically evaluate our system on textual simi-larity identification and paraphrase detection.
Herewe discuss each task and the results of evaluation.4.2.1 Semantic Textual SimilarityGiven two short input text fragments (commonlysentences), the goal of this task is to identify thedegree to which the two fragments are semanticallysimilar.
The *SEM 2013 STS task (Agirre et al.,2013) assessed a number of STS systems on four testdatasets by comparing their output scores to humanannotations.
Pearson correlation coefficient with hu-man annotations was computed individually for eachtest set and a weighted sum of the correlations wasused as the final evaluation metric (the weight foreach dataset was proportional to its size).We apply our aligner to the task by aligning eachsentence pair and taking the proportion of contentwords aligned in the two sentences (by normalizingwith the harmonic mean of their number of contentwords) as a proxy of their semantic similarity.
Onlythree of the four STS 2013 datasets are freely avail-able4 (headlines, OnWN, and FNWN), which we usefor our experiments (leaving out the SMT dataset).4http://ixa2.si.ehu.es/sts/System Correl.% RankHan et al.
(2013) 73.7 1 (original)JacanaAlign 46.2 66This Work 67.2 7Table 4: Extrinsic evaluation on STS 2013 dataThese three sets contain 1500 annotated sentencepairs in total.Table 4 shows the results.
The first row shows theperformance of the top system in the task.
With adirect application of our aligner (no parameter tun-ing), our STS algorithm acheives a 67.15% weightedcorrelation, which would earn it the 7th rank among90 participating systems.
Considering the fact thatalignment is one of many components of STS, thisresult is truly promising.For comparison, we also evaluate the previous bestaligner named JacanaAlign (Yao et al., 2013a) onSTS 2013 data (the JacanaAlign public release5 isused, which is a version of the original aligner withextra lexical resources).
We apply three different val-ues derived from its output as proxies of semanticsimilarity: a) aligned content word proportion, b) theViterbi decoding score, and c) the normalized decod-ing score.
Of the three, (b) gives the best results,which we show in row 2 of Table 4.
Our aligneroutperforms JacanaAlign by a large margin.4.2.2 Paraphrase IdentificationThe goal of paraphrase identification is to decide iftwo sentences have the same meaning.
The output isa yes/no decision instead of a real-valued similarityscore as in STS.
We use the MSR paraphrase cor-pus6 (4076 dev pairs, 1725 test pairs) (Dolan et al.,2004) to evaluate our aligner and compare with otheraligners.
Following earlier work (MacCartney et al.,2008; Yao et al., 2013b), we use a normalized align-ment score of the two sentences to make a decisionbased on a threshold which we set using the dev set.Alignments with a higher-than-threshold score aretaken to be paraphrases and the rest non-paraphrases.Again, this is an oversimplified application of thealigner, even more so than in STS, since a smallchange in linguistic properties of two sentences(e.g.
polarity or modality) can turn them into non-5https://code.google.com/p/jacana/6http://research.microsoft.com/en-us/downloads/607d14d9-20cd-47e3-85bc-a2f65cd28042/227System Acc.% P% R% F1%Madnani et al.
(2012) 77.4 79.0 89.9 84.1Yao et al.
(2013a) 70.0 72.6 88.1 79.6Yao et al.
(2013b) 68.1 68.6 95.8 79.9This Work 73.4 76.6 86.4 81.2Table 5: Extrinsic evaluation on MSR paraphrase dataparaphrases despite having a high degree of align-ment.
So the aligner was not expected to demonstratestate-of-the-art performance, but still it gets close asshown in Table 5.
The first column shows the accu-racy of each system in classifying the input sentencesinto one of two classes: true (paraphrases) and false(non-paraphrases).
The rest of the columns show theperformance of the system for the true class in termsof precision, recall, and F1 score.
Italicized numbersrepresent scores that were not reported by the authorsof the corresponding papers, but have been recon-structed from the reported data (and hence are likelyto have small precision errors).The first row shows the best performance by anysystem on the test set to the best of our knowledge.The next two rows show the performances of twostate-of-the-art aligners (performances of both sys-tems were reported in (Yao et al., 2013b)).
Thelast row shows the performance of our aligner.
Al-though it does worse than the best paraphrase system,it outperforms the other aligners.5 DiscussionOur experiments reveal that a word aligner based onsimple measures of lexical and contextual similar-ity can demonstrate state-of-the-art accuracy.
How-ever, as alignment is frequently a component of largertasks, high accuracy alone is not always sufficient.Other dimensions of an aligner?s usability includespeed, consumption of computing resources, replica-bility, and generalizability to different applications.Our design goals include achieving a balance amongsuch multifarious and conflicting goals.A speed advantage of our aligner stems from for-mulating the problem as one-to-one word alignmentand thus avoiding an expensive decoding phase.
Thepresence of multiple phases is offset by discardingalready aligned words in subsequent phases.
Theuse of PPDB as the only (hashable) word similarityresource helps in reducing latency as well as spacerequirements.
As shown in Section 4.1.1, furtherspeedup could be achieved with only a small perfor-mance degradation by considering only the textualneighborhood as source of contextual evidence.However, the two major goals that we believe thealigner achieves to the greatest extent are replicabil-ity and generalizability.
The easy replicability ofthe aligner stems from its use of only basic and fre-quently used NLP modules (a lemmatizer, a POStagger, an NER module, and a dependency parser: allavailable as part of the Stanford CoreNLP suite7; weuse a Python wrapper8) and a single word similarityresource (PPDB).We experimentally show that the aligner can besuccessfully applied to different alignment datasetsas well as multiple end tasks.
We believe a designcharacteristic that enhances the generalizability ofthe aligner is its minimal dependence on the MSRalignment training data, which originates from a tex-tual entailment corpus having unique properties suchas disparities in the lengths of the input sentencesand a directional nature of their relationship (i.e.,the premise implying the hypothesis, but not viceversa).
A related potential reason is the symmetryof the aligner?s output (caused by its assumption ofno directionality) ?
the fact that it outputs the sameset of alignments regardless of the order of the inputsentences, in contrast to most existing aligners.Major limitations of the aligner include the inabil-ity to align phrases, including multiword expressions.It is incapable of capturing and exploiting long dis-tance dependencies among words (e.g.
coreferences).No word similarity resource is perfect and PPDB isno exception, therefore certain word alignments alsoremain undetected.6 ConclusionsWe show how contextual evidence can be used toconstruct a monolingual word aligner with certain de-sired properties, including state-of-the-art accuracy,easy replicability, and high generalizability.
Somepotential avenues for future work include: allow-ing phrase-level alignment via phrasal similarity re-sources (e.g.
the phrasal paraphrases of PPDB), in-cluding other sources of similarity such as vectorspace models or WordNet relations, expanding the set7http://nlp.stanford.edu/downloads/corenlp.shtml8https://github.com/dasmith/stanford-corenlp-python228of dependency equivalences and/or using semanticrole equivalences, and formulating our alignment al-gorithm as objective optimization rather than greedysearch.The aligner is available for download athttps://github.com/ma-sultan/monolingual-word-aligner.AcknowledgmentsThis material is based in part upon work supported bythe National Science Foundation under Grant Num-bers EHR/0835393 and EHR/0835381.
Any opin-ions, findings, and conclusions or recommendationsexpressed in this material are those of the author(s)and do not necessarily reflect the views of the Na-tional Science Foundation.ReferencesEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic Textual Similarity.
In Proceedings ofthe Second Joint Conference on Lexical and Compu-tational Semantics.
Association for ComputationalLinguistics, 32-43.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics.
Association for Computa-tional Linguistics, 597-604.Daniel Ba?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
UKP: computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the First Joint Conference onLexical and Computational Semantics.
Association forComputational Linguistics, 435-440.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The Second PASCAL Recognising Textual En-tailment Challenge.
In Proceedings of The SecondPASCAL Recognising Textual Entailment Challenge.Chris Brockett.
2007.
Aligning the RTE 2006 Cor-pus.
Technical Report MSR-TR-2007-77, MicrosoftResearch.Nathanael Chambers, Daniel Cer, Trond Grenager, DavidHall, Chloe Kiddon, Bill MacCartney, Marie-Catherinede Marneffe, Daniel Ramage, Eric Yeh, and ChristopherD Manning.
2007.
Learning alignments and leverag-ing natural logic.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing As-sociation for Computational Linguistics, 165-170.Ming-Wei Chang, Dan Goldwasser, Dan Roth, and VivekSrikumar.
2010.
Discriminative Learning over Con-strained Latent Representations.
In Proceedings of the2010 Annual Conference of the North American Chap-ter of the Association for Computational LinguisticsAssociation for Computational Linguistics, 429-437.Dipanjan Das and Noah A. Smith.
2009.
Paraphrase Iden-tication as Probabilistic Quasi-Synchronous Recogni-tion.
In Proceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th InternationalJoint Conference on Natural Language Processing ofthe AFNLP.
Association for Computational Linguistics,468-476.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InProceedings of the International Conference on Lan-guage Resources and Evaluation.
449-454.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.Technical Report, Stanford University.Bill Dolan, Chris Quirk, and Chris Brockett.
2004.
Un-supervised Construction of Large Paraphrase Corpora:Exploiting Massively Parallel News Sources.
In Pro-ceedings of the International Conference on Compu-tational Linguistics.
Association for ComputationalLinguistics, 350-356.Jenny Rose Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating Non-local Information intoInformation Extraction Systems by Gibbs Sampling.
InProceedings of the 43rd Annual Meeting of the Associ-ation for Computational Linguistics.
Association forComputational Linguistics, 363-370.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The ParaphraseDatabase.
In Proceedings of the 2013 Conference of theNorth American Chapter of the Association for Com-putational Linguistics.
Association for ComputationalLinguistics, 758-764.Lushan Han, Abhay Kashyap, Tim Finin, James Mayeld,and Jonathan Weese.
2013.
UMBC EBIQUITY-CORE:Semantic Textual Similarity Systems.
In Proceedingsof the Second Joint Conference on Lexical and Compu-tational Semantics, Volume 1.
Association for Compu-tational Linguistics, 44-52.Andrew Hickl and Jeremy Bensley.
2007.
A DiscourseCommitment-Based Framework for Recognizing Tex-tual Entailment.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Paraphrasing.
As-sociation for Computational Linguistics, 171-176.Andrew Hickl, Jeremy Bensley, John Williams, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Recog-nizing Textual Entailment with LCCs GROUNDHOG229System.
In Proceedings of the Second PASCAL Chal-lenges Workshop on Recognizing Textual Entailment.Milen Kouylekov and Bernardo Magnini.
2005.
Rec-ognizing textual entailment with tree edit distance al-gorithms.
In Proceedings of the PASCAL ChallengesWorkshop: Recognising Textual Entailment Challenge17-20.Bill MacCartney, Michel Galley, and Christopher D. Man-ning.
2008.
A Phrase-Based Alignment Model for Nat-ural Language Inference.
In Proceedings of the 2008Conference on Empirical Methods in Natural LanguageProcessing.
Association for Computational Linguistics,802-811.Nitin Madnani, Joel Tetreault, and Martin Chodorow.2012.
Re-examining Machine Translation Metrics forParaphrase Identification.
In Proceedings of 2012 Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics.
Association forComputational Linguistics, 182-190.Kapil Thadani and Kathleen McKeown.
2011.
Optimaland Syntactically-Informed Decoding for MonolingualPhrase-Based Alignment.
In Proceedings of the 49thAnnual Meeting of the Association for ComputationalLinguistics: Human Language Technologies.
Associa-tion for Computational Linguistics, 254-259.Kapil Thadani, Scott Martin, and Michael White.
2012.A Joint Phrasal and Dependency Model for ParaphraseAlignment.
In Proceedings of COLING 2012: Posters.1229-1238.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003 Feature-rich Part-of-speechTagging with a Cyclic Dependency Network In Pro-ceedings of the 2003 Human Language TechnologyConference of the North American Chapter of the Asso-ciation for Computational Linguistics.
Association forComputational Linguistics, 173-180.Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,and Peter Clark.
2013a.
A Lightweight and High Per-formance Monolingual Word Aligner.
In Proceedingsof the 51st Annual Meeting of the Association for Com-putational Linguistics.
Association for ComputationalLinguistics, 702-707.Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,and Peter Clark.
2013b.
Semi-Markov Phrase-basedMonolingual Alignment.
In Proceedings of the 2013Conference on Empirical Methods in Natural LanguageProcessing.
Association for Computational Linguistics,590-600.Xuchen Yao, Benjamin Van Durme, Chris Callison-Burch,and Peter Clark.
2013c.
Answer Extraction as Se-quence Tagging with Tree Edit Distance.
In Proceed-ings of the 2013 Conference of the North AmericanChapter of the Association for Computational Linguis-tics.
Association for Computational Linguistics, 858-867.230
