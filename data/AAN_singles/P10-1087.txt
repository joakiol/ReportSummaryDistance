Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 844?853,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsUntangling the Cross-Lingual Link Structure of WikipediaGerard de MeloMax Planck Institute for InformaticsSaarbru?cken, Germanydemelo@mpi-inf.mpg.deGerhard WeikumMax Planck Institute for InformaticsSaarbru?cken, Germanyweikum@mpi-inf.mpg.deAbstractWikipedia articles in different languagesare connected by interwiki links that areincreasingly being recognized as a valu-able source of cross-lingual information.Unfortunately, large numbers of links areimprecise or simply wrong.
In this pa-per, techniques to detect such problems areidentified.
We formalize their removal asan optimization task based on graph re-pair operations.
We then present an al-gorithm with provable properties that useslinear programming and a region growingtechnique to tackle this challenge.
Thisallows us to transform Wikipedia into amuch more consistent multilingual regis-ter of the world?s entities and concepts.1 IntroductionMotivation.
The open community-maintained en-cyclopedia Wikipedia has not only turned the In-ternet into a more useful and linguistically di-verse source of information, but is also increas-ingly being used in computational applications asa large-scale source of linguistic and encyclope-dic knowledge.
To allow cross-lingual navigation,Wikipedia offers cross-lingual interwiki links thatfor instance connect the Indonesian article aboutAlbert Einstein to the corresponding articles inover 100 other languages.
Such links are extraor-dinarily valuable for cross-lingual applications.In the ideal case, a set of articles connected di-rectly or indirectly via such links would all de-scribe the same entity or concept.
Due to concep-tual drift, different granularities, as well as mis-takes made by editors, we frequently find con-cepts as different as economics and manager in thesame connected component.
Filtering out inaccu-rate links enables us to exploit Wikipedia?s multi-linguality in a much safer manner and allows us tocreate a multilingual register of named entities.Contribution.
Our research contributions are:1) We identify criteria to detect inaccurate connec-tions in Wikipedia?s cross-lingual link structure.2) We formalize the task of removing such linksas an optimization problem.
3) We introduce analgorithm that attempts to repair the cross-lingualgraph in a minimally invasive way.
This algorithmhas an approximation guarantee with respect tooptimal solutions.
4) We show how this algorithmcan be used to combine all editions of Wikipediainto a single large-scale multilingual register ofnamed entities and concepts.2 Detecting Inaccurate LinksIn this paper, we model the union of cross-linguallinks provided by all editions of Wikipedia as anundirected graph G = (V,E) with edge weightsw(e) for e ?
E. In our experiments, we simplyhonour each individual link equally by definingw(e) = 2 if there are reciprocal links between thetwo pages, 1 if there is a single link, and 0 other-wise.
However, our framework is flexible enoughto deal with more advanced weighting schemes,e.g.
one could easily plug in cross-lingual mea-sures of semantic relatedness between article texts.It turns out that an astonishing number of con-nected components in this graph harbour inac-curate links between articles.
For instance, theEsperanto article ?Germana Imperiestro?
is aboutGerman emporers and another Esperanto article?Germana Imperiestra Regno?
is about the Ger-man Empire, but, as of June 2010, both are linkedto the English and German articles about the Ger-man Empire.
Over time, some inaccurate linksmay be fixed, but in this and in large numbers ofother cases, the imprecise connection has persistedfor many years.
In order to detect such cases, weneed to have some way of specifying that two ar-ticles are likely to be distinct.844Figure 1: Connected component with inaccuratelinks (simplified)2.1 Distinctness AssertionsFigure 1 shows a connected component that con-flates the concept of television as a medium withthe concept of TV sets as devices.
Among otherthings, we would like to state that ?Television?
and?T.V.?
are distinct from ?Television set?
and ?TVset?.
In general, we may have several sets of enti-ties Di,1, .
.
.
, Di,li , for which we assume that anytwo entities u,v from different sets are pairwisedistinct with some degree of confidence or weight.In our example, Di,1 = {?Television?,?T.V.?
}would be one set, andDi,2 = {?Television set?,?TVset?}
would be another set, which means that weare assuming ?Television?, for example, to be dis-tinct from both ?Television set?
and ?TV set?.Definition 1.
(Distinctness Assertions) Given aset of nodes V , a distinctness assertion is a col-lection Di = (Di,1, .
.
.
, Di,li) of pairwise dis-joint (i.e.
Di,j ?
Di,k = ?
for j 6= k) sub-sets Di,j ?
V that expresses that any two nodesu ?
Di,j , v ?
Di,k from different subsets (j 6= k)are asserted to be distinct from each other withsome weight w(Di) ?
R.We found that many components with inaccuratelinks can be identified automatically with the fol-lowing distinctness assertions.Criterion 1.
(Distinctness between articles fromthe same Wikipedia edition) For each language-specific edition of Wikipedia, a separate asser-tion (Di,1, Di,2, .
.
. )
can be made, where eachDi,j contains an individual article together withits respective redirection pages.
Two articles fromthe same Wikipedia very likely describe distinctconcepts unless they are redirects of each other.For example, ?Georgia (country)?
is distinct from?Georgia (U.S. State)?.
Additionally, there are alsoredirects that are clearly marked by a category ortemplate as involving topic drift, e.g.
redirectsfrom songs to albums or artists, from products tocompanies, etc.
We keep such redirects in a Di,jdistinct from the one of their redirect targets.Criterion 2.
(Distinctness between categoriesfrom the same Wikipedia edition) For eachlanguage-specific edition of Wikipedia, a separateassertion (Di,1, Di,2, .
.
. )
is made, where eachDi,j contains a category page together with anyredirects.
For instance, ?Category:Writers?
is dis-tinct from ?Category:Writing?.Criterion 3.
(Distinctness for links with anchoridentifiers) The English ?Division by zero?, for in-stance, links to the German ?Null#Division?.
Thelatter is only a part of a larger article about thenumber zero in general, so we can make a dis-tinctness assertion to separate ?Division by zero?from ?Null?.
In general, for each interwiki link orredirection with an anchor identifier, we add an as-sertion (Di,1, Di,2) where Di,1,Di,2 represent therespective articles without anchor identifiers.These three types of distinctness assertions areinstantiated for all articles and categories of allWikipedia editions.
The assertion weights are tun-able; the simplest choice is using a uniform weightfor all assertions (note that these weights are dif-ferent from the edge weights in the graph).
Wewill revisit this issue in our experiments.2.2 Enforcing ConsistencyGiven a graph G representing cross-lingual linksbetween Wikipedia pages, as well as distinctnessassertions D1, .
.
.
, Dn with weights w(Di), wemay find that nodes that are asserted to be dis-tinct are in the same connected component.
Wecan then try to apply repair operations to recon-cile the graph?s link structure with the distinctnessasssertions and obtain global consistency.
Thereare two ways to modify the input, and for eachwe can also consider the corresponding weightsas a sort of cost that quantifies how much we arechanging the original input:a) Edge cutting: We may remove an edge e ?E from the graph, paying cost w(e).b) Distinctness assertion relaxation: We mayremove a node v ?
V from a distinctness as-sertion Di, paying cost w(Di).845Removing edges allows us to split connected com-ponents into multiple smaller components, therebyensuring that two nodes asserted to be distinct areno longer connected directly or indirectly.
In Fig-ure 1, for instance, we could delete the edge fromthe Spanish ?TV set?
article to the Japanese ?televi-sion?
article.
In constrast, removing nodes fromdistinctness assertions means that we decide togive up our claim of them being distinct, insteadallowing them to share a connected component.Our reliance on costs is based on the assump-tion that the link structure or topology of the graphprovides the best indication of which cross-linguallinks to remove.
In Figure 1, we have distinct-ness assertions between nodes in two densely con-nected clusters that are tied together only by a sin-gle spurious link.
In such cases, edge removalscan easily yield separate connected components.When, however, the two nodes are strongly con-nected via many different paths with high weights,we may instead opt for removing one of the twonodes from the distinctness assertion.The aim will be to balance the costs for remov-ing edges from the graph with the costs for remov-ing nodes from distinctness assertions to producea consistent solution with a minimal total repaircost.
We accommodate our knowledge about dis-tinctness while staying as close as possible to whatWikipedia provides as input.This can be formalized as the WeightedDistinctness-Based Graph Separation (WDGS)problem.
Let G be an undirected graph with a setof vertices V and a set of edges E weighted byw : E ?
R. If we use a set C ?
V to spec-ify which edges we want to cut from the originalgraph, and sets Ui to specify which nodes we wantto remove from distinctness assertions, we can be-gin by defining WDGS solutions as follows.Definition 2.
(WDGS Solution).
Given a graphG = (V,E) and n distinctness assertionsD1, .
.
.
,Dn, a tuple (C,U1, .
.
.
, Un) is a valid WDGS so-lution if and only if ?i, j, k 6= j, u ?
Di,j \ Ui,v ?
Di,k \ Ui: P(u, v, E \ C) = ?, i.e.
the set ofpaths from u to v in the graph (V,E \C) is empty.Definition 3.
(WDGS Cost).
Let w : E ?
Rbe a weight function for edges e ?
E, and w(Di)(i = 1 .
.
.
n) be weights for the distinctness as-sertions.
The (total) cost of a WDGS solutionS = (C,U1, .
.
.
, Un) is then defined asc(S) = c(C,U1, .
.
.
, Un)=[?e?Cw(e)]+[n?i=1|Ui|w(Di)]Definition 4.
(WDGS).
A WDGS problem instanceP consists of a graph G = (V,E) with edgeweights w(e) and n distinctness assertions D1,.
.
.
, Dn with weights w(Di).
The objective con-sists in finding a solution (C,U1, .
.
.
, Un) withminimal cost c(C,U1, .
.
.
, Un).It turns out that finding optimal solutions effi-ciently is a hard problem (proofs in Appendix A).Theorem 1.
WDGS is NP-hard and APX-hard.
Ifthe Unique Games Conjecture (Khot, 2002) holds,then it is NP-hard to approximate WDGS withinany constant factor ?
> 0.3 Approximation AlgorithmDue to the hardness of WDGS, we devise apolynomial-time approximation algorithm with anapproximation factor of 4 ln(nq + 1) where n isthe number of distinctness assertions and q =maxi,j|Di,j |.
This means that for all problem in-stances P , we can guaranteec(S(P ))c(S?
(P ))?
4 ln(nq + 1),where S(P ) is the solution determined by our al-gorithm, and S?
(P ) is an optimal solution.
Notethat this approximation guarantee is independentof how long each Di is, and that it merely repre-sents an upper bound on the worst case scenario.In practice, the results tend to be much closer tothe optimum, as will be shown in Section 4.Our algorithm first solves a linear program (LP)relaxation of the original problem, which givesus hints as to which edges should most likely becut and which nodes should most likely be re-moved from distinctness assertions.
Note that thisis a continuous LP, not an integer linear program(ILP); the latter would not be tractable due to thelarge number of variables and constraints of theproblem.
After solving the linear program, a new?
extended ?
graph is constructed and the optimalLP solution is used to define a distance metric onit.
The final solution is obtained by smartly se-lecting regions in this extended graph as the in-dividual output components, employing a region846growing technique in the spirit of the seminal workby Leighton and Rao (1999).
Edges that cross theboundaries of these regions are cut.Definition 5.
Given a WDGS instance, we define alinear program of the following form:minimize?e?Edew(e) +n?i=1li?j=1?v?Di,jui,vw(Di)subject topi,j,v = ui,v ?i, j<li, v ?
Di,j (1)pi,j,v + ui,v ?
1 ?i, j<li, v ?Sk>jDi,k (2)pi,j,v ?
pi,j,u + de ?i, j<li, e=(u,v) ?
E (3)de ?
0 ?e ?
E (4)ui,v ?
0 ?i, v ?liSj=1Di,j (5)pi,j,v ?
0 ?i, j<li, v?V (6)The LP uses decision variables de and ui,v, andauxiliary variables pi,j,v that we refer to as poten-tial variables.
The de variables indicate whether(in the continuous LP: to what degree) an edgee should be deleted, and the ui,v variables indi-cate whether (to what degree) v should be removedfrom a distinctness assertion Di.
The LP objec-tive function corresponds to Definition 3, aimingto minimize the total costs.
A potential variablepi,j,v reflects a sort of potential difference betweenan assertionDi,j and a node v. If pi,j,v = 0, then vis still connected to nodes in Di,j .
Constraints (1)and (2) enforce potential differences between Di,jand all nodes in Di,k with k > j.
For instance,for distinctness between ?New York City?
and ?NewYork?
(the state), they might require ?New York?to have a potential of 1, while ?New York City?has a potential of 0.
The potential variables aretied to the deletion variables de for edges in Con-straint (3) as well as to the ui,v in Constraints (1)and (2).
This means that the potential differencepi,j,v + ui,v ?
1 can only be obtained if edges aredeleted on every path between ?New York City?
and?New York?, or if at least one of these two nodes isremoved from the distinctness assertion (by settingthe corresponding ui,v to non-zero values).
Con-straints (4), (5), (6) ensure non-negativity.Having solved the linear program, the next ma-jor step is to convert the optimal LP solution intothe final ?
discrete ?
solution.
We cannot relyon standard rounding methods to turn the optimalfractional values of the de and ui,v variables intoa valid solution.
Often, all solution variables havesmall values and rounding will merely produce anempty (C,U1, .
.
.
, Un) = (?, ?, .
.
.
, ?).
Instead,a more sophisticated technique is necessary.
Theoptimal solution of the LP can be used to definean extended graph G?
with a distance metric d be-tween nodes.
The algorithm then operates on thisgraph, in each iteration selecting regions that be-come output components and removing them fromthe graph.
A simple example is shown in Figure 2.The extended graph contains additional nodes andedges representing distinctness assertions.
Cuttingone of these additional edges corresponds to re-moving a node from a distinctness assertion.Definition 6.
Given G = (V,E) and distinct-ness assertions D1, .
.
.
, Dn with weights w(Di),we define an undirected graph G?
= (V ?, E?
)where V ?
= V ?
{vi,v | i = 1 .
.
.
n, w(Di) >0, v ?
?j Di,j}, E?
= {e ?
E | w(e) > 0} ?
{(v, vi,v) | v ?
Di,j , w(Di) > 0}.
We accordinglyextend the definition of w(e) to additionally coverthe new edges by defining w(e) = w(Di) for e =(v, vi,v).
We also extend it for sets S of edges bydefining w(S) =?e?S w(e).
Finally, we define anode distance metricd(u, v) =????????????????????
?0 u = vde (u, v) ?
Eui,v u = vi,vui,u v = vi,uminp?P(u,v,E?)?(u?,v?
)?pd(u?, v?)
otherwise,where P(u, v, E?)
denotes the set of acyclic pathsbetween two nodes in E?.
We further fixc?f =?
(u,v)?E?d(u, v)w(e)as the weight of the fractional solution of the LP(c?f is a constant based on the original E?, irre-spective of later modifications to the graph).Definition 7.
Around a given node v in G?, weconsider regions R(v, r) ?
V with radius r. Thecut C(v, r) of a given region is defined as the setof edges in G?
with one endpoint within the regionand one outside the region:R(v, r) = {v?
?
V ?
| d(v, v?)
?
r}C(v, r) = {e ?
E?
| |e ?R(v, r)| = 1}For sets of nodes S ?
V , we define R(S, r) =?v?SR(v, r) and C(S, r) =?v?SC(v, r).847Figure 2: Extended graph with two added nodesv1,u, v1,v representing distinctness between ?Tele-visio?n?
and ?Televisor?, and a region around v1,uthat would cut the link from the Japanese ?Televi-sion?
to ?Televisor?Definition 8.
Given q = maxi,j|Di,j |, we approxi-mate the optimal cost of regions as:c?
(v, r) =?e=(u,u?)?E?
:e?R(v,r)d(u, u?
)w(e) (1)+?e?C(v,r)v?
?e?R(v,r)(r ?
d(v, v?))w(e)c?
(S, r) =1nqc?f +?v?Sc?
(v, r) (2)The first summand accounts for the edges en-tirely within the region, and the second one ac-counts for the edges in C(v, r) to the extent thatthey are within the radius.
The definition of c?
(S, r)contains an additional slack component that is re-quired for the approximation guarantee proof.Based on these definitions, Algorithm 3.1 usesthe LP solution to construct the extended graph.It then repeatedly, as long as there is an unsatis-fied assertion Di, chooses a set S of nodes con-taining one node from each relevant Di,j .
Aroundthe nodes in S it simultaneously grows |S| regionswith the same radius, a technique previously sug-gested by Avidor and Langberg (2007).
These re-gions are essentially output components that de-termine the solution.
Repeatedly choosing theradius that minimizes w(C(S,r))c?
(S,r) allows us to ob-tain the approximation guarantee, because the dis-tances in this extended graph are based on the so-lution of the LP.
The properties of this algorithmare given by the following two theorems (proofs inAppendix A).Theorem 2.
The algorithm yields a valid WDGSsolution (C,U1, .
.
.
, Un).Theorem 3.
The algorithm yields a solution(C,U1, .
.
.
, Un) with an approximation factor of4 ln(nq + 1) with respect to the cost of the op-timal WDGS solution (C?, U?1 , .
.
.
, U?n), where nis the number of distinctness assertions and q =maxi,j|Di,j |.
This solution can be obtained in poly-nomial time.4 Results4.1 WikipediaWe downloaded February 2010 XML dumps ofall available editions of Wikipedia, in total 272editions that amount to 86.5 GB uncompressed.From these dumps we produced two datasets.Dataset A captures cross-lingual interwiki linksbetween pages, in total 77.07 million undirectededges (146.76 million original links).
DatasetB additionally includes 2.2 million redirect-basededges.
Wikipedia deals with interwiki links toredirects transparently, however there are manyredirects with titles that do not co-refer, e.g.
redi-rects from members of a band to the band, or fromaspects of a topic to the topic in general.
We onlyincluded redirects in the following cases:?
the titles of redirect and redirect target matchafter Unicode NFKD normalization, diacrit-ics removal, case conversion, and removal ofpunctuation characters?
the redirect uses certain templates or cate-gories that indicate co-reference with the tar-get (alternative names, abbreviations, etc.
)We treated them like reciprocal interwiki links byassigning them a weight of 2.4.2 Application of AlgorithmThe choice of distinctness assertion weights de-pends on how lenient we wish to be towards con-ceptual drift, allowing us to opt for more fine- ormore coarse-grained distinctions.
In our experi-ments, we decided to prefer fine-grained concep-tual distinctions, and settled on a weight of 100.We analysed over 20 million connected com-ponents in each dataset, checking for distinctnessassertions.
For the roughly 110,000 connectedcomponents with relevant distinctness assertions,848Algorithm 3.1 WDGS Approximation Algorithm1: procedure SELECT(V,E, V ?, E?, w,D1, .
.
.
, Dn, l1, .
.
.
, ln)2: solve linear program given by Definition 5 .
determine optimal fractional solution3: construct G?
= (V ?, E?)
.
extended graph (Definition 6)4: C ?
{e ?
E | w(e) = 0} .
cut zero-weighted edges5: Ui ?li?1?j=1Di,j ?i : w(Di) = 0 .
remove zero-weighted Di6: while ?i, j, k > j, u ?
Di,j , v ?
Di,k : P(vi,u, vi,v, E?)
6= ?
do .
find unsatisfied assertion7: S ?
?
.
set of nodes around which regions will be grown8: for all j in 1 .
.
.
li ?
1 do .
arbitrarily choose node from each Di,j9: if ?v ?
Di,j : vi,v ?
V ?
then S ?
S ?
vi,v10: D ?
{d(u, v) ?
12 | u ?
S, v ?
V?}
?
{12} .
set of distances11: choose  such that ?d, d?
?
D : 0 <  |d?
d?| .
infinitesimally small12: r ?
argminr=d?: d?D\{0}w(C(S, r))c?
(S, r).
choose optimal radius (ties broken arbitrarily)13: V ?
?
V ?
\R(S, r) .
remove regions from G?14: E?
?
{e ?
E?
| e ?
V ?
}15: C ?
C ?
(C(S, r) ?
E) .
update global solution16: for all i?
in 1 .
.
.
n do17: Ui?
?
Ui?
?
{v | (vi?,v, v) ?
C(S, r)}18: for all j in 1 .
.
.
li?
do Di?,j ?
Di?,j ?
V ?
.
prune distinctness assertions19: return (C,U1, .
.
.
, Un)we applied our algorithm, relying on the commer-cial CPLEX tool to solve the linear programs.
Inmost cases, the LP solving took less than a second,however the LP sizes grow exponentially with thenumber of nodes and hence the time complex-ity increases similarly.
In about 300 cases perdataset, CPLEX took too long and was automat-ically killed or the linear program was a priorideemed too large to complete in a short amountof time.
For these cases, we adopted an alternativestrategy described later on.Table 1 provides the experimental results for thetwo datasets.
Dataset B is more connected andthus has fewer connected components with morepairs of nodes asserted to be distinct by distinct-ness assertions.
The LP given by Definition 5provides fractional solutions that constitute lowerbounds on the optimal solution (cf.
also Lemma5 in Appendix A), so the optimal solution can-not have a cost lower than the fractional LP solu-tion.
Table 1 shows that in practice, our algorithmachieves near-optimal results.4.3 Linguistic AdequacyThe near-optimal results of our algorithm applywith respect to our problem formalization, whichaims at repairing the graph in a minimally inva-Table 1: Algorithm ResultsDataset A Dataset BConnectedcomponents23,356,027 21,161,631?
with distinctnessassertions112,857 113,714?
algorithm appliedsuccessfully112,580 113,387Distinctnessassertions380,694 379,724Node pairs con-sidered distinct916,554 1,047,299Lower bound onoptimal cost1,255,111 1,245,004Cost of our solution 1,306,747 1,294,196Factor 1.04 1.04Edges to be deleted(undirected)1,209,798 1,199,181Nodes to be merged 603 573sive way.
It may happen, however, that the graph?stopology is misleading, and that in a specific casedeleting many cross-lingual links to separate twoentities is more appropriate than looking for aconservative way to separate them.
This led us849to study the linguistic adequacy.
Two annotatorsevaluated 200 randomly selected separated pairsfrom Dataset A consisting of an English and aGerman article, with an inter-annotator agreement(Cohen ?)
of 0.656.
Examples are given in Table2.
We obtained a precision of 87.97% ?
0.04%(Wilson score interval) against the consensus an-notation.
Many of the errors are the result of ar-ticles having many inaccurate outgoing links, inwhich case they may be assigned to the wrongcomponent.
In other cases, we noted duplicate ar-ticles in Wikipedia.Occasionally, we also observed differences inscope, where one article would actually describetwo related concepts in a single page.
Our algo-rithm will then either make a somewhat arbitraryassignment to the component of either the first orsecond concept, or the broader generalization ofthe two concepts becomes a separate, more gen-eral connected component.4.4 Large Problem InstancesWhen problem instances become too large, the lin-ear programs can become too unwieldy for lin-ear optimization software to cope with on currenthardware.
In such cases, the graphs tend to be verysparsely connected, consisting of many smaller,more densely connected subgraphs.
We thus in-vestigated graph partitioning heuristics to decom-pose larger graphs into smaller parts that can moreeasily be handled with our algorithm.
The METISalgorithms (Karypis and Kumar, 1998) can de-compose graphs with hundreds of thousands ofnodes almost instantly, but favour equally sizedclusters over lower cut costs.
We obtained parti-tionings with costs orders of magnitude lower us-ing the heuristic by Dhillon et al (2007).4.5 Database of Named EntitiesThe partitioning heuristics allowed us to processall entries in the complete set of Wikipedia dumpsand produce a clean output set of connected com-ponents where each Wikipedia article or categorybelongs to a connected component consisting ofpages about the same entity or concept.
We can re-gard these connected components as equivalenceclasses.
This means that we obtain a large-scalemultilingual database of named entities and theirtranslations.
We are also able to more safely trans-fer information cross-lingually between editions.For example, when an article a has a category c inthe French Wikipedia, we can suggest the corre-sponding Indonesian category for the correspond-ing Indonesian article.Moreover, we believe that this database willhelp extend resources like DBPedia and YAGOthat to date have exclusively used the EnglishWikipedia as their repository of entities andclasses.
With YAGO?s category heuristics, evenentirely non-English connected components canbe assigned a class in WordNet as long as at leastone of the relevant categories has an English page.So, the French Wikipedia article on the Dutchschooner ?JR Tolkien?, despite the lack of a cor-responding English article, can be assigned to theWordNet synset for ?ship?.
Using YAGO?s plu-ral heuristic to distinguish classes (Einstein is aphysicist) from topic descriptors (Einstein belongsto the topic physics), we determined that over 4.8million connected components can be linked toWordNet, greatly surpassing the 3.2 million arti-cles covered by the English Wikipedia alone.5 Related WorkA number of projects have used Wikipedia as adatabase of named entities (Ponzetto and Strube,2007; Silberer et al, 2008).
The most well-known are probably DBpedia (Auer et al, 2007),which serves as a hub in the Linked Data Web,Freebase1, which combines human input and au-tomatic extractors, and YAGO (Suchanek et al,2007), which adds an ontological structure on topof Wikipedia?s entities.
Wikipedia has been usedcross-lingually for cross-lingual IR (Nguyen et al,2009), question answering (Ferra?ndez et al, 2007)as well as for learning transliterations (Pasternackand Roth, 2009), among other things.Mihalcea and Csomai (2007) have studied pre-dicting new links within a single edition ofWikipedia.
Sorg and Cimiano (2008) consideredthe problem of suggesting new cross-lingual links,which could be used as additional inputs in ourproblem.
Adar et al (2009) and Bouma et al(2009) show how cross-lingual links can be usedto propagate information from one Wikipedia?s in-foboxes to another edition.Our aggregation consistency algorithm usestheoretical ideas put forward by researchers study-ing graph cuts (Leighton and Rao, 1999; Garg etal., 1996; Avidor and Langberg, 2007).
Our prob-lem setting is related to that of correlation cluster-ing (Bansal et al, 2004), where a graph consist-1http://www.freebase.com/850Table 2: Examples of separated conceptsEnglish concept German concept(translated)ExplanationCoffee percolator French Press different types of brewing devicesBaqa-Jatt Baqa al-Gharbiyye Baqa-Jatt is a city resulting from a mergerof Baqa al-Gharbiyye and JattLeucothoe (plant) Leucothea (Orchamos) the second refers to a figure of GreekmythologyOld Belarusian language Ruthenian language the second is often considered slightlybroadering of positively and negatively labelled similar-ity edges is clustered such that similar items aregrouped together, however our approach is muchmore generic than conventional correlation clus-tering.
Charikar et al (2005) studied a variationof correlation clustering that is similar to WDGS,but since a negative edge would have to be addedbetween each relevant pair of entities in a distinct-ness assertion, the approximation guarantee wouldonly be O(log(n |V |2)).
Minimally invasive re-pair operations on graphs have also been stud-ied for graph similarity computation (Zeng et al,2009), where two graphs are provided as input.6 Conclusions and Future WorkWe have presented an algorithmic framework forthe problem of co-reference that produces consis-tent partitions by intelligently removing edges orallowing nodes to remain connected.
This algo-rithm has successfully been applied to Wikipedia?scross-lingual graph, where we identified and elim-inated surprisingly large numbers of inaccurateconnections, leading to a large-scale multilingualregister of names.In future work, we would like to investigatehow our algorithm behaves in extended settings,e.g.
we can use heuristics to connect isolated,unconnected articles to likely candidates in otherWikipedias using weighted edges.
This can beextended to include mappings from multiple lan-guages to WordNet synsets, with the hope thatthe weights and link structure will then allow thealgorithm to make the final disambiguation deci-sion.
Additional scenarios include dealing withco-reference on the Linked Data Web or mappingsbetween thesauri.
As such resources are increas-ingly being linked to Wikipedia and DBpedia, webelieve that our techniques will prove useful inmaking mappings more consistent.A ProofsProof (Theorem 1).
We shall reduce the mini-mum multicut problem to WDGS.
The hardnessclaims then follow from Chawla et al (2005).Given a graph G = (V,E) with a positive costc(e) for each e ?
E, and a set D = {(si, ti) | i =1 .
.
.
k} of k demand pairs, our goal is to finda multicut M with respect to D with minimumtotal cost?e?M c(e).
We convert each demandpair (si, ti) into a distinctness assertion Di =({si}, {ti}) with weight w(Di) = 1+?e?E c(e).An optimal WDGS solution (C,U1, .
.
.
, Uk) withcost c then implies a multicut C with the sameweight, because each w(Di) >?e?E c(e), soall demand pairs will be satisfied.
C is a minimalmulticut because any multicut C ?
with lower costwould imply a valid WDGS solution (C ?, ?, .
.
.
, ?
)with a cost lower than the optimal one, which is acontradiction.Lemma 4.
The linear program given by Defini-tion 5 enforces that for any i,j,k 6= j,u ?
Di,j ,v ?
Di,k, and any path v0, .
.
.
, vt with v0 = u,vt = v we obtain ui,u+?t?1l=0 d(vl,vl+1)+ui,v ?
1.The integer linear program obtained by aug-menting Definition 5 with integer constraintsde, ui,v, pi,j,v ?
{0, 1} (for all applicable e, i, j,v) produces optimal solutions (C,U1, .
.
.
, Uk) forWDGS problems, obtained as C = ({e ?
E | de =1}, Ui = {v | ui,v = 1}.Proof.
Without loss of generality, let us assumethat j < k. The LP constraints give us pi,j,vt ?pi,j,vt?1 +d(vt?1,vt), .
.
.
, pi,j,v1 ?
pi,j,v0 +d(v0,v1),as well as pi,j,v0 = ui,u and pi,j,vt + ui,v ?
1.Hence 1 ?
pi,j,vt+ui,v ?
ui,u+?t?1l=0 d(vl,vl+1)+ui,v.With added integrality constraints, we obtain ei-ther u ?
Ui, v ?
Ui, or at least one edge along anypath from u to v is cut, i.e.
P(u, v, E \ C) = ?.851This proves that any ILP solution enduces a validWDGS solution (Definition 2).Clearly, the integer program?s objective func-tion minimizes c(C,U1, .
.
.
, Un) (Definition 3) ifC = ({e ?
E | de = 1}, Ui = {v | ui,v = 1}.To see that the solutions are optimal, it thus suf-fices to observe that any optimal WDGS solution(C?, U?1 , .
.
.
, U?n) yields a feasible ILP solutionde = IC?
(e), ui,v = IU?i (v).Proof (Theorem 2).
ri < 12 holds for any ra-dius ri chosen by the algorithm, so for any re-gion R(v0, r) grown around a node v0, and anytwo nodes u, v within that region, the triangle in-equality gives us d(u, v) ?
d(u, v0) + d(v0, v) <12 +12 = 1 (maximal distance condition).
Atthe same time, by Lemma 4 and Definition 6 forany u ?
Di,j , v ?
Di,k (j 6= k), we obtaind(vi,u, vi,v) = d(vi,u, u) + d(u, v) + d(v, vi,v) ?1.
With the maximal distance condition above, thismeans that vi,u and vi,v cannot be in the same re-gion.
Hence u, v cannot be in the same region,unless the edge from vi,u to u is cut (in which caseu will be placed in Ui) or the edge from v to vi,vis cut (in which case v will be placed in Ui).
Sinceeach region is separated from other regions via C,we obtain that ?i, j, k 6= j, u, v: u ?
Di,j \ Ui,v ?
Di,k \ Ui implies P(u, v, E \ C) = ?, so avalid solution is obtained.Lemma 5 (essentially due to Garg et al (1996)).For any i where ?j, k > j, u ?
Di,j , v ?
Di,k :P(vi,u, vi,v, E?)
6= ?
and w(Di) > 0, there existsan r such that w(C(S, r)) ?
2 ln(nq + 1) c?
(S, r),0 ?
r < 12 for any set S consisting of vi,v nodes.Proof.
Define w(S, r) =?v?S w(C(v, r)).
Wewill prove that there exists an appropriate r withw(C(S, r)) ?
w(S, r) ?
2 ln(nq+1) c?
(S, r).
As-sume, for reductio ad absurdum, that ?r ?
[0, 12) :w(S, r) > 2 ln(nq + 1)c?
(S, r).
As we expandthe radius r, we note that c?
(S, r) ddr = w(S, r)whereever c?
is differentiable with respect to r.There are only a finite number of points r1,.
.
.
,rl?1in (0, 12) where this is not the case (namely, when?u ?
S, v ?
V ?
: d(u, v) = ri).
Also notethat c?
increases monotonically for increasing val-ues of r, and that it is universally greater thanzero (since there is a path between vi,u, vi,v).
Setr0 = 0, rl = 12 and choose  such that 0 < min{rj+1 ?
rj | j < l}.
Our assumption thenimplies:l?j=1?
rj?rj?1+w(S,r)c?
(S,r) dr>[l?j=1rj ?
rj?1 ?
2]2 ln(nq + 1)l?j=1ln c?
(S, rj ?
)?
ln c?
(S, rj?1 + )>(12 ?
2l)2 ln(nq + 1)ln c?
(S, 12 ?
)?
ln c?
(S, 0)> (1?
4l) ln(nq + 1)c?
(S, 12?)c?
(S,0) > (nq + 1)1?4lc?
(S, 12 ?
) > (nq + 1)1?4lc?
(S, 0)For small , the right term can get arbitrarily closeto (nq+1)c?
(S, 0) ?
c?f + c?
(S, 0), which is strictlylarger than c?
(S, 12 ?
) no matter how small  be-comes, so the initial assumption is false.Proof (Theorem 3).
Let Si, ri denote the setS and radius r chosen in particular iterations,and ci the corresponding costs incurred: ci =w(C(Si, r) ?
E) + |Ui|w(Di) = w(C(Di, r)).Note that any ri chosen by the algorithm will infact fulfil the criterion described by Lemma 5, be-cause ri is chosen to minimize the ratio betweenthe two terms, and the minimizing r ?
[0, 12)must be among the r considered by the algo-rithm (w(C(Di, r)) only changes at one of thosepoints, so the minimum is reached by approach-ing the points from the left).
Hence, we obtainci ?
2 ln(n+ 1)c?
(Si, ri).
For our global solution,note that there is no overlap between the regionschosen within an iteration, since regions have aradius strictly smaller than 12 , while vi,u, vi,v foru ?
Di,j , v ?
Di,k, j 6= k have a distance ofat least 1.
Nor is there any overlap between re-gions from different iterations, because in each it-eration the selected regions are removed from G?.Globally, we therefore obtain c(C,U1, .
.
.
, Un) =?i ci < 2 ln(nq + 1)?i c?
(Si, ri) ?
2 ln(nq +1)2c?f (observe that i ?
nq).
Since c?f is the ob-jective score for the fractional LP relaxation solu-tion of the WDGS ILP (Lemma 4), we obtain c?f ?c(C?, U?1 , .
.
.
, U?n), and thus c(C,U1, .
.
.
, Un) <4 ln(n+ 1)c(C?, U?1 , .
.
.
, U?n).To obtain a solution in polynomial time, notethat the LP size is polynomial with respect to nqand may be solved using a polynomial algorithm(Karmarkar, 1984).
The subsequent steps run inO(nq) iterations, each growing up to |V | regionsusing O(|V |2) uniform cost searches.852ReferencesEytan Adar, Michael Skinner, and Daniel S. Weld.2009.
Information arbitrage across multi-lingualWikipedia.
In Ricardo A. Baeza-Yates, Paolo Boldi,Berthier A. Ribeiro-Neto, and Berkant Barla Cam-bazoglu, editors, Proceedings of the 2nd Interna-tional Conference on Web Search and Web DataMining, WSDM 2009, pages 94?103.
ACM.So?ren Auer, Chris Bizer, Jens Lehmann, Georgi Kobi-larov, Richard Cyganiak, and Zachary Ives.
2007.DBpedia: a nucleus for a web of open data.
InAberer et al, editor, The Semantic Web, 6th Interna-tional Semantic Web Conference, 2nd Asian Seman-tic Web Conference, ISWC 2007 + ASWC 2007, Bu-san, Korea, November 11?15, 2007, Lecture Notesin Computer Science 4825.
Springer.Adi Avidor and Michael Langberg.
2007.
The multi-multiway cut problem.
Theoretical Computer Sci-ence, 377(1-3):35?42.Nikhil Bansal, Avrim Blum, and Shuchi Chawla.
2004.Correlation clustering.
Machine Learning, 56(1-3):89?113.Gosse Bouma, Sergio Duarte, and Zahurul Islam.2009.
Cross-lingual alignment and completion ofWikipedia templates.
In CLIAWS3 ?09: Proceed-ings of the Third International Workshop on CrossLingual Information Access, pages 21?29, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Moses Charikar, Venkatesan Guruswami, and AnthonyWirth.
2005.
Clustering with qualitative informa-tion.
Journal of Computer and System Sciences,71(3):360?383.Shuchi Chawla, Robert Krauthgamer, Ravi Kumar, Yu-val Rabani, and D. Sivakumar.
2005.
On the hard-ness of approximating multicut and sparsest-cut.
InIn Proceedings of the 20th Annual IEEE Conferenceon Computational Complexity, pages 144?153.Inderjit S. Dhillon, Yuqiang Guan, and Brian Kulis.2007.
Weighted graph cuts without eigenvectors.a multilevel approach.
IEEE Trans.
Pattern Anal.Mach.
Intell., 29(11):1944?1957.Sergio Ferra?ndez, Antonio Toral, O?scar Ferra?ndez, An-tonio Ferra?ndez, and Rafael Mun?oz.
2007.
Ap-plying Wikipedia?s multilingual knowledge to cross-lingual question answering.
In NLDB, pages 352?363.Naveen Garg, Vijay V. Vazirani, and Mihalis Yan-nakakis.
1996.
Approximate max-flow min-(multi)cut theorems and their applications.
SIAMJournal on Computing (SICOMP), 25:698?707.Narendra Karmarkar.
1984.
A new polynomial-timealgorithm for linear programming.
In STOC ?84:Proceedings of the 16th Annual ACM Symposium onTheory of Computing, pages 302?311, New York,NY, USA.
ACM.George Karypis and Vipin Kumar.
1998.
A fast andhigh quality multilevel scheme for partitioning irreg-ular graphs.
SIAM Journal on Scientific Computing,20(1):359?392.Subhash Khot.
2002.
On the power of unique 2-prover1-round games.
In STOC ?02: Proceedings of the34th Annual ACM Symposium on Theory of Com-puting, pages 767?775, New York, NY, USA.
ACM.Tom Leighton and Satish Rao.
1999.
Multicommoditymax-flow min-cut theorems and their use in design-ing approximation algorithms.
Journal of the ACM,46(6):787?832.Rada Mihalcea and Andras Csomai.
2007.
Wikify!
:Linking documents to encyclopedic knowledge.
InProceedings of the 16th ACM Conference on Infor-mation and Knowledge Management (CIKM 2007),pages 233?242, New York, NY, USA.
ACM.D.
Nguyen, A. Overwijk, C. Hauff, R.B.
Trieschnigg,D.
Hiemstra, and F.M.G.
Jong de.
2009.
Wiki-Translate: query translation for cross-lingual infor-mation retrieval using only Wikipedia.
In CarolPeters, Thomas Deselaers, Nicola Ferro, and JulioGonzalo, editors, Evaluating Systems for Multilin-gual and Multimodal Information Access, LectureNotes in Computer Science 5706, pages 58?65.Jeff Pasternack and Dan Roth.
2009.
Learning bet-ter transliterations.
In CIKM ?09: Proceeding of the18th ACM Conference on Information and Knowl-edge Management, pages 177?186, New York, NY,USA.
ACM.Simone Paolo Ponzetto and Michael Strube.
2007.
De-riving a large scale taxonomy from Wikipedia.
InAAAI 2007: Proceedings of the 22nd Conferenceon Artificial Intelligence, pages 1440?1445.
AAAIPress.Carina Silberer, Wolodja Wentland, Johannes Knopp,and Matthias Hartung.
2008.
Building a multilin-gual lexical resource for named entity disambigua-tion, translation and transliteration.
In European,editor, Proceedings of the Sixth International Lan-guage Resources and Evaluation (LREC?08), Mar-rakech, Morocco.Philipp Sorg and Philipp Cimiano.
2008.
Enrich-ing the crosslingual link structure of Wikipedia - aclassification-based approach.
In Proceedings of theAAAI 2008 Workshop on Wikipedia and Artifical In-telligence.Fabian M. Suchanek, Gjergji Kasneci, and GerhardWeikum.
2007.
Yago: A Core of Semantic Knowl-edge.
In Proceedings of the 16th InternationalWorld Wide Web conference, WWW, New York, NY,USA.
ACM Press.Zhiping Zeng, Anthony K. H. Tung, Jianyong Wang,Jianhua Feng, and Lizhu Zhou.
2009.
Comparingstars: On approximating graph edit distance.
Pro-ceedings of the VLDB Endowment, 2(1):25?36.853
