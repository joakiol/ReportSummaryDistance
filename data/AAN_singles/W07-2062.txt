Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 288?291,Prague, June 2007. c?2007 Association for Computational LinguisticsRTV: Tree Kernels for Thematic Role ClassificationDaniele PighinFBK-irst; University of Trento, DITpighin@itc.itAlessandro MoschittiUniversity of Trento, DITmoschitti@dit.unitn.itRoberto BasiliUniversity of Rome Tor Vergata, DISPbasili@info.uniroma2.itAbstractWe present a simple, two-steps supervisedstrategy for the identification and classifica-tion of thematic roles in natural languagetexts.
We employ no external source of in-formation but automatic parse trees of the in-put sentences.
We use a few attribute-valuefeatures and tree kernel functions applied tospecialized structured features.
The result-ing system has an F1 of 75.44 on the Se-mEval2007 closed task on semantic role la-beling.1 IntroductionIn this paper we present a system for the labelingof semantic roles that produces VerbNet (Kipper etal., 2000) like annotations of free text sentences us-ing only full syntactic parses of the input sentences.The labeling process is modeled as a cascade of twodistinct classification steps: (1) boundary detection(BD), in which the word sequences that encode athematic role for a given predicate are recognized,and (2) role classification (RC), in which the typeof thematic role with respect to the predicate is as-signed.
After role classification, a set of simpleheuristics are applied in order to ensure that onlywell formed annotations are output.We designed our system on a per-predicate basis,training one boundary classifier and a battery of roleclassifiers for each predicate word.
We clustered allthe senses of the same verb together and ended upwith 50 distinct boundary classifiers (one for eachtarget predicate word) and 619 role classifiers to rec-ognize the 47 distinct role labels that appear in thetraining set.The remainder of this paper is structured as fol-lows: Section 2 describes in some detail the archi-tecture of our labeling system; Section 3 describesthe features that we use to represent the classifierexamples; Section 4 describes the experimental set-ting and reports the accuracy of the system on theSemEval2007 semantic role labeling closed task; fi-nally, Section 5 discusses the results and presentsour conclusions.2 System DescriptionGiven a target predicate word in a natural languagesentence, a SRL system is meant to correctly iden-tify all the arguments of the predicate.
This problemis usually divided in two sub-tasks: (a) the detectionof the boundaries (i. e. the word span) of each argu-ment and (b) the classification of the argument type,e.g.
Arg0 or ArgM in PropBank or Agent and Goalin FrameNet or VerbNet.The standard approach to learn both the detectionand the classification of predicate arguments is sum-marized by the following steps:1 Given a sentence from the training-set, gener-ate a full syntactic parse-tree;2 let P and A be the set of predicates and theset of parse-tree nodes (i.e.
the potential argu-ments), respectively;3 for each pair ?p, a?
?
P ?A:3.1 extract the feature representation set, Fp,a;3.2 if the sub-tree rooted in a covers exactly thewords of one argument of p, put Fp,a in T+(positive examples), otherwise put it in T?
(negative examples).For instance, in Figure 1.a, for each combinationof the predicate approve with any other tree node a288that does not overlap with the predicate, a classifierexample Fapprove,a is generated.
If a exactly coversone of the predicate arguments (in this case: ?Thecharter?, ?by the EC Commission?
or ?on Sept.
21?
)it is regarded as a positive instance, otherwise it willbe a negative one, e. g. Fapprove,(NN charter).The T+ and T?
sets are used to train the bound-ary classifier.
To train the role multi-class classifier,T+ can be reorganized as positive T+argi and nega-tive T?argi examples for each argument i.
In this way,an individual ONE-vs-ALL classifier for each argu-ment i can be trained.
We adopted this solution, ac-cording to (Pradhan et al, 2005), since it is simpleand effective.
In the classification phase, given anunseen sentence, all its Fp,a are generated and clas-sified by each individual role classifier.
The role la-bel associated with the maximum among the scoresprovided by the individual classifiers is eventuallyselected.To make the annotations consistent with the un-derlying linguistic model, we employ a few simpleheuristics to resolve the overlap situations that mayoccur, e. g. both ?charter?
and ?the charter?
in Figure1 may be assigned a role:?
if more than two nodes are involved, i. e. a noded and two or more of its descendants ni areclassified as arguments, then assume that d isnot an argument.
This choice is justified by pre-vious studies (Moschitti et al, 2006b) showingthat the accuracy of classification is higher forlower nodes;?
if only two nodes are involved, i. e. they dom-inate each other, then keep the one with thehighest classification score.3 Features for Semantic Role LabelingWe explicitly represent as attribute-value pairs thefollowing features of each Fp,a pair:?
Phrase Type, Predicate Word, Head Word, Po-sition and Voice as defined in (Gildea and Juras-fky, 2002);?
Partial Path, No Direction Path, Head WordPOS, First and Last Word/POS in Constituentand SubCategorization as proposed in (Pradhanet al, 2005);a) SNPDTTheNNcharterVPAUXwasVPVBNapprovedPPINbyNPDTtheNNPECNNPCommissionPPINonNPNNPSept.CD21..b) SNP-BDTTheNNcharterVPVPVBN-PapprovedVPVBN-PapprovedPP-BINbyNPDTtheNNPECNNPCommissionCauseExperiencer ARGM-TMPFigure 1: A sentence parse tree (a) and two example ASTm1structures relative to the predicate approve (b).Set Props T T+ T?Train 15,838 793,104 45,157 747,947Dev 1,606 75,302 4,291 71,011Train - Dev 14,232 717,802 40,866 676,936Table 1: Composition of the dataset in terms of: number ofannotations (Props); number of candidate argument nodes (T );positive (T+) and negative (T?)
boundary classifier examples.?
Syntactic Frame as designed in (Xue andPalmer, 2004).We also employ structured features derived by thefull parses in an attempt to capture relevant aspectsthat may not be emphasized by the explicit featurerepresentation.
(Moschitti et al, 2006a) and (Mos-chitti et al, 2006b) defined several classes of struc-tured features that were successfully employed withtree kernels for the different stages of an SRL pro-cess.
Figure 1 shows an example of the ASTm1 struc-tures that we used for both the boundary detectionand the role classification stages.4 ExperimentsIn this section we discuss the setup and the resultsof the experiments carried out on the dataset of theSemEval2007 closed task on SRL.289Task Kernel(s) Precision Recall F?=1BD poly 94.34% 71.26% 81.19poly + TK 92.89% 76.09% 83.65BD + RC poly 88.72% 68.76% 77.47poly + TK 86.60% 72.40% 78.86Table 2: SRL accuracy on the development test for the bound-ary detection (BD) and the complete SRL task (BD+RC) usingthe polynomial kernel alone (poly) or combined with a tree ker-nel function (poly + TK).4.1 SetupThe training set comprises 15,8381 training annota-tions organized on a per-verb basis.
In order to builda development set (Dev), we sampled about onetenth, i. e. 1,606 annotations, of the original train-ing set.
For the final evaluation on the test set (Test),consisting of 3,094 annotations, we trained our clas-sifiers on the whole training data.
Statistics on thedataset composition are shown in Table 1.The evaluations were carried out with the SVM-Light-TK2 software (Moschitti, 2004) which ex-tends the SVM-Light package (Joachims, 1999)with tree kernel functions.
We used the defaultpolynomial kernel (degree=3) for the linear featuresand a SubSet Tree (SST) kernel (Collins and Duffy,2002) for the comparison of ASTm1 structured fea-tures.
The kernels are normalized and summed byassigning a weight of 0.3 to the TK contribution.Training all the 50 boundary classifiers and the619 role classifiers on the whole dataset took about4 hours on a 64 bits machine (2.2GHz, 1GB RAM)3.4.2 EvaluationAll the evaluations were carried out usingthe CoNLL2005 evaluator tool available athttp://www.lsi.upc.es/?srlconll/soft.html.Table 2 shows the aggregate results on boundarydetection (BD) and the complete SRL task (BD+RC)on the development set using the polynomial kernelalone (poly) or in conjunction with the tree kernelsand structured features (poly+TK).
For both tasks,tree kernel functions do trigger automatic feature se-1A bunch of unaligned annotations were removed from thedataset.2http://ai-nlp.info.uniroma2.it/moschitti/3In order to have a faster development cycle, we only used60k training examples to train the boundary classifier of the verbsay.
The accuracy on this relation is still very high, as we mea-sured an overall F1 of 87.18 on the development set and of 85.13on the test set.Role #TI Precision Recall F?=1Ov(BD) 6931 87.09% 72.96% 79.40Ov(BD+RC) 81.58% 70.16% 75.44ARG2 4 100.00% 25.00% 40.00ARG3 17 61.11% 64.71% 62.86ARG4 4 0.00% 0.00% 0.00ARGM-ADV 188 55.14% 31.38% 40.00ARGM-CAU 13 50.00% 23.08% 31.58ARGM-DIR 4 100.00% 25.00% 40.00ARGM-EXT 3 0.00% 0.00% 0.00ARGM-LOC 151 51.66% 51.66% 51.66ARGM-MNR 85 41.94% 15.29% 22.41ARGM-PNC 28 38.46% 17.86% 24.39ARGM-PRD 9 83.33% 55.56% 66.67ARGM-REC 1 0.00% 0.00% 0.00ARGM-TMP 386 55.65% 35.75% 43.53Actor1 12 85.71% 50.00% 63.16Actor2 1 100.00% 100.00% 100.00Agent 2551 91.38% 77.34% 83.78Asset 21 42.42% 66.67% 51.85Attribute 17 60.00% 70.59% 64.86Beneficiary 24 65.00% 54.17% 59.09Cause 48 75.56% 70.83% 73.12Experiencer 132 86.49% 72.73% 79.01Location 12 83.33% 41.67% 55.56Material 7 100.00% 14.29% 25.00Patient 37 76.67% 62.16% 68.66Patient1 20 72.73% 40.00% 51.61Predicate 181 63.75% 56.35% 59.82Product 106 70.79% 59.43% 64.62R-ARGM-LOC 2 0.00% 0.00% 0.00R-ARGM-MNR 2 0.00% 0.00% 0.00R-ARGM-TMP 4 0.00% 0.00% 0.00R-Agent 74 70.15% 63.51% 66.67R-Experiencer 5 100.00% 20.00% 33.33R-Patient 2 0.00% 0.00% 0.00R-Predicate 1 0.00% 0.00% 0.00R-Product 2 0.00% 0.00% 0.00R-Recipient 8 100.00% 87.50% 93.33R-Theme 7 75.00% 42.86% 54.55R-Theme1 7 100.00% 85.71% 92.31R-Theme2 1 50.00% 100.00% 66.67R-Topic 14 66.67% 42.86% 52.17Recipient 48 75.51% 77.08% 76.29Source 25 65.22% 60.00% 62.50Stimulus 21 33.33% 19.05% 24.24Theme 650 79.22% 68.62% 73.54Theme1 69 77.42% 69.57% 73.28Theme2 60 74.55% 68.33% 71.30Topic 1867 84.26% 82.27% 83.25Table 3: Evaluation of the semantic role labeling accuracy onthe SemEval2007 - Task 17 test set using the poly + TK kernel.Column #TI reports the number of instances of each role labelin the test set.
Rows Ov(BD) and Ov(BD + RC) show the overallaccuracy on the boundary detection and the complete SRL task,respectively.lection and improve the polynomial kernel by 2.46and 1.39 F1 points, respectively.The SRL accuracy for each one of the 47 dis-tinct role labels is shown in Table 3.
Column 2 lists290the number of instances of each role in the test set.Many roles have very few positive examples both inthe training and the test sets, and therefore have littleor no impact on the overall accuracy which is domi-nated by the few roles which are very frequent, suchas Theme, Agent, Topic and ARGM-TMP which ac-count for almost 80% of all the test roles.5 Final RemarksIn this paper we presented a system that employstree kernels and a basic set of flat features for theclassification of thematic roles.We adopted a very simple approach that is meantto be as general and fast as possible.
The issueof generality is addressed by training the bound-ary and role classifiers on a per-predicate basis andby employing tree kernel and structured features inthe learning algorithm.
The resulting architecturecan indeed be used to learn the classification ofroles of non-verbal predicates as well, and the au-tomatic feature selection triggered by the tree kernelshould compensate for the lack of ad-hoc, well es-tablished explicit features for some classes of non-verbal predicates, e. g. adverbs or prepositions.Splitting the learning problem also has the clearadvantage of noticeably improving the efficiency ofthe classifiers, thus reducing training and classifica-tion time.
On the other hand, this split results insome classifiers having too few training instancesand therefore being very inaccurate.
This is espe-cially true for the boundary classifiers, which con-versely need to be very accurate in order to posi-tively support the following stages of the SRL pro-cess.
The solution of a monolithic boundary classi-fier that we previously employed (Moschitti et al,2006b) is noticeably more accurate though muchless efficient, especially for training.
Indeed, afterthe SemEval2007 evaluation period was over, weran another experiment using a monolithic boundaryclassifier.
On the test set, we measured F1 values of82.09 vs 79.40 and 77.17 vs 75.44 for the boundarydetection and the complete SRL tasks, respectively.Although it was provided as part of both the train-ing and test data, we chose not to use the verb senseinformation.
This choice is motivated by our in-tention to depend on as less external resources aspossible in order to be able to port our SRL systemto other linguistic models and languages, for whichsuch resources may not exist.
Still, identifying thepredicate sense is a key issue especially for role clas-sification, as the argument structure of a predicate islargely determined by its sense.
In the near featurewe plan to use larger structured features, i. e. span-ning all the potential arguments of a predicate, toimprove the accuracy of our role classifiers.AcknowledgmentsThe development of the SRL system was carried outat the University of Rome Tor Vergata and financedby the EU project PrestoSpace4 (FP6-507336).ReferencesMichael Collins and Nigel Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In ACL02.Daniel Gildea and Daniel Jurasfky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistic, 28(3):496?530.T.
Joachims.
1999.
Making large-scale SVM learning practical.In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advancesin Kernel Methods - Support Vector Learning.Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000.Class-based construction of a verb lexicon.
In Proceedingsof AAAI-2000 Seventeenth National Conference on ArtificialIntelligence, Austin, TX.Alessandro Moschitti, Daniele Pighin, and Roberto Basili.2006a.
Semantic role labeling via tree kernel joint inference.In Proceedings of the Tenth Conference on ComputationalNatural Language Learning, CoNLL-X.Alessandro Moschitti, Daniele Pighin, and Roberto Basili.2006b.
Tree kernel engineering in semantic role labelingsystems.
In Proceedings of the Workshop on Learning Struc-tured Information in Natural Language Applications, EACL2006, pages 49?56, Trento, Italy, April.
European Chapterof the Association for Computational Linguistics.Alessandro Moschitti.
2004.
A study on convolution kernelfor shallow semantic parsing.
In proceedings of ACL-2004,Barcelona, Spain.Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,James H. Martin, and Daniel Jurafsky.
2005.
Support vectorlearning for semantic argument classification.
to appear inMachine Learning Journal.Nianwen Xue and Martha Palmer.
2004.
Calibrating featuresfor semantic role labeling.
In Proceedings of EMNLP 2004,pages 88?94, Barcelona, Spain, July.4http://www.prestospace.org291
