Two Step Chinese Named Entity Recognition Based on ConditionalRandom Fields ModelsYuanyong Feng*          Ruihong Huang*         Le Sun?
*Institute of Software, Graduate UniversityChinese Academy of SciencesBeijing, China, 100080{comerfeng,ruihong2}@iscas.cn?Institute of SoftwareChinese Academy of SciencesBeijing, China, 100080sunle@iscas.cnAbstractThis paper mainly describes a Chinesenamed entity recognition (NER) systemNER@ISCAS, which integrates text, part-of-speech and a small-vocabulary-character-lists feature and heristic post-process rules for MSRA NER open trackunder the framework of Conditional Ran-dom Fields (CRFs) model.1  IntroductionThe system NER@ISCAS is designed under theConditional Random Fields (CRFs.
Lafferty et al,2001) framework.
It integrates multiple featuresbased on single Chinese character or space sepa-rated ASCII words.
The early designed system(Feng et al, 2006) is used for the MSRA NERopen track this year.
The output of an externalpart-of-speech tagging tool and some carefullycollected small-scale-character-lists are used asopen knowledge.
Some post process steps are alsoapplied to complement the local limitation inmodel?s feature engineering.The remaining of this paper is organized as fol-lows.
Section 2 introduces Conditional RandomFields model.
Section 3 presents the details of oursystem on Chinese NER integrating multiple fea-tures.
Section 4 describes the post-processingsbased on some heuristic rules.
Section 5 gives theevaluation results.
We end our paper with someconclusions and future works.2  Conditional Random Fields ModelConditional random fields are undirected graphicalmodels for calculating the conditional probabilityfor output vertices based on input ones.
Whilesharing the same exponential form with maximumentropy models, they have more efficient proce-dures for complete, non-greedy finite-state infer-ence and training.Given an observation sequence o=<o1, o2, ...,oT>, linear-chain CRFs model based on the as-sumption of first order Markov chains defines thecorresponding state sequence s?
probability as fol-lows (Lafferty et al, 2001):111( | ) exp( ( , , , ))Tk k t tt kp fZ??
?== ?
?os o os s t(1)Where ?
is the model parameter set, Zo is the nor-malization factor over all state sequences, fk is anarbitrary feature function, and ?k is the learned fea-ture weight.
A feature function defines its value tobe 0 in most cases, and to be 1 in some designatedcases.
For example, the value of a feature named?MAYBE-SURNAME?
is 1 if and only if st-1 isOTHER, st is PER, and the t-th character in o is acommon-surname.The inference and training procedures of CRFscan be derived directly from those equivalences inHMM.
For instance, the forward variable ?t(si)defines the probability that state at time t being siat time t given the observation sequence o. As-sumed that we know the probabilities of eachpossible value si for the beginning state  ?0(si),then we have1( ) ( )exp( ( , , , )t i t k k is ks s f s s t?
?
?+??
?=?
?
o (2)120Sixth SIGHAN Workshop on Chinese Language ProcessingIn similar ways, we can obtain the backwardvariables and Baum-Welch algorithm.3  Chinese NER Using CRFs Model Inte-grating Multiple FeaturesBesides the text feature(TXT), simplified part-of-speech (POS) feature, and small-vocabulary-character-lists (SVCL) feature, which use in theearly system (Feng et al, 2006), some new fea-tures such as word boundary, adjoining state bi-gram ?
observation and early NE output are alsocombined under the unified CRFs framework.The text feature includes single Chinese charac-ter, some continuous digits or letters.POS feature is an important feature which car-ries some syntactic information.
Unlike those inthe earyly system, the POS tag set are merged into9 categories from the criterion of modern Chinesecorpora construction (Yu, 1999), which contains39 tags.The third type of features are derived from thesmall-vocabulary-character lists which are essen-tially same as the ones used in last year exceptwith some additional items.
Some examples of thislist are given in Table 1.Value Description Examplesdigit Arabic digit(s) 1,2,3letter Letter(s) A,B,C,...,a, b, cContinuous digits and/or letters (The sequence isregarded as a single token)chseq Chinese order 1 ?
?
?
?, , ,chdigit Chinese digit ?
?
?, ,tianseq Chinese order 2 ?
?
?
?, , ,chsurn Surname ?
?
?
?, , ,notname Not name ?
?
?
?
?
?, , , , ,loctch LOC tail charac-ter?
?
?
?
?, , , , ,?
?,orgtch ORG tail charac- ?
?
?
?
?, , , , ,ter ?, ?other Other case ?
?, ?, ,   ?, ?Table 1.
Some Examples of SVCL FeatureThe fourth type of feature is word boundary.
Weuse the B, I, E, U, and O to indicate Begining, In-ner, Ending, and Uniq part of, or outside of a wordgiven a word segmentation.
The O case occurswhen a token, for example the charactor ?&?, isignored by the segmentator.
We do not combinethe boundary information with other features be-cause we argue it is very limited and may causeerrors.The last type of features is bigram state com-bined with observations.
We argue that observa-toin (mainly is of named entity derived by earlysystem or character text itself) and state transitionare not conditionally independent and entails dedi-cate considerings.Each token is presented by its feature vector,which is combined by these features we just dis-cussed.
Once all token feature (Maybe includingcontext features) values are determined, an obser-vation sequence is feed into the model.Each token state is a combination of the type ofthe named entity it belongs to and the boundarytype it locates within.
The entity types are personname (PER), location name (LOC), organizationname (ORG), date expression (DAT), time expres-sion (TIM), numeric expression (NUM), and notnamed entity (OTH).
The boundary types are sim-ply Beginning, Inside, and Outside (BIO).All above types of features are extracted from avarying length window.
The main criteria is thatwider window with smaller feature space and nar-row window when the observation features are in alarge range.The main feature set is shown the following.Character Texts(TXT):TXT-2, TXT-1, TXT0, TXT1, TXT2,TXT-1TXT0,  TXT1TXT0, TXT1TXT2simplified part-of-speech (POS):unigram: POS-4 ~ POS4121Sixth SIGHAN Workshop on Chinese Language Processingsmall-vocabulary-character-lists (SVCL):unigram: SVCL-2 ~ SVCL7bigram:SVCL0SVCL1, SVCL1SVCL2Word Boundary (WB):WB-1,WB0,WB1Named Entity (NE):unigram: NE-4 ~ NE4bigram:NE-2NE-1, NE-1NE0, NE0NE1, NE1NE2State Bigram (B) ?
Observation:B, B-TXT0, B-NE-1, B-NE0, B-NE1Table 2.
The Main Feature Set4  Post Processing on Heuristic RulesObserving from the evaluation, our model hasworse performance on ORG and PER than LOC.Furthermore, the analysis of the errors tells us thatthey are hard to be tackled with the improvementof the model itself.
Therefore, we decided to dosome post-process to correct certain types of tag-ging errors of the unified model mainly concerningthe two kinds of entities, ORG and PER.At the training phrase, we compare the taggingoutput of the model with the correct tags and col-lect the falsely tagged instances.
To identify therules used in the post-process, we categorize theerrors into several types, discriminate the typesand encode them into the rules according to twoprinciples:1) the rules are applied on the tagged sequencesoutput by the unified model.2) The rules applied shouldn?t introduce moreother errors.As a result, we have extracted eight rules, sevenfor ORG, one for PER.
Generally, the rules workonly on the local context of the examined tags,they correct some type of error by changing sometags when seeing certain pattern of context beforeor after the current tags in a limited distance.
Wewant to give one rule as one example to explainthe way they function.Example: {<LOC>}+<ORG> ==> <ORG>,After this rule is applied, one or more locationsfollowed by a organization name will be taggedORG.
This is the case where there are a locationname in a organization name.
Besides, we can seesince the location and latter part of the organiza-tion name are tagged separately in the unifiedmodel, we may only resort to the post-process toget the right government boundary.5  Evaluation5.1  ResultsThe evaluations in training phrase tell us thepost-process can improve the performance by onepercent.
We are satisfied since we just appliedeight rules.The formal eveluation results of our system areshown in Table 3.R P FOverall 86.74 90.03 88.36PER 90.83 92.16 91.49LOC 89.89 91.66 90.77ORG 77.99 85.16 81.41Table 3.
Formal Results on MSRA NER Open5.2  Errors  from NER TrackThe NER errors in our system are mainly of asfollows:z AbbreviationsAbbreviations are very common among the er-rors.
Among them, a significant part of abbrevia-tions are mentioned before their corresponding fullnames.
Some common abbreviations has no corre-sponding full names appeared in document.
Hereare some examples:R1: ??[????
LOC]????
?K: [??????
LOC]????
?R: [?
?
LOC]?
?K: [?
LOC][?
LOC]?
?In current system, the recognition is fully de-pended on the linear-chain CRFs model, which isheavily based on local window observation fea-tures; no abbreviation list or special abbreviation1 R stands for system response, K for key.122Sixth SIGHAN Workshop on Chinese Language Processingrecognition involved.
Because lack of constraintchecking on distant entity mentions, the systemfails to catch the interaction among similar textfragments cross sentences.z Concatenated NamesFor many reasons, Chinese names in titles andsome sentences, especially in news, are not sepa-rated.
The system often fails to judge the rightboundaries and the reasonable type classification.For example:R:?[????
LOC]?[????
PER]??K:?[????
PER]?[????
PER]?
?z HintsThough it helps to recognize an entity at mostcases, the small-vocabulary-list hint feature mayrecommend a wrong decision sometimes.
For in-stance, common surname character ???
in thefollowing sentence is wrongly labeled when noword segmentation information given:R:[??
LOC]?[?
????????
PER]K:[??
LOC]?
?[????????
PER]Other errors of this type may result from failingto identify verbs and prepositions, such as:R:???????????
?????K:[???????????
ORG]????
?6  Conclusions and Future WorkWe mainly described a Chinese named entity rec-ognition system NER@ISCAS, which integratestext, part-of-speech and a small-vocabulary-character-lists feature for MSRA NER open trackunder the framework of Conditional RandomFields (CRFs) model.
Although it provides a uni-fied framework to integrate multiple flexible fea-tures, and to achieve global optimization on inputtext sequence, the popular linear chained Condi-tional Random Fields model often fails to catchsemantic relations among reoccurred mentions andadjoining entities in a catenation structure.The situations containing exact reoccurrenceand shortened occurrence enlighten us to takemore effort on feature engineering or post process-ing on abbreviations / recurrence recognition.Another effort may be poured on the commonpatterns, such as paraphrase, counting, and con-straints on Chinese person name lengths.From current point of view, enriching the hintlists is also desirable.AcknowledgementsThis work is partially supported by National Natural ScienceFoundation of China under grant #60773027, #60736044 andby ?863?
Key Projects #2006AA010108.ReferencesChinese 863 program.
2005.
Results on NamedEntity Recognition.
The 2004HTRDP ChineseInformation Processing and Intelligent Human-Machine Interface Technology Evaluation.Yuanyong Feng, Le Sun, Yuanhua Lv.
2006.Chinese Word Segmentation and Named EntityRecognition Based on Conditional RandomFields Models.
Proceedings of SIGHAN-2006,Fifth SIGHAN Workshop on Chinese LanguageProcessing, Sydney, Australia,.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional Random Fields:Probabilistic Models for Segmenting and Label-ing Sequence Data.
ICML.Shiwen Yu.
1999.
Manual on Modern ChineseCorpora Construction.
Institute of Computa-tional Language, Peking Unversity.
Beijing.123Sixth SIGHAN Workshop on Chinese Language Processing
