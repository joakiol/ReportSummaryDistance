Coling 2010: Poster Volume, pages 997?1005,Beijing, August 2010An Evaluation Framework for Plagiarism DetectionMartin Potthast Benno SteinWeb Technology & Information SystemsBauhaus-Universit?t Weimar{martin.potthast, benno.stein}@uni-weimar.deAlberto Barr?n-Cede?o Paolo RossoNatural Language Engineering Lab?ELiRFUniversidad Polit?cnica de Valencia{lbarron, prosso}@dsic.upv.esAbstractWe present an evaluation framework forplagiarism detection.1 The frameworkprovides performance measures that ad-dress the specifics of plagiarism detec-tion, and the PAN-PC-10 corpus, whichcontains 64 558 artificial and 4 000 sim-ulated plagiarism cases, the latter gener-ated via Amazon?s Mechanical Turk.
Wediscuss the construction principles behindthe measures and the corpus, and we com-pare the quality of our corpus to exist-ing corpora.
Our analysis gives empiricalevidence that the construction of tailoredtraining corpora for plagiarism detectioncan be automated, and hence be done on alarge scale.1 IntroductionThe lack of an evaluation framework is a seri-ous problem for every empirical research field.In the case of plagiarism detection this short-coming has recently been addressed for the firsttime in the context of our benchmarking work-shop PAN [15, 16].
This paper presents the eval-uation framework developed in the course of theworkshop.
But before going into details, we sur-vey the state of the art in evaluating plagiarism de-tection, which has not been studied systematicallyuntil now.1.1 A Survey of Evaluation MethodsWe have queried academic databases and searchengines to get an overview of all kinds of con-tributions to automatic plagiarism detection.
Al-together 275 papers were retrieved, from which139 deal with plagiarism detection in text,1The framework is available free of charge athttp://www.webis.de/research/corpora.Table 1: Summary of the plagiarism detectionevaluations in 205 papers, from which 104 dealwith text and 101 deal with code.Evaluation Aspect Text CodeExperiment Tasklocal collection 80% 95%Web retrieval 15% 0%other 5% 5%Performance Measureprecision, recall 43% 18%manual, similarity 35% 69%runtime only 15% 1%other 7% 12%Comparisonnone 46% 51%parameter settings 19% 9%other algorithms 35% 40%Evaluation Aspect Text CodeCorpus Acquisitionexisting corpus 20% 18%homemade corpus 80% 82%Corpus Size [# documents][1, 10) 11% 10%[10, 102) 19% 30%[102, 103) 38% 33%[103, 104) 8% 11%[104, 105) 16% 4%[105, 106) 8% 0%123 deal with plagiarism detection in code, and13 deal with other media types.
From the pa-pers related to text and code we analyzed the205 which present evaluations.
Our analysiscovers the following aspects: experiment tasks,performance measures, underlying corpora, and,whether comparisons to other plagiarism detec-tion approaches were conducted.
Table 1 summa-rizes our findings.With respect to the experiment tasks the ma-jority of the approaches perform overlap detec-tion by exhaustive comparison against some lo-cally stored document collection?albeit a Webretrieval scenario is more realistic.
We explainthis shortcoming by the facts that the Web can-not be utilized easily as a corpus, and, that in thecase of code plagiarism the focus is on collusiondetection in student courseworks.
With respect toperformance measures the picture is less clear: amanual result evaluation based on similarity mea-sures is used about the same number of times fortext (35%), and even more often for code (69%),as an automatic computation of precision and re-call.
21% and 13% of the evaluations on text andcode use custom measures or examine only the de-997tection runtime.
This indicates that precision andrecall may not be well-defined in the context ofplagiarism detection.
Moreover, comparisons toexisting research are conducted in less than halfof the papers, a fact that underlines the lack of anevaluation framework.The right-hand side of Table 1 overviews twocorpus-related aspects: the use of existing cor-pora versus the use of handmade corpora, and thesize distribution of the used corpora.
In particu-lar, we found that researchers follow two strate-gies to compile a corpus.
Small corpora (<1 000documents) are built from student courseworks orfrom arbitrary documents into which plagiarism-alike overlap is manually inserted.
Large corpora(>1 000 documents) are collected from sourceswhere overlap occurs more frequently, such asrewritten versions of news wire articles, or fromconsecutive versions of open source software.
Al-together, we see a need for an open, commonlyused plagiarism detection corpus.1.2 Related WorkThere are a few surveys about automatic plagia-rism detection in text [7, 8, 14] and in code [12,17, 19, 20].
These papers, as well as nearly allpapers of our survey, omit a discussion of evalua-tion methodologies; the following 4 papers are anexception.In [21] the authors introduce graph-based per-formance measures for code plagiarism detectionthat are intended for unsupervised evaluations.We argue that evaluations in this field should bedone in a supervised manner.
An aside: the pro-posed measures have not been adopted since theirfirst publication.
In [15] we introduce preliminaryparts of our framework.
However, the focus ofthat paper is less on methodology but on the com-parison of the detection approaches that were sub-mitted to the first PAN benchmarking workshop.In [9, 10] the authors report on an unnamed cor-pus that comprises 57 cases of simulated plagia-rism.
We refer to this corpus as the Clough09 cor-pus; a comparison to our approach is given lateron.
Finally, a kind of related corpus is the ME-TER corpus, which has been the only alternativefor the text domain up to now [11].
It comprises445 cases of text reuse among 1 716 news articles.Although the corpus can be used to evaluate pla-giarism detection its design does not support thistask.
This is maybe the reason why it has not beenused more often.
Furthermore, it is an open ques-tion whether or not cases of news reuse differ fromplagiarism cases where the plagiarists strive to re-main undetected.1.3 ContributionsBesides the above survey, the contributions of ourpaper are threefold: Section 2 presents formalfoundations for the evaluation of plagiarism detec-tion and introduces three performance measures.Section 3 introduces methods to create artificialand simulated plagiarism cases on a large scale,and the PAN-PC-10 corpus in which these meth-ods have been operationalized.
Section 4 thencompares our corpus with the Clough09 corpusand the METER corpus.
The comparison revealsimportant insights for the different kinds of textreuse in these corpora.2 Plagiarism Detection PerformanceThis section introduces measures to quantify theprecision and recall performance of a plagiarismdetection algorithm; we present a micro-averagedand a macro-averaged variant.
Moreover, the so-called detection granularity is introduced, whichquantifies whether the contiguity between plagia-rized text passages is properly recognized.
Thisconcept is important: a low granularity simpli-fies both the human inspection of algorithmicallydetected passages as well as an algorithmic styleanalysis within a potential post-process.
The threemeasures can be applied in isolation but alsobe combined into a single, overall performancescore.
A reference implementation of the perfor-mance measures is distributed with our corpus.2.1 Precision, Recall, and GranularityLet dplg denote a document that contains pla-giarism.
A plagiarism case in dplg is a 4-tuples = ?splg, dplg, ssrc, dsrc?, where splg is a plagia-rized passage in dplg, and ssrc is its original coun-terpart in some source document dsrc.
Likewise,a plagiarism detection for document dplg is de-noted as r = ?rplg, dplg, rsrc, d?src?
; r associatesan allegedly plagiarized passage rplg in dplg with998a passage rsrc in d?src.
We say that r detects s iffrplg ?
splg = ?, rsrc ?
ssrc = ?, and d?src = dsrc.With regard to a plagiarized document dplg it is as-sumed that different plagiarized passages of dplgdo not intersect; with regard to detections for dplgno such restriction applies.
Finally, S and R de-note sets of plagiarism cases and detections.While the above 4-tuples resemble an intu-itive view of plagiarism detection we resort toan equivalent, more concise view to simplify thesubsequent notations: a document d is repre-sented as a set of references to its characters d ={(1, d), .
.
.
, (|d|, d)}, where (i, d) refers to thei-th character in d. A plagiarism case s can then berepresented as s = splg ?
ssrc, where splg ?
dplgand ssrc ?
dsrc.
The characters referred to in splgand ssrc form the passages splg and ssrc.
Likewise,a detection r can be represented as r = rplg?rsrc.It follows that r detects s iff rplg ?
splg = ?
andrsrc?ssrc = ?.
Based on these representations, themicro-averaged precision and recall of R under Sare defined as follows:precmicro(S,R) =|?(s,r)?
(S?R)(s 	 r)||?r?R r|, (1)recmicro(S,R) =|?(s,r)?
(S?R)(s 	 r)||?s?S s|, (2)where s 	 r ={s ?
r if r detects s,?
otherwise.The macro-averaged precision and recall areunaffected by the length of a plagiarism case; theyare defined as follows:precmacro(S,R) =1|R|?r?R|?s?S(s 	 r)||r| , (3)recmacro(S,R) =1|S|?s?S|?r?R(s 	 r)||s| , (4)Besides precision and recall there is anotherconcept that characterizes the power of a detec-tion algorithm, namely, whether a plagiarism cases ?
S is detected as a whole or in several pieces.The latter can be observed in today?s commercialplagiarism detectors, and the user is left to com-bine these pieces to a consistent approximationof s. Ideally, an algorithm should report detec-tions R in a one-to-one manner to the true cases S.To capture this characteristic we define the detec-tion granularity of R under S:gran(S,R) = 1|SR|?s?SR|Rs|, (5)where SR ?
S are cases detected by detectionsin R, and Rs ?
R are the detections of a given s:SR = {s | s ?
S ?
?r ?
R : r detects s},Rs = {r | r ?
R ?
r detects s}.The domain of gran(S,R) is [1, |R|], with 1indicating the desired one-to-one correspondenceand |R| indicating the worst case, where a singles ?
S is detected over and over again.Precision, recall, and granularity allow for apartial ordering among plagiarism detection algo-rithms.
To obtain an absolute order they must becombined to an overall score:plagdet(S,R) = F?log2(1 + gran(S,R)), (6)where F?
denotes the F?-Measure, i.e., theweighted harmonic mean of precision and recall.We suggest using ?
= 1 (precision and recallequally weighted) since there is currently no indi-cation that either of the two is more important.
Wetake the logarithm of the granularity to decreaseits impact on the overall score.2.2 DiscussionPlagiarism detection is both a retrieval task andan extraction task.
In light of this fact not onlyretrieval performance but also extraction accuracybecomes important, the latter of which being ne-glected in the literature.
Our measures incorpo-rate both.
Another design objective of our mea-sures is the minimization of restrictions imposedon plagiarism detectors.
The overlap restrictionfor plagiarism cases within a document assumesthat a certain plagiarized passage is unlikely tohave more than one source.
Imprecision or lackof evidence, however, may cause humans or algo-rithms to report overlapping detections, e.g., whenbeing unsure about the true source of a plagia-rized passage.
The measures (1)-(4) provide for asensible treatment of this fact since the set-based999passage representations eliminate duplicate detec-tions of characters.
The macro-averaged vari-ants allot equal weight to each plagiarism case,regardless of its length.
Conversely, the micro-averaged variants favor the detection of long pla-giarism passages, which are generally easier to bedetected.
Which of both is to be preferred, how-ever, is still an open question.3 Plagiarism Corpus ConstructionThis section organizes and analyzes the practicesthat are employed?most of the time implicitly?for the construction of plagiarism corpora.
Weintroduce three levels of plagiarism authentic-ity, namely, real plagiarism, simulated plagiarism,and artificial plagiarism.
It turns out that simu-lated plagiarism and artificial plagiarism are theonly viable alternatives for corpus construction.We propose a new approach to scale up the gen-eration of simulated plagiarism based on crowd-sourcing, and heuristics to generate artificial pla-giarism.
Moreover, based on these methods, wecompile the PAN plagiarism corpus 2010 (PAN-PC-10) which is the first corpus of its kind thatcontains both a large number and a high diversityof artificial and simulated plagiarism cases.3.1 Real, Simulated, and Artificial PlagiarismSyntactically, a plagiarism case is the result ofcopying a passage ssrc from a source documentinto another document dplg.
Since verbatimcopies can be detected easily, plagiarists oftenrewrite ssrc to obfuscate their illegitimate act.This behavior must be modeled when constructinga training corpus for plagiarism detection, whichcan be done at three levels of authenticity.
Ide-ally, one would secretly observe a large numberof plagiarists and use their real plagiarism cases;at least, one could resort to plagiarism cases whichhave been detected in the past.
The following as-pects object against this approach:?
The distribution of detected real plagiarismis skewed towards ease of detectability.?
The acquisition of real plagiarism is expen-sive since it is often concealed.?
Publishing real cases requires the consentsfrom the plagiarist and the original author.?
A public corpus with real cases is question-able from an ethical and legal viewpoint.?
The anonymization of real plagiarism is dif-ficult due to Web search engines and author-ship attribution technology.It is hence more practical to let people createplagiarism cases by ?purposeful?
modifications,or to tap resources that contain similar kinds oftext reuse.
We subsume these strategies under theterm simulated plagiarism.
The first strategy hasoften been applied in the past, though on a smallscale and without a public release of the corpora;the second strategy comes in the form of the ME-TER corpus [11].
Note that, from a psycholog-ical viewpoint, people who simulate plagiarismact under a different mental attitude than plagia-rists.
From a linguistic viewpoint, however, it isunclear whether real plagiarism differs from sim-ulated plagiarism.A third possibility is to generate plagiarism al-gorithmically [6, 15, 18], which we call artificialplagiarism.
Generating artificial plagiarism casesis a non-trivial task if one requires semantic equiv-alence between a source passage ssrc and the pas-sage splg that is obtained by an automatic obfus-cation of ssrc.
Such semantics-preserving algo-rithms are still in their infancy; however, the sim-ilarity computation between texts is usually doneon the basis of document models like the bag ofwords model and not on the basis of the originaltext, which makes obfuscation amenable to sim-pler approaches.3.2 Creating Simulated PlagiarismOur approach to scale up the creation of simu-lated plagiarism is based on Amazon?s Mechani-cal Turk, AMT, a commercial crowdsourcing ser-vice [3].
This service has gathered considerableinterest, among others to recreate TREC assess-ments [1], but also to write and translate texts [2].We offered the following task on the Mechani-cal Turk platform: Rewrite the original text foundbelow [on the task Web page] so that the rewrittenversion has the same meaning as the original, butwith a different wording and phrasing.
Imagine ascholar copying a friend?s homework just beforeclass, or imagine a plagiarist willing to use the1000Table 2: Summary of 4 000 Mechanical Turk taskscompleted by 907 workers.Worker DemographicsAge Education18, 19 10% HS 11%20?29 37% College 30%30?39 16% BSc.
17%40?49 7% MSc.
11%50?59 4% Dr. 2%60?69 1%n/a 25% n/a 29%Native Speaker Genderyes 62% male 37%no 14% female 39%n/a 23% n/a 24%Prof.
Writer Plagiarizedyes 10% yes 16%no 66% no 60%n/a 24% n/a 25%Task StatisticsTasks per Workeraverage 15std.
deviation 20minimum 1maximum 103Work Time (minutes)average 14std.
deviation 21minimum 1maximum 180Compensationpay per task 0.5 US$rejected results 25%original text without proper citation.Workers were required to be fluent in Englishreading and writing, and they were informed thatevery result was to be reviewed.
A questionnairedisplayed alongside the task description askedabout the worker?s age, education, gender, and na-tive speaking ability.
Further we asked whetherthe worker is a professional writer, and whetherhe or she has ever plagiarized.
Completing thequestionnaire was optional in order to minimizefalse answers, but still, these numbers have tobe taken with a grain of salt: the MechanicalTurk is not the best environment for such sur-veys.
Table 2 overviews the worker demographicsand task statistics.
The average worker appearsto be a well-educated male or female in the twen-ties, whose mother tongue is English.
16% of theworkers claim to have plagiarized at least once,and if at least the order of magnitude of the lat-ter number can be taken seriously this shows thatplagiarism is a prevalent problem.A number of pilot experiments were conductedto determine the pay per task, depending on thetext length and the task completion time: for50 US-cents about 500 words get rewritten inabout half an hour.
We observed that decreasingor increasing the pay per task has proportional ef-fect on the task completion time, but not on theresult quality.
This observation is in concordancewith earlier research [13].
Table 3 contrasts asource passage ssrc and its rewritten, plagiarizedpassage splg obtained via the Mechanical Turk.3.3 Creating Artificial PlagiarismTo create artificial plagiarism, we propose threeobfuscation strategies.
Given a source passagessrc a plagiarized passage splg can be created asfollows (see Table 4):?
Random Text Operations.
splg is createdfrom ssrc by shuffling, removing, inserting,or replacing words or short phrases at ran-dom.
Insertions and replacements are takenfrom the document dplg where splg is to beinserted.?
Semantic Word Variation.
splg is createdfrom ssrc by replacing words by one of theirsynonyms, antonyms, hyponyms, or hyper-nyms, chosen at random.
A word is kept ifnone of them is available.Table 3: Example of a simulated plagiarism case s, generated with Mechanical Turk.Source Passage ssrc Plagiarized Passage splgThe emigrants who sailed with Gilbert were better fitted for acrusade than a colony, and, disappointed at not at once find-ing mines of gold and silver, many deserted; and soon therewere not enough sailors to man all the four ships.
Accord-ingly, the Swallow was sent back to England with the sick;and with the remainder of the fleet, well supplied at St. John?swith fish and other necessaries, Gilbert (August 20) sailedsouth as far as forty-four degrees north latitude.
Off SableIsland a storm assailed them, and the largest of the ves-sels, called the Delight, carrying most of the provisions, wasdriven on a rock and went to pieces.
[Excerpt from ?Abraham Lincoln: A History?
by John Nicolay and John Hay.
]The people who left their countries and sailed with Gilbertwere more suited for fighting the crusades than for leading asettled life in the colonies.
They were bitterly disappointed asit was not the America that they had expected.
Since they didnot immediately find gold and silver mines, many deserted.At one stage, there were not even enough man to help sailthe four ships.
So the Swallow was sent back to Englandcarrying the sick.
The other fleet was supplied with fish andthe other necessities from St. John.
On August 20, Gilberthad sailed as far as forty-four degrees to the north latitude.His ship known as the Delight, which bore all the requiredsupplies, was attacked by a violent storm near Sable Island.The storm had driven it into a rock shattering it into pieces.1001Table 4: Examples of the obfuscation strategies.Obfuscation ExamplesOriginal TextThe quick brown fox jumps over the lazy dog.Manual Obfuscation (by a human)Over the dog which is lazy jumps quickly the fox which is brown.Dogs are lazy which is why brown foxes quickly jump over them.A fast auburn vulpine hops over an idle canine.Random Text Operationsover The.
the quick lazy dog <context word> jumps brown foxover jumps quick brown fox The lazy.
thebrown jumps the.
quick dog The lazy fox overSemantic Word VariationThe quick brown dodger leaps over the lazy canine.The quick brown canine jumps over the lazy canine.The quick brown vixen leaps over the lazy puppy.POS-preserving Word ShufflingThe brown lazy fox jumps over the quick dog.The lazy quick dog jumps over the brown fox.The brown lazy dog jumps over the quick fox.?
POS-preserving Word Shuffling.
The se-quence of parts of speech in ssrc is deter-mined and splg is created by shuffling wordsat random while retaining the original POSsequence.To generate different degrees of obfuscation thestrategies can be adjusted by varying the numberof operations made on ssrc, and by limiting therange of affected phrases within ssrc.
For our cor-pus, the strategies were combined and adjusted tomatch an intuitive understanding of a ?low?
anda ?high?
obfuscation.
Of course other obfusca-tion strategies are conceivable, e.g., based on au-tomatic paraphrasing methods [4], but for perfor-mance reasons simple strategies are preferred atthe expense of readability of the obfuscated text.3.4 Overview of the PAN-PC-10To compile the PAN plagiarism corpus 2010, sev-eral other parameters besides the above plagiarismobfuscation methods have been varied.
Table 5gives an overview.The documents used in the corpus are derivedfrom books from the Project Gutenberg.2 Everydocument in the corpus serves one of two pur-poses: it is either used as a source for plagiarismor as a document suspicious of plagiarism.
Thelatter documents divide into documents that actu-ally contain plagiarism and documents that don?t.2http://www.gutenberg.orgTable 5: Corpus statistics of the PAN-PC-10 forits 27 073 documents and 68 558 plagiarism cases.Document StatisticsDocument Purposesource documents 50%suspicious documents?
with plagiarism 25%?
w/o plagiarism 25%Intended Algorithmsexternal detection 70%intrinsic detection 30%Plagiarism per Documenthardly (5%-20%) 45%medium (20%-50%) 15%much (50%-80%) 25%entirely (>80%) 15%Document Lengthshort (1-10 pp.)
50%medium (10-100 pp.)
35%long (100-1000 pp.)
15%Plagiarism Case StatisticsTopic Matchintra-topic cases 50%inter-topic cases 50%Obfuscationnone 40%artificial?
low obfuscation 20%?
high obfuscation 20%simulated (AMT) 6%translated ({de,es} to en) 14%Case Lengthshort (50-150 words) 34%medium (300-500 words) 33%long (3000-5000 words) 33%The documents without plagiarism allow to deter-mine whether or not a detector can distinguish pla-giarism cases from overlaps that occur naturallybetween random documents.The corpus is split into two parts, correspond-ing to the two paradigms of plagiarism detection,namely external plagiarism detection and intrinsicplagiarism detection.
Note that in the case of in-trinsic plagiarism detection the source documentsused to generate the plagiarism cases are omitted:intrinsic detection algorithms are expected to de-tect plagiarism in a suspicious document by an-alyzing the document in isolation.
Moreover, theintrinsic plagiarism cases are not obfuscated in or-der to preserve the writing style of the original au-thor; the 40% of unobfuscated plagiarism cases inthe corpus include the 30% of the cases belongingto the intrinsic part.The fraction of plagiarism per document, thelengths of the documents and plagiarism cases,and the degree of obfuscation per case deter-mine the difficulty of the cases: the corpus con-tains short documents with a short, unobfuscatedplagiarism case, resulting in a 5% fraction ofplagiarism, but it also contains large documentswith several obfuscated plagiarism cases of vary-ing lengths, drawn from different source docu-ments and resulting in fractions of plagiarism upto 100%.
Since the true distributions of these pa-rameters in real plagiarism are unknown, sensible1002estimations were made for the corpus.
E.g., thereare more simple plagiarism cases than complexones, where ?simple?
refers to short cases, hardlyplagiarism per document, and less obfuscation.Finally, plagiarism cases were generated be-tween topically related documents and betweenunrelated documents.
To this end, the source doc-uments and the suspicious documents were clus-tered into k = 30 clusters using bisecting k-means [22].
Then an equal share of plagiarismcases were generated for pairs of source docu-ments and suspicious documents within as wellas between clusters.
Presuming the clusters cor-respond to (broad) topics, we thus obtained intra-topic plagiarism and inter-topic plagiarism.4 Corpus ValidationThis section reports on validation results aboutthe ?quality?
of the plagiarism cases created forour corpus.
We compare both artificial plagia-rism cases and simulated plagiarism cases to casesof the two corpora Clough09 and METER.
Pre-suming that the authors of these corpora put theirbest efforts into case construction and annotation,the comparison gives insights whether our scale-up strategies are reasonable in terms of case qual-ity.
To foreclose the results, we observe that sim-ulated plagiarism and, in particular, artificial pla-giarism behave similar to the two handmade cor-pora.
In the light of the employed strategies toconstruct plagiarism this result may or may notbe surprising?however, we argue that it is neces-sary to run such a comparison in order to providea broadly accepted evaluation framework in thissensitive area.The experimental setup is as follows: given aplagiarism case s = ?splg, dplg, ssrc, dsrc?, the pla-giarized passage splg is compared to the sourcepassage ssrc using 10 different retrieval models.Each model is an n-gram vector space model(VSM) where n ranges from 1 to 10 words,employing stemming, stop word removal, tf -weighting, and the cosine similarity.
Similarityvalues are computed for all cases found in eachcorpus, but since the corpora are of different sizes,100 similarities are sampled from each corpus toensure comparability.The rationale of this setup is as follows: a well-known fact from near-duplicate detection is thatif two documents share only a few 8-grams?so-called shingles?it is highly probable that they areduplicates [5].
Another well-known fact is thattwo documents which are longer than a few sen-tences and which are exactly about the same topicwill, with a high probability, share a considerableportion of their vocabulary.
I.e., they have a high10.80.60.40.20n = 1 2 3 4 5 6 7 8 9 10Similarityn-gram VSMClough09ArtificialMedian25% Quartile75% QuartileSimulated (AMT)METERLeft to right:Figure 1: Comparison of four corpora of text reuse and plagiarism: each box plot shows the middlerange of the measured similarities when comparing source passages to their rewritten versions.
Basis isan n-gram VSM, where n ?
{1, 2, .
.
.
, 10} words.1003similarity under a 1-gram VSM.
It follows for pla-giarism detection that a common shingle betweensplg and ssrc pinpoints very accurately an unob-fuscated portion of splg, while it is inevitable thateven a highly obfuscated splg will share a portionof its vocabulary with ssrc.
The same holds for allother kinds of text reuse.Figure 1 shows the obtained similarities, con-trasting each n-gram VSM and each corpus.
Thebox plots show the middle 50% of the respectivesimilarity distributions as well as median similar-ities.
The corpora divide into groups with compa-rable behavior: in terms of the similarity rangescovered, the artificial plagiarism compares to theMETER corpus, except for n ?
{2, 3}, while thesimulated plagiarism from the Clough09 corpusbehaves like that from our corpus, but with a dif-ferent amplitude.
In terms of median similarity,METER, Clough09, and our simulated plagiarismbehave almost identical, while the artificial plagia-rism differs.
Also note that our simulated plagia-rism as well as the Clough09 corpus contain somecases which are hardly obfuscated.We interpret these results as follows: (1) Dif-ferent kinds of plagiarism and text reuse do notdiffer very much under n-gram models.
(2) Ar-tificial plagiarism, if carefully generated, is a vi-able alternative to simulated plagiarism cases andreal text reuse cases.
(3) Our strategies to scale-upthe construction of plagiarism corpora works wellcompared to existing, handmade corpora.5 SummaryCurrent evaluation methodologies in the fieldof plagiarism detection research have conceptualshortcomings and allow only for a limited compa-rability.
Our research contributes right here: wepresent tailored performance measures for plagia-rism detection and the large-scale corpus PAN-PC-10 for the controlled evaluation of detectionalgorithms.
The corpus features various kindsof plagiarism cases, including obfuscated casesthat have been generated automatically and man-ually.
An evaluation of the corpus in relation toprevious corpora reveals a high degree of matu-rity.
Until now, 31 plagiarism detectors have beencompared using our evaluation framework.
Thishigh number of systems has been achieved basedon two benchmarking workshops in which theframework was employed and developed, namelyPAN?09 [15] and PAN?10 [16].
We hope that ourframework will be beneficial as a challenging andyet realistic test bed for researchers in order to pin-point the room for the development of better pla-giarism detection systems.AcknowledgementsWe thank Andreas Eiselt for his devoted workon the corpus over the past two years.
Thiswork is partially funded by CONACYT-Mexicoand the MICINN project TEXT-ENTERPRISE2.0 TIN2009-13391-C04-03 (Plan I+D+i).Bibliography[1] Omar Alonso and Stefano Mizzaro.
CanWe Get Rid of TREC Assessors?
UsingMechanical Turk for RelevanceAssessment.
In SIGIR?09: Proceedings ofthe Workshop on The Future of IREvaluation, 2009.
[2] Vamshi Ambati, Stephan Vogel, and JaimeCarbonell.
Active learning andcrowd-sourcing for machine translation.
InNicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, JosephMariani, Jan Odijk, Stelios Piperidis, MikeRosner, and Daniel Tapias, editors,Proceedings of the Seventh conference onInternational Language Resources andEvaluation (LREC?10), Valletta, Malta, may2010.
European Language ResourcesAssociation (ELRA).
ISBN 2-9517408-6-7.
[3] Jeff Barr and Luis Felipe Cabrera.
AI Getsa Brain.
Queue, 4(4):24?29, 2006.
ISSN1542-7730.
doi:10.1145/1142055.1142067.
[4] Regina Barzilay and Lillian Lee.
Learningto Paraphrase: An Unsupervised ApproachUsing Multiple-Sequence Alignment.
InNAACL?03: Proceedings of the 2003Conference of the North American Chapterof the Association for ComputationalLinguistics on Human LanguageTechnology, pages 16?23, Morristown, NJ,USA, 2003.
Association for ComputationalLinguistics.
doi:10.3115/1073445.1073448.
[5] Andrei Z. Broder.
Identifying and FilteringNear-Duplicate Documents.
In COM?00:Proceedings of the 11th Annual Symposiumon Combinatorial Pattern Matching, pages10041?10, London, UK, 2000.
Springer-Verlag.ISBN 3-540-67633-3.
[6] Manuel Cebrian, Manuel Alfonseca, andAlfonso Ortega.
Towards the Validation ofPlagiarism Detection Tools by Means ofGrammar Evolution.
IEEE Transactions onEvolutionary Computation, 13(3):477?485,June 2009.
ISSN 1089-778X.
[7] Paul Clough.
Plagiarism in Natural andProgramming Languages: An Overview ofCurrent Tools and Technologies.
InternalReport CS-00-05, University of Sheffield,2000.
[8] Paul Clough.
Old and New Challenges inAutomatic Plagiarism Detection.
NationalUK Plagiarism Advisory Service,http://ir.shef.ac.uk/cloughie/papers/pas_plagiarism.pdf,2003.
[9] Paul Clough and Mark Stevenson.
Creatinga Corpus of Plagiarised Academic Texts.
InProceedings of Corpus LinguisticsConference, CL?09 (to appear), 2009.
[10] Paul Clough and Mark Stevenson.Developing A Corpus of Plagiarised ShortAnswers.
Language Resources andEvaluation: Special Issue on Plagiarismand Authorship Analysis (in press), 2010.
[11] Paul Clough, Robert Gaizauskas, and S. L.Piao.
Building and Annotating a Corpus forthe Study of Journalistic Text Reuse.
InProceedings of the 3rd InternationalConference on Language Resources andEvaluation (LREC-02), pages 1678?1691,2002.
[12] Wiebe Hordijk, Mar?a L. Ponisio, and RoelWieringa.
Structured Review of CodeClone Literature.
Technical ReportTR-CTIT-08-33, Centre for Telematics andInformation Technology, University ofTwente, Enschede, 2008.
[13] Winter Mason and Duncan J. Watts.Financial Incentives and the "Performanceof Crowds".
In HCOMP?09: Proceedingsof the ACM SIGKDD Workshop on HumanComputation, pages 77?85, New York, NY,USA, 2009.
ACM.
ISBN978-1-60558-672-4.
doi:10.1145/1600150.1600175.
[14] Hermann Maurer, Frank Kappe, and BilalZaka.
Plagiarism - A Survey.
Journal ofUniversal Computer Science, 12(8):1050?1084, 2006.
[15] Martin Potthast, Benno Stein, AndreasEiselt, Alberto Barr?n-Cede?o, and PaoloRosso.
Overview of the 1st InternationalCompetition on Plagiarism Detection.
InBenno Stein, Paolo Rosso, EfstathiosStamatatos, Moshe Koppel, and EnekoAgirre, editors, SEPLN 2009 Workshop onUncovering Plagiarism, Authorship, andSocial Software Misuse (PAN 09), pages1?9.
CEUR-WS.org, September 2009.
URLhttp://ceur-ws.org/Vol-502.
[16] Martin Potthast, Benno Stein, AndreasEiselt, Alberto Barr?n-Cede?o, and PaoloRosso.
Overview of the 2nd InternationalBenchmarking Workshop on PlagiarismDetection.
In Benno Stein, Paolo Rosso,Efstathios Stamatatos, and Moshe Koppel,editors, Proceedings of PAN at CLEF 2010:Uncovering Plagiarism, Authorship, andSocial Software Misuse, September 2010.
[17] Chanchal K. Roy and James R. Cordy.Scenario-Based Comparison of CloneDetection Techniques.
In ICPC ?08:Proceedings of the 2008 The 16th IEEEInternational Conference on ProgramComprehension, pages 153?162,Washington, DC, USA, 2008.
IEEEComputer Society.
ISBN978-0-7695-3176-2.
[18] Chanchal K. Roy and James R. Cordy.Towards a Mutation-based AutomaticFramework for Evaluating Code CloneDetection Tools.
In C3S2E ?08:Proceedings of the 2008 C3S2E conference,pages 137?140, New York, NY, USA, 2008.ACM.
ISBN 978-1-60558-101-9.
[19] Chanchal K. Roy, James R. Cordy, andRainer Koschke.
Comparison andEvaluation of Code Clone DetectionTechniques and Tools: A QualitativeApproach.
Sci.
Comput.
Program., 74(7):470?495, 2009.
ISSN 0167-6423.
[20] Chanchal K. Roy and James R. Cordy.
Asurvey on software clone detectionresearch.
Technical Report 2007-541,School of Computing, Queen?s Universityat Kingston, Ontario, Canada, 2007.
[21] Geoffrey R. Whale.
Identification ofProgram Similarity in Large Populations.The Computer Journal, 33(2):140?146,1990.
doi: 10.1093/comjnl/33.2.140.
[22] Ying Zhao, George Karypis, and UsamaFayyad.
Hierarchical Clustering Algorithmsfor Document Datasets.
Data Min.
Knowl.Discov., 10(2):141?168, 2005.
ISSN1384-5810.
doi:10.1007/s10618-005-0361-3.1005
