Proceedings of the 9th SIGdial Workshop on Discourse and Dialogue, pages 68?71,Columbus, June 2008. c?2008 Association for Computational LinguisticsPersistent Information State in a Data-Centric Architecture?Sebastian Varges, Giuseppe Riccardi, Silvia QuarteroniDepartment of Information Engineering and Computer ScienceUniversity of Trento38050 Povo di Trento, Italy{varges|riccardi|silviaq}@disi.unitn.itAbstractWe present the ADAMACH data centric dia-log system, that allows to perform on- and off-line mining of dialog context, speech recog-nition results and other system-generated rep-resentations, both within and across dialogs.The architecture implements a ?fat pipeline?for speech and language processing.
We detailhow the approach integrates domain knowl-edge and evolving empirical data, based on auser study in the University Helpdesk domain.1 IntroductionIn this paper, we argue that the ability to store andquery large amounts of data is a key requirementfor data-driven dialog systems, in which the data isgenerated by the spoken dialog system (SDS) com-ponents (spoken language understanding (SLU), di-alog management (DM), natural language genera-tion (NLG) etc.)
and the world it is interactingwith (news streams, ambient sensors etc.).
Wedescribe an SDS that is built around a databasemanagement system (DBMS), uses the web ser-vice paradigm (in contrast to the architecture de-scribed in (Varges and Riccardi, 2007)), and em-ploys a Voice XML (VXML) server for interfac-ing with Automatic Speech Recognition (ASR) andText-to-Speech (TTS) components.
We would liketo emphasize upfront that this does not mean thatwe follow a VXML dialog model.
?This work was partially supported by the European Com-mission Marie Curie Excellence Grant for the ADAMACHproject (contract No.
022593) and by LUNA STREP project(contract no33549).The data centric architecture we adopt has sev-eral advantages: first, the database concentrates het-erogeneous types of information allowing to uni-formly query the evolving data at any time, e.g.
byperforming queries across various types of infor-mation.
Second, the architecture facilitates dialogevaluation, data mining and online learning becausedata is available for querying as soon as it has beenstored.
Third, multiple systems/applications can bemade available on the same infrastructure due to aclean separation of its processing modules (SLU,DM, NLG etc.)
from data storage and persistency(DBMS), and monitoring/analysis/visualization andannotation tools.
Fourth, there is no need for sep-arate ?logging?
mechanisms: the state of the SDSis contained in the database, and is therefore persis-tently available for analysis after the dialog ends.As opposed to the presented architecture, theOpen Agent Architecture (OAA) (Martin et al,1999) and DARPA Communicator (Seneff et al,1998) treat data as peripheral: they were not specif-ically designed to handle large volumes of data, anddata is not automatically persistent.
In contrast tothe CSLI-DM (Mirkovic and Cavedon, 2005), andTrindiKit (Larsson and Traum, 2000), but similarto Communicator, the ADAMACH architecture isserver-based, thus enabling continuous operation.To prove our concept, we test it on a Universityhelpdesk application (section 4).2 Dialog System ArchitectureFigure 1 shows our vision for the architecture of theADAMACH system.
We implemented and evalu-ated the speech modality based core of this system68Figure 1: Architecture vision(figure 2).
A typical interaction is initiated by aphone call that arrives at an telephony server whichroutes it to a VXML platform.
A VXML page iscontinuously rewritten by the dialog manager, con-taining the system utterance and other TTS param-eters, and the ASR recognition parameters for thenext user utterance.
Thus, VXML is used as a low-level interface to the ASR and TTS engines, but notfor representing dialog strategies.
Once a user utter-ance is recognized, a web service request is issuedto a dialog management server.All communication between the above-mentionedcomponents is stored in the DBMS: ASR recogni-tion results, TTS parameters and ASR recognitionparameters reside in separate tables.
The dialogmanager uses the basic tables as its communicationprotocol with ASR and TTS engines, and addition-ally stores its Information State (IS) in the database.This means that the IS is automatically persistent,and that dialog management becomes a function thatmaps ASR results and old IS to the TTS and ASRparameters and a new IS.
The tables of the databaseare organized into turns, several of which belong to acall (dialog), thus resulting in a tree structure that isenforced by foreign key constraints in the relationaldatabase.The VXML standard is based on the web infras-tructure.
In particular, a VXML platform can issueHTTP requests that can be served by a web serverjust like any (HTML) page.
The VXML server onlysees the generated VXML page, the ?return value?of the HTTP request.
This allows us to organize theprocessing modules of the dialog system (SLU, DM,VXML generator) as web services that are invokedby the HTTP request.
As a consequence, each sys-tem turn of a dialog is a separate, stateless request.The state of the dialog is stored in the database.Furthermore, by threading the VXML session IDthrough the processing loop (including the VXMLpages generated on-the-fly) and distinguishing en-tries in the DB by sessions, the SDS is inherentlyparallelizable, just as a conventional web server canserve many users in parallel.
Figure 2 shows howinformation is processed for each turn.
The HTTPrequests that invoke the processing modules pass onvarious IDs and parameters, but the actual data isstored in the DB and retrieved only if a processingmodule requires it.
This effectively implements a?fat pipeline?
: each speech, language and DM mod-ule has access to the database for rescoring and mod-eling (i.e.
data within and across dialogs).
At the im-plementation level, this balances a lightweight com-munication protocol downstream with data flowinglaterally towards the database.3 Dialog ManagementDialog management works in two stages: retriev-ing and preprocessing facts (tuples) taken from thedatabase, and inferencing over those facts to gen-erate a system response.
We distinguish betweenthe ?context model?
of the first phase and the ?dialogmove engine?
(DME) of the second phase (Larssonand Traum, 2000).The first stage entails retrieving from the persis-tent Information State the following information:all open questions for the current dialog from thedatabase, any application information already pro-vided by the user (including their grounding status),the ASR recognition results of last user turn, andconfidence and other thresholds.
The context modelthat is applied when retrieving the relevant dialoghistory from the database can be characterized as a?linear default model?
: application parameters pro-vided by the user, such as student ID, are overrid-den if the user provides a new value, for example tocorrect a previous misunderstanding.
Task bound-aries are detected and prevent an application param-eter from carrying over directly to the new task.The second stage employs an inference engineto determine the system action and response: SLUrules match the user utterance to open questions.69Figure 2: Turn-level information flowThis may result in the decision to verify the applica-tion parameter in question, and the action is verbal-ized by language generation rules.
If the parameteris accepted, application dependent task rules deter-mine the next parameter to be acquired, resulting inthe generation of an appropriate request.
For reasonsof space, we cannot provide more details here.4 ExperimentsOur current application is a University helpdeskin Italian which students call to perform 5 tasks:receive information about exams (times, rooms.
.
.
), subscribe/cancel subscriptions to exams, obtainexammark, or request to talk to an operator.
Follow-ing experimentations, we annotated the dialogs andconducted performance statistics using the system?sbuilt-in annotation tool.Two Italian mothertongues were in charge ofmanually annotating a total of 423 interactions.Each annotator independently annotated each dialogturn according to whether one of the five availabletasks was being requested or completed in it.
Tocompute inter-annotator agreement, 24 dialogs wereprocessed by both annotators; the remaining oneswere partitioned equally among them.We computed agreement at both turn and dialoglevel.
Turn level agreement is concerned with whichtasks are requested and completed at a given dia-log turn according to each annotator.
An agree-ment matrix is compiled where rows and columnscorrespond to the five task types in our application.Cohen?s ?
(Cohen, 1960), computed over the turnmatrix, gave a turn agreement of 0.72 resp.
0.77for requests resp.
completions, exceeding the rec-ommended 0.7 threshold.
While turn-level agree-ment refers to which tasks occurred and at whatturn, dialog level agreement refers to how many taskrequests/completions occurred.
Also at the dialoglevel, the ?
statistic gave good results (0.71 for re-quests and 0.9 for completions).General dialog statistics The average duration ofthe 423 annotated dialogs is 63.1 seconds, with anaverage of 7.43 turn (i.e.
adjacency) pairs.
356 ofthe dialogs contained at least one task; the majority(338) contained exactly one, 17 dialogs contained 2tasks, and one dialog contained 3.
In the remain-ing 67 dialogs, no tasks were detected: from theaudio files, it seems that these generally happenedby accident or in noisy environments, hence noin-put/hangup events occurred shortly after the initialsystem prompt.Furthermore, relative frequencies of task requestsand task completions are reported in Table 1.
In to-tal, according to the two annotators, there were 375task requests and 234 task completions.
Among therequested tasks, the vast majority was composed by?Get exam mark?
?a striking 96%?
while ?Examwithdrawal?
never occurred and the three otherswere barely performed.
Indeed, it seems that stu-dents preferred to use the system to carry on ?in-formative?
tasks such as obtaining exam marks andgeneral information rather than ?active?
tasks suchas exam subscription and withdrawal.Table 1: Task request and completion frequencies (%)Task Request CompletionGet exam mark 96 (360) 96.6 (226)Info on exam 1.9 (7) 1.7 (4)Exam subscription 1.1 (4) 0.4 (1)Exam withdrawal 0.0 (0) 0.0 (0)Talk to operator 1.1 (4) 1.3 (3)Total 100 (375) 100 (234)Task and dialog success Based on the annotationof task requests and completions, we defined tasksuccess as a binary measure of whether the requestof a given task type is eventually followed by a taskcompletion of the same type.
Table 2 reports the av-erage success of each task type according to the an-70notators1.
Our results show that the most frequentlyrequested type, ?Get exammark?, has a 64.64% suc-cess rate (it seems that failure was mostly due to thesystem?s inability to recognize student IDs).Table 2: Top: annotator (srM ) and automatic (srA) tasksuccess rates.
Mean ?
binomial proportion confidenceinterval on the average task success (?= 95%) is reported.Bottom: mean annotator (dsrM ) and automatic (dsrA)dialog success rates ?
normal law c.i.
(?= 95%).Task srM (%) srA(%)Get exam mark 64.64 77.97Info on exam 57.14 71.43Exam subscription 25 100Exam withdrawal - -Talk to operator 75 75Average 64.17?4.96 78.06?4.28Dialog dsrM (%) dsrA(%)Average 64.47?4.95 88.31?9.2In fact, while it is straightforward to obtain tasksuccess information using the manual annotation ofdialogs, when the dialog system cannot rely on hu-man judgments, unsupervised approaches must bedefined for a rapid (on-line or off-line) evaluation.For this purpose, an automatic approximation of the?manual?
task success estimation has been definedusing a set of database queries associated to eachtask type.
For instance, the task success query as-sociated to ?Info on exam?
checks that two condi-tions are met in the current dialog: 1) it includesa turn where an action is requested the interpreta-tion of which contains ?information?
; 2) it containsa turn where the concept Exam Name is in focus.Automatic task success rates have been computedon the same dialogs for which manual task successrates were available and are reported in Table 2, col.2.
The comparison shows that the automatic metricsrA is more ?optimistic?
than the manual one srM .Indeed, automatic estimators rely on ?punctual?
in-dicators (such as the occurrence of confirmations ofa given value) in the whole dialog, regardless of thetask they appear in (this information is only avail-able from human annotation) and also of the orderwith which such indicators appear in the dialog.1As several task types occur seldom, we only report the con-fidence intervals on the means relating to the overall (?Aver-age?)
task success, computed according to the normal law.As a by-product of task success evaluation, we de-fined dialog success rate (dsr) as the average successrate of the tasks in a dialog: dsr =?ti?Tsr(ti)|T | ,T being the set of requested tasks.
Depending onwhether srM or srA is used, we obtain two metrics,dsrM resp.
dsrA.Our dialog success results (last row of Table 2) arecomparable to the task success ones; also, the differ-ence between the automatic and manual estimatorsof dialog success is similar to their difference at thethe task level.
This is not surprising when consider-ing that most of the dialogs contained only one task.5 ConclusionsWe have presented a data-centric Spoken DialogSystem whose novel aspect is the storage and re-trieval of dialog management state, ASR results andother information in a database.
As a consequence,dialog management can be lightweight and operateon a turn-by-turn basis, and dialog system evaluationand logging are facilitated.AcknowledgmentsWe would like to thank Pierluigi Roberti for helpingwith the speech platform and annotation tools, andLOQUENDO for providing the VXML platform.ReferencesJ.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20:37?46.S.
Larsson and D. Traum.
2000.
Information State anddialogue management in the TRINDI Dialogue MoveEngine Toolkit.
Natural Language Engineering, 6(3?4):323?340.D.
L. Martin, A. J. Cheyer, and D. B. Moran.
1999.
TheOpen Agent Architecture: A framework for buildingdistributed software systems.
Applied Artificial Intel-ligence: An International Journal, 13(1-2):91?128.D.
Mirkovic and L. Cavedon.
2005.
Practical Plug-and-Play Dialogue Management.
In Proceedings of PA-CLING, Tokyo, Japan.S.
Seneff, E. Hurley, R. Lau, C. Pao, P. Schmid, andV.
Zue.
1998.
GALAXY-II: A reference architecturefor conversational system development.
In Proc.
ofICSLP 1998, Sydney, Australia.S.
Varges and G. Riccardi.
2007.
A data-centric archi-tecture for data-driven spoken dialogue systems.
InProceedings of ASRU, Kyoto, Japan.71
