Coling 2010: Poster Volume, pages 1059?1067,Beijing, August 2010Multilingual Summarization Evaluation without Human ModelsHoracio SaggionTALN - DTICUniversitat Pompeu Fabrahoracio.saggion@upf.eduJuan-Manuel Torres-MorenoLIA/Universite?
d?AvignonE?cole Polytechnique de Montre?aljuan-manuel.torres@univ-avignon.frIria da CunhaIULA/Universitat Pompeu FabraLIA/Universite?
d?Avignoniria.dacunha@upf.eduEric SanJuanLIA/Universite?
d?Avignoneric.sanjuan@univ-avignon.frPatricia Vela?zquez-MoralesVM Labspatricia vazquez@yahoo.comAbstractWe study correlation of rankings of textsummarization systems using evaluationmethods with and without human mod-els.
We apply our comparison frame-work to various well-established content-based evaluation measures in text sum-marization such as coverage, Responsive-ness, Pyramids and ROUGE studying theirassociations in various text summarizationtasks including generic and focus-basedmulti-document summarization in Englishand generic single-document summariza-tion in French and Spanish.
The researchis carried out using a new content-basedevaluation framework called FRESA tocompute a variety of divergences amongprobability distributions.1 IntroductionText summarization evaluation has always been acomplex and controversial issue in computationallinguistics.
In the last decade, significant ad-vances have been made in the summarization eval-uation field.
Various evaluation frameworks havebeen established and evaluation measures devel-oped.
SUMMAC (Mani et al, 2002), in 1998,provided the first system independent frameworkfor summary evaluation; the Document Under-standing Conference (DUC) (Over et al, 2007)was the main evaluation forum from 2000 until2007; nowadays, the Text Analysis Conference(TAC)1 provides a forum for assessment of dif-ferent information access technologies includingtext summarization.Evaluation in text summarization can be extrin-sic or intrinsic (Spa?rck-Jones and Galliers, 1996).In an extrinsic evaluation, the summaries are as-sessed in the context of an specific task a humanor machine has to carry out; in an intrinsic eval-uation, the summaries are evaluated in referenceto some ideal model.
SUMMAC was mainly ex-trinsic while DUC and TAC followed an intrinsicevaluation paradigm.
In order to intrinsically eval-uate summaries, the automatic summary (peer)has to be compared to a model summary or sum-maries.
DUC used an interface called SEE to al-low human judges compare a peer summary to amodel summary.
Using SEE, human judges give acoverage score to the peer summary representingthe degree of overlap with the model summary.Summarization systems obtain a final coveragescore which is the average of the coverage?s scoresassociated to their summaries.
The system?s cov-erage score can then be used to rank summariza-tion systems.
In the case of query-focused sum-marization (e.g.
when the summary has to re-spond to a question or set of questions) a Respon-siveness score is also assigned to each summarywhich indicates how responsive the summary is tothe question(s).Because manual comparison of peer summarieswith model summaries is an arduous and costly1http://www.nist.gov/tac1059process, a body of research has been produced inthe last decade on automatic content-based eval-uation procedures.
Early studies used text simi-larity measures such as cosine similarity (with orwithout weighting schema) to compare peer andmodel summaries (Donaway et al, 2000), vari-ous vocabulary overlap measures such as set ofn-grams overlap or longest common subsequencebetween peer and model have also been pro-posed (Saggion et al, 2002; Radev et al, 2003).The Bleu machine translation evaluation measure(Papineni et al, 2002) has also been tested insummarization (Pastra and Saggion, 2003).
TheDUC conferences adopted the ROUGE packagefor content-based evaluation (Lin, 2004).
It im-plements a series of recall measures based on n-gram co-occurrence statistics between a peer sum-mary and a set of model summaries.
ROUGE mea-sures can be used to produce systems ranks.
Ithas been shown that system rankings producedby some ROUGE measures (e.g., ROUGE-2 whichuses bi-grams) correlate with rankings producedusing coverage.
In recent years the Pyramids eval-uation method (Nenkova and Passonneau, 2004)was introduced.
It is based on the distributionof ?content?
in a set of model summaries.
Sum-mary Content Units (SCUs) are first identified inthe model summaries, then each SCU receivesa weight which is the number of models con-taining or expressing the same unit.
Peer SCUsare identified in the peer, matched against modelSCUs, and weighted accordingly.
The Pyramidsscore given to the peer is the ratio of the sumof the weights of its units and the sum of theweights of the best possible ideal summary withthe same number of SCUs as the peer.
The Pyra-mids scores can be used for ranking summariza-tion systems.
Nenkova and Passonneau (2004)showed that Pyramids scores produced reliablesystem rankings when multiple (4 or more) mod-els were used and that Pyramids rankings cor-relate with rankings produced by ROUGE-2 andROUGE-SU2 (i.e.
ROUGE with skip bi-grams).Still this method requires the creation of modelsand the identification, matching, and weighting ofSCUs in both models and peers.Donaway et al (2000) put forward the idea ofusing directly the full document for comparisonpurposes, and argued that content-based measureswhich compare the document to the summary maybe acceptable substitutes for those using modelsummaries.
A method for evaluation of sum-marization systems without models has been re-cently proposed (Louis and Nenkova, 2009).
It isbased on the direct content-based comparison be-tween summaries and their corresponding sourcedocuments.
Louis and Nenkova (2009) evalu-ated the effectiveness of the Jensen-Shannon (Lin,1991b) theoretic measure in predicting systemsranks in two summarization tasks query-focusedand update summarization.
They have shown thatranks produced by Pyramids and ranks producedby the Jensen-Shannon measure correlate.
How-ever, they did not investigate the effect of the mea-sure in past summarization tasks such as genericmulti-document summarization (DUC 2004 Task2), biographical summarization (DUC 2004 Task5), opinion summarization (TAC 2008 OS), andsummarization in languages other than English.We think that, in order to have a better under-standing of document-summary evaluation mea-sures, more research is needed.
In this paper wepresent a series of experiments aimed at a betterunderstanding of the value of the Jensen-Shannondivergence for ranking summarization systems.We have carried out experimentation with theproposed measure and have verified that in cer-tain tasks (such as those studied by (Louis andNenkova, 2009)) there is a strong correlationamong Pyramids and Responsiveness and theJensen-Shannon divergence, but as we will showin this paper, there are datasets in which the cor-relation is not so strong.
We also present exper-iments in Spanish and French showing positivecorrelation between the Jensen-Shannon measureand ROUGE.The rest of the paper is organized in the follow-ing way: First in Section 2 we introduce relatedwork in the area of content-based evaluation iden-tifying the departing point for our inquiry; then inSection 3 we explain the methodology adopted inour work and the tools and resources used for ex-perimentation.
In Section 4 we present the experi-ments carried out together with the results.
Sec-tion 5 discusses the results and Section 6 con-cludes the paper.10602 Related WorkOne of the first works to use content-based mea-sures in text summarization evaluation is due to(Donaway et al, 2000) who presented an evalu-ation framework to compare rankings of summa-rization systems produced by recall and cosine-based measures.
They showed that there wasweak correlation between rankings produced byrecall, but that content-based measures producerankings which were strongly correlated, thuspaving the way for content-based measures in textsummarization evaluation.Radev et al (2003) also compared various eval-uation measures based on vocabulary overlap.
Al-though these measures were able to separate ran-dom from non-random systems, no clear conclu-sion was reached on the value of each of the mea-sures studied.Nowadays, a widespread summarization evalu-ation framework is ROUGE (Lin and Hovy, 2003)which, as we have mentioned before, offers a setof statistics that compare peer summaries withmodels.
Various statistics exist depending on theused n-gram and on the type of text processing ap-plied to the input texts (e.g., lemmatization, stop-word removal).Lin et al (2006) proposed a method of evalua-tion based on the use of ?distances?
or divergencesbetween two probability distributions (the distri-bution of units in the automatic summary and thedistribution of units in the model summary).
Theystudied two different Information Theoretic mea-sures of divergence: the Kullback-Leibler (KL)(Kullback and Leibler, 1951) and Jensen-Shannon(JS) (Lin, 1991a) divergences.
In this work weuse the Jensen-Shannon (JS) divergence that isdefined as follows:DJS(P ||Q) = 12?wPw log22PwPw +Qw+ Qw log22QwPw +Qw(1)This measure can be applied to the distribu-tion of units in system summaries P and refer-ence summaries Q and the value obtained usedas a score for the system summary.
The methodhas been tested by (Lin et al, 2006) over theDUC 2002 corpus for single and multi docu-ment summarization tasks showing good correla-tion among divergence measures and both cover-age and ROUGE rankings.Louis and Nenkova (2009) went even furtherand, as in (Donaway et al, 2000), proposed todirectly compare the distribution of words in fulldocuments with the distribution of words in auto-matic summaries to derive a content-based eval-uation measure.
They found high correlationamong rankings produced using models and rank-ings produced without models.
This work is thedeparting point for our inquiry into the value ofmeasures that do not rely on human models.3 MethodologyThe methodology of this paper mirrors the oneadopted in past work (Donaway et al, 2000;Louis and Nenkova, 2009).
Given a particularsummarization task T , p data points to be sum-marized with input material {Ii}p?1i=0 (e.g.
doc-ument(s), questions, topics), s peer summaries{SUMi,k}s?1k=0 for input i, and m model sum-maries {MODELi,j}m?1j=0 for input i, we will com-pare rankings of the s peer summaries producedby various evaluation measures.
Some measureswe use compare summaries with n out of the mmodels:MEASUREM (SUMi,k, {MODELi,j}nj=0) (2)while other measures compare peers with all orsome of the input material:MEASUREM (SUMi,k, I ?i) (3)where I ?i is some subset of input Ii.
The val-ues produced by the measures for each sum-mary SUMi,k are averaged for each system k =0, .
.
.
, s ?
1 and these averages are used to pro-duce a ranking.
Rankings are compared usingSpearman Rank correlation (Spiegel and Castel-lan, 1998) used to measure the degree of associa-tion between two variables whose values are usedto rank objects.
We use this correlation to directlycompare results to those presented in (Louis andNenkova, 2009).
Computation of correlations is1061done using the CPAN Statistics-RankCorrelation-0.12 package2, which computes the rank correla-tion between two vectors.3.1 ToolsWe carry out experimentation using a new sum-marization evaluation framework: FRESA?FRamework for Evaluating SummariesAutomatically?
which includes document-based summary evaluation measures based onprobabilities distribution.
As in the ROUGEpackage, FRESA supports different n-gramsand skip n-grams probability distributions.The FRESA environment can be used in theevaluation of summaries in English, French,Spanish and Catalan, and it integrates filteringand lemmatization in the treatment of summariesand documents.
It is developed in Perl and will bemade publicly available.
We also use the ROUGEpackage to compute various ROUGE statistics innew datasets.3.2 Summarization Tasks and Data SetsWe have conducted our experimentation with thefollowing summarization tasks and data sets:Generic multi-document-summarization in En-glish (i.e.
production a short summary of a clusterof related documents) using data fromDUC 20043corpus task 2: 50 clusters (10 documents each) ?294,636 words.Focused-based summarization in English (i.e.production a short focused multi-document sum-mary focused on the question ?who is X?
?, whereX is a person?s name) using data from the DUC2004 task 5: 50 clusters ( 10 documents each plusa target person name) ?
284,440 words.Update-summarization task that consists of cre-ating a summary out of a cluster of documents anda topic.
Two sub-tasks are considered here: A)an initial summary has to be produced based onan initial set of documents and topic; B) an up-date summary has to be produced from a differ-ent (but related) cluster assuming documents usedin A) are known.
The English TAC 2008 Update2http://search.cpan.org/?gene/Statistics-RankCorrelation-0.12/3http://www-nlpir.nist.gov/projects/duc/guidelines/2004.htmlSummarization dataset is used which consists of48 topics with 20 documents each ?
36,911 words.Opinion summarization where systems have toanalyze a set of blog articles and summarize theopinions about a target in the articles.
The TAC2008 Opinion Summarization in English4 data set(taken from the Blogs06 Text Collection) is used:25 clusters and targets (i.e., target entity and ques-tions) were used ?
1,167,735 words.Generic single-document summarization inSpanish using the ?Spanish Medicina Cl?
?nica?5corpus which is composed of 50 biomedical ar-ticles in Spanish, each one with its correspondingauthor abstract ?
124,929 words.Generic single document summarization inFrench using the ?Canadien French Sociologi-cal Articles?
corpus from the journal Perspec-tives interdisciplinaires sur le travail et la sante?(PISTES)6.
It contains 50 sociological articles inFrench with their corresponding author abstracts?
381,039 words.3.3 Summarization SystemsFor experimentation in the TAC and the DUCdatasets we directly use the peer summariesproduced by systems participating in the eval-uations.
For experimentation in Spanish andFrench (single-document summarization) wehave created summaries at the compression ratesof the model summaries using the followingsummarization systems:?
CORTEX (Torres-Moreno et al, 2002), asingle-document sentence extraction systemfor Spanish and French that combines vari-ous statistical measures of relevance (anglebetween sentence and topic, various Ham-ming weights for sentences, etc.)
and appliesan optimal decision algorithm for sentenceselection;?
ENERTEX (Fernandez et al, 2007), a sum-marizer based on a theory of textual energy;4http://www.nist.gov/tac/data/index.html5http://www.elsevier.es/revistas/ctl servlet?
f=7032&revistaid=26http://www.pistes.uqam.ca/1062?
SUMMTERM (Vivaldi et al, 2010), aterminology-based summarizer that is usedfor summarization of medical articles anduses specialized terminology for scoring andranking sentences;?
JS summarizer, a summarization system thatscores and ranks sentences according to theirJensen-Shannon divergence to the sourcedocument;?
a lead-based summarization system that se-lects the lead sentences of the document;?
a random-based summarization system thatselects sentences at random;?
the multilingual word-frequency Open TextSummarizer (Yatsko and Vishnyakov, 2007);?
the AutoSummarize program of MicrosoftWord;?
the commercial SSSummarizer7;?
the Pertinence summarizer8;?
the Copernic summarizer9.3.4 Evaluation MeasuresThe following measures derived from humanassessment of the content of the summaries areused in our experiments:?
Coverage is understood as the degree towhich one peer summary conveys the sameinformation as a model summary (Over et al,2007).
Coverage was used in DUC evalua-tions.?
Responsiveness ranks summaries in a 5-pointscale indicating how well the summary sat-isfied a given information need (Over et al,2007).
It is used in focused-based summa-rization tasks.
Responsiveness was used inDUC-TAC evaluations.7http://www.kryltech.com/summarizer.htm8http://www.pertinence.net9http://www.copernic.com/en/products/summarizer?
Pyramids (briefly introduced in Section 1)(Nenkova and Passonneau, 2004) is a contentassessment measure which compares contentunits in a peer summary to weighted contentunits in a set of model summaries.
Pyramidsis the adopted metric for content-based eval-uation in the TAC evaluations.For DUC and TAC datasets the values of thesemeasures are available and we used them directly.We used the following automatic evaluationmeasures in our experiments:?
We use the Rouge package (Lin, 2004) tocompute various statistics.
For the experi-ments presented here we used uni-grams, bi-grams, and the skip bi-grams with maximumskip distance of 4 (ROUGE-1, ROUGE-2 andROUGE-SU4).
ROUGE is used to compare apeer summary to a set of model summariesin our framework.?
Jensen-Shannon divergence formula given inEquation 1 is implemented in our FRESApackage with the following specification forthe probability distribution of words w.Pw =CTwN (4)Qw ={CSwNS if w ?
SCTw+?N+?
?B elsewhere(5)Where P is the probability distribution ofwords w in text T and Q is the probabil-ity distribution of words w in summary S;N is the number of words in text and sum-mary N = NT + NS , B = 1.5|V |, CTw isthe number of words in the text and CSw isthe number of words in the summary.
Forsmoothing the summary?s probabilities wehave used ?
= 0.005.4 Experiments and ResultsWe first replicated the experiments presented in(Louis and Nenkova, 2009) to verify that our im-plementation of JS produced correlation resultscompatible with that work.
We used the TAC2008 Update Summarization data set and com-puted JS and ROUGE measures for each peer1063summary.
We produced two system rankings (onefor each measure), which were compared to rank-ings produced using the manual Pyramids and Re-sponsiveness scores.
Spearman correlations werecomputed among the different rankings.
The re-sults are presented in Table 1.
These results con-firm a high correlation among Pyramids, Respon-siveness, and JS.
We also verified high corre-lation between JS and ROUGE-2 (0.83 Spearmancorrelation, not shown in the table) in this task anddataset.Measure Pyr.
p-value Resp.
p-valueROUGE-2 0.96 p < 0.005 0.92 p < 0.005JS 0.85 p < 0.005 0.74 p < 0.005Table 1: Spearman system rank correlation ofcontent-based measures in TAC 2008 UpdateSummarization taskThen, we experimented with data from DUC2004, TAC 2008 Opinion Summarization pilotand with single document summarization in Span-ish and French.
In spite of the fact that the exper-iments for French and Spanish corpora use lessdata points (i.e., less summarizers per task) thanfor English, results are still quite significant.For DUC 2004, we computed the JS measurefor each peer summary in tasks 2 and 5 and weused JS and the official ROUGE, coverage, andResponsiveness scores to produce systems?
rank-ings.
The various Spearman?s rank correlationvalues for DUC 2004 are presented in Tables 2(for task 2) and 3 (for task 5).
For task 2, we haveverified a strong correlation between JS and cov-erage.
For task 5, the correlation between JS andcoverage is weak, and the correlation between JSand Responsiveness weak and negative.Measure Cov.
p-valueROUGE-2 0.79 p < 0.0050JS 0.68 p < 0.0025Table 2: Spearman system rank correlation ofcontent-based measures with coverage in DUC2004 Task 2Although the Opinion Summarization task is anew type of summarization task and its evaluationis a complicated issue, we have decided to com-pare JS rankings with those obtained using Pyra-Measure Cov.
p-value Resp.
p-valueROUGE-2 0.78 p < 0.001 0.44 p < 0.05JS 0.40 p < 0.050 -0.18 p < 0.25Table 3: Spearman system rank correlation ofcontent-based measures in DUC 2004 Task 5mids and Responsiveness in TAC 2008.
Spear-man?s correlation values are listed in Table 4.
Ascan be seen, there is weak and negative correla-tion of JS with both Pyramids and Responsive-ness.
Correlation between Pyramids and Respon-siveness rankings is high for this task (0.71 Spear-man?s correlation value).Measure Pyr.
p-value Resp.
p-valueJS -0.13 p < 0.25 -0.14 p < 0.25Table 4: Spearman system rank correlation ofcontent-based measures in TAC 2008 OpinionSummarization taskFor experimentation in Spanish and French, wehave run 11 multi-lingual summarization systemsover each of the documents in the two corpora,producing summaries at a compression rate closeto the compression rate of the provided authors?abstracts.
We have computed JS and ROUGEmeasures for each summary and we have aver-aged the measure?s values for each system.
Theseaverages were used to produce rankings per eachmeasure.
We computed Spearman?s correlationsfor all pairs of rankings.
Results are presented inTables 5-6.
All results show medium to strongcorrelation between JS and ROUGE measures.However the JS measure based on uni-grams haslower correlation than JSs which use n-grams ofhigher order.5 DiscussionThe departing point for our inquiry into text sum-marization evaluation has been recent work on theuse of content-based evaluation metrics that donot rely on human models but that compare sum-mary content to input content directly (Louis andNenkova, 2009).
We have some positive and somenegative results regarding the direct use of the fulldocument in content-based evaluation.
We haveverified that in both generic muti-document sum-1064Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-SU4 p-valueJS 0.56 p < 0.100 0.46 p < 0.100 0.45 p < 0.200JS2 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005JS4 0.88 p < 0.001 0.80 p < 0.002 0.81 p < 0.005JSM 0.82 p < 0.005 0.71 p < 0.020 0.71 p < 0.010Table 5: Spearman system rank correlation of content-based measures with ROUGE in the MedicinaClinica Corpus (Spanish)Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-2 p-valueJS 0.70 p < 0.050 0.73 p < 0.05 0.73 p < 0.500JS2 0.93 p < 0.002 0.86 p < 0.01 0.86 p < 0.005JS4 0.83 p < 0.020 0.76 p < 0.05 0.76 p < 0.050JSM 0.88 p < 0.010 0.83 p < 0.02 0.83 p < 0.010Table 6: Spearman system rank correlation of content-based measures with ROUGE in the PISTESSociological Articles Corpus (French)marization and in topic-based multi-documentsummarization in English correlation among mea-sures that use human models (Pyramids, Respon-siveness, and ROUGE) and a measure that doesnot use models (the Jensen Shannon divergence)is strong.
We have found that correlation amongthe same measures is weak for summarization ofbiographical information and summarization ofopinions in blogs.
We believe that in these casescontent-based measures should consider in addi-tion to the input document, the summarizationtask (i.e.
its text-based representation) to betterassess the content of the peers, the task being adeterminant factor in the selection of content forthe summary.
Our multi-lingual experiments ingeneric single-document summarization confirm astrong correlation among the Jensen-Shannon di-vergence and ROUGE measures.
It is worth not-ing that ROUGE is in general the chosen frame-work for presenting content-based evaluation re-sults in non-English summarization.
For the ex-periments in Spanish, we are conscious that weonly have one model summary to compare withthe peers.
Nevertheless, these models are the cor-responding abstracts written by the authors of thearticles and this is in fact the reason for choosingthis corpus.
As the experiments in (da Cunha etal., 2007) show, the professionals of a specializeddomain (as, for example, the medical domain)adopt similar strategies to summarize their textsand they tend to choose roughly the same contentchunks for their summaries.
Because of this, thesummary of the author of a medical article can betaken as reference for summaries evaluation.
It isworth noting that there is still debate on the num-ber of models to be used in summarization evalu-ation (Owkzarzak and Dang, 2009).
In the Frenchcorpus PISTES, we suspect the situation is similarto the Spanish case.6 Conclusions and Future WorkThis paper has presented a series of experimentsin content evaluation in text summarization to as-sess the value of content-based measures that donot rely on the use of model summaries for com-parison purposes.
We have carried out exten-sive experimentation with different summariza-tion tasks drawing a clearer picture of tasks wherethe measures could be applied.
This paper makesthe following contributions:?
We have shown that if we are only interestedin ranking summarization systems accordingto the content of their automatic summaries,there are tasks where models could be sub-stituted by the full document in the computa-tion of the Jensen-Shannon divergence mea-sure obtaining reliable rankings.
However,we have also found that the substitution ofmodels by full-documents is not always ad-visable.
We have found weak correlationamong different rankings in complex sum-marization tasks such as the summarizationof biographical information and the summa-1065Measure ROUGE-1 p-value ROUGE-2 p-value ROUGE-2 p-valueJS 0.83 p < 0.002 0.66 p < 0.05 0.741 p < 0.01JS2 0.80 p < 0.005 0.59 p < 0.05 0.68 p < 0.02JS4 0.75 p < 0.010 0.52 p < 0.10 0.62 p < 0.05JSM 0.85 p < 0.002 0.64 p < 0.05 0.74 p < 0.01Table 7: Spearman system rank correlation of content-based measures with ROUGE in the RPM2 Cor-pus (French)rization of opinions about an ?entity?.?
We have also carried out large-scale exper-iments in Spanish and French which showpositive medium to strong correlation amongsystem?s ranks produced by ROUGE and di-vergence measures that do not use the modelsummaries.?
We have also presented a new framework,FRESA, for the computation of measuresbased on Jensen-Shannon divergence.
Fol-lowing the ROUGE approach, FRESA imple-ments word uni-grams, bi-grams and skip n-grams for the computation of divergences.The framework is being made available to thecommunity for research purposes.Although we have made a number of contribu-tions, this paper leaves many questions open thatneed to be addressed.
In order to verify correlationbetween ROUGE and JS, in the short term we in-tend to extend our investigation to other languagesand datasets such as Portuguese and Chinese forwhich we have access to data and summarizationtechnology.
We also plan to apply our evaluationframework to the rest of the DUC and TAC sum-marization tasks to have a full picture of the corre-lations among measures with and without humanmodels.
In the long term we plan to incorporate arepresentation of the task/topic in the computationof the measures.AcknowledgementsWe thank three anonymous reviewers for theirvaluable and enthusiastic comments.
HoracioSaggion is grateful to the Programa Ramo?n y Ca-jal from the Ministerio de Ciencia e Innovacio?n,Spain and to a Comenc?a grant from UniversitatPompeu Fabra (COMENC?A10.004).
This workis partially supported by a postdoctoral grant (Na-tional Program for Mobility of Research HumanResources; National Plan of Scientific Research,Development and Innovation 2008-2011) given toIria da Cunha by the Ministerio de Ciencia e In-novacio?n, Spain.Referencesda Cunha, Iria, Leo Wanner, and M. Teresa Cabre?.2007.
Summarization of specialized discourse: Thecase of medical articles in spanish.
Terminology,13(2):249?286.Donaway, Robert L., Kevin W. Drummey, andLaura A. Mather.
2000.
A comparison of rank-ings produced by summarization evaluation mea-sures.
In NAACL-ANLP 2000 Workshop on Au-tomatic Summarization, pages 69?78, Morristown,NJ, USA.
ACL.Fernandez, Silvia, Eric SanJuan, and Juan-ManuelTorres-Moreno.
2007.
Textual Energy of Associa-tive Memories: performants applications of Enertexalgorithm in text summarization and topic segmen-tation.
In MICAI?07, pages 861?871.Kullback, S. and R.A. Leibler.
1951.
On informationand sufficiency.
Annals of Mathematical Statistics,22(1):79?86.Lin, C.-Y.
and E. Hovy.
2003.
Automatic evaluationof summaries using n-gram co-occurrence statistics.In Proceedings of HLT-NAACL 2003, pages 71?78,Morristown, NJ, USA.
ACL.Lin, Chin-Yew, Guihong Cao, Jianfeng Gao, and Jian-Yun Nie.
2006.
An information-theoretic approachto automatic evaluation of summaries.
In Confer-ence on Human Language Technology Conferenceof the North American Chapter of the Associationof Computational Linguistics, pages 463?470, Mor-ristown, NJ, USA.
ACL.Lin, J.
1991a.
Divergence measures based on theshannon entropy.
IEEE Transactions on Informa-tion Theory, 37(145-151).1066Lin, Jianhua.
1991b.
Divergence measures based onthe shannon entropy.
IEEE Transactions on Infor-mation theory, 37:145?151.Lin, Chin-Yew.
2004.
ROUGE: A Package forAutomatic Evaluation of Summaries.
In Marie-Francine Moens, Stan Szpakowicz, editor, TextSummarization Branches Out: ACL-04 Workshop,pages 74?81, Barcelona, Spain, July.Louis, Annie and Ani Nenkova.
2009.
Automati-cally Evaluating Content Selection in Summariza-tion without Human Models.
In Conference on Em-pirical Methods in Natural Language Processing,pages 306?314, Singapore, August.
ACL.Mani, I., G. Klein, D. House, L. Hirschman, T. Firmin,and B. Sundheim.
2002.
Summac: a text summa-rization evaluation.
Natural Language Engineering,8(1):43?68.Nenkova, Ani and Rebecca Passonneau.
2004.
Eval-uating Content Selection in Summarization: ThePyramid Method.
In Proceedings of NAACL-HLT2004.Over, Paul, Hoa Dang, and Donna Harman.
2007.
Ducin context.
Information Processing & Management,43(6):1506?1520.Owkzarzak, Karolina and Hoa Trang Dang.
2009.Evaluation of automatic summaries: Metrics undervarying data conditions.
In Proceedings of the 2009Workshop on Language Generation and Summari-sation (UCNLG+Sum 2009), pages 23?30, Suntec,Singapore, August.
ACL.Papineni, K., S. Roukos, T. Ward, , and W. J. Zhu.2002.
BLEU: a method for automatic evaluationof machine translation.
In ACL?02: 40th Annualmeeting of the Association for Computational Lin-guistics, pages 311?318.Pastra, K. and H. Saggion.
2003.
Colouring sum-maries Bleu.
In Proceedings of Evaluation Initia-tives in Natural Language Processing, Budapest,Hungary, 14 April.
EACL.Radev, Dragomir R., Simone Teufel, Horacio Sag-gion, Wai Lam, John Blitzer, Hong Qi, Arda C?elebi,Danyu Liu, and Elliott Dra?bek.
2003.
Evaluationchallenges in large-scale document summarization.In ACL, pages 375?382.Saggion, H., D. Radev, S. Teufel, and W. Lam.
2002.Meta-evaluation of Summaries in a Cross-lingualEnvironment using Content-based Metrics.
In Pro-ceedings of COLING 2002, pages 849?855, Taipei,Taiwan, August 24-September 1.Spa?rck-Jones, Karen and Julia Rose Galliers, editors.1996.
Evaluating Natural Language ProcessingSystems, An Analysis and Review, volume 1083 ofLecture Notes in Computer Science.
Springer.Spiegel, S. and N.J. Castellan, Jr. 1998.
Nonparamet-ric Statistics for the Behavioral Sciences.
McGraw-Hill International.Torres-Moreno, Juan-Manuel, Patricia Velz?quez-Morales, and Jean-Guy Meunier.
2002.
Condenss?de textes par des me?thodes numr?iques.
In JADT?02,volume 2, pages 723?734, St Malo, France.Vivaldi, Jorge, Iria da Cunha, Juan-Manuel Torres-Moreno, and Patricia Vela?zquez-Morales.
2010.Automatic summarization using terminological andsemantic resources.
In LREC?10, volume 2,page 10, Malta.Yatsko, V.A.
and T.N.
Vishnyakov.
2007.
A methodfor evaluating modern systems of automatic textsummarization.
Automatic Documentation andMathematical Linguistics, 41(3):93?103.1067
