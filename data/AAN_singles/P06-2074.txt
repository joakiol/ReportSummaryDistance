Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 571?578,Sydney, July 2006. c?2006 Association for Computational LinguisticsARE: Instance Splitting Strategies for Dependency Relation-basedInformation ExtractionMstislav Maslennikov Hai-Kiat Goh Tat-Seng ChuaDepartment of Computer ScienceSchool of ComputingNational University of Singapore{maslenni, gohhaiki, chuats}@ comp.nus.edu.sgAbstractInformation Extraction (IE) is a fundamen-tal technology for NLP.
Previous methodsfor IE were relying on co-occurrence rela-tions, soft patterns and properties of thetarget (for example, syntactic role), whichresult in problems of handling paraphrasingand alignment of instances.
Our systemARE (Anchor and Relation) is based on thedependency relation model and tacklesthese problems by unifying entities accord-ing to their dependency relations, which wefound to provide more invariant relationsbetween entities in many cases.
In order toexploit the complexity and characteristicsof relation paths, we further classify the re-lation paths into the categories of ?easy?,?average?
and ?hard?, and utilize differentextraction strategies based on the character-istics of those categories.
Our extractionmethod leads to improvement in perform-ance by 3% and 6% for MUC4 and MUC6respectively as compared to the state-of-artIE systems.1 IntroductionInformation Extraction (IE) is one of the funda-mental problems of natural language processing.Progress in IE is important to enhance results insuch tasks as Question Answering, InformationRetrieval and Text Summarization.
Multiple effortsin MUC series allowed IE systems to achieve near-human performance in such domains as biological(Humphreys et al, 2000), terrorism (Kaufmann,1992; Kaufmann, 1993) and management succes-sion (Kaufmann, 1995).The IE task is formulated for MUC series asfilling of several predefined slots in a template.
Theterrorism template consists of slots Perpetrator,Victim and Target; the slots in the managementsuccession template are Org, PersonIn, PersonOutand Post.
We decided to choose both terrorism andmanagement succession domains, from MUC4 andMUC6 respectively, in order to demonstrate thatour idea is applicable to multiple domains.Paraphrasing of instances is one of the crucialproblems in IE.
This problem leads to data sparse-ness in situations when information is expressed indifferent ways.
As an example, consider the ex-cerpts ?Terrorists attacked victims?
and ?Victimswere attacked by unidentified terrorists?.
Theseinstances have very similar semantic meaning.However, context-based approaches such asAutoslog-TS by Riloff (1996) and Yangarber et al(2002) may face difficulties in handling these in-stances effectively because the context of entity?victims?
is located on the left context in the firstinstance and on the right context in the second.
Forthese cases, we found that we are able to verify thecontext by performing dependency relation parsing(Lin, 1997), which outputs the word ?victims?
as anobject in both instances, with ?attacked?
as a verband ?terrorists?
as a subject.
After grouping of samesyntactic roles in the above examples, we are ableto unify these instances.Another problem in IE systems is word align-ment.
Insertion or deletion of tokens prevents in-stances from being generalized effectively duringlearning.
Therefore, the instances ?Victims wereattacked by terrorists?
and ?Victims were recentlyattacked by terrorists?
are difficult to unify.
Thecommon approach adopted in GRID by Xiao et al(2003) is to apply more stable chunks such as nounphrases and verb phrases.
Another recent approachby Cui et al (2005) utilizes soft patterns for prob-abilistic matching of tokens.
However, a longerinsertion leads to a more complicated structure, asin the instance ?Victims, living near the shop, wentout for a walk and were attacked by terrorists?.Since there may be many inserted words, both ap-proaches may also be inefficient for this case.
Simi-lar to the paraphrasing problem, the word align-ment problem may be handled with dependencyrelations in many cases.
We found that the relationsubject-verb-object for words ?victims?, ?attacked?and ?terrorists?
remains invariant for the above twoinstances.Before IE can be performed, we need to iden-tify sentences containing possible slots.
This is571done through the identification of cue phraseswhich we call anchors or anchor cues.
However,natural texts tend to have diverse terminologies,which require semantic features for generalization.These features include semantic classes, NamedEntities (NE) and support from ontology (for ex-ample, synsets in Wordnet).
If such features arepredefined, then changes in terminology (for in-stance, addition of new terrorism organization) willlead to a loss in recall.
To avoid this, we exploitautomatic mining techniques for anchor cues.
Ex-amples of anchors are the words ?terrorists?
or?guerrilla?
that signify a possible candidate for thePerpetrator slot.From the reviewed works, we observe that theinefficient use of relations causes problems ofparaphrasing and alignment and the related datasparseness problem in current IE systems.
As a re-sult, training and testing instances in the systemsoften lack generality.
This paper aims to tacklethese problems with the help of dependency rela-tion-based model for IE.
Although dependency re-lations provide invariant structures for many in-stances as illustrated above, they tend to be effi-cient only for short sentences and make errors onlong distance relations.
To tackle this problem, weclassify relations into ?simple?, ?average?
and?hard?
categories, depending on the complexity ofthe dependency relation paths.
We then employdifferent strategies to perform IE in each category.The main contributions of our work are as fol-lows.
First, we propose a dependency relationbased model for IE.
Second, we perform classifica-tion of instances into several categories based onthe complexity of dependency relation structures,and employ the action promotion strategy to tacklethe problem of long distance relations.The remaining parts of the paper are organizedas follows.
Section 2 discusses related work andSection 3 introduces our approach for constructingARE.
Section 4 introduces our method for splittinginstances into categories.
Section 5 describes ourexperimental setups and results and, finally, Sec-tion 6 concludes the paper.2 Related workThere are several research directions in InformationExtraction.
We highlight a few directions in IEsuch as case frame based modeling in PALKA byKim and Moldovan (1995) and CRYSTAL by So-derland et al (1995); rule-based learning inAutoslog-TS by Riloff et al (1996); and classifica-tion-based learning by Chieu et al (2002).
Al-though systems representing these directions havevery different learning models, paraphrasing andalignment problems still have no reliable solution.Case frame based IE systems incorporate do-main-dependent knowledge in the processing andlearning of semantic constraints.
However, concepthierarchy used in case frames is typically encodedmanually and requires additional human labor forporting across domains.
Moreover, the systemstend to rely on heuristics in order to match caseframes.
PALKA by Kim and Moldovan (1995) per-forms keyword-based matching of concepts, whileCRYSTAL by Soderland et al (1995) relied onadditional domain-specific annotation and associ-ated lexicon for matching.Rule-based IE models allow differentiation ofrules according to their performance.
Autoslog-TSby Riloff (1996) learns the context rules for extrac-tion and ranks them according to their performanceon the training corpus.
Although this approach issuitable for automatic training, Xiao et al (2004)stated that hard matching techniques tend to havelow recall due to data sparseness problem.
To over-come this problem, (LP)2 by Ciravegna (2002) util-izes rules with high precision in order to improvethe precision of rules with average recall.
However,(LP)2 is developed for semi-structured textual do-main, where we can find consistent lexical patternsat surface text level.
This is not the same for free-text, in which different order of words or an extraclause in a sentence may cause paraphrasing andalignment problems respectively, such as the ex-ample excerpts ?terrorists attacked peasants?
and?peasants were attacked 2 months ago by terrorists?.The classification-based approaches such as byChieu and Ng (2002) tend to outperform rule-basedapproaches.
However, Ciravegna (2001) arguedthat it is difficult to examine the result obtained byclassifiers.
Thus, interpretability of the learnedknowledge is a serious bottleneck of the classifica-tion approach.
Additionally, Zhou and Su (2002)trained classifiers for Named Entity extraction andreported that performance degrades rapidly if thetraining corpus size is below 100KB.
It implies thathuman experts have to spend long hours to annotatea sufficiently large amount of training corpus.Several recent researches focused on the ex-traction of relationships using classifiers.
Roth andYih (2002) learned the entities and relations to-gether.
The joint learning improves the perform-ance of NE recognition in cases such as ?X killedY?.
It also prevents the propagation of mistakes inNE extraction to the extraction of relations.
How-ever, long distance relations between entities arelikely to cause mistakes in relation extraction.
Apossible approach for modeling relations of differ-ent complexity is the use of dependency-based ker-nel trees in support vector machines by Culotta andSorensen (2004).
The authors reported that non-relation instances are very heterogeneous, and572hence they suggested the additional step of extract-ing candidate relations before classification.3 Our approachDiffering from previous systems, the languagemodel in ARE is based on dependency relationsobtained from Minipar by Lin (1997).
In the firststage, ARE tries to identify possible candidates forfilling slots in a sentence.
For example, words suchas ?terrorist?
or ?guerrilla?
can fill the slot for Per-petrator in the terrorism domain.
We refer to thesecandidates as anchors or anchor cues.
In the sec-ond stage, ARE defines the dependency relationsthat connect anchor cues.
We exploit dependencyrelations to provide more invariant structures forsimilar sentences with different syntactic structures.After extracting the possible relations between an-chor cues, we form several possible parsing pathsand rank them.
Based on the ranking, we choosethe optimal filling of slots.Ranking strategy may be unnecessary in caseswhen entities are represented in the SVO form.Ranking strategy may also fail in situations of longdistance relations.
To handle such problems, wecategorize the sentences into 3 categories of: sim-ple, average and hard, depending on the complexityof the dependency relations.
We then apply differ-ent strategies to tackle sentences in each categoryeffectively.
The following subsections discuss de-tails of our approach.Features Perpetrator_Cue(A)Action_Cue(D)Victim_Cue(A)Target_Cue(A)Lexical(Headnoun)terrorists,individuals,soldiersattacked,murder,massacremayor,general,priestsbridge,house,ministryPart-of-SpeechNoun Verb Noun NounNamedEntitiesSoldiers(PERSON)- Jesuit priests(PERSON)WTC(OBJECT)Synonyms Synset 130, 166 Synset 22 Synset 68 Synset 71ConceptClassID 2, 3 ID 9  ID 22, 43 ID 61, 48Co-referencedentityHe -> terrorist,soldier- They ->peasants-Table 1.
Linguistic features for anchor extractionEvery token in ARE may be represented at adifferent level of representations, including: Lexi-cal, Part-of-Speech, Named Entities, Synonyms andConcept classes.
The synonym set and conceptclasses are mainly obtained from Wordnet.
We useNLProcessor from Infogistics Ltd for the extractionof part-of-speech, noun phrases and verb phrases(we refer to them as phrases).
Named Entities areextracted with the program used in Yang et al(2003).
Additionally, we employed the co-reference module for the extraction of meaningfulpronouns.
It is used for linking entities acrossclauses or sentences, for example in ?John works inXYZ Corp.
He was appointed as a vice-president amonth ago?
and could achieve an accuracy of 62%.After preprocessing and feature extraction, we ob-tain the linguistic features in Table 1.3.1 Mining of anchor cuesIn order to extract possible anchors and relationsfrom every sentence, we need to select features tosupport the generalization of words.
This generali-zation may be different for different classes ofwords.
For example, person names may be general-ized as a Named Entity PERSON, whereas for?murder?
and ?assassinate?, the optimal generaliza-tion would be the concept class ?kill?
in the Word-Net hypernym tree.
To support several generaliza-tions, we need to store multiple representations ofevery word or token.Mining of anchor cues or anchors is crucial inorder to unify meaningful entities in a sentence, forexample words ?terrorists?, ?individuals?
and ?sol-diers?
from Table 1.
In the terrorism domain, weconsider 4 types of anchor cues: Perpetrator, Action,Victim, and Target of destruction.
For managementsuccession domain, we have 6 types: Post, PersonIn, Person Out, Action and Organization.
Each setof anchor cues may be seen as a pre-defined se-mantic type where the tokens are mined automati-cally.
The anchor cues are further classified intotwo categories: general type A and action type D.Action type anchor cues are those with verbs orverb phrases describing a particular action ormovement.
General type encompasses any prede-fined type that does not fall under the action typecues.In the first stage, we need to extract anchorcues for every type.
Let P be an input phrase, andAj be the anchor of type j that we want to match.The similarity score of P for Aj in sentence S isgiven by:Phrase_Scores(P,Aj)=?1* S_lexicalS(P,Aj+?2* S_POSS(P,Aj)+?3* S_NES(P,Aj) +?4 * S_SynS(P,Aj)+?5* S_Concept-ClassS(P,Aj)   (1)where S_XXXS(P,Aj) is a score function for the typeAj and ?i is the importance weight for Aj.
In order toextract the score function, we use entities fromslots in the training instances.
Each S_XXXS(P,Aj) iscalculated as a ratio of occurrence in positive slotsversus all the slots:)2()(#)(#),(_jjjS AtypetheofslotsallAtypetheofslotspositiveinPAPXXXS =We classify the phrase P as belonging to an anchorcue A of type j if Phrase_ScoreS(P,Aj) ?
?, where?
is an empirically determined threshold.
Theweights ( )51 ,..., ???
= are learned automaticallyusing Expectation Maximization by Dempster et al(1977).
Using anchors from training instances asground truth, we iteratively input different sets ofweights into EM to maximize the overall score.573Consider the excerpts ?Terrorists attackedvictims?, ?Peasants were murdered by unidentifiedindividuals?
and ?Soldiers participated in massacreof Jesuit priests?.
Let Wi denotes the position oftoken i in the instances.
After mining of anchors,we are able to extract meaningful anchor cues inthese sentences as shown in Table 2:W-3 W-2 W-1 W0 W1 W2 W3Perp_Cue Action_Cue Victim_CueVictim_Cue were Action_Cue byIn Action_Cue Of Victim_CueTable 2.
Instances with anchor cues3.2 Relationship extraction and rankingIn the next stage, we need tofind meaningful relations tounify instances using the anchorcues.
This unification is doneusing dependency trees of sen-tences.
The dependencyrelations for the first sentenceare given in Figure 1.From the dependency tree, we need to identifythe SVO relations between anchor cues.
In caseswhen there are multiple relations linking many po-tential subjects, verbs or objects, we need to selectthe best relations under the circumstances.
Ourscheme for relation ranking is as follows.First, we rank each single relation individuallybased on the probability that it appears in the re-spective context template slot in the training data.We use the following formula to capture the qualityof a relation Rel which gives higher weight to morefrequently occurring relations:)3(||}|{||||},|{||),,( 21 ??
?=?=S iiiS iiiSRRelRRRRRAAleRQualitywhere S is a set of sentences containing relationRel, anchors A1 and A2; R denotes relation path con-necting A1 and A2 in a sentence Si; ||X|| denotes sizeof the set X.Second, we need to take into account the entityheight in the dependency tree.
We calculate heightas a distance to the root node.
Our intuition is thatthe nodes on the higher level of dependency treeare more important, because they may be linked tomore nodes or entities.
The following example inFigure 2 illustrates it.Figure 2.
Example of entity in a dependency treeHere, the node ?terrorists?
is the most representativein the whole tree, and thus relations nearer to ?ter-rorists?
should have higher weight.
Therefore, wegive a slightly higher weight to the links that arecloser to the root node as follows:Heights(Rel) = log2(Const ?
Distance(Root, Rel))         (4)where Const is set to be larger than the depth ofnodes in the tree.Third, we need to calculate the score of rela-tion path Ri->j between each pair of anchors Ai andAj, where Ai and Aj belong to different anchor cuetypes.
The path score of Ri->j depends on both qual-ity and height of participating relations:Scores(Ai, Aj)=?Ri?R {Heights(Ri)*Quality(Ri)}/Lengthij   (5)where Lengthij is the length of path Ri->j.
Divisionon Lengthij allows normalizing Score against thelength of Ri->j.
The formula (5) tends to give higherscores to shorter paths.
Therefore, the path endingwith ?terrorist?
will be preferred in the previousexample to the equivalent path ending with?MRTA?.Finally, we need to find optimal filling of atemplate T. Let C = {C1, .. , CK} be the set of slottypes in T and A = {A1, .., AL} be the set of ex-tracted anchors.
First, we regroup anchors A ac-cording to their respective types.
Let},...,{ )()(1)( kLkkkAAA =  be the projection of A ontothe type Ck, ?k?N, k ?
K. Let F = A(1) ?
A(2) ?..
?A(K) be the set of possible template fillings.
Theelements of F are denoted as F1, ..,FM, where everyFi ?
F is represented as Fi = {Ai(1),..,Ai(K)}.
Our aimis to evaluate F and find the optimal filling F0 ?
F.For this purpose, we use the previously calculatedscores of relation paths between every two anchorsAi and Aj.Based on the previously defined ScoreS(Ai, Aj),it is possible to rank all the fillings in F. For eachfilling Fi?F we calculate the aggregate score for allthe involved anchor pairs:)7(),()(_ ,1MAAcoreSFScoreelationRjiSKjiiS?
?
?=where K is number of slot types and M denotes thenumber of relation paths between anchors in Fi.After calculating Relation_ScoreS(Fi), it is usedfor ranking all possible template fillings.
The nextstep is to join entity and relation scores.
We definedthe entity score of Fi as an average of the scores ofparticipating anchors:)8(/)(_)(_1)(?
?
?= Kk kiSiS KAScorePhraseFScoreEntityWe combine entity and relation scores of Fi into theoverall formula for ranking.RankS(Fi)=?*Entity_ScoreS(Fi)+(1-?
)*Relation_ScoreS(Fi )      (9)The application of Subject-Verb-Object (SVO)relations facilitates the grouping of subjects,Figure 1.Dependency tree574verbs and objects together.
For the 3 instances inTable 2 containing the anchor cues, the unifiedSVO relations are given in Table 3.W-2 W-1 W0 Instance isPerp_Cue attacked Victim_Cue +Perp_Cue murdered Victim_Cue +Perp_Cue participated ?
-Table 3.
Unification based on SVO relationsThe first 2 instances are unified correctly.
Theonly exception is the slot in the third case, whichis missing because the target is not an object of?participated?.4 Category SplittingThrough our experiments, we found that the com-bination of relations and anchors are essential forimproving IE performance.
However, relationsalone are not applicable across all situations be-cause of long distance relations and possible de-pendency relation parsing errors, especially forlong sentences.
Since the relations in long sen-tences are often complicated, parsing errors arevery difficult to avoid.
Furthermore, application ofdependency relations on long sentences may lead toincorrect extractions and decrease the performance.Through the analysis of instances, we noticedthat dependency trees have different complexity fordifferent sentences.
Therefore, we decided to clas-sify sentences into 3 categories based on the com-plexity of dependency relations between the actioncues (V) and the likely subject (S) and object cues(O).
Category 1 is when the potential SVO?s areconnected directly to each other (simple category);Category 2 is when S or O is one link away from Vin terms of nouns or verbs (average category); andCategory 3 is when the path distances between po-tential S, V, and Os are more than 2 links away(hard category).Figure 3.
Simple category   Figure 4.
Average categoryFigure 3 and Figure 4 illustrate the dependencyparse trees for the simple and average categoriesrespectively derived from the sentences: ?50 peas-ants of have been kidnapped by terrorists?
and ?acolonel was involved in the massacre of the Jesu-its?.
These trees represent 2 common structures inthe MUC4 domain.
By taking advantage of thiscommonality, we can further improve the perform-ance of extraction.
We notice that in the simplecategory, the perpetrator cue (?terrorists?)
is alwaysa subject, action cue (?kidnapped?)
a verb, and vic-tim cue (?peasants?)
an object.
For the averagecategory, perpetrator and victim commonly appearunder 3 relations: subject, object and pcomp-n. Themost difficult category is the hard category, sincein this category relations can be distant.
We thusprimarily rely on anchors for extraction and have togive less importance to dependency parsing.In order to process the different categories, weutilize the specific strategies for each category.
Asan example, the instance ?X murdered Y?
requiresonly the analysis of the context verb ?murdered?
inthe simple category.
It is different from the in-stances ?X investigated murder of Y?
and ?X con-ducted murder of Y?
in the average category, inwhich transition of word ?investigated?
into ?con-ducted?
makes X a perpetrator.
We refer to the an-chor ?murder?
in the first and second instances aspromotable and non-promotable respectively.
Ad-ditionally, we denote that the token ?conducted?
isthe optimal node for promotion of ?murder?,whereas the anchor ?investigate?
is not.
This exam-ple illustrates the importance of support verb analy-sis specifically for the average category.Figure 5.
Category processingThe main steps of our algorithm for performing IEin different categories are given in Figure 5.
Al-though some steps are common for every category,the processing strategies are different.Simple categoryFor simple category, we reorder tokens accordingto their slot types.
Based on this reordering, we fillthe template.Algorithm1) Analyze categoryIf(simple)- Perform token reordering based on SVO relationsElse if (average) ProcessAverageElse ProcessHard2) Fill template slotsFunction ProcessAverage1) Find the nearest missing anchor in the previous sentences2) Find the optimal linking node for action anchor in every Fi3) Find the filling Fi(0) = argmaxi Rank(Fi)4) Use Fi for filling the template if Rank0 > ?2, where ?2 is anempirical thresholdFunction ProcessHard1) Perform token reordering based on anchors2) Use linguistic+ syntactic + semantic feature of the headnoun.
Eg.
Caps, ?subj?, etc3) Find the optimal linking node for action anchor in every Fi4) Find the filling Fi(0) = argmaxi Rank(Fi)5) Use Fi for filling the template if Rank0 > ?3, where ?3 is anempirical threshold575Average categoryFor average category, our strategy consists of 4steps.
First, in the case of missing anchor type wetry to find it in the nearest previous sentence.
Con-sider an example from MUC-6: ?Look at what hap-pened to John Sculley, Apple Computer's formerchairman.
Earlier this month he abruptly resignedas chairman of troubled Spectrum InformationTechnologies.?
In this example, a noisy cue ?he?needs to be substituted with ?John Sculley?, whichis a strong anchor cue.
Second, we need to find anoptimal promotion of a support verb.
For example,in ?X conducted murder of Y?, the verb ?murder?should be linked with X and in the excerpt ?X in-vestigated murder of Y?, it should not be promoted.Thus, we need to make 2 steps for promotion: (a)calculate importance of every word connecting theaction cue such as ?murder?
and ?distributed?
and (b)find the optimal promotion for the word ?murder?.Third, using the predefined threshold ?
we cutoffthe instances with irrelevant support verbs (e.g.,?investigated?).
Fourth, we reorder the tokens inorder to group them according to the anchor types.The following algorithm in Figure 6 estimatesthe importance of a token W for type D in the sup-port verb structure.
The input of the algorithm con-sists of sentences S1?SN and two sets of tokensVneg, Vpos co-occurring with anchor cue of type D.Vneg and Vpos are automatically tagged as irrelevantand relevant respectively based on preliminarymarked keys in the training instances.
The algo-rithm output represents the importance value be-tween 0 to 1.Figure 6.
Evaluation of word importanceWe use the linguistic features for W and D as givenin Table 1 to form the instances.Hard categoryIn the hard category, we have to deal with long-distance relations: at least 2 anchors are more than2 links away in the dependency tree.
Consequently,dependency tree alone is not reliable for connectingnodes.
To find an optimal connection, we primarilyrely on comparison between several possible fill-ings of slots based on previously extracted anchorcues.
Depending on the results of such comparison,we chose the filling that has the highest score.
Asan example, consider the hard category in the ex-cerpt ?MRTA today distributed leaflets claimingresponsibility for the murder of former defenseminister Enrique Lopez Albujar?.
The dependencytree for this instance is given in Figure 7.Although words ?MRTA?, ?murder?
and ?min-ister?
might be correctly extracted as anchors, thechallenging problem is to decide whether ?MRTA?is a perpetrator.
Anchors ?MRTA?
and ?minister?are connected via the verb ?distributed?.
However,the word ?murder?
belongs to another branch of thisverb.Figure 7.
Hard caseProcessing of such categories is challenging.Since relations are not reliable, we first need to relyon the anchor extraction stage.
Nevertheless, thepromotion strategy for the anchor cue ?murder?
isstill possible, although the corresponding branch inthe dependency tree is long.
Henceforth, we try toreplace the verb ?distributed?
by promoting the an-chor ?murder?.
To do so, we need to evaluatewhether the nodes in between may be eliminated.For example, such elimination is possible in thepairs ?conducted?
-> ?murder?
and not possible inthe pair ?investigated?
-> ?murder?, since in the ex-cerpt ?X investigated murder?
X is not a perpetra-tor.
If the elimination is possible, we apply thepromotion algorithm given on Figure 8:Figure 8.
Token promotion algorithmThe algorithm checks path Pj1->j2 that connect an-chors Ai(j1) and Ai(j2) in the filling Fi; the nodes fromPj1->j2 are added to the set Z.
Finally, the top nodeof the set Z is chosen as an optimal node for thepromotion.
The example optimal node for promo-tion of the word ?murder?
on Figure 7 is the node?distributed?.Another important difference between the hardand average cases is in the calculation of RankS (Fi)in Equation (9).
We set ?hard > ?average because longdistance relations are less reliable in the hard casethan in the average case.CalculateImportance (W, D)1) Select sentences that contain anchor cue D2) Extract linguistic features of Vpos, Vneg and D3) Train using SVM on instances (Vpos,D) andinstances (Vneg,D)4) Return Importance(W) using SVMFindOptimalPromotion (Fi)1) Z = ?2) For each Ai(j1), Ai(j2) ?
FiZ = Z ?
Pj1->j2End_for3) Output Top(Z)5765 EvaluationIn order to evaluate the efficiency of our method,we conduct our experiments in 2 domains: MUC4(Kaufmann, 1992) and MUC6 (Kaufmann, 1995).The official corpus of MUC4 is released withMUC3; it covers terrorism in the Latin Americaregion and consists of 1,700 texts.
Among them,1,300 documents belong to the training corpus.Testing was done on 25 relevant and 25 irrelevanttexts from TST3, plus 25 relevant and 25 irrelevanttexts from TST4, as is done in Xiao et al (2004).MUC6 covers news articles in Management Suc-cession domain.
Its training corpus consists of 1201instances, whereas the testing corpus consists of 76person-ins, 82 person-outs, 123 positions, and 79organizations.
These slots we extracted in order tofill templates on a sentence-by-sentence basis, as isdone by Chieu et al (2002) and Soderland (1999).Our experiments were designed to test theeffectiveness of both case splitting and action verbpromotion.
The performance of ARE is comparedto both the state-of-art systems and our baselineapproach.
We use 2 state-of-art systems for MUC4and 1 system for MUC6.
Our baseline system,Anc+rel, utilizes only anchors and relationswithout category splitting as described in Section 3.For our ARE system with case splitting, we presentthe results on Overall corpus, as well as separateresults on Simple, Average and Hard categories.The Overall performance of ARE represents theresult for all the categories combined together.Additionally, we test the impact of the actionpromotion (in the right column) for the average andhard categories.Without promotion With promotionCase (%) P R F1 P R F1GRID 58% 56% 57% - - -Riloff?05 46% 52% 48% - - -Anc+rel (100%) 58% 59% 58% 58% 59% 58%Overall (100%) 57% 60% 59% 58% 61% 60%Simple (13%) 79% 86% 82% 79% 86% 82%Average (22%) 64% 70% 67% 67% 71% 69%Hard (65%) 50% 52% 51% 51% 53% 52%Table 4.
Results on MUC4 with case splittingThe comparative results are presented in Table4 and Table 5 for MUC4 and MUC6, respectively.First, we review our experimental results on MUC4corpus without promotion (left column) before pro-ceeding to the right column.a) From the results on Table 4 we observe that ourbaseline approach Anc+rel outperforms all thestate-of-art systems.
It demonstrates that both an-chors and relations are useful.
Anchors allow us togroup entities according to their semantic meaningsand thus to select of the most prominent candidates.Relations allow us to capture more invariant repre-sentation of instances.
However, a sentence maycontain very few high-quality relations.
It impliesthat the relations ranking step is fuzzy in nature.
Inaddition, we noticed that some anchor cues may bemissing, whereas the other anchor types may berepresented by several anchor cues.
All these fac-tors lead only to moderate improvement in per-formance, especially in comparison with GRIDsystem.b) Overall, the splitting of instances into categoriesturned out to be useful.
Due to the application ofspecific strategies the performance increased by 1%over the baseline.
However, the large dominance ofthe hard cases (65%) made this improvement mod-est.c) We notice that the amount of variations for con-necting anchor cues in the Simple category is rela-tively small.
Therefore, the overall performance forthis case reaches F1=82%.
The main errors herecome from missing anchors resulting partly frommistakes in such component as NE detection.d) The performance in the Average category isF1=67%.
It is lower than that for the simple cate-gory because of higher variability in relations andnegative influence of support verbs.
For example,for excerpt such as ?X investigated murder of Y?,the processing tends to make mistake without theanalysis of semantic value of support verb ?investi-gated?.e) Hard category achieves the lowest performanceof F1=51% among all the categories.
Since for thiscategory we have to rely mostly on anchors, theproblem arises if these anchors provide the wrongclues.
It happens if some of them are missing or arewrongly extracted.
The other cause of mistakes iswhen ARE finds several anchor cues which belongto the same type.Additional usage of promotion strategies al-lowed us to improve the performance further.f) Overall, the addition of promotion strategy en-ables the system to further boost the performance toF1=60%.
It means that the promotion strategy isuseful, especially for the average case.
The im-provement in comparison to the state-of-art systemGRID is about 3%.g) It achieved an F1=69%, which is an improve-ment of 2%, for the Average category.
It impliesthat the analysis of support verbs helps in revealingthe differences between the instances such as ?Xwas involved in kidnapping of Y?
and ?X reportedkidnapping of Y?.h) The results in the Hard category improved mod-erately to F1=52%.
The reason for the improvementis that more anchor cues are captured after thepromotion.
Still, there are 2 types of common mis-577takes: 1) multiple or missing anchor cues of thesame type and 2) anchors can be spread across sev-eral sentences or several clauses in the same sen-tence.Without promotion With promotionCase (%) P R F1 P R F1Chieu et al?02 74% 49% 59% - - -Anc+rel (100%) 78% 52% 62% 78% 52% 62%Overall (100%) 72% 58% 64% 73% 58% 65%Simple (45%) 85% 67% 75% 87% 68% 76%Average (27%) 61% 55% 58% 64% 56% 60%Hard (28%) 59% 44% 50% 59% 44% 50%Table 5.
Results on MUC6 with case splittingFor the MUC6 results given in Table 5, we ob-serve that the overall improvement in performanceof ARE system over Chieu et al?02 is 6%.
Thetrends of results for MUC6 are similar to that inMUC4.
However, there are few important differ-ences.
First, 45% of instances in MUC6 fall intothe Simple category, therefore this category domi-nates.
The reason for this is that the terminologiesused in Management Succession domain are morestable in comparison to the Terrorism domain.
Sec-ond, there are more anchor types for this case andtherefore the promotion strategy is applicable alsoto the simple case.
Third, there is no improvementin performance for the Hard category.
We believethe primary reason for it is that more stable lan-guage patterns are used in MUC6.
Therefore, de-pendency relations are also more stable in MUC6and the promotion strategy is not very important.Similar to MUC4, there are problems of missinganchors and mistakes in dependency parsing.6 ConclusionThe current state-of-art IE methods tend to use co-occurrence relations for extraction of entities.
Al-though context may provide a meaningful clue, theuse of co-occurrence relations alone has seriouslimitations because of alignment and paraphrasingproblems.
In our work, we proposed to utilize de-pendency relations to tackle these problems.
Basedon the extracted anchor cues and relations betweenthem, we split instances into ?simple?, ?average?and ?hard?
categories.
For each category, we ap-plied specific strategy.
This approach allowed us tooutperform the existing state-of-art approaches by3% on Terrorism domain and 6% on ManagementSuccession domain.
In our future work we plan toinvestigate the role of semantic relations and inte-grate ontology in the rule generation process.
An-other direction is to explore the use of bootstrap-ping and transduction approaches that may requireless training instances.ReferencesH.L.
Chieu and H.T.
Ng.
2002.
A Maximum Entropy Ap-proach to Information Extraction from Semi-Structuredand Free Text.
In Proc of AAAI-2002, 786-791.H.
Cui, M.Y.
Kan, and Chua T.S.
2005.
Generic Soft Pat-tern Models for Definitional Question Answering.
InProc of ACM SIGIR-2005.A.
Culotta and J. Sorensen J.
2004.
Dependency tree kernelsfor relation extraction.
In Proc of ACL-2004.F.
Ciravegna.
2001.
Adaptive Information Extraction fromText by Rule Induction and Generalization.
In Proc ofIJCAI-2001.A.
Dempster, N. Laird, and D. Rubin.
1977.
Maximum like-lihood from incomplete data via the EM algorithm.
Jour-nal of the Royal Statistical Society B, 39(1):1?38K.
Humphreys, G. Demetriou and R. Gaizuskas.
2000.
Twoapplications of Information Extraction to Biological Sci-ence: Enzyme interactions and Protein structures.
InProc of the Pacific Symposium on Biocomputing, 502-513M.
Kaufmann.
1992.
MUC-4.
In Proc of  MUC-4.M.
Kaufmann.
1995.
MUC-6.
In Proc of MUC-6.J.
Kim and D. Moldovan.
1995.
Acquisition of linguisticpatterns for knowledge-based information extraction.IEEE Transactions on KDE, 7(5): 713-724D.
Lin.
1997.
Using Syntactic Dependency as Local Contextto Resolve Word Sense Ambiguity.
In Proc of ACL-97.E.
Riloff.
1996.
Automatically Generating Extraction Pat-terns from Untagged Text.
In Proc of AAAI-96, 1044-1049.D.
Roth and W. Yih.
2002.
Probabilistic Reasoning for En-tity & Relation Recognition.
In Proc of COLING-2002.S.
Soderland, D. Fisher, J. Aseltine and W. Lehnert.
1995.Crystal: Inducing a Conceptual Dictionary.
In Proc ofIJCAI-95, 1314-1319.S.
Soderland.
1999.
Learning Information Extraction Rulesfor Semi-Structured and Free Text.
Machine Learning34:233-272.J.
Xiao, T.S.
Chua and H. Cui.
2004.
Cascading Use of Softand Hard Matching Pattern Rules for Weakly SupervisedInformation Extraction.
In Proc of COLING-2004.H.
Yang, H. Cui, M.-Y.
Kan, M. Maslennikov, L. Qiu andT.-S. Chua.
2003.
QUALIFIER in TREC 12 QA MainTask.
In Proc of TREC-12, 54-65.R.
Yangarber, W. Lin, R. Grishman.
2002.
UnsupervisedLearning of Generalized Names.
In Proc of COLING-2002.G.D.
Zhou and J. Su.
2002.
Named entity recognition usingan HMM-based chunk tagger.
In Proc of ACL-2002,473-480578
