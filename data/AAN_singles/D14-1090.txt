Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 831?843,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsRelieving the Computational Bottleneck: Joint Inference forEvent Extraction with High-Dimensional FeaturesDeepak Venugopal and Chen Chen and Vibhav Gogate and Vincent NgDepartment of Computer Science and Human Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688dxv021000@utdallas.edu, {yzcchen,vgogate,vince}@hlt.utdallas.eduAbstractSeveral state-of-the-art event extraction sys-tems employ models based on Support Vec-tor Machines (SVMs) in a pipeline architec-ture, which fails to exploit the joint depen-dencies that typically exist among eventsand arguments.
While there have been at-tempts to overcome this limitation usingMarkov Logic Networks (MLNs), it re-mains challenging to perform joint infer-ence in MLNs when the model encodesmany high-dimensional sophisticated fea-tures such as those essential for event ex-traction.
In this paper, we propose a newmodel for event extraction that combinesthe power of MLNs and SVMs, dwarfingtheir limitations.
The key idea is to reli-ably learn and process high-dimensionalfeatures using SVMs; encode the outputof SVMs as low-dimensional, soft formu-las in MLNs; and use the superior joint in-ferencing power of MLNs to enforce jointconsistency constraints over the soft for-mulas.
We evaluate our approach for thetask of extracting biomedical events onthe BioNLP 2013, 2011 and 2009 Geniashared task datasets.
Our approach yieldsthe best F1 score to date on the BioNLP?13(53.61) and BioNLP?11 (58.07) datasetsand the second-best F1 score to date on theBioNLP?09 dataset (58.16).1 IntroductionEvent extraction is the task of extracting and la-beling all instances in a text document that corre-spond to a pre-defined event type.
This task is quitechallenging for a multitude of reasons: events areoften nested, recursive and have several arguments;there is no clear distinction between arguments andevents; etc.
For instance, consider the BioNLP Ge-nia event extraction shared task (N?edellec et al.,2013).
In this task, participants are asked to extractinstances of a pre-defined set of biomedical eventsfrom text.
An event is identified by a keywordcalled the trigger and can have an arbitrary numberof arguments that correspond to pre-defined argu-ment types.
The task is complicated by the factthat an event may serve as an argument of anotherevent (nested events).
An example of the task isshown in Figure 1.
As we can see, event E13 takesas arguments two events, E14 and E12, which inturn has E11 as one of its arguments.A standard method that has been frequently em-ployed to perform this shared task uses a pipelinearchitecture with three steps: (1) detect if a tokenis a trigger and assign a trigger type label to it; (2)for every detected trigger, determine all its argu-ments and assign types to each detected argument;and (3) combine the extracted triggers and argu-ments to obtain events.
Though adopted by thetop-performing systems such as the highest scoringsystem on the BioNLP?13 Genia shared task (Kimet al., 2013), this approach is problematic for atleast two reasons.
First, as is typical in pipelinearchitectures, errors may propagate from one stageto the next.
Second, since each event/argument isidentified and assigned a type independently of theothers, it fails to capture the relationship betweena trigger and its neighboring triggers, an argumentand its neighboring arguments, etc.More recently, researchers have investigatedjoint inference techniques for event extraction us-ing Markov Logic Networks (MLNs) (e.g., Poonand Domingos (2007), Poon and Vanderwende(2010), Riedel and McCallum (2011a)), a statis-tical relational model that enables us to model thedependencies between different instances of a datasample.
However, it is extremely challenging tomake joint inference using MLNs work well inpractice (Poon and Domingos, 2007).
One reasonis that it is generally difficult to model sophisti-cated linguistic features using MLNs.
The diffi-831.
.
.
demonstrated that HOIL-1L interacting protein (HOIP), a ubiquitin ligase that can catalyze the assembly of linearpolyubiquitin chains, is recruited to DC40 in a TRAF2-dependent manner following engagement of CD40 .
.
.
(a) Sentence fragmentID Event Type Trigger ArgumentsE11 Binding recruited Theme={HOIL-1L interacting protein,CD40}E12 Regulation dependent Theme=E11, Cause=TRAF2E13 +ve Regulation following Theme=E12, Cause=E14E14 Binding engagement Theme=CD40(b) EventsFigure 1: Example of event extraction in the BioNLP Genia task.
The table in (b) shows all the eventsextracted from sentence (a).
Note that successful extraction of E13 depends on E12 and E14.culty stems from the fact that some of these fea-tures are extremely high dimensional (e.g., Chenand Ng (2012), Huang and Riloff (2012b), Li et al.
(2012), Li et al.
(2013b), Li et al.
(2013c)), and toreliably learn weights of formulas that encode suchfeatures, one would require an enormous numberof data samples.
Moreover, even the complexity ofapproximate inference on such models is quite high,often prohibitively so.
For example, a trigram canbe encoded as an MLN formula, Word(w1, p?1)?Word(w2, p) ?
Word(w3, p + 1)?
Type(p, T ).For any given position (p), this formula has W3groundings, where W is the number of possiblewords, making it too large for learning/inference.Therefore, current MLN-based systems tend to in-clude a highly simplified model ignoring powerfullinguistic features.
This is problematic becausesuch features are essential for event extraction.Our contributions in this paper are two-fold.First, we propose a novel model for biomedicalevent extraction based on MLNs that addresses theaforementioned limitations by leveraging the powerof Support Vector Machines (SVMs) (Vapnik,1995; Joachims, 1999) to handle high-dimensionalfeatures.
Specifically, we (1) learn SVM models us-ing rich linguistic features for trigger and argumentdetection and type labeling; (2) design an MLNcomposed of soft formulas (each of which encodesa soft constraint whose associated weight indicateshow important it is to satisfy the constraint) andhard formulas (constraints that always need to besatisfied, thus having a weight of ?)
to capturethe relational dependencies between triggers andarguments; and (3) encode the SVM output as priorknowledge in the MLN in the form of soft formulas,whose weights are computed using the confidencevalues generated by the SVMs.
This formulationnaturally allows SVMs and MLNs to complementeach other?s strengths and weaknesses: learningin a large and sparse feature space is much easierwith SVMs than with MLNs, whereas modelingrelational dependencies is much easier with MLNsthan with SVMs.Our second contribution concerns making infer-ence with this MLN feasible.
Recall that inferenceinvolves detecting and assigning the type label toall the triggers and arguments.
We show that exist-ing Maximum-a-posteriori (MAP) inference meth-ods, even the most advanced approximate ones(e.g., Selman et al.
(1996), Marinescu and Dechter(2009), Sontag and Globerson (2011) ), are infea-sible on our proposed MLN because of their highmemory cost.
Consequently, we identify decompo-sitions of the MLN into disconnected componentsand solve each independently, thereby drasticallyreducing the memory requirements.We evaluate our approach on the BioNLP 2009,2011 and 2013 Genia shared task datasets.
Onthe BioNLP?13 dataset, our model significantlyoutperforms state-of-the-art pipeline approachesand achieves the best F1 score to date.
On theBioNLP?11 and BioNLP?09 datasets, our scoresare slightly better and slightly worse respectivelythan the best reported results.
However, theyare significantly better than state-of-the-art MLN-based systems.2 Background2.1 Related WorkAs a core task in information extraction, event ex-traction has received significant attention in the nat-ural language processing (NLP) community.
Thedevelopment and evaluation of large-scale learning-based event extraction systems was propelled inpart by the availability of annotated corpora pro-duced as part of the Message Understanding Con-ferences (MUCs), the Automatic Content Extrac-tion (ACE) evaluations, and the BioNLP shared832tasks on event extraction.
Previous work on eventextraction can be broadly divided into two cate-gories, one focusing on the development of fea-tures (henceforth feature-based approaches) andthe other focusing on the development of models(henceforth model-based approaches).Feature-based approaches.
Early work onfeature-based approaches has primarily focusedon designing local sentence-level features such astoken and syntactic features (Grishman et al., 2005;Ahn, 2006).
Later, it was realized that local featureswere insufficient to reliably and accurately performevent extraction in complex domains and thereforeseveral researchers proposed using high-level fea-tures.
For instance, Ji and Grishman (2008) usedglobal information from related documents; Guptaand Ji (2009) extracted implicit time information;Patwardhan and Riloff (2009) used broader sen-tential context; Liao and Grishman (2010; 2011)leveraged document-level cross-event informationand topic-based features; and Huang and Riloff(2012b) explored discourse properties.Model-based approaches.
The model-based ap-proaches developed to date have focused on mod-eling global properties and seldom use rich, high-dimensional features.
To capture global event struc-ture properties, McClosky et al.
(2011a) proposeda dependency parsing model.
To extract event ar-guments, Li et al.
(2013b) proposed an IntegerLinear Programming (ILP) model to encode therelationship between event mentions.
To overcomethe error propagation problem associated with thepipeline architecture, several joint models havebeen proposed, including those that are based onMLNs (e.g., Poon and Domingos (2007), Riedel etal.
(2009), Poon and Vanderwende (2010)), struc-tured perceptrons (e.g., Li et al.
(2013c)), and dualdecomposition with minimal domain adaptation(e.g., Riedel and McCallum (2011a; 2011b)).In light of the high annotation cost required bysupervised learning-based event extraction systems,several semi-supervised, unsupervised, and rule-based systems have been proposed.
For instance,Huang and Riloff (2012a) proposed a bootstrap-ping method to extract event arguments using onlya small amount of annotated data; Lu and Roth(2012) developed a novel unsupervised sequencelabeling model; Bui et al.
(2013) implemented arule-based approach to extract biomedical events;and Ritter et al.
(2012) used unsupervised learningto extract events from Twitter data.Our work extends prior work by developing arich framework that leverages sophisticated feature-based approaches as well as joint inference usingMLNs.
This combination gives us the best of bothworlds because on one hand, it is challenging tomodel sophisticated linguistic features using MLNswhile on the other hand, feature-based approachesemploying sophisticated high-dimensional featuressuffer from error propagation as the model is gen-erally not rich enough for joint inference.2.2 The Genia Event Extraction TaskThe BioNLP Shared Task (BioNLP-ST) series(Kim et al.
(2009), Kim et al.
(2011a) and N?edellecet al.
(2013)) is designed to tackle the problem ofextracting structured information from the biomedi-cal literature.
The Genia Event Extraction task is ar-guably the most important of all the tasks proposedin BioNLP-ST and is also the only task organizedin all three events in the series.The 2009 edition of the Genia task (Kim etal., 2009) was conducted on the Genia eventcorpus (Kim et al., 2008), which only containsabstracts of the articles that represent domainknowledge around NF?B proteins.
The 2011 edi-tion (Kim et al., 2011b) augmented the dataset toinclude full text articles, resulting in two collec-tions, the abstract collection and the full text col-lection.
The 2013 edition (Kim et al., 2013) furtheraugmented the dataset with recent full text articlesbut removed the abstract collection entirely.The targeted event types have also changedslightly over the years.
Both the 2009 and 2011editions are concerned with nine fine-grained eventsub-types that can be categorized into three maintypes, namely simple, binding and regulationevents.
These three main event types can be dis-tinguished by the kinds of arguments they take.
Asimple event can take exactly one protein as itsTheme argument.
A binding event can take oneor more proteins as its Theme arguments, and istherefore slightly more difficult to extract than asimple event.
A regulation event takes exactly oneprotein or event as its Theme argument and option-ally one protein or event as its Cause argument.
Ifa regulation event takes another event as its Themeor Cause argument, it will lead to a nested event.Regulation events are considered the most difficult-to-extract among the three event types owing in partto the presence of an optional Cause argument andtheir recursive structure.
The 2013 edition intro-833duced a new event type, protein-mod, and its threesub-types.
Theoretically, a protein-mod event takesexactly one protein as its Theme argument andoptionally one protein or event as its Cause argu-ment.
In practice, however, it rarely occurs: thereare only six protein-mod events having Cause ar-guments in the training data for the 2013 edition.Consequently, our model makes the simplifyingassumption that a protein-mod event can only takeone Theme argument, meaning that we are effec-tively processing protein-mod events in the sameway as simple events.2.3 Markov Logic NetworksStatistical relational learning (SRL) (Getoor andTaskar, 2007) is an emerging field that seeks tounify logic and probability, and since most NLPtechniques are grounded either in logic or proba-bility or both, NLP serves as an ideal applicationdomain for SRL.
In this paper, we will employ apopular SRL approach called Markov logic net-works (MLNs) (Domingos and Lowd, 2009).
At ahigh level, an MLN is a set of weighted first-orderlogic formulas (fi, wi), where wiis the weightassociated with formula fi.
Given a set of con-stants that model objects in the domain, it defines aMarkov network or a log-linear model (Koller andFriedman, 2009) in which we have one node perground first-order atom and a propositional featurecorresponding to each grounding of each first-orderformula.
The weight of the feature is the weight ofthe corresponding first-order formula.Formally, the probability of a world ?, whichrepresents an assignment of values to all groundatoms in the Markov network, is given by:Pr(?)
=1Zexp(?iwiN(fi, ?
))where N(fi, ?)
is the number of groundings of fithat evaluate to True in ?
and Z is a normalizationconstant called the partition function.The key inference tasks over MLNs are com-puting the partition function (Z) and the most-probable explanation given evidence (the MAPtask).
Most queries, including those required byevent extraction, can be reduced to these inferencetasks.
Formally, the partition function and the MAPtasks are given by:Z =?
?exp(?iwiN(fi, ?
))(1)arg max?P (?)
= arg max?
?iwiN(fi, ?)
(2)3 Pipeline ModelWe implement a pipeline event extraction systemusing SVMs.
This pipeline model serves two im-portant functions: (1) providing a baseline for eval-uation and (2) producing prior knowledge for thejoint model.Our pipeline model consists of two steps: trig-ger labeling and argument labeling.
In the triggerlabeling step, we determine whether a candidatetrigger is a true trigger and label each true triggerwith its trigger type.
Then, in the argument label-ing step, we identify the arguments for each truetrigger discovered in the trigger labeling step andassign a role to each argument.We recast each of the two steps as a classificationtask and employ SVMmulticlass(Tsochantaridiset al., 2004) to train the two classifiers.
We describeeach step in detail below.3.1 Trigger LabelingA preliminary study of the BioNLP?13 trainingdata suggests that 98.7% of the true triggers?
headwords1are either verbs, nouns or adjectives.
There-fore, we consider only those words whose part-of-speech tags belong to the above three categoriesas candidate triggers.
To train the trigger classifier,we create one training instance for each candidatetrigger in the training data.
If the candidate triggeris not a trigger, the class label of the correspondinginstance is None; otherwise, the label is the typeof the trigger.
Thus, the number of class labelsequals the number of trigger types plus one.
Eachtraining instance is represented by the features de-scribed in Table 1(a).
These features closely mirrorthose used in state-of-the-art trigger labeling sys-tems such as Miwa et al.
(2010b) and Bj?orne andSalakoski (2013).After training, we apply the resulting trigger clas-sifier to classify the test instances, which are cre-ated in the same way as the training instances.
If atest instance is predicted as None by the classifier,the corresponding candidate trigger is labeled asa non-trigger; otherwise, the corresponding candi-date trigger is posited as a true trigger whose typeis the class value assigned by the classifier.1Head words are found using Collins?
(1999) rules.834(a) Features for trigger labelingToken features The basic token features (see Table 1(c)) computed from (1) the candidate trigger word and (2) thesurrounding tokens in a window of two; character bigrams and trigrams of the candidate trigger word;word n-grams (n=1,2,3) of the candidate trigger word and its context words in a window of three; whetherthe candidate trigger word contains a digit; whether the candidate trigger word contains an upper caseletter; whether the candidate trigger word contains a symbol.DependencyfeaturesThe basic dependency path features (see Table 1(c)) computed using the shortest paths from the candidatetrigger to (1) the nearest protein word, (2) the nearest protein word to its left, and (3) the nearest proteinword to its right.OtherfeaturesThe distances from the candidate trigger word to (1) the nearest protein word, (2) the nearest proteinword to its left, and (3) the nearest protein word to its right; the number of protein words in the sentence.
(b) Features for argument labelingToken features Word n-grams (n=1,2,3) of (1) the candidate trigger word and its context in a window of three and (2) thecandidate argument word and its context in a window of three; the basic token features (see Table 1(c))computed from (1) the candidate trigger word and (2) the candidate argument word; the trigger type ofthe candidate trigger word.DependencyfeaturesThe basic dependency features (see Table 1(c)) computed using the shortest path from the candidatetrigger word to the candidate argument word.OtherfeaturesThe distance between the candidate trigger word and the candidate argument word; the number ofproteins between the candidate trigger word and the candidate argument word; the concatenation of thecandidate trigger word and the candidate argument word; the concatenation of the candidate trigger typeand the candidate argument word.
(c) Basic token and dependency featuresBasic token fea-turesSix features are computed given a token t, including: (a) the lexical string of t, (b) the lemma of t, (c) thestem of t obtained using the Porter stemmer (Porter, 1980), (d) the part-of-speech tag of t, (e) whether tappears as a true trigger in the training data, and (f) whether t is a protein name.BasicdependencyfeaturesSix features are computed given a dependency path p, including: (a) the vertex walk in p, (b) the edgewalk in p, (c) the n-grams (n=2,3,4) of the (stemmed) words associated with the vertices in p, (d) then-grams (n=2,3,4) of the part-of-speech tags of the words associated with the vertices in p, (e) then-grams (n=2,3,4) of the dependency types associated with the edges in p, and (f) the length of p.Table 1: Features for trigger labeling and argument labeling.3.2 Argument LabelingThe argument classifier is trained as follows.
Eachtraining instance corresponds to a candidate triggerand one of its candidate arguments.2A candidateargument for a candidate trigger ct is either a pro-tein or a candidate trigger that appears in the samesentence as ct.
If ct is not a true trigger, the label ofthe associated instance is set toNone.
On the otherhand, if ct is a true trigger, we check whether thecandidate argument in the associated instance is in-deed one of ct?s arguments.
If so, the class label ofthe instance is the argument?s role; otherwise, theclass label is None.
The features used for repre-senting each training instance, which are modeledafter those used in Miwa et al.
(2010b) and Bj?orneand Salakoski (2013), are shown in Table 1(b).After training, we can apply the resulting clas-sifier to classify the test instances, which are cre-ated in the same way as the training instances.
Ifa test instance is assigned the class None by theclassifier, the corresponding candidate argument isclassified as not an argument of the trigger.
Other-2Following the definition of the GENIA event extractiontask, the protein names are provided as part of the input.wise, the candidate argument is a true argument ofthe trigger whose role is the class value assignedby the classifier.4 Joint ModelIn this section, we describe our Markov logic modelthat encodes the relational dependencies in theshared task and uses the output of the pipelinemodel as prior knowledge (soft evidence).
We be-gin by describing the structure of our Markov logicmodel, and then describe the parameter learningand inference algorithms for it.4.1 MLN StructureFigure 2 shows our proposed MLN for BioNLPevent extraction, which we refer to as BioMLN.The MLN contains six predicates.The query predicates in Figure 2(a) are thosewhose assignments are not given during infer-ence and thus need to be predicted.
PredicateTriggerType(sid,tid,ttype!)
is true when thetoken located in sentence sid at position tid hastype ttype.
?ttype, which denotes the set of con-stants (or objects) that the logical variable ttype835TriggerType(sid,tid,ttype!)ArgumentRole(sid,aid,tid,arole!
)(a) QuerySimple(sid,tid)Regulation(sid,tid)(b) HiddenWord(sid,tid,word)DepType(sid,aid,tid,dtype)(c) Evidence1.
?t TriggerType(i,j,t).2.
?a ArgumentRole(i,k,j,a).3.
?TriggerType(i,j,None) ?
?k ArgumentRole(i,k,j,Theme).4.
Simple(i,j) ??
?k ArgumentRole(i,k,j,Cause).5.
TriggerType(i,j,None) ?
ArgumentRole(i,k,j,None).6.
?ArgumentRole(i,k,j,None) ?
?TriggerType(i,k,None) ?
Regulation(i,j).7.
Simple(i,j)?
TriggerType(i,j,Simple1) ?
.
.
.?
TriggerType(i,j,Binding).8.
Regulation(i,j) ?
TriggerType(i,j,Reg) ?
TriggerType(i,j,PosReg)?
TriggerType(i,j,NegReg).9.
Word(i,j,+w) ?
TriggerType(i,j,+t) ?
DepType(i,k,j,+d) ?
ArgumentRole(i,k,j,+a)(d) Joint FormulasFigure 2: The BioMLN structure.can be instantiated to, includes all possible triggertypes in the dataset plus None (which indicatesthat the token is not a trigger).
The ?!?
symbol mod-els commonsense knowledge that only one of thetypes in the domain ?ttypeof ttype is true for everyunique combination of sid and tid.
Similarly, pred-icate ArgumentRole(sid,aid,tid,arole!)
as-serts that a token in sentence sid at position aidplays exactly one argument role, denoted by arole,with respect to the token at position tid.
?aroleincludes the two argument types, namely, Themeand Cause plus the additional None that indicatesthat the token is not an argument.The hidden predicates in Figure 2(b) are ?clus-ters?
of trigger types.
Predicate Simple(sid,tid)is true when the token in sentence sid at posi-tion tid corresponds to one of the Simple eventtrigger types (BioNLP?13 has 9 simple events,BioNLP?09/?11 have 5) or a binding event trig-ger type.
Similarly, Regulation(sid,tid) assertsthat the token in sentence sid at position tid corre-sponds to any of the three regulation event triggertypes.The evidence predicates in Figure 2(c) are thosethat are always assumed to be known during in-ference.
We define two evidence predicates basedon dependency structures.
Word(sid,tid,word) istrue when the word in sentence sid at position tidis equal to word.
DepType(sid,aid,tid,dtype)asserts that dtype is the dependency type in the de-pendency parse tree that connects the token at posi-tion tid to the token at position aid in sentence sid.If the word at tid and the word at aid are directlyconnected in the dependency tree, then dtype is thelabel of dependency edge with direction; otherwisedtype is None.The MLN formulas, expressing commonsense,prior knowledge in the domain (Poon and Van-derwende, 2010; Riedel and McCallum, 2011a),are shown in Fig.
2(d).
All formulas, except For-mula (9), are hard formulas, meaning that they haveinfinite weights.
Note that during weight learning,we only learn the weights of soft formulas.Formulas (1) and (2) along with the ?!?
con-straint in the predicate definition ensure that thetoken types are mutually exclusive and exhaustive.Formula (3) asserts that every trigger should havean argument of type Theme, since a Theme argu-ment is mandatory for any event.
Formula (4) mod-els the constraint that a Simple orBinding triggerhas no arguments of type Cause since only regu-lation events have a Cause.
Formula (5) assertsthat non-triggers have no arguments and vice-versa.Formula (6) models the constraint that if a tokenis both an argument of t and a trigger by itself,then t must belong to one of the three regulationtrigger types.
This formula captures the recursiverelationship between triggers.
Formulas (7) and(8) connect the hidden predicates with the querypredicates.
Formula (9) is a soft formula encoding836the relationship between triggers and arguments ina dependency parse tree.
It joins a word and thedependency type label that connects the word tokento the argument token in the dependency parse treewith the trigger types and argument types of thetwo tokens.
The ?+?
symbol indicates that eachgrounding of Formula (9) may have a differentweight.4.2 Weight LearningWe can learn BioMLN from data either discrimina-tively or generatively.
Since discriminative learningis much faster than generative learning, we use theformer.
In discriminative training, we maximizethe conditional log-likelihood (CLL) of the queryand the hidden variables given an assignment tothe evidence variables.
In principle, we can use thestandard gradient descent algorithm for maximiz-ing the CLL.
In each iteration of gradient descent,we update the weights using the following equation(cf.
Singla and Domingos (2005) and Domingosand Lowd (2009)):wt+1j= wtj?
?(Ew(nj)?
nj) (3)where wtjrepresents the weight of the jthformulain the tthiteration, njis the number of groundingsin which the jthformula is satisfied in the trainingdata, Ew(nj) is the expected number of ground-ings in which the jthformula is satisfied given thecurrent weight vector w, and ?
is the learning rate.As such, the update rule given in Equation (3)is likely to yield poor accuracy because the num-ber of training examples of some types (e.g.,None) far outnumber other types.
To rectify thisill-conditioning problem (Singla and Domingos,2005; Lowd and Domingos, 2007), we divide thegradient with the number of true groundings inthe data, namely, we compute the gradient using(Ew(nj)?nj)nj.Another key issue with using Equation (3) is thatcomputing Ew(nj) requires performing inferenceover the MLN.
This step is intractable, #P-completein the worst case.
To circumvent this problem andfor fast, scalable training, we instead propose touse the voted perceptron algorithm (Collins, 2002;Singla and Domingos, 2005).
This algorithm ap-proximates Ew(nj) by counting the number ofsatisfied groundings of each formula in the MAPassignment.
Computing the MAP assignment ismuch easier (although still NP-hard in the worstcase) than computing Ew(nj), and as a result thevoted perceptron algorithm is more scalable thanthe standard gradient descent algorithm.
In addi-tion, it converges much faster.4.3 TestingIn the testing phase, we combine BioMLN with theoutput of the pipeline model (see Section 3) to ob-tain a new MLN, which we refer to as BioMLN+.For every candidate trigger, the SVM trigger clas-sifier outputs a vector of signed confidence val-ues (which is proportional to the distance fromthe separating hyperplane) of dimension ?ttypewith one entry for each trigger type.
Similarly,for every candidate argument, the SVM argu-ment classifier outputs a vector of signed confi-dence values of dimension ?arolewith one en-try for each argument role.
In BioMLN+, wemodel the SVM output as soft evidence, usingtwo soft unit clauses, TriggerType(i,+j,+t) andArgumentRole(i,+k,+j,+a).
We use the con-fidence values to determine the weights of theseclauses.
Intuitively, higher (smaller) the confidence,higher (smaller) the weight.Specifically, the weights of the soft unit clausesare set as follows.
If the SVM trigger classifierdetermines that the trigger in sentence i at po-sition j belongs to type t with confidence Ci,j,then we attach a weight ofCi,j?nito the clauseTriggerType(i,j,t).
Here, nidenotes the num-ber of trigger candidates in sentence i. Similarly,if the SVM argument classifier determines that thetoken at position k in sentence i belongs to the ar-gument role a with respect to the token at positionj, with confidence C?i,k,j, then we attach a weightofC?i,k,j?
?nij=1mijto the clause ArgumentRole(i, k,j,a).
Here, mijdenotes the number of argumentcandidates for the jthtrigger candidate in sentencei.
?
and ?
act as scale parameters for the confi-dence values ensuring that the weights don?t gettoo large (or too small).4.4 InferenceAs we need to perform MAP inference, both attraining time and at test time, in this subsection wewill describe how to do it efficiently by exploitingunique properties of our proposed BioMLN.Naively, we can perform MAP inference bygrounding BioMLN to a Markov network andthen reducing the Markov network by removingfrom it all (grounded propositional) formulas thatare inconsistent with the evidence.
On the re-837duced Markov network, we can then compute theMAP solution using standard MAP solvers such asMaxWalkSAT (a state-of-the-art local search basedMAP solver) (Selman et al., 1996) and Gurobi3(astate-of-the-art, parallelized ILP solver).The problem with the above approach is thatgrounding the MLN is infeasible in practice; eventhe reduced Markov network is just too large.
Forexample, assuming a total of |?sid| sentences anda maximum of N tokens in a sentence, Formula (3)alone has O(|?sid|N3) groundings.
Concretely, attraining time, assuming 1000 sentences with 10tokens per sentence, Formula (3) itself yields onemillion groundings.
Clearly, this approach is notscalable.
It turns out, however, that the (ground)Markov network can be decomposed into severaldisconnected components, each of which can besolved independently.
This greatly reduces thememory requirement of the inference step.
Specif-ically, for every grounding of sid, we get a set ofnodes in the Markov network that are disconnectedfrom the rest of the Markov network and thereforeindependent of the rest of the network.
Formally,Proposition 1.
For any world ?
of the BioMLN,PM(?)
= PMi(?i)PM\Mi(?
\ ?i) (4)where ?iis the world ?
projected on the ground-ings of sentence i andMiis BioMLN groundedonly using sentence i.Using Equation (4), it is easy to see that the MLNM can be decomposed into |?sid| disjoint MLNs,{Mk}|?sid|k=1.
The MAP assignment toM can becomputed using,|?sid|?i=1(arg max?iPMi(?i)).
Thisresult ensures that to approximate the expectedcounts Ew(nj), it is sufficient to keep exactly onesentence?s groundings in memory.
Specifically,Ew(nj) can be written as?|?sid|k=1Ew(nkj), whereEw(nkj) indicates the expected number of satisfiedgroundings of the jthformula in the kthsentence.Since the MAP computation is decomposable, wecan estimate Ew(nkj) using MAP inference on justthe kthsentence.5 Evaluation5.1 Experimental SetupWe evaluate our system on the BioNLP?13 (Kimet al., 2013), ?11 (Kim et al., 2011a) and ?09 (Kim3http://www.gurobi.com/Dataset #Papers #Abstracts #TT #EventsBioNLP?13 (10,10,14) (0,0,0) 13 (2817,3199,3348)BioNLP?11 (5,5,4) (800,150,260) 9 (10310,4690,5301)BioNLP?09 (0,0,0) (800,150,260) 9 (8597,1809,3182)Table 2: Statistics on the BioNLP datasets, whichconsist of annotated papers/abstracts from PubMed.
(x, y, z): x in training, y in development and zin test.
#TT indicates the total number of triggertypes.
The total number of argument types is 2.et al., 2009) Genia datasets for the main event ex-traction shared task.
Note that this task is the mostimportant one for Genia and therefore has the mostactive participation.
Statistics on the datasets areshown in Table 2.
All our evaluations use the on-line tool provided by the shared task organizers.We report scores obtained using the approximatespan, recursive evaluation.To generate features, we employ the support-ing resources provided by the organizers.
Specif-ically, sentence split and tokenization are doneusing the GENIA tools, while part-of-speech in-formation is provided by the BLLIP parser thatuses the self-trained biomedical model (McClosky,2010).
Also, we create dependency features fromthe parse trees provided by two dependency parsers,the Enju parser (Miyao and Tsujii, 2008) and theaforementioned BLLIP parser that uses the self-trained biomedical model, which results in two setsof dependency features.For MAP inference, we use Gurobi, a par-allelized ILP solver.
After inference, a post-processing step is required to generate biomedi-cal events from the extracted triggers and argu-ments.
Specifically, for binding events, we em-ploy a learning-based method similar to Bj?orne andSalakoski (2011), while for the other events, weemploy a rule-based approach similar to Bj?orne etal.
(2009).
Both the SVM baseline system and thecombined MLN+SVM system employ the samepost-processing strategy.During weight learning, in order to combat theproblem of different initializations yielding radi-cally different parameter estimates, we start at sev-eral different initialization points and average theweights obtained after 100 iterations of gradientdescent.
However, we noticed that if we simplychoose random initialization points, the variance ofthe weights was quite high and some initializationpoints were much worse than others.
To counterthis, we use the following method to systematically838System Rec.
Prec.
F1Our System 48.95 59.24 53.61EVEX (Hakala et al., 2013) 45.44 58.03 50.97TEES-2.1 (Bj?orne and Salakoski, 2013) 46.17 56.32 50.74BIOSEM (Bui et al., 2013) 42.47 62.83 50.68NCBI (Liu et al., 2013) 40.53 61.72 48.93DLUTNLP (Li et al., 2013a) 40.81 57.00 47.56Table 3: Recall (Rec.
), Precision (Prec.)
and F1score on the BioNLP?13 test data.initialize the weights.
Let nibe the number of sat-isfied groundings of formula fiin the training dataand mibe the total number of possible groundingsof fi.
We use a threshold ?
to determine whetherwe wish to make the initial weight positive or neg-ative.
Ifnimi?
?, then we choose the initial weightuniformly at random from the range [?0.1, 0].
Oth-erwise, we chose it from the range [0, 0.1].
Thesesteps ensure that the weights generated from dif-ferent initialization points have smaller variance.Also, in the testing phase, we set the scale parame-ters for the soft evidence as ?
= ?
= maxc?C|c|, whereC is the set of SVM confidence values.5.2 Results on the BioNLP?13 DatasetAmong the three datasets, the BioNLP?13 datasetis most ?realistic?
one because it is the only onethat contains full papers and no abstracts.
As a re-sult, it is also the most challenging dataset amongthe three.
Table 3 shows the results of our systemalong with the results of other top systems pub-lished in the official evaluation of BioNLP?13.
Oursystem achieves the best F1-score (an improvementof 2.64 points over the top-performing system) andhas a much higher recall (mainly because our sys-tem detects more regulation events which outnum-ber other event types in the dataset) and a slightlyhigher precision than the winning system.
Of thetop five teams, NCBI is the only other joint infer-ence system, which adopts joint pattern matchingto predict triggers and arguments at the same time.These results illustrate the challenge in using jointinference effectively.
NCBI performed much worsethan the SVM-based pipeline systems, EVEX andTEES2.1.
It was also worse than BIOSEM, a rule-based system that uses considerable domain exper-tise.
Nevertheless, it was better than DLUTNLP,another SVM-based system.Figure 3 compares our baseline pipeline modelwith our combined model.
We can clearly see thatthe combined model has a significantly better F1score than the pipeline model on most event types.System Rec.
Prec.
F1Our System 53.42 63.61 58.07Miwa12 (Miwa et al., 2012) 53.35 63.48 57.98Riedel11 (Riedel et al., 2011) ?
?
56UTurku (Bj?orne and Salakoski, 2011) 49.56 57.65 53.30MSR-NLP (Quirk et al., 2011) 48.64 54.71 51.50Table 4: Results on the BioNLP?11 test data.The regulation events are considered the most com-plex events to detect because they have a recursivestructure.
At the same time, this structure yields alarge number of joint dependencies.
The advantageof using a rich model such as MLNs can be clearlyseen in this case; the combined model yields a 10point and 6 point increase in F1-score on the testdata and development data respectively comparedto the pipeline model.5.3 Results on the BioNLP?11 DatasetTable 4 shows the results on the BioNLP?11 dataset.We can see that our system is marginally better thanMiwa12, which is a pipeline-based system.
It isalso more than two points better than Riedel11,a state-of-the-art structured prediction-based jointinference system.
Reidel11 incorporates the Stan-ford predictions (McClosky et al., 2011b) as fea-tures in the model.
On the two hardest, mostcomplex tasks, detecting regulation events (whichhave recursive structures and more joint dependen-cies than other event types) and detecting bind-ing events (which may have multiple arguments),our system performs better than both Miwa12 andRiedel11.4Specifically, our system?s F1 score forregulation events is 46.84, while those of Miwa12and Riedel11 are 45.46 and 44.94 respectively.
Oursystem?s F1 score for the binding event is 58.79,while those of Miwa12 and Riedel11 are 56.64 and48.49 respectively.
These results clearly demon-strate the effectiveness of enforcing joint dependen-cies along with high-dimensional features.5.4 Results on the BioNLP?09 DatasetTable 5 shows the results on the BioNLP?09 dataset.Our system has a marginally lower score (by 0.11points) than Miwa12, which is the best performingsystem on this dataset.
Specifically, our systemachieves a higher recall but a lower precision thanMiwa12.
However, note that Miwa12 used co-reference features while we are able to achieve4Detailed results are not shown for any of these threedatasets due to space limitations.839SVM MLN+SVMType Rec.
Prec.
F1 Rec.
Prec.
F1Simple 64.47 87.89 74.38 73.11 78.99 75.94Protein-Mod 66.49 79.87 72.57 72.25 69.70 70.95Binding 39.04 50.00 43.84 48.05 43.84 45.85Regulation 23.51 56.21 33.15 36.47 50.86 42.48Overall 37.90 67.88 48.64 48.95 59.24 53.61(a) TestSVM MLN+SVMType Rec.
Prec.
F1 Rec.
Prec.
F1Simple 55.79 81.63 66.28 63.21 75.10 68.64Protein-Mod 64.47 87.89 74.38 71.14 85.63 77.72Binding 31.90 48.77 38.57 47.99 50.00 48.97Regulation 20.13 52.46 29.10 28.57 43.41 34.46Overall 34.42 66.14 45.28 43.50 57.45 49.51(b) DevelopmentFigure 3: Comparison of the combined model (MLN+SVM) with the pipeline model on the BioNLP?13test and development data.System Rec.
Prec.
F1Miwa12 (Miwa et al., 2012) 52.67 65.19 58.27Our System 53.96 63.08 58.16Riedel11 (Riedel et al., 2011) ?
?
57.4Miwa10 (Miwa et al., 2010a) 50.13 64.16 56.28Bjorne (Bj?orne et al., 2009) 46.73 58.48 51.95PoonMLN (Poon&Vanderwende,2010) 43.7 58.6 50.0RiedelMLN (Riedel et al., 2009) 36.9 55.6 44.4Table 5: Results on the BioNLP?09 test data.
??
?indicates that the corresponding values are notknown.similar accuracy without the use of co-referencedata.
The F1 score of Miwa10, which does notuse co-reference features, is nearly 2 points lowerthan that of our system.
Our system also has ahigher F1 score than Reidel11, which is the bestjoint inference-based system for this task.On the regulation events, our system (47.55) out-performs both Miwa12 (45.99) and Riedel11 (46.9),while on the binding event, our system (59.88) ismarginally worse than Miwa12 (59.91) and signifi-cantly better than Riedel11 (52.6).
As mentionedearlier, these are the hardest events to extract.
Also,existing MLN-based joint inference systems suchas RiedelMLN and PoonMLN do not achieve state-of-the-art results because they do not leverage com-plex, high-dimensional features.6 Summary and Future WorkMarkov logic networks (MLNs) are a powerfulrepresentation that can compactly encode rich rela-tional structures and ambiguities (uncertainty).
Asa result, they are an ideal representation for com-plex NLP tasks that require joint inference, suchas event extraction.
Unfortunately, the superiorrepresentational power greatly complicates infer-ence and learning over MLN models.
Even themost advanced methods for inference and learningin MLNs (Gogate and Domingos, 2011) are un-able to handle complex, high-dimensional features,and therefore existing MLN systems primarily uselow-dimensional features.
This limitation severelyaffects the accuracy of MLN-based NLP systems,and as a result, in some cases their performanceis inferior to pipeline methods that do not employjoint inference.In this paper, we presented a general approachfor exploiting the power of high-dimensional lin-guistic features in MLNs.
Our approach involvesreliably processing and learning high-dimensionalfeatures using SVMs and encoding their output aslow-dimensional features in MLNs.
We showedthat we could achieve scalable learning and in-ference in our proposed MLN model by exploit-ing decomposition.
Our results on the BioNLPshared tasks from ?13, ?11, and ?09 clearly showthat our proposed combination is extremely effec-tive, achieving the best or second best score on allthree datasets.In future work, we plan to (1) improve our jointmodel by incorporating co-reference informationand developing model ensembles; (2) transfer theresults of this investigation to other complex NLPtasks that can potentially benefit from joint infer-ence; and (3) develop scalable inference and learn-ing algorithms (Ahmadi et al., 2013).AcknowledgmentsThis work was supported in part by the AFRL un-der contract number FA8750-14-C-0021, by theARO MURI grant W911NF-08-1-0242, and by theDARPA Probabilistic Programming for Advanced-Machine Learning Program under AFRL primecontract number FA8750-14-C-0005.
Any opin-ions, findings, conclusions, or recommendationsexpressed in this paper are those of the authorsand do not necessarily reflect the views or officialpolicies, either expressed or implied, of DARPA,AFRL, ARO or the US government.840ReferencesBabak Ahmadi, Kristian Kersting, Martin Mladenov,and Sriraam Natarajan.
2013.
Exploiting symme-tries for scaling loopy belief propagation and rela-tional training.
Machine Learning, 92(1):91?132.David Ahn.
2006.
The stages of event extraction.In Proceedings of the Workshop on Annotating andReasoning About Time and Events, pages 1?8.Jari Bj?orne and Tapio Salakoski.
2011.
Generaliz-ing biomedical event extraction.
In Proceedings ofthe BioNLP Shared Task 2011 Workshop, pages 183?191.Jari Bj?orne and Tapio Salakoski.
2013.
TEES 2.1: Au-tomated annotation scheme learning in the bionlp2013 shared task.
In Proceedings of the BioNLPShared Task 2013 Workshop, pages 16?25.Jari Bj?orne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Ex-tracting complex biological events with rich graph-based feature sets.
In Proceedings of the BioNLP2009 Workshop Companion Volume for Shared Task,pages 10?18.Quoc-Chinh Bui, David Campos, Erik van Mulligen,and Jan Kors.
2013.
A fast rule-based approachfor biomedical event extraction.
In Proceedings ofthe BioNLP Shared Task 2013 Workshop, pages 104?108.Chen Chen and Vincent Ng.
2012.
Joint modeling forChinese event extraction with rich linguistic features.In Proceedings of the 24th International Conferenceon Computational Linguistics, pages 529?544.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis, Uni-versity of Pennsylvania, Philadelphia, PA.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing, pages 1?8.Pedro Domingos and Daniel Lowd.
2009.
MarkovLogic: An Interface Layer for Artificial Intelligence.Morgan & Claypool, San Rafael, CA.Lise Getoor and Ben Taskar, editors.
2007.
Introduc-tion to Statistical Relational Learning.
MIT Press.Vibhav Gogate and Pedro Domingos.
2011.
Proba-bilistic theorem proving.
In Proceedings of the 27thConference on Uncertainty in Artificial Intelligence,pages 256?265.Ralph Grishman, David Westbrook, and Adam Meyers.2005.
NYU?s English ACE 2005 system description.In Proceedings of the ACE 2005 Evaluation Work-shop.
Washington.Prashant Gupta and Heng Ji.
2009.
Predicting un-known time arguments based on cross-event prop-agation.
In Proceedings of the ACL-IJCNLP 2009Conference Short Papers, pages 369?372.Kai Hakala, Sofie Van Landeghem, Tapio Salakoski,Yves Van de Peer, and Filip Ginter.
2013.
EVEXin ST?13: Application of a large-scale text miningresource to event extraction and network construc-tion.
In Proceedings of the BioNLP Shared Task2013 Workshop, pages 26?34.Ruihong Huang and Ellen Riloff.
2012a.
Bootstrappedtraining of event extraction classifiers.
In Proceed-ings of the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,pages 286?295.Ruihong Huang and Ellen Riloff.
2012b.
Modelingtextual cohesion for event extraction.
In Proceed-ings of the 26th AAAI Conference on Artificial Intel-ligence.Heng Ji and Ralph Grishman.
2008.
Refining event ex-traction through cross-document inference.
In Pro-ceedings of the 46th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies, pages 254?262.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Schlkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods -Support Vector Learning.
MIT Press, Cambridge,MA, USA.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.2008.
Corpus annotation for mining biomedi-cal events from literature.
BMC bioinformatics,9(1):10.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overview ofBioNLP?09 shared task on event extraction.
In Pro-ceedings of the BioNLP 2009 Workshop CompanionVolume for Shared Task, pages 1?9.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, Ngan Nguyen, and Jun?ichi Tsujii.
2011a.Overview of BioNLP shared task 2011.
In Pro-ceedings of the BioNLP Shared Task 2011 Workshop,pages 1?6.Jin-Dong Kim, Yue Wang, Toshihisa Takagi, and Aki-nori Yonezawa.
2011b.
Overview of Genia eventtask in BioNLP shared task 2011.
In Proceedingsof the BioNLP Shared Task 2011 Workshop, pages7?15.Jin-Dong Kim, Yue Wang, and Yamamoto Yasunori.2013.
The Genia event extraction shared task, 2013edition - overview.
In Proceedings of the BioNLPShared Task 2013 Workshop, pages 8?15.Daphne Koller and Nir Friedman.
2009.
ProbabilisticGraphical Models: Principles and Techniques.
MITPress.841Peifeng Li, Guodong Zhou, Qiaoming Zhu, and LibinHou.
2012.
Employing compositional semanticsand discourse consistency in Chinese event extrac-tion.
In Proceedings of the 2012 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning,pages 1006?1016.Lishuang Li, Yiwen Wang, and Degen Huang.
2013a.Improving feature-based biomedical event extrac-tion system by integrating argument information.
InProceedings of the BioNLP Shared Task 2013 Work-shop, pages 109?115.Peifeng Li, Qiaoming Zhu, and Guodong Zhou.
2013b.Argument inference from relevant event mentions inChinese argument extraction.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, pages 1477?1487.Qi Li, Heng Ji, and Liang Huang.
2013c.
Joint eventextraction via structured prediction with global fea-tures.
In Proceedings of the 51st Annual Meetingof the Association for Computational Linguistics,pages 73?82.Shasha Liao and Ralph Grishman.
2010.
Using doc-ument level cross-event inference to improve eventextraction.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 789?797.Shasha Liao and Ralph Grishman.
2011.
Acquiringtopic features to improve event extraction: in pre-selected and balanced collections.
In Proceedingsof the International Conference Recent Advances inNatural Language Processing 2011, pages 9?16.Haibin Liu, Karin Verspoor, Donald C. Comeau, An-drew MacKinlay, and W John Wilbur.
2013.
Gen-eralizing an approximate subgraph matching-basedsystem to extract events in molecular biology andcancer genetics.
In Proceedings of the BioNLPShared Task 2013 Workshop, pages 76?85.Daniel Lowd and Pedro Domingos.
2007.
Efficientweight learning for markov logic networks.
InProceedings of the 11th European Conference onPrinciples and Practice of Knowledge Discovery inDatabases, pages 200?211.Wei Lu and Dan Roth.
2012.
Automatic event extrac-tion with structured preference modeling.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics, pages 835?844.Radu Marinescu and Rina Dechter.
2009.
AND/ORbranch-and-bound search for combinatorial opti-mization in graphical models.
Artificial Intelligence,173(16-17):1457?1491.David McClosky, Mihai Surdeanu, and Chris Manning.2011a.
Event extraction as dependency parsing.
InProceedings of the Association for ComputationalLinguistics: Human Language Technologies, pages1626?1635.David McClosky, Mihai Surdeanu, and ChristopherManning.
2011b.
Event extraction as dependencyparsing for BioNLP 2011.
In Proceedings of theBioNLP Shared Task 2011 Workshop, pages 41?45.David McClosky.
2010.
Any domain parsing: Auto-matic domain adaptation for natural language pars-ing.
Ph.D. thesis, Ph.D. thesis, Brown University,Providence, RI.Makoto Miwa, Sampo Pyysalo, Tadayoshi Hara, andJun?ichi Tsujii.
2010a.
Evaluating dependency rep-resentation for event extraction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics, pages 779?787.Makoto Miwa, Rune S?tre, Jin-Dong Kim, andJun?ichi Tsujii.
2010b.
Event extraction with com-plex event classification using rich features.
Jour-nal of Bioinformatics and Computational Biology,8(01):131?146.Makoto Miwa, Paul Thompson, and Sophia Ananiadou.2012.
Boosting automatic event extraction from theliterature using domain adaptation and coreferenceresolution.
Bioinformatics, 28(13):1759?1765.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature for-est models for probabilistic HPSG parsing.
Compu-tational Linguistics, 34(1):35?80.Claire N?edellec, Robert Bossy, Jin-Dong Kim, Jung-Jae Kim, Tomoko Ohta, Sampo Pyysalo, and PierreZweigenbaum.
2013.
Overview of BioNLP sharedtask 2013.
In Proceedings of the BioNLP SharedTask 2013 Workshop, pages 1?7.Siddharth Patwardhan and Ellen Riloff.
2009.
A uni-fied model of phrasal and sentential evidence for in-formation extraction.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 151?160.Hoifung Poon and Pedro Domingos.
2007.
Joint in-ference in information extraction.
In Proceedingsof the 22nd National Conference on Artificial Intelli-gence, pages 913?918.Hoifung Poon and Lucy Vanderwende.
2010.
Jointinference for knowledge extraction from biomedi-cal literature.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 813?821.Martin F. Porter.
1980.
An algorithm for suffix strip-ping.
Program, 14(3):130?137.Chris Quirk, Pallavi Choudhury, Michael Gamon, andLucy Vanderwende.
2011.
MSR-NLP entry inBioNLP shared task 2011.
In Proceedings of theBioNLP Shared Task 2011 Workshop, pages 155?163.842Sebastian Riedel and Andrew McCallum.
2011a.
Fastand robust joint models for biomedical event extrac-tion.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Processing,pages 1?12.Sebastian Riedel and Andrew McCallum.
2011b.
Ro-bust biomedical event extraction with dual decom-position and minimal domain adaptation.
In Pro-ceedings of the BioNLP Shared Task 2011 Workshop,pages 46?50.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A Markov logic approachto bio-molecular event extraction.
In Proceedings ofthe BioNLP 2009 Workshop Companion Volume forShared Task, pages 41?49.Sebastian Riedel, David McClosky, Mihai Surdeanu,Andrew McCallum, and Christopher D. Manning.2011.
Model combination for event extraction inbionlp 2011.
In Proceedings of the BioNLP SharedTask 2011 Workshop, pages 51?55.Alan Ritter, Mausam, Oren Etzioni, and Sam Clark.2012.
Open domain event extraction from Twit-ter.
In Proceedings of the 18th ACM SIGKDD Inter-national Conference on Knowledge Discovery andData Mining, pages 1104?1112.Bart Selman, Henry Kautz, and Bram Cohen.
1996.Local Search Strategies for Satisfiability Testing.
InD.
S. Johnson and M. A.
Trick, editors, Cliques,Coloring, and Satisfiability: Second DIMACS Im-plementation Challenge, pages 521?532.
AmericanMathematical Society, Washington, DC.Parag Singla and Pedro Domingos.
2005.
Discrimina-tive training of Markov logic networks.
In Proceed-ings of the 20th National Conference on ArtificialIntelligence, pages 868?873.David Sontag and Amir Globerson.
2011.
Introductionto Dual Decomposition for Inference.
Optimizationfor Machine Learning.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vectormachine learning for interdependent and structuredoutput spaces.
In Proceedings of the 21st Interna-tional Conference on Machine Learning, pages 104?112.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, New York, NY.843
