CRL/BRANDEIS :DESCRIPTION OF THE DIDEROT SYSTEM AS USEDFOR MUC-5Jim Cowie, Louise Guthrie, Wang Jin, Rong Wang, Takahiro Waka oComputing Research Laboratory, New Mexico State UniversityEmail: jcowie@nmsu .edu &James Pustejovsky, Scott WatermanComputer Science Department, Brandeis Universit yEmail : jamesp@cs.brandeis .eduAbstractThis report describes the major developments over the last six months in completing th eDiderot information extraction system for the MUC-5 evaluation .Diderot is an information extraction system built at CRL and Brandeis University over th epast two years.
It was produced as part of our efforts in the Tipster project .
The same overallsystem architecture has been used for English and Japanese and for the micro-electronics and join tventure domains.The past history of the system is discussed and the operation of its major components described .A summary of scores at the 24 month workshop is given and the performance of the system o nthe texts selected for the system walkthrough is discussed .INTRODUCTIONThe Computing Research Laboratory at New Mexico State University, in collaboration with BrandeisUniversity, was one of four sites selected to develop systems to extract relevant information automat-ically from English and Japanese texts .
The systems produced by the Tipster research groups hav ealready been evaluated at 12 and 18 months into the project .
The performance of the Diderot Systemhas improved both for English and Japanese .
The performance in Japanese, however, is still far ahea dof our English performance .The Tipster project is, without a doubt, the largest scale Applied Natural Language Processin gtask yet undertaken anywhere in the world.
The government data preparation effort involved th eselection and analysis of more than 5,000 individual texts .
The results of this analysis have been use dto develop and test the systems produced by each site .
The software used to support this huma nextraction task, both for English and Japanese, was developed and supported by the CRL under aseparate subcontract .Because of the emphasis on different languages and different subject areas the research has focusedon the development of general purpose, re-usable techniques.
The CRL/Brandeis group have imple-mented statistical methods for focusing on the relevant parts of texts, programs which recognize an dmark names of people, places and organizations and also dates .
The actual analysis of the critical part sof the texts is carried out by a parser controlled by lexical structures for the `key ' words in the text .
Toextend the system's coverage of English and Japanese some of the content of these lexical structure swas derived from machine readable dictionaries .
These were then enhanced with information extractedfrom corpora.161The system has already been evaluated in the 4th Message Understanding Conference (MUC-4 )where it was required to extract information from 200 texts on South American terrorism .
Consideringthe very short development time allowed for this additional domain the system performed adequately .The system was then adapted to handle the business domain and also to process Japanese texts .Further extensions to the system allowed it to process texts on micro-electronics development .
Per-formance at the 12 and 18 month evaluations was good for Japanese, but less good for English wher ewe have been attempting to automate much of the development process .
A more pragmatic approachwas adopted for the final 24 month evaluation, using the same hand-crafted techniques for English a shad been used for Japanese .We estimate the amount of effort used directly to build the systems described here is around sixt yman months .MAIN RESEARCH OBJECTIVESOur objectives in this research have been as follows :?
to develop and implement a language-independent framework for lexical semantic representation ,and develop and implement a robust integration of that framework into a language-independenttheory of semantic processing ;?
to investigate and implement language independent techniques for automating the building o flexical knowledge bases from machine readable resources ;?
to implement statistical tools for the tuning of lexical structures to specific domains ;?
to implement the use of language independent statistical techniques for identifying relevan tpassages of documents for more detailed analysis ;?
to develop and implement a set of robust multi-pass finite-state feature taggers ;?
to develop and implement the equivalent methods for Japanese .SYSTEM OVERVIEWAn outline of the functions of the main system modules are given here .
This is intended to provide acontext for the more detailed description of each module which follows .
The structures of the Japaneseand English systems are very similar .
In the examples of intermediate output either Japanese or Englis hmay be shown.
The system architecture is shown in figure 1 .The input text to the system is processed by three independent pre-processing modules :?
A chain of finite-state feature taggers - these mark : names, organization names, place names ,date expressions and other proper names (depending on the domain) ,?
A part of speech tagger ,?
A statistically based determiner of text relevance (micro only) .If the statistical determination rejects the text processing proceeds to the final output stage an dan empty template is produced .
Otherwise the results of the other two stages are converted to Prolo gfacts and these then pass into the head of a chain of processes each of which gives rise to furthe rrefinements of the text :?
Merge - Here semantic tags, which may mark phrasal units, are merged with POS tags, whichmark individual words .?
Compound noun recognizer - this groups words and phrases into compound nouns using PO Sand semantic information .162Part ofRelevanceStatisticserSpeechTaTransforme rReferenceResolverFigure 1 : System Overview?
Parser - the relevant paragraph information is used to select which sentences to process fur-ther .
The sentences containing the marked up noun-phrase groups are then parsed to produce apartially completed representation of the relevant semantic content of the sentence (frames) .+ Reference resolver - the frames are then merged based on name matching and noun compound sbeginning with definite articles .
?Template formatter - this transforms the resolved frames into the final output form .Statistical Filtering TechniquesStatistical information is used to predict whether a text holds important information that is relevan tto completing a template .
This allows the parser to skip non-relevant texts .
This is based on word list swhich are derived from training on relevant and irrelevant texts .
The theoretical results on which themethod [6] is based assure us that documents can be classified correctly if appropriate sets of word scan be chosen for each document type .
The method was only applied to the micro domain for MUC- 5as almost all texts in the joint venture domain are relevant and the use of this statistical method i sessentially a way of improving precision in text filtering .The results for the micro electronics domain for text filtering are 84% recall and 90% precision (7 3and 83 at 18 month) for Japanese, and 78% recall and 83% precision (77 and 76 at 18 month) fo r163English .Semantic TaggingThis component is based on a pipeline of programs .
These are all written in `C' or flex .
It marksorganization names, human names, place names, date expressions, equipment names, process typesand a variety of measurements (including money) .
Many of these have converted forms and additiona lvalues attached by the tagger .The tagging programs use three separate methods?
?Direct recognition of already known unambiguous names, using a longest string match .?
Recognition using textual patterns only.?
Two pass method marking ambiguous, but potential names, and subsequently verifying they fita pattern .?
final pass recognizing short forms and isolated occurrences of names not in a strong context 'The system uses the case of letters used when available .
The final text is tagged using SGML-likemarkers .BRIDGESTONE SPORTS CO .
SAID FRIDAY IT HAS SET UP A JOINT VENTURE I NTAIWAN WITH A LOCAL CONCERN AND A JAPANESE TRADING HOUSE T OPRODUCE GOLF CLUBS TO BE SHIPPED TO JAPAN .<organ> BRIDGESTONE SPORTS CO .
{type([{}[entity type,'COMPANY']{}])} <\endorgan> sai d<date> FRIDAY{type([{}[date,'241189']{}])} <\enddate> it has set up a join tventure in <country> TAIWAN {type([{}[nationality,'TAIWAN']{}]) }<\endcountry> with a local concern and a<country>japanese {type([{}[nationality,'JAPAN'], [word_type,sp noun]{}])} <\endcountry >trading house to produce golf clubs to be shippedto <country> JAPAN {type([{}[nationality,'JAPAN']{}])} <\endcountry> .At this point the tags are converted into Prolog facts :organ('BRIDGESTONE SPORTS CO .
',type([[entity_type, 'COMPANY']])) ,res('said',type([[undefined,'said']])) ,time('FRIDAY',type([[date_adverb,'UNSPEC'],[date,'241189']])) ,cs('it',type([[it,[pron]]])) ,cs('has',type([[has,[pastv,presv]]])) ,gls('set up',type([['set up',v]])) ,cs('a',type([[a,[determiner]]])) ,gls('joint venture',type([['joint venture',comp]])) ,date_adverb('in',type([[date_adverb,during]])) ,country('TAIWAN',type([[nationality,'TAIWAN']])) ,cs('with',type([[with,[prep]]])) ,The Japanese system preprocesses the article to change the original encoding (Shift JIS) to EUC fo ra given article .
The original and unsegmented text goes through a series of taggers for known names ,i .e .
organizations, places, GLS verbs .
This process is exactly the same as in the English system .The next step is to tag organization, personal and place names which are not known to the system .These are detected by using local context, using Japanese-specific patterns, which use particles, specifi cwords and the text tags to recognize the unknown names .
In addition, date expressions are tagged an dchanged into the normalized form .
Date expressions in the Japanese articles seem straightforward, fo rexample, `20 nichi' (20 day) is used even if the document date is 21st and 20th can be expressed a s`yesterday', and this convention `XX day' (where XX is a number) to express a date is consistentlyused in the articles .
Era names such as `H U' (Showa) or (Heisei) are Japanese specific and theyear in the era, e .g . "
(Showa 60th year), is correctly recognized and normalized.
Here is the firs tsentence of a typical article after the tagging process .164<\organ> **7 ?XX k {type(C[entity_type,'COMPANY ']]) }<\endorgan> i <\time> -Pe A 1 s {type( [[date_adverb ,after] ,[date, '8501']]) }<\endtime> <\organ>]?il:* {type([[entity_type,'COMPANY']]) }<\endorgan><\gls> t I# l LT {type(['4 '$' i' ,v]) } <\endgls>Just as for the English system this is then converted into the form of Prolog facts ready to be rea dinto the merging phase .Part-Of-Speech Taggin gEnglish text is also fed through the POST part of speech tagger .
This attaches the Penn Treebankparts of speech to the text .
The output is converted to Prolog facts .
The Japanese text is segmentedwith part-of-speech information by the JUMAN program, which was developed by Kyoto University .The following is the result for exactly the same sentence .
The segmented units are converted to Prolo gfacts ready to input to the next stage .juman( ' 3k ', 'proper_noun') .juman(' i','proper_noun') .juman(' k &', 'normal_noun') .juman('fI','normal_noun') .juman('ii','topic_particle') .juman('+ ', 'normal_noun') .juman('','case_particle') .juman (' *f1' , 'normal_noun' ) .juman('i ','normal_noun') .juman('','case_particle') .juman('','noun_verb') .juman(' LZ' ,'verb') .MergingThe semantic and syntactic information are merged to give lexical items in the form of triples .
Themerging is done in such a way that if it is not possible to match up words (eg due to different treatment sof hyphens) a syntactic tag of 'UNK' is allocated and merging continues with the next word .Noun Phrase GroupingNoun phrases are identified by scanning back through a sentence to identify head nouns .
Both seman-tically and syntactically marked units qualify as nouns .
The grouping stops when closed class word sare encountered .
A second forward pass gathers any trailing adjectives .
The main use of the nou nphrase in the present system is to attach related strings to company names to help with the referenc eresolution.
They are also used by a retrieval process which uses the string to determine the SIC cod eindustry type .A similar grouping is carried out for Japanese .noun_phrase([[undefined,house]] ,[unit(cs,a,type([[a,[determiner]]]),['DT']) ,unit(country,japanese,type([[nationality,'JAPAN'],[word_type,sp noun]]),['JJ']) ,unit(res,trading,type([[undefined,trading]]),['NN']) ,unit(res,house,type([[undefined,house]]),['NN'])] )165noun_phrase(money ,[unit(num,'20',type([[num_value,20]]),['CD']) ,unit(num,million,type([[num_value,1000000]]),['CD']) ,unit(money,'NEW TAIWAN dollars',type([[denom,'TWD']]),['NP','NP','NNS'])] )ParsingThe parser has GLS cospecification patterns built into it .
It uses these and ancillary rules for th erecognition of semantic objects to fill a frame format which was given as an application specific fiel din the GLS entry.
The frame formats provide a bridge between the sentence level parse and the fina ltemplate output .
Semantic objects are named in the cospecification and special rules which handle typ echecking, conjunction and co-ordination are used to return a structure for the object.
The followin gshows an example of a tie-up between two companies .
The child company is unmatched, shown by anunderscore.
The parser has grouped a date with one of the companies .
The tie-up status is providedby the GLS template semantics .prim_tie up(1,1, [[[f(name ,_9947,[unit (organ,'I7 7 Ef cI' ,type ([Cent ity type, 'COMPANY']]), [proper_noun])]) ,f(entitytype,_9953,[unit (organ,'?I' ,type( [Cent ity_type,'COMPANY ']]),[proper_noun])])]] ,[ [f (name , _10102 , [unit (organ,'*fq 3 ' ,type( [[entity_type,'COMPANY ']]), [propernoun])]) ,f (entity_type, _10108, [unit (organ, ' jcf:tlIlf' ,type( [[entity_type,'COMPANY ']]), [propernoun])]) ,f(time,_10114, [unit(time ,type( [[date_adverb,after] ,[date,'8501']]),[propernoun])])]]] ,[f(tie up_status,existing,0 )]) .Reference ResolutionThe task of this component is to gather all the relevant information scattered in a text together .
Themajor task is to resolve reference or anaphora.
For the current application only references betwee ntie-up events, between entities, and between entity relations are considered .Since entities are expressed in noun phrases, the references for entities are resolved by resolvin gthe reference between noun phrases.
Since the entity can either be referred to by definite or indefinit enoun phrase or by name, it is necessary to detect the reference between two definite or indefinite nou nphrases, between two names, as well as between one name and one definite or indefinite noun phrase.All entities are represented as frames of the form :entity(Sen#, Para#, Noun-phrase, Name ,Location, Nationality ,Ent-type, alias-list, np-list) .The reference between two entities is resolved by looking at the similarity between their namesand/or their noun phrases.
Since companies are often referred by their nationality or location, theLocation and Nationality slot fillers in the entity frame also contribute to the reference resolution .Some special noun phrases which refer to some particular role of a tie-up (the newly formed venturei nparticular) are also recognized and resolved.
For example, a phrase which refers to the child entity ,such as `the new company' or `the venture', will be recognized and merged with the child of the tie-upevent in focus.
A stack of entities found in the text is maintained .166Definite noun phrases can only be used for local reference .
So they can only be used to refer toentities involved in the tie-up event which is in focus.
On the contrary, names can be used for bot hlocal and global reference, so they can refer to any entity referred to before in the text .When a reference relation between two entities is resolved they are merged to create one singl eentity which contains all the information about that particular entity .Since a tie-up is generally referenced by an entire sentence rather than a single noun phrase, th ereference of tie-up events is handled by resolving the reference between its participants and some othe rinformation mentioned about the event .
Other heuristics are also applied .
These mostly block theoverapplication of merging .
For example, two tie-ups cannot be merged if their dates are different ;similarly, entities with different locations will not be merged .
There are currently two types of tex tstructures which are considered .
In the first type, one tie-up-event is in focus until the next one i smentioned and after the new one is mentioned the old one will not be mentioned again .
In the secon dtype, a list of tie-up-events are mentioned shortly in one paragraph, and more details of each event ar egiven sequentially later .
Finally, when the reference between two tie-ups is resolved they will also bemerged to form a single tie-up event .
The final result is a set of new frames which are linked in suc ha way as to reduce the requirement on the final stage of maintaining pointers to the various objects .With the exception of the use of definite articles ?an obvious cross-linguistic difference betwee nthe languages studied?
the reference resolution process for Japanese is identical to English .
Theresolved entities, entity-relation, and tie-up for a typical text are shown below .final_entity(2,[f(name,['k', ' ELI',lIE', ' ?`'],'UNSPEC') ,f(entity_type,'COMPANY', 'UNSPEC') ,f ( industry_product , ' (63 "At)fi*z") ' , w j) ,f (time , [after, '8501 '] ,wj ) ,f (entity_relationship,l, inf ) ,f(entity_relationship,3,inf)]) .final_entity(9,[f(name,['s','*','rs','i', ,1~,'{'c',I ],'UNSPEC') ,f(entity type,'COMPANY','UNSPEC'),f(name,['*','*','4','?']
,'UNSPEC') ,f(entity_relationship,1,inf),f(entity_relationship, 3,inf)]) .final_rel(1,[9,2],'UNSPEC','PARTNER','UNSPEC') .final_tie up(1,[9,2],'UNSPEC','UNSPEC', 'UNSPEC' ,existing, 'UNSPEC' ,1 ,'UNSPEC') .The system uses character-based rules for identifying aliases .
For example, if a company nam estarts with 'Mr (Hitachi) as in '13 f'P7r' (Hitachi Manufacturing), then the system ; looks for thestring `QJ'L' (Hitachi) or the first two characters of the company name as its alias .Template FormattingThe final stage generates sequence numbers and incorporates document numbers into the labels .
I talso eliminates objects which are completely empty.
The final output from the English system exampl etext, #0592, is shown below .<TEMPLATE-0592-1> : =DOC NR: 0592DOC DATE : 241189DOCUMENT SOURCE : "Jiji Press Ltd .
"CONTENT : <TIE_UP_RELATIONSHIP-0592-1 ><TIE_UPRELATIONSHIP-0592-1> : =TIE-UP STATUS : existingENTITY : <ENTITY-0592-3 >JOINT VENTURE CO : <ENTITY-0592-1 >OWNERSHIP : <OWNERSHIP-0592-1>167<ENTITY-0592-1> : =NAME : BRIDGESTONE SPORTS TAIWAN COALIASES : "BRIDGESTONE SPORTS"TYPE : COMPANYENTITY RELATIONSHIP : <ENTITY_RELATIONSHIP-0592-1><ENTITY-0592-3> : =NAME : BRIDGESTONE SPORTS COALIASES : "BRIDGESTONE SPORTS"TYPE : COMPANYENTITY RELATIONSHIP : <ENTITY_RELATIONSHIP-0592-1><ENTITY_RELATIONSHIP-0592-1> : =ENTITY1 : <ENTITY-0592-3>ENTITY2 : <ENTITY-0592-1 >REL OF ENTITY2 TO ENTITY1 : CHILDSTATUS : CURRENT<OWNERSHIP-0592-1> : =OWNED : <ENTITY-0592-1>TOTAL-CAPITALIZATION : 20000000 TWDOWNERSHIP-''/.
: (<ENTITY-0592-3> 75 )SUMMARY OF PERFORMANCECurrent performance for the CRL English Tipster systems as evaluated for the fifth Message Under-standing Conference (MUC-5) are given in the appendix to this paper .
All the scores have improvedsince the 18 month Tipster evaluation .
The scores for Japanese, using an identical architecture, bu twith much more intensive human tuning, are much higher .We feel the huge difference between performance in Japanese and English is principally due toone person being dedicated for Japanese to running and tuning the system .
All other personnel wer eworking on particular components to be used first in the English and then in the Japanese system andno one person was repeatedly testing the operation of the English System .
Another difference might b edue to the focus of effort on automatic and semi-automatic pattern generation for the English systems ,a process which was not attempted for Japanese development .
Although this differential would appearto speak slightly against these particular automatic development techniques, we feel that additiona ltesting and refinement of the patterns would have brought the scores more in line with the Japanes esystems, since they use the same architecture .KNOWLEDGE ENGINEERIN GThe DIDEROT system has involved the development of a significant amount of diverse knowledge .This consisted of the automatic construction of the lexicon and partial syntactic forms for a give nlanguage and domain, along with subsequent tuning and refinement .
Human tuning was also neededfor both languages .This off-line component included the derivation of vocabulary automatically from machine readabl esources, and made use of statistically-based techniques to determine the relevant domain-dependen tvocabulary of words and phrases from text samples .Statistical techniques were used extensively to assist in the development of the various lexicon sof the Tipster system .
Initially, a simple frequency count of the tokens in the initial corpora wa sused to highlight those words which should be targeted, essentially determining what core vocabular yelements are of rnost importance .
In general, the lexical structures used by the system can be thoughtof as providing for the shallowest possible semantic decomposition while still capturing significan tgeneralizations about how words relate conceptually to one another .168Deriving the Lexicon from Machine-Readable Resource sThe lexical knowledge base consists of lexical items called generative lexical structures (GLSs), afterPustejovsky[9] .
This model of semantic knowledge associated with words is based on a system o fgenerative devices which is able to recursively define new word senses for lexical items in the language .For this reason, the algorithm and associated dictionary is called a generative lexicon .
The lexicalstructures contain conventional syntactic and morphological information along with detailed typin ginformation about argumentsThe creation of the GLS lexicon begins with the printer 's tape of the Longman Dictionary ofContemporary English (LDOCE), Proctor[8] .
This was parsed and analysed by the CRL lexical grou pto give a tractable formatted version of LDOCE called LEXBASE[5] .
LEXBASE contains syntacticcodes, inflectional variants, and boxcodes, selectional information for verbs and nouns, indicatin ggenerally what kind of arguments are well-formed with that lexical item .
A GLS entry is automaticall yderived from LEXBASE by parsing the LEXBASE format for specific semantic information [11] .
Themost novel aspect of this conversion involves parsing the example sentences as well as parentheticaltexts in the definition .
This gives a much better indication of argument selection for an item than dothe the boxcodes alone .
For example, the verb market is converted into the following GLS entry as aresult of this initial mapping .gls(market ,syn([type(v) ,code (gcode_t1) ,eventstr([]) ,ldoce_id(market_1_1) ,caseinfo([subcatl(A1) ,subcat2(A2) ,case(A1,np) ,case(A2,np)]) ,inflection([ing(marketing) ,pastp(marketed) ,plpast(marketed) ,singpastl(marketed) ,singpast2(marketed) ,singpast3(marketed) ,past(marketed) ,p1(market) ,singl(market) ,sing2(market) ,sing3(markets)])]) ,qualia([formal([sell]) ,telic ([]) ,conat([]) ,agent([])]) ,args([argi(A1 ,syn([type(np)]) ,qualia( [formal( [organization])])) ,arg2(A2,syn([type(np)]) ,qualia( [formal( [device])]))]) ,cospec([[A1,*,self,*,A2] ,[A1,*,in,*,A2,self] ,[self,*,A2] ,[A2,*,self ]]) ,types(capabilityverb) ,template_semantics(prim_cap([purch(A1)],A2))) .169The two arguments to the verb market are minimally typed as a result of the conversion from LDOCE ,this information being represented as a type-path for each argument, Pustejovsky and Boguraev[12] .For example, the subject is typed as a organization and the object as the type device.The syntactic and collocational behavior of a word is represented in the cospec (cospecification )field of the entry .
The cospec of a lexical item can be seen as.
paradigmatic syntactic behavior for aword, involving both abstract types as well as lexical collocations .
This field is created automaticall yby reference to the syntactic codes of the verb, as represented in LDOCE, in this case T1 (i .e .
,basic transitive) .
That is, the cospec encodes explicit information regarding the linear positioning ofarguments, as well as semantic constraints on the arguments as imposed by the typing information i nthe qualia .
The syntactic representation of a word's environment may appear flat, but the semanti cinterpretation is based on a unification-like algorithm which creates a much richer functional structure .Theoretically, the expressive power of converting the cospecs of a GLS into DCG parse rules i sequivalent to the power of a Lexicalized Tree Adjoining Grammar with collocations (Shieber[14]) ,what we have termed Hyper Lexicalized Tree Adjoining Grammars (HTAGs) (Pustejovsky[13]) .Lexically Encoding Idiomatic and Phrasal Structure sOne of the advantages to the highly lexical approach being taken here is the ability to encode idiomati cexpressions and phrasal expressions as part of the lexicon proper, where motivated by statistical confir -mation of the collocations.
For example, in the English JV corpus, it so happens that reporting verb ssuch as announce very often appear with tensed clause complements carrying pronominal subjects, a sbelow :IBM; announced that it; had entered a joint venture with Mark Computers .The standard linguistic approach to resolving the coreference between the pronoun and the compan yname is to syntactically "walk" the parse tree and bind the pronoun according to standard syntacti canaphora constraints .
That is, the subject of the matrix sentence is a possible antecedent to thepronoun in the lower clause .
Within a lexical approach, such stereotypical antecedent-anaphora pair ssuch as that above are directly encoded in the cospec of the appropriate lexical trigger .
For example ,the cospec for such a pattern with the verb announce is given below :gls(announce ,syn([type(v) ,args([argl(Al ,syn([type(np)]) ,qualia([formal([organization])])) ,.
.
.
.])])
,cospec([[Al,self,that,it,*,A2] ,] ) ,template_semantics0) .This corresponds to the tree fragment of a Hyper-lexicalized TAG shown on the next page .The ability to lexically encode domain- or sublanguage-specific idioms or phrases is also useful fo rreferring (i .e ., naming) expressions .
For example, in the JV domain, a phrase such a sthe new company, called COMPANY-NAME170is triggered by the lexical item called, with the cospecification constraint of being an anaphoric referenc eto the newly created child company .SNPV PAlTYPE = Li InaS/ \NPVPvs'announceCthati tS ECreation of a Lexically-Driven Partial ParserThe generative lexical structures are "universal" in character in the same way that phrase structuraldescriptions provide a general, expressive language for describing the syntactic structures for widel ydifferent linguistic behaviors from different languages .
The lexical entries for Japanese follow exactl ythe same specifications, with the same degree of flexibility .The partial grammar is derived from the tuned GLS entries .
Prolog Definite Clause Grammarrules are produced automatically from the patterns given in the GLS cospecification .
The rules arethen compiled into the working version of the system .
A similar process also produces lists of wordswith GLS entries for the tagging programs .For example this partial GLS entry will give rise to two parse rules .gls(join ,syn([type (v) ,code (gcode_t1) ,eventstr([]) ,ldoce_id(join_1_2) ,arg3(A3 ,syn([type(np)]) ,qualia([formal([organization] ) ,telic([]) ,const([]) ,agent([])]))]) ,cospec([[A1,*,self,*,A2,*,vith,A3] ,[A1,self,A3,to,*,A2] ,[A1,A3,*,self,*,A2] ,[Al,together,vith,A3,*,self ,*,A2]]) ,types(tie_up_verb) ,template_semantics(prim_tie_up([A1,A3],A2,[f(tie_up_status,existing , q )]))) .171The single pattern - [A1,*,self,*,A2,*, with, A3] gives the rule -rule(join ,template semantics(prim_tie up([A1,A3],A2 ,,[f(tie up_status,existing,[])])) )-->glsphrase(A1,[type(np)],formal([organization])) ,ignore ,term(gls(_,type([join,v]),_)) ,ignore ,glsphrase(A2,[type(np)],formal([organization])) ,ignore ,word(with) ,glsphrase(A3,[type(np)],formal([organization])) .Specific Issues ?
EJV Text 059 2(1) Coreference determinatio nfor : "LOCAL CONCERN" ,"UNION PRECISION CASTING CO .
OF TAIWAN"We do not have a mechanism to associate "IN TAIWAN" and " LOCAL CONCERN" we additionall yfailed to identify the company name completely .for : "A JAPANESE TRADING HOUSE" ,"TAGA CO ., A COMPANY ACTIVE IN TRADING WITH TAIWAN"We did not have any lexical entry for "TRADING HOUSE", when this was added we detected aJapanese company.
However, it is not at all clear how we determine that "TAGA" is the company .The reference depends on three companies being mentioned and three referred too .for : "A JOINT VENTURE" ,"THE NEW COMPANY, BASED IN KAOHSIUNG, SOUTHERN TAIWAN" ,We missed the location ?
couldn't tie "new company" to anything .
"THE JOINT VENTURE, BRIDGESTONE SPORTS TAIWAN CO ."
,The noun phrase marks the company as the child and this is resolved back to the empty child slo tfrom the previous sentence .
"THE TAIWAN UNIT"Missed this as we have no lexical entry for " UNIT" .for : "BRIDGESTONE SPORTS CO ."
,We get this first reference, then throw it away in the inference stage (not enough evidence from th eparse ?
one parent name, no child name or partne r"BRIDGESTON SPORTS" ,This ref.
appears in Para 2, naming an announcement .
We miss it .
"BRIDGESTONE SPORTS" ,This ref is the in the ownership, and we get it here .
It 's then resolved across to the tie-up .172"THE JAPANESE SPORTS GOODS MAKER"Once again, no lexical entry for "MAKER"(la) Which coreferences did your system get?
Of those, which could it have gotten 6 months ag o(at the previous evaluation)?
How can you improve the system to get the rest?At the 18 month, we got 2 separate tie-ups for two different mentions of a jv-like event .
Our systemis more precise now and we need to improve recall .
The ownership patterns, which are essentially list sof pairs, need an additional mechanism, or a new semantic type (COMPANY+PERCENT) to ensur etheir detection .
(2) Did your system get the OWNERSHIPs, in particular from " .
.
.
THE REMAINDER BY TAG ACO."
?The quote from the article is :THE NEW COMPANY, BASED IN KAOHSIUNG, SOUTHERN TAIWAN, IS OWNED 75 PCT B YBRIDGESTONE SPORTS, 15 PCT BY UNION PRECISION CASTING CO .
OF TAIWAN AND THEREMAINDER BY TAGA CO ., A COMPANY ACTIVE IN TRADING WITH .
TAIWAN, THE OFFICIALSSAIDWe get the 75 percent, and none of the others .Score: Precision 94, Recall 58, P&R 72 (after fix to lexicon )Specific Issues ?
EME Text 278956 8(1) What information triggers the instantiation of each of the two LITHOGRAPHY objects ?
"Stepper" has semantic tags which indicate it participates in a LITHOGRAPHY object-equip( 'stepper ',type ([[process type,'LITHOGRAPHY'], [equipment_type,'STEPPER ')]) )(2) What information indicates the role of the Nikon Corp .
for each Microelectronics Capability?The GLS patterns ?
<Organization> .
.
.
market .
.
.
<Equipment> gives us manufacturer anddistributor .
(3) Explain how your system captured the GRANULARITY information for "The company's lates tstepper .
"Didn't get either one, even though the measures are tagged, and the word "resolution" is marked a sa process identifier .
The parser co-ordination mechanism split the granularity measures into a separat eprocess list .
This did not get resolved, as it had no process type or other information for the resolver .4) How does your system determine EQUIPMENT_TYPE for " the new stepper " ?
and for " thecompany's latest stepper" ?They are "stepper"s ?
there is a lexical item for this .
(5) How does your system determine the STATUS of each equipment object ?This is supplied by the GLS entry.
The parse, or the inference stage in some cases, detects th eentity role - here it is " MANUFACTURER" - and infers that the equipment is in use .
Entity rolesare also inferred from roles attached to particular nouns (e .g .
University = Developer) .
(6) Why is the DEVICE object only instantiated for LITHOGRAPHY-1 ?The reference resolver will always resolve devices to the closest process type .
Here we have onl yproduced only one anyway.Score: Precision 83, Recall 34, P&R 49Specific Issues ?
JJV Text 0002(1) How to detect a reportable tie-u pThe GLS verb and the cospec patterns in the text determine whether or not there is a reportabl etie-up in the text .173(2) How many tie-ups in article 0002 and strategies to detect the second tie-up in the articleThe system detects three tie-ups .
The tie-up in the second sentence is captured based on a GLSverb ('#' teikei = tie up) in the sentence and its copec patterns, and there is only one tie up in th esentence and all the detected organizations are treated as partners .
(3) How to detect entities in a tie-upAt the sentence level, the cospec patterns of GLS verbs determine the number of entities in th etie-up.
In addition, within the sentence the tags (for organization, person) and local context are usedby the parser to decide the entities in the tie-up .
(4) The number of discourse entities and how to determine whether they are reportabl eThere are four entities, three tie-ups, two industries and two activities .
The entities and tie-upsare determined by the GLS cospec patterns which have been transformed into grammar rules .
(5) Difficulties in identifying the correct number of entities, tie-ups, and tie-up relationsThe entities and tie-ups are determined by the GLS cospec patterns which have been transforme dinto grammar rules .
As the recall goes up, the system captures more objects and the resolver shoul dresolve them correctly .
If the resolver does so too loosely and resolve many objects into a few, it i sthe case of under-generation .
If the resolver does it too strictly, then it is over-generation .
The thirdtie-up for article 0002 should be resolved with the first tie-up .
(6) How to detect aliasesThe system uses character-based rules for identifying aliases .
For example, if a company nam estarts with ` El A ' (Hitachi) as in ` ATI t' (Hitachi Manufacturing), then the system looks for thestring 'El A' (Hitachi) or the first two characters of the company name as its alias .
The system alsostores special aliases in its knowledge base which are difficult to recognize with the character-basedrules, for example, J A L ' for ' 8 *full' (Japan Airlines) .
(7) Problems in detecting the alias for the ENTITY named Toukyou Kaijou Kasai Hoke nBoth the full name and the alias of the ENTITY are tagged as organization at the tagging stag eand they are resolved as the same organization by the resolver later on in the system .
Thus the systemcorrectly recognized the alias for the ENTITY .
(8) Ilow to decide a general description of an activity in the second sentence of article 000 2Whether a description is general or not, if there is an activity term together with a verb whichshows some on-going economic activity in a sentence, they will produce an activity object .
(9) The way the system handles ` ryousha' (the both companies) in sentence 2 in 0002 and th eparticle(no of)The system treats 'ryousha ' as legitimate organization without a specific name .
The referenc eresolver tries to resolve 'ryousha ' with two particular organizations .
However, in the case of an orga-nization name followed by 'no ryousha' COME') is commented out at the tagging stage because i tdoes not give any more specific information than two particular organization names which precedes it ,and thus ignored by the following processes in the system .Score: Precision 72, Recall 72, P&R 72Specific Issues ?
JME Text 045 2(1) How to determine the existence of a reportable microelectronics capability .The GLS verb and the cospec patterns in the text determine whether or not there is a reportabl emicroelectronics capability in the text .
(2) Three entities are mentioned in the article .
How does the system determine which were involvedin the ME capability ?At the sentence level, the cospec patterns of GLS verbs determine the number of entities in th eME capability.
The tie-up company is not captured because of the lack of the coverage of current GL Sverbs .
(3) How to identify company names and how to associate them with their locations .Companies or organizations are detected in two ways .
First for known company names are taggedby straightforward string matching using a list of company names .
Second, for unknown companies ,174the system tries to find them by using particle information and local context in the sentence .
The twocompany names in the first sentence are identified because they are in the list of known organizatio nnames.
If the location information is located close to the company name, it is grouped with th ecompany name and treated as a part of the company information by the system .
(4) How to associate film type with each ME capability .If the film type information is located close to the detected process, then it is associated with theprocess as in article 0452 .
If the film type is found remotely, then reference resolver tries to associat eit with a proper process if found .
(5) How to determine the existence of reportable equipment .An equipment name is tagged at the tagging stage using a list of known equipment names suchas CVD system, PVD system .
If a process is followed by specific terms which indicate clearly a nequipment, such as 'ff' (souchi = equipment or system), then the process and the term are changedinto an equipment with the process information included .
At the parsing stage, if the equipment i slocated in the sentence in such a way that it matches with one of the cospec patterns of the GLS ver bin the sentence, then it is detected as reportable equipment .
It is usually associated with one or mor eorganizations.
Detection of a stand alone equipment does not by itself generate a new ME capability.Score: Precision 91, Recall 72, P&R 8 1CONCLUSIONSWe have learned a great deal over the past two years, partly through the many mistakes we hav emade.
The project has depended a great deal on the skill and care of the people working on it t oensure consistency in our data and code.
Given the large number of knowledge bases in our syste mthis is an onerous task and one task needed for the future is a system which allows this knowledge t obe integrated and held in one central data-base, where consistency can be maintained .
The second isto develop an easily configurable and portable reference resolution engine .There are no major differences in the structure of the English and Japanese systems .
It wouldseem that a critical part of achieving high precision and recall is to have at least one person with areasonable knowledge of the whole system to carry out repeated test/improve cycles .The current system is robust and provides a good starting point for the application of more sophis-ticated techniques, some of them simply refined versions of the current architecture .
Given appropriatedata it should be possible to produce a similar system for a different domain in a matter of months .Many parts of the system are portable in particular the semantic tagging mechanisms, the statistica lfiltering component .
Dates, companies and people - all of which occur in many kinds of text - we no whandle with good levels of accuracy .
The strange conventions of equipment names have provided uswith some interesting new challenges .ACKNOWLEDGEMENT SThe system described here has been funded by DARPA under contract number MDA904-91-C-9328 .We would like to express our thanks to our colleagues at BBN who have shared their part o fspeech tagger (POST) with us .
Thanks also to Kyoto University for allowing us to use the JUMA Nsegementor and part of speech tagger .Diderot is a team effort and is a result of the work of many people .
The following colleagues at CR Land Brandeis have contributed time, ideas, programming ability and enthusiasm, to the developmen tof the Diderot system; Paul Buitellar, Federica Busa, Peter Dilworth, Steve Helmreich and Fang Lin .References[1] DARPA .
Proceedings of the Third Message Understanding Conference (MUC-3), San Mateo, CA ,1991 .
Morgan Kaufmann .175[2] DARPA.
Proceedings of the Fourth Message Understanding Conference (MUC-4), San Mateo, CA ,1992 .
Morgan Kaufmann .
[3] Cowie, J ., Guthrie, L ., Wakao, T ., Jin, W., Pustejovsky, J .
and Waterman, S ., The DiderotInformation Extraction System .
In Proceedings of the First Conference of the Pacific Associatio nfor Computational Linguistics (PACLING93), Vancouver, Canada, 1993 .
[4] Grishman, R., and Sterling, J ., Acquisition of selectional patterns.
In Proceedings of the 14thInternational Conference on Computational Linguistics (COLING92), Nantes, France, 1992 .
[5] Guthrie, L., Bruce, R., Stein, G .C., and Weng, F ., Development of an application independentlexicon : Lexbase .
Technical Report MCCS-92-247, Computing Research Laboratory, New Mexic oState University, 1992 .
[6] Guthrie, L ., and Walker, E., Some comments on classification by machine .
Technical Repor tMCCS-92-935, Computing Research Laboratory, New Mexico State University, 1992 .
[7] Lehnert, W., and Sundheim, B., An evaluation of text analysis technologies .
AI Magazine ,12(3) :81-94, 1991 .
[8] Proctor, P., editor .
Longman Dictionary of Contemporary English .
Longman, Harlow, 1978 .
[9] Pustejovsky, J ., The generative lexicon .
Computational Linguistics, 17(4), 1991 .
[10] Pustejovsky, J ., The acquisition of lexical semantic knowledge from large corpora .
In Proceedingsof the DARPA Spoken and Written Language Workshop.
Morgan Kaufmann, 1992 .
[11] Pustejovsky, J ., Bergler, S ., and Anick, P ., Lexical semantic techniques for corpus analysis .Computational Linguistics, 1993 .
[12] Pustejovsky, J .
and Boguraev, B ., Lexical knowledge representation and natural language pro-cessing .
Artificial Intelligence, 1993 .
[13] Pustejovsky, J ., The Generative Lexicon : A Computational Theory of Lexical Semantics MITPress, Cambridge, MA, 1994 .
[14] Schabes, Y .
and Shieber, S ., An alternative conception of tree-adjoining derivation .
In Proceedingsof 80th Annual Meeting of the Association for Computational Linguistics, 1992 .176Summary of Error-based Score sJAPANESE MICROERR UND OVG SUB Min Max18-Month 72 60 28 18 .74 .8024-Month 65 54 24 12 .69 .73JAPANESE JVERR UND OVG SUB Min Max18-Month 79 71 22 22 .86 .8624-Month 63 51 23 12 .70 .72ENGLISH MICROERR UND OVG SUB Min Max18-Month 86 76 33 37 .87 .9324-Month 74 60 33 24 .80 .84ENGLISH JVERR UND OVG SUB Min Max18-Month 91 76 40 56 1 .06 1.0824-Month 79 67 28 28 0.89 0.91177Summary of Recall/Precision-based ScoresJAPANESE MICROTF(R/P) REC PRE P & R18 - Month 73/83 32 59 41 .9924 - Month 84/90 40 66 50.37JAPANESE JVTF(R/P) REC PRE P & R18 - Month 82/99 26 61 32.824 - Month 88/98 42 67 52.
1ENGLISH MICROTF(R/P) REC PRE P & R18 - Month 77!76 15 42 22.2824 - Month 78/83 31 51 38.49ENGLISH JVTF(R/P) REC PRE P & R18 - Month 67/86 10 26 15.1 024 - Month 76/92 24 51 32.6417 8Progress since 18 month worksho p_40_ 30_ 100?
70?
60_50JAPANESE JV 18MIH AND 24MIH C) PARJSO N_ 70JAPANESE ME 18MIH AND 24MIH COMPARISON_ 70_60PRECISIONPRECISIO N_50_ 40_50402010_ 302010lI_I	 118 months24 months 018 months24 months179
