Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 720?727,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Probabilistic Approach to Syntax-based Reorderingfor Statistical Machine TranslationChi-Ho Li, Dongdong Zhang, Mu Li, Ming ZhouMicrosoft Research AsiaBeijing, Chinachl, dozhang@microsoft.commuli, mingzhou@microsoft.comMinghui Li, Yi GuanHarbin Institute of TechnologyHarbin, Chinamhli@insun.hit.edu.cnguanyi@insun.hit.edu.cnAbstractInspired by previous preprocessing ap-proaches to SMT, this paper proposes anovel, probabilistic approach to reorderingwhich combines the merits of syntax andphrase-based SMT.
Given a source sentenceand its parse tree, our method generates,by tree operations, an n-best list of re-ordered inputs, which are then fed to stan-dard phrase-based decoder to produce theoptimal translation.
Experiments show that,for the NIST MT-05 task of Chinese-to-English translation, the proposal leads toBLEU improvement of 1.56%.1 IntroductionThe phrase-based approach has been considered thedefault strategy to Statistical Machine Translation(SMT) in recent years.
It is widely known that thephrase-based approach is powerful in local lexicalchoice and word reordering within short distance.However, long-distance reordering is problematicin phrase-based SMT.
For example, the distance-based reordering model (Koehn et al, 2003) al-lows a decoder to translate in non-monotonous or-der, under the constraint that the distance betweentwo phrases translated consecutively does not ex-ceed a limit known as distortion limit.
In theory thedistortion limit can be assigned a very large valueso that all possible reorderings are allowed, yet inpractise it is observed that too high a distortion limitnot only harms efficiency but also translation per-formance (Koehn et al, 2005).
In our own exper-iment setting, the best distortion limit for Chinese-English translation is 4.
However, some ideal trans-lations exhibit reorderings longer than such distor-tion limit.
Consider the sentence pair in NIST MT-2005 test set shown in figure 1(a): after translatingthe word ?V/mend?, the decoder should ?jump?across six words and translate the last phrase ??
?_/fissures in the relationship?.
Therefore,while short-distance reordering is under the scopeof the distance-based model, long-distance reorder-ing is simply out of the question.A terminological remark: In the rest of the paper,we will use the terms global reordering and localreordering in place of long-distance reordering andshort-distance reordering respectively.
The distinc-tion between long and short distance reordering issolely defined by distortion limit.Syntax1 is certainly a potential solution to globalreordering.
For example, for the last two Chinesephrases in figure 1(a), simply swapping the two chil-dren of the NP node will produce the correct wordorder on the English side.
However, there are alsoreorderings which do not agree with syntactic anal-ysis.
Figure 1(b) shows how our phrase-based de-coder2 obtains a good English translation by reorder-ing two blocks.
It should be noted that the secondChinese block ?
?e?
and its English counterpart?at the end of?
are not constituents at all.In this paper, our interest is the value of syntax inreordering, and the major statement is that syntacticinformation is useful in handling global reordering1Here by syntax it is meant linguistic syntax rather than for-mal syntax.2The decoder is introduced in section 6.720Figure 1: Examples on how syntax (a) helps and (b) harms reordering in Chinese-to-English translationThe lines and nodes on the top half of the figures show the phrase structure of the Chinese sentences, while the links on the bottomhalf of the figures show the alignments between Chinese and English phrases.
Square brackets indicate the boundaries of blocksfound by our decoder.and it achieves better MT performance on the ba-sis of the standard phrase-based model.
To prove it,we developed a hybrid approach which preserves thestrength of phrase-based SMT in local reordering aswell as the strength of syntax in global reordering.Our method is inspired by previous preprocessingapproaches like (Xia and McCord, 2004), (Collinset al, 2005), and (Costa-jussa` and Fonollosa, 2006),which split translation into two stages:S ?
S?
?
T (1)where a sentence of the source language (SL), S,is first reordered with respect to the word order ofthe target language (TL), and then the reordered SLsentence S?
is translated as a TL sentence T bymonotonous translation.Our first contribution is a new translation modelas represented by formula 2:S ?
n?
S?
?
n?
T ?
T?
(2)where an n-best list of S?, instead of only one S?, isgenerated.
The reason of such change will be givenin section 2.
Note also that the translation processS?
?T is not monotonous, since the distance-basedmodel is needed for local reordering.
Our secondcontribution is our definition of the best translation:argmaxTexp(?rlogPr(S?S?)+?i?iFi(S?
?T ))where Fi are the features in the standard phrase-based model and Pr(S ?
S?)
is our new feature,viz.
the probability of reordering S as S?.
The de-tails of this model are elaborated in sections 3 to 6.The settings and results of experiments on this newmodel are given in section 7.2 Related WorkThere have been various attempts to syntax-based SMT, such as (Yamada and Knight, 2001)and (Quirk et al, 2005).
We do not adopt thesemodels since a lot of subtle issues would then be in-troduced due to the complexity of syntax-based de-coder, and the impact of syntax on reordering willbe difficult to single out.There have been many reordering strategies un-der the phrase-based camp.
A notable approach islexicalized reordering (Koehn et al, 2005) and (Till-mann, 2004).
It should be noted that this approachachieves the best result within certain distortion limitand is therefore not a good model for global reorder-ing.There are a few attempts to the preprocessingapproach to reordering.
The most notable onesare (Xia and McCord, 2004) and (Collins et al,2005), both of which make use of linguistic syntaxin the preprocessing stage.
(Collins et al, 2005) an-alyze German clause structure and propose six types721of rules for transforming German parse trees withrespect to English word order.
Instead of relyingon manual rules, (Xia and McCord, 2004) proposea method in learning patterns of rewriting SL sen-tences.
This method parses training data and usessome heuristics to align SL phrases with TL ones.From such alignment it can extract rewriting pat-terns, of which the units are words and POSs.
Thelearned rewriting rules are then applied to rewrite SLsentences before monotonous translation.Despite the encouraging results reported in thesepapers, the two attempts share the same shortcomingthat their reordering is deterministic.
As pointed outin (Al-Onaizan and Papineni, 2006), these strategiesmake hard decisions in reordering which cannot beundone during decoding.
That is, the choice of re-ordering is independent from other translation fac-tors, and once a reordering mistake is made, it can-not be corrected by the subsequent decoding.To overcome this weakness, we suggest a methodto ?soften?
the hard decisions in preprocessing.
Theessence is that our preprocessing module generatesn-best S?s rather than merely one S?.
A variety ofreordered SL sentences are fed to the decoder sothat the decoder can consider, to certain extent, theinteraction between reordering and other factors oftranslation.
The entire process can be depicted byformula 2, recapitulated as follows:S ?
n?
S?
?
n?
T ?
T?
.Apart from their deterministic nature, the twoprevious preprocessing approaches have their ownweaknesses.
(Collins et al, 2005) count on man-ual rules and it is suspicious if reordering rules forother language pairs can be easily made.
(Xia andMcCord, 2004) propose a way to learn rewritingpatterns, nevertheless the units of such patterns arewords and their POSs.
Although there is no limit tothe length of rewriting patterns, due to data sparse-ness most patterns being applied would be shortones.
Many instances of global reordering are there-fore left unhandled.3 The Acquisition of ReorderingKnowledgeTo avoid this problem, we give up using rewritingpatterns and design a form of reordering knowledgewhich can be directly applied to parse tree nodes.Given a node N on the parse tree of an SL sentence,the required reordering knowledge should enable thepreprocessing module to determine how probablethe children of N are reordered.3 For simplicity, letus first consider the case of binary nodes only.
LetN1 and N2, which yield phrases p1 and p2 respec-tively, be the child nodes of N .
We want to deter-mine the order of p1 and p2 with respect to their TLcounterparts, T (p1) and T (p2).
The knowledge formaking such a decision can be learned from a word-aligned parallel corpus.
There are two questions in-volved in obtaining training instances:?
How to define T (pi)??
How to define the order of T (pi)s?For the first question, we adopt a similar methodas in (Fox, 2002): given an SL phrase ps =s1 .
.
.
si .
.
.
sn and a word alignment matrix A, wecan enumerate the set of TL words {ti : ti?A(si)},and then arrange the words in the order as they ap-pear in the TL sentence.
Let first(t) be the first wordin this sorted set and last(t) be the last word.
T (ps)is defined as the phrase first(t) .
.
.
last(t) in the TLsentence.
Note that T (ps) may contain words not inthe set {ti}.The question of the order of two TL phrases is nota trivial one.
Since a word alignment matrix usu-ally contains a lot of noises as well as one-to-manyand many-to-many alignments, two TL phrases mayoverlap with each other.
For the sake of the qualityof reordering knowledge, if T (p1) and T (p2) over-lap, then the node N with children N1 and N2 isnot taken as a training instance.
Obviously it willgreatly reduce the amount of training input.
To rem-edy data sparseness, less probable alignment pointsare removed so as to minimize overlapping phrases,since, after removing some alignment point, one ofthe TL phrases may become shorter and the twophrases may no longer overlap.
The implementationis similar to the idea of lexical weight in (Koehn etal., 2003): all points in the alignment matrices of theentire training corpus are collected to calculate theprobabilistic distribution, P (t|s), of some TL word3Some readers may prefer the expression the subtree rootedat node N to node N .
The latter term is used in this paper forsimplicity.722t given some SL word s. Any pair of overlappingT (pi)s will be redefined by iteratively removing lessprobable word alignments until they no longer over-lap.
If they still overlap after all one/many-to-manyalignments have been removed, then the refinementwill stop and N , which covers pis, is no longer takenas a training instance.In sum, given a bilingual training corpus, a parserfor the SL, and a word alignment tool, we can collectall binary parse tree nodes, each of which may be aninstance of the required reordering knowledge.
Thenext question is what kind of reordering knowledgecan be formed out of these training instances.
Twoforms of reordering knowledge are investigated:1.
Reordering Rules, which have the formZ : X Y ?
{X Y Pr(IN-ORDER)Y X Pr(INVERTED)where Z is the phrase label of a binary nodeand X and Y are the phrase labels of Z?s chil-dren, and Pr(INVERTED) and Pr(IN-ORDER)are the probability that X and Y are inverted onTL side and that not inverted, respectively.
Theprobability figures are estimated by MaximumLikelihood Estimation.2.
Maximum Entropy (ME) Model, which doesthe binary classification whether a binarynode?s children are inverted or not, based on aset of features over the SL phrases correspond-ing to the two children nodes.
The features thatwe investigated include the leftmost, rightmost,head, and context words4, and their POSs, ofthe SL phrases, as well as the phrase labels ofthe SL phrases and their parent.4 The Application of ReorderingKnowledgeAfter learning reordering knowledge, the prepro-cessing module can apply it to the parse tree, tS ,of an SL sentence S and obtain the n-best list ofS?.
Since a ranking of S?
is needed, we need someway to score each S?.
Here probability is used asthe scoring metric.
In this section it is explained4The context words of the SL phrases are the word to the leftof the left phrase and the word to the right of the right phrase.how the n-best reorderings of S and their associatedscores/probabilites are computed.Let us first look into the scoring of a particularreordering.
Let Pr(p?p?)
be the probability of re-ordering a phrase p into p?.
For a phrase q yielded bya non-binary node, there is only one ?reordering?
ofq, viz.
q itself, thus Pr(q?q) = 1.
For a phrase pyielded by a binary node N , whose left child N1 hasreorderings pi1 and right child N2 has the reorder-ings pj2 (1 ?
i, j ?
n), p?
has the form pi1pj2 or pj2pi1.Therefore, Pr(p?p?)
={Pr(IN-ORDER)?
Pr(pi1?pi?1 )?
Pr(pj2?pj?2 )Pr(INVERTED)?
Pr(pj2?pj?2 )?
Pr(pi1?pi?1 )The figures Pr(IN-ORDER) and Pr(INVERTED) areobtained from the learned reordering knowledge.
Ifreordering knowledge is represented as rules, thenthe required probability is the probability associatedwith the rule that can apply to N .
If reorderingknowledge is represented as an ME model, then therequired probability is:P (r|N) = exp(?i ?ifi(N, r))?r?
exp(?i ?ifi(N, r?
))where r?
{IN-ORDER, INVERTED}, and fi?s are fea-tures used in the ME model.Let us turn to the computation of the n-best re-ordering list.
Let R(N) be the number of reorder-ings of the phrase yielded by N , then:R(N) ={2R(N1)R(N2) if N has children N1, N21 otherwiseIt is easily seen that the number of S?s increases ex-ponentially.
Fortunately, what we need is merely ann-best list rather than a full list of reorderings.
Start-ing from the leaves of tS , for each node N coveringphrase p, we only keep track of the n p?s that havethe highest reordering probability.
Thus R(N) ?
n.There are at most 2n2 reorderings for any node andonly the top-scored n reorderings are recorded.
Then-best reorderings of S, i.e.
the n-best reorderingsof the yield of the root node of tS , can be obtainedby this efficient bottom-up method.5 The Generalization of ReorderingKnowledgeIn the last two sections reordering knowledge islearned from and applied to binary parse tree nodes723only.
It is not difficult to generalize the theory ofreordering knowledge to nodes of other branchingfactors.
The case of binary nodes is simple as thereare only two possible reorderings.
The case of 3-arynodes is a bit more complicated as there are six.5 Ingeneral, an n-ary node has n!
possible reorderingsof its children.
The maximum entropy model has thesame form as in the binary case, except that there aremore classes of reordering patterns as n increases.The form of reordering rules, and the calculation ofreordering probability for a particular node, can alsobe generalized easily.6 The only problem for thegeneralized reordering knowledge is that, as thereare more classes, data sparseness becomes more se-vere.6 The DecoderThe last three sections explain how the S?n?S?part of formula 2 is done.
The S?
?Tpart is simply done by our re-implementationof PHARAOH (Koehn, 2004).
Note that non-monotonous translation is used here since thedistance-based model is needed for local reordering.For the n?T?
T?
part, the factors in considerationinclude the score of T returned by the decoder, andthe reordering probability Pr(S ?
S?).
In orderto conform to the log-linear model used in the de-coder, we integrate the two factors by defining thetotal score of T as formula 3:exp(?r logPr(S?S?)
+?i?iFi(S?
?T )) (3)The first term corresponds to the contribution ofsyntax-based reordering, while the second term thatof the features Fi used in the decoder.
All the fea-ture weights (?s) were trained using our implemen-tation of Minimum Error Rate Training (Och, 2003).The final translation T?
is the T with the highest totalscore.5Namely, N1N2N3, N1N3N2, N2N1N3, N2N3N1,N3N1N2, and N3N2N1, if the child nodes in the original orderare N1, N2, and N3.6For example, the reordering probability of a phrase p =p1p2p3 generated by a 3-ary node N isPr(r)?Pr(pi1)?Pr(pj2)?Pr(pk3)where r is one of the six reordering patterns for 3-ary nodes.It is observed in pilot experiments that, for a lot oflong sentences containing several clauses, only oneof the clauses is reordered.
That is, our greedy re-ordering algorithm (c.f.
section 4) has a tendency tofocus only on a particular clause of a long sentence.The problem was remedied by modifying our de-coder such that it no longer translates a sentence atonce; instead the new decoder does:1. split an input sentence S into clauses {Ci};2. obtain the reorderings among {Ci}, {Sj};3. for each Sj , do(a) for each clause Ci in Sj , doi.
reorder Ci into n-best C ?is,ii.
translate each C ?i into T (C?i),iii.
select T?
(C ?i);(b) concatenate {T?
(C ?i)} into Tj ;4. select T?j .Step 1 is done by checking the parse tree if thereare any IP or CP nodes7 immediately under the rootnode.
If yes, then all these IPs, CPs, and the remain-ing segments are treated as clauses.
If no, then theentire input is treated as one single clause.
Step 2and step 3(a)(i) still follow the algorithm in sec-tion 4.
Step 3(a)(ii) is trivial, but there is a subtlepoint about the calculation of language model score:the language model score of a translated clause is notindependent from other clauses; it should take intoaccount the last few words of the previous translatedclause.
The best translated clause T?
(C ?i) is selectedin step 3(a)(iii) by equation 3.
In step 4 the besttranslation T?j isargmaxTjexp(?rlogPr(S?Sj)+?iscore(T (C ?i))).7 Experiments7.1 CorporaOur experiments are about Chinese-to-Englishtranslation.
The NIST MT-2005 test data set is usedfor evaluation.
(Case-sensitive) BLEU-4 (Papineniet al, 2002) is used as the evaluation metric.
The7 IP stands for inflectional phrase and CP for complementizerphrase.
These two types of phrases are clauses in terms of theGovernment and Binding Theory.724Branching Factor 2 3 >3Count 12294 3173 1280Percentage 73.41 18.95 7.64Table 1: Distribution of Parse Tree Nodes with Dif-ferent Branching Factors Note that nodes with only onechild are excluded from the survey as reordering does not applyto such nodes.test set and development set of NIST MT-2002 aremerged to form our development set.
The trainingdata for both reordering knowledge and translationtable is the one for NIST MT-2005.
The GIGA-WORD corpus is used for training language model.The Chinese side of all corpora are segmented intowords by our implementation of (Gao et al, 2003).7.2 The Preprocessing ModuleAs mentioned in section 3, the preprocessing mod-ule for reordering needs a parser of the SL, a wordalignment tool, and a Maximum Entropy trainingtool.
We use the Stanford parser (Klein and Man-ning, 2003) with its default Chinese grammar, theGIZA++ (Och and Ney, 2000) alignment packagewith its default settings, and the ME tool developedby (Zhang, 2004).Section 5 mentions that our reordering model canapply to nodes of any branching factor.
It is inter-esting to know how many branching factors shouldbe included.
The distribution of parse tree nodesas shown in table 1 is based on the result of pars-ing the Chinese side of NIST MT-2002 test set bythe Stanford parser.
It is easily seen that the major-ity of parse tree nodes are binary ones.
Nodes withmore than 3 children seem to be negligible.
The 3-ary nodes occupy a certain proportion of the distri-bution, and their impact on translation performancewill be shown in our experiments.7.3 The decoderThe data needed by our Pharaoh-like decoder aretranslation table and language model.
Our 5-gramlanguage model is trained by the SRI language mod-eling toolkit (Stolcke, 2002).
The translation tableis obtained as described in (Koehn et al, 2003), i.e.the alignment tool GIZA++ is run over the trainingdata in both translation directions, and the two align-Test Setting BLEUB1 standard phrase-based SMT 29.22B2 (B1) + clause splitting 29.13Table 2: Experiment BaselineTest Setting BLEU BLEU2-ary 2,3-ary1 rule 29.77 30.312 ME (phrase label) 29.93 30.493 ME (left,right) 30.10 30.534 ME ((3)+head) 30.24 30.715 ME ((3)+phrase label) 30.12 30.306 ME ((4)+context) 30.24 30.76Table 3: Tests on Various Reordering ModelsThe 3rd column comprises the BLEU scores obtained by re-ordering binary nodes only, the 4th column the scores by re-ordering both binary and 3-ary nodes.
The features used in theME models are explained in section 3.ment matrices are integrated by the GROW-DIAG-FINAL method into one matrix, from which phrasetranslation probabilities and lexical weights of bothdirections are obtained.The most important system parameter is, ofcourse, distortion limit.
Pilot experiments using thestandard phrase-based model show that the optimaldistortion limit is 4, which was therefore selected forall our experiments.7.4 Experiment Results and AnalysisThe baseline of our experiments is the standardphrase-based model, which achieves, as shown bytable 2, the BLEU score of 29.22.
From the sametable we can also see that the clause splitting mech-anism introduced in section 6 does not significantlyaffect translation performance.Two sets of experiments were run.
The first set,of which the results are shown in table 3, tests theeffect of different forms of reordering knowledge.In all these tests only the top 10 reorderings ofeach clause are generated.
The contrast betweentests 1 and 2 shows that ME modeling of reorderingoutperforms reordering rules.
Tests 3 and 4 showthat phrase labels can achieve as good performanceas the lexical features of mere leftmost and right-most words.
However, when more lexical features725Input 0 2005#?R????q?Z??/??
?=?Reference Hainan province will continue to increase its investment in the public services andsocial services infrastructures in 2005Baseline Hainan Province in 2005 will continue to increase for the public service and socialinfrastructure investmentTranslation withPreprocessingHainan Province in 2005 will continue to increase investment in public servicesand social infrastructureTable 4: Translation Example 1Test Setting BLEUa length constraint 30.52b DL=0 30.48c n=100 30.78Table 5: Tests on Various Constraintsare added (tests 4 and 6), phrase labels can no longercompete with lexical features.
Surprisingly, test 5shows that the combination of phrase labels and lex-ical features is even worse than using either phraselabels or lexical features only.Apart from quantitative evaluation, let us con-sider the translation example of test 6 shown in ta-ble 4.
To generate the correct translation, a phrase-based decoder should, after translating the word??
as ?increase?, jump to the last word ?=?(investment)?.
This is obviously out of the capa-bility of the baseline model, and our approach canaccomplish the desired reordering as expected.By and large, the experiment results show that nomatter what kind of reordering knowledge is used,the preprocessing of syntax-based reordering doesgreatly improve translation performance, and thatthe reordering of 3-ary nodes is crucial.The second set of experiments test the effect ofsome constraints.
The basic setting is the same asthat of test 6 in the first experiment set, and reorder-ing is applied to both binary and 3-ary nodes.
Theresults are shown in table 5.In test (a), the constraint is that the module doesnot consider any reordering of a node if the yieldof this node contains not more than four words.The underlying rationale is that reordering withindistortion limit should be left to the distance-basedmodel during decoding, and syntax-based reorder-ing should focus on global reordering only.
Theresult shows that this hypothesis does not hold.In practice syntax-based reordering also helps lo-cal reordering.
Consider the translation exampleof test (a) shown in table 6.
Both the baselinemodel and our model translate in the same way upto the word ??w?
(which is incorrectly translatedas ?and?).
From this point, the proposed preprocess-ing model correctly jump to the last phrase ??q?
?X/discussed?, while the baseline model fail to doso for the best translation.
It should be noted, how-ever, that there are only four words between ?
?w?and the last phrase, and the desired order of decod-ing is within the capability of the baseline system.With the feature of syntax-based global reordering,a phrase-based decoder performs better even withrespect to local reordering.
It is because syntax-based reordering adds more weight to a hypothesisthat moves words across longer distance, which ispenalized by the distance-based model.In test (b) distortion limit is set as 0; i.e.
reorder-ing is done merely by syntax-based preprocessing.The worse result is not surprising since, after all,preprocessing discards many possibilities and thusreduce the search space of the decoder.
Some localreordering model is still needed during decoding.Finally, test (c) shows that translation perfor-mance does not improve significantly by raising thenumber of reorderings.
This implies that our ap-proach is very efficient in that only a small value ofn is capable of capturing the most important globalreordering patterns.8 Conclusion and Future WorkThis paper proposes a novel, probabilistic approachto reordering which combines the merits of syntaxand phrase-based SMT.
On the one hand, globalreordering, which cannot be accomplished by the726Input ?$3 ,?
)Z?C?wOcu?
?q?
?XReference Meanwhile , Yushchenko and his assistants discussed issues concerning the estab-lishment of a new governmentBaseline The same time , Yushchenko assistants and a new Government on issues discussedTranslation withPreprocessingThe same time , Yushchenko assistants and held discussions on the issue of a newgovernmentTable 6: Translation Example 2phrase-based model, is enabled by the tree opera-tions in preprocessing.
On the other hand, local re-ordering is preserved and even strengthened in ourapproach.
Experiments show that, for the NIST MT-05 task of Chinese-to-English translation, the pro-posal leads to BLEU improvement of 1.56%.Despite the encouraging experiment results, itis still not very clear how the syntax-based anddistance-based models complement each other inimproving word reordering.
In future we need toinvestigate their interaction and identify the contri-bution of each component.
Moreover, it is observedthat the parse trees returned by a full parser likethe Stanford parser contain too many nodes whichseem not be involved in desired reorderings.
Shal-low parsers should be tried to see if they improvethe quality of reordering knowledge.ReferencesYaser Al-Onaizan, and Kishore Papineni.
2006.
Distor-tion Models for Statistical Machine Translation.
Pro-ceedings for ACL 2006.Michael Collins, Philipp Koehn, and Ivona Kucerova.2005.
Clause Restructuring for Statistical MachineTranslation.
Proceedings for ACL 2005.M.R.
Costa-jussa`, and J.A.R.
Fonollosa.
2006.
Statis-tical Machine Reordering.
Proceedings for EMNLP2006.Heidi Fox.
2002.
Phrase Cohesion and Statistical Ma-chine Translation.
Proceedings for EMNLP 2002.Jianfeng Gao, Mu Li, and Chang-Ning Huang 2003.Improved Source-Channel Models for Chinese WordSegmentation.
Proceedings for ACL 2003.Dan Klein and Christopher D. Manning.
2003.
AccurateUnlexicalized Parsing.
Proceedings for ACL 2003.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-based Translation.
Proceedings forHLT-NAACL 2003.Philipp Koehn.
2004.
Pharaoh: a Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
Proceedings for AMTA 2004.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot 2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.Proceedings for IWSLT 2005.Franz J. Och.
2003.
Minimum Error Rate Training inStatistical Machine Translation.
Proceedings for ACL2003.Franz J. Och, and Hermann Ney.
2000.
Improved Statis-tical Alignment Models.
Proceedings for ACL 2000.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for Automatic Eval-uation of Machine Translation.
Proceedings for ACL2002.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency Treelet Translation: Syntactically InformedPhrasal SMT.
Proceedings for ACL 2005.Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
Proceedings for the Interna-tional Conference on Spoken Language Understand-ing 2002.Christoph Tillmann.
2004.
A Unigram OrientationModel for Statistical Machine Translation.
Proceed-ings for ACL 2004.Fei Xia, and Michael McCord 2004.
Improving a Statis-tical MT System with Automatically Learned RewritePatterns.
Proceedings for COLING 2004.Kenji Yamada, and Kevin Knight.
2001.
A syntax-based statistical translation model.
Proceedings forACL 2001.Le Zhang.
2004.
Maximum EntropyModeling Toolkit for Python and C++.http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.727
