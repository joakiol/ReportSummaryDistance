Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 110?114,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsEstimating Compact Yet Rich Tree Insertion GrammarsElif Yamangil and Stuart M. ShieberHarvard UniversityCambridge, Massachusetts, USA{elif, shieber}@seas.harvard.eduAbstractWe present a Bayesian nonparametric modelfor estimating tree insertion grammars (TIG),building upon recent work in Bayesian in-ference of tree substitution grammars (TSG)via Dirichlet processes.
Under our generalvariant of TIG, grammars are estimated viathe Metropolis-Hastings algorithm that usesa context free grammar transformation as aproposal, which allows for cubic-time stringparsing as well as tree-wide joint sampling ofderivations in the spirit of Cohn and Blun-som (2010).
We use the Penn treebank forour experiments and find that our proposalBayesian TIG model not only has competitiveparsing performance but also finds compactyet linguistically rich TIG representations ofthe data.1 IntroductionThere is a deep tension in statistical modeling ofgrammatical structure between providing good ex-pressivity ?
to allow accurate modeling of the datawith sparse grammars ?
and low complexity ?making induction of the grammars and parsing ofnovel sentences computationally practical.
Recentwork that incorporated Dirichlet process (DP) non-parametric models into TSGs has provided an effi-cient solution to the problem of segmenting train-ing data trees into elementary parse tree fragmentsto form the grammar (Cohn et al, 2009; Cohn andBlunsom, 2010; Post and Gildea, 2009).
DP infer-ence tackles this problem by exploring the space ofall possible segmentations of the data, in search forfragments that are on the one hand large enough sothat they incorporate the useful dependencies, andon the other small enough so that they recur and havea chance to be useful in analyzing unseen data.The elementary trees combined in a TSG are, in-tuitively, primitives of the language, yet certain lin-guistic phenomena (notably various forms of modifi-cation) ?split them up?, preventing their reuse, lead-ing to less sparse grammars than might be ideal.For instance, imagine modeling the following set ofstructures:?
[NP the [NN [NN [NN president] of the university] whoresigned yesterday]]?
[NP the [NN former [NN [NN president] of the univer-sity]]]?
[NP the [NN [NN president] who resigned yesterday]]A natural recurring structure here would be thestructure ?
[NP the [NN president]]?, yet it occursnot at all in the data.TSGs are a special case of the more flexible gram-mar formalism of tree adjoining grammar (TAG)(Joshi et al, 1975).
TAG augments TSG with anadjunction operator and a set of auxiliary trees inaddition to the substitution operator and initial treesof TSG, allowing for ?splicing in?
of syntactic frag-ments within trees.
In the example, by augmenting aTSG with an operation of adjunction, a grammar thathypothesizes auxiliary trees corresponding to ad-joining ?
[NN former NN ]?, ?
[NN NN of the uni-versity]?, and ?
[NN NN who resigned yesterday]?would be able to reuse the basic structure ?
[NP the[NN president]]?.Unfortunately, TAG?s expressivity comes at thecost of greatly increased complexity.
Parsing com-plexity for unconstrained TAG scales as O(n6), im-110NPDT NNthe presidentNPNP* SBARWHNPSwhoNPNP* PPINNPofNNJJNN*formerFigure 1: Example TIG derivation of an NP constituent:One left insertion (at NN) and two simultaneous right in-sertions (at NP).practical as compared to CFG and TSG?s O(n3).
Inaddition, the model selection problem for TAG issignificantly more complicated than for TSG sinceone must reason about many more combinatorial op-tions with two types of derivation operators.1 Thishas led researchers to resort to heuristic grammar ex-traction techniques (Chiang, 2000; Carreras et al,2008) or using a very small number of grammar cat-egories (Hwa, 1998).Hwa (1998) first proposed to use tree-insertiongrammars (TIG), a kind of expressive compromisebetween TSG and TAG, as a substrate on which tobuild grammatical inference.
TIG constrains the ad-junction operation so that spliced-in material fallscompletely to the left or completely to the right ofthe splice point.
By restricting the form of possibleauxiliary trees to only left or right auxiliary trees inthis way, TIG remains within the realm of context-free formalisms (with cubic complexity) while stillmodeling rich linguistic phenomena (Schabes andWaters, 1995).
Figure 1 depicts some examples ofTIG derivations.Sharing the same intuitions, Shindo et al (2011)have provided a previous attempt at combining TIGand Bayesian nonparametric principles, albeit withsevere limitations.
Their TIG variant (which we willrefer to as TIG0) is highly constrained in the follow-ing ways.1.
The foot node in an auxiliary tree must be the immediatechild of the root node.2.
Only one adjunction can occur at a given node.1This can be seen by the fact that tree-path languages underTAG are context free, whereas they are regular for TSG.
(Sch-abes and Waters, 1995)(a)(b)NPNPRNPLNPDT NNtheNPL?NNpresidentNNLNNRNNR?NPNP*PPIN NPof?NPSBARWHNPSwhoNPRNPNP* PPIN NPofNPNPR?NPRNPNP* SBARWHNPSwho?NPRNNJJNN*former ?NNLFigure 2: TIG-to-TSG transform: (a) and (b) illus-trate transformed TSG derivations for two different TIGderivations of the same parse tree structure.
The TIGnodes where we illustrate the transformation are in bold.
(We suppress the rest of the transformational nodes.)3.
Even modeling multiple adjunction with root adjunctionis disallowed.
There is thus no recursion possibility withadjunction, no stacking of auxiliary trees.4.
As a consequence of the prior two constraints, no adjunc-tion along the spines of auxiliary trees is allowed.5.
As a consequence of the first constraint, all nonterminalsalong the spine of an auxiliary tree are identical.In this paper we explore a Bayesian nonparamet-ric model for estimating a far more expressive ver-sion of TIG, and compare its performance againstTSG and the restricted TIG0 variant.
Our more gen-eral formulation avoids these limitations by support-ing the following features and thus relaxing four ofthe five restrictions of TIG0.1.
Auxiliary trees may have the foot node at depth greaterthan one.22.
Both left and right adjunctions may occur at the samenode.3.
Simultanous adjunction (that is, more than one left orright adjunction per node) is allowed via root adjunction.4.
Adjunctions may occur along the spines of auxiliary trees.The increased expressivity of our TIG variant ismotivated both linguistically and practically.
Froma linguistic point of view: Deeper auxiliary trees canhelp model large patterns of insertion and potentialcorrelations between lexical items that extend overmultiple levels of tree.
Combining left and rightauxiliary trees can help model modifiers of the samenode from left and right (combination of adjectives2Throughout the paper, we will refer to the depth of an aux-iliary tree to indicate the length of its spine.111and relative clauses for instance).
Simultaneous in-sertion allows us to deal with multiple independentmodifiers for the same constituent (for example, aseries of adjectives).
From a practical point of view,we show that an induced TIG provides modelingperformance superior to TSG and comparable withTIG0.
However we show that the grammars we in-duce are compact yet rich, in that they succinctlyrepresent complex linguistic structures.2 Probabilistic ModelIn the basic nonparametric TSG model, there is anindependent DP for every grammar category (suchas c = NP ), each of which uses a base distributionP0 that generates an initial tree by making stepwisedecisions.Ginitc ?
DP(?initc , Pinit0 (?
| c))The canonical P0 uses a probabilistic CFG P?
thatis fixed a priori to sample CFG rules top-down andBernoulli variables for determining where substitu-tions should occur (Cohn et al, 2009; Cohn andBlunsom, 2010).We extend this model by adding specialized DPsfor left and right auxiliary trees.3Grightc ?
DP(?rightc , Pright0 (?
| c))Therefore, we have an exchangeable process forgenerating right auxiliary treesp(aj | a<j) =naj + ?rightc Pright0 (aj | c)j ?
1 + ?rightc(1)as for initial trees in TSG.We must define three distinct base distributionsfor initial trees, left auxiliary trees, and right aux-iliary trees.
P init0 generates an initial tree with rootlabel c by sampling CFG rules from P?
and makinga binary decision at every node generated whetherto leave it as a frontier node or further expand (withprobability ?c) (Cohn et al, 2009).
Similarly, ourP right0 generates a right auxiliary tree with root la-bel c by first making a binary decision whether togenerate an immediate foot or not (with probability?rightc ), and then sampling an appropriate CFG rule3We use right insertions for illustration; the symmetric ana-log applies to left insertions.
(VP (, ,) (VP PP (VP (, ,) VP*)))(VP (SBAR (WHADVP (WRB (WRB When) ) ) S) (VP (, ,) VP*))(VP (PP (IN For) (NP NN )) (VP (, ,) VP*))(VP (CC But) (VP PP (VP (, ,) VP*)))(VP ADVP (VP (, ,) VP*))(IN (ADVP (RB (RB particularly) ) ) IN*)(NP PP (NP (CC and) (NP PP NP*)))Figure 3: Example left auxiliary trees that occur in thetop derivations for Section 23.
Simultaneous insertionsoccur most frequently for the labels VP (85 times), NNS(21 times), NNP (14 times).from P?
.
For the right child, we sample an initial treefrom P init0 .
For the left child, if decision to gener-ate an immediate foot was made, we generate a footnode, and stop.
Otherwise we recur into P right0 whichgenerates a right auxiliary tree that becomes the leftchild.We bring together these three sets of processesvia a set of insertion parameters ?leftc , ?rightc .
In anyderivation, for every initial tree node labelled c (ex-cept for frontier nodes) we determine whether ornot there are insertions at this node by sampling aBernoulli(?leftc ) distributed left insertion variable anda Bernoulli(?rightc ) distributed right insertion vari-able.
For left auxiliary trees, we treat the nodes thatare not along the spine of the auxiliary tree the sameway we treat initial tree nodes, however for nodesthat are along the spine (including root nodes, ex-cluding foot nodes) we consider only left insertionsby sampling the left insertion variable (symmetri-cally for right insertions).3 InferenceGiven this model, our inference task is to exploreoptimal derivations underlying the data.
Since TIGderivations are highly structured objects, a basicsampling strategy based on local node-level movessuch as Gibbs sampling (Geman and Geman, 1984)would not hold much promise.
Following previ-ous work, we design a blocked Metropolis-Hastingssampler that samples derivations per entire parsetrees all at once in a joint fashion (Cohn and Blun-som, 2010; Shindo et al, 2011).
This is achieved byproposing derivations from an approximating distri-bution and stochastically correcting via accept/rejectto achieve convergence into the correct posterior(Johnson et al, 2007).Since our base distributions factorize over levelsof tree, CFG is the most convenient choice for a112CFG rule CFG probabilityBase distribution: P init0NP?
NPinit ?initc /(ninitNP + ?initc )NPinit?
NPL NPinit NPR 1.0NPinit?
DT NN P?
(NP?
DT NN)?
(1?
?DT)?
(1?
?NN)NPinit?
DT NNinit P?
(NP?
DT NN)?
(1?
?DT)?
?NNNPinit?
DTinit NN P?
(NP?
DT NN)?
?DT ?
(1?
?NN)NPinit?
DTinit NNinit P?
(NP?
DT NN)?
?DT ?
?NNBase distribution: P right0NPR?
NPright ?rightNP ?
(?rightc /(nrightNP + ?rightc ))NPR?
 1?
?rightNPNPright?
NPright NPR 1.0NPright?
NP* SBARinit P?
(NP?
NP SBAR | NP?
NP )?(1?
?rightNP )?
(1?
?SBAR)NPright?
NP* SBAR P?
(NP?
NP SBAR | NP?
NP )?(1?
?rightNP )?
?SBARNPright?
NPright SBARinit P?
(NP?
NP SBAR | NP?
NP )?
?rightNP ?
(1?
?SBAR)NPright?
NPright SBAR P?
(NP?
NP SBAR | NP?
NP )?
?rightNP ?
?SBARFigure 4: Transformation CFG rules that represent infi-nite base distributions.
P init0 is taken from Cohn and Blun-som (2010).
Underscored labels (such as NPright as op-posed to NPright) are used to differentiate the pre-insertionnodes in Figure 2 from the post-insertion ones.
P left0 rulesare omitted for brevity and mirror the P right0 rules above.Model FMeasure # Initial Trees # Auxiliary Trees (# Left)TSG 77.51 6.2K -TIG0 78.46 6.0K 251 (137)TIG 78.62 5.6K 604 (334)Figure 5: EVALB results after training on Section 2 andtesting on Section 23.
Note that TIG finds a compact yetrich representation.
Elementary tree counts are based onones with count > 1.proposal distribution.
Fortunately, Schabes and Wa-ters (1995) provide an (exact) transformation from afully general TIG into a TSG that generates the samestring languages.
It is then straightforward to repre-sent this TSG as a CFG using the Goodman trans-form (Goodman, 2002; Cohn and Blunsom, 2010).Figure 4 lists the additional CFG productions wehave designed, as well as the rules used that triggerthem.4 Evaluation ResultsWe use the standard Penn treebank methodology oftraining on sections 2?21 and testing on section 23.All our data is head-binarized and words occurringonly once are mapped into unknown categories ofthe Berkeley parser.
As has become standard, wecarried out a small treebank experiment where wetrain on Section 2, and a large one where we trainon the full training set.
All hyperparameters are re-sampled under appropriate vague gamma and betapriors.
All reported numbers are averages over threeruns.
Parsing results are based on the maximumprobability parse which was obtained by samplingderivations under the transform CFG.We compare our system (referred to as TIG) toour implementation of the TSG system of (Cohnand Blunsom, 2010) (referred to as TSG) and theconstrained TIG variant of (Shindo et al, 2011) (re-ferred to as TIG0).
The upshot of our experiments isthat, while on the large training set al models havesimilar performance (85.6, 85.3, 85.4 for TSG, TIG0and TIG respectively), on the small dataset inser-tion helps nonparametric model to find more com-pact and generalizable representations for the data,which affects parsing performance (Figure 4).
Al-though TIG0 has performance close to TIG, note thatTIG achieves this performance using a more suc-cinct representation and extracting a rich set of aux-iliary trees.
As a result, TIG finds many chances toapply insertions to test sentences, whereas TIG0 de-pends mostly on TSG rules.
If we look at the mostlikely derivations for the test data, TIG0 assigns 663insertions (351 left insertions) in the parsing of en-tire Section 23, meanwhile TIG assigns 3924 (2100left insertions).
Some of these linguistically sophis-ticated auxiliary trees that apply to test data are listedin Figure 3.5 ConclusionWe described a nonparametric Bayesian inferencescheme for estimating TIG grammars and showedthe power of TIG formalism over TSG for returningrich, generalizable, yet compact representations ofdata.
The nonparametric inference scheme presentsa principled way of addressing the difficult modelselection problem with TIG which has been pro-hibitive in this area of research.
TIG still remainswithin context free and both our sampling and pars-ing techniques are highly scalable.AcknowledgementsThe first author was supported in part by a GooglePhD Fellowship in Natural Language Processing.113ReferencesXavier Carreras, Michael Collins, and Terry Koo.
2008.TAG, dynamic programming, and the perceptron forefficient, feature-rich parsing.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, CoNLL ?08, pages 9?16, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of the 38th Annual Meeting on Associa-tion for Computational Linguistics, ACL ?00, pages456?463, Morristown, NJ, USA.
Association for Com-putational Linguistics.Trevor Cohn and Phil Blunsom.
2010.
Blocked in-ference in Bayesian tree substitution grammars.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 225?230, Stroudsburg, PA,USA.
Association for Computational Linguistics.Trevor Cohn, Sharon Goldwater, and Phil Blunsom.2009.
Inducing compact but accurate tree-substitutiongrammars.
In NAACL ?09: Proceedings of HumanLanguage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Association forComputational Linguistics, pages 548?556, Morris-town, NJ, USA.
Association for Computational Lin-guistics.S.
Geman and D. Geman.
1984.
Stochastic Relaxation,Gibbs Distributions and the Bayesian Restoration ofImages.
pages 6:721?741.J.
Goodman.
2002.
Efficient parsing of DOP withPCFG-reductions.
Bod et al 2003.Rebecca Hwa.
1998.
An empirical evaluation of prob-abilistic lexicalized tree insertion grammars.
In Pro-ceedings of the 17th international conference on Com-putational linguistics - Volume 1, pages 557?563, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 139?146,Rochester, New York, April.
Association for Compu-tational Linguistics.Aravind K. Joshi, Leon S. Levy, and Masako Takahashi.1975.
Tree adjunct grammars.
Journal of Computerand System Sciences, 10(1):136?163.Matt Post and Daniel Gildea.
2009.
Bayesian learn-ing of a tree substitution grammar.
In Proceedingsof the ACL-IJCNLP 2009 Conference Short Papers,pages 45?48, Suntec, Singapore, August.
Associationfor Computational Linguistics.Remko Scha and Rens Bod.
2003.
Efficient parsing ofDOP with PCFG-reductions, October.Yves Schabes and Richard C. Waters.
1995.
Tree in-sertion grammar: a cubic-time parsable formalism thatlexicalizes context-free grammar without changing thetrees produced.
Comput.
Linguist., 21:479?513, De-cember.Hiroyuki Shindo, Akinori Fujino, and Masaaki Nagata.2011.
Insertion operator for Bayesian tree substitutiongrammars.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics: Hu-man Language Technologies: short papers - Volume2, HLT ?11, pages 206?211, Stroudsburg, PA, USA.Association for Computational Linguistics.114
