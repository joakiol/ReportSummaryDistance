Proceedings of the ACL-08: HLT Workshop on Mobile Language Processing, pages 21?24,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsMixture Pruning and Roughening for Scalable Acoustic ModelsDavid Huggins-DainesLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USAdhuggins@cs.cmu.eduAlexander I. RudnickyComputer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213, USAair@cs.cmu.eduAbstractIn an automatic speech recognition system us-ing a tied-mixture acoustic model, the maincost in CPU time and memory lies not inthe evaluation and storage of Gaussians them-selves but rather in evaluating the mixturelikelihoods for each state output distribution.Using a simple entropy-based technique forpruning the mixture weight distributions, wecan achieve a significant speedup in recogni-tion for a 5000-word vocabulary with a negli-gible increase in word error rate.
This allowsus to achieve real-time connected-word dicta-tion on an ARM-based mobile device.1 IntroductionAs transistors shrink and CPUs become faster andmore power-efficient, we find ourselves entering anew age of intelligent mobile devices.
We believethat not only will these devices provide access to richsources of on-line information and entertainment,but they themselves will find new applications aspersonal knowledge management agents.
Given theconstraints of the mobile form factor, natural speechinput is crucial to these applications.
However, de-spite the advances in processor technology, mobiledevices are still highly constrained by their memoryand storage subsystems.2 Semi-Continuous Acoustic ModelsRecent research into acoustic model compressionand optimization of acoustic scoring has focusedon ?Fully Continuous?
acoustic models, where eachHidden Markov Model state?s output probability dis-tribution is modeled by a mixture of multivariateGaussian densities.
This type of model allows largeamounts of training data to be efficiently exploited toproduce detailed models.
However, due to the largenumber of parameters in these models, approximatecomputation techniques (Woszczyna, 1998) are re-quired in order to achieve real-time recognition evenon workstation-class hardware.Another historically popular type of acousticmodel is the so-called ?Semi-Continuous?
or tied-mixture model, where a single codebook of Gaus-sians is shared by all HMM states (Huang, 1989).The parameters of this codebook are updated usingthe usual Baum-Welch equations during training, us-ing sufficient statistics from all states.
The mix-ture weight distributions therefore become the mainsource of information used to distinguish betweendifferent speech sounds.There are several benefits to semi-continuousmodels for efficient speech recognition.
The mostobvious is the greatly reduced number of Gaussiandensities which need to be computed.
With fullycontinuous models, we must compute one codebookof 16 or more Gaussians for each HMM state, ofwhich there may be several thousand active for anygiven frame of speech input.
For semi-continuousmodels, there is a single codebook with a small num-ber of Gaussians, typically 256.
In addition, sinceonly a few Gaussians will have non-negligible den-sities for each frame of speech, and this set of Gaus-sians tends to change slowly, partial computation ofeach density is possible.Another useful property of semi-continuous mod-els is that the mixture weights for each state havethe form of a multinomial distribution, and are thusamenable to various smoothing and adaptation tech-niques.
In particular, Bayesian and quasi-Bayes21adaptation (Huo and Chan, 1995) are effective andcomputationally efficient.3 Experimental SetupAll experiments in this paper were performed usingPocketSphinx (Huggins-Daines et al, 2006).
Thebaseline acoustic model was trained from the com-bined WSJ0 and WSJ1 ?long?
training sets (Paul andBaker, 1992), for a total of 192 hours of speech.This speech was converted to MFCC features us-ing a bank of 20 mel-scale filters spaced from 0to 4000Hz, allowing the model to work with au-dio sampled at 8kHz, as is typical on mobile de-vices.
We used 5-state Hidden Markov Modelsfor all phones.
Output distributions were modeledby a codebook of 256 Gaussians, shared between5000 tied states and 220 context-independent states.Only the first pass of recognition (static lexicon treesearch) was performed.Our test platform is the Nokia N800, a hand-held Internet Tablet.
It uses a Texas InstrumentsOMAPTM 2420 processor, which combines anARM11 RISC core and a C55x DSP core on a singlechip.
The RISC core is clocked at 400MHz while theDSP is clocked at 220MHz.
In these experiments,we used the ARM core for all processing, althoughwe have also ported the MFCC extraction code to theDSP.
The decoder binaries, models and audio fileswere stored on a high-speed SD flash card format-ted with the ext3 journaling filesystem.
Using thestandard bcb05cnp bigram language model, weobtained a baseline word error rate of 9.46% on thesi_et_05 test set.
The baseline performance ofthis platform on the test set is 1.40 times real-time,that is, for every second of speech, 1.40 seconds ofCPU time are required for recognition.We used the oprofile utility1 on the NokiaN800 to collect statistical profiling information fora subset of the test corpus.
The results are shown inTable 1.
We can see that three operations occupy thevast majority of CPU time used in decoding: man-aging the list of active HMM states, computing thecodebook of Gaussians, and computing mixture den-sities.The size of the files in the acoustic model is shownin Table 2.
The amount of CPU time required to1http://oprofile.sourceforge.net/Function %CPUHMM evaluation 22.41hmm vit eval 5st lr 13.36hmm vit eval 5st lr mpx 3.71Mixture Evaluation 21.66get scores4 8b 14.94fast logmath add 6.72Lexicon Tree Search 19.89last phone transition 5.25prune nonroot chan 4.15Active List Management 15.57hmm sen active 13.75compute sen active 1.19Language Model Evaluation 7.80find bg 2.55ngram ng score 2.13Gaussian Evaluation 5.87eval cb 5.59eval topn 0.28Acoustic Feature Extraction 3.60fe fft real 1.59fixlog2 0.77Table 1: CPU profiling, OMAP platformFile Size (bytes)sendump (mixture weights) 5345920mdef (triphone mappings) 1693280means (Gaussians) 52304variances (Gaussians) 52304transition_matrices 5344Table 2: File sizes, WSJ1 acoustic modelcalculate densities is related to the size of the mix-ture weight distribution by the fact that the N800has a single-level 32Kb data cache, while a typicaldesktop processor has two levels of cache totallingat least 1Mb.
We used cachegrind2 to simulatethe memory hierarchy of an OMAP versus an AMDK8 desktop processor with 64Kb of L1 cache and512Kb of L2 cache, with results shown in Table 3.While other work on efficient recognition has fo-cused on quantization of the Gaussian parameters(Leppa?nen and Kiss, 2005), in a semi-continuousmodel, the number of these parameters is small2http://valgrind.org/22Function ARM K8hmm vit eval 5st lr 4.71 3.95hmm sen active 3.55 3.76get scores4 8b 2.87 1.92prune root chan 2.07 2.29prune nonroot chan 1.99 1.73eval cb 1.73 1.77hmm vit eval 5st lr mpx 1.30 0.80Table 3: Data cache misses (units of 107)enough that little cost is incurred by storing and cal-culating them as 32-bit fixed-point numbers.
There-fore, we focus here on ways to reduce the amount ofstorage and computation used by the mixture weightdistributions.4 Mixture RougheningOur method for speeding up mixture computation isbased on the observation that mixture weight distri-butions are typically fairly ?spiky?, with most of theprobability mass concentrated in a small number ofmixture weights.
One can quantify this by calculat-ing the perplexity of the mixture distributions:pplx(wi) = expN?k=0wik log1wikA histogram of perplexities is shown in Figure1.
The perplexity can be interpreted as the averagenumber of Gaussians which were used to generatean observation drawn from a particular distribution.Therefore, on average, the vast majority of the 256Gaussians contribute minimally to the likelihood ofthe data given a particular mixture model.When evaluating mixture densities with prunedmodels, one can either treat these mixture weightsas having a small but non-negligible value, or setthem to zero3.
Note that the mixture weights arerenormalized in both cases, and thus the former ismore or less equivalent to add-one smoothing.
Thelatter can be thought of as exactly the opposite ofsmoothing - ?roughening?
the distribution.
To in-vestigate this, we set al but the top 16 values in eachmixture weight distribution to zero and ran a num-ber of trials on a K8-based workstation, varing the3Meaning a very small number, since they are stored in logdomain.0 50 100 150 200Perplexity(w)02004006008001000# of mixture weights mode = 10Figure 1: Perplexity distribution of mixture weights3 4 5 6 7 8-log10(mixw_floor)0.000.050.100.150.200.250.30Performance (xRT)51015202530Error Rate (%WER)xRT (16 mixtures)xRT (baseline)WER (16 mixtures)WER (baseline)Figure 2: Smoothing vs. Roughening, 16 mixturesmixture weight floor to produce either a smoothingor roughening effect.
We discovered something ini-tially surprising: ?roughening?
the mixture weightsspeeds up decoding significantly, while smoothingthem slows it down.
A plot of speed and error rateversus mixture weight floor is shown in Figure 2.However, there is a simple explanation for this.At each frame, only the top N Gaussian densitiesare actually used to calculate the likelihood of thedata:p(x|?i) =?k?topNwikN(x; ~?ik, ~?2ik)When we remove mixture weights, we increasethe probability that these top N densities will bematched with pruned-out weights.
If we smooth theweights, we may raise some of these weights abovetheir maximum-likelihood estimate, thus increasing233 4 5 6 7 8-log10(mixw_floor)0.050.100.150.200.250.30Performance (xRT)0.10 xRT, 9.68 %WER9.09.510.010.511.011.5Error Rate (%WER)xRT (64 mixtures)xRT (96 mixtures)xRT (baseline)WER (64 mixtures)WER (96 mixtures)WER (baseline)Figure 3: Speed-accuracy tradeoff with pruned mixturesthe likelihood for models whose top mixture weightsdo not overlap with the top N densities.
This mayprevent HMM states whose output distributions aremodeled by said models from being pruned by beamsearch, therefore slowing down the decoder.
By?roughening?
the weights, we decrease the likeli-hood of the data for these models, and hence makethem more likely to be pruned, speeding up the de-coder and increasing the error rate.
This is a kindof ?soft?
GMM selection, where instead of exclud-ing some models, we simply make some more likelyand others less likely.As we increase the number of retained mixtureweights, we can achieve an optimal tradeoff betweenspeed and accuracy, as shown in Figure 3.
Addition-ally, the perplexity calculation suggests a principledway to vary the number of retained mixture weightsfor each model.
We propose setting a target numberof mixture weights, then calculating a scaling factorbased on the ratio of this target to the average per-plexity of all models:topKi =target1N?Ni=0 pplx(wi)pplx(wi)One problem is that many models have very lowperplexity, such that we end up retaining only a fewmixture weights.
When the mixture weights are?roughened?, this guarantees that these models willscore poorly, regardless of the data.
We compensatefor this by keeping a minimum number of mixtureweights regardless of the perplexity.
Using a tar-get of 96 mixtures, a minimum of 16, and a mixtureweight floor of 10?8, we achieve 9.90% word errorrate in 0.09 times real-time, a 21% speedup with a2.7% relative increase in error (baseline error rate is9.64% on the desktop).Using the same entropy-pruned mixture weightson the N800, we achieve an error rate of 9.79%, run-ning in 1.19 times real-time, a 15% speedup with a3.4% relative increase in error.
After applying ab-solute pruning thresholds of 800 HMMs per frameand 5 words per frame, we obtained a 10.01% worderror rate in 1.01 times real-time.5 ConclusionWe have shown that a simple pruning technique al-lows acoustic models trained for large-vocabularycontinuous speech recognition to be ?scaled down?to run in real-time on a mobile device without majorincreases in error.
In related work, we are exper-imenting with bottom-up clustering techniques onthe mixture weights to produce truly scalable acous-tic models, and subvector clustering to derive semi-continuous models automatically from well-trainedfully-continuous models.AcknowledgmentsWe wish to thank Nokia for donating the N800 tabletused in these experiments.ReferencesX.
D. Huang.
1989.
Semi-continuous Hidden MarkovModels for Speech Recognition.
Ph.D. thesis, Univer-sity of Edinburgh.D.
Huggins-Daines, M. Kumar, A. Chan, A. Black,M.
Ravishankar, and A. Rudnicky.
2006.
Pocket-sphinx: A free, real-time continuous speech recogni-tion system for hand-held devices.
In Proceedings ofICASSP 2006, Toulouse, France.Q.
Huo and C. Chan.
1995.
On-line Bayes adaptation ofSCHMM parameters for speech recognition.
In Pro-ceedings of ICASSP 1995, Detroit, USA.J.
Leppa?nen and I.
Kiss.
2005.
Comparison of low foot-print acoustic modeling techniques for embedded ASRsystems.
In Proceedings of Interspeech 2005, Lisbon,Portugal.D.
Paul and J. Baker.
1992.
The design for the WallStreet Journal based CSR corpus.
In Proceedings ofthe ACL workshop on Speech and Natural Language.M.
Woszczyna.
1998.
Fast Speaker Independent LargeVocabulary Continuous Speech Recognition.
Ph.D.thesis, University of Karlsruhe.24
