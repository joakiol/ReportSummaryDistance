A CONNECTIONIST PARSERFOR STRUCTURE UNIFICATION GRAMMARJ ames  B .
Henderson*Depar tment  of Computer  and In format ion ScienceUnivers i ty  of Pennsy lvan ia200 South 33rdPhi ladelphia,  PA 19104, USA(henders@l inc.c is .upenn.edu)ABSTRACTThis paper presents a connectionist yntacticparser which uses Structure Unification Grammaras its grammatical framework.
The parser is im-plemented in a connectionist architecture whichstores and dynamically manipulates symbolic rep-resentations, but which can't represent arbitrarydisjunction and has bounded memory.
Theseproblems can be overcome with Structure Unifica-tion Grammar's extensive use of partial descrip-tions.INTRODUCTIONThe similarity between connectionist models ofcomputation and neuron computation suggeststhat a study of syntactic parsing in a connection-ist computational rchitecture could lead to sig-nificant insights into ways natural language canbe parsed efficiently.
Unfortunately, previous in-vestigations into connectionist parsing (Cottrell,1989, Fanty, 1985, Selman and Hirst, 1987) havenot been very successful.
They cannot parse arbi-trarily long sentences and have inadequate gram-mar representations.
However, the difficulties withconnectionist parsing can be overcome by adopt-ing a different connectionist model of computa-tion, namely that proposed by Shastri and Ajjana-gadde (1990).
This connectionist computationalarchitecture differs from others in that it directlymanifests the symbolic interpretation of the infor-mation it stores and manipulates.
It also sharesthe massive parallelism, evidential reasoning abil-ity, and neurological plausibility of other connec-tionist architectures.
Since virtually all charac-terizations of natural language syntax have reliedheavily on symbolic representations, this architec-ture is ideally suited for the investigation of syn-tactic parsing.
*This research was supported by ARO grantDAAL 03-89-C-0031, DARPA grant N00014-90-J-1863, NSF grant IRI 90-16592, and Ben Franklin grant91S.3078C-1.The computational rchitecture proposed byShastri and Ajjanagadde (1990) provides a rathergeneral purpose computing framework, but it doeshave significant limitations.
A computing mod-ule can represent entities, store predications overthose entities, and use pattern-action rules to ma-nipulate this stored information.
This form of rep-resentation is very expressive, and pattern-actionrules are a general purpose way to do compu-tation.
However, this architecture has two lim-itations which pose difficult problems for pars-ing natural language.
First, only a conjunctionof predications can be stored.
The architecturecannot represent arbitrary disjunction.
This lim-itation implies that the parser's representation fsyntactic structure must be able to leave unspec-ified the information which the input has not yetdetermined, rather than having a disjunction ofmore completely specified possibilities for com-pleting the sentence.
Second, the memory ca-pacity of any module is bounded.
The numberof entities which can be stored is bounded by asmall constant, and the number of predicationsper predicate is also bounded.
These bounds poseproblems for parsing because the syntactic struc-tures which need to be recovered can be arbitrarilylarge.
This problem can be solved by allowing theparser to output the syntactic structure incremen-tally, thus allowing the parser to forget the infor-mation which it has already output and which itno longer needs to complete the parse.
This tech-nique requires that the representation f syntacticstructure be able to leave unspecified the informa-tion which has already been determined but whichis no longer needed for the completion of the parse.Thus the limitations of the architecture mean thatthe parser's representation of syntactic structuremust be able to leave unspecified both the infor-mation which the input has not yet determinedand the information which is no longer needed.In order to comply with these requirements,the parser uses Structure Unification Grammar(Henderson, 1990) as its grammatical framework.SUG is a formalization of accumulating informa-144tion about the phrase structure of a sentence un-til a complete description of the sentence's phrasestructure tree is constructed.
Its extensive useof partial descriptions makes it ideally suited fordealing with the limitations of the architecture.This paper focuses on the parser's represen-tation of phrase structure information and on theway the parser accumulates this information dur-ing a parse.
Brief descriptions of the grammarformalism and the implementation in the connec-tionist architecture are also given.
Except whereotherwise noted, a simulation of the implementa-tion has been written, and its grammar supportsa small set of examples.
A more extensive gram-mar is under development.
SUG is clearly an ade-quate grammatical framework, due to its abilityto straightforwardly simulate Feature StructureBased Tree Adjoining Grammar (Vijay-Shanker,1987), as well as other formalisms (Henderson,1990).
Initial investigations suggest that the con-straints imposed by the parser do not interferewith this linguistic adequacy, and more extensiveempirical verification of this claim is in progress.The remainder of this paper will first give anoverview of Structure Unification Grammar, thenpresent he parser design, and finally a sketch ofits implementation.STRUCTURE UNIF ICAT IONGRAMMARStructure Unification Grammar is a formaliza-tion of accumulating information about he phrasestructure of a sentence until this structure is com-pletely described.
This information is specified inpartial descriptions of phrase structure trees.
AnSUG grammar issimply a set of these descriptions.The descriptions cannot use disjunction or nega-tion, but their partiality makes them both flexi-ble enough and powerful enough to state what isknown and only what is known where it is known.There is also a simple abstraction operation forSUG descriptions which allows unneeded informa-tion to be forgotten, as will be discussed in thesection on the parser design.
In an SUG deriva-tion, descriptions are combined by equating nodes.This way of combining descriptions i extremelyflexible, thus allowing the parser to take full ad-vantage of the flexibility of SUG descriptions, andalso providing for efficient parsing strategies.
Thefinal description produced by a derivation mustcompletely describe some phrase structure tree.This tree is the result of the derivation.
The de-sign of SUG incorporates ideas from Tree Adjoin-ing Grammar, Description Theory (Marcus et al,1983), Combinatory Categorial Grammar, Lexi-cal Functional Grammar, and Head-driven PhraseStructure Grammar.An SUG grammar is a set of partial descrip-tions of phrase structure trees.
Each SUG gram-mar entry simply specifies an allowable groupingof information, thus expressing the information i -terdependencies.
The language which SUG pro-vides for specifying these descriptions allows par-tiality both in the information about individualnodes, and (crucially) in the information aboutthe structural relations between nodes.
As inmany formalisms, nodes are described with fea-ture structures.
The use of feature structures al-lows unknown characteristics of a node to be leftunspecified.
Nodes are divided into nonterminals,which are arbitrary feature structures, and termi-nals, which are atomic instances of strings.
Unlikemost formalisms, SUG allows the specification ofthe structural relations to be equally partial.
Forexample, if a description specifies children for anode, this does not preclude that node from ac-quiring other children, such as modifiers.
Thispartiality also allows grammar entries to under-specify ordering constraints between odes, thusallowing for variations in word order.
This partial-ity in structural information is imperative to allowincremental parsing without disjunction (Marcuset al, 1983).
In addition to the immediate domi-nance relation for specifying parent-child relation-ships and linear precedence for specifying orderingconstraints, SUG allows chains of immediate dom-inance relationships to be partially specified usingthe dominance relation.
A dominance constraintbetween two nodes specifies that there must be achain of zero or more immediate dominance con-straints between the two nodes, but it does notsay anything about the chain.
This relation isnecessary to express long distance dependencies ina single grammar entry.
Some examples of SUGphrase structure descriptions are given in figure 1,and will be discussed below.A complete description of a phrase structuretree is constructed from the partial descriptions inan SUG grammar by conjoining a set of grammarentries and specifying how these descriptions sharenodes.
More formally, an SUG derivation startswith descriptions from the grammar, and in eachstep conjoins a set of one or more descriptions andadds zero or more statements of equality betweennonterminal nodes.
The description which resultsfrom a derivation step must be satisfiable, so thefeature structures of any two equated nodes mustunify and the resulting structural constraints mustbe consistent with some phrase structure tree.
Thefinal description produced by a derivation mustbe a complete description of some phrase struc-ture tree.
This tree is the result of the derivation.The sentences generated by a derivation are allthose terminal strings which are consistent withthe ordering constraints on the resulting tree.
Fig-145h AP-'~\[\] t N~tet plzzatkey: ~ x immediately "~h y is the headdominates y x j .
feature valueY Y of xX', x dominates y x t x is a terminal\[\] empty featurex--~y x precedes y structureFigure 1: Example grammar entries.
They canbe combined to form a structure for the sentence"Who ate white pizza?
".ure 2 shows an example derivation with one stepin which all grammar entries are combined andall equations are done.
This definition of deriva-tions provides a very flexible framework for investi-gating various parsing strategies.
Any ordering ofcombining rammar entries and doing equations isa valid derivation.
The only constraints on deriva-tions come from the meanings of the descriptionprimitives and from the need to have a unique re-sulting tree.
This flexibility is crucial to allow theparser to compensate for the connectionist archi-tecture's limitations and to parse efficiently.Because the resulting description of an SUGderivation must be both a consistent descriptionand a complete description of some tree, an SUGgrammar entry can state both what is true aboutthe phrase structure tree and what needs to betrue.
For a description to be complete it mustspecify a single immediate dominance tree and allterminals mentioned in the description must havesome (possibly empty) string specified for them.Otherwise there would be no way to determine theexact tree structure or the word for each terminalin the resulting tree.
A grammar entry can expressgrammatical requirements by not satisfying thesecompletion requirements locally.
For example, infigure 1 the structure for "ate" has a subject nodewith category NP and with a terminal as the val-ues of its head feature.
Because this terminal doesnot have its word specified, this NP must equatewith another NP node which does have a word forthe value of its head feature.
The unification of thetwo NP's feature structures will cause the equationof the two head terminals.
In this way the struc-ture for "ate" expresses the fact that it obligatorilysubcategorizes for a subject NP.
The structure for"ate" also expresses its subcategorization for anobject NP, but this object is not obligatory sinceit does not have an underspecified terminal head.Like the subject of "ate", the root of the structurefor "white" in figure 1 has an underspecified ter-minal head.
This expresses the fact that "white"obligatorily modifies N's.
The need to constructa single immediate dominance tree is used in thestructure for "who" to express the need for thesubcategorized S to have an NP gap.
Because thedominated NP node does not have an immediateparent, it must equate with some node which hasan immediate parent.
The site of this equation isthe gap associated with "who".THE PARSERThe parser presented in this paper accumulatesphrase structure information in the same way asdoes Structure Unification Grammar.
It calcu-lates SUG derivation steps using a small set ofoperations, and incrementally outputs the deriva-tion as it parses.
The parser is implemented inthe connectionist architecture proposed by Shastriand Ajjanagadde (1990) as a special purpose mod-ule for syntactic onstituent structure parsing.
AnSUG description is stored in the module's mem-ory by representing nonterminal nodes as entitiesand all other needed information as predicationsover these nodes.
If the parser starts to run outof memory space, then it can remove some nodesfrom the memory, thus forgetting all informationabout those nodes.
The parser operations are im-plemented in pattern-action rules.
As each wordis input to the parser, one of these rules combinesone of the word's grammar entries with the currentdescription.
When the parse is finished the parserchecks to make sure it has produced a completedescription of some phrase structure tree.THE GRAMMARSThe grammars which are supported by the parserare a subset of those for Structure UnificationGrammar.
These grammars are for the most partlexicalized.
Each lexicalized grammar entry is arooted tree fragment with exactly one phoneti-cally realized terminal, which is the word of theentry.
Such grammar entries specify what infor-mation is known about the phrase structure ofthe sentence given the presence of the word, andcan be used (Henderson, 1990) to simulate Lexi-calized Tree Adjoining Grammar (Schabes, 1990).Nonlexical grammar entries are rooted tree frag-ments with no words.
They can be used to ex-press constructions like reduced relative clauses,for which no lexical information is necessary.
The146I l l ', - tIIIi ;1who t did t Barbie seet ail h/:; p!ct~, \!fS4h~\[\]~yest!r!ay,y,A,., 12I14  .
; i / i"7 : .4 h , , ,  ..Nl;!
!l!\l..,") who  did lhrbie  see  p cture o J yesti1~aytxis u., lwith y IFigure 2: A derivation for the sentence 'TVho did Barbie see a picture of yesterday"..current mechanism the parser uses to find possiblelong distance dependencies requires ome informa-tion about possible extractions to be specified ingrammar entries, despite the fact that this infor-mation currently only has meaning at the level ofthe parser.The primary limitations on the parser's abil-ity to parse the sentences derivable with a gram-max are due to the architecture's lack of disjunc-tion and limited memory capacity.
Technically,constraints on long distance dependencies are en-forced by the parser's limited ability to calcu-late dominance relationships, but the definitionof an SUG derivation could be changed to man-ifest these constraints.
This new definition wouldbe necessary to maintain the traditional split be-tween competence and performance phenomena.The remaining constraints imposed at the level ofthe parser are traditionally treated as performanceconstraints.
For example, the parser's boundedmemory prevents it from being able to parse arbi-trarily center embedded sentences or from allow-ing arbitrarily many phrases on the right frontierof a sentence to be modified.
These are well es-tablished performance constraints on natural an-guage (Chomsky, 1959, and many others).
Thelack of a disjunction operator limits the parser'sability to represent local ambiguities.
This re-sults in some locally ambiguous grammatical sen-tences being unparsable.
The existence of suchsentences for the human parser, called garden pathsentences, is also well documented (Bever, 1970,among others).
The representations currentlyused for handling local ambiguities appear to beadequate for building the constituent s ructure ofany non-garden path sentences.
The full verifi-cation of this claim awaits a study of how effec-tively probabilistic onstraints can be used to re-solve ambiguities.
The work presented in this pa-per does not directly address the question of howambiguities between possible predicate-argumentstructures are resolved.
Also, the current parseris not intended to be a model of performance phe-nomena, although since the parser is intended tobe computationally adequate, all limitations im-posed by the parser must fall within the set ofperformance constraints on natural anguage.THE PARSER DES IGNThe parser follows SUG derivations, incrementallycombining a grammar entry for each word with thedescription built from the previous words of thesentence.
Like in SUG the intermediate descrip-tions can specify multiple rooted tree fragments,but the parser epresents such a set as a list in or-der to represent the ordering between terminals inthe fragments.
The parser begins with a descrip-tion containing only an S node which needs a head.This description expresses the parser's expectationfor a sentence.
As each word is read, a gram-mar entry for that word is chosen and combined147current grammardescription: entry:~ ~ attaching=~.
at xcurrent grammardescription: entry:,A r\ leftwardattachingatycurrent grammardescription: entry:QcurrentZ:ordominanceinstantiatingat zcurrentdescription:current grammardescription: entry://~ J/~ equa~onle~s~ ,  ~ combininginternalequationkey:.~ x is they host of yxisxi YO equatablewith yFigure 3: The operations of the parser.with the current description using one of four com-bination operations.
Nonlexical grammar entriescan be combined with the current description atany time using the same operations.
There is alsoan internal operation which equates two nodes al-ready in the current description without using agrammar entry.
The parser outputs each opera-tion it does as it does them, thus providing incre-mental output to other language modules.
Aftereach operation the parser's representation f thecurrent description is updated so that it fully re-flects the new information added by the operation.The five operations used by the parser axeshown in figure 3.
The first combination opera-tion, called attaching, adds the grammar entry tothe current description and equates the root of thegrammar entry with some node already in the cur-rent description.
The second, called dominance in-stantiating, equates a node without a parent in thecurrent description with a node in the grammarentry, and equates the host of the unparented nodewith the root of the grammar entry.
The host func-tion is used in the parser's mechanism for enforc-ing dominance constraints, and represents he factthat the unparented node is potentially dominatedby its current host.
In the case of long distancedependencies, a node's host is changed to nodesfurther and further down in the tree in a man-ner similar to slash passing in Generalized PhraseStructure Grammar, but the resulting domain ofpossible xtractions i  more similar to that of TreeAdjoining Grammar.
The equationless combiningoperation simply adds a grammar entry to the endof the tree fragment list.
This operation is some-times necessary in order to delay attachment de-cisions long enough to make the right choice.
Theleftward attaching operation equates the root ofthe tree fragment on the end of the list with somenode in the grammar entry, as long as this root isnot the initializing matrix S 1.
The one parser op-eration which does not involve a grammar entry iscalled internal equating.
When the parser's rep-resentation of the current description is updatedso that it fully reflects newly added information,some potential equations are calculated for nodeswhich do not yet have immediate parents.
Theinternal equating operation executes one of thesepotential equations.
There are two cases when thiscan occur, equating fillers with gaps and equatinga root of a tree fragment with a node in the nextearlier tree fragment on the list.
The later is howtree fragments are removed from the list.The bound on the number of entities whichcan be stored in the parser's memory requires thatthe parser be able to forget entities.
The imple-mentation of the parser only represents nontermi-nal nodes as entities.
The number of nontermi-nals in the memory is kept low simply by forget-ting nodes when the memory starts getting full,thereby also forgetting the predications over thenodes.
This forgetting operation abstracts awayfrom the existence of the forgotten ode in thephrase structure.
Once a node is forgotten it canno longer be equated with, so nodes which mustbe equated with in order for the total descrip-tion to be complete can not be forgotten.
Forget-ting nodes may eliminate some otherwise possibleparses, but it will never allow parses which violate1As of this writing the implementation f the treefragment list and these later two combination opera-tions has been designed, but not coded in the simula-tion of the parser's implementation.148parser state :S hattachingS hBarbie t dominance instantiatingh~rLie t~~ r~ forgettinghSequationless combiningtit AP.,,hfashiolllab~\]ytBa sest fashionablytinternalequatinggrammar entries :Barbiet Barbieth - :VPdres~est\fashionablyth~esses tFigure 4: An example parse of "Barbie dresses fashionably".the forgotten constraints.
Any forgetting strategycan be used as long as the only eliminated parsesare for readings which people do not get.
Severalsuch strategies have been proposed in the litera-ture.As a simple example parse consider the parseof "Barbie dresses fashionably" sketched in fig-ure 4.
The parser begins with an S which needsa head, and receives the word "Barbie".
The un-derlined grammar entry is chosen because it canattach to the S in the current description usingthe attaching operation.
The next word input is"dresses", and its verb grammar entry is chosenand combined with the current description usingthe dominance instantiating operation.
In the re-sulting description the subject NP is no longer onthe right frontier, so it will not be involved in anyfuture equations and thus can be forgotten.
Re-member that the output of the parser is incremen-tal, so forgetting the subject will not interfere withsemantic interpretation.
The next word input is"fashionably", which is a VP modifier.
The parsercould simply attach "fashionably", but for the pur-poses of exposition assume the parser is not surewhere to attach this modifier, so it simply addsthis grammar entry to the end of the tree frag-ment list using equationless combining.
The up-dating rules of the parser then calculate that theVP root of this tree fragment could equate withthe VP for "dresses", and it records this fact.
Theinternal equating operation can then apply to dothis equation, thereby choosing this attachmentsite for "fashionably".
This technique can be usedto delay resolving any attachment ambiguity.
Atthis point the end of the sentence has been reachedand the current description is complete, so a suc-cessful parse is signaled.Another example which illustrates the parser'sability to use underspecification to delay disam-biguation decisions i  given in figure 5.
The featuredecomposition ~:A,:EV is used for the major cate-gories (N, V, A, and P) in order to allow the objectof "know" to be underspecified asto whether it isof category i (\[-A,-V\]) or V (\[-A,TV\]).
When149parser state : grammar entry:Barbiet knows t ~ at mant  lefttFigure 5: Delaying the resolution of the ambigu-ity between "Barbie knows a man."
and "Barbieknows a man left.
""a man" is input the parser is not sure if it is theobject of "know" or the subject of this object, sothe structure for "a man" is simply added to theparser state using equationless combining.
Thisunderspecification can be maintained for as longas necessary, provided there are resources availableto maintain it.
If no verb is subsequently inputthen the NP can be equated with the -A  nodeusing internal equation, thus making "a man" theobject of "know".
If, as shown, a verb is inputthen leftward attaching can be used to attach "aman" as the subject of the verb, and then theverb's S node can be equated with the -A  node tomake it the object of "know".
Since this parser isonly concerned with constituent structure and notwith predicate-argument structure, the fact thatthe -A  node plays two different semantic roles inthe two cases is not a problem.THE CONNECTIONISTIMPLEMENTATIONThe above parser is implemented using the con-nectionist computational rchitecture proposed byShastri and Ajjanagadde (1990).
This architecturesolves the variable binding problem 2 by using unitswhich pulse periodically, and representing differ-ent entities in different phases.
Units which arestoring predications about the same entity pulsesynchronously, and units which are storing pred-ications about different entities pulse in differentphases.
The number of distinct entities which canbe stored in a module's memory at one time isdetermined by the width of a pulse spike and thetime between periodic firings (the period).
Neuro-logically plausible stimates of these values put themaximum number of entities in the general vicin-ity of 7-4-2.
The architecture does computationwith sets of units which implement pattern-actionrules.
When such a set of units finds its patternin the predications in the memory, it modifies thememory contents in accordance with its action and2The variable binding problem is keeping track ofwhat predications are for what variables when morethan one variable is being used.Figure 6: The architecture of the parser.the entity(s) which matched.This connectionist computational rchitectureis used to implement a special purpose modulefor syntactic onstituent structure parsing.
A di~agram of the parser's architecture is shown in fig-ure 6.
This parsing module uses its memory tostore information about the phrase structure de-scription being built.
Nonterminals are the enti-ties in the memory, and predications over nonter-minals are used to represent all the informationthe parser needs about the current description.Pattern-action rules are used to make changes tothis information.
Most of these rules implementthe grammar.
For each grammar entry there isa rule for each way of using that grammar en-try in a combination operation.
The patterns forthese rules look for nodes in the current descrip-tion where their grammar entry can be combinedin their way.
The actions for these rules add in-formation to the memory so as to represent hechanges to the current description which resultfrom their combination.
If the grammar entry islexical then its rules are only activated when itsword is the next word in the sentence.
A generalpurpose connectionist arbitrator is used to choosebetween multiple rule pattern matches, as withother disambiguation decisions 3.
This arbitrator3Because a rule's pattern matches must be commu-nicated to the rule's action through an arbitrator, theexistence and quality of a match must be specified ina single node's phase.
For rules which involve morethan one node, information about one of the nodesmust be represented in the phase of the other node forthe purposes of testing patterns.
This is the purpose150weighs the preferences for the possible choices andmakes a decision.
This mechanism for doing dis-ambiguation allows higher level components of thelanguage system to influence disambiguation byadding to the preferences of the arbitrator 4.
Italso allows probabilistic onstraints uch as lexi-cal preferences and structural biases to be used,although these aspects of the parser design havenot yet been adequately investigated.
Because theparser's grammar is implemented in rules whichall compute in parallel, the speed of the parseris independent of the size of the grammar.
Theinternal equating operation is implemented witha rule that looks for pairs of nodes which havebeen specified as possible equations, and equatesthem, provided that that equation is chosen bythe arbitrator.
Equation is done by translatingall predications for one node to the phase of theother node, then forgetting the first node.
The for-getting operation is implemented with links whichsuppress all predications stored for the node to beforgotten.
The only other rules update the parserstate to fully reflects any new information addedby a grammar ule.
These rules act whenever theyapply, and include the calculation of equatabilityand host relationships.CONCLUSIONThis paper has given an overview of a connection-ist syntactic constituent structure parser whichuses Structure Unification Grammar as its gram-matical framework.
The connectionist computa-tional architecture which is used stores and dy-namically manipulates ymbolic representations,thus making it ideally suited for syntactic parsing.However, the architecture's inability to representarbitrary disjunction and its bounded memory ca-pacity pose problems for parsing.
These difficul-ties can be overcome by using Structure Unifica-tion Grammar as the grammatical framework, dueto SUG's extensive use of partial descriptions.This investigation has indeed led to insightsinto efficient natural language parsing.
Thisparser's speed is independent of the size of itsgrammar.
It only uses a bounded amount of mem-ory.
Its output is incremental, monotonic, anddoes not include disjunction.
Its disambiguationof the signal generation box in figure 6.
For all suchrules, the identity of one of the nodes can be deter-mined uniquely given the other node and the parserstate.
For example in the dominance instantiating op-eration, given the unparented node, the host of thatnode can be found because host is a function.
Thisconstraint on parser operations seems to have signifi-cant linguistic import, but more investigation of thispossibility is necessary.4In the current simulation of the parser implemen-tation the arbitrators are controlled by the user.mechanism provides a parallel interface for the in-fluence of higher level language modules.
Assum-ing neurologically plausible timing characteristicsfor the computing units of the connectionist archi-tecture, the parser's peed is roughly compatiblewith the speed of human speech.
In the future theability of this architecture to do evidential reason-ing should allow the use of statistical informationin the parser, thus making use of both grammat-ical and statistical approaches to language in asingle framework.REFERENCESBever, Thomas G (1970).
The cognitive basis forlinguistic structures.
In J. R. Hayes, editor,Cognition and the Development of Language.John Wiley, New York, NY.Chomsky, Noam (1959).
On certain formal prop-erties of grammars.
Information and Control,2: 137-167.Cottrell, Garrison Weeks (1989).
A ConnectionistApproach to Word Sense Disambiguation.
Mor-gan Kaufmann Publishers, Los Altos, CA.Fanty, Mark (1985).
Context-free parsing in con-nectionist networks.
Technical Report TR174,University of Rochester, Rochester, NY.Henderson, James (1990).
Structure unifica-tion grammar: A unifying framework for in-vestigating natural language.
Technical Re-port MS-CIS-90-94, University of Pennsylvania,Philadelphia, PA.Marcus, Mitchell; Hindle, Donald; and Fleck,Margaret (1983).
D-theory: Talking about talk-ing about trees.
In Proceedings of the 21st An-nual Meeting of the ACL, Cambridge, MA.Schabes, Yves (1990).
Mathematical nd Compu-tational Aspects of Lexicalized Grammars.
PhDthesis, University of Pennsylvania, Philadelphia,PA.Selman, Bart and Hirst, Graeme (1987).
Pars-ing as an energy minimization problem.
InLawrence Davis, editor, Genetic Algorithms andSimulated Annealing, chapter 11, pages 141-154.
Morgan Kaufmann Publishers, Los Altos,CA.Shastri, Lokendra and Ajjanagadde, Venkat(1990).
From simple associations to system-atic reasoning: A connectionist representationof rules, variables and dynamic bindings.
Tech-nical Report MS-CIS-90-05, University of Penn-sylvania, Philadelphia, PA. Revised Jan 1992.Vijay-Shanker, K. (1987).
A Study of Tree Ad-joining Grammars.
PhD thesis, University ofPennsylvania, Philadelphia, PA.151
