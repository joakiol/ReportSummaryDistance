Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 329?334,Dublin, Ireland, August 23-24, 2014.Illinois-LH: A Denotational and Distributional Approach to SemanticsAlice Lai and Julia HockenmaierDepartment of Computer ScienceUniversity of Illinois at Urbana-Champaign{aylai2, juliahmr}@illinois.eduAbstractThis paper describes and analyzes our Se-mEval 2014 Task 1 system.
Its featuresare based on distributional and denota-tional similarities; word alignment; nega-tion; and hypernym/hyponym, synonym,and antonym relations.1 Task DescriptionSemEval 2014 Task 1 (Marelli et al., 2014a) eval-uates system predictions of semantic relatedness(SR) and textual entailment (TE) relations on sen-tence pairs from the SICK dataset (Marelli et al.,2014b).
The dataset is intended to test compo-sitional knowledge without requiring the worldknowledge that is often required for paraphraseclassification or Recognizing Textual Entailmenttasks.
SR scores range from 1 to 5.
TE relationsare ?entailment,?
?contradiction,?
and ?neutral.
?Our system uses features that depend on theamount of word overlap and alignment betweenthe two sentences, the presence of negation, andthe semantic similarities of the words and sub-strings that are not shared across the two sen-tences.
We use simple distributional similaritiesas well as the recently proposed denotational sim-ilarities of Young et al.
(2014), which are intendedas more precise metrics for tasks that require en-tailment.
Both similarity types are estimated onYoung et al.
?s corpus, which contains 31,783 im-ages of everyday scenes, each paired with five de-scriptive captions.2 Our SystemOur system combines different sources of seman-tic similarity to predict semantic relatedness andThis work is licensed under a Creative Commons At-tribution 4.0 International License.
Page numbers and pro-ceedings footer are added by the organizers.
License de-tails: http://creativecommons.org/licenses/by/4.0/textual entailment.
We use distributional sim-ilarity features, denotational similarity features,and alignment features based on shallow syntac-tic structure.2.1 PreprocessingWe lemmatize all sentences with the StanfordCoreNLP system1and extract syntactic chunkswith the Illinois Chunker (Punyakanok and Roth,2001).
Like Young et al.
(2014), we use the Maltparser (Nivre et al., 2006) to identify 5 sets of con-stituents for each sentence: subject NPs, verbs,VPs, direct object NPs, and other NPs.For stopwords, we use the NLTK English stop-word list of 127 high-frequency words.
We re-move negation words (no, not, and nor) from thestopword list since their presence is informativefor this dataset and task.2.2 Distributional SimilaritiesAfter stopword removal and lemmatization, wecompute vectors for tokens that appear at least 10times in Young et al.
(2014)?s image descriptioncorpus.
In the vector space, each dimension corre-sponds to one of the 1000 most frequent lemmas(contexts).
The jth entry of the vector of wiis thepositive normalized pointwise mutual information(pnPMI) between target wiand context wj:pnPMI(wi, wj) = max?
?0,log(P (wi,wj)P (wi)P (wj))?
log (P (wi, wj))?
?We define P (wi) as the fraction of images withat least one caption containing wi, and P (wi, wj)as the fraction of images whose captions containboth wiand wj.
Following recent work that ex-tends distributional similarities to phrases and sen-tences (Mitchell and Lapata, 2010; Baroni andZamparelli, 2010; Grefenstette and Sadrzadeh,1http://nlp.stanford.edu/software/corenlp.shtml329Features Description # of featuresNegation True if either sentence contains explicit negation; False otherwise 1Word overlap Ratio of overlapping word types to total word types in s1and s21Denotational constituent similarity Positive normalized PMI of constituent nodes in the denotationgraph30Distributional constituent similarity Cosine similarity of vector representations of constituent phrases 30Alignment Ratio of number of aligned words to length of s1and s2; max, min,average unaligned chunk length; number of unaligned chunks23Unaligned matching Ratio of number of matched chunks to unaligned chunks; max, min,average matched chunk similarity; number of crossings in matching31Chunk alignment Number of chunks; number of unaligned chunk labels; ratio of un-aligned chunk labels to number of chunks; number of matched la-bels; ratio of matched to unmatched chunk labels17Synonym Number of matched synonym pairs (w1, w2) 1Hypernym Number of matched hypernym pairs (w1, w2), number of matchedhypernym pairs (w2, w1)2Antonym Number of matched antonym pairs (w1, w2) 1Table 1: Summary of features.2011; Socher et al., 2012), we define a phrase vec-tor p to be the pointwise multiplication product ofthe vectors of the words in the phrase:p = w1... wnwhere  is the multiplication of correspondingvector components, i.e.
pi= ui?
vi.2.3 Denotational SimilaritiesIn Young et al.
(2014), we introduce denotationalsimilarities, which we argue provide a more pre-cise metric for semantic inferences.
We use animage-caption corpus to define the (visual) de-notation of a phrase as the set of images it de-scribes, and construct a denotation graph, i.e.
asubsumption hierarchy (lattice) of phrases pairedwith their denotations.
For example, the denota-tion of the node man is the set of images in thecorpus that contain a man, and the denotation ofthe node person is rock climbing is the set of im-ages that depict a person rock climbing.
We de-fine the (symmetric) denotational similarity of twophrases as the pnPMI between their correspond-ing sets of images.
We associate each constituentin the SICK dataset with a node in the denotationgraph, but new nodes that are unique to the SICKdata have no quantifiable similarity to other nodesin the graph.2.4 FeaturesTable 1 summarizes our features.
Since TE is adirectional task and SR is symmetric, we expressfeatures that depend on sentence order twice: 1)f1are the features of s1and f2are the features ofs2, 2) f1are the features of the longer sentenceand f2are the features of the shorter sentence.These directional features are specified in thefollowing feature descriptions.Negation In this dataset, contradictory sentencepairs are often marked by explicit negation, e.g.
s1= ?The man is stirring the sauce for the chicken?and s2= ?The man is not stirring the sauce forthe chicken.?
A binary feature is set to 1 if eithersentence contains not, no, or nobody, and set to 0otherwise.Word Overlap We compute|W1?W2||W1?W2|on lemma-tized sentences without stopwords where Wiisthe set of word types that appear in si.
Traininga MaxEnt or log-linear model using this featureachieves better performance than the word overlapbaseline provided by the task organizers.Denotational Constituent Similarity Denota-tional similarity captures entailment-like relationsbetween events.
For example, sit and eat lunchhave a high pnPMI, which follows our intuitionthat a person who is eating lunch is likely to besitting.
We use the same denotational constituentfeatures that Young et al.
(2014) use for a textualsimilarity task.
C are original nodes, Cancare par-ent and grandparent nodes, and sim(Ca, Cb) is themaximum pnPMI of any pair of nodes a ?
Ca,b ?
Cb.C-C features compare constituents of the sametype.
These features express how often we expectcorresponding constituents to describe the samesituation.
For example, s1= ?Girls are doingbackbends and playing outdoors?
and s2= ?Chil-330dren are doing backbends?
have subject nodes{girl} and {child}.
Girls are sometimes de-scribed as children, so sim(girl, child) = 0.498.In addition, child is a parent node of girl, somax(sim(anc(girl), child)) = 1.
There are 15C-C features: sim(C1, C2), max(sim(C1, Canc2),sim(Canc1, C2)), sim(Canc1, Canc2) for each con-stituent type.C-all features compare different constituenttypes.
These features express how often weexpect any pair of constituents to describe thesame scene.
For example, s1= ?Two teams arecompeting in a football match?
and s2= ?Aplayer is throwing a football?
are topically relatedsentences.
Comparing constituents of differenttypes like player and compete or player andfootball match gives us more information aboutthe similarity of the sentences.
There are 15 C-allfeatures: the maximum, minimum, and sum ofsim(Ct1, C2) and sim(C1, Ct2) for each constituenttype.Distributional Constituent Similarity Distribu-tional vector-based similarity may alleviate thesparsity of the denotation graph.
For example,for subject NP C-C features, we have non-zerodistributional similarity for 87% of instances inthe trial data, but non-zero denotational simi-larity for only 56% of the same instances.
Thefootball and team nodes may have no commonimages in the denotation graph, but we stillhave distributional vectors for football and forteam.
The 30 distributional similarity features arethe same as the denotational similarity featuresexcept sim(a, b) is the cosine similarity betweenconstituent phrase vectors.Alignment Since contradictory and entailing sen-tences have limited syntactic variation in thisdataset, aligning sentences can help to predict se-mantic relatedness and textual entailment.
We usethe Needleman-Wunsch algorithm (1970) to com-pute an alignment based on exact word matchesbetween two lemmatized sentences.
The similar-ity between two lemmas is 1.0 if the words areidentical and 0.0 otherwise, and we do not penal-ize gaps.
This gives us the longest subsequence ofmatching lemmas.The alignment algorithm results in a sentencepair alignment and 2 unaligned chunk sets definedby syntactic chunks.
For example, s1= ?A brownand white dog is running through the tall grass?and s2= ?A brown and white dog is movingthrough the wild grass?
are mostly aligned, withthe remaining chunks u1= {[VP run], [NP tall]}and u2= {[VP move], [NP wild]}.There are 23 alignment features.
Directionalfeatures per sentence are the number of words(2 features), the number of aligned words (2features), and the ratio between those counts (2features).
These features are expressed twice,once according to the sentence order in the datasetand once ordered by longer sentence beforeshorter sentence, for a total of 12 directional fea-tures.
Non-directional features are the maximum,minimum, and average unaligned chunk length foreach sentence and for both sentences combined (9features), and the number of unaligned chunks ineach sentence (2 features).Unaligned Chunk Matching We want to knowthe similarity of the remaining unaligned chunksbecause when two sentences have a high overlap,their differences are very informative.
For exam-ple, in the case that two sentences are identicalexcept for a single word in each sentence, if weknow that the two words are synonymous, then weshould predict that the two sentences are highlysimilar.
However, if the two words are antonyms,the sentences are likely to be contradictory.We use phrase vector similarity to compute themost likely matches between unaligned chunks.We repeat the matching process twice: for sim-ple matching, any 2 chunks with non-zero phrasesimilarity can be matched across sentences, whilefor strict matching, chunks can match only if theyhave the same type, e.g.
NP or VP.
This gives ustwo sets of features.For s1= ?A brown and white dog is runningthrough the tall grass?
and s2= ?A brown andwhite dog is moving through the wild grass,?
theunaligned chunks are u1= {[VP run], [NP tall]}and u2= {[VP move], [NP wild]}.
For strictmatching, the only valid matches are [VP run]?
[VP move] and [NP tall]?
[NP wild].
For simplematching, [NP tall] could also match [VP move]instead and [VP run] could match [NP wild].There are a total of 31 unaligned chunk match-ing features.
Directional features per sentenceinclude the number of unaligned chunks (2features) and the ratio of the number of matchedchunks to the total number of chunks (2 fea-331tures).
These features are expressed twice, onceaccording to the sentence order in the dataset andonce ordered by longer sentence before shortersentence, for a total of 8 directional features.Non-directional features per sentence pair includethe maximum, minimum, and average similarityof the matched chunks (3 features); the maximum,minimum, and average length of the matchedchunks (3 features); and the number of matchedchunks (1 feature).
We extract these 15 featuresfor both simple matching and strict matching.
Inaddition, we also count the number of crossingsthat result from matching the unaligned chunks inplace (1 feature).
This penalizes matched sets thatcontain many crossings or long-distance matches.Chunk Label Alignment and Matching Sincesimilar sentences in this dataset often havesimilar syntax, we compare their chunk labelsequences, e.g.
[NP A brown and white dog][VP is running] [PP through] [NP the tall grass]becomes NP VP PP NP.
We compute 17 featuresbased on aligning and matching these chunklabel sequences.
Directional features are the totalnumber of labels in the sequence (2 features),the number of unaligned labels (2 features), theratio of the number of unaligned labels to thetotal number of labels (2 features), and the ratioof the number of matched labels to the number ofunaligned labels (2 features).
These features areexpressed twice, once according to the sentenceorder in the dataset and once ordered by longersentence before shorter sentence, for a total of 16directional features.
We also count the number ofmatched labels for the sentence pair (1 feature).Synonyms and Hypernyms We count the num-ber of synonyms and hypernyms in the matchedchunks for each sentence pair.
Synonyms arewords that share a WordNet synset, and hyper-nyms are words that have a hypernym relationin WordNet.
There are two hypernym featuresbecause hypernymy is directional: num hyp1isthe number of words in s1that have a hypernymin s2, while num hyp2is the number of wordsin s2that have a hypernym in s1.
For example,s1= ?A woman is cutting a lemon?
and s2= ?Awoman is cutting a fruit?
have num hyp1= 1.For synonyms, num syn is the number of wordpairs in s1and s2that are synonyms.
For example,s1= ?A brown and white dog is running throughthe tall grass?
and s2= ?A brown and whitedog is moving through the wild grass?
havenum syn = 1.Antonyms When we match unaligned chunks, thehighest similarity pair are sometimes antonyms,e.g.
s1= ?Some people are on a crowded street?and s2= ?Some people are on an empty street.
?In other cases, they are terms that we think of asmutually exclusive, e.g.
man and woman.
In bothcases, the sentences are unlikely to be in an en-tailing relationship.
Since resources like WordNetwill fail to identify the mutually exclusive pairsthat are common in this dataset, e.g.
bike and caror piano and guitar, we use the training data tobuild a list of these pairs.
We identify the matchedchunks that occur in contradictory or neutral sen-tences but not entailed sentences.
We exclude syn-onyms and hypernyms and apply a frequency filterof n = 2.
Commonly matched chunks in neutralor contradictory sentences include sit?stand, boy?girl, and cat?dog.
These are terms with differ-ent and often mutually exclusive meanings.
Com-monly matched chunks in entailed sentences in-clude man?person, and lady?woman.
These areterms that could easily be used to describe thesame situation.
However, cut?slice is a commonpair in both neutral and entailed sentences and wedo not want to count it as an antonym pair.
There-fore, we consider frequent pairs that occur in con-tradictory or neutral but not entailed sentences tobe antonyms.The feature num ant is the number of matchedantonyms in a sentence pair.
We identify anantonym if caand cbare on the antonym list oroccur in one of these patterns: X?not X, X?no X,X?no head-noun(X) (e.g.
blue hat?no hat), X?no hypernym(X) (e.g.
poodle?no dog), X?no syn-onym(X) (e.g.
kid?no child).
For each antonympair, we set the similarity score of that match to0.0.For example, num ant = 1 for s1= ?A smallwhite dog is running across a lawn?
and s2= ?Abig white dog is running across a lawn.?
In addi-tion, num ant = 1 for s1= ?A woman is leaningon the ledge of a balcony?
and s2= ?A man isleaning on the ledge of a balcony.
?2.5 ModelsFor the SR task, we implement a log-linear regres-sion model using Weka (Hall et al., 2009).
Specif-332Accuracy Pearson ?Chance baseline 33.3 ?Majority baseline 56.7 ?Probability baseline 41.8 ?Overlap baseline 56.2 0.627Submitted system 84.5 0.799Table 2: TE and SR results on test data.Model Accuracy Pearson ?Overlap baseline 56.8 0.646Negation 61.0 0.093Word overlap 65.0 0.694(+Vector composition) 66.4 0.697+Denotational similarity 74.4 0.751+Distributional similarity 71.8 0.756+Den +Dist 77.0 0.782+Alignment 70.4 0.697+Unaligned chunk matching 75.8 0.719+Align +Match 75.2 0.728+Synonyms 65.2 0.696+Hypernyms 66.8 0.716+Antonyms 71.0 0.704All features 84.2 0.802Table 3: TE and SR results on trial data.ically, under Weka?s default settings, we train aridge regression model with regularization param-eter ?
= 1?10?8.
For the TE task, we use a Max-Ent model implemented with MALLET (McCal-lum, 2002).
The MaxEnt model is optimized withL-BFGS, using the default settings.
Both modelsuse the same set of features.3 ResultsOur submitted system was trained on the full train-ing and trial data (5000 sentences).
Table 2 showsour results on the test data.
We substantially out-perform all baselines.3.1 Feature AblationWe train models on the training data and test onthe trial data.
Models marked with + include ourword overlap feature.
We also examine a singlecompositional feature (vector composition): thecosine similarity of two sentence vectors.
A sen-tence vector is the pointwise multiplication prod-uct of component word vectors.Table 3 compares performance on both tasks.For TE, unaligned chunk matching outperformsother features.
Denotational constituent similarityalso does well.
For SR, distributional and deno-tational features have the highest correlation withgold scores.
Combining them further improvesperformance.% AccuracyModel N E COverlap baseline 77.3 44.8 0.0Negation 85.4 0.0 86.4Word overlap 82.9 63.8 0.0(+Vector composition) 84.7 64.5 0.0+Denotational similarity 83.6 67.3 52.7+Distributional similarity 86.5 60.4 37.8+Den +Dist 85.4 68.7 60.8+Alignment 87.9 50.6 41.8+Unaligned chunk matching 90.4 66.6 37.8+Align +Match 88.6 61.8 50.0+Synonyms 82.2 65.2 0.0+Hypernyms 84.0 68.0 0.0+Antonyms 83.6 82.6 0.0All features 86.5 83.3 77.0Table 4: TE accuracy on trial data by entailmenttype (Neutral, Entailment, Contradiction).Table 4 shows TE accuracy of each model byentailment label.
On contradictions, the negationmodel has 86.0% accuracy while our final systemhas only 77.0% accuracy.
However, the negationmodel cannot identify entailment.
Its performanceis due to the high proportion of contradictions thatcan be identified by explicit negation.We expected antonyms to improve classifica-tion of contradictions, but the antonym featureactually has the highest accuracy of any featureon entailed sentences.
The dataset contains fewcontradictions, and most involve explicit negation,not antonyms.
The antonym feature indicates thatwhen two sentences have high word overlap andno antonyms, one is likely to entail the other.
Neu-tral sentences often contain word pairs that aremutually exclusive, so the antonym feature distin-guishes between neutral and entailed sentences.4 ConclusionOur system combines multiple similarity metricsto predict semantic relatedness and textual entail-ment.
A binary negation feature and similaritycomparisons based on chunking do very well, asdo denotational constituent similarity features.
Inthe future, we would like to focus on multiwordparaphrases and prepositional phrases, which ourcurrent system has trouble analyzing.AcknowledgementsWe gratefully acknowledge support of the Na-tional Science Foundation under grants 1053856and 1205627, as well as an NSF Graduate Re-search Fellowship to Alice Lai.333ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193, Cambridge, MA, October.Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.Experimental support for a categorical composi-tional distributional model of meaning.
In Proceed-ings of the 2011 Conference on Empirical Methodsin Natural Language Processing, pages 1394?1404,Edinburgh, Scotland, UK., July.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An up-date.
SIGKDD Explorations, 11(1):10?18.Marco Marelli, Luisa Bentivogli, Marco Baroni, Raf-faela Bernardi, Stefano Menini, and Roberto Zam-parelli.
2014a.
SemEval-2014 task 1: Evaluationof compositional distributional semantic models onfull sentences through semantic relatedness and tex-tual entailment.
In Proceedings of SemEval 2014:International Workshop on Semantic Evaluation.Marco Marelli, Stefano Menini, Marco Baroni, LuisaBentivogli, Raffaela Bernardi, and Roberto Zampar-elli.
2014b.
A SICK cure for the evaluation of com-positional distributional semantic models.
In Pro-ceedings of LREC.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Jeff Mitchell and Mirella Lapata.
2010.
Compositionin distributional models of semantics.
Cognitive Sci-ence, 34(8):1388?1429.Joakim Nivre, Johan Hall, and Jens Nilsson.
2006.Maltparser: A data-driven parser-generator for de-pendency parsing.
In Proceedings of LREC, vol-ume 6, pages 2216?2219.Vasin Punyakanok and Dan Roth.
2001.
The use ofclassifiers in sequential inference.
In NIPS, pages995?1001.
MIT Press.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211, Jeju Island, Korea, July.Peter Young, Alice Lai, Micah Hodosh, and JuliaHockenmaier.
2014.
From image descriptions tovisual denotations: New similarity metrics for se-mantic inference over event descriptions.
Transac-tions of the Association for Computational Linguis-tics, 2:67?78.334
