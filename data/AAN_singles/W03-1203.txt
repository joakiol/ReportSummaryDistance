Combining Optimal Clustering and Hidden Markov Models forExtractive SummarizationPascale FungHuman Language Technology Center,Dept.
of Electrical & ElectronicEngineering,University of Science & Technology(HKUST)Clear Water Bay, Hong Kongpascale@ee.ust.hkGrace NgaiDept.
of Computing,Hong Kong PolytechnicUniversity,Kowloon, Hong Kongcsgngai@polyu.edu.hkCHEUNG, Chi-ShunHuman Language Technology Center,Dept.
of Electrical & ElectronicEngineering,University of Science & Technology(HKUST)Clear Water Bay, Hong Kongeepercy@ee.ust.hkAbstractWe propose Hidden Markov models withunsupervised training for extractive sum-marization.
Extractive summarization se-lects salient sentences from documents tobe included in a summary.
Unsupervisedclustering combined with heuristics is apopular approach because no annotateddata is required.
However, conventionalclustering methods such as K-means donot take text cohesion into consideration.Probabilistic methods are more rigorousand robust, but they usually require su-pervised training with annotated data.
Ourmethod incorporates unsupervised train-ing with clustering, into a probabilisticframework.
Clustering is done by modi-fied K-means (MKM)--a method thatyields more optimal clusters than the con-ventional K-means method.
Text cohesionis modeled by the transition probabilitiesof an HMM, and term distribution ismodeled by the emission probabilities.The final decoding process tags sentencesin a text with theme class labels.
Parame-ter training is carried out by the segmentalK-means (SKM) algorithm.
The output ofour system can be used to extract salientsentences for summaries, or used for topicdetection.
Content-based evaluationshows that our method outperforms an ex-isting extractive summarizer by 22.8% interms of relative similarity, and outper-forms a baseline summarizer that selectsthe top N sentences as salient sentencesby 46.3%.1 IntroductionMulti-document summarization (MDS) is thesummarization of a collection of related documents(Mani (1999)).
Its application includes the summa-rization of a news story from different sourceswhere document sources are related by the themeor topic of the story.
Another application is thetracking of news stories from the single sourceover different time frame.
In this case, documentsare related by topic over time.Multi-document summarization is also an exten-sion of single document summarization.
One of themost robust and domain-independent summariza-tion approaches is extraction-based or shallowsummarization (Mani (1999)).
In extraction-basedsummarization, salient sentences are automaticallyextracted to form a summary directly  (Kupiec et.al, (1995), Myaeng & Jang (1999), Jing et.
al,(2000), Nomoto & Matsumoto (2001,2002), Zha(2002), Osborne (2002)), or followed by a synthe-sis stage to generate a more natural summary(McKeown & Radev (1999), Hovy & Lin (1999)).Summarization therefore involves some theme ortopic identification and then extraction of salientsegments in a document.Story segmentation, document and sentence andclassification can often be accomplished by unsu-pervised, clustering methods, with little or no re-quirement of human labeled data (Deerwester(1991), White & Cardie (2002), Jing et.
al (2000)).Unsupervised methods or hybrids of supervisedand unsupervised methods for extractive summari-zation have been found to yield promising resultsthat are either comparable or superior to supervisedmethods (Nomoto & Matsumoto (2001,2002)).
Inthese works, vector space models are used anddocument or sentence vectors are clustered to-gether according to some similarity measure(Deerwester (1991), Dagan et al (1997)).The disadvantage of clustering methods lies intheir ad hoc nature.
Since sentence vectors are con-sidered to be independent sample points, the sen-tence order information is lost.
Various heuristicsand revision strategies have been applied to thegeneral sentence selection schema to take into con-sideration text cohesion (White & Cardie (2002),Mani and Bloedorn (1999), Aone et.
al (1999), Zha(2002), Barzilay et al, (2001)).
We would like topreserve the natural linear cohesion of sentences ina text as a baseline prior to the application of anyrevision strategies.To compensate for the ad hoc nature of vectorspace models, probabilistic approaches have re-gained some interests in information retrieval inrecent years (Knight & Marcu (2000), Berger &Lafferty (1999), Miller et al, (1999)).
These re-cent probabilistic methods in information retrievalare largely inspired by the success of probabilisticmodels in machine translation in the early 90s(Brown et.
al), and regard information retrieval asa noisy channel problem.
Hidden Markov Modelsproposed by Miller et al (1999), and have shownto outperform tf, idf in TREC information retrievaltasks.
The advantage of probabilistic models is thatthey provide a more rigorous and robust frame-work to model query-document relations than adhoc information retrieval.
Nevertheless, such prob-abilistic IR models still require annotated trainingdata.In this paper, we propose an iterative unsupervisedtraining method for multi-document extractivesummarization, combining vectors space modelwith a probabilistic model.
We iteratively classifynews articles, then paragraphs within articles, andfinally sentences within paragraphs into commonstory themes, by using modified K-means (MKM)clustering and segmental K-means (SKM) decod-ing.
We obtain an initial clustering of articleclasses by MKM, which determines the inherentnumber of theme classes of all news articles.
Next,we use SKM to classify paragraphs and then sen-tences.
SKM iterates between a k-means clusteringstep, and a Viterbi decoding step, to obtain a finalclassification of sentences into theme classes.
OurMKM-SKM paradigm combines vector space clus-tering model with a probabilistic framework, pre-serving some of the natural sentence cohesion,without the requirement of annotated data.
Ourmethod also avoids any arbitrary or ad hoc settingof parameters.In section 2, we introduce the modified K-meansalgorithm as a better alternative than conventionalK-means for document clustering.
In section 3 wepresent the stochastic framework of theme classifi-cation and sentence extraction.
We describe thetraining algorithm in section 4, where details of themodel parameters and Viterbi scoring are pre-sented.
Our sentence selection algorithm is de-scribed in Section 5.
Section 6 describes ourevaluation experiments.
We discuss the results andconclude in section 7.2 Story Segmentation using Modified K-means (MKM) ClusteringThe first step in multi-document summarization isto segment and classify documents that have acommon theme or story.
Vector space models canbe used to compare documents (Ando et al (2000),Deerwester et al (1991)).
K-means clustering iscommonly used to cluster related document or sen-tence vectors together.
A typical k-means cluster-ing algorithm for summarization is as follows:1.
Arbitrarily choose K vectors as initial centroids;2.
Assign vectors closest to each centroid to its cluster;3.
Update centroid using all vectors assigned to eachcluster;4.
Iterate until average intra-cluster distance falls be-low a threshold;We have found three problems with the standard k-means algorithm for sentence clustering.
First, theinitial number of clusters k, has to be set arbitrarilyby humans.
Second, the initial partition of a clusteris arbitrarily set by thresholding.
Hence, the initialset of centroids is arbitrary.
Finally, during cluster-ing, the centroids are selected as the sentenceamong a group of sentences that has the least aver-age distance to other sentences in the cluster.
Allthese characteristics of K-means can be the causeof a non-optimal cluster configuration at the finalstage.To avoid the above problems, we propose usingmodified K-means (MKM) clustering algorithm(Wilpon & Rabiner(1985)), coupled with virtualdocument centroids.
MKM starts from a globalcentroid and splits the clusters top down until theclusters stabilize:1.
Compute the centroid of the entire training set;2.
Assign vectors closest to each centroid to its cluster;3.
Update centroid using all vectors assigned to eachcluster;4.
Iterate 2-4 until vectors stop moving between clus-ters;5.
Stop if clusters stabilizes, and output final clusters,else goto step 6;6.
Split the cluster with largest intra-cluster distanceinto two by finding the pair of vectors with largestdistance in the cluster.
Use these two vectors as newcentroids, and repeat steps 2-5.In addition, we do not use any existing documentin the collection as the selected centroid.
Rather,we introduce virtual centroids that contain the ex-pected value of all documents in a cluster.
An ele-ment of the centroid is the average weight of thesame index term in all documents within that clus-ter:MwMmmii==1?The vectors are document vectors in this step.
Thenumber of clusters is determined after the clustersare stabilized.
The resultant cluster configuration ismore optimal and balanced than that from usingconventional k-means clustering.
Using the MKMalgorithm with virtual centroids, we segment thecollection of news articles into clusters of relatedarticles.
Articles covering the same story from dif-ferent sources now carry the same theme label.Articles from the same source over different timeperiod also carry the same theme label.
In the nextstage, we iteratively re-classify each paragraph,and then re-classify each sentence in each para-graph into final theme classes.3 A Stochastic Process of Theme Classifi-cationAfter we have obtained story labels of each article,we need to classify the paragraphs and then thesentences according to these labels.
Each para-graph in the article is assigned the cluster numberof that article, as we assume all paragraphs in thesame article share the same story theme.We suggest that the entire text generation processcan be considered as a stochastic process that startsin some theme class, generates sentences one afteranother for that theme class, then goes to the nexttheme and generates the sentences, so on and soforth, until it reaches the final theme class in adocument, and finishes generating sentences in thatclass.
This is an approximation of the authoringprocess where a writer thinks of a certain structurefor his/her article, starts from the first section,writes sentences in that section, proceeds to thenext section, etc., until s/he finishes the last sen-tence in the last section.Given a document of sentences, the task of sum-mary extraction involves discovering the underly-ing theme class transitions at the sentenceboundaries, classify each sentence according tothese theme concepts, and then extract the salientsentences in each theme class cluster.We want to find )|(maxarg DCPC  where Dis adocument consisting of linearly ordered sentencesequences ))(,),(,),2(),1(( TstsssD = , and Cisa theme class sequence which consists of the classlabels of all the sentences in D,)))((,)),((,)),2(()),1((( TsctscscscC = .Following Bayes Rule gives us)(/)()|()|( DPCPCDPDCP  = .
We assume )(DP  isequally likely for all documents, so that finding thebest class sequence becomes:)))(()),((,)),2(()),1((()))((),()),((),(,)),2((),2()),1((),1((maxarg)()|(maxarg)|(maxargTsctscscscPTscTstsctsscsscsPCPCDPDCPCCC?=?Note that the total number of theme classes is farfewer than the total number of sentences in adocument and the mapping is not one-to-one.
Ourtask is similar to the concept of discourse parsing(Marcu (1997)), where discourse structures areextracted from the text.
In our case, we are carry-ing out discourse tagging, whereby we assign theclass labels or tags to each sentence in the docu-ment.We use Hidden Markov Model for this stochasticprocess, where the classes are assumed to be hid-den states.We make the following assumptions:?
The probability of the sentence given its past onlydepends on its theme class (emission probabilities);?
The probability of the theme class only depends onthe theme classes of the previous N sentences (tran-sition probabilities).The above assumptions lead to a Hidden MarkovModel with M states representing M differenttheme classes.
Each state can generate differentsentences according to some probability distribu-tion?the emission probabilities.
These states arehidden as we only observe the generated sentencesin the text.
Every theme/state can transit to anyother theme/state, or to itself, according to someprobabilities?the transition probabilities.2C1C 3CjC?=Lijt CiSP0)|)(( Figure 1: An ergodic HMM for theme taggingOur theme tagging task then becomes a searchproblem for HMM: Given the observation se-quence ))(,),(,),2(),1(( TstsssD = , and themodel ?
, how do we choose a corresponding statesequence )))((,)),((,)),2(()),1((( TsctscscscC =  ,that best explains the sentence sequence?To train the model parameter ?
, we need to solveanother problem in HMM: How do we adjust themodel parameters ),,( pi?
BA= , the transition,emission and initial probabilities, to maximize thelikelihood of the observation sentence sequencesgiven our model?In a supervised training paradigm, we can obtainhuman labeled class-sentence pairs and carry out arelative frequency count for training the emissionand transition probabilities.
However, hand label-ing some large collection of texts with themeclasses is very tedious.
One main reason is thatthere is a considerable amount of disagreementbetween humans on manual annotation of themesand topics.
How many themes should there be?Where should each theme start and end?It is therefore desirable to decode the hidden themeor topic states using an unsupervised trainingmethod without manually annotated data.
Conse-quently, we only need to cluster and label the ini-tial document according to cluster number.
In theHMM framework, we then improve upon this ini-tial clustering by iteratively estimate ),,( pi?
BA= ,and maximize )|( DCP   using a Viterbi decoder.3.1 Sentence Feature Vector and SimilarityMeasurePrior to the training process, we need to define sen-tence feature vector and the similarity measure forcomparing two sentence vectors.As we consider a document Dto be a sequence ofsentences, the sentences themselves are repre-sented as feature vectors )(ts  of length L, where tis the position of the sentence in the document andL is the size of the vocabulary.
Each element of thevector )(ts  is an index term in the sentence,weighted by its text frequency (tf) and inversedocument frequency (idf) where tf is defined as thefrequency of the word in that particular sentence,and idf  is the inverse frequency of the word in thelarger document collectionNdflog?
where df isthe number of sentences this particular word ap-pears in and N is the total number of sentences inthe training corpus.
We select the sentences asdocuments in computing the tf and idf because weare comparing sentence against sentence.In the initial clustering and subsequent Viterbitraining process, sentence feature vectors need tobe compared to the centroid of each cluster.
Vari-ous similarity measures and metrics include thecosine measure, Dice coefficient, Jaccard coeffi-cient, inclusion coefficient, Euclidean distance, KLconvergence, information radius, etc (Manning &Sch   tze (1999), Dagan et al (1997), Salton andMcGill (1983)).
We chose the cosine similaritymeasure for its ease in computation: = ==?
?=LiLiiiLiiivstsvstsvsts1 1221))(())(()()())(),(cos(4 Segmental K-means Clustering for Pa-rameter TrainingIn this section, we describe an iterative trainingprocess for estimation of our HMM parameters.We consider the output of the MKM clusteringprocess in Section 2 as an initial  segmentation oftext into class sequences.
To improve upon thisinitial segmentation, we use an iterative Viterbitraining method that is similar to the segmental k-means clustering for speech processing (Rabiner &Juang(1993)).
All articles in the same story clusterare processed as follows:1.
Initialization: All paragraphs in the same story classare clustered again.
Then all sentences in the sameparagraph shares the same class label as that para-graph.
This is the initial class-sentence segmentation.Initial class transitions are counted.2.
(Re-)clustering: Sentence vectors with their classlabels are repartitioned into K clusters (K is obtainedfrom the MKM step previously) using the K-meansalgorithm.
This step is iterated until the clusters sta-bilize.3.
(Re-)estimation of probabilities: The centroids ofeach cluster are estimated.
Update emission prob-abilities from the new clusters.4.
(Re-)classification by decoding: the updated set ofmodel parameters from step 2 are used to rescore the(unlabeled) training documents into sequences ofclass given sentences, using Viterbi decoding.
Up-date class transitions from this output.5.
Iteration: Stop if convergence conditions are met,else repeat steps 2-4.The segmental clustering algorithm is iterated untilthe decoding likelihood converges.
The finaltrained Viterbi decoder is then used to tag un-annotated data sets into class-sentence pairs.In the following Sections 4.1 and 4.2, we discuss inmore detail steps 3 and 4.4.1 Estimation of ProbabilitiesWe need to train the parameters of our HMM suchthat the model can best describe the training data.During the iterative process, the probabilities areestimated from class-sentence pair sequences ei-ther from the initialization stage or the re-classification stage.4.1.1 Transition Probabilities: Text Cohesionand Text SegmentationText cohesion (Halliday and Hasan (1996)) is animportant concept in summarization as it under-lines the theme of a text segment based on connec-tivity patterns between sentences (Mani (2002)).When an author writes from theme to theme in alinear text, s/he generates sentences that are tightlylinked together within a theme.
When s/he pro-ceeds to the next theme, the sentences that are gen-erated next are quite separate from the previoustheme of sentences but are they themselves tightlylinked again.As mentioned in the introduction, most extraction-based summarization approaches give certain con-sideration to the linearity between sentences in atext.
For example, Mani (1999) uses spread activa-tion weight between sentence links,  (Barzilay et al2001) uses a cohesion constraint that led to im-provement in summary quality.
Anone et al (1999)uses linguistic knowledge such as aliases, syno-nyms, and morphological variations to link lexicalitems together across sentences.Term distribution has been studied by many NLPresearchers.
Manning & Sch?tze (1999) gives agood overview of various probability distributionsused to describe how a term appears in a text.
Thedistributions are in general non-Gaussian in nature.Our Hidden Markov Model provides a unifiedframework to incorporate text cohesion and termdistribution information in the transition probabili-ties of theme classes.
The class of a sentence de-pends on the class labels of the previous Nsentences.
The linearity of the text is hence pre-served in our model.
In the preliminary experi-ment, we set N to be one, that is, we are using a bi-gram class model.4.1.2 Emission Probabilities: Poisson distribu-tion of termsFor the emission probabilities, there are a numberof possible formulations.
We cannot use relativefrequency counts of number of sentences in clus-ters divided by the total sentences in the clustersince most sentences occur only once in the entirecorpus.
Looking at the sentence feature vector, wetake the view that the probability of a sentencevector being generated by a particular cluster is theproduct of the probabilities of the index terms inthe sentence occurring in that cluster according tosome distribution, and that these term distributionprobabilities are independent of each other.For a sentence vector of length L, where L is thetotal size of the vocabulary, its elements?the in-dex terms?have certain probability density func-tion (pdf).
In speech processing, spectral featuresare assumed to follow independent Gaussian dis-tributions.
In language processing, several modelshave been proposed for term distribution, includingthe Poisson distribution, the two-Poisson model forcontent and non-content words (Bookstein andSwanson (1975)), the negative binomial (Mostellerand Wallace (1984), Church and Gale (1995)) andKatz?s k-mixture (Katz (1996)).
We adopt twoschemes for comparison (1) the unigram distribu-tion of each index term in the clusters; (2) the Pois-son distribution as pdf.
for modeling the termemission probabilities:!);(kekpkiii??
?
?=At each estimation step of the training process, the?
for the Poisson distribution is estimated from thecentroid of each theme cluster.
11Strictly speaking, we ought to re-estimate the IDF in the k-mixtureduring each iteration by using the re-estimated clusters from the k-means step as the documents.
However, we simplify the process byusing the pre-computed IDF from all training documents.4.2 Viterbi Decoding: Re-classification withsentence cohesionAfter each re-estimation, we use a Viterbi decoderto find the best class sequence given a documentcontaining sentence sequences.
The ?time se-quence?
corresponds to the sequence of sentencesin a document whereas the states are the themeclasses.At each node of the trellis, the probability of a sen-tence given any class state is computed from thetransition probabilities and the emission probabili-ties.
After Viterbi backtracking, the best class se-quence of a document is found and the sentencesare relabeled by the class tags.5 Salient Sentence ExtractionThe SKM algorithm is iterated until the decodinglikelihood converges.
The final trained Viterbi de-coder is then used to tag un-annotated data setsinto class-sentence pairs.
We can then extract sali-ent sentences from each class to be included in asummary, or for question-answering.To evaluate the effectiveness of our method as afoundation for extractive summarization, we ex-tract sentences from each theme class in eachdocument using four features, namely:(1) the position of the sentencenp 1= -- the further it isfrom the title, the less important it is supposed to be;(2) the cosine similarity of the sentence with the centroid ofits class ?1;(3) its similarity with the first sentence in the article ?2; and(4) the so-called Z model (Zechner (1996), Nomoto & Ma-tsumoto (2000)), where the mass of a sentence is com-puted as the sum of tf, idf values of index terms in thatsentence and the center of mass  is chosen as the salientsentence to be included in a summary.
))(())))((log(1(maxarg1tsidftstfz iLiis=?+=The above features are linearly combined to yield afinal saliency score for every sentence:zwwwpwsw ?+?+?+?= 423121)( ?
?Our features are similar to those in an existing sys-tem (Radev 2002), with the difference in the cen-troid computation (and cluster definition), resultingfrom our stochastic system.6 ExperimentsMany researchers have proposed various evalua-tion methods for summarization.
We find that ex-trinsic, task-oriented evaluation method to be mosteasily automated, and quantifiable (Radev 2000).We choose to evaluate our stochastic theme classi-fication system (STCS) on a multi-documentsummarization task, among other possible tasksWe choose a content-based method to evaluate thesummaries extracted by our system, compared tothose by another extraction-based system MEAD(Radev 2002), and against a baseline system thatchooses the top N sentence in each document assalient sentences.
All three systems are consideredunsupervised.The evaluation corpus we use is a segment of theEnglish part of HKSAR news from the LDC, con-sisting of 215 articles.
We first use MEAD to ex-tract summaries from 1%-20% compression ratio.We then use our system to extract the same num-ber of salient sentences as MEAD, according to thesentence weights.
The baseline system also ex-tracts the same amount of data as the other twosystems.
We plot the cosine similarities of theoriginal 215 documents with each individual ex-tracted summaries from these three systems.
Thefollowing figure shows a plot of cosine similarityscores against compression ratio of each extractedsummary.
In terms of relative similarity score, oursystem is 22.8% higher on average than MEAD,and 46.3% higher on average than the base-line.00.
10.
20.
30.
40.
50.
60.
70.
80.
90% 10% 20% 30%Our  syst emMEADTOP-N SentFigure 2: Our system outperforms an existing multi-documentsummarizer (MEAD) by 22.8% on average, and outperformsthe baseline top-N sentence selection system by 46.3% onaverage.We would like to note that in our comparativeevaluation, it is necessary to normalize all variablefactors that might affect the system performance,other than the intrinsic algorithm in each system.For example, we ensure that the sentence segmen-tation function is identical in all three systems.
Inaddition, index term weights need to be properlytrained within their own document clusters.
SinceMEAD discards all sentences below the length 9,the other two systems also discard such sentences.The feature weights in both our system and MEADare all set to the default value one.
Since all otherfeatures are the same between our system andMEAD, the difference in performance is attributedto the core clustering and centroid computationalgorithms in both systems.7 Conclusion and DiscussionWe have presented a stochastic HMM frameworkwith modified K-means and segmental K-meansalgorithms for extractive summarization.
Ourmethod uses an unsupervised, probabilistic ap-proach to find class centroids, class sequences andclass boundaries in linear, unrestricted texts in or-der to yield salient sentences and topic segmentsfor summarization and question and answer tasks.We define a class to be a group of connected sen-tences that corresponds to one or multiple topics inthe text.
Such topics can be answers to a userquery, or simply one concept to be included in thesummary.
We define a Markov model where thestates correspond to the different classes, and theobservations are continuous sequences of sen-tences in a document.
Transition probabilities arethe class transitions obtained from a training cor-pus.
Emission probabilities are the probabilities ofan observed sentence given a specific class, fol-lowing a Poisson distribution.
Unlike conventionalmethods where texts are treated as independentsentences to be clustered together, our method in-corporates text cohesion information in the classtransition probabilities.
Unlike other HMM andnoisy channel, probabilistic approaches for infor-mation retrieval, our method does not require an-notated data as it is unsupervised.We also suggest using modified K-means cluster-ing algorithm to avoid ad hoc choices of initialcluster set as in the conventional K-means algo-rithm.
For unsupervised training, we use a segmen-tal K-means training method to iteratively improvethe clusters.
Experimental results show that thecontent-based performance of our system is 22.8%above that of an existing extractive summarizationsystem, and 46.3% above that of simple top-N sen-tence selection system.
Even though the evalua-tion on the training set is not a close evaluationsince the training is unsupervised, we will alsoevaluate on testing data not included in the trainingset as our trained decoder can be used to classifysentences in unseen texts.
Our framework serves asa foundation for future incorporation of other sta-tistical and linguistic information as vector features,such as part-of-speech tags, name aliases, syno-nyms, and morphological variations.ReferencesChinatsu Aone, James Gorlinsky, Bjornar Larsen, and Mary Ellen Oku-rowski, 1999.
A trainable summarizer with knowledge acquired fromrobust NLP techniques.
In Advances in automatic text summarization,ed.
Inderjeet Mani and Mark T. Maybury.
pp 71-80.Michele Banko, Vibhu O. Mittal & Michael J. Witbrock.
2000.
HeadlineGeneration Based on Statistical Translation.
In Proc.
Of the Associa-tion for Computational Linguistics.Regina Barzilay,  Noemie Elhadad & Kathleen R. McKeown.
2001.
Sen-tence Ordering in Multi-document Summarization.
In  Proceedings ofthe 1st Human Language Technology Conference.
pp 149-156.
SanDiego, CA, US.Adam Berger & John Lafferty.
1999.
Information Retrieval as StatisticalTranslation.
.
.
In Proc.
Of the 22nd  ACM SIGIR Conference (SIGIR-99).
pp 222-229.
Berkeley, CA, USA.Branimir Boguraev & Christopher Kennedy.
1999.
Salience-Based ContentCharacterisation of Text Documents.
In Advances in automatic textsummarization / edited.
Inderjeet Mani and Mark T. Maybury.
pp 99-110.Kenneth Ward Church.
1988.
A Stochastic Parts Program and Noun PhraseParser for Unrestricted Text.
In  Proceedings of the Second Conferenceon Applied Natural Language Processing.
pp 136--143.
Austin, TX.Ido Dagan, Lillian Lee, & Fernando Pereira.
1997.
Similarity-based meth-ods for word sense disambiguation.
In Proc.
Of the 32nd  Conference ofthe Association of Computational Linguistics, pp 56-63.Halliday & Hasan, 1976.
Cohesion in English.
London: Longman.Marti A. Hearst.
1994.
Multi-Paragraph Segmentation of       ExpositoryText.
In  Proc.
Of the Association for Computational Linguistics.
pp 9-16.
Las Cruces, NM.Eduard Hovy & Chin-Yew Lin.
1999.
Automated Text Summarization inSUMMARIST In Advances in automatic text summarization / edited.Inderjeet Mani and Mark T. Maybury.
pp 81-97.H.
Jing.
Dragomir R. Radev and M. Budzikowska.
Centroid-based summa-rization of multiple documents: sentence extaction, utility-basedevaluation and user studies.
In Proceedings of ANLP/NAACL-2000.Slava M. Katz.
1996.
Distribution of content words and phrases in text andlanguage modeling.
In Natural Language Engineering, Vol.2 Part.1,pp15-60.Kevin Knight & Daniel Marcu.
2000.
Statistics-Based Summarization ?Step One: Sentence Compression.
In Proc.
Of the 17th Annual Confer-ence of the American Association for Artificial Intelligence.
pp  703-710.
Austin, Texas, US.Julian Kupiec, Jan Pedersen & Francine Chen.
1995.
A Trainable Docu-ment Summarizer.
In Proc.
Of the 18th ACM-SIRGIR Conference.
pp68-73.Inderjeet Mani & Eric Bloedorn.
1999.
Summarizing Similarities andDifferences Among Related Documents.
In Information Retrieval, 1.pp 35-67.Christopher D. Manning & Hinrich Sch   tze 1999.
Foundations of statisti-cal natural language processing.
The MIT Press  Cambridge, Massachu-setts.
London, England.Daniel Marcu.
1997.
The Rhetorical Parsing of Natural Language Texts.In Proc.
of the 35th Annual Meeting of Association for ComputationalLinguistics and 8th Conference of European Chapter of Association forComputational Linguistics.
pp 96-103.
Madrid, Spain.Bernard Merialdo.
1994.
Tagging English Text with a Probabilistic Model.In Computational Linguistics, 20.2. pp 155-172.David R.H. Miller, Tim Leek & Richard M. Schwartz.
1999.
A HiddenMarkov Model Information Retrieval System.
In Proc.
Of theSIGIR?99   pp 214?221.
Berkley, CA, US.Sung Hyon Myaeng & Dong-Hyun Jang.
1999.
Development and Evalua-tion of a Statistically-Based Document Summarization System.
In Ad-vances in automatic text summarization / edited.
Inderjeet Mani andMark T. Maybury.
MIT Press.
pp 61-70.Tadashi Nomoto & Yuji Matsumoto.
2002.
Supervised ranking in opendomain summarization.
In Proc.
Of the 40th  Conference of the Associa-tion of Computational Linguistics, pp.. Pennsylvania, US.Tadashi Nomoto & Yuji Matsumoto.
2001.
A New Approach to Unsu-pervised Text Summarization.
In  Proc.
Of the SIGIR?01, pp 26-34  NewOrleans, Louisiana, USA.Jahna C. Otterbacher, Dragomir R. Radev & Airong Luo.
2002.
Revisionthat Improve Cohesion in Multi-document Summaries.
In  Proc.
Of theWorkshop on Automatic Summarization (including DUC 2002), pp 27-36.
Association for Computational Linguistics.
Philadelphia, US.Miles Osborne.
2002.
Using Maximum Entropy for Sentence Extraction.In  Proc.
Of the Workshop on Automatic Summarization (includingDUC 2002), pp 1-8.
Association for Computational Linguistics.
Phila-delphia, US.Dragomir Radev, Adam Winkel & Michael Topper .
2002.
Multi-Document Centroid-based Text Summarization..
In Proc.
Of the ACL-02 Demonstration Session, pp112-113.
Pennsylvania, US.Michael White & Claire Cardie.
2002.
Selecting Sentences for Multidocu-ment Summaries using Randomized Local Search.
In Proc.
Of theWorkshop on Automatic Summarization (including DUC 2002), pp 9-18.
Association for Computational Linguistics.
Philadelphia, US.J.G.
Wilpon & L.R.
Rabiner 1985.
A modified K-means clustering algo-rithm for use in isolated word recognition.
In IEEE Trans.
Acoustics,Speech, Signal Proc.
ASSP-33(3), pp 587-594.K.
Zechner.
1996.
Fast generation of abstracts from general domain textcorpora by extracting relevant sentences.
In Proc.
Of the 16th  Interna-tional Conference on Computational Linguistics, pp 986-989.
Copen-hagen, Denmark.Hongyuan Zha.
2002.
Generic Summarization and Keyphrase ExtractionUsing Mutual Reinforcement Principle and Sentence Clustering.
InProc.
Of the SIGIR?02.
pp 113-120.
Tampere, Finland Endre Boros,Paul B. Kantor & David J. Neu.
2001.
A Clustering Based Approach toCreating Multi-Document Summaries.
