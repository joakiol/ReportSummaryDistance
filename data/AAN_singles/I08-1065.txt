Acquiring Event Relation Knowledge by Learning Cooccurrence Patternsand Fertilizing Cooccurrence Samples with Verbal NounsShuya Abe Kentaro Inui Yuji MatsumotoGraduate School of Information Science,Nara Institute of Science and Technology{shuya-a,inui,matsu}@is.naist.jpAbstractAiming at acquiring semantic relations be-tween events from a large corpus, this paperproposes several extensions to a state-of-the-art method originally designed for entity re-lation extraction, reporting on the present re-sults of our experiments on a Japanese Webcorpus.
The results show that (a) there areindeed specific cooccurrence patterns use-ful for event relation acquisition, (b) theuse of cooccurrence samples involving ver-bal nouns has positive impacts on both re-call and precision, and (c) over five thou-sand relation instances are acquired from a500M-sentence Web corpus with a precisionof about 66% for action-effect relations.1 IntroductionThe growing interest in practical NLP applicationssuch as question answering, information extractionand multi-document summarization places increas-ing demands on the processing of relations betweentextual fragments such as entailment and causal rela-tions.
Such applications often need to rely on a largeamount of lexical semantic knowledge.
For exam-ple, a causal (and entailment) relation holds betweenthe verb phrases wash something and something isclean, which reflects the commonsense notion that ifsomeone has washed something, this object is cleanas a result of the washing event.
A crucial issue ishow to obtain and maintain a potentially huge col-lection of such event relations instances.Motivated by this background, several researchgroups have reported their experiments on automaticacquisition of causal, temporal and entailment re-lations between event mentions (typically verbs orverb phrases) (Lin and Pantel, 2001; Inui et al,2003; Chklovski and Pantel, 2005; Torisawa, 2006;Pekar, 2006; Zanzotto et al, 2006, etc.).
The com-mon idea behind them is to use a small number ofmanually selected generic lexico-syntactic cooccur-rence patterns (LSPs or simply patterns).
to Verb-Xand then Verb-Y, for example, is used to obtain tem-poral relations such as marry and divorce (Chklovskiand Pantel, 2005).
The use of such generic patterns,however, tends to be high recall but low precision,which requires an additional component for pruningextracted relations.
This issue has been addressed inbasically two approaches, either by devising heuris-tic statistical scores (Chklovski and Pantel, 2005;Torisawa, 2006; Zanzotto et al, 2006) or trainingclassifiers for disambiguation with heavy supervi-sion (Inui et al, 2003).This paper explores a third way for enhancingpresent LSP-based methods for event relation acqui-sition.
The basic idea is inspired by the followingrecent findings in relation extraction (Ravichandranand Hovy, 2002; Pantel and Pennacchiotti, 2006,etc.
), which aims at extracting semantic relations be-tween entities (as opposed to events) from texts.
(a)The use of generic patterns tends to be high recallbut low precision, which requires an additional com-ponent for pruning.
(b) On the other hand, there arespecific patterns that are highly reliable but they aremuch less frequent than generic patterns and eachmakes only a small contribution to recall.
(c) Com-bining a few generic patters with a much larger col-lection of reliable specific patterns boosts both pre-497cision and recall.
Such specific patterns can be ac-quired from a very large corpus with seeds.Given these insights, an intriguing question iswhether the same story applies to event relation ac-quisition as well or not.
In this paper, we explore thisissue through the following steps.
First, while previ-ous methods use only verb-verb cooccurrences, weuse cooccurrences between verbal nouns and verbssuch as cannot ?find out (something)?
due to thelack of ?investigation?
as well as verb-verb cooc-currences.
This extension dramatically enlarge thepool of potential candidate LSPs (Section 4.1).
Sec-ond, we extend Pantel and Pennacchiotti (2006)?sEspresso algorithm, which induces specific reliableLSPs in a bootstrapping manner for entity relationextraction, so that the extended algorithm can applyto event relations (Sections 4.2 to 4.4).
Third, wereport on the present results of our empirical experi-ments, where the extended algorithm is applied to aJapanese 500M-sentence Web corpus to acquire twotypes of event relations, action-effect and action-means relations (Section 5)2 Related workPerhaps a simplest way of using LSPs for event rela-tion acquisition can be seen in the method Chklovskiand Pantel (2005) employ to develop VerbOcean.Their method uses a small number of manually se-lected generic LSPs such as to Verb-X and then Verb-Y to obtain six types of semantic relations includingstrength (e.g.
taint ?
poison) and happens-before(e.g.
marry ?
divorce) and obtain about 29,000 verbpairs with 65.5% precision.One way for pruning extracted relations is to in-corporate a classifier trained with supervision.
Inuiet al (2003), for example, use a Japanese genericcausal connective marker tame (because) and a su-pervised classifier learner to separately obtain fourtypes of causal relations: cause, precondition, effectand means.Torisawa (2006), on the other hand, acquires en-tailment relations by combining the verb pairs ex-tracted with a highly generic connective patternVerb-X and Verb-Y together with the cooccurrencestatistics between verbs and their arguments.
Whilethe results Torisawa reports look promising, it is notclear yet if the method applies to other types of rela-tions because it relies on relation-specific heuristics.Another direction from (Chklovski and Pantel,2005) is in the use of LSPs involving nominalizedverbs.
Zanzotto et al (2006) obtain, for example, anentailment relation X wins ?
X plays from such apattern as player wins.
However, their way of usingnominalized verbs is highly limited compared withour way of using verbal nouns.3 EspressoThis section overviews Pantel and Pennacchiotti(2006)?s Espresso algorithm.
Espresso takes as inputa small number of seed instances of a given targetrelation and iteratively learns cooccurrence patternsand relation instances in a bootstrapping manner.Ranking cooccurrence patterns For each givenrelation instance {x, y}, Espresso retrieves the sen-tences including both x and y from a corpus andextracts from them cooccurrence samples.
For ex-ample, given an instance of the is-a relation suchas ?Italy,country?, Espresso may find cooccurrencesamples such as countries such as Italy and extractsuch a pattern as Y such as X. Espresso defines thereliability rpi(p) of pattern p as the average strengthof its association with each relation instance i inthe current instance set I , where each instance i isweighted by its reliability r?
(i):rpi(p) = 1|I|?i?Ipmi(i, p)max pmi ?
r?
(i) (1)where pmi(i, p) is the pointwise mutual informationbetween i and p, and maxpmi is the maximum PMIbetween all patterns and all instances.Ranking relation instances Intuitively, a reliablerelation instance is one that is highly associated withmultiple reliable patterns.
Hence, analogously to theabove pattern reliability measure, Espresso definesthe reliability r?
(i) of instance i as:r?
(i) = 1|P |?p?Ppmi(i, p)max pmi ?
rpi(p) (2)where rpi(p) is the reliability of pattern p, definedabove in (1), and maxpmi is as before.
r?
(i) andrpi(p) are recursively defined, where r?
(i) = 1 foreach manually supplied seed instance i1.1For our extension, r?
(i) = ?1 for each manually suppliednegative instance.4984 Event relation acquisitionOur primary concerns are whether there are in-deed specific cooccurrence patterns useful for ac-quiring event relations and whether such patternscan be found in a bootstrapping manner analogous toEspresso.
To address these issues, we make severalextensions to Espresso, which is originally designedfor entity relations (not scoping event relations).4.1 Cooccurences with verbal nounsMost previous methods for event relation acquisitionrely on verb-verb cooccurrences because verbs (orverb phrases) are the most typical device for refer-ring to events.
However, languages have anotherlarge class of words for event reference, namelyverbal nouns or nominalized forms of verbs.
InJapanese, for example, verbal nouns such as kenkyu(research) constitute the largest morphological cate-gory used for event reference.Japanese verbal nouns have dual statuses, as verbsand nouns.
When occurring with the verb suru (do-PRES), verbal nouns function as a verb as in (1a).On the other hand, when accompanied by case mark-ers such as ga (NOMINATIVE) and o (ACCUSATIVE),they function as a noun as in (1b).
Finally, but evenmore importantly, when accompanied by a large va-riety of suffixes, verbal nouns constitute compoundnouns highly productively as in (1c).
(1) a. Ken-ga gengo-o kenkyu-suruKen-NOM language-ACC research-PRESKen researches on language.b.
Ken-ga gengo-no kenkyu-o yame-taKen-NOM language-on research-ACC quit-PASTKen quitted research on language.c.
-sha (person):e.g.
kenkyu-sha (researcher)-shitsu (place):e.g.
kenkyu-shitsu (laboratory)-go (after):e.g.
kenkyu-go (after research)These characteristics of verbal nouns can be madeuse of to substantially increase both cooccurrenceinstances and candidate cooccurrence patterns (seeSection 5.1 for statistics).
For example, the verbalnoun kenkyu (research) often cooccurs with the verbjikken (experiment) in the pattern of (2a).
Fromthose cooccurrences, one may learn that jikken-suru(to experiment) is an action that is often taken as apart of kenkyu-suru (to research).
In such a case, wemay consider a pattern as shown in (2b) useful foracquiring part-of relations between actions.
(2) a. kenkyu-shitsu-de jikken-sururesearch-place-in experiment-VERBconduct experiments in the laboratoryb.
(Act-X)-shitsu-de (Act-Y)-suru(Act-X)-place-in (Act-X)-VERB(Act-Y) is often done in doing (Act-X)When functioning as a noun, verbal nouns are po-tentially ambiguous between the event reading andthe entity/object reading.
For example, the ver-bal noun denwa (phone) in the context denwa-de(phone-by) may refer to either a phone-call eventor a physical phone.
While, ideally, such event-hood ambiguities should be resolved before collect-ing cooccurrence samples with verbal nouns, wesimply use all the occurrences of verbal nouns incollecting cooccurrences in our experiments.
It isan interesting issue for future work whether event-hood determination would have a strong impact onthe performance of event relation extraction.4.2 Selection of argumentsOne major step from the extraction of entity rela-tions to the extraction of event relations is how toaddress the issue of generalization.
In entity rela-tion extraction, relations are typically assumed tohold between chunks like named entities or simplybetween one-word terms, where the issue of deter-mining the appropriate level of the generality of ex-tracted relations has not been salient.
In event rela-tion extraction, on the other hand, this issue imme-diately arises.
For example, the cooccurrence sam-ple in (3) suggests the action-effect relation betweenniku-o yaku (grill the meat) and (niku-ni) kogeme-gatsuku ((the meat) gets brown)2.
(3) ( kogeme-ga tsuku ) -kurai niku-o yakua burn-NOM get -so that meat-ACC grillgrill the meat so that it gets brown(grill the meat to a deep brown)In this relation, the argument niku (meat) of theverb yaku (grill) can be dropped and generalized2The parenthesis in the first row of (3) indicates a subordi-nate clause.499to something to grill; namely the action-effect rela-tion still holds between X-o yaku (grill X) and X-nikogeme-ga tsuku (X gets brown).
On the other hand,however, the argument kogeme (a burn) of the verbtsuku (get) cannot be dropped; otherwise, the rela-tion would no longer hold.One straightforward way to address this problemis to expand each cooccurrence sample to those cor-responding to different degrees of generalization andfeed them to the relation extraction model so that itsscoring function can select appropriate event pairsfrom expanded samples.
For example, cooccurrencesample (3) is expanded to those as in (4):(4) a.
( kogeme-ga tsuku ) -kurai niku-o yakua burn-NOM get -so that meat-ACC grillb.
( tsuku ) -kurai niku-o yakuget -so that meat-ACC grillc.
( kogeme-ga tsuku ) -kurai yakua burn-NOM get -so that grilld.
( tsuku ) -kurai yakuget -so that grillIn practice, in our experiments (Section 5), we re-strict the number of arguments for each event up toone to avoid the explosion of the types of infrequentcandidate relation instances.4.3 Volitionality of eventsInui et al (2003) discuss how causal rela-tions between events should be typologized forthe purpose of semantic inference and classifycausal relations basically into four types ?
Ef-fect, Means, Precondition and Cause relations?
based primarily on the volitionality of in-volved events.
For example, Effect relations holdbetween volitional actions and their resultativenon-volitional states/happenings/experiences, whileCause relations hold between only non-volitionalstates/happenings/experiences.Following this typology, we are concerned withthe volitionality of each event mention.
For ourexperiments, we manually built a lexicon of over12,000 verbs (including verbal nouns) with volition-ality labels, obtaining 8,968 volitional verbs, 3,597non-volitional and 547 ambiguous.
Volitional verbsinclude taberu (eat) and kenkyu-suru (research),while non-volitional verbs include atatamaru (getwarm), kowareru (to break-vi) and kanashimu (besad).
We discarded the ambiguous verbs in the ex-periments.4.4 Dependency-based cooccurrence patternsThe original Espresso encodes patterns simply as aword sequence because entity mentions in the rela-tions it scopes tend to cooccur locally in a singlephrase or clause.
In event relation extraction, how-ever, cooccurrence patterns of event mentions in therelations we consider (causal relations, temporal re-lations, etc.)
can be captured better as a path ona syntactic dependency tree because (i) such men-tion pairs tend to cooccur in a longer dependencypath and (ii) as discussed in Section 4.2, we wantto exclude the arguments of event mentions fromcooccurrence patterns, which would be difficult withword sequence-based representations of patterns.A Japanese sentence can be analyzed as a se-quence of base phrase (BP) chunks called bunsetsuchunks, each which typically consists of one con-tent (multi-)word followed by functional words.
Weassume each sentence of our corpus is given a de-pendency parse tree over its BP chunks.
Let us calla BP chunk containing a verb or verbal noun anevent chunk.
We create a cooccurrence sample fromany pair of event chunks that cooccur if either (a)one event chunk depends directly on the other, or(b) one event chunk depends indirectly on the othervia one intermediate chunk.
Additionally, we applythe Japanese functional expressions dictionary (Mat-suyoshi et al, 2006) to a cooccurrence pattern forgeneralization.In (5), for example, the two event chunks,taishoku-go-ni (after retirement) and hajimeru (be-gin), meet the condition (b) above and the depen-dency path designated by bold font is identified as acandidate cooccurrence pattern.
The argument PC-oof the verb hajimeru is excluded from the path.
(5) (taishoku-go-no tanoshimi)-ni PC-o hajimeruretirement-after as a hobby PC-ACC beginbegin a PC as a hobby after retirement5 Experiments5.1 SettingsFor an empirical evaluation, we used a sampleof approximately 500M sentences taken from the500Table 1: Examples of acuired cooccurrence patterns and relatio instances for the action-effect relationfreq cooccurrence patterns relation instances94477 ?verb;action?temo?verb;effect?nai(to do ?action?
though ?effect?
dose not happen)sagasu::mitsukaru (search::be found),asaru::mitsukaru (hunt::be found), purei-suru::kuria-suru (play::finish)6250 ?verb;action?takeredomo?verb;effect?nai(to do ?action?
though ?effect?
dose not happen)shashin-wo-toru::toreru (shot photograph::be shot),meiru-wo-okuru::henji-ga-kaeru (send a mail::get ananswer)1851 ?noun;action?wo-shitemo?verb;effect?nai(to do ?action?
though ?effect?
dose not happen)setsumei-suru::nattoku-suru (explain::agree), siai-suru::katsu (play::win), siai-suru::makeru (play::lose)1329 ?verb;action?yasukute?adjective;effect?
(to simply do ?action?
and ?effect?
)utau::kimochiyoi (sing::feel good),hashiru::kimochiyoi (run::feel good)4429 ?noun;action?wo-kiite?verb;effect?
(to hear ?action?
so that ?effect?
)setsumei-suru::nattoku-suru (explain::agree), setsumei-suru::rikai-dekiru (explain::can understand)Web corpus collected by Kawahara and Kuro-hashi (2006).
The sentences were dependency-parsed with Cabocha (Kudo and Matsumoto, 2002),and cooccurrence samples of event mentions wereextracted.
Event mentions with patterns whose fre-quency was less than 20 were discarded in order toreduce computational costs.
As a result, we obtained34M cooccurrence tokens with 11M types.
Notethat among those cooccurrence samples 15M tokens(44%) with 4.8M types (43%) are those with ver-bal nouns, suggesting the potential impacts of usingverbal nouns.In our experiments, we considered two of Inui etal.
(2003)?s four types of causal relations: action-effect relations (Effect in Inui et al?s terminology)and action-means relations (Means).
An action-effect relation holds between events x and y if andonly if non-volitional event y is likely to happen aseither a direct or indirect effect of volitional actionx.
For example, the action X-ga undou-suru (X exer-cises) and the event X-ga ase-o kaku (X sweats) areconsidered to be in this type of relation.
A action-means relation holds between events x and y if andonly if volitional action y is likely to be done as apart/means of volitional action x.
For example, ifcase a event-pair is X-ga hashiru (X runs) is consid-ered as a typical action that is often done as a part ofthe action X-ga undou-suru (X exercises).Note that in these experiments we do not differ-entiate between relations with the same subject andthose with a different subject.
However we plan toconduct further experiments in the future that makeuse of this distinction.In addition, we have collected action-effect rela-tion instances for a baseline measure.
The baselineconsists of instances that cooccur with eleven pat-terns that indicate action-effect relation.
The dif-ference between the extended Espresso and baselineis caused by the low number and constant scores ofpatterns.5.2 ResultsWe ran the extended Espresso algorithm startingwith 971 positive and 1069 negative seed relationinstances for action-effect relation and 860 positiveand 74 negative seed relations for action-means re-lation.
As a result, we obtained 34,993 cooccurrencepatterns with 173,806 relation instances for theaction-effect relation and 23,281 coocurrence rela-tions with 237,476 relation instances for the action-means relation after 20 iterations of pattern rank-ing/selection and instance ranking/selection.
Thethreshold parameters for selecting patterns and in-stances were decided in a preliminary trial.
Someof the acquired patterns and instances for the action-effect relation are shown in Table 1.5.2.1 PrecisionTo estimate precision, 100 relation instances wererandomly sampled from each of four sections of theranks of the acquired instances for each of the tworelations (1?500, 501?1500, 1501?3500 and 3500?7500), and the correctness of each sampled instancewas judged by two graduate students (i.e.
800 rela-tion instances in total were judged).Note that in these experiments we asked the asses-sors to both (a) the degree of the likeliness that theeffect/means takes place and (b) which argumentsare shared between the two events.
For example,while nomu (drink) does not necessarily result in501futsukayoi-ni naru (have a hangover), the assessorsjudged this pair correct because one can at least saythat the latter sometimes happens as a result of theformer.
For criterion (b), as shown in Table 1, therelation instances judged correct include both the X-ga VP1::X-ga VP2 type (i.e.
two subjects are shared)and the X-o VP1::X-ga VP2 type (the object of theformer and the subject of the latter are shared).
Theissue of how to control patterns of argument sharingis left for future work.The precision for the assessed samples are shownin Figures 1 to 3.
?2 judges?
means that an instanceis acceptable to both judges.
?1 judges?
means thatit is an acceptable instance to at least one of the twojudges.
?strict?
indicates correct instance relationswhile ?lenient?3 indicates correct instance relations?
when a judge appends the right cases.As a result of this strictness in judgement, theinter-assessor agreement turned out to be poor.
Thekappa statistics was 0.53 for the action-effect rela-tions, 0.49 for the action-effect relations (=baseline)and 0.55 for action-means relations.The figures show that both types of relations wereacquired with reasonable precision not only for thehigher-ranked instances but also for lower-rankedinstances.
It may seem strange that the precisionof the lower-ranked action-means instances is some-times even better than the higher-ranked ones, whichmay mean that the scoring function given in Section3 did not work properly.
While further investiga-tion is clearly needed, it should also be noted thathigher-ranked instances tended to be more specificthan lower-ranked ones.5.2.2 Effects of seed numberWe reran the extended Espresso algorithm for theaction-effect relation, starting with 500 positive and500 negative seed relation instances.
The preci-sion is shown in Figure 44.
This precision is fairlylower than that of action-effect relations with allseed instances.
Additionally, the number of seed in-stances affects the precision of both higher-rankedand lower-ranked instances.
This result indicatesthat while the proposed algorithm is designed towork with a small seed set, in reality its performance3If an instance is judged as ?strict?
by one assessor and ?le-nient?
by the other, then the instance is assessed as ?lenient?.4It was only judged by one assessor.severely depends on the number of seeds.5.2.3 Effects of using verbal nounsWe also examine the effect of using verbal nouns.Of the 500 highest scored patterns for the action-effect relation, 128 patterns include verbal nounslots, and for action-means, 495 patterns.
Hence,the presence of verbal nouns greatly effects someacquired instances.
Additionally, to see the influ-ence of frequency, of the 500 high frequent patternsselected from the 2000 highest scored patterns foraction-effect relation, 177 include verbal noun slots,and for action-means, 407 patterns.
This result pro-vides further evidence that the inclusion of verbalnouns has a positive effect in this task.5.2.4 Argument selectionAccording to our further investigation on argu-ment selection, 49 instances (12%) of the correctaction-effect relation instances that are judged cor-rect have a specific argument in at least one event,and all of them would be judged incorrect (i.e.
over-generalized) if they did not have those arguments(Recall the example of kogeme-ga tsuku (get brown)in Section 4.2).
This figure indicates that our methodfor argument selection works to a reasonable degree.However, clearly there is still much room for im-provement.
According to our investigation, up to26% of the instances that are judged incorrect couldbe saved if appropriate arguments were selected.
Forexample, X-ga taberu (X eats) and X-ga shinu (Xdies) would constitute an action-effect relation if theformer event took such an argument as dokukinoko-o (toadstool-ACC).
The overall precision could beboosted if an effective method for argument selec-tion method were devised.6 Conclusion and future workIn this paper, we have addressed the issue of howto learn lexico-syntactic patterns useful for acquir-ing event relation knowledge from a large corpus,and proposed several extensions to a state-of-the-artmethod originally designed for entity relation ex-traction, reporting on the present results of our em-pirical evaluation.
The results have shown that (a)there are indeed specific cooccurrence patterns use-ful for event relation acquisition, (b) the use of cooc-currence samples involving verbal nouns has pos-50200.20.40.60.810  1000  2000  3000  4000  5000  6000  7000  8000precision[%]rankstrict (2 judged)lenient (2 judged)strict (1 judged)lenient (1 judged)Figure 1: action-effect00.20.40.60.810  1000  2000  3000  4000  5000  6000  7000  8000precision[%]rankstrict (2 judged)lenient (2 judged)strict (1 judged)lenient (1 judged)Figure 2: action-means00.20.40.60.810  1000  2000  3000  4000  5000  6000  7000  8000precision[%]rankstrict (2 judged)lenient (2 judged)strict (1 judged)lenient (1 judged)Figure 3: action-effect (baseline)00.20.40.60.810  1000  2000  3000  4000  5000  6000  7000  8000precision[%]ranksystem (strict)system (lenient)baseline (strict)baseline (lenient)half (strict)half (lenient)Figure 4: action-effect (half seed)503itive impacts on both recall and precision, and (c)over five thousand relation instances are acquiredfrom the 500M-sentence Web corpus with a preci-sion of about 66% for action-effect relations.Clearly, there is still much room for explorationand improvement.
First of all, more comprehensiveevaluations need to be done.
For example, the ac-quired relations should be evaluated in terms of re-call and usefulness.
A deep error analysis is alsoneeded.
Second, the experiments have revealed thatone major problem to challenge is how to optimizeargument selection.
We are seeking a way to incor-porate a probabilistic model of predicate-argumentcooccurrences into the ranking function for relationinstances.
Related to this issue, it is also crucialto devise a method for controlling argument shar-ing patterns.
One possible approach is to employstate-of-the-art techniques for coreference and zero-anaphora resolution (Iida et al, 2006; Komachi etal., 2007, etc.)
in preprocessing cooccurrence sam-ples.ReferencesTimothy Chklovski and Patrick Pantel.
2005.
Globalpath-based refinement of noisy graphs applied to verbsemantics.
In Proceedings of Joint Conference on Nat-ural Language Processing (IJCNLP-05), pages 792?803.Ryu Iida, Kentaro Inui, and Yuji Matsumoto.
2006.
Ex-ploiting syntactic patterns as clues in zero-anaphoraresolution.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the 44thannual meeting of the ACL, pages 625?632.Takashi Inui, Kentaro Inui, and Yuji Matsumoto.
2003.What kinds and amounts of causal knowledge can beacquired from text by using connective markers asclues?
In Proceedings of the 6th International Con-ference on Discovery Science, pages 180?193.
An ex-tended version: Takashi Inui, Kentaro Inui, and YujiMatsumoto (2005).
Acquiring causal knowledge fromtext using the connective marker tame.
ACM Trans-actions on Asian Language Information Processing(TALIP), 4(4):435?474.Daisuke Kawahara and Sadao Kurohashi.
2006.
A fully-lexicalized probabilistic model for japanese syntacticand case structure analysis.
In Proceedings of the Hu-man Language Technology Conference of the NAACL,Main Conference, pages 176?183.Mamoru Komachi, Ryu Iida, Kentaro Inui, and Yuji Mat-sumoto.
2007.
Learning based argument structureanalysis of event-nouns in japanese.
In Proceedingsof the Conference of the Pacific Association for Com-putational Linguistics (PACLING), pages 120?128.Taku Kudo and Yuji Matsumoto.
2002.
Japanese depen-dency analysis using cascaded chunking.
In CoNLL2002: Proceedings of the 6th Conference on Natu-ral Language Learning 2002 (COLING 2002 Post-Conference Workshops), pages 63?69.Dekang Lin and Patrick Pantel.
2001.
DIRT - discov-ery of inference rules from text.
In Proceedings ofACM SIGKDD Conference on Knowledge Discoveryand Data Mining 2001, pages 323?328.Suguru Matsuyoshi, Satoshi Sato, and Takehito Utsuro.2006.
Compilation of a dictionary of japanese func-tional expressions with hierarchical organization.
InProceedings of the 21st International Conference onComputer Processing of Oriental Languages, pages395?402.Patric Pantel and Marco Pennacchiotti.
2006.
Espresso:Leveraging generic patterns for automatically harvest-ing semantic relations.
In the 21st International Con-ference on Computational Linguistics and 44th AnnualMeeting of the ACL, pages 113?120.Viktor Pekar.
2006.
Acquisition of verb entailment fromtext.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Main Conference,pages 49?56.Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of the 21st International Conferenceon Computational Linguistics and 40th Annual Meet-ing of the Association for Computational Linguistics,pages 41?47.Kentaro Torisawa.
2006.
Acquiring inference rules withtemporal constraints by using japanese coordinatedsentences and noun-verb co-occurrences.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Main Conference, pages 57?64.Fabio Massimo Zanzotto, Marco Pennacchiotti, andMaria Teresa Pazienza.
2006.
Discovering asym-metric entailment relations between verbs using selec-tional preferences.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics and44th Annual Meeting of the Association for Computa-tional Linguistics, pages 849?856.504
