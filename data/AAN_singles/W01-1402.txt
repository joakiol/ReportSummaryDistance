Overcoming the customization bottleneck using example-based MTStephen D. Richardson, William B. Dolan, Arul Menezes, Monica Corston-Oliver?Microsoft Research ?Butler Hill GroupOne Microsoft Way 4610 Wallingford Ave. N.Redmond, WA 98052 Seattle WA 98103{steveri, billdol, arulm}@microsoft.com moco@butlerhill.comAbstractWe describe MSR-MT, a large-scalehybrid machine translation systemunder development for severallanguage pairs.
This system?s ability toacquire its primary translationknowledge automatically by parsing abilingual corpus of hundreds ofthousands of sentence pairs andaligning resulting logical formsdemonstrates true promise forovercoming the so-called MTcustomization bottleneck.
Trained onEnglish and Spanish technical prose, ablind evaluation shows that MSR-MT?sintegration of rule-based parsers,example based processing, andstatistical techniques producestranslations whose quality exceeds thatof uncustomized commercial MTsystems in this domain.1 IntroductionCommercially available machine translation(MT) systems have long been limited in theircost effectiveness and overall utility by the needfor domain customization.
Such customizationtypically includes identifying relevantterminology (esp.
multi-word collocations),entering this terminology into system lexicons,and making additional tweaks to handleformatting and even some syntacticidiosyncrasies.
One of the goals of data-drivenMT research has been to overcome thiscustomization bottleneck through automated orsemi-automated extraction of translationknowledge from bilingual corpora.To address this bottleneck, a variety ofexample based machine translation (EBMT)systems have been created and described in theliterature.
Some of these employ parsers toproduce dependency structures for the sentencepairs in aligned bilingual corpora, which arethen aligned to obtain transfer rules or examples(Meyers et al 2000; Watanabe et al 2000).Other systems extract and use examples that arerepresented as linear patterns of varyingcomplexity (Brown 1999; Watanabe and Takeda1998; Turcato et al 1999).For some EBMT systems, substantialcollections of examples are also manuallycrafted or at least reviewed for correctness afterbeing identified automatically (Watanabe et al2000; Brown 1999; Franz et al 2000).
Theefforts that report accuracy results for fullyautomatic example extraction (Meyers et al2000; Watanabe et al 2000) do so for verymodest amounts of training data (a few thousandsentence pairs).
Previous work in this area thusraises the possibility that manual review orcrafting is required to obtain example bases ofsufficient coverage and accuracy to be trulyuseful.Other variations of EBMT systems arehybrids that integrate an EBMT component asone of multiple sources of transfer knowledge(in addition to other transfer rule or knowledgebased components) used during translation(Frederking et al 1994; Takeda et al 1992).To our knowledge, commercial quality MThas so far been achieved only through years ofeffort in creating hand-coded transfer rules.Systems whose primary source of translationknowledge comes from an automatically createdexample base have not been shown capable ofmatching or exceeding the quality ofcommercial systems.This paper reports on MSR-MT, an MTsystem that attempts to break the customizationbottleneck by exploiting example-based (andsome statistical) techniques to automaticallyacquire its primary translation knowledge from abilingual corpus of several million words.
Thesystem leverages the linguistic generality ofexisting rule-based parsers to enable broadcoverage and to overcome some of thelimitations on locality of context characteristicof data-driven approaches.
The ability of MSR-MT to adapt automatically to a particulardomain, and to produce reasonable translationsfor that domain, is validated through a blindassessment by human evaluators.
The quality ofMSR-MT?s output in this one domain is shownto exceed the output quality of two highly rated(though not domain-customized) commerciallyavailable MT systems.We believe that this demonstration is the firstin the literature to show that automatic trainingmethods can produce a commercially viablelevel of translation quality.2 MSR-MTMSR-MT is a data-driven hybrid MT system,combining rule-based analysis and generationcomponents with example-based transfer.
Theautomatic alignment procedure used to createthe example base relies on the same parseremployed during analysis and also makes use ofits own small set of rules for determiningpermissible alignments.
Moderately sizedbilingual dictionaries, containing only wordpairs and their parts of speech, providetranslation candidates for the alignmentprocedure and are also used as a backup sourceof translations during transfer.
Statisticaltechniques supply additional translation paircandidates for alignment and identify certainmulti-word terms for parsing and transfer.The robust, broad-coverage parsers used byMSR-MT were created originally formonolingual applications and have been used incommercial grammar checkers.1 These parsersproduce a logical form (LF) representation thatis compatible across multiple languages (seesection 3 below).
Parsers now exist for sevenlanguages (English, French, German, Spanish,Chinese, Japanese, and Korean), and activedevelopment continues to improve theiraccuracy and coverage.1 Parsers for English, Spanish, French, and Germanprovide linguistic analyses for the grammar checkerin Microsoft Word.Figure 1.
MSR-MT architecture.Generation components are currently beingdeveloped for English, Spanish, Chinese, andJapanese.
Given the automated learningtechniques used to create MSR-MT transfercomponents, it should theoretically be possible,provided with appropriate aligned bilingualcorpora, to create MT systems for any languagepair for which we have the necessary parsingand generation components.
In practice, wehave thus far created systems that translate intoEnglish from all other languages and thattranslate from English to Spanish, Chinese, andJapanese.
We have experimented onlypreliminarily with Korean and Chinese toJapanese.Results from our Spanish-English andEnglish-Spanish systems are reported at the endof this paper.
The bilingual corpus used toproduce these systems comes from Microsoftmanuals and help text.
The sentence alignmentof this corpus is the result of using a commercialtranslation memory (TM) tool during thetranslation process.The architecture of MSR-MT is presented inFigure 1.
During the training phase, source andtarget sentences from the aligned bilingualcorpus are parsed to produce corresponding LFs.The normalized word forms resulting fromparsing are also fed to a statistical wordassociation learner (described in section 4.1),which outputs learned single word translationpairs as well as a special class of multi-wordpairs.
The LFs are then aligned with the aid oftranslations from a bilingual dictionary and thelearned single word pairs (section 4.2).
Transfermappings that result from LF alignment, in theform of linked source and target LF segments,are stored in a special repository known asMindNet (section 4.3).
Additionally, the learnedmulti-word pairs are added to the bilingualdictionary for possible backup use duringtranslation and to the main parsing lexicon toimprove parse quality in certain cases.At runtime, MSR-MT?s analysis parsessource sentences with the same parser used forsource text during the training phase (section5.1).
The resulting LFs then undergo a processknown as MindMeld, which matches themagainst the LF transfer mappings stored inMindNet (section 5.2).
MindMeld also linkssegments of source LFs with correspondingtarget LF segments stored in MindNet.
Thesetarget LF segments are stitched together into asingle target LF during transfer, and anytranslations for words or phrases not foundduring MindMeld are searched for in theupdated bilingual dictionary and inserted in thetarget LF (section 5.3).
Generation receives thetarget LF as input, from which it produces atarget sentence (section 5.4).3 Logical formMSR-MT?s broad-coverage parsers produceconventional phrase structure analysesaugmented with grammatical relations (Heidornet al 2000).
Syntactic analyses undergo furtherprocessing in order to derive logical forms(LFs), which are graph structures that describelabeled dependencies among content words inthe original input.
LFs normalize certainsyntactic alternations (e.g.
active/passive) andresolve both intrasentential anaphora and long-distance dependencies.MT has proven to be an excellent applicationfor driving the development of our LFrepresentation.
The code that builds LFs fromsyntactic analyses is shared across all seven ofthe languages under development.
This sharedarchitecture greatly simplifies the task ofaligning LF segments (section 4.2) fromdifferent languages, since superficially distinctconstructions in two languages frequentlycollapse onto similar or identical LFrepresentations.
Even when two alignedsentences produce divergent LFs, the alignmentand generation components can count on aconsistent interpretation of the representationalmachinery used to build the two.
Thus themeaning of the relation Topic, for instance, isconsistent across all seven languages, althoughits surface realizations in the various languagesvary dramatically.4 Training MSR-MTThis section describes the two primarymechanisms used by MSR-MT to automaticallyextract translation mappings from parallelcorpora and the repository in which they arestored.4.1 Statistical learning of single word-and multi-word associationsThe software domain that has been ourprimary research focus contains many wordsand phrases that are not included in our general-domain lexicons.
Identifying translationcorrespondences between these unknown wordsand phrases across an aligned dataset canprovide crucial lexical anchors for the alignmentalgorithm described in section 4.2.In order to identify these associations, sourceand target text are first parsed, and normalizedword forms (lemmas) are extracted.
In themulti-word case, English ?captoid?
processing isexploited to identify sequences of related,capitalized words.
Both single word and multi-word associations are iteratively hypothesizedand scored by the algorithm under certainconstraints until a reliable set of each isobtained.Over the English/Spanish bilingual corpusused for the present work, 9,563 single word and4,884 multi-word associations not alreadyknown to our system were identified using thismethod.Moore (2001) describes this technique indetail, while Pinkham & Corston-Oliver (2001)describes its integration with MSR-MT andinvestigates its effect on translation quality.4.2 Logical form alignmentAs described in section 2, MSR-MT acquirestransfer mappings by aligning pairs of LFsobtained from parsing sentence pairs in abilingual corpus.
The LF alignment algorithmfirst establishes tentative lexicalcorrespondences between nodes in the sourceand target LFs using translation pairs from abilingual lexicon.
Our English/Spanish lexiconpresently contains 88,500 translation pairs,which are then augmented with single wordtranslations acquired using the statistical methoddescribed in section 4.1.
After establishingpossible correspondences, the algorithm uses asmall set of alignment grammar rules to alignLF nodes according to both lexical andstructural considerations and to create LFtransfer mappings.
The final step is to filter themappings based on the frequency of their sourceand target sides.
Menezes & Richardson (2001)provides further details and an evaluation of theLF alignment algorithm.The English/Spanish bilingual trainingcorpus, consisting largely of Microsoft manualsand help text, averaged 14.1 words per Englishsentence.
A 2.5 million word sample of Englishdata contained almost 40K unique word forms.The data was arbitrarily split in two for use inour Spanish-English and English-Spanishsystems.
The first sub-corpus contains over208,000 sentence pairs and the second over183,000 sentence pairs.
Only pairs for whichboth Spanish and English parsers producecomplete, spanning parses and LFs are currentlyused for alignment.
Table 1 provides thenumber of pairs used and the number of transfermappings extracted and used in each case.Spanish-EnglishEnglish-SpanishTotal sentence pairs 208,730 183,110Sentence pairs used 161,606 138,280Transfer mappingsextracted1,208,828 1,001,078Unique, filteredmappings used58,314 47,136Table 1.
English/Spanish transfer mappings fromLF alignment4.3 MindNetThe repository into which transfer mappingsfrom LF alignment are stored is known asMindNet.
Richardson et al (1998) describeshow MindNet began as a lexical knowledgebase containing LF-like structures that wereproduced automatically from the definitions andexample sentences in machine-readabledictionaries.
Later, MindNet was generalized,becoming an architecture for a class ofrepositories that can store and access LFsproduced for a variety of expository texts,including but not limited to dictionaries,encyclopedias, and technical manuals.For MSR-MT, MindNet serves as theoptimal example base, specifically designed tostore and retrieve the linked source and targetLF segments comprising the transfer mappingsextracted during LF alignment.
As part of dailyregression testing for MSR-MT, all the sentencepairs in the combined English/Spanish corpusare parsed, the resulting spanning LFs arealigned, and a separate MindNet for each of thetwo directed language pairs is built from the LFtransfer mappings obtained.
These MindNetsare about 7MB each in size and take roughly 6.5hours each to create on a 550 Mhz PC.5 Running MSR-MTMSR-MT translates sentences in four processingsteps, which were illustrated in Figure 1 andoutlined in section 2 above.
These steps aredetailed using a simple example in the followingsections.5.1 AnalysisThe input source sentence is parsed with thesame parser used on source text during MSR-MT?s training.
The parser produces an LF forthe sentence, as described in section 3.
For theexample LF in Figure 2, the Spanish inputsentence is Haga clic en el bot?n de opci?n.
InEnglish, this is literally Make click in the buttonof option.
In fluent, translated English, it isClick the option button.Figure 2.
LF produced for Haga clic en el bot?nde opci?n.5.2 MindMeldThe source LF produced by analysis is nextmatched by the MindMeld process to the sourceLF segments that are part of the transfermappings stored in MindNet.
Multiple transfermappings may match portions of the source LF.MindMeld attempts to find the best set ofmatching transfer mappings by first searchingfor LF segments in MindNet that have matchinglemmas, parts of speech, and other featureinformation.
Larger (more specific) mappingsare preferred to smaller (more general)mappings.
In other words, transfers with contextwill be matched preferentially, but the systemwill fall back to the smaller transfers when nomatching context is found.
Among mappings ofequal size, MindMeld prefers higher-frequencymappings.
Mappings are also allowed to matchoverlapping portions of the source LF so long asthey do not conflict in any way.After an optimal set of matching transfermappings is found, MindMeld creates Links onnodes in the source LF to copies of thecorresponding target LF segments retrievedfrom the mappings.
Figure 3 shows the sourceLF for the example sentence with additionalLinks to target LF segments.
Note that Linksfor multi-word mappings are represented bylinking the root nodes (e.g., hacer and click) ofthe corresponding segments, then linking anasterisk (*) to the other source nodesparticipating in the multi-word mapping (e.g.,usted and clic).
Sublinks betweencorresponding individual source and targetnodes of such a mapping (not shown in thefigure) are also created for use during transfer.Figure 3.
Linked LF for Haga clic en el bot?n deopci?n.5.3 TransferThe responsibility of transfer is to take a linkedLF from MindMeld and create a target LF thatwill be the basis for the target translation.
Thisis accomplished through a top down traversal ofthe linked LF in which the target LF segmentspointed to by Links on the source LF nodes arestitched together.
When stitching together LFsegments from possibly complex multi-wordmappings, the sublinks set by MindMeldbetween individual nodes are used to determinecorrect attachment points for modifiers, etc.Default attachment points are used if needed.Also, a very small set of simple, general, hand-coded transfer rules (currently four for Englishto/from Spanish) may apply to fill current (andwe hope, temporary) gaps in learned transfermappings.In cases where no applicable transfermapping was found during MindMeld, thenodes in the source LF and their relations aresimply copied into the target LF.
Default (i.e.,most commonly occurring) single wordtranslations may still be found in the MindNetfor these nodes and inserted in the target LF, butif not, translations are obtained, if possible, fromthe same bilingual dictionary used during LFalignment.Figure 4 shows the target LF created bytransfer from the linked LF shown in Figure 3.Figure 4.
Target LF for Click the option button.5.4 GenerationA rule-based generation component mapsfrom the target LF to the target string (Aikawaet al 2001).
The generation components for thetarget languages currently handled by MSR-MTare application-independent, having beendesigned to apply to a range of tasks, includingquestion answering, grammar checking, andtranslation.
In its application to translation,generation has no information about the sourcelanguage for a given input LF, workingexclusively with the information passed to it bythe transfer component.
It uses this information,in conjunction with a monolingual (targetlanguage) dictionary to produce its output.
Onegeneric generation component is thus sufficientfor each language.In some cases, transfer produces anunmistakably ?non-native?
target LF.
In order tocorrect some of the worst of these anomalies, asmall set of source-language independent rulesis applied prior to generation.
The need for suchrules reflects deficiencies in our current data-driven learning techniques during transfer.6 Evaluating MSR-MTIn evaluating progress, we have found noeffective alternative to the most obvioussolution: periodic, blind human evaluationsfocused on translations of single sentences.
Thehuman raters used for these evaluations work foran independent agency and played nodevelopment role building the systems they test.Each language pair under active development isperiodically subjected to the evaluation processdescribed in this section.6.1 Evaluation MethodologyFor each evaluation, five to seven evaluatorsare asked to evaluate the same set of 200 to 250blind test sentences.
For each sentence, ratersare presented with a reference sentence in thetarget language, which is a human translation ofthe corresponding source sentence.
In order tomaintain consistency among raters who mayhave different levels of fluency in the sourcelanguage, raters are not shown the sourcesentence.
Instead, they are presented with twomachine-generated target translations presentedin random order: one translation by the systemto be evaluated (the experimental system), andanother translation by a comparison system (thecontrol system).
The order of presentation ofsentences is also randomized for each rater inorder to eliminate any ordering effect.Raters are asked to make a three-way choice.For each sentence, raters may choose one of thetwo automatically translated sentences as thebetter translation of the (unseen) sourcesentence, assuming that the reference sentencerepresents a perfect translation, or, they mayindicate that neither of the two is better.
Ratersare instructed to use their best judgment aboutthe relative importance of fluency/style andaccuracy/content preservation.
We chose to usethis simple three-way scale in order to avoidmaking any a priori judgments about therelative importance of these parameters forsubjective judgments of quality.
The three-wayscale also allows sentences to be rated on thesame scale, regardless of whether thedifferences between output from system 1 andsystem 2 are substantial or negligible.The scoring system is similarly simple; eachjudgment by a rater is represented as 1 (sentencefrom experimental system judged better), 0(neither sentence judged better), or -1 (sentencefrom control system judged better).
For eachsentence, the score is the mean of all raters?judgments; for each comparison, the score is themean of the scores of all sentences.6.2 Evaluation resultsAlthough work on MSR-MT encompasses anumber of language pairs, we focus here on theevaluation of just two, Spanish-English andEnglish-Spanish.
Training data was heldconstant for each of these evaluations.6.2.1 Spanish-English over timeSpanish-EnglishsystemsMean preferencescore (7 raters)SamplesizeMSR-MT 9/00vs.MSR-MT 12/000.30 ?
0.09(at 0.95)200sentencesMSR-MT 12/00vs.MSR-MT 4/010.28 ?
0.07(at 0.99)250sentencesThis table summarizes two evaluationstracking progress in MSR-MT?s Spanish-English (SE) translation quality over a sevenmonth development period.
The first evaluation,with seven raters, compared a September 2000version of the system to a December 2000version.
The second evaluation, carried out bysix raters, examined progress betweenDecember 2000 and April 2001.A score of -1 would mean that ratersuniformly preferred the control system, while ascore of 1 would indicate that all raters preferredthe comparison system for all sentences.
In eachof these evaluations, all raters significantlypreferred the comparison, or newer, version ofMSR-MT, as reflected in the mean preferencescores of 0.30 and 0.28.
These numbers confirmthat the system made considerable progress overa relatively short time span.6.2.2 Spanish-English vs. alternative systemSpanish-EnglishsystemsMean preferencescore (7 raters)SamplesizeMSR-MT 9/00 vs.Babelfish-0.23 ?
0.12(at 0.95)200sentencesMSR-MT 12/00vs.
Babelfish0.11 ?
0.10(at 0.95)200sentencesMSR-MT 4/01 vs.Babelfish0.32 ?
0.11(at .99)250sentencesThis table summarizes our comparison ofMSR-MT?s Spanish-English (SE) output to theoutput of Babelfish (http://world.altavista.com/).Three separate evaluations were performed, inorder to track MSR-MT?s progress over sevenmonths.
The first two evaluations involvedseven raters, while the third involved six.The shift in the mean preference score from-0.23 to 0.32 shows clear progress againstBabelfish; by the second evaluation, raters veryslightly preferred MSR-MT in this domain.
ByApril, all six raters strongly preferred MSR-MT.6.2.3 English-Spanish vs. alternative systemEnglish-SpanishsystemsMean preferencescore (5 raters)SamplesizeMSR-MT 2/01vs.
L&H0.078 ?
0.13(at 0.95)250sentencesMSR-MT 4/01vs.
L&H0.19 ?
0.14(at 0.99)250sentencesThe evaluations summarized in this tablecompared February and April 2001 versions ofMSR-MT?s English-Spanish (ES) output to theoutput of the Lernout & Hauspie (L&H) ESsystem (http://officeupdate.lhsl.com/) for 250source sentences.
Five raters participated in thefirst evaluation, and six in the second.The mean preference scores show that byApril, MSR-MT was strongly preferred overL&H.
Interestingly, though, one rater whoparticipated in both evaluations maintained aslight but systematic preference for L&H?stranslations.
Determining which aspects of thetranslations might have caused this rater tobehave differently from the others is a topic forfuture investigation.6.3 DiscussionThese results document steady progress inthe quality of MSR-MT?s output over arelatively short time.
By April 2001, both the SEand ES versions of the system had surpassedBabelfish in translation quality for this domain.While these versions of MSR-MT are the mostfully developed, the other language pairs underdevelopment are also progressing rapidly.In interpreting our results, it is important tokeep in mind that MSR-MT has beencustomized to the test domain, while theBabelfish and Lernout & Hauspie systems havenot.2 This certainly affects our results, and2Babelfish was chosen for these comparisons onlyafter we experimentally compared its output to thatof the related Systran system augmented with itscomputer domain dictionary.
Surprisingly, themeans that our comparisons have a certainasymmetry.
As our work progresses, we hope toevaluate MSR-MT against a quality bar that isperhaps more meaningful: the output of acommercial system that has been hand-customized for a specific domain.The asymmetrical nature of our comparisoncuts both ways, however.
Customizationproduces better translations, and a system thatcan be automatically customized has an inherentadvantage over one that requires laboriousmanual customization.
Comparing anautomatically-customized version of MSR-MTto a commercial system which has undergoneyears of hand-customization will represent acomparison that is at least as asymmetrical asthose we have presented here.We have another, more concrete, purpose inregularly evaluating our system relative to theoutput of systems like Babelfish and L&H: thesecommercial systems serve as (nearly) staticbenchmarks that allow us to track our ownprogress without reference to absolute quality.7 Conclusions and Future WorkThis paper has described MSR-MT, anEBMT system that produces MT output whosequality in a specific domain exceeds that ofcommercial MT systems, thus attacking head-onthe customization bottleneck.
This workdemonstrates that automatic data-drivenmethods can provide commercial-quality MT.In future work we hope to demonstrate thatMSR-MT can be rapidly adapted to verydifferent semantic domains, and that it cancompete in translation quality even withcommercial systems that have been hand-customized to a particular domain.AcknowledgementsWe would like to acknowledge the efforts ofthe MSR NLP group in carrying out this work.ReferencesAikawa, T., M. Melero, L. Schwartz, and A. Wu2001 ?Multilingual natural languagegeneration,?
Proceedings of 8th EuropeanWorkshop on Natural Language Generation,Toulouse.generic SE Babelfish engine produced slightly bettertranslations of our technical data.Brown, R. 1999.
?Adding linguistic knowledgeto a lexical example-based translationsystem,?
Proceedings of TMI 99.Franz, A., K. Horiguchi, L. Duan, D. Ecker, E.Koontz, and K. Uchida 2000.
?An integratedarchitecture for example-based machinetranslation,?
Proceedings of COLING2000.Frederking, R., S. Nirenburg, D. Farwell, S.Helmreich, E. Hovy, K. Knight, S. Beale, C.Domashnev, D. Attardo, D. Grannes, and R.Brown 1994.
?Integrating translations frommultiple sources within the Pangloss MarkIII machine translation system,?
Proceedingsof AMTA94.Heidorn, G., K. Jensen, S. Richardson, and A.Viesse 2000.
In R. Dale, H. Moisl and H.Somers (eds) Handbook of Natural LanguageProcessing.
Marcel Dekker Inc.Meyers, A., M. Kosaka, and R. Grishman.
2000.?Chart-based transfer rule application inmachine translation,?
Proceedings ofCOLING98.Menezes, A. and S. Richardson 2001.
?A best-first alignment algorithm for automaticextraction of transfer mappings frombilingual corpora,?
Proceedings of theWorkshop on Data-Driven MachineTranslation, ACL 2001.Moore, R. 2001 ?Towards a Simple andAccurate Statistical Approach to LearningTranslation Relationships Among Words,?Proceedings of the Workshop on Data-Driven Machine Translation, ACL 2001.Pinkham, J and M. Corston-Oliver 2001?Adding Domain Specificity to an MTsystem,?
Proceedings of the Workshop onData-Driven Machine Translation, ACL2001.Richardson, S. D., W. Dolan, and L.Vanderwende 1998.
?MindNet: Acquiringand Structuring Semantic Information fromText,?
Proceedings of COLING-ACL ?98,Montreal.Takeda, K., N. Uramoto, T. Nasukawa, and T.Tsutsumi 1992.
?Shalt 2?a symmetricmachine translation system with conceptualtransfer,?
Proceedings of COLING92.Turcato, D., P. McFetridge, F. Popowich, and J.Toole 1999.
?A unified example-based andlexicalist approach to machine translation,?Proceedings of TMI 99.Watanabe, W. Kurohashi, S. and E. Aramaki2000.
?Finding structural correspondencesfrom bilingual parsed corpus for corpus-based translation,?
Proceedings ofCOLING2000.Watanabe, H. and K. Takeda 1998.
?A pattern-based machine translation system extendedby example-based processing,?
Proceedingsof COLING98.
