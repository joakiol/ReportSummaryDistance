Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 424?434,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsSurface Realisation from Knowledge-BasesBikash GyawaliUniversite?
de Lorraine, LORIAVillers-le`s-Nancy, F-54600, Francebikash.gyawali@loria.frClaire GardentCNRS, LORIA, UMR 7503Vandoeuvre-le`s-Nancy, F-54500, Franceclaire.gardent@loria.frAbstractWe present a simple, data-driven approachto generation from knowledge bases (KB).A key feature of this approach is thatgrammar induction is driven by the ex-tended domain of locality principle ofTAG (Tree Adjoining Grammar); and thatit takes into account both syntactic andsemantic information.
The resulting ex-tracted TAG includes a unification basedsemantics and can be used by an existingsurface realiser to generate sentences fromKB data.
Experimental evaluation on theKBGen data shows that our model outper-forms a data-driven generate-and-rank ap-proach based on an automatically inducedprobabilistic grammar; and is comparablewith a handcrafted symbolic approach.1 IntroductionIn this paper we present a grammar based ap-proach for generating from knowledge bases (KB)which is linguistically principled and conceptuallysimple.
A key feature of this approach is thatgrammar induction is driven by the extended do-main of locality principle of TAG (Tree AdjoiningGrammar) and takes into account both syntacticand semantic information.
The resulting extractedTAGs include a unification based semantics andcan be used by an existing surface realiser to gen-erate sentences from KB data.To evaluate our approach, we use the bench-mark provided by the KBGen challenge (Baniket al, 2012; Banik et al, 2013), a challengedesigned to evaluate generation from knowledgebases; where the input is a KB subset; and wherethe expected output is a complex sentence convey-ing the meaning represented by the input.
Whencompared with two other systems having takenpart in the KBGen challenge, our system outper-forms a data-driven, generate-and-rank approachbased on an automatically induced probabilis-tic grammar; and produces results comparable tothose obtained by a symbolic, rule based approach.Most importantly, we obtain these results using ageneral purpose approach that we believe is sim-pler and more transparent than current state of theart surface realisation systems generating from KBor DB data.2 Related WorkOur work is related to work on concept to text gen-eration.Earlier work on concept to text generationmainly focuses on generation from logical formsusing rule-based methods.
(Wang, 1980) useshand-written rules to generate sentences from anextended predicate logic formalism; (Shieber etal., 1990) introduces a head-driven algorithm forgenerating from logical forms; (Kay, 1996) de-fines a chart based algorithm which enhances effi-ciency by minimising the number of semanticallyincomplete phrases being built; and (Shemtov,1996) presents an extension of the chart based gen-eration algorithm presented in (Kay, 1996) whichsupports the generation of multiple paraphrasesfrom underspecified semantic input.
In all theseapproaches, grammar and lexicon are developedmanually and it is assumed that the lexicon as-sociates semantic sub-formulae with natural lan-guage expressions.
Our approach is similar tothese approaches in that it assumes a grammar en-coding a compositional semantics.
It differs fromthem however in that, in our approach, grammarand lexicon are automatically acquired from thedata.With the development of the semantic web andthe proliferation of knowledge bases, generationfrom knowledge bases has attracted increased in-terest and so called ontology verbalisers havebeen proposed which support the generation oftext from (parts of) knowledge bases.
One main424strand of work maps each axiom in the knowledgebase to a clause.
Thus the OWL verbaliser inte-grated in the Prote?ge?
tool (Kaljurand and Fuchs,2007) provides a verbalisation of every axiompresent in the ontology under consideration and(Wilcock, 2003) describes an ontology verbaliserusing XML-based generation.
As discussed in(Power and Third, 2010), one important limita-tion of these approaches is that they assume asimple deterministic mapping between knowledgerepresentation languages and some controlled nat-ural language (CNL).
Specifically, the assump-tion is that each atomic term (individual, class,property) maps to a word and each axiom mapsto a sentence.
As a result, the verbalisation oflarger ontology parts can produce very unnaturaltext such as, Every cat is an animal.
Every dogis an animal.
Every horse is an animal.
Everyrabbit is an animal.
More generally, the CNLbased approaches to ontology verbalisation gen-erate clauses (one per axiom) rather than complexsentences and thus cannot adequately handle theverbalisation of more complex input such as theKBGen data where the KB input often requires thegeneration of a complex sentence rather than a se-quence of base clauses.To generate more complex output from KBdata, several alternative approaches have been pro-posed.The MIAKT project (Bontcheva and Wilks.,2004) and the ONTOGENERATION project(Aguado et al, 1998) use symbolic NLG tech-niques to produce textual descriptions from somesemantic information contained in a knowledgebase.
Both systems require some manual in-put (lexicons and domain schemas).
More so-phisticated NLG systems such as TAILOR (Paris,1988), MIGRAINE (Mittal et al, 1994), andSTOP (Reiter et al, 2003) offer tailored outputbased on user/patient models.
While offeringmore flexibility and expressiveness, these systemsare difficult to adapt by non-NLG experts becausethey require the user to understand the architec-ture of the NLG systems (Bontcheva and Wilks.,2004).
Similarly, the NaturalOWL system (Gala-nis et al, 2009) has been proposed to generate flu-ent descriptions of museum exhibits from an OWLontology.
This approach however relies on exten-sive manual annotation of the input data.The SWAT project has focused on producingdescriptions of ontologies that are both coherentand efficient (Williams and Power, 2010).
For in-stance, instead of the above output, the SWAT sys-tem would generate the sentence: The followingare kinds of animals: cats, dogs, horses and rab-bits.
.
In this approach too however, the verbaliseroutput is strongly constrained by a simple DefiniteClause Grammar covering simple clauses and sen-tences verbalising aggregation patterns such as theabove.
More generally, the sentences generated byontology verbalisers cover a limited set of linguis-tics constructions; the grammar used is manuallydefined; and the mapping between semantics andstrings is assumed to be deterministic (e.g., a verbmaps to a relation and a noun to a concept).
Inconstrast, we propose an approach which can gen-erate complex sentences from KB data; where thegrammar is acquired from the data; and where noassumption is made about the mapping betweensemantics and NL expressions.Recent work has focused on data-driven gener-ation from frames, lambda terms and data base en-tries.
(DeVault et al, 2008) describes an approach forgenerating from the frames produced by a dialogsystem.
They induce a probabilistic Tree Adjoin-ing Grammar from a training set algning framesand sentences using the grammar induction tech-nique of (Chiang, 2000) and use a beam searchthat uses weighted features learned from the train-ing data to rank alternative expansions at eachstep.
(Lu and Ng, 2011) focuses on generating nat-ural language sentences from logical form (i.e.,lambda terms) using a synchronous context-freegrammar.
They introduce a novel synchronouscontext free grammar formalism for generatingfrom lambda terms; induce such a synchronousgrammar using a generative model; and extract thebest output sentence from the generated forest us-ing a log linear model.
(Wong and Mooney, 2007; Lu et al, 2009)focuses on generating from variable-free tree-structured representations such as the CLANG for-mal language used in the ROBOCUP competitionand the database entries collected by (Liang etal., 2009) for weather forecast generation and forthe air travel domain (ATIS dataset) by (Dahl etal., 1994).
(Wong and Mooney, 2007) uses syn-chronous grammars to transform a variable freetree structured meaning representation into sen-tences.
(Lu et al, 2009) uses a Conditional Ran-425The function of a gated channel is to release particles from the endoplasmic reticulum:TRIPLES ((|Release-Of-Calcium646| |object| |Particle-In-Motion64582|)(|Release-Of-Calcium646| |base| |Endoplasmic-Reticulum64603|)(|Gated-Channel64605| |has-function||Release-Of-Calcium646|)(|Release-Of-Calcium646| |agent| |Gated-Channel64605|)):INSTANCE-TYPES(|Particle-In-Motion64582| |instance-of| |Particle-In-Motion|)(|Endoplasmic-Reticulum64603| |instance-of| |Endoplasmic-Reticulum|)(|Gated-Channel64605| |instance-of| |Gated-Channel|)|Release-Of-Calcium646| |instance-of| |Release-Of-Calcium|)):ROOT-TYPES ((|Release-Of-Calcium646| |instance-of| |Event|)(|Particle-In-Motion64582| |instance-of| |Entity|)(|Endoplasmic-Reticulum64603| |instance-of| |Entity|)(|Gated-Channel64605| |instance-of| |Entity|)))Figure 1: Example KBGEN Scenariodom Field to generate from the same meaning rep-resentations.Finally, more recent papers propose approacheswhich perform both surface realisation and con-tent selection.
(Angeli et al, 2010) proposes a loglinear model which decomposes into a sequenceof discriminative local decisions.
The first classi-fier determines which records to mention; the sec-ond, which fields of these records to select; and thethird, which words to use to verbalise the selectedfields.
(Kim and Mooney, 2010) uses a genera-tive model for content selection and verbalises theselected input using WASP?1, an existing gener-ator.
Finally, (Konstas and Lapata, 2012b; Kon-stas and Lapata, 2012a) develop a joint optimi-sation approach for content selection and surfacerealisation using a generic, domain independentprobabilistic grammar which captures the struc-ture of the database and the mapping from fieldsto strings.
They intersect the grammar with a lan-guage model to improve fluency; use a weightedhypergraph to pack the derivations; and find thebest derivation tree using Viterbi algorithm.Our approach differs from the approacheswhich assume variable free tree structured repre-sentations (Wong and Mooney, 2007; Lu et al,2009) and data-based entries (Kim and Mooney,2010; Konstas and Lapata, 2012b; Konstas andLapata, 2012a) in that it handles graph-based, KBinput and assumes a compositional semantics.
Itis closest to (DeVault et al, 2008) and (Lu andNg, 2011) who extract a grammar encoding syn-tax and semantics from frames and lambda termsrespectively.
It differs from the former however inthat it enforces a tighter syntax/semantics integra-tion by requiring that the elementary trees of ourextracted grammar encode the appropriate linkinginformation.
While (DeVault et al, 2008) extractsa TAG grammar associating each elementary treewith a semantics, we additionnally require thatthese trees encode the appropriate linking betweensyntactic and semantic arguments thereby restrict-ing the space of possible tree combinations anddrastically reducing the search space.
Althoughconceptually related to (Lu and Ng, 2011), our ap-proach extracts a unification based grammar ratherthan one with lambda terms.
The extraction pro-cess and the generation algorithms are also funda-mentally different.
We use a simple mainly sym-bolic approach whereas they use a generative ap-proach for grammar induction and a discriminativeapproach for sentence generation.3 The KBGen TaskThe KBGen task was introduced as a new sharedtask at Generation Challenges 2013 (Banik et al,2013)1and aimed to compare different generationsystems on KB data.
Specifically, the task is toverbalise a subset of a knowledge base.
For in-stance, the KB input shown in Figure 1 can be ver-balised as:(1) The function of a gated channel is to releaseparticles from the endoplasmic reticulumThe KB subsets forming the KBGen input datawere pre-selected from the AURA biology knowl-edge base (Gunning et al, 2010), a knowledgebase about biology which was manually encodedby biology teachers and encodes knowledge aboutevents, entities, properties and relations whererelations include event-to-entity, event-to-event,1http://www.kbgen.org426NPGCDT NN NNa gated channelinstance-of(GC,Gated-Channel)SRoC1NP?GCVPRoC1RoCVBZRoCNP?PMreleasesinstance-of(RoC,Release-of-Calcium)object(RoC,PM)agent(RoC,GC)NPPMparticlesinstance-of(PM,Particle-In-Motion)VPRoCVP?RoCPPIN NP?ERfrombase(RoC,ER)NPERDT NN NNthe endoplasmic reticuluminstance-of(ER,Endoplasmic-Reticulum)Figure 2: Example FB-LTAG with Unification-Based Semantics.
Dotted lines indicate substitution andadjunction operations between trees.
The variables decorating the tree nodes (e.g., GC) abbreviate fea-ture structures of the form [idx : V ] where V is a unification variable shared with the semantics.event-to-property and entity-to-property relations.AURA uses a frame-based knowledge representa-tion and reasoning system called Knowledge Ma-chine (Clark and Porter, 1997) which was trans-lated into first-order logic with equality and fromthere, into multiple different formats includingSILK (Grosof, 2012) and OWL2 (Motik et al,2009).
It is available for download in various for-mats including OWL2.4 Generating from the KBGenKnowledge-BaseTo generate from the KBGen data, we induce aFeature-Based Lexicalised Tree Adjoining Gram-mar (FB-LTAG, (Vijay-Shanker and Joshi, 1988))augmented with a unification-based semantics(Gardent and Kallmeyer, 2003) from the trainingdata.
We then use this grammar and an existingsurface realiser to generate from the test data.4.1 Feature-Based Lexicalised TreeAdjoining GrammarFigure 2 shows an example FB-LTAG augmentedwith a unification-based semantics.Briefly, an FB-LTAG consists of a set of ele-mentary trees which can be either initial or auxil-iary.
Initial trees are trees whose leaves are labeledwith substitution nodes (marked with a down-arrow) or terminal categories.
Auxiliary trees aredistinguished by a foot node (marked with a star)2http://www.ai.sri.com/halo/halobook2010/exported-kb/biokb.htmlwhose category must be the same as that of theroot node.
In addition, in an FB-LTAG, each el-ementary tree is anchored by a lexical item (lexi-calisation) and the nodes in the elementary treesare decorated with two feature structures calledtop and bottom which are unified during deriva-tion.
Two tree-composition operations are usedto combine trees namely, substitution and adjunc-tion.
While substitution inserts a tree in a substi-tution node of another tree, adjunction inserts anauxiliary tree into a tree.
In terms of unifications,substitution unifies the top feature structure of thesubstitution node with the top feature structure ofthe root of the tree being substituted in.
Adjunc-tion unifies the top feature structure of the root ofthe tree being adjoined with the top feature struc-ture of the node being adjoined to; and the bottomfeature structure of the foot node of the auxiliarytree being adjoined with the bottom feature struc-ture of the node being adjoined to.In an FB-LTAG augmented with a unification-based semantics, each tree is associated with asemantics i.e., a set of literals whose argumentsmay be constants or unification variables.
Thesemantics of a derived tree is the union of thesemantics of the tree contributing to its deriva-tion modulo unification.
Importantly, semanticvariables are shared with syntactic variables(i.e., variables occurring in the feature structuresdecorating the tree nodes) so that when trees arecombined, the appropriate syntax/semantics link-ing is enforced.
For instance given the semantics:427instance-of(RoC,Release-Of-Calcium),object(RoC,PM),agent(RoC,GC),base(RoC,ER),instance-of(ER,Endoplasmic-Reticulum),instance-of(GC,Gated-Channel),instance-of(PM,Particle-In-Motion)the grammar will generate A gated channel re-leases particles from the endoplasmic reticulumbut not e.g., Particles releases a gated channelfrom the endoplasmic reticulum.4.2 Grammar ExtractionWe extract our FB-LTAG with unification seman-tics from the KBGen training data in two mainsteps.
First, we align the KB data with the inputstring.
Second, we induce a Tree Adjoining Gram-mar augmented with a unification-based semanticsfrom the aligned data.4.2.1 AlignmentGiven a Sentence/Input pair (S, I) provided by theKBGen Challenge, the alignment procedure asso-ciates each entity and event variable in I to a sub-string in S. To do this, we use the entity and theevent lexicon provided by the KBGen organiser.The event lexicon maps event types to verbs, theirinflected forms and nominalizations while the en-tity lexicon maps entity types to a noun and itsplural form.
For instance, the lexicon entries forthe event and entity types shown in Figure 1 are asshown in Figure 3.For each entity and each event vari-able V in I , we retrieve the correspondingtype (e.g., Particle-In-Motion forParticle-In-Motion64582); searchthe KBGen lexicon for the corresponding phrases(e.g., molecule in motion,molecules in motion);and associate V with the phrase in S whichmatches one of these phrases.
Figure 3 showsan example lexicon and the resulting alignmentobtained for the scenario shown in Figure 1.
Notethat there is not always an exact match betweenthe phrase associated in the KBGen lexicon witha type and the phrase occurring in the trainingsentence.
To account for this, we use someadditional similarity based heuristics to identifythe phrase in the input string that is most likelyto be associated with a variable lacking an exactmatch in the input string.
E.g., for entity variables(e.g., Particle-In-Motion64582), wesearch the input string for nouns (e.g., particles)whose overlap with the variable type (e.g.,Particle-In-Motion) is not empty.4.2.2 Inducing a based FB-LTAG from thealigned dataTo extract a Feature-Based Lexicalised TreeAdjoining Grammar (FB-LTAG) from the KBGendata, we parse the sentences of the training cor-pus; project the entity and event variables to thesyntactic projection of the strings they are alignedwith; and extract the elementary trees of the result-ing FB-LTAG from the parse tree using semanticinformation.
Figure 4 shows the trees extractedfrom the scenario given in Figure 1.To associate each training example sentencewith a syntactic parse, we use the Stanford parser.After alignment, the entity and event variables oc-curring in the input semantics are associated withsubstrings of the yield of the syntactic parse tree.We project these variables up the syntactic tree toreflect headedness.
A variable aligned with a nounis projected to the NP level or to the immediatelydominating PP if it occurs in the subtree domi-nated by the leftmost daughter of that PP.
A vari-able aligned with a verb is projected to the first Snode immediately dominating that verb or, in thecase of a predicative sentence, to the root of thatsentence3.Once entity and event variables have been pro-jected up the parse trees, we extract elementaryFB-LTAG trees and their semantics from the inputscenario as follows.First, the subtrees whose root node is indexedwith an entity variable are extracted.
This resultsin a set of NP and PP trees anchored with entitynames and associated with the predication true ofthe indexing variable.Second, the subtrees capturing relations be-tween variables are extracted.
To perform this ex-traction, each input variable X is associated with aset of dependent variables i.e., the set of variablesY such that X is related to Y (R(X,Y )).
Theminimal tree containing all and only the dependentvariables D(X) of a variable X is then extractedand associated with the set of literals ?
such that?
= {R(Y,Z) | (Y = X?Z ?
D(X))?
(Y,Z ?D(X))}.
This procedure extracts the subtrees re-lating the argument variables of a semantic func-tors such as an event or a role e.g., a tree describ-ing a verb and its arguments as shown in the top3Initially, we used the head information provided by theStanford parser.
In practice however, we found that theheuristics we defined to project semantic variables to the cor-responding syntactic projection were more accurate and bet-ter supported our grammar extraction process.428Particle-In-Motion molecule in motion,molecules in motionEndoplasmic-Reticulum endoplasmic reticulum,endoplasmic reticulumGated-Channel gated Channel,gated ChannelsRelease-Of-Calcium releases,release,released,releaseThe function of a (gated channel, Gated-Channel64605) is to (release,Release-Of-Calcium646) (particles, Particle-In-Motion64582) from the (endoplas-mic reticulum, Endoplasmic-Reticulum64603 )Figure 3: Example Entries from the KBGen Lexicon and example alignmentSRoC3NP VPRoC3RoC2NP PP VBZ SRoC2RoC1DT NN IN NP?GCis VPRoC1RoCthe fn of TO VBRoCNP?PMPPto release IN NP?ERfrominstance-of(RoC,Release-of-Calcium)object(RoC,PM)base(RoC,ER)has-function(GC,RoC)agent(RoC,GC)NPGCDT NN NNa gated channelinstance-of(GC,Gated-Channel)NPPMparticlesinstance-of(PM,Particle-In-Motion)NPERDT NN NNthe endoplasmic reticuluminstance-of(ER,Endoplasmic-Reticulum)Figure 4: Extracted Grammar for ?The function of a gated channel is to release particles from the endoplasmic reticulum?.Variable names have been abbreviated and the KBGen tuple notation converted to terms so as to fit the input format expected byour surface realiser.429part of Figure 4.
Note that such a tree may cap-ture a verb occurring in a relative or a subordinateclause (together with its arguments) thus allowingfor complex sentences including a relative or re-lating a main and a subordinate clause.The resulting grammar extracted from the parsetrees (cf.
e.g., Figure 4) is a Feature-BasedTree Adjoining Grammar with a Unification-basedcompositional semantics as described in (Gardentand Kallmeyer, 2003).
In particular, our gram-mars differs from the traditional probabilistic TreeAdjoining Grammar extracted as described in e.g.,(Chiang, 2000) in that they encode both syntax andsemantics rather than just syntax.
They also differfrom the semantic FB-TAG extracted by (DeVaultet al, 2008) in that (i) they encode the linking be-tween syntactic and semantic arguments; (ii) theyallow for elementary trees spanning discontiguousstrings (e.g., The function of X is to release Y); and(iii) they enforce the semantic principle underly-ing TAG namely that an elementary tree contain-ing a syntactic functor also contains its syntacticarguments.4.3 GenerationTo generate with the grammar extracted from theKBGen data, we use the GenI surface realiser (Gar-dent et al, 2007).
Briefly, given an input seman-tics and a FB-LTAG with a unification based se-mantics, GenI selects all grammar entries whosesemantics subsumes the input semantics; com-bines these entries using the FB-LTAG combina-tion operations (i.e., adjunction and substitution);and outputs the yield of all derived trees which aresyntactically complete and whose semantics is theinput semantics.
To rank the generator output, wetrain a language model on the GeniA corpus4, acorpus of 2000 MEDLINE asbtracts about biol-ogy containing more than 400000 words (Kim etal., 2003) and use this model to rank the generatedsentences by decreasing probability.Thus for instance, given the input semanticsshown in Figure 1 and the grammar depicted inFigure 4, the surface realiser will select all of thesetrees; combine them using FB-LTAG substitutionoperation; and output as generated sentence theyield of the resulting derived tree namely the sen-tence The function of a gated channel is to releaseparticles from the endoplasmic reticulum.However, this procedure only works if the en-4http://www.nactem.ac.uk/genia/tries necessary to generate from the given inputare present in the grammar.
To handle new, un-seen input, we proceed in two ways.
First, we tryto guess a grammar entry from the shape of the in-put and the existing grammar.
Second, we expandthe grammar by decomposing the extracted treesinto simpler ones.4.4 Guessing new grammar entries.Given the limited size of the training data, it is of-ten the case that input from the test data will haveno matching grammar unit.
To handle such pre-viously unseen input, we start by partitioning theinput semantics into sub-semantics correspondingto events, entities and role.For each entity variable X of type Type, wecreate a default NP tree whose semantics is a lit-eral of the form instance-of(X,Type).For event variables, we search the lexicon foran entry with a matching or similar semantics i.e.,an entry with the same number and same type ofliterals (literals with same arity and with identicalrelations).
When one is found, a grammar entry isconstructed for the unseen event variable by sub-stituting the event type of the matching entry withthe type of the event variable.
For instance, giventhe input semantics instance-of(C,Carry), object(C,X),base(C,Y), has-function(Z,C), agent(C,Z), this procedurewill create a grammar entry identical to that shownat the top of Figure 4 except that the event typeRelease-of-Calcium is changed to Carry and the ter-minal release to the word form associated in theKBGen lexicon with this concept, namely to theverb carry.4.5 Expanding the GrammarWhile the extracted grammar nicely captures pred-icate/argument dependencies, it is very specific tothe items seen in the training data.
To reduce over-fitting, we generalise the extracted grammar by ex-tracting from each event tree, subtrees that cap-ture structures with fewer arguments and optionalmodifiers.For each event tree ?
extracted from the train-ing data which contains a subject-verb-object sub-tree ?
?, we add ?
?to the grammar and associate itwith the semantics of ?
minus the relations associ-ated with the arguments that have been removed.For instance, given the extracted tree for the sen-tence ?Aquaporin facilitates the movement of wa-ter molecules through hydrophilic channels.
?, this430procedure will construct a new grammar tree cor-responding to the subphrase ?Aquaporin facili-tates the movement of water molecules?.We also construct both simpler event trees andoptional modifiers trees by extracting from eventtrees, PP trees which are associated with a re-lational semantics.
For instance, given the treeshown in Figure 4, the PP tree associated withthe relation base(RoC,ET) is removed thus creatingtwo new trees as illustrated in Figure 5: an S treecorresponding to the sentence The function of agated channel is to release particles and an aux-iliary PP tree corresponding to the phrase fromthe endoplasmic reticulum.
Similarly in the aboveexample, a PP tree corresponding to the phrase?through hydrophilic channels.?
will be extracted.As with the base grammar, missing grammarentries are guessed from the expanded grammar.However we do this only in cases where a correctgrammar entry cannot be guessed from the basegrammar.5 Experimental SetupWe evaluate our approach on the KBGen data andcompare it with the KBGen reference and two othersystems having taken part to the KBGen challenge.5.1 Training and test data.Following a practice introduced by (Angeli et al,2010), we use the term scenario to denote a KBsubset paired with a sentence.
The KBGen bench-mark contains 207 scenarii for training and 72 fortesting.
Each KB subset consists of a set of triplesand each scenario contains on average 16 triplesand 17 words.5.2 SystemsWe evaluate three configurations of our approachon the KBGen test data: one without grammar ex-pansion (BASE); a second with a manual grammarexpansion MANEXP; and a third one with auto-mated grammar expansion AUTEXP.
We comparethe results obtained with those obtained by twoother systems participating in the KBGen chal-lenge, namely the UDEL system, a symbolic rulebased system developed by a group of students atthe University of Delaware; and the IMS system,a statistical system using a probabilistic grammarinduced from the training data.5.3 Metrics.We evaluate system output automatically, usingthe BLEU-4 modified precision score (Papineni etal., 2002) with the human written sentences as ref-erence.
We also report results from a human basedevaluation.
In this evaluation, participants wereasked to rate sentences along three dimensions:fluency (Is the text easy to read?
), grammatical-ity and meaning similarity or adequacy (Does themeaning conveyed by the generated sentence cor-respond to the meaning conveyed by the referencesentence?).
The evaluation was done on line us-ing the LG-Eval toolkit (Kow and Belz, 2012),subjects used a sliding scale from -50 to +50 anda Latin Square Experimental Design was used toensure that each evaluator sees the same numberof outputs from each system and for each test setitem.
12 subjects participated in the evaluation and3 judgments were collected for each output.6 Results and DiscussionSystem All Covered Coverage # TreesIMS 0.12 0.12 100%UDEL 0.32 0.32 100%Base 0.04 0.39 30.5% 371ManExp 0.28 0.34 83 % 412AutExp 0.29 0.29 100% 477Figure 6: BLEU scores and Grammar Size (Num-ber of Elementary TAG treesTable 6 summarises the results of the automaticevaluation and shows the size (number of elemen-tary TAG trees) of the grammars extracted fromthe KBGen data.The average BLEU score is given with respectto all input (All) and to those inputs for whichthe systems generate at least one sentence (Cov-ered).
While both the IMS and the UDEL systemhave full coverage, our BASE system strongly un-dergenerates failing to account for 69.5% of thetest data.
However, because the extracted gram-mar is linguistically principled and relatively com-pact, it is possible to manually edit it.
Indeed, theMANEXP results show that, by adding 41 trees tothe grammar, coverage can be increased by 52.5points reaching a coverage of 83%.
Finally, theAUTEXP results demonstrate that the automatedexpansion mechanism permits achieving full cov-erage while keeping a relative small grammar (477trees).431SRoC3NP VPRoC3RoC2NP PP VBZ SRoC2RoC1DT NN IN NP?GCis VPRoC1RoCthe fn of TO VBRoCNP?PMto releaseinstance-of(RoC,Release-of-Calcium)object(RoC,PM)has-function(GC,RoC)agent(RoC,GC)VPRoCVP?,RoCPPIN NP?ERfrombase(RoC,ER)Figure 5: Trees Added by the Expansion ProcessFluency Grammaticality Meaning SimilaritySystem Mean Homogeneous Subsets Mean Homogeneous Subsets Mean Homogeneous SubsetsUDEL 4.36 A 4.48 A 3.69 AAutExp 3.45 B 3.55 B 3.65 AIMS 1.91 C 2.05 C 1.31 BFigure 7: Human Evaluation Results on a scale of 0 to 5.
Homogeneous subsets are determined usingTukey?s Post Hoc Test with p < 0.05In terms of BLEU score, the best version of oursystem (AUTEXP) outperforms the probabilisticapproach of IMS by a large margin (+0.17) andproduces results similar to the fully handcraftedUDEL system (-0.03).In sum, our approach permits obtaining BLEUscores and a coverage which are similar to thatobtained by a hand crafted system and outper-forms a probabilistic approach.
One key feature ofour approach is that the grammar extracted fromthe training data is linguistically principled in thatit obeys the extended locality principle of TreeAdjoining Grammars.
As a result, the extractedgrammar is compact and can be manually modi-fied to fit the need of an application as shown bythe good results obtained when using the MAN-EXP configuration.We now turn to the results of the human eval-uation.
Table 7 summarises the results wherebysystems are grouped by letters when there is nosignificant difference between them (significancelevel: p < 0.05).
We used ANOVAs and post-hoc Tukey tests to test for significance.
The dif-ferences between systems are statistically signifi-cant throughout except for meaning similarity (ad-equacy) where UDEL and our system are on thesame level.
Across the metrics, our system consis-tently ranks second behind the symbolic, UDELsystem and before the statistical IMS one thus con-firming the ranking based on BLEU.7 ConclusionIn Tree Adjoining Grammar, the extended domainof locality principle ensures that TAG trees grouptogether in a single structure a syntactic predi-cate and its arguments.
Moreover, the semanticprinciple requires that each elementary tree cap-tures a single semantic unit.
Together these twoprinciples ensure that TAG elementary trees cap-ture basic semantic units and their dependencies.In this paper, we presented a grammar extractionapproach which ensures that extracted grammarscomply with these two basic TAG principles.
Us-ing the KBGen benchmark, we then showed thatthe resulting induced FB-LTAG compares favor-ably with competing symbolic and statistical ap-proaches when used to generate from knowledgebase data.In the current version of the generator, theoutput is ranked using a simple language modeltrained on the GENIA corpus.
We observed thatthis often fails to return the best output in termsof BLEU score, fluency, grammaticality and/ormeaning.
In the future, we plan to remedy this us-ing a ranking approach such as proposed in (Vell-dal and Oepen, 2006; White and Rajkumar, 2009).432ReferencesG.
Aguado, A.
Ban?o?n, J. Bateman, S. Bernardos,M.
Ferna?ndez, A.
Go?mez-Pe?rez, E. Nieto, A. Olalla,R.
Plaza, and A. Sa?nchez.
1998.
Ontogeneration:Reusing domain and linguistic ontologies for span-ish text generation.
In Workshop on Applicationsof Ontologies and Problem Solving Methods, ECAI,volume 98.Gabor Angeli, Percy Liang, and Dan Klein.
2010.
Asimple domain-independent probabilistic approachto generation.
In Proceedings of the 2010 Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 502?512.
Association for Com-putational Linguistics.Eva Banik, Claire Gardent, Donia Scott, Nikhil Dinesh,and Fennie Liang.
2012.
Kbgen: Text generationfrom knowledge bases as a new shared task.
In Pro-ceedings of the seventh International Natural Lan-guage Generation Conference, pages 141?145.
As-sociation for Computational Linguistics.Eva Banik, Claire Gardent, Eric Kow, et al 2013.
Thekbgen challenge.
In Proceedings of the 14th Eu-ropean Workshop on Natural Language Generation(ENLG), pages 94?97.K.
Bontcheva and Y. Wilks.
2004.
Automatic re-port generation from ontologies: the miakt ap-proach.
In Ninth International Conference on Appli-cations of Natural Language to Information Systems(NLDB?2004).
Lecture Notes in Computer Science3136, Springer, Manchester, UK.David Chiang.
2000.
Statistical parsing with anautomatically-extracted tree adjoining grammar.
InProceedings of the 38th AnnualMeeting on Associa-tion for Computational Linguistics, pages 456?463.Association for Computational Linguistics.Peter Clark and Bruce Porter.
1997.
Building con-cept representations from reusable components.
InAAAI/IAAI, pages 369?376.
Citeseer.Deborah A Dahl, Madeleine Bates, Michael Brown,William Fisher, Kate Hunicke-Smith, David Pallett,Christine Pao, Alexander Rudnicky, and ElizabethShriberg.
1994.
Expanding the scope of the atistask: The atis-3 corpus.
In Proceedings of the work-shop on Human Language Technology, pages 43?48.Association for Computational Linguistics.David DeVault, David Traum, and Ron Artstein.
2008.Making grammar-based generation easier to deployin dialogue systems.
In Proceedings of the 9th SIG-dial Workshop on Discourse and Dialogue, pages198?207.
Association for Computational Linguis-tics.D.
Galanis, G. Karakatsiotis, G. Lampouras, and I. An-droutsopoulos.
2009.
An open-source natural lan-guage generator for owl ontologies and its use inprote?ge?
and second life.
In Proceedings of the 12thConference of the European Chapter of the Asso-ciation for Computational Linguistics: Demonstra-tions Session, pages 17?20.
Association for Compu-tational Linguistics.Claire Gardent and Laura Kallmeyer.
2003.
Semanticconstruction in feature-based tag.
In Proceedings ofthe tenth conference on European chapter of the As-sociation for Computational Linguistics-Volume 1,pages 123?130.
Association for Computational Lin-guistics.Claire Gardent, Eric Kow, et al 2007.
A symbolic ap-proach to near-deterministic surface realisation us-ing tree adjoining grammar.
In ACL, volume 7,pages 328?335.B.
Grosof.
2012.
The silk project: Semantic infer-encing on large knowledge.
Technical report, SRI.http://silk.semwebcentral.org/.D.
Gunning, V. K. Chaudhri, P. Clark, K. Barker, Shaw-Yi Chaw, M. Greaves, B. Grosof, A. Leung, D. Mc-Donald, S. Mishra, J. Pacheco, B. Porter, A. Spauld-ing, D. Tecuci, and J. Tien.
2010.
Project halo up-date - progress toward digital aristotle.
AIMagazine,Fall:33?58.K.
Kaljurand and N.E.
Fuchs.
2007.
Verbalizingowl in attempto controlled english.
Proceedings ofOWLED07.Martin Kay.
1996.
Chart generation.
In Proceedingsof the 34th annual meeting on Association for Com-putational Linguistics, pages 200?204.
Associationfor Computational Linguistics.Joohyun Kim and Raymond J Mooney.
2010.
Gen-erative alignment and semantic parsing for learn-ing from ambiguous supervision.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics: Posters, pages 543?551.
Associ-ation for Computational Linguistics.J-D Kim, Tomoko Ohta, Yuka Tateisi, and Junichi Tsu-jii.
2003.
Genia corpusa semantically annotatedcorpus for bio-textmining.
Bioinformatics, 19(suppl1):i180?i182.Ioannis Konstas and Mirella Lapata.
2012a.
Concept-to-text generation via discriminative reranking.
InProceedings of the 50th Annual Meeting of theAssociation for Computational Linguistics: LongPapers-Volume 1, pages 369?378.
Association forComputational Linguistics.Ioannis Konstas and Mirella Lapata.
2012b.
Unsuper-vised concept-to-text generation with hypergraphs.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 752?761.
Association for Computational Lin-guistics.Eric Kow and Anja Belz.
2012.
Lg-eval: A toolkit forcreating online language evaluation experiments.
InLREC, pages 4033?4037.433Percy Liang, Michael I Jordan, and Dan Klein.
2009.Learning semantic correspondences with less super-vision.
In Proceedings of the Joint Conference of the47th AnnualMeeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Pro-cessing of the AFNLP: Volume 1-Volume 1, pages91?99.
Association for Computational Linguistics.Wei Lu and Hwee Tou Ng.
2011.
A probabilisticforest-to-string model for language generation fromtyped lambda calculus expressions.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 1611?1622.
Asso-ciation for Computational Linguistics.Wei Lu, Hwee Tou Ng, and Wee Sun Lee.
2009.
Nat-ural language generation with tree conditional ran-dom fields.
In Proceedings of the 2009 Conferenceon Empirical Methods in Natural Language Pro-cessing: Volume 1-Volume 1, pages 400?409.
As-sociation for Computational Linguistics.VO Mittal, G. Carenini, and JD Moore.
1994.
Gen-erating patient specific explanations in migraine.
InProceedings of the eighteenth annual symposium oncomputer applications in medical care.
McGraw-Hill Inc.Boris Motik, Peter F Patel-Schneider, Bijan Parsia,Conrad Bock, Achille Fokoue, Peter Haase, RinkeHoekstra, Ian Horrocks, Alan Ruttenberg, Uli Sat-tler, et al 2009.
Owl 2 web ontology language:Structural specification and functional-style syntax.W3C recommendation, 27:17.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.C.L.
Paris.
1988.
Tailoring object descriptions to auser?s level of expertise.
Computational Linguistics,14(3):64?78.R.
Power and A.
Third.
2010.
Expressing owl ax-ioms by english sentences: dubious in theory, fea-sible in practice.
In Proceedings of the 23rd Inter-national Conference on Computational Linguistics:Posters, pages 1006?1013.
Association for Compu-tational Linguistics.E.
Reiter, R. Robertson, and L.M.
Osman.
2003.Lessons from a failure: Generating tailored smokingcessation letters.
Artificial Intelligence, 144(1):41?58.Hadar Shemtov.
1996.
Generation of paraphrases fromambiguous logical forms.
In Proceedings of the 16thconference on Computational linguistics-Volume 2,pages 919?924.
Association for Computational Lin-guistics.Stuart M Shieber, Gertjan Van Noord, Fernando CNPereira, and Robert C Moore.
1990.
Semantic-head-driven generation.
Computational Linguistics,16(1):30?42.Erik Velldal and Stephan Oepen.
2006.
Statisticalranking in tactical generation.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing, pages 517?525.
Associationfor Computational Linguistics.K.
Vijay-Shanker and AK Joshi.
1988.
Feature struc-tures based tree adjoining grammars.
In Proceed-ings of the 12th International Conference on Com-putational Linguistics, Budapest, Hungary.Juen-tin Wang.
1980.
On computational sentence gen-eration from logical form.
In Proceedings of the8th conference on Computational linguistics, pages405?411.
Association for Computational Linguis-tics.Michael White and Rajakrishnan Rajkumar.
2009.Perceptron reranking for ccg realization.
In Pro-ceedings of the 2009 Conference on Empirical Meth-ods in Natural Language Processing: Volume 1-Volume 1, pages 410?419.
Association for Compu-tational Linguistics.G.
Wilcock.
2003.
Talking owls: Towards an ontologyverbalizer.
Human Language Technology for the Se-mantic Web and Web Services, ISWC, 3:109?112.Sandra Williams and Richard Power.
2010.
Groupingaxioms for more coherent ontology descriptions.
InProceedings of the 6th International Natural Lan-guage Generation Conference (INLG 2010), pages197?202, Dublin.Yuk Wah Wong and Raymond J Mooney.
2007.
Gen-eration by inverting a semantic parser that uses sta-tistical machine translation.
In HLT-NAACL, pages172?179.434
