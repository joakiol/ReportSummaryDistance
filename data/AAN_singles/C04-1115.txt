Feature Weighting for Co-occurrence-based Classification of WordsViktor PEKARCLG, U. of WolverhamptonWolverhamptonUK, WV1 1SBv.pekar@wlv.ac.ukMichael KRKOSKAMentasys GmbHSchonfeldstrasse 8Karlsruhe, Germany, 76131michael@mentasys.deSteffen STAABOntoprise GmbH & InstituteAIFB, U. of KarlsruheKarlsruhe, Germany, 76128staab@aifb.uni-karlsruhe.deAbstract*The paper comparatively studies methods offeature weighting in application to the task ofcooccurrence-based classification of wordsaccording to their meaning.
We explore parameteroptimization of several weighting methodsfrequently used for similar problems such as textclassification.
We find that successful applicationof all the methods crucially depends on a numberof parameters; only a carefully chosen weightingprocedure allows to obtain consistent improvementon a classifier learned from non-weighted data.1 IntroductionLexical repositories like thesauri and lexicons aretoday a key component of many NLP technologies,where they serve as background knowledge forprocessing the semantics of text.
But, as is wellknown, manual compilation of such resources is avery costly procedure, and their automatedconstruction is an important research issue.One promising possibility to speed up the lexicalacquisition process is to glean the semantics ofwords from a corpus by adopting the co-occurrence model of word meaning.
Previousresearch has investigated a wide range of itsapplications, including automatic construction ofthesauri, their enrichment, acquisition of bilinguallexicons, learning of information extractionpatterns, named entity classification and others..The basic idea behind the approach is that thedistribution of a word across lexical contexts (otherwords and phrases it co-occurs with) is highlyindicative of its meaning.
The method representsthe meaning of a word as a vector where eachfeature corresponds to a context and its value to thefrequency of the word?s occurring in that context.Once such representation is built, machine learningtechniques can be used to perform various lexicalacquisition tasks, e.g.
automatically classify orcluster words according to their meaning.However, using natural language words asfeatures inevitably results in very noisy.
* The study was partially supported by the RussianFoundation Basic Research grant #03-06-80008.representations.
Because of their inherentpolysemy and synonymy, many context wordsbecome ambiguous or redundant features.
It istherefore desirable to determine a measure ofusefulness of each feature and weight itaccordingly.
Still, despite a wide variety of featureweighting methods existing in machine learning,these methods are poorly explored in application tolexical acquisition.
There have been a few studies(e.g., Lin, 1998; Ciaramita, 2002; Alfonseca andManandhar, 2002) where word representations aremodified through this or that kind of featureweighting.
But in these studies it is performed onlyas a standard pre-processing step on the analogywith similar tasks like text categorization, and thechoice of a particular weighting procedure isseldom motivated.
To our knowledge, there is nowork yet on evaluation and comparison of differentweighting methods for lexical acquisition.The goal of this paper is to comparatively studya number of popular feature weighting methods inapplication to the task of word classification.The structure of the paper is the following.Section 2 more formally describes the task offeature weighting.
Section 3 describes theweighting methods under study.
Section 4 detailsthe experimental data, classification algorithmsused, and evaluation methods.
Section 5 isconcerned with the results of the experiments andtheir discussion.
Section 6 presents conclusionsfrom the study.2 Two feature weighting strategiesIn machine learning, feature weighting beforeclassification is performed with the purpose toreflect how much particular features reveal aboutclass membership of instances.
The weights offeatures are determined from their distributionacross training classes, which is why the weightingprocedure can be called supervised.
In the contextof word classification this procedure can beformalized as follows.Let us assume that each word n?N of thetraining set is represented as a feature vector,consisting of features f ?
F, and that each n isassigned a class label c?C, i.e.
"n$c?C: n?c.
Foreach f, from its distribution across C, a certainfunction computes its relevance score, specific toeach class.
This score can be used directly as itslocal weight w(f,c).
Alternatively, from class-specific weights of a feature, one can compute itssingle global weight, using some globalizationpolicy.
For example, as a global weight one canuse the maximum local weight of f across allclasses wglob(f)= ),(max cfwCc?
.
After the weightshave been applied to the training data, a classifieris learned and evaluated on the test data.A key decision in the weighting procedure is tochoose a function computing w(f,c).
Such functionstypically try to capture the intuition that the bestfeatures for a class are the ones that bestdiscriminate  the sets of its positive and negativeexamples.
They determine w(f,c) from thedistribution of f between c and c , attributinggreater weights to those f that correlate with c or cmost.
In the present study we include three suchfunctions widely used in text categorization:mutual information, information gain ratio andodds ratio.There is another view on feature scoring that it issometimes adopted in classification tasks.According to this view, useful are those featuresthat are shared by the largest number of positiveexamples of c. The purpose of emphasizing thesefeatures is to characterize the class withoutnecessarily discriminating it from other classes.Functions embodying this view assess w(f,c) fromthe distribution of f across n ?
c, giving greaterweight to those f that are distributed most uniformly.Although they do not explicitly aim at underpinningdifferences between classes, these functions wereshown to enhance text retrieval (Wilbur andSirotkin, 1992) and text categorization (Yang andPedersen, 1997).
In this paper we experimented withterm strength, a feature scoring function previouslyshown to be quite competitive in informationretrieval.
Since term strength is an unsupervisedfunction, we develop two supervised variants of ittailoring them for the classification task.3 Feature Weighting Functions3.1 Mutual InformationMutual information (MI) is an information-theoretic measure of association between twowords, widely used in statistical NLP.
PointwiseMI between class c and feature f measures howmuch information presence of f contains about c:)()(),(log),(cPfPcfPcfMI =             (1)3.2 Gain RatioGain Ratio (GR) is a normalized variant ofInformation Gain (IG), introduced into machinelearning from information theory (Quinlan, 1993).IG measures the number of bits of informationobtained about presence and absence of a class byknowing the presence or absence of the feature1:?
??
?=},{ },{ )()(),(log),(),(ccd ffg dPgPdgPdgPcfIG    (2)Gain Ratio aims to overcome one disadvantageof IG which is the fact that IG grows not only withthe increase of dependence between f and c, butalso with the increase of the entropy of f. That iswhy features with low entropy receive smaller IGweights although they may be strongly correlatedwith a class.
GR removes this factor bynormalizing IG by the entropy of the class:?
?-=},{)(log)(),(),(ffggPgPcgIGcfGR             (3)3.3 Odds RatioOdds Ratio (OR) is used in information retrievalto rank documents according to their relevance onthe basis of association of their features with a setof positive documents.
Mladenic (1998) reportsOR to be a particularly successful method ofselecting features for text categorization.
The ORof a feature f, given the set of positive examplesand negative examples for class c, is defined as2:)|())|(1())|(1()|(),(cfpcfpcfpcfpcfOR?--?=               (4)3.4 Term StrengthTerm Strength (TS) was introduced by Wilburand Sirotkin (1992) for improving efficiency ofdocument retrieval by feature selection.
It was laterstudied in a number of works by Yang and hercolleagues (e.g., Yang and Pedersen, 1997), whofound that it performs on par with bestdiscriminative functions on the documentcategorization task.
This method is based on theidea that most valuable features are shared byrelated documents.
It defines the weight of a1 Strictly speaking, the definition does not define IG,but conditional entropy H(c|f) ; the other ingredient ofthe IG function, the entropy of c, being constant andthus omitted from actual weight calculation.2 In cases when p(f|c) equals 1 or p(f|c ) equals 0, wemapped the weight to the maximum OR weight in the class.feature as the probability of finding it in somedocument d given that it has also appeared in thedocument d?, similar to d. To calculate TS forfeature f, for each n we first retrieved severalrelated words n?
using a distributional similaritymeasure, thus preparing a set of pairs (n, n?).
TheTS weight for f was then calculated as theconditional probability of f appearing in n giventhat f appears also in n?
(the ordering of wordsinside a pair is ignored):)'|()( nfnfPfTS ?
?=                          (5)An important parameter in TS is the threshold onthe similarity measure used to judge two words tobe sufficiently related.
Yang and Pedersendetermined this threshold by first deciding howmany documents can be related to a given one andthen finding the average minimum similaritymeasure for this number of neighbors over alldocuments in the collection.
It should be noted thatTS does not make use of the information aboutfeature-class associations and therefore isunsupervised and can be used only for globalfeature weighting.We introduce two supervised variants of TS,which can be applied locally: TSL1 and TSL2.
Thefirst one is different from TS in that, firstly, relatedwords for n are looked for not in the entire trainingset, but within the class of n; secondly, the weightfor a feature is estimated from the distribution ofthe feature across pairs of members of only thatclass:c  ,with ),'|(),(1 ??
?= n'nnfnfPcfTSL       (6)Thus, by weighting features using TSL1 we aimto increase similarity between members of a classand disregard possible similarities across classes.Both TS and TSL1 require computation ofsimilarities between a large set of words and thusincur significant computational costs.
We thereforetried another, much more efficient method toidentify features characteristic of a class, calledTSL2.
As TSL1, it looks at how many members ofa class share a feature.
But instead of computing aset of nearest neighbors for each member, itsimply uses all the words in the class as the setof related words.
TSL2 is the proportion ofinstances which possess feature f to the totalnumber of instances in c :|}{||}|{|),(2 cnnfcncfTSL??
?=                         (7)Table 1 illustrates the 10 highest scored featuresaccording to five supervised functions for the class{ambulance, car, bike, coupe, jeep, motorbike,taxi, truck} (estimated from the BNC co-occurrence data described in Section 4).MI GR OR TSL1 TSL2see_intodie_afterdrive_intoremand_torun_fromprivatiserelease_intoswitch_tomake_aboutentrust_toknock_byclimb _ofdie_afterdrive_intoremand_toprivatisemake_aboutforce_ofplan_withrecover_indie_afterdrive_intoremand_toprivatisemake_aboutforce_ofplan_withrecover_instart_upexplode_inseedrivetakegetget_intohearneedcallsendgo_byseedrivegettakeget_intoparkhearwait_forneedcallTable 1.
10 highest scored features for class{ambulance, car, bike, coupe, jeep, motorbike,taxi, truck} according to MI, GR, OR, TSL1, TSL2The examples vividly demonstrate the basicdifferences between the functions emphasizingdiscriminative features vs. those emphasizingcharacteristic features.
The former attributegreatest weights to very rare context words, someof which seem rather informative (knock_by,climb_of, see_into), some also appear to beoccasional collocates (remand_to, recover_in ) orparsing mistakes (entrust_to, force_of).
In contrast,the latter encourage frequent context words.Among them are those that are intuitively useful(drive, park, get_into), but also those that are tooabstract (see, get, take).
The inspection of the weightssuggests that both feature scoring strategies are ableto identify different potentially useful features, but atthe same time often attribute great relevance to quitenon-informative features.
We next describe anempirical evaluation of these functions.4 Experimental Settings4.1 DataThe evaluation was carried out on the task ofclassifying English nouns into predefined semanticclasses.
The meaning of each noun n?N wasrepresented by a vector where features are verbsv?V with which the nouns are used as either director prepositional objects.
The values of the featureswere conditional probabilities p(v|n).
Two differentdatasets were used in the experiments: verb-nounco-occurrence pairs extracted from the BritishNational Corpus (BNC)3 and from the AssociatedPress 1988 corpus (AP)4.
Rare nouns were filtered:the BNC data contained nouns that appeared withat least 5 different verbs and the AP data contained1000 most frequent nouns, each of which appeared3 http://www.wlv.ac.uk/~in8113/data/bnc.tar.gz4 http://www.cs.cornell.edu/home/llee/data/sim.htmlwith at least 19 different verbs.
Co-occurrencesthat appeared only once were removed.To provide the extracted nouns with class labelsneeded for training and evaluation, the nouns werearranged into classes using WordNet in thefollowing manner.
Each class was made up ofthose nouns whose most frequent senses arehyponyms to a node seven edges below the rootlevel of WordNet.
Only those classes were used inthe study that had 5 or more members.
Thus, fromthe BNC data we formed 60 classes with 514 nounsand from the AP data 42 classes with 375 nouns.4.2 Classification algorithmsTwo classification algorithms were used in thestudy: k  nearest neighbors (kNN) and Na?ve Bayes,which were previously shown to be quite robust onhighly dimensional representations on tasksincluding word classification (e.g., Ciaramita 2002).The kNN algorithm classifies a test instance byfirst identifying its k  nearest neighbors among thetraining instances according to some similaritymeasure and then assigning it to the class that hasthe majority in the set of nearest neighbors.
Weused the weighted kNN algorithm: the vote of eachneighbor was weighted by the score of itssimilarity to the test instance.As is well known, kNN?s performance is highlysensitive to the choice of the similarity metric.Therefore, we experimented with several similaritymetrics and found that on both datasets Jensen-Shannon Divergence yields the best classificationresults (see Table 1).
Incidentally, this is inaccordance with a study by (Dagan et al, 1997)who found that it consistently performed betterthan a number of other popular functions.Similarity function BNC APJensen-Shannon 41.67 41.33L1 distance 38.15 39.72Jaccard 36.82 37.01Cosine 36.80 34.95Skew Divergence 35.82 37.34L2 distance 24.15 26.62Table 2.
Comparison of similarity functions forthe kNN algorithm.Jensen-Shannon Divergence measures the(dis)similarity between a train instance n and testinstance m as:)]||()||([21),( ,, mnmn avgmDavgnDmnJ +=   (8)where D is the Kullback Leibler divergencebetween two probability distributions x and y:?
?= Vv yvpxvpxvpyxD)|()|(log)|()||(                (9)and avgn,m is the average of the distributions of nand m.In testing each weighting method, weexperimented with k = 1, 3, 5, 7, 10, 15, 20, 30, 50,70, and 100 in order to take into account the factthat feature weighting typically changes theoptimal value of k .
The results for kNN reportedbelow indicate the highest effectiveness measuresobtained among all k  in a particular test.The Na?ve Bayes algorithm classifies a testinstance m by finding a class c that maximizesp(c|Vm?m).
Assuming independence betweenfeatures, the goal of the algorithm can be stated as:)|()(maxarg)|(maxarg iVviimii cvpcpVcpm???
(10)where p(ci) and p(v|ci) are estimated during thetraining process from the corpus data.The Na?ve Bayes classifier adopted in the studywas the binary independence model, whichestimates p(v|ci) assuming the binomial distributionof features across classes.
In order to introduce theinformation inherent in the frequencies of featuresinto the model all input probabilities werecalculated from the real values of features, assuggested in (Lewis, 1998).4.3 Evaluation methodTo evaluate the quality of classifications, weadopted the ten-fold cross-validation technique.
Thesame 10 test-train splits were used in all experiments.Since we found that the difficulty of particular testsets can vary quite a lot, using the same test-trainsplits allowed for estimation of the statisticalsignificance of differences between the results ofparticular methods (one-tailed paired t-test was usedfor this purpose).
Effectiveness was first measuredin terms of precision and recall, which were thenused to compute the Fb score5.
The reportedevaluation measure is microaveraged F scores.As a baseline, we used the k-nn and the Na?veBayes classifiers trained and tested on non-weighted instances.5 Results5.1 Term StrengthWe first describe experiments on finding themost optimal parameter settings for Term Strength.As was mentioned in Section 3.4, an importantparameter of term strength that needs to be tuned for5 b was set to 1.a task is the similarity threshold which is used tojudge a pair of words to be semantically related.Since in both datasets the minimum number ofwords in a class was 5, we chose 4 to be the numberof words that can be related to any given word.Finding the four nearest neighbors for each word inthe collection, we calculated the average minimumsimilarity score that a pair of words must have inorder to be considered related.
However, since wordsvary a lot in terms of the amount of corpus dataavailable on them, the average similarity thresholdmight be inappropriate for many words.
Thereforewe tried also another way to select pairs of relatedwords by simply taking the four most similar wordsfor each particular word.
Table 4 compares the twomethods of locating related words (significantimprovements at a=0.05 are shown in bold).kNN Na?ve BayesTS TSL1 TS TSL1BNCThreshold 39.54 36.84 41.67 37.97Top 4 words 40.90 39.74 41.86 42.64APThreshold 42.12 40.22 37.80 33.82Top 4 words 42.12 44.45 38.07 36.47Table 4.
Two methods of identifying semanticallyrelated words for TS and TSL1.We see that using a similarity threshold indeedproduces worse results, significantly so for TSL1.
Inthe rest of the experiments we used a fixed numberof related words in calculating TS and TSL1.5.2 Globalization methodsBefore comparing global and local variants ofthe functions, we studied three ways to derive aglobal weight for a feature: (1) using the maximumlocal relevance score of a feature across all classes,(2) its weighted average score (the contribution ofeach class-specific score is weighted by the size ofthe class), and (3) the sum of all local scores.
Theresults are shown on Tables 5 and 6 (in bold arethe figures that are significantly different from thesecond-best achievement at a=0.05).MI GR OR TSL1 TSL2BNCmax 42.83 48.88 46.35 37.9 41.52wavg 41.29 45.95 42.65 26.47 27.24sum 41.29 45.95 42.65 26.46 27.24APmax 43.17 43.44 44.77 32.48 35.95wavg 42.93 43.98 41.
37.31 38.12sum 43.20 43.99 41.61 37.04 37.85Table 5.
Globalization methods on kNN.MI GR OR TSL1 TSL2BNCmax 46.52 42.82 43.96 37.17 38.53wavg 43.4 41.45 43.98 21.
24.5sum 40.48 43.59 45.15 18.66 22.96APmax 39.68 38.6 42.07 38.1 38.64wavg 39.15 42.1 40.23 33.82 35.15sum 39.68 42.38 40.76 34.1 35.96Table 6.
Globalization methods on Na?ve Bayes.As one can see, using a maximum local weight isusually the best method of globalization.
Itsperformance is often significantly higher than thatof the other methods.
The explanation for this canbe the fact that a feature often has very high scoresrelative to specific classes, while in the rest of theclasses its weight is low.
Using its weightedaverage score or a sum of local scores results inobscuring its high relevance to some classes.
Incontrast, the maximum local score does reflecthigh relevance of the feature to these classes.
If, inaddition to that, the feature appears in very fewclasses, it is unlikely that its being weighted toohighly interferes with the representations ofirrelevant classes.
This is confirmed by the factthat the maximum weight is noticeably better onthe BNC dataset, which contains much more rarefeatures than the AP one.5.3 Global vs. Local WeightingIn carrying out either local or global weighting,there is a choice either to weight only traininginstances or also test instances before theirclassification.
The test instance can be weightedeither by the global weights or by the local weightsof the class it is compared with.
Tables 7 and 8present the results of the evaluation of thefunctions along two dimensions: (1) local versusglobal weighting and (2) weighted versus un-weighted test instances.
As before, the results forthose methods whose superiority over other ones isstatistically significant appear in bold.MI GR OR TS TSL1 TSL2BNCgl y 42.83 48.88 46.35 34.72 32.48 35.95loc y 28.43 35.84 34.29 - 15.38 20.75gl n 40.12 39.74 38.36 40.90 38.35 36.99loc n 40.32 40.33 39.72 - 39.74 41.29APgl y 43.17 43.44 44.77 38.12 37.9 41.52loc y 37.31 31.74 37.04 - 33.06 36.68gl n 41.59 40.78 40.51 42.12 39.25 40.24loc n 40.74 37.86 41.34 - 44.45 43.70Table 7.
Local vs. global weighting schemason kNN.MI GR OR TS TSL1 TSL2BNCgl y 46.52 42.82 43.96 33.87 37.17 38.53loc y 41.84 43.79 38.32 - 36.01 39.53gl n 45.54 42.63 40.87 41.86 41.65 45.54loc n 43.99 38.93 44.38 - 42.64 46.53APgl y 39.68 38.60 42.07 36.22 38.10 38.64loc y 36.50 31.72 37.04 - 33.56 35.66gl n 39.16 35.44 38.65 38.07 39.43 39.95loc n 38.89 27.96 38.15 - 36.47 39.42Table 8.
Local vs. global weighting schemas onNa?ve Bayes.The results are largely consistent both across thedatasets and across the classification methods.Discriminative functions are almost always best intheir global variants; when applying them globally,it is also advisable to weight test instances.
Incontrast, the characteristic functions TSL1 andTSL2 are usually better when applied locally.
It isalso noteworthy that all the variants of TS farebetter when test instances are not weighted.We believe that the good performance of theglobal versions of MI, GR, and OR should beexplained by the fact that features they weighthighest are rare and likely to appear only in oneclass so that using the same weight for all classesdoes not cause confusion between them.
It is alsobeneficial to weight test instances globally,because this guarantees that most features of a testinstance always have a non-zero weight.
Withcharacteristic functions, however, highest weightedare rather frequent features which are often presentin other classes as well.
Using the same weight ofthese features for all classes therefore fails todifferentiate classes from each other.
Local TSL1and TSL2 are more advantageous.
Althoughindividual features they weight highest may bemediocre separators, usually several such featuresare given prominence within a class.
Takencollectively they appear to be able to successfullydiscriminate a class from other classes.An interesting observation is that thecombination of a local schema with weighted testinstances is very undesirable with all the functions.The reason for this is that very often a test instancehas many features different from those in thetraining class to which it is being compared.Because of this, these features receive zero localweights, which renders the representation of thetest instance extremely sparse.Table 9 shows how the performance of the mostoptimal settings for the six studied functioncompares with the baseline (improvements on thebaseline are in bold).kNN Na?ve BayesBNC AP BNC APMI 42.83 43.17 46.52 39.68GR 48.88 43.44 43.79 38.60OR 46.35 44.77 43.96 42.07TS 40.90 42.12 41.86 38.07TSL1 39.74 44.45 42.64 39.43TSL2 41.29 43.70 46.53 39.95baseline 41.67 41.33 45.55 39.16Table 9.
The most optimal settings for MI, GR,OR, TS, TSL1 and TSL2 compared to the baselines.All the functions often show superiority over thebaseline, except for TS which only once slightlyoutperformed it.
However, statistical significanceof the improvement was registered only for MI andOR on the BNC data, using the kNN classifier,which was 17% and 11% better than the baselinecorrespondingly.Comparing discriminative and characteristicweighting functions we see that the supervisedvariants of TS frequently perform on a par withMI, GR, and OR.
Particularly, TSL2 was the bestperformer on Naive Bayes, BNC and the secondbest on kNN, AP.
We also see that the supervisedvariants of TS very often surpass its originalunsupervised variant, but the improvement issignificant only for TSL2, on the BNC datasetusing Naive Bayes (at a=0.001).5.4 Correlations between the functionsAs was mentioned before, an informal inspectionof features emphasized by different functionssuggests that the discriminative functions tend togive greater weights to rare features, whilecharacteristic ones to frequent features.
In order tosee if this results in disagreement between them asto the classifications of test instances, we measuredthe extent to which classifications resulting fromMI, GR, OR, TSL1, and TSL2 overlap.
For thispurpose, we calculated the Kappa coefficient forall the 10 possible pairs of these functions.
Theresults are reported in Table 10.GR OR TSL1 TSL2MI 0.6760.7620.7110.7290.5840.6680.8010.788GR 0.8730.8550.4830.6170.5710.703OR 0.4730.6140.5880.695TSL1 0.6580.721Table 10.
The agreement in classifications usingNa?ve Bayes between MI, GR, OR, TSL1, andTSL2 on the BNC and AP datasets.On results from both datasets, we see that thehighest agreement is indeed between MI, OR, andGR and between TSL1 and TSL2.
Interestingly,there is also a relatively strong correlation betweenclassification resulting from using MI and TSL2.The lowest agreement is between thediscriminative functions and TSL1.kNN Na?ve BayesBNC AP BNC APMI 42.83 43.17 46.52 39.68GR 48.88 43.44 43.79 38.60OR 46.35 44.77 43.96 42.07TSL1 39.74 44.45 42.64 39.43TSL2 41.29 43.70 46.53 39.95MI*TSL1 40.51 45.27 44.2 38.1GR*TSL1 42.47 43.2 43.6 34.37OR*TSL1 41.49 44.72 46.32 37.3MI*TSL2 44.81 45.04 46.73 42.36GR*TSL2 47.53 44.77 44.17 37.55OR*TSL2 46.35 45.29 46.5 40.47Table 11.
Combinations of TSL1 and TSL2 withMI, GR, and OR.5.5 Combination of the functionsAn obvious question is whether the effectivenessof classifications can be increased by combiningthe discriminative and the characteristic weights ofa feature given that both provide useful, butdifferent kinds of evidence about the correct classlabel of test instances.
To investigate this, we triedcombining each of the discriminative weights of afeature with each of its supervised characteristicweights in the following manner.
First, both kindsof weights were estimated from non-weightedtraining data.
Then they were applied one after theother to the training data.
During the test procedure,test instances were weighted only with the globalweights.
The results of these experiments areshown in Table 11.
Results for those combinedweighting methods which outperformed both of thecomponent functions are shown in bold.Certain combined weighting procedures didimprove on both of the component methods.However, none of them showed an improvementover 2.5% on the best of the component weightingmethods (no significance for any of theimprovements could be established).6 ConclusionIn the paper we studied several feature weightingmethods in application to automatic wordclassification.
Our particular focus was on thedifferences between those weighting methodswhich encourage features discriminating classesfrom each other (odds ratio, gain ratio, mutualinformation) and those which favor features thatbest characterize classes (term strength).We find that classification of words into flatlyorganized classes is a very challenging task withquite low upper and lower bounds, which suggeststhat a considerable improvement on the baseline ishard to achieve.
We explicitly exploredparameterization of the weighting functions,finding that the choice of certain parameters,notably the application of local vs. global weightsand weighted vs. un-weighted test instances, iscritical for the performance of the classifier.
Wefind that the most optimal weighting procedureoften brings the performance of a classifiersignificantly closer to the upper bound, achievingup to 17% improvement on the baseline.We find that discriminative and characteristicweighting procedures are able to identify differentkinds of features useful for learning a classifier,both consistently enhancing the classificationaccuracy.
These findings indicate that althoughindividual characteristic features may be lesspowerful class separators, several such features,taken collectively, are helpful in differentiatingbetween classes.ReferencesE.
Alfonseca and S. Manandhar.
2002.
Extending alexical ontology by a combination ofdistributional semantics signatures.
InProceedings of EKAW?02, pp.1-7.M.
Ciaramita.
2002.
Boosting automatic lexicalacquisition with morphological information.
InProceedings of the ACL-02 Workshop onUnsupervised Lexical Acquisition.
pp.17-25.I.
Dagan, L. Lee, and F. C. N. Pereira.
1997.Similarity-based methods for word sense dis-ambiguation.
In Proceedings of ACL?97, pp.
56-63.D.
Lewis.
1998.
Naive (Bayes) at forty: Theindependence assumption in information re-trieval.
In Proceedings of ECML?98, pp.4-15.D.
Lin (1998) Automatic retrieval and clustering ofsimilar words.
In Proceedings of COLING-ACL?98, pp.
768-773.D.
Mladenic.
1998.
Feature subset selection in textlearning.
In Proceedings of ECML?98, pp.95-100.J.R.
Quinlan.
1993.
C4.5: Programs for MachineLearning.
San Mateo, CA: Morgan Kaufmann.J.W.
Wilbur and K. Sirotkin.
1992.
The automaticidentification of stopwords.
Journal ofInformation Science, (18):45-55.Y.
Yang and J.O.
Pedersen.
1997.
A comparativestudy on feature selection in text categorization.Proceedings of ICML?97, pp.
412-420.
