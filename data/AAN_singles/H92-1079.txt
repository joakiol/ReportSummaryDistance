Large Vocabulary Recognition ofWall Street Journal Sentences at Dragon SystemsJames Baker, Janet Baker, Paul Bamberg, Kathleen Bishop, Larry Gillick,Vera Helman, Zezhen Huang, Yoshiko Ito, Stephen Lowe,Barbara Peskin, Robert Roth, Francesco ScattoneDragon Systems, Inc.Newton, Massachusetts 02160ABSTRACTIn this paper we present some of the algorithm improvementsthat have been made to Dragon's continuous peech recog-nition and training prograxns, improvements that have morethan halved our error rate on the Resource Management tasksince the last SLS meeting in February 1991.
We also reportthe "dry run" results that we have obtMned on the 5000-wordspeaker-dependent Wall Street Journal recognition task, andoutline our overall research strategy and plans for the future.In our system, a set of output distributions, known as theset of PELs (phonetic elements), is associated with eachphoneme.
The HMM for a PIC (phoneme-in-context) is rep-resented as a linear sequence of states, each having an out-put distribution chosen from the set of PELs for the givenphoneme, and a (double exponential) duration distribution.In this paper we report on two methods of acoustic modelingand tr~ning.
The first method involves generating a set of(unimodal) PELs for a given speaker by clustering the hypo-thetical frames found in the spectral models for that speaker,and then constructing speaker-dependent PEL sequences torepresent each PIC.
The "spectral model" for a PIC is sim-ply the expected value of the sequence of frames that wouldbe generated by the PIC.
The second method represents heprobability distribution for each parameter in a PEL as amixture of a fixed set of unimodal components, the mixingweights being estimated using the EM algorithm.
In bothmodels we assume that the parameters axe statistically inde-pendent.We report results obtained using each of these two meth-ods (RePELing/Respelling and univariate "tied mixtures")on the 5000-word closed-vocabulary verbalized punctuationversion of the Wall Street Journal task.1.
INTRODUCTIONThis paper presents "dry run" results of work doneat Dragon Systems on the Wall Street Journal (WSJ)benchmark task.
After we give a brief description of ourcontinuous peech recognition system, we describe thetwo different kinds of acoustic models that were usedand explain how they were trained.
Then we present*This work was sponsored by the Defense Advanced ResearchProjects Agency and was monitored by the Space and Naval War-fare Systems Command under contract N00039-86-C-0307.and discuss the results obtained so far and review ourplans for further research.In our system a set of output distributions, known as theset of PELs (phonetic elements), is associated with eachphoneme.
The HMM for a PIC (phoneme-in-context)is represented as a linear sequence of states, each hav-ing an output distribution chosen from the set of PELsfor the given phoneme, and a (double exponential) dura-tion distribution.
The model for a particular hypothesisis constructed by concatenating the necessary sequenceof PICs, based on the specified pronunciation (sequenceof phonemes) for each of the component words.
Thusour system models both word-internal and cross-wordco-articulation.
When a model for a PIC that is neededdoes not exist, a "backoff" strategy is used, whereby themodel for a different, but related, PIC is used instead.The two methods to be compared in this paper consti-tute different strategies for representing and training theoutput distributions to be used for the nodes found in thePIC models.
The first method involves generating a setof (unimodal) PELs for a given speaker by clustering thehypothetical frames found in the spectral models for thatspeaker, a step we call "rePELing", and then construct-ing speaker-dependent PEL sequences to represent eachPIC as an HMM, which we call "respelling".
The spec-tral model for a PIC can be thought of as the expectedvalue of the sequence of frames that would be generatedby the PIC, normalized to an average length.
The sec-ond method, a univariate version of tied mixtures, rep-resents the probability distribution for each parameterin a PEL as a mixture of a fixed set of unimodal compo-nents, the mixing weights being estimated using the EMalgorithm \[9\].
In both the RePELing/Respell ing and thetied mixture models, we assume that the parameters arestatistically independent.
A more detailed explanationof these two methods can be found in sections 3 and 4.2.
OVERVIEW OF DRAGONTRAIN ING AND RECOGNIT IONThe continuous peech recognition system developed byDragon Systems was presented at the June 1990 DARPA387SLS meeting (\[5\], \[6\], \[11\]) and at the February 1991DARPA SLS meeting (\[4\]).
The version presented inthis paper is speaker-dependent, a dwas demonstratedto be capable of near real-time performance on a 1000-word task when running on a 486-based PC.
When run-ning live, a TMS320C25-based board performs the sig-nal processing and the speech is sampled at 12ktIz.
Inthe experiments reported in this paper, the speech wassampled at 16kiiz, the speech waveforms having beensupplied in a standard format by NIST.An important contribution to our improved performancein the last year was our switch to 32 signal processingparameters (consisting of our eight original spectral pa-rameters together with 12 cepstral parameters and theirestimated time derivatives).
The cepstral parameterswere computed via an inverse Fourier transform of thelog magnitude spectrum.
At recognition time, the pa-rameters are computed every 20 ms, while for purposesof training, 10 ms data was used.The recognition algorithm relies on frame-synchronousdynamic programming (an implementation f the for-ward pass of the Baum-Welch algorithm) to extend sen-tence hypotheses subject o the elimination of poor pathsby beam pruning.
In addition, the Continuous SpeechRecognizer uses the DARPA-mandated digram languagemodel (\[15\]), which is a modification of the backoff al-gorithm from \[13\].
The rapid matcher, as described in\[11\], is another important component of the system.
Forany frame, it limits the number of word candidates thatcan be hypothesized asstarting at that frame.
For pur-poses of this paper, which is primarily concerned withthe quality of our modeling, most of the rapid match er-rors have been eliminated by passing through long listsof words for the detailed match to consider, at the costof considerable additional computation.
Similarly, mostof the pruning errors have been eliminated by runningwith a high threshold.
A companion paper \[10\], that ap-pears in this volume, describes a new strategy for train-ing the rapid match models directly from the IIiddenMarkov Models specified by the PICs.
This new strat-egy shows promise for reducing the average length of therapid match list that must be returned at any given time,and thus, speeding up the recognizer.In the experiments described below, models were trainedfor each of the 12 speaker-dependent Wall Street Jour-nal speakers, using the approximately 600 training sen-tences (300 with verbalized punctuation and 300 with-out).
Testing was done using the approximately 40recorded sentences (per speaker) available as the 5000-word closed-vocabulary verbalized punctuation develop-ment test set.In order to incorporate context information at thephoneme l vel, triphone structures were constructed thatinclude information about the immediate phonetic en-vironment that affects a phoneme's acoustic haracter.These augmented triphones, called "PIC"s, are the fun-damental unit of the system, and are closely related toother approaches that have appeared in the literature(\[16\] and \[14\]).
The information that the PICs currentlycontain is the identity of the preceding and succeedingphonemes, and, optionally, an estimate of the degree ofthe phoneme's prepausal lengthening.
Each PIC is rep-resented acoustically by a sequence of nodes.
Each nodeis taken to have an output distribution specified by aPEL, and a duration distribution.
PIC models repre-senting the same phoneme may share PELs, but PELscan never be shared across phonemes.
The parametricfamily used for modeling the probability distributions ofthe durations as well as of the individual acoustic param-eters is assumed to have the double exponential form1 _ Ix - , , IP(x)  = ~ae , ,where p is the mean and a is the mean absolute devia-tion.A detailed description of the original models for PICsand how they were formerly trained can be found in \[6\].The following sections explain how a variety of modi-fications have been made to the original PIC trainingalgorithm.The English phoneme alphabet used by the system in-cludes 26 consonants (including the syllabic consonants,/ L / , /M/ ,  and/N/)  and three levels of stress for each of17 vowels, constituting a total of 77 phonemes.
Approxi-mately 10% of the lexical entries for the 5000-word WSJtask have multiple pronunciations, because of stress dif-ferences in the vowels and expected pronunciation vari-ations.Of course, the number of possible PICs that can ap-pear in hypotheses at recognition time (including cross-word PICs) is vast compared to the number of PICs thattypically appear in 600 sentences of Wall Street Journaltraining data.
This paper reports results when around35,000 PICs are built for the rePEL/respell models andwhen around 14,000 PICs are built for the tied mixturemodels.
When the recognizer asks for a model for a PICthat has not been built, a backoff strategy is invokedwhich supplies a model for a related PIC instead.3883.
REPEL ING/RESPELL INGIn earlier reports \[6\], \[7\], we described a straightforwardprocedure that generated speaker-dependent models viaseveral passes of adaptation of the reference speaker'smodels.
The adaptation process modified the PEL  prob-ability distributions and the PIC-dependent durationdistributions.
However, no new PELs were created, norwas the PEL  sequence for a given PIC allowed to change.The sharing of PELs by different PICs was determinedby the acoustics of the reference speaker's speech, andwas assumed to generalize to other speakers.At the last SLS meeting in Feb 1991 \[4\], we reported ona method for choosing the sequence of PELs for a PICin a speaker-dependent fashion, essentially in the samemanner as had been done for the reference speaker.
Thisstep could be performed once the original PELs had beenadapted using the reference speaker's PIC spellings.
Tothe extent hat differences in PEL sequences for a givenPIC can reflect different choices of allophones, this ex-tra step can capture allophonic variation among differ-ent speakers, and lifts the restriction that the sharing ofPELs be the same for all speakers.
This change produceda significant improvement in performance.In order to take full advantage of our new more infor-mative signal processing parameters, however, a furtherchange was required.
We needed to construct a new setof PELs to serve as the class of output distributions forthe HMMs to be constructed.
It was not adequate tosimply extend, by adaptation, the 8 parameter PELs wehad been working with, to 32 parameter PELs, as thiswould prevent us from making distinctions that couldnot even be seen with the old signal processing.In the previous reports \[6\] and \[4\], we described howa set of PELs for the reference speaker was initiallyhand-constructed while running an interactive programfor "labeling" spectrograms of the reference speaker'sspeech.
We needed to be able to construct a new setof PELs automatically; thus, we implemented a k-meansclustering algorithm whose purpose was to create a newset of (32 parameter) PELs for each speaker whose mod-els were to be trained.
This step involved clusteringthe fxames in the "spectral models" for all of the PICsto be constructed for that phoneme.
A spectral modelfor a PIC is obtained by performing linear stretchingand shrinking operations on PIC tokens (examples of thegiven PIC and of related PICs, available from a prior seg-mentation of the training data, based on the best modelsthen available) and then averaging the resulting trans-formed tokens (which have a common length), to obtaina kind of "expected" PIC token.The primary motivation behind the rePELing step wasto make it likely that each spectral frame would have atleast one PEL that matched it fairly well.
As each of the77 phonemes was limited to having only 63 PELs avail-able for building PICs, about 4500 PELs were createdper speaker.Once the new set of PELs had been created, a dy-namic programming algorithm was used for convertingthe spectral model to an HMM containing up to sixnodes, with each node assigned a PEL and a durationdistribution.
This respelling step drew on about 4000 ofthe 4500 PELs in constructing the HMMs.A summary of the overall training procedure is outlinedbelow, with rePELing and respelling appearing as steps4 and 5:1.
Six passes of adaptation were run on each speaker'straining data, starting with the reference speaker'smodels, using the old 8 parameter signal processing.2.
Segmentation ofeach speaker's data was performed,using the best available models (originally, thoseproduced in step 1).3.
Spectral models were built for each PIC, using all32 parameters, based on the segmentation i  step 2.4.
RePELing was done for each speaker in order togenerate a speaker-dependent se of output distri-butions.5.
For each speaker, respelling was performed to de-termine the PEL sequences that would be used inthe resulting HMMs.6.
For each speaker, one additional pass of adapta-tion was performed in order to better estimate themean absolute deviations for each parameter foreach PEL.7.
Steps 2 - 6 could then be repeated, if desired.Results for this method appear in section 5.4.
TIED MIXTURESWere the model described in section 3 correct, the 32parameters in each acoustic frame corresponding to agiven PEL would be distributed as if they were gener-ated by 32 independent (unimodal) double exponentialdistributions.
However, graphical displays reveal thatthe frame distributions for many PELs have multiplemodes.
Furthermore, it is well known that the parame-ters within a frame are correlated.
In order to deal withthe multimodality of the data and to capture the de-pendence among parameters, Dragon has implemented389a modeling strategy in which the output distributionsare represented in a more flexible way.
This represen-tation, similar to other tied mixture models developedelsewhere (\[8\], \[12\]), also provides the basis for achievingspeaker independence.If we divide the parameters into groups or "streams",with the property that parameters in different streamscan be assumed to be independent, hen our new mod-eling strategy represents the probability of a frame ina given state as the product of probability densities foreach stream, and the probability density for a stream isassumed to be a mixture distribution over a fixed set ofbasis distributions pecific to the stream.More formally, we let f ( z )  represent the probability den-sity of a PEL, where x is a frame, and we assume thatf (z )  is the product of s probability densities, f i (z i ) ,  onefor each stream:3f(~) = H fi(xi).i=1Furthermore, we assume that each fi can be representedin terms of a set of basis distributions gij :Cifi = ~ Aijgij,j=lwhere Ci is the number of components for stream i.At the present ime, we are using 32 streams; i.e., eachparameter is assumed to be statistically independent ofevery other parameter in a given state.
We have as-sumed the 32 parameters to be independent both as away of relating our new results to our old results (whichwere also based on the same strong independence as-sumption), and as a debugging tool.
We chose our ba-sis distributions to be equally spaced double exponentialdistributions with a fixed mean absolute deviation, ar-ranged so as to cover the full range of each parameter.Thus, when a mixture distribution was estimated, it waseasy to see what values in the space were relatively likelyor unlikely.
In the system reported here, the set of basiscomponents i the same for each stream, which wouldnot be the case in a more general setting.The tied mixture P!C models were assumed to be ei-ther 1-node or 2-node models, with the number of nodesbeing determined based on the proportion of very shortPIC tokens.
At the present ime, no PEL is used as anoutput distribution for more than one node.
Each tiedmixture PIC model was built via the EM algorithm frominstances of the given PIC found in the training data forthe given speaker (based on segmentations obtained us-ing the best available models).
Unfortunately, most ofthe PICs that occur in the training data occur very fewtimes, and, not surprisingly, most of the PICs that couldin principle occur never in fact do.Thus, two key problems that must be solved in trainingthe recognizer are (1) the smoothing problem and (2)the backoff problem.
The maximum likelihood estima-tor (MLE), together with many related asymptoticallyefficient estimators, has the defect of being a rather poorestimator when it is given only a small amount of data towork with: think of estimating the probability of "heads"from only one coin flip.
Thus, it is important o smooththe MLE when there is clearly an insufficient supply ofdata.
We have chosen to implement a smoothing al-gorithm with a strong Bayesian flavor.
In this paper wewill not address the backoff problem in any detail; at thepresent ime, when we do not have a model for a PICavailable to the recognizer, we substitute a "generic" PICmodel, which has less specific context information.The Bayesian solution to the coin flip problem amountsto representing the prior information we may have aboutthe probability of "heads" as a prior number of flips, ofwhich a certain number are taken to be heads, and thencombining those "prior" flips with the real flips.
We havetaken a similar approach to the problem of estimatingthe mixing probabilities in our tied mixture models.
Webuild the more common PICs before we build the lesscommon PICs (see below).
At the time that we are readyto build a given PIC, we make our best judgement as towhat the mixing probabilities are for each stream of eachstate in the PIC.
This guess is based on the models thathave already been built for related PICs.
Not only dowe guess the mixing probabilities, but we also make ajudgement about the "relevance" of our estimate, whichis to say, the number of frames of real data that wejudge our guess to be wor th .
We then use these priorestimates to initialize the EM algorithm, and in addition,we combine the accumulated fractional counts for eachmixture component with the prior counts based on ourprior guess, in forming the estimate to be used duringthe next iteration.
Thus we have as our re-estimationformula:~ij = k~j  "4" nijE(k Tt + n.)'where Ai~ is the a priori estimate based on the PICsthat have already been built, k is the relevance of thisestimate, and nij is the accumulated fractional count forthe j th component when estimating the distribution for390the ith parameter in a given node.PICs are currently built in a prescribed order in our sys-tem: we build those for which there is the most data first.Thus, we begin by building the doubly-sided genericPICs, i.e.
models for phonemes averaged over all leftand right contexts.
Then we move on to build singly-sided generic PICs, i.e.
models for phonemes where thecontext is specified only on the right or on the left; weuse the doubly generic PIC models to smooth the mod-els for the singly generic ones.
Finally we build our fullycontextual PICs, but again we build the most commonones first, using the doubly and singly generic PICs tosmooth the fully contextual ones.
When building a rel-atively uncommon fully contextual PIC, it is useful tosmooth the model using models of related fully contex-tual PICs which share some of the context or have closelyrelated contexts.5.
RESULTS ON WSJ  DATAThis section contains results on the 5000-word closed-vocabulary speaker-dependent verbalized punctuationversion of the Wall Street Journal task, using the devel-opment est data.
Table 1 lists results for all the WSJspeakers, displaying the word error rates using three dif-ferent models.
The first column contains the results ofthe first recognition run we did using models obtained bymerely adapting our reference speaker's original models,using our old 8 parameter signal processing, yielding anoverall word error rate of 35.6%.
The second column con-tains our best 32 parameter unimodal models using therePELing/respelling training strategy, after several iter-ations of training, with an overall error rate of 16.4%.
Fi-nally the last column contains the results of our first ex-periment recognizing Wall Street Journal sentences withthe 32 stream tied mixture models described above, butbased on only one segmentation step (segmentation i tophonemes).
This produced a word error rate of 14.8%.It is encouraging that the tied mixture models yieldedbetter performance than did the unimodal models on 11out of the 12 speakers, given that there has not yet beenany opportunity for parameter optimization.6.
CONCLUSIONSThe training paradigm outlined above in the descriptionof our tied mixture modeling has only recently been fullyimplemented at Dragon.
Many aspects of the trainingstrategy await full exploration, but the early results wehave described are very encouraging.
Already we haveimproved our performance relative to our old modelingand training paradigms.In the coming months we plan to focus on a numberof different aspects of training.
First, we will be con-Speaker I Adapt REPELI only Respell001 24.8 8.0002 21.9 9.600A 64.5 24.600B 36.8 22.300C 47.1 28.600D 56.5 23.000F 43.3 20.6203 27.1 14.4400 27.6 12.5430 30.5 13.5431 29.4 14.5432 18.3 5.5AVG I 35.6 16.46.76.726.821.828.120.516.913.912.49.69.84.714.8TiedMixturesTable 1: Summary of Wall Street Journal Results.5000-word speaker-dependent closed-vocabularydevelopment test set word error rate (%) using verbalizedpunctuation.structing basis distributions for streams with more thanone parameter and studying the effect of this modelingon performance.
We anticipate that we should obtainimproved performance as we will then be modeling thedependence among parameters in an individual frame.We will also be studying a variety of backoff strategies,which involve substituting fully contextual PICs insteadof generic PICs, when a PIC model has not been built.Another issue of importance will be the nature of ourBayesian smoothing, which we hope to implement in amore "data driven" way.
Furthermore, we expect thatthe use of tied mixture modeling will allow us to developa high-performance speaker-independent recognizer, animportant goal for the coming year.REFERENCES1.
X. L. Aubert, "Fast Look-Ahead Pruning Strategies inContinuous Speech Recognition", Proc.
ICASSP, May1989, Glasgow.2.
L. Bahl et al, "Large Vocabulary Natural LanguageContinuous Speech Recognition", Proc.
ICASSP, May1989, Glasgow.3.
L. Bahl, P.S.
Gopalakrishnan d D. Kanevsky, "MatrixFast Match: A Fast Method for Identifying a Short Listof Candidate Words for Decoding", Proc.
ICASSP, May1989, Glasgow.4.
J.K. Baker, J.M.
Baker, P. Bamberg, L. Gillick, L.Lamel, F. Scattone, R. Roth, D. Sturtevant, O. Ba andR.
Benedict, "Dragon Systems Resource ManagementBenchmark Results- February 1991", Proceedings of theDARPA Speech and Natural Language Workshop, Febru-ary 1991, Pacific Grove, California.3915.
P. Bamberg, Y.L.
Chow, L. Gillick, R. Roth and D.Sturtevant, "The Dragon Continuous Speech Recogni-tion System: A Real-Time Implementation", Proceed-ings of the DARPA Speech and Natural Language Work-shop, June 1990, Hidden Valley, Pennsylvania.6.
P. Bamberg and L. Gillick, "Phoneme-in-Context Mod-eling for Dragon's Continuous Speech Recognizer", Pro-ceedings of the DARPA Speech and Natural LanguageWorkshop, June 1990, Hidden Valley, Pennsylvania.7.
P. Bamberg and M. Mandel, "Adaptable phoneme-basedmodels for large-vocabulary speech recognition",SpeechComm., 10 (1991) 437-451.8.
J.R. Bellegaxda and D.H. Nahamoo, "Tied MixtureContinuous Parameter Models for Large VocabularyIsolated-Speech Recognition", Proc.
ICASSP, May 1989,Glasgow.9.
A. Dempster, N. Laird, and D. Rubin, "Maximum Like-lihood from Incomplete Data via the EM Algorithm",Journal of the Royal Statistical Society, 39(B), pp.1-38,1977.10.
L. GiUick, B. Peskin and R. Roth, "Rapid Match Train-ing for Large Vocabularies", Proceedings ofthe DARPASpeech and Natural Language Workshop, February 1992,Harriman, New York.11.
L. Gillick and R. Roth, "A Rapid Match Algorithmfor Continuous Speech Recognition", Proceedings of theDARPA Speech and Natural Language Workshop, June1990, Hidden Valley, Pennsylvania.12.
X. D. Huang and M. A. Jack, "Semi-continuous Hid-den Markov Models for Speech Recognition" , ComputerSpeech and Language, Vol.
3, 1989.13.
S.M.
Katz, "Estimation of Probabilities from SparseData for the Language Model Component of a SpeechRecognizer", ASSP.35, pp.400-401, March 1987.14.
K.F.
Lee et al, "The SPHINX Speech Recognition Sys-tem", Proc.
ICASSP, May 1989, Glasgow.15.
D.B.
Paul, "New Results with the Lincoln Tied-MixtureHMM CSR System", Proceedings o]the DARPA Speechand Natural Language Workshop, February 1991, PacificGrove, California.16.
R. Schwartz et al, "A Context-Dependent Modeling forAcoustic-Phonetic Recognition of Continuous Speech",Proc.
ICASSP, April 1985.392
